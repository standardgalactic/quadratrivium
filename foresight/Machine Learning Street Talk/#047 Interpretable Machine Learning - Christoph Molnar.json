{"text": " Coming up later in today's presentation, I'm wondering at what point we're just developing complex math models to explain complex math models and we really haven't, you know, made much progress along the interpretability axis. So you have something you don't understand and you explain it with something you don't understand? If I have if I have some general formula, just some very general formula, and then I go in there and I go, you know what, this formula has five parameters. And if I make this 1.75 and that one one-third and this one two and that one zero, and I call this the Megatron activation potential, and I go and write a paper about it, that's really just an arbitrary selection of a bunch of numbers. And then you gave it a fancy mathematical passport and you got it published in some journal. And now everybody has to memorize that as you know, the Megatron potential and kind of learn about it. And that's a lot of what's going on right now is that it's really just a bunch of hacking. Welcome back to Street Talk. Today, we're going to be talking about interpretable machine learning. Enjoy. Interpretability has become one of the most important topics in machine learning. And it's something that every data scientist needs to be familiar with. For hundreds of years, we've had simple interpretable models like linear regression and rules-based systems. But in recent years, there's obviously been a huge rise in more complex, bigger, nonlinear models. And of course, predictions from these models are not always so easy to explain. So as we start to use these more powerful, nonlinear models to actually make decisions on real world matters, then it's inevitable that our attention must now turn to interpretability and explainability. When I first started learning about machine learning algorithms, I was told they could be dangerous. They were hard to understand. They were black boxes. But as Christoph lays out, it turns out there is a whole plethora of techniques out there to explain why a model made a certain prediction. Some models like low-dimensional linear regression are intrinsically interpretable. You can just look at the model coefficients and that tells you exactly how the model is working under the hood. Then there is a whole suite of methods that will actually work with any ML model. Like training a local surrogate or a global surrogate. There's also Shapley values, which is a really cool technique that allows you to distribute blame for the prediction amongst the input features in a really theoretically sound, really principled way. And then there are domain specific methods. For example, to explain image models, you can try to highlight the most relevant parts of an input image by making saliency maps. And there's more. You can look at things like example-based explanations where you try to find the smallest change in the input data that would cause the output prediction to change. So maybe with this awesome new interpretability toolkit, we can start to dispel that myth that machine learning models are all just black boxes that can't be understood and can't be trusted. Christoph Molnar is one of the most important people in the interpretable machine learning space. In 2018, he released his magnum opus, interpretable machine learning, a guide for making black box models explainable. Interpretability is often a deciding factor when a machine learning model is used in a product, a decision process or research. Interpretability methods can be used to discover knowledge, to debug or justify a model and its predictions, to control and improve the model, to reason about potential biases in the model, as well as increase the societal acceptance of models. But interpretability methods can be quite esoteric. They add an additional layer of complexity and the potential pitfalls require expert understanding. Machine learning models are inherently less interpretable than classical statistical models, but typically they have a better predictive performance and that's because of their ability to handle nonlinear relationships and also higher order feature interactions automatically. But do we have to suffer this implicit trade-off between the complexity of a model and the lack of our ability to understand it? Simplistic model approximations can often mask important information and be misleading as a result. In classical statistics there's an entire field called model diagnostics to do exactly this, to check that assumptions and simplifications have not been violated. This is something that does not yet exist in interpretable machine learning. Interpretability has exploded and matured in the last few years, in particular since the deep learning revolution. We now have a better understanding of the weaknesses and strengths of interpretability methods. A growing number of techniques are available at our fingertips that can lead to the wrong conclusions if applied incorrectly. Is it even possible to understand complex models or even humans for that matter in any meaningful way? That is one of the questions that we're going to be discussing this evening. Molnar also recently released a couple of papers where he discusses some of the important pitfalls of interpretable machine learning methods. So some of the things that Christoph Molnar is really concerned about is the lack of statistical rigor in IML methods. Molnar used to be a statistician. Also he is exasperated with some of the misguided causal interpretations from some of these IML methods. He also points out feature dependence or situations where you have shared information between features. It completely breaks many of the IML methods and this is something that he focuses on a lot. He also focuses philosophically on the broader impact of interpretability and what interpretability even means frankly. It's a very nebulous term. So let's have a quick flick through this paper, interpretable machine learning, a brief history, state of the art and challenges and as well as pointing out some of the history of IML methods, we'll jump straight into one of the challenges which is feature dependence. Molnar points out that feature dependence makes attribution and extrapolation problematic. This is exactly what happens in partial dependency plots for example. We are basically extrapolating and we are creating fictitious data points that didn't really exist and these fictitious data points probably exist outside of the data distribution. So Molnar thinks that the models that we build should reflect the causal structure in the world but of course that is not really the case most of the time and he points out that statistical learning is just reflecting surface feature correlations not the true causal structure beneath the scenes. Causal structures would be more robust if we could actually capture them and the predicted performance and learning causal factors is a conflicting goal which I think not many people have thought about. So we need to think about when we can make causal interpretations and a lot of work is underway in this field but being completely frank this is very nascent. There's not really much out there at the moment. Molnar also points out this lack of statistical rigor having been a statistician himself. He was exasperated when he came into the IML field just to see that most IML methods do not even give you confidence estimates something which is completely standard in the statistical world. Models and explanations are computed from data which means they are subject to uncertainty but this is something which is just not captured using current methods. He says that we need to be making distributional and structural assumptions. He points out this risk of p-hacking something which is prevalence in the natural sciences. This is something that could be coming to the world of IML very soon if we don't start thinking about this more carefully. Molnar also points out that there is no accepted definition of interpretable machine learning methods so it's not entirely clear how we can compare IML methods to machine learning models. It's really easy to assess machine learning models because we have benchmarks and we have ground truth labels. Those benchmarks are fraught with problems as well but we can't really quantify how correct an explanation is and it doesn't really help that there's a taxonomy of interpretability methods. There are objective methods like sparsity and interaction strength and there are human-centered evaluations from domain experts or from lay people and quite often you need to have quite a lot of technical knowledge to even understand these assessments. He says that the setting of machine learning is too static. It doesn't reflect how these models are actually used in practice and I really love this idea of thinking about a process rather than thinking about just the model so he says we need to have a holistic view of the entire process. He thinks that we need to think about how we explain predictions to folks from diverse backgrounds, how we have interpretability at the societal level or at the institutional level thinking much more broadly than we are at the moment. He also thinks that we need to reach out to other disciplines for example psychologists and social scientists and he thinks that there's lots of rich knowledge in computer science and statistics that we're just not using yet. So in July of last year he also released this paper pitfalls to avoid when interpreting machine learning models. In this paper he points out that there's a growing number of techniques providing model interpretations but many will lead to the wrong conclusions if used incorrectly and he goes on to point out many of those pitfalls. For example the first one is assuming that the model generalizes well so assuming that the model has been fit correctly if the model is underfit or overfit then the interpretation method will perform badly as well and interpretation can only be as good as the model underlying it. So the next pitfall he points out is the unnecessary use of complex models which is to say the use of opaque or complex machine learning models when an interpretable model would have sufficed which is to say when the performance of an interpretable model is only negligibly worse than one of these black box models and to be honest this is something I see all the time I think the gratuitous use of complex machine learning models is something which is really serious. One of the things I don't like about machine learning is the laziness. I think we should always seek to understand and simplify problems wherever we can it's the same thing in software engineering. We should always be trying to create the most elegant and simple and maintainable solution. We shouldn't be trying to over complicate things and I think that's a very you know the kiss principle is very generalizable here. So he recommends to start with simple interpretable models like generalized linear models or lasso models or additive models decision trees or decision rules and gradually ratcheting up the complexity as required. So he also points out that ignoring feature dependence is super important right and this is a problem that many of the IML methods have so he gives an example of partial dependency plots where they extrapolate in areas where the model has little training data and it can cause misleading interpretations. So these perturbations produce artificial data points that are used for model predictions which in turn are aggregated to produce global interpretation so that's a big problem. Another thing he points out is confusing correlation with dependence so he gives an example here features with a Pearson correlation coefficient close to zero can still be dependent and cause misleading model interpretations while independence between two features implies that the Pearson correlation coefficient is zero the converse is generally false. So there's a pretty cool example here this is a couple of features that absolutely have a dependence on each other you can see it visualized here but you wouldn't know that if you looked at the Pearson correlation it would have said that it wasn't significant. Another one misleading effect due to interactions so there's a couple of things here there's the partial dependency plots on a couple of dependent features and then he's used a simulation to kind of trace all of these different features to see what the predicted label was and according to these IML methods there is actually no clear dependency between these features and the predicted outcome whereas you can see that that's just blatantly false. So something I've been meaning to do for more than a year now is to go through Molnar's interpretability book and to make some bite-sized videos on every single approach well Connor and I are actually going to do that over on Machine Learning Dojo with the first one next week on Shapley Values so make sure you subscribe to Dojo and check that out. Remember to like comment and subscribe we love reading your comments and we'll see you back next week. Welcome back to the Machine Learning Street Talk YouTube channel and podcast with my two compadres Connor Tan who runs the Thomas Bay's Appreciation Society and MIT PhD Dr Keith Duggar. Now they say that Germans are known for beer, sausages, precision and these days interpretable machine learning. We have an exemplar German on the show Christoph Molnar. Now Christoph made waves in the community when he released his Magnum Opus interpretable machine learning a guide for making black box models explainable. If a machine learning model performs well why don't we just trust the model and ignore why it made a certain decision? Well the problem is that a single metric such as classification accuracy is an incomplete description of most real-world tasks now as according to Doshi Values and Kim in 2017. In Christoph's book he introduces the importance of interpretability and reports an incredibly detailed taxonomy of interpretability methods and his style of writing is at times entertaining and entirely absent of hype and nonsense. He runs the gamut of interpretability models so for example model agnostic methods like Lyme and Shapley values. Example-based methods such as counterfactual examples and adversarial examples he motivates the importance of interpretability methods but he's also extremely transparent about its current weaknesses and pitfalls. He's currently finishing his PhD in interpretable machine learning at the Ludwig Maximilians University in Munich after getting a stats master's from the same institution. He's recently written several very interesting papers on interpretable machine learning for example pitfalls to avoid when interpreting machine learning models in July of 2020 where Christoph detailed several problematic model interpretations for example ignoring estimation uncertainty feature interactions or confusing correlations with dependence. More recently he published a paper called interpretable machine learning a brief history state of the art and challenges while he acknowledged that the field is maturing nicely. He also spoke about some of the serious challenges in IML methods such as the lack of statistical uncertainty, shared information between features, lack of a clear definition of interpretability and the need for a more holistic view. Christoph Mulner it's an absolute pleasure and welcome to the show. Thank you very much for the invitation glad to be here. You know Christoph I have to say I really enjoyed your book. I read this actually some months back in preparation for a completely different show. I loved how scientific it was you know it was it was very much laying out essentially a survey of the facts a lay of the land very objective evaluation. It had both the pros and cons you know of different approaches examples to make them you know more understandable so kudos to you. I thought it was a great book very enjoyable and very informative. I also loved how it lays out the beginning you know what the goals that we're trying to achieve with with interpretability are especially kind of the human goals right like what does it mean for an explanation to be good for people what kind of explanations do people like and sometimes there can be conflicting conflicting goals there and I think one thing that that I realized from reading your book is that that actually explanations can be deceptively good yeah like I think I think the the the sort of cognitive bias maybe that we have to look for contrastive explanations or counterfactual explanations like and principle it seems good it's kind of like you know I'm sorry we can't give you this loan you know well why not like why can't you give this loan well well we've detected really that you're a that you're a deadbeat what do you mean I'm a deadbeat yeah you know you never pay your bills well let's see why okay let's look through this and we find a decision tree here and and some big decision tree and we get to this one little point what says here you know you didn't pay this furniture bill back in 2018 you know if if only you'd have paid that furniture bill like we'd be able to give you the loan right but the truth isn't that simple like it's actually buried all throughout yeah throughout the decision tree right with so many contributing points yeah I think I like this chapter that you referenced was about like kind of from the social view or the human view what what what people like or prefer is explanations and the whole chapter is based on I forgot the title of the paper it's like from Miller about like what we can learn from the social sciences about what a good explanation is and was like a paper where I learned a lot and it was super interesting also to see how like what people think are good explanation as you mentioned they should be contrastive they should be short but they should also confirm to some prior knowledge that the people have and I mean like objectively a lot of those things might not like you wouldn't say these are good explanations in some sense like maybe maybe it's not good to give an explanation that fits with a prior knowledge because it's not the correct one maybe so it was quite interesting to learn and to think about like what's the human side of it that's a very cool part of your book I thought the fact that it's actually quite interesting thinking about what we really want out of an explanation I remember first of all looking at you know sharp values that are very fair and will distribute the blame equally amongst all the different relevant features and then you turn to something else like you know selective interpretations that in a way are way less good because they're kind of arbitrary they'll just select a few a few a subset of the features and give them all the blame but then it turns out that apparently that's what people actually want as a useful intuptable explanation yeah so as I see there's like many many dimensions of explainability or like what what can be a good explanation and one of these dimensions is maybe sparsity that you have a short explanation with just a few facts that's something that people prefer maybe but this might be in conflict with other dimensions of a good explanation which should be maybe that all causes should be addressed by the explanation that plays some role at least but this is of course in conflict with sparsity if you want this full attribution like you get with shepley values for example so I that's why I also think there's not like just one correct explanation but there's like many attributes or many dimensions on which you can judge explanations yeah I think this is one of the problems because even machine learning is really difficult right because we use benchmarks and benchmarks are just something that people have come up with but you say in your you know you talk about one of the problems being that there's no definition of IML methods to start with but at least in machine learning methods we have ground truth which is which is significantly better in a way but if we if we can't quantify how good an explanation is then where are we really because you talk about a kind of taxonomy of interpretability methods right you say that there are objective evaluations like sparsity and interaction strength and fidelity and humans human-centered evaluations you know which might come from domain experts or lay people so I suppose you're just hitting straight on the fact that this is actually quite nebulous isn't it yeah so yeah so in some sense like there's this big criticisms okay this is not scientific or not well defined at least what interpretability is how can we even do research in this area but I have a bit more relaxed view I mean otherwise I should have stopped writing the book before I really started so I kind of see like this endeavor of giving interpretability or bringing interpretability to machine learning it's more like a first of all it's just a keyword so it's it kind of bundles all the methods together that kind of aim to reduce this high-dimensional function to something well mostly it's something in a lower dimension so we kind of just do this mapping something gets lost in a way this is fine and it's I think part of science to find out like or to yeah some part of analysis to find out what part gets lost so when you for example look at just some feature importance values for example of course it's a summary of your model and a lot of information gets lost but I still think it's useful to have obviously so many people use it but it's useful to have these tools and we just have to understand what they do and how to interpret the results so how do you interpret when like the feature importance is zero of a feature could that be quite dangerous though because this you gave the example of random forests when you have a lot of shared information between the features it would actually tell you that these correlated features have a higher feature importance than you might otherwise expect so does this imply that we need to have very detailed knowledge of how we should how we should use this information that we get from IML methods yeah so it's also kind of the direction in which I write papers like this pitfalls to avoid and stuff like this so I think so these are just tools so they do something with the model a kind of distill some knowledge so for example for feature importance you kind of measure how well does your model perform and then you measure again after you shuffle one of the features and and then then you get something out of it so then we can ask questions is this interpretable or not and it's kind of well not so relevant the question because you just have to understand what what happens when you shuffle feature and one is for example you kind of break the association between the feature and the prediction because now it doesn't carry the information about the target anymore because you're shuffled in randomly in your model so you kind of this this feature importance now measures how much performance you lose because of this break of information but then you also when you think about this method and want to use it you also have to understand that this shuffling for example breaks also association with your other data feature like the features in your data so this is a limitation of the method and what I think is needed is that we understand in which way these methods break or which scenarios we're allowed to use them or how we are allowed to interpret them and I think the situation is kind of similar to statistics where you have these models and and then you interpret like the coefficients of your models you still like have to learn how you do the interpretation what are the assumptions that have to be met that you're allowed to do this interpretation and I think it's we're in a similar situation here with interpretability of machine learning and I'm glad you mentioned sort of the old school linear models as well as dimensionality in the thread because you make a very good point in the book which is look even these so-called intrinsically interpretable models are only interpretable up to a certain dimensionality and you know I have I have tons of experience with with multi-linear regression right and and I can guarantee that beyond a very small number of dimensions those coefficients are not interpretable because it starts to play a bunch of games where it's inflating one coefficient and another because their difference is important and you know whatever else is happening a lot of correlated structures are all essentially getting compressed into the small number of small number of weights right and so as the dimensionality goes up I would say like no model is is intrinsically interpretable same can be said of decision trees like anybody who's looked at a decision tree that's come from real data you're going to find out it's not interpretable it's like oh look at this you know market capitalization matters oh and it matters over here too and down here and and actually I have to go through five checks on market capitalization before I get down to this decision and maybe the features aren't that intuitive either and then you have to kind of like mentally stack up like until you get to the decision like five decisions and then we have a very complex rule that led you to the prediction yeah so I agree that there's it's not like I mean I have I have this distinction the book like interpretable models and not so interpretable models um but it's as you say it's like a gray like it's a scale really people could definitely overhype how interpretable these white box models are right whether it's linear models as I'm a I've worked with many physicists who uh have had guidelines that you should only ever use models like a decision tree because it's possible in theory to write down on a piece of paper exactly how the decision is made right yeah you can trace every decision but that's never actually useful in practice is it since when have you ever looked at a decision tree fitted to data there's any complexity and the fact that in theory it's possible to go and examine how it works yeah it's completely irrelevant in practice isn't it yeah I mean it can be useful to have like a short decision tree sometimes it but in practice it will not like give you probably the best predictions um but it might be useful sometimes to shorten it artificially so even like you're throwing away some some predictive accuracy um but you shorten it so you understand that somehow it's manageable you can have a look at it and and see what's going on well there's also you know the other issue is that as I was looking through a lot of the methods that you describe and you survey in your book you know some of them um are not simple I mean if you start looking at partial dependency plots and trying to explain what those are I mean you know you have to almost have a deep mathematical knowledge really to appreciate them in the first place so I'm wondering at what point we're just developing complex math models to explain complex math models and we really haven't you know made much progress along the interpretability axis yeah yeah it's true it's also like the criticism too like um so you have something you don't understand and you explain it with something you don't understand um I think some methods are complex um but for some at least there's some intuition how they work for partial dependence plot is kind of your this um intuition that you do some intervention on your more or intervention on your data so you replace all your like for one feature you replace all the values with one fixed value and kind of look at the average prediction that you get afterwards and do this for a lot of points and then you connect the points and you have this curve so kind of gives gives you the expected change over the feature range maybe there was already a bit complex I don't know um maybe I'm too deep into the method already um but yeah of course it's it's something additional people have to learn or if they agree to use it of course could I get a quick take from you on saliency maps as an example because you said in one of your youtube videos that saliency maps are glorified edge detectors they are not good explanation at all and I've noticed now that many machine learning platform providers are building these kind of um saliency maps into their models you know into their platforms and then it becomes a kind of box ticking exercise where you can say okay well yeah we've done interoperability now that's all you need to know and that really is quite a false sense of security isn't it it's funny you mentioned the saliency maps because I'm writing a book chapter about it and actually I'm I wanted to publish it today maybe I will or at least in the next few days um it has been a long time in the making and it was very very frustrating like by far the most frustrating chapter to write um number one reason is because there's so many methods out there uh reason number two is I I can't judge really or if they work and it seems like they mostly don't or it's it's still unclear like how you say you would judge that they work so they're like dozens of these like integrated gradients gradients deconfinate deep tailor decomposition layer-wise relevance propagation in 10 variants um so and then I mean you in the end you when you apply these methods on they are also like for image classification and you get these nice-looking images and some areas are highlighted some or not sometimes you can say okay this doesn't make sense at all um but if if it kind of makes sense then you maybe would be inclined to trust the method um but then there's this uh paper which is called um sanity checks for saliency maps and they kind of found out that they the most of the methods are very similar to edge detectors meaning that they are kind of insensitive to the model and the data which is very bad of course well if you change the model um the explanation should obviously change um could could you expand on that a little bit so you said it wasn't really reflection of the model or the data but what what would a perfect saliency map look like well I don't know myself actually so I mean the the ideas that you saw the basic idea of most of these methods is that you you have your class prediction or your class score and you want to back propagate it not you want to back propagate it to the original image so you look at the gradient um with respect to your input pixels and that's there's no not not one way to do this but there are many different ways so that's also why we have so many different methods and then they highlight which pixels were relevant for the classification um but yeah they these these methods they have like a lot of like issues for example there's the issue of saturation for example because of the real unit where you have flat parts of the gradient so if you pass the gradient through that then um your method would say that some some neuron might not be uh important at all and there's a lot of these little issues that these methods have um yeah so but but back to your question like I think that's also the issue that I don't wouldn't know how to answer I mean obviously it should be some area that should be highlighted on this aliens method was important for the for the neural network um but then again I don't know how the network decides so I couldn't like if I see an image I couldn't like highlight the part I mean I could highlight the part where I think the network should look but then again I mean there are lots of papers like the clever hands paper which saw like the reveal that there are some sometimes it would look at watermarks on on the photo um so these are like these things that we just don't know uh what the neural network basis this on have if I could take a stab at that answer for one I think just the idea of quote a saliency map is a problem like there isn't one map of of the importance of the pixels it's like they're they're operating on multiple multiple dimensions or at least sort of multiple feature sets it's like if you ask me to tell you know why is this image a dog you know well for one thing it's it's the overall shape you know it has four legs and you know two ears sticking out over here that's one saliency another is that it's it's got a certain color you know and it and it's coat and that's a that's a different concept of what's salient and another is that there's a frisbee flying at it and its mouth is open and it's about to catch it and I know dogs do that so they're kind of you know when your mind analyzes an image it breaks it down into these many large scale kind of structural features and I think that gets completely lost and most of the approach is the saliency maps this is really important point actually because if you're just looking at the pixels on this kind of 2d planar manifold that's only a very it is quite literally a surface view and I think Christoph you said that there are all sorts of causal structures and even in the model itself right there are these entangled neurons and surely that's giving me more insight into what's actually happening just seeing a bunch of pixels and the other thing is that these models that they are completely lacking in robustness so probably if you changed a few of the wrong pixels your saliency map has just got completely broken right yeah so um but in in that vein some of these feature visualization techniques you know like the deep dream type stuff maybe maybe that's a better way of of interpreting these models yeah um so like for the one point you mentioned about the adversarial examples so there's also a paper I forgot to title again um which that manipulated neural networks so they would give the same prediction for all the images but different explanation like different saliency maps so this is perfectly possible to create different explanations um for these saliency maps um but but keeping the model like at least for the predictions the same there's another criticism you can throw at saliency maps where they they can be quite deceiving you think they're useful and they turn out not to be useful yeah there's a classic example of looking at you know comparing a dog to a wolf and sometimes you see it's looking at the snow in the background and that's helpful sometimes it highlights the animal and you think okay I understand it's looking at the face that's why it thinks it's a dog because it's in the face and then you look at the predicted class for something else like you know a cat or a frisbee or a house or a boat and it highlights the face as well yeah so the saliency map for all these different classes looks the same and when you realize that you realize this this saliency map hasn't actually told you anything about why it's gone for one class versus the other all it said is that it's just highlighted the thing in the middle of the picture yeah I think that's especially also when when you look at images you know like we're very good with images yeah like we were very quick to see what's happening on a scene and such so I think we're also very quick to make judgments oh yeah this makes sense this doesn't make sense it's more difficult to interpret like if you have like a graph and there's like things going on inside you have to like now understand what the method does and stuff like this but for an image like a heatmap IR this area is highlighted makes sense case closed I like the method yeah and that gets exactly back to the deceptively deceptively good explanations problem and explaining complex things with complex things we don't understand you know so I think a lot of people if they looked at it and again one of the points of this interpretability is really the social aspects of it right like being able to convince people to be at ease with machine learning models or to accept the results of of a machine learned you know decision process and I think if somebody looks at an image of a dog you know they have no problem understanding that but if you showed them a bunch of salience maps or or any of the other sort of you know feature projections if you will like you said it takes a lot of deep understanding to understand those whereas the image is kind of immediately obvious I think two of the main themes that you touch on is we'll get to the to the probabilistic stuff the the Judeo pearls stuff in in a minute but I think the main issue that you point out is feature dependence okay and and you say that when you have feature dependence it makes attribution and extrapolation problematic so a dependence just means that you got correlated or shared information between your features right so you say that in feature permutation methods these things basically break everything when you have the shared information and the extrapolated data points are no longer in the distribution and you say that there are conditional permutation schemes you know that try and and maintain that joint distribution but those things sometimes make it even worse right so do you think that's one of the most important things that people should think about when using iml methods yeah at least so that's at least like a very um deep issue I would say which is inherent in in most of the model agnostic methods where you manipulate your data see what how the model prediction changes and then create your explanations out of this sort of select the shadowy value line partial dependence plot feature importance they all work with this mechanism of manipulation of the data prediction and then kind of aggregating the results and most manipulations happen in isolation so that you for example when you for feature importance you can meet one of the features as I like said before and then well you break the association of target but also a few other features but similar things happen if you use lines or you kind of replace parts of your image but then again you also have to replace it with something like which is I think in line the defaults with just a gray image and then of course it's not like it's outside of your data distribution subtly because your network was not confronted with like these patchy images before they had like just normal photographs usually and depends on your neural network but I mean you certainly didn't train it on on images where parts were grayed out so it's pretty likely what the model should predict and what will predict at this point but but you use these images to create your data set like you send it through the neural network you get predictions and you kind of aggregate from this your explanation but you left your data distribution and your model can do anything then and the hope is that it doesn't do anything crazy but yeah you don't know like it like a simple example from the medical field would be that you know height and weight are highly correlated right and and on the other hand sort of the ratio or some relationship between your your weight to your height that actually has very important medical consequences right that's that's the measure of of health and so if I were to sit there and just permute say the height index and create a whole bunch of people that had all these bizarre combinations of of height and weight you know first of all those don't even probably exist in the data set and the ones that do exist in the data set probably had some medical issues right yeah well you actually gave a similar example I think you gave the example Christoph of a baby that earns a hundred thousand dollars a year which is which is insane but when you talk about something like lime maybe that's different because the cnn the what you know it shines a flashlight over the input space in it and it's a kind of local method so in some sense you could argue that it doesn't matter that you've grayed out all this other stuff because if the model was sufficiently well trained in the first place it should hopefully learn to ignore the background or is that just wishful thinking that's an interesting thought I haven't thought about it because like the property of like that you have these filters that trust a wander over the image yeah maybe it would make it more robust for these kind of interventions that we do when we create these images with lime and shetley um yeah I haven't thought about it it could be uh well we've mentioned all of these ways in which interpretability methods can go wrong right how the model might not be a realistic the interpretability model might not be a good approximation to the actual ml model so some people a bit controversially perhaps take the idea and remember that and say you're just working up completely the wrong tree and you should give up using interpretability interpretability methods to explain these black boxes this dish them instead just use an interpretable model to begin with use a white box model I think there's an example um from compass in the US which is that model to predict reoffending and I think quite famously there was a investigative journalists that tried to interpret this model it was a black box model because it's proprietary right it's a trade secret and they they fit it a proxy model a kind of a linear model and they made a report saying okay we think your model is racist because it looks like it's it's taking race as a factor and then some further work was done and they came back and said well actually you've just used a uh interpretability model that doesn't really fit our model very well you've made some assumptions that don't hold if you use a different interpretability model you get a completely different answer that it doesn't use race at all as a factor and so you've got to kind of a you've got to a wrong assumption by using a bad interpretability model and I think they were saying that this model instead you could get just just as good a model of reoffending with like three FL statements you know ditching this massive complex 100 and something features and just use three FL statements based on I think age and reoffending so is that was that what we should do should we just drop these methods and start using white boxes instead so I mean like one one thing to mention here is that a white box is very soon also like a gray or black box if you add interactions if you have many features and so on but putting that aside I would agree if you're in first place like that you say you should start with like a white box so if you start modeling then then you then you should consider these first like maybe they already solved your problem then it's perfect and you have a model that is quite I mean stable it's interpretable I think that would be great but then I think the next step would be to see like what like a black box or a machine learning model would give you in terms of performance and then maybe if you see the gap is really big then maybe you can try some feature engineering and close the gap maybe from the interpretative model to the machine learning model but then you're probably already infusing some features that are not so interpretable or maybe if you're using a linear regression model you're maybe using then splines and interactions so you're already moving towards more complex models usually but then if you still have a gap then I think you have to decide is the gap and predictive performance like worth changing to a black box model so I think that's your decision will be different in many cases as you relates back to the point you made on your paper about criticisms of using interpretable machine learning models that some people leap straight away at using an overly complex model and sometimes depending on the situation sometimes you know a linear model can do just as well and have all these advantages it's so much easier to explain do you have a philosophy from a high level here right because if it if it were a human if it were an airplane pilot we don't really understand how the brain works right we would just test the pilot you've got to fly the plane for 10 000 hours and if you don't crash then we'll let you fly so we don't really seek to understand how his or her brain works but with machine learning models there's this continuum right so if you use these complex black box models the predictive performance is usually better but you're trading off understandability and assuming those things are completely mutually exclusive what kind of decision process do you go through when you select these models but by the way with machine learning right the reason why we use machine learning is because we don't understand how to do something explicitly yeah is that a fair statement um yeah I would say um when when it ate us like so high-dimensional so complex many interactions and so on that your simple models don't cover the complex cannot cover the complexity I think then you need machine learning would you rather understand exactly how the plane worked or would you rather I mean if I was saying to you you can go and fly in a plane would you rather that you understood how the plane worked or would you rather that the plane was tested why not both so um I think we can do both it's to some degree so um of course with black box model we don't exactly understand how they work um but in comparison to a pilot we can test them for three more or less um so because I mean maybe it's not as good as an interpretable model but we still can use a lot of methods to at least approximate and and try to understand a few properties of this model so I think we are even in a situation where we don't have like these complete like A or B decisions but we can have so if if the machine learning works much better and it's like really robustly tested with lots of different data I would prefer machine learning model I guess um but then I would also want to like people to to apply all these methods that are available even if they are not perfect but still they give you something they give you some insights so yeah and I think so Tim one answer to your question is that a lot of people's response here and kind of demanding interpretability and having concerns about machine learning it all comes down to generalizability and we've seen through using machine learning that it breaks down in ways that that we don't like like for example sure maybe the soap dispenser you know is really great at dispensing it dispensing soap you know 87 percent of time but it just so happens to kind of be a race sensitive soap dispenser and just doesn't give any soap to people with a certain skin color like we've kind of decided is a society that are that there are certain generalizations or certain dimensions along which our models just have to perform and also because a lot of these these things that break machine learning models are things that happen quite quite regularly in the real world it's like a pilot you know a human being pilot flying around if he looks down at the ground and sees a hot air balloon with a big smiley face on it he's not going to crash the plane he's just going to be like oh yeah I forgot about the uh the hot air balloon contest that's going on today where's a machine learning model if it looks out a camera and sees something with a particular shape of lightning bolt you know it might just decide it's time to like dive for the ground right and crash the plane like that's sort of what these adversarial examples kind of show and I think that's why people are really hungering for human understandable explanations because still to this day the human brain is the only AGI really that that we have around yeah but deep learning models that they they essentially memorize lots and lots of things and they have this sparse coding so in a way it's just like the white box model even if we use interpretability methods we could enumerate all of the things that they are learning and one of those things might be a sensitivity or lack of sensitivity to hot air balloons or smiley faces on but even if we could enumerate all the things that they are learning we wouldn't understand that either in the same way we don't understand how a real human's brain works and I'm not sure whether we should view a human brain as a computer program and whether that's a good thing or a bad thing but at some I guess what I'm saying is at some point we have to accept that we're not going to understand a totally it's actually it's a good comparison I think with humans you know if you're interviewing and you want to hire let's say a software developer you tend to set them a coding interview you wouldn't think of taking them into surgery opening up their brain and trying to find the neuron that predicts what the next you know but if code is going to be and understanding how that works it's just why would you do it that way instead you learn to trust humans by working with them giving them a test seeing how they perform in the real world and I know maybe we're asking too much of a machine learning model if you want to be able to understand these complex things in terms of like a bottom-up white box set of rules. I think what a comparison falls a bit short is that that we have the luxury that we can cut off the like cut open the brain of a machine learning model without breaking it and without hurting it hopefully and we can do all these try out all these interpretation methods see how it behaves under certain situations and I also would make a distinction between we understand what's going on inside and doing like this kind of sensitivity analysis where we just try out what happens in certain scenarios so it always do that that we can like check like how it behaves so I mean feature importance is basically like a way to see like how does it behave if we break some features and then we rank the features by this as an importance we can do it and that's also the big difference between humans because we can't test in the same way and yeah shouldn't probably. I think there's one other difference though which is to do with the substrate of how neural networks work I think if I'm giving someone a job interview or something I mean of course it's a very valuable process but I'm looking at their values and I'm looking to try and understand how they would behave in different situations and I'm coming up with lots of illustrative examples but the difference is with humans we have that level of generalization we have a kind of guiding taxonomy of behaviors which means if I know if I have guiding examples of what a human will do in certain situations I expect that to generalize whereas my hypothesis is is that a deep neural network model is almost like an infinite number of rules and there's absolutely no carryover between the rules so knowing even some or even most of the rules doesn't really tell me about those edge cases. Yeah I would agree that the edge cases are quite unforeseeable probably I mean at least we know that they exist like with adversarial examples so even if we don't know like exactly what they would look like or there's even an infinite amount of uh I mean there's an infinite amount of like how you can change the image to make it like have a different class so we know so I think it's important that we know that these exist at least. Yeah I love the point you made though that we have we have the luxury to do analysis on these on these boxes because we can open them up and that that's another point I'm pretty sure that you make this in your book as well which is that part of this is just scientific inquiry it's it's like understanding better how to interpret and explain machine learning models will probably actually contribute to us being able to construct even better machine learning models isn't that true? So so you're basically saying that um also interpretability might help to to be better at like create better machine learning models themselves? Yeah like as we as we develop these interpretability methods because in a sense like you point out earlier there are statistical projections of kind of the behavior of the model and so like a saliency map you know if we can if we can kind of use that to learn the way in which the the neural networks are behaving it may it can certainly give rise to intuitions on ways to alter the model. Yeah so I also have seen approaches where they try like kind of fuse also these two worlds like interpretative models or white box models and black box models so that you try to to generate features out of the black box model which you then use in your more understandable white box model so um I think this and and this also like by using similar techniques um to which you would use for interpretability like detecting interactions for examples for example so um yeah these can be used also to to build better models and also to build better interpretable models. The other and I'll make one last point here which is another social good that can come out of interpretability is imagine we've got you know an ML model that's not trying to make any decisions but it's just trying to figure out what leads to happiness and success in life you know and so we analyze a whole bunch of data and we find out well it's really important if you graduate from high school and it's really important if you you know don't have children before you're married and you know all these other factors if we can dive in and kind of isolate those factors it actually allows people to have some guidance on oh look we've had this machine learning model that's analyzed a bunch of data and it actually has some some understandable recommendations for how to lead a healthier life or a better life or whatever. Yeah I think that this very good example were the fact that you have a prediction model doesn't solve your problem so actually it's just a means to to some other goal in this case understanding like what are the factors for happiness and one example I am from a friend who worked at a telecom company and they built like a churn prediction model to see like who will quit the telecom contract and then they started like the ones with the highest likelihood they started sending out emails say maybe offering them a better deal but actually the outcome was that well they they when they once they wrote to the customers they well left and quit their contract so it's kind of had like so this is a case where the prediction model actually works but then they people leave in this case probably because they realized ah shit I have this contract still going on time to quit now so if you knew the reasons why they are likely to churn then you could like better select like when you write some email or maybe some other campaign or when maybe not to write anything at all yeah right now um Christoph you have a background in stats which means you I mean you like Connor as well we take an incredibly dim view of machine learning and you wonder how how is it possible for us to be stabbing in the dark like this but you know you said that we need to be more rigorous and there's no quantification of uncertainty with the current IML methods and I suspect you might be working on some methods behind the the scenes on this but you know when you have models and explanations which are computed from data they are subject to uncertainty and that's just not modeled at all at the moment right so we need to be making some distributional and structural assumptions that we're not making now and you point out that there's this phenomenon of p-hacking which is a huge problem in the natural sciences which hasn't quite made its way to IML methods yet but probably will do yeah so yeah the I think and in statistics we're really good at quantifying uncertainty I mean this also has some darker sides with like the p-hacking and so on but I still would say it's better to have um not only just one number or one explanation but also have the distribution to do of this explanation or this number and to quantify what uncertainties behind computing this number so when you have a linear model then you get some coefficient which you interpret in the end but usually you don't just interpret the coefficient but you look at the confidence intervals but we don't do it at the moment for interpretability so you maybe get the saliency maps but how certain are you about maybe it's a bad example because we don't so much on it but if you have like a feature importance value and you get some result how like what's the range actually like how much variance is behind it if I were to use slightly different data or refit my model again how similar would the number be and I think that's something that will or should come to interpretability as well it's funny how when we come to machine learning it's almost like open season and forgetting everything you know about maths and stats you throw it all out the window okay so excited about these algorithms right like one example is if you take a if you're fitting a model to predict something that was unlikely I don't know maybe it was like a covid test for example and then if you know the prevalence of covid you get it back you kind of know what the false positive rate is going to be and so you notice that you think it's the multiple comparison issue right you know that you're expecting a certain level of false positives when it comes to doing something like feature importance or looking at interpretability from a thousand features and then five come through is really really important you mentioned sometimes we just forget that multiple comparison issue forget the fact that probably these five are going to be completely false positives and probably completely meaningless yeah I agree especially if you have like these high-dimensionality features and for the record I have to say I mean there are already approaches so especially for feature importance because there's like a huge community in random forests for example and they thought a lot about these issues and their tests for this and stuff like it but for the rest of interpretability I think it could gain a lot thinking more about I mean this is very simple stuff like multiple comparisons quantifying uncertainty does the stuff like statisticians think like a long time already about it and I mean if you even if you leave the area of interpretability and look at the benchmarks so even like if you have like accuracy like a table and you see accuracies in it but there's no variance attached to it then it should be like suspicious of it but because if you just retrain your neural network with a different seed you might end up with a different accuracy in the end so and if you want to say a method is better than another method you want to quantify how larger ranges of uncertainty do you I mean there are a lot of things like the choice of data choice of splitting points and training and test data weight initialization and so on so I think a lot of this rigor from statistics could help the machine learning community and and machine learning science to become better yeah but let's let's never forget this uh quite quite well known saying which is there are three kinds of lies lies damned lies and statistics so you know that that's a lot of what's going on right is is you know fundamentally whenever we go measure data and we have a model what we're actually able to extract from that data and and the model is inherently probabilistic it's a probability distribution right at the end of the day and we get into trouble anytime we try to take that probability distribution and project it to numbers i.e. statistics like as soon as we start trying to to generate and it doesn't matter whether it's it's a mean plus a confidence interval or whatever the fact is we're throwing away information the totality of the information sitting there in that weird multimodal you know spread out distribution right and then we we find some way to simplify it and project it down to a set of numbers we've got a problem like that's and if you forget that that's happening if you forget that you're throwing away all this information I think that you know the tendency to do that isn't just simplification it's that oftentimes we have to use these probabilistic things to reach a decision and as soon as we get to the point where look i've got to choose either to go left or right give the loan or not give the loan as soon as we get down to some point where we have to make a concrete decision we're forced you know we're forced to project it right but along the way it's important not to lose sight of the the fact that we're throwing away information fantastic well and by the way you also said something interesting a minute ago Christoph which is about at least in most machine learning algorithms if you change the random seed you know that there's enough stability there that it still gives you roughly the the same model every time but in reinforcement learning if you change the random seed the entire thing is completely broken but but yeah what Keith was saying about this this this information and structure in models I think that's really interesting because people have said with reinforcement learning you can actually learn causal factors right but that's not really true you're interacting with the system but what you're learning is is a surface representation of causal factors so you might learn that there's a causal factor between like a hose putting out fire but it wouldn't actually learn that it was the water that put out you know that there was a causal relationship between the water and the fire and this is the case with so many of our models as we were saying earlier that there's there's just a a surface representation which doesn't actually represent the reality of our world at all but this brings me on to the next point because you have a real problem with causal interpretations of some of these iml metrics right and you you say that models well the the goal of models is that they should reflect the causal structure right this is what we want to do in science but most statistical learning just reflects these surface feature correlations they they don't even scratch the surface of what we want so what are we going to do right are you doing some work in this field to help us out here and and why are people making these fallacious interpretations yeah so so i'm not not working on anything causality related at the moment yeah but about about causality i mean kind of like um i i studied statistics bachelor and master and zero i mean the only time we were talked about causality was when i heard the sentence correlation does not imply causality and it was really about it so um i think it's like really um yeah should be taught a lot more like how to think about causality like just super simple things like you should include confounders or um like what types of features if you include them in the model um like destroy your causal interpretation of another features these these are not super difficult things so um then you don't have to like learn any like difficult frameworks to work with or read like causality books on it that's like super simple yeah rules of thumb for your features even um yeah and i think you also have to decide or distinguish between like what's the goal of your model do you want a causal interpretation or do you want to like because in a sense and you have to also distinguish between two levels you have the real world level and the model level i mean once you use features for the model they are causal for the model prediction of course because you designed it that way and the question is when you are you allowed to go to the real world level where you say okay this um the feature importance that i see here or also the feature dependence plot that i see is also causal um or and may interpret it as a cause and or as a causal effect also for for the real world and i think that also depends like if you need this interpretation if you do scientific modeling for example then you probably want it um but that there can also be good reasons to include non-causal features into your model if your goal is really just prediction and and some feature might help you with the with a good prediction um but it might not be causal at all yes but the problem is when we're using these deep learning models they they will learn a structure which probably has no relationship to the real world whatsoever but um i think causal um factors do generalize much better there's the example of um i don't know with car crashes right male testosterone levels is a causal factor so that will probably generalize to other locations where you didn't train your data on but unfortunately models don't really do that well but so just real quickly on that tim like the only reason that we know testosterone is a causal factor is is not from that data set it's from a bunch of mechanistic you know scientific research and biology and and elsewhere um so you know i i'm kind of wondering how it would be nice if at least machine learning methods could indicate that there may be the possibility of a causal structure so just looking for underlying hidden structures um that that you know they're more generalizable that could explain large pieces of the of the data and give kind of a list of hey there might be a causal factor here like go investigate it but on on that there's a difference between a causal factor and a causal structure i think that the challenge is that we don't have enough fidelity in the structure that benjo by the way is doing some interesting work on this using data driven approaches to um you know learn causal factors but it's the structure of the factor graph which i think is the important thing i mean this is one of the most interesting parts i think of machine learning actually trying to learn causation from a data set which you can do right things like beta networks where you specify all your variables you connect these nodes with edges and you can try to learn the optimal structure like the simplest structure and that sometimes turns out to be the real life causal effect if you do it well but the difference is you as a human you you know the causal structure and you've you've created that that graph so it's not learned you've created it you can learn these things from data right you can actually you can search over the set of all possible graphs all the possible edges and you have a bit of a loss function you try to find a graph that fits the data well so it's got enough edges but it's not too complex you're not relating everything to everything and so just from data without any human input with structure learning you can sometimes get a model that kind of out of nothing will give you the cause of relationships sometimes there is redundancy right because a graph that says a implied near causes b equals a c that's identical to c causes b causes a right but even then with structure learning you you've got this adjacency matrix and all of those nodes you've already come up with a priori so what you want to learn is what the nodes are themselves right yeah i think like what kona mentioned there's lots of moda setting where you have well defined features and i think what tim referred to was more like the you don't even know what the features are like if you have a convolutional neural network and and like what's an object um what is a feature that like is disentangled also from other objects um so i think also there is the big issue that you have this entanglement between concepts that i don't know that the frisbee is always with uh on the same image as a as a dog so um maybe the the neural network can't even separate these two things then because they are too entangled in the data set to even discover the structure that that is really underlying the the real world in this case that's a fascinating point actually because one of the reasons why there's no easy solution to adversarial examples is because you you learn these these non robust features and you might just think to yourself well um fur is a low magnitude feature it's really easy just to kind of create fur on anything and for the neural network into thinking it's a cat and you just say well this is obvious right you just create some rules to say well if it's if it's not an animal and fur then ignore the fur but actually the features are entangled in this complex neural network so you can't do that but i wanted to move the discussion on a bit so you said that there are some really interesting challenges ahead in in iml and what's fascinating is you start talking about the process so you say that the setting of machine learning is too static it doesn't reflect how these models are used in reality and models are embedded in a process or a product or even complex people interactions and i love this right because i talk about ml devops and machine learning models in isolation are irrelevant it's the people in the process that's where the complexity is even with intelligence itself it's a process right you know intelligence is the interaction between a brain a body and an environment and you know within the context of this process you know we've got all of this rich information that we could be bringing in from other disciplines and you're saying we should bring in compsci and stats folks and we should be bringing in psychologists and social scientists and we need to also have interpretability at a higher level at the institutional level right or at the society level so when you kind of broaden the discussion out a little bit i think it adds a nice bit of flavor yeah so it's very especially as a scientist it's so convenient to just have this fixed model a fixed data set and then you just geek out and invent all these methods and so on but reality is that that you use the method some place and then it interacts with the institution it's built with the developers it's built by with the people it affects and my favorite example there is when you have this closed loop where your model makes predictions and these predictions generate the next generation's data so for the next generation of the model it produces the data so there's this example of the rent index where you have this model that tells you how much rent you should like pay for a certain kind of apartment and so on and this is actually like um legally binding so if you're a land lord you have to accept kind of the range that is outputted by the model which also means that the data that is produced so the new flats that are rented out in new apartments they all have to fit the model kind of and then but then you use this data again to train your model so you have this very weird feedback loop and I think it's also difficult to wrap your head around it and understand implications of it that that same thing a very similar feedback loop was a fear in the you know in our algo shambles video about the uk testing since they couldn't conduct the the uh what was it the a-level test right Tim they they built some some models around that and so it would do things like well you know if this school historically never had anyone in this grade bucket then we're not going to assign anyone to that grade bucket in that school and so it's sort of this self-perpetuating you know feedback loop we were reading through a lot of your work and you I mean I'm just going to hit this point head on um you don't really talk that much about AI ethics and you know there's the f word which is the fairness word and and I don't I don't recall you ever using that word and is that something that you've deliberately shied away from um yeah I just like um define it as outside of the scope like to talk to to I don't know talk about ethics so um fairness metrics or so on because I think there's a really big field on its own and I just don't know as much like about all these things so I know a little bit like about the fairness metrics that they're out there um I also think I mean they're kind of like research wise a little bit overlapping but more or less separate fields I think interpretability and fairness um but of course they have some commonalities that I mean when you kind of to for fairness you have to look it's not necessarily inside the model but you have to study how the model behaves um and that's kind of the connection to interpretability I would say yeah well where yeah where I see the connection is work like yours is helping to build the tool set that will allow people to apply human you know intuition and and ethics and evaluations to machine learning because at the end of the day a lot of these are human moral judgments or ethical judgments and it's important that people be happy with them because you know we have to have the population as a whole understand and accept and be able to move forward with the increasing role that machine learning is having in our lives and building that tool set is necessary so it's like you said very early in this talk you know what do we do just stick our heads in the sand and ignore it and just accept machine learning models are going to do whatever they do as long as they fly the plane or you know don't kill too many people we're okay like I don't think that's going to work like we have to build the tool set that you're talking about and continue this process of exploring how to better explain and interpret ML models so that human beings can have that oversight because it's the only thing that's going to give us comfort really as a society I suppose the reason I segue to this is we were just talking about the process and you you mentioned some of these feedback loops because we can have a very superficial discussion and you could say well we need to be able to represent reality better than we do and we have a whole tool set here to identify sources of bias or you know lack of robustness etc in models but it's so much more complex than that because these models are used in a very complex process and you get these very very complex dynamics emerging as a result of that and I think we're only really just scratching the surface of understanding those dynamics yeah I think so too as I said I think in science it's always very easy to to study things in isolation like study one type of model study one type of adjustment for a deep neural network and hopefully we will see more work emerge on this I think I've never read the paper like I mean of course discussed implications but really like analyze like what happens in terms of the data and the model when we have like multiple generations for example of a model and how it changes over time but this thing I mean to study those things also means that you have to wait for a long time until you have these dynamics and I think in many cases it's just starting that we use these models more extensively in our daily life I have a question is is anyone because look interpretability metrics whatever they are say saliency maps and you know could be partial dependency plots whatever you could actually build in some requirements of those into the objective functions when you go to train models so for example I'm just going to come up with a crazy idea I have no idea if this is relevant at all but somebody could say look I want all my saliency maps to be you know sets of of a bezier curves or something like that like they have to have a certain smoothness property and you could actually put that as a constraint in the objective function has anybody tried anything like that yeah there are approaches so I saw one paper they added some some parts to their objective function so that when you create line explanations with line that they were most more stable and there are a lot of things like for neural networks you have disentanglement that you try that the feature maps or that the nodes learn disentangled concepts there are ways to introduce like monotonicity so that a feature can always go into the effect of a feature can always be in one direction not to like zigzag around so there are approaches to do this to like have okay like interpretability constraints in your modeling here yeah because I was just thinking this can go back to Connor you know Connor was saying earlier on why don't we just create white box models maybe we can use if the definition to a human being of white box is that it's interpretable and understandable if we can build into the objective functions when we're actually training the network that it has these properties then we'll actually be helping to create more white box you know models even if they are complex that's definitely an option but I think the issue remains the same that you with it's similar to a white box model I mean you'll make some trade offs in the end you have to make the judgment whether so when you put more constraints I mean you can actually also help the model of course that if you I mean if you have some inductive biases also which which you infuse into the model which help with predicting or be more more stable but sometimes you might maybe also trade off with accuracy and you just have to like in the end you have this yeah this set of models where some are more accurate some better than this one interpretability dimension the other is cheaper to deploy and then you have this so this kind of going into direction of like automatic machine learning and you don't get like just the best performing one but you have this parater set like well so we have multiple objectives that you want to hit and then there's not one model that works best but you have a set of models that that have different trade-offs between these objectives and then you have to decide what is the trade-off that you want to do that you want to have I guess there's no getting away from it is the interpretability it is going to get more important and more important I think you mentioned Christoph that you know we've had linear models for hundreds of years and then there's been this big explosion and deep learning and then would you say about 2016 to 2018 that's when interpretability is really kicked off I what do you think do you think where's it going are we just going to get more and more attention paid to this area I don't know if you can get more than than this I don't know yeah but I think it's at least here to stay and I think it's important I mean it has been important before but of course with like the push from deep learning especially and that it just became more clear to a lot of people that we need interpretability in some sense at least yeah of course people have attempted it before and worked on it before it's just more urgent now I really like the bit in your book talking about what's changed recently how interpretability is coming together as a field you know with this a unification so you know in physics we love a big unification when you take all these different things in the past and say oh they're all just part of this one big framework and it was chap that chap paper was amazing wasn't it saying things like lime deep lift layer-wise propagation shapely ah forget all them they're all special cases of these additive feature attribution methods and we can prove that this is the only one that's theoretically valid because it has these properties of symmetry it's got the stummy property so you know everything that's been done before in interpretability well they're all in our framework now and shapely values are the way forward yeah they're quite uh quite famous to shapely values yeah would you believe them then I mean I guess in their paper they're kind of they kind of disagree with lime a bit don't they they say well lime is a count of this but they're going to be breaking our properties of efficiency and symmetry so lime is using the wrong weights right they should be using this yeah kernel shape weights rather than the line weights I think that's just a different approach also to think about it I mean you don't maybe you don't I think I think the properties are quite attractive or meaningful at least um but also also the line approach I'm very critical about lime because it um I think it's difficult to have the correct to know like how to parameterize your your local models um so I think I'm a bit more of a fan of shapely values because of the theoretical properties it comes with are you talking about that distance measure in lime where you have to be able to quantify how far away is the permutation yeah the like the kernel width yeah which is set to 0.75 I think so I just looked it up and I mean it's it's a very difficult question it goes to the heart of like what's local um because like I mean you have this this kernel that decides like how much you weight all the data points around the point you want to explain and and like how how big is this area I think this is very dependent on your model and your data and there's no answer no easy answer to how to set it yeah or even more generally than that this this whole notion of what does it mean to have a local interpretation method in you know in text or vision so in in vision there's this super pixel concept which is something that seems to make intuitive sense but but does it you know when you create all of these different uh maskings of different parts of the input space but um with shapely values as well that they they are a beautiful a beautiful technique especially because the the values are are quite meaningful but if you have shared information between the features I mean Connor and I were talking about this for example you if you had the same model where you were predicting someone's income and you put their I don't know let's say you had salary in the model twice then the shapely value would be divided between the two duplicate fields right so there just seems to be so much esoterica in these IML methods right are we expected to know all of this stuff yeah I I think I mean that's why I wrote the book um to to capture these things that you have to know all the these these uh disadvantages of the methods where I try to be very honest I mean because I'm not too invested in them but yeah I think that's worth all tools that we usually have um also with statistics and so on you you have to know like um these like as you mentioned this if you have salary twice then it will just I mean depends also on what your model does if it just picks one of the salary features or if it itself uses both so this also something that will define how the shapely value will look like later on um but you have to know these things if you want to use shapely values and interpret interpret them correctly yeah because I think philosophically we've got we've got the real behavior and then we use these interpretability methods and then we've got the kind of perceived behavior so we've got these these levels of modeling or or do you know what I mean simplification and it's all well and true if you are dealing with data scientists who understand how these you know methods work that's fine but invariably data scientists need to present this information to lay people and they are not going to understand all of the various different trade-offs and how information is being compressed and and lost and so on so do you see that as a as a serious problem uh yes but it's not a new problem it's with any number that you read in any newspaper uh I mean so in a sense I mean when when you look at uh outcomes of statistical models that uh well everyone can understand well of course not because you need training to understand how to interpret a linear model or any regression model yeah there's difficulty but I don't think it's new in any sense um because there's always I mean any number that you read anywhere has a very complex process um so I don't know if you have like COVID testing numbers it's very complex like how the number was generated maybe like how it was aggregated over many states and like what cases it includes and which it doesn't and so the number looks very innocent and simple but there's a very long process behind it to produce it um maybe this process is a bit uh a bit more black box or a bit more difficult if it comes out if there's some machine learning in between machine learning model in between to generate a number um but yeah I think this problem is well old well let me let me challenge a little bit here on something which is okay if I have if I have some general formula just some very general formula and then I go in there and I go you know what this formula has five parameters and if I make this one point seven five and that one one third and this one two and that one zero and I call this the megatron you know uh activation potential and I go and write a paper about it that's really just an arbitrary you know kind of selection of a bunch of numbers and then you gave it a fancy mathematical passport and you got it published in some journal and now everybody has to memorize that as you know the megatron potential and kind of learn about it and that's a lot of what's going on right now is that it's really just a bunch of hacking like it's people just they don't really know a general solution and they don't know how to solve like in general the problem they're trying to solve and so they just hack around and then the ones that are kind of famous or demonstrate some success in a particular combination you know competition over in this corner or something it now becomes something that's part of the lexicon that we all have to learn and I think like I look back on this like imagine what physics was like before Leibniz and Newton you know invented calculus it's like everybody memorizing a whole bunch of little purpose built kind of formulas and then along comes a general framework which now we can just learn calculus and derive the special circumstances as needed. You're onto something really interesting there which is that with IML methods we are we are kind of compressing information down into a representation you know and then that that is a transport that can be understood by different people but there's a trade-off right because as you said you can learn calculus and that's a compact framework for doing lots of stuff but it's all about the amount of common knowledge that is required so it's possible to compress something down just to one symbol and that symbol could represent all of that knowledge but it doesn't help you because I still need to learn all of that knowledge. Yeah but so to me calculus was a very beautiful and simple framework that I could learn and then once I learned that simple thing I could go and solve all kinds of problems with it that before I would have to memorize specific solutions or like the quadratic formula for example is a student I didn't actually memorize the quadratic formula I just learned how to complete the square and then I would just do complete the square and if somebody asked me what the quadratic formula was I would just quickly derive it right because it was easier to memorize the rule and then apply the rule to any situation rather than to memorize all these little one-off you know kinds of hacks that we come up with. You're not normal Keith right so most people won't be able to go and understand this because I think it's well no these IML methods are brilliant for data scientists who can it's a framework right it's a reference of understanding so assuming that people can understand how Shapley values work then this is a beautiful representation to reason about the behavior of models. Sure but when I first saw Shapley values I realized immediately there's a connection in you know Bayesian analysis to marginalization you know all we're really doing here is computing the expected marginal you know contribution to this value it's not a probability but it's still the same procedure being done right and I think I'm going to throw myself in with the lay people to a degree because the reason I'm always striving for simplifications is because I don't have the capacity to memorize all these little arbitrary kinds of hacks and but I yet I could totally understand Bayesian analysis and like I said you know previously in some other videos statistics made no sense to me until I learned the Bayesian framework because that was based on very simple rules that I could then reapply as needed. I think that what you refer to Keith maybe the worst situation is with the saliency maps because you have so many methods and they all like back propagate the gradient and to do input pixels and to um now do you have to like learn like how dozens of these framework works or like how to interpret do interpretation with all of these and they're all kind of variants of each other so mostly because they just there's some ambiguity how you how you back propagate the gradient because because of the non-linear units and stuff and a little bit differences how you can define this and so you have this huge like a sea of many different methods I think it would be nice therefore as you said to have some like simplification where you say okay this is like all these methods work under this one principle basically and we have these two parameters and that's how they differ I think that's also that some I think I wrote something in a chapter that the police stop inventing new methods for saliency maps so I think it's enough and we should focus more on like doing this consolidation to like understand the limitations of the methods and consolidate them to see like what's the commonalities in which ways do they differ and so on that's probably actually my first impression actually when I first opened the interpretable ML book I was amazed how many different things there are I've you know heard people say ah you can't use machine learning it's just a black box so many times it'd almost been drilled into my head then seeing all the things you know from white box models ways of training salient models counterfactual explanations it's what a wonderful recipe right there are so many different things that you can do I feel like now I trust ML models more than other kinds of things because I have this amazing toolbox of ways to understand them the thing that strikes me though is most of these methods as we were just saying they require interpretation by a human and a human who understands how the method works I love this concept of turning machine learning into an engineering discipline and being able to do a lot of these tests non-interactively and I think Marco Rubirio has done a lot of work around the counterfactual examples and the data grouping and what excites me about these methods is they seem like methods that we could actually run as part of an automated process we still have to set thresholds maybe we could set a threshold that said if this if this counterfactual example flips the switch on more than one percent of examples then fail the build that seems reasonable but a saliency method I mean how the hell do you say well if there's lots of red pixels over here then break the build I mean it's just ridiculous yeah yeah so I've seen interesting approaches to like using interpretability also more automatically like when you do model monitoring you can do things like create interpretations and see if they significantly change over time for example so then have thresholds that warn you that hey something's going on with your model so I think that's also interesting approaches there yeah you know Tim to your point of making this an engineering field and even making interpretability and and understandability an engineering field I mean I think that maybe that's why I like your book so much Christoph as I think it's it's a step towards that direction it's like let's survey everything and more importantly let's create a finite and hopefully smallish set of simple concepts that we can all agree on and understand that we can use to catalog you know what's out there so please keep up the good work you know I'm interested to see where this goes so final question for you Christoph then I wonder what's what's next for interpretability like are we going to the point where it's going to be almost a box ticking exercise where we can say yes our process we've done the standard interpretability step I mean is it is computing power going to change it I remember when Shaq the library came out it made that approach possible whereas previously you know it was very hard very computation infusible my friend Angem who's a wonderful data scientist sent me n-video rapids they've got that running on GPUs way faster than of course before is it just going to become a standard step do you think or is it going to be something where you need decent subject matter expertise and some real thought to do to really understand how a model works so well predictions about the future are always hard so maybe more like what I wish or what yeah maybe think we'll have code to happen um so I mean what we're seeing already is like a lot of implementations of these methods so they're kind of getting a commonality a rook one can use it very easily there's a lot of libraries out there in python r but also in like this machine learning cloud tools they also have a lot of interpretation methods available now so in that sense I think and it's maturing a lot I still believe that we need some expertise to understand them or at least some good references and there will also be hopefully more than my book maybe have some documentation when for these tools and people answering on stack overflow questions and whatnot so I think um yeah it's it's getting we're getting that everyone can use it easily I think it should never be a box ticking exercise it's a similar thing when if you have an AI ethics you know governance process or something the last thing you want is for it just to be an automatic response so I've just you know yeah I've thought about AI ethics um it needs to be something that that we really engage with I think we need to abstract away a lot of the complexity at the moment I think it's possible to come up with an interface to standardize the way that we do interpretability and we can reduce down what we have now to certain primitives which means that it can plug into an engineering process and it also means that we can abstract away some of the complexity I think that's possible yeah I also would agree that it shouldn't be like just box ticking but you can like for the initial um when you start interpreting a model that you just have like with a click you have a report and then it shows you the most basic things but then you still like should ask the like the question like does it really make sense that this feature is the most important one or what's happening there with these weird interactions between the two features let's dig a bit deeper here and see what's going on so I think um there's this one portion that is just like this automated reporting thing um but this should then be like the starting point for more critical uh questioning of the model and and and checking what's going on um for some specific problems maybe so it's going to be you click on the molnar report and it gives you the the report from the book right yeah there will be convenience well um christoph molnar thank you very much for joining us today it's an absolute honor to have you on the show thanks for having me hey folks this is tim in post script there's just a couple of thoughts that didn't come to my mind during the interview that I think I'd like to quickly cover now the first thing is on the lack of fairness the reason why I raised that is most folks who talk about AI ethics and fairness they use the toolkit of interpretability methods quite often you know to apply their trade there are tools out there to mitigate fairness and to detect fairness microsoft's fair learn is a great example of this what we really need is an operating model or a set of guidelines on how to implement these tools how do I identify sources of problematic correlations we need to have a database of problematic correlations having a tool that allows me to identify and mitigate bias frankly is useless what do I do with that as we mentioned on the show many of the machine learning cloud providers whether it's data iq or azure ml and sage maker they all have these interpretability methods built in now including saliency maps and it's just a box ticking exercise frankly it's completely useless there is no accepted guidance on how these tools should be used right so if I'm a large company and I'm building an operating model around how to implement fairness techniques just having the technology is irrelevant it's about the people and the process and the the kind of operating model of how we implement it and there is basically no useful information out there to help us do that the other thing is we spoke about this becoming an engineering discipline which is to say what if we could create an interface to abstract away some of the vagaries and esoteric of interpretability methods we might come up with some primitives or some common language and then we can hide the complexity behind the interface this is kind of what we do with mo dev ops already we automate as much as we can then we templatize and remove friction out of the process we even create building blocks using domain specific languages or yaml files and pipelines and and so on so what we do is is we create a level of abstraction where people can compose together pipelines remember when Conor made the comment that this might just become a box ticking exercise and this is something we see in security and AI ethics already we can't really trust people to self report that the model is behaving correctly or that the project has no concerns from an AI ethics point of view the whole point here is process if we want to create an operating model and ensure best practices are followed or any kind of standardization in a large organization we have to design a process and many eyes make shallow holes so the process would mandate that a certain number of stakeholders were involved in assessing the particular iml technique and validating it essentially and then we would need to record that assessment so who said what when and then if the company ever became audited or if god forbid there was some kind of a problem where the iml model did something wrong and it caused the company lots of damage or it harmed the environment or society or something like that we would then be able to rewind the clock and say okay well joe blogs said it was okay because of xyz so that is an operating model it's a process and how to design such a process again is completely absent speaking as a chief data scientist myself that's the kind of thing that i'm interested in and it's very difficult for me to do that i really hope you've enjoyed the episode today we've had so much fun making it remember to like comment and subscribe and we'll see you back next week", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 5.28, "text": " Coming up later in today's presentation, I'm wondering at what point we're just developing", "tokens": [50364, 12473, 493, 1780, 294, 965, 311, 5860, 11, 286, 478, 6359, 412, 437, 935, 321, 434, 445, 6416, 50628], "temperature": 0.0, "avg_logprob": -0.12175863356817336, "compression_ratio": 1.8097165991902835, "no_speech_prob": 0.2237691581249237}, {"id": 1, "seek": 0, "start": 6.16, "end": 11.040000000000001, "text": " complex math models to explain complex math models and we really haven't, you know,", "tokens": [50672, 3997, 5221, 5245, 281, 2903, 3997, 5221, 5245, 293, 321, 534, 2378, 380, 11, 291, 458, 11, 50916], "temperature": 0.0, "avg_logprob": -0.12175863356817336, "compression_ratio": 1.8097165991902835, "no_speech_prob": 0.2237691581249237}, {"id": 2, "seek": 0, "start": 11.040000000000001, "end": 16.72, "text": " made much progress along the interpretability axis. So you have something you don't understand", "tokens": [50916, 1027, 709, 4205, 2051, 264, 7302, 2310, 10298, 13, 407, 291, 362, 746, 291, 500, 380, 1223, 51200], "temperature": 0.0, "avg_logprob": -0.12175863356817336, "compression_ratio": 1.8097165991902835, "no_speech_prob": 0.2237691581249237}, {"id": 3, "seek": 0, "start": 16.72, "end": 21.76, "text": " and you explain it with something you don't understand? If I have if I have some general", "tokens": [51200, 293, 291, 2903, 309, 365, 746, 291, 500, 380, 1223, 30, 759, 286, 362, 498, 286, 362, 512, 2674, 51452], "temperature": 0.0, "avg_logprob": -0.12175863356817336, "compression_ratio": 1.8097165991902835, "no_speech_prob": 0.2237691581249237}, {"id": 4, "seek": 0, "start": 21.76, "end": 26.32, "text": " formula, just some very general formula, and then I go in there and I go, you know what,", "tokens": [51452, 8513, 11, 445, 512, 588, 2674, 8513, 11, 293, 550, 286, 352, 294, 456, 293, 286, 352, 11, 291, 458, 437, 11, 51680], "temperature": 0.0, "avg_logprob": -0.12175863356817336, "compression_ratio": 1.8097165991902835, "no_speech_prob": 0.2237691581249237}, {"id": 5, "seek": 2632, "start": 26.32, "end": 34.24, "text": " this formula has five parameters. And if I make this 1.75 and that one one-third and this one two", "tokens": [50364, 341, 8513, 575, 1732, 9834, 13, 400, 498, 286, 652, 341, 502, 13, 11901, 293, 300, 472, 472, 12, 25095, 293, 341, 472, 732, 50760], "temperature": 0.0, "avg_logprob": -0.12400610903476147, "compression_ratio": 1.596638655462185, "no_speech_prob": 0.0069032274186611176}, {"id": 6, "seek": 2632, "start": 34.24, "end": 41.52, "text": " and that one zero, and I call this the Megatron activation potential, and I go and write a paper", "tokens": [50760, 293, 300, 472, 4018, 11, 293, 286, 818, 341, 264, 9986, 267, 2044, 24433, 3995, 11, 293, 286, 352, 293, 2464, 257, 3035, 51124], "temperature": 0.0, "avg_logprob": -0.12400610903476147, "compression_ratio": 1.596638655462185, "no_speech_prob": 0.0069032274186611176}, {"id": 7, "seek": 2632, "start": 41.52, "end": 47.04, "text": " about it, that's really just an arbitrary selection of a bunch of numbers. And then you gave it a", "tokens": [51124, 466, 309, 11, 300, 311, 534, 445, 364, 23211, 9450, 295, 257, 3840, 295, 3547, 13, 400, 550, 291, 2729, 309, 257, 51400], "temperature": 0.0, "avg_logprob": -0.12400610903476147, "compression_ratio": 1.596638655462185, "no_speech_prob": 0.0069032274186611176}, {"id": 8, "seek": 2632, "start": 47.04, "end": 52.16, "text": " fancy mathematical passport and you got it published in some journal. And now everybody", "tokens": [51400, 10247, 18894, 24694, 293, 291, 658, 309, 6572, 294, 512, 6708, 13, 400, 586, 2201, 51656], "temperature": 0.0, "avg_logprob": -0.12400610903476147, "compression_ratio": 1.596638655462185, "no_speech_prob": 0.0069032274186611176}, {"id": 9, "seek": 5216, "start": 52.16, "end": 57.44, "text": " has to memorize that as you know, the Megatron potential and kind of learn about it. And that's", "tokens": [50364, 575, 281, 27478, 300, 382, 291, 458, 11, 264, 9986, 267, 2044, 3995, 293, 733, 295, 1466, 466, 309, 13, 400, 300, 311, 50628], "temperature": 0.0, "avg_logprob": -0.11506725365007428, "compression_ratio": 1.4705882352941178, "no_speech_prob": 0.005384343210607767}, {"id": 10, "seek": 5216, "start": 57.44, "end": 68.08, "text": " a lot of what's going on right now is that it's really just a bunch of hacking.", "tokens": [50628, 257, 688, 295, 437, 311, 516, 322, 558, 586, 307, 300, 309, 311, 534, 445, 257, 3840, 295, 31422, 13, 51160], "temperature": 0.0, "avg_logprob": -0.11506725365007428, "compression_ratio": 1.4705882352941178, "no_speech_prob": 0.005384343210607767}, {"id": 11, "seek": 5216, "start": 74.96, "end": 79.75999999999999, "text": " Welcome back to Street Talk. Today, we're going to be talking about interpretable machine learning.", "tokens": [51504, 4027, 646, 281, 7638, 8780, 13, 2692, 11, 321, 434, 516, 281, 312, 1417, 466, 7302, 712, 3479, 2539, 13, 51744], "temperature": 0.0, "avg_logprob": -0.11506725365007428, "compression_ratio": 1.4705882352941178, "no_speech_prob": 0.005384343210607767}, {"id": 12, "seek": 7976, "start": 79.76, "end": 87.12, "text": " Enjoy. Interpretability has become one of the most important topics in machine learning.", "tokens": [50364, 15411, 13, 5751, 6629, 2310, 575, 1813, 472, 295, 264, 881, 1021, 8378, 294, 3479, 2539, 13, 50732], "temperature": 0.0, "avg_logprob": -0.10436195418948219, "compression_ratio": 1.5333333333333334, "no_speech_prob": 0.0020506277214735746}, {"id": 13, "seek": 7976, "start": 87.12, "end": 92.56, "text": " And it's something that every data scientist needs to be familiar with. For hundreds of years,", "tokens": [50732, 400, 309, 311, 746, 300, 633, 1412, 12662, 2203, 281, 312, 4963, 365, 13, 1171, 6779, 295, 924, 11, 51004], "temperature": 0.0, "avg_logprob": -0.10436195418948219, "compression_ratio": 1.5333333333333334, "no_speech_prob": 0.0020506277214735746}, {"id": 14, "seek": 7976, "start": 92.56, "end": 99.28, "text": " we've had simple interpretable models like linear regression and rules-based systems.", "tokens": [51004, 321, 600, 632, 2199, 7302, 712, 5245, 411, 8213, 24590, 293, 4474, 12, 6032, 3652, 13, 51340], "temperature": 0.0, "avg_logprob": -0.10436195418948219, "compression_ratio": 1.5333333333333334, "no_speech_prob": 0.0020506277214735746}, {"id": 15, "seek": 7976, "start": 100.32000000000001, "end": 107.36000000000001, "text": " But in recent years, there's obviously been a huge rise in more complex, bigger, nonlinear models.", "tokens": [51392, 583, 294, 5162, 924, 11, 456, 311, 2745, 668, 257, 2603, 6272, 294, 544, 3997, 11, 3801, 11, 2107, 28263, 5245, 13, 51744], "temperature": 0.0, "avg_logprob": -0.10436195418948219, "compression_ratio": 1.5333333333333334, "no_speech_prob": 0.0020506277214735746}, {"id": 16, "seek": 10736, "start": 108.32, "end": 115.44, "text": " And of course, predictions from these models are not always so easy to explain. So as we start to use", "tokens": [50412, 400, 295, 1164, 11, 21264, 490, 613, 5245, 366, 406, 1009, 370, 1858, 281, 2903, 13, 407, 382, 321, 722, 281, 764, 50768], "temperature": 0.0, "avg_logprob": -0.07576347259153803, "compression_ratio": 1.5933609958506223, "no_speech_prob": 0.0023229324724525213}, {"id": 17, "seek": 10736, "start": 115.44, "end": 123.36, "text": " these more powerful, nonlinear models to actually make decisions on real world matters, then it's", "tokens": [50768, 613, 544, 4005, 11, 2107, 28263, 5245, 281, 767, 652, 5327, 322, 957, 1002, 7001, 11, 550, 309, 311, 51164], "temperature": 0.0, "avg_logprob": -0.07576347259153803, "compression_ratio": 1.5933609958506223, "no_speech_prob": 0.0023229324724525213}, {"id": 18, "seek": 10736, "start": 123.36, "end": 129.52, "text": " inevitable that our attention must now turn to interpretability and explainability. When I", "tokens": [51164, 21451, 300, 527, 3202, 1633, 586, 1261, 281, 7302, 2310, 293, 2903, 2310, 13, 1133, 286, 51472], "temperature": 0.0, "avg_logprob": -0.07576347259153803, "compression_ratio": 1.5933609958506223, "no_speech_prob": 0.0023229324724525213}, {"id": 19, "seek": 10736, "start": 129.52, "end": 135.04, "text": " first started learning about machine learning algorithms, I was told they could be dangerous.", "tokens": [51472, 700, 1409, 2539, 466, 3479, 2539, 14642, 11, 286, 390, 1907, 436, 727, 312, 5795, 13, 51748], "temperature": 0.0, "avg_logprob": -0.07576347259153803, "compression_ratio": 1.5933609958506223, "no_speech_prob": 0.0023229324724525213}, {"id": 20, "seek": 13504, "start": 135.12, "end": 142.79999999999998, "text": " They were hard to understand. They were black boxes. But as Christoph lays out, it turns out", "tokens": [50368, 814, 645, 1152, 281, 1223, 13, 814, 645, 2211, 9002, 13, 583, 382, 2040, 5317, 32714, 484, 11, 309, 4523, 484, 50752], "temperature": 0.0, "avg_logprob": -0.07933121807170364, "compression_ratio": 1.7028985507246377, "no_speech_prob": 0.0014549258630722761}, {"id": 21, "seek": 13504, "start": 142.79999999999998, "end": 147.44, "text": " there is a whole plethora of techniques out there to explain why a model made a certain", "tokens": [50752, 456, 307, 257, 1379, 499, 302, 7013, 295, 7512, 484, 456, 281, 2903, 983, 257, 2316, 1027, 257, 1629, 50984], "temperature": 0.0, "avg_logprob": -0.07933121807170364, "compression_ratio": 1.7028985507246377, "no_speech_prob": 0.0014549258630722761}, {"id": 22, "seek": 13504, "start": 147.44, "end": 153.12, "text": " prediction. Some models like low-dimensional linear regression are intrinsically interpretable.", "tokens": [50984, 17630, 13, 2188, 5245, 411, 2295, 12, 18759, 8213, 24590, 366, 28621, 984, 7302, 712, 13, 51268], "temperature": 0.0, "avg_logprob": -0.07933121807170364, "compression_ratio": 1.7028985507246377, "no_speech_prob": 0.0014549258630722761}, {"id": 23, "seek": 13504, "start": 153.92, "end": 159.12, "text": " You can just look at the model coefficients and that tells you exactly how the model is working", "tokens": [51308, 509, 393, 445, 574, 412, 264, 2316, 31994, 293, 300, 5112, 291, 2293, 577, 264, 2316, 307, 1364, 51568], "temperature": 0.0, "avg_logprob": -0.07933121807170364, "compression_ratio": 1.7028985507246377, "no_speech_prob": 0.0014549258630722761}, {"id": 24, "seek": 13504, "start": 159.12, "end": 164.88, "text": " under the hood. Then there is a whole suite of methods that will actually work with any ML model.", "tokens": [51568, 833, 264, 13376, 13, 1396, 456, 307, 257, 1379, 14205, 295, 7150, 300, 486, 767, 589, 365, 604, 21601, 2316, 13, 51856], "temperature": 0.0, "avg_logprob": -0.07933121807170364, "compression_ratio": 1.7028985507246377, "no_speech_prob": 0.0014549258630722761}, {"id": 25, "seek": 16504, "start": 165.28, "end": 170.64, "text": " Like training a local surrogate or a global surrogate. There's also Shapley values,", "tokens": [50376, 1743, 3097, 257, 2654, 1022, 6675, 473, 420, 257, 4338, 1022, 6675, 473, 13, 821, 311, 611, 44160, 3420, 4190, 11, 50644], "temperature": 0.0, "avg_logprob": -0.06815759995404412, "compression_ratio": 1.5532786885245902, "no_speech_prob": 0.001284158555790782}, {"id": 26, "seek": 16504, "start": 170.64, "end": 177.44, "text": " which is a really cool technique that allows you to distribute blame for the prediction amongst", "tokens": [50644, 597, 307, 257, 534, 1627, 6532, 300, 4045, 291, 281, 20594, 10127, 337, 264, 17630, 12918, 50984], "temperature": 0.0, "avg_logprob": -0.06815759995404412, "compression_ratio": 1.5532786885245902, "no_speech_prob": 0.001284158555790782}, {"id": 27, "seek": 16504, "start": 177.44, "end": 183.84, "text": " the input features in a really theoretically sound, really principled way. And then there are domain", "tokens": [50984, 264, 4846, 4122, 294, 257, 534, 29400, 1626, 11, 534, 3681, 15551, 636, 13, 400, 550, 456, 366, 9274, 51304], "temperature": 0.0, "avg_logprob": -0.06815759995404412, "compression_ratio": 1.5532786885245902, "no_speech_prob": 0.001284158555790782}, {"id": 28, "seek": 16504, "start": 183.84, "end": 189.84, "text": " specific methods. For example, to explain image models, you can try to highlight the most relevant", "tokens": [51304, 2685, 7150, 13, 1171, 1365, 11, 281, 2903, 3256, 5245, 11, 291, 393, 853, 281, 5078, 264, 881, 7340, 51604], "temperature": 0.0, "avg_logprob": -0.06815759995404412, "compression_ratio": 1.5532786885245902, "no_speech_prob": 0.001284158555790782}, {"id": 29, "seek": 18984, "start": 189.84, "end": 196.64000000000001, "text": " parts of an input image by making saliency maps. And there's more. You can look at things like", "tokens": [50364, 3166, 295, 364, 4846, 3256, 538, 1455, 1845, 7848, 11317, 13, 400, 456, 311, 544, 13, 509, 393, 574, 412, 721, 411, 50704], "temperature": 0.0, "avg_logprob": -0.06961979024550494, "compression_ratio": 1.5833333333333333, "no_speech_prob": 0.009410923346877098}, {"id": 30, "seek": 18984, "start": 196.64000000000001, "end": 203.04, "text": " example-based explanations where you try to find the smallest change in the input data that would", "tokens": [50704, 1365, 12, 6032, 28708, 689, 291, 853, 281, 915, 264, 16998, 1319, 294, 264, 4846, 1412, 300, 576, 51024], "temperature": 0.0, "avg_logprob": -0.06961979024550494, "compression_ratio": 1.5833333333333333, "no_speech_prob": 0.009410923346877098}, {"id": 31, "seek": 18984, "start": 203.04, "end": 209.92000000000002, "text": " cause the output prediction to change. So maybe with this awesome new interpretability toolkit,", "tokens": [51024, 3082, 264, 5598, 17630, 281, 1319, 13, 407, 1310, 365, 341, 3476, 777, 7302, 2310, 40167, 11, 51368], "temperature": 0.0, "avg_logprob": -0.06961979024550494, "compression_ratio": 1.5833333333333333, "no_speech_prob": 0.009410923346877098}, {"id": 32, "seek": 18984, "start": 209.92000000000002, "end": 217.12, "text": " we can start to dispel that myth that machine learning models are all just black boxes that", "tokens": [51368, 321, 393, 722, 281, 4920, 338, 300, 9474, 300, 3479, 2539, 5245, 366, 439, 445, 2211, 9002, 300, 51728], "temperature": 0.0, "avg_logprob": -0.06961979024550494, "compression_ratio": 1.5833333333333333, "no_speech_prob": 0.009410923346877098}, {"id": 33, "seek": 21712, "start": 217.12, "end": 223.68, "text": " can't be understood and can't be trusted. Christoph Molnar is one of the most important people", "tokens": [50364, 393, 380, 312, 7320, 293, 393, 380, 312, 16034, 13, 2040, 5317, 376, 14110, 289, 307, 472, 295, 264, 881, 1021, 561, 50692], "temperature": 0.0, "avg_logprob": -0.10255051576174222, "compression_ratio": 1.7698412698412698, "no_speech_prob": 0.0006070035160519183}, {"id": 34, "seek": 21712, "start": 223.68, "end": 229.76, "text": " in the interpretable machine learning space. In 2018, he released his magnum opus,", "tokens": [50692, 294, 264, 7302, 712, 3479, 2539, 1901, 13, 682, 6096, 11, 415, 4736, 702, 4944, 449, 999, 301, 11, 50996], "temperature": 0.0, "avg_logprob": -0.10255051576174222, "compression_ratio": 1.7698412698412698, "no_speech_prob": 0.0006070035160519183}, {"id": 35, "seek": 21712, "start": 229.76, "end": 235.44, "text": " interpretable machine learning, a guide for making black box models explainable.", "tokens": [50996, 7302, 712, 3479, 2539, 11, 257, 5934, 337, 1455, 2211, 2424, 5245, 2903, 712, 13, 51280], "temperature": 0.0, "avg_logprob": -0.10255051576174222, "compression_ratio": 1.7698412698412698, "no_speech_prob": 0.0006070035160519183}, {"id": 36, "seek": 21712, "start": 235.44, "end": 240.88, "text": " Interpretability is often a deciding factor when a machine learning model is used in a product,", "tokens": [51280, 5751, 6629, 2310, 307, 2049, 257, 17990, 5952, 562, 257, 3479, 2539, 2316, 307, 1143, 294, 257, 1674, 11, 51552], "temperature": 0.0, "avg_logprob": -0.10255051576174222, "compression_ratio": 1.7698412698412698, "no_speech_prob": 0.0006070035160519183}, {"id": 37, "seek": 21712, "start": 240.88, "end": 247.04000000000002, "text": " a decision process or research. Interpretability methods can be used to discover knowledge,", "tokens": [51552, 257, 3537, 1399, 420, 2132, 13, 5751, 6629, 2310, 7150, 393, 312, 1143, 281, 4411, 3601, 11, 51860], "temperature": 0.0, "avg_logprob": -0.10255051576174222, "compression_ratio": 1.7698412698412698, "no_speech_prob": 0.0006070035160519183}, {"id": 38, "seek": 24704, "start": 247.12, "end": 254.48, "text": " to debug or justify a model and its predictions, to control and improve the model, to reason about", "tokens": [50368, 281, 24083, 420, 20833, 257, 2316, 293, 1080, 21264, 11, 281, 1969, 293, 3470, 264, 2316, 11, 281, 1778, 466, 50736], "temperature": 0.0, "avg_logprob": -0.04591225958489752, "compression_ratio": 1.625, "no_speech_prob": 0.00018804708088282496}, {"id": 39, "seek": 24704, "start": 254.48, "end": 260.4, "text": " potential biases in the model, as well as increase the societal acceptance of models.", "tokens": [50736, 3995, 32152, 294, 264, 2316, 11, 382, 731, 382, 3488, 264, 33472, 20351, 295, 5245, 13, 51032], "temperature": 0.0, "avg_logprob": -0.04591225958489752, "compression_ratio": 1.625, "no_speech_prob": 0.00018804708088282496}, {"id": 40, "seek": 24704, "start": 261.03999999999996, "end": 267.2, "text": " But interpretability methods can be quite esoteric. They add an additional layer of complexity", "tokens": [51064, 583, 7302, 2310, 7150, 393, 312, 1596, 785, 21585, 299, 13, 814, 909, 364, 4497, 4583, 295, 14024, 51372], "temperature": 0.0, "avg_logprob": -0.04591225958489752, "compression_ratio": 1.625, "no_speech_prob": 0.00018804708088282496}, {"id": 41, "seek": 24704, "start": 267.2, "end": 272.64, "text": " and the potential pitfalls require expert understanding. Machine learning models are", "tokens": [51372, 293, 264, 3995, 10147, 18542, 3651, 5844, 3701, 13, 22155, 2539, 5245, 366, 51644], "temperature": 0.0, "avg_logprob": -0.04591225958489752, "compression_ratio": 1.625, "no_speech_prob": 0.00018804708088282496}, {"id": 42, "seek": 27264, "start": 272.64, "end": 278.56, "text": " inherently less interpretable than classical statistical models, but typically they have a", "tokens": [50364, 27993, 1570, 7302, 712, 813, 13735, 22820, 5245, 11, 457, 5850, 436, 362, 257, 50660], "temperature": 0.0, "avg_logprob": -0.07407931531413217, "compression_ratio": 1.6715867158671587, "no_speech_prob": 0.002800583140924573}, {"id": 43, "seek": 27264, "start": 278.56, "end": 283.44, "text": " better predictive performance and that's because of their ability to handle nonlinear", "tokens": [50660, 1101, 35521, 3389, 293, 300, 311, 570, 295, 641, 3485, 281, 4813, 2107, 28263, 50904], "temperature": 0.0, "avg_logprob": -0.07407931531413217, "compression_ratio": 1.6715867158671587, "no_speech_prob": 0.002800583140924573}, {"id": 44, "seek": 27264, "start": 283.44, "end": 289.76, "text": " relationships and also higher order feature interactions automatically. But do we have", "tokens": [50904, 6159, 293, 611, 2946, 1668, 4111, 13280, 6772, 13, 583, 360, 321, 362, 51220], "temperature": 0.0, "avg_logprob": -0.07407931531413217, "compression_ratio": 1.6715867158671587, "no_speech_prob": 0.002800583140924573}, {"id": 45, "seek": 27264, "start": 289.76, "end": 296.08, "text": " to suffer this implicit trade-off between the complexity of a model and the lack of our ability", "tokens": [51220, 281, 9753, 341, 26947, 4923, 12, 4506, 1296, 264, 14024, 295, 257, 2316, 293, 264, 5011, 295, 527, 3485, 51536], "temperature": 0.0, "avg_logprob": -0.07407931531413217, "compression_ratio": 1.6715867158671587, "no_speech_prob": 0.002800583140924573}, {"id": 46, "seek": 27264, "start": 296.08, "end": 302.15999999999997, "text": " to understand it? Simplistic model approximations can often mask important information and be", "tokens": [51536, 281, 1223, 309, 30, 3998, 564, 3142, 2316, 8542, 763, 393, 2049, 6094, 1021, 1589, 293, 312, 51840], "temperature": 0.0, "avg_logprob": -0.07407931531413217, "compression_ratio": 1.6715867158671587, "no_speech_prob": 0.002800583140924573}, {"id": 47, "seek": 30216, "start": 302.16, "end": 308.64000000000004, "text": " misleading as a result. In classical statistics there's an entire field called model diagnostics", "tokens": [50364, 36429, 382, 257, 1874, 13, 682, 13735, 12523, 456, 311, 364, 2302, 2519, 1219, 2316, 43215, 1167, 50688], "temperature": 0.0, "avg_logprob": -0.05840762456258138, "compression_ratio": 1.6481481481481481, "no_speech_prob": 0.0002779862261377275}, {"id": 48, "seek": 30216, "start": 308.64000000000004, "end": 314.56, "text": " to do exactly this, to check that assumptions and simplifications have not been violated.", "tokens": [50688, 281, 360, 2293, 341, 11, 281, 1520, 300, 17695, 293, 6883, 7833, 362, 406, 668, 33239, 13, 50984], "temperature": 0.0, "avg_logprob": -0.05840762456258138, "compression_ratio": 1.6481481481481481, "no_speech_prob": 0.0002779862261377275}, {"id": 49, "seek": 30216, "start": 314.56, "end": 318.24, "text": " This is something that does not yet exist in interpretable machine learning.", "tokens": [50984, 639, 307, 746, 300, 775, 406, 1939, 2514, 294, 7302, 712, 3479, 2539, 13, 51168], "temperature": 0.0, "avg_logprob": -0.05840762456258138, "compression_ratio": 1.6481481481481481, "no_speech_prob": 0.0002779862261377275}, {"id": 50, "seek": 30216, "start": 318.88, "end": 324.8, "text": " Interpretability has exploded and matured in the last few years, in particular since the", "tokens": [51200, 5751, 6629, 2310, 575, 27049, 293, 14442, 67, 294, 264, 1036, 1326, 924, 11, 294, 1729, 1670, 264, 51496], "temperature": 0.0, "avg_logprob": -0.05840762456258138, "compression_ratio": 1.6481481481481481, "no_speech_prob": 0.0002779862261377275}, {"id": 51, "seek": 30216, "start": 324.8, "end": 331.12, "text": " deep learning revolution. We now have a better understanding of the weaknesses and strengths", "tokens": [51496, 2452, 2539, 8894, 13, 492, 586, 362, 257, 1101, 3701, 295, 264, 24381, 293, 16986, 51812], "temperature": 0.0, "avg_logprob": -0.05840762456258138, "compression_ratio": 1.6481481481481481, "no_speech_prob": 0.0002779862261377275}, {"id": 52, "seek": 33112, "start": 331.2, "end": 336.8, "text": " of interpretability methods. A growing number of techniques are available at our fingertips", "tokens": [50368, 295, 7302, 2310, 7150, 13, 316, 4194, 1230, 295, 7512, 366, 2435, 412, 527, 27715, 50648], "temperature": 0.0, "avg_logprob": -0.0782377335333055, "compression_ratio": 1.5919117647058822, "no_speech_prob": 0.0013241447741165757}, {"id": 53, "seek": 33112, "start": 336.8, "end": 343.68, "text": " that can lead to the wrong conclusions if applied incorrectly. Is it even possible to", "tokens": [50648, 300, 393, 1477, 281, 264, 2085, 22865, 498, 6456, 42892, 13, 1119, 309, 754, 1944, 281, 50992], "temperature": 0.0, "avg_logprob": -0.0782377335333055, "compression_ratio": 1.5919117647058822, "no_speech_prob": 0.0013241447741165757}, {"id": 54, "seek": 33112, "start": 343.68, "end": 349.6, "text": " understand complex models or even humans for that matter in any meaningful way?", "tokens": [50992, 1223, 3997, 5245, 420, 754, 6255, 337, 300, 1871, 294, 604, 10995, 636, 30, 51288], "temperature": 0.0, "avg_logprob": -0.0782377335333055, "compression_ratio": 1.5919117647058822, "no_speech_prob": 0.0013241447741165757}, {"id": 55, "seek": 33112, "start": 350.32, "end": 353.28000000000003, "text": " That is one of the questions that we're going to be discussing this evening.", "tokens": [51324, 663, 307, 472, 295, 264, 1651, 300, 321, 434, 516, 281, 312, 10850, 341, 5634, 13, 51472], "temperature": 0.0, "avg_logprob": -0.0782377335333055, "compression_ratio": 1.5919117647058822, "no_speech_prob": 0.0013241447741165757}, {"id": 56, "seek": 33112, "start": 354.32, "end": 360.48, "text": " Molnar also recently released a couple of papers where he discusses some of the important pitfalls", "tokens": [51524, 376, 14110, 289, 611, 3938, 4736, 257, 1916, 295, 10577, 689, 415, 2248, 279, 512, 295, 264, 1021, 10147, 18542, 51832], "temperature": 0.0, "avg_logprob": -0.0782377335333055, "compression_ratio": 1.5919117647058822, "no_speech_prob": 0.0013241447741165757}, {"id": 57, "seek": 36048, "start": 360.48, "end": 364.8, "text": " of interpretable machine learning methods. So some of the things that Christoph Molnar is", "tokens": [50364, 295, 7302, 712, 3479, 2539, 7150, 13, 407, 512, 295, 264, 721, 300, 2040, 5317, 376, 14110, 289, 307, 50580], "temperature": 0.0, "avg_logprob": -0.07549484426325018, "compression_ratio": 1.7806691449814127, "no_speech_prob": 0.0034242861438542604}, {"id": 58, "seek": 36048, "start": 364.8, "end": 370.8, "text": " really concerned about is the lack of statistical rigor in IML methods. Molnar used to be a statistician.", "tokens": [50580, 534, 5922, 466, 307, 264, 5011, 295, 22820, 42191, 294, 286, 12683, 7150, 13, 376, 14110, 289, 1143, 281, 312, 257, 29588, 952, 13, 50880], "temperature": 0.0, "avg_logprob": -0.07549484426325018, "compression_ratio": 1.7806691449814127, "no_speech_prob": 0.0034242861438542604}, {"id": 59, "seek": 36048, "start": 370.8, "end": 377.68, "text": " Also he is exasperated with some of the misguided causal interpretations from some of these IML", "tokens": [50880, 2743, 415, 307, 454, 296, 610, 770, 365, 512, 295, 264, 3346, 2794, 2112, 38755, 37547, 490, 512, 295, 613, 286, 12683, 51224], "temperature": 0.0, "avg_logprob": -0.07549484426325018, "compression_ratio": 1.7806691449814127, "no_speech_prob": 0.0034242861438542604}, {"id": 60, "seek": 36048, "start": 377.68, "end": 382.48, "text": " methods. He also points out feature dependence or situations where you have shared information", "tokens": [51224, 7150, 13, 634, 611, 2793, 484, 4111, 31704, 420, 6851, 689, 291, 362, 5507, 1589, 51464], "temperature": 0.0, "avg_logprob": -0.07549484426325018, "compression_ratio": 1.7806691449814127, "no_speech_prob": 0.0034242861438542604}, {"id": 61, "seek": 36048, "start": 382.48, "end": 387.20000000000005, "text": " between features. It completely breaks many of the IML methods and this is something that he", "tokens": [51464, 1296, 4122, 13, 467, 2584, 9857, 867, 295, 264, 286, 12683, 7150, 293, 341, 307, 746, 300, 415, 51700], "temperature": 0.0, "avg_logprob": -0.07549484426325018, "compression_ratio": 1.7806691449814127, "no_speech_prob": 0.0034242861438542604}, {"id": 62, "seek": 38720, "start": 387.2, "end": 393.59999999999997, "text": " focuses on a lot. He also focuses philosophically on the broader impact of interpretability and", "tokens": [50364, 16109, 322, 257, 688, 13, 634, 611, 16109, 14529, 984, 322, 264, 13227, 2712, 295, 7302, 2310, 293, 50684], "temperature": 0.0, "avg_logprob": -0.09863637577403675, "compression_ratio": 1.736462093862816, "no_speech_prob": 0.009644596837460995}, {"id": 63, "seek": 38720, "start": 394.47999999999996, "end": 400.15999999999997, "text": " what interpretability even means frankly. It's a very nebulous term. So let's have a quick flick", "tokens": [50728, 437, 7302, 2310, 754, 1355, 11939, 13, 467, 311, 257, 588, 408, 65, 6893, 1433, 13, 407, 718, 311, 362, 257, 1702, 22774, 51012], "temperature": 0.0, "avg_logprob": -0.09863637577403675, "compression_ratio": 1.736462093862816, "no_speech_prob": 0.009644596837460995}, {"id": 64, "seek": 38720, "start": 400.15999999999997, "end": 404.8, "text": " through this paper, interpretable machine learning, a brief history, state of the art and challenges", "tokens": [51012, 807, 341, 3035, 11, 7302, 712, 3479, 2539, 11, 257, 5353, 2503, 11, 1785, 295, 264, 1523, 293, 4759, 51244], "temperature": 0.0, "avg_logprob": -0.09863637577403675, "compression_ratio": 1.736462093862816, "no_speech_prob": 0.009644596837460995}, {"id": 65, "seek": 38720, "start": 404.8, "end": 408.88, "text": " and as well as pointing out some of the history of IML methods, we'll jump straight into one of", "tokens": [51244, 293, 382, 731, 382, 12166, 484, 512, 295, 264, 2503, 295, 286, 12683, 7150, 11, 321, 603, 3012, 2997, 666, 472, 295, 51448], "temperature": 0.0, "avg_logprob": -0.09863637577403675, "compression_ratio": 1.736462093862816, "no_speech_prob": 0.009644596837460995}, {"id": 66, "seek": 38720, "start": 408.88, "end": 414.15999999999997, "text": " the challenges which is feature dependence. Molnar points out that feature dependence makes", "tokens": [51448, 264, 4759, 597, 307, 4111, 31704, 13, 376, 14110, 289, 2793, 484, 300, 4111, 31704, 1669, 51712], "temperature": 0.0, "avg_logprob": -0.09863637577403675, "compression_ratio": 1.736462093862816, "no_speech_prob": 0.009644596837460995}, {"id": 67, "seek": 41416, "start": 414.16, "end": 418.96000000000004, "text": " attribution and extrapolation problematic. This is exactly what happens in partial dependency", "tokens": [50364, 9080, 1448, 293, 48224, 399, 19011, 13, 639, 307, 2293, 437, 2314, 294, 14641, 33621, 50604], "temperature": 0.0, "avg_logprob": -0.05687514092158345, "compression_ratio": 1.8435114503816794, "no_speech_prob": 0.013831489719450474}, {"id": 68, "seek": 41416, "start": 418.96000000000004, "end": 424.72, "text": " plots for example. We are basically extrapolating and we are creating fictitious data points that", "tokens": [50604, 28609, 337, 1365, 13, 492, 366, 1936, 48224, 990, 293, 321, 366, 4084, 283, 985, 16401, 1412, 2793, 300, 50892], "temperature": 0.0, "avg_logprob": -0.05687514092158345, "compression_ratio": 1.8435114503816794, "no_speech_prob": 0.013831489719450474}, {"id": 69, "seek": 41416, "start": 424.72, "end": 429.84000000000003, "text": " didn't really exist and these fictitious data points probably exist outside of the data distribution.", "tokens": [50892, 994, 380, 534, 2514, 293, 613, 283, 985, 16401, 1412, 2793, 1391, 2514, 2380, 295, 264, 1412, 7316, 13, 51148], "temperature": 0.0, "avg_logprob": -0.05687514092158345, "compression_ratio": 1.8435114503816794, "no_speech_prob": 0.013831489719450474}, {"id": 70, "seek": 41416, "start": 430.48, "end": 437.36, "text": " So Molnar thinks that the models that we build should reflect the causal structure in the world", "tokens": [51180, 407, 376, 14110, 289, 7309, 300, 264, 5245, 300, 321, 1322, 820, 5031, 264, 38755, 3877, 294, 264, 1002, 51524], "temperature": 0.0, "avg_logprob": -0.05687514092158345, "compression_ratio": 1.8435114503816794, "no_speech_prob": 0.013831489719450474}, {"id": 71, "seek": 41416, "start": 437.36, "end": 442.96000000000004, "text": " but of course that is not really the case most of the time and he points out that statistical", "tokens": [51524, 457, 295, 1164, 300, 307, 406, 534, 264, 1389, 881, 295, 264, 565, 293, 415, 2793, 484, 300, 22820, 51804], "temperature": 0.0, "avg_logprob": -0.05687514092158345, "compression_ratio": 1.8435114503816794, "no_speech_prob": 0.013831489719450474}, {"id": 72, "seek": 44296, "start": 442.96, "end": 448.88, "text": " learning is just reflecting surface feature correlations not the true causal structure beneath", "tokens": [50364, 2539, 307, 445, 23543, 3753, 4111, 13983, 763, 406, 264, 2074, 38755, 3877, 17149, 50660], "temperature": 0.0, "avg_logprob": -0.06009811284590741, "compression_ratio": 1.730909090909091, "no_speech_prob": 0.0033465551678091288}, {"id": 73, "seek": 44296, "start": 448.88, "end": 454.23999999999995, "text": " the scenes. Causal structures would be more robust if we could actually capture them", "tokens": [50660, 264, 8026, 13, 7544, 11765, 9227, 576, 312, 544, 13956, 498, 321, 727, 767, 7983, 552, 50928], "temperature": 0.0, "avg_logprob": -0.06009811284590741, "compression_ratio": 1.730909090909091, "no_speech_prob": 0.0033465551678091288}, {"id": 74, "seek": 44296, "start": 454.23999999999995, "end": 459.76, "text": " and the predicted performance and learning causal factors is a conflicting goal which I think not", "tokens": [50928, 293, 264, 19147, 3389, 293, 2539, 38755, 6771, 307, 257, 43784, 3387, 597, 286, 519, 406, 51204], "temperature": 0.0, "avg_logprob": -0.06009811284590741, "compression_ratio": 1.730909090909091, "no_speech_prob": 0.0033465551678091288}, {"id": 75, "seek": 44296, "start": 459.76, "end": 464.71999999999997, "text": " many people have thought about. So we need to think about when we can make causal interpretations", "tokens": [51204, 867, 561, 362, 1194, 466, 13, 407, 321, 643, 281, 519, 466, 562, 321, 393, 652, 38755, 37547, 51452], "temperature": 0.0, "avg_logprob": -0.06009811284590741, "compression_ratio": 1.730909090909091, "no_speech_prob": 0.0033465551678091288}, {"id": 76, "seek": 44296, "start": 464.71999999999997, "end": 469.52, "text": " and a lot of work is underway in this field but being completely frank this is very nascent. There's", "tokens": [51452, 293, 257, 688, 295, 589, 307, 27534, 294, 341, 2519, 457, 885, 2584, 10455, 341, 307, 588, 5382, 2207, 13, 821, 311, 51692], "temperature": 0.0, "avg_logprob": -0.06009811284590741, "compression_ratio": 1.730909090909091, "no_speech_prob": 0.0033465551678091288}, {"id": 77, "seek": 46952, "start": 469.52, "end": 475.91999999999996, "text": " not really much out there at the moment. Molnar also points out this lack of statistical rigor", "tokens": [50364, 406, 534, 709, 484, 456, 412, 264, 1623, 13, 376, 14110, 289, 611, 2793, 484, 341, 5011, 295, 22820, 42191, 50684], "temperature": 0.0, "avg_logprob": -0.06734087420444862, "compression_ratio": 1.711111111111111, "no_speech_prob": 0.013235362246632576}, {"id": 78, "seek": 46952, "start": 476.56, "end": 481.12, "text": " having been a statistician himself. He was exasperated when he came into the IML field just", "tokens": [50716, 1419, 668, 257, 29588, 952, 3647, 13, 634, 390, 454, 296, 610, 770, 562, 415, 1361, 666, 264, 286, 12683, 2519, 445, 50944], "temperature": 0.0, "avg_logprob": -0.06734087420444862, "compression_ratio": 1.711111111111111, "no_speech_prob": 0.013235362246632576}, {"id": 79, "seek": 46952, "start": 481.12, "end": 486.71999999999997, "text": " to see that most IML methods do not even give you confidence estimates something which is", "tokens": [50944, 281, 536, 300, 881, 286, 12683, 7150, 360, 406, 754, 976, 291, 6687, 20561, 746, 597, 307, 51224], "temperature": 0.0, "avg_logprob": -0.06734087420444862, "compression_ratio": 1.711111111111111, "no_speech_prob": 0.013235362246632576}, {"id": 80, "seek": 46952, "start": 486.71999999999997, "end": 492.08, "text": " completely standard in the statistical world. Models and explanations are computed from data", "tokens": [51224, 2584, 3832, 294, 264, 22820, 1002, 13, 6583, 1625, 293, 28708, 366, 40610, 490, 1412, 51492], "temperature": 0.0, "avg_logprob": -0.06734087420444862, "compression_ratio": 1.711111111111111, "no_speech_prob": 0.013235362246632576}, {"id": 81, "seek": 46952, "start": 492.08, "end": 497.35999999999996, "text": " which means they are subject to uncertainty but this is something which is just not captured", "tokens": [51492, 597, 1355, 436, 366, 3983, 281, 15697, 457, 341, 307, 746, 597, 307, 445, 406, 11828, 51756], "temperature": 0.0, "avg_logprob": -0.06734087420444862, "compression_ratio": 1.711111111111111, "no_speech_prob": 0.013235362246632576}, {"id": 82, "seek": 49736, "start": 497.36, "end": 502.64, "text": " using current methods. He says that we need to be making distributional and structural assumptions.", "tokens": [50364, 1228, 2190, 7150, 13, 634, 1619, 300, 321, 643, 281, 312, 1455, 7316, 304, 293, 15067, 17695, 13, 50628], "temperature": 0.0, "avg_logprob": -0.06439549009376597, "compression_ratio": 1.7096774193548387, "no_speech_prob": 0.0034390678629279137}, {"id": 83, "seek": 49736, "start": 502.64, "end": 509.28000000000003, "text": " He points out this risk of p-hacking something which is prevalence in the natural sciences.", "tokens": [50628, 634, 2793, 484, 341, 3148, 295, 280, 12, 71, 14134, 746, 597, 307, 42583, 294, 264, 3303, 17677, 13, 50960], "temperature": 0.0, "avg_logprob": -0.06439549009376597, "compression_ratio": 1.7096774193548387, "no_speech_prob": 0.0034390678629279137}, {"id": 84, "seek": 49736, "start": 509.28000000000003, "end": 514.0, "text": " This is something that could be coming to the world of IML very soon if we don't start thinking", "tokens": [50960, 639, 307, 746, 300, 727, 312, 1348, 281, 264, 1002, 295, 286, 12683, 588, 2321, 498, 321, 500, 380, 722, 1953, 51196], "temperature": 0.0, "avg_logprob": -0.06439549009376597, "compression_ratio": 1.7096774193548387, "no_speech_prob": 0.0034390678629279137}, {"id": 85, "seek": 49736, "start": 514.0, "end": 519.52, "text": " about this more carefully. Molnar also points out that there is no accepted definition of", "tokens": [51196, 466, 341, 544, 7500, 13, 376, 14110, 289, 611, 2793, 484, 300, 456, 307, 572, 9035, 7123, 295, 51472], "temperature": 0.0, "avg_logprob": -0.06439549009376597, "compression_ratio": 1.7096774193548387, "no_speech_prob": 0.0034390678629279137}, {"id": 86, "seek": 49736, "start": 519.52, "end": 525.9200000000001, "text": " interpretable machine learning methods so it's not entirely clear how we can compare IML methods to", "tokens": [51472, 7302, 712, 3479, 2539, 7150, 370, 309, 311, 406, 7696, 1850, 577, 321, 393, 6794, 286, 12683, 7150, 281, 51792], "temperature": 0.0, "avg_logprob": -0.06439549009376597, "compression_ratio": 1.7096774193548387, "no_speech_prob": 0.0034390678629279137}, {"id": 87, "seek": 52592, "start": 525.92, "end": 531.52, "text": " machine learning models. It's really easy to assess machine learning models because we have", "tokens": [50364, 3479, 2539, 5245, 13, 467, 311, 534, 1858, 281, 5877, 3479, 2539, 5245, 570, 321, 362, 50644], "temperature": 0.0, "avg_logprob": -0.07111385403847208, "compression_ratio": 1.7677902621722847, "no_speech_prob": 0.009119425900280476}, {"id": 88, "seek": 52592, "start": 531.52, "end": 536.88, "text": " benchmarks and we have ground truth labels. Those benchmarks are fraught with problems as well", "tokens": [50644, 43751, 293, 321, 362, 2727, 3494, 16949, 13, 3950, 43751, 366, 6600, 1599, 365, 2740, 382, 731, 50912], "temperature": 0.0, "avg_logprob": -0.07111385403847208, "compression_ratio": 1.7677902621722847, "no_speech_prob": 0.009119425900280476}, {"id": 89, "seek": 52592, "start": 536.88, "end": 542.8, "text": " but we can't really quantify how correct an explanation is and it doesn't really help that", "tokens": [50912, 457, 321, 393, 380, 534, 40421, 577, 3006, 364, 10835, 307, 293, 309, 1177, 380, 534, 854, 300, 51208], "temperature": 0.0, "avg_logprob": -0.07111385403847208, "compression_ratio": 1.7677902621722847, "no_speech_prob": 0.009119425900280476}, {"id": 90, "seek": 52592, "start": 542.8, "end": 547.68, "text": " there's a taxonomy of interpretability methods. There are objective methods like sparsity and", "tokens": [51208, 456, 311, 257, 3366, 23423, 295, 7302, 2310, 7150, 13, 821, 366, 10024, 7150, 411, 637, 685, 507, 293, 51452], "temperature": 0.0, "avg_logprob": -0.07111385403847208, "compression_ratio": 1.7677902621722847, "no_speech_prob": 0.009119425900280476}, {"id": 91, "seek": 52592, "start": 547.68, "end": 554.0, "text": " interaction strength and there are human-centered evaluations from domain experts or from lay people", "tokens": [51452, 9285, 3800, 293, 456, 366, 1952, 12, 36814, 43085, 490, 9274, 8572, 420, 490, 2360, 561, 51768], "temperature": 0.0, "avg_logprob": -0.07111385403847208, "compression_ratio": 1.7677902621722847, "no_speech_prob": 0.009119425900280476}, {"id": 92, "seek": 55400, "start": 554.0, "end": 558.64, "text": " and quite often you need to have quite a lot of technical knowledge to even understand", "tokens": [50364, 293, 1596, 2049, 291, 643, 281, 362, 1596, 257, 688, 295, 6191, 3601, 281, 754, 1223, 50596], "temperature": 0.0, "avg_logprob": -0.04447978677101506, "compression_ratio": 1.8160919540229885, "no_speech_prob": 0.0002952278882730752}, {"id": 93, "seek": 55400, "start": 558.64, "end": 564.72, "text": " these assessments. He says that the setting of machine learning is too static. It doesn't reflect", "tokens": [50596, 613, 24338, 13, 634, 1619, 300, 264, 3287, 295, 3479, 2539, 307, 886, 13437, 13, 467, 1177, 380, 5031, 50900], "temperature": 0.0, "avg_logprob": -0.04447978677101506, "compression_ratio": 1.8160919540229885, "no_speech_prob": 0.0002952278882730752}, {"id": 94, "seek": 55400, "start": 564.72, "end": 569.36, "text": " how these models are actually used in practice and I really love this idea of thinking about a", "tokens": [50900, 577, 613, 5245, 366, 767, 1143, 294, 3124, 293, 286, 534, 959, 341, 1558, 295, 1953, 466, 257, 51132], "temperature": 0.0, "avg_logprob": -0.04447978677101506, "compression_ratio": 1.8160919540229885, "no_speech_prob": 0.0002952278882730752}, {"id": 95, "seek": 55400, "start": 569.36, "end": 574.32, "text": " process rather than thinking about just the model so he says we need to have a holistic view of the", "tokens": [51132, 1399, 2831, 813, 1953, 466, 445, 264, 2316, 370, 415, 1619, 321, 643, 281, 362, 257, 30334, 1910, 295, 264, 51380], "temperature": 0.0, "avg_logprob": -0.04447978677101506, "compression_ratio": 1.8160919540229885, "no_speech_prob": 0.0002952278882730752}, {"id": 96, "seek": 55400, "start": 574.32, "end": 579.52, "text": " entire process. He thinks that we need to think about how we explain predictions to folks from", "tokens": [51380, 2302, 1399, 13, 634, 7309, 300, 321, 643, 281, 519, 466, 577, 321, 2903, 21264, 281, 4024, 490, 51640], "temperature": 0.0, "avg_logprob": -0.04447978677101506, "compression_ratio": 1.8160919540229885, "no_speech_prob": 0.0002952278882730752}, {"id": 97, "seek": 57952, "start": 579.52, "end": 585.76, "text": " diverse backgrounds, how we have interpretability at the societal level or at the institutional level", "tokens": [50364, 9521, 17336, 11, 577, 321, 362, 7302, 2310, 412, 264, 33472, 1496, 420, 412, 264, 18391, 1496, 50676], "temperature": 0.0, "avg_logprob": -0.07292444537384342, "compression_ratio": 1.7282608695652173, "no_speech_prob": 0.04217151552438736}, {"id": 98, "seek": 57952, "start": 585.76, "end": 589.92, "text": " thinking much more broadly than we are at the moment. He also thinks that we need to reach out", "tokens": [50676, 1953, 709, 544, 19511, 813, 321, 366, 412, 264, 1623, 13, 634, 611, 7309, 300, 321, 643, 281, 2524, 484, 50884], "temperature": 0.0, "avg_logprob": -0.07292444537384342, "compression_ratio": 1.7282608695652173, "no_speech_prob": 0.04217151552438736}, {"id": 99, "seek": 57952, "start": 589.92, "end": 595.28, "text": " to other disciplines for example psychologists and social scientists and he thinks that there's", "tokens": [50884, 281, 661, 21919, 337, 1365, 41562, 293, 2093, 7708, 293, 415, 7309, 300, 456, 311, 51152], "temperature": 0.0, "avg_logprob": -0.07292444537384342, "compression_ratio": 1.7282608695652173, "no_speech_prob": 0.04217151552438736}, {"id": 100, "seek": 57952, "start": 595.28, "end": 599.76, "text": " lots of rich knowledge in computer science and statistics that we're just not using yet.", "tokens": [51152, 3195, 295, 4593, 3601, 294, 3820, 3497, 293, 12523, 300, 321, 434, 445, 406, 1228, 1939, 13, 51376], "temperature": 0.0, "avg_logprob": -0.07292444537384342, "compression_ratio": 1.7282608695652173, "no_speech_prob": 0.04217151552438736}, {"id": 101, "seek": 57952, "start": 599.76, "end": 605.6, "text": " So in July of last year he also released this paper pitfalls to avoid when interpreting machine", "tokens": [51376, 407, 294, 7370, 295, 1036, 1064, 415, 611, 4736, 341, 3035, 10147, 18542, 281, 5042, 562, 37395, 3479, 51668], "temperature": 0.0, "avg_logprob": -0.07292444537384342, "compression_ratio": 1.7282608695652173, "no_speech_prob": 0.04217151552438736}, {"id": 102, "seek": 60560, "start": 605.6, "end": 609.9200000000001, "text": " learning models. In this paper he points out that there's a growing number of techniques", "tokens": [50364, 2539, 5245, 13, 682, 341, 3035, 415, 2793, 484, 300, 456, 311, 257, 4194, 1230, 295, 7512, 50580], "temperature": 0.0, "avg_logprob": -0.06928033133347829, "compression_ratio": 1.8072289156626506, "no_speech_prob": 0.09556180238723755}, {"id": 103, "seek": 60560, "start": 609.9200000000001, "end": 614.8000000000001, "text": " providing model interpretations but many will lead to the wrong conclusions if used incorrectly", "tokens": [50580, 6530, 2316, 37547, 457, 867, 486, 1477, 281, 264, 2085, 22865, 498, 1143, 42892, 50824], "temperature": 0.0, "avg_logprob": -0.06928033133347829, "compression_ratio": 1.8072289156626506, "no_speech_prob": 0.09556180238723755}, {"id": 104, "seek": 60560, "start": 614.8000000000001, "end": 619.44, "text": " and he goes on to point out many of those pitfalls. For example the first one is", "tokens": [50824, 293, 415, 1709, 322, 281, 935, 484, 867, 295, 729, 10147, 18542, 13, 1171, 1365, 264, 700, 472, 307, 51056], "temperature": 0.0, "avg_logprob": -0.06928033133347829, "compression_ratio": 1.8072289156626506, "no_speech_prob": 0.09556180238723755}, {"id": 105, "seek": 60560, "start": 619.44, "end": 624.16, "text": " assuming that the model generalizes well so assuming that the model has been fit correctly", "tokens": [51056, 11926, 300, 264, 2316, 2674, 5660, 731, 370, 11926, 300, 264, 2316, 575, 668, 3318, 8944, 51292], "temperature": 0.0, "avg_logprob": -0.06928033133347829, "compression_ratio": 1.8072289156626506, "no_speech_prob": 0.09556180238723755}, {"id": 106, "seek": 60560, "start": 624.16, "end": 629.44, "text": " if the model is underfit or overfit then the interpretation method will perform badly as well", "tokens": [51292, 498, 264, 2316, 307, 833, 6845, 420, 670, 6845, 550, 264, 14174, 3170, 486, 2042, 13425, 382, 731, 51556], "temperature": 0.0, "avg_logprob": -0.06928033133347829, "compression_ratio": 1.8072289156626506, "no_speech_prob": 0.09556180238723755}, {"id": 107, "seek": 62944, "start": 629.44, "end": 635.6800000000001, "text": " and interpretation can only be as good as the model underlying it. So the next pitfall he points", "tokens": [50364, 293, 14174, 393, 787, 312, 382, 665, 382, 264, 2316, 14217, 309, 13, 407, 264, 958, 10147, 6691, 415, 2793, 50676], "temperature": 0.0, "avg_logprob": -0.059293452824387596, "compression_ratio": 1.8695652173913044, "no_speech_prob": 0.04632778838276863}, {"id": 108, "seek": 62944, "start": 635.6800000000001, "end": 641.44, "text": " out is the unnecessary use of complex models which is to say the use of opaque or complex", "tokens": [50676, 484, 307, 264, 19350, 764, 295, 3997, 5245, 597, 307, 281, 584, 264, 764, 295, 42687, 420, 3997, 50964], "temperature": 0.0, "avg_logprob": -0.059293452824387596, "compression_ratio": 1.8695652173913044, "no_speech_prob": 0.04632778838276863}, {"id": 109, "seek": 62944, "start": 641.44, "end": 646.96, "text": " machine learning models when an interpretable model would have sufficed which is to say when", "tokens": [50964, 3479, 2539, 5245, 562, 364, 7302, 712, 2316, 576, 362, 459, 3341, 292, 597, 307, 281, 584, 562, 51240], "temperature": 0.0, "avg_logprob": -0.059293452824387596, "compression_ratio": 1.8695652173913044, "no_speech_prob": 0.04632778838276863}, {"id": 110, "seek": 62944, "start": 646.96, "end": 651.6, "text": " the performance of an interpretable model is only negligibly worse than one of these black box", "tokens": [51240, 264, 3389, 295, 364, 7302, 712, 2316, 307, 787, 32570, 3545, 5324, 813, 472, 295, 613, 2211, 2424, 51472], "temperature": 0.0, "avg_logprob": -0.059293452824387596, "compression_ratio": 1.8695652173913044, "no_speech_prob": 0.04632778838276863}, {"id": 111, "seek": 62944, "start": 651.6, "end": 656.6400000000001, "text": " models and to be honest this is something I see all the time I think the gratuitous use of complex", "tokens": [51472, 5245, 293, 281, 312, 3245, 341, 307, 746, 286, 536, 439, 264, 565, 286, 519, 264, 38342, 563, 764, 295, 3997, 51724], "temperature": 0.0, "avg_logprob": -0.059293452824387596, "compression_ratio": 1.8695652173913044, "no_speech_prob": 0.04632778838276863}, {"id": 112, "seek": 65664, "start": 656.64, "end": 660.88, "text": " machine learning models is something which is really serious. One of the things I don't like", "tokens": [50364, 3479, 2539, 5245, 307, 746, 597, 307, 534, 3156, 13, 1485, 295, 264, 721, 286, 500, 380, 411, 50576], "temperature": 0.0, "avg_logprob": -0.07510505532318691, "compression_ratio": 1.8052434456928839, "no_speech_prob": 0.053327079862356186}, {"id": 113, "seek": 65664, "start": 660.88, "end": 667.12, "text": " about machine learning is the laziness. I think we should always seek to understand and simplify", "tokens": [50576, 466, 3479, 2539, 307, 264, 19320, 1324, 13, 286, 519, 321, 820, 1009, 8075, 281, 1223, 293, 20460, 50888], "temperature": 0.0, "avg_logprob": -0.07510505532318691, "compression_ratio": 1.8052434456928839, "no_speech_prob": 0.053327079862356186}, {"id": 114, "seek": 65664, "start": 667.12, "end": 671.12, "text": " problems wherever we can it's the same thing in software engineering. We should always be trying", "tokens": [50888, 2740, 8660, 321, 393, 309, 311, 264, 912, 551, 294, 4722, 7043, 13, 492, 820, 1009, 312, 1382, 51088], "temperature": 0.0, "avg_logprob": -0.07510505532318691, "compression_ratio": 1.8052434456928839, "no_speech_prob": 0.053327079862356186}, {"id": 115, "seek": 65664, "start": 671.12, "end": 677.68, "text": " to create the most elegant and simple and maintainable solution. We shouldn't be trying to over", "tokens": [51088, 281, 1884, 264, 881, 21117, 293, 2199, 293, 6909, 712, 3827, 13, 492, 4659, 380, 312, 1382, 281, 670, 51416], "temperature": 0.0, "avg_logprob": -0.07510505532318691, "compression_ratio": 1.8052434456928839, "no_speech_prob": 0.053327079862356186}, {"id": 116, "seek": 65664, "start": 677.68, "end": 682.8, "text": " complicate things and I think that's a very you know the kiss principle is very generalizable here.", "tokens": [51416, 1209, 8700, 721, 293, 286, 519, 300, 311, 257, 588, 291, 458, 264, 7704, 8665, 307, 588, 2674, 22395, 510, 13, 51672], "temperature": 0.0, "avg_logprob": -0.07510505532318691, "compression_ratio": 1.8052434456928839, "no_speech_prob": 0.053327079862356186}, {"id": 117, "seek": 68280, "start": 683.4399999999999, "end": 688.56, "text": " So he recommends to start with simple interpretable models like generalized linear models or lasso", "tokens": [50396, 407, 415, 34556, 281, 722, 365, 2199, 7302, 712, 5245, 411, 44498, 8213, 5245, 420, 2439, 539, 50652], "temperature": 0.0, "avg_logprob": -0.06570744760257681, "compression_ratio": 1.7075812274368232, "no_speech_prob": 0.001668834127485752}, {"id": 118, "seek": 68280, "start": 688.56, "end": 694.16, "text": " models or additive models decision trees or decision rules and gradually ratcheting up the", "tokens": [50652, 5245, 420, 45558, 5245, 3537, 5852, 420, 3537, 4474, 293, 13145, 367, 852, 9880, 493, 264, 50932], "temperature": 0.0, "avg_logprob": -0.06570744760257681, "compression_ratio": 1.7075812274368232, "no_speech_prob": 0.001668834127485752}, {"id": 119, "seek": 68280, "start": 694.16, "end": 700.4799999999999, "text": " complexity as required. So he also points out that ignoring feature dependence is super important", "tokens": [50932, 14024, 382, 4739, 13, 407, 415, 611, 2793, 484, 300, 26258, 4111, 31704, 307, 1687, 1021, 51248], "temperature": 0.0, "avg_logprob": -0.06570744760257681, "compression_ratio": 1.7075812274368232, "no_speech_prob": 0.001668834127485752}, {"id": 120, "seek": 68280, "start": 700.4799999999999, "end": 705.1999999999999, "text": " right and this is a problem that many of the IML methods have so he gives an example of", "tokens": [51248, 558, 293, 341, 307, 257, 1154, 300, 867, 295, 264, 286, 12683, 7150, 362, 370, 415, 2709, 364, 1365, 295, 51484], "temperature": 0.0, "avg_logprob": -0.06570744760257681, "compression_ratio": 1.7075812274368232, "no_speech_prob": 0.001668834127485752}, {"id": 121, "seek": 68280, "start": 705.1999999999999, "end": 710.64, "text": " partial dependency plots where they extrapolate in areas where the model has little training data", "tokens": [51484, 14641, 33621, 28609, 689, 436, 48224, 473, 294, 3179, 689, 264, 2316, 575, 707, 3097, 1412, 51756], "temperature": 0.0, "avg_logprob": -0.06570744760257681, "compression_ratio": 1.7075812274368232, "no_speech_prob": 0.001668834127485752}, {"id": 122, "seek": 71064, "start": 710.64, "end": 715.84, "text": " and it can cause misleading interpretations. So these perturbations produce artificial data", "tokens": [50364, 293, 309, 393, 3082, 36429, 37547, 13, 407, 613, 40468, 763, 5258, 11677, 1412, 50624], "temperature": 0.0, "avg_logprob": -0.061713127174762766, "compression_ratio": 1.946236559139785, "no_speech_prob": 0.0037929737009108067}, {"id": 123, "seek": 71064, "start": 715.84, "end": 720.8, "text": " points that are used for model predictions which in turn are aggregated to produce global", "tokens": [50624, 2793, 300, 366, 1143, 337, 2316, 21264, 597, 294, 1261, 366, 16743, 770, 281, 5258, 4338, 50872], "temperature": 0.0, "avg_logprob": -0.061713127174762766, "compression_ratio": 1.946236559139785, "no_speech_prob": 0.0037929737009108067}, {"id": 124, "seek": 71064, "start": 720.8, "end": 725.76, "text": " interpretation so that's a big problem. Another thing he points out is confusing correlation", "tokens": [50872, 14174, 370, 300, 311, 257, 955, 1154, 13, 3996, 551, 415, 2793, 484, 307, 13181, 20009, 51120], "temperature": 0.0, "avg_logprob": -0.061713127174762766, "compression_ratio": 1.946236559139785, "no_speech_prob": 0.0037929737009108067}, {"id": 125, "seek": 71064, "start": 725.76, "end": 730.72, "text": " with dependence so he gives an example here features with a Pearson correlation coefficient", "tokens": [51120, 365, 31704, 370, 415, 2709, 364, 1365, 510, 4122, 365, 257, 39041, 20009, 17619, 51368], "temperature": 0.0, "avg_logprob": -0.061713127174762766, "compression_ratio": 1.946236559139785, "no_speech_prob": 0.0037929737009108067}, {"id": 126, "seek": 71064, "start": 730.72, "end": 735.28, "text": " close to zero can still be dependent and cause misleading model interpretations", "tokens": [51368, 1998, 281, 4018, 393, 920, 312, 12334, 293, 3082, 36429, 2316, 37547, 51596], "temperature": 0.0, "avg_logprob": -0.061713127174762766, "compression_ratio": 1.946236559139785, "no_speech_prob": 0.0037929737009108067}, {"id": 127, "seek": 71064, "start": 735.28, "end": 740.16, "text": " while independence between two features implies that the Pearson correlation coefficient is zero", "tokens": [51596, 1339, 14640, 1296, 732, 4122, 18779, 300, 264, 39041, 20009, 17619, 307, 4018, 51840], "temperature": 0.0, "avg_logprob": -0.061713127174762766, "compression_ratio": 1.946236559139785, "no_speech_prob": 0.0037929737009108067}, {"id": 128, "seek": 74016, "start": 740.16, "end": 745.6, "text": " the converse is generally false. So there's a pretty cool example here this is a couple of features", "tokens": [50364, 264, 416, 4308, 307, 5101, 7908, 13, 407, 456, 311, 257, 1238, 1627, 1365, 510, 341, 307, 257, 1916, 295, 4122, 50636], "temperature": 0.0, "avg_logprob": -0.06287261927239249, "compression_ratio": 1.7846715328467153, "no_speech_prob": 0.0007114876061677933}, {"id": 129, "seek": 74016, "start": 745.6, "end": 751.1999999999999, "text": " that absolutely have a dependence on each other you can see it visualized here but you wouldn't", "tokens": [50636, 300, 3122, 362, 257, 31704, 322, 1184, 661, 291, 393, 536, 309, 5056, 1602, 510, 457, 291, 2759, 380, 50916], "temperature": 0.0, "avg_logprob": -0.06287261927239249, "compression_ratio": 1.7846715328467153, "no_speech_prob": 0.0007114876061677933}, {"id": 130, "seek": 74016, "start": 751.1999999999999, "end": 755.36, "text": " know that if you looked at the Pearson correlation it would have said that it wasn't significant.", "tokens": [50916, 458, 300, 498, 291, 2956, 412, 264, 39041, 20009, 309, 576, 362, 848, 300, 309, 2067, 380, 4776, 13, 51124], "temperature": 0.0, "avg_logprob": -0.06287261927239249, "compression_ratio": 1.7846715328467153, "no_speech_prob": 0.0007114876061677933}, {"id": 131, "seek": 74016, "start": 755.36, "end": 760.9599999999999, "text": " Another one misleading effect due to interactions so there's a couple of things here there's the", "tokens": [51124, 3996, 472, 36429, 1802, 3462, 281, 13280, 370, 456, 311, 257, 1916, 295, 721, 510, 456, 311, 264, 51404], "temperature": 0.0, "avg_logprob": -0.06287261927239249, "compression_ratio": 1.7846715328467153, "no_speech_prob": 0.0007114876061677933}, {"id": 132, "seek": 74016, "start": 760.9599999999999, "end": 766.3199999999999, "text": " partial dependency plots on a couple of dependent features and then he's used a simulation to kind", "tokens": [51404, 14641, 33621, 28609, 322, 257, 1916, 295, 12334, 4122, 293, 550, 415, 311, 1143, 257, 16575, 281, 733, 51672], "temperature": 0.0, "avg_logprob": -0.06287261927239249, "compression_ratio": 1.7846715328467153, "no_speech_prob": 0.0007114876061677933}, {"id": 133, "seek": 76632, "start": 766.4000000000001, "end": 772.32, "text": " of trace all of these different features to see what the predicted label was and according to", "tokens": [50368, 295, 13508, 439, 295, 613, 819, 4122, 281, 536, 437, 264, 19147, 7645, 390, 293, 4650, 281, 50664], "temperature": 0.0, "avg_logprob": -0.10119583921612434, "compression_ratio": 1.6843971631205674, "no_speech_prob": 0.02899414487183094}, {"id": 134, "seek": 76632, "start": 772.32, "end": 778.5600000000001, "text": " these IML methods there is actually no clear dependency between these features and the predicted", "tokens": [50664, 613, 286, 12683, 7150, 456, 307, 767, 572, 1850, 33621, 1296, 613, 4122, 293, 264, 19147, 50976], "temperature": 0.0, "avg_logprob": -0.10119583921612434, "compression_ratio": 1.6843971631205674, "no_speech_prob": 0.02899414487183094}, {"id": 135, "seek": 76632, "start": 778.5600000000001, "end": 782.6400000000001, "text": " outcome whereas you can see that that's just blatantly false. So something I've been meaning", "tokens": [50976, 9700, 9735, 291, 393, 536, 300, 300, 311, 445, 42780, 3627, 7908, 13, 407, 746, 286, 600, 668, 3620, 51180], "temperature": 0.0, "avg_logprob": -0.10119583921612434, "compression_ratio": 1.6843971631205674, "no_speech_prob": 0.02899414487183094}, {"id": 136, "seek": 76632, "start": 782.6400000000001, "end": 788.24, "text": " to do for more than a year now is to go through Molnar's interpretability book and to make some", "tokens": [51180, 281, 360, 337, 544, 813, 257, 1064, 586, 307, 281, 352, 807, 376, 14110, 289, 311, 7302, 2310, 1446, 293, 281, 652, 512, 51460], "temperature": 0.0, "avg_logprob": -0.10119583921612434, "compression_ratio": 1.6843971631205674, "no_speech_prob": 0.02899414487183094}, {"id": 137, "seek": 76632, "start": 788.24, "end": 792.96, "text": " bite-sized videos on every single approach well Connor and I are actually going to do that over", "tokens": [51460, 7988, 12, 20614, 2145, 322, 633, 2167, 3109, 731, 33133, 293, 286, 366, 767, 516, 281, 360, 300, 670, 51696], "temperature": 0.0, "avg_logprob": -0.10119583921612434, "compression_ratio": 1.6843971631205674, "no_speech_prob": 0.02899414487183094}, {"id": 138, "seek": 79296, "start": 792.96, "end": 798.08, "text": " on Machine Learning Dojo with the first one next week on Shapley Values so make sure you", "tokens": [50364, 322, 22155, 15205, 1144, 5134, 365, 264, 700, 472, 958, 1243, 322, 44160, 3420, 7188, 1247, 370, 652, 988, 291, 50620], "temperature": 0.0, "avg_logprob": -0.10131670128215443, "compression_ratio": 1.5950413223140496, "no_speech_prob": 0.03613957017660141}, {"id": 139, "seek": 79296, "start": 798.08, "end": 803.6800000000001, "text": " subscribe to Dojo and check that out. Remember to like comment and subscribe we love reading your", "tokens": [50620, 3022, 281, 1144, 5134, 293, 1520, 300, 484, 13, 5459, 281, 411, 2871, 293, 3022, 321, 959, 3760, 428, 50900], "temperature": 0.0, "avg_logprob": -0.10131670128215443, "compression_ratio": 1.5950413223140496, "no_speech_prob": 0.03613957017660141}, {"id": 140, "seek": 79296, "start": 803.6800000000001, "end": 811.6800000000001, "text": " comments and we'll see you back next week. Welcome back to the Machine Learning Street Talk YouTube", "tokens": [50900, 3053, 293, 321, 603, 536, 291, 646, 958, 1243, 13, 4027, 646, 281, 264, 22155, 15205, 7638, 8780, 3088, 51300], "temperature": 0.0, "avg_logprob": -0.10131670128215443, "compression_ratio": 1.5950413223140496, "no_speech_prob": 0.03613957017660141}, {"id": 141, "seek": 79296, "start": 811.6800000000001, "end": 818.08, "text": " channel and podcast with my two compadres Connor Tan who runs the Thomas Bay's Appreciation Society", "tokens": [51300, 2269, 293, 7367, 365, 452, 732, 715, 345, 495, 33133, 17046, 567, 6676, 264, 8500, 7840, 311, 33669, 399, 13742, 51620], "temperature": 0.0, "avg_logprob": -0.10131670128215443, "compression_ratio": 1.5950413223140496, "no_speech_prob": 0.03613957017660141}, {"id": 142, "seek": 81808, "start": 818.1600000000001, "end": 826.32, "text": " and MIT PhD Dr Keith Duggar. Now they say that Germans are known for beer, sausages, precision", "tokens": [50368, 293, 13100, 14476, 2491, 20613, 413, 697, 2976, 13, 823, 436, 584, 300, 18116, 366, 2570, 337, 8795, 11, 41157, 11, 18356, 50776], "temperature": 0.0, "avg_logprob": -0.09281946818033854, "compression_ratio": 1.6351931330472103, "no_speech_prob": 0.08634516596794128}, {"id": 143, "seek": 81808, "start": 826.32, "end": 832.96, "text": " and these days interpretable machine learning. We have an exemplar German on the show Christoph", "tokens": [50776, 293, 613, 1708, 7302, 712, 3479, 2539, 13, 492, 362, 364, 24112, 289, 6521, 322, 264, 855, 2040, 5317, 51108], "temperature": 0.0, "avg_logprob": -0.09281946818033854, "compression_ratio": 1.6351931330472103, "no_speech_prob": 0.08634516596794128}, {"id": 144, "seek": 81808, "start": 832.96, "end": 840.64, "text": " Molnar. Now Christoph made waves in the community when he released his Magnum Opus interpretable", "tokens": [51108, 376, 14110, 289, 13, 823, 2040, 5317, 1027, 9417, 294, 264, 1768, 562, 415, 4736, 702, 19664, 449, 12011, 301, 7302, 712, 51492], "temperature": 0.0, "avg_logprob": -0.09281946818033854, "compression_ratio": 1.6351931330472103, "no_speech_prob": 0.08634516596794128}, {"id": 145, "seek": 81808, "start": 840.64, "end": 846.1600000000001, "text": " machine learning a guide for making black box models explainable. If a machine learning model", "tokens": [51492, 3479, 2539, 257, 5934, 337, 1455, 2211, 2424, 5245, 2903, 712, 13, 759, 257, 3479, 2539, 2316, 51768], "temperature": 0.0, "avg_logprob": -0.09281946818033854, "compression_ratio": 1.6351931330472103, "no_speech_prob": 0.08634516596794128}, {"id": 146, "seek": 84616, "start": 846.16, "end": 851.8399999999999, "text": " performs well why don't we just trust the model and ignore why it made a certain decision? Well", "tokens": [50364, 26213, 731, 983, 500, 380, 321, 445, 3361, 264, 2316, 293, 11200, 983, 309, 1027, 257, 1629, 3537, 30, 1042, 50648], "temperature": 0.0, "avg_logprob": -0.0949485683441162, "compression_ratio": 1.6354166666666667, "no_speech_prob": 0.04683510586619377}, {"id": 147, "seek": 84616, "start": 851.8399999999999, "end": 857.12, "text": " the problem is that a single metric such as classification accuracy is an incomplete description", "tokens": [50648, 264, 1154, 307, 300, 257, 2167, 20678, 1270, 382, 21538, 14170, 307, 364, 31709, 3855, 50912], "temperature": 0.0, "avg_logprob": -0.0949485683441162, "compression_ratio": 1.6354166666666667, "no_speech_prob": 0.04683510586619377}, {"id": 148, "seek": 84616, "start": 857.12, "end": 863.68, "text": " of most real-world tasks now as according to Doshi Values and Kim in 2017. In Christoph's book he", "tokens": [50912, 295, 881, 957, 12, 13217, 9608, 586, 382, 4650, 281, 413, 17392, 7188, 1247, 293, 5652, 294, 6591, 13, 682, 2040, 5317, 311, 1446, 415, 51240], "temperature": 0.0, "avg_logprob": -0.0949485683441162, "compression_ratio": 1.6354166666666667, "no_speech_prob": 0.04683510586619377}, {"id": 149, "seek": 84616, "start": 863.68, "end": 868.9599999999999, "text": " introduces the importance of interpretability and reports an incredibly detailed taxonomy", "tokens": [51240, 31472, 264, 7379, 295, 7302, 2310, 293, 7122, 364, 6252, 9942, 3366, 23423, 51504], "temperature": 0.0, "avg_logprob": -0.0949485683441162, "compression_ratio": 1.6354166666666667, "no_speech_prob": 0.04683510586619377}, {"id": 150, "seek": 84616, "start": 868.9599999999999, "end": 874.0, "text": " of interpretability methods and his style of writing is at times entertaining and entirely", "tokens": [51504, 295, 7302, 2310, 7150, 293, 702, 3758, 295, 3579, 307, 412, 1413, 20402, 293, 7696, 51756], "temperature": 0.0, "avg_logprob": -0.0949485683441162, "compression_ratio": 1.6354166666666667, "no_speech_prob": 0.04683510586619377}, {"id": 151, "seek": 87400, "start": 874.0, "end": 880.32, "text": " absent of hype and nonsense. He runs the gamut of interpretability models so for example model", "tokens": [50364, 25185, 295, 24144, 293, 14925, 13, 634, 6676, 264, 8019, 325, 295, 7302, 2310, 5245, 370, 337, 1365, 2316, 50680], "temperature": 0.0, "avg_logprob": -0.10762666702270508, "compression_ratio": 1.7179487179487178, "no_speech_prob": 0.02816273272037506}, {"id": 152, "seek": 87400, "start": 880.32, "end": 885.52, "text": " agnostic methods like Lyme and Shapley values. Example-based methods such as counterfactual", "tokens": [50680, 623, 77, 19634, 7150, 411, 12687, 1398, 293, 44160, 3420, 4190, 13, 24755, 781, 12, 6032, 7150, 1270, 382, 5682, 44919, 901, 50940], "temperature": 0.0, "avg_logprob": -0.10762666702270508, "compression_ratio": 1.7179487179487178, "no_speech_prob": 0.02816273272037506}, {"id": 153, "seek": 87400, "start": 885.52, "end": 890.56, "text": " examples and adversarial examples he motivates the importance of interpretability methods", "tokens": [50940, 5110, 293, 17641, 44745, 5110, 415, 42569, 264, 7379, 295, 7302, 2310, 7150, 51192], "temperature": 0.0, "avg_logprob": -0.10762666702270508, "compression_ratio": 1.7179487179487178, "no_speech_prob": 0.02816273272037506}, {"id": 154, "seek": 87400, "start": 890.56, "end": 896.08, "text": " but he's also extremely transparent about its current weaknesses and pitfalls. He's currently", "tokens": [51192, 457, 415, 311, 611, 4664, 12737, 466, 1080, 2190, 24381, 293, 10147, 18542, 13, 634, 311, 4362, 51468], "temperature": 0.0, "avg_logprob": -0.10762666702270508, "compression_ratio": 1.7179487179487178, "no_speech_prob": 0.02816273272037506}, {"id": 155, "seek": 87400, "start": 896.08, "end": 901.92, "text": " finishing his PhD in interpretable machine learning at the Ludwig Maximilians University in Munich", "tokens": [51468, 12693, 702, 14476, 294, 7302, 712, 3479, 2539, 412, 264, 30550, 33313, 29076, 21738, 3535, 294, 40601, 51760], "temperature": 0.0, "avg_logprob": -0.10762666702270508, "compression_ratio": 1.7179487179487178, "no_speech_prob": 0.02816273272037506}, {"id": 156, "seek": 90192, "start": 901.92, "end": 906.56, "text": " after getting a stats master's from the same institution. He's recently written several", "tokens": [50364, 934, 1242, 257, 18152, 4505, 311, 490, 264, 912, 7818, 13, 634, 311, 3938, 3720, 2940, 50596], "temperature": 0.0, "avg_logprob": -0.08536526621604452, "compression_ratio": 1.8245033112582782, "no_speech_prob": 0.007716798223555088}, {"id": 157, "seek": 90192, "start": 906.56, "end": 911.04, "text": " very interesting papers on interpretable machine learning for example pitfalls to avoid when", "tokens": [50596, 588, 1880, 10577, 322, 7302, 712, 3479, 2539, 337, 1365, 10147, 18542, 281, 5042, 562, 50820], "temperature": 0.0, "avg_logprob": -0.08536526621604452, "compression_ratio": 1.8245033112582782, "no_speech_prob": 0.007716798223555088}, {"id": 158, "seek": 90192, "start": 911.04, "end": 916.88, "text": " interpreting machine learning models in July of 2020 where Christoph detailed several problematic", "tokens": [50820, 37395, 3479, 2539, 5245, 294, 7370, 295, 4808, 689, 2040, 5317, 9942, 2940, 19011, 51112], "temperature": 0.0, "avg_logprob": -0.08536526621604452, "compression_ratio": 1.8245033112582782, "no_speech_prob": 0.007716798223555088}, {"id": 159, "seek": 90192, "start": 916.88, "end": 921.76, "text": " model interpretations for example ignoring estimation uncertainty feature interactions", "tokens": [51112, 2316, 37547, 337, 1365, 26258, 35701, 15697, 4111, 13280, 51356], "temperature": 0.0, "avg_logprob": -0.08536526621604452, "compression_ratio": 1.8245033112582782, "no_speech_prob": 0.007716798223555088}, {"id": 160, "seek": 90192, "start": 921.76, "end": 926.64, "text": " or confusing correlations with dependence. More recently he published a paper called", "tokens": [51356, 420, 13181, 13983, 763, 365, 31704, 13, 5048, 3938, 415, 6572, 257, 3035, 1219, 51600], "temperature": 0.0, "avg_logprob": -0.08536526621604452, "compression_ratio": 1.8245033112582782, "no_speech_prob": 0.007716798223555088}, {"id": 161, "seek": 90192, "start": 926.64, "end": 931.5999999999999, "text": " interpretable machine learning a brief history state of the art and challenges while he acknowledged", "tokens": [51600, 7302, 712, 3479, 2539, 257, 5353, 2503, 1785, 295, 264, 1523, 293, 4759, 1339, 415, 27262, 51848], "temperature": 0.0, "avg_logprob": -0.08536526621604452, "compression_ratio": 1.8245033112582782, "no_speech_prob": 0.007716798223555088}, {"id": 162, "seek": 93160, "start": 931.6, "end": 936.48, "text": " that the field is maturing nicely. He also spoke about some of the serious challenges in IML methods", "tokens": [50364, 300, 264, 2519, 307, 3803, 1345, 9594, 13, 634, 611, 7179, 466, 512, 295, 264, 3156, 4759, 294, 286, 12683, 7150, 50608], "temperature": 0.0, "avg_logprob": -0.11364148911975679, "compression_ratio": 1.6686217008797655, "no_speech_prob": 0.00685117719694972}, {"id": 163, "seek": 93160, "start": 936.48, "end": 941.2, "text": " such as the lack of statistical uncertainty, shared information between features, lack of a", "tokens": [50608, 1270, 382, 264, 5011, 295, 22820, 15697, 11, 5507, 1589, 1296, 4122, 11, 5011, 295, 257, 50844], "temperature": 0.0, "avg_logprob": -0.11364148911975679, "compression_ratio": 1.6686217008797655, "no_speech_prob": 0.00685117719694972}, {"id": 164, "seek": 93160, "start": 941.2, "end": 946.16, "text": " clear definition of interpretability and the need for a more holistic view. Christoph Mulner", "tokens": [50844, 1850, 7123, 295, 7302, 2310, 293, 264, 643, 337, 257, 544, 30334, 1910, 13, 2040, 5317, 29960, 1193, 51092], "temperature": 0.0, "avg_logprob": -0.11364148911975679, "compression_ratio": 1.6686217008797655, "no_speech_prob": 0.00685117719694972}, {"id": 165, "seek": 93160, "start": 946.16, "end": 951.12, "text": " it's an absolute pleasure and welcome to the show. Thank you very much for the invitation glad to be here.", "tokens": [51092, 309, 311, 364, 8236, 6834, 293, 2928, 281, 264, 855, 13, 1044, 291, 588, 709, 337, 264, 17890, 5404, 281, 312, 510, 13, 51340], "temperature": 0.0, "avg_logprob": -0.11364148911975679, "compression_ratio": 1.6686217008797655, "no_speech_prob": 0.00685117719694972}, {"id": 166, "seek": 93160, "start": 951.76, "end": 956.0, "text": " You know Christoph I have to say I really enjoyed your book. I read this actually", "tokens": [51372, 509, 458, 2040, 5317, 286, 362, 281, 584, 286, 534, 4626, 428, 1446, 13, 286, 1401, 341, 767, 51584], "temperature": 0.0, "avg_logprob": -0.11364148911975679, "compression_ratio": 1.6686217008797655, "no_speech_prob": 0.00685117719694972}, {"id": 167, "seek": 93160, "start": 956.0, "end": 961.12, "text": " some months back in preparation for a completely different show. I loved how scientific it was", "tokens": [51584, 512, 2493, 646, 294, 13081, 337, 257, 2584, 819, 855, 13, 286, 4333, 577, 8134, 309, 390, 51840], "temperature": 0.0, "avg_logprob": -0.11364148911975679, "compression_ratio": 1.6686217008797655, "no_speech_prob": 0.00685117719694972}, {"id": 168, "seek": 96160, "start": 961.6800000000001, "end": 966.88, "text": " you know it was it was very much laying out essentially a survey of the facts a lay of the", "tokens": [50368, 291, 458, 309, 390, 309, 390, 588, 709, 14903, 484, 4476, 257, 8984, 295, 264, 9130, 257, 2360, 295, 264, 50628], "temperature": 0.0, "avg_logprob": -0.12457646869477772, "compression_ratio": 1.7537313432835822, "no_speech_prob": 0.0009692942840047181}, {"id": 169, "seek": 96160, "start": 966.88, "end": 972.8000000000001, "text": " land very objective evaluation. It had both the pros and cons you know of different approaches", "tokens": [50628, 2117, 588, 10024, 13344, 13, 467, 632, 1293, 264, 6267, 293, 1014, 291, 458, 295, 819, 11587, 50924], "temperature": 0.0, "avg_logprob": -0.12457646869477772, "compression_ratio": 1.7537313432835822, "no_speech_prob": 0.0009692942840047181}, {"id": 170, "seek": 96160, "start": 972.8000000000001, "end": 977.9200000000001, "text": " examples to make them you know more understandable so kudos to you. I thought it was a great book", "tokens": [50924, 5110, 281, 652, 552, 291, 458, 544, 25648, 370, 350, 35063, 281, 291, 13, 286, 1194, 309, 390, 257, 869, 1446, 51180], "temperature": 0.0, "avg_logprob": -0.12457646869477772, "compression_ratio": 1.7537313432835822, "no_speech_prob": 0.0009692942840047181}, {"id": 171, "seek": 96160, "start": 977.9200000000001, "end": 983.0400000000001, "text": " very enjoyable and very informative. I also loved how it lays out the beginning you know what the", "tokens": [51180, 588, 20305, 293, 588, 27759, 13, 286, 611, 4333, 577, 309, 32714, 484, 264, 2863, 291, 458, 437, 264, 51436], "temperature": 0.0, "avg_logprob": -0.12457646869477772, "compression_ratio": 1.7537313432835822, "no_speech_prob": 0.0009692942840047181}, {"id": 172, "seek": 96160, "start": 984.08, "end": 988.5600000000001, "text": " goals that we're trying to achieve with with interpretability are especially kind of the", "tokens": [51488, 5493, 300, 321, 434, 1382, 281, 4584, 365, 365, 7302, 2310, 366, 2318, 733, 295, 264, 51712], "temperature": 0.0, "avg_logprob": -0.12457646869477772, "compression_ratio": 1.7537313432835822, "no_speech_prob": 0.0009692942840047181}, {"id": 173, "seek": 98856, "start": 988.56, "end": 995.3599999999999, "text": " human goals right like what does it mean for an explanation to be good for people what kind of", "tokens": [50364, 1952, 5493, 558, 411, 437, 775, 309, 914, 337, 364, 10835, 281, 312, 665, 337, 561, 437, 733, 295, 50704], "temperature": 0.0, "avg_logprob": -0.09772675295910203, "compression_ratio": 1.8840579710144927, "no_speech_prob": 0.009857201017439365}, {"id": 174, "seek": 98856, "start": 995.3599999999999, "end": 1000.4, "text": " explanations do people like and sometimes there can be conflicting conflicting goals there and", "tokens": [50704, 28708, 360, 561, 411, 293, 2171, 456, 393, 312, 43784, 43784, 5493, 456, 293, 50956], "temperature": 0.0, "avg_logprob": -0.09772675295910203, "compression_ratio": 1.8840579710144927, "no_speech_prob": 0.009857201017439365}, {"id": 175, "seek": 98856, "start": 1000.4, "end": 1006.0799999999999, "text": " I think one thing that that I realized from reading your book is that that actually explanations can", "tokens": [50956, 286, 519, 472, 551, 300, 300, 286, 5334, 490, 3760, 428, 1446, 307, 300, 300, 767, 28708, 393, 51240], "temperature": 0.0, "avg_logprob": -0.09772675295910203, "compression_ratio": 1.8840579710144927, "no_speech_prob": 0.009857201017439365}, {"id": 176, "seek": 98856, "start": 1006.0799999999999, "end": 1013.8399999999999, "text": " be deceptively good yeah like I think I think the the the sort of cognitive bias maybe that we have", "tokens": [51240, 312, 368, 1336, 3413, 665, 1338, 411, 286, 519, 286, 519, 264, 264, 264, 1333, 295, 15605, 12577, 1310, 300, 321, 362, 51628], "temperature": 0.0, "avg_logprob": -0.09772675295910203, "compression_ratio": 1.8840579710144927, "no_speech_prob": 0.009857201017439365}, {"id": 177, "seek": 101384, "start": 1013.84, "end": 1021.2, "text": " to look for contrastive explanations or counterfactual explanations like and principle it seems good", "tokens": [50364, 281, 574, 337, 8712, 488, 28708, 420, 5682, 44919, 901, 28708, 411, 293, 8665, 309, 2544, 665, 50732], "temperature": 0.0, "avg_logprob": -0.07965645242909916, "compression_ratio": 1.9604743083003953, "no_speech_prob": 0.10517850518226624}, {"id": 178, "seek": 101384, "start": 1021.2, "end": 1028.56, "text": " it's kind of like you know I'm sorry we can't give you this loan you know well why not like why", "tokens": [50732, 309, 311, 733, 295, 411, 291, 458, 286, 478, 2597, 321, 393, 380, 976, 291, 341, 10529, 291, 458, 731, 983, 406, 411, 983, 51100], "temperature": 0.0, "avg_logprob": -0.07965645242909916, "compression_ratio": 1.9604743083003953, "no_speech_prob": 0.10517850518226624}, {"id": 179, "seek": 101384, "start": 1028.56, "end": 1033.1200000000001, "text": " can't you give this loan well well we've detected really that you're a that you're a deadbeat what", "tokens": [51100, 393, 380, 291, 976, 341, 10529, 731, 731, 321, 600, 21896, 534, 300, 291, 434, 257, 300, 291, 434, 257, 3116, 4169, 437, 51328], "temperature": 0.0, "avg_logprob": -0.07965645242909916, "compression_ratio": 1.9604743083003953, "no_speech_prob": 0.10517850518226624}, {"id": 180, "seek": 101384, "start": 1033.1200000000001, "end": 1037.6000000000001, "text": " do you mean I'm a deadbeat yeah you know you never pay your bills well let's see why okay let's look", "tokens": [51328, 360, 291, 914, 286, 478, 257, 3116, 4169, 1338, 291, 458, 291, 1128, 1689, 428, 12433, 731, 718, 311, 536, 983, 1392, 718, 311, 574, 51552], "temperature": 0.0, "avg_logprob": -0.07965645242909916, "compression_ratio": 1.9604743083003953, "no_speech_prob": 0.10517850518226624}, {"id": 181, "seek": 101384, "start": 1037.6000000000001, "end": 1042.16, "text": " through this and we find a decision tree here and and some big decision tree and we get to this one", "tokens": [51552, 807, 341, 293, 321, 915, 257, 3537, 4230, 510, 293, 293, 512, 955, 3537, 4230, 293, 321, 483, 281, 341, 472, 51780], "temperature": 0.0, "avg_logprob": -0.07965645242909916, "compression_ratio": 1.9604743083003953, "no_speech_prob": 0.10517850518226624}, {"id": 182, "seek": 104216, "start": 1042.16, "end": 1048.8000000000002, "text": " little point what says here you know you didn't pay this furniture bill back in 2018 you know if", "tokens": [50364, 707, 935, 437, 1619, 510, 291, 458, 291, 994, 380, 1689, 341, 15671, 2961, 646, 294, 6096, 291, 458, 498, 50696], "temperature": 0.0, "avg_logprob": -0.0711066722869873, "compression_ratio": 1.762962962962963, "no_speech_prob": 0.013218889012932777}, {"id": 183, "seek": 104216, "start": 1048.8000000000002, "end": 1053.6000000000001, "text": " if only you'd have paid that furniture bill like we'd be able to give you the loan right but the", "tokens": [50696, 498, 787, 291, 1116, 362, 4835, 300, 15671, 2961, 411, 321, 1116, 312, 1075, 281, 976, 291, 264, 10529, 558, 457, 264, 50936], "temperature": 0.0, "avg_logprob": -0.0711066722869873, "compression_ratio": 1.762962962962963, "no_speech_prob": 0.013218889012932777}, {"id": 184, "seek": 104216, "start": 1053.6000000000001, "end": 1058.3200000000002, "text": " truth isn't that simple like it's actually buried all throughout yeah throughout the decision tree", "tokens": [50936, 3494, 1943, 380, 300, 2199, 411, 309, 311, 767, 14101, 439, 3710, 1338, 3710, 264, 3537, 4230, 51172], "temperature": 0.0, "avg_logprob": -0.0711066722869873, "compression_ratio": 1.762962962962963, "no_speech_prob": 0.013218889012932777}, {"id": 185, "seek": 104216, "start": 1058.3200000000002, "end": 1064.3200000000002, "text": " right with so many contributing points yeah I think I like this chapter that you referenced", "tokens": [51172, 558, 365, 370, 867, 19270, 2793, 1338, 286, 519, 286, 411, 341, 7187, 300, 291, 32734, 51472], "temperature": 0.0, "avg_logprob": -0.0711066722869873, "compression_ratio": 1.762962962962963, "no_speech_prob": 0.013218889012932777}, {"id": 186, "seek": 104216, "start": 1064.3200000000002, "end": 1069.52, "text": " was about like kind of from the social view or the human view what what what people like or", "tokens": [51472, 390, 466, 411, 733, 295, 490, 264, 2093, 1910, 420, 264, 1952, 1910, 437, 437, 437, 561, 411, 420, 51732], "temperature": 0.0, "avg_logprob": -0.0711066722869873, "compression_ratio": 1.762962962962963, "no_speech_prob": 0.013218889012932777}, {"id": 187, "seek": 106952, "start": 1069.52, "end": 1075.36, "text": " prefer is explanations and the whole chapter is based on I forgot the title of the paper it's", "tokens": [50364, 4382, 307, 28708, 293, 264, 1379, 7187, 307, 2361, 322, 286, 5298, 264, 4876, 295, 264, 3035, 309, 311, 50656], "temperature": 0.0, "avg_logprob": -0.09122621895063042, "compression_ratio": 1.8870967741935485, "no_speech_prob": 0.007935570552945137}, {"id": 188, "seek": 106952, "start": 1075.36, "end": 1081.04, "text": " like from Miller about like what we can learn from the social sciences about what a good", "tokens": [50656, 411, 490, 16932, 466, 411, 437, 321, 393, 1466, 490, 264, 2093, 17677, 466, 437, 257, 665, 50940], "temperature": 0.0, "avg_logprob": -0.09122621895063042, "compression_ratio": 1.8870967741935485, "no_speech_prob": 0.007935570552945137}, {"id": 189, "seek": 106952, "start": 1081.04, "end": 1087.76, "text": " explanation is and was like a paper where I learned a lot and it was super interesting also to see", "tokens": [50940, 10835, 307, 293, 390, 411, 257, 3035, 689, 286, 3264, 257, 688, 293, 309, 390, 1687, 1880, 611, 281, 536, 51276], "temperature": 0.0, "avg_logprob": -0.09122621895063042, "compression_ratio": 1.8870967741935485, "no_speech_prob": 0.007935570552945137}, {"id": 190, "seek": 106952, "start": 1087.76, "end": 1091.84, "text": " how like what people think are good explanation as you mentioned they should be contrastive", "tokens": [51276, 577, 411, 437, 561, 519, 366, 665, 10835, 382, 291, 2835, 436, 820, 312, 8712, 488, 51480], "temperature": 0.0, "avg_logprob": -0.09122621895063042, "compression_ratio": 1.8870967741935485, "no_speech_prob": 0.007935570552945137}, {"id": 191, "seek": 106952, "start": 1091.84, "end": 1096.56, "text": " they should be short but they should also confirm to some prior knowledge that the people have", "tokens": [51480, 436, 820, 312, 2099, 457, 436, 820, 611, 9064, 281, 512, 4059, 3601, 300, 264, 561, 362, 51716], "temperature": 0.0, "avg_logprob": -0.09122621895063042, "compression_ratio": 1.8870967741935485, "no_speech_prob": 0.007935570552945137}, {"id": 192, "seek": 109656, "start": 1097.44, "end": 1102.32, "text": " and I mean like objectively a lot of those things might not like you wouldn't say these are good", "tokens": [50408, 293, 286, 914, 411, 46067, 257, 688, 295, 729, 721, 1062, 406, 411, 291, 2759, 380, 584, 613, 366, 665, 50652], "temperature": 0.0, "avg_logprob": -0.07413742277357313, "compression_ratio": 1.7894736842105263, "no_speech_prob": 0.007918884046375751}, {"id": 193, "seek": 109656, "start": 1102.32, "end": 1109.6799999999998, "text": " explanations in some sense like maybe maybe it's not good to give an explanation that fits with", "tokens": [50652, 28708, 294, 512, 2020, 411, 1310, 1310, 309, 311, 406, 665, 281, 976, 364, 10835, 300, 9001, 365, 51020], "temperature": 0.0, "avg_logprob": -0.07413742277357313, "compression_ratio": 1.7894736842105263, "no_speech_prob": 0.007918884046375751}, {"id": 194, "seek": 109656, "start": 1109.6799999999998, "end": 1116.8, "text": " a prior knowledge because it's not the correct one maybe so it was quite interesting to learn", "tokens": [51020, 257, 4059, 3601, 570, 309, 311, 406, 264, 3006, 472, 1310, 370, 309, 390, 1596, 1880, 281, 1466, 51376], "temperature": 0.0, "avg_logprob": -0.07413742277357313, "compression_ratio": 1.7894736842105263, "no_speech_prob": 0.007918884046375751}, {"id": 195, "seek": 109656, "start": 1116.8, "end": 1121.52, "text": " and to think about like what's the human side of it that's a very cool part of your book I thought", "tokens": [51376, 293, 281, 519, 466, 411, 437, 311, 264, 1952, 1252, 295, 309, 300, 311, 257, 588, 1627, 644, 295, 428, 1446, 286, 1194, 51612], "temperature": 0.0, "avg_logprob": -0.07413742277357313, "compression_ratio": 1.7894736842105263, "no_speech_prob": 0.007918884046375751}, {"id": 196, "seek": 109656, "start": 1121.52, "end": 1126.0, "text": " the fact that it's actually quite interesting thinking about what we really want out of an", "tokens": [51612, 264, 1186, 300, 309, 311, 767, 1596, 1880, 1953, 466, 437, 321, 534, 528, 484, 295, 364, 51836], "temperature": 0.0, "avg_logprob": -0.07413742277357313, "compression_ratio": 1.7894736842105263, "no_speech_prob": 0.007918884046375751}, {"id": 197, "seek": 112600, "start": 1126.0, "end": 1130.64, "text": " explanation I remember first of all looking at you know sharp values that are very fair and", "tokens": [50364, 10835, 286, 1604, 700, 295, 439, 1237, 412, 291, 458, 8199, 4190, 300, 366, 588, 3143, 293, 50596], "temperature": 0.0, "avg_logprob": -0.07029354449399967, "compression_ratio": 1.7633587786259541, "no_speech_prob": 0.005515535827726126}, {"id": 198, "seek": 112600, "start": 1130.64, "end": 1135.76, "text": " will distribute the blame equally amongst all the different relevant features and then you turn", "tokens": [50596, 486, 20594, 264, 10127, 12309, 12918, 439, 264, 819, 7340, 4122, 293, 550, 291, 1261, 50852], "temperature": 0.0, "avg_logprob": -0.07029354449399967, "compression_ratio": 1.7633587786259541, "no_speech_prob": 0.005515535827726126}, {"id": 199, "seek": 112600, "start": 1135.76, "end": 1141.36, "text": " to something else like you know selective interpretations that in a way are way less", "tokens": [50852, 281, 746, 1646, 411, 291, 458, 33930, 37547, 300, 294, 257, 636, 366, 636, 1570, 51132], "temperature": 0.0, "avg_logprob": -0.07029354449399967, "compression_ratio": 1.7633587786259541, "no_speech_prob": 0.005515535827726126}, {"id": 200, "seek": 112600, "start": 1141.36, "end": 1146.32, "text": " good because they're kind of arbitrary they'll just select a few a few a subset of the features", "tokens": [51132, 665, 570, 436, 434, 733, 295, 23211, 436, 603, 445, 3048, 257, 1326, 257, 1326, 257, 25993, 295, 264, 4122, 51380], "temperature": 0.0, "avg_logprob": -0.07029354449399967, "compression_ratio": 1.7633587786259541, "no_speech_prob": 0.005515535827726126}, {"id": 201, "seek": 112600, "start": 1146.32, "end": 1150.0, "text": " and give them all the blame but then it turns out that apparently that's what people actually", "tokens": [51380, 293, 976, 552, 439, 264, 10127, 457, 550, 309, 4523, 484, 300, 7970, 300, 311, 437, 561, 767, 51564], "temperature": 0.0, "avg_logprob": -0.07029354449399967, "compression_ratio": 1.7633587786259541, "no_speech_prob": 0.005515535827726126}, {"id": 202, "seek": 115000, "start": 1150.0, "end": 1156.16, "text": " want as a useful intuptable explanation yeah so as I see there's like many many dimensions of", "tokens": [50364, 528, 382, 257, 4420, 560, 84, 662, 712, 10835, 1338, 370, 382, 286, 536, 456, 311, 411, 867, 867, 12819, 295, 50672], "temperature": 0.0, "avg_logprob": -0.10820858737072313, "compression_ratio": 1.85, "no_speech_prob": 0.02031686156988144}, {"id": 203, "seek": 115000, "start": 1156.16, "end": 1163.68, "text": " explainability or like what what can be a good explanation and one of these dimensions is maybe", "tokens": [50672, 2903, 2310, 420, 411, 437, 437, 393, 312, 257, 665, 10835, 293, 472, 295, 613, 12819, 307, 1310, 51048], "temperature": 0.0, "avg_logprob": -0.10820858737072313, "compression_ratio": 1.85, "no_speech_prob": 0.02031686156988144}, {"id": 204, "seek": 115000, "start": 1163.68, "end": 1168.4, "text": " sparsity that you have a short explanation with just a few facts that's something that people", "tokens": [51048, 637, 685, 507, 300, 291, 362, 257, 2099, 10835, 365, 445, 257, 1326, 9130, 300, 311, 746, 300, 561, 51284], "temperature": 0.0, "avg_logprob": -0.10820858737072313, "compression_ratio": 1.85, "no_speech_prob": 0.02031686156988144}, {"id": 205, "seek": 115000, "start": 1168.4, "end": 1174.8, "text": " prefer maybe but this might be in conflict with other dimensions of a good explanation", "tokens": [51284, 4382, 1310, 457, 341, 1062, 312, 294, 6596, 365, 661, 12819, 295, 257, 665, 10835, 51604], "temperature": 0.0, "avg_logprob": -0.10820858737072313, "compression_ratio": 1.85, "no_speech_prob": 0.02031686156988144}, {"id": 206, "seek": 117480, "start": 1174.8799999999999, "end": 1180.0, "text": " which should be maybe that all causes should be addressed by the explanation that plays some", "tokens": [50368, 597, 820, 312, 1310, 300, 439, 7700, 820, 312, 13847, 538, 264, 10835, 300, 5749, 512, 50624], "temperature": 0.0, "avg_logprob": -0.10721498216901507, "compression_ratio": 1.7932330827067668, "no_speech_prob": 0.026329683139920235}, {"id": 207, "seek": 117480, "start": 1180.0, "end": 1186.24, "text": " role at least but this is of course in conflict with sparsity if you want this full attribution", "tokens": [50624, 3090, 412, 1935, 457, 341, 307, 295, 1164, 294, 6596, 365, 637, 685, 507, 498, 291, 528, 341, 1577, 9080, 1448, 50936], "temperature": 0.0, "avg_logprob": -0.10721498216901507, "compression_ratio": 1.7932330827067668, "no_speech_prob": 0.026329683139920235}, {"id": 208, "seek": 117480, "start": 1186.24, "end": 1192.8799999999999, "text": " like you get with shepley values for example so I that's why I also think there's not like just", "tokens": [50936, 411, 291, 483, 365, 402, 595, 3420, 4190, 337, 1365, 370, 286, 300, 311, 983, 286, 611, 519, 456, 311, 406, 411, 445, 51268], "temperature": 0.0, "avg_logprob": -0.10721498216901507, "compression_ratio": 1.7932330827067668, "no_speech_prob": 0.026329683139920235}, {"id": 209, "seek": 117480, "start": 1192.8799999999999, "end": 1198.32, "text": " one correct explanation but there's like many attributes or many dimensions on which you can", "tokens": [51268, 472, 3006, 10835, 457, 456, 311, 411, 867, 17212, 420, 867, 12819, 322, 597, 291, 393, 51540], "temperature": 0.0, "avg_logprob": -0.10721498216901507, "compression_ratio": 1.7932330827067668, "no_speech_prob": 0.026329683139920235}, {"id": 210, "seek": 117480, "start": 1198.32, "end": 1203.52, "text": " judge explanations yeah I think this is one of the problems because even machine learning is really", "tokens": [51540, 6995, 28708, 1338, 286, 519, 341, 307, 472, 295, 264, 2740, 570, 754, 3479, 2539, 307, 534, 51800], "temperature": 0.0, "avg_logprob": -0.10721498216901507, "compression_ratio": 1.7932330827067668, "no_speech_prob": 0.026329683139920235}, {"id": 211, "seek": 120352, "start": 1203.52, "end": 1208.6399999999999, "text": " difficult right because we use benchmarks and benchmarks are just something that people have", "tokens": [50364, 2252, 558, 570, 321, 764, 43751, 293, 43751, 366, 445, 746, 300, 561, 362, 50620], "temperature": 0.0, "avg_logprob": -0.06984730867239144, "compression_ratio": 1.769811320754717, "no_speech_prob": 0.004967004992067814}, {"id": 212, "seek": 120352, "start": 1208.6399999999999, "end": 1213.44, "text": " come up with but you say in your you know you talk about one of the problems being that there's no", "tokens": [50620, 808, 493, 365, 457, 291, 584, 294, 428, 291, 458, 291, 751, 466, 472, 295, 264, 2740, 885, 300, 456, 311, 572, 50860], "temperature": 0.0, "avg_logprob": -0.06984730867239144, "compression_ratio": 1.769811320754717, "no_speech_prob": 0.004967004992067814}, {"id": 213, "seek": 120352, "start": 1213.44, "end": 1218.8799999999999, "text": " definition of IML methods to start with but at least in machine learning methods we have ground", "tokens": [50860, 7123, 295, 286, 12683, 7150, 281, 722, 365, 457, 412, 1935, 294, 3479, 2539, 7150, 321, 362, 2727, 51132], "temperature": 0.0, "avg_logprob": -0.06984730867239144, "compression_ratio": 1.769811320754717, "no_speech_prob": 0.004967004992067814}, {"id": 214, "seek": 120352, "start": 1218.8799999999999, "end": 1226.16, "text": " truth which is which is significantly better in a way but if we if we can't quantify how good", "tokens": [51132, 3494, 597, 307, 597, 307, 10591, 1101, 294, 257, 636, 457, 498, 321, 498, 321, 393, 380, 40421, 577, 665, 51496], "temperature": 0.0, "avg_logprob": -0.06984730867239144, "compression_ratio": 1.769811320754717, "no_speech_prob": 0.004967004992067814}, {"id": 215, "seek": 120352, "start": 1226.16, "end": 1231.2, "text": " an explanation is then where are we really because you talk about a kind of taxonomy of", "tokens": [51496, 364, 10835, 307, 550, 689, 366, 321, 534, 570, 291, 751, 466, 257, 733, 295, 3366, 23423, 295, 51748], "temperature": 0.0, "avg_logprob": -0.06984730867239144, "compression_ratio": 1.769811320754717, "no_speech_prob": 0.004967004992067814}, {"id": 216, "seek": 123120, "start": 1231.2, "end": 1235.76, "text": " interpretability methods right you say that there are objective evaluations like sparsity and", "tokens": [50364, 7302, 2310, 7150, 558, 291, 584, 300, 456, 366, 10024, 43085, 411, 637, 685, 507, 293, 50592], "temperature": 0.0, "avg_logprob": -0.09201295448072029, "compression_ratio": 1.7279411764705883, "no_speech_prob": 0.010857480578124523}, {"id": 217, "seek": 123120, "start": 1235.76, "end": 1241.52, "text": " interaction strength and fidelity and humans human-centered evaluations you know which might", "tokens": [50592, 9285, 3800, 293, 46404, 293, 6255, 1952, 12, 36814, 43085, 291, 458, 597, 1062, 50880], "temperature": 0.0, "avg_logprob": -0.09201295448072029, "compression_ratio": 1.7279411764705883, "no_speech_prob": 0.010857480578124523}, {"id": 218, "seek": 123120, "start": 1241.52, "end": 1247.2, "text": " come from domain experts or lay people so I suppose you're just hitting straight on the", "tokens": [50880, 808, 490, 9274, 8572, 420, 2360, 561, 370, 286, 7297, 291, 434, 445, 8850, 2997, 322, 264, 51164], "temperature": 0.0, "avg_logprob": -0.09201295448072029, "compression_ratio": 1.7279411764705883, "no_speech_prob": 0.010857480578124523}, {"id": 219, "seek": 123120, "start": 1247.2, "end": 1253.52, "text": " fact that this is actually quite nebulous isn't it yeah so yeah so in some sense like there's this", "tokens": [51164, 1186, 300, 341, 307, 767, 1596, 408, 65, 6893, 1943, 380, 309, 1338, 370, 1338, 370, 294, 512, 2020, 411, 456, 311, 341, 51480], "temperature": 0.0, "avg_logprob": -0.09201295448072029, "compression_ratio": 1.7279411764705883, "no_speech_prob": 0.010857480578124523}, {"id": 220, "seek": 123120, "start": 1253.52, "end": 1259.04, "text": " big criticisms okay this is not scientific or not well defined at least what interpretability is", "tokens": [51480, 955, 48519, 1392, 341, 307, 406, 8134, 420, 406, 731, 7642, 412, 1935, 437, 7302, 2310, 307, 51756], "temperature": 0.0, "avg_logprob": -0.09201295448072029, "compression_ratio": 1.7279411764705883, "no_speech_prob": 0.010857480578124523}, {"id": 221, "seek": 125904, "start": 1259.04, "end": 1265.68, "text": " how can we even do research in this area but I have a bit more relaxed view I mean otherwise I", "tokens": [50364, 577, 393, 321, 754, 360, 2132, 294, 341, 1859, 457, 286, 362, 257, 857, 544, 14628, 1910, 286, 914, 5911, 286, 50696], "temperature": 0.0, "avg_logprob": -0.07972891190472771, "compression_ratio": 1.716279069767442, "no_speech_prob": 0.007120111491531134}, {"id": 222, "seek": 125904, "start": 1265.68, "end": 1270.56, "text": " should have stopped writing the book before I really started so I kind of see like this", "tokens": [50696, 820, 362, 5936, 3579, 264, 1446, 949, 286, 534, 1409, 370, 286, 733, 295, 536, 411, 341, 50940], "temperature": 0.0, "avg_logprob": -0.07972891190472771, "compression_ratio": 1.716279069767442, "no_speech_prob": 0.007120111491531134}, {"id": 223, "seek": 125904, "start": 1271.68, "end": 1277.2, "text": " endeavor of giving interpretability or bringing interpretability to machine learning it's more", "tokens": [50996, 34975, 295, 2902, 7302, 2310, 420, 5062, 7302, 2310, 281, 3479, 2539, 309, 311, 544, 51272], "temperature": 0.0, "avg_logprob": -0.07972891190472771, "compression_ratio": 1.716279069767442, "no_speech_prob": 0.007120111491531134}, {"id": 224, "seek": 125904, "start": 1277.2, "end": 1283.52, "text": " like a first of all it's just a keyword so it's it kind of bundles all the methods together", "tokens": [51272, 411, 257, 700, 295, 439, 309, 311, 445, 257, 20428, 370, 309, 311, 309, 733, 295, 13882, 904, 439, 264, 7150, 1214, 51588], "temperature": 0.0, "avg_logprob": -0.07972891190472771, "compression_ratio": 1.716279069767442, "no_speech_prob": 0.007120111491531134}, {"id": 225, "seek": 128352, "start": 1283.52, "end": 1291.12, "text": " that kind of aim to reduce this high-dimensional function to something well mostly it's something", "tokens": [50364, 300, 733, 295, 5939, 281, 5407, 341, 1090, 12, 18759, 2445, 281, 746, 731, 5240, 309, 311, 746, 50744], "temperature": 0.0, "avg_logprob": -0.0728494290555461, "compression_ratio": 1.8309859154929577, "no_speech_prob": 0.031136251986026764}, {"id": 226, "seek": 128352, "start": 1291.12, "end": 1296.08, "text": " in a lower dimension so we kind of just do this mapping something gets lost in a way this is fine", "tokens": [50744, 294, 257, 3126, 10139, 370, 321, 733, 295, 445, 360, 341, 18350, 746, 2170, 2731, 294, 257, 636, 341, 307, 2489, 50992], "temperature": 0.0, "avg_logprob": -0.0728494290555461, "compression_ratio": 1.8309859154929577, "no_speech_prob": 0.031136251986026764}, {"id": 227, "seek": 128352, "start": 1296.08, "end": 1302.24, "text": " and it's I think part of science to find out like or to yeah some part of analysis to find out what", "tokens": [50992, 293, 309, 311, 286, 519, 644, 295, 3497, 281, 915, 484, 411, 420, 281, 1338, 512, 644, 295, 5215, 281, 915, 484, 437, 51300], "temperature": 0.0, "avg_logprob": -0.0728494290555461, "compression_ratio": 1.8309859154929577, "no_speech_prob": 0.031136251986026764}, {"id": 228, "seek": 128352, "start": 1302.24, "end": 1309.84, "text": " part gets lost so when you for example look at just some feature importance values for example", "tokens": [51300, 644, 2170, 2731, 370, 562, 291, 337, 1365, 574, 412, 445, 512, 4111, 7379, 4190, 337, 1365, 51680], "temperature": 0.0, "avg_logprob": -0.0728494290555461, "compression_ratio": 1.8309859154929577, "no_speech_prob": 0.031136251986026764}, {"id": 229, "seek": 130984, "start": 1309.84, "end": 1318.8, "text": " of course it's a summary of your model and a lot of information gets lost but I still think", "tokens": [50364, 295, 1164, 309, 311, 257, 12691, 295, 428, 2316, 293, 257, 688, 295, 1589, 2170, 2731, 457, 286, 920, 519, 50812], "temperature": 0.0, "avg_logprob": -0.08542287917364211, "compression_ratio": 1.7169811320754718, "no_speech_prob": 0.0018365489086136222}, {"id": 230, "seek": 130984, "start": 1319.6, "end": 1323.6799999999998, "text": " it's useful to have obviously so many people use it but it's useful to have these", "tokens": [50852, 309, 311, 4420, 281, 362, 2745, 370, 867, 561, 764, 309, 457, 309, 311, 4420, 281, 362, 613, 51056], "temperature": 0.0, "avg_logprob": -0.08542287917364211, "compression_ratio": 1.7169811320754718, "no_speech_prob": 0.0018365489086136222}, {"id": 231, "seek": 130984, "start": 1324.3999999999999, "end": 1331.4399999999998, "text": " tools and we just have to understand what they do and how to interpret the results so how do", "tokens": [51092, 3873, 293, 321, 445, 362, 281, 1223, 437, 436, 360, 293, 577, 281, 7302, 264, 3542, 370, 577, 360, 51444], "temperature": 0.0, "avg_logprob": -0.08542287917364211, "compression_ratio": 1.7169811320754718, "no_speech_prob": 0.0018365489086136222}, {"id": 232, "seek": 130984, "start": 1331.4399999999998, "end": 1336.6399999999999, "text": " you interpret when like the feature importance is zero of a feature could that be quite dangerous", "tokens": [51444, 291, 7302, 562, 411, 264, 4111, 7379, 307, 4018, 295, 257, 4111, 727, 300, 312, 1596, 5795, 51704], "temperature": 0.0, "avg_logprob": -0.08542287917364211, "compression_ratio": 1.7169811320754718, "no_speech_prob": 0.0018365489086136222}, {"id": 233, "seek": 133664, "start": 1336.64, "end": 1341.3600000000001, "text": " though because this you gave the example of random forests when you have a lot of shared", "tokens": [50364, 1673, 570, 341, 291, 2729, 264, 1365, 295, 4974, 21700, 562, 291, 362, 257, 688, 295, 5507, 50600], "temperature": 0.0, "avg_logprob": -0.08966617391567037, "compression_ratio": 1.7744360902255638, "no_speech_prob": 0.034673061221838}, {"id": 234, "seek": 133664, "start": 1341.3600000000001, "end": 1347.3600000000001, "text": " information between the features it would actually tell you that these correlated features have a", "tokens": [50600, 1589, 1296, 264, 4122, 309, 576, 767, 980, 291, 300, 613, 38574, 4122, 362, 257, 50900], "temperature": 0.0, "avg_logprob": -0.08966617391567037, "compression_ratio": 1.7744360902255638, "no_speech_prob": 0.034673061221838}, {"id": 235, "seek": 133664, "start": 1347.3600000000001, "end": 1353.0400000000002, "text": " higher feature importance than you might otherwise expect so does this imply that we need to have", "tokens": [50900, 2946, 4111, 7379, 813, 291, 1062, 5911, 2066, 370, 775, 341, 33616, 300, 321, 643, 281, 362, 51184], "temperature": 0.0, "avg_logprob": -0.08966617391567037, "compression_ratio": 1.7744360902255638, "no_speech_prob": 0.034673061221838}, {"id": 236, "seek": 133664, "start": 1353.0400000000002, "end": 1357.68, "text": " very detailed knowledge of how we should how we should use this information that we get from", "tokens": [51184, 588, 9942, 3601, 295, 577, 321, 820, 577, 321, 820, 764, 341, 1589, 300, 321, 483, 490, 51416], "temperature": 0.0, "avg_logprob": -0.08966617391567037, "compression_ratio": 1.7744360902255638, "no_speech_prob": 0.034673061221838}, {"id": 237, "seek": 133664, "start": 1357.68, "end": 1363.2, "text": " IML methods yeah so it's also kind of the direction in which I write papers like this pitfalls", "tokens": [51416, 286, 12683, 7150, 1338, 370, 309, 311, 611, 733, 295, 264, 3513, 294, 597, 286, 2464, 10577, 411, 341, 10147, 18542, 51692], "temperature": 0.0, "avg_logprob": -0.08966617391567037, "compression_ratio": 1.7744360902255638, "no_speech_prob": 0.034673061221838}, {"id": 238, "seek": 136320, "start": 1364.0, "end": 1369.28, "text": " to avoid and stuff like this so I think so these are just tools so they do something with the", "tokens": [50404, 281, 5042, 293, 1507, 411, 341, 370, 286, 519, 370, 613, 366, 445, 3873, 370, 436, 360, 746, 365, 264, 50668], "temperature": 0.0, "avg_logprob": -0.09854166578538347, "compression_ratio": 1.8559670781893005, "no_speech_prob": 0.0027568789664655924}, {"id": 239, "seek": 136320, "start": 1369.28, "end": 1374.64, "text": " model a kind of distill some knowledge so for example for feature importance you kind of", "tokens": [50668, 2316, 257, 733, 295, 42923, 512, 3601, 370, 337, 1365, 337, 4111, 7379, 291, 733, 295, 50936], "temperature": 0.0, "avg_logprob": -0.09854166578538347, "compression_ratio": 1.8559670781893005, "no_speech_prob": 0.0027568789664655924}, {"id": 240, "seek": 136320, "start": 1376.16, "end": 1380.4, "text": " measure how well does your model perform and then you measure again after you", "tokens": [51012, 3481, 577, 731, 775, 428, 2316, 2042, 293, 550, 291, 3481, 797, 934, 291, 51224], "temperature": 0.0, "avg_logprob": -0.09854166578538347, "compression_ratio": 1.8559670781893005, "no_speech_prob": 0.0027568789664655924}, {"id": 241, "seek": 136320, "start": 1380.4, "end": 1385.44, "text": " shuffle one of the features and and then then you get something out of it so then we can ask", "tokens": [51224, 39426, 472, 295, 264, 4122, 293, 293, 550, 550, 291, 483, 746, 484, 295, 309, 370, 550, 321, 393, 1029, 51476], "temperature": 0.0, "avg_logprob": -0.09854166578538347, "compression_ratio": 1.8559670781893005, "no_speech_prob": 0.0027568789664655924}, {"id": 242, "seek": 136320, "start": 1385.44, "end": 1391.44, "text": " questions is this interpretable or not and it's kind of well not so relevant the question because", "tokens": [51476, 1651, 307, 341, 7302, 712, 420, 406, 293, 309, 311, 733, 295, 731, 406, 370, 7340, 264, 1168, 570, 51776], "temperature": 0.0, "avg_logprob": -0.09854166578538347, "compression_ratio": 1.8559670781893005, "no_speech_prob": 0.0027568789664655924}, {"id": 243, "seek": 139144, "start": 1391.44, "end": 1396.0800000000002, "text": " you just have to understand what what happens when you shuffle feature and one is for example", "tokens": [50364, 291, 445, 362, 281, 1223, 437, 437, 2314, 562, 291, 39426, 4111, 293, 472, 307, 337, 1365, 50596], "temperature": 0.0, "avg_logprob": -0.06364560436892819, "compression_ratio": 1.7887323943661972, "no_speech_prob": 0.0026307429652661085}, {"id": 244, "seek": 139144, "start": 1396.72, "end": 1404.8, "text": " you kind of break the association between the feature and the prediction because now it doesn't", "tokens": [50628, 291, 733, 295, 1821, 264, 14598, 1296, 264, 4111, 293, 264, 17630, 570, 586, 309, 1177, 380, 51032], "temperature": 0.0, "avg_logprob": -0.06364560436892819, "compression_ratio": 1.7887323943661972, "no_speech_prob": 0.0026307429652661085}, {"id": 245, "seek": 139144, "start": 1404.8, "end": 1409.1200000000001, "text": " carry the information about the target anymore because you're shuffled in randomly in your model", "tokens": [51032, 3985, 264, 1589, 466, 264, 3779, 3602, 570, 291, 434, 402, 33974, 294, 16979, 294, 428, 2316, 51248], "temperature": 0.0, "avg_logprob": -0.06364560436892819, "compression_ratio": 1.7887323943661972, "no_speech_prob": 0.0026307429652661085}, {"id": 246, "seek": 139144, "start": 1409.8400000000001, "end": 1415.3600000000001, "text": " so you kind of this this feature importance now measures how much performance you lose because", "tokens": [51284, 370, 291, 733, 295, 341, 341, 4111, 7379, 586, 8000, 577, 709, 3389, 291, 3624, 570, 51560], "temperature": 0.0, "avg_logprob": -0.06364560436892819, "compression_ratio": 1.7887323943661972, "no_speech_prob": 0.0026307429652661085}, {"id": 247, "seek": 141536, "start": 1415.4399999999998, "end": 1421.6799999999998, "text": " of this break of information but then you also when you think about this method and want to use", "tokens": [50368, 295, 341, 1821, 295, 1589, 457, 550, 291, 611, 562, 291, 519, 466, 341, 3170, 293, 528, 281, 764, 50680], "temperature": 0.0, "avg_logprob": -0.06651066807867254, "compression_ratio": 1.9618644067796611, "no_speech_prob": 0.028427042067050934}, {"id": 248, "seek": 141536, "start": 1421.6799999999998, "end": 1426.32, "text": " it you also have to understand that this shuffling for example breaks also association with your", "tokens": [50680, 309, 291, 611, 362, 281, 1223, 300, 341, 402, 1245, 1688, 337, 1365, 9857, 611, 14598, 365, 428, 50912], "temperature": 0.0, "avg_logprob": -0.06651066807867254, "compression_ratio": 1.9618644067796611, "no_speech_prob": 0.028427042067050934}, {"id": 249, "seek": 141536, "start": 1426.32, "end": 1431.12, "text": " other data feature like the features in your data so this is a limitation of the method and", "tokens": [50912, 661, 1412, 4111, 411, 264, 4122, 294, 428, 1412, 370, 341, 307, 257, 27432, 295, 264, 3170, 293, 51152], "temperature": 0.0, "avg_logprob": -0.06651066807867254, "compression_ratio": 1.9618644067796611, "no_speech_prob": 0.028427042067050934}, {"id": 250, "seek": 141536, "start": 1432.0, "end": 1438.4799999999998, "text": " what I think is needed is that we understand in which way these methods break or which", "tokens": [51196, 437, 286, 519, 307, 2978, 307, 300, 321, 1223, 294, 597, 636, 613, 7150, 1821, 420, 597, 51520], "temperature": 0.0, "avg_logprob": -0.06651066807867254, "compression_ratio": 1.9618644067796611, "no_speech_prob": 0.028427042067050934}, {"id": 251, "seek": 141536, "start": 1438.4799999999998, "end": 1443.6, "text": " scenarios we're allowed to use them or how we are allowed to interpret them and I think the", "tokens": [51520, 15077, 321, 434, 4350, 281, 764, 552, 420, 577, 321, 366, 4350, 281, 7302, 552, 293, 286, 519, 264, 51776], "temperature": 0.0, "avg_logprob": -0.06651066807867254, "compression_ratio": 1.9618644067796611, "no_speech_prob": 0.028427042067050934}, {"id": 252, "seek": 144360, "start": 1443.6, "end": 1449.76, "text": " situation is kind of similar to statistics where you have these models and and then you", "tokens": [50364, 2590, 307, 733, 295, 2531, 281, 12523, 689, 291, 362, 613, 5245, 293, 293, 550, 291, 50672], "temperature": 0.0, "avg_logprob": -0.06964439815945095, "compression_ratio": 1.9061224489795918, "no_speech_prob": 0.0009398787515237927}, {"id": 253, "seek": 144360, "start": 1449.76, "end": 1455.36, "text": " interpret like the coefficients of your models you still like have to learn how you do the", "tokens": [50672, 7302, 411, 264, 31994, 295, 428, 5245, 291, 920, 411, 362, 281, 1466, 577, 291, 360, 264, 50952], "temperature": 0.0, "avg_logprob": -0.06964439815945095, "compression_ratio": 1.9061224489795918, "no_speech_prob": 0.0009398787515237927}, {"id": 254, "seek": 144360, "start": 1455.36, "end": 1459.52, "text": " interpretation what are the assumptions that have to be met that you're allowed to do this", "tokens": [50952, 14174, 437, 366, 264, 17695, 300, 362, 281, 312, 1131, 300, 291, 434, 4350, 281, 360, 341, 51160], "temperature": 0.0, "avg_logprob": -0.06964439815945095, "compression_ratio": 1.9061224489795918, "no_speech_prob": 0.0009398787515237927}, {"id": 255, "seek": 144360, "start": 1459.52, "end": 1465.9199999999998, "text": " interpretation and I think it's we're in a similar situation here with interpretability of machine", "tokens": [51160, 14174, 293, 286, 519, 309, 311, 321, 434, 294, 257, 2531, 2590, 510, 365, 7302, 2310, 295, 3479, 51480], "temperature": 0.0, "avg_logprob": -0.06964439815945095, "compression_ratio": 1.9061224489795918, "no_speech_prob": 0.0009398787515237927}, {"id": 256, "seek": 144360, "start": 1465.9199999999998, "end": 1471.4399999999998, "text": " learning and I'm glad you mentioned sort of the old school linear models as well as dimensionality", "tokens": [51480, 2539, 293, 286, 478, 5404, 291, 2835, 1333, 295, 264, 1331, 1395, 8213, 5245, 382, 731, 382, 10139, 1860, 51756], "temperature": 0.0, "avg_logprob": -0.06964439815945095, "compression_ratio": 1.9061224489795918, "no_speech_prob": 0.0009398787515237927}, {"id": 257, "seek": 147144, "start": 1471.52, "end": 1477.2, "text": " in the thread because you make a very good point in the book which is look even these so-called", "tokens": [50368, 294, 264, 7207, 570, 291, 652, 257, 588, 665, 935, 294, 264, 1446, 597, 307, 574, 754, 613, 370, 12, 11880, 50652], "temperature": 0.0, "avg_logprob": -0.0789153552749782, "compression_ratio": 1.794776119402985, "no_speech_prob": 0.05496624484658241}, {"id": 258, "seek": 147144, "start": 1477.2, "end": 1482.72, "text": " intrinsically interpretable models are only interpretable up to a certain dimensionality", "tokens": [50652, 28621, 984, 7302, 712, 5245, 366, 787, 7302, 712, 493, 281, 257, 1629, 10139, 1860, 50928], "temperature": 0.0, "avg_logprob": -0.0789153552749782, "compression_ratio": 1.794776119402985, "no_speech_prob": 0.05496624484658241}, {"id": 259, "seek": 147144, "start": 1482.72, "end": 1488.0, "text": " and you know I have I have tons of experience with with multi-linear regression right and and I can", "tokens": [50928, 293, 291, 458, 286, 362, 286, 362, 9131, 295, 1752, 365, 365, 4825, 12, 28263, 24590, 558, 293, 293, 286, 393, 51192], "temperature": 0.0, "avg_logprob": -0.0789153552749782, "compression_ratio": 1.794776119402985, "no_speech_prob": 0.05496624484658241}, {"id": 260, "seek": 147144, "start": 1488.0, "end": 1494.16, "text": " guarantee that beyond a very small number of dimensions those coefficients are not interpretable", "tokens": [51192, 10815, 300, 4399, 257, 588, 1359, 1230, 295, 12819, 729, 31994, 366, 406, 7302, 712, 51500], "temperature": 0.0, "avg_logprob": -0.0789153552749782, "compression_ratio": 1.794776119402985, "no_speech_prob": 0.05496624484658241}, {"id": 261, "seek": 147144, "start": 1494.16, "end": 1499.04, "text": " because it starts to play a bunch of games where it's inflating one coefficient and another because", "tokens": [51500, 570, 309, 3719, 281, 862, 257, 3840, 295, 2813, 689, 309, 311, 9922, 990, 472, 17619, 293, 1071, 570, 51744], "temperature": 0.0, "avg_logprob": -0.0789153552749782, "compression_ratio": 1.794776119402985, "no_speech_prob": 0.05496624484658241}, {"id": 262, "seek": 149904, "start": 1499.04, "end": 1503.44, "text": " their difference is important and you know whatever else is happening a lot of correlated", "tokens": [50364, 641, 2649, 307, 1021, 293, 291, 458, 2035, 1646, 307, 2737, 257, 688, 295, 38574, 50584], "temperature": 0.0, "avg_logprob": -0.03740501016136107, "compression_ratio": 1.8225806451612903, "no_speech_prob": 0.002714582486078143}, {"id": 263, "seek": 149904, "start": 1503.44, "end": 1508.56, "text": " structures are all essentially getting compressed into the small number of small number of weights", "tokens": [50584, 9227, 366, 439, 4476, 1242, 30353, 666, 264, 1359, 1230, 295, 1359, 1230, 295, 17443, 50840], "temperature": 0.0, "avg_logprob": -0.03740501016136107, "compression_ratio": 1.8225806451612903, "no_speech_prob": 0.002714582486078143}, {"id": 264, "seek": 149904, "start": 1508.56, "end": 1514.08, "text": " right and so as the dimensionality goes up I would say like no model is is intrinsically", "tokens": [50840, 558, 293, 370, 382, 264, 10139, 1860, 1709, 493, 286, 576, 584, 411, 572, 2316, 307, 307, 28621, 984, 51116], "temperature": 0.0, "avg_logprob": -0.03740501016136107, "compression_ratio": 1.8225806451612903, "no_speech_prob": 0.002714582486078143}, {"id": 265, "seek": 149904, "start": 1514.08, "end": 1518.96, "text": " interpretable same can be said of decision trees like anybody who's looked at a decision tree that's", "tokens": [51116, 7302, 712, 912, 393, 312, 848, 295, 3537, 5852, 411, 4472, 567, 311, 2956, 412, 257, 3537, 4230, 300, 311, 51360], "temperature": 0.0, "avg_logprob": -0.03740501016136107, "compression_ratio": 1.8225806451612903, "no_speech_prob": 0.002714582486078143}, {"id": 266, "seek": 149904, "start": 1518.96, "end": 1523.52, "text": " come from real data you're going to find out it's not interpretable it's like oh look at this you", "tokens": [51360, 808, 490, 957, 1412, 291, 434, 516, 281, 915, 484, 309, 311, 406, 7302, 712, 309, 311, 411, 1954, 574, 412, 341, 291, 51588], "temperature": 0.0, "avg_logprob": -0.03740501016136107, "compression_ratio": 1.8225806451612903, "no_speech_prob": 0.002714582486078143}, {"id": 267, "seek": 149904, "start": 1523.52, "end": 1528.48, "text": " know market capitalization matters oh and it matters over here too and down here and and", "tokens": [51588, 458, 2142, 4238, 2144, 7001, 1954, 293, 309, 7001, 670, 510, 886, 293, 760, 510, 293, 293, 51836], "temperature": 0.0, "avg_logprob": -0.03740501016136107, "compression_ratio": 1.8225806451612903, "no_speech_prob": 0.002714582486078143}, {"id": 268, "seek": 152848, "start": 1528.48, "end": 1533.52, "text": " actually I have to go through five checks on market capitalization before I get down to this", "tokens": [50364, 767, 286, 362, 281, 352, 807, 1732, 13834, 322, 2142, 4238, 2144, 949, 286, 483, 760, 281, 341, 50616], "temperature": 0.0, "avg_logprob": -0.10929499116054801, "compression_ratio": 1.7339449541284404, "no_speech_prob": 0.0008295124862343073}, {"id": 269, "seek": 152848, "start": 1533.52, "end": 1539.1200000000001, "text": " decision and maybe the features aren't that intuitive either and then you have to kind of", "tokens": [50616, 3537, 293, 1310, 264, 4122, 3212, 380, 300, 21769, 2139, 293, 550, 291, 362, 281, 733, 295, 50896], "temperature": 0.0, "avg_logprob": -0.10929499116054801, "compression_ratio": 1.7339449541284404, "no_speech_prob": 0.0008295124862343073}, {"id": 270, "seek": 152848, "start": 1539.1200000000001, "end": 1546.4, "text": " like mentally stack up like until you get to the decision like five decisions and then we have a", "tokens": [50896, 411, 17072, 8630, 493, 411, 1826, 291, 483, 281, 264, 3537, 411, 1732, 5327, 293, 550, 321, 362, 257, 51260], "temperature": 0.0, "avg_logprob": -0.10929499116054801, "compression_ratio": 1.7339449541284404, "no_speech_prob": 0.0008295124862343073}, {"id": 271, "seek": 152848, "start": 1546.4, "end": 1556.56, "text": " very complex rule that led you to the prediction yeah so I agree that there's it's not like I mean", "tokens": [51260, 588, 3997, 4978, 300, 4684, 291, 281, 264, 17630, 1338, 370, 286, 3986, 300, 456, 311, 309, 311, 406, 411, 286, 914, 51768], "temperature": 0.0, "avg_logprob": -0.10929499116054801, "compression_ratio": 1.7339449541284404, "no_speech_prob": 0.0008295124862343073}, {"id": 272, "seek": 155656, "start": 1556.6399999999999, "end": 1560.8, "text": " I have I have this distinction the book like interpretable models and not so interpretable", "tokens": [50368, 286, 362, 286, 362, 341, 16844, 264, 1446, 411, 7302, 712, 5245, 293, 406, 370, 7302, 712, 50576], "temperature": 0.0, "avg_logprob": -0.09858069288621255, "compression_ratio": 1.8091603053435115, "no_speech_prob": 0.003883379977196455}, {"id": 273, "seek": 155656, "start": 1560.8, "end": 1567.6799999999998, "text": " models um but it's as you say it's like a gray like it's a scale really people could definitely", "tokens": [50576, 5245, 1105, 457, 309, 311, 382, 291, 584, 309, 311, 411, 257, 10855, 411, 309, 311, 257, 4373, 534, 561, 727, 2138, 50920], "temperature": 0.0, "avg_logprob": -0.09858069288621255, "compression_ratio": 1.8091603053435115, "no_speech_prob": 0.003883379977196455}, {"id": 274, "seek": 155656, "start": 1567.6799999999998, "end": 1573.12, "text": " overhype how interpretable these white box models are right whether it's linear models", "tokens": [50920, 670, 3495, 494, 577, 7302, 712, 613, 2418, 2424, 5245, 366, 558, 1968, 309, 311, 8213, 5245, 51192], "temperature": 0.0, "avg_logprob": -0.09858069288621255, "compression_ratio": 1.8091603053435115, "no_speech_prob": 0.003883379977196455}, {"id": 275, "seek": 155656, "start": 1573.12, "end": 1579.2, "text": " as I'm a I've worked with many physicists who uh have had guidelines that you should only ever use", "tokens": [51192, 382, 286, 478, 257, 286, 600, 2732, 365, 867, 48716, 567, 2232, 362, 632, 12470, 300, 291, 820, 787, 1562, 764, 51496], "temperature": 0.0, "avg_logprob": -0.09858069288621255, "compression_ratio": 1.8091603053435115, "no_speech_prob": 0.003883379977196455}, {"id": 276, "seek": 155656, "start": 1579.2, "end": 1584.8799999999999, "text": " models like a decision tree because it's possible in theory to write down on a piece of paper exactly", "tokens": [51496, 5245, 411, 257, 3537, 4230, 570, 309, 311, 1944, 294, 5261, 281, 2464, 760, 322, 257, 2522, 295, 3035, 2293, 51780], "temperature": 0.0, "avg_logprob": -0.09858069288621255, "compression_ratio": 1.8091603053435115, "no_speech_prob": 0.003883379977196455}, {"id": 277, "seek": 158488, "start": 1584.88, "end": 1590.24, "text": " how the decision is made right yeah you can trace every decision but that's never actually useful", "tokens": [50364, 577, 264, 3537, 307, 1027, 558, 1338, 291, 393, 13508, 633, 3537, 457, 300, 311, 1128, 767, 4420, 50632], "temperature": 0.0, "avg_logprob": -0.07960039714597306, "compression_ratio": 1.8565891472868217, "no_speech_prob": 0.0049803247675299644}, {"id": 278, "seek": 158488, "start": 1590.24, "end": 1595.44, "text": " in practice is it since when have you ever looked at a decision tree fitted to data there's any", "tokens": [50632, 294, 3124, 307, 309, 1670, 562, 362, 291, 1562, 2956, 412, 257, 3537, 4230, 26321, 281, 1412, 456, 311, 604, 50892], "temperature": 0.0, "avg_logprob": -0.07960039714597306, "compression_ratio": 1.8565891472868217, "no_speech_prob": 0.0049803247675299644}, {"id": 279, "seek": 158488, "start": 1595.44, "end": 1600.24, "text": " complexity and the fact that in theory it's possible to go and examine how it works yeah it's", "tokens": [50892, 14024, 293, 264, 1186, 300, 294, 5261, 309, 311, 1944, 281, 352, 293, 17496, 577, 309, 1985, 1338, 309, 311, 51132], "temperature": 0.0, "avg_logprob": -0.07960039714597306, "compression_ratio": 1.8565891472868217, "no_speech_prob": 0.0049803247675299644}, {"id": 280, "seek": 158488, "start": 1600.24, "end": 1604.5600000000002, "text": " completely irrelevant in practice isn't it yeah I mean it can be useful to have like a short decision", "tokens": [51132, 2584, 28682, 294, 3124, 1943, 380, 309, 1338, 286, 914, 309, 393, 312, 4420, 281, 362, 411, 257, 2099, 3537, 51348], "temperature": 0.0, "avg_logprob": -0.07960039714597306, "compression_ratio": 1.8565891472868217, "no_speech_prob": 0.0049803247675299644}, {"id": 281, "seek": 158488, "start": 1604.5600000000002, "end": 1609.7600000000002, "text": " tree sometimes it but in practice it will not like give you probably the best predictions", "tokens": [51348, 4230, 2171, 309, 457, 294, 3124, 309, 486, 406, 411, 976, 291, 1391, 264, 1151, 21264, 51608], "temperature": 0.0, "avg_logprob": -0.07960039714597306, "compression_ratio": 1.8565891472868217, "no_speech_prob": 0.0049803247675299644}, {"id": 282, "seek": 160976, "start": 1610.72, "end": 1614.64, "text": " um but it might be useful sometimes to shorten it artificially so even like", "tokens": [50412, 1105, 457, 309, 1062, 312, 4420, 2171, 281, 39632, 309, 39905, 2270, 370, 754, 411, 50608], "temperature": 0.0, "avg_logprob": -0.06300173423908374, "compression_ratio": 1.7876447876447876, "no_speech_prob": 0.0057298908941447735}, {"id": 283, "seek": 160976, "start": 1614.64, "end": 1619.68, "text": " you're throwing away some some predictive accuracy um but you shorten it so you understand", "tokens": [50608, 291, 434, 10238, 1314, 512, 512, 35521, 14170, 1105, 457, 291, 39632, 309, 370, 291, 1223, 50860], "temperature": 0.0, "avg_logprob": -0.06300173423908374, "compression_ratio": 1.7876447876447876, "no_speech_prob": 0.0057298908941447735}, {"id": 284, "seek": 160976, "start": 1619.68, "end": 1626.24, "text": " that somehow it's manageable you can have a look at it and and see what's going on well there's also", "tokens": [50860, 300, 6063, 309, 311, 38798, 291, 393, 362, 257, 574, 412, 309, 293, 293, 536, 437, 311, 516, 322, 731, 456, 311, 611, 51188], "temperature": 0.0, "avg_logprob": -0.06300173423908374, "compression_ratio": 1.7876447876447876, "no_speech_prob": 0.0057298908941447735}, {"id": 285, "seek": 160976, "start": 1626.24, "end": 1630.8, "text": " you know the other issue is that as I was looking through a lot of the methods that you describe", "tokens": [51188, 291, 458, 264, 661, 2734, 307, 300, 382, 286, 390, 1237, 807, 257, 688, 295, 264, 7150, 300, 291, 6786, 51416], "temperature": 0.0, "avg_logprob": -0.06300173423908374, "compression_ratio": 1.7876447876447876, "no_speech_prob": 0.0057298908941447735}, {"id": 286, "seek": 160976, "start": 1630.8, "end": 1636.72, "text": " and you survey in your book you know some of them um are not simple I mean if you start looking at", "tokens": [51416, 293, 291, 8984, 294, 428, 1446, 291, 458, 512, 295, 552, 1105, 366, 406, 2199, 286, 914, 498, 291, 722, 1237, 412, 51712], "temperature": 0.0, "avg_logprob": -0.06300173423908374, "compression_ratio": 1.7876447876447876, "no_speech_prob": 0.0057298908941447735}, {"id": 287, "seek": 163672, "start": 1636.72, "end": 1642.56, "text": " partial dependency plots and trying to explain what those are I mean you know you have to almost", "tokens": [50364, 14641, 33621, 28609, 293, 1382, 281, 2903, 437, 729, 366, 286, 914, 291, 458, 291, 362, 281, 1920, 50656], "temperature": 0.0, "avg_logprob": -0.05645767693380708, "compression_ratio": 1.8164794007490637, "no_speech_prob": 0.019119257107377052}, {"id": 288, "seek": 163672, "start": 1642.56, "end": 1647.3600000000001, "text": " have a deep mathematical knowledge really to appreciate them in the first place so I'm wondering", "tokens": [50656, 362, 257, 2452, 18894, 3601, 534, 281, 4449, 552, 294, 264, 700, 1081, 370, 286, 478, 6359, 50896], "temperature": 0.0, "avg_logprob": -0.05645767693380708, "compression_ratio": 1.8164794007490637, "no_speech_prob": 0.019119257107377052}, {"id": 289, "seek": 163672, "start": 1647.3600000000001, "end": 1654.56, "text": " at what point we're just developing complex math models to explain complex math models and we really", "tokens": [50896, 412, 437, 935, 321, 434, 445, 6416, 3997, 5221, 5245, 281, 2903, 3997, 5221, 5245, 293, 321, 534, 51256], "temperature": 0.0, "avg_logprob": -0.05645767693380708, "compression_ratio": 1.8164794007490637, "no_speech_prob": 0.019119257107377052}, {"id": 290, "seek": 163672, "start": 1654.56, "end": 1660.72, "text": " haven't you know made much progress along the interpretability axis yeah yeah it's true it's", "tokens": [51256, 2378, 380, 291, 458, 1027, 709, 4205, 2051, 264, 7302, 2310, 10298, 1338, 1338, 309, 311, 2074, 309, 311, 51564], "temperature": 0.0, "avg_logprob": -0.05645767693380708, "compression_ratio": 1.8164794007490637, "no_speech_prob": 0.019119257107377052}, {"id": 291, "seek": 163672, "start": 1660.72, "end": 1665.44, "text": " also like the criticism too like um so you have something you don't understand and you explain it", "tokens": [51564, 611, 411, 264, 15835, 886, 411, 1105, 370, 291, 362, 746, 291, 500, 380, 1223, 293, 291, 2903, 309, 51800], "temperature": 0.0, "avg_logprob": -0.05645767693380708, "compression_ratio": 1.8164794007490637, "no_speech_prob": 0.019119257107377052}, {"id": 292, "seek": 166544, "start": 1665.44, "end": 1672.64, "text": " with something you don't understand um I think some methods are complex um but for some at least", "tokens": [50364, 365, 746, 291, 500, 380, 1223, 1105, 286, 519, 512, 7150, 366, 3997, 1105, 457, 337, 512, 412, 1935, 50724], "temperature": 0.0, "avg_logprob": -0.07796580974872296, "compression_ratio": 1.9047619047619047, "no_speech_prob": 0.004069716669619083}, {"id": 293, "seek": 166544, "start": 1672.64, "end": 1678.8, "text": " there's some intuition how they work for partial dependence plot is kind of your this um intuition", "tokens": [50724, 456, 311, 512, 24002, 577, 436, 589, 337, 14641, 31704, 7542, 307, 733, 295, 428, 341, 1105, 24002, 51032], "temperature": 0.0, "avg_logprob": -0.07796580974872296, "compression_ratio": 1.9047619047619047, "no_speech_prob": 0.004069716669619083}, {"id": 294, "seek": 166544, "start": 1678.8, "end": 1683.76, "text": " that you do some intervention on your more or intervention on your data so you replace all your", "tokens": [51032, 300, 291, 360, 512, 13176, 322, 428, 544, 420, 13176, 322, 428, 1412, 370, 291, 7406, 439, 428, 51280], "temperature": 0.0, "avg_logprob": -0.07796580974872296, "compression_ratio": 1.9047619047619047, "no_speech_prob": 0.004069716669619083}, {"id": 295, "seek": 166544, "start": 1684.3200000000002, "end": 1690.0800000000002, "text": " like for one feature you replace all the values with one fixed value and kind of look at the", "tokens": [51308, 411, 337, 472, 4111, 291, 7406, 439, 264, 4190, 365, 472, 6806, 2158, 293, 733, 295, 574, 412, 264, 51596], "temperature": 0.0, "avg_logprob": -0.07796580974872296, "compression_ratio": 1.9047619047619047, "no_speech_prob": 0.004069716669619083}, {"id": 296, "seek": 166544, "start": 1690.0800000000002, "end": 1694.8, "text": " average prediction that you get afterwards and do this for a lot of points and then you connect", "tokens": [51596, 4274, 17630, 300, 291, 483, 10543, 293, 360, 341, 337, 257, 688, 295, 2793, 293, 550, 291, 1745, 51832], "temperature": 0.0, "avg_logprob": -0.07796580974872296, "compression_ratio": 1.9047619047619047, "no_speech_prob": 0.004069716669619083}, {"id": 297, "seek": 169480, "start": 1694.8, "end": 1700.8799999999999, "text": " the points and you have this curve so kind of gives gives you the expected change over the feature", "tokens": [50364, 264, 2793, 293, 291, 362, 341, 7605, 370, 733, 295, 2709, 2709, 291, 264, 5176, 1319, 670, 264, 4111, 50668], "temperature": 0.0, "avg_logprob": -0.07595804981563402, "compression_ratio": 1.6767241379310345, "no_speech_prob": 0.008033683523535728}, {"id": 298, "seek": 169480, "start": 1700.8799999999999, "end": 1707.12, "text": " range maybe there was already a bit complex I don't know um maybe I'm too deep into the method", "tokens": [50668, 3613, 1310, 456, 390, 1217, 257, 857, 3997, 286, 500, 380, 458, 1105, 1310, 286, 478, 886, 2452, 666, 264, 3170, 50980], "temperature": 0.0, "avg_logprob": -0.07595804981563402, "compression_ratio": 1.6767241379310345, "no_speech_prob": 0.008033683523535728}, {"id": 299, "seek": 169480, "start": 1707.12, "end": 1714.56, "text": " already um but yeah of course it's it's something additional people have to learn or if they agree", "tokens": [50980, 1217, 1105, 457, 1338, 295, 1164, 309, 311, 309, 311, 746, 4497, 561, 362, 281, 1466, 420, 498, 436, 3986, 51352], "temperature": 0.0, "avg_logprob": -0.07595804981563402, "compression_ratio": 1.6767241379310345, "no_speech_prob": 0.008033683523535728}, {"id": 300, "seek": 169480, "start": 1714.56, "end": 1721.44, "text": " to use it of course could I get a quick take from you on saliency maps as an example because you", "tokens": [51352, 281, 764, 309, 295, 1164, 727, 286, 483, 257, 1702, 747, 490, 291, 322, 1845, 7848, 11317, 382, 364, 1365, 570, 291, 51696], "temperature": 0.0, "avg_logprob": -0.07595804981563402, "compression_ratio": 1.6767241379310345, "no_speech_prob": 0.008033683523535728}, {"id": 301, "seek": 172144, "start": 1721.44, "end": 1726.56, "text": " said in one of your youtube videos that saliency maps are glorified edge detectors they are not", "tokens": [50364, 848, 294, 472, 295, 428, 12487, 2145, 300, 1845, 7848, 11317, 366, 26623, 2587, 4691, 46866, 436, 366, 406, 50620], "temperature": 0.0, "avg_logprob": -0.06849917295937226, "compression_ratio": 1.7591240875912408, "no_speech_prob": 0.03829231113195419}, {"id": 302, "seek": 172144, "start": 1726.56, "end": 1732.3200000000002, "text": " good explanation at all and I've noticed now that many machine learning platform providers are building", "tokens": [50620, 665, 10835, 412, 439, 293, 286, 600, 5694, 586, 300, 867, 3479, 2539, 3663, 11330, 366, 2390, 50908], "temperature": 0.0, "avg_logprob": -0.06849917295937226, "compression_ratio": 1.7591240875912408, "no_speech_prob": 0.03829231113195419}, {"id": 303, "seek": 172144, "start": 1732.3200000000002, "end": 1737.76, "text": " these kind of um saliency maps into their models you know into their platforms and then it becomes a", "tokens": [50908, 613, 733, 295, 1105, 1845, 7848, 11317, 666, 641, 5245, 291, 458, 666, 641, 9473, 293, 550, 309, 3643, 257, 51180], "temperature": 0.0, "avg_logprob": -0.06849917295937226, "compression_ratio": 1.7591240875912408, "no_speech_prob": 0.03829231113195419}, {"id": 304, "seek": 172144, "start": 1737.76, "end": 1742.3200000000002, "text": " kind of box ticking exercise where you can say okay well yeah we've done interoperability now", "tokens": [51180, 733, 295, 2424, 33999, 5380, 689, 291, 393, 584, 1392, 731, 1338, 321, 600, 1096, 728, 7192, 2310, 586, 51408], "temperature": 0.0, "avg_logprob": -0.06849917295937226, "compression_ratio": 1.7591240875912408, "no_speech_prob": 0.03829231113195419}, {"id": 305, "seek": 172144, "start": 1742.3200000000002, "end": 1747.3600000000001, "text": " that's all you need to know and that really is quite a false sense of security isn't it", "tokens": [51408, 300, 311, 439, 291, 643, 281, 458, 293, 300, 534, 307, 1596, 257, 7908, 2020, 295, 3825, 1943, 380, 309, 51660], "temperature": 0.0, "avg_logprob": -0.06849917295937226, "compression_ratio": 1.7591240875912408, "no_speech_prob": 0.03829231113195419}, {"id": 306, "seek": 174736, "start": 1747.36, "end": 1752.8799999999999, "text": " it's funny you mentioned the saliency maps because I'm writing a book chapter about it and", "tokens": [50364, 309, 311, 4074, 291, 2835, 264, 1845, 7848, 11317, 570, 286, 478, 3579, 257, 1446, 7187, 466, 309, 293, 50640], "temperature": 0.0, "avg_logprob": -0.08899078369140626, "compression_ratio": 1.71875, "no_speech_prob": 0.013845542445778847}, {"id": 307, "seek": 174736, "start": 1752.8799999999999, "end": 1759.28, "text": " actually I'm I wanted to publish it today maybe I will or at least in the next few days um it has", "tokens": [50640, 767, 286, 478, 286, 1415, 281, 11374, 309, 965, 1310, 286, 486, 420, 412, 1935, 294, 264, 958, 1326, 1708, 1105, 309, 575, 50960], "temperature": 0.0, "avg_logprob": -0.08899078369140626, "compression_ratio": 1.71875, "no_speech_prob": 0.013845542445778847}, {"id": 308, "seek": 174736, "start": 1759.28, "end": 1763.84, "text": " been a long time in the making and it was very very frustrating like by far the most frustrating", "tokens": [50960, 668, 257, 938, 565, 294, 264, 1455, 293, 309, 390, 588, 588, 16522, 411, 538, 1400, 264, 881, 16522, 51188], "temperature": 0.0, "avg_logprob": -0.08899078369140626, "compression_ratio": 1.71875, "no_speech_prob": 0.013845542445778847}, {"id": 309, "seek": 174736, "start": 1763.84, "end": 1770.1599999999999, "text": " chapter to write um number one reason is because there's so many methods out there uh reason number", "tokens": [51188, 7187, 281, 2464, 1105, 1230, 472, 1778, 307, 570, 456, 311, 370, 867, 7150, 484, 456, 2232, 1778, 1230, 51504], "temperature": 0.0, "avg_logprob": -0.08899078369140626, "compression_ratio": 1.71875, "no_speech_prob": 0.013845542445778847}, {"id": 310, "seek": 177016, "start": 1770.16, "end": 1779.0400000000002, "text": " two is I I can't judge really or if they work and it seems like they mostly don't or it's it's", "tokens": [50364, 732, 307, 286, 286, 393, 380, 6995, 534, 420, 498, 436, 589, 293, 309, 2544, 411, 436, 5240, 500, 380, 420, 309, 311, 309, 311, 50808], "temperature": 0.0, "avg_logprob": -0.12868414754452911, "compression_ratio": 1.6946902654867257, "no_speech_prob": 0.13654674589633942}, {"id": 311, "seek": 177016, "start": 1779.0400000000002, "end": 1784.8000000000002, "text": " still unclear like how you say you would judge that they work so they're like dozens of these", "tokens": [50808, 920, 25636, 411, 577, 291, 584, 291, 576, 6995, 300, 436, 589, 370, 436, 434, 411, 18431, 295, 613, 51096], "temperature": 0.0, "avg_logprob": -0.12868414754452911, "compression_ratio": 1.6946902654867257, "no_speech_prob": 0.13654674589633942}, {"id": 312, "seek": 177016, "start": 1784.8000000000002, "end": 1791.3600000000001, "text": " like integrated gradients gradients deconfinate deep tailor decomposition layer-wise relevance", "tokens": [51096, 411, 10919, 2771, 2448, 2771, 2448, 979, 266, 5194, 473, 2452, 33068, 48356, 4583, 12, 3711, 32684, 51424], "temperature": 0.0, "avg_logprob": -0.12868414754452911, "compression_ratio": 1.6946902654867257, "no_speech_prob": 0.13654674589633942}, {"id": 313, "seek": 177016, "start": 1791.3600000000001, "end": 1799.0400000000002, "text": " propagation in 10 variants um so and then I mean you in the end you when you apply these methods on", "tokens": [51424, 38377, 294, 1266, 21669, 1105, 370, 293, 550, 286, 914, 291, 294, 264, 917, 291, 562, 291, 3079, 613, 7150, 322, 51808], "temperature": 0.0, "avg_logprob": -0.12868414754452911, "compression_ratio": 1.6946902654867257, "no_speech_prob": 0.13654674589633942}, {"id": 314, "seek": 179904, "start": 1799.04, "end": 1803.44, "text": " they are also like for image classification and you get these nice-looking images and some areas", "tokens": [50364, 436, 366, 611, 411, 337, 3256, 21538, 293, 291, 483, 613, 1481, 12, 16129, 5267, 293, 512, 3179, 50584], "temperature": 0.0, "avg_logprob": -0.09687747430363926, "compression_ratio": 1.818867924528302, "no_speech_prob": 0.005468514282256365}, {"id": 315, "seek": 179904, "start": 1803.44, "end": 1808.96, "text": " are highlighted some or not sometimes you can say okay this doesn't make sense at all um but if", "tokens": [50584, 366, 17173, 512, 420, 406, 2171, 291, 393, 584, 1392, 341, 1177, 380, 652, 2020, 412, 439, 1105, 457, 498, 50860], "temperature": 0.0, "avg_logprob": -0.09687747430363926, "compression_ratio": 1.818867924528302, "no_speech_prob": 0.005468514282256365}, {"id": 316, "seek": 179904, "start": 1809.84, "end": 1815.6, "text": " if it kind of makes sense then you maybe would be inclined to trust the method um but then there's", "tokens": [50904, 498, 309, 733, 295, 1669, 2020, 550, 291, 1310, 576, 312, 28173, 281, 3361, 264, 3170, 1105, 457, 550, 456, 311, 51192], "temperature": 0.0, "avg_logprob": -0.09687747430363926, "compression_ratio": 1.818867924528302, "no_speech_prob": 0.005468514282256365}, {"id": 317, "seek": 179904, "start": 1815.6, "end": 1821.44, "text": " this uh paper which is called um sanity checks for saliency maps and they kind of found out that", "tokens": [51192, 341, 2232, 3035, 597, 307, 1219, 1105, 47892, 13834, 337, 1845, 7848, 11317, 293, 436, 733, 295, 1352, 484, 300, 51484], "temperature": 0.0, "avg_logprob": -0.09687747430363926, "compression_ratio": 1.818867924528302, "no_speech_prob": 0.005468514282256365}, {"id": 318, "seek": 179904, "start": 1821.44, "end": 1828.72, "text": " they the most of the methods are very similar to edge detectors meaning that they are kind of", "tokens": [51484, 436, 264, 881, 295, 264, 7150, 366, 588, 2531, 281, 4691, 46866, 3620, 300, 436, 366, 733, 295, 51848], "temperature": 0.0, "avg_logprob": -0.09687747430363926, "compression_ratio": 1.818867924528302, "no_speech_prob": 0.005468514282256365}, {"id": 319, "seek": 182872, "start": 1828.72, "end": 1834.96, "text": " insensitive to the model and the data which is very bad of course well if you change the model um", "tokens": [50364, 1028, 34465, 281, 264, 2316, 293, 264, 1412, 597, 307, 588, 1578, 295, 1164, 731, 498, 291, 1319, 264, 2316, 1105, 50676], "temperature": 0.0, "avg_logprob": -0.06131404240926107, "compression_ratio": 1.7716894977168949, "no_speech_prob": 0.00029116275254637003}, {"id": 320, "seek": 182872, "start": 1834.96, "end": 1841.3600000000001, "text": " the explanation should obviously change um could could you expand on that a little bit so you said", "tokens": [50676, 264, 10835, 820, 2745, 1319, 1105, 727, 727, 291, 5268, 322, 300, 257, 707, 857, 370, 291, 848, 50996], "temperature": 0.0, "avg_logprob": -0.06131404240926107, "compression_ratio": 1.7716894977168949, "no_speech_prob": 0.00029116275254637003}, {"id": 321, "seek": 182872, "start": 1841.3600000000001, "end": 1847.04, "text": " it wasn't really reflection of the model or the data but what what would a perfect saliency map", "tokens": [50996, 309, 2067, 380, 534, 12914, 295, 264, 2316, 420, 264, 1412, 457, 437, 437, 576, 257, 2176, 1845, 7848, 4471, 51280], "temperature": 0.0, "avg_logprob": -0.06131404240926107, "compression_ratio": 1.7716894977168949, "no_speech_prob": 0.00029116275254637003}, {"id": 322, "seek": 182872, "start": 1847.04, "end": 1854.24, "text": " look like well I don't know myself actually so I mean the the ideas that you saw the basic idea", "tokens": [51280, 574, 411, 731, 286, 500, 380, 458, 2059, 767, 370, 286, 914, 264, 264, 3487, 300, 291, 1866, 264, 3875, 1558, 51640], "temperature": 0.0, "avg_logprob": -0.06131404240926107, "compression_ratio": 1.7716894977168949, "no_speech_prob": 0.00029116275254637003}, {"id": 323, "seek": 185424, "start": 1854.24, "end": 1859.36, "text": " of most of these methods is that you you have your class prediction or your class score and you", "tokens": [50364, 295, 881, 295, 613, 7150, 307, 300, 291, 291, 362, 428, 1508, 17630, 420, 428, 1508, 6175, 293, 291, 50620], "temperature": 0.0, "avg_logprob": -0.07765630158511075, "compression_ratio": 1.8620689655172413, "no_speech_prob": 0.009124389849603176}, {"id": 324, "seek": 185424, "start": 1859.36, "end": 1866.56, "text": " want to back propagate it not you want to back propagate it to the original image so you look", "tokens": [50620, 528, 281, 646, 48256, 309, 406, 291, 528, 281, 646, 48256, 309, 281, 264, 3380, 3256, 370, 291, 574, 50980], "temperature": 0.0, "avg_logprob": -0.07765630158511075, "compression_ratio": 1.8620689655172413, "no_speech_prob": 0.009124389849603176}, {"id": 325, "seek": 185424, "start": 1866.56, "end": 1873.1200000000001, "text": " at the gradient um with respect to your input pixels and that's there's no not not one way to", "tokens": [50980, 412, 264, 16235, 1105, 365, 3104, 281, 428, 4846, 18668, 293, 300, 311, 456, 311, 572, 406, 406, 472, 636, 281, 51308], "temperature": 0.0, "avg_logprob": -0.07765630158511075, "compression_ratio": 1.8620689655172413, "no_speech_prob": 0.009124389849603176}, {"id": 326, "seek": 185424, "start": 1873.1200000000001, "end": 1878.48, "text": " do this but there are many different ways so that's also why we have so many different methods", "tokens": [51308, 360, 341, 457, 456, 366, 867, 819, 2098, 370, 300, 311, 611, 983, 321, 362, 370, 867, 819, 7150, 51576], "temperature": 0.0, "avg_logprob": -0.07765630158511075, "compression_ratio": 1.8620689655172413, "no_speech_prob": 0.009124389849603176}, {"id": 327, "seek": 187848, "start": 1878.48, "end": 1886.0, "text": " and then they highlight which pixels were relevant for the classification um but yeah", "tokens": [50364, 293, 550, 436, 5078, 597, 18668, 645, 7340, 337, 264, 21538, 1105, 457, 1338, 50740], "temperature": 0.0, "avg_logprob": -0.09289834469179564, "compression_ratio": 1.832512315270936, "no_speech_prob": 0.0032223276793956757}, {"id": 328, "seek": 187848, "start": 1886.64, "end": 1891.68, "text": " they these these methods they have like a lot of like issues for example there's the issue of", "tokens": [50772, 436, 613, 613, 7150, 436, 362, 411, 257, 688, 295, 411, 2663, 337, 1365, 456, 311, 264, 2734, 295, 51024], "temperature": 0.0, "avg_logprob": -0.09289834469179564, "compression_ratio": 1.832512315270936, "no_speech_prob": 0.0032223276793956757}, {"id": 329, "seek": 187848, "start": 1891.68, "end": 1897.44, "text": " saturation for example because of the real unit where you have flat parts of the gradient so if", "tokens": [51024, 27090, 337, 1365, 570, 295, 264, 957, 4985, 689, 291, 362, 4962, 3166, 295, 264, 16235, 370, 498, 51312], "temperature": 0.0, "avg_logprob": -0.09289834469179564, "compression_ratio": 1.832512315270936, "no_speech_prob": 0.0032223276793956757}, {"id": 330, "seek": 187848, "start": 1897.44, "end": 1903.28, "text": " you pass the gradient through that then um your method would say that some some neuron might not", "tokens": [51312, 291, 1320, 264, 16235, 807, 300, 550, 1105, 428, 3170, 576, 584, 300, 512, 512, 34090, 1062, 406, 51604], "temperature": 0.0, "avg_logprob": -0.09289834469179564, "compression_ratio": 1.832512315270936, "no_speech_prob": 0.0032223276793956757}, {"id": 331, "seek": 190328, "start": 1903.28, "end": 1909.68, "text": " be uh important at all and there's a lot of these little issues that these methods have um", "tokens": [50364, 312, 2232, 1021, 412, 439, 293, 456, 311, 257, 688, 295, 613, 707, 2663, 300, 613, 7150, 362, 1105, 50684], "temperature": 0.0, "avg_logprob": -0.06983928595270429, "compression_ratio": 1.8804780876494025, "no_speech_prob": 0.01590256579220295}, {"id": 332, "seek": 190328, "start": 1910.8, "end": 1915.92, "text": " yeah so but but back to your question like I think that's also the issue that I don't", "tokens": [50740, 1338, 370, 457, 457, 646, 281, 428, 1168, 411, 286, 519, 300, 311, 611, 264, 2734, 300, 286, 500, 380, 50996], "temperature": 0.0, "avg_logprob": -0.06983928595270429, "compression_ratio": 1.8804780876494025, "no_speech_prob": 0.01590256579220295}, {"id": 333, "seek": 190328, "start": 1915.92, "end": 1920.48, "text": " wouldn't know how to answer I mean obviously it should be some area that should be highlighted on", "tokens": [50996, 2759, 380, 458, 577, 281, 1867, 286, 914, 2745, 309, 820, 312, 512, 1859, 300, 820, 312, 17173, 322, 51224], "temperature": 0.0, "avg_logprob": -0.06983928595270429, "compression_ratio": 1.8804780876494025, "no_speech_prob": 0.01590256579220295}, {"id": 334, "seek": 190328, "start": 1920.48, "end": 1925.6, "text": " this aliens method was important for the for the neural network um but then again I don't know how", "tokens": [51224, 341, 21594, 3170, 390, 1021, 337, 264, 337, 264, 18161, 3209, 1105, 457, 550, 797, 286, 500, 380, 458, 577, 51480], "temperature": 0.0, "avg_logprob": -0.06983928595270429, "compression_ratio": 1.8804780876494025, "no_speech_prob": 0.01590256579220295}, {"id": 335, "seek": 190328, "start": 1925.6, "end": 1931.76, "text": " the network decides so I couldn't like if I see an image I couldn't like highlight the part I mean", "tokens": [51480, 264, 3209, 14898, 370, 286, 2809, 380, 411, 498, 286, 536, 364, 3256, 286, 2809, 380, 411, 5078, 264, 644, 286, 914, 51788], "temperature": 0.0, "avg_logprob": -0.06983928595270429, "compression_ratio": 1.8804780876494025, "no_speech_prob": 0.01590256579220295}, {"id": 336, "seek": 193176, "start": 1931.76, "end": 1936.8799999999999, "text": " I could highlight the part where I think the network should look but then again I mean there are", "tokens": [50364, 286, 727, 5078, 264, 644, 689, 286, 519, 264, 3209, 820, 574, 457, 550, 797, 286, 914, 456, 366, 50620], "temperature": 0.0, "avg_logprob": -0.12475683472373268, "compression_ratio": 1.7887323943661972, "no_speech_prob": 0.0014100308762863278}, {"id": 337, "seek": 193176, "start": 1936.8799999999999, "end": 1942.24, "text": " lots of papers like the clever hands paper which saw like the reveal that there are some", "tokens": [50620, 3195, 295, 10577, 411, 264, 13494, 2377, 3035, 597, 1866, 411, 264, 10658, 300, 456, 366, 512, 50888], "temperature": 0.0, "avg_logprob": -0.12475683472373268, "compression_ratio": 1.7887323943661972, "no_speech_prob": 0.0014100308762863278}, {"id": 338, "seek": 193176, "start": 1943.36, "end": 1949.52, "text": " sometimes it would look at watermarks on on the photo um so these are like these things that we", "tokens": [50944, 2171, 309, 576, 574, 412, 1281, 37307, 322, 322, 264, 5052, 1105, 370, 613, 366, 411, 613, 721, 300, 321, 51252], "temperature": 0.0, "avg_logprob": -0.12475683472373268, "compression_ratio": 1.7887323943661972, "no_speech_prob": 0.0014100308762863278}, {"id": 339, "seek": 193176, "start": 1949.52, "end": 1955.6, "text": " just don't know uh what the neural network basis this on have if I could take a stab at that answer", "tokens": [51252, 445, 500, 380, 458, 2232, 437, 264, 18161, 3209, 5143, 341, 322, 362, 498, 286, 727, 747, 257, 16343, 412, 300, 1867, 51556], "temperature": 0.0, "avg_logprob": -0.12475683472373268, "compression_ratio": 1.7887323943661972, "no_speech_prob": 0.0014100308762863278}, {"id": 340, "seek": 195560, "start": 1956.24, "end": 1962.6399999999999, "text": " for one I think just the idea of quote a saliency map is a problem like there isn't one", "tokens": [50396, 337, 472, 286, 519, 445, 264, 1558, 295, 6513, 257, 1845, 7848, 4471, 307, 257, 1154, 411, 456, 1943, 380, 472, 50716], "temperature": 0.0, "avg_logprob": -0.06130686977453399, "compression_ratio": 1.8785425101214575, "no_speech_prob": 0.12585614621639252}, {"id": 341, "seek": 195560, "start": 1963.52, "end": 1967.76, "text": " map of of the importance of the pixels it's like they're they're operating on multiple", "tokens": [50760, 4471, 295, 295, 264, 7379, 295, 264, 18668, 309, 311, 411, 436, 434, 436, 434, 7447, 322, 3866, 50972], "temperature": 0.0, "avg_logprob": -0.06130686977453399, "compression_ratio": 1.8785425101214575, "no_speech_prob": 0.12585614621639252}, {"id": 342, "seek": 195560, "start": 1968.48, "end": 1972.7199999999998, "text": " multiple dimensions or at least sort of multiple feature sets it's like if you ask me to tell you", "tokens": [51008, 3866, 12819, 420, 412, 1935, 1333, 295, 3866, 4111, 6352, 309, 311, 411, 498, 291, 1029, 385, 281, 980, 291, 51220], "temperature": 0.0, "avg_logprob": -0.06130686977453399, "compression_ratio": 1.8785425101214575, "no_speech_prob": 0.12585614621639252}, {"id": 343, "seek": 195560, "start": 1972.7199999999998, "end": 1978.32, "text": " know why is this image a dog you know well for one thing it's it's the overall shape you know it has", "tokens": [51220, 458, 983, 307, 341, 3256, 257, 3000, 291, 458, 731, 337, 472, 551, 309, 311, 309, 311, 264, 4787, 3909, 291, 458, 309, 575, 51500], "temperature": 0.0, "avg_logprob": -0.06130686977453399, "compression_ratio": 1.8785425101214575, "no_speech_prob": 0.12585614621639252}, {"id": 344, "seek": 195560, "start": 1978.32, "end": 1984.7199999999998, "text": " four legs and you know two ears sticking out over here that's one saliency another is that", "tokens": [51500, 1451, 5668, 293, 291, 458, 732, 8798, 13465, 484, 670, 510, 300, 311, 472, 1845, 7848, 1071, 307, 300, 51820], "temperature": 0.0, "avg_logprob": -0.06130686977453399, "compression_ratio": 1.8785425101214575, "no_speech_prob": 0.12585614621639252}, {"id": 345, "seek": 198472, "start": 1984.72, "end": 1989.04, "text": " it's it's got a certain color you know and it and it's coat and that's a that's a different", "tokens": [50364, 309, 311, 309, 311, 658, 257, 1629, 2017, 291, 458, 293, 309, 293, 309, 311, 10690, 293, 300, 311, 257, 300, 311, 257, 819, 50580], "temperature": 0.0, "avg_logprob": -0.07998076206495781, "compression_ratio": 1.8645418326693226, "no_speech_prob": 0.0013652602210640907}, {"id": 346, "seek": 198472, "start": 1989.04, "end": 1994.0, "text": " concept of what's salient and another is that there's a frisbee flying at it and its mouth is open", "tokens": [50580, 3410, 295, 437, 311, 1845, 1196, 293, 1071, 307, 300, 456, 311, 257, 431, 271, 24872, 7137, 412, 309, 293, 1080, 4525, 307, 1269, 50828], "temperature": 0.0, "avg_logprob": -0.07998076206495781, "compression_ratio": 1.8645418326693226, "no_speech_prob": 0.0013652602210640907}, {"id": 347, "seek": 198472, "start": 1994.0, "end": 1999.2, "text": " and it's about to catch it and I know dogs do that so they're kind of you know when your mind", "tokens": [50828, 293, 309, 311, 466, 281, 3745, 309, 293, 286, 458, 7197, 360, 300, 370, 436, 434, 733, 295, 291, 458, 562, 428, 1575, 51088], "temperature": 0.0, "avg_logprob": -0.07998076206495781, "compression_ratio": 1.8645418326693226, "no_speech_prob": 0.0013652602210640907}, {"id": 348, "seek": 198472, "start": 1999.2, "end": 2004.64, "text": " analyzes an image it breaks it down into these many large scale kind of structural features", "tokens": [51088, 6459, 12214, 364, 3256, 309, 9857, 309, 760, 666, 613, 867, 2416, 4373, 733, 295, 15067, 4122, 51360], "temperature": 0.0, "avg_logprob": -0.07998076206495781, "compression_ratio": 1.8645418326693226, "no_speech_prob": 0.0013652602210640907}, {"id": 349, "seek": 198472, "start": 2004.64, "end": 2009.2, "text": " and I think that gets completely lost and most of the approach is the saliency maps this is", "tokens": [51360, 293, 286, 519, 300, 2170, 2584, 2731, 293, 881, 295, 264, 3109, 307, 264, 1845, 7848, 11317, 341, 307, 51588], "temperature": 0.0, "avg_logprob": -0.07998076206495781, "compression_ratio": 1.8645418326693226, "no_speech_prob": 0.0013652602210640907}, {"id": 350, "seek": 200920, "start": 2010.0, "end": 2016.48, "text": " really important point actually because if you're just looking at the pixels on this kind of 2d", "tokens": [50404, 534, 1021, 935, 767, 570, 498, 291, 434, 445, 1237, 412, 264, 18668, 322, 341, 733, 295, 568, 67, 50728], "temperature": 0.0, "avg_logprob": -0.04812153861636207, "compression_ratio": 1.7683823529411764, "no_speech_prob": 0.03690909221768379}, {"id": 351, "seek": 200920, "start": 2016.48, "end": 2022.24, "text": " planar manifold that's only a very it is quite literally a surface view and I think Christoph you", "tokens": [50728, 1393, 289, 47138, 300, 311, 787, 257, 588, 309, 307, 1596, 3736, 257, 3753, 1910, 293, 286, 519, 2040, 5317, 291, 51016], "temperature": 0.0, "avg_logprob": -0.04812153861636207, "compression_ratio": 1.7683823529411764, "no_speech_prob": 0.03690909221768379}, {"id": 352, "seek": 200920, "start": 2022.24, "end": 2028.88, "text": " said that there are all sorts of causal structures and even in the model itself right there are", "tokens": [51016, 848, 300, 456, 366, 439, 7527, 295, 38755, 9227, 293, 754, 294, 264, 2316, 2564, 558, 456, 366, 51348], "temperature": 0.0, "avg_logprob": -0.04812153861636207, "compression_ratio": 1.7683823529411764, "no_speech_prob": 0.03690909221768379}, {"id": 353, "seek": 200920, "start": 2029.52, "end": 2034.56, "text": " these entangled neurons and surely that's giving me more insight into what's actually happening", "tokens": [51380, 613, 948, 39101, 22027, 293, 11468, 300, 311, 2902, 385, 544, 11269, 666, 437, 311, 767, 2737, 51632], "temperature": 0.0, "avg_logprob": -0.04812153861636207, "compression_ratio": 1.7683823529411764, "no_speech_prob": 0.03690909221768379}, {"id": 354, "seek": 200920, "start": 2034.56, "end": 2038.16, "text": " just seeing a bunch of pixels and the other thing is that these models that they are completely", "tokens": [51632, 445, 2577, 257, 3840, 295, 18668, 293, 264, 661, 551, 307, 300, 613, 5245, 300, 436, 366, 2584, 51812], "temperature": 0.0, "avg_logprob": -0.04812153861636207, "compression_ratio": 1.7683823529411764, "no_speech_prob": 0.03690909221768379}, {"id": 355, "seek": 203816, "start": 2038.16, "end": 2042.4, "text": " lacking in robustness so probably if you changed a few of the wrong pixels your saliency map has", "tokens": [50364, 20889, 294, 13956, 1287, 370, 1391, 498, 291, 3105, 257, 1326, 295, 264, 2085, 18668, 428, 1845, 7848, 4471, 575, 50576], "temperature": 0.0, "avg_logprob": -0.08806207342055238, "compression_ratio": 1.6724738675958188, "no_speech_prob": 0.010675374418497086}, {"id": 356, "seek": 203816, "start": 2042.4, "end": 2049.52, "text": " just got completely broken right yeah so um but in in that vein some of these feature visualization", "tokens": [50576, 445, 658, 2584, 5463, 558, 1338, 370, 1105, 457, 294, 294, 300, 30669, 512, 295, 613, 4111, 25801, 50932], "temperature": 0.0, "avg_logprob": -0.08806207342055238, "compression_ratio": 1.6724738675958188, "no_speech_prob": 0.010675374418497086}, {"id": 357, "seek": 203816, "start": 2049.52, "end": 2054.96, "text": " techniques you know like the deep dream type stuff maybe maybe that's a better way of of", "tokens": [50932, 7512, 291, 458, 411, 264, 2452, 3055, 2010, 1507, 1310, 1310, 300, 311, 257, 1101, 636, 295, 295, 51204], "temperature": 0.0, "avg_logprob": -0.08806207342055238, "compression_ratio": 1.6724738675958188, "no_speech_prob": 0.010675374418497086}, {"id": 358, "seek": 203816, "start": 2054.96, "end": 2059.92, "text": " interpreting these models yeah um so like for the one point you mentioned about the adversarial", "tokens": [51204, 37395, 613, 5245, 1338, 1105, 370, 411, 337, 264, 472, 935, 291, 2835, 466, 264, 17641, 44745, 51452], "temperature": 0.0, "avg_logprob": -0.08806207342055238, "compression_ratio": 1.6724738675958188, "no_speech_prob": 0.010675374418497086}, {"id": 359, "seek": 203816, "start": 2059.92, "end": 2067.36, "text": " examples so there's also a paper I forgot to title again um which that manipulated neural networks", "tokens": [51452, 5110, 370, 456, 311, 611, 257, 3035, 286, 5298, 281, 4876, 797, 1105, 597, 300, 37161, 18161, 9590, 51824], "temperature": 0.0, "avg_logprob": -0.08806207342055238, "compression_ratio": 1.6724738675958188, "no_speech_prob": 0.010675374418497086}, {"id": 360, "seek": 206736, "start": 2067.36, "end": 2072.48, "text": " so they would give the same prediction for all the images but different explanation like different", "tokens": [50364, 370, 436, 576, 976, 264, 912, 17630, 337, 439, 264, 5267, 457, 819, 10835, 411, 819, 50620], "temperature": 0.0, "avg_logprob": -0.06590503912705642, "compression_ratio": 1.9221789883268483, "no_speech_prob": 0.0005522272549569607}, {"id": 361, "seek": 206736, "start": 2072.48, "end": 2079.2000000000003, "text": " saliency maps so this is perfectly possible to create different explanations um for these saliency", "tokens": [50620, 1845, 7848, 11317, 370, 341, 307, 6239, 1944, 281, 1884, 819, 28708, 1105, 337, 613, 1845, 7848, 50956], "temperature": 0.0, "avg_logprob": -0.06590503912705642, "compression_ratio": 1.9221789883268483, "no_speech_prob": 0.0005522272549569607}, {"id": 362, "seek": 206736, "start": 2079.2000000000003, "end": 2086.32, "text": " maps um but but keeping the model like at least for the predictions the same there's another criticism", "tokens": [50956, 11317, 1105, 457, 457, 5145, 264, 2316, 411, 412, 1935, 337, 264, 21264, 264, 912, 456, 311, 1071, 15835, 51312], "temperature": 0.0, "avg_logprob": -0.06590503912705642, "compression_ratio": 1.9221789883268483, "no_speech_prob": 0.0005522272549569607}, {"id": 363, "seek": 206736, "start": 2086.32, "end": 2090.32, "text": " you can throw at saliency maps where they they can be quite deceiving you think they're useful and", "tokens": [51312, 291, 393, 3507, 412, 1845, 7848, 11317, 689, 436, 436, 393, 312, 1596, 14088, 2123, 291, 519, 436, 434, 4420, 293, 51512], "temperature": 0.0, "avg_logprob": -0.06590503912705642, "compression_ratio": 1.9221789883268483, "no_speech_prob": 0.0005522272549569607}, {"id": 364, "seek": 206736, "start": 2090.32, "end": 2094.32, "text": " they turn out not to be useful yeah there's a classic example of looking at you know comparing", "tokens": [51512, 436, 1261, 484, 406, 281, 312, 4420, 1338, 456, 311, 257, 7230, 1365, 295, 1237, 412, 291, 458, 15763, 51712], "temperature": 0.0, "avg_logprob": -0.06590503912705642, "compression_ratio": 1.9221789883268483, "no_speech_prob": 0.0005522272549569607}, {"id": 365, "seek": 209432, "start": 2094.32, "end": 2099.04, "text": " a dog to a wolf and sometimes you see it's looking at the snow in the background and that's helpful", "tokens": [50364, 257, 3000, 281, 257, 19216, 293, 2171, 291, 536, 309, 311, 1237, 412, 264, 5756, 294, 264, 3678, 293, 300, 311, 4961, 50600], "temperature": 0.0, "avg_logprob": -0.052052457567671656, "compression_ratio": 2.0549828178694156, "no_speech_prob": 0.04322522133588791}, {"id": 366, "seek": 209432, "start": 2099.04, "end": 2103.52, "text": " sometimes it highlights the animal and you think okay I understand it's looking at the face that's", "tokens": [50600, 2171, 309, 14254, 264, 5496, 293, 291, 519, 1392, 286, 1223, 309, 311, 1237, 412, 264, 1851, 300, 311, 50824], "temperature": 0.0, "avg_logprob": -0.052052457567671656, "compression_ratio": 2.0549828178694156, "no_speech_prob": 0.04322522133588791}, {"id": 367, "seek": 209432, "start": 2103.52, "end": 2107.92, "text": " why it thinks it's a dog because it's in the face and then you look at the predicted class for something", "tokens": [50824, 983, 309, 7309, 309, 311, 257, 3000, 570, 309, 311, 294, 264, 1851, 293, 550, 291, 574, 412, 264, 19147, 1508, 337, 746, 51044], "temperature": 0.0, "avg_logprob": -0.052052457567671656, "compression_ratio": 2.0549828178694156, "no_speech_prob": 0.04322522133588791}, {"id": 368, "seek": 209432, "start": 2107.92, "end": 2113.1200000000003, "text": " else like you know a cat or a frisbee or a house or a boat and it highlights the face as well yeah", "tokens": [51044, 1646, 411, 291, 458, 257, 3857, 420, 257, 431, 271, 24872, 420, 257, 1782, 420, 257, 6582, 293, 309, 14254, 264, 1851, 382, 731, 1338, 51304], "temperature": 0.0, "avg_logprob": -0.052052457567671656, "compression_ratio": 2.0549828178694156, "no_speech_prob": 0.04322522133588791}, {"id": 369, "seek": 209432, "start": 2113.1200000000003, "end": 2117.1200000000003, "text": " so the saliency map for all these different classes looks the same and when you realize that", "tokens": [51304, 370, 264, 1845, 7848, 4471, 337, 439, 613, 819, 5359, 1542, 264, 912, 293, 562, 291, 4325, 300, 51504], "temperature": 0.0, "avg_logprob": -0.052052457567671656, "compression_ratio": 2.0549828178694156, "no_speech_prob": 0.04322522133588791}, {"id": 370, "seek": 209432, "start": 2117.1200000000003, "end": 2122.6400000000003, "text": " you realize this this saliency map hasn't actually told you anything about why it's gone for one class", "tokens": [51504, 291, 4325, 341, 341, 1845, 7848, 4471, 6132, 380, 767, 1907, 291, 1340, 466, 983, 309, 311, 2780, 337, 472, 1508, 51780], "temperature": 0.0, "avg_logprob": -0.052052457567671656, "compression_ratio": 2.0549828178694156, "no_speech_prob": 0.04322522133588791}, {"id": 371, "seek": 212264, "start": 2122.64, "end": 2126.48, "text": " versus the other all it said is that it's just highlighted the thing in the middle of the picture", "tokens": [50364, 5717, 264, 661, 439, 309, 848, 307, 300, 309, 311, 445, 17173, 264, 551, 294, 264, 2808, 295, 264, 3036, 50556], "temperature": 0.0, "avg_logprob": -0.08471895331767068, "compression_ratio": 1.9233333333333333, "no_speech_prob": 0.014481945894658566}, {"id": 372, "seek": 212264, "start": 2126.48, "end": 2131.68, "text": " yeah I think that's especially also when when you look at images you know like we're very good", "tokens": [50556, 1338, 286, 519, 300, 311, 2318, 611, 562, 562, 291, 574, 412, 5267, 291, 458, 411, 321, 434, 588, 665, 50816], "temperature": 0.0, "avg_logprob": -0.08471895331767068, "compression_ratio": 1.9233333333333333, "no_speech_prob": 0.014481945894658566}, {"id": 373, "seek": 212264, "start": 2132.64, "end": 2136.72, "text": " with images yeah like we were very quick to see what's happening on a scene and such", "tokens": [50864, 365, 5267, 1338, 411, 321, 645, 588, 1702, 281, 536, 437, 311, 2737, 322, 257, 4145, 293, 1270, 51068], "temperature": 0.0, "avg_logprob": -0.08471895331767068, "compression_ratio": 1.9233333333333333, "no_speech_prob": 0.014481945894658566}, {"id": 374, "seek": 212264, "start": 2136.72, "end": 2141.3599999999997, "text": " so I think we're also very quick to make judgments oh yeah this makes sense this doesn't make sense", "tokens": [51068, 370, 286, 519, 321, 434, 611, 588, 1702, 281, 652, 40337, 1954, 1338, 341, 1669, 2020, 341, 1177, 380, 652, 2020, 51300], "temperature": 0.0, "avg_logprob": -0.08471895331767068, "compression_ratio": 1.9233333333333333, "no_speech_prob": 0.014481945894658566}, {"id": 375, "seek": 212264, "start": 2141.3599999999997, "end": 2146.4, "text": " it's more difficult to interpret like if you have like a graph and there's like things going on inside", "tokens": [51300, 309, 311, 544, 2252, 281, 7302, 411, 498, 291, 362, 411, 257, 4295, 293, 456, 311, 411, 721, 516, 322, 1854, 51552], "temperature": 0.0, "avg_logprob": -0.08471895331767068, "compression_ratio": 1.9233333333333333, "no_speech_prob": 0.014481945894658566}, {"id": 376, "seek": 212264, "start": 2146.4, "end": 2151.7599999999998, "text": " you have to like now understand what the method does and stuff like this but for an image like a", "tokens": [51552, 291, 362, 281, 411, 586, 1223, 437, 264, 3170, 775, 293, 1507, 411, 341, 457, 337, 364, 3256, 411, 257, 51820], "temperature": 0.0, "avg_logprob": -0.08471895331767068, "compression_ratio": 1.9233333333333333, "no_speech_prob": 0.014481945894658566}, {"id": 377, "seek": 215176, "start": 2151.76, "end": 2157.2000000000003, "text": " heatmap IR this area is highlighted makes sense case closed I like the method", "tokens": [50364, 3738, 24223, 16486, 341, 1859, 307, 17173, 1669, 2020, 1389, 5395, 286, 411, 264, 3170, 50636], "temperature": 0.0, "avg_logprob": -0.09980841910484994, "compression_ratio": 1.7222222222222223, "no_speech_prob": 0.0003250140289310366}, {"id": 378, "seek": 215176, "start": 2159.1200000000003, "end": 2165.6800000000003, "text": " yeah and that gets exactly back to the deceptively deceptively good explanations problem and explaining", "tokens": [50732, 1338, 293, 300, 2170, 2293, 646, 281, 264, 368, 1336, 3413, 368, 1336, 3413, 665, 28708, 1154, 293, 13468, 51060], "temperature": 0.0, "avg_logprob": -0.09980841910484994, "compression_ratio": 1.7222222222222223, "no_speech_prob": 0.0003250140289310366}, {"id": 379, "seek": 215176, "start": 2165.6800000000003, "end": 2170.6400000000003, "text": " complex things with complex things we don't understand you know so I think a lot of people", "tokens": [51060, 3997, 721, 365, 3997, 721, 321, 500, 380, 1223, 291, 458, 370, 286, 519, 257, 688, 295, 561, 51308], "temperature": 0.0, "avg_logprob": -0.09980841910484994, "compression_ratio": 1.7222222222222223, "no_speech_prob": 0.0003250140289310366}, {"id": 380, "seek": 215176, "start": 2170.6400000000003, "end": 2174.8, "text": " if they looked at it and again one of the points of this interpretability is really the social", "tokens": [51308, 498, 436, 2956, 412, 309, 293, 797, 472, 295, 264, 2793, 295, 341, 7302, 2310, 307, 534, 264, 2093, 51516], "temperature": 0.0, "avg_logprob": -0.09980841910484994, "compression_ratio": 1.7222222222222223, "no_speech_prob": 0.0003250140289310366}, {"id": 381, "seek": 215176, "start": 2174.8, "end": 2181.2000000000003, "text": " aspects of it right like being able to convince people to be at ease with machine learning models", "tokens": [51516, 7270, 295, 309, 558, 411, 885, 1075, 281, 13447, 561, 281, 312, 412, 12708, 365, 3479, 2539, 5245, 51836], "temperature": 0.0, "avg_logprob": -0.09980841910484994, "compression_ratio": 1.7222222222222223, "no_speech_prob": 0.0003250140289310366}, {"id": 382, "seek": 218120, "start": 2181.2799999999997, "end": 2187.9199999999996, "text": " or to accept the results of of a machine learned you know decision process and I think if somebody", "tokens": [50368, 420, 281, 3241, 264, 3542, 295, 295, 257, 3479, 3264, 291, 458, 3537, 1399, 293, 286, 519, 498, 2618, 50700], "temperature": 0.0, "avg_logprob": -0.07749969503852759, "compression_ratio": 1.7433628318584071, "no_speech_prob": 0.0034148835111409426}, {"id": 383, "seek": 218120, "start": 2187.9199999999996, "end": 2192.3199999999997, "text": " looks at an image of a dog you know they have no problem understanding that but if you showed them a", "tokens": [50700, 1542, 412, 364, 3256, 295, 257, 3000, 291, 458, 436, 362, 572, 1154, 3701, 300, 457, 498, 291, 4712, 552, 257, 50920], "temperature": 0.0, "avg_logprob": -0.07749969503852759, "compression_ratio": 1.7433628318584071, "no_speech_prob": 0.0034148835111409426}, {"id": 384, "seek": 218120, "start": 2192.3199999999997, "end": 2199.2, "text": " bunch of salience maps or or any of the other sort of you know feature projections if you will", "tokens": [50920, 3840, 295, 1845, 1182, 11317, 420, 420, 604, 295, 264, 661, 1333, 295, 291, 458, 4111, 32371, 498, 291, 486, 51264], "temperature": 0.0, "avg_logprob": -0.07749969503852759, "compression_ratio": 1.7433628318584071, "no_speech_prob": 0.0034148835111409426}, {"id": 385, "seek": 218120, "start": 2199.2, "end": 2204.8799999999997, "text": " like you said it takes a lot of deep understanding to understand those whereas the image is kind of", "tokens": [51264, 411, 291, 848, 309, 2516, 257, 688, 295, 2452, 3701, 281, 1223, 729, 9735, 264, 3256, 307, 733, 295, 51548], "temperature": 0.0, "avg_logprob": -0.07749969503852759, "compression_ratio": 1.7433628318584071, "no_speech_prob": 0.0034148835111409426}, {"id": 386, "seek": 220488, "start": 2204.88, "end": 2210.56, "text": " immediately obvious I think two of the main themes that you touch on is we'll get to the", "tokens": [50364, 4258, 6322, 286, 519, 732, 295, 264, 2135, 13544, 300, 291, 2557, 322, 307, 321, 603, 483, 281, 264, 50648], "temperature": 0.0, "avg_logprob": -0.09120915236982327, "compression_ratio": 1.88671875, "no_speech_prob": 0.04996216669678688}, {"id": 387, "seek": 220488, "start": 2211.36, "end": 2216.6400000000003, "text": " to the probabilistic stuff the the Judeo pearls stuff in in a minute but I think the main issue", "tokens": [50688, 281, 264, 31959, 3142, 1507, 264, 264, 36521, 78, 35111, 1507, 294, 294, 257, 3456, 457, 286, 519, 264, 2135, 2734, 50952], "temperature": 0.0, "avg_logprob": -0.09120915236982327, "compression_ratio": 1.88671875, "no_speech_prob": 0.04996216669678688}, {"id": 388, "seek": 220488, "start": 2216.6400000000003, "end": 2222.56, "text": " that you point out is feature dependence okay and and you say that when you have feature dependence", "tokens": [50952, 300, 291, 935, 484, 307, 4111, 31704, 1392, 293, 293, 291, 584, 300, 562, 291, 362, 4111, 31704, 51248], "temperature": 0.0, "avg_logprob": -0.09120915236982327, "compression_ratio": 1.88671875, "no_speech_prob": 0.04996216669678688}, {"id": 389, "seek": 220488, "start": 2222.56, "end": 2227.84, "text": " it makes attribution and extrapolation problematic so a dependence just means that you got correlated", "tokens": [51248, 309, 1669, 9080, 1448, 293, 48224, 399, 19011, 370, 257, 31704, 445, 1355, 300, 291, 658, 38574, 51512], "temperature": 0.0, "avg_logprob": -0.09120915236982327, "compression_ratio": 1.88671875, "no_speech_prob": 0.04996216669678688}, {"id": 390, "seek": 220488, "start": 2227.84, "end": 2232.56, "text": " or shared information between your features right so you say that in feature permutation methods", "tokens": [51512, 420, 5507, 1589, 1296, 428, 4122, 558, 370, 291, 584, 300, 294, 4111, 4784, 11380, 7150, 51748], "temperature": 0.0, "avg_logprob": -0.09120915236982327, "compression_ratio": 1.88671875, "no_speech_prob": 0.04996216669678688}, {"id": 391, "seek": 223256, "start": 2233.52, "end": 2237.2, "text": " these things basically break everything when you have the shared information", "tokens": [50412, 613, 721, 1936, 1821, 1203, 562, 291, 362, 264, 5507, 1589, 50596], "temperature": 0.0, "avg_logprob": -0.09694543480873108, "compression_ratio": 1.859437751004016, "no_speech_prob": 0.003356562228873372}, {"id": 392, "seek": 223256, "start": 2237.2, "end": 2242.4, "text": " and the extrapolated data points are no longer in the distribution and you say that there are", "tokens": [50596, 293, 264, 48224, 770, 1412, 2793, 366, 572, 2854, 294, 264, 7316, 293, 291, 584, 300, 456, 366, 50856], "temperature": 0.0, "avg_logprob": -0.09694543480873108, "compression_ratio": 1.859437751004016, "no_speech_prob": 0.003356562228873372}, {"id": 393, "seek": 223256, "start": 2242.4, "end": 2246.96, "text": " conditional permutation schemes you know that try and and maintain that joint distribution but", "tokens": [50856, 27708, 4784, 11380, 26954, 291, 458, 300, 853, 293, 293, 6909, 300, 7225, 7316, 457, 51084], "temperature": 0.0, "avg_logprob": -0.09694543480873108, "compression_ratio": 1.859437751004016, "no_speech_prob": 0.003356562228873372}, {"id": 394, "seek": 223256, "start": 2246.96, "end": 2251.04, "text": " those things sometimes make it even worse right so do you think that's one of the most important", "tokens": [51084, 729, 721, 2171, 652, 309, 754, 5324, 558, 370, 360, 291, 519, 300, 311, 472, 295, 264, 881, 1021, 51288], "temperature": 0.0, "avg_logprob": -0.09694543480873108, "compression_ratio": 1.859437751004016, "no_speech_prob": 0.003356562228873372}, {"id": 395, "seek": 223256, "start": 2251.04, "end": 2257.12, "text": " things that people should think about when using iml methods yeah at least so that's at least like a", "tokens": [51288, 721, 300, 561, 820, 519, 466, 562, 1228, 566, 75, 7150, 1338, 412, 1935, 370, 300, 311, 412, 1935, 411, 257, 51592], "temperature": 0.0, "avg_logprob": -0.09694543480873108, "compression_ratio": 1.859437751004016, "no_speech_prob": 0.003356562228873372}, {"id": 396, "seek": 225712, "start": 2257.12, "end": 2265.44, "text": " very um deep issue I would say which is inherent in in most of the model agnostic methods where you", "tokens": [50364, 588, 1105, 2452, 2734, 286, 576, 584, 597, 307, 26387, 294, 294, 881, 295, 264, 2316, 623, 77, 19634, 7150, 689, 291, 50780], "temperature": 0.0, "avg_logprob": -0.09858074894657841, "compression_ratio": 1.7533632286995515, "no_speech_prob": 0.003823975333943963}, {"id": 397, "seek": 225712, "start": 2265.44, "end": 2271.2799999999997, "text": " manipulate your data see what how the model prediction changes and then create your explanations", "tokens": [50780, 20459, 428, 1412, 536, 437, 577, 264, 2316, 17630, 2962, 293, 550, 1884, 428, 28708, 51072], "temperature": 0.0, "avg_logprob": -0.09858074894657841, "compression_ratio": 1.7533632286995515, "no_speech_prob": 0.003823975333943963}, {"id": 398, "seek": 225712, "start": 2271.2799999999997, "end": 2276.56, "text": " out of this sort of select the shadowy value line partial dependence plot feature importance", "tokens": [51072, 484, 295, 341, 1333, 295, 3048, 264, 8576, 88, 2158, 1622, 14641, 31704, 7542, 4111, 7379, 51336], "temperature": 0.0, "avg_logprob": -0.09858074894657841, "compression_ratio": 1.7533632286995515, "no_speech_prob": 0.003823975333943963}, {"id": 399, "seek": 225712, "start": 2276.56, "end": 2283.6, "text": " they all work with this mechanism of manipulation of the data prediction and then kind of aggregating", "tokens": [51336, 436, 439, 589, 365, 341, 7513, 295, 26475, 295, 264, 1412, 17630, 293, 550, 733, 295, 16743, 990, 51688], "temperature": 0.0, "avg_logprob": -0.09858074894657841, "compression_ratio": 1.7533632286995515, "no_speech_prob": 0.003823975333943963}, {"id": 400, "seek": 228360, "start": 2283.6, "end": 2292.88, "text": " the results and most manipulations happen in isolation so that you for example when you", "tokens": [50364, 264, 3542, 293, 881, 9258, 4136, 1051, 294, 16001, 370, 300, 291, 337, 1365, 562, 291, 50828], "temperature": 0.0, "avg_logprob": -0.15636875204844017, "compression_ratio": 1.7061855670103092, "no_speech_prob": 0.004680784419178963}, {"id": 401, "seek": 228360, "start": 2294.64, "end": 2299.8399999999997, "text": " for feature importance you can meet one of the features as I like said before and then", "tokens": [50916, 337, 4111, 7379, 291, 393, 1677, 472, 295, 264, 4122, 382, 286, 411, 848, 949, 293, 550, 51176], "temperature": 0.0, "avg_logprob": -0.15636875204844017, "compression_ratio": 1.7061855670103092, "no_speech_prob": 0.004680784419178963}, {"id": 402, "seek": 228360, "start": 2301.12, "end": 2304.24, "text": " well you break the association of target but also a few other features", "tokens": [51240, 731, 291, 1821, 264, 14598, 295, 3779, 457, 611, 257, 1326, 661, 4122, 51396], "temperature": 0.0, "avg_logprob": -0.15636875204844017, "compression_ratio": 1.7061855670103092, "no_speech_prob": 0.004680784419178963}, {"id": 403, "seek": 228360, "start": 2304.96, "end": 2310.7999999999997, "text": " but similar things happen if you use lines or you kind of replace parts of your image", "tokens": [51432, 457, 2531, 721, 1051, 498, 291, 764, 3876, 420, 291, 733, 295, 7406, 3166, 295, 428, 3256, 51724], "temperature": 0.0, "avg_logprob": -0.15636875204844017, "compression_ratio": 1.7061855670103092, "no_speech_prob": 0.004680784419178963}, {"id": 404, "seek": 231080, "start": 2310.8, "end": 2315.44, "text": " but then again you also have to replace it with something like which is I think in line the", "tokens": [50364, 457, 550, 797, 291, 611, 362, 281, 7406, 309, 365, 746, 411, 597, 307, 286, 519, 294, 1622, 264, 50596], "temperature": 0.0, "avg_logprob": -0.10345756782675689, "compression_ratio": 1.7389705882352942, "no_speech_prob": 0.002672580536454916}, {"id": 405, "seek": 231080, "start": 2315.44, "end": 2321.1200000000003, "text": " defaults with just a gray image and then of course it's not like it's outside of your data", "tokens": [50596, 7576, 82, 365, 445, 257, 10855, 3256, 293, 550, 295, 1164, 309, 311, 406, 411, 309, 311, 2380, 295, 428, 1412, 50880], "temperature": 0.0, "avg_logprob": -0.10345756782675689, "compression_ratio": 1.7389705882352942, "no_speech_prob": 0.002672580536454916}, {"id": 406, "seek": 231080, "start": 2321.1200000000003, "end": 2326.6400000000003, "text": " distribution subtly because your network was not confronted with like these patchy images before", "tokens": [50880, 7316, 7257, 356, 570, 428, 3209, 390, 406, 31257, 365, 411, 613, 9972, 88, 5267, 949, 51156], "temperature": 0.0, "avg_logprob": -0.10345756782675689, "compression_ratio": 1.7389705882352942, "no_speech_prob": 0.002672580536454916}, {"id": 407, "seek": 231080, "start": 2326.6400000000003, "end": 2332.0800000000004, "text": " they had like just normal photographs usually and depends on your neural network but I mean you", "tokens": [51156, 436, 632, 411, 445, 2710, 17649, 2673, 293, 5946, 322, 428, 18161, 3209, 457, 286, 914, 291, 51428], "temperature": 0.0, "avg_logprob": -0.10345756782675689, "compression_ratio": 1.7389705882352942, "no_speech_prob": 0.002672580536454916}, {"id": 408, "seek": 231080, "start": 2332.0800000000004, "end": 2337.6800000000003, "text": " certainly didn't train it on on images where parts were grayed out so it's pretty likely what the", "tokens": [51428, 3297, 994, 380, 3847, 309, 322, 322, 5267, 689, 3166, 645, 10855, 292, 484, 370, 309, 311, 1238, 3700, 437, 264, 51708], "temperature": 0.0, "avg_logprob": -0.10345756782675689, "compression_ratio": 1.7389705882352942, "no_speech_prob": 0.002672580536454916}, {"id": 409, "seek": 233768, "start": 2337.68, "end": 2343.7599999999998, "text": " model should predict and what will predict at this point but but you use these images to", "tokens": [50364, 2316, 820, 6069, 293, 437, 486, 6069, 412, 341, 935, 457, 457, 291, 764, 613, 5267, 281, 50668], "temperature": 0.0, "avg_logprob": -0.10002883864037784, "compression_ratio": 1.7971014492753623, "no_speech_prob": 0.007809324190020561}, {"id": 410, "seek": 233768, "start": 2344.48, "end": 2348.96, "text": " create your data set like you send it through the neural network you get predictions and", "tokens": [50704, 1884, 428, 1412, 992, 411, 291, 2845, 309, 807, 264, 18161, 3209, 291, 483, 21264, 293, 50928], "temperature": 0.0, "avg_logprob": -0.10002883864037784, "compression_ratio": 1.7971014492753623, "no_speech_prob": 0.007809324190020561}, {"id": 411, "seek": 233768, "start": 2348.96, "end": 2355.6, "text": " you kind of aggregate from this your explanation but you left your data distribution and your model", "tokens": [50928, 291, 733, 295, 26118, 490, 341, 428, 10835, 457, 291, 1411, 428, 1412, 7316, 293, 428, 2316, 51260], "temperature": 0.0, "avg_logprob": -0.10002883864037784, "compression_ratio": 1.7971014492753623, "no_speech_prob": 0.007809324190020561}, {"id": 412, "seek": 233768, "start": 2355.6, "end": 2361.9199999999996, "text": " can do anything then and the hope is that it doesn't do anything crazy but yeah you don't know", "tokens": [51260, 393, 360, 1340, 550, 293, 264, 1454, 307, 300, 309, 1177, 380, 360, 1340, 3219, 457, 1338, 291, 500, 380, 458, 51576], "temperature": 0.0, "avg_logprob": -0.10002883864037784, "compression_ratio": 1.7971014492753623, "no_speech_prob": 0.007809324190020561}, {"id": 413, "seek": 236192, "start": 2362.64, "end": 2368.8, "text": " like it like a simple example from the medical field would be that you know height and weight", "tokens": [50400, 411, 309, 411, 257, 2199, 1365, 490, 264, 4625, 2519, 576, 312, 300, 291, 458, 6681, 293, 3364, 50708], "temperature": 0.0, "avg_logprob": -0.07297771117266487, "compression_ratio": 1.7945205479452055, "no_speech_prob": 0.004608735907822847}, {"id": 414, "seek": 236192, "start": 2368.8, "end": 2375.92, "text": " are highly correlated right and and on the other hand sort of the ratio or some relationship between", "tokens": [50708, 366, 5405, 38574, 558, 293, 293, 322, 264, 661, 1011, 1333, 295, 264, 8509, 420, 512, 2480, 1296, 51064], "temperature": 0.0, "avg_logprob": -0.07297771117266487, "compression_ratio": 1.7945205479452055, "no_speech_prob": 0.004608735907822847}, {"id": 415, "seek": 236192, "start": 2375.92, "end": 2381.52, "text": " your your weight to your height that actually has very important medical consequences right that's", "tokens": [51064, 428, 428, 3364, 281, 428, 6681, 300, 767, 575, 588, 1021, 4625, 10098, 558, 300, 311, 51344], "temperature": 0.0, "avg_logprob": -0.07297771117266487, "compression_ratio": 1.7945205479452055, "no_speech_prob": 0.004608735907822847}, {"id": 416, "seek": 236192, "start": 2381.52, "end": 2387.12, "text": " that's the measure of of health and so if I were to sit there and just permute say the height index", "tokens": [51344, 300, 311, 264, 3481, 295, 295, 1585, 293, 370, 498, 286, 645, 281, 1394, 456, 293, 445, 4784, 1169, 584, 264, 6681, 8186, 51624], "temperature": 0.0, "avg_logprob": -0.07297771117266487, "compression_ratio": 1.7945205479452055, "no_speech_prob": 0.004608735907822847}, {"id": 417, "seek": 238712, "start": 2387.12, "end": 2390.7999999999997, "text": " and create a whole bunch of people that had all these bizarre combinations of", "tokens": [50364, 293, 1884, 257, 1379, 3840, 295, 561, 300, 632, 439, 613, 18265, 21267, 295, 50548], "temperature": 0.0, "avg_logprob": -0.0783380211376753, "compression_ratio": 1.8283828382838283, "no_speech_prob": 0.00406518904492259}, {"id": 418, "seek": 238712, "start": 2391.3599999999997, "end": 2395.92, "text": " of height and weight you know first of all those don't even probably exist in the data set and the", "tokens": [50576, 295, 6681, 293, 3364, 291, 458, 700, 295, 439, 729, 500, 380, 754, 1391, 2514, 294, 264, 1412, 992, 293, 264, 50804], "temperature": 0.0, "avg_logprob": -0.0783380211376753, "compression_ratio": 1.8283828382838283, "no_speech_prob": 0.00406518904492259}, {"id": 419, "seek": 238712, "start": 2395.92, "end": 2402.7999999999997, "text": " ones that do exist in the data set probably had some medical issues right yeah well you actually", "tokens": [50804, 2306, 300, 360, 2514, 294, 264, 1412, 992, 1391, 632, 512, 4625, 2663, 558, 1338, 731, 291, 767, 51148], "temperature": 0.0, "avg_logprob": -0.0783380211376753, "compression_ratio": 1.8283828382838283, "no_speech_prob": 0.00406518904492259}, {"id": 420, "seek": 238712, "start": 2402.7999999999997, "end": 2406.56, "text": " gave a similar example I think you gave the example Christoph of a baby that earns a hundred", "tokens": [51148, 2729, 257, 2531, 1365, 286, 519, 291, 2729, 264, 1365, 2040, 5317, 295, 257, 3186, 300, 46936, 257, 3262, 51336], "temperature": 0.0, "avg_logprob": -0.0783380211376753, "compression_ratio": 1.8283828382838283, "no_speech_prob": 0.00406518904492259}, {"id": 421, "seek": 238712, "start": 2406.56, "end": 2411.04, "text": " thousand dollars a year which is which is insane but when you talk about something like lime", "tokens": [51336, 4714, 3808, 257, 1064, 597, 307, 597, 307, 10838, 457, 562, 291, 751, 466, 746, 411, 22035, 51560], "temperature": 0.0, "avg_logprob": -0.0783380211376753, "compression_ratio": 1.8283828382838283, "no_speech_prob": 0.00406518904492259}, {"id": 422, "seek": 238712, "start": 2411.04, "end": 2416.4, "text": " maybe that's different because the cnn the what you know it shines a flashlight over the input", "tokens": [51560, 1310, 300, 311, 819, 570, 264, 269, 26384, 264, 437, 291, 458, 309, 28056, 257, 30835, 670, 264, 4846, 51828], "temperature": 0.0, "avg_logprob": -0.0783380211376753, "compression_ratio": 1.8283828382838283, "no_speech_prob": 0.00406518904492259}, {"id": 423, "seek": 241640, "start": 2416.4, "end": 2421.44, "text": " space in it and it's a kind of local method so in some sense you could argue that it doesn't", "tokens": [50364, 1901, 294, 309, 293, 309, 311, 257, 733, 295, 2654, 3170, 370, 294, 512, 2020, 291, 727, 9695, 300, 309, 1177, 380, 50616], "temperature": 0.0, "avg_logprob": -0.06986367927407319, "compression_ratio": 1.730909090909091, "no_speech_prob": 0.008171048946678638}, {"id": 424, "seek": 241640, "start": 2421.44, "end": 2425.28, "text": " matter that you've grayed out all this other stuff because if the model was sufficiently well", "tokens": [50616, 1871, 300, 291, 600, 10855, 292, 484, 439, 341, 661, 1507, 570, 498, 264, 2316, 390, 31868, 731, 50808], "temperature": 0.0, "avg_logprob": -0.06986367927407319, "compression_ratio": 1.730909090909091, "no_speech_prob": 0.008171048946678638}, {"id": 425, "seek": 241640, "start": 2425.28, "end": 2429.44, "text": " trained in the first place it should hopefully learn to ignore the background or is that just", "tokens": [50808, 8895, 294, 264, 700, 1081, 309, 820, 4696, 1466, 281, 11200, 264, 3678, 420, 307, 300, 445, 51016], "temperature": 0.0, "avg_logprob": -0.06986367927407319, "compression_ratio": 1.730909090909091, "no_speech_prob": 0.008171048946678638}, {"id": 426, "seek": 241640, "start": 2429.44, "end": 2433.92, "text": " wishful thinking that's an interesting thought I haven't thought about it because like the property", "tokens": [51016, 3172, 906, 1953, 300, 311, 364, 1880, 1194, 286, 2378, 380, 1194, 466, 309, 570, 411, 264, 4707, 51240], "temperature": 0.0, "avg_logprob": -0.06986367927407319, "compression_ratio": 1.730909090909091, "no_speech_prob": 0.008171048946678638}, {"id": 427, "seek": 241640, "start": 2433.92, "end": 2441.12, "text": " of like that you have these filters that trust a wander over the image yeah maybe it would make", "tokens": [51240, 295, 411, 300, 291, 362, 613, 15995, 300, 3361, 257, 27541, 670, 264, 3256, 1338, 1310, 309, 576, 652, 51600], "temperature": 0.0, "avg_logprob": -0.06986367927407319, "compression_ratio": 1.730909090909091, "no_speech_prob": 0.008171048946678638}, {"id": 428, "seek": 244112, "start": 2441.12, "end": 2447.2, "text": " it more robust for these kind of interventions that we do when we create these images with lime", "tokens": [50364, 309, 544, 13956, 337, 613, 733, 295, 20924, 300, 321, 360, 562, 321, 1884, 613, 5267, 365, 22035, 50668], "temperature": 0.0, "avg_logprob": -0.09893069494338262, "compression_ratio": 1.763157894736842, "no_speech_prob": 0.0031708914320915937}, {"id": 429, "seek": 244112, "start": 2447.2, "end": 2454.72, "text": " and shetley um yeah I haven't thought about it it could be uh well we've mentioned all of these", "tokens": [50668, 293, 402, 302, 3420, 1105, 1338, 286, 2378, 380, 1194, 466, 309, 309, 727, 312, 2232, 731, 321, 600, 2835, 439, 295, 613, 51044], "temperature": 0.0, "avg_logprob": -0.09893069494338262, "compression_ratio": 1.763157894736842, "no_speech_prob": 0.0031708914320915937}, {"id": 430, "seek": 244112, "start": 2454.72, "end": 2460.4, "text": " ways in which interpretability methods can go wrong right how the model might not be a realistic", "tokens": [51044, 2098, 294, 597, 7302, 2310, 7150, 393, 352, 2085, 558, 577, 264, 2316, 1062, 406, 312, 257, 12465, 51328], "temperature": 0.0, "avg_logprob": -0.09893069494338262, "compression_ratio": 1.763157894736842, "no_speech_prob": 0.0031708914320915937}, {"id": 431, "seek": 244112, "start": 2460.4, "end": 2464.88, "text": " the interpretability model might not be a good approximation to the actual ml model", "tokens": [51328, 264, 7302, 2310, 2316, 1062, 406, 312, 257, 665, 28023, 281, 264, 3539, 23271, 2316, 51552], "temperature": 0.0, "avg_logprob": -0.09893069494338262, "compression_ratio": 1.763157894736842, "no_speech_prob": 0.0031708914320915937}, {"id": 432, "seek": 244112, "start": 2464.88, "end": 2469.2799999999997, "text": " so some people a bit controversially perhaps take the idea and remember that and say you're just", "tokens": [51552, 370, 512, 561, 257, 857, 11542, 2270, 4317, 747, 264, 1558, 293, 1604, 300, 293, 584, 291, 434, 445, 51772], "temperature": 0.0, "avg_logprob": -0.09893069494338262, "compression_ratio": 1.763157894736842, "no_speech_prob": 0.0031708914320915937}, {"id": 433, "seek": 246928, "start": 2469.28, "end": 2474.0800000000004, "text": " working up completely the wrong tree and you should give up using interpretability", "tokens": [50364, 1364, 493, 2584, 264, 2085, 4230, 293, 291, 820, 976, 493, 1228, 7302, 2310, 50604], "temperature": 0.0, "avg_logprob": -0.13255922610943133, "compression_ratio": 1.8479087452471483, "no_speech_prob": 0.008010120131075382}, {"id": 434, "seek": 246928, "start": 2474.0800000000004, "end": 2479.52, "text": " interpretability methods to explain these black boxes this dish them instead just use an interpretable", "tokens": [50604, 7302, 2310, 7150, 281, 2903, 613, 2211, 2424, 279, 341, 5025, 552, 2602, 445, 764, 364, 7302, 712, 50876], "temperature": 0.0, "avg_logprob": -0.13255922610943133, "compression_ratio": 1.8479087452471483, "no_speech_prob": 0.008010120131075382}, {"id": 435, "seek": 246928, "start": 2479.52, "end": 2484.96, "text": " model to begin with use a white box model I think there's an example um from compass in the US which", "tokens": [50876, 2316, 281, 1841, 365, 764, 257, 2418, 2424, 2316, 286, 519, 456, 311, 364, 1365, 1105, 490, 10707, 294, 264, 2546, 597, 51148], "temperature": 0.0, "avg_logprob": -0.13255922610943133, "compression_ratio": 1.8479087452471483, "no_speech_prob": 0.008010120131075382}, {"id": 436, "seek": 246928, "start": 2484.96, "end": 2490.88, "text": " is that model to predict reoffending and I think quite famously there was a investigative journalists", "tokens": [51148, 307, 300, 2316, 281, 6069, 319, 4506, 2029, 293, 286, 519, 1596, 34360, 456, 390, 257, 45495, 19535, 51444], "temperature": 0.0, "avg_logprob": -0.13255922610943133, "compression_ratio": 1.8479087452471483, "no_speech_prob": 0.008010120131075382}, {"id": 437, "seek": 246928, "start": 2490.88, "end": 2496.32, "text": " that tried to interpret this model it was a black box model because it's proprietary right it's a", "tokens": [51444, 300, 3031, 281, 7302, 341, 2316, 309, 390, 257, 2211, 2424, 2316, 570, 309, 311, 38992, 558, 309, 311, 257, 51716], "temperature": 0.0, "avg_logprob": -0.13255922610943133, "compression_ratio": 1.8479087452471483, "no_speech_prob": 0.008010120131075382}, {"id": 438, "seek": 249632, "start": 2496.32, "end": 2502.32, "text": " trade secret and they they fit it a proxy model a kind of a linear model and they made a report", "tokens": [50364, 4923, 4054, 293, 436, 436, 3318, 309, 257, 29690, 2316, 257, 733, 295, 257, 8213, 2316, 293, 436, 1027, 257, 2275, 50664], "temperature": 0.0, "avg_logprob": -0.0630868345067121, "compression_ratio": 2.0206896551724136, "no_speech_prob": 0.010785241611301899}, {"id": 439, "seek": 249632, "start": 2502.32, "end": 2507.1200000000003, "text": " saying okay we think your model is racist because it looks like it's it's taking race as a factor", "tokens": [50664, 1566, 1392, 321, 519, 428, 2316, 307, 16419, 570, 309, 1542, 411, 309, 311, 309, 311, 1940, 4569, 382, 257, 5952, 50904], "temperature": 0.0, "avg_logprob": -0.0630868345067121, "compression_ratio": 2.0206896551724136, "no_speech_prob": 0.010785241611301899}, {"id": 440, "seek": 249632, "start": 2507.6800000000003, "end": 2511.6800000000003, "text": " and then some further work was done and they came back and said well actually you've just used a", "tokens": [50932, 293, 550, 512, 3052, 589, 390, 1096, 293, 436, 1361, 646, 293, 848, 731, 767, 291, 600, 445, 1143, 257, 51132], "temperature": 0.0, "avg_logprob": -0.0630868345067121, "compression_ratio": 2.0206896551724136, "no_speech_prob": 0.010785241611301899}, {"id": 441, "seek": 249632, "start": 2511.6800000000003, "end": 2516.2400000000002, "text": " uh interpretability model that doesn't really fit our model very well you've made some assumptions", "tokens": [51132, 2232, 7302, 2310, 2316, 300, 1177, 380, 534, 3318, 527, 2316, 588, 731, 291, 600, 1027, 512, 17695, 51360], "temperature": 0.0, "avg_logprob": -0.0630868345067121, "compression_ratio": 2.0206896551724136, "no_speech_prob": 0.010785241611301899}, {"id": 442, "seek": 249632, "start": 2516.2400000000002, "end": 2520.32, "text": " that don't hold if you use a different interpretability model you get a completely different answer", "tokens": [51360, 300, 500, 380, 1797, 498, 291, 764, 257, 819, 7302, 2310, 2316, 291, 483, 257, 2584, 819, 1867, 51564], "temperature": 0.0, "avg_logprob": -0.0630868345067121, "compression_ratio": 2.0206896551724136, "no_speech_prob": 0.010785241611301899}, {"id": 443, "seek": 249632, "start": 2520.32, "end": 2525.04, "text": " that it doesn't use race at all as a factor and so you've got to kind of a you've got to a wrong", "tokens": [51564, 300, 309, 1177, 380, 764, 4569, 412, 439, 382, 257, 5952, 293, 370, 291, 600, 658, 281, 733, 295, 257, 291, 600, 658, 281, 257, 2085, 51800], "temperature": 0.0, "avg_logprob": -0.0630868345067121, "compression_ratio": 2.0206896551724136, "no_speech_prob": 0.010785241611301899}, {"id": 444, "seek": 252504, "start": 2525.04, "end": 2530.64, "text": " assumption by using a bad interpretability model and I think they were saying that this model", "tokens": [50364, 15302, 538, 1228, 257, 1578, 7302, 2310, 2316, 293, 286, 519, 436, 645, 1566, 300, 341, 2316, 50644], "temperature": 0.0, "avg_logprob": -0.07207382849927219, "compression_ratio": 1.8314176245210727, "no_speech_prob": 0.007309336215257645}, {"id": 445, "seek": 252504, "start": 2530.64, "end": 2535.68, "text": " instead you could get just just as good a model of reoffending with like three FL statements you", "tokens": [50644, 2602, 291, 727, 483, 445, 445, 382, 665, 257, 2316, 295, 319, 4506, 2029, 365, 411, 1045, 24720, 12363, 291, 50896], "temperature": 0.0, "avg_logprob": -0.07207382849927219, "compression_ratio": 1.8314176245210727, "no_speech_prob": 0.007309336215257645}, {"id": 446, "seek": 252504, "start": 2535.68, "end": 2542.16, "text": " know ditching this massive complex 100 and something features and just use three FL statements", "tokens": [50896, 458, 25325, 278, 341, 5994, 3997, 2319, 293, 746, 4122, 293, 445, 764, 1045, 24720, 12363, 51220], "temperature": 0.0, "avg_logprob": -0.07207382849927219, "compression_ratio": 1.8314176245210727, "no_speech_prob": 0.007309336215257645}, {"id": 447, "seek": 252504, "start": 2542.16, "end": 2547.6, "text": " based on I think age and reoffending so is that was that what we should do should we just drop", "tokens": [51220, 2361, 322, 286, 519, 3205, 293, 319, 4506, 2029, 370, 307, 300, 390, 300, 437, 321, 820, 360, 820, 321, 445, 3270, 51492], "temperature": 0.0, "avg_logprob": -0.07207382849927219, "compression_ratio": 1.8314176245210727, "no_speech_prob": 0.007309336215257645}, {"id": 448, "seek": 252504, "start": 2547.6, "end": 2552.56, "text": " these methods and start using white boxes instead so I mean like one one thing to mention here is", "tokens": [51492, 613, 7150, 293, 722, 1228, 2418, 9002, 2602, 370, 286, 914, 411, 472, 472, 551, 281, 2152, 510, 307, 51740], "temperature": 0.0, "avg_logprob": -0.07207382849927219, "compression_ratio": 1.8314176245210727, "no_speech_prob": 0.007309336215257645}, {"id": 449, "seek": 255256, "start": 2552.56, "end": 2558.32, "text": " that a white box is very soon also like a gray or black box if you add interactions if you have", "tokens": [50364, 300, 257, 2418, 2424, 307, 588, 2321, 611, 411, 257, 10855, 420, 2211, 2424, 498, 291, 909, 13280, 498, 291, 362, 50652], "temperature": 0.0, "avg_logprob": -0.11228503761710701, "compression_ratio": 1.8211009174311927, "no_speech_prob": 0.006796625908464193}, {"id": 450, "seek": 255256, "start": 2558.32, "end": 2564.16, "text": " many features and so on but putting that aside I would agree if you're in first place like that you", "tokens": [50652, 867, 4122, 293, 370, 322, 457, 3372, 300, 7359, 286, 576, 3986, 498, 291, 434, 294, 700, 1081, 411, 300, 291, 50944], "temperature": 0.0, "avg_logprob": -0.11228503761710701, "compression_ratio": 1.8211009174311927, "no_speech_prob": 0.006796625908464193}, {"id": 451, "seek": 255256, "start": 2564.16, "end": 2569.84, "text": " say you should start with like a white box so if you start modeling then then you then you should", "tokens": [50944, 584, 291, 820, 722, 365, 411, 257, 2418, 2424, 370, 498, 291, 722, 15983, 550, 550, 291, 550, 291, 820, 51228], "temperature": 0.0, "avg_logprob": -0.11228503761710701, "compression_ratio": 1.8211009174311927, "no_speech_prob": 0.006796625908464193}, {"id": 452, "seek": 255256, "start": 2571.04, "end": 2575.36, "text": " consider these first like maybe they already solved your problem then it's perfect and you have a model", "tokens": [51288, 1949, 613, 700, 411, 1310, 436, 1217, 13041, 428, 1154, 550, 309, 311, 2176, 293, 291, 362, 257, 2316, 51504], "temperature": 0.0, "avg_logprob": -0.11228503761710701, "compression_ratio": 1.8211009174311927, "no_speech_prob": 0.006796625908464193}, {"id": 453, "seek": 257536, "start": 2575.36, "end": 2584.88, "text": " that is quite I mean stable it's interpretable I think that would be great but then I think the", "tokens": [50364, 300, 307, 1596, 286, 914, 8351, 309, 311, 7302, 712, 286, 519, 300, 576, 312, 869, 457, 550, 286, 519, 264, 50840], "temperature": 0.0, "avg_logprob": -0.09241657907312567, "compression_ratio": 1.8009259259259258, "no_speech_prob": 0.004681201186031103}, {"id": 454, "seek": 257536, "start": 2584.88, "end": 2589.52, "text": " next step would be to see like what like a black box or a machine learning model would give you in", "tokens": [50840, 958, 1823, 576, 312, 281, 536, 411, 437, 411, 257, 2211, 2424, 420, 257, 3479, 2539, 2316, 576, 976, 291, 294, 51072], "temperature": 0.0, "avg_logprob": -0.09241657907312567, "compression_ratio": 1.8009259259259258, "no_speech_prob": 0.004681201186031103}, {"id": 455, "seek": 257536, "start": 2589.52, "end": 2595.44, "text": " terms of performance and then maybe if you see the gap is really big then maybe you can try", "tokens": [51072, 2115, 295, 3389, 293, 550, 1310, 498, 291, 536, 264, 7417, 307, 534, 955, 550, 1310, 291, 393, 853, 51368], "temperature": 0.0, "avg_logprob": -0.09241657907312567, "compression_ratio": 1.8009259259259258, "no_speech_prob": 0.004681201186031103}, {"id": 456, "seek": 257536, "start": 2595.44, "end": 2600.8, "text": " some feature engineering and close the gap maybe from the interpretative model to the machine learning", "tokens": [51368, 512, 4111, 7043, 293, 1998, 264, 7417, 1310, 490, 264, 7302, 1166, 2316, 281, 264, 3479, 2539, 51636], "temperature": 0.0, "avg_logprob": -0.09241657907312567, "compression_ratio": 1.8009259259259258, "no_speech_prob": 0.004681201186031103}, {"id": 457, "seek": 260080, "start": 2600.8, "end": 2607.2000000000003, "text": " model but then you're probably already infusing some features that are not so interpretable", "tokens": [50364, 2316, 457, 550, 291, 434, 1391, 1217, 1536, 7981, 512, 4122, 300, 366, 406, 370, 7302, 712, 50684], "temperature": 0.0, "avg_logprob": -0.0884526394031666, "compression_ratio": 1.758139534883721, "no_speech_prob": 0.014498256146907806}, {"id": 458, "seek": 260080, "start": 2607.2000000000003, "end": 2612.7200000000003, "text": " or maybe if you're using a linear regression model you're maybe using then splines and", "tokens": [50684, 420, 1310, 498, 291, 434, 1228, 257, 8213, 24590, 2316, 291, 434, 1310, 1228, 550, 4732, 1652, 293, 50960], "temperature": 0.0, "avg_logprob": -0.0884526394031666, "compression_ratio": 1.758139534883721, "no_speech_prob": 0.014498256146907806}, {"id": 459, "seek": 260080, "start": 2612.7200000000003, "end": 2618.8, "text": " interactions so you're already moving towards more complex models usually but then if you still have", "tokens": [50960, 13280, 370, 291, 434, 1217, 2684, 3030, 544, 3997, 5245, 2673, 457, 550, 498, 291, 920, 362, 51264], "temperature": 0.0, "avg_logprob": -0.0884526394031666, "compression_ratio": 1.758139534883721, "no_speech_prob": 0.014498256146907806}, {"id": 460, "seek": 260080, "start": 2618.8, "end": 2626.2400000000002, "text": " a gap then I think you have to decide is the gap and predictive performance like worth changing to", "tokens": [51264, 257, 7417, 550, 286, 519, 291, 362, 281, 4536, 307, 264, 7417, 293, 35521, 3389, 411, 3163, 4473, 281, 51636], "temperature": 0.0, "avg_logprob": -0.0884526394031666, "compression_ratio": 1.758139534883721, "no_speech_prob": 0.014498256146907806}, {"id": 461, "seek": 262624, "start": 2626.24, "end": 2632.4799999999996, "text": " a black box model so I think that's your decision will be different in many cases", "tokens": [50364, 257, 2211, 2424, 2316, 370, 286, 519, 300, 311, 428, 3537, 486, 312, 819, 294, 867, 3331, 50676], "temperature": 0.0, "avg_logprob": -0.10052908847206517, "compression_ratio": 1.6988416988416988, "no_speech_prob": 0.004588899668306112}, {"id": 462, "seek": 262624, "start": 2633.7599999999998, "end": 2636.72, "text": " as you relates back to the point you made on your paper about criticisms of using", "tokens": [50740, 382, 291, 16155, 646, 281, 264, 935, 291, 1027, 322, 428, 3035, 466, 48519, 295, 1228, 50888], "temperature": 0.0, "avg_logprob": -0.10052908847206517, "compression_ratio": 1.6988416988416988, "no_speech_prob": 0.004588899668306112}, {"id": 463, "seek": 262624, "start": 2637.3599999999997, "end": 2641.2, "text": " interpretable machine learning models that some people leap straight away at using an overly", "tokens": [50920, 7302, 712, 3479, 2539, 5245, 300, 512, 561, 19438, 2997, 1314, 412, 1228, 364, 24324, 51112], "temperature": 0.0, "avg_logprob": -0.10052908847206517, "compression_ratio": 1.6988416988416988, "no_speech_prob": 0.004588899668306112}, {"id": 464, "seek": 262624, "start": 2641.2, "end": 2646.64, "text": " complex model and sometimes depending on the situation sometimes you know a linear model can", "tokens": [51112, 3997, 2316, 293, 2171, 5413, 322, 264, 2590, 2171, 291, 458, 257, 8213, 2316, 393, 51384], "temperature": 0.0, "avg_logprob": -0.10052908847206517, "compression_ratio": 1.6988416988416988, "no_speech_prob": 0.004588899668306112}, {"id": 465, "seek": 262624, "start": 2646.64, "end": 2651.3599999999997, "text": " do just as well and have all these advantages it's so much easier to explain do you have a", "tokens": [51384, 360, 445, 382, 731, 293, 362, 439, 613, 14906, 309, 311, 370, 709, 3571, 281, 2903, 360, 291, 362, 257, 51620], "temperature": 0.0, "avg_logprob": -0.10052908847206517, "compression_ratio": 1.6988416988416988, "no_speech_prob": 0.004588899668306112}, {"id": 466, "seek": 265136, "start": 2651.36, "end": 2657.76, "text": " philosophy from a high level here right because if it if it were a human if it were an airplane", "tokens": [50364, 10675, 490, 257, 1090, 1496, 510, 558, 570, 498, 309, 498, 309, 645, 257, 1952, 498, 309, 645, 364, 17130, 50684], "temperature": 0.0, "avg_logprob": -0.0649432961000215, "compression_ratio": 1.8307692307692307, "no_speech_prob": 0.024382099509239197}, {"id": 467, "seek": 265136, "start": 2657.76, "end": 2664.88, "text": " pilot we don't really understand how the brain works right we would just test the pilot you've", "tokens": [50684, 9691, 321, 500, 380, 534, 1223, 577, 264, 3567, 1985, 558, 321, 576, 445, 1500, 264, 9691, 291, 600, 51040], "temperature": 0.0, "avg_logprob": -0.0649432961000215, "compression_ratio": 1.8307692307692307, "no_speech_prob": 0.024382099509239197}, {"id": 468, "seek": 265136, "start": 2664.88, "end": 2671.6800000000003, "text": " got to fly the plane for 10 000 hours and if you don't crash then we'll let you fly so we don't", "tokens": [51040, 658, 281, 3603, 264, 5720, 337, 1266, 13711, 2496, 293, 498, 291, 500, 380, 8252, 550, 321, 603, 718, 291, 3603, 370, 321, 500, 380, 51380], "temperature": 0.0, "avg_logprob": -0.0649432961000215, "compression_ratio": 1.8307692307692307, "no_speech_prob": 0.024382099509239197}, {"id": 469, "seek": 265136, "start": 2671.6800000000003, "end": 2675.52, "text": " really seek to understand how his or her brain works but with machine learning models there's", "tokens": [51380, 534, 8075, 281, 1223, 577, 702, 420, 720, 3567, 1985, 457, 365, 3479, 2539, 5245, 456, 311, 51572], "temperature": 0.0, "avg_logprob": -0.0649432961000215, "compression_ratio": 1.8307692307692307, "no_speech_prob": 0.024382099509239197}, {"id": 470, "seek": 265136, "start": 2675.52, "end": 2680.7200000000003, "text": " this continuum right so if you use these complex black box models the predictive performance is", "tokens": [51572, 341, 36120, 558, 370, 498, 291, 764, 613, 3997, 2211, 2424, 5245, 264, 35521, 3389, 307, 51832], "temperature": 0.0, "avg_logprob": -0.0649432961000215, "compression_ratio": 1.8307692307692307, "no_speech_prob": 0.024382099509239197}, {"id": 471, "seek": 268072, "start": 2680.72, "end": 2686.16, "text": " usually better but you're trading off understandability and assuming those things are completely", "tokens": [50364, 2673, 1101, 457, 291, 434, 9529, 766, 1223, 2310, 293, 11926, 729, 721, 366, 2584, 50636], "temperature": 0.0, "avg_logprob": -0.05775710252615122, "compression_ratio": 1.6782608695652175, "no_speech_prob": 0.0027949081268161535}, {"id": 472, "seek": 268072, "start": 2686.16, "end": 2690.64, "text": " mutually exclusive what kind of decision process do you go through when you select these models", "tokens": [50636, 39144, 13005, 437, 733, 295, 3537, 1399, 360, 291, 352, 807, 562, 291, 3048, 613, 5245, 50860], "temperature": 0.0, "avg_logprob": -0.05775710252615122, "compression_ratio": 1.6782608695652175, "no_speech_prob": 0.0027949081268161535}, {"id": 473, "seek": 268072, "start": 2690.64, "end": 2695.6, "text": " but by the way with machine learning right the reason why we use machine learning is because", "tokens": [50860, 457, 538, 264, 636, 365, 3479, 2539, 558, 264, 1778, 983, 321, 764, 3479, 2539, 307, 570, 51108], "temperature": 0.0, "avg_logprob": -0.05775710252615122, "compression_ratio": 1.6782608695652175, "no_speech_prob": 0.0027949081268161535}, {"id": 474, "seek": 268072, "start": 2695.6, "end": 2702.64, "text": " we don't understand how to do something explicitly yeah is that a fair statement um yeah I would say", "tokens": [51108, 321, 500, 380, 1223, 577, 281, 360, 746, 20803, 1338, 307, 300, 257, 3143, 5629, 1105, 1338, 286, 576, 584, 51460], "temperature": 0.0, "avg_logprob": -0.05775710252615122, "compression_ratio": 1.6782608695652175, "no_speech_prob": 0.0027949081268161535}, {"id": 475, "seek": 270264, "start": 2702.8799999999997, "end": 2708.96, "text": " um when when it ate us like so high-dimensional so complex many interactions and so on", "tokens": [50376, 1105, 562, 562, 309, 8468, 505, 411, 370, 1090, 12, 18759, 370, 3997, 867, 13280, 293, 370, 322, 50680], "temperature": 0.0, "avg_logprob": -0.10926991642111598, "compression_ratio": 2.0132743362831858, "no_speech_prob": 0.028716715052723885}, {"id": 476, "seek": 270264, "start": 2710.24, "end": 2716.0, "text": " that your simple models don't cover the complex cannot cover the complexity I think then you", "tokens": [50744, 300, 428, 2199, 5245, 500, 380, 2060, 264, 3997, 2644, 2060, 264, 14024, 286, 519, 550, 291, 51032], "temperature": 0.0, "avg_logprob": -0.10926991642111598, "compression_ratio": 2.0132743362831858, "no_speech_prob": 0.028716715052723885}, {"id": 477, "seek": 270264, "start": 2716.0, "end": 2722.56, "text": " need machine learning would you rather understand exactly how the plane worked or would you", "tokens": [51032, 643, 3479, 2539, 576, 291, 2831, 1223, 2293, 577, 264, 5720, 2732, 420, 576, 291, 51360], "temperature": 0.0, "avg_logprob": -0.10926991642111598, "compression_ratio": 2.0132743362831858, "no_speech_prob": 0.028716715052723885}, {"id": 478, "seek": 270264, "start": 2722.56, "end": 2726.48, "text": " rather I mean if I was saying to you you can go and fly in a plane would you rather that you", "tokens": [51360, 2831, 286, 914, 498, 286, 390, 1566, 281, 291, 291, 393, 352, 293, 3603, 294, 257, 5720, 576, 291, 2831, 300, 291, 51556], "temperature": 0.0, "avg_logprob": -0.10926991642111598, "compression_ratio": 2.0132743362831858, "no_speech_prob": 0.028716715052723885}, {"id": 479, "seek": 270264, "start": 2726.48, "end": 2731.2, "text": " understood how the plane worked or would you rather that the plane was tested why not both", "tokens": [51556, 7320, 577, 264, 5720, 2732, 420, 576, 291, 2831, 300, 264, 5720, 390, 8246, 983, 406, 1293, 51792], "temperature": 0.0, "avg_logprob": -0.10926991642111598, "compression_ratio": 2.0132743362831858, "no_speech_prob": 0.028716715052723885}, {"id": 480, "seek": 273120, "start": 2732.0, "end": 2740.16, "text": " so um I think we can do both it's to some degree so um of course with black box model we don't", "tokens": [50404, 370, 1105, 286, 519, 321, 393, 360, 1293, 309, 311, 281, 512, 4314, 370, 1105, 295, 1164, 365, 2211, 2424, 2316, 321, 500, 380, 50812], "temperature": 0.0, "avg_logprob": -0.09724303654261998, "compression_ratio": 1.6278026905829597, "no_speech_prob": 0.0010815977584570646}, {"id": 481, "seek": 273120, "start": 2740.16, "end": 2746.0, "text": " exactly understand how they work um but in comparison to a pilot we can test them for", "tokens": [50812, 2293, 1223, 577, 436, 589, 1105, 457, 294, 9660, 281, 257, 9691, 321, 393, 1500, 552, 337, 51104], "temperature": 0.0, "avg_logprob": -0.09724303654261998, "compression_ratio": 1.6278026905829597, "no_speech_prob": 0.0010815977584570646}, {"id": 482, "seek": 273120, "start": 2746.0, "end": 2752.72, "text": " three more or less um so because I mean maybe it's not as good as an interpretable model", "tokens": [51104, 1045, 544, 420, 1570, 1105, 370, 570, 286, 914, 1310, 309, 311, 406, 382, 665, 382, 364, 7302, 712, 2316, 51440], "temperature": 0.0, "avg_logprob": -0.09724303654261998, "compression_ratio": 1.6278026905829597, "no_speech_prob": 0.0010815977584570646}, {"id": 483, "seek": 273120, "start": 2753.52, "end": 2759.52, "text": " but we still can use a lot of methods to at least approximate and and try to understand a few", "tokens": [51480, 457, 321, 920, 393, 764, 257, 688, 295, 7150, 281, 412, 1935, 30874, 293, 293, 853, 281, 1223, 257, 1326, 51780], "temperature": 0.0, "avg_logprob": -0.09724303654261998, "compression_ratio": 1.6278026905829597, "no_speech_prob": 0.0010815977584570646}, {"id": 484, "seek": 275952, "start": 2759.52, "end": 2765.84, "text": " properties of this model so I think we are even in a situation where we don't have like these", "tokens": [50364, 7221, 295, 341, 2316, 370, 286, 519, 321, 366, 754, 294, 257, 2590, 689, 321, 500, 380, 362, 411, 613, 50680], "temperature": 0.0, "avg_logprob": -0.06931848087530026, "compression_ratio": 1.676991150442478, "no_speech_prob": 0.006002826150506735}, {"id": 485, "seek": 275952, "start": 2765.84, "end": 2773.28, "text": " complete like A or B decisions but we can have so if if the machine learning works much better", "tokens": [50680, 3566, 411, 316, 420, 363, 5327, 457, 321, 393, 362, 370, 498, 498, 264, 3479, 2539, 1985, 709, 1101, 51052], "temperature": 0.0, "avg_logprob": -0.06931848087530026, "compression_ratio": 1.676991150442478, "no_speech_prob": 0.006002826150506735}, {"id": 486, "seek": 275952, "start": 2773.28, "end": 2779.28, "text": " and it's like really robustly tested with lots of different data I would prefer machine learning", "tokens": [51052, 293, 309, 311, 411, 534, 13956, 356, 8246, 365, 3195, 295, 819, 1412, 286, 576, 4382, 3479, 2539, 51352], "temperature": 0.0, "avg_logprob": -0.06931848087530026, "compression_ratio": 1.676991150442478, "no_speech_prob": 0.006002826150506735}, {"id": 487, "seek": 275952, "start": 2779.28, "end": 2788.48, "text": " model I guess um but then I would also want to like people to to apply all these methods that", "tokens": [51352, 2316, 286, 2041, 1105, 457, 550, 286, 576, 611, 528, 281, 411, 561, 281, 281, 3079, 439, 613, 7150, 300, 51812], "temperature": 0.0, "avg_logprob": -0.06931848087530026, "compression_ratio": 1.676991150442478, "no_speech_prob": 0.006002826150506735}, {"id": 488, "seek": 278848, "start": 2788.48, "end": 2792.72, "text": " are available even if they are not perfect but still they give you something they give you some", "tokens": [50364, 366, 2435, 754, 498, 436, 366, 406, 2176, 457, 920, 436, 976, 291, 746, 436, 976, 291, 512, 50576], "temperature": 0.0, "avg_logprob": -0.0445393713394014, "compression_ratio": 1.7378277153558053, "no_speech_prob": 0.004330649971961975}, {"id": 489, "seek": 278848, "start": 2792.72, "end": 2799.92, "text": " insights so yeah and I think so Tim one answer to your question is that a lot of people's response", "tokens": [50576, 14310, 370, 1338, 293, 286, 519, 370, 7172, 472, 1867, 281, 428, 1168, 307, 300, 257, 688, 295, 561, 311, 4134, 50936], "temperature": 0.0, "avg_logprob": -0.0445393713394014, "compression_ratio": 1.7378277153558053, "no_speech_prob": 0.004330649971961975}, {"id": 490, "seek": 278848, "start": 2800.64, "end": 2804.96, "text": " here and kind of demanding interpretability and having concerns about machine learning", "tokens": [50972, 510, 293, 733, 295, 19960, 7302, 2310, 293, 1419, 7389, 466, 3479, 2539, 51188], "temperature": 0.0, "avg_logprob": -0.0445393713394014, "compression_ratio": 1.7378277153558053, "no_speech_prob": 0.004330649971961975}, {"id": 491, "seek": 278848, "start": 2804.96, "end": 2809.6, "text": " it all comes down to generalizability and we've seen through using machine learning", "tokens": [51188, 309, 439, 1487, 760, 281, 2674, 590, 2310, 293, 321, 600, 1612, 807, 1228, 3479, 2539, 51420], "temperature": 0.0, "avg_logprob": -0.0445393713394014, "compression_ratio": 1.7378277153558053, "no_speech_prob": 0.004330649971961975}, {"id": 492, "seek": 278848, "start": 2810.16, "end": 2817.52, "text": " that it breaks down in ways that that we don't like like for example sure maybe the soap dispenser", "tokens": [51448, 300, 309, 9857, 760, 294, 2098, 300, 300, 321, 500, 380, 411, 411, 337, 1365, 988, 1310, 264, 14587, 4920, 30041, 51816], "temperature": 0.0, "avg_logprob": -0.0445393713394014, "compression_ratio": 1.7378277153558053, "no_speech_prob": 0.004330649971961975}, {"id": 493, "seek": 281752, "start": 2817.52, "end": 2824.32, "text": " you know is really great at dispensing it dispensing soap you know 87 percent of time but", "tokens": [50364, 291, 458, 307, 534, 869, 412, 4920, 22481, 309, 4920, 22481, 14587, 291, 458, 27990, 3043, 295, 565, 457, 50704], "temperature": 0.0, "avg_logprob": -0.08290268610982061, "compression_ratio": 1.7923076923076924, "no_speech_prob": 0.0007792218821123242}, {"id": 494, "seek": 281752, "start": 2824.32, "end": 2830.64, "text": " it just so happens to kind of be a race sensitive soap dispenser and just doesn't give any soap to", "tokens": [50704, 309, 445, 370, 2314, 281, 733, 295, 312, 257, 4569, 9477, 14587, 4920, 30041, 293, 445, 1177, 380, 976, 604, 14587, 281, 51020], "temperature": 0.0, "avg_logprob": -0.08290268610982061, "compression_ratio": 1.7923076923076924, "no_speech_prob": 0.0007792218821123242}, {"id": 495, "seek": 281752, "start": 2830.64, "end": 2835.2, "text": " people with a certain skin color like we've kind of decided is a society that are that there are", "tokens": [51020, 561, 365, 257, 1629, 3178, 2017, 411, 321, 600, 733, 295, 3047, 307, 257, 4086, 300, 366, 300, 456, 366, 51248], "temperature": 0.0, "avg_logprob": -0.08290268610982061, "compression_ratio": 1.7923076923076924, "no_speech_prob": 0.0007792218821123242}, {"id": 496, "seek": 281752, "start": 2835.2, "end": 2841.36, "text": " certain generalizations or certain dimensions along which our models just have to perform", "tokens": [51248, 1629, 2674, 14455, 420, 1629, 12819, 2051, 597, 527, 5245, 445, 362, 281, 2042, 51556], "temperature": 0.0, "avg_logprob": -0.08290268610982061, "compression_ratio": 1.7923076923076924, "no_speech_prob": 0.0007792218821123242}, {"id": 497, "seek": 281752, "start": 2841.36, "end": 2846.72, "text": " and also because a lot of these these things that break machine learning models are things", "tokens": [51556, 293, 611, 570, 257, 688, 295, 613, 613, 721, 300, 1821, 3479, 2539, 5245, 366, 721, 51824], "temperature": 0.0, "avg_logprob": -0.08290268610982061, "compression_ratio": 1.7923076923076924, "no_speech_prob": 0.0007792218821123242}, {"id": 498, "seek": 284672, "start": 2846.72, "end": 2852.64, "text": " that happen quite quite regularly in the real world it's like a pilot you know a human being", "tokens": [50364, 300, 1051, 1596, 1596, 11672, 294, 264, 957, 1002, 309, 311, 411, 257, 9691, 291, 458, 257, 1952, 885, 50660], "temperature": 0.0, "avg_logprob": -0.04665089079311916, "compression_ratio": 1.8115384615384615, "no_speech_prob": 0.0017543939175084233}, {"id": 499, "seek": 284672, "start": 2852.64, "end": 2858.16, "text": " pilot flying around if he looks down at the ground and sees a hot air balloon with a big", "tokens": [50660, 9691, 7137, 926, 498, 415, 1542, 760, 412, 264, 2727, 293, 8194, 257, 2368, 1988, 16994, 365, 257, 955, 50936], "temperature": 0.0, "avg_logprob": -0.04665089079311916, "compression_ratio": 1.8115384615384615, "no_speech_prob": 0.0017543939175084233}, {"id": 500, "seek": 284672, "start": 2858.16, "end": 2863.12, "text": " smiley face on it he's not going to crash the plane he's just going to be like oh yeah I forgot", "tokens": [50936, 7563, 88, 1851, 322, 309, 415, 311, 406, 516, 281, 8252, 264, 5720, 415, 311, 445, 516, 281, 312, 411, 1954, 1338, 286, 5298, 51184], "temperature": 0.0, "avg_logprob": -0.04665089079311916, "compression_ratio": 1.8115384615384615, "no_speech_prob": 0.0017543939175084233}, {"id": 501, "seek": 284672, "start": 2863.12, "end": 2868.3199999999997, "text": " about the uh the hot air balloon contest that's going on today where's a machine learning model", "tokens": [51184, 466, 264, 2232, 264, 2368, 1988, 16994, 10287, 300, 311, 516, 322, 965, 689, 311, 257, 3479, 2539, 2316, 51444], "temperature": 0.0, "avg_logprob": -0.04665089079311916, "compression_ratio": 1.8115384615384615, "no_speech_prob": 0.0017543939175084233}, {"id": 502, "seek": 284672, "start": 2868.3199999999997, "end": 2873.4399999999996, "text": " if it looks out a camera and sees something with a particular shape of lightning bolt you know it", "tokens": [51444, 498, 309, 1542, 484, 257, 2799, 293, 8194, 746, 365, 257, 1729, 3909, 295, 16589, 13436, 291, 458, 309, 51700], "temperature": 0.0, "avg_logprob": -0.04665089079311916, "compression_ratio": 1.8115384615384615, "no_speech_prob": 0.0017543939175084233}, {"id": 503, "seek": 287344, "start": 2873.44, "end": 2877.84, "text": " might just decide it's time to like dive for the ground right and crash the plane like that's", "tokens": [50364, 1062, 445, 4536, 309, 311, 565, 281, 411, 9192, 337, 264, 2727, 558, 293, 8252, 264, 5720, 411, 300, 311, 50584], "temperature": 0.0, "avg_logprob": -0.06568560373215449, "compression_ratio": 1.7407407407407407, "no_speech_prob": 0.0005526957102119923}, {"id": 504, "seek": 287344, "start": 2877.84, "end": 2883.04, "text": " sort of what these adversarial examples kind of show and I think that's why people are really", "tokens": [50584, 1333, 295, 437, 613, 17641, 44745, 5110, 733, 295, 855, 293, 286, 519, 300, 311, 983, 561, 366, 534, 50844], "temperature": 0.0, "avg_logprob": -0.06568560373215449, "compression_ratio": 1.7407407407407407, "no_speech_prob": 0.0005526957102119923}, {"id": 505, "seek": 287344, "start": 2883.04, "end": 2889.28, "text": " hungering for human understandable explanations because still to this day the human brain", "tokens": [50844, 5753, 1794, 337, 1952, 25648, 28708, 570, 920, 281, 341, 786, 264, 1952, 3567, 51156], "temperature": 0.0, "avg_logprob": -0.06568560373215449, "compression_ratio": 1.7407407407407407, "no_speech_prob": 0.0005526957102119923}, {"id": 506, "seek": 287344, "start": 2890.0, "end": 2896.96, "text": " is the only AGI really that that we have around yeah but deep learning models that they they", "tokens": [51192, 307, 264, 787, 316, 26252, 534, 300, 300, 321, 362, 926, 1338, 457, 2452, 2539, 5245, 300, 436, 436, 51540], "temperature": 0.0, "avg_logprob": -0.06568560373215449, "compression_ratio": 1.7407407407407407, "no_speech_prob": 0.0005526957102119923}, {"id": 507, "seek": 287344, "start": 2896.96, "end": 2902.96, "text": " essentially memorize lots and lots of things and they have this sparse coding so in a way it's just", "tokens": [51540, 4476, 27478, 3195, 293, 3195, 295, 721, 293, 436, 362, 341, 637, 11668, 17720, 370, 294, 257, 636, 309, 311, 445, 51840], "temperature": 0.0, "avg_logprob": -0.06568560373215449, "compression_ratio": 1.7407407407407407, "no_speech_prob": 0.0005526957102119923}, {"id": 508, "seek": 290296, "start": 2902.96, "end": 2907.36, "text": " like the white box model even if we use interpretability methods we could enumerate all of the", "tokens": [50364, 411, 264, 2418, 2424, 2316, 754, 498, 321, 764, 7302, 2310, 7150, 321, 727, 465, 15583, 473, 439, 295, 264, 50584], "temperature": 0.0, "avg_logprob": -0.039791689509839086, "compression_ratio": 1.937007874015748, "no_speech_prob": 0.004269931465387344}, {"id": 509, "seek": 290296, "start": 2907.36, "end": 2913.04, "text": " things that they are learning and one of those things might be a sensitivity or lack of sensitivity", "tokens": [50584, 721, 300, 436, 366, 2539, 293, 472, 295, 729, 721, 1062, 312, 257, 19392, 420, 5011, 295, 19392, 50868], "temperature": 0.0, "avg_logprob": -0.039791689509839086, "compression_ratio": 1.937007874015748, "no_speech_prob": 0.004269931465387344}, {"id": 510, "seek": 290296, "start": 2913.04, "end": 2918.16, "text": " to hot air balloons or smiley faces on but even if we could enumerate all the things that they are", "tokens": [50868, 281, 2368, 1988, 26193, 420, 7563, 88, 8475, 322, 457, 754, 498, 321, 727, 465, 15583, 473, 439, 264, 721, 300, 436, 366, 51124], "temperature": 0.0, "avg_logprob": -0.039791689509839086, "compression_ratio": 1.937007874015748, "no_speech_prob": 0.004269931465387344}, {"id": 511, "seek": 290296, "start": 2918.16, "end": 2923.44, "text": " learning we wouldn't understand that either in the same way we don't understand how a real human's", "tokens": [51124, 2539, 321, 2759, 380, 1223, 300, 2139, 294, 264, 912, 636, 321, 500, 380, 1223, 577, 257, 957, 1952, 311, 51388], "temperature": 0.0, "avg_logprob": -0.039791689509839086, "compression_ratio": 1.937007874015748, "no_speech_prob": 0.004269931465387344}, {"id": 512, "seek": 290296, "start": 2923.44, "end": 2928.08, "text": " brain works and I'm not sure whether we should view a human brain as a computer program and whether", "tokens": [51388, 3567, 1985, 293, 286, 478, 406, 988, 1968, 321, 820, 1910, 257, 1952, 3567, 382, 257, 3820, 1461, 293, 1968, 51620], "temperature": 0.0, "avg_logprob": -0.039791689509839086, "compression_ratio": 1.937007874015748, "no_speech_prob": 0.004269931465387344}, {"id": 513, "seek": 292808, "start": 2928.08, "end": 2934.3199999999997, "text": " that's a good thing or a bad thing but at some I guess what I'm saying is at some point we have", "tokens": [50364, 300, 311, 257, 665, 551, 420, 257, 1578, 551, 457, 412, 512, 286, 2041, 437, 286, 478, 1566, 307, 412, 512, 935, 321, 362, 50676], "temperature": 0.0, "avg_logprob": -0.06978571856463398, "compression_ratio": 1.8963210702341138, "no_speech_prob": 0.020903777331113815}, {"id": 514, "seek": 292808, "start": 2934.3199999999997, "end": 2939.6, "text": " to accept that we're not going to understand a totally it's actually it's a good comparison", "tokens": [50676, 281, 3241, 300, 321, 434, 406, 516, 281, 1223, 257, 3879, 309, 311, 767, 309, 311, 257, 665, 9660, 50940], "temperature": 0.0, "avg_logprob": -0.06978571856463398, "compression_ratio": 1.8963210702341138, "no_speech_prob": 0.020903777331113815}, {"id": 515, "seek": 292808, "start": 2939.6, "end": 2944.3199999999997, "text": " I think with humans you know if you're interviewing and you want to hire let's say a software developer", "tokens": [50940, 286, 519, 365, 6255, 291, 458, 498, 291, 434, 26524, 293, 291, 528, 281, 11158, 718, 311, 584, 257, 4722, 10754, 51176], "temperature": 0.0, "avg_logprob": -0.06978571856463398, "compression_ratio": 1.8963210702341138, "no_speech_prob": 0.020903777331113815}, {"id": 516, "seek": 292808, "start": 2945.04, "end": 2949.2, "text": " you tend to set them a coding interview you wouldn't think of taking them into surgery", "tokens": [51212, 291, 3928, 281, 992, 552, 257, 17720, 4049, 291, 2759, 380, 519, 295, 1940, 552, 666, 7930, 51420], "temperature": 0.0, "avg_logprob": -0.06978571856463398, "compression_ratio": 1.8963210702341138, "no_speech_prob": 0.020903777331113815}, {"id": 517, "seek": 292808, "start": 2949.2, "end": 2952.88, "text": " opening up their brain and trying to find the neuron that predicts what the next you know", "tokens": [51420, 5193, 493, 641, 3567, 293, 1382, 281, 915, 264, 34090, 300, 6069, 82, 437, 264, 958, 291, 458, 51604], "temperature": 0.0, "avg_logprob": -0.06978571856463398, "compression_ratio": 1.8963210702341138, "no_speech_prob": 0.020903777331113815}, {"id": 518, "seek": 292808, "start": 2952.88, "end": 2957.2, "text": " but if code is going to be and understanding how that works it's just why would you do it that way", "tokens": [51604, 457, 498, 3089, 307, 516, 281, 312, 293, 3701, 577, 300, 1985, 309, 311, 445, 983, 576, 291, 360, 309, 300, 636, 51820], "temperature": 0.0, "avg_logprob": -0.06978571856463398, "compression_ratio": 1.8963210702341138, "no_speech_prob": 0.020903777331113815}, {"id": 519, "seek": 295720, "start": 2957.2, "end": 2962.0, "text": " instead you learn to trust humans by working with them giving them a test seeing how they", "tokens": [50364, 2602, 291, 1466, 281, 3361, 6255, 538, 1364, 365, 552, 2902, 552, 257, 1500, 2577, 577, 436, 50604], "temperature": 0.0, "avg_logprob": -0.10933767424689399, "compression_ratio": 1.7613636363636365, "no_speech_prob": 0.0020155857782810926}, {"id": 520, "seek": 295720, "start": 2962.0, "end": 2967.12, "text": " perform in the real world and I know maybe we're asking too much of a machine learning model if", "tokens": [50604, 2042, 294, 264, 957, 1002, 293, 286, 458, 1310, 321, 434, 3365, 886, 709, 295, 257, 3479, 2539, 2316, 498, 50860], "temperature": 0.0, "avg_logprob": -0.10933767424689399, "compression_ratio": 1.7613636363636365, "no_speech_prob": 0.0020155857782810926}, {"id": 521, "seek": 295720, "start": 2967.12, "end": 2972.72, "text": " you want to be able to understand these complex things in terms of like a bottom-up white box", "tokens": [50860, 291, 528, 281, 312, 1075, 281, 1223, 613, 3997, 721, 294, 2115, 295, 411, 257, 2767, 12, 1010, 2418, 2424, 51140], "temperature": 0.0, "avg_logprob": -0.10933767424689399, "compression_ratio": 1.7613636363636365, "no_speech_prob": 0.0020155857782810926}, {"id": 522, "seek": 295720, "start": 2973.2799999999997, "end": 2979.2799999999997, "text": " set of rules. I think what a comparison falls a bit short is that that we have the luxury that we can", "tokens": [51168, 992, 295, 4474, 13, 286, 519, 437, 257, 9660, 8804, 257, 857, 2099, 307, 300, 300, 321, 362, 264, 15558, 300, 321, 393, 51468], "temperature": 0.0, "avg_logprob": -0.10933767424689399, "compression_ratio": 1.7613636363636365, "no_speech_prob": 0.0020155857782810926}, {"id": 523, "seek": 295720, "start": 2980.3999999999996, "end": 2985.2, "text": " cut off the like cut open the brain of a machine learning model without breaking it", "tokens": [51524, 1723, 766, 264, 411, 1723, 1269, 264, 3567, 295, 257, 3479, 2539, 2316, 1553, 7697, 309, 51764], "temperature": 0.0, "avg_logprob": -0.10933767424689399, "compression_ratio": 1.7613636363636365, "no_speech_prob": 0.0020155857782810926}, {"id": 524, "seek": 298520, "start": 2985.2, "end": 2994.16, "text": " and without hurting it hopefully and we can do all these try out all these interpretation methods", "tokens": [50364, 293, 1553, 17744, 309, 4696, 293, 321, 393, 360, 439, 613, 853, 484, 439, 613, 14174, 7150, 50812], "temperature": 0.0, "avg_logprob": -0.0831105207142077, "compression_ratio": 1.7136150234741785, "no_speech_prob": 0.0013248041504994035}, {"id": 525, "seek": 298520, "start": 2994.16, "end": 2999.2799999999997, "text": " see how it behaves under certain situations and I also would make a distinction between", "tokens": [50812, 536, 577, 309, 36896, 833, 1629, 6851, 293, 286, 611, 576, 652, 257, 16844, 1296, 51068], "temperature": 0.0, "avg_logprob": -0.0831105207142077, "compression_ratio": 1.7136150234741785, "no_speech_prob": 0.0013248041504994035}, {"id": 526, "seek": 298520, "start": 2999.2799999999997, "end": 3004.7999999999997, "text": " we understand what's going on inside and doing like this kind of sensitivity analysis", "tokens": [51068, 321, 1223, 437, 311, 516, 322, 1854, 293, 884, 411, 341, 733, 295, 19392, 5215, 51344], "temperature": 0.0, "avg_logprob": -0.0831105207142077, "compression_ratio": 1.7136150234741785, "no_speech_prob": 0.0013248041504994035}, {"id": 527, "seek": 298520, "start": 3004.7999999999997, "end": 3011.3599999999997, "text": " where we just try out what happens in certain scenarios so it always do that that we can like", "tokens": [51344, 689, 321, 445, 853, 484, 437, 2314, 294, 1629, 15077, 370, 309, 1009, 360, 300, 300, 321, 393, 411, 51672], "temperature": 0.0, "avg_logprob": -0.0831105207142077, "compression_ratio": 1.7136150234741785, "no_speech_prob": 0.0013248041504994035}, {"id": 528, "seek": 301136, "start": 3012.32, "end": 3017.6800000000003, "text": " check like how it behaves so I mean feature importance is basically like a way to see", "tokens": [50412, 1520, 411, 577, 309, 36896, 370, 286, 914, 4111, 7379, 307, 1936, 411, 257, 636, 281, 536, 50680], "temperature": 0.0, "avg_logprob": -0.09929686046781994, "compression_ratio": 1.8023715415019763, "no_speech_prob": 0.011997881345450878}, {"id": 529, "seek": 301136, "start": 3017.6800000000003, "end": 3021.76, "text": " like how does it behave if we break some features and then we rank the features by this", "tokens": [50680, 411, 577, 775, 309, 15158, 498, 321, 1821, 512, 4122, 293, 550, 321, 6181, 264, 4122, 538, 341, 50884], "temperature": 0.0, "avg_logprob": -0.09929686046781994, "compression_ratio": 1.8023715415019763, "no_speech_prob": 0.011997881345450878}, {"id": 530, "seek": 301136, "start": 3021.76, "end": 3028.0, "text": " as an importance we can do it and that's also the big difference between humans because we", "tokens": [50884, 382, 364, 7379, 321, 393, 360, 309, 293, 300, 311, 611, 264, 955, 2649, 1296, 6255, 570, 321, 51196], "temperature": 0.0, "avg_logprob": -0.09929686046781994, "compression_ratio": 1.8023715415019763, "no_speech_prob": 0.011997881345450878}, {"id": 531, "seek": 301136, "start": 3028.0, "end": 3033.52, "text": " can't test in the same way and yeah shouldn't probably. I think there's one other difference", "tokens": [51196, 393, 380, 1500, 294, 264, 912, 636, 293, 1338, 4659, 380, 1391, 13, 286, 519, 456, 311, 472, 661, 2649, 51472], "temperature": 0.0, "avg_logprob": -0.09929686046781994, "compression_ratio": 1.8023715415019763, "no_speech_prob": 0.011997881345450878}, {"id": 532, "seek": 301136, "start": 3033.52, "end": 3039.2000000000003, "text": " though which is to do with the substrate of how neural networks work I think if I'm giving someone", "tokens": [51472, 1673, 597, 307, 281, 360, 365, 264, 27585, 295, 577, 18161, 9590, 589, 286, 519, 498, 286, 478, 2902, 1580, 51756], "temperature": 0.0, "avg_logprob": -0.09929686046781994, "compression_ratio": 1.8023715415019763, "no_speech_prob": 0.011997881345450878}, {"id": 533, "seek": 303920, "start": 3039.2, "end": 3044.96, "text": " a job interview or something I mean of course it's a very valuable process but I'm looking at", "tokens": [50364, 257, 1691, 4049, 420, 746, 286, 914, 295, 1164, 309, 311, 257, 588, 8263, 1399, 457, 286, 478, 1237, 412, 50652], "temperature": 0.0, "avg_logprob": -0.04652985242696909, "compression_ratio": 1.7923076923076924, "no_speech_prob": 0.018815474584698677}, {"id": 534, "seek": 303920, "start": 3044.96, "end": 3049.3599999999997, "text": " their values and I'm looking to try and understand how they would behave in different situations", "tokens": [50652, 641, 4190, 293, 286, 478, 1237, 281, 853, 293, 1223, 577, 436, 576, 15158, 294, 819, 6851, 50872], "temperature": 0.0, "avg_logprob": -0.04652985242696909, "compression_ratio": 1.7923076923076924, "no_speech_prob": 0.018815474584698677}, {"id": 535, "seek": 303920, "start": 3049.3599999999997, "end": 3053.9199999999996, "text": " and I'm coming up with lots of illustrative examples but the difference is with humans", "tokens": [50872, 293, 286, 478, 1348, 493, 365, 3195, 295, 8490, 30457, 5110, 457, 264, 2649, 307, 365, 6255, 51100], "temperature": 0.0, "avg_logprob": -0.04652985242696909, "compression_ratio": 1.7923076923076924, "no_speech_prob": 0.018815474584698677}, {"id": 536, "seek": 303920, "start": 3053.9199999999996, "end": 3061.12, "text": " we have that level of generalization we have a kind of guiding taxonomy of behaviors which means", "tokens": [51100, 321, 362, 300, 1496, 295, 2674, 2144, 321, 362, 257, 733, 295, 25061, 3366, 23423, 295, 15501, 597, 1355, 51460], "temperature": 0.0, "avg_logprob": -0.04652985242696909, "compression_ratio": 1.7923076923076924, "no_speech_prob": 0.018815474584698677}, {"id": 537, "seek": 303920, "start": 3061.12, "end": 3067.04, "text": " if I know if I have guiding examples of what a human will do in certain situations I expect", "tokens": [51460, 498, 286, 458, 498, 286, 362, 25061, 5110, 295, 437, 257, 1952, 486, 360, 294, 1629, 6851, 286, 2066, 51756], "temperature": 0.0, "avg_logprob": -0.04652985242696909, "compression_ratio": 1.7923076923076924, "no_speech_prob": 0.018815474584698677}, {"id": 538, "seek": 306704, "start": 3067.04, "end": 3072.72, "text": " that to generalize whereas my hypothesis is is that a deep neural network model is almost like", "tokens": [50364, 300, 281, 2674, 1125, 9735, 452, 17291, 307, 307, 300, 257, 2452, 18161, 3209, 2316, 307, 1920, 411, 50648], "temperature": 0.0, "avg_logprob": -0.07495134785061791, "compression_ratio": 1.6266666666666667, "no_speech_prob": 0.011258989572525024}, {"id": 539, "seek": 306704, "start": 3072.72, "end": 3077.44, "text": " an infinite number of rules and there's absolutely no carryover between the rules", "tokens": [50648, 364, 13785, 1230, 295, 4474, 293, 456, 311, 3122, 572, 3985, 3570, 1296, 264, 4474, 50884], "temperature": 0.0, "avg_logprob": -0.07495134785061791, "compression_ratio": 1.6266666666666667, "no_speech_prob": 0.011258989572525024}, {"id": 540, "seek": 306704, "start": 3077.44, "end": 3083.36, "text": " so knowing even some or even most of the rules doesn't really tell me about those edge cases.", "tokens": [50884, 370, 5276, 754, 512, 420, 754, 881, 295, 264, 4474, 1177, 380, 534, 980, 385, 466, 729, 4691, 3331, 13, 51180], "temperature": 0.0, "avg_logprob": -0.07495134785061791, "compression_ratio": 1.6266666666666667, "no_speech_prob": 0.011258989572525024}, {"id": 541, "seek": 306704, "start": 3085.92, "end": 3092.0, "text": " Yeah I would agree that the edge cases are quite unforeseeable probably I mean at least we know", "tokens": [51308, 865, 286, 576, 3986, 300, 264, 4691, 3331, 366, 1596, 517, 845, 17109, 712, 1391, 286, 914, 412, 1935, 321, 458, 51612], "temperature": 0.0, "avg_logprob": -0.07495134785061791, "compression_ratio": 1.6266666666666667, "no_speech_prob": 0.011258989572525024}, {"id": 542, "seek": 309200, "start": 3092.0, "end": 3097.28, "text": " that they exist like with adversarial examples so even if we don't know like exactly what they", "tokens": [50364, 300, 436, 2514, 411, 365, 17641, 44745, 5110, 370, 754, 498, 321, 500, 380, 458, 411, 2293, 437, 436, 50628], "temperature": 0.0, "avg_logprob": -0.07639755086695894, "compression_ratio": 1.7818181818181817, "no_speech_prob": 0.013846197165548801}, {"id": 543, "seek": 309200, "start": 3097.28, "end": 3102.8, "text": " would look like or there's even an infinite amount of uh I mean there's an infinite amount of like", "tokens": [50628, 576, 574, 411, 420, 456, 311, 754, 364, 13785, 2372, 295, 2232, 286, 914, 456, 311, 364, 13785, 2372, 295, 411, 50904], "temperature": 0.0, "avg_logprob": -0.07639755086695894, "compression_ratio": 1.7818181818181817, "no_speech_prob": 0.013846197165548801}, {"id": 544, "seek": 309200, "start": 3102.8, "end": 3110.48, "text": " how you can change the image to make it like have a different class so we know so I think it's important", "tokens": [50904, 577, 291, 393, 1319, 264, 3256, 281, 652, 309, 411, 362, 257, 819, 1508, 370, 321, 458, 370, 286, 519, 309, 311, 1021, 51288], "temperature": 0.0, "avg_logprob": -0.07639755086695894, "compression_ratio": 1.7818181818181817, "no_speech_prob": 0.013846197165548801}, {"id": 545, "seek": 309200, "start": 3110.48, "end": 3116.48, "text": " that we know that these exist at least. Yeah I love the point you made though that we have we", "tokens": [51288, 300, 321, 458, 300, 613, 2514, 412, 1935, 13, 865, 286, 959, 264, 935, 291, 1027, 1673, 300, 321, 362, 321, 51588], "temperature": 0.0, "avg_logprob": -0.07639755086695894, "compression_ratio": 1.7818181818181817, "no_speech_prob": 0.013846197165548801}, {"id": 546, "seek": 311648, "start": 3116.48, "end": 3122.64, "text": " have the luxury to do analysis on these on these boxes because we can open them up and that that's", "tokens": [50364, 362, 264, 15558, 281, 360, 5215, 322, 613, 322, 613, 9002, 570, 321, 393, 1269, 552, 493, 293, 300, 300, 311, 50672], "temperature": 0.0, "avg_logprob": -0.06888027985890706, "compression_ratio": 1.673728813559322, "no_speech_prob": 0.02296786569058895}, {"id": 547, "seek": 311648, "start": 3122.64, "end": 3126.88, "text": " another point I'm pretty sure that you make this in your book as well which is that part of this is", "tokens": [50672, 1071, 935, 286, 478, 1238, 988, 300, 291, 652, 341, 294, 428, 1446, 382, 731, 597, 307, 300, 644, 295, 341, 307, 50884], "temperature": 0.0, "avg_logprob": -0.06888027985890706, "compression_ratio": 1.673728813559322, "no_speech_prob": 0.02296786569058895}, {"id": 548, "seek": 311648, "start": 3126.88, "end": 3134.2400000000002, "text": " just scientific inquiry it's it's like understanding better how to interpret and explain machine", "tokens": [50884, 445, 8134, 25736, 309, 311, 309, 311, 411, 3701, 1101, 577, 281, 7302, 293, 2903, 3479, 51252], "temperature": 0.0, "avg_logprob": -0.06888027985890706, "compression_ratio": 1.673728813559322, "no_speech_prob": 0.02296786569058895}, {"id": 549, "seek": 311648, "start": 3134.2400000000002, "end": 3140.16, "text": " learning models will probably actually contribute to us being able to construct even better machine", "tokens": [51252, 2539, 5245, 486, 1391, 767, 10586, 281, 505, 885, 1075, 281, 7690, 754, 1101, 3479, 51548], "temperature": 0.0, "avg_logprob": -0.06888027985890706, "compression_ratio": 1.673728813559322, "no_speech_prob": 0.02296786569058895}, {"id": 550, "seek": 314016, "start": 3140.16, "end": 3145.52, "text": " learning models isn't that true? So so you're basically saying that um also interpretability", "tokens": [50364, 2539, 5245, 1943, 380, 300, 2074, 30, 407, 370, 291, 434, 1936, 1566, 300, 1105, 611, 7302, 2310, 50632], "temperature": 0.0, "avg_logprob": -0.0936100258017486, "compression_ratio": 1.8106060606060606, "no_speech_prob": 0.0715608298778534}, {"id": 551, "seek": 314016, "start": 3145.52, "end": 3151.12, "text": " might help to to be better at like create better machine learning models themselves?", "tokens": [50632, 1062, 854, 281, 281, 312, 1101, 412, 411, 1884, 1101, 3479, 2539, 5245, 2969, 30, 50912], "temperature": 0.0, "avg_logprob": -0.0936100258017486, "compression_ratio": 1.8106060606060606, "no_speech_prob": 0.0715608298778534}, {"id": 552, "seek": 314016, "start": 3152.24, "end": 3157.2, "text": " Yeah like as we as we develop these interpretability methods because in a sense like you point out", "tokens": [50968, 865, 411, 382, 321, 382, 321, 1499, 613, 7302, 2310, 7150, 570, 294, 257, 2020, 411, 291, 935, 484, 51216], "temperature": 0.0, "avg_logprob": -0.0936100258017486, "compression_ratio": 1.8106060606060606, "no_speech_prob": 0.0715608298778534}, {"id": 553, "seek": 314016, "start": 3157.2, "end": 3163.04, "text": " earlier there are statistical projections of kind of the behavior of the model and so like a saliency", "tokens": [51216, 3071, 456, 366, 22820, 32371, 295, 733, 295, 264, 5223, 295, 264, 2316, 293, 370, 411, 257, 1845, 7848, 51508], "temperature": 0.0, "avg_logprob": -0.0936100258017486, "compression_ratio": 1.8106060606060606, "no_speech_prob": 0.0715608298778534}, {"id": 554, "seek": 314016, "start": 3163.04, "end": 3168.48, "text": " map you know if we can if we can kind of use that to learn the way in which the the neural networks", "tokens": [51508, 4471, 291, 458, 498, 321, 393, 498, 321, 393, 733, 295, 764, 300, 281, 1466, 264, 636, 294, 597, 264, 264, 18161, 9590, 51780], "temperature": 0.0, "avg_logprob": -0.0936100258017486, "compression_ratio": 1.8106060606060606, "no_speech_prob": 0.0715608298778534}, {"id": 555, "seek": 316848, "start": 3168.48, "end": 3174.72, "text": " are behaving it may it can certainly give rise to intuitions on ways to alter the model. Yeah so I", "tokens": [50364, 366, 35263, 309, 815, 309, 393, 3297, 976, 6272, 281, 16224, 626, 322, 2098, 281, 11337, 264, 2316, 13, 865, 370, 286, 50676], "temperature": 0.0, "avg_logprob": -0.11605273593555797, "compression_ratio": 1.722466960352423, "no_speech_prob": 0.0018671929137781262}, {"id": 556, "seek": 316848, "start": 3174.72, "end": 3181.92, "text": " also have seen approaches where they try like kind of fuse also these two worlds like interpretative", "tokens": [50676, 611, 362, 1612, 11587, 689, 436, 853, 411, 733, 295, 31328, 611, 613, 732, 13401, 411, 7302, 1166, 51036], "temperature": 0.0, "avg_logprob": -0.11605273593555797, "compression_ratio": 1.722466960352423, "no_speech_prob": 0.0018671929137781262}, {"id": 557, "seek": 316848, "start": 3181.92, "end": 3186.88, "text": " models or white box models and black box models so that you try to to generate features out of", "tokens": [51036, 5245, 420, 2418, 2424, 5245, 293, 2211, 2424, 5245, 370, 300, 291, 853, 281, 281, 8460, 4122, 484, 295, 51284], "temperature": 0.0, "avg_logprob": -0.11605273593555797, "compression_ratio": 1.722466960352423, "no_speech_prob": 0.0018671929137781262}, {"id": 558, "seek": 316848, "start": 3186.88, "end": 3194.96, "text": " the black box model which you then use in your more understandable white box model so um I think", "tokens": [51284, 264, 2211, 2424, 2316, 597, 291, 550, 764, 294, 428, 544, 25648, 2418, 2424, 2316, 370, 1105, 286, 519, 51688], "temperature": 0.0, "avg_logprob": -0.11605273593555797, "compression_ratio": 1.722466960352423, "no_speech_prob": 0.0018671929137781262}, {"id": 559, "seek": 319496, "start": 3195.04, "end": 3201.44, "text": " this and and this also like by using similar techniques um to which you would use for interpretability", "tokens": [50368, 341, 293, 293, 341, 611, 411, 538, 1228, 2531, 7512, 1105, 281, 597, 291, 576, 764, 337, 7302, 2310, 50688], "temperature": 0.0, "avg_logprob": -0.1320259659378617, "compression_ratio": 1.816425120772947, "no_speech_prob": 0.0008692774572409689}, {"id": 560, "seek": 319496, "start": 3201.44, "end": 3208.08, "text": " like detecting interactions for examples for example so um yeah these can be used also to", "tokens": [50688, 411, 40237, 13280, 337, 5110, 337, 1365, 370, 1105, 1338, 613, 393, 312, 1143, 611, 281, 51020], "temperature": 0.0, "avg_logprob": -0.1320259659378617, "compression_ratio": 1.816425120772947, "no_speech_prob": 0.0008692774572409689}, {"id": 561, "seek": 319496, "start": 3208.08, "end": 3214.4, "text": " to build better models and also to build better interpretable models. The other and I'll make", "tokens": [51020, 281, 1322, 1101, 5245, 293, 611, 281, 1322, 1101, 7302, 712, 5245, 13, 440, 661, 293, 286, 603, 652, 51336], "temperature": 0.0, "avg_logprob": -0.1320259659378617, "compression_ratio": 1.816425120772947, "no_speech_prob": 0.0008692774572409689}, {"id": 562, "seek": 319496, "start": 3214.4, "end": 3220.16, "text": " one last point here which is another social good that can come out of interpretability is", "tokens": [51336, 472, 1036, 935, 510, 597, 307, 1071, 2093, 665, 300, 393, 808, 484, 295, 7302, 2310, 307, 51624], "temperature": 0.0, "avg_logprob": -0.1320259659378617, "compression_ratio": 1.816425120772947, "no_speech_prob": 0.0008692774572409689}, {"id": 563, "seek": 322016, "start": 3220.16, "end": 3225.44, "text": " imagine we've got you know an ML model that's not trying to make any decisions but it's just", "tokens": [50364, 3811, 321, 600, 658, 291, 458, 364, 21601, 2316, 300, 311, 406, 1382, 281, 652, 604, 5327, 457, 309, 311, 445, 50628], "temperature": 0.0, "avg_logprob": -0.04696728099476207, "compression_ratio": 1.821969696969697, "no_speech_prob": 0.0015485817566514015}, {"id": 564, "seek": 322016, "start": 3225.44, "end": 3232.08, "text": " trying to figure out what leads to happiness and success in life you know and so we analyze a whole", "tokens": [50628, 1382, 281, 2573, 484, 437, 6689, 281, 8324, 293, 2245, 294, 993, 291, 458, 293, 370, 321, 12477, 257, 1379, 50960], "temperature": 0.0, "avg_logprob": -0.04696728099476207, "compression_ratio": 1.821969696969697, "no_speech_prob": 0.0015485817566514015}, {"id": 565, "seek": 322016, "start": 3232.08, "end": 3237.3599999999997, "text": " bunch of data and we find out well it's really important if you graduate from high school and", "tokens": [50960, 3840, 295, 1412, 293, 321, 915, 484, 731, 309, 311, 534, 1021, 498, 291, 8080, 490, 1090, 1395, 293, 51224], "temperature": 0.0, "avg_logprob": -0.04696728099476207, "compression_ratio": 1.821969696969697, "no_speech_prob": 0.0015485817566514015}, {"id": 566, "seek": 322016, "start": 3237.3599999999997, "end": 3241.68, "text": " it's really important if you you know don't have children before you're married and you know all", "tokens": [51224, 309, 311, 534, 1021, 498, 291, 291, 458, 500, 380, 362, 2227, 949, 291, 434, 5259, 293, 291, 458, 439, 51440], "temperature": 0.0, "avg_logprob": -0.04696728099476207, "compression_ratio": 1.821969696969697, "no_speech_prob": 0.0015485817566514015}, {"id": 567, "seek": 322016, "start": 3241.68, "end": 3247.44, "text": " these other factors if we can dive in and kind of isolate those factors it actually allows people", "tokens": [51440, 613, 661, 6771, 498, 321, 393, 9192, 294, 293, 733, 295, 25660, 729, 6771, 309, 767, 4045, 561, 51728], "temperature": 0.0, "avg_logprob": -0.04696728099476207, "compression_ratio": 1.821969696969697, "no_speech_prob": 0.0015485817566514015}, {"id": 568, "seek": 324744, "start": 3247.44, "end": 3252.32, "text": " to have some guidance on oh look we've had this machine learning model that's analyzed a bunch of", "tokens": [50364, 281, 362, 512, 10056, 322, 1954, 574, 321, 600, 632, 341, 3479, 2539, 2316, 300, 311, 28181, 257, 3840, 295, 50608], "temperature": 0.0, "avg_logprob": -0.08042923983405618, "compression_ratio": 1.6324786324786325, "no_speech_prob": 0.006486272905021906}, {"id": 569, "seek": 324744, "start": 3252.32, "end": 3257.92, "text": " data and it actually has some some understandable recommendations for how to lead a healthier", "tokens": [50608, 1412, 293, 309, 767, 575, 512, 512, 25648, 10434, 337, 577, 281, 1477, 257, 19580, 50888], "temperature": 0.0, "avg_logprob": -0.08042923983405618, "compression_ratio": 1.6324786324786325, "no_speech_prob": 0.006486272905021906}, {"id": 570, "seek": 324744, "start": 3257.92, "end": 3264.0, "text": " life or a better life or whatever. Yeah I think that this very good example were the fact that", "tokens": [50888, 993, 420, 257, 1101, 993, 420, 2035, 13, 865, 286, 519, 300, 341, 588, 665, 1365, 645, 264, 1186, 300, 51192], "temperature": 0.0, "avg_logprob": -0.08042923983405618, "compression_ratio": 1.6324786324786325, "no_speech_prob": 0.006486272905021906}, {"id": 571, "seek": 324744, "start": 3264.0, "end": 3271.76, "text": " you have a prediction model doesn't solve your problem so actually it's just a means to to some", "tokens": [51192, 291, 362, 257, 17630, 2316, 1177, 380, 5039, 428, 1154, 370, 767, 309, 311, 445, 257, 1355, 281, 281, 512, 51580], "temperature": 0.0, "avg_logprob": -0.08042923983405618, "compression_ratio": 1.6324786324786325, "no_speech_prob": 0.006486272905021906}, {"id": 572, "seek": 327176, "start": 3271.84, "end": 3278.6400000000003, "text": " other goal in this case understanding like what are the factors for happiness and one example I am", "tokens": [50368, 661, 3387, 294, 341, 1389, 3701, 411, 437, 366, 264, 6771, 337, 8324, 293, 472, 1365, 286, 669, 50708], "temperature": 0.0, "avg_logprob": -0.11217255592346191, "compression_ratio": 1.705069124423963, "no_speech_prob": 0.12417126446962357}, {"id": 573, "seek": 327176, "start": 3278.6400000000003, "end": 3282.8, "text": " from a friend who worked at a telecom company and they built like a churn prediction model", "tokens": [50708, 490, 257, 1277, 567, 2732, 412, 257, 4304, 1112, 2237, 293, 436, 3094, 411, 257, 417, 925, 17630, 2316, 50916], "temperature": 0.0, "avg_logprob": -0.11217255592346191, "compression_ratio": 1.705069124423963, "no_speech_prob": 0.12417126446962357}, {"id": 574, "seek": 327176, "start": 3284.0800000000004, "end": 3291.92, "text": " to see like who will quit the telecom contract and then they started like the ones with the", "tokens": [50980, 281, 536, 411, 567, 486, 10366, 264, 4304, 1112, 4364, 293, 550, 436, 1409, 411, 264, 2306, 365, 264, 51372], "temperature": 0.0, "avg_logprob": -0.11217255592346191, "compression_ratio": 1.705069124423963, "no_speech_prob": 0.12417126446962357}, {"id": 575, "seek": 327176, "start": 3291.92, "end": 3296.6400000000003, "text": " highest likelihood they started sending out emails say maybe offering them a better deal", "tokens": [51372, 6343, 22119, 436, 1409, 7750, 484, 12524, 584, 1310, 8745, 552, 257, 1101, 2028, 51608], "temperature": 0.0, "avg_logprob": -0.11217255592346191, "compression_ratio": 1.705069124423963, "no_speech_prob": 0.12417126446962357}, {"id": 576, "seek": 329664, "start": 3297.44, "end": 3304.48, "text": " but actually the outcome was that well they they when they once they wrote to the customers they", "tokens": [50404, 457, 767, 264, 9700, 390, 300, 731, 436, 436, 562, 436, 1564, 436, 4114, 281, 264, 4581, 436, 50756], "temperature": 0.0, "avg_logprob": -0.13054524398431544, "compression_ratio": 1.7264150943396226, "no_speech_prob": 0.004754187539219856}, {"id": 577, "seek": 329664, "start": 3304.48, "end": 3311.92, "text": " well left and quit their contract so it's kind of had like so this is a case where the prediction", "tokens": [50756, 731, 1411, 293, 10366, 641, 4364, 370, 309, 311, 733, 295, 632, 411, 370, 341, 307, 257, 1389, 689, 264, 17630, 51128], "temperature": 0.0, "avg_logprob": -0.13054524398431544, "compression_ratio": 1.7264150943396226, "no_speech_prob": 0.004754187539219856}, {"id": 578, "seek": 329664, "start": 3311.92, "end": 3316.24, "text": " model actually works but then they people leave in this case probably because they", "tokens": [51128, 2316, 767, 1985, 457, 550, 436, 561, 1856, 294, 341, 1389, 1391, 570, 436, 51344], "temperature": 0.0, "avg_logprob": -0.13054524398431544, "compression_ratio": 1.7264150943396226, "no_speech_prob": 0.004754187539219856}, {"id": 579, "seek": 329664, "start": 3316.24, "end": 3323.92, "text": " realized ah shit I have this contract still going on time to quit now so if you knew the", "tokens": [51344, 5334, 3716, 4611, 286, 362, 341, 4364, 920, 516, 322, 565, 281, 10366, 586, 370, 498, 291, 2586, 264, 51728], "temperature": 0.0, "avg_logprob": -0.13054524398431544, "compression_ratio": 1.7264150943396226, "no_speech_prob": 0.004754187539219856}, {"id": 580, "seek": 332392, "start": 3323.92, "end": 3330.8, "text": " reasons why they are likely to churn then you could like better select like when you write some", "tokens": [50364, 4112, 983, 436, 366, 3700, 281, 417, 925, 550, 291, 727, 411, 1101, 3048, 411, 562, 291, 2464, 512, 50708], "temperature": 0.0, "avg_logprob": -0.10008548596583375, "compression_ratio": 1.7380073800738007, "no_speech_prob": 0.0037971269339323044}, {"id": 581, "seek": 332392, "start": 3330.8, "end": 3337.04, "text": " email or maybe some other campaign or when maybe not to write anything at all yeah right now um", "tokens": [50708, 3796, 420, 1310, 512, 661, 5129, 420, 562, 1310, 406, 281, 2464, 1340, 412, 439, 1338, 558, 586, 1105, 51020], "temperature": 0.0, "avg_logprob": -0.10008548596583375, "compression_ratio": 1.7380073800738007, "no_speech_prob": 0.0037971269339323044}, {"id": 582, "seek": 332392, "start": 3337.04, "end": 3342.08, "text": " Christoph you have a background in stats which means you I mean you like Connor as well we take", "tokens": [51020, 2040, 5317, 291, 362, 257, 3678, 294, 18152, 597, 1355, 291, 286, 914, 291, 411, 33133, 382, 731, 321, 747, 51272], "temperature": 0.0, "avg_logprob": -0.10008548596583375, "compression_ratio": 1.7380073800738007, "no_speech_prob": 0.0037971269339323044}, {"id": 583, "seek": 332392, "start": 3342.08, "end": 3347.36, "text": " an incredibly dim view of machine learning and you wonder how how is it possible for us to be", "tokens": [51272, 364, 6252, 5013, 1910, 295, 3479, 2539, 293, 291, 2441, 577, 577, 307, 309, 1944, 337, 505, 281, 312, 51536], "temperature": 0.0, "avg_logprob": -0.10008548596583375, "compression_ratio": 1.7380073800738007, "no_speech_prob": 0.0037971269339323044}, {"id": 584, "seek": 332392, "start": 3347.36, "end": 3351.92, "text": " stabbing in the dark like this but you know you said that we need to be more rigorous and", "tokens": [51536, 16343, 4324, 294, 264, 2877, 411, 341, 457, 291, 458, 291, 848, 300, 321, 643, 281, 312, 544, 29882, 293, 51764], "temperature": 0.0, "avg_logprob": -0.10008548596583375, "compression_ratio": 1.7380073800738007, "no_speech_prob": 0.0037971269339323044}, {"id": 585, "seek": 335192, "start": 3352.0, "end": 3357.52, "text": " there's no quantification of uncertainty with the current IML methods and I suspect you might", "tokens": [50368, 456, 311, 572, 4426, 3774, 295, 15697, 365, 264, 2190, 286, 12683, 7150, 293, 286, 9091, 291, 1062, 50644], "temperature": 0.0, "avg_logprob": -0.06262292937626915, "compression_ratio": 1.7836990595611286, "no_speech_prob": 0.01119033433496952}, {"id": 586, "seek": 335192, "start": 3357.52, "end": 3361.84, "text": " be working on some methods behind the the scenes on this but you know when you have models and", "tokens": [50644, 312, 1364, 322, 512, 7150, 2261, 264, 264, 8026, 322, 341, 457, 291, 458, 562, 291, 362, 5245, 293, 50860], "temperature": 0.0, "avg_logprob": -0.06262292937626915, "compression_ratio": 1.7836990595611286, "no_speech_prob": 0.01119033433496952}, {"id": 587, "seek": 335192, "start": 3361.84, "end": 3367.28, "text": " explanations which are computed from data they are subject to uncertainty and that's just not", "tokens": [50860, 28708, 597, 366, 40610, 490, 1412, 436, 366, 3983, 281, 15697, 293, 300, 311, 445, 406, 51132], "temperature": 0.0, "avg_logprob": -0.06262292937626915, "compression_ratio": 1.7836990595611286, "no_speech_prob": 0.01119033433496952}, {"id": 588, "seek": 335192, "start": 3367.28, "end": 3371.76, "text": " modeled at all at the moment right so we need to be making some distributional and structural", "tokens": [51132, 37140, 412, 439, 412, 264, 1623, 558, 370, 321, 643, 281, 312, 1455, 512, 7316, 304, 293, 15067, 51356], "temperature": 0.0, "avg_logprob": -0.06262292937626915, "compression_ratio": 1.7836990595611286, "no_speech_prob": 0.01119033433496952}, {"id": 589, "seek": 335192, "start": 3371.76, "end": 3375.92, "text": " assumptions that we're not making now and you point out that there's this phenomenon of p-hacking", "tokens": [51356, 17695, 300, 321, 434, 406, 1455, 586, 293, 291, 935, 484, 300, 456, 311, 341, 14029, 295, 280, 12, 71, 14134, 51564], "temperature": 0.0, "avg_logprob": -0.06262292937626915, "compression_ratio": 1.7836990595611286, "no_speech_prob": 0.01119033433496952}, {"id": 590, "seek": 335192, "start": 3375.92, "end": 3380.32, "text": " which is a huge problem in the natural sciences which hasn't quite made its way to IML methods", "tokens": [51564, 597, 307, 257, 2603, 1154, 294, 264, 3303, 17677, 597, 6132, 380, 1596, 1027, 1080, 636, 281, 286, 12683, 7150, 51784], "temperature": 0.0, "avg_logprob": -0.06262292937626915, "compression_ratio": 1.7836990595611286, "no_speech_prob": 0.01119033433496952}, {"id": 591, "seek": 338032, "start": 3380.32, "end": 3387.52, "text": " yet but probably will do yeah so yeah the I think and in statistics we're really good at", "tokens": [50364, 1939, 457, 1391, 486, 360, 1338, 370, 1338, 264, 286, 519, 293, 294, 12523, 321, 434, 534, 665, 412, 50724], "temperature": 0.0, "avg_logprob": -0.06095714347307072, "compression_ratio": 1.6681818181818182, "no_speech_prob": 0.003301039105281234}, {"id": 592, "seek": 338032, "start": 3387.52, "end": 3392.4, "text": " quantifying uncertainty I mean this also has some darker sides with like the p-hacking and so on", "tokens": [50724, 4426, 5489, 15697, 286, 914, 341, 611, 575, 512, 12741, 4881, 365, 411, 264, 280, 12, 71, 14134, 293, 370, 322, 50968], "temperature": 0.0, "avg_logprob": -0.06095714347307072, "compression_ratio": 1.6681818181818182, "no_speech_prob": 0.003301039105281234}, {"id": 593, "seek": 338032, "start": 3392.4, "end": 3398.2400000000002, "text": " but I still would say it's better to have um not only just one number or one explanation", "tokens": [50968, 457, 286, 920, 576, 584, 309, 311, 1101, 281, 362, 1105, 406, 787, 445, 472, 1230, 420, 472, 10835, 51260], "temperature": 0.0, "avg_logprob": -0.06095714347307072, "compression_ratio": 1.6681818181818182, "no_speech_prob": 0.003301039105281234}, {"id": 594, "seek": 338032, "start": 3398.2400000000002, "end": 3405.44, "text": " but also have the distribution to do of this explanation or this number and to quantify what", "tokens": [51260, 457, 611, 362, 264, 7316, 281, 360, 295, 341, 10835, 420, 341, 1230, 293, 281, 40421, 437, 51620], "temperature": 0.0, "avg_logprob": -0.06095714347307072, "compression_ratio": 1.6681818181818182, "no_speech_prob": 0.003301039105281234}, {"id": 595, "seek": 340544, "start": 3405.44, "end": 3410.8, "text": " uncertainties behind computing this number so when you have a linear model then you get", "tokens": [50364, 11308, 6097, 2261, 15866, 341, 1230, 370, 562, 291, 362, 257, 8213, 2316, 550, 291, 483, 50632], "temperature": 0.0, "avg_logprob": -0.07195668127022538, "compression_ratio": 1.876543209876543, "no_speech_prob": 0.01406013686209917}, {"id": 596, "seek": 340544, "start": 3412.16, "end": 3416.96, "text": " some coefficient which you interpret in the end but usually you don't just interpret the", "tokens": [50700, 512, 17619, 597, 291, 7302, 294, 264, 917, 457, 2673, 291, 500, 380, 445, 7302, 264, 50940], "temperature": 0.0, "avg_logprob": -0.07195668127022538, "compression_ratio": 1.876543209876543, "no_speech_prob": 0.01406013686209917}, {"id": 597, "seek": 340544, "start": 3416.96, "end": 3422.56, "text": " coefficient but you look at the confidence intervals but we don't do it at the moment for", "tokens": [50940, 17619, 457, 291, 574, 412, 264, 6687, 26651, 457, 321, 500, 380, 360, 309, 412, 264, 1623, 337, 51220], "temperature": 0.0, "avg_logprob": -0.07195668127022538, "compression_ratio": 1.876543209876543, "no_speech_prob": 0.01406013686209917}, {"id": 598, "seek": 340544, "start": 3422.56, "end": 3428.4, "text": " interpretability so you maybe get the saliency maps but how certain are you about maybe it's", "tokens": [51220, 7302, 2310, 370, 291, 1310, 483, 264, 1845, 7848, 11317, 457, 577, 1629, 366, 291, 466, 1310, 309, 311, 51512], "temperature": 0.0, "avg_logprob": -0.07195668127022538, "compression_ratio": 1.876543209876543, "no_speech_prob": 0.01406013686209917}, {"id": 599, "seek": 340544, "start": 3428.4, "end": 3433.92, "text": " a bad example because we don't so much on it but if you have like a feature importance value and", "tokens": [51512, 257, 1578, 1365, 570, 321, 500, 380, 370, 709, 322, 309, 457, 498, 291, 362, 411, 257, 4111, 7379, 2158, 293, 51788], "temperature": 0.0, "avg_logprob": -0.07195668127022538, "compression_ratio": 1.876543209876543, "no_speech_prob": 0.01406013686209917}, {"id": 600, "seek": 343392, "start": 3433.92, "end": 3439.04, "text": " you get some result how like what's the range actually like how much variance is behind it", "tokens": [50364, 291, 483, 512, 1874, 577, 411, 437, 311, 264, 3613, 767, 411, 577, 709, 21977, 307, 2261, 309, 50620], "temperature": 0.0, "avg_logprob": -0.05327184129469466, "compression_ratio": 1.6954887218045114, "no_speech_prob": 0.004005861468613148}, {"id": 601, "seek": 343392, "start": 3439.04, "end": 3444.48, "text": " if I were to use slightly different data or refit my model again how similar would the number be", "tokens": [50620, 498, 286, 645, 281, 764, 4748, 819, 1412, 420, 1895, 270, 452, 2316, 797, 577, 2531, 576, 264, 1230, 312, 50892], "temperature": 0.0, "avg_logprob": -0.05327184129469466, "compression_ratio": 1.6954887218045114, "no_speech_prob": 0.004005861468613148}, {"id": 602, "seek": 343392, "start": 3445.6, "end": 3450.4, "text": " and I think that's something that will or should come to interpretability as well", "tokens": [50948, 293, 286, 519, 300, 311, 746, 300, 486, 420, 820, 808, 281, 7302, 2310, 382, 731, 51188], "temperature": 0.0, "avg_logprob": -0.05327184129469466, "compression_ratio": 1.6954887218045114, "no_speech_prob": 0.004005861468613148}, {"id": 603, "seek": 343392, "start": 3451.84, "end": 3456.08, "text": " it's funny how when we come to machine learning it's almost like open season and forgetting", "tokens": [51260, 309, 311, 4074, 577, 562, 321, 808, 281, 3479, 2539, 309, 311, 1920, 411, 1269, 3196, 293, 25428, 51472], "temperature": 0.0, "avg_logprob": -0.05327184129469466, "compression_ratio": 1.6954887218045114, "no_speech_prob": 0.004005861468613148}, {"id": 604, "seek": 343392, "start": 3456.08, "end": 3459.6800000000003, "text": " everything you know about maths and stats you throw it all out the window okay so excited", "tokens": [51472, 1203, 291, 458, 466, 36287, 293, 18152, 291, 3507, 309, 439, 484, 264, 4910, 1392, 370, 2919, 51652], "temperature": 0.0, "avg_logprob": -0.05327184129469466, "compression_ratio": 1.6954887218045114, "no_speech_prob": 0.004005861468613148}, {"id": 605, "seek": 345968, "start": 3459.68, "end": 3465.3599999999997, "text": " about these algorithms right like one example is if you take a if you're fitting a model to predict", "tokens": [50364, 466, 613, 14642, 558, 411, 472, 1365, 307, 498, 291, 747, 257, 498, 291, 434, 15669, 257, 2316, 281, 6069, 50648], "temperature": 0.0, "avg_logprob": -0.05477742385864258, "compression_ratio": 1.9042904290429044, "no_speech_prob": 0.07115694135427475}, {"id": 606, "seek": 345968, "start": 3465.3599999999997, "end": 3470.24, "text": " something that was unlikely I don't know maybe it was like a covid test for example and then", "tokens": [50648, 746, 300, 390, 17518, 286, 500, 380, 458, 1310, 309, 390, 411, 257, 25616, 1500, 337, 1365, 293, 550, 50892], "temperature": 0.0, "avg_logprob": -0.05477742385864258, "compression_ratio": 1.9042904290429044, "no_speech_prob": 0.07115694135427475}, {"id": 607, "seek": 345968, "start": 3470.24, "end": 3473.7599999999998, "text": " if you know the prevalence of covid you get it back you kind of know what the false positive", "tokens": [50892, 498, 291, 458, 264, 42583, 295, 25616, 291, 483, 309, 646, 291, 733, 295, 458, 437, 264, 7908, 3353, 51068], "temperature": 0.0, "avg_logprob": -0.05477742385864258, "compression_ratio": 1.9042904290429044, "no_speech_prob": 0.07115694135427475}, {"id": 608, "seek": 345968, "start": 3473.7599999999998, "end": 3478.64, "text": " rate is going to be and so you notice that you think it's the multiple comparison issue right", "tokens": [51068, 3314, 307, 516, 281, 312, 293, 370, 291, 3449, 300, 291, 519, 309, 311, 264, 3866, 9660, 2734, 558, 51312], "temperature": 0.0, "avg_logprob": -0.05477742385864258, "compression_ratio": 1.9042904290429044, "no_speech_prob": 0.07115694135427475}, {"id": 609, "seek": 345968, "start": 3478.64, "end": 3481.9199999999996, "text": " you know that you're expecting a certain level of false positives when it comes to doing something", "tokens": [51312, 291, 458, 300, 291, 434, 9650, 257, 1629, 1496, 295, 7908, 35127, 562, 309, 1487, 281, 884, 746, 51476], "temperature": 0.0, "avg_logprob": -0.05477742385864258, "compression_ratio": 1.9042904290429044, "no_speech_prob": 0.07115694135427475}, {"id": 610, "seek": 345968, "start": 3481.9199999999996, "end": 3488.08, "text": " like feature importance or looking at interpretability from a thousand features and then five come", "tokens": [51476, 411, 4111, 7379, 420, 1237, 412, 7302, 2310, 490, 257, 4714, 4122, 293, 550, 1732, 808, 51784], "temperature": 0.0, "avg_logprob": -0.05477742385864258, "compression_ratio": 1.9042904290429044, "no_speech_prob": 0.07115694135427475}, {"id": 611, "seek": 348808, "start": 3488.16, "end": 3492.7999999999997, "text": " through is really really important you mentioned sometimes we just forget that multiple comparison", "tokens": [50368, 807, 307, 534, 534, 1021, 291, 2835, 2171, 321, 445, 2870, 300, 3866, 9660, 50600], "temperature": 0.0, "avg_logprob": -0.09425849914550781, "compression_ratio": 1.9235880398671097, "no_speech_prob": 0.015637172386050224}, {"id": 612, "seek": 348808, "start": 3492.7999999999997, "end": 3496.7999999999997, "text": " issue forget the fact that probably these five are going to be completely false positives and", "tokens": [50600, 2734, 2870, 264, 1186, 300, 1391, 613, 1732, 366, 516, 281, 312, 2584, 7908, 35127, 293, 50800], "temperature": 0.0, "avg_logprob": -0.09425849914550781, "compression_ratio": 1.9235880398671097, "no_speech_prob": 0.015637172386050224}, {"id": 613, "seek": 348808, "start": 3496.7999999999997, "end": 3501.52, "text": " probably completely meaningless yeah I agree especially if you have like these high-dimensionality", "tokens": [50800, 1391, 2584, 33232, 1338, 286, 3986, 2318, 498, 291, 362, 411, 613, 1090, 12, 13595, 3378, 1860, 51036], "temperature": 0.0, "avg_logprob": -0.09425849914550781, "compression_ratio": 1.9235880398671097, "no_speech_prob": 0.015637172386050224}, {"id": 614, "seek": 348808, "start": 3501.52, "end": 3506.88, "text": " features and for the record I have to say I mean there are already approaches so especially for", "tokens": [51036, 4122, 293, 337, 264, 2136, 286, 362, 281, 584, 286, 914, 456, 366, 1217, 11587, 370, 2318, 337, 51304], "temperature": 0.0, "avg_logprob": -0.09425849914550781, "compression_ratio": 1.9235880398671097, "no_speech_prob": 0.015637172386050224}, {"id": 615, "seek": 348808, "start": 3506.88, "end": 3513.2, "text": " feature importance because there's like a huge community in random forests for example and they", "tokens": [51304, 4111, 7379, 570, 456, 311, 411, 257, 2603, 1768, 294, 4974, 21700, 337, 1365, 293, 436, 51620], "temperature": 0.0, "avg_logprob": -0.09425849914550781, "compression_ratio": 1.9235880398671097, "no_speech_prob": 0.015637172386050224}, {"id": 616, "seek": 348808, "start": 3513.2, "end": 3517.7599999999998, "text": " thought a lot about these issues and their tests for this and stuff like it but for the rest of", "tokens": [51620, 1194, 257, 688, 466, 613, 2663, 293, 641, 6921, 337, 341, 293, 1507, 411, 309, 457, 337, 264, 1472, 295, 51848], "temperature": 0.0, "avg_logprob": -0.09425849914550781, "compression_ratio": 1.9235880398671097, "no_speech_prob": 0.015637172386050224}, {"id": 617, "seek": 351776, "start": 3517.76, "end": 3522.5600000000004, "text": " interpretability I think it could gain a lot thinking more about I mean this is very simple", "tokens": [50364, 7302, 2310, 286, 519, 309, 727, 6052, 257, 688, 1953, 544, 466, 286, 914, 341, 307, 588, 2199, 50604], "temperature": 0.0, "avg_logprob": -0.09537687396058941, "compression_ratio": 1.8313253012048192, "no_speech_prob": 0.00638708146288991}, {"id": 618, "seek": 351776, "start": 3522.5600000000004, "end": 3528.48, "text": " stuff like multiple comparisons quantifying uncertainty does the stuff like statisticians", "tokens": [50604, 1507, 411, 3866, 33157, 4426, 5489, 15697, 775, 264, 1507, 411, 29588, 2567, 50900], "temperature": 0.0, "avg_logprob": -0.09537687396058941, "compression_ratio": 1.8313253012048192, "no_speech_prob": 0.00638708146288991}, {"id": 619, "seek": 351776, "start": 3528.48, "end": 3535.5200000000004, "text": " think like a long time already about it and I mean if you even if you leave the area of", "tokens": [50900, 519, 411, 257, 938, 565, 1217, 466, 309, 293, 286, 914, 498, 291, 754, 498, 291, 1856, 264, 1859, 295, 51252], "temperature": 0.0, "avg_logprob": -0.09537687396058941, "compression_ratio": 1.8313253012048192, "no_speech_prob": 0.00638708146288991}, {"id": 620, "seek": 351776, "start": 3535.5200000000004, "end": 3540.6400000000003, "text": " interpretability and look at the benchmarks so even like if you have like accuracy like a table", "tokens": [51252, 7302, 2310, 293, 574, 412, 264, 43751, 370, 754, 411, 498, 291, 362, 411, 14170, 411, 257, 3199, 51508], "temperature": 0.0, "avg_logprob": -0.09537687396058941, "compression_ratio": 1.8313253012048192, "no_speech_prob": 0.00638708146288991}, {"id": 621, "seek": 351776, "start": 3540.6400000000003, "end": 3545.44, "text": " and you see accuracies in it but there's no variance attached to it then it should be like", "tokens": [51508, 293, 291, 536, 5771, 20330, 294, 309, 457, 456, 311, 572, 21977, 8570, 281, 309, 550, 309, 820, 312, 411, 51748], "temperature": 0.0, "avg_logprob": -0.09537687396058941, "compression_ratio": 1.8313253012048192, "no_speech_prob": 0.00638708146288991}, {"id": 622, "seek": 354544, "start": 3545.44, "end": 3550.88, "text": " suspicious of it but because if you just retrain your neural network with a different seed you might", "tokens": [50364, 17931, 295, 309, 457, 570, 498, 291, 445, 1533, 7146, 428, 18161, 3209, 365, 257, 819, 8871, 291, 1062, 50636], "temperature": 0.0, "avg_logprob": -0.10294628143310547, "compression_ratio": 1.7971698113207548, "no_speech_prob": 0.011506718583405018}, {"id": 623, "seek": 354544, "start": 3550.88, "end": 3555.44, "text": " end up with a different accuracy in the end so and if you want to say a method is better than", "tokens": [50636, 917, 493, 365, 257, 819, 14170, 294, 264, 917, 370, 293, 498, 291, 528, 281, 584, 257, 3170, 307, 1101, 813, 50864], "temperature": 0.0, "avg_logprob": -0.10294628143310547, "compression_ratio": 1.7971698113207548, "no_speech_prob": 0.011506718583405018}, {"id": 624, "seek": 354544, "start": 3555.44, "end": 3562.7200000000003, "text": " another method you want to quantify how larger ranges of uncertainty do you I mean there are a", "tokens": [50864, 1071, 3170, 291, 528, 281, 40421, 577, 4833, 22526, 295, 15697, 360, 291, 286, 914, 456, 366, 257, 51228], "temperature": 0.0, "avg_logprob": -0.10294628143310547, "compression_ratio": 1.7971698113207548, "no_speech_prob": 0.011506718583405018}, {"id": 625, "seek": 354544, "start": 3562.7200000000003, "end": 3567.2000000000003, "text": " lot of things like the choice of data choice of splitting points and training and test data", "tokens": [51228, 688, 295, 721, 411, 264, 3922, 295, 1412, 3922, 295, 30348, 2793, 293, 3097, 293, 1500, 1412, 51452], "temperature": 0.0, "avg_logprob": -0.10294628143310547, "compression_ratio": 1.7971698113207548, "no_speech_prob": 0.011506718583405018}, {"id": 626, "seek": 356720, "start": 3567.3599999999997, "end": 3574.7999999999997, "text": " weight initialization and so on so I think a lot of this rigor from statistics could help", "tokens": [50372, 3364, 5883, 2144, 293, 370, 322, 370, 286, 519, 257, 688, 295, 341, 42191, 490, 12523, 727, 854, 50744], "temperature": 0.0, "avg_logprob": -0.09718516977821909, "compression_ratio": 1.7440758293838863, "no_speech_prob": 0.032091982662677765}, {"id": 627, "seek": 356720, "start": 3575.3599999999997, "end": 3578.72, "text": " the machine learning community and and machine learning science to become better", "tokens": [50772, 264, 3479, 2539, 1768, 293, 293, 3479, 2539, 3497, 281, 1813, 1101, 50940], "temperature": 0.0, "avg_logprob": -0.09718516977821909, "compression_ratio": 1.7440758293838863, "no_speech_prob": 0.032091982662677765}, {"id": 628, "seek": 356720, "start": 3579.8399999999997, "end": 3584.3199999999997, "text": " yeah but let's let's never forget this uh quite quite well known saying which is there are three", "tokens": [50996, 1338, 457, 718, 311, 718, 311, 1128, 2870, 341, 2232, 1596, 1596, 731, 2570, 1566, 597, 307, 456, 366, 1045, 51220], "temperature": 0.0, "avg_logprob": -0.09718516977821909, "compression_ratio": 1.7440758293838863, "no_speech_prob": 0.032091982662677765}, {"id": 629, "seek": 356720, "start": 3584.3199999999997, "end": 3591.52, "text": " kinds of lies lies damned lies and statistics so you know that that's a lot of what's going on right", "tokens": [51220, 3685, 295, 9134, 9134, 46397, 9134, 293, 12523, 370, 291, 458, 300, 300, 311, 257, 688, 295, 437, 311, 516, 322, 558, 51580], "temperature": 0.0, "avg_logprob": -0.09718516977821909, "compression_ratio": 1.7440758293838863, "no_speech_prob": 0.032091982662677765}, {"id": 630, "seek": 359152, "start": 3591.52, "end": 3597.28, "text": " is is you know fundamentally whenever we go measure data and we have a model what we're", "tokens": [50364, 307, 307, 291, 458, 17879, 5699, 321, 352, 3481, 1412, 293, 321, 362, 257, 2316, 437, 321, 434, 50652], "temperature": 0.0, "avg_logprob": -0.05497576155752506, "compression_ratio": 1.8053435114503817, "no_speech_prob": 0.0003150234406348318}, {"id": 631, "seek": 359152, "start": 3597.28, "end": 3603.12, "text": " actually able to extract from that data and and the model is inherently probabilistic", "tokens": [50652, 767, 1075, 281, 8947, 490, 300, 1412, 293, 293, 264, 2316, 307, 27993, 31959, 3142, 50944], "temperature": 0.0, "avg_logprob": -0.05497576155752506, "compression_ratio": 1.8053435114503817, "no_speech_prob": 0.0003150234406348318}, {"id": 632, "seek": 359152, "start": 3603.12, "end": 3608.48, "text": " it's a probability distribution right at the end of the day and we get into trouble anytime we try", "tokens": [50944, 309, 311, 257, 8482, 7316, 558, 412, 264, 917, 295, 264, 786, 293, 321, 483, 666, 5253, 13038, 321, 853, 51212], "temperature": 0.0, "avg_logprob": -0.05497576155752506, "compression_ratio": 1.8053435114503817, "no_speech_prob": 0.0003150234406348318}, {"id": 633, "seek": 359152, "start": 3608.48, "end": 3614.48, "text": " to take that probability distribution and project it to numbers i.e. statistics like as soon as we", "tokens": [51212, 281, 747, 300, 8482, 7316, 293, 1716, 309, 281, 3547, 741, 13, 68, 13, 12523, 411, 382, 2321, 382, 321, 51512], "temperature": 0.0, "avg_logprob": -0.05497576155752506, "compression_ratio": 1.8053435114503817, "no_speech_prob": 0.0003150234406348318}, {"id": 634, "seek": 359152, "start": 3614.48, "end": 3620.64, "text": " start trying to to generate and it doesn't matter whether it's it's a mean plus a confidence interval", "tokens": [51512, 722, 1382, 281, 281, 8460, 293, 309, 1177, 380, 1871, 1968, 309, 311, 309, 311, 257, 914, 1804, 257, 6687, 15035, 51820], "temperature": 0.0, "avg_logprob": -0.05497576155752506, "compression_ratio": 1.8053435114503817, "no_speech_prob": 0.0003150234406348318}, {"id": 635, "seek": 362064, "start": 3620.64, "end": 3624.96, "text": " or whatever the fact is we're throwing away information the totality of the information", "tokens": [50364, 420, 2035, 264, 1186, 307, 321, 434, 10238, 1314, 1589, 264, 1993, 1860, 295, 264, 1589, 50580], "temperature": 0.0, "avg_logprob": -0.047430343098110624, "compression_ratio": 1.8968253968253967, "no_speech_prob": 0.0017543613212183118}, {"id": 636, "seek": 362064, "start": 3624.96, "end": 3631.44, "text": " sitting there in that weird multimodal you know spread out distribution right and then we we find", "tokens": [50580, 3798, 456, 294, 300, 3657, 32972, 378, 304, 291, 458, 3974, 484, 7316, 558, 293, 550, 321, 321, 915, 50904], "temperature": 0.0, "avg_logprob": -0.047430343098110624, "compression_ratio": 1.8968253968253967, "no_speech_prob": 0.0017543613212183118}, {"id": 637, "seek": 362064, "start": 3631.44, "end": 3636.3199999999997, "text": " some way to simplify it and project it down to a set of numbers we've got a problem like that's", "tokens": [50904, 512, 636, 281, 20460, 309, 293, 1716, 309, 760, 281, 257, 992, 295, 3547, 321, 600, 658, 257, 1154, 411, 300, 311, 51148], "temperature": 0.0, "avg_logprob": -0.047430343098110624, "compression_ratio": 1.8968253968253967, "no_speech_prob": 0.0017543613212183118}, {"id": 638, "seek": 362064, "start": 3636.3199999999997, "end": 3641.2, "text": " and if you forget that that's happening if you forget that you're throwing away all this information", "tokens": [51148, 293, 498, 291, 2870, 300, 300, 311, 2737, 498, 291, 2870, 300, 291, 434, 10238, 1314, 439, 341, 1589, 51392], "temperature": 0.0, "avg_logprob": -0.047430343098110624, "compression_ratio": 1.8968253968253967, "no_speech_prob": 0.0017543613212183118}, {"id": 639, "seek": 362064, "start": 3641.2, "end": 3646.56, "text": " I think that you know the tendency to do that isn't just simplification it's that oftentimes we", "tokens": [51392, 286, 519, 300, 291, 458, 264, 18187, 281, 360, 300, 1943, 380, 445, 6883, 3774, 309, 311, 300, 18349, 321, 51660], "temperature": 0.0, "avg_logprob": -0.047430343098110624, "compression_ratio": 1.8968253968253967, "no_speech_prob": 0.0017543613212183118}, {"id": 640, "seek": 364656, "start": 3646.56, "end": 3651.68, "text": " have to use these probabilistic things to reach a decision and as soon as we get to the point where", "tokens": [50364, 362, 281, 764, 613, 31959, 3142, 721, 281, 2524, 257, 3537, 293, 382, 2321, 382, 321, 483, 281, 264, 935, 689, 50620], "temperature": 0.0, "avg_logprob": -0.05329504765962299, "compression_ratio": 1.91015625, "no_speech_prob": 0.003471364500001073}, {"id": 641, "seek": 364656, "start": 3651.68, "end": 3656.7999999999997, "text": " look i've got to choose either to go left or right give the loan or not give the loan as soon as we", "tokens": [50620, 574, 741, 600, 658, 281, 2826, 2139, 281, 352, 1411, 420, 558, 976, 264, 10529, 420, 406, 976, 264, 10529, 382, 2321, 382, 321, 50876], "temperature": 0.0, "avg_logprob": -0.05329504765962299, "compression_ratio": 1.91015625, "no_speech_prob": 0.003471364500001073}, {"id": 642, "seek": 364656, "start": 3656.7999999999997, "end": 3662.08, "text": " get down to some point where we have to make a concrete decision we're forced you know we're", "tokens": [50876, 483, 760, 281, 512, 935, 689, 321, 362, 281, 652, 257, 9859, 3537, 321, 434, 7579, 291, 458, 321, 434, 51140], "temperature": 0.0, "avg_logprob": -0.05329504765962299, "compression_ratio": 1.91015625, "no_speech_prob": 0.003471364500001073}, {"id": 643, "seek": 364656, "start": 3662.08, "end": 3668.24, "text": " forced to project it right but along the way it's important not to lose sight of the the fact that", "tokens": [51140, 7579, 281, 1716, 309, 558, 457, 2051, 264, 636, 309, 311, 1021, 406, 281, 3624, 7860, 295, 264, 264, 1186, 300, 51448], "temperature": 0.0, "avg_logprob": -0.05329504765962299, "compression_ratio": 1.91015625, "no_speech_prob": 0.003471364500001073}, {"id": 644, "seek": 364656, "start": 3668.24, "end": 3674.32, "text": " we're throwing away information fantastic well and by the way you also said something interesting", "tokens": [51448, 321, 434, 10238, 1314, 1589, 5456, 731, 293, 538, 264, 636, 291, 611, 848, 746, 1880, 51752], "temperature": 0.0, "avg_logprob": -0.05329504765962299, "compression_ratio": 1.91015625, "no_speech_prob": 0.003471364500001073}, {"id": 645, "seek": 367432, "start": 3674.4, "end": 3678.8, "text": " a minute ago Christoph which is about at least in most machine learning algorithms if you change", "tokens": [50368, 257, 3456, 2057, 2040, 5317, 597, 307, 466, 412, 1935, 294, 881, 3479, 2539, 14642, 498, 291, 1319, 50588], "temperature": 0.0, "avg_logprob": -0.07168777235623064, "compression_ratio": 1.9276315789473684, "no_speech_prob": 0.010317536070942879}, {"id": 646, "seek": 367432, "start": 3678.8, "end": 3682.96, "text": " the random seed you know that there's enough stability there that it still gives you roughly", "tokens": [50588, 264, 4974, 8871, 291, 458, 300, 456, 311, 1547, 11826, 456, 300, 309, 920, 2709, 291, 9810, 50796], "temperature": 0.0, "avg_logprob": -0.07168777235623064, "compression_ratio": 1.9276315789473684, "no_speech_prob": 0.010317536070942879}, {"id": 647, "seek": 367432, "start": 3682.96, "end": 3688.8, "text": " the the same model every time but in reinforcement learning if you change the random seed the entire", "tokens": [50796, 264, 264, 912, 2316, 633, 565, 457, 294, 29280, 2539, 498, 291, 1319, 264, 4974, 8871, 264, 2302, 51088], "temperature": 0.0, "avg_logprob": -0.07168777235623064, "compression_ratio": 1.9276315789473684, "no_speech_prob": 0.010317536070942879}, {"id": 648, "seek": 367432, "start": 3688.8, "end": 3694.7200000000003, "text": " thing is completely broken but but yeah what Keith was saying about this this this information", "tokens": [51088, 551, 307, 2584, 5463, 457, 457, 1338, 437, 20613, 390, 1566, 466, 341, 341, 341, 1589, 51384], "temperature": 0.0, "avg_logprob": -0.07168777235623064, "compression_ratio": 1.9276315789473684, "no_speech_prob": 0.010317536070942879}, {"id": 649, "seek": 367432, "start": 3694.7200000000003, "end": 3699.28, "text": " and structure in models I think that's really interesting because people have said with reinforcement", "tokens": [51384, 293, 3877, 294, 5245, 286, 519, 300, 311, 534, 1880, 570, 561, 362, 848, 365, 29280, 51612], "temperature": 0.0, "avg_logprob": -0.07168777235623064, "compression_ratio": 1.9276315789473684, "no_speech_prob": 0.010317536070942879}, {"id": 650, "seek": 367432, "start": 3699.28, "end": 3703.6800000000003, "text": " learning you can actually learn causal factors right but that's not really true you're interacting", "tokens": [51612, 2539, 291, 393, 767, 1466, 38755, 6771, 558, 457, 300, 311, 406, 534, 2074, 291, 434, 18017, 51832], "temperature": 0.0, "avg_logprob": -0.07168777235623064, "compression_ratio": 1.9276315789473684, "no_speech_prob": 0.010317536070942879}, {"id": 651, "seek": 370368, "start": 3703.68, "end": 3707.8399999999997, "text": " with the system but what you're learning is is a surface representation of causal factors so you", "tokens": [50364, 365, 264, 1185, 457, 437, 291, 434, 2539, 307, 307, 257, 3753, 10290, 295, 38755, 6771, 370, 291, 50572], "temperature": 0.0, "avg_logprob": -0.03598795913335845, "compression_ratio": 2.03914590747331, "no_speech_prob": 0.006175974849611521}, {"id": 652, "seek": 370368, "start": 3707.8399999999997, "end": 3714.3199999999997, "text": " might learn that there's a causal factor between like a hose putting out fire but it wouldn't", "tokens": [50572, 1062, 1466, 300, 456, 311, 257, 38755, 5952, 1296, 411, 257, 20061, 3372, 484, 2610, 457, 309, 2759, 380, 50896], "temperature": 0.0, "avg_logprob": -0.03598795913335845, "compression_ratio": 2.03914590747331, "no_speech_prob": 0.006175974849611521}, {"id": 653, "seek": 370368, "start": 3714.3199999999997, "end": 3718.96, "text": " actually learn that it was the water that put out you know that there was a causal relationship", "tokens": [50896, 767, 1466, 300, 309, 390, 264, 1281, 300, 829, 484, 291, 458, 300, 456, 390, 257, 38755, 2480, 51128], "temperature": 0.0, "avg_logprob": -0.03598795913335845, "compression_ratio": 2.03914590747331, "no_speech_prob": 0.006175974849611521}, {"id": 654, "seek": 370368, "start": 3718.96, "end": 3723.04, "text": " between the water and the fire and this is the case with so many of our models as we were saying", "tokens": [51128, 1296, 264, 1281, 293, 264, 2610, 293, 341, 307, 264, 1389, 365, 370, 867, 295, 527, 5245, 382, 321, 645, 1566, 51332], "temperature": 0.0, "avg_logprob": -0.03598795913335845, "compression_ratio": 2.03914590747331, "no_speech_prob": 0.006175974849611521}, {"id": 655, "seek": 370368, "start": 3723.04, "end": 3726.96, "text": " earlier that there's there's just a a surface representation which doesn't actually represent", "tokens": [51332, 3071, 300, 456, 311, 456, 311, 445, 257, 257, 3753, 10290, 597, 1177, 380, 767, 2906, 51528], "temperature": 0.0, "avg_logprob": -0.03598795913335845, "compression_ratio": 2.03914590747331, "no_speech_prob": 0.006175974849611521}, {"id": 656, "seek": 370368, "start": 3726.96, "end": 3731.68, "text": " the reality of our world at all but this brings me on to the next point because you have a real", "tokens": [51528, 264, 4103, 295, 527, 1002, 412, 439, 457, 341, 5607, 385, 322, 281, 264, 958, 935, 570, 291, 362, 257, 957, 51764], "temperature": 0.0, "avg_logprob": -0.03598795913335845, "compression_ratio": 2.03914590747331, "no_speech_prob": 0.006175974849611521}, {"id": 657, "seek": 373168, "start": 3731.68, "end": 3737.52, "text": " problem with causal interpretations of some of these iml metrics right and you you say that models", "tokens": [50364, 1154, 365, 38755, 37547, 295, 512, 295, 613, 566, 75, 16367, 558, 293, 291, 291, 584, 300, 5245, 50656], "temperature": 0.0, "avg_logprob": -0.07216560382109422, "compression_ratio": 1.859375, "no_speech_prob": 0.005298110190778971}, {"id": 658, "seek": 373168, "start": 3737.52, "end": 3741.6, "text": " well the the goal of models is that they should reflect the causal structure right this is what", "tokens": [50656, 731, 264, 264, 3387, 295, 5245, 307, 300, 436, 820, 5031, 264, 38755, 3877, 558, 341, 307, 437, 50860], "temperature": 0.0, "avg_logprob": -0.07216560382109422, "compression_ratio": 1.859375, "no_speech_prob": 0.005298110190778971}, {"id": 659, "seek": 373168, "start": 3741.6, "end": 3746.48, "text": " we want to do in science but most statistical learning just reflects these surface feature", "tokens": [50860, 321, 528, 281, 360, 294, 3497, 457, 881, 22820, 2539, 445, 18926, 613, 3753, 4111, 51104], "temperature": 0.0, "avg_logprob": -0.07216560382109422, "compression_ratio": 1.859375, "no_speech_prob": 0.005298110190778971}, {"id": 660, "seek": 373168, "start": 3746.48, "end": 3750.96, "text": " correlations they they don't even scratch the surface of what we want so what are we going", "tokens": [51104, 13983, 763, 436, 436, 500, 380, 754, 8459, 264, 3753, 295, 437, 321, 528, 370, 437, 366, 321, 516, 51328], "temperature": 0.0, "avg_logprob": -0.07216560382109422, "compression_ratio": 1.859375, "no_speech_prob": 0.005298110190778971}, {"id": 661, "seek": 373168, "start": 3750.96, "end": 3755.7599999999998, "text": " to do right are you doing some work in this field to help us out here and and why are people making", "tokens": [51328, 281, 360, 558, 366, 291, 884, 512, 589, 294, 341, 2519, 281, 854, 505, 484, 510, 293, 293, 983, 366, 561, 1455, 51568], "temperature": 0.0, "avg_logprob": -0.07216560382109422, "compression_ratio": 1.859375, "no_speech_prob": 0.005298110190778971}, {"id": 662, "seek": 375576, "start": 3755.76, "end": 3762.8, "text": " these fallacious interpretations yeah so so i'm not not working on anything causality related at the", "tokens": [50364, 613, 2100, 22641, 37547, 1338, 370, 370, 741, 478, 406, 406, 1364, 322, 1340, 3302, 1860, 4077, 412, 264, 50716], "temperature": 0.0, "avg_logprob": -0.10313503132310024, "compression_ratio": 1.7746478873239437, "no_speech_prob": 0.050195977091789246}, {"id": 663, "seek": 375576, "start": 3762.8, "end": 3770.48, "text": " moment yeah but about about causality i mean kind of like um i i studied statistics bachelor and", "tokens": [50716, 1623, 1338, 457, 466, 466, 3302, 1860, 741, 914, 733, 295, 411, 1105, 741, 741, 9454, 12523, 25947, 293, 51100], "temperature": 0.0, "avg_logprob": -0.10313503132310024, "compression_ratio": 1.7746478873239437, "no_speech_prob": 0.050195977091789246}, {"id": 664, "seek": 375576, "start": 3770.48, "end": 3778.1600000000003, "text": " master and zero i mean the only time we were talked about causality was when i heard the", "tokens": [51100, 4505, 293, 4018, 741, 914, 264, 787, 565, 321, 645, 2825, 466, 3302, 1860, 390, 562, 741, 2198, 264, 51484], "temperature": 0.0, "avg_logprob": -0.10313503132310024, "compression_ratio": 1.7746478873239437, "no_speech_prob": 0.050195977091789246}, {"id": 665, "seek": 375576, "start": 3778.1600000000003, "end": 3785.1200000000003, "text": " sentence correlation does not imply causality and it was really about it so um i think it's", "tokens": [51484, 8174, 20009, 775, 406, 33616, 3302, 1860, 293, 309, 390, 534, 466, 309, 370, 1105, 741, 519, 309, 311, 51832], "temperature": 0.0, "avg_logprob": -0.10313503132310024, "compression_ratio": 1.7746478873239437, "no_speech_prob": 0.050195977091789246}, {"id": 666, "seek": 378512, "start": 3785.2, "end": 3791.04, "text": " like really um yeah should be taught a lot more like how to think about causality like", "tokens": [50368, 411, 534, 1105, 1338, 820, 312, 5928, 257, 688, 544, 411, 577, 281, 519, 466, 3302, 1860, 411, 50660], "temperature": 0.0, "avg_logprob": -0.08291860863014504, "compression_ratio": 1.7740384615384615, "no_speech_prob": 0.0026725735515356064}, {"id": 667, "seek": 378512, "start": 3791.04, "end": 3797.68, "text": " just super simple things like you should include confounders or um like what types of features", "tokens": [50660, 445, 1687, 2199, 721, 411, 291, 820, 4090, 1497, 554, 433, 420, 1105, 411, 437, 3467, 295, 4122, 50992], "temperature": 0.0, "avg_logprob": -0.08291860863014504, "compression_ratio": 1.7740384615384615, "no_speech_prob": 0.0026725735515356064}, {"id": 668, "seek": 378512, "start": 3797.68, "end": 3803.52, "text": " if you include them in the model um like destroy your causal interpretation of another features", "tokens": [50992, 498, 291, 4090, 552, 294, 264, 2316, 1105, 411, 5293, 428, 38755, 14174, 295, 1071, 4122, 51284], "temperature": 0.0, "avg_logprob": -0.08291860863014504, "compression_ratio": 1.7740384615384615, "no_speech_prob": 0.0026725735515356064}, {"id": 669, "seek": 378512, "start": 3803.52, "end": 3808.96, "text": " these these are not super difficult things so um then you don't have to like learn any like", "tokens": [51284, 613, 613, 366, 406, 1687, 2252, 721, 370, 1105, 550, 291, 500, 380, 362, 281, 411, 1466, 604, 411, 51556], "temperature": 0.0, "avg_logprob": -0.08291860863014504, "compression_ratio": 1.7740384615384615, "no_speech_prob": 0.0026725735515356064}, {"id": 670, "seek": 380896, "start": 3808.96, "end": 3815.68, "text": " difficult frameworks to work with or read like causality books on it that's like super simple", "tokens": [50364, 2252, 29834, 281, 589, 365, 420, 1401, 411, 3302, 1860, 3642, 322, 309, 300, 311, 411, 1687, 2199, 50700], "temperature": 0.0, "avg_logprob": -0.12565559148788452, "compression_ratio": 1.7586206896551724, "no_speech_prob": 0.016150472685694695}, {"id": 671, "seek": 380896, "start": 3817.2, "end": 3823.52, "text": " yeah rules of thumb for your features even um yeah and i think you also have to decide", "tokens": [50776, 1338, 4474, 295, 9298, 337, 428, 4122, 754, 1105, 1338, 293, 741, 519, 291, 611, 362, 281, 4536, 51092], "temperature": 0.0, "avg_logprob": -0.12565559148788452, "compression_ratio": 1.7586206896551724, "no_speech_prob": 0.016150472685694695}, {"id": 672, "seek": 380896, "start": 3824.08, "end": 3830.8, "text": " or distinguish between like what's the goal of your model do you want a causal interpretation", "tokens": [51120, 420, 20206, 1296, 411, 437, 311, 264, 3387, 295, 428, 2316, 360, 291, 528, 257, 38755, 14174, 51456], "temperature": 0.0, "avg_logprob": -0.12565559148788452, "compression_ratio": 1.7586206896551724, "no_speech_prob": 0.016150472685694695}, {"id": 673, "seek": 380896, "start": 3832.64, "end": 3837.2, "text": " or do you want to like because in a sense and you have to also distinguish between", "tokens": [51548, 420, 360, 291, 528, 281, 411, 570, 294, 257, 2020, 293, 291, 362, 281, 611, 20206, 1296, 51776], "temperature": 0.0, "avg_logprob": -0.12565559148788452, "compression_ratio": 1.7586206896551724, "no_speech_prob": 0.016150472685694695}, {"id": 674, "seek": 383720, "start": 3837.2, "end": 3841.7599999999998, "text": " two levels you have the real world level and the model level i mean once you use features for the", "tokens": [50364, 732, 4358, 291, 362, 264, 957, 1002, 1496, 293, 264, 2316, 1496, 741, 914, 1564, 291, 764, 4122, 337, 264, 50592], "temperature": 0.0, "avg_logprob": -0.08676965420062725, "compression_ratio": 2.017777777777778, "no_speech_prob": 0.00364868575707078}, {"id": 675, "seek": 383720, "start": 3841.7599999999998, "end": 3846.24, "text": " model they are causal for the model prediction of course because you designed it that way", "tokens": [50592, 2316, 436, 366, 38755, 337, 264, 2316, 17630, 295, 1164, 570, 291, 4761, 309, 300, 636, 50816], "temperature": 0.0, "avg_logprob": -0.08676965420062725, "compression_ratio": 2.017777777777778, "no_speech_prob": 0.00364868575707078}, {"id": 676, "seek": 383720, "start": 3846.24, "end": 3850.0, "text": " and the question is when you are you allowed to go to the real world level where you say", "tokens": [50816, 293, 264, 1168, 307, 562, 291, 366, 291, 4350, 281, 352, 281, 264, 957, 1002, 1496, 689, 291, 584, 51004], "temperature": 0.0, "avg_logprob": -0.08676965420062725, "compression_ratio": 2.017777777777778, "no_speech_prob": 0.00364868575707078}, {"id": 677, "seek": 383720, "start": 3850.0, "end": 3854.3999999999996, "text": " okay this um the feature importance that i see here or also the feature dependence", "tokens": [51004, 1392, 341, 1105, 264, 4111, 7379, 300, 741, 536, 510, 420, 611, 264, 4111, 31704, 51224], "temperature": 0.0, "avg_logprob": -0.08676965420062725, "compression_ratio": 2.017777777777778, "no_speech_prob": 0.00364868575707078}, {"id": 678, "seek": 383720, "start": 3855.12, "end": 3863.6, "text": " plot that i see is also causal um or and may interpret it as a cause and or as a causal effect", "tokens": [51260, 7542, 300, 741, 536, 307, 611, 38755, 1105, 420, 293, 815, 7302, 309, 382, 257, 3082, 293, 420, 382, 257, 38755, 1802, 51684], "temperature": 0.0, "avg_logprob": -0.08676965420062725, "compression_ratio": 2.017777777777778, "no_speech_prob": 0.00364868575707078}, {"id": 679, "seek": 386360, "start": 3863.6, "end": 3871.04, "text": " also for for the real world and i think that also depends like if you need this interpretation", "tokens": [50364, 611, 337, 337, 264, 957, 1002, 293, 741, 519, 300, 611, 5946, 411, 498, 291, 643, 341, 14174, 50736], "temperature": 0.0, "avg_logprob": -0.09359661929578666, "compression_ratio": 1.7523364485981308, "no_speech_prob": 0.021226076409220695}, {"id": 680, "seek": 386360, "start": 3871.7599999999998, "end": 3878.48, "text": " if you do scientific modeling for example then you probably want it um but that there can also", "tokens": [50772, 498, 291, 360, 8134, 15983, 337, 1365, 550, 291, 1391, 528, 309, 1105, 457, 300, 456, 393, 611, 51108], "temperature": 0.0, "avg_logprob": -0.09359661929578666, "compression_ratio": 1.7523364485981308, "no_speech_prob": 0.021226076409220695}, {"id": 681, "seek": 386360, "start": 3878.48, "end": 3882.96, "text": " be good reasons to include non-causal features into your model if your goal is really just", "tokens": [51108, 312, 665, 4112, 281, 4090, 2107, 12, 496, 11765, 4122, 666, 428, 2316, 498, 428, 3387, 307, 534, 445, 51332], "temperature": 0.0, "avg_logprob": -0.09359661929578666, "compression_ratio": 1.7523364485981308, "no_speech_prob": 0.021226076409220695}, {"id": 682, "seek": 386360, "start": 3882.96, "end": 3888.64, "text": " prediction and and some feature might help you with the with a good prediction um but it might", "tokens": [51332, 17630, 293, 293, 512, 4111, 1062, 854, 291, 365, 264, 365, 257, 665, 17630, 1105, 457, 309, 1062, 51616], "temperature": 0.0, "avg_logprob": -0.09359661929578666, "compression_ratio": 1.7523364485981308, "no_speech_prob": 0.021226076409220695}, {"id": 683, "seek": 388864, "start": 3888.72, "end": 3893.92, "text": " not be causal at all yes but the problem is when we're using these deep learning models", "tokens": [50368, 406, 312, 38755, 412, 439, 2086, 457, 264, 1154, 307, 562, 321, 434, 1228, 613, 2452, 2539, 5245, 50628], "temperature": 0.0, "avg_logprob": -0.07083696532017976, "compression_ratio": 1.7822878228782288, "no_speech_prob": 0.07202717661857605}, {"id": 684, "seek": 388864, "start": 3894.56, "end": 3900.72, "text": " they they will learn a structure which probably has no relationship to the real world whatsoever", "tokens": [50660, 436, 436, 486, 1466, 257, 3877, 597, 1391, 575, 572, 2480, 281, 264, 957, 1002, 17076, 50968], "temperature": 0.0, "avg_logprob": -0.07083696532017976, "compression_ratio": 1.7822878228782288, "no_speech_prob": 0.07202717661857605}, {"id": 685, "seek": 388864, "start": 3900.72, "end": 3907.6, "text": " but um i think causal um factors do generalize much better there's the example of um i don't know", "tokens": [50968, 457, 1105, 741, 519, 38755, 1105, 6771, 360, 2674, 1125, 709, 1101, 456, 311, 264, 1365, 295, 1105, 741, 500, 380, 458, 51312], "temperature": 0.0, "avg_logprob": -0.07083696532017976, "compression_ratio": 1.7822878228782288, "no_speech_prob": 0.07202717661857605}, {"id": 686, "seek": 388864, "start": 3907.6, "end": 3913.3599999999997, "text": " with car crashes right male testosterone levels is a causal factor so that will probably generalize", "tokens": [51312, 365, 1032, 28642, 558, 7133, 33417, 4358, 307, 257, 38755, 5952, 370, 300, 486, 1391, 2674, 1125, 51600], "temperature": 0.0, "avg_logprob": -0.07083696532017976, "compression_ratio": 1.7822878228782288, "no_speech_prob": 0.07202717661857605}, {"id": 687, "seek": 388864, "start": 3913.3599999999997, "end": 3918.24, "text": " to other locations where you didn't train your data on but unfortunately models don't really do that", "tokens": [51600, 281, 661, 9253, 689, 291, 994, 380, 3847, 428, 1412, 322, 457, 7015, 5245, 500, 380, 534, 360, 300, 51844], "temperature": 0.0, "avg_logprob": -0.07083696532017976, "compression_ratio": 1.7822878228782288, "no_speech_prob": 0.07202717661857605}, {"id": 688, "seek": 391864, "start": 3918.96, "end": 3924.0, "text": " well but so just real quickly on that tim like the only reason that we know testosterone", "tokens": [50380, 731, 457, 370, 445, 957, 2661, 322, 300, 524, 411, 264, 787, 1778, 300, 321, 458, 33417, 50632], "temperature": 0.0, "avg_logprob": -0.07188330500958914, "compression_ratio": 1.6837209302325582, "no_speech_prob": 8.480490942019969e-05}, {"id": 689, "seek": 391864, "start": 3924.56, "end": 3930.4, "text": " is a causal factor is is not from that data set it's from a bunch of mechanistic you know", "tokens": [50660, 307, 257, 38755, 5952, 307, 307, 406, 490, 300, 1412, 992, 309, 311, 490, 257, 3840, 295, 4236, 3142, 291, 458, 50952], "temperature": 0.0, "avg_logprob": -0.07188330500958914, "compression_ratio": 1.6837209302325582, "no_speech_prob": 8.480490942019969e-05}, {"id": 690, "seek": 391864, "start": 3930.4, "end": 3937.44, "text": " scientific research and biology and and elsewhere um so you know i i'm kind of wondering how", "tokens": [50952, 8134, 2132, 293, 14956, 293, 293, 14517, 1105, 370, 291, 458, 741, 741, 478, 733, 295, 6359, 577, 51304], "temperature": 0.0, "avg_logprob": -0.07188330500958914, "compression_ratio": 1.6837209302325582, "no_speech_prob": 8.480490942019969e-05}, {"id": 691, "seek": 391864, "start": 3938.3199999999997, "end": 3942.7999999999997, "text": " it would be nice if at least machine learning methods could indicate that there may be the", "tokens": [51348, 309, 576, 312, 1481, 498, 412, 1935, 3479, 2539, 7150, 727, 13330, 300, 456, 815, 312, 264, 51572], "temperature": 0.0, "avg_logprob": -0.07188330500958914, "compression_ratio": 1.6837209302325582, "no_speech_prob": 8.480490942019969e-05}, {"id": 692, "seek": 394280, "start": 3942.8, "end": 3949.6000000000004, "text": " possibility of a causal structure so just looking for underlying hidden structures um that that you", "tokens": [50364, 7959, 295, 257, 38755, 3877, 370, 445, 1237, 337, 14217, 7633, 9227, 1105, 300, 300, 291, 50704], "temperature": 0.0, "avg_logprob": -0.08795268513331904, "compression_ratio": 1.8327137546468402, "no_speech_prob": 0.01970655284821987}, {"id": 693, "seek": 394280, "start": 3949.6000000000004, "end": 3955.44, "text": " know they're more generalizable that could explain large pieces of the of the data and give kind of", "tokens": [50704, 458, 436, 434, 544, 2674, 22395, 300, 727, 2903, 2416, 3755, 295, 264, 295, 264, 1412, 293, 976, 733, 295, 50996], "temperature": 0.0, "avg_logprob": -0.08795268513331904, "compression_ratio": 1.8327137546468402, "no_speech_prob": 0.01970655284821987}, {"id": 694, "seek": 394280, "start": 3955.44, "end": 3961.6800000000003, "text": " a list of hey there might be a causal factor here like go investigate it but on on that there's a", "tokens": [50996, 257, 1329, 295, 4177, 456, 1062, 312, 257, 38755, 5952, 510, 411, 352, 15013, 309, 457, 322, 322, 300, 456, 311, 257, 51308], "temperature": 0.0, "avg_logprob": -0.08795268513331904, "compression_ratio": 1.8327137546468402, "no_speech_prob": 0.01970655284821987}, {"id": 695, "seek": 394280, "start": 3961.6800000000003, "end": 3966.5600000000004, "text": " difference between a causal factor and a causal structure i think that the challenge is that we", "tokens": [51308, 2649, 1296, 257, 38755, 5952, 293, 257, 38755, 3877, 741, 519, 300, 264, 3430, 307, 300, 321, 51552], "temperature": 0.0, "avg_logprob": -0.08795268513331904, "compression_ratio": 1.8327137546468402, "no_speech_prob": 0.01970655284821987}, {"id": 696, "seek": 394280, "start": 3966.5600000000004, "end": 3971.1200000000003, "text": " don't have enough fidelity in the structure that benjo by the way is doing some interesting work on", "tokens": [51552, 500, 380, 362, 1547, 46404, 294, 264, 3877, 300, 3271, 5134, 538, 264, 636, 307, 884, 512, 1880, 589, 322, 51780], "temperature": 0.0, "avg_logprob": -0.08795268513331904, "compression_ratio": 1.8327137546468402, "no_speech_prob": 0.01970655284821987}, {"id": 697, "seek": 397112, "start": 3971.12, "end": 3977.68, "text": " this using data driven approaches to um you know learn causal factors but it's the structure of the", "tokens": [50364, 341, 1228, 1412, 9555, 11587, 281, 1105, 291, 458, 1466, 38755, 6771, 457, 309, 311, 264, 3877, 295, 264, 50692], "temperature": 0.0, "avg_logprob": -0.07198816242784557, "compression_ratio": 1.861003861003861, "no_speech_prob": 0.001808152417652309}, {"id": 698, "seek": 397112, "start": 3977.68, "end": 3981.3599999999997, "text": " factor graph which i think is the important thing i mean this is one of the most interesting parts", "tokens": [50692, 5952, 4295, 597, 741, 519, 307, 264, 1021, 551, 741, 914, 341, 307, 472, 295, 264, 881, 1880, 3166, 50876], "temperature": 0.0, "avg_logprob": -0.07198816242784557, "compression_ratio": 1.861003861003861, "no_speech_prob": 0.001808152417652309}, {"id": 699, "seek": 397112, "start": 3981.3599999999997, "end": 3985.8399999999997, "text": " i think of machine learning actually trying to learn causation from a data set which you can do", "tokens": [50876, 741, 519, 295, 3479, 2539, 767, 1382, 281, 1466, 3302, 399, 490, 257, 1412, 992, 597, 291, 393, 360, 51100], "temperature": 0.0, "avg_logprob": -0.07198816242784557, "compression_ratio": 1.861003861003861, "no_speech_prob": 0.001808152417652309}, {"id": 700, "seek": 397112, "start": 3985.8399999999997, "end": 3990.3199999999997, "text": " right things like beta networks where you specify all your variables you connect these nodes with", "tokens": [51100, 558, 721, 411, 9861, 9590, 689, 291, 16500, 439, 428, 9102, 291, 1745, 613, 13891, 365, 51324], "temperature": 0.0, "avg_logprob": -0.07198816242784557, "compression_ratio": 1.861003861003861, "no_speech_prob": 0.001808152417652309}, {"id": 701, "seek": 397112, "start": 3990.3199999999997, "end": 3996.4, "text": " edges and you can try to learn the optimal structure like the simplest structure and that", "tokens": [51324, 8819, 293, 291, 393, 853, 281, 1466, 264, 16252, 3877, 411, 264, 22811, 3877, 293, 300, 51628], "temperature": 0.0, "avg_logprob": -0.07198816242784557, "compression_ratio": 1.861003861003861, "no_speech_prob": 0.001808152417652309}, {"id": 702, "seek": 399640, "start": 3996.4, "end": 4002.2400000000002, "text": " sometimes turns out to be the real life causal effect if you do it well but the difference is", "tokens": [50364, 2171, 4523, 484, 281, 312, 264, 957, 993, 38755, 1802, 498, 291, 360, 309, 731, 457, 264, 2649, 307, 50656], "temperature": 0.0, "avg_logprob": -0.04354194972826087, "compression_ratio": 1.9130434782608696, "no_speech_prob": 0.0014051946345716715}, {"id": 703, "seek": 399640, "start": 4002.2400000000002, "end": 4009.2000000000003, "text": " you as a human you you know the causal structure and you've you've created that that graph so it's", "tokens": [50656, 291, 382, 257, 1952, 291, 291, 458, 264, 38755, 3877, 293, 291, 600, 291, 600, 2942, 300, 300, 4295, 370, 309, 311, 51004], "temperature": 0.0, "avg_logprob": -0.04354194972826087, "compression_ratio": 1.9130434782608696, "no_speech_prob": 0.0014051946345716715}, {"id": 704, "seek": 399640, "start": 4009.2000000000003, "end": 4013.76, "text": " not learned you've created it you can learn these things from data right you can actually you can", "tokens": [51004, 406, 3264, 291, 600, 2942, 309, 291, 393, 1466, 613, 721, 490, 1412, 558, 291, 393, 767, 291, 393, 51232], "temperature": 0.0, "avg_logprob": -0.04354194972826087, "compression_ratio": 1.9130434782608696, "no_speech_prob": 0.0014051946345716715}, {"id": 705, "seek": 399640, "start": 4013.76, "end": 4019.2000000000003, "text": " search over the set of all possible graphs all the possible edges and you have a bit of a loss", "tokens": [51232, 3164, 670, 264, 992, 295, 439, 1944, 24877, 439, 264, 1944, 8819, 293, 291, 362, 257, 857, 295, 257, 4470, 51504], "temperature": 0.0, "avg_logprob": -0.04354194972826087, "compression_ratio": 1.9130434782608696, "no_speech_prob": 0.0014051946345716715}, {"id": 706, "seek": 399640, "start": 4019.2000000000003, "end": 4023.36, "text": " function you try to find a graph that fits the data well so it's got enough edges but it's not too", "tokens": [51504, 2445, 291, 853, 281, 915, 257, 4295, 300, 9001, 264, 1412, 731, 370, 309, 311, 658, 1547, 8819, 457, 309, 311, 406, 886, 51712], "temperature": 0.0, "avg_logprob": -0.04354194972826087, "compression_ratio": 1.9130434782608696, "no_speech_prob": 0.0014051946345716715}, {"id": 707, "seek": 402336, "start": 4023.36, "end": 4028.1600000000003, "text": " complex you're not relating everything to everything and so just from data without any human input", "tokens": [50364, 3997, 291, 434, 406, 23968, 1203, 281, 1203, 293, 370, 445, 490, 1412, 1553, 604, 1952, 4846, 50604], "temperature": 0.0, "avg_logprob": -0.08822220737494312, "compression_ratio": 1.8467432950191571, "no_speech_prob": 0.002405324950814247}, {"id": 708, "seek": 402336, "start": 4028.1600000000003, "end": 4033.6800000000003, "text": " with structure learning you can sometimes get a model that kind of out of nothing will give you", "tokens": [50604, 365, 3877, 2539, 291, 393, 2171, 483, 257, 2316, 300, 733, 295, 484, 295, 1825, 486, 976, 291, 50880], "temperature": 0.0, "avg_logprob": -0.08822220737494312, "compression_ratio": 1.8467432950191571, "no_speech_prob": 0.002405324950814247}, {"id": 709, "seek": 402336, "start": 4033.6800000000003, "end": 4038.56, "text": " the cause of relationships sometimes there is redundancy right because a graph that says a", "tokens": [50880, 264, 3082, 295, 6159, 2171, 456, 307, 27830, 6717, 558, 570, 257, 4295, 300, 1619, 257, 51124], "temperature": 0.0, "avg_logprob": -0.08822220737494312, "compression_ratio": 1.8467432950191571, "no_speech_prob": 0.002405324950814247}, {"id": 710, "seek": 402336, "start": 4038.56, "end": 4045.6, "text": " implied near causes b equals a c that's identical to c causes b causes a right but even then with", "tokens": [51124, 32614, 2651, 7700, 272, 6915, 257, 269, 300, 311, 14800, 281, 269, 7700, 272, 7700, 257, 558, 457, 754, 550, 365, 51476], "temperature": 0.0, "avg_logprob": -0.08822220737494312, "compression_ratio": 1.8467432950191571, "no_speech_prob": 0.002405324950814247}, {"id": 711, "seek": 402336, "start": 4045.6, "end": 4051.28, "text": " structure learning you you've got this adjacency matrix and all of those nodes you've already come", "tokens": [51476, 3877, 2539, 291, 291, 600, 658, 341, 22940, 3020, 8141, 293, 439, 295, 729, 13891, 291, 600, 1217, 808, 51760], "temperature": 0.0, "avg_logprob": -0.08822220737494312, "compression_ratio": 1.8467432950191571, "no_speech_prob": 0.002405324950814247}, {"id": 712, "seek": 405128, "start": 4051.28, "end": 4056.8, "text": " up with a priori so what you want to learn is what the nodes are themselves right yeah i think", "tokens": [50364, 493, 365, 257, 4059, 72, 370, 437, 291, 528, 281, 1466, 307, 437, 264, 13891, 366, 2969, 558, 1338, 741, 519, 50640], "temperature": 0.0, "avg_logprob": -0.10890180030755237, "compression_ratio": 1.8818897637795275, "no_speech_prob": 0.0063650887459516525}, {"id": 713, "seek": 405128, "start": 4056.8, "end": 4061.2000000000003, "text": " like what kona mentioned there's lots of moda setting where you have well defined features", "tokens": [50640, 411, 437, 350, 4037, 2835, 456, 311, 3195, 295, 1072, 64, 3287, 689, 291, 362, 731, 7642, 4122, 50860], "temperature": 0.0, "avg_logprob": -0.10890180030755237, "compression_ratio": 1.8818897637795275, "no_speech_prob": 0.0063650887459516525}, {"id": 714, "seek": 405128, "start": 4061.92, "end": 4067.28, "text": " and i think what tim referred to was more like the you don't even know what the features are like", "tokens": [50896, 293, 741, 519, 437, 524, 10839, 281, 390, 544, 411, 264, 291, 500, 380, 754, 458, 437, 264, 4122, 366, 411, 51164], "temperature": 0.0, "avg_logprob": -0.10890180030755237, "compression_ratio": 1.8818897637795275, "no_speech_prob": 0.0063650887459516525}, {"id": 715, "seek": 405128, "start": 4067.28, "end": 4073.2000000000003, "text": " if you have a convolutional neural network and and like what's an object um what is a feature that", "tokens": [51164, 498, 291, 362, 257, 45216, 304, 18161, 3209, 293, 293, 411, 437, 311, 364, 2657, 1105, 437, 307, 257, 4111, 300, 51460], "temperature": 0.0, "avg_logprob": -0.10890180030755237, "compression_ratio": 1.8818897637795275, "no_speech_prob": 0.0063650887459516525}, {"id": 716, "seek": 405128, "start": 4073.2000000000003, "end": 4079.28, "text": " like is disentangled also from other objects um so i think also there is the big issue that you", "tokens": [51460, 411, 307, 37313, 39101, 611, 490, 661, 6565, 1105, 370, 741, 519, 611, 456, 307, 264, 955, 2734, 300, 291, 51764], "temperature": 0.0, "avg_logprob": -0.10890180030755237, "compression_ratio": 1.8818897637795275, "no_speech_prob": 0.0063650887459516525}, {"id": 717, "seek": 407928, "start": 4079.28, "end": 4085.1200000000003, "text": " have this entanglement between concepts that i don't know that the frisbee is always with uh on", "tokens": [50364, 362, 341, 948, 656, 3054, 1296, 10392, 300, 741, 500, 380, 458, 300, 264, 431, 271, 24872, 307, 1009, 365, 2232, 322, 50656], "temperature": 0.0, "avg_logprob": -0.10588160713950356, "compression_ratio": 1.7623318385650224, "no_speech_prob": 0.0015661476645618677}, {"id": 718, "seek": 407928, "start": 4085.1200000000003, "end": 4092.96, "text": " the same image as a as a dog so um maybe the the neural network can't even separate these two things", "tokens": [50656, 264, 912, 3256, 382, 257, 382, 257, 3000, 370, 1105, 1310, 264, 264, 18161, 3209, 393, 380, 754, 4994, 613, 732, 721, 51048], "temperature": 0.0, "avg_logprob": -0.10588160713950356, "compression_ratio": 1.7623318385650224, "no_speech_prob": 0.0015661476645618677}, {"id": 719, "seek": 407928, "start": 4092.96, "end": 4099.6, "text": " then because they are too entangled in the data set to even discover the structure that that is", "tokens": [51048, 550, 570, 436, 366, 886, 948, 39101, 294, 264, 1412, 992, 281, 754, 4411, 264, 3877, 300, 300, 307, 51380], "temperature": 0.0, "avg_logprob": -0.10588160713950356, "compression_ratio": 1.7623318385650224, "no_speech_prob": 0.0015661476645618677}, {"id": 720, "seek": 407928, "start": 4099.6, "end": 4104.88, "text": " really underlying the the real world in this case that's a fascinating point actually because one of", "tokens": [51380, 534, 14217, 264, 264, 957, 1002, 294, 341, 1389, 300, 311, 257, 10343, 935, 767, 570, 472, 295, 51644], "temperature": 0.0, "avg_logprob": -0.10588160713950356, "compression_ratio": 1.7623318385650224, "no_speech_prob": 0.0015661476645618677}, {"id": 721, "seek": 410488, "start": 4104.88, "end": 4109.6, "text": " the reasons why there's no easy solution to adversarial examples is because you you learn these", "tokens": [50364, 264, 4112, 983, 456, 311, 572, 1858, 3827, 281, 17641, 44745, 5110, 307, 570, 291, 291, 1466, 613, 50600], "temperature": 0.0, "avg_logprob": -0.05044122596285237, "compression_ratio": 1.9114754098360656, "no_speech_prob": 0.05882633477449417}, {"id": 722, "seek": 410488, "start": 4109.6, "end": 4115.12, "text": " these non robust features and you might just think to yourself well um fur is a low magnitude", "tokens": [50600, 613, 2107, 13956, 4122, 293, 291, 1062, 445, 519, 281, 1803, 731, 1105, 2687, 307, 257, 2295, 15668, 50876], "temperature": 0.0, "avg_logprob": -0.05044122596285237, "compression_ratio": 1.9114754098360656, "no_speech_prob": 0.05882633477449417}, {"id": 723, "seek": 410488, "start": 4115.12, "end": 4119.12, "text": " feature it's really easy just to kind of create fur on anything and for the neural network into", "tokens": [50876, 4111, 309, 311, 534, 1858, 445, 281, 733, 295, 1884, 2687, 322, 1340, 293, 337, 264, 18161, 3209, 666, 51076], "temperature": 0.0, "avg_logprob": -0.05044122596285237, "compression_ratio": 1.9114754098360656, "no_speech_prob": 0.05882633477449417}, {"id": 724, "seek": 410488, "start": 4119.12, "end": 4123.4400000000005, "text": " thinking it's a cat and you just say well this is obvious right you just create some rules to say", "tokens": [51076, 1953, 309, 311, 257, 3857, 293, 291, 445, 584, 731, 341, 307, 6322, 558, 291, 445, 1884, 512, 4474, 281, 584, 51292], "temperature": 0.0, "avg_logprob": -0.05044122596285237, "compression_ratio": 1.9114754098360656, "no_speech_prob": 0.05882633477449417}, {"id": 725, "seek": 410488, "start": 4123.4400000000005, "end": 4129.76, "text": " well if it's if it's not an animal and fur then ignore the fur but actually the features are entangled", "tokens": [51292, 731, 498, 309, 311, 498, 309, 311, 406, 364, 5496, 293, 2687, 550, 11200, 264, 2687, 457, 767, 264, 4122, 366, 948, 39101, 51608], "temperature": 0.0, "avg_logprob": -0.05044122596285237, "compression_ratio": 1.9114754098360656, "no_speech_prob": 0.05882633477449417}, {"id": 726, "seek": 410488, "start": 4129.76, "end": 4134.4800000000005, "text": " in this complex neural network so you can't do that but i wanted to move the discussion on a bit", "tokens": [51608, 294, 341, 3997, 18161, 3209, 370, 291, 393, 380, 360, 300, 457, 741, 1415, 281, 1286, 264, 5017, 322, 257, 857, 51844], "temperature": 0.0, "avg_logprob": -0.05044122596285237, "compression_ratio": 1.9114754098360656, "no_speech_prob": 0.05882633477449417}, {"id": 727, "seek": 413448, "start": 4134.48, "end": 4139.919999999999, "text": " so you said that there are some really interesting challenges ahead in in iml and what's fascinating", "tokens": [50364, 370, 291, 848, 300, 456, 366, 512, 534, 1880, 4759, 2286, 294, 294, 566, 75, 293, 437, 311, 10343, 50636], "temperature": 0.0, "avg_logprob": -0.05336433503685928, "compression_ratio": 1.9898305084745762, "no_speech_prob": 0.0018522365717217326}, {"id": 728, "seek": 413448, "start": 4139.919999999999, "end": 4143.759999999999, "text": " is you start talking about the process so you say that the setting of machine learning is too static", "tokens": [50636, 307, 291, 722, 1417, 466, 264, 1399, 370, 291, 584, 300, 264, 3287, 295, 3479, 2539, 307, 886, 13437, 50828], "temperature": 0.0, "avg_logprob": -0.05336433503685928, "compression_ratio": 1.9898305084745762, "no_speech_prob": 0.0018522365717217326}, {"id": 729, "seek": 413448, "start": 4143.759999999999, "end": 4148.4, "text": " it doesn't reflect how these models are used in reality and models are embedded in a process or a", "tokens": [50828, 309, 1177, 380, 5031, 577, 613, 5245, 366, 1143, 294, 4103, 293, 5245, 366, 16741, 294, 257, 1399, 420, 257, 51060], "temperature": 0.0, "avg_logprob": -0.05336433503685928, "compression_ratio": 1.9898305084745762, "no_speech_prob": 0.0018522365717217326}, {"id": 730, "seek": 413448, "start": 4148.4, "end": 4153.28, "text": " product or even complex people interactions and i love this right because i talk about ml devops", "tokens": [51060, 1674, 420, 754, 3997, 561, 13280, 293, 741, 959, 341, 558, 570, 741, 751, 466, 23271, 1905, 3370, 51304], "temperature": 0.0, "avg_logprob": -0.05336433503685928, "compression_ratio": 1.9898305084745762, "no_speech_prob": 0.0018522365717217326}, {"id": 731, "seek": 413448, "start": 4153.28, "end": 4157.679999999999, "text": " and machine learning models in isolation are irrelevant it's the people in the process that's", "tokens": [51304, 293, 3479, 2539, 5245, 294, 16001, 366, 28682, 309, 311, 264, 561, 294, 264, 1399, 300, 311, 51524], "temperature": 0.0, "avg_logprob": -0.05336433503685928, "compression_ratio": 1.9898305084745762, "no_speech_prob": 0.0018522365717217326}, {"id": 732, "seek": 413448, "start": 4157.679999999999, "end": 4162.5599999999995, "text": " where the complexity is even with intelligence itself it's a process right you know intelligence", "tokens": [51524, 689, 264, 14024, 307, 754, 365, 7599, 2564, 309, 311, 257, 1399, 558, 291, 458, 7599, 51768], "temperature": 0.0, "avg_logprob": -0.05336433503685928, "compression_ratio": 1.9898305084745762, "no_speech_prob": 0.0018522365717217326}, {"id": 733, "seek": 416256, "start": 4162.56, "end": 4167.68, "text": " is the interaction between a brain a body and an environment and you know within the context of", "tokens": [50364, 307, 264, 9285, 1296, 257, 3567, 257, 1772, 293, 364, 2823, 293, 291, 458, 1951, 264, 4319, 295, 50620], "temperature": 0.0, "avg_logprob": -0.07529701051257906, "compression_ratio": 1.8641509433962264, "no_speech_prob": 0.02617039531469345}, {"id": 734, "seek": 416256, "start": 4167.68, "end": 4171.76, "text": " this process you know we've got all of this rich information that we could be bringing in from other", "tokens": [50620, 341, 1399, 291, 458, 321, 600, 658, 439, 295, 341, 4593, 1589, 300, 321, 727, 312, 5062, 294, 490, 661, 50824], "temperature": 0.0, "avg_logprob": -0.07529701051257906, "compression_ratio": 1.8641509433962264, "no_speech_prob": 0.02617039531469345}, {"id": 735, "seek": 416256, "start": 4171.76, "end": 4176.4800000000005, "text": " disciplines and you're saying we should bring in compsci and stats folks and we should be bringing", "tokens": [50824, 21919, 293, 291, 434, 1566, 321, 820, 1565, 294, 715, 82, 537, 293, 18152, 4024, 293, 321, 820, 312, 5062, 51060], "temperature": 0.0, "avg_logprob": -0.07529701051257906, "compression_ratio": 1.8641509433962264, "no_speech_prob": 0.02617039531469345}, {"id": 736, "seek": 416256, "start": 4176.4800000000005, "end": 4181.4400000000005, "text": " in psychologists and social scientists and we need to also have interpretability at a higher level", "tokens": [51060, 294, 41562, 293, 2093, 7708, 293, 321, 643, 281, 611, 362, 7302, 2310, 412, 257, 2946, 1496, 51308], "temperature": 0.0, "avg_logprob": -0.07529701051257906, "compression_ratio": 1.8641509433962264, "no_speech_prob": 0.02617039531469345}, {"id": 737, "seek": 416256, "start": 4181.4400000000005, "end": 4186.4800000000005, "text": " at the institutional level right or at the society level so when you kind of broaden the discussion", "tokens": [51308, 412, 264, 18391, 1496, 558, 420, 412, 264, 4086, 1496, 370, 562, 291, 733, 295, 47045, 264, 5017, 51560], "temperature": 0.0, "avg_logprob": -0.07529701051257906, "compression_ratio": 1.8641509433962264, "no_speech_prob": 0.02617039531469345}, {"id": 738, "seek": 418648, "start": 4186.48, "end": 4193.28, "text": " out a little bit i think it adds a nice bit of flavor yeah so it's very especially as a scientist", "tokens": [50364, 484, 257, 707, 857, 741, 519, 309, 10860, 257, 1481, 857, 295, 6813, 1338, 370, 309, 311, 588, 2318, 382, 257, 12662, 50704], "temperature": 0.0, "avg_logprob": -0.08973807873933212, "compression_ratio": 1.7935779816513762, "no_speech_prob": 0.060032304376363754}, {"id": 739, "seek": 418648, "start": 4193.28, "end": 4199.28, "text": " it's so convenient to just have this fixed model a fixed data set and then you just geek out and", "tokens": [50704, 309, 311, 370, 10851, 281, 445, 362, 341, 6806, 2316, 257, 6806, 1412, 992, 293, 550, 291, 445, 36162, 484, 293, 51004], "temperature": 0.0, "avg_logprob": -0.08973807873933212, "compression_ratio": 1.7935779816513762, "no_speech_prob": 0.060032304376363754}, {"id": 740, "seek": 418648, "start": 4199.28, "end": 4205.599999999999, "text": " invent all these methods and so on but reality is that that you use the method some place and then", "tokens": [51004, 7962, 439, 613, 7150, 293, 370, 322, 457, 4103, 307, 300, 300, 291, 764, 264, 3170, 512, 1081, 293, 550, 51320], "temperature": 0.0, "avg_logprob": -0.08973807873933212, "compression_ratio": 1.7935779816513762, "no_speech_prob": 0.060032304376363754}, {"id": 741, "seek": 418648, "start": 4205.599999999999, "end": 4211.599999999999, "text": " it interacts with the institution it's built with the developers it's built by with the people it", "tokens": [51320, 309, 43582, 365, 264, 7818, 309, 311, 3094, 365, 264, 8849, 309, 311, 3094, 538, 365, 264, 561, 309, 51620], "temperature": 0.0, "avg_logprob": -0.08973807873933212, "compression_ratio": 1.7935779816513762, "no_speech_prob": 0.060032304376363754}, {"id": 742, "seek": 421160, "start": 4211.6, "end": 4218.72, "text": " affects and my favorite example there is when you have this closed loop where your model makes", "tokens": [50364, 11807, 293, 452, 2954, 1365, 456, 307, 562, 291, 362, 341, 5395, 6367, 689, 428, 2316, 1669, 50720], "temperature": 0.0, "avg_logprob": -0.07030698729724419, "compression_ratio": 1.9104477611940298, "no_speech_prob": 0.014501082710921764}, {"id": 743, "seek": 421160, "start": 4218.72, "end": 4225.68, "text": " predictions and these predictions generate the next generation's data so for the next generation", "tokens": [50720, 21264, 293, 613, 21264, 8460, 264, 958, 5125, 311, 1412, 370, 337, 264, 958, 5125, 51068], "temperature": 0.0, "avg_logprob": -0.07030698729724419, "compression_ratio": 1.9104477611940298, "no_speech_prob": 0.014501082710921764}, {"id": 744, "seek": 421160, "start": 4225.68, "end": 4231.84, "text": " of the model it produces the data so there's this example of the rent index where you have this model", "tokens": [51068, 295, 264, 2316, 309, 14725, 264, 1412, 370, 456, 311, 341, 1365, 295, 264, 6214, 8186, 689, 291, 362, 341, 2316, 51376], "temperature": 0.0, "avg_logprob": -0.07030698729724419, "compression_ratio": 1.9104477611940298, "no_speech_prob": 0.014501082710921764}, {"id": 745, "seek": 421160, "start": 4231.84, "end": 4238.56, "text": " that tells you how much rent you should like pay for a certain kind of apartment and so on", "tokens": [51376, 300, 5112, 291, 577, 709, 6214, 291, 820, 411, 1689, 337, 257, 1629, 733, 295, 9587, 293, 370, 322, 51712], "temperature": 0.0, "avg_logprob": -0.07030698729724419, "compression_ratio": 1.9104477611940298, "no_speech_prob": 0.014501082710921764}, {"id": 746, "seek": 423856, "start": 4239.04, "end": 4247.04, "text": " and this is actually like um legally binding so if you're a land lord you have to accept kind of", "tokens": [50388, 293, 341, 307, 767, 411, 1105, 21106, 17359, 370, 498, 291, 434, 257, 2117, 15448, 291, 362, 281, 3241, 733, 295, 50788], "temperature": 0.0, "avg_logprob": -0.09790601730346679, "compression_ratio": 1.7813953488372094, "no_speech_prob": 0.0022865994833409786}, {"id": 747, "seek": 423856, "start": 4247.04, "end": 4252.8, "text": " the range that is outputted by the model which also means that the data that is produced so the", "tokens": [50788, 264, 3613, 300, 307, 5598, 14727, 538, 264, 2316, 597, 611, 1355, 300, 264, 1412, 300, 307, 7126, 370, 264, 51076], "temperature": 0.0, "avg_logprob": -0.09790601730346679, "compression_ratio": 1.7813953488372094, "no_speech_prob": 0.0022865994833409786}, {"id": 748, "seek": 423856, "start": 4252.8, "end": 4259.76, "text": " new flats that are rented out in new apartments they all have to fit the model kind of and then", "tokens": [51076, 777, 43075, 300, 366, 32381, 484, 294, 777, 29056, 436, 439, 362, 281, 3318, 264, 2316, 733, 295, 293, 550, 51424], "temperature": 0.0, "avg_logprob": -0.09790601730346679, "compression_ratio": 1.7813953488372094, "no_speech_prob": 0.0022865994833409786}, {"id": 749, "seek": 423856, "start": 4259.76, "end": 4265.04, "text": " but then you use this data again to train your model so you have this very weird feedback loop", "tokens": [51424, 457, 550, 291, 764, 341, 1412, 797, 281, 3847, 428, 2316, 370, 291, 362, 341, 588, 3657, 5824, 6367, 51688], "temperature": 0.0, "avg_logprob": -0.09790601730346679, "compression_ratio": 1.7813953488372094, "no_speech_prob": 0.0022865994833409786}, {"id": 750, "seek": 426504, "start": 4265.76, "end": 4271.68, "text": " and I think it's also difficult to wrap your head around it and understand implications of it", "tokens": [50400, 293, 286, 519, 309, 311, 611, 2252, 281, 7019, 428, 1378, 926, 309, 293, 1223, 16602, 295, 309, 50696], "temperature": 0.0, "avg_logprob": -0.11269080638885498, "compression_ratio": 1.6607142857142858, "no_speech_prob": 0.0019873962737619877}, {"id": 751, "seek": 426504, "start": 4273.6, "end": 4281.04, "text": " that that same thing a very similar feedback loop was a fear in the you know in our algo", "tokens": [50792, 300, 300, 912, 551, 257, 588, 2531, 5824, 6367, 390, 257, 4240, 294, 264, 291, 458, 294, 527, 8655, 51164], "temperature": 0.0, "avg_logprob": -0.11269080638885498, "compression_ratio": 1.6607142857142858, "no_speech_prob": 0.0019873962737619877}, {"id": 752, "seek": 426504, "start": 4281.04, "end": 4287.36, "text": " shambles video about the uk testing since they couldn't conduct the the uh what was it the", "tokens": [51164, 402, 2173, 904, 960, 466, 264, 26769, 4997, 1670, 436, 2809, 380, 6018, 264, 264, 2232, 437, 390, 309, 264, 51480], "temperature": 0.0, "avg_logprob": -0.11269080638885498, "compression_ratio": 1.6607142857142858, "no_speech_prob": 0.0019873962737619877}, {"id": 753, "seek": 426504, "start": 4287.36, "end": 4292.32, "text": " a-level test right Tim they they built some some models around that and so it would do things like", "tokens": [51480, 257, 12, 12418, 1500, 558, 7172, 436, 436, 3094, 512, 512, 5245, 926, 300, 293, 370, 309, 576, 360, 721, 411, 51728], "temperature": 0.0, "avg_logprob": -0.11269080638885498, "compression_ratio": 1.6607142857142858, "no_speech_prob": 0.0019873962737619877}, {"id": 754, "seek": 429232, "start": 4292.32, "end": 4297.5199999999995, "text": " well you know if this school historically never had anyone in this grade bucket then we're not", "tokens": [50364, 731, 291, 458, 498, 341, 1395, 16180, 1128, 632, 2878, 294, 341, 7204, 13058, 550, 321, 434, 406, 50624], "temperature": 0.0, "avg_logprob": -0.0919154217368678, "compression_ratio": 1.7280701754385965, "no_speech_prob": 0.0038817008025944233}, {"id": 755, "seek": 429232, "start": 4297.5199999999995, "end": 4303.04, "text": " going to assign anyone to that grade bucket in that school and so it's sort of this self-perpetuating", "tokens": [50624, 516, 281, 6269, 2878, 281, 300, 7204, 13058, 294, 300, 1395, 293, 370, 309, 311, 1333, 295, 341, 2698, 12, 610, 7275, 32438, 50900], "temperature": 0.0, "avg_logprob": -0.0919154217368678, "compression_ratio": 1.7280701754385965, "no_speech_prob": 0.0038817008025944233}, {"id": 756, "seek": 429232, "start": 4303.599999999999, "end": 4310.88, "text": " you know feedback loop we were reading through a lot of your work and you I mean I'm just going", "tokens": [50928, 291, 458, 5824, 6367, 321, 645, 3760, 807, 257, 688, 295, 428, 589, 293, 291, 286, 914, 286, 478, 445, 516, 51292], "temperature": 0.0, "avg_logprob": -0.0919154217368678, "compression_ratio": 1.7280701754385965, "no_speech_prob": 0.0038817008025944233}, {"id": 757, "seek": 429232, "start": 4310.88, "end": 4316.639999999999, "text": " to hit this point head on um you don't really talk that much about AI ethics and you know there's the", "tokens": [51292, 281, 2045, 341, 935, 1378, 322, 1105, 291, 500, 380, 534, 751, 300, 709, 466, 7318, 19769, 293, 291, 458, 456, 311, 264, 51580], "temperature": 0.0, "avg_logprob": -0.0919154217368678, "compression_ratio": 1.7280701754385965, "no_speech_prob": 0.0038817008025944233}, {"id": 758, "seek": 431664, "start": 4316.64, "end": 4321.6, "text": " f word which is the fairness word and and I don't I don't recall you ever using that word", "tokens": [50364, 283, 1349, 597, 307, 264, 29765, 1349, 293, 293, 286, 500, 380, 286, 500, 380, 9901, 291, 1562, 1228, 300, 1349, 50612], "temperature": 0.0, "avg_logprob": -0.09637063947217218, "compression_ratio": 1.8991935483870968, "no_speech_prob": 0.061637427657842636}, {"id": 759, "seek": 431664, "start": 4321.6, "end": 4329.04, "text": " and is that something that you've deliberately shied away from um yeah I just like um", "tokens": [50612, 293, 307, 300, 746, 300, 291, 600, 23506, 402, 1091, 1314, 490, 1105, 1338, 286, 445, 411, 1105, 50984], "temperature": 0.0, "avg_logprob": -0.09637063947217218, "compression_ratio": 1.8991935483870968, "no_speech_prob": 0.061637427657842636}, {"id": 760, "seek": 431664, "start": 4330.320000000001, "end": 4336.400000000001, "text": " define it as outside of the scope like to talk to to I don't know talk about ethics so um fairness", "tokens": [51048, 6964, 309, 382, 2380, 295, 264, 11923, 411, 281, 751, 281, 281, 286, 500, 380, 458, 751, 466, 19769, 370, 1105, 29765, 51352], "temperature": 0.0, "avg_logprob": -0.09637063947217218, "compression_ratio": 1.8991935483870968, "no_speech_prob": 0.061637427657842636}, {"id": 761, "seek": 431664, "start": 4336.400000000001, "end": 4341.92, "text": " metrics or so on because I think there's a really big field on its own and I just don't know as much", "tokens": [51352, 16367, 420, 370, 322, 570, 286, 519, 456, 311, 257, 534, 955, 2519, 322, 1080, 1065, 293, 286, 445, 500, 380, 458, 382, 709, 51628], "temperature": 0.0, "avg_logprob": -0.09637063947217218, "compression_ratio": 1.8991935483870968, "no_speech_prob": 0.061637427657842636}, {"id": 762, "seek": 431664, "start": 4341.92, "end": 4346.160000000001, "text": " like about all these things so I know a little bit like about the fairness metrics that they're", "tokens": [51628, 411, 466, 439, 613, 721, 370, 286, 458, 257, 707, 857, 411, 466, 264, 29765, 16367, 300, 436, 434, 51840], "temperature": 0.0, "avg_logprob": -0.09637063947217218, "compression_ratio": 1.8991935483870968, "no_speech_prob": 0.061637427657842636}, {"id": 763, "seek": 434616, "start": 4346.16, "end": 4354.08, "text": " out there um I also think I mean they're kind of like research wise a little bit overlapping but", "tokens": [50364, 484, 456, 1105, 286, 611, 519, 286, 914, 436, 434, 733, 295, 411, 2132, 10829, 257, 707, 857, 33535, 457, 50760], "temperature": 0.0, "avg_logprob": -0.05649272420189597, "compression_ratio": 1.7808219178082192, "no_speech_prob": 0.002182369353249669}, {"id": 764, "seek": 434616, "start": 4354.08, "end": 4359.04, "text": " more or less separate fields I think interpretability and fairness um but of course they have some", "tokens": [50760, 544, 420, 1570, 4994, 7909, 286, 519, 7302, 2310, 293, 29765, 1105, 457, 295, 1164, 436, 362, 512, 51008], "temperature": 0.0, "avg_logprob": -0.05649272420189597, "compression_ratio": 1.7808219178082192, "no_speech_prob": 0.002182369353249669}, {"id": 765, "seek": 434616, "start": 4359.04, "end": 4366.16, "text": " commonalities that I mean when you kind of to for fairness you have to look it's not necessarily", "tokens": [51008, 2689, 16110, 300, 286, 914, 562, 291, 733, 295, 281, 337, 29765, 291, 362, 281, 574, 309, 311, 406, 4725, 51364], "temperature": 0.0, "avg_logprob": -0.05649272420189597, "compression_ratio": 1.7808219178082192, "no_speech_prob": 0.002182369353249669}, {"id": 766, "seek": 434616, "start": 4366.16, "end": 4372.96, "text": " inside the model but you have to study how the model behaves um and that's kind of the connection", "tokens": [51364, 1854, 264, 2316, 457, 291, 362, 281, 2979, 577, 264, 2316, 36896, 1105, 293, 300, 311, 733, 295, 264, 4984, 51704], "temperature": 0.0, "avg_logprob": -0.05649272420189597, "compression_ratio": 1.7808219178082192, "no_speech_prob": 0.002182369353249669}, {"id": 767, "seek": 437296, "start": 4373.04, "end": 4378.8, "text": " to interpretability I would say yeah well where yeah where I see the connection is work", "tokens": [50368, 281, 7302, 2310, 286, 576, 584, 1338, 731, 689, 1338, 689, 286, 536, 264, 4984, 307, 589, 50656], "temperature": 0.0, "avg_logprob": -0.0737911433708377, "compression_ratio": 1.7239819004524888, "no_speech_prob": 0.015400160104036331}, {"id": 768, "seek": 437296, "start": 4378.8, "end": 4386.24, "text": " like yours is helping to build the tool set that will allow people to apply human you know intuition", "tokens": [50656, 411, 6342, 307, 4315, 281, 1322, 264, 2290, 992, 300, 486, 2089, 561, 281, 3079, 1952, 291, 458, 24002, 51028], "temperature": 0.0, "avg_logprob": -0.0737911433708377, "compression_ratio": 1.7239819004524888, "no_speech_prob": 0.015400160104036331}, {"id": 769, "seek": 437296, "start": 4386.24, "end": 4392.56, "text": " and and ethics and evaluations to machine learning because at the end of the day a lot of these are", "tokens": [51028, 293, 293, 19769, 293, 43085, 281, 3479, 2539, 570, 412, 264, 917, 295, 264, 786, 257, 688, 295, 613, 366, 51344], "temperature": 0.0, "avg_logprob": -0.0737911433708377, "compression_ratio": 1.7239819004524888, "no_speech_prob": 0.015400160104036331}, {"id": 770, "seek": 437296, "start": 4392.56, "end": 4398.64, "text": " human moral judgments or ethical judgments and it's important that people be happy with them", "tokens": [51344, 1952, 9723, 40337, 420, 18890, 40337, 293, 309, 311, 1021, 300, 561, 312, 2055, 365, 552, 51648], "temperature": 0.0, "avg_logprob": -0.0737911433708377, "compression_ratio": 1.7239819004524888, "no_speech_prob": 0.015400160104036331}, {"id": 771, "seek": 439864, "start": 4398.64, "end": 4404.240000000001, "text": " because you know we have to have the population as a whole understand and accept and be able to", "tokens": [50364, 570, 291, 458, 321, 362, 281, 362, 264, 4415, 382, 257, 1379, 1223, 293, 3241, 293, 312, 1075, 281, 50644], "temperature": 0.0, "avg_logprob": -0.047742723344682575, "compression_ratio": 1.8377358490566038, "no_speech_prob": 0.004608663730323315}, {"id": 772, "seek": 439864, "start": 4404.240000000001, "end": 4410.0, "text": " move forward with the increasing role that machine learning is having in our lives and building that", "tokens": [50644, 1286, 2128, 365, 264, 5662, 3090, 300, 3479, 2539, 307, 1419, 294, 527, 2909, 293, 2390, 300, 50932], "temperature": 0.0, "avg_logprob": -0.047742723344682575, "compression_ratio": 1.8377358490566038, "no_speech_prob": 0.004608663730323315}, {"id": 773, "seek": 439864, "start": 4410.0, "end": 4414.8, "text": " tool set is necessary so it's like you said very early in this talk you know what do we do just", "tokens": [50932, 2290, 992, 307, 4818, 370, 309, 311, 411, 291, 848, 588, 2440, 294, 341, 751, 291, 458, 437, 360, 321, 360, 445, 51172], "temperature": 0.0, "avg_logprob": -0.047742723344682575, "compression_ratio": 1.8377358490566038, "no_speech_prob": 0.004608663730323315}, {"id": 774, "seek": 439864, "start": 4414.8, "end": 4419.76, "text": " stick our heads in the sand and ignore it and just accept machine learning models are going to do", "tokens": [51172, 2897, 527, 8050, 294, 264, 4932, 293, 11200, 309, 293, 445, 3241, 3479, 2539, 5245, 366, 516, 281, 360, 51420], "temperature": 0.0, "avg_logprob": -0.047742723344682575, "compression_ratio": 1.8377358490566038, "no_speech_prob": 0.004608663730323315}, {"id": 775, "seek": 439864, "start": 4419.76, "end": 4425.12, "text": " whatever they do as long as they fly the plane or you know don't kill too many people we're okay", "tokens": [51420, 2035, 436, 360, 382, 938, 382, 436, 3603, 264, 5720, 420, 291, 458, 500, 380, 1961, 886, 867, 561, 321, 434, 1392, 51688], "temperature": 0.0, "avg_logprob": -0.047742723344682575, "compression_ratio": 1.8377358490566038, "no_speech_prob": 0.004608663730323315}, {"id": 776, "seek": 442512, "start": 4425.12, "end": 4429.5199999999995, "text": " like I don't think that's going to work like we have to build the tool set that you're talking", "tokens": [50364, 411, 286, 500, 380, 519, 300, 311, 516, 281, 589, 411, 321, 362, 281, 1322, 264, 2290, 992, 300, 291, 434, 1417, 50584], "temperature": 0.0, "avg_logprob": -0.04396908238249005, "compression_ratio": 1.7806691449814127, "no_speech_prob": 0.003162260865792632}, {"id": 777, "seek": 442512, "start": 4429.5199999999995, "end": 4436.08, "text": " about and continue this process of exploring how to better explain and interpret ML models so that", "tokens": [50584, 466, 293, 2354, 341, 1399, 295, 12736, 577, 281, 1101, 2903, 293, 7302, 21601, 5245, 370, 300, 50912], "temperature": 0.0, "avg_logprob": -0.04396908238249005, "compression_ratio": 1.7806691449814127, "no_speech_prob": 0.003162260865792632}, {"id": 778, "seek": 442512, "start": 4436.08, "end": 4441.2, "text": " human beings can have that oversight because it's the only thing that's going to give us", "tokens": [50912, 1952, 8958, 393, 362, 300, 29146, 570, 309, 311, 264, 787, 551, 300, 311, 516, 281, 976, 505, 51168], "temperature": 0.0, "avg_logprob": -0.04396908238249005, "compression_ratio": 1.7806691449814127, "no_speech_prob": 0.003162260865792632}, {"id": 779, "seek": 442512, "start": 4441.2, "end": 4446.96, "text": " comfort really as a society I suppose the reason I segue to this is we were just talking about", "tokens": [51168, 3400, 534, 382, 257, 4086, 286, 7297, 264, 1778, 286, 33850, 281, 341, 307, 321, 645, 445, 1417, 466, 51456], "temperature": 0.0, "avg_logprob": -0.04396908238249005, "compression_ratio": 1.7806691449814127, "no_speech_prob": 0.003162260865792632}, {"id": 780, "seek": 442512, "start": 4446.96, "end": 4452.8, "text": " the process and you you mentioned some of these feedback loops because we can have a very superficial", "tokens": [51456, 264, 1399, 293, 291, 291, 2835, 512, 295, 613, 5824, 16121, 570, 321, 393, 362, 257, 588, 34622, 51748], "temperature": 0.0, "avg_logprob": -0.04396908238249005, "compression_ratio": 1.7806691449814127, "no_speech_prob": 0.003162260865792632}, {"id": 781, "seek": 445280, "start": 4452.8, "end": 4458.72, "text": " discussion and you could say well we need to be able to represent reality better than we do and", "tokens": [50364, 5017, 293, 291, 727, 584, 731, 321, 643, 281, 312, 1075, 281, 2906, 4103, 1101, 813, 321, 360, 293, 50660], "temperature": 0.0, "avg_logprob": -0.0595073789920447, "compression_ratio": 1.7769516728624535, "no_speech_prob": 0.052698392421007156}, {"id": 782, "seek": 445280, "start": 4458.72, "end": 4464.56, "text": " we have a whole tool set here to identify sources of bias or you know lack of robustness etc in", "tokens": [50660, 321, 362, 257, 1379, 2290, 992, 510, 281, 5876, 7139, 295, 12577, 420, 291, 458, 5011, 295, 13956, 1287, 5183, 294, 50952], "temperature": 0.0, "avg_logprob": -0.0595073789920447, "compression_ratio": 1.7769516728624535, "no_speech_prob": 0.052698392421007156}, {"id": 783, "seek": 445280, "start": 4464.56, "end": 4469.92, "text": " models but it's so much more complex than that because these models are used in a very complex", "tokens": [50952, 5245, 457, 309, 311, 370, 709, 544, 3997, 813, 300, 570, 613, 5245, 366, 1143, 294, 257, 588, 3997, 51220], "temperature": 0.0, "avg_logprob": -0.0595073789920447, "compression_ratio": 1.7769516728624535, "no_speech_prob": 0.052698392421007156}, {"id": 784, "seek": 445280, "start": 4469.92, "end": 4475.76, "text": " process and you get these very very complex dynamics emerging as a result of that and I think", "tokens": [51220, 1399, 293, 291, 483, 613, 588, 588, 3997, 15679, 14989, 382, 257, 1874, 295, 300, 293, 286, 519, 51512], "temperature": 0.0, "avg_logprob": -0.0595073789920447, "compression_ratio": 1.7769516728624535, "no_speech_prob": 0.052698392421007156}, {"id": 785, "seek": 445280, "start": 4475.76, "end": 4481.52, "text": " we're only really just scratching the surface of understanding those dynamics yeah I think so too", "tokens": [51512, 321, 434, 787, 534, 445, 29699, 264, 3753, 295, 3701, 729, 15679, 1338, 286, 519, 370, 886, 51800], "temperature": 0.0, "avg_logprob": -0.0595073789920447, "compression_ratio": 1.7769516728624535, "no_speech_prob": 0.052698392421007156}, {"id": 786, "seek": 448152, "start": 4482.0, "end": 4489.360000000001, "text": " as I said I think in science it's always very easy to to study things in isolation like study one", "tokens": [50388, 382, 286, 848, 286, 519, 294, 3497, 309, 311, 1009, 588, 1858, 281, 281, 2979, 721, 294, 16001, 411, 2979, 472, 50756], "temperature": 0.0, "avg_logprob": -0.08497363328933716, "compression_ratio": 1.7074235807860263, "no_speech_prob": 0.006486398633569479}, {"id": 787, "seek": 448152, "start": 4489.360000000001, "end": 4496.64, "text": " type of model study one type of adjustment for a deep neural network and hopefully we will see more", "tokens": [50756, 2010, 295, 2316, 2979, 472, 2010, 295, 17132, 337, 257, 2452, 18161, 3209, 293, 4696, 321, 486, 536, 544, 51120], "temperature": 0.0, "avg_logprob": -0.08497363328933716, "compression_ratio": 1.7074235807860263, "no_speech_prob": 0.006486398633569479}, {"id": 788, "seek": 448152, "start": 4496.64, "end": 4502.72, "text": " work emerge on this I think I've never read the paper like I mean of course discussed implications", "tokens": [51120, 589, 21511, 322, 341, 286, 519, 286, 600, 1128, 1401, 264, 3035, 411, 286, 914, 295, 1164, 7152, 16602, 51424], "temperature": 0.0, "avg_logprob": -0.08497363328933716, "compression_ratio": 1.7074235807860263, "no_speech_prob": 0.006486398633569479}, {"id": 789, "seek": 448152, "start": 4502.72, "end": 4509.4400000000005, "text": " but really like analyze like what happens in terms of the data and the model when we have like", "tokens": [51424, 457, 534, 411, 12477, 411, 437, 2314, 294, 2115, 295, 264, 1412, 293, 264, 2316, 562, 321, 362, 411, 51760], "temperature": 0.0, "avg_logprob": -0.08497363328933716, "compression_ratio": 1.7074235807860263, "no_speech_prob": 0.006486398633569479}, {"id": 790, "seek": 450944, "start": 4509.44, "end": 4514.639999999999, "text": " multiple generations for example of a model and how it changes over time but this thing I mean to", "tokens": [50364, 3866, 10593, 337, 1365, 295, 257, 2316, 293, 577, 309, 2962, 670, 565, 457, 341, 551, 286, 914, 281, 50624], "temperature": 0.0, "avg_logprob": -0.06546094557818244, "compression_ratio": 1.670940170940171, "no_speech_prob": 0.0019567860290408134}, {"id": 791, "seek": 450944, "start": 4514.639999999999, "end": 4521.28, "text": " study those things also means that you have to wait for a long time until you have these dynamics", "tokens": [50624, 2979, 729, 721, 611, 1355, 300, 291, 362, 281, 1699, 337, 257, 938, 565, 1826, 291, 362, 613, 15679, 50956], "temperature": 0.0, "avg_logprob": -0.06546094557818244, "compression_ratio": 1.670940170940171, "no_speech_prob": 0.0019567860290408134}, {"id": 792, "seek": 450944, "start": 4521.28, "end": 4527.12, "text": " and I think in many cases it's just starting that we use these models more extensively in our daily", "tokens": [50956, 293, 286, 519, 294, 867, 3331, 309, 311, 445, 2891, 300, 321, 764, 613, 5245, 544, 32636, 294, 527, 5212, 51248], "temperature": 0.0, "avg_logprob": -0.06546094557818244, "compression_ratio": 1.670940170940171, "no_speech_prob": 0.0019567860290408134}, {"id": 793, "seek": 450944, "start": 4527.12, "end": 4534.639999999999, "text": " life I have a question is is anyone because look interpretability metrics whatever they are say", "tokens": [51248, 993, 286, 362, 257, 1168, 307, 307, 2878, 570, 574, 7302, 2310, 16367, 2035, 436, 366, 584, 51624], "temperature": 0.0, "avg_logprob": -0.06546094557818244, "compression_ratio": 1.670940170940171, "no_speech_prob": 0.0019567860290408134}, {"id": 794, "seek": 453464, "start": 4534.72, "end": 4539.68, "text": " saliency maps and you know could be partial dependency plots whatever you could actually", "tokens": [50368, 1845, 7848, 11317, 293, 291, 458, 727, 312, 14641, 33621, 28609, 2035, 291, 727, 767, 50616], "temperature": 0.0, "avg_logprob": -0.07597100156024822, "compression_ratio": 1.7061068702290076, "no_speech_prob": 0.06369100511074066}, {"id": 795, "seek": 453464, "start": 4539.68, "end": 4545.360000000001, "text": " build in some requirements of those into the objective functions when you go to train models", "tokens": [50616, 1322, 294, 512, 7728, 295, 729, 666, 264, 10024, 6828, 562, 291, 352, 281, 3847, 5245, 50900], "temperature": 0.0, "avg_logprob": -0.07597100156024822, "compression_ratio": 1.7061068702290076, "no_speech_prob": 0.06369100511074066}, {"id": 796, "seek": 453464, "start": 4545.360000000001, "end": 4549.52, "text": " so for example I'm just going to come up with a crazy idea I have no idea if this is relevant at", "tokens": [50900, 370, 337, 1365, 286, 478, 445, 516, 281, 808, 493, 365, 257, 3219, 1558, 286, 362, 572, 1558, 498, 341, 307, 7340, 412, 51108], "temperature": 0.0, "avg_logprob": -0.07597100156024822, "compression_ratio": 1.7061068702290076, "no_speech_prob": 0.06369100511074066}, {"id": 797, "seek": 453464, "start": 4549.52, "end": 4555.92, "text": " all but somebody could say look I want all my saliency maps to be you know sets of of a", "tokens": [51108, 439, 457, 2618, 727, 584, 574, 286, 528, 439, 452, 1845, 7848, 11317, 281, 312, 291, 458, 6352, 295, 295, 257, 51428], "temperature": 0.0, "avg_logprob": -0.07597100156024822, "compression_ratio": 1.7061068702290076, "no_speech_prob": 0.06369100511074066}, {"id": 798, "seek": 453464, "start": 4555.92, "end": 4559.360000000001, "text": " bezier curves or something like that like they have to have a certain smoothness", "tokens": [51428, 10782, 811, 19490, 420, 746, 411, 300, 411, 436, 362, 281, 362, 257, 1629, 5508, 1287, 51600], "temperature": 0.0, "avg_logprob": -0.07597100156024822, "compression_ratio": 1.7061068702290076, "no_speech_prob": 0.06369100511074066}, {"id": 799, "seek": 455936, "start": 4560.0, "end": 4564.88, "text": " property and you could actually put that as a constraint in the objective function has anybody", "tokens": [50396, 4707, 293, 291, 727, 767, 829, 300, 382, 257, 25534, 294, 264, 10024, 2445, 575, 4472, 50640], "temperature": 0.0, "avg_logprob": -0.13581045050370066, "compression_ratio": 1.7644230769230769, "no_speech_prob": 0.1096419170498848}, {"id": 800, "seek": 455936, "start": 4564.88, "end": 4570.16, "text": " tried anything like that yeah there are approaches so I saw one paper they added some", "tokens": [50640, 3031, 1340, 411, 300, 1338, 456, 366, 11587, 370, 286, 1866, 472, 3035, 436, 3869, 512, 50904], "temperature": 0.0, "avg_logprob": -0.13581045050370066, "compression_ratio": 1.7644230769230769, "no_speech_prob": 0.1096419170498848}, {"id": 801, "seek": 455936, "start": 4572.08, "end": 4578.0, "text": " some parts to their objective function so that when you create line explanations with line that", "tokens": [51000, 512, 3166, 281, 641, 10024, 2445, 370, 300, 562, 291, 1884, 1622, 28708, 365, 1622, 300, 51296], "temperature": 0.0, "avg_logprob": -0.13581045050370066, "compression_ratio": 1.7644230769230769, "no_speech_prob": 0.1096419170498848}, {"id": 802, "seek": 455936, "start": 4578.0, "end": 4585.2, "text": " they were most more stable and there are a lot of things like for neural networks you have", "tokens": [51296, 436, 645, 881, 544, 8351, 293, 456, 366, 257, 688, 295, 721, 411, 337, 18161, 9590, 291, 362, 51656], "temperature": 0.0, "avg_logprob": -0.13581045050370066, "compression_ratio": 1.7644230769230769, "no_speech_prob": 0.1096419170498848}, {"id": 803, "seek": 458520, "start": 4585.2, "end": 4593.28, "text": " disentanglement that you try that the feature maps or that the nodes learn disentangled concepts", "tokens": [50364, 37313, 656, 3054, 300, 291, 853, 300, 264, 4111, 11317, 420, 300, 264, 13891, 1466, 37313, 39101, 10392, 50768], "temperature": 0.0, "avg_logprob": -0.15020444499912547, "compression_ratio": 1.8012422360248448, "no_speech_prob": 0.007119580637663603}, {"id": 804, "seek": 458520, "start": 4594.639999999999, "end": 4600.32, "text": " there are ways to introduce like monotonicity so that a feature can always go into the effect of", "tokens": [50836, 456, 366, 2098, 281, 5366, 411, 1108, 310, 11630, 507, 370, 300, 257, 4111, 393, 1009, 352, 666, 264, 1802, 295, 51120], "temperature": 0.0, "avg_logprob": -0.15020444499912547, "compression_ratio": 1.8012422360248448, "no_speech_prob": 0.007119580637663603}, {"id": 805, "seek": 458520, "start": 4600.32, "end": 4607.599999999999, "text": " a feature can always be in one direction not to like zigzag around so there are approaches to do", "tokens": [51120, 257, 4111, 393, 1009, 312, 294, 472, 3513, 406, 281, 411, 38290, 43886, 926, 370, 456, 366, 11587, 281, 360, 51484], "temperature": 0.0, "avg_logprob": -0.15020444499912547, "compression_ratio": 1.8012422360248448, "no_speech_prob": 0.007119580637663603}, {"id": 806, "seek": 460760, "start": 4607.6, "end": 4617.200000000001, "text": " this to like have okay like interpretability constraints in your modeling here yeah because", "tokens": [50364, 341, 281, 411, 362, 1392, 411, 7302, 2310, 18491, 294, 428, 15983, 510, 1338, 570, 50844], "temperature": 0.0, "avg_logprob": -0.08755011652030197, "compression_ratio": 1.77007299270073, "no_speech_prob": 0.02128274366259575}, {"id": 807, "seek": 460760, "start": 4617.200000000001, "end": 4620.96, "text": " I was just thinking this can go back to Connor you know Connor was saying earlier on why don't we", "tokens": [50844, 286, 390, 445, 1953, 341, 393, 352, 646, 281, 33133, 291, 458, 33133, 390, 1566, 3071, 322, 983, 500, 380, 321, 51032], "temperature": 0.0, "avg_logprob": -0.08755011652030197, "compression_ratio": 1.77007299270073, "no_speech_prob": 0.02128274366259575}, {"id": 808, "seek": 460760, "start": 4620.96, "end": 4627.280000000001, "text": " just create white box models maybe we can use if the definition to a human being of white box is", "tokens": [51032, 445, 1884, 2418, 2424, 5245, 1310, 321, 393, 764, 498, 264, 7123, 281, 257, 1952, 885, 295, 2418, 2424, 307, 51348], "temperature": 0.0, "avg_logprob": -0.08755011652030197, "compression_ratio": 1.77007299270073, "no_speech_prob": 0.02128274366259575}, {"id": 809, "seek": 460760, "start": 4627.280000000001, "end": 4632.0, "text": " that it's interpretable and understandable if we can build into the objective functions when we're", "tokens": [51348, 300, 309, 311, 7302, 712, 293, 25648, 498, 321, 393, 1322, 666, 264, 10024, 6828, 562, 321, 434, 51584], "temperature": 0.0, "avg_logprob": -0.08755011652030197, "compression_ratio": 1.77007299270073, "no_speech_prob": 0.02128274366259575}, {"id": 810, "seek": 460760, "start": 4632.0, "end": 4637.200000000001, "text": " actually training the network that it has these properties then we'll actually be helping to create", "tokens": [51584, 767, 3097, 264, 3209, 300, 309, 575, 613, 7221, 550, 321, 603, 767, 312, 4315, 281, 1884, 51844], "temperature": 0.0, "avg_logprob": -0.08755011652030197, "compression_ratio": 1.77007299270073, "no_speech_prob": 0.02128274366259575}, {"id": 811, "seek": 463760, "start": 4637.6, "end": 4643.4400000000005, "text": " more white box you know models even if they are complex that's definitely an option but I think the", "tokens": [50364, 544, 2418, 2424, 291, 458, 5245, 754, 498, 436, 366, 3997, 300, 311, 2138, 364, 3614, 457, 286, 519, 264, 50656], "temperature": 0.0, "avg_logprob": -0.09828985255697499, "compression_ratio": 1.7668161434977578, "no_speech_prob": 0.0032215442042797804}, {"id": 812, "seek": 463760, "start": 4644.4800000000005, "end": 4649.200000000001, "text": " issue remains the same that you with it's similar to a white box model I mean you'll make some trade", "tokens": [50708, 2734, 7023, 264, 912, 300, 291, 365, 309, 311, 2531, 281, 257, 2418, 2424, 2316, 286, 914, 291, 603, 652, 512, 4923, 50944], "temperature": 0.0, "avg_logprob": -0.09828985255697499, "compression_ratio": 1.7668161434977578, "no_speech_prob": 0.0032215442042797804}, {"id": 813, "seek": 463760, "start": 4649.200000000001, "end": 4655.84, "text": " offs in the end you have to make the judgment whether so when you put more constraints I mean", "tokens": [50944, 39457, 294, 264, 917, 291, 362, 281, 652, 264, 12216, 1968, 370, 562, 291, 829, 544, 18491, 286, 914, 51276], "temperature": 0.0, "avg_logprob": -0.09828985255697499, "compression_ratio": 1.7668161434977578, "no_speech_prob": 0.0032215442042797804}, {"id": 814, "seek": 463760, "start": 4655.84, "end": 4660.96, "text": " you can actually also help the model of course that if you I mean if you have some inductive biases", "tokens": [51276, 291, 393, 767, 611, 854, 264, 2316, 295, 1164, 300, 498, 291, 286, 914, 498, 291, 362, 512, 31612, 488, 32152, 51532], "temperature": 0.0, "avg_logprob": -0.09828985255697499, "compression_ratio": 1.7668161434977578, "no_speech_prob": 0.0032215442042797804}, {"id": 815, "seek": 466096, "start": 4660.96, "end": 4667.76, "text": " also which which you infuse into the model which help with predicting or be more more stable", "tokens": [50364, 611, 597, 597, 291, 1536, 438, 666, 264, 2316, 597, 854, 365, 32884, 420, 312, 544, 544, 8351, 50704], "temperature": 0.0, "avg_logprob": -0.11575511564691383, "compression_ratio": 1.8529411764705883, "no_speech_prob": 0.012429407797753811}, {"id": 816, "seek": 466096, "start": 4669.28, "end": 4674.96, "text": " but sometimes you might maybe also trade off with accuracy and you just have to like in the end you", "tokens": [50780, 457, 2171, 291, 1062, 1310, 611, 4923, 766, 365, 14170, 293, 291, 445, 362, 281, 411, 294, 264, 917, 291, 51064], "temperature": 0.0, "avg_logprob": -0.11575511564691383, "compression_ratio": 1.8529411764705883, "no_speech_prob": 0.012429407797753811}, {"id": 817, "seek": 466096, "start": 4674.96, "end": 4683.12, "text": " have this yeah this set of models where some are more accurate some better than this one", "tokens": [51064, 362, 341, 1338, 341, 992, 295, 5245, 689, 512, 366, 544, 8559, 512, 1101, 813, 341, 472, 51472], "temperature": 0.0, "avg_logprob": -0.11575511564691383, "compression_ratio": 1.8529411764705883, "no_speech_prob": 0.012429407797753811}, {"id": 818, "seek": 466096, "start": 4683.12, "end": 4689.44, "text": " interpretability dimension the other is cheaper to deploy and then you have this so this kind of", "tokens": [51472, 7302, 2310, 10139, 264, 661, 307, 12284, 281, 7274, 293, 550, 291, 362, 341, 370, 341, 733, 295, 51788], "temperature": 0.0, "avg_logprob": -0.11575511564691383, "compression_ratio": 1.8529411764705883, "no_speech_prob": 0.012429407797753811}, {"id": 819, "seek": 468944, "start": 4689.44, "end": 4694.16, "text": " going into direction of like automatic machine learning and you don't get like just the best", "tokens": [50364, 516, 666, 3513, 295, 411, 12509, 3479, 2539, 293, 291, 500, 380, 483, 411, 445, 264, 1151, 50600], "temperature": 0.0, "avg_logprob": -0.14950538775242797, "compression_ratio": 1.9789915966386555, "no_speech_prob": 0.01201651245355606}, {"id": 820, "seek": 468944, "start": 4694.799999999999, "end": 4700.0, "text": " performing one but you have this parater set like well so we have multiple objectives that you want", "tokens": [50632, 10205, 472, 457, 291, 362, 341, 971, 771, 992, 411, 731, 370, 321, 362, 3866, 15961, 300, 291, 528, 50892], "temperature": 0.0, "avg_logprob": -0.14950538775242797, "compression_ratio": 1.9789915966386555, "no_speech_prob": 0.01201651245355606}, {"id": 821, "seek": 468944, "start": 4700.0, "end": 4704.879999999999, "text": " to hit and then there's not one model that works best but you have a set of models that", "tokens": [50892, 281, 2045, 293, 550, 456, 311, 406, 472, 2316, 300, 1985, 1151, 457, 291, 362, 257, 992, 295, 5245, 300, 51136], "temperature": 0.0, "avg_logprob": -0.14950538775242797, "compression_ratio": 1.9789915966386555, "no_speech_prob": 0.01201651245355606}, {"id": 822, "seek": 468944, "start": 4704.879999999999, "end": 4710.16, "text": " that have different trade-offs between these objectives and then you have to decide what", "tokens": [51136, 300, 362, 819, 4923, 12, 19231, 1296, 613, 15961, 293, 550, 291, 362, 281, 4536, 437, 51400], "temperature": 0.0, "avg_logprob": -0.14950538775242797, "compression_ratio": 1.9789915966386555, "no_speech_prob": 0.01201651245355606}, {"id": 823, "seek": 468944, "start": 4710.16, "end": 4716.96, "text": " is the trade-off that you want to do that you want to have I guess there's no getting away from it is", "tokens": [51400, 307, 264, 4923, 12, 4506, 300, 291, 528, 281, 360, 300, 291, 528, 281, 362, 286, 2041, 456, 311, 572, 1242, 1314, 490, 309, 307, 51740], "temperature": 0.0, "avg_logprob": -0.14950538775242797, "compression_ratio": 1.9789915966386555, "no_speech_prob": 0.01201651245355606}, {"id": 824, "seek": 471696, "start": 4717.04, "end": 4721.68, "text": " the interpretability it is going to get more important and more important I think you mentioned", "tokens": [50368, 264, 7302, 2310, 309, 307, 516, 281, 483, 544, 1021, 293, 544, 1021, 286, 519, 291, 2835, 50600], "temperature": 0.0, "avg_logprob": -0.11620045552211525, "compression_ratio": 1.866412213740458, "no_speech_prob": 0.05099442973732948}, {"id": 825, "seek": 471696, "start": 4721.68, "end": 4726.24, "text": " Christoph that you know we've had linear models for hundreds of years and then there's been this", "tokens": [50600, 2040, 5317, 300, 291, 458, 321, 600, 632, 8213, 5245, 337, 6779, 295, 924, 293, 550, 456, 311, 668, 341, 50828], "temperature": 0.0, "avg_logprob": -0.11620045552211525, "compression_ratio": 1.866412213740458, "no_speech_prob": 0.05099442973732948}, {"id": 826, "seek": 471696, "start": 4726.24, "end": 4732.8, "text": " big explosion and deep learning and then would you say about 2016 to 2018 that's when interpretability", "tokens": [50828, 955, 15673, 293, 2452, 2539, 293, 550, 576, 291, 584, 466, 6549, 281, 6096, 300, 311, 562, 7302, 2310, 51156], "temperature": 0.0, "avg_logprob": -0.11620045552211525, "compression_ratio": 1.866412213740458, "no_speech_prob": 0.05099442973732948}, {"id": 827, "seek": 471696, "start": 4732.8, "end": 4736.72, "text": " is really kicked off I what do you think do you think where's it going are we just going to get", "tokens": [51156, 307, 534, 14609, 766, 286, 437, 360, 291, 519, 360, 291, 519, 689, 311, 309, 516, 366, 321, 445, 516, 281, 483, 51352], "temperature": 0.0, "avg_logprob": -0.11620045552211525, "compression_ratio": 1.866412213740458, "no_speech_prob": 0.05099442973732948}, {"id": 828, "seek": 471696, "start": 4736.72, "end": 4742.96, "text": " more and more attention paid to this area I don't know if you can get more than than this I don't", "tokens": [51352, 544, 293, 544, 3202, 4835, 281, 341, 1859, 286, 500, 380, 458, 498, 291, 393, 483, 544, 813, 813, 341, 286, 500, 380, 51664], "temperature": 0.0, "avg_logprob": -0.11620045552211525, "compression_ratio": 1.866412213740458, "no_speech_prob": 0.05099442973732948}, {"id": 829, "seek": 474296, "start": 4742.96, "end": 4750.56, "text": " know yeah but I think it's at least here to stay and I think it's important I mean it has been", "tokens": [50364, 458, 1338, 457, 286, 519, 309, 311, 412, 1935, 510, 281, 1754, 293, 286, 519, 309, 311, 1021, 286, 914, 309, 575, 668, 50744], "temperature": 0.0, "avg_logprob": -0.0873125668229728, "compression_ratio": 1.7914691943127963, "no_speech_prob": 0.06265456229448318}, {"id": 830, "seek": 474296, "start": 4750.56, "end": 4758.96, "text": " important before but of course with like the push from deep learning especially and that it just", "tokens": [50744, 1021, 949, 457, 295, 1164, 365, 411, 264, 2944, 490, 2452, 2539, 2318, 293, 300, 309, 445, 51164], "temperature": 0.0, "avg_logprob": -0.0873125668229728, "compression_ratio": 1.7914691943127963, "no_speech_prob": 0.06265456229448318}, {"id": 831, "seek": 474296, "start": 4758.96, "end": 4763.6, "text": " became more clear to a lot of people that we need interpretability in some sense at least", "tokens": [51164, 3062, 544, 1850, 281, 257, 688, 295, 561, 300, 321, 643, 7302, 2310, 294, 512, 2020, 412, 1935, 51396], "temperature": 0.0, "avg_logprob": -0.0873125668229728, "compression_ratio": 1.7914691943127963, "no_speech_prob": 0.06265456229448318}, {"id": 832, "seek": 474296, "start": 4764.96, "end": 4771.12, "text": " yeah of course people have attempted it before and worked on it before it's just more urgent now", "tokens": [51464, 1338, 295, 1164, 561, 362, 18997, 309, 949, 293, 2732, 322, 309, 949, 309, 311, 445, 544, 19022, 586, 51772], "temperature": 0.0, "avg_logprob": -0.0873125668229728, "compression_ratio": 1.7914691943127963, "no_speech_prob": 0.06265456229448318}, {"id": 833, "seek": 477112, "start": 4771.36, "end": 4777.76, "text": " I really like the bit in your book talking about what's changed recently how interpretability is", "tokens": [50376, 286, 534, 411, 264, 857, 294, 428, 1446, 1417, 466, 437, 311, 3105, 3938, 577, 7302, 2310, 307, 50696], "temperature": 0.0, "avg_logprob": -0.12315790002996271, "compression_ratio": 1.7472924187725631, "no_speech_prob": 0.0060823336243629456}, {"id": 834, "seek": 477112, "start": 4777.76, "end": 4781.92, "text": " coming together as a field you know with this a unification so you know in physics we love a", "tokens": [50696, 1348, 1214, 382, 257, 2519, 291, 458, 365, 341, 257, 517, 3774, 370, 291, 458, 294, 10649, 321, 959, 257, 50904], "temperature": 0.0, "avg_logprob": -0.12315790002996271, "compression_ratio": 1.7472924187725631, "no_speech_prob": 0.0060823336243629456}, {"id": 835, "seek": 477112, "start": 4781.92, "end": 4785.92, "text": " big unification when you take all these different things in the past and say oh they're all just", "tokens": [50904, 955, 517, 3774, 562, 291, 747, 439, 613, 819, 721, 294, 264, 1791, 293, 584, 1954, 436, 434, 439, 445, 51104], "temperature": 0.0, "avg_logprob": -0.12315790002996271, "compression_ratio": 1.7472924187725631, "no_speech_prob": 0.0060823336243629456}, {"id": 836, "seek": 477112, "start": 4785.92, "end": 4791.12, "text": " part of this one big framework and it was chap that chap paper was amazing wasn't it saying things", "tokens": [51104, 644, 295, 341, 472, 955, 8388, 293, 309, 390, 13223, 300, 13223, 3035, 390, 2243, 2067, 380, 309, 1566, 721, 51364], "temperature": 0.0, "avg_logprob": -0.12315790002996271, "compression_ratio": 1.7472924187725631, "no_speech_prob": 0.0060823336243629456}, {"id": 837, "seek": 477112, "start": 4791.12, "end": 4797.5199999999995, "text": " like lime deep lift layer-wise propagation shapely ah forget all them they're all special cases of", "tokens": [51364, 411, 22035, 2452, 5533, 4583, 12, 3711, 38377, 6706, 736, 3716, 2870, 439, 552, 436, 434, 439, 2121, 3331, 295, 51684], "temperature": 0.0, "avg_logprob": -0.12315790002996271, "compression_ratio": 1.7472924187725631, "no_speech_prob": 0.0060823336243629456}, {"id": 838, "seek": 479752, "start": 4797.52, "end": 4802.4800000000005, "text": " these additive feature attribution methods and we can prove that this is the only one that's", "tokens": [50364, 613, 45558, 4111, 9080, 1448, 7150, 293, 321, 393, 7081, 300, 341, 307, 264, 787, 472, 300, 311, 50612], "temperature": 0.0, "avg_logprob": -0.10357225799560547, "compression_ratio": 1.8217821782178218, "no_speech_prob": 0.03560265526175499}, {"id": 839, "seek": 479752, "start": 4802.4800000000005, "end": 4807.4400000000005, "text": " theoretically valid because it has these properties of symmetry it's got the stummy property so", "tokens": [50612, 29400, 7363, 570, 309, 575, 613, 7221, 295, 25440, 309, 311, 658, 264, 342, 8620, 4707, 370, 50860], "temperature": 0.0, "avg_logprob": -0.10357225799560547, "compression_ratio": 1.8217821782178218, "no_speech_prob": 0.03560265526175499}, {"id": 840, "seek": 479752, "start": 4808.160000000001, "end": 4811.040000000001, "text": " you know everything that's been done before in interpretability well they're all in our", "tokens": [50896, 291, 458, 1203, 300, 311, 668, 1096, 949, 294, 7302, 2310, 731, 436, 434, 439, 294, 527, 51040], "temperature": 0.0, "avg_logprob": -0.10357225799560547, "compression_ratio": 1.8217821782178218, "no_speech_prob": 0.03560265526175499}, {"id": 841, "seek": 479752, "start": 4811.040000000001, "end": 4816.080000000001, "text": " framework now and shapely values are the way forward yeah they're quite uh quite famous to", "tokens": [51040, 8388, 586, 293, 6706, 736, 4190, 366, 264, 636, 2128, 1338, 436, 434, 1596, 2232, 1596, 4618, 281, 51292], "temperature": 0.0, "avg_logprob": -0.10357225799560547, "compression_ratio": 1.8217821782178218, "no_speech_prob": 0.03560265526175499}, {"id": 842, "seek": 479752, "start": 4816.080000000001, "end": 4821.120000000001, "text": " shapely values yeah would you believe them then I mean I guess in their paper they're kind of", "tokens": [51292, 6706, 736, 4190, 1338, 576, 291, 1697, 552, 550, 286, 914, 286, 2041, 294, 641, 3035, 436, 434, 733, 295, 51544], "temperature": 0.0, "avg_logprob": -0.10357225799560547, "compression_ratio": 1.8217821782178218, "no_speech_prob": 0.03560265526175499}, {"id": 843, "seek": 479752, "start": 4821.120000000001, "end": 4824.96, "text": " they kind of disagree with lime a bit don't they they say well lime is a count of this but", "tokens": [51544, 436, 733, 295, 14091, 365, 22035, 257, 857, 500, 380, 436, 436, 584, 731, 22035, 307, 257, 1207, 295, 341, 457, 51736], "temperature": 0.0, "avg_logprob": -0.10357225799560547, "compression_ratio": 1.8217821782178218, "no_speech_prob": 0.03560265526175499}, {"id": 844, "seek": 482496, "start": 4825.04, "end": 4829.76, "text": " they're going to be breaking our properties of efficiency and symmetry so lime is using the", "tokens": [50368, 436, 434, 516, 281, 312, 7697, 527, 7221, 295, 10493, 293, 25440, 370, 22035, 307, 1228, 264, 50604], "temperature": 0.0, "avg_logprob": -0.1145631472269694, "compression_ratio": 1.7741935483870968, "no_speech_prob": 0.0007319473079405725}, {"id": 845, "seek": 482496, "start": 4829.76, "end": 4834.24, "text": " wrong weights right they should be using this yeah kernel shape weights rather than the line weights", "tokens": [50604, 2085, 17443, 558, 436, 820, 312, 1228, 341, 1338, 28256, 3909, 17443, 2831, 813, 264, 1622, 17443, 50828], "temperature": 0.0, "avg_logprob": -0.1145631472269694, "compression_ratio": 1.7741935483870968, "no_speech_prob": 0.0007319473079405725}, {"id": 846, "seek": 482496, "start": 4834.24, "end": 4840.32, "text": " I think that's just a different approach also to think about it I mean you don't maybe you don't", "tokens": [50828, 286, 519, 300, 311, 445, 257, 819, 3109, 611, 281, 519, 466, 309, 286, 914, 291, 500, 380, 1310, 291, 500, 380, 51132], "temperature": 0.0, "avg_logprob": -0.1145631472269694, "compression_ratio": 1.7741935483870968, "no_speech_prob": 0.0007319473079405725}, {"id": 847, "seek": 482496, "start": 4840.32, "end": 4848.32, "text": " I think I think the properties are quite attractive or meaningful at least um but also also the", "tokens": [51132, 286, 519, 286, 519, 264, 7221, 366, 1596, 12609, 420, 10995, 412, 1935, 1105, 457, 611, 611, 264, 51532], "temperature": 0.0, "avg_logprob": -0.1145631472269694, "compression_ratio": 1.7741935483870968, "no_speech_prob": 0.0007319473079405725}, {"id": 848, "seek": 484832, "start": 4848.48, "end": 4855.84, "text": " line approach I'm very critical about lime because it um I think it's difficult to have the correct", "tokens": [50372, 1622, 3109, 286, 478, 588, 4924, 466, 22035, 570, 309, 1105, 286, 519, 309, 311, 2252, 281, 362, 264, 3006, 50740], "temperature": 0.0, "avg_logprob": -0.15736249859413404, "compression_ratio": 1.6812227074235808, "no_speech_prob": 0.024037335067987442}, {"id": 849, "seek": 484832, "start": 4856.5599999999995, "end": 4865.599999999999, "text": " to know like how to parameterize your your local models um so I think I'm a bit more of a fan", "tokens": [50776, 281, 458, 411, 577, 281, 13075, 1125, 428, 428, 2654, 5245, 1105, 370, 286, 519, 286, 478, 257, 857, 544, 295, 257, 3429, 51228], "temperature": 0.0, "avg_logprob": -0.15736249859413404, "compression_ratio": 1.6812227074235808, "no_speech_prob": 0.024037335067987442}, {"id": 850, "seek": 484832, "start": 4865.599999999999, "end": 4871.44, "text": " of shapely values because of the theoretical properties it comes with are you talking about", "tokens": [51228, 295, 6706, 736, 4190, 570, 295, 264, 20864, 7221, 309, 1487, 365, 366, 291, 1417, 466, 51520], "temperature": 0.0, "avg_logprob": -0.15736249859413404, "compression_ratio": 1.6812227074235808, "no_speech_prob": 0.024037335067987442}, {"id": 851, "seek": 484832, "start": 4871.44, "end": 4875.84, "text": " that distance measure in lime where you have to be able to quantify how far away is the permutation", "tokens": [51520, 300, 4560, 3481, 294, 22035, 689, 291, 362, 281, 312, 1075, 281, 40421, 577, 1400, 1314, 307, 264, 4784, 11380, 51740], "temperature": 0.0, "avg_logprob": -0.15736249859413404, "compression_ratio": 1.6812227074235808, "no_speech_prob": 0.024037335067987442}, {"id": 852, "seek": 487584, "start": 4875.84, "end": 4883.6, "text": " yeah the like the kernel width yeah which is set to 0.75 I think so I just looked it up and", "tokens": [50364, 1338, 264, 411, 264, 28256, 11402, 1338, 597, 307, 992, 281, 1958, 13, 11901, 286, 519, 370, 286, 445, 2956, 309, 493, 293, 50752], "temperature": 0.0, "avg_logprob": -0.10854937966945952, "compression_ratio": 1.7782101167315174, "no_speech_prob": 0.005383854266256094}, {"id": 853, "seek": 487584, "start": 4883.6, "end": 4888.96, "text": " I mean it's it's a very difficult question it goes to the heart of like what's local um", "tokens": [50752, 286, 914, 309, 311, 309, 311, 257, 588, 2252, 1168, 309, 1709, 281, 264, 1917, 295, 411, 437, 311, 2654, 1105, 51020], "temperature": 0.0, "avg_logprob": -0.10854937966945952, "compression_ratio": 1.7782101167315174, "no_speech_prob": 0.005383854266256094}, {"id": 854, "seek": 487584, "start": 4889.84, "end": 4893.84, "text": " because like I mean you have this this kernel that decides like how much you weight", "tokens": [51064, 570, 411, 286, 914, 291, 362, 341, 341, 28256, 300, 14898, 411, 577, 709, 291, 3364, 51264], "temperature": 0.0, "avg_logprob": -0.10854937966945952, "compression_ratio": 1.7782101167315174, "no_speech_prob": 0.005383854266256094}, {"id": 855, "seek": 487584, "start": 4893.84, "end": 4899.92, "text": " all the data points around the point you want to explain and and like how how big is this area", "tokens": [51264, 439, 264, 1412, 2793, 926, 264, 935, 291, 528, 281, 2903, 293, 293, 411, 577, 577, 955, 307, 341, 1859, 51568], "temperature": 0.0, "avg_logprob": -0.10854937966945952, "compression_ratio": 1.7782101167315174, "no_speech_prob": 0.005383854266256094}, {"id": 856, "seek": 487584, "start": 4899.92, "end": 4905.2, "text": " I think this is very dependent on your model and your data and there's no answer no easy answer to", "tokens": [51568, 286, 519, 341, 307, 588, 12334, 322, 428, 2316, 293, 428, 1412, 293, 456, 311, 572, 1867, 572, 1858, 1867, 281, 51832], "temperature": 0.0, "avg_logprob": -0.10854937966945952, "compression_ratio": 1.7782101167315174, "no_speech_prob": 0.005383854266256094}, {"id": 857, "seek": 490520, "start": 4905.2, "end": 4911.2, "text": " how to set it yeah or even more generally than that this this whole notion of what does it mean", "tokens": [50364, 577, 281, 992, 309, 1338, 420, 754, 544, 5101, 813, 300, 341, 341, 1379, 10710, 295, 437, 775, 309, 914, 50664], "temperature": 0.0, "avg_logprob": -0.07218321391514369, "compression_ratio": 1.8212927756653992, "no_speech_prob": 0.0021864932496100664}, {"id": 858, "seek": 490520, "start": 4911.2, "end": 4916.72, "text": " to have a local interpretation method in you know in text or vision so in in vision there's this", "tokens": [50664, 281, 362, 257, 2654, 14174, 3170, 294, 291, 458, 294, 2487, 420, 5201, 370, 294, 294, 5201, 456, 311, 341, 50940], "temperature": 0.0, "avg_logprob": -0.07218321391514369, "compression_ratio": 1.8212927756653992, "no_speech_prob": 0.0021864932496100664}, {"id": 859, "seek": 490520, "start": 4916.72, "end": 4921.12, "text": " super pixel concept which is something that seems to make intuitive sense but but does it you know", "tokens": [50940, 1687, 19261, 3410, 597, 307, 746, 300, 2544, 281, 652, 21769, 2020, 457, 457, 775, 309, 291, 458, 51160], "temperature": 0.0, "avg_logprob": -0.07218321391514369, "compression_ratio": 1.8212927756653992, "no_speech_prob": 0.0021864932496100664}, {"id": 860, "seek": 490520, "start": 4921.12, "end": 4926.8, "text": " when you create all of these different uh maskings of different parts of the input space but um", "tokens": [51160, 562, 291, 1884, 439, 295, 613, 819, 2232, 6094, 1109, 295, 819, 3166, 295, 264, 4846, 1901, 457, 1105, 51444], "temperature": 0.0, "avg_logprob": -0.07218321391514369, "compression_ratio": 1.8212927756653992, "no_speech_prob": 0.0021864932496100664}, {"id": 861, "seek": 490520, "start": 4926.8, "end": 4932.0, "text": " with shapely values as well that they they are a beautiful a beautiful technique especially", "tokens": [51444, 365, 6706, 736, 4190, 382, 731, 300, 436, 436, 366, 257, 2238, 257, 2238, 6532, 2318, 51704], "temperature": 0.0, "avg_logprob": -0.07218321391514369, "compression_ratio": 1.8212927756653992, "no_speech_prob": 0.0021864932496100664}, {"id": 862, "seek": 493200, "start": 4932.08, "end": 4938.0, "text": " because the the values are are quite meaningful but if you have shared information between the", "tokens": [50368, 570, 264, 264, 4190, 366, 366, 1596, 10995, 457, 498, 291, 362, 5507, 1589, 1296, 264, 50664], "temperature": 0.0, "avg_logprob": -0.08140891569632071, "compression_ratio": 1.7240143369175627, "no_speech_prob": 0.022870905697345734}, {"id": 863, "seek": 493200, "start": 4938.0, "end": 4942.24, "text": " features I mean Connor and I were talking about this for example you if you had the same model", "tokens": [50664, 4122, 286, 914, 33133, 293, 286, 645, 1417, 466, 341, 337, 1365, 291, 498, 291, 632, 264, 912, 2316, 50876], "temperature": 0.0, "avg_logprob": -0.08140891569632071, "compression_ratio": 1.7240143369175627, "no_speech_prob": 0.022870905697345734}, {"id": 864, "seek": 493200, "start": 4942.24, "end": 4948.08, "text": " where you were predicting someone's income and you put their I don't know let's say you had salary", "tokens": [50876, 689, 291, 645, 32884, 1580, 311, 5742, 293, 291, 829, 641, 286, 500, 380, 458, 718, 311, 584, 291, 632, 15360, 51168], "temperature": 0.0, "avg_logprob": -0.08140891569632071, "compression_ratio": 1.7240143369175627, "no_speech_prob": 0.022870905697345734}, {"id": 865, "seek": 493200, "start": 4948.08, "end": 4954.96, "text": " in the model twice then the shapely value would be divided between the two duplicate fields right", "tokens": [51168, 294, 264, 2316, 6091, 550, 264, 6706, 736, 2158, 576, 312, 6666, 1296, 264, 732, 23976, 7909, 558, 51512], "temperature": 0.0, "avg_logprob": -0.08140891569632071, "compression_ratio": 1.7240143369175627, "no_speech_prob": 0.022870905697345734}, {"id": 866, "seek": 493200, "start": 4954.96, "end": 4961.52, "text": " so there just seems to be so much esoterica in these IML methods right are we expected to know", "tokens": [51512, 370, 456, 445, 2544, 281, 312, 370, 709, 785, 21585, 2262, 294, 613, 286, 12683, 7150, 558, 366, 321, 5176, 281, 458, 51840], "temperature": 0.0, "avg_logprob": -0.08140891569632071, "compression_ratio": 1.7240143369175627, "no_speech_prob": 0.022870905697345734}, {"id": 867, "seek": 496152, "start": 4961.52, "end": 4966.96, "text": " all of this stuff yeah I I think I mean that's why I wrote the book um to to capture these things", "tokens": [50364, 439, 295, 341, 1507, 1338, 286, 286, 519, 286, 914, 300, 311, 983, 286, 4114, 264, 1446, 1105, 281, 281, 7983, 613, 721, 50636], "temperature": 0.0, "avg_logprob": -0.10173786097559435, "compression_ratio": 1.8615384615384616, "no_speech_prob": 0.003427323419600725}, {"id": 868, "seek": 496152, "start": 4966.96, "end": 4971.52, "text": " that you have to know all the these these uh disadvantages of the methods where I try to", "tokens": [50636, 300, 291, 362, 281, 458, 439, 264, 613, 613, 2232, 37431, 295, 264, 7150, 689, 286, 853, 281, 50864], "temperature": 0.0, "avg_logprob": -0.10173786097559435, "compression_ratio": 1.8615384615384616, "no_speech_prob": 0.003427323419600725}, {"id": 869, "seek": 496152, "start": 4971.52, "end": 4978.64, "text": " be very honest I mean because I'm not too invested in them but yeah I think that's worth all tools", "tokens": [50864, 312, 588, 3245, 286, 914, 570, 286, 478, 406, 886, 13104, 294, 552, 457, 1338, 286, 519, 300, 311, 3163, 439, 3873, 51220], "temperature": 0.0, "avg_logprob": -0.10173786097559435, "compression_ratio": 1.8615384615384616, "no_speech_prob": 0.003427323419600725}, {"id": 870, "seek": 496152, "start": 4978.64, "end": 4985.52, "text": " that we usually have um also with statistics and so on you you have to know like um these like as", "tokens": [51220, 300, 321, 2673, 362, 1105, 611, 365, 12523, 293, 370, 322, 291, 291, 362, 281, 458, 411, 1105, 613, 411, 382, 51564], "temperature": 0.0, "avg_logprob": -0.10173786097559435, "compression_ratio": 1.8615384615384616, "no_speech_prob": 0.003427323419600725}, {"id": 871, "seek": 496152, "start": 4985.52, "end": 4989.84, "text": " you mentioned this if you have salary twice then it will just I mean depends also on what your model", "tokens": [51564, 291, 2835, 341, 498, 291, 362, 15360, 6091, 550, 309, 486, 445, 286, 914, 5946, 611, 322, 437, 428, 2316, 51780], "temperature": 0.0, "avg_logprob": -0.10173786097559435, "compression_ratio": 1.8615384615384616, "no_speech_prob": 0.003427323419600725}, {"id": 872, "seek": 498984, "start": 4989.84, "end": 4996.16, "text": " does if it just picks one of the salary features or if it itself uses both so this also something", "tokens": [50364, 775, 498, 309, 445, 16137, 472, 295, 264, 15360, 4122, 420, 498, 309, 2564, 4960, 1293, 370, 341, 611, 746, 50680], "temperature": 0.0, "avg_logprob": -0.05842168754506334, "compression_ratio": 1.900398406374502, "no_speech_prob": 0.0039011717308312654}, {"id": 873, "seek": 498984, "start": 4996.16, "end": 5002.64, "text": " that will define how the shapely value will look like later on um but you have to know these things", "tokens": [50680, 300, 486, 6964, 577, 264, 6706, 736, 2158, 486, 574, 411, 1780, 322, 1105, 457, 291, 362, 281, 458, 613, 721, 51004], "temperature": 0.0, "avg_logprob": -0.05842168754506334, "compression_ratio": 1.900398406374502, "no_speech_prob": 0.0039011717308312654}, {"id": 874, "seek": 498984, "start": 5002.64, "end": 5007.52, "text": " if you want to use shapely values and interpret interpret them correctly yeah because I think", "tokens": [51004, 498, 291, 528, 281, 764, 6706, 736, 4190, 293, 7302, 7302, 552, 8944, 1338, 570, 286, 519, 51248], "temperature": 0.0, "avg_logprob": -0.05842168754506334, "compression_ratio": 1.900398406374502, "no_speech_prob": 0.0039011717308312654}, {"id": 875, "seek": 498984, "start": 5007.52, "end": 5012.8, "text": " philosophically we've got we've got the real behavior and then we use these interpretability", "tokens": [51248, 14529, 984, 321, 600, 658, 321, 600, 658, 264, 957, 5223, 293, 550, 321, 764, 613, 7302, 2310, 51512], "temperature": 0.0, "avg_logprob": -0.05842168754506334, "compression_ratio": 1.900398406374502, "no_speech_prob": 0.0039011717308312654}, {"id": 876, "seek": 498984, "start": 5012.8, "end": 5017.76, "text": " methods and then we've got the kind of perceived behavior so we've got these these levels of", "tokens": [51512, 7150, 293, 550, 321, 600, 658, 264, 733, 295, 19049, 5223, 370, 321, 600, 658, 613, 613, 4358, 295, 51760], "temperature": 0.0, "avg_logprob": -0.05842168754506334, "compression_ratio": 1.900398406374502, "no_speech_prob": 0.0039011717308312654}, {"id": 877, "seek": 501776, "start": 5018.400000000001, "end": 5024.72, "text": " modeling or or do you know what I mean simplification and it's all well and true if you are dealing", "tokens": [50396, 15983, 420, 420, 360, 291, 458, 437, 286, 914, 6883, 3774, 293, 309, 311, 439, 731, 293, 2074, 498, 291, 366, 6260, 50712], "temperature": 0.0, "avg_logprob": -0.07426597958519346, "compression_ratio": 1.7612612612612613, "no_speech_prob": 0.016303930431604385}, {"id": 878, "seek": 501776, "start": 5024.72, "end": 5031.52, "text": " with data scientists who understand how these you know methods work that's fine but invariably", "tokens": [50712, 365, 1412, 7708, 567, 1223, 577, 613, 291, 458, 7150, 589, 300, 311, 2489, 457, 33270, 1188, 51052], "temperature": 0.0, "avg_logprob": -0.07426597958519346, "compression_ratio": 1.7612612612612613, "no_speech_prob": 0.016303930431604385}, {"id": 879, "seek": 501776, "start": 5031.52, "end": 5037.76, "text": " data scientists need to present this information to lay people and they are not going to understand", "tokens": [51052, 1412, 7708, 643, 281, 1974, 341, 1589, 281, 2360, 561, 293, 436, 366, 406, 516, 281, 1223, 51364], "temperature": 0.0, "avg_logprob": -0.07426597958519346, "compression_ratio": 1.7612612612612613, "no_speech_prob": 0.016303930431604385}, {"id": 880, "seek": 501776, "start": 5038.4800000000005, "end": 5042.4800000000005, "text": " all of the various different trade-offs and how information is being compressed and and lost and", "tokens": [51400, 439, 295, 264, 3683, 819, 4923, 12, 19231, 293, 577, 1589, 307, 885, 30353, 293, 293, 2731, 293, 51600], "temperature": 0.0, "avg_logprob": -0.07426597958519346, "compression_ratio": 1.7612612612612613, "no_speech_prob": 0.016303930431604385}, {"id": 881, "seek": 504248, "start": 5042.48, "end": 5048.879999999999, "text": " so on so do you see that as a as a serious problem uh yes but it's not a new problem it's with any", "tokens": [50364, 370, 322, 370, 360, 291, 536, 300, 382, 257, 382, 257, 3156, 1154, 2232, 2086, 457, 309, 311, 406, 257, 777, 1154, 309, 311, 365, 604, 50684], "temperature": 0.0, "avg_logprob": -0.08298505007565676, "compression_ratio": 1.755656108597285, "no_speech_prob": 0.011482534930109978}, {"id": 882, "seek": 504248, "start": 5048.879999999999, "end": 5057.919999999999, "text": " number that you read in any newspaper uh I mean so in a sense I mean when when you look at uh", "tokens": [50684, 1230, 300, 291, 1401, 294, 604, 13669, 2232, 286, 914, 370, 294, 257, 2020, 286, 914, 562, 562, 291, 574, 412, 2232, 51136], "temperature": 0.0, "avg_logprob": -0.08298505007565676, "compression_ratio": 1.755656108597285, "no_speech_prob": 0.011482534930109978}, {"id": 883, "seek": 504248, "start": 5057.919999999999, "end": 5063.04, "text": " outcomes of statistical models that uh well everyone can understand well of course not because", "tokens": [51136, 10070, 295, 22820, 5245, 300, 2232, 731, 1518, 393, 1223, 731, 295, 1164, 406, 570, 51392], "temperature": 0.0, "avg_logprob": -0.08298505007565676, "compression_ratio": 1.755656108597285, "no_speech_prob": 0.011482534930109978}, {"id": 884, "seek": 504248, "start": 5063.04, "end": 5068.08, "text": " you need training to understand how to interpret a linear model or any regression model yeah there's", "tokens": [51392, 291, 643, 3097, 281, 1223, 577, 281, 7302, 257, 8213, 2316, 420, 604, 24590, 2316, 1338, 456, 311, 51644], "temperature": 0.0, "avg_logprob": -0.08298505007565676, "compression_ratio": 1.755656108597285, "no_speech_prob": 0.011482534930109978}, {"id": 885, "seek": 506808, "start": 5068.16, "end": 5074.08, "text": " difficulty but I don't think it's new in any sense um because there's always I mean any number that", "tokens": [50368, 10360, 457, 286, 500, 380, 519, 309, 311, 777, 294, 604, 2020, 1105, 570, 456, 311, 1009, 286, 914, 604, 1230, 300, 50664], "temperature": 0.0, "avg_logprob": -0.05454493823804354, "compression_ratio": 1.8327137546468402, "no_speech_prob": 0.05031665787100792}, {"id": 886, "seek": 506808, "start": 5074.08, "end": 5080.8, "text": " you read anywhere has a very complex process um so I don't know if you have like COVID testing", "tokens": [50664, 291, 1401, 4992, 575, 257, 588, 3997, 1399, 1105, 370, 286, 500, 380, 458, 498, 291, 362, 411, 4566, 4997, 51000], "temperature": 0.0, "avg_logprob": -0.05454493823804354, "compression_ratio": 1.8327137546468402, "no_speech_prob": 0.05031665787100792}, {"id": 887, "seek": 506808, "start": 5080.8, "end": 5086.64, "text": " numbers it's very complex like how the number was generated maybe like how it was aggregated over many", "tokens": [51000, 3547, 309, 311, 588, 3997, 411, 577, 264, 1230, 390, 10833, 1310, 411, 577, 309, 390, 16743, 770, 670, 867, 51292], "temperature": 0.0, "avg_logprob": -0.05454493823804354, "compression_ratio": 1.8327137546468402, "no_speech_prob": 0.05031665787100792}, {"id": 888, "seek": 506808, "start": 5086.64, "end": 5092.4, "text": " states and like what cases it includes and which it doesn't and so the number looks very innocent", "tokens": [51292, 4368, 293, 411, 437, 3331, 309, 5974, 293, 597, 309, 1177, 380, 293, 370, 264, 1230, 1542, 588, 13171, 51580], "temperature": 0.0, "avg_logprob": -0.05454493823804354, "compression_ratio": 1.8327137546468402, "no_speech_prob": 0.05031665787100792}, {"id": 889, "seek": 506808, "start": 5092.4, "end": 5098.0, "text": " and simple but there's a very long process behind it to produce it um maybe this process is a bit", "tokens": [51580, 293, 2199, 457, 456, 311, 257, 588, 938, 1399, 2261, 309, 281, 5258, 309, 1105, 1310, 341, 1399, 307, 257, 857, 51860], "temperature": 0.0, "avg_logprob": -0.05454493823804354, "compression_ratio": 1.8327137546468402, "no_speech_prob": 0.05031665787100792}, {"id": 890, "seek": 509800, "start": 5098.56, "end": 5104.24, "text": " uh a bit more black box or a bit more difficult if it comes out if there's some machine learning", "tokens": [50392, 2232, 257, 857, 544, 2211, 2424, 420, 257, 857, 544, 2252, 498, 309, 1487, 484, 498, 456, 311, 512, 3479, 2539, 50676], "temperature": 0.0, "avg_logprob": -0.11866863038804797, "compression_ratio": 1.8599033816425121, "no_speech_prob": 0.0012842181604355574}, {"id": 891, "seek": 509800, "start": 5104.24, "end": 5110.16, "text": " in between machine learning model in between to generate a number um but yeah I think this", "tokens": [50676, 294, 1296, 3479, 2539, 2316, 294, 1296, 281, 8460, 257, 1230, 1105, 457, 1338, 286, 519, 341, 50972], "temperature": 0.0, "avg_logprob": -0.11866863038804797, "compression_ratio": 1.8599033816425121, "no_speech_prob": 0.0012842181604355574}, {"id": 892, "seek": 509800, "start": 5111.04, "end": 5119.12, "text": " problem is well old well let me let me challenge a little bit here on something which is okay if", "tokens": [51016, 1154, 307, 731, 1331, 731, 718, 385, 718, 385, 3430, 257, 707, 857, 510, 322, 746, 597, 307, 1392, 498, 51420], "temperature": 0.0, "avg_logprob": -0.11866863038804797, "compression_ratio": 1.8599033816425121, "no_speech_prob": 0.0012842181604355574}, {"id": 893, "seek": 509800, "start": 5119.12, "end": 5124.96, "text": " I have if I have some general formula just some very general formula and then I go in there and I go", "tokens": [51420, 286, 362, 498, 286, 362, 512, 2674, 8513, 445, 512, 588, 2674, 8513, 293, 550, 286, 352, 294, 456, 293, 286, 352, 51712], "temperature": 0.0, "avg_logprob": -0.11866863038804797, "compression_ratio": 1.8599033816425121, "no_speech_prob": 0.0012842181604355574}, {"id": 894, "seek": 512496, "start": 5124.96, "end": 5131.2, "text": " you know what this formula has five parameters and if I make this one point seven five and that one", "tokens": [50364, 291, 458, 437, 341, 8513, 575, 1732, 9834, 293, 498, 286, 652, 341, 472, 935, 3407, 1732, 293, 300, 472, 50676], "temperature": 0.0, "avg_logprob": -0.06943296076177241, "compression_ratio": 1.768181818181818, "no_speech_prob": 0.0014101855922490358}, {"id": 895, "seek": 512496, "start": 5131.2, "end": 5139.84, "text": " one third and this one two and that one zero and I call this the megatron you know uh activation", "tokens": [50676, 472, 2636, 293, 341, 472, 732, 293, 300, 472, 4018, 293, 286, 818, 341, 264, 10816, 267, 2044, 291, 458, 2232, 24433, 51108], "temperature": 0.0, "avg_logprob": -0.06943296076177241, "compression_ratio": 1.768181818181818, "no_speech_prob": 0.0014101855922490358}, {"id": 896, "seek": 512496, "start": 5139.84, "end": 5145.76, "text": " potential and I go and write a paper about it that's really just an arbitrary you know kind of", "tokens": [51108, 3995, 293, 286, 352, 293, 2464, 257, 3035, 466, 309, 300, 311, 534, 445, 364, 23211, 291, 458, 733, 295, 51404], "temperature": 0.0, "avg_logprob": -0.06943296076177241, "compression_ratio": 1.768181818181818, "no_speech_prob": 0.0014101855922490358}, {"id": 897, "seek": 512496, "start": 5145.76, "end": 5151.12, "text": " selection of a bunch of numbers and then you gave it a fancy mathematical passport and you got it", "tokens": [51404, 9450, 295, 257, 3840, 295, 3547, 293, 550, 291, 2729, 309, 257, 10247, 18894, 24694, 293, 291, 658, 309, 51672], "temperature": 0.0, "avg_logprob": -0.06943296076177241, "compression_ratio": 1.768181818181818, "no_speech_prob": 0.0014101855922490358}, {"id": 898, "seek": 515112, "start": 5151.2, "end": 5157.12, "text": " published in some journal and now everybody has to memorize that as you know the megatron potential", "tokens": [50368, 6572, 294, 512, 6708, 293, 586, 2201, 575, 281, 27478, 300, 382, 291, 458, 264, 10816, 267, 2044, 3995, 50664], "temperature": 0.0, "avg_logprob": -0.04006395754606827, "compression_ratio": 1.862453531598513, "no_speech_prob": 0.026753313839435577}, {"id": 899, "seek": 515112, "start": 5157.12, "end": 5161.92, "text": " and kind of learn about it and that's a lot of what's going on right now is that it's really just", "tokens": [50664, 293, 733, 295, 1466, 466, 309, 293, 300, 311, 257, 688, 295, 437, 311, 516, 322, 558, 586, 307, 300, 309, 311, 534, 445, 50904], "temperature": 0.0, "avg_logprob": -0.04006395754606827, "compression_ratio": 1.862453531598513, "no_speech_prob": 0.026753313839435577}, {"id": 900, "seek": 515112, "start": 5161.92, "end": 5166.5599999999995, "text": " a bunch of hacking like it's people just they don't really know a general solution and they don't", "tokens": [50904, 257, 3840, 295, 31422, 411, 309, 311, 561, 445, 436, 500, 380, 534, 458, 257, 2674, 3827, 293, 436, 500, 380, 51136], "temperature": 0.0, "avg_logprob": -0.04006395754606827, "compression_ratio": 1.862453531598513, "no_speech_prob": 0.026753313839435577}, {"id": 901, "seek": 515112, "start": 5166.5599999999995, "end": 5171.5199999999995, "text": " know how to solve like in general the problem they're trying to solve and so they just hack around and", "tokens": [51136, 458, 577, 281, 5039, 411, 294, 2674, 264, 1154, 436, 434, 1382, 281, 5039, 293, 370, 436, 445, 10339, 926, 293, 51384], "temperature": 0.0, "avg_logprob": -0.04006395754606827, "compression_ratio": 1.862453531598513, "no_speech_prob": 0.026753313839435577}, {"id": 902, "seek": 515112, "start": 5171.5199999999995, "end": 5177.12, "text": " then the ones that are kind of famous or demonstrate some success in a particular combination you know", "tokens": [51384, 550, 264, 2306, 300, 366, 733, 295, 4618, 420, 11698, 512, 2245, 294, 257, 1729, 6562, 291, 458, 51664], "temperature": 0.0, "avg_logprob": -0.04006395754606827, "compression_ratio": 1.862453531598513, "no_speech_prob": 0.026753313839435577}, {"id": 903, "seek": 517712, "start": 5177.2, "end": 5181.76, "text": " competition over in this corner or something it now becomes something that's part of the lexicon", "tokens": [50368, 6211, 670, 294, 341, 4538, 420, 746, 309, 586, 3643, 746, 300, 311, 644, 295, 264, 476, 87, 11911, 50596], "temperature": 0.0, "avg_logprob": -0.05841352389408992, "compression_ratio": 1.6643109540636043, "no_speech_prob": 0.01267150603234768}, {"id": 904, "seek": 517712, "start": 5181.76, "end": 5186.4, "text": " that we all have to learn and I think like I look back on this like imagine what physics was like", "tokens": [50596, 300, 321, 439, 362, 281, 1466, 293, 286, 519, 411, 286, 574, 646, 322, 341, 411, 3811, 437, 10649, 390, 411, 50828], "temperature": 0.0, "avg_logprob": -0.05841352389408992, "compression_ratio": 1.6643109540636043, "no_speech_prob": 0.01267150603234768}, {"id": 905, "seek": 517712, "start": 5186.4, "end": 5191.84, "text": " before Leibniz and Newton you know invented calculus it's like everybody memorizing a whole", "tokens": [50828, 949, 1456, 897, 77, 590, 293, 19541, 291, 458, 14479, 33400, 309, 311, 411, 2201, 10560, 3319, 257, 1379, 51100], "temperature": 0.0, "avg_logprob": -0.05841352389408992, "compression_ratio": 1.6643109540636043, "no_speech_prob": 0.01267150603234768}, {"id": 906, "seek": 517712, "start": 5191.84, "end": 5196.96, "text": " bunch of little purpose built kind of formulas and then along comes a general framework which", "tokens": [51100, 3840, 295, 707, 4334, 3094, 733, 295, 30546, 293, 550, 2051, 1487, 257, 2674, 8388, 597, 51356], "temperature": 0.0, "avg_logprob": -0.05841352389408992, "compression_ratio": 1.6643109540636043, "no_speech_prob": 0.01267150603234768}, {"id": 907, "seek": 517712, "start": 5196.96, "end": 5202.24, "text": " now we can just learn calculus and derive the special circumstances as needed. You're onto", "tokens": [51356, 586, 321, 393, 445, 1466, 33400, 293, 28446, 264, 2121, 9121, 382, 2978, 13, 509, 434, 3911, 51620], "temperature": 0.0, "avg_logprob": -0.05841352389408992, "compression_ratio": 1.6643109540636043, "no_speech_prob": 0.01267150603234768}, {"id": 908, "seek": 520224, "start": 5202.24, "end": 5208.08, "text": " something really interesting there which is that with IML methods we are we are kind of", "tokens": [50364, 746, 534, 1880, 456, 597, 307, 300, 365, 286, 12683, 7150, 321, 366, 321, 366, 733, 295, 50656], "temperature": 0.0, "avg_logprob": -0.06649876557863675, "compression_ratio": 1.7527272727272727, "no_speech_prob": 0.04834151268005371}, {"id": 909, "seek": 520224, "start": 5208.08, "end": 5213.44, "text": " compressing information down into a representation you know and then that that is a transport that", "tokens": [50656, 14778, 278, 1589, 760, 666, 257, 10290, 291, 458, 293, 550, 300, 300, 307, 257, 5495, 300, 50924], "temperature": 0.0, "avg_logprob": -0.06649876557863675, "compression_ratio": 1.7527272727272727, "no_speech_prob": 0.04834151268005371}, {"id": 910, "seek": 520224, "start": 5213.44, "end": 5218.16, "text": " can be understood by different people but there's a trade-off right because as you said you can learn", "tokens": [50924, 393, 312, 7320, 538, 819, 561, 457, 456, 311, 257, 4923, 12, 4506, 558, 570, 382, 291, 848, 291, 393, 1466, 51160], "temperature": 0.0, "avg_logprob": -0.06649876557863675, "compression_ratio": 1.7527272727272727, "no_speech_prob": 0.04834151268005371}, {"id": 911, "seek": 520224, "start": 5218.16, "end": 5223.04, "text": " calculus and that's a compact framework for doing lots of stuff but it's all about the amount of", "tokens": [51160, 33400, 293, 300, 311, 257, 14679, 8388, 337, 884, 3195, 295, 1507, 457, 309, 311, 439, 466, 264, 2372, 295, 51404], "temperature": 0.0, "avg_logprob": -0.06649876557863675, "compression_ratio": 1.7527272727272727, "no_speech_prob": 0.04834151268005371}, {"id": 912, "seek": 520224, "start": 5223.04, "end": 5230.24, "text": " common knowledge that is required so it's possible to compress something down just to one symbol", "tokens": [51404, 2689, 3601, 300, 307, 4739, 370, 309, 311, 1944, 281, 14778, 746, 760, 445, 281, 472, 5986, 51764], "temperature": 0.0, "avg_logprob": -0.06649876557863675, "compression_ratio": 1.7527272727272727, "no_speech_prob": 0.04834151268005371}, {"id": 913, "seek": 523024, "start": 5230.24, "end": 5233.84, "text": " and that symbol could represent all of that knowledge but it doesn't help you because I still", "tokens": [50364, 293, 300, 5986, 727, 2906, 439, 295, 300, 3601, 457, 309, 1177, 380, 854, 291, 570, 286, 920, 50544], "temperature": 0.0, "avg_logprob": -0.06420820662118856, "compression_ratio": 1.8244274809160306, "no_speech_prob": 0.0018673022277653217}, {"id": 914, "seek": 523024, "start": 5233.84, "end": 5240.639999999999, "text": " need to learn all of that knowledge. Yeah but so to me calculus was a very beautiful and simple", "tokens": [50544, 643, 281, 1466, 439, 295, 300, 3601, 13, 865, 457, 370, 281, 385, 33400, 390, 257, 588, 2238, 293, 2199, 50884], "temperature": 0.0, "avg_logprob": -0.06420820662118856, "compression_ratio": 1.8244274809160306, "no_speech_prob": 0.0018673022277653217}, {"id": 915, "seek": 523024, "start": 5240.639999999999, "end": 5246.16, "text": " framework that I could learn and then once I learned that simple thing I could go and solve all", "tokens": [50884, 8388, 300, 286, 727, 1466, 293, 550, 1564, 286, 3264, 300, 2199, 551, 286, 727, 352, 293, 5039, 439, 51160], "temperature": 0.0, "avg_logprob": -0.06420820662118856, "compression_ratio": 1.8244274809160306, "no_speech_prob": 0.0018673022277653217}, {"id": 916, "seek": 523024, "start": 5246.16, "end": 5250.32, "text": " kinds of problems with it that before I would have to memorize specific solutions or like the", "tokens": [51160, 3685, 295, 2740, 365, 309, 300, 949, 286, 576, 362, 281, 27478, 2685, 6547, 420, 411, 264, 51368], "temperature": 0.0, "avg_logprob": -0.06420820662118856, "compression_ratio": 1.8244274809160306, "no_speech_prob": 0.0018673022277653217}, {"id": 917, "seek": 523024, "start": 5250.32, "end": 5255.84, "text": " quadratic formula for example is a student I didn't actually memorize the quadratic formula I just", "tokens": [51368, 37262, 8513, 337, 1365, 307, 257, 3107, 286, 994, 380, 767, 27478, 264, 37262, 8513, 286, 445, 51644], "temperature": 0.0, "avg_logprob": -0.06420820662118856, "compression_ratio": 1.8244274809160306, "no_speech_prob": 0.0018673022277653217}, {"id": 918, "seek": 525584, "start": 5255.92, "end": 5260.24, "text": " learned how to complete the square and then I would just do complete the square and if somebody", "tokens": [50368, 3264, 577, 281, 3566, 264, 3732, 293, 550, 286, 576, 445, 360, 3566, 264, 3732, 293, 498, 2618, 50584], "temperature": 0.0, "avg_logprob": -0.08135529284207325, "compression_ratio": 1.7547169811320755, "no_speech_prob": 0.04207146540284157}, {"id": 919, "seek": 525584, "start": 5260.24, "end": 5264.72, "text": " asked me what the quadratic formula was I would just quickly derive it right because it was", "tokens": [50584, 2351, 385, 437, 264, 37262, 8513, 390, 286, 576, 445, 2661, 28446, 309, 558, 570, 309, 390, 50808], "temperature": 0.0, "avg_logprob": -0.08135529284207325, "compression_ratio": 1.7547169811320755, "no_speech_prob": 0.04207146540284157}, {"id": 920, "seek": 525584, "start": 5264.72, "end": 5270.08, "text": " easier to memorize the rule and then apply the rule to any situation rather than to memorize", "tokens": [50808, 3571, 281, 27478, 264, 4978, 293, 550, 3079, 264, 4978, 281, 604, 2590, 2831, 813, 281, 27478, 51076], "temperature": 0.0, "avg_logprob": -0.08135529284207325, "compression_ratio": 1.7547169811320755, "no_speech_prob": 0.04207146540284157}, {"id": 921, "seek": 525584, "start": 5270.08, "end": 5275.84, "text": " all these little one-off you know kinds of hacks that we come up with. You're not normal Keith", "tokens": [51076, 439, 613, 707, 472, 12, 4506, 291, 458, 3685, 295, 33617, 300, 321, 808, 493, 365, 13, 509, 434, 406, 2710, 20613, 51364], "temperature": 0.0, "avg_logprob": -0.08135529284207325, "compression_ratio": 1.7547169811320755, "no_speech_prob": 0.04207146540284157}, {"id": 922, "seek": 525584, "start": 5275.84, "end": 5280.64, "text": " right so most people won't be able to go and understand this because I think it's well no", "tokens": [51364, 558, 370, 881, 561, 1582, 380, 312, 1075, 281, 352, 293, 1223, 341, 570, 286, 519, 309, 311, 731, 572, 51604], "temperature": 0.0, "avg_logprob": -0.08135529284207325, "compression_ratio": 1.7547169811320755, "no_speech_prob": 0.04207146540284157}, {"id": 923, "seek": 528064, "start": 5280.64, "end": 5287.52, "text": " these IML methods are brilliant for data scientists who can it's a framework right it's a", "tokens": [50364, 613, 286, 12683, 7150, 366, 10248, 337, 1412, 7708, 567, 393, 309, 311, 257, 8388, 558, 309, 311, 257, 50708], "temperature": 0.0, "avg_logprob": -0.10049511687924163, "compression_ratio": 1.686832740213523, "no_speech_prob": 0.007340694312006235}, {"id": 924, "seek": 528064, "start": 5287.52, "end": 5292.0, "text": " reference of understanding so assuming that people can understand how Shapley values work", "tokens": [50708, 6408, 295, 3701, 370, 11926, 300, 561, 393, 1223, 577, 44160, 3420, 4190, 589, 50932], "temperature": 0.0, "avg_logprob": -0.10049511687924163, "compression_ratio": 1.686832740213523, "no_speech_prob": 0.007340694312006235}, {"id": 925, "seek": 528064, "start": 5292.0, "end": 5298.320000000001, "text": " then this is a beautiful representation to reason about the behavior of models. Sure but when I", "tokens": [50932, 550, 341, 307, 257, 2238, 10290, 281, 1778, 466, 264, 5223, 295, 5245, 13, 4894, 457, 562, 286, 51248], "temperature": 0.0, "avg_logprob": -0.10049511687924163, "compression_ratio": 1.686832740213523, "no_speech_prob": 0.007340694312006235}, {"id": 926, "seek": 528064, "start": 5298.320000000001, "end": 5303.84, "text": " first saw Shapley values I realized immediately there's a connection in you know Bayesian analysis", "tokens": [51248, 700, 1866, 44160, 3420, 4190, 286, 5334, 4258, 456, 311, 257, 4984, 294, 291, 458, 7840, 42434, 5215, 51524], "temperature": 0.0, "avg_logprob": -0.10049511687924163, "compression_ratio": 1.686832740213523, "no_speech_prob": 0.007340694312006235}, {"id": 927, "seek": 528064, "start": 5303.84, "end": 5309.52, "text": " to marginalization you know all we're really doing here is computing the expected marginal you know", "tokens": [51524, 281, 16885, 2144, 291, 458, 439, 321, 434, 534, 884, 510, 307, 15866, 264, 5176, 16885, 291, 458, 51808], "temperature": 0.0, "avg_logprob": -0.10049511687924163, "compression_ratio": 1.686832740213523, "no_speech_prob": 0.007340694312006235}, {"id": 928, "seek": 530952, "start": 5309.52, "end": 5315.280000000001, "text": " contribution to this value it's not a probability but it's still the same procedure being done right", "tokens": [50364, 13150, 281, 341, 2158, 309, 311, 406, 257, 8482, 457, 309, 311, 920, 264, 912, 10747, 885, 1096, 558, 50652], "temperature": 0.0, "avg_logprob": -0.049709612468503556, "compression_ratio": 1.6701030927835052, "no_speech_prob": 0.002980625256896019}, {"id": 929, "seek": 530952, "start": 5315.280000000001, "end": 5320.0, "text": " and I think I'm going to throw myself in with the lay people to a degree because the reason I'm", "tokens": [50652, 293, 286, 519, 286, 478, 516, 281, 3507, 2059, 294, 365, 264, 2360, 561, 281, 257, 4314, 570, 264, 1778, 286, 478, 50888], "temperature": 0.0, "avg_logprob": -0.049709612468503556, "compression_ratio": 1.6701030927835052, "no_speech_prob": 0.002980625256896019}, {"id": 930, "seek": 530952, "start": 5320.0, "end": 5326.320000000001, "text": " always striving for simplifications is because I don't have the capacity to memorize all these little", "tokens": [50888, 1009, 36582, 337, 6883, 7833, 307, 570, 286, 500, 380, 362, 264, 6042, 281, 27478, 439, 613, 707, 51204], "temperature": 0.0, "avg_logprob": -0.049709612468503556, "compression_ratio": 1.6701030927835052, "no_speech_prob": 0.002980625256896019}, {"id": 931, "seek": 530952, "start": 5326.320000000001, "end": 5332.080000000001, "text": " arbitrary kinds of hacks and but I yet I could totally understand Bayesian analysis and like I", "tokens": [51204, 23211, 3685, 295, 33617, 293, 457, 286, 1939, 286, 727, 3879, 1223, 7840, 42434, 5215, 293, 411, 286, 51492], "temperature": 0.0, "avg_logprob": -0.049709612468503556, "compression_ratio": 1.6701030927835052, "no_speech_prob": 0.002980625256896019}, {"id": 932, "seek": 530952, "start": 5332.080000000001, "end": 5337.360000000001, "text": " said you know previously in some other videos statistics made no sense to me until I learned", "tokens": [51492, 848, 291, 458, 8046, 294, 512, 661, 2145, 12523, 1027, 572, 2020, 281, 385, 1826, 286, 3264, 51756], "temperature": 0.0, "avg_logprob": -0.049709612468503556, "compression_ratio": 1.6701030927835052, "no_speech_prob": 0.002980625256896019}, {"id": 933, "seek": 533736, "start": 5337.36, "end": 5344.08, "text": " the Bayesian framework because that was based on very simple rules that I could then reapply as needed.", "tokens": [50364, 264, 7840, 42434, 8388, 570, 300, 390, 2361, 322, 588, 2199, 4474, 300, 286, 727, 550, 35638, 356, 382, 2978, 13, 50700], "temperature": 0.0, "avg_logprob": -0.11998336965387518, "compression_ratio": 1.5392670157068062, "no_speech_prob": 0.009124161675572395}, {"id": 934, "seek": 533736, "start": 5345.599999999999, "end": 5352.719999999999, "text": " I think that what you refer to Keith maybe the worst situation is with the saliency maps because", "tokens": [50776, 286, 519, 300, 437, 291, 2864, 281, 20613, 1310, 264, 5855, 2590, 307, 365, 264, 1845, 7848, 11317, 570, 51132], "temperature": 0.0, "avg_logprob": -0.11998336965387518, "compression_ratio": 1.5392670157068062, "no_speech_prob": 0.009124161675572395}, {"id": 935, "seek": 533736, "start": 5352.719999999999, "end": 5359.36, "text": " you have so many methods and they all like back propagate the gradient and to do input pixels", "tokens": [51132, 291, 362, 370, 867, 7150, 293, 436, 439, 411, 646, 48256, 264, 16235, 293, 281, 360, 4846, 18668, 51464], "temperature": 0.0, "avg_logprob": -0.11998336965387518, "compression_ratio": 1.5392670157068062, "no_speech_prob": 0.009124161675572395}, {"id": 936, "seek": 535936, "start": 5359.92, "end": 5367.12, "text": " and to um now do you have to like learn like how dozens of these framework works or like", "tokens": [50392, 293, 281, 1105, 586, 360, 291, 362, 281, 411, 1466, 411, 577, 18431, 295, 613, 8388, 1985, 420, 411, 50752], "temperature": 0.0, "avg_logprob": -0.13047757962854897, "compression_ratio": 1.7619047619047619, "no_speech_prob": 0.03358405828475952}, {"id": 937, "seek": 535936, "start": 5368.24, "end": 5373.28, "text": " how to interpret do interpretation with all of these and they're all kind of variants of each", "tokens": [50808, 577, 281, 7302, 360, 14174, 365, 439, 295, 613, 293, 436, 434, 439, 733, 295, 21669, 295, 1184, 51060], "temperature": 0.0, "avg_logprob": -0.13047757962854897, "compression_ratio": 1.7619047619047619, "no_speech_prob": 0.03358405828475952}, {"id": 938, "seek": 535936, "start": 5373.28, "end": 5381.679999999999, "text": " other so mostly because they just there's some ambiguity how you how you back propagate the", "tokens": [51060, 661, 370, 5240, 570, 436, 445, 456, 311, 512, 46519, 577, 291, 577, 291, 646, 48256, 264, 51480], "temperature": 0.0, "avg_logprob": -0.13047757962854897, "compression_ratio": 1.7619047619047619, "no_speech_prob": 0.03358405828475952}, {"id": 939, "seek": 535936, "start": 5381.679999999999, "end": 5388.799999999999, "text": " gradient because because of the non-linear units and stuff and a little bit differences how you", "tokens": [51480, 16235, 570, 570, 295, 264, 2107, 12, 28263, 6815, 293, 1507, 293, 257, 707, 857, 7300, 577, 291, 51836], "temperature": 0.0, "avg_logprob": -0.13047757962854897, "compression_ratio": 1.7619047619047619, "no_speech_prob": 0.03358405828475952}, {"id": 940, "seek": 538880, "start": 5388.88, "end": 5398.320000000001, "text": " can define this and so you have this huge like a sea of many different methods I think it would be", "tokens": [50368, 393, 6964, 341, 293, 370, 291, 362, 341, 2603, 411, 257, 4158, 295, 867, 819, 7150, 286, 519, 309, 576, 312, 50840], "temperature": 0.0, "avg_logprob": -0.08816656224867876, "compression_ratio": 1.7867298578199051, "no_speech_prob": 0.0053012073040008545}, {"id": 941, "seek": 538880, "start": 5398.320000000001, "end": 5403.28, "text": " nice therefore as you said to have some like simplification where you say okay this is like", "tokens": [50840, 1481, 4412, 382, 291, 848, 281, 362, 512, 411, 6883, 3774, 689, 291, 584, 1392, 341, 307, 411, 51088], "temperature": 0.0, "avg_logprob": -0.08816656224867876, "compression_ratio": 1.7867298578199051, "no_speech_prob": 0.0053012073040008545}, {"id": 942, "seek": 538880, "start": 5404.4800000000005, "end": 5409.28, "text": " all these methods work under this one principle basically and we have these two parameters", "tokens": [51148, 439, 613, 7150, 589, 833, 341, 472, 8665, 1936, 293, 321, 362, 613, 732, 9834, 51388], "temperature": 0.0, "avg_logprob": -0.08816656224867876, "compression_ratio": 1.7867298578199051, "no_speech_prob": 0.0053012073040008545}, {"id": 943, "seek": 538880, "start": 5410.320000000001, "end": 5416.64, "text": " and that's how they differ I think that's also that some I think I wrote something in a chapter", "tokens": [51440, 293, 300, 311, 577, 436, 743, 286, 519, 300, 311, 611, 300, 512, 286, 519, 286, 4114, 746, 294, 257, 7187, 51756], "temperature": 0.0, "avg_logprob": -0.08816656224867876, "compression_ratio": 1.7867298578199051, "no_speech_prob": 0.0053012073040008545}, {"id": 944, "seek": 541664, "start": 5416.72, "end": 5425.84, "text": " that the police stop inventing new methods for saliency maps so I think it's enough and we", "tokens": [50368, 300, 264, 3804, 1590, 7962, 278, 777, 7150, 337, 1845, 7848, 11317, 370, 286, 519, 309, 311, 1547, 293, 321, 50824], "temperature": 0.0, "avg_logprob": -0.1273611347849776, "compression_ratio": 1.6872246696035242, "no_speech_prob": 0.0033210471738129854}, {"id": 945, "seek": 541664, "start": 5425.84, "end": 5433.04, "text": " should focus more on like doing this consolidation to like understand the limitations of the methods", "tokens": [50824, 820, 1879, 544, 322, 411, 884, 341, 39114, 281, 411, 1223, 264, 15705, 295, 264, 7150, 51184], "temperature": 0.0, "avg_logprob": -0.1273611347849776, "compression_ratio": 1.6872246696035242, "no_speech_prob": 0.0033210471738129854}, {"id": 946, "seek": 541664, "start": 5433.04, "end": 5438.72, "text": " and consolidate them to see like what's the commonalities in which ways do they differ and so on", "tokens": [51184, 293, 49521, 552, 281, 536, 411, 437, 311, 264, 2689, 16110, 294, 597, 2098, 360, 436, 743, 293, 370, 322, 51468], "temperature": 0.0, "avg_logprob": -0.1273611347849776, "compression_ratio": 1.6872246696035242, "no_speech_prob": 0.0033210471738129854}, {"id": 947, "seek": 541664, "start": 5438.72, "end": 5442.96, "text": " that's probably actually my first impression actually when I first opened the interpretable ML", "tokens": [51468, 300, 311, 1391, 767, 452, 700, 9995, 767, 562, 286, 700, 5625, 264, 7302, 712, 21601, 51680], "temperature": 0.0, "avg_logprob": -0.1273611347849776, "compression_ratio": 1.6872246696035242, "no_speech_prob": 0.0033210471738129854}, {"id": 948, "seek": 544296, "start": 5442.96, "end": 5447.68, "text": " book I was amazed how many different things there are I've you know heard people say ah", "tokens": [50364, 1446, 286, 390, 20507, 577, 867, 819, 721, 456, 366, 286, 600, 291, 458, 2198, 561, 584, 3716, 50600], "temperature": 0.0, "avg_logprob": -0.08726633036578144, "compression_ratio": 1.7472527472527473, "no_speech_prob": 0.14764270186424255}, {"id": 949, "seek": 544296, "start": 5447.68, "end": 5452.08, "text": " you can't use machine learning it's just a black box so many times it'd almost been drilled into my", "tokens": [50600, 291, 393, 380, 764, 3479, 2539, 309, 311, 445, 257, 2211, 2424, 370, 867, 1413, 309, 1116, 1920, 668, 38210, 666, 452, 50820], "temperature": 0.0, "avg_logprob": -0.08726633036578144, "compression_ratio": 1.7472527472527473, "no_speech_prob": 0.14764270186424255}, {"id": 950, "seek": 544296, "start": 5452.08, "end": 5458.24, "text": " head then seeing all the things you know from white box models ways of training salient models", "tokens": [50820, 1378, 550, 2577, 439, 264, 721, 291, 458, 490, 2418, 2424, 5245, 2098, 295, 3097, 1845, 1196, 5245, 51128], "temperature": 0.0, "avg_logprob": -0.08726633036578144, "compression_ratio": 1.7472527472527473, "no_speech_prob": 0.14764270186424255}, {"id": 951, "seek": 544296, "start": 5458.24, "end": 5462.4, "text": " counterfactual explanations it's what a wonderful recipe right there are so many different things", "tokens": [51128, 5682, 44919, 901, 28708, 309, 311, 437, 257, 3715, 6782, 558, 456, 366, 370, 867, 819, 721, 51336], "temperature": 0.0, "avg_logprob": -0.08726633036578144, "compression_ratio": 1.7472527472527473, "no_speech_prob": 0.14764270186424255}, {"id": 952, "seek": 544296, "start": 5462.4, "end": 5467.84, "text": " that you can do I feel like now I trust ML models more than other kinds of things because I have", "tokens": [51336, 300, 291, 393, 360, 286, 841, 411, 586, 286, 3361, 21601, 5245, 544, 813, 661, 3685, 295, 721, 570, 286, 362, 51608], "temperature": 0.0, "avg_logprob": -0.08726633036578144, "compression_ratio": 1.7472527472527473, "no_speech_prob": 0.14764270186424255}, {"id": 953, "seek": 546784, "start": 5467.84, "end": 5473.52, "text": " this amazing toolbox of ways to understand them the thing that strikes me though is", "tokens": [50364, 341, 2243, 44593, 295, 2098, 281, 1223, 552, 264, 551, 300, 16750, 385, 1673, 307, 50648], "temperature": 0.0, "avg_logprob": -0.10015082359313965, "compression_ratio": 1.7265917602996255, "no_speech_prob": 0.005722935777157545}, {"id": 954, "seek": 546784, "start": 5474.400000000001, "end": 5478.8, "text": " most of these methods as we were just saying they require interpretation by a human and a human who", "tokens": [50692, 881, 295, 613, 7150, 382, 321, 645, 445, 1566, 436, 3651, 14174, 538, 257, 1952, 293, 257, 1952, 567, 50912], "temperature": 0.0, "avg_logprob": -0.10015082359313965, "compression_ratio": 1.7265917602996255, "no_speech_prob": 0.005722935777157545}, {"id": 955, "seek": 546784, "start": 5478.8, "end": 5484.4800000000005, "text": " understands how the method works I love this concept of turning machine learning into an", "tokens": [50912, 15146, 577, 264, 3170, 1985, 286, 959, 341, 3410, 295, 6246, 3479, 2539, 666, 364, 51196], "temperature": 0.0, "avg_logprob": -0.10015082359313965, "compression_ratio": 1.7265917602996255, "no_speech_prob": 0.005722935777157545}, {"id": 956, "seek": 546784, "start": 5484.4800000000005, "end": 5492.400000000001, "text": " engineering discipline and being able to do a lot of these tests non-interactively and I think", "tokens": [51196, 7043, 13635, 293, 885, 1075, 281, 360, 257, 688, 295, 613, 6921, 2107, 12, 5106, 45679, 293, 286, 519, 51592], "temperature": 0.0, "avg_logprob": -0.10015082359313965, "compression_ratio": 1.7265917602996255, "no_speech_prob": 0.005722935777157545}, {"id": 957, "seek": 546784, "start": 5493.12, "end": 5497.6, "text": " Marco Rubirio has done a lot of work around the counterfactual examples and the data grouping", "tokens": [51628, 26535, 10518, 347, 1004, 575, 1096, 257, 688, 295, 589, 926, 264, 5682, 44919, 901, 5110, 293, 264, 1412, 40149, 51852], "temperature": 0.0, "avg_logprob": -0.10015082359313965, "compression_ratio": 1.7265917602996255, "no_speech_prob": 0.005722935777157545}, {"id": 958, "seek": 549760, "start": 5497.6, "end": 5503.360000000001, "text": " and what excites me about these methods is they seem like methods that we could actually run", "tokens": [50364, 293, 437, 1624, 3324, 385, 466, 613, 7150, 307, 436, 1643, 411, 7150, 300, 321, 727, 767, 1190, 50652], "temperature": 0.0, "avg_logprob": -0.045860028048174094, "compression_ratio": 1.8423076923076922, "no_speech_prob": 0.002992242807522416}, {"id": 959, "seek": 549760, "start": 5503.360000000001, "end": 5508.400000000001, "text": " as part of an automated process we still have to set thresholds maybe we could set a threshold", "tokens": [50652, 382, 644, 295, 364, 18473, 1399, 321, 920, 362, 281, 992, 14678, 82, 1310, 321, 727, 992, 257, 14678, 50904], "temperature": 0.0, "avg_logprob": -0.045860028048174094, "compression_ratio": 1.8423076923076922, "no_speech_prob": 0.002992242807522416}, {"id": 960, "seek": 549760, "start": 5508.400000000001, "end": 5513.84, "text": " that said if this if this counterfactual example flips the switch on more than one percent of", "tokens": [50904, 300, 848, 498, 341, 498, 341, 5682, 44919, 901, 1365, 40249, 264, 3679, 322, 544, 813, 472, 3043, 295, 51176], "temperature": 0.0, "avg_logprob": -0.045860028048174094, "compression_ratio": 1.8423076923076922, "no_speech_prob": 0.002992242807522416}, {"id": 961, "seek": 549760, "start": 5513.84, "end": 5519.200000000001, "text": " examples then fail the build that seems reasonable but a saliency method I mean how the hell do you", "tokens": [51176, 5110, 550, 3061, 264, 1322, 300, 2544, 10585, 457, 257, 1845, 7848, 3170, 286, 914, 577, 264, 4921, 360, 291, 51444], "temperature": 0.0, "avg_logprob": -0.045860028048174094, "compression_ratio": 1.8423076923076922, "no_speech_prob": 0.002992242807522416}, {"id": 962, "seek": 549760, "start": 5519.200000000001, "end": 5523.84, "text": " say well if there's lots of red pixels over here then break the build I mean it's just ridiculous", "tokens": [51444, 584, 731, 498, 456, 311, 3195, 295, 2182, 18668, 670, 510, 550, 1821, 264, 1322, 286, 914, 309, 311, 445, 11083, 51676], "temperature": 0.0, "avg_logprob": -0.045860028048174094, "compression_ratio": 1.8423076923076922, "no_speech_prob": 0.002992242807522416}, {"id": 963, "seek": 552384, "start": 5524.72, "end": 5532.64, "text": " yeah yeah so I've seen interesting approaches to like using interpretability also more", "tokens": [50408, 1338, 1338, 370, 286, 600, 1612, 1880, 11587, 281, 411, 1228, 7302, 2310, 611, 544, 50804], "temperature": 0.0, "avg_logprob": -0.10316762924194336, "compression_ratio": 1.5738636363636365, "no_speech_prob": 0.020310258492827415}, {"id": 964, "seek": 552384, "start": 5532.64, "end": 5541.68, "text": " automatically like when you do model monitoring you can do things like create interpretations", "tokens": [50804, 6772, 411, 562, 291, 360, 2316, 11028, 291, 393, 360, 721, 411, 1884, 37547, 51256], "temperature": 0.0, "avg_logprob": -0.10316762924194336, "compression_ratio": 1.5738636363636365, "no_speech_prob": 0.020310258492827415}, {"id": 965, "seek": 552384, "start": 5541.68, "end": 5547.92, "text": " and see if they significantly change over time for example so then have thresholds that warn you", "tokens": [51256, 293, 536, 498, 436, 10591, 1319, 670, 565, 337, 1365, 370, 550, 362, 14678, 82, 300, 12286, 291, 51568], "temperature": 0.0, "avg_logprob": -0.10316762924194336, "compression_ratio": 1.5738636363636365, "no_speech_prob": 0.020310258492827415}, {"id": 966, "seek": 554792, "start": 5547.92, "end": 5554.88, "text": " that hey something's going on with your model so I think that's also interesting approaches there", "tokens": [50364, 300, 4177, 746, 311, 516, 322, 365, 428, 2316, 370, 286, 519, 300, 311, 611, 1880, 11587, 456, 50712], "temperature": 0.0, "avg_logprob": -0.09597703780250988, "compression_ratio": 1.7695852534562213, "no_speech_prob": 0.01911964826285839}, {"id": 967, "seek": 554792, "start": 5555.52, "end": 5561.76, "text": " yeah you know Tim to your point of making this an engineering field and even making interpretability", "tokens": [50744, 1338, 291, 458, 7172, 281, 428, 935, 295, 1455, 341, 364, 7043, 2519, 293, 754, 1455, 7302, 2310, 51056], "temperature": 0.0, "avg_logprob": -0.09597703780250988, "compression_ratio": 1.7695852534562213, "no_speech_prob": 0.01911964826285839}, {"id": 968, "seek": 554792, "start": 5561.76, "end": 5566.64, "text": " and and understandability an engineering field I mean I think that maybe that's why I like your", "tokens": [51056, 293, 293, 1223, 2310, 364, 7043, 2519, 286, 914, 286, 519, 300, 1310, 300, 311, 983, 286, 411, 428, 51300], "temperature": 0.0, "avg_logprob": -0.09597703780250988, "compression_ratio": 1.7695852534562213, "no_speech_prob": 0.01911964826285839}, {"id": 969, "seek": 554792, "start": 5566.64, "end": 5571.52, "text": " book so much Christoph as I think it's it's a step towards that direction it's like let's", "tokens": [51300, 1446, 370, 709, 2040, 5317, 382, 286, 519, 309, 311, 309, 311, 257, 1823, 3030, 300, 3513, 309, 311, 411, 718, 311, 51544], "temperature": 0.0, "avg_logprob": -0.09597703780250988, "compression_ratio": 1.7695852534562213, "no_speech_prob": 0.01911964826285839}, {"id": 970, "seek": 557152, "start": 5571.52, "end": 5579.280000000001, "text": " survey everything and more importantly let's create a finite and hopefully smallish set of simple", "tokens": [50364, 8984, 1203, 293, 544, 8906, 718, 311, 1884, 257, 19362, 293, 4696, 1359, 742, 992, 295, 2199, 50752], "temperature": 0.0, "avg_logprob": -0.07748411351984197, "compression_ratio": 1.7509025270758123, "no_speech_prob": 0.039034154266119}, {"id": 971, "seek": 557152, "start": 5579.280000000001, "end": 5585.76, "text": " concepts that we can all agree on and understand that we can use to catalog you know what's out there", "tokens": [50752, 10392, 300, 321, 393, 439, 3986, 322, 293, 1223, 300, 321, 393, 764, 281, 19746, 291, 458, 437, 311, 484, 456, 51076], "temperature": 0.0, "avg_logprob": -0.07748411351984197, "compression_ratio": 1.7509025270758123, "no_speech_prob": 0.039034154266119}, {"id": 972, "seek": 557152, "start": 5586.56, "end": 5591.68, "text": " so please keep up the good work you know I'm interested to see where this goes so final", "tokens": [51116, 370, 1767, 1066, 493, 264, 665, 589, 291, 458, 286, 478, 3102, 281, 536, 689, 341, 1709, 370, 2572, 51372], "temperature": 0.0, "avg_logprob": -0.07748411351984197, "compression_ratio": 1.7509025270758123, "no_speech_prob": 0.039034154266119}, {"id": 973, "seek": 557152, "start": 5591.68, "end": 5595.4400000000005, "text": " question for you Christoph then I wonder what's what's next for interpretability like are we going", "tokens": [51372, 1168, 337, 291, 2040, 5317, 550, 286, 2441, 437, 311, 437, 311, 958, 337, 7302, 2310, 411, 366, 321, 516, 51560], "temperature": 0.0, "avg_logprob": -0.07748411351984197, "compression_ratio": 1.7509025270758123, "no_speech_prob": 0.039034154266119}, {"id": 974, "seek": 557152, "start": 5595.4400000000005, "end": 5599.92, "text": " to the point where it's going to be almost a box ticking exercise where we can say yes our process", "tokens": [51560, 281, 264, 935, 689, 309, 311, 516, 281, 312, 1920, 257, 2424, 33999, 5380, 689, 321, 393, 584, 2086, 527, 1399, 51784], "temperature": 0.0, "avg_logprob": -0.07748411351984197, "compression_ratio": 1.7509025270758123, "no_speech_prob": 0.039034154266119}, {"id": 975, "seek": 559992, "start": 5599.92, "end": 5605.84, "text": " we've done the standard interpretability step I mean is it is computing power going to change it", "tokens": [50364, 321, 600, 1096, 264, 3832, 7302, 2310, 1823, 286, 914, 307, 309, 307, 15866, 1347, 516, 281, 1319, 309, 50660], "temperature": 0.0, "avg_logprob": -0.14047591750686234, "compression_ratio": 1.6586206896551725, "no_speech_prob": 0.027072103694081306}, {"id": 976, "seek": 559992, "start": 5605.84, "end": 5611.68, "text": " I remember when Shaq the library came out it made that approach possible whereas previously you know", "tokens": [50660, 286, 1604, 562, 14944, 80, 264, 6405, 1361, 484, 309, 1027, 300, 3109, 1944, 9735, 8046, 291, 458, 50952], "temperature": 0.0, "avg_logprob": -0.14047591750686234, "compression_ratio": 1.6586206896551725, "no_speech_prob": 0.027072103694081306}, {"id": 977, "seek": 559992, "start": 5611.68, "end": 5616.32, "text": " it was very hard very computation infusible my friend Angem who's a wonderful data scientist", "tokens": [50952, 309, 390, 588, 1152, 588, 24903, 1536, 301, 964, 452, 1277, 4521, 443, 567, 311, 257, 3715, 1412, 12662, 51184], "temperature": 0.0, "avg_logprob": -0.14047591750686234, "compression_ratio": 1.6586206896551725, "no_speech_prob": 0.027072103694081306}, {"id": 978, "seek": 559992, "start": 5616.32, "end": 5621.52, "text": " sent me n-video rapids they've got that running on GPUs way faster than of course before is it", "tokens": [51184, 2279, 385, 297, 12, 40876, 5099, 3742, 436, 600, 658, 300, 2614, 322, 18407, 82, 636, 4663, 813, 295, 1164, 949, 307, 309, 51444], "temperature": 0.0, "avg_logprob": -0.14047591750686234, "compression_ratio": 1.6586206896551725, "no_speech_prob": 0.027072103694081306}, {"id": 979, "seek": 559992, "start": 5621.52, "end": 5626.4, "text": " just going to become a standard step do you think or is it going to be something where you need", "tokens": [51444, 445, 516, 281, 1813, 257, 3832, 1823, 360, 291, 519, 420, 307, 309, 516, 281, 312, 746, 689, 291, 643, 51688], "temperature": 0.0, "avg_logprob": -0.14047591750686234, "compression_ratio": 1.6586206896551725, "no_speech_prob": 0.027072103694081306}, {"id": 980, "seek": 562640, "start": 5626.4, "end": 5631.839999999999, "text": " decent subject matter expertise and some real thought to do to really understand how a model", "tokens": [50364, 8681, 3983, 1871, 11769, 293, 512, 957, 1194, 281, 360, 281, 534, 1223, 577, 257, 2316, 50636], "temperature": 0.0, "avg_logprob": -0.18284524570811878, "compression_ratio": 1.6244725738396624, "no_speech_prob": 0.014253756031394005}, {"id": 981, "seek": 562640, "start": 5631.839999999999, "end": 5638.639999999999, "text": " works so well predictions about the future are always hard so maybe more like what I wish or", "tokens": [50636, 1985, 370, 731, 21264, 466, 264, 2027, 366, 1009, 1152, 370, 1310, 544, 411, 437, 286, 3172, 420, 50976], "temperature": 0.0, "avg_logprob": -0.18284524570811878, "compression_ratio": 1.6244725738396624, "no_speech_prob": 0.014253756031394005}, {"id": 982, "seek": 562640, "start": 5638.639999999999, "end": 5644.32, "text": " what yeah maybe think we'll have code to happen um so I mean what we're seeing already is like a lot", "tokens": [50976, 437, 1338, 1310, 519, 321, 603, 362, 3089, 281, 1051, 1105, 370, 286, 914, 437, 321, 434, 2577, 1217, 307, 411, 257, 688, 51260], "temperature": 0.0, "avg_logprob": -0.18284524570811878, "compression_ratio": 1.6244725738396624, "no_speech_prob": 0.014253756031394005}, {"id": 983, "seek": 562640, "start": 5644.32, "end": 5650.32, "text": " of implementations of these methods so they're kind of getting a commonality a rook one can use it", "tokens": [51260, 295, 4445, 763, 295, 613, 7150, 370, 436, 434, 733, 295, 1242, 257, 2689, 1860, 257, 24692, 472, 393, 764, 309, 51560], "temperature": 0.0, "avg_logprob": -0.18284524570811878, "compression_ratio": 1.6244725738396624, "no_speech_prob": 0.014253756031394005}, {"id": 984, "seek": 565032, "start": 5650.32, "end": 5658.719999999999, "text": " very easily there's a lot of libraries out there in python r but also in like this machine learning", "tokens": [50364, 588, 3612, 456, 311, 257, 688, 295, 15148, 484, 456, 294, 38797, 367, 457, 611, 294, 411, 341, 3479, 2539, 50784], "temperature": 0.0, "avg_logprob": -0.13234974089122953, "compression_ratio": 1.6768558951965065, "no_speech_prob": 0.004133081529289484}, {"id": 985, "seek": 565032, "start": 5659.679999999999, "end": 5666.96, "text": " cloud tools they also have a lot of interpretation methods available now so in that sense I think", "tokens": [50832, 4588, 3873, 436, 611, 362, 257, 688, 295, 14174, 7150, 2435, 586, 370, 294, 300, 2020, 286, 519, 51196], "temperature": 0.0, "avg_logprob": -0.13234974089122953, "compression_ratio": 1.6768558951965065, "no_speech_prob": 0.004133081529289484}, {"id": 986, "seek": 565032, "start": 5667.599999999999, "end": 5674.799999999999, "text": " and it's maturing a lot I still believe that we need some expertise to understand them or at", "tokens": [51228, 293, 309, 311, 3803, 1345, 257, 688, 286, 920, 1697, 300, 321, 643, 512, 11769, 281, 1223, 552, 420, 412, 51588], "temperature": 0.0, "avg_logprob": -0.13234974089122953, "compression_ratio": 1.6768558951965065, "no_speech_prob": 0.004133081529289484}, {"id": 987, "seek": 565032, "start": 5674.799999999999, "end": 5679.2, "text": " least some good references and there will also be hopefully more than my book maybe have some", "tokens": [51588, 1935, 512, 665, 15400, 293, 456, 486, 611, 312, 4696, 544, 813, 452, 1446, 1310, 362, 512, 51808], "temperature": 0.0, "avg_logprob": -0.13234974089122953, "compression_ratio": 1.6768558951965065, "no_speech_prob": 0.004133081529289484}, {"id": 988, "seek": 568032, "start": 5680.799999999999, "end": 5686.48, "text": " documentation when for these tools and people answering on stack overflow questions and whatnot", "tokens": [50388, 14333, 562, 337, 613, 3873, 293, 561, 13430, 322, 8630, 37772, 1651, 293, 25882, 50672], "temperature": 0.0, "avg_logprob": -0.13503727280949973, "compression_ratio": 1.6502242152466369, "no_speech_prob": 0.0022405709605664015}, {"id": 989, "seek": 568032, "start": 5688.24, "end": 5694.24, "text": " so I think um yeah it's it's getting we're getting that everyone can use it easily", "tokens": [50760, 370, 286, 519, 1105, 1338, 309, 311, 309, 311, 1242, 321, 434, 1242, 300, 1518, 393, 764, 309, 3612, 51060], "temperature": 0.0, "avg_logprob": -0.13503727280949973, "compression_ratio": 1.6502242152466369, "no_speech_prob": 0.0022405709605664015}, {"id": 990, "seek": 568032, "start": 5695.84, "end": 5701.84, "text": " I think it should never be a box ticking exercise it's a similar thing when if you have an AI ethics", "tokens": [51140, 286, 519, 309, 820, 1128, 312, 257, 2424, 33999, 5380, 309, 311, 257, 2531, 551, 562, 498, 291, 362, 364, 7318, 19769, 51440], "temperature": 0.0, "avg_logprob": -0.13503727280949973, "compression_ratio": 1.6502242152466369, "no_speech_prob": 0.0022405709605664015}, {"id": 991, "seek": 568032, "start": 5702.719999999999, "end": 5706.4, "text": " you know governance process or something the last thing you want is for it just to be an", "tokens": [51484, 291, 458, 17449, 1399, 420, 746, 264, 1036, 551, 291, 528, 307, 337, 309, 445, 281, 312, 364, 51668], "temperature": 0.0, "avg_logprob": -0.13503727280949973, "compression_ratio": 1.6502242152466369, "no_speech_prob": 0.0022405709605664015}, {"id": 992, "seek": 570640, "start": 5706.4, "end": 5711.12, "text": " automatic response so I've just you know yeah I've thought about AI ethics um it needs to be", "tokens": [50364, 12509, 4134, 370, 286, 600, 445, 291, 458, 1338, 286, 600, 1194, 466, 7318, 19769, 1105, 309, 2203, 281, 312, 50600], "temperature": 0.0, "avg_logprob": -0.05678312561728738, "compression_ratio": 1.7851851851851852, "no_speech_prob": 0.03743116557598114}, {"id": 993, "seek": 570640, "start": 5711.12, "end": 5716.4, "text": " something that that we really engage with I think we need to abstract away a lot of the complexity", "tokens": [50600, 746, 300, 300, 321, 534, 4683, 365, 286, 519, 321, 643, 281, 12649, 1314, 257, 688, 295, 264, 14024, 50864], "temperature": 0.0, "avg_logprob": -0.05678312561728738, "compression_ratio": 1.7851851851851852, "no_speech_prob": 0.03743116557598114}, {"id": 994, "seek": 570640, "start": 5716.4, "end": 5722.08, "text": " at the moment I think it's possible to come up with an interface to standardize the way that we", "tokens": [50864, 412, 264, 1623, 286, 519, 309, 311, 1944, 281, 808, 493, 365, 364, 9226, 281, 3832, 1125, 264, 636, 300, 321, 51148], "temperature": 0.0, "avg_logprob": -0.05678312561728738, "compression_ratio": 1.7851851851851852, "no_speech_prob": 0.03743116557598114}, {"id": 995, "seek": 570640, "start": 5722.08, "end": 5728.799999999999, "text": " do interpretability and we can reduce down what we have now to certain primitives which means that", "tokens": [51148, 360, 7302, 2310, 293, 321, 393, 5407, 760, 437, 321, 362, 586, 281, 1629, 2886, 38970, 597, 1355, 300, 51484], "temperature": 0.0, "avg_logprob": -0.05678312561728738, "compression_ratio": 1.7851851851851852, "no_speech_prob": 0.03743116557598114}, {"id": 996, "seek": 570640, "start": 5728.799999999999, "end": 5733.28, "text": " it can plug into an engineering process and it also means that we can abstract away some of the", "tokens": [51484, 309, 393, 5452, 666, 364, 7043, 1399, 293, 309, 611, 1355, 300, 321, 393, 12649, 1314, 512, 295, 264, 51708], "temperature": 0.0, "avg_logprob": -0.05678312561728738, "compression_ratio": 1.7851851851851852, "no_speech_prob": 0.03743116557598114}, {"id": 997, "seek": 573328, "start": 5733.28, "end": 5738.88, "text": " complexity I think that's possible yeah I also would agree that it shouldn't be like just box", "tokens": [50364, 14024, 286, 519, 300, 311, 1944, 1338, 286, 611, 576, 3986, 300, 309, 4659, 380, 312, 411, 445, 2424, 50644], "temperature": 0.0, "avg_logprob": -0.06031264804658436, "compression_ratio": 1.8233082706766917, "no_speech_prob": 0.005607426632195711}, {"id": 998, "seek": 573328, "start": 5738.88, "end": 5744.4, "text": " ticking but you can like for the initial um when you start interpreting a model that you just have", "tokens": [50644, 33999, 457, 291, 393, 411, 337, 264, 5883, 1105, 562, 291, 722, 37395, 257, 2316, 300, 291, 445, 362, 50920], "temperature": 0.0, "avg_logprob": -0.06031264804658436, "compression_ratio": 1.8233082706766917, "no_speech_prob": 0.005607426632195711}, {"id": 999, "seek": 573328, "start": 5744.4, "end": 5749.2, "text": " like with a click you have a report and then it shows you the most basic things but then you still", "tokens": [50920, 411, 365, 257, 2052, 291, 362, 257, 2275, 293, 550, 309, 3110, 291, 264, 881, 3875, 721, 457, 550, 291, 920, 51160], "temperature": 0.0, "avg_logprob": -0.06031264804658436, "compression_ratio": 1.8233082706766917, "no_speech_prob": 0.005607426632195711}, {"id": 1000, "seek": 573328, "start": 5749.2, "end": 5753.36, "text": " like should ask the like the question like does it really make sense that this feature is the most", "tokens": [51160, 411, 820, 1029, 264, 411, 264, 1168, 411, 775, 309, 534, 652, 2020, 300, 341, 4111, 307, 264, 881, 51368], "temperature": 0.0, "avg_logprob": -0.06031264804658436, "compression_ratio": 1.8233082706766917, "no_speech_prob": 0.005607426632195711}, {"id": 1001, "seek": 573328, "start": 5753.36, "end": 5757.679999999999, "text": " important one or what's happening there with these weird interactions between the two features", "tokens": [51368, 1021, 472, 420, 437, 311, 2737, 456, 365, 613, 3657, 13280, 1296, 264, 732, 4122, 51584], "temperature": 0.0, "avg_logprob": -0.06031264804658436, "compression_ratio": 1.8233082706766917, "no_speech_prob": 0.005607426632195711}, {"id": 1002, "seek": 575768, "start": 5757.68, "end": 5764.72, "text": " let's dig a bit deeper here and see what's going on so I think um there's this one portion that is", "tokens": [50364, 718, 311, 2528, 257, 857, 7731, 510, 293, 536, 437, 311, 516, 322, 370, 286, 519, 1105, 456, 311, 341, 472, 8044, 300, 307, 50716], "temperature": 0.0, "avg_logprob": -0.125043713528177, "compression_ratio": 1.7737556561085972, "no_speech_prob": 0.002359200967475772}, {"id": 1003, "seek": 575768, "start": 5764.72, "end": 5772.16, "text": " just like this automated reporting thing um but this should then be like the starting point for", "tokens": [50716, 445, 411, 341, 18473, 10031, 551, 1105, 457, 341, 820, 550, 312, 411, 264, 2891, 935, 337, 51088], "temperature": 0.0, "avg_logprob": -0.125043713528177, "compression_ratio": 1.7737556561085972, "no_speech_prob": 0.002359200967475772}, {"id": 1004, "seek": 575768, "start": 5772.16, "end": 5779.200000000001, "text": " more critical uh questioning of the model and and and checking what's going on um for some specific", "tokens": [51088, 544, 4924, 2232, 21257, 295, 264, 2316, 293, 293, 293, 8568, 437, 311, 516, 322, 1105, 337, 512, 2685, 51440], "temperature": 0.0, "avg_logprob": -0.125043713528177, "compression_ratio": 1.7737556561085972, "no_speech_prob": 0.002359200967475772}, {"id": 1005, "seek": 575768, "start": 5780.16, "end": 5786.4800000000005, "text": " problems maybe so it's going to be you click on the molnar report and it gives you the the report", "tokens": [51488, 2740, 1310, 370, 309, 311, 516, 281, 312, 291, 2052, 322, 264, 8015, 20062, 2275, 293, 309, 2709, 291, 264, 264, 2275, 51804], "temperature": 0.0, "avg_logprob": -0.125043713528177, "compression_ratio": 1.7737556561085972, "no_speech_prob": 0.002359200967475772}, {"id": 1006, "seek": 578648, "start": 5786.48, "end": 5791.839999999999, "text": " from the book right yeah there will be convenience well um christoph molnar thank you very much for", "tokens": [50364, 490, 264, 1446, 558, 1338, 456, 486, 312, 19283, 731, 1105, 26586, 5317, 8015, 20062, 1309, 291, 588, 709, 337, 50632], "temperature": 0.0, "avg_logprob": -0.08601320873607289, "compression_ratio": 1.7477477477477477, "no_speech_prob": 0.013862170279026031}, {"id": 1007, "seek": 578648, "start": 5791.839999999999, "end": 5795.599999999999, "text": " joining us today it's an absolute honor to have you on the show thanks for having me hey folks this", "tokens": [50632, 5549, 505, 965, 309, 311, 364, 8236, 5968, 281, 362, 291, 322, 264, 855, 3231, 337, 1419, 385, 4177, 4024, 341, 50820], "temperature": 0.0, "avg_logprob": -0.08601320873607289, "compression_ratio": 1.7477477477477477, "no_speech_prob": 0.013862170279026031}, {"id": 1008, "seek": 578648, "start": 5795.599999999999, "end": 5798.959999999999, "text": " is tim in post script there's just a couple of thoughts that didn't come to my mind during the", "tokens": [50820, 307, 524, 294, 2183, 5755, 456, 311, 445, 257, 1916, 295, 4598, 300, 994, 380, 808, 281, 452, 1575, 1830, 264, 50988], "temperature": 0.0, "avg_logprob": -0.08601320873607289, "compression_ratio": 1.7477477477477477, "no_speech_prob": 0.013862170279026031}, {"id": 1009, "seek": 578648, "start": 5798.959999999999, "end": 5803.599999999999, "text": " interview that I think I'd like to quickly cover now the first thing is on the lack of fairness", "tokens": [50988, 4049, 300, 286, 519, 286, 1116, 411, 281, 2661, 2060, 586, 264, 700, 551, 307, 322, 264, 5011, 295, 29765, 51220], "temperature": 0.0, "avg_logprob": -0.08601320873607289, "compression_ratio": 1.7477477477477477, "no_speech_prob": 0.013862170279026031}, {"id": 1010, "seek": 578648, "start": 5803.599999999999, "end": 5810.16, "text": " the reason why I raised that is most folks who talk about AI ethics and fairness they use the", "tokens": [51220, 264, 1778, 983, 286, 6005, 300, 307, 881, 4024, 567, 751, 466, 7318, 19769, 293, 29765, 436, 764, 264, 51548], "temperature": 0.0, "avg_logprob": -0.08601320873607289, "compression_ratio": 1.7477477477477477, "no_speech_prob": 0.013862170279026031}, {"id": 1011, "seek": 578648, "start": 5810.16, "end": 5815.599999999999, "text": " toolkit of interpretability methods quite often you know to apply their trade there are tools out", "tokens": [51548, 40167, 295, 7302, 2310, 7150, 1596, 2049, 291, 458, 281, 3079, 641, 4923, 456, 366, 3873, 484, 51820], "temperature": 0.0, "avg_logprob": -0.08601320873607289, "compression_ratio": 1.7477477477477477, "no_speech_prob": 0.013862170279026031}, {"id": 1012, "seek": 581560, "start": 5815.6, "end": 5822.0, "text": " there to mitigate fairness and to detect fairness microsoft's fair learn is a great example of this", "tokens": [50364, 456, 281, 27336, 29765, 293, 281, 5531, 29765, 3123, 7856, 311, 3143, 1466, 307, 257, 869, 1365, 295, 341, 50684], "temperature": 0.0, "avg_logprob": -0.07015061672822928, "compression_ratio": 1.7897196261682242, "no_speech_prob": 0.0038042424712330103}, {"id": 1013, "seek": 581560, "start": 5822.0, "end": 5830.400000000001, "text": " what we really need is an operating model or a set of guidelines on how to implement these tools", "tokens": [50684, 437, 321, 534, 643, 307, 364, 7447, 2316, 420, 257, 992, 295, 12470, 322, 577, 281, 4445, 613, 3873, 51104], "temperature": 0.0, "avg_logprob": -0.07015061672822928, "compression_ratio": 1.7897196261682242, "no_speech_prob": 0.0038042424712330103}, {"id": 1014, "seek": 581560, "start": 5831.120000000001, "end": 5837.76, "text": " how do I identify sources of problematic correlations we need to have a database of problematic", "tokens": [51140, 577, 360, 286, 5876, 7139, 295, 19011, 13983, 763, 321, 643, 281, 362, 257, 8149, 295, 19011, 51472], "temperature": 0.0, "avg_logprob": -0.07015061672822928, "compression_ratio": 1.7897196261682242, "no_speech_prob": 0.0038042424712330103}, {"id": 1015, "seek": 581560, "start": 5837.76, "end": 5843.84, "text": " correlations having a tool that allows me to identify and mitigate bias frankly is useless", "tokens": [51472, 13983, 763, 1419, 257, 2290, 300, 4045, 385, 281, 5876, 293, 27336, 12577, 11939, 307, 14115, 51776], "temperature": 0.0, "avg_logprob": -0.07015061672822928, "compression_ratio": 1.7897196261682242, "no_speech_prob": 0.0038042424712330103}, {"id": 1016, "seek": 584384, "start": 5844.56, "end": 5850.16, "text": " what do I do with that as we mentioned on the show many of the machine learning cloud providers", "tokens": [50400, 437, 360, 286, 360, 365, 300, 382, 321, 2835, 322, 264, 855, 867, 295, 264, 3479, 2539, 4588, 11330, 50680], "temperature": 0.0, "avg_logprob": -0.05550948153720813, "compression_ratio": 1.5918367346938775, "no_speech_prob": 0.003071732819080353}, {"id": 1017, "seek": 584384, "start": 5850.16, "end": 5856.56, "text": " whether it's data iq or azure ml and sage maker they all have these interpretability methods built", "tokens": [50680, 1968, 309, 311, 1412, 741, 80, 420, 7883, 540, 23271, 293, 19721, 17127, 436, 439, 362, 613, 7302, 2310, 7150, 3094, 51000], "temperature": 0.0, "avg_logprob": -0.05550948153720813, "compression_ratio": 1.5918367346938775, "no_speech_prob": 0.003071732819080353}, {"id": 1018, "seek": 584384, "start": 5856.56, "end": 5862.64, "text": " in now including saliency maps and it's just a box ticking exercise frankly it's completely useless", "tokens": [51000, 294, 586, 3009, 1845, 7848, 11317, 293, 309, 311, 445, 257, 2424, 33999, 5380, 11939, 309, 311, 2584, 14115, 51304], "temperature": 0.0, "avg_logprob": -0.05550948153720813, "compression_ratio": 1.5918367346938775, "no_speech_prob": 0.003071732819080353}, {"id": 1019, "seek": 584384, "start": 5862.64, "end": 5869.68, "text": " there is no accepted guidance on how these tools should be used right so if I'm a large company", "tokens": [51304, 456, 307, 572, 9035, 10056, 322, 577, 613, 3873, 820, 312, 1143, 558, 370, 498, 286, 478, 257, 2416, 2237, 51656], "temperature": 0.0, "avg_logprob": -0.05550948153720813, "compression_ratio": 1.5918367346938775, "no_speech_prob": 0.003071732819080353}, {"id": 1020, "seek": 586968, "start": 5869.68, "end": 5875.6, "text": " and I'm building an operating model around how to implement fairness techniques just having the", "tokens": [50364, 293, 286, 478, 2390, 364, 7447, 2316, 926, 577, 281, 4445, 29765, 7512, 445, 1419, 264, 50660], "temperature": 0.0, "avg_logprob": -0.047739515026796214, "compression_ratio": 1.8044280442804428, "no_speech_prob": 0.014935149811208248}, {"id": 1021, "seek": 586968, "start": 5875.6, "end": 5881.12, "text": " technology is irrelevant it's about the people and the process and the the kind of operating", "tokens": [50660, 2899, 307, 28682, 309, 311, 466, 264, 561, 293, 264, 1399, 293, 264, 264, 733, 295, 7447, 50936], "temperature": 0.0, "avg_logprob": -0.047739515026796214, "compression_ratio": 1.8044280442804428, "no_speech_prob": 0.014935149811208248}, {"id": 1022, "seek": 586968, "start": 5881.12, "end": 5887.280000000001, "text": " model of how we implement it and there is basically no useful information out there to help us do that", "tokens": [50936, 2316, 295, 577, 321, 4445, 309, 293, 456, 307, 1936, 572, 4420, 1589, 484, 456, 281, 854, 505, 360, 300, 51244], "temperature": 0.0, "avg_logprob": -0.047739515026796214, "compression_ratio": 1.8044280442804428, "no_speech_prob": 0.014935149811208248}, {"id": 1023, "seek": 586968, "start": 5887.280000000001, "end": 5892.96, "text": " the other thing is we spoke about this becoming an engineering discipline which is to say what if we", "tokens": [51244, 264, 661, 551, 307, 321, 7179, 466, 341, 5617, 364, 7043, 13635, 597, 307, 281, 584, 437, 498, 321, 51528], "temperature": 0.0, "avg_logprob": -0.047739515026796214, "compression_ratio": 1.8044280442804428, "no_speech_prob": 0.014935149811208248}, {"id": 1024, "seek": 586968, "start": 5892.96, "end": 5898.72, "text": " could create an interface to abstract away some of the vagaries and esoteric of interpretability", "tokens": [51528, 727, 1884, 364, 9226, 281, 12649, 1314, 512, 295, 264, 13501, 4889, 293, 785, 21585, 299, 295, 7302, 2310, 51816], "temperature": 0.0, "avg_logprob": -0.047739515026796214, "compression_ratio": 1.8044280442804428, "no_speech_prob": 0.014935149811208248}, {"id": 1025, "seek": 589872, "start": 5898.72, "end": 5904.400000000001, "text": " methods we might come up with some primitives or some common language and then we can hide the", "tokens": [50364, 7150, 321, 1062, 808, 493, 365, 512, 2886, 38970, 420, 512, 2689, 2856, 293, 550, 321, 393, 6479, 264, 50648], "temperature": 0.0, "avg_logprob": -0.07450737194581465, "compression_ratio": 1.7454545454545454, "no_speech_prob": 0.021857770159840584}, {"id": 1026, "seek": 589872, "start": 5904.400000000001, "end": 5909.4400000000005, "text": " complexity behind the interface this is kind of what we do with mo dev ops already we automate", "tokens": [50648, 14024, 2261, 264, 9226, 341, 307, 733, 295, 437, 321, 360, 365, 705, 1905, 44663, 1217, 321, 31605, 50900], "temperature": 0.0, "avg_logprob": -0.07450737194581465, "compression_ratio": 1.7454545454545454, "no_speech_prob": 0.021857770159840584}, {"id": 1027, "seek": 589872, "start": 5909.4400000000005, "end": 5914.96, "text": " as much as we can then we templatize and remove friction out of the process we even create building", "tokens": [50900, 382, 709, 382, 321, 393, 550, 321, 9100, 267, 1125, 293, 4159, 17710, 484, 295, 264, 1399, 321, 754, 1884, 2390, 51176], "temperature": 0.0, "avg_logprob": -0.07450737194581465, "compression_ratio": 1.7454545454545454, "no_speech_prob": 0.021857770159840584}, {"id": 1028, "seek": 589872, "start": 5914.96, "end": 5922.64, "text": " blocks using domain specific languages or yaml files and pipelines and and so on so what we do", "tokens": [51176, 8474, 1228, 9274, 2685, 8650, 420, 288, 335, 75, 7098, 293, 40168, 293, 293, 370, 322, 370, 437, 321, 360, 51560], "temperature": 0.0, "avg_logprob": -0.07450737194581465, "compression_ratio": 1.7454545454545454, "no_speech_prob": 0.021857770159840584}, {"id": 1029, "seek": 592264, "start": 5922.64, "end": 5928.8, "text": " is is we create a level of abstraction where people can compose together pipelines remember", "tokens": [50364, 307, 307, 321, 1884, 257, 1496, 295, 37765, 689, 561, 393, 35925, 1214, 40168, 1604, 50672], "temperature": 0.0, "avg_logprob": -0.08186359290617058, "compression_ratio": 1.6470588235294117, "no_speech_prob": 0.08836392313241959}, {"id": 1030, "seek": 592264, "start": 5928.8, "end": 5933.6, "text": " when Conor made the comment that this might just become a box ticking exercise and this is something", "tokens": [50672, 562, 2656, 284, 1027, 264, 2871, 300, 341, 1062, 445, 1813, 257, 2424, 33999, 5380, 293, 341, 307, 746, 50912], "temperature": 0.0, "avg_logprob": -0.08186359290617058, "compression_ratio": 1.6470588235294117, "no_speech_prob": 0.08836392313241959}, {"id": 1031, "seek": 592264, "start": 5933.6, "end": 5940.64, "text": " we see in security and AI ethics already we can't really trust people to self report that the model", "tokens": [50912, 321, 536, 294, 3825, 293, 7318, 19769, 1217, 321, 393, 380, 534, 3361, 561, 281, 2698, 2275, 300, 264, 2316, 51264], "temperature": 0.0, "avg_logprob": -0.08186359290617058, "compression_ratio": 1.6470588235294117, "no_speech_prob": 0.08836392313241959}, {"id": 1032, "seek": 592264, "start": 5940.64, "end": 5947.04, "text": " is behaving correctly or that the project has no concerns from an AI ethics point of view the whole", "tokens": [51264, 307, 35263, 8944, 420, 300, 264, 1716, 575, 572, 7389, 490, 364, 7318, 19769, 935, 295, 1910, 264, 1379, 51584], "temperature": 0.0, "avg_logprob": -0.08186359290617058, "compression_ratio": 1.6470588235294117, "no_speech_prob": 0.08836392313241959}, {"id": 1033, "seek": 594704, "start": 5947.04, "end": 5952.96, "text": " point here is process if we want to create an operating model and ensure best practices are", "tokens": [50364, 935, 510, 307, 1399, 498, 321, 528, 281, 1884, 364, 7447, 2316, 293, 5586, 1151, 7525, 366, 50660], "temperature": 0.0, "avg_logprob": -0.06588397527995862, "compression_ratio": 1.7110091743119267, "no_speech_prob": 0.08816950768232346}, {"id": 1034, "seek": 594704, "start": 5952.96, "end": 5959.36, "text": " followed or any kind of standardization in a large organization we have to design a process", "tokens": [50660, 6263, 420, 604, 733, 295, 3832, 2144, 294, 257, 2416, 4475, 321, 362, 281, 1715, 257, 1399, 50980], "temperature": 0.0, "avg_logprob": -0.06588397527995862, "compression_ratio": 1.7110091743119267, "no_speech_prob": 0.08816950768232346}, {"id": 1035, "seek": 594704, "start": 5959.36, "end": 5967.76, "text": " and many eyes make shallow holes so the process would mandate that a certain number of stakeholders", "tokens": [50980, 293, 867, 2575, 652, 20488, 8118, 370, 264, 1399, 576, 23885, 300, 257, 1629, 1230, 295, 17779, 51400], "temperature": 0.0, "avg_logprob": -0.06588397527995862, "compression_ratio": 1.7110091743119267, "no_speech_prob": 0.08816950768232346}, {"id": 1036, "seek": 594704, "start": 5967.76, "end": 5974.32, "text": " were involved in assessing the particular iml technique and validating it essentially and", "tokens": [51400, 645, 3288, 294, 34348, 264, 1729, 566, 75, 6532, 293, 7363, 990, 309, 4476, 293, 51728], "temperature": 0.0, "avg_logprob": -0.06588397527995862, "compression_ratio": 1.7110091743119267, "no_speech_prob": 0.08816950768232346}, {"id": 1037, "seek": 597432, "start": 5974.32, "end": 5979.759999999999, "text": " then we would need to record that assessment so who said what when and then if the company ever", "tokens": [50364, 550, 321, 576, 643, 281, 2136, 300, 9687, 370, 567, 848, 437, 562, 293, 550, 498, 264, 2237, 1562, 50636], "temperature": 0.0, "avg_logprob": -0.03760914635239986, "compression_ratio": 1.8352059925093633, "no_speech_prob": 0.01386241428554058}, {"id": 1038, "seek": 597432, "start": 5979.759999999999, "end": 5984.96, "text": " became audited or if god forbid there was some kind of a problem where the iml model did something", "tokens": [50636, 3062, 2379, 1226, 420, 498, 3044, 34117, 456, 390, 512, 733, 295, 257, 1154, 689, 264, 566, 75, 2316, 630, 746, 50896], "temperature": 0.0, "avg_logprob": -0.03760914635239986, "compression_ratio": 1.8352059925093633, "no_speech_prob": 0.01386241428554058}, {"id": 1039, "seek": 597432, "start": 5984.96, "end": 5989.92, "text": " wrong and it caused the company lots of damage or it harmed the environment or society or something", "tokens": [50896, 2085, 293, 309, 7008, 264, 2237, 3195, 295, 4344, 420, 309, 41478, 264, 2823, 420, 4086, 420, 746, 51144], "temperature": 0.0, "avg_logprob": -0.03760914635239986, "compression_ratio": 1.8352059925093633, "no_speech_prob": 0.01386241428554058}, {"id": 1040, "seek": 597432, "start": 5989.92, "end": 5995.04, "text": " like that we would then be able to rewind the clock and say okay well joe blogs said it was okay", "tokens": [51144, 411, 300, 321, 576, 550, 312, 1075, 281, 41458, 264, 7830, 293, 584, 1392, 731, 1488, 68, 31038, 848, 309, 390, 1392, 51400], "temperature": 0.0, "avg_logprob": -0.03760914635239986, "compression_ratio": 1.8352059925093633, "no_speech_prob": 0.01386241428554058}, {"id": 1041, "seek": 597432, "start": 5995.04, "end": 6002.96, "text": " because of xyz so that is an operating model it's a process and how to design such a process again", "tokens": [51400, 570, 295, 2031, 37433, 370, 300, 307, 364, 7447, 2316, 309, 311, 257, 1399, 293, 577, 281, 1715, 1270, 257, 1399, 797, 51796], "temperature": 0.0, "avg_logprob": -0.03760914635239986, "compression_ratio": 1.8352059925093633, "no_speech_prob": 0.01386241428554058}, {"id": 1042, "seek": 600296, "start": 6002.96, "end": 6009.6, "text": " is completely absent speaking as a chief data scientist myself that's the kind of thing that", "tokens": [50364, 307, 2584, 25185, 4124, 382, 257, 9588, 1412, 12662, 2059, 300, 311, 264, 733, 295, 551, 300, 50696], "temperature": 0.0, "avg_logprob": -0.05029277434715858, "compression_ratio": 1.5642458100558658, "no_speech_prob": 0.09778044372797012}, {"id": 1043, "seek": 600296, "start": 6009.6, "end": 6014.56, "text": " i'm interested in and it's very difficult for me to do that i really hope you've enjoyed the", "tokens": [50696, 741, 478, 3102, 294, 293, 309, 311, 588, 2252, 337, 385, 281, 360, 300, 741, 534, 1454, 291, 600, 4626, 264, 50944], "temperature": 0.0, "avg_logprob": -0.05029277434715858, "compression_ratio": 1.5642458100558658, "no_speech_prob": 0.09778044372797012}, {"id": 1044, "seek": 600296, "start": 6014.56, "end": 6019.68, "text": " episode today we've had so much fun making it remember to like comment and subscribe and we'll", "tokens": [50944, 3500, 965, 321, 600, 632, 370, 709, 1019, 1455, 309, 1604, 281, 411, 2871, 293, 3022, 293, 321, 603, 51200], "temperature": 0.0, "avg_logprob": -0.05029277434715858, "compression_ratio": 1.5642458100558658, "no_speech_prob": 0.09778044372797012}, {"id": 1045, "seek": 601968, "start": 6019.68, "end": 6025.52, "text": " see you back next week", "tokens": [50364, 536, 291, 646, 958, 1243, 50656], "temperature": 0.0, "avg_logprob": -0.5019454956054688, "compression_ratio": 0.7333333333333333, "no_speech_prob": 0.051375504583120346}], "language": "en"}