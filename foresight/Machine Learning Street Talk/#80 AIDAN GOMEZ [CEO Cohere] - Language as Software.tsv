start	end	text
0	7440	Aiden Gomez is a computer scientist and widely recognized AI expert who co founded AI company co here before that.
7440	14480	He worked as an intern in the Google brain team in Toronto, alongside Jeffrey Hinton, one of the Godfathers of deep learning.
14480	17880	Actually, the only Godfather who hasn't been on MLST.
18000	19960	Aiden, you need to put a good word in for us.
21360	23360	I will bring it up.
24280	28880	So Gomez was the kind of person Hinton recalled who had so many ideas.
29160	33920	It was difficult to pin him down and to get him to focus on what he was actually supposed to be doing.
34520	37760	Now Gomez was particularly interested in learning to translate languages.
38120	50360	And while he was at it, he casually invented transformers, which, as we all know, have become the de facto basic reference architecture for all neural networks and not only for language tasks.
51120	58200	Now co here is a startup which uses artificial intelligence to help users build the next generation of language based applications.
58640	65480	It's headquartered in Toronto and the company has raised $175 million in funding so far.
65880	75920	And the first round was from index ventures and the round also included Hinton, who we just spoke about, Fei-Fei Li and Peter Abial, who are very, very famous folks in the ML space.
76680	88160	Now co here say that their competitive advantage or at least at least one of their competitive advantages lies in its focus on safety, which is crucial for customers deploying models which could potentially out for something harmful.
88520	90600	And we'll discuss what we mean by that a bit later on.
91240	97080	Now, I can honestly say personally that language models have transformed how I use computers over the last six months or so.
97080	113760	I've been using them on a daily basis in the form of scripts, which helped me use the command line better using an interactive REPL playground for doing all sorts of stuff like composing emails for me, understanding error logs when I'm coding, summarizing information, performing repetitive tasks.
113760	114320	Do you name it?
114600	120640	Actually, I regularly discover new tasks that I can use for language models just through kind of creative exploration.
120640	125440	So I'm a bit of a convert, although caveat M tour is still definitely in effect.
125760	129920	The words of Chomsky, Fodor and Felician are still ringing loudly in my ears.
129920	132040	We just did some content about that on our last show.
132040	133880	But anyway, Aidan, it's an absolute honor.
133880	135120	Welcome to MLST.
135400	139440	And what's it like being the CEO of one of the fastest growing startups?
141760	142920	Thank you so much for having me.
143160	145800	I'm stoked to get to meet you and chat about Coheir.
147200	149520	What's it like being the CEO of Coheir?
149520	155440	It's a privilege and a thrilling ride.
155560	159960	Like you're just hanging on for dear life as things are going and trying to keep up.
161000	167400	But it's honestly just such a privilege to get to work with the people that I work with on the problem that we work on.
167800	172640	I think I'm just so, so lucky and fortunate.
173040	178000	I'm really, really excited to get into the discussion about transformers and attention is all you need.
178000	184960	So I mean, I remember when this came out in about 2017, actually, it changed the landscape of deep learning forever.
185320	187840	It took me a very long time to understand it.
187840	192960	And I probably only did understand it after reading Jay's famous blog post, you know, The Illustrator Transformer.
193200	195600	Jay, of course, is he your engineering director now?
196480	197200	Yeah, yeah.
197200	202680	So Jay is the best communicator in machine learning that I know.
202840	203680	He's incredible.
204960	212120	And I think if you search, I think if you search Transformer NeuralNet on Google, it's not our paper that comes up.
212120	213800	It's Jay's blog.
214240	218760	So it tells you like how much better his communication is than ours.
219640	220640	It was an epic blog.
220640	224480	He's actually done a new blog post on stable diffusion as well.
224880	227760	And folks should also subscribe to his YouTube channel.
227760	229000	He's got an amazing YouTube channel.
229000	238720	But anyway, there was Jay's blog post and Janik's video that made me finally understand it because at the time it was the most exciting kind of technology.
239080	246800	And I remember I was interviewing for the Bing team at the time and I spent a long time studying the paper just so that I could be conversant in it when I had that interview.
246800	252160	But anyway, could you tell us, you know, what was the story behind the paper and also what's special about transformers?
252920	271480	Yeah, so the story behind the paper, I should say that, like, I was the intern on this project and so I was the baby, like, doe-eyed, showed up in Google Brain down in Mountain View and my contribution was really on the software.
271480	299120	So there were existing threads inside of Google Brain, primarily driven by Noam Shazir, Jakob Uskarite, Lukash Ashish, which were pushing along this idea of deploying these autoregressive sequence models for text because they had been really popularized in WaveNet for audio and language generation.
301600	312640	But there was like a big push towards how do we deploy these against text and how do we incorporate attention, which was something that was groundbreaking for RNNs, like the previous generation of model.
313040	316960	They wanted to incorporate this into these new autoregressive sequence models.
317640	331280	And so when I came in, I was working with Lukash focused on, like, the software side of things, scalable training frameworks, supporting training that, you know, could be distributed across not just tens of
331280	335000	machines or hundreds, but thousands of accelerators.
337000	343040	And so we built Tensor2Tensor, which was the framework that was used to develop the transformer.
345240	358040	Shortly after we started putting that together, I was sitting next to Noam Shazir and we heard that he was also kind of thinking along similar threads for these autoregressive models.
358040	363600	And so Lukash and I convinced him to come over to Tensor2Tensor and start doing it on our framework.
364600	367160	And then we heard over in the translate department.
367160	369200	So Brain was one division of Google.
369560	370840	Translate was a separate one.
371440	387680	Over in translate, there was Yakka, Bashish, Nikki, working on a similar project to Noam where they wanted to create a purely attentive model, strip back all the complexity of RNNs, all of these, like, very complex,
387680	397640	gates and states and et cetera, and just rip that all away and just have MLPs and attention layers.
400200	401360	And so we all came together.
401360	405280	We all consolidated on this framework called Tensor2Tensor.
405880	417640	And the next three months, up until the NERP deadline, was just a sprint, like sleeping in the office, just going as fast as you can, running experiments.
417680	420640	And iteration and iteration and finding this bug and that bug.
422840	427960	And so really like a huge piece of the project came together within 12 weeks.
430440	434800	So it was like an extraordinary pace.
437240	444320	And it was, as an intern, this was like my first experience in like proper research.
444800	446080	And I just thought this was normal.
446080	448720	I thought, OK, this is what everyone does.
448720	450880	We all just crank out papers in three months.
450880	452000	We sleep at the office.
454920	459680	And I didn't really have an appreciation for what we had accomplished at the end.
459720	464480	Like, I can't say that I was particularly prescient at the time.
464920	469920	I remember like the night before the NERP deadline, the night before we had to submit.
470760	485840	It was like whatever, 3 a.m. Ashish and I were sitting on a couch in the brain office and he turned to me and he he said, you know, like, Aiden, like, this is going to be huge.
486600	491760	And my reaction was like, we bumped up blue, like the blue score by one point.
492000	493040	I was like, really?
493040	493680	You think so?
493840	494400	Oh, cool.
494400	496600	Like, don't we all like, isn't this what research is?
496600	497640	We just bumped it up a point.
497800	498560	What's the big deal?
499400	512080	But I think what I didn't appreciate was the fact that such a simple method could achieve such insanely high performance.
512960	518560	Like, at that time, we were training on eight accelerators for one of these models and they could be trained in within a day.
521560	526720	But the architecture was so stripped down, it was so refined, it was so easy to scale and to grow.
527120	530080	I just, it was hard to see the future.
531280	535400	And it was hard to see what would happen, which was this massive scaling project.
535840	539520	And I think Ashish saw that, that happening.
541680	543080	So yeah, that was my contribution.
543080	546160	That was the part of the team that I brought.
548040	551320	But yeah, it was a very, very exciting few months.
551320	556920	And then it was an extremely exciting few years afterwards before I left Google and started cohere.
557280	558000	I can imagine.
558040	560840	And as you say, some of the best work happens in sprints.
560840	570920	And even though it only moved the needle a tiny bit on the blur score, something we'll explore later is sometimes it might actually be performing better than the metric might suggest.
571160	573240	And simple models are often better.
573280	575800	But I wanted to ask a bit of a technical question.
575800	580920	So DeepMind recently released a paper called Neural Networks and the Chomsky Hierarchy.
581680	586720	They grouped a bunch of tasks according to the formal language classes in the Chomsky Hierarchy.
587160	593920	And they showed that transformers were only able to represent finite languages, so not not even regular languages, which RNNs could do.
594640	600640	Neural networks could only support the higher modes, like context-free languages, if they were memory augmented with a stack.
601200	603880	And they said that this had implication for scaling laws.
604640	610320	So for example, the transformers architecture, they said, could never perfectly learn tasks which were higher up in the Hierarchy.
610640	618080	So we've done some shows on MLSD where we've spoken about neural networks not being too incomplete because they are finite state automators without the augmented memory.
618080	621360	So, for example, they couldn't approximate pi to the nth digit.
621720	626240	And there's this photo and collision connectionism critique paper, which we spoke about on the last show.
626280	633640	And that basically means that neural networks are not able to perform symbolic compositional reasoning or in plain English, they can't represent infinite objects.
634040	643880	And we spoke to Randall Belastriereo and he had this paper called The Spline Theory of Neural Networks, which showed that a neural network is a linear operation given a single input example.
643880	654840	So given all of this, I personally find the conversation about scaling laws, I'm a bit skeptical about it, although I'm genuinely interested in emergent properties or transients that happen during the scaling process.
654840	661760	But given all of this, I mean, how do you think about the practical limitations of transformers in terms of what algorithms they can learn?
663720	668200	Yeah, I really hope the transformers aren't the last architecture.
670400	676480	I would be extremely disappointed if this is as creative and high performing as we can get.
676840	682480	I think that transformers took off because of their scaling properties and also because of a network effect.
683040	691640	The community consolidated around this one architecture, we started to build all of this infrastructure, specifically for transformers.
692080	695080	There was a network effect and it had very nice scaling properties.
695080	702360	And so the community really came together around this architecture and built up infrastructure to support its adoption.
704080	708080	I think that's what's led to their proliferation or their success.
710720	716440	I hope that it's not the last architecture that would be super, super disappointing and boring.
717160	722160	You point out that they're not Turing complete.
722160	730880	I should clarify that I'm not a linguist, I'm not too familiar with Trump's hierarchies or the implications of the deep mind paper.
730880	734920	But one thing that's interesting about it, when I read it,
737480	744440	they're not speaking in theoretical terms or speaking in empirical terms of what functions are achievable.
745440	748440	From a normal initialization.
751440	753440	I think that's a fascinating lens.
753440	760440	In theory, a transformer is a universal approximator and it might be even Turing complete.
760440	766440	But in practice, if you can't explore all permutations of parameters,
766440	772440	it's very true that it'll find the simplest function that satisfies the task.
775440	780440	I think that's like a good guiding lens when thinking about what architectures come next.
780440	782440	Where do we go from here?
782440	792440	What are the sorts of components we need to add into neural networks to support them in representing these more complex functions?
795440	802440	So I do think that transformers are limited and I really hope they're not our final architecture.
802440	810440	I hope that we come up with something that's significantly better and I see promising efforts along those directions.
810440	818440	I think that retro from deep mind, like augmenting transformers with a searchable memory, I think that's a huge step forward.
818440	824440	The next thing we need to support is the ability for these transformers to keep state over long time horizons,
824440	831440	to be able to write into their own memory in order to make notes about what they've seen in the past.
833440	838440	And so I think there's a lot of work to be done and it's not happening fast enough.
838440	845440	I think more people should pick up these research questions and look for new scalable ways of doing it.
845440	853440	Because to speak to the scaling hypothesis, the real bottleneck for feature adoption,
853440	859440	the real bottleneck for adding in a new component to the transformer architecture,
859440	867440	it's all scale and efficiency. People will just adopt the thing that is simplest, fastest and best performing.
867440	875440	And so we need to do the work in order to make these other components like addressable memory efficient and scalable
875440	880440	in the same way that the core vanilla transformer architecture is today.
880440	889440	Yeah, it's such a paradox, isn't it? Because the deep mind paper was saying that a Turing machine is the most powerful computational model,
889440	893440	but neural networks are trainable and they scale really, really well.
893440	898440	So I'm sure we'll find some innovation that will somehow bridge that gap somehow.
898440	900440	But I wanted to learn a little bit more about Cohere.
900440	905440	So could you tell us about your core product portfolio and what's on your roadmap?
905440	911440	Yeah, for sure. So the mission with Cohere is to really just give technology language.
911440	916440	And the way that we want to do that is put the tech, put these large language models into more hands.
916440	923440	Today, if you're building a product and you want to deploy a language model as part of that,
923440	930440	you've got to learn how to use some framework like PyTorch or Jax or TensorFlow.
930440	938440	You need to learn how to install CUDA kernels on a VM, which is actually a huge, huge task.
938440	943440	And so it's this very painful process, requires a lot of learning.
943440	949440	It's very unnatural and requires a ton of prerequisite knowledge.
949440	952440	So for Cohere, what we want to do is abstract that away.
952440	957440	We want to present an interface which is just intuitive, natural, like Cohere.classify.
957440	961440	You feed in the tweet that you want to classify.
961440	965440	You give some examples of tweets being classified into the categories that you care about,
965440	973440	and then you get a response which says, yeah, that tweet fits into sports or whatever.
973440	979440	So we want to create a portfolio of these endpoints, which just makes this technology more accessible.
979440	984440	And the goal being that it starts to proliferate.
984440	989440	Because I think the transformer was released half a decade ago.
989440	999440	And I saw extraordinary research-level results or contributions from it, right?
999440	1009440	Like the ability to write really compelling tags, the ability to few-shot prompt and get pretty good performance on a huge swath of problems.
1009440	1015440	But it just hasn't been changing the fabric of consumer applications.
1015440	1018440	And I'm a consumer. You're a consumer. We all use these apps.
1018440	1023440	And so as a researcher who's seen the potential of the technology, it's super frustrating.
1023440	1032440	You just wonder, like, what's stopping this from getting out there into apps faster?
1032440	1037440	And I think two of the reasons are there's this huge compute barrier, right?
1037440	1042440	When you train these big models, you need a supercomputer and tons of data.
1042440	1047440	That's very difficult to use and collect.
1047440	1049440	So the compute is definitely one of the big barriers.
1049440	1055440	But the second piece is that really it's like the people, right?
1055440	1060440	At the moment, yeah, we have millions of developers on our planet.
1060440	1066440	But a tiny, tiny, tiny fraction of them actually know how to do this specific thing, machine learning and training models.
1066440	1072440	So there's not a lot of people out there actually doing the work to integrate this into every product on Earth.
1072440	1080440	And so for us, like, what we want to do is just blow that open, put this stuff into the hands of every single developer.
1080440	1085440	It doesn't matter what your specialty is if you're a database developer, whatever you do, it doesn't matter.
1085440	1090440	The important thing is that now you can build with large language models because you're given an interface,
1090440	1095440	which doesn't require, you know, three years of study to get up to speed.
1095440	1098440	So that's really what we're pushing for.
1098440	1102440	It's quite ironic actually because I was involved in ML DevOps for many years,
1102440	1107440	and normally introducing machine learning into software engineering makes it exponentially harder.
1107440	1111440	But now we seem to have jumped to language models where it's become easier again.
1111440	1118440	And I want to talk about the language being a new type of interface for software, but we'll save that just for a minute.
1118440	1122440	I want to talk about the friction using large language models as a startup owner.
1122440	1127440	So I'm a startup founder myself, and I want to use large language models because I'm very excited about them.
1127440	1133440	And, you know, I can speak to some of the friction that I've been experiencing looking into this.
1133440	1142440	So, I mean, if we look at what's happening over OpenAI, for example, the Microsoft Signup form and the content policy over there is quite intimidating.
1142440	1145440	They've made it quite challenging to use in production.
1145440	1149440	And actually, it makes me wonder how many people are using it in production.
1149440	1153440	They say that content creation applications have a higher chance of misuse,
1153440	1158440	which seems like an oxymoron to me because GPT-3 is literally like a content creator.
1158440	1161440	They say you're going to expect a call from the Microsoft vetting service.
1161440	1164440	They can pull the rug from under you at any time for any reason.
1164440	1167440	They log and record everything under the guise of safety.
1167440	1169440	They can't be held liable for anything.
1169440	1171440	As I said, open-ended applications are refused.
1171440	1176440	And in my opinion, the most exciting applications are the open-ended applications, right?
1176440	1178440	I love the playful nature of it.
1178440	1184440	Inside my application, I want to build a community of tinkerers who discover interesting new sub-applications, right?
1184440	1189440	I want to build a marketplace of prefab prompt structures on top of my platform.
1189440	1194440	I don't want to specify exactly how my application will be used, right?
1194440	1196440	You know, I want my application to be fluid.
1196440	1203440	And large language models make the consumers of an application kind of like programmers of that platform themselves.
1203440	1205440	I think it's an entirely new paradigm.
1205440	1211440	So with that in mind and the friction, do you think that would ever be allowed on Cohir?
1211440	1213440	The startup that you're describing.
1213440	1219440	Well, this idea that I shouldn't need to say to Cohir exactly how I'm going to be using the platform.
1219440	1221440	I want it to be very open-ended and playful.
1221440	1227440	I want my users to create new prompts and share the prompts and use it in interesting ways.
1227440	1230440	I want it to be as open-ended as possible.
1230440	1240440	Yeah, I think on Cohir, that would be fine subject to your users complying with some basic ethical principles.
1240440	1249440	So presumably, I imagine that you don't want your users creating bots which propagate false information or hate speech on Twitter.
1249440	1256440	You don't want like a billion bot accounts responding to every article.
1256440	1262440	And so I assume that you're also incentivized to have some degree of terms of use.
1262440	1264440	Is that right?
1264440	1267440	Yeah, so I completely agree with your terms of use.
1267440	1269440	I think it's very good actually.
1269440	1277440	The one sticking point for me is that in order to have my application vetted, I have to say how it's being used.
1277440	1282440	And it seems to be slightly away from having just quite an open-ended application.
1282440	1286440	But I absolutely agree with all of the points in your content policy.
1286440	1287440	Okay, yeah.
1287440	1290440	So I think that startup should absolutely exist.
1290440	1291440	That sounds awesome.
1291440	1296440	Like a community of people sharing prompts, iterating with each other, figuring out stuff that works, doesn't work.
1296440	1300440	I think we saw a lot of that with mid-journey and stable diffusion, right?
1300440	1304440	Like just this collective effort to like, let's figure this thing out.
1304440	1307440	Let's discover new ways to use it.
1307440	1314440	That should 100% exist in the world and cohere would 100% support that.
1314440	1326440	I think at the same time, there are just application domains that we don't want people building, like, you know, the ones that I just described.
1326440	1336440	So as long as you're okay filtering those out and working with us to make sure that your product doesn't get used in those ways,
1336440	1337440	we're fully on board.
1337440	1338440	Like that should exist.
1338440	1340440	That sounds amazing.
1340440	1348440	And I think like more broadly, coherence is we really want to see a proliferation of this technology.
1348440	1352440	We want to see a million new startups born from it.
1352440	1361440	And so we view our users as like partners in bringing this to fruition and putting this tech in front of more users, more consumers, more businesses.
1361440	1364440	And so we're very collaborative.
1364440	1366440	Like we don't want to rug pull anyone.
1366440	1368440	We don't want any surprises.
1368440	1372440	So long as like our terms are met, we want to be a partner.
1372440	1374440	We want to help you build.
1374440	1376440	We want to like support you in the best way possible.
1376440	1381440	I think some of this paradox might be from a legal point of view because I agree with you.
1381440	1385440	You call it the playground and that's a great term for it.
1385440	1390440	I think that some of the most exciting applications of large language models haven't been discovered yet.
1390440	1396440	And they'll be discovered when you have a diverse community of people kind of sharing and trying interesting things.
1396440	1398440	But just from that legal point of view.
1398440	1408440	So we were a bit worried actually that some of the customers of our application might send us up malicious prompts and have our service terminated.
1408440	1417440	And the way that we've been thinking about working around it on open AI, it's not possible on Coho, I don't think, is kind of getting the users to sign up for the service directly.
1417440	1424440	And then just pasting their key inside our application so that they're responsible, they're getting themselves blocked if they do anything bad.
1424440	1427440	And legally that puts us in a much safer position.
1427440	1429440	But how do you feel about that?
1429440	1431440	I mean, that's a great technique.
1431440	1434440	I think it should be supported on Coho if it's not already.
1434440	1435440	I think it is.
1435440	1442440	We have a few people doing stuff like that, like a bring your own key type application.
1442440	1455440	Alternatively, Coho would want to work with you to help you moderate use to catch bad actors, to catch misuse, out of terms use, etc.
1455440	1457440	So we'll be very collaborative.
1457440	1458440	We'll help you do that.
1458440	1459440	We'll help you look at the data.
1459440	1461440	We'll help you find users who are misusing it.
1461440	1469440	We won't just blanket ban you because one of your users is trying to adversarily attack your business.
1469440	1480440	We're trying to build with you and so I think we'll be quite reasonable assuming that you're reasonable too and that you don't want that type of activity on your product.
1480440	1490440	So it's about supporting startup founders, helping them build their own tools to catch this sort of stuff and ban those users.
1490440	1498440	It's like a collaborative building approach as opposed to, yeah, you're just a user, either comply or get banned.
1499440	1506440	We're much more, I don't know, present, engaged.
1506440	1507440	That makes sense.
1507440	1516440	I mean, you can appreciate my fear that in a sense startup founders have lost their autonomy because it costs millions and millions of dollars.
1516440	1525440	You folks are hiring some of the most talented people in the world to do this stuff and we're building on top of that foundation, which at the moment it seems like a risk.
1525440	1527440	I appreciate that in spirit.
1527440	1531440	It shouldn't actually be a risk, but just talking high level.
1531440	1537440	So how would you distinguish your service from, let's say, GPT-3?
1537440	1547440	So I think the Open AI team and GPT-3, they did like a fantastic thing by opening that up and giving it to the world.
1547440	1554440	In terms of distinguishing ourselves from them, I think they've taken a very hands-off approach to this stuff.
1555440	1561440	And they put out endpoints and it's kind of like a good luck, have fun, go build.
1561440	1570440	With Cohere, we're trying to be more present and engaged and we're trying to tailor our roadmap towards the needs of users.
1570440	1578440	So, for instance, we're listening to users and seeing what they're signing up for, what they're asking from us.
1578440	1581440	One of those things is summarization.
1581440	1590440	And so now we've spun up an effort to release a summarization endpoint that's generally useful across summarizing chat transcripts,
1590440	1593440	like long documents, that type of thing.
1593440	1595440	And so it's a two-way street.
1595440	1601440	It's not just that our users are the consumers of our path towards whatever we want.
1601440	1605440	It's like a dialogue and a conversation of what do you need?
1605440	1606440	What should we build next?
1606440	1608440	What do you see coming?
1608440	1610440	And then we go build it.
1610440	1617440	So it's very much a, it feels more two-way, community-oriented.
1617440	1622440	We're trying to build the right product for our users and the most useful product possible.
1622440	1630440	And so the way we do that is just through dialogue and conversation and people asking for the thing that they need, they want.
1630440	1634440	OpenAI service suddenly kind of got a lot better recently.
1634440	1637440	And I think they call it DaVinci 2.
1637440	1642440	It's a bit of a mystery because I reviewed GPT-3 when it first came out a couple of years ago.
1642440	1644440	And recently it seems much better.
1644440	1653440	And if I understand correctly, they've done some kind of fine-tuning using reinforcement learning to align it to human preferences that instruct GPT or something like that.
1653440	1655440	I don't even know if that's the case.
1655440	1661440	But I just wondered if you could comment on that and do you folks plan to do something similar?
1661440	1664440	Yeah, so we don't call them instruct models.
1664440	1668440	We call them command models because of the co and co here.
1668440	1673440	But we do have something currently in private beta.
1673440	1676440	Hopefully we'll release it soon.
1676440	1679440	But yeah, it has a huge impact on model performance.
1679440	1690440	Like the ability to specify an instruction, specify an intent, describe the type of problem that you're solving completely changes model performance.
1690440	1692440	And in some ways it's surprising.
1692440	1698440	In other ways, it's very not surprising in that these models are just trained on web scraped data.
1698440	1701440	Like there's no reason why you would expect them to behave the way that they do.
1701440	1707440	We're just kind of lucky that they work as few-shot programs.
1707440	1718440	And so I think this aligning with humans intent, human commands, human instructions,
1718440	1721440	it's just a much more natural way to interface with these models.
1721440	1730440	I think before instruct style models, it was a bit like you would have to discover the language of the model, right?
1730440	1735440	And it was like this very opaque process of shifting things around, rephrasing things,
1735440	1740440	trying to figure out what makes sense to this model.
1740440	1744440	Super brittle, super painful.
1744440	1749440	This command style model actually pulls that away and it's much more intuitive, it's much more fluid.
1749440	1752440	It's the way that you would expect to interact with the model.
1752440	1756440	Yeah, it's really interesting how, you know, when Steve Jobs released the iPad,
1756440	1759440	he said there was something magic about it, something of magic about that interface.
1759440	1766440	And similarly with these newer models, it feels like an invisible boundary is being crossed where I trust it.
1766440	1771440	In a way, it's a trap because I'm anthropomorphizing it more because of exactly what you just said before.
1771440	1775440	But I wanted to get into some engineering characteristics of code here.
1775440	1777440	So I'll send you some quick fire questions.
1777440	1780440	So how many tokens in a context window?
1780440	1785440	So at the moment, it's 2048 and it should flip to 4096 shortly.
1785440	1790440	But our goal is for an infinite token width.
1790440	1791440	Wow.
1791440	1795440	And so there's a few efforts that we're pursuing to enable that.
1795440	1796440	Yeah.
1796440	1800440	How many concurrent requests can your customers have, Perky?
1800440	1804440	Infinite, as many as you'd like.
1804440	1806440	Oh, even now?
1806440	1807440	Yes, yeah.
1807440	1814440	I think there's a top level bottleneck which we can actually remove for specific customers who need more.
1814440	1821440	I think it's like 10,000 queries per minute.
1821440	1827440	But we have folks doing billions and billions of characters a day.
1827440	1834440	And so, yeah, we're happy to remove that restriction for those applications that need it.
1834440	1842440	Do you support, let's say, enterprise security scenarios like single sign-on, key rotation, that kind of thing?
1842440	1845440	So we do SSO.
1845440	1846440	We don't do key rotation yet.
1846440	1854440	We're hoping to build that out like with other enterprise requirements like co-location and that sort of thing.
1854440	1859440	We're also building out the capability to do that on different clouds.
1859440	1864440	But at present, we don't have that yet, but it's roadmaped.
1864440	1865440	Okay.
1865440	1872440	And you were just touching on this before, but what's the kind of largest, most highly-scaled application deployed on Co-Hit?
1872440	1883440	So, similar to, I think, what everyone's been seeing, the first sort of application that has hit product market fit is copywriting.
1883440	1884440	Yeah.
1884440	1889440	There's a lot of companies, Jasper, Copy AI, HyperWrite.
1889440	1899440	They've really found something that works for the average person, drives tons of value, speeds people, makes them more efficient, makes them more creative.
1899440	1904440	And so that's where we're seeing volumes just skyrocket.
1904440	1909440	So I'm really excited about the prospect of stacking calls to these large language models.
1909440	1914440	So building a large computational graph of recursive calls.
1914440	1917440	But doing all the round trips at the moment is pretty slow.
1917440	1920440	It's almost like, I want to create this graph, this computation graph.
1920440	1922440	I want to ship it over to you.
1922440	1925440	You do it behind the scenes on your app fabric with parallelism.
1925440	1930440	You send me the results because right now there's a significant engineering challenge for me to do that.
1930440	1935440	But I think that the next generation of application platforms will be doing something like that.
1935440	1937440	There'll be a fabric on top of Co-Hit.
1937440	1943440	So are you planning anything to kind of make my life easier to do that?
1943440	1945440	That's so cool.
1945440	1947440	I hadn't actually thought about that.
1947440	1948440	Sorry.
1948440	1951440	I hadn't actually thought about that.
1951440	1959440	Like basically compiling a graph of chained prompts to Co-Hit and shipping it over.
1959440	1962440	That's fascinating.
1962440	1969440	I guess what you gain is the two-way network cost, right?
1969440	1971440	Like that's what you're saving.
1971440	1975440	And network costs tend to be quite small in practice.
1975440	1980440	So depending on how big your prompt is and how many tokens you're generating,
1980440	1986440	I could see it not giving you a huge amount of lift.
1986440	1994440	But yeah, I'd love to look at your use case and kind of understand
1994440	2000440	is the issue actually with the fact that you're having to make multiple network requests in sequence?
2000440	2003440	Or is it that the underlying model itself is too small?
2003440	2008440	And what you need is actually just speed-ups to that core model.
2008440	2014440	Another thing that Co-Hit does is we'll do specific deployments for customers.
2014440	2017440	So some folks have very low latency requirements.
2017440	2023440	And we can split up our model across more nodes, more GPUs.
2023440	2027440	And that makes the latency go way, way down.
2027440	2032440	So if you were looking for like a 6x speed-up, a 4x speed-up in latency,
2032440	2036440	that's one easy way to do it.
2036440	2039440	Yeah, I think we're only just scratching the surface of what's possible here.
2039440	2045440	So if I did this, what I just spoke about, I would need to create a workflow engine.
2045440	2050440	I would need to do all sorts of DAG optimization and parallelism.
2050440	2054440	And I might be able to cache certain steps.
2054440	2058440	And certain steps I might be able to do some metamachine learning to optimize the performance.
2058440	2062440	That needs a large one. That needs a small one.
2062440	2066440	There's a whole kind of universe I think that we can explore here,
2066440	2069440	if you know, just going one level of abstraction higher.
2069440	2072440	You know, because the next startup founders will be building platforms
2072440	2079440	that essentially compose human knowledge on top of these large language models in some kind of a fabric.
2079440	2083440	So I think that's what excites me at the moment.
2083440	2085440	That's really cool. That's really cool.
2085440	2091440	Okay, so you express this graph of like prompts chaining into each other.
2091440	2096440	You might have loops and conditions and you compile that,
2096440	2100440	you ship it to the large language model provider, they perform optimizations,
2100440	2103440	they handle all the parallelism, all the conditions.
2103440	2106440	That's a really fascinating product idea. I like that.
2106440	2109440	Yeah, please build it so I don't have to.
2109440	2113440	Anyway, I wanted to come on to some more LLM discussion.
2113440	2119440	So our friend, Francois Chouelet, he says that large language models are a bit like databases, right?
2119440	2123440	But to me, they seem like so much more than that, right?
2123440	2126440	So I think Francois was saying they are the representation, you know,
2126440	2132440	equivalent to the B tree structure in a database, but I think they're also the database engine as well.
2132440	2134440	But it's a new type of database engine.
2134440	2137440	We don't understand how this database engine works.
2137440	2138440	It's kind of unintelligible to us.
2138440	2147440	But do you think that's a good analogy or like how do you mentally kind of think about what's going on in a language model?
2147440	2152440	Yeah, I like the analogy. I like yours a bit better.
2152440	2165440	I think my own, I kind of view just a raw large language model trained on the web as the next iteration of a search engine
2165440	2174440	instead of explicitly retrieving results and references out into the web.
2174440	2177440	So I like to use the analogy of like search engines.
2177440	2184440	And instead of like explicit hard references where you make a query, you get back a link out to some site.
2184440	2187440	A language model, it's kind of more like a dialogue.
2187440	2189440	You can extract information from it.
2189440	2193440	It has all of this knowledge that it's seen throughout the web.
2193440	2197440	It's like a soft version of a search engine.
2197440	2204440	And the mode of discovery is still language is just a much more natural interface.
2204440	2211440	It's like a conversation as opposed to what we type into Google, which is kind of its own language, right?
2211440	2213440	Like we don't query in the same way.
2213440	2218440	We would ask our teacher, our Calc teacher about a theorem.
2218440	2221440	We write in a very strange way.
2221440	2229440	I think language models are a much more natural modality to the corpus of human knowledge.
2229440	2233440	It's much more intuitive, natural, seamless.
2233440	2236440	The problem is they hallucinate a lot.
2236440	2238440	And so they'll fill in gaps.
2238440	2239440	They're compressors, right?
2239440	2245440	And so they see the web and they try to compress that down into their parameters and they lose little bits and pieces.
2245440	2253440	And then when they have to regurgitate it, reconstruct it, they'll just fill in a plausible answer inside those gaps.
2253440	2260440	And so the other reason I'm really excited about retrieval models is that they ground them more in reality.
2260440	2267440	You put less burden on the model's parameters to memorize every single fact that's out there.
2267440	2271440	And you instead just have them do the distillation process.
2271440	2277440	So the model makes a query out to this knowledge base, this database of true facts.
2277440	2279440	It pulls back some references.
2279440	2284440	And then the model is just asked, hey, given these references, answer my question.
2284440	2294440	And so it can actually make reference to true facts instead of having to hallucinate and imagine its own facts.
2294440	2300440	So yeah, my kind of like the analogy that I like to build off of is this is the next search engine.
2300440	2302440	It's the next interface to the internet.
2302440	2306440	It's the next interface to human knowledge.
2306440	2309440	It's funny you say that it might actually be the next search engine soon.
2309440	2313440	I think search engines are about to be revolutionized.
2313440	2317440	But yeah, I was going to ask you about the next frontier of large language models.
2317440	2319440	And we've already been speaking about some of it.
2319440	2328440	We've been talking about the integration of information retrieval, which will ground the responses, you know, based on actual things which exist in your operational systems.
2328440	2338440	We just spoke before about hierarchical compositions, having this exciting, you know, computational graph, maybe some different architecture types or interaction patterns as well.
2338440	2343440	I mean, you were just saying that there's this back and forth interaction, which I think is really interesting.
2343440	2351440	On Google at the moment, you just type in a search and you press search once, you're not iteratively refining reflexively, recursively.
2351440	2355440	Multimodality was another thing I thought of, you know, maybe having language in and text out.
2355440	2363440	But I mean, those are a few examples, but does anything come top of mind to you for the next frontier?
2363440	2372440	Yeah, like in addition to what you just listed, I think the ability to keep state over a long term horizon, like my dream for Coheir
2372440	2376440	is that it knows its users.
2376440	2380440	It can see into the past of all the past interactions with each one of these users.
2380440	2382440	It knows their preferences.
2382440	2387440	It knows about the user and the way that they like to interact with this model.
2387440	2391440	And to do that, it can't be transactional.
2391440	2400440	It can't just be like text in, text out, you know, forget everything else.
2400440	2405440	In the history, we need to be able to like maintain a state between the model and its user.
2405440	2408440	We need to be able to keep a record of that.
2408440	2417440	I was talking about with retro, the ability to add into that corpus, keep notes, kind of like maintain an internal dialogue.
2417440	2429440	We've seen how useful that is with like the scratch pad techniques that people have been working on, just giving the model space to write out its thoughts before giving an answer.
2429440	2436440	I think we need the ability to store that forever and to have that referenceable down the line in the future.
2436440	2439440	So, yeah, I think that's one of the most exciting ones.
2439440	2444440	And then of course, there's multi-modality, which is blowing up now.
2444440	2447440	Again, it's like another form of grounding.
2447440	2455440	It ties the model into the real world, into the physical reality of our world.
2455440	2457440	I think that's super exciting.
2457440	2463440	So, yeah, I think the ability to reference external knowledge bases contribute to its own knowledge base.
2463440	2469440	The ability to understand the world not just through text, but through audio, video, images.
2469440	2476440	And then lastly, augmenting large language models with the ability to use tools.
2476440	2481440	Obviously, like, we built this world for ourselves.
2481440	2484440	We built all these little things to integrate with us.
2484440	2487440	And one of our primary modalities is language.
2487440	2491440	And so a lot of our tools are language-driven.
2491440	2497440	If you think about web browsing, it's mostly you reading text and clicking links and following.
2497440	2499440	It's very text-driven.
2499440	2508440	And so something I'm super excited about is, okay, well, we have these language models which have pretty good grasp of language.
2508440	2512440	They seem to have some modest level of understanding.
2512440	2519440	Can we actually get them to use the tools that we built for us humans, stuff like web browsers?
2519440	2524440	And the results we're seeing there are just super exciting, super exciting.
2524440	2525440	Cool.
2525440	2529440	I wanted to ask you about different modes of understanding.
2529440	2533440	I'm actually making a video at the moment on the Chinese rim argument.
2534440	2538440	They say that shortcut learning is an often discussed characteristic of machine learning.
2538440	2543440	So when a system achieves human-like performance on a benchmark by the slight of hand,
2543440	2550440	if you like, of spurious correlations in the data, rather than what we intuit to be human-like comprehension.
2550440	2555440	Now, large language models have learned this very intricate statistical correlation,
2555440	2559440	which allows near-perfect performance in lieu of human cognition.
2559440	2566440	And it's possible that evaluating these algorithms against standards designed to gauge human cognition might be barking up the wrong tree.
2566440	2571440	Now, human comprehension is difficult to pin down exactly what it means,
2571440	2577440	but it doesn't seem to be entirely reflected in large language models, not always in how they behave at least.
2577440	2583440	Now, for humans, it's not always enough to know the statistical features of linguistic symbols.
2583440	2589440	We also need to grasp the underlying ideas and contexts in which those symbols convey,
2589440	2594440	while language models can pick up on these tiny statistical patterns that we never could.
2594440	2602440	Melanie Mitchell said in her most recent paper that recent years in AI have produced machines with new modes of knowledge.
2602440	2606440	So in the same way that various animals are better suited to different settings,
2606440	2612440	so too will our intelligent systems be, you know, more adapted to various issues.
2612440	2619440	She said that large-scale statistical models will continue to be favored for matters requiring these vast amounts of historically encoded knowledge
2619440	2626440	and where performance is paramount, and human intelligence might be favored for problems where we have very limited knowledge
2626440	2628440	and we have strong causal mechanisms.
2628440	2633440	So, you know, my question to you basically is what is the difference between a machine learning algorithm
2633440	2639440	which has learned these very intricate statistical correlations and a human which has developed a more lifelike comprehension?
2639440	2645440	I think that the first and shortest answer is probably the objective function, right?
2645440	2651440	Like the context that we evolved in is very, very different than the context that we're training these models in.
2651440	2655440	Certainly, like humans have a model of language.
2655440	2659440	We're doing language modeling for sure, but we have to do a lot on top of that.
2659440	2669440	And that's just one component that we rely on in order to achieve our objective and go through life and procreate.
2669440	2674440	I think for these language models, they're given just that one piece of it.
2674440	2679440	And there's a lot within that one piece, just within language modeling, like you're forced to learn a ton of facts,
2679440	2682440	you're forced to learn a ton of patterns.
2683440	2693440	And so, it is quite extraordinary that all of this amazing behavior falls out of just the language modeling problem.
2693440	2695440	But it's also not, right?
2695440	2701440	Because in order to accurately model language in a generalizable way, you're forced to pick up stuff like,
2701440	2704440	how do I translate? How do I classify stuff?
2704440	2707440	Because that's represented in the data.
2707440	2712440	But when you're asking questions about how come language models get this thing wrong,
2712440	2717440	which humans find super intuitive or it's so obvious, so logical,
2717440	2719440	the contexts are completely different, right?
2719440	2721440	We didn't evolve to just be a language model.
2721440	2728440	I think that anyone who's claiming that has a high burden of proof.
2729440	2737440	And it also points back to what we were saying earlier around, is this the last architecture?
2737440	2740440	Is it just this plus scale?
2740440	2745440	There's more that needs to be there and also in the objective function, right?
2745440	2748440	We can't just do language modeling all the way.
2748440	2751440	I think language modeling will get us very far.
2751440	2757440	And I think that we can take language models and augment them with these very useful additional components.
2757440	2761440	We can give them access to tools.
2761440	2769440	But fundamentally, if you're looking for something that's human-like and acts and behaves like us,
2769440	2774440	there's an objective function change that needs to happen.
2774440	2776440	It can't just be language modeling.
2776440	2778440	There needs to be something more.
2778440	2782440	Yeah, we all anthropomorphize AI to different degrees, I think.
2782440	2787440	I mean, what I took from Melanie is that she thinks that language models are not anthropomorphic,
2787440	2789440	that it's a completely different mode.
2789440	2795440	But do you think that humans do think like language models and we have some other apparatus on the top,
2795440	2799440	or do you kind of agree with Melanie that they're completely different?
2799440	2801440	I don't think they're completely different.
2801440	2808440	I would not agree that they're categorically different.
2808440	2813440	I think they're categorically our brains and modeling and statistical models.
2813440	2819440	I think that they overlap very, very heavily.
2819440	2823440	Again, I think the objective function is just completely different.
2823440	2829440	I'm a big believer that the real value of AI models exists as a kind of entangled form of interactive,
2829440	2833440	creative pairing between a human brain and a model.
2833440	2837440	So David Chalmers coined the term extended mind to talk about us and phones.
2837440	2842440	And I think actually large language models, that's the real extended mind
2842440	2845440	because I feel this when I play with large language models.
2845440	2851440	I feel that it's truly intelligent in combination or in tandem with me.
2851440	2856440	So large language models, I don't have agency or intentionality, but we do,
2856440	2860440	and we can use them and something very interesting emerges from that.
2860440	2864440	As an example, I want to build a virtual assistant that comes with me everywhere.
2864440	2869440	It's part of my everyday experience and I'm just in tandem with a large language model
2869440	2876440	producing all sorts of creative thoughts that are embodied in the environment I'm in.
2876440	2880440	That, I think, is an extension of my intelligence.
2880440	2883440	Yeah, I 100% agree.
2883440	2892440	I'm thinking about what happened with search and the outsourcing of knowledge.
2892440	2893440	Not even that long ago.
2893440	2895440	Like pre-Google, right?
2895440	2897440	We had to memorize a lot of stuff.
2897440	2899440	It had to be digested.
2899440	2902440	Some stuff we could outsource into books and that type of thing
2902440	2909440	and we would retrieve them very slowly and very painfully as needed.
2909440	2912440	And then came the search engine and mobile phones
2912440	2915440	and at any moment, any piece of information that you need,
2915440	2920440	you have a source to go get it.
2920440	2924440	And so we took that taxing on our neurons,
2924440	2929440	taxing on our time, activity of having to pull a book off the shelf
2929440	2932440	and we turned it into like an instantaneous thing.
2932440	2940440	And so it freed up our minds, freed up our time to do a lot more high impact activities.
2940440	2945440	And we've been significantly more productive as a product.
2945440	2952440	And I think the next step is to continue to outsource things that we don't like doing,
2952440	2956440	things that are taxing, they're time consuming,
2956440	2962440	that we put off, that we don't like to hand more of that work over to the language model,
2962440	2967440	over to these ML systems, AI systems,
2967440	2971440	and to free ourselves up to just focus on the things that humans care about,
2972440	2975440	the things that we're best at.
2975440	2978440	So I think it's another complete transformation.
2978440	2983440	It just turns out that you can hand way, way more than just the knowledge over to a language model.
2983440	2986440	You can hand entire activities.
2986440	2989440	Yeah, that's what I'm excited about.
2989440	2992440	I like your idea of like a personal assistant coming around with you.
2992440	2994440	You can give it an instructor,
2994440	2998440	oh yeah, I need to buy body wash and it shows up at your door tomorrow.
2998440	2999440	I'm building it.
2999440	3000440	I'm building it.
3000440	3001440	You're building it.
3001440	3005440	I haven't done that topic for another day.
3005440	3007440	Quickfire question.
3007440	3011440	We spoke about language being the next interface for application development.
3011440	3014440	So I'm really excited about this possible future.
3014440	3015440	I'm already building it.
3015440	3022440	But there's also a bit of a panacea element to it because the sales pitch is that
3022440	3026440	even my community, my users could become software developers.
3026440	3028440	They could actually build my platform.
3028440	3034440	My platform becomes kind of like amorphous because it's so distributed.
3034440	3039440	But the worry is that we'll get into this situation where we might feel we need to reinvent code
3039440	3045440	because the prompts might become so complex that you need the equivalent of lawyers to understand them.
3045440	3048440	Do you think about that concern?
3048440	3055440	I have not because Coher's whole point is to abstract that away
3055440	3058440	and to try and make things as simple to use as possible
3058440	3065440	and to come up with canonical standards, abstractions even above prompting
3065440	3068440	that are easier to use so that it pushes out further.
3068440	3074440	It becomes less of a dark art or becomes less complex.
3074440	3078440	I think in order for this stuff to proliferate, that's like a necessary condition.
3078440	3079440	Things have to get easier.
3079440	3088440	It can't be like you're speaking to an alien or you're having to divine a prompt that happens to work.
3088440	3093440	So yeah, I think that's where the arrow of progress in this field points
3093440	3097440	is towards more abstraction, easier to use, more intuitive interfaces.
3097440	3103440	So I'm not too concerned about that because that's something that's very much top of mind.
3104440	3107440	Aiden Gomez, it's been an absolute honor. Thank you for joining us.
3107440	3108440	Likewise.
3108440	3110440	Likewise. Thank you so much, Tim.
