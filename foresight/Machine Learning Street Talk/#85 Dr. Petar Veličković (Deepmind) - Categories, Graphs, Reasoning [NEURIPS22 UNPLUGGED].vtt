WEBVTT

00:00.000 --> 00:06.080
Peter Velichkovich is a staff research scientist at DeepMind.

00:06.080 --> 00:11.040
He's firmly established himself as one of the most significant and up-and-coming researchers

00:11.040 --> 00:13.280
in the deep learning space.

00:13.280 --> 00:18.720
He invented graph attention networks in 2017 and he's been a leading light in the field

00:18.720 --> 00:24.800
ever since, pioneering research in graph neural networks, geometric deep learning and also

00:24.800 --> 00:26.800
neural algorithmic reasoning.

00:26.800 --> 00:31.640
Recently he's been applying category theory to take the geometric deep learning ideas

00:31.640 --> 00:33.400
one step further.

00:33.400 --> 00:36.760
If you haven't already, you should check out our show that we did on the geometric deep

00:36.760 --> 00:41.480
learning blueprint, which of course featured Peter and I caught up with him last week

00:41.480 --> 00:42.480
at NeurIPS.

00:42.480 --> 00:43.480
Enjoy.

00:43.480 --> 00:45.720
Peter, it's fantastic to see you again.

00:45.720 --> 00:48.600
So this is the first time that I've actually met you in person.

00:48.600 --> 00:52.680
We did that really cool show together on geometric deep learning with your proto book with Takko.

00:52.680 --> 00:58.400
I spoke with Takko yesterday, but also Michael Bronstein and Joanne Brunner.

00:58.400 --> 01:02.480
So anyway, it's been a little while since we've really synced.

01:02.480 --> 01:07.040
Now you've had this really, really interesting category theory series.

01:07.040 --> 01:10.160
Can you start by just letting us know what you've been doing there?

01:10.160 --> 01:15.640
Yeah, that's a great point and great to finally meet you in person, Tim.

01:15.640 --> 01:19.200
It's really great to catch up after some time has passed.

01:19.200 --> 01:25.440
And yeah, I mean, I like to think that all four of us, myself, Michael, Joanne and Takko

01:25.440 --> 01:29.280
have a greater understanding of the implications of these methods since the last time we spoke.

01:29.280 --> 01:33.520
If you remember back when we did our conversation, I kind of hinted at the fact that category

01:33.520 --> 01:39.280
theory might hold some of the answers to maybe generalize some of these geometric concepts

01:39.280 --> 01:42.560
beyond the notion of just pure symmetries.

01:42.560 --> 01:47.200
And we believe that now we have a sufficient understanding of these kinds of things that

01:47.280 --> 01:50.720
we were able to make this kind of mini course on categories for deep learning.

01:50.720 --> 01:55.680
And to me, it really feels like the natural continuation of these concepts of geometric

01:55.680 --> 01:58.040
deep learning into the realm beyond.

01:58.040 --> 02:00.720
And I'll explain that in a moment.

02:00.720 --> 02:05.720
But one other kind of very related point is that here at NeurIPS, we're actually presenting

02:05.720 --> 02:10.920
a full conference paper which deals with using category theoretic tools to invent new kinds

02:10.920 --> 02:11.920
of graph neural networks.

02:11.920 --> 02:15.920
So basically, it's not just that we're throwing a bunch of new theory, it actually leads to

02:15.920 --> 02:20.120
empirical findings that we can actionably use in our models day to day.

02:20.120 --> 02:21.560
So that's one point.

02:21.560 --> 02:22.560
That is incredible.

02:22.560 --> 02:24.040
Can you sketch out the paper?

02:24.040 --> 02:25.040
Yeah, sure.

02:25.040 --> 02:30.920
So basically, maybe I'll first take a step back to explain why do we think categories

02:30.920 --> 02:34.760
are important and in what sense they're kind of a step further from what geometric deep

02:34.760 --> 02:36.200
learning already gives us.

02:36.200 --> 02:40.840
So geometric deep learning concerns itself with giving us these equivalent layers, right?

02:40.840 --> 02:45.640
So layers that are in some sense resistant to operations of these symmetry transformations

02:45.640 --> 02:48.860
is that fundamentally change an object, but the object is still the same.

02:48.860 --> 02:50.400
We still have all of it, right?

02:50.400 --> 02:54.680
And this immediately implies that these symmetries have to be composable, invertible, all that

02:54.680 --> 02:56.200
sort of stuff.

02:56.200 --> 03:03.600
And yeah, essentially, the category theory framework is in some sense mindful of the

03:03.600 --> 03:08.560
fact that while symmetries are a very nice way to reason about things that happen and

03:08.560 --> 03:14.800
that we see in nature, they're often not completely an accurate representation of what happens.

03:14.800 --> 03:19.240
Very often there are operations both in nature, but especially in general computation, like

03:19.240 --> 03:24.080
say in algorithmic stuff, where an operation of an algorithm might destroy half of your

03:24.080 --> 03:25.080
data.

03:25.080 --> 03:26.080
So that is no longer a symmetry.

03:26.080 --> 03:27.080
You cannot invert it.

03:27.080 --> 03:31.360
But you might still be interested in building a neural network model that is in some sense

03:31.360 --> 03:37.640
resistant to the operations of say this algorithm or this natural phenomenon that you're studying.

03:37.640 --> 03:42.440
One simple example that maybe predates our work a little bit is building some kind of

03:42.560 --> 03:45.520
equivalence to scaling operations.

03:45.520 --> 03:49.480
Obviously if you scale or course in something, these are not always invertible transformations

03:49.480 --> 03:53.920
because if you course in the pixels of your image, you cannot perfectly reconstruct where

03:53.920 --> 03:54.920
you came from.

03:54.920 --> 03:58.280
Yet you still might want to build a model that will give you the same answer as regardless

03:58.280 --> 04:00.560
of how you scale up your input, right?

04:00.560 --> 04:04.360
So these are obviously things that are going to be very important as we move to more generic

04:04.360 --> 04:09.640
domains than ones that can be described purely through geometric and symmetry transformations,

04:09.640 --> 04:10.640
right?

04:10.640 --> 04:16.400
And in that sense, the same way we had groups, representations, and equivalence in geometric

04:16.400 --> 04:21.040
deep learning, these are all special cases of categorical concepts like categories,

04:21.040 --> 04:25.920
functors, and natural transformations, which basically generalize all the stuff of geometric

04:25.920 --> 04:28.000
deep learning into their own beyond.

04:28.000 --> 04:33.360
And in our paper, we try to use exactly these kinds of category theoretic tools to study

04:33.360 --> 04:38.240
what it would mean to build a say graph neural network that is capable of behaving like a

04:38.240 --> 04:40.480
classical computer science algorithm.

04:40.480 --> 04:44.920
In the sense that if you have some data that's transformed by an algorithm, you may imagine,

04:44.920 --> 04:49.080
say, a path finding algorithm where at every step in every node, you have your knowledge

04:49.080 --> 04:52.160
of how far away is that node from the source vertex.

04:52.160 --> 04:55.400
And one step of the algorithm kind of looks at all the immediate neighbors and updates

04:55.400 --> 04:57.360
those beliefs of how far away you are.

04:57.360 --> 05:00.720
And I'll say you want to have a GNN that simulates that, like we typically do in algorithmic

05:00.720 --> 05:01.720
reasoning.

05:01.720 --> 05:05.880
You take your algorithmic state, you encode it with a neural network into this high dimensional

05:05.880 --> 05:06.880
space.

05:06.880 --> 05:09.680
Your GNN then processes it to update the latent space.

05:09.680 --> 05:13.600
And now you want to be able to decode it so that you predict whatever the next state

05:13.600 --> 05:14.600
is going to be.

05:14.600 --> 05:19.080
So you have something which in category theory we use a lot is known as a commutative diagram.

05:19.080 --> 05:22.640
So basically it's saying you can either take the step of the algorithm or you can encode,

05:22.640 --> 05:25.360
process, decode, and hopefully end up in the same place.

05:25.360 --> 05:28.400
So category theory seems like a very nice language to study.

05:28.400 --> 05:34.320
These kinds of, I won't call them symmetries, they're basically like interchangeable sequences

05:34.320 --> 05:39.040
of operations because the step of an algorithm might not be invertible.

05:39.040 --> 05:43.240
You might not be able to go back after you do one step of, you know, shortest path algorithm

05:43.240 --> 05:45.280
because it's a contraction map, right?

05:45.280 --> 05:48.880
When you find the final solution of a shortest path algorithm, you won't necessarily know

05:48.880 --> 05:53.560
which previous state led you there because there could be many equivalent states that

05:53.560 --> 05:56.340
could lead you to the same contracted solution, right?

05:56.340 --> 06:01.280
So our method, using these category theory frameworks, try to characterize how these

06:01.280 --> 06:06.520
graph neural networks align with a target algorithm that we might want to simulate.

06:06.520 --> 06:10.520
Then we detect various ways not only to explain the code of graph neural networks from this

06:10.520 --> 06:14.360
kind of perspective, but also it gives us a very interesting sort of, if you've done

06:14.360 --> 06:18.760
any functional programming, a type checker of sorts to kind of detect whenever we're

06:18.760 --> 06:22.040
using our representations in slightly broken ways.

06:22.040 --> 06:27.160
So specifically to give you one very concrete example, in a categorical framework, just

06:27.160 --> 06:30.760
like in functional programming, you expect your transformations to be functions.

06:30.760 --> 06:33.840
That is, for every input there should be a unique output.

06:33.840 --> 06:37.600
However, one thing that people very often do in graph representation learning, when

06:37.600 --> 06:42.040
they want to predict outputs not only in the nodes, but also in the edges, is to reuse

06:42.040 --> 06:47.520
the edge messages both as edge outputs and integrated overall the other messages to get

06:47.520 --> 06:49.040
node outputs, right?

06:49.040 --> 06:53.000
But this is a problem from the categorical perspective because this is no longer a function.

06:53.000 --> 06:58.640
You cannot get a function that takes, you know, edges to edges plus nodes without sending

06:58.640 --> 07:01.760
the same thing into two different places in this case, right?

07:01.760 --> 07:05.200
And, you know, just because it mathematically breaks doesn't mean you cannot implement it.

07:05.200 --> 07:12.000
In fact, 99% of the GNN implementations you'll find online will do this exactly in this particular

07:12.000 --> 07:13.000
way.

07:13.000 --> 07:15.280
DeepMindsGraphNet's library does this, for example.

07:15.280 --> 07:18.800
However, you know, just because you can implement it doesn't mean that there's something not

07:18.800 --> 07:22.840
potentially a bit tricky going on in the sense that you're putting a bit of representational

07:22.840 --> 07:25.120
pressure on that edge message, right?

07:25.120 --> 07:29.560
Because now it has to be used for two potentially very different things, both for some output

07:29.560 --> 07:34.320
in the edges, but also it needs to be integratable into nodes where it predicts something potentially

07:34.320 --> 07:35.320
wildly different.

07:35.320 --> 07:39.360
And, you know, while gradient descent can take care of this and give you a model that

07:39.360 --> 07:43.520
fits your training distribution well, you're not like, to deal with this pressure, it's

07:43.520 --> 07:46.240
probably going to have to learn something which has nothing to do with the algorithm

07:46.240 --> 07:47.840
that you want to align to.

07:47.840 --> 07:51.440
And as a result, you're out of distribution, extrapolation performance is going to be much,

07:51.440 --> 07:52.440
much worse.

07:52.440 --> 07:55.120
And any self-respecting algorithm should extrapolate well.

07:55.120 --> 07:57.400
That's the main property of algorithmic reasoning, right?

07:57.400 --> 08:01.160
And we find that just by, you know, splitting this message function into two streams, one

08:01.160 --> 08:04.920
which goes into the edges and one which goes into the nodes, we get basically significant

08:04.920 --> 08:08.240
empirical benefits when extrapolating on edge-centric algorithms.

08:08.240 --> 08:09.240
Yeah.

08:09.240 --> 08:10.240
Amazing.

08:10.240 --> 08:13.520
So, Epeta has just produced this incredible series which is available on YouTube.

08:13.520 --> 08:14.720
Where can folks find it?

08:14.720 --> 08:15.720
Yeah.

08:15.720 --> 08:23.160
So, basically, if you just go to cats.4.ai, you can see all of the main series lectures

08:23.160 --> 08:30.560
from our course, which starts off with assuming kind of a foundational knowledge of deep learning

08:30.560 --> 08:35.600
with neural networks, back propagation, and so on, and then also tries to introduce these

08:35.600 --> 08:41.000
concepts of category theory and how we can use them to rethink the way we might go about

08:41.000 --> 08:45.480
some of our standard ideas in deep learning like compositionality or functional structure

08:45.480 --> 08:50.560
of deep learning pipelines, or even how can we reinterpret back propagation from the

08:50.560 --> 08:53.200
perspective of categorical theory.

08:53.200 --> 08:58.960
And each lecture basically deals with one particular aspect and we try to keep it grounded

08:58.960 --> 09:03.200
from the beginning to keep it motivated so every single lecture is aligning itself with

09:03.200 --> 09:07.840
one particular top-tier paper that one of us has published on one of these venues like

09:07.840 --> 09:09.080
in Europe.

09:09.080 --> 09:12.520
And one thing I'll also mention is that the course is actually, in principle, still ongoing

09:12.520 --> 09:18.520
because besides the main series of five lectures that myself, Bruno, Pym, and Andrew have given,

09:18.520 --> 09:24.160
we also have several interesting guest lectures where we try to bring in other influential

09:24.160 --> 09:30.120
people at the intersection of popularizing category theory with deep learning concepts

09:30.120 --> 09:35.640
in a way that can bring an even wider area of views once you're kind of trained in the

09:35.640 --> 09:39.480
basics of these techniques, how they're applied to various other things like causality.

09:39.480 --> 09:45.200
We had Taco Cohen tell us about how he uses these concepts to reimagine causality through

09:45.200 --> 09:46.760
a categorical lens.

09:47.720 --> 09:52.760
We're going to have Tydenay Bradley, she's a very popular mathematics educator generally.

09:52.760 --> 09:57.040
She will show how she used some of these concepts to explain transformers.

09:57.040 --> 10:01.280
And one thing I'm very excited about early next year, we will have actually a guest talk

10:01.280 --> 10:07.600
from David Spivak, which is one of the co-authors of the very famous Seven Sketches in Compositionality

10:07.600 --> 10:11.040
book, which is what initially one of the things that got me really excited about category

10:11.040 --> 10:12.880
theory in the first place.

10:12.880 --> 10:15.480
So I'm really keen to hear all these perspectives as well.

10:15.640 --> 10:16.520
The man is a legend.

10:16.520 --> 10:20.920
And also on Taco, I interviewed him yesterday and his work on causality is really, really

10:20.920 --> 10:22.120
exciting.

10:22.120 --> 10:26.560
What would you say to people who might be intimidated or scared by category theory?

10:26.560 --> 10:31.600
So one thing that I should mention here is that one point about being intimidated or

10:31.600 --> 10:36.360
scared about category theory is that to really be able to utilize these ideas in how you

10:36.360 --> 10:42.160
do research or build your models or anything, it does require a reasonably significant buy-in.

10:42.200 --> 10:46.360
So this is not something that you can just read one blog post and suddenly you're empowered

10:46.360 --> 10:47.360
to do it.

10:47.360 --> 10:48.920
This is like one key thing.

10:48.920 --> 10:53.440
But I would say the main thing that might make people a bit scared to do it is the fact

10:53.440 --> 10:59.520
that many category theory resources out there are a bit guided towards mathematicians.

10:59.520 --> 11:03.960
So they will tend to use the kind of language and the kind of examples that will be quite

11:03.960 --> 11:08.560
attractive to someone who has studied, say, various kinds of differential geometry or

11:08.560 --> 11:10.080
topology or something like this.

11:10.080 --> 11:14.520
And these kinds of areas tend to generally scare off people who come from a more computer

11:14.520 --> 11:16.520
science style background.

11:16.520 --> 11:21.280
And basically I would say the answer to that is you need to find the right resource for

11:21.280 --> 11:22.280
you.

11:22.280 --> 11:26.680
Category theory is no more or no less than a way to take a bird's eye view of the phenomena

11:26.680 --> 11:28.200
that you try to study.

11:28.200 --> 11:33.120
And when you study these phenomena from high in the sky, details become invisible, but

11:33.120 --> 11:36.960
you suddenly get a much better feel for the structure and you can utilize kind of the

11:36.960 --> 11:40.560
nice patterns that reappear across various fields.

11:40.560 --> 11:44.320
And this you would argue is kind of the essence of what we're trying to do in deep learning.

11:44.320 --> 11:48.080
We have a lot of analogical way in which these architectures are constructed, right?

11:48.080 --> 11:51.200
So cats for AI is one possible answer to that.

11:51.200 --> 11:55.520
It's our way to kind of, as half of us are deep learners and half of us are category

11:55.520 --> 11:58.480
theorists, trying to apply these techniques to deep learning.

11:58.480 --> 12:03.280
We believe we have a sort of unique perspective of we and like we understand what makes people

12:03.280 --> 12:07.480
afraid to try to talk about these things because some of us had to go through it ourselves

12:07.480 --> 12:11.760
to deal with the way in which the materials are arranged online right now.

12:11.760 --> 12:17.880
So yeah, maybe just these kinds of resources, starting with them and basically trying as

12:17.880 --> 12:23.440
much as possible not to descend into the depths of NCAT lab as the very first thing that you

12:23.440 --> 12:29.320
do can be a good way to maybe stay sane during the first few weeks or months of trying to

12:29.320 --> 12:30.320
explore this field.

12:30.320 --> 12:31.320
Wonderful.

12:31.320 --> 12:34.800
I wondered if you could give a couple of examples of where category theory has been used in

12:34.800 --> 12:35.800
an adjacent field.

12:35.800 --> 12:36.800
I can think of too.

12:36.800 --> 12:41.120
I can think of Rosen using category theory to describe, you know, sort of ecosystems

12:41.120 --> 12:42.120
and life.

12:42.120 --> 12:46.920
I can also think of some quantum mechanics folks that have come up with a category theoretical

12:46.920 --> 12:49.120
conception of quantum mechanics.

12:49.120 --> 12:50.120
Right.

12:50.120 --> 12:51.120
Are there any other ones?

12:51.120 --> 12:52.120
Yeah.

12:52.120 --> 12:56.480
So I mean, I can start by giving the examples that I know about closest in terms of just

12:56.480 --> 12:57.480
deep learning.

12:57.480 --> 13:03.560
So one particular example that I think could be quite interesting is the work that was

13:03.560 --> 13:07.000
published at NeurIPS two years ago, which I think is one of the first papers that really

13:07.000 --> 13:12.000
tried to use categorical concepts to build these structures, is the natural graph networks

13:12.000 --> 13:18.800
paper from Pimdehan, Tapocoin and Max Swelling, which effectively realizes the fact that the

13:18.800 --> 13:23.200
way we build graph neural networks very often we have this one shared message function that's

13:23.200 --> 13:27.760
applied everywhere on every single edge on every single graph that you get.

13:27.760 --> 13:31.520
But in reality, is this necessary for it to be a legitimate graph neural network?

13:31.520 --> 13:36.780
That's actually not the case because if I give you two completely non-isomorphic graphs,

13:36.780 --> 13:40.400
if I choose to have completely different message functions in those two graphs, that's totally

13:40.400 --> 13:42.440
fine because it's still a valid graph net.

13:42.440 --> 13:46.240
If I permute any of those graphs, I'll get the permutation equivalent function for the

13:46.240 --> 13:47.560
two of them separately.

13:47.560 --> 13:51.760
There needs to be no weight sharing between them and naturally concepts like these.

13:51.760 --> 13:57.720
So this kind of requires taking a step above the group theoretic view of geometric deep

13:57.720 --> 14:00.840
learning and into the realm of what is known as a group poid.

14:00.840 --> 14:06.160
You kind of imagine every single graph structure, isomorphic graph structure, living on a sort

14:06.160 --> 14:10.120
of island of possible adjacency matrix representations of it.

14:10.120 --> 14:14.320
And for those graphs living on those islands, you need to have some weight sharing.

14:14.320 --> 14:18.080
But for separate islands, you don't need to have any weight sharing whatsoever.

14:18.080 --> 14:22.080
Of course, in practice, these kinds of layers, you would need to have some kind of sharing

14:22.080 --> 14:25.680
of weights in order to make them scalable to arbitrary new graph structures you haven't

14:25.680 --> 14:30.480
seen at training time, but it allows you a lot more flexibility about how you go about

14:30.480 --> 14:31.480
building your functions.

14:31.480 --> 14:35.080
And you're no longer constrained to have just one function everywhere repeated, right?

14:35.080 --> 14:39.240
So that's maybe one example that, at least to me, was what first motivated me and made

14:39.240 --> 14:43.640
me realize that there's more to this stuff than just to say what group theory will give

14:43.640 --> 14:44.640
us.

14:44.640 --> 14:45.640
Amazing.

14:45.640 --> 14:48.840
I'm really interested in your work in algorithmic reasoning, and I know you were just discussing

14:48.840 --> 14:54.040
it as an adjacent thing, and very soon we want to make a show, actually, on your work

14:54.040 --> 14:55.040
on that.

14:55.040 --> 14:57.480
But if you wouldn't mind, could you just sketch out algorithmic reasoning?

14:57.480 --> 14:58.480
Yes, wonderful.

14:58.480 --> 15:00.200
So, very happy to.

15:00.200 --> 15:05.480
Basically, what are we interested in algorithmic reasoning is building neural networks.

15:05.480 --> 15:08.840
They tend to be graph neural networks, but generally speaking, neural networks that are

15:08.840 --> 15:12.480
capable of executing algorithmic computation.

15:12.480 --> 15:17.240
So if I give you some context on what is the state of a particular algorithm, can my network

15:17.240 --> 15:22.320
somehow learn to execute that algorithm ideally in some latent space such that at every single

15:22.320 --> 15:26.920
step of the way, I could if I wanted to decode the states of that algorithm.

15:26.920 --> 15:29.200
So that's basically the main premise.

15:29.200 --> 15:30.680
Why do we care about this?

15:30.680 --> 15:35.680
Well, basically, I think of algorithms as a sort of basic foundational building block

15:35.680 --> 15:40.760
of reasoning, and it's kind of a timeless principle where a software engineer reads

15:40.760 --> 15:45.760
through one of these textbooks on algorithms and learns these 30 or 40 basis algorithms,

15:45.760 --> 15:50.040
and then that knowledge serves them for life in a whole career of software engineering.

15:50.040 --> 15:54.800
So basically, we have this hypothesis that you have this nice basis of algorithms that

15:54.800 --> 15:59.760
if you can master how to do them robustly, you can try to mimic any kind of at least

15:59.760 --> 16:02.080
polynomial time reasoning behavior.

16:02.080 --> 16:06.560
And that's really nice because if you look at the way current state-of-the-art large-scale

16:06.560 --> 16:12.320
models tend to have shortcomings, it's usually in those kinds of robust extrapolation problems.

16:12.320 --> 16:18.680
Basically, if we want to have a really good AI scientist that's able to not just make

16:18.680 --> 16:23.280
great sense of a bunch of training data from the internet, but also use that training data

16:23.280 --> 16:28.640
to derive new knowledge, you need some robustified way to apply rules to get infinite knowledge

16:28.640 --> 16:30.440
from finite means.

16:30.440 --> 16:32.320
So basically, that's what we want to do.

16:32.320 --> 16:36.440
We want to find ways inductive biases or training procedures to build neural networks

16:36.440 --> 16:39.200
that are more algorithmically capable.

16:39.200 --> 16:44.240
And in algorithmic reasoning, we obviously spent a lot of time trying to make this happen,

16:44.240 --> 16:48.600
just building better graph neural networks that align better with target algorithms so

16:48.600 --> 16:53.400
that you can execute them better, but then the really exciting part comes where we've

16:53.400 --> 16:57.400
actually taken some of these graph neural networks that have been pre-trained to execute

16:57.400 --> 17:02.480
one particular algorithm, and then we deployed it in a real-world problem where that algorithm

17:02.480 --> 17:07.260
is required, and we achieved, say, significant representational benefits in terms of downstream

17:07.260 --> 17:08.260
accuracy.

17:08.260 --> 17:11.440
So the idea behind this, and I'll give an example from Google Maps.

17:11.440 --> 17:16.040
This is an application that I worked on at DeepMind, so it's something that I've thought

17:16.040 --> 17:18.040
about quite a bit.

17:18.040 --> 17:23.420
We've invented these algorithms, like Dijkstra's algorithm, to be able to resolve these kinds

17:23.420 --> 17:25.840
of real-world routing problems.

17:25.840 --> 17:29.440
That's the kind of motivation for why you want to build the shortest path algorithm.

17:29.440 --> 17:33.480
And it comes as a little surprise that when you have real-world traffic data, you might

17:33.480 --> 17:39.000
be tempted to apply Dijkstra's algorithm to solve it, to route agents in traffic.

17:39.000 --> 17:43.280
However, what is the actual data that, say, Google Maps has access to?

17:43.280 --> 17:47.280
It's not this nice, abstractified graph with a single scalar in every edge where you can

17:47.280 --> 17:49.320
just go ahead and apply an algorithm.

17:49.320 --> 17:53.360
In fact, there's a huge bridge that must be built between the real data and the input

17:53.360 --> 17:54.360
to the algorithm.

17:54.360 --> 17:59.160
In fact, Google Maps data is typically people's cell phones in their cars, and the cars move,

17:59.160 --> 18:03.440
the phones move, and then based on the movement of the phones, you somehow infer how fast the

18:03.440 --> 18:05.880
car is going or something like that.

18:05.880 --> 18:11.240
And this is very noisy, not very well-structured, and you have to somehow go from there to a

18:11.240 --> 18:13.600
graph where you can apply this heuristic.

18:13.600 --> 18:18.640
Previously, it was always done exclusively by humans, like feature engineers, effectively.

18:18.640 --> 18:22.760
And whenever there's a human feature engineer in the loop like this, you are almost certainly

18:22.760 --> 18:26.280
going to drop a lot of information that you might need to solve the problem.

18:26.280 --> 18:29.560
So basically, you have a huge kind of bridge to cross there.

18:29.560 --> 18:32.600
And with algorithmic reasoning, we now don't use Dijkstra's algorithm.

18:32.600 --> 18:37.440
We use a high-dimensional graph net that was pre-trained to execute Dijkstra's algorithm

18:37.440 --> 18:38.600
in a latent space.

18:38.600 --> 18:42.640
So now this gives us a differentiable component that we can hook up to any encoder and decoder

18:42.640 --> 18:47.520
function we want to, so we can go straight from raw data and code it into the GNN's latent

18:47.520 --> 18:51.920
space, run the algorithm there, and then decode whatever it is that you need, like routing

18:51.920 --> 18:53.680
the vehicles in traffic.

18:53.680 --> 18:58.600
So now purely through backprop, this encoder function now learns to do what the human feature

18:58.600 --> 18:59.600
engineer did.

18:59.600 --> 19:04.560
It learns how to most effectively map that complicated, noisy, real-world data into the

19:04.560 --> 19:07.280
latent space where this GNN can best do its thing.

19:07.280 --> 19:08.440
That really is software 2.0.

19:08.440 --> 19:13.120
But I wanted to ask you about the computational limitations, because you just said something

19:13.120 --> 19:17.640
interesting about representing infinite objects with a finite memory.

19:17.640 --> 19:23.960
So neural networks are not Turing machines, but they can extrapolate, of course.

19:23.960 --> 19:25.520
What's the realistic limitation?

19:25.520 --> 19:30.560
Let's say you're trying to learn an algorithm, how far can you go with a neural network?

19:30.560 --> 19:33.880
So the thing is, there are cases where you can go very far.

19:33.880 --> 19:38.600
We do have theory that is very robust about this, and I think it's theory that is actually

19:38.600 --> 19:40.600
quite easily understandable.

19:40.600 --> 19:44.200
So let me try to kind of visualize it.

19:44.200 --> 19:48.360
When you have a real UMLP, your standard universal approximator, it's basically a piecewise

19:48.360 --> 19:49.800
linear function.

19:49.800 --> 19:54.920
So as you go far enough away from the training data, you're going to hit that level of extrapolation

19:54.920 --> 19:57.680
where you hit the linear part of the piecewise linear.

19:57.680 --> 20:01.840
And at that point, if your target function is not linear, no extrapolation is going to

20:01.840 --> 20:02.840
happen.

20:02.840 --> 20:04.640
You're not going to fit the function properly.

20:04.640 --> 20:08.880
So what's one outcome of this theory is that if you use real UMLPs, this was a great paper

20:08.880 --> 20:14.800
from MIT a few years back, which showed that basically you need to line up parts of your

20:14.800 --> 20:18.440
neural network such that they learn linear functions in the target.

20:18.440 --> 20:21.880
And that's the reason why, say, when you want to imitate a pathfinding algorithm, you want

20:21.880 --> 20:26.760
to use a max aggregation, your GNN, and not sum, where sum is universal.

20:26.760 --> 20:28.120
It can fit anything.

20:28.120 --> 20:31.960
But the function you have to learn, because pathfinding is like minimum overall neighbors

20:31.960 --> 20:36.080
of distance to neighbor plus the edge weight, suddenly when you put max in there, it's a

20:36.080 --> 20:37.080
linear function.

20:37.080 --> 20:40.320
When you put sum in there, it's a highly nonlinear function, so it's going to extrapolate much

20:40.320 --> 20:41.320
worse.

20:41.320 --> 20:45.800
Now, there's been some great follow-up work on this from Beatrice Bevilacqua, Bruno

20:45.800 --> 20:47.920
Ribeiro from Purdue University.

20:47.920 --> 20:52.040
That was at ICML a few years back, which showed that this idea with, like, you want linear

20:52.040 --> 20:57.160
targets with real UMLPs, it's really just a special case of a more general idea that

20:57.160 --> 21:02.960
if you want to extrapolate, say, on different sizes of graphs, you need to have some implicit

21:02.960 --> 21:06.640
causal model of what your test data is going to look like.

21:06.640 --> 21:10.560
This linear algorithmic alignment is just one special case of a causal model like that.

21:10.560 --> 21:15.720
So basically, if you line things up properly from a causal perspective, you should, in

21:15.720 --> 21:17.280
principle, be able to extrapolate.

21:17.280 --> 21:21.200
I mean, we have a clear nonparametric evidence that you can extrapolate is the algorithm

21:21.200 --> 21:22.200
itself, right?

21:22.200 --> 21:26.800
Now, the key is to find the right sweet spot between full universal approximator MLPs and

21:26.800 --> 21:28.280
algorithms on the other side, right?

21:28.280 --> 21:29.280
Interesting.

21:29.280 --> 21:30.280
I spoke to Jan the other day.

21:30.280 --> 21:34.000
He had a paper a couple of years ago about extrapolation in neural networks, saying they

21:34.000 --> 21:35.000
always extrapolate.

21:35.000 --> 21:36.000
Yes.

21:36.000 --> 21:39.080
And speaking with Randall Belastriero, and he's got this paper, the Spline Theory of

21:39.080 --> 21:43.680
Neural Networks, which is about, you know, these input sensitive polyhedra in the ambient

21:43.680 --> 21:44.680
space.

21:44.680 --> 21:48.520
And I always took that to mean why they're quite interpolative and it's just an affine

21:48.520 --> 21:50.160
transformation for a single input.

21:50.160 --> 21:55.080
But what he's shown, though, is that actually, even an MLP with relus is extremely extrapolative

21:55.080 --> 22:00.560
because you can remove a whole bunch of data and, depending on how you've designed the

22:00.560 --> 22:04.320
network architecture, it will still inform that region that you've taken away.

22:04.320 --> 22:08.080
So, I mean, are you familiar with the Spline Theory and do you think it's a useful framework?

22:08.080 --> 22:09.080
Yes.

22:09.080 --> 22:13.320
So, one thing I would say, the way I understand Jan's paper, it could be that I missed some

22:13.320 --> 22:17.040
detail, but the way I understand it is that here we're talking about interpolation and

22:17.040 --> 22:19.600
extrapolation with respect to the geometry of the data.

22:19.600 --> 22:23.840
So, like, you take, say, the convex hull of all the training points and then, yeah, it's

22:23.840 --> 22:27.760
very common, especially in these high dimensional image spaces, right?

22:27.760 --> 22:31.840
It's very easy to push one dimension sufficiently to escape the convex hull of what you've seen

22:31.840 --> 22:32.840
so far.

22:32.840 --> 22:36.720
So, I guess when I say extrapolation out of distribution, I'm actually maybe thinking

22:36.720 --> 22:41.640
of a more probabilistic argument, so something like if you think of the probability distribution

22:41.640 --> 22:45.480
induced by the training set, which obviously allows you to extrapolate away from the convex

22:45.480 --> 22:46.600
hull, right?

22:46.600 --> 22:50.920
But if you go sufficiently far from the modes of that distribution, so you explore a part

22:50.920 --> 22:55.960
of the space that hasn't really been covered, you know, from a probabilistic mass point

22:55.960 --> 23:00.440
of view in the training data, that is what we're actually thinking of when we say out

23:00.440 --> 23:01.440
of distribution generalization.

23:01.440 --> 23:05.840
But, yeah, I fully agree with you, like, in terms of just convex hull arguments, we very

23:05.840 --> 23:09.360
often ask these regular MOPs to go beyond the convex hull, and they seem to work quite

23:09.360 --> 23:11.280
well in those regimes.

23:11.280 --> 23:15.920
But here, I'm talking really about going, like, significantly beyond the convex hull

23:15.920 --> 23:19.160
to, like, some region that really wasn't touched.

23:19.160 --> 23:23.960
And what we do, for example, in our papers is we train on 16 node graphs to execute these

23:23.960 --> 23:28.240
algorithms, and then we test it on four times larger, 64 node graphs.

23:28.240 --> 23:31.920
And what this means, because an algorithm might have, say, n-cubed time complexity, it means

23:31.920 --> 23:35.560
the trajectory over which you have to roll it out is also much, much longer than what

23:35.560 --> 23:36.680
you've seen in training time.

23:36.680 --> 23:41.240
So it's really a test of, like, very different conditions than what you've seen in training

23:41.240 --> 23:42.240
time, right?

23:42.240 --> 23:43.240
That's interesting.

23:43.240 --> 23:45.520
And first of all, I completely agree with you that this binary convex hull notion of

23:45.520 --> 23:48.720
extrapolation probably isn't particularly useful.

23:48.720 --> 23:53.320
But, you know, folks like Francois Relais describe the way Neuron Network's work is kind of bending

23:53.320 --> 23:55.960
the space, you know, progressively with layers.

23:56.080 --> 23:59.600
I really like this polyhedra idea.

23:59.600 --> 24:03.600
Contrast the algorithmic reasoning with GNNs, so, I mean, I spoke with Hattie from a Google

24:03.600 --> 24:08.280
brain team the other day, she's doing the in-consex prompting, you know, sort of algorithm

24:08.280 --> 24:09.280
learning.

24:09.280 --> 24:11.720
How would you contrast those two approaches?

24:11.720 --> 24:17.520
So basically, I would really like these approaches to be reconciled going forward in the sense

24:17.520 --> 24:22.520
that, like, I don't see them as going one without the other, if that makes sense.

24:22.520 --> 24:28.720
So on one side, and I'm going to invoke the same principles I mentioned during our MLST

24:28.720 --> 24:32.400
episode, you know, Daniel Kahneman's book, System 1 and System 2, right?

24:32.400 --> 24:34.080
I think you cannot have one without the other.

24:34.080 --> 24:38.680
So you have these amazing large-scale perceptive models that are really amazing at, you know,

24:38.680 --> 24:42.880
taking the complexities of the real world and somehow getting interpretable enough concepts

24:42.880 --> 24:46.760
out of there that they can, you know, make sense of what's going on and, like, drive many

24:46.760 --> 24:51.440
interesting real-world decision-making problems, although they might lose a little bit on having

24:51.480 --> 24:56.000
to do something like what an AI scientist would be expected to do, which is, like, extrapolate

24:56.000 --> 24:58.520
and generate new concepts out of what they've seen.

24:58.520 --> 25:03.640
And as you said, these kinds of specifically tailored prompts might enable the model to

25:03.640 --> 25:08.800
take things a step or two further, but it's always, like, it's kind of, in spirit, it's

25:08.800 --> 25:12.920
the same thing as algorithmic reasoning, because we teach a model to execute an algorithm by

25:12.920 --> 25:15.480
forcing it to imitate the algorithm step by step.

25:15.480 --> 25:19.720
Here you prompt a language model by telling it what are some of the steps, like, just

25:19.760 --> 25:23.280
like you're trying to teach a student how to solve a homework, right, telling them the

25:23.280 --> 25:26.640
individual steps they need to do, and then letting the language model go off on its own

25:26.640 --> 25:27.640
to solve it.

25:27.640 --> 25:32.040
But where I see the real future of these two methods converging is you're going to have

25:32.040 --> 25:36.560
your system one component that gets your concepts out very nicely, cleanly.

25:36.560 --> 25:40.040
And then those concepts, because we're working with transformers nowadays anyway most of

25:40.040 --> 25:42.440
the time, are going to be very slot-based.

25:42.440 --> 25:46.640
So that plays very nicely with GNNs, which expect nodes as input, right, so you can maybe

25:46.640 --> 25:51.520
hook up in some nice way those concepts into a graph neural network that was trained to

25:51.520 --> 25:55.680
execute a bunch of algorithms, and then, you know, kind of get the best of both worlds.

25:55.680 --> 26:00.320
So have your perceptual component do the perception, and maybe prompt it as well to kind of do

26:00.320 --> 26:04.960
it in a particularly step-by-step manner, and then further have a robust component that

26:04.960 --> 26:08.880
makes you not have to relearn all those things that neural networks we know theoretically

26:08.880 --> 26:12.400
cannot learn to do that well because of these extrapolation arguments.

26:12.400 --> 26:15.200
Maybe one last point I would make to kind of cement this.

26:15.200 --> 26:18.880
If you've been around the archive recently, you might have seen our paper on a generalist

26:18.880 --> 26:25.080
neural algorithmic learner where we have actually used GATO-style ideas to train one graph neural

26:25.080 --> 26:29.800
network that can execute 30 very diverse algorithms all in the same architecture with a single

26:29.800 --> 26:34.480
set of weights, so sorting, searching, pathfinding, dynamic programming, comics, hauls, all those

26:34.480 --> 26:37.240
kinds of nice things, very diverse ways of reasoning.

26:37.240 --> 26:41.120
We believe something like that could maybe be a basis of, say, a foundation model of

26:41.120 --> 26:45.160
reasoning in the future that could nicely hook up to the foundation models we already

26:45.160 --> 26:47.280
know and love in the realm of perception.

26:47.280 --> 26:48.280
Amazing.

26:48.280 --> 26:50.760
And what's the biggest research challenge for you next year?

26:50.760 --> 26:56.120
So next year, I would really like to show to what extent these things can scale in the

26:56.120 --> 26:57.120
real world.

26:57.120 --> 27:02.000
So we already have several isolated papers that showed that these ideas can work on

27:02.000 --> 27:03.000
real problems.

27:03.000 --> 27:05.660
We have Excelvin where we applied it to reinforcement learning.

27:05.660 --> 27:07.560
That was in Europe Spotlight last year.

27:07.560 --> 27:10.800
We have RMR where we applied it to self-supervision problems.

27:10.800 --> 27:15.760
We also have one paper currently under review at iClear where we successfully applied to

27:15.760 --> 27:16.760
supervised learning.

27:16.760 --> 27:21.040
So we say pre-trained on a flow algorithm and we deploy it on brain vessel segmentation

27:21.040 --> 27:22.680
tasks and stuff like that.

27:22.680 --> 27:26.640
So we have many isolated cases where you learn a particular algorithm and it works really

27:26.640 --> 27:29.400
well in a real world scenario.

27:29.400 --> 27:34.240
I would like to see how can we take this idea and truly put it to the test at larger scales,

27:34.240 --> 27:39.640
both in terms of number of problems we attack or number of nodes that we support or anything

27:39.640 --> 27:40.640
in between.

27:40.640 --> 27:41.640
Amazing.

27:41.640 --> 27:45.640
Dr. Patovaličković, let's just, we'll get a shaking handshot.

27:45.640 --> 27:46.640
All right.

27:46.640 --> 27:49.120
Thank you so much for joining us.

27:49.120 --> 27:50.120
Thank you for having me.

27:50.120 --> 27:51.120
I really appreciate it.

27:51.120 --> 27:57.920
Dr. Ishan Mizra of Meta and Lex Friedman fame came over and had a chat with us.

27:57.920 --> 28:01.640
Ishan is one of the world's leading experts in computer vision.

28:01.640 --> 28:03.880
So what was your paper about?

28:03.880 --> 28:09.440
Yeah, basically we try to have global propagation, the likes of which you see in transformers,

28:09.440 --> 28:13.360
not like with sparse costs.

28:13.360 --> 28:17.960
So but in a way that will still allow you to have nice global communication properties

28:17.960 --> 28:19.720
and no bottlenecks and stuff like that.

28:19.720 --> 28:25.400
So we basically have this idea of you could generate these expander graphs which allow

28:25.400 --> 28:28.400
you to have nice sparsity properties.

28:28.400 --> 28:33.000
So basically every node I think has degree four in the graphs we compute and you need

28:33.000 --> 28:36.400
only logarithmically many steps to traverse the graph, which means you can still do it

28:36.400 --> 28:39.480
efficiently with a small number of steps.

28:39.480 --> 28:42.240
And yeah, it seems to empirically work well on a bunch of graph benchmarks.

28:42.240 --> 28:47.240
So yeah, it's a, I think it's only scratching the surface of what we can do because we literally

28:47.240 --> 28:52.040
just generate a graph at random and slap it onto like mask the computations, but yeah,

28:52.040 --> 28:53.040
it's an interesting start.

28:53.040 --> 28:54.040
Very nice.

28:54.040 --> 28:55.040
Yeah.

28:55.040 --> 28:56.040
How about your conference?

28:56.040 --> 28:57.040
How's it been?

28:57.040 --> 28:58.040
So it's been pretty good.

28:58.040 --> 29:01.280
We're organizing the self supervised learning workshop tomorrow, which is going to be probably,

29:01.280 --> 29:05.600
I hope like useful to a lot of people, we're going to have a bunch of speakers coming from

29:05.600 --> 29:09.880
vision, language, NLP, like speech and so on.

29:09.880 --> 29:13.920
And yeah, we're also presenting a poster there, which is about learning joint image and video

29:13.920 --> 29:17.800
representations, which are state of the art across image and video benchmarks using a

29:17.800 --> 29:19.300
single model.

29:19.300 --> 29:23.120
On the final day of the conference, I caught up with Petra again at the poster session for

29:23.120 --> 29:28.440
new reps, which is the symmetry and geometry and neural representations group.

29:28.440 --> 29:32.720
And his paper was selected by all of the reviewers at the conference as being in the

29:32.720 --> 29:39.440
top 10, which is super impressive, but this is Petra talking about his paper.

29:39.440 --> 29:45.080
So in the expander graph propagation work, we are trying to solve what is, in my opinion,

29:45.080 --> 29:49.360
one of the most important problems in graph representation learning currently unsolved,

29:49.360 --> 29:51.320
which is the oversplashing problem.

29:51.320 --> 29:56.160
And effectively it is a task, which it's a problem which plagues graph neural networks

29:56.160 --> 29:59.900
regardless of which parameters you choose or which model you choose.

29:59.900 --> 30:04.020
It's really something that often depends on the topology of the graph, and it's a situation

30:04.020 --> 30:09.940
where no matter how hard you try, no matter which parameters you set, the amount of features

30:09.940 --> 30:13.760
you would need to compute, so the size of your latent space would have to be exponential

30:13.760 --> 30:17.780
in the number of layers for the pairs of nodes to efficiently communicate.

30:17.780 --> 30:21.760
We don't always know when it happens, but very often it tends to happen around these

30:21.760 --> 30:22.760
bottlenecks.

30:22.760 --> 30:26.420
So basically in this particular graph, you have these two communities that are tightly

30:26.420 --> 30:31.040
connected, and you have this just one critical edge connecting them, and this edge is now

30:31.040 --> 30:32.640
under a lot of pressure.

30:32.640 --> 30:37.860
If you want data from these nodes to travel to these nodes and vice versa, this edge has

30:37.860 --> 30:44.180
to be mindful of a lot of things, so the size of the feature space required for this edge

30:44.180 --> 30:47.540
grows exponentially, and things get even worse when you look at trees.

30:47.540 --> 30:52.840
Trees are like the canonical worst case example, where cutting off this edge would really trigger

30:52.840 --> 30:58.260
all sorts of bottleneck cases, and essentially you need basically a number of, to store information

30:58.260 --> 31:04.240
about a number of nodes that goes exponentially in the number of steps, just to be able to

31:04.240 --> 31:05.960
travel to the other side of the tree.

31:05.960 --> 31:09.960
So this is a fundamental problem of propagating data, which has nothing to do with the choice

31:09.960 --> 31:13.100
of model, just topology.

31:13.100 --> 31:15.860
And what do we try to do to fix this problem?

31:15.860 --> 31:19.720
You would ideally want, so first we start off with the assumption that this kind of global

31:19.720 --> 31:21.580
talking is actually beneficial.

31:21.580 --> 31:25.520
Of course there are some tasks where you might not want data to travel in this way, because

31:25.520 --> 31:30.560
if it's a highly homophilus data-driven problem, then you might want information to stay in

31:30.560 --> 31:32.760
the community, to not get diluted.

31:32.760 --> 31:36.800
But we assume in many tasks, like say molecular property prediction tasks, you actually want

31:36.800 --> 31:40.560
data to travel globally, so that's exactly what we do.

31:40.560 --> 31:42.000
That's our first assumption.

31:42.000 --> 31:45.280
As we just described, we don't really want these bottlenecks to exist, because if there's

31:45.280 --> 31:49.800
a bottleneck, no matter what you do with the model, it's not going to work well.

31:49.800 --> 31:53.800
We would ideally want the complexity to be scalable, so we can apply this to graphs of

31:53.800 --> 31:55.680
arbitrary sizes.

31:55.680 --> 31:59.840
One simple solution to this problem is to use a graph transformer, which would connect

31:59.840 --> 32:05.280
every node to every other node and give you a trivially setting with no bottlenecks.

32:05.280 --> 32:11.400
However, as we will show later, these fully connected graphs are trivially dense expanders,

32:11.400 --> 32:12.400
actually.

32:12.400 --> 32:14.840
So they fit our theory, but they are dense and they won't scale.

32:14.840 --> 32:16.600
So we don't necessarily want that.

32:16.600 --> 32:23.080
And lastly, because it's often quite computationally painful to clear these bottlenecks in an input

32:23.080 --> 32:28.120
data-driven way, especially if you have lots of online graphs coming into your problem,

32:28.120 --> 32:31.720
we might ideally want a method that doesn't have to do like dedicated pre-processing of

32:31.720 --> 32:32.880
the input graph.

32:32.880 --> 32:37.000
And actually, satisfying all four of these at the same time turns out to be quite tricky.

32:37.000 --> 32:41.600
We actually have done a literature survey of a bunch of related works, and it seems

32:41.600 --> 32:44.720
really hard to tick all four of these boxes.

32:44.720 --> 32:48.180
And our method, the expander graph propagation, tends to tick all four of them.

32:48.180 --> 32:49.400
So how do we do it?

32:49.400 --> 32:53.800
Basically, we propose to propagate information over these expander graphs, which are known

32:53.800 --> 32:55.720
constructs from graph theory.

32:55.720 --> 33:00.480
Specifically, expander graphs have mathematical properties of a high-chigger constant, so

33:00.480 --> 33:05.120
a very low bottleneck, which is good, a low diameter, meaning you'll get global information

33:05.120 --> 33:06.720
propagation very efficiently.

33:06.720 --> 33:11.960
However, additionally, we can build expanders in a sparse manner using this standard mathematical

33:11.960 --> 33:14.840
construction from the special linear group.

33:14.840 --> 33:19.240
And that actually guarantees us that the degree of every node will be four, therefore the

33:19.240 --> 33:21.200
graph will be sparse.

33:21.200 --> 33:26.240
And actually, the only generative parameter of these graphs is the size of the group,

33:26.240 --> 33:27.600
this N over here.

33:27.600 --> 33:31.040
So it's very easy to generate an expander for a particular number of nodes.

33:31.040 --> 33:34.760
You just tell me what N you want, and I'll give you a graph.

33:34.760 --> 33:37.640
So when you look at an expander, it looks something like this.

33:37.640 --> 33:41.400
It is basically, what I like to say, it looks a bit like the human brain, right?

33:41.400 --> 33:45.560
Every node kind of has this very local connectivity to its four immediate neighbors.

33:45.560 --> 33:50.560
But as you go far away, like log N steps, you get a lot of cycles being closed very quickly,

33:50.560 --> 33:53.920
and the global communication properties get like really good.

33:53.920 --> 33:55.520
So that's our proposal.

33:55.520 --> 33:59.480
Take basically, you know, your state-of-the-art graph net that you care about.

33:59.480 --> 34:02.920
We literally just take the code actively available implementation.

34:02.920 --> 34:08.280
We switch the graph neural network connectivity in every even layer to operate over one of

34:08.280 --> 34:10.640
these guys rather than the input graph.

34:10.640 --> 34:14.400
So basically, you kind of alternate input graph, expander graph, input graph, expander

34:14.400 --> 34:19.320
graph, so that the input graphs layers are responsible for the usual local computations

34:19.320 --> 34:21.080
that a GNN wants to do.

34:21.080 --> 34:25.240
And the expander layers are responsible within diffusing that information globally in a sparse

34:25.240 --> 34:26.760
and scalable way.

34:26.760 --> 34:27.760
And this seems to work well.

34:27.760 --> 34:31.880
So on all the data sets we tried this construction, it was better than the baseline.

34:31.960 --> 34:35.600
As I said, all we did was change the connectivity, so the number of parameters is exactly the

34:35.600 --> 34:36.600
same.

34:36.600 --> 34:41.200
It's really like an apples-to-apples comparison, and it led to statistically significant results.

34:41.200 --> 34:44.760
One last point I would like to make is, you know, we're not the only group that tried

34:44.760 --> 34:49.840
to study this problem, concurrently to us, the group of Michael Bronstein with Jake

34:49.840 --> 34:54.120
Topping and Francesco DiGiovanni had this great paper on curvature analysis, which was

34:54.120 --> 34:58.120
actually one of the best paper awardees at iClear 2022.

34:58.120 --> 35:02.840
And basically in this paper, they claim that if you have negatively curved edges, so edges

35:02.840 --> 35:08.200
with very negative curvature, those tend to be the ones responsible for the formation

35:08.200 --> 35:10.720
of bottlenecks and therefore over-squashing.

35:10.720 --> 35:15.920
So naturally we wanted to connect our expander to this theory, so we computed the curvature

35:15.920 --> 35:17.120
of our graphs.

35:17.120 --> 35:21.000
But we found that actually the graphs that we built are negatively curved everywhere.

35:21.000 --> 35:26.480
So it has a curvature of negative 1 very quickly as you increase the size of the graph, right?

35:26.480 --> 35:31.280
So obviously, you know, we built a negatively curved graph everywhere, yet it still seems

35:31.280 --> 35:32.280
to work well.

35:32.280 --> 35:33.720
So what gives, right?

35:33.720 --> 35:35.480
We try to analyze this a bit further.

35:35.480 --> 35:39.200
First we show that the curvature of negative 1 is actually not that small.

35:39.200 --> 35:44.860
Like the theorem in this paper is only invoked when the curvature is close to minus 2.

35:44.860 --> 35:48.520
So in our case with curvature of negative 1, it's actually not sufficiently negative

35:48.520 --> 35:51.440
to trigger that failure case of this theorem.

35:51.440 --> 35:55.840
And additionally, we took it a step further and we actually tried to analyze how easy

35:55.840 --> 35:58.160
it is to satisfy these three properties at once.

35:58.160 --> 36:01.400
So to have sparsity, we said sparsity is good for scalability.

36:01.400 --> 36:05.520
To have a low bottleneck, so a high trigger constant, which would mean you don't have

36:05.520 --> 36:08.560
these kinds of pathological propagation problems.

36:08.560 --> 36:13.560
And thirdly, to have positive curvature, which seems to be a good idea based on the analysis

36:13.560 --> 36:14.560
of this paper.

36:14.560 --> 36:17.560
And we actually proved, there is a theorem in our paper that proves that these three

36:17.560 --> 36:22.080
things are incompatible with each other, in that there's only finitely many graphs

36:22.080 --> 36:25.700
that satisfy these three properties simultaneously.

36:25.700 --> 36:30.860
So as you go to large enough input graphs to be sparsed and to have no bottlenecks,

36:30.860 --> 36:32.820
you have to be negatively curved somewhere.

36:32.820 --> 36:34.540
It's impossible to avoid it.

36:34.540 --> 36:39.260
So while we don't study the implications of this any further, we do believe that it calls

36:39.260 --> 36:45.160
on the community in the future to study what happens in this gray area where the curvature

36:45.160 --> 36:47.220
is negative but not too negative.

36:47.220 --> 36:50.860
Because it seems like something like that might be critical to having the most optimal

36:50.860 --> 36:52.660
message passing possible.

36:52.660 --> 36:54.980
And that is basically the rough summary of our work.

