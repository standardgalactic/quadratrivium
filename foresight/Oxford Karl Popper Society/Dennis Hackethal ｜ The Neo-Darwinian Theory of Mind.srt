1
00:00:00,000 --> 00:00:11,080
So everyone can hear me then. Good afternoon. I'm very pleased to announce their speaker

2
00:00:11,080 --> 00:00:17,240
today is Dennis Hackenthal, who is a software engineer, artificial intelligence researcher

3
00:00:17,240 --> 00:00:22,680
and author of the book, A Window on Intelligence. And today we'll speak about the role of replicators

4
00:00:22,680 --> 00:00:28,400
in creativity, a topic in which Dennis has written for Conjection Magazine in an article

5
00:00:28,400 --> 00:00:33,280
called The Neodorinian Theory of Mind, which we've linked to on Twitter and which we'll

6
00:00:33,280 --> 00:00:39,280
link to in the video as well. And I should also say that I think Conjection Magazine

7
00:00:39,280 --> 00:00:45,080
is open to pictures from writers, for those of you who are interested in possibly contributing

8
00:00:45,080 --> 00:00:50,960
an article to the website. And in general, I also really recommend checking out Conjection

9
00:00:50,960 --> 00:00:58,200
Magazine, I think is a great initiative. And yeah, with that introduction, I give the floor

10
00:00:58,200 --> 00:01:02,280
to Dennis. Dennis, thanks so much for joining us. And I look forward to your talk.

11
00:01:02,280 --> 00:01:08,400
Hi, I'm glad to be here. It was very nice of you to invite me. And I should also mention

12
00:01:08,400 --> 00:01:11,640
Logan Chipkin, who I think is behind this. So shout out to him as well.

13
00:01:11,640 --> 00:01:14,560
Yeah, shout out to Logan.

14
00:01:14,560 --> 00:01:19,960
Yeah, so I'm here to talk about what I call the Neodorinian Theory of Mind. Before I start,

15
00:01:19,960 --> 00:01:26,160
I should say that to give credit where credit is due, I'm indebted to Karl Popper and David

16
00:01:26,200 --> 00:01:30,320
Deutsch, because without their epistemological work, I could never have thought of what I'm

17
00:01:30,320 --> 00:01:35,200
about to tell you. And also Richard Dawkins, who's influenced my thinking on evolution

18
00:01:35,200 --> 00:01:43,320
quite a bit. And it occurred to me recently that maybe the Neodorinian Theory of the Mind

19
00:01:43,320 --> 00:01:48,440
is much too grand a term. I think maybe it should be called the Neodorinian aspect of

20
00:01:48,440 --> 00:01:52,520
the mind or the Neodorinian approach to the mind or something like that. I think that

21
00:01:52,560 --> 00:01:59,520
would be a better name because I think theories of the mind are a dime a dozen and none of

22
00:01:59,520 --> 00:02:05,440
them work. And otherwise, we could already build AGI, artificial general intelligence.

23
00:02:05,440 --> 00:02:13,680
And so it might be better to choose a smaller name than that. But also there's some construction

24
00:02:13,680 --> 00:02:20,760
going on outside. So apologies if you hear anything, if there's any noise. But yeah.

25
00:02:20,800 --> 00:02:26,120
So the meat of the theory of the approach or whatever you want to call it is very simple.

26
00:02:28,680 --> 00:02:36,320
But it does take just a moment to kind of sneak up on it. And so in the papyrian fashion,

27
00:02:36,320 --> 00:02:43,000
what I'd like to do is I'd like to start with some problems. Some problems that I like to

28
00:02:43,000 --> 00:02:50,600
think that the theory or the approach solves are, for example, one, how does the mind create

29
00:02:50,600 --> 00:02:59,480
new conjectures? Two, how does memory work? Which includes things like why do we forget

30
00:02:59,480 --> 00:03:04,440
things? Why are memories so unreliable? Why do some memories last longer than others? Those

31
00:03:04,440 --> 00:03:12,320
kinds of things. And how did people evolve? And then lastly, I want to give by no means

32
00:03:12,320 --> 00:03:17,200
any theories just some small pointers for why I think we are conscious of some things

33
00:03:17,240 --> 00:03:22,880
and not of others. It's interesting to note that originally I only set out to solve the

34
00:03:22,880 --> 00:03:26,880
problem of how people evolved. And then I later found that this approach also allows me to

35
00:03:26,880 --> 00:03:34,280
solve these other problems. So I only have a few slides. This is the last slide. I'll leave

36
00:03:34,280 --> 00:03:43,360
this open. I want to give, I actually want to start in biology. And then I'll tie this back in

37
00:03:43,360 --> 00:03:48,840
with a mind later on. I promise they're related. And I'd like to talk about the origin of life

38
00:03:48,840 --> 00:03:56,760
for a moment. And many have said that what happened there was so unlikely and incredible. It's

39
00:03:56,760 --> 00:04:03,280
almost too good to be true that that happened. And as far as I understand it, there are still

40
00:04:03,280 --> 00:04:08,760
some mysteries around it, what exactly happened there. But there are some good explanations

41
00:04:08,840 --> 00:04:14,920
that I think give us a pretty good idea of what may have happened. And the theory that I

42
00:04:14,920 --> 00:04:22,680
subscribe to is called the RNA world hypothesis. And the idea is basically that a long, long time

43
00:04:22,680 --> 00:04:29,880
ago when the Earth had just formed, the oceans were still highly chemically active. And there were

44
00:04:29,880 --> 00:04:38,720
molecules forming spontaneously in those oceans and disintegrating. And it was a big mess. It was

45
00:04:38,760 --> 00:04:46,080
a big chaos in this so-called primordial soup. And what some of those molecules were is they were

46
00:04:46,120 --> 00:04:52,040
enzymes, they were chemical catalysts. And what that means is that they were able to, they were,

47
00:04:52,040 --> 00:04:57,560
they were able to cause a change in other molecules, for example, while retaining the ability to cause

48
00:04:57,560 --> 00:05:06,080
those changes again at a later time, they didn't undergo any changes themselves. And what happened

49
00:05:06,080 --> 00:05:14,760
then, so to say, that some of these molecules happened to, to cause chemical reactions that

50
00:05:14,800 --> 00:05:20,840
produced some of their own components. So that in this primordial soup, over time, you would get

51
00:05:20,840 --> 00:05:26,160
more and more of the building blocks of which these molecules themselves were made. And so what

52
00:05:26,160 --> 00:05:32,240
I'm kind of sneaking up on is the role of replication in biological evolution. Because once

53
00:05:32,240 --> 00:05:39,800
these molecules, once these enzymes get targeted enough, it makes sense to call them

54
00:05:39,800 --> 00:05:47,200
replicators, because that is what they end up doing. They create a copy of themselves. So the

55
00:05:47,200 --> 00:05:54,400
idea is that this way, gradually, we get replication happening in the primordial soup. But

56
00:05:54,400 --> 00:06:00,840
this alone isn't enough for evolution to occur. You also need variation in selection. But I think

57
00:06:00,840 --> 00:06:09,000
once you have replication, variation is bound to happen sooner or later, because replication is

58
00:06:09,000 --> 00:06:14,440
not going to be perfect forever. And sooner or later, the replicators are going to make a mistake

59
00:06:14,440 --> 00:06:24,640
during replication. And this is what introduces variants. And what you get then is you have, if

60
00:06:24,640 --> 00:06:30,600
the variant is even fit enough still to keep replicating, what you get then is you have slightly

61
00:06:30,600 --> 00:06:35,520
different kind of replicator. And if that is still able to replicate, now what you're getting is

62
00:06:35,520 --> 00:06:39,800
you're getting two different pockets of this population. One pocket of the population is still

63
00:06:39,800 --> 00:06:46,040
the old replicator, the copies of that replicator. And the new pocket of the population is this new

64
00:06:46,040 --> 00:06:51,680
kind of replicator. And so then the question is, well, which one is better at replicating than

65
00:06:52,560 --> 00:07:00,960
the other? And these differences in the rate of replication is what we call selection. I think

66
00:07:00,960 --> 00:07:08,280
Richard Dawkins in his book, The Selfish Gene, I think he uses the technical term as the non-random

67
00:07:08,280 --> 00:07:17,240
differential reproduction rate or something. And usually, because a replicator is already adapted

68
00:07:17,240 --> 00:07:23,480
to replicating, that means that any variant, if a mutation is going to occur, more often than not,

69
00:07:23,480 --> 00:07:28,960
it's going to be worse at replicating. So usually, you could expect that if the replicator mutates,

70
00:07:28,960 --> 00:07:39,640
the mutant variant is going to have a harder time replicating. Because by definition, this is how

71
00:07:39,640 --> 00:07:45,560
William Paley, I think, defined adaptation, something is adapted to something, for example,

72
00:07:45,560 --> 00:07:49,480
replication, that by definition means that if you make any slight change to it, it's going to get

73
00:07:49,480 --> 00:07:57,960
worse. It's very difficult to get it to make it better. But every now and then, a mutation can

74
00:07:57,960 --> 00:08:05,240
lead to a benefit. And by benefit, I don't mean that there is some well-meaning force in the

75
00:08:05,240 --> 00:08:13,160
biosphere. It just means that it helps the variant spread through the population of replicators at

76
00:08:13,160 --> 00:08:20,760
the expense of its rivals. And I think it's important to keep things simple when it comes to the theory

77
00:08:20,760 --> 00:08:26,360
of evolution. Really, all that the replicator wants to do, I say in scare quotes, it's not a

78
00:08:26,360 --> 00:08:31,880
conscious being, but all the replicator wants to do is spread through the population of replicators.

79
00:08:31,880 --> 00:08:38,600
So it's all it cares about. And sometimes, in service of that goal, complex adaptations can

80
00:08:38,600 --> 00:08:50,440
arise. So I'm going to start tying this back in with the mind now. Carl Popper said that the

81
00:08:50,440 --> 00:08:58,200
information that is stored in such replicators is adapted. And so therefore, it's knowledge.

82
00:08:58,840 --> 00:09:02,680
Adapted information is knowledge. Or maybe that particular phrasing originated with David Deutsch.

83
00:09:02,760 --> 00:09:11,160
And Popper's conjecture is that people also create human knowledge by evolution. So evolution, in

84
00:09:11,160 --> 00:09:18,840
that sense, is not limited to the biosphere. Excuse me. Although there are important differences

85
00:09:18,840 --> 00:09:25,080
between biological evolution and the evolution that occurs in our minds, there are strong parallels

86
00:09:25,160 --> 00:09:32,840
are strong parallels between them. And so Popper offered this analogy of a new conjecture that

87
00:09:32,840 --> 00:09:39,720
people come up with being analogous to the mutation of a gene and criticism being analogous to the

88
00:09:39,720 --> 00:09:46,920
selection of a gene. And the old theories that we derive knowledge from the census, for example,

89
00:09:46,920 --> 00:09:53,480
which is empiricism, or that we create them by extrapolating repeated observations, which is

90
00:09:53,480 --> 00:09:58,840
inductivism, which is completely false, as Popper explained. And instead he argued that we

91
00:09:59,560 --> 00:10:05,480
create knowledge by starting with problems, not with observations, which are conflicts between

92
00:10:05,480 --> 00:10:11,880
ideas as David Deutsch defines them. And then we guess solutions to these problems and we criticize

93
00:10:11,880 --> 00:10:18,440
them until we hopefully come up with a solution that we deem good enough to adopt tentatively.

94
00:10:19,480 --> 00:10:23,000
And this is not only scientific knowledge that grows that way. All human knowledge grows that

95
00:10:23,000 --> 00:10:29,640
way. And an example I like to give is that of losing your keys. If you're not sure where your

96
00:10:29,640 --> 00:10:34,760
keys are, and let's say you need them now because you want to leave, really all you can do is guess

97
00:10:34,760 --> 00:10:41,240
where they might be. So maybe you left them on the kitchen counter. So that is a tentative

98
00:10:41,880 --> 00:10:46,200
solution to the problem. And then you can go look for them there. And if you don't find there,

99
00:10:46,200 --> 00:10:49,560
then you know that your theory was false. And so you have to guess another solution, maybe they're

100
00:10:49,560 --> 00:10:53,640
in your pocket or something. And if you find them there, that's great. Then you've made progress.

101
00:10:55,240 --> 00:11:02,200
So knowledge, even in the most mundane scenarios like that, knowledge grows that way.

102
00:11:03,000 --> 00:11:12,680
And I do think that Popper was right about all of this. And I even like to think of them as the

103
00:11:13,560 --> 00:11:17,480
foremost artificial general intelligence researcher of this time, although I don't think

104
00:11:17,480 --> 00:11:24,200
he would have called himself that. But as should be expected, if we're papyrans and fallibilists,

105
00:11:24,200 --> 00:11:29,000
there are open problems with hispistemology. And these are some of the open problems.

106
00:11:29,880 --> 00:11:35,240
The open problems I mentioned at the beginning are some of those. So just to reiterate them,

107
00:11:35,240 --> 00:11:42,520
those were how does the mind create new conjectures? How does memory work? How did people evolve?

108
00:11:43,480 --> 00:11:45,720
And then the big one is consciousness.

109
00:11:51,080 --> 00:11:58,600
And to solve them, what I would like to suggest is the following, that if we can think of the

110
00:11:58,600 --> 00:12:08,360
biosphere as an arena of self replicating genes, that analogously, we can think of the mind as an

111
00:12:08,360 --> 00:12:13,880
arena of self replicating ideas. Now, having said that there are still many important differences

112
00:12:13,880 --> 00:12:18,200
between the mind and the biosphere, I don't by no means do I want to equivocate those two.

113
00:12:19,000 --> 00:12:22,760
But I do think there are strong parallels that may help us explain some things.

114
00:12:23,640 --> 00:12:33,480
I should also mention, once I say replicating ideas, we may jump to memes, which are Richard

115
00:12:33,480 --> 00:12:40,280
Dawkins idea of the cultural unit of a replicator. Those are not what I mean, just to clarify. Memes

116
00:12:40,280 --> 00:12:46,840
are ideas that replicate across minds. Memes are not just funny pictures on the internet.

117
00:12:48,280 --> 00:12:54,920
It's basically any idea that manages to jump from one mind to another, or at least enough minds that

118
00:12:54,920 --> 00:13:01,960
it becomes meaningful to call it a meme. So a joke is a meme, but also catchy song could be a meme.

119
00:13:02,760 --> 00:13:08,760
And with the example of the joke, I think this is an example David gives in his book.

120
00:13:09,480 --> 00:13:15,800
You know, if the joke is good enough, then it causes the holder, the person who knows the joke,

121
00:13:15,800 --> 00:13:22,440
to tell it to somebody else. And so now the joke is in two minds, not just one. And that's why

122
00:13:22,440 --> 00:13:27,960
it's meaningful to call it a replicator. But again, I'm not talking about memes here. I'm only

123
00:13:27,960 --> 00:13:34,760
talking about ideas that replicate strictly within a single mind. So it can help, I think, to just

124
00:13:34,760 --> 00:13:38,840
think of a mind that's just cut off completely from the outside world to make things easier. So you

125
00:13:38,840 --> 00:13:43,560
can just imagine a brain in a vat, if you like, doesn't have any access to the outside world.

126
00:13:43,560 --> 00:13:48,440
And still, I think there would be self replicating ideas in the mind. It's also a nice way to

127
00:13:50,440 --> 00:13:55,240
make sure that we can't accidentally adopt any empiricist or inductive notions if we just completely

128
00:13:55,240 --> 00:14:04,120
disregard sense data. And so I think with this simple premise that the mind is an arena of

129
00:14:04,120 --> 00:14:13,880
self replicating ideas, we can now start solving the problems that I mentioned. And I think when it

130
00:14:13,880 --> 00:14:23,000
comes to the question of how people evolved, I would say that the evolution of the mind and

131
00:14:23,080 --> 00:14:29,720
the evolution of people is analogous to the origin of life in the biosphere. Because I'd like to

132
00:14:29,720 --> 00:14:35,080
suggest that the evolution of self replicating ideas in a mind happened analogously to the

133
00:14:35,080 --> 00:14:42,280
evolution of self replicating molecules in the primordial soup. And I think what happened is,

134
00:14:43,880 --> 00:14:50,200
well, so this is Popper again, so all organisms contain knowledge in the objective sense,

135
00:14:50,920 --> 00:14:56,520
meaning, for example, wolves know how to hunt in packs and dogs know how to fetch balls, beavers

136
00:14:56,520 --> 00:15:01,640
know how to build dams and so forth. This all takes knowledge. All of these activities have

137
00:15:01,640 --> 00:15:06,920
the appearance of design. So we know that there must be knowledge behind them. And that knowledge

138
00:15:06,920 --> 00:15:12,360
is stored in their genes. And so we could just call the knowledge that results in these behaviors.

139
00:15:12,360 --> 00:15:19,480
We could call those ideas in the objective sense. And the set of all ideas that an organism has,

140
00:15:20,200 --> 00:15:29,400
I call its idea pool. And some of these ideas, I think, act just like the enzymes,

141
00:15:29,400 --> 00:15:36,200
those molecular catalysts from the primordial soup. They're able to make a change in other ideas

142
00:15:36,200 --> 00:15:45,000
within the same organism without undergoing any net change themselves. And in one of our ancestors,

143
00:15:45,000 --> 00:15:51,000
because of a genetic mutation, there was one such idea, one such catalyst, that happened to

144
00:15:51,000 --> 00:15:55,720
promote the production of ideas of which it itself was made. But more technically speaking,

145
00:15:55,720 --> 00:16:00,600
you could say that it happened to promote the production of source code of which it itself was

146
00:16:00,600 --> 00:16:06,440
made. And then the same thing happened that happened in the primordial soup. Whenever that

147
00:16:06,440 --> 00:16:12,440
catalyst happened to become a little more targeted, it produced more of its components more

148
00:16:12,520 --> 00:16:18,040
faithfully, until at some point you could say it was targeted to meaningfully call it a replicator.

149
00:16:19,160 --> 00:16:24,200
Except this time it's not a molecule that self replicates. It's not a material, a physical thing.

150
00:16:24,200 --> 00:16:28,760
It's an abstract thing. It's an idea. And this thing now replicates over and over

151
00:16:29,640 --> 00:16:33,720
during that single organism's lifetime. So in addition to biological evolution that we have

152
00:16:33,720 --> 00:16:38,680
happening, we now have what we could call mental evolution happening during that single organism

153
00:16:38,680 --> 00:16:44,920
lifetime. I also call it runtime, the runtime of that software. And like I said, just like in

154
00:16:44,920 --> 00:16:50,440
the primordial soup, once you have replication, well, replication is not perfect forever. Even

155
00:16:50,440 --> 00:16:55,160
the best replicator makes mistakes every now and then. So sooner or later, variation and selection

156
00:16:55,160 --> 00:17:01,080
will kick in. And that's how you get evolution in the mind. And so that's how you get a dynamically

157
00:17:01,080 --> 00:17:07,400
changing idea pool, much like you have a dynamically changing gene pool in nature.

158
00:17:08,360 --> 00:17:15,000
And this is how this organism can now create new knowledge, knowledge that was not genetically

159
00:17:15,000 --> 00:17:21,800
given. And that's something people do all the time. Everyone in this column is doing it right now.

160
00:17:21,800 --> 00:17:27,880
And I believe that this, what I've laid out is the underlying logic that allows them to do so.

161
00:17:28,120 --> 00:17:33,800
So we've solved now the, I mean, hopefully, we've solved the problem of how people evolved.

162
00:17:34,680 --> 00:17:41,080
And the second problem also, which is where do conjectures come from? I think they're just

163
00:17:41,080 --> 00:17:46,360
the result of a long string of ideas replicating them perfectly, accidentally morphing into a new idea.

164
00:17:49,560 --> 00:17:53,560
And we can also explain now how memory works. Memory,

165
00:17:54,280 --> 00:17:59,960
as with so many things of the mind that are unfortunately explained on the level of the brain,

166
00:18:00,840 --> 00:18:07,880
memory is often explained on the level of neurons. And so I'm going to butcher it probably, but

167
00:18:07,880 --> 00:18:15,800
one idea that I've heard many times is that one set of neurons is said to encode one idea. And

168
00:18:15,800 --> 00:18:21,480
then if that set is wired to another bundle of neurons, then whenever one thinks of one,

169
00:18:22,440 --> 00:18:25,640
one also thinks of the other. So this is how associative memory is explained.

170
00:18:26,440 --> 00:18:31,000
And also they say that the more you think of those ideas, the stronger the physical connection

171
00:18:31,000 --> 00:18:39,000
that these neurons get. I think that's completely false. I think it's reductionist. And it also

172
00:18:39,000 --> 00:18:46,600
sounds a bit like there might just be a tinge of Lamarck's use and disuse theory sprinkled in there.

173
00:18:47,320 --> 00:18:54,360
But I think the reason that can't be true is that we know from computational universality,

174
00:18:54,360 --> 00:18:58,280
here I'm influenced by David Deutsch again, that we know from computational universality

175
00:18:58,280 --> 00:19:04,200
that we could simulate a mind on a computer, including that mind's memory, even if that

176
00:19:04,200 --> 00:19:11,720
computer hardware doesn't have any neurons. So you don't need neurons. We need to explain

177
00:19:12,360 --> 00:19:18,040
memory on the appropriate level of emergence. And that is software, not hardware.

178
00:19:21,480 --> 00:19:26,200
And I think with this approach, the new Darwinian approach with the replicator approach,

179
00:19:27,800 --> 00:19:34,280
we can answer the question of how memory works. In the biosphere, some species survive

180
00:19:34,280 --> 00:19:38,440
for much longer than others, although it's really the genes that we're concerned with,

181
00:19:38,440 --> 00:19:45,320
not with the animals themselves. And I think the same is true in a mind. Some self-replicating

182
00:19:45,320 --> 00:19:52,680
ideas just manage to stick around longer in that mind's idea pool. And those replicators that are

183
00:19:52,680 --> 00:19:58,440
longer lived than others and happen to encode events from the past, those are the ones that we call

184
00:19:58,440 --> 00:20:03,640
memories. But there's nothing else that's special or different about them that separates them from

185
00:20:03,640 --> 00:20:09,960
other ideas. I think memories are usually thought of as a special class of idea. I certainly used

186
00:20:09,960 --> 00:20:15,960
to think of them that way, but I think that's wrong. And then when a certain population of ideas

187
00:20:15,960 --> 00:20:23,480
and coding memory dies out, let's say because they can't compete with rival ideas in that idea

188
00:20:23,480 --> 00:20:31,000
pool, then that's just what it means to forget something. And we can also explain why memories

189
00:20:31,000 --> 00:20:35,640
are so unreliable. It's because replication isn't perfect and mutations happen eventually.

190
00:20:36,360 --> 00:20:43,800
So memory is just an emergent phenomenon of that underlying pool of self-replicating ideas.

191
00:20:43,800 --> 00:20:49,080
And memory being unreliable is just a special case of knowledge being unreliable.

192
00:20:52,840 --> 00:20:57,640
And now just some small pointers for when it comes to consciousness. Why are we conscious

193
00:20:57,640 --> 00:21:05,240
of some things and not others? Popper conjectured that consciousness has to do with disappointed

194
00:21:05,240 --> 00:21:10,120
expectations. I think this was in his book, Objective Knowledge. If I recall correctly,

195
00:21:10,120 --> 00:21:17,240
he gave this example of walking up a flight of stairs. And if you get to the top and you think

196
00:21:17,240 --> 00:21:22,760
that there's one more step, but there isn't, we've all been there, it feels very weird. And you,

197
00:21:23,720 --> 00:21:30,440
not only are you surprised at that sensation, but you also realize that you had an expectation

198
00:21:30,440 --> 00:21:36,040
that there was another step. And you wouldn't have realized that if there had been another step.

199
00:21:37,080 --> 00:21:42,200
So this is what why Popper argues that consciousness may have to do with disappointed

200
00:21:42,200 --> 00:21:52,120
expectations. Something I'd like to add to that is that when we, for example,

201
00:21:52,680 --> 00:21:58,920
when we're children and we learn how to ride a bike, initially this process is a very

202
00:21:58,920 --> 00:22:05,320
conscious, effortful process. We're very aware of everything we're doing. We're aware of the

203
00:22:05,320 --> 00:22:10,360
peddling. We're aware of keeping our balance. We're aware of steering and everything because

204
00:22:10,360 --> 00:22:16,920
all of this requires error correcting. But as you get better at riding your bicycle,

205
00:22:17,080 --> 00:22:23,800
as you get better at riding your bicycle, you become less aware of those things.

206
00:22:24,840 --> 00:22:30,040
Until now, when I ride my bike, I don't really know how I do it anymore. I just do it,

207
00:22:30,040 --> 00:22:35,480
but I couldn't tell you how I keep my balance, for example. And that allows me to free up my

208
00:22:35,480 --> 00:22:41,000
attention and focus it on the road, for example, so that I can avoid potholes or avoid accidents.

209
00:22:41,800 --> 00:22:47,800
So it seems to me that consciousness has to do with error correction, generally.

210
00:22:48,760 --> 00:22:56,280
And I'd also like to add this observation that I think what's curious is that despite the mutations

211
00:22:56,280 --> 00:23:02,360
of ideas that we know must be happening in our minds because mutations are inevitable,

212
00:23:02,360 --> 00:23:09,480
we never seem to be aware of any junk ideas. But if mutations are usually detrimental,

213
00:23:09,880 --> 00:23:14,280
and only rarely beneficial, that must mean that at any given moment, there are probably

214
00:23:14,280 --> 00:23:20,920
many of these junk ideas in our minds, but we're never aware of them. I think that's curious.

215
00:23:20,920 --> 00:23:28,600
So I conjecture that maybe a necessary condition for something to enter our consciousness

216
00:23:29,480 --> 00:23:35,960
is that it be sufficiently adapted by some yet to be defined criterion. Now, to be clear,

217
00:23:36,920 --> 00:23:42,440
none of this explains consciousness by any means, and it's not meant to. But these are just some

218
00:23:42,440 --> 00:23:51,880
pointers. Something else that I think this approach allows us to do is we can start thinking, taking

219
00:23:54,760 --> 00:23:59,400
maybe what Richard Dawkins would have called the genes I view, and we can take the

220
00:24:00,360 --> 00:24:07,240
ideas view, ideas I view, I guess it would be called, and think about what it would be like for

221
00:24:07,240 --> 00:24:12,600
an idea to live in that self-replicating pool of ideas. And we can think about different

222
00:24:12,600 --> 00:24:18,680
replication strategies that these ideas may have and what effect this could have on the mind.

223
00:24:20,840 --> 00:24:26,520
So now I do want to take it back to the level of memes for a moment. Dave, do I just conjecture

224
00:24:26,520 --> 00:24:31,160
that there are two different replication strategies for memes, static and dynamic.

225
00:24:32,520 --> 00:24:40,680
And I quote from the beginning of Affinity, a dynamic meme is an idea that relies on the

226
00:24:40,680 --> 00:24:47,400
recipient's critical faculties to cause itself to be replicated. So these are ideas that,

227
00:24:47,400 --> 00:24:53,480
broadly speaking, help progress and they help their holder make progress. They can at least.

228
00:24:54,440 --> 00:25:00,760
And opposed to that are static memes or anti-rational memes. And those are quote,

229
00:25:01,320 --> 00:25:05,560
an anti-rational meme is quote, an idea that relies on disabling the recipient's critical

230
00:25:05,560 --> 00:25:13,640
faculties to cause itself to be replicated. So these are ideas that prevent criticism of themselves

231
00:25:14,440 --> 00:25:18,040
and thereby prevent their holder from making much progress. And so

232
00:25:18,600 --> 00:25:25,080
the David Deutsch calls the societies that are dominated by either kind of those memes,

233
00:25:25,080 --> 00:25:30,440
static versus dynamic societies. And so static societies, he says, are the kinds of societies

234
00:25:30,440 --> 00:25:35,640
which rarely ever change on timescales that the people living in those societies could notice.

235
00:25:37,160 --> 00:25:41,800
Whereas the dynamic societies change rapidly and they can make rapid progress because they're

236
00:25:41,880 --> 00:25:47,800
dominated by dynamic means. So those are the memes. But what I wonder is if we could

237
00:25:48,680 --> 00:25:54,600
introduce the same replication strategies inside of minds. So instead of the level of society,

238
00:25:55,320 --> 00:26:00,280
we're now looking at the level of the mind with ideas being the individual actors, not people.

239
00:26:02,360 --> 00:26:07,000
Or ideas, or I should say the self replicating ideas in a mind being the actors, not memes being

240
00:26:07,000 --> 00:26:15,160
the actors. And so that could then lead us to identify, say, certain minds as static

241
00:26:15,160 --> 00:26:21,720
and certain minds as dynamic or more or less of one of the other. And then we could think about

242
00:26:21,720 --> 00:26:26,600
how static minds differ from dynamic minds and maybe what that would mean for mental ailments

243
00:26:26,600 --> 00:26:34,600
and their alleviation. Given the damage that static memes can do to static societies,

244
00:26:35,560 --> 00:26:41,400
or societies, I would not be surprised if static ideas inside a mind can also do great damage to

245
00:26:41,400 --> 00:26:46,520
that mind. And I would expect a static mind to have a much harder time making progress than a

246
00:26:46,520 --> 00:26:56,440
dynamic mind. And perhaps, just like a static society, a mind that's dominated by static ideas

247
00:26:56,440 --> 00:27:02,840
would be overly concerned with faithfully enacting some lifestyle and focusing too much on prevention

248
00:27:02,840 --> 00:27:07,880
strategies, which isn't sustainable. I'm borrowing here from Dave Deutsche again, which isn't

249
00:27:07,880 --> 00:27:11,800
sustainable because the mind would eventually encounter a problem that overwhelms completely.

250
00:27:14,600 --> 00:27:19,640
So that's just an idea to play with. Maybe we can find something interesting there

251
00:27:19,640 --> 00:27:25,800
when it comes to replication strategies inside minds. And then one last thing that I'd like to

252
00:27:25,800 --> 00:27:36,360
mention when it comes to the approach itself is another reason or another explanation for why

253
00:27:37,080 --> 00:27:43,640
you cannot download an idea from one mind to another, which is a direct conclusion from Popper's

254
00:27:44,920 --> 00:27:48,360
critique of what he called instruction from without. It's not like we need another explanation,

255
00:27:48,360 --> 00:27:53,720
but I think this maybe illustrates a little more. So as a thought experiment,

256
00:27:54,280 --> 00:28:00,440
let's say that we have the technology to read people's minds or read their ideas somehow.

257
00:28:00,440 --> 00:28:03,720
It doesn't matter how it works, but let's say you somehow connect their brain to a computer

258
00:28:03,720 --> 00:28:09,000
interface, and then the program in that interface parses all the ideas in that brain and presents

259
00:28:09,000 --> 00:28:15,160
their source code to you. So you could read them technically. And let's just say that's a given.

260
00:28:15,160 --> 00:28:20,200
And let's say that you could then, via the same interface, you could connect to another person's

261
00:28:20,840 --> 00:28:27,000
brain, and you could just copy-paste the idea from one brain to another. So technically,

262
00:28:28,680 --> 00:28:34,360
couldn't we say that that would constitute a successful download of an idea from one mind to

263
00:28:34,360 --> 00:28:42,760
another? And maybe we could, but even if so, I think it still wouldn't refute what Popper meant

264
00:28:43,480 --> 00:28:48,200
when he said that instruction from without is impossible. And I think the reason is that

265
00:28:48,440 --> 00:28:53,240
the transferred idea, once you copy and paste that idea, it most likely will not make it in the new

266
00:28:53,240 --> 00:28:58,920
mind. It's a bit like taking a penguin from an arctic on a place you get in the African jungle.

267
00:28:59,880 --> 00:29:05,880
That penguin is not going to survive, because I think our minds are all extremely different,

268
00:29:05,880 --> 00:29:11,240
just like extremely different ecosystems. So placing an idea from one mind is not going to

269
00:29:11,240 --> 00:29:17,320
make it. We shouldn't expect it to survive in the other mind. The idea, if we want to

270
00:29:17,320 --> 00:29:24,920
get the second mind to have that idea, a much better approach, I think, is to get that mind to

271
00:29:24,920 --> 00:29:34,680
evolve the idea itself, in other words, through persuasion. So this is the new Devonine approach,

272
00:29:34,680 --> 00:29:42,520
more or less, in a nutshell. What I'd like to do is respond to some criticism that the approach

273
00:29:42,840 --> 00:29:53,080
has received. And I'll start with Ella Hepner, who most notably has offered several different

274
00:29:53,080 --> 00:30:02,600
criticisms of the theory. One such criticism says that replication is wasteful. It's computationally

275
00:30:02,600 --> 00:30:11,480
wasteful. And so there wouldn't have been enough memory in our early ancestors' brains for ideas

276
00:30:11,480 --> 00:30:17,240
to replicate. And now I mean memory in the sense of storage space, not in the sense of remembering

277
00:30:17,240 --> 00:30:25,720
things. So my response to that is I'm not sure that that is true. Many complex animals already seem

278
00:30:25,720 --> 00:30:33,000
to have enough spare memory to store additional information during their lifetimes. And in particular,

279
00:30:33,000 --> 00:30:39,960
our ancestors must have had enough memory, at least memory that was free at the time of birth,

280
00:30:40,840 --> 00:30:46,360
for them to copy and store relatively complex memes, at least complex compared to other animals'

281
00:30:46,360 --> 00:30:51,960
memes. Now to be clear, they didn't replicate those memes creatively because this is pre

282
00:30:52,920 --> 00:30:58,200
the evolution of minds. But as David Deutsch points out in the beginning of infinity,

283
00:30:58,200 --> 00:31:03,800
meme evolution already drove the evolution of our ancestors. So there was already selection pressure

284
00:31:04,520 --> 00:31:10,040
favoring the development of ever more free memory in our ancestors' brains so that they

285
00:31:10,040 --> 00:31:14,200
could copy ever more complex memes. And that would have been space that our ancestors'

286
00:31:14,920 --> 00:31:22,520
self-replicating ideas could have used. On the note of memory, I would think that

287
00:31:23,480 --> 00:31:30,920
biological evolution, even long before the evolution of people, would have discovered

288
00:31:30,920 --> 00:31:36,040
basic memory management solutions, such as garbage collection, for example. Garbage collection,

289
00:31:37,400 --> 00:31:41,080
I don't have a computer science degree or anything, but my basic understanding is that garbage

290
00:31:41,080 --> 00:31:49,400
collection is a way to monitor a program's memory usage and monitor its memory pressure.

291
00:31:50,200 --> 00:31:54,920
And if you only have so much memory available on your computer and your program is using,

292
00:31:54,920 --> 00:32:00,840
say, 95% of it, then the garbage collector will go in and look for any stale references

293
00:32:00,840 --> 00:32:04,680
to data that you're not using anymore, that the program isn't using anymore, and then it just

294
00:32:04,680 --> 00:32:09,960
deletes those references. Though I may be butchering now, but I think that's the gist of it.

295
00:32:11,480 --> 00:32:17,400
And I would expect that evolution would have stumbled upon something like that much earlier,

296
00:32:17,400 --> 00:32:24,760
because it seems to me that any organism that can store additional information during its lifetime

297
00:32:24,760 --> 00:32:28,840
would eventually run into memory pressure issues. And so evolution must have evolved,

298
00:32:28,840 --> 00:32:34,360
there must have been a solution to this problem. Also, these increases in memory that I think are

299
00:32:34,360 --> 00:32:41,400
needed for those self-replicating ideas, they would have happened very gradually. So I'm not

300
00:32:41,400 --> 00:32:46,440
suggesting that once self-replicating ideas came on the scene within minds that suddenly brains

301
00:32:46,440 --> 00:32:51,480
had plenty of extra storage, certainly possible that initially there wasn't all that much extra

302
00:32:52,200 --> 00:33:00,120
memory to go around, but maybe just enough to help that adaptation spread. And then there was

303
00:33:00,120 --> 00:33:04,280
selection pressure, biological selection pressure to increase memory in humans brains if that

304
00:33:04,280 --> 00:33:10,120
adaptation was helpful enough. By some accounts, there was a large and relatively sudden increase

305
00:33:11,240 --> 00:33:15,720
in memory in our evolutionary history, and I've heard theories that attribute that to a change

306
00:33:15,720 --> 00:33:21,320
in diet or upright posture or opposable thumbs and all that stuff, but it could also be possible

307
00:33:21,320 --> 00:33:26,280
that that increase in memory in our brains was caused by the selection pressure I've just described.

308
00:33:29,400 --> 00:33:39,080
And lastly, for this particular point, the claim that replication is wasteful or redundant

309
00:33:41,400 --> 00:33:47,000
in the sense of being unnecessary, it reminds me a bit of, although this criticism is not meant

310
00:33:47,720 --> 00:33:52,440
as a creationist standpoint, but it reminds me a bit of what a creationist might say about

311
00:33:52,440 --> 00:33:59,000
replication and biological evolution. A creationist might think that genes carrying

312
00:33:59,800 --> 00:34:04,520
the same knowledge millions of times over in each organism is unnecessary. To them, it might seem

313
00:34:05,320 --> 00:34:12,200
much more parsimonious and efficient to simply say that a single entity, God, for example,

314
00:34:12,200 --> 00:34:16,040
contains a single copy of all the knowledge that's required to create each organism,

315
00:34:16,040 --> 00:34:18,280
and then each organism is just the result of him doing that.

316
00:34:22,360 --> 00:34:27,640
Okay, then another criticism that Ella has offered is the self-replicating ideas. They sound

317
00:34:27,640 --> 00:34:31,880
dangerous. They could have led to all sorts of dangerous behavior in our ancestors,

318
00:34:32,840 --> 00:34:39,640
and so biological evolution would have selected against them. I don't think this is true either.

319
00:34:40,600 --> 00:34:49,400
I think animals around us are evidence that they contain sophisticated knowledge with

320
00:34:49,400 --> 00:34:55,640
enough reach to guard against unwanted behavior, for example,

321
00:34:55,640 --> 00:35:01,480
being stuck in an endless loop or something. I've tried this. Even a well-behaved dog won't repeat

322
00:35:01,480 --> 00:35:08,200
a trick forever when instructed to. It'll only do it so many times. It just stops at some point.

323
00:35:08,920 --> 00:35:17,640
There seem to be built-in criteria that can override erroneous behavior. It seems to me that

324
00:35:17,640 --> 00:35:25,800
biological evolution has already has built-in safeguards. Self-replicating ideas wouldn't

325
00:35:25,800 --> 00:35:29,480
automatically have led to ever repeating behaviors or something that would

326
00:35:30,520 --> 00:35:36,120
certainly cause death for the organism. I think actually the opposite is the case.

327
00:35:37,080 --> 00:35:44,520
Self-replicating ideas were then and are beneficial, and they helped the genes spread

328
00:35:44,520 --> 00:35:53,640
that code for them. Because whenever there was a detrimental genetic mutation present in an organism

329
00:35:54,600 --> 00:36:02,600
that led to a bad mutation in one of those organisms' ideas, it was thanks to those

330
00:36:02,680 --> 00:36:07,080
self-replicating ideas that there was an opportunity that broke in functionality

331
00:36:07,080 --> 00:36:14,920
with working functionality. Because detrimental mutations are much more numerous than beneficial

332
00:36:14,920 --> 00:36:20,440
ones, that actually means that the existence of self-replicating ideas was favored by biological

333
00:36:20,440 --> 00:36:25,720
evolution up to a rate at which detrimental mutations occur. I think there would have been

334
00:36:25,720 --> 00:36:30,520
strong selective pressure in favor of self-replicating ideas in mind because it would have helped the

335
00:36:30,520 --> 00:36:39,880
genes spread. And then Ella has offered another approach that one could implement the same

336
00:36:40,440 --> 00:36:46,840
algorithms, the same self-replicating idea pool without replication. And the idea is basically

337
00:36:46,840 --> 00:36:51,960
you just have a single instance of each idea and a score or weight that you attribute to each,

338
00:36:51,960 --> 00:36:59,560
and then increasing and decreasing the weight as you go. And David George recently independently

339
00:36:59,560 --> 00:37:07,080
suggested this criticism as well. Now, it seems to me that denotationally that sounds

340
00:37:07,080 --> 00:37:13,640
similar to my theory, and I think Ella in particular has in mind, although she's here,

341
00:37:13,640 --> 00:37:19,640
she should correct me on this, but I think Ella in particular has in mind what's his name,

342
00:37:19,640 --> 00:37:25,400
Donald Campbell's work on evolution, that you don't really need replication so the argument

343
00:37:25,400 --> 00:37:30,040
goes, you just need variation and selection. Imperfect replication can be the source of

344
00:37:31,480 --> 00:37:40,040
variation, but it need not be. Now, I've said in the past, and I'm still of the opinion that I'm

345
00:37:40,040 --> 00:37:46,360
agnostic as to the question of necessity, but if we were to make this change, I think it would come

346
00:37:46,360 --> 00:37:52,760
at some costs. The one thing that I think the New Darwinian approach, the approach that does adopt

347
00:37:52,760 --> 00:37:58,440
replication brings to the table is that we can explain how people evolved. I'm not sure how we

348
00:37:58,440 --> 00:38:04,440
would do that, because it allows us to explain with a very simple accidental small mutation.

349
00:38:05,240 --> 00:38:11,560
I'm not sure this algorithm that would store and decrease weights, we would have to explain

350
00:38:11,560 --> 00:38:16,520
how that one came about. That seems to require a different explanation that I'm not aware,

351
00:38:16,520 --> 00:38:22,600
I'm not familiar with. And then the question is, when does it decide to mutate an idea and

352
00:38:22,680 --> 00:38:26,680
why and how, and at which location in the idea source code?

353
00:38:30,280 --> 00:38:36,280
It also seems to me that such an algorithm would make plant mutations, but in real evolution,

354
00:38:36,280 --> 00:38:47,560
mutations are not planned. So, what I like about the, what I like about my approach is that

355
00:38:48,360 --> 00:38:52,120
none of these questions need to be answered in explicit programming. These are all just

356
00:38:52,760 --> 00:38:56,680
things that automatically fall out of having self-replicating ideas.

357
00:38:58,600 --> 00:39:02,760
So, in a way, actually structurally speaking, maybe it's the pool of self-replicating ideas

358
00:39:02,760 --> 00:39:07,240
that's more parsimonious, because you don't need to explicitly program any of these other things.

359
00:39:07,480 --> 00:39:21,480
And if the goal of that approach is to, if the goal of that approach is to get rid of replication,

360
00:39:21,480 --> 00:39:27,800
because it might be wasteful or redundant or whatever, then the thing is, let's say you'd

361
00:39:27,800 --> 00:39:32,520
have one instance of an idea and you'd have a weight associated to it, and let's say at some

362
00:39:32,520 --> 00:39:38,120
point this master algorithm decides to make a copy of it and mutate it, and so at that point

363
00:39:38,120 --> 00:39:43,320
you would store a second entry, and over time you would get more and more entries in some registry

364
00:39:43,320 --> 00:39:53,720
of ideas or whatever. And I think then you would still get source code that is shared among many

365
00:39:53,720 --> 00:40:00,680
of these entries, and so it's a little bit similar to what Dawkins has pointed out with in regard to

366
00:40:01,480 --> 00:40:07,560
the cisterns and the DNA versus the genes, which he defines separately. The cisterns,

367
00:40:07,560 --> 00:40:11,640
or maybe butchering the term, but I think that's what is called the cisterns, are like the actual

368
00:40:11,640 --> 00:40:18,440
section in the genome that code for eye color say, whereas the gene he defines as the longest

369
00:40:18,440 --> 00:40:22,360
possible stretch of the genome that gets replicated over and over without changes.

370
00:40:22,920 --> 00:40:28,120
You'd get a very similar phenomenon in this registry of ideas where you'd have sections

371
00:40:28,120 --> 00:40:33,800
of those ideas that stay the same across those entries, and then that would be the replicator.

372
00:40:34,920 --> 00:40:39,240
So although the idea is set out to get rid of replication, I'm not sure it actually does.

373
00:40:41,800 --> 00:40:47,800
And I think what's nice about adopting replication is that it gives us consistency

374
00:40:47,800 --> 00:40:52,600
with other evolutionary phenomena that we know exist and we know are powered by replication,

375
00:40:52,600 --> 00:40:57,400
that's biological evolution and meme evolution, and I think that consistency is valuable

376
00:40:58,360 --> 00:41:01,560
because it means that we can share knowledge between these different disciplines.

377
00:41:04,120 --> 00:41:08,280
So I like to think that this new Darwinian approach has a unifying character and I think

378
00:41:08,280 --> 00:41:16,680
that's worth keeping. And we couldn't, excuse me, we couldn't investigate the static and dynamic

379
00:41:16,680 --> 00:41:23,080
replication strategies in the mind anymore without replication. Those really are replication

380
00:41:23,080 --> 00:41:31,080
strategies and they don't work without replication. And then the last criticism that I'm familiar with

381
00:41:31,080 --> 00:41:39,400
that Ella has offered is that it's unclear how the mind creates and discovers problems

382
00:41:39,400 --> 00:41:44,600
and contradictions and that that's the driving force of creativity. So that's a really important

383
00:41:44,600 --> 00:41:52,520
thing to answer. And I agree with that. So I want to clarify. My current idea is that

384
00:41:53,320 --> 00:41:59,560
when there are populations of ideas that are competing in a mind, especially when they compete

385
00:41:59,560 --> 00:42:06,920
fiercely over resources, memory, for example, then that leads to cognitive dissonance that

386
00:42:06,920 --> 00:42:11,960
the cognitive dissonance that we experience when we experience a problem. And depending on the nature

387
00:42:11,960 --> 00:42:18,680
or the fierceness of the competition that we may, a problem may seem interesting or annoying or

388
00:42:18,680 --> 00:42:24,600
downright depressing, like I said, depending on the nature of the competition. And then when we solve

389
00:42:24,600 --> 00:42:30,440
such a conflict, it could mean, for example, that there's a third population of ideas that

390
00:42:30,440 --> 00:42:36,680
has evolved and operates as a sort of mediator, enabling the two conflicting populations to

391
00:42:36,680 --> 00:42:43,800
coexist peacefully, or it could fight and win against both populations and largely replace them.

392
00:42:43,800 --> 00:42:49,560
I think that's what might be happening when you have one theory that supersedes and explains

393
00:42:49,560 --> 00:42:53,240
two conflicting ones, which then live on in the new theory as approximations.

394
00:42:55,560 --> 00:42:58,840
Or it could just enable one of the two to win over the other.

395
00:43:00,920 --> 00:43:07,000
And then recently, David Deutsch suggested that we need not mimic how biological evolution

396
00:43:07,000 --> 00:43:12,120
created creativity, the same way that planes don't need to mimic birds to fly.

397
00:43:12,920 --> 00:43:17,480
And I agree with that. I just, I don't know of any other way yet.

398
00:43:21,560 --> 00:43:28,200
And, yeah, and I forget if I mentioned it, but he also just like, just like Ella Hepner

399
00:43:28,200 --> 00:43:34,280
suggested that replication would be inefficient, that it wouldn't be necessarily the approach that

400
00:43:34,280 --> 00:43:42,440
a programmer would take. So all these criticisms would be very helpful and have motivated me to

401
00:43:42,440 --> 00:43:49,240
think more about this idea. But what strikes me is that I think none of these are conclusive

402
00:43:49,240 --> 00:43:55,160
criticisms. And this brings me to something that many of them have in common. I think they're

403
00:43:55,160 --> 00:44:01,240
along the lines of, they say something like self replicating ideas are not necessary to explain

404
00:44:01,240 --> 00:44:07,880
the mind. And like I said, I agree with that. In fact, I'm pretty certain that my, that my

405
00:44:07,880 --> 00:44:20,520
approach is wrong. What I'd like to find out is why. And I think the, a, an avenue that I would

406
00:44:20,520 --> 00:44:29,240
find more, more productive is what, what is an explanation for why self replicating ideas

407
00:44:29,240 --> 00:44:35,240
cannot be part of the mind, not why they need not. And like a real refutation of the idea

408
00:44:35,240 --> 00:44:40,440
would be extremely helpful. That also brings me to what could change my mind about this,

409
00:44:40,440 --> 00:44:44,920
this approach. That is one of them, just just an explanation of why it cannot involve self

410
00:44:44,920 --> 00:44:49,880
replicating ideas. And then also an internal contradiction or something like that that I'm

411
00:44:49,880 --> 00:44:54,040
unable to fix. At least that would, that would definitely constitute a big problem that I would

412
00:44:54,040 --> 00:45:01,160
need to solve. I should point out that the idea that self replication, for example, would have hurt

413
00:45:01,960 --> 00:45:10,200
the genes encoding that it would have hurt genes in biological evolution. I think that was a

414
00:45:10,200 --> 00:45:15,800
candidate for why creativity cannot involve several self replicating ideas. So I think something

415
00:45:15,800 --> 00:45:22,600
like that would be promising. And then lastly, some outstanding problems. There are still big

416
00:45:22,600 --> 00:45:28,600
unknowns left. Like I said, consciousness is the big one. Something that I've been thinking about

417
00:45:28,600 --> 00:45:34,840
recently is that when you, the, the approach kind of goes along the lines of, well, as long as you

418
00:45:34,840 --> 00:45:39,880
have self replicating ideas, you kind of get variation automatically eventually because the

419
00:45:39,880 --> 00:45:46,440
mind is messy. And therefore you also get selection automatically because different variants spread

420
00:45:47,400 --> 00:45:53,000
with different rates at different rates. But if you actually write a self replicating computer

421
00:45:53,000 --> 00:45:58,040
program, they're also called a quine. They don't ever mutate. You can run them a billion times in

422
00:45:58,040 --> 00:46:06,920
a row. They don't mutate unless you force them to, but then mutations are planned. So that tells

423
00:46:06,920 --> 00:46:11,320
me that the self replicating programs that people have written are too good at replication for

424
00:46:11,320 --> 00:46:18,360
evolution to occur. And then there's the problem with evolutionary algorithms generally that David

425
00:46:18,360 --> 00:46:22,920
Dorch has written about in the beginning infinity, which is that as a programmer, you can't really

426
00:46:22,920 --> 00:46:27,960
tell if the knowledge that you find in the program after running it is knowledge that the

427
00:46:27,960 --> 00:46:32,040
program created itself, or if that is knowledge that you as a program just happen to put into it

428
00:46:32,040 --> 00:46:37,640
without realizing it, you leak that knowledge into the program. And so you can't really judge

429
00:46:37,640 --> 00:46:43,800
whether the program is creative. You'd need to know to make that call. So we'd need to find a

430
00:46:43,800 --> 00:46:52,600
solution for that before even trying to implement this theory. And yeah, so that is the approach.

431
00:46:53,720 --> 00:46:59,560
And I'd be ready to take questions if there are any. Great. Thanks so much for the talk.

432
00:46:59,880 --> 00:47:07,080
Um, yeah, so we have about 45 minutes of questions. And I think what we'll do is,

433
00:47:07,640 --> 00:47:14,760
Dennis, if you could first close the presentation, because I think it's nice.

434
00:47:18,440 --> 00:47:19,160
Go back to

435
00:47:22,120 --> 00:47:27,960
And then I will open with a question. And then anyone who wants to answer,

436
00:47:28,040 --> 00:47:35,320
who wants to ask a question can do so afterwards either by raising your hand, as I see David has

437
00:47:35,320 --> 00:47:44,120
done, or by typing in the chat. Yeah, so the first question I had was, I really liked the idea set

438
00:47:44,120 --> 00:47:51,960
out. And I like, especially explanation of memory. And this idea that memories are replicators and

439
00:47:52,440 --> 00:48:00,440
the memories that you retain, other ones that have replicated best. But memory in my computer

440
00:48:00,440 --> 00:48:07,240
is just accessible, like the files in my computer don't replicate. If I require an old file,

441
00:48:08,120 --> 00:48:13,960
I get an exact copy of that file, and I just retrieve it as it was when I first stored it.

442
00:48:15,240 --> 00:48:20,440
Why do you think that minds store memories differently? Why do you think that they will

443
00:48:20,440 --> 00:48:30,120
have to replicate? Yes. I think it's sort of a quirk, maybe of the English language that we

444
00:48:30,120 --> 00:48:35,720
use the term memory to refer to both of these phenomena, even though I think they describe

445
00:48:35,720 --> 00:48:42,120
different things. Memory in the second sense, in the sense that your computer uses,

446
00:48:42,840 --> 00:48:49,240
actually our brains have too, because they need to store data in that sense. And that

447
00:48:49,240 --> 00:48:54,360
is just the plain old memory storage that you've described, and nothing changes. Like if I store

448
00:48:55,080 --> 00:48:59,880
very rarely, because our hard drives are very reliable. So yeah, if you store a file on your

449
00:48:59,880 --> 00:49:05,640
computer, it just stays there and it doesn't replicate. So this is, I think that is like a

450
00:49:05,640 --> 00:49:11,000
lower level kind of memory phenomenon, like the direct interaction with the hardware for how to

451
00:49:11,000 --> 00:49:16,440
store information. The kind of memory in the sense of remembering things,

452
00:49:19,480 --> 00:49:24,440
I think it's just a different phenomenon. And that is the phenomenon that involves

453
00:49:25,080 --> 00:49:29,880
the self-replicating ideas. So I would just distinguish between the two.

454
00:49:30,760 --> 00:49:36,440
Very nice. Okay, thanks for your answer. Then I see that David has a raised hand. David,

455
00:49:36,440 --> 00:49:43,960
go ahead and ask a few questions. Hi. Well, I too found that very persuasive,

456
00:49:43,960 --> 00:49:52,680
and I also liked the way that it kind of explains features of the would otherwise

457
00:49:52,680 --> 00:50:00,760
perhaps seem accidental features of the human mind and memory as well. I just,

458
00:50:00,760 --> 00:50:13,560
I can't quite see the overall picture though. So in RNA world, there were, as it were,

459
00:50:15,000 --> 00:50:26,840
genes, but no organisms. Right. And in the brain, in your picture, I think there's also

460
00:50:27,640 --> 00:50:36,280
genes, but no organisms. Is that right, first of all? Yes, that is right. I've wondered if maybe

461
00:50:36,280 --> 00:50:42,680
sometimes ideas, as they get more complex, as they replicate in the mind, if they stumble upon

462
00:50:43,480 --> 00:50:47,720
a replication strategy, something like the genes stumbled upon when they invented organisms.

463
00:50:47,960 --> 00:50:58,600
Well, yeah. Well, so the thing is that if you imagine a huge soup of RNA, then it doesn't have

464
00:50:58,600 --> 00:51:05,000
an outside world. That is the only outside world it interacts with is other RNA molecules.

465
00:51:05,480 --> 00:51:20,760
Things started happening in RNA world to make more sophisticated replicators when RNA started

466
00:51:20,760 --> 00:51:32,120
manipulating things that aren't RNA and started, for example, making enzymes, making proteins,

467
00:51:32,760 --> 00:51:40,520
and so on. So I would guess that there's only so far you can get by twisting RNA into different

468
00:51:40,520 --> 00:51:48,600
shapes and trying to get them to produce more of those. But at some point, they invented enzymes.

469
00:51:48,600 --> 00:51:58,680
And then, from then on, there's phenotypes. There's organisms. I, I, I don't know, I don't know,

470
00:51:58,680 --> 00:52:09,400
there's organisms. I, I'm not, I'm not asking how you explain the origin of species, you know,

471
00:52:09,400 --> 00:52:18,280
why a bunch of DNA just moves along through the world together. It's not that. It's, it's the,

472
00:52:19,080 --> 00:52:28,840
it's, it's the, when it gets to encounter the non RNA world. And I think most selection in the

473
00:52:28,840 --> 00:52:41,160
biosphere and most selection in the brain, I'm guessing, is caused by problems to do with the

474
00:52:41,640 --> 00:52:48,600
non RNA, to do with the outside world. It's the outside world that causes things in the inner

475
00:52:48,600 --> 00:52:56,840
world to come into conflict. Right. Okay. So, you know, I'm just kind of asking or asking for

476
00:52:56,840 --> 00:53:02,200
comment or whatever. I mean, I don't have any objection to the idea. I think it's a great idea.

477
00:53:02,200 --> 00:53:09,720
And, you know, please pursue it. Great. No, you know, I think that is a really helpful

478
00:53:09,800 --> 00:53:14,120
pointer that is, I don't have an answer off the bat for you for that. I think that's something

479
00:53:14,120 --> 00:53:20,120
I'd very much like to explore. So I'll, I'll keep thinking about it. Okay. Can't ask for more.

480
00:53:22,120 --> 00:53:29,560
Oh, then Timothy, do you have a race time? Yeah. Hi, Dennis. Thanks for the talk. I really enjoyed

481
00:53:29,560 --> 00:53:37,880
it. Hi. I think, I think one term that I'm sort of stumbling over in this framework is accidental.

482
00:53:38,600 --> 00:53:45,640
How do you think about our sort of effectiveness at problem solving or the kind of deliberateness

483
00:53:45,640 --> 00:53:49,880
of it in terms of this accidental idea creation?

484
00:53:53,080 --> 00:53:58,760
Do I understand correctly that you're trying to square the purposeful thinking or the, at

485
00:53:58,760 --> 00:54:03,960
least the purposeful, the feeling that we get that we're purposefully solving a problem, say,

486
00:54:04,040 --> 00:54:08,200
with the notion that ideas mutate accidentally? Am I understanding that right?

487
00:54:08,200 --> 00:54:13,640
Yeah. I mean, not just the feeling, the fact that we actually approach problems and solve them.

488
00:54:14,760 --> 00:54:25,320
Right. Yeah. I do, I've thought about this too, and I think it's a really interesting

489
00:54:25,320 --> 00:54:35,800
thing to explore. The, I think those things can be squared. And I think the way we introspect and

490
00:54:35,800 --> 00:54:42,840
look at the way we solve problems is often deceptive. And this has led to ideas such as induction,

491
00:54:42,840 --> 00:54:48,840
for example, where people even though they could not have induced from experience,

492
00:54:48,840 --> 00:54:56,600
that's what they thought they did. So people can be mistaken about what they, what they're doing

493
00:54:56,600 --> 00:55:06,520
when they solve problems. And the thing that ideas happen to morph into a new conjecture that,

494
00:55:06,520 --> 00:55:13,720
that solves a problem, say, I think isn't really at odds with purposeful problem solving.

495
00:55:13,720 --> 00:55:19,000
We, we purpose, we want to solve problems. That means we have ideas that we would like when

496
00:55:19,000 --> 00:55:25,000
there's a conflict, we would like to solve it. And that idea exists even before we encounter a

497
00:55:25,000 --> 00:55:32,600
solution to a problem. But the, the solution to the problem itself, we can't really move toward

498
00:55:32,600 --> 00:55:37,960
in a targeted fashion because we don't know yet what it might be. It's unknowable in advance. We

499
00:55:37,960 --> 00:55:42,360
only know it once we've evolved it. And at that point, we just kind of happened to know it.

500
00:55:43,320 --> 00:55:48,920
I think that's just true of evolution generally is whether it's the gene or the self-replicating

501
00:55:48,920 --> 00:55:55,560
idea in the mind. There's only, I think Popper called them happy accidents. There's only happy

502
00:55:55,560 --> 00:56:03,800
accidents, or sometimes not so happy ones. That's, I think, all we have. We can't think of a solution

503
00:56:03,800 --> 00:56:11,720
before we, before that idea mutates into one. Does that make sense? I'm not sure that I,

504
00:56:11,720 --> 00:56:13,960
that I really put my finger on your, on your question.

505
00:56:18,760 --> 00:56:24,680
I am not hearing the participants. Sorry, sorry, that, that, that does make sense. I'm not,

506
00:56:25,400 --> 00:56:29,480
yeah, I'm not sure if it, it quite feels the gap for me, but I'm,

507
00:56:32,440 --> 00:56:37,400
yeah, I'm not sure of a better way of framing it, but thanks for the, thanks for your response.

508
00:56:37,400 --> 00:56:45,960
Okay, sure. Okay, then I see you have two more raised hands. I don't know who was first. I think

509
00:56:45,960 --> 00:56:51,720
I'll go with Paarek. I, I think he must be asking your name. So, so go ahead.

510
00:56:51,720 --> 00:56:56,840
That's all, yeah. That's Paarek. Thanks a million. Really enjoyed the talk, Dennis. Thanks a lot for

511
00:56:56,840 --> 00:57:04,120
it. And so yeah, my question was sort of related to the last question a little bit, but you remarked,

512
00:57:04,920 --> 00:57:12,280
I think about halfway through the talk about, there may be, because ideas will self-replicate,

513
00:57:12,280 --> 00:57:19,400
there will be these junk ideas, which are analogous in some ways to, to junk DNA. So,

514
00:57:19,400 --> 00:57:32,200
I guess my question is about the self-replication. So within this model is the replication a matter

515
00:57:32,200 --> 00:57:40,280
of degree, so that the, the idea, which isn't the junk idea, which presumably is the one that

516
00:57:40,280 --> 00:57:47,800
we're, we're then conscious of, you know, because you're, you were mentioning that we, we, there's

517
00:57:47,800 --> 00:57:54,920
a lot that we're, we're unconscious of, and those would be the, the sort of the ridiculous answers,

518
00:57:54,920 --> 00:58:00,440
let's say, to a problem or the ones that we don't get promoted into our awareness.

519
00:58:01,160 --> 00:58:09,400
But the ones we are aware of, would those be more active replicators in, in, in this way, or, or

520
00:58:09,400 --> 00:58:17,640
they're, so yeah, I guess it's sort of a two-part question. One is like, is the replication a

521
00:58:17,640 --> 00:58:25,320
matter of degree? And secondly, are the, the ones we become conscious of, the sort of better at

522
00:58:26,280 --> 00:58:32,520
causing their, their copying? Yeah, I'll, I'll, I'll try to answer the second one first.

523
00:58:33,880 --> 00:58:42,440
They, I think being better at copying oneself is one way to just retain the, the faithfulness and

524
00:58:42,440 --> 00:58:50,520
the, the coherence of an idea, for sure. So that if, if that idea wants and scare quotes to be,

525
00:58:51,160 --> 00:58:56,600
to be thought of again, one, a good way to do that is to make sure that one retains one's

526
00:58:56,600 --> 00:59:06,280
faithfulness. I could see that. Now, with regard to whether that makes it a more active replicator,

527
00:59:06,280 --> 00:59:13,560
I don't know, I could certainly imagine that there are ideas with different replication rates.

528
00:59:14,200 --> 00:59:23,320
One might replicate twice as often as the other. And if it can make up for, for, you know, the

529
00:59:23,320 --> 00:59:28,760
mutations that might be introduced during that, the, the double replication, then it's fine.

530
00:59:31,080 --> 00:59:37,720
So I guess it just depends on how reliable the replicator it is, or it is not very reliable,

531
00:59:37,720 --> 00:59:46,200
but thanks to those mutations, it creates a new idea. And then that new idea is coherent enough

532
00:59:46,200 --> 00:59:51,560
that you become aware of it. So that could also be the case. So I'm not sure that we could tie it to

533
00:59:52,280 --> 00:59:57,880
the activity of a replicator per se. With regard to your, your first question, I'm not sure what

534
00:59:57,880 --> 01:00:04,760
you mean by degree of replication. Yeah, I guess just to clarify it a little bit. So, you know, some

535
01:00:08,120 --> 01:00:12,840
questions. So to, to bring it back to the analogy that within genes, there will be,

536
01:00:14,280 --> 01:00:20,920
like the junk DNA is a replicator of sorts, although it's, it's not usually referred to

537
01:00:20,920 --> 01:00:27,320
as a replicator because it's, it's not, you know, it would contribute within its environment,

538
01:00:27,320 --> 01:00:33,880
it contributes to its copying more than, you know, a whole lot of other physical objects.

539
01:00:34,360 --> 01:00:40,840
And both it's, you know, it's, it's, for some, it's maybe not worth calling a replicator because

540
01:00:40,840 --> 01:00:47,720
it does so far less well than, than the ones we call replicators. So I was just wondering if that's

541
01:00:47,720 --> 01:00:54,360
the same sort of implication that was being drawn between the analogy of junk ideas and junk DNA,

542
01:00:54,360 --> 01:00:59,960
that they are sort of, they're still self replicating, but they're, they're less,

543
01:01:00,520 --> 01:01:08,440
less good. I got you. Yeah. No, I think, I think absolutely that could happen. Yeah, you get,

544
01:01:08,440 --> 01:01:13,240
you could through an, through mutation, you could get a junk idea that's, it's junk in the sense

545
01:01:13,240 --> 01:01:17,640
of that doesn't result in a coherent idea anymore, but it's still kind of manages to get itself

546
01:01:17,640 --> 01:01:23,080
replicated every now and then that could happen. Or sometimes the mutation might be so detrimental

547
01:01:23,080 --> 01:01:32,360
that it's just, it dies immediately. Yeah. Then I see Danny has a raised hands. Danny, go ahead.

548
01:01:34,040 --> 01:01:39,880
Hi Dennis. I really enjoyed the talk as well. And one of the things that I really like about this

549
01:01:39,880 --> 01:01:44,840
theory, which is something that I've wondered about myself in the past, is the fact that

550
01:01:45,480 --> 01:01:50,120
Popper's notion where we conjecture ideas, I've always wondered why the ideas that appear in

551
01:01:50,120 --> 01:01:56,120
our conscious mind seem to be non random. You know, so the, you know, to take your example about

552
01:01:56,120 --> 01:02:00,760
the keys, you know, some silly idea about your keys being somewhere that you've never been before,

553
01:02:00,760 --> 01:02:06,200
that idea doesn't appear in your mind. So I suppose my question would be, do you have any conjectures

554
01:02:06,200 --> 01:02:14,040
around what is happening in the mind in that moment when an idea, when that first idea appears in your

555
01:02:14,040 --> 01:02:21,800
conscious mind? So what is happening when your mind quote unquote, like selects one idea to

556
01:02:21,800 --> 01:02:25,960
present to you consciously? Have you, have you thought about that at all?

557
01:02:27,560 --> 01:02:34,840
Yeah, I have. And one idea that I've just played with is that maybe what's happening is that

558
01:02:37,160 --> 01:02:41,160
in the pocket of the population of ideas that's relevant to this problem,

559
01:02:42,040 --> 01:02:45,960
um, maybe the idea that you've left your keys on a kitchen counter

560
01:02:47,480 --> 01:02:53,480
is just very good at spreading through this pocket and it outnumbers rivals. And

561
01:02:54,600 --> 01:03:00,600
that's why it gains a foothold in that niche. And maybe that is why you become aware of it. This

562
01:03:00,600 --> 01:03:06,120
is just a, you know, purely conjectural, I'm making stuff up kind of thing. But

563
01:03:06,520 --> 01:03:13,480
um, that's an idea I've played with. So, so more broadly than it's, you think it might be possible that

564
01:03:14,040 --> 01:03:20,920
um, the distinction between, you know, the ideas in the conscious mind and the ideas in the unconscious

565
01:03:20,920 --> 01:03:28,760
mind has something to do with, we'll say the number of, of replicas of an idea or, or am I maybe

566
01:03:28,760 --> 01:03:34,040
stretching, stretching what you just said as Marlon? Well, I think it may have to do with that.

567
01:03:34,040 --> 01:03:43,160
And um, it, it may have to do with one idea outnumbering or overwhelming,

568
01:03:44,760 --> 01:03:49,080
or one population of that idea, like one set of replicas of that idea, outnumbering another

569
01:03:49,080 --> 01:03:54,360
competing set. And that's why you don't think of the other set. I think that is one, one thing.

570
01:03:54,360 --> 01:04:00,840
And then as I've mentioned, there's the thing about ideas competing fiercely and that constituting a

571
01:04:00,840 --> 01:04:06,600
problem. That is also something that become aware. I would imagine that their ideas competing all the

572
01:04:06,600 --> 01:04:12,040
time, countless ideas competing all the time in our minds, but the competition is so small and,

573
01:04:12,040 --> 01:04:18,040
you know, the conflict is so small that you don't, you don't realize it. But because the, the key,

574
01:04:18,680 --> 01:04:23,960
and the key example of the missing key, it must mean that just by virtue of you thinking and

575
01:04:23,960 --> 01:04:30,760
recognizing it as a problem, it could mean that there were two populations of ideas

576
01:04:31,560 --> 01:04:39,640
encoding, um, conflicting preferences. Say one, I want to leave, um, that is my preference,

577
01:04:39,640 --> 01:04:44,520
conflicting with the idea that I can't find my key, so I can't leave. And this, this conflict is

578
01:04:44,520 --> 01:04:49,160
both on both sides, the ideas are numerous enough that the conflict conflict is noticeable

579
01:04:49,800 --> 01:04:54,680
that you, I mean, it's a bit circular because I'm trying to explain what we notice in terms of what

580
01:04:54,680 --> 01:05:00,600
is noticeable. Um, but I'm suggesting that that is what makes it noticeable. Uh,

581
01:05:02,120 --> 01:05:05,400
if that makes, I hope that makes sense. Yeah, yeah, I know it does. Thank you. Thank you.

582
01:05:07,240 --> 01:05:15,560
Okay, I don't see other raised hands, so I will ask another question. And it has to do with

583
01:05:16,520 --> 01:05:26,200
uh, these higher level concepts. So we, we, we also, like, you, you draw this picture of, of

584
01:05:26,200 --> 01:05:35,080
single ideas, uh, evolving in a mind and, and partly you say that, for example, we

585
01:05:36,040 --> 01:05:41,320
experienced suffering as a, as a consequence of competition between ideas. Uh, and I think what

586
01:05:41,320 --> 01:05:47,160
I'm wondering is, like, where does the sense of self come in and where does, uh, this is kind of

587
01:05:47,160 --> 01:05:55,640
related to the question of where does conscious problem solving come in? And, uh, just truly,

588
01:05:55,640 --> 01:06:01,560
I mean, I can imagine there would be such an explanation of consciousness or, of, uh, maybe

589
01:06:01,560 --> 01:06:07,320
that's asking too much, but where, where do these seemingly higher level, uh, entities come in? The

590
01:06:07,320 --> 01:06:13,160
sense of self, uh, sense of direction, sense of purpose. Um, and yeah, this is kind of a vague

591
01:06:13,160 --> 01:06:18,840
question that is pointing at something. I hope it's enough. No, no, it's a great question. Um, yeah,

592
01:06:18,840 --> 01:06:24,520
I'm afraid it really is the big one. I don't have any, any good answers for you. I mean, I, I have

593
01:06:24,520 --> 01:06:30,200
thought that there, that there is a sort of meta algorithm in the mind that, that we've inherited

594
01:06:30,200 --> 01:06:36,280
from, from our ancestors that used to just be, um, responsible for interrupting, you know, bad

595
01:06:36,280 --> 01:06:42,920
loops or stuff like that. But, um, and maybe that is, because it looks over the pool of replicating

596
01:06:42,920 --> 01:06:47,240
ideas, that maybe that is why we have this, we, at least that's how we feel, that we have this

597
01:06:47,240 --> 01:06:52,920
bird's eye view of our ideas. But I don't know. Uh, I would really like to know.

598
01:06:54,120 --> 01:06:58,280
Yeah, same. But it's, yeah, as I said, it's, it's maybe too big of a question.

599
01:07:00,040 --> 01:07:04,280
I see there's more race hands now. Uh, and I think Antonio was first. So,

600
01:07:04,280 --> 01:07:11,080
Antonio, if you want to ask a question, go ahead. That's right. Thank you. Um, so I don't want to get

601
01:07:11,080 --> 01:07:19,960
into a play of definitions, but what do you mean when you say an idea? And do you differentiate

602
01:07:19,960 --> 01:07:26,200
it between the explicit content or the inexplicit content? Do you think it matters at all how they

603
01:07:26,200 --> 01:07:33,160
are instantiated in the brain? What would be like the goal of an idea? Why wouldn't want to exist

604
01:07:33,160 --> 01:07:39,720
in your brain in the first place? Because, yeah, unless you actually define what you mean by idea,

605
01:07:40,280 --> 01:07:45,720
having self-replicating ideas doesn't say much about what's going on.

606
01:07:47,480 --> 01:07:52,680
So the, the, what's throughout to me is when you said that, why would an idea want to be

607
01:07:53,320 --> 01:07:59,400
in a, in a mind? I don't think it does. Um, this, the same way that there's no gene that wants to

608
01:07:59,400 --> 01:08:03,160
be in the biosphere just kind of finds itself there. And because it's a replicator, it makes

609
01:08:03,160 --> 01:08:09,960
ever more of itself. Um, there, there, there's nothing, um, you know, the, the, on the level

610
01:08:09,960 --> 01:08:14,280
of the replicator, there's, there is no consciousness or purposeful purposefulness. So,

611
01:08:15,160 --> 01:08:21,560
um, now when it comes to what, like, how, what do these ideas actually look like? How are they

612
01:08:21,560 --> 01:08:26,040
implemented? Um, I have given this some thought now at the risk of it getting too technical.

613
01:08:26,040 --> 01:08:30,600
I think of ideas the way they're stored in the brain when it comes to source code as

614
01:08:31,400 --> 01:08:37,000
functions in the sense of the lambda calculus. And I do think there are important parallels

615
01:08:37,000 --> 01:08:41,240
between, or not just parallels, I actually think they're the same between functions in the sense

616
01:08:41,240 --> 01:08:47,320
of the lambda calculus and explanations. And so I use the term idea just for those functions.

617
01:08:48,600 --> 01:08:54,360
Um, before just those functions, I should say. Um, so that would be my technical answer to

618
01:08:54,920 --> 01:09:03,160
how our ideas implemented in the mind. Um, if that helps. Okay. If you imagine a brain,

619
01:09:04,040 --> 01:09:10,360
a human brain, okay, human baby raised without language. Okay.

620
01:09:12,200 --> 01:09:18,440
In 2021, now you can raise a human being without, say, speaking to it, to humor her, whatever.

621
01:09:19,320 --> 01:09:27,560
Um, do you think that it will contain knowledge? Will it only contain like inexplicit knowledge,

622
01:09:27,560 --> 01:09:33,560
but it will be able to move around, to hand, to eat, to feed, to interact with other human beings

623
01:09:34,680 --> 01:09:36,520
purely on an inexplicit level?

624
01:09:39,000 --> 01:09:45,160
Yeah. Um, I think so. I think, I mean, our ancestors at some point were in that stage,

625
01:09:45,160 --> 01:09:50,760
right? Before the invention of language, presumably they had the same brain, the same genes.

626
01:09:52,360 --> 01:09:59,240
Yeah. Um, no, I do think that language comes after. Um, you have creativity and that's what

627
01:09:59,240 --> 01:10:04,280
allows you to learn language. It's not the other way around. Um, so yeah, I think there's plenty

628
01:10:04,280 --> 01:10:09,880
of things that people can learn even without language and the inexplicit ideas will do.

629
01:10:10,440 --> 01:10:17,240
And what is the mechanism of how they learn all these inexplicit ideas be different

630
01:10:17,240 --> 01:10:19,160
to how they learn explicit ideas?

631
01:10:20,440 --> 01:10:26,200
Um, I don't think so. I think the, the method and scare quotes of conjecture and criticism is universal.

632
01:10:28,440 --> 01:10:34,120
I mean, yeah, I think it worked the same for, for explicit ideas as for inexplicit ideas.

633
01:10:34,680 --> 01:10:40,600
And wouldn't it be more easy if the mechanism is the same to try to explain

634
01:10:42,040 --> 01:10:49,240
how we learn inexplicit ideas before jumping to the explicit ones and having all these extra

635
01:10:49,240 --> 01:10:55,000
dimensions of language of science, all these meta levels?

636
01:10:56,760 --> 01:10:58,200
Um, I would agree. Yeah.

637
01:10:58,760 --> 01:11:02,040
Yeah. Okay. I'll pause here and maybe we can

638
01:11:03,400 --> 01:11:10,920
thank you. Yeah, go ahead, Dan. Okay. Hi. Hi, Dennis. I missed the beginning of your talk,

639
01:11:10,920 --> 01:11:17,400
but I think this is a really important line of inquiry and my apology for exploring this.

640
01:11:18,120 --> 01:11:28,360
Um, I, I feel like the area which needs to be fleshed out more is the selection

641
01:11:29,240 --> 01:11:38,840
mechanism. And I wanted to mention that there is an existing theory called neural Darwinism,

642
01:11:40,120 --> 01:11:44,360
but it seems to be that theory is more on the, to explain perception

643
01:11:45,240 --> 01:11:54,040
rather than the formation of ideas. And in, in neural Darwinism, the idea is you have,

644
01:11:54,040 --> 01:11:59,720
you have basically a bunch of, when, when, when, in a newborn brain, it's known like you have tons

645
01:11:59,720 --> 01:12:07,720
of random connections and then there's sort of a pruning process. And the idea of neural Darwinism is

646
01:12:08,440 --> 01:12:15,000
the different groups of neurons are selected by how faithfully they replicate the incoming

647
01:12:15,640 --> 01:12:27,240
sensory data streams. With your, with your theory, you, you were talking about more of an internal

648
01:12:27,240 --> 01:12:37,000
process, right? So I was thinking, when, when you need some kind of like world simulator,

649
01:12:37,720 --> 01:12:44,440
to, to, to test different ideas, to see if it makes sense. And like, have you thought about how

650
01:12:44,440 --> 01:12:54,920
there might be like a world simulator and then basically, so for example, with the missing

651
01:12:54,920 --> 01:13:01,240
keys example, which is, which is really interesting one example, different parts of your brain would,

652
01:13:01,240 --> 01:13:11,000
would be simulating different scenarios and sort of seeing if they, if, if,

653
01:13:15,160 --> 01:13:21,880
if the idea, if, if the simulation maps onto the idea, like, like you'd have different ideas,

654
01:13:24,440 --> 01:13:30,440
like you'd have an idea of the keys or location X, and then you'd run a simulation based on the,

655
01:13:30,440 --> 01:13:36,120
all the information you know, and you see if it confirms that idea. Have you thought about anything

656
01:13:36,120 --> 01:13:50,360
like that? Well, just to give a general answer, the, as I, as I said earlier, I like to just

657
01:13:50,360 --> 01:13:59,000
leave sense data out of it completely. I mean, I do think that sense data is important to, to test

658
01:13:59,000 --> 01:14:07,400
your theories. But so to make progress, sense data is important in that sense, but I don't think sense

659
01:14:07,400 --> 01:14:13,160
data is required for, for creativity to work, or for any of the evolution that occurs in a mind to

660
01:14:13,160 --> 01:14:20,360
work. And I mean, you could, you could simply ask, you know, if somebody is born, and I don't want to

661
01:14:20,360 --> 01:14:23,560
put words in his mind, I think David does this in the beginning of infinity, and if somebody is born

662
01:14:23,560 --> 01:14:29,400
blind, they're not any less creative. I mean, they might, they're still people. They're fully

663
01:14:29,400 --> 01:14:35,160
qualified people, right? So they might have a harder time correcting some errors because they

664
01:14:35,160 --> 01:14:40,680
don't have access to sense data from the eyes, but then they might creatively find other ways to,

665
01:14:40,680 --> 01:14:47,000
to correct those errors. But what I, like I said, what I like to do is just leave sense data out of

666
01:14:47,000 --> 01:14:54,600
it completely just to avoid any empiricist pitfalls. And I also like to leave the brain out of it

667
01:14:54,600 --> 01:15:01,320
completely. And I like to think only in terms of the mind, just to avoid any, any accidental

668
01:15:01,320 --> 01:15:06,840
reductionist mistakes. So I, so I, you know, the thing about neuronal structure is self replicating.

669
01:15:08,280 --> 01:15:13,640
I don't know. I would, I was more just mentioning that for the audience's

670
01:15:14,600 --> 01:15:20,600
benefit, not, not, I'm not saying that you should go in that direction. I actually think the

671
01:15:20,600 --> 01:15:25,720
direction you're going, which is more at the higher, higher level, extracting away from the,

672
01:15:27,480 --> 01:15:32,200
from the, the neural circuitry is, is, is very, is very interesting.

673
01:15:35,560 --> 01:15:41,880
As I guess my question was really about the selection mechanism, like could, could you

674
01:15:41,880 --> 01:15:46,840
elaborate more? Cause so far on the selection mechanism, cause so far you, I mean, at least for

675
01:15:46,840 --> 01:15:54,040
the part I heard, you just mentioned, you know, some will be better at replicating than others, but

676
01:15:56,040 --> 01:16:03,960
um, what is the selection? How is, how does selection occur? Is it context dependent or is

677
01:16:04,040 --> 01:16:11,720
there some, how does that, how does that work? Yeah. So again, borrowing Dawkins definition of

678
01:16:11,720 --> 01:16:16,600
selection, that is, it's the non random differential reproduction of a replicator in a pool of

679
01:16:16,600 --> 01:16:23,240
replicators. Um, that is how I would describe the selection effect or the selection mechanism

680
01:16:23,240 --> 01:16:28,200
in the pool of self replicating ideas in the mind as well. It's just the fact that once a

681
01:16:28,200 --> 01:16:35,480
mutation arises, if the mutation either helps or harms the, the new copies ability to spread,

682
01:16:35,480 --> 01:16:40,280
now you have differences in the rate of replication and that itself constitutes selection.

683
01:16:40,280 --> 01:16:46,760
In addition to that, um, the, the, this meta algorithm, which by the way caught myself

684
01:16:46,760 --> 01:16:51,240
not crediting the, uh, this, the, the idea of the meta algorithm originated with

685
01:16:51,800 --> 01:16:58,440
a temple in a different context, but the, the meta algorithm could also, um,

686
01:17:00,520 --> 01:17:08,040
have, you know, act as a sort of arbiter of ideas or select or arbiters, but impose additional

687
01:17:08,040 --> 01:17:15,640
selection pressures on the idea pool. Um, now how in detail that would work, I think is depends on

688
01:17:15,640 --> 01:17:24,200
what happens at runtime, um, because it might, it might, um, look at some ideas for what to do

689
01:17:24,200 --> 01:17:29,720
and what selection pressures to enact on the idea pool itself. So there's some kind of feedback

690
01:17:29,720 --> 01:17:33,240
loop there, but the details of it, the kinks I think need to be worked out there.

691
01:17:38,040 --> 01:17:45,400
Okay. Then we have two questions in the chat by Taha. The first one is in the case of genes,

692
01:17:46,200 --> 01:17:52,600
RNA, DNA, uh, we know what the replication unit is made of, a sequence of nucleotides.

693
01:17:53,240 --> 01:17:56,200
What constitutes the replication unit of an, of an idea?

694
01:17:58,280 --> 01:18:06,120
Yes, this is a great question. Um, I would, uh, I would follow Dawkins here who, who says, well,

695
01:18:06,680 --> 01:18:11,160
you know, on the one hand we have the, the physical instance of the replicator,

696
01:18:12,040 --> 01:18:21,560
um, which is the molecular structure itself. Um, but, um, so actually the terminology I use is that

697
01:18:22,440 --> 01:18:26,680
that I would call an instance of a replicator, a physical instance of that replicator. And

698
01:18:26,680 --> 01:18:31,720
although maybe I'm jumbling biological terms, but I would consider even that, um, physical

699
01:18:31,720 --> 01:18:37,640
instance of that string of molecules itself, part of that replicator's phenotype, which may sound

700
01:18:37,720 --> 01:18:42,280
a bit confusing until you realize that the replicator itself is actually an abstraction.

701
01:18:43,080 --> 01:18:49,640
Um, the replicator is not physical. It has physical instant physical instantiations,

702
01:18:49,640 --> 01:18:56,040
but the replicator is an abstraction because as Dawkins explains, the replicator is that part of

703
01:18:56,040 --> 01:19:01,880
this, this string of DNA that manages to stay the same over many, many generations for, you know,

704
01:19:01,880 --> 01:19:09,720
for as long as possible. And so this is just a chain of instances and that, of course, that chain

705
01:19:09,720 --> 01:19:15,800
physically exists, but it also exists over time. And so some of the older chains, some of the older

706
01:19:15,800 --> 01:19:19,320
part of the chain would already be gone at that point. And still we would consider all that part

707
01:19:19,320 --> 01:19:25,720
of the same replicator. So it's abstract. Those are independent of the, the physical instantiation.

708
01:19:26,440 --> 01:19:34,360
Um, but so the unit of, the unit of replication, in that sense, we could ask the same question

709
01:19:34,360 --> 01:19:43,880
also when it comes to memes. Um, in the mind, I think are, are again these, the, again, at the

710
01:19:43,880 --> 01:19:48,600
risk of sounding too technical, the functions in the sense of the lambda calculus, these little

711
01:19:48,600 --> 01:19:54,360
programs, just think of them as programs, really. Um, those are the ones that replicate. And of

712
01:19:54,360 --> 01:19:59,880
course they're going to have a physical instantiation in the physical memory now again, in the sense of

713
01:19:59,880 --> 01:20:06,440
storing stuff, not in the sense of remembering stuff in the brain. Um, I think Dawkins even went

714
01:20:06,440 --> 01:20:13,560
so far as to say one time that, you know, he proposed this conjecture that maybe we, if, if you have a

715
01:20:13,560 --> 01:20:17,480
joke in your mind and I have a joke in my mind or in your brain, I have a joke in my brain,

716
01:20:17,480 --> 01:20:21,880
that maybe that means that the neuronal structures and coding that joke are the same. So maybe

717
01:20:22,840 --> 01:20:29,080
meme replication implies, uh, replication of neuronal structures across brains or something.

718
01:20:29,880 --> 01:20:34,440
Um, actually I think I'm getting ahead of myself here. Um, so the replication unit of an idea

719
01:20:35,400 --> 01:20:39,960
is the function in the sense of the lambda calculus. I hope that helps.

720
01:20:40,680 --> 01:20:46,920
Yes, I think it makes sense. Just get a clear picture for myself. Do you mean that, uh,

721
01:20:47,400 --> 01:20:52,280
there is, there is something that you and I have in common when we have the same idea in, in our

722
01:20:52,280 --> 01:20:57,080
minds, or that we have a similarity when we have the same joke, then whatever is encoding that joke

723
01:20:57,080 --> 01:21:07,880
in our, uh, in our brains has to be, uh, fundamentally the same thing. Like it is, it is encoding the

724
01:21:07,880 --> 01:21:12,520
same abstraction, as you would say. And it might do so differently. It might use a different encoding,

725
01:21:12,520 --> 01:21:16,760
a different encoding, but it is, they're both encoding the same abstractions in that sense.

726
01:21:17,960 --> 01:21:23,240
Yeah, provided that enough error correction has gone into it, the, the, I agree, the abstraction

727
01:21:23,240 --> 01:21:31,080
that, that encodes the idea in your mind should be roughly at least similar to, to the encoding

728
01:21:31,080 --> 01:21:36,600
in my mind. Yes. Right. Okay. And then there's the second question, uh, also from Taha, uh,

729
01:21:36,600 --> 01:21:41,560
which goes knowing that the sequence of a gene allows us to target it with a strip of DNA,

730
01:21:42,600 --> 01:21:45,880
how can we do the same and target this specific idea?

731
01:21:48,040 --> 01:21:54,280
Yeah. Um, that's also a really great question. Uh, well, if you, maybe we can go back to the,

732
01:21:54,280 --> 01:21:58,680
the thought experiment I suggested earlier with the, with the brain interface that allows you to

733
01:21:58,680 --> 01:22:04,200
connect to someone's brain and, you know, read the source code. I don't know how exactly you would

734
01:22:04,200 --> 01:22:11,640
do that, but if you somehow manage to, and I think that process itself would involve

735
01:22:12,520 --> 01:22:19,160
conjecture and refutation. So the brain interface may need to be a person, but, um,

736
01:22:20,200 --> 01:22:26,680
if somehow you manage to do that and you, you can turn the zeros and ones in the brain somehow back

737
01:22:26,680 --> 01:22:33,080
into, um, functions, you know, represent them as functions in the sense of the Lambda calculus,

738
01:22:33,080 --> 01:22:37,240
then what you could do is you, at any point in that source code, you could make modifications or

739
01:22:38,280 --> 01:22:42,520
split it or put something new in there. Um, all this stuff would be available to you.

740
01:22:43,320 --> 01:22:46,600
Seems to me a challenge to, to make that happen, that, that interface.

741
01:22:48,040 --> 01:22:54,600
Great. Um, then yeah, we, uh, we're almost through the 45 minutes

742
01:22:55,960 --> 01:23:01,320
and I don't see any further questions, uh, unless someone has them now in which case,

743
01:23:01,640 --> 01:23:08,680
please, please go ahead and ask. Um, uh, yeah, go ahead. Yeah, if you don't mind me asking a

744
01:23:08,680 --> 01:23:15,400
second one, that's okay. So I was just wondering about like, uh, so do you think, uh, a conflict

745
01:23:15,400 --> 01:23:23,480
or problems are a necessary precondition for a conjecture or, uh, because I was thinking about,

746
01:23:23,480 --> 01:23:31,560
like in order for us to, um, identify a problem, would we first need to conjecture that a problem

747
01:23:31,560 --> 01:23:36,520
exists? So in other words, there's like, let's take a simplistic, probably unrealistically

748
01:23:36,520 --> 01:23:45,480
simplistic case. There's two conflicting ideas. Um, and, uh, we're, we're conjecturing solutions to

749
01:23:45,480 --> 01:23:53,320
that problem. Do we need to conjecture that there is a problem? Uh, and then like just as a related

750
01:23:53,320 --> 01:24:00,040
idea, uh, in order to conjecture anything in order for any new variation of an idea to, to come about,

751
01:24:00,840 --> 01:24:05,160
does that kind of conflict need to, need to exist? If that makes sense.

752
01:24:06,760 --> 01:24:15,480
Um, I think the answer is no for, because new ideas will just evolve by virtue of replication

753
01:24:15,480 --> 01:24:25,640
being imperfect over time. Um, but when there is a conflict between ideas, um, that certainly

754
01:24:25,640 --> 01:24:31,640
helps. And I would imagine that at least that is the main thing. Like I said, that we're consciously

755
01:24:31,640 --> 01:24:37,560
aware of when we do come up with a new conjecture is, is because it is in response to a problem. So

756
01:24:37,560 --> 01:24:42,040
I still think that problems are the raw material of creativity and so forth. Um, but strictly

757
01:24:42,040 --> 01:24:48,920
speaking, no conjectures could arise just by virtue of, of imperfect replication. Okay. Okay. Thanks.

758
01:24:51,640 --> 01:24:53,880
Okay. Uh, any other questions?

759
01:24:58,360 --> 01:25:05,080
Antonio, go ahead. Thank you. Um, do you think there's a difference between

760
01:25:06,040 --> 01:25:13,000
the perception of a problem, so this conflict between ideas and sensory perception in general?

761
01:25:17,000 --> 01:25:23,640
Um, could you elaborate a little more? Yeah. Do you think there is, I mean, when you realize

762
01:25:23,640 --> 01:25:32,120
that there is a conflict between two ideas, isn't this something you perceive via your senses?

763
01:25:33,080 --> 01:25:41,320
Is there, is it a different physical structure, say from vision or from looking at the screen or

764
01:25:41,320 --> 01:25:47,160
recognizing a bicycle? When you recognize that there is a problem between two ideas,

765
01:25:47,880 --> 01:25:55,800
do you think it's a different object in a way than recognizing a physical object around you?

766
01:25:56,360 --> 01:26:03,800
Um, I think those are, those are different things. Um, when we experience a problem,

767
01:26:04,840 --> 01:26:11,400
um, experience, yeah. Right. When we experience a problem, I mean, maybe I use the term experience

768
01:26:11,400 --> 01:26:16,520
differently, but this, the sensation we have, sensations also tricky term because it has sense,

769
01:26:17,400 --> 01:26:22,040
but the, the sensation we have when we experience a problem with cognitive dissonance

770
01:26:23,000 --> 01:26:28,600
need not result from sense data. It certainly can happen that you're, you know, with an optical

771
01:26:28,600 --> 01:26:32,040
illusion, for example, you think you're looking at one thing, but it's actually another and you

772
01:26:32,040 --> 01:26:35,080
can change, if you change your perspective, you see, and then you experience that problem,

773
01:26:35,080 --> 01:26:38,600
that's a problem you want to solve, but the experience of the problem itself

774
01:26:39,720 --> 01:26:45,320
as all experience, I think, is entirely within. That is not something that is induced somehow by,

775
01:26:45,320 --> 01:26:53,000
by sensory data. And again, if I agree, I agree, but they're both inside, both when you perceive

776
01:26:53,000 --> 01:26:59,080
a bicycle on the road and when you perceive a problem in your mind, both of them are inside you.

777
01:26:59,640 --> 01:27:06,360
That's what I'm saying. Is there a difference? Yes. I think there's still a difference when

778
01:27:06,360 --> 01:27:11,240
you just look at a bicycle and say you have, you know, I imagine that children learn this when

779
01:27:11,240 --> 01:27:16,920
they're very young, they, they creatively conjecture recognition algorithms like shape

780
01:27:16,920 --> 01:27:21,960
recognition algorithms so that later they can recognize bicycles. That stuff just happens

781
01:27:21,960 --> 01:27:28,200
automatically. Once you see it, once you've created that algorithm, you just recognize bicycles,

782
01:27:28,200 --> 01:27:34,760
unless you're horribly deformed or something. Whereas when you have a problem, there's something

783
01:27:35,880 --> 01:27:40,760
unknown there. There's something mysterious there. So I still think those are qualitatively

784
01:27:40,840 --> 01:27:45,240
different experiences. If that, if maybe if that's where you're getting at it, I don't know if I'm

785
01:27:45,240 --> 01:27:53,640
answering the question. Okay. I don't think there is a difference, but yeah. Well, can you elaborate?

786
01:27:53,640 --> 01:27:58,360
What, what is it? You think that is the same about it? I mean, at some point to a child,

787
01:27:59,720 --> 01:28:05,400
seeing a bicycle was a mystery in the same way that when you think about supernovae or whatever,

788
01:28:06,120 --> 01:28:13,320
it's a mystery for you. Right. There is no automatic thing going on. I mean, if at some point,

789
01:28:14,440 --> 01:28:20,840
because if you're never exposed to bicycle, then you can be as old as you want, but at some point,

790
01:28:21,560 --> 01:28:30,200
you will be mystified by this new object. So I see. Yeah, no, I think you're right that when,

791
01:28:30,200 --> 01:28:34,280
for the, when, for the first time you see a bicycle that is a mystery, you know, what is that thing,

792
01:28:34,280 --> 01:28:41,400
how does it work and how would I recognize it again? At like that first time, you're confronted

793
01:28:41,400 --> 01:28:45,960
with a problem and you want to solve that problem. And in response to it, you learn about the bicycle

794
01:28:45,960 --> 01:28:51,080
and you come up with a shape recognition algorithm for the bicycle and so forth. But once you have that,

795
01:28:52,520 --> 01:28:56,280
now it's automatic. Now you can just recognize bicycles again. If I look at the window and I

796
01:28:56,280 --> 01:29:03,080
see a bicycle that doesn't constitute a problem. So I would, I would still consider those two

797
01:29:03,080 --> 01:29:14,760
different things. Well, fair enough. Yeah, I think aren't they just the same thing only one is subjected

798
01:29:14,760 --> 01:29:23,800
to empirical criticism and the other is just an internal idea. I'm maybe well. Yeah. I mean,

799
01:29:23,800 --> 01:29:30,760
your internal ideas, aren't they criticism? How do you experience? How do you think?

800
01:29:31,720 --> 01:29:34,520
Don't you visualize your objections to something?

801
01:29:36,920 --> 01:29:40,520
Why do we need to distinguish between these two worlds?

802
01:29:41,480 --> 01:29:45,960
It's the same distinction as between science and philosophy, I think. And it's not really a

803
01:29:45,960 --> 01:29:52,280
distinction. It's just a matter of where the criticisms come from. And it's useful in some

804
01:29:52,280 --> 01:29:59,240
cases, but this is not a fundamental distinction, I think. I'm not sure if Dennis agrees with that,

805
01:29:59,720 --> 01:30:06,920
this is how I would understand that. I do, although I still have the feeling that I can't

806
01:30:06,920 --> 01:30:12,360
quite put my finger on, on what's your name, Antonio's question. So I feel that I haven't

807
01:30:12,360 --> 01:30:22,840
really answered it satisfactorily. Yeah, well, maybe a topic for future discussion. I'm

808
01:30:22,840 --> 01:30:30,120
afraid we've run out of time, though. And yeah, I just want to thank Dennis for his great talk. I

809
01:30:30,120 --> 01:30:39,160
will do so as usual with appalling in emoji. There you go. And that was a lot of fun. It was

810
01:30:39,160 --> 01:30:43,960
great to be here. Yeah, yeah. Thanks so much for coming. And again, I encourage everyone to check

811
01:30:43,960 --> 01:30:49,960
out your article on the Conjection Magazine website, which we'll link to in the description of the

812
01:30:49,960 --> 01:30:58,600
video. And yeah, so thank you for Logan for helping range this. Thank you and good luck.

813
01:30:58,600 --> 01:31:03,720
Yeah, thank you. Okay, goodbye. Thank you, everybody.

