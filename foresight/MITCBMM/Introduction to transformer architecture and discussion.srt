1
00:00:00,000 --> 00:00:10,000
If we look at history, models of the brain have always been the most fashionable technology.

2
00:00:10,000 --> 00:00:22,000
It was hydrodynamics in the 17th century and digital computers a few decades ago.

3
00:00:22,000 --> 00:00:29,000
And ten years ago was convolutional neural networks.

4
00:00:29,000 --> 00:00:33,000
And now, transformers of course.

5
00:00:33,000 --> 00:00:37,000
So in preparation for this, we have an introduction to transformers.

6
00:00:37,000 --> 00:00:47,000
I think this is something that Michael Fee, chairman of this department here and Ghibli Carlo asked for.

7
00:00:47,000 --> 00:00:54,000
Of course they are not here, but Michael actually asked me to record it.

8
00:00:54,000 --> 00:00:57,000
Is that possible?

9
00:00:57,000 --> 00:01:01,000
It's recording now as long as that's okay with you guys.

10
00:01:01,000 --> 00:01:05,000
Everybody agrees to be recorded. Okay, all right.

11
00:01:05,000 --> 00:01:15,000
So with this we have an introduction to transformers and this probably will be the first of some of the research meeting where we'll speak about transformers.

12
00:01:15,000 --> 00:01:23,000
Next time perhaps more in comparison in the framework of models of the brain.

13
00:01:23,000 --> 00:01:29,000
But this time it's more on just technical aspects of transformers.

14
00:01:29,000 --> 00:01:40,000
And so Philip Tinsola, C-Sale and Brian Chung, this building, will tell us how Jim is here after all.

15
00:01:40,000 --> 00:01:44,000
Okay, so Lupus in Fabulous.

16
00:01:44,000 --> 00:01:46,000
What was that?

17
00:01:46,000 --> 00:01:50,000
Lupus in Fabulous exactly means what happened, that I will speak about you and you came in.

18
00:01:51,000 --> 00:01:55,000
So Philip will start.

19
00:01:55,000 --> 00:02:06,000
And then we have Brian and then perhaps discussion and in the meantime, of course, very welcome to ask a lot of questions.

20
00:02:06,000 --> 00:02:07,000
All right.

21
00:02:07,000 --> 00:02:10,000
Okay, great. Thank you, Tommy.

22
00:02:10,000 --> 00:02:16,000
So this can be, as far as I'm concerned, very informal and happy to go back and forth.

23
00:02:16,000 --> 00:02:29,000
I took slides from a lecture, like hour and a half electron transformers I've given, and tried to pick a few slides. So it's going to be a quick intro, but we can go into as much detail as you want, and happy to make it also just more discussion.

24
00:02:29,000 --> 00:02:36,000
But I'll start by talking about, you know, what are transformers, you know, for those of you who haven't encountered them yet.

25
00:02:36,000 --> 00:02:48,000
Okay, so I think that there are two key ideas that are not new but are relatively popularized by transformers. And the first is called the idea of tokens.

26
00:02:48,000 --> 00:02:51,000
And then the second is going to be the idea of attention.

27
00:02:51,000 --> 00:02:56,000
There's a few other, you know, bits and pieces and transformers, but to me, those are the two kind of key ideas.

28
00:02:56,000 --> 00:03:03,000
Okay, so tokens are essentially a new data structure.

29
00:03:03,000 --> 00:03:17,000
And they're a replacement for neurons. Okay, so in artificial neural nets, neurons are just real numbers, just scalar numbers, and tokens are vectors. So that's, that is the distinction that, as far as I'm concerned.

30
00:03:17,000 --> 00:03:21,000
So it's just lingo for a vector of neurons, a vector of scalars.

31
00:03:21,000 --> 00:03:29,000
But when you start thinking of your primitive units in a network as tokens, whether they're neurons, then sometimes the math will look a little bit different.

32
00:03:29,000 --> 00:03:37,000
And I think it's just a powerful way of working with new kinds of neural networks made out of tokens instead of neurons.

33
00:03:37,000 --> 00:03:41,000
Okay, again, it's a patch in an image.

34
00:03:41,000 --> 00:03:55,000
It's a patch in an image can be a token. I'll give a few examples and not not any ideas vector neurons vectors of, you know, features for a unit inside a network that's an old idea, you know shows up in graph neural networks if you've seen those two.

35
00:03:55,000 --> 00:04:10,000
Okay, but the way I like to think of it is we used to work with arrays of neurons and now in transformers we work with arrays, tensors of various shape of tokens, but they're kind of like encapsulated, you know, vectors of information.

36
00:04:10,000 --> 00:04:19,000
Okay, so you can tokenize just about anything and that is like the big trend right now is just show how to turn whatever data you have into a sequence of tokens.

37
00:04:19,000 --> 00:04:24,000
So here's an example, like Tommy's mentioning of tokenizing an image.

38
00:04:24,000 --> 00:04:35,000
You could do this in a million different ways but the way that is very common is you start with an image you break it into patches, you flatten all the patches to create just take all the rows of that.

39
00:04:35,000 --> 00:04:41,000
Just pixels and just concatenate them into a really long row or really long colon vector.

40
00:04:41,000 --> 00:04:47,000
And then you get this thing called a token which is just an n dimensional vector representing that patch.

41
00:04:47,000 --> 00:04:58,000
And this arrow up here, if you can see my mouse, it could just be flattened or concatenate, but commonly it will actually be a linear projection to some, you know, fixed dimensionality.

42
00:04:58,000 --> 00:05:01,000
So project to 128 dimensional vector for example.

43
00:05:01,000 --> 00:05:04,000
Should I think of a token as an embedding? Is it the same or what?

44
00:05:04,000 --> 00:05:09,000
Same thing. Yeah, so vector embedding of a patch would be a token in this context. Yeah.

45
00:05:09,000 --> 00:05:17,000
So there's a lot of names for this but to me like yeah once you start thinking of things in terms of tokens like that's that's how I like to think of it.

46
00:05:17,000 --> 00:05:20,000
Okay, so you can tokenize anything.

47
00:05:20,000 --> 00:05:23,000
That's often a linear projection.

48
00:05:23,000 --> 00:05:27,000
We used to operate over neurons now we operate over tokens.

49
00:05:27,000 --> 00:05:36,000
And just a few more examples so whenever you can take your signal and chop it up into chunks and project each chunk to a fixed dimensionality vector.

50
00:05:36,000 --> 00:05:45,000
Then you can tokenize that data so for language, people do this as well they chop up into little chunks which are often, you know, two characters or a few characters at a time.

51
00:05:45,000 --> 00:05:51,000
And they project those into 120 dimensional vectors with sound you chop up into little tiny snippets of sound.

52
00:05:51,000 --> 00:05:59,000
So you can tokenize anything and once you've tokenized it then it's just a sequence like representation a sequence of vectors.

53
00:05:59,000 --> 00:06:06,000
Although really the way transformers work is they think of it as they treat it as a set not a sequence but people often talk of it as a sequence.

54
00:06:06,000 --> 00:06:08,000
Okay.

55
00:06:08,000 --> 00:06:17,000
So that's tokenization that's, I think that's the first critical idea and then everything ends up being the operators to find over tokens as opposed to over neurons.

56
00:06:17,000 --> 00:06:30,000
So rather than taking linear combinations of neurons which is the common, you know, linear layer in a network, we take linear combinations of tokens where you just take a weighted combination of vectors instead of weighted combination of scalars.

57
00:06:30,000 --> 00:06:32,000
Okay.

58
00:06:32,000 --> 00:06:40,000
So I'm not in this, because it's only like a little intro going to go into the math and detail, but we can always come back if you're interested.

59
00:06:40,000 --> 00:06:52,000
Okay, so in standard artificial neural networks, there's two key layers, there's the linear combination layer, the linear layer, and there is the point wise nonlinearity, which might be a ReLU or a sigmoid function.

60
00:06:52,000 --> 00:07:03,000
So in tokens, there's the same thing, there's a linear layer, which we have here, just a linear combination of vectors, and then there's a token wise nonlinearity which is completely analogous to a neuron wise nonlinearity.

61
00:07:03,000 --> 00:07:17,000
So this is a neural net that does a point wise nonlinearity over neurons that applies the same function ReLU to every neuron in a list, and the token net applies the same function F to every token in a list for an asset.

62
00:07:17,000 --> 00:07:24,000
Okay, usually F is going to be itself a multilayer perceptron, it will be some parameterized function.

63
00:07:24,000 --> 00:07:29,000
So equivalently think of this as a convolution over the tokens.

64
00:07:29,000 --> 00:07:35,000
So, transformers, CNNs, it's all the same ideas kind of rehash, we can talk about that if people are interested.

65
00:07:35,000 --> 00:07:45,000
But here's what it looks like, it's just a token wise operation that slides along the list of tokens. Okay, so you can see that looks like convolution sliding across the signal.

66
00:07:45,000 --> 00:07:48,000
Okay, it's usually one layer, right?

67
00:07:48,000 --> 00:07:55,000
It's usually one layer, I think that is usually linear ReLU to linear. Yeah, so it's a nonlinear function.

68
00:07:55,000 --> 00:08:00,000
If it, yeah, it just has to be nonlinear so you get nonlinearities into the system.

69
00:08:00,000 --> 00:08:02,000
I have a question.

70
00:08:02,000 --> 00:08:03,000
MLP.

71
00:08:03,000 --> 00:08:05,000
One layer of neither.

72
00:08:05,000 --> 00:08:10,000
Well, two linear layer MLP, but sure. Yeah, you could call it something else.

73
00:08:10,000 --> 00:08:19,000
I have a good question for the last slide, like, I think I have missed what is the Z here, like, in the last slide.

74
00:08:19,000 --> 00:08:25,000
Oh, Z is the token code vector, meaning the vector of neurons inside that token.

75
00:08:25,000 --> 00:08:26,000
Okay.

76
00:08:26,000 --> 00:08:29,000
It has this code vector that lives inside it. Yeah.

77
00:08:29,000 --> 00:08:31,000
Okay, thank you.

78
00:08:31,000 --> 00:08:32,000
You're welcome.

79
00:08:32,000 --> 00:08:35,000
We'll see you now with convolution here.

80
00:08:35,000 --> 00:08:49,000
Conquering applies the same operator independently and identically to every item in a sequence, and this is doing the same thing. So it's like, you know, slide the filter across here to slide this nonlinear filter across the sequence.

81
00:08:49,000 --> 00:09:01,000
But we give it as a one by one kernel because it doesn't the like the receptive field in the sequence dimension is just looking at one token at a time.

82
00:09:01,000 --> 00:09:14,000
Okay, so here's a neural net. And here is what I'll call a token net. So it's just like a neural net alternating linear combination point wise nonlinearity but now it's linear combination of tokens and token wise nonlinearity.

83
00:09:14,000 --> 00:09:17,000
And transformers are in that family.

84
00:09:17,000 --> 00:09:23,000
Another name for that family is graph neural networks they have the exact same structure.

85
00:09:23,000 --> 00:09:26,000
Okay, but the terminology is just different.

86
00:09:26,000 --> 00:09:31,000
But they're the same thing token nets graph nets transformers are all the same thing.

87
00:09:31,000 --> 00:09:38,000
You could say that token sorry transformers are special kind of graph net if you're looking at that way.

88
00:09:38,000 --> 00:09:44,000
So so many connections that could be made, but I'm going to zoom right ahead and we can discuss all the connections.

89
00:09:44,000 --> 00:09:51,000
So that idea number one is build things out of tokens vector valued units as opposed to out of neurons scalar value units.

90
00:09:51,000 --> 00:09:58,000
Number two to me is attention. Maybe this is the most famous idea of transformers is they have this new layer called attention.

91
00:09:58,000 --> 00:10:01,000
So let's look at what attention is.

92
00:10:01,000 --> 00:10:14,000
Okay, so in a neural net or what I'm calling a token that you'll have linear combinations of inputs to produce an output or weight of some of the inputs to produce an output, and you will parameterize that mapping with weights w.

93
00:10:14,000 --> 00:10:24,000
And in attention, you have a weighted combination of inputs to produce an output, but but the parameters or the values in the matrix a are not learnable.

94
00:10:24,000 --> 00:10:29,000
Instead, they're going to be data dependent. They're going to be a function of the some other data.

95
00:10:29,000 --> 00:10:38,000
So when you have data dependent linear combination where the weights are a function of some other data, then that's called attention.

96
00:10:38,000 --> 00:10:40,000
Okay.

97
00:10:40,000 --> 00:10:49,000
And so a is the weight matrix, and it's just a function of something else. It tells you what to attend to how much weight to apply to each token input sequence.

98
00:10:49,000 --> 00:10:56,000
And you'll take a, you know, weighted sum, which is just matrix multiply a by the tokens.

99
00:10:56,000 --> 00:11:07,000
Okay, notation, don't worry about it. So here's here's the intuition. So I can have attention given to me by some other branch of my neural network, maybe it's going to be a branch that passes a question.

100
00:11:07,000 --> 00:11:14,000
That question will tell me what patches in the input I tend to the input patches are represented by tokens.

101
00:11:14,000 --> 00:11:22,000
And I will say these are the patches that I'm going to place high weight on and then I'll take a weighted sum of the code vectors inside those patches pretty soon output.

102
00:11:22,000 --> 00:11:30,000
So I could ask, you know, question, what is the color of the birds head I'll attend to the birds head. What is the color of the vegetation I'll attend to the background.

103
00:11:30,000 --> 00:11:38,000
I'm just saying which tokens tokens or patches here are going to be getting a lot of weight to make my decision.

104
00:11:38,000 --> 00:11:47,000
Okay, and then I'll report the best green because the token code vectors will have represented some information like the color of the token.

105
00:11:47,000 --> 00:11:49,000
Okay.

106
00:11:49,000 --> 00:11:57,000
There's a little too much detail to fully understand in just a few minutes but here's the most common kind of attention layer just to show you the mechanics really quickly.

107
00:11:57,000 --> 00:12:11,000
The question submits a query that is matched against a key in the data that you're querying so the data you're querying is a set of tokens and each token has a key vector which gets matched against the query vector of the question.

108
00:12:11,000 --> 00:12:26,000
The dot product to get the similarity between the key and the query and that dot product that similarity becomes the way you apply to another transformation of your tokens which is called the value vector and you take a weighted combination of the value vector.

109
00:12:26,000 --> 00:12:44,000
And so, although all the fancy math here is just to say, the question will tell me which tokens to weigh heavily in a weighted sum to produce an output, but it'll be via these three transformations the token vectors, which are the key the value in the query transformations.

110
00:12:44,000 --> 00:12:46,000
Okay.

111
00:12:46,000 --> 00:12:55,000
So that's a little mechanistic detail we can go back to that if we want to discuss the nitty-gritty. The way it kind of looks like is this and this is common to most of the transformer architectures.

112
00:12:55,000 --> 00:13:01,000
You have a bunch of you break into patches then you have a bunch of layers which are basically these one by one convolutional layers.

113
00:13:01,000 --> 00:13:15,000
And then every now and then you have something that tries to mix information across space across tokens and that's called attention and that is just the tokens based on their values queries and keys will decide what should I you know which are the other tokens

114
00:13:15,000 --> 00:13:28,000
should I average together to produce a new representation in the output. So this head might attend. So you know I should look at the other heads to decide how I can better recognize what's going on in this patch.

115
00:13:28,000 --> 00:13:32,000
When you say one by one evolution you mean a patch is a pixel.

116
00:13:32,000 --> 00:13:44,000
A patch is so you vectorize a patch and then you do like one by one convolution across all of those tokens where the channel dimension is the token vector.

117
00:13:44,000 --> 00:13:54,000
OK, so here's MLPs and then transformers are to two changes. One is that rather than scalars they have tokens and other changes rather than having parameterized linear weights.

118
00:13:54,000 --> 00:14:05,000
They have data dependent linear weights that are given by that special attention operator and that attention operator itself has parameters that define the key the query and the value and those are the learnable parameters of the system.

119
00:14:05,000 --> 00:14:12,000
Philip, just a small detail, but you start with self attention and then you have a one layer.

120
00:14:12,000 --> 00:14:17,000
Yeah, but in the previous slide you had the opposite order is that.

121
00:14:17,000 --> 00:14:22,000
Well, so it will vary a lot between different architectures like you could alternate these different orders.

122
00:14:22,000 --> 00:14:27,000
And I did gloss over the detail of this is attention from an external question.

123
00:14:28,000 --> 00:14:35,000
But the more common thing is self attention where attention is coming from the image itself from the data itself.

124
00:14:35,000 --> 00:14:37,000
And that's.

125
00:14:37,000 --> 00:14:48,000
Okay, this is like attention from the question branch, but I could just have the data choose what in its own token sequence to attend to each token chooses which other tokens to attend to and that's called self attention.

126
00:14:48,000 --> 00:14:56,000
That's like this picture here where this patch will decide what to attend to to get a better representation of that patch essentially.

127
00:14:56,000 --> 00:15:12,000
Okay, so this is self attention really as opposed to you. That's why the dependency of the weights on the data is like this is attending to itself as opposed to attending f is not coming from some external source.

128
00:15:12,000 --> 00:15:22,000
Okay, so I think this is the last one just to connect it to one of the most common common demonstrations of transformers is to do sequence modeling.

129
00:15:22,000 --> 00:15:29,000
But really transformers are more about set to set operations, but you can you can represent a sequence as a set. So it's fine.

130
00:15:29,000 --> 00:15:33,000
But, oh, oh, no, I think I got the wrong slide here.

131
00:15:33,000 --> 00:15:38,000
Let me pull up the right one.

132
00:15:38,000 --> 00:15:48,000
Yeah, this is what I wanted to show. Okay, so, so here's how it looks for doing next word prediction, and you can do next, you know,

133
00:15:48,000 --> 00:15:58,000
prediction and sequence of proteins or the next sound wave prediction, you next meeting prediction. That's really common framework. So we, this is like a one layer transformer.

134
00:15:58,000 --> 00:16:10,000
You know, we're going to say colorless green idea sleep, put that into the transformer we want to predict the blank, the words attend to each other, then you pass to this token wise nonlinearity, and then you make a prediction at the very end.

135
00:16:10,000 --> 00:16:20,000
So just one example of what you have seen as transformers for sequences, but really transformers are more general and they're not just about sequence modeling. This is just one way you could use them.

136
00:16:20,000 --> 00:16:24,000
Okay, so that's the 10 minute overview of transformers.

137
00:16:24,000 --> 00:16:29,000
Let's see, should we do questions or Brian, do you want to just jump right in. Maybe you do questions.

138
00:16:29,000 --> 00:16:36,000
Okay, questions will Brian set up.

139
00:16:36,000 --> 00:16:45,000
What is the history of these terms, you know, quest, query value.

140
00:16:45,000 --> 00:16:55,000
So he's probably other people here know better than I do but I think it's all coming from the data retrieval database literature so you have a database where you have knowledge stored in kind of cells.

141
00:16:55,000 --> 00:17:03,000
And you can query that database you say I want to find things related to drafts.

142
00:17:03,000 --> 00:17:16,000
And then every cell will have like a label the key will be like this is a cell for mammals and this is a cell for drafts and here's a cell for plants, and you'll match the query to the key, and then the stuff inside the cell that you retrieve will be the value.

143
00:17:16,000 --> 00:17:21,000
So I think it's coming from that.

144
00:17:21,000 --> 00:17:30,000
This isn't like a fully formed question but I guess like in the framework of having question key value.

145
00:17:30,000 --> 00:17:45,000
If, let's say the question was like what is the color of the turkey's head and for some reason, the turkey's head is like a feature that's complex enough that it won't, it can't be like well represented within a single question.

146
00:17:45,000 --> 00:17:53,000
So is that like cause issues with like attending to the right thing or like waiting the right thing properly when

147
00:17:53,000 --> 00:17:56,000
Yeah.

148
00:17:56,000 --> 00:18:11,000
I suppose it could one one answer would be that you often do multi headed attention where you'll have, you'll take your sequence or your set of tokens and then you'll have K different query vectors and K different value vectors and K different

149
00:18:11,000 --> 00:18:20,000
value vectors and each query can be asking a different type of you can say I want to match to the same color. Another one can say I want to match to the same geometry.

150
00:18:20,000 --> 00:18:34,000
And, you know, when you optimize the parameters of this network it will somehow, you know, self organize that, well, it's useful to factor things into geometry and color then there'll be one attention head that cares about color and one that cares about geometry.

151
00:18:34,000 --> 00:18:41,000
I'm just one quick thing on the transformer architecture and the non linearity.

152
00:18:41,000 --> 00:18:49,000
If I recall correctly you do a normalization of each token right intuition for that why do you do that.

153
00:18:49,000 --> 00:18:53,000
I haven't seen that too much before transformers.

154
00:18:53,000 --> 00:19:02,000
Yeah, that's a great question so so normally you want your weighted sum to add up to one. So the weight should add up to one.

155
00:19:02,000 --> 00:19:14,000
People do that. I treat that via the softmax over the weights. Yeah, is that we're referring to. Yeah, no I'm not referring to post the residual connection you do as part of the right before that.

156
00:19:14,000 --> 00:19:23,000
Other layer norms. So, okay my understanding of transformers has progressed to the level of tokens and attention and all the rest layer norm and residual connections.

157
00:19:23,000 --> 00:19:32,000
At this point I don't rock it I don't I don't know why that feels kind of like tricks that usually help neural networks train it's good to normalize things is good to have principles.

158
00:19:32,000 --> 00:19:39,000
I don't see anything. I haven't noticed that anything specific to transformers about those ideas yet. They're generally useful tricks. Okay.

159
00:19:39,000 --> 00:19:47,000
Transformers are very hard to train especially like several later ones like 12 and up if you don't have like residual connections or layer norm.

160
00:19:48,000 --> 00:19:52,000
Okay, the training isn't very stable.

161
00:19:52,000 --> 00:19:58,000
So it's not it's not. Yeah, also this attention between multiplication.

162
00:19:58,000 --> 00:20:03,000
And that might not be as stable as normal non intentional layers just do addition.

163
00:20:03,000 --> 00:20:10,000
That's the softmax right the softmax can probably help with that. Yeah.

164
00:20:10,000 --> 00:20:19,000
So how do you decide what patches to use for instance the image or the text you also had like what seemed to be arbitrary.

165
00:20:19,000 --> 00:20:21,000
Yeah, segmentation of the data.

166
00:20:21,000 --> 00:20:30,000
Yeah, that's a great question. How like how do you do the tokenization like how do you design that I think it's super hacky right now so that feels like somewhere that people could do a lot of work.

167
00:20:30,000 --> 00:20:36,000
One thing that does seem to happen is that the smaller you make your tokens if you have tokens that are a single pixel it's just the better you get.

168
00:20:37,000 --> 00:20:40,000
Maybe the

169
00:20:40,000 --> 00:20:53,000
what will happen is we'll just stop having clever tokenizations and just go down to whatever the atomic units of the data are like a single character a single pixel and that that just makes a choice for us because you can't go below that really.

170
00:20:53,000 --> 00:20:59,000
I think the smaller envision transformers the thing I've seen is that the smaller the tokens are the better they tend to perform.

171
00:20:59,000 --> 00:21:10,000
But because the attention mechanism is every token it tends to every token it's like n squared if you make the tokens too small, there's too many tokens and then you run out of memory.

172
00:21:10,000 --> 00:21:31,000
So there's probably clever tokenization schemes like super pixels or segmentation or in language spaders, you know, a lot of tokenization schemes that are like bite code by parent coding is the name of one and work that that could be used that as your first tokenization layer.

173
00:21:31,000 --> 00:21:34,000
But yeah that feels like a kind of hacky area right now to me.

174
00:21:34,000 --> 00:21:46,000
I can try to usually leverage at least some degree of topology by tokenizing it that respects you know spatial coordinates for images or yeah language order for words.

175
00:21:46,000 --> 00:21:55,000
Because like there's also something called positioning coding, which gives you knowledge about the topology of why this element is in this position versus other position.

176
00:21:55,000 --> 00:22:03,000
And that tells you a lot about you know the structure of the image or at least where this token is with respect to the overall structure of the image.

177
00:22:03,000 --> 00:22:13,000
Yeah, and then another thing that seems missing right now is people usually envision transformers which I'm most familiar with they usually break into non overlapping tokens.

178
00:22:13,000 --> 00:22:27,000
But we know from continents and like single processing that you'll get a time of aliasing and like the filtering operations if you have these huge strides you're breaking it non overlapping patches you really would want to have overlapping patches or blur the signal.

179
00:22:27,000 --> 00:22:33,000
So all that single processing stuff I think it's been kind of thrown away and probably it's a good idea to put it back in there.

180
00:22:33,000 --> 00:22:35,000
Someone has tried overlapping patches.

181
00:22:35,000 --> 00:22:36,000
And it does help.

182
00:22:36,000 --> 00:22:44,000
And, and by the way everything I say like, as you probably know there's 10,000 transformer papers now so I'm sure everything that you can imagine has been tried.

183
00:22:44,000 --> 00:23:01,000
So the part about working with sequences versus sets confused me a little bit and this relates to position and coding so I think it is important when you send these tokens to the transformer to actually inform it where it is in the sequence so

184
00:23:01,000 --> 00:23:07,000
transformers operates and sequences or sets like how like I'm,

185
00:23:07,000 --> 00:23:10,000
Brian do you mind if I pull the slides over one more time.

186
00:23:10,000 --> 00:23:15,000
I do have some slides that I can clarify that.

187
00:23:15,000 --> 00:23:18,000
So,

188
00:23:18,000 --> 00:23:20,000
yeah.

189
00:23:20,000 --> 00:23:33,000
So the positional encoding which I think is the like third big idea of transformers but it's been used in other contexts too so maybe it's not just about transformers but if you have a component and you don't want it to be invariant to shift

190
00:23:33,000 --> 00:23:48,000
convolution shift invariant, you can just tell it till the filter where you are by adding this positional code to, I can just say you know at the bottom right of the image and then my mapping will end up being conditioned on the position.

191
00:23:48,000 --> 00:23:53,000
So transformers is the same thing I can tell you, you can tell you where the token comes from.

192
00:23:53,000 --> 00:24:02,000
And if you do that then you do get sequential, you are modeling the sequence because you're telling the token you know I'm the first item in the sequence or the second item.

193
00:24:02,000 --> 00:24:11,000
If you don't do that, then you have the property of permutation, equal variance.

194
00:24:11,000 --> 00:24:18,000
That's why I said it's really a set to set operation. So if you don't tell the tokens where they come from, you don't give them positional codes.

195
00:24:18,000 --> 00:24:25,000
Then if I take the tokens on the input layer and I permute them into any order, I will simply permute the tokens on the output layer.

196
00:24:25,000 --> 00:24:28,000
The mapping will be the same after permutation.

197
00:24:28,000 --> 00:24:35,000
It takes a little thinking to see why that's true but essentially the reason is because the attention layer is permutation invariant.

198
00:24:36,000 --> 00:24:43,000
The way self attention works is it looks at the color of this and the color of that and it makes a similarity comparison.

199
00:24:43,000 --> 00:24:56,000
And then the weight of the weighted sum that goes to here is just going to be something about the similarity between the color, the query and the key of this.

200
00:24:56,000 --> 00:25:03,000
So no matter where you move that edge around, as long as the input and output are orange and blue, it will get the same weight.

201
00:25:03,000 --> 00:25:07,000
You can kind of work it out and see that attention is permutation invariant.

202
00:25:07,000 --> 00:25:11,000
The token whites operator is point wise. That's permutation invariant too.

203
00:25:11,000 --> 00:25:16,000
And then that means that the whole transformers are permutation invariant function.

204
00:25:16,000 --> 00:25:21,000
And you can make it model sequences by telling it the position of every token in that set.

205
00:25:21,000 --> 00:25:30,000
But if you don't have positioning coding, it's more appropriate for set to set mapping.

206
00:25:30,000 --> 00:25:50,000
One more question or comment is you started with the idea of tokens, but just a few sentences before you said that something that to me sounds like the idea of tokens is rather weekend that the transformers work better when you have individual bits of data like pixels.

207
00:25:50,000 --> 00:26:13,000
So it feels to me that the idea of tokens is not really essential in the whole concept to work, but that tension is the most critical part and the idea that being able to attend dynamically anywhere in the picture if it is visual data is the idea that gives power in these algorithms, not the token percent.

208
00:26:13,000 --> 00:26:17,000
Yeah, I don't know. And I think that's kind of the open debate.

209
00:26:17,000 --> 00:26:19,000
Yeah, fun to keep discussing.

210
00:26:19,000 --> 00:26:32,000
So I think the field is kind of split right now between people thinking that it's the tokens and the vector encapsulated vectors that are important to this versus the attention and really look at you just have an intentional mechanism.

211
00:26:32,000 --> 00:26:36,000
It can still be operating over neurons. It doesn't really have to be about tokens.

212
00:26:36,000 --> 00:26:52,000
I feel like both probably are important to the success. But one, one counter example to that attention matters is there are these other architectures now sometimes called like MLP mixers, one architecture in this family, which uses tokens and not attention.

213
00:26:52,000 --> 00:27:05,000
So it's basically a component over tokens, one by one comms over tokens. They call it MLP.com and that's super confusing. But anyway, we're going to get the terminology because these things are all just like small transformations in each other.

214
00:27:05,000 --> 00:27:21,000
Okay, but anyway, this MLP mixer thing doesn't have attention, but it's still in my view is as a token net. And that seems competitive on some tasks with attention networks. So maybe attention is maybe it's not the thing that matters.

215
00:27:21,000 --> 00:27:35,000
Related to this question. So it seems like especially in this self attention mode that you're describing. Yeah, if you unrolled that it's like a series of matrices I think going on there that could just be implemented as a straight up feed forwards chain if I'm following.

216
00:27:36,000 --> 00:27:49,000
Is it just very deep, you know, it's going to be skips, I think, but can you on that idea makes sense to you. So you can always kind of like, you know, always take any one of these arrows and just say, Oh, that's actually just a matrix multiply.

217
00:27:49,000 --> 00:28:01,000
But it's a matrix with special structure. It's not like a full rank matrix. And I think that that's, that's one way of just understanding all the person I didn't mean it was any matrix but just like you could re express this as another as I was trying to understand what the

218
00:28:01,000 --> 00:28:16,000
model might look like and then generalize that to and then you're back in standard mode again. Maybe that was closely related to what we were saying. Yeah. Yeah, where you have the second branch with a queue that's different that's sort of active state versus this kind of deterministic processing

219
00:28:16,000 --> 00:28:23,000
based on kind of multiplicative there. Okay, that's where like it differs from like standard MLPs where they don't have this kind of multiplicative interaction.

220
00:28:23,000 --> 00:28:31,000
That means that, you know, you're dependent on the moment, you're self contesting yourself meaning that you're dependent on your own input when you process this particular input.

221
00:28:31,000 --> 00:28:41,000
So it's kind of like a hyper network, I guess in that case, we're like, yeah, you change the representation based on what your presentation already is.

222
00:28:41,000 --> 00:28:58,000
Yeah. So it's not like, yeah, you can't just rewrite it exactly as these linear combinations on like a regular feed forward network, because it does do these multiplies maybe that's the one mathematical atomic unit that's different.

223
00:28:58,000 --> 00:29:13,000
I think you can express these in different languages and I actually do like thinking of it as oh it's just a special kind of matrix the matrix weights come from this other source is self attention mechanism but then it's just a matrix but that matrix has special structure and understanding that low ring

224
00:29:13,000 --> 00:29:22,000
structure is kind of that's just a way of understanding architectures is what is the special structure you're imposing on the on the linear transformations on the matrices.

225
00:29:22,000 --> 00:29:37,000
This dot product here between the query and the key involves multiplication, which is not something that you would directly get in a regular network.

226
00:29:37,000 --> 00:29:46,000
Oh, yeah, related to this and also the permutation if you variance that's within a block. So what about across transformer blocks.

227
00:29:46,000 --> 00:29:55,000
Yeah, so you could have, you can kind of any set of tokens attend to any set of tokens, and they could come from one layer of the net and another layer of the net or one block another block.

228
00:29:55,000 --> 00:30:11,000
It could come from a text processing network and image processing network and then the text attends to the tokens of the image. And then it could come from just the text tokens attend to themselves and that's mostly what I talked about but actually sorry did that answer is that your question or on the wrong way.

229
00:30:11,000 --> 00:30:23,000
She said that you could commute like within a block like the operations. Yeah, pretty much like I'm just saying if you have like 12 transformer blocks is shown that some of the earlier blocks like learn more like surface form type of features where like later learn.

230
00:30:23,000 --> 00:30:28,000
Oh yeah, like high level things and I was wondering about like can you also commute.

231
00:30:28,000 --> 00:30:31,000
Yeah, I think no probably.

232
00:30:31,000 --> 00:30:37,000
So I don't think you can commute depth wise I think just you can commute within the sequence. Yeah.

233
00:30:37,000 --> 00:30:44,000
So if you move the head attention, you know how they can catnate all the heads together to go to the next layer.

234
00:30:44,000 --> 00:30:53,000
Don't they lose the permutation variance, or the permutation that we're going to do that nation process.

235
00:30:53,000 --> 00:30:55,000
Don't think so.

236
00:30:55,000 --> 00:30:58,000
Because your contamination has to have some sort of ordering.

237
00:30:58,000 --> 00:31:01,000
Oh, interesting.

238
00:31:01,000 --> 00:31:05,000
Yeah, maybe. Okay, so it's possible that transformers aren't.

239
00:31:05,000 --> 00:31:08,000
It's not always multi transformers.

240
00:31:08,000 --> 00:31:12,000
Individual transformer heads are accurate.

241
00:31:12,000 --> 00:31:15,000
Yeah, I hadn't thought about the multi headed thing. Okay.

242
00:31:15,000 --> 00:31:18,000
Yeah.

243
00:31:18,000 --> 00:31:33,000
In the multi headed you take a weighted sum of the heads and then that the feature vectors they can carry together and that's a parameterized some and if you change the order of what I think you're right. Okay, interesting.

244
00:31:33,000 --> 00:31:43,000
Did anybody try instead of using the Q and K matrix or the WQ and WK use a single matrix.

245
00:31:43,000 --> 00:32:02,000
Yeah. Okay, so one thing you can do is just get rid of queries keys and values and just have your code vectors at Z and take like the inner product of for the outer product of Z with the Z as your attention and I think that that can sometimes work just as well.

246
00:32:02,000 --> 00:32:14,000
Yeah, I haven't really followed the latest on that, but you can have queries keys and values that are linear functions of your code vectors Z, or they can be nonlinear functions or they could be identity.

247
00:32:14,000 --> 00:32:20,000
And I'm not sure that there's a consensus on when you need which.

248
00:32:20,000 --> 00:32:32,000
So one, one thing that's kind of interesting is that if you use the identity to create your queries and keys, you're basically creating that grant matrix between all of your token vectors with themselves.

249
00:32:32,000 --> 00:32:47,000
And it's going to act kind of like it's going to cluster the data and there's been some analysis of how, okay, identity attention, identity queries and keys will create this, you know, clustering like spectral clustering type matrix it'll create this similarity matrix when you hit the data with a

250
00:32:47,000 --> 00:32:57,000
matrix and literary matrix or group things are similar. And maybe we can understand a little bit what's going on from that perspective like linear queries and keys are just some projection of that kind of thing.

251
00:32:57,000 --> 00:33:04,000
And how does that work if you don't have the WQ and WK matrix but just the identity.

252
00:33:04,000 --> 00:33:07,000
Actually, yeah, do you know Brian or just, just anyone know.

253
00:33:07,000 --> 00:33:16,000
The keys are actually kind of like this more exactly like this spectral matrix because what you do is you process the input transpose it process it again.

254
00:33:16,000 --> 00:33:22,000
You know if you work out both operations in sequence, they end up becoming Z transpose Z like a matrix in the middle.

255
00:33:22,000 --> 00:33:26,000
So I think it does work, I guess.

256
00:33:26,000 --> 00:33:29,000
That was my, maybe not as well but it does work.

257
00:33:29,000 --> 00:33:36,000
It works just maybe not as well. Yeah, we sort of interleaving with these nonlinear operations and still get expressive power.

258
00:33:36,000 --> 00:33:44,000
And one other question that you mentioned and there is a paper I think from the mind about using a single pixel as a token.

259
00:33:44,000 --> 00:33:55,000
Yeah, it seems very little in terms of being able to establish similarities with other tokens because you have just color and intensity.

260
00:33:55,000 --> 00:34:00,000
So my feeling there is that, yes, on the first layer, that's a very bad token.

261
00:34:00,000 --> 00:34:04,000
You can have like, you have a one to eight dimensional vector that coats just the color of a single pixel.

262
00:34:04,000 --> 00:34:06,000
It's not going to do much.

263
00:34:06,000 --> 00:34:16,000
But then when you do this linear combination operation, you know, it's like taking a one by one patch and making a bigger patch out of it.

264
00:34:16,000 --> 00:34:23,000
I mean, it can learn to mix information across the whole image and say all of these similar colors.

265
00:34:23,000 --> 00:34:29,000
Maybe all the white stripes on the zebra will go all the black stripes in the zebras because the black keys will match the white queries.

266
00:34:29,000 --> 00:34:36,000
So it could create these abstracted tokens as you go deeper into the network and build up.

267
00:34:36,000 --> 00:34:49,000
Many years ago in computer vision, there was the idea of instead of using a patch of using at a single pixel, the value of the pixel and derivatives.

268
00:34:50,000 --> 00:35:01,000
Which means essentially you're having no local information because the derivatives gives you information about neighboring pixel, special derivatives.

269
00:35:01,000 --> 00:35:04,000
It's kind of like a little token.

270
00:35:04,000 --> 00:35:09,000
It was a vector of and it's very much like a token.

271
00:35:09,000 --> 00:35:10,000
Yeah.

272
00:35:11,000 --> 00:35:29,000
Yes, I don't think that these are new ideas really is just it helped me to coalesce around the idea of a token like we're gonna think all the operations in terms of tokens but we used to talk about you know, hyper columns and feature vectors and like there's a million names for the same this in concepts.

273
00:35:30,000 --> 00:35:33,000
Do you want to pull up just at a time for you to.

274
00:35:33,000 --> 00:35:42,000
Yeah, you make you made a connection earlier with graph neural networks. I don't recall which one you said is more general of the two.

275
00:35:42,000 --> 00:35:55,000
Well, but they also don't feel the connection because for graph neural networks it's important like the connection in the graph the structure is very important whereas in transformers you said it is less of a case.

276
00:35:55,000 --> 00:36:08,000
So, yeah, I think it depends on you to find these things but I like to think of graph nets as more a broader class and so transformers graph nets with the fully connected graph so every token talks to every token.

277
00:36:08,000 --> 00:36:19,000
And the aggregation function that decides how to do like the weighted sum of incoming messages is given by this attention operator.

278
00:36:19,000 --> 00:36:27,000
The similarity between node A and node B tells you how much the messages from node B will be summed up into node A.

279
00:36:27,000 --> 00:36:40,000
And you can have attention graph neural networks that's why you feel it's and that's, I think it was independently invented over there or maybe even first invented over there so graph net people have done attention.

280
00:36:40,000 --> 00:36:51,000
Before transformers made attention really popular, although of course attention is an old idea as well and it has shown up a lot of times.

281
00:36:51,000 --> 00:36:53,000
So I guess I can start now.

282
00:36:53,000 --> 00:37:07,000
Yeah, still gave a great overview which actually is going to make this a lot easier, where I think I want to go over more of the AI machine learning communities perspective of attention and kind of the developments happening in that community, and go over some kind of

283
00:37:07,000 --> 00:37:14,000
things about transformers that I think are surprising I guess to a lot of people now.

284
00:37:14,000 --> 00:37:28,000
To go over that let's kind of go over how it started so originally the transformer was a language model, and the language model paper had possibly the most arrogant title you could imagine, saying that attention is all you need.

285
00:37:28,000 --> 00:37:44,000
So whether they know that that actually turned out to be something that would actually have more truth to it. And I think most people realize including I think myself, which is that this now is a model that covers I still mentioned language speech vision.

286
00:37:44,000 --> 00:37:53,000
It's basically a universal model now for all the modalities that people apply to data actually.

287
00:37:53,000 --> 00:38:12,000
And attention is not a new idea. This is something that was proposed, even before this paper but this paper was something that was very very similar to the vision transformer attention, where they created essentially attendable feature maps at the last layer or the next to

288
00:38:12,000 --> 00:38:23,000
the CG network that was still spatial, and they're able to use this to show that when you do image captioning, it would attend to the correct parts of an object.

289
00:38:23,000 --> 00:38:38,000
So, the reason why I think this kind of seminar is important is because, classically, you know, whatever works well has always been something that we always ask ourselves is the brain also doing this because that's been the classical thing for a lot of the past, you know, 10 years of research

290
00:38:38,000 --> 00:38:52,000
actually a lot from Jim DeClos group. And I think we have to understand what's going on I think to understand why transformers are so important and potentially why we should either care about them or not care about them actually.

291
00:38:52,000 --> 00:39:05,000
But I think there's this all too familiar aspect where whatever is working extremely well, we need to compare that to the brain now. And I think we should kind of get ahead of it this time, because in this case we want to see where they're going.

292
00:39:05,000 --> 00:39:22,000
And, again, one of the things that's kind of interesting is that when people compare these attention models specifically certain versions of them like clip to human behavior, you get a substantially improved performance in terms of like explaining

293
00:39:22,000 --> 00:39:39,000
whether the misclassification errors are not in human visual systems. And I think the issue is like, we don't know why this is happening in the sense that like, in this paper, they mentioned that the particular vision transformer model that they tested was far better than all their

294
00:39:39,000 --> 00:39:55,000
other models for explaining human behavior, but they weren't sure why at the time this was from a 2021 paper so I believe that they're still not sure but they what all they found was that the results show that this particular model clip was an outlier for their comparisons to human behavior.

295
00:39:55,000 --> 00:40:07,000
And also there's been, like I said, you know this natural tendency to now compare convolution networks to transformer networks because the transformer networks are starting to outperform covenants, a lot of vision tasks.

296
00:40:07,000 --> 00:40:20,000
So the question is now, which models are more similar to neuroscience or just human vision in general whether it be cognitive science as well, our contacts more similar or transformers more similar.

297
00:40:20,000 --> 00:40:33,000
And intuitively you would think that there are certain properties of convolutions that make convolution convolution and transformer transformer. And what makes what we believe convolution and convolution is the fact that of

298
00:40:33,000 --> 00:40:48,000
equivariance, meaning as Phil mentioned, when you apply an operation to the input, the operation applied to the output should be the same type of operation. Now the issue is that this turns out not to be exactly true it turns out after training a vision

299
00:40:48,000 --> 00:40:57,000
transformer is more equivariant to translation than a continent, which I think is quite surprising so Phil might actually already know the issues with the convolution not being equivariant.

300
00:40:57,000 --> 00:41:11,000
One thing being that a lot of the pooling and other operations even the non linearity contribute to hurting or harming the covariance performance. So this plot here is showing by this paper's measure, which is a lead derivative, the

301
00:41:12,000 --> 00:41:19,000
equivariance error of a component. This is Resnet 50 versus a vision transformer and turns out after training.

302
00:41:19,000 --> 00:41:26,000
A vision transformer is translationally more equivariant than a component, which I think is quite not intuitive.

303
00:41:26,000 --> 00:41:35,000
And the, in the sense that we built in the equivariance specifically for a component, yet here we are and vision transformers are more for variant after training.

304
00:41:35,000 --> 00:41:50,000
And then this on the right shows this is not just specific to these two architectures but many different architectures here. It's also an interesting to be also measure MLP mixers, which as Phil mentioned is kind of a new architecture also that also has surprisingly high

305
00:41:50,000 --> 00:41:59,000
equivariance after training. And this is equivariance improves as you increase image net test accuracy. So you have any questions here.

306
00:41:59,000 --> 00:42:03,000
Brian, one question. Do you have physical encoding on these networks?

307
00:42:03,000 --> 00:42:04,000
Yeah, you do have visual.

308
00:42:04,000 --> 00:42:07,000
So it shouldn't be equivariant.

309
00:42:07,000 --> 00:42:09,000
Oh, well, this is like a measure of equivariant.

310
00:42:09,000 --> 00:42:11,000
No, no, I mean, it shouldn't be like.

311
00:42:11,000 --> 00:42:12,000
Oh, it shouldn't be.

312
00:42:12,000 --> 00:42:13,000
Yeah, it shouldn't be.

313
00:42:13,000 --> 00:42:14,000
We have to learn to be.

314
00:42:14,000 --> 00:42:19,000
So equivariance is very strictly by the patch. In this case, they're doing small transformations.

315
00:42:19,000 --> 00:42:22,000
So it's not the size of a patch.

316
00:42:22,000 --> 00:42:38,000
But since you asked that question, there's another paper that shows about invariance. And this is like much larger translations. And they show that actually after training a train vision transformer, which is a transformer model is as invariant to

317
00:42:38,000 --> 00:42:43,000
a translation shifts as a ResNet 18, which is a component.

318
00:42:43,000 --> 00:42:58,000
And this is kind of interesting because then the question is what's going on here, like, you know, we build in these things for our models, but apparently not building them in also gives us the thing that we want it.

319
00:42:58,000 --> 00:43:10,000
So this kind of goes to the issue that maybe might be controversial, but has been kind of true, which is the idea of scalability. And are people familiar with the bitter lesson here, or has anyone heard of the bitter lesson.

320
00:43:10,000 --> 00:43:21,000
This is like a classic controversial statement. I actually think a professor, a former professor here actually gave a retort to this thing by Rich Sutton who wrote that the lesson that we should take away.

321
00:43:21,000 --> 00:43:31,000
And I think in 2019 or 2018 is that we should not be working on inductive biases for models as much as we should be working on inductive biases for learning.

322
00:43:31,000 --> 00:43:48,000
And I think one of the kind of key directions that organizations like OpenAI are going towards is the idea that we shouldn't really try to bake in these priors that we think are really, really useful, because at some point your data might give you that prior anyway.

323
00:43:48,000 --> 00:43:52,000
And that could be what's happening here.

324
00:43:52,000 --> 00:44:02,000
And to give you a flavor for what's going on in the machine learning community, they're essentially optimizing the upper right hand corner of this curve.

325
00:44:02,000 --> 00:44:18,000
The x-axis here is the compute required to train these models. And the y-axis is the negative log-proplacity, which is essentially the task error, or task performance, I guess, not error.

326
00:44:18,000 --> 00:44:37,000
And in this case, what they're doing is they're trying to find architectures that go all the way up here in the performance curve. And they're essentially paying for that performance, not by incorporating inductive biases into their model, but by trying to absorb those inductive biases through data.

327
00:44:37,000 --> 00:44:56,000
So I think to give you a glimpse of what's happening in the future is that there probably will be less inductive biases in this community of machine learning, because they're willing to pay this cost of compute to not have to build in the inductive bias that they would normally have to build in for smaller data sets.

328
00:44:56,000 --> 00:45:18,000
And as Saul alluded to, which is the, or as actually Tommy also mentioned, it's just these architectures are getting more and more general purpose. So in this case, this is a transformer architecture, but unlike a transformer, this is an architecture that can attend not over the token level, but treats each pixel as a token.

329
00:45:19,000 --> 00:45:43,000
And what's interesting about this is that they actually train this on ImageNet without any image prior. So the positioning coding that Phil mentioned was actually learned by the model, and they could actually get performance on the order of, you know, normal image prior oriented models like comp nets and, you know, patch vision transformers.

330
00:45:43,000 --> 00:46:00,000
So the remarkable consequence of this is that there's no actual image prior or modality prior built into this model. It learned it on its own. And even in that perspective, it's still competitive on ImageNet. So this is not necessarily a very large data set either.

331
00:46:00,000 --> 00:46:06,000
Do you know if the perceiver of Brian also has like a texture bias, for example, or if in general robustness?

332
00:46:06,000 --> 00:46:27,000
I don't think we've looked into that. I think the robustness qualities are probably not too different from standard models. I think in terms of adversarial examples, like transformers are, well, I think it's controversial, but I think people say transformers are a little bit more robust adversarial examples than comp nets, but on the grand scheme of things, they're both very susceptible to adversarial examples.

333
00:46:28,000 --> 00:46:41,000
I mean, just to finish, I was just curious to see if, you know, I'm sure there's many models now that they can all do whatever 75% on ImageNet, but not all of them will see like a human, whether that's a goal or not, right?

334
00:46:41,000 --> 00:46:50,000
Right, that's a good question. And I think like we have to now be aware that this is where the field is going. Is that where we're going as well as a field? Not, yeah.

335
00:46:50,000 --> 00:46:53,000
What does it mean to learn position features in the case of an image?

336
00:46:53,000 --> 00:46:58,000
Well, you randomly initialize the position encoding to be just from like a random Gaussian or something.

337
00:46:58,000 --> 00:47:03,000
The position of the pixels in the image, so it's like an unordered set of pixels.

338
00:47:03,000 --> 00:47:09,000
Right, so you treat the pixels as, you treat each pixel as an unordered element in a set.

339
00:47:09,000 --> 00:47:16,000
Wait, Brian, is there the case that the top left pixel will always get the same positional encoding? It'll be a learned.

340
00:47:16,000 --> 00:47:21,000
Yeah, so, yeah, so it's not like you shuffle every image independently of every other image and then get.

341
00:47:21,000 --> 00:47:24,000
So I think it's still inserting a lot of information there.

342
00:47:24,000 --> 00:47:28,000
Well, the prior that they're inserting is that the topology is consistent across samples.

343
00:47:28,000 --> 00:47:29,000
Yeah.

344
00:47:29,000 --> 00:47:37,000
Which I think is reasonable about modalities mostly, like you would imagine that the position shouldn't be unique to every single sample.

345
00:47:37,000 --> 00:47:44,000
Because in that case, then I think the problem would be almost like really, really hard.

346
00:47:44,000 --> 00:47:46,000
I don't know if you can even learn anything.

347
00:47:46,000 --> 00:47:48,000
Like randomly shuffle all the pixels in an image.

348
00:47:48,000 --> 00:47:50,000
Yeah, I don't know what you're going to be able to learn.

349
00:47:50,000 --> 00:47:53,000
Every image has its own permutation that you give to the models.

350
00:47:53,000 --> 00:48:00,000
You could learn like higher like statistics, I guess, over like what fixes together.

351
00:48:00,000 --> 00:48:08,000
Another number is how many training example it takes to train this compared to a convolutional network.

352
00:48:08,000 --> 00:48:14,000
Right, so I think one of the things that we know is that it takes more data to train a vision transfer.

353
00:48:14,000 --> 00:48:16,000
But do you have the number for this?

354
00:48:16,000 --> 00:48:19,000
This is, I think, just ImageNet with augmentations, actually.

355
00:48:19,000 --> 00:48:21,000
This isn't like a larger data set.

356
00:48:21,000 --> 00:48:25,000
This is just augmentation, providing the extra information.

357
00:48:25,000 --> 00:48:30,000
Yeah, I don't have the numbers, but the cursor I've seen tend to be, you know, we have a whiteboard.

358
00:48:30,000 --> 00:48:36,000
So the connet is it starts here and goes kind of flat and the transformer starts down here and goes up.

359
00:48:36,000 --> 00:48:40,000
So it scales better, but for low data, it's very worse.

360
00:48:40,000 --> 00:48:58,000
I think that's one of the things that why the community is going towards like industry, especially going towards this direction, because they're willing to pay for the scale via compute costs, rather than having the kind of built in data points, I guess, be not, you know, for free, I guess,

361
00:48:58,000 --> 00:49:15,000
What happens is that convolutions after a certain level of scale will start saturating and transformers still continue to go up in terms of increasing the parameter count will give you still positive returns in performance.

362
00:49:15,000 --> 00:49:20,000
What's interesting in this talk is also there have been a lot of other variations of transformers as well.

363
00:49:20,000 --> 00:49:29,000
And it's kind of strange, actually, people have at least Google has done a meta analysis on transformers and all the zoo of their variations of them.

364
00:49:29,000 --> 00:49:36,000
And at least in terms of scalability, it seems like the original transformers actually scales the best, which is a little bit odd.

365
00:49:36,000 --> 00:49:38,000
Which one's the best.

366
00:49:38,000 --> 00:49:41,000
The vision transformers are also sparse mixture of extra transformers.

367
00:49:41,000 --> 00:49:46,000
Those apparently both scale the AT the AT well.

368
00:49:46,000 --> 00:49:54,000
Don't remember if they were specifically doing vision. I think it was more language tasks.

369
00:49:54,000 --> 00:50:09,000
I think this kind of tells you a story that maybe the machine learning community is going towards, which is not, you know, the fact that architecture matters the most, but the fact that data is actually the really important aspect and I think they're

370
00:50:09,000 --> 00:50:22,000
building now architectures that aren't necessarily good at working well without, you know, being trained, but furthermore work well at absorbing data a lot more efficiently than other architectures.

371
00:50:22,000 --> 00:50:38,000
So like we should think about the things that leads to the resulting model that we work with these days, which is the idea that it's not just the model itself with its architecture, but there are priors about, you know, the compute the capacity of the model to train

372
00:50:38,000 --> 00:50:56,000
that data and also more importantly now more recently. Nice thing is that like as Phil mentioned the notion that a lot of these models don't require supervision so they can absorb larger and larger data sets without much economic cost to the person training it.

373
00:50:56,000 --> 00:51:10,000
One also another aspect of Transformers is the way it was created was mainly as an alternative to recurrent networks. And the reason for that is because as you see in the left arrow I mentioned hardware, and one of the things about Transformers is that they're much more

374
00:51:10,000 --> 00:51:26,000
on GPUs than a recurrent network is and they run much more better on like, you know, stupidly parallel software and a recurrent network does and that's why they become very popular also is because the parallelization of them is much easier than other sequence models.

375
00:51:26,000 --> 00:51:38,000
One comment if I may make how I view this like I feel when you go to Transformers just by introducing the idea of attention dynamically handled by the data you kind of have.

376
00:51:38,000 --> 00:51:51,000
It's an overstatement but you know as general architecture as you can have kind of like, you know, you have really powerful architecture and now to train it you need a lot of data and this is really where we are now.

377
00:51:51,000 --> 00:52:07,000
And more data does better just because the architecture is extremely general, much more than conventional as you said. So I don't know if there will be any other fancy architecture.

378
00:52:07,000 --> 00:52:19,000
One is there are lacking one thing which is vanilla Transformers don't have memory. They don't have feedback connections. And so they're not doing complete in the same way that RNN is.

379
00:52:19,000 --> 00:52:31,000
Of course people are adding memory and recurrence to Transformers but the still the majority of them don't have that so that's like actually I think a big limitation they don't have memory.

380
00:52:32,000 --> 00:52:46,000
And then two is yeah, you know, eventually look up tables will perform best or you know nearest neighbor will perform best because the limit of infinite data those work and we don't have these work in the limit of infinite data.

381
00:52:46,000 --> 00:52:52,000
Yeah, I agree it's not surprising that you need less traction with more data.

382
00:52:52,000 --> 00:52:57,000
Brian you were saying like there's choices to be made for if we're interested in models of brain systems.

383
00:52:57,000 --> 00:53:09,000
So the question is like, what are we interested in in terms of if these architectures become less biased towards being structurally more relevant to neuroscience, but just being more task relevant to neuroscience.

384
00:53:09,000 --> 00:53:14,000
Like, are we going to be stuck at some level of understanding that's only functional.

385
00:53:14,000 --> 00:53:21,000
What makes them structurally that's relevant that's sort of why I was asking these kind of wait questions right. Why do you say that.

386
00:53:21,000 --> 00:53:29,000
Well, I don't. Yeah, so I think the tricky part is like, I think, because things work well people will find ways to say that they're more structurally relevant.

387
00:53:29,000 --> 00:53:42,000
I don't know if Transformers are more structurally relevant than comp nets like obviously there's Fukushima, you know neocognitron which is inspired by neuroscience but attention themselves is never inspired by transformers was never inspired by neuroscience.

388
00:53:42,000 --> 00:53:57,000
So I don't know if they're actually more neuroscience friendly, I guess, in the terms of similarity. But I think what the bigger picture is that they're going to be more and more generic.

389
00:53:57,000 --> 00:54:01,000
And they're going to take less inspiration from structural biases that we know about.

390
00:54:01,000 --> 00:54:10,000
Not necessarily until you know full mentioned the idea of like, you know, recurrence and you know feedback and those things become more.

391
00:54:10,000 --> 00:54:25,000
It's like, actually, one thing that's interesting is that like, there hasn't been much. I mean people have proposed these architectures and they do work but people aren't really using feedback versions of Transformers, mainly because there's no recurrent nature to them.

392
00:54:25,000 --> 00:54:30,000
So for example, all these things are still fee forward architectures that still process from the bottom up.

393
00:54:31,000 --> 00:54:46,000
I think the divergence is going to be like, are we going to be interested in the same models that the AI machine learning committee is going to be interested in? Are we going to be interested in specific models that work on neuroscience but don't necessarily have a functional performance equivalence to what these models have?

394
00:54:46,000 --> 00:54:57,000
And one question I have for the neuroscientists in the room is, what is the scale of data that the human brain is trained on when they reach adulthood?

395
00:54:57,000 --> 00:55:08,000
And my rough estimate or understanding is it's similar to what the biggest Transformers are currently trained on or a little bit smaller than that, but much more data than comp nets were trained on.

396
00:55:08,000 --> 00:55:21,000
So all this stuff about how, you know, in which data regime do you get what kinds of performance? Well, the data regime that seems most relevant to neuroscience, to me, seems more like this Transformer regime, but I don't know if that's true.

397
00:55:21,000 --> 00:55:25,000
Like how many images do humans see compared to these models?

398
00:55:25,000 --> 00:55:27,000
It's not true for text.

399
00:55:27,000 --> 00:55:29,000
Text, I think they're trained on much more data. Yeah.

400
00:55:29,000 --> 00:55:33,000
Yeah, but for you, I'm thinking images, everything on the internet.

401
00:55:33,000 --> 00:55:34,000
Yeah.

402
00:55:34,000 --> 00:55:51,000
There was a paper for a friend to rank this group recently that showed that even if you train a GPT two on a 10 year old amount of text data, it does explain fMRI responses, almost as well as even having a lot more days because fMRI is bad.

403
00:55:52,000 --> 00:56:04,000
So I think he's training a Transformer around 10 million tokens, which is like what a child would be exposed to at the age of 10, while most Transformers are trained in like, like, millions of different systems.

404
00:56:04,000 --> 00:56:12,000
And like, yeah, it could accept my data, but yeah, fMRI is its own.

405
00:56:13,000 --> 00:56:21,000
This is, there's an assumption under this question, which is like, are we actually interested in models of the system in the adult state, or are we interested in models of how the system gets to be in the adult state?

406
00:56:21,000 --> 00:56:24,000
Those are not the same question, right?

407
00:56:24,000 --> 00:56:41,000
So there may be a shift here between models that are like, and you sort of called it out like, hey, we can, instead of us having to hand design them in, this is the bitter lesson version, we'll just lean on the data with a general flexible thing and let the data push it as long as our compute can handle that and we have enough data.

408
00:56:41,000 --> 00:56:51,000
And I think the question you were asking, that's sort of interesting to us, some of us is, does that end state, which of those end states looks more like the adult and the state that's agnostic to whether,

409
00:56:51,000 --> 00:56:57,000
neither of them probably followed the same biology path, but just even in that assumption state, what is the state of affairs?

410
00:56:57,000 --> 00:57:05,000
I don't think we know what the state of affairs is, visual Transformers relative to combat sun alignments with even visual processing.

411
00:57:05,000 --> 00:57:11,000
I mean, somebody was asking here about like somebody who was asking about similarity, maybe that was you.

412
00:57:11,000 --> 00:57:20,000
You know, and then because also at the neural level also requires mapping assumptions that, and they get more complicated with the Transformers, right, like,

413
00:57:20,000 --> 00:57:30,000
but behaviorally, it sounds like, you know, in the Gary was papers are pointing out like there's some, maybe better alignment, but I don't have their compare against the latest, you know, at trained, you know, comments.

414
00:57:31,000 --> 00:57:40,000
Yeah, I don't know about, I think those stories haven't been done to my knowledge of like actual alignment with neural recordings and, of course, I'm sure people here will do that.

415
00:57:40,000 --> 00:57:46,000
But alignment in terms of functional capabilities does seem quite a bit better.

416
00:57:46,000 --> 00:57:56,000
And it don't relate to me because we're a confidence. What can they do in what's been demonstrated with confidence of 10 years ago, classify 1000 animals, cats and dogs and ancient categories.

417
00:57:56,000 --> 00:58:09,000
What can transfer? Well, sure, you can make confidence that grow bigger, but the current generation of the best models are these Transformers like clip, but of course there's a clip non transformers and let's just say clip.

418
00:58:09,000 --> 00:58:18,000
And that seems much closer to the functionality of the human visual system that you can recognize millions of categories or way more than 1000s of categories.

419
00:58:18,000 --> 00:58:30,000
And you can recognize compositions of categories you can type in a red ball and have you recognize the red ball and just see one example of that and, and these, these networks are getting to that point.

420
00:58:30,000 --> 00:58:36,000
So the kind of psychophysical level I think they're getting closer. I don't know at the neural embedding level.

421
00:58:36,000 --> 00:58:50,000
So something that I'd like to share is the, there was one paper that me and William McCullough that we submitted to nerves, it actually got rejected, but there was one about a we just submitted isolar about this transformer model that

422
00:58:50,000 --> 00:59:01,000
achieved state of the art and brain score for area before, which is kind of interesting because we went to the brainstorm competition at the beginning of this year, like just hoping to participate.

423
00:59:01,000 --> 00:59:13,000
And all of a sudden William trained this transformer it was a dual stream transformer with adversarial training and rotations, and we just like broke the record in v4 unexpectedly and wrote a paper about that.

424
00:59:13,000 --> 00:59:27,000
In any case, what I think was interesting is that the same model exact same architecture if you trained it another way just classical SGD image net no fans augmentations adversarial perturbations, the score wasn't that great.

425
00:59:27,000 --> 00:59:40,000
So I wonder, just in general, should we also just be thinking about transformer model or the interaction of transformer models plus any type of training regime or maybe a fancier loss function that we haven't even conceived.

426
00:59:40,000 --> 00:59:53,000
And suppose we do hit like the like explain variants or one correlation in brain score for it like how do we even reverse engineer from that right because the model is just so big.

427
00:59:53,000 --> 01:00:04,000
I'm saying playing devil's advocate on my own work really the model so big, like, how do how do we even go back and isn't open a question, you know just I don't have any ideas or

428
01:00:04,000 --> 01:00:14,000
I mean I think one of the things that I think we often forget is that a model isn't just its architecture like you know the slide before a model is also data.

429
01:00:14,000 --> 01:00:21,000
And once it's interact with data, we have to understand data now to understand what that model is doing can't just understand the architecture.

430
01:00:21,000 --> 01:00:31,000
I think as these models become these architectures become more generic data is going to play a larger and larger role and we're kind of back to now trying to understand data now and understanding what the architecture is.

431
01:00:31,000 --> 01:00:35,000
And I think that's not that's easier or harder.

432
01:00:35,000 --> 01:00:53,000
But also the message is, you know, for the last 10 years until transformers came like three or four years ago, I think the success stories in machine learning was convolutional networks, there was one architect.

433
01:00:53,000 --> 01:00:55,000
Now there are several.

434
01:00:55,000 --> 01:00:59,000
We have quite a few options.

435
01:00:59,000 --> 01:01:02,000
You know they all perform pretty well.

436
01:01:02,000 --> 01:01:04,000
That's right.

437
01:01:04,000 --> 01:01:15,000
And I think if you just compare functions input output or a while they fit the neurons you'll find they're all doing okay.

438
01:01:15,000 --> 01:01:25,000
You need a lot of other constraints, which means what can be implemented by neurons and synapses.

439
01:01:25,000 --> 01:01:31,000
And what cannot or very difficult to see how.

440
01:01:32,000 --> 01:01:39,000
I agree and I gave this guest lecture and Brian's class.

441
01:01:39,000 --> 01:01:53,000
And I was calling it the anachronism conjecture that like as systems get more and more intelligent they kind of converge on the same representations abstractions models and so forth, which other people have you know put forth.

442
01:01:53,000 --> 01:02:01,000
And I think it's kind of the same here I don't actually think the difference between transformers and confidence and MLP is that dramatic. I think it's more.

443
01:02:01,000 --> 01:02:08,000
As you get more and more data and you optimize more and more toward success at some objective.

444
01:02:08,000 --> 01:02:10,000
The models will converge.

445
01:02:10,000 --> 01:02:18,000
It's one way but the other way is that, you know what I said that the meeting a few weeks ago.

446
01:02:18,000 --> 01:02:21,000
It could be like flight.

447
01:02:21,000 --> 01:02:23,000
You know, it could be.

448
01:02:23,000 --> 01:02:29,000
You have, you know, a model of a bird.

449
01:02:29,000 --> 01:02:34,000
But that's, that's not really good for everything.

450
01:02:34,000 --> 01:02:51,000
The important is to understand the principles of aerodynamics, then you can understand how birds fly and how to build airplanes and other things and maybe how fly flies, which is different from birds.

451
01:02:51,000 --> 01:03:04,000
Those aerodynamics involved is different. So, I think principles are much more important than the specific implementations which can be quite different.

452
01:03:04,000 --> 01:03:08,000
The question is what are the principles here.

453
01:03:08,000 --> 01:03:11,000
Yeah, and I think they're similar principles, I think.

454
01:03:11,000 --> 01:03:19,000
All of these pictures are like just reweighting at the same few ingredients factorization is in all of them.

455
01:03:19,000 --> 01:03:37,000
Hierarchy is in all of them right like, I don't know, and even in transformers and confidence like transformers can be written as 90% convolution and just a few little layers that are attention like if you look at the actual operations almost every operation is is a convolution in the sense of being a one by one,

456
01:03:37,000 --> 01:03:43,000
you can just chop up the signal into patches and process each one independently and identically.

457
01:03:43,000 --> 01:03:47,000
Yeah, so I think the principles are going to turn out to be very similar.

458
01:03:47,000 --> 01:03:57,000
The question is, which principles should we care about now, given this kind of heterogeneity and architecture, but similarity and functional performance.

459
01:03:57,000 --> 01:04:09,000
One of the things that it becomes easier if the community like has something they cannot do, whether it be like, you know, to fly, I guess, for example, flight, which is like, how do we achieve something that we can't currently do right now.

460
01:04:09,000 --> 01:04:18,000
I think the general spirit of the machine committee is that all we're done, we can just keep making these models bigger and keep doing this and we'll be fine.

461
01:04:18,000 --> 01:04:28,000
I don't think that's true. But right now it seems like the spirit is in that direction and that's why we're kind of revalidating. Oh, of course this must be like the brain. Of course this is like what we care about.

462
01:04:28,000 --> 01:04:32,000
Of course, all these kind of back explanations are working.

463
01:04:32,000 --> 01:04:43,000
But once we hit a wall, I feel like then we know what's wrong and what's correct. Otherwise, I guess it becomes kind of hard to tell right now.

464
01:04:44,000 --> 01:05:03,000
The topic of like data efficiency. This might have an obvious answer, but like, I was wondering if when you have multimodal data, whether like learning becomes a lot more efficient if you have like experts of information from visual stuff,

465
01:05:04,000 --> 01:05:19,000
like texts, that like maybe our captions associated with the image or something versus like two aspects of just text data.

466
01:05:20,000 --> 01:05:33,000
I don't know how well those things have been estimated, but the language vision models are a lot better on certain benchmarks than the vision only models and it does seem like language must be like incredibly valuable per word.

467
01:05:33,000 --> 01:05:37,000
There's a lot more information than per pixel.

468
01:05:37,000 --> 01:05:43,000
I feel like a lot of the recent successes, just leveraging language, at least in computer vision.

469
01:05:43,000 --> 01:06:01,000
Same with robotics a few other areas. I think what made clip a lot more, at least from that psychophysics experiment from Garrus at all, a lot more like powerful for their results was the fact that clip was trained on classification, but on caption similarity matching

470
01:06:01,000 --> 01:06:07,000
to a text caption which has a lot more information than just a single label for this entire image.

471
01:06:07,000 --> 01:06:21,000
Like caption can tell you, you know, things about geometry tells you what things on the left, what things included, what thing is, what thing what what season it is or what time it is like it tells you a lot more information and a single word would be to

472
01:06:22,000 --> 01:06:24,000
ImageNet class.

473
01:06:24,000 --> 01:06:38,000
Another, okay, this is a little anecdotal, but what I've heard is that for training diffusion models, if you train them without language, just unconditional diffusion model generative model of imagery is really expensive and we all thought like, okay, we're not getting that game is not for us is for Google.

474
01:06:38,000 --> 01:06:50,000
But if you train them text conditional, they're actually much according to the students I've talked to, they train much faster because the text conditional models the text gives you so much leverage.

475
01:06:50,000 --> 01:06:58,000
And so they said that no, no, we can train text conditional like no dolly type models stable diffusion those things are within the budget of MIT.

476
01:06:58,000 --> 01:07:06,000
You do that to trade condition because you have to have text image pairs see if a lot. This is a huge source of supervision here, as opposed to just random images.

477
01:07:06,000 --> 01:07:15,000
And if you have that, then you're in like the hundreds of thousands of dollars of range to train one of those big models, as opposed to the tens of millions of dollars range.

478
01:07:15,000 --> 01:07:18,000
This is anecdotal.

479
01:07:18,000 --> 01:07:21,000
They might be just trying to get some GPUs. I'm not sure.

480
01:07:21,000 --> 01:07:34,000
By the way, address a question to both of you, but an important feature of transformers compared to previous networks is the fact that you don't have to worry about labeling, especially when you use text, right?

481
01:07:34,000 --> 01:07:36,000
Yeah.

482
01:07:36,000 --> 01:07:45,000
I think that's the issue about the supervision of supervision, which is like I never found a consistent definition of what a supervised task is versus not supervised one besides economic cost.

483
01:07:45,000 --> 01:07:49,000
Like how much did you spend to acquire this data seems to be the only consistent label.

484
01:07:49,000 --> 01:07:53,000
But if you speak about neuroscience is not only cost, right?

485
01:07:53,000 --> 01:07:59,000
Right, but then like I think, yeah, this is another divergence in between two communities, right, which is like.

486
01:07:59,000 --> 01:08:10,000
But the real problem is the brain. Come on. Everything else is

487
01:08:10,000 --> 01:08:16,000
More, more questions. I'm happy to keep chatting much for one more spester.

488
01:08:16,000 --> 01:08:25,000
I think we want to quite some time, we can adjourn and to the next iteration sometime in the next few weeks.

489
01:08:25,000 --> 01:08:27,000
Okay. Thank you.

