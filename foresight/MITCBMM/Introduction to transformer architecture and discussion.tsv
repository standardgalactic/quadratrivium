start	end	text
0	10000	If we look at history, models of the brain have always been the most fashionable technology.
10000	22000	It was hydrodynamics in the 17th century and digital computers a few decades ago.
22000	29000	And ten years ago was convolutional neural networks.
29000	33000	And now, transformers of course.
33000	37000	So in preparation for this, we have an introduction to transformers.
37000	47000	I think this is something that Michael Fee, chairman of this department here and Ghibli Carlo asked for.
47000	54000	Of course they are not here, but Michael actually asked me to record it.
54000	57000	Is that possible?
57000	61000	It's recording now as long as that's okay with you guys.
61000	65000	Everybody agrees to be recorded. Okay, all right.
65000	75000	So with this we have an introduction to transformers and this probably will be the first of some of the research meeting where we'll speak about transformers.
75000	83000	Next time perhaps more in comparison in the framework of models of the brain.
83000	89000	But this time it's more on just technical aspects of transformers.
89000	100000	And so Philip Tinsola, C-Sale and Brian Chung, this building, will tell us how Jim is here after all.
100000	104000	Okay, so Lupus in Fabulous.
104000	106000	What was that?
106000	110000	Lupus in Fabulous exactly means what happened, that I will speak about you and you came in.
111000	115000	So Philip will start.
115000	126000	And then we have Brian and then perhaps discussion and in the meantime, of course, very welcome to ask a lot of questions.
126000	127000	All right.
127000	130000	Okay, great. Thank you, Tommy.
130000	136000	So this can be, as far as I'm concerned, very informal and happy to go back and forth.
136000	149000	I took slides from a lecture, like hour and a half electron transformers I've given, and tried to pick a few slides. So it's going to be a quick intro, but we can go into as much detail as you want, and happy to make it also just more discussion.
149000	156000	But I'll start by talking about, you know, what are transformers, you know, for those of you who haven't encountered them yet.
156000	168000	Okay, so I think that there are two key ideas that are not new but are relatively popularized by transformers. And the first is called the idea of tokens.
168000	171000	And then the second is going to be the idea of attention.
171000	176000	There's a few other, you know, bits and pieces and transformers, but to me, those are the two kind of key ideas.
176000	183000	Okay, so tokens are essentially a new data structure.
183000	197000	And they're a replacement for neurons. Okay, so in artificial neural nets, neurons are just real numbers, just scalar numbers, and tokens are vectors. So that's, that is the distinction that, as far as I'm concerned.
197000	201000	So it's just lingo for a vector of neurons, a vector of scalars.
201000	209000	But when you start thinking of your primitive units in a network as tokens, whether they're neurons, then sometimes the math will look a little bit different.
209000	217000	And I think it's just a powerful way of working with new kinds of neural networks made out of tokens instead of neurons.
217000	221000	Okay, again, it's a patch in an image.
221000	235000	It's a patch in an image can be a token. I'll give a few examples and not not any ideas vector neurons vectors of, you know, features for a unit inside a network that's an old idea, you know shows up in graph neural networks if you've seen those two.
235000	250000	Okay, but the way I like to think of it is we used to work with arrays of neurons and now in transformers we work with arrays, tensors of various shape of tokens, but they're kind of like encapsulated, you know, vectors of information.
250000	259000	Okay, so you can tokenize just about anything and that is like the big trend right now is just show how to turn whatever data you have into a sequence of tokens.
259000	264000	So here's an example, like Tommy's mentioning of tokenizing an image.
264000	275000	You could do this in a million different ways but the way that is very common is you start with an image you break it into patches, you flatten all the patches to create just take all the rows of that.
275000	281000	Just pixels and just concatenate them into a really long row or really long colon vector.
281000	287000	And then you get this thing called a token which is just an n dimensional vector representing that patch.
287000	298000	And this arrow up here, if you can see my mouse, it could just be flattened or concatenate, but commonly it will actually be a linear projection to some, you know, fixed dimensionality.
298000	301000	So project to 128 dimensional vector for example.
301000	304000	Should I think of a token as an embedding? Is it the same or what?
304000	309000	Same thing. Yeah, so vector embedding of a patch would be a token in this context. Yeah.
309000	317000	So there's a lot of names for this but to me like yeah once you start thinking of things in terms of tokens like that's that's how I like to think of it.
317000	320000	Okay, so you can tokenize anything.
320000	323000	That's often a linear projection.
323000	327000	We used to operate over neurons now we operate over tokens.
327000	336000	And just a few more examples so whenever you can take your signal and chop it up into chunks and project each chunk to a fixed dimensionality vector.
336000	345000	Then you can tokenize that data so for language, people do this as well they chop up into little chunks which are often, you know, two characters or a few characters at a time.
345000	351000	And they project those into 120 dimensional vectors with sound you chop up into little tiny snippets of sound.
351000	359000	So you can tokenize anything and once you've tokenized it then it's just a sequence like representation a sequence of vectors.
359000	366000	Although really the way transformers work is they think of it as they treat it as a set not a sequence but people often talk of it as a sequence.
366000	368000	Okay.
368000	377000	So that's tokenization that's, I think that's the first critical idea and then everything ends up being the operators to find over tokens as opposed to over neurons.
377000	390000	So rather than taking linear combinations of neurons which is the common, you know, linear layer in a network, we take linear combinations of tokens where you just take a weighted combination of vectors instead of weighted combination of scalars.
390000	392000	Okay.
392000	400000	So I'm not in this, because it's only like a little intro going to go into the math and detail, but we can always come back if you're interested.
400000	412000	Okay, so in standard artificial neural networks, there's two key layers, there's the linear combination layer, the linear layer, and there is the point wise nonlinearity, which might be a ReLU or a sigmoid function.
412000	423000	So in tokens, there's the same thing, there's a linear layer, which we have here, just a linear combination of vectors, and then there's a token wise nonlinearity which is completely analogous to a neuron wise nonlinearity.
423000	437000	So this is a neural net that does a point wise nonlinearity over neurons that applies the same function ReLU to every neuron in a list, and the token net applies the same function F to every token in a list for an asset.
437000	444000	Okay, usually F is going to be itself a multilayer perceptron, it will be some parameterized function.
444000	449000	So equivalently think of this as a convolution over the tokens.
449000	455000	So, transformers, CNNs, it's all the same ideas kind of rehash, we can talk about that if people are interested.
455000	465000	But here's what it looks like, it's just a token wise operation that slides along the list of tokens. Okay, so you can see that looks like convolution sliding across the signal.
465000	468000	Okay, it's usually one layer, right?
468000	475000	It's usually one layer, I think that is usually linear ReLU to linear. Yeah, so it's a nonlinear function.
475000	480000	If it, yeah, it just has to be nonlinear so you get nonlinearities into the system.
480000	482000	I have a question.
482000	483000	MLP.
483000	485000	One layer of neither.
485000	490000	Well, two linear layer MLP, but sure. Yeah, you could call it something else.
490000	499000	I have a good question for the last slide, like, I think I have missed what is the Z here, like, in the last slide.
499000	505000	Oh, Z is the token code vector, meaning the vector of neurons inside that token.
505000	506000	Okay.
506000	509000	It has this code vector that lives inside it. Yeah.
509000	511000	Okay, thank you.
511000	512000	You're welcome.
512000	515000	We'll see you now with convolution here.
515000	529000	Conquering applies the same operator independently and identically to every item in a sequence, and this is doing the same thing. So it's like, you know, slide the filter across here to slide this nonlinear filter across the sequence.
529000	541000	But we give it as a one by one kernel because it doesn't the like the receptive field in the sequence dimension is just looking at one token at a time.
541000	554000	Okay, so here's a neural net. And here is what I'll call a token net. So it's just like a neural net alternating linear combination point wise nonlinearity but now it's linear combination of tokens and token wise nonlinearity.
554000	557000	And transformers are in that family.
557000	563000	Another name for that family is graph neural networks they have the exact same structure.
563000	566000	Okay, but the terminology is just different.
566000	571000	But they're the same thing token nets graph nets transformers are all the same thing.
571000	578000	You could say that token sorry transformers are special kind of graph net if you're looking at that way.
578000	584000	So so many connections that could be made, but I'm going to zoom right ahead and we can discuss all the connections.
584000	591000	So that idea number one is build things out of tokens vector valued units as opposed to out of neurons scalar value units.
591000	598000	Number two to me is attention. Maybe this is the most famous idea of transformers is they have this new layer called attention.
598000	601000	So let's look at what attention is.
601000	614000	Okay, so in a neural net or what I'm calling a token that you'll have linear combinations of inputs to produce an output or weight of some of the inputs to produce an output, and you will parameterize that mapping with weights w.
614000	624000	And in attention, you have a weighted combination of inputs to produce an output, but but the parameters or the values in the matrix a are not learnable.
624000	629000	Instead, they're going to be data dependent. They're going to be a function of the some other data.
629000	638000	So when you have data dependent linear combination where the weights are a function of some other data, then that's called attention.
638000	640000	Okay.
640000	649000	And so a is the weight matrix, and it's just a function of something else. It tells you what to attend to how much weight to apply to each token input sequence.
649000	656000	And you'll take a, you know, weighted sum, which is just matrix multiply a by the tokens.
656000	667000	Okay, notation, don't worry about it. So here's here's the intuition. So I can have attention given to me by some other branch of my neural network, maybe it's going to be a branch that passes a question.
667000	674000	That question will tell me what patches in the input I tend to the input patches are represented by tokens.
674000	682000	And I will say these are the patches that I'm going to place high weight on and then I'll take a weighted sum of the code vectors inside those patches pretty soon output.
682000	690000	So I could ask, you know, question, what is the color of the birds head I'll attend to the birds head. What is the color of the vegetation I'll attend to the background.
690000	698000	I'm just saying which tokens tokens or patches here are going to be getting a lot of weight to make my decision.
698000	707000	Okay, and then I'll report the best green because the token code vectors will have represented some information like the color of the token.
707000	709000	Okay.
709000	717000	There's a little too much detail to fully understand in just a few minutes but here's the most common kind of attention layer just to show you the mechanics really quickly.
717000	731000	The question submits a query that is matched against a key in the data that you're querying so the data you're querying is a set of tokens and each token has a key vector which gets matched against the query vector of the question.
731000	746000	The dot product to get the similarity between the key and the query and that dot product that similarity becomes the way you apply to another transformation of your tokens which is called the value vector and you take a weighted combination of the value vector.
746000	764000	And so, although all the fancy math here is just to say, the question will tell me which tokens to weigh heavily in a weighted sum to produce an output, but it'll be via these three transformations the token vectors, which are the key the value in the query transformations.
764000	766000	Okay.
766000	775000	So that's a little mechanistic detail we can go back to that if we want to discuss the nitty-gritty. The way it kind of looks like is this and this is common to most of the transformer architectures.
775000	781000	You have a bunch of you break into patches then you have a bunch of layers which are basically these one by one convolutional layers.
781000	795000	And then every now and then you have something that tries to mix information across space across tokens and that's called attention and that is just the tokens based on their values queries and keys will decide what should I you know which are the other tokens
795000	808000	should I average together to produce a new representation in the output. So this head might attend. So you know I should look at the other heads to decide how I can better recognize what's going on in this patch.
808000	812000	When you say one by one evolution you mean a patch is a pixel.
812000	824000	A patch is so you vectorize a patch and then you do like one by one convolution across all of those tokens where the channel dimension is the token vector.
824000	834000	OK, so here's MLPs and then transformers are to two changes. One is that rather than scalars they have tokens and other changes rather than having parameterized linear weights.
834000	845000	They have data dependent linear weights that are given by that special attention operator and that attention operator itself has parameters that define the key the query and the value and those are the learnable parameters of the system.
845000	852000	Philip, just a small detail, but you start with self attention and then you have a one layer.
852000	857000	Yeah, but in the previous slide you had the opposite order is that.
857000	862000	Well, so it will vary a lot between different architectures like you could alternate these different orders.
862000	867000	And I did gloss over the detail of this is attention from an external question.
868000	875000	But the more common thing is self attention where attention is coming from the image itself from the data itself.
875000	877000	And that's.
877000	888000	Okay, this is like attention from the question branch, but I could just have the data choose what in its own token sequence to attend to each token chooses which other tokens to attend to and that's called self attention.
888000	896000	That's like this picture here where this patch will decide what to attend to to get a better representation of that patch essentially.
896000	912000	Okay, so this is self attention really as opposed to you. That's why the dependency of the weights on the data is like this is attending to itself as opposed to attending f is not coming from some external source.
912000	922000	Okay, so I think this is the last one just to connect it to one of the most common common demonstrations of transformers is to do sequence modeling.
922000	929000	But really transformers are more about set to set operations, but you can you can represent a sequence as a set. So it's fine.
929000	933000	But, oh, oh, no, I think I got the wrong slide here.
933000	938000	Let me pull up the right one.
938000	948000	Yeah, this is what I wanted to show. Okay, so, so here's how it looks for doing next word prediction, and you can do next, you know,
948000	958000	prediction and sequence of proteins or the next sound wave prediction, you next meeting prediction. That's really common framework. So we, this is like a one layer transformer.
958000	970000	You know, we're going to say colorless green idea sleep, put that into the transformer we want to predict the blank, the words attend to each other, then you pass to this token wise nonlinearity, and then you make a prediction at the very end.
970000	980000	So just one example of what you have seen as transformers for sequences, but really transformers are more general and they're not just about sequence modeling. This is just one way you could use them.
980000	984000	Okay, so that's the 10 minute overview of transformers.
984000	989000	Let's see, should we do questions or Brian, do you want to just jump right in. Maybe you do questions.
989000	996000	Okay, questions will Brian set up.
996000	1005000	What is the history of these terms, you know, quest, query value.
1005000	1015000	So he's probably other people here know better than I do but I think it's all coming from the data retrieval database literature so you have a database where you have knowledge stored in kind of cells.
1015000	1023000	And you can query that database you say I want to find things related to drafts.
1023000	1036000	And then every cell will have like a label the key will be like this is a cell for mammals and this is a cell for drafts and here's a cell for plants, and you'll match the query to the key, and then the stuff inside the cell that you retrieve will be the value.
1036000	1041000	So I think it's coming from that.
1041000	1050000	This isn't like a fully formed question but I guess like in the framework of having question key value.
1050000	1065000	If, let's say the question was like what is the color of the turkey's head and for some reason, the turkey's head is like a feature that's complex enough that it won't, it can't be like well represented within a single question.
1065000	1073000	So is that like cause issues with like attending to the right thing or like waiting the right thing properly when
1073000	1076000	Yeah.
1076000	1091000	I suppose it could one one answer would be that you often do multi headed attention where you'll have, you'll take your sequence or your set of tokens and then you'll have K different query vectors and K different value vectors and K different
1091000	1100000	value vectors and each query can be asking a different type of you can say I want to match to the same color. Another one can say I want to match to the same geometry.
1100000	1114000	And, you know, when you optimize the parameters of this network it will somehow, you know, self organize that, well, it's useful to factor things into geometry and color then there'll be one attention head that cares about color and one that cares about geometry.
1114000	1121000	I'm just one quick thing on the transformer architecture and the non linearity.
1121000	1129000	If I recall correctly you do a normalization of each token right intuition for that why do you do that.
1129000	1133000	I haven't seen that too much before transformers.
1133000	1142000	Yeah, that's a great question so so normally you want your weighted sum to add up to one. So the weight should add up to one.
1142000	1154000	People do that. I treat that via the softmax over the weights. Yeah, is that we're referring to. Yeah, no I'm not referring to post the residual connection you do as part of the right before that.
1154000	1163000	Other layer norms. So, okay my understanding of transformers has progressed to the level of tokens and attention and all the rest layer norm and residual connections.
1163000	1172000	At this point I don't rock it I don't I don't know why that feels kind of like tricks that usually help neural networks train it's good to normalize things is good to have principles.
1172000	1179000	I don't see anything. I haven't noticed that anything specific to transformers about those ideas yet. They're generally useful tricks. Okay.
1179000	1187000	Transformers are very hard to train especially like several later ones like 12 and up if you don't have like residual connections or layer norm.
1188000	1192000	Okay, the training isn't very stable.
1192000	1198000	So it's not it's not. Yeah, also this attention between multiplication.
1198000	1203000	And that might not be as stable as normal non intentional layers just do addition.
1203000	1210000	That's the softmax right the softmax can probably help with that. Yeah.
1210000	1219000	So how do you decide what patches to use for instance the image or the text you also had like what seemed to be arbitrary.
1219000	1221000	Yeah, segmentation of the data.
1221000	1230000	Yeah, that's a great question. How like how do you do the tokenization like how do you design that I think it's super hacky right now so that feels like somewhere that people could do a lot of work.
1230000	1236000	One thing that does seem to happen is that the smaller you make your tokens if you have tokens that are a single pixel it's just the better you get.
1237000	1240000	Maybe the
1240000	1253000	what will happen is we'll just stop having clever tokenizations and just go down to whatever the atomic units of the data are like a single character a single pixel and that that just makes a choice for us because you can't go below that really.
1253000	1259000	I think the smaller envision transformers the thing I've seen is that the smaller the tokens are the better they tend to perform.
1259000	1270000	But because the attention mechanism is every token it tends to every token it's like n squared if you make the tokens too small, there's too many tokens and then you run out of memory.
1270000	1291000	So there's probably clever tokenization schemes like super pixels or segmentation or in language spaders, you know, a lot of tokenization schemes that are like bite code by parent coding is the name of one and work that that could be used that as your first tokenization layer.
1291000	1294000	But yeah that feels like a kind of hacky area right now to me.
1294000	1306000	I can try to usually leverage at least some degree of topology by tokenizing it that respects you know spatial coordinates for images or yeah language order for words.
1306000	1315000	Because like there's also something called positioning coding, which gives you knowledge about the topology of why this element is in this position versus other position.
1315000	1323000	And that tells you a lot about you know the structure of the image or at least where this token is with respect to the overall structure of the image.
1323000	1333000	Yeah, and then another thing that seems missing right now is people usually envision transformers which I'm most familiar with they usually break into non overlapping tokens.
1333000	1347000	But we know from continents and like single processing that you'll get a time of aliasing and like the filtering operations if you have these huge strides you're breaking it non overlapping patches you really would want to have overlapping patches or blur the signal.
1347000	1353000	So all that single processing stuff I think it's been kind of thrown away and probably it's a good idea to put it back in there.
1353000	1355000	Someone has tried overlapping patches.
1355000	1356000	And it does help.
1356000	1364000	And, and by the way everything I say like, as you probably know there's 10,000 transformer papers now so I'm sure everything that you can imagine has been tried.
1364000	1381000	So the part about working with sequences versus sets confused me a little bit and this relates to position and coding so I think it is important when you send these tokens to the transformer to actually inform it where it is in the sequence so
1381000	1387000	transformers operates and sequences or sets like how like I'm,
1387000	1390000	Brian do you mind if I pull the slides over one more time.
1390000	1395000	I do have some slides that I can clarify that.
1395000	1398000	So,
1398000	1400000	yeah.
1400000	1413000	So the positional encoding which I think is the like third big idea of transformers but it's been used in other contexts too so maybe it's not just about transformers but if you have a component and you don't want it to be invariant to shift
1413000	1428000	convolution shift invariant, you can just tell it till the filter where you are by adding this positional code to, I can just say you know at the bottom right of the image and then my mapping will end up being conditioned on the position.
1428000	1433000	So transformers is the same thing I can tell you, you can tell you where the token comes from.
1433000	1442000	And if you do that then you do get sequential, you are modeling the sequence because you're telling the token you know I'm the first item in the sequence or the second item.
1442000	1451000	If you don't do that, then you have the property of permutation, equal variance.
1451000	1458000	That's why I said it's really a set to set operation. So if you don't tell the tokens where they come from, you don't give them positional codes.
1458000	1465000	Then if I take the tokens on the input layer and I permute them into any order, I will simply permute the tokens on the output layer.
1465000	1468000	The mapping will be the same after permutation.
1468000	1475000	It takes a little thinking to see why that's true but essentially the reason is because the attention layer is permutation invariant.
1476000	1483000	The way self attention works is it looks at the color of this and the color of that and it makes a similarity comparison.
1483000	1496000	And then the weight of the weighted sum that goes to here is just going to be something about the similarity between the color, the query and the key of this.
1496000	1503000	So no matter where you move that edge around, as long as the input and output are orange and blue, it will get the same weight.
1503000	1507000	You can kind of work it out and see that attention is permutation invariant.
1507000	1511000	The token whites operator is point wise. That's permutation invariant too.
1511000	1516000	And then that means that the whole transformers are permutation invariant function.
1516000	1521000	And you can make it model sequences by telling it the position of every token in that set.
1521000	1530000	But if you don't have positioning coding, it's more appropriate for set to set mapping.
1530000	1550000	One more question or comment is you started with the idea of tokens, but just a few sentences before you said that something that to me sounds like the idea of tokens is rather weekend that the transformers work better when you have individual bits of data like pixels.
1550000	1573000	So it feels to me that the idea of tokens is not really essential in the whole concept to work, but that tension is the most critical part and the idea that being able to attend dynamically anywhere in the picture if it is visual data is the idea that gives power in these algorithms, not the token percent.
1573000	1577000	Yeah, I don't know. And I think that's kind of the open debate.
1577000	1579000	Yeah, fun to keep discussing.
1579000	1592000	So I think the field is kind of split right now between people thinking that it's the tokens and the vector encapsulated vectors that are important to this versus the attention and really look at you just have an intentional mechanism.
1592000	1596000	It can still be operating over neurons. It doesn't really have to be about tokens.
1596000	1612000	I feel like both probably are important to the success. But one, one counter example to that attention matters is there are these other architectures now sometimes called like MLP mixers, one architecture in this family, which uses tokens and not attention.
1612000	1625000	So it's basically a component over tokens, one by one comms over tokens. They call it MLP.com and that's super confusing. But anyway, we're going to get the terminology because these things are all just like small transformations in each other.
1625000	1641000	Okay, but anyway, this MLP mixer thing doesn't have attention, but it's still in my view is as a token net. And that seems competitive on some tasks with attention networks. So maybe attention is maybe it's not the thing that matters.
1641000	1655000	Related to this question. So it seems like especially in this self attention mode that you're describing. Yeah, if you unrolled that it's like a series of matrices I think going on there that could just be implemented as a straight up feed forwards chain if I'm following.
1656000	1669000	Is it just very deep, you know, it's going to be skips, I think, but can you on that idea makes sense to you. So you can always kind of like, you know, always take any one of these arrows and just say, Oh, that's actually just a matrix multiply.
1669000	1681000	But it's a matrix with special structure. It's not like a full rank matrix. And I think that that's, that's one way of just understanding all the person I didn't mean it was any matrix but just like you could re express this as another as I was trying to understand what the
1681000	1696000	model might look like and then generalize that to and then you're back in standard mode again. Maybe that was closely related to what we were saying. Yeah. Yeah, where you have the second branch with a queue that's different that's sort of active state versus this kind of deterministic processing
1696000	1703000	based on kind of multiplicative there. Okay, that's where like it differs from like standard MLPs where they don't have this kind of multiplicative interaction.
1703000	1711000	That means that, you know, you're dependent on the moment, you're self contesting yourself meaning that you're dependent on your own input when you process this particular input.
1711000	1721000	So it's kind of like a hyper network, I guess in that case, we're like, yeah, you change the representation based on what your presentation already is.
1721000	1738000	Yeah. So it's not like, yeah, you can't just rewrite it exactly as these linear combinations on like a regular feed forward network, because it does do these multiplies maybe that's the one mathematical atomic unit that's different.
1738000	1753000	I think you can express these in different languages and I actually do like thinking of it as oh it's just a special kind of matrix the matrix weights come from this other source is self attention mechanism but then it's just a matrix but that matrix has special structure and understanding that low ring
1753000	1762000	structure is kind of that's just a way of understanding architectures is what is the special structure you're imposing on the on the linear transformations on the matrices.
1762000	1777000	This dot product here between the query and the key involves multiplication, which is not something that you would directly get in a regular network.
1777000	1786000	Oh, yeah, related to this and also the permutation if you variance that's within a block. So what about across transformer blocks.
1786000	1795000	Yeah, so you could have, you can kind of any set of tokens attend to any set of tokens, and they could come from one layer of the net and another layer of the net or one block another block.
1795000	1811000	It could come from a text processing network and image processing network and then the text attends to the tokens of the image. And then it could come from just the text tokens attend to themselves and that's mostly what I talked about but actually sorry did that answer is that your question or on the wrong way.
1811000	1823000	She said that you could commute like within a block like the operations. Yeah, pretty much like I'm just saying if you have like 12 transformer blocks is shown that some of the earlier blocks like learn more like surface form type of features where like later learn.
1823000	1828000	Oh yeah, like high level things and I was wondering about like can you also commute.
1828000	1831000	Yeah, I think no probably.
1831000	1837000	So I don't think you can commute depth wise I think just you can commute within the sequence. Yeah.
1837000	1844000	So if you move the head attention, you know how they can catnate all the heads together to go to the next layer.
1844000	1853000	Don't they lose the permutation variance, or the permutation that we're going to do that nation process.
1853000	1855000	Don't think so.
1855000	1858000	Because your contamination has to have some sort of ordering.
1858000	1861000	Oh, interesting.
1861000	1865000	Yeah, maybe. Okay, so it's possible that transformers aren't.
1865000	1868000	It's not always multi transformers.
1868000	1872000	Individual transformer heads are accurate.
1872000	1875000	Yeah, I hadn't thought about the multi headed thing. Okay.
1875000	1878000	Yeah.
1878000	1893000	In the multi headed you take a weighted sum of the heads and then that the feature vectors they can carry together and that's a parameterized some and if you change the order of what I think you're right. Okay, interesting.
1893000	1903000	Did anybody try instead of using the Q and K matrix or the WQ and WK use a single matrix.
1903000	1922000	Yeah. Okay, so one thing you can do is just get rid of queries keys and values and just have your code vectors at Z and take like the inner product of for the outer product of Z with the Z as your attention and I think that that can sometimes work just as well.
1922000	1934000	Yeah, I haven't really followed the latest on that, but you can have queries keys and values that are linear functions of your code vectors Z, or they can be nonlinear functions or they could be identity.
1934000	1940000	And I'm not sure that there's a consensus on when you need which.
1940000	1952000	So one, one thing that's kind of interesting is that if you use the identity to create your queries and keys, you're basically creating that grant matrix between all of your token vectors with themselves.
1952000	1967000	And it's going to act kind of like it's going to cluster the data and there's been some analysis of how, okay, identity attention, identity queries and keys will create this, you know, clustering like spectral clustering type matrix it'll create this similarity matrix when you hit the data with a
1967000	1977000	matrix and literary matrix or group things are similar. And maybe we can understand a little bit what's going on from that perspective like linear queries and keys are just some projection of that kind of thing.
1977000	1984000	And how does that work if you don't have the WQ and WK matrix but just the identity.
1984000	1987000	Actually, yeah, do you know Brian or just, just anyone know.
1987000	1996000	The keys are actually kind of like this more exactly like this spectral matrix because what you do is you process the input transpose it process it again.
1996000	2002000	You know if you work out both operations in sequence, they end up becoming Z transpose Z like a matrix in the middle.
2002000	2006000	So I think it does work, I guess.
2006000	2009000	That was my, maybe not as well but it does work.
2009000	2016000	It works just maybe not as well. Yeah, we sort of interleaving with these nonlinear operations and still get expressive power.
2016000	2024000	And one other question that you mentioned and there is a paper I think from the mind about using a single pixel as a token.
2024000	2035000	Yeah, it seems very little in terms of being able to establish similarities with other tokens because you have just color and intensity.
2035000	2040000	So my feeling there is that, yes, on the first layer, that's a very bad token.
2040000	2044000	You can have like, you have a one to eight dimensional vector that coats just the color of a single pixel.
2044000	2046000	It's not going to do much.
2046000	2056000	But then when you do this linear combination operation, you know, it's like taking a one by one patch and making a bigger patch out of it.
2056000	2063000	I mean, it can learn to mix information across the whole image and say all of these similar colors.
2063000	2069000	Maybe all the white stripes on the zebra will go all the black stripes in the zebras because the black keys will match the white queries.
2069000	2076000	So it could create these abstracted tokens as you go deeper into the network and build up.
2076000	2089000	Many years ago in computer vision, there was the idea of instead of using a patch of using at a single pixel, the value of the pixel and derivatives.
2090000	2101000	Which means essentially you're having no local information because the derivatives gives you information about neighboring pixel, special derivatives.
2101000	2104000	It's kind of like a little token.
2104000	2109000	It was a vector of and it's very much like a token.
2109000	2110000	Yeah.
2111000	2129000	Yes, I don't think that these are new ideas really is just it helped me to coalesce around the idea of a token like we're gonna think all the operations in terms of tokens but we used to talk about you know, hyper columns and feature vectors and like there's a million names for the same this in concepts.
2130000	2133000	Do you want to pull up just at a time for you to.
2133000	2142000	Yeah, you make you made a connection earlier with graph neural networks. I don't recall which one you said is more general of the two.
2142000	2155000	Well, but they also don't feel the connection because for graph neural networks it's important like the connection in the graph the structure is very important whereas in transformers you said it is less of a case.
2155000	2168000	So, yeah, I think it depends on you to find these things but I like to think of graph nets as more a broader class and so transformers graph nets with the fully connected graph so every token talks to every token.
2168000	2179000	And the aggregation function that decides how to do like the weighted sum of incoming messages is given by this attention operator.
2179000	2187000	The similarity between node A and node B tells you how much the messages from node B will be summed up into node A.
2187000	2200000	And you can have attention graph neural networks that's why you feel it's and that's, I think it was independently invented over there or maybe even first invented over there so graph net people have done attention.
2200000	2211000	Before transformers made attention really popular, although of course attention is an old idea as well and it has shown up a lot of times.
2211000	2213000	So I guess I can start now.
2213000	2227000	Yeah, still gave a great overview which actually is going to make this a lot easier, where I think I want to go over more of the AI machine learning communities perspective of attention and kind of the developments happening in that community, and go over some kind of
2227000	2234000	things about transformers that I think are surprising I guess to a lot of people now.
2234000	2248000	To go over that let's kind of go over how it started so originally the transformer was a language model, and the language model paper had possibly the most arrogant title you could imagine, saying that attention is all you need.
2248000	2264000	So whether they know that that actually turned out to be something that would actually have more truth to it. And I think most people realize including I think myself, which is that this now is a model that covers I still mentioned language speech vision.
2264000	2273000	It's basically a universal model now for all the modalities that people apply to data actually.
2273000	2292000	And attention is not a new idea. This is something that was proposed, even before this paper but this paper was something that was very very similar to the vision transformer attention, where they created essentially attendable feature maps at the last layer or the next to
2292000	2303000	the CG network that was still spatial, and they're able to use this to show that when you do image captioning, it would attend to the correct parts of an object.
2303000	2318000	So, the reason why I think this kind of seminar is important is because, classically, you know, whatever works well has always been something that we always ask ourselves is the brain also doing this because that's been the classical thing for a lot of the past, you know, 10 years of research
2318000	2332000	actually a lot from Jim DeClos group. And I think we have to understand what's going on I think to understand why transformers are so important and potentially why we should either care about them or not care about them actually.
2332000	2345000	But I think there's this all too familiar aspect where whatever is working extremely well, we need to compare that to the brain now. And I think we should kind of get ahead of it this time, because in this case we want to see where they're going.
2345000	2362000	And, again, one of the things that's kind of interesting is that when people compare these attention models specifically certain versions of them like clip to human behavior, you get a substantially improved performance in terms of like explaining
2362000	2379000	whether the misclassification errors are not in human visual systems. And I think the issue is like, we don't know why this is happening in the sense that like, in this paper, they mentioned that the particular vision transformer model that they tested was far better than all their
2379000	2395000	other models for explaining human behavior, but they weren't sure why at the time this was from a 2021 paper so I believe that they're still not sure but they what all they found was that the results show that this particular model clip was an outlier for their comparisons to human behavior.
2395000	2407000	And also there's been, like I said, you know this natural tendency to now compare convolution networks to transformer networks because the transformer networks are starting to outperform covenants, a lot of vision tasks.
2407000	2420000	So the question is now, which models are more similar to neuroscience or just human vision in general whether it be cognitive science as well, our contacts more similar or transformers more similar.
2420000	2433000	And intuitively you would think that there are certain properties of convolutions that make convolution convolution and transformer transformer. And what makes what we believe convolution and convolution is the fact that of
2433000	2448000	equivariance, meaning as Phil mentioned, when you apply an operation to the input, the operation applied to the output should be the same type of operation. Now the issue is that this turns out not to be exactly true it turns out after training a vision
2448000	2457000	transformer is more equivariant to translation than a continent, which I think is quite surprising so Phil might actually already know the issues with the convolution not being equivariant.
2457000	2471000	One thing being that a lot of the pooling and other operations even the non linearity contribute to hurting or harming the covariance performance. So this plot here is showing by this paper's measure, which is a lead derivative, the
2472000	2479000	equivariance error of a component. This is Resnet 50 versus a vision transformer and turns out after training.
2479000	2486000	A vision transformer is translationally more equivariant than a component, which I think is quite not intuitive.
2486000	2495000	And the, in the sense that we built in the equivariance specifically for a component, yet here we are and vision transformers are more for variant after training.
2495000	2510000	And then this on the right shows this is not just specific to these two architectures but many different architectures here. It's also an interesting to be also measure MLP mixers, which as Phil mentioned is kind of a new architecture also that also has surprisingly high
2510000	2519000	equivariance after training. And this is equivariance improves as you increase image net test accuracy. So you have any questions here.
2519000	2523000	Brian, one question. Do you have physical encoding on these networks?
2523000	2524000	Yeah, you do have visual.
2524000	2527000	So it shouldn't be equivariant.
2527000	2529000	Oh, well, this is like a measure of equivariant.
2529000	2531000	No, no, I mean, it shouldn't be like.
2531000	2532000	Oh, it shouldn't be.
2532000	2533000	Yeah, it shouldn't be.
2533000	2534000	We have to learn to be.
2534000	2539000	So equivariance is very strictly by the patch. In this case, they're doing small transformations.
2539000	2542000	So it's not the size of a patch.
2542000	2558000	But since you asked that question, there's another paper that shows about invariance. And this is like much larger translations. And they show that actually after training a train vision transformer, which is a transformer model is as invariant to
2558000	2563000	a translation shifts as a ResNet 18, which is a component.
2563000	2578000	And this is kind of interesting because then the question is what's going on here, like, you know, we build in these things for our models, but apparently not building them in also gives us the thing that we want it.
2578000	2590000	So this kind of goes to the issue that maybe might be controversial, but has been kind of true, which is the idea of scalability. And are people familiar with the bitter lesson here, or has anyone heard of the bitter lesson.
2590000	2601000	This is like a classic controversial statement. I actually think a professor, a former professor here actually gave a retort to this thing by Rich Sutton who wrote that the lesson that we should take away.
2601000	2611000	And I think in 2019 or 2018 is that we should not be working on inductive biases for models as much as we should be working on inductive biases for learning.
2611000	2628000	And I think one of the kind of key directions that organizations like OpenAI are going towards is the idea that we shouldn't really try to bake in these priors that we think are really, really useful, because at some point your data might give you that prior anyway.
2628000	2632000	And that could be what's happening here.
2632000	2642000	And to give you a flavor for what's going on in the machine learning community, they're essentially optimizing the upper right hand corner of this curve.
2642000	2658000	The x-axis here is the compute required to train these models. And the y-axis is the negative log-proplacity, which is essentially the task error, or task performance, I guess, not error.
2658000	2677000	And in this case, what they're doing is they're trying to find architectures that go all the way up here in the performance curve. And they're essentially paying for that performance, not by incorporating inductive biases into their model, but by trying to absorb those inductive biases through data.
2677000	2696000	So I think to give you a glimpse of what's happening in the future is that there probably will be less inductive biases in this community of machine learning, because they're willing to pay this cost of compute to not have to build in the inductive bias that they would normally have to build in for smaller data sets.
2696000	2718000	And as Saul alluded to, which is the, or as actually Tommy also mentioned, it's just these architectures are getting more and more general purpose. So in this case, this is a transformer architecture, but unlike a transformer, this is an architecture that can attend not over the token level, but treats each pixel as a token.
2719000	2743000	And what's interesting about this is that they actually train this on ImageNet without any image prior. So the positioning coding that Phil mentioned was actually learned by the model, and they could actually get performance on the order of, you know, normal image prior oriented models like comp nets and, you know, patch vision transformers.
2743000	2760000	So the remarkable consequence of this is that there's no actual image prior or modality prior built into this model. It learned it on its own. And even in that perspective, it's still competitive on ImageNet. So this is not necessarily a very large data set either.
2760000	2766000	Do you know if the perceiver of Brian also has like a texture bias, for example, or if in general robustness?
2766000	2787000	I don't think we've looked into that. I think the robustness qualities are probably not too different from standard models. I think in terms of adversarial examples, like transformers are, well, I think it's controversial, but I think people say transformers are a little bit more robust adversarial examples than comp nets, but on the grand scheme of things, they're both very susceptible to adversarial examples.
2788000	2801000	I mean, just to finish, I was just curious to see if, you know, I'm sure there's many models now that they can all do whatever 75% on ImageNet, but not all of them will see like a human, whether that's a goal or not, right?
2801000	2810000	Right, that's a good question. And I think like we have to now be aware that this is where the field is going. Is that where we're going as well as a field? Not, yeah.
2810000	2813000	What does it mean to learn position features in the case of an image?
2813000	2818000	Well, you randomly initialize the position encoding to be just from like a random Gaussian or something.
2818000	2823000	The position of the pixels in the image, so it's like an unordered set of pixels.
2823000	2829000	Right, so you treat the pixels as, you treat each pixel as an unordered element in a set.
2829000	2836000	Wait, Brian, is there the case that the top left pixel will always get the same positional encoding? It'll be a learned.
2836000	2841000	Yeah, so, yeah, so it's not like you shuffle every image independently of every other image and then get.
2841000	2844000	So I think it's still inserting a lot of information there.
2844000	2848000	Well, the prior that they're inserting is that the topology is consistent across samples.
2848000	2849000	Yeah.
2849000	2857000	Which I think is reasonable about modalities mostly, like you would imagine that the position shouldn't be unique to every single sample.
2857000	2864000	Because in that case, then I think the problem would be almost like really, really hard.
2864000	2866000	I don't know if you can even learn anything.
2866000	2868000	Like randomly shuffle all the pixels in an image.
2868000	2870000	Yeah, I don't know what you're going to be able to learn.
2870000	2873000	Every image has its own permutation that you give to the models.
2873000	2880000	You could learn like higher like statistics, I guess, over like what fixes together.
2880000	2888000	Another number is how many training example it takes to train this compared to a convolutional network.
2888000	2894000	Right, so I think one of the things that we know is that it takes more data to train a vision transfer.
2894000	2896000	But do you have the number for this?
2896000	2899000	This is, I think, just ImageNet with augmentations, actually.
2899000	2901000	This isn't like a larger data set.
2901000	2905000	This is just augmentation, providing the extra information.
2905000	2910000	Yeah, I don't have the numbers, but the cursor I've seen tend to be, you know, we have a whiteboard.
2910000	2916000	So the connet is it starts here and goes kind of flat and the transformer starts down here and goes up.
2916000	2920000	So it scales better, but for low data, it's very worse.
2920000	2938000	I think that's one of the things that why the community is going towards like industry, especially going towards this direction, because they're willing to pay for the scale via compute costs, rather than having the kind of built in data points, I guess, be not, you know, for free, I guess,
2938000	2955000	What happens is that convolutions after a certain level of scale will start saturating and transformers still continue to go up in terms of increasing the parameter count will give you still positive returns in performance.
2955000	2960000	What's interesting in this talk is also there have been a lot of other variations of transformers as well.
2960000	2969000	And it's kind of strange, actually, people have at least Google has done a meta analysis on transformers and all the zoo of their variations of them.
2969000	2976000	And at least in terms of scalability, it seems like the original transformers actually scales the best, which is a little bit odd.
2976000	2978000	Which one's the best.
2978000	2981000	The vision transformers are also sparse mixture of extra transformers.
2981000	2986000	Those apparently both scale the AT the AT well.
2986000	2994000	Don't remember if they were specifically doing vision. I think it was more language tasks.
2994000	3009000	I think this kind of tells you a story that maybe the machine learning community is going towards, which is not, you know, the fact that architecture matters the most, but the fact that data is actually the really important aspect and I think they're
3009000	3022000	building now architectures that aren't necessarily good at working well without, you know, being trained, but furthermore work well at absorbing data a lot more efficiently than other architectures.
3022000	3038000	So like we should think about the things that leads to the resulting model that we work with these days, which is the idea that it's not just the model itself with its architecture, but there are priors about, you know, the compute the capacity of the model to train
3038000	3056000	that data and also more importantly now more recently. Nice thing is that like as Phil mentioned the notion that a lot of these models don't require supervision so they can absorb larger and larger data sets without much economic cost to the person training it.
3056000	3070000	One also another aspect of Transformers is the way it was created was mainly as an alternative to recurrent networks. And the reason for that is because as you see in the left arrow I mentioned hardware, and one of the things about Transformers is that they're much more
3070000	3086000	on GPUs than a recurrent network is and they run much more better on like, you know, stupidly parallel software and a recurrent network does and that's why they become very popular also is because the parallelization of them is much easier than other sequence models.
3086000	3098000	One comment if I may make how I view this like I feel when you go to Transformers just by introducing the idea of attention dynamically handled by the data you kind of have.
3098000	3111000	It's an overstatement but you know as general architecture as you can have kind of like, you know, you have really powerful architecture and now to train it you need a lot of data and this is really where we are now.
3111000	3127000	And more data does better just because the architecture is extremely general, much more than conventional as you said. So I don't know if there will be any other fancy architecture.
3127000	3139000	One is there are lacking one thing which is vanilla Transformers don't have memory. They don't have feedback connections. And so they're not doing complete in the same way that RNN is.
3139000	3151000	Of course people are adding memory and recurrence to Transformers but the still the majority of them don't have that so that's like actually I think a big limitation they don't have memory.
3152000	3166000	And then two is yeah, you know, eventually look up tables will perform best or you know nearest neighbor will perform best because the limit of infinite data those work and we don't have these work in the limit of infinite data.
3166000	3172000	Yeah, I agree it's not surprising that you need less traction with more data.
3172000	3177000	Brian you were saying like there's choices to be made for if we're interested in models of brain systems.
3177000	3189000	So the question is like, what are we interested in in terms of if these architectures become less biased towards being structurally more relevant to neuroscience, but just being more task relevant to neuroscience.
3189000	3194000	Like, are we going to be stuck at some level of understanding that's only functional.
3194000	3201000	What makes them structurally that's relevant that's sort of why I was asking these kind of wait questions right. Why do you say that.
3201000	3209000	Well, I don't. Yeah, so I think the tricky part is like, I think, because things work well people will find ways to say that they're more structurally relevant.
3209000	3222000	I don't know if Transformers are more structurally relevant than comp nets like obviously there's Fukushima, you know neocognitron which is inspired by neuroscience but attention themselves is never inspired by transformers was never inspired by neuroscience.
3222000	3237000	So I don't know if they're actually more neuroscience friendly, I guess, in the terms of similarity. But I think what the bigger picture is that they're going to be more and more generic.
3237000	3241000	And they're going to take less inspiration from structural biases that we know about.
3241000	3250000	Not necessarily until you know full mentioned the idea of like, you know, recurrence and you know feedback and those things become more.
3250000	3265000	It's like, actually, one thing that's interesting is that like, there hasn't been much. I mean people have proposed these architectures and they do work but people aren't really using feedback versions of Transformers, mainly because there's no recurrent nature to them.
3265000	3270000	So for example, all these things are still fee forward architectures that still process from the bottom up.
3271000	3286000	I think the divergence is going to be like, are we going to be interested in the same models that the AI machine learning committee is going to be interested in? Are we going to be interested in specific models that work on neuroscience but don't necessarily have a functional performance equivalence to what these models have?
3286000	3297000	And one question I have for the neuroscientists in the room is, what is the scale of data that the human brain is trained on when they reach adulthood?
3297000	3308000	And my rough estimate or understanding is it's similar to what the biggest Transformers are currently trained on or a little bit smaller than that, but much more data than comp nets were trained on.
3308000	3321000	So all this stuff about how, you know, in which data regime do you get what kinds of performance? Well, the data regime that seems most relevant to neuroscience, to me, seems more like this Transformer regime, but I don't know if that's true.
3321000	3325000	Like how many images do humans see compared to these models?
3325000	3327000	It's not true for text.
3327000	3329000	Text, I think they're trained on much more data. Yeah.
3329000	3333000	Yeah, but for you, I'm thinking images, everything on the internet.
3333000	3334000	Yeah.
3334000	3351000	There was a paper for a friend to rank this group recently that showed that even if you train a GPT two on a 10 year old amount of text data, it does explain fMRI responses, almost as well as even having a lot more days because fMRI is bad.
3352000	3364000	So I think he's training a Transformer around 10 million tokens, which is like what a child would be exposed to at the age of 10, while most Transformers are trained in like, like, millions of different systems.
3364000	3372000	And like, yeah, it could accept my data, but yeah, fMRI is its own.
3373000	3381000	This is, there's an assumption under this question, which is like, are we actually interested in models of the system in the adult state, or are we interested in models of how the system gets to be in the adult state?
3381000	3384000	Those are not the same question, right?
3384000	3401000	So there may be a shift here between models that are like, and you sort of called it out like, hey, we can, instead of us having to hand design them in, this is the bitter lesson version, we'll just lean on the data with a general flexible thing and let the data push it as long as our compute can handle that and we have enough data.
3401000	3411000	And I think the question you were asking, that's sort of interesting to us, some of us is, does that end state, which of those end states looks more like the adult and the state that's agnostic to whether,
3411000	3417000	neither of them probably followed the same biology path, but just even in that assumption state, what is the state of affairs?
3417000	3425000	I don't think we know what the state of affairs is, visual Transformers relative to combat sun alignments with even visual processing.
3425000	3431000	I mean, somebody was asking here about like somebody who was asking about similarity, maybe that was you.
3431000	3440000	You know, and then because also at the neural level also requires mapping assumptions that, and they get more complicated with the Transformers, right, like,
3440000	3450000	but behaviorally, it sounds like, you know, in the Gary was papers are pointing out like there's some, maybe better alignment, but I don't have their compare against the latest, you know, at trained, you know, comments.
3451000	3460000	Yeah, I don't know about, I think those stories haven't been done to my knowledge of like actual alignment with neural recordings and, of course, I'm sure people here will do that.
3460000	3466000	But alignment in terms of functional capabilities does seem quite a bit better.
3466000	3476000	And it don't relate to me because we're a confidence. What can they do in what's been demonstrated with confidence of 10 years ago, classify 1000 animals, cats and dogs and ancient categories.
3476000	3489000	What can transfer? Well, sure, you can make confidence that grow bigger, but the current generation of the best models are these Transformers like clip, but of course there's a clip non transformers and let's just say clip.
3489000	3498000	And that seems much closer to the functionality of the human visual system that you can recognize millions of categories or way more than 1000s of categories.
3498000	3510000	And you can recognize compositions of categories you can type in a red ball and have you recognize the red ball and just see one example of that and, and these, these networks are getting to that point.
3510000	3516000	So the kind of psychophysical level I think they're getting closer. I don't know at the neural embedding level.
3516000	3530000	So something that I'd like to share is the, there was one paper that me and William McCullough that we submitted to nerves, it actually got rejected, but there was one about a we just submitted isolar about this transformer model that
3530000	3541000	achieved state of the art and brain score for area before, which is kind of interesting because we went to the brainstorm competition at the beginning of this year, like just hoping to participate.
3541000	3553000	And all of a sudden William trained this transformer it was a dual stream transformer with adversarial training and rotations, and we just like broke the record in v4 unexpectedly and wrote a paper about that.
3553000	3567000	In any case, what I think was interesting is that the same model exact same architecture if you trained it another way just classical SGD image net no fans augmentations adversarial perturbations, the score wasn't that great.
3567000	3580000	So I wonder, just in general, should we also just be thinking about transformer model or the interaction of transformer models plus any type of training regime or maybe a fancier loss function that we haven't even conceived.
3580000	3593000	And suppose we do hit like the like explain variants or one correlation in brain score for it like how do we even reverse engineer from that right because the model is just so big.
3593000	3604000	I'm saying playing devil's advocate on my own work really the model so big, like, how do how do we even go back and isn't open a question, you know just I don't have any ideas or
3604000	3614000	I mean I think one of the things that I think we often forget is that a model isn't just its architecture like you know the slide before a model is also data.
3614000	3621000	And once it's interact with data, we have to understand data now to understand what that model is doing can't just understand the architecture.
3621000	3631000	I think as these models become these architectures become more generic data is going to play a larger and larger role and we're kind of back to now trying to understand data now and understanding what the architecture is.
3631000	3635000	And I think that's not that's easier or harder.
3635000	3653000	But also the message is, you know, for the last 10 years until transformers came like three or four years ago, I think the success stories in machine learning was convolutional networks, there was one architect.
3653000	3655000	Now there are several.
3655000	3659000	We have quite a few options.
3659000	3662000	You know they all perform pretty well.
3662000	3664000	That's right.
3664000	3675000	And I think if you just compare functions input output or a while they fit the neurons you'll find they're all doing okay.
3675000	3685000	You need a lot of other constraints, which means what can be implemented by neurons and synapses.
3685000	3691000	And what cannot or very difficult to see how.
3692000	3699000	I agree and I gave this guest lecture and Brian's class.
3699000	3713000	And I was calling it the anachronism conjecture that like as systems get more and more intelligent they kind of converge on the same representations abstractions models and so forth, which other people have you know put forth.
3713000	3721000	And I think it's kind of the same here I don't actually think the difference between transformers and confidence and MLP is that dramatic. I think it's more.
3721000	3728000	As you get more and more data and you optimize more and more toward success at some objective.
3728000	3730000	The models will converge.
3730000	3738000	It's one way but the other way is that, you know what I said that the meeting a few weeks ago.
3738000	3741000	It could be like flight.
3741000	3743000	You know, it could be.
3743000	3749000	You have, you know, a model of a bird.
3749000	3754000	But that's, that's not really good for everything.
3754000	3771000	The important is to understand the principles of aerodynamics, then you can understand how birds fly and how to build airplanes and other things and maybe how fly flies, which is different from birds.
3771000	3784000	Those aerodynamics involved is different. So, I think principles are much more important than the specific implementations which can be quite different.
3784000	3788000	The question is what are the principles here.
3788000	3791000	Yeah, and I think they're similar principles, I think.
3791000	3799000	All of these pictures are like just reweighting at the same few ingredients factorization is in all of them.
3799000	3817000	Hierarchy is in all of them right like, I don't know, and even in transformers and confidence like transformers can be written as 90% convolution and just a few little layers that are attention like if you look at the actual operations almost every operation is is a convolution in the sense of being a one by one,
3817000	3823000	you can just chop up the signal into patches and process each one independently and identically.
3823000	3827000	Yeah, so I think the principles are going to turn out to be very similar.
3827000	3837000	The question is, which principles should we care about now, given this kind of heterogeneity and architecture, but similarity and functional performance.
3837000	3849000	One of the things that it becomes easier if the community like has something they cannot do, whether it be like, you know, to fly, I guess, for example, flight, which is like, how do we achieve something that we can't currently do right now.
3849000	3858000	I think the general spirit of the machine committee is that all we're done, we can just keep making these models bigger and keep doing this and we'll be fine.
3858000	3868000	I don't think that's true. But right now it seems like the spirit is in that direction and that's why we're kind of revalidating. Oh, of course this must be like the brain. Of course this is like what we care about.
3868000	3872000	Of course, all these kind of back explanations are working.
3872000	3883000	But once we hit a wall, I feel like then we know what's wrong and what's correct. Otherwise, I guess it becomes kind of hard to tell right now.
3884000	3903000	The topic of like data efficiency. This might have an obvious answer, but like, I was wondering if when you have multimodal data, whether like learning becomes a lot more efficient if you have like experts of information from visual stuff,
3904000	3919000	like texts, that like maybe our captions associated with the image or something versus like two aspects of just text data.
3920000	3933000	I don't know how well those things have been estimated, but the language vision models are a lot better on certain benchmarks than the vision only models and it does seem like language must be like incredibly valuable per word.
3933000	3937000	There's a lot more information than per pixel.
3937000	3943000	I feel like a lot of the recent successes, just leveraging language, at least in computer vision.
3943000	3961000	Same with robotics a few other areas. I think what made clip a lot more, at least from that psychophysics experiment from Garrus at all, a lot more like powerful for their results was the fact that clip was trained on classification, but on caption similarity matching
3961000	3967000	to a text caption which has a lot more information than just a single label for this entire image.
3967000	3981000	Like caption can tell you, you know, things about geometry tells you what things on the left, what things included, what thing is, what thing what what season it is or what time it is like it tells you a lot more information and a single word would be to
3982000	3984000	ImageNet class.
3984000	3998000	Another, okay, this is a little anecdotal, but what I've heard is that for training diffusion models, if you train them without language, just unconditional diffusion model generative model of imagery is really expensive and we all thought like, okay, we're not getting that game is not for us is for Google.
3998000	4010000	But if you train them text conditional, they're actually much according to the students I've talked to, they train much faster because the text conditional models the text gives you so much leverage.
4010000	4018000	And so they said that no, no, we can train text conditional like no dolly type models stable diffusion those things are within the budget of MIT.
4018000	4026000	You do that to trade condition because you have to have text image pairs see if a lot. This is a huge source of supervision here, as opposed to just random images.
4026000	4035000	And if you have that, then you're in like the hundreds of thousands of dollars of range to train one of those big models, as opposed to the tens of millions of dollars range.
4035000	4038000	This is anecdotal.
4038000	4041000	They might be just trying to get some GPUs. I'm not sure.
4041000	4054000	By the way, address a question to both of you, but an important feature of transformers compared to previous networks is the fact that you don't have to worry about labeling, especially when you use text, right?
4054000	4056000	Yeah.
4056000	4065000	I think that's the issue about the supervision of supervision, which is like I never found a consistent definition of what a supervised task is versus not supervised one besides economic cost.
4065000	4069000	Like how much did you spend to acquire this data seems to be the only consistent label.
4069000	4073000	But if you speak about neuroscience is not only cost, right?
4073000	4079000	Right, but then like I think, yeah, this is another divergence in between two communities, right, which is like.
4079000	4090000	But the real problem is the brain. Come on. Everything else is
4090000	4096000	More, more questions. I'm happy to keep chatting much for one more spester.
4096000	4105000	I think we want to quite some time, we can adjourn and to the next iteration sometime in the next few weeks.
4105000	4107000	Okay. Thank you.
