If we look at history, models of the brain have always been the most fashionable technology.
It was hydrodynamics in the 17th century and digital computers a few decades ago.
And ten years ago was convolutional neural networks.
And now, transformers of course.
So in preparation for this, we have an introduction to transformers.
I think this is something that Michael Fee, chairman of this department here and Ghibli Carlo asked for.
Of course they are not here, but Michael actually asked me to record it.
Is that possible?
It's recording now as long as that's okay with you guys.
Everybody agrees to be recorded. Okay, all right.
So with this we have an introduction to transformers and this probably will be the first of some of the research meeting where we'll speak about transformers.
Next time perhaps more in comparison in the framework of models of the brain.
But this time it's more on just technical aspects of transformers.
And so Philip Tinsola, C-Sale and Brian Chung, this building, will tell us how Jim is here after all.
Okay, so Lupus in Fabulous.
What was that?
Lupus in Fabulous exactly means what happened, that I will speak about you and you came in.
So Philip will start.
And then we have Brian and then perhaps discussion and in the meantime, of course, very welcome to ask a lot of questions.
All right.
Okay, great. Thank you, Tommy.
So this can be, as far as I'm concerned, very informal and happy to go back and forth.
I took slides from a lecture, like hour and a half electron transformers I've given, and tried to pick a few slides. So it's going to be a quick intro, but we can go into as much detail as you want, and happy to make it also just more discussion.
But I'll start by talking about, you know, what are transformers, you know, for those of you who haven't encountered them yet.
Okay, so I think that there are two key ideas that are not new but are relatively popularized by transformers. And the first is called the idea of tokens.
And then the second is going to be the idea of attention.
There's a few other, you know, bits and pieces and transformers, but to me, those are the two kind of key ideas.
Okay, so tokens are essentially a new data structure.
And they're a replacement for neurons. Okay, so in artificial neural nets, neurons are just real numbers, just scalar numbers, and tokens are vectors. So that's, that is the distinction that, as far as I'm concerned.
So it's just lingo for a vector of neurons, a vector of scalars.
But when you start thinking of your primitive units in a network as tokens, whether they're neurons, then sometimes the math will look a little bit different.
And I think it's just a powerful way of working with new kinds of neural networks made out of tokens instead of neurons.
Okay, again, it's a patch in an image.
It's a patch in an image can be a token. I'll give a few examples and not not any ideas vector neurons vectors of, you know, features for a unit inside a network that's an old idea, you know shows up in graph neural networks if you've seen those two.
Okay, but the way I like to think of it is we used to work with arrays of neurons and now in transformers we work with arrays, tensors of various shape of tokens, but they're kind of like encapsulated, you know, vectors of information.
Okay, so you can tokenize just about anything and that is like the big trend right now is just show how to turn whatever data you have into a sequence of tokens.
So here's an example, like Tommy's mentioning of tokenizing an image.
You could do this in a million different ways but the way that is very common is you start with an image you break it into patches, you flatten all the patches to create just take all the rows of that.
Just pixels and just concatenate them into a really long row or really long colon vector.
And then you get this thing called a token which is just an n dimensional vector representing that patch.
And this arrow up here, if you can see my mouse, it could just be flattened or concatenate, but commonly it will actually be a linear projection to some, you know, fixed dimensionality.
So project to 128 dimensional vector for example.
Should I think of a token as an embedding? Is it the same or what?
Same thing. Yeah, so vector embedding of a patch would be a token in this context. Yeah.
So there's a lot of names for this but to me like yeah once you start thinking of things in terms of tokens like that's that's how I like to think of it.
Okay, so you can tokenize anything.
That's often a linear projection.
We used to operate over neurons now we operate over tokens.
And just a few more examples so whenever you can take your signal and chop it up into chunks and project each chunk to a fixed dimensionality vector.
Then you can tokenize that data so for language, people do this as well they chop up into little chunks which are often, you know, two characters or a few characters at a time.
And they project those into 120 dimensional vectors with sound you chop up into little tiny snippets of sound.
So you can tokenize anything and once you've tokenized it then it's just a sequence like representation a sequence of vectors.
Although really the way transformers work is they think of it as they treat it as a set not a sequence but people often talk of it as a sequence.
Okay.
So that's tokenization that's, I think that's the first critical idea and then everything ends up being the operators to find over tokens as opposed to over neurons.
So rather than taking linear combinations of neurons which is the common, you know, linear layer in a network, we take linear combinations of tokens where you just take a weighted combination of vectors instead of weighted combination of scalars.
Okay.
So I'm not in this, because it's only like a little intro going to go into the math and detail, but we can always come back if you're interested.
Okay, so in standard artificial neural networks, there's two key layers, there's the linear combination layer, the linear layer, and there is the point wise nonlinearity, which might be a ReLU or a sigmoid function.
So in tokens, there's the same thing, there's a linear layer, which we have here, just a linear combination of vectors, and then there's a token wise nonlinearity which is completely analogous to a neuron wise nonlinearity.
So this is a neural net that does a point wise nonlinearity over neurons that applies the same function ReLU to every neuron in a list, and the token net applies the same function F to every token in a list for an asset.
Okay, usually F is going to be itself a multilayer perceptron, it will be some parameterized function.
So equivalently think of this as a convolution over the tokens.
So, transformers, CNNs, it's all the same ideas kind of rehash, we can talk about that if people are interested.
But here's what it looks like, it's just a token wise operation that slides along the list of tokens. Okay, so you can see that looks like convolution sliding across the signal.
Okay, it's usually one layer, right?
It's usually one layer, I think that is usually linear ReLU to linear. Yeah, so it's a nonlinear function.
If it, yeah, it just has to be nonlinear so you get nonlinearities into the system.
I have a question.
MLP.
One layer of neither.
Well, two linear layer MLP, but sure. Yeah, you could call it something else.
I have a good question for the last slide, like, I think I have missed what is the Z here, like, in the last slide.
Oh, Z is the token code vector, meaning the vector of neurons inside that token.
Okay.
It has this code vector that lives inside it. Yeah.
Okay, thank you.
You're welcome.
We'll see you now with convolution here.
Conquering applies the same operator independently and identically to every item in a sequence, and this is doing the same thing. So it's like, you know, slide the filter across here to slide this nonlinear filter across the sequence.
But we give it as a one by one kernel because it doesn't the like the receptive field in the sequence dimension is just looking at one token at a time.
Okay, so here's a neural net. And here is what I'll call a token net. So it's just like a neural net alternating linear combination point wise nonlinearity but now it's linear combination of tokens and token wise nonlinearity.
And transformers are in that family.
Another name for that family is graph neural networks they have the exact same structure.
Okay, but the terminology is just different.
But they're the same thing token nets graph nets transformers are all the same thing.
You could say that token sorry transformers are special kind of graph net if you're looking at that way.
So so many connections that could be made, but I'm going to zoom right ahead and we can discuss all the connections.
So that idea number one is build things out of tokens vector valued units as opposed to out of neurons scalar value units.
Number two to me is attention. Maybe this is the most famous idea of transformers is they have this new layer called attention.
So let's look at what attention is.
Okay, so in a neural net or what I'm calling a token that you'll have linear combinations of inputs to produce an output or weight of some of the inputs to produce an output, and you will parameterize that mapping with weights w.
And in attention, you have a weighted combination of inputs to produce an output, but but the parameters or the values in the matrix a are not learnable.
Instead, they're going to be data dependent. They're going to be a function of the some other data.
So when you have data dependent linear combination where the weights are a function of some other data, then that's called attention.
Okay.
And so a is the weight matrix, and it's just a function of something else. It tells you what to attend to how much weight to apply to each token input sequence.
And you'll take a, you know, weighted sum, which is just matrix multiply a by the tokens.
Okay, notation, don't worry about it. So here's here's the intuition. So I can have attention given to me by some other branch of my neural network, maybe it's going to be a branch that passes a question.
That question will tell me what patches in the input I tend to the input patches are represented by tokens.
And I will say these are the patches that I'm going to place high weight on and then I'll take a weighted sum of the code vectors inside those patches pretty soon output.
So I could ask, you know, question, what is the color of the birds head I'll attend to the birds head. What is the color of the vegetation I'll attend to the background.
I'm just saying which tokens tokens or patches here are going to be getting a lot of weight to make my decision.
Okay, and then I'll report the best green because the token code vectors will have represented some information like the color of the token.
Okay.
There's a little too much detail to fully understand in just a few minutes but here's the most common kind of attention layer just to show you the mechanics really quickly.
The question submits a query that is matched against a key in the data that you're querying so the data you're querying is a set of tokens and each token has a key vector which gets matched against the query vector of the question.
The dot product to get the similarity between the key and the query and that dot product that similarity becomes the way you apply to another transformation of your tokens which is called the value vector and you take a weighted combination of the value vector.
And so, although all the fancy math here is just to say, the question will tell me which tokens to weigh heavily in a weighted sum to produce an output, but it'll be via these three transformations the token vectors, which are the key the value in the query transformations.
Okay.
So that's a little mechanistic detail we can go back to that if we want to discuss the nitty-gritty. The way it kind of looks like is this and this is common to most of the transformer architectures.
You have a bunch of you break into patches then you have a bunch of layers which are basically these one by one convolutional layers.
And then every now and then you have something that tries to mix information across space across tokens and that's called attention and that is just the tokens based on their values queries and keys will decide what should I you know which are the other tokens
should I average together to produce a new representation in the output. So this head might attend. So you know I should look at the other heads to decide how I can better recognize what's going on in this patch.
When you say one by one evolution you mean a patch is a pixel.
A patch is so you vectorize a patch and then you do like one by one convolution across all of those tokens where the channel dimension is the token vector.
OK, so here's MLPs and then transformers are to two changes. One is that rather than scalars they have tokens and other changes rather than having parameterized linear weights.
They have data dependent linear weights that are given by that special attention operator and that attention operator itself has parameters that define the key the query and the value and those are the learnable parameters of the system.
Philip, just a small detail, but you start with self attention and then you have a one layer.
Yeah, but in the previous slide you had the opposite order is that.
Well, so it will vary a lot between different architectures like you could alternate these different orders.
And I did gloss over the detail of this is attention from an external question.
But the more common thing is self attention where attention is coming from the image itself from the data itself.
And that's.
Okay, this is like attention from the question branch, but I could just have the data choose what in its own token sequence to attend to each token chooses which other tokens to attend to and that's called self attention.
That's like this picture here where this patch will decide what to attend to to get a better representation of that patch essentially.
Okay, so this is self attention really as opposed to you. That's why the dependency of the weights on the data is like this is attending to itself as opposed to attending f is not coming from some external source.
Okay, so I think this is the last one just to connect it to one of the most common common demonstrations of transformers is to do sequence modeling.
But really transformers are more about set to set operations, but you can you can represent a sequence as a set. So it's fine.
But, oh, oh, no, I think I got the wrong slide here.
Let me pull up the right one.
Yeah, this is what I wanted to show. Okay, so, so here's how it looks for doing next word prediction, and you can do next, you know,
prediction and sequence of proteins or the next sound wave prediction, you next meeting prediction. That's really common framework. So we, this is like a one layer transformer.
You know, we're going to say colorless green idea sleep, put that into the transformer we want to predict the blank, the words attend to each other, then you pass to this token wise nonlinearity, and then you make a prediction at the very end.
So just one example of what you have seen as transformers for sequences, but really transformers are more general and they're not just about sequence modeling. This is just one way you could use them.
Okay, so that's the 10 minute overview of transformers.
Let's see, should we do questions or Brian, do you want to just jump right in. Maybe you do questions.
Okay, questions will Brian set up.
What is the history of these terms, you know, quest, query value.
So he's probably other people here know better than I do but I think it's all coming from the data retrieval database literature so you have a database where you have knowledge stored in kind of cells.
And you can query that database you say I want to find things related to drafts.
And then every cell will have like a label the key will be like this is a cell for mammals and this is a cell for drafts and here's a cell for plants, and you'll match the query to the key, and then the stuff inside the cell that you retrieve will be the value.
So I think it's coming from that.
This isn't like a fully formed question but I guess like in the framework of having question key value.
If, let's say the question was like what is the color of the turkey's head and for some reason, the turkey's head is like a feature that's complex enough that it won't, it can't be like well represented within a single question.
So is that like cause issues with like attending to the right thing or like waiting the right thing properly when
Yeah.
I suppose it could one one answer would be that you often do multi headed attention where you'll have, you'll take your sequence or your set of tokens and then you'll have K different query vectors and K different value vectors and K different
value vectors and each query can be asking a different type of you can say I want to match to the same color. Another one can say I want to match to the same geometry.
And, you know, when you optimize the parameters of this network it will somehow, you know, self organize that, well, it's useful to factor things into geometry and color then there'll be one attention head that cares about color and one that cares about geometry.
I'm just one quick thing on the transformer architecture and the non linearity.
If I recall correctly you do a normalization of each token right intuition for that why do you do that.
I haven't seen that too much before transformers.
Yeah, that's a great question so so normally you want your weighted sum to add up to one. So the weight should add up to one.
People do that. I treat that via the softmax over the weights. Yeah, is that we're referring to. Yeah, no I'm not referring to post the residual connection you do as part of the right before that.
Other layer norms. So, okay my understanding of transformers has progressed to the level of tokens and attention and all the rest layer norm and residual connections.
At this point I don't rock it I don't I don't know why that feels kind of like tricks that usually help neural networks train it's good to normalize things is good to have principles.
I don't see anything. I haven't noticed that anything specific to transformers about those ideas yet. They're generally useful tricks. Okay.
Transformers are very hard to train especially like several later ones like 12 and up if you don't have like residual connections or layer norm.
Okay, the training isn't very stable.
So it's not it's not. Yeah, also this attention between multiplication.
And that might not be as stable as normal non intentional layers just do addition.
That's the softmax right the softmax can probably help with that. Yeah.
So how do you decide what patches to use for instance the image or the text you also had like what seemed to be arbitrary.
Yeah, segmentation of the data.
Yeah, that's a great question. How like how do you do the tokenization like how do you design that I think it's super hacky right now so that feels like somewhere that people could do a lot of work.
One thing that does seem to happen is that the smaller you make your tokens if you have tokens that are a single pixel it's just the better you get.
Maybe the
what will happen is we'll just stop having clever tokenizations and just go down to whatever the atomic units of the data are like a single character a single pixel and that that just makes a choice for us because you can't go below that really.
I think the smaller envision transformers the thing I've seen is that the smaller the tokens are the better they tend to perform.
But because the attention mechanism is every token it tends to every token it's like n squared if you make the tokens too small, there's too many tokens and then you run out of memory.
So there's probably clever tokenization schemes like super pixels or segmentation or in language spaders, you know, a lot of tokenization schemes that are like bite code by parent coding is the name of one and work that that could be used that as your first tokenization layer.
But yeah that feels like a kind of hacky area right now to me.
I can try to usually leverage at least some degree of topology by tokenizing it that respects you know spatial coordinates for images or yeah language order for words.
Because like there's also something called positioning coding, which gives you knowledge about the topology of why this element is in this position versus other position.
And that tells you a lot about you know the structure of the image or at least where this token is with respect to the overall structure of the image.
Yeah, and then another thing that seems missing right now is people usually envision transformers which I'm most familiar with they usually break into non overlapping tokens.
But we know from continents and like single processing that you'll get a time of aliasing and like the filtering operations if you have these huge strides you're breaking it non overlapping patches you really would want to have overlapping patches or blur the signal.
So all that single processing stuff I think it's been kind of thrown away and probably it's a good idea to put it back in there.
Someone has tried overlapping patches.
And it does help.
And, and by the way everything I say like, as you probably know there's 10,000 transformer papers now so I'm sure everything that you can imagine has been tried.
So the part about working with sequences versus sets confused me a little bit and this relates to position and coding so I think it is important when you send these tokens to the transformer to actually inform it where it is in the sequence so
transformers operates and sequences or sets like how like I'm,
Brian do you mind if I pull the slides over one more time.
I do have some slides that I can clarify that.
So,
yeah.
So the positional encoding which I think is the like third big idea of transformers but it's been used in other contexts too so maybe it's not just about transformers but if you have a component and you don't want it to be invariant to shift
convolution shift invariant, you can just tell it till the filter where you are by adding this positional code to, I can just say you know at the bottom right of the image and then my mapping will end up being conditioned on the position.
So transformers is the same thing I can tell you, you can tell you where the token comes from.
And if you do that then you do get sequential, you are modeling the sequence because you're telling the token you know I'm the first item in the sequence or the second item.
If you don't do that, then you have the property of permutation, equal variance.
That's why I said it's really a set to set operation. So if you don't tell the tokens where they come from, you don't give them positional codes.
Then if I take the tokens on the input layer and I permute them into any order, I will simply permute the tokens on the output layer.
The mapping will be the same after permutation.
It takes a little thinking to see why that's true but essentially the reason is because the attention layer is permutation invariant.
The way self attention works is it looks at the color of this and the color of that and it makes a similarity comparison.
And then the weight of the weighted sum that goes to here is just going to be something about the similarity between the color, the query and the key of this.
So no matter where you move that edge around, as long as the input and output are orange and blue, it will get the same weight.
You can kind of work it out and see that attention is permutation invariant.
The token whites operator is point wise. That's permutation invariant too.
And then that means that the whole transformers are permutation invariant function.
And you can make it model sequences by telling it the position of every token in that set.
But if you don't have positioning coding, it's more appropriate for set to set mapping.
One more question or comment is you started with the idea of tokens, but just a few sentences before you said that something that to me sounds like the idea of tokens is rather weekend that the transformers work better when you have individual bits of data like pixels.
So it feels to me that the idea of tokens is not really essential in the whole concept to work, but that tension is the most critical part and the idea that being able to attend dynamically anywhere in the picture if it is visual data is the idea that gives power in these algorithms, not the token percent.
Yeah, I don't know. And I think that's kind of the open debate.
Yeah, fun to keep discussing.
So I think the field is kind of split right now between people thinking that it's the tokens and the vector encapsulated vectors that are important to this versus the attention and really look at you just have an intentional mechanism.
It can still be operating over neurons. It doesn't really have to be about tokens.
I feel like both probably are important to the success. But one, one counter example to that attention matters is there are these other architectures now sometimes called like MLP mixers, one architecture in this family, which uses tokens and not attention.
So it's basically a component over tokens, one by one comms over tokens. They call it MLP.com and that's super confusing. But anyway, we're going to get the terminology because these things are all just like small transformations in each other.
Okay, but anyway, this MLP mixer thing doesn't have attention, but it's still in my view is as a token net. And that seems competitive on some tasks with attention networks. So maybe attention is maybe it's not the thing that matters.
Related to this question. So it seems like especially in this self attention mode that you're describing. Yeah, if you unrolled that it's like a series of matrices I think going on there that could just be implemented as a straight up feed forwards chain if I'm following.
Is it just very deep, you know, it's going to be skips, I think, but can you on that idea makes sense to you. So you can always kind of like, you know, always take any one of these arrows and just say, Oh, that's actually just a matrix multiply.
But it's a matrix with special structure. It's not like a full rank matrix. And I think that that's, that's one way of just understanding all the person I didn't mean it was any matrix but just like you could re express this as another as I was trying to understand what the
model might look like and then generalize that to and then you're back in standard mode again. Maybe that was closely related to what we were saying. Yeah. Yeah, where you have the second branch with a queue that's different that's sort of active state versus this kind of deterministic processing
based on kind of multiplicative there. Okay, that's where like it differs from like standard MLPs where they don't have this kind of multiplicative interaction.
That means that, you know, you're dependent on the moment, you're self contesting yourself meaning that you're dependent on your own input when you process this particular input.
So it's kind of like a hyper network, I guess in that case, we're like, yeah, you change the representation based on what your presentation already is.
Yeah. So it's not like, yeah, you can't just rewrite it exactly as these linear combinations on like a regular feed forward network, because it does do these multiplies maybe that's the one mathematical atomic unit that's different.
I think you can express these in different languages and I actually do like thinking of it as oh it's just a special kind of matrix the matrix weights come from this other source is self attention mechanism but then it's just a matrix but that matrix has special structure and understanding that low ring
structure is kind of that's just a way of understanding architectures is what is the special structure you're imposing on the on the linear transformations on the matrices.
This dot product here between the query and the key involves multiplication, which is not something that you would directly get in a regular network.
Oh, yeah, related to this and also the permutation if you variance that's within a block. So what about across transformer blocks.
Yeah, so you could have, you can kind of any set of tokens attend to any set of tokens, and they could come from one layer of the net and another layer of the net or one block another block.
It could come from a text processing network and image processing network and then the text attends to the tokens of the image. And then it could come from just the text tokens attend to themselves and that's mostly what I talked about but actually sorry did that answer is that your question or on the wrong way.
She said that you could commute like within a block like the operations. Yeah, pretty much like I'm just saying if you have like 12 transformer blocks is shown that some of the earlier blocks like learn more like surface form type of features where like later learn.
Oh yeah, like high level things and I was wondering about like can you also commute.
Yeah, I think no probably.
So I don't think you can commute depth wise I think just you can commute within the sequence. Yeah.
So if you move the head attention, you know how they can catnate all the heads together to go to the next layer.
Don't they lose the permutation variance, or the permutation that we're going to do that nation process.
Don't think so.
Because your contamination has to have some sort of ordering.
Oh, interesting.
Yeah, maybe. Okay, so it's possible that transformers aren't.
It's not always multi transformers.
Individual transformer heads are accurate.
Yeah, I hadn't thought about the multi headed thing. Okay.
Yeah.
In the multi headed you take a weighted sum of the heads and then that the feature vectors they can carry together and that's a parameterized some and if you change the order of what I think you're right. Okay, interesting.
Did anybody try instead of using the Q and K matrix or the WQ and WK use a single matrix.
Yeah. Okay, so one thing you can do is just get rid of queries keys and values and just have your code vectors at Z and take like the inner product of for the outer product of Z with the Z as your attention and I think that that can sometimes work just as well.
Yeah, I haven't really followed the latest on that, but you can have queries keys and values that are linear functions of your code vectors Z, or they can be nonlinear functions or they could be identity.
And I'm not sure that there's a consensus on when you need which.
So one, one thing that's kind of interesting is that if you use the identity to create your queries and keys, you're basically creating that grant matrix between all of your token vectors with themselves.
And it's going to act kind of like it's going to cluster the data and there's been some analysis of how, okay, identity attention, identity queries and keys will create this, you know, clustering like spectral clustering type matrix it'll create this similarity matrix when you hit the data with a
matrix and literary matrix or group things are similar. And maybe we can understand a little bit what's going on from that perspective like linear queries and keys are just some projection of that kind of thing.
And how does that work if you don't have the WQ and WK matrix but just the identity.
Actually, yeah, do you know Brian or just, just anyone know.
The keys are actually kind of like this more exactly like this spectral matrix because what you do is you process the input transpose it process it again.
You know if you work out both operations in sequence, they end up becoming Z transpose Z like a matrix in the middle.
So I think it does work, I guess.
That was my, maybe not as well but it does work.
It works just maybe not as well. Yeah, we sort of interleaving with these nonlinear operations and still get expressive power.
And one other question that you mentioned and there is a paper I think from the mind about using a single pixel as a token.
Yeah, it seems very little in terms of being able to establish similarities with other tokens because you have just color and intensity.
So my feeling there is that, yes, on the first layer, that's a very bad token.
You can have like, you have a one to eight dimensional vector that coats just the color of a single pixel.
It's not going to do much.
But then when you do this linear combination operation, you know, it's like taking a one by one patch and making a bigger patch out of it.
I mean, it can learn to mix information across the whole image and say all of these similar colors.
Maybe all the white stripes on the zebra will go all the black stripes in the zebras because the black keys will match the white queries.
So it could create these abstracted tokens as you go deeper into the network and build up.
Many years ago in computer vision, there was the idea of instead of using a patch of using at a single pixel, the value of the pixel and derivatives.
Which means essentially you're having no local information because the derivatives gives you information about neighboring pixel, special derivatives.
It's kind of like a little token.
It was a vector of and it's very much like a token.
Yeah.
Yes, I don't think that these are new ideas really is just it helped me to coalesce around the idea of a token like we're gonna think all the operations in terms of tokens but we used to talk about you know, hyper columns and feature vectors and like there's a million names for the same this in concepts.
Do you want to pull up just at a time for you to.
Yeah, you make you made a connection earlier with graph neural networks. I don't recall which one you said is more general of the two.
Well, but they also don't feel the connection because for graph neural networks it's important like the connection in the graph the structure is very important whereas in transformers you said it is less of a case.
So, yeah, I think it depends on you to find these things but I like to think of graph nets as more a broader class and so transformers graph nets with the fully connected graph so every token talks to every token.
And the aggregation function that decides how to do like the weighted sum of incoming messages is given by this attention operator.
The similarity between node A and node B tells you how much the messages from node B will be summed up into node A.
And you can have attention graph neural networks that's why you feel it's and that's, I think it was independently invented over there or maybe even first invented over there so graph net people have done attention.
Before transformers made attention really popular, although of course attention is an old idea as well and it has shown up a lot of times.
So I guess I can start now.
Yeah, still gave a great overview which actually is going to make this a lot easier, where I think I want to go over more of the AI machine learning communities perspective of attention and kind of the developments happening in that community, and go over some kind of
things about transformers that I think are surprising I guess to a lot of people now.
To go over that let's kind of go over how it started so originally the transformer was a language model, and the language model paper had possibly the most arrogant title you could imagine, saying that attention is all you need.
So whether they know that that actually turned out to be something that would actually have more truth to it. And I think most people realize including I think myself, which is that this now is a model that covers I still mentioned language speech vision.
It's basically a universal model now for all the modalities that people apply to data actually.
And attention is not a new idea. This is something that was proposed, even before this paper but this paper was something that was very very similar to the vision transformer attention, where they created essentially attendable feature maps at the last layer or the next to
the CG network that was still spatial, and they're able to use this to show that when you do image captioning, it would attend to the correct parts of an object.
So, the reason why I think this kind of seminar is important is because, classically, you know, whatever works well has always been something that we always ask ourselves is the brain also doing this because that's been the classical thing for a lot of the past, you know, 10 years of research
actually a lot from Jim DeClos group. And I think we have to understand what's going on I think to understand why transformers are so important and potentially why we should either care about them or not care about them actually.
But I think there's this all too familiar aspect where whatever is working extremely well, we need to compare that to the brain now. And I think we should kind of get ahead of it this time, because in this case we want to see where they're going.
And, again, one of the things that's kind of interesting is that when people compare these attention models specifically certain versions of them like clip to human behavior, you get a substantially improved performance in terms of like explaining
whether the misclassification errors are not in human visual systems. And I think the issue is like, we don't know why this is happening in the sense that like, in this paper, they mentioned that the particular vision transformer model that they tested was far better than all their
other models for explaining human behavior, but they weren't sure why at the time this was from a 2021 paper so I believe that they're still not sure but they what all they found was that the results show that this particular model clip was an outlier for their comparisons to human behavior.
And also there's been, like I said, you know this natural tendency to now compare convolution networks to transformer networks because the transformer networks are starting to outperform covenants, a lot of vision tasks.
So the question is now, which models are more similar to neuroscience or just human vision in general whether it be cognitive science as well, our contacts more similar or transformers more similar.
And intuitively you would think that there are certain properties of convolutions that make convolution convolution and transformer transformer. And what makes what we believe convolution and convolution is the fact that of
equivariance, meaning as Phil mentioned, when you apply an operation to the input, the operation applied to the output should be the same type of operation. Now the issue is that this turns out not to be exactly true it turns out after training a vision
transformer is more equivariant to translation than a continent, which I think is quite surprising so Phil might actually already know the issues with the convolution not being equivariant.
One thing being that a lot of the pooling and other operations even the non linearity contribute to hurting or harming the covariance performance. So this plot here is showing by this paper's measure, which is a lead derivative, the
equivariance error of a component. This is Resnet 50 versus a vision transformer and turns out after training.
A vision transformer is translationally more equivariant than a component, which I think is quite not intuitive.
And the, in the sense that we built in the equivariance specifically for a component, yet here we are and vision transformers are more for variant after training.
And then this on the right shows this is not just specific to these two architectures but many different architectures here. It's also an interesting to be also measure MLP mixers, which as Phil mentioned is kind of a new architecture also that also has surprisingly high
equivariance after training. And this is equivariance improves as you increase image net test accuracy. So you have any questions here.
Brian, one question. Do you have physical encoding on these networks?
Yeah, you do have visual.
So it shouldn't be equivariant.
Oh, well, this is like a measure of equivariant.
No, no, I mean, it shouldn't be like.
Oh, it shouldn't be.
Yeah, it shouldn't be.
We have to learn to be.
So equivariance is very strictly by the patch. In this case, they're doing small transformations.
So it's not the size of a patch.
But since you asked that question, there's another paper that shows about invariance. And this is like much larger translations. And they show that actually after training a train vision transformer, which is a transformer model is as invariant to
a translation shifts as a ResNet 18, which is a component.
And this is kind of interesting because then the question is what's going on here, like, you know, we build in these things for our models, but apparently not building them in also gives us the thing that we want it.
So this kind of goes to the issue that maybe might be controversial, but has been kind of true, which is the idea of scalability. And are people familiar with the bitter lesson here, or has anyone heard of the bitter lesson.
This is like a classic controversial statement. I actually think a professor, a former professor here actually gave a retort to this thing by Rich Sutton who wrote that the lesson that we should take away.
And I think in 2019 or 2018 is that we should not be working on inductive biases for models as much as we should be working on inductive biases for learning.
And I think one of the kind of key directions that organizations like OpenAI are going towards is the idea that we shouldn't really try to bake in these priors that we think are really, really useful, because at some point your data might give you that prior anyway.
And that could be what's happening here.
And to give you a flavor for what's going on in the machine learning community, they're essentially optimizing the upper right hand corner of this curve.
The x-axis here is the compute required to train these models. And the y-axis is the negative log-proplacity, which is essentially the task error, or task performance, I guess, not error.
And in this case, what they're doing is they're trying to find architectures that go all the way up here in the performance curve. And they're essentially paying for that performance, not by incorporating inductive biases into their model, but by trying to absorb those inductive biases through data.
So I think to give you a glimpse of what's happening in the future is that there probably will be less inductive biases in this community of machine learning, because they're willing to pay this cost of compute to not have to build in the inductive bias that they would normally have to build in for smaller data sets.
And as Saul alluded to, which is the, or as actually Tommy also mentioned, it's just these architectures are getting more and more general purpose. So in this case, this is a transformer architecture, but unlike a transformer, this is an architecture that can attend not over the token level, but treats each pixel as a token.
And what's interesting about this is that they actually train this on ImageNet without any image prior. So the positioning coding that Phil mentioned was actually learned by the model, and they could actually get performance on the order of, you know, normal image prior oriented models like comp nets and, you know, patch vision transformers.
So the remarkable consequence of this is that there's no actual image prior or modality prior built into this model. It learned it on its own. And even in that perspective, it's still competitive on ImageNet. So this is not necessarily a very large data set either.
Do you know if the perceiver of Brian also has like a texture bias, for example, or if in general robustness?
I don't think we've looked into that. I think the robustness qualities are probably not too different from standard models. I think in terms of adversarial examples, like transformers are, well, I think it's controversial, but I think people say transformers are a little bit more robust adversarial examples than comp nets, but on the grand scheme of things, they're both very susceptible to adversarial examples.
I mean, just to finish, I was just curious to see if, you know, I'm sure there's many models now that they can all do whatever 75% on ImageNet, but not all of them will see like a human, whether that's a goal or not, right?
Right, that's a good question. And I think like we have to now be aware that this is where the field is going. Is that where we're going as well as a field? Not, yeah.
What does it mean to learn position features in the case of an image?
Well, you randomly initialize the position encoding to be just from like a random Gaussian or something.
The position of the pixels in the image, so it's like an unordered set of pixels.
Right, so you treat the pixels as, you treat each pixel as an unordered element in a set.
Wait, Brian, is there the case that the top left pixel will always get the same positional encoding? It'll be a learned.
Yeah, so, yeah, so it's not like you shuffle every image independently of every other image and then get.
So I think it's still inserting a lot of information there.
Well, the prior that they're inserting is that the topology is consistent across samples.
Yeah.
Which I think is reasonable about modalities mostly, like you would imagine that the position shouldn't be unique to every single sample.
Because in that case, then I think the problem would be almost like really, really hard.
I don't know if you can even learn anything.
Like randomly shuffle all the pixels in an image.
Yeah, I don't know what you're going to be able to learn.
Every image has its own permutation that you give to the models.
You could learn like higher like statistics, I guess, over like what fixes together.
Another number is how many training example it takes to train this compared to a convolutional network.
Right, so I think one of the things that we know is that it takes more data to train a vision transfer.
But do you have the number for this?
This is, I think, just ImageNet with augmentations, actually.
This isn't like a larger data set.
This is just augmentation, providing the extra information.
Yeah, I don't have the numbers, but the cursor I've seen tend to be, you know, we have a whiteboard.
So the connet is it starts here and goes kind of flat and the transformer starts down here and goes up.
So it scales better, but for low data, it's very worse.
I think that's one of the things that why the community is going towards like industry, especially going towards this direction, because they're willing to pay for the scale via compute costs, rather than having the kind of built in data points, I guess, be not, you know, for free, I guess,
What happens is that convolutions after a certain level of scale will start saturating and transformers still continue to go up in terms of increasing the parameter count will give you still positive returns in performance.
What's interesting in this talk is also there have been a lot of other variations of transformers as well.
And it's kind of strange, actually, people have at least Google has done a meta analysis on transformers and all the zoo of their variations of them.
And at least in terms of scalability, it seems like the original transformers actually scales the best, which is a little bit odd.
Which one's the best.
The vision transformers are also sparse mixture of extra transformers.
Those apparently both scale the AT the AT well.
Don't remember if they were specifically doing vision. I think it was more language tasks.
I think this kind of tells you a story that maybe the machine learning community is going towards, which is not, you know, the fact that architecture matters the most, but the fact that data is actually the really important aspect and I think they're
building now architectures that aren't necessarily good at working well without, you know, being trained, but furthermore work well at absorbing data a lot more efficiently than other architectures.
So like we should think about the things that leads to the resulting model that we work with these days, which is the idea that it's not just the model itself with its architecture, but there are priors about, you know, the compute the capacity of the model to train
that data and also more importantly now more recently. Nice thing is that like as Phil mentioned the notion that a lot of these models don't require supervision so they can absorb larger and larger data sets without much economic cost to the person training it.
One also another aspect of Transformers is the way it was created was mainly as an alternative to recurrent networks. And the reason for that is because as you see in the left arrow I mentioned hardware, and one of the things about Transformers is that they're much more
on GPUs than a recurrent network is and they run much more better on like, you know, stupidly parallel software and a recurrent network does and that's why they become very popular also is because the parallelization of them is much easier than other sequence models.
One comment if I may make how I view this like I feel when you go to Transformers just by introducing the idea of attention dynamically handled by the data you kind of have.
It's an overstatement but you know as general architecture as you can have kind of like, you know, you have really powerful architecture and now to train it you need a lot of data and this is really where we are now.
And more data does better just because the architecture is extremely general, much more than conventional as you said. So I don't know if there will be any other fancy architecture.
One is there are lacking one thing which is vanilla Transformers don't have memory. They don't have feedback connections. And so they're not doing complete in the same way that RNN is.
Of course people are adding memory and recurrence to Transformers but the still the majority of them don't have that so that's like actually I think a big limitation they don't have memory.
And then two is yeah, you know, eventually look up tables will perform best or you know nearest neighbor will perform best because the limit of infinite data those work and we don't have these work in the limit of infinite data.
Yeah, I agree it's not surprising that you need less traction with more data.
Brian you were saying like there's choices to be made for if we're interested in models of brain systems.
So the question is like, what are we interested in in terms of if these architectures become less biased towards being structurally more relevant to neuroscience, but just being more task relevant to neuroscience.
Like, are we going to be stuck at some level of understanding that's only functional.
What makes them structurally that's relevant that's sort of why I was asking these kind of wait questions right. Why do you say that.
Well, I don't. Yeah, so I think the tricky part is like, I think, because things work well people will find ways to say that they're more structurally relevant.
I don't know if Transformers are more structurally relevant than comp nets like obviously there's Fukushima, you know neocognitron which is inspired by neuroscience but attention themselves is never inspired by transformers was never inspired by neuroscience.
So I don't know if they're actually more neuroscience friendly, I guess, in the terms of similarity. But I think what the bigger picture is that they're going to be more and more generic.
And they're going to take less inspiration from structural biases that we know about.
Not necessarily until you know full mentioned the idea of like, you know, recurrence and you know feedback and those things become more.
It's like, actually, one thing that's interesting is that like, there hasn't been much. I mean people have proposed these architectures and they do work but people aren't really using feedback versions of Transformers, mainly because there's no recurrent nature to them.
So for example, all these things are still fee forward architectures that still process from the bottom up.
I think the divergence is going to be like, are we going to be interested in the same models that the AI machine learning committee is going to be interested in? Are we going to be interested in specific models that work on neuroscience but don't necessarily have a functional performance equivalence to what these models have?
And one question I have for the neuroscientists in the room is, what is the scale of data that the human brain is trained on when they reach adulthood?
And my rough estimate or understanding is it's similar to what the biggest Transformers are currently trained on or a little bit smaller than that, but much more data than comp nets were trained on.
So all this stuff about how, you know, in which data regime do you get what kinds of performance? Well, the data regime that seems most relevant to neuroscience, to me, seems more like this Transformer regime, but I don't know if that's true.
Like how many images do humans see compared to these models?
It's not true for text.
Text, I think they're trained on much more data. Yeah.
Yeah, but for you, I'm thinking images, everything on the internet.
Yeah.
There was a paper for a friend to rank this group recently that showed that even if you train a GPT two on a 10 year old amount of text data, it does explain fMRI responses, almost as well as even having a lot more days because fMRI is bad.
So I think he's training a Transformer around 10 million tokens, which is like what a child would be exposed to at the age of 10, while most Transformers are trained in like, like, millions of different systems.
And like, yeah, it could accept my data, but yeah, fMRI is its own.
This is, there's an assumption under this question, which is like, are we actually interested in models of the system in the adult state, or are we interested in models of how the system gets to be in the adult state?
Those are not the same question, right?
So there may be a shift here between models that are like, and you sort of called it out like, hey, we can, instead of us having to hand design them in, this is the bitter lesson version, we'll just lean on the data with a general flexible thing and let the data push it as long as our compute can handle that and we have enough data.
And I think the question you were asking, that's sort of interesting to us, some of us is, does that end state, which of those end states looks more like the adult and the state that's agnostic to whether,
neither of them probably followed the same biology path, but just even in that assumption state, what is the state of affairs?
I don't think we know what the state of affairs is, visual Transformers relative to combat sun alignments with even visual processing.
I mean, somebody was asking here about like somebody who was asking about similarity, maybe that was you.
You know, and then because also at the neural level also requires mapping assumptions that, and they get more complicated with the Transformers, right, like,
but behaviorally, it sounds like, you know, in the Gary was papers are pointing out like there's some, maybe better alignment, but I don't have their compare against the latest, you know, at trained, you know, comments.
Yeah, I don't know about, I think those stories haven't been done to my knowledge of like actual alignment with neural recordings and, of course, I'm sure people here will do that.
But alignment in terms of functional capabilities does seem quite a bit better.
And it don't relate to me because we're a confidence. What can they do in what's been demonstrated with confidence of 10 years ago, classify 1000 animals, cats and dogs and ancient categories.
What can transfer? Well, sure, you can make confidence that grow bigger, but the current generation of the best models are these Transformers like clip, but of course there's a clip non transformers and let's just say clip.
And that seems much closer to the functionality of the human visual system that you can recognize millions of categories or way more than 1000s of categories.
And you can recognize compositions of categories you can type in a red ball and have you recognize the red ball and just see one example of that and, and these, these networks are getting to that point.
So the kind of psychophysical level I think they're getting closer. I don't know at the neural embedding level.
So something that I'd like to share is the, there was one paper that me and William McCullough that we submitted to nerves, it actually got rejected, but there was one about a we just submitted isolar about this transformer model that
achieved state of the art and brain score for area before, which is kind of interesting because we went to the brainstorm competition at the beginning of this year, like just hoping to participate.
And all of a sudden William trained this transformer it was a dual stream transformer with adversarial training and rotations, and we just like broke the record in v4 unexpectedly and wrote a paper about that.
In any case, what I think was interesting is that the same model exact same architecture if you trained it another way just classical SGD image net no fans augmentations adversarial perturbations, the score wasn't that great.
So I wonder, just in general, should we also just be thinking about transformer model or the interaction of transformer models plus any type of training regime or maybe a fancier loss function that we haven't even conceived.
And suppose we do hit like the like explain variants or one correlation in brain score for it like how do we even reverse engineer from that right because the model is just so big.
I'm saying playing devil's advocate on my own work really the model so big, like, how do how do we even go back and isn't open a question, you know just I don't have any ideas or
I mean I think one of the things that I think we often forget is that a model isn't just its architecture like you know the slide before a model is also data.
And once it's interact with data, we have to understand data now to understand what that model is doing can't just understand the architecture.
I think as these models become these architectures become more generic data is going to play a larger and larger role and we're kind of back to now trying to understand data now and understanding what the architecture is.
And I think that's not that's easier or harder.
But also the message is, you know, for the last 10 years until transformers came like three or four years ago, I think the success stories in machine learning was convolutional networks, there was one architect.
Now there are several.
We have quite a few options.
You know they all perform pretty well.
That's right.
And I think if you just compare functions input output or a while they fit the neurons you'll find they're all doing okay.
You need a lot of other constraints, which means what can be implemented by neurons and synapses.
And what cannot or very difficult to see how.
I agree and I gave this guest lecture and Brian's class.
And I was calling it the anachronism conjecture that like as systems get more and more intelligent they kind of converge on the same representations abstractions models and so forth, which other people have you know put forth.
And I think it's kind of the same here I don't actually think the difference between transformers and confidence and MLP is that dramatic. I think it's more.
As you get more and more data and you optimize more and more toward success at some objective.
The models will converge.
It's one way but the other way is that, you know what I said that the meeting a few weeks ago.
It could be like flight.
You know, it could be.
You have, you know, a model of a bird.
But that's, that's not really good for everything.
The important is to understand the principles of aerodynamics, then you can understand how birds fly and how to build airplanes and other things and maybe how fly flies, which is different from birds.
Those aerodynamics involved is different. So, I think principles are much more important than the specific implementations which can be quite different.
The question is what are the principles here.
Yeah, and I think they're similar principles, I think.
All of these pictures are like just reweighting at the same few ingredients factorization is in all of them.
Hierarchy is in all of them right like, I don't know, and even in transformers and confidence like transformers can be written as 90% convolution and just a few little layers that are attention like if you look at the actual operations almost every operation is is a convolution in the sense of being a one by one,
you can just chop up the signal into patches and process each one independently and identically.
Yeah, so I think the principles are going to turn out to be very similar.
The question is, which principles should we care about now, given this kind of heterogeneity and architecture, but similarity and functional performance.
One of the things that it becomes easier if the community like has something they cannot do, whether it be like, you know, to fly, I guess, for example, flight, which is like, how do we achieve something that we can't currently do right now.
I think the general spirit of the machine committee is that all we're done, we can just keep making these models bigger and keep doing this and we'll be fine.
I don't think that's true. But right now it seems like the spirit is in that direction and that's why we're kind of revalidating. Oh, of course this must be like the brain. Of course this is like what we care about.
Of course, all these kind of back explanations are working.
But once we hit a wall, I feel like then we know what's wrong and what's correct. Otherwise, I guess it becomes kind of hard to tell right now.
The topic of like data efficiency. This might have an obvious answer, but like, I was wondering if when you have multimodal data, whether like learning becomes a lot more efficient if you have like experts of information from visual stuff,
like texts, that like maybe our captions associated with the image or something versus like two aspects of just text data.
I don't know how well those things have been estimated, but the language vision models are a lot better on certain benchmarks than the vision only models and it does seem like language must be like incredibly valuable per word.
There's a lot more information than per pixel.
I feel like a lot of the recent successes, just leveraging language, at least in computer vision.
Same with robotics a few other areas. I think what made clip a lot more, at least from that psychophysics experiment from Garrus at all, a lot more like powerful for their results was the fact that clip was trained on classification, but on caption similarity matching
to a text caption which has a lot more information than just a single label for this entire image.
Like caption can tell you, you know, things about geometry tells you what things on the left, what things included, what thing is, what thing what what season it is or what time it is like it tells you a lot more information and a single word would be to
ImageNet class.
Another, okay, this is a little anecdotal, but what I've heard is that for training diffusion models, if you train them without language, just unconditional diffusion model generative model of imagery is really expensive and we all thought like, okay, we're not getting that game is not for us is for Google.
But if you train them text conditional, they're actually much according to the students I've talked to, they train much faster because the text conditional models the text gives you so much leverage.
And so they said that no, no, we can train text conditional like no dolly type models stable diffusion those things are within the budget of MIT.
You do that to trade condition because you have to have text image pairs see if a lot. This is a huge source of supervision here, as opposed to just random images.
And if you have that, then you're in like the hundreds of thousands of dollars of range to train one of those big models, as opposed to the tens of millions of dollars range.
This is anecdotal.
They might be just trying to get some GPUs. I'm not sure.
By the way, address a question to both of you, but an important feature of transformers compared to previous networks is the fact that you don't have to worry about labeling, especially when you use text, right?
Yeah.
I think that's the issue about the supervision of supervision, which is like I never found a consistent definition of what a supervised task is versus not supervised one besides economic cost.
Like how much did you spend to acquire this data seems to be the only consistent label.
But if you speak about neuroscience is not only cost, right?
Right, but then like I think, yeah, this is another divergence in between two communities, right, which is like.
But the real problem is the brain. Come on. Everything else is
More, more questions. I'm happy to keep chatting much for one more spester.
I think we want to quite some time, we can adjourn and to the next iteration sometime in the next few weeks.
Okay. Thank you.
