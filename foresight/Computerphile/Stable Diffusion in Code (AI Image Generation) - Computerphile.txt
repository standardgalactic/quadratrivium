Last time we talked about how these kind of networks and image generation systems work.
But there are different kinds, aren't there?
There are, there's Dali2, there's Imogen, there's StableDiffusion,
and I didn't talk about them in the last video because they are, for the sake of understanding diffusion models in general,
essentially the same, but actually they are quite different underneath, right?
And it comes down to, you know, the resolution, exactly where you do the embeddings,
how you do the embeddings, how you structure your network and so on and so forth, right?
So, and in fact, actually in StableDiffusion's case, it comes down to where you do the diffusion as well.
So, let's look, we'll look at the StableDiffusion code because I've got access to that, right?
And we'll go into it in quite some detail, I think would be quite interesting.
I've really enjoyed using it because first of all, it's given me a better understanding of how it works.
And also, you can do some pretty cool stuff by messing about down there and saying,
well, what if I gave it a frog but also a snake, right?
And the answer is you get a frog snake.
A frog?
Yeah, exactly. Snake giraffe was the stuff of nightmares.
There were questions about FX, there were questions about how these are trained,
maybe we deal with them at another time.
Let's talk about how they work.
So, Dali 2 is perhaps at the moment the biggest one,
but it's been, I think rapidly overtaken by StableDiffusion,
primarily because StableDiffusion is more available to people.
I can download the code and run it of StableDiffusion,
Dali, you access via an API and you say, I would like an image, please,
and it gives you something back.
If you don't have any interest in the code, then sure, just use the API.
But if, for example, like me, you might be interested in what the applications
for generating images in your area of research, like plants or medical imaging,
maybe I want the access to the code and I can train up the network myself, right?
So, Dali builds on a lot of stuff, it's from OpenAI,
it builds on a lot of stuff that they've already done, right?
The first one is something called clip embeddings.
Clip embeddings are the way of taking your text tokens
and turning them into some meaningful numbers, right?
And remember, we're going through a transformer,
so we're not just saying, well, the word that is a five
and the word football is a 17.
What we're doing is we're taking the whole sentence,
we're doing a lot of cross-attention and saying,
this is the overall meaning of a sentence reflected numerically.
So, you get some context.
Yeah, that's the idea.
And clip is trained with image and text pairs.
So, you put in an image, you put in the text that describes that image
and what you try and do is align those two embeddings
so that they kind of make sense
and that way you've got a kind of semantically meaningful text embedding.
So, it's a bit like a supervised data set of some sort.
It is, yeah, it's sort of, it is a supervised data set.
It's trained using a contrastive loss, which is what this CL stands for
and the idea is that basically you want to try and make the embeddings
of an image and its text description very, very similar
and the embeddings of an image with a different text description
very, very different, right?
And not in a dissimilar way to how when we were doing the face ID stuff,
we're trying to put my face near previous shots of my face
so that you can unlock a phone.
Unlock a phone with your face, that's the one.
If you want to unlock a face with your phone,
so you've got a clip embedding, this is the text embedding.
You also have various other things that work.
In Darley, and I'm going to sort of simplify it slightly,
you put in an image which I think is at 64 by 64 pixels.
You put in a noise image at 64 by 64, right?
You put in your time, you put in your clip text embeddings
and you also put them into the network like we described in the previous video.
You have a giant unit structure that produces an estimate for the noise
and you loop.
Now, that produces a not bad image, but only at 64 by 64 pixels.
This process of randomly producing noise,
checking whether it's work, subtracting it, producing it,
this takes a long time at high resolution
and the sort of network you would need would be astronomically big.
So to make that easier, we only run it at 64 by 64.
Now, of course, how do we then make that nice, right?
Because just Darley 2 outputs 1K by 1K images.
The answer is we have another network that does the same thing,
but this time its job is to upsample.
So you basically put in a noisy 64 by 64 and say,
output me the 256 version, right?
And so on and so forth.
So you put this through, I think, two levels of upsampling
to go from 64 to 256 to 1024.
Will you pardon my dumb question?
Yeah.
Are we finally at the point where we can say enhance?
You know what we are?
Yeah.
And it will work exactly like it does in the TV,
where it will just make up nonsense and they'll call it a win.
It works pretty well.
Imagen, Google's version, works in a very similar way.
You have a network that's trained to denoise and generate 64 by 64 images,
guided by text, and then you have two upsampling networks
that go up to 1024.
Stable diffusion does its diffusion process,
sort of in this bit in some sense.
You have what we call an autoencoder, which takes some noise
and turns it into a lower resolution but detailed representation.
You then do the diffusion process this way,
which denoises that latent space.
And then you have the other side of the autoencoder,
which expands it back out into an image.
So this is a different way of doing it.
And the advantage is that this is much lower resolution than this.
And they call it stable diffusion.
There's an argument that is slightly more stable.
I don't know to what extent that's true.
There are some differences in the way that these produce images.
But in all other regards, basically, it's the same kind of process.
You're still doing guidance from text.
You're still putting in T.
It's just that you're now doing it in this latent space
instead of in the full image space.
Think of it like you put it through a zip, right?
And you compress it down and then you do all the diffusion in that space.
And then at the end, you expand it back out again.
That's the idea.
And actually, the autoencoder is very, very good.
You can take an image, you can compress it right down
and it'll still produce much the same image again.
Let's dive into the code and have a look.
Right, so I'm in Google Colab.
Now, for those of you who don't know,
Google Colab is a sort of Jupiter notebook style environment
that allows you to access also Google's GPUs for running machine learning things.
Now, I don't tend to use Google Colab generally
because a lot of our processes last longer than you're really meant to use it for.
But for this, it's excellent, right?
So I've got this code from a guy called Jonathan Whitaker,
which I've then repurposed and done my own stuff with it
and I've been messing about.
And so thanks very much to him for that.
But I've taken it and I've played around.
I've changed the resolution.
I've toyed around with a lot of stuff.
And what I wanted to do was talk through some of these lines of code
so you can see what it is that it's doing.
It's the same exact process that I just described.
It's just a few lines of code to do it.
Now, obviously, there's a lot of deep networks and stuff going on behind the scenes,
but they end up getting abstracted away in function calls
and so it becomes very straightforward.
Okay, I've imported all my libraries already.
And then what we've got here is one go.
We're going to have our text prompt.
And what we want to do is take that text prompt and produce an image, right?
So we have various things that we want it to be, 512 pixels tall, 768 pixels wide.
We're going to run 50 steps of inference
and then a few other things that we can talk about in a moment.
Like, for example, we're going to seed it with the number four.
Now, why four?
Because I don't know, I picked it at random.
I can seed it at 77 if you like.
This allows us to run the exact same code again
and produce the exact same image another time.
If we just used a random seed, if you got to an image you liked,
you accidentally get rid of it, you never get it back, right?
So, but if you change this number, you get entirely different images
because the noise that you start with is entirely different, right?
So let's put in a prompt.
Well, what should we do?
Frogs on stilts?
I think we need to do frogs on stilts.
I mean, this may not work.
I don't, you know, anything else you want to add, like in, you know,
in a park or just just just frogs on stilts?
What about on a stage?
OK, frogs on stilts, on a stage at the theatre, right?
Yeah.
Now, the first thing we have to do is we have to embed this
into some kind of usable space in which the machine learning can work.
So what we do is we tokenize, this is the function that tokenizes
the text input and basically turns it into a numerical code for each word.
And then that goes into the texting coder, which is our clipping beddings.
So that's the bit where it sort of works out the context.
Yeah, that's the transformer that goes, well, OK, this one kind of
goes with this word and then this means they share weights and so on.
And then you go through the transform and you end up with essentially
to us, meaningless numbers, but to this semantic information
on the meaning of the sentence.
We also, because if you remember, we put it through the network twice,
one with the text embeddings and one without.
So we also have to produce a dummy text embedding with nothing in it.
Right. And that's what this unconditioned input is.
Then we're going to text encoder unconditioned embeddings and we get
two text embeddings, one of which is unconditioned and one of which is conditioned.
Right. So this one has fogs on stilts.
This one is just sort of blank.
Now we need to set our scheduler.
Remember, you can choose a scheduler that produces different amounts of noise
at each time step and which one you use will depend on to an extent
the kind of images you want out, but also how you've trained the network.
Where is going to be using the standard one that came with stable diffusion?
And I'm going to run for 50 time steps.
So what this will do is distribute the amount of noise it adds from zero to 50.
Right. So when I say 50, it's going to produce a maximum amount of noise.
And when I say one, it's going to produce a tiny amount of noise.
Right. That's the idea.
And then we're going to actually produce our latent noise that we're going to be diffusing.
So we create a random array of numbers of the right size and we're going to call these
latents and we're going to stick them on the graphics card.
And then we're going to do some scaling to our latents as well,
because the scales of some of these different parts of a network of
difference, you have to move them in and out.
And then we're nearly done.
Like this is our loop.
So how does the loop work?
Well, the first thing we do is we calculate the noise to be added at this particular
iterations.
We're going through all the different time steps and we're going to add a different
amount of noise.
We're going to add this noise to our latent space.
Right.
So basically we're noising up the image here.
Now remember, this is an embedded version of this image, but it is noised.
Then we're going to predict the noise with our unit.
So that is saying how much noise do you think was in this image, such that we can
get back to the original image, bearing in mind this text, and then we can do our
actual classifier for your guidance.
Right.
So what we're going to do is we're going to take our noise prediction with text and
our noise prediction without text.
We're going to calculate the difference and amplify it.
And then we're going to work out what our official noise prediction is.
And then finally, we're going to then use that noise prediction to calculate a
slightly less noise version of the image, which is what this line does here.
And we're going to repeat this process.
Right.
So we repeat the process.
We calculate the new noise at the next time step.
We predict it.
We subtract it away and add a bit more noise.
And we repeat this process.
And the idea is that over 50 iterations, we go from fully noise to some reasonable image.
Shall we see?
OK, so let's run at this resolution.
I'm pushing the amount of image size I can get on this graphics card.
So this is running on your graphics card here.
No, this is running on Google's graphics card over at Google.
Right.
Give me somewhere in London.
Do I owe you another eight pounds for this?
You can give me eight pounds.
No, this is covered under the original eight pounds per month.
But hopefully this won't take a month to record.
So we're choosing 50 iterations for this and because that's a decent amount.
Right.
You'll notice that if you don't do enough iterations, you're trying to move the
noise too quickly.
It becomes a bit unstable, doesn't produce nice results.
Because I've not done this before.
I don't know what these other results will be.
Will it be frogs on stilts?
Will it be bits of wood next to a frog?
Will it be something different because it's failed horribly?
Let's see.
Actually, that's not bad.
That is pretty impressive.
Now there's a weird leg coming out of this fog here, but I would I would say
that is a comparatively successful attempt.
This was produced from a noisy image.
So what we can do is we can change the noise seeds so we can say, you know,
128 and what that will do is create a complete different noise, which will
probably lead to a tiny different image.
Right.
I mean, it's still the same text prompt, so it's still guided in the same way.
But if this allows us to produce see near infinite numbers, basically,
of frogs on stilts, if that's your thing, right?
It is my thing.
Yeah, I've got quite into producing like cityscape, futuristic cityscape.
So I think that's where I spend most of my time on this.
I mean, that's gone a bit wrong, but actually still not bad.
It looks like a kind of stage.
They're just a bit not foggy.
Although yeah.
All right, all right.
OK, so anyway, we could spend, let's say another 20, 30 minutes
producing frogs on stilts, but yeah.
So what you could there's loads of cool stuff you can do.
Presumably you could just automate that.
So it just kept giving you loads of.
And in fact, I've done that right.
So for example, I created some nice pictures of dystopian,
abandoned, futuristic cities with overgrown plants, right?
And then I just put them in a for loop and just produce 200 of them
so I can pick the nice ones.
For example, in here, I've just got a bunch of awesome looking
city vistas, overgrown of plants.
They all look really, really good, right?
I'm quite pleased.
I mean, I've got no use for this, but it's quite fun.
And the other thing is that because you can do image to image guidance,
right, so what you do is you take an image that's your guide image,
you nearly noise it all the way and then you reconstruct, right?
So the noise is somewhat not come from a random place.
Then you can get an image that sort of bears some reflections.
You can say, well, I want a building over here and a tree over here.
So I'll draw them in and then I'll produce this and it will bear the same
that have the same shapes and stuff.
So you can control this process, even if you basically, like me,
have absolute zero artistic ability at all.
To give you an example, what I did was, so if I go down,
let me, let me go.
So this is a picture of my colleague's rabbit, very cute rabbit.
And what I did was I embedded this added noise, but not totally noise
to remove the image.
And then I reconstructed it with the text wooden carving of a rabbit
eating a leaf, highly detailed 4K artisan.
I don't know if the artisan word does anything.
I just thought it'd be fun.
It's trending on art station.
I see a lot of that put on the end of things.
Does that make a difference?
I don't know.
Anyway, and it produce a wooden carving of a rabbit, right?
And if you look at the original image versus this image,
some things have changed, sure, but the shape is roughly the same, right?
So it has guided this process using the original image.
And that's how image to image works.
So if you wanted to create an animation, you could create a quite simple
animation of a rabbit jumping about with no artistic ability, right?
I mean, actually, I would struggle to do even that, but, and then each frame,
you could then use this process to produce it.
At the moment, there's no kind of temporal consistency.
So you will see flickering, right?
If you ever see one of these videos, someone's produced online, it'll look cool,
but maybe not consistent and interesting because each frame might subtly change
things.
But that's the idea, right?
Now, you can do loads of weird stuff, right?
So this mixed guidance is one of my favorite things.
Here we have two text inputs.
And what we're going to do is we're going to embed both of them.
We're actually going to guide the generation using the midpoint of those two, right?
So I'm going to say, OK, I want a rabbit, right?
And I want a frog.
And I want you to produce me a 50-50 rabbit frog, right?
And what it will do is it'll embed both of them and it will do
the exact same process.
It's just that now, its text prompt is halfway between these two embeddings.
So you could potentially come up with a system with sliders, you know, 10-70 frog.
You could, to what amount of frog do you want in this image, right?
I mean, again, not sure what the use case is, but it's quite cool.
Here we go.
So it only takes about, I think, I'm training for 50 steps again.
So I'm running it for 50 steps.
While this work, you could do loads of stuff.
So for example, you could generate an image and then you could take half of it
and try and generate the other half to expand it outwards
and slowly grow your image to make it even higher as one, right?
If you're limited by the resolution.
And there's going to be a lot of people playing around with a lot of different ways to use this.
I've already seen the plugins for Gimp and for Photoshop and stuff.
There it is.
It's a strange one.
We'll put links to the code in the description.
Have a go.
You need to register for Huggingface to get access to the weights originally.
But then you can use something like Google Colab or your own hardware to generate pictures.
And people are having a lot of fun.
There are websites now where you can find cool images and the prompts that we use to generate them
to give you some ideas.
So there's lots of cool stuff to do.
And the rabbit, it's the same shape rabbit.
There's a bit more noise, right?
And then we come over here and we come over here
and we end up with just noise.
It looks like nonsense.
And so the question is...
It takes the same amount of time to make one sandwich, but you've got two people doing it.
So they make twice as many sandwiches each time they make a sandwich.
Same with the computer.
We could either make the computer processor faster or...
