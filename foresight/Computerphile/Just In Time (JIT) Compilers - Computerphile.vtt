WEBVTT

00:00.000 --> 00:04.400
We're going to be looking at just-in-time compilation, which is a technique for making

00:04.400 --> 00:08.960
programming languages run faster. We all want faster programming languages because then we get

00:08.960 --> 00:14.240
more speed, and there are various ways we can do it. We can statically compile things. That's

00:14.240 --> 00:18.720
typically done for languages like C, or we can just-in-time compile them, which is often done

00:18.720 --> 00:23.680
for languages like Java or JavaScript. The difference between these two is that

00:23.680 --> 00:28.560
just-in-time compilation is looking at the program running and optimizing it after it's

00:28.560 --> 00:33.600
observed it running. Just-in-time is really a terrible name because it should have happened

00:33.600 --> 00:40.080
before you actually got to that stage, but that's the name we've got. It's a technique that you

00:40.080 --> 00:45.760
will all have used today. If you've used a browser, you're using a JavaScript just-in-time compiler,

00:46.800 --> 00:51.520
and they're sometimes considered a bit magical, so I'm hoping we can look a little bit at the magic.

00:53.760 --> 00:57.920
I'm digging the idea of magical computing. There's no wizards.

00:57.920 --> 01:02.800
I don't have a beard, and I kept the cape at home today. Actually, to disambiguate something that

01:02.800 --> 01:07.120
people often confuse, people often talk about compiled and interpreted languages, and there's

01:07.120 --> 01:12.960
no such thing. For any given program language, you can implement it as a compiler or an interpreter

01:12.960 --> 01:17.600
or just-in-time compiler. People say C is a compiled language, but you can, and there are C

01:17.600 --> 01:23.520
interpreters, and people say JavaScript is an interpreted or just-in-time compiled language,

01:23.520 --> 01:28.160
but you can write a static compiler for that as well. So what do I mean in the sense by those

01:28.160 --> 01:34.480
things? A static compiler reads a programming, just looks at the code the program has written,

01:34.480 --> 01:40.960
and then tries to convert it into machine code, let's say. An interpreter looks at the program,

01:40.960 --> 01:46.800
and then it doesn't convert it into machine code. It executes it almost as is. It probably converts

01:46.800 --> 01:50.560
into some other representation, but it's a very simple way of implementing a programming language.

01:51.520 --> 01:56.240
And a just-in-time compiler nearly always starts a program running an interpreter,

01:56.240 --> 01:59.760
looks at it for a while, says things like, oh, look, you're calling that function a lot, or

02:00.400 --> 02:05.040
there's a function that takes some parameters in, and there are always integers, or always strings.

02:05.040 --> 02:09.760
So I will now produce an optimized version of that function, or part of the program,

02:09.760 --> 02:14.880
based on that information I've observed. And it's really that dynamic analysis and

02:14.880 --> 02:19.360
conversion into machine code at runtime that makes just-in-time compilers very effective.

02:19.360 --> 02:23.440
They can be faster than a static compiler because they've got more information.

02:23.440 --> 02:27.840
So if you just look at a program at compile time, you've just got the code you've written on screen,

02:28.960 --> 02:34.160
you can't fully analyze it as much as you want. So you'll make certain assumptions and guesses,

02:34.160 --> 02:38.480
but they may be incomplete or even incorrect, and you'll optimize based on that.

02:38.480 --> 02:42.400
When you're actually running the thing, you've got more information, so you can make

02:42.400 --> 02:48.160
a much better quality optimization, but you've had to watch the program and observe it. So it

02:48.160 --> 02:54.240
started slow, and then hopefully, over time, hopefully it's warmed up is the term, and then

02:54.240 --> 02:58.800
it gets faster. Warming up turns out to be another kettle of fish. It doesn't always quite work as

02:58.800 --> 03:03.440
we expect. These things have some very surprising emergent behavior, but generally they do work,

03:03.440 --> 03:08.720
and when they do work, they can be very effective. So this is things like, it knows it, and you kind

03:08.720 --> 03:13.280
of, again, alluded to before, it knows it's going to be calling this function quite a bit,

03:13.280 --> 03:17.200
so it keeps that nearby, and things like that. Is that- It can do those sorts of things. Maybe

03:17.200 --> 03:24.400
we could try a little simple example. So if I log in, so what I'll- It's just a simple example,

03:24.400 --> 03:29.280
so there's a load of these that I could use, the Java virtual machines, a just-in-time compiler,

03:29.280 --> 03:34.000
the JavaScript, VMs, all of the ones, VA and Chrome, spider monkey and Firefox, and those

03:34.000 --> 03:38.560
wall jits, but I'll look at one for Python, because that's one that I happen to know fairly well,

03:38.560 --> 03:43.840
and let's just write a very silly little Python program. So I can write this function,

03:43.840 --> 03:47.920
let me make a little bit of a bigger font size for you, and it takes two parameters in, and I

03:47.920 --> 03:53.040
will just add those two parameters together. Thing is, am I going to pass it integers, strings,

03:53.040 --> 03:57.200
I can do all of the above. Let's just check that I'm not lying, because I do sometimes,

03:58.080 --> 04:02.880
sometimes intentionally, but mostly unintentionally. Oh, if I do Hello World, I have to have a space in

04:02.880 --> 04:10.080
there. If I save that file, and then- Right, so it prints out five or Hello World, so you can see

04:10.080 --> 04:14.640
I'm calling the function in different ways. So this is why it's hard to statically optimize that

04:14.640 --> 04:19.680
function, because integers, strings, it can take in all sorts of things. Now, the normal

04:19.680 --> 04:23.840
implementation of Python, which I'm using here with the Python 3 binary, is an interpreter,

04:23.840 --> 04:29.440
doesn't have a just-in-time compiler, and we can see some obvious consequences of this. Let's put

04:29.440 --> 04:34.880
this in some sort of loop. So I'll just put some very big number here, that looks like a big number

04:34.880 --> 04:40.720
to me, and just repeatedly call that function with some integers, it doesn't really matter. Now,

04:40.720 --> 04:44.880
if I run that, and I'll just call it with the time function, and I'm going to say this is my

04:44.880 --> 04:53.680
newest laptop that has from memory six fast cores and eight slower cores, and it's going to run

04:53.680 --> 04:56.880
randomly on those each time I do it. So some of the performance numbers are not going to be quite

04:56.880 --> 05:01.600
as clear cut as I would like. So I'll try and explain it if I see that happening. So I run that,

05:01.600 --> 05:06.000
and it's taken that a tenth of a second to run. And if I make that number a lot bigger, so I make

05:06.000 --> 05:11.120
it an order of magnitude bigger, this for loop, it now takes a bit longer. And if I make it longer

05:11.120 --> 05:16.800
again, we'll see depending on which style of core it's gone on, I think it's, yeah, it's roughly

05:16.800 --> 05:21.440
gone on the two slower cores. It's roughly as I make the loop run 10 times longer, the program is

05:21.440 --> 05:25.360
taking roughly 10 times longer to run. So that's what we'd expect to see an interpreter. Now,

05:25.440 --> 05:31.040
let's get rid of a couple of those zeros. And what I can do instead is use a different

05:31.040 --> 05:35.040
implementation of Python. And this is one thing that we often confuse, we say Python,

05:35.040 --> 05:38.960
when we mean the language, and there's Python, the language, and there's Python, the implementation,

05:38.960 --> 05:44.560
I can use something called pi pi. And I think it has a jit. Let me turn the jit off. And hope that

05:44.560 --> 05:49.360
I've not made it run too slow. So we can see that running. And I've still got a relatively large

05:49.360 --> 05:55.280
number. So it's about the same speed as the normal version of Python. Now we'll turn the jit

05:55.360 --> 06:05.120
on. And it now runs in, well, that's a 10th of second, it has run two orders of magnitude faster.

06:05.120 --> 06:09.680
And in fact, I think we'll see if I've got this right, let's add another couple of zeros there,

06:09.680 --> 06:14.560
something very big. It's actually doesn't really matter how big I make that loop, it's been able

06:14.560 --> 06:19.200
to observe it a runtime, realize it can just optimize the whole thing away. And that's the

06:19.200 --> 06:24.480
power of just in time compilation. It's looked at my runtime values and made the thing very fast.

06:24.480 --> 06:27.920
It's particularly effective on this sort of numeric code, but it will often work well on

06:27.920 --> 06:32.320
things like strings and so on. And of course, there are some cases where it won't work. It

06:32.320 --> 06:35.920
isn't, as we said earlier, magic, but it is very effective in many cases.

06:36.560 --> 06:42.240
So as you scale up, is that still a benefit to using it, you know, when you've got a million

06:42.240 --> 06:48.800
lines of code, for instance? Good question. Very dependent on your program. In some sense,

06:48.800 --> 06:53.600
actually, the scale of it is less important than the nature of your program. There's a

06:53.600 --> 06:58.400
fundamental assumption here that programs tend to do the same thing over and over again.

06:58.960 --> 07:04.560
So in this case, Pi Pi is looking for loops. And it's what's called a tracing just in time

07:04.560 --> 07:08.880
compiler. So it looks at what the loop is doing and optimizes that there's also method base,

07:08.880 --> 07:11.600
which just look at a function, don't need to worry about the difference too much.

07:12.880 --> 07:16.720
If your loop or your function does the same thing repeatedly with only minor variations,

07:16.720 --> 07:21.920
this will be very effective. If you have a program, which every time it goes around the loop does

07:21.920 --> 07:26.960
something completely different or every time you call the method, then this will be less effective.

07:26.960 --> 07:30.880
And in some cases, then it will even slow things down because the program will never appear to

07:30.880 --> 07:38.160
stabilize. And that's really what your expecting programs do is that they typically, when they

07:38.160 --> 07:42.400
start up, they do some initialization, that's all a bit random. And then they tend to hit

07:42.400 --> 07:45.600
some sort of main part where they do the same thing over and over and over again. And that's

07:45.600 --> 07:51.840
where JIT compilation really comes to the fore. And our process is doing a bit of that anyway.

07:53.360 --> 07:59.040
Yeah, so I think of modern processes as basically a just in time compiler. I write machine code and

07:59.040 --> 08:02.320
okay, I write the textual form and it looks like ARM or x86 or whatever.

08:03.120 --> 08:09.120
That's not what the processor eventually executes. It turns that into some other representation. It

08:09.120 --> 08:14.080
does all sorts of clever optimizations. If you ever want to see things like the

08:16.080 --> 08:22.240
processor optimizes program reorders it like the reorder buffer, they're scary at how clever

08:22.240 --> 08:26.080
they are. And that's why they've got a lot faster, even though the gigahertz part hasn't changed too

08:26.080 --> 08:31.680
much. And there's been a little bit of knowledge transfer between the processor JIT world and the

08:31.680 --> 08:36.080
programming language JIT world. Is this a new thing or how long have these kind of just in time

08:36.080 --> 08:40.960
compilers been around them? They have been around in one form or another for a while, but they really

08:41.040 --> 08:46.240
trace their modern lineage back to the 1980s in a language called self, which has been largely

08:46.240 --> 08:52.880
forgotten a really interesting language that had a just in time compiler via a long sequence of

08:52.880 --> 08:57.680
events that ended up going to the company called son and is then formed the basis and literally

08:57.680 --> 09:03.440
some of the code is still there in the Java virtual machine. So the Java JIT traces itself back to

09:03.440 --> 09:09.920
self. V8, the JavaScript VM in Chrome also traces its lineage back to the Java virtual machine hot

09:09.920 --> 09:16.320
spot and back to self. So really, that's been those have been the big movers. And now you've

09:16.320 --> 09:22.720
got systems like Pi Pi and V8 and spider monkey that have modernized the concept or spread it to

09:22.720 --> 09:27.280
more languages will probably be a better way of putting it. And is this, you know, obviously

09:27.280 --> 09:31.040
traces it through it's quite a long way back. But is it only really being used now because

09:31.040 --> 09:37.200
machines have got that much faster? Yeah, I think there's an element of that. Because for you,

09:37.200 --> 09:42.160
when I was a kid, you could burn your computer every 18 months, and it was twice as fast. And

09:42.960 --> 09:47.440
the death of single core performance is a little exaggerated, partly because the processes are

09:47.440 --> 09:52.640
now doing just in time compilation sorts of things. But yeah, we definitely are looking

09:52.640 --> 09:57.520
increasingly to programming languages to work faster and for many languages, particularly,

09:57.520 --> 10:02.800
but not only those that are dynamically typed like Python or Java. This is really the only

10:02.800 --> 10:07.840
effective technique. And that's why you've seen increasing numbers of them being released for

10:07.840 --> 10:12.480
more and more languages, despite the fact that they're really complicated and expensive to create.

10:12.480 --> 10:15.440
You know, these are not the sort of things you can knock out in an afternoon, they take

10:16.240 --> 10:19.120
some big teams, many years to create in most cases.

10:21.760 --> 10:27.200
Is train a network to undo this process? That's the idea. And if we can do that, then we can start

10:27.280 --> 10:30.560
with random noise, a bit like our GAN, and we can just iterate this process.

10:30.560 --> 10:39.280
Four dice. Die A, B, C and D. And I tell you that die A has a value four. How much did you learn

10:39.280 --> 10:39.920
about the data?

