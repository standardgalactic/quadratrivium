1
00:00:00,000 --> 00:00:04,400
We're going to be looking at just-in-time compilation, which is a technique for making

2
00:00:04,400 --> 00:00:08,960
programming languages run faster. We all want faster programming languages because then we get

3
00:00:08,960 --> 00:00:14,240
more speed, and there are various ways we can do it. We can statically compile things. That's

4
00:00:14,240 --> 00:00:18,720
typically done for languages like C, or we can just-in-time compile them, which is often done

5
00:00:18,720 --> 00:00:23,680
for languages like Java or JavaScript. The difference between these two is that

6
00:00:23,680 --> 00:00:28,560
just-in-time compilation is looking at the program running and optimizing it after it's

7
00:00:28,560 --> 00:00:33,600
observed it running. Just-in-time is really a terrible name because it should have happened

8
00:00:33,600 --> 00:00:40,080
before you actually got to that stage, but that's the name we've got. It's a technique that you

9
00:00:40,080 --> 00:00:45,760
will all have used today. If you've used a browser, you're using a JavaScript just-in-time compiler,

10
00:00:46,800 --> 00:00:51,520
and they're sometimes considered a bit magical, so I'm hoping we can look a little bit at the magic.

11
00:00:53,760 --> 00:00:57,920
I'm digging the idea of magical computing. There's no wizards.

12
00:00:57,920 --> 00:01:02,800
I don't have a beard, and I kept the cape at home today. Actually, to disambiguate something that

13
00:01:02,800 --> 00:01:07,120
people often confuse, people often talk about compiled and interpreted languages, and there's

14
00:01:07,120 --> 00:01:12,960
no such thing. For any given program language, you can implement it as a compiler or an interpreter

15
00:01:12,960 --> 00:01:17,600
or just-in-time compiler. People say C is a compiled language, but you can, and there are C

16
00:01:17,600 --> 00:01:23,520
interpreters, and people say JavaScript is an interpreted or just-in-time compiled language,

17
00:01:23,520 --> 00:01:28,160
but you can write a static compiler for that as well. So what do I mean in the sense by those

18
00:01:28,160 --> 00:01:34,480
things? A static compiler reads a programming, just looks at the code the program has written,

19
00:01:34,480 --> 00:01:40,960
and then tries to convert it into machine code, let's say. An interpreter looks at the program,

20
00:01:40,960 --> 00:01:46,800
and then it doesn't convert it into machine code. It executes it almost as is. It probably converts

21
00:01:46,800 --> 00:01:50,560
into some other representation, but it's a very simple way of implementing a programming language.

22
00:01:51,520 --> 00:01:56,240
And a just-in-time compiler nearly always starts a program running an interpreter,

23
00:01:56,240 --> 00:01:59,760
looks at it for a while, says things like, oh, look, you're calling that function a lot, or

24
00:02:00,400 --> 00:02:05,040
there's a function that takes some parameters in, and there are always integers, or always strings.

25
00:02:05,040 --> 00:02:09,760
So I will now produce an optimized version of that function, or part of the program,

26
00:02:09,760 --> 00:02:14,880
based on that information I've observed. And it's really that dynamic analysis and

27
00:02:14,880 --> 00:02:19,360
conversion into machine code at runtime that makes just-in-time compilers very effective.

28
00:02:19,360 --> 00:02:23,440
They can be faster than a static compiler because they've got more information.

29
00:02:23,440 --> 00:02:27,840
So if you just look at a program at compile time, you've just got the code you've written on screen,

30
00:02:28,960 --> 00:02:34,160
you can't fully analyze it as much as you want. So you'll make certain assumptions and guesses,

31
00:02:34,160 --> 00:02:38,480
but they may be incomplete or even incorrect, and you'll optimize based on that.

32
00:02:38,480 --> 00:02:42,400
When you're actually running the thing, you've got more information, so you can make

33
00:02:42,400 --> 00:02:48,160
a much better quality optimization, but you've had to watch the program and observe it. So it

34
00:02:48,160 --> 00:02:54,240
started slow, and then hopefully, over time, hopefully it's warmed up is the term, and then

35
00:02:54,240 --> 00:02:58,800
it gets faster. Warming up turns out to be another kettle of fish. It doesn't always quite work as

36
00:02:58,800 --> 00:03:03,440
we expect. These things have some very surprising emergent behavior, but generally they do work,

37
00:03:03,440 --> 00:03:08,720
and when they do work, they can be very effective. So this is things like, it knows it, and you kind

38
00:03:08,720 --> 00:03:13,280
of, again, alluded to before, it knows it's going to be calling this function quite a bit,

39
00:03:13,280 --> 00:03:17,200
so it keeps that nearby, and things like that. Is that- It can do those sorts of things. Maybe

40
00:03:17,200 --> 00:03:24,400
we could try a little simple example. So if I log in, so what I'll- It's just a simple example,

41
00:03:24,400 --> 00:03:29,280
so there's a load of these that I could use, the Java virtual machines, a just-in-time compiler,

42
00:03:29,280 --> 00:03:34,000
the JavaScript, VMs, all of the ones, VA and Chrome, spider monkey and Firefox, and those

43
00:03:34,000 --> 00:03:38,560
wall jits, but I'll look at one for Python, because that's one that I happen to know fairly well,

44
00:03:38,560 --> 00:03:43,840
and let's just write a very silly little Python program. So I can write this function,

45
00:03:43,840 --> 00:03:47,920
let me make a little bit of a bigger font size for you, and it takes two parameters in, and I

46
00:03:47,920 --> 00:03:53,040
will just add those two parameters together. Thing is, am I going to pass it integers, strings,

47
00:03:53,040 --> 00:03:57,200
I can do all of the above. Let's just check that I'm not lying, because I do sometimes,

48
00:03:58,080 --> 00:04:02,880
sometimes intentionally, but mostly unintentionally. Oh, if I do Hello World, I have to have a space in

49
00:04:02,880 --> 00:04:10,080
there. If I save that file, and then- Right, so it prints out five or Hello World, so you can see

50
00:04:10,080 --> 00:04:14,640
I'm calling the function in different ways. So this is why it's hard to statically optimize that

51
00:04:14,640 --> 00:04:19,680
function, because integers, strings, it can take in all sorts of things. Now, the normal

52
00:04:19,680 --> 00:04:23,840
implementation of Python, which I'm using here with the Python 3 binary, is an interpreter,

53
00:04:23,840 --> 00:04:29,440
doesn't have a just-in-time compiler, and we can see some obvious consequences of this. Let's put

54
00:04:29,440 --> 00:04:34,880
this in some sort of loop. So I'll just put some very big number here, that looks like a big number

55
00:04:34,880 --> 00:04:40,720
to me, and just repeatedly call that function with some integers, it doesn't really matter. Now,

56
00:04:40,720 --> 00:04:44,880
if I run that, and I'll just call it with the time function, and I'm going to say this is my

57
00:04:44,880 --> 00:04:53,680
newest laptop that has from memory six fast cores and eight slower cores, and it's going to run

58
00:04:53,680 --> 00:04:56,880
randomly on those each time I do it. So some of the performance numbers are not going to be quite

59
00:04:56,880 --> 00:05:01,600
as clear cut as I would like. So I'll try and explain it if I see that happening. So I run that,

60
00:05:01,600 --> 00:05:06,000
and it's taken that a tenth of a second to run. And if I make that number a lot bigger, so I make

61
00:05:06,000 --> 00:05:11,120
it an order of magnitude bigger, this for loop, it now takes a bit longer. And if I make it longer

62
00:05:11,120 --> 00:05:16,800
again, we'll see depending on which style of core it's gone on, I think it's, yeah, it's roughly

63
00:05:16,800 --> 00:05:21,440
gone on the two slower cores. It's roughly as I make the loop run 10 times longer, the program is

64
00:05:21,440 --> 00:05:25,360
taking roughly 10 times longer to run. So that's what we'd expect to see an interpreter. Now,

65
00:05:25,440 --> 00:05:31,040
let's get rid of a couple of those zeros. And what I can do instead is use a different

66
00:05:31,040 --> 00:05:35,040
implementation of Python. And this is one thing that we often confuse, we say Python,

67
00:05:35,040 --> 00:05:38,960
when we mean the language, and there's Python, the language, and there's Python, the implementation,

68
00:05:38,960 --> 00:05:44,560
I can use something called pi pi. And I think it has a jit. Let me turn the jit off. And hope that

69
00:05:44,560 --> 00:05:49,360
I've not made it run too slow. So we can see that running. And I've still got a relatively large

70
00:05:49,360 --> 00:05:55,280
number. So it's about the same speed as the normal version of Python. Now we'll turn the jit

71
00:05:55,360 --> 00:06:05,120
on. And it now runs in, well, that's a 10th of second, it has run two orders of magnitude faster.

72
00:06:05,120 --> 00:06:09,680
And in fact, I think we'll see if I've got this right, let's add another couple of zeros there,

73
00:06:09,680 --> 00:06:14,560
something very big. It's actually doesn't really matter how big I make that loop, it's been able

74
00:06:14,560 --> 00:06:19,200
to observe it a runtime, realize it can just optimize the whole thing away. And that's the

75
00:06:19,200 --> 00:06:24,480
power of just in time compilation. It's looked at my runtime values and made the thing very fast.

76
00:06:24,480 --> 00:06:27,920
It's particularly effective on this sort of numeric code, but it will often work well on

77
00:06:27,920 --> 00:06:32,320
things like strings and so on. And of course, there are some cases where it won't work. It

78
00:06:32,320 --> 00:06:35,920
isn't, as we said earlier, magic, but it is very effective in many cases.

79
00:06:36,560 --> 00:06:42,240
So as you scale up, is that still a benefit to using it, you know, when you've got a million

80
00:06:42,240 --> 00:06:48,800
lines of code, for instance? Good question. Very dependent on your program. In some sense,

81
00:06:48,800 --> 00:06:53,600
actually, the scale of it is less important than the nature of your program. There's a

82
00:06:53,600 --> 00:06:58,400
fundamental assumption here that programs tend to do the same thing over and over again.

83
00:06:58,960 --> 00:07:04,560
So in this case, Pi Pi is looking for loops. And it's what's called a tracing just in time

84
00:07:04,560 --> 00:07:08,880
compiler. So it looks at what the loop is doing and optimizes that there's also method base,

85
00:07:08,880 --> 00:07:11,600
which just look at a function, don't need to worry about the difference too much.

86
00:07:12,880 --> 00:07:16,720
If your loop or your function does the same thing repeatedly with only minor variations,

87
00:07:16,720 --> 00:07:21,920
this will be very effective. If you have a program, which every time it goes around the loop does

88
00:07:21,920 --> 00:07:26,960
something completely different or every time you call the method, then this will be less effective.

89
00:07:26,960 --> 00:07:30,880
And in some cases, then it will even slow things down because the program will never appear to

90
00:07:30,880 --> 00:07:38,160
stabilize. And that's really what your expecting programs do is that they typically, when they

91
00:07:38,160 --> 00:07:42,400
start up, they do some initialization, that's all a bit random. And then they tend to hit

92
00:07:42,400 --> 00:07:45,600
some sort of main part where they do the same thing over and over and over again. And that's

93
00:07:45,600 --> 00:07:51,840
where JIT compilation really comes to the fore. And our process is doing a bit of that anyway.

94
00:07:53,360 --> 00:07:59,040
Yeah, so I think of modern processes as basically a just in time compiler. I write machine code and

95
00:07:59,040 --> 00:08:02,320
okay, I write the textual form and it looks like ARM or x86 or whatever.

96
00:08:03,120 --> 00:08:09,120
That's not what the processor eventually executes. It turns that into some other representation. It

97
00:08:09,120 --> 00:08:14,080
does all sorts of clever optimizations. If you ever want to see things like the

98
00:08:16,080 --> 00:08:22,240
processor optimizes program reorders it like the reorder buffer, they're scary at how clever

99
00:08:22,240 --> 00:08:26,080
they are. And that's why they've got a lot faster, even though the gigahertz part hasn't changed too

100
00:08:26,080 --> 00:08:31,680
much. And there's been a little bit of knowledge transfer between the processor JIT world and the

101
00:08:31,680 --> 00:08:36,080
programming language JIT world. Is this a new thing or how long have these kind of just in time

102
00:08:36,080 --> 00:08:40,960
compilers been around them? They have been around in one form or another for a while, but they really

103
00:08:41,040 --> 00:08:46,240
trace their modern lineage back to the 1980s in a language called self, which has been largely

104
00:08:46,240 --> 00:08:52,880
forgotten a really interesting language that had a just in time compiler via a long sequence of

105
00:08:52,880 --> 00:08:57,680
events that ended up going to the company called son and is then formed the basis and literally

106
00:08:57,680 --> 00:09:03,440
some of the code is still there in the Java virtual machine. So the Java JIT traces itself back to

107
00:09:03,440 --> 00:09:09,920
self. V8, the JavaScript VM in Chrome also traces its lineage back to the Java virtual machine hot

108
00:09:09,920 --> 00:09:16,320
spot and back to self. So really, that's been those have been the big movers. And now you've

109
00:09:16,320 --> 00:09:22,720
got systems like Pi Pi and V8 and spider monkey that have modernized the concept or spread it to

110
00:09:22,720 --> 00:09:27,280
more languages will probably be a better way of putting it. And is this, you know, obviously

111
00:09:27,280 --> 00:09:31,040
traces it through it's quite a long way back. But is it only really being used now because

112
00:09:31,040 --> 00:09:37,200
machines have got that much faster? Yeah, I think there's an element of that. Because for you,

113
00:09:37,200 --> 00:09:42,160
when I was a kid, you could burn your computer every 18 months, and it was twice as fast. And

114
00:09:42,960 --> 00:09:47,440
the death of single core performance is a little exaggerated, partly because the processes are

115
00:09:47,440 --> 00:09:52,640
now doing just in time compilation sorts of things. But yeah, we definitely are looking

116
00:09:52,640 --> 00:09:57,520
increasingly to programming languages to work faster and for many languages, particularly,

117
00:09:57,520 --> 00:10:02,800
but not only those that are dynamically typed like Python or Java. This is really the only

118
00:10:02,800 --> 00:10:07,840
effective technique. And that's why you've seen increasing numbers of them being released for

119
00:10:07,840 --> 00:10:12,480
more and more languages, despite the fact that they're really complicated and expensive to create.

120
00:10:12,480 --> 00:10:15,440
You know, these are not the sort of things you can knock out in an afternoon, they take

121
00:10:16,240 --> 00:10:19,120
some big teams, many years to create in most cases.

122
00:10:21,760 --> 00:10:27,200
Is train a network to undo this process? That's the idea. And if we can do that, then we can start

123
00:10:27,280 --> 00:10:30,560
with random noise, a bit like our GAN, and we can just iterate this process.

124
00:10:30,560 --> 00:10:39,280
Four dice. Die A, B, C and D. And I tell you that die A has a value four. How much did you learn

125
00:10:39,280 --> 00:10:39,920
about the data?

