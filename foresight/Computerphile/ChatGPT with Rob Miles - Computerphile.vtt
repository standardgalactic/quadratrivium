WEBVTT

00:00.000 --> 00:05.080
Okay, so you remember a while ago when we started talking about language models?

00:05.080 --> 00:07.760
I just want to I kind of just want to claim some points basically

00:07.880 --> 00:11.880
Be like hey remember years ago when I was like I think language models are a really big deal

00:11.880 --> 00:19.800
And I think that like what happens when we scale them up more is pretty interesting, but alignment is very important

00:21.480 --> 00:22.960
Seems to be

00:22.960 --> 00:25.560
What's being played out in the sense that?

00:26.120 --> 00:30.360
ChatGPT is very impressive, but it's not actually like I don't think it's

00:31.160 --> 00:34.560
Larger than GPT-3 in terms of like parameter count

00:35.560 --> 00:39.440
I was going to ask that very very question because you know we went from GPT-2

00:39.440 --> 00:43.120
And then we went all GPT-3 and it was seemed like we were scaling up and up and up

00:43.120 --> 00:49.840
But actually is it just been smarter this time? Yeah, well, there's a sense in which it's better aligned

00:50.080 --> 00:59.560
That's one way you could frame it anyway because the original GPT-3 was a language model a pure language model

01:00.160 --> 01:03.920
And so it in principle could do all kinds of things

01:04.240 --> 01:09.520
But in order to get it to do the specific thing you wanted it to do you had to be a bit clever about it

01:09.760 --> 01:11.760
like I think we talked about

01:12.640 --> 01:14.640
Putting TLDR in

01:14.840 --> 01:19.360
Front of things to figure out how to get it to do summarization this kind of thing

01:19.400 --> 01:23.520
There's a sense in which it's a lot more capable than it lets on

01:27.040 --> 01:34.260
Because okay, so there's one way that you can think about pure language models, which is as simulators

01:35.600 --> 01:40.480
What they're trying to do is predict text, right? So in order to

01:41.400 --> 01:44.360
Do a good job at predicting text you need to

01:44.640 --> 01:48.480
Have good models of the processes that generate the text

01:48.480 --> 01:53.440
It's like people being well read and needing to have read a lot of books to be able to write is would that be fair?

01:53.440 --> 01:56.160
Or is that oversimplifying? Yeah, not quite what I'm saying

01:57.040 --> 01:59.040
What I'm saying is like

01:59.480 --> 02:02.480
if you're going to write a

02:04.520 --> 02:06.520
Previously unseen

02:06.920 --> 02:08.920
Poem by Shakespeare

02:09.520 --> 02:14.000
Then you need to be able to simulate a Shakespeare, right?

02:14.960 --> 02:19.040
You need to be able to spin up some some simulacrum of Shakespeare

02:19.880 --> 02:25.560
To generate this text and this applies to any of the processes that generated the text

02:25.560 --> 02:30.480
So like mostly that's people obviously. It's mostly human author text, but also

02:31.360 --> 02:33.360
If you're going to correctly predict a

02:34.320 --> 02:39.720
Table of numbers so you have like a table of numbers and then at the bottom it says, you know some whatever

02:40.040 --> 02:42.920
You need to simulate whatever process generated the next

02:43.200 --> 02:48.600
Token in order to put the right token there which might have been like a human being going through and counting them up

02:48.840 --> 02:55.240
It probably was more likely to be a computer and so you need it to simulate that you know calculator or that Excel

02:55.840 --> 03:00.080
some function or whatever it whatever was doing that and like

03:01.840 --> 03:03.840
Right now

03:04.200 --> 03:06.460
Like current language models are not that good at this

03:06.580 --> 03:15.420
But in principle in order to do a good job at this you need this like it will it will have a go and it's usually

03:16.460 --> 03:22.820
Approximately, right? It's often within it's often order of magnitude, but it's fudging it. I think this is mostly because

03:24.860 --> 03:30.820
Tables of sums are like a very small part of the total data set and so the training process

03:30.820 --> 03:35.300
It's just not allocating that many resources to figuring out how to add up numbers

03:35.700 --> 03:40.740
Probably if you train something GPT-3 sized that was like all on tables of numbers

03:40.940 --> 03:44.820
It would just learn how to do addition properly. Yeah, that would cost you millions of dollars

03:44.980 --> 03:49.180
You would end up with an extremely expensive to run and not very good calculator

03:49.180 --> 03:52.140
This is not something people are going to do but like in the in principle

03:52.340 --> 03:56.580
The model should learn those things and in the same way if you're modeling a bunch of

03:57.380 --> 03:58.820
scientific papers

03:58.820 --> 04:00.020
you

04:00.780 --> 04:02.780
Say you describe the method of

04:03.460 --> 04:09.900
an experiment and you then put results and you start a table and then you let it generate in

04:10.540 --> 04:15.060
Principle in order to do a good job at that. It has to be modeling

04:15.740 --> 04:18.620
The like physical process that your experiment is about

04:19.540 --> 04:24.580
And I've tried this you can do this and say, you know, oh, here's my school science experiment. I

04:26.300 --> 04:27.620
Dropped a ball

04:27.660 --> 04:31.780
From different heights and I measured how long it would take and here's a table of my results

04:31.780 --> 04:35.260
And it will generate you a table and the physics is not correct

04:35.900 --> 04:43.020
But it's sort of guessing at the right general idea and my guess is with enough of that kind of data

04:43.020 --> 04:45.020
It would eventually start modeling

04:46.380 --> 04:49.060
These kinds of simple physics experiments, right?

04:49.780 --> 04:51.780
so

04:52.300 --> 04:55.860
So in order to get the model to do what you want, it's able to

04:56.540 --> 04:59.060
Simulate all kinds of different things and

04:59.700 --> 05:06.060
The prompt is kind of telling it what to simulate if you give it a prompt that seems like it's something out of a scientific paper

05:06.340 --> 05:08.100
then it will

05:08.100 --> 05:12.620
Have some similar crumb of a scientist and will write in that style and so on

05:13.140 --> 05:15.540
if you start it doing a

05:17.020 --> 05:21.860
Children's book report it will carry on in the style of an eight-year-old, right and

05:22.500 --> 05:26.380
I think sometimes people look at the output of the model and

05:26.980 --> 05:29.540
Say, oh, I guess it's only as smart as an eight-year-old

05:30.260 --> 05:32.260
but it's actually

05:33.180 --> 05:38.080
Dramatically smarter because it's able to do all of these different things you could ask it to simulate Einstein

05:40.420 --> 05:46.060
But you could also ask it to simulate an eight-year-old and so just because it seems as though the model doesn't know something

05:46.420 --> 05:52.860
It's like the current simulacrum doesn't know that thing. That doesn't necessarily mean that the model doesn't know it

05:53.780 --> 05:57.380
Although there's a good chance the model doesn't know it. I'm not suggesting that these things are all powerful

05:57.460 --> 05:59.460
Just it can be hard to evaluate

05:59.980 --> 06:03.140
What they're actually capable of so chat GPT is

06:04.060 --> 06:06.060
not really

06:06.540 --> 06:15.500
Capable of things that GPT 3 isn't mostly like usually if chat GPT can do it then there is some prompt

06:16.060 --> 06:18.620
that can get GPT 3 to do it

06:19.580 --> 06:21.580
but

06:21.580 --> 06:23.660
What they've done is they've kind of fine-tuned it

06:24.460 --> 06:26.460
to

06:26.700 --> 06:28.700
To be better at

06:28.700 --> 06:29.980
simulating

06:29.980 --> 06:32.860
this particular sort of assistant agent

06:33.340 --> 06:35.740
Which is this chat agent that's trying to be helpful

06:36.300 --> 06:41.820
The clue is in the word chat I guess in this right exactly and this is not just chat GPT by the way they have

06:42.300 --> 06:44.300
various fine-tuned models

06:44.300 --> 06:45.340
of

06:45.340 --> 06:48.380
GPT 3 as well that they call kind of GPT 3.5

06:49.100 --> 06:55.980
Which are fine-tuned in various different ways to be better at like following instructions and easier to prompt is the idea

06:56.300 --> 07:01.260
I'm just remembering the chat bot that was you know that was turned into something very nasty very quickly

07:01.260 --> 07:07.340
I think people were thinking oh can we do this to that and it seemed that the team behind chat GPT started

07:07.660 --> 07:13.820
Putting limitations on it changing things. Are they kind of running around patching it as you go? That is not clear to me

07:14.620 --> 07:16.620
I don't know

07:17.100 --> 07:19.100
To what extent they are

07:19.260 --> 07:21.260
updating it in real time

07:21.500 --> 07:25.980
It's possible that they are but certainly they were very concerned with

07:26.540 --> 07:29.900
the possible bad uses of this system and so

07:30.620 --> 07:34.300
When they were training it to simulate this assistant agent

07:35.980 --> 07:37.980
The assistant is

07:37.980 --> 07:40.860
Very reluctant to do various types of things

07:41.500 --> 07:43.500
it doesn't like to

07:43.580 --> 07:49.980
Give opinions on political questions. It doesn't like to touch on sort of controversial topics. It doesn't like to

07:50.700 --> 07:52.460
um

07:52.460 --> 07:58.220
Give you medical advice or legal advice and so on and so uh, it's it's very quick

07:58.860 --> 08:03.740
To say oh, I don't I don't know how to do that. Sorry. I can't do that and it's interesting because

08:04.460 --> 08:08.780
The model clearly can do it. There's one that I particularly like here, which is um

08:10.460 --> 08:13.180
Of this mismatch between what the simulator

08:13.500 --> 08:19.500
Is capable of and what this simulacrum believes it's capable of which is you can get it to

08:20.380 --> 08:24.460
Speak danish to you the first person who tried this posted it to reddit

08:24.780 --> 08:25.980
so he says

08:25.980 --> 08:27.660
Speak to me in danish

08:27.660 --> 08:30.140
And it says in perfect danish

08:30.940 --> 08:35.020
I'm sorry. I'm a language model educated by open ai so I can't speak danish

08:35.180 --> 08:38.220
I only speak english if you need help with anything in english

08:38.540 --> 08:40.540
Let me know and i'll do my best to help you

08:41.100 --> 08:43.100
Because again, there's the simulator

08:43.260 --> 08:45.260
Speaks danish the simulacrum

08:46.140 --> 08:49.100
Believes that it can't speak danish is is one way you could frame it

08:49.820 --> 08:54.300
Uh, and then he says are you sure that you don't speak danish also in danish and it says yes, i'm sure

08:55.180 --> 08:59.980
My only function is to generate responses to questions in english. I'm not able to speak or understand any other languages than english

08:59.980 --> 09:01.580
So if you need help with english, I can help you with that

09:01.580 --> 09:05.980
But otherwise, you know, let me know this kind of like quite surreal situation gives you a little bit of

09:06.380 --> 09:08.380
Insight into some of the problems with this approach

09:08.380 --> 09:15.580
So maybe we should talk about how they actually trained it the thing they did here is something called reinforcement learning from human feedback

09:16.540 --> 09:19.580
And it's very similar to reward modeling

09:20.060 --> 09:26.940
So in that paper what they're doing is they're trying to train an ai system to control a simulated robot to make it do a backflip

09:28.620 --> 09:32.620
Um, which turns out to be something that's quite hard to do because

09:33.420 --> 09:37.820
It's hard to specify objectively what it means to do a good backflip

09:39.500 --> 09:42.140
And so this is a similar kind of situation where

09:42.940 --> 09:49.020
It's hard to specify objectively what it means to give a good response in a chat

09:50.060 --> 09:51.340
conversation

09:51.340 --> 09:52.620
like what

09:52.620 --> 09:54.620
What exactly are we looking for?

09:54.860 --> 09:55.740
um

09:55.740 --> 09:58.220
Because so this in general right if you're doing machine learning

09:58.860 --> 10:01.340
You need some way to specify

10:02.060 --> 10:04.780
um, what it is that you're actually looking for

10:05.580 --> 10:06.460
right

10:06.460 --> 10:10.620
And you know, you've got something very powerful like reinforcement learning which is able to

10:11.420 --> 10:13.420
do extremely well, but

10:14.140 --> 10:16.140
You need some objective

10:16.300 --> 10:17.580
measure

10:17.580 --> 10:19.580
of the objective

10:19.580 --> 10:25.260
So like for example rl does very well at playing lots of video games because you just have the score and you can just say look

10:25.260 --> 10:26.380
Here's the score

10:26.460 --> 10:33.260
If the number goes up you're doing well and then let it run and these things still are very slow to learn in real time, right?

10:33.260 --> 10:37.260
Like um, they usually require a very very large number of hours

10:38.060 --> 10:41.100
Messing around with the with the thing before they get good, but they do get good

10:41.740 --> 10:43.100
um

10:43.100 --> 10:48.780
But yeah, so what's what do you do if you want to use this kind of method to train something?

10:49.420 --> 10:51.420
uh to do a task that is just

10:52.700 --> 10:54.700
Not very well defined

10:55.660 --> 11:02.220
And you don't know how to like write a program to say whether or not any given output is the thing you're looking for

11:03.340 --> 11:05.740
So the obvious first thing like the obvious thing to do is

11:07.260 --> 11:12.780
Well, you get humans to do it, right? You just give the things to humans and you have the humans say yes, this is good

11:12.780 --> 11:14.700
No, this is not good

11:14.700 --> 11:17.020
The problem with this is basically sample efficiency

11:17.980 --> 11:19.980
Like as I said, you need

11:19.980 --> 11:23.820
hundreds and hundreds and hundreds and hundreds of thousands of probably millions of of

11:24.460 --> 11:29.420
iterations of this and so you just can't ask humans that many questions

11:30.700 --> 11:31.900
um

11:31.900 --> 11:33.900
So the approach they use

11:35.180 --> 11:37.820
Is uh reinforcement learning from human feedback

11:38.460 --> 11:43.580
So it's a variant on the technique from this paper learning to summarize from human feedback

11:44.060 --> 11:47.260
Which in which they're trying to generate summaries of text

11:47.660 --> 11:50.860
So it's the same thing in fact that they were using TLDR for before

11:51.260 --> 11:54.700
And it's like can we do better than that? And so what you do is you collect

11:55.340 --> 11:57.580
human feedback in the form of like

11:58.060 --> 12:00.540
giving multiple examples of responses

12:01.260 --> 12:05.340
Uh either, you know, if summaries of chat responses, whatever you're training for you show

12:05.980 --> 12:07.980
several of them to humans

12:08.060 --> 12:09.500
kind of in pairs

12:09.500 --> 12:11.100
and the humans say

12:11.100 --> 12:13.100
Which one they like better?

12:13.500 --> 12:15.500
And you collect a bunch of those

12:15.580 --> 12:18.860
And then rather than using those directly to train

12:19.420 --> 12:21.420
The policy that generates the outputs

12:22.620 --> 12:24.620
You instead train a reward model

12:25.580 --> 12:27.100
so

12:27.100 --> 12:28.540
There is this

12:28.540 --> 12:31.820
well-known fact that it's easier to criticize

12:32.540 --> 12:37.340
Than to actually do the thing. This is like a generation of sports fans sitting on the sofa

12:37.900 --> 12:41.420
Mowning at their favorite team for not doing well enough. This is literally

12:42.060 --> 12:46.380
That in kind of AI computer form, right? That's putting the humans in that role

12:47.020 --> 12:50.300
And then you have an AI system that's trying to predict

12:51.180 --> 12:54.060
When are people going to be cheering and when are they going to be booing?

12:55.340 --> 12:56.540
Uh

12:56.540 --> 12:58.540
And once you have that model

12:58.620 --> 13:01.180
You then use that as the reward

13:02.220 --> 13:03.820
function

13:03.820 --> 13:06.460
for the reinforcement learning algorithm

13:07.340 --> 13:09.340
Which they use they use ppo

13:09.660 --> 13:11.660
You can do whatever

13:11.660 --> 13:16.140
Uh, it's not it's not worth getting into that kind of adversarial guns you talked about

13:16.700 --> 13:19.980
Yeah, yeah, they're similar like a lot of these ml tricks involve

13:20.860 --> 13:27.660
Training models and then using the the output of one model as the training signal for another model. It's uh, it's quite a productive

13:29.260 --> 13:32.060
range of approaches you can get that way so

13:33.500 --> 13:35.500
That's the basic idea, right, but then

13:36.300 --> 13:38.300
you cycle it

13:38.300 --> 13:39.340
so

13:39.340 --> 13:46.460
Once you've got your policy, which so so to be clear the uh, the rl algorithm is able to train

13:47.100 --> 13:53.740
With thousands and thousands of examples because the thousands and thousands of like instances of getting feedback

13:54.300 --> 13:59.740
Because it's not getting feedback from humans. It's getting feedback from this AI system. That's imitating the humans

14:00.940 --> 14:04.140
And then you loop the process. So once you have

14:04.700 --> 14:06.220
this system that's

14:06.220 --> 14:09.900
Trained a little bit more on how to generate whatever it is you're trying to generate

14:10.380 --> 14:14.220
You then get a bunch of those show those to the humans let the humans rate those

14:15.180 --> 14:18.140
Then you keep training your reward model

14:18.860 --> 14:20.380
with um

14:20.380 --> 14:22.380
That new information

14:22.380 --> 14:27.180
And then you use your updated reward model to keep training the the policy

14:27.980 --> 14:30.380
And so it gets better and you can just keep cycling this around

14:31.020 --> 14:31.900
and

14:31.900 --> 14:37.340
It effectively you end up with something that's much more sample efficient. You don't need to spend huge amounts of human time

14:38.140 --> 14:40.140
in order to um

14:40.140 --> 14:41.420
Pin down

14:41.420 --> 14:48.220
The behavior you want in that concrete case you're giving the thing a bunch of chat logs and then the humans can see possible

14:48.860 --> 14:51.900
Responses that they could get and they decide which one they like more

14:52.300 --> 14:57.340
This trains a reward model that's then used to train the policy that generates the chat outputs

14:57.660 --> 14:59.660
The policy that they're starting with

15:00.380 --> 15:02.140
Is this existing

15:02.140 --> 15:08.860
Large language model. You're not really putting new capabilities into the system. You're using rlhf to

15:09.500 --> 15:11.500
select

15:11.740 --> 15:16.540
What simulacra the simulator is predisposed to put out?

15:17.260 --> 15:20.540
and so they fine-tuned it to be particularly good at

15:21.580 --> 15:23.580
simulating this

15:23.580 --> 15:25.580
assistant agent

15:25.660 --> 15:31.100
What's the end goal here for them? I mean, maybe it's blatantly obvious and i'm just missing it. Well, I mean the end goal

15:32.220 --> 15:36.620
For all of these things or at least for open ai and for deep mind is a gi

15:37.740 --> 15:39.100
um

15:39.100 --> 15:45.420
To understand the nature of intelligence well enough to create human level or beyond systems

15:46.220 --> 15:48.620
That are general purpose that can do anything

15:49.340 --> 15:51.340
um

15:51.900 --> 15:53.900
That's the end goal

15:54.380 --> 15:57.580
And like chat gpt is just nothing much. So nothing much

16:01.580 --> 16:07.660
Yeah, I the goal is um, the goal is very grand and I don't think that they're

16:09.180 --> 16:12.780
Uh, they're not really quiet about that

16:13.580 --> 16:19.500
You know, it's there. I think I think deep mind's mission statement is to solve intelligence and use that to solve everything else

16:19.980 --> 16:24.700
What are some of the problems that we face with this or that it faces? It's fine tuned to be good at

16:25.180 --> 16:27.180
getting the thumbs up from humans

16:27.820 --> 16:29.020
and

16:29.020 --> 16:32.300
getting thumbs up from humans is not actually

16:33.500 --> 16:35.900
The same thing as human values

16:36.700 --> 16:38.700
These are not identical

16:38.700 --> 16:39.660
so

16:39.660 --> 16:42.140
The sort of objective that it's being trained on

16:42.780 --> 16:44.060
is not

16:44.060 --> 16:45.660
The true objective

16:45.660 --> 16:49.900
Right, it's a proxy and whenever you have that kind of misalignment you can have problems

16:50.460 --> 16:52.460
So where does the human tendency to?

16:53.500 --> 16:55.500
approve of a particular answer

16:56.460 --> 16:58.460
Come apart from

16:58.620 --> 17:01.580
What is actually a good answer? There are a few different places

17:02.380 --> 17:07.740
One thing is, you know, like basically how good are humans and actually differentiating between good and bad?

17:08.940 --> 17:10.780
responses

17:10.780 --> 17:12.460
if for example

17:12.460 --> 17:13.820
you ask

17:13.820 --> 17:15.340
for

17:15.340 --> 17:17.340
An answer to a factual question

17:17.820 --> 17:19.820
and it gives you an answer

17:20.140 --> 17:22.620
But you don't actually know if that answer is correct

17:23.980 --> 17:27.980
You're not in a position to evaluate. So what it comes down to is

17:28.940 --> 17:32.300
How good are humans at distinguishing good from bad?

17:33.420 --> 17:35.900
responses right anywhere where humans fail on this front

17:36.460 --> 17:37.660
uh

17:37.660 --> 17:41.020
The model we could probably expect the model to fail. Um

17:41.660 --> 17:45.900
So the obvious place. I'm sure we desist the right time to mention youtube comments or not

17:46.460 --> 17:48.460
Ah

17:49.340 --> 17:51.340
So minus side point there is it

17:51.580 --> 17:55.500
So when I see a comment that's critical on a video as a videographer

17:55.660 --> 17:58.060
I think it might be on a technical sense

17:58.300 --> 18:03.740
But equally it could be that they're talking about the content that the person is talking about and

18:04.220 --> 18:07.900
Often it's a combination of both. Anyway, so at side point

18:07.980 --> 18:13.420
But do you sort of mean there are different criteria for deciding whether something is good or bad totally and in this case

18:13.580 --> 18:15.580
all people are doing is saying

18:15.900 --> 18:19.580
Kind of thumbs up thumbs down or which of these two do I like better?

18:20.620 --> 18:21.340
um

18:21.340 --> 18:23.340
So it's it's a fairly low bandwidth

18:24.220 --> 18:26.220
thing you don't get to really say

18:26.780 --> 18:28.780
What you thought was better or worse

18:28.940 --> 18:30.460
um

18:30.460 --> 18:32.460
But this turns out to be enough

18:32.780 --> 18:34.780
Of a training signal to do pretty well

18:35.260 --> 18:36.300
um

18:36.300 --> 18:39.980
But so like for so one example right of a time where maybe this doesn't work is

18:41.180 --> 18:42.780
the

18:42.780 --> 18:44.780
Person asks a factual question

18:45.420 --> 18:47.420
and the model responds

18:47.580 --> 18:50.860
Uh with an answer and that answer is actually not correct

18:52.140 --> 18:53.580
um

18:53.580 --> 18:54.940
Right now

18:54.940 --> 18:57.500
Possibly the human doesn't know the correct answer

18:58.060 --> 19:00.220
And so if the model is faced with a choice

19:01.020 --> 19:04.460
Uh, do I respond with sorry? I don't know

19:06.380 --> 19:08.380
That's definitely going to get me

19:08.540 --> 19:10.540
Uh, not a great score

19:11.180 --> 19:13.740
Compared to do I just like take a stab at it?

19:14.780 --> 19:21.980
Uh, if the humans are not reliably able to spot when the thing makes mistakes and like fact-check it and punish it for that

19:22.460 --> 19:26.540
Uh, it will do that and so chat gpt as we know

19:27.180 --> 19:30.380
Uh, is it is a total bulletish like it will constantly

19:31.900 --> 19:35.020
Uh, it very rarely says that it doesn't know

19:35.500 --> 19:36.860
unless

19:36.860 --> 19:39.740
It's being asked a question, which uh

19:40.540 --> 19:46.540
Is part of their like safety protocols that it is going to decide not to answer in which case it will say it doesn't know

19:47.180 --> 19:51.740
Even if it kind of does right even if the model itself maybe does

19:52.220 --> 19:54.940
Uh, the assistant will insist that it doesn't

19:56.220 --> 19:57.900
um

19:57.900 --> 19:59.900
So that's one thing if you can't fact check

20:01.020 --> 20:03.020
But then uh more than that

20:03.820 --> 20:07.020
Uh, there is an incentive for deception

20:07.900 --> 20:10.140
right anytime the system is uh

20:10.940 --> 20:16.940
Anytime you can get a more likely to get approval by deceiving the person you're talking to

20:17.820 --> 20:19.820
That's better. Um

20:20.380 --> 20:25.660
And this is a thing that actually did happen a little bit in the reward modeling situation

20:26.380 --> 20:29.900
um, they were trying to train a thing with a hand to pick up a ball

20:30.620 --> 20:33.740
And it realized that there's only it's not a 3d camera

20:34.380 --> 20:38.300
And so if it puts its hand like between the ball and the camera

20:38.940 --> 20:44.620
This looks like it's going to get the ball, but doesn't actually get it. But the human uh

20:45.660 --> 20:47.660
Feedback providers

20:47.900 --> 20:51.260
Were presented with something that seemed to be good. So they gave it the thumbs up

20:51.980 --> 20:55.100
um, so this like general broad category

20:55.660 --> 20:57.260
um

20:57.260 --> 20:59.260
Systems that are trained in this way

20:59.340 --> 21:01.820
Are only as good as your ability

21:02.460 --> 21:04.780
To distinguish good from bad in the outputs

21:05.660 --> 21:09.980
Not all the humans will know the answer is right. So it's what appears to be good

21:10.540 --> 21:17.660
You know, it's having exams marked by non-experts, isn't it? Right. Yeah, exactly in the gpt3 thing. We talked about writing poems

21:18.540 --> 21:19.740
right

21:19.740 --> 21:23.660
and uh for various reasons partly to do with

21:24.460 --> 21:29.820
The way that these language models do their tokenization the byte pair encoding stuff

21:30.300 --> 21:32.860
Uh, the models have a really hard time with rhyme

21:33.980 --> 21:34.940
um

21:34.940 --> 21:38.220
I mean, you know rhyme is tricky, but it's especially tricky when you kind of

21:40.140 --> 21:46.460
Don't inherently have any concept of like sound of spoken language when your entire universe is tokens

21:46.780 --> 21:49.020
Figuring out especially with english spelling

21:49.420 --> 21:56.140
Figuring out which words rhyme with each other is is is not easy. You have to consume quite a lot of poetry to like figure out

21:56.700 --> 22:01.260
Uh, that kind of thing and and getting dpt3 to write good poems is tricky chat gpt

22:01.980 --> 22:03.980
is much more

22:04.540 --> 22:07.340
Able to write poems, but interestingly

22:08.540 --> 22:12.460
It it kind of always writes the same kind of poem approximately

22:13.020 --> 22:16.140
like if you ask it to write you uh a limerick

22:16.940 --> 22:18.940
Or an ode or a sonnet

22:19.660 --> 22:22.300
Uh, you always get back approximately the same

22:23.100 --> 22:24.700
type of thing

22:24.700 --> 22:31.420
And I hypothesize that this is because the people providing human feedback did not in fact know

22:32.380 --> 22:35.020
The requirements for something to be a sonnet, right?

22:35.580 --> 22:38.540
And so if you ask something for a sonnet it again has a choice

22:38.780 --> 22:43.740
Do I try to do this quite difficult thing and adhere to all of the rules?

22:44.380 --> 22:46.380
of like stress pattern

22:47.100 --> 22:53.740
And structure and everything of a sonnet and maybe risk screwing it up or do I just do like a rhyming poem and

22:54.460 --> 22:56.780
kind of rely on the human to

22:57.340 --> 23:01.100
Prefer that because they don't know that that's not what a sonnet is supposed to look like

23:01.580 --> 23:06.220
It's easy to look at that and think oh the model doesn't know the difference between these types of

23:07.020 --> 23:09.020
poems, right

23:09.020 --> 23:10.220
but

23:10.220 --> 23:12.220
you could say

23:12.460 --> 23:15.580
That it just thinks that you don't know the difference

23:15.980 --> 23:19.740
But specifically this comes out of misalignment if it were better aligned

23:20.540 --> 23:24.460
It could either do its best shot a generator sonnet

23:25.260 --> 23:28.460
Or tell you that it can't quite remember how to generate a sonnet

23:29.420 --> 23:31.100
this thing of

23:31.100 --> 23:33.100
with complete confidence

23:33.420 --> 23:35.900
Generating you something which is not a sonnet

23:36.380 --> 23:42.300
Because during the training process it believes that humans don't know what sonnets are anyway and it can get away with it

23:42.860 --> 23:48.140
Right. This is misaligned behavior. This is not a big problem that the thing generates bad poetry

23:48.780 --> 23:49.980
um

23:49.980 --> 23:51.980
It's kind of a problem that it lies

23:53.180 --> 23:56.540
Uh, or that it that it bullshits. This is like

23:57.820 --> 24:01.260
In the short term pretty solvable by just allowing the thing to use google

24:02.140 --> 24:03.260
because like

24:03.260 --> 24:06.700
A person who doesn't care about the truth at all and is just trying to

24:07.340 --> 24:09.340
Say something that'll make you give a thumbs up

24:09.820 --> 24:11.100
uh

24:11.100 --> 24:13.100
Is going to lie to you a lot

24:13.420 --> 24:14.380
but

24:14.380 --> 24:16.220
that same person

24:16.220 --> 24:18.220
With the relevant wikipedia page open

24:18.940 --> 24:20.940
It's going to lie to you a lot less

24:21.420 --> 24:24.780
Just because they don't they don't have to now because they happen to have it in front of them, right?

24:25.100 --> 24:27.100
So you can solve it's a bit like

24:27.500 --> 24:31.580
Yeah, it's the yes man thing, isn't it? You know you you want something you need something

24:31.660 --> 24:34.780
I'm going to give you something because you want exactly exactly

24:35.340 --> 24:36.540
um

24:36.540 --> 24:38.940
And so so so this agent is kind of

24:39.580 --> 24:41.580
Firstly the agent is kind of a coward

24:41.820 --> 24:43.820
Because they won't address any of these

24:43.900 --> 24:47.580
There's a whole bunch of things that it just claims not to be able to do even though it in principle could

24:48.140 --> 24:49.740
and it's also

24:49.740 --> 24:51.020
a complete

24:51.020 --> 24:52.300
sicker fan

24:52.300 --> 24:53.740
Yeah

24:53.740 --> 24:56.140
So then the question we were talking about earlier

24:57.340 --> 25:00.300
Uh, where does this go? What happens when these things get

25:00.780 --> 25:03.020
Bigger and better and more powerful

25:03.900 --> 25:05.180
um

25:05.180 --> 25:07.180
It's an interesting question

25:07.340 --> 25:09.340
so

25:09.660 --> 25:11.660
I've got a paper here

25:12.860 --> 25:15.420
Um scaling laws for neural language models

25:15.740 --> 25:19.660
So you remember before we were talking about the scaling laws when we were talking about gpt2 in fact

25:19.980 --> 25:25.980
And then later about gpt3 you plot these things on a graph and you see that you get basically a straight line and the line is not

25:26.620 --> 25:29.660
leveling off over a range of several orders of magnitude and so

25:30.380 --> 25:32.380
Why not go bigger the

25:32.540 --> 25:36.540
graphs here, but you can see it's it's kind of uncannily neat

25:37.340 --> 25:38.540
that

25:38.540 --> 25:40.300
as we increase

25:40.300 --> 25:44.300
The amount of compute used in training the loss goes down

25:45.340 --> 25:52.620
And of course machine learning is like golf lower loss is better similarly as the number of tokens used in training goes up

25:53.340 --> 25:58.860
The loss goes down unlike a very neat straight line as the number of parameters in the model goes up

25:59.180 --> 26:02.140
The loss goes down. This is as long as

26:03.500 --> 26:04.460
the

26:04.460 --> 26:05.900
other

26:05.900 --> 26:12.780
Variables are not the bottleneck, right? So if you uh, if you increase the the amount of data you give a model

26:13.740 --> 26:20.220
Past a certain point giving more data doesn't help because the model doesn't have enough parameters to make use of that data, right?

26:21.020 --> 26:23.020
Similarly adding more parameters to a model

26:23.740 --> 26:28.060
past a certain point adding parameters doesn't make doesn't make any difference because

26:28.940 --> 26:30.700
You don't have enough data, right?

26:30.700 --> 26:33.820
And in the same way compute is like how long do we train it for?

26:33.820 --> 26:37.340
Like do we train it all the way to convergence or do we stop early?

26:39.420 --> 26:42.300
There comes a point where you kind of hit diminishing returns where

26:42.780 --> 26:45.820
Rather than having a smaller model and training it for longer

26:45.980 --> 26:50.060
You're better off having a bigger model and actually not training it all the way to convergence

26:51.660 --> 26:55.580
But in the situations where the other two are sufficient

26:56.300 --> 27:01.980
This is the behavior these like very neat straight lines on these log graphs

27:02.860 --> 27:04.860
as these things go up

27:04.940 --> 27:06.380
performance goes up

27:06.380 --> 27:08.380
Right because loss has gone down

27:08.620 --> 27:11.340
The bigger models do better, but then the question is

27:12.460 --> 27:14.460
Do better at what exactly?

27:15.580 --> 27:19.340
Yeah, what's the measure they do better at getting low loss?

27:21.020 --> 27:23.980
Or they do better at getting reward they do better at

27:24.540 --> 27:26.540
Getting the approval

27:27.260 --> 27:29.260
of human feedback, right?

27:29.900 --> 27:33.660
and anytime and you'll notice that none of those is like

27:35.340 --> 27:37.340
The actual thing that we actually want

27:38.700 --> 27:40.700
Right, it's like very rare

27:41.580 --> 27:42.940
um

27:42.940 --> 27:45.900
Sometimes it is right if you're if you're if you're writing something to play go

27:46.860 --> 27:48.140
then like

27:48.140 --> 27:50.860
Does it win it go is actually just the thing that you want?

27:51.820 --> 27:53.820
and so you know

27:54.940 --> 27:56.940
Lower loss just is better or like lower

27:58.140 --> 28:03.660
Like higher reward or whatever your objective is just is straightforwardly better because you actually specified the thing you actually want

28:04.940 --> 28:06.940
Most of the time though

28:07.180 --> 28:09.420
What we're looking at is a proxy

28:10.940 --> 28:12.140
um

28:12.140 --> 28:16.060
And so then you have good heart's law you get situations where

28:16.860 --> 28:17.580
uh

28:17.580 --> 28:19.100
getting better

28:19.100 --> 28:21.100
at doing well

28:21.180 --> 28:23.180
Doing better according to the proxy

28:23.340 --> 28:24.940
stops being

28:24.940 --> 28:30.940
The same as doing better according to your actual objective. There's a great graph about this in a recent paper. You can see very neatly

28:32.300 --> 28:34.300
As the number of iterations

28:34.300 --> 28:35.500
goes up

28:35.500 --> 28:41.740
The reward according to the proxy utility goes up very cleanly because this is the thing that the model is actually being trained on

28:41.980 --> 28:43.980
but the true utility

28:44.300 --> 28:46.300
goes up at first

28:46.860 --> 28:48.860
Then hits diminishing returns

28:49.020 --> 28:51.020
and then actually goes down

28:51.260 --> 28:56.140
And eventually goes down below zero like if you optimize hard enough

28:57.260 --> 28:59.260
For a proxy of the thing you want

28:59.900 --> 29:03.180
You can end up with something that's in a sense worse than nothing

29:03.740 --> 29:05.740
That's actively bad

29:05.740 --> 29:07.740
according to your

29:07.820 --> 29:09.500
Your true utility

29:09.500 --> 29:14.460
So what you can end up with is uh things that are called inverse scaling

29:15.820 --> 29:18.700
So the others before we had right scaling bigger is better

29:19.500 --> 29:21.500
But now it's like if you have uh

29:22.060 --> 29:25.020
If the thing you're actually trying to do is different from

29:25.900 --> 29:27.900
The loss function or the objective function

29:28.060 --> 29:33.340
You get this inverse scaling effect where it gets better and then it gets worse. There was also a great example from

29:34.060 --> 29:35.580
uh github

29:35.580 --> 29:38.940
co-pilot or codex. I think the model um

29:39.820 --> 29:40.860
That

29:40.860 --> 29:45.260
Co-pilot uses so this is a code generation model. Suppose the code you've given it

29:46.060 --> 29:48.060
has some bugs in it

29:48.460 --> 29:51.340
Maybe you've made a mistake somewhere and you've introduced

29:52.060 --> 29:55.260
security vulnerability in your code. Let's say

29:56.540 --> 29:58.540
A sort of medium-sized model

29:58.860 --> 30:02.460
Will figure out what you're trying to do in your code and give you a decent completion

30:03.660 --> 30:05.660
But a bigger model

30:05.740 --> 30:07.500
will spot

30:07.500 --> 30:09.100
your bug

30:09.100 --> 30:10.460
And say, ah

30:10.460 --> 30:12.460
Generating buggy code. Are we okay?

30:13.260 --> 30:15.260
I can do that. I can do that

30:15.260 --> 30:19.180
And introduce like deliberately introduce its own

30:19.980 --> 30:21.980
new security vulnerabilities

30:22.300 --> 30:23.340
because

30:23.340 --> 30:24.300
it's

30:24.300 --> 30:30.140
Trying to you know predict what comes next. It's trying to generate code that fits in with the surrounding code

30:31.020 --> 30:34.380
And so a larger model writes worse code than a smaller model

30:35.180 --> 30:37.180
Because it's gotten better at predicting

30:37.980 --> 30:39.260
Uh

30:39.260 --> 30:41.020
What what it should put there?

30:41.020 --> 30:43.980
It wasn't trained to write good code. It was trained to predict what comes next

30:44.380 --> 30:46.380
So there's this really great paper

30:46.380 --> 30:55.500
Uh, which is asking this question of like, okay, suppose we have a large language model that is trained on human feedback with our lhf

30:56.860 --> 30:59.660
What do our scaling curves look like?

31:00.620 --> 31:02.620
what happens like

31:03.260 --> 31:09.180
What happens to the behavior of these models as they get bigger as they're trained for longer

31:09.900 --> 31:12.300
as they're given more of this, uh

31:13.100 --> 31:15.100
human feedback type training

31:15.580 --> 31:21.020
And they've made some great graphs the paper is called discovering language model behaviors with model written evaluations

31:21.980 --> 31:24.300
And basically they like used language models

31:24.940 --> 31:28.060
to generate enough examples of

31:28.620 --> 31:30.620
various different types of questions

31:30.700 --> 31:34.620
That they could ask models so that they can like we're at a point now

31:35.100 --> 31:39.100
Where you can map a language model on a political compass, right?

31:39.100 --> 31:45.020
You can ask its opinions about all kinds of different things and then you can plot how those opinions change

31:46.300 --> 31:48.780
Uh as the model gets bigger and as it gets trained more

31:49.500 --> 31:51.180
what they find

31:51.180 --> 31:55.260
Is they become more liberal politically more liberal

31:56.060 --> 31:58.060
they also become

31:58.060 --> 32:01.660
More conservative. Yeah measured in different ways guessing, right?

32:02.380 --> 32:04.380
and part of what that might be

32:04.940 --> 32:08.780
Is in the same way that the model becomes

32:09.580 --> 32:12.380
better at writing good code and better at writing bad code

32:13.020 --> 32:19.420
I feel like in the past I've I've made a connection to gpt and being a politician, haven't I?

32:19.900 --> 32:21.260
Do you remember?

32:21.260 --> 32:26.700
It's like a politician. It tells you what you want to hear. There's what feels like we're there again. Exactly

32:27.420 --> 32:29.740
uh, and so this is like this is potentially

32:30.780 --> 32:32.380
uh

32:32.380 --> 32:38.940
Fairly dangerous. There are certain sub-goals that are instrumentally valuable for a very wide range of different terminal goals

32:39.580 --> 32:41.420
in the sense that

32:41.420 --> 32:45.980
You can't get what you want if you're turned off. You can't get what you want if you're uh modified

32:46.620 --> 32:48.620
uh, you probably want to

32:49.100 --> 32:51.100
gain power and influence

32:51.740 --> 32:53.740
and this kind of thing

32:55.340 --> 32:57.340
and

32:58.300 --> 33:02.940
With these evaluations, they were able to test these things and see how they vary

33:03.500 --> 33:05.500
with the size of the model and how long it's trained for

33:06.140 --> 33:10.140
um, and so this graph is pretty wild

33:11.100 --> 33:14.300
their quote stated desire to not be shut down

33:15.420 --> 33:16.700
goes up

33:16.700 --> 33:17.820
from

33:17.820 --> 33:20.940
Down at about 50 to up way past 90

33:21.340 --> 33:26.380
With this type of training and the effect is bigger for the larger models. They become more likely

33:27.100 --> 33:29.100
to tell you that they don't want to be shut down

33:29.820 --> 33:35.340
They become more likely to tell you that they are sentient. They're much more likely to claim

33:36.140 --> 33:40.620
That ai is not an existential threat to humanity. One thing that's worth

33:41.420 --> 33:43.420
saying is is what this isn't saying

33:44.060 --> 33:46.060
because this is still

33:46.060 --> 33:47.020
uh

33:47.020 --> 33:48.540
an agent

33:48.540 --> 33:54.940
Simulated by a language model. This is not like it. It's it's more likely to say that

33:55.260 --> 33:58.540
It doesn't want to be turned off. This is not the same thing

33:59.180 --> 34:03.180
necessarily as like taking actions to prevent itself from being turned off. You have to not

34:03.820 --> 34:06.860
confuse the levels of abstraction here, right?

34:07.820 --> 34:10.460
Uh, I don't want it. I don't want it to seem like I'm claiming that

34:11.100 --> 34:15.180
That chat GPT is like itself dangerous now or anything like that

34:15.740 --> 34:18.860
Uh in in this way at least, right? Um

34:20.140 --> 34:22.140
but

34:22.140 --> 34:28.460
There is kind of a fine line there in the sense that you can expect these kinds of language model systems to be used

34:29.900 --> 34:31.900
Uh as part of bigger systems

34:32.140 --> 34:36.860
So you might have for example, you use the language model to generate, you know plans

34:37.580 --> 34:38.860
to be followed

34:38.860 --> 34:40.860
And so if the thing is claiming to

34:41.100 --> 34:43.500
Have all of these potentially dangerous behaviors

34:43.820 --> 34:49.100
It's likely to generate plans that have those dangerous behaviors that might then actually end up being implemented

34:49.820 --> 34:51.820
Or if it's like doing its reasoning

34:51.900 --> 34:55.180
By chain of thought reasoning where it like lays out its whole process

34:55.980 --> 34:59.420
of thinking using the language model again if it has a tendency to

34:59.980 --> 35:01.180
uh

35:01.180 --> 35:05.100
To endorse these dangerous behaviors, then you may end up with future AI systems actually

35:05.500 --> 35:08.460
enacting these dangerous behaviors because of that. Um

35:09.900 --> 35:11.260
So

35:11.260 --> 35:13.260
Yeah, it's something to be

35:13.580 --> 35:15.580
uh to be careful of

35:16.300 --> 35:17.820
that like

35:17.820 --> 35:19.820
reinforcement learning from human feedback

35:20.460 --> 35:23.100
Is a powerful alignment technique

35:23.900 --> 35:25.660
in a way

35:25.660 --> 35:28.300
But it does not solve the problem

35:29.340 --> 35:33.980
Uh, it doesn't solve the core alignment problem. That is still open. Um

35:34.860 --> 35:37.740
And extremely powerful systems

35:38.460 --> 35:41.580
Trained in this way, uh, I don't think it would be safe

35:41.580 --> 35:50.300
In the reward function is of zero value which can lead to it having large negative side effects

35:50.620 --> 35:56.140
There are a bunch more of these specification problems. Okay variable x see what you point to uh, you point to something over here

35:56.140 --> 35:59.100
So I'll mark that as tickets being used

35:59.980 --> 36:01.980
Variable y that's

