{"text": " So we looked at clip embeddings and we've talked a lot about using generative AI to produce new sentences to produce new images and so on and so to understand images all these kind of different things and the idea was that if we look at enough pairs of images and text we will learn to distill what it is in an image into that kind of language. So the idea is you have an image, you have some text and you can find a representation where they're both the same. The argument has gone that it's only a matter of time before we have so many images that we train this on and such a big network and all this kind of business that we get this kind of general intelligence or we get some kind of extremely effective AI that works across all domains. That's the implication. The argument is and you see a lot in the sort of tech sector from some of these sort of big tech companies who, to be fair, want to sell products, that if you just keep adding more and more data or bigger and bigger models or a combination of both, ultimately you will move beyond just recognizing cats and you'll be able to do anything. That's the idea. You show enough cats and dogs and eventually the elephant just is implied. As someone who works in science we don't hypothesize about what happens, we experimentally justify it. So I would say if you're going to say to me that the only upward trajectory is going to be amazing, I would say go on and prove it and do it and then we'll see. We'll sit here for a couple of years and we'll see what happens. But in the meantime, let's look at this paper which came out just recently. This paper is saying that that is not true. This paper is saying that the amount of data you will need to get that kind of general zero-shot performance, that is to say performance on new tasks that you've never seen, is going to be astronomically vast to the point where we cannot do it. That's the idea. So it basically is arguing against the idea that we can just add more data and more models and we'll solve it. Now this is only one paper and of course your mileage may vary if you have a bigger GPU than these people and so on. But I think that this is actual numbers which is what I like because I want to see tables of data that show a trend actually happening or not happening. I think that's much more interesting than someone's blog post that says I think this is going to what's going to happen. So let's talk about what this paper does and why it's interesting. We have clip embeddings. So we have an image, we have a big vision transformer and we have a big text encoder which is another transformer, bit like the sort of thing you would see in a large language model which takes text strings, my text string today, and we have some shared embedded space and that embedded space is just a numerical fingerprint for the meaning in these two items and they're trained remember across many, many images such that when you put the same image and the text that describes that image in, you get something in the middle that matches. And the idea then is you can use that for other tasks like you can use that for classification, you can use it for image recall. If you use a streaming service like Spotify or Netflix, they have this thing called a recommender system. A recommender system is where you've watched this program, this program and this program, what should you watch next. And you might have noticed that your mileage may vary on how effective that is but actually I think they're pretty impressive for what they have to do. But you could use this for a recommender system because you could say basically what programs have I got that embed into the same space of all the things I just watched and recommend them that way. So there are downstream tasks like classification and recommendations that we could use based on a system like this. What this paper is showing is that you cannot apply effectively these downstream tasks for difficult problems without massive amounts of data to back it up. And the idea that you can apply this kind of classification on hard things. So not just cats and dogs but specific cats and specific dogs or subspecies of tree. Or difficult problems where the answer is more difficult than just the broad category that there isn't enough data on those things to train these models in an effective way. I've got one of those apps that tells you what specific species a tree is. So what is it not just similar to that? No, because they're just doing classification or some other problem. They're not using this kind of generative giant AI. The argument has been why do that silly little problem where you can do a general problem and solve all your problems. And the response is because it didn't work. That's why we're doing it. So there are pros and cons for both. I'm not going to say that no generative AI is useful or these models are incredibly effective for what they do. But I'm perhaps suggesting that it may not be reasonable to expect them to do very difficult medical diagnosis because you haven't got the data set to back that up. So how does this paper do this? Well what they do is they define these core concepts. So some of the concepts are going to be simple ones like a cat or a person. Some of them are going to be slightly more difficult like a specific species of cat or a specific disease in an image or something like this. And they come up with about 4,000 different concepts. And these are simple text concepts. These are not complicated philosophical ideas. I don't know how well it embeds those. And what they do is they look at the prevalence of these concepts in these data sets. And then they test how well the downstream task of let's say zero shot classification or recall the recommender systems works on all of these different concepts. And they plot that against the amount of data that they had for that specific concept. So let's draw a graph and that will make me make it more clear. So let's imagine we have a graph here like this. And this is the number of examples in our training set of a specific concept. So let's say a cat, a dog, something more difficult. And this is the performance on the actual task of let's say recommender system or recall of an object or the ability to actually classify it as a cat. Remember we talked about how you could use this to zero shot classification by just seeing if it embeds to the same place as a picture of a cat, the text a picture of a cat, that kind of process. So this is performance. The best case scenario if you want to have an all powerful AI that can solve all the world's problems is that this line goes very steeply upwards. This is the exciting case. It goes like this. That's the exciting case. This is the kind of AI explosion argument that basically says we're on the customer something that's about to happen whatever that may be, where the scale is going to be such that this can just do anything. Then there's the perhaps slightly more reasonable, should we say, pragmatic interpretation, which is like just call it balanced, which is that there's a sort of linear movement. So the idea is that we have to add a lot of examples, but we are going to get a decent performance boost from it. So we just keep adding examples, we'll keep getting better, and that's going to be great. And remember that if we ended up up here, we have something that could take any image and tell you exactly what's in it under any circumstance. That's kind of what we're aiming for. And similarly for large language models, this would be something that could write with incredible accuracy on lots of different topics. Or for image generation, it would be something that could take your prompt and generate a photo realistic image of that with almost no coercion at all. That's kind of the goal. This paper has done a lot of experiments on a lot of these concepts across a lot of models, across a lot of downstream tasks. And let's call this the evidence. It's all you're going to call it pessimistic now and then. It is pessimistic also, right? It's logarithmic. So it basically goes like this, right? It flattens out. It flattens out. Now, this is just one paper, right? It doesn't necessarily mean that it will always flatten out. But the argument is, I think, that and it's not an argument they necessarily make in the paper. The paper is very reasonable. I'm being a bit more cavalier with my wording. The suggestion is that you can keep adding more examples. You can keep making your models bigger, but we are soon about to hit a plateau where we don't get any better. And it's costing you millions and millions of dollars to train this. At what point do you go, that's probably about as good as we're going to get the technology, right? And then the argument goes, we need something else. We need something in the transform or some other way of representing data or some other machine learning strategy or some other strategy that's better than this in the long term if we want to have this line go up here or this line go up here. That's kind of the argument. And so this is essentially evidence, I would argue, against the kind of explosion possibility of that just you just add a bit more data when we're on the cusp of something. We might come back here in a couple of years, you know, if you'll still allow me on computer file after this absolute embarrassment of these claims that I made. And we say, actually, the performance has improved massively, right? Or we might say we've doubled the number of data sets to 10 billion images, and we've got 1% more on the classification task, which is good, but is it worth it? I don't know. This is a really interesting paper because it's very, very thorough, right? If there's a lot of evidence, there's a lot of curves, and they all look exactly the same. It doesn't matter what method you use, doesn't matter what data set you train on, it doesn't matter what your downstream task is, the vast majority of them show this kind of problem. And the other problem is that we don't have a nice, even distribution of classes and concepts within our data set. So for example, cats, you can imagine are over-emphasized or over-represented in the data set by an order of magnitude, right? Whereas specific planes or specific trees are incredibly underrepresented because you just have tree, right? So, I mean, trees are probably going to be less represented than cats anyway, but then specific species of tree very, very underrepresented, which is why, when you ask one of these models, what kind of cat is this or what kind of tree is this, it performs worse than when you ask it what animal is this because it's such a much easier problem. And you see the same thing in image generation. If you ask it to draw a picture of something really obvious like a castle where that comes up a lot in the training set, you can draw your fantastic castle in the style of Monet and it can do all this other stuff. But if you ask it to draw some obscure artifact from a video game that barely even made it into the training set, suddenly it's starting to draw something a little bit less quality. And the same with large language models. This paper isn't about large language models, but the same process you can see actually already happening. If you talk to something like chatGPT, when you ask it about a really important topic from physics or something like this, it will usually give you a pretty good explanation of that thing because that's in the training set. But the question is what happens when you ask it about something more difficult, right? When you ask it to write that code, which is actually quite difficult to write, and it starts to make things up, it starts to hallucinate, and it starts to be less accurate. And that is essentially the performance degrading because it's underrepresented in the training set. The argument I think is, at least it's the argument that I'm starting to come around to thinking, if you want performance on hard tasks, tasks that are underrepresented on just general internet texts and searches, we have to find some other way of doing it than just collecting more and more data, particularly because it's incredibly inefficient to do this. On the other hand, these companies will, they've got a lot more GPUs than me. They're going to train on bigger and bigger corpuses, better quality data, they're going to use human feedback to better train their language models and things. So they may find ways to improve this up this way a little bit as we go forward. But it's going to be really interesting to see what happens because I'll, you know, will it plateau out? Will we see chapter GPT seven or eight or nine be roughly the same as chapter GPT four? Or will we see another state of the art performance boost every time? I'm kind of trending this way, but you know, it'll be excited to see if it goes this way. Take a look at this puzzle devised by today's episode sponsor, Jane Strait. It's called bug bite, inspired by debugging code. That world we're all too familiar with, where solving one problem might lead to a whole chain of others. We'll link to the puzzle in the video description. Let me know how you get on. And speaking of Jane Strait, we're also going to link to some programs that they're running at the moment. These events are all expenses paid and give a little taste of the tech and problem solving used at trading firms like Jane Strait. Are you curious? Are you problem solver? Are you into computers? I think maybe you are. If so, well, you may well be eligible to apply for one of these programs. Check out the links below or visit the Jane Strait website and follow these links. There are some deadlines coming up for ones you might want to look at, and there are always more on the horizon. Our thanks to Jane Strait for running great programs like this and also supporting our channel. And don't forget to check out that bug bite puzzle.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 5.36, "text": " So we looked at clip embeddings and we've talked a lot about using generative AI to produce new", "tokens": [50364, 407, 321, 2956, 412, 7353, 12240, 29432, 293, 321, 600, 2825, 257, 688, 466, 1228, 1337, 1166, 7318, 281, 5258, 777, 50632], "temperature": 0.0, "avg_logprob": -0.09638793779456097, "compression_ratio": 1.8171641791044777, "no_speech_prob": 0.00798089150339365}, {"id": 1, "seek": 0, "start": 5.36, "end": 10.16, "text": " sentences to produce new images and so on and so to understand images all these kind of different", "tokens": [50632, 16579, 281, 5258, 777, 5267, 293, 370, 322, 293, 370, 281, 1223, 5267, 439, 613, 733, 295, 819, 50872], "temperature": 0.0, "avg_logprob": -0.09638793779456097, "compression_ratio": 1.8171641791044777, "no_speech_prob": 0.00798089150339365}, {"id": 2, "seek": 0, "start": 10.16, "end": 17.84, "text": " things and the idea was that if we look at enough pairs of images and text we will learn to distill", "tokens": [50872, 721, 293, 264, 1558, 390, 300, 498, 321, 574, 412, 1547, 15494, 295, 5267, 293, 2487, 321, 486, 1466, 281, 42923, 51256], "temperature": 0.0, "avg_logprob": -0.09638793779456097, "compression_ratio": 1.8171641791044777, "no_speech_prob": 0.00798089150339365}, {"id": 3, "seek": 0, "start": 17.84, "end": 22.400000000000002, "text": " what it is in an image into that kind of language. So the idea is you have an image, you have some", "tokens": [51256, 437, 309, 307, 294, 364, 3256, 666, 300, 733, 295, 2856, 13, 407, 264, 1558, 307, 291, 362, 364, 3256, 11, 291, 362, 512, 51484], "temperature": 0.0, "avg_logprob": -0.09638793779456097, "compression_ratio": 1.8171641791044777, "no_speech_prob": 0.00798089150339365}, {"id": 4, "seek": 0, "start": 22.400000000000002, "end": 26.8, "text": " text and you can find a representation where they're both the same. The argument has gone that", "tokens": [51484, 2487, 293, 291, 393, 915, 257, 10290, 689, 436, 434, 1293, 264, 912, 13, 440, 6770, 575, 2780, 300, 51704], "temperature": 0.0, "avg_logprob": -0.09638793779456097, "compression_ratio": 1.8171641791044777, "no_speech_prob": 0.00798089150339365}, {"id": 5, "seek": 2680, "start": 26.8, "end": 31.52, "text": " it's only a matter of time before we have so many images that we train this on and such a big", "tokens": [50364, 309, 311, 787, 257, 1871, 295, 565, 949, 321, 362, 370, 867, 5267, 300, 321, 3847, 341, 322, 293, 1270, 257, 955, 50600], "temperature": 0.0, "avg_logprob": -0.08554342518682065, "compression_ratio": 1.7188612099644127, "no_speech_prob": 0.0015130548272281885}, {"id": 6, "seek": 2680, "start": 31.52, "end": 35.68, "text": " network and all this kind of business that we get this kind of general intelligence or we get some", "tokens": [50600, 3209, 293, 439, 341, 733, 295, 1606, 300, 321, 483, 341, 733, 295, 2674, 7599, 420, 321, 483, 512, 50808], "temperature": 0.0, "avg_logprob": -0.08554342518682065, "compression_ratio": 1.7188612099644127, "no_speech_prob": 0.0015130548272281885}, {"id": 7, "seek": 2680, "start": 35.68, "end": 42.16, "text": " kind of extremely effective AI that works across all domains. That's the implication. The argument", "tokens": [50808, 733, 295, 4664, 4942, 7318, 300, 1985, 2108, 439, 25514, 13, 663, 311, 264, 37814, 13, 440, 6770, 51132], "temperature": 0.0, "avg_logprob": -0.08554342518682065, "compression_ratio": 1.7188612099644127, "no_speech_prob": 0.0015130548272281885}, {"id": 8, "seek": 2680, "start": 42.16, "end": 48.0, "text": " is and you see a lot in the sort of tech sector from some of these sort of big tech companies", "tokens": [51132, 307, 293, 291, 536, 257, 688, 294, 264, 1333, 295, 7553, 6977, 490, 512, 295, 613, 1333, 295, 955, 7553, 3431, 51424], "temperature": 0.0, "avg_logprob": -0.08554342518682065, "compression_ratio": 1.7188612099644127, "no_speech_prob": 0.0015130548272281885}, {"id": 9, "seek": 2680, "start": 48.0, "end": 54.72, "text": " who, to be fair, want to sell products, that if you just keep adding more and more data or bigger", "tokens": [51424, 567, 11, 281, 312, 3143, 11, 528, 281, 3607, 3383, 11, 300, 498, 291, 445, 1066, 5127, 544, 293, 544, 1412, 420, 3801, 51760], "temperature": 0.0, "avg_logprob": -0.08554342518682065, "compression_ratio": 1.7188612099644127, "no_speech_prob": 0.0015130548272281885}, {"id": 10, "seek": 5472, "start": 54.72, "end": 59.839999999999996, "text": " and bigger models or a combination of both, ultimately you will move beyond just recognizing", "tokens": [50364, 293, 3801, 5245, 420, 257, 6562, 295, 1293, 11, 6284, 291, 486, 1286, 4399, 445, 18538, 50620], "temperature": 0.0, "avg_logprob": -0.10215910549821525, "compression_ratio": 1.6428571428571428, "no_speech_prob": 0.0036716600880026817}, {"id": 11, "seek": 5472, "start": 59.839999999999996, "end": 64.24, "text": " cats and you'll be able to do anything. That's the idea. You show enough cats and dogs and eventually", "tokens": [50620, 11111, 293, 291, 603, 312, 1075, 281, 360, 1340, 13, 663, 311, 264, 1558, 13, 509, 855, 1547, 11111, 293, 7197, 293, 4728, 50840], "temperature": 0.0, "avg_logprob": -0.10215910549821525, "compression_ratio": 1.6428571428571428, "no_speech_prob": 0.0036716600880026817}, {"id": 12, "seek": 5472, "start": 64.24, "end": 71.92, "text": " the elephant just is implied. As someone who works in science we don't hypothesize about what happens,", "tokens": [50840, 264, 19791, 445, 307, 32614, 13, 1018, 1580, 567, 1985, 294, 3497, 321, 500, 380, 14276, 1125, 466, 437, 2314, 11, 51224], "temperature": 0.0, "avg_logprob": -0.10215910549821525, "compression_ratio": 1.6428571428571428, "no_speech_prob": 0.0036716600880026817}, {"id": 13, "seek": 5472, "start": 71.92, "end": 78.48, "text": " we experimentally justify it. So I would say if you're going to say to me that the only upward", "tokens": [51224, 321, 5120, 379, 20833, 309, 13, 407, 286, 576, 584, 498, 291, 434, 516, 281, 584, 281, 385, 300, 264, 787, 23452, 51552], "temperature": 0.0, "avg_logprob": -0.10215910549821525, "compression_ratio": 1.6428571428571428, "no_speech_prob": 0.0036716600880026817}, {"id": 14, "seek": 5472, "start": 78.48, "end": 84.64, "text": " trajectory is going to be amazing, I would say go on and prove it and do it and then we'll", "tokens": [51552, 21512, 307, 516, 281, 312, 2243, 11, 286, 576, 584, 352, 322, 293, 7081, 309, 293, 360, 309, 293, 550, 321, 603, 51860], "temperature": 0.0, "avg_logprob": -0.10215910549821525, "compression_ratio": 1.6428571428571428, "no_speech_prob": 0.0036716600880026817}, {"id": 15, "seek": 8464, "start": 84.72, "end": 88.16, "text": " see. We'll sit here for a couple of years and we'll see what happens. But in the meantime,", "tokens": [50368, 536, 13, 492, 603, 1394, 510, 337, 257, 1916, 295, 924, 293, 321, 603, 536, 437, 2314, 13, 583, 294, 264, 14991, 11, 50540], "temperature": 0.0, "avg_logprob": -0.07487419446309408, "compression_ratio": 1.7381818181818183, "no_speech_prob": 0.0009960886090993881}, {"id": 16, "seek": 8464, "start": 88.16, "end": 93.84, "text": " let's look at this paper which came out just recently. This paper is saying that that is not", "tokens": [50540, 718, 311, 574, 412, 341, 3035, 597, 1361, 484, 445, 3938, 13, 639, 3035, 307, 1566, 300, 300, 307, 406, 50824], "temperature": 0.0, "avg_logprob": -0.07487419446309408, "compression_ratio": 1.7381818181818183, "no_speech_prob": 0.0009960886090993881}, {"id": 17, "seek": 8464, "start": 93.84, "end": 99.84, "text": " true. This paper is saying that the amount of data you will need to get that kind of general", "tokens": [50824, 2074, 13, 639, 3035, 307, 1566, 300, 264, 2372, 295, 1412, 291, 486, 643, 281, 483, 300, 733, 295, 2674, 51124], "temperature": 0.0, "avg_logprob": -0.07487419446309408, "compression_ratio": 1.7381818181818183, "no_speech_prob": 0.0009960886090993881}, {"id": 18, "seek": 8464, "start": 99.84, "end": 106.16, "text": " zero-shot performance, that is to say performance on new tasks that you've never seen, is going to be", "tokens": [51124, 4018, 12, 18402, 3389, 11, 300, 307, 281, 584, 3389, 322, 777, 9608, 300, 291, 600, 1128, 1612, 11, 307, 516, 281, 312, 51440], "temperature": 0.0, "avg_logprob": -0.07487419446309408, "compression_ratio": 1.7381818181818183, "no_speech_prob": 0.0009960886090993881}, {"id": 19, "seek": 8464, "start": 106.16, "end": 111.68, "text": " astronomically vast to the point where we cannot do it. That's the idea. So it basically is arguing", "tokens": [51440, 26302, 984, 8369, 281, 264, 935, 689, 321, 2644, 360, 309, 13, 663, 311, 264, 1558, 13, 407, 309, 1936, 307, 19697, 51716], "temperature": 0.0, "avg_logprob": -0.07487419446309408, "compression_ratio": 1.7381818181818183, "no_speech_prob": 0.0009960886090993881}, {"id": 20, "seek": 11168, "start": 111.68, "end": 119.28, "text": " against the idea that we can just add more data and more models and we'll solve it. Now this is", "tokens": [50364, 1970, 264, 1558, 300, 321, 393, 445, 909, 544, 1412, 293, 544, 5245, 293, 321, 603, 5039, 309, 13, 823, 341, 307, 50744], "temperature": 0.0, "avg_logprob": -0.08312057236493645, "compression_ratio": 1.7366548042704626, "no_speech_prob": 0.005039691925048828}, {"id": 21, "seek": 11168, "start": 119.28, "end": 124.24000000000001, "text": " only one paper and of course your mileage may vary if you have a bigger GPU than these people and so", "tokens": [50744, 787, 472, 3035, 293, 295, 1164, 428, 43121, 815, 10559, 498, 291, 362, 257, 3801, 18407, 813, 613, 561, 293, 370, 50992], "temperature": 0.0, "avg_logprob": -0.08312057236493645, "compression_ratio": 1.7366548042704626, "no_speech_prob": 0.005039691925048828}, {"id": 22, "seek": 11168, "start": 124.24000000000001, "end": 129.20000000000002, "text": " on. But I think that this is actual numbers which is what I like because I want to see tables of", "tokens": [50992, 322, 13, 583, 286, 519, 300, 341, 307, 3539, 3547, 597, 307, 437, 286, 411, 570, 286, 528, 281, 536, 8020, 295, 51240], "temperature": 0.0, "avg_logprob": -0.08312057236493645, "compression_ratio": 1.7366548042704626, "no_speech_prob": 0.005039691925048828}, {"id": 23, "seek": 11168, "start": 129.20000000000002, "end": 133.52, "text": " data that show a trend actually happening or not happening. I think that's much more interesting", "tokens": [51240, 1412, 300, 855, 257, 6028, 767, 2737, 420, 406, 2737, 13, 286, 519, 300, 311, 709, 544, 1880, 51456], "temperature": 0.0, "avg_logprob": -0.08312057236493645, "compression_ratio": 1.7366548042704626, "no_speech_prob": 0.005039691925048828}, {"id": 24, "seek": 11168, "start": 133.52, "end": 138.56, "text": " than someone's blog post that says I think this is going to what's going to happen. So let's talk", "tokens": [51456, 813, 1580, 311, 6968, 2183, 300, 1619, 286, 519, 341, 307, 516, 281, 437, 311, 516, 281, 1051, 13, 407, 718, 311, 751, 51708], "temperature": 0.0, "avg_logprob": -0.08312057236493645, "compression_ratio": 1.7366548042704626, "no_speech_prob": 0.005039691925048828}, {"id": 25, "seek": 13856, "start": 138.56, "end": 143.36, "text": " about what this paper does and why it's interesting. We have clip embeddings. So we have an image,", "tokens": [50364, 466, 437, 341, 3035, 775, 293, 983, 309, 311, 1880, 13, 492, 362, 7353, 12240, 29432, 13, 407, 321, 362, 364, 3256, 11, 50604], "temperature": 0.0, "avg_logprob": -0.10118690637441782, "compression_ratio": 1.8451612903225807, "no_speech_prob": 0.002796632470563054}, {"id": 26, "seek": 13856, "start": 143.36, "end": 149.04, "text": " we have a big vision transformer and we have a big text encoder which is another transformer,", "tokens": [50604, 321, 362, 257, 955, 5201, 31782, 293, 321, 362, 257, 955, 2487, 2058, 19866, 597, 307, 1071, 31782, 11, 50888], "temperature": 0.0, "avg_logprob": -0.10118690637441782, "compression_ratio": 1.8451612903225807, "no_speech_prob": 0.002796632470563054}, {"id": 27, "seek": 13856, "start": 149.04, "end": 152.72, "text": " bit like the sort of thing you would see in a large language model which takes text strings,", "tokens": [50888, 857, 411, 264, 1333, 295, 551, 291, 576, 536, 294, 257, 2416, 2856, 2316, 597, 2516, 2487, 13985, 11, 51072], "temperature": 0.0, "avg_logprob": -0.10118690637441782, "compression_ratio": 1.8451612903225807, "no_speech_prob": 0.002796632470563054}, {"id": 28, "seek": 13856, "start": 152.72, "end": 157.76, "text": " my text string today, and we have some shared embedded space and that embedded space is just", "tokens": [51072, 452, 2487, 6798, 965, 11, 293, 321, 362, 512, 5507, 16741, 1901, 293, 300, 16741, 1901, 307, 445, 51324], "temperature": 0.0, "avg_logprob": -0.10118690637441782, "compression_ratio": 1.8451612903225807, "no_speech_prob": 0.002796632470563054}, {"id": 29, "seek": 13856, "start": 157.76, "end": 162.8, "text": " a numerical fingerprint for the meaning in these two items and they're trained remember across many,", "tokens": [51324, 257, 29054, 30715, 337, 264, 3620, 294, 613, 732, 4754, 293, 436, 434, 8895, 1604, 2108, 867, 11, 51576], "temperature": 0.0, "avg_logprob": -0.10118690637441782, "compression_ratio": 1.8451612903225807, "no_speech_prob": 0.002796632470563054}, {"id": 30, "seek": 13856, "start": 162.8, "end": 167.52, "text": " many images such that when you put the same image and the text that describes that image in,", "tokens": [51576, 867, 5267, 1270, 300, 562, 291, 829, 264, 912, 3256, 293, 264, 2487, 300, 15626, 300, 3256, 294, 11, 51812], "temperature": 0.0, "avg_logprob": -0.10118690637441782, "compression_ratio": 1.8451612903225807, "no_speech_prob": 0.002796632470563054}, {"id": 31, "seek": 16752, "start": 167.52, "end": 171.84, "text": " you get something in the middle that matches. And the idea then is you can use that for other tasks", "tokens": [50364, 291, 483, 746, 294, 264, 2808, 300, 10676, 13, 400, 264, 1558, 550, 307, 291, 393, 764, 300, 337, 661, 9608, 50580], "temperature": 0.0, "avg_logprob": -0.09371308927182798, "compression_ratio": 1.9313725490196079, "no_speech_prob": 0.0011360906064510345}, {"id": 32, "seek": 16752, "start": 171.84, "end": 175.84, "text": " like you can use that for classification, you can use it for image recall. If you use a streaming", "tokens": [50580, 411, 291, 393, 764, 300, 337, 21538, 11, 291, 393, 764, 309, 337, 3256, 9901, 13, 759, 291, 764, 257, 11791, 50780], "temperature": 0.0, "avg_logprob": -0.09371308927182798, "compression_ratio": 1.9313725490196079, "no_speech_prob": 0.0011360906064510345}, {"id": 33, "seek": 16752, "start": 175.84, "end": 181.28, "text": " service like Spotify or Netflix, they have this thing called a recommender system. A recommender", "tokens": [50780, 2643, 411, 29036, 420, 12778, 11, 436, 362, 341, 551, 1219, 257, 2748, 260, 1185, 13, 316, 2748, 260, 51052], "temperature": 0.0, "avg_logprob": -0.09371308927182798, "compression_ratio": 1.9313725490196079, "no_speech_prob": 0.0011360906064510345}, {"id": 34, "seek": 16752, "start": 181.28, "end": 185.52, "text": " system is where you've watched this program, this program and this program, what should you watch", "tokens": [51052, 1185, 307, 689, 291, 600, 6337, 341, 1461, 11, 341, 1461, 293, 341, 1461, 11, 437, 820, 291, 1159, 51264], "temperature": 0.0, "avg_logprob": -0.09371308927182798, "compression_ratio": 1.9313725490196079, "no_speech_prob": 0.0011360906064510345}, {"id": 35, "seek": 16752, "start": 185.52, "end": 190.8, "text": " next. And you might have noticed that your mileage may vary on how effective that is but actually I", "tokens": [51264, 958, 13, 400, 291, 1062, 362, 5694, 300, 428, 43121, 815, 10559, 322, 577, 4942, 300, 307, 457, 767, 286, 51528], "temperature": 0.0, "avg_logprob": -0.09371308927182798, "compression_ratio": 1.9313725490196079, "no_speech_prob": 0.0011360906064510345}, {"id": 36, "seek": 16752, "start": 190.8, "end": 195.04000000000002, "text": " think they're pretty impressive for what they have to do. But you could use this for a recommender", "tokens": [51528, 519, 436, 434, 1238, 8992, 337, 437, 436, 362, 281, 360, 13, 583, 291, 727, 764, 341, 337, 257, 2748, 260, 51740], "temperature": 0.0, "avg_logprob": -0.09371308927182798, "compression_ratio": 1.9313725490196079, "no_speech_prob": 0.0011360906064510345}, {"id": 37, "seek": 19504, "start": 195.04, "end": 198.79999999999998, "text": " system because you could say basically what programs have I got that embed into the same", "tokens": [50364, 1185, 570, 291, 727, 584, 1936, 437, 4268, 362, 286, 658, 300, 12240, 666, 264, 912, 50552], "temperature": 0.0, "avg_logprob": -0.07732044160366058, "compression_ratio": 1.7433962264150944, "no_speech_prob": 0.005880364216864109}, {"id": 38, "seek": 19504, "start": 198.79999999999998, "end": 203.68, "text": " space of all the things I just watched and recommend them that way. So there are downstream tasks", "tokens": [50552, 1901, 295, 439, 264, 721, 286, 445, 6337, 293, 2748, 552, 300, 636, 13, 407, 456, 366, 30621, 9608, 50796], "temperature": 0.0, "avg_logprob": -0.07732044160366058, "compression_ratio": 1.7433962264150944, "no_speech_prob": 0.005880364216864109}, {"id": 39, "seek": 19504, "start": 203.68, "end": 207.92, "text": " like classification and recommendations that we could use based on a system like this.", "tokens": [50796, 411, 21538, 293, 10434, 300, 321, 727, 764, 2361, 322, 257, 1185, 411, 341, 13, 51008], "temperature": 0.0, "avg_logprob": -0.07732044160366058, "compression_ratio": 1.7433962264150944, "no_speech_prob": 0.005880364216864109}, {"id": 40, "seek": 19504, "start": 207.92, "end": 214.79999999999998, "text": " What this paper is showing is that you cannot apply effectively these downstream tasks for", "tokens": [51008, 708, 341, 3035, 307, 4099, 307, 300, 291, 2644, 3079, 8659, 613, 30621, 9608, 337, 51352], "temperature": 0.0, "avg_logprob": -0.07732044160366058, "compression_ratio": 1.7433962264150944, "no_speech_prob": 0.005880364216864109}, {"id": 41, "seek": 19504, "start": 214.79999999999998, "end": 221.51999999999998, "text": " difficult problems without massive amounts of data to back it up. And the idea that you can apply", "tokens": [51352, 2252, 2740, 1553, 5994, 11663, 295, 1412, 281, 646, 309, 493, 13, 400, 264, 1558, 300, 291, 393, 3079, 51688], "temperature": 0.0, "avg_logprob": -0.07732044160366058, "compression_ratio": 1.7433962264150944, "no_speech_prob": 0.005880364216864109}, {"id": 42, "seek": 22152, "start": 222.48000000000002, "end": 227.92000000000002, "text": " this kind of classification on hard things. So not just cats and dogs but specific cats and", "tokens": [50412, 341, 733, 295, 21538, 322, 1152, 721, 13, 407, 406, 445, 11111, 293, 7197, 457, 2685, 11111, 293, 50684], "temperature": 0.0, "avg_logprob": -0.11792510146394782, "compression_ratio": 1.7865168539325842, "no_speech_prob": 0.006837840192019939}, {"id": 43, "seek": 22152, "start": 227.92000000000002, "end": 234.56, "text": " specific dogs or subspecies of tree. Or difficult problems where the answer is more difficult than", "tokens": [50684, 2685, 7197, 420, 2090, 494, 4629, 295, 4230, 13, 1610, 2252, 2740, 689, 264, 1867, 307, 544, 2252, 813, 51016], "temperature": 0.0, "avg_logprob": -0.11792510146394782, "compression_ratio": 1.7865168539325842, "no_speech_prob": 0.006837840192019939}, {"id": 44, "seek": 22152, "start": 234.56, "end": 239.60000000000002, "text": " just the broad category that there isn't enough data on those things to train these models in an", "tokens": [51016, 445, 264, 4152, 7719, 300, 456, 1943, 380, 1547, 1412, 322, 729, 721, 281, 3847, 613, 5245, 294, 364, 51268], "temperature": 0.0, "avg_logprob": -0.11792510146394782, "compression_ratio": 1.7865168539325842, "no_speech_prob": 0.006837840192019939}, {"id": 45, "seek": 22152, "start": 239.60000000000002, "end": 244.48000000000002, "text": " effective way. I've got one of those apps that tells you what specific species a tree is. So", "tokens": [51268, 4942, 636, 13, 286, 600, 658, 472, 295, 729, 7733, 300, 5112, 291, 437, 2685, 6172, 257, 4230, 307, 13, 407, 51512], "temperature": 0.0, "avg_logprob": -0.11792510146394782, "compression_ratio": 1.7865168539325842, "no_speech_prob": 0.006837840192019939}, {"id": 46, "seek": 22152, "start": 244.48000000000002, "end": 248.88, "text": " what is it not just similar to that? No, because they're just doing classification or some other", "tokens": [51512, 437, 307, 309, 406, 445, 2531, 281, 300, 30, 883, 11, 570, 436, 434, 445, 884, 21538, 420, 512, 661, 51732], "temperature": 0.0, "avg_logprob": -0.11792510146394782, "compression_ratio": 1.7865168539325842, "no_speech_prob": 0.006837840192019939}, {"id": 47, "seek": 24888, "start": 248.96, "end": 255.84, "text": " problem. They're not using this kind of generative giant AI. The argument has been why do that silly", "tokens": [50368, 1154, 13, 814, 434, 406, 1228, 341, 733, 295, 1337, 1166, 7410, 7318, 13, 440, 6770, 575, 668, 983, 360, 300, 11774, 50712], "temperature": 0.0, "avg_logprob": -0.09787547462864926, "compression_ratio": 1.6455696202531647, "no_speech_prob": 0.0048616547137498856}, {"id": 48, "seek": 24888, "start": 255.84, "end": 260.71999999999997, "text": " little problem where you can do a general problem and solve all your problems. And the response is", "tokens": [50712, 707, 1154, 689, 291, 393, 360, 257, 2674, 1154, 293, 5039, 439, 428, 2740, 13, 400, 264, 4134, 307, 50956], "temperature": 0.0, "avg_logprob": -0.09787547462864926, "compression_ratio": 1.6455696202531647, "no_speech_prob": 0.0048616547137498856}, {"id": 49, "seek": 24888, "start": 260.71999999999997, "end": 268.24, "text": " because it didn't work. That's why we're doing it. So there are pros and cons for both. I'm not", "tokens": [50956, 570, 309, 994, 380, 589, 13, 663, 311, 983, 321, 434, 884, 309, 13, 407, 456, 366, 6267, 293, 1014, 337, 1293, 13, 286, 478, 406, 51332], "temperature": 0.0, "avg_logprob": -0.09787547462864926, "compression_ratio": 1.6455696202531647, "no_speech_prob": 0.0048616547137498856}, {"id": 50, "seek": 24888, "start": 268.24, "end": 272.96, "text": " going to say that no generative AI is useful or these models are incredibly effective for what", "tokens": [51332, 516, 281, 584, 300, 572, 1337, 1166, 7318, 307, 4420, 420, 613, 5245, 366, 6252, 4942, 337, 437, 51568], "temperature": 0.0, "avg_logprob": -0.09787547462864926, "compression_ratio": 1.6455696202531647, "no_speech_prob": 0.0048616547137498856}, {"id": 51, "seek": 27296, "start": 272.96, "end": 279.28, "text": " they do. But I'm perhaps suggesting that it may not be reasonable to expect them to do very difficult", "tokens": [50364, 436, 360, 13, 583, 286, 478, 4317, 18094, 300, 309, 815, 406, 312, 10585, 281, 2066, 552, 281, 360, 588, 2252, 50680], "temperature": 0.0, "avg_logprob": -0.07230541033622546, "compression_ratio": 1.7706093189964158, "no_speech_prob": 0.025761254131793976}, {"id": 52, "seek": 27296, "start": 279.28, "end": 284.71999999999997, "text": " medical diagnosis because you haven't got the data set to back that up. So how does this paper do", "tokens": [50680, 4625, 15217, 570, 291, 2378, 380, 658, 264, 1412, 992, 281, 646, 300, 493, 13, 407, 577, 775, 341, 3035, 360, 50952], "temperature": 0.0, "avg_logprob": -0.07230541033622546, "compression_ratio": 1.7706093189964158, "no_speech_prob": 0.025761254131793976}, {"id": 53, "seek": 27296, "start": 284.71999999999997, "end": 289.35999999999996, "text": " this? Well what they do is they define these core concepts. So some of the concepts are going to be", "tokens": [50952, 341, 30, 1042, 437, 436, 360, 307, 436, 6964, 613, 4965, 10392, 13, 407, 512, 295, 264, 10392, 366, 516, 281, 312, 51184], "temperature": 0.0, "avg_logprob": -0.07230541033622546, "compression_ratio": 1.7706093189964158, "no_speech_prob": 0.025761254131793976}, {"id": 54, "seek": 27296, "start": 289.35999999999996, "end": 293.2, "text": " simple ones like a cat or a person. Some of them are going to be slightly more difficult like a", "tokens": [51184, 2199, 2306, 411, 257, 3857, 420, 257, 954, 13, 2188, 295, 552, 366, 516, 281, 312, 4748, 544, 2252, 411, 257, 51376], "temperature": 0.0, "avg_logprob": -0.07230541033622546, "compression_ratio": 1.7706093189964158, "no_speech_prob": 0.025761254131793976}, {"id": 55, "seek": 27296, "start": 293.2, "end": 299.28, "text": " specific species of cat or a specific disease in an image or something like this. And they come up", "tokens": [51376, 2685, 6172, 295, 3857, 420, 257, 2685, 4752, 294, 364, 3256, 420, 746, 411, 341, 13, 400, 436, 808, 493, 51680], "temperature": 0.0, "avg_logprob": -0.07230541033622546, "compression_ratio": 1.7706093189964158, "no_speech_prob": 0.025761254131793976}, {"id": 56, "seek": 29928, "start": 299.35999999999996, "end": 305.91999999999996, "text": " with about 4,000 different concepts. And these are simple text concepts. These are not complicated", "tokens": [50368, 365, 466, 1017, 11, 1360, 819, 10392, 13, 400, 613, 366, 2199, 2487, 10392, 13, 1981, 366, 406, 6179, 50696], "temperature": 0.0, "avg_logprob": -0.11232465985177577, "compression_ratio": 1.6964285714285714, "no_speech_prob": 0.010951678268611431}, {"id": 57, "seek": 29928, "start": 305.91999999999996, "end": 312.47999999999996, "text": " philosophical ideas. I don't know how well it embeds those. And what they do is they look at", "tokens": [50696, 25066, 3487, 13, 286, 500, 380, 458, 577, 731, 309, 12240, 82, 729, 13, 400, 437, 436, 360, 307, 436, 574, 412, 51024], "temperature": 0.0, "avg_logprob": -0.11232465985177577, "compression_ratio": 1.6964285714285714, "no_speech_prob": 0.010951678268611431}, {"id": 58, "seek": 29928, "start": 312.47999999999996, "end": 320.4, "text": " the prevalence of these concepts in these data sets. And then they test how well the downstream", "tokens": [51024, 264, 42583, 295, 613, 10392, 294, 613, 1412, 6352, 13, 400, 550, 436, 1500, 577, 731, 264, 30621, 51420], "temperature": 0.0, "avg_logprob": -0.11232465985177577, "compression_ratio": 1.6964285714285714, "no_speech_prob": 0.010951678268611431}, {"id": 59, "seek": 29928, "start": 320.4, "end": 326.08, "text": " task of let's say zero shot classification or recall the recommender systems works on all of", "tokens": [51420, 5633, 295, 718, 311, 584, 4018, 3347, 21538, 420, 9901, 264, 2748, 260, 3652, 1985, 322, 439, 295, 51704], "temperature": 0.0, "avg_logprob": -0.11232465985177577, "compression_ratio": 1.6964285714285714, "no_speech_prob": 0.010951678268611431}, {"id": 60, "seek": 32608, "start": 326.08, "end": 331.2, "text": " these different concepts. And they plot that against the amount of data that they had for that", "tokens": [50364, 613, 819, 10392, 13, 400, 436, 7542, 300, 1970, 264, 2372, 295, 1412, 300, 436, 632, 337, 300, 50620], "temperature": 0.0, "avg_logprob": -0.09528301117268015, "compression_ratio": 1.794392523364486, "no_speech_prob": 0.002770810155197978}, {"id": 61, "seek": 32608, "start": 331.2, "end": 335.2, "text": " specific concept. So let's draw a graph and that will make me make it more clear. So let's imagine", "tokens": [50620, 2685, 3410, 13, 407, 718, 311, 2642, 257, 4295, 293, 300, 486, 652, 385, 652, 309, 544, 1850, 13, 407, 718, 311, 3811, 50820], "temperature": 0.0, "avg_logprob": -0.09528301117268015, "compression_ratio": 1.794392523364486, "no_speech_prob": 0.002770810155197978}, {"id": 62, "seek": 32608, "start": 335.2, "end": 344.96, "text": " we have a graph here like this. And this is the number of examples in our training set of a", "tokens": [50820, 321, 362, 257, 4295, 510, 411, 341, 13, 400, 341, 307, 264, 1230, 295, 5110, 294, 527, 3097, 992, 295, 257, 51308], "temperature": 0.0, "avg_logprob": -0.09528301117268015, "compression_ratio": 1.794392523364486, "no_speech_prob": 0.002770810155197978}, {"id": 63, "seek": 32608, "start": 344.96, "end": 351.2, "text": " specific concept. So let's say a cat, a dog, something more difficult. And this is the performance", "tokens": [51308, 2685, 3410, 13, 407, 718, 311, 584, 257, 3857, 11, 257, 3000, 11, 746, 544, 2252, 13, 400, 341, 307, 264, 3389, 51620], "temperature": 0.0, "avg_logprob": -0.09528301117268015, "compression_ratio": 1.794392523364486, "no_speech_prob": 0.002770810155197978}, {"id": 64, "seek": 35120, "start": 351.92, "end": 358.47999999999996, "text": " on the actual task of let's say recommender system or recall of an object or the ability to", "tokens": [50400, 322, 264, 3539, 5633, 295, 718, 311, 584, 2748, 260, 1185, 420, 9901, 295, 364, 2657, 420, 264, 3485, 281, 50728], "temperature": 0.0, "avg_logprob": -0.0891162595178327, "compression_ratio": 1.696113074204947, "no_speech_prob": 0.015514831058681011}, {"id": 65, "seek": 35120, "start": 358.47999999999996, "end": 363.59999999999997, "text": " actually classify it as a cat. Remember we talked about how you could use this to zero shot classification", "tokens": [50728, 767, 33872, 309, 382, 257, 3857, 13, 5459, 321, 2825, 466, 577, 291, 727, 764, 341, 281, 4018, 3347, 21538, 50984], "temperature": 0.0, "avg_logprob": -0.0891162595178327, "compression_ratio": 1.696113074204947, "no_speech_prob": 0.015514831058681011}, {"id": 66, "seek": 35120, "start": 363.59999999999997, "end": 368.24, "text": " by just seeing if it embeds to the same place as a picture of a cat, the text a picture of a cat,", "tokens": [50984, 538, 445, 2577, 498, 309, 12240, 82, 281, 264, 912, 1081, 382, 257, 3036, 295, 257, 3857, 11, 264, 2487, 257, 3036, 295, 257, 3857, 11, 51216], "temperature": 0.0, "avg_logprob": -0.0891162595178327, "compression_ratio": 1.696113074204947, "no_speech_prob": 0.015514831058681011}, {"id": 67, "seek": 35120, "start": 368.24, "end": 374.56, "text": " that kind of process. So this is performance. The best case scenario if you want to have an", "tokens": [51216, 300, 733, 295, 1399, 13, 407, 341, 307, 3389, 13, 440, 1151, 1389, 9005, 498, 291, 528, 281, 362, 364, 51532], "temperature": 0.0, "avg_logprob": -0.0891162595178327, "compression_ratio": 1.696113074204947, "no_speech_prob": 0.015514831058681011}, {"id": 68, "seek": 35120, "start": 374.56, "end": 379.91999999999996, "text": " all powerful AI that can solve all the world's problems is that this line goes very steeply", "tokens": [51532, 439, 4005, 7318, 300, 393, 5039, 439, 264, 1002, 311, 2740, 307, 300, 341, 1622, 1709, 588, 16841, 356, 51800], "temperature": 0.0, "avg_logprob": -0.0891162595178327, "compression_ratio": 1.696113074204947, "no_speech_prob": 0.015514831058681011}, {"id": 69, "seek": 37992, "start": 379.92, "end": 385.68, "text": " upwards. This is the exciting case. It goes like this. That's the exciting case. This is the kind", "tokens": [50364, 22167, 13, 639, 307, 264, 4670, 1389, 13, 467, 1709, 411, 341, 13, 663, 311, 264, 4670, 1389, 13, 639, 307, 264, 733, 50652], "temperature": 0.0, "avg_logprob": -0.12825731260586629, "compression_ratio": 1.7446043165467626, "no_speech_prob": 0.005380375310778618}, {"id": 70, "seek": 37992, "start": 385.68, "end": 390.40000000000003, "text": " of AI explosion argument that basically says we're on the customer something that's about to happen", "tokens": [50652, 295, 7318, 15673, 6770, 300, 1936, 1619, 321, 434, 322, 264, 5474, 746, 300, 311, 466, 281, 1051, 50888], "temperature": 0.0, "avg_logprob": -0.12825731260586629, "compression_ratio": 1.7446043165467626, "no_speech_prob": 0.005380375310778618}, {"id": 71, "seek": 37992, "start": 390.40000000000003, "end": 396.64, "text": " whatever that may be, where the scale is going to be such that this can just do anything. Then", "tokens": [50888, 2035, 300, 815, 312, 11, 689, 264, 4373, 307, 516, 281, 312, 1270, 300, 341, 393, 445, 360, 1340, 13, 1396, 51200], "temperature": 0.0, "avg_logprob": -0.12825731260586629, "compression_ratio": 1.7446043165467626, "no_speech_prob": 0.005380375310778618}, {"id": 72, "seek": 37992, "start": 396.64, "end": 401.6, "text": " there's the perhaps slightly more reasonable, should we say, pragmatic interpretation, which is", "tokens": [51200, 456, 311, 264, 4317, 4748, 544, 10585, 11, 820, 321, 584, 11, 46904, 14174, 11, 597, 307, 51448], "temperature": 0.0, "avg_logprob": -0.12825731260586629, "compression_ratio": 1.7446043165467626, "no_speech_prob": 0.005380375310778618}, {"id": 73, "seek": 37992, "start": 401.6, "end": 407.28000000000003, "text": " like just call it balanced, which is that there's a sort of linear movement. So the idea is that", "tokens": [51448, 411, 445, 818, 309, 13902, 11, 597, 307, 300, 456, 311, 257, 1333, 295, 8213, 3963, 13, 407, 264, 1558, 307, 300, 51732], "temperature": 0.0, "avg_logprob": -0.12825731260586629, "compression_ratio": 1.7446043165467626, "no_speech_prob": 0.005380375310778618}, {"id": 74, "seek": 40728, "start": 407.28, "end": 410.71999999999997, "text": " we have to add a lot of examples, but we are going to get a decent performance boost from it.", "tokens": [50364, 321, 362, 281, 909, 257, 688, 295, 5110, 11, 457, 321, 366, 516, 281, 483, 257, 8681, 3389, 9194, 490, 309, 13, 50536], "temperature": 0.0, "avg_logprob": -0.09858394296545732, "compression_ratio": 1.8342696629213484, "no_speech_prob": 0.0018598142778500915}, {"id": 75, "seek": 40728, "start": 410.71999999999997, "end": 414.32, "text": " So we just keep adding examples, we'll keep getting better, and that's going to be great.", "tokens": [50536, 407, 321, 445, 1066, 5127, 5110, 11, 321, 603, 1066, 1242, 1101, 11, 293, 300, 311, 516, 281, 312, 869, 13, 50716], "temperature": 0.0, "avg_logprob": -0.09858394296545732, "compression_ratio": 1.8342696629213484, "no_speech_prob": 0.0018598142778500915}, {"id": 76, "seek": 40728, "start": 414.32, "end": 418.55999999999995, "text": " And remember that if we ended up up here, we have something that could take any image and", "tokens": [50716, 400, 1604, 300, 498, 321, 4590, 493, 493, 510, 11, 321, 362, 746, 300, 727, 747, 604, 3256, 293, 50928], "temperature": 0.0, "avg_logprob": -0.09858394296545732, "compression_ratio": 1.8342696629213484, "no_speech_prob": 0.0018598142778500915}, {"id": 77, "seek": 40728, "start": 418.55999999999995, "end": 422.96, "text": " tell you exactly what's in it under any circumstance. That's kind of what we're aiming for. And", "tokens": [50928, 980, 291, 2293, 437, 311, 294, 309, 833, 604, 27640, 13, 663, 311, 733, 295, 437, 321, 434, 20253, 337, 13, 400, 51148], "temperature": 0.0, "avg_logprob": -0.09858394296545732, "compression_ratio": 1.8342696629213484, "no_speech_prob": 0.0018598142778500915}, {"id": 78, "seek": 40728, "start": 422.96, "end": 426.23999999999995, "text": " similarly for large language models, this would be something that could write with incredible", "tokens": [51148, 14138, 337, 2416, 2856, 5245, 11, 341, 576, 312, 746, 300, 727, 2464, 365, 4651, 51312], "temperature": 0.0, "avg_logprob": -0.09858394296545732, "compression_ratio": 1.8342696629213484, "no_speech_prob": 0.0018598142778500915}, {"id": 79, "seek": 40728, "start": 426.79999999999995, "end": 431.28, "text": " accuracy on lots of different topics. Or for image generation, it would be something that could", "tokens": [51340, 14170, 322, 3195, 295, 819, 8378, 13, 1610, 337, 3256, 5125, 11, 309, 576, 312, 746, 300, 727, 51564], "temperature": 0.0, "avg_logprob": -0.09858394296545732, "compression_ratio": 1.8342696629213484, "no_speech_prob": 0.0018598142778500915}, {"id": 80, "seek": 40728, "start": 431.28, "end": 436.32, "text": " take your prompt and generate a photo realistic image of that with almost no coercion at all.", "tokens": [51564, 747, 428, 12391, 293, 8460, 257, 5052, 12465, 3256, 295, 300, 365, 1920, 572, 49741, 313, 412, 439, 13, 51816], "temperature": 0.0, "avg_logprob": -0.09858394296545732, "compression_ratio": 1.8342696629213484, "no_speech_prob": 0.0018598142778500915}, {"id": 81, "seek": 43632, "start": 436.32, "end": 440.64, "text": " That's kind of the goal. This paper has done a lot of experiments on a lot of these concepts", "tokens": [50364, 663, 311, 733, 295, 264, 3387, 13, 639, 3035, 575, 1096, 257, 688, 295, 12050, 322, 257, 688, 295, 613, 10392, 50580], "temperature": 0.0, "avg_logprob": -0.13288841766565024, "compression_ratio": 1.8715277777777777, "no_speech_prob": 0.0020431531593203545}, {"id": 82, "seek": 43632, "start": 440.64, "end": 447.2, "text": " across a lot of models, across a lot of downstream tasks. And let's call this the evidence.", "tokens": [50580, 2108, 257, 688, 295, 5245, 11, 2108, 257, 688, 295, 30621, 9608, 13, 400, 718, 311, 818, 341, 264, 4467, 13, 50908], "temperature": 0.0, "avg_logprob": -0.13288841766565024, "compression_ratio": 1.8715277777777777, "no_speech_prob": 0.0020431531593203545}, {"id": 83, "seek": 43632, "start": 447.2, "end": 449.68, "text": " It's all you're going to call it pessimistic now and then.", "tokens": [50908, 467, 311, 439, 291, 434, 516, 281, 818, 309, 37399, 3142, 586, 293, 550, 13, 51032], "temperature": 0.0, "avg_logprob": -0.13288841766565024, "compression_ratio": 1.8715277777777777, "no_speech_prob": 0.0020431531593203545}, {"id": 84, "seek": 43632, "start": 449.68, "end": 454.4, "text": " It is pessimistic also, right? It's logarithmic. So it basically goes like this, right?", "tokens": [51032, 467, 307, 37399, 3142, 611, 11, 558, 30, 467, 311, 41473, 355, 13195, 13, 407, 309, 1936, 1709, 411, 341, 11, 558, 30, 51268], "temperature": 0.0, "avg_logprob": -0.13288841766565024, "compression_ratio": 1.8715277777777777, "no_speech_prob": 0.0020431531593203545}, {"id": 85, "seek": 43632, "start": 454.4, "end": 455.03999999999996, "text": " It flattens out.", "tokens": [51268, 467, 932, 1591, 694, 484, 13, 51300], "temperature": 0.0, "avg_logprob": -0.13288841766565024, "compression_ratio": 1.8715277777777777, "no_speech_prob": 0.0020431531593203545}, {"id": 86, "seek": 43632, "start": 455.03999999999996, "end": 459.6, "text": " It flattens out. Now, this is just one paper, right? It doesn't necessarily mean that it will", "tokens": [51300, 467, 932, 1591, 694, 484, 13, 823, 11, 341, 307, 445, 472, 3035, 11, 558, 30, 467, 1177, 380, 4725, 914, 300, 309, 486, 51528], "temperature": 0.0, "avg_logprob": -0.13288841766565024, "compression_ratio": 1.8715277777777777, "no_speech_prob": 0.0020431531593203545}, {"id": 87, "seek": 43632, "start": 459.6, "end": 465.36, "text": " always flatten out. But the argument is, I think, that and it's not an argument they necessarily", "tokens": [51528, 1009, 24183, 484, 13, 583, 264, 6770, 307, 11, 286, 519, 11, 300, 293, 309, 311, 406, 364, 6770, 436, 4725, 51816], "temperature": 0.0, "avg_logprob": -0.13288841766565024, "compression_ratio": 1.8715277777777777, "no_speech_prob": 0.0020431531593203545}, {"id": 88, "seek": 46536, "start": 465.36, "end": 469.84000000000003, "text": " make in the paper. The paper is very reasonable. I'm being a bit more cavalier with my wording.", "tokens": [50364, 652, 294, 264, 3035, 13, 440, 3035, 307, 588, 10585, 13, 286, 478, 885, 257, 857, 544, 32805, 811, 365, 452, 47602, 13, 50588], "temperature": 0.0, "avg_logprob": -0.0959908582162166, "compression_ratio": 1.7546012269938651, "no_speech_prob": 0.0006529142265208066}, {"id": 89, "seek": 46536, "start": 469.84000000000003, "end": 473.52000000000004, "text": " The suggestion is that you can keep adding more examples. You can keep making your models bigger,", "tokens": [50588, 440, 16541, 307, 300, 291, 393, 1066, 5127, 544, 5110, 13, 509, 393, 1066, 1455, 428, 5245, 3801, 11, 50772], "temperature": 0.0, "avg_logprob": -0.0959908582162166, "compression_ratio": 1.7546012269938651, "no_speech_prob": 0.0006529142265208066}, {"id": 90, "seek": 46536, "start": 473.52000000000004, "end": 479.28000000000003, "text": " but we are soon about to hit a plateau where we don't get any better. And it's costing you millions", "tokens": [50772, 457, 321, 366, 2321, 466, 281, 2045, 257, 39885, 689, 321, 500, 380, 483, 604, 1101, 13, 400, 309, 311, 37917, 291, 6803, 51060], "temperature": 0.0, "avg_logprob": -0.0959908582162166, "compression_ratio": 1.7546012269938651, "no_speech_prob": 0.0006529142265208066}, {"id": 91, "seek": 46536, "start": 479.28000000000003, "end": 482.88, "text": " and millions of dollars to train this. At what point do you go, that's probably about as good as", "tokens": [51060, 293, 6803, 295, 3808, 281, 3847, 341, 13, 1711, 437, 935, 360, 291, 352, 11, 300, 311, 1391, 466, 382, 665, 382, 51240], "temperature": 0.0, "avg_logprob": -0.0959908582162166, "compression_ratio": 1.7546012269938651, "no_speech_prob": 0.0006529142265208066}, {"id": 92, "seek": 46536, "start": 482.88, "end": 487.76, "text": " we're going to get the technology, right? And then the argument goes, we need something else.", "tokens": [51240, 321, 434, 516, 281, 483, 264, 2899, 11, 558, 30, 400, 550, 264, 6770, 1709, 11, 321, 643, 746, 1646, 13, 51484], "temperature": 0.0, "avg_logprob": -0.0959908582162166, "compression_ratio": 1.7546012269938651, "no_speech_prob": 0.0006529142265208066}, {"id": 93, "seek": 46536, "start": 487.76, "end": 492.0, "text": " We need something in the transform or some other way of representing data or some other", "tokens": [51484, 492, 643, 746, 294, 264, 4088, 420, 512, 661, 636, 295, 13460, 1412, 420, 512, 661, 51696], "temperature": 0.0, "avg_logprob": -0.0959908582162166, "compression_ratio": 1.7546012269938651, "no_speech_prob": 0.0006529142265208066}, {"id": 94, "seek": 49200, "start": 492.0, "end": 497.52, "text": " machine learning strategy or some other strategy that's better than this in the long term if we", "tokens": [50364, 3479, 2539, 5206, 420, 512, 661, 5206, 300, 311, 1101, 813, 341, 294, 264, 938, 1433, 498, 321, 50640], "temperature": 0.0, "avg_logprob": -0.12763314454451852, "compression_ratio": 1.711191335740072, "no_speech_prob": 0.00237895711325109}, {"id": 95, "seek": 49200, "start": 497.52, "end": 501.68, "text": " want to have this line go up here or this line go up here. That's kind of the argument.", "tokens": [50640, 528, 281, 362, 341, 1622, 352, 493, 510, 420, 341, 1622, 352, 493, 510, 13, 663, 311, 733, 295, 264, 6770, 13, 50848], "temperature": 0.0, "avg_logprob": -0.12763314454451852, "compression_ratio": 1.711191335740072, "no_speech_prob": 0.00237895711325109}, {"id": 96, "seek": 49200, "start": 501.68, "end": 509.76, "text": " And so this is essentially evidence, I would argue, against the kind of explosion possibility of", "tokens": [50848, 400, 370, 341, 307, 4476, 4467, 11, 286, 576, 9695, 11, 1970, 264, 733, 295, 15673, 7959, 295, 51252], "temperature": 0.0, "avg_logprob": -0.12763314454451852, "compression_ratio": 1.711191335740072, "no_speech_prob": 0.00237895711325109}, {"id": 97, "seek": 49200, "start": 509.76, "end": 513.2, "text": " that just you just add a bit more data when we're on the cusp of something. We might come back here", "tokens": [51252, 300, 445, 291, 445, 909, 257, 857, 544, 1412, 562, 321, 434, 322, 264, 269, 22490, 295, 746, 13, 492, 1062, 808, 646, 510, 51424], "temperature": 0.0, "avg_logprob": -0.12763314454451852, "compression_ratio": 1.711191335740072, "no_speech_prob": 0.00237895711325109}, {"id": 98, "seek": 49200, "start": 513.2, "end": 516.64, "text": " in a couple of years, you know, if you'll still allow me on computer file after this absolute", "tokens": [51424, 294, 257, 1916, 295, 924, 11, 291, 458, 11, 498, 291, 603, 920, 2089, 385, 322, 3820, 3991, 934, 341, 8236, 51596], "temperature": 0.0, "avg_logprob": -0.12763314454451852, "compression_ratio": 1.711191335740072, "no_speech_prob": 0.00237895711325109}, {"id": 99, "seek": 51664, "start": 516.64, "end": 522.24, "text": " embarrassment of these claims that I made. And we say, actually, the performance has improved", "tokens": [50364, 43536, 295, 613, 9441, 300, 286, 1027, 13, 400, 321, 584, 11, 767, 11, 264, 3389, 575, 9689, 50644], "temperature": 0.0, "avg_logprob": -0.1215723011944745, "compression_ratio": 1.7507507507507507, "no_speech_prob": 0.029875539243221283}, {"id": 100, "seek": 51664, "start": 522.24, "end": 526.96, "text": " massively, right? Or we might say we've doubled the number of data sets to 10 billion images,", "tokens": [50644, 29379, 11, 558, 30, 1610, 321, 1062, 584, 321, 600, 24405, 264, 1230, 295, 1412, 6352, 281, 1266, 5218, 5267, 11, 50880], "temperature": 0.0, "avg_logprob": -0.1215723011944745, "compression_ratio": 1.7507507507507507, "no_speech_prob": 0.029875539243221283}, {"id": 101, "seek": 51664, "start": 526.96, "end": 532.88, "text": " and we've got 1% more on the classification task, which is good, but is it worth it? I don't know.", "tokens": [50880, 293, 321, 600, 658, 502, 4, 544, 322, 264, 21538, 5633, 11, 597, 307, 665, 11, 457, 307, 309, 3163, 309, 30, 286, 500, 380, 458, 13, 51176], "temperature": 0.0, "avg_logprob": -0.1215723011944745, "compression_ratio": 1.7507507507507507, "no_speech_prob": 0.029875539243221283}, {"id": 102, "seek": 51664, "start": 532.88, "end": 536.3199999999999, "text": " This is a really interesting paper because it's very, very thorough, right? If there's a lot of", "tokens": [51176, 639, 307, 257, 534, 1880, 3035, 570, 309, 311, 588, 11, 588, 12934, 11, 558, 30, 759, 456, 311, 257, 688, 295, 51348], "temperature": 0.0, "avg_logprob": -0.1215723011944745, "compression_ratio": 1.7507507507507507, "no_speech_prob": 0.029875539243221283}, {"id": 103, "seek": 51664, "start": 536.3199999999999, "end": 540.16, "text": " evidence, there's a lot of curves, and they all look exactly the same. It doesn't matter what method", "tokens": [51348, 4467, 11, 456, 311, 257, 688, 295, 19490, 11, 293, 436, 439, 574, 2293, 264, 912, 13, 467, 1177, 380, 1871, 437, 3170, 51540], "temperature": 0.0, "avg_logprob": -0.1215723011944745, "compression_ratio": 1.7507507507507507, "no_speech_prob": 0.029875539243221283}, {"id": 104, "seek": 51664, "start": 540.16, "end": 544.0, "text": " you use, doesn't matter what data set you train on, it doesn't matter what your downstream task is,", "tokens": [51540, 291, 764, 11, 1177, 380, 1871, 437, 1412, 992, 291, 3847, 322, 11, 309, 1177, 380, 1871, 437, 428, 30621, 5633, 307, 11, 51732], "temperature": 0.0, "avg_logprob": -0.1215723011944745, "compression_ratio": 1.7507507507507507, "no_speech_prob": 0.029875539243221283}, {"id": 105, "seek": 54400, "start": 544.0, "end": 548.32, "text": " the vast majority of them show this kind of problem. And the other problem is that we don't", "tokens": [50364, 264, 8369, 6286, 295, 552, 855, 341, 733, 295, 1154, 13, 400, 264, 661, 1154, 307, 300, 321, 500, 380, 50580], "temperature": 0.0, "avg_logprob": -0.13355506852615712, "compression_ratio": 1.5726495726495726, "no_speech_prob": 0.0009682725067250431}, {"id": 106, "seek": 54400, "start": 548.32, "end": 555.28, "text": " have a nice, even distribution of classes and concepts within our data set. So for example,", "tokens": [50580, 362, 257, 1481, 11, 754, 7316, 295, 5359, 293, 10392, 1951, 527, 1412, 992, 13, 407, 337, 1365, 11, 50928], "temperature": 0.0, "avg_logprob": -0.13355506852615712, "compression_ratio": 1.5726495726495726, "no_speech_prob": 0.0009682725067250431}, {"id": 107, "seek": 54400, "start": 555.28, "end": 564.88, "text": " cats, you can imagine are over-emphasized or over-represented in the data set by an order", "tokens": [50928, 11111, 11, 291, 393, 3811, 366, 670, 12, 443, 7485, 1602, 420, 670, 12, 38293, 294, 264, 1412, 992, 538, 364, 1668, 51408], "temperature": 0.0, "avg_logprob": -0.13355506852615712, "compression_ratio": 1.5726495726495726, "no_speech_prob": 0.0009682725067250431}, {"id": 108, "seek": 54400, "start": 564.88, "end": 571.12, "text": " of magnitude, right? Whereas specific planes or specific trees are incredibly underrepresented", "tokens": [51408, 295, 15668, 11, 558, 30, 13813, 2685, 14952, 420, 2685, 5852, 366, 6252, 833, 38293, 51720], "temperature": 0.0, "avg_logprob": -0.13355506852615712, "compression_ratio": 1.5726495726495726, "no_speech_prob": 0.0009682725067250431}, {"id": 109, "seek": 57112, "start": 571.12, "end": 575.84, "text": " because you just have tree, right? So, I mean, trees are probably going to be less represented", "tokens": [50364, 570, 291, 445, 362, 4230, 11, 558, 30, 407, 11, 286, 914, 11, 5852, 366, 1391, 516, 281, 312, 1570, 10379, 50600], "temperature": 0.0, "avg_logprob": -0.11837505093581385, "compression_ratio": 1.796875, "no_speech_prob": 0.003073684638366103}, {"id": 110, "seek": 57112, "start": 575.84, "end": 580.72, "text": " than cats anyway, but then specific species of tree very, very underrepresented, which is why,", "tokens": [50600, 813, 11111, 4033, 11, 457, 550, 2685, 6172, 295, 4230, 588, 11, 588, 833, 38293, 11, 597, 307, 983, 11, 50844], "temperature": 0.0, "avg_logprob": -0.11837505093581385, "compression_ratio": 1.796875, "no_speech_prob": 0.003073684638366103}, {"id": 111, "seek": 57112, "start": 580.72, "end": 584.88, "text": " when you ask one of these models, what kind of cat is this or what kind of tree is this,", "tokens": [50844, 562, 291, 1029, 472, 295, 613, 5245, 11, 437, 733, 295, 3857, 307, 341, 420, 437, 733, 295, 4230, 307, 341, 11, 51052], "temperature": 0.0, "avg_logprob": -0.11837505093581385, "compression_ratio": 1.796875, "no_speech_prob": 0.003073684638366103}, {"id": 112, "seek": 57112, "start": 584.88, "end": 590.0, "text": " it performs worse than when you ask it what animal is this because it's such a much easier problem.", "tokens": [51052, 309, 26213, 5324, 813, 562, 291, 1029, 309, 437, 5496, 307, 341, 570, 309, 311, 1270, 257, 709, 3571, 1154, 13, 51308], "temperature": 0.0, "avg_logprob": -0.11837505093581385, "compression_ratio": 1.796875, "no_speech_prob": 0.003073684638366103}, {"id": 113, "seek": 57112, "start": 590.0, "end": 594.5600000000001, "text": " And you see the same thing in image generation. If you ask it to draw a picture of something really", "tokens": [51308, 400, 291, 536, 264, 912, 551, 294, 3256, 5125, 13, 759, 291, 1029, 309, 281, 2642, 257, 3036, 295, 746, 534, 51536], "temperature": 0.0, "avg_logprob": -0.11837505093581385, "compression_ratio": 1.796875, "no_speech_prob": 0.003073684638366103}, {"id": 114, "seek": 57112, "start": 594.5600000000001, "end": 599.76, "text": " obvious like a castle where that comes up a lot in the training set, you can draw your fantastic", "tokens": [51536, 6322, 411, 257, 14114, 689, 300, 1487, 493, 257, 688, 294, 264, 3097, 992, 11, 291, 393, 2642, 428, 5456, 51796], "temperature": 0.0, "avg_logprob": -0.11837505093581385, "compression_ratio": 1.796875, "no_speech_prob": 0.003073684638366103}, {"id": 115, "seek": 59976, "start": 599.76, "end": 604.64, "text": " castle in the style of Monet and it can do all this other stuff. But if you ask it to draw some", "tokens": [50364, 14114, 294, 264, 3758, 295, 47871, 293, 309, 393, 360, 439, 341, 661, 1507, 13, 583, 498, 291, 1029, 309, 281, 2642, 512, 50608], "temperature": 0.0, "avg_logprob": -0.08112855391068892, "compression_ratio": 1.7349397590361446, "no_speech_prob": 0.0021908809430897236}, {"id": 116, "seek": 59976, "start": 604.64, "end": 609.76, "text": " obscure artifact from a video game that barely even made it into the training set, suddenly it's", "tokens": [50608, 34443, 34806, 490, 257, 960, 1216, 300, 10268, 754, 1027, 309, 666, 264, 3097, 992, 11, 5800, 309, 311, 50864], "temperature": 0.0, "avg_logprob": -0.08112855391068892, "compression_ratio": 1.7349397590361446, "no_speech_prob": 0.0021908809430897236}, {"id": 117, "seek": 59976, "start": 609.76, "end": 614.3199999999999, "text": " starting to draw something a little bit less quality. And the same with large language models.", "tokens": [50864, 2891, 281, 2642, 746, 257, 707, 857, 1570, 3125, 13, 400, 264, 912, 365, 2416, 2856, 5245, 13, 51092], "temperature": 0.0, "avg_logprob": -0.08112855391068892, "compression_ratio": 1.7349397590361446, "no_speech_prob": 0.0021908809430897236}, {"id": 118, "seek": 59976, "start": 614.3199999999999, "end": 617.92, "text": " This paper isn't about large language models, but the same process you can see actually already", "tokens": [51092, 639, 3035, 1943, 380, 466, 2416, 2856, 5245, 11, 457, 264, 912, 1399, 291, 393, 536, 767, 1217, 51272], "temperature": 0.0, "avg_logprob": -0.08112855391068892, "compression_ratio": 1.7349397590361446, "no_speech_prob": 0.0021908809430897236}, {"id": 119, "seek": 59976, "start": 617.92, "end": 624.4, "text": " happening. If you talk to something like chatGPT, when you ask it about a really important topic", "tokens": [51272, 2737, 13, 759, 291, 751, 281, 746, 411, 5081, 38, 47, 51, 11, 562, 291, 1029, 309, 466, 257, 534, 1021, 4829, 51596], "temperature": 0.0, "avg_logprob": -0.08112855391068892, "compression_ratio": 1.7349397590361446, "no_speech_prob": 0.0021908809430897236}, {"id": 120, "seek": 59976, "start": 624.4, "end": 628.3199999999999, "text": " from physics or something like this, it will usually give you a pretty good explanation of that", "tokens": [51596, 490, 10649, 420, 746, 411, 341, 11, 309, 486, 2673, 976, 291, 257, 1238, 665, 10835, 295, 300, 51792], "temperature": 0.0, "avg_logprob": -0.08112855391068892, "compression_ratio": 1.7349397590361446, "no_speech_prob": 0.0021908809430897236}, {"id": 121, "seek": 62832, "start": 628.32, "end": 632.48, "text": " thing because that's in the training set. But the question is what happens when you ask it about", "tokens": [50364, 551, 570, 300, 311, 294, 264, 3097, 992, 13, 583, 264, 1168, 307, 437, 2314, 562, 291, 1029, 309, 466, 50572], "temperature": 0.0, "avg_logprob": -0.08172521306507623, "compression_ratio": 1.9761904761904763, "no_speech_prob": 0.010861868970096111}, {"id": 122, "seek": 62832, "start": 632.48, "end": 636.4000000000001, "text": " something more difficult, right? When you ask it to write that code, which is actually quite difficult", "tokens": [50572, 746, 544, 2252, 11, 558, 30, 1133, 291, 1029, 309, 281, 2464, 300, 3089, 11, 597, 307, 767, 1596, 2252, 50768], "temperature": 0.0, "avg_logprob": -0.08172521306507623, "compression_ratio": 1.9761904761904763, "no_speech_prob": 0.010861868970096111}, {"id": 123, "seek": 62832, "start": 636.4000000000001, "end": 641.36, "text": " to write, and it starts to make things up, it starts to hallucinate, and it starts to be less", "tokens": [50768, 281, 2464, 11, 293, 309, 3719, 281, 652, 721, 493, 11, 309, 3719, 281, 35212, 13923, 11, 293, 309, 3719, 281, 312, 1570, 51016], "temperature": 0.0, "avg_logprob": -0.08172521306507623, "compression_ratio": 1.9761904761904763, "no_speech_prob": 0.010861868970096111}, {"id": 124, "seek": 62832, "start": 641.36, "end": 645.5200000000001, "text": " accurate. And that is essentially the performance degrading because it's underrepresented in the", "tokens": [51016, 8559, 13, 400, 300, 307, 4476, 264, 3389, 24740, 278, 570, 309, 311, 833, 38293, 294, 264, 51224], "temperature": 0.0, "avg_logprob": -0.08172521306507623, "compression_ratio": 1.9761904761904763, "no_speech_prob": 0.010861868970096111}, {"id": 125, "seek": 62832, "start": 645.5200000000001, "end": 650.32, "text": " training set. The argument I think is, at least it's the argument that I'm starting to come around", "tokens": [51224, 3097, 992, 13, 440, 6770, 286, 519, 307, 11, 412, 1935, 309, 311, 264, 6770, 300, 286, 478, 2891, 281, 808, 926, 51464], "temperature": 0.0, "avg_logprob": -0.08172521306507623, "compression_ratio": 1.9761904761904763, "no_speech_prob": 0.010861868970096111}, {"id": 126, "seek": 62832, "start": 650.32, "end": 654.72, "text": " to thinking, if you want performance on hard tasks, tasks that are underrepresented on just", "tokens": [51464, 281, 1953, 11, 498, 291, 528, 3389, 322, 1152, 9608, 11, 9608, 300, 366, 833, 38293, 322, 445, 51684], "temperature": 0.0, "avg_logprob": -0.08172521306507623, "compression_ratio": 1.9761904761904763, "no_speech_prob": 0.010861868970096111}, {"id": 127, "seek": 65472, "start": 654.72, "end": 659.76, "text": " general internet texts and searches, we have to find some other way of doing it than just collecting", "tokens": [50364, 2674, 4705, 15765, 293, 26701, 11, 321, 362, 281, 915, 512, 661, 636, 295, 884, 309, 813, 445, 12510, 50616], "temperature": 0.0, "avg_logprob": -0.11091005802154541, "compression_ratio": 1.6807017543859648, "no_speech_prob": 0.007739247288554907}, {"id": 128, "seek": 65472, "start": 659.76, "end": 664.64, "text": " more and more data, particularly because it's incredibly inefficient to do this. On the other", "tokens": [50616, 544, 293, 544, 1412, 11, 4098, 570, 309, 311, 6252, 43495, 281, 360, 341, 13, 1282, 264, 661, 50860], "temperature": 0.0, "avg_logprob": -0.11091005802154541, "compression_ratio": 1.6807017543859648, "no_speech_prob": 0.007739247288554907}, {"id": 129, "seek": 65472, "start": 664.64, "end": 670.8000000000001, "text": " hand, these companies will, they've got a lot more GPUs than me. They're going to train on", "tokens": [50860, 1011, 11, 613, 3431, 486, 11, 436, 600, 658, 257, 688, 544, 18407, 82, 813, 385, 13, 814, 434, 516, 281, 3847, 322, 51168], "temperature": 0.0, "avg_logprob": -0.11091005802154541, "compression_ratio": 1.6807017543859648, "no_speech_prob": 0.007739247288554907}, {"id": 130, "seek": 65472, "start": 670.8000000000001, "end": 674.96, "text": " bigger and bigger corpuses, better quality data, they're going to use human feedback to better", "tokens": [51168, 3801, 293, 3801, 1181, 79, 8355, 11, 1101, 3125, 1412, 11, 436, 434, 516, 281, 764, 1952, 5824, 281, 1101, 51376], "temperature": 0.0, "avg_logprob": -0.11091005802154541, "compression_ratio": 1.6807017543859648, "no_speech_prob": 0.007739247288554907}, {"id": 131, "seek": 65472, "start": 674.96, "end": 680.24, "text": " train their language models and things. So they may find ways to improve this up this way a little", "tokens": [51376, 3847, 641, 2856, 5245, 293, 721, 13, 407, 436, 815, 915, 2098, 281, 3470, 341, 493, 341, 636, 257, 707, 51640], "temperature": 0.0, "avg_logprob": -0.11091005802154541, "compression_ratio": 1.6807017543859648, "no_speech_prob": 0.007739247288554907}, {"id": 132, "seek": 68024, "start": 680.24, "end": 684.64, "text": " bit as we go forward. But it's going to be really interesting to see what happens because I'll,", "tokens": [50364, 857, 382, 321, 352, 2128, 13, 583, 309, 311, 516, 281, 312, 534, 1880, 281, 536, 437, 2314, 570, 286, 603, 11, 50584], "temperature": 0.0, "avg_logprob": -0.13504327833652496, "compression_ratio": 1.619672131147541, "no_speech_prob": 0.03303982689976692}, {"id": 133, "seek": 68024, "start": 684.64, "end": 690.08, "text": " you know, will it plateau out? Will we see chapter GPT seven or eight or nine be roughly the same as", "tokens": [50584, 291, 458, 11, 486, 309, 39885, 484, 30, 3099, 321, 536, 7187, 26039, 51, 3407, 420, 3180, 420, 4949, 312, 9810, 264, 912, 382, 50856], "temperature": 0.0, "avg_logprob": -0.13504327833652496, "compression_ratio": 1.619672131147541, "no_speech_prob": 0.03303982689976692}, {"id": 134, "seek": 68024, "start": 690.08, "end": 694.96, "text": " chapter GPT four? Or will we see another state of the art performance boost every time? I'm kind of", "tokens": [50856, 7187, 26039, 51, 1451, 30, 1610, 486, 321, 536, 1071, 1785, 295, 264, 1523, 3389, 9194, 633, 565, 30, 286, 478, 733, 295, 51100], "temperature": 0.0, "avg_logprob": -0.13504327833652496, "compression_ratio": 1.619672131147541, "no_speech_prob": 0.03303982689976692}, {"id": 135, "seek": 68024, "start": 694.96, "end": 699.76, "text": " trending this way, but you know, it'll be excited to see if it goes this way. Take a look at this", "tokens": [51100, 28692, 341, 636, 11, 457, 291, 458, 11, 309, 603, 312, 2919, 281, 536, 498, 309, 1709, 341, 636, 13, 3664, 257, 574, 412, 341, 51340], "temperature": 0.0, "avg_logprob": -0.13504327833652496, "compression_ratio": 1.619672131147541, "no_speech_prob": 0.03303982689976692}, {"id": 136, "seek": 68024, "start": 699.76, "end": 707.84, "text": " puzzle devised by today's episode sponsor, Jane Strait. It's called bug bite, inspired by debugging", "tokens": [51340, 12805, 1905, 2640, 538, 965, 311, 3500, 16198, 11, 13048, 745, 8645, 13, 467, 311, 1219, 7426, 7988, 11, 7547, 538, 45592, 51744], "temperature": 0.0, "avg_logprob": -0.13504327833652496, "compression_ratio": 1.619672131147541, "no_speech_prob": 0.03303982689976692}, {"id": 137, "seek": 70784, "start": 707.84, "end": 713.2, "text": " code. That world we're all too familiar with, where solving one problem might lead to a whole", "tokens": [50364, 3089, 13, 663, 1002, 321, 434, 439, 886, 4963, 365, 11, 689, 12606, 472, 1154, 1062, 1477, 281, 257, 1379, 50632], "temperature": 0.0, "avg_logprob": -0.07754628709022035, "compression_ratio": 1.5860655737704918, "no_speech_prob": 0.03408410772681236}, {"id": 138, "seek": 70784, "start": 713.2, "end": 720.0, "text": " chain of others. We'll link to the puzzle in the video description. Let me know how you get on.", "tokens": [50632, 5021, 295, 2357, 13, 492, 603, 2113, 281, 264, 12805, 294, 264, 960, 3855, 13, 961, 385, 458, 577, 291, 483, 322, 13, 50972], "temperature": 0.0, "avg_logprob": -0.07754628709022035, "compression_ratio": 1.5860655737704918, "no_speech_prob": 0.03408410772681236}, {"id": 139, "seek": 70784, "start": 720.0, "end": 724.24, "text": " And speaking of Jane Strait, we're also going to link to some programs that they're running at the", "tokens": [50972, 400, 4124, 295, 13048, 745, 8645, 11, 321, 434, 611, 516, 281, 2113, 281, 512, 4268, 300, 436, 434, 2614, 412, 264, 51184], "temperature": 0.0, "avg_logprob": -0.07754628709022035, "compression_ratio": 1.5860655737704918, "no_speech_prob": 0.03408410772681236}, {"id": 140, "seek": 70784, "start": 724.24, "end": 730.1600000000001, "text": " moment. These events are all expenses paid and give a little taste of the tech and problem solving", "tokens": [51184, 1623, 13, 1981, 3931, 366, 439, 15506, 4835, 293, 976, 257, 707, 3939, 295, 264, 7553, 293, 1154, 12606, 51480], "temperature": 0.0, "avg_logprob": -0.07754628709022035, "compression_ratio": 1.5860655737704918, "no_speech_prob": 0.03408410772681236}, {"id": 141, "seek": 73016, "start": 730.16, "end": 739.12, "text": " used at trading firms like Jane Strait. Are you curious? Are you problem solver? Are you into", "tokens": [50364, 1143, 412, 9529, 18055, 411, 13048, 745, 8645, 13, 2014, 291, 6369, 30, 2014, 291, 1154, 1404, 331, 30, 2014, 291, 666, 50812], "temperature": 0.0, "avg_logprob": -0.06188020706176758, "compression_ratio": 1.7179487179487178, "no_speech_prob": 0.3240841329097748}, {"id": 142, "seek": 73016, "start": 739.12, "end": 744.8, "text": " computers? I think maybe you are. If so, well, you may well be eligible to apply for one of these", "tokens": [50812, 10807, 30, 286, 519, 1310, 291, 366, 13, 759, 370, 11, 731, 11, 291, 815, 731, 312, 14728, 281, 3079, 337, 472, 295, 613, 51096], "temperature": 0.0, "avg_logprob": -0.06188020706176758, "compression_ratio": 1.7179487179487178, "no_speech_prob": 0.3240841329097748}, {"id": 143, "seek": 73016, "start": 744.8, "end": 749.68, "text": " programs. Check out the links below or visit the Jane Strait website and follow these links.", "tokens": [51096, 4268, 13, 6881, 484, 264, 6123, 2507, 420, 3441, 264, 13048, 745, 8645, 3144, 293, 1524, 613, 6123, 13, 51340], "temperature": 0.0, "avg_logprob": -0.06188020706176758, "compression_ratio": 1.7179487179487178, "no_speech_prob": 0.3240841329097748}, {"id": 144, "seek": 73016, "start": 750.7199999999999, "end": 754.0799999999999, "text": " There are some deadlines coming up for ones you might want to look at, and there are always", "tokens": [51392, 821, 366, 512, 37548, 1348, 493, 337, 2306, 291, 1062, 528, 281, 574, 412, 11, 293, 456, 366, 1009, 51560], "temperature": 0.0, "avg_logprob": -0.06188020706176758, "compression_ratio": 1.7179487179487178, "no_speech_prob": 0.3240841329097748}, {"id": 145, "seek": 73016, "start": 754.0799999999999, "end": 758.8, "text": " more on the horizon. Our thanks to Jane Strait for running great programs like this and also", "tokens": [51560, 544, 322, 264, 18046, 13, 2621, 3231, 281, 13048, 745, 8645, 337, 2614, 869, 4268, 411, 341, 293, 611, 51796], "temperature": 0.0, "avg_logprob": -0.06188020706176758, "compression_ratio": 1.7179487179487178, "no_speech_prob": 0.3240841329097748}, {"id": 146, "seek": 75880, "start": 758.8, "end": 767.3599999999999, "text": " supporting our channel. And don't forget to check out that bug bite puzzle.", "tokens": [50364, 7231, 527, 2269, 13, 400, 500, 380, 2870, 281, 1520, 484, 300, 7426, 7988, 12805, 13, 50792], "temperature": 0.0, "avg_logprob": -0.2777566156889263, "compression_ratio": 1.0, "no_speech_prob": 0.5259168148040771}], "language": "en"}