WEBVTT

00:00.000 --> 00:05.360
So we looked at clip embeddings and we've talked a lot about using generative AI to produce new

00:05.360 --> 00:10.160
sentences to produce new images and so on and so to understand images all these kind of different

00:10.160 --> 00:17.840
things and the idea was that if we look at enough pairs of images and text we will learn to distill

00:17.840 --> 00:22.400
what it is in an image into that kind of language. So the idea is you have an image, you have some

00:22.400 --> 00:26.800
text and you can find a representation where they're both the same. The argument has gone that

00:26.800 --> 00:31.520
it's only a matter of time before we have so many images that we train this on and such a big

00:31.520 --> 00:35.680
network and all this kind of business that we get this kind of general intelligence or we get some

00:35.680 --> 00:42.160
kind of extremely effective AI that works across all domains. That's the implication. The argument

00:42.160 --> 00:48.000
is and you see a lot in the sort of tech sector from some of these sort of big tech companies

00:48.000 --> 00:54.720
who, to be fair, want to sell products, that if you just keep adding more and more data or bigger

00:54.720 --> 00:59.840
and bigger models or a combination of both, ultimately you will move beyond just recognizing

00:59.840 --> 01:04.240
cats and you'll be able to do anything. That's the idea. You show enough cats and dogs and eventually

01:04.240 --> 01:11.920
the elephant just is implied. As someone who works in science we don't hypothesize about what happens,

01:11.920 --> 01:18.480
we experimentally justify it. So I would say if you're going to say to me that the only upward

01:18.480 --> 01:24.640
trajectory is going to be amazing, I would say go on and prove it and do it and then we'll

01:24.720 --> 01:28.160
see. We'll sit here for a couple of years and we'll see what happens. But in the meantime,

01:28.160 --> 01:33.840
let's look at this paper which came out just recently. This paper is saying that that is not

01:33.840 --> 01:39.840
true. This paper is saying that the amount of data you will need to get that kind of general

01:39.840 --> 01:46.160
zero-shot performance, that is to say performance on new tasks that you've never seen, is going to be

01:46.160 --> 01:51.680
astronomically vast to the point where we cannot do it. That's the idea. So it basically is arguing

01:51.680 --> 01:59.280
against the idea that we can just add more data and more models and we'll solve it. Now this is

01:59.280 --> 02:04.240
only one paper and of course your mileage may vary if you have a bigger GPU than these people and so

02:04.240 --> 02:09.200
on. But I think that this is actual numbers which is what I like because I want to see tables of

02:09.200 --> 02:13.520
data that show a trend actually happening or not happening. I think that's much more interesting

02:13.520 --> 02:18.560
than someone's blog post that says I think this is going to what's going to happen. So let's talk

02:18.560 --> 02:23.360
about what this paper does and why it's interesting. We have clip embeddings. So we have an image,

02:23.360 --> 02:29.040
we have a big vision transformer and we have a big text encoder which is another transformer,

02:29.040 --> 02:32.720
bit like the sort of thing you would see in a large language model which takes text strings,

02:32.720 --> 02:37.760
my text string today, and we have some shared embedded space and that embedded space is just

02:37.760 --> 02:42.800
a numerical fingerprint for the meaning in these two items and they're trained remember across many,

02:42.800 --> 02:47.520
many images such that when you put the same image and the text that describes that image in,

02:47.520 --> 02:51.840
you get something in the middle that matches. And the idea then is you can use that for other tasks

02:51.840 --> 02:55.840
like you can use that for classification, you can use it for image recall. If you use a streaming

02:55.840 --> 03:01.280
service like Spotify or Netflix, they have this thing called a recommender system. A recommender

03:01.280 --> 03:05.520
system is where you've watched this program, this program and this program, what should you watch

03:05.520 --> 03:10.800
next. And you might have noticed that your mileage may vary on how effective that is but actually I

03:10.800 --> 03:15.040
think they're pretty impressive for what they have to do. But you could use this for a recommender

03:15.040 --> 03:18.800
system because you could say basically what programs have I got that embed into the same

03:18.800 --> 03:23.680
space of all the things I just watched and recommend them that way. So there are downstream tasks

03:23.680 --> 03:27.920
like classification and recommendations that we could use based on a system like this.

03:27.920 --> 03:34.800
What this paper is showing is that you cannot apply effectively these downstream tasks for

03:34.800 --> 03:41.520
difficult problems without massive amounts of data to back it up. And the idea that you can apply

03:42.480 --> 03:47.920
this kind of classification on hard things. So not just cats and dogs but specific cats and

03:47.920 --> 03:54.560
specific dogs or subspecies of tree. Or difficult problems where the answer is more difficult than

03:54.560 --> 03:59.600
just the broad category that there isn't enough data on those things to train these models in an

03:59.600 --> 04:04.480
effective way. I've got one of those apps that tells you what specific species a tree is. So

04:04.480 --> 04:08.880
what is it not just similar to that? No, because they're just doing classification or some other

04:08.960 --> 04:15.840
problem. They're not using this kind of generative giant AI. The argument has been why do that silly

04:15.840 --> 04:20.720
little problem where you can do a general problem and solve all your problems. And the response is

04:20.720 --> 04:28.240
because it didn't work. That's why we're doing it. So there are pros and cons for both. I'm not

04:28.240 --> 04:32.960
going to say that no generative AI is useful or these models are incredibly effective for what

04:32.960 --> 04:39.280
they do. But I'm perhaps suggesting that it may not be reasonable to expect them to do very difficult

04:39.280 --> 04:44.720
medical diagnosis because you haven't got the data set to back that up. So how does this paper do

04:44.720 --> 04:49.360
this? Well what they do is they define these core concepts. So some of the concepts are going to be

04:49.360 --> 04:53.200
simple ones like a cat or a person. Some of them are going to be slightly more difficult like a

04:53.200 --> 04:59.280
specific species of cat or a specific disease in an image or something like this. And they come up

04:59.360 --> 05:05.920
with about 4,000 different concepts. And these are simple text concepts. These are not complicated

05:05.920 --> 05:12.480
philosophical ideas. I don't know how well it embeds those. And what they do is they look at

05:12.480 --> 05:20.400
the prevalence of these concepts in these data sets. And then they test how well the downstream

05:20.400 --> 05:26.080
task of let's say zero shot classification or recall the recommender systems works on all of

05:26.080 --> 05:31.200
these different concepts. And they plot that against the amount of data that they had for that

05:31.200 --> 05:35.200
specific concept. So let's draw a graph and that will make me make it more clear. So let's imagine

05:35.200 --> 05:44.960
we have a graph here like this. And this is the number of examples in our training set of a

05:44.960 --> 05:51.200
specific concept. So let's say a cat, a dog, something more difficult. And this is the performance

05:51.920 --> 05:58.480
on the actual task of let's say recommender system or recall of an object or the ability to

05:58.480 --> 06:03.600
actually classify it as a cat. Remember we talked about how you could use this to zero shot classification

06:03.600 --> 06:08.240
by just seeing if it embeds to the same place as a picture of a cat, the text a picture of a cat,

06:08.240 --> 06:14.560
that kind of process. So this is performance. The best case scenario if you want to have an

06:14.560 --> 06:19.920
all powerful AI that can solve all the world's problems is that this line goes very steeply

06:19.920 --> 06:25.680
upwards. This is the exciting case. It goes like this. That's the exciting case. This is the kind

06:25.680 --> 06:30.400
of AI explosion argument that basically says we're on the customer something that's about to happen

06:30.400 --> 06:36.640
whatever that may be, where the scale is going to be such that this can just do anything. Then

06:36.640 --> 06:41.600
there's the perhaps slightly more reasonable, should we say, pragmatic interpretation, which is

06:41.600 --> 06:47.280
like just call it balanced, which is that there's a sort of linear movement. So the idea is that

06:47.280 --> 06:50.720
we have to add a lot of examples, but we are going to get a decent performance boost from it.

06:50.720 --> 06:54.320
So we just keep adding examples, we'll keep getting better, and that's going to be great.

06:54.320 --> 06:58.560
And remember that if we ended up up here, we have something that could take any image and

06:58.560 --> 07:02.960
tell you exactly what's in it under any circumstance. That's kind of what we're aiming for. And

07:02.960 --> 07:06.240
similarly for large language models, this would be something that could write with incredible

07:06.800 --> 07:11.280
accuracy on lots of different topics. Or for image generation, it would be something that could

07:11.280 --> 07:16.320
take your prompt and generate a photo realistic image of that with almost no coercion at all.

07:16.320 --> 07:20.640
That's kind of the goal. This paper has done a lot of experiments on a lot of these concepts

07:20.640 --> 07:27.200
across a lot of models, across a lot of downstream tasks. And let's call this the evidence.

07:27.200 --> 07:29.680
It's all you're going to call it pessimistic now and then.

07:29.680 --> 07:34.400
It is pessimistic also, right? It's logarithmic. So it basically goes like this, right?

07:34.400 --> 07:35.040
It flattens out.

07:35.040 --> 07:39.600
It flattens out. Now, this is just one paper, right? It doesn't necessarily mean that it will

07:39.600 --> 07:45.360
always flatten out. But the argument is, I think, that and it's not an argument they necessarily

07:45.360 --> 07:49.840
make in the paper. The paper is very reasonable. I'm being a bit more cavalier with my wording.

07:49.840 --> 07:53.520
The suggestion is that you can keep adding more examples. You can keep making your models bigger,

07:53.520 --> 07:59.280
but we are soon about to hit a plateau where we don't get any better. And it's costing you millions

07:59.280 --> 08:02.880
and millions of dollars to train this. At what point do you go, that's probably about as good as

08:02.880 --> 08:07.760
we're going to get the technology, right? And then the argument goes, we need something else.

08:07.760 --> 08:12.000
We need something in the transform or some other way of representing data or some other

08:12.000 --> 08:17.520
machine learning strategy or some other strategy that's better than this in the long term if we

08:17.520 --> 08:21.680
want to have this line go up here or this line go up here. That's kind of the argument.

08:21.680 --> 08:29.760
And so this is essentially evidence, I would argue, against the kind of explosion possibility of

08:29.760 --> 08:33.200
that just you just add a bit more data when we're on the cusp of something. We might come back here

08:33.200 --> 08:36.640
in a couple of years, you know, if you'll still allow me on computer file after this absolute

08:36.640 --> 08:42.240
embarrassment of these claims that I made. And we say, actually, the performance has improved

08:42.240 --> 08:46.960
massively, right? Or we might say we've doubled the number of data sets to 10 billion images,

08:46.960 --> 08:52.880
and we've got 1% more on the classification task, which is good, but is it worth it? I don't know.

08:52.880 --> 08:56.320
This is a really interesting paper because it's very, very thorough, right? If there's a lot of

08:56.320 --> 09:00.160
evidence, there's a lot of curves, and they all look exactly the same. It doesn't matter what method

09:00.160 --> 09:04.000
you use, doesn't matter what data set you train on, it doesn't matter what your downstream task is,

09:04.000 --> 09:08.320
the vast majority of them show this kind of problem. And the other problem is that we don't

09:08.320 --> 09:15.280
have a nice, even distribution of classes and concepts within our data set. So for example,

09:15.280 --> 09:24.880
cats, you can imagine are over-emphasized or over-represented in the data set by an order

09:24.880 --> 09:31.120
of magnitude, right? Whereas specific planes or specific trees are incredibly underrepresented

09:31.120 --> 09:35.840
because you just have tree, right? So, I mean, trees are probably going to be less represented

09:35.840 --> 09:40.720
than cats anyway, but then specific species of tree very, very underrepresented, which is why,

09:40.720 --> 09:44.880
when you ask one of these models, what kind of cat is this or what kind of tree is this,

09:44.880 --> 09:50.000
it performs worse than when you ask it what animal is this because it's such a much easier problem.

09:50.000 --> 09:54.560
And you see the same thing in image generation. If you ask it to draw a picture of something really

09:54.560 --> 09:59.760
obvious like a castle where that comes up a lot in the training set, you can draw your fantastic

09:59.760 --> 10:04.640
castle in the style of Monet and it can do all this other stuff. But if you ask it to draw some

10:04.640 --> 10:09.760
obscure artifact from a video game that barely even made it into the training set, suddenly it's

10:09.760 --> 10:14.320
starting to draw something a little bit less quality. And the same with large language models.

10:14.320 --> 10:17.920
This paper isn't about large language models, but the same process you can see actually already

10:17.920 --> 10:24.400
happening. If you talk to something like chatGPT, when you ask it about a really important topic

10:24.400 --> 10:28.320
from physics or something like this, it will usually give you a pretty good explanation of that

10:28.320 --> 10:32.480
thing because that's in the training set. But the question is what happens when you ask it about

10:32.480 --> 10:36.400
something more difficult, right? When you ask it to write that code, which is actually quite difficult

10:36.400 --> 10:41.360
to write, and it starts to make things up, it starts to hallucinate, and it starts to be less

10:41.360 --> 10:45.520
accurate. And that is essentially the performance degrading because it's underrepresented in the

10:45.520 --> 10:50.320
training set. The argument I think is, at least it's the argument that I'm starting to come around

10:50.320 --> 10:54.720
to thinking, if you want performance on hard tasks, tasks that are underrepresented on just

10:54.720 --> 10:59.760
general internet texts and searches, we have to find some other way of doing it than just collecting

10:59.760 --> 11:04.640
more and more data, particularly because it's incredibly inefficient to do this. On the other

11:04.640 --> 11:10.800
hand, these companies will, they've got a lot more GPUs than me. They're going to train on

11:10.800 --> 11:14.960
bigger and bigger corpuses, better quality data, they're going to use human feedback to better

11:14.960 --> 11:20.240
train their language models and things. So they may find ways to improve this up this way a little

11:20.240 --> 11:24.640
bit as we go forward. But it's going to be really interesting to see what happens because I'll,

11:24.640 --> 11:30.080
you know, will it plateau out? Will we see chapter GPT seven or eight or nine be roughly the same as

11:30.080 --> 11:34.960
chapter GPT four? Or will we see another state of the art performance boost every time? I'm kind of

11:34.960 --> 11:39.760
trending this way, but you know, it'll be excited to see if it goes this way. Take a look at this

11:39.760 --> 11:47.840
puzzle devised by today's episode sponsor, Jane Strait. It's called bug bite, inspired by debugging

11:47.840 --> 11:53.200
code. That world we're all too familiar with, where solving one problem might lead to a whole

11:53.200 --> 12:00.000
chain of others. We'll link to the puzzle in the video description. Let me know how you get on.

12:00.000 --> 12:04.240
And speaking of Jane Strait, we're also going to link to some programs that they're running at the

12:04.240 --> 12:10.160
moment. These events are all expenses paid and give a little taste of the tech and problem solving

12:10.160 --> 12:19.120
used at trading firms like Jane Strait. Are you curious? Are you problem solver? Are you into

12:19.120 --> 12:24.800
computers? I think maybe you are. If so, well, you may well be eligible to apply for one of these

12:24.800 --> 12:29.680
programs. Check out the links below or visit the Jane Strait website and follow these links.

12:30.720 --> 12:34.080
There are some deadlines coming up for ones you might want to look at, and there are always

12:34.080 --> 12:38.800
more on the horizon. Our thanks to Jane Strait for running great programs like this and also

12:38.800 --> 12:47.360
supporting our channel. And don't forget to check out that bug bite puzzle.

