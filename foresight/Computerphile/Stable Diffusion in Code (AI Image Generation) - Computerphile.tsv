start	end	text
0	5280	Last time we talked about how these kind of networks and image generation systems work.
5280	6760	But there are different kinds, aren't there?
6760	11000	There are, there's Dali2, there's Imogen, there's StableDiffusion,
11000	16680	and I didn't talk about them in the last video because they are, for the sake of understanding diffusion models in general,
16680	20200	essentially the same, but actually they are quite different underneath, right?
20200	25080	And it comes down to, you know, the resolution, exactly where you do the embeddings,
25080	29120	how you do the embeddings, how you structure your network and so on and so forth, right?
29120	34520	So, and in fact, actually in StableDiffusion's case, it comes down to where you do the diffusion as well.
34520	39560	So, let's look, we'll look at the StableDiffusion code because I've got access to that, right?
39560	43680	And we'll go into it in quite some detail, I think would be quite interesting.
43680	47840	I've really enjoyed using it because first of all, it's given me a better understanding of how it works.
47840	51280	And also, you can do some pretty cool stuff by messing about down there and saying,
51280	54160	well, what if I gave it a frog but also a snake, right?
54160	56040	And the answer is you get a frog snake.
56920	57920	A frog?
57920	61480	Yeah, exactly. Snake giraffe was the stuff of nightmares.
64920	67840	There were questions about FX, there were questions about how these are trained,
67840	69840	maybe we deal with them at another time.
69840	71200	Let's talk about how they work.
71200	75400	So, Dali 2 is perhaps at the moment the biggest one,
75400	79000	but it's been, I think rapidly overtaken by StableDiffusion,
79000	81440	primarily because StableDiffusion is more available to people.
81440	83920	I can download the code and run it of StableDiffusion,
83920	87560	Dali, you access via an API and you say, I would like an image, please,
87560	90000	and it gives you something back.
90000	94840	If you don't have any interest in the code, then sure, just use the API.
94840	98160	But if, for example, like me, you might be interested in what the applications
98160	102640	for generating images in your area of research, like plants or medical imaging,
102640	106200	maybe I want the access to the code and I can train up the network myself, right?
106200	110800	So, Dali builds on a lot of stuff, it's from OpenAI,
110800	114040	it builds on a lot of stuff that they've already done, right?
114040	115920	The first one is something called clip embeddings.
115920	119200	Clip embeddings are the way of taking your text tokens
119200	122360	and turning them into some meaningful numbers, right?
122360	124040	And remember, we're going through a transformer,
124040	127520	so we're not just saying, well, the word that is a five
127520	129920	and the word football is a 17.
129920	131840	What we're doing is we're taking the whole sentence,
131840	134200	we're doing a lot of cross-attention and saying,
134200	137400	this is the overall meaning of a sentence reflected numerically.
137400	138560	So, you get some context.
138560	139960	Yeah, that's the idea.
139960	143400	And clip is trained with image and text pairs.
143400	147320	So, you put in an image, you put in the text that describes that image
147320	150120	and what you try and do is align those two embeddings
150120	151360	so that they kind of make sense
151360	155320	and that way you've got a kind of semantically meaningful text embedding.
155320	157800	So, it's a bit like a supervised data set of some sort.
157800	161760	It is, yeah, it's sort of, it is a supervised data set.
161760	165760	It's trained using a contrastive loss, which is what this CL stands for
165800	169280	and the idea is that basically you want to try and make the embeddings
169280	173280	of an image and its text description very, very similar
173280	177240	and the embeddings of an image with a different text description
177240	178600	very, very different, right?
178600	182120	And not in a dissimilar way to how when we were doing the face ID stuff,
182120	185320	we're trying to put my face near previous shots of my face
185320	187160	so that you can unlock a phone.
187160	189800	Unlock a phone with your face, that's the one.
189800	191840	If you want to unlock a face with your phone,
191840	194160	so you've got a clip embedding, this is the text embedding.
194160	196400	You also have various other things that work.
196400	199200	In Darley, and I'm going to sort of simplify it slightly,
199200	204200	you put in an image which I think is at 64 by 64 pixels.
205600	209160	You put in a noise image at 64 by 64, right?
209160	213160	You put in your time, you put in your clip text embeddings
213160	216440	and you also put them into the network like we described in the previous video.
216440	220120	You have a giant unit structure that produces an estimate for the noise
220120	221480	and you loop.
221480	227360	Now, that produces a not bad image, but only at 64 by 64 pixels.
227360	231000	This process of randomly producing noise,
231000	233200	checking whether it's work, subtracting it, producing it,
233200	235720	this takes a long time at high resolution
235720	238720	and the sort of network you would need would be astronomically big.
238720	242240	So to make that easier, we only run it at 64 by 64.
242240	245680	Now, of course, how do we then make that nice, right?
245680	249120	Because just Darley 2 outputs 1K by 1K images.
249120	251560	The answer is we have another network that does the same thing,
251560	253960	but this time its job is to upsample.
253960	256880	So you basically put in a noisy 64 by 64 and say,
256880	259000	output me the 256 version, right?
259000	260280	And so on and so forth.
260280	263280	So you put this through, I think, two levels of upsampling
263280	267800	to go from 64 to 256 to 1024.
267800	269400	Will you pardon my dumb question?
269400	269840	Yeah.
269840	273840	Are we finally at the point where we can say enhance?
273840	274800	You know what we are?
274800	276480	Yeah.
277440	279600	And it will work exactly like it does in the TV,
279600	283040	where it will just make up nonsense and they'll call it a win.
283040	283840	It works pretty well.
283840	287160	Imagen, Google's version, works in a very similar way.
287160	292000	You have a network that's trained to denoise and generate 64 by 64 images,
292000	296680	guided by text, and then you have two upsampling networks
296680	299280	that go up to 1024.
299280	302400	Stable diffusion does its diffusion process,
302400	304240	sort of in this bit in some sense.
304240	309320	You have what we call an autoencoder, which takes some noise
309320	314680	and turns it into a lower resolution but detailed representation.
314680	317720	You then do the diffusion process this way,
317720	321480	which denoises that latent space.
321480	323920	And then you have the other side of the autoencoder,
323920	325560	which expands it back out into an image.
325560	326720	So this is a different way of doing it.
326720	330240	And the advantage is that this is much lower resolution than this.
330240	331440	And they call it stable diffusion.
331440	333320	There's an argument that is slightly more stable.
333360	335120	I don't know to what extent that's true.
335120	337840	There are some differences in the way that these produce images.
337840	341440	But in all other regards, basically, it's the same kind of process.
341440	344800	You're still doing guidance from text.
344800	346080	You're still putting in T.
346080	348040	It's just that you're now doing it in this latent space
348040	349680	instead of in the full image space.
349680	352480	Think of it like you put it through a zip, right?
352480	355880	And you compress it down and then you do all the diffusion in that space.
355880	357640	And then at the end, you expand it back out again.
357640	358440	That's the idea.
358440	361080	And actually, the autoencoder is very, very good.
361080	363000	You can take an image, you can compress it right down
363000	365160	and it'll still produce much the same image again.
365160	366680	Let's dive into the code and have a look.
366680	368040	Right, so I'm in Google Colab.
368040	369000	Now, for those of you who don't know,
369000	372040	Google Colab is a sort of Jupiter notebook style environment
372040	377720	that allows you to access also Google's GPUs for running machine learning things.
377720	379840	Now, I don't tend to use Google Colab generally
379840	383680	because a lot of our processes last longer than you're really meant to use it for.
383680	386520	But for this, it's excellent, right?
386520	390240	So I've got this code from a guy called Jonathan Whitaker,
390240	392200	which I've then repurposed and done my own stuff with it
392200	393360	and I've been messing about.
393360	395080	And so thanks very much to him for that.
395080	396760	But I've taken it and I've played around.
396760	397760	I've changed the resolution.
397760	399960	I've toyed around with a lot of stuff.
399960	402880	And what I wanted to do was talk through some of these lines of code
402880	404600	so you can see what it is that it's doing.
404600	407760	It's the same exact process that I just described.
407760	409320	It's just a few lines of code to do it.
409320	412600	Now, obviously, there's a lot of deep networks and stuff going on behind the scenes,
412600	415960	but they end up getting abstracted away in function calls
415960	418040	and so it becomes very straightforward.
418040	420120	Okay, I've imported all my libraries already.
420120	422440	And then what we've got here is one go.
422440	423680	We're going to have our text prompt.
423680	427160	And what we want to do is take that text prompt and produce an image, right?
427160	432560	So we have various things that we want it to be, 512 pixels tall, 768 pixels wide.
432560	434880	We're going to run 50 steps of inference
434880	437520	and then a few other things that we can talk about in a moment.
437520	440120	Like, for example, we're going to seed it with the number four.
440120	441040	Now, why four?
441040	443120	Because I don't know, I picked it at random.
443120	445160	I can seed it at 77 if you like.
445160	447240	This allows us to run the exact same code again
447240	449160	and produce the exact same image another time.
449200	452680	If we just used a random seed, if you got to an image you liked,
452680	454880	you accidentally get rid of it, you never get it back, right?
454880	458760	So, but if you change this number, you get entirely different images
458760	462440	because the noise that you start with is entirely different, right?
462440	463520	So let's put in a prompt.
463520	464240	Well, what should we do?
464240	465080	Frogs on stilts?
465080	466960	I think we need to do frogs on stilts.
468320	469480	I mean, this may not work.
469480	472600	I don't, you know, anything else you want to add, like in, you know,
472600	474800	in a park or just just just frogs on stilts?
474800	476520	What about on a stage?
477520	480200	OK, frogs on stilts, on a stage at the theatre, right?
480200	480680	Yeah.
480680	484320	Now, the first thing we have to do is we have to embed this
485320	488760	into some kind of usable space in which the machine learning can work.
488760	492520	So what we do is we tokenize, this is the function that tokenizes
492520	496520	the text input and basically turns it into a numerical code for each word.
496520	500480	And then that goes into the texting coder, which is our clipping beddings.
500480	502680	So that's the bit where it sort of works out the context.
502680	506360	Yeah, that's the transformer that goes, well, OK, this one kind of
506360	509120	goes with this word and then this means they share weights and so on.
509120	512040	And then you go through the transform and you end up with essentially
512120	516960	to us, meaningless numbers, but to this semantic information
516960	519560	on the meaning of the sentence.
519560	523240	We also, because if you remember, we put it through the network twice,
523400	525560	one with the text embeddings and one without.
525560	528720	So we also have to produce a dummy text embedding with nothing in it.
528920	531200	Right. And that's what this unconditioned input is.
531240	535880	Then we're going to text encoder unconditioned embeddings and we get
536400	540160	two text embeddings, one of which is unconditioned and one of which is conditioned.
540240	542040	Right. So this one has fogs on stilts.
542040	543760	This one is just sort of blank.
543760	545040	Now we need to set our scheduler.
545040	548040	Remember, you can choose a scheduler that produces different amounts of noise
548040	552280	at each time step and which one you use will depend on to an extent
552280	555160	the kind of images you want out, but also how you've trained the network.
555320	558520	Where is going to be using the standard one that came with stable diffusion?
558720	560960	And I'm going to run for 50 time steps.
561160	566320	So what this will do is distribute the amount of noise it adds from zero to 50.
566520	569440	Right. So when I say 50, it's going to produce a maximum amount of noise.
569440	571680	And when I say one, it's going to produce a tiny amount of noise.
571960	573400	Right. That's the idea.
573400	579520	And then we're going to actually produce our latent noise that we're going to be diffusing.
579600	584440	So we create a random array of numbers of the right size and we're going to call these
584440	586840	latents and we're going to stick them on the graphics card.
586840	589360	And then we're going to do some scaling to our latents as well,
589360	592960	because the scales of some of these different parts of a network of
592960	594360	difference, you have to move them in and out.
594440	595320	And then we're nearly done.
595400	596200	Like this is our loop.
596480	597880	So how does the loop work?
597880	605360	Well, the first thing we do is we calculate the noise to be added at this particular
605560	606160	iterations.
606160	609240	We're going through all the different time steps and we're going to add a different
609240	609880	amount of noise.
609920	613440	We're going to add this noise to our latent space.
613480	613720	Right.
613720	615360	So basically we're noising up the image here.
615400	619400	Now remember, this is an embedded version of this image, but it is noised.
619440	621440	Then we're going to predict the noise with our unit.
621440	626040	So that is saying how much noise do you think was in this image, such that we can
626040	630160	get back to the original image, bearing in mind this text, and then we can do our
630160	631960	actual classifier for your guidance.
632000	632240	Right.
632240	635640	So what we're going to do is we're going to take our noise prediction with text and
635640	637120	our noise prediction without text.
637320	639280	We're going to calculate the difference and amplify it.
639680	643160	And then we're going to work out what our official noise prediction is.
643320	648120	And then finally, we're going to then use that noise prediction to calculate a
648120	651400	slightly less noise version of the image, which is what this line does here.
651760	653120	And we're going to repeat this process.
653400	653640	Right.
653640	654760	So we repeat the process.
654880	657080	We calculate the new noise at the next time step.
657440	658480	We predict it.
658880	661280	We subtract it away and add a bit more noise.
661320	662360	And we repeat this process.
662360	669160	And the idea is that over 50 iterations, we go from fully noise to some reasonable image.
669880	670440	Shall we see?
671000	673840	OK, so let's run at this resolution.
673840	676520	I'm pushing the amount of image size I can get on this graphics card.
676520	678480	So this is running on your graphics card here.
678520	681880	No, this is running on Google's graphics card over at Google.
682200	682440	Right.
682440	683520	Give me somewhere in London.
683520	685200	Do I owe you another eight pounds for this?
686040	686880	You can give me eight pounds.
686880	689400	No, this is covered under the original eight pounds per month.
690480	692040	But hopefully this won't take a month to record.
692520	695880	So we're choosing 50 iterations for this and because that's a decent amount.
695920	696240	Right.
696240	698840	You'll notice that if you don't do enough iterations, you're trying to move the
698840	699640	noise too quickly.
699640	702080	It becomes a bit unstable, doesn't produce nice results.
702760	704200	Because I've not done this before.
704200	705480	I don't know what these other results will be.
705480	706440	Will it be frogs on stilts?
706440	708240	Will it be bits of wood next to a frog?
708680	710760	Will it be something different because it's failed horribly?
711720	712320	Let's see.
714960	715640	Actually, that's not bad.
718880	719720	That is pretty impressive.
719720	722800	Now there's a weird leg coming out of this fog here, but I would I would say
722800	725040	that is a comparatively successful attempt.
725080	727360	This was produced from a noisy image.
728120	732040	So what we can do is we can change the noise seeds so we can say, you know,
732040	736280	128 and what that will do is create a complete different noise, which will
736280	737800	probably lead to a tiny different image.
737920	738240	Right.
738240	741240	I mean, it's still the same text prompt, so it's still guided in the same way.
741760	745200	But if this allows us to produce see near infinite numbers, basically,
745200	747400	of frogs on stilts, if that's your thing, right?
747720	748480	It is my thing.
748480	752320	Yeah, I've got quite into producing like cityscape, futuristic cityscape.
752320	754680	So I think that's where I spend most of my time on this.
758720	761080	I mean, that's gone a bit wrong, but actually still not bad.
761080	762480	It looks like a kind of stage.
763440	764720	They're just a bit not foggy.
765360	766640	Although yeah.
766640	767400	All right, all right.
767400	771480	OK, so anyway, we could spend, let's say another 20, 30 minutes
771480	774720	producing frogs on stilts, but yeah.
774720	776840	So what you could there's loads of cool stuff you can do.
776840	778720	Presumably you could just automate that.
778720	780200	So it just kept giving you loads of.
780200	781280	And in fact, I've done that right.
781280	784160	So for example, I created some nice pictures of dystopian,
784160	787400	abandoned, futuristic cities with overgrown plants, right?
787400	790440	And then I just put them in a for loop and just produce 200 of them
790440	791360	so I can pick the nice ones.
791400	796360	For example, in here, I've just got a bunch of awesome looking
796360	798520	city vistas, overgrown of plants.
798520	800000	They all look really, really good, right?
800000	800840	I'm quite pleased.
800840	804000	I mean, I've got no use for this, but it's quite fun.
804000	807040	And the other thing is that because you can do image to image guidance,
807040	810360	right, so what you do is you take an image that's your guide image,
810360	814080	you nearly noise it all the way and then you reconstruct, right?
814080	817240	So the noise is somewhat not come from a random place.
817240	819760	Then you can get an image that sort of bears some reflections.
819760	821920	You can say, well, I want a building over here and a tree over here.
821920	826480	So I'll draw them in and then I'll produce this and it will bear the same
826480	827960	that have the same shapes and stuff.
827960	830320	So you can control this process, even if you basically, like me,
830320	833240	have absolute zero artistic ability at all.
833240	837160	To give you an example, what I did was, so if I go down,
837160	838040	let me, let me go.
838040	841440	So this is a picture of my colleague's rabbit, very cute rabbit.
841440	845960	And what I did was I embedded this added noise, but not totally noise
845960	847080	to remove the image.
847080	850160	And then I reconstructed it with the text wooden carving of a rabbit
850160	852440	eating a leaf, highly detailed 4K artisan.
852440	853880	I don't know if the artisan word does anything.
853880	855040	I just thought it'd be fun.
855040	856200	It's trending on art station.
856200	857960	I see a lot of that put on the end of things.
857960	859160	Does that make a difference?
859160	860480	I don't know.
860480	864280	Anyway, and it produce a wooden carving of a rabbit, right?
864280	867040	And if you look at the original image versus this image,
867040	871320	some things have changed, sure, but the shape is roughly the same, right?
871320	875200	So it has guided this process using the original image.
875280	877320	And that's how image to image works.
877320	880360	So if you wanted to create an animation, you could create a quite simple
880360	885240	animation of a rabbit jumping about with no artistic ability, right?
885240	888960	I mean, actually, I would struggle to do even that, but, and then each frame,
888960	891360	you could then use this process to produce it.
891360	894720	At the moment, there's no kind of temporal consistency.
894720	895920	So you will see flickering, right?
895920	899440	If you ever see one of these videos, someone's produced online, it'll look cool,
899440	903240	but maybe not consistent and interesting because each frame might subtly change
903240	905040	things.
905040	907440	But that's the idea, right?
907440	909680	Now, you can do loads of weird stuff, right?
909680	911800	So this mixed guidance is one of my favorite things.
911800	913600	Here we have two text inputs.
913600	915880	And what we're going to do is we're going to embed both of them.
915880	921240	We're actually going to guide the generation using the midpoint of those two, right?
921240	924040	So I'm going to say, OK, I want a rabbit, right?
924040	925960	And I want a frog.
925960	929800	And I want you to produce me a 50-50 rabbit frog, right?
929800	932440	And what it will do is it'll embed both of them and it will do
932440	933440	the exact same process.
933440	937760	It's just that now, its text prompt is halfway between these two embeddings.
937760	942640	So you could potentially come up with a system with sliders, you know, 10-70 frog.
942640	945440	You could, to what amount of frog do you want in this image, right?
945440	950840	I mean, again, not sure what the use case is, but it's quite cool.
950840	951440	Here we go.
951440	954240	So it only takes about, I think, I'm training for 50 steps again.
954240	955560	So I'm running it for 50 steps.
955560	956760	While this work, you could do loads of stuff.
956760	960560	So for example, you could generate an image and then you could take half of it
960560	962960	and try and generate the other half to expand it outwards
962960	966720	and slowly grow your image to make it even higher as one, right?
966720	968360	If you're limited by the resolution.
968360	971360	And there's going to be a lot of people playing around with a lot of different ways to use this.
971360	976480	I've already seen the plugins for Gimp and for Photoshop and stuff.
976480	977440	There it is.
977440	978400	It's a strange one.
978400	980040	We'll put links to the code in the description.
980040	980800	Have a go.
980800	983880	You need to register for Huggingface to get access to the weights originally.
983880	987840	But then you can use something like Google Colab or your own hardware to generate pictures.
987840	989040	And people are having a lot of fun.
989080	993080	There are websites now where you can find cool images and the prompts that we use to generate them
993080	994240	to give you some ideas.
994240	995640	So there's lots of cool stuff to do.
995640	997800	And the rabbit, it's the same shape rabbit.
997800	999320	There's a bit more noise, right?
999320	1002280	And then we come over here and we come over here
1002280	1004360	and we end up with just noise.
1004360	1005440	It looks like nonsense.
1005440	1006120	And so the question is...
1006120	1009080	It takes the same amount of time to make one sandwich, but you've got two people doing it.
1009080	1011720	So they make twice as many sandwiches each time they make a sandwich.
1011720	1012560	Same with the computer.
1012560	1015160	We could either make the computer processor faster or...
