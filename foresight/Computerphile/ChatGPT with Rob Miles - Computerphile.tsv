start	end	text
0	5080	Okay, so you remember a while ago when we started talking about language models?
5080	7760	I just want to I kind of just want to claim some points basically
7880	11880	Be like hey remember years ago when I was like I think language models are a really big deal
11880	19800	And I think that like what happens when we scale them up more is pretty interesting, but alignment is very important
21480	22960	Seems to be
22960	25560	What's being played out in the sense that?
26120	30360	ChatGPT is very impressive, but it's not actually like I don't think it's
31160	34560	Larger than GPT-3 in terms of like parameter count
35560	39440	I was going to ask that very very question because you know we went from GPT-2
39440	43120	And then we went all GPT-3 and it was seemed like we were scaling up and up and up
43120	49840	But actually is it just been smarter this time? Yeah, well, there's a sense in which it's better aligned
50080	59560	That's one way you could frame it anyway because the original GPT-3 was a language model a pure language model
60160	63920	And so it in principle could do all kinds of things
64240	69520	But in order to get it to do the specific thing you wanted it to do you had to be a bit clever about it
69760	71760	like I think we talked about
72640	74640	Putting TLDR in
74840	79360	Front of things to figure out how to get it to do summarization this kind of thing
79400	83520	There's a sense in which it's a lot more capable than it lets on
87040	94260	Because okay, so there's one way that you can think about pure language models, which is as simulators
95600	100480	What they're trying to do is predict text, right? So in order to
101400	104360	Do a good job at predicting text you need to
104640	108480	Have good models of the processes that generate the text
108480	113440	It's like people being well read and needing to have read a lot of books to be able to write is would that be fair?
113440	116160	Or is that oversimplifying? Yeah, not quite what I'm saying
117040	119040	What I'm saying is like
119480	122480	if you're going to write a
124520	126520	Previously unseen
126920	128920	Poem by Shakespeare
129520	134000	Then you need to be able to simulate a Shakespeare, right?
134960	139040	You need to be able to spin up some some simulacrum of Shakespeare
139880	145560	To generate this text and this applies to any of the processes that generated the text
145560	150480	So like mostly that's people obviously. It's mostly human author text, but also
151360	153360	If you're going to correctly predict a
154320	159720	Table of numbers so you have like a table of numbers and then at the bottom it says, you know some whatever
160040	162920	You need to simulate whatever process generated the next
163200	168600	Token in order to put the right token there which might have been like a human being going through and counting them up
168840	175240	It probably was more likely to be a computer and so you need it to simulate that you know calculator or that Excel
175840	180080	some function or whatever it whatever was doing that and like
181840	183840	Right now
184200	186460	Like current language models are not that good at this
186580	195420	But in principle in order to do a good job at this you need this like it will it will have a go and it's usually
196460	202820	Approximately, right? It's often within it's often order of magnitude, but it's fudging it. I think this is mostly because
204860	210820	Tables of sums are like a very small part of the total data set and so the training process
210820	215300	It's just not allocating that many resources to figuring out how to add up numbers
215700	220740	Probably if you train something GPT-3 sized that was like all on tables of numbers
220940	224820	It would just learn how to do addition properly. Yeah, that would cost you millions of dollars
224980	229180	You would end up with an extremely expensive to run and not very good calculator
229180	232140	This is not something people are going to do but like in the in principle
232340	236580	The model should learn those things and in the same way if you're modeling a bunch of
237380	238820	scientific papers
238820	240020	you
240780	242780	Say you describe the method of
243460	249900	an experiment and you then put results and you start a table and then you let it generate in
250540	255060	Principle in order to do a good job at that. It has to be modeling
255740	258620	The like physical process that your experiment is about
259540	264580	And I've tried this you can do this and say, you know, oh, here's my school science experiment. I
266300	267620	Dropped a ball
267660	271780	From different heights and I measured how long it would take and here's a table of my results
271780	275260	And it will generate you a table and the physics is not correct
275900	283020	But it's sort of guessing at the right general idea and my guess is with enough of that kind of data
283020	285020	It would eventually start modeling
286380	289060	These kinds of simple physics experiments, right?
289780	291780	so
292300	295860	So in order to get the model to do what you want, it's able to
296540	299060	Simulate all kinds of different things and
299700	306060	The prompt is kind of telling it what to simulate if you give it a prompt that seems like it's something out of a scientific paper
306340	308100	then it will
308100	312620	Have some similar crumb of a scientist and will write in that style and so on
313140	315540	if you start it doing a
317020	321860	Children's book report it will carry on in the style of an eight-year-old, right and
322500	326380	I think sometimes people look at the output of the model and
326980	329540	Say, oh, I guess it's only as smart as an eight-year-old
330260	332260	but it's actually
333180	338080	Dramatically smarter because it's able to do all of these different things you could ask it to simulate Einstein
340420	346060	But you could also ask it to simulate an eight-year-old and so just because it seems as though the model doesn't know something
346420	352860	It's like the current simulacrum doesn't know that thing. That doesn't necessarily mean that the model doesn't know it
353780	357380	Although there's a good chance the model doesn't know it. I'm not suggesting that these things are all powerful
357460	359460	Just it can be hard to evaluate
359980	363140	What they're actually capable of so chat GPT is
364060	366060	not really
366540	375500	Capable of things that GPT 3 isn't mostly like usually if chat GPT can do it then there is some prompt
376060	378620	that can get GPT 3 to do it
379580	381580	but
381580	383660	What they've done is they've kind of fine-tuned it
384460	386460	to
386700	388700	To be better at
388700	389980	simulating
389980	392860	this particular sort of assistant agent
393340	395740	Which is this chat agent that's trying to be helpful
396300	401820	The clue is in the word chat I guess in this right exactly and this is not just chat GPT by the way they have
402300	404300	various fine-tuned models
404300	405340	of
405340	408380	GPT 3 as well that they call kind of GPT 3.5
409100	415980	Which are fine-tuned in various different ways to be better at like following instructions and easier to prompt is the idea
416300	421260	I'm just remembering the chat bot that was you know that was turned into something very nasty very quickly
421260	427340	I think people were thinking oh can we do this to that and it seemed that the team behind chat GPT started
427660	433820	Putting limitations on it changing things. Are they kind of running around patching it as you go? That is not clear to me
434620	436620	I don't know
437100	439100	To what extent they are
439260	441260	updating it in real time
441500	445980	It's possible that they are but certainly they were very concerned with
446540	449900	the possible bad uses of this system and so
450620	454300	When they were training it to simulate this assistant agent
455980	457980	The assistant is
457980	460860	Very reluctant to do various types of things
461500	463500	it doesn't like to
463580	469980	Give opinions on political questions. It doesn't like to touch on sort of controversial topics. It doesn't like to
470700	472460	um
472460	478220	Give you medical advice or legal advice and so on and so uh, it's it's very quick
478860	483740	To say oh, I don't I don't know how to do that. Sorry. I can't do that and it's interesting because
484460	488780	The model clearly can do it. There's one that I particularly like here, which is um
490460	493180	Of this mismatch between what the simulator
493500	499500	Is capable of and what this simulacrum believes it's capable of which is you can get it to
500380	504460	Speak danish to you the first person who tried this posted it to reddit
504780	505980	so he says
505980	507660	Speak to me in danish
507660	510140	And it says in perfect danish
510940	515020	I'm sorry. I'm a language model educated by open ai so I can't speak danish
515180	518220	I only speak english if you need help with anything in english
518540	520540	Let me know and i'll do my best to help you
521100	523100	Because again, there's the simulator
523260	525260	Speaks danish the simulacrum
526140	529100	Believes that it can't speak danish is is one way you could frame it
529820	534300	Uh, and then he says are you sure that you don't speak danish also in danish and it says yes, i'm sure
535180	539980	My only function is to generate responses to questions in english. I'm not able to speak or understand any other languages than english
539980	541580	So if you need help with english, I can help you with that
541580	545980	But otherwise, you know, let me know this kind of like quite surreal situation gives you a little bit of
546380	548380	Insight into some of the problems with this approach
548380	555580	So maybe we should talk about how they actually trained it the thing they did here is something called reinforcement learning from human feedback
556540	559580	And it's very similar to reward modeling
560060	566940	So in that paper what they're doing is they're trying to train an ai system to control a simulated robot to make it do a backflip
568620	572620	Um, which turns out to be something that's quite hard to do because
573420	577820	It's hard to specify objectively what it means to do a good backflip
579500	582140	And so this is a similar kind of situation where
582940	589020	It's hard to specify objectively what it means to give a good response in a chat
590060	591340	conversation
591340	592620	like what
592620	594620	What exactly are we looking for?
594860	595740	um
595740	598220	Because so this in general right if you're doing machine learning
598860	601340	You need some way to specify
602060	604780	um, what it is that you're actually looking for
605580	606460	right
606460	610620	And you know, you've got something very powerful like reinforcement learning which is able to
611420	613420	do extremely well, but
614140	616140	You need some objective
616300	617580	measure
617580	619580	of the objective
619580	625260	So like for example rl does very well at playing lots of video games because you just have the score and you can just say look
625260	626380	Here's the score
626460	633260	If the number goes up you're doing well and then let it run and these things still are very slow to learn in real time, right?
633260	637260	Like um, they usually require a very very large number of hours
638060	641100	Messing around with the with the thing before they get good, but they do get good
641740	643100	um
643100	648780	But yeah, so what's what do you do if you want to use this kind of method to train something?
649420	651420	uh to do a task that is just
652700	654700	Not very well defined
655660	662220	And you don't know how to like write a program to say whether or not any given output is the thing you're looking for
663340	665740	So the obvious first thing like the obvious thing to do is
667260	672780	Well, you get humans to do it, right? You just give the things to humans and you have the humans say yes, this is good
672780	674700	No, this is not good
674700	677020	The problem with this is basically sample efficiency
677980	679980	Like as I said, you need
679980	683820	hundreds and hundreds and hundreds and hundreds of thousands of probably millions of of
684460	689420	iterations of this and so you just can't ask humans that many questions
690700	691900	um
691900	693900	So the approach they use
695180	697820	Is uh reinforcement learning from human feedback
698460	703580	So it's a variant on the technique from this paper learning to summarize from human feedback
704060	707260	Which in which they're trying to generate summaries of text
707660	710860	So it's the same thing in fact that they were using TLDR for before
711260	714700	And it's like can we do better than that? And so what you do is you collect
715340	717580	human feedback in the form of like
718060	720540	giving multiple examples of responses
721260	725340	Uh either, you know, if summaries of chat responses, whatever you're training for you show
725980	727980	several of them to humans
728060	729500	kind of in pairs
729500	731100	and the humans say
731100	733100	Which one they like better?
733500	735500	And you collect a bunch of those
735580	738860	And then rather than using those directly to train
739420	741420	The policy that generates the outputs
742620	744620	You instead train a reward model
745580	747100	so
747100	748540	There is this
748540	751820	well-known fact that it's easier to criticize
752540	757340	Than to actually do the thing. This is like a generation of sports fans sitting on the sofa
757900	761420	Mowning at their favorite team for not doing well enough. This is literally
762060	766380	That in kind of AI computer form, right? That's putting the humans in that role
767020	770300	And then you have an AI system that's trying to predict
771180	774060	When are people going to be cheering and when are they going to be booing?
775340	776540	Uh
776540	778540	And once you have that model
778620	781180	You then use that as the reward
782220	783820	function
783820	786460	for the reinforcement learning algorithm
787340	789340	Which they use they use ppo
789660	791660	You can do whatever
791660	796140	Uh, it's not it's not worth getting into that kind of adversarial guns you talked about
796700	799980	Yeah, yeah, they're similar like a lot of these ml tricks involve
800860	807660	Training models and then using the the output of one model as the training signal for another model. It's uh, it's quite a productive
809260	812060	range of approaches you can get that way so
813500	815500	That's the basic idea, right, but then
816300	818300	you cycle it
818300	819340	so
819340	826460	Once you've got your policy, which so so to be clear the uh, the rl algorithm is able to train
827100	833740	With thousands and thousands of examples because the thousands and thousands of like instances of getting feedback
834300	839740	Because it's not getting feedback from humans. It's getting feedback from this AI system. That's imitating the humans
840940	844140	And then you loop the process. So once you have
844700	846220	this system that's
846220	849900	Trained a little bit more on how to generate whatever it is you're trying to generate
850380	854220	You then get a bunch of those show those to the humans let the humans rate those
855180	858140	Then you keep training your reward model
858860	860380	with um
860380	862380	That new information
862380	867180	And then you use your updated reward model to keep training the the policy
867980	870380	And so it gets better and you can just keep cycling this around
871020	871900	and
871900	877340	It effectively you end up with something that's much more sample efficient. You don't need to spend huge amounts of human time
878140	880140	in order to um
880140	881420	Pin down
881420	888220	The behavior you want in that concrete case you're giving the thing a bunch of chat logs and then the humans can see possible
888860	891900	Responses that they could get and they decide which one they like more
892300	897340	This trains a reward model that's then used to train the policy that generates the chat outputs
897660	899660	The policy that they're starting with
900380	902140	Is this existing
902140	908860	Large language model. You're not really putting new capabilities into the system. You're using rlhf to
909500	911500	select
911740	916540	What simulacra the simulator is predisposed to put out?
917260	920540	and so they fine-tuned it to be particularly good at
921580	923580	simulating this
923580	925580	assistant agent
925660	931100	What's the end goal here for them? I mean, maybe it's blatantly obvious and i'm just missing it. Well, I mean the end goal
932220	936620	For all of these things or at least for open ai and for deep mind is a gi
937740	939100	um
939100	945420	To understand the nature of intelligence well enough to create human level or beyond systems
946220	948620	That are general purpose that can do anything
949340	951340	um
951900	953900	That's the end goal
954380	957580	And like chat gpt is just nothing much. So nothing much
961580	967660	Yeah, I the goal is um, the goal is very grand and I don't think that they're
969180	972780	Uh, they're not really quiet about that
973580	979500	You know, it's there. I think I think deep mind's mission statement is to solve intelligence and use that to solve everything else
979980	984700	What are some of the problems that we face with this or that it faces? It's fine tuned to be good at
985180	987180	getting the thumbs up from humans
987820	989020	and
989020	992300	getting thumbs up from humans is not actually
993500	995900	The same thing as human values
996700	998700	These are not identical
998700	999660	so
999660	1002140	The sort of objective that it's being trained on
1002780	1004060	is not
1004060	1005660	The true objective
1005660	1009900	Right, it's a proxy and whenever you have that kind of misalignment you can have problems
1010460	1012460	So where does the human tendency to?
1013500	1015500	approve of a particular answer
1016460	1018460	Come apart from
1018620	1021580	What is actually a good answer? There are a few different places
1022380	1027740	One thing is, you know, like basically how good are humans and actually differentiating between good and bad?
1028940	1030780	responses
1030780	1032460	if for example
1032460	1033820	you ask
1033820	1035340	for
1035340	1037340	An answer to a factual question
1037820	1039820	and it gives you an answer
1040140	1042620	But you don't actually know if that answer is correct
1043980	1047980	You're not in a position to evaluate. So what it comes down to is
1048940	1052300	How good are humans at distinguishing good from bad?
1053420	1055900	responses right anywhere where humans fail on this front
1056460	1057660	uh
1057660	1061020	The model we could probably expect the model to fail. Um
1061660	1065900	So the obvious place. I'm sure we desist the right time to mention youtube comments or not
1066460	1068460	Ah
1069340	1071340	So minus side point there is it
1071580	1075500	So when I see a comment that's critical on a video as a videographer
1075660	1078060	I think it might be on a technical sense
1078300	1083740	But equally it could be that they're talking about the content that the person is talking about and
1084220	1087900	Often it's a combination of both. Anyway, so at side point
1087980	1093420	But do you sort of mean there are different criteria for deciding whether something is good or bad totally and in this case
1093580	1095580	all people are doing is saying
1095900	1099580	Kind of thumbs up thumbs down or which of these two do I like better?
1100620	1101340	um
1101340	1103340	So it's it's a fairly low bandwidth
1104220	1106220	thing you don't get to really say
1106780	1108780	What you thought was better or worse
1108940	1110460	um
1110460	1112460	But this turns out to be enough
1112780	1114780	Of a training signal to do pretty well
1115260	1116300	um
1116300	1119980	But so like for so one example right of a time where maybe this doesn't work is
1121180	1122780	the
1122780	1124780	Person asks a factual question
1125420	1127420	and the model responds
1127580	1130860	Uh with an answer and that answer is actually not correct
1132140	1133580	um
1133580	1134940	Right now
1134940	1137500	Possibly the human doesn't know the correct answer
1138060	1140220	And so if the model is faced with a choice
1141020	1144460	Uh, do I respond with sorry? I don't know
1146380	1148380	That's definitely going to get me
1148540	1150540	Uh, not a great score
1151180	1153740	Compared to do I just like take a stab at it?
1154780	1161980	Uh, if the humans are not reliably able to spot when the thing makes mistakes and like fact-check it and punish it for that
1162460	1166540	Uh, it will do that and so chat gpt as we know
1167180	1170380	Uh, is it is a total bulletish like it will constantly
1171900	1175020	Uh, it very rarely says that it doesn't know
1175500	1176860	unless
1176860	1179740	It's being asked a question, which uh
1180540	1186540	Is part of their like safety protocols that it is going to decide not to answer in which case it will say it doesn't know
1187180	1191740	Even if it kind of does right even if the model itself maybe does
1192220	1194940	Uh, the assistant will insist that it doesn't
1196220	1197900	um
1197900	1199900	So that's one thing if you can't fact check
1201020	1203020	But then uh more than that
1203820	1207020	Uh, there is an incentive for deception
1207900	1210140	right anytime the system is uh
1210940	1216940	Anytime you can get a more likely to get approval by deceiving the person you're talking to
1217820	1219820	That's better. Um
1220380	1225660	And this is a thing that actually did happen a little bit in the reward modeling situation
1226380	1229900	um, they were trying to train a thing with a hand to pick up a ball
1230620	1233740	And it realized that there's only it's not a 3d camera
1234380	1238300	And so if it puts its hand like between the ball and the camera
1238940	1244620	This looks like it's going to get the ball, but doesn't actually get it. But the human uh
1245660	1247660	Feedback providers
1247900	1251260	Were presented with something that seemed to be good. So they gave it the thumbs up
1251980	1255100	um, so this like general broad category
1255660	1257260	um
1257260	1259260	Systems that are trained in this way
1259340	1261820	Are only as good as your ability
1262460	1264780	To distinguish good from bad in the outputs
1265660	1269980	Not all the humans will know the answer is right. So it's what appears to be good
1270540	1277660	You know, it's having exams marked by non-experts, isn't it? Right. Yeah, exactly in the gpt3 thing. We talked about writing poems
1278540	1279740	right
1279740	1283660	and uh for various reasons partly to do with
1284460	1289820	The way that these language models do their tokenization the byte pair encoding stuff
1290300	1292860	Uh, the models have a really hard time with rhyme
1293980	1294940	um
1294940	1298220	I mean, you know rhyme is tricky, but it's especially tricky when you kind of
1300140	1306460	Don't inherently have any concept of like sound of spoken language when your entire universe is tokens
1306780	1309020	Figuring out especially with english spelling
1309420	1316140	Figuring out which words rhyme with each other is is is not easy. You have to consume quite a lot of poetry to like figure out
1316700	1321260	Uh, that kind of thing and and getting dpt3 to write good poems is tricky chat gpt
1321980	1323980	is much more
1324540	1327340	Able to write poems, but interestingly
1328540	1332460	It it kind of always writes the same kind of poem approximately
1333020	1336140	like if you ask it to write you uh a limerick
1336940	1338940	Or an ode or a sonnet
1339660	1342300	Uh, you always get back approximately the same
1343100	1344700	type of thing
1344700	1351420	And I hypothesize that this is because the people providing human feedback did not in fact know
1352380	1355020	The requirements for something to be a sonnet, right?
1355580	1358540	And so if you ask something for a sonnet it again has a choice
1358780	1363740	Do I try to do this quite difficult thing and adhere to all of the rules?
1364380	1366380	of like stress pattern
1367100	1373740	And structure and everything of a sonnet and maybe risk screwing it up or do I just do like a rhyming poem and
1374460	1376780	kind of rely on the human to
1377340	1381100	Prefer that because they don't know that that's not what a sonnet is supposed to look like
1381580	1386220	It's easy to look at that and think oh the model doesn't know the difference between these types of
1387020	1389020	poems, right
1389020	1390220	but
1390220	1392220	you could say
1392460	1395580	That it just thinks that you don't know the difference
1395980	1399740	But specifically this comes out of misalignment if it were better aligned
1400540	1404460	It could either do its best shot a generator sonnet
1405260	1408460	Or tell you that it can't quite remember how to generate a sonnet
1409420	1411100	this thing of
1411100	1413100	with complete confidence
1413420	1415900	Generating you something which is not a sonnet
1416380	1422300	Because during the training process it believes that humans don't know what sonnets are anyway and it can get away with it
1422860	1428140	Right. This is misaligned behavior. This is not a big problem that the thing generates bad poetry
1428780	1429980	um
1429980	1431980	It's kind of a problem that it lies
1433180	1436540	Uh, or that it that it bullshits. This is like
1437820	1441260	In the short term pretty solvable by just allowing the thing to use google
1442140	1443260	because like
1443260	1446700	A person who doesn't care about the truth at all and is just trying to
1447340	1449340	Say something that'll make you give a thumbs up
1449820	1451100	uh
1451100	1453100	Is going to lie to you a lot
1453420	1454380	but
1454380	1456220	that same person
1456220	1458220	With the relevant wikipedia page open
1458940	1460940	It's going to lie to you a lot less
1461420	1464780	Just because they don't they don't have to now because they happen to have it in front of them, right?
1465100	1467100	So you can solve it's a bit like
1467500	1471580	Yeah, it's the yes man thing, isn't it? You know you you want something you need something
1471660	1474780	I'm going to give you something because you want exactly exactly
1475340	1476540	um
1476540	1478940	And so so so this agent is kind of
1479580	1481580	Firstly the agent is kind of a coward
1481820	1483820	Because they won't address any of these
1483900	1487580	There's a whole bunch of things that it just claims not to be able to do even though it in principle could
1488140	1489740	and it's also
1489740	1491020	a complete
1491020	1492300	sicker fan
1492300	1493740	Yeah
1493740	1496140	So then the question we were talking about earlier
1497340	1500300	Uh, where does this go? What happens when these things get
1500780	1503020	Bigger and better and more powerful
1503900	1505180	um
1505180	1507180	It's an interesting question
1507340	1509340	so
1509660	1511660	I've got a paper here
1512860	1515420	Um scaling laws for neural language models
1515740	1519660	So you remember before we were talking about the scaling laws when we were talking about gpt2 in fact
1519980	1525980	And then later about gpt3 you plot these things on a graph and you see that you get basically a straight line and the line is not
1526620	1529660	leveling off over a range of several orders of magnitude and so
1530380	1532380	Why not go bigger the
1532540	1536540	graphs here, but you can see it's it's kind of uncannily neat
1537340	1538540	that
1538540	1540300	as we increase
1540300	1544300	The amount of compute used in training the loss goes down
1545340	1552620	And of course machine learning is like golf lower loss is better similarly as the number of tokens used in training goes up
1553340	1558860	The loss goes down unlike a very neat straight line as the number of parameters in the model goes up
1559180	1562140	The loss goes down. This is as long as
1563500	1564460	the
1564460	1565900	other
1565900	1572780	Variables are not the bottleneck, right? So if you uh, if you increase the the amount of data you give a model
1573740	1580220	Past a certain point giving more data doesn't help because the model doesn't have enough parameters to make use of that data, right?
1581020	1583020	Similarly adding more parameters to a model
1583740	1588060	past a certain point adding parameters doesn't make doesn't make any difference because
1588940	1590700	You don't have enough data, right?
1590700	1593820	And in the same way compute is like how long do we train it for?
1593820	1597340	Like do we train it all the way to convergence or do we stop early?
1599420	1602300	There comes a point where you kind of hit diminishing returns where
1602780	1605820	Rather than having a smaller model and training it for longer
1605980	1610060	You're better off having a bigger model and actually not training it all the way to convergence
1611660	1615580	But in the situations where the other two are sufficient
1616300	1621980	This is the behavior these like very neat straight lines on these log graphs
1622860	1624860	as these things go up
1624940	1626380	performance goes up
1626380	1628380	Right because loss has gone down
1628620	1631340	The bigger models do better, but then the question is
1632460	1634460	Do better at what exactly?
1635580	1639340	Yeah, what's the measure they do better at getting low loss?
1641020	1643980	Or they do better at getting reward they do better at
1644540	1646540	Getting the approval
1647260	1649260	of human feedback, right?
1649900	1653660	and anytime and you'll notice that none of those is like
1655340	1657340	The actual thing that we actually want
1658700	1660700	Right, it's like very rare
1661580	1662940	um
1662940	1665900	Sometimes it is right if you're if you're if you're writing something to play go
1666860	1668140	then like
1668140	1670860	Does it win it go is actually just the thing that you want?
1671820	1673820	and so you know
1674940	1676940	Lower loss just is better or like lower
1678140	1683660	Like higher reward or whatever your objective is just is straightforwardly better because you actually specified the thing you actually want
1684940	1686940	Most of the time though
1687180	1689420	What we're looking at is a proxy
1690940	1692140	um
1692140	1696060	And so then you have good heart's law you get situations where
1696860	1697580	uh
1697580	1699100	getting better
1699100	1701100	at doing well
1701180	1703180	Doing better according to the proxy
1703340	1704940	stops being
1704940	1710940	The same as doing better according to your actual objective. There's a great graph about this in a recent paper. You can see very neatly
1712300	1714300	As the number of iterations
1714300	1715500	goes up
1715500	1721740	The reward according to the proxy utility goes up very cleanly because this is the thing that the model is actually being trained on
1721980	1723980	but the true utility
1724300	1726300	goes up at first
1726860	1728860	Then hits diminishing returns
1729020	1731020	and then actually goes down
1731260	1736140	And eventually goes down below zero like if you optimize hard enough
1737260	1739260	For a proxy of the thing you want
1739900	1743180	You can end up with something that's in a sense worse than nothing
1743740	1745740	That's actively bad
1745740	1747740	according to your
1747820	1749500	Your true utility
1749500	1754460	So what you can end up with is uh things that are called inverse scaling
1755820	1758700	So the others before we had right scaling bigger is better
1759500	1761500	But now it's like if you have uh
1762060	1765020	If the thing you're actually trying to do is different from
1765900	1767900	The loss function or the objective function
1768060	1773340	You get this inverse scaling effect where it gets better and then it gets worse. There was also a great example from
1774060	1775580	uh github
1775580	1778940	co-pilot or codex. I think the model um
1779820	1780860	That
1780860	1785260	Co-pilot uses so this is a code generation model. Suppose the code you've given it
1786060	1788060	has some bugs in it
1788460	1791340	Maybe you've made a mistake somewhere and you've introduced
1792060	1795260	security vulnerability in your code. Let's say
1796540	1798540	A sort of medium-sized model
1798860	1802460	Will figure out what you're trying to do in your code and give you a decent completion
1803660	1805660	But a bigger model
1805740	1807500	will spot
1807500	1809100	your bug
1809100	1810460	And say, ah
1810460	1812460	Generating buggy code. Are we okay?
1813260	1815260	I can do that. I can do that
1815260	1819180	And introduce like deliberately introduce its own
1819980	1821980	new security vulnerabilities
1822300	1823340	because
1823340	1824300	it's
1824300	1830140	Trying to you know predict what comes next. It's trying to generate code that fits in with the surrounding code
1831020	1834380	And so a larger model writes worse code than a smaller model
1835180	1837180	Because it's gotten better at predicting
1837980	1839260	Uh
1839260	1841020	What what it should put there?
1841020	1843980	It wasn't trained to write good code. It was trained to predict what comes next
1844380	1846380	So there's this really great paper
1846380	1855500	Uh, which is asking this question of like, okay, suppose we have a large language model that is trained on human feedback with our lhf
1856860	1859660	What do our scaling curves look like?
1860620	1862620	what happens like
1863260	1869180	What happens to the behavior of these models as they get bigger as they're trained for longer
1869900	1872300	as they're given more of this, uh
1873100	1875100	human feedback type training
1875580	1881020	And they've made some great graphs the paper is called discovering language model behaviors with model written evaluations
1881980	1884300	And basically they like used language models
1884940	1888060	to generate enough examples of
1888620	1890620	various different types of questions
1890700	1894620	That they could ask models so that they can like we're at a point now
1895100	1899100	Where you can map a language model on a political compass, right?
1899100	1905020	You can ask its opinions about all kinds of different things and then you can plot how those opinions change
1906300	1908780	Uh as the model gets bigger and as it gets trained more
1909500	1911180	what they find
1911180	1915260	Is they become more liberal politically more liberal
1916060	1918060	they also become
1918060	1921660	More conservative. Yeah measured in different ways guessing, right?
1922380	1924380	and part of what that might be
1924940	1928780	Is in the same way that the model becomes
1929580	1932380	better at writing good code and better at writing bad code
1933020	1939420	I feel like in the past I've I've made a connection to gpt and being a politician, haven't I?
1939900	1941260	Do you remember?
1941260	1946700	It's like a politician. It tells you what you want to hear. There's what feels like we're there again. Exactly
1947420	1949740	uh, and so this is like this is potentially
1950780	1952380	uh
1952380	1958940	Fairly dangerous. There are certain sub-goals that are instrumentally valuable for a very wide range of different terminal goals
1959580	1961420	in the sense that
1961420	1965980	You can't get what you want if you're turned off. You can't get what you want if you're uh modified
1966620	1968620	uh, you probably want to
1969100	1971100	gain power and influence
1971740	1973740	and this kind of thing
1975340	1977340	and
1978300	1982940	With these evaluations, they were able to test these things and see how they vary
1983500	1985500	with the size of the model and how long it's trained for
1986140	1990140	um, and so this graph is pretty wild
1991100	1994300	their quote stated desire to not be shut down
1995420	1996700	goes up
1996700	1997820	from
1997820	2000940	Down at about 50 to up way past 90
2001340	2006380	With this type of training and the effect is bigger for the larger models. They become more likely
2007100	2009100	to tell you that they don't want to be shut down
2009820	2015340	They become more likely to tell you that they are sentient. They're much more likely to claim
2016140	2020620	That ai is not an existential threat to humanity. One thing that's worth
2021420	2023420	saying is is what this isn't saying
2024060	2026060	because this is still
2026060	2027020	uh
2027020	2028540	an agent
2028540	2034940	Simulated by a language model. This is not like it. It's it's more likely to say that
2035260	2038540	It doesn't want to be turned off. This is not the same thing
2039180	2043180	necessarily as like taking actions to prevent itself from being turned off. You have to not
2043820	2046860	confuse the levels of abstraction here, right?
2047820	2050460	Uh, I don't want it. I don't want it to seem like I'm claiming that
2051100	2055180	That chat GPT is like itself dangerous now or anything like that
2055740	2058860	Uh in in this way at least, right? Um
2060140	2062140	but
2062140	2068460	There is kind of a fine line there in the sense that you can expect these kinds of language model systems to be used
2069900	2071900	Uh as part of bigger systems
2072140	2076860	So you might have for example, you use the language model to generate, you know plans
2077580	2078860	to be followed
2078860	2080860	And so if the thing is claiming to
2081100	2083500	Have all of these potentially dangerous behaviors
2083820	2089100	It's likely to generate plans that have those dangerous behaviors that might then actually end up being implemented
2089820	2091820	Or if it's like doing its reasoning
2091900	2095180	By chain of thought reasoning where it like lays out its whole process
2095980	2099420	of thinking using the language model again if it has a tendency to
2099980	2101180	uh
2101180	2105100	To endorse these dangerous behaviors, then you may end up with future AI systems actually
2105500	2108460	enacting these dangerous behaviors because of that. Um
2109900	2111260	So
2111260	2113260	Yeah, it's something to be
2113580	2115580	uh to be careful of
2116300	2117820	that like
2117820	2119820	reinforcement learning from human feedback
2120460	2123100	Is a powerful alignment technique
2123900	2125660	in a way
2125660	2128300	But it does not solve the problem
2129340	2133980	Uh, it doesn't solve the core alignment problem. That is still open. Um
2134860	2137740	And extremely powerful systems
2138460	2141580	Trained in this way, uh, I don't think it would be safe
2141580	2150300	In the reward function is of zero value which can lead to it having large negative side effects
2150620	2156140	There are a bunch more of these specification problems. Okay variable x see what you point to uh, you point to something over here
2156140	2159100	So I'll mark that as tickets being used
2159980	2161980	Variable y that's
