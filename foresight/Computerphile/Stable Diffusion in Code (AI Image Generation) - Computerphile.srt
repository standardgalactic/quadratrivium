1
00:00:00,000 --> 00:00:05,280
Last time we talked about how these kind of networks and image generation systems work.

2
00:00:05,280 --> 00:00:06,760
But there are different kinds, aren't there?

3
00:00:06,760 --> 00:00:11,000
There are, there's Dali2, there's Imogen, there's StableDiffusion,

4
00:00:11,000 --> 00:00:16,680
and I didn't talk about them in the last video because they are, for the sake of understanding diffusion models in general,

5
00:00:16,680 --> 00:00:20,200
essentially the same, but actually they are quite different underneath, right?

6
00:00:20,200 --> 00:00:25,080
And it comes down to, you know, the resolution, exactly where you do the embeddings,

7
00:00:25,080 --> 00:00:29,120
how you do the embeddings, how you structure your network and so on and so forth, right?

8
00:00:29,120 --> 00:00:34,520
So, and in fact, actually in StableDiffusion's case, it comes down to where you do the diffusion as well.

9
00:00:34,520 --> 00:00:39,560
So, let's look, we'll look at the StableDiffusion code because I've got access to that, right?

10
00:00:39,560 --> 00:00:43,680
And we'll go into it in quite some detail, I think would be quite interesting.

11
00:00:43,680 --> 00:00:47,840
I've really enjoyed using it because first of all, it's given me a better understanding of how it works.

12
00:00:47,840 --> 00:00:51,280
And also, you can do some pretty cool stuff by messing about down there and saying,

13
00:00:51,280 --> 00:00:54,160
well, what if I gave it a frog but also a snake, right?

14
00:00:54,160 --> 00:00:56,040
And the answer is you get a frog snake.

15
00:00:56,920 --> 00:00:57,920
A frog?

16
00:00:57,920 --> 00:01:01,480
Yeah, exactly. Snake giraffe was the stuff of nightmares.

17
00:01:04,920 --> 00:01:07,840
There were questions about FX, there were questions about how these are trained,

18
00:01:07,840 --> 00:01:09,840
maybe we deal with them at another time.

19
00:01:09,840 --> 00:01:11,200
Let's talk about how they work.

20
00:01:11,200 --> 00:01:15,400
So, Dali 2 is perhaps at the moment the biggest one,

21
00:01:15,400 --> 00:01:19,000
but it's been, I think rapidly overtaken by StableDiffusion,

22
00:01:19,000 --> 00:01:21,440
primarily because StableDiffusion is more available to people.

23
00:01:21,440 --> 00:01:23,920
I can download the code and run it of StableDiffusion,

24
00:01:23,920 --> 00:01:27,560
Dali, you access via an API and you say, I would like an image, please,

25
00:01:27,560 --> 00:01:30,000
and it gives you something back.

26
00:01:30,000 --> 00:01:34,840
If you don't have any interest in the code, then sure, just use the API.

27
00:01:34,840 --> 00:01:38,160
But if, for example, like me, you might be interested in what the applications

28
00:01:38,160 --> 00:01:42,640
for generating images in your area of research, like plants or medical imaging,

29
00:01:42,640 --> 00:01:46,200
maybe I want the access to the code and I can train up the network myself, right?

30
00:01:46,200 --> 00:01:50,800
So, Dali builds on a lot of stuff, it's from OpenAI,

31
00:01:50,800 --> 00:01:54,040
it builds on a lot of stuff that they've already done, right?

32
00:01:54,040 --> 00:01:55,920
The first one is something called clip embeddings.

33
00:01:55,920 --> 00:01:59,200
Clip embeddings are the way of taking your text tokens

34
00:01:59,200 --> 00:02:02,360
and turning them into some meaningful numbers, right?

35
00:02:02,360 --> 00:02:04,040
And remember, we're going through a transformer,

36
00:02:04,040 --> 00:02:07,520
so we're not just saying, well, the word that is a five

37
00:02:07,520 --> 00:02:09,920
and the word football is a 17.

38
00:02:09,920 --> 00:02:11,840
What we're doing is we're taking the whole sentence,

39
00:02:11,840 --> 00:02:14,200
we're doing a lot of cross-attention and saying,

40
00:02:14,200 --> 00:02:17,400
this is the overall meaning of a sentence reflected numerically.

41
00:02:17,400 --> 00:02:18,560
So, you get some context.

42
00:02:18,560 --> 00:02:19,960
Yeah, that's the idea.

43
00:02:19,960 --> 00:02:23,400
And clip is trained with image and text pairs.

44
00:02:23,400 --> 00:02:27,320
So, you put in an image, you put in the text that describes that image

45
00:02:27,320 --> 00:02:30,120
and what you try and do is align those two embeddings

46
00:02:30,120 --> 00:02:31,360
so that they kind of make sense

47
00:02:31,360 --> 00:02:35,320
and that way you've got a kind of semantically meaningful text embedding.

48
00:02:35,320 --> 00:02:37,800
So, it's a bit like a supervised data set of some sort.

49
00:02:37,800 --> 00:02:41,760
It is, yeah, it's sort of, it is a supervised data set.

50
00:02:41,760 --> 00:02:45,760
It's trained using a contrastive loss, which is what this CL stands for

51
00:02:45,800 --> 00:02:49,280
and the idea is that basically you want to try and make the embeddings

52
00:02:49,280 --> 00:02:53,280
of an image and its text description very, very similar

53
00:02:53,280 --> 00:02:57,240
and the embeddings of an image with a different text description

54
00:02:57,240 --> 00:02:58,600
very, very different, right?

55
00:02:58,600 --> 00:03:02,120
And not in a dissimilar way to how when we were doing the face ID stuff,

56
00:03:02,120 --> 00:03:05,320
we're trying to put my face near previous shots of my face

57
00:03:05,320 --> 00:03:07,160
so that you can unlock a phone.

58
00:03:07,160 --> 00:03:09,800
Unlock a phone with your face, that's the one.

59
00:03:09,800 --> 00:03:11,840
If you want to unlock a face with your phone,

60
00:03:11,840 --> 00:03:14,160
so you've got a clip embedding, this is the text embedding.

61
00:03:14,160 --> 00:03:16,400
You also have various other things that work.

62
00:03:16,400 --> 00:03:19,200
In Darley, and I'm going to sort of simplify it slightly,

63
00:03:19,200 --> 00:03:24,200
you put in an image which I think is at 64 by 64 pixels.

64
00:03:25,600 --> 00:03:29,160
You put in a noise image at 64 by 64, right?

65
00:03:29,160 --> 00:03:33,160
You put in your time, you put in your clip text embeddings

66
00:03:33,160 --> 00:03:36,440
and you also put them into the network like we described in the previous video.

67
00:03:36,440 --> 00:03:40,120
You have a giant unit structure that produces an estimate for the noise

68
00:03:40,120 --> 00:03:41,480
and you loop.

69
00:03:41,480 --> 00:03:47,360
Now, that produces a not bad image, but only at 64 by 64 pixels.

70
00:03:47,360 --> 00:03:51,000
This process of randomly producing noise,

71
00:03:51,000 --> 00:03:53,200
checking whether it's work, subtracting it, producing it,

72
00:03:53,200 --> 00:03:55,720
this takes a long time at high resolution

73
00:03:55,720 --> 00:03:58,720
and the sort of network you would need would be astronomically big.

74
00:03:58,720 --> 00:04:02,240
So to make that easier, we only run it at 64 by 64.

75
00:04:02,240 --> 00:04:05,680
Now, of course, how do we then make that nice, right?

76
00:04:05,680 --> 00:04:09,120
Because just Darley 2 outputs 1K by 1K images.

77
00:04:09,120 --> 00:04:11,560
The answer is we have another network that does the same thing,

78
00:04:11,560 --> 00:04:13,960
but this time its job is to upsample.

79
00:04:13,960 --> 00:04:16,880
So you basically put in a noisy 64 by 64 and say,

80
00:04:16,880 --> 00:04:19,000
output me the 256 version, right?

81
00:04:19,000 --> 00:04:20,280
And so on and so forth.

82
00:04:20,280 --> 00:04:23,280
So you put this through, I think, two levels of upsampling

83
00:04:23,280 --> 00:04:27,800
to go from 64 to 256 to 1024.

84
00:04:27,800 --> 00:04:29,400
Will you pardon my dumb question?

85
00:04:29,400 --> 00:04:29,840
Yeah.

86
00:04:29,840 --> 00:04:33,840
Are we finally at the point where we can say enhance?

87
00:04:33,840 --> 00:04:34,800
You know what we are?

88
00:04:34,800 --> 00:04:36,480
Yeah.

89
00:04:37,440 --> 00:04:39,600
And it will work exactly like it does in the TV,

90
00:04:39,600 --> 00:04:43,040
where it will just make up nonsense and they'll call it a win.

91
00:04:43,040 --> 00:04:43,840
It works pretty well.

92
00:04:43,840 --> 00:04:47,160
Imagen, Google's version, works in a very similar way.

93
00:04:47,160 --> 00:04:52,000
You have a network that's trained to denoise and generate 64 by 64 images,

94
00:04:52,000 --> 00:04:56,680
guided by text, and then you have two upsampling networks

95
00:04:56,680 --> 00:04:59,280
that go up to 1024.

96
00:04:59,280 --> 00:05:02,400
Stable diffusion does its diffusion process,

97
00:05:02,400 --> 00:05:04,240
sort of in this bit in some sense.

98
00:05:04,240 --> 00:05:09,320
You have what we call an autoencoder, which takes some noise

99
00:05:09,320 --> 00:05:14,680
and turns it into a lower resolution but detailed representation.

100
00:05:14,680 --> 00:05:17,720
You then do the diffusion process this way,

101
00:05:17,720 --> 00:05:21,480
which denoises that latent space.

102
00:05:21,480 --> 00:05:23,920
And then you have the other side of the autoencoder,

103
00:05:23,920 --> 00:05:25,560
which expands it back out into an image.

104
00:05:25,560 --> 00:05:26,720
So this is a different way of doing it.

105
00:05:26,720 --> 00:05:30,240
And the advantage is that this is much lower resolution than this.

106
00:05:30,240 --> 00:05:31,440
And they call it stable diffusion.

107
00:05:31,440 --> 00:05:33,320
There's an argument that is slightly more stable.

108
00:05:33,360 --> 00:05:35,120
I don't know to what extent that's true.

109
00:05:35,120 --> 00:05:37,840
There are some differences in the way that these produce images.

110
00:05:37,840 --> 00:05:41,440
But in all other regards, basically, it's the same kind of process.

111
00:05:41,440 --> 00:05:44,800
You're still doing guidance from text.

112
00:05:44,800 --> 00:05:46,080
You're still putting in T.

113
00:05:46,080 --> 00:05:48,040
It's just that you're now doing it in this latent space

114
00:05:48,040 --> 00:05:49,680
instead of in the full image space.

115
00:05:49,680 --> 00:05:52,480
Think of it like you put it through a zip, right?

116
00:05:52,480 --> 00:05:55,880
And you compress it down and then you do all the diffusion in that space.

117
00:05:55,880 --> 00:05:57,640
And then at the end, you expand it back out again.

118
00:05:57,640 --> 00:05:58,440
That's the idea.

119
00:05:58,440 --> 00:06:01,080
And actually, the autoencoder is very, very good.

120
00:06:01,080 --> 00:06:03,000
You can take an image, you can compress it right down

121
00:06:03,000 --> 00:06:05,160
and it'll still produce much the same image again.

122
00:06:05,160 --> 00:06:06,680
Let's dive into the code and have a look.

123
00:06:06,680 --> 00:06:08,040
Right, so I'm in Google Colab.

124
00:06:08,040 --> 00:06:09,000
Now, for those of you who don't know,

125
00:06:09,000 --> 00:06:12,040
Google Colab is a sort of Jupiter notebook style environment

126
00:06:12,040 --> 00:06:17,720
that allows you to access also Google's GPUs for running machine learning things.

127
00:06:17,720 --> 00:06:19,840
Now, I don't tend to use Google Colab generally

128
00:06:19,840 --> 00:06:23,680
because a lot of our processes last longer than you're really meant to use it for.

129
00:06:23,680 --> 00:06:26,520
But for this, it's excellent, right?

130
00:06:26,520 --> 00:06:30,240
So I've got this code from a guy called Jonathan Whitaker,

131
00:06:30,240 --> 00:06:32,200
which I've then repurposed and done my own stuff with it

132
00:06:32,200 --> 00:06:33,360
and I've been messing about.

133
00:06:33,360 --> 00:06:35,080
And so thanks very much to him for that.

134
00:06:35,080 --> 00:06:36,760
But I've taken it and I've played around.

135
00:06:36,760 --> 00:06:37,760
I've changed the resolution.

136
00:06:37,760 --> 00:06:39,960
I've toyed around with a lot of stuff.

137
00:06:39,960 --> 00:06:42,880
And what I wanted to do was talk through some of these lines of code

138
00:06:42,880 --> 00:06:44,600
so you can see what it is that it's doing.

139
00:06:44,600 --> 00:06:47,760
It's the same exact process that I just described.

140
00:06:47,760 --> 00:06:49,320
It's just a few lines of code to do it.

141
00:06:49,320 --> 00:06:52,600
Now, obviously, there's a lot of deep networks and stuff going on behind the scenes,

142
00:06:52,600 --> 00:06:55,960
but they end up getting abstracted away in function calls

143
00:06:55,960 --> 00:06:58,040
and so it becomes very straightforward.

144
00:06:58,040 --> 00:07:00,120
Okay, I've imported all my libraries already.

145
00:07:00,120 --> 00:07:02,440
And then what we've got here is one go.

146
00:07:02,440 --> 00:07:03,680
We're going to have our text prompt.

147
00:07:03,680 --> 00:07:07,160
And what we want to do is take that text prompt and produce an image, right?

148
00:07:07,160 --> 00:07:12,560
So we have various things that we want it to be, 512 pixels tall, 768 pixels wide.

149
00:07:12,560 --> 00:07:14,880
We're going to run 50 steps of inference

150
00:07:14,880 --> 00:07:17,520
and then a few other things that we can talk about in a moment.

151
00:07:17,520 --> 00:07:20,120
Like, for example, we're going to seed it with the number four.

152
00:07:20,120 --> 00:07:21,040
Now, why four?

153
00:07:21,040 --> 00:07:23,120
Because I don't know, I picked it at random.

154
00:07:23,120 --> 00:07:25,160
I can seed it at 77 if you like.

155
00:07:25,160 --> 00:07:27,240
This allows us to run the exact same code again

156
00:07:27,240 --> 00:07:29,160
and produce the exact same image another time.

157
00:07:29,200 --> 00:07:32,680
If we just used a random seed, if you got to an image you liked,

158
00:07:32,680 --> 00:07:34,880
you accidentally get rid of it, you never get it back, right?

159
00:07:34,880 --> 00:07:38,760
So, but if you change this number, you get entirely different images

160
00:07:38,760 --> 00:07:42,440
because the noise that you start with is entirely different, right?

161
00:07:42,440 --> 00:07:43,520
So let's put in a prompt.

162
00:07:43,520 --> 00:07:44,240
Well, what should we do?

163
00:07:44,240 --> 00:07:45,080
Frogs on stilts?

164
00:07:45,080 --> 00:07:46,960
I think we need to do frogs on stilts.

165
00:07:48,320 --> 00:07:49,480
I mean, this may not work.

166
00:07:49,480 --> 00:07:52,600
I don't, you know, anything else you want to add, like in, you know,

167
00:07:52,600 --> 00:07:54,800
in a park or just just just frogs on stilts?

168
00:07:54,800 --> 00:07:56,520
What about on a stage?

169
00:07:57,520 --> 00:08:00,200
OK, frogs on stilts, on a stage at the theatre, right?

170
00:08:00,200 --> 00:08:00,680
Yeah.

171
00:08:00,680 --> 00:08:04,320
Now, the first thing we have to do is we have to embed this

172
00:08:05,320 --> 00:08:08,760
into some kind of usable space in which the machine learning can work.

173
00:08:08,760 --> 00:08:12,520
So what we do is we tokenize, this is the function that tokenizes

174
00:08:12,520 --> 00:08:16,520
the text input and basically turns it into a numerical code for each word.

175
00:08:16,520 --> 00:08:20,480
And then that goes into the texting coder, which is our clipping beddings.

176
00:08:20,480 --> 00:08:22,680
So that's the bit where it sort of works out the context.

177
00:08:22,680 --> 00:08:26,360
Yeah, that's the transformer that goes, well, OK, this one kind of

178
00:08:26,360 --> 00:08:29,120
goes with this word and then this means they share weights and so on.

179
00:08:29,120 --> 00:08:32,040
And then you go through the transform and you end up with essentially

180
00:08:32,120 --> 00:08:36,960
to us, meaningless numbers, but to this semantic information

181
00:08:36,960 --> 00:08:39,560
on the meaning of the sentence.

182
00:08:39,560 --> 00:08:43,240
We also, because if you remember, we put it through the network twice,

183
00:08:43,400 --> 00:08:45,560
one with the text embeddings and one without.

184
00:08:45,560 --> 00:08:48,720
So we also have to produce a dummy text embedding with nothing in it.

185
00:08:48,920 --> 00:08:51,200
Right. And that's what this unconditioned input is.

186
00:08:51,240 --> 00:08:55,880
Then we're going to text encoder unconditioned embeddings and we get

187
00:08:56,400 --> 00:09:00,160
two text embeddings, one of which is unconditioned and one of which is conditioned.

188
00:09:00,240 --> 00:09:02,040
Right. So this one has fogs on stilts.

189
00:09:02,040 --> 00:09:03,760
This one is just sort of blank.

190
00:09:03,760 --> 00:09:05,040
Now we need to set our scheduler.

191
00:09:05,040 --> 00:09:08,040
Remember, you can choose a scheduler that produces different amounts of noise

192
00:09:08,040 --> 00:09:12,280
at each time step and which one you use will depend on to an extent

193
00:09:12,280 --> 00:09:15,160
the kind of images you want out, but also how you've trained the network.

194
00:09:15,320 --> 00:09:18,520
Where is going to be using the standard one that came with stable diffusion?

195
00:09:18,720 --> 00:09:20,960
And I'm going to run for 50 time steps.

196
00:09:21,160 --> 00:09:26,320
So what this will do is distribute the amount of noise it adds from zero to 50.

197
00:09:26,520 --> 00:09:29,440
Right. So when I say 50, it's going to produce a maximum amount of noise.

198
00:09:29,440 --> 00:09:31,680
And when I say one, it's going to produce a tiny amount of noise.

199
00:09:31,960 --> 00:09:33,400
Right. That's the idea.

200
00:09:33,400 --> 00:09:39,520
And then we're going to actually produce our latent noise that we're going to be diffusing.

201
00:09:39,600 --> 00:09:44,440
So we create a random array of numbers of the right size and we're going to call these

202
00:09:44,440 --> 00:09:46,840
latents and we're going to stick them on the graphics card.

203
00:09:46,840 --> 00:09:49,360
And then we're going to do some scaling to our latents as well,

204
00:09:49,360 --> 00:09:52,960
because the scales of some of these different parts of a network of

205
00:09:52,960 --> 00:09:54,360
difference, you have to move them in and out.

206
00:09:54,440 --> 00:09:55,320
And then we're nearly done.

207
00:09:55,400 --> 00:09:56,200
Like this is our loop.

208
00:09:56,480 --> 00:09:57,880
So how does the loop work?

209
00:09:57,880 --> 00:10:05,360
Well, the first thing we do is we calculate the noise to be added at this particular

210
00:10:05,560 --> 00:10:06,160
iterations.

211
00:10:06,160 --> 00:10:09,240
We're going through all the different time steps and we're going to add a different

212
00:10:09,240 --> 00:10:09,880
amount of noise.

213
00:10:09,920 --> 00:10:13,440
We're going to add this noise to our latent space.

214
00:10:13,480 --> 00:10:13,720
Right.

215
00:10:13,720 --> 00:10:15,360
So basically we're noising up the image here.

216
00:10:15,400 --> 00:10:19,400
Now remember, this is an embedded version of this image, but it is noised.

217
00:10:19,440 --> 00:10:21,440
Then we're going to predict the noise with our unit.

218
00:10:21,440 --> 00:10:26,040
So that is saying how much noise do you think was in this image, such that we can

219
00:10:26,040 --> 00:10:30,160
get back to the original image, bearing in mind this text, and then we can do our

220
00:10:30,160 --> 00:10:31,960
actual classifier for your guidance.

221
00:10:32,000 --> 00:10:32,240
Right.

222
00:10:32,240 --> 00:10:35,640
So what we're going to do is we're going to take our noise prediction with text and

223
00:10:35,640 --> 00:10:37,120
our noise prediction without text.

224
00:10:37,320 --> 00:10:39,280
We're going to calculate the difference and amplify it.

225
00:10:39,680 --> 00:10:43,160
And then we're going to work out what our official noise prediction is.

226
00:10:43,320 --> 00:10:48,120
And then finally, we're going to then use that noise prediction to calculate a

227
00:10:48,120 --> 00:10:51,400
slightly less noise version of the image, which is what this line does here.

228
00:10:51,760 --> 00:10:53,120
And we're going to repeat this process.

229
00:10:53,400 --> 00:10:53,640
Right.

230
00:10:53,640 --> 00:10:54,760
So we repeat the process.

231
00:10:54,880 --> 00:10:57,080
We calculate the new noise at the next time step.

232
00:10:57,440 --> 00:10:58,480
We predict it.

233
00:10:58,880 --> 00:11:01,280
We subtract it away and add a bit more noise.

234
00:11:01,320 --> 00:11:02,360
And we repeat this process.

235
00:11:02,360 --> 00:11:09,160
And the idea is that over 50 iterations, we go from fully noise to some reasonable image.

236
00:11:09,880 --> 00:11:10,440
Shall we see?

237
00:11:11,000 --> 00:11:13,840
OK, so let's run at this resolution.

238
00:11:13,840 --> 00:11:16,520
I'm pushing the amount of image size I can get on this graphics card.

239
00:11:16,520 --> 00:11:18,480
So this is running on your graphics card here.

240
00:11:18,520 --> 00:11:21,880
No, this is running on Google's graphics card over at Google.

241
00:11:22,200 --> 00:11:22,440
Right.

242
00:11:22,440 --> 00:11:23,520
Give me somewhere in London.

243
00:11:23,520 --> 00:11:25,200
Do I owe you another eight pounds for this?

244
00:11:26,040 --> 00:11:26,880
You can give me eight pounds.

245
00:11:26,880 --> 00:11:29,400
No, this is covered under the original eight pounds per month.

246
00:11:30,480 --> 00:11:32,040
But hopefully this won't take a month to record.

247
00:11:32,520 --> 00:11:35,880
So we're choosing 50 iterations for this and because that's a decent amount.

248
00:11:35,920 --> 00:11:36,240
Right.

249
00:11:36,240 --> 00:11:38,840
You'll notice that if you don't do enough iterations, you're trying to move the

250
00:11:38,840 --> 00:11:39,640
noise too quickly.

251
00:11:39,640 --> 00:11:42,080
It becomes a bit unstable, doesn't produce nice results.

252
00:11:42,760 --> 00:11:44,200
Because I've not done this before.

253
00:11:44,200 --> 00:11:45,480
I don't know what these other results will be.

254
00:11:45,480 --> 00:11:46,440
Will it be frogs on stilts?

255
00:11:46,440 --> 00:11:48,240
Will it be bits of wood next to a frog?

256
00:11:48,680 --> 00:11:50,760
Will it be something different because it's failed horribly?

257
00:11:51,720 --> 00:11:52,320
Let's see.

258
00:11:54,960 --> 00:11:55,640
Actually, that's not bad.

259
00:11:58,880 --> 00:11:59,720
That is pretty impressive.

260
00:11:59,720 --> 00:12:02,800
Now there's a weird leg coming out of this fog here, but I would I would say

261
00:12:02,800 --> 00:12:05,040
that is a comparatively successful attempt.

262
00:12:05,080 --> 00:12:07,360
This was produced from a noisy image.

263
00:12:08,120 --> 00:12:12,040
So what we can do is we can change the noise seeds so we can say, you know,

264
00:12:12,040 --> 00:12:16,280
128 and what that will do is create a complete different noise, which will

265
00:12:16,280 --> 00:12:17,800
probably lead to a tiny different image.

266
00:12:17,920 --> 00:12:18,240
Right.

267
00:12:18,240 --> 00:12:21,240
I mean, it's still the same text prompt, so it's still guided in the same way.

268
00:12:21,760 --> 00:12:25,200
But if this allows us to produce see near infinite numbers, basically,

269
00:12:25,200 --> 00:12:27,400
of frogs on stilts, if that's your thing, right?

270
00:12:27,720 --> 00:12:28,480
It is my thing.

271
00:12:28,480 --> 00:12:32,320
Yeah, I've got quite into producing like cityscape, futuristic cityscape.

272
00:12:32,320 --> 00:12:34,680
So I think that's where I spend most of my time on this.

273
00:12:38,720 --> 00:12:41,080
I mean, that's gone a bit wrong, but actually still not bad.

274
00:12:41,080 --> 00:12:42,480
It looks like a kind of stage.

275
00:12:43,440 --> 00:12:44,720
They're just a bit not foggy.

276
00:12:45,360 --> 00:12:46,640
Although yeah.

277
00:12:46,640 --> 00:12:47,400
All right, all right.

278
00:12:47,400 --> 00:12:51,480
OK, so anyway, we could spend, let's say another 20, 30 minutes

279
00:12:51,480 --> 00:12:54,720
producing frogs on stilts, but yeah.

280
00:12:54,720 --> 00:12:56,840
So what you could there's loads of cool stuff you can do.

281
00:12:56,840 --> 00:12:58,720
Presumably you could just automate that.

282
00:12:58,720 --> 00:13:00,200
So it just kept giving you loads of.

283
00:13:00,200 --> 00:13:01,280
And in fact, I've done that right.

284
00:13:01,280 --> 00:13:04,160
So for example, I created some nice pictures of dystopian,

285
00:13:04,160 --> 00:13:07,400
abandoned, futuristic cities with overgrown plants, right?

286
00:13:07,400 --> 00:13:10,440
And then I just put them in a for loop and just produce 200 of them

287
00:13:10,440 --> 00:13:11,360
so I can pick the nice ones.

288
00:13:11,400 --> 00:13:16,360
For example, in here, I've just got a bunch of awesome looking

289
00:13:16,360 --> 00:13:18,520
city vistas, overgrown of plants.

290
00:13:18,520 --> 00:13:20,000
They all look really, really good, right?

291
00:13:20,000 --> 00:13:20,840
I'm quite pleased.

292
00:13:20,840 --> 00:13:24,000
I mean, I've got no use for this, but it's quite fun.

293
00:13:24,000 --> 00:13:27,040
And the other thing is that because you can do image to image guidance,

294
00:13:27,040 --> 00:13:30,360
right, so what you do is you take an image that's your guide image,

295
00:13:30,360 --> 00:13:34,080
you nearly noise it all the way and then you reconstruct, right?

296
00:13:34,080 --> 00:13:37,240
So the noise is somewhat not come from a random place.

297
00:13:37,240 --> 00:13:39,760
Then you can get an image that sort of bears some reflections.

298
00:13:39,760 --> 00:13:41,920
You can say, well, I want a building over here and a tree over here.

299
00:13:41,920 --> 00:13:46,480
So I'll draw them in and then I'll produce this and it will bear the same

300
00:13:46,480 --> 00:13:47,960
that have the same shapes and stuff.

301
00:13:47,960 --> 00:13:50,320
So you can control this process, even if you basically, like me,

302
00:13:50,320 --> 00:13:53,240
have absolute zero artistic ability at all.

303
00:13:53,240 --> 00:13:57,160
To give you an example, what I did was, so if I go down,

304
00:13:57,160 --> 00:13:58,040
let me, let me go.

305
00:13:58,040 --> 00:14:01,440
So this is a picture of my colleague's rabbit, very cute rabbit.

306
00:14:01,440 --> 00:14:05,960
And what I did was I embedded this added noise, but not totally noise

307
00:14:05,960 --> 00:14:07,080
to remove the image.

308
00:14:07,080 --> 00:14:10,160
And then I reconstructed it with the text wooden carving of a rabbit

309
00:14:10,160 --> 00:14:12,440
eating a leaf, highly detailed 4K artisan.

310
00:14:12,440 --> 00:14:13,880
I don't know if the artisan word does anything.

311
00:14:13,880 --> 00:14:15,040
I just thought it'd be fun.

312
00:14:15,040 --> 00:14:16,200
It's trending on art station.

313
00:14:16,200 --> 00:14:17,960
I see a lot of that put on the end of things.

314
00:14:17,960 --> 00:14:19,160
Does that make a difference?

315
00:14:19,160 --> 00:14:20,480
I don't know.

316
00:14:20,480 --> 00:14:24,280
Anyway, and it produce a wooden carving of a rabbit, right?

317
00:14:24,280 --> 00:14:27,040
And if you look at the original image versus this image,

318
00:14:27,040 --> 00:14:31,320
some things have changed, sure, but the shape is roughly the same, right?

319
00:14:31,320 --> 00:14:35,200
So it has guided this process using the original image.

320
00:14:35,280 --> 00:14:37,320
And that's how image to image works.

321
00:14:37,320 --> 00:14:40,360
So if you wanted to create an animation, you could create a quite simple

322
00:14:40,360 --> 00:14:45,240
animation of a rabbit jumping about with no artistic ability, right?

323
00:14:45,240 --> 00:14:48,960
I mean, actually, I would struggle to do even that, but, and then each frame,

324
00:14:48,960 --> 00:14:51,360
you could then use this process to produce it.

325
00:14:51,360 --> 00:14:54,720
At the moment, there's no kind of temporal consistency.

326
00:14:54,720 --> 00:14:55,920
So you will see flickering, right?

327
00:14:55,920 --> 00:14:59,440
If you ever see one of these videos, someone's produced online, it'll look cool,

328
00:14:59,440 --> 00:15:03,240
but maybe not consistent and interesting because each frame might subtly change

329
00:15:03,240 --> 00:15:05,040
things.

330
00:15:05,040 --> 00:15:07,440
But that's the idea, right?

331
00:15:07,440 --> 00:15:09,680
Now, you can do loads of weird stuff, right?

332
00:15:09,680 --> 00:15:11,800
So this mixed guidance is one of my favorite things.

333
00:15:11,800 --> 00:15:13,600
Here we have two text inputs.

334
00:15:13,600 --> 00:15:15,880
And what we're going to do is we're going to embed both of them.

335
00:15:15,880 --> 00:15:21,240
We're actually going to guide the generation using the midpoint of those two, right?

336
00:15:21,240 --> 00:15:24,040
So I'm going to say, OK, I want a rabbit, right?

337
00:15:24,040 --> 00:15:25,960
And I want a frog.

338
00:15:25,960 --> 00:15:29,800
And I want you to produce me a 50-50 rabbit frog, right?

339
00:15:29,800 --> 00:15:32,440
And what it will do is it'll embed both of them and it will do

340
00:15:32,440 --> 00:15:33,440
the exact same process.

341
00:15:33,440 --> 00:15:37,760
It's just that now, its text prompt is halfway between these two embeddings.

342
00:15:37,760 --> 00:15:42,640
So you could potentially come up with a system with sliders, you know, 10-70 frog.

343
00:15:42,640 --> 00:15:45,440
You could, to what amount of frog do you want in this image, right?

344
00:15:45,440 --> 00:15:50,840
I mean, again, not sure what the use case is, but it's quite cool.

345
00:15:50,840 --> 00:15:51,440
Here we go.

346
00:15:51,440 --> 00:15:54,240
So it only takes about, I think, I'm training for 50 steps again.

347
00:15:54,240 --> 00:15:55,560
So I'm running it for 50 steps.

348
00:15:55,560 --> 00:15:56,760
While this work, you could do loads of stuff.

349
00:15:56,760 --> 00:16:00,560
So for example, you could generate an image and then you could take half of it

350
00:16:00,560 --> 00:16:02,960
and try and generate the other half to expand it outwards

351
00:16:02,960 --> 00:16:06,720
and slowly grow your image to make it even higher as one, right?

352
00:16:06,720 --> 00:16:08,360
If you're limited by the resolution.

353
00:16:08,360 --> 00:16:11,360
And there's going to be a lot of people playing around with a lot of different ways to use this.

354
00:16:11,360 --> 00:16:16,480
I've already seen the plugins for Gimp and for Photoshop and stuff.

355
00:16:16,480 --> 00:16:17,440
There it is.

356
00:16:17,440 --> 00:16:18,400
It's a strange one.

357
00:16:18,400 --> 00:16:20,040
We'll put links to the code in the description.

358
00:16:20,040 --> 00:16:20,800
Have a go.

359
00:16:20,800 --> 00:16:23,880
You need to register for Huggingface to get access to the weights originally.

360
00:16:23,880 --> 00:16:27,840
But then you can use something like Google Colab or your own hardware to generate pictures.

361
00:16:27,840 --> 00:16:29,040
And people are having a lot of fun.

362
00:16:29,080 --> 00:16:33,080
There are websites now where you can find cool images and the prompts that we use to generate them

363
00:16:33,080 --> 00:16:34,240
to give you some ideas.

364
00:16:34,240 --> 00:16:35,640
So there's lots of cool stuff to do.

365
00:16:35,640 --> 00:16:37,800
And the rabbit, it's the same shape rabbit.

366
00:16:37,800 --> 00:16:39,320
There's a bit more noise, right?

367
00:16:39,320 --> 00:16:42,280
And then we come over here and we come over here

368
00:16:42,280 --> 00:16:44,360
and we end up with just noise.

369
00:16:44,360 --> 00:16:45,440
It looks like nonsense.

370
00:16:45,440 --> 00:16:46,120
And so the question is...

371
00:16:46,120 --> 00:16:49,080
It takes the same amount of time to make one sandwich, but you've got two people doing it.

372
00:16:49,080 --> 00:16:51,720
So they make twice as many sandwiches each time they make a sandwich.

373
00:16:51,720 --> 00:16:52,560
Same with the computer.

374
00:16:52,560 --> 00:16:55,160
We could either make the computer processor faster or...

