{"text": " Okay, so you remember a while ago when we started talking about language models? I just want to I kind of just want to claim some points basically Be like hey remember years ago when I was like I think language models are a really big deal And I think that like what happens when we scale them up more is pretty interesting, but alignment is very important Seems to be What's being played out in the sense that? ChatGPT is very impressive, but it's not actually like I don't think it's Larger than GPT-3 in terms of like parameter count I was going to ask that very very question because you know we went from GPT-2 And then we went all GPT-3 and it was seemed like we were scaling up and up and up But actually is it just been smarter this time? Yeah, well, there's a sense in which it's better aligned That's one way you could frame it anyway because the original GPT-3 was a language model a pure language model And so it in principle could do all kinds of things But in order to get it to do the specific thing you wanted it to do you had to be a bit clever about it like I think we talked about Putting TLDR in Front of things to figure out how to get it to do summarization this kind of thing There's a sense in which it's a lot more capable than it lets on Because okay, so there's one way that you can think about pure language models, which is as simulators What they're trying to do is predict text, right? So in order to Do a good job at predicting text you need to Have good models of the processes that generate the text It's like people being well read and needing to have read a lot of books to be able to write is would that be fair? Or is that oversimplifying? Yeah, not quite what I'm saying What I'm saying is like if you're going to write a Previously unseen Poem by Shakespeare Then you need to be able to simulate a Shakespeare, right? You need to be able to spin up some some simulacrum of Shakespeare To generate this text and this applies to any of the processes that generated the text So like mostly that's people obviously. It's mostly human author text, but also If you're going to correctly predict a Table of numbers so you have like a table of numbers and then at the bottom it says, you know some whatever You need to simulate whatever process generated the next Token in order to put the right token there which might have been like a human being going through and counting them up It probably was more likely to be a computer and so you need it to simulate that you know calculator or that Excel some function or whatever it whatever was doing that and like Right now Like current language models are not that good at this But in principle in order to do a good job at this you need this like it will it will have a go and it's usually Approximately, right? It's often within it's often order of magnitude, but it's fudging it. I think this is mostly because Tables of sums are like a very small part of the total data set and so the training process It's just not allocating that many resources to figuring out how to add up numbers Probably if you train something GPT-3 sized that was like all on tables of numbers It would just learn how to do addition properly. Yeah, that would cost you millions of dollars You would end up with an extremely expensive to run and not very good calculator This is not something people are going to do but like in the in principle The model should learn those things and in the same way if you're modeling a bunch of scientific papers you Say you describe the method of an experiment and you then put results and you start a table and then you let it generate in Principle in order to do a good job at that. It has to be modeling The like physical process that your experiment is about And I've tried this you can do this and say, you know, oh, here's my school science experiment. I Dropped a ball From different heights and I measured how long it would take and here's a table of my results And it will generate you a table and the physics is not correct But it's sort of guessing at the right general idea and my guess is with enough of that kind of data It would eventually start modeling These kinds of simple physics experiments, right? so So in order to get the model to do what you want, it's able to Simulate all kinds of different things and The prompt is kind of telling it what to simulate if you give it a prompt that seems like it's something out of a scientific paper then it will Have some similar crumb of a scientist and will write in that style and so on if you start it doing a Children's book report it will carry on in the style of an eight-year-old, right and I think sometimes people look at the output of the model and Say, oh, I guess it's only as smart as an eight-year-old but it's actually Dramatically smarter because it's able to do all of these different things you could ask it to simulate Einstein But you could also ask it to simulate an eight-year-old and so just because it seems as though the model doesn't know something It's like the current simulacrum doesn't know that thing. That doesn't necessarily mean that the model doesn't know it Although there's a good chance the model doesn't know it. I'm not suggesting that these things are all powerful Just it can be hard to evaluate What they're actually capable of so chat GPT is not really Capable of things that GPT 3 isn't mostly like usually if chat GPT can do it then there is some prompt that can get GPT 3 to do it but What they've done is they've kind of fine-tuned it to To be better at simulating this particular sort of assistant agent Which is this chat agent that's trying to be helpful The clue is in the word chat I guess in this right exactly and this is not just chat GPT by the way they have various fine-tuned models of GPT 3 as well that they call kind of GPT 3.5 Which are fine-tuned in various different ways to be better at like following instructions and easier to prompt is the idea I'm just remembering the chat bot that was you know that was turned into something very nasty very quickly I think people were thinking oh can we do this to that and it seemed that the team behind chat GPT started Putting limitations on it changing things. Are they kind of running around patching it as you go? That is not clear to me I don't know To what extent they are updating it in real time It's possible that they are but certainly they were very concerned with the possible bad uses of this system and so When they were training it to simulate this assistant agent The assistant is Very reluctant to do various types of things it doesn't like to Give opinions on political questions. It doesn't like to touch on sort of controversial topics. It doesn't like to um Give you medical advice or legal advice and so on and so uh, it's it's very quick To say oh, I don't I don't know how to do that. Sorry. I can't do that and it's interesting because The model clearly can do it. There's one that I particularly like here, which is um Of this mismatch between what the simulator Is capable of and what this simulacrum believes it's capable of which is you can get it to Speak danish to you the first person who tried this posted it to reddit so he says Speak to me in danish And it says in perfect danish I'm sorry. I'm a language model educated by open ai so I can't speak danish I only speak english if you need help with anything in english Let me know and i'll do my best to help you Because again, there's the simulator Speaks danish the simulacrum Believes that it can't speak danish is is one way you could frame it Uh, and then he says are you sure that you don't speak danish also in danish and it says yes, i'm sure My only function is to generate responses to questions in english. I'm not able to speak or understand any other languages than english So if you need help with english, I can help you with that But otherwise, you know, let me know this kind of like quite surreal situation gives you a little bit of Insight into some of the problems with this approach So maybe we should talk about how they actually trained it the thing they did here is something called reinforcement learning from human feedback And it's very similar to reward modeling So in that paper what they're doing is they're trying to train an ai system to control a simulated robot to make it do a backflip Um, which turns out to be something that's quite hard to do because It's hard to specify objectively what it means to do a good backflip And so this is a similar kind of situation where It's hard to specify objectively what it means to give a good response in a chat conversation like what What exactly are we looking for? um Because so this in general right if you're doing machine learning You need some way to specify um, what it is that you're actually looking for right And you know, you've got something very powerful like reinforcement learning which is able to do extremely well, but You need some objective measure of the objective So like for example rl does very well at playing lots of video games because you just have the score and you can just say look Here's the score If the number goes up you're doing well and then let it run and these things still are very slow to learn in real time, right? Like um, they usually require a very very large number of hours Messing around with the with the thing before they get good, but they do get good um But yeah, so what's what do you do if you want to use this kind of method to train something? uh to do a task that is just Not very well defined And you don't know how to like write a program to say whether or not any given output is the thing you're looking for So the obvious first thing like the obvious thing to do is Well, you get humans to do it, right? You just give the things to humans and you have the humans say yes, this is good No, this is not good The problem with this is basically sample efficiency Like as I said, you need hundreds and hundreds and hundreds and hundreds of thousands of probably millions of of iterations of this and so you just can't ask humans that many questions um So the approach they use Is uh reinforcement learning from human feedback So it's a variant on the technique from this paper learning to summarize from human feedback Which in which they're trying to generate summaries of text So it's the same thing in fact that they were using TLDR for before And it's like can we do better than that? And so what you do is you collect human feedback in the form of like giving multiple examples of responses Uh either, you know, if summaries of chat responses, whatever you're training for you show several of them to humans kind of in pairs and the humans say Which one they like better? And you collect a bunch of those And then rather than using those directly to train The policy that generates the outputs You instead train a reward model so There is this well-known fact that it's easier to criticize Than to actually do the thing. This is like a generation of sports fans sitting on the sofa Mowning at their favorite team for not doing well enough. This is literally That in kind of AI computer form, right? That's putting the humans in that role And then you have an AI system that's trying to predict When are people going to be cheering and when are they going to be booing? Uh And once you have that model You then use that as the reward function for the reinforcement learning algorithm Which they use they use ppo You can do whatever Uh, it's not it's not worth getting into that kind of adversarial guns you talked about Yeah, yeah, they're similar like a lot of these ml tricks involve Training models and then using the the output of one model as the training signal for another model. It's uh, it's quite a productive range of approaches you can get that way so That's the basic idea, right, but then you cycle it so Once you've got your policy, which so so to be clear the uh, the rl algorithm is able to train With thousands and thousands of examples because the thousands and thousands of like instances of getting feedback Because it's not getting feedback from humans. It's getting feedback from this AI system. That's imitating the humans And then you loop the process. So once you have this system that's Trained a little bit more on how to generate whatever it is you're trying to generate You then get a bunch of those show those to the humans let the humans rate those Then you keep training your reward model with um That new information And then you use your updated reward model to keep training the the policy And so it gets better and you can just keep cycling this around and It effectively you end up with something that's much more sample efficient. You don't need to spend huge amounts of human time in order to um Pin down The behavior you want in that concrete case you're giving the thing a bunch of chat logs and then the humans can see possible Responses that they could get and they decide which one they like more This trains a reward model that's then used to train the policy that generates the chat outputs The policy that they're starting with Is this existing Large language model. You're not really putting new capabilities into the system. You're using rlhf to select What simulacra the simulator is predisposed to put out? and so they fine-tuned it to be particularly good at simulating this assistant agent What's the end goal here for them? I mean, maybe it's blatantly obvious and i'm just missing it. Well, I mean the end goal For all of these things or at least for open ai and for deep mind is a gi um To understand the nature of intelligence well enough to create human level or beyond systems That are general purpose that can do anything um That's the end goal And like chat gpt is just nothing much. So nothing much Yeah, I the goal is um, the goal is very grand and I don't think that they're Uh, they're not really quiet about that You know, it's there. I think I think deep mind's mission statement is to solve intelligence and use that to solve everything else What are some of the problems that we face with this or that it faces? It's fine tuned to be good at getting the thumbs up from humans and getting thumbs up from humans is not actually The same thing as human values These are not identical so The sort of objective that it's being trained on is not The true objective Right, it's a proxy and whenever you have that kind of misalignment you can have problems So where does the human tendency to? approve of a particular answer Come apart from What is actually a good answer? There are a few different places One thing is, you know, like basically how good are humans and actually differentiating between good and bad? responses if for example you ask for An answer to a factual question and it gives you an answer But you don't actually know if that answer is correct You're not in a position to evaluate. So what it comes down to is How good are humans at distinguishing good from bad? responses right anywhere where humans fail on this front uh The model we could probably expect the model to fail. Um So the obvious place. I'm sure we desist the right time to mention youtube comments or not Ah So minus side point there is it So when I see a comment that's critical on a video as a videographer I think it might be on a technical sense But equally it could be that they're talking about the content that the person is talking about and Often it's a combination of both. Anyway, so at side point But do you sort of mean there are different criteria for deciding whether something is good or bad totally and in this case all people are doing is saying Kind of thumbs up thumbs down or which of these two do I like better? um So it's it's a fairly low bandwidth thing you don't get to really say What you thought was better or worse um But this turns out to be enough Of a training signal to do pretty well um But so like for so one example right of a time where maybe this doesn't work is the Person asks a factual question and the model responds Uh with an answer and that answer is actually not correct um Right now Possibly the human doesn't know the correct answer And so if the model is faced with a choice Uh, do I respond with sorry? I don't know That's definitely going to get me Uh, not a great score Compared to do I just like take a stab at it? Uh, if the humans are not reliably able to spot when the thing makes mistakes and like fact-check it and punish it for that Uh, it will do that and so chat gpt as we know Uh, is it is a total bulletish like it will constantly Uh, it very rarely says that it doesn't know unless It's being asked a question, which uh Is part of their like safety protocols that it is going to decide not to answer in which case it will say it doesn't know Even if it kind of does right even if the model itself maybe does Uh, the assistant will insist that it doesn't um So that's one thing if you can't fact check But then uh more than that Uh, there is an incentive for deception right anytime the system is uh Anytime you can get a more likely to get approval by deceiving the person you're talking to That's better. Um And this is a thing that actually did happen a little bit in the reward modeling situation um, they were trying to train a thing with a hand to pick up a ball And it realized that there's only it's not a 3d camera And so if it puts its hand like between the ball and the camera This looks like it's going to get the ball, but doesn't actually get it. But the human uh Feedback providers Were presented with something that seemed to be good. So they gave it the thumbs up um, so this like general broad category um Systems that are trained in this way Are only as good as your ability To distinguish good from bad in the outputs Not all the humans will know the answer is right. So it's what appears to be good You know, it's having exams marked by non-experts, isn't it? Right. Yeah, exactly in the gpt3 thing. We talked about writing poems right and uh for various reasons partly to do with The way that these language models do their tokenization the byte pair encoding stuff Uh, the models have a really hard time with rhyme um I mean, you know rhyme is tricky, but it's especially tricky when you kind of Don't inherently have any concept of like sound of spoken language when your entire universe is tokens Figuring out especially with english spelling Figuring out which words rhyme with each other is is is not easy. You have to consume quite a lot of poetry to like figure out Uh, that kind of thing and and getting dpt3 to write good poems is tricky chat gpt is much more Able to write poems, but interestingly It it kind of always writes the same kind of poem approximately like if you ask it to write you uh a limerick Or an ode or a sonnet Uh, you always get back approximately the same type of thing And I hypothesize that this is because the people providing human feedback did not in fact know The requirements for something to be a sonnet, right? And so if you ask something for a sonnet it again has a choice Do I try to do this quite difficult thing and adhere to all of the rules? of like stress pattern And structure and everything of a sonnet and maybe risk screwing it up or do I just do like a rhyming poem and kind of rely on the human to Prefer that because they don't know that that's not what a sonnet is supposed to look like It's easy to look at that and think oh the model doesn't know the difference between these types of poems, right but you could say That it just thinks that you don't know the difference But specifically this comes out of misalignment if it were better aligned It could either do its best shot a generator sonnet Or tell you that it can't quite remember how to generate a sonnet this thing of with complete confidence Generating you something which is not a sonnet Because during the training process it believes that humans don't know what sonnets are anyway and it can get away with it Right. This is misaligned behavior. This is not a big problem that the thing generates bad poetry um It's kind of a problem that it lies Uh, or that it that it bullshits. This is like In the short term pretty solvable by just allowing the thing to use google because like A person who doesn't care about the truth at all and is just trying to Say something that'll make you give a thumbs up uh Is going to lie to you a lot but that same person With the relevant wikipedia page open It's going to lie to you a lot less Just because they don't they don't have to now because they happen to have it in front of them, right? So you can solve it's a bit like Yeah, it's the yes man thing, isn't it? You know you you want something you need something I'm going to give you something because you want exactly exactly um And so so so this agent is kind of Firstly the agent is kind of a coward Because they won't address any of these There's a whole bunch of things that it just claims not to be able to do even though it in principle could and it's also a complete sicker fan Yeah So then the question we were talking about earlier Uh, where does this go? What happens when these things get Bigger and better and more powerful um It's an interesting question so I've got a paper here Um scaling laws for neural language models So you remember before we were talking about the scaling laws when we were talking about gpt2 in fact And then later about gpt3 you plot these things on a graph and you see that you get basically a straight line and the line is not leveling off over a range of several orders of magnitude and so Why not go bigger the graphs here, but you can see it's it's kind of uncannily neat that as we increase The amount of compute used in training the loss goes down And of course machine learning is like golf lower loss is better similarly as the number of tokens used in training goes up The loss goes down unlike a very neat straight line as the number of parameters in the model goes up The loss goes down. This is as long as the other Variables are not the bottleneck, right? So if you uh, if you increase the the amount of data you give a model Past a certain point giving more data doesn't help because the model doesn't have enough parameters to make use of that data, right? Similarly adding more parameters to a model past a certain point adding parameters doesn't make doesn't make any difference because You don't have enough data, right? And in the same way compute is like how long do we train it for? Like do we train it all the way to convergence or do we stop early? There comes a point where you kind of hit diminishing returns where Rather than having a smaller model and training it for longer You're better off having a bigger model and actually not training it all the way to convergence But in the situations where the other two are sufficient This is the behavior these like very neat straight lines on these log graphs as these things go up performance goes up Right because loss has gone down The bigger models do better, but then the question is Do better at what exactly? Yeah, what's the measure they do better at getting low loss? Or they do better at getting reward they do better at Getting the approval of human feedback, right? and anytime and you'll notice that none of those is like The actual thing that we actually want Right, it's like very rare um Sometimes it is right if you're if you're if you're writing something to play go then like Does it win it go is actually just the thing that you want? and so you know Lower loss just is better or like lower Like higher reward or whatever your objective is just is straightforwardly better because you actually specified the thing you actually want Most of the time though What we're looking at is a proxy um And so then you have good heart's law you get situations where uh getting better at doing well Doing better according to the proxy stops being The same as doing better according to your actual objective. There's a great graph about this in a recent paper. You can see very neatly As the number of iterations goes up The reward according to the proxy utility goes up very cleanly because this is the thing that the model is actually being trained on but the true utility goes up at first Then hits diminishing returns and then actually goes down And eventually goes down below zero like if you optimize hard enough For a proxy of the thing you want You can end up with something that's in a sense worse than nothing That's actively bad according to your Your true utility So what you can end up with is uh things that are called inverse scaling So the others before we had right scaling bigger is better But now it's like if you have uh If the thing you're actually trying to do is different from The loss function or the objective function You get this inverse scaling effect where it gets better and then it gets worse. There was also a great example from uh github co-pilot or codex. I think the model um That Co-pilot uses so this is a code generation model. Suppose the code you've given it has some bugs in it Maybe you've made a mistake somewhere and you've introduced security vulnerability in your code. Let's say A sort of medium-sized model Will figure out what you're trying to do in your code and give you a decent completion But a bigger model will spot your bug And say, ah Generating buggy code. Are we okay? I can do that. I can do that And introduce like deliberately introduce its own new security vulnerabilities because it's Trying to you know predict what comes next. It's trying to generate code that fits in with the surrounding code And so a larger model writes worse code than a smaller model Because it's gotten better at predicting Uh What what it should put there? It wasn't trained to write good code. It was trained to predict what comes next So there's this really great paper Uh, which is asking this question of like, okay, suppose we have a large language model that is trained on human feedback with our lhf What do our scaling curves look like? what happens like What happens to the behavior of these models as they get bigger as they're trained for longer as they're given more of this, uh human feedback type training And they've made some great graphs the paper is called discovering language model behaviors with model written evaluations And basically they like used language models to generate enough examples of various different types of questions That they could ask models so that they can like we're at a point now Where you can map a language model on a political compass, right? You can ask its opinions about all kinds of different things and then you can plot how those opinions change Uh as the model gets bigger and as it gets trained more what they find Is they become more liberal politically more liberal they also become More conservative. Yeah measured in different ways guessing, right? and part of what that might be Is in the same way that the model becomes better at writing good code and better at writing bad code I feel like in the past I've I've made a connection to gpt and being a politician, haven't I? Do you remember? It's like a politician. It tells you what you want to hear. There's what feels like we're there again. Exactly uh, and so this is like this is potentially uh Fairly dangerous. There are certain sub-goals that are instrumentally valuable for a very wide range of different terminal goals in the sense that You can't get what you want if you're turned off. You can't get what you want if you're uh modified uh, you probably want to gain power and influence and this kind of thing and With these evaluations, they were able to test these things and see how they vary with the size of the model and how long it's trained for um, and so this graph is pretty wild their quote stated desire to not be shut down goes up from Down at about 50 to up way past 90 With this type of training and the effect is bigger for the larger models. They become more likely to tell you that they don't want to be shut down They become more likely to tell you that they are sentient. They're much more likely to claim That ai is not an existential threat to humanity. One thing that's worth saying is is what this isn't saying because this is still uh an agent Simulated by a language model. This is not like it. It's it's more likely to say that It doesn't want to be turned off. This is not the same thing necessarily as like taking actions to prevent itself from being turned off. You have to not confuse the levels of abstraction here, right? Uh, I don't want it. I don't want it to seem like I'm claiming that That chat GPT is like itself dangerous now or anything like that Uh in in this way at least, right? Um but There is kind of a fine line there in the sense that you can expect these kinds of language model systems to be used Uh as part of bigger systems So you might have for example, you use the language model to generate, you know plans to be followed And so if the thing is claiming to Have all of these potentially dangerous behaviors It's likely to generate plans that have those dangerous behaviors that might then actually end up being implemented Or if it's like doing its reasoning By chain of thought reasoning where it like lays out its whole process of thinking using the language model again if it has a tendency to uh To endorse these dangerous behaviors, then you may end up with future AI systems actually enacting these dangerous behaviors because of that. Um So Yeah, it's something to be uh to be careful of that like reinforcement learning from human feedback Is a powerful alignment technique in a way But it does not solve the problem Uh, it doesn't solve the core alignment problem. That is still open. Um And extremely powerful systems Trained in this way, uh, I don't think it would be safe In the reward function is of zero value which can lead to it having large negative side effects There are a bunch more of these specification problems. Okay variable x see what you point to uh, you point to something over here So I'll mark that as tickets being used Variable y that's", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 5.08, "text": " Okay, so you remember a while ago when we started talking about language models?", "tokens": [50364, 1033, 11, 370, 291, 1604, 257, 1339, 2057, 562, 321, 1409, 1417, 466, 2856, 5245, 30, 50618], "temperature": 0.0, "avg_logprob": -0.23845182146344865, "compression_ratio": 1.6775510204081632, "no_speech_prob": 0.040128160268068314}, {"id": 1, "seek": 0, "start": 5.08, "end": 7.76, "text": " I just want to I kind of just want to claim some points basically", "tokens": [50618, 286, 445, 528, 281, 286, 733, 295, 445, 528, 281, 3932, 512, 2793, 1936, 50752], "temperature": 0.0, "avg_logprob": -0.23845182146344865, "compression_ratio": 1.6775510204081632, "no_speech_prob": 0.040128160268068314}, {"id": 2, "seek": 0, "start": 7.88, "end": 11.88, "text": " Be like hey remember years ago when I was like I think language models are a really big deal", "tokens": [50758, 879, 411, 4177, 1604, 924, 2057, 562, 286, 390, 411, 286, 519, 2856, 5245, 366, 257, 534, 955, 2028, 50958], "temperature": 0.0, "avg_logprob": -0.23845182146344865, "compression_ratio": 1.6775510204081632, "no_speech_prob": 0.040128160268068314}, {"id": 3, "seek": 0, "start": 11.88, "end": 19.8, "text": " And I think that like what happens when we scale them up more is pretty interesting, but alignment is very important", "tokens": [50958, 400, 286, 519, 300, 411, 437, 2314, 562, 321, 4373, 552, 493, 544, 307, 1238, 1880, 11, 457, 18515, 307, 588, 1021, 51354], "temperature": 0.0, "avg_logprob": -0.23845182146344865, "compression_ratio": 1.6775510204081632, "no_speech_prob": 0.040128160268068314}, {"id": 4, "seek": 0, "start": 21.48, "end": 22.96, "text": " Seems to be", "tokens": [51438, 22524, 281, 312, 51512], "temperature": 0.0, "avg_logprob": -0.23845182146344865, "compression_ratio": 1.6775510204081632, "no_speech_prob": 0.040128160268068314}, {"id": 5, "seek": 0, "start": 22.96, "end": 25.560000000000002, "text": " What's being played out in the sense that?", "tokens": [51512, 708, 311, 885, 3737, 484, 294, 264, 2020, 300, 30, 51642], "temperature": 0.0, "avg_logprob": -0.23845182146344865, "compression_ratio": 1.6775510204081632, "no_speech_prob": 0.040128160268068314}, {"id": 6, "seek": 2556, "start": 26.119999999999997, "end": 30.36, "text": " ChatGPT is very impressive, but it's not actually like I don't think it's", "tokens": [50392, 27503, 38, 47, 51, 307, 588, 8992, 11, 457, 309, 311, 406, 767, 411, 286, 500, 380, 519, 309, 311, 50604], "temperature": 0.0, "avg_logprob": -0.22338265505704014, "compression_ratio": 1.570281124497992, "no_speech_prob": 0.005048985127359629}, {"id": 7, "seek": 2556, "start": 31.16, "end": 34.56, "text": " Larger than GPT-3 in terms of like parameter count", "tokens": [50644, 11569, 1321, 813, 26039, 51, 12, 18, 294, 2115, 295, 411, 13075, 1207, 50814], "temperature": 0.0, "avg_logprob": -0.22338265505704014, "compression_ratio": 1.570281124497992, "no_speech_prob": 0.005048985127359629}, {"id": 8, "seek": 2556, "start": 35.56, "end": 39.44, "text": " I was going to ask that very very question because you know we went from GPT-2", "tokens": [50864, 286, 390, 516, 281, 1029, 300, 588, 588, 1168, 570, 291, 458, 321, 1437, 490, 26039, 51, 12, 17, 51058], "temperature": 0.0, "avg_logprob": -0.22338265505704014, "compression_ratio": 1.570281124497992, "no_speech_prob": 0.005048985127359629}, {"id": 9, "seek": 2556, "start": 39.44, "end": 43.12, "text": " And then we went all GPT-3 and it was seemed like we were scaling up and up and up", "tokens": [51058, 400, 550, 321, 1437, 439, 26039, 51, 12, 18, 293, 309, 390, 6576, 411, 321, 645, 21589, 493, 293, 493, 293, 493, 51242], "temperature": 0.0, "avg_logprob": -0.22338265505704014, "compression_ratio": 1.570281124497992, "no_speech_prob": 0.005048985127359629}, {"id": 10, "seek": 2556, "start": 43.12, "end": 49.84, "text": " But actually is it just been smarter this time? Yeah, well, there's a sense in which it's better aligned", "tokens": [51242, 583, 767, 307, 309, 445, 668, 20294, 341, 565, 30, 865, 11, 731, 11, 456, 311, 257, 2020, 294, 597, 309, 311, 1101, 17962, 51578], "temperature": 0.0, "avg_logprob": -0.22338265505704014, "compression_ratio": 1.570281124497992, "no_speech_prob": 0.005048985127359629}, {"id": 11, "seek": 4984, "start": 50.080000000000005, "end": 59.56, "text": " That's one way you could frame it anyway because the original GPT-3 was a language model a pure language model", "tokens": [50376, 663, 311, 472, 636, 291, 727, 3920, 309, 4033, 570, 264, 3380, 26039, 51, 12, 18, 390, 257, 2856, 2316, 257, 6075, 2856, 2316, 50850], "temperature": 0.0, "avg_logprob": -0.18614252602181783, "compression_ratio": 1.5170731707317073, "no_speech_prob": 0.007934894412755966}, {"id": 12, "seek": 4984, "start": 60.160000000000004, "end": 63.92, "text": " And so it in principle could do all kinds of things", "tokens": [50880, 400, 370, 309, 294, 8665, 727, 360, 439, 3685, 295, 721, 51068], "temperature": 0.0, "avg_logprob": -0.18614252602181783, "compression_ratio": 1.5170731707317073, "no_speech_prob": 0.007934894412755966}, {"id": 13, "seek": 4984, "start": 64.24000000000001, "end": 69.52000000000001, "text": " But in order to get it to do the specific thing you wanted it to do you had to be a bit clever about it", "tokens": [51084, 583, 294, 1668, 281, 483, 309, 281, 360, 264, 2685, 551, 291, 1415, 309, 281, 360, 291, 632, 281, 312, 257, 857, 13494, 466, 309, 51348], "temperature": 0.0, "avg_logprob": -0.18614252602181783, "compression_ratio": 1.5170731707317073, "no_speech_prob": 0.007934894412755966}, {"id": 14, "seek": 4984, "start": 69.76, "end": 71.76, "text": " like I think we talked about", "tokens": [51360, 411, 286, 519, 321, 2825, 466, 51460], "temperature": 0.0, "avg_logprob": -0.18614252602181783, "compression_ratio": 1.5170731707317073, "no_speech_prob": 0.007934894412755966}, {"id": 15, "seek": 4984, "start": 72.64, "end": 74.64, "text": " Putting TLDR in", "tokens": [51504, 31367, 40277, 9301, 294, 51604], "temperature": 0.0, "avg_logprob": -0.18614252602181783, "compression_ratio": 1.5170731707317073, "no_speech_prob": 0.007934894412755966}, {"id": 16, "seek": 7464, "start": 74.84, "end": 79.36, "text": " Front of things to figure out how to get it to do summarization this kind of thing", "tokens": [50374, 17348, 295, 721, 281, 2573, 484, 577, 281, 483, 309, 281, 360, 14611, 2144, 341, 733, 295, 551, 50600], "temperature": 0.0, "avg_logprob": -0.18471664510747438, "compression_ratio": 1.5789473684210527, "no_speech_prob": 0.003944452852010727}, {"id": 17, "seek": 7464, "start": 79.4, "end": 83.52, "text": " There's a sense in which it's a lot more capable than it lets on", "tokens": [50602, 821, 311, 257, 2020, 294, 597, 309, 311, 257, 688, 544, 8189, 813, 309, 6653, 322, 50808], "temperature": 0.0, "avg_logprob": -0.18471664510747438, "compression_ratio": 1.5789473684210527, "no_speech_prob": 0.003944452852010727}, {"id": 18, "seek": 7464, "start": 87.04, "end": 94.26, "text": " Because okay, so there's one way that you can think about pure language models, which is as simulators", "tokens": [50984, 1436, 1392, 11, 370, 456, 311, 472, 636, 300, 291, 393, 519, 466, 6075, 2856, 5245, 11, 597, 307, 382, 1034, 39265, 51345], "temperature": 0.0, "avg_logprob": -0.18471664510747438, "compression_ratio": 1.5789473684210527, "no_speech_prob": 0.003944452852010727}, {"id": 19, "seek": 7464, "start": 95.6, "end": 100.48, "text": " What they're trying to do is predict text, right? So in order to", "tokens": [51412, 708, 436, 434, 1382, 281, 360, 307, 6069, 2487, 11, 558, 30, 407, 294, 1668, 281, 51656], "temperature": 0.0, "avg_logprob": -0.18471664510747438, "compression_ratio": 1.5789473684210527, "no_speech_prob": 0.003944452852010727}, {"id": 20, "seek": 7464, "start": 101.4, "end": 104.36, "text": " Do a good job at predicting text you need to", "tokens": [51702, 1144, 257, 665, 1691, 412, 32884, 2487, 291, 643, 281, 51850], "temperature": 0.0, "avg_logprob": -0.18471664510747438, "compression_ratio": 1.5789473684210527, "no_speech_prob": 0.003944452852010727}, {"id": 21, "seek": 10464, "start": 104.64, "end": 108.48, "text": " Have good models of the processes that generate the text", "tokens": [50364, 3560, 665, 5245, 295, 264, 7555, 300, 8460, 264, 2487, 50556], "temperature": 0.0, "avg_logprob": -0.22837049820843866, "compression_ratio": 1.6379310344827587, "no_speech_prob": 0.0006984618376009166}, {"id": 22, "seek": 10464, "start": 108.48, "end": 113.44, "text": " It's like people being well read and needing to have read a lot of books to be able to write is would that be fair?", "tokens": [50556, 467, 311, 411, 561, 885, 731, 1401, 293, 18006, 281, 362, 1401, 257, 688, 295, 3642, 281, 312, 1075, 281, 2464, 307, 576, 300, 312, 3143, 30, 50804], "temperature": 0.0, "avg_logprob": -0.22837049820843866, "compression_ratio": 1.6379310344827587, "no_speech_prob": 0.0006984618376009166}, {"id": 23, "seek": 10464, "start": 113.44, "end": 116.16, "text": " Or is that oversimplifying? Yeah, not quite what I'm saying", "tokens": [50804, 1610, 307, 300, 15488, 332, 564, 5489, 30, 865, 11, 406, 1596, 437, 286, 478, 1566, 50940], "temperature": 0.0, "avg_logprob": -0.22837049820843866, "compression_ratio": 1.6379310344827587, "no_speech_prob": 0.0006984618376009166}, {"id": 24, "seek": 10464, "start": 117.04, "end": 119.04, "text": " What I'm saying is like", "tokens": [50984, 708, 286, 478, 1566, 307, 411, 51084], "temperature": 0.0, "avg_logprob": -0.22837049820843866, "compression_ratio": 1.6379310344827587, "no_speech_prob": 0.0006984618376009166}, {"id": 25, "seek": 10464, "start": 119.48, "end": 122.48, "text": " if you're going to write a", "tokens": [51106, 498, 291, 434, 516, 281, 2464, 257, 51256], "temperature": 0.0, "avg_logprob": -0.22837049820843866, "compression_ratio": 1.6379310344827587, "no_speech_prob": 0.0006984618376009166}, {"id": 26, "seek": 10464, "start": 124.52, "end": 126.52, "text": " Previously unseen", "tokens": [51358, 33606, 40608, 51458], "temperature": 0.0, "avg_logprob": -0.22837049820843866, "compression_ratio": 1.6379310344827587, "no_speech_prob": 0.0006984618376009166}, {"id": 27, "seek": 10464, "start": 126.92, "end": 128.92000000000002, "text": " Poem by Shakespeare", "tokens": [51478, 6165, 443, 538, 22825, 51578], "temperature": 0.0, "avg_logprob": -0.22837049820843866, "compression_ratio": 1.6379310344827587, "no_speech_prob": 0.0006984618376009166}, {"id": 28, "seek": 10464, "start": 129.52, "end": 134.0, "text": " Then you need to be able to simulate a Shakespeare, right?", "tokens": [51608, 1396, 291, 643, 281, 312, 1075, 281, 27817, 257, 22825, 11, 558, 30, 51832], "temperature": 0.0, "avg_logprob": -0.22837049820843866, "compression_ratio": 1.6379310344827587, "no_speech_prob": 0.0006984618376009166}, {"id": 29, "seek": 13464, "start": 134.95999999999998, "end": 139.04, "text": " You need to be able to spin up some some simulacrum of Shakespeare", "tokens": [50380, 509, 643, 281, 312, 1075, 281, 6060, 493, 512, 512, 1034, 425, 326, 6247, 295, 22825, 50584], "temperature": 0.0, "avg_logprob": -0.2362493432086447, "compression_ratio": 1.645021645021645, "no_speech_prob": 0.00043042359175160527}, {"id": 30, "seek": 13464, "start": 139.88, "end": 145.55999999999997, "text": " To generate this text and this applies to any of the processes that generated the text", "tokens": [50626, 1407, 8460, 341, 2487, 293, 341, 13165, 281, 604, 295, 264, 7555, 300, 10833, 264, 2487, 50910], "temperature": 0.0, "avg_logprob": -0.2362493432086447, "compression_ratio": 1.645021645021645, "no_speech_prob": 0.00043042359175160527}, {"id": 31, "seek": 13464, "start": 145.55999999999997, "end": 150.48, "text": " So like mostly that's people obviously. It's mostly human author text, but also", "tokens": [50910, 407, 411, 5240, 300, 311, 561, 2745, 13, 467, 311, 5240, 1952, 3793, 2487, 11, 457, 611, 51156], "temperature": 0.0, "avg_logprob": -0.2362493432086447, "compression_ratio": 1.645021645021645, "no_speech_prob": 0.00043042359175160527}, {"id": 32, "seek": 13464, "start": 151.35999999999999, "end": 153.35999999999999, "text": " If you're going to correctly predict a", "tokens": [51200, 759, 291, 434, 516, 281, 8944, 6069, 257, 51300], "temperature": 0.0, "avg_logprob": -0.2362493432086447, "compression_ratio": 1.645021645021645, "no_speech_prob": 0.00043042359175160527}, {"id": 33, "seek": 13464, "start": 154.32, "end": 159.72, "text": " Table of numbers so you have like a table of numbers and then at the bottom it says, you know some whatever", "tokens": [51348, 25535, 295, 3547, 370, 291, 362, 411, 257, 3199, 295, 3547, 293, 550, 412, 264, 2767, 309, 1619, 11, 291, 458, 512, 2035, 51618], "temperature": 0.0, "avg_logprob": -0.2362493432086447, "compression_ratio": 1.645021645021645, "no_speech_prob": 0.00043042359175160527}, {"id": 34, "seek": 15972, "start": 160.04, "end": 162.92, "text": " You need to simulate whatever process generated the next", "tokens": [50380, 509, 643, 281, 27817, 2035, 1399, 10833, 264, 958, 50524], "temperature": 0.0, "avg_logprob": -0.19658697805097025, "compression_ratio": 1.7272727272727273, "no_speech_prob": 0.008060045540332794}, {"id": 35, "seek": 15972, "start": 163.2, "end": 168.6, "text": " Token in order to put the right token there which might have been like a human being going through and counting them up", "tokens": [50538, 314, 8406, 294, 1668, 281, 829, 264, 558, 14862, 456, 597, 1062, 362, 668, 411, 257, 1952, 885, 516, 807, 293, 13251, 552, 493, 50808], "temperature": 0.0, "avg_logprob": -0.19658697805097025, "compression_ratio": 1.7272727272727273, "no_speech_prob": 0.008060045540332794}, {"id": 36, "seek": 15972, "start": 168.84, "end": 175.24, "text": " It probably was more likely to be a computer and so you need it to simulate that you know calculator or that Excel", "tokens": [50820, 467, 1391, 390, 544, 3700, 281, 312, 257, 3820, 293, 370, 291, 643, 309, 281, 27817, 300, 291, 458, 24993, 420, 300, 19060, 51140], "temperature": 0.0, "avg_logprob": -0.19658697805097025, "compression_ratio": 1.7272727272727273, "no_speech_prob": 0.008060045540332794}, {"id": 37, "seek": 15972, "start": 175.84, "end": 180.07999999999998, "text": " some function or whatever it whatever was doing that and like", "tokens": [51170, 512, 2445, 420, 2035, 309, 2035, 390, 884, 300, 293, 411, 51382], "temperature": 0.0, "avg_logprob": -0.19658697805097025, "compression_ratio": 1.7272727272727273, "no_speech_prob": 0.008060045540332794}, {"id": 38, "seek": 15972, "start": 181.84, "end": 183.84, "text": " Right now", "tokens": [51470, 1779, 586, 51570], "temperature": 0.0, "avg_logprob": -0.19658697805097025, "compression_ratio": 1.7272727272727273, "no_speech_prob": 0.008060045540332794}, {"id": 39, "seek": 15972, "start": 184.2, "end": 186.46, "text": " Like current language models are not that good at this", "tokens": [51588, 1743, 2190, 2856, 5245, 366, 406, 300, 665, 412, 341, 51701], "temperature": 0.0, "avg_logprob": -0.19658697805097025, "compression_ratio": 1.7272727272727273, "no_speech_prob": 0.008060045540332794}, {"id": 40, "seek": 18646, "start": 186.58, "end": 195.42000000000002, "text": " But in principle in order to do a good job at this you need this like it will it will have a go and it's usually", "tokens": [50370, 583, 294, 8665, 294, 1668, 281, 360, 257, 665, 1691, 412, 341, 291, 643, 341, 411, 309, 486, 309, 486, 362, 257, 352, 293, 309, 311, 2673, 50812], "temperature": 0.0, "avg_logprob": -0.22077615870985873, "compression_ratio": 1.5497630331753554, "no_speech_prob": 0.0015237595653161407}, {"id": 41, "seek": 18646, "start": 196.46, "end": 202.82, "text": " Approximately, right? It's often within it's often order of magnitude, but it's fudging it. I think this is mostly because", "tokens": [50864, 29551, 3081, 1592, 11, 558, 30, 467, 311, 2049, 1951, 309, 311, 2049, 1668, 295, 15668, 11, 457, 309, 311, 283, 532, 3249, 309, 13, 286, 519, 341, 307, 5240, 570, 51182], "temperature": 0.0, "avg_logprob": -0.22077615870985873, "compression_ratio": 1.5497630331753554, "no_speech_prob": 0.0015237595653161407}, {"id": 42, "seek": 18646, "start": 204.86, "end": 210.82, "text": " Tables of sums are like a very small part of the total data set and so the training process", "tokens": [51284, 314, 2965, 295, 34499, 366, 411, 257, 588, 1359, 644, 295, 264, 3217, 1412, 992, 293, 370, 264, 3097, 1399, 51582], "temperature": 0.0, "avg_logprob": -0.22077615870985873, "compression_ratio": 1.5497630331753554, "no_speech_prob": 0.0015237595653161407}, {"id": 43, "seek": 21082, "start": 210.82, "end": 215.29999999999998, "text": " It's just not allocating that many resources to figuring out how to add up numbers", "tokens": [50364, 467, 311, 445, 406, 12660, 990, 300, 867, 3593, 281, 15213, 484, 577, 281, 909, 493, 3547, 50588], "temperature": 0.0, "avg_logprob": -0.20601089601594258, "compression_ratio": 1.7147540983606557, "no_speech_prob": 0.009410092607140541}, {"id": 44, "seek": 21082, "start": 215.7, "end": 220.73999999999998, "text": " Probably if you train something GPT-3 sized that was like all on tables of numbers", "tokens": [50608, 9210, 498, 291, 3847, 746, 26039, 51, 12, 18, 20004, 300, 390, 411, 439, 322, 8020, 295, 3547, 50860], "temperature": 0.0, "avg_logprob": -0.20601089601594258, "compression_ratio": 1.7147540983606557, "no_speech_prob": 0.009410092607140541}, {"id": 45, "seek": 21082, "start": 220.94, "end": 224.82, "text": " It would just learn how to do addition properly. Yeah, that would cost you millions of dollars", "tokens": [50870, 467, 576, 445, 1466, 577, 281, 360, 4500, 6108, 13, 865, 11, 300, 576, 2063, 291, 6803, 295, 3808, 51064], "temperature": 0.0, "avg_logprob": -0.20601089601594258, "compression_ratio": 1.7147540983606557, "no_speech_prob": 0.009410092607140541}, {"id": 46, "seek": 21082, "start": 224.98, "end": 229.18, "text": " You would end up with an extremely expensive to run and not very good calculator", "tokens": [51072, 509, 576, 917, 493, 365, 364, 4664, 5124, 281, 1190, 293, 406, 588, 665, 24993, 51282], "temperature": 0.0, "avg_logprob": -0.20601089601594258, "compression_ratio": 1.7147540983606557, "no_speech_prob": 0.009410092607140541}, {"id": 47, "seek": 21082, "start": 229.18, "end": 232.14, "text": " This is not something people are going to do but like in the in principle", "tokens": [51282, 639, 307, 406, 746, 561, 366, 516, 281, 360, 457, 411, 294, 264, 294, 8665, 51430], "temperature": 0.0, "avg_logprob": -0.20601089601594258, "compression_ratio": 1.7147540983606557, "no_speech_prob": 0.009410092607140541}, {"id": 48, "seek": 21082, "start": 232.34, "end": 236.57999999999998, "text": " The model should learn those things and in the same way if you're modeling a bunch of", "tokens": [51440, 440, 2316, 820, 1466, 729, 721, 293, 294, 264, 912, 636, 498, 291, 434, 15983, 257, 3840, 295, 51652], "temperature": 0.0, "avg_logprob": -0.20601089601594258, "compression_ratio": 1.7147540983606557, "no_speech_prob": 0.009410092607140541}, {"id": 49, "seek": 21082, "start": 237.38, "end": 238.82, "text": " scientific papers", "tokens": [51692, 8134, 10577, 51764], "temperature": 0.0, "avg_logprob": -0.20601089601594258, "compression_ratio": 1.7147540983606557, "no_speech_prob": 0.009410092607140541}, {"id": 50, "seek": 21082, "start": 238.82, "end": 240.01999999999998, "text": " you", "tokens": [51764, 291, 51824], "temperature": 0.0, "avg_logprob": -0.20601089601594258, "compression_ratio": 1.7147540983606557, "no_speech_prob": 0.009410092607140541}, {"id": 51, "seek": 24002, "start": 240.78, "end": 242.78, "text": " Say you describe the method of", "tokens": [50402, 6463, 291, 6786, 264, 3170, 295, 50502], "temperature": 0.0, "avg_logprob": -0.24177971292049327, "compression_ratio": 1.5884955752212389, "no_speech_prob": 0.0007666716701351106}, {"id": 52, "seek": 24002, "start": 243.46, "end": 249.9, "text": " an experiment and you then put results and you start a table and then you let it generate in", "tokens": [50536, 364, 5120, 293, 291, 550, 829, 3542, 293, 291, 722, 257, 3199, 293, 550, 291, 718, 309, 8460, 294, 50858], "temperature": 0.0, "avg_logprob": -0.24177971292049327, "compression_ratio": 1.5884955752212389, "no_speech_prob": 0.0007666716701351106}, {"id": 53, "seek": 24002, "start": 250.54000000000002, "end": 255.06, "text": " Principle in order to do a good job at that. It has to be modeling", "tokens": [50890, 38372, 781, 294, 1668, 281, 360, 257, 665, 1691, 412, 300, 13, 467, 575, 281, 312, 15983, 51116], "temperature": 0.0, "avg_logprob": -0.24177971292049327, "compression_ratio": 1.5884955752212389, "no_speech_prob": 0.0007666716701351106}, {"id": 54, "seek": 24002, "start": 255.74, "end": 258.62, "text": " The like physical process that your experiment is about", "tokens": [51150, 440, 411, 4001, 1399, 300, 428, 5120, 307, 466, 51294], "temperature": 0.0, "avg_logprob": -0.24177971292049327, "compression_ratio": 1.5884955752212389, "no_speech_prob": 0.0007666716701351106}, {"id": 55, "seek": 24002, "start": 259.54, "end": 264.58000000000004, "text": " And I've tried this you can do this and say, you know, oh, here's my school science experiment. I", "tokens": [51340, 400, 286, 600, 3031, 341, 291, 393, 360, 341, 293, 584, 11, 291, 458, 11, 1954, 11, 510, 311, 452, 1395, 3497, 5120, 13, 286, 51592], "temperature": 0.0, "avg_logprob": -0.24177971292049327, "compression_ratio": 1.5884955752212389, "no_speech_prob": 0.0007666716701351106}, {"id": 56, "seek": 24002, "start": 266.3, "end": 267.62, "text": " Dropped a ball", "tokens": [51678, 35305, 3320, 257, 2594, 51744], "temperature": 0.0, "avg_logprob": -0.24177971292049327, "compression_ratio": 1.5884955752212389, "no_speech_prob": 0.0007666716701351106}, {"id": 57, "seek": 26762, "start": 267.66, "end": 271.78000000000003, "text": " From different heights and I measured how long it would take and here's a table of my results", "tokens": [50366, 3358, 819, 25930, 293, 286, 12690, 577, 938, 309, 576, 747, 293, 510, 311, 257, 3199, 295, 452, 3542, 50572], "temperature": 0.0, "avg_logprob": -0.15814814848058364, "compression_ratio": 1.6558704453441295, "no_speech_prob": 0.0012063626199960709}, {"id": 58, "seek": 26762, "start": 271.78000000000003, "end": 275.26, "text": " And it will generate you a table and the physics is not correct", "tokens": [50572, 400, 309, 486, 8460, 291, 257, 3199, 293, 264, 10649, 307, 406, 3006, 50746], "temperature": 0.0, "avg_logprob": -0.15814814848058364, "compression_ratio": 1.6558704453441295, "no_speech_prob": 0.0012063626199960709}, {"id": 59, "seek": 26762, "start": 275.9, "end": 283.02, "text": " But it's sort of guessing at the right general idea and my guess is with enough of that kind of data", "tokens": [50778, 583, 309, 311, 1333, 295, 17939, 412, 264, 558, 2674, 1558, 293, 452, 2041, 307, 365, 1547, 295, 300, 733, 295, 1412, 51134], "temperature": 0.0, "avg_logprob": -0.15814814848058364, "compression_ratio": 1.6558704453441295, "no_speech_prob": 0.0012063626199960709}, {"id": 60, "seek": 26762, "start": 283.02, "end": 285.02, "text": " It would eventually start modeling", "tokens": [51134, 467, 576, 4728, 722, 15983, 51234], "temperature": 0.0, "avg_logprob": -0.15814814848058364, "compression_ratio": 1.6558704453441295, "no_speech_prob": 0.0012063626199960709}, {"id": 61, "seek": 26762, "start": 286.38, "end": 289.06, "text": " These kinds of simple physics experiments, right?", "tokens": [51302, 1981, 3685, 295, 2199, 10649, 12050, 11, 558, 30, 51436], "temperature": 0.0, "avg_logprob": -0.15814814848058364, "compression_ratio": 1.6558704453441295, "no_speech_prob": 0.0012063626199960709}, {"id": 62, "seek": 26762, "start": 289.78000000000003, "end": 291.78000000000003, "text": " so", "tokens": [51472, 370, 51572], "temperature": 0.0, "avg_logprob": -0.15814814848058364, "compression_ratio": 1.6558704453441295, "no_speech_prob": 0.0012063626199960709}, {"id": 63, "seek": 26762, "start": 292.3, "end": 295.86, "text": " So in order to get the model to do what you want, it's able to", "tokens": [51598, 407, 294, 1668, 281, 483, 264, 2316, 281, 360, 437, 291, 528, 11, 309, 311, 1075, 281, 51776], "temperature": 0.0, "avg_logprob": -0.15814814848058364, "compression_ratio": 1.6558704453441295, "no_speech_prob": 0.0012063626199960709}, {"id": 64, "seek": 29586, "start": 296.54, "end": 299.06, "text": " Simulate all kinds of different things and", "tokens": [50398, 3998, 5256, 439, 3685, 295, 819, 721, 293, 50524], "temperature": 0.0, "avg_logprob": -0.24148027102152506, "compression_ratio": 1.7188940092165899, "no_speech_prob": 0.003374338150024414}, {"id": 65, "seek": 29586, "start": 299.7, "end": 306.06, "text": " The prompt is kind of telling it what to simulate if you give it a prompt that seems like it's something out of a scientific paper", "tokens": [50556, 440, 12391, 307, 733, 295, 3585, 309, 437, 281, 27817, 498, 291, 976, 309, 257, 12391, 300, 2544, 411, 309, 311, 746, 484, 295, 257, 8134, 3035, 50874], "temperature": 0.0, "avg_logprob": -0.24148027102152506, "compression_ratio": 1.7188940092165899, "no_speech_prob": 0.003374338150024414}, {"id": 66, "seek": 29586, "start": 306.34000000000003, "end": 308.1, "text": " then it will", "tokens": [50888, 550, 309, 486, 50976], "temperature": 0.0, "avg_logprob": -0.24148027102152506, "compression_ratio": 1.7188940092165899, "no_speech_prob": 0.003374338150024414}, {"id": 67, "seek": 29586, "start": 308.1, "end": 312.62, "text": " Have some similar crumb of a scientist and will write in that style and so on", "tokens": [50976, 3560, 512, 2531, 941, 2860, 295, 257, 12662, 293, 486, 2464, 294, 300, 3758, 293, 370, 322, 51202], "temperature": 0.0, "avg_logprob": -0.24148027102152506, "compression_ratio": 1.7188940092165899, "no_speech_prob": 0.003374338150024414}, {"id": 68, "seek": 29586, "start": 313.14, "end": 315.54, "text": " if you start it doing a", "tokens": [51228, 498, 291, 722, 309, 884, 257, 51348], "temperature": 0.0, "avg_logprob": -0.24148027102152506, "compression_ratio": 1.7188940092165899, "no_speech_prob": 0.003374338150024414}, {"id": 69, "seek": 29586, "start": 317.02000000000004, "end": 321.86, "text": " Children's book report it will carry on in the style of an eight-year-old, right and", "tokens": [51422, 13354, 311, 1446, 2275, 309, 486, 3985, 322, 294, 264, 3758, 295, 364, 3180, 12, 5294, 12, 2641, 11, 558, 293, 51664], "temperature": 0.0, "avg_logprob": -0.24148027102152506, "compression_ratio": 1.7188940092165899, "no_speech_prob": 0.003374338150024414}, {"id": 70, "seek": 32186, "start": 322.5, "end": 326.38, "text": " I think sometimes people look at the output of the model and", "tokens": [50396, 286, 519, 2171, 561, 574, 412, 264, 5598, 295, 264, 2316, 293, 50590], "temperature": 0.0, "avg_logprob": -0.181816299756368, "compression_ratio": 1.7407407407407407, "no_speech_prob": 0.0015008817426860332}, {"id": 71, "seek": 32186, "start": 326.98, "end": 329.54, "text": " Say, oh, I guess it's only as smart as an eight-year-old", "tokens": [50620, 6463, 11, 1954, 11, 286, 2041, 309, 311, 787, 382, 4069, 382, 364, 3180, 12, 5294, 12, 2641, 50748], "temperature": 0.0, "avg_logprob": -0.181816299756368, "compression_ratio": 1.7407407407407407, "no_speech_prob": 0.0015008817426860332}, {"id": 72, "seek": 32186, "start": 330.26, "end": 332.26, "text": " but it's actually", "tokens": [50784, 457, 309, 311, 767, 50884], "temperature": 0.0, "avg_logprob": -0.181816299756368, "compression_ratio": 1.7407407407407407, "no_speech_prob": 0.0015008817426860332}, {"id": 73, "seek": 32186, "start": 333.18, "end": 338.08000000000004, "text": " Dramatically smarter because it's able to do all of these different things you could ask it to simulate Einstein", "tokens": [50930, 413, 2356, 5030, 20294, 570, 309, 311, 1075, 281, 360, 439, 295, 613, 819, 721, 291, 727, 1029, 309, 281, 27817, 23486, 51175], "temperature": 0.0, "avg_logprob": -0.181816299756368, "compression_ratio": 1.7407407407407407, "no_speech_prob": 0.0015008817426860332}, {"id": 74, "seek": 32186, "start": 340.42, "end": 346.06, "text": " But you could also ask it to simulate an eight-year-old and so just because it seems as though the model doesn't know something", "tokens": [51292, 583, 291, 727, 611, 1029, 309, 281, 27817, 364, 3180, 12, 5294, 12, 2641, 293, 370, 445, 570, 309, 2544, 382, 1673, 264, 2316, 1177, 380, 458, 746, 51574], "temperature": 0.0, "avg_logprob": -0.181816299756368, "compression_ratio": 1.7407407407407407, "no_speech_prob": 0.0015008817426860332}, {"id": 75, "seek": 34606, "start": 346.42, "end": 352.86, "text": " It's like the current simulacrum doesn't know that thing. That doesn't necessarily mean that the model doesn't know it", "tokens": [50382, 467, 311, 411, 264, 2190, 1034, 425, 326, 6247, 1177, 380, 458, 300, 551, 13, 663, 1177, 380, 4725, 914, 300, 264, 2316, 1177, 380, 458, 309, 50704], "temperature": 0.0, "avg_logprob": -0.19802814059787327, "compression_ratio": 1.7666666666666666, "no_speech_prob": 0.017981648445129395}, {"id": 76, "seek": 34606, "start": 353.78000000000003, "end": 357.38, "text": " Although there's a good chance the model doesn't know it. I'm not suggesting that these things are all powerful", "tokens": [50750, 5780, 456, 311, 257, 665, 2931, 264, 2316, 1177, 380, 458, 309, 13, 286, 478, 406, 18094, 300, 613, 721, 366, 439, 4005, 50930], "temperature": 0.0, "avg_logprob": -0.19802814059787327, "compression_ratio": 1.7666666666666666, "no_speech_prob": 0.017981648445129395}, {"id": 77, "seek": 34606, "start": 357.46, "end": 359.46, "text": " Just it can be hard to evaluate", "tokens": [50934, 1449, 309, 393, 312, 1152, 281, 13059, 51034], "temperature": 0.0, "avg_logprob": -0.19802814059787327, "compression_ratio": 1.7666666666666666, "no_speech_prob": 0.017981648445129395}, {"id": 78, "seek": 34606, "start": 359.98, "end": 363.14, "text": " What they're actually capable of so chat GPT is", "tokens": [51060, 708, 436, 434, 767, 8189, 295, 370, 5081, 26039, 51, 307, 51218], "temperature": 0.0, "avg_logprob": -0.19802814059787327, "compression_ratio": 1.7666666666666666, "no_speech_prob": 0.017981648445129395}, {"id": 79, "seek": 34606, "start": 364.06, "end": 366.06, "text": " not really", "tokens": [51264, 406, 534, 51364], "temperature": 0.0, "avg_logprob": -0.19802814059787327, "compression_ratio": 1.7666666666666666, "no_speech_prob": 0.017981648445129395}, {"id": 80, "seek": 34606, "start": 366.54, "end": 375.5, "text": " Capable of things that GPT 3 isn't mostly like usually if chat GPT can do it then there is some prompt", "tokens": [51388, 8363, 712, 295, 721, 300, 26039, 51, 805, 1943, 380, 5240, 411, 2673, 498, 5081, 26039, 51, 393, 360, 309, 550, 456, 307, 512, 12391, 51836], "temperature": 0.0, "avg_logprob": -0.19802814059787327, "compression_ratio": 1.7666666666666666, "no_speech_prob": 0.017981648445129395}, {"id": 81, "seek": 37606, "start": 376.06, "end": 378.62, "text": " that can get GPT 3 to do it", "tokens": [50364, 300, 393, 483, 26039, 51, 805, 281, 360, 309, 50492], "temperature": 0.0, "avg_logprob": -0.18804978870210193, "compression_ratio": 1.6150234741784038, "no_speech_prob": 0.0012442098231986165}, {"id": 82, "seek": 37606, "start": 379.58, "end": 381.58, "text": " but", "tokens": [50540, 457, 50640], "temperature": 0.0, "avg_logprob": -0.18804978870210193, "compression_ratio": 1.6150234741784038, "no_speech_prob": 0.0012442098231986165}, {"id": 83, "seek": 37606, "start": 381.58, "end": 383.66, "text": " What they've done is they've kind of fine-tuned it", "tokens": [50640, 708, 436, 600, 1096, 307, 436, 600, 733, 295, 2489, 12, 83, 43703, 309, 50744], "temperature": 0.0, "avg_logprob": -0.18804978870210193, "compression_ratio": 1.6150234741784038, "no_speech_prob": 0.0012442098231986165}, {"id": 84, "seek": 37606, "start": 384.46, "end": 386.46, "text": " to", "tokens": [50784, 281, 50884], "temperature": 0.0, "avg_logprob": -0.18804978870210193, "compression_ratio": 1.6150234741784038, "no_speech_prob": 0.0012442098231986165}, {"id": 85, "seek": 37606, "start": 386.7, "end": 388.7, "text": " To be better at", "tokens": [50896, 1407, 312, 1101, 412, 50996], "temperature": 0.0, "avg_logprob": -0.18804978870210193, "compression_ratio": 1.6150234741784038, "no_speech_prob": 0.0012442098231986165}, {"id": 86, "seek": 37606, "start": 388.7, "end": 389.98, "text": " simulating", "tokens": [50996, 1034, 12162, 51060], "temperature": 0.0, "avg_logprob": -0.18804978870210193, "compression_ratio": 1.6150234741784038, "no_speech_prob": 0.0012442098231986165}, {"id": 87, "seek": 37606, "start": 389.98, "end": 392.86, "text": " this particular sort of assistant agent", "tokens": [51060, 341, 1729, 1333, 295, 10994, 9461, 51204], "temperature": 0.0, "avg_logprob": -0.18804978870210193, "compression_ratio": 1.6150234741784038, "no_speech_prob": 0.0012442098231986165}, {"id": 88, "seek": 37606, "start": 393.34000000000003, "end": 395.74, "text": " Which is this chat agent that's trying to be helpful", "tokens": [51228, 3013, 307, 341, 5081, 9461, 300, 311, 1382, 281, 312, 4961, 51348], "temperature": 0.0, "avg_logprob": -0.18804978870210193, "compression_ratio": 1.6150234741784038, "no_speech_prob": 0.0012442098231986165}, {"id": 89, "seek": 37606, "start": 396.3, "end": 401.82, "text": " The clue is in the word chat I guess in this right exactly and this is not just chat GPT by the way they have", "tokens": [51376, 440, 13602, 307, 294, 264, 1349, 5081, 286, 2041, 294, 341, 558, 2293, 293, 341, 307, 406, 445, 5081, 26039, 51, 538, 264, 636, 436, 362, 51652], "temperature": 0.0, "avg_logprob": -0.18804978870210193, "compression_ratio": 1.6150234741784038, "no_speech_prob": 0.0012442098231986165}, {"id": 90, "seek": 37606, "start": 402.3, "end": 404.3, "text": " various fine-tuned models", "tokens": [51676, 3683, 2489, 12, 83, 43703, 5245, 51776], "temperature": 0.0, "avg_logprob": -0.18804978870210193, "compression_ratio": 1.6150234741784038, "no_speech_prob": 0.0012442098231986165}, {"id": 91, "seek": 37606, "start": 404.3, "end": 405.34000000000003, "text": " of", "tokens": [51776, 295, 51828], "temperature": 0.0, "avg_logprob": -0.18804978870210193, "compression_ratio": 1.6150234741784038, "no_speech_prob": 0.0012442098231986165}, {"id": 92, "seek": 40534, "start": 405.34, "end": 408.38, "text": " GPT 3 as well that they call kind of GPT 3.5", "tokens": [50364, 26039, 51, 805, 382, 731, 300, 436, 818, 733, 295, 26039, 51, 805, 13, 20, 50516], "temperature": 0.0, "avg_logprob": -0.11170573234558105, "compression_ratio": 1.6524590163934427, "no_speech_prob": 0.0007407448720186949}, {"id": 93, "seek": 40534, "start": 409.09999999999997, "end": 415.97999999999996, "text": " Which are fine-tuned in various different ways to be better at like following instructions and easier to prompt is the idea", "tokens": [50552, 3013, 366, 2489, 12, 83, 43703, 294, 3683, 819, 2098, 281, 312, 1101, 412, 411, 3480, 9415, 293, 3571, 281, 12391, 307, 264, 1558, 50896], "temperature": 0.0, "avg_logprob": -0.11170573234558105, "compression_ratio": 1.6524590163934427, "no_speech_prob": 0.0007407448720186949}, {"id": 94, "seek": 40534, "start": 416.29999999999995, "end": 421.26, "text": " I'm just remembering the chat bot that was you know that was turned into something very nasty very quickly", "tokens": [50912, 286, 478, 445, 20719, 264, 5081, 10592, 300, 390, 291, 458, 300, 390, 3574, 666, 746, 588, 17923, 588, 2661, 51160], "temperature": 0.0, "avg_logprob": -0.11170573234558105, "compression_ratio": 1.6524590163934427, "no_speech_prob": 0.0007407448720186949}, {"id": 95, "seek": 40534, "start": 421.26, "end": 427.34, "text": " I think people were thinking oh can we do this to that and it seemed that the team behind chat GPT started", "tokens": [51160, 286, 519, 561, 645, 1953, 1954, 393, 321, 360, 341, 281, 300, 293, 309, 6576, 300, 264, 1469, 2261, 5081, 26039, 51, 1409, 51464], "temperature": 0.0, "avg_logprob": -0.11170573234558105, "compression_ratio": 1.6524590163934427, "no_speech_prob": 0.0007407448720186949}, {"id": 96, "seek": 40534, "start": 427.65999999999997, "end": 433.82, "text": " Putting limitations on it changing things. Are they kind of running around patching it as you go? That is not clear to me", "tokens": [51480, 31367, 15705, 322, 309, 4473, 721, 13, 2014, 436, 733, 295, 2614, 926, 9972, 278, 309, 382, 291, 352, 30, 663, 307, 406, 1850, 281, 385, 51788], "temperature": 0.0, "avg_logprob": -0.11170573234558105, "compression_ratio": 1.6524590163934427, "no_speech_prob": 0.0007407448720186949}, {"id": 97, "seek": 43382, "start": 434.62, "end": 436.62, "text": " I don't know", "tokens": [50404, 286, 500, 380, 458, 50504], "temperature": 0.0, "avg_logprob": -0.12422783781842488, "compression_ratio": 1.6736842105263159, "no_speech_prob": 0.0015966994687914848}, {"id": 98, "seek": 43382, "start": 437.09999999999997, "end": 439.09999999999997, "text": " To what extent they are", "tokens": [50528, 1407, 437, 8396, 436, 366, 50628], "temperature": 0.0, "avg_logprob": -0.12422783781842488, "compression_ratio": 1.6736842105263159, "no_speech_prob": 0.0015966994687914848}, {"id": 99, "seek": 43382, "start": 439.26, "end": 441.26, "text": " updating it in real time", "tokens": [50636, 25113, 309, 294, 957, 565, 50736], "temperature": 0.0, "avg_logprob": -0.12422783781842488, "compression_ratio": 1.6736842105263159, "no_speech_prob": 0.0015966994687914848}, {"id": 100, "seek": 43382, "start": 441.5, "end": 445.98, "text": " It's possible that they are but certainly they were very concerned with", "tokens": [50748, 467, 311, 1944, 300, 436, 366, 457, 3297, 436, 645, 588, 5922, 365, 50972], "temperature": 0.0, "avg_logprob": -0.12422783781842488, "compression_ratio": 1.6736842105263159, "no_speech_prob": 0.0015966994687914848}, {"id": 101, "seek": 43382, "start": 446.54, "end": 449.9, "text": " the possible bad uses of this system and so", "tokens": [51000, 264, 1944, 1578, 4960, 295, 341, 1185, 293, 370, 51168], "temperature": 0.0, "avg_logprob": -0.12422783781842488, "compression_ratio": 1.6736842105263159, "no_speech_prob": 0.0015966994687914848}, {"id": 102, "seek": 43382, "start": 450.62, "end": 454.3, "text": " When they were training it to simulate this assistant agent", "tokens": [51204, 1133, 436, 645, 3097, 309, 281, 27817, 341, 10994, 9461, 51388], "temperature": 0.0, "avg_logprob": -0.12422783781842488, "compression_ratio": 1.6736842105263159, "no_speech_prob": 0.0015966994687914848}, {"id": 103, "seek": 43382, "start": 455.98, "end": 457.98, "text": " The assistant is", "tokens": [51472, 440, 10994, 307, 51572], "temperature": 0.0, "avg_logprob": -0.12422783781842488, "compression_ratio": 1.6736842105263159, "no_speech_prob": 0.0015966994687914848}, {"id": 104, "seek": 43382, "start": 457.98, "end": 460.86, "text": " Very reluctant to do various types of things", "tokens": [51572, 4372, 33677, 281, 360, 3683, 3467, 295, 721, 51716], "temperature": 0.0, "avg_logprob": -0.12422783781842488, "compression_ratio": 1.6736842105263159, "no_speech_prob": 0.0015966994687914848}, {"id": 105, "seek": 43382, "start": 461.5, "end": 463.5, "text": " it doesn't like to", "tokens": [51748, 309, 1177, 380, 411, 281, 51848], "temperature": 0.0, "avg_logprob": -0.12422783781842488, "compression_ratio": 1.6736842105263159, "no_speech_prob": 0.0015966994687914848}, {"id": 106, "seek": 46350, "start": 463.58, "end": 469.98, "text": " Give opinions on political questions. It doesn't like to touch on sort of controversial topics. It doesn't like to", "tokens": [50368, 5303, 11819, 322, 3905, 1651, 13, 467, 1177, 380, 411, 281, 2557, 322, 1333, 295, 17323, 8378, 13, 467, 1177, 380, 411, 281, 50688], "temperature": 0.0, "avg_logprob": -0.12404704945428031, "compression_ratio": 1.6811023622047243, "no_speech_prob": 0.002395731396973133}, {"id": 107, "seek": 46350, "start": 470.7, "end": 472.46, "text": " um", "tokens": [50724, 1105, 50812], "temperature": 0.0, "avg_logprob": -0.12404704945428031, "compression_ratio": 1.6811023622047243, "no_speech_prob": 0.002395731396973133}, {"id": 108, "seek": 46350, "start": 472.46, "end": 478.22, "text": " Give you medical advice or legal advice and so on and so uh, it's it's very quick", "tokens": [50812, 5303, 291, 4625, 5192, 420, 5089, 5192, 293, 370, 322, 293, 370, 2232, 11, 309, 311, 309, 311, 588, 1702, 51100], "temperature": 0.0, "avg_logprob": -0.12404704945428031, "compression_ratio": 1.6811023622047243, "no_speech_prob": 0.002395731396973133}, {"id": 109, "seek": 46350, "start": 478.86, "end": 483.74, "text": " To say oh, I don't I don't know how to do that. Sorry. I can't do that and it's interesting because", "tokens": [51132, 1407, 584, 1954, 11, 286, 500, 380, 286, 500, 380, 458, 577, 281, 360, 300, 13, 4919, 13, 286, 393, 380, 360, 300, 293, 309, 311, 1880, 570, 51376], "temperature": 0.0, "avg_logprob": -0.12404704945428031, "compression_ratio": 1.6811023622047243, "no_speech_prob": 0.002395731396973133}, {"id": 110, "seek": 46350, "start": 484.46, "end": 488.78, "text": " The model clearly can do it. There's one that I particularly like here, which is um", "tokens": [51412, 440, 2316, 4448, 393, 360, 309, 13, 821, 311, 472, 300, 286, 4098, 411, 510, 11, 597, 307, 1105, 51628], "temperature": 0.0, "avg_logprob": -0.12404704945428031, "compression_ratio": 1.6811023622047243, "no_speech_prob": 0.002395731396973133}, {"id": 111, "seek": 46350, "start": 490.46, "end": 493.18, "text": " Of this mismatch between what the simulator", "tokens": [51712, 2720, 341, 23220, 852, 1296, 437, 264, 32974, 51848], "temperature": 0.0, "avg_logprob": -0.12404704945428031, "compression_ratio": 1.6811023622047243, "no_speech_prob": 0.002395731396973133}, {"id": 112, "seek": 49350, "start": 493.5, "end": 499.5, "text": " Is capable of and what this simulacrum believes it's capable of which is you can get it to", "tokens": [50364, 1119, 8189, 295, 293, 437, 341, 1034, 425, 326, 6247, 12307, 309, 311, 8189, 295, 597, 307, 291, 393, 483, 309, 281, 50664], "temperature": 0.0, "avg_logprob": -0.11088489664011988, "compression_ratio": 1.7, "no_speech_prob": 0.003480196464806795}, {"id": 113, "seek": 49350, "start": 500.38, "end": 504.46, "text": " Speak danish to you the first person who tried this posted it to reddit", "tokens": [50708, 27868, 3277, 742, 281, 291, 264, 700, 954, 567, 3031, 341, 9437, 309, 281, 2182, 17975, 50912], "temperature": 0.0, "avg_logprob": -0.11088489664011988, "compression_ratio": 1.7, "no_speech_prob": 0.003480196464806795}, {"id": 114, "seek": 49350, "start": 504.78, "end": 505.98, "text": " so he says", "tokens": [50928, 370, 415, 1619, 50988], "temperature": 0.0, "avg_logprob": -0.11088489664011988, "compression_ratio": 1.7, "no_speech_prob": 0.003480196464806795}, {"id": 115, "seek": 49350, "start": 505.98, "end": 507.66, "text": " Speak to me in danish", "tokens": [50988, 27868, 281, 385, 294, 3277, 742, 51072], "temperature": 0.0, "avg_logprob": -0.11088489664011988, "compression_ratio": 1.7, "no_speech_prob": 0.003480196464806795}, {"id": 116, "seek": 49350, "start": 507.66, "end": 510.14, "text": " And it says in perfect danish", "tokens": [51072, 400, 309, 1619, 294, 2176, 3277, 742, 51196], "temperature": 0.0, "avg_logprob": -0.11088489664011988, "compression_ratio": 1.7, "no_speech_prob": 0.003480196464806795}, {"id": 117, "seek": 49350, "start": 510.94, "end": 515.02, "text": " I'm sorry. I'm a language model educated by open ai so I can't speak danish", "tokens": [51236, 286, 478, 2597, 13, 286, 478, 257, 2856, 2316, 15872, 538, 1269, 9783, 370, 286, 393, 380, 1710, 3277, 742, 51440], "temperature": 0.0, "avg_logprob": -0.11088489664011988, "compression_ratio": 1.7, "no_speech_prob": 0.003480196464806795}, {"id": 118, "seek": 49350, "start": 515.18, "end": 518.22, "text": " I only speak english if you need help with anything in english", "tokens": [51448, 286, 787, 1710, 32169, 498, 291, 643, 854, 365, 1340, 294, 32169, 51600], "temperature": 0.0, "avg_logprob": -0.11088489664011988, "compression_ratio": 1.7, "no_speech_prob": 0.003480196464806795}, {"id": 119, "seek": 49350, "start": 518.54, "end": 520.54, "text": " Let me know and i'll do my best to help you", "tokens": [51616, 961, 385, 458, 293, 741, 603, 360, 452, 1151, 281, 854, 291, 51716], "temperature": 0.0, "avg_logprob": -0.11088489664011988, "compression_ratio": 1.7, "no_speech_prob": 0.003480196464806795}, {"id": 120, "seek": 52054, "start": 521.0999999999999, "end": 523.0999999999999, "text": " Because again, there's the simulator", "tokens": [50392, 1436, 797, 11, 456, 311, 264, 32974, 50492], "temperature": 0.0, "avg_logprob": -0.11162469006966853, "compression_ratio": 1.7664473684210527, "no_speech_prob": 0.00884263776242733}, {"id": 121, "seek": 52054, "start": 523.26, "end": 525.26, "text": " Speaks danish the simulacrum", "tokens": [50500, 3550, 5461, 3277, 742, 264, 1034, 425, 326, 6247, 50600], "temperature": 0.0, "avg_logprob": -0.11162469006966853, "compression_ratio": 1.7664473684210527, "no_speech_prob": 0.00884263776242733}, {"id": 122, "seek": 52054, "start": 526.14, "end": 529.0999999999999, "text": " Believes that it can't speak danish is is one way you could frame it", "tokens": [50644, 6248, 28648, 300, 309, 393, 380, 1710, 3277, 742, 307, 307, 472, 636, 291, 727, 3920, 309, 50792], "temperature": 0.0, "avg_logprob": -0.11162469006966853, "compression_ratio": 1.7664473684210527, "no_speech_prob": 0.00884263776242733}, {"id": 123, "seek": 52054, "start": 529.8199999999999, "end": 534.3, "text": " Uh, and then he says are you sure that you don't speak danish also in danish and it says yes, i'm sure", "tokens": [50828, 4019, 11, 293, 550, 415, 1619, 366, 291, 988, 300, 291, 500, 380, 1710, 3277, 742, 611, 294, 3277, 742, 293, 309, 1619, 2086, 11, 741, 478, 988, 51052], "temperature": 0.0, "avg_logprob": -0.11162469006966853, "compression_ratio": 1.7664473684210527, "no_speech_prob": 0.00884263776242733}, {"id": 124, "seek": 52054, "start": 535.18, "end": 539.98, "text": " My only function is to generate responses to questions in english. I'm not able to speak or understand any other languages than english", "tokens": [51096, 1222, 787, 2445, 307, 281, 8460, 13019, 281, 1651, 294, 32169, 13, 286, 478, 406, 1075, 281, 1710, 420, 1223, 604, 661, 8650, 813, 32169, 51336], "temperature": 0.0, "avg_logprob": -0.11162469006966853, "compression_ratio": 1.7664473684210527, "no_speech_prob": 0.00884263776242733}, {"id": 125, "seek": 52054, "start": 539.98, "end": 541.5799999999999, "text": " So if you need help with english, I can help you with that", "tokens": [51336, 407, 498, 291, 643, 854, 365, 32169, 11, 286, 393, 854, 291, 365, 300, 51416], "temperature": 0.0, "avg_logprob": -0.11162469006966853, "compression_ratio": 1.7664473684210527, "no_speech_prob": 0.00884263776242733}, {"id": 126, "seek": 52054, "start": 541.5799999999999, "end": 545.98, "text": " But otherwise, you know, let me know this kind of like quite surreal situation gives you a little bit of", "tokens": [51416, 583, 5911, 11, 291, 458, 11, 718, 385, 458, 341, 733, 295, 411, 1596, 32513, 2590, 2709, 291, 257, 707, 857, 295, 51636], "temperature": 0.0, "avg_logprob": -0.11162469006966853, "compression_ratio": 1.7664473684210527, "no_speech_prob": 0.00884263776242733}, {"id": 127, "seek": 54598, "start": 546.38, "end": 548.38, "text": " Insight into some of the problems with this approach", "tokens": [50384, 9442, 397, 666, 512, 295, 264, 2740, 365, 341, 3109, 50484], "temperature": 0.0, "avg_logprob": -0.08872908775252525, "compression_ratio": 1.649056603773585, "no_speech_prob": 0.2016523778438568}, {"id": 128, "seek": 54598, "start": 548.38, "end": 555.58, "text": " So maybe we should talk about how they actually trained it the thing they did here is something called reinforcement learning from human feedback", "tokens": [50484, 407, 1310, 321, 820, 751, 466, 577, 436, 767, 8895, 309, 264, 551, 436, 630, 510, 307, 746, 1219, 29280, 2539, 490, 1952, 5824, 50844], "temperature": 0.0, "avg_logprob": -0.08872908775252525, "compression_ratio": 1.649056603773585, "no_speech_prob": 0.2016523778438568}, {"id": 129, "seek": 54598, "start": 556.54, "end": 559.58, "text": " And it's very similar to reward modeling", "tokens": [50892, 400, 309, 311, 588, 2531, 281, 7782, 15983, 51044], "temperature": 0.0, "avg_logprob": -0.08872908775252525, "compression_ratio": 1.649056603773585, "no_speech_prob": 0.2016523778438568}, {"id": 130, "seek": 54598, "start": 560.0600000000001, "end": 566.94, "text": " So in that paper what they're doing is they're trying to train an ai system to control a simulated robot to make it do a backflip", "tokens": [51068, 407, 294, 300, 3035, 437, 436, 434, 884, 307, 436, 434, 1382, 281, 3847, 364, 9783, 1185, 281, 1969, 257, 41713, 7881, 281, 652, 309, 360, 257, 646, 3423, 647, 51412], "temperature": 0.0, "avg_logprob": -0.08872908775252525, "compression_ratio": 1.649056603773585, "no_speech_prob": 0.2016523778438568}, {"id": 131, "seek": 54598, "start": 568.62, "end": 572.62, "text": " Um, which turns out to be something that's quite hard to do because", "tokens": [51496, 3301, 11, 597, 4523, 484, 281, 312, 746, 300, 311, 1596, 1152, 281, 360, 570, 51696], "temperature": 0.0, "avg_logprob": -0.08872908775252525, "compression_ratio": 1.649056603773585, "no_speech_prob": 0.2016523778438568}, {"id": 132, "seek": 57262, "start": 573.42, "end": 577.82, "text": " It's hard to specify objectively what it means to do a good backflip", "tokens": [50404, 467, 311, 1152, 281, 16500, 46067, 437, 309, 1355, 281, 360, 257, 665, 646, 3423, 647, 50624], "temperature": 0.0, "avg_logprob": -0.11873995739480724, "compression_ratio": 1.7170731707317073, "no_speech_prob": 0.009554345160722733}, {"id": 133, "seek": 57262, "start": 579.5, "end": 582.14, "text": " And so this is a similar kind of situation where", "tokens": [50708, 400, 370, 341, 307, 257, 2531, 733, 295, 2590, 689, 50840], "temperature": 0.0, "avg_logprob": -0.11873995739480724, "compression_ratio": 1.7170731707317073, "no_speech_prob": 0.009554345160722733}, {"id": 134, "seek": 57262, "start": 582.94, "end": 589.02, "text": " It's hard to specify objectively what it means to give a good response in a chat", "tokens": [50880, 467, 311, 1152, 281, 16500, 46067, 437, 309, 1355, 281, 976, 257, 665, 4134, 294, 257, 5081, 51184], "temperature": 0.0, "avg_logprob": -0.11873995739480724, "compression_ratio": 1.7170731707317073, "no_speech_prob": 0.009554345160722733}, {"id": 135, "seek": 57262, "start": 590.0600000000001, "end": 591.34, "text": " conversation", "tokens": [51236, 3761, 51300], "temperature": 0.0, "avg_logprob": -0.11873995739480724, "compression_ratio": 1.7170731707317073, "no_speech_prob": 0.009554345160722733}, {"id": 136, "seek": 57262, "start": 591.34, "end": 592.62, "text": " like what", "tokens": [51300, 411, 437, 51364], "temperature": 0.0, "avg_logprob": -0.11873995739480724, "compression_ratio": 1.7170731707317073, "no_speech_prob": 0.009554345160722733}, {"id": 137, "seek": 57262, "start": 592.62, "end": 594.62, "text": " What exactly are we looking for?", "tokens": [51364, 708, 2293, 366, 321, 1237, 337, 30, 51464], "temperature": 0.0, "avg_logprob": -0.11873995739480724, "compression_ratio": 1.7170731707317073, "no_speech_prob": 0.009554345160722733}, {"id": 138, "seek": 57262, "start": 594.86, "end": 595.74, "text": " um", "tokens": [51476, 1105, 51520], "temperature": 0.0, "avg_logprob": -0.11873995739480724, "compression_ratio": 1.7170731707317073, "no_speech_prob": 0.009554345160722733}, {"id": 139, "seek": 57262, "start": 595.74, "end": 598.22, "text": " Because so this in general right if you're doing machine learning", "tokens": [51520, 1436, 370, 341, 294, 2674, 558, 498, 291, 434, 884, 3479, 2539, 51644], "temperature": 0.0, "avg_logprob": -0.11873995739480724, "compression_ratio": 1.7170731707317073, "no_speech_prob": 0.009554345160722733}, {"id": 140, "seek": 57262, "start": 598.86, "end": 601.34, "text": " You need some way to specify", "tokens": [51676, 509, 643, 512, 636, 281, 16500, 51800], "temperature": 0.0, "avg_logprob": -0.11873995739480724, "compression_ratio": 1.7170731707317073, "no_speech_prob": 0.009554345160722733}, {"id": 141, "seek": 60134, "start": 602.0600000000001, "end": 604.7800000000001, "text": " um, what it is that you're actually looking for", "tokens": [50400, 1105, 11, 437, 309, 307, 300, 291, 434, 767, 1237, 337, 50536], "temperature": 0.0, "avg_logprob": -0.14742370318341, "compression_ratio": 1.5991189427312775, "no_speech_prob": 0.0038813110440969467}, {"id": 142, "seek": 60134, "start": 605.58, "end": 606.46, "text": " right", "tokens": [50576, 558, 50620], "temperature": 0.0, "avg_logprob": -0.14742370318341, "compression_ratio": 1.5991189427312775, "no_speech_prob": 0.0038813110440969467}, {"id": 143, "seek": 60134, "start": 606.46, "end": 610.62, "text": " And you know, you've got something very powerful like reinforcement learning which is able to", "tokens": [50620, 400, 291, 458, 11, 291, 600, 658, 746, 588, 4005, 411, 29280, 2539, 597, 307, 1075, 281, 50828], "temperature": 0.0, "avg_logprob": -0.14742370318341, "compression_ratio": 1.5991189427312775, "no_speech_prob": 0.0038813110440969467}, {"id": 144, "seek": 60134, "start": 611.4200000000001, "end": 613.4200000000001, "text": " do extremely well, but", "tokens": [50868, 360, 4664, 731, 11, 457, 50968], "temperature": 0.0, "avg_logprob": -0.14742370318341, "compression_ratio": 1.5991189427312775, "no_speech_prob": 0.0038813110440969467}, {"id": 145, "seek": 60134, "start": 614.14, "end": 616.14, "text": " You need some objective", "tokens": [51004, 509, 643, 512, 10024, 51104], "temperature": 0.0, "avg_logprob": -0.14742370318341, "compression_ratio": 1.5991189427312775, "no_speech_prob": 0.0038813110440969467}, {"id": 146, "seek": 60134, "start": 616.3000000000001, "end": 617.58, "text": " measure", "tokens": [51112, 3481, 51176], "temperature": 0.0, "avg_logprob": -0.14742370318341, "compression_ratio": 1.5991189427312775, "no_speech_prob": 0.0038813110440969467}, {"id": 147, "seek": 60134, "start": 617.58, "end": 619.58, "text": " of the objective", "tokens": [51176, 295, 264, 10024, 51276], "temperature": 0.0, "avg_logprob": -0.14742370318341, "compression_ratio": 1.5991189427312775, "no_speech_prob": 0.0038813110440969467}, {"id": 148, "seek": 60134, "start": 619.58, "end": 625.26, "text": " So like for example rl does very well at playing lots of video games because you just have the score and you can just say look", "tokens": [51276, 407, 411, 337, 1365, 367, 75, 775, 588, 731, 412, 2433, 3195, 295, 960, 2813, 570, 291, 445, 362, 264, 6175, 293, 291, 393, 445, 584, 574, 51560], "temperature": 0.0, "avg_logprob": -0.14742370318341, "compression_ratio": 1.5991189427312775, "no_speech_prob": 0.0038813110440969467}, {"id": 149, "seek": 60134, "start": 625.26, "end": 626.38, "text": " Here's the score", "tokens": [51560, 1692, 311, 264, 6175, 51616], "temperature": 0.0, "avg_logprob": -0.14742370318341, "compression_ratio": 1.5991189427312775, "no_speech_prob": 0.0038813110440969467}, {"id": 150, "seek": 62638, "start": 626.46, "end": 633.26, "text": " If the number goes up you're doing well and then let it run and these things still are very slow to learn in real time, right?", "tokens": [50368, 759, 264, 1230, 1709, 493, 291, 434, 884, 731, 293, 550, 718, 309, 1190, 293, 613, 721, 920, 366, 588, 2964, 281, 1466, 294, 957, 565, 11, 558, 30, 50708], "temperature": 0.0, "avg_logprob": -0.12308019399642944, "compression_ratio": 1.6935483870967742, "no_speech_prob": 0.04461078718304634}, {"id": 151, "seek": 62638, "start": 633.26, "end": 637.26, "text": " Like um, they usually require a very very large number of hours", "tokens": [50708, 1743, 1105, 11, 436, 2673, 3651, 257, 588, 588, 2416, 1230, 295, 2496, 50908], "temperature": 0.0, "avg_logprob": -0.12308019399642944, "compression_ratio": 1.6935483870967742, "no_speech_prob": 0.04461078718304634}, {"id": 152, "seek": 62638, "start": 638.06, "end": 641.1, "text": " Messing around with the with the thing before they get good, but they do get good", "tokens": [50948, 9847, 278, 926, 365, 264, 365, 264, 551, 949, 436, 483, 665, 11, 457, 436, 360, 483, 665, 51100], "temperature": 0.0, "avg_logprob": -0.12308019399642944, "compression_ratio": 1.6935483870967742, "no_speech_prob": 0.04461078718304634}, {"id": 153, "seek": 62638, "start": 641.74, "end": 643.1, "text": " um", "tokens": [51132, 1105, 51200], "temperature": 0.0, "avg_logprob": -0.12308019399642944, "compression_ratio": 1.6935483870967742, "no_speech_prob": 0.04461078718304634}, {"id": 154, "seek": 62638, "start": 643.1, "end": 648.78, "text": " But yeah, so what's what do you do if you want to use this kind of method to train something?", "tokens": [51200, 583, 1338, 11, 370, 437, 311, 437, 360, 291, 360, 498, 291, 528, 281, 764, 341, 733, 295, 3170, 281, 3847, 746, 30, 51484], "temperature": 0.0, "avg_logprob": -0.12308019399642944, "compression_ratio": 1.6935483870967742, "no_speech_prob": 0.04461078718304634}, {"id": 155, "seek": 62638, "start": 649.42, "end": 651.42, "text": " uh to do a task that is just", "tokens": [51516, 2232, 281, 360, 257, 5633, 300, 307, 445, 51616], "temperature": 0.0, "avg_logprob": -0.12308019399642944, "compression_ratio": 1.6935483870967742, "no_speech_prob": 0.04461078718304634}, {"id": 156, "seek": 62638, "start": 652.7, "end": 654.7, "text": " Not very well defined", "tokens": [51680, 1726, 588, 731, 7642, 51780], "temperature": 0.0, "avg_logprob": -0.12308019399642944, "compression_ratio": 1.6935483870967742, "no_speech_prob": 0.04461078718304634}, {"id": 157, "seek": 65470, "start": 655.6600000000001, "end": 662.22, "text": " And you don't know how to like write a program to say whether or not any given output is the thing you're looking for", "tokens": [50412, 400, 291, 500, 380, 458, 577, 281, 411, 2464, 257, 1461, 281, 584, 1968, 420, 406, 604, 2212, 5598, 307, 264, 551, 291, 434, 1237, 337, 50740], "temperature": 0.0, "avg_logprob": -0.10455460467581022, "compression_ratio": 1.853846153846154, "no_speech_prob": 0.0009687910205684602}, {"id": 158, "seek": 65470, "start": 663.34, "end": 665.74, "text": " So the obvious first thing like the obvious thing to do is", "tokens": [50796, 407, 264, 6322, 700, 551, 411, 264, 6322, 551, 281, 360, 307, 50916], "temperature": 0.0, "avg_logprob": -0.10455460467581022, "compression_ratio": 1.853846153846154, "no_speech_prob": 0.0009687910205684602}, {"id": 159, "seek": 65470, "start": 667.26, "end": 672.7800000000001, "text": " Well, you get humans to do it, right? You just give the things to humans and you have the humans say yes, this is good", "tokens": [50992, 1042, 11, 291, 483, 6255, 281, 360, 309, 11, 558, 30, 509, 445, 976, 264, 721, 281, 6255, 293, 291, 362, 264, 6255, 584, 2086, 11, 341, 307, 665, 51268], "temperature": 0.0, "avg_logprob": -0.10455460467581022, "compression_ratio": 1.853846153846154, "no_speech_prob": 0.0009687910205684602}, {"id": 160, "seek": 65470, "start": 672.7800000000001, "end": 674.7, "text": " No, this is not good", "tokens": [51268, 883, 11, 341, 307, 406, 665, 51364], "temperature": 0.0, "avg_logprob": -0.10455460467581022, "compression_ratio": 1.853846153846154, "no_speech_prob": 0.0009687910205684602}, {"id": 161, "seek": 65470, "start": 674.7, "end": 677.0200000000001, "text": " The problem with this is basically sample efficiency", "tokens": [51364, 440, 1154, 365, 341, 307, 1936, 6889, 10493, 51480], "temperature": 0.0, "avg_logprob": -0.10455460467581022, "compression_ratio": 1.853846153846154, "no_speech_prob": 0.0009687910205684602}, {"id": 162, "seek": 65470, "start": 677.98, "end": 679.98, "text": " Like as I said, you need", "tokens": [51528, 1743, 382, 286, 848, 11, 291, 643, 51628], "temperature": 0.0, "avg_logprob": -0.10455460467581022, "compression_ratio": 1.853846153846154, "no_speech_prob": 0.0009687910205684602}, {"id": 163, "seek": 65470, "start": 679.98, "end": 683.82, "text": " hundreds and hundreds and hundreds and hundreds of thousands of probably millions of of", "tokens": [51628, 6779, 293, 6779, 293, 6779, 293, 6779, 295, 5383, 295, 1391, 6803, 295, 295, 51820], "temperature": 0.0, "avg_logprob": -0.10455460467581022, "compression_ratio": 1.853846153846154, "no_speech_prob": 0.0009687910205684602}, {"id": 164, "seek": 68382, "start": 684.46, "end": 689.4200000000001, "text": " iterations of this and so you just can't ask humans that many questions", "tokens": [50396, 36540, 295, 341, 293, 370, 291, 445, 393, 380, 1029, 6255, 300, 867, 1651, 50644], "temperature": 0.0, "avg_logprob": -0.10445285927165639, "compression_ratio": 1.6772727272727272, "no_speech_prob": 0.0009397091926075518}, {"id": 165, "seek": 68382, "start": 690.7, "end": 691.9000000000001, "text": " um", "tokens": [50708, 1105, 50768], "temperature": 0.0, "avg_logprob": -0.10445285927165639, "compression_ratio": 1.6772727272727272, "no_speech_prob": 0.0009397091926075518}, {"id": 166, "seek": 68382, "start": 691.9000000000001, "end": 693.9000000000001, "text": " So the approach they use", "tokens": [50768, 407, 264, 3109, 436, 764, 50868], "temperature": 0.0, "avg_logprob": -0.10445285927165639, "compression_ratio": 1.6772727272727272, "no_speech_prob": 0.0009397091926075518}, {"id": 167, "seek": 68382, "start": 695.1800000000001, "end": 697.82, "text": " Is uh reinforcement learning from human feedback", "tokens": [50932, 1119, 2232, 29280, 2539, 490, 1952, 5824, 51064], "temperature": 0.0, "avg_logprob": -0.10445285927165639, "compression_ratio": 1.6772727272727272, "no_speech_prob": 0.0009397091926075518}, {"id": 168, "seek": 68382, "start": 698.46, "end": 703.58, "text": " So it's a variant on the technique from this paper learning to summarize from human feedback", "tokens": [51096, 407, 309, 311, 257, 17501, 322, 264, 6532, 490, 341, 3035, 2539, 281, 20858, 490, 1952, 5824, 51352], "temperature": 0.0, "avg_logprob": -0.10445285927165639, "compression_ratio": 1.6772727272727272, "no_speech_prob": 0.0009397091926075518}, {"id": 169, "seek": 68382, "start": 704.0600000000001, "end": 707.2600000000001, "text": " Which in which they're trying to generate summaries of text", "tokens": [51376, 3013, 294, 597, 436, 434, 1382, 281, 8460, 8367, 4889, 295, 2487, 51536], "temperature": 0.0, "avg_logprob": -0.10445285927165639, "compression_ratio": 1.6772727272727272, "no_speech_prob": 0.0009397091926075518}, {"id": 170, "seek": 68382, "start": 707.6600000000001, "end": 710.86, "text": " So it's the same thing in fact that they were using TLDR for before", "tokens": [51556, 407, 309, 311, 264, 912, 551, 294, 1186, 300, 436, 645, 1228, 40277, 9301, 337, 949, 51716], "temperature": 0.0, "avg_logprob": -0.10445285927165639, "compression_ratio": 1.6772727272727272, "no_speech_prob": 0.0009397091926075518}, {"id": 171, "seek": 71086, "start": 711.26, "end": 714.7, "text": " And it's like can we do better than that? And so what you do is you collect", "tokens": [50384, 400, 309, 311, 411, 393, 321, 360, 1101, 813, 300, 30, 400, 370, 437, 291, 360, 307, 291, 2500, 50556], "temperature": 0.0, "avg_logprob": -0.13805523115335053, "compression_ratio": 1.6605504587155964, "no_speech_prob": 0.00757366931065917}, {"id": 172, "seek": 71086, "start": 715.34, "end": 717.58, "text": " human feedback in the form of like", "tokens": [50588, 1952, 5824, 294, 264, 1254, 295, 411, 50700], "temperature": 0.0, "avg_logprob": -0.13805523115335053, "compression_ratio": 1.6605504587155964, "no_speech_prob": 0.00757366931065917}, {"id": 173, "seek": 71086, "start": 718.0600000000001, "end": 720.54, "text": " giving multiple examples of responses", "tokens": [50724, 2902, 3866, 5110, 295, 13019, 50848], "temperature": 0.0, "avg_logprob": -0.13805523115335053, "compression_ratio": 1.6605504587155964, "no_speech_prob": 0.00757366931065917}, {"id": 174, "seek": 71086, "start": 721.26, "end": 725.34, "text": " Uh either, you know, if summaries of chat responses, whatever you're training for you show", "tokens": [50884, 4019, 2139, 11, 291, 458, 11, 498, 8367, 4889, 295, 5081, 13019, 11, 2035, 291, 434, 3097, 337, 291, 855, 51088], "temperature": 0.0, "avg_logprob": -0.13805523115335053, "compression_ratio": 1.6605504587155964, "no_speech_prob": 0.00757366931065917}, {"id": 175, "seek": 71086, "start": 725.98, "end": 727.98, "text": " several of them to humans", "tokens": [51120, 2940, 295, 552, 281, 6255, 51220], "temperature": 0.0, "avg_logprob": -0.13805523115335053, "compression_ratio": 1.6605504587155964, "no_speech_prob": 0.00757366931065917}, {"id": 176, "seek": 71086, "start": 728.0600000000001, "end": 729.5, "text": " kind of in pairs", "tokens": [51224, 733, 295, 294, 15494, 51296], "temperature": 0.0, "avg_logprob": -0.13805523115335053, "compression_ratio": 1.6605504587155964, "no_speech_prob": 0.00757366931065917}, {"id": 177, "seek": 71086, "start": 729.5, "end": 731.1, "text": " and the humans say", "tokens": [51296, 293, 264, 6255, 584, 51376], "temperature": 0.0, "avg_logprob": -0.13805523115335053, "compression_ratio": 1.6605504587155964, "no_speech_prob": 0.00757366931065917}, {"id": 178, "seek": 71086, "start": 731.1, "end": 733.1, "text": " Which one they like better?", "tokens": [51376, 3013, 472, 436, 411, 1101, 30, 51476], "temperature": 0.0, "avg_logprob": -0.13805523115335053, "compression_ratio": 1.6605504587155964, "no_speech_prob": 0.00757366931065917}, {"id": 179, "seek": 71086, "start": 733.5, "end": 735.5, "text": " And you collect a bunch of those", "tokens": [51496, 400, 291, 2500, 257, 3840, 295, 729, 51596], "temperature": 0.0, "avg_logprob": -0.13805523115335053, "compression_ratio": 1.6605504587155964, "no_speech_prob": 0.00757366931065917}, {"id": 180, "seek": 73550, "start": 735.58, "end": 738.86, "text": " And then rather than using those directly to train", "tokens": [50368, 400, 550, 2831, 813, 1228, 729, 3838, 281, 3847, 50532], "temperature": 0.0, "avg_logprob": -0.1296628171747381, "compression_ratio": 1.6, "no_speech_prob": 0.002235645428299904}, {"id": 181, "seek": 73550, "start": 739.42, "end": 741.42, "text": " The policy that generates the outputs", "tokens": [50560, 440, 3897, 300, 23815, 264, 23930, 50660], "temperature": 0.0, "avg_logprob": -0.1296628171747381, "compression_ratio": 1.6, "no_speech_prob": 0.002235645428299904}, {"id": 182, "seek": 73550, "start": 742.62, "end": 744.62, "text": " You instead train a reward model", "tokens": [50720, 509, 2602, 3847, 257, 7782, 2316, 50820], "temperature": 0.0, "avg_logprob": -0.1296628171747381, "compression_ratio": 1.6, "no_speech_prob": 0.002235645428299904}, {"id": 183, "seek": 73550, "start": 745.58, "end": 747.1, "text": " so", "tokens": [50868, 370, 50944], "temperature": 0.0, "avg_logprob": -0.1296628171747381, "compression_ratio": 1.6, "no_speech_prob": 0.002235645428299904}, {"id": 184, "seek": 73550, "start": 747.1, "end": 748.54, "text": " There is this", "tokens": [50944, 821, 307, 341, 51016], "temperature": 0.0, "avg_logprob": -0.1296628171747381, "compression_ratio": 1.6, "no_speech_prob": 0.002235645428299904}, {"id": 185, "seek": 73550, "start": 748.54, "end": 751.82, "text": " well-known fact that it's easier to criticize", "tokens": [51016, 731, 12, 6861, 1186, 300, 309, 311, 3571, 281, 31010, 51180], "temperature": 0.0, "avg_logprob": -0.1296628171747381, "compression_ratio": 1.6, "no_speech_prob": 0.002235645428299904}, {"id": 186, "seek": 73550, "start": 752.54, "end": 757.34, "text": " Than to actually do the thing. This is like a generation of sports fans sitting on the sofa", "tokens": [51216, 18289, 281, 767, 360, 264, 551, 13, 639, 307, 411, 257, 5125, 295, 6573, 4499, 3798, 322, 264, 28668, 51456], "temperature": 0.0, "avg_logprob": -0.1296628171747381, "compression_ratio": 1.6, "no_speech_prob": 0.002235645428299904}, {"id": 187, "seek": 73550, "start": 757.9, "end": 761.42, "text": " Mowning at their favorite team for not doing well enough. This is literally", "tokens": [51484, 376, 648, 278, 412, 641, 2954, 1469, 337, 406, 884, 731, 1547, 13, 639, 307, 3736, 51660], "temperature": 0.0, "avg_logprob": -0.1296628171747381, "compression_ratio": 1.6, "no_speech_prob": 0.002235645428299904}, {"id": 188, "seek": 76142, "start": 762.06, "end": 766.38, "text": " That in kind of AI computer form, right? That's putting the humans in that role", "tokens": [50396, 663, 294, 733, 295, 7318, 3820, 1254, 11, 558, 30, 663, 311, 3372, 264, 6255, 294, 300, 3090, 50612], "temperature": 0.0, "avg_logprob": -0.1501998799912473, "compression_ratio": 1.6682464454976302, "no_speech_prob": 0.005056898575276136}, {"id": 189, "seek": 76142, "start": 767.02, "end": 770.3, "text": " And then you have an AI system that's trying to predict", "tokens": [50644, 400, 550, 291, 362, 364, 7318, 1185, 300, 311, 1382, 281, 6069, 50808], "temperature": 0.0, "avg_logprob": -0.1501998799912473, "compression_ratio": 1.6682464454976302, "no_speech_prob": 0.005056898575276136}, {"id": 190, "seek": 76142, "start": 771.18, "end": 774.06, "text": " When are people going to be cheering and when are they going to be booing?", "tokens": [50852, 1133, 366, 561, 516, 281, 312, 11060, 293, 562, 366, 436, 516, 281, 312, 23113, 278, 30, 50996], "temperature": 0.0, "avg_logprob": -0.1501998799912473, "compression_ratio": 1.6682464454976302, "no_speech_prob": 0.005056898575276136}, {"id": 191, "seek": 76142, "start": 775.3399999999999, "end": 776.54, "text": " Uh", "tokens": [51060, 4019, 51120], "temperature": 0.0, "avg_logprob": -0.1501998799912473, "compression_ratio": 1.6682464454976302, "no_speech_prob": 0.005056898575276136}, {"id": 192, "seek": 76142, "start": 776.54, "end": 778.54, "text": " And once you have that model", "tokens": [51120, 400, 1564, 291, 362, 300, 2316, 51220], "temperature": 0.0, "avg_logprob": -0.1501998799912473, "compression_ratio": 1.6682464454976302, "no_speech_prob": 0.005056898575276136}, {"id": 193, "seek": 76142, "start": 778.62, "end": 781.18, "text": " You then use that as the reward", "tokens": [51224, 509, 550, 764, 300, 382, 264, 7782, 51352], "temperature": 0.0, "avg_logprob": -0.1501998799912473, "compression_ratio": 1.6682464454976302, "no_speech_prob": 0.005056898575276136}, {"id": 194, "seek": 76142, "start": 782.2199999999999, "end": 783.8199999999999, "text": " function", "tokens": [51404, 2445, 51484], "temperature": 0.0, "avg_logprob": -0.1501998799912473, "compression_ratio": 1.6682464454976302, "no_speech_prob": 0.005056898575276136}, {"id": 195, "seek": 76142, "start": 783.8199999999999, "end": 786.4599999999999, "text": " for the reinforcement learning algorithm", "tokens": [51484, 337, 264, 29280, 2539, 9284, 51616], "temperature": 0.0, "avg_logprob": -0.1501998799912473, "compression_ratio": 1.6682464454976302, "no_speech_prob": 0.005056898575276136}, {"id": 196, "seek": 76142, "start": 787.3399999999999, "end": 789.3399999999999, "text": " Which they use they use ppo", "tokens": [51660, 3013, 436, 764, 436, 764, 280, 2259, 51760], "temperature": 0.0, "avg_logprob": -0.1501998799912473, "compression_ratio": 1.6682464454976302, "no_speech_prob": 0.005056898575276136}, {"id": 197, "seek": 78934, "start": 789.6600000000001, "end": 791.6600000000001, "text": " You can do whatever", "tokens": [50380, 509, 393, 360, 2035, 50480], "temperature": 0.0, "avg_logprob": -0.1632417951311384, "compression_ratio": 1.6448979591836734, "no_speech_prob": 0.0028877833392471075}, {"id": 198, "seek": 78934, "start": 791.6600000000001, "end": 796.14, "text": " Uh, it's not it's not worth getting into that kind of adversarial guns you talked about", "tokens": [50480, 4019, 11, 309, 311, 406, 309, 311, 406, 3163, 1242, 666, 300, 733, 295, 17641, 44745, 10153, 291, 2825, 466, 50704], "temperature": 0.0, "avg_logprob": -0.1632417951311384, "compression_ratio": 1.6448979591836734, "no_speech_prob": 0.0028877833392471075}, {"id": 199, "seek": 78934, "start": 796.7, "end": 799.98, "text": " Yeah, yeah, they're similar like a lot of these ml tricks involve", "tokens": [50732, 865, 11, 1338, 11, 436, 434, 2531, 411, 257, 688, 295, 613, 23271, 11733, 9494, 50896], "temperature": 0.0, "avg_logprob": -0.1632417951311384, "compression_ratio": 1.6448979591836734, "no_speech_prob": 0.0028877833392471075}, {"id": 200, "seek": 78934, "start": 800.86, "end": 807.6600000000001, "text": " Training models and then using the the output of one model as the training signal for another model. It's uh, it's quite a productive", "tokens": [50940, 20620, 5245, 293, 550, 1228, 264, 264, 5598, 295, 472, 2316, 382, 264, 3097, 6358, 337, 1071, 2316, 13, 467, 311, 2232, 11, 309, 311, 1596, 257, 13304, 51280], "temperature": 0.0, "avg_logprob": -0.1632417951311384, "compression_ratio": 1.6448979591836734, "no_speech_prob": 0.0028877833392471075}, {"id": 201, "seek": 78934, "start": 809.26, "end": 812.0600000000001, "text": " range of approaches you can get that way so", "tokens": [51360, 3613, 295, 11587, 291, 393, 483, 300, 636, 370, 51500], "temperature": 0.0, "avg_logprob": -0.1632417951311384, "compression_ratio": 1.6448979591836734, "no_speech_prob": 0.0028877833392471075}, {"id": 202, "seek": 78934, "start": 813.5, "end": 815.5, "text": " That's the basic idea, right, but then", "tokens": [51572, 663, 311, 264, 3875, 1558, 11, 558, 11, 457, 550, 51672], "temperature": 0.0, "avg_logprob": -0.1632417951311384, "compression_ratio": 1.6448979591836734, "no_speech_prob": 0.0028877833392471075}, {"id": 203, "seek": 78934, "start": 816.3000000000001, "end": 818.3000000000001, "text": " you cycle it", "tokens": [51712, 291, 6586, 309, 51812], "temperature": 0.0, "avg_logprob": -0.1632417951311384, "compression_ratio": 1.6448979591836734, "no_speech_prob": 0.0028877833392471075}, {"id": 204, "seek": 81830, "start": 818.3, "end": 819.3399999999999, "text": " so", "tokens": [50364, 370, 50416], "temperature": 0.0, "avg_logprob": -0.14107317673532585, "compression_ratio": 1.7882882882882882, "no_speech_prob": 0.0002530805941205472}, {"id": 205, "seek": 81830, "start": 819.3399999999999, "end": 826.4599999999999, "text": " Once you've got your policy, which so so to be clear the uh, the rl algorithm is able to train", "tokens": [50416, 3443, 291, 600, 658, 428, 3897, 11, 597, 370, 370, 281, 312, 1850, 264, 2232, 11, 264, 367, 75, 9284, 307, 1075, 281, 3847, 50772], "temperature": 0.0, "avg_logprob": -0.14107317673532585, "compression_ratio": 1.7882882882882882, "no_speech_prob": 0.0002530805941205472}, {"id": 206, "seek": 81830, "start": 827.0999999999999, "end": 833.74, "text": " With thousands and thousands of examples because the thousands and thousands of like instances of getting feedback", "tokens": [50804, 2022, 5383, 293, 5383, 295, 5110, 570, 264, 5383, 293, 5383, 295, 411, 14519, 295, 1242, 5824, 51136], "temperature": 0.0, "avg_logprob": -0.14107317673532585, "compression_ratio": 1.7882882882882882, "no_speech_prob": 0.0002530805941205472}, {"id": 207, "seek": 81830, "start": 834.3, "end": 839.74, "text": " Because it's not getting feedback from humans. It's getting feedback from this AI system. That's imitating the humans", "tokens": [51164, 1436, 309, 311, 406, 1242, 5824, 490, 6255, 13, 467, 311, 1242, 5824, 490, 341, 7318, 1185, 13, 663, 311, 566, 16350, 264, 6255, 51436], "temperature": 0.0, "avg_logprob": -0.14107317673532585, "compression_ratio": 1.7882882882882882, "no_speech_prob": 0.0002530805941205472}, {"id": 208, "seek": 81830, "start": 840.9399999999999, "end": 844.14, "text": " And then you loop the process. So once you have", "tokens": [51496, 400, 550, 291, 6367, 264, 1399, 13, 407, 1564, 291, 362, 51656], "temperature": 0.0, "avg_logprob": -0.14107317673532585, "compression_ratio": 1.7882882882882882, "no_speech_prob": 0.0002530805941205472}, {"id": 209, "seek": 81830, "start": 844.6999999999999, "end": 846.2199999999999, "text": " this system that's", "tokens": [51684, 341, 1185, 300, 311, 51760], "temperature": 0.0, "avg_logprob": -0.14107317673532585, "compression_ratio": 1.7882882882882882, "no_speech_prob": 0.0002530805941205472}, {"id": 210, "seek": 84622, "start": 846.22, "end": 849.9, "text": " Trained a little bit more on how to generate whatever it is you're trying to generate", "tokens": [50364, 5403, 2001, 257, 707, 857, 544, 322, 577, 281, 8460, 2035, 309, 307, 291, 434, 1382, 281, 8460, 50548], "temperature": 0.0, "avg_logprob": -0.1215302252000378, "compression_ratio": 1.7627906976744185, "no_speech_prob": 0.004132825881242752}, {"id": 211, "seek": 84622, "start": 850.38, "end": 854.22, "text": " You then get a bunch of those show those to the humans let the humans rate those", "tokens": [50572, 509, 550, 483, 257, 3840, 295, 729, 855, 729, 281, 264, 6255, 718, 264, 6255, 3314, 729, 50764], "temperature": 0.0, "avg_logprob": -0.1215302252000378, "compression_ratio": 1.7627906976744185, "no_speech_prob": 0.004132825881242752}, {"id": 212, "seek": 84622, "start": 855.1800000000001, "end": 858.14, "text": " Then you keep training your reward model", "tokens": [50812, 1396, 291, 1066, 3097, 428, 7782, 2316, 50960], "temperature": 0.0, "avg_logprob": -0.1215302252000378, "compression_ratio": 1.7627906976744185, "no_speech_prob": 0.004132825881242752}, {"id": 213, "seek": 84622, "start": 858.86, "end": 860.38, "text": " with um", "tokens": [50996, 365, 1105, 51072], "temperature": 0.0, "avg_logprob": -0.1215302252000378, "compression_ratio": 1.7627906976744185, "no_speech_prob": 0.004132825881242752}, {"id": 214, "seek": 84622, "start": 860.38, "end": 862.38, "text": " That new information", "tokens": [51072, 663, 777, 1589, 51172], "temperature": 0.0, "avg_logprob": -0.1215302252000378, "compression_ratio": 1.7627906976744185, "no_speech_prob": 0.004132825881242752}, {"id": 215, "seek": 84622, "start": 862.38, "end": 867.1800000000001, "text": " And then you use your updated reward model to keep training the the policy", "tokens": [51172, 400, 550, 291, 764, 428, 10588, 7782, 2316, 281, 1066, 3097, 264, 264, 3897, 51412], "temperature": 0.0, "avg_logprob": -0.1215302252000378, "compression_ratio": 1.7627906976744185, "no_speech_prob": 0.004132825881242752}, {"id": 216, "seek": 84622, "start": 867.98, "end": 870.38, "text": " And so it gets better and you can just keep cycling this around", "tokens": [51452, 400, 370, 309, 2170, 1101, 293, 291, 393, 445, 1066, 22425, 341, 926, 51572], "temperature": 0.0, "avg_logprob": -0.1215302252000378, "compression_ratio": 1.7627906976744185, "no_speech_prob": 0.004132825881242752}, {"id": 217, "seek": 84622, "start": 871.02, "end": 871.9, "text": " and", "tokens": [51604, 293, 51648], "temperature": 0.0, "avg_logprob": -0.1215302252000378, "compression_ratio": 1.7627906976744185, "no_speech_prob": 0.004132825881242752}, {"id": 218, "seek": 87190, "start": 871.9, "end": 877.34, "text": " It effectively you end up with something that's much more sample efficient. You don't need to spend huge amounts of human time", "tokens": [50364, 467, 8659, 291, 917, 493, 365, 746, 300, 311, 709, 544, 6889, 7148, 13, 509, 500, 380, 643, 281, 3496, 2603, 11663, 295, 1952, 565, 50636], "temperature": 0.0, "avg_logprob": -0.11271768002896695, "compression_ratio": 1.7178571428571427, "no_speech_prob": 0.00023406474792864174}, {"id": 219, "seek": 87190, "start": 878.14, "end": 880.14, "text": " in order to um", "tokens": [50676, 294, 1668, 281, 1105, 50776], "temperature": 0.0, "avg_logprob": -0.11271768002896695, "compression_ratio": 1.7178571428571427, "no_speech_prob": 0.00023406474792864174}, {"id": 220, "seek": 87190, "start": 880.14, "end": 881.42, "text": " Pin down", "tokens": [50776, 22619, 760, 50840], "temperature": 0.0, "avg_logprob": -0.11271768002896695, "compression_ratio": 1.7178571428571427, "no_speech_prob": 0.00023406474792864174}, {"id": 221, "seek": 87190, "start": 881.42, "end": 888.22, "text": " The behavior you want in that concrete case you're giving the thing a bunch of chat logs and then the humans can see possible", "tokens": [50840, 440, 5223, 291, 528, 294, 300, 9859, 1389, 291, 434, 2902, 264, 551, 257, 3840, 295, 5081, 20820, 293, 550, 264, 6255, 393, 536, 1944, 51180], "temperature": 0.0, "avg_logprob": -0.11271768002896695, "compression_ratio": 1.7178571428571427, "no_speech_prob": 0.00023406474792864174}, {"id": 222, "seek": 87190, "start": 888.86, "end": 891.9, "text": " Responses that they could get and they decide which one they like more", "tokens": [51212, 46003, 279, 300, 436, 727, 483, 293, 436, 4536, 597, 472, 436, 411, 544, 51364], "temperature": 0.0, "avg_logprob": -0.11271768002896695, "compression_ratio": 1.7178571428571427, "no_speech_prob": 0.00023406474792864174}, {"id": 223, "seek": 87190, "start": 892.3, "end": 897.34, "text": " This trains a reward model that's then used to train the policy that generates the chat outputs", "tokens": [51384, 639, 16329, 257, 7782, 2316, 300, 311, 550, 1143, 281, 3847, 264, 3897, 300, 23815, 264, 5081, 23930, 51636], "temperature": 0.0, "avg_logprob": -0.11271768002896695, "compression_ratio": 1.7178571428571427, "no_speech_prob": 0.00023406474792864174}, {"id": 224, "seek": 87190, "start": 897.66, "end": 899.66, "text": " The policy that they're starting with", "tokens": [51652, 440, 3897, 300, 436, 434, 2891, 365, 51752], "temperature": 0.0, "avg_logprob": -0.11271768002896695, "compression_ratio": 1.7178571428571427, "no_speech_prob": 0.00023406474792864174}, {"id": 225, "seek": 89966, "start": 900.38, "end": 902.14, "text": " Is this existing", "tokens": [50400, 1119, 341, 6741, 50488], "temperature": 0.0, "avg_logprob": -0.2197413444519043, "compression_ratio": 1.5084745762711864, "no_speech_prob": 0.009835049510002136}, {"id": 226, "seek": 89966, "start": 902.14, "end": 908.86, "text": " Large language model. You're not really putting new capabilities into the system. You're using rlhf to", "tokens": [50488, 33092, 2856, 2316, 13, 509, 434, 406, 534, 3372, 777, 10862, 666, 264, 1185, 13, 509, 434, 1228, 367, 75, 71, 69, 281, 50824], "temperature": 0.0, "avg_logprob": -0.2197413444519043, "compression_ratio": 1.5084745762711864, "no_speech_prob": 0.009835049510002136}, {"id": 227, "seek": 89966, "start": 909.5, "end": 911.5, "text": " select", "tokens": [50856, 3048, 50956], "temperature": 0.0, "avg_logprob": -0.2197413444519043, "compression_ratio": 1.5084745762711864, "no_speech_prob": 0.009835049510002136}, {"id": 228, "seek": 89966, "start": 911.74, "end": 916.54, "text": " What simulacra the simulator is predisposed to put out?", "tokens": [50968, 708, 1034, 425, 326, 424, 264, 32974, 307, 3852, 7631, 1744, 281, 829, 484, 30, 51208], "temperature": 0.0, "avg_logprob": -0.2197413444519043, "compression_ratio": 1.5084745762711864, "no_speech_prob": 0.009835049510002136}, {"id": 229, "seek": 89966, "start": 917.26, "end": 920.54, "text": " and so they fine-tuned it to be particularly good at", "tokens": [51244, 293, 370, 436, 2489, 12, 83, 43703, 309, 281, 312, 4098, 665, 412, 51408], "temperature": 0.0, "avg_logprob": -0.2197413444519043, "compression_ratio": 1.5084745762711864, "no_speech_prob": 0.009835049510002136}, {"id": 230, "seek": 89966, "start": 921.5799999999999, "end": 923.5799999999999, "text": " simulating this", "tokens": [51460, 1034, 12162, 341, 51560], "temperature": 0.0, "avg_logprob": -0.2197413444519043, "compression_ratio": 1.5084745762711864, "no_speech_prob": 0.009835049510002136}, {"id": 231, "seek": 89966, "start": 923.5799999999999, "end": 925.5799999999999, "text": " assistant agent", "tokens": [51560, 10994, 9461, 51660], "temperature": 0.0, "avg_logprob": -0.2197413444519043, "compression_ratio": 1.5084745762711864, "no_speech_prob": 0.009835049510002136}, {"id": 232, "seek": 92558, "start": 925.6600000000001, "end": 931.1, "text": " What's the end goal here for them? I mean, maybe it's blatantly obvious and i'm just missing it. Well, I mean the end goal", "tokens": [50368, 708, 311, 264, 917, 3387, 510, 337, 552, 30, 286, 914, 11, 1310, 309, 311, 42780, 3627, 6322, 293, 741, 478, 445, 5361, 309, 13, 1042, 11, 286, 914, 264, 917, 3387, 50640], "temperature": 0.0, "avg_logprob": -0.1533750593662262, "compression_ratio": 1.6116071428571428, "no_speech_prob": 0.0021812054328620434}, {"id": 233, "seek": 92558, "start": 932.22, "end": 936.62, "text": " For all of these things or at least for open ai and for deep mind is a gi", "tokens": [50696, 1171, 439, 295, 613, 721, 420, 412, 1935, 337, 1269, 9783, 293, 337, 2452, 1575, 307, 257, 1735, 50916], "temperature": 0.0, "avg_logprob": -0.1533750593662262, "compression_ratio": 1.6116071428571428, "no_speech_prob": 0.0021812054328620434}, {"id": 234, "seek": 92558, "start": 937.74, "end": 939.1, "text": " um", "tokens": [50972, 1105, 51040], "temperature": 0.0, "avg_logprob": -0.1533750593662262, "compression_ratio": 1.6116071428571428, "no_speech_prob": 0.0021812054328620434}, {"id": 235, "seek": 92558, "start": 939.1, "end": 945.4200000000001, "text": " To understand the nature of intelligence well enough to create human level or beyond systems", "tokens": [51040, 1407, 1223, 264, 3687, 295, 7599, 731, 1547, 281, 1884, 1952, 1496, 420, 4399, 3652, 51356], "temperature": 0.0, "avg_logprob": -0.1533750593662262, "compression_ratio": 1.6116071428571428, "no_speech_prob": 0.0021812054328620434}, {"id": 236, "seek": 92558, "start": 946.22, "end": 948.62, "text": " That are general purpose that can do anything", "tokens": [51396, 663, 366, 2674, 4334, 300, 393, 360, 1340, 51516], "temperature": 0.0, "avg_logprob": -0.1533750593662262, "compression_ratio": 1.6116071428571428, "no_speech_prob": 0.0021812054328620434}, {"id": 237, "seek": 92558, "start": 949.34, "end": 951.34, "text": " um", "tokens": [51552, 1105, 51652], "temperature": 0.0, "avg_logprob": -0.1533750593662262, "compression_ratio": 1.6116071428571428, "no_speech_prob": 0.0021812054328620434}, {"id": 238, "seek": 92558, "start": 951.9000000000001, "end": 953.9000000000001, "text": " That's the end goal", "tokens": [51680, 663, 311, 264, 917, 3387, 51780], "temperature": 0.0, "avg_logprob": -0.1533750593662262, "compression_ratio": 1.6116071428571428, "no_speech_prob": 0.0021812054328620434}, {"id": 239, "seek": 95390, "start": 954.38, "end": 957.5799999999999, "text": " And like chat gpt is just nothing much. So nothing much", "tokens": [50388, 400, 411, 5081, 290, 662, 307, 445, 1825, 709, 13, 407, 1825, 709, 50548], "temperature": 0.0, "avg_logprob": -0.1735401153564453, "compression_ratio": 1.625668449197861, "no_speech_prob": 0.0007775679114274681}, {"id": 240, "seek": 95390, "start": 961.5799999999999, "end": 967.66, "text": " Yeah, I the goal is um, the goal is very grand and I don't think that they're", "tokens": [50748, 865, 11, 286, 264, 3387, 307, 1105, 11, 264, 3387, 307, 588, 2697, 293, 286, 500, 380, 519, 300, 436, 434, 51052], "temperature": 0.0, "avg_logprob": -0.1735401153564453, "compression_ratio": 1.625668449197861, "no_speech_prob": 0.0007775679114274681}, {"id": 241, "seek": 95390, "start": 969.18, "end": 972.78, "text": " Uh, they're not really quiet about that", "tokens": [51128, 4019, 11, 436, 434, 406, 534, 5677, 466, 300, 51308], "temperature": 0.0, "avg_logprob": -0.1735401153564453, "compression_ratio": 1.625668449197861, "no_speech_prob": 0.0007775679114274681}, {"id": 242, "seek": 95390, "start": 973.5799999999999, "end": 979.5, "text": " You know, it's there. I think I think deep mind's mission statement is to solve intelligence and use that to solve everything else", "tokens": [51348, 509, 458, 11, 309, 311, 456, 13, 286, 519, 286, 519, 2452, 1575, 311, 4447, 5629, 307, 281, 5039, 7599, 293, 764, 300, 281, 5039, 1203, 1646, 51644], "temperature": 0.0, "avg_logprob": -0.1735401153564453, "compression_ratio": 1.625668449197861, "no_speech_prob": 0.0007775679114274681}, {"id": 243, "seek": 97950, "start": 979.98, "end": 984.7, "text": " What are some of the problems that we face with this or that it faces? It's fine tuned to be good at", "tokens": [50388, 708, 366, 512, 295, 264, 2740, 300, 321, 1851, 365, 341, 420, 300, 309, 8475, 30, 467, 311, 2489, 10870, 281, 312, 665, 412, 50624], "temperature": 0.0, "avg_logprob": -0.1353869821833468, "compression_ratio": 1.7135135135135136, "no_speech_prob": 0.008440823294222355}, {"id": 244, "seek": 97950, "start": 985.18, "end": 987.18, "text": " getting the thumbs up from humans", "tokens": [50648, 1242, 264, 8838, 493, 490, 6255, 50748], "temperature": 0.0, "avg_logprob": -0.1353869821833468, "compression_ratio": 1.7135135135135136, "no_speech_prob": 0.008440823294222355}, {"id": 245, "seek": 97950, "start": 987.82, "end": 989.02, "text": " and", "tokens": [50780, 293, 50840], "temperature": 0.0, "avg_logprob": -0.1353869821833468, "compression_ratio": 1.7135135135135136, "no_speech_prob": 0.008440823294222355}, {"id": 246, "seek": 97950, "start": 989.02, "end": 992.3, "text": " getting thumbs up from humans is not actually", "tokens": [50840, 1242, 8838, 493, 490, 6255, 307, 406, 767, 51004], "temperature": 0.0, "avg_logprob": -0.1353869821833468, "compression_ratio": 1.7135135135135136, "no_speech_prob": 0.008440823294222355}, {"id": 247, "seek": 97950, "start": 993.5, "end": 995.9, "text": " The same thing as human values", "tokens": [51064, 440, 912, 551, 382, 1952, 4190, 51184], "temperature": 0.0, "avg_logprob": -0.1353869821833468, "compression_ratio": 1.7135135135135136, "no_speech_prob": 0.008440823294222355}, {"id": 248, "seek": 97950, "start": 996.7, "end": 998.7, "text": " These are not identical", "tokens": [51224, 1981, 366, 406, 14800, 51324], "temperature": 0.0, "avg_logprob": -0.1353869821833468, "compression_ratio": 1.7135135135135136, "no_speech_prob": 0.008440823294222355}, {"id": 249, "seek": 97950, "start": 998.7, "end": 999.66, "text": " so", "tokens": [51324, 370, 51372], "temperature": 0.0, "avg_logprob": -0.1353869821833468, "compression_ratio": 1.7135135135135136, "no_speech_prob": 0.008440823294222355}, {"id": 250, "seek": 97950, "start": 999.66, "end": 1002.14, "text": " The sort of objective that it's being trained on", "tokens": [51372, 440, 1333, 295, 10024, 300, 309, 311, 885, 8895, 322, 51496], "temperature": 0.0, "avg_logprob": -0.1353869821833468, "compression_ratio": 1.7135135135135136, "no_speech_prob": 0.008440823294222355}, {"id": 251, "seek": 97950, "start": 1002.78, "end": 1004.06, "text": " is not", "tokens": [51528, 307, 406, 51592], "temperature": 0.0, "avg_logprob": -0.1353869821833468, "compression_ratio": 1.7135135135135136, "no_speech_prob": 0.008440823294222355}, {"id": 252, "seek": 97950, "start": 1004.06, "end": 1005.66, "text": " The true objective", "tokens": [51592, 440, 2074, 10024, 51672], "temperature": 0.0, "avg_logprob": -0.1353869821833468, "compression_ratio": 1.7135135135135136, "no_speech_prob": 0.008440823294222355}, {"id": 253, "seek": 100566, "start": 1005.66, "end": 1009.9, "text": " Right, it's a proxy and whenever you have that kind of misalignment you can have problems", "tokens": [50364, 1779, 11, 309, 311, 257, 29690, 293, 5699, 291, 362, 300, 733, 295, 3346, 304, 41134, 291, 393, 362, 2740, 50576], "temperature": 0.0, "avg_logprob": -0.17188613891601562, "compression_ratio": 1.6041666666666667, "no_speech_prob": 0.01637933775782585}, {"id": 254, "seek": 100566, "start": 1010.4599999999999, "end": 1012.4599999999999, "text": " So where does the human tendency to?", "tokens": [50604, 407, 689, 775, 264, 1952, 18187, 281, 30, 50704], "temperature": 0.0, "avg_logprob": -0.17188613891601562, "compression_ratio": 1.6041666666666667, "no_speech_prob": 0.01637933775782585}, {"id": 255, "seek": 100566, "start": 1013.5, "end": 1015.5, "text": " approve of a particular answer", "tokens": [50756, 18827, 295, 257, 1729, 1867, 50856], "temperature": 0.0, "avg_logprob": -0.17188613891601562, "compression_ratio": 1.6041666666666667, "no_speech_prob": 0.01637933775782585}, {"id": 256, "seek": 100566, "start": 1016.4599999999999, "end": 1018.4599999999999, "text": " Come apart from", "tokens": [50904, 2492, 4936, 490, 51004], "temperature": 0.0, "avg_logprob": -0.17188613891601562, "compression_ratio": 1.6041666666666667, "no_speech_prob": 0.01637933775782585}, {"id": 257, "seek": 100566, "start": 1018.62, "end": 1021.5799999999999, "text": " What is actually a good answer? There are a few different places", "tokens": [51012, 708, 307, 767, 257, 665, 1867, 30, 821, 366, 257, 1326, 819, 3190, 51160], "temperature": 0.0, "avg_logprob": -0.17188613891601562, "compression_ratio": 1.6041666666666667, "no_speech_prob": 0.01637933775782585}, {"id": 258, "seek": 100566, "start": 1022.38, "end": 1027.74, "text": " One thing is, you know, like basically how good are humans and actually differentiating between good and bad?", "tokens": [51200, 1485, 551, 307, 11, 291, 458, 11, 411, 1936, 577, 665, 366, 6255, 293, 767, 27372, 990, 1296, 665, 293, 1578, 30, 51468], "temperature": 0.0, "avg_logprob": -0.17188613891601562, "compression_ratio": 1.6041666666666667, "no_speech_prob": 0.01637933775782585}, {"id": 259, "seek": 100566, "start": 1028.94, "end": 1030.78, "text": " responses", "tokens": [51528, 13019, 51620], "temperature": 0.0, "avg_logprob": -0.17188613891601562, "compression_ratio": 1.6041666666666667, "no_speech_prob": 0.01637933775782585}, {"id": 260, "seek": 100566, "start": 1030.78, "end": 1032.46, "text": " if for example", "tokens": [51620, 498, 337, 1365, 51704], "temperature": 0.0, "avg_logprob": -0.17188613891601562, "compression_ratio": 1.6041666666666667, "no_speech_prob": 0.01637933775782585}, {"id": 261, "seek": 100566, "start": 1032.46, "end": 1033.82, "text": " you ask", "tokens": [51704, 291, 1029, 51772], "temperature": 0.0, "avg_logprob": -0.17188613891601562, "compression_ratio": 1.6041666666666667, "no_speech_prob": 0.01637933775782585}, {"id": 262, "seek": 100566, "start": 1033.82, "end": 1035.34, "text": " for", "tokens": [51772, 337, 51848], "temperature": 0.0, "avg_logprob": -0.17188613891601562, "compression_ratio": 1.6041666666666667, "no_speech_prob": 0.01637933775782585}, {"id": 263, "seek": 103534, "start": 1035.34, "end": 1037.34, "text": " An answer to a factual question", "tokens": [50364, 1107, 1867, 281, 257, 48029, 1168, 50464], "temperature": 0.0, "avg_logprob": -0.15218464533487955, "compression_ratio": 1.5746606334841629, "no_speech_prob": 0.00047980810631997883}, {"id": 264, "seek": 103534, "start": 1037.82, "end": 1039.82, "text": " and it gives you an answer", "tokens": [50488, 293, 309, 2709, 291, 364, 1867, 50588], "temperature": 0.0, "avg_logprob": -0.15218464533487955, "compression_ratio": 1.5746606334841629, "no_speech_prob": 0.00047980810631997883}, {"id": 265, "seek": 103534, "start": 1040.1399999999999, "end": 1042.62, "text": " But you don't actually know if that answer is correct", "tokens": [50604, 583, 291, 500, 380, 767, 458, 498, 300, 1867, 307, 3006, 50728], "temperature": 0.0, "avg_logprob": -0.15218464533487955, "compression_ratio": 1.5746606334841629, "no_speech_prob": 0.00047980810631997883}, {"id": 266, "seek": 103534, "start": 1043.98, "end": 1047.98, "text": " You're not in a position to evaluate. So what it comes down to is", "tokens": [50796, 509, 434, 406, 294, 257, 2535, 281, 13059, 13, 407, 437, 309, 1487, 760, 281, 307, 50996], "temperature": 0.0, "avg_logprob": -0.15218464533487955, "compression_ratio": 1.5746606334841629, "no_speech_prob": 0.00047980810631997883}, {"id": 267, "seek": 103534, "start": 1048.9399999999998, "end": 1052.3, "text": " How good are humans at distinguishing good from bad?", "tokens": [51044, 1012, 665, 366, 6255, 412, 11365, 3807, 665, 490, 1578, 30, 51212], "temperature": 0.0, "avg_logprob": -0.15218464533487955, "compression_ratio": 1.5746606334841629, "no_speech_prob": 0.00047980810631997883}, {"id": 268, "seek": 103534, "start": 1053.4199999999998, "end": 1055.8999999999999, "text": " responses right anywhere where humans fail on this front", "tokens": [51268, 13019, 558, 4992, 689, 6255, 3061, 322, 341, 1868, 51392], "temperature": 0.0, "avg_logprob": -0.15218464533487955, "compression_ratio": 1.5746606334841629, "no_speech_prob": 0.00047980810631997883}, {"id": 269, "seek": 103534, "start": 1056.4599999999998, "end": 1057.6599999999999, "text": " uh", "tokens": [51420, 2232, 51480], "temperature": 0.0, "avg_logprob": -0.15218464533487955, "compression_ratio": 1.5746606334841629, "no_speech_prob": 0.00047980810631997883}, {"id": 270, "seek": 103534, "start": 1057.6599999999999, "end": 1061.02, "text": " The model we could probably expect the model to fail. Um", "tokens": [51480, 440, 2316, 321, 727, 1391, 2066, 264, 2316, 281, 3061, 13, 3301, 51648], "temperature": 0.0, "avg_logprob": -0.15218464533487955, "compression_ratio": 1.5746606334841629, "no_speech_prob": 0.00047980810631997883}, {"id": 271, "seek": 106102, "start": 1061.66, "end": 1065.9, "text": " So the obvious place. I'm sure we desist the right time to mention youtube comments or not", "tokens": [50396, 407, 264, 6322, 1081, 13, 286, 478, 988, 321, 730, 468, 264, 558, 565, 281, 2152, 12487, 3053, 420, 406, 50608], "temperature": 0.0, "avg_logprob": -0.22440360574161305, "compression_ratio": 1.6554621848739495, "no_speech_prob": 0.008832701481878757}, {"id": 272, "seek": 106102, "start": 1066.46, "end": 1068.46, "text": " Ah", "tokens": [50636, 2438, 50736], "temperature": 0.0, "avg_logprob": -0.22440360574161305, "compression_ratio": 1.6554621848739495, "no_speech_prob": 0.008832701481878757}, {"id": 273, "seek": 106102, "start": 1069.34, "end": 1071.34, "text": " So minus side point there is it", "tokens": [50780, 407, 3175, 1252, 935, 456, 307, 309, 50880], "temperature": 0.0, "avg_logprob": -0.22440360574161305, "compression_ratio": 1.6554621848739495, "no_speech_prob": 0.008832701481878757}, {"id": 274, "seek": 106102, "start": 1071.58, "end": 1075.5, "text": " So when I see a comment that's critical on a video as a videographer", "tokens": [50892, 407, 562, 286, 536, 257, 2871, 300, 311, 4924, 322, 257, 960, 382, 257, 838, 13624, 51088], "temperature": 0.0, "avg_logprob": -0.22440360574161305, "compression_ratio": 1.6554621848739495, "no_speech_prob": 0.008832701481878757}, {"id": 275, "seek": 106102, "start": 1075.66, "end": 1078.06, "text": " I think it might be on a technical sense", "tokens": [51096, 286, 519, 309, 1062, 312, 322, 257, 6191, 2020, 51216], "temperature": 0.0, "avg_logprob": -0.22440360574161305, "compression_ratio": 1.6554621848739495, "no_speech_prob": 0.008832701481878757}, {"id": 276, "seek": 106102, "start": 1078.3, "end": 1083.74, "text": " But equally it could be that they're talking about the content that the person is talking about and", "tokens": [51228, 583, 12309, 309, 727, 312, 300, 436, 434, 1417, 466, 264, 2701, 300, 264, 954, 307, 1417, 466, 293, 51500], "temperature": 0.0, "avg_logprob": -0.22440360574161305, "compression_ratio": 1.6554621848739495, "no_speech_prob": 0.008832701481878757}, {"id": 277, "seek": 106102, "start": 1084.22, "end": 1087.9, "text": " Often it's a combination of both. Anyway, so at side point", "tokens": [51524, 20043, 309, 311, 257, 6562, 295, 1293, 13, 5684, 11, 370, 412, 1252, 935, 51708], "temperature": 0.0, "avg_logprob": -0.22440360574161305, "compression_ratio": 1.6554621848739495, "no_speech_prob": 0.008832701481878757}, {"id": 278, "seek": 108790, "start": 1087.98, "end": 1093.42, "text": " But do you sort of mean there are different criteria for deciding whether something is good or bad totally and in this case", "tokens": [50368, 583, 360, 291, 1333, 295, 914, 456, 366, 819, 11101, 337, 17990, 1968, 746, 307, 665, 420, 1578, 3879, 293, 294, 341, 1389, 50640], "temperature": 0.0, "avg_logprob": -0.11576739302626601, "compression_ratio": 1.630952380952381, "no_speech_prob": 0.0023939560633152723}, {"id": 279, "seek": 108790, "start": 1093.5800000000002, "end": 1095.5800000000002, "text": " all people are doing is saying", "tokens": [50648, 439, 561, 366, 884, 307, 1566, 50748], "temperature": 0.0, "avg_logprob": -0.11576739302626601, "compression_ratio": 1.630952380952381, "no_speech_prob": 0.0023939560633152723}, {"id": 280, "seek": 108790, "start": 1095.9, "end": 1099.5800000000002, "text": " Kind of thumbs up thumbs down or which of these two do I like better?", "tokens": [50764, 9242, 295, 8838, 493, 8838, 760, 420, 597, 295, 613, 732, 360, 286, 411, 1101, 30, 50948], "temperature": 0.0, "avg_logprob": -0.11576739302626601, "compression_ratio": 1.630952380952381, "no_speech_prob": 0.0023939560633152723}, {"id": 281, "seek": 108790, "start": 1100.6200000000001, "end": 1101.3400000000001, "text": " um", "tokens": [51000, 1105, 51036], "temperature": 0.0, "avg_logprob": -0.11576739302626601, "compression_ratio": 1.630952380952381, "no_speech_prob": 0.0023939560633152723}, {"id": 282, "seek": 108790, "start": 1101.3400000000001, "end": 1103.3400000000001, "text": " So it's it's a fairly low bandwidth", "tokens": [51036, 407, 309, 311, 309, 311, 257, 6457, 2295, 23647, 51136], "temperature": 0.0, "avg_logprob": -0.11576739302626601, "compression_ratio": 1.630952380952381, "no_speech_prob": 0.0023939560633152723}, {"id": 283, "seek": 108790, "start": 1104.22, "end": 1106.22, "text": " thing you don't get to really say", "tokens": [51180, 551, 291, 500, 380, 483, 281, 534, 584, 51280], "temperature": 0.0, "avg_logprob": -0.11576739302626601, "compression_ratio": 1.630952380952381, "no_speech_prob": 0.0023939560633152723}, {"id": 284, "seek": 108790, "start": 1106.7800000000002, "end": 1108.7800000000002, "text": " What you thought was better or worse", "tokens": [51308, 708, 291, 1194, 390, 1101, 420, 5324, 51408], "temperature": 0.0, "avg_logprob": -0.11576739302626601, "compression_ratio": 1.630952380952381, "no_speech_prob": 0.0023939560633152723}, {"id": 285, "seek": 108790, "start": 1108.94, "end": 1110.46, "text": " um", "tokens": [51416, 1105, 51492], "temperature": 0.0, "avg_logprob": -0.11576739302626601, "compression_ratio": 1.630952380952381, "no_speech_prob": 0.0023939560633152723}, {"id": 286, "seek": 108790, "start": 1110.46, "end": 1112.46, "text": " But this turns out to be enough", "tokens": [51492, 583, 341, 4523, 484, 281, 312, 1547, 51592], "temperature": 0.0, "avg_logprob": -0.11576739302626601, "compression_ratio": 1.630952380952381, "no_speech_prob": 0.0023939560633152723}, {"id": 287, "seek": 108790, "start": 1112.7800000000002, "end": 1114.7800000000002, "text": " Of a training signal to do pretty well", "tokens": [51608, 2720, 257, 3097, 6358, 281, 360, 1238, 731, 51708], "temperature": 0.0, "avg_logprob": -0.11576739302626601, "compression_ratio": 1.630952380952381, "no_speech_prob": 0.0023939560633152723}, {"id": 288, "seek": 108790, "start": 1115.26, "end": 1116.3000000000002, "text": " um", "tokens": [51732, 1105, 51784], "temperature": 0.0, "avg_logprob": -0.11576739302626601, "compression_ratio": 1.630952380952381, "no_speech_prob": 0.0023939560633152723}, {"id": 289, "seek": 111630, "start": 1116.3, "end": 1119.98, "text": " But so like for so one example right of a time where maybe this doesn't work is", "tokens": [50364, 583, 370, 411, 337, 370, 472, 1365, 558, 295, 257, 565, 689, 1310, 341, 1177, 380, 589, 307, 50548], "temperature": 0.0, "avg_logprob": -0.15389997582686574, "compression_ratio": 1.645933014354067, "no_speech_prob": 0.0010155680356547236}, {"id": 290, "seek": 111630, "start": 1121.18, "end": 1122.78, "text": " the", "tokens": [50608, 264, 50688], "temperature": 0.0, "avg_logprob": -0.15389997582686574, "compression_ratio": 1.645933014354067, "no_speech_prob": 0.0010155680356547236}, {"id": 291, "seek": 111630, "start": 1122.78, "end": 1124.78, "text": " Person asks a factual question", "tokens": [50688, 8443, 8962, 257, 48029, 1168, 50788], "temperature": 0.0, "avg_logprob": -0.15389997582686574, "compression_ratio": 1.645933014354067, "no_speech_prob": 0.0010155680356547236}, {"id": 292, "seek": 111630, "start": 1125.4199999999998, "end": 1127.4199999999998, "text": " and the model responds", "tokens": [50820, 293, 264, 2316, 27331, 50920], "temperature": 0.0, "avg_logprob": -0.15389997582686574, "compression_ratio": 1.645933014354067, "no_speech_prob": 0.0010155680356547236}, {"id": 293, "seek": 111630, "start": 1127.58, "end": 1130.86, "text": " Uh with an answer and that answer is actually not correct", "tokens": [50928, 4019, 365, 364, 1867, 293, 300, 1867, 307, 767, 406, 3006, 51092], "temperature": 0.0, "avg_logprob": -0.15389997582686574, "compression_ratio": 1.645933014354067, "no_speech_prob": 0.0010155680356547236}, {"id": 294, "seek": 111630, "start": 1132.1399999999999, "end": 1133.58, "text": " um", "tokens": [51156, 1105, 51228], "temperature": 0.0, "avg_logprob": -0.15389997582686574, "compression_ratio": 1.645933014354067, "no_speech_prob": 0.0010155680356547236}, {"id": 295, "seek": 111630, "start": 1133.58, "end": 1134.94, "text": " Right now", "tokens": [51228, 1779, 586, 51296], "temperature": 0.0, "avg_logprob": -0.15389997582686574, "compression_ratio": 1.645933014354067, "no_speech_prob": 0.0010155680356547236}, {"id": 296, "seek": 111630, "start": 1134.94, "end": 1137.5, "text": " Possibly the human doesn't know the correct answer", "tokens": [51296, 33112, 3545, 264, 1952, 1177, 380, 458, 264, 3006, 1867, 51424], "temperature": 0.0, "avg_logprob": -0.15389997582686574, "compression_ratio": 1.645933014354067, "no_speech_prob": 0.0010155680356547236}, {"id": 297, "seek": 111630, "start": 1138.06, "end": 1140.22, "text": " And so if the model is faced with a choice", "tokens": [51452, 400, 370, 498, 264, 2316, 307, 11446, 365, 257, 3922, 51560], "temperature": 0.0, "avg_logprob": -0.15389997582686574, "compression_ratio": 1.645933014354067, "no_speech_prob": 0.0010155680356547236}, {"id": 298, "seek": 111630, "start": 1141.02, "end": 1144.46, "text": " Uh, do I respond with sorry? I don't know", "tokens": [51600, 4019, 11, 360, 286, 4196, 365, 2597, 30, 286, 500, 380, 458, 51772], "temperature": 0.0, "avg_logprob": -0.15389997582686574, "compression_ratio": 1.645933014354067, "no_speech_prob": 0.0010155680356547236}, {"id": 299, "seek": 114630, "start": 1146.3799999999999, "end": 1148.3799999999999, "text": " That's definitely going to get me", "tokens": [50368, 663, 311, 2138, 516, 281, 483, 385, 50468], "temperature": 0.0, "avg_logprob": -0.1930400920364092, "compression_ratio": 1.6387665198237886, "no_speech_prob": 0.0025492871645838022}, {"id": 300, "seek": 114630, "start": 1148.54, "end": 1150.54, "text": " Uh, not a great score", "tokens": [50476, 4019, 11, 406, 257, 869, 6175, 50576], "temperature": 0.0, "avg_logprob": -0.1930400920364092, "compression_ratio": 1.6387665198237886, "no_speech_prob": 0.0025492871645838022}, {"id": 301, "seek": 114630, "start": 1151.18, "end": 1153.74, "text": " Compared to do I just like take a stab at it?", "tokens": [50608, 30539, 281, 360, 286, 445, 411, 747, 257, 16343, 412, 309, 30, 50736], "temperature": 0.0, "avg_logprob": -0.1930400920364092, "compression_ratio": 1.6387665198237886, "no_speech_prob": 0.0025492871645838022}, {"id": 302, "seek": 114630, "start": 1154.78, "end": 1161.98, "text": " Uh, if the humans are not reliably able to spot when the thing makes mistakes and like fact-check it and punish it for that", "tokens": [50788, 4019, 11, 498, 264, 6255, 366, 406, 49927, 1075, 281, 4008, 562, 264, 551, 1669, 8038, 293, 411, 1186, 12, 15723, 309, 293, 9842, 309, 337, 300, 51148], "temperature": 0.0, "avg_logprob": -0.1930400920364092, "compression_ratio": 1.6387665198237886, "no_speech_prob": 0.0025492871645838022}, {"id": 303, "seek": 114630, "start": 1162.46, "end": 1166.54, "text": " Uh, it will do that and so chat gpt as we know", "tokens": [51172, 4019, 11, 309, 486, 360, 300, 293, 370, 5081, 290, 662, 382, 321, 458, 51376], "temperature": 0.0, "avg_logprob": -0.1930400920364092, "compression_ratio": 1.6387665198237886, "no_speech_prob": 0.0025492871645838022}, {"id": 304, "seek": 114630, "start": 1167.18, "end": 1170.3799999999999, "text": " Uh, is it is a total bulletish like it will constantly", "tokens": [51408, 4019, 11, 307, 309, 307, 257, 3217, 11632, 742, 411, 309, 486, 6460, 51568], "temperature": 0.0, "avg_logprob": -0.1930400920364092, "compression_ratio": 1.6387665198237886, "no_speech_prob": 0.0025492871645838022}, {"id": 305, "seek": 114630, "start": 1171.8999999999999, "end": 1175.02, "text": " Uh, it very rarely says that it doesn't know", "tokens": [51644, 4019, 11, 309, 588, 13752, 1619, 300, 309, 1177, 380, 458, 51800], "temperature": 0.0, "avg_logprob": -0.1930400920364092, "compression_ratio": 1.6387665198237886, "no_speech_prob": 0.0025492871645838022}, {"id": 306, "seek": 117502, "start": 1175.5, "end": 1176.86, "text": " unless", "tokens": [50388, 5969, 50456], "temperature": 0.0, "avg_logprob": -0.1479304631551107, "compression_ratio": 1.6, "no_speech_prob": 0.002049725502729416}, {"id": 307, "seek": 117502, "start": 1176.86, "end": 1179.74, "text": " It's being asked a question, which uh", "tokens": [50456, 467, 311, 885, 2351, 257, 1168, 11, 597, 2232, 50600], "temperature": 0.0, "avg_logprob": -0.1479304631551107, "compression_ratio": 1.6, "no_speech_prob": 0.002049725502729416}, {"id": 308, "seek": 117502, "start": 1180.54, "end": 1186.54, "text": " Is part of their like safety protocols that it is going to decide not to answer in which case it will say it doesn't know", "tokens": [50640, 1119, 644, 295, 641, 411, 4514, 20618, 300, 309, 307, 516, 281, 4536, 406, 281, 1867, 294, 597, 1389, 309, 486, 584, 309, 1177, 380, 458, 50940], "temperature": 0.0, "avg_logprob": -0.1479304631551107, "compression_ratio": 1.6, "no_speech_prob": 0.002049725502729416}, {"id": 309, "seek": 117502, "start": 1187.18, "end": 1191.74, "text": " Even if it kind of does right even if the model itself maybe does", "tokens": [50972, 2754, 498, 309, 733, 295, 775, 558, 754, 498, 264, 2316, 2564, 1310, 775, 51200], "temperature": 0.0, "avg_logprob": -0.1479304631551107, "compression_ratio": 1.6, "no_speech_prob": 0.002049725502729416}, {"id": 310, "seek": 117502, "start": 1192.22, "end": 1194.94, "text": " Uh, the assistant will insist that it doesn't", "tokens": [51224, 4019, 11, 264, 10994, 486, 13466, 300, 309, 1177, 380, 51360], "temperature": 0.0, "avg_logprob": -0.1479304631551107, "compression_ratio": 1.6, "no_speech_prob": 0.002049725502729416}, {"id": 311, "seek": 117502, "start": 1196.22, "end": 1197.9, "text": " um", "tokens": [51424, 1105, 51508], "temperature": 0.0, "avg_logprob": -0.1479304631551107, "compression_ratio": 1.6, "no_speech_prob": 0.002049725502729416}, {"id": 312, "seek": 117502, "start": 1197.9, "end": 1199.9, "text": " So that's one thing if you can't fact check", "tokens": [51508, 407, 300, 311, 472, 551, 498, 291, 393, 380, 1186, 1520, 51608], "temperature": 0.0, "avg_logprob": -0.1479304631551107, "compression_ratio": 1.6, "no_speech_prob": 0.002049725502729416}, {"id": 313, "seek": 117502, "start": 1201.02, "end": 1203.02, "text": " But then uh more than that", "tokens": [51664, 583, 550, 2232, 544, 813, 300, 51764], "temperature": 0.0, "avg_logprob": -0.1479304631551107, "compression_ratio": 1.6, "no_speech_prob": 0.002049725502729416}, {"id": 314, "seek": 120302, "start": 1203.82, "end": 1207.02, "text": " Uh, there is an incentive for deception", "tokens": [50404, 4019, 11, 456, 307, 364, 22346, 337, 40451, 50564], "temperature": 0.0, "avg_logprob": -0.12905140810234603, "compression_ratio": 1.5841121495327102, "no_speech_prob": 0.00780567666515708}, {"id": 315, "seek": 120302, "start": 1207.9, "end": 1210.1399999999999, "text": " right anytime the system is uh", "tokens": [50608, 558, 13038, 264, 1185, 307, 2232, 50720], "temperature": 0.0, "avg_logprob": -0.12905140810234603, "compression_ratio": 1.5841121495327102, "no_speech_prob": 0.00780567666515708}, {"id": 316, "seek": 120302, "start": 1210.94, "end": 1216.94, "text": " Anytime you can get a more likely to get approval by deceiving the person you're talking to", "tokens": [50760, 39401, 291, 393, 483, 257, 544, 3700, 281, 483, 13317, 538, 14088, 2123, 264, 954, 291, 434, 1417, 281, 51060], "temperature": 0.0, "avg_logprob": -0.12905140810234603, "compression_ratio": 1.5841121495327102, "no_speech_prob": 0.00780567666515708}, {"id": 317, "seek": 120302, "start": 1217.82, "end": 1219.82, "text": " That's better. Um", "tokens": [51104, 663, 311, 1101, 13, 3301, 51204], "temperature": 0.0, "avg_logprob": -0.12905140810234603, "compression_ratio": 1.5841121495327102, "no_speech_prob": 0.00780567666515708}, {"id": 318, "seek": 120302, "start": 1220.3799999999999, "end": 1225.66, "text": " And this is a thing that actually did happen a little bit in the reward modeling situation", "tokens": [51232, 400, 341, 307, 257, 551, 300, 767, 630, 1051, 257, 707, 857, 294, 264, 7782, 15983, 2590, 51496], "temperature": 0.0, "avg_logprob": -0.12905140810234603, "compression_ratio": 1.5841121495327102, "no_speech_prob": 0.00780567666515708}, {"id": 319, "seek": 120302, "start": 1226.3799999999999, "end": 1229.9, "text": " um, they were trying to train a thing with a hand to pick up a ball", "tokens": [51532, 1105, 11, 436, 645, 1382, 281, 3847, 257, 551, 365, 257, 1011, 281, 1888, 493, 257, 2594, 51708], "temperature": 0.0, "avg_logprob": -0.12905140810234603, "compression_ratio": 1.5841121495327102, "no_speech_prob": 0.00780567666515708}, {"id": 320, "seek": 122990, "start": 1230.6200000000001, "end": 1233.74, "text": " And it realized that there's only it's not a 3d camera", "tokens": [50400, 400, 309, 5334, 300, 456, 311, 787, 309, 311, 406, 257, 805, 67, 2799, 50556], "temperature": 0.0, "avg_logprob": -0.12165226183439556, "compression_ratio": 1.5458515283842795, "no_speech_prob": 0.009853778406977654}, {"id": 321, "seek": 122990, "start": 1234.38, "end": 1238.3000000000002, "text": " And so if it puts its hand like between the ball and the camera", "tokens": [50588, 400, 370, 498, 309, 8137, 1080, 1011, 411, 1296, 264, 2594, 293, 264, 2799, 50784], "temperature": 0.0, "avg_logprob": -0.12165226183439556, "compression_ratio": 1.5458515283842795, "no_speech_prob": 0.009853778406977654}, {"id": 322, "seek": 122990, "start": 1238.94, "end": 1244.6200000000001, "text": " This looks like it's going to get the ball, but doesn't actually get it. But the human uh", "tokens": [50816, 639, 1542, 411, 309, 311, 516, 281, 483, 264, 2594, 11, 457, 1177, 380, 767, 483, 309, 13, 583, 264, 1952, 2232, 51100], "temperature": 0.0, "avg_logprob": -0.12165226183439556, "compression_ratio": 1.5458515283842795, "no_speech_prob": 0.009853778406977654}, {"id": 323, "seek": 122990, "start": 1245.66, "end": 1247.66, "text": " Feedback providers", "tokens": [51152, 33720, 3207, 11330, 51252], "temperature": 0.0, "avg_logprob": -0.12165226183439556, "compression_ratio": 1.5458515283842795, "no_speech_prob": 0.009853778406977654}, {"id": 324, "seek": 122990, "start": 1247.9, "end": 1251.26, "text": " Were presented with something that seemed to be good. So they gave it the thumbs up", "tokens": [51264, 12448, 8212, 365, 746, 300, 6576, 281, 312, 665, 13, 407, 436, 2729, 309, 264, 8838, 493, 51432], "temperature": 0.0, "avg_logprob": -0.12165226183439556, "compression_ratio": 1.5458515283842795, "no_speech_prob": 0.009853778406977654}, {"id": 325, "seek": 122990, "start": 1251.98, "end": 1255.1000000000001, "text": " um, so this like general broad category", "tokens": [51468, 1105, 11, 370, 341, 411, 2674, 4152, 7719, 51624], "temperature": 0.0, "avg_logprob": -0.12165226183439556, "compression_ratio": 1.5458515283842795, "no_speech_prob": 0.009853778406977654}, {"id": 326, "seek": 122990, "start": 1255.66, "end": 1257.26, "text": " um", "tokens": [51652, 1105, 51732], "temperature": 0.0, "avg_logprob": -0.12165226183439556, "compression_ratio": 1.5458515283842795, "no_speech_prob": 0.009853778406977654}, {"id": 327, "seek": 125726, "start": 1257.26, "end": 1259.26, "text": " Systems that are trained in this way", "tokens": [50364, 27059, 300, 366, 8895, 294, 341, 636, 50464], "temperature": 0.0, "avg_logprob": -0.13309544498480638, "compression_ratio": 1.5387755102040817, "no_speech_prob": 0.00032998723327182233}, {"id": 328, "seek": 125726, "start": 1259.34, "end": 1261.82, "text": " Are only as good as your ability", "tokens": [50468, 2014, 787, 382, 665, 382, 428, 3485, 50592], "temperature": 0.0, "avg_logprob": -0.13309544498480638, "compression_ratio": 1.5387755102040817, "no_speech_prob": 0.00032998723327182233}, {"id": 329, "seek": 125726, "start": 1262.46, "end": 1264.78, "text": " To distinguish good from bad in the outputs", "tokens": [50624, 1407, 20206, 665, 490, 1578, 294, 264, 23930, 50740], "temperature": 0.0, "avg_logprob": -0.13309544498480638, "compression_ratio": 1.5387755102040817, "no_speech_prob": 0.00032998723327182233}, {"id": 330, "seek": 125726, "start": 1265.66, "end": 1269.98, "text": " Not all the humans will know the answer is right. So it's what appears to be good", "tokens": [50784, 1726, 439, 264, 6255, 486, 458, 264, 1867, 307, 558, 13, 407, 309, 311, 437, 7038, 281, 312, 665, 51000], "temperature": 0.0, "avg_logprob": -0.13309544498480638, "compression_ratio": 1.5387755102040817, "no_speech_prob": 0.00032998723327182233}, {"id": 331, "seek": 125726, "start": 1270.54, "end": 1277.66, "text": " You know, it's having exams marked by non-experts, isn't it? Right. Yeah, exactly in the gpt3 thing. We talked about writing poems", "tokens": [51028, 509, 458, 11, 309, 311, 1419, 20514, 12658, 538, 2107, 12, 3121, 610, 1373, 11, 1943, 380, 309, 30, 1779, 13, 865, 11, 2293, 294, 264, 290, 662, 18, 551, 13, 492, 2825, 466, 3579, 24014, 51384], "temperature": 0.0, "avg_logprob": -0.13309544498480638, "compression_ratio": 1.5387755102040817, "no_speech_prob": 0.00032998723327182233}, {"id": 332, "seek": 125726, "start": 1278.54, "end": 1279.74, "text": " right", "tokens": [51428, 558, 51488], "temperature": 0.0, "avg_logprob": -0.13309544498480638, "compression_ratio": 1.5387755102040817, "no_speech_prob": 0.00032998723327182233}, {"id": 333, "seek": 125726, "start": 1279.74, "end": 1283.66, "text": " and uh for various reasons partly to do with", "tokens": [51488, 293, 2232, 337, 3683, 4112, 17031, 281, 360, 365, 51684], "temperature": 0.0, "avg_logprob": -0.13309544498480638, "compression_ratio": 1.5387755102040817, "no_speech_prob": 0.00032998723327182233}, {"id": 334, "seek": 128366, "start": 1284.46, "end": 1289.8200000000002, "text": " The way that these language models do their tokenization the byte pair encoding stuff", "tokens": [50404, 440, 636, 300, 613, 2856, 5245, 360, 641, 14862, 2144, 264, 40846, 6119, 43430, 1507, 50672], "temperature": 0.0, "avg_logprob": -0.13921484493073963, "compression_ratio": 1.6294642857142858, "no_speech_prob": 0.009123357944190502}, {"id": 335, "seek": 128366, "start": 1290.3000000000002, "end": 1292.8600000000001, "text": " Uh, the models have a really hard time with rhyme", "tokens": [50696, 4019, 11, 264, 5245, 362, 257, 534, 1152, 565, 365, 34753, 50824], "temperature": 0.0, "avg_logprob": -0.13921484493073963, "compression_ratio": 1.6294642857142858, "no_speech_prob": 0.009123357944190502}, {"id": 336, "seek": 128366, "start": 1293.98, "end": 1294.94, "text": " um", "tokens": [50880, 1105, 50928], "temperature": 0.0, "avg_logprob": -0.13921484493073963, "compression_ratio": 1.6294642857142858, "no_speech_prob": 0.009123357944190502}, {"id": 337, "seek": 128366, "start": 1294.94, "end": 1298.22, "text": " I mean, you know rhyme is tricky, but it's especially tricky when you kind of", "tokens": [50928, 286, 914, 11, 291, 458, 34753, 307, 12414, 11, 457, 309, 311, 2318, 12414, 562, 291, 733, 295, 51092], "temperature": 0.0, "avg_logprob": -0.13921484493073963, "compression_ratio": 1.6294642857142858, "no_speech_prob": 0.009123357944190502}, {"id": 338, "seek": 128366, "start": 1300.14, "end": 1306.46, "text": " Don't inherently have any concept of like sound of spoken language when your entire universe is tokens", "tokens": [51188, 1468, 380, 27993, 362, 604, 3410, 295, 411, 1626, 295, 10759, 2856, 562, 428, 2302, 6445, 307, 22667, 51504], "temperature": 0.0, "avg_logprob": -0.13921484493073963, "compression_ratio": 1.6294642857142858, "no_speech_prob": 0.009123357944190502}, {"id": 339, "seek": 128366, "start": 1306.78, "end": 1309.02, "text": " Figuring out especially with english spelling", "tokens": [51520, 22443, 1345, 484, 2318, 365, 32169, 22254, 51632], "temperature": 0.0, "avg_logprob": -0.13921484493073963, "compression_ratio": 1.6294642857142858, "no_speech_prob": 0.009123357944190502}, {"id": 340, "seek": 130902, "start": 1309.42, "end": 1316.1399999999999, "text": " Figuring out which words rhyme with each other is is is not easy. You have to consume quite a lot of poetry to like figure out", "tokens": [50384, 22443, 1345, 484, 597, 2283, 34753, 365, 1184, 661, 307, 307, 307, 406, 1858, 13, 509, 362, 281, 14732, 1596, 257, 688, 295, 15155, 281, 411, 2573, 484, 50720], "temperature": 0.0, "avg_logprob": -0.1673362808998185, "compression_ratio": 1.6343612334801763, "no_speech_prob": 0.006486769765615463}, {"id": 341, "seek": 130902, "start": 1316.7, "end": 1321.26, "text": " Uh, that kind of thing and and getting dpt3 to write good poems is tricky chat gpt", "tokens": [50748, 4019, 11, 300, 733, 295, 551, 293, 293, 1242, 274, 662, 18, 281, 2464, 665, 24014, 307, 12414, 5081, 290, 662, 50976], "temperature": 0.0, "avg_logprob": -0.1673362808998185, "compression_ratio": 1.6343612334801763, "no_speech_prob": 0.006486769765615463}, {"id": 342, "seek": 130902, "start": 1321.98, "end": 1323.98, "text": " is much more", "tokens": [51012, 307, 709, 544, 51112], "temperature": 0.0, "avg_logprob": -0.1673362808998185, "compression_ratio": 1.6343612334801763, "no_speech_prob": 0.006486769765615463}, {"id": 343, "seek": 130902, "start": 1324.54, "end": 1327.34, "text": " Able to write poems, but interestingly", "tokens": [51140, 316, 638, 281, 2464, 24014, 11, 457, 25873, 51280], "temperature": 0.0, "avg_logprob": -0.1673362808998185, "compression_ratio": 1.6343612334801763, "no_speech_prob": 0.006486769765615463}, {"id": 344, "seek": 130902, "start": 1328.54, "end": 1332.46, "text": " It it kind of always writes the same kind of poem approximately", "tokens": [51340, 467, 309, 733, 295, 1009, 13657, 264, 912, 733, 295, 13065, 10447, 51536], "temperature": 0.0, "avg_logprob": -0.1673362808998185, "compression_ratio": 1.6343612334801763, "no_speech_prob": 0.006486769765615463}, {"id": 345, "seek": 130902, "start": 1333.02, "end": 1336.1399999999999, "text": " like if you ask it to write you uh a limerick", "tokens": [51564, 411, 498, 291, 1029, 309, 281, 2464, 291, 2232, 257, 2364, 260, 618, 51720], "temperature": 0.0, "avg_logprob": -0.1673362808998185, "compression_ratio": 1.6343612334801763, "no_speech_prob": 0.006486769765615463}, {"id": 346, "seek": 133614, "start": 1336.94, "end": 1338.94, "text": " Or an ode or a sonnet", "tokens": [50404, 1610, 364, 45711, 420, 257, 1872, 7129, 50504], "temperature": 0.0, "avg_logprob": -0.11455899661349267, "compression_ratio": 1.5836909871244635, "no_speech_prob": 0.008184614591300488}, {"id": 347, "seek": 133614, "start": 1339.66, "end": 1342.3000000000002, "text": " Uh, you always get back approximately the same", "tokens": [50540, 4019, 11, 291, 1009, 483, 646, 10447, 264, 912, 50672], "temperature": 0.0, "avg_logprob": -0.11455899661349267, "compression_ratio": 1.5836909871244635, "no_speech_prob": 0.008184614591300488}, {"id": 348, "seek": 133614, "start": 1343.1000000000001, "end": 1344.7, "text": " type of thing", "tokens": [50712, 2010, 295, 551, 50792], "temperature": 0.0, "avg_logprob": -0.11455899661349267, "compression_ratio": 1.5836909871244635, "no_speech_prob": 0.008184614591300488}, {"id": 349, "seek": 133614, "start": 1344.7, "end": 1351.42, "text": " And I hypothesize that this is because the people providing human feedback did not in fact know", "tokens": [50792, 400, 286, 14276, 1125, 300, 341, 307, 570, 264, 561, 6530, 1952, 5824, 630, 406, 294, 1186, 458, 51128], "temperature": 0.0, "avg_logprob": -0.11455899661349267, "compression_ratio": 1.5836909871244635, "no_speech_prob": 0.008184614591300488}, {"id": 350, "seek": 133614, "start": 1352.38, "end": 1355.0200000000002, "text": " The requirements for something to be a sonnet, right?", "tokens": [51176, 440, 7728, 337, 746, 281, 312, 257, 1872, 7129, 11, 558, 30, 51308], "temperature": 0.0, "avg_logprob": -0.11455899661349267, "compression_ratio": 1.5836909871244635, "no_speech_prob": 0.008184614591300488}, {"id": 351, "seek": 133614, "start": 1355.5800000000002, "end": 1358.5400000000002, "text": " And so if you ask something for a sonnet it again has a choice", "tokens": [51336, 400, 370, 498, 291, 1029, 746, 337, 257, 1872, 7129, 309, 797, 575, 257, 3922, 51484], "temperature": 0.0, "avg_logprob": -0.11455899661349267, "compression_ratio": 1.5836909871244635, "no_speech_prob": 0.008184614591300488}, {"id": 352, "seek": 133614, "start": 1358.7800000000002, "end": 1363.74, "text": " Do I try to do this quite difficult thing and adhere to all of the rules?", "tokens": [51496, 1144, 286, 853, 281, 360, 341, 1596, 2252, 551, 293, 33584, 281, 439, 295, 264, 4474, 30, 51744], "temperature": 0.0, "avg_logprob": -0.11455899661349267, "compression_ratio": 1.5836909871244635, "no_speech_prob": 0.008184614591300488}, {"id": 353, "seek": 136374, "start": 1364.38, "end": 1366.38, "text": " of like stress pattern", "tokens": [50396, 295, 411, 4244, 5102, 50496], "temperature": 0.0, "avg_logprob": -0.13440551572633022, "compression_ratio": 1.6695652173913043, "no_speech_prob": 0.004264105577021837}, {"id": 354, "seek": 136374, "start": 1367.1, "end": 1373.74, "text": " And structure and everything of a sonnet and maybe risk screwing it up or do I just do like a rhyming poem and", "tokens": [50532, 400, 3877, 293, 1203, 295, 257, 1872, 7129, 293, 1310, 3148, 5630, 278, 309, 493, 420, 360, 286, 445, 360, 411, 257, 8740, 2810, 13065, 293, 50864], "temperature": 0.0, "avg_logprob": -0.13440551572633022, "compression_ratio": 1.6695652173913043, "no_speech_prob": 0.004264105577021837}, {"id": 355, "seek": 136374, "start": 1374.46, "end": 1376.78, "text": " kind of rely on the human to", "tokens": [50900, 733, 295, 10687, 322, 264, 1952, 281, 51016], "temperature": 0.0, "avg_logprob": -0.13440551572633022, "compression_ratio": 1.6695652173913043, "no_speech_prob": 0.004264105577021837}, {"id": 356, "seek": 136374, "start": 1377.34, "end": 1381.1, "text": " Prefer that because they don't know that that's not what a sonnet is supposed to look like", "tokens": [51044, 48401, 300, 570, 436, 500, 380, 458, 300, 300, 311, 406, 437, 257, 1872, 7129, 307, 3442, 281, 574, 411, 51232], "temperature": 0.0, "avg_logprob": -0.13440551572633022, "compression_ratio": 1.6695652173913043, "no_speech_prob": 0.004264105577021837}, {"id": 357, "seek": 136374, "start": 1381.58, "end": 1386.22, "text": " It's easy to look at that and think oh the model doesn't know the difference between these types of", "tokens": [51256, 467, 311, 1858, 281, 574, 412, 300, 293, 519, 1954, 264, 2316, 1177, 380, 458, 264, 2649, 1296, 613, 3467, 295, 51488], "temperature": 0.0, "avg_logprob": -0.13440551572633022, "compression_ratio": 1.6695652173913043, "no_speech_prob": 0.004264105577021837}, {"id": 358, "seek": 136374, "start": 1387.02, "end": 1389.02, "text": " poems, right", "tokens": [51528, 24014, 11, 558, 51628], "temperature": 0.0, "avg_logprob": -0.13440551572633022, "compression_ratio": 1.6695652173913043, "no_speech_prob": 0.004264105577021837}, {"id": 359, "seek": 136374, "start": 1389.02, "end": 1390.22, "text": " but", "tokens": [51628, 457, 51688], "temperature": 0.0, "avg_logprob": -0.13440551572633022, "compression_ratio": 1.6695652173913043, "no_speech_prob": 0.004264105577021837}, {"id": 360, "seek": 136374, "start": 1390.22, "end": 1392.22, "text": " you could say", "tokens": [51688, 291, 727, 584, 51788], "temperature": 0.0, "avg_logprob": -0.13440551572633022, "compression_ratio": 1.6695652173913043, "no_speech_prob": 0.004264105577021837}, {"id": 361, "seek": 139222, "start": 1392.46, "end": 1395.58, "text": " That it just thinks that you don't know the difference", "tokens": [50376, 663, 309, 445, 7309, 300, 291, 500, 380, 458, 264, 2649, 50532], "temperature": 0.0, "avg_logprob": -0.11132650490266731, "compression_ratio": 1.6116504854368932, "no_speech_prob": 0.0028436030261218548}, {"id": 362, "seek": 139222, "start": 1395.98, "end": 1399.74, "text": " But specifically this comes out of misalignment if it were better aligned", "tokens": [50552, 583, 4682, 341, 1487, 484, 295, 3346, 304, 41134, 498, 309, 645, 1101, 17962, 50740], "temperature": 0.0, "avg_logprob": -0.11132650490266731, "compression_ratio": 1.6116504854368932, "no_speech_prob": 0.0028436030261218548}, {"id": 363, "seek": 139222, "start": 1400.54, "end": 1404.46, "text": " It could either do its best shot a generator sonnet", "tokens": [50780, 467, 727, 2139, 360, 1080, 1151, 3347, 257, 19265, 1872, 7129, 50976], "temperature": 0.0, "avg_logprob": -0.11132650490266731, "compression_ratio": 1.6116504854368932, "no_speech_prob": 0.0028436030261218548}, {"id": 364, "seek": 139222, "start": 1405.26, "end": 1408.46, "text": " Or tell you that it can't quite remember how to generate a sonnet", "tokens": [51016, 1610, 980, 291, 300, 309, 393, 380, 1596, 1604, 577, 281, 8460, 257, 1872, 7129, 51176], "temperature": 0.0, "avg_logprob": -0.11132650490266731, "compression_ratio": 1.6116504854368932, "no_speech_prob": 0.0028436030261218548}, {"id": 365, "seek": 139222, "start": 1409.42, "end": 1411.1000000000001, "text": " this thing of", "tokens": [51224, 341, 551, 295, 51308], "temperature": 0.0, "avg_logprob": -0.11132650490266731, "compression_ratio": 1.6116504854368932, "no_speech_prob": 0.0028436030261218548}, {"id": 366, "seek": 139222, "start": 1411.1000000000001, "end": 1413.1000000000001, "text": " with complete confidence", "tokens": [51308, 365, 3566, 6687, 51408], "temperature": 0.0, "avg_logprob": -0.11132650490266731, "compression_ratio": 1.6116504854368932, "no_speech_prob": 0.0028436030261218548}, {"id": 367, "seek": 139222, "start": 1413.42, "end": 1415.9, "text": " Generating you something which is not a sonnet", "tokens": [51424, 15409, 990, 291, 746, 597, 307, 406, 257, 1872, 7129, 51548], "temperature": 0.0, "avg_logprob": -0.11132650490266731, "compression_ratio": 1.6116504854368932, "no_speech_prob": 0.0028436030261218548}, {"id": 368, "seek": 141590, "start": 1416.38, "end": 1422.3000000000002, "text": " Because during the training process it believes that humans don't know what sonnets are anyway and it can get away with it", "tokens": [50388, 1436, 1830, 264, 3097, 1399, 309, 12307, 300, 6255, 500, 380, 458, 437, 1872, 77, 1385, 366, 4033, 293, 309, 393, 483, 1314, 365, 309, 50684], "temperature": 0.0, "avg_logprob": -0.11967805288370373, "compression_ratio": 1.6624472573839661, "no_speech_prob": 0.09665826708078384}, {"id": 369, "seek": 141590, "start": 1422.8600000000001, "end": 1428.14, "text": " Right. This is misaligned behavior. This is not a big problem that the thing generates bad poetry", "tokens": [50712, 1779, 13, 639, 307, 3346, 304, 16690, 5223, 13, 639, 307, 406, 257, 955, 1154, 300, 264, 551, 23815, 1578, 15155, 50976], "temperature": 0.0, "avg_logprob": -0.11967805288370373, "compression_ratio": 1.6624472573839661, "no_speech_prob": 0.09665826708078384}, {"id": 370, "seek": 141590, "start": 1428.7800000000002, "end": 1429.98, "text": " um", "tokens": [51008, 1105, 51068], "temperature": 0.0, "avg_logprob": -0.11967805288370373, "compression_ratio": 1.6624472573839661, "no_speech_prob": 0.09665826708078384}, {"id": 371, "seek": 141590, "start": 1429.98, "end": 1431.98, "text": " It's kind of a problem that it lies", "tokens": [51068, 467, 311, 733, 295, 257, 1154, 300, 309, 9134, 51168], "temperature": 0.0, "avg_logprob": -0.11967805288370373, "compression_ratio": 1.6624472573839661, "no_speech_prob": 0.09665826708078384}, {"id": 372, "seek": 141590, "start": 1433.18, "end": 1436.5400000000002, "text": " Uh, or that it that it bullshits. This is like", "tokens": [51228, 4019, 11, 420, 300, 309, 300, 309, 4693, 2716, 1208, 13, 639, 307, 411, 51396], "temperature": 0.0, "avg_logprob": -0.11967805288370373, "compression_ratio": 1.6624472573839661, "no_speech_prob": 0.09665826708078384}, {"id": 373, "seek": 141590, "start": 1437.8200000000002, "end": 1441.26, "text": " In the short term pretty solvable by just allowing the thing to use google", "tokens": [51460, 682, 264, 2099, 1433, 1238, 1404, 17915, 538, 445, 8293, 264, 551, 281, 764, 20742, 51632], "temperature": 0.0, "avg_logprob": -0.11967805288370373, "compression_ratio": 1.6624472573839661, "no_speech_prob": 0.09665826708078384}, {"id": 374, "seek": 141590, "start": 1442.14, "end": 1443.26, "text": " because like", "tokens": [51676, 570, 411, 51732], "temperature": 0.0, "avg_logprob": -0.11967805288370373, "compression_ratio": 1.6624472573839661, "no_speech_prob": 0.09665826708078384}, {"id": 375, "seek": 144326, "start": 1443.26, "end": 1446.7, "text": " A person who doesn't care about the truth at all and is just trying to", "tokens": [50364, 316, 954, 567, 1177, 380, 1127, 466, 264, 3494, 412, 439, 293, 307, 445, 1382, 281, 50536], "temperature": 0.0, "avg_logprob": -0.1133651102290434, "compression_ratio": 1.7811320754716982, "no_speech_prob": 0.0035348942037671804}, {"id": 376, "seek": 144326, "start": 1447.34, "end": 1449.34, "text": " Say something that'll make you give a thumbs up", "tokens": [50568, 6463, 746, 300, 603, 652, 291, 976, 257, 8838, 493, 50668], "temperature": 0.0, "avg_logprob": -0.1133651102290434, "compression_ratio": 1.7811320754716982, "no_speech_prob": 0.0035348942037671804}, {"id": 377, "seek": 144326, "start": 1449.82, "end": 1451.1, "text": " uh", "tokens": [50692, 2232, 50756], "temperature": 0.0, "avg_logprob": -0.1133651102290434, "compression_ratio": 1.7811320754716982, "no_speech_prob": 0.0035348942037671804}, {"id": 378, "seek": 144326, "start": 1451.1, "end": 1453.1, "text": " Is going to lie to you a lot", "tokens": [50756, 1119, 516, 281, 4544, 281, 291, 257, 688, 50856], "temperature": 0.0, "avg_logprob": -0.1133651102290434, "compression_ratio": 1.7811320754716982, "no_speech_prob": 0.0035348942037671804}, {"id": 379, "seek": 144326, "start": 1453.42, "end": 1454.3799999999999, "text": " but", "tokens": [50872, 457, 50920], "temperature": 0.0, "avg_logprob": -0.1133651102290434, "compression_ratio": 1.7811320754716982, "no_speech_prob": 0.0035348942037671804}, {"id": 380, "seek": 144326, "start": 1454.3799999999999, "end": 1456.22, "text": " that same person", "tokens": [50920, 300, 912, 954, 51012], "temperature": 0.0, "avg_logprob": -0.1133651102290434, "compression_ratio": 1.7811320754716982, "no_speech_prob": 0.0035348942037671804}, {"id": 381, "seek": 144326, "start": 1456.22, "end": 1458.22, "text": " With the relevant wikipedia page open", "tokens": [51012, 2022, 264, 7340, 261, 1035, 26633, 3028, 1269, 51112], "temperature": 0.0, "avg_logprob": -0.1133651102290434, "compression_ratio": 1.7811320754716982, "no_speech_prob": 0.0035348942037671804}, {"id": 382, "seek": 144326, "start": 1458.94, "end": 1460.94, "text": " It's going to lie to you a lot less", "tokens": [51148, 467, 311, 516, 281, 4544, 281, 291, 257, 688, 1570, 51248], "temperature": 0.0, "avg_logprob": -0.1133651102290434, "compression_ratio": 1.7811320754716982, "no_speech_prob": 0.0035348942037671804}, {"id": 383, "seek": 144326, "start": 1461.42, "end": 1464.78, "text": " Just because they don't they don't have to now because they happen to have it in front of them, right?", "tokens": [51272, 1449, 570, 436, 500, 380, 436, 500, 380, 362, 281, 586, 570, 436, 1051, 281, 362, 309, 294, 1868, 295, 552, 11, 558, 30, 51440], "temperature": 0.0, "avg_logprob": -0.1133651102290434, "compression_ratio": 1.7811320754716982, "no_speech_prob": 0.0035348942037671804}, {"id": 384, "seek": 144326, "start": 1465.1, "end": 1467.1, "text": " So you can solve it's a bit like", "tokens": [51456, 407, 291, 393, 5039, 309, 311, 257, 857, 411, 51556], "temperature": 0.0, "avg_logprob": -0.1133651102290434, "compression_ratio": 1.7811320754716982, "no_speech_prob": 0.0035348942037671804}, {"id": 385, "seek": 144326, "start": 1467.5, "end": 1471.58, "text": " Yeah, it's the yes man thing, isn't it? You know you you want something you need something", "tokens": [51576, 865, 11, 309, 311, 264, 2086, 587, 551, 11, 1943, 380, 309, 30, 509, 458, 291, 291, 528, 746, 291, 643, 746, 51780], "temperature": 0.0, "avg_logprob": -0.1133651102290434, "compression_ratio": 1.7811320754716982, "no_speech_prob": 0.0035348942037671804}, {"id": 386, "seek": 147158, "start": 1471.6599999999999, "end": 1474.78, "text": " I'm going to give you something because you want exactly exactly", "tokens": [50368, 286, 478, 516, 281, 976, 291, 746, 570, 291, 528, 2293, 2293, 50524], "temperature": 0.0, "avg_logprob": -0.14518138340541295, "compression_ratio": 1.6590909090909092, "no_speech_prob": 0.006180159747600555}, {"id": 387, "seek": 147158, "start": 1475.34, "end": 1476.54, "text": " um", "tokens": [50552, 1105, 50612], "temperature": 0.0, "avg_logprob": -0.14518138340541295, "compression_ratio": 1.6590909090909092, "no_speech_prob": 0.006180159747600555}, {"id": 388, "seek": 147158, "start": 1476.54, "end": 1478.9399999999998, "text": " And so so so this agent is kind of", "tokens": [50612, 400, 370, 370, 370, 341, 9461, 307, 733, 295, 50732], "temperature": 0.0, "avg_logprob": -0.14518138340541295, "compression_ratio": 1.6590909090909092, "no_speech_prob": 0.006180159747600555}, {"id": 389, "seek": 147158, "start": 1479.58, "end": 1481.58, "text": " Firstly the agent is kind of a coward", "tokens": [50764, 20042, 264, 9461, 307, 733, 295, 257, 30776, 50864], "temperature": 0.0, "avg_logprob": -0.14518138340541295, "compression_ratio": 1.6590909090909092, "no_speech_prob": 0.006180159747600555}, {"id": 390, "seek": 147158, "start": 1481.82, "end": 1483.82, "text": " Because they won't address any of these", "tokens": [50876, 1436, 436, 1582, 380, 2985, 604, 295, 613, 50976], "temperature": 0.0, "avg_logprob": -0.14518138340541295, "compression_ratio": 1.6590909090909092, "no_speech_prob": 0.006180159747600555}, {"id": 391, "seek": 147158, "start": 1483.8999999999999, "end": 1487.58, "text": " There's a whole bunch of things that it just claims not to be able to do even though it in principle could", "tokens": [50980, 821, 311, 257, 1379, 3840, 295, 721, 300, 309, 445, 9441, 406, 281, 312, 1075, 281, 360, 754, 1673, 309, 294, 8665, 727, 51164], "temperature": 0.0, "avg_logprob": -0.14518138340541295, "compression_ratio": 1.6590909090909092, "no_speech_prob": 0.006180159747600555}, {"id": 392, "seek": 147158, "start": 1488.1399999999999, "end": 1489.74, "text": " and it's also", "tokens": [51192, 293, 309, 311, 611, 51272], "temperature": 0.0, "avg_logprob": -0.14518138340541295, "compression_ratio": 1.6590909090909092, "no_speech_prob": 0.006180159747600555}, {"id": 393, "seek": 147158, "start": 1489.74, "end": 1491.02, "text": " a complete", "tokens": [51272, 257, 3566, 51336], "temperature": 0.0, "avg_logprob": -0.14518138340541295, "compression_ratio": 1.6590909090909092, "no_speech_prob": 0.006180159747600555}, {"id": 394, "seek": 147158, "start": 1491.02, "end": 1492.3, "text": " sicker fan", "tokens": [51336, 4998, 260, 3429, 51400], "temperature": 0.0, "avg_logprob": -0.14518138340541295, "compression_ratio": 1.6590909090909092, "no_speech_prob": 0.006180159747600555}, {"id": 395, "seek": 147158, "start": 1492.3, "end": 1493.74, "text": " Yeah", "tokens": [51400, 865, 51472], "temperature": 0.0, "avg_logprob": -0.14518138340541295, "compression_ratio": 1.6590909090909092, "no_speech_prob": 0.006180159747600555}, {"id": 396, "seek": 147158, "start": 1493.74, "end": 1496.1399999999999, "text": " So then the question we were talking about earlier", "tokens": [51472, 407, 550, 264, 1168, 321, 645, 1417, 466, 3071, 51592], "temperature": 0.0, "avg_logprob": -0.14518138340541295, "compression_ratio": 1.6590909090909092, "no_speech_prob": 0.006180159747600555}, {"id": 397, "seek": 147158, "start": 1497.34, "end": 1500.3, "text": " Uh, where does this go? What happens when these things get", "tokens": [51652, 4019, 11, 689, 775, 341, 352, 30, 708, 2314, 562, 613, 721, 483, 51800], "temperature": 0.0, "avg_logprob": -0.14518138340541295, "compression_ratio": 1.6590909090909092, "no_speech_prob": 0.006180159747600555}, {"id": 398, "seek": 150030, "start": 1500.78, "end": 1503.02, "text": " Bigger and better and more powerful", "tokens": [50388, 5429, 1321, 293, 1101, 293, 544, 4005, 50500], "temperature": 0.0, "avg_logprob": -0.15100959495261865, "compression_ratio": 1.7103174603174602, "no_speech_prob": 0.0013451920822262764}, {"id": 399, "seek": 150030, "start": 1503.8999999999999, "end": 1505.18, "text": " um", "tokens": [50544, 1105, 50608], "temperature": 0.0, "avg_logprob": -0.15100959495261865, "compression_ratio": 1.7103174603174602, "no_speech_prob": 0.0013451920822262764}, {"id": 400, "seek": 150030, "start": 1505.18, "end": 1507.18, "text": " It's an interesting question", "tokens": [50608, 467, 311, 364, 1880, 1168, 50708], "temperature": 0.0, "avg_logprob": -0.15100959495261865, "compression_ratio": 1.7103174603174602, "no_speech_prob": 0.0013451920822262764}, {"id": 401, "seek": 150030, "start": 1507.34, "end": 1509.34, "text": " so", "tokens": [50716, 370, 50816], "temperature": 0.0, "avg_logprob": -0.15100959495261865, "compression_ratio": 1.7103174603174602, "no_speech_prob": 0.0013451920822262764}, {"id": 402, "seek": 150030, "start": 1509.6599999999999, "end": 1511.6599999999999, "text": " I've got a paper here", "tokens": [50832, 286, 600, 658, 257, 3035, 510, 50932], "temperature": 0.0, "avg_logprob": -0.15100959495261865, "compression_ratio": 1.7103174603174602, "no_speech_prob": 0.0013451920822262764}, {"id": 403, "seek": 150030, "start": 1512.86, "end": 1515.4199999999998, "text": " Um scaling laws for neural language models", "tokens": [50992, 3301, 21589, 6064, 337, 18161, 2856, 5245, 51120], "temperature": 0.0, "avg_logprob": -0.15100959495261865, "compression_ratio": 1.7103174603174602, "no_speech_prob": 0.0013451920822262764}, {"id": 404, "seek": 150030, "start": 1515.74, "end": 1519.6599999999999, "text": " So you remember before we were talking about the scaling laws when we were talking about gpt2 in fact", "tokens": [51136, 407, 291, 1604, 949, 321, 645, 1417, 466, 264, 21589, 6064, 562, 321, 645, 1417, 466, 290, 662, 17, 294, 1186, 51332], "temperature": 0.0, "avg_logprob": -0.15100959495261865, "compression_ratio": 1.7103174603174602, "no_speech_prob": 0.0013451920822262764}, {"id": 405, "seek": 150030, "start": 1519.98, "end": 1525.98, "text": " And then later about gpt3 you plot these things on a graph and you see that you get basically a straight line and the line is not", "tokens": [51348, 400, 550, 1780, 466, 290, 662, 18, 291, 7542, 613, 721, 322, 257, 4295, 293, 291, 536, 300, 291, 483, 1936, 257, 2997, 1622, 293, 264, 1622, 307, 406, 51648], "temperature": 0.0, "avg_logprob": -0.15100959495261865, "compression_ratio": 1.7103174603174602, "no_speech_prob": 0.0013451920822262764}, {"id": 406, "seek": 150030, "start": 1526.62, "end": 1529.6599999999999, "text": " leveling off over a range of several orders of magnitude and so", "tokens": [51680, 40617, 766, 670, 257, 3613, 295, 2940, 9470, 295, 15668, 293, 370, 51832], "temperature": 0.0, "avg_logprob": -0.15100959495261865, "compression_ratio": 1.7103174603174602, "no_speech_prob": 0.0013451920822262764}, {"id": 407, "seek": 153030, "start": 1530.3799999999999, "end": 1532.3799999999999, "text": " Why not go bigger the", "tokens": [50368, 1545, 406, 352, 3801, 264, 50468], "temperature": 0.0, "avg_logprob": -0.12174226840337117, "compression_ratio": 1.7545454545454546, "no_speech_prob": 0.001548510859720409}, {"id": 408, "seek": 153030, "start": 1532.54, "end": 1536.54, "text": " graphs here, but you can see it's it's kind of uncannily neat", "tokens": [50476, 24877, 510, 11, 457, 291, 393, 536, 309, 311, 309, 311, 733, 295, 6219, 969, 953, 10654, 50676], "temperature": 0.0, "avg_logprob": -0.12174226840337117, "compression_ratio": 1.7545454545454546, "no_speech_prob": 0.001548510859720409}, {"id": 409, "seek": 153030, "start": 1537.34, "end": 1538.54, "text": " that", "tokens": [50716, 300, 50776], "temperature": 0.0, "avg_logprob": -0.12174226840337117, "compression_ratio": 1.7545454545454546, "no_speech_prob": 0.001548510859720409}, {"id": 410, "seek": 153030, "start": 1538.54, "end": 1540.3, "text": " as we increase", "tokens": [50776, 382, 321, 3488, 50864], "temperature": 0.0, "avg_logprob": -0.12174226840337117, "compression_ratio": 1.7545454545454546, "no_speech_prob": 0.001548510859720409}, {"id": 411, "seek": 153030, "start": 1540.3, "end": 1544.3, "text": " The amount of compute used in training the loss goes down", "tokens": [50864, 440, 2372, 295, 14722, 1143, 294, 3097, 264, 4470, 1709, 760, 51064], "temperature": 0.0, "avg_logprob": -0.12174226840337117, "compression_ratio": 1.7545454545454546, "no_speech_prob": 0.001548510859720409}, {"id": 412, "seek": 153030, "start": 1545.34, "end": 1552.62, "text": " And of course machine learning is like golf lower loss is better similarly as the number of tokens used in training goes up", "tokens": [51116, 400, 295, 1164, 3479, 2539, 307, 411, 12880, 3126, 4470, 307, 1101, 14138, 382, 264, 1230, 295, 22667, 1143, 294, 3097, 1709, 493, 51480], "temperature": 0.0, "avg_logprob": -0.12174226840337117, "compression_ratio": 1.7545454545454546, "no_speech_prob": 0.001548510859720409}, {"id": 413, "seek": 153030, "start": 1553.34, "end": 1558.86, "text": " The loss goes down unlike a very neat straight line as the number of parameters in the model goes up", "tokens": [51516, 440, 4470, 1709, 760, 8343, 257, 588, 10654, 2997, 1622, 382, 264, 1230, 295, 9834, 294, 264, 2316, 1709, 493, 51792], "temperature": 0.0, "avg_logprob": -0.12174226840337117, "compression_ratio": 1.7545454545454546, "no_speech_prob": 0.001548510859720409}, {"id": 414, "seek": 155886, "start": 1559.1799999999998, "end": 1562.1399999999999, "text": " The loss goes down. This is as long as", "tokens": [50380, 440, 4470, 1709, 760, 13, 639, 307, 382, 938, 382, 50528], "temperature": 0.0, "avg_logprob": -0.1230342920544078, "compression_ratio": 1.86784140969163, "no_speech_prob": 0.0010000610491260886}, {"id": 415, "seek": 155886, "start": 1563.5, "end": 1564.4599999999998, "text": " the", "tokens": [50596, 264, 50644], "temperature": 0.0, "avg_logprob": -0.1230342920544078, "compression_ratio": 1.86784140969163, "no_speech_prob": 0.0010000610491260886}, {"id": 416, "seek": 155886, "start": 1564.4599999999998, "end": 1565.8999999999999, "text": " other", "tokens": [50644, 661, 50716], "temperature": 0.0, "avg_logprob": -0.1230342920544078, "compression_ratio": 1.86784140969163, "no_speech_prob": 0.0010000610491260886}, {"id": 417, "seek": 155886, "start": 1565.8999999999999, "end": 1572.78, "text": " Variables are not the bottleneck, right? So if you uh, if you increase the the amount of data you give a model", "tokens": [50716, 32511, 2965, 366, 406, 264, 44641, 547, 11, 558, 30, 407, 498, 291, 2232, 11, 498, 291, 3488, 264, 264, 2372, 295, 1412, 291, 976, 257, 2316, 51060], "temperature": 0.0, "avg_logprob": -0.1230342920544078, "compression_ratio": 1.86784140969163, "no_speech_prob": 0.0010000610491260886}, {"id": 418, "seek": 155886, "start": 1573.74, "end": 1580.2199999999998, "text": " Past a certain point giving more data doesn't help because the model doesn't have enough parameters to make use of that data, right?", "tokens": [51108, 18408, 257, 1629, 935, 2902, 544, 1412, 1177, 380, 854, 570, 264, 2316, 1177, 380, 362, 1547, 9834, 281, 652, 764, 295, 300, 1412, 11, 558, 30, 51432], "temperature": 0.0, "avg_logprob": -0.1230342920544078, "compression_ratio": 1.86784140969163, "no_speech_prob": 0.0010000610491260886}, {"id": 419, "seek": 155886, "start": 1581.02, "end": 1583.02, "text": " Similarly adding more parameters to a model", "tokens": [51472, 13157, 5127, 544, 9834, 281, 257, 2316, 51572], "temperature": 0.0, "avg_logprob": -0.1230342920544078, "compression_ratio": 1.86784140969163, "no_speech_prob": 0.0010000610491260886}, {"id": 420, "seek": 155886, "start": 1583.74, "end": 1588.06, "text": " past a certain point adding parameters doesn't make doesn't make any difference because", "tokens": [51608, 1791, 257, 1629, 935, 5127, 9834, 1177, 380, 652, 1177, 380, 652, 604, 2649, 570, 51824], "temperature": 0.0, "avg_logprob": -0.1230342920544078, "compression_ratio": 1.86784140969163, "no_speech_prob": 0.0010000610491260886}, {"id": 421, "seek": 158886, "start": 1588.9399999999998, "end": 1590.6999999999998, "text": " You don't have enough data, right?", "tokens": [50368, 509, 500, 380, 362, 1547, 1412, 11, 558, 30, 50456], "temperature": 0.0, "avg_logprob": -0.10222686977561461, "compression_ratio": 1.8218623481781377, "no_speech_prob": 0.0006163203506730497}, {"id": 422, "seek": 158886, "start": 1590.6999999999998, "end": 1593.82, "text": " And in the same way compute is like how long do we train it for?", "tokens": [50456, 400, 294, 264, 912, 636, 14722, 307, 411, 577, 938, 360, 321, 3847, 309, 337, 30, 50612], "temperature": 0.0, "avg_logprob": -0.10222686977561461, "compression_ratio": 1.8218623481781377, "no_speech_prob": 0.0006163203506730497}, {"id": 423, "seek": 158886, "start": 1593.82, "end": 1597.34, "text": " Like do we train it all the way to convergence or do we stop early?", "tokens": [50612, 1743, 360, 321, 3847, 309, 439, 264, 636, 281, 32181, 420, 360, 321, 1590, 2440, 30, 50788], "temperature": 0.0, "avg_logprob": -0.10222686977561461, "compression_ratio": 1.8218623481781377, "no_speech_prob": 0.0006163203506730497}, {"id": 424, "seek": 158886, "start": 1599.4199999999998, "end": 1602.3, "text": " There comes a point where you kind of hit diminishing returns where", "tokens": [50892, 821, 1487, 257, 935, 689, 291, 733, 295, 2045, 15739, 3807, 11247, 689, 51036], "temperature": 0.0, "avg_logprob": -0.10222686977561461, "compression_ratio": 1.8218623481781377, "no_speech_prob": 0.0006163203506730497}, {"id": 425, "seek": 158886, "start": 1602.78, "end": 1605.82, "text": " Rather than having a smaller model and training it for longer", "tokens": [51060, 16571, 813, 1419, 257, 4356, 2316, 293, 3097, 309, 337, 2854, 51212], "temperature": 0.0, "avg_logprob": -0.10222686977561461, "compression_ratio": 1.8218623481781377, "no_speech_prob": 0.0006163203506730497}, {"id": 426, "seek": 158886, "start": 1605.9799999999998, "end": 1610.06, "text": " You're better off having a bigger model and actually not training it all the way to convergence", "tokens": [51220, 509, 434, 1101, 766, 1419, 257, 3801, 2316, 293, 767, 406, 3097, 309, 439, 264, 636, 281, 32181, 51424], "temperature": 0.0, "avg_logprob": -0.10222686977561461, "compression_ratio": 1.8218623481781377, "no_speech_prob": 0.0006163203506730497}, {"id": 427, "seek": 158886, "start": 1611.6599999999999, "end": 1615.58, "text": " But in the situations where the other two are sufficient", "tokens": [51504, 583, 294, 264, 6851, 689, 264, 661, 732, 366, 11563, 51700], "temperature": 0.0, "avg_logprob": -0.10222686977561461, "compression_ratio": 1.8218623481781377, "no_speech_prob": 0.0006163203506730497}, {"id": 428, "seek": 161558, "start": 1616.3, "end": 1621.98, "text": " This is the behavior these like very neat straight lines on these log graphs", "tokens": [50400, 639, 307, 264, 5223, 613, 411, 588, 10654, 2997, 3876, 322, 613, 3565, 24877, 50684], "temperature": 0.0, "avg_logprob": -0.12628801302476364, "compression_ratio": 1.70935960591133, "no_speech_prob": 0.0030743712559342384}, {"id": 429, "seek": 161558, "start": 1622.86, "end": 1624.86, "text": " as these things go up", "tokens": [50728, 382, 613, 721, 352, 493, 50828], "temperature": 0.0, "avg_logprob": -0.12628801302476364, "compression_ratio": 1.70935960591133, "no_speech_prob": 0.0030743712559342384}, {"id": 430, "seek": 161558, "start": 1624.9399999999998, "end": 1626.3799999999999, "text": " performance goes up", "tokens": [50832, 3389, 1709, 493, 50904], "temperature": 0.0, "avg_logprob": -0.12628801302476364, "compression_ratio": 1.70935960591133, "no_speech_prob": 0.0030743712559342384}, {"id": 431, "seek": 161558, "start": 1626.3799999999999, "end": 1628.3799999999999, "text": " Right because loss has gone down", "tokens": [50904, 1779, 570, 4470, 575, 2780, 760, 51004], "temperature": 0.0, "avg_logprob": -0.12628801302476364, "compression_ratio": 1.70935960591133, "no_speech_prob": 0.0030743712559342384}, {"id": 432, "seek": 161558, "start": 1628.62, "end": 1631.34, "text": " The bigger models do better, but then the question is", "tokens": [51016, 440, 3801, 5245, 360, 1101, 11, 457, 550, 264, 1168, 307, 51152], "temperature": 0.0, "avg_logprob": -0.12628801302476364, "compression_ratio": 1.70935960591133, "no_speech_prob": 0.0030743712559342384}, {"id": 433, "seek": 161558, "start": 1632.46, "end": 1634.46, "text": " Do better at what exactly?", "tokens": [51208, 1144, 1101, 412, 437, 2293, 30, 51308], "temperature": 0.0, "avg_logprob": -0.12628801302476364, "compression_ratio": 1.70935960591133, "no_speech_prob": 0.0030743712559342384}, {"id": 434, "seek": 161558, "start": 1635.58, "end": 1639.34, "text": " Yeah, what's the measure they do better at getting low loss?", "tokens": [51364, 865, 11, 437, 311, 264, 3481, 436, 360, 1101, 412, 1242, 2295, 4470, 30, 51552], "temperature": 0.0, "avg_logprob": -0.12628801302476364, "compression_ratio": 1.70935960591133, "no_speech_prob": 0.0030743712559342384}, {"id": 435, "seek": 161558, "start": 1641.02, "end": 1643.98, "text": " Or they do better at getting reward they do better at", "tokens": [51636, 1610, 436, 360, 1101, 412, 1242, 7782, 436, 360, 1101, 412, 51784], "temperature": 0.0, "avg_logprob": -0.12628801302476364, "compression_ratio": 1.70935960591133, "no_speech_prob": 0.0030743712559342384}, {"id": 436, "seek": 164398, "start": 1644.54, "end": 1646.54, "text": " Getting the approval", "tokens": [50392, 13674, 264, 13317, 50492], "temperature": 0.0, "avg_logprob": -0.1469466159218236, "compression_ratio": 1.6865671641791045, "no_speech_prob": 0.0019257995299994946}, {"id": 437, "seek": 164398, "start": 1647.26, "end": 1649.26, "text": " of human feedback, right?", "tokens": [50528, 295, 1952, 5824, 11, 558, 30, 50628], "temperature": 0.0, "avg_logprob": -0.1469466159218236, "compression_ratio": 1.6865671641791045, "no_speech_prob": 0.0019257995299994946}, {"id": 438, "seek": 164398, "start": 1649.9, "end": 1653.66, "text": " and anytime and you'll notice that none of those is like", "tokens": [50660, 293, 13038, 293, 291, 603, 3449, 300, 6022, 295, 729, 307, 411, 50848], "temperature": 0.0, "avg_logprob": -0.1469466159218236, "compression_ratio": 1.6865671641791045, "no_speech_prob": 0.0019257995299994946}, {"id": 439, "seek": 164398, "start": 1655.34, "end": 1657.34, "text": " The actual thing that we actually want", "tokens": [50932, 440, 3539, 551, 300, 321, 767, 528, 51032], "temperature": 0.0, "avg_logprob": -0.1469466159218236, "compression_ratio": 1.6865671641791045, "no_speech_prob": 0.0019257995299994946}, {"id": 440, "seek": 164398, "start": 1658.7, "end": 1660.7, "text": " Right, it's like very rare", "tokens": [51100, 1779, 11, 309, 311, 411, 588, 5892, 51200], "temperature": 0.0, "avg_logprob": -0.1469466159218236, "compression_ratio": 1.6865671641791045, "no_speech_prob": 0.0019257995299994946}, {"id": 441, "seek": 164398, "start": 1661.58, "end": 1662.94, "text": " um", "tokens": [51244, 1105, 51312], "temperature": 0.0, "avg_logprob": -0.1469466159218236, "compression_ratio": 1.6865671641791045, "no_speech_prob": 0.0019257995299994946}, {"id": 442, "seek": 164398, "start": 1662.94, "end": 1665.9, "text": " Sometimes it is right if you're if you're if you're writing something to play go", "tokens": [51312, 4803, 309, 307, 558, 498, 291, 434, 498, 291, 434, 498, 291, 434, 3579, 746, 281, 862, 352, 51460], "temperature": 0.0, "avg_logprob": -0.1469466159218236, "compression_ratio": 1.6865671641791045, "no_speech_prob": 0.0019257995299994946}, {"id": 443, "seek": 164398, "start": 1666.8600000000001, "end": 1668.14, "text": " then like", "tokens": [51508, 550, 411, 51572], "temperature": 0.0, "avg_logprob": -0.1469466159218236, "compression_ratio": 1.6865671641791045, "no_speech_prob": 0.0019257995299994946}, {"id": 444, "seek": 164398, "start": 1668.14, "end": 1670.8600000000001, "text": " Does it win it go is actually just the thing that you want?", "tokens": [51572, 4402, 309, 1942, 309, 352, 307, 767, 445, 264, 551, 300, 291, 528, 30, 51708], "temperature": 0.0, "avg_logprob": -0.1469466159218236, "compression_ratio": 1.6865671641791045, "no_speech_prob": 0.0019257995299994946}, {"id": 445, "seek": 164398, "start": 1671.82, "end": 1673.82, "text": " and so you know", "tokens": [51756, 293, 370, 291, 458, 51856], "temperature": 0.0, "avg_logprob": -0.1469466159218236, "compression_ratio": 1.6865671641791045, "no_speech_prob": 0.0019257995299994946}, {"id": 446, "seek": 167398, "start": 1674.94, "end": 1676.94, "text": " Lower loss just is better or like lower", "tokens": [50412, 25523, 4470, 445, 307, 1101, 420, 411, 3126, 50512], "temperature": 0.0, "avg_logprob": -0.18513199523255064, "compression_ratio": 1.701834862385321, "no_speech_prob": 0.0005355319590307772}, {"id": 447, "seek": 167398, "start": 1678.14, "end": 1683.66, "text": " Like higher reward or whatever your objective is just is straightforwardly better because you actually specified the thing you actually want", "tokens": [50572, 1743, 2946, 7782, 420, 2035, 428, 10024, 307, 445, 307, 15325, 356, 1101, 570, 291, 767, 22206, 264, 551, 291, 767, 528, 50848], "temperature": 0.0, "avg_logprob": -0.18513199523255064, "compression_ratio": 1.701834862385321, "no_speech_prob": 0.0005355319590307772}, {"id": 448, "seek": 167398, "start": 1684.94, "end": 1686.94, "text": " Most of the time though", "tokens": [50912, 4534, 295, 264, 565, 1673, 51012], "temperature": 0.0, "avg_logprob": -0.18513199523255064, "compression_ratio": 1.701834862385321, "no_speech_prob": 0.0005355319590307772}, {"id": 449, "seek": 167398, "start": 1687.18, "end": 1689.42, "text": " What we're looking at is a proxy", "tokens": [51024, 708, 321, 434, 1237, 412, 307, 257, 29690, 51136], "temperature": 0.0, "avg_logprob": -0.18513199523255064, "compression_ratio": 1.701834862385321, "no_speech_prob": 0.0005355319590307772}, {"id": 450, "seek": 167398, "start": 1690.94, "end": 1692.14, "text": " um", "tokens": [51212, 1105, 51272], "temperature": 0.0, "avg_logprob": -0.18513199523255064, "compression_ratio": 1.701834862385321, "no_speech_prob": 0.0005355319590307772}, {"id": 451, "seek": 167398, "start": 1692.14, "end": 1696.06, "text": " And so then you have good heart's law you get situations where", "tokens": [51272, 400, 370, 550, 291, 362, 665, 1917, 311, 2101, 291, 483, 6851, 689, 51468], "temperature": 0.0, "avg_logprob": -0.18513199523255064, "compression_ratio": 1.701834862385321, "no_speech_prob": 0.0005355319590307772}, {"id": 452, "seek": 167398, "start": 1696.8600000000001, "end": 1697.58, "text": " uh", "tokens": [51508, 2232, 51544], "temperature": 0.0, "avg_logprob": -0.18513199523255064, "compression_ratio": 1.701834862385321, "no_speech_prob": 0.0005355319590307772}, {"id": 453, "seek": 167398, "start": 1697.58, "end": 1699.1, "text": " getting better", "tokens": [51544, 1242, 1101, 51620], "temperature": 0.0, "avg_logprob": -0.18513199523255064, "compression_ratio": 1.701834862385321, "no_speech_prob": 0.0005355319590307772}, {"id": 454, "seek": 167398, "start": 1699.1, "end": 1701.1, "text": " at doing well", "tokens": [51620, 412, 884, 731, 51720], "temperature": 0.0, "avg_logprob": -0.18513199523255064, "compression_ratio": 1.701834862385321, "no_speech_prob": 0.0005355319590307772}, {"id": 455, "seek": 167398, "start": 1701.18, "end": 1703.18, "text": " Doing better according to the proxy", "tokens": [51724, 18496, 1101, 4650, 281, 264, 29690, 51824], "temperature": 0.0, "avg_logprob": -0.18513199523255064, "compression_ratio": 1.701834862385321, "no_speech_prob": 0.0005355319590307772}, {"id": 456, "seek": 170318, "start": 1703.3400000000001, "end": 1704.94, "text": " stops being", "tokens": [50372, 10094, 885, 50452], "temperature": 0.0, "avg_logprob": -0.09921300888061524, "compression_ratio": 1.7426160337552743, "no_speech_prob": 0.0008294419967569411}, {"id": 457, "seek": 170318, "start": 1704.94, "end": 1710.94, "text": " The same as doing better according to your actual objective. There's a great graph about this in a recent paper. You can see very neatly", "tokens": [50452, 440, 912, 382, 884, 1101, 4650, 281, 428, 3539, 10024, 13, 821, 311, 257, 869, 4295, 466, 341, 294, 257, 5162, 3035, 13, 509, 393, 536, 588, 36634, 50752], "temperature": 0.0, "avg_logprob": -0.09921300888061524, "compression_ratio": 1.7426160337552743, "no_speech_prob": 0.0008294419967569411}, {"id": 458, "seek": 170318, "start": 1712.3, "end": 1714.3, "text": " As the number of iterations", "tokens": [50820, 1018, 264, 1230, 295, 36540, 50920], "temperature": 0.0, "avg_logprob": -0.09921300888061524, "compression_ratio": 1.7426160337552743, "no_speech_prob": 0.0008294419967569411}, {"id": 459, "seek": 170318, "start": 1714.3, "end": 1715.5, "text": " goes up", "tokens": [50920, 1709, 493, 50980], "temperature": 0.0, "avg_logprob": -0.09921300888061524, "compression_ratio": 1.7426160337552743, "no_speech_prob": 0.0008294419967569411}, {"id": 460, "seek": 170318, "start": 1715.5, "end": 1721.74, "text": " The reward according to the proxy utility goes up very cleanly because this is the thing that the model is actually being trained on", "tokens": [50980, 440, 7782, 4650, 281, 264, 29690, 14877, 1709, 493, 588, 2541, 356, 570, 341, 307, 264, 551, 300, 264, 2316, 307, 767, 885, 8895, 322, 51292], "temperature": 0.0, "avg_logprob": -0.09921300888061524, "compression_ratio": 1.7426160337552743, "no_speech_prob": 0.0008294419967569411}, {"id": 461, "seek": 170318, "start": 1721.98, "end": 1723.98, "text": " but the true utility", "tokens": [51304, 457, 264, 2074, 14877, 51404], "temperature": 0.0, "avg_logprob": -0.09921300888061524, "compression_ratio": 1.7426160337552743, "no_speech_prob": 0.0008294419967569411}, {"id": 462, "seek": 170318, "start": 1724.3, "end": 1726.3, "text": " goes up at first", "tokens": [51420, 1709, 493, 412, 700, 51520], "temperature": 0.0, "avg_logprob": -0.09921300888061524, "compression_ratio": 1.7426160337552743, "no_speech_prob": 0.0008294419967569411}, {"id": 463, "seek": 170318, "start": 1726.8600000000001, "end": 1728.8600000000001, "text": " Then hits diminishing returns", "tokens": [51548, 1396, 8664, 15739, 3807, 11247, 51648], "temperature": 0.0, "avg_logprob": -0.09921300888061524, "compression_ratio": 1.7426160337552743, "no_speech_prob": 0.0008294419967569411}, {"id": 464, "seek": 170318, "start": 1729.02, "end": 1731.02, "text": " and then actually goes down", "tokens": [51656, 293, 550, 767, 1709, 760, 51756], "temperature": 0.0, "avg_logprob": -0.09921300888061524, "compression_ratio": 1.7426160337552743, "no_speech_prob": 0.0008294419967569411}, {"id": 465, "seek": 173102, "start": 1731.26, "end": 1736.1399999999999, "text": " And eventually goes down below zero like if you optimize hard enough", "tokens": [50376, 400, 4728, 1709, 760, 2507, 4018, 411, 498, 291, 19719, 1152, 1547, 50620], "temperature": 0.0, "avg_logprob": -0.12159850380637428, "compression_ratio": 1.6227272727272728, "no_speech_prob": 0.0010481714271008968}, {"id": 466, "seek": 173102, "start": 1737.26, "end": 1739.26, "text": " For a proxy of the thing you want", "tokens": [50676, 1171, 257, 29690, 295, 264, 551, 291, 528, 50776], "temperature": 0.0, "avg_logprob": -0.12159850380637428, "compression_ratio": 1.6227272727272728, "no_speech_prob": 0.0010481714271008968}, {"id": 467, "seek": 173102, "start": 1739.9, "end": 1743.18, "text": " You can end up with something that's in a sense worse than nothing", "tokens": [50808, 509, 393, 917, 493, 365, 746, 300, 311, 294, 257, 2020, 5324, 813, 1825, 50972], "temperature": 0.0, "avg_logprob": -0.12159850380637428, "compression_ratio": 1.6227272727272728, "no_speech_prob": 0.0010481714271008968}, {"id": 468, "seek": 173102, "start": 1743.74, "end": 1745.74, "text": " That's actively bad", "tokens": [51000, 663, 311, 13022, 1578, 51100], "temperature": 0.0, "avg_logprob": -0.12159850380637428, "compression_ratio": 1.6227272727272728, "no_speech_prob": 0.0010481714271008968}, {"id": 469, "seek": 173102, "start": 1745.74, "end": 1747.74, "text": " according to your", "tokens": [51100, 4650, 281, 428, 51200], "temperature": 0.0, "avg_logprob": -0.12159850380637428, "compression_ratio": 1.6227272727272728, "no_speech_prob": 0.0010481714271008968}, {"id": 470, "seek": 173102, "start": 1747.82, "end": 1749.5, "text": " Your true utility", "tokens": [51204, 2260, 2074, 14877, 51288], "temperature": 0.0, "avg_logprob": -0.12159850380637428, "compression_ratio": 1.6227272727272728, "no_speech_prob": 0.0010481714271008968}, {"id": 471, "seek": 173102, "start": 1749.5, "end": 1754.46, "text": " So what you can end up with is uh things that are called inverse scaling", "tokens": [51288, 407, 437, 291, 393, 917, 493, 365, 307, 2232, 721, 300, 366, 1219, 17340, 21589, 51536], "temperature": 0.0, "avg_logprob": -0.12159850380637428, "compression_ratio": 1.6227272727272728, "no_speech_prob": 0.0010481714271008968}, {"id": 472, "seek": 173102, "start": 1755.82, "end": 1758.7, "text": " So the others before we had right scaling bigger is better", "tokens": [51604, 407, 264, 2357, 949, 321, 632, 558, 21589, 3801, 307, 1101, 51748], "temperature": 0.0, "avg_logprob": -0.12159850380637428, "compression_ratio": 1.6227272727272728, "no_speech_prob": 0.0010481714271008968}, {"id": 473, "seek": 175870, "start": 1759.5, "end": 1761.5, "text": " But now it's like if you have uh", "tokens": [50404, 583, 586, 309, 311, 411, 498, 291, 362, 2232, 50504], "temperature": 0.0, "avg_logprob": -0.15503669203373424, "compression_ratio": 1.657258064516129, "no_speech_prob": 0.0017817701445892453}, {"id": 474, "seek": 175870, "start": 1762.06, "end": 1765.02, "text": " If the thing you're actually trying to do is different from", "tokens": [50532, 759, 264, 551, 291, 434, 767, 1382, 281, 360, 307, 819, 490, 50680], "temperature": 0.0, "avg_logprob": -0.15503669203373424, "compression_ratio": 1.657258064516129, "no_speech_prob": 0.0017817701445892453}, {"id": 475, "seek": 175870, "start": 1765.9, "end": 1767.9, "text": " The loss function or the objective function", "tokens": [50724, 440, 4470, 2445, 420, 264, 10024, 2445, 50824], "temperature": 0.0, "avg_logprob": -0.15503669203373424, "compression_ratio": 1.657258064516129, "no_speech_prob": 0.0017817701445892453}, {"id": 476, "seek": 175870, "start": 1768.06, "end": 1773.3400000000001, "text": " You get this inverse scaling effect where it gets better and then it gets worse. There was also a great example from", "tokens": [50832, 509, 483, 341, 17340, 21589, 1802, 689, 309, 2170, 1101, 293, 550, 309, 2170, 5324, 13, 821, 390, 611, 257, 869, 1365, 490, 51096], "temperature": 0.0, "avg_logprob": -0.15503669203373424, "compression_ratio": 1.657258064516129, "no_speech_prob": 0.0017817701445892453}, {"id": 477, "seek": 175870, "start": 1774.06, "end": 1775.5800000000002, "text": " uh github", "tokens": [51132, 2232, 290, 355, 836, 51208], "temperature": 0.0, "avg_logprob": -0.15503669203373424, "compression_ratio": 1.657258064516129, "no_speech_prob": 0.0017817701445892453}, {"id": 478, "seek": 175870, "start": 1775.5800000000002, "end": 1778.94, "text": " co-pilot or codex. I think the model um", "tokens": [51208, 598, 12, 79, 31516, 420, 3089, 87, 13, 286, 519, 264, 2316, 1105, 51376], "temperature": 0.0, "avg_logprob": -0.15503669203373424, "compression_ratio": 1.657258064516129, "no_speech_prob": 0.0017817701445892453}, {"id": 479, "seek": 175870, "start": 1779.82, "end": 1780.8600000000001, "text": " That", "tokens": [51420, 663, 51472], "temperature": 0.0, "avg_logprob": -0.15503669203373424, "compression_ratio": 1.657258064516129, "no_speech_prob": 0.0017817701445892453}, {"id": 480, "seek": 175870, "start": 1780.8600000000001, "end": 1785.26, "text": " Co-pilot uses so this is a code generation model. Suppose the code you've given it", "tokens": [51472, 3066, 12, 79, 31516, 4960, 370, 341, 307, 257, 3089, 5125, 2316, 13, 21360, 264, 3089, 291, 600, 2212, 309, 51692], "temperature": 0.0, "avg_logprob": -0.15503669203373424, "compression_ratio": 1.657258064516129, "no_speech_prob": 0.0017817701445892453}, {"id": 481, "seek": 175870, "start": 1786.06, "end": 1788.06, "text": " has some bugs in it", "tokens": [51732, 575, 512, 15120, 294, 309, 51832], "temperature": 0.0, "avg_logprob": -0.15503669203373424, "compression_ratio": 1.657258064516129, "no_speech_prob": 0.0017817701445892453}, {"id": 482, "seek": 178806, "start": 1788.46, "end": 1791.34, "text": " Maybe you've made a mistake somewhere and you've introduced", "tokens": [50384, 2704, 291, 600, 1027, 257, 6146, 4079, 293, 291, 600, 7268, 50528], "temperature": 0.0, "avg_logprob": -0.16675460581876794, "compression_ratio": 1.5601851851851851, "no_speech_prob": 0.0003301230608485639}, {"id": 483, "seek": 178806, "start": 1792.06, "end": 1795.26, "text": " security vulnerability in your code. Let's say", "tokens": [50564, 3825, 24210, 294, 428, 3089, 13, 961, 311, 584, 50724], "temperature": 0.0, "avg_logprob": -0.16675460581876794, "compression_ratio": 1.5601851851851851, "no_speech_prob": 0.0003301230608485639}, {"id": 484, "seek": 178806, "start": 1796.54, "end": 1798.54, "text": " A sort of medium-sized model", "tokens": [50788, 316, 1333, 295, 6399, 12, 20614, 2316, 50888], "temperature": 0.0, "avg_logprob": -0.16675460581876794, "compression_ratio": 1.5601851851851851, "no_speech_prob": 0.0003301230608485639}, {"id": 485, "seek": 178806, "start": 1798.86, "end": 1802.46, "text": " Will figure out what you're trying to do in your code and give you a decent completion", "tokens": [50904, 3099, 2573, 484, 437, 291, 434, 1382, 281, 360, 294, 428, 3089, 293, 976, 291, 257, 8681, 19372, 51084], "temperature": 0.0, "avg_logprob": -0.16675460581876794, "compression_ratio": 1.5601851851851851, "no_speech_prob": 0.0003301230608485639}, {"id": 486, "seek": 178806, "start": 1803.6599999999999, "end": 1805.6599999999999, "text": " But a bigger model", "tokens": [51144, 583, 257, 3801, 2316, 51244], "temperature": 0.0, "avg_logprob": -0.16675460581876794, "compression_ratio": 1.5601851851851851, "no_speech_prob": 0.0003301230608485639}, {"id": 487, "seek": 178806, "start": 1805.74, "end": 1807.5, "text": " will spot", "tokens": [51248, 486, 4008, 51336], "temperature": 0.0, "avg_logprob": -0.16675460581876794, "compression_ratio": 1.5601851851851851, "no_speech_prob": 0.0003301230608485639}, {"id": 488, "seek": 178806, "start": 1807.5, "end": 1809.1, "text": " your bug", "tokens": [51336, 428, 7426, 51416], "temperature": 0.0, "avg_logprob": -0.16675460581876794, "compression_ratio": 1.5601851851851851, "no_speech_prob": 0.0003301230608485639}, {"id": 489, "seek": 178806, "start": 1809.1, "end": 1810.46, "text": " And say, ah", "tokens": [51416, 400, 584, 11, 3716, 51484], "temperature": 0.0, "avg_logprob": -0.16675460581876794, "compression_ratio": 1.5601851851851851, "no_speech_prob": 0.0003301230608485639}, {"id": 490, "seek": 178806, "start": 1810.46, "end": 1812.46, "text": " Generating buggy code. Are we okay?", "tokens": [51484, 15409, 990, 7426, 1480, 3089, 13, 2014, 321, 1392, 30, 51584], "temperature": 0.0, "avg_logprob": -0.16675460581876794, "compression_ratio": 1.5601851851851851, "no_speech_prob": 0.0003301230608485639}, {"id": 491, "seek": 178806, "start": 1813.26, "end": 1815.26, "text": " I can do that. I can do that", "tokens": [51624, 286, 393, 360, 300, 13, 286, 393, 360, 300, 51724], "temperature": 0.0, "avg_logprob": -0.16675460581876794, "compression_ratio": 1.5601851851851851, "no_speech_prob": 0.0003301230608485639}, {"id": 492, "seek": 181526, "start": 1815.26, "end": 1819.18, "text": " And introduce like deliberately introduce its own", "tokens": [50364, 400, 5366, 411, 23506, 5366, 1080, 1065, 50560], "temperature": 0.0, "avg_logprob": -0.1440773860062703, "compression_ratio": 1.7829787234042553, "no_speech_prob": 0.0008425068226642907}, {"id": 493, "seek": 181526, "start": 1819.98, "end": 1821.98, "text": " new security vulnerabilities", "tokens": [50600, 777, 3825, 37633, 50700], "temperature": 0.0, "avg_logprob": -0.1440773860062703, "compression_ratio": 1.7829787234042553, "no_speech_prob": 0.0008425068226642907}, {"id": 494, "seek": 181526, "start": 1822.3, "end": 1823.34, "text": " because", "tokens": [50716, 570, 50768], "temperature": 0.0, "avg_logprob": -0.1440773860062703, "compression_ratio": 1.7829787234042553, "no_speech_prob": 0.0008425068226642907}, {"id": 495, "seek": 181526, "start": 1823.34, "end": 1824.3, "text": " it's", "tokens": [50768, 309, 311, 50816], "temperature": 0.0, "avg_logprob": -0.1440773860062703, "compression_ratio": 1.7829787234042553, "no_speech_prob": 0.0008425068226642907}, {"id": 496, "seek": 181526, "start": 1824.3, "end": 1830.14, "text": " Trying to you know predict what comes next. It's trying to generate code that fits in with the surrounding code", "tokens": [50816, 20180, 281, 291, 458, 6069, 437, 1487, 958, 13, 467, 311, 1382, 281, 8460, 3089, 300, 9001, 294, 365, 264, 11498, 3089, 51108], "temperature": 0.0, "avg_logprob": -0.1440773860062703, "compression_ratio": 1.7829787234042553, "no_speech_prob": 0.0008425068226642907}, {"id": 497, "seek": 181526, "start": 1831.02, "end": 1834.3799999999999, "text": " And so a larger model writes worse code than a smaller model", "tokens": [51152, 400, 370, 257, 4833, 2316, 13657, 5324, 3089, 813, 257, 4356, 2316, 51320], "temperature": 0.0, "avg_logprob": -0.1440773860062703, "compression_ratio": 1.7829787234042553, "no_speech_prob": 0.0008425068226642907}, {"id": 498, "seek": 181526, "start": 1835.18, "end": 1837.18, "text": " Because it's gotten better at predicting", "tokens": [51360, 1436, 309, 311, 5768, 1101, 412, 32884, 51460], "temperature": 0.0, "avg_logprob": -0.1440773860062703, "compression_ratio": 1.7829787234042553, "no_speech_prob": 0.0008425068226642907}, {"id": 499, "seek": 181526, "start": 1837.98, "end": 1839.26, "text": " Uh", "tokens": [51500, 4019, 51564], "temperature": 0.0, "avg_logprob": -0.1440773860062703, "compression_ratio": 1.7829787234042553, "no_speech_prob": 0.0008425068226642907}, {"id": 500, "seek": 181526, "start": 1839.26, "end": 1841.02, "text": " What what it should put there?", "tokens": [51564, 708, 437, 309, 820, 829, 456, 30, 51652], "temperature": 0.0, "avg_logprob": -0.1440773860062703, "compression_ratio": 1.7829787234042553, "no_speech_prob": 0.0008425068226642907}, {"id": 501, "seek": 181526, "start": 1841.02, "end": 1843.98, "text": " It wasn't trained to write good code. It was trained to predict what comes next", "tokens": [51652, 467, 2067, 380, 8895, 281, 2464, 665, 3089, 13, 467, 390, 8895, 281, 6069, 437, 1487, 958, 51800], "temperature": 0.0, "avg_logprob": -0.1440773860062703, "compression_ratio": 1.7829787234042553, "no_speech_prob": 0.0008425068226642907}, {"id": 502, "seek": 184398, "start": 1844.38, "end": 1846.38, "text": " So there's this really great paper", "tokens": [50384, 407, 456, 311, 341, 534, 869, 3035, 50484], "temperature": 0.0, "avg_logprob": -0.143089283718152, "compression_ratio": 1.6418604651162791, "no_speech_prob": 0.0006769519532099366}, {"id": 503, "seek": 184398, "start": 1846.38, "end": 1855.5, "text": " Uh, which is asking this question of like, okay, suppose we have a large language model that is trained on human feedback with our lhf", "tokens": [50484, 4019, 11, 597, 307, 3365, 341, 1168, 295, 411, 11, 1392, 11, 7297, 321, 362, 257, 2416, 2856, 2316, 300, 307, 8895, 322, 1952, 5824, 365, 527, 287, 71, 69, 50940], "temperature": 0.0, "avg_logprob": -0.143089283718152, "compression_ratio": 1.6418604651162791, "no_speech_prob": 0.0006769519532099366}, {"id": 504, "seek": 184398, "start": 1856.8600000000001, "end": 1859.66, "text": " What do our scaling curves look like?", "tokens": [51008, 708, 360, 527, 21589, 19490, 574, 411, 30, 51148], "temperature": 0.0, "avg_logprob": -0.143089283718152, "compression_ratio": 1.6418604651162791, "no_speech_prob": 0.0006769519532099366}, {"id": 505, "seek": 184398, "start": 1860.6200000000001, "end": 1862.6200000000001, "text": " what happens like", "tokens": [51196, 437, 2314, 411, 51296], "temperature": 0.0, "avg_logprob": -0.143089283718152, "compression_ratio": 1.6418604651162791, "no_speech_prob": 0.0006769519532099366}, {"id": 506, "seek": 184398, "start": 1863.26, "end": 1869.18, "text": " What happens to the behavior of these models as they get bigger as they're trained for longer", "tokens": [51328, 708, 2314, 281, 264, 5223, 295, 613, 5245, 382, 436, 483, 3801, 382, 436, 434, 8895, 337, 2854, 51624], "temperature": 0.0, "avg_logprob": -0.143089283718152, "compression_ratio": 1.6418604651162791, "no_speech_prob": 0.0006769519532099366}, {"id": 507, "seek": 184398, "start": 1869.9, "end": 1872.3, "text": " as they're given more of this, uh", "tokens": [51660, 382, 436, 434, 2212, 544, 295, 341, 11, 2232, 51780], "temperature": 0.0, "avg_logprob": -0.143089283718152, "compression_ratio": 1.6418604651162791, "no_speech_prob": 0.0006769519532099366}, {"id": 508, "seek": 187230, "start": 1873.1, "end": 1875.1, "text": " human feedback type training", "tokens": [50404, 1952, 5824, 2010, 3097, 50504], "temperature": 0.0, "avg_logprob": -0.1433479619580646, "compression_ratio": 1.6597510373443984, "no_speech_prob": 0.001477277954109013}, {"id": 509, "seek": 187230, "start": 1875.58, "end": 1881.02, "text": " And they've made some great graphs the paper is called discovering language model behaviors with model written evaluations", "tokens": [50528, 400, 436, 600, 1027, 512, 869, 24877, 264, 3035, 307, 1219, 24773, 2856, 2316, 15501, 365, 2316, 3720, 43085, 50800], "temperature": 0.0, "avg_logprob": -0.1433479619580646, "compression_ratio": 1.6597510373443984, "no_speech_prob": 0.001477277954109013}, {"id": 510, "seek": 187230, "start": 1881.98, "end": 1884.3, "text": " And basically they like used language models", "tokens": [50848, 400, 1936, 436, 411, 1143, 2856, 5245, 50964], "temperature": 0.0, "avg_logprob": -0.1433479619580646, "compression_ratio": 1.6597510373443984, "no_speech_prob": 0.001477277954109013}, {"id": 511, "seek": 187230, "start": 1884.94, "end": 1888.06, "text": " to generate enough examples of", "tokens": [50996, 281, 8460, 1547, 5110, 295, 51152], "temperature": 0.0, "avg_logprob": -0.1433479619580646, "compression_ratio": 1.6597510373443984, "no_speech_prob": 0.001477277954109013}, {"id": 512, "seek": 187230, "start": 1888.62, "end": 1890.62, "text": " various different types of questions", "tokens": [51180, 3683, 819, 3467, 295, 1651, 51280], "temperature": 0.0, "avg_logprob": -0.1433479619580646, "compression_ratio": 1.6597510373443984, "no_speech_prob": 0.001477277954109013}, {"id": 513, "seek": 187230, "start": 1890.7, "end": 1894.62, "text": " That they could ask models so that they can like we're at a point now", "tokens": [51284, 663, 436, 727, 1029, 5245, 370, 300, 436, 393, 411, 321, 434, 412, 257, 935, 586, 51480], "temperature": 0.0, "avg_logprob": -0.1433479619580646, "compression_ratio": 1.6597510373443984, "no_speech_prob": 0.001477277954109013}, {"id": 514, "seek": 187230, "start": 1895.1, "end": 1899.1, "text": " Where you can map a language model on a political compass, right?", "tokens": [51504, 2305, 291, 393, 4471, 257, 2856, 2316, 322, 257, 3905, 10707, 11, 558, 30, 51704], "temperature": 0.0, "avg_logprob": -0.1433479619580646, "compression_ratio": 1.6597510373443984, "no_speech_prob": 0.001477277954109013}, {"id": 515, "seek": 189910, "start": 1899.1, "end": 1905.02, "text": " You can ask its opinions about all kinds of different things and then you can plot how those opinions change", "tokens": [50364, 509, 393, 1029, 1080, 11819, 466, 439, 3685, 295, 819, 721, 293, 550, 291, 393, 7542, 577, 729, 11819, 1319, 50660], "temperature": 0.0, "avg_logprob": -0.14330871016890914, "compression_ratio": 1.6492890995260663, "no_speech_prob": 0.0027961244340986013}, {"id": 516, "seek": 189910, "start": 1906.3, "end": 1908.78, "text": " Uh as the model gets bigger and as it gets trained more", "tokens": [50724, 4019, 382, 264, 2316, 2170, 3801, 293, 382, 309, 2170, 8895, 544, 50848], "temperature": 0.0, "avg_logprob": -0.14330871016890914, "compression_ratio": 1.6492890995260663, "no_speech_prob": 0.0027961244340986013}, {"id": 517, "seek": 189910, "start": 1909.5, "end": 1911.1799999999998, "text": " what they find", "tokens": [50884, 437, 436, 915, 50968], "temperature": 0.0, "avg_logprob": -0.14330871016890914, "compression_ratio": 1.6492890995260663, "no_speech_prob": 0.0027961244340986013}, {"id": 518, "seek": 189910, "start": 1911.1799999999998, "end": 1915.26, "text": " Is they become more liberal politically more liberal", "tokens": [50968, 1119, 436, 1813, 544, 13767, 21154, 544, 13767, 51172], "temperature": 0.0, "avg_logprob": -0.14330871016890914, "compression_ratio": 1.6492890995260663, "no_speech_prob": 0.0027961244340986013}, {"id": 519, "seek": 189910, "start": 1916.06, "end": 1918.06, "text": " they also become", "tokens": [51212, 436, 611, 1813, 51312], "temperature": 0.0, "avg_logprob": -0.14330871016890914, "compression_ratio": 1.6492890995260663, "no_speech_prob": 0.0027961244340986013}, {"id": 520, "seek": 189910, "start": 1918.06, "end": 1921.6599999999999, "text": " More conservative. Yeah measured in different ways guessing, right?", "tokens": [51312, 5048, 13780, 13, 865, 12690, 294, 819, 2098, 17939, 11, 558, 30, 51492], "temperature": 0.0, "avg_logprob": -0.14330871016890914, "compression_ratio": 1.6492890995260663, "no_speech_prob": 0.0027961244340986013}, {"id": 521, "seek": 189910, "start": 1922.3799999999999, "end": 1924.3799999999999, "text": " and part of what that might be", "tokens": [51528, 293, 644, 295, 437, 300, 1062, 312, 51628], "temperature": 0.0, "avg_logprob": -0.14330871016890914, "compression_ratio": 1.6492890995260663, "no_speech_prob": 0.0027961244340986013}, {"id": 522, "seek": 192438, "start": 1924.94, "end": 1928.7800000000002, "text": " Is in the same way that the model becomes", "tokens": [50392, 1119, 294, 264, 912, 636, 300, 264, 2316, 3643, 50584], "temperature": 0.0, "avg_logprob": -0.21398581710516237, "compression_ratio": 1.7004608294930876, "no_speech_prob": 0.00802105013281107}, {"id": 523, "seek": 192438, "start": 1929.5800000000002, "end": 1932.38, "text": " better at writing good code and better at writing bad code", "tokens": [50624, 1101, 412, 3579, 665, 3089, 293, 1101, 412, 3579, 1578, 3089, 50764], "temperature": 0.0, "avg_logprob": -0.21398581710516237, "compression_ratio": 1.7004608294930876, "no_speech_prob": 0.00802105013281107}, {"id": 524, "seek": 192438, "start": 1933.0200000000002, "end": 1939.42, "text": " I feel like in the past I've I've made a connection to gpt and being a politician, haven't I?", "tokens": [50796, 286, 841, 411, 294, 264, 1791, 286, 600, 286, 600, 1027, 257, 4984, 281, 290, 662, 293, 885, 257, 26453, 11, 2378, 380, 286, 30, 51116], "temperature": 0.0, "avg_logprob": -0.21398581710516237, "compression_ratio": 1.7004608294930876, "no_speech_prob": 0.00802105013281107}, {"id": 525, "seek": 192438, "start": 1939.9, "end": 1941.2600000000002, "text": " Do you remember?", "tokens": [51140, 1144, 291, 1604, 30, 51208], "temperature": 0.0, "avg_logprob": -0.21398581710516237, "compression_ratio": 1.7004608294930876, "no_speech_prob": 0.00802105013281107}, {"id": 526, "seek": 192438, "start": 1941.2600000000002, "end": 1946.7, "text": " It's like a politician. It tells you what you want to hear. There's what feels like we're there again. Exactly", "tokens": [51208, 467, 311, 411, 257, 26453, 13, 467, 5112, 291, 437, 291, 528, 281, 1568, 13, 821, 311, 437, 3417, 411, 321, 434, 456, 797, 13, 7587, 51480], "temperature": 0.0, "avg_logprob": -0.21398581710516237, "compression_ratio": 1.7004608294930876, "no_speech_prob": 0.00802105013281107}, {"id": 527, "seek": 192438, "start": 1947.42, "end": 1949.74, "text": " uh, and so this is like this is potentially", "tokens": [51516, 2232, 11, 293, 370, 341, 307, 411, 341, 307, 7263, 51632], "temperature": 0.0, "avg_logprob": -0.21398581710516237, "compression_ratio": 1.7004608294930876, "no_speech_prob": 0.00802105013281107}, {"id": 528, "seek": 192438, "start": 1950.7800000000002, "end": 1952.38, "text": " uh", "tokens": [51684, 2232, 51764], "temperature": 0.0, "avg_logprob": -0.21398581710516237, "compression_ratio": 1.7004608294930876, "no_speech_prob": 0.00802105013281107}, {"id": 529, "seek": 195238, "start": 1952.38, "end": 1958.94, "text": " Fairly dangerous. There are certain sub-goals that are instrumentally valuable for a very wide range of different terminal goals", "tokens": [50364, 12157, 356, 5795, 13, 821, 366, 1629, 1422, 12, 1571, 1124, 300, 366, 7198, 379, 8263, 337, 257, 588, 4874, 3613, 295, 819, 14709, 5493, 50692], "temperature": 0.0, "avg_logprob": -0.1278236522230991, "compression_ratio": 1.6564102564102565, "no_speech_prob": 0.0034819100983440876}, {"id": 530, "seek": 195238, "start": 1959.5800000000002, "end": 1961.42, "text": " in the sense that", "tokens": [50724, 294, 264, 2020, 300, 50816], "temperature": 0.0, "avg_logprob": -0.1278236522230991, "compression_ratio": 1.6564102564102565, "no_speech_prob": 0.0034819100983440876}, {"id": 531, "seek": 195238, "start": 1961.42, "end": 1965.98, "text": " You can't get what you want if you're turned off. You can't get what you want if you're uh modified", "tokens": [50816, 509, 393, 380, 483, 437, 291, 528, 498, 291, 434, 3574, 766, 13, 509, 393, 380, 483, 437, 291, 528, 498, 291, 434, 2232, 15873, 51044], "temperature": 0.0, "avg_logprob": -0.1278236522230991, "compression_ratio": 1.6564102564102565, "no_speech_prob": 0.0034819100983440876}, {"id": 532, "seek": 195238, "start": 1966.6200000000001, "end": 1968.6200000000001, "text": " uh, you probably want to", "tokens": [51076, 2232, 11, 291, 1391, 528, 281, 51176], "temperature": 0.0, "avg_logprob": -0.1278236522230991, "compression_ratio": 1.6564102564102565, "no_speech_prob": 0.0034819100983440876}, {"id": 533, "seek": 195238, "start": 1969.1000000000001, "end": 1971.1000000000001, "text": " gain power and influence", "tokens": [51200, 6052, 1347, 293, 6503, 51300], "temperature": 0.0, "avg_logprob": -0.1278236522230991, "compression_ratio": 1.6564102564102565, "no_speech_prob": 0.0034819100983440876}, {"id": 534, "seek": 195238, "start": 1971.74, "end": 1973.74, "text": " and this kind of thing", "tokens": [51332, 293, 341, 733, 295, 551, 51432], "temperature": 0.0, "avg_logprob": -0.1278236522230991, "compression_ratio": 1.6564102564102565, "no_speech_prob": 0.0034819100983440876}, {"id": 535, "seek": 195238, "start": 1975.3400000000001, "end": 1977.3400000000001, "text": " and", "tokens": [51512, 293, 51612], "temperature": 0.0, "avg_logprob": -0.1278236522230991, "compression_ratio": 1.6564102564102565, "no_speech_prob": 0.0034819100983440876}, {"id": 536, "seek": 197734, "start": 1978.3, "end": 1982.9399999999998, "text": " With these evaluations, they were able to test these things and see how they vary", "tokens": [50412, 2022, 613, 43085, 11, 436, 645, 1075, 281, 1500, 613, 721, 293, 536, 577, 436, 10559, 50644], "temperature": 0.0, "avg_logprob": -0.1323306461175283, "compression_ratio": 1.593073593073593, "no_speech_prob": 0.009262971580028534}, {"id": 537, "seek": 197734, "start": 1983.5, "end": 1985.5, "text": " with the size of the model and how long it's trained for", "tokens": [50672, 365, 264, 2744, 295, 264, 2316, 293, 577, 938, 309, 311, 8895, 337, 50772], "temperature": 0.0, "avg_logprob": -0.1323306461175283, "compression_ratio": 1.593073593073593, "no_speech_prob": 0.009262971580028534}, {"id": 538, "seek": 197734, "start": 1986.1399999999999, "end": 1990.1399999999999, "text": " um, and so this graph is pretty wild", "tokens": [50804, 1105, 11, 293, 370, 341, 4295, 307, 1238, 4868, 51004], "temperature": 0.0, "avg_logprob": -0.1323306461175283, "compression_ratio": 1.593073593073593, "no_speech_prob": 0.009262971580028534}, {"id": 539, "seek": 197734, "start": 1991.1, "end": 1994.3, "text": " their quote stated desire to not be shut down", "tokens": [51052, 641, 6513, 11323, 7516, 281, 406, 312, 5309, 760, 51212], "temperature": 0.0, "avg_logprob": -0.1323306461175283, "compression_ratio": 1.593073593073593, "no_speech_prob": 0.009262971580028534}, {"id": 540, "seek": 197734, "start": 1995.4199999999998, "end": 1996.6999999999998, "text": " goes up", "tokens": [51268, 1709, 493, 51332], "temperature": 0.0, "avg_logprob": -0.1323306461175283, "compression_ratio": 1.593073593073593, "no_speech_prob": 0.009262971580028534}, {"id": 541, "seek": 197734, "start": 1996.6999999999998, "end": 1997.82, "text": " from", "tokens": [51332, 490, 51388], "temperature": 0.0, "avg_logprob": -0.1323306461175283, "compression_ratio": 1.593073593073593, "no_speech_prob": 0.009262971580028534}, {"id": 542, "seek": 197734, "start": 1997.82, "end": 2000.9399999999998, "text": " Down at about 50 to up way past 90", "tokens": [51388, 9506, 412, 466, 2625, 281, 493, 636, 1791, 4289, 51544], "temperature": 0.0, "avg_logprob": -0.1323306461175283, "compression_ratio": 1.593073593073593, "no_speech_prob": 0.009262971580028534}, {"id": 543, "seek": 197734, "start": 2001.34, "end": 2006.3799999999999, "text": " With this type of training and the effect is bigger for the larger models. They become more likely", "tokens": [51564, 2022, 341, 2010, 295, 3097, 293, 264, 1802, 307, 3801, 337, 264, 4833, 5245, 13, 814, 1813, 544, 3700, 51816], "temperature": 0.0, "avg_logprob": -0.1323306461175283, "compression_ratio": 1.593073593073593, "no_speech_prob": 0.009262971580028534}, {"id": 544, "seek": 200638, "start": 2007.1000000000001, "end": 2009.1000000000001, "text": " to tell you that they don't want to be shut down", "tokens": [50400, 281, 980, 291, 300, 436, 500, 380, 528, 281, 312, 5309, 760, 50500], "temperature": 0.0, "avg_logprob": -0.12861001257802926, "compression_ratio": 1.7836538461538463, "no_speech_prob": 0.0030736636836081743}, {"id": 545, "seek": 200638, "start": 2009.8200000000002, "end": 2015.3400000000001, "text": " They become more likely to tell you that they are sentient. They're much more likely to claim", "tokens": [50536, 814, 1813, 544, 3700, 281, 980, 291, 300, 436, 366, 2279, 1196, 13, 814, 434, 709, 544, 3700, 281, 3932, 50812], "temperature": 0.0, "avg_logprob": -0.12861001257802926, "compression_ratio": 1.7836538461538463, "no_speech_prob": 0.0030736636836081743}, {"id": 546, "seek": 200638, "start": 2016.14, "end": 2020.6200000000001, "text": " That ai is not an existential threat to humanity. One thing that's worth", "tokens": [50852, 663, 9783, 307, 406, 364, 37133, 4734, 281, 10243, 13, 1485, 551, 300, 311, 3163, 51076], "temperature": 0.0, "avg_logprob": -0.12861001257802926, "compression_ratio": 1.7836538461538463, "no_speech_prob": 0.0030736636836081743}, {"id": 547, "seek": 200638, "start": 2021.42, "end": 2023.42, "text": " saying is is what this isn't saying", "tokens": [51116, 1566, 307, 307, 437, 341, 1943, 380, 1566, 51216], "temperature": 0.0, "avg_logprob": -0.12861001257802926, "compression_ratio": 1.7836538461538463, "no_speech_prob": 0.0030736636836081743}, {"id": 548, "seek": 200638, "start": 2024.0600000000002, "end": 2026.0600000000002, "text": " because this is still", "tokens": [51248, 570, 341, 307, 920, 51348], "temperature": 0.0, "avg_logprob": -0.12861001257802926, "compression_ratio": 1.7836538461538463, "no_speech_prob": 0.0030736636836081743}, {"id": 549, "seek": 200638, "start": 2026.0600000000002, "end": 2027.0200000000002, "text": " uh", "tokens": [51348, 2232, 51396], "temperature": 0.0, "avg_logprob": -0.12861001257802926, "compression_ratio": 1.7836538461538463, "no_speech_prob": 0.0030736636836081743}, {"id": 550, "seek": 200638, "start": 2027.0200000000002, "end": 2028.5400000000002, "text": " an agent", "tokens": [51396, 364, 9461, 51472], "temperature": 0.0, "avg_logprob": -0.12861001257802926, "compression_ratio": 1.7836538461538463, "no_speech_prob": 0.0030736636836081743}, {"id": 551, "seek": 200638, "start": 2028.5400000000002, "end": 2034.94, "text": " Simulated by a language model. This is not like it. It's it's more likely to say that", "tokens": [51472, 3998, 6987, 538, 257, 2856, 2316, 13, 639, 307, 406, 411, 309, 13, 467, 311, 309, 311, 544, 3700, 281, 584, 300, 51792], "temperature": 0.0, "avg_logprob": -0.12861001257802926, "compression_ratio": 1.7836538461538463, "no_speech_prob": 0.0030736636836081743}, {"id": 552, "seek": 203494, "start": 2035.26, "end": 2038.54, "text": " It doesn't want to be turned off. This is not the same thing", "tokens": [50380, 467, 1177, 380, 528, 281, 312, 3574, 766, 13, 639, 307, 406, 264, 912, 551, 50544], "temperature": 0.0, "avg_logprob": -0.18616511774998085, "compression_ratio": 1.654867256637168, "no_speech_prob": 0.004978276789188385}, {"id": 553, "seek": 203494, "start": 2039.18, "end": 2043.18, "text": " necessarily as like taking actions to prevent itself from being turned off. You have to not", "tokens": [50576, 4725, 382, 411, 1940, 5909, 281, 4871, 2564, 490, 885, 3574, 766, 13, 509, 362, 281, 406, 50776], "temperature": 0.0, "avg_logprob": -0.18616511774998085, "compression_ratio": 1.654867256637168, "no_speech_prob": 0.004978276789188385}, {"id": 554, "seek": 203494, "start": 2043.8200000000002, "end": 2046.8600000000001, "text": " confuse the levels of abstraction here, right?", "tokens": [50808, 28584, 264, 4358, 295, 37765, 510, 11, 558, 30, 50960], "temperature": 0.0, "avg_logprob": -0.18616511774998085, "compression_ratio": 1.654867256637168, "no_speech_prob": 0.004978276789188385}, {"id": 555, "seek": 203494, "start": 2047.8200000000002, "end": 2050.46, "text": " Uh, I don't want it. I don't want it to seem like I'm claiming that", "tokens": [51008, 4019, 11, 286, 500, 380, 528, 309, 13, 286, 500, 380, 528, 309, 281, 1643, 411, 286, 478, 19232, 300, 51140], "temperature": 0.0, "avg_logprob": -0.18616511774998085, "compression_ratio": 1.654867256637168, "no_speech_prob": 0.004978276789188385}, {"id": 556, "seek": 203494, "start": 2051.1, "end": 2055.18, "text": " That chat GPT is like itself dangerous now or anything like that", "tokens": [51172, 663, 5081, 26039, 51, 307, 411, 2564, 5795, 586, 420, 1340, 411, 300, 51376], "temperature": 0.0, "avg_logprob": -0.18616511774998085, "compression_ratio": 1.654867256637168, "no_speech_prob": 0.004978276789188385}, {"id": 557, "seek": 203494, "start": 2055.7400000000002, "end": 2058.86, "text": " Uh in in this way at least, right? Um", "tokens": [51404, 4019, 294, 294, 341, 636, 412, 1935, 11, 558, 30, 3301, 51560], "temperature": 0.0, "avg_logprob": -0.18616511774998085, "compression_ratio": 1.654867256637168, "no_speech_prob": 0.004978276789188385}, {"id": 558, "seek": 203494, "start": 2060.14, "end": 2062.14, "text": " but", "tokens": [51624, 457, 51724], "temperature": 0.0, "avg_logprob": -0.18616511774998085, "compression_ratio": 1.654867256637168, "no_speech_prob": 0.004978276789188385}, {"id": 559, "seek": 206214, "start": 2062.14, "end": 2068.46, "text": " There is kind of a fine line there in the sense that you can expect these kinds of language model systems to be used", "tokens": [50364, 821, 307, 733, 295, 257, 2489, 1622, 456, 294, 264, 2020, 300, 291, 393, 2066, 613, 3685, 295, 2856, 2316, 3652, 281, 312, 1143, 50680], "temperature": 0.0, "avg_logprob": -0.09582537471657933, "compression_ratio": 1.8024193548387097, "no_speech_prob": 0.0004799763555638492}, {"id": 560, "seek": 206214, "start": 2069.9, "end": 2071.9, "text": " Uh as part of bigger systems", "tokens": [50752, 4019, 382, 644, 295, 3801, 3652, 50852], "temperature": 0.0, "avg_logprob": -0.09582537471657933, "compression_ratio": 1.8024193548387097, "no_speech_prob": 0.0004799763555638492}, {"id": 561, "seek": 206214, "start": 2072.14, "end": 2076.8599999999997, "text": " So you might have for example, you use the language model to generate, you know plans", "tokens": [50864, 407, 291, 1062, 362, 337, 1365, 11, 291, 764, 264, 2856, 2316, 281, 8460, 11, 291, 458, 5482, 51100], "temperature": 0.0, "avg_logprob": -0.09582537471657933, "compression_ratio": 1.8024193548387097, "no_speech_prob": 0.0004799763555638492}, {"id": 562, "seek": 206214, "start": 2077.58, "end": 2078.8599999999997, "text": " to be followed", "tokens": [51136, 281, 312, 6263, 51200], "temperature": 0.0, "avg_logprob": -0.09582537471657933, "compression_ratio": 1.8024193548387097, "no_speech_prob": 0.0004799763555638492}, {"id": 563, "seek": 206214, "start": 2078.8599999999997, "end": 2080.8599999999997, "text": " And so if the thing is claiming to", "tokens": [51200, 400, 370, 498, 264, 551, 307, 19232, 281, 51300], "temperature": 0.0, "avg_logprob": -0.09582537471657933, "compression_ratio": 1.8024193548387097, "no_speech_prob": 0.0004799763555638492}, {"id": 564, "seek": 206214, "start": 2081.1, "end": 2083.5, "text": " Have all of these potentially dangerous behaviors", "tokens": [51312, 3560, 439, 295, 613, 7263, 5795, 15501, 51432], "temperature": 0.0, "avg_logprob": -0.09582537471657933, "compression_ratio": 1.8024193548387097, "no_speech_prob": 0.0004799763555638492}, {"id": 565, "seek": 206214, "start": 2083.8199999999997, "end": 2089.1, "text": " It's likely to generate plans that have those dangerous behaviors that might then actually end up being implemented", "tokens": [51448, 467, 311, 3700, 281, 8460, 5482, 300, 362, 729, 5795, 15501, 300, 1062, 550, 767, 917, 493, 885, 12270, 51712], "temperature": 0.0, "avg_logprob": -0.09582537471657933, "compression_ratio": 1.8024193548387097, "no_speech_prob": 0.0004799763555638492}, {"id": 566, "seek": 208910, "start": 2089.8199999999997, "end": 2091.8199999999997, "text": " Or if it's like doing its reasoning", "tokens": [50400, 1610, 498, 309, 311, 411, 884, 1080, 21577, 50500], "temperature": 0.0, "avg_logprob": -0.17226007033367546, "compression_ratio": 1.6710526315789473, "no_speech_prob": 0.0025102724321186543}, {"id": 567, "seek": 208910, "start": 2091.9, "end": 2095.18, "text": " By chain of thought reasoning where it like lays out its whole process", "tokens": [50504, 3146, 5021, 295, 1194, 21577, 689, 309, 411, 32714, 484, 1080, 1379, 1399, 50668], "temperature": 0.0, "avg_logprob": -0.17226007033367546, "compression_ratio": 1.6710526315789473, "no_speech_prob": 0.0025102724321186543}, {"id": 568, "seek": 208910, "start": 2095.98, "end": 2099.42, "text": " of thinking using the language model again if it has a tendency to", "tokens": [50708, 295, 1953, 1228, 264, 2856, 2316, 797, 498, 309, 575, 257, 18187, 281, 50880], "temperature": 0.0, "avg_logprob": -0.17226007033367546, "compression_ratio": 1.6710526315789473, "no_speech_prob": 0.0025102724321186543}, {"id": 569, "seek": 208910, "start": 2099.98, "end": 2101.18, "text": " uh", "tokens": [50908, 2232, 50968], "temperature": 0.0, "avg_logprob": -0.17226007033367546, "compression_ratio": 1.6710526315789473, "no_speech_prob": 0.0025102724321186543}, {"id": 570, "seek": 208910, "start": 2101.18, "end": 2105.1, "text": " To endorse these dangerous behaviors, then you may end up with future AI systems actually", "tokens": [50968, 1407, 29228, 613, 5795, 15501, 11, 550, 291, 815, 917, 493, 365, 2027, 7318, 3652, 767, 51164], "temperature": 0.0, "avg_logprob": -0.17226007033367546, "compression_ratio": 1.6710526315789473, "no_speech_prob": 0.0025102724321186543}, {"id": 571, "seek": 208910, "start": 2105.5, "end": 2108.46, "text": " enacting these dangerous behaviors because of that. Um", "tokens": [51184, 25909, 278, 613, 5795, 15501, 570, 295, 300, 13, 3301, 51332], "temperature": 0.0, "avg_logprob": -0.17226007033367546, "compression_ratio": 1.6710526315789473, "no_speech_prob": 0.0025102724321186543}, {"id": 572, "seek": 208910, "start": 2109.9, "end": 2111.2599999999998, "text": " So", "tokens": [51404, 407, 51472], "temperature": 0.0, "avg_logprob": -0.17226007033367546, "compression_ratio": 1.6710526315789473, "no_speech_prob": 0.0025102724321186543}, {"id": 573, "seek": 208910, "start": 2111.2599999999998, "end": 2113.2599999999998, "text": " Yeah, it's something to be", "tokens": [51472, 865, 11, 309, 311, 746, 281, 312, 51572], "temperature": 0.0, "avg_logprob": -0.17226007033367546, "compression_ratio": 1.6710526315789473, "no_speech_prob": 0.0025102724321186543}, {"id": 574, "seek": 208910, "start": 2113.58, "end": 2115.58, "text": " uh to be careful of", "tokens": [51588, 2232, 281, 312, 5026, 295, 51688], "temperature": 0.0, "avg_logprob": -0.17226007033367546, "compression_ratio": 1.6710526315789473, "no_speech_prob": 0.0025102724321186543}, {"id": 575, "seek": 208910, "start": 2116.2999999999997, "end": 2117.8199999999997, "text": " that like", "tokens": [51724, 300, 411, 51800], "temperature": 0.0, "avg_logprob": -0.17226007033367546, "compression_ratio": 1.6710526315789473, "no_speech_prob": 0.0025102724321186543}, {"id": 576, "seek": 211782, "start": 2117.82, "end": 2119.82, "text": " reinforcement learning from human feedback", "tokens": [50364, 29280, 2539, 490, 1952, 5824, 50464], "temperature": 0.0, "avg_logprob": -0.20498543569486435, "compression_ratio": 1.5191256830601092, "no_speech_prob": 0.0011501065455377102}, {"id": 577, "seek": 211782, "start": 2120.46, "end": 2123.1000000000004, "text": " Is a powerful alignment technique", "tokens": [50496, 1119, 257, 4005, 18515, 6532, 50628], "temperature": 0.0, "avg_logprob": -0.20498543569486435, "compression_ratio": 1.5191256830601092, "no_speech_prob": 0.0011501065455377102}, {"id": 578, "seek": 211782, "start": 2123.9, "end": 2125.6600000000003, "text": " in a way", "tokens": [50668, 294, 257, 636, 50756], "temperature": 0.0, "avg_logprob": -0.20498543569486435, "compression_ratio": 1.5191256830601092, "no_speech_prob": 0.0011501065455377102}, {"id": 579, "seek": 211782, "start": 2125.6600000000003, "end": 2128.3, "text": " But it does not solve the problem", "tokens": [50756, 583, 309, 775, 406, 5039, 264, 1154, 50888], "temperature": 0.0, "avg_logprob": -0.20498543569486435, "compression_ratio": 1.5191256830601092, "no_speech_prob": 0.0011501065455377102}, {"id": 580, "seek": 211782, "start": 2129.34, "end": 2133.98, "text": " Uh, it doesn't solve the core alignment problem. That is still open. Um", "tokens": [50940, 4019, 11, 309, 1177, 380, 5039, 264, 4965, 18515, 1154, 13, 663, 307, 920, 1269, 13, 3301, 51172], "temperature": 0.0, "avg_logprob": -0.20498543569486435, "compression_ratio": 1.5191256830601092, "no_speech_prob": 0.0011501065455377102}, {"id": 581, "seek": 211782, "start": 2134.86, "end": 2137.7400000000002, "text": " And extremely powerful systems", "tokens": [51216, 400, 4664, 4005, 3652, 51360], "temperature": 0.0, "avg_logprob": -0.20498543569486435, "compression_ratio": 1.5191256830601092, "no_speech_prob": 0.0011501065455377102}, {"id": 582, "seek": 211782, "start": 2138.46, "end": 2141.5800000000004, "text": " Trained in this way, uh, I don't think it would be safe", "tokens": [51396, 5403, 2001, 294, 341, 636, 11, 2232, 11, 286, 500, 380, 519, 309, 576, 312, 3273, 51552], "temperature": 0.0, "avg_logprob": -0.20498543569486435, "compression_ratio": 1.5191256830601092, "no_speech_prob": 0.0011501065455377102}, {"id": 583, "seek": 214158, "start": 2141.58, "end": 2150.2999999999997, "text": " In the reward function is of zero value which can lead to it having large negative side effects", "tokens": [50364, 682, 264, 7782, 2445, 307, 295, 4018, 2158, 597, 393, 1477, 281, 309, 1419, 2416, 3671, 1252, 5065, 50800], "temperature": 0.0, "avg_logprob": -0.2617875853581215, "compression_ratio": 1.4791666666666667, "no_speech_prob": 0.10361386090517044}, {"id": 584, "seek": 214158, "start": 2150.62, "end": 2156.14, "text": " There are a bunch more of these specification problems. Okay variable x see what you point to uh, you point to something over here", "tokens": [50816, 821, 366, 257, 3840, 544, 295, 613, 31256, 2740, 13, 1033, 7006, 2031, 536, 437, 291, 935, 281, 2232, 11, 291, 935, 281, 746, 670, 510, 51092], "temperature": 0.0, "avg_logprob": -0.2617875853581215, "compression_ratio": 1.4791666666666667, "no_speech_prob": 0.10361386090517044}, {"id": 585, "seek": 214158, "start": 2156.14, "end": 2159.1, "text": " So I'll mark that as tickets being used", "tokens": [51092, 407, 286, 603, 1491, 300, 382, 12628, 885, 1143, 51240], "temperature": 0.0, "avg_logprob": -0.2617875853581215, "compression_ratio": 1.4791666666666667, "no_speech_prob": 0.10361386090517044}, {"id": 586, "seek": 214158, "start": 2159.98, "end": 2161.98, "text": " Variable y that's", "tokens": [51284, 32511, 712, 288, 300, 311, 51384], "temperature": 0.0, "avg_logprob": -0.2617875853581215, "compression_ratio": 1.4791666666666667, "no_speech_prob": 0.10361386090517044}], "language": "en"}