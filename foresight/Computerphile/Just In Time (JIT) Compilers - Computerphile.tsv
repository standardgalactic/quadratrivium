start	end	text
0	4400	We're going to be looking at just-in-time compilation, which is a technique for making
4400	8960	programming languages run faster. We all want faster programming languages because then we get
8960	14240	more speed, and there are various ways we can do it. We can statically compile things. That's
14240	18720	typically done for languages like C, or we can just-in-time compile them, which is often done
18720	23680	for languages like Java or JavaScript. The difference between these two is that
23680	28560	just-in-time compilation is looking at the program running and optimizing it after it's
28560	33600	observed it running. Just-in-time is really a terrible name because it should have happened
33600	40080	before you actually got to that stage, but that's the name we've got. It's a technique that you
40080	45760	will all have used today. If you've used a browser, you're using a JavaScript just-in-time compiler,
46800	51520	and they're sometimes considered a bit magical, so I'm hoping we can look a little bit at the magic.
53760	57920	I'm digging the idea of magical computing. There's no wizards.
57920	62800	I don't have a beard, and I kept the cape at home today. Actually, to disambiguate something that
62800	67120	people often confuse, people often talk about compiled and interpreted languages, and there's
67120	72960	no such thing. For any given program language, you can implement it as a compiler or an interpreter
72960	77600	or just-in-time compiler. People say C is a compiled language, but you can, and there are C
77600	83520	interpreters, and people say JavaScript is an interpreted or just-in-time compiled language,
83520	88160	but you can write a static compiler for that as well. So what do I mean in the sense by those
88160	94480	things? A static compiler reads a programming, just looks at the code the program has written,
94480	100960	and then tries to convert it into machine code, let's say. An interpreter looks at the program,
100960	106800	and then it doesn't convert it into machine code. It executes it almost as is. It probably converts
106800	110560	into some other representation, but it's a very simple way of implementing a programming language.
111520	116240	And a just-in-time compiler nearly always starts a program running an interpreter,
116240	119760	looks at it for a while, says things like, oh, look, you're calling that function a lot, or
120400	125040	there's a function that takes some parameters in, and there are always integers, or always strings.
125040	129760	So I will now produce an optimized version of that function, or part of the program,
129760	134880	based on that information I've observed. And it's really that dynamic analysis and
134880	139360	conversion into machine code at runtime that makes just-in-time compilers very effective.
139360	143440	They can be faster than a static compiler because they've got more information.
143440	147840	So if you just look at a program at compile time, you've just got the code you've written on screen,
148960	154160	you can't fully analyze it as much as you want. So you'll make certain assumptions and guesses,
154160	158480	but they may be incomplete or even incorrect, and you'll optimize based on that.
158480	162400	When you're actually running the thing, you've got more information, so you can make
162400	168160	a much better quality optimization, but you've had to watch the program and observe it. So it
168160	174240	started slow, and then hopefully, over time, hopefully it's warmed up is the term, and then
174240	178800	it gets faster. Warming up turns out to be another kettle of fish. It doesn't always quite work as
178800	183440	we expect. These things have some very surprising emergent behavior, but generally they do work,
183440	188720	and when they do work, they can be very effective. So this is things like, it knows it, and you kind
188720	193280	of, again, alluded to before, it knows it's going to be calling this function quite a bit,
193280	197200	so it keeps that nearby, and things like that. Is that- It can do those sorts of things. Maybe
197200	204400	we could try a little simple example. So if I log in, so what I'll- It's just a simple example,
204400	209280	so there's a load of these that I could use, the Java virtual machines, a just-in-time compiler,
209280	214000	the JavaScript, VMs, all of the ones, VA and Chrome, spider monkey and Firefox, and those
214000	218560	wall jits, but I'll look at one for Python, because that's one that I happen to know fairly well,
218560	223840	and let's just write a very silly little Python program. So I can write this function,
223840	227920	let me make a little bit of a bigger font size for you, and it takes two parameters in, and I
227920	233040	will just add those two parameters together. Thing is, am I going to pass it integers, strings,
233040	237200	I can do all of the above. Let's just check that I'm not lying, because I do sometimes,
238080	242880	sometimes intentionally, but mostly unintentionally. Oh, if I do Hello World, I have to have a space in
242880	250080	there. If I save that file, and then- Right, so it prints out five or Hello World, so you can see
250080	254640	I'm calling the function in different ways. So this is why it's hard to statically optimize that
254640	259680	function, because integers, strings, it can take in all sorts of things. Now, the normal
259680	263840	implementation of Python, which I'm using here with the Python 3 binary, is an interpreter,
263840	269440	doesn't have a just-in-time compiler, and we can see some obvious consequences of this. Let's put
269440	274880	this in some sort of loop. So I'll just put some very big number here, that looks like a big number
274880	280720	to me, and just repeatedly call that function with some integers, it doesn't really matter. Now,
280720	284880	if I run that, and I'll just call it with the time function, and I'm going to say this is my
284880	293680	newest laptop that has from memory six fast cores and eight slower cores, and it's going to run
293680	296880	randomly on those each time I do it. So some of the performance numbers are not going to be quite
296880	301600	as clear cut as I would like. So I'll try and explain it if I see that happening. So I run that,
301600	306000	and it's taken that a tenth of a second to run. And if I make that number a lot bigger, so I make
306000	311120	it an order of magnitude bigger, this for loop, it now takes a bit longer. And if I make it longer
311120	316800	again, we'll see depending on which style of core it's gone on, I think it's, yeah, it's roughly
316800	321440	gone on the two slower cores. It's roughly as I make the loop run 10 times longer, the program is
321440	325360	taking roughly 10 times longer to run. So that's what we'd expect to see an interpreter. Now,
325440	331040	let's get rid of a couple of those zeros. And what I can do instead is use a different
331040	335040	implementation of Python. And this is one thing that we often confuse, we say Python,
335040	338960	when we mean the language, and there's Python, the language, and there's Python, the implementation,
338960	344560	I can use something called pi pi. And I think it has a jit. Let me turn the jit off. And hope that
344560	349360	I've not made it run too slow. So we can see that running. And I've still got a relatively large
349360	355280	number. So it's about the same speed as the normal version of Python. Now we'll turn the jit
355360	365120	on. And it now runs in, well, that's a 10th of second, it has run two orders of magnitude faster.
365120	369680	And in fact, I think we'll see if I've got this right, let's add another couple of zeros there,
369680	374560	something very big. It's actually doesn't really matter how big I make that loop, it's been able
374560	379200	to observe it a runtime, realize it can just optimize the whole thing away. And that's the
379200	384480	power of just in time compilation. It's looked at my runtime values and made the thing very fast.
384480	387920	It's particularly effective on this sort of numeric code, but it will often work well on
387920	392320	things like strings and so on. And of course, there are some cases where it won't work. It
392320	395920	isn't, as we said earlier, magic, but it is very effective in many cases.
396560	402240	So as you scale up, is that still a benefit to using it, you know, when you've got a million
402240	408800	lines of code, for instance? Good question. Very dependent on your program. In some sense,
408800	413600	actually, the scale of it is less important than the nature of your program. There's a
413600	418400	fundamental assumption here that programs tend to do the same thing over and over again.
418960	424560	So in this case, Pi Pi is looking for loops. And it's what's called a tracing just in time
424560	428880	compiler. So it looks at what the loop is doing and optimizes that there's also method base,
428880	431600	which just look at a function, don't need to worry about the difference too much.
432880	436720	If your loop or your function does the same thing repeatedly with only minor variations,
436720	441920	this will be very effective. If you have a program, which every time it goes around the loop does
441920	446960	something completely different or every time you call the method, then this will be less effective.
446960	450880	And in some cases, then it will even slow things down because the program will never appear to
450880	458160	stabilize. And that's really what your expecting programs do is that they typically, when they
458160	462400	start up, they do some initialization, that's all a bit random. And then they tend to hit
462400	465600	some sort of main part where they do the same thing over and over and over again. And that's
465600	471840	where JIT compilation really comes to the fore. And our process is doing a bit of that anyway.
473360	479040	Yeah, so I think of modern processes as basically a just in time compiler. I write machine code and
479040	482320	okay, I write the textual form and it looks like ARM or x86 or whatever.
483120	489120	That's not what the processor eventually executes. It turns that into some other representation. It
489120	494080	does all sorts of clever optimizations. If you ever want to see things like the
496080	502240	processor optimizes program reorders it like the reorder buffer, they're scary at how clever
502240	506080	they are. And that's why they've got a lot faster, even though the gigahertz part hasn't changed too
506080	511680	much. And there's been a little bit of knowledge transfer between the processor JIT world and the
511680	516080	programming language JIT world. Is this a new thing or how long have these kind of just in time
516080	520960	compilers been around them? They have been around in one form or another for a while, but they really
521040	526240	trace their modern lineage back to the 1980s in a language called self, which has been largely
526240	532880	forgotten a really interesting language that had a just in time compiler via a long sequence of
532880	537680	events that ended up going to the company called son and is then formed the basis and literally
537680	543440	some of the code is still there in the Java virtual machine. So the Java JIT traces itself back to
543440	549920	self. V8, the JavaScript VM in Chrome also traces its lineage back to the Java virtual machine hot
549920	556320	spot and back to self. So really, that's been those have been the big movers. And now you've
556320	562720	got systems like Pi Pi and V8 and spider monkey that have modernized the concept or spread it to
562720	567280	more languages will probably be a better way of putting it. And is this, you know, obviously
567280	571040	traces it through it's quite a long way back. But is it only really being used now because
571040	577200	machines have got that much faster? Yeah, I think there's an element of that. Because for you,
577200	582160	when I was a kid, you could burn your computer every 18 months, and it was twice as fast. And
582960	587440	the death of single core performance is a little exaggerated, partly because the processes are
587440	592640	now doing just in time compilation sorts of things. But yeah, we definitely are looking
592640	597520	increasingly to programming languages to work faster and for many languages, particularly,
597520	602800	but not only those that are dynamically typed like Python or Java. This is really the only
602800	607840	effective technique. And that's why you've seen increasing numbers of them being released for
607840	612480	more and more languages, despite the fact that they're really complicated and expensive to create.
612480	615440	You know, these are not the sort of things you can knock out in an afternoon, they take
616240	619120	some big teams, many years to create in most cases.
621760	627200	Is train a network to undo this process? That's the idea. And if we can do that, then we can start
627280	630560	with random noise, a bit like our GAN, and we can just iterate this process.
630560	639280	Four dice. Die A, B, C and D. And I tell you that die A has a value four. How much did you learn
639280	639920	about the data?
