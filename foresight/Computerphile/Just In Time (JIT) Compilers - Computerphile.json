{"text": " We're going to be looking at just-in-time compilation, which is a technique for making programming languages run faster. We all want faster programming languages because then we get more speed, and there are various ways we can do it. We can statically compile things. That's typically done for languages like C, or we can just-in-time compile them, which is often done for languages like Java or JavaScript. The difference between these two is that just-in-time compilation is looking at the program running and optimizing it after it's observed it running. Just-in-time is really a terrible name because it should have happened before you actually got to that stage, but that's the name we've got. It's a technique that you will all have used today. If you've used a browser, you're using a JavaScript just-in-time compiler, and they're sometimes considered a bit magical, so I'm hoping we can look a little bit at the magic. I'm digging the idea of magical computing. There's no wizards. I don't have a beard, and I kept the cape at home today. Actually, to disambiguate something that people often confuse, people often talk about compiled and interpreted languages, and there's no such thing. For any given program language, you can implement it as a compiler or an interpreter or just-in-time compiler. People say C is a compiled language, but you can, and there are C interpreters, and people say JavaScript is an interpreted or just-in-time compiled language, but you can write a static compiler for that as well. So what do I mean in the sense by those things? A static compiler reads a programming, just looks at the code the program has written, and then tries to convert it into machine code, let's say. An interpreter looks at the program, and then it doesn't convert it into machine code. It executes it almost as is. It probably converts into some other representation, but it's a very simple way of implementing a programming language. And a just-in-time compiler nearly always starts a program running an interpreter, looks at it for a while, says things like, oh, look, you're calling that function a lot, or there's a function that takes some parameters in, and there are always integers, or always strings. So I will now produce an optimized version of that function, or part of the program, based on that information I've observed. And it's really that dynamic analysis and conversion into machine code at runtime that makes just-in-time compilers very effective. They can be faster than a static compiler because they've got more information. So if you just look at a program at compile time, you've just got the code you've written on screen, you can't fully analyze it as much as you want. So you'll make certain assumptions and guesses, but they may be incomplete or even incorrect, and you'll optimize based on that. When you're actually running the thing, you've got more information, so you can make a much better quality optimization, but you've had to watch the program and observe it. So it started slow, and then hopefully, over time, hopefully it's warmed up is the term, and then it gets faster. Warming up turns out to be another kettle of fish. It doesn't always quite work as we expect. These things have some very surprising emergent behavior, but generally they do work, and when they do work, they can be very effective. So this is things like, it knows it, and you kind of, again, alluded to before, it knows it's going to be calling this function quite a bit, so it keeps that nearby, and things like that. Is that- It can do those sorts of things. Maybe we could try a little simple example. So if I log in, so what I'll- It's just a simple example, so there's a load of these that I could use, the Java virtual machines, a just-in-time compiler, the JavaScript, VMs, all of the ones, VA and Chrome, spider monkey and Firefox, and those wall jits, but I'll look at one for Python, because that's one that I happen to know fairly well, and let's just write a very silly little Python program. So I can write this function, let me make a little bit of a bigger font size for you, and it takes two parameters in, and I will just add those two parameters together. Thing is, am I going to pass it integers, strings, I can do all of the above. Let's just check that I'm not lying, because I do sometimes, sometimes intentionally, but mostly unintentionally. Oh, if I do Hello World, I have to have a space in there. If I save that file, and then- Right, so it prints out five or Hello World, so you can see I'm calling the function in different ways. So this is why it's hard to statically optimize that function, because integers, strings, it can take in all sorts of things. Now, the normal implementation of Python, which I'm using here with the Python 3 binary, is an interpreter, doesn't have a just-in-time compiler, and we can see some obvious consequences of this. Let's put this in some sort of loop. So I'll just put some very big number here, that looks like a big number to me, and just repeatedly call that function with some integers, it doesn't really matter. Now, if I run that, and I'll just call it with the time function, and I'm going to say this is my newest laptop that has from memory six fast cores and eight slower cores, and it's going to run randomly on those each time I do it. So some of the performance numbers are not going to be quite as clear cut as I would like. So I'll try and explain it if I see that happening. So I run that, and it's taken that a tenth of a second to run. And if I make that number a lot bigger, so I make it an order of magnitude bigger, this for loop, it now takes a bit longer. And if I make it longer again, we'll see depending on which style of core it's gone on, I think it's, yeah, it's roughly gone on the two slower cores. It's roughly as I make the loop run 10 times longer, the program is taking roughly 10 times longer to run. So that's what we'd expect to see an interpreter. Now, let's get rid of a couple of those zeros. And what I can do instead is use a different implementation of Python. And this is one thing that we often confuse, we say Python, when we mean the language, and there's Python, the language, and there's Python, the implementation, I can use something called pi pi. And I think it has a jit. Let me turn the jit off. And hope that I've not made it run too slow. So we can see that running. And I've still got a relatively large number. So it's about the same speed as the normal version of Python. Now we'll turn the jit on. And it now runs in, well, that's a 10th of second, it has run two orders of magnitude faster. And in fact, I think we'll see if I've got this right, let's add another couple of zeros there, something very big. It's actually doesn't really matter how big I make that loop, it's been able to observe it a runtime, realize it can just optimize the whole thing away. And that's the power of just in time compilation. It's looked at my runtime values and made the thing very fast. It's particularly effective on this sort of numeric code, but it will often work well on things like strings and so on. And of course, there are some cases where it won't work. It isn't, as we said earlier, magic, but it is very effective in many cases. So as you scale up, is that still a benefit to using it, you know, when you've got a million lines of code, for instance? Good question. Very dependent on your program. In some sense, actually, the scale of it is less important than the nature of your program. There's a fundamental assumption here that programs tend to do the same thing over and over again. So in this case, Pi Pi is looking for loops. And it's what's called a tracing just in time compiler. So it looks at what the loop is doing and optimizes that there's also method base, which just look at a function, don't need to worry about the difference too much. If your loop or your function does the same thing repeatedly with only minor variations, this will be very effective. If you have a program, which every time it goes around the loop does something completely different or every time you call the method, then this will be less effective. And in some cases, then it will even slow things down because the program will never appear to stabilize. And that's really what your expecting programs do is that they typically, when they start up, they do some initialization, that's all a bit random. And then they tend to hit some sort of main part where they do the same thing over and over and over again. And that's where JIT compilation really comes to the fore. And our process is doing a bit of that anyway. Yeah, so I think of modern processes as basically a just in time compiler. I write machine code and okay, I write the textual form and it looks like ARM or x86 or whatever. That's not what the processor eventually executes. It turns that into some other representation. It does all sorts of clever optimizations. If you ever want to see things like the processor optimizes program reorders it like the reorder buffer, they're scary at how clever they are. And that's why they've got a lot faster, even though the gigahertz part hasn't changed too much. And there's been a little bit of knowledge transfer between the processor JIT world and the programming language JIT world. Is this a new thing or how long have these kind of just in time compilers been around them? They have been around in one form or another for a while, but they really trace their modern lineage back to the 1980s in a language called self, which has been largely forgotten a really interesting language that had a just in time compiler via a long sequence of events that ended up going to the company called son and is then formed the basis and literally some of the code is still there in the Java virtual machine. So the Java JIT traces itself back to self. V8, the JavaScript VM in Chrome also traces its lineage back to the Java virtual machine hot spot and back to self. So really, that's been those have been the big movers. And now you've got systems like Pi Pi and V8 and spider monkey that have modernized the concept or spread it to more languages will probably be a better way of putting it. And is this, you know, obviously traces it through it's quite a long way back. But is it only really being used now because machines have got that much faster? Yeah, I think there's an element of that. Because for you, when I was a kid, you could burn your computer every 18 months, and it was twice as fast. And the death of single core performance is a little exaggerated, partly because the processes are now doing just in time compilation sorts of things. But yeah, we definitely are looking increasingly to programming languages to work faster and for many languages, particularly, but not only those that are dynamically typed like Python or Java. This is really the only effective technique. And that's why you've seen increasing numbers of them being released for more and more languages, despite the fact that they're really complicated and expensive to create. You know, these are not the sort of things you can knock out in an afternoon, they take some big teams, many years to create in most cases. Is train a network to undo this process? That's the idea. And if we can do that, then we can start with random noise, a bit like our GAN, and we can just iterate this process. Four dice. Die A, B, C and D. And I tell you that die A has a value four. How much did you learn about the data?", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 4.4, "text": " We're going to be looking at just-in-time compilation, which is a technique for making", "tokens": [50364, 492, 434, 516, 281, 312, 1237, 412, 445, 12, 259, 12, 3766, 40261, 11, 597, 307, 257, 6532, 337, 1455, 50584], "temperature": 0.0, "avg_logprob": -0.09349800872802734, "compression_ratio": 1.9386281588447654, "no_speech_prob": 0.0033694307785481215}, {"id": 1, "seek": 0, "start": 4.4, "end": 8.96, "text": " programming languages run faster. We all want faster programming languages because then we get", "tokens": [50584, 9410, 8650, 1190, 4663, 13, 492, 439, 528, 4663, 9410, 8650, 570, 550, 321, 483, 50812], "temperature": 0.0, "avg_logprob": -0.09349800872802734, "compression_ratio": 1.9386281588447654, "no_speech_prob": 0.0033694307785481215}, {"id": 2, "seek": 0, "start": 8.96, "end": 14.24, "text": " more speed, and there are various ways we can do it. We can statically compile things. That's", "tokens": [50812, 544, 3073, 11, 293, 456, 366, 3683, 2098, 321, 393, 360, 309, 13, 492, 393, 2219, 984, 31413, 721, 13, 663, 311, 51076], "temperature": 0.0, "avg_logprob": -0.09349800872802734, "compression_ratio": 1.9386281588447654, "no_speech_prob": 0.0033694307785481215}, {"id": 3, "seek": 0, "start": 14.24, "end": 18.72, "text": " typically done for languages like C, or we can just-in-time compile them, which is often done", "tokens": [51076, 5850, 1096, 337, 8650, 411, 383, 11, 420, 321, 393, 445, 12, 259, 12, 3766, 31413, 552, 11, 597, 307, 2049, 1096, 51300], "temperature": 0.0, "avg_logprob": -0.09349800872802734, "compression_ratio": 1.9386281588447654, "no_speech_prob": 0.0033694307785481215}, {"id": 4, "seek": 0, "start": 18.72, "end": 23.68, "text": " for languages like Java or JavaScript. The difference between these two is that", "tokens": [51300, 337, 8650, 411, 10745, 420, 15778, 13, 440, 2649, 1296, 613, 732, 307, 300, 51548], "temperature": 0.0, "avg_logprob": -0.09349800872802734, "compression_ratio": 1.9386281588447654, "no_speech_prob": 0.0033694307785481215}, {"id": 5, "seek": 0, "start": 23.68, "end": 28.560000000000002, "text": " just-in-time compilation is looking at the program running and optimizing it after it's", "tokens": [51548, 445, 12, 259, 12, 3766, 40261, 307, 1237, 412, 264, 1461, 2614, 293, 40425, 309, 934, 309, 311, 51792], "temperature": 0.0, "avg_logprob": -0.09349800872802734, "compression_ratio": 1.9386281588447654, "no_speech_prob": 0.0033694307785481215}, {"id": 6, "seek": 2856, "start": 28.56, "end": 33.6, "text": " observed it running. Just-in-time is really a terrible name because it should have happened", "tokens": [50364, 13095, 309, 2614, 13, 1449, 12, 259, 12, 3766, 307, 534, 257, 6237, 1315, 570, 309, 820, 362, 2011, 50616], "temperature": 0.0, "avg_logprob": -0.10150546701545389, "compression_ratio": 1.6678966789667897, "no_speech_prob": 0.0019698948599398136}, {"id": 7, "seek": 2856, "start": 33.6, "end": 40.08, "text": " before you actually got to that stage, but that's the name we've got. It's a technique that you", "tokens": [50616, 949, 291, 767, 658, 281, 300, 3233, 11, 457, 300, 311, 264, 1315, 321, 600, 658, 13, 467, 311, 257, 6532, 300, 291, 50940], "temperature": 0.0, "avg_logprob": -0.10150546701545389, "compression_ratio": 1.6678966789667897, "no_speech_prob": 0.0019698948599398136}, {"id": 8, "seek": 2856, "start": 40.08, "end": 45.76, "text": " will all have used today. If you've used a browser, you're using a JavaScript just-in-time compiler,", "tokens": [50940, 486, 439, 362, 1143, 965, 13, 759, 291, 600, 1143, 257, 11185, 11, 291, 434, 1228, 257, 15778, 445, 12, 259, 12, 3766, 31958, 11, 51224], "temperature": 0.0, "avg_logprob": -0.10150546701545389, "compression_ratio": 1.6678966789667897, "no_speech_prob": 0.0019698948599398136}, {"id": 9, "seek": 2856, "start": 46.8, "end": 51.519999999999996, "text": " and they're sometimes considered a bit magical, so I'm hoping we can look a little bit at the magic.", "tokens": [51276, 293, 436, 434, 2171, 4888, 257, 857, 12066, 11, 370, 286, 478, 7159, 321, 393, 574, 257, 707, 857, 412, 264, 5585, 13, 51512], "temperature": 0.0, "avg_logprob": -0.10150546701545389, "compression_ratio": 1.6678966789667897, "no_speech_prob": 0.0019698948599398136}, {"id": 10, "seek": 2856, "start": 53.76, "end": 57.92, "text": " I'm digging the idea of magical computing. There's no wizards.", "tokens": [51624, 286, 478, 17343, 264, 1558, 295, 12066, 15866, 13, 821, 311, 572, 40808, 2287, 13, 51832], "temperature": 0.0, "avg_logprob": -0.10150546701545389, "compression_ratio": 1.6678966789667897, "no_speech_prob": 0.0019698948599398136}, {"id": 11, "seek": 5792, "start": 57.92, "end": 62.800000000000004, "text": " I don't have a beard, and I kept the cape at home today. Actually, to disambiguate something that", "tokens": [50364, 286, 500, 380, 362, 257, 17455, 11, 293, 286, 4305, 264, 30414, 412, 1280, 965, 13, 5135, 11, 281, 717, 2173, 328, 10107, 746, 300, 50608], "temperature": 0.0, "avg_logprob": -0.08051227701121363, "compression_ratio": 1.8814229249011858, "no_speech_prob": 0.00048664744826965034}, {"id": 12, "seek": 5792, "start": 62.800000000000004, "end": 67.12, "text": " people often confuse, people often talk about compiled and interpreted languages, and there's", "tokens": [50608, 561, 2049, 28584, 11, 561, 2049, 751, 466, 36548, 293, 26749, 8650, 11, 293, 456, 311, 50824], "temperature": 0.0, "avg_logprob": -0.08051227701121363, "compression_ratio": 1.8814229249011858, "no_speech_prob": 0.00048664744826965034}, {"id": 13, "seek": 5792, "start": 67.12, "end": 72.96000000000001, "text": " no such thing. For any given program language, you can implement it as a compiler or an interpreter", "tokens": [50824, 572, 1270, 551, 13, 1171, 604, 2212, 1461, 2856, 11, 291, 393, 4445, 309, 382, 257, 31958, 420, 364, 34132, 51116], "temperature": 0.0, "avg_logprob": -0.08051227701121363, "compression_ratio": 1.8814229249011858, "no_speech_prob": 0.00048664744826965034}, {"id": 14, "seek": 5792, "start": 72.96000000000001, "end": 77.6, "text": " or just-in-time compiler. People say C is a compiled language, but you can, and there are C", "tokens": [51116, 420, 445, 12, 259, 12, 3766, 31958, 13, 3432, 584, 383, 307, 257, 36548, 2856, 11, 457, 291, 393, 11, 293, 456, 366, 383, 51348], "temperature": 0.0, "avg_logprob": -0.08051227701121363, "compression_ratio": 1.8814229249011858, "no_speech_prob": 0.00048664744826965034}, {"id": 15, "seek": 5792, "start": 77.6, "end": 83.52000000000001, "text": " interpreters, and people say JavaScript is an interpreted or just-in-time compiled language,", "tokens": [51348, 17489, 1559, 11, 293, 561, 584, 15778, 307, 364, 26749, 420, 445, 12, 259, 12, 3766, 36548, 2856, 11, 51644], "temperature": 0.0, "avg_logprob": -0.08051227701121363, "compression_ratio": 1.8814229249011858, "no_speech_prob": 0.00048664744826965034}, {"id": 16, "seek": 8352, "start": 83.52, "end": 88.16, "text": " but you can write a static compiler for that as well. So what do I mean in the sense by those", "tokens": [50364, 457, 291, 393, 2464, 257, 13437, 31958, 337, 300, 382, 731, 13, 407, 437, 360, 286, 914, 294, 264, 2020, 538, 729, 50596], "temperature": 0.0, "avg_logprob": -0.10496424792105691, "compression_ratio": 1.8295454545454546, "no_speech_prob": 0.0008378281490877271}, {"id": 17, "seek": 8352, "start": 88.16, "end": 94.47999999999999, "text": " things? A static compiler reads a programming, just looks at the code the program has written,", "tokens": [50596, 721, 30, 316, 13437, 31958, 15700, 257, 9410, 11, 445, 1542, 412, 264, 3089, 264, 1461, 575, 3720, 11, 50912], "temperature": 0.0, "avg_logprob": -0.10496424792105691, "compression_ratio": 1.8295454545454546, "no_speech_prob": 0.0008378281490877271}, {"id": 18, "seek": 8352, "start": 94.47999999999999, "end": 100.96, "text": " and then tries to convert it into machine code, let's say. An interpreter looks at the program,", "tokens": [50912, 293, 550, 9898, 281, 7620, 309, 666, 3479, 3089, 11, 718, 311, 584, 13, 1107, 34132, 1542, 412, 264, 1461, 11, 51236], "temperature": 0.0, "avg_logprob": -0.10496424792105691, "compression_ratio": 1.8295454545454546, "no_speech_prob": 0.0008378281490877271}, {"id": 19, "seek": 8352, "start": 100.96, "end": 106.8, "text": " and then it doesn't convert it into machine code. It executes it almost as is. It probably converts", "tokens": [51236, 293, 550, 309, 1177, 380, 7620, 309, 666, 3479, 3089, 13, 467, 4454, 1819, 309, 1920, 382, 307, 13, 467, 1391, 38874, 51528], "temperature": 0.0, "avg_logprob": -0.10496424792105691, "compression_ratio": 1.8295454545454546, "no_speech_prob": 0.0008378281490877271}, {"id": 20, "seek": 8352, "start": 106.8, "end": 110.56, "text": " into some other representation, but it's a very simple way of implementing a programming language.", "tokens": [51528, 666, 512, 661, 10290, 11, 457, 309, 311, 257, 588, 2199, 636, 295, 18114, 257, 9410, 2856, 13, 51716], "temperature": 0.0, "avg_logprob": -0.10496424792105691, "compression_ratio": 1.8295454545454546, "no_speech_prob": 0.0008378281490877271}, {"id": 21, "seek": 11056, "start": 111.52, "end": 116.24000000000001, "text": " And a just-in-time compiler nearly always starts a program running an interpreter,", "tokens": [50412, 400, 257, 445, 12, 259, 12, 3766, 31958, 6217, 1009, 3719, 257, 1461, 2614, 364, 34132, 11, 50648], "temperature": 0.0, "avg_logprob": -0.08949465304613113, "compression_ratio": 1.7674418604651163, "no_speech_prob": 0.0014080229448154569}, {"id": 22, "seek": 11056, "start": 116.24000000000001, "end": 119.76, "text": " looks at it for a while, says things like, oh, look, you're calling that function a lot, or", "tokens": [50648, 1542, 412, 309, 337, 257, 1339, 11, 1619, 721, 411, 11, 1954, 11, 574, 11, 291, 434, 5141, 300, 2445, 257, 688, 11, 420, 50824], "temperature": 0.0, "avg_logprob": -0.08949465304613113, "compression_ratio": 1.7674418604651163, "no_speech_prob": 0.0014080229448154569}, {"id": 23, "seek": 11056, "start": 120.4, "end": 125.04, "text": " there's a function that takes some parameters in, and there are always integers, or always strings.", "tokens": [50856, 456, 311, 257, 2445, 300, 2516, 512, 9834, 294, 11, 293, 456, 366, 1009, 41674, 11, 420, 1009, 13985, 13, 51088], "temperature": 0.0, "avg_logprob": -0.08949465304613113, "compression_ratio": 1.7674418604651163, "no_speech_prob": 0.0014080229448154569}, {"id": 24, "seek": 11056, "start": 125.04, "end": 129.76, "text": " So I will now produce an optimized version of that function, or part of the program,", "tokens": [51088, 407, 286, 486, 586, 5258, 364, 26941, 3037, 295, 300, 2445, 11, 420, 644, 295, 264, 1461, 11, 51324], "temperature": 0.0, "avg_logprob": -0.08949465304613113, "compression_ratio": 1.7674418604651163, "no_speech_prob": 0.0014080229448154569}, {"id": 25, "seek": 11056, "start": 129.76, "end": 134.88, "text": " based on that information I've observed. And it's really that dynamic analysis and", "tokens": [51324, 2361, 322, 300, 1589, 286, 600, 13095, 13, 400, 309, 311, 534, 300, 8546, 5215, 293, 51580], "temperature": 0.0, "avg_logprob": -0.08949465304613113, "compression_ratio": 1.7674418604651163, "no_speech_prob": 0.0014080229448154569}, {"id": 26, "seek": 11056, "start": 134.88, "end": 139.36, "text": " conversion into machine code at runtime that makes just-in-time compilers very effective.", "tokens": [51580, 14298, 666, 3479, 3089, 412, 34474, 300, 1669, 445, 12, 259, 12, 3766, 715, 388, 433, 588, 4942, 13, 51804], "temperature": 0.0, "avg_logprob": -0.08949465304613113, "compression_ratio": 1.7674418604651163, "no_speech_prob": 0.0014080229448154569}, {"id": 27, "seek": 13936, "start": 139.36, "end": 143.44000000000003, "text": " They can be faster than a static compiler because they've got more information.", "tokens": [50364, 814, 393, 312, 4663, 813, 257, 13437, 31958, 570, 436, 600, 658, 544, 1589, 13, 50568], "temperature": 0.0, "avg_logprob": -0.07141568679218144, "compression_ratio": 1.8047138047138047, "no_speech_prob": 0.00035761305480264127}, {"id": 28, "seek": 13936, "start": 143.44000000000003, "end": 147.84, "text": " So if you just look at a program at compile time, you've just got the code you've written on screen,", "tokens": [50568, 407, 498, 291, 445, 574, 412, 257, 1461, 412, 31413, 565, 11, 291, 600, 445, 658, 264, 3089, 291, 600, 3720, 322, 2568, 11, 50788], "temperature": 0.0, "avg_logprob": -0.07141568679218144, "compression_ratio": 1.8047138047138047, "no_speech_prob": 0.00035761305480264127}, {"id": 29, "seek": 13936, "start": 148.96, "end": 154.16000000000003, "text": " you can't fully analyze it as much as you want. So you'll make certain assumptions and guesses,", "tokens": [50844, 291, 393, 380, 4498, 12477, 309, 382, 709, 382, 291, 528, 13, 407, 291, 603, 652, 1629, 17695, 293, 42703, 11, 51104], "temperature": 0.0, "avg_logprob": -0.07141568679218144, "compression_ratio": 1.8047138047138047, "no_speech_prob": 0.00035761305480264127}, {"id": 30, "seek": 13936, "start": 154.16000000000003, "end": 158.48000000000002, "text": " but they may be incomplete or even incorrect, and you'll optimize based on that.", "tokens": [51104, 457, 436, 815, 312, 31709, 420, 754, 18424, 11, 293, 291, 603, 19719, 2361, 322, 300, 13, 51320], "temperature": 0.0, "avg_logprob": -0.07141568679218144, "compression_ratio": 1.8047138047138047, "no_speech_prob": 0.00035761305480264127}, {"id": 31, "seek": 13936, "start": 158.48000000000002, "end": 162.4, "text": " When you're actually running the thing, you've got more information, so you can make", "tokens": [51320, 1133, 291, 434, 767, 2614, 264, 551, 11, 291, 600, 658, 544, 1589, 11, 370, 291, 393, 652, 51516], "temperature": 0.0, "avg_logprob": -0.07141568679218144, "compression_ratio": 1.8047138047138047, "no_speech_prob": 0.00035761305480264127}, {"id": 32, "seek": 13936, "start": 162.4, "end": 168.16000000000003, "text": " a much better quality optimization, but you've had to watch the program and observe it. So it", "tokens": [51516, 257, 709, 1101, 3125, 19618, 11, 457, 291, 600, 632, 281, 1159, 264, 1461, 293, 11441, 309, 13, 407, 309, 51804], "temperature": 0.0, "avg_logprob": -0.07141568679218144, "compression_ratio": 1.8047138047138047, "no_speech_prob": 0.00035761305480264127}, {"id": 33, "seek": 16816, "start": 168.16, "end": 174.24, "text": " started slow, and then hopefully, over time, hopefully it's warmed up is the term, and then", "tokens": [50364, 1409, 2964, 11, 293, 550, 4696, 11, 670, 565, 11, 4696, 309, 311, 38201, 493, 307, 264, 1433, 11, 293, 550, 50668], "temperature": 0.0, "avg_logprob": -0.13070050713156356, "compression_ratio": 1.7826086956521738, "no_speech_prob": 0.0010615235660225153}, {"id": 34, "seek": 16816, "start": 174.24, "end": 178.8, "text": " it gets faster. Warming up turns out to be another kettle of fish. It doesn't always quite work as", "tokens": [50668, 309, 2170, 4663, 13, 3630, 2810, 493, 4523, 484, 281, 312, 1071, 39088, 295, 3506, 13, 467, 1177, 380, 1009, 1596, 589, 382, 50896], "temperature": 0.0, "avg_logprob": -0.13070050713156356, "compression_ratio": 1.7826086956521738, "no_speech_prob": 0.0010615235660225153}, {"id": 35, "seek": 16816, "start": 178.8, "end": 183.44, "text": " we expect. These things have some very surprising emergent behavior, but generally they do work,", "tokens": [50896, 321, 2066, 13, 1981, 721, 362, 512, 588, 8830, 4345, 6930, 5223, 11, 457, 5101, 436, 360, 589, 11, 51128], "temperature": 0.0, "avg_logprob": -0.13070050713156356, "compression_ratio": 1.7826086956521738, "no_speech_prob": 0.0010615235660225153}, {"id": 36, "seek": 16816, "start": 183.44, "end": 188.72, "text": " and when they do work, they can be very effective. So this is things like, it knows it, and you kind", "tokens": [51128, 293, 562, 436, 360, 589, 11, 436, 393, 312, 588, 4942, 13, 407, 341, 307, 721, 411, 11, 309, 3255, 309, 11, 293, 291, 733, 51392], "temperature": 0.0, "avg_logprob": -0.13070050713156356, "compression_ratio": 1.7826086956521738, "no_speech_prob": 0.0010615235660225153}, {"id": 37, "seek": 16816, "start": 188.72, "end": 193.28, "text": " of, again, alluded to before, it knows it's going to be calling this function quite a bit,", "tokens": [51392, 295, 11, 797, 11, 33919, 281, 949, 11, 309, 3255, 309, 311, 516, 281, 312, 5141, 341, 2445, 1596, 257, 857, 11, 51620], "temperature": 0.0, "avg_logprob": -0.13070050713156356, "compression_ratio": 1.7826086956521738, "no_speech_prob": 0.0010615235660225153}, {"id": 38, "seek": 16816, "start": 193.28, "end": 197.2, "text": " so it keeps that nearby, and things like that. Is that- It can do those sorts of things. Maybe", "tokens": [51620, 370, 309, 5965, 300, 11184, 11, 293, 721, 411, 300, 13, 1119, 300, 12, 467, 393, 360, 729, 7527, 295, 721, 13, 2704, 51816], "temperature": 0.0, "avg_logprob": -0.13070050713156356, "compression_ratio": 1.7826086956521738, "no_speech_prob": 0.0010615235660225153}, {"id": 39, "seek": 19720, "start": 197.2, "end": 204.39999999999998, "text": " we could try a little simple example. So if I log in, so what I'll- It's just a simple example,", "tokens": [50364, 321, 727, 853, 257, 707, 2199, 1365, 13, 407, 498, 286, 3565, 294, 11, 370, 437, 286, 603, 12, 467, 311, 445, 257, 2199, 1365, 11, 50724], "temperature": 0.0, "avg_logprob": -0.18688286182492278, "compression_ratio": 1.6619217081850535, "no_speech_prob": 0.0016880020266398787}, {"id": 40, "seek": 19720, "start": 204.39999999999998, "end": 209.28, "text": " so there's a load of these that I could use, the Java virtual machines, a just-in-time compiler,", "tokens": [50724, 370, 456, 311, 257, 3677, 295, 613, 300, 286, 727, 764, 11, 264, 10745, 6374, 8379, 11, 257, 445, 12, 259, 12, 3766, 31958, 11, 50968], "temperature": 0.0, "avg_logprob": -0.18688286182492278, "compression_ratio": 1.6619217081850535, "no_speech_prob": 0.0016880020266398787}, {"id": 41, "seek": 19720, "start": 209.28, "end": 214.0, "text": " the JavaScript, VMs, all of the ones, VA and Chrome, spider monkey and Firefox, and those", "tokens": [50968, 264, 15778, 11, 18038, 82, 11, 439, 295, 264, 2306, 11, 18527, 293, 15327, 11, 17614, 17847, 293, 46613, 11, 293, 729, 51204], "temperature": 0.0, "avg_logprob": -0.18688286182492278, "compression_ratio": 1.6619217081850535, "no_speech_prob": 0.0016880020266398787}, {"id": 42, "seek": 19720, "start": 214.0, "end": 218.56, "text": " wall jits, but I'll look at one for Python, because that's one that I happen to know fairly well,", "tokens": [51204, 2929, 361, 1208, 11, 457, 286, 603, 574, 412, 472, 337, 15329, 11, 570, 300, 311, 472, 300, 286, 1051, 281, 458, 6457, 731, 11, 51432], "temperature": 0.0, "avg_logprob": -0.18688286182492278, "compression_ratio": 1.6619217081850535, "no_speech_prob": 0.0016880020266398787}, {"id": 43, "seek": 19720, "start": 218.56, "end": 223.83999999999997, "text": " and let's just write a very silly little Python program. So I can write this function,", "tokens": [51432, 293, 718, 311, 445, 2464, 257, 588, 11774, 707, 15329, 1461, 13, 407, 286, 393, 2464, 341, 2445, 11, 51696], "temperature": 0.0, "avg_logprob": -0.18688286182492278, "compression_ratio": 1.6619217081850535, "no_speech_prob": 0.0016880020266398787}, {"id": 44, "seek": 22384, "start": 223.84, "end": 227.92000000000002, "text": " let me make a little bit of a bigger font size for you, and it takes two parameters in, and I", "tokens": [50364, 718, 385, 652, 257, 707, 857, 295, 257, 3801, 10703, 2744, 337, 291, 11, 293, 309, 2516, 732, 9834, 294, 11, 293, 286, 50568], "temperature": 0.0, "avg_logprob": -0.11578231304883957, "compression_ratio": 1.7107142857142856, "no_speech_prob": 0.012428002431988716}, {"id": 45, "seek": 22384, "start": 227.92000000000002, "end": 233.04, "text": " will just add those two parameters together. Thing is, am I going to pass it integers, strings,", "tokens": [50568, 486, 445, 909, 729, 732, 9834, 1214, 13, 30902, 307, 11, 669, 286, 516, 281, 1320, 309, 41674, 11, 13985, 11, 50824], "temperature": 0.0, "avg_logprob": -0.11578231304883957, "compression_ratio": 1.7107142857142856, "no_speech_prob": 0.012428002431988716}, {"id": 46, "seek": 22384, "start": 233.04, "end": 237.2, "text": " I can do all of the above. Let's just check that I'm not lying, because I do sometimes,", "tokens": [50824, 286, 393, 360, 439, 295, 264, 3673, 13, 961, 311, 445, 1520, 300, 286, 478, 406, 8493, 11, 570, 286, 360, 2171, 11, 51032], "temperature": 0.0, "avg_logprob": -0.11578231304883957, "compression_ratio": 1.7107142857142856, "no_speech_prob": 0.012428002431988716}, {"id": 47, "seek": 22384, "start": 238.08, "end": 242.88, "text": " sometimes intentionally, but mostly unintentionally. Oh, if I do Hello World, I have to have a space in", "tokens": [51076, 2171, 22062, 11, 457, 5240, 45514, 379, 13, 876, 11, 498, 286, 360, 2425, 3937, 11, 286, 362, 281, 362, 257, 1901, 294, 51316], "temperature": 0.0, "avg_logprob": -0.11578231304883957, "compression_ratio": 1.7107142857142856, "no_speech_prob": 0.012428002431988716}, {"id": 48, "seek": 22384, "start": 242.88, "end": 250.08, "text": " there. If I save that file, and then- Right, so it prints out five or Hello World, so you can see", "tokens": [51316, 456, 13, 759, 286, 3155, 300, 3991, 11, 293, 550, 12, 1779, 11, 370, 309, 22305, 484, 1732, 420, 2425, 3937, 11, 370, 291, 393, 536, 51676], "temperature": 0.0, "avg_logprob": -0.11578231304883957, "compression_ratio": 1.7107142857142856, "no_speech_prob": 0.012428002431988716}, {"id": 49, "seek": 25008, "start": 250.08, "end": 254.64000000000001, "text": " I'm calling the function in different ways. So this is why it's hard to statically optimize that", "tokens": [50364, 286, 478, 5141, 264, 2445, 294, 819, 2098, 13, 407, 341, 307, 983, 309, 311, 1152, 281, 2219, 984, 19719, 300, 50592], "temperature": 0.0, "avg_logprob": -0.07157777945200602, "compression_ratio": 1.6323024054982818, "no_speech_prob": 0.01047561876475811}, {"id": 50, "seek": 25008, "start": 254.64000000000001, "end": 259.68, "text": " function, because integers, strings, it can take in all sorts of things. Now, the normal", "tokens": [50592, 2445, 11, 570, 41674, 11, 13985, 11, 309, 393, 747, 294, 439, 7527, 295, 721, 13, 823, 11, 264, 2710, 50844], "temperature": 0.0, "avg_logprob": -0.07157777945200602, "compression_ratio": 1.6323024054982818, "no_speech_prob": 0.01047561876475811}, {"id": 51, "seek": 25008, "start": 259.68, "end": 263.84000000000003, "text": " implementation of Python, which I'm using here with the Python 3 binary, is an interpreter,", "tokens": [50844, 11420, 295, 15329, 11, 597, 286, 478, 1228, 510, 365, 264, 15329, 805, 17434, 11, 307, 364, 34132, 11, 51052], "temperature": 0.0, "avg_logprob": -0.07157777945200602, "compression_ratio": 1.6323024054982818, "no_speech_prob": 0.01047561876475811}, {"id": 52, "seek": 25008, "start": 263.84000000000003, "end": 269.44, "text": " doesn't have a just-in-time compiler, and we can see some obvious consequences of this. Let's put", "tokens": [51052, 1177, 380, 362, 257, 445, 12, 259, 12, 3766, 31958, 11, 293, 321, 393, 536, 512, 6322, 10098, 295, 341, 13, 961, 311, 829, 51332], "temperature": 0.0, "avg_logprob": -0.07157777945200602, "compression_ratio": 1.6323024054982818, "no_speech_prob": 0.01047561876475811}, {"id": 53, "seek": 25008, "start": 269.44, "end": 274.88, "text": " this in some sort of loop. So I'll just put some very big number here, that looks like a big number", "tokens": [51332, 341, 294, 512, 1333, 295, 6367, 13, 407, 286, 603, 445, 829, 512, 588, 955, 1230, 510, 11, 300, 1542, 411, 257, 955, 1230, 51604], "temperature": 0.0, "avg_logprob": -0.07157777945200602, "compression_ratio": 1.6323024054982818, "no_speech_prob": 0.01047561876475811}, {"id": 54, "seek": 27488, "start": 274.88, "end": 280.71999999999997, "text": " to me, and just repeatedly call that function with some integers, it doesn't really matter. Now,", "tokens": [50364, 281, 385, 11, 293, 445, 18227, 818, 300, 2445, 365, 512, 41674, 11, 309, 1177, 380, 534, 1871, 13, 823, 11, 50656], "temperature": 0.0, "avg_logprob": -0.09712134552001953, "compression_ratio": 1.7582417582417582, "no_speech_prob": 0.012741945683956146}, {"id": 55, "seek": 27488, "start": 280.71999999999997, "end": 284.88, "text": " if I run that, and I'll just call it with the time function, and I'm going to say this is my", "tokens": [50656, 498, 286, 1190, 300, 11, 293, 286, 603, 445, 818, 309, 365, 264, 565, 2445, 11, 293, 286, 478, 516, 281, 584, 341, 307, 452, 50864], "temperature": 0.0, "avg_logprob": -0.09712134552001953, "compression_ratio": 1.7582417582417582, "no_speech_prob": 0.012741945683956146}, {"id": 56, "seek": 27488, "start": 284.88, "end": 293.68, "text": " newest laptop that has from memory six fast cores and eight slower cores, and it's going to run", "tokens": [50864, 17569, 10732, 300, 575, 490, 4675, 2309, 2370, 24826, 293, 3180, 14009, 24826, 11, 293, 309, 311, 516, 281, 1190, 51304], "temperature": 0.0, "avg_logprob": -0.09712134552001953, "compression_ratio": 1.7582417582417582, "no_speech_prob": 0.012741945683956146}, {"id": 57, "seek": 27488, "start": 293.68, "end": 296.88, "text": " randomly on those each time I do it. So some of the performance numbers are not going to be quite", "tokens": [51304, 16979, 322, 729, 1184, 565, 286, 360, 309, 13, 407, 512, 295, 264, 3389, 3547, 366, 406, 516, 281, 312, 1596, 51464], "temperature": 0.0, "avg_logprob": -0.09712134552001953, "compression_ratio": 1.7582417582417582, "no_speech_prob": 0.012741945683956146}, {"id": 58, "seek": 27488, "start": 296.88, "end": 301.6, "text": " as clear cut as I would like. So I'll try and explain it if I see that happening. So I run that,", "tokens": [51464, 382, 1850, 1723, 382, 286, 576, 411, 13, 407, 286, 603, 853, 293, 2903, 309, 498, 286, 536, 300, 2737, 13, 407, 286, 1190, 300, 11, 51700], "temperature": 0.0, "avg_logprob": -0.09712134552001953, "compression_ratio": 1.7582417582417582, "no_speech_prob": 0.012741945683956146}, {"id": 59, "seek": 30160, "start": 301.6, "end": 306.0, "text": " and it's taken that a tenth of a second to run. And if I make that number a lot bigger, so I make", "tokens": [50364, 293, 309, 311, 2726, 300, 257, 27269, 295, 257, 1150, 281, 1190, 13, 400, 498, 286, 652, 300, 1230, 257, 688, 3801, 11, 370, 286, 652, 50584], "temperature": 0.0, "avg_logprob": -0.10544799862051368, "compression_ratio": 1.8233082706766917, "no_speech_prob": 0.0009098079171963036}, {"id": 60, "seek": 30160, "start": 306.0, "end": 311.12, "text": " it an order of magnitude bigger, this for loop, it now takes a bit longer. And if I make it longer", "tokens": [50584, 309, 364, 1668, 295, 15668, 3801, 11, 341, 337, 6367, 11, 309, 586, 2516, 257, 857, 2854, 13, 400, 498, 286, 652, 309, 2854, 50840], "temperature": 0.0, "avg_logprob": -0.10544799862051368, "compression_ratio": 1.8233082706766917, "no_speech_prob": 0.0009098079171963036}, {"id": 61, "seek": 30160, "start": 311.12, "end": 316.8, "text": " again, we'll see depending on which style of core it's gone on, I think it's, yeah, it's roughly", "tokens": [50840, 797, 11, 321, 603, 536, 5413, 322, 597, 3758, 295, 4965, 309, 311, 2780, 322, 11, 286, 519, 309, 311, 11, 1338, 11, 309, 311, 9810, 51124], "temperature": 0.0, "avg_logprob": -0.10544799862051368, "compression_ratio": 1.8233082706766917, "no_speech_prob": 0.0009098079171963036}, {"id": 62, "seek": 30160, "start": 316.8, "end": 321.44, "text": " gone on the two slower cores. It's roughly as I make the loop run 10 times longer, the program is", "tokens": [51124, 2780, 322, 264, 732, 14009, 24826, 13, 467, 311, 9810, 382, 286, 652, 264, 6367, 1190, 1266, 1413, 2854, 11, 264, 1461, 307, 51356], "temperature": 0.0, "avg_logprob": -0.10544799862051368, "compression_ratio": 1.8233082706766917, "no_speech_prob": 0.0009098079171963036}, {"id": 63, "seek": 30160, "start": 321.44, "end": 325.36, "text": " taking roughly 10 times longer to run. So that's what we'd expect to see an interpreter. Now,", "tokens": [51356, 1940, 9810, 1266, 1413, 2854, 281, 1190, 13, 407, 300, 311, 437, 321, 1116, 2066, 281, 536, 364, 34132, 13, 823, 11, 51552], "temperature": 0.0, "avg_logprob": -0.10544799862051368, "compression_ratio": 1.8233082706766917, "no_speech_prob": 0.0009098079171963036}, {"id": 64, "seek": 32536, "start": 325.44, "end": 331.04, "text": " let's get rid of a couple of those zeros. And what I can do instead is use a different", "tokens": [50368, 718, 311, 483, 3973, 295, 257, 1916, 295, 729, 35193, 13, 400, 437, 286, 393, 360, 2602, 307, 764, 257, 819, 50648], "temperature": 0.0, "avg_logprob": -0.12138024648030599, "compression_ratio": 1.8609271523178808, "no_speech_prob": 0.0017497052904218435}, {"id": 65, "seek": 32536, "start": 331.04, "end": 335.04, "text": " implementation of Python. And this is one thing that we often confuse, we say Python,", "tokens": [50648, 11420, 295, 15329, 13, 400, 341, 307, 472, 551, 300, 321, 2049, 28584, 11, 321, 584, 15329, 11, 50848], "temperature": 0.0, "avg_logprob": -0.12138024648030599, "compression_ratio": 1.8609271523178808, "no_speech_prob": 0.0017497052904218435}, {"id": 66, "seek": 32536, "start": 335.04, "end": 338.96000000000004, "text": " when we mean the language, and there's Python, the language, and there's Python, the implementation,", "tokens": [50848, 562, 321, 914, 264, 2856, 11, 293, 456, 311, 15329, 11, 264, 2856, 11, 293, 456, 311, 15329, 11, 264, 11420, 11, 51044], "temperature": 0.0, "avg_logprob": -0.12138024648030599, "compression_ratio": 1.8609271523178808, "no_speech_prob": 0.0017497052904218435}, {"id": 67, "seek": 32536, "start": 338.96000000000004, "end": 344.56, "text": " I can use something called pi pi. And I think it has a jit. Let me turn the jit off. And hope that", "tokens": [51044, 286, 393, 764, 746, 1219, 3895, 3895, 13, 400, 286, 519, 309, 575, 257, 361, 270, 13, 961, 385, 1261, 264, 361, 270, 766, 13, 400, 1454, 300, 51324], "temperature": 0.0, "avg_logprob": -0.12138024648030599, "compression_ratio": 1.8609271523178808, "no_speech_prob": 0.0017497052904218435}, {"id": 68, "seek": 32536, "start": 344.56, "end": 349.36, "text": " I've not made it run too slow. So we can see that running. And I've still got a relatively large", "tokens": [51324, 286, 600, 406, 1027, 309, 1190, 886, 2964, 13, 407, 321, 393, 536, 300, 2614, 13, 400, 286, 600, 920, 658, 257, 7226, 2416, 51564], "temperature": 0.0, "avg_logprob": -0.12138024648030599, "compression_ratio": 1.8609271523178808, "no_speech_prob": 0.0017497052904218435}, {"id": 69, "seek": 32536, "start": 349.36, "end": 355.28000000000003, "text": " number. So it's about the same speed as the normal version of Python. Now we'll turn the jit", "tokens": [51564, 1230, 13, 407, 309, 311, 466, 264, 912, 3073, 382, 264, 2710, 3037, 295, 15329, 13, 823, 321, 603, 1261, 264, 361, 270, 51860], "temperature": 0.0, "avg_logprob": -0.12138024648030599, "compression_ratio": 1.8609271523178808, "no_speech_prob": 0.0017497052904218435}, {"id": 70, "seek": 35528, "start": 355.35999999999996, "end": 365.11999999999995, "text": " on. And it now runs in, well, that's a 10th of second, it has run two orders of magnitude faster.", "tokens": [50368, 322, 13, 400, 309, 586, 6676, 294, 11, 731, 11, 300, 311, 257, 1266, 392, 295, 1150, 11, 309, 575, 1190, 732, 9470, 295, 15668, 4663, 13, 50856], "temperature": 0.0, "avg_logprob": -0.12093299204908957, "compression_ratio": 1.6807017543859648, "no_speech_prob": 0.00035540579119697213}, {"id": 71, "seek": 35528, "start": 365.11999999999995, "end": 369.67999999999995, "text": " And in fact, I think we'll see if I've got this right, let's add another couple of zeros there,", "tokens": [50856, 400, 294, 1186, 11, 286, 519, 321, 603, 536, 498, 286, 600, 658, 341, 558, 11, 718, 311, 909, 1071, 1916, 295, 35193, 456, 11, 51084], "temperature": 0.0, "avg_logprob": -0.12093299204908957, "compression_ratio": 1.6807017543859648, "no_speech_prob": 0.00035540579119697213}, {"id": 72, "seek": 35528, "start": 369.67999999999995, "end": 374.55999999999995, "text": " something very big. It's actually doesn't really matter how big I make that loop, it's been able", "tokens": [51084, 746, 588, 955, 13, 467, 311, 767, 1177, 380, 534, 1871, 577, 955, 286, 652, 300, 6367, 11, 309, 311, 668, 1075, 51328], "temperature": 0.0, "avg_logprob": -0.12093299204908957, "compression_ratio": 1.6807017543859648, "no_speech_prob": 0.00035540579119697213}, {"id": 73, "seek": 35528, "start": 374.55999999999995, "end": 379.2, "text": " to observe it a runtime, realize it can just optimize the whole thing away. And that's the", "tokens": [51328, 281, 11441, 309, 257, 34474, 11, 4325, 309, 393, 445, 19719, 264, 1379, 551, 1314, 13, 400, 300, 311, 264, 51560], "temperature": 0.0, "avg_logprob": -0.12093299204908957, "compression_ratio": 1.6807017543859648, "no_speech_prob": 0.00035540579119697213}, {"id": 74, "seek": 35528, "start": 379.2, "end": 384.47999999999996, "text": " power of just in time compilation. It's looked at my runtime values and made the thing very fast.", "tokens": [51560, 1347, 295, 445, 294, 565, 40261, 13, 467, 311, 2956, 412, 452, 34474, 4190, 293, 1027, 264, 551, 588, 2370, 13, 51824], "temperature": 0.0, "avg_logprob": -0.12093299204908957, "compression_ratio": 1.6807017543859648, "no_speech_prob": 0.00035540579119697213}, {"id": 75, "seek": 38448, "start": 384.48, "end": 387.92, "text": " It's particularly effective on this sort of numeric code, but it will often work well on", "tokens": [50364, 467, 311, 4098, 4942, 322, 341, 1333, 295, 7866, 299, 3089, 11, 457, 309, 486, 2049, 589, 731, 322, 50536], "temperature": 0.0, "avg_logprob": -0.08336584411398337, "compression_ratio": 1.6903225806451614, "no_speech_prob": 0.0009189608390443027}, {"id": 76, "seek": 38448, "start": 387.92, "end": 392.32, "text": " things like strings and so on. And of course, there are some cases where it won't work. It", "tokens": [50536, 721, 411, 13985, 293, 370, 322, 13, 400, 295, 1164, 11, 456, 366, 512, 3331, 689, 309, 1582, 380, 589, 13, 467, 50756], "temperature": 0.0, "avg_logprob": -0.08336584411398337, "compression_ratio": 1.6903225806451614, "no_speech_prob": 0.0009189608390443027}, {"id": 77, "seek": 38448, "start": 392.32, "end": 395.92, "text": " isn't, as we said earlier, magic, but it is very effective in many cases.", "tokens": [50756, 1943, 380, 11, 382, 321, 848, 3071, 11, 5585, 11, 457, 309, 307, 588, 4942, 294, 867, 3331, 13, 50936], "temperature": 0.0, "avg_logprob": -0.08336584411398337, "compression_ratio": 1.6903225806451614, "no_speech_prob": 0.0009189608390443027}, {"id": 78, "seek": 38448, "start": 396.56, "end": 402.24, "text": " So as you scale up, is that still a benefit to using it, you know, when you've got a million", "tokens": [50968, 407, 382, 291, 4373, 493, 11, 307, 300, 920, 257, 5121, 281, 1228, 309, 11, 291, 458, 11, 562, 291, 600, 658, 257, 2459, 51252], "temperature": 0.0, "avg_logprob": -0.08336584411398337, "compression_ratio": 1.6903225806451614, "no_speech_prob": 0.0009189608390443027}, {"id": 79, "seek": 38448, "start": 402.24, "end": 408.8, "text": " lines of code, for instance? Good question. Very dependent on your program. In some sense,", "tokens": [51252, 3876, 295, 3089, 11, 337, 5197, 30, 2205, 1168, 13, 4372, 12334, 322, 428, 1461, 13, 682, 512, 2020, 11, 51580], "temperature": 0.0, "avg_logprob": -0.08336584411398337, "compression_ratio": 1.6903225806451614, "no_speech_prob": 0.0009189608390443027}, {"id": 80, "seek": 38448, "start": 408.8, "end": 413.6, "text": " actually, the scale of it is less important than the nature of your program. There's a", "tokens": [51580, 767, 11, 264, 4373, 295, 309, 307, 1570, 1021, 813, 264, 3687, 295, 428, 1461, 13, 821, 311, 257, 51820], "temperature": 0.0, "avg_logprob": -0.08336584411398337, "compression_ratio": 1.6903225806451614, "no_speech_prob": 0.0009189608390443027}, {"id": 81, "seek": 41360, "start": 413.6, "end": 418.40000000000003, "text": " fundamental assumption here that programs tend to do the same thing over and over again.", "tokens": [50364, 8088, 15302, 510, 300, 4268, 3928, 281, 360, 264, 912, 551, 670, 293, 670, 797, 13, 50604], "temperature": 0.0, "avg_logprob": -0.10227102087449658, "compression_ratio": 1.7796052631578947, "no_speech_prob": 9.72924754023552e-05}, {"id": 82, "seek": 41360, "start": 418.96000000000004, "end": 424.56, "text": " So in this case, Pi Pi is looking for loops. And it's what's called a tracing just in time", "tokens": [50632, 407, 294, 341, 1389, 11, 17741, 17741, 307, 1237, 337, 16121, 13, 400, 309, 311, 437, 311, 1219, 257, 25262, 445, 294, 565, 50912], "temperature": 0.0, "avg_logprob": -0.10227102087449658, "compression_ratio": 1.7796052631578947, "no_speech_prob": 9.72924754023552e-05}, {"id": 83, "seek": 41360, "start": 424.56, "end": 428.88, "text": " compiler. So it looks at what the loop is doing and optimizes that there's also method base,", "tokens": [50912, 31958, 13, 407, 309, 1542, 412, 437, 264, 6367, 307, 884, 293, 5028, 5660, 300, 456, 311, 611, 3170, 3096, 11, 51128], "temperature": 0.0, "avg_logprob": -0.10227102087449658, "compression_ratio": 1.7796052631578947, "no_speech_prob": 9.72924754023552e-05}, {"id": 84, "seek": 41360, "start": 428.88, "end": 431.6, "text": " which just look at a function, don't need to worry about the difference too much.", "tokens": [51128, 597, 445, 574, 412, 257, 2445, 11, 500, 380, 643, 281, 3292, 466, 264, 2649, 886, 709, 13, 51264], "temperature": 0.0, "avg_logprob": -0.10227102087449658, "compression_ratio": 1.7796052631578947, "no_speech_prob": 9.72924754023552e-05}, {"id": 85, "seek": 41360, "start": 432.88, "end": 436.72, "text": " If your loop or your function does the same thing repeatedly with only minor variations,", "tokens": [51328, 759, 428, 6367, 420, 428, 2445, 775, 264, 912, 551, 18227, 365, 787, 6696, 17840, 11, 51520], "temperature": 0.0, "avg_logprob": -0.10227102087449658, "compression_ratio": 1.7796052631578947, "no_speech_prob": 9.72924754023552e-05}, {"id": 86, "seek": 41360, "start": 436.72, "end": 441.92, "text": " this will be very effective. If you have a program, which every time it goes around the loop does", "tokens": [51520, 341, 486, 312, 588, 4942, 13, 759, 291, 362, 257, 1461, 11, 597, 633, 565, 309, 1709, 926, 264, 6367, 775, 51780], "temperature": 0.0, "avg_logprob": -0.10227102087449658, "compression_ratio": 1.7796052631578947, "no_speech_prob": 9.72924754023552e-05}, {"id": 87, "seek": 44192, "start": 441.92, "end": 446.96000000000004, "text": " something completely different or every time you call the method, then this will be less effective.", "tokens": [50364, 746, 2584, 819, 420, 633, 565, 291, 818, 264, 3170, 11, 550, 341, 486, 312, 1570, 4942, 13, 50616], "temperature": 0.0, "avg_logprob": -0.09245777997103605, "compression_ratio": 1.8223938223938223, "no_speech_prob": 0.0003376840613782406}, {"id": 88, "seek": 44192, "start": 446.96000000000004, "end": 450.88, "text": " And in some cases, then it will even slow things down because the program will never appear to", "tokens": [50616, 400, 294, 512, 3331, 11, 550, 309, 486, 754, 2964, 721, 760, 570, 264, 1461, 486, 1128, 4204, 281, 50812], "temperature": 0.0, "avg_logprob": -0.09245777997103605, "compression_ratio": 1.8223938223938223, "no_speech_prob": 0.0003376840613782406}, {"id": 89, "seek": 44192, "start": 450.88, "end": 458.16, "text": " stabilize. And that's really what your expecting programs do is that they typically, when they", "tokens": [50812, 31870, 13, 400, 300, 311, 534, 437, 428, 9650, 4268, 360, 307, 300, 436, 5850, 11, 562, 436, 51176], "temperature": 0.0, "avg_logprob": -0.09245777997103605, "compression_ratio": 1.8223938223938223, "no_speech_prob": 0.0003376840613782406}, {"id": 90, "seek": 44192, "start": 458.16, "end": 462.40000000000003, "text": " start up, they do some initialization, that's all a bit random. And then they tend to hit", "tokens": [51176, 722, 493, 11, 436, 360, 512, 5883, 2144, 11, 300, 311, 439, 257, 857, 4974, 13, 400, 550, 436, 3928, 281, 2045, 51388], "temperature": 0.0, "avg_logprob": -0.09245777997103605, "compression_ratio": 1.8223938223938223, "no_speech_prob": 0.0003376840613782406}, {"id": 91, "seek": 44192, "start": 462.40000000000003, "end": 465.6, "text": " some sort of main part where they do the same thing over and over and over again. And that's", "tokens": [51388, 512, 1333, 295, 2135, 644, 689, 436, 360, 264, 912, 551, 670, 293, 670, 293, 670, 797, 13, 400, 300, 311, 51548], "temperature": 0.0, "avg_logprob": -0.09245777997103605, "compression_ratio": 1.8223938223938223, "no_speech_prob": 0.0003376840613782406}, {"id": 92, "seek": 46560, "start": 465.6, "end": 471.84000000000003, "text": " where JIT compilation really comes to the fore. And our process is doing a bit of that anyway.", "tokens": [50364, 689, 508, 3927, 40261, 534, 1487, 281, 264, 2091, 13, 400, 527, 1399, 307, 884, 257, 857, 295, 300, 4033, 13, 50676], "temperature": 0.0, "avg_logprob": -0.17792205457334165, "compression_ratio": 1.6313868613138687, "no_speech_prob": 0.016813920810818672}, {"id": 93, "seek": 46560, "start": 473.36, "end": 479.04, "text": " Yeah, so I think of modern processes as basically a just in time compiler. I write machine code and", "tokens": [50752, 865, 11, 370, 286, 519, 295, 4363, 7555, 382, 1936, 257, 445, 294, 565, 31958, 13, 286, 2464, 3479, 3089, 293, 51036], "temperature": 0.0, "avg_logprob": -0.17792205457334165, "compression_ratio": 1.6313868613138687, "no_speech_prob": 0.016813920810818672}, {"id": 94, "seek": 46560, "start": 479.04, "end": 482.32000000000005, "text": " okay, I write the textual form and it looks like ARM or x86 or whatever.", "tokens": [51036, 1392, 11, 286, 2464, 264, 2487, 901, 1254, 293, 309, 1542, 411, 45209, 420, 2031, 22193, 420, 2035, 13, 51200], "temperature": 0.0, "avg_logprob": -0.17792205457334165, "compression_ratio": 1.6313868613138687, "no_speech_prob": 0.016813920810818672}, {"id": 95, "seek": 46560, "start": 483.12, "end": 489.12, "text": " That's not what the processor eventually executes. It turns that into some other representation. It", "tokens": [51240, 663, 311, 406, 437, 264, 15321, 4728, 4454, 1819, 13, 467, 4523, 300, 666, 512, 661, 10290, 13, 467, 51540], "temperature": 0.0, "avg_logprob": -0.17792205457334165, "compression_ratio": 1.6313868613138687, "no_speech_prob": 0.016813920810818672}, {"id": 96, "seek": 46560, "start": 489.12, "end": 494.08000000000004, "text": " does all sorts of clever optimizations. If you ever want to see things like the", "tokens": [51540, 775, 439, 7527, 295, 13494, 5028, 14455, 13, 759, 291, 1562, 528, 281, 536, 721, 411, 264, 51788], "temperature": 0.0, "avg_logprob": -0.17792205457334165, "compression_ratio": 1.6313868613138687, "no_speech_prob": 0.016813920810818672}, {"id": 97, "seek": 49560, "start": 496.08000000000004, "end": 502.24, "text": " processor optimizes program reorders it like the reorder buffer, they're scary at how clever", "tokens": [50388, 15321, 5028, 5660, 1461, 319, 10400, 309, 411, 264, 319, 4687, 21762, 11, 436, 434, 6958, 412, 577, 13494, 50696], "temperature": 0.0, "avg_logprob": -0.11767040093739828, "compression_ratio": 1.6746575342465753, "no_speech_prob": 0.00013114471221342683}, {"id": 98, "seek": 49560, "start": 502.24, "end": 506.08000000000004, "text": " they are. And that's why they've got a lot faster, even though the gigahertz part hasn't changed too", "tokens": [50696, 436, 366, 13, 400, 300, 311, 983, 436, 600, 658, 257, 688, 4663, 11, 754, 1673, 264, 8741, 64, 35655, 644, 6132, 380, 3105, 886, 50888], "temperature": 0.0, "avg_logprob": -0.11767040093739828, "compression_ratio": 1.6746575342465753, "no_speech_prob": 0.00013114471221342683}, {"id": 99, "seek": 49560, "start": 506.08000000000004, "end": 511.68, "text": " much. And there's been a little bit of knowledge transfer between the processor JIT world and the", "tokens": [50888, 709, 13, 400, 456, 311, 668, 257, 707, 857, 295, 3601, 5003, 1296, 264, 15321, 508, 3927, 1002, 293, 264, 51168], "temperature": 0.0, "avg_logprob": -0.11767040093739828, "compression_ratio": 1.6746575342465753, "no_speech_prob": 0.00013114471221342683}, {"id": 100, "seek": 49560, "start": 511.68, "end": 516.08, "text": " programming language JIT world. Is this a new thing or how long have these kind of just in time", "tokens": [51168, 9410, 2856, 508, 3927, 1002, 13, 1119, 341, 257, 777, 551, 420, 577, 938, 362, 613, 733, 295, 445, 294, 565, 51388], "temperature": 0.0, "avg_logprob": -0.11767040093739828, "compression_ratio": 1.6746575342465753, "no_speech_prob": 0.00013114471221342683}, {"id": 101, "seek": 49560, "start": 516.08, "end": 520.96, "text": " compilers been around them? They have been around in one form or another for a while, but they really", "tokens": [51388, 715, 388, 433, 668, 926, 552, 30, 814, 362, 668, 926, 294, 472, 1254, 420, 1071, 337, 257, 1339, 11, 457, 436, 534, 51632], "temperature": 0.0, "avg_logprob": -0.11767040093739828, "compression_ratio": 1.6746575342465753, "no_speech_prob": 0.00013114471221342683}, {"id": 102, "seek": 52096, "start": 521.0400000000001, "end": 526.24, "text": " trace their modern lineage back to the 1980s in a language called self, which has been largely", "tokens": [50368, 13508, 641, 4363, 38257, 646, 281, 264, 13626, 82, 294, 257, 2856, 1219, 2698, 11, 597, 575, 668, 11611, 50628], "temperature": 0.0, "avg_logprob": -0.12980894211235397, "compression_ratio": 1.7925925925925925, "no_speech_prob": 0.004655489698052406}, {"id": 103, "seek": 52096, "start": 526.24, "end": 532.88, "text": " forgotten a really interesting language that had a just in time compiler via a long sequence of", "tokens": [50628, 11832, 257, 534, 1880, 2856, 300, 632, 257, 445, 294, 565, 31958, 5766, 257, 938, 8310, 295, 50960], "temperature": 0.0, "avg_logprob": -0.12980894211235397, "compression_ratio": 1.7925925925925925, "no_speech_prob": 0.004655489698052406}, {"id": 104, "seek": 52096, "start": 532.88, "end": 537.6800000000001, "text": " events that ended up going to the company called son and is then formed the basis and literally", "tokens": [50960, 3931, 300, 4590, 493, 516, 281, 264, 2237, 1219, 1872, 293, 307, 550, 8693, 264, 5143, 293, 3736, 51200], "temperature": 0.0, "avg_logprob": -0.12980894211235397, "compression_ratio": 1.7925925925925925, "no_speech_prob": 0.004655489698052406}, {"id": 105, "seek": 52096, "start": 537.6800000000001, "end": 543.44, "text": " some of the code is still there in the Java virtual machine. So the Java JIT traces itself back to", "tokens": [51200, 512, 295, 264, 3089, 307, 920, 456, 294, 264, 10745, 6374, 3479, 13, 407, 264, 10745, 508, 3927, 26076, 2564, 646, 281, 51488], "temperature": 0.0, "avg_logprob": -0.12980894211235397, "compression_ratio": 1.7925925925925925, "no_speech_prob": 0.004655489698052406}, {"id": 106, "seek": 52096, "start": 543.44, "end": 549.9200000000001, "text": " self. V8, the JavaScript VM in Chrome also traces its lineage back to the Java virtual machine hot", "tokens": [51488, 2698, 13, 691, 23, 11, 264, 15778, 18038, 294, 15327, 611, 26076, 1080, 38257, 646, 281, 264, 10745, 6374, 3479, 2368, 51812], "temperature": 0.0, "avg_logprob": -0.12980894211235397, "compression_ratio": 1.7925925925925925, "no_speech_prob": 0.004655489698052406}, {"id": 107, "seek": 54992, "start": 549.92, "end": 556.3199999999999, "text": " spot and back to self. So really, that's been those have been the big movers. And now you've", "tokens": [50364, 4008, 293, 646, 281, 2698, 13, 407, 534, 11, 300, 311, 668, 729, 362, 668, 264, 955, 705, 840, 13, 400, 586, 291, 600, 50684], "temperature": 0.0, "avg_logprob": -0.1545591915355009, "compression_ratio": 1.5918367346938775, "no_speech_prob": 0.0014712767442688346}, {"id": 108, "seek": 54992, "start": 556.3199999999999, "end": 562.7199999999999, "text": " got systems like Pi Pi and V8 and spider monkey that have modernized the concept or spread it to", "tokens": [50684, 658, 3652, 411, 17741, 17741, 293, 691, 23, 293, 17614, 17847, 300, 362, 4363, 1602, 264, 3410, 420, 3974, 309, 281, 51004], "temperature": 0.0, "avg_logprob": -0.1545591915355009, "compression_ratio": 1.5918367346938775, "no_speech_prob": 0.0014712767442688346}, {"id": 109, "seek": 54992, "start": 562.7199999999999, "end": 567.28, "text": " more languages will probably be a better way of putting it. And is this, you know, obviously", "tokens": [51004, 544, 8650, 486, 1391, 312, 257, 1101, 636, 295, 3372, 309, 13, 400, 307, 341, 11, 291, 458, 11, 2745, 51232], "temperature": 0.0, "avg_logprob": -0.1545591915355009, "compression_ratio": 1.5918367346938775, "no_speech_prob": 0.0014712767442688346}, {"id": 110, "seek": 54992, "start": 567.28, "end": 571.04, "text": " traces it through it's quite a long way back. But is it only really being used now because", "tokens": [51232, 26076, 309, 807, 309, 311, 1596, 257, 938, 636, 646, 13, 583, 307, 309, 787, 534, 885, 1143, 586, 570, 51420], "temperature": 0.0, "avg_logprob": -0.1545591915355009, "compression_ratio": 1.5918367346938775, "no_speech_prob": 0.0014712767442688346}, {"id": 111, "seek": 54992, "start": 571.04, "end": 577.1999999999999, "text": " machines have got that much faster? Yeah, I think there's an element of that. Because for you,", "tokens": [51420, 8379, 362, 658, 300, 709, 4663, 30, 865, 11, 286, 519, 456, 311, 364, 4478, 295, 300, 13, 1436, 337, 291, 11, 51728], "temperature": 0.0, "avg_logprob": -0.1545591915355009, "compression_ratio": 1.5918367346938775, "no_speech_prob": 0.0014712767442688346}, {"id": 112, "seek": 57720, "start": 577.2, "end": 582.1600000000001, "text": " when I was a kid, you could burn your computer every 18 months, and it was twice as fast. And", "tokens": [50364, 562, 286, 390, 257, 1636, 11, 291, 727, 5064, 428, 3820, 633, 2443, 2493, 11, 293, 309, 390, 6091, 382, 2370, 13, 400, 50612], "temperature": 0.0, "avg_logprob": -0.09139497756958008, "compression_ratio": 1.5902777777777777, "no_speech_prob": 0.003257752861827612}, {"id": 113, "seek": 57720, "start": 582.96, "end": 587.44, "text": " the death of single core performance is a little exaggerated, partly because the processes are", "tokens": [50652, 264, 2966, 295, 2167, 4965, 3389, 307, 257, 707, 36196, 11, 17031, 570, 264, 7555, 366, 50876], "temperature": 0.0, "avg_logprob": -0.09139497756958008, "compression_ratio": 1.5902777777777777, "no_speech_prob": 0.003257752861827612}, {"id": 114, "seek": 57720, "start": 587.44, "end": 592.6400000000001, "text": " now doing just in time compilation sorts of things. But yeah, we definitely are looking", "tokens": [50876, 586, 884, 445, 294, 565, 40261, 7527, 295, 721, 13, 583, 1338, 11, 321, 2138, 366, 1237, 51136], "temperature": 0.0, "avg_logprob": -0.09139497756958008, "compression_ratio": 1.5902777777777777, "no_speech_prob": 0.003257752861827612}, {"id": 115, "seek": 57720, "start": 592.6400000000001, "end": 597.5200000000001, "text": " increasingly to programming languages to work faster and for many languages, particularly,", "tokens": [51136, 12980, 281, 9410, 8650, 281, 589, 4663, 293, 337, 867, 8650, 11, 4098, 11, 51380], "temperature": 0.0, "avg_logprob": -0.09139497756958008, "compression_ratio": 1.5902777777777777, "no_speech_prob": 0.003257752861827612}, {"id": 116, "seek": 57720, "start": 597.5200000000001, "end": 602.8000000000001, "text": " but not only those that are dynamically typed like Python or Java. This is really the only", "tokens": [51380, 457, 406, 787, 729, 300, 366, 43492, 33941, 411, 15329, 420, 10745, 13, 639, 307, 534, 264, 787, 51644], "temperature": 0.0, "avg_logprob": -0.09139497756958008, "compression_ratio": 1.5902777777777777, "no_speech_prob": 0.003257752861827612}, {"id": 117, "seek": 60280, "start": 602.8, "end": 607.8399999999999, "text": " effective technique. And that's why you've seen increasing numbers of them being released for", "tokens": [50364, 4942, 6532, 13, 400, 300, 311, 983, 291, 600, 1612, 5662, 3547, 295, 552, 885, 4736, 337, 50616], "temperature": 0.0, "avg_logprob": -0.12007616116450383, "compression_ratio": 1.638783269961977, "no_speech_prob": 0.00986004900187254}, {"id": 118, "seek": 60280, "start": 607.8399999999999, "end": 612.4799999999999, "text": " more and more languages, despite the fact that they're really complicated and expensive to create.", "tokens": [50616, 544, 293, 544, 8650, 11, 7228, 264, 1186, 300, 436, 434, 534, 6179, 293, 5124, 281, 1884, 13, 50848], "temperature": 0.0, "avg_logprob": -0.12007616116450383, "compression_ratio": 1.638783269961977, "no_speech_prob": 0.00986004900187254}, {"id": 119, "seek": 60280, "start": 612.4799999999999, "end": 615.4399999999999, "text": " You know, these are not the sort of things you can knock out in an afternoon, they take", "tokens": [50848, 509, 458, 11, 613, 366, 406, 264, 1333, 295, 721, 291, 393, 6728, 484, 294, 364, 6499, 11, 436, 747, 50996], "temperature": 0.0, "avg_logprob": -0.12007616116450383, "compression_ratio": 1.638783269961977, "no_speech_prob": 0.00986004900187254}, {"id": 120, "seek": 60280, "start": 616.24, "end": 619.12, "text": " some big teams, many years to create in most cases.", "tokens": [51036, 512, 955, 5491, 11, 867, 924, 281, 1884, 294, 881, 3331, 13, 51180], "temperature": 0.0, "avg_logprob": -0.12007616116450383, "compression_ratio": 1.638783269961977, "no_speech_prob": 0.00986004900187254}, {"id": 121, "seek": 60280, "start": 621.76, "end": 627.1999999999999, "text": " Is train a network to undo this process? That's the idea. And if we can do that, then we can start", "tokens": [51312, 1119, 3847, 257, 3209, 281, 23779, 341, 1399, 30, 663, 311, 264, 1558, 13, 400, 498, 321, 393, 360, 300, 11, 550, 321, 393, 722, 51584], "temperature": 0.0, "avg_logprob": -0.12007616116450383, "compression_ratio": 1.638783269961977, "no_speech_prob": 0.00986004900187254}, {"id": 122, "seek": 62720, "start": 627.2800000000001, "end": 630.5600000000001, "text": " with random noise, a bit like our GAN, and we can just iterate this process.", "tokens": [50368, 365, 4974, 5658, 11, 257, 857, 411, 527, 460, 1770, 11, 293, 321, 393, 445, 44497, 341, 1399, 13, 50532], "temperature": 0.0, "avg_logprob": -0.20794365769725734, "compression_ratio": 1.303448275862069, "no_speech_prob": 0.09496457129716873}, {"id": 123, "seek": 62720, "start": 630.5600000000001, "end": 639.2800000000001, "text": " Four dice. Die A, B, C and D. And I tell you that die A has a value four. How much did you learn", "tokens": [50532, 7451, 10313, 13, 3229, 316, 11, 363, 11, 383, 293, 413, 13, 400, 286, 980, 291, 300, 978, 316, 575, 257, 2158, 1451, 13, 1012, 709, 630, 291, 1466, 50968], "temperature": 0.0, "avg_logprob": -0.20794365769725734, "compression_ratio": 1.303448275862069, "no_speech_prob": 0.09496457129716873}, {"id": 124, "seek": 62720, "start": 639.2800000000001, "end": 639.9200000000001, "text": " about the data?", "tokens": [50968, 466, 264, 1412, 30, 51000], "temperature": 0.0, "avg_logprob": -0.20794365769725734, "compression_ratio": 1.303448275862069, "no_speech_prob": 0.09496457129716873}], "language": "en"}