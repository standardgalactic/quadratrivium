start	end	text
0	6520	Last week, I think it was maybe the week before Apple had one of their usual press conferences and they announced their latest
6960	12560	Possibly last version of the M1 chip, which was the M1 Ultra
12560	20000	And one of the things that they said as they launched it was that they designed it using two M1 Max chips
20520	27440	Basically stuck together using something called Ultra Fusion to join them together. Now, Ultra Fusion is just a marketing buzzword
28440	34400	Literally all they've got is a high-speed interconnect between the two silicon dyes to transfer data between them
34400	38480	But one of the things that they said which was interesting is that the reason they've done this
38920	42640	Was so that you didn't have to write the software in a different way
42640	45960	And I thought it was interesting just to pick up on that and to explain
46360	49640	Why if they hadn't made that interconnect fast enough
49920	53640	You would have to write the software in a different way because if you think about it
53760	60440	All they seem to be doing is adding more cores to the CPU making it a 20-core CPU instead of a 10-core CPU
60440	63240	And you think well if it's a multiprocessor system
63240	66800	And if you watch the videos we've done previously on multiprocessor systems
67000	71480	You're going to have to write the software to split the tasks up over the multiple cores to run
71600	77520	So why are you not going to have to write things differently with this architecture of chip?
77520	79520	So I thought we'd have a look at that today
79840	87200	So to understand what Apple's done, we need to go back to basics and think about how a computer
87720	89960	Actually works and we'll go with the von Neumann model
89960	93880	I know technically most modern CPUs are modified Harvard architecture
93880	98760	But the von Neumann model is good for what we want to look at. We have at the center of our system
99120	100800	the CPU
100800	103840	Whatever we want and that is then connected
104720	109040	To some memory and I'm just going to write RAM here
109040	110200	So it fits into the box
110200	115880	Of course some of it would be ROM and other things and then the other thing that we have in there is we have the IO
116400	121000	And things and that's basically the model we use for a computer
121000	127360	We've got the CPU talking to the RAM where the instructions and data are stored and you can talk to the IO to talk to the rest of the world
127360	128560	So that's things like your
128560	133480	Disk controllers with a solid-state hard disk your graphics card your network card now what happens?
133960	138760	When we have a multi-processor system the general way that we build multi-processor systems
138760	144640	Certainly the ones that we use in laptops or using desktop computers is using what's called a shared memory model
144760	147800	So just as before with the von Neumann architecture
148080	153160	We're going to have a single block of RAM and that's going to be connected
153800	155960	Not to one CPU now
157160	160640	But we'll give it to CPUs. So we've got two
161640	168440	CPUs that it's connected to so it's connected to a shared bus between them and then each of those CPUs are
169040	174520	Connected to it now effectively. That's how you build a multi-processor system. There's a bit more
175280	178320	Involved for example, you need some sort of logic here
178960	181040	for bus
181040	183840	Arbitrations will call that the ball the bus arbitration logic
183840	189940	So you need something to sort of control well, which CPU can talk to the RAM at any one point now
190640	195400	One thing I need to say here is that I've drawn this is the CPU talking directly to the RAM
195400	200280	If you think about it, if you watch the video I did many years ago on CPU caches
200640	202880	You need to have a cache here because otherwise
203640	206240	Only one CPU can ever talk to RAM at the same time
206240	211080	If there's no cache this CPU tries to talk to RAM this one can't
211440	214680	If that's this CPU tries to talk to RAM that one can't at the same time
214680	215880	It would effectively
215880	221600	Result in serializing the operation so you wouldn't get any speed up. You need a cache in there and that
221880	228080	Sort of leads us to the first part of the problem only one CPU can access the RAM at any one point
228080	231520	Now if we've got a cache in our system and I'm going to draw that as a red line
231840	235480	Which sits between the CPU and between the RAM?
235640	238640	That's not a problem because as a CPU accesses data
238840	241080	It stores a local copy in its cache
241080	248040	So when it needs to try and fetch that data or those instructions again it can fetch them from the cache and not access the RAM
248800	250440	So that's absolutely fine
250440	256360	Most of the time we want to get it so the CPUs are satisfying their data and instruction fetches from the cache
256360	258720	And then only occasionally they go to the RAM
258840	264360	So that actually whenever one of the CPU goes to the RAM needs to go to the main memory to fetch a value
264720	266280	Then effectively
266280	272760	It's unlikely to be being used by the occasion that you'll get the situation where they both try and access a value main memory at
272760	278920	The same time at which point that's why you have the bus arbitration logic to say this CPU is going to fetch the value
278920	281160	Then that CPU is going to fetch the value
281800	286200	So we can build a shared memory multiprocessor system like that
286840	290360	I'm going to say relatively straightforwardly. There's a lot involved, but
290920	296120	That's the basic idea of what's going on and we can extend that to have more CPUs
296120	298120	So we can just add another CPU in
298600	301400	Up here so we could have a three CPU system
301800	304360	Normally you'd probably go up to four and things but I've run out of paper
304760	310640	It's got its cache as well and you could extend that for as many CPUs as you like
311720	313000	except
313000	314920	There was a slight issue
314920	316920	we said that
317640	323400	There are occasions where one CPU might be trying to access the memory at the same time as another CPU
324440	326360	Hopefully we can build the cache system
326360	331880	We can load more data than we need each time we fetch things and so on we can build an intelligent memory system that can
332440	336920	Satisfy this so that the probability of that happening is relatively low
338040	340920	But if we think about it if we add more and more CPUs
341560	343560	onto the same shared memory bus
344520	347640	Then we're going to end up with more chance
348280	352920	Of a collision happening of two CPUs trying to access memory at the same time
353240	358680	And the caches on each CPU mitigate that to some effect so that they reduce the probability
359160	361240	Of two things trying to access at a time
362120	365800	But a bit like the old birthday problem, you know the sort of question you ask
366120	367480	If you've got a class of school children
367480	370360	What is the probability that two of them share a birthday in there?
370600	374600	Turns out it's quite likely once you get a bit of about 20 or so children in the class
375240	381640	The same thing applies here as you increase more CPUs the chance that two of them will try and access memory at the same time
382280	383560	increases
383560	386520	As you add more CPUs and so this will scale
386920	391320	Scale, but it'll only scale up to a point once you get past a certain number of CPUs
391960	394360	You will find that you're back to the point where actually more
394920	399560	It's more likely than not that two of them will be trying to access memory at the same time
400440	403880	So we can scale this up to a certain number of CPUs
404360	409320	So does that form a limit? Is there a limit on how many CPUs we can have working together in the multiprocessor system?
410440	413160	Well, not as such because there's another way
414040	416440	We can design a multiprocessor system
416760	419880	So this is what's known as a uniform memory access system
420280	422680	And the reason it's known is that is that for any location
423560	425560	in RAM
425800	427800	Any of these CPUs can access it
428360	433640	With the same sort of speed so it doesn't matter whether it's coming from CPU one up here or CPU three down here
433960	437000	It'll take the same amount of time for them to access the value
437560	439560	In that memory location
439560	443560	Different memory locations may have different speeds your ROM might be slower than the RAM
444120	446840	You may have things mapped in there which are slower still and so on
447000	453800	But for any particular memory location each CPU can access it in the same time or within the same nanosecond ballpark
453800	459720	So it makes no difference in reality as we said that will scale up to a certain number of CPUs
459720	461720	But if we want to take it to beyond that
462040	466760	Then we need to change that system. We need to build a system that no longer has
467400	473240	Uniform memory access rather than from a memory location each CPU being able to access at the same speed
473800	479080	For each memory location the speed it takes to access it or how long it takes for it to access the data value there
479560	486120	Depends on which CPU core is trying to access it. So it might be that for one CPU core it takes
486840	488840	I don't know let's say
488920	491400	100 nanoseconds just picking a time off the top of my head
491880	495080	But for another CPU core it takes 200 nanoseconds
495320	498920	They're just ballpark times and not all the magnitudes are just sort of show there's a difference
499560	505800	Between the two. Okay, let's have a look at how we build a system like that. So what we're talking about is what's referred to
506760	508280	as a non
508280	513160	Uniform memory access system. So non uniform memory access
514040	515800	system or Numa
515800	521160	For short. So how does that differ? Well, let's think about it. It starts off in the same way. We have a block of ram
522760	524760	I'm going to turn the diagram around
525480	527960	Ram like that and that is connected
528520	535400	To our CPUs just as before I'm missing out the caches and the arbitration logic from this diagram just for simplicity
535480	540040	So this looks relatively similar to what we had before we've got a some ram and some CPU cores
540440	546680	Sharing access to it. No difference there with a Numa system though. We also have some other ram
547400	552360	That's part of our system connected to a different set of CPU cores
552920	558200	Over here now at this point you've got effectively two computer systems. These CPUs can access this RAM
558520	560520	These CPUs can access this RAM
560600	563960	The difference in the Numa system is that there is actually a link
564680	566680	between the two
566760	568760	Systems here and you've got a distributed
569320	573400	Shared memory system think of it like a sort of network, but it's often done at
574040	577880	The CPU level and things even within on some between cores
578360	582760	Now what this means is as far as the program's running there is one block of memory
582840	584840	There is so this is if this was 16 gig
585480	590920	And this was 16 gig the programs would see 32 gigabytes. They're not separate blocks of memory
591560	594360	They they're seen by the programs as one block of memory
595080	599480	But the difference is is if we've got a program running on this CPU over here
600920	603800	It's got direct access to this block of memory here
604520	607080	So let's say it takes I don't let's say it takes
607640	609880	100 nanoseconds again to access memory
610360	613560	So we've got 100 nanoseconds to access memory if he wants to access memory in here
614120	617720	It will take 100 nanoseconds to access that memory value
617720	623320	But if the data it's trying to access is over in this memory over here
625080	628120	A CPU over here could access it in 100 nanoseconds
628600	631000	But for this CPU over here a CPU over here
631560	634360	It's got to go over this distributed shared memory connection
634920	640440	From this set of RAM and this set of CPUs to this set of RAM and this set of CPU cores over here
640920	642920	And that would take
643000	647160	A significant amount of time. I mean it would take 100 nanoseconds over here
647720	649720	To get from here to here
649720	651720	So to get from here to here plus this
652360	656280	Let's say this is 200 nanoseconds. I'm just making a number up over here. It's a longer
657000	658280	amount of time
658280	659880	I'm making these numbers off of the top of my head
659880	663880	So don't take them as any sort of things other than so it's longer to go from here over here
664040	666040	So we'd have to go over here
666200	668200	across the distributed shared memory link
669080	671560	To get the value and then we could bring the value back
671560	676120	So rather than taking 100 nanoseconds it would take in the order of 300 nanoseconds
676200	680760	It would take a significantly longer amount of time. So if we build a computer system like this
681000	683000	We have the situation where
683080	687720	Depending where an instruction is in memory or where data is in memory
688360	694840	It could either access it very very quickly on this CPU core if it can go directly to the RAM that it's directly connected to
695480	697480	Or it would end up taking a long time
698280	701960	This relatively to access it because it would have to go over the shared link
702360	705560	And fetch it from the other block of RAM over there
705960	708280	It would still appear to be the same memory system
708760	712680	But we've now got the situation where the access to it depends
713240	718680	On which cpu is trying to access it. So we have what's called a non uniform memory access system
718840	721560	Now originally non uniform memory access systems were the sort of
721960	727240	Domain of high-end cluster system and sort of sgi type workstations and things but these days
727320	730680	You've actually seen it drop down on to sort of workstation type machines
731000	734360	Some of the amd thread ripper some of the higher end intel clusters are all
735000	741720	Numer based systems and what this means is if you want to run that cpu at the fastest possible speed
742360	745720	You need to write your software to take into account
746360	749160	Which cpus have fast access to which bits of ram
749640	754440	So that you can put the data that those cpus are processing and you can put the instructions that they're running
754920	762040	In that block of memory and have the cpu date the cpu instructions and the data that's being executed on these cpus
762600	764360	In this block of memory over here
764360	768440	So they can all access it very very quickly and you only have a very small amount of data
768760	773720	Which is needed to synchronize things and keep things working passing over the shared memory network
774520	776520	Now you can do it and it works great
776840	780920	But you have to write your software knowing where things are and in fact if you look
781240	784280	You can find papers and presentations from companies like netflix
784520	789160	Where they're really trying to optimize the performance of their service to serve the videos to you
789400	792840	I'm sure youtube's doing the same as well, but netflix have actually written about it
793160	796120	Really optimized the speed of serving the videos to you
796520	801000	So they actually have to take into all this account so that the network card is connected to one
801720	802840	cpu
802840	807720	gets that data and doesn't have to go and pass it over the shared memory link to another one which then passes it over
808200	810200	to another one to fetch the data from a
811240	816040	Hard disk and so on and feed it back to you and you get everything's passing over the slow link all the time
816040	818040	You really have to take into account
818040	820040	where things are
820200	824520	Which brings us back to apple's marketing buzzword of m1 ultrafusion
825160	827880	What have apple done with m1 ultrafusion?
828360	833560	Well, effectively they have built a system like this. They've taken two m1 max chips
834040	837880	And glued them together. So you've got two 10 core m1 max chips
838040	841000	Each hacks to sing their own blocks of memory or two blocks each
841000	846200	Which is why you can get up to 128 gig on there because you double the amount of cpu cause you can double the amount of memory
846760	852120	That they can access and what they've built in the middle the thing they call ultrafusion
852120	855960	It's just a very very fast distributed shared memory link between the two
856520	859320	And I think what they've actually done is they've just made it so fast
859880	865640	That actually the time it takes to go across from one cpu core to the other to get the value from the ram and
866200	870360	Push it back into the cpu is so quick the latency is so low
870920	875240	That effectively it behaves as if it was the uniform memory access system
875240	878040	It's fast enough that when the cpu requests the data
878440	882120	It gets it before it actually needs it in which point it doesn't slow it down
882760	887160	So it's a nice system because it means as a programmer. We don't have to worry about where
887800	891320	The data is in relation to the cpu cause which one's attached to which
891720	894760	Core and things to make things run as fast as possible
895160	898920	We can just write our programs and let the operating system and the design of the hardware
899320	902840	Sort out the hard problems of executing it as fast as possible
904520	907000	Chunks and do them all at the same time
907640	913400	So one way for example to make sandwiches faster is that you butter the bread faster
913400	915880	You put the filling in faster. You put the bread faster the other person
915880	920440	It's also got an analysis of where I went wrong says Fred Brooks
920440	923480	Why did he make his name with that and what was it all about?
