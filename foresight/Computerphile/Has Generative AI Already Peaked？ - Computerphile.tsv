start	end	text
0	5360	So we looked at clip embeddings and we've talked a lot about using generative AI to produce new
5360	10160	sentences to produce new images and so on and so to understand images all these kind of different
10160	17840	things and the idea was that if we look at enough pairs of images and text we will learn to distill
17840	22400	what it is in an image into that kind of language. So the idea is you have an image, you have some
22400	26800	text and you can find a representation where they're both the same. The argument has gone that
26800	31520	it's only a matter of time before we have so many images that we train this on and such a big
31520	35680	network and all this kind of business that we get this kind of general intelligence or we get some
35680	42160	kind of extremely effective AI that works across all domains. That's the implication. The argument
42160	48000	is and you see a lot in the sort of tech sector from some of these sort of big tech companies
48000	54720	who, to be fair, want to sell products, that if you just keep adding more and more data or bigger
54720	59840	and bigger models or a combination of both, ultimately you will move beyond just recognizing
59840	64240	cats and you'll be able to do anything. That's the idea. You show enough cats and dogs and eventually
64240	71920	the elephant just is implied. As someone who works in science we don't hypothesize about what happens,
71920	78480	we experimentally justify it. So I would say if you're going to say to me that the only upward
78480	84640	trajectory is going to be amazing, I would say go on and prove it and do it and then we'll
84720	88160	see. We'll sit here for a couple of years and we'll see what happens. But in the meantime,
88160	93840	let's look at this paper which came out just recently. This paper is saying that that is not
93840	99840	true. This paper is saying that the amount of data you will need to get that kind of general
99840	106160	zero-shot performance, that is to say performance on new tasks that you've never seen, is going to be
106160	111680	astronomically vast to the point where we cannot do it. That's the idea. So it basically is arguing
111680	119280	against the idea that we can just add more data and more models and we'll solve it. Now this is
119280	124240	only one paper and of course your mileage may vary if you have a bigger GPU than these people and so
124240	129200	on. But I think that this is actual numbers which is what I like because I want to see tables of
129200	133520	data that show a trend actually happening or not happening. I think that's much more interesting
133520	138560	than someone's blog post that says I think this is going to what's going to happen. So let's talk
138560	143360	about what this paper does and why it's interesting. We have clip embeddings. So we have an image,
143360	149040	we have a big vision transformer and we have a big text encoder which is another transformer,
149040	152720	bit like the sort of thing you would see in a large language model which takes text strings,
152720	157760	my text string today, and we have some shared embedded space and that embedded space is just
157760	162800	a numerical fingerprint for the meaning in these two items and they're trained remember across many,
162800	167520	many images such that when you put the same image and the text that describes that image in,
167520	171840	you get something in the middle that matches. And the idea then is you can use that for other tasks
171840	175840	like you can use that for classification, you can use it for image recall. If you use a streaming
175840	181280	service like Spotify or Netflix, they have this thing called a recommender system. A recommender
181280	185520	system is where you've watched this program, this program and this program, what should you watch
185520	190800	next. And you might have noticed that your mileage may vary on how effective that is but actually I
190800	195040	think they're pretty impressive for what they have to do. But you could use this for a recommender
195040	198800	system because you could say basically what programs have I got that embed into the same
198800	203680	space of all the things I just watched and recommend them that way. So there are downstream tasks
203680	207920	like classification and recommendations that we could use based on a system like this.
207920	214800	What this paper is showing is that you cannot apply effectively these downstream tasks for
214800	221520	difficult problems without massive amounts of data to back it up. And the idea that you can apply
222480	227920	this kind of classification on hard things. So not just cats and dogs but specific cats and
227920	234560	specific dogs or subspecies of tree. Or difficult problems where the answer is more difficult than
234560	239600	just the broad category that there isn't enough data on those things to train these models in an
239600	244480	effective way. I've got one of those apps that tells you what specific species a tree is. So
244480	248880	what is it not just similar to that? No, because they're just doing classification or some other
248960	255840	problem. They're not using this kind of generative giant AI. The argument has been why do that silly
255840	260720	little problem where you can do a general problem and solve all your problems. And the response is
260720	268240	because it didn't work. That's why we're doing it. So there are pros and cons for both. I'm not
268240	272960	going to say that no generative AI is useful or these models are incredibly effective for what
272960	279280	they do. But I'm perhaps suggesting that it may not be reasonable to expect them to do very difficult
279280	284720	medical diagnosis because you haven't got the data set to back that up. So how does this paper do
284720	289360	this? Well what they do is they define these core concepts. So some of the concepts are going to be
289360	293200	simple ones like a cat or a person. Some of them are going to be slightly more difficult like a
293200	299280	specific species of cat or a specific disease in an image or something like this. And they come up
299360	305920	with about 4,000 different concepts. And these are simple text concepts. These are not complicated
305920	312480	philosophical ideas. I don't know how well it embeds those. And what they do is they look at
312480	320400	the prevalence of these concepts in these data sets. And then they test how well the downstream
320400	326080	task of let's say zero shot classification or recall the recommender systems works on all of
326080	331200	these different concepts. And they plot that against the amount of data that they had for that
331200	335200	specific concept. So let's draw a graph and that will make me make it more clear. So let's imagine
335200	344960	we have a graph here like this. And this is the number of examples in our training set of a
344960	351200	specific concept. So let's say a cat, a dog, something more difficult. And this is the performance
351920	358480	on the actual task of let's say recommender system or recall of an object or the ability to
358480	363600	actually classify it as a cat. Remember we talked about how you could use this to zero shot classification
363600	368240	by just seeing if it embeds to the same place as a picture of a cat, the text a picture of a cat,
368240	374560	that kind of process. So this is performance. The best case scenario if you want to have an
374560	379920	all powerful AI that can solve all the world's problems is that this line goes very steeply
379920	385680	upwards. This is the exciting case. It goes like this. That's the exciting case. This is the kind
385680	390400	of AI explosion argument that basically says we're on the customer something that's about to happen
390400	396640	whatever that may be, where the scale is going to be such that this can just do anything. Then
396640	401600	there's the perhaps slightly more reasonable, should we say, pragmatic interpretation, which is
401600	407280	like just call it balanced, which is that there's a sort of linear movement. So the idea is that
407280	410720	we have to add a lot of examples, but we are going to get a decent performance boost from it.
410720	414320	So we just keep adding examples, we'll keep getting better, and that's going to be great.
414320	418560	And remember that if we ended up up here, we have something that could take any image and
418560	422960	tell you exactly what's in it under any circumstance. That's kind of what we're aiming for. And
422960	426240	similarly for large language models, this would be something that could write with incredible
426800	431280	accuracy on lots of different topics. Or for image generation, it would be something that could
431280	436320	take your prompt and generate a photo realistic image of that with almost no coercion at all.
436320	440640	That's kind of the goal. This paper has done a lot of experiments on a lot of these concepts
440640	447200	across a lot of models, across a lot of downstream tasks. And let's call this the evidence.
447200	449680	It's all you're going to call it pessimistic now and then.
449680	454400	It is pessimistic also, right? It's logarithmic. So it basically goes like this, right?
454400	455040	It flattens out.
455040	459600	It flattens out. Now, this is just one paper, right? It doesn't necessarily mean that it will
459600	465360	always flatten out. But the argument is, I think, that and it's not an argument they necessarily
465360	469840	make in the paper. The paper is very reasonable. I'm being a bit more cavalier with my wording.
469840	473520	The suggestion is that you can keep adding more examples. You can keep making your models bigger,
473520	479280	but we are soon about to hit a plateau where we don't get any better. And it's costing you millions
479280	482880	and millions of dollars to train this. At what point do you go, that's probably about as good as
482880	487760	we're going to get the technology, right? And then the argument goes, we need something else.
487760	492000	We need something in the transform or some other way of representing data or some other
492000	497520	machine learning strategy or some other strategy that's better than this in the long term if we
497520	501680	want to have this line go up here or this line go up here. That's kind of the argument.
501680	509760	And so this is essentially evidence, I would argue, against the kind of explosion possibility of
509760	513200	that just you just add a bit more data when we're on the cusp of something. We might come back here
513200	516640	in a couple of years, you know, if you'll still allow me on computer file after this absolute
516640	522240	embarrassment of these claims that I made. And we say, actually, the performance has improved
522240	526960	massively, right? Or we might say we've doubled the number of data sets to 10 billion images,
526960	532880	and we've got 1% more on the classification task, which is good, but is it worth it? I don't know.
532880	536320	This is a really interesting paper because it's very, very thorough, right? If there's a lot of
536320	540160	evidence, there's a lot of curves, and they all look exactly the same. It doesn't matter what method
540160	544000	you use, doesn't matter what data set you train on, it doesn't matter what your downstream task is,
544000	548320	the vast majority of them show this kind of problem. And the other problem is that we don't
548320	555280	have a nice, even distribution of classes and concepts within our data set. So for example,
555280	564880	cats, you can imagine are over-emphasized or over-represented in the data set by an order
564880	571120	of magnitude, right? Whereas specific planes or specific trees are incredibly underrepresented
571120	575840	because you just have tree, right? So, I mean, trees are probably going to be less represented
575840	580720	than cats anyway, but then specific species of tree very, very underrepresented, which is why,
580720	584880	when you ask one of these models, what kind of cat is this or what kind of tree is this,
584880	590000	it performs worse than when you ask it what animal is this because it's such a much easier problem.
590000	594560	And you see the same thing in image generation. If you ask it to draw a picture of something really
594560	599760	obvious like a castle where that comes up a lot in the training set, you can draw your fantastic
599760	604640	castle in the style of Monet and it can do all this other stuff. But if you ask it to draw some
604640	609760	obscure artifact from a video game that barely even made it into the training set, suddenly it's
609760	614320	starting to draw something a little bit less quality. And the same with large language models.
614320	617920	This paper isn't about large language models, but the same process you can see actually already
617920	624400	happening. If you talk to something like chatGPT, when you ask it about a really important topic
624400	628320	from physics or something like this, it will usually give you a pretty good explanation of that
628320	632480	thing because that's in the training set. But the question is what happens when you ask it about
632480	636400	something more difficult, right? When you ask it to write that code, which is actually quite difficult
636400	641360	to write, and it starts to make things up, it starts to hallucinate, and it starts to be less
641360	645520	accurate. And that is essentially the performance degrading because it's underrepresented in the
645520	650320	training set. The argument I think is, at least it's the argument that I'm starting to come around
650320	654720	to thinking, if you want performance on hard tasks, tasks that are underrepresented on just
654720	659760	general internet texts and searches, we have to find some other way of doing it than just collecting
659760	664640	more and more data, particularly because it's incredibly inefficient to do this. On the other
664640	670800	hand, these companies will, they've got a lot more GPUs than me. They're going to train on
670800	674960	bigger and bigger corpuses, better quality data, they're going to use human feedback to better
674960	680240	train their language models and things. So they may find ways to improve this up this way a little
680240	684640	bit as we go forward. But it's going to be really interesting to see what happens because I'll,
684640	690080	you know, will it plateau out? Will we see chapter GPT seven or eight or nine be roughly the same as
690080	694960	chapter GPT four? Or will we see another state of the art performance boost every time? I'm kind of
694960	699760	trending this way, but you know, it'll be excited to see if it goes this way. Take a look at this
699760	707840	puzzle devised by today's episode sponsor, Jane Strait. It's called bug bite, inspired by debugging
707840	713200	code. That world we're all too familiar with, where solving one problem might lead to a whole
713200	720000	chain of others. We'll link to the puzzle in the video description. Let me know how you get on.
720000	724240	And speaking of Jane Strait, we're also going to link to some programs that they're running at the
724240	730160	moment. These events are all expenses paid and give a little taste of the tech and problem solving
730160	739120	used at trading firms like Jane Strait. Are you curious? Are you problem solver? Are you into
739120	744800	computers? I think maybe you are. If so, well, you may well be eligible to apply for one of these
744800	749680	programs. Check out the links below or visit the Jane Strait website and follow these links.
750720	754080	There are some deadlines coming up for ones you might want to look at, and there are always
754080	758800	more on the horizon. Our thanks to Jane Strait for running great programs like this and also
758800	767360	supporting our channel. And don't forget to check out that bug bite puzzle.
