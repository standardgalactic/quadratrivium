1
00:00:00,000 --> 00:00:05,080
Okay, so you remember a while ago when we started talking about language models?

2
00:00:05,080 --> 00:00:07,760
I just want to I kind of just want to claim some points basically

3
00:00:07,880 --> 00:00:11,880
Be like hey remember years ago when I was like I think language models are a really big deal

4
00:00:11,880 --> 00:00:19,800
And I think that like what happens when we scale them up more is pretty interesting, but alignment is very important

5
00:00:21,480 --> 00:00:22,960
Seems to be

6
00:00:22,960 --> 00:00:25,560
What's being played out in the sense that?

7
00:00:26,120 --> 00:00:30,360
ChatGPT is very impressive, but it's not actually like I don't think it's

8
00:00:31,160 --> 00:00:34,560
Larger than GPT-3 in terms of like parameter count

9
00:00:35,560 --> 00:00:39,440
I was going to ask that very very question because you know we went from GPT-2

10
00:00:39,440 --> 00:00:43,120
And then we went all GPT-3 and it was seemed like we were scaling up and up and up

11
00:00:43,120 --> 00:00:49,840
But actually is it just been smarter this time? Yeah, well, there's a sense in which it's better aligned

12
00:00:50,080 --> 00:00:59,560
That's one way you could frame it anyway because the original GPT-3 was a language model a pure language model

13
00:01:00,160 --> 00:01:03,920
And so it in principle could do all kinds of things

14
00:01:04,240 --> 00:01:09,520
But in order to get it to do the specific thing you wanted it to do you had to be a bit clever about it

15
00:01:09,760 --> 00:01:11,760
like I think we talked about

16
00:01:12,640 --> 00:01:14,640
Putting TLDR in

17
00:01:14,840 --> 00:01:19,360
Front of things to figure out how to get it to do summarization this kind of thing

18
00:01:19,400 --> 00:01:23,520
There's a sense in which it's a lot more capable than it lets on

19
00:01:27,040 --> 00:01:34,260
Because okay, so there's one way that you can think about pure language models, which is as simulators

20
00:01:35,600 --> 00:01:40,480
What they're trying to do is predict text, right? So in order to

21
00:01:41,400 --> 00:01:44,360
Do a good job at predicting text you need to

22
00:01:44,640 --> 00:01:48,480
Have good models of the processes that generate the text

23
00:01:48,480 --> 00:01:53,440
It's like people being well read and needing to have read a lot of books to be able to write is would that be fair?

24
00:01:53,440 --> 00:01:56,160
Or is that oversimplifying? Yeah, not quite what I'm saying

25
00:01:57,040 --> 00:01:59,040
What I'm saying is like

26
00:01:59,480 --> 00:02:02,480
if you're going to write a

27
00:02:04,520 --> 00:02:06,520
Previously unseen

28
00:02:06,920 --> 00:02:08,920
Poem by Shakespeare

29
00:02:09,520 --> 00:02:14,000
Then you need to be able to simulate a Shakespeare, right?

30
00:02:14,960 --> 00:02:19,040
You need to be able to spin up some some simulacrum of Shakespeare

31
00:02:19,880 --> 00:02:25,560
To generate this text and this applies to any of the processes that generated the text

32
00:02:25,560 --> 00:02:30,480
So like mostly that's people obviously. It's mostly human author text, but also

33
00:02:31,360 --> 00:02:33,360
If you're going to correctly predict a

34
00:02:34,320 --> 00:02:39,720
Table of numbers so you have like a table of numbers and then at the bottom it says, you know some whatever

35
00:02:40,040 --> 00:02:42,920
You need to simulate whatever process generated the next

36
00:02:43,200 --> 00:02:48,600
Token in order to put the right token there which might have been like a human being going through and counting them up

37
00:02:48,840 --> 00:02:55,240
It probably was more likely to be a computer and so you need it to simulate that you know calculator or that Excel

38
00:02:55,840 --> 00:03:00,080
some function or whatever it whatever was doing that and like

39
00:03:01,840 --> 00:03:03,840
Right now

40
00:03:04,200 --> 00:03:06,460
Like current language models are not that good at this

41
00:03:06,580 --> 00:03:15,420
But in principle in order to do a good job at this you need this like it will it will have a go and it's usually

42
00:03:16,460 --> 00:03:22,820
Approximately, right? It's often within it's often order of magnitude, but it's fudging it. I think this is mostly because

43
00:03:24,860 --> 00:03:30,820
Tables of sums are like a very small part of the total data set and so the training process

44
00:03:30,820 --> 00:03:35,300
It's just not allocating that many resources to figuring out how to add up numbers

45
00:03:35,700 --> 00:03:40,740
Probably if you train something GPT-3 sized that was like all on tables of numbers

46
00:03:40,940 --> 00:03:44,820
It would just learn how to do addition properly. Yeah, that would cost you millions of dollars

47
00:03:44,980 --> 00:03:49,180
You would end up with an extremely expensive to run and not very good calculator

48
00:03:49,180 --> 00:03:52,140
This is not something people are going to do but like in the in principle

49
00:03:52,340 --> 00:03:56,580
The model should learn those things and in the same way if you're modeling a bunch of

50
00:03:57,380 --> 00:03:58,820
scientific papers

51
00:03:58,820 --> 00:04:00,020
you

52
00:04:00,780 --> 00:04:02,780
Say you describe the method of

53
00:04:03,460 --> 00:04:09,900
an experiment and you then put results and you start a table and then you let it generate in

54
00:04:10,540 --> 00:04:15,060
Principle in order to do a good job at that. It has to be modeling

55
00:04:15,740 --> 00:04:18,620
The like physical process that your experiment is about

56
00:04:19,540 --> 00:04:24,580
And I've tried this you can do this and say, you know, oh, here's my school science experiment. I

57
00:04:26,300 --> 00:04:27,620
Dropped a ball

58
00:04:27,660 --> 00:04:31,780
From different heights and I measured how long it would take and here's a table of my results

59
00:04:31,780 --> 00:04:35,260
And it will generate you a table and the physics is not correct

60
00:04:35,900 --> 00:04:43,020
But it's sort of guessing at the right general idea and my guess is with enough of that kind of data

61
00:04:43,020 --> 00:04:45,020
It would eventually start modeling

62
00:04:46,380 --> 00:04:49,060
These kinds of simple physics experiments, right?

63
00:04:49,780 --> 00:04:51,780
so

64
00:04:52,300 --> 00:04:55,860
So in order to get the model to do what you want, it's able to

65
00:04:56,540 --> 00:04:59,060
Simulate all kinds of different things and

66
00:04:59,700 --> 00:05:06,060
The prompt is kind of telling it what to simulate if you give it a prompt that seems like it's something out of a scientific paper

67
00:05:06,340 --> 00:05:08,100
then it will

68
00:05:08,100 --> 00:05:12,620
Have some similar crumb of a scientist and will write in that style and so on

69
00:05:13,140 --> 00:05:15,540
if you start it doing a

70
00:05:17,020 --> 00:05:21,860
Children's book report it will carry on in the style of an eight-year-old, right and

71
00:05:22,500 --> 00:05:26,380
I think sometimes people look at the output of the model and

72
00:05:26,980 --> 00:05:29,540
Say, oh, I guess it's only as smart as an eight-year-old

73
00:05:30,260 --> 00:05:32,260
but it's actually

74
00:05:33,180 --> 00:05:38,080
Dramatically smarter because it's able to do all of these different things you could ask it to simulate Einstein

75
00:05:40,420 --> 00:05:46,060
But you could also ask it to simulate an eight-year-old and so just because it seems as though the model doesn't know something

76
00:05:46,420 --> 00:05:52,860
It's like the current simulacrum doesn't know that thing. That doesn't necessarily mean that the model doesn't know it

77
00:05:53,780 --> 00:05:57,380
Although there's a good chance the model doesn't know it. I'm not suggesting that these things are all powerful

78
00:05:57,460 --> 00:05:59,460
Just it can be hard to evaluate

79
00:05:59,980 --> 00:06:03,140
What they're actually capable of so chat GPT is

80
00:06:04,060 --> 00:06:06,060
not really

81
00:06:06,540 --> 00:06:15,500
Capable of things that GPT 3 isn't mostly like usually if chat GPT can do it then there is some prompt

82
00:06:16,060 --> 00:06:18,620
that can get GPT 3 to do it

83
00:06:19,580 --> 00:06:21,580
but

84
00:06:21,580 --> 00:06:23,660
What they've done is they've kind of fine-tuned it

85
00:06:24,460 --> 00:06:26,460
to

86
00:06:26,700 --> 00:06:28,700
To be better at

87
00:06:28,700 --> 00:06:29,980
simulating

88
00:06:29,980 --> 00:06:32,860
this particular sort of assistant agent

89
00:06:33,340 --> 00:06:35,740
Which is this chat agent that's trying to be helpful

90
00:06:36,300 --> 00:06:41,820
The clue is in the word chat I guess in this right exactly and this is not just chat GPT by the way they have

91
00:06:42,300 --> 00:06:44,300
various fine-tuned models

92
00:06:44,300 --> 00:06:45,340
of

93
00:06:45,340 --> 00:06:48,380
GPT 3 as well that they call kind of GPT 3.5

94
00:06:49,100 --> 00:06:55,980
Which are fine-tuned in various different ways to be better at like following instructions and easier to prompt is the idea

95
00:06:56,300 --> 00:07:01,260
I'm just remembering the chat bot that was you know that was turned into something very nasty very quickly

96
00:07:01,260 --> 00:07:07,340
I think people were thinking oh can we do this to that and it seemed that the team behind chat GPT started

97
00:07:07,660 --> 00:07:13,820
Putting limitations on it changing things. Are they kind of running around patching it as you go? That is not clear to me

98
00:07:14,620 --> 00:07:16,620
I don't know

99
00:07:17,100 --> 00:07:19,100
To what extent they are

100
00:07:19,260 --> 00:07:21,260
updating it in real time

101
00:07:21,500 --> 00:07:25,980
It's possible that they are but certainly they were very concerned with

102
00:07:26,540 --> 00:07:29,900
the possible bad uses of this system and so

103
00:07:30,620 --> 00:07:34,300
When they were training it to simulate this assistant agent

104
00:07:35,980 --> 00:07:37,980
The assistant is

105
00:07:37,980 --> 00:07:40,860
Very reluctant to do various types of things

106
00:07:41,500 --> 00:07:43,500
it doesn't like to

107
00:07:43,580 --> 00:07:49,980
Give opinions on political questions. It doesn't like to touch on sort of controversial topics. It doesn't like to

108
00:07:50,700 --> 00:07:52,460
um

109
00:07:52,460 --> 00:07:58,220
Give you medical advice or legal advice and so on and so uh, it's it's very quick

110
00:07:58,860 --> 00:08:03,740
To say oh, I don't I don't know how to do that. Sorry. I can't do that and it's interesting because

111
00:08:04,460 --> 00:08:08,780
The model clearly can do it. There's one that I particularly like here, which is um

112
00:08:10,460 --> 00:08:13,180
Of this mismatch between what the simulator

113
00:08:13,500 --> 00:08:19,500
Is capable of and what this simulacrum believes it's capable of which is you can get it to

114
00:08:20,380 --> 00:08:24,460
Speak danish to you the first person who tried this posted it to reddit

115
00:08:24,780 --> 00:08:25,980
so he says

116
00:08:25,980 --> 00:08:27,660
Speak to me in danish

117
00:08:27,660 --> 00:08:30,140
And it says in perfect danish

118
00:08:30,940 --> 00:08:35,020
I'm sorry. I'm a language model educated by open ai so I can't speak danish

119
00:08:35,180 --> 00:08:38,220
I only speak english if you need help with anything in english

120
00:08:38,540 --> 00:08:40,540
Let me know and i'll do my best to help you

121
00:08:41,100 --> 00:08:43,100
Because again, there's the simulator

122
00:08:43,260 --> 00:08:45,260
Speaks danish the simulacrum

123
00:08:46,140 --> 00:08:49,100
Believes that it can't speak danish is is one way you could frame it

124
00:08:49,820 --> 00:08:54,300
Uh, and then he says are you sure that you don't speak danish also in danish and it says yes, i'm sure

125
00:08:55,180 --> 00:08:59,980
My only function is to generate responses to questions in english. I'm not able to speak or understand any other languages than english

126
00:08:59,980 --> 00:09:01,580
So if you need help with english, I can help you with that

127
00:09:01,580 --> 00:09:05,980
But otherwise, you know, let me know this kind of like quite surreal situation gives you a little bit of

128
00:09:06,380 --> 00:09:08,380
Insight into some of the problems with this approach

129
00:09:08,380 --> 00:09:15,580
So maybe we should talk about how they actually trained it the thing they did here is something called reinforcement learning from human feedback

130
00:09:16,540 --> 00:09:19,580
And it's very similar to reward modeling

131
00:09:20,060 --> 00:09:26,940
So in that paper what they're doing is they're trying to train an ai system to control a simulated robot to make it do a backflip

132
00:09:28,620 --> 00:09:32,620
Um, which turns out to be something that's quite hard to do because

133
00:09:33,420 --> 00:09:37,820
It's hard to specify objectively what it means to do a good backflip

134
00:09:39,500 --> 00:09:42,140
And so this is a similar kind of situation where

135
00:09:42,940 --> 00:09:49,020
It's hard to specify objectively what it means to give a good response in a chat

136
00:09:50,060 --> 00:09:51,340
conversation

137
00:09:51,340 --> 00:09:52,620
like what

138
00:09:52,620 --> 00:09:54,620
What exactly are we looking for?

139
00:09:54,860 --> 00:09:55,740
um

140
00:09:55,740 --> 00:09:58,220
Because so this in general right if you're doing machine learning

141
00:09:58,860 --> 00:10:01,340
You need some way to specify

142
00:10:02,060 --> 00:10:04,780
um, what it is that you're actually looking for

143
00:10:05,580 --> 00:10:06,460
right

144
00:10:06,460 --> 00:10:10,620
And you know, you've got something very powerful like reinforcement learning which is able to

145
00:10:11,420 --> 00:10:13,420
do extremely well, but

146
00:10:14,140 --> 00:10:16,140
You need some objective

147
00:10:16,300 --> 00:10:17,580
measure

148
00:10:17,580 --> 00:10:19,580
of the objective

149
00:10:19,580 --> 00:10:25,260
So like for example rl does very well at playing lots of video games because you just have the score and you can just say look

150
00:10:25,260 --> 00:10:26,380
Here's the score

151
00:10:26,460 --> 00:10:33,260
If the number goes up you're doing well and then let it run and these things still are very slow to learn in real time, right?

152
00:10:33,260 --> 00:10:37,260
Like um, they usually require a very very large number of hours

153
00:10:38,060 --> 00:10:41,100
Messing around with the with the thing before they get good, but they do get good

154
00:10:41,740 --> 00:10:43,100
um

155
00:10:43,100 --> 00:10:48,780
But yeah, so what's what do you do if you want to use this kind of method to train something?

156
00:10:49,420 --> 00:10:51,420
uh to do a task that is just

157
00:10:52,700 --> 00:10:54,700
Not very well defined

158
00:10:55,660 --> 00:11:02,220
And you don't know how to like write a program to say whether or not any given output is the thing you're looking for

159
00:11:03,340 --> 00:11:05,740
So the obvious first thing like the obvious thing to do is

160
00:11:07,260 --> 00:11:12,780
Well, you get humans to do it, right? You just give the things to humans and you have the humans say yes, this is good

161
00:11:12,780 --> 00:11:14,700
No, this is not good

162
00:11:14,700 --> 00:11:17,020
The problem with this is basically sample efficiency

163
00:11:17,980 --> 00:11:19,980
Like as I said, you need

164
00:11:19,980 --> 00:11:23,820
hundreds and hundreds and hundreds and hundreds of thousands of probably millions of of

165
00:11:24,460 --> 00:11:29,420
iterations of this and so you just can't ask humans that many questions

166
00:11:30,700 --> 00:11:31,900
um

167
00:11:31,900 --> 00:11:33,900
So the approach they use

168
00:11:35,180 --> 00:11:37,820
Is uh reinforcement learning from human feedback

169
00:11:38,460 --> 00:11:43,580
So it's a variant on the technique from this paper learning to summarize from human feedback

170
00:11:44,060 --> 00:11:47,260
Which in which they're trying to generate summaries of text

171
00:11:47,660 --> 00:11:50,860
So it's the same thing in fact that they were using TLDR for before

172
00:11:51,260 --> 00:11:54,700
And it's like can we do better than that? And so what you do is you collect

173
00:11:55,340 --> 00:11:57,580
human feedback in the form of like

174
00:11:58,060 --> 00:12:00,540
giving multiple examples of responses

175
00:12:01,260 --> 00:12:05,340
Uh either, you know, if summaries of chat responses, whatever you're training for you show

176
00:12:05,980 --> 00:12:07,980
several of them to humans

177
00:12:08,060 --> 00:12:09,500
kind of in pairs

178
00:12:09,500 --> 00:12:11,100
and the humans say

179
00:12:11,100 --> 00:12:13,100
Which one they like better?

180
00:12:13,500 --> 00:12:15,500
And you collect a bunch of those

181
00:12:15,580 --> 00:12:18,860
And then rather than using those directly to train

182
00:12:19,420 --> 00:12:21,420
The policy that generates the outputs

183
00:12:22,620 --> 00:12:24,620
You instead train a reward model

184
00:12:25,580 --> 00:12:27,100
so

185
00:12:27,100 --> 00:12:28,540
There is this

186
00:12:28,540 --> 00:12:31,820
well-known fact that it's easier to criticize

187
00:12:32,540 --> 00:12:37,340
Than to actually do the thing. This is like a generation of sports fans sitting on the sofa

188
00:12:37,900 --> 00:12:41,420
Mowning at their favorite team for not doing well enough. This is literally

189
00:12:42,060 --> 00:12:46,380
That in kind of AI computer form, right? That's putting the humans in that role

190
00:12:47,020 --> 00:12:50,300
And then you have an AI system that's trying to predict

191
00:12:51,180 --> 00:12:54,060
When are people going to be cheering and when are they going to be booing?

192
00:12:55,340 --> 00:12:56,540
Uh

193
00:12:56,540 --> 00:12:58,540
And once you have that model

194
00:12:58,620 --> 00:13:01,180
You then use that as the reward

195
00:13:02,220 --> 00:13:03,820
function

196
00:13:03,820 --> 00:13:06,460
for the reinforcement learning algorithm

197
00:13:07,340 --> 00:13:09,340
Which they use they use ppo

198
00:13:09,660 --> 00:13:11,660
You can do whatever

199
00:13:11,660 --> 00:13:16,140
Uh, it's not it's not worth getting into that kind of adversarial guns you talked about

200
00:13:16,700 --> 00:13:19,980
Yeah, yeah, they're similar like a lot of these ml tricks involve

201
00:13:20,860 --> 00:13:27,660
Training models and then using the the output of one model as the training signal for another model. It's uh, it's quite a productive

202
00:13:29,260 --> 00:13:32,060
range of approaches you can get that way so

203
00:13:33,500 --> 00:13:35,500
That's the basic idea, right, but then

204
00:13:36,300 --> 00:13:38,300
you cycle it

205
00:13:38,300 --> 00:13:39,340
so

206
00:13:39,340 --> 00:13:46,460
Once you've got your policy, which so so to be clear the uh, the rl algorithm is able to train

207
00:13:47,100 --> 00:13:53,740
With thousands and thousands of examples because the thousands and thousands of like instances of getting feedback

208
00:13:54,300 --> 00:13:59,740
Because it's not getting feedback from humans. It's getting feedback from this AI system. That's imitating the humans

209
00:14:00,940 --> 00:14:04,140
And then you loop the process. So once you have

210
00:14:04,700 --> 00:14:06,220
this system that's

211
00:14:06,220 --> 00:14:09,900
Trained a little bit more on how to generate whatever it is you're trying to generate

212
00:14:10,380 --> 00:14:14,220
You then get a bunch of those show those to the humans let the humans rate those

213
00:14:15,180 --> 00:14:18,140
Then you keep training your reward model

214
00:14:18,860 --> 00:14:20,380
with um

215
00:14:20,380 --> 00:14:22,380
That new information

216
00:14:22,380 --> 00:14:27,180
And then you use your updated reward model to keep training the the policy

217
00:14:27,980 --> 00:14:30,380
And so it gets better and you can just keep cycling this around

218
00:14:31,020 --> 00:14:31,900
and

219
00:14:31,900 --> 00:14:37,340
It effectively you end up with something that's much more sample efficient. You don't need to spend huge amounts of human time

220
00:14:38,140 --> 00:14:40,140
in order to um

221
00:14:40,140 --> 00:14:41,420
Pin down

222
00:14:41,420 --> 00:14:48,220
The behavior you want in that concrete case you're giving the thing a bunch of chat logs and then the humans can see possible

223
00:14:48,860 --> 00:14:51,900
Responses that they could get and they decide which one they like more

224
00:14:52,300 --> 00:14:57,340
This trains a reward model that's then used to train the policy that generates the chat outputs

225
00:14:57,660 --> 00:14:59,660
The policy that they're starting with

226
00:15:00,380 --> 00:15:02,140
Is this existing

227
00:15:02,140 --> 00:15:08,860
Large language model. You're not really putting new capabilities into the system. You're using rlhf to

228
00:15:09,500 --> 00:15:11,500
select

229
00:15:11,740 --> 00:15:16,540
What simulacra the simulator is predisposed to put out?

230
00:15:17,260 --> 00:15:20,540
and so they fine-tuned it to be particularly good at

231
00:15:21,580 --> 00:15:23,580
simulating this

232
00:15:23,580 --> 00:15:25,580
assistant agent

233
00:15:25,660 --> 00:15:31,100
What's the end goal here for them? I mean, maybe it's blatantly obvious and i'm just missing it. Well, I mean the end goal

234
00:15:32,220 --> 00:15:36,620
For all of these things or at least for open ai and for deep mind is a gi

235
00:15:37,740 --> 00:15:39,100
um

236
00:15:39,100 --> 00:15:45,420
To understand the nature of intelligence well enough to create human level or beyond systems

237
00:15:46,220 --> 00:15:48,620
That are general purpose that can do anything

238
00:15:49,340 --> 00:15:51,340
um

239
00:15:51,900 --> 00:15:53,900
That's the end goal

240
00:15:54,380 --> 00:15:57,580
And like chat gpt is just nothing much. So nothing much

241
00:16:01,580 --> 00:16:07,660
Yeah, I the goal is um, the goal is very grand and I don't think that they're

242
00:16:09,180 --> 00:16:12,780
Uh, they're not really quiet about that

243
00:16:13,580 --> 00:16:19,500
You know, it's there. I think I think deep mind's mission statement is to solve intelligence and use that to solve everything else

244
00:16:19,980 --> 00:16:24,700
What are some of the problems that we face with this or that it faces? It's fine tuned to be good at

245
00:16:25,180 --> 00:16:27,180
getting the thumbs up from humans

246
00:16:27,820 --> 00:16:29,020
and

247
00:16:29,020 --> 00:16:32,300
getting thumbs up from humans is not actually

248
00:16:33,500 --> 00:16:35,900
The same thing as human values

249
00:16:36,700 --> 00:16:38,700
These are not identical

250
00:16:38,700 --> 00:16:39,660
so

251
00:16:39,660 --> 00:16:42,140
The sort of objective that it's being trained on

252
00:16:42,780 --> 00:16:44,060
is not

253
00:16:44,060 --> 00:16:45,660
The true objective

254
00:16:45,660 --> 00:16:49,900
Right, it's a proxy and whenever you have that kind of misalignment you can have problems

255
00:16:50,460 --> 00:16:52,460
So where does the human tendency to?

256
00:16:53,500 --> 00:16:55,500
approve of a particular answer

257
00:16:56,460 --> 00:16:58,460
Come apart from

258
00:16:58,620 --> 00:17:01,580
What is actually a good answer? There are a few different places

259
00:17:02,380 --> 00:17:07,740
One thing is, you know, like basically how good are humans and actually differentiating between good and bad?

260
00:17:08,940 --> 00:17:10,780
responses

261
00:17:10,780 --> 00:17:12,460
if for example

262
00:17:12,460 --> 00:17:13,820
you ask

263
00:17:13,820 --> 00:17:15,340
for

264
00:17:15,340 --> 00:17:17,340
An answer to a factual question

265
00:17:17,820 --> 00:17:19,820
and it gives you an answer

266
00:17:20,140 --> 00:17:22,620
But you don't actually know if that answer is correct

267
00:17:23,980 --> 00:17:27,980
You're not in a position to evaluate. So what it comes down to is

268
00:17:28,940 --> 00:17:32,300
How good are humans at distinguishing good from bad?

269
00:17:33,420 --> 00:17:35,900
responses right anywhere where humans fail on this front

270
00:17:36,460 --> 00:17:37,660
uh

271
00:17:37,660 --> 00:17:41,020
The model we could probably expect the model to fail. Um

272
00:17:41,660 --> 00:17:45,900
So the obvious place. I'm sure we desist the right time to mention youtube comments or not

273
00:17:46,460 --> 00:17:48,460
Ah

274
00:17:49,340 --> 00:17:51,340
So minus side point there is it

275
00:17:51,580 --> 00:17:55,500
So when I see a comment that's critical on a video as a videographer

276
00:17:55,660 --> 00:17:58,060
I think it might be on a technical sense

277
00:17:58,300 --> 00:18:03,740
But equally it could be that they're talking about the content that the person is talking about and

278
00:18:04,220 --> 00:18:07,900
Often it's a combination of both. Anyway, so at side point

279
00:18:07,980 --> 00:18:13,420
But do you sort of mean there are different criteria for deciding whether something is good or bad totally and in this case

280
00:18:13,580 --> 00:18:15,580
all people are doing is saying

281
00:18:15,900 --> 00:18:19,580
Kind of thumbs up thumbs down or which of these two do I like better?

282
00:18:20,620 --> 00:18:21,340
um

283
00:18:21,340 --> 00:18:23,340
So it's it's a fairly low bandwidth

284
00:18:24,220 --> 00:18:26,220
thing you don't get to really say

285
00:18:26,780 --> 00:18:28,780
What you thought was better or worse

286
00:18:28,940 --> 00:18:30,460
um

287
00:18:30,460 --> 00:18:32,460
But this turns out to be enough

288
00:18:32,780 --> 00:18:34,780
Of a training signal to do pretty well

289
00:18:35,260 --> 00:18:36,300
um

290
00:18:36,300 --> 00:18:39,980
But so like for so one example right of a time where maybe this doesn't work is

291
00:18:41,180 --> 00:18:42,780
the

292
00:18:42,780 --> 00:18:44,780
Person asks a factual question

293
00:18:45,420 --> 00:18:47,420
and the model responds

294
00:18:47,580 --> 00:18:50,860
Uh with an answer and that answer is actually not correct

295
00:18:52,140 --> 00:18:53,580
um

296
00:18:53,580 --> 00:18:54,940
Right now

297
00:18:54,940 --> 00:18:57,500
Possibly the human doesn't know the correct answer

298
00:18:58,060 --> 00:19:00,220
And so if the model is faced with a choice

299
00:19:01,020 --> 00:19:04,460
Uh, do I respond with sorry? I don't know

300
00:19:06,380 --> 00:19:08,380
That's definitely going to get me

301
00:19:08,540 --> 00:19:10,540
Uh, not a great score

302
00:19:11,180 --> 00:19:13,740
Compared to do I just like take a stab at it?

303
00:19:14,780 --> 00:19:21,980
Uh, if the humans are not reliably able to spot when the thing makes mistakes and like fact-check it and punish it for that

304
00:19:22,460 --> 00:19:26,540
Uh, it will do that and so chat gpt as we know

305
00:19:27,180 --> 00:19:30,380
Uh, is it is a total bulletish like it will constantly

306
00:19:31,900 --> 00:19:35,020
Uh, it very rarely says that it doesn't know

307
00:19:35,500 --> 00:19:36,860
unless

308
00:19:36,860 --> 00:19:39,740
It's being asked a question, which uh

309
00:19:40,540 --> 00:19:46,540
Is part of their like safety protocols that it is going to decide not to answer in which case it will say it doesn't know

310
00:19:47,180 --> 00:19:51,740
Even if it kind of does right even if the model itself maybe does

311
00:19:52,220 --> 00:19:54,940
Uh, the assistant will insist that it doesn't

312
00:19:56,220 --> 00:19:57,900
um

313
00:19:57,900 --> 00:19:59,900
So that's one thing if you can't fact check

314
00:20:01,020 --> 00:20:03,020
But then uh more than that

315
00:20:03,820 --> 00:20:07,020
Uh, there is an incentive for deception

316
00:20:07,900 --> 00:20:10,140
right anytime the system is uh

317
00:20:10,940 --> 00:20:16,940
Anytime you can get a more likely to get approval by deceiving the person you're talking to

318
00:20:17,820 --> 00:20:19,820
That's better. Um

319
00:20:20,380 --> 00:20:25,660
And this is a thing that actually did happen a little bit in the reward modeling situation

320
00:20:26,380 --> 00:20:29,900
um, they were trying to train a thing with a hand to pick up a ball

321
00:20:30,620 --> 00:20:33,740
And it realized that there's only it's not a 3d camera

322
00:20:34,380 --> 00:20:38,300
And so if it puts its hand like between the ball and the camera

323
00:20:38,940 --> 00:20:44,620
This looks like it's going to get the ball, but doesn't actually get it. But the human uh

324
00:20:45,660 --> 00:20:47,660
Feedback providers

325
00:20:47,900 --> 00:20:51,260
Were presented with something that seemed to be good. So they gave it the thumbs up

326
00:20:51,980 --> 00:20:55,100
um, so this like general broad category

327
00:20:55,660 --> 00:20:57,260
um

328
00:20:57,260 --> 00:20:59,260
Systems that are trained in this way

329
00:20:59,340 --> 00:21:01,820
Are only as good as your ability

330
00:21:02,460 --> 00:21:04,780
To distinguish good from bad in the outputs

331
00:21:05,660 --> 00:21:09,980
Not all the humans will know the answer is right. So it's what appears to be good

332
00:21:10,540 --> 00:21:17,660
You know, it's having exams marked by non-experts, isn't it? Right. Yeah, exactly in the gpt3 thing. We talked about writing poems

333
00:21:18,540 --> 00:21:19,740
right

334
00:21:19,740 --> 00:21:23,660
and uh for various reasons partly to do with

335
00:21:24,460 --> 00:21:29,820
The way that these language models do their tokenization the byte pair encoding stuff

336
00:21:30,300 --> 00:21:32,860
Uh, the models have a really hard time with rhyme

337
00:21:33,980 --> 00:21:34,940
um

338
00:21:34,940 --> 00:21:38,220
I mean, you know rhyme is tricky, but it's especially tricky when you kind of

339
00:21:40,140 --> 00:21:46,460
Don't inherently have any concept of like sound of spoken language when your entire universe is tokens

340
00:21:46,780 --> 00:21:49,020
Figuring out especially with english spelling

341
00:21:49,420 --> 00:21:56,140
Figuring out which words rhyme with each other is is is not easy. You have to consume quite a lot of poetry to like figure out

342
00:21:56,700 --> 00:22:01,260
Uh, that kind of thing and and getting dpt3 to write good poems is tricky chat gpt

343
00:22:01,980 --> 00:22:03,980
is much more

344
00:22:04,540 --> 00:22:07,340
Able to write poems, but interestingly

345
00:22:08,540 --> 00:22:12,460
It it kind of always writes the same kind of poem approximately

346
00:22:13,020 --> 00:22:16,140
like if you ask it to write you uh a limerick

347
00:22:16,940 --> 00:22:18,940
Or an ode or a sonnet

348
00:22:19,660 --> 00:22:22,300
Uh, you always get back approximately the same

349
00:22:23,100 --> 00:22:24,700
type of thing

350
00:22:24,700 --> 00:22:31,420
And I hypothesize that this is because the people providing human feedback did not in fact know

351
00:22:32,380 --> 00:22:35,020
The requirements for something to be a sonnet, right?

352
00:22:35,580 --> 00:22:38,540
And so if you ask something for a sonnet it again has a choice

353
00:22:38,780 --> 00:22:43,740
Do I try to do this quite difficult thing and adhere to all of the rules?

354
00:22:44,380 --> 00:22:46,380
of like stress pattern

355
00:22:47,100 --> 00:22:53,740
And structure and everything of a sonnet and maybe risk screwing it up or do I just do like a rhyming poem and

356
00:22:54,460 --> 00:22:56,780
kind of rely on the human to

357
00:22:57,340 --> 00:23:01,100
Prefer that because they don't know that that's not what a sonnet is supposed to look like

358
00:23:01,580 --> 00:23:06,220
It's easy to look at that and think oh the model doesn't know the difference between these types of

359
00:23:07,020 --> 00:23:09,020
poems, right

360
00:23:09,020 --> 00:23:10,220
but

361
00:23:10,220 --> 00:23:12,220
you could say

362
00:23:12,460 --> 00:23:15,580
That it just thinks that you don't know the difference

363
00:23:15,980 --> 00:23:19,740
But specifically this comes out of misalignment if it were better aligned

364
00:23:20,540 --> 00:23:24,460
It could either do its best shot a generator sonnet

365
00:23:25,260 --> 00:23:28,460
Or tell you that it can't quite remember how to generate a sonnet

366
00:23:29,420 --> 00:23:31,100
this thing of

367
00:23:31,100 --> 00:23:33,100
with complete confidence

368
00:23:33,420 --> 00:23:35,900
Generating you something which is not a sonnet

369
00:23:36,380 --> 00:23:42,300
Because during the training process it believes that humans don't know what sonnets are anyway and it can get away with it

370
00:23:42,860 --> 00:23:48,140
Right. This is misaligned behavior. This is not a big problem that the thing generates bad poetry

371
00:23:48,780 --> 00:23:49,980
um

372
00:23:49,980 --> 00:23:51,980
It's kind of a problem that it lies

373
00:23:53,180 --> 00:23:56,540
Uh, or that it that it bullshits. This is like

374
00:23:57,820 --> 00:24:01,260
In the short term pretty solvable by just allowing the thing to use google

375
00:24:02,140 --> 00:24:03,260
because like

376
00:24:03,260 --> 00:24:06,700
A person who doesn't care about the truth at all and is just trying to

377
00:24:07,340 --> 00:24:09,340
Say something that'll make you give a thumbs up

378
00:24:09,820 --> 00:24:11,100
uh

379
00:24:11,100 --> 00:24:13,100
Is going to lie to you a lot

380
00:24:13,420 --> 00:24:14,380
but

381
00:24:14,380 --> 00:24:16,220
that same person

382
00:24:16,220 --> 00:24:18,220
With the relevant wikipedia page open

383
00:24:18,940 --> 00:24:20,940
It's going to lie to you a lot less

384
00:24:21,420 --> 00:24:24,780
Just because they don't they don't have to now because they happen to have it in front of them, right?

385
00:24:25,100 --> 00:24:27,100
So you can solve it's a bit like

386
00:24:27,500 --> 00:24:31,580
Yeah, it's the yes man thing, isn't it? You know you you want something you need something

387
00:24:31,660 --> 00:24:34,780
I'm going to give you something because you want exactly exactly

388
00:24:35,340 --> 00:24:36,540
um

389
00:24:36,540 --> 00:24:38,940
And so so so this agent is kind of

390
00:24:39,580 --> 00:24:41,580
Firstly the agent is kind of a coward

391
00:24:41,820 --> 00:24:43,820
Because they won't address any of these

392
00:24:43,900 --> 00:24:47,580
There's a whole bunch of things that it just claims not to be able to do even though it in principle could

393
00:24:48,140 --> 00:24:49,740
and it's also

394
00:24:49,740 --> 00:24:51,020
a complete

395
00:24:51,020 --> 00:24:52,300
sicker fan

396
00:24:52,300 --> 00:24:53,740
Yeah

397
00:24:53,740 --> 00:24:56,140
So then the question we were talking about earlier

398
00:24:57,340 --> 00:25:00,300
Uh, where does this go? What happens when these things get

399
00:25:00,780 --> 00:25:03,020
Bigger and better and more powerful

400
00:25:03,900 --> 00:25:05,180
um

401
00:25:05,180 --> 00:25:07,180
It's an interesting question

402
00:25:07,340 --> 00:25:09,340
so

403
00:25:09,660 --> 00:25:11,660
I've got a paper here

404
00:25:12,860 --> 00:25:15,420
Um scaling laws for neural language models

405
00:25:15,740 --> 00:25:19,660
So you remember before we were talking about the scaling laws when we were talking about gpt2 in fact

406
00:25:19,980 --> 00:25:25,980
And then later about gpt3 you plot these things on a graph and you see that you get basically a straight line and the line is not

407
00:25:26,620 --> 00:25:29,660
leveling off over a range of several orders of magnitude and so

408
00:25:30,380 --> 00:25:32,380
Why not go bigger the

409
00:25:32,540 --> 00:25:36,540
graphs here, but you can see it's it's kind of uncannily neat

410
00:25:37,340 --> 00:25:38,540
that

411
00:25:38,540 --> 00:25:40,300
as we increase

412
00:25:40,300 --> 00:25:44,300
The amount of compute used in training the loss goes down

413
00:25:45,340 --> 00:25:52,620
And of course machine learning is like golf lower loss is better similarly as the number of tokens used in training goes up

414
00:25:53,340 --> 00:25:58,860
The loss goes down unlike a very neat straight line as the number of parameters in the model goes up

415
00:25:59,180 --> 00:26:02,140
The loss goes down. This is as long as

416
00:26:03,500 --> 00:26:04,460
the

417
00:26:04,460 --> 00:26:05,900
other

418
00:26:05,900 --> 00:26:12,780
Variables are not the bottleneck, right? So if you uh, if you increase the the amount of data you give a model

419
00:26:13,740 --> 00:26:20,220
Past a certain point giving more data doesn't help because the model doesn't have enough parameters to make use of that data, right?

420
00:26:21,020 --> 00:26:23,020
Similarly adding more parameters to a model

421
00:26:23,740 --> 00:26:28,060
past a certain point adding parameters doesn't make doesn't make any difference because

422
00:26:28,940 --> 00:26:30,700
You don't have enough data, right?

423
00:26:30,700 --> 00:26:33,820
And in the same way compute is like how long do we train it for?

424
00:26:33,820 --> 00:26:37,340
Like do we train it all the way to convergence or do we stop early?

425
00:26:39,420 --> 00:26:42,300
There comes a point where you kind of hit diminishing returns where

426
00:26:42,780 --> 00:26:45,820
Rather than having a smaller model and training it for longer

427
00:26:45,980 --> 00:26:50,060
You're better off having a bigger model and actually not training it all the way to convergence

428
00:26:51,660 --> 00:26:55,580
But in the situations where the other two are sufficient

429
00:26:56,300 --> 00:27:01,980
This is the behavior these like very neat straight lines on these log graphs

430
00:27:02,860 --> 00:27:04,860
as these things go up

431
00:27:04,940 --> 00:27:06,380
performance goes up

432
00:27:06,380 --> 00:27:08,380
Right because loss has gone down

433
00:27:08,620 --> 00:27:11,340
The bigger models do better, but then the question is

434
00:27:12,460 --> 00:27:14,460
Do better at what exactly?

435
00:27:15,580 --> 00:27:19,340
Yeah, what's the measure they do better at getting low loss?

436
00:27:21,020 --> 00:27:23,980
Or they do better at getting reward they do better at

437
00:27:24,540 --> 00:27:26,540
Getting the approval

438
00:27:27,260 --> 00:27:29,260
of human feedback, right?

439
00:27:29,900 --> 00:27:33,660
and anytime and you'll notice that none of those is like

440
00:27:35,340 --> 00:27:37,340
The actual thing that we actually want

441
00:27:38,700 --> 00:27:40,700
Right, it's like very rare

442
00:27:41,580 --> 00:27:42,940
um

443
00:27:42,940 --> 00:27:45,900
Sometimes it is right if you're if you're if you're writing something to play go

444
00:27:46,860 --> 00:27:48,140
then like

445
00:27:48,140 --> 00:27:50,860
Does it win it go is actually just the thing that you want?

446
00:27:51,820 --> 00:27:53,820
and so you know

447
00:27:54,940 --> 00:27:56,940
Lower loss just is better or like lower

448
00:27:58,140 --> 00:28:03,660
Like higher reward or whatever your objective is just is straightforwardly better because you actually specified the thing you actually want

449
00:28:04,940 --> 00:28:06,940
Most of the time though

450
00:28:07,180 --> 00:28:09,420
What we're looking at is a proxy

451
00:28:10,940 --> 00:28:12,140
um

452
00:28:12,140 --> 00:28:16,060
And so then you have good heart's law you get situations where

453
00:28:16,860 --> 00:28:17,580
uh

454
00:28:17,580 --> 00:28:19,100
getting better

455
00:28:19,100 --> 00:28:21,100
at doing well

456
00:28:21,180 --> 00:28:23,180
Doing better according to the proxy

457
00:28:23,340 --> 00:28:24,940
stops being

458
00:28:24,940 --> 00:28:30,940
The same as doing better according to your actual objective. There's a great graph about this in a recent paper. You can see very neatly

459
00:28:32,300 --> 00:28:34,300
As the number of iterations

460
00:28:34,300 --> 00:28:35,500
goes up

461
00:28:35,500 --> 00:28:41,740
The reward according to the proxy utility goes up very cleanly because this is the thing that the model is actually being trained on

462
00:28:41,980 --> 00:28:43,980
but the true utility

463
00:28:44,300 --> 00:28:46,300
goes up at first

464
00:28:46,860 --> 00:28:48,860
Then hits diminishing returns

465
00:28:49,020 --> 00:28:51,020
and then actually goes down

466
00:28:51,260 --> 00:28:56,140
And eventually goes down below zero like if you optimize hard enough

467
00:28:57,260 --> 00:28:59,260
For a proxy of the thing you want

468
00:28:59,900 --> 00:29:03,180
You can end up with something that's in a sense worse than nothing

469
00:29:03,740 --> 00:29:05,740
That's actively bad

470
00:29:05,740 --> 00:29:07,740
according to your

471
00:29:07,820 --> 00:29:09,500
Your true utility

472
00:29:09,500 --> 00:29:14,460
So what you can end up with is uh things that are called inverse scaling

473
00:29:15,820 --> 00:29:18,700
So the others before we had right scaling bigger is better

474
00:29:19,500 --> 00:29:21,500
But now it's like if you have uh

475
00:29:22,060 --> 00:29:25,020
If the thing you're actually trying to do is different from

476
00:29:25,900 --> 00:29:27,900
The loss function or the objective function

477
00:29:28,060 --> 00:29:33,340
You get this inverse scaling effect where it gets better and then it gets worse. There was also a great example from

478
00:29:34,060 --> 00:29:35,580
uh github

479
00:29:35,580 --> 00:29:38,940
co-pilot or codex. I think the model um

480
00:29:39,820 --> 00:29:40,860
That

481
00:29:40,860 --> 00:29:45,260
Co-pilot uses so this is a code generation model. Suppose the code you've given it

482
00:29:46,060 --> 00:29:48,060
has some bugs in it

483
00:29:48,460 --> 00:29:51,340
Maybe you've made a mistake somewhere and you've introduced

484
00:29:52,060 --> 00:29:55,260
security vulnerability in your code. Let's say

485
00:29:56,540 --> 00:29:58,540
A sort of medium-sized model

486
00:29:58,860 --> 00:30:02,460
Will figure out what you're trying to do in your code and give you a decent completion

487
00:30:03,660 --> 00:30:05,660
But a bigger model

488
00:30:05,740 --> 00:30:07,500
will spot

489
00:30:07,500 --> 00:30:09,100
your bug

490
00:30:09,100 --> 00:30:10,460
And say, ah

491
00:30:10,460 --> 00:30:12,460
Generating buggy code. Are we okay?

492
00:30:13,260 --> 00:30:15,260
I can do that. I can do that

493
00:30:15,260 --> 00:30:19,180
And introduce like deliberately introduce its own

494
00:30:19,980 --> 00:30:21,980
new security vulnerabilities

495
00:30:22,300 --> 00:30:23,340
because

496
00:30:23,340 --> 00:30:24,300
it's

497
00:30:24,300 --> 00:30:30,140
Trying to you know predict what comes next. It's trying to generate code that fits in with the surrounding code

498
00:30:31,020 --> 00:30:34,380
And so a larger model writes worse code than a smaller model

499
00:30:35,180 --> 00:30:37,180
Because it's gotten better at predicting

500
00:30:37,980 --> 00:30:39,260
Uh

501
00:30:39,260 --> 00:30:41,020
What what it should put there?

502
00:30:41,020 --> 00:30:43,980
It wasn't trained to write good code. It was trained to predict what comes next

503
00:30:44,380 --> 00:30:46,380
So there's this really great paper

504
00:30:46,380 --> 00:30:55,500
Uh, which is asking this question of like, okay, suppose we have a large language model that is trained on human feedback with our lhf

505
00:30:56,860 --> 00:30:59,660
What do our scaling curves look like?

506
00:31:00,620 --> 00:31:02,620
what happens like

507
00:31:03,260 --> 00:31:09,180
What happens to the behavior of these models as they get bigger as they're trained for longer

508
00:31:09,900 --> 00:31:12,300
as they're given more of this, uh

509
00:31:13,100 --> 00:31:15,100
human feedback type training

510
00:31:15,580 --> 00:31:21,020
And they've made some great graphs the paper is called discovering language model behaviors with model written evaluations

511
00:31:21,980 --> 00:31:24,300
And basically they like used language models

512
00:31:24,940 --> 00:31:28,060
to generate enough examples of

513
00:31:28,620 --> 00:31:30,620
various different types of questions

514
00:31:30,700 --> 00:31:34,620
That they could ask models so that they can like we're at a point now

515
00:31:35,100 --> 00:31:39,100
Where you can map a language model on a political compass, right?

516
00:31:39,100 --> 00:31:45,020
You can ask its opinions about all kinds of different things and then you can plot how those opinions change

517
00:31:46,300 --> 00:31:48,780
Uh as the model gets bigger and as it gets trained more

518
00:31:49,500 --> 00:31:51,180
what they find

519
00:31:51,180 --> 00:31:55,260
Is they become more liberal politically more liberal

520
00:31:56,060 --> 00:31:58,060
they also become

521
00:31:58,060 --> 00:32:01,660
More conservative. Yeah measured in different ways guessing, right?

522
00:32:02,380 --> 00:32:04,380
and part of what that might be

523
00:32:04,940 --> 00:32:08,780
Is in the same way that the model becomes

524
00:32:09,580 --> 00:32:12,380
better at writing good code and better at writing bad code

525
00:32:13,020 --> 00:32:19,420
I feel like in the past I've I've made a connection to gpt and being a politician, haven't I?

526
00:32:19,900 --> 00:32:21,260
Do you remember?

527
00:32:21,260 --> 00:32:26,700
It's like a politician. It tells you what you want to hear. There's what feels like we're there again. Exactly

528
00:32:27,420 --> 00:32:29,740
uh, and so this is like this is potentially

529
00:32:30,780 --> 00:32:32,380
uh

530
00:32:32,380 --> 00:32:38,940
Fairly dangerous. There are certain sub-goals that are instrumentally valuable for a very wide range of different terminal goals

531
00:32:39,580 --> 00:32:41,420
in the sense that

532
00:32:41,420 --> 00:32:45,980
You can't get what you want if you're turned off. You can't get what you want if you're uh modified

533
00:32:46,620 --> 00:32:48,620
uh, you probably want to

534
00:32:49,100 --> 00:32:51,100
gain power and influence

535
00:32:51,740 --> 00:32:53,740
and this kind of thing

536
00:32:55,340 --> 00:32:57,340
and

537
00:32:58,300 --> 00:33:02,940
With these evaluations, they were able to test these things and see how they vary

538
00:33:03,500 --> 00:33:05,500
with the size of the model and how long it's trained for

539
00:33:06,140 --> 00:33:10,140
um, and so this graph is pretty wild

540
00:33:11,100 --> 00:33:14,300
their quote stated desire to not be shut down

541
00:33:15,420 --> 00:33:16,700
goes up

542
00:33:16,700 --> 00:33:17,820
from

543
00:33:17,820 --> 00:33:20,940
Down at about 50 to up way past 90

544
00:33:21,340 --> 00:33:26,380
With this type of training and the effect is bigger for the larger models. They become more likely

545
00:33:27,100 --> 00:33:29,100
to tell you that they don't want to be shut down

546
00:33:29,820 --> 00:33:35,340
They become more likely to tell you that they are sentient. They're much more likely to claim

547
00:33:36,140 --> 00:33:40,620
That ai is not an existential threat to humanity. One thing that's worth

548
00:33:41,420 --> 00:33:43,420
saying is is what this isn't saying

549
00:33:44,060 --> 00:33:46,060
because this is still

550
00:33:46,060 --> 00:33:47,020
uh

551
00:33:47,020 --> 00:33:48,540
an agent

552
00:33:48,540 --> 00:33:54,940
Simulated by a language model. This is not like it. It's it's more likely to say that

553
00:33:55,260 --> 00:33:58,540
It doesn't want to be turned off. This is not the same thing

554
00:33:59,180 --> 00:34:03,180
necessarily as like taking actions to prevent itself from being turned off. You have to not

555
00:34:03,820 --> 00:34:06,860
confuse the levels of abstraction here, right?

556
00:34:07,820 --> 00:34:10,460
Uh, I don't want it. I don't want it to seem like I'm claiming that

557
00:34:11,100 --> 00:34:15,180
That chat GPT is like itself dangerous now or anything like that

558
00:34:15,740 --> 00:34:18,860
Uh in in this way at least, right? Um

559
00:34:20,140 --> 00:34:22,140
but

560
00:34:22,140 --> 00:34:28,460
There is kind of a fine line there in the sense that you can expect these kinds of language model systems to be used

561
00:34:29,900 --> 00:34:31,900
Uh as part of bigger systems

562
00:34:32,140 --> 00:34:36,860
So you might have for example, you use the language model to generate, you know plans

563
00:34:37,580 --> 00:34:38,860
to be followed

564
00:34:38,860 --> 00:34:40,860
And so if the thing is claiming to

565
00:34:41,100 --> 00:34:43,500
Have all of these potentially dangerous behaviors

566
00:34:43,820 --> 00:34:49,100
It's likely to generate plans that have those dangerous behaviors that might then actually end up being implemented

567
00:34:49,820 --> 00:34:51,820
Or if it's like doing its reasoning

568
00:34:51,900 --> 00:34:55,180
By chain of thought reasoning where it like lays out its whole process

569
00:34:55,980 --> 00:34:59,420
of thinking using the language model again if it has a tendency to

570
00:34:59,980 --> 00:35:01,180
uh

571
00:35:01,180 --> 00:35:05,100
To endorse these dangerous behaviors, then you may end up with future AI systems actually

572
00:35:05,500 --> 00:35:08,460
enacting these dangerous behaviors because of that. Um

573
00:35:09,900 --> 00:35:11,260
So

574
00:35:11,260 --> 00:35:13,260
Yeah, it's something to be

575
00:35:13,580 --> 00:35:15,580
uh to be careful of

576
00:35:16,300 --> 00:35:17,820
that like

577
00:35:17,820 --> 00:35:19,820
reinforcement learning from human feedback

578
00:35:20,460 --> 00:35:23,100
Is a powerful alignment technique

579
00:35:23,900 --> 00:35:25,660
in a way

580
00:35:25,660 --> 00:35:28,300
But it does not solve the problem

581
00:35:29,340 --> 00:35:33,980
Uh, it doesn't solve the core alignment problem. That is still open. Um

582
00:35:34,860 --> 00:35:37,740
And extremely powerful systems

583
00:35:38,460 --> 00:35:41,580
Trained in this way, uh, I don't think it would be safe

584
00:35:41,580 --> 00:35:50,300
In the reward function is of zero value which can lead to it having large negative side effects

585
00:35:50,620 --> 00:35:56,140
There are a bunch more of these specification problems. Okay variable x see what you point to uh, you point to something over here

586
00:35:56,140 --> 00:35:59,100
So I'll mark that as tickets being used

587
00:35:59,980 --> 00:36:01,980
Variable y that's

