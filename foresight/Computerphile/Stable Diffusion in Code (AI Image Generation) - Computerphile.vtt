WEBVTT

00:00.000 --> 00:05.280
Last time we talked about how these kind of networks and image generation systems work.

00:05.280 --> 00:06.760
But there are different kinds, aren't there?

00:06.760 --> 00:11.000
There are, there's Dali2, there's Imogen, there's StableDiffusion,

00:11.000 --> 00:16.680
and I didn't talk about them in the last video because they are, for the sake of understanding diffusion models in general,

00:16.680 --> 00:20.200
essentially the same, but actually they are quite different underneath, right?

00:20.200 --> 00:25.080
And it comes down to, you know, the resolution, exactly where you do the embeddings,

00:25.080 --> 00:29.120
how you do the embeddings, how you structure your network and so on and so forth, right?

00:29.120 --> 00:34.520
So, and in fact, actually in StableDiffusion's case, it comes down to where you do the diffusion as well.

00:34.520 --> 00:39.560
So, let's look, we'll look at the StableDiffusion code because I've got access to that, right?

00:39.560 --> 00:43.680
And we'll go into it in quite some detail, I think would be quite interesting.

00:43.680 --> 00:47.840
I've really enjoyed using it because first of all, it's given me a better understanding of how it works.

00:47.840 --> 00:51.280
And also, you can do some pretty cool stuff by messing about down there and saying,

00:51.280 --> 00:54.160
well, what if I gave it a frog but also a snake, right?

00:54.160 --> 00:56.040
And the answer is you get a frog snake.

00:56.920 --> 00:57.920
A frog?

00:57.920 --> 01:01.480
Yeah, exactly. Snake giraffe was the stuff of nightmares.

01:04.920 --> 01:07.840
There were questions about FX, there were questions about how these are trained,

01:07.840 --> 01:09.840
maybe we deal with them at another time.

01:09.840 --> 01:11.200
Let's talk about how they work.

01:11.200 --> 01:15.400
So, Dali 2 is perhaps at the moment the biggest one,

01:15.400 --> 01:19.000
but it's been, I think rapidly overtaken by StableDiffusion,

01:19.000 --> 01:21.440
primarily because StableDiffusion is more available to people.

01:21.440 --> 01:23.920
I can download the code and run it of StableDiffusion,

01:23.920 --> 01:27.560
Dali, you access via an API and you say, I would like an image, please,

01:27.560 --> 01:30.000
and it gives you something back.

01:30.000 --> 01:34.840
If you don't have any interest in the code, then sure, just use the API.

01:34.840 --> 01:38.160
But if, for example, like me, you might be interested in what the applications

01:38.160 --> 01:42.640
for generating images in your area of research, like plants or medical imaging,

01:42.640 --> 01:46.200
maybe I want the access to the code and I can train up the network myself, right?

01:46.200 --> 01:50.800
So, Dali builds on a lot of stuff, it's from OpenAI,

01:50.800 --> 01:54.040
it builds on a lot of stuff that they've already done, right?

01:54.040 --> 01:55.920
The first one is something called clip embeddings.

01:55.920 --> 01:59.200
Clip embeddings are the way of taking your text tokens

01:59.200 --> 02:02.360
and turning them into some meaningful numbers, right?

02:02.360 --> 02:04.040
And remember, we're going through a transformer,

02:04.040 --> 02:07.520
so we're not just saying, well, the word that is a five

02:07.520 --> 02:09.920
and the word football is a 17.

02:09.920 --> 02:11.840
What we're doing is we're taking the whole sentence,

02:11.840 --> 02:14.200
we're doing a lot of cross-attention and saying,

02:14.200 --> 02:17.400
this is the overall meaning of a sentence reflected numerically.

02:17.400 --> 02:18.560
So, you get some context.

02:18.560 --> 02:19.960
Yeah, that's the idea.

02:19.960 --> 02:23.400
And clip is trained with image and text pairs.

02:23.400 --> 02:27.320
So, you put in an image, you put in the text that describes that image

02:27.320 --> 02:30.120
and what you try and do is align those two embeddings

02:30.120 --> 02:31.360
so that they kind of make sense

02:31.360 --> 02:35.320
and that way you've got a kind of semantically meaningful text embedding.

02:35.320 --> 02:37.800
So, it's a bit like a supervised data set of some sort.

02:37.800 --> 02:41.760
It is, yeah, it's sort of, it is a supervised data set.

02:41.760 --> 02:45.760
It's trained using a contrastive loss, which is what this CL stands for

02:45.800 --> 02:49.280
and the idea is that basically you want to try and make the embeddings

02:49.280 --> 02:53.280
of an image and its text description very, very similar

02:53.280 --> 02:57.240
and the embeddings of an image with a different text description

02:57.240 --> 02:58.600
very, very different, right?

02:58.600 --> 03:02.120
And not in a dissimilar way to how when we were doing the face ID stuff,

03:02.120 --> 03:05.320
we're trying to put my face near previous shots of my face

03:05.320 --> 03:07.160
so that you can unlock a phone.

03:07.160 --> 03:09.800
Unlock a phone with your face, that's the one.

03:09.800 --> 03:11.840
If you want to unlock a face with your phone,

03:11.840 --> 03:14.160
so you've got a clip embedding, this is the text embedding.

03:14.160 --> 03:16.400
You also have various other things that work.

03:16.400 --> 03:19.200
In Darley, and I'm going to sort of simplify it slightly,

03:19.200 --> 03:24.200
you put in an image which I think is at 64 by 64 pixels.

03:25.600 --> 03:29.160
You put in a noise image at 64 by 64, right?

03:29.160 --> 03:33.160
You put in your time, you put in your clip text embeddings

03:33.160 --> 03:36.440
and you also put them into the network like we described in the previous video.

03:36.440 --> 03:40.120
You have a giant unit structure that produces an estimate for the noise

03:40.120 --> 03:41.480
and you loop.

03:41.480 --> 03:47.360
Now, that produces a not bad image, but only at 64 by 64 pixels.

03:47.360 --> 03:51.000
This process of randomly producing noise,

03:51.000 --> 03:53.200
checking whether it's work, subtracting it, producing it,

03:53.200 --> 03:55.720
this takes a long time at high resolution

03:55.720 --> 03:58.720
and the sort of network you would need would be astronomically big.

03:58.720 --> 04:02.240
So to make that easier, we only run it at 64 by 64.

04:02.240 --> 04:05.680
Now, of course, how do we then make that nice, right?

04:05.680 --> 04:09.120
Because just Darley 2 outputs 1K by 1K images.

04:09.120 --> 04:11.560
The answer is we have another network that does the same thing,

04:11.560 --> 04:13.960
but this time its job is to upsample.

04:13.960 --> 04:16.880
So you basically put in a noisy 64 by 64 and say,

04:16.880 --> 04:19.000
output me the 256 version, right?

04:19.000 --> 04:20.280
And so on and so forth.

04:20.280 --> 04:23.280
So you put this through, I think, two levels of upsampling

04:23.280 --> 04:27.800
to go from 64 to 256 to 1024.

04:27.800 --> 04:29.400
Will you pardon my dumb question?

04:29.400 --> 04:29.840
Yeah.

04:29.840 --> 04:33.840
Are we finally at the point where we can say enhance?

04:33.840 --> 04:34.800
You know what we are?

04:34.800 --> 04:36.480
Yeah.

04:37.440 --> 04:39.600
And it will work exactly like it does in the TV,

04:39.600 --> 04:43.040
where it will just make up nonsense and they'll call it a win.

04:43.040 --> 04:43.840
It works pretty well.

04:43.840 --> 04:47.160
Imagen, Google's version, works in a very similar way.

04:47.160 --> 04:52.000
You have a network that's trained to denoise and generate 64 by 64 images,

04:52.000 --> 04:56.680
guided by text, and then you have two upsampling networks

04:56.680 --> 04:59.280
that go up to 1024.

04:59.280 --> 05:02.400
Stable diffusion does its diffusion process,

05:02.400 --> 05:04.240
sort of in this bit in some sense.

05:04.240 --> 05:09.320
You have what we call an autoencoder, which takes some noise

05:09.320 --> 05:14.680
and turns it into a lower resolution but detailed representation.

05:14.680 --> 05:17.720
You then do the diffusion process this way,

05:17.720 --> 05:21.480
which denoises that latent space.

05:21.480 --> 05:23.920
And then you have the other side of the autoencoder,

05:23.920 --> 05:25.560
which expands it back out into an image.

05:25.560 --> 05:26.720
So this is a different way of doing it.

05:26.720 --> 05:30.240
And the advantage is that this is much lower resolution than this.

05:30.240 --> 05:31.440
And they call it stable diffusion.

05:31.440 --> 05:33.320
There's an argument that is slightly more stable.

05:33.360 --> 05:35.120
I don't know to what extent that's true.

05:35.120 --> 05:37.840
There are some differences in the way that these produce images.

05:37.840 --> 05:41.440
But in all other regards, basically, it's the same kind of process.

05:41.440 --> 05:44.800
You're still doing guidance from text.

05:44.800 --> 05:46.080
You're still putting in T.

05:46.080 --> 05:48.040
It's just that you're now doing it in this latent space

05:48.040 --> 05:49.680
instead of in the full image space.

05:49.680 --> 05:52.480
Think of it like you put it through a zip, right?

05:52.480 --> 05:55.880
And you compress it down and then you do all the diffusion in that space.

05:55.880 --> 05:57.640
And then at the end, you expand it back out again.

05:57.640 --> 05:58.440
That's the idea.

05:58.440 --> 06:01.080
And actually, the autoencoder is very, very good.

06:01.080 --> 06:03.000
You can take an image, you can compress it right down

06:03.000 --> 06:05.160
and it'll still produce much the same image again.

06:05.160 --> 06:06.680
Let's dive into the code and have a look.

06:06.680 --> 06:08.040
Right, so I'm in Google Colab.

06:08.040 --> 06:09.000
Now, for those of you who don't know,

06:09.000 --> 06:12.040
Google Colab is a sort of Jupiter notebook style environment

06:12.040 --> 06:17.720
that allows you to access also Google's GPUs for running machine learning things.

06:17.720 --> 06:19.840
Now, I don't tend to use Google Colab generally

06:19.840 --> 06:23.680
because a lot of our processes last longer than you're really meant to use it for.

06:23.680 --> 06:26.520
But for this, it's excellent, right?

06:26.520 --> 06:30.240
So I've got this code from a guy called Jonathan Whitaker,

06:30.240 --> 06:32.200
which I've then repurposed and done my own stuff with it

06:32.200 --> 06:33.360
and I've been messing about.

06:33.360 --> 06:35.080
And so thanks very much to him for that.

06:35.080 --> 06:36.760
But I've taken it and I've played around.

06:36.760 --> 06:37.760
I've changed the resolution.

06:37.760 --> 06:39.960
I've toyed around with a lot of stuff.

06:39.960 --> 06:42.880
And what I wanted to do was talk through some of these lines of code

06:42.880 --> 06:44.600
so you can see what it is that it's doing.

06:44.600 --> 06:47.760
It's the same exact process that I just described.

06:47.760 --> 06:49.320
It's just a few lines of code to do it.

06:49.320 --> 06:52.600
Now, obviously, there's a lot of deep networks and stuff going on behind the scenes,

06:52.600 --> 06:55.960
but they end up getting abstracted away in function calls

06:55.960 --> 06:58.040
and so it becomes very straightforward.

06:58.040 --> 07:00.120
Okay, I've imported all my libraries already.

07:00.120 --> 07:02.440
And then what we've got here is one go.

07:02.440 --> 07:03.680
We're going to have our text prompt.

07:03.680 --> 07:07.160
And what we want to do is take that text prompt and produce an image, right?

07:07.160 --> 07:12.560
So we have various things that we want it to be, 512 pixels tall, 768 pixels wide.

07:12.560 --> 07:14.880
We're going to run 50 steps of inference

07:14.880 --> 07:17.520
and then a few other things that we can talk about in a moment.

07:17.520 --> 07:20.120
Like, for example, we're going to seed it with the number four.

07:20.120 --> 07:21.040
Now, why four?

07:21.040 --> 07:23.120
Because I don't know, I picked it at random.

07:23.120 --> 07:25.160
I can seed it at 77 if you like.

07:25.160 --> 07:27.240
This allows us to run the exact same code again

07:27.240 --> 07:29.160
and produce the exact same image another time.

07:29.200 --> 07:32.680
If we just used a random seed, if you got to an image you liked,

07:32.680 --> 07:34.880
you accidentally get rid of it, you never get it back, right?

07:34.880 --> 07:38.760
So, but if you change this number, you get entirely different images

07:38.760 --> 07:42.440
because the noise that you start with is entirely different, right?

07:42.440 --> 07:43.520
So let's put in a prompt.

07:43.520 --> 07:44.240
Well, what should we do?

07:44.240 --> 07:45.080
Frogs on stilts?

07:45.080 --> 07:46.960
I think we need to do frogs on stilts.

07:48.320 --> 07:49.480
I mean, this may not work.

07:49.480 --> 07:52.600
I don't, you know, anything else you want to add, like in, you know,

07:52.600 --> 07:54.800
in a park or just just just frogs on stilts?

07:54.800 --> 07:56.520
What about on a stage?

07:57.520 --> 08:00.200
OK, frogs on stilts, on a stage at the theatre, right?

08:00.200 --> 08:00.680
Yeah.

08:00.680 --> 08:04.320
Now, the first thing we have to do is we have to embed this

08:05.320 --> 08:08.760
into some kind of usable space in which the machine learning can work.

08:08.760 --> 08:12.520
So what we do is we tokenize, this is the function that tokenizes

08:12.520 --> 08:16.520
the text input and basically turns it into a numerical code for each word.

08:16.520 --> 08:20.480
And then that goes into the texting coder, which is our clipping beddings.

08:20.480 --> 08:22.680
So that's the bit where it sort of works out the context.

08:22.680 --> 08:26.360
Yeah, that's the transformer that goes, well, OK, this one kind of

08:26.360 --> 08:29.120
goes with this word and then this means they share weights and so on.

08:29.120 --> 08:32.040
And then you go through the transform and you end up with essentially

08:32.120 --> 08:36.960
to us, meaningless numbers, but to this semantic information

08:36.960 --> 08:39.560
on the meaning of the sentence.

08:39.560 --> 08:43.240
We also, because if you remember, we put it through the network twice,

08:43.400 --> 08:45.560
one with the text embeddings and one without.

08:45.560 --> 08:48.720
So we also have to produce a dummy text embedding with nothing in it.

08:48.920 --> 08:51.200
Right. And that's what this unconditioned input is.

08:51.240 --> 08:55.880
Then we're going to text encoder unconditioned embeddings and we get

08:56.400 --> 09:00.160
two text embeddings, one of which is unconditioned and one of which is conditioned.

09:00.240 --> 09:02.040
Right. So this one has fogs on stilts.

09:02.040 --> 09:03.760
This one is just sort of blank.

09:03.760 --> 09:05.040
Now we need to set our scheduler.

09:05.040 --> 09:08.040
Remember, you can choose a scheduler that produces different amounts of noise

09:08.040 --> 09:12.280
at each time step and which one you use will depend on to an extent

09:12.280 --> 09:15.160
the kind of images you want out, but also how you've trained the network.

09:15.320 --> 09:18.520
Where is going to be using the standard one that came with stable diffusion?

09:18.720 --> 09:20.960
And I'm going to run for 50 time steps.

09:21.160 --> 09:26.320
So what this will do is distribute the amount of noise it adds from zero to 50.

09:26.520 --> 09:29.440
Right. So when I say 50, it's going to produce a maximum amount of noise.

09:29.440 --> 09:31.680
And when I say one, it's going to produce a tiny amount of noise.

09:31.960 --> 09:33.400
Right. That's the idea.

09:33.400 --> 09:39.520
And then we're going to actually produce our latent noise that we're going to be diffusing.

09:39.600 --> 09:44.440
So we create a random array of numbers of the right size and we're going to call these

09:44.440 --> 09:46.840
latents and we're going to stick them on the graphics card.

09:46.840 --> 09:49.360
And then we're going to do some scaling to our latents as well,

09:49.360 --> 09:52.960
because the scales of some of these different parts of a network of

09:52.960 --> 09:54.360
difference, you have to move them in and out.

09:54.440 --> 09:55.320
And then we're nearly done.

09:55.400 --> 09:56.200
Like this is our loop.

09:56.480 --> 09:57.880
So how does the loop work?

09:57.880 --> 10:05.360
Well, the first thing we do is we calculate the noise to be added at this particular

10:05.560 --> 10:06.160
iterations.

10:06.160 --> 10:09.240
We're going through all the different time steps and we're going to add a different

10:09.240 --> 10:09.880
amount of noise.

10:09.920 --> 10:13.440
We're going to add this noise to our latent space.

10:13.480 --> 10:13.720
Right.

10:13.720 --> 10:15.360
So basically we're noising up the image here.

10:15.400 --> 10:19.400
Now remember, this is an embedded version of this image, but it is noised.

10:19.440 --> 10:21.440
Then we're going to predict the noise with our unit.

10:21.440 --> 10:26.040
So that is saying how much noise do you think was in this image, such that we can

10:26.040 --> 10:30.160
get back to the original image, bearing in mind this text, and then we can do our

10:30.160 --> 10:31.960
actual classifier for your guidance.

10:32.000 --> 10:32.240
Right.

10:32.240 --> 10:35.640
So what we're going to do is we're going to take our noise prediction with text and

10:35.640 --> 10:37.120
our noise prediction without text.

10:37.320 --> 10:39.280
We're going to calculate the difference and amplify it.

10:39.680 --> 10:43.160
And then we're going to work out what our official noise prediction is.

10:43.320 --> 10:48.120
And then finally, we're going to then use that noise prediction to calculate a

10:48.120 --> 10:51.400
slightly less noise version of the image, which is what this line does here.

10:51.760 --> 10:53.120
And we're going to repeat this process.

10:53.400 --> 10:53.640
Right.

10:53.640 --> 10:54.760
So we repeat the process.

10:54.880 --> 10:57.080
We calculate the new noise at the next time step.

10:57.440 --> 10:58.480
We predict it.

10:58.880 --> 11:01.280
We subtract it away and add a bit more noise.

11:01.320 --> 11:02.360
And we repeat this process.

11:02.360 --> 11:09.160
And the idea is that over 50 iterations, we go from fully noise to some reasonable image.

11:09.880 --> 11:10.440
Shall we see?

11:11.000 --> 11:13.840
OK, so let's run at this resolution.

11:13.840 --> 11:16.520
I'm pushing the amount of image size I can get on this graphics card.

11:16.520 --> 11:18.480
So this is running on your graphics card here.

11:18.520 --> 11:21.880
No, this is running on Google's graphics card over at Google.

11:22.200 --> 11:22.440
Right.

11:22.440 --> 11:23.520
Give me somewhere in London.

11:23.520 --> 11:25.200
Do I owe you another eight pounds for this?

11:26.040 --> 11:26.880
You can give me eight pounds.

11:26.880 --> 11:29.400
No, this is covered under the original eight pounds per month.

11:30.480 --> 11:32.040
But hopefully this won't take a month to record.

11:32.520 --> 11:35.880
So we're choosing 50 iterations for this and because that's a decent amount.

11:35.920 --> 11:36.240
Right.

11:36.240 --> 11:38.840
You'll notice that if you don't do enough iterations, you're trying to move the

11:38.840 --> 11:39.640
noise too quickly.

11:39.640 --> 11:42.080
It becomes a bit unstable, doesn't produce nice results.

11:42.760 --> 11:44.200
Because I've not done this before.

11:44.200 --> 11:45.480
I don't know what these other results will be.

11:45.480 --> 11:46.440
Will it be frogs on stilts?

11:46.440 --> 11:48.240
Will it be bits of wood next to a frog?

11:48.680 --> 11:50.760
Will it be something different because it's failed horribly?

11:51.720 --> 11:52.320
Let's see.

11:54.960 --> 11:55.640
Actually, that's not bad.

11:58.880 --> 11:59.720
That is pretty impressive.

11:59.720 --> 12:02.800
Now there's a weird leg coming out of this fog here, but I would I would say

12:02.800 --> 12:05.040
that is a comparatively successful attempt.

12:05.080 --> 12:07.360
This was produced from a noisy image.

12:08.120 --> 12:12.040
So what we can do is we can change the noise seeds so we can say, you know,

12:12.040 --> 12:16.280
128 and what that will do is create a complete different noise, which will

12:16.280 --> 12:17.800
probably lead to a tiny different image.

12:17.920 --> 12:18.240
Right.

12:18.240 --> 12:21.240
I mean, it's still the same text prompt, so it's still guided in the same way.

12:21.760 --> 12:25.200
But if this allows us to produce see near infinite numbers, basically,

12:25.200 --> 12:27.400
of frogs on stilts, if that's your thing, right?

12:27.720 --> 12:28.480
It is my thing.

12:28.480 --> 12:32.320
Yeah, I've got quite into producing like cityscape, futuristic cityscape.

12:32.320 --> 12:34.680
So I think that's where I spend most of my time on this.

12:38.720 --> 12:41.080
I mean, that's gone a bit wrong, but actually still not bad.

12:41.080 --> 12:42.480
It looks like a kind of stage.

12:43.440 --> 12:44.720
They're just a bit not foggy.

12:45.360 --> 12:46.640
Although yeah.

12:46.640 --> 12:47.400
All right, all right.

12:47.400 --> 12:51.480
OK, so anyway, we could spend, let's say another 20, 30 minutes

12:51.480 --> 12:54.720
producing frogs on stilts, but yeah.

12:54.720 --> 12:56.840
So what you could there's loads of cool stuff you can do.

12:56.840 --> 12:58.720
Presumably you could just automate that.

12:58.720 --> 13:00.200
So it just kept giving you loads of.

13:00.200 --> 13:01.280
And in fact, I've done that right.

13:01.280 --> 13:04.160
So for example, I created some nice pictures of dystopian,

13:04.160 --> 13:07.400
abandoned, futuristic cities with overgrown plants, right?

13:07.400 --> 13:10.440
And then I just put them in a for loop and just produce 200 of them

13:10.440 --> 13:11.360
so I can pick the nice ones.

13:11.400 --> 13:16.360
For example, in here, I've just got a bunch of awesome looking

13:16.360 --> 13:18.520
city vistas, overgrown of plants.

13:18.520 --> 13:20.000
They all look really, really good, right?

13:20.000 --> 13:20.840
I'm quite pleased.

13:20.840 --> 13:24.000
I mean, I've got no use for this, but it's quite fun.

13:24.000 --> 13:27.040
And the other thing is that because you can do image to image guidance,

13:27.040 --> 13:30.360
right, so what you do is you take an image that's your guide image,

13:30.360 --> 13:34.080
you nearly noise it all the way and then you reconstruct, right?

13:34.080 --> 13:37.240
So the noise is somewhat not come from a random place.

13:37.240 --> 13:39.760
Then you can get an image that sort of bears some reflections.

13:39.760 --> 13:41.920
You can say, well, I want a building over here and a tree over here.

13:41.920 --> 13:46.480
So I'll draw them in and then I'll produce this and it will bear the same

13:46.480 --> 13:47.960
that have the same shapes and stuff.

13:47.960 --> 13:50.320
So you can control this process, even if you basically, like me,

13:50.320 --> 13:53.240
have absolute zero artistic ability at all.

13:53.240 --> 13:57.160
To give you an example, what I did was, so if I go down,

13:57.160 --> 13:58.040
let me, let me go.

13:58.040 --> 14:01.440
So this is a picture of my colleague's rabbit, very cute rabbit.

14:01.440 --> 14:05.960
And what I did was I embedded this added noise, but not totally noise

14:05.960 --> 14:07.080
to remove the image.

14:07.080 --> 14:10.160
And then I reconstructed it with the text wooden carving of a rabbit

14:10.160 --> 14:12.440
eating a leaf, highly detailed 4K artisan.

14:12.440 --> 14:13.880
I don't know if the artisan word does anything.

14:13.880 --> 14:15.040
I just thought it'd be fun.

14:15.040 --> 14:16.200
It's trending on art station.

14:16.200 --> 14:17.960
I see a lot of that put on the end of things.

14:17.960 --> 14:19.160
Does that make a difference?

14:19.160 --> 14:20.480
I don't know.

14:20.480 --> 14:24.280
Anyway, and it produce a wooden carving of a rabbit, right?

14:24.280 --> 14:27.040
And if you look at the original image versus this image,

14:27.040 --> 14:31.320
some things have changed, sure, but the shape is roughly the same, right?

14:31.320 --> 14:35.200
So it has guided this process using the original image.

14:35.280 --> 14:37.320
And that's how image to image works.

14:37.320 --> 14:40.360
So if you wanted to create an animation, you could create a quite simple

14:40.360 --> 14:45.240
animation of a rabbit jumping about with no artistic ability, right?

14:45.240 --> 14:48.960
I mean, actually, I would struggle to do even that, but, and then each frame,

14:48.960 --> 14:51.360
you could then use this process to produce it.

14:51.360 --> 14:54.720
At the moment, there's no kind of temporal consistency.

14:54.720 --> 14:55.920
So you will see flickering, right?

14:55.920 --> 14:59.440
If you ever see one of these videos, someone's produced online, it'll look cool,

14:59.440 --> 15:03.240
but maybe not consistent and interesting because each frame might subtly change

15:03.240 --> 15:05.040
things.

15:05.040 --> 15:07.440
But that's the idea, right?

15:07.440 --> 15:09.680
Now, you can do loads of weird stuff, right?

15:09.680 --> 15:11.800
So this mixed guidance is one of my favorite things.

15:11.800 --> 15:13.600
Here we have two text inputs.

15:13.600 --> 15:15.880
And what we're going to do is we're going to embed both of them.

15:15.880 --> 15:21.240
We're actually going to guide the generation using the midpoint of those two, right?

15:21.240 --> 15:24.040
So I'm going to say, OK, I want a rabbit, right?

15:24.040 --> 15:25.960
And I want a frog.

15:25.960 --> 15:29.800
And I want you to produce me a 50-50 rabbit frog, right?

15:29.800 --> 15:32.440
And what it will do is it'll embed both of them and it will do

15:32.440 --> 15:33.440
the exact same process.

15:33.440 --> 15:37.760
It's just that now, its text prompt is halfway between these two embeddings.

15:37.760 --> 15:42.640
So you could potentially come up with a system with sliders, you know, 10-70 frog.

15:42.640 --> 15:45.440
You could, to what amount of frog do you want in this image, right?

15:45.440 --> 15:50.840
I mean, again, not sure what the use case is, but it's quite cool.

15:50.840 --> 15:51.440
Here we go.

15:51.440 --> 15:54.240
So it only takes about, I think, I'm training for 50 steps again.

15:54.240 --> 15:55.560
So I'm running it for 50 steps.

15:55.560 --> 15:56.760
While this work, you could do loads of stuff.

15:56.760 --> 16:00.560
So for example, you could generate an image and then you could take half of it

16:00.560 --> 16:02.960
and try and generate the other half to expand it outwards

16:02.960 --> 16:06.720
and slowly grow your image to make it even higher as one, right?

16:06.720 --> 16:08.360
If you're limited by the resolution.

16:08.360 --> 16:11.360
And there's going to be a lot of people playing around with a lot of different ways to use this.

16:11.360 --> 16:16.480
I've already seen the plugins for Gimp and for Photoshop and stuff.

16:16.480 --> 16:17.440
There it is.

16:17.440 --> 16:18.400
It's a strange one.

16:18.400 --> 16:20.040
We'll put links to the code in the description.

16:20.040 --> 16:20.800
Have a go.

16:20.800 --> 16:23.880
You need to register for Huggingface to get access to the weights originally.

16:23.880 --> 16:27.840
But then you can use something like Google Colab or your own hardware to generate pictures.

16:27.840 --> 16:29.040
And people are having a lot of fun.

16:29.080 --> 16:33.080
There are websites now where you can find cool images and the prompts that we use to generate them

16:33.080 --> 16:34.240
to give you some ideas.

16:34.240 --> 16:35.640
So there's lots of cool stuff to do.

16:35.640 --> 16:37.800
And the rabbit, it's the same shape rabbit.

16:37.800 --> 16:39.320
There's a bit more noise, right?

16:39.320 --> 16:42.280
And then we come over here and we come over here

16:42.280 --> 16:44.360
and we end up with just noise.

16:44.360 --> 16:45.440
It looks like nonsense.

16:45.440 --> 16:46.120
And so the question is...

16:46.120 --> 16:49.080
It takes the same amount of time to make one sandwich, but you've got two people doing it.

16:49.080 --> 16:51.720
So they make twice as many sandwiches each time they make a sandwich.

16:51.720 --> 16:52.560
Same with the computer.

16:52.560 --> 16:55.160
We could either make the computer processor faster or...

