So we looked at clip embeddings and we've talked a lot about using generative AI to produce new
sentences to produce new images and so on and so to understand images all these kind of different
things and the idea was that if we look at enough pairs of images and text we will learn to distill
what it is in an image into that kind of language. So the idea is you have an image, you have some
text and you can find a representation where they're both the same. The argument has gone that
it's only a matter of time before we have so many images that we train this on and such a big
network and all this kind of business that we get this kind of general intelligence or we get some
kind of extremely effective AI that works across all domains. That's the implication. The argument
is and you see a lot in the sort of tech sector from some of these sort of big tech companies
who, to be fair, want to sell products, that if you just keep adding more and more data or bigger
and bigger models or a combination of both, ultimately you will move beyond just recognizing
cats and you'll be able to do anything. That's the idea. You show enough cats and dogs and eventually
the elephant just is implied. As someone who works in science we don't hypothesize about what happens,
we experimentally justify it. So I would say if you're going to say to me that the only upward
trajectory is going to be amazing, I would say go on and prove it and do it and then we'll
see. We'll sit here for a couple of years and we'll see what happens. But in the meantime,
let's look at this paper which came out just recently. This paper is saying that that is not
true. This paper is saying that the amount of data you will need to get that kind of general
zero-shot performance, that is to say performance on new tasks that you've never seen, is going to be
astronomically vast to the point where we cannot do it. That's the idea. So it basically is arguing
against the idea that we can just add more data and more models and we'll solve it. Now this is
only one paper and of course your mileage may vary if you have a bigger GPU than these people and so
on. But I think that this is actual numbers which is what I like because I want to see tables of
data that show a trend actually happening or not happening. I think that's much more interesting
than someone's blog post that says I think this is going to what's going to happen. So let's talk
about what this paper does and why it's interesting. We have clip embeddings. So we have an image,
we have a big vision transformer and we have a big text encoder which is another transformer,
bit like the sort of thing you would see in a large language model which takes text strings,
my text string today, and we have some shared embedded space and that embedded space is just
a numerical fingerprint for the meaning in these two items and they're trained remember across many,
many images such that when you put the same image and the text that describes that image in,
you get something in the middle that matches. And the idea then is you can use that for other tasks
like you can use that for classification, you can use it for image recall. If you use a streaming
service like Spotify or Netflix, they have this thing called a recommender system. A recommender
system is where you've watched this program, this program and this program, what should you watch
next. And you might have noticed that your mileage may vary on how effective that is but actually I
think they're pretty impressive for what they have to do. But you could use this for a recommender
system because you could say basically what programs have I got that embed into the same
space of all the things I just watched and recommend them that way. So there are downstream tasks
like classification and recommendations that we could use based on a system like this.
What this paper is showing is that you cannot apply effectively these downstream tasks for
difficult problems without massive amounts of data to back it up. And the idea that you can apply
this kind of classification on hard things. So not just cats and dogs but specific cats and
specific dogs or subspecies of tree. Or difficult problems where the answer is more difficult than
just the broad category that there isn't enough data on those things to train these models in an
effective way. I've got one of those apps that tells you what specific species a tree is. So
what is it not just similar to that? No, because they're just doing classification or some other
problem. They're not using this kind of generative giant AI. The argument has been why do that silly
little problem where you can do a general problem and solve all your problems. And the response is
because it didn't work. That's why we're doing it. So there are pros and cons for both. I'm not
going to say that no generative AI is useful or these models are incredibly effective for what
they do. But I'm perhaps suggesting that it may not be reasonable to expect them to do very difficult
medical diagnosis because you haven't got the data set to back that up. So how does this paper do
this? Well what they do is they define these core concepts. So some of the concepts are going to be
simple ones like a cat or a person. Some of them are going to be slightly more difficult like a
specific species of cat or a specific disease in an image or something like this. And they come up
with about 4,000 different concepts. And these are simple text concepts. These are not complicated
philosophical ideas. I don't know how well it embeds those. And what they do is they look at
the prevalence of these concepts in these data sets. And then they test how well the downstream
task of let's say zero shot classification or recall the recommender systems works on all of
these different concepts. And they plot that against the amount of data that they had for that
specific concept. So let's draw a graph and that will make me make it more clear. So let's imagine
we have a graph here like this. And this is the number of examples in our training set of a
specific concept. So let's say a cat, a dog, something more difficult. And this is the performance
on the actual task of let's say recommender system or recall of an object or the ability to
actually classify it as a cat. Remember we talked about how you could use this to zero shot classification
by just seeing if it embeds to the same place as a picture of a cat, the text a picture of a cat,
that kind of process. So this is performance. The best case scenario if you want to have an
all powerful AI that can solve all the world's problems is that this line goes very steeply
upwards. This is the exciting case. It goes like this. That's the exciting case. This is the kind
of AI explosion argument that basically says we're on the customer something that's about to happen
whatever that may be, where the scale is going to be such that this can just do anything. Then
there's the perhaps slightly more reasonable, should we say, pragmatic interpretation, which is
like just call it balanced, which is that there's a sort of linear movement. So the idea is that
we have to add a lot of examples, but we are going to get a decent performance boost from it.
So we just keep adding examples, we'll keep getting better, and that's going to be great.
And remember that if we ended up up here, we have something that could take any image and
tell you exactly what's in it under any circumstance. That's kind of what we're aiming for. And
similarly for large language models, this would be something that could write with incredible
accuracy on lots of different topics. Or for image generation, it would be something that could
take your prompt and generate a photo realistic image of that with almost no coercion at all.
That's kind of the goal. This paper has done a lot of experiments on a lot of these concepts
across a lot of models, across a lot of downstream tasks. And let's call this the evidence.
It's all you're going to call it pessimistic now and then.
It is pessimistic also, right? It's logarithmic. So it basically goes like this, right?
It flattens out.
It flattens out. Now, this is just one paper, right? It doesn't necessarily mean that it will
always flatten out. But the argument is, I think, that and it's not an argument they necessarily
make in the paper. The paper is very reasonable. I'm being a bit more cavalier with my wording.
The suggestion is that you can keep adding more examples. You can keep making your models bigger,
but we are soon about to hit a plateau where we don't get any better. And it's costing you millions
and millions of dollars to train this. At what point do you go, that's probably about as good as
we're going to get the technology, right? And then the argument goes, we need something else.
We need something in the transform or some other way of representing data or some other
machine learning strategy or some other strategy that's better than this in the long term if we
want to have this line go up here or this line go up here. That's kind of the argument.
And so this is essentially evidence, I would argue, against the kind of explosion possibility of
that just you just add a bit more data when we're on the cusp of something. We might come back here
in a couple of years, you know, if you'll still allow me on computer file after this absolute
embarrassment of these claims that I made. And we say, actually, the performance has improved
massively, right? Or we might say we've doubled the number of data sets to 10 billion images,
and we've got 1% more on the classification task, which is good, but is it worth it? I don't know.
This is a really interesting paper because it's very, very thorough, right? If there's a lot of
evidence, there's a lot of curves, and they all look exactly the same. It doesn't matter what method
you use, doesn't matter what data set you train on, it doesn't matter what your downstream task is,
the vast majority of them show this kind of problem. And the other problem is that we don't
have a nice, even distribution of classes and concepts within our data set. So for example,
cats, you can imagine are over-emphasized or over-represented in the data set by an order
of magnitude, right? Whereas specific planes or specific trees are incredibly underrepresented
because you just have tree, right? So, I mean, trees are probably going to be less represented
than cats anyway, but then specific species of tree very, very underrepresented, which is why,
when you ask one of these models, what kind of cat is this or what kind of tree is this,
it performs worse than when you ask it what animal is this because it's such a much easier problem.
And you see the same thing in image generation. If you ask it to draw a picture of something really
obvious like a castle where that comes up a lot in the training set, you can draw your fantastic
castle in the style of Monet and it can do all this other stuff. But if you ask it to draw some
obscure artifact from a video game that barely even made it into the training set, suddenly it's
starting to draw something a little bit less quality. And the same with large language models.
This paper isn't about large language models, but the same process you can see actually already
happening. If you talk to something like chatGPT, when you ask it about a really important topic
from physics or something like this, it will usually give you a pretty good explanation of that
thing because that's in the training set. But the question is what happens when you ask it about
something more difficult, right? When you ask it to write that code, which is actually quite difficult
to write, and it starts to make things up, it starts to hallucinate, and it starts to be less
accurate. And that is essentially the performance degrading because it's underrepresented in the
training set. The argument I think is, at least it's the argument that I'm starting to come around
to thinking, if you want performance on hard tasks, tasks that are underrepresented on just
general internet texts and searches, we have to find some other way of doing it than just collecting
more and more data, particularly because it's incredibly inefficient to do this. On the other
hand, these companies will, they've got a lot more GPUs than me. They're going to train on
bigger and bigger corpuses, better quality data, they're going to use human feedback to better
train their language models and things. So they may find ways to improve this up this way a little
bit as we go forward. But it's going to be really interesting to see what happens because I'll,
you know, will it plateau out? Will we see chapter GPT seven or eight or nine be roughly the same as
chapter GPT four? Or will we see another state of the art performance boost every time? I'm kind of
trending this way, but you know, it'll be excited to see if it goes this way. Take a look at this
puzzle devised by today's episode sponsor, Jane Strait. It's called bug bite, inspired by debugging
code. That world we're all too familiar with, where solving one problem might lead to a whole
chain of others. We'll link to the puzzle in the video description. Let me know how you get on.
And speaking of Jane Strait, we're also going to link to some programs that they're running at the
moment. These events are all expenses paid and give a little taste of the tech and problem solving
used at trading firms like Jane Strait. Are you curious? Are you problem solver? Are you into
computers? I think maybe you are. If so, well, you may well be eligible to apply for one of these
programs. Check out the links below or visit the Jane Strait website and follow these links.
There are some deadlines coming up for ones you might want to look at, and there are always
more on the horizon. Our thanks to Jane Strait for running great programs like this and also
supporting our channel. And don't forget to check out that bug bite puzzle.
