Last week, I think it was maybe the week before Apple had one of their usual press conferences and they announced their latest
Possibly last version of the M1 chip, which was the M1 Ultra
And one of the things that they said as they launched it was that they designed it using two M1 Max chips
Basically stuck together using something called Ultra Fusion to join them together. Now, Ultra Fusion is just a marketing buzzword
Literally all they've got is a high-speed interconnect between the two silicon dyes to transfer data between them
But one of the things that they said which was interesting is that the reason they've done this
Was so that you didn't have to write the software in a different way
And I thought it was interesting just to pick up on that and to explain
Why if they hadn't made that interconnect fast enough
You would have to write the software in a different way because if you think about it
All they seem to be doing is adding more cores to the CPU making it a 20-core CPU instead of a 10-core CPU
And you think well if it's a multiprocessor system
And if you watch the videos we've done previously on multiprocessor systems
You're going to have to write the software to split the tasks up over the multiple cores to run
So why are you not going to have to write things differently with this architecture of chip?
So I thought we'd have a look at that today
So to understand what Apple's done, we need to go back to basics and think about how a computer
Actually works and we'll go with the von Neumann model
I know technically most modern CPUs are modified Harvard architecture
But the von Neumann model is good for what we want to look at. We have at the center of our system
the CPU
Whatever we want and that is then connected
To some memory and I'm just going to write RAM here
So it fits into the box
Of course some of it would be ROM and other things and then the other thing that we have in there is we have the IO
And things and that's basically the model we use for a computer
We've got the CPU talking to the RAM where the instructions and data are stored and you can talk to the IO to talk to the rest of the world
So that's things like your
Disk controllers with a solid-state hard disk your graphics card your network card now what happens?
When we have a multi-processor system the general way that we build multi-processor systems
Certainly the ones that we use in laptops or using desktop computers is using what's called a shared memory model
So just as before with the von Neumann architecture
We're going to have a single block of RAM and that's going to be connected
Not to one CPU now
But we'll give it to CPUs. So we've got two
CPUs that it's connected to so it's connected to a shared bus between them and then each of those CPUs are
Connected to it now effectively. That's how you build a multi-processor system. There's a bit more
Involved for example, you need some sort of logic here
for bus
Arbitrations will call that the ball the bus arbitration logic
So you need something to sort of control well, which CPU can talk to the RAM at any one point now
One thing I need to say here is that I've drawn this is the CPU talking directly to the RAM
If you think about it, if you watch the video I did many years ago on CPU caches
You need to have a cache here because otherwise
Only one CPU can ever talk to RAM at the same time
If there's no cache this CPU tries to talk to RAM this one can't
If that's this CPU tries to talk to RAM that one can't at the same time
It would effectively
Result in serializing the operation so you wouldn't get any speed up. You need a cache in there and that
Sort of leads us to the first part of the problem only one CPU can access the RAM at any one point
Now if we've got a cache in our system and I'm going to draw that as a red line
Which sits between the CPU and between the RAM?
That's not a problem because as a CPU accesses data
It stores a local copy in its cache
So when it needs to try and fetch that data or those instructions again it can fetch them from the cache and not access the RAM
So that's absolutely fine
Most of the time we want to get it so the CPUs are satisfying their data and instruction fetches from the cache
And then only occasionally they go to the RAM
So that actually whenever one of the CPU goes to the RAM needs to go to the main memory to fetch a value
Then effectively
It's unlikely to be being used by the occasion that you'll get the situation where they both try and access a value main memory at
The same time at which point that's why you have the bus arbitration logic to say this CPU is going to fetch the value
Then that CPU is going to fetch the value
So we can build a shared memory multiprocessor system like that
I'm going to say relatively straightforwardly. There's a lot involved, but
That's the basic idea of what's going on and we can extend that to have more CPUs
So we can just add another CPU in
Up here so we could have a three CPU system
Normally you'd probably go up to four and things but I've run out of paper
It's got its cache as well and you could extend that for as many CPUs as you like
except
There was a slight issue
we said that
There are occasions where one CPU might be trying to access the memory at the same time as another CPU
Hopefully we can build the cache system
We can load more data than we need each time we fetch things and so on we can build an intelligent memory system that can
Satisfy this so that the probability of that happening is relatively low
But if we think about it if we add more and more CPUs
onto the same shared memory bus
Then we're going to end up with more chance
Of a collision happening of two CPUs trying to access memory at the same time
And the caches on each CPU mitigate that to some effect so that they reduce the probability
Of two things trying to access at a time
But a bit like the old birthday problem, you know the sort of question you ask
If you've got a class of school children
What is the probability that two of them share a birthday in there?
Turns out it's quite likely once you get a bit of about 20 or so children in the class
The same thing applies here as you increase more CPUs the chance that two of them will try and access memory at the same time
increases
As you add more CPUs and so this will scale
Scale, but it'll only scale up to a point once you get past a certain number of CPUs
You will find that you're back to the point where actually more
It's more likely than not that two of them will be trying to access memory at the same time
So we can scale this up to a certain number of CPUs
So does that form a limit? Is there a limit on how many CPUs we can have working together in the multiprocessor system?
Well, not as such because there's another way
We can design a multiprocessor system
So this is what's known as a uniform memory access system
And the reason it's known is that is that for any location
in RAM
Any of these CPUs can access it
With the same sort of speed so it doesn't matter whether it's coming from CPU one up here or CPU three down here
It'll take the same amount of time for them to access the value
In that memory location
Different memory locations may have different speeds your ROM might be slower than the RAM
You may have things mapped in there which are slower still and so on
But for any particular memory location each CPU can access it in the same time or within the same nanosecond ballpark
So it makes no difference in reality as we said that will scale up to a certain number of CPUs
But if we want to take it to beyond that
Then we need to change that system. We need to build a system that no longer has
Uniform memory access rather than from a memory location each CPU being able to access at the same speed
For each memory location the speed it takes to access it or how long it takes for it to access the data value there
Depends on which CPU core is trying to access it. So it might be that for one CPU core it takes
I don't know let's say
100 nanoseconds just picking a time off the top of my head
But for another CPU core it takes 200 nanoseconds
They're just ballpark times and not all the magnitudes are just sort of show there's a difference
Between the two. Okay, let's have a look at how we build a system like that. So what we're talking about is what's referred to
as a non
Uniform memory access system. So non uniform memory access
system or Numa
For short. So how does that differ? Well, let's think about it. It starts off in the same way. We have a block of ram
I'm going to turn the diagram around
Ram like that and that is connected
To our CPUs just as before I'm missing out the caches and the arbitration logic from this diagram just for simplicity
So this looks relatively similar to what we had before we've got a some ram and some CPU cores
Sharing access to it. No difference there with a Numa system though. We also have some other ram
That's part of our system connected to a different set of CPU cores
Over here now at this point you've got effectively two computer systems. These CPUs can access this RAM
These CPUs can access this RAM
The difference in the Numa system is that there is actually a link
between the two
Systems here and you've got a distributed
Shared memory system think of it like a sort of network, but it's often done at
The CPU level and things even within on some between cores
Now what this means is as far as the program's running there is one block of memory
There is so this is if this was 16 gig
And this was 16 gig the programs would see 32 gigabytes. They're not separate blocks of memory
They they're seen by the programs as one block of memory
But the difference is is if we've got a program running on this CPU over here
It's got direct access to this block of memory here
So let's say it takes I don't let's say it takes
100 nanoseconds again to access memory
So we've got 100 nanoseconds to access memory if he wants to access memory in here
It will take 100 nanoseconds to access that memory value
But if the data it's trying to access is over in this memory over here
A CPU over here could access it in 100 nanoseconds
But for this CPU over here a CPU over here
It's got to go over this distributed shared memory connection
From this set of RAM and this set of CPUs to this set of RAM and this set of CPU cores over here
And that would take
A significant amount of time. I mean it would take 100 nanoseconds over here
To get from here to here
So to get from here to here plus this
Let's say this is 200 nanoseconds. I'm just making a number up over here. It's a longer
amount of time
I'm making these numbers off of the top of my head
So don't take them as any sort of things other than so it's longer to go from here over here
So we'd have to go over here
across the distributed shared memory link
To get the value and then we could bring the value back
So rather than taking 100 nanoseconds it would take in the order of 300 nanoseconds
It would take a significantly longer amount of time. So if we build a computer system like this
We have the situation where
Depending where an instruction is in memory or where data is in memory
It could either access it very very quickly on this CPU core if it can go directly to the RAM that it's directly connected to
Or it would end up taking a long time
This relatively to access it because it would have to go over the shared link
And fetch it from the other block of RAM over there
It would still appear to be the same memory system
But we've now got the situation where the access to it depends
On which cpu is trying to access it. So we have what's called a non uniform memory access system
Now originally non uniform memory access systems were the sort of
Domain of high-end cluster system and sort of sgi type workstations and things but these days
You've actually seen it drop down on to sort of workstation type machines
Some of the amd thread ripper some of the higher end intel clusters are all
Numer based systems and what this means is if you want to run that cpu at the fastest possible speed
You need to write your software to take into account
Which cpus have fast access to which bits of ram
So that you can put the data that those cpus are processing and you can put the instructions that they're running
In that block of memory and have the cpu date the cpu instructions and the data that's being executed on these cpus
In this block of memory over here
So they can all access it very very quickly and you only have a very small amount of data
Which is needed to synchronize things and keep things working passing over the shared memory network
Now you can do it and it works great
But you have to write your software knowing where things are and in fact if you look
You can find papers and presentations from companies like netflix
Where they're really trying to optimize the performance of their service to serve the videos to you
I'm sure youtube's doing the same as well, but netflix have actually written about it
Really optimized the speed of serving the videos to you
So they actually have to take into all this account so that the network card is connected to one
cpu
gets that data and doesn't have to go and pass it over the shared memory link to another one which then passes it over
to another one to fetch the data from a
Hard disk and so on and feed it back to you and you get everything's passing over the slow link all the time
You really have to take into account
where things are
Which brings us back to apple's marketing buzzword of m1 ultrafusion
What have apple done with m1 ultrafusion?
Well, effectively they have built a system like this. They've taken two m1 max chips
And glued them together. So you've got two 10 core m1 max chips
Each hacks to sing their own blocks of memory or two blocks each
Which is why you can get up to 128 gig on there because you double the amount of cpu cause you can double the amount of memory
That they can access and what they've built in the middle the thing they call ultrafusion
It's just a very very fast distributed shared memory link between the two
And I think what they've actually done is they've just made it so fast
That actually the time it takes to go across from one cpu core to the other to get the value from the ram and
Push it back into the cpu is so quick the latency is so low
That effectively it behaves as if it was the uniform memory access system
It's fast enough that when the cpu requests the data
It gets it before it actually needs it in which point it doesn't slow it down
So it's a nice system because it means as a programmer. We don't have to worry about where
The data is in relation to the cpu cause which one's attached to which
Core and things to make things run as fast as possible
We can just write our programs and let the operating system and the design of the hardware
Sort out the hard problems of executing it as fast as possible
Chunks and do them all at the same time
So one way for example to make sandwiches faster is that you butter the bread faster
You put the filling in faster. You put the bread faster the other person
It's also got an analysis of where I went wrong says Fred Brooks
Why did he make his name with that and what was it all about?
