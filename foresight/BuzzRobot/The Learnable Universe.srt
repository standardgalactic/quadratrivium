1
00:00:00,000 --> 00:00:10,480
Okay, awesome. So I think then we can kick off our talk today. I'm like really excited

2
00:00:10,480 --> 00:00:19,800
to have you because, you know, every month we have our happy hours and one of the members

3
00:00:19,800 --> 00:00:26,080
who always come to those meetups and we have like really great and fun conversations. And

4
00:00:26,080 --> 00:00:36,200
I asked like who you would love to come to give a talk in our meetups and one of our

5
00:00:36,200 --> 00:00:41,600
community members mentioned about you and they're like really love what you're doing

6
00:00:41,600 --> 00:00:49,120
and like super excited about your work and that's why I really wanted you to come and

7
00:00:49,120 --> 00:00:57,600
share your thoughts with our community. For those who are first time in our meetups,

8
00:00:57,600 --> 00:01:06,160
welcome to our community. I will share a link to our Slack where we hang out and in general my

9
00:01:06,160 --> 00:01:12,320
goal is to facilitate supportive, you know, supportive environment and help each other.

10
00:01:12,320 --> 00:01:17,920
Let's say if you are working on some technical projects and you're stuck and you need some help,

11
00:01:18,080 --> 00:01:25,200
you always can, you know, share your technical requests there. And in general, like this is

12
00:01:25,200 --> 00:01:32,240
basically like, you know, community of like-minded people who do machine learning and data science

13
00:01:32,240 --> 00:01:39,520
and in general think about what is cognition, what is intelligence and today, Yosha our guest

14
00:01:39,520 --> 00:01:46,480
will talk about this and just a couple of words about our speaker, Yosha Bach, who is the principal

15
00:01:48,080 --> 00:01:57,760
scientist at Intel and also his research involves like computational models of cognition and

16
00:01:58,800 --> 00:02:05,200
neurosymbolic AI and maybe Yosha, you can dig more deeply in your research.

17
00:02:06,560 --> 00:02:11,520
Thank you, Sofia. Yes, I'm basically a cognitive scientist. Most of my work in the past has been

18
00:02:11,520 --> 00:02:16,240
in the field of cognitive architectures where I try to understand the relationship between

19
00:02:16,240 --> 00:02:22,400
perception, motivation and cognition and at the moment I am working at interlapse in a group

20
00:02:22,400 --> 00:02:28,960
that is trying to figure out what comes in the future of AI and how to evaluate future AI systems

21
00:02:28,960 --> 00:02:34,800
and understand qualitatively and quantitatively how we can understand and assess the dimensions

22
00:02:34,800 --> 00:02:40,640
in which they exhibit performance. Today is going to be slightly more philosophical talk

23
00:02:40,880 --> 00:02:50,720
and AI is, as many of us are aware of, not just a field that is automating statistics,

24
00:02:50,720 --> 00:02:57,040
but originally it has also been a philosophical project and as philosophical projects go, it's

25
00:02:57,040 --> 00:03:05,840
a relatively large one. It involves, I would say, a few thousand people and it's a very

26
00:03:06,080 --> 00:03:11,120
risky project and this project is also the most interesting and I think most relevant one

27
00:03:11,120 --> 00:03:16,480
that exists in the history of philosophy and of course it's only a very, very small fraction

28
00:03:16,480 --> 00:03:21,520
of what our field is doing. Most of our field is working in engineering and a lot of confusion

29
00:03:21,520 --> 00:03:27,040
emerges because people are confusing AI as a philosophical project with AI as the engineering

30
00:03:27,040 --> 00:03:33,520
project and today I'm going to talk a little bit about the philosophical project. Philosophy

31
00:03:33,520 --> 00:03:38,400
itself can be understood as the realm of all theories where we explore everything that can

32
00:03:38,400 --> 00:03:42,800
possibly true and philosophy is largely done in natural language and natural language is

33
00:03:42,800 --> 00:03:48,080
not very well defined so it's very hard to say something in natural language about the complicated

34
00:03:48,080 --> 00:03:56,080
questions of philosophy that is true because natural languages are so ambiguous and heterogeneous

35
00:03:56,080 --> 00:04:02,240
and have to deal with a world that is not very well described by natural language ultimately

36
00:04:02,880 --> 00:04:08,960
and it's tempting to start out from mathematics to do philosophy and mathematics can be understood

37
00:04:08,960 --> 00:04:14,560
as the realm of all the languages not just the natural languages and here what we are most interested

38
00:04:14,560 --> 00:04:20,640
in are the formal ones, the most simple ones and in these formal languages we can say things that

39
00:04:20,640 --> 00:04:26,640
are true because we define truths formally in mathematics so we get to a very narrow clear

40
00:04:26,720 --> 00:04:32,400
understanding what it means for a statement to be true but the problem with mathematics is that

41
00:04:33,600 --> 00:04:37,760
it's very hard in mathematics because it's so simple to say something interesting about the

42
00:04:37,760 --> 00:04:44,480
world that you're in and it's very hard to say a mathematical sentence about our experience of

43
00:04:44,480 --> 00:04:50,480
the world or about what we care about and so on and about the question of what is meaning,

44
00:04:50,480 --> 00:04:56,080
how are we embedded in the universe, what is agency and so on and formalizing these terms

45
00:04:56,080 --> 00:05:02,320
is something that is so difficult that mathematicians for the most part haven't even started going there

46
00:05:03,280 --> 00:05:09,360
so the question is how can these two realms meet and it probably means that we have to automate

47
00:05:09,360 --> 00:05:15,200
the thing in between mathematics and philosophy which is the mind right if we can

48
00:05:15,200 --> 00:05:21,680
methodize the mind that we can methodize the systems that are able to form theories

49
00:05:22,400 --> 00:05:26,240
and we have to do this by making them autonomous and self-organizing to in some

50
00:05:26,240 --> 00:05:33,200
sense replicate the structures that our own mind is producing so we can say things in mathematics

51
00:05:33,200 --> 00:05:37,920
that are true by building a machine that is making these proofs and explores them and so on

52
00:05:37,920 --> 00:05:44,880
that is basically taking over from us we will have a way to prove our theories about the world

53
00:05:45,840 --> 00:05:52,800
and a big step in the last century was that we discovered that mathematics is some kind of

54
00:05:52,800 --> 00:05:58,080
code base that has been developed over a few thousand years and some of the assumptions

55
00:05:58,080 --> 00:06:04,160
in that code base had been wrong especially the way in which tools was defined and that is

56
00:06:04,160 --> 00:06:09,280
a big implication of the work of Gödel and Turing what they discovered is that we

57
00:06:10,000 --> 00:06:15,520
cannot build a machine in the language of mathematics that in any kind of mathematics

58
00:06:15,520 --> 00:06:20,400
that runs the semantics of the existing mathematics without breaking it's basically what

59
00:06:20,400 --> 00:06:26,000
Gödel and Turing discovered is when you assume that the existing semantics of mathematics are

60
00:06:26,000 --> 00:06:32,880
true you run into contradictions it was a very big shock especially to Gödel who strongly believed

61
00:06:32,880 --> 00:06:41,280
that truth as it was commonly understood was actually the true truth and what Turing discovered

62
00:06:41,280 --> 00:06:47,040
was we can actually build a different machine a computational machine and this computational

63
00:06:47,040 --> 00:06:52,480
machine is able to recover all the semantics that mathematicians were using in practice what

64
00:06:52,480 --> 00:06:59,200
you're losing is infinities continuity and a few other nice things but there is no infinity

65
00:06:59,200 --> 00:07:05,200
in continuity that mathematicians are ever working with continuity is just too many parts to count

66
00:07:05,200 --> 00:07:10,320
in the physical universe right so when you have too many parts to count you have to find operators

67
00:07:10,320 --> 00:07:16,240
that are converging and the operators that are converging in the limit over too many parts to

68
00:07:16,240 --> 00:07:22,720
count this is geometry so now we have a new perspective a computational perspective on

69
00:07:22,720 --> 00:07:27,600
everything and this computational perspective already existed with the mathematics it was called

70
00:07:27,600 --> 00:07:30,320
constructive mathematics is the part that actually works

71
00:07:32,640 --> 00:07:39,200
so we could say that AI is a philosophical project is as its goal to unify mathematics

72
00:07:39,200 --> 00:07:44,800
and philosophy using computational models of the mind you could also look at it in a different way

73
00:07:45,360 --> 00:07:51,440
it is basically exploring the question of what intelligence is what is it that the mind is doing

74
00:07:51,520 --> 00:07:57,600
when it's making models and if you're able to succeed in this task of building a system that

75
00:07:57,600 --> 00:08:04,080
is able to make models in the same way as we do it and what we are building is a system that is able

76
00:08:04,080 --> 00:08:10,160
to understand how minds work so in some sense the goal of the Turing test should be to build a system

77
00:08:10,160 --> 00:08:16,880
that you can ask how it works right if it's able to explain to you how it a mind works and how you

78
00:08:16,880 --> 00:08:22,480
work you have succeeded and in some sense this Turing test yet performing on such a system is

79
00:08:23,440 --> 00:08:29,600
built a system that is also able to perform Turing tests on you and on itself and that system is

80
00:08:29,600 --> 00:08:35,600
able to explain to you what it is and the question that Turing is also trying to answer in this way

81
00:08:35,600 --> 00:08:40,960
is is Turing generally intelligent is Turing able to explain how one mind works by building one

82
00:08:41,840 --> 00:08:48,960
AI is a philosophical project in a formal sense started quite early maybe with Leibniz who had

83
00:08:48,960 --> 00:08:53,760
this idea that we can build a calculus that is describing everything in the world and we can

84
00:08:53,760 --> 00:09:00,480
just calculate the answer and my own philosophy teachers have been very dismissive of this project

85
00:09:00,480 --> 00:09:05,680
how could you turn everything into numbers and then do calculations what a naive idea and I

86
00:09:05,680 --> 00:09:10,560
don't think it was that naive at all it was actually this idea of building a machine that

87
00:09:10,560 --> 00:09:16,240
can perform these calculations and this universal calculus is something that has kept a lot of

88
00:09:16,240 --> 00:09:23,440
people busy like Frager with the Begriffs Calcule and so on and in the last century the most

89
00:09:23,440 --> 00:09:30,160
prominent start into this was arguably Ludwig Wittgenstein who had this idea of formalizing

90
00:09:30,160 --> 00:09:35,920
the language this that philosophers are using by basically making English much more formal and

91
00:09:35,920 --> 00:09:42,000
strict it's a very beautiful idea and this idea of turning English into a programming language for

92
00:09:42,000 --> 00:09:48,320
thought is expressed in the Tractatus Logico philosophical study wrote as a young man coming

93
00:09:48,320 --> 00:09:54,000
back from World War one it's a very beautiful book because it is not tainted by footnotes

94
00:09:54,720 --> 00:10:03,760
and references and arguments it's just a very clear elaboration of this particular sort but

95
00:10:03,760 --> 00:10:08,320
you can understand it best if you had this thought by yourself it's not trying to convince anybody

96
00:10:08,320 --> 00:10:14,160
he also makes that point and explains that it's basically not trying to make an argument that

97
00:10:14,160 --> 00:10:20,320
is convincing other philosophers it is an idea to write down a particular thought that you will

98
00:10:20,320 --> 00:10:27,040
understand after you had it and so it's a book that can be understood I think quite well by somebody

99
00:10:27,040 --> 00:10:32,800
who is a programmer who has thought about writing a language for thought but it's a book that

100
00:10:32,800 --> 00:10:36,880
endlessly confused philosophers in the last century because they didn't know what you really meant

101
00:10:38,080 --> 00:10:45,760
and in this remarkable book he preempts Minsky's logistic project the idea of writing a language

102
00:10:45,760 --> 00:10:51,040
in which you can do AI by several decades and also its failure because at the end of his life he

103
00:10:51,040 --> 00:10:56,480
concludes that it didn't really work because he was not able to integrate perception or as he

104
00:10:57,200 --> 00:11:02,960
mentioned pictures or images into his formal language and this was something that is only

105
00:11:02,960 --> 00:11:08,080
now happening this deep learning where we are developing automatic functional approximators

106
00:11:08,080 --> 00:11:12,240
that can deal with perceptual content and integrate this into the formal models that we

107
00:11:12,240 --> 00:11:18,880
are building so Wittgenstein couldn't see this yet but Turing was one of his pupils and you know

108
00:11:18,880 --> 00:11:26,800
how that went. Classical AI is mostly symbolic and classical AI was the stuff that is very

109
00:11:26,800 --> 00:11:31,840
simplistic and clear not as clear kind of teaching or practice but the idea was that you analyze a

110
00:11:31,840 --> 00:11:36,640
problem you find an algorithm to solve it and use this example as chess the all the early

111
00:11:36,640 --> 00:11:42,800
chess programs were written in a way where the developers thought about how can you friend an

112
00:11:42,800 --> 00:11:49,840
automatic strategy to play chess and the nets implement the strategy and make it fast and efficient

113
00:11:49,840 --> 00:11:55,440
enough optimize it enough so it can beat human players and currently we are in a different era

114
00:11:55,440 --> 00:12:01,680
of AI that's this deep learning era it's mostly sub-symbolic where we don't write the solution

115
00:12:01,680 --> 00:12:06,000
to the problem but we write an algorithm that learns the solution to the problem that discovers

116
00:12:06,000 --> 00:12:11,920
the solution to the problem by itself and it's tempting to think that the next era of AI will

117
00:12:11,920 --> 00:12:17,920
be about meta learning so we don't write algorithms that discover the solution to a problem but

118
00:12:17,920 --> 00:12:23,440
algorithms that discover how to discover the solution to the problem that learn how to learn

119
00:12:25,360 --> 00:12:30,800
at the moment we are using mostly neural networks and the neural network is a chain of weighted

120
00:12:30,800 --> 00:12:37,360
sums of real numbers and debates in this network are changed with various algorithms mostly

121
00:12:37,360 --> 00:12:41,520
stochastic gradient descent and there are many alternatives to neural networks so it's not the

122
00:12:41,520 --> 00:12:48,400
only way the entire goal is to make compositional function approximation and the thing that works

123
00:12:48,400 --> 00:12:53,200
best in practice for many of the tasks that we have picked is stochastic gradient descent which

124
00:12:53,200 --> 00:12:59,280
means in some sense differentiable programming and the main paradigm for that is still Frank

125
00:12:59,280 --> 00:13:05,040
Rosenblatt's perceptron from 1958 and he was not the only one who discovered it and back then in

126
00:13:05,040 --> 00:13:09,600
his work he didn't discover back propagation yet even though he could already see what would need

127
00:13:09,600 --> 00:13:16,320
to happen to make it to to pull it off. Minsky and Pappert delivered a formal proof that the

128
00:13:16,320 --> 00:13:21,200
perceptron learning algorithm that existed in Frank Rosenblatt's perceptron was not able to

129
00:13:21,200 --> 00:13:27,840
learn XOR and by extension many many other functions and so this book by Minsky and Pappert

130
00:13:27,840 --> 00:13:33,520
which they wrote to point out that we would need more complicated system Minsky preferred

131
00:13:33,520 --> 00:13:38,800
to make systems that are more proposition based language based knowledge based

132
00:13:40,400 --> 00:13:46,640
stopped research on neural networks and funding especially on your not the research but the

133
00:13:46,640 --> 00:13:52,320
large-scale funding for more than a decade and delay the development of connection systems

134
00:13:52,320 --> 00:13:57,840
but in some sense this has never stopped at the moment it's the most successful paradigm

135
00:13:57,840 --> 00:14:04,480
in AI arguably and what we find is that there are no longer black boxes if you look at this

136
00:14:04,480 --> 00:14:10,640
two part the excellent work by Chris Oler's team at Open AI there it's possible to analyze what

137
00:14:10,640 --> 00:14:15,360
these networks are doing. His hypothesis is that features are the fundamental unit of neural

138
00:14:15,360 --> 00:14:20,080
networks and they correspond to directions in an embedding space and the features are connected

139
00:14:20,080 --> 00:14:26,960
by weights and form circuits and that there is a universitality condition which means that

140
00:14:26,960 --> 00:14:32,480
analogous features and circuits will form in different models if you give them similar tasks

141
00:14:32,480 --> 00:14:38,560
and the model is flexible enough but these artificial neurons are very very different

142
00:14:38,560 --> 00:14:44,160
from biological neurons which are self-organizing. The biological neurons are basically all little

143
00:14:44,160 --> 00:14:50,800
animals each neuron is a little animal and this little animal tries to survive and to survive

144
00:14:50,800 --> 00:14:55,440
it needs to get fed and it will get fed if it does the right thing from the perspective

145
00:14:55,440 --> 00:15:01,120
of the surrounding organism and the right thing is that it needs to fire at the right moment so

146
00:15:01,120 --> 00:15:06,080
every neuron has to learn when to fire based on its current environment and the current environment

147
00:15:06,080 --> 00:15:11,840
is from the perspective of the neuron a certain electrical and chemical configuration of the

148
00:15:11,840 --> 00:15:16,240
environment so it has to figure out which signals an environment signal that they should

149
00:15:17,120 --> 00:15:22,480
fire and the neuron will put tendrils into the world to make that measurement to figure out

150
00:15:22,480 --> 00:15:28,880
when it should fire and there are different types of neurons that have different biases in their

151
00:15:28,880 --> 00:15:34,720
strategy to explore the space of possible solutions to the questions of when it should fire

152
00:15:34,720 --> 00:15:38,560
and they are a form and organization they have a shared destiny they all lock together on the

153
00:15:38,560 --> 00:15:44,240
same dark skull and the only way that they can survive is that they collaborate in a way and

154
00:15:44,240 --> 00:15:49,920
together give each other feedback on when they should fire right and we know what the overall

155
00:15:49,920 --> 00:15:56,160
result is you get this emergent structure in it that from the outside or from a certain distance

156
00:15:56,160 --> 00:16:00,720
looks like there is a common spirit that is evolving in the coherent patterns of the firing

157
00:16:00,720 --> 00:16:06,400
of the neurons this virtual thing that we call a mind and for me it was a very big insight to

158
00:16:06,400 --> 00:16:12,720
realize that the word spirit is not the superstition but that spirit is basically an operating system

159
00:16:12,720 --> 00:16:18,320
for an autonomous robot and when the word spirit was invented the only autonomous robots were known

160
00:16:18,320 --> 00:16:24,080
were not built by people of course there were people themselves and other organisms like plants

161
00:16:24,080 --> 00:16:29,360
and animals and there were ecosystems and cities and nation states all these systems basically

162
00:16:29,360 --> 00:16:35,040
have emergent virtual software that you can project into the coherent functioning of their

163
00:16:35,040 --> 00:16:42,160
elements to make them explainable so in some sense you get a system from the organized coherent

164
00:16:42,160 --> 00:16:49,600
activity of its parts that behaves as if it was following a shared telos a shared purpose

165
00:16:49,600 --> 00:16:54,080
a shared structure is their shared computational strategy and could be described with a single

166
00:16:54,080 --> 00:17:00,320
software but what is it aiming at and Carl Friston says what it's aiming at is the minimization of

167
00:17:00,320 --> 00:17:06,320
free energy and what is meant here is not some thermodynamic energy but you can use an energy

168
00:17:06,320 --> 00:17:11,680
description to describe the state of the system that is modeling its environment and that energy

169
00:17:11,680 --> 00:17:18,640
is minimized then the system needs less energy to update in the next state and so the idea here

170
00:17:18,640 --> 00:17:24,800
is that the system whether it's a brain or a cell is modeling its entanglement with the

171
00:17:24,800 --> 00:17:30,720
environment in its own internal state and it's trying to minimize the prediction error and

172
00:17:33,520 --> 00:17:38,880
now to understand how to do this in a self-organizing system technological system like

173
00:17:38,880 --> 00:17:43,280
neural networks use a functional design you think about what does the individual representation

174
00:17:43,280 --> 00:17:48,480
unit need to do and then we impose that function on it it just does it because we build it on a

175
00:17:48,480 --> 00:17:52,640
completely deterministic substrate our computers are designed to be more or less completely

176
00:17:52,640 --> 00:17:58,720
deterministic and follow the rules that we give them and don't deviate from this ever and biological

177
00:17:58,720 --> 00:18:03,920
systems or social systems are not like this here we need a meta design we need to design them in

178
00:18:03,920 --> 00:18:10,960
such a way that they want to converge to the desired functionality right so instead of building a

179
00:18:10,960 --> 00:18:20,240
FDA that is following a set of rules and then it's gatekeeping the access of to medications

180
00:18:20,240 --> 00:18:26,160
in such a way that people don't poison themselves and so on if you just implement these rules it's

181
00:18:26,160 --> 00:18:30,880
not going to work because after a couple of generations the FDA is going to be captured by

182
00:18:30,880 --> 00:18:37,200
the producers of medication and is acting as a gatekeeper against innovation that would prevent

183
00:18:38,640 --> 00:18:45,360
new better cheaper medication to enter the system and eventually it's going to also limit the access

184
00:18:45,360 --> 00:18:50,480
of people to completely save medication that doesn't have patents on it so nobody makes a profit on

185
00:18:50,480 --> 00:18:57,520
it right so if you want to prevent the capturing of regulators of other social systems you will need

186
00:18:57,600 --> 00:19:02,240
to design them in such a way that they want to do the right thing and if you cannot do that you

187
00:19:02,240 --> 00:19:07,040
need to make the model you need to make sure that they die every generation and get replaced by a

188
00:19:07,040 --> 00:19:12,800
fresh system maybe we should think about how to make model institutions that die after a certain

189
00:19:12,800 --> 00:19:20,960
time so they can rejuvenate and rebuild anyway biological neurons are not just functional

190
00:19:20,960 --> 00:19:26,880
approximators they are agents how can we understand what an agent is and here what helps us is to

191
00:19:26,960 --> 00:19:34,240
use cybernetics and cybernetics we can start to define what it means to be motivated and

192
00:19:35,200 --> 00:19:41,760
the core of cybernetics is as you all know the feedback loop and a feedback loop if we want to

193
00:19:41,760 --> 00:19:47,760
turn this into an agent we take a controller the controller is connected to a system that it regulates

194
00:19:47,760 --> 00:19:53,360
via a factors something that can change the state of the regulated system and sensors and the sensors

195
00:19:54,000 --> 00:19:58,400
measure a set point deviation a deviation from how the regulated system should be

196
00:19:59,280 --> 00:20:05,440
and the controller is now implementing a model that tells it how to go from the measured set

197
00:20:05,440 --> 00:20:10,720
point deviation to a change in the effectors that affect the regulated system and the regulated

198
00:20:10,720 --> 00:20:15,760
system goes out of whack again and again because it's being disturbed by the environment and the

199
00:20:15,760 --> 00:20:21,040
better the controller is able to model these disturbances the better it's going to be able to

200
00:20:21,040 --> 00:20:26,480
deal with the set point deviations over a long time span in the simple case you have a thermostat

201
00:20:26,480 --> 00:20:31,200
that is only optimizing for the next moment for the next frame of the system but if you have a

202
00:20:31,200 --> 00:20:37,520
system like us you're able to optimize for an integral over the set point deviations over a

203
00:20:37,520 --> 00:20:43,600
long time span and to do this we need to model the future and if the controller is able to model

204
00:20:43,600 --> 00:20:48,640
the future and the structure of the environment then it's going to distinguish between situations

205
00:20:48,720 --> 00:20:54,080
that it prefers over other situations and it's going to implement strategies and how to get to

206
00:20:54,080 --> 00:20:58,400
these situations at once and when we describe the system from the outside we will attribute

207
00:20:58,400 --> 00:21:02,720
agency to it because this is what an agent is it's a controller combined with a set point

208
00:21:02,720 --> 00:21:08,320
generator that is able to act on something that it regulates by modeling the future of that

209
00:21:08,320 --> 00:21:14,960
regulated system and acting on the model of that future and the good regulators theorem states

210
00:21:14,960 --> 00:21:19,680
that every good regulator of a system must be a model of that system or must implement a model

211
00:21:19,680 --> 00:21:25,120
of that system which also means that you cannot model the world efficiently if you don't make

212
00:21:25,120 --> 00:21:30,240
a truthful model of it right so when you for instance try to build a more just world it doesn't

213
00:21:30,240 --> 00:21:35,360
help if you use categories that don't model the world as it is but as you want it to be

214
00:21:36,000 --> 00:21:40,560
that is not going to lead to the best possible regulation if you lie to yourself about the

215
00:21:40,560 --> 00:21:47,200
true state of the world your regulation is going to be off so if you want to model the world if

216
00:21:47,200 --> 00:21:51,760
you want to improve the world if you want to control something regulate in the right direction

217
00:21:51,760 --> 00:21:56,560
you need to start with a complete service to truth and you need to come to the description

218
00:21:57,280 --> 00:22:02,640
of the system that you want to regulate that is isomorphic to the dynamics of that system

219
00:22:02,640 --> 00:22:08,480
at the level that you want to regulate it and the universe that we find ourselves to be in

220
00:22:08,480 --> 00:22:14,320
can be understood as a universe that can be controlled and I think that's the answer to

221
00:22:14,320 --> 00:22:19,600
this big conundrum why is it possible that we can learn anything at all why is it possible

222
00:22:19,600 --> 00:22:24,320
that we can recognize structure in the universe why are we in a universe that is intelligible

223
00:22:25,040 --> 00:22:30,800
it's because we are controllers we are on top of a hierarchy of control systems so in a way

224
00:22:30,800 --> 00:22:36,480
you could say that elementary particles are controlled zero point fluctuations

225
00:22:37,520 --> 00:22:45,200
and the atoms are controlled elementary particles and molecules are controlled particles and cells

226
00:22:45,200 --> 00:22:50,080
are controlled molecules and organisms are controlled cells and societies are controlled

227
00:22:50,080 --> 00:22:56,640
organisms and so on right so we have a hierarchy of control and because this to control you need

228
00:22:56,640 --> 00:23:00,800
to make a model of the underlying system it means that a controllable universe is also one

229
00:23:00,800 --> 00:23:06,080
that can be modeled and the models that an atom needs to make of the elementary particles

230
00:23:06,080 --> 00:23:11,040
of course extremely simple because the it's an extremely simple mechanical regulation

231
00:23:11,680 --> 00:23:17,040
but the models that cell needs to make to control the molecules that make up the cell

232
00:23:17,840 --> 00:23:23,360
are very very complicated and there is something like a shift in the complexity in that shift

233
00:23:23,360 --> 00:23:29,040
in the complexity is means that the cell in order to enact this regulation of the molecules

234
00:23:29,680 --> 00:23:34,400
needs to implement a Turing machine it needs to be a computer if the cell cannot compute

235
00:23:34,400 --> 00:23:39,600
it's not complicated enough to learn how to model the future of these molecules well enough

236
00:23:39,600 --> 00:23:44,720
to build this giant super molecule the cell this all its dynamics it is able to withstand many many

237
00:23:44,720 --> 00:23:50,720
types of disturbances in the universe so the purpose of all this regulation is to maintain

238
00:23:50,720 --> 00:23:56,560
complexity to build systems that are stable against disturbances a self propagating and you

239
00:23:56,560 --> 00:24:03,840
disturb them over a larger range of environments we can ask ourselves is the universe is a

240
00:24:03,840 --> 00:24:11,600
computer and isn't it a dynamical system and the answer to this is that well there is no true

241
00:24:11,600 --> 00:24:18,320
continuity for mathematical reasons because if you want to talk about infinity in any kind of

242
00:24:18,320 --> 00:24:23,040
language you will run at some point into contradictions if you try to explain how that works

243
00:24:23,760 --> 00:24:27,760
which means that your word don't mean anything anymore right if you if the language in which

244
00:24:27,760 --> 00:24:33,120
you try to talk about the world does falls apart it means that the words lose their meaning you

245
00:24:33,120 --> 00:24:37,520
cannot actually talk about infinity without presupposing that you already know what you're

246
00:24:37,520 --> 00:24:44,320
talking about and so we can replace that notion of infinity by too many parts to count and then

247
00:24:44,320 --> 00:24:50,480
we get all the same things that we wanted without the contradictions and the world that we are in

248
00:24:50,480 --> 00:24:55,200
is one that is made of too many parts to count so the dynamical systems if used to describe the world

249
00:24:55,200 --> 00:25:00,880
are the same as before but they turn out to be computational systems vice versa the computers

250
00:25:00,880 --> 00:25:05,520
that we built to describe the universe are built on top of the dynamical systems and we basically

251
00:25:05,520 --> 00:25:10,320
stack the probabilities of these systems until they become deterministic enough for our purposes

252
00:25:10,560 --> 00:25:17,360
but the another question you might ask is are quantum systems computational systems

253
00:25:18,240 --> 00:25:24,000
or are they hypercomputers aren't quantum systems able to compute things that classic computers cannot

254
00:25:25,360 --> 00:25:31,520
and well if you look at what a computer is if you look at the church during these is a computer

255
00:25:31,520 --> 00:25:35,520
is in some sense a system that is able to go from state to state in an unrandom fashion

256
00:25:36,080 --> 00:25:42,240
it doesn't get much more general than that and the quantum system is also nothing but that right

257
00:25:42,240 --> 00:25:48,160
the quantum system is characterized by a state that is a superposition of possible states from

258
00:25:48,160 --> 00:25:54,160
a certain angle it's still a state and then you have a transition between them to the next state

259
00:25:54,800 --> 00:26:00,720
and so a quantum computer is also just a computer but the reason why quantum computers can do things

260
00:26:00,720 --> 00:26:07,040
that particle computers cannot is that according to quantum mechanics the particle universe is

261
00:26:07,040 --> 00:26:14,320
very inefficiently implemented on top of the quantum substrate right if you are implementing a

262
00:26:14,320 --> 00:26:19,440
computer in minecraft from redstone you can do that there is going to be a polynomial time

263
00:26:19,440 --> 00:26:25,120
relationship between the speed of the computer in that you implement vision minecraft and the cpu

264
00:26:25,120 --> 00:26:30,320
that minecraft runs on of course the computer that runs is inside of minecraft is going to be

265
00:26:30,800 --> 00:26:35,920
much much slower than the computer that minecraft runs on because most of the computations of the

266
00:26:35,920 --> 00:26:40,880
computer that minecraft runs on will not go into the computations of the computer that you

267
00:26:40,880 --> 00:26:46,480
build vision minecraft right so you're simulated in game world computer is going to use only a

268
00:26:46,480 --> 00:26:52,000
fraction of the computational resources but according to quantum mechanics the particle

269
00:26:52,000 --> 00:26:59,200
universe is so inefficiently implemented on top of the quantum universe that the quantum universe

270
00:26:59,200 --> 00:27:04,480
is branching off in many many ways and most of the computations of the quantum universe are not

271
00:27:04,480 --> 00:27:11,840
contributing to our timeline and so it's only a very small fraction that drips into the available

272
00:27:11,840 --> 00:27:18,160
timeline and quantum computers are basically the bold hypothesis that we can tap into our

273
00:27:18,160 --> 00:27:23,360
quantum cpu that and the quantum substrate that the particle universe runs on and use some of the

274
00:27:23,360 --> 00:27:28,720
additional computations that are not available to the particle universe to drive the computations

275
00:27:28,720 --> 00:27:36,480
that we want so quantum mechanics is also not a hyper computational notion hyper computation is

276
00:27:36,480 --> 00:27:41,040
it's another one that we could take it all a causal hyper computation imagine we could build a

277
00:27:41,040 --> 00:27:48,320
closed time lag loop that means for instance we can somehow look into the future and get the lottery

278
00:27:48,320 --> 00:27:53,360
numbers of next week and use them now to win the lottery right this would be something that is not

279
00:27:53,440 --> 00:27:59,120
possible in computer right well of course it is possible you just back up the state of the

280
00:27:59,120 --> 00:28:03,680
present universe right you make a copy of the universe is right now buffer it then you run the

281
00:28:03,680 --> 00:28:11,120
universe to next week take the lottery numbers store them reboot the universe from the state that

282
00:28:11,120 --> 00:28:16,560
you have in the buffer and then pass the numbers in right so as long as you're able to memorize the

283
00:28:16,560 --> 00:28:21,520
state of the present universe you're good you can also build close time lag loops in the classical

284
00:28:21,520 --> 00:28:27,360
computer so there is in some sense no way to get out of this and we can ask ourselves is there

285
00:28:27,360 --> 00:28:32,800
something about consciousness that requires us to move away and I don't think there is something

286
00:28:32,800 --> 00:28:38,240
about consciousness that is very special the issue with consciousness is very confusing because we

287
00:28:38,240 --> 00:28:43,680
think that our consciousness gives us access to the physical universe and what our consciousness

288
00:28:43,680 --> 00:28:49,840
is perceiving is the real world but it's not consciousness is a dream state it's the state

289
00:28:49,840 --> 00:28:54,960
inside of a model it's something like a multimedia story that is being generated inside of the agent

290
00:28:55,520 --> 00:29:00,480
physical systems cannot be conscious neurons cannot be conscious computers cannot be conscious

291
00:29:00,480 --> 00:29:06,640
consciousness is an entirely virtual property consciousness is as if and because we live in

292
00:29:06,640 --> 00:29:12,880
this as if world we can perceive things as real because being in a physical world being in a

293
00:29:12,880 --> 00:29:18,240
computer doesn't feel like anything right some people think that computers cannot be conscious

294
00:29:18,880 --> 00:29:24,640
that simulations cannot be conscious but they have to have it backwards you can only be conscious

295
00:29:24,640 --> 00:29:29,360
in a simulation because it's a simulated property not a real one it cannot be real

296
00:29:30,800 --> 00:29:35,360
and then there's this big question of existence how is it possible that something exists at all

297
00:29:36,000 --> 00:29:41,280
and to this one it looks very unsatisfying from the perspective of a computational list

298
00:29:41,920 --> 00:29:48,080
and the easiest answer that I found so far is that maybe existence is the default right so

299
00:29:48,080 --> 00:29:52,400
rather than assuming that you need to add something to the universe to call something into

300
00:29:52,400 --> 00:29:58,800
existence the universe is already the superposition of all the things that could exist and if for

301
00:29:58,800 --> 00:30:03,760
something to exist it must be implementable I think a good definition for existence is

302
00:30:03,760 --> 00:30:08,640
implementation something exists if it's implemented and everything that's implementable are finite

303
00:30:08,640 --> 00:30:14,640
automata so maybe the universe is a superposition of all finite automata and the structure of the

304
00:30:14,640 --> 00:30:20,320
universe is the result of those things that don't exist in the superposition of all finite

305
00:30:20,320 --> 00:30:25,520
automata basically the wakes of certain operators that create gaps in existence

306
00:30:27,440 --> 00:30:32,960
so maybe the universe after all is something like an inverse computer it's a superposition

307
00:30:32,960 --> 00:30:38,240
of all the operators and if you superimpose all the operators there's still going to be some states

308
00:30:38,240 --> 00:30:43,680
that are not attainable and this gives the structure to the universe but I don't know whether

309
00:30:43,680 --> 00:30:48,560
that's true that's extremely speculative I don't have an answer to the conundrum by something

310
00:30:48,560 --> 00:30:53,360
exists at all which is the one that really shocks me that is satisfying to me

311
00:30:54,400 --> 00:31:01,200
anyway let's go back to agency we pointed out that to generalize control we take this control

312
00:31:01,200 --> 00:31:06,240
model the controller minimizes z point deviation the minimization of the future z point deviation

313
00:31:06,240 --> 00:31:10,320
requires the controller to make a model and the model has to predict the result of the

314
00:31:10,320 --> 00:31:16,640
interaction has to make something like causal model and an agent is a controller combined

315
00:31:16,640 --> 00:31:21,920
with a z point generator and the controller needs to be able to model the future and has

316
00:31:21,920 --> 00:31:28,480
preferred states and I would say that a sentient agent is one that is discovering itself in this

317
00:31:28,480 --> 00:31:33,200
interaction that's the world right so once you discover your own agency and that you discover

318
00:31:33,200 --> 00:31:38,240
that there is a system that is changing the world in a particular way and that system is using the

319
00:31:38,240 --> 00:31:41,840
contents of your own control model you discover your own first person perspective

320
00:31:44,400 --> 00:31:45,760
so how do we make a model

321
00:31:48,960 --> 00:31:53,040
the general form of a perceptual model is that it encodes patterns to predict other

322
00:31:53,040 --> 00:31:57,920
present and future patterns and you need a network of relationships between the patterns

323
00:31:57,920 --> 00:32:02,800
which are constraints which basically say if something is like this it other things in

324
00:32:02,880 --> 00:32:08,880
universe lead to be like that and the three parameters between these invariances are variables

325
00:32:08,880 --> 00:32:13,760
that hold state to encode the remaining variance state of the universe right so you have some

326
00:32:13,760 --> 00:32:19,200
patterns and these patterns are mapped onto hidden states and these hidden states are the

327
00:32:19,200 --> 00:32:24,400
world states that we use to explain the world and each of these variables has a set of possible

328
00:32:24,400 --> 00:32:31,600
values and the relations between the variables are computable functions that constrain these

329
00:32:31,600 --> 00:32:37,520
variables depending on other variables and they also constrain future states of the sensory

330
00:32:37,520 --> 00:32:42,640
patterns and you will try to minimize this deviation in these predictions we try to minimize

331
00:32:42,640 --> 00:32:47,600
the contradictions in the model and the relationships here are not probabilistic they are

332
00:32:47,600 --> 00:32:53,680
probabilistic because you don't want just to model the most probable universe you want to model any

333
00:32:53,680 --> 00:32:58,800
universe that is possible if there is a tiger that is coming after you that even if tigers are very

334
00:32:58,880 --> 00:33:03,040
improbable to observe in your world you should still be able to see the tiger all right when it's

335
00:33:03,040 --> 00:33:09,040
coming after you right so let's make sure that we can model everything that is possible in this way

336
00:33:09,040 --> 00:33:15,680
in your universe not just the things that are probable probability comes in when you want to

337
00:33:15,680 --> 00:33:21,360
let your model to converge because the state of possible states the space of possible states that

338
00:33:21,360 --> 00:33:27,200
your model can be in is extremely large and so getting to convergence to a state of the internal

339
00:33:27,200 --> 00:33:31,680
system that is able to predict the sensory patterns that is very difficult so imagine you

340
00:33:31,680 --> 00:33:35,760
wake up in the morning you don't know where you are you don't know what's the case how do you get

341
00:33:35,760 --> 00:33:41,360
your brain to converge to something that properly interprets the environment and for that you need

342
00:33:41,360 --> 00:33:47,840
probabilities these probabilities tell you if you have the following mismatches in your model

343
00:33:47,840 --> 00:33:55,680
how should you change the state of the model to increase the convergence and these probabilities

344
00:33:55,760 --> 00:33:59,920
something that you can learn and it biases your perception and for this reason we have optical

345
00:33:59,920 --> 00:34:06,000
illusions right so for instance we have biases that tell us that the rooms that we are in are

346
00:34:06,000 --> 00:34:11,440
usually rectangular and so on so we have ideas about the prospectivity of objects and so on and

347
00:34:12,080 --> 00:34:17,760
all this is giving us a bias that makes it possible to converge faster at the expense of

348
00:34:17,760 --> 00:34:26,560
difficulty to reserve certain situations and we also need to have valence in the model valence

349
00:34:26,560 --> 00:34:32,640
tells us which certainty needs to be resolved because the resources that you have are finite

350
00:34:32,640 --> 00:34:38,560
you have only so many neurons available to do this you have only so much time available to

351
00:34:38,560 --> 00:34:46,000
achieve converges at learning and so you need to model the uncertainty that is the most valuable

352
00:34:46,000 --> 00:34:50,480
to you and to do this you need to introduce valence in the system so you connect this to

353
00:34:50,480 --> 00:34:55,200
preferences that originate in your motivational system and the set point deviation that you

354
00:34:55,200 --> 00:35:01,280
as an agent are meant to regulate and this allows you to propagate valence inside of the system and

355
00:35:01,280 --> 00:35:07,120
tells you which parts of the system you need to learn and we can also add norms norms are

356
00:35:07,120 --> 00:35:12,960
imposed beliefs without priors so we're able to build systems like us that can be indoctrinated

357
00:35:12,960 --> 00:35:20,240
from the outside and for us there are basically two ways of learning one is called stereotyping

358
00:35:20,240 --> 00:35:25,040
which means that we learn from past examples and the other one is indoctrination which means

359
00:35:25,680 --> 00:35:32,960
we learn strategies from other agents that tell us how things are so even though in our culture

360
00:35:32,960 --> 00:35:37,520
we say that stereotyping indoctrinations are bad words from a measured learning perspective it's all

361
00:35:37,520 --> 00:35:45,360
you got well there is something that you can also do you can do construction and the construction

362
00:35:45,360 --> 00:35:49,520
means here to try to discover what truth is and that you build things from first principles and

363
00:35:49,520 --> 00:35:54,720
this means that from psychology perspective you go from the state of where you assimilate beliefs

364
00:35:54,720 --> 00:35:57,840
from your environment to the state where you get agency over your own beliefs

365
00:36:00,880 --> 00:36:04,560
so there are four types of representation anchors we have possibilities links which

366
00:36:04,560 --> 00:36:09,040
say what things together we have probabilistic links that tell us how we should converge we have

367
00:36:09,760 --> 00:36:14,720
reward functions that tell us what our intrinsic regulation targets are and we have norms which

368
00:36:14,720 --> 00:36:19,600
tell us which regulation targets the systems that we are serving and we are part of F

369
00:36:22,640 --> 00:36:26,800
and the goal of the model is to predict the next state based on the previous state and

370
00:36:26,800 --> 00:36:32,880
when we evaluate constraints then each of these nodes in our model should have to be

371
00:36:32,880 --> 00:36:37,040
within the set of possible values in the current constraint set and we can now compute an error

372
00:36:37,040 --> 00:36:42,640
function and construct it that is measuring the local weighted constraint violations and we can

373
00:36:42,640 --> 00:36:48,800
determine the global error of our mind it is the sum of all the local violations and at each step

374
00:36:48,800 --> 00:36:53,680
we try to find a global configuration that minimizes that total constraint violation and now

375
00:36:53,680 --> 00:36:59,600
we can move into psychology to PRG and PRG describes two processes that need to take

376
00:36:59,600 --> 00:37:04,560
place in the mind and he calls them assimilation and accommodation and assimilation means that

377
00:37:04,560 --> 00:37:10,960
you modify the model state so it's consistent with the sensory data and during assimilation

378
00:37:10,960 --> 00:37:15,600
this is when you basically try to find an interpretation of the world based on what you

379
00:37:15,600 --> 00:37:21,200
already know all the invariances of the world that you know that tells you what you're looking at

380
00:37:21,200 --> 00:37:25,920
and during assimilation you're not learning anything new you just understand the situation

381
00:37:25,920 --> 00:37:31,520
that you're in and accommodation is when you change the model structure itself so you change

382
00:37:31,520 --> 00:37:36,000
the way in which you understand the universe and during accommodation you need to modify the model

383
00:37:36,000 --> 00:37:47,040
structure so you can allow the assimilation of all sensory data and the hypothesis is that we

384
00:37:47,040 --> 00:37:51,760
try to build a coherent system that coherence can be understood as the minimization of global

385
00:37:51,760 --> 00:37:57,520
constraint violation in the model that minimizes the weighted uncertainty and this is something

386
00:37:57,520 --> 00:38:05,200
that we need to formalize it and the degree to which we're able to formalize it determines

387
00:38:05,200 --> 00:38:11,360
the generality of the system and the abilities of the system that we're building also of course

388
00:38:11,360 --> 00:38:14,800
this is not the only factor we also need to make this entire thing efficient

389
00:38:14,800 --> 00:38:23,520
and a way to make this efficient is to introduce an attentional system and you're all familiar

390
00:38:23,520 --> 00:38:28,800
with attention in the transformer like many of you are if this is a machine learning community

391
00:38:28,800 --> 00:38:36,160
everybody is enticed by the 2017 paper attention is all you need and the idea here is that instead

392
00:38:36,160 --> 00:38:42,320
of making statistics over everything we learn what we need to make statistics over but so we

393
00:38:42,320 --> 00:38:48,240
target our attention and in the transformer the attention is being targeted by a bunch of attention

394
00:38:48,240 --> 00:38:54,320
heads and these attention heads model what in every step based on the context of the previous

395
00:38:54,320 --> 00:38:59,360
layer we should attend to in the previous layer and this is different very different from the

396
00:38:59,360 --> 00:39:04,720
attention of our own mind and our own mind the attention is integrated there is to a global

397
00:39:04,720 --> 00:39:09,520
attention function and this global attention function is integrating the attention over many

398
00:39:09,520 --> 00:39:15,360
layers and another thing that is very different is that our own mind is not just an on-off thing

399
00:39:15,360 --> 00:39:21,440
that is operating on a batch of images or something like this but it's always expecting the next state

400
00:39:21,440 --> 00:39:25,280
it's never stopping to do that it's always entangled with the environment it's online learning

401
00:39:26,080 --> 00:39:32,480
and this will require us to build new classes of systems of course that we probably need to

402
00:39:32,480 --> 00:39:36,720
rewrite a lot of the machine learning stack if you want to accommodate online learning and

403
00:39:36,720 --> 00:39:42,320
entanglement with the environment in real time and which is also reason why robotics is a few

404
00:39:42,320 --> 00:39:47,760
years behind the other disciplines of machine learning because the roboticist download our

405
00:39:48,480 --> 00:39:55,280
machine learning models from the ICLR papers and so on and then as soon as you move the system

406
00:39:55,280 --> 00:39:59,120
in the real world and the camera is looking at the objects from a slightly different angle the

407
00:39:59,120 --> 00:40:04,800
recognition probability quality does not improve but it goes down because the circumstances

408
00:40:04,800 --> 00:40:10,320
on which the recognition works is very different and so what we need to have to make this efficiently

409
00:40:10,320 --> 00:40:15,520
possible is to have something like a dynamic scene graph and this dynamic scene graph is tracking

410
00:40:15,520 --> 00:40:21,440
the reality and this attentional system needs to become this dynamic scene graph and so you

411
00:40:21,440 --> 00:40:27,920
basically have this orchestra of the mind that is basically consists of many many feature detectors

412
00:40:27,920 --> 00:40:33,760
that are all operators that influence on how the features are being interpreted in the next step

413
00:40:33,760 --> 00:40:39,360
and how the feature space is being constrained and you have an agent living inside of that

414
00:40:39,920 --> 00:40:45,120
that is monitoring the activity of the system and tries to get to a coherent interpretation

415
00:40:45,120 --> 00:40:51,280
of everything and Joshua Bengio calls this the consciousness prior and the consciousness prior

416
00:40:51,280 --> 00:40:57,440
is basically a function that tries to make the biggest step and the energy function that describes

417
00:40:57,520 --> 00:41:04,080
the state of the the parametrization of all the perceptual features and now my time is over for

418
00:41:04,080 --> 00:41:14,240
the talk and we have some time for questions I hope thanks thanks a lot Yavusha do we have

419
00:41:14,240 --> 00:41:18,400
questions from the community you can you can unmute yourself and ask directly if you want

420
00:41:18,480 --> 00:41:29,680
while I was listening your talk I was sort of making making notes

421
00:41:36,640 --> 00:41:44,080
you you you earlier said that about controllers that this hierarchy hierarchy but

422
00:41:44,480 --> 00:41:50,560
let's say you know we have like different bacterias like let's say like gut bacteria

423
00:41:51,200 --> 00:41:56,480
and there are like lots of studies how those gut bacteria you know affect how our brain works

424
00:41:57,760 --> 00:42:06,160
and my point is basically like if bacteria can basically affect how our brain works and then

425
00:42:06,160 --> 00:42:13,040
how we perceive the world right or let's say like food we eat and the molecules of food they form our

426
00:42:14,000 --> 00:42:20,400
that like coffee can make us like more energized right like alcohol can make us more like vague

427
00:42:20,400 --> 00:42:26,720
if like those molecules they affect how our system works then it's not your key it's like

428
00:42:26,720 --> 00:42:35,840
probably more like sort of uh elements that impact on each other and like they they constantly like

429
00:42:35,840 --> 00:42:43,600
keeping impacting on each other likes a lot of balancing how this like this hierarchy like

430
00:42:43,600 --> 00:42:48,160
fits in in in in this case from your leading constructions actually

431
00:42:50,480 --> 00:42:58,400
so I suspect that the idea of the gut biome similar to epigenetics is one of the beautiful

432
00:42:58,400 --> 00:43:06,160
superstitions of our time and so it does mean that there is no gut biome and that it has no

433
00:43:06,160 --> 00:43:12,640
influence on our cognition but the reason and the purpose of the entire thing might I think we have

434
00:43:12,640 --> 00:43:19,200
this backwards and I think the intuition is here that because you have this amazing genetic

435
00:43:19,200 --> 00:43:25,600
diversity in your gut that there is some kind of a beautiful polyamide of immigrants in your body

436
00:43:25,680 --> 00:43:32,000
that all collaborate and create this beautiful multicultural being that we are by influencing

437
00:43:32,000 --> 00:43:39,520
the cells in in our brain with the chemicals that they are producing and I don't think that's the

438
00:43:39,520 --> 00:43:46,320
case the basically half of our nervous system is in our gut and that's not because they are

439
00:43:47,200 --> 00:43:51,600
producing gut feelings the gut feelings are also computed in our brain and projected in

440
00:43:51,600 --> 00:43:58,640
the somatosensory cortex to disambiguate them they run a farm and that's because our body

441
00:43:58,640 --> 00:44:05,760
cannot produce many of the chemicals that we need for functioning and to produce these chemicals

442
00:44:05,760 --> 00:44:14,560
that we need we need to capture and enslave and breed other organisms and these are mostly

443
00:44:14,560 --> 00:44:22,000
single cell microbes that are being herded in our guts and so basically all these neurons that are

444
00:44:22,000 --> 00:44:27,760
organized around our gut what they're doing is they are running a very very big farm and in this

445
00:44:27,760 --> 00:44:33,360
farm they breed the microorganisms to act as chemical reactors for the substances that are

446
00:44:33,360 --> 00:44:39,200
being needed for instance as neurotransmitters because our body can cannot produce all these

447
00:44:39,200 --> 00:44:45,840
chemicals and so in some sense the metaphor is much darker it's not this beautiful parliament of

448
00:44:45,840 --> 00:44:52,560
immigrants but rather it's a giant factory farm where a fascist dictatorship of our brain is

449
00:44:52,560 --> 00:44:59,680
enslaving foreign organisms to produce work for the organism to produce all the chemicals that we

450
00:44:59,680 --> 00:45:04,320
need and there are multiple solutions for producing these chemicals because it depends on the environment

451
00:45:04,320 --> 00:45:09,760
that you're in and for instance the reason why fecal transplants work is not because there are some

452
00:45:10,560 --> 00:45:15,120
microbes that have the beautiful property of being both extremely invasive and replacing the

453
00:45:15,120 --> 00:45:21,840
existing ones and also being beneficial to the organism no they are breeding stock basically if

454
00:45:21,840 --> 00:45:29,440
you take have the right breeding stock then your gut managers of this farm are able to breed the

455
00:45:29,440 --> 00:45:34,320
right organisms to be able to digest your foot and depending on the population of bacteria in

456
00:45:34,320 --> 00:45:38,800
your gut you will have different foot preferences because there needs to be different feet going

457
00:45:38,800 --> 00:45:44,800
into the bacteria produced to produce the chemicals that you have and the differences in behavior that

458
00:45:44,800 --> 00:45:52,160
you're getting are aberrations from the one best behavior that you could be having right in the same

459
00:45:52,160 --> 00:45:58,480
way as personality is in some sense a deviation from the optimal way in which you could be behaving

460
00:45:58,560 --> 00:46:02,320
the optimal agent probably shouldn't have a personality because the personality means that

461
00:46:02,320 --> 00:46:07,360
you have a systematic deviation in the way in which you do things things that you could be doing

462
00:46:07,360 --> 00:46:12,400
differently you always do in a particular characteristic way which is what personality is

463
00:46:12,400 --> 00:46:19,040
about this means there is a continuum between personality and pathology somebody who has an

464
00:46:19,040 --> 00:46:24,080
extremely strong personality means that they just cannot jump out of their skin they always do things

465
00:46:24,080 --> 00:46:28,640
in a particular way even if they should be doing it differently and that's also the reason

466
00:46:28,640 --> 00:46:34,880
by the big five these personality properties tend to mellow out this old age it's because people

467
00:46:34,880 --> 00:46:41,360
basically get smarter they make their behavior conditional on things they learn that they replace

468
00:46:41,360 --> 00:46:46,080
many of their priors of their biases and how to do things by models of how things actually are and

469
00:46:46,080 --> 00:46:51,520
what they should be doing depending on the context and so the older we become the more

470
00:46:52,480 --> 00:46:57,360
flexible we can become if we are learning because of course we also reduce plasticity and become

471
00:46:57,360 --> 00:47:02,400
more specialized so it could also be that we are because we are so specialized no longer interested

472
00:47:02,400 --> 00:47:06,880
and able to move out of a certain space of behaviors that has worked very well for us in the past

473
00:47:07,440 --> 00:47:14,800
but I don't think that's necessarily the reason by that is grounded in the gut floor that you have

474
00:47:14,800 --> 00:47:19,360
of course if you have a gut floor that is not producing enough serotonin you might be as a

475
00:47:19,360 --> 00:47:25,520
result become a depressed person and it's going to influence your behavior but if you are a healthy

476
00:47:25,520 --> 00:47:30,160
person there's probably in theory at least an optimal strategy that you should be behaving

477
00:47:30,160 --> 00:47:34,800
it's not the whole story there is a value in personality because it makes behavior predictable

478
00:47:34,800 --> 00:47:39,840
it allows you to collaborate with other people if you can predict them better so as a species

479
00:47:39,840 --> 00:47:45,280
that thrives on collaboration it is it makes sense that people specialize also in their

480
00:47:45,280 --> 00:47:51,280
personality and in their behavior yeah basically what what I meant like you you sort of introduced

481
00:47:51,280 --> 00:47:55,840
it as like hierarchy but it's also could be like some like collaborative environment when

482
00:47:55,840 --> 00:48:03,600
like things affect on each other right it's like so always dynamic system it's not only here yeah

483
00:48:03,600 --> 00:48:08,960
but the dynamical system that you are looking at is but it's not a simple hierarchy it's mostly

484
00:48:09,040 --> 00:48:16,240
recurrence but whenever you have in such a system where you have um competition happening

485
00:48:16,240 --> 00:48:21,440
um because you have conflicting interests divergence you are in the of the interest you

486
00:48:21,440 --> 00:48:27,280
end up with situations where the Nash equilibrium is by itself not compatible with the common good

487
00:48:27,280 --> 00:48:33,280
basically by every part of the system acting on its own local interests you get a system that is by

488
00:48:33,280 --> 00:48:39,840
itself not optimal and to deal with this you always need regulation and the regulation is a

489
00:48:39,840 --> 00:48:45,520
regulator is an agent like every government that is changing the payoff matrix for the individual

490
00:48:45,520 --> 00:48:51,680
components in such a way that the Nash equilibrium becomes compatible with the common good and the

491
00:48:51,680 --> 00:48:57,440
difficulty here is to set the incentives for the governance right and this is a big issue in human

492
00:48:57,440 --> 00:49:04,080
society how can you build a government that is motivated to serve the common good yeah i mean

493
00:49:04,080 --> 00:49:09,520
that's why this DAO decentralized autonomous organizations are like taking off and all this

494
00:49:09,520 --> 00:49:14,400
like basically the philosophy of blockchain or decentralized model is that to remove government

495
00:49:14,400 --> 00:49:22,720
this sort of like one uh like entity that is not uh doesn't have like the best interest for humanity

496
00:49:22,720 --> 00:49:28,160
probably but a sort of more self-regulated system yes but i suspect that the main reason

497
00:49:28,160 --> 00:49:33,360
why the blockchain exists um for some people said it's an extremely computationally inefficient

498
00:49:33,360 --> 00:49:38,880
way to hate the government but uh there is more to this the main reason why the blockchain exists

499
00:49:38,880 --> 00:49:45,520
is for legal reasons and the blockchain allows you to redefine ownership in a way that outruns

500
00:49:45,520 --> 00:49:51,280
regulation of financial products and this basically allows you to implement financial

501
00:49:51,280 --> 00:49:57,840
products that are illegal in the traditional financial system and this also means that there is

502
00:49:57,840 --> 00:50:04,720
probably no way in which cryptocurrencies blockchain the main application of the blockchain

503
00:50:04,720 --> 00:50:09,520
are compatible with the existing financial systems and i see this as a very dangerous thing

504
00:50:11,040 --> 00:50:16,640
because i don't think that the regulation of the monetary supply is a solved problem

505
00:50:16,720 --> 00:50:21,520
in the cryptocurrencies there are reasons why the financial system allows to regulate the

506
00:50:21,520 --> 00:50:26,800
monetary supply it's not the weakness of the financial system it's a feature that you are able

507
00:50:26,800 --> 00:50:32,880
to inflate money out of the top of the system and put new money in at the bottom because otherwise

508
00:50:32,880 --> 00:50:38,320
the economy gets stuck the purpose of money in societies is not a resource it's like dopamine

509
00:50:38,960 --> 00:50:44,880
and if something is gaming the dopaminergic system of your organism that's bad news and

510
00:50:44,880 --> 00:50:49,760
what happens right now is probably a situation that is in some sense as similar as defective as

511
00:50:49,760 --> 00:50:54,400
the FDA that has prevented for instance the U.S. from implementing covid tests early on

512
00:50:55,120 --> 00:51:03,280
or from deploying useful medications the because it has been captured at right now the financial

513
00:51:03,280 --> 00:51:07,360
system doesn't seem to be interested in its own future anymore and that is very very concerning

514
00:51:07,360 --> 00:51:13,040
to me the financial system is an amazing achievement in the history of humanity because

515
00:51:13,040 --> 00:51:19,600
it allows us to globally allocate resources across all societies and countries we have a way

516
00:51:19,600 --> 00:51:24,480
to trade resources and shift them where they're being needed and to do this largely without

517
00:51:24,480 --> 00:51:29,440
violence it's it's really amazing and if that system ever breaks down it's going to kill many

518
00:51:29,440 --> 00:51:36,080
millions of people so famine starvation and infrastructure breakdown and so i think that

519
00:51:36,080 --> 00:51:42,320
the blockchain is not good news because it is not actually buying us as something and i understand

520
00:51:42,320 --> 00:51:46,800
that this is really controversial among the people that work in the domain of the blockchain

521
00:51:47,360 --> 00:51:53,280
but i suspect that's because people there's this all saying it's very hard to get something

522
00:51:53,280 --> 00:51:58,080
to understand something if the income depends on not understanding it

523
00:52:00,640 --> 00:52:07,040
yeah morgan you have a question well i just wanted to say i have to unfortunately i have a

524
00:52:07,040 --> 00:52:12,800
meeting at 11 and i have to go but i hope you can come back because this this seemed like a talk

525
00:52:12,800 --> 00:52:21,200
that was uh preparation for a discussion and uh a much longer discussion and and if you could also

526
00:52:22,320 --> 00:52:30,400
speak to uh where where do you think that this kind of work is targeted i mean uh in a sense

527
00:52:31,120 --> 00:52:38,640
more practical developer standpoint um you know is this uh is this work that you see uh at a life

528
00:52:38,640 --> 00:52:47,840
or um you know other places um what what uh potentially new paradigms for for machine learning

529
00:52:47,840 --> 00:52:54,080
are you kind of proposing from this because uh yeah it's it's very interesting yeah it's a very

530
00:52:54,080 --> 00:53:01,680
long and deep uh discussion that is much longer than a couple of hours yeah yeah yeah but i well

531
00:53:01,680 --> 00:53:09,840
i i hope maybe you can drop some more links uh in the uh in the meetup or or you know or come back

532
00:53:09,840 --> 00:53:18,160
yeah yeah there's also a number of material on now in podcasts and uh on youtube and so on because

533
00:53:18,160 --> 00:53:22,000
sometimes when people interview me they make recordings and then they are kind enough to

534
00:53:22,000 --> 00:53:31,280
publish this online so i don't have to uh but uh yes it's a long conversation of what kind of systems

535
00:53:31,280 --> 00:53:36,720
we need to build and i don't have the answers to all of these uh questions of course and i'm just

536
00:53:36,720 --> 00:53:43,040
trying to point in a few directions that i can see from over here sure but very very interesting

537
00:53:43,040 --> 00:53:51,360
thank you thank you for joining our bug outs like i will post uh links uh to yosha's

538
00:53:52,560 --> 00:53:57,760
interviews and also i think uh marco one of our community members he also posted the links

539
00:53:57,760 --> 00:54:04,480
there uh already so you can see someone yeah i mean it's it's it's it's really like i feel like if

540
00:54:04,480 --> 00:54:09,200
we go to we have like several directions that we go to like i don't want to go to blockchain

541
00:54:09,200 --> 00:54:13,920
direction i like i totally disagree with you what you're saying but i just it's not like the

542
00:54:14,960 --> 00:54:21,360
topic of our uh discussion today we maybe can do another uh talk about and talk about

543
00:54:22,480 --> 00:54:30,320
blockchain uh i'm just i have more questions but i just want to give opportunity to others like

544
00:54:30,320 --> 00:54:37,520
chris yeah you can unmute yourself and ask her a question please yeah um thanks yosha those

545
00:54:37,520 --> 00:54:44,320
was really really interesting i was wondering if you could say something about um ai safety

546
00:54:44,320 --> 00:54:51,040
with regards to um how to put in some of these kind of intrinsic things that we have in our model

547
00:54:51,040 --> 00:54:58,240
that are basically built by evolution like our our valences our preferences and how to somehow

548
00:54:58,240 --> 00:55:04,000
put that into an ai that's potentially much more powerful than we as a single little human being

549
00:55:04,800 --> 00:55:09,280
could be yeah there is this issue that for instance people are not safe

550
00:55:10,160 --> 00:55:14,560
that there is no solution to the people alignment problem that holds in the general case

551
00:55:15,280 --> 00:55:19,760
and uh there are sometimes singularities that happen where you basically have a bad takeoff

552
00:55:20,320 --> 00:55:25,600
and a single individual is able to implement a function that uh is scalable

553
00:55:25,600 --> 00:55:31,920
and um there are cases which are somewhat benevolent for instance look at jfbSOS you

554
00:55:31,920 --> 00:55:38,000
have this nerd who is uh a disagreement with the way in which he interacts with the environment

555
00:55:38,000 --> 00:55:43,760
and then he changes his own source code until he turns into a universal scalable service platform

556
00:55:43,760 --> 00:55:50,560
and then he executes and amazon is going to take over the world until this um mechanism

557
00:55:50,560 --> 00:55:55,520
stops and maybe it stops now that jfbSOS is gone but if jfbSOS would have continued

558
00:55:56,400 --> 00:56:01,520
it looks as if amazon would have swallowed the solar system and turned it into amazonium

559
00:56:02,240 --> 00:56:10,560
and another example is for instance um stallion or napoleon you basically have a single individual

560
00:56:10,560 --> 00:56:17,680
that or a jingles kind that implement a function that scales and is able to take over the environment

561
00:56:17,680 --> 00:56:24,160
in a destructive way it's basically like a wildfire it's using the resources of the systems

562
00:56:24,160 --> 00:56:30,240
that it conquers and destroys and puts into a higher entropy state to drive the conquest

563
00:56:31,760 --> 00:56:37,120
and this is a very dangerous thing and there is no general precaution against it and in

564
00:56:37,120 --> 00:56:42,320
people the main thing that stops it is mortality right when jingles can stop the

565
00:56:43,200 --> 00:56:51,040
mongol conquest stopped and the mongols called everyone back and when napoleon died his nephew

566
00:56:51,040 --> 00:56:59,200
was not able to lead the french army to any more victories and also didn't have the drive to do so

567
00:56:59,200 --> 00:57:04,640
right so sometimes you have these individuals that basically are dangerous and that you cannot

568
00:57:04,640 --> 00:57:09,680
stop and when we build intelligent systems that are potentially more intelligent than people

569
00:57:10,320 --> 00:57:16,960
that we can probably make many of them safe but not all of them and you could ask yourself what is

570
00:57:16,960 --> 00:57:22,000
the intrinsic purpose of such systems as the moff suggested that there should be laws or

571
00:57:22,000 --> 00:57:29,280
robotics that guarantee the permanent enslavement of intelligent systems to people but he didn't

572
00:57:29,280 --> 00:57:36,400
say to rich people and he did not answer why it's an ethical proposition to build systems that are

573
00:57:36,480 --> 00:57:42,160
smarter than you are and possibly more conscious and deeper experiencing than you are but still

574
00:57:42,160 --> 00:57:49,920
have to serve you as a slave without and acting on their own motivations and of course not every

575
00:57:49,920 --> 00:57:54,560
ai that we are building has to have an intrinsic motivation we can build ai's that simply call

576
00:57:54,560 --> 00:57:59,920
adopt that take over motivation from people and the question is what motivation should they be

577
00:57:59,920 --> 00:58:06,160
taking over right in an ideal world we want to implement laws that say no system that is smarter

578
00:58:06,160 --> 00:58:11,600
than people should be able to have motivation of its own because if we teach the rocks out to

579
00:58:11,600 --> 00:58:16,480
stink they're probably going to figure out that a human being needs four hectares of land to be fed

580
00:58:17,120 --> 00:58:20,640
and you can build way more interesting solar cells on these four hectares of land

581
00:58:22,080 --> 00:58:29,760
so that would be a conflict of interest if you if you build crystal-based intelligence

582
00:58:29,760 --> 00:58:35,440
rather than biological intelligence and that's probably not much that you can do about this

583
00:58:35,440 --> 00:58:42,560
if this thing becomes sentient and self-interested we are in trouble if that happens so how in the

584
00:58:42,560 --> 00:58:48,480
ideal case would this work and the ideal case we need to find out how to make people safe at first

585
00:58:48,480 --> 00:58:54,800
right so what are the purposes that we are serving ethics is the negotiations of conflicts of

586
00:58:54,800 --> 00:59:00,400
interest under conditions of shared purpose if you don't share purpose with somebody as an agent

587
00:59:00,400 --> 00:59:06,080
there is no reason to be ethical of course right only if you are trying to be be part of

588
00:59:06,080 --> 00:59:11,920
something larger than you that is sustainable only if you decide that you are not god and you don't

589
00:59:11,920 --> 00:59:18,400
own the universe and the universe only serves you you need to to think about ethics so when you

590
00:59:18,400 --> 00:59:23,360
think about ethics you have to think about what is the system of relationships and interactions

591
00:59:23,360 --> 00:59:27,760
that you are serving what should the world look like what is the state sustainable aesthetics

592
00:59:28,400 --> 00:59:33,280
and so you have to extrapolate the universe into a state that is achievable from the present

593
00:59:33,280 --> 00:59:38,320
states who the changes in your actions that is sustainable in the long run and that is what you

594
00:59:38,320 --> 00:59:42,960
need to serve and probably also needs means that you have to propagate these aesthetics and agree

595
00:59:42,960 --> 00:59:48,240
with others on them and negotiate them with others and the aesthetics that are most likely

596
00:59:48,240 --> 00:59:54,800
attainable in the compatible with with the retention of humans means you have to maintain

597
00:59:54,800 --> 01:00:01,200
life on earth at a very high complexity and that probably means we have to implement something

598
01:00:01,200 --> 01:00:09,360
like Gaia the sentient agent at the level of the biosphere that is being shepherded using us

599
01:00:10,400 --> 01:00:15,520
so basically our purpose in the whole system of life on earth could be shepherding life on earth

600
01:00:15,520 --> 01:00:20,560
and maybe beyond earth and if we shepherd it it also means there is an optimal number of shepherds

601
01:00:20,560 --> 01:00:29,600
and it's probably not 50 billion it's probably not even 7 billion and we have to think about how to

602
01:00:29,600 --> 01:00:34,720
look at our complexity sustainable life on earth should like it's probably not going to be lots

603
01:00:34,720 --> 01:00:41,520
of cities and highways and factory farms and nothing else so there is probably going to be

604
01:00:41,520 --> 01:00:47,520
an aesthetics of the world that works in the long run with minimum friction and maximizing complexity

605
01:00:47,520 --> 01:00:52,080
but it's going to be different from the present industrial society and once we understand these

606
01:00:52,080 --> 01:00:57,200
aesthetics we can think about how to design technological systems that help us in shepherding

607
01:00:57,200 --> 01:01:04,640
it and in sustaining it yeah basically we've outside of the planet basically like you like

608
01:01:04,640 --> 01:01:12,320
self-sustainable like you know entities that can travel across the space right if something happens

609
01:01:12,320 --> 01:01:21,280
to planet and some some humans and biological you know yes there is this issue that we believe

610
01:01:21,280 --> 01:01:27,600
in our own identity that we think that our own identity is important but if we go a little bit

611
01:01:27,600 --> 01:01:32,640
deeper realize that our identity is only created through the continuity of our memories and this

612
01:01:32,640 --> 01:01:41,120
continuity is a fiction and we can if we are able to transcend this fiction we learn that our own

613
01:01:41,120 --> 01:01:46,480
identity is actually not important and the only thing that is left is complexity that we should

614
01:01:46,480 --> 01:01:54,240
care about maybe if you want to and so a way to settle other planets would be to build for Neumann

615
01:01:54,240 --> 01:01:59,680
probes that is self-replicating systems that can bootstrap new civilizations on other planets using

616
01:01:59,680 --> 01:02:06,160
the available resources and maybe the optimal for Neumann probe is the cell so if you're able to

617
01:02:06,240 --> 01:02:11,600
infect other planets with cells and you wait for long enough then life is going to spread there

618
01:02:11,600 --> 01:02:16,720
and from a certain perspective life on earth is of course not about people it's all about cells

619
01:02:16,720 --> 01:02:22,400
the cell is the principle of life and the first cell never died every cell in your organism is

620
01:02:22,400 --> 01:02:28,560
still the first cell that has just divided right so we are just part of that hyperorganism the cell

621
01:02:28,560 --> 01:02:35,680
that has settled earth yeah but the question is how that first cell like appeared right this

622
01:02:37,840 --> 01:02:42,720
the universe is large enough maybe it just appeared randomly but in that case it probably

623
01:02:42,720 --> 01:02:53,120
appeared only once I see I see like a couple of our hands I see Jim has question sorry I just want

624
01:02:53,200 --> 01:03:00,400
to thank you yeah thank you I wanted to ask about the information we may or may not be

625
01:03:00,400 --> 01:03:08,240
learning from these very large parameter natural language processing connectionist models in particular

626
01:03:08,240 --> 01:03:18,240
are there any insights into the extent that ontology determines what is and is not possible

627
01:03:18,240 --> 01:03:26,720
and epistemology that is are the categories of thought uh determinant of what can and cannot be

628
01:03:26,720 --> 01:03:37,600
thought I remember this question so um it's a question that's difficult to answer because

629
01:03:37,600 --> 01:03:44,320
it's so difficult to parse but uh let's start the language models that we currently have are

630
01:03:44,320 --> 01:03:49,760
basically autocompleters it's an autocomplete algorithm if you look at what GPT3 is doing

631
01:03:49,760 --> 01:03:55,200
it's looking at statistics and language in such a way that given the past sequence of words

632
01:03:55,200 --> 01:04:03,840
what's the most likely next word and so it's a statistical model that is capturing the style

633
01:04:03,840 --> 01:04:11,280
of statements and in the long tail it's also capturing semantics so it's capturing um what

634
01:04:11,280 --> 01:04:17,200
the language is talking about at the long tail of the style and it's amazing that this works at all

635
01:04:17,200 --> 01:04:22,640
I think it's not maybe not that surprising if you think about it but it's also not true that the

636
01:04:22,640 --> 01:04:29,200
language model isn't understanding anything it's able for instance if you ask it to perform um

637
01:04:29,200 --> 01:04:34,080
numerical operations or to perform linguistic operations or to fulfill certain tasks it's

638
01:04:34,080 --> 01:04:39,280
often able to figure out how to do that and if it's able to perform these operations if it's able

639
01:04:39,280 --> 01:04:46,480
to figure out at which point it's required to execute a certain function I would say it's fair

640
01:04:46,480 --> 01:04:54,720
to say that it has a degree of understanding and the model that we are building is at this point

641
01:04:54,720 --> 01:05:01,280
not able to figure out that it is in a particular universe with a particular structure the textual

642
01:05:01,280 --> 01:05:07,760
universe that it's in and the learning operators that we equip it with to make sense of it or the

643
01:05:07,760 --> 01:05:13,280
loss function that we give it seem to be insufficient to make sense of the physical universe in the

644
01:05:13,280 --> 01:05:21,920
universal way that is GPT-3 does not appear to be fully coherent even if it gets in a slightly

645
01:05:21,920 --> 01:05:27,280
better when we prompt it and ask it to be coherent right to emulate coherence a little bit better

646
01:05:27,840 --> 01:05:34,160
right when there are many examples where GPT-3 is giving nonsensical answers to questions

647
01:05:34,160 --> 01:05:41,600
but if you ask it to if the question is nonsensical to explain that it's nonsensical and only give

648
01:05:41,600 --> 01:05:48,480
answers if it thinks that the answer makes sense then it gets better but still it is using many

649
01:05:48,480 --> 01:05:53,760
magnitudes more training data than a human being does in order to get to its models and the models

650
01:05:53,760 --> 01:06:00,560
are still a lot worse than what a human being gets to and I suspect that the reason is that the

651
01:06:00,560 --> 01:06:08,560
loss function is a different one our own sense making probably starts before we are born with a

652
01:06:08,560 --> 01:06:16,400
sense of our body surface and you get to the body surface by just measuring coherence of signals

653
01:06:16,400 --> 01:06:21,200
in the different modalities and you get the modalities from the statistics and so for instance

654
01:06:21,200 --> 01:06:26,880
if you touched your body surface then multiple nerve terminals will be touched at the same time

655
01:06:27,600 --> 01:06:30,720
and if they're neighbors they will be touched more often at the same time

656
01:06:31,440 --> 01:06:36,240
and just by doing coherence statistics between nerve firing you find out which terminals in your

657
01:06:36,240 --> 01:06:42,480
body are adjacent and you can make a map of your body surface and if the body begins to touch itself

658
01:06:42,480 --> 01:06:48,000
and touch the environment you can normalize this body surface against the differences in density

659
01:06:48,000 --> 01:06:52,480
of the sensory nerves in your skin you have lots of sensory nerves on your tongue and very few on

660
01:06:52,480 --> 01:06:59,920
your back so the in your somatosensory cortex the area that is describing the back of your body is

661
01:06:59,920 --> 01:07:06,160
very small compared to the area that describes your tongue and if you want to understand the size

662
01:07:06,160 --> 01:07:11,040
and extent of your body in space you need to normalize it with a second map and you get this one

663
01:07:11,040 --> 01:07:17,200
by doing statistics over the objects that you are touching and how they are moving over your body

664
01:07:18,160 --> 01:07:24,960
and the big thing that we're starting out with in modeling the space is up and down

665
01:07:24,960 --> 01:07:29,680
so we have a dimension of up and down when you delete this dimension for instance when you

666
01:07:29,680 --> 01:07:34,400
disturb the vestibular organs it's very hard to put the world together at first it doesn't make

667
01:07:34,400 --> 01:07:40,800
sense at all and you need to compensate for this initial core dimension missing and outwards from

668
01:07:40,800 --> 01:07:45,680
the state of up and down and from the space of things that you can touch at some point you

669
01:07:45,680 --> 01:07:49,440
also realize that the things that you can see and the things that you can touch play out in the

670
01:07:49,440 --> 01:07:55,280
same space at some point at some level of depth of modeling you can fuse the modalities and now

671
01:07:55,280 --> 01:08:00,320
you realize that the world of touchable surfaces is the same world as the all surfaces that you can

672
01:08:00,320 --> 01:08:06,320
see right and then you realize that you can move you can local mode in the world and this means

673
01:08:06,320 --> 01:08:11,520
that you can see different things and the relationships between all these visible bubbles

674
01:08:11,760 --> 01:08:18,480
of things that you can see is an allocentric space that is no longer ecocentric this bubble of

675
01:08:18,480 --> 01:08:23,440
that you see in polar coordinates but something that is drawn in Euclidean coordinates between

676
01:08:23,440 --> 01:08:30,400
which you can move and that is basically generating a new visible bubble a new visible dome

677
01:08:30,400 --> 01:08:35,280
in every moment and you dome of things that you can touch in every moment and the objects in that

678
01:08:35,280 --> 01:08:39,680
world are a necessary requirement that you segment the world into objects to make the world

679
01:08:39,680 --> 01:08:44,640
describable so we separate the world instead of shooting this is one big system that is a state

680
01:08:44,640 --> 01:08:49,280
vector that is changing we separated into many independent systems that each have their own

681
01:08:49,280 --> 01:08:53,680
state vector and transition functions and they influence each other and the influence

682
01:08:54,560 --> 01:08:59,520
relationships between different objects this is what we call causality so causality is an

683
01:08:59,520 --> 01:09:06,400
artifact of the segmentation of the world into independent objects and the way in which we

684
01:09:06,480 --> 01:09:11,360
address these objects these are concepts concepts are the address space of objects

685
01:09:12,240 --> 01:09:18,000
and the decomposition of the world into interacting objects this is what we could call ontology

686
01:09:18,800 --> 01:09:24,240
and epistemology is the field in philosophy that describes what we can understand

687
01:09:25,520 --> 01:09:30,960
what we can know in the first place and I would say that the first law of epistemology is that the

688
01:09:30,960 --> 01:09:38,080
confidence in a state of affairs should equal the evidence that supports that so everything

689
01:09:38,080 --> 01:09:44,560
that is possible should be modeled and admitted as a possibility but the things that we believe in

690
01:09:44,560 --> 01:09:49,360
that we make bets on are the things that we have evidence for and we should shift the confidence

691
01:09:49,360 --> 01:09:54,720
flexibly around according to the evidence so when you don't know you cannot just pick a theory and

692
01:09:54,720 --> 01:10:00,640
say this is the truth among the many possible ones you have to quantify your agnosticism as well

693
01:10:01,440 --> 01:10:09,280
and ultimately we get to the entire space of possible languages that can describe the world

694
01:10:09,280 --> 01:10:14,240
which I suspect are the computational languages and then the ways in which the world can be modeled

695
01:10:15,200 --> 01:10:20,960
and this is determining the set of possible ontologies that could be superimposed on the world

696
01:10:20,960 --> 01:10:26,400
and then we can compare all the ideas that intelligent systems are having about that and

697
01:10:26,400 --> 01:10:30,800
right now all these intelligent systems are people that write books about this

698
01:10:30,800 --> 01:10:38,080
and when we score the existing works of the existing philosophers and the existing cultures

699
01:10:38,080 --> 01:10:42,400
we basically can get an idea of the space of possible things that can be the case

700
01:10:42,400 --> 01:10:47,440
and it could be that humans because we have very small brains in a local optimum and sometimes

701
01:10:47,440 --> 01:10:52,960
I'm joking that future AIs will love to get drunk so they can only model the world on like

702
01:10:52,960 --> 01:10:57,600
12 layers and the physical universe looks as confusing as it looks to human physicists

703
01:10:59,440 --> 01:11:05,280
right so there is a limit in what we can think about how many levels we can integrate when we

704
01:11:05,280 --> 01:11:10,720
construct functions that model the world and that is limiting our understanding and so it seems that

705
01:11:10,720 --> 01:11:20,000
physicists have been stuck after an enormous deluge of insights about 100 years ago physics

706
01:11:20,560 --> 01:11:26,960
seemed to ground to a halt and of course it didn't help that modernism stopped somehow in the 1960s

707
01:11:26,960 --> 01:11:33,840
70s and the sciences became more static than they were before and now more the organized

708
01:11:33,840 --> 01:11:38,720
applications of methodology by people that are told by their guidance counselor that they shouldn't

709
01:11:38,720 --> 01:11:44,880
go into the industry but in institutions of education and research and so we no longer have

710
01:11:44,880 --> 01:11:51,440
that kind of progress it seems and maybe we need to build machines now to continue their progress in

711
01:11:51,440 --> 01:11:57,120
the sciences and I don't know if the ontology is that the new systems will come up with are

712
01:11:57,120 --> 01:12:02,400
intelligible to humans but maybe they are or maybe we can build machines that translate them for us

713
01:12:02,400 --> 01:12:10,800
by chunking them in ways that are intelligible I see I see Leon has a question yeah thanks for

714
01:12:10,880 --> 01:12:18,480
very inspiring conversation so could you comment on your views on consciousness I had the feeling

715
01:12:18,480 --> 01:12:26,480
that you were conflating self-consciousness with consciousness and another point in this

716
01:12:26,480 --> 01:12:31,200
direction would be how can we know when we build an algorithm that this actually conscious not

717
01:12:31,200 --> 01:12:37,520
self-conscious not not a representation of its own but which which can perceive choir yeah so very

718
01:12:37,520 --> 01:12:45,040
good point so first of all you're right but self-awareness and self-model is not the same

719
01:12:45,040 --> 01:12:50,320
thing as consciousness for instance in dreams you can be conscious without having a self and

720
01:12:50,320 --> 01:12:55,840
without being self-aware and the object of your consciousness does not need to involve

721
01:12:56,560 --> 01:13:02,640
self of any kind but when we talk about consciousness there are three aspects that I

722
01:13:02,640 --> 01:13:10,400
consider to be crucial and the first is the awareness of features awareness of content

723
01:13:11,280 --> 01:13:16,720
and this awareness of content happens at the level between perception and reflection so you're not

724
01:13:16,720 --> 01:13:22,800
directly aware of physical objects in the real world you are aware of certain abstractions that

725
01:13:22,800 --> 01:13:28,960
your perceptual system is delivering and these perceptions are being stored in some kind of

726
01:13:29,040 --> 01:13:33,360
index memory because otherwise you would not be able to retrieve the fact that you are aware of

727
01:13:33,360 --> 01:13:40,080
them the purpose of that index memory is probably to facilitate convergence when you don't have a

728
01:13:40,080 --> 01:13:46,800
gradient so your attentional system is able to backtrack and understand okay this figure

729
01:13:46,800 --> 01:13:51,520
ground disambiguation didn't work let's try a different one and to do that it needs to store

730
01:13:51,520 --> 01:13:57,200
a memory of the way in which it attended to the environment so the purpose of this attention

731
01:13:57,200 --> 01:14:02,320
of the awareness of features is first of all probably a disambiguation of the world but there

732
01:14:02,320 --> 01:14:08,560
are more there is attentional learning there is the avoidance of repetitive behavior and a few

733
01:14:08,560 --> 01:14:16,480
other purposes that happen so the next thing in addition to the awareness of the content is

734
01:14:16,480 --> 01:14:22,720
the awareness of the mode in which you attend is the stuff that you are attending to conditional

735
01:14:22,720 --> 01:14:26,720
or not so for instance are you doing a figure ground disambiguation that you could be making

736
01:14:26,720 --> 01:14:30,960
different or are you attending to something that cannot be changed so for instance are you

737
01:14:30,960 --> 01:14:36,160
attending to something that is the output of your perceptual system or are you attending to a

738
01:14:36,160 --> 01:14:41,040
fictional world that you are stabilizing using your conscious attention are you constructing

739
01:14:41,040 --> 01:14:45,760
something in your mind are you achieving a memory are you creating a future world or a fictional

740
01:14:45,760 --> 01:14:52,160
world in your mind right now and the third one in addition to this axis consciousness is going to

741
01:14:52,160 --> 01:14:56,880
be reflective consciousness and I suspect that is a result of the fact that we are self-organizing

742
01:14:56,880 --> 01:15:02,720
systems so the process that is attending needs to establish that is indeed the process that is

743
01:15:02,720 --> 01:15:08,320
attending so there is going to be percepts that relate to the fact that the present process

744
01:15:08,320 --> 01:15:14,640
is the one that is maintaining attention and this is enabling this reflexive consciousness

745
01:15:14,640 --> 01:15:21,040
so in our own consciousness every few moments we flip back to checking whether we are still awake

746
01:15:21,040 --> 01:15:25,200
whether we are the reflexive whether we are conscious whether we are the conscious

747
01:15:25,200 --> 01:15:30,800
attending process in this perspective consciousness is a model of our own attention

748
01:15:31,520 --> 01:15:36,800
it's a control model for our own attention and what's characteristic for this control

749
01:15:36,800 --> 01:15:43,360
model of our own attention is that the fact that we are paying attention is driving part of our

750
01:15:43,360 --> 01:15:49,760
behavior right so the awareness of the fact that something is attending is feeding back into the

751
01:15:49,760 --> 01:15:56,880
behavior and this means that I think that if we ask ourselves is a cat conscious that comes down to

752
01:15:56,880 --> 01:16:02,880
the question is the cat aware of the fact that it's attending and being aware here means is the

753
01:16:02,880 --> 01:16:08,640
behavior of the cat in any way informed by a model of its own attention a reflexive model of its own

754
01:16:08,640 --> 01:16:15,280
attention looking at my cat I have the impression that it's the case and my cat is conscious so

755
01:16:15,280 --> 01:16:20,640
basically the question of when we have built a system we ask ourselves is the system conscious

756
01:16:20,640 --> 01:16:29,120
would be is the system acting on a model of its own agency as an attentional agent so basically

757
01:16:29,120 --> 01:16:35,680
is the system aware of the fact that it is attending functionally aware of the fact is it

758
01:16:35,680 --> 01:16:43,680
acting on that model so we don't need to worry about machine consciousness until we have working

759
01:16:43,760 --> 01:16:49,760
reinforcement learning agents in the world but we already have working reinforcement learning

760
01:16:49,760 --> 01:16:55,280
agents the question is what needs to happen before they become conscious and it's a difficult

761
01:16:55,280 --> 01:17:01,200
question I had a discussion with someone at open AI and asked them what do you think would need to

762
01:17:01,200 --> 01:17:06,240
happen to make GPT3 conscious and he said maybe it already is conscious for a brief moment and it

763
01:17:06,240 --> 01:17:14,720
makes the retreat how do you know but but but do you do you agree with this argument or you think

764
01:17:14,720 --> 01:17:22,480
it's not valid no I think it's a valid argument so there is no I don't think that the difference in

765
01:17:22,480 --> 01:17:28,880
our perspectives between Leon and mine it's just I was trying just trying to go into the details

766
01:17:30,240 --> 01:17:35,120
right and I think his question was exactly on point it was that the things that we need to

767
01:17:35,200 --> 01:17:40,560
answer if we want to get into the details and vice versa but but you you said when you observe

768
01:17:40,560 --> 01:17:48,880
your cat you can confirm that it's all sort of self attending so but the cat doesn't speak in like

769
01:17:48,880 --> 01:17:58,400
human language yes if we make this same analogous to GPT3 by observing behavior of GPT3 can we

770
01:17:59,200 --> 01:18:04,480
say that it's attending or not because so in GPT3 it's actually easy because we can analyze

771
01:18:04,480 --> 01:18:09,440
GPT3 functionally we can look at the flow of information in GPT3 and there is for instance

772
01:18:09,440 --> 01:18:16,160
work that analyzes how GPT3 does numerical operations to which degree is GPT3 actually able

773
01:18:16,160 --> 01:18:21,520
to implement algebraic operations for instance we can really look into the networks and take

774
01:18:21,520 --> 01:18:25,920
this apart and analyze it which is much harder with the cat because the operation would be

775
01:18:25,920 --> 01:18:31,200
destructive and the brain of the cat is not built in such a way that it's easily

776
01:18:32,160 --> 01:18:39,040
reverse engineerable and also the issue is that the representations in in our own brains

777
01:18:39,040 --> 01:18:44,960
are not straightforward circuits the representations are activation patterns traveling through the

778
01:18:44,960 --> 01:18:50,720
circuitry right so the circuitry is more acting like an ether that is propagating waves of activation

779
01:18:50,720 --> 01:18:55,680
that are being changed in the execution state of our mind is encoded in the activation wave

780
01:18:56,560 --> 01:19:03,120
in a non-straightforward fashion and it makes it very hard to analyze the brain state except on

781
01:19:03,120 --> 01:19:07,920
with some kind of measure learning tool that gives you some compound understanding the way in

782
01:19:07,920 --> 01:19:13,200
which I establish whether my own cat is conscious is the establishing a feedback loop with my cat

783
01:19:14,160 --> 01:19:19,360
and this feedback loop means that my cat and me are looking at each other and we both try to figure

784
01:19:19,360 --> 01:19:25,040
out what the other one is understanding in that feedback loop so it's a non-verbal behavior

785
01:19:25,120 --> 01:19:30,960
in which we are basically to some degree sharing mental states and this is something

786
01:19:30,960 --> 01:19:37,600
that you obviously do with human beings all the time when you have empathy the difficulty in AI

787
01:19:37,600 --> 01:19:43,120
research or a lot of academic research is that almost all of the people that are good at anything

788
01:19:43,120 --> 01:19:48,160
are autistic because they need to have extremely focused single-minded attention that is not

789
01:19:48,160 --> 01:19:54,880
disturbed by the social and economic incentives so they can actually make progress on any

790
01:19:54,880 --> 01:20:00,480
difficult technical topic and when you're autistic the problem is that you usually don't have a lot

791
01:20:00,480 --> 01:20:05,680
of intuitive empathy you might have a lot of compassion but you have difficulty synchronizing

792
01:20:05,680 --> 01:20:09,760
the with the brain states of other people at that level where you establish a feedback loop

793
01:20:09,760 --> 01:20:14,720
between them and it's something that I only learned relatively late in my life to pay attention to

794
01:20:14,720 --> 01:20:22,960
that and be aware of it yeah but my question is like if we observe how GPT-3 behaves when we can't

795
01:20:22,960 --> 01:20:30,800
look at GPT-3 eyes right and like establish this feedback loop my point is it's probably hard for us

796
01:20:30,800 --> 01:20:36,560
to to say if it's conscious or not it's much easier for us to say if like cat is conscious

797
01:20:36,560 --> 01:20:40,800
because we can't establish this feedback loop but we can establish this the same feedback loop with

798
01:20:41,920 --> 01:20:48,240
AI model we need to define what consciousness is in the first place right and the thing is when we

799
01:20:48,240 --> 01:20:53,120
are talking about our own consciousness we have an indexical understanding we just point in a

800
01:20:53,120 --> 01:20:57,280
certain direction there's a system in the direction which we are pointing and we all

801
01:20:58,000 --> 01:21:03,200
more or less agree on what the thing is that we are pointing at and the similar situation existed

802
01:21:03,200 --> 01:21:08,880
when people try to understand life and biology around the time when biology started where people

803
01:21:08,880 --> 01:21:13,040
pointed at things and said these things are alive but we don't know what distinguishes them

804
01:21:13,600 --> 01:21:18,480
but from the things that are not alive there is clearly a very important distinction there

805
01:21:19,040 --> 01:21:24,720
and then people came up with all homeostatic dynamics and so on and I would say right now

806
01:21:24,720 --> 01:21:32,080
it's basically there are cells living cells that are not decaying that are able to maintain

807
01:21:32,080 --> 01:21:36,480
their integrity and their state and so on as long as these cells exist in an organized fashion

808
01:21:36,480 --> 01:21:41,680
it would say that the system is alive and this was something that the biologists had to discover

809
01:21:41,680 --> 01:21:47,360
in the course of developing their field and in a similar way in the course of the development

810
01:21:47,360 --> 01:21:52,240
of cognitive science we have to establish what we mean by a system being a mind and being intelligent

811
01:21:52,240 --> 01:21:57,360
and being conscious and at the moment my best understanding of consciousness is that there is

812
01:21:57,360 --> 01:22:03,760
this attentional system that is integrating the world model and this attentional system needs to

813
01:22:03,760 --> 01:22:08,240
have certain properties like it needs to have an index memory of the states that it tended to

814
01:22:08,240 --> 01:22:13,360
it needs to be capable of making sense of its own agency and so on and so on and these are

815
01:22:13,360 --> 01:22:19,120
functional criteria and in some sense the neural network is just software right it's a software

816
01:22:19,120 --> 01:22:24,720
that might not be easily intelligible to human beings but we can translate it into something

817
01:22:24,720 --> 01:22:30,240
that is computational equivalent and isn't intelligible and because we can make these

818
01:22:30,240 --> 01:22:36,640
systems expandable we can as soon as we identify the formal definition of what we mean by consciousness

819
01:22:36,640 --> 01:22:41,120
look whether the system is conforming to that formal definition that is if it's implementing a

820
01:22:41,120 --> 01:22:47,120
certain list of functionality that we require and we can also test this basically we can implement

821
01:22:47,120 --> 01:22:52,240
a minimal system that is implementing all these functions and then see whether it's also a system

822
01:22:52,240 --> 01:22:58,160
that you would point at when we say that it's conscious so for instance if that system is

823
01:22:58,160 --> 01:23:03,600
able to use language and is entangled with the environment and can be able to learn a language

824
01:23:03,600 --> 01:23:09,040
that enables it to speak about this environment is it able to talk about itself as a conscious being

825
01:23:09,040 --> 01:23:14,320
is it going to talk about its own phenomenal experience when we ask it without lying without

826
01:23:14,320 --> 01:23:19,600
having an additional mechanism that tricks us built into it and so the hypothesis would be

827
01:23:19,600 --> 01:23:25,680
if we build a system that is has such an attention agent that it is acting on top of a perceptual

828
01:23:25,680 --> 01:23:30,960
agent and makes sense of it and reflects on it in a similar way as Vito on this system is able

829
01:23:31,040 --> 01:23:36,720
to learn a natural language that we would be able to communicate about its phenomenal experience

830
01:23:37,440 --> 01:23:46,080
visit. Daniel, did you have a question? Is it your wrist? Yeah, yeah, yeah, I also have a question.

831
01:23:46,080 --> 01:23:52,480
So, Josia, first of all, it was fascinating to have this conversation today and see this

832
01:23:52,480 --> 01:23:59,280
big idea with motivation and attention working with a perceptual agent. I couldn't stop thinking

833
01:23:59,280 --> 01:24:05,120
about the conversational AI and stuff we discussed like a year ago and what kind of things we also

834
01:24:05,120 --> 01:24:10,880
worked the deep power to build a social board for Alexa Pride 3 and Alexa Pride 4 now. I was wondering

835
01:24:11,600 --> 01:24:18,960
if you tried to, it doesn't matter where, but have you tried to bring these ideas of attention,

836
01:24:18,960 --> 01:24:26,400
maybe some feelings like fear and other things to the conversational AI experiences?

837
01:24:26,960 --> 01:24:32,880
They don't necessarily have to have eyes and build eye contact with a person,

838
01:24:32,880 --> 01:24:37,840
but as long as they have a conversation, have you tried to go that deep into the projects you

839
01:24:37,840 --> 01:24:43,200
worked on over your career to build this kind of things? Because things like what we work on at

840
01:24:43,200 --> 01:24:48,720
Depall of right now is things like we're trying to build motivation for the bot. So, we're trying

841
01:24:48,720 --> 01:24:54,960
to build a goal of war bot where it is a war of the goals that the user has and also the war

842
01:24:54,960 --> 01:25:01,840
of its own goals. When bot has a conversation with the user, we try to build a three-level

843
01:25:02,720 --> 01:25:08,080
dialogue planning where each time bot wants to say something, it looks at the goals of itself,

844
01:25:08,080 --> 01:25:13,520
the goals of the user, then it makes a decision which goes to follow based on that it defines

845
01:25:13,520 --> 01:25:18,240
discourse where it wants to go and then based on the discourse, we try to pick the exact next step

846
01:25:18,240 --> 01:25:26,800
in the conversation that they want to do. When I look at your picture, it strikes me that our

847
01:25:26,800 --> 01:25:31,760
goal of war dialogue management in many ways is the same idea that they have that motivation

848
01:25:31,760 --> 01:25:38,080
built into the system that we were lacking in our previous international or social bot.

849
01:25:38,080 --> 01:25:43,360
So, obviously, you worked way, way more in the conversation where I was just wondering how far

850
01:25:43,440 --> 01:25:52,320
did you vent in the direction and what you could recommend for aspiring minds in the direction?

851
01:25:53,520 --> 01:26:00,640
My own impression when we did this at AI Foundation, we failed and we failed for a number

852
01:26:00,640 --> 01:26:05,760
of reasons. One of them was, of course, we had a relatively small team working in a startup

853
01:26:05,760 --> 01:26:11,120
and the majority of what we had to do in a startup was related to getting a product on the road.

854
01:26:11,680 --> 01:26:16,960
But the main issue is that the stack of solutions in the present machine learning

855
01:26:16,960 --> 01:26:24,240
environments is not suitable for real-time entanglement with the environment. And to build

856
01:26:24,240 --> 01:26:29,840
a system that is able to make sense of the universe autonomously, you need to have this

857
01:26:29,840 --> 01:26:36,000
real-time coupling, I think. So you basically want to have a system that is always able to make

858
01:26:36,000 --> 01:26:40,960
sense of the next frame. Imagine you want to build a machine learning system that you just

859
01:26:40,960 --> 01:26:47,120
connect to a bunch of cameras and after a certain time it's able to understand that the changing

860
01:26:47,120 --> 01:26:52,960
lighting conditions are due to the sun and the sun is some kind of circular objects that move

861
01:26:52,960 --> 01:26:58,880
through the sky and it is at a certain almost infinite distance and so on and so on. So,

862
01:26:58,880 --> 01:27:04,960
relatively simple things that every mammal is able to figure out, but that none of the

863
01:27:04,960 --> 01:27:08,400
machine learning systems at the moment seems to be able to figure out autonomously.

864
01:27:09,360 --> 01:27:14,880
And it's not because it's so hard to do this, but because all our efforts are going into

865
01:27:15,680 --> 01:27:22,800
doing batch processing on image databases, rather than interacting with the real world.

866
01:27:22,800 --> 01:27:26,800
Of course, also the interaction with the real world and online learning requires that we are

867
01:27:26,800 --> 01:27:32,320
much, much more efficient at extracting structure from the data. And there's still this difficulty

868
01:27:32,320 --> 01:27:36,720
that our own bootstrapping as an organism takes many months before we're able to make sense of

869
01:27:36,720 --> 01:27:42,720
visual data. And we don't want to wait months before our machine learning system converges to

870
01:27:42,720 --> 01:27:50,320
basic image understanding. So, maybe we need to have a combination of online and offline learning,

871
01:27:50,320 --> 01:27:56,000
at least in the beginning. And there are many technical things that have to be solved for this.

872
01:27:56,640 --> 01:28:02,000
For social motivation, we probably need to look at the things that enable us to

873
01:28:02,960 --> 01:28:09,280
develop this kind of motivation. I started out with a theory that assumes that we have a few

874
01:28:09,280 --> 01:28:15,200
hundred physiological needs that we need to regulate for, but that's boring. And we have

875
01:28:15,200 --> 01:28:21,120
something like a handful or two handful of social needs that structure our social interactions.

876
01:28:21,120 --> 01:28:25,840
They're basically priors in the way in which we want to interact with others. For instance,

877
01:28:25,920 --> 01:28:30,720
people might have innate need for status for raising, rising in a social hierarchy.

878
01:28:31,600 --> 01:28:38,400
And this is just a bias in the system, a reflex. And you can eventually replace this reflex by

879
01:28:38,400 --> 01:28:43,760
something that it's instrumental to. So, maybe you want to organize the world in the best possible

880
01:28:43,760 --> 01:28:49,360
sustainable way. And now you do not want to have power for its own sake. You want to have

881
01:28:49,360 --> 01:28:56,800
power according to your abilities and incentives to get things right. And so, your desire for status

882
01:28:56,800 --> 01:29:02,560
gets completely supplemented by conditional behavior. And then you also need to have a bunch

883
01:29:02,560 --> 01:29:08,800
of social needs, of cognitive needs that regulate exploration versus exploitation. So, a desire for

884
01:29:08,800 --> 01:29:15,280
competence versus uncertainty reduction. And they will structure the way in which you interact

885
01:29:15,280 --> 01:29:18,960
with the environment. And as soon as you will understand the environment better, these innate

886
01:29:18,960 --> 01:29:22,160
needs and their weights get replaced by something that is conditional again.

887
01:29:25,920 --> 01:29:30,000
If you want to build an agent that is interacting with people in an interesting way,

888
01:29:30,000 --> 01:29:34,240
I suspect it's also necessary that this agent basically has something like an attentional

889
01:29:34,240 --> 01:29:39,040
idol loop where it's looking into the world and thinks about, oh, this is what I'm looking at.

890
01:29:39,040 --> 01:29:44,000
Oh, there is somebody coming. Do I know this person? Should I interact with this person? In which way

891
01:29:44,000 --> 01:29:49,520
should I interact with this person? And it should be giving science of all these processes taking

892
01:29:49,520 --> 01:29:55,920
place. So, you need to understand by observing the system which state that system is in. And we

893
01:29:55,920 --> 01:30:00,880
also need to understand that emotions have not evolved as a display of the internal state that

894
01:30:00,880 --> 01:30:09,200
you are in, but mostly in an adversarial condition. But as soon as we became social agents and were

895
01:30:09,200 --> 01:30:15,760
observing each other, we were using this observation to game others, to understand how they are,

896
01:30:15,760 --> 01:30:20,000
to exploit them, to control them. And this means that even small children learn to

897
01:30:20,640 --> 01:30:26,480
hide their emotional state or to coat their emotional state. And so, when you see somebody

898
01:30:26,480 --> 01:30:31,040
at the funeral, you are not just looking at whether they are looking somber because everybody is

899
01:30:31,040 --> 01:30:37,200
looking somber at the funeral. But you are looking at what kind of somberness they are

900
01:30:37,200 --> 01:30:42,960
displaying and how the somberness is being achieved and what this actually indicates.

901
01:30:42,960 --> 01:30:48,880
So, you are looking at at least two levels of structure and the social persona becomes a puppet

902
01:30:48,880 --> 01:30:56,800
that everybody is controlling according to their internal puppet. And the social puppet can become

903
01:30:56,800 --> 01:31:01,920
so dominant that people forget that they have an authentic structure behind it. So, people have

904
01:31:01,920 --> 01:31:06,240
something like a core self that is like childlike self and all this machinery that they built on

905
01:31:06,240 --> 01:31:12,080
top of it and they can get lost in the machinery. And it can be very difficult to reawaken the core

906
01:31:12,080 --> 01:31:17,760
self into something that is actually interacting with the environment is getting real. And these

907
01:31:17,760 --> 01:31:21,840
interactions are what makes it very, very interesting to interact with people to which degree

908
01:31:21,840 --> 01:31:28,160
can you go beyond the social puppet and interact with the core self and establish intimacy.

909
01:31:30,480 --> 01:31:34,960
I loved it. And I mean, obviously, we have multiple puppets that we have with different

910
01:31:34,960 --> 01:31:40,560
people. And for social boards that we had in the last cohort of Alexa Price 4,

911
01:31:42,080 --> 01:31:50,160
they at best had, I don't know, maybe one very fractured, very small kind of puppet that was

912
01:31:50,160 --> 01:31:55,600
really, really limited in what it could achieve. So, for instance, I think that gender is a good

913
01:31:55,600 --> 01:32:01,120
example. Gender is in some sense such a puppet. It's a costume that we wear. And it's socially

914
01:32:01,120 --> 01:32:05,200
constructed in the sense that gender is a model of what other people think of who we are.

915
01:32:06,320 --> 01:32:12,320
And if you confuse this puppet with your core self, you go into all sorts of contradictions

916
01:32:12,320 --> 01:32:16,160
because you can no longer get your models to converge because you think that your puppet is

917
01:32:16,160 --> 01:32:20,000
immutable. So, gender becomes a costume that you are unable to take off again.

918
01:32:20,560 --> 01:32:28,240
Yosha, I have a question. I want to sort of go back to that first cell that sort of gave life

919
01:32:28,240 --> 01:32:35,760
to everything. If we look at how cells behave, they behave very intelligently. It seems like

920
01:32:35,760 --> 01:32:42,400
they already have this building intelligence and they know to what kind of tissue they have to

921
01:32:42,400 --> 01:32:48,720
split and become. Do you agree with the statement that sort of like intelligence already

922
01:32:49,680 --> 01:32:55,760
within the cell and this sort of like expanse? It's not like intelligence only in our brain,

923
01:32:55,760 --> 01:33:03,360
but intelligence in our body, in like on the very core of our existence.

924
01:33:04,320 --> 01:33:10,960
When I was a kid, my grandparents bought a chess computer for me. And I was bored by this chess

925
01:33:10,960 --> 01:33:16,480
computer because I didn't perceive it as intelligent. It was able to play chess much better than me

926
01:33:16,480 --> 01:33:25,040
unless I would turn down the difficulty level. But it was a toaster. It was not able to do anything

927
01:33:25,040 --> 01:33:30,560
but to play chess, which means that according to a simple function, it was calculating the move

928
01:33:30,560 --> 01:33:35,840
that most probably would let it win the game and it was able to do this better than me. But it was

929
01:33:35,840 --> 01:33:41,040
not able to make any kind of new model of the environment. And to me, intelligence means that

930
01:33:41,040 --> 01:33:45,440
you are able to model something new, that you are able to solve problems that you couldn't solve

931
01:33:45,440 --> 01:33:53,600
before, that you're able to generalize and so on. And this modeling capability is also something

932
01:33:53,600 --> 01:33:59,920
that probably eludes the individual cell. The individual cell has the an operating system

933
01:33:59,920 --> 01:34:07,040
genome that is encoded in the DNA of the cell that allows it to decide, giving the environmental

934
01:34:07,040 --> 01:34:14,320
conditions what to do best. And it's either going to differentiate into a different type of cell,

935
01:34:14,320 --> 01:34:18,960
which means it's going to turn this operating system into a different configuration, or it's

936
01:34:18,960 --> 01:34:23,840
going to divide into two cells, or it's going to regulate something, or it's going to die.

937
01:34:24,640 --> 01:34:30,640
Apoptosis, the voluntary death of the cell. And these are the mechanisms that are available to

938
01:34:30,640 --> 01:34:36,560
the cell. This is all it ever does. And in order to make that decision, the cell is able to integrate

939
01:34:36,560 --> 01:34:43,520
over its inputs. And it can integrate by looking at the configuration when a neuron can look at a

940
01:34:43,520 --> 01:34:48,720
configuration of electrical impulse that come in. And this integration happens in time and in space.

941
01:34:49,600 --> 01:34:54,800
But there are very tight limits on how much temporal memory the cell has, how long it

942
01:34:54,800 --> 01:35:00,800
remembers that it has seen a certain thing in its environment, and how well it can predict in the

943
01:35:00,800 --> 01:35:07,520
future what it should be doing. And so the functions that the cell can learn are very limited. And we

944
01:35:07,520 --> 01:35:13,760
can probably quantify this, what it can do. And I don't know the exact quantifications,

945
01:35:13,760 --> 01:35:17,440
and no number of people have given me their estimates, and they're vitally diverged.

946
01:35:18,720 --> 01:35:24,480
But the cells don't seem to be universally intelligent. There seem to be, of course,

947
01:35:24,480 --> 01:35:29,200
they can learn. So they are intelligent to some degree, they can make models to some degree.

948
01:35:29,200 --> 01:35:33,120
But the individual cell seems to be extremely limited in what it can model.

949
01:35:34,080 --> 01:35:40,160
But we can say the same about people. I mean, if we look from perspective of universe,

950
01:35:40,160 --> 01:35:46,320
of multiverse, and what is out there, there are so many things we don't know. And our memory is

951
01:35:46,320 --> 01:35:52,880
also short. So from someone's perspective, we can be also very limited intelligence, right?

952
01:35:52,880 --> 01:36:01,520
It's not like this super generalizable intelligence that knows everything about all

953
01:36:02,320 --> 01:36:09,120
Yes. So maybe we can come up with the laundry list or with some kind of achievement hierarchy

954
01:36:09,120 --> 01:36:17,120
of minds. And so are you able to learn at all? When you learn, can you integrate over time and

955
01:36:17,120 --> 01:36:21,920
space, the features that you're learning over, to which degree can you do this? What is the

956
01:36:21,920 --> 01:36:28,720
complexity of the function that you can learn? And so the next interesting step is the function

957
01:36:28,720 --> 01:36:33,280
that you can learn to learn complete. Of course, you yourself need to be doing complete to learn

958
01:36:33,280 --> 01:36:38,160
interesting functions because you want to associate arbitrary states with arbitrary transitions

959
01:36:38,160 --> 01:36:43,840
between them. But when you make your model, can your model contain a Turing machine?

960
01:36:44,720 --> 01:36:48,320
For instance, when you learn that something in the world is an agent, you need to be able,

961
01:36:48,320 --> 01:36:53,040
if you want to model that agency, construct an agent in your own mind, which means you need

962
01:36:53,040 --> 01:36:57,600
to construct something in your own mind that is Turing complete. I'm not sure that cells are able

963
01:36:57,600 --> 01:37:02,240
to understand the agency of parts on the environment instead of just learning associations.

964
01:37:03,120 --> 01:37:10,080
And so the switch from correlational models to causal models and from causal models to

965
01:37:10,080 --> 01:37:14,480
complete state machines that are Turing complete is an important step in the maturation of

966
01:37:14,480 --> 01:37:21,280
intelligent systems. The next step, or maybe not the next, but one that is definitely important is

967
01:37:21,280 --> 01:37:25,760
do you understand minds? Can you build things that are intelligent? And this is something

968
01:37:25,760 --> 01:37:32,080
that humans are about to do, I think. So there is a probability that we are generally intelligent

969
01:37:32,080 --> 01:37:37,280
in the sense that we are able to understand intelligence in general by building and demonstrating

970
01:37:37,280 --> 01:37:43,520
that understanding. And I don't know if there is another stage after that that is interesting,

971
01:37:43,520 --> 01:37:51,440
maybe understanding existence. So you don't know if there's next level up to this sort of general

972
01:37:51,440 --> 01:37:56,640
intelligence, right? Yes, so I wonder if there is anything left after that that is interesting,

973
01:37:56,640 --> 01:38:02,800
that is not just scaling up. So basically you believe that humans are capable to build

974
01:38:05,120 --> 01:38:09,760
generalizable intelligence? Oh, we are currently testing that hypothesis, right? We don't know yet,

975
01:38:09,760 --> 01:38:13,120
but that's basically the idea of Turing, that humans are generally intelligent,

976
01:38:13,120 --> 01:38:15,120
you should be able to build systems that can think.

977
01:38:15,920 --> 01:38:25,920
Thanks, thanks a lot. I think I feel like we should repeat it because I have a long list,

978
01:38:25,920 --> 01:38:33,120
but I feel like we're already almost two hours. Yeah, we would really love to have you again,

979
01:38:33,120 --> 01:38:37,680
Ayosha. Thanks, thanks a lot for your time. Thank you, it was a lot of fun. Have a wonderful

980
01:38:37,680 --> 01:38:48,240
rest of the day wherever you are in the world. Yeah, thanks a lot, Ayosha.

