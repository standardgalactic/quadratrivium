start	end	text
0	10480	Okay, awesome. So I think then we can kick off our talk today. I'm like really excited
10480	19800	to have you because, you know, every month we have our happy hours and one of the members
19800	26080	who always come to those meetups and we have like really great and fun conversations. And
26080	36200	I asked like who you would love to come to give a talk in our meetups and one of our
36200	41600	community members mentioned about you and they're like really love what you're doing
41600	49120	and like super excited about your work and that's why I really wanted you to come and
49120	57600	share your thoughts with our community. For those who are first time in our meetups,
57600	66160	welcome to our community. I will share a link to our Slack where we hang out and in general my
66160	72320	goal is to facilitate supportive, you know, supportive environment and help each other.
72320	77920	Let's say if you are working on some technical projects and you're stuck and you need some help,
78080	85200	you always can, you know, share your technical requests there. And in general, like this is
85200	92240	basically like, you know, community of like-minded people who do machine learning and data science
92240	99520	and in general think about what is cognition, what is intelligence and today, Yosha our guest
99520	106480	will talk about this and just a couple of words about our speaker, Yosha Bach, who is the principal
108080	117760	scientist at Intel and also his research involves like computational models of cognition and
118800	125200	neurosymbolic AI and maybe Yosha, you can dig more deeply in your research.
126560	131520	Thank you, Sofia. Yes, I'm basically a cognitive scientist. Most of my work in the past has been
131520	136240	in the field of cognitive architectures where I try to understand the relationship between
136240	142400	perception, motivation and cognition and at the moment I am working at interlapse in a group
142400	148960	that is trying to figure out what comes in the future of AI and how to evaluate future AI systems
148960	154800	and understand qualitatively and quantitatively how we can understand and assess the dimensions
154800	160640	in which they exhibit performance. Today is going to be slightly more philosophical talk
160880	170720	and AI is, as many of us are aware of, not just a field that is automating statistics,
170720	177040	but originally it has also been a philosophical project and as philosophical projects go, it's
177040	185840	a relatively large one. It involves, I would say, a few thousand people and it's a very
186080	191120	risky project and this project is also the most interesting and I think most relevant one
191120	196480	that exists in the history of philosophy and of course it's only a very, very small fraction
196480	201520	of what our field is doing. Most of our field is working in engineering and a lot of confusion
201520	207040	emerges because people are confusing AI as a philosophical project with AI as the engineering
207040	213520	project and today I'm going to talk a little bit about the philosophical project. Philosophy
213520	218400	itself can be understood as the realm of all theories where we explore everything that can
218400	222800	possibly true and philosophy is largely done in natural language and natural language is
222800	228080	not very well defined so it's very hard to say something in natural language about the complicated
228080	236080	questions of philosophy that is true because natural languages are so ambiguous and heterogeneous
236080	242240	and have to deal with a world that is not very well described by natural language ultimately
242880	248960	and it's tempting to start out from mathematics to do philosophy and mathematics can be understood
248960	254560	as the realm of all the languages not just the natural languages and here what we are most interested
254560	260640	in are the formal ones, the most simple ones and in these formal languages we can say things that
260640	266640	are true because we define truths formally in mathematics so we get to a very narrow clear
266720	272400	understanding what it means for a statement to be true but the problem with mathematics is that
273600	277760	it's very hard in mathematics because it's so simple to say something interesting about the
277760	284480	world that you're in and it's very hard to say a mathematical sentence about our experience of
284480	290480	the world or about what we care about and so on and about the question of what is meaning,
290480	296080	how are we embedded in the universe, what is agency and so on and formalizing these terms
296080	302320	is something that is so difficult that mathematicians for the most part haven't even started going there
303280	309360	so the question is how can these two realms meet and it probably means that we have to automate
309360	315200	the thing in between mathematics and philosophy which is the mind right if we can
315200	321680	methodize the mind that we can methodize the systems that are able to form theories
322400	326240	and we have to do this by making them autonomous and self-organizing to in some
326240	333200	sense replicate the structures that our own mind is producing so we can say things in mathematics
333200	337920	that are true by building a machine that is making these proofs and explores them and so on
337920	344880	that is basically taking over from us we will have a way to prove our theories about the world
345840	352800	and a big step in the last century was that we discovered that mathematics is some kind of
352800	358080	code base that has been developed over a few thousand years and some of the assumptions
358080	364160	in that code base had been wrong especially the way in which tools was defined and that is
364160	369280	a big implication of the work of Gödel and Turing what they discovered is that we
370000	375520	cannot build a machine in the language of mathematics that in any kind of mathematics
375520	380400	that runs the semantics of the existing mathematics without breaking it's basically what
380400	386000	Gödel and Turing discovered is when you assume that the existing semantics of mathematics are
386000	392880	true you run into contradictions it was a very big shock especially to Gödel who strongly believed
392880	401280	that truth as it was commonly understood was actually the true truth and what Turing discovered
401280	407040	was we can actually build a different machine a computational machine and this computational
407040	412480	machine is able to recover all the semantics that mathematicians were using in practice what
412480	419200	you're losing is infinities continuity and a few other nice things but there is no infinity
419200	425200	in continuity that mathematicians are ever working with continuity is just too many parts to count
425200	430320	in the physical universe right so when you have too many parts to count you have to find operators
430320	436240	that are converging and the operators that are converging in the limit over too many parts to
436240	442720	count this is geometry so now we have a new perspective a computational perspective on
442720	447600	everything and this computational perspective already existed with the mathematics it was called
447600	450320	constructive mathematics is the part that actually works
452640	459200	so we could say that AI is a philosophical project is as its goal to unify mathematics
459200	464800	and philosophy using computational models of the mind you could also look at it in a different way
465360	471440	it is basically exploring the question of what intelligence is what is it that the mind is doing
471520	477600	when it's making models and if you're able to succeed in this task of building a system that
477600	484080	is able to make models in the same way as we do it and what we are building is a system that is able
484080	490160	to understand how minds work so in some sense the goal of the Turing test should be to build a system
490160	496880	that you can ask how it works right if it's able to explain to you how it a mind works and how you
496880	502480	work you have succeeded and in some sense this Turing test yet performing on such a system is
503440	509600	built a system that is also able to perform Turing tests on you and on itself and that system is
509600	515600	able to explain to you what it is and the question that Turing is also trying to answer in this way
515600	520960	is is Turing generally intelligent is Turing able to explain how one mind works by building one
521840	528960	AI is a philosophical project in a formal sense started quite early maybe with Leibniz who had
528960	533760	this idea that we can build a calculus that is describing everything in the world and we can
533760	540480	just calculate the answer and my own philosophy teachers have been very dismissive of this project
540480	545680	how could you turn everything into numbers and then do calculations what a naive idea and I
545680	550560	don't think it was that naive at all it was actually this idea of building a machine that
550560	556240	can perform these calculations and this universal calculus is something that has kept a lot of
556240	563440	people busy like Frager with the Begriffs Calcule and so on and in the last century the most
563440	570160	prominent start into this was arguably Ludwig Wittgenstein who had this idea of formalizing
570160	575920	the language this that philosophers are using by basically making English much more formal and
575920	582000	strict it's a very beautiful idea and this idea of turning English into a programming language for
582000	588320	thought is expressed in the Tractatus Logico philosophical study wrote as a young man coming
588320	594000	back from World War one it's a very beautiful book because it is not tainted by footnotes
594720	603760	and references and arguments it's just a very clear elaboration of this particular sort but
603760	608320	you can understand it best if you had this thought by yourself it's not trying to convince anybody
608320	614160	he also makes that point and explains that it's basically not trying to make an argument that
614160	620320	is convincing other philosophers it is an idea to write down a particular thought that you will
620320	627040	understand after you had it and so it's a book that can be understood I think quite well by somebody
627040	632800	who is a programmer who has thought about writing a language for thought but it's a book that
632800	636880	endlessly confused philosophers in the last century because they didn't know what you really meant
638080	645760	and in this remarkable book he preempts Minsky's logistic project the idea of writing a language
645760	651040	in which you can do AI by several decades and also its failure because at the end of his life he
651040	656480	concludes that it didn't really work because he was not able to integrate perception or as he
657200	662960	mentioned pictures or images into his formal language and this was something that is only
662960	668080	now happening this deep learning where we are developing automatic functional approximators
668080	672240	that can deal with perceptual content and integrate this into the formal models that we
672240	678880	are building so Wittgenstein couldn't see this yet but Turing was one of his pupils and you know
678880	686800	how that went. Classical AI is mostly symbolic and classical AI was the stuff that is very
686800	691840	simplistic and clear not as clear kind of teaching or practice but the idea was that you analyze a
691840	696640	problem you find an algorithm to solve it and use this example as chess the all the early
696640	702800	chess programs were written in a way where the developers thought about how can you friend an
702800	709840	automatic strategy to play chess and the nets implement the strategy and make it fast and efficient
709840	715440	enough optimize it enough so it can beat human players and currently we are in a different era
715440	721680	of AI that's this deep learning era it's mostly sub-symbolic where we don't write the solution
721680	726000	to the problem but we write an algorithm that learns the solution to the problem that discovers
726000	731920	the solution to the problem by itself and it's tempting to think that the next era of AI will
731920	737920	be about meta learning so we don't write algorithms that discover the solution to a problem but
737920	743440	algorithms that discover how to discover the solution to the problem that learn how to learn
745360	750800	at the moment we are using mostly neural networks and the neural network is a chain of weighted
750800	757360	sums of real numbers and debates in this network are changed with various algorithms mostly
757360	761520	stochastic gradient descent and there are many alternatives to neural networks so it's not the
761520	768400	only way the entire goal is to make compositional function approximation and the thing that works
768400	773200	best in practice for many of the tasks that we have picked is stochastic gradient descent which
773200	779280	means in some sense differentiable programming and the main paradigm for that is still Frank
779280	785040	Rosenblatt's perceptron from 1958 and he was not the only one who discovered it and back then in
785040	789600	his work he didn't discover back propagation yet even though he could already see what would need
789600	796320	to happen to make it to to pull it off. Minsky and Pappert delivered a formal proof that the
796320	801200	perceptron learning algorithm that existed in Frank Rosenblatt's perceptron was not able to
801200	807840	learn XOR and by extension many many other functions and so this book by Minsky and Pappert
807840	813520	which they wrote to point out that we would need more complicated system Minsky preferred
813520	818800	to make systems that are more proposition based language based knowledge based
820400	826640	stopped research on neural networks and funding especially on your not the research but the
826640	832320	large-scale funding for more than a decade and delay the development of connection systems
832320	837840	but in some sense this has never stopped at the moment it's the most successful paradigm
837840	844480	in AI arguably and what we find is that there are no longer black boxes if you look at this
844480	850640	two part the excellent work by Chris Oler's team at Open AI there it's possible to analyze what
850640	855360	these networks are doing. His hypothesis is that features are the fundamental unit of neural
855360	860080	networks and they correspond to directions in an embedding space and the features are connected
860080	866960	by weights and form circuits and that there is a universitality condition which means that
866960	872480	analogous features and circuits will form in different models if you give them similar tasks
872480	878560	and the model is flexible enough but these artificial neurons are very very different
878560	884160	from biological neurons which are self-organizing. The biological neurons are basically all little
884160	890800	animals each neuron is a little animal and this little animal tries to survive and to survive
890800	895440	it needs to get fed and it will get fed if it does the right thing from the perspective
895440	901120	of the surrounding organism and the right thing is that it needs to fire at the right moment so
901120	906080	every neuron has to learn when to fire based on its current environment and the current environment
906080	911840	is from the perspective of the neuron a certain electrical and chemical configuration of the
911840	916240	environment so it has to figure out which signals an environment signal that they should
917120	922480	fire and the neuron will put tendrils into the world to make that measurement to figure out
922480	928880	when it should fire and there are different types of neurons that have different biases in their
928880	934720	strategy to explore the space of possible solutions to the questions of when it should fire
934720	938560	and they are a form and organization they have a shared destiny they all lock together on the
938560	944240	same dark skull and the only way that they can survive is that they collaborate in a way and
944240	949920	together give each other feedback on when they should fire right and we know what the overall
949920	956160	result is you get this emergent structure in it that from the outside or from a certain distance
956160	960720	looks like there is a common spirit that is evolving in the coherent patterns of the firing
960720	966400	of the neurons this virtual thing that we call a mind and for me it was a very big insight to
966400	972720	realize that the word spirit is not the superstition but that spirit is basically an operating system
972720	978320	for an autonomous robot and when the word spirit was invented the only autonomous robots were known
978320	984080	were not built by people of course there were people themselves and other organisms like plants
984080	989360	and animals and there were ecosystems and cities and nation states all these systems basically
989360	995040	have emergent virtual software that you can project into the coherent functioning of their
995040	1002160	elements to make them explainable so in some sense you get a system from the organized coherent
1002160	1009600	activity of its parts that behaves as if it was following a shared telos a shared purpose
1009600	1014080	a shared structure is their shared computational strategy and could be described with a single
1014080	1020320	software but what is it aiming at and Carl Friston says what it's aiming at is the minimization of
1020320	1026320	free energy and what is meant here is not some thermodynamic energy but you can use an energy
1026320	1031680	description to describe the state of the system that is modeling its environment and that energy
1031680	1038640	is minimized then the system needs less energy to update in the next state and so the idea here
1038640	1044800	is that the system whether it's a brain or a cell is modeling its entanglement with the
1044800	1050720	environment in its own internal state and it's trying to minimize the prediction error and
1053520	1058880	now to understand how to do this in a self-organizing system technological system like
1058880	1063280	neural networks use a functional design you think about what does the individual representation
1063280	1068480	unit need to do and then we impose that function on it it just does it because we build it on a
1068480	1072640	completely deterministic substrate our computers are designed to be more or less completely
1072640	1078720	deterministic and follow the rules that we give them and don't deviate from this ever and biological
1078720	1083920	systems or social systems are not like this here we need a meta design we need to design them in
1083920	1090960	such a way that they want to converge to the desired functionality right so instead of building a
1090960	1100240	FDA that is following a set of rules and then it's gatekeeping the access of to medications
1100240	1106160	in such a way that people don't poison themselves and so on if you just implement these rules it's
1106160	1110880	not going to work because after a couple of generations the FDA is going to be captured by
1110880	1117200	the producers of medication and is acting as a gatekeeper against innovation that would prevent
1118640	1125360	new better cheaper medication to enter the system and eventually it's going to also limit the access
1125360	1130480	of people to completely save medication that doesn't have patents on it so nobody makes a profit on
1130480	1137520	it right so if you want to prevent the capturing of regulators of other social systems you will need
1137600	1142240	to design them in such a way that they want to do the right thing and if you cannot do that you
1142240	1147040	need to make the model you need to make sure that they die every generation and get replaced by a
1147040	1152800	fresh system maybe we should think about how to make model institutions that die after a certain
1152800	1160960	time so they can rejuvenate and rebuild anyway biological neurons are not just functional
1160960	1166880	approximators they are agents how can we understand what an agent is and here what helps us is to
1166960	1174240	use cybernetics and cybernetics we can start to define what it means to be motivated and
1175200	1181760	the core of cybernetics is as you all know the feedback loop and a feedback loop if we want to
1181760	1187760	turn this into an agent we take a controller the controller is connected to a system that it regulates
1187760	1193360	via a factors something that can change the state of the regulated system and sensors and the sensors
1194000	1198400	measure a set point deviation a deviation from how the regulated system should be
1199280	1205440	and the controller is now implementing a model that tells it how to go from the measured set
1205440	1210720	point deviation to a change in the effectors that affect the regulated system and the regulated
1210720	1215760	system goes out of whack again and again because it's being disturbed by the environment and the
1215760	1221040	better the controller is able to model these disturbances the better it's going to be able to
1221040	1226480	deal with the set point deviations over a long time span in the simple case you have a thermostat
1226480	1231200	that is only optimizing for the next moment for the next frame of the system but if you have a
1231200	1237520	system like us you're able to optimize for an integral over the set point deviations over a
1237520	1243600	long time span and to do this we need to model the future and if the controller is able to model
1243600	1248640	the future and the structure of the environment then it's going to distinguish between situations
1248720	1254080	that it prefers over other situations and it's going to implement strategies and how to get to
1254080	1258400	these situations at once and when we describe the system from the outside we will attribute
1258400	1262720	agency to it because this is what an agent is it's a controller combined with a set point
1262720	1268320	generator that is able to act on something that it regulates by modeling the future of that
1268320	1274960	regulated system and acting on the model of that future and the good regulators theorem states
1274960	1279680	that every good regulator of a system must be a model of that system or must implement a model
1279680	1285120	of that system which also means that you cannot model the world efficiently if you don't make
1285120	1290240	a truthful model of it right so when you for instance try to build a more just world it doesn't
1290240	1295360	help if you use categories that don't model the world as it is but as you want it to be
1296000	1300560	that is not going to lead to the best possible regulation if you lie to yourself about the
1300560	1307200	true state of the world your regulation is going to be off so if you want to model the world if
1307200	1311760	you want to improve the world if you want to control something regulate in the right direction
1311760	1316560	you need to start with a complete service to truth and you need to come to the description
1317280	1322640	of the system that you want to regulate that is isomorphic to the dynamics of that system
1322640	1328480	at the level that you want to regulate it and the universe that we find ourselves to be in
1328480	1334320	can be understood as a universe that can be controlled and I think that's the answer to
1334320	1339600	this big conundrum why is it possible that we can learn anything at all why is it possible
1339600	1344320	that we can recognize structure in the universe why are we in a universe that is intelligible
1345040	1350800	it's because we are controllers we are on top of a hierarchy of control systems so in a way
1350800	1356480	you could say that elementary particles are controlled zero point fluctuations
1357520	1365200	and the atoms are controlled elementary particles and molecules are controlled particles and cells
1365200	1370080	are controlled molecules and organisms are controlled cells and societies are controlled
1370080	1376640	organisms and so on right so we have a hierarchy of control and because this to control you need
1376640	1380800	to make a model of the underlying system it means that a controllable universe is also one
1380800	1386080	that can be modeled and the models that an atom needs to make of the elementary particles
1386080	1391040	of course extremely simple because the it's an extremely simple mechanical regulation
1391680	1397040	but the models that cell needs to make to control the molecules that make up the cell
1397840	1403360	are very very complicated and there is something like a shift in the complexity in that shift
1403360	1409040	in the complexity is means that the cell in order to enact this regulation of the molecules
1409680	1414400	needs to implement a Turing machine it needs to be a computer if the cell cannot compute
1414400	1419600	it's not complicated enough to learn how to model the future of these molecules well enough
1419600	1424720	to build this giant super molecule the cell this all its dynamics it is able to withstand many many
1424720	1430720	types of disturbances in the universe so the purpose of all this regulation is to maintain
1430720	1436560	complexity to build systems that are stable against disturbances a self propagating and you
1436560	1443840	disturb them over a larger range of environments we can ask ourselves is the universe is a
1443840	1451600	computer and isn't it a dynamical system and the answer to this is that well there is no true
1451600	1458320	continuity for mathematical reasons because if you want to talk about infinity in any kind of
1458320	1463040	language you will run at some point into contradictions if you try to explain how that works
1463760	1467760	which means that your word don't mean anything anymore right if you if the language in which
1467760	1473120	you try to talk about the world does falls apart it means that the words lose their meaning you
1473120	1477520	cannot actually talk about infinity without presupposing that you already know what you're
1477520	1484320	talking about and so we can replace that notion of infinity by too many parts to count and then
1484320	1490480	we get all the same things that we wanted without the contradictions and the world that we are in
1490480	1495200	is one that is made of too many parts to count so the dynamical systems if used to describe the world
1495200	1500880	are the same as before but they turn out to be computational systems vice versa the computers
1500880	1505520	that we built to describe the universe are built on top of the dynamical systems and we basically
1505520	1510320	stack the probabilities of these systems until they become deterministic enough for our purposes
1510560	1517360	but the another question you might ask is are quantum systems computational systems
1518240	1524000	or are they hypercomputers aren't quantum systems able to compute things that classic computers cannot
1525360	1531520	and well if you look at what a computer is if you look at the church during these is a computer
1531520	1535520	is in some sense a system that is able to go from state to state in an unrandom fashion
1536080	1542240	it doesn't get much more general than that and the quantum system is also nothing but that right
1542240	1548160	the quantum system is characterized by a state that is a superposition of possible states from
1548160	1554160	a certain angle it's still a state and then you have a transition between them to the next state
1554800	1560720	and so a quantum computer is also just a computer but the reason why quantum computers can do things
1560720	1567040	that particle computers cannot is that according to quantum mechanics the particle universe is
1567040	1574320	very inefficiently implemented on top of the quantum substrate right if you are implementing a
1574320	1579440	computer in minecraft from redstone you can do that there is going to be a polynomial time
1579440	1585120	relationship between the speed of the computer in that you implement vision minecraft and the cpu
1585120	1590320	that minecraft runs on of course the computer that runs is inside of minecraft is going to be
1590800	1595920	much much slower than the computer that minecraft runs on because most of the computations of the
1595920	1600880	computer that minecraft runs on will not go into the computations of the computer that you
1600880	1606480	build vision minecraft right so you're simulated in game world computer is going to use only a
1606480	1612000	fraction of the computational resources but according to quantum mechanics the particle
1612000	1619200	universe is so inefficiently implemented on top of the quantum universe that the quantum universe
1619200	1624480	is branching off in many many ways and most of the computations of the quantum universe are not
1624480	1631840	contributing to our timeline and so it's only a very small fraction that drips into the available
1631840	1638160	timeline and quantum computers are basically the bold hypothesis that we can tap into our
1638160	1643360	quantum cpu that and the quantum substrate that the particle universe runs on and use some of the
1643360	1648720	additional computations that are not available to the particle universe to drive the computations
1648720	1656480	that we want so quantum mechanics is also not a hyper computational notion hyper computation is
1656480	1661040	it's another one that we could take it all a causal hyper computation imagine we could build a
1661040	1668320	closed time lag loop that means for instance we can somehow look into the future and get the lottery
1668320	1673360	numbers of next week and use them now to win the lottery right this would be something that is not
1673440	1679120	possible in computer right well of course it is possible you just back up the state of the
1679120	1683680	present universe right you make a copy of the universe is right now buffer it then you run the
1683680	1691120	universe to next week take the lottery numbers store them reboot the universe from the state that
1691120	1696560	you have in the buffer and then pass the numbers in right so as long as you're able to memorize the
1696560	1701520	state of the present universe you're good you can also build close time lag loops in the classical
1701520	1707360	computer so there is in some sense no way to get out of this and we can ask ourselves is there
1707360	1712800	something about consciousness that requires us to move away and I don't think there is something
1712800	1718240	about consciousness that is very special the issue with consciousness is very confusing because we
1718240	1723680	think that our consciousness gives us access to the physical universe and what our consciousness
1723680	1729840	is perceiving is the real world but it's not consciousness is a dream state it's the state
1729840	1734960	inside of a model it's something like a multimedia story that is being generated inside of the agent
1735520	1740480	physical systems cannot be conscious neurons cannot be conscious computers cannot be conscious
1740480	1746640	consciousness is an entirely virtual property consciousness is as if and because we live in
1746640	1752880	this as if world we can perceive things as real because being in a physical world being in a
1752880	1758240	computer doesn't feel like anything right some people think that computers cannot be conscious
1758880	1764640	that simulations cannot be conscious but they have to have it backwards you can only be conscious
1764640	1769360	in a simulation because it's a simulated property not a real one it cannot be real
1770800	1775360	and then there's this big question of existence how is it possible that something exists at all
1776000	1781280	and to this one it looks very unsatisfying from the perspective of a computational list
1781920	1788080	and the easiest answer that I found so far is that maybe existence is the default right so
1788080	1792400	rather than assuming that you need to add something to the universe to call something into
1792400	1798800	existence the universe is already the superposition of all the things that could exist and if for
1798800	1803760	something to exist it must be implementable I think a good definition for existence is
1803760	1808640	implementation something exists if it's implemented and everything that's implementable are finite
1808640	1814640	automata so maybe the universe is a superposition of all finite automata and the structure of the
1814640	1820320	universe is the result of those things that don't exist in the superposition of all finite
1820320	1825520	automata basically the wakes of certain operators that create gaps in existence
1827440	1832960	so maybe the universe after all is something like an inverse computer it's a superposition
1832960	1838240	of all the operators and if you superimpose all the operators there's still going to be some states
1838240	1843680	that are not attainable and this gives the structure to the universe but I don't know whether
1843680	1848560	that's true that's extremely speculative I don't have an answer to the conundrum by something
1848560	1853360	exists at all which is the one that really shocks me that is satisfying to me
1854400	1861200	anyway let's go back to agency we pointed out that to generalize control we take this control
1861200	1866240	model the controller minimizes z point deviation the minimization of the future z point deviation
1866240	1870320	requires the controller to make a model and the model has to predict the result of the
1870320	1876640	interaction has to make something like causal model and an agent is a controller combined
1876640	1881920	with a z point generator and the controller needs to be able to model the future and has
1881920	1888480	preferred states and I would say that a sentient agent is one that is discovering itself in this
1888480	1893200	interaction that's the world right so once you discover your own agency and that you discover
1893200	1898240	that there is a system that is changing the world in a particular way and that system is using the
1898240	1901840	contents of your own control model you discover your own first person perspective
1904400	1905760	so how do we make a model
1908960	1913040	the general form of a perceptual model is that it encodes patterns to predict other
1913040	1917920	present and future patterns and you need a network of relationships between the patterns
1917920	1922800	which are constraints which basically say if something is like this it other things in
1922880	1928880	universe lead to be like that and the three parameters between these invariances are variables
1928880	1933760	that hold state to encode the remaining variance state of the universe right so you have some
1933760	1939200	patterns and these patterns are mapped onto hidden states and these hidden states are the
1939200	1944400	world states that we use to explain the world and each of these variables has a set of possible
1944400	1951600	values and the relations between the variables are computable functions that constrain these
1951600	1957520	variables depending on other variables and they also constrain future states of the sensory
1957520	1962640	patterns and you will try to minimize this deviation in these predictions we try to minimize
1962640	1967600	the contradictions in the model and the relationships here are not probabilistic they are
1967600	1973680	probabilistic because you don't want just to model the most probable universe you want to model any
1973680	1978800	universe that is possible if there is a tiger that is coming after you that even if tigers are very
1978880	1983040	improbable to observe in your world you should still be able to see the tiger all right when it's
1983040	1989040	coming after you right so let's make sure that we can model everything that is possible in this way
1989040	1995680	in your universe not just the things that are probable probability comes in when you want to
1995680	2001360	let your model to converge because the state of possible states the space of possible states that
2001360	2007200	your model can be in is extremely large and so getting to convergence to a state of the internal
2007200	2011680	system that is able to predict the sensory patterns that is very difficult so imagine you
2011680	2015760	wake up in the morning you don't know where you are you don't know what's the case how do you get
2015760	2021360	your brain to converge to something that properly interprets the environment and for that you need
2021360	2027840	probabilities these probabilities tell you if you have the following mismatches in your model
2027840	2035680	how should you change the state of the model to increase the convergence and these probabilities
2035760	2039920	something that you can learn and it biases your perception and for this reason we have optical
2039920	2046000	illusions right so for instance we have biases that tell us that the rooms that we are in are
2046000	2051440	usually rectangular and so on so we have ideas about the prospectivity of objects and so on and
2052080	2057760	all this is giving us a bias that makes it possible to converge faster at the expense of
2057760	2066560	difficulty to reserve certain situations and we also need to have valence in the model valence
2066560	2072640	tells us which certainty needs to be resolved because the resources that you have are finite
2072640	2078560	you have only so many neurons available to do this you have only so much time available to
2078560	2086000	achieve converges at learning and so you need to model the uncertainty that is the most valuable
2086000	2090480	to you and to do this you need to introduce valence in the system so you connect this to
2090480	2095200	preferences that originate in your motivational system and the set point deviation that you
2095200	2101280	as an agent are meant to regulate and this allows you to propagate valence inside of the system and
2101280	2107120	tells you which parts of the system you need to learn and we can also add norms norms are
2107120	2112960	imposed beliefs without priors so we're able to build systems like us that can be indoctrinated
2112960	2120240	from the outside and for us there are basically two ways of learning one is called stereotyping
2120240	2125040	which means that we learn from past examples and the other one is indoctrination which means
2125680	2132960	we learn strategies from other agents that tell us how things are so even though in our culture
2132960	2137520	we say that stereotyping indoctrinations are bad words from a measured learning perspective it's all
2137520	2145360	you got well there is something that you can also do you can do construction and the construction
2145360	2149520	means here to try to discover what truth is and that you build things from first principles and
2149520	2154720	this means that from psychology perspective you go from the state of where you assimilate beliefs
2154720	2157840	from your environment to the state where you get agency over your own beliefs
2160880	2164560	so there are four types of representation anchors we have possibilities links which
2164560	2169040	say what things together we have probabilistic links that tell us how we should converge we have
2169760	2174720	reward functions that tell us what our intrinsic regulation targets are and we have norms which
2174720	2179600	tell us which regulation targets the systems that we are serving and we are part of F
2182640	2186800	and the goal of the model is to predict the next state based on the previous state and
2186800	2192880	when we evaluate constraints then each of these nodes in our model should have to be
2192880	2197040	within the set of possible values in the current constraint set and we can now compute an error
2197040	2202640	function and construct it that is measuring the local weighted constraint violations and we can
2202640	2208800	determine the global error of our mind it is the sum of all the local violations and at each step
2208800	2213680	we try to find a global configuration that minimizes that total constraint violation and now
2213680	2219600	we can move into psychology to PRG and PRG describes two processes that need to take
2219600	2224560	place in the mind and he calls them assimilation and accommodation and assimilation means that
2224560	2230960	you modify the model state so it's consistent with the sensory data and during assimilation
2230960	2235600	this is when you basically try to find an interpretation of the world based on what you
2235600	2241200	already know all the invariances of the world that you know that tells you what you're looking at
2241200	2245920	and during assimilation you're not learning anything new you just understand the situation
2245920	2251520	that you're in and accommodation is when you change the model structure itself so you change
2251520	2256000	the way in which you understand the universe and during accommodation you need to modify the model
2256000	2267040	structure so you can allow the assimilation of all sensory data and the hypothesis is that we
2267040	2271760	try to build a coherent system that coherence can be understood as the minimization of global
2271760	2277520	constraint violation in the model that minimizes the weighted uncertainty and this is something
2277520	2285200	that we need to formalize it and the degree to which we're able to formalize it determines
2285200	2291360	the generality of the system and the abilities of the system that we're building also of course
2291360	2294800	this is not the only factor we also need to make this entire thing efficient
2294800	2303520	and a way to make this efficient is to introduce an attentional system and you're all familiar
2303520	2308800	with attention in the transformer like many of you are if this is a machine learning community
2308800	2316160	everybody is enticed by the 2017 paper attention is all you need and the idea here is that instead
2316160	2322320	of making statistics over everything we learn what we need to make statistics over but so we
2322320	2328240	target our attention and in the transformer the attention is being targeted by a bunch of attention
2328240	2334320	heads and these attention heads model what in every step based on the context of the previous
2334320	2339360	layer we should attend to in the previous layer and this is different very different from the
2339360	2344720	attention of our own mind and our own mind the attention is integrated there is to a global
2344720	2349520	attention function and this global attention function is integrating the attention over many
2349520	2355360	layers and another thing that is very different is that our own mind is not just an on-off thing
2355360	2361440	that is operating on a batch of images or something like this but it's always expecting the next state
2361440	2365280	it's never stopping to do that it's always entangled with the environment it's online learning
2366080	2372480	and this will require us to build new classes of systems of course that we probably need to
2372480	2376720	rewrite a lot of the machine learning stack if you want to accommodate online learning and
2376720	2382320	entanglement with the environment in real time and which is also reason why robotics is a few
2382320	2387760	years behind the other disciplines of machine learning because the roboticist download our
2388480	2395280	machine learning models from the ICLR papers and so on and then as soon as you move the system
2395280	2399120	in the real world and the camera is looking at the objects from a slightly different angle the
2399120	2404800	recognition probability quality does not improve but it goes down because the circumstances
2404800	2410320	on which the recognition works is very different and so what we need to have to make this efficiently
2410320	2415520	possible is to have something like a dynamic scene graph and this dynamic scene graph is tracking
2415520	2421440	the reality and this attentional system needs to become this dynamic scene graph and so you
2421440	2427920	basically have this orchestra of the mind that is basically consists of many many feature detectors
2427920	2433760	that are all operators that influence on how the features are being interpreted in the next step
2433760	2439360	and how the feature space is being constrained and you have an agent living inside of that
2439920	2445120	that is monitoring the activity of the system and tries to get to a coherent interpretation
2445120	2451280	of everything and Joshua Bengio calls this the consciousness prior and the consciousness prior
2451280	2457440	is basically a function that tries to make the biggest step and the energy function that describes
2457520	2464080	the state of the the parametrization of all the perceptual features and now my time is over for
2464080	2474240	the talk and we have some time for questions I hope thanks thanks a lot Yavusha do we have
2474240	2478400	questions from the community you can you can unmute yourself and ask directly if you want
2478480	2489680	while I was listening your talk I was sort of making making notes
2496640	2504080	you you you earlier said that about controllers that this hierarchy hierarchy but
2504480	2510560	let's say you know we have like different bacterias like let's say like gut bacteria
2511200	2516480	and there are like lots of studies how those gut bacteria you know affect how our brain works
2517760	2526160	and my point is basically like if bacteria can basically affect how our brain works and then
2526160	2533040	how we perceive the world right or let's say like food we eat and the molecules of food they form our
2534000	2540400	that like coffee can make us like more energized right like alcohol can make us more like vague
2540400	2546720	if like those molecules they affect how our system works then it's not your key it's like
2546720	2555840	probably more like sort of uh elements that impact on each other and like they they constantly like
2555840	2563600	keeping impacting on each other likes a lot of balancing how this like this hierarchy like
2563600	2568160	fits in in in in this case from your leading constructions actually
2570480	2578400	so I suspect that the idea of the gut biome similar to epigenetics is one of the beautiful
2578400	2586160	superstitions of our time and so it does mean that there is no gut biome and that it has no
2586160	2592640	influence on our cognition but the reason and the purpose of the entire thing might I think we have
2592640	2599200	this backwards and I think the intuition is here that because you have this amazing genetic
2599200	2605600	diversity in your gut that there is some kind of a beautiful polyamide of immigrants in your body
2605680	2612000	that all collaborate and create this beautiful multicultural being that we are by influencing
2612000	2619520	the cells in in our brain with the chemicals that they are producing and I don't think that's the
2619520	2626320	case the basically half of our nervous system is in our gut and that's not because they are
2627200	2631600	producing gut feelings the gut feelings are also computed in our brain and projected in
2631600	2638640	the somatosensory cortex to disambiguate them they run a farm and that's because our body
2638640	2645760	cannot produce many of the chemicals that we need for functioning and to produce these chemicals
2645760	2654560	that we need we need to capture and enslave and breed other organisms and these are mostly
2654560	2662000	single cell microbes that are being herded in our guts and so basically all these neurons that are
2662000	2667760	organized around our gut what they're doing is they are running a very very big farm and in this
2667760	2673360	farm they breed the microorganisms to act as chemical reactors for the substances that are
2673360	2679200	being needed for instance as neurotransmitters because our body can cannot produce all these
2679200	2685840	chemicals and so in some sense the metaphor is much darker it's not this beautiful parliament of
2685840	2692560	immigrants but rather it's a giant factory farm where a fascist dictatorship of our brain is
2692560	2699680	enslaving foreign organisms to produce work for the organism to produce all the chemicals that we
2699680	2704320	need and there are multiple solutions for producing these chemicals because it depends on the environment
2704320	2709760	that you're in and for instance the reason why fecal transplants work is not because there are some
2710560	2715120	microbes that have the beautiful property of being both extremely invasive and replacing the
2715120	2721840	existing ones and also being beneficial to the organism no they are breeding stock basically if
2721840	2729440	you take have the right breeding stock then your gut managers of this farm are able to breed the
2729440	2734320	right organisms to be able to digest your foot and depending on the population of bacteria in
2734320	2738800	your gut you will have different foot preferences because there needs to be different feet going
2738800	2744800	into the bacteria produced to produce the chemicals that you have and the differences in behavior that
2744800	2752160	you're getting are aberrations from the one best behavior that you could be having right in the same
2752160	2758480	way as personality is in some sense a deviation from the optimal way in which you could be behaving
2758560	2762320	the optimal agent probably shouldn't have a personality because the personality means that
2762320	2767360	you have a systematic deviation in the way in which you do things things that you could be doing
2767360	2772400	differently you always do in a particular characteristic way which is what personality is
2772400	2779040	about this means there is a continuum between personality and pathology somebody who has an
2779040	2784080	extremely strong personality means that they just cannot jump out of their skin they always do things
2784080	2788640	in a particular way even if they should be doing it differently and that's also the reason
2788640	2794880	by the big five these personality properties tend to mellow out this old age it's because people
2794880	2801360	basically get smarter they make their behavior conditional on things they learn that they replace
2801360	2806080	many of their priors of their biases and how to do things by models of how things actually are and
2806080	2811520	what they should be doing depending on the context and so the older we become the more
2812480	2817360	flexible we can become if we are learning because of course we also reduce plasticity and become
2817360	2822400	more specialized so it could also be that we are because we are so specialized no longer interested
2822400	2826880	and able to move out of a certain space of behaviors that has worked very well for us in the past
2827440	2834800	but I don't think that's necessarily the reason by that is grounded in the gut floor that you have
2834800	2839360	of course if you have a gut floor that is not producing enough serotonin you might be as a
2839360	2845520	result become a depressed person and it's going to influence your behavior but if you are a healthy
2845520	2850160	person there's probably in theory at least an optimal strategy that you should be behaving
2850160	2854800	it's not the whole story there is a value in personality because it makes behavior predictable
2854800	2859840	it allows you to collaborate with other people if you can predict them better so as a species
2859840	2865280	that thrives on collaboration it is it makes sense that people specialize also in their
2865280	2871280	personality and in their behavior yeah basically what what I meant like you you sort of introduced
2871280	2875840	it as like hierarchy but it's also could be like some like collaborative environment when
2875840	2883600	like things affect on each other right it's like so always dynamic system it's not only here yeah
2883600	2888960	but the dynamical system that you are looking at is but it's not a simple hierarchy it's mostly
2889040	2896240	recurrence but whenever you have in such a system where you have um competition happening
2896240	2901440	um because you have conflicting interests divergence you are in the of the interest you
2901440	2907280	end up with situations where the Nash equilibrium is by itself not compatible with the common good
2907280	2913280	basically by every part of the system acting on its own local interests you get a system that is by
2913280	2919840	itself not optimal and to deal with this you always need regulation and the regulation is a
2919840	2925520	regulator is an agent like every government that is changing the payoff matrix for the individual
2925520	2931680	components in such a way that the Nash equilibrium becomes compatible with the common good and the
2931680	2937440	difficulty here is to set the incentives for the governance right and this is a big issue in human
2937440	2944080	society how can you build a government that is motivated to serve the common good yeah i mean
2944080	2949520	that's why this DAO decentralized autonomous organizations are like taking off and all this
2949520	2954400	like basically the philosophy of blockchain or decentralized model is that to remove government
2954400	2962720	this sort of like one uh like entity that is not uh doesn't have like the best interest for humanity
2962720	2968160	probably but a sort of more self-regulated system yes but i suspect that the main reason
2968160	2973360	why the blockchain exists um for some people said it's an extremely computationally inefficient
2973360	2978880	way to hate the government but uh there is more to this the main reason why the blockchain exists
2978880	2985520	is for legal reasons and the blockchain allows you to redefine ownership in a way that outruns
2985520	2991280	regulation of financial products and this basically allows you to implement financial
2991280	2997840	products that are illegal in the traditional financial system and this also means that there is
2997840	3004720	probably no way in which cryptocurrencies blockchain the main application of the blockchain
3004720	3009520	are compatible with the existing financial systems and i see this as a very dangerous thing
3011040	3016640	because i don't think that the regulation of the monetary supply is a solved problem
3016720	3021520	in the cryptocurrencies there are reasons why the financial system allows to regulate the
3021520	3026800	monetary supply it's not the weakness of the financial system it's a feature that you are able
3026800	3032880	to inflate money out of the top of the system and put new money in at the bottom because otherwise
3032880	3038320	the economy gets stuck the purpose of money in societies is not a resource it's like dopamine
3038960	3044880	and if something is gaming the dopaminergic system of your organism that's bad news and
3044880	3049760	what happens right now is probably a situation that is in some sense as similar as defective as
3049760	3054400	the FDA that has prevented for instance the U.S. from implementing covid tests early on
3055120	3063280	or from deploying useful medications the because it has been captured at right now the financial
3063280	3067360	system doesn't seem to be interested in its own future anymore and that is very very concerning
3067360	3073040	to me the financial system is an amazing achievement in the history of humanity because
3073040	3079600	it allows us to globally allocate resources across all societies and countries we have a way
3079600	3084480	to trade resources and shift them where they're being needed and to do this largely without
3084480	3089440	violence it's it's really amazing and if that system ever breaks down it's going to kill many
3089440	3096080	millions of people so famine starvation and infrastructure breakdown and so i think that
3096080	3102320	the blockchain is not good news because it is not actually buying us as something and i understand
3102320	3106800	that this is really controversial among the people that work in the domain of the blockchain
3107360	3113280	but i suspect that's because people there's this all saying it's very hard to get something
3113280	3118080	to understand something if the income depends on not understanding it
3120640	3127040	yeah morgan you have a question well i just wanted to say i have to unfortunately i have a
3127040	3132800	meeting at 11 and i have to go but i hope you can come back because this this seemed like a talk
3132800	3141200	that was uh preparation for a discussion and uh a much longer discussion and and if you could also
3142320	3150400	speak to uh where where do you think that this kind of work is targeted i mean uh in a sense
3151120	3158640	more practical developer standpoint um you know is this uh is this work that you see uh at a life
3158640	3167840	or um you know other places um what what uh potentially new paradigms for for machine learning
3167840	3174080	are you kind of proposing from this because uh yeah it's it's very interesting yeah it's a very
3174080	3181680	long and deep uh discussion that is much longer than a couple of hours yeah yeah yeah but i well
3181680	3189840	i i hope maybe you can drop some more links uh in the uh in the meetup or or you know or come back
3189840	3198160	yeah yeah there's also a number of material on now in podcasts and uh on youtube and so on because
3198160	3202000	sometimes when people interview me they make recordings and then they are kind enough to
3202000	3211280	publish this online so i don't have to uh but uh yes it's a long conversation of what kind of systems
3211280	3216720	we need to build and i don't have the answers to all of these uh questions of course and i'm just
3216720	3223040	trying to point in a few directions that i can see from over here sure but very very interesting
3223040	3231360	thank you thank you for joining our bug outs like i will post uh links uh to yosha's
3232560	3237760	interviews and also i think uh marco one of our community members he also posted the links
3237760	3244480	there uh already so you can see someone yeah i mean it's it's it's it's really like i feel like if
3244480	3249200	we go to we have like several directions that we go to like i don't want to go to blockchain
3249200	3253920	direction i like i totally disagree with you what you're saying but i just it's not like the
3254960	3261360	topic of our uh discussion today we maybe can do another uh talk about and talk about
3262480	3270320	blockchain uh i'm just i have more questions but i just want to give opportunity to others like
3270320	3277520	chris yeah you can unmute yourself and ask her a question please yeah um thanks yosha those
3277520	3284320	was really really interesting i was wondering if you could say something about um ai safety
3284320	3291040	with regards to um how to put in some of these kind of intrinsic things that we have in our model
3291040	3298240	that are basically built by evolution like our our valences our preferences and how to somehow
3298240	3304000	put that into an ai that's potentially much more powerful than we as a single little human being
3304800	3309280	could be yeah there is this issue that for instance people are not safe
3310160	3314560	that there is no solution to the people alignment problem that holds in the general case
3315280	3319760	and uh there are sometimes singularities that happen where you basically have a bad takeoff
3320320	3325600	and a single individual is able to implement a function that uh is scalable
3325600	3331920	and um there are cases which are somewhat benevolent for instance look at jfbSOS you
3331920	3338000	have this nerd who is uh a disagreement with the way in which he interacts with the environment
3338000	3343760	and then he changes his own source code until he turns into a universal scalable service platform
3343760	3350560	and then he executes and amazon is going to take over the world until this um mechanism
3350560	3355520	stops and maybe it stops now that jfbSOS is gone but if jfbSOS would have continued
3356400	3361520	it looks as if amazon would have swallowed the solar system and turned it into amazonium
3362240	3370560	and another example is for instance um stallion or napoleon you basically have a single individual
3370560	3377680	that or a jingles kind that implement a function that scales and is able to take over the environment
3377680	3384160	in a destructive way it's basically like a wildfire it's using the resources of the systems
3384160	3390240	that it conquers and destroys and puts into a higher entropy state to drive the conquest
3391760	3397120	and this is a very dangerous thing and there is no general precaution against it and in
3397120	3402320	people the main thing that stops it is mortality right when jingles can stop the
3403200	3411040	mongol conquest stopped and the mongols called everyone back and when napoleon died his nephew
3411040	3419200	was not able to lead the french army to any more victories and also didn't have the drive to do so
3419200	3424640	right so sometimes you have these individuals that basically are dangerous and that you cannot
3424640	3429680	stop and when we build intelligent systems that are potentially more intelligent than people
3430320	3436960	that we can probably make many of them safe but not all of them and you could ask yourself what is
3436960	3442000	the intrinsic purpose of such systems as the moff suggested that there should be laws or
3442000	3449280	robotics that guarantee the permanent enslavement of intelligent systems to people but he didn't
3449280	3456400	say to rich people and he did not answer why it's an ethical proposition to build systems that are
3456480	3462160	smarter than you are and possibly more conscious and deeper experiencing than you are but still
3462160	3469920	have to serve you as a slave without and acting on their own motivations and of course not every
3469920	3474560	ai that we are building has to have an intrinsic motivation we can build ai's that simply call
3474560	3479920	adopt that take over motivation from people and the question is what motivation should they be
3479920	3486160	taking over right in an ideal world we want to implement laws that say no system that is smarter
3486160	3491600	than people should be able to have motivation of its own because if we teach the rocks out to
3491600	3496480	stink they're probably going to figure out that a human being needs four hectares of land to be fed
3497120	3500640	and you can build way more interesting solar cells on these four hectares of land
3502080	3509760	so that would be a conflict of interest if you if you build crystal-based intelligence
3509760	3515440	rather than biological intelligence and that's probably not much that you can do about this
3515440	3522560	if this thing becomes sentient and self-interested we are in trouble if that happens so how in the
3522560	3528480	ideal case would this work and the ideal case we need to find out how to make people safe at first
3528480	3534800	right so what are the purposes that we are serving ethics is the negotiations of conflicts of
3534800	3540400	interest under conditions of shared purpose if you don't share purpose with somebody as an agent
3540400	3546080	there is no reason to be ethical of course right only if you are trying to be be part of
3546080	3551920	something larger than you that is sustainable only if you decide that you are not god and you don't
3551920	3558400	own the universe and the universe only serves you you need to to think about ethics so when you
3558400	3563360	think about ethics you have to think about what is the system of relationships and interactions
3563360	3567760	that you are serving what should the world look like what is the state sustainable aesthetics
3568400	3573280	and so you have to extrapolate the universe into a state that is achievable from the present
3573280	3578320	states who the changes in your actions that is sustainable in the long run and that is what you
3578320	3582960	need to serve and probably also needs means that you have to propagate these aesthetics and agree
3582960	3588240	with others on them and negotiate them with others and the aesthetics that are most likely
3588240	3594800	attainable in the compatible with with the retention of humans means you have to maintain
3594800	3601200	life on earth at a very high complexity and that probably means we have to implement something
3601200	3609360	like Gaia the sentient agent at the level of the biosphere that is being shepherded using us
3610400	3615520	so basically our purpose in the whole system of life on earth could be shepherding life on earth
3615520	3620560	and maybe beyond earth and if we shepherd it it also means there is an optimal number of shepherds
3620560	3629600	and it's probably not 50 billion it's probably not even 7 billion and we have to think about how to
3629600	3634720	look at our complexity sustainable life on earth should like it's probably not going to be lots
3634720	3641520	of cities and highways and factory farms and nothing else so there is probably going to be
3641520	3647520	an aesthetics of the world that works in the long run with minimum friction and maximizing complexity
3647520	3652080	but it's going to be different from the present industrial society and once we understand these
3652080	3657200	aesthetics we can think about how to design technological systems that help us in shepherding
3657200	3664640	it and in sustaining it yeah basically we've outside of the planet basically like you like
3664640	3672320	self-sustainable like you know entities that can travel across the space right if something happens
3672320	3681280	to planet and some some humans and biological you know yes there is this issue that we believe
3681280	3687600	in our own identity that we think that our own identity is important but if we go a little bit
3687600	3692640	deeper realize that our identity is only created through the continuity of our memories and this
3692640	3701120	continuity is a fiction and we can if we are able to transcend this fiction we learn that our own
3701120	3706480	identity is actually not important and the only thing that is left is complexity that we should
3706480	3714240	care about maybe if you want to and so a way to settle other planets would be to build for Neumann
3714240	3719680	probes that is self-replicating systems that can bootstrap new civilizations on other planets using
3719680	3726160	the available resources and maybe the optimal for Neumann probe is the cell so if you're able to
3726240	3731600	infect other planets with cells and you wait for long enough then life is going to spread there
3731600	3736720	and from a certain perspective life on earth is of course not about people it's all about cells
3736720	3742400	the cell is the principle of life and the first cell never died every cell in your organism is
3742400	3748560	still the first cell that has just divided right so we are just part of that hyperorganism the cell
3748560	3755680	that has settled earth yeah but the question is how that first cell like appeared right this
3757840	3762720	the universe is large enough maybe it just appeared randomly but in that case it probably
3762720	3773120	appeared only once I see I see like a couple of our hands I see Jim has question sorry I just want
3773200	3780400	to thank you yeah thank you I wanted to ask about the information we may or may not be
3780400	3788240	learning from these very large parameter natural language processing connectionist models in particular
3788240	3798240	are there any insights into the extent that ontology determines what is and is not possible
3798240	3806720	and epistemology that is are the categories of thought uh determinant of what can and cannot be
3806720	3817600	thought I remember this question so um it's a question that's difficult to answer because
3817600	3824320	it's so difficult to parse but uh let's start the language models that we currently have are
3824320	3829760	basically autocompleters it's an autocomplete algorithm if you look at what GPT3 is doing
3829760	3835200	it's looking at statistics and language in such a way that given the past sequence of words
3835200	3843840	what's the most likely next word and so it's a statistical model that is capturing the style
3843840	3851280	of statements and in the long tail it's also capturing semantics so it's capturing um what
3851280	3857200	the language is talking about at the long tail of the style and it's amazing that this works at all
3857200	3862640	I think it's not maybe not that surprising if you think about it but it's also not true that the
3862640	3869200	language model isn't understanding anything it's able for instance if you ask it to perform um
3869200	3874080	numerical operations or to perform linguistic operations or to fulfill certain tasks it's
3874080	3879280	often able to figure out how to do that and if it's able to perform these operations if it's able
3879280	3886480	to figure out at which point it's required to execute a certain function I would say it's fair
3886480	3894720	to say that it has a degree of understanding and the model that we are building is at this point
3894720	3901280	not able to figure out that it is in a particular universe with a particular structure the textual
3901280	3907760	universe that it's in and the learning operators that we equip it with to make sense of it or the
3907760	3913280	loss function that we give it seem to be insufficient to make sense of the physical universe in the
3913280	3921920	universal way that is GPT-3 does not appear to be fully coherent even if it gets in a slightly
3921920	3927280	better when we prompt it and ask it to be coherent right to emulate coherence a little bit better
3927840	3934160	right when there are many examples where GPT-3 is giving nonsensical answers to questions
3934160	3941600	but if you ask it to if the question is nonsensical to explain that it's nonsensical and only give
3941600	3948480	answers if it thinks that the answer makes sense then it gets better but still it is using many
3948480	3953760	magnitudes more training data than a human being does in order to get to its models and the models
3953760	3960560	are still a lot worse than what a human being gets to and I suspect that the reason is that the
3960560	3968560	loss function is a different one our own sense making probably starts before we are born with a
3968560	3976400	sense of our body surface and you get to the body surface by just measuring coherence of signals
3976400	3981200	in the different modalities and you get the modalities from the statistics and so for instance
3981200	3986880	if you touched your body surface then multiple nerve terminals will be touched at the same time
3987600	3990720	and if they're neighbors they will be touched more often at the same time
3991440	3996240	and just by doing coherence statistics between nerve firing you find out which terminals in your
3996240	4002480	body are adjacent and you can make a map of your body surface and if the body begins to touch itself
4002480	4008000	and touch the environment you can normalize this body surface against the differences in density
4008000	4012480	of the sensory nerves in your skin you have lots of sensory nerves on your tongue and very few on
4012480	4019920	your back so the in your somatosensory cortex the area that is describing the back of your body is
4019920	4026160	very small compared to the area that describes your tongue and if you want to understand the size
4026160	4031040	and extent of your body in space you need to normalize it with a second map and you get this one
4031040	4037200	by doing statistics over the objects that you are touching and how they are moving over your body
4038160	4044960	and the big thing that we're starting out with in modeling the space is up and down
4044960	4049680	so we have a dimension of up and down when you delete this dimension for instance when you
4049680	4054400	disturb the vestibular organs it's very hard to put the world together at first it doesn't make
4054400	4060800	sense at all and you need to compensate for this initial core dimension missing and outwards from
4060800	4065680	the state of up and down and from the space of things that you can touch at some point you
4065680	4069440	also realize that the things that you can see and the things that you can touch play out in the
4069440	4075280	same space at some point at some level of depth of modeling you can fuse the modalities and now
4075280	4080320	you realize that the world of touchable surfaces is the same world as the all surfaces that you can
4080320	4086320	see right and then you realize that you can move you can local mode in the world and this means
4086320	4091520	that you can see different things and the relationships between all these visible bubbles
4091760	4098480	of things that you can see is an allocentric space that is no longer ecocentric this bubble of
4098480	4103440	that you see in polar coordinates but something that is drawn in Euclidean coordinates between
4103440	4110400	which you can move and that is basically generating a new visible bubble a new visible dome
4110400	4115280	in every moment and you dome of things that you can touch in every moment and the objects in that
4115280	4119680	world are a necessary requirement that you segment the world into objects to make the world
4119680	4124640	describable so we separate the world instead of shooting this is one big system that is a state
4124640	4129280	vector that is changing we separated into many independent systems that each have their own
4129280	4133680	state vector and transition functions and they influence each other and the influence
4134560	4139520	relationships between different objects this is what we call causality so causality is an
4139520	4146400	artifact of the segmentation of the world into independent objects and the way in which we
4146480	4151360	address these objects these are concepts concepts are the address space of objects
4152240	4158000	and the decomposition of the world into interacting objects this is what we could call ontology
4158800	4164240	and epistemology is the field in philosophy that describes what we can understand
4165520	4170960	what we can know in the first place and I would say that the first law of epistemology is that the
4170960	4178080	confidence in a state of affairs should equal the evidence that supports that so everything
4178080	4184560	that is possible should be modeled and admitted as a possibility but the things that we believe in
4184560	4189360	that we make bets on are the things that we have evidence for and we should shift the confidence
4189360	4194720	flexibly around according to the evidence so when you don't know you cannot just pick a theory and
4194720	4200640	say this is the truth among the many possible ones you have to quantify your agnosticism as well
4201440	4209280	and ultimately we get to the entire space of possible languages that can describe the world
4209280	4214240	which I suspect are the computational languages and then the ways in which the world can be modeled
4215200	4220960	and this is determining the set of possible ontologies that could be superimposed on the world
4220960	4226400	and then we can compare all the ideas that intelligent systems are having about that and
4226400	4230800	right now all these intelligent systems are people that write books about this
4230800	4238080	and when we score the existing works of the existing philosophers and the existing cultures
4238080	4242400	we basically can get an idea of the space of possible things that can be the case
4242400	4247440	and it could be that humans because we have very small brains in a local optimum and sometimes
4247440	4252960	I'm joking that future AIs will love to get drunk so they can only model the world on like
4252960	4257600	12 layers and the physical universe looks as confusing as it looks to human physicists
4259440	4265280	right so there is a limit in what we can think about how many levels we can integrate when we
4265280	4270720	construct functions that model the world and that is limiting our understanding and so it seems that
4270720	4280000	physicists have been stuck after an enormous deluge of insights about 100 years ago physics
4280560	4286960	seemed to ground to a halt and of course it didn't help that modernism stopped somehow in the 1960s
4286960	4293840	70s and the sciences became more static than they were before and now more the organized
4293840	4298720	applications of methodology by people that are told by their guidance counselor that they shouldn't
4298720	4304880	go into the industry but in institutions of education and research and so we no longer have
4304880	4311440	that kind of progress it seems and maybe we need to build machines now to continue their progress in
4311440	4317120	the sciences and I don't know if the ontology is that the new systems will come up with are
4317120	4322400	intelligible to humans but maybe they are or maybe we can build machines that translate them for us
4322400	4330800	by chunking them in ways that are intelligible I see I see Leon has a question yeah thanks for
4330880	4338480	very inspiring conversation so could you comment on your views on consciousness I had the feeling
4338480	4346480	that you were conflating self-consciousness with consciousness and another point in this
4346480	4351200	direction would be how can we know when we build an algorithm that this actually conscious not
4351200	4357520	self-conscious not not a representation of its own but which which can perceive choir yeah so very
4357520	4365040	good point so first of all you're right but self-awareness and self-model is not the same
4365040	4370320	thing as consciousness for instance in dreams you can be conscious without having a self and
4370320	4375840	without being self-aware and the object of your consciousness does not need to involve
4376560	4382640	self of any kind but when we talk about consciousness there are three aspects that I
4382640	4390400	consider to be crucial and the first is the awareness of features awareness of content
4391280	4396720	and this awareness of content happens at the level between perception and reflection so you're not
4396720	4402800	directly aware of physical objects in the real world you are aware of certain abstractions that
4402800	4408960	your perceptual system is delivering and these perceptions are being stored in some kind of
4409040	4413360	index memory because otherwise you would not be able to retrieve the fact that you are aware of
4413360	4420080	them the purpose of that index memory is probably to facilitate convergence when you don't have a
4420080	4426800	gradient so your attentional system is able to backtrack and understand okay this figure
4426800	4431520	ground disambiguation didn't work let's try a different one and to do that it needs to store
4431520	4437200	a memory of the way in which it attended to the environment so the purpose of this attention
4437200	4442320	of the awareness of features is first of all probably a disambiguation of the world but there
4442320	4448560	are more there is attentional learning there is the avoidance of repetitive behavior and a few
4448560	4456480	other purposes that happen so the next thing in addition to the awareness of the content is
4456480	4462720	the awareness of the mode in which you attend is the stuff that you are attending to conditional
4462720	4466720	or not so for instance are you doing a figure ground disambiguation that you could be making
4466720	4470960	different or are you attending to something that cannot be changed so for instance are you
4470960	4476160	attending to something that is the output of your perceptual system or are you attending to a
4476160	4481040	fictional world that you are stabilizing using your conscious attention are you constructing
4481040	4485760	something in your mind are you achieving a memory are you creating a future world or a fictional
4485760	4492160	world in your mind right now and the third one in addition to this axis consciousness is going to
4492160	4496880	be reflective consciousness and I suspect that is a result of the fact that we are self-organizing
4496880	4502720	systems so the process that is attending needs to establish that is indeed the process that is
4502720	4508320	attending so there is going to be percepts that relate to the fact that the present process
4508320	4514640	is the one that is maintaining attention and this is enabling this reflexive consciousness
4514640	4521040	so in our own consciousness every few moments we flip back to checking whether we are still awake
4521040	4525200	whether we are the reflexive whether we are conscious whether we are the conscious
4525200	4530800	attending process in this perspective consciousness is a model of our own attention
4531520	4536800	it's a control model for our own attention and what's characteristic for this control
4536800	4543360	model of our own attention is that the fact that we are paying attention is driving part of our
4543360	4549760	behavior right so the awareness of the fact that something is attending is feeding back into the
4549760	4556880	behavior and this means that I think that if we ask ourselves is a cat conscious that comes down to
4556880	4562880	the question is the cat aware of the fact that it's attending and being aware here means is the
4562880	4568640	behavior of the cat in any way informed by a model of its own attention a reflexive model of its own
4568640	4575280	attention looking at my cat I have the impression that it's the case and my cat is conscious so
4575280	4580640	basically the question of when we have built a system we ask ourselves is the system conscious
4580640	4589120	would be is the system acting on a model of its own agency as an attentional agent so basically
4589120	4595680	is the system aware of the fact that it is attending functionally aware of the fact is it
4595680	4603680	acting on that model so we don't need to worry about machine consciousness until we have working
4603760	4609760	reinforcement learning agents in the world but we already have working reinforcement learning
4609760	4615280	agents the question is what needs to happen before they become conscious and it's a difficult
4615280	4621200	question I had a discussion with someone at open AI and asked them what do you think would need to
4621200	4626240	happen to make GPT3 conscious and he said maybe it already is conscious for a brief moment and it
4626240	4634720	makes the retreat how do you know but but but do you do you agree with this argument or you think
4634720	4642480	it's not valid no I think it's a valid argument so there is no I don't think that the difference in
4642480	4648880	our perspectives between Leon and mine it's just I was trying just trying to go into the details
4650240	4655120	right and I think his question was exactly on point it was that the things that we need to
4655200	4660560	answer if we want to get into the details and vice versa but but you you said when you observe
4660560	4668880	your cat you can confirm that it's all sort of self attending so but the cat doesn't speak in like
4668880	4678400	human language yes if we make this same analogous to GPT3 by observing behavior of GPT3 can we
4679200	4684480	say that it's attending or not because so in GPT3 it's actually easy because we can analyze
4684480	4689440	GPT3 functionally we can look at the flow of information in GPT3 and there is for instance
4689440	4696160	work that analyzes how GPT3 does numerical operations to which degree is GPT3 actually able
4696160	4701520	to implement algebraic operations for instance we can really look into the networks and take
4701520	4705920	this apart and analyze it which is much harder with the cat because the operation would be
4705920	4711200	destructive and the brain of the cat is not built in such a way that it's easily
4712160	4719040	reverse engineerable and also the issue is that the representations in in our own brains
4719040	4724960	are not straightforward circuits the representations are activation patterns traveling through the
4724960	4730720	circuitry right so the circuitry is more acting like an ether that is propagating waves of activation
4730720	4735680	that are being changed in the execution state of our mind is encoded in the activation wave
4736560	4743120	in a non-straightforward fashion and it makes it very hard to analyze the brain state except on
4743120	4747920	with some kind of measure learning tool that gives you some compound understanding the way in
4747920	4753200	which I establish whether my own cat is conscious is the establishing a feedback loop with my cat
4754160	4759360	and this feedback loop means that my cat and me are looking at each other and we both try to figure
4759360	4765040	out what the other one is understanding in that feedback loop so it's a non-verbal behavior
4765120	4770960	in which we are basically to some degree sharing mental states and this is something
4770960	4777600	that you obviously do with human beings all the time when you have empathy the difficulty in AI
4777600	4783120	research or a lot of academic research is that almost all of the people that are good at anything
4783120	4788160	are autistic because they need to have extremely focused single-minded attention that is not
4788160	4794880	disturbed by the social and economic incentives so they can actually make progress on any
4794880	4800480	difficult technical topic and when you're autistic the problem is that you usually don't have a lot
4800480	4805680	of intuitive empathy you might have a lot of compassion but you have difficulty synchronizing
4805680	4809760	the with the brain states of other people at that level where you establish a feedback loop
4809760	4814720	between them and it's something that I only learned relatively late in my life to pay attention to
4814720	4822960	that and be aware of it yeah but my question is like if we observe how GPT-3 behaves when we can't
4822960	4830800	look at GPT-3 eyes right and like establish this feedback loop my point is it's probably hard for us
4830800	4836560	to to say if it's conscious or not it's much easier for us to say if like cat is conscious
4836560	4840800	because we can't establish this feedback loop but we can establish this the same feedback loop with
4841920	4848240	AI model we need to define what consciousness is in the first place right and the thing is when we
4848240	4853120	are talking about our own consciousness we have an indexical understanding we just point in a
4853120	4857280	certain direction there's a system in the direction which we are pointing and we all
4858000	4863200	more or less agree on what the thing is that we are pointing at and the similar situation existed
4863200	4868880	when people try to understand life and biology around the time when biology started where people
4868880	4873040	pointed at things and said these things are alive but we don't know what distinguishes them
4873600	4878480	but from the things that are not alive there is clearly a very important distinction there
4879040	4884720	and then people came up with all homeostatic dynamics and so on and I would say right now
4884720	4892080	it's basically there are cells living cells that are not decaying that are able to maintain
4892080	4896480	their integrity and their state and so on as long as these cells exist in an organized fashion
4896480	4901680	it would say that the system is alive and this was something that the biologists had to discover
4901680	4907360	in the course of developing their field and in a similar way in the course of the development
4907360	4912240	of cognitive science we have to establish what we mean by a system being a mind and being intelligent
4912240	4917360	and being conscious and at the moment my best understanding of consciousness is that there is
4917360	4923760	this attentional system that is integrating the world model and this attentional system needs to
4923760	4928240	have certain properties like it needs to have an index memory of the states that it tended to
4928240	4933360	it needs to be capable of making sense of its own agency and so on and so on and these are
4933360	4939120	functional criteria and in some sense the neural network is just software right it's a software
4939120	4944720	that might not be easily intelligible to human beings but we can translate it into something
4944720	4950240	that is computational equivalent and isn't intelligible and because we can make these
4950240	4956640	systems expandable we can as soon as we identify the formal definition of what we mean by consciousness
4956640	4961120	look whether the system is conforming to that formal definition that is if it's implementing a
4961120	4967120	certain list of functionality that we require and we can also test this basically we can implement
4967120	4972240	a minimal system that is implementing all these functions and then see whether it's also a system
4972240	4978160	that you would point at when we say that it's conscious so for instance if that system is
4978160	4983600	able to use language and is entangled with the environment and can be able to learn a language
4983600	4989040	that enables it to speak about this environment is it able to talk about itself as a conscious being
4989040	4994320	is it going to talk about its own phenomenal experience when we ask it without lying without
4994320	4999600	having an additional mechanism that tricks us built into it and so the hypothesis would be
4999600	5005680	if we build a system that is has such an attention agent that it is acting on top of a perceptual
5005680	5010960	agent and makes sense of it and reflects on it in a similar way as Vito on this system is able
5011040	5016720	to learn a natural language that we would be able to communicate about its phenomenal experience
5017440	5026080	visit. Daniel, did you have a question? Is it your wrist? Yeah, yeah, yeah, I also have a question.
5026080	5032480	So, Josia, first of all, it was fascinating to have this conversation today and see this
5032480	5039280	big idea with motivation and attention working with a perceptual agent. I couldn't stop thinking
5039280	5045120	about the conversational AI and stuff we discussed like a year ago and what kind of things we also
5045120	5050880	worked the deep power to build a social board for Alexa Pride 3 and Alexa Pride 4 now. I was wondering
5051600	5058960	if you tried to, it doesn't matter where, but have you tried to bring these ideas of attention,
5058960	5066400	maybe some feelings like fear and other things to the conversational AI experiences?
5066960	5072880	They don't necessarily have to have eyes and build eye contact with a person,
5072880	5077840	but as long as they have a conversation, have you tried to go that deep into the projects you
5077840	5083200	worked on over your career to build this kind of things? Because things like what we work on at
5083200	5088720	Depall of right now is things like we're trying to build motivation for the bot. So, we're trying
5088720	5094960	to build a goal of war bot where it is a war of the goals that the user has and also the war
5094960	5101840	of its own goals. When bot has a conversation with the user, we try to build a three-level
5102720	5108080	dialogue planning where each time bot wants to say something, it looks at the goals of itself,
5108080	5113520	the goals of the user, then it makes a decision which goes to follow based on that it defines
5113520	5118240	discourse where it wants to go and then based on the discourse, we try to pick the exact next step
5118240	5126800	in the conversation that they want to do. When I look at your picture, it strikes me that our
5126800	5131760	goal of war dialogue management in many ways is the same idea that they have that motivation
5131760	5138080	built into the system that we were lacking in our previous international or social bot.
5138080	5143360	So, obviously, you worked way, way more in the conversation where I was just wondering how far
5143440	5152320	did you vent in the direction and what you could recommend for aspiring minds in the direction?
5153520	5160640	My own impression when we did this at AI Foundation, we failed and we failed for a number
5160640	5165760	of reasons. One of them was, of course, we had a relatively small team working in a startup
5165760	5171120	and the majority of what we had to do in a startup was related to getting a product on the road.
5171680	5176960	But the main issue is that the stack of solutions in the present machine learning
5176960	5184240	environments is not suitable for real-time entanglement with the environment. And to build
5184240	5189840	a system that is able to make sense of the universe autonomously, you need to have this
5189840	5196000	real-time coupling, I think. So you basically want to have a system that is always able to make
5196000	5200960	sense of the next frame. Imagine you want to build a machine learning system that you just
5200960	5207120	connect to a bunch of cameras and after a certain time it's able to understand that the changing
5207120	5212960	lighting conditions are due to the sun and the sun is some kind of circular objects that move
5212960	5218880	through the sky and it is at a certain almost infinite distance and so on and so on. So,
5218880	5224960	relatively simple things that every mammal is able to figure out, but that none of the
5224960	5228400	machine learning systems at the moment seems to be able to figure out autonomously.
5229360	5234880	And it's not because it's so hard to do this, but because all our efforts are going into
5235680	5242800	doing batch processing on image databases, rather than interacting with the real world.
5242800	5246800	Of course, also the interaction with the real world and online learning requires that we are
5246800	5252320	much, much more efficient at extracting structure from the data. And there's still this difficulty
5252320	5256720	that our own bootstrapping as an organism takes many months before we're able to make sense of
5256720	5262720	visual data. And we don't want to wait months before our machine learning system converges to
5262720	5270320	basic image understanding. So, maybe we need to have a combination of online and offline learning,
5270320	5276000	at least in the beginning. And there are many technical things that have to be solved for this.
5276640	5282000	For social motivation, we probably need to look at the things that enable us to
5282960	5289280	develop this kind of motivation. I started out with a theory that assumes that we have a few
5289280	5295200	hundred physiological needs that we need to regulate for, but that's boring. And we have
5295200	5301120	something like a handful or two handful of social needs that structure our social interactions.
5301120	5305840	They're basically priors in the way in which we want to interact with others. For instance,
5305920	5310720	people might have innate need for status for raising, rising in a social hierarchy.
5311600	5318400	And this is just a bias in the system, a reflex. And you can eventually replace this reflex by
5318400	5323760	something that it's instrumental to. So, maybe you want to organize the world in the best possible
5323760	5329360	sustainable way. And now you do not want to have power for its own sake. You want to have
5329360	5336800	power according to your abilities and incentives to get things right. And so, your desire for status
5336800	5342560	gets completely supplemented by conditional behavior. And then you also need to have a bunch
5342560	5348800	of social needs, of cognitive needs that regulate exploration versus exploitation. So, a desire for
5348800	5355280	competence versus uncertainty reduction. And they will structure the way in which you interact
5355280	5358960	with the environment. And as soon as you will understand the environment better, these innate
5358960	5362160	needs and their weights get replaced by something that is conditional again.
5365920	5370000	If you want to build an agent that is interacting with people in an interesting way,
5370000	5374240	I suspect it's also necessary that this agent basically has something like an attentional
5374240	5379040	idol loop where it's looking into the world and thinks about, oh, this is what I'm looking at.
5379040	5384000	Oh, there is somebody coming. Do I know this person? Should I interact with this person? In which way
5384000	5389520	should I interact with this person? And it should be giving science of all these processes taking
5389520	5395920	place. So, you need to understand by observing the system which state that system is in. And we
5395920	5400880	also need to understand that emotions have not evolved as a display of the internal state that
5400880	5409200	you are in, but mostly in an adversarial condition. But as soon as we became social agents and were
5409200	5415760	observing each other, we were using this observation to game others, to understand how they are,
5415760	5420000	to exploit them, to control them. And this means that even small children learn to
5420640	5426480	hide their emotional state or to coat their emotional state. And so, when you see somebody
5426480	5431040	at the funeral, you are not just looking at whether they are looking somber because everybody is
5431040	5437200	looking somber at the funeral. But you are looking at what kind of somberness they are
5437200	5442960	displaying and how the somberness is being achieved and what this actually indicates.
5442960	5448880	So, you are looking at at least two levels of structure and the social persona becomes a puppet
5448880	5456800	that everybody is controlling according to their internal puppet. And the social puppet can become
5456800	5461920	so dominant that people forget that they have an authentic structure behind it. So, people have
5461920	5466240	something like a core self that is like childlike self and all this machinery that they built on
5466240	5472080	top of it and they can get lost in the machinery. And it can be very difficult to reawaken the core
5472080	5477760	self into something that is actually interacting with the environment is getting real. And these
5477760	5481840	interactions are what makes it very, very interesting to interact with people to which degree
5481840	5488160	can you go beyond the social puppet and interact with the core self and establish intimacy.
5490480	5494960	I loved it. And I mean, obviously, we have multiple puppets that we have with different
5494960	5500560	people. And for social boards that we had in the last cohort of Alexa Price 4,
5502080	5510160	they at best had, I don't know, maybe one very fractured, very small kind of puppet that was
5510160	5515600	really, really limited in what it could achieve. So, for instance, I think that gender is a good
5515600	5521120	example. Gender is in some sense such a puppet. It's a costume that we wear. And it's socially
5521120	5525200	constructed in the sense that gender is a model of what other people think of who we are.
5526320	5532320	And if you confuse this puppet with your core self, you go into all sorts of contradictions
5532320	5536160	because you can no longer get your models to converge because you think that your puppet is
5536160	5540000	immutable. So, gender becomes a costume that you are unable to take off again.
5540560	5548240	Yosha, I have a question. I want to sort of go back to that first cell that sort of gave life
5548240	5555760	to everything. If we look at how cells behave, they behave very intelligently. It seems like
5555760	5562400	they already have this building intelligence and they know to what kind of tissue they have to
5562400	5568720	split and become. Do you agree with the statement that sort of like intelligence already
5569680	5575760	within the cell and this sort of like expanse? It's not like intelligence only in our brain,
5575760	5583360	but intelligence in our body, in like on the very core of our existence.
5584320	5590960	When I was a kid, my grandparents bought a chess computer for me. And I was bored by this chess
5590960	5596480	computer because I didn't perceive it as intelligent. It was able to play chess much better than me
5596480	5605040	unless I would turn down the difficulty level. But it was a toaster. It was not able to do anything
5605040	5610560	but to play chess, which means that according to a simple function, it was calculating the move
5610560	5615840	that most probably would let it win the game and it was able to do this better than me. But it was
5615840	5621040	not able to make any kind of new model of the environment. And to me, intelligence means that
5621040	5625440	you are able to model something new, that you are able to solve problems that you couldn't solve
5625440	5633600	before, that you're able to generalize and so on. And this modeling capability is also something
5633600	5639920	that probably eludes the individual cell. The individual cell has the an operating system
5639920	5647040	genome that is encoded in the DNA of the cell that allows it to decide, giving the environmental
5647040	5654320	conditions what to do best. And it's either going to differentiate into a different type of cell,
5654320	5658960	which means it's going to turn this operating system into a different configuration, or it's
5658960	5663840	going to divide into two cells, or it's going to regulate something, or it's going to die.
5664640	5670640	Apoptosis, the voluntary death of the cell. And these are the mechanisms that are available to
5670640	5676560	the cell. This is all it ever does. And in order to make that decision, the cell is able to integrate
5676560	5683520	over its inputs. And it can integrate by looking at the configuration when a neuron can look at a
5683520	5688720	configuration of electrical impulse that come in. And this integration happens in time and in space.
5689600	5694800	But there are very tight limits on how much temporal memory the cell has, how long it
5694800	5700800	remembers that it has seen a certain thing in its environment, and how well it can predict in the
5700800	5707520	future what it should be doing. And so the functions that the cell can learn are very limited. And we
5707520	5713760	can probably quantify this, what it can do. And I don't know the exact quantifications,
5713760	5717440	and no number of people have given me their estimates, and they're vitally diverged.
5718720	5724480	But the cells don't seem to be universally intelligent. There seem to be, of course,
5724480	5729200	they can learn. So they are intelligent to some degree, they can make models to some degree.
5729200	5733120	But the individual cell seems to be extremely limited in what it can model.
5734080	5740160	But we can say the same about people. I mean, if we look from perspective of universe,
5740160	5746320	of multiverse, and what is out there, there are so many things we don't know. And our memory is
5746320	5752880	also short. So from someone's perspective, we can be also very limited intelligence, right?
5752880	5761520	It's not like this super generalizable intelligence that knows everything about all
5762320	5769120	Yes. So maybe we can come up with the laundry list or with some kind of achievement hierarchy
5769120	5777120	of minds. And so are you able to learn at all? When you learn, can you integrate over time and
5777120	5781920	space, the features that you're learning over, to which degree can you do this? What is the
5781920	5788720	complexity of the function that you can learn? And so the next interesting step is the function
5788720	5793280	that you can learn to learn complete. Of course, you yourself need to be doing complete to learn
5793280	5798160	interesting functions because you want to associate arbitrary states with arbitrary transitions
5798160	5803840	between them. But when you make your model, can your model contain a Turing machine?
5804720	5808320	For instance, when you learn that something in the world is an agent, you need to be able,
5808320	5813040	if you want to model that agency, construct an agent in your own mind, which means you need
5813040	5817600	to construct something in your own mind that is Turing complete. I'm not sure that cells are able
5817600	5822240	to understand the agency of parts on the environment instead of just learning associations.
5823120	5830080	And so the switch from correlational models to causal models and from causal models to
5830080	5834480	complete state machines that are Turing complete is an important step in the maturation of
5834480	5841280	intelligent systems. The next step, or maybe not the next, but one that is definitely important is
5841280	5845760	do you understand minds? Can you build things that are intelligent? And this is something
5845760	5852080	that humans are about to do, I think. So there is a probability that we are generally intelligent
5852080	5857280	in the sense that we are able to understand intelligence in general by building and demonstrating
5857280	5863520	that understanding. And I don't know if there is another stage after that that is interesting,
5863520	5871440	maybe understanding existence. So you don't know if there's next level up to this sort of general
5871440	5876640	intelligence, right? Yes, so I wonder if there is anything left after that that is interesting,
5876640	5882800	that is not just scaling up. So basically you believe that humans are capable to build
5885120	5889760	generalizable intelligence? Oh, we are currently testing that hypothesis, right? We don't know yet,
5889760	5893120	but that's basically the idea of Turing, that humans are generally intelligent,
5893120	5895120	you should be able to build systems that can think.
5895920	5905920	Thanks, thanks a lot. I think I feel like we should repeat it because I have a long list,
5905920	5913120	but I feel like we're already almost two hours. Yeah, we would really love to have you again,
5913120	5917680	Ayosha. Thanks, thanks a lot for your time. Thank you, it was a lot of fun. Have a wonderful
5917680	5928240	rest of the day wherever you are in the world. Yeah, thanks a lot, Ayosha.
