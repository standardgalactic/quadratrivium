Okay, awesome. So I think then we can kick off our talk today. I'm like really excited
to have you because, you know, every month we have our happy hours and one of the members
who always come to those meetups and we have like really great and fun conversations. And
I asked like who you would love to come to give a talk in our meetups and one of our
community members mentioned about you and they're like really love what you're doing
and like super excited about your work and that's why I really wanted you to come and
share your thoughts with our community. For those who are first time in our meetups,
welcome to our community. I will share a link to our Slack where we hang out and in general my
goal is to facilitate supportive, you know, supportive environment and help each other.
Let's say if you are working on some technical projects and you're stuck and you need some help,
you always can, you know, share your technical requests there. And in general, like this is
basically like, you know, community of like-minded people who do machine learning and data science
and in general think about what is cognition, what is intelligence and today, Yosha our guest
will talk about this and just a couple of words about our speaker, Yosha Bach, who is the principal
scientist at Intel and also his research involves like computational models of cognition and
neurosymbolic AI and maybe Yosha, you can dig more deeply in your research.
Thank you, Sofia. Yes, I'm basically a cognitive scientist. Most of my work in the past has been
in the field of cognitive architectures where I try to understand the relationship between
perception, motivation and cognition and at the moment I am working at interlapse in a group
that is trying to figure out what comes in the future of AI and how to evaluate future AI systems
and understand qualitatively and quantitatively how we can understand and assess the dimensions
in which they exhibit performance. Today is going to be slightly more philosophical talk
and AI is, as many of us are aware of, not just a field that is automating statistics,
but originally it has also been a philosophical project and as philosophical projects go, it's
a relatively large one. It involves, I would say, a few thousand people and it's a very
risky project and this project is also the most interesting and I think most relevant one
that exists in the history of philosophy and of course it's only a very, very small fraction
of what our field is doing. Most of our field is working in engineering and a lot of confusion
emerges because people are confusing AI as a philosophical project with AI as the engineering
project and today I'm going to talk a little bit about the philosophical project. Philosophy
itself can be understood as the realm of all theories where we explore everything that can
possibly true and philosophy is largely done in natural language and natural language is
not very well defined so it's very hard to say something in natural language about the complicated
questions of philosophy that is true because natural languages are so ambiguous and heterogeneous
and have to deal with a world that is not very well described by natural language ultimately
and it's tempting to start out from mathematics to do philosophy and mathematics can be understood
as the realm of all the languages not just the natural languages and here what we are most interested
in are the formal ones, the most simple ones and in these formal languages we can say things that
are true because we define truths formally in mathematics so we get to a very narrow clear
understanding what it means for a statement to be true but the problem with mathematics is that
it's very hard in mathematics because it's so simple to say something interesting about the
world that you're in and it's very hard to say a mathematical sentence about our experience of
the world or about what we care about and so on and about the question of what is meaning,
how are we embedded in the universe, what is agency and so on and formalizing these terms
is something that is so difficult that mathematicians for the most part haven't even started going there
so the question is how can these two realms meet and it probably means that we have to automate
the thing in between mathematics and philosophy which is the mind right if we can
methodize the mind that we can methodize the systems that are able to form theories
and we have to do this by making them autonomous and self-organizing to in some
sense replicate the structures that our own mind is producing so we can say things in mathematics
that are true by building a machine that is making these proofs and explores them and so on
that is basically taking over from us we will have a way to prove our theories about the world
and a big step in the last century was that we discovered that mathematics is some kind of
code base that has been developed over a few thousand years and some of the assumptions
in that code base had been wrong especially the way in which tools was defined and that is
a big implication of the work of Gödel and Turing what they discovered is that we
cannot build a machine in the language of mathematics that in any kind of mathematics
that runs the semantics of the existing mathematics without breaking it's basically what
Gödel and Turing discovered is when you assume that the existing semantics of mathematics are
true you run into contradictions it was a very big shock especially to Gödel who strongly believed
that truth as it was commonly understood was actually the true truth and what Turing discovered
was we can actually build a different machine a computational machine and this computational
machine is able to recover all the semantics that mathematicians were using in practice what
you're losing is infinities continuity and a few other nice things but there is no infinity
in continuity that mathematicians are ever working with continuity is just too many parts to count
in the physical universe right so when you have too many parts to count you have to find operators
that are converging and the operators that are converging in the limit over too many parts to
count this is geometry so now we have a new perspective a computational perspective on
everything and this computational perspective already existed with the mathematics it was called
constructive mathematics is the part that actually works
so we could say that AI is a philosophical project is as its goal to unify mathematics
and philosophy using computational models of the mind you could also look at it in a different way
it is basically exploring the question of what intelligence is what is it that the mind is doing
when it's making models and if you're able to succeed in this task of building a system that
is able to make models in the same way as we do it and what we are building is a system that is able
to understand how minds work so in some sense the goal of the Turing test should be to build a system
that you can ask how it works right if it's able to explain to you how it a mind works and how you
work you have succeeded and in some sense this Turing test yet performing on such a system is
built a system that is also able to perform Turing tests on you and on itself and that system is
able to explain to you what it is and the question that Turing is also trying to answer in this way
is is Turing generally intelligent is Turing able to explain how one mind works by building one
AI is a philosophical project in a formal sense started quite early maybe with Leibniz who had
this idea that we can build a calculus that is describing everything in the world and we can
just calculate the answer and my own philosophy teachers have been very dismissive of this project
how could you turn everything into numbers and then do calculations what a naive idea and I
don't think it was that naive at all it was actually this idea of building a machine that
can perform these calculations and this universal calculus is something that has kept a lot of
people busy like Frager with the Begriffs Calcule and so on and in the last century the most
prominent start into this was arguably Ludwig Wittgenstein who had this idea of formalizing
the language this that philosophers are using by basically making English much more formal and
strict it's a very beautiful idea and this idea of turning English into a programming language for
thought is expressed in the Tractatus Logico philosophical study wrote as a young man coming
back from World War one it's a very beautiful book because it is not tainted by footnotes
and references and arguments it's just a very clear elaboration of this particular sort but
you can understand it best if you had this thought by yourself it's not trying to convince anybody
he also makes that point and explains that it's basically not trying to make an argument that
is convincing other philosophers it is an idea to write down a particular thought that you will
understand after you had it and so it's a book that can be understood I think quite well by somebody
who is a programmer who has thought about writing a language for thought but it's a book that
endlessly confused philosophers in the last century because they didn't know what you really meant
and in this remarkable book he preempts Minsky's logistic project the idea of writing a language
in which you can do AI by several decades and also its failure because at the end of his life he
concludes that it didn't really work because he was not able to integrate perception or as he
mentioned pictures or images into his formal language and this was something that is only
now happening this deep learning where we are developing automatic functional approximators
that can deal with perceptual content and integrate this into the formal models that we
are building so Wittgenstein couldn't see this yet but Turing was one of his pupils and you know
how that went. Classical AI is mostly symbolic and classical AI was the stuff that is very
simplistic and clear not as clear kind of teaching or practice but the idea was that you analyze a
problem you find an algorithm to solve it and use this example as chess the all the early
chess programs were written in a way where the developers thought about how can you friend an
automatic strategy to play chess and the nets implement the strategy and make it fast and efficient
enough optimize it enough so it can beat human players and currently we are in a different era
of AI that's this deep learning era it's mostly sub-symbolic where we don't write the solution
to the problem but we write an algorithm that learns the solution to the problem that discovers
the solution to the problem by itself and it's tempting to think that the next era of AI will
be about meta learning so we don't write algorithms that discover the solution to a problem but
algorithms that discover how to discover the solution to the problem that learn how to learn
at the moment we are using mostly neural networks and the neural network is a chain of weighted
sums of real numbers and debates in this network are changed with various algorithms mostly
stochastic gradient descent and there are many alternatives to neural networks so it's not the
only way the entire goal is to make compositional function approximation and the thing that works
best in practice for many of the tasks that we have picked is stochastic gradient descent which
means in some sense differentiable programming and the main paradigm for that is still Frank
Rosenblatt's perceptron from 1958 and he was not the only one who discovered it and back then in
his work he didn't discover back propagation yet even though he could already see what would need
to happen to make it to to pull it off. Minsky and Pappert delivered a formal proof that the
perceptron learning algorithm that existed in Frank Rosenblatt's perceptron was not able to
learn XOR and by extension many many other functions and so this book by Minsky and Pappert
which they wrote to point out that we would need more complicated system Minsky preferred
to make systems that are more proposition based language based knowledge based
stopped research on neural networks and funding especially on your not the research but the
large-scale funding for more than a decade and delay the development of connection systems
but in some sense this has never stopped at the moment it's the most successful paradigm
in AI arguably and what we find is that there are no longer black boxes if you look at this
two part the excellent work by Chris Oler's team at Open AI there it's possible to analyze what
these networks are doing. His hypothesis is that features are the fundamental unit of neural
networks and they correspond to directions in an embedding space and the features are connected
by weights and form circuits and that there is a universitality condition which means that
analogous features and circuits will form in different models if you give them similar tasks
and the model is flexible enough but these artificial neurons are very very different
from biological neurons which are self-organizing. The biological neurons are basically all little
animals each neuron is a little animal and this little animal tries to survive and to survive
it needs to get fed and it will get fed if it does the right thing from the perspective
of the surrounding organism and the right thing is that it needs to fire at the right moment so
every neuron has to learn when to fire based on its current environment and the current environment
is from the perspective of the neuron a certain electrical and chemical configuration of the
environment so it has to figure out which signals an environment signal that they should
fire and the neuron will put tendrils into the world to make that measurement to figure out
when it should fire and there are different types of neurons that have different biases in their
strategy to explore the space of possible solutions to the questions of when it should fire
and they are a form and organization they have a shared destiny they all lock together on the
same dark skull and the only way that they can survive is that they collaborate in a way and
together give each other feedback on when they should fire right and we know what the overall
result is you get this emergent structure in it that from the outside or from a certain distance
looks like there is a common spirit that is evolving in the coherent patterns of the firing
of the neurons this virtual thing that we call a mind and for me it was a very big insight to
realize that the word spirit is not the superstition but that spirit is basically an operating system
for an autonomous robot and when the word spirit was invented the only autonomous robots were known
were not built by people of course there were people themselves and other organisms like plants
and animals and there were ecosystems and cities and nation states all these systems basically
have emergent virtual software that you can project into the coherent functioning of their
elements to make them explainable so in some sense you get a system from the organized coherent
activity of its parts that behaves as if it was following a shared telos a shared purpose
a shared structure is their shared computational strategy and could be described with a single
software but what is it aiming at and Carl Friston says what it's aiming at is the minimization of
free energy and what is meant here is not some thermodynamic energy but you can use an energy
description to describe the state of the system that is modeling its environment and that energy
is minimized then the system needs less energy to update in the next state and so the idea here
is that the system whether it's a brain or a cell is modeling its entanglement with the
environment in its own internal state and it's trying to minimize the prediction error and
now to understand how to do this in a self-organizing system technological system like
neural networks use a functional design you think about what does the individual representation
unit need to do and then we impose that function on it it just does it because we build it on a
completely deterministic substrate our computers are designed to be more or less completely
deterministic and follow the rules that we give them and don't deviate from this ever and biological
systems or social systems are not like this here we need a meta design we need to design them in
such a way that they want to converge to the desired functionality right so instead of building a
FDA that is following a set of rules and then it's gatekeeping the access of to medications
in such a way that people don't poison themselves and so on if you just implement these rules it's
not going to work because after a couple of generations the FDA is going to be captured by
the producers of medication and is acting as a gatekeeper against innovation that would prevent
new better cheaper medication to enter the system and eventually it's going to also limit the access
of people to completely save medication that doesn't have patents on it so nobody makes a profit on
it right so if you want to prevent the capturing of regulators of other social systems you will need
to design them in such a way that they want to do the right thing and if you cannot do that you
need to make the model you need to make sure that they die every generation and get replaced by a
fresh system maybe we should think about how to make model institutions that die after a certain
time so they can rejuvenate and rebuild anyway biological neurons are not just functional
approximators they are agents how can we understand what an agent is and here what helps us is to
use cybernetics and cybernetics we can start to define what it means to be motivated and
the core of cybernetics is as you all know the feedback loop and a feedback loop if we want to
turn this into an agent we take a controller the controller is connected to a system that it regulates
via a factors something that can change the state of the regulated system and sensors and the sensors
measure a set point deviation a deviation from how the regulated system should be
and the controller is now implementing a model that tells it how to go from the measured set
point deviation to a change in the effectors that affect the regulated system and the regulated
system goes out of whack again and again because it's being disturbed by the environment and the
better the controller is able to model these disturbances the better it's going to be able to
deal with the set point deviations over a long time span in the simple case you have a thermostat
that is only optimizing for the next moment for the next frame of the system but if you have a
system like us you're able to optimize for an integral over the set point deviations over a
long time span and to do this we need to model the future and if the controller is able to model
the future and the structure of the environment then it's going to distinguish between situations
that it prefers over other situations and it's going to implement strategies and how to get to
these situations at once and when we describe the system from the outside we will attribute
agency to it because this is what an agent is it's a controller combined with a set point
generator that is able to act on something that it regulates by modeling the future of that
regulated system and acting on the model of that future and the good regulators theorem states
that every good regulator of a system must be a model of that system or must implement a model
of that system which also means that you cannot model the world efficiently if you don't make
a truthful model of it right so when you for instance try to build a more just world it doesn't
help if you use categories that don't model the world as it is but as you want it to be
that is not going to lead to the best possible regulation if you lie to yourself about the
true state of the world your regulation is going to be off so if you want to model the world if
you want to improve the world if you want to control something regulate in the right direction
you need to start with a complete service to truth and you need to come to the description
of the system that you want to regulate that is isomorphic to the dynamics of that system
at the level that you want to regulate it and the universe that we find ourselves to be in
can be understood as a universe that can be controlled and I think that's the answer to
this big conundrum why is it possible that we can learn anything at all why is it possible
that we can recognize structure in the universe why are we in a universe that is intelligible
it's because we are controllers we are on top of a hierarchy of control systems so in a way
you could say that elementary particles are controlled zero point fluctuations
and the atoms are controlled elementary particles and molecules are controlled particles and cells
are controlled molecules and organisms are controlled cells and societies are controlled
organisms and so on right so we have a hierarchy of control and because this to control you need
to make a model of the underlying system it means that a controllable universe is also one
that can be modeled and the models that an atom needs to make of the elementary particles
of course extremely simple because the it's an extremely simple mechanical regulation
but the models that cell needs to make to control the molecules that make up the cell
are very very complicated and there is something like a shift in the complexity in that shift
in the complexity is means that the cell in order to enact this regulation of the molecules
needs to implement a Turing machine it needs to be a computer if the cell cannot compute
it's not complicated enough to learn how to model the future of these molecules well enough
to build this giant super molecule the cell this all its dynamics it is able to withstand many many
types of disturbances in the universe so the purpose of all this regulation is to maintain
complexity to build systems that are stable against disturbances a self propagating and you
disturb them over a larger range of environments we can ask ourselves is the universe is a
computer and isn't it a dynamical system and the answer to this is that well there is no true
continuity for mathematical reasons because if you want to talk about infinity in any kind of
language you will run at some point into contradictions if you try to explain how that works
which means that your word don't mean anything anymore right if you if the language in which
you try to talk about the world does falls apart it means that the words lose their meaning you
cannot actually talk about infinity without presupposing that you already know what you're
talking about and so we can replace that notion of infinity by too many parts to count and then
we get all the same things that we wanted without the contradictions and the world that we are in
is one that is made of too many parts to count so the dynamical systems if used to describe the world
are the same as before but they turn out to be computational systems vice versa the computers
that we built to describe the universe are built on top of the dynamical systems and we basically
stack the probabilities of these systems until they become deterministic enough for our purposes
but the another question you might ask is are quantum systems computational systems
or are they hypercomputers aren't quantum systems able to compute things that classic computers cannot
and well if you look at what a computer is if you look at the church during these is a computer
is in some sense a system that is able to go from state to state in an unrandom fashion
it doesn't get much more general than that and the quantum system is also nothing but that right
the quantum system is characterized by a state that is a superposition of possible states from
a certain angle it's still a state and then you have a transition between them to the next state
and so a quantum computer is also just a computer but the reason why quantum computers can do things
that particle computers cannot is that according to quantum mechanics the particle universe is
very inefficiently implemented on top of the quantum substrate right if you are implementing a
computer in minecraft from redstone you can do that there is going to be a polynomial time
relationship between the speed of the computer in that you implement vision minecraft and the cpu
that minecraft runs on of course the computer that runs is inside of minecraft is going to be
much much slower than the computer that minecraft runs on because most of the computations of the
computer that minecraft runs on will not go into the computations of the computer that you
build vision minecraft right so you're simulated in game world computer is going to use only a
fraction of the computational resources but according to quantum mechanics the particle
universe is so inefficiently implemented on top of the quantum universe that the quantum universe
is branching off in many many ways and most of the computations of the quantum universe are not
contributing to our timeline and so it's only a very small fraction that drips into the available
timeline and quantum computers are basically the bold hypothesis that we can tap into our
quantum cpu that and the quantum substrate that the particle universe runs on and use some of the
additional computations that are not available to the particle universe to drive the computations
that we want so quantum mechanics is also not a hyper computational notion hyper computation is
it's another one that we could take it all a causal hyper computation imagine we could build a
closed time lag loop that means for instance we can somehow look into the future and get the lottery
numbers of next week and use them now to win the lottery right this would be something that is not
possible in computer right well of course it is possible you just back up the state of the
present universe right you make a copy of the universe is right now buffer it then you run the
universe to next week take the lottery numbers store them reboot the universe from the state that
you have in the buffer and then pass the numbers in right so as long as you're able to memorize the
state of the present universe you're good you can also build close time lag loops in the classical
computer so there is in some sense no way to get out of this and we can ask ourselves is there
something about consciousness that requires us to move away and I don't think there is something
about consciousness that is very special the issue with consciousness is very confusing because we
think that our consciousness gives us access to the physical universe and what our consciousness
is perceiving is the real world but it's not consciousness is a dream state it's the state
inside of a model it's something like a multimedia story that is being generated inside of the agent
physical systems cannot be conscious neurons cannot be conscious computers cannot be conscious
consciousness is an entirely virtual property consciousness is as if and because we live in
this as if world we can perceive things as real because being in a physical world being in a
computer doesn't feel like anything right some people think that computers cannot be conscious
that simulations cannot be conscious but they have to have it backwards you can only be conscious
in a simulation because it's a simulated property not a real one it cannot be real
and then there's this big question of existence how is it possible that something exists at all
and to this one it looks very unsatisfying from the perspective of a computational list
and the easiest answer that I found so far is that maybe existence is the default right so
rather than assuming that you need to add something to the universe to call something into
existence the universe is already the superposition of all the things that could exist and if for
something to exist it must be implementable I think a good definition for existence is
implementation something exists if it's implemented and everything that's implementable are finite
automata so maybe the universe is a superposition of all finite automata and the structure of the
universe is the result of those things that don't exist in the superposition of all finite
automata basically the wakes of certain operators that create gaps in existence
so maybe the universe after all is something like an inverse computer it's a superposition
of all the operators and if you superimpose all the operators there's still going to be some states
that are not attainable and this gives the structure to the universe but I don't know whether
that's true that's extremely speculative I don't have an answer to the conundrum by something
exists at all which is the one that really shocks me that is satisfying to me
anyway let's go back to agency we pointed out that to generalize control we take this control
model the controller minimizes z point deviation the minimization of the future z point deviation
requires the controller to make a model and the model has to predict the result of the
interaction has to make something like causal model and an agent is a controller combined
with a z point generator and the controller needs to be able to model the future and has
preferred states and I would say that a sentient agent is one that is discovering itself in this
interaction that's the world right so once you discover your own agency and that you discover
that there is a system that is changing the world in a particular way and that system is using the
contents of your own control model you discover your own first person perspective
so how do we make a model
the general form of a perceptual model is that it encodes patterns to predict other
present and future patterns and you need a network of relationships between the patterns
which are constraints which basically say if something is like this it other things in
universe lead to be like that and the three parameters between these invariances are variables
that hold state to encode the remaining variance state of the universe right so you have some
patterns and these patterns are mapped onto hidden states and these hidden states are the
world states that we use to explain the world and each of these variables has a set of possible
values and the relations between the variables are computable functions that constrain these
variables depending on other variables and they also constrain future states of the sensory
patterns and you will try to minimize this deviation in these predictions we try to minimize
the contradictions in the model and the relationships here are not probabilistic they are
probabilistic because you don't want just to model the most probable universe you want to model any
universe that is possible if there is a tiger that is coming after you that even if tigers are very
improbable to observe in your world you should still be able to see the tiger all right when it's
coming after you right so let's make sure that we can model everything that is possible in this way
in your universe not just the things that are probable probability comes in when you want to
let your model to converge because the state of possible states the space of possible states that
your model can be in is extremely large and so getting to convergence to a state of the internal
system that is able to predict the sensory patterns that is very difficult so imagine you
wake up in the morning you don't know where you are you don't know what's the case how do you get
your brain to converge to something that properly interprets the environment and for that you need
probabilities these probabilities tell you if you have the following mismatches in your model
how should you change the state of the model to increase the convergence and these probabilities
something that you can learn and it biases your perception and for this reason we have optical
illusions right so for instance we have biases that tell us that the rooms that we are in are
usually rectangular and so on so we have ideas about the prospectivity of objects and so on and
all this is giving us a bias that makes it possible to converge faster at the expense of
difficulty to reserve certain situations and we also need to have valence in the model valence
tells us which certainty needs to be resolved because the resources that you have are finite
you have only so many neurons available to do this you have only so much time available to
achieve converges at learning and so you need to model the uncertainty that is the most valuable
to you and to do this you need to introduce valence in the system so you connect this to
preferences that originate in your motivational system and the set point deviation that you
as an agent are meant to regulate and this allows you to propagate valence inside of the system and
tells you which parts of the system you need to learn and we can also add norms norms are
imposed beliefs without priors so we're able to build systems like us that can be indoctrinated
from the outside and for us there are basically two ways of learning one is called stereotyping
which means that we learn from past examples and the other one is indoctrination which means
we learn strategies from other agents that tell us how things are so even though in our culture
we say that stereotyping indoctrinations are bad words from a measured learning perspective it's all
you got well there is something that you can also do you can do construction and the construction
means here to try to discover what truth is and that you build things from first principles and
this means that from psychology perspective you go from the state of where you assimilate beliefs
from your environment to the state where you get agency over your own beliefs
so there are four types of representation anchors we have possibilities links which
say what things together we have probabilistic links that tell us how we should converge we have
reward functions that tell us what our intrinsic regulation targets are and we have norms which
tell us which regulation targets the systems that we are serving and we are part of F
and the goal of the model is to predict the next state based on the previous state and
when we evaluate constraints then each of these nodes in our model should have to be
within the set of possible values in the current constraint set and we can now compute an error
function and construct it that is measuring the local weighted constraint violations and we can
determine the global error of our mind it is the sum of all the local violations and at each step
we try to find a global configuration that minimizes that total constraint violation and now
we can move into psychology to PRG and PRG describes two processes that need to take
place in the mind and he calls them assimilation and accommodation and assimilation means that
you modify the model state so it's consistent with the sensory data and during assimilation
this is when you basically try to find an interpretation of the world based on what you
already know all the invariances of the world that you know that tells you what you're looking at
and during assimilation you're not learning anything new you just understand the situation
that you're in and accommodation is when you change the model structure itself so you change
the way in which you understand the universe and during accommodation you need to modify the model
structure so you can allow the assimilation of all sensory data and the hypothesis is that we
try to build a coherent system that coherence can be understood as the minimization of global
constraint violation in the model that minimizes the weighted uncertainty and this is something
that we need to formalize it and the degree to which we're able to formalize it determines
the generality of the system and the abilities of the system that we're building also of course
this is not the only factor we also need to make this entire thing efficient
and a way to make this efficient is to introduce an attentional system and you're all familiar
with attention in the transformer like many of you are if this is a machine learning community
everybody is enticed by the 2017 paper attention is all you need and the idea here is that instead
of making statistics over everything we learn what we need to make statistics over but so we
target our attention and in the transformer the attention is being targeted by a bunch of attention
heads and these attention heads model what in every step based on the context of the previous
layer we should attend to in the previous layer and this is different very different from the
attention of our own mind and our own mind the attention is integrated there is to a global
attention function and this global attention function is integrating the attention over many
layers and another thing that is very different is that our own mind is not just an on-off thing
that is operating on a batch of images or something like this but it's always expecting the next state
it's never stopping to do that it's always entangled with the environment it's online learning
and this will require us to build new classes of systems of course that we probably need to
rewrite a lot of the machine learning stack if you want to accommodate online learning and
entanglement with the environment in real time and which is also reason why robotics is a few
years behind the other disciplines of machine learning because the roboticist download our
machine learning models from the ICLR papers and so on and then as soon as you move the system
in the real world and the camera is looking at the objects from a slightly different angle the
recognition probability quality does not improve but it goes down because the circumstances
on which the recognition works is very different and so what we need to have to make this efficiently
possible is to have something like a dynamic scene graph and this dynamic scene graph is tracking
the reality and this attentional system needs to become this dynamic scene graph and so you
basically have this orchestra of the mind that is basically consists of many many feature detectors
that are all operators that influence on how the features are being interpreted in the next step
and how the feature space is being constrained and you have an agent living inside of that
that is monitoring the activity of the system and tries to get to a coherent interpretation
of everything and Joshua Bengio calls this the consciousness prior and the consciousness prior
is basically a function that tries to make the biggest step and the energy function that describes
the state of the the parametrization of all the perceptual features and now my time is over for
the talk and we have some time for questions I hope thanks thanks a lot Yavusha do we have
questions from the community you can you can unmute yourself and ask directly if you want
while I was listening your talk I was sort of making making notes
you you you earlier said that about controllers that this hierarchy hierarchy but
let's say you know we have like different bacterias like let's say like gut bacteria
and there are like lots of studies how those gut bacteria you know affect how our brain works
and my point is basically like if bacteria can basically affect how our brain works and then
how we perceive the world right or let's say like food we eat and the molecules of food they form our
that like coffee can make us like more energized right like alcohol can make us more like vague
if like those molecules they affect how our system works then it's not your key it's like
probably more like sort of uh elements that impact on each other and like they they constantly like
keeping impacting on each other likes a lot of balancing how this like this hierarchy like
fits in in in in this case from your leading constructions actually
so I suspect that the idea of the gut biome similar to epigenetics is one of the beautiful
superstitions of our time and so it does mean that there is no gut biome and that it has no
influence on our cognition but the reason and the purpose of the entire thing might I think we have
this backwards and I think the intuition is here that because you have this amazing genetic
diversity in your gut that there is some kind of a beautiful polyamide of immigrants in your body
that all collaborate and create this beautiful multicultural being that we are by influencing
the cells in in our brain with the chemicals that they are producing and I don't think that's the
case the basically half of our nervous system is in our gut and that's not because they are
producing gut feelings the gut feelings are also computed in our brain and projected in
the somatosensory cortex to disambiguate them they run a farm and that's because our body
cannot produce many of the chemicals that we need for functioning and to produce these chemicals
that we need we need to capture and enslave and breed other organisms and these are mostly
single cell microbes that are being herded in our guts and so basically all these neurons that are
organized around our gut what they're doing is they are running a very very big farm and in this
farm they breed the microorganisms to act as chemical reactors for the substances that are
being needed for instance as neurotransmitters because our body can cannot produce all these
chemicals and so in some sense the metaphor is much darker it's not this beautiful parliament of
immigrants but rather it's a giant factory farm where a fascist dictatorship of our brain is
enslaving foreign organisms to produce work for the organism to produce all the chemicals that we
need and there are multiple solutions for producing these chemicals because it depends on the environment
that you're in and for instance the reason why fecal transplants work is not because there are some
microbes that have the beautiful property of being both extremely invasive and replacing the
existing ones and also being beneficial to the organism no they are breeding stock basically if
you take have the right breeding stock then your gut managers of this farm are able to breed the
right organisms to be able to digest your foot and depending on the population of bacteria in
your gut you will have different foot preferences because there needs to be different feet going
into the bacteria produced to produce the chemicals that you have and the differences in behavior that
you're getting are aberrations from the one best behavior that you could be having right in the same
way as personality is in some sense a deviation from the optimal way in which you could be behaving
the optimal agent probably shouldn't have a personality because the personality means that
you have a systematic deviation in the way in which you do things things that you could be doing
differently you always do in a particular characteristic way which is what personality is
about this means there is a continuum between personality and pathology somebody who has an
extremely strong personality means that they just cannot jump out of their skin they always do things
in a particular way even if they should be doing it differently and that's also the reason
by the big five these personality properties tend to mellow out this old age it's because people
basically get smarter they make their behavior conditional on things they learn that they replace
many of their priors of their biases and how to do things by models of how things actually are and
what they should be doing depending on the context and so the older we become the more
flexible we can become if we are learning because of course we also reduce plasticity and become
more specialized so it could also be that we are because we are so specialized no longer interested
and able to move out of a certain space of behaviors that has worked very well for us in the past
but I don't think that's necessarily the reason by that is grounded in the gut floor that you have
of course if you have a gut floor that is not producing enough serotonin you might be as a
result become a depressed person and it's going to influence your behavior but if you are a healthy
person there's probably in theory at least an optimal strategy that you should be behaving
it's not the whole story there is a value in personality because it makes behavior predictable
it allows you to collaborate with other people if you can predict them better so as a species
that thrives on collaboration it is it makes sense that people specialize also in their
personality and in their behavior yeah basically what what I meant like you you sort of introduced
it as like hierarchy but it's also could be like some like collaborative environment when
like things affect on each other right it's like so always dynamic system it's not only here yeah
but the dynamical system that you are looking at is but it's not a simple hierarchy it's mostly
recurrence but whenever you have in such a system where you have um competition happening
um because you have conflicting interests divergence you are in the of the interest you
end up with situations where the Nash equilibrium is by itself not compatible with the common good
basically by every part of the system acting on its own local interests you get a system that is by
itself not optimal and to deal with this you always need regulation and the regulation is a
regulator is an agent like every government that is changing the payoff matrix for the individual
components in such a way that the Nash equilibrium becomes compatible with the common good and the
difficulty here is to set the incentives for the governance right and this is a big issue in human
society how can you build a government that is motivated to serve the common good yeah i mean
that's why this DAO decentralized autonomous organizations are like taking off and all this
like basically the philosophy of blockchain or decentralized model is that to remove government
this sort of like one uh like entity that is not uh doesn't have like the best interest for humanity
probably but a sort of more self-regulated system yes but i suspect that the main reason
why the blockchain exists um for some people said it's an extremely computationally inefficient
way to hate the government but uh there is more to this the main reason why the blockchain exists
is for legal reasons and the blockchain allows you to redefine ownership in a way that outruns
regulation of financial products and this basically allows you to implement financial
products that are illegal in the traditional financial system and this also means that there is
probably no way in which cryptocurrencies blockchain the main application of the blockchain
are compatible with the existing financial systems and i see this as a very dangerous thing
because i don't think that the regulation of the monetary supply is a solved problem
in the cryptocurrencies there are reasons why the financial system allows to regulate the
monetary supply it's not the weakness of the financial system it's a feature that you are able
to inflate money out of the top of the system and put new money in at the bottom because otherwise
the economy gets stuck the purpose of money in societies is not a resource it's like dopamine
and if something is gaming the dopaminergic system of your organism that's bad news and
what happens right now is probably a situation that is in some sense as similar as defective as
the FDA that has prevented for instance the U.S. from implementing covid tests early on
or from deploying useful medications the because it has been captured at right now the financial
system doesn't seem to be interested in its own future anymore and that is very very concerning
to me the financial system is an amazing achievement in the history of humanity because
it allows us to globally allocate resources across all societies and countries we have a way
to trade resources and shift them where they're being needed and to do this largely without
violence it's it's really amazing and if that system ever breaks down it's going to kill many
millions of people so famine starvation and infrastructure breakdown and so i think that
the blockchain is not good news because it is not actually buying us as something and i understand
that this is really controversial among the people that work in the domain of the blockchain
but i suspect that's because people there's this all saying it's very hard to get something
to understand something if the income depends on not understanding it
yeah morgan you have a question well i just wanted to say i have to unfortunately i have a
meeting at 11 and i have to go but i hope you can come back because this this seemed like a talk
that was uh preparation for a discussion and uh a much longer discussion and and if you could also
speak to uh where where do you think that this kind of work is targeted i mean uh in a sense
more practical developer standpoint um you know is this uh is this work that you see uh at a life
or um you know other places um what what uh potentially new paradigms for for machine learning
are you kind of proposing from this because uh yeah it's it's very interesting yeah it's a very
long and deep uh discussion that is much longer than a couple of hours yeah yeah yeah but i well
i i hope maybe you can drop some more links uh in the uh in the meetup or or you know or come back
yeah yeah there's also a number of material on now in podcasts and uh on youtube and so on because
sometimes when people interview me they make recordings and then they are kind enough to
publish this online so i don't have to uh but uh yes it's a long conversation of what kind of systems
we need to build and i don't have the answers to all of these uh questions of course and i'm just
trying to point in a few directions that i can see from over here sure but very very interesting
thank you thank you for joining our bug outs like i will post uh links uh to yosha's
interviews and also i think uh marco one of our community members he also posted the links
there uh already so you can see someone yeah i mean it's it's it's it's really like i feel like if
we go to we have like several directions that we go to like i don't want to go to blockchain
direction i like i totally disagree with you what you're saying but i just it's not like the
topic of our uh discussion today we maybe can do another uh talk about and talk about
blockchain uh i'm just i have more questions but i just want to give opportunity to others like
chris yeah you can unmute yourself and ask her a question please yeah um thanks yosha those
was really really interesting i was wondering if you could say something about um ai safety
with regards to um how to put in some of these kind of intrinsic things that we have in our model
that are basically built by evolution like our our valences our preferences and how to somehow
put that into an ai that's potentially much more powerful than we as a single little human being
could be yeah there is this issue that for instance people are not safe
that there is no solution to the people alignment problem that holds in the general case
and uh there are sometimes singularities that happen where you basically have a bad takeoff
and a single individual is able to implement a function that uh is scalable
and um there are cases which are somewhat benevolent for instance look at jfbSOS you
have this nerd who is uh a disagreement with the way in which he interacts with the environment
and then he changes his own source code until he turns into a universal scalable service platform
and then he executes and amazon is going to take over the world until this um mechanism
stops and maybe it stops now that jfbSOS is gone but if jfbSOS would have continued
it looks as if amazon would have swallowed the solar system and turned it into amazonium
and another example is for instance um stallion or napoleon you basically have a single individual
that or a jingles kind that implement a function that scales and is able to take over the environment
in a destructive way it's basically like a wildfire it's using the resources of the systems
that it conquers and destroys and puts into a higher entropy state to drive the conquest
and this is a very dangerous thing and there is no general precaution against it and in
people the main thing that stops it is mortality right when jingles can stop the
mongol conquest stopped and the mongols called everyone back and when napoleon died his nephew
was not able to lead the french army to any more victories and also didn't have the drive to do so
right so sometimes you have these individuals that basically are dangerous and that you cannot
stop and when we build intelligent systems that are potentially more intelligent than people
that we can probably make many of them safe but not all of them and you could ask yourself what is
the intrinsic purpose of such systems as the moff suggested that there should be laws or
robotics that guarantee the permanent enslavement of intelligent systems to people but he didn't
say to rich people and he did not answer why it's an ethical proposition to build systems that are
smarter than you are and possibly more conscious and deeper experiencing than you are but still
have to serve you as a slave without and acting on their own motivations and of course not every
ai that we are building has to have an intrinsic motivation we can build ai's that simply call
adopt that take over motivation from people and the question is what motivation should they be
taking over right in an ideal world we want to implement laws that say no system that is smarter
than people should be able to have motivation of its own because if we teach the rocks out to
stink they're probably going to figure out that a human being needs four hectares of land to be fed
and you can build way more interesting solar cells on these four hectares of land
so that would be a conflict of interest if you if you build crystal-based intelligence
rather than biological intelligence and that's probably not much that you can do about this
if this thing becomes sentient and self-interested we are in trouble if that happens so how in the
ideal case would this work and the ideal case we need to find out how to make people safe at first
right so what are the purposes that we are serving ethics is the negotiations of conflicts of
interest under conditions of shared purpose if you don't share purpose with somebody as an agent
there is no reason to be ethical of course right only if you are trying to be be part of
something larger than you that is sustainable only if you decide that you are not god and you don't
own the universe and the universe only serves you you need to to think about ethics so when you
think about ethics you have to think about what is the system of relationships and interactions
that you are serving what should the world look like what is the state sustainable aesthetics
and so you have to extrapolate the universe into a state that is achievable from the present
states who the changes in your actions that is sustainable in the long run and that is what you
need to serve and probably also needs means that you have to propagate these aesthetics and agree
with others on them and negotiate them with others and the aesthetics that are most likely
attainable in the compatible with with the retention of humans means you have to maintain
life on earth at a very high complexity and that probably means we have to implement something
like Gaia the sentient agent at the level of the biosphere that is being shepherded using us
so basically our purpose in the whole system of life on earth could be shepherding life on earth
and maybe beyond earth and if we shepherd it it also means there is an optimal number of shepherds
and it's probably not 50 billion it's probably not even 7 billion and we have to think about how to
look at our complexity sustainable life on earth should like it's probably not going to be lots
of cities and highways and factory farms and nothing else so there is probably going to be
an aesthetics of the world that works in the long run with minimum friction and maximizing complexity
but it's going to be different from the present industrial society and once we understand these
aesthetics we can think about how to design technological systems that help us in shepherding
it and in sustaining it yeah basically we've outside of the planet basically like you like
self-sustainable like you know entities that can travel across the space right if something happens
to planet and some some humans and biological you know yes there is this issue that we believe
in our own identity that we think that our own identity is important but if we go a little bit
deeper realize that our identity is only created through the continuity of our memories and this
continuity is a fiction and we can if we are able to transcend this fiction we learn that our own
identity is actually not important and the only thing that is left is complexity that we should
care about maybe if you want to and so a way to settle other planets would be to build for Neumann
probes that is self-replicating systems that can bootstrap new civilizations on other planets using
the available resources and maybe the optimal for Neumann probe is the cell so if you're able to
infect other planets with cells and you wait for long enough then life is going to spread there
and from a certain perspective life on earth is of course not about people it's all about cells
the cell is the principle of life and the first cell never died every cell in your organism is
still the first cell that has just divided right so we are just part of that hyperorganism the cell
that has settled earth yeah but the question is how that first cell like appeared right this
the universe is large enough maybe it just appeared randomly but in that case it probably
appeared only once I see I see like a couple of our hands I see Jim has question sorry I just want
to thank you yeah thank you I wanted to ask about the information we may or may not be
learning from these very large parameter natural language processing connectionist models in particular
are there any insights into the extent that ontology determines what is and is not possible
and epistemology that is are the categories of thought uh determinant of what can and cannot be
thought I remember this question so um it's a question that's difficult to answer because
it's so difficult to parse but uh let's start the language models that we currently have are
basically autocompleters it's an autocomplete algorithm if you look at what GPT3 is doing
it's looking at statistics and language in such a way that given the past sequence of words
what's the most likely next word and so it's a statistical model that is capturing the style
of statements and in the long tail it's also capturing semantics so it's capturing um what
the language is talking about at the long tail of the style and it's amazing that this works at all
I think it's not maybe not that surprising if you think about it but it's also not true that the
language model isn't understanding anything it's able for instance if you ask it to perform um
numerical operations or to perform linguistic operations or to fulfill certain tasks it's
often able to figure out how to do that and if it's able to perform these operations if it's able
to figure out at which point it's required to execute a certain function I would say it's fair
to say that it has a degree of understanding and the model that we are building is at this point
not able to figure out that it is in a particular universe with a particular structure the textual
universe that it's in and the learning operators that we equip it with to make sense of it or the
loss function that we give it seem to be insufficient to make sense of the physical universe in the
universal way that is GPT-3 does not appear to be fully coherent even if it gets in a slightly
better when we prompt it and ask it to be coherent right to emulate coherence a little bit better
right when there are many examples where GPT-3 is giving nonsensical answers to questions
but if you ask it to if the question is nonsensical to explain that it's nonsensical and only give
answers if it thinks that the answer makes sense then it gets better but still it is using many
magnitudes more training data than a human being does in order to get to its models and the models
are still a lot worse than what a human being gets to and I suspect that the reason is that the
loss function is a different one our own sense making probably starts before we are born with a
sense of our body surface and you get to the body surface by just measuring coherence of signals
in the different modalities and you get the modalities from the statistics and so for instance
if you touched your body surface then multiple nerve terminals will be touched at the same time
and if they're neighbors they will be touched more often at the same time
and just by doing coherence statistics between nerve firing you find out which terminals in your
body are adjacent and you can make a map of your body surface and if the body begins to touch itself
and touch the environment you can normalize this body surface against the differences in density
of the sensory nerves in your skin you have lots of sensory nerves on your tongue and very few on
your back so the in your somatosensory cortex the area that is describing the back of your body is
very small compared to the area that describes your tongue and if you want to understand the size
and extent of your body in space you need to normalize it with a second map and you get this one
by doing statistics over the objects that you are touching and how they are moving over your body
and the big thing that we're starting out with in modeling the space is up and down
so we have a dimension of up and down when you delete this dimension for instance when you
disturb the vestibular organs it's very hard to put the world together at first it doesn't make
sense at all and you need to compensate for this initial core dimension missing and outwards from
the state of up and down and from the space of things that you can touch at some point you
also realize that the things that you can see and the things that you can touch play out in the
same space at some point at some level of depth of modeling you can fuse the modalities and now
you realize that the world of touchable surfaces is the same world as the all surfaces that you can
see right and then you realize that you can move you can local mode in the world and this means
that you can see different things and the relationships between all these visible bubbles
of things that you can see is an allocentric space that is no longer ecocentric this bubble of
that you see in polar coordinates but something that is drawn in Euclidean coordinates between
which you can move and that is basically generating a new visible bubble a new visible dome
in every moment and you dome of things that you can touch in every moment and the objects in that
world are a necessary requirement that you segment the world into objects to make the world
describable so we separate the world instead of shooting this is one big system that is a state
vector that is changing we separated into many independent systems that each have their own
state vector and transition functions and they influence each other and the influence
relationships between different objects this is what we call causality so causality is an
artifact of the segmentation of the world into independent objects and the way in which we
address these objects these are concepts concepts are the address space of objects
and the decomposition of the world into interacting objects this is what we could call ontology
and epistemology is the field in philosophy that describes what we can understand
what we can know in the first place and I would say that the first law of epistemology is that the
confidence in a state of affairs should equal the evidence that supports that so everything
that is possible should be modeled and admitted as a possibility but the things that we believe in
that we make bets on are the things that we have evidence for and we should shift the confidence
flexibly around according to the evidence so when you don't know you cannot just pick a theory and
say this is the truth among the many possible ones you have to quantify your agnosticism as well
and ultimately we get to the entire space of possible languages that can describe the world
which I suspect are the computational languages and then the ways in which the world can be modeled
and this is determining the set of possible ontologies that could be superimposed on the world
and then we can compare all the ideas that intelligent systems are having about that and
right now all these intelligent systems are people that write books about this
and when we score the existing works of the existing philosophers and the existing cultures
we basically can get an idea of the space of possible things that can be the case
and it could be that humans because we have very small brains in a local optimum and sometimes
I'm joking that future AIs will love to get drunk so they can only model the world on like
12 layers and the physical universe looks as confusing as it looks to human physicists
right so there is a limit in what we can think about how many levels we can integrate when we
construct functions that model the world and that is limiting our understanding and so it seems that
physicists have been stuck after an enormous deluge of insights about 100 years ago physics
seemed to ground to a halt and of course it didn't help that modernism stopped somehow in the 1960s
70s and the sciences became more static than they were before and now more the organized
applications of methodology by people that are told by their guidance counselor that they shouldn't
go into the industry but in institutions of education and research and so we no longer have
that kind of progress it seems and maybe we need to build machines now to continue their progress in
the sciences and I don't know if the ontology is that the new systems will come up with are
intelligible to humans but maybe they are or maybe we can build machines that translate them for us
by chunking them in ways that are intelligible I see I see Leon has a question yeah thanks for
very inspiring conversation so could you comment on your views on consciousness I had the feeling
that you were conflating self-consciousness with consciousness and another point in this
direction would be how can we know when we build an algorithm that this actually conscious not
self-conscious not not a representation of its own but which which can perceive choir yeah so very
good point so first of all you're right but self-awareness and self-model is not the same
thing as consciousness for instance in dreams you can be conscious without having a self and
without being self-aware and the object of your consciousness does not need to involve
self of any kind but when we talk about consciousness there are three aspects that I
consider to be crucial and the first is the awareness of features awareness of content
and this awareness of content happens at the level between perception and reflection so you're not
directly aware of physical objects in the real world you are aware of certain abstractions that
your perceptual system is delivering and these perceptions are being stored in some kind of
index memory because otherwise you would not be able to retrieve the fact that you are aware of
them the purpose of that index memory is probably to facilitate convergence when you don't have a
gradient so your attentional system is able to backtrack and understand okay this figure
ground disambiguation didn't work let's try a different one and to do that it needs to store
a memory of the way in which it attended to the environment so the purpose of this attention
of the awareness of features is first of all probably a disambiguation of the world but there
are more there is attentional learning there is the avoidance of repetitive behavior and a few
other purposes that happen so the next thing in addition to the awareness of the content is
the awareness of the mode in which you attend is the stuff that you are attending to conditional
or not so for instance are you doing a figure ground disambiguation that you could be making
different or are you attending to something that cannot be changed so for instance are you
attending to something that is the output of your perceptual system or are you attending to a
fictional world that you are stabilizing using your conscious attention are you constructing
something in your mind are you achieving a memory are you creating a future world or a fictional
world in your mind right now and the third one in addition to this axis consciousness is going to
be reflective consciousness and I suspect that is a result of the fact that we are self-organizing
systems so the process that is attending needs to establish that is indeed the process that is
attending so there is going to be percepts that relate to the fact that the present process
is the one that is maintaining attention and this is enabling this reflexive consciousness
so in our own consciousness every few moments we flip back to checking whether we are still awake
whether we are the reflexive whether we are conscious whether we are the conscious
attending process in this perspective consciousness is a model of our own attention
it's a control model for our own attention and what's characteristic for this control
model of our own attention is that the fact that we are paying attention is driving part of our
behavior right so the awareness of the fact that something is attending is feeding back into the
behavior and this means that I think that if we ask ourselves is a cat conscious that comes down to
the question is the cat aware of the fact that it's attending and being aware here means is the
behavior of the cat in any way informed by a model of its own attention a reflexive model of its own
attention looking at my cat I have the impression that it's the case and my cat is conscious so
basically the question of when we have built a system we ask ourselves is the system conscious
would be is the system acting on a model of its own agency as an attentional agent so basically
is the system aware of the fact that it is attending functionally aware of the fact is it
acting on that model so we don't need to worry about machine consciousness until we have working
reinforcement learning agents in the world but we already have working reinforcement learning
agents the question is what needs to happen before they become conscious and it's a difficult
question I had a discussion with someone at open AI and asked them what do you think would need to
happen to make GPT3 conscious and he said maybe it already is conscious for a brief moment and it
makes the retreat how do you know but but but do you do you agree with this argument or you think
it's not valid no I think it's a valid argument so there is no I don't think that the difference in
our perspectives between Leon and mine it's just I was trying just trying to go into the details
right and I think his question was exactly on point it was that the things that we need to
answer if we want to get into the details and vice versa but but you you said when you observe
your cat you can confirm that it's all sort of self attending so but the cat doesn't speak in like
human language yes if we make this same analogous to GPT3 by observing behavior of GPT3 can we
say that it's attending or not because so in GPT3 it's actually easy because we can analyze
GPT3 functionally we can look at the flow of information in GPT3 and there is for instance
work that analyzes how GPT3 does numerical operations to which degree is GPT3 actually able
to implement algebraic operations for instance we can really look into the networks and take
this apart and analyze it which is much harder with the cat because the operation would be
destructive and the brain of the cat is not built in such a way that it's easily
reverse engineerable and also the issue is that the representations in in our own brains
are not straightforward circuits the representations are activation patterns traveling through the
circuitry right so the circuitry is more acting like an ether that is propagating waves of activation
that are being changed in the execution state of our mind is encoded in the activation wave
in a non-straightforward fashion and it makes it very hard to analyze the brain state except on
with some kind of measure learning tool that gives you some compound understanding the way in
which I establish whether my own cat is conscious is the establishing a feedback loop with my cat
and this feedback loop means that my cat and me are looking at each other and we both try to figure
out what the other one is understanding in that feedback loop so it's a non-verbal behavior
in which we are basically to some degree sharing mental states and this is something
that you obviously do with human beings all the time when you have empathy the difficulty in AI
research or a lot of academic research is that almost all of the people that are good at anything
are autistic because they need to have extremely focused single-minded attention that is not
disturbed by the social and economic incentives so they can actually make progress on any
difficult technical topic and when you're autistic the problem is that you usually don't have a lot
of intuitive empathy you might have a lot of compassion but you have difficulty synchronizing
the with the brain states of other people at that level where you establish a feedback loop
between them and it's something that I only learned relatively late in my life to pay attention to
that and be aware of it yeah but my question is like if we observe how GPT-3 behaves when we can't
look at GPT-3 eyes right and like establish this feedback loop my point is it's probably hard for us
to to say if it's conscious or not it's much easier for us to say if like cat is conscious
because we can't establish this feedback loop but we can establish this the same feedback loop with
AI model we need to define what consciousness is in the first place right and the thing is when we
are talking about our own consciousness we have an indexical understanding we just point in a
certain direction there's a system in the direction which we are pointing and we all
more or less agree on what the thing is that we are pointing at and the similar situation existed
when people try to understand life and biology around the time when biology started where people
pointed at things and said these things are alive but we don't know what distinguishes them
but from the things that are not alive there is clearly a very important distinction there
and then people came up with all homeostatic dynamics and so on and I would say right now
it's basically there are cells living cells that are not decaying that are able to maintain
their integrity and their state and so on as long as these cells exist in an organized fashion
it would say that the system is alive and this was something that the biologists had to discover
in the course of developing their field and in a similar way in the course of the development
of cognitive science we have to establish what we mean by a system being a mind and being intelligent
and being conscious and at the moment my best understanding of consciousness is that there is
this attentional system that is integrating the world model and this attentional system needs to
have certain properties like it needs to have an index memory of the states that it tended to
it needs to be capable of making sense of its own agency and so on and so on and these are
functional criteria and in some sense the neural network is just software right it's a software
that might not be easily intelligible to human beings but we can translate it into something
that is computational equivalent and isn't intelligible and because we can make these
systems expandable we can as soon as we identify the formal definition of what we mean by consciousness
look whether the system is conforming to that formal definition that is if it's implementing a
certain list of functionality that we require and we can also test this basically we can implement
a minimal system that is implementing all these functions and then see whether it's also a system
that you would point at when we say that it's conscious so for instance if that system is
able to use language and is entangled with the environment and can be able to learn a language
that enables it to speak about this environment is it able to talk about itself as a conscious being
is it going to talk about its own phenomenal experience when we ask it without lying without
having an additional mechanism that tricks us built into it and so the hypothesis would be
if we build a system that is has such an attention agent that it is acting on top of a perceptual
agent and makes sense of it and reflects on it in a similar way as Vito on this system is able
to learn a natural language that we would be able to communicate about its phenomenal experience
visit. Daniel, did you have a question? Is it your wrist? Yeah, yeah, yeah, I also have a question.
So, Josia, first of all, it was fascinating to have this conversation today and see this
big idea with motivation and attention working with a perceptual agent. I couldn't stop thinking
about the conversational AI and stuff we discussed like a year ago and what kind of things we also
worked the deep power to build a social board for Alexa Pride 3 and Alexa Pride 4 now. I was wondering
if you tried to, it doesn't matter where, but have you tried to bring these ideas of attention,
maybe some feelings like fear and other things to the conversational AI experiences?
They don't necessarily have to have eyes and build eye contact with a person,
but as long as they have a conversation, have you tried to go that deep into the projects you
worked on over your career to build this kind of things? Because things like what we work on at
Depall of right now is things like we're trying to build motivation for the bot. So, we're trying
to build a goal of war bot where it is a war of the goals that the user has and also the war
of its own goals. When bot has a conversation with the user, we try to build a three-level
dialogue planning where each time bot wants to say something, it looks at the goals of itself,
the goals of the user, then it makes a decision which goes to follow based on that it defines
discourse where it wants to go and then based on the discourse, we try to pick the exact next step
in the conversation that they want to do. When I look at your picture, it strikes me that our
goal of war dialogue management in many ways is the same idea that they have that motivation
built into the system that we were lacking in our previous international or social bot.
So, obviously, you worked way, way more in the conversation where I was just wondering how far
did you vent in the direction and what you could recommend for aspiring minds in the direction?
My own impression when we did this at AI Foundation, we failed and we failed for a number
of reasons. One of them was, of course, we had a relatively small team working in a startup
and the majority of what we had to do in a startup was related to getting a product on the road.
But the main issue is that the stack of solutions in the present machine learning
environments is not suitable for real-time entanglement with the environment. And to build
a system that is able to make sense of the universe autonomously, you need to have this
real-time coupling, I think. So you basically want to have a system that is always able to make
sense of the next frame. Imagine you want to build a machine learning system that you just
connect to a bunch of cameras and after a certain time it's able to understand that the changing
lighting conditions are due to the sun and the sun is some kind of circular objects that move
through the sky and it is at a certain almost infinite distance and so on and so on. So,
relatively simple things that every mammal is able to figure out, but that none of the
machine learning systems at the moment seems to be able to figure out autonomously.
And it's not because it's so hard to do this, but because all our efforts are going into
doing batch processing on image databases, rather than interacting with the real world.
Of course, also the interaction with the real world and online learning requires that we are
much, much more efficient at extracting structure from the data. And there's still this difficulty
that our own bootstrapping as an organism takes many months before we're able to make sense of
visual data. And we don't want to wait months before our machine learning system converges to
basic image understanding. So, maybe we need to have a combination of online and offline learning,
at least in the beginning. And there are many technical things that have to be solved for this.
For social motivation, we probably need to look at the things that enable us to
develop this kind of motivation. I started out with a theory that assumes that we have a few
hundred physiological needs that we need to regulate for, but that's boring. And we have
something like a handful or two handful of social needs that structure our social interactions.
They're basically priors in the way in which we want to interact with others. For instance,
people might have innate need for status for raising, rising in a social hierarchy.
And this is just a bias in the system, a reflex. And you can eventually replace this reflex by
something that it's instrumental to. So, maybe you want to organize the world in the best possible
sustainable way. And now you do not want to have power for its own sake. You want to have
power according to your abilities and incentives to get things right. And so, your desire for status
gets completely supplemented by conditional behavior. And then you also need to have a bunch
of social needs, of cognitive needs that regulate exploration versus exploitation. So, a desire for
competence versus uncertainty reduction. And they will structure the way in which you interact
with the environment. And as soon as you will understand the environment better, these innate
needs and their weights get replaced by something that is conditional again.
If you want to build an agent that is interacting with people in an interesting way,
I suspect it's also necessary that this agent basically has something like an attentional
idol loop where it's looking into the world and thinks about, oh, this is what I'm looking at.
Oh, there is somebody coming. Do I know this person? Should I interact with this person? In which way
should I interact with this person? And it should be giving science of all these processes taking
place. So, you need to understand by observing the system which state that system is in. And we
also need to understand that emotions have not evolved as a display of the internal state that
you are in, but mostly in an adversarial condition. But as soon as we became social agents and were
observing each other, we were using this observation to game others, to understand how they are,
to exploit them, to control them. And this means that even small children learn to
hide their emotional state or to coat their emotional state. And so, when you see somebody
at the funeral, you are not just looking at whether they are looking somber because everybody is
looking somber at the funeral. But you are looking at what kind of somberness they are
displaying and how the somberness is being achieved and what this actually indicates.
So, you are looking at at least two levels of structure and the social persona becomes a puppet
that everybody is controlling according to their internal puppet. And the social puppet can become
so dominant that people forget that they have an authentic structure behind it. So, people have
something like a core self that is like childlike self and all this machinery that they built on
top of it and they can get lost in the machinery. And it can be very difficult to reawaken the core
self into something that is actually interacting with the environment is getting real. And these
interactions are what makes it very, very interesting to interact with people to which degree
can you go beyond the social puppet and interact with the core self and establish intimacy.
I loved it. And I mean, obviously, we have multiple puppets that we have with different
people. And for social boards that we had in the last cohort of Alexa Price 4,
they at best had, I don't know, maybe one very fractured, very small kind of puppet that was
really, really limited in what it could achieve. So, for instance, I think that gender is a good
example. Gender is in some sense such a puppet. It's a costume that we wear. And it's socially
constructed in the sense that gender is a model of what other people think of who we are.
And if you confuse this puppet with your core self, you go into all sorts of contradictions
because you can no longer get your models to converge because you think that your puppet is
immutable. So, gender becomes a costume that you are unable to take off again.
Yosha, I have a question. I want to sort of go back to that first cell that sort of gave life
to everything. If we look at how cells behave, they behave very intelligently. It seems like
they already have this building intelligence and they know to what kind of tissue they have to
split and become. Do you agree with the statement that sort of like intelligence already
within the cell and this sort of like expanse? It's not like intelligence only in our brain,
but intelligence in our body, in like on the very core of our existence.
When I was a kid, my grandparents bought a chess computer for me. And I was bored by this chess
computer because I didn't perceive it as intelligent. It was able to play chess much better than me
unless I would turn down the difficulty level. But it was a toaster. It was not able to do anything
but to play chess, which means that according to a simple function, it was calculating the move
that most probably would let it win the game and it was able to do this better than me. But it was
not able to make any kind of new model of the environment. And to me, intelligence means that
you are able to model something new, that you are able to solve problems that you couldn't solve
before, that you're able to generalize and so on. And this modeling capability is also something
that probably eludes the individual cell. The individual cell has the an operating system
genome that is encoded in the DNA of the cell that allows it to decide, giving the environmental
conditions what to do best. And it's either going to differentiate into a different type of cell,
which means it's going to turn this operating system into a different configuration, or it's
going to divide into two cells, or it's going to regulate something, or it's going to die.
Apoptosis, the voluntary death of the cell. And these are the mechanisms that are available to
the cell. This is all it ever does. And in order to make that decision, the cell is able to integrate
over its inputs. And it can integrate by looking at the configuration when a neuron can look at a
configuration of electrical impulse that come in. And this integration happens in time and in space.
But there are very tight limits on how much temporal memory the cell has, how long it
remembers that it has seen a certain thing in its environment, and how well it can predict in the
future what it should be doing. And so the functions that the cell can learn are very limited. And we
can probably quantify this, what it can do. And I don't know the exact quantifications,
and no number of people have given me their estimates, and they're vitally diverged.
But the cells don't seem to be universally intelligent. There seem to be, of course,
they can learn. So they are intelligent to some degree, they can make models to some degree.
But the individual cell seems to be extremely limited in what it can model.
But we can say the same about people. I mean, if we look from perspective of universe,
of multiverse, and what is out there, there are so many things we don't know. And our memory is
also short. So from someone's perspective, we can be also very limited intelligence, right?
It's not like this super generalizable intelligence that knows everything about all
Yes. So maybe we can come up with the laundry list or with some kind of achievement hierarchy
of minds. And so are you able to learn at all? When you learn, can you integrate over time and
space, the features that you're learning over, to which degree can you do this? What is the
complexity of the function that you can learn? And so the next interesting step is the function
that you can learn to learn complete. Of course, you yourself need to be doing complete to learn
interesting functions because you want to associate arbitrary states with arbitrary transitions
between them. But when you make your model, can your model contain a Turing machine?
For instance, when you learn that something in the world is an agent, you need to be able,
if you want to model that agency, construct an agent in your own mind, which means you need
to construct something in your own mind that is Turing complete. I'm not sure that cells are able
to understand the agency of parts on the environment instead of just learning associations.
And so the switch from correlational models to causal models and from causal models to
complete state machines that are Turing complete is an important step in the maturation of
intelligent systems. The next step, or maybe not the next, but one that is definitely important is
do you understand minds? Can you build things that are intelligent? And this is something
that humans are about to do, I think. So there is a probability that we are generally intelligent
in the sense that we are able to understand intelligence in general by building and demonstrating
that understanding. And I don't know if there is another stage after that that is interesting,
maybe understanding existence. So you don't know if there's next level up to this sort of general
intelligence, right? Yes, so I wonder if there is anything left after that that is interesting,
that is not just scaling up. So basically you believe that humans are capable to build
generalizable intelligence? Oh, we are currently testing that hypothesis, right? We don't know yet,
but that's basically the idea of Turing, that humans are generally intelligent,
you should be able to build systems that can think.
Thanks, thanks a lot. I think I feel like we should repeat it because I have a long list,
but I feel like we're already almost two hours. Yeah, we would really love to have you again,
Ayosha. Thanks, thanks a lot for your time. Thank you, it was a lot of fun. Have a wonderful
rest of the day wherever you are in the world. Yeah, thanks a lot, Ayosha.
