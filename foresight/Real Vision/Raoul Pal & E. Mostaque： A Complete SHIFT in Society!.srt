1
00:00:00,000 --> 00:00:04,000
As you know, I have a thesis about the exponential age

2
00:00:04,000 --> 00:00:07,000
that we're about to go through the fastest pace of technology

3
00:00:07,000 --> 00:00:09,000
the world has ever seen.

4
00:00:09,000 --> 00:00:11,000
That's what this whole series is about.

5
00:00:11,000 --> 00:00:14,000
This next interview with an old friend of mine

6
00:00:14,000 --> 00:00:18,000
is probably one of the most important single interviews

7
00:00:18,000 --> 00:00:20,000
you will watch.

8
00:00:20,000 --> 00:00:24,000
I can't stress enough how important the concepts

9
00:00:24,000 --> 00:00:26,000
and what is happening and the speed of which

10
00:00:26,000 --> 00:00:29,000
artificial intelligence is going to change humanity.

11
00:00:29,000 --> 00:00:33,000
It is something that we all need to pay attention to,

12
00:00:33,000 --> 00:00:37,000
understand and use in our favour because it's happening

13
00:00:37,000 --> 00:00:40,000
and it's happening at a scale of which you cannot comprehend.

14
00:00:43,000 --> 00:00:45,000
Emma, fantastic you see you back on Real Vision.

15
00:00:45,000 --> 00:00:47,000
Good to be back. Thanks for having me.

16
00:00:49,000 --> 00:00:52,000
You were known in the past in various guises on Real Vision

17
00:00:52,000 --> 00:00:55,000
because this is not your first rodeo.

18
00:00:55,000 --> 00:00:58,000
You were the frontier guy

19
00:00:58,000 --> 00:01:01,000
and we're going to talk about a new frontier today

20
00:01:01,000 --> 00:01:04,000
and then also in the pandemic you helped guide us

21
00:01:04,000 --> 00:01:06,000
through some of that too.

22
00:01:06,000 --> 00:01:08,000
Give people a bit about your background

23
00:01:08,000 --> 00:01:10,000
because it's a fascinating background

24
00:01:10,000 --> 00:01:12,000
and it's an interesting journey to where you are today.

25
00:01:12,000 --> 00:01:15,000
Yeah, it has been a few different skins.

26
00:01:15,000 --> 00:01:17,000
I was a hedge fund manager

27
00:01:17,000 --> 00:01:19,000
at the start of my career running Global Macro

28
00:01:19,000 --> 00:01:22,000
then switched to emerging markets, frontier markets

29
00:01:22,000 --> 00:01:24,000
and tech.

30
00:01:24,000 --> 00:01:26,000
At one point I was one of the larger video game investors

31
00:01:26,000 --> 00:01:28,000
which is also quite fun.

32
00:01:28,000 --> 00:01:31,000
Took a bit of a break when my son was diagnosed with autism

33
00:01:31,000 --> 00:01:34,000
and I used artificial intelligence to re-put up his drugs

34
00:01:34,000 --> 00:01:36,000
to make him it better.

35
00:01:36,000 --> 00:01:38,000
Well advised him some top hedge fund managers

36
00:01:38,000 --> 00:01:41,000
and governments on various things including counter extremism

37
00:01:41,000 --> 00:01:43,000
and fun things like that.

38
00:01:43,000 --> 00:01:46,000
Went back to being a hedge fund manager, did okay

39
00:01:46,000 --> 00:01:48,000
and then had a look at the world.

40
00:01:48,000 --> 00:01:51,000
What we do as fund managers is we look for arbitrage opportunities

41
00:01:51,000 --> 00:01:54,000
and we look for where things are building up and breaking down

42
00:01:54,000 --> 00:01:56,000
and it struck me that a lot of stuff was breaking down.

43
00:01:56,000 --> 00:02:00,000
I decided like let's try and make it better effectively

44
00:02:00,000 --> 00:02:02,000
by combining some of these systems

45
00:02:02,000 --> 00:02:04,000
and some of these arbitrage opportunities.

46
00:02:04,000 --> 00:02:06,000
So one of the big things was education.

47
00:02:06,000 --> 00:02:10,000
Another big thing was leading the United Nations

48
00:02:10,000 --> 00:02:12,000
AI initiative against COVID-19.

49
00:02:12,000 --> 00:02:14,000
I think the last time I was on Real Vision

50
00:02:14,000 --> 00:02:17,000
was February this thick, 2020

51
00:02:17,000 --> 00:02:20,000
and we had a nice chat about how the world was going to change

52
00:02:20,000 --> 00:02:22,000
dramatically and it did

53
00:02:22,000 --> 00:02:24,000
and then I was like oh crap I have to do something about it.

54
00:02:24,000 --> 00:02:27,000
So we made all the COVID research in the world free

55
00:02:27,000 --> 00:02:30,000
and pushed that and then I was lead architect of Kayak

56
00:02:30,000 --> 00:02:34,000
which was the UNESCO World Bank WHO backed initiatives

57
00:02:34,000 --> 00:02:37,000
to use artificial intelligence to take all that knowledge

58
00:02:37,000 --> 00:02:40,000
and compress it down to reports for policymakers

59
00:02:40,000 --> 00:02:42,000
so that we can be a bit more coordinated.

60
00:02:42,000 --> 00:02:45,000
Similar to how we take all that information and fund management

61
00:02:45,000 --> 00:02:48,000
compress it down and try to figure out those key drivers.

62
00:02:48,000 --> 00:02:51,000
It was a mixed success but I think it did some difference

63
00:02:51,000 --> 00:02:54,000
and now from that something a bit bigger

64
00:02:54,000 --> 00:02:57,000
which is AI for everything basically.

65
00:02:57,000 --> 00:03:00,000
So AI has clearly now become your thing, right?

66
00:03:00,000 --> 00:03:02,000
This is your focus.

67
00:03:02,000 --> 00:03:05,000
So before we get into what you're doing now

68
00:03:05,000 --> 00:03:08,000
what the hell gave a hedge fund manager

69
00:03:08,000 --> 00:03:12,000
the thought process that hey I can help the WHO

70
00:03:12,000 --> 00:03:15,000
and everybody else with artificial intelligence?

71
00:03:15,000 --> 00:03:17,000
Well it's because to be a hedge fund manager

72
00:03:17,000 --> 00:03:19,000
you have to be a bit of an egomaniac, right?

73
00:03:19,000 --> 00:03:22,000
I know that you're right and everybody else is kind of wrong.

74
00:03:22,000 --> 00:03:26,000
Look I mean all of what we do is information classification

75
00:03:26,000 --> 00:03:28,000
and that's the nature of artificial intelligence.

76
00:03:28,000 --> 00:03:31,000
So information comes in and then Claude Shannon

77
00:03:31,000 --> 00:03:33,000
style of information theory.

78
00:03:33,000 --> 00:03:35,000
Information is only valuable as much as the change in the state

79
00:03:35,000 --> 00:03:37,000
so you're always looking for something on that graph

80
00:03:37,000 --> 00:03:39,000
or that new thing that will cause you to change

81
00:03:39,000 --> 00:03:41,000
from a buy to a sell or an increase or decrease

82
00:03:41,000 --> 00:03:43,000
in your positioning, right?

83
00:03:43,000 --> 00:03:45,000
I kind of realized over the years though

84
00:03:45,000 --> 00:03:48,000
like my background was mathematics, computer science originally

85
00:03:48,000 --> 00:03:50,000
quite quantitative, quite analytical.

86
00:03:50,000 --> 00:03:54,000
The next wave of AI was something a bit different

87
00:03:54,000 --> 00:03:56,000
that would enable us to do something a bit different

88
00:03:56,000 --> 00:03:59,000
because we moved from this big data age, you know

89
00:03:59,000 --> 00:04:01,000
where you had massive amounts of information

90
00:04:01,000 --> 00:04:03,000
and extrapolated it and targeted it to kind of sell

91
00:04:03,000 --> 00:04:06,000
Raoul some suntan or whatever, you know?

92
00:04:06,000 --> 00:04:08,000
To something a bit different, a big model

93
00:04:08,000 --> 00:04:10,000
You're jealous of my tan.

94
00:04:10,000 --> 00:04:12,000
I'm jealous of your tan.

95
00:04:12,000 --> 00:04:14,000
I'm in London, you're in Cayman Island, I'm turning pasty.

96
00:04:14,000 --> 00:04:16,000
Completely.

97
00:04:16,000 --> 00:04:18,000
I have to fly out there too.

98
00:04:18,000 --> 00:04:20,000
So moving from that area was about extrapolation

99
00:04:20,000 --> 00:04:23,000
in the individual to something new in 2017

100
00:04:23,000 --> 00:04:25,000
which was a big model age.

101
00:04:25,000 --> 00:04:28,000
AI went from being able just to extrapolation

102
00:04:28,000 --> 00:04:30,000
to being able to pay attention.

103
00:04:30,000 --> 00:04:32,000
In fact, there was a seminal paper called

104
00:04:32,000 --> 00:04:35,000
attention is all you need about how to build an AI

105
00:04:35,000 --> 00:04:37,000
that paid attention to the most important parts of a sentence

106
00:04:37,000 --> 00:04:39,000
the most important parts of an image

107
00:04:39,000 --> 00:04:42,000
to do principle based analysis

108
00:04:42,000 --> 00:04:44,000
which is insane if you think about it

109
00:04:44,000 --> 00:04:46,000
this is exactly what we do, right?

110
00:04:46,000 --> 00:04:48,000
What do you mean by principle based analysis?

111
00:04:48,000 --> 00:04:50,000
Sorry.

112
00:04:50,000 --> 00:04:52,000
So rather than doing extrapolation

113
00:04:52,000 --> 00:04:54,000
so rather than doing momentum, beta

114
00:04:54,000 --> 00:04:57,000
AI became able to do alpha

115
00:04:57,000 --> 00:05:00,000
it became able to basically come up with principles

116
00:05:00,000 --> 00:05:02,000
that allowed it to understand

117
00:05:02,000 --> 00:05:04,000
the hidden layers of meaning

118
00:05:04,000 --> 00:05:06,000
with things.

119
00:05:06,000 --> 00:05:08,000
So you can go into a bit more detail

120
00:05:08,000 --> 00:05:10,000
but this is how the human brain works, right?

121
00:05:10,000 --> 00:05:12,000
We have two parts of our brain

122
00:05:12,000 --> 00:05:14,000
which is like the past

123
00:05:14,000 --> 00:05:16,000
and that's a tiger over there

124
00:05:16,000 --> 00:05:18,000
in that bush, right?

125
00:05:18,000 --> 00:05:20,000
It's the type one type two kind of thinking

126
00:05:20,000 --> 00:05:22,000
so AI was always the extrapolation

127
00:05:22,000 --> 00:05:24,000
and now we have a new type of AI

128
00:05:24,000 --> 00:05:26,000
that mimics the human brain

129
00:05:26,000 --> 00:05:28,000
by figuring out principles

130
00:05:28,000 --> 00:05:30,000
from highly structured data

131
00:05:30,000 --> 00:05:32,000
which is kind of what we did every single day

132
00:05:32,000 --> 00:05:34,000
as the finance guys, right?

133
00:05:34,000 --> 00:05:36,000
But at a completely different scale

134
00:05:36,000 --> 00:05:38,000
I don't think even now people have appreciated

135
00:05:38,000 --> 00:05:40,000
how big a change that's going to be

136
00:05:40,000 --> 00:05:42,000
I think we're starting to figure that out now.

137
00:05:42,000 --> 00:05:44,000
So

138
00:05:44,000 --> 00:05:46,000
before we get into the guts of this

139
00:05:46,000 --> 00:05:48,000
let's go on a bit of the journey

140
00:05:48,000 --> 00:05:50,000
of AI

141
00:05:50,000 --> 00:05:52,000
because you've talked about where it was

142
00:05:52,000 --> 00:05:54,000
and then we started to see stuff like

143
00:05:54,000 --> 00:05:56,000
DeepMind

144
00:05:56,000 --> 00:05:58,000
and then GPT-3

145
00:05:58,000 --> 00:06:00,000
OpenAI, all of this.

146
00:06:00,000 --> 00:06:02,000
So if you can just frame it for people

147
00:06:02,000 --> 00:06:04,000
because a lot of this is going to be new for people

148
00:06:04,000 --> 00:06:06,000
and I think it's incredibly important

149
00:06:06,000 --> 00:06:08,000
people understand what's happening

150
00:06:08,000 --> 00:06:10,000
and where the hell of this is going.

151
00:06:10,000 --> 00:06:12,000
Yeah, so if you look at DeepMind

152
00:06:12,000 --> 00:06:14,000
they are famous for

153
00:06:14,000 --> 00:06:16,000
many things, you know, Demos has done

154
00:06:16,000 --> 00:06:18,000
a fantastic job.

155
00:06:18,000 --> 00:06:20,000
One of the things they did is

156
00:06:20,000 --> 00:06:22,000
thinking back, you had Gary Kasparov

157
00:06:22,000 --> 00:06:24,000
being beaten by Deep Blue

158
00:06:24,000 --> 00:06:26,000
so you know, as older fellows we remember that

159
00:06:26,000 --> 00:06:28,000
not like the younger with the snapkins

160
00:06:28,000 --> 00:06:30,000
who'd never seen a bear market in their lives

161
00:06:30,000 --> 00:06:32,000
you know, kind of changes happening

162
00:06:32,000 --> 00:06:34,000
what Deep Blue did was

163
00:06:34,000 --> 00:06:36,000
it did an analysis of every single game

164
00:06:36,000 --> 00:06:38,000
in the past and then extrapolated

165
00:06:38,000 --> 00:06:40,000
and then it brute-forced it.

166
00:06:40,000 --> 00:06:42,000
So Gary could only think X moves in the future

167
00:06:42,000 --> 00:06:44,000
he could think X plus 2, X plus 3

168
00:06:44,000 --> 00:06:46,000
and that's how he got beaten by brute-force.

169
00:06:46,000 --> 00:06:48,000
Now, the Chinese game of Go

170
00:06:48,000 --> 00:06:50,000
Chinese checkers, effectively

171
00:06:50,000 --> 00:06:52,000
people thought you'd never be able to brute-force it

172
00:06:52,000 --> 00:06:54,000
because there's too many things that you can do

173
00:06:54,000 --> 00:06:56,000
there's too many moves on that board, right?

174
00:06:56,000 --> 00:06:58,000
And so they were like,

175
00:06:58,000 --> 00:07:00,000
that'll never be beaten by computer

176
00:07:00,000 --> 00:07:02,000
because you need to build a computer that's

177
00:07:02,000 --> 00:07:04,000
the biggest computer ever

178
00:07:04,000 --> 00:07:06,000
times a million.

179
00:07:06,000 --> 00:07:08,000
What DeepMind did

180
00:07:08,000 --> 00:07:10,000
is that basically they came up with

181
00:07:10,000 --> 00:07:12,000
a self-supervised learning algorithm

182
00:07:12,000 --> 00:07:14,000
that

183
00:07:14,000 --> 00:07:16,000
learned how to dream

184
00:07:16,000 --> 00:07:18,000
that's probably the best way to kind of put it

185
00:07:18,000 --> 00:07:20,000
about how one would play

186
00:07:20,000 --> 00:07:22,000
Go in a principled way

187
00:07:22,000 --> 00:07:24,000
so it didn't even have

188
00:07:24,000 --> 00:07:26,000
all the past games in its memory

189
00:07:26,000 --> 00:07:28,000
it just played against itself and tried to figure out

190
00:07:28,000 --> 00:07:30,000
how to do this. They made a whole base of these

191
00:07:30,000 --> 00:07:32,000
agents with reinforcement learning

192
00:07:32,000 --> 00:07:34,000
that you gave it an Atari with no instructions

193
00:07:34,000 --> 00:07:36,000
that learned how to play Breakout

194
00:07:36,000 --> 00:07:38,000
and then Starcraft

195
00:07:38,000 --> 00:07:40,000
without any instructions

196
00:07:40,000 --> 00:07:42,000
people need to understand that, they figured it out

197
00:07:42,000 --> 00:07:44,000
you put it in front of the computer

198
00:07:44,000 --> 00:07:46,000
and it just figures it out and it gets better and better

199
00:07:46,000 --> 00:07:48,000
so when you see it playing Breakout

200
00:07:48,000 --> 00:07:50,000
it's suddenly doing these crazy moves like that

201
00:07:50,000 --> 00:07:52,000
and so Lisa Doll was like a seventh Dan

202
00:07:52,000 --> 00:07:54,000
in Go

203
00:07:54,000 --> 00:07:56,000
so she was a Magnus Carlson

204
00:07:56,000 --> 00:07:58,000
of Go effectively

205
00:07:58,000 --> 00:08:00,000
and it wasn't a current Magnus Carlson

206
00:08:00,000 --> 00:08:02,000
type situation, it's very difficult to cheat in Go

207
00:08:02,000 --> 00:08:04,000
because you didn't have to be able to talk to you

208
00:08:04,000 --> 00:08:06,000
we're not talking about anal probes in this one

209
00:08:06,000 --> 00:08:08,000
we're talking about

210
00:08:08,000 --> 00:08:10,000
compensation is going pretty well

211
00:08:10,000 --> 00:08:12,000
so what happened is

212
00:08:12,000 --> 00:08:14,000
that the next highest person

213
00:08:14,000 --> 00:08:16,000
was like a fourth Dan or something

214
00:08:16,000 --> 00:08:18,000
so he's like Federer Carlson rolled into one

215
00:08:18,000 --> 00:08:20,000
that much better, I was like never beat him

216
00:08:20,000 --> 00:08:22,000
he won one game

217
00:08:22,000 --> 00:08:24,000
the computer won seven

218
00:08:24,000 --> 00:08:26,000
and everyone was like holy crap

219
00:08:26,000 --> 00:08:28,000
what is that, because computer learned to play

220
00:08:28,000 --> 00:08:30,000
in a completely different way

221
00:08:30,000 --> 00:08:32,000
and it was like it was playing with an alien

222
00:08:32,000 --> 00:08:34,000
because it didn't learn past games

223
00:08:36,000 --> 00:08:38,000
as you said it learns an entirely new way

224
00:08:38,000 --> 00:08:40,000
literally it played against itself

225
00:08:40,000 --> 00:08:42,000
it dreamt

226
00:08:42,000 --> 00:08:44,000
so then it plays against itself in its memory

227
00:08:44,000 --> 00:08:46,000
and then it does that again and again and again

228
00:08:46,000 --> 00:08:48,000
and so within just a few weeks

229
00:08:48,000 --> 00:08:50,000
it outperformed him

230
00:08:50,000 --> 00:08:52,000
and then these models got better and better

231
00:08:52,000 --> 00:08:54,000
and smaller and smaller, we haven't seen the generalization of the models yet

232
00:08:54,000 --> 00:08:56,000
but they're coming

233
00:08:56,000 --> 00:08:58,000
so you have to optimize when you don't even tell it the rules

234
00:08:58,000 --> 00:09:00,000
which is kind of insane

235
00:09:00,000 --> 00:09:02,000
then on the other hand

236
00:09:02,000 --> 00:09:04,000
you had what was known as foundation models

237
00:09:04,000 --> 00:09:06,000
which is where this attention based

238
00:09:06,000 --> 00:09:08,000
architecture came in

239
00:09:08,000 --> 00:09:10,000
so in the attention thing they were seeing when the games were

240
00:09:10,000 --> 00:09:12,000
actually a final thing about this

241
00:09:12,000 --> 00:09:14,000
and you can see this in the AlphaGo documentary

242
00:09:14,000 --> 00:09:16,000
that's on YouTube if you want to see more details

243
00:09:16,000 --> 00:09:18,000
is that as a result of this

244
00:09:18,000 --> 00:09:20,000
did Lisa doll go and say

245
00:09:20,000 --> 00:09:22,000
I'm hanging up my go pieces

246
00:09:22,000 --> 00:09:24,000
and I'm going to pack them away, no

247
00:09:24,000 --> 00:09:26,000
he started playing against it and now he's even better

248
00:09:26,000 --> 00:09:28,000
as a player

249
00:09:28,000 --> 00:09:30,000
and the entire go

250
00:09:30,000 --> 00:09:32,000
competitive scene has improved

251
00:09:32,000 --> 00:09:34,000
because now they're figuring out brand new ways

252
00:09:34,000 --> 00:09:36,000
and gambits and things they'd never seen before

253
00:09:36,000 --> 00:09:38,000
which I think is really interesting

254
00:09:38,000 --> 00:09:40,000
then you had this other big area

255
00:09:40,000 --> 00:09:42,000
which was kind of some of this deep learning area

256
00:09:42,000 --> 00:09:44,000
we have these big corpuses

257
00:09:44,000 --> 00:09:46,000
and like I said you can do extrapolations

258
00:09:46,000 --> 00:09:48,000
but you don't understand meaning

259
00:09:48,000 --> 00:09:50,000
and so this is where these attention based architectures came in

260
00:09:50,000 --> 00:09:52,000
it's actually quite interesting

261
00:09:52,000 --> 00:09:54,000
when I was taking a break

262
00:09:54,000 --> 00:09:56,000
from fund management to work with my son

263
00:09:56,000 --> 00:09:58,000
autism is a very interesting condition

264
00:09:58,000 --> 00:10:00,000
no cure, nothing to be done

265
00:10:00,000 --> 00:10:02,000
so of course you know you go and quit being a hedge fund manager

266
00:10:02,000 --> 00:10:04,000
and build a team and try to do it

267
00:10:04,000 --> 00:10:06,000
because you go maniacs right

268
00:10:06,000 --> 00:10:08,000
my wife is a applied behavioural

269
00:10:08,000 --> 00:10:10,000
analyst who just treats kids

270
00:10:10,000 --> 00:10:12,000
with autism

271
00:10:12,000 --> 00:10:14,000
so I lived that life

272
00:10:14,000 --> 00:10:16,000
exactly so applied behavioural

273
00:10:16,000 --> 00:10:18,000
analysis uses variable

274
00:10:18,000 --> 00:10:20,000
rewards and a lot of the stuff you use in video games

275
00:10:20,000 --> 00:10:22,000
to rebuild words

276
00:10:22,000 --> 00:10:24,000
so if you have a stroke or you have autism

277
00:10:24,000 --> 00:10:26,000
you haven't basically said can

278
00:10:26,000 --> 00:10:28,000
right so the word can can mean I can

279
00:10:28,000 --> 00:10:30,000
it can mean that can

280
00:10:30,000 --> 00:10:32,000
it can mean the can can

281
00:10:32,000 --> 00:10:34,000
lots of different things

282
00:10:34,000 --> 00:10:36,000
but because there's so much noise in the brain

283
00:10:36,000 --> 00:10:38,000
which is why you see a lot of kids and adults with autism

284
00:10:38,000 --> 00:10:40,000
not being able to handle large amounts of information

285
00:10:40,000 --> 00:10:42,000
I don't have aspergers myself and ADHD

286
00:10:42,000 --> 00:10:44,000
they usually balance out

287
00:10:44,000 --> 00:10:46,000
sometimes not quite

288
00:10:46,000 --> 00:10:48,000
it's very difficult to pay attention

289
00:10:48,000 --> 00:10:50,000
to how you feel that

290
00:10:50,000 --> 00:10:52,000
like you know when your leg is tapping

291
00:10:52,000 --> 00:10:54,000
there's just too much going on

292
00:10:54,000 --> 00:10:56,000
that's the fugue state

293
00:10:56,000 --> 00:10:58,000
because there's two transmitters in the brain

294
00:10:58,000 --> 00:11:00,000
GABA which calms you down

295
00:11:00,000 --> 00:11:02,000
and GLUTAMATE which excites you

296
00:11:02,000 --> 00:11:04,000
so when you pop a valiant the GABA levels go up

297
00:11:04,000 --> 00:11:06,000
and imagine if your brain was always excited

298
00:11:06,000 --> 00:11:08,000
for one reason or another

299
00:11:08,000 --> 00:11:10,000
you wouldn't be able to concentrate

300
00:11:10,000 --> 00:11:12,000
and build those connections to the word can

301
00:11:12,000 --> 00:11:14,000
you know that hidden layer of meaning

302
00:11:14,000 --> 00:11:16,000
for what can is

303
00:11:16,000 --> 00:11:18,000
what can actually cause it

304
00:11:18,000 --> 00:11:20,000
but because medicine treats everyone the same

305
00:11:20,000 --> 00:11:22,000
it meant that one drug that made 34% of kids better

306
00:11:22,000 --> 00:11:24,000
made 28% of kids worse

307
00:11:24,000 --> 00:11:26,000
so there's no cure and there's no treatment

308
00:11:26,000 --> 00:11:28,000
so that's a different bigger story

309
00:11:28,000 --> 00:11:30,000
and something again this AI can help with

310
00:11:30,000 --> 00:11:32,000
and why is that relevant

311
00:11:32,000 --> 00:11:34,000
it's because what this AI does now

312
00:11:34,000 --> 00:11:36,000
is exactly the same

313
00:11:36,000 --> 00:11:38,000
it uses giant honking supercomputers

314
00:11:38,000 --> 00:11:40,000
so that if you take a terabyte of text

315
00:11:40,000 --> 00:11:42,000
it learns what the most important words

316
00:11:42,000 --> 00:11:44,000
in the sentence are

317
00:11:44,000 --> 00:11:46,000
it took

318
00:11:46,000 --> 00:11:48,000
a thousand gigabytes of text

319
00:11:48,000 --> 00:11:50,000
and it learned how to write in any style

320
00:11:50,000 --> 00:11:52,000
and any extension

321
00:11:52,000 --> 00:11:54,000
so it paid the most important part of any sentence

322
00:11:54,000 --> 00:11:56,000
so you give it a sentence like legolas and gimli

323
00:11:56,000 --> 00:11:58,000
that's all you give it and it'll write an entire scene

324
00:11:58,000 --> 00:12:00,000
in the style of Tolkien

325
00:12:00,000 --> 00:12:02,000
that's never been seen before

326
00:12:02,000 --> 00:12:04,000
you know you give it a sentence

327
00:12:04,000 --> 00:12:06,000
and you say make it happier

328
00:12:06,000 --> 00:12:08,000
it understands what happier is

329
00:12:08,000 --> 00:12:10,000
and this is what's called a latent space of meaning

330
00:12:10,000 --> 00:12:12,000
because it takes

331
00:12:12,000 --> 00:12:14,000
that took one terabyte

332
00:12:14,000 --> 00:12:16,000
of information so a thousand gigabytes

333
00:12:16,000 --> 00:12:18,000
and GPT-3 is about

334
00:12:18,000 --> 00:12:20,000
40 gigabytes in size

335
00:12:20,000 --> 00:12:22,000
but it can recreate any style of writing

336
00:12:22,000 --> 00:12:24,000
an understanding style of writing

337
00:12:24,000 --> 00:12:26,000
which is again a bit crazy

338
00:12:26,000 --> 00:12:28,000
I mean it's

339
00:12:28,000 --> 00:12:30,000
it's staggering and we'll come into the societal impact

340
00:12:30,000 --> 00:12:32,000
on what this all means in a bit

341
00:12:32,000 --> 00:12:34,000
but just want to get through the technology

342
00:12:34,000 --> 00:12:36,000
it is astonishing when you see GPT-3

343
00:12:36,000 --> 00:12:38,000
because basically it writes authentic

344
00:12:38,000 --> 00:12:40,000
pieces

345
00:12:40,000 --> 00:12:42,000
that are not stolen from other things

346
00:12:42,000 --> 00:12:44,000
much like

347
00:12:44,000 --> 00:12:46,000
the deep

348
00:12:46,000 --> 00:12:48,000
deep mind did with go

349
00:12:48,000 --> 00:12:50,000
it wasn't stealing from old games

350
00:12:50,000 --> 00:12:52,000
it learnt how to do it

351
00:12:52,000 --> 00:12:54,000
is that right?

352
00:12:54,000 --> 00:12:56,000
it learnt principles, it learnt stars

353
00:12:56,000 --> 00:12:58,000
it learnt what's called a latent space

354
00:12:58,000 --> 00:13:00,000
the hidden layers of meanings

355
00:13:00,000 --> 00:13:02,000
between different types of words

356
00:13:02,000 --> 00:13:04,000
so like we just released

357
00:13:04,000 --> 00:13:06,000
the most advanced image version of that

358
00:13:06,000 --> 00:13:08,000
stable diffusion

359
00:13:08,000 --> 00:13:10,000
stable diffusion took

360
00:13:10,000 --> 00:13:12,000
100,000 gigabytes

361
00:13:12,000 --> 00:13:14,000
so 100 terabytes of images

362
00:13:14,000 --> 00:13:16,000
and we made a 2 gigabyte file

363
00:13:16,000 --> 00:13:18,000
that can do any style

364
00:13:18,000 --> 00:13:20,000
and any image of any type

365
00:13:20,000 --> 00:13:22,000
so this is like Dali

366
00:13:22,000 --> 00:13:24,000
right?

367
00:13:24,000 --> 00:13:26,000
it's Dali on steroids

368
00:13:38,000 --> 00:13:40,000
so subscribe now to the channel

369
00:13:40,000 --> 00:13:42,000
and never miss an update

370
00:13:42,000 --> 00:13:44,000
there is simply too much going on

371
00:13:44,000 --> 00:13:46,000
so subscribe now, thank you

372
00:13:46,000 --> 00:13:48,000
so

373
00:13:48,000 --> 00:13:50,000
explain Dali and then we'll come into what you're doing

374
00:13:50,000 --> 00:13:52,000
because again

375
00:13:52,000 --> 00:13:54,000
I just want to get people up the knowledge graph

376
00:13:54,000 --> 00:13:56,000
because Dali was the next big one that I stopped

377
00:13:56,000 --> 00:13:58,000
in my tracks and went holy shit this is

378
00:13:58,000 --> 00:14:00,000
amazing

379
00:14:00,000 --> 00:14:02,000
so GPT-3 came out in 2020

380
00:14:02,000 --> 00:14:04,000
and then we released

381
00:14:04,000 --> 00:14:06,000
the open source version of that GPT-Mia

382
00:14:06,000 --> 00:14:08,000
which I'll convert to

383
00:14:08,000 --> 00:14:10,000
last year there was a breakthrough in image generation

384
00:14:10,000 --> 00:14:12,000
so you can generate text and everyone was like

385
00:14:12,000 --> 00:14:14,000
image is too difficult right?

386
00:14:14,000 --> 00:14:16,000
because a picture paints a thousand words

387
00:14:16,000 --> 00:14:18,000
like it's really going to be a thousand kinds of complex

388
00:14:18,000 --> 00:14:20,000
turns out it wasn't

389
00:14:20,000 --> 00:14:22,000
so you had something called clip

390
00:14:22,000 --> 00:14:24,000
which basically you took any image

391
00:14:24,000 --> 00:14:26,000
and it would be able to classify it

392
00:14:26,000 --> 00:14:28,000
so it would be able to say that's a Raoul

393
00:14:28,000 --> 00:14:30,000
and that's a red sofa and all these things

394
00:14:30,000 --> 00:14:32,000
understand loads of concepts because they compress that down

395
00:14:32,000 --> 00:14:34,000
then you had a generative model that took words to images

396
00:14:34,000 --> 00:14:36,000
so you had two models, a word to image model

397
00:14:36,000 --> 00:14:38,000
and image to word model

398
00:14:38,000 --> 00:14:40,000
and at the start of last year we figured out how to bounce them off each other

399
00:14:40,000 --> 00:14:42,000
so it would generate an image

400
00:14:42,000 --> 00:14:44,000
and it would check if that image was the same as the prompt

401
00:14:44,000 --> 00:14:46,000
and it would go back and forth, back and forth

402
00:14:46,000 --> 00:14:48,000
and after 10 minutes you'd generate

403
00:14:48,000 --> 00:14:50,000
a semulchrum of an image

404
00:14:50,000 --> 00:14:52,000
from the word input so words go in and images come out

405
00:14:52,000 --> 00:14:54,000
and you're like oh crap

406
00:14:54,000 --> 00:14:56,000
it looks a bit mushy

407
00:14:56,000 --> 00:14:58,000
but that's insane, it can generate an image of anything

408
00:14:58,000 --> 00:15:00,000
but it isn't high quality

409
00:15:00,000 --> 00:15:02,000
then what happened is that

410
00:15:02,000 --> 00:15:04,000
OpenAI and us accelerated that whole space

411
00:15:04,000 --> 00:15:06,000
because we thought it was freaking cool

412
00:15:06,000 --> 00:15:08,000
and put lots of research into it

413
00:15:08,000 --> 00:15:10,000
and so you've got to the point now whereby

414
00:15:10,000 --> 00:15:12,000
you can put in Robert De Niro's Gandalf

415
00:15:12,000 --> 00:15:14,000
and one second later

416
00:15:14,000 --> 00:15:16,000
you get an image output

417
00:15:16,000 --> 00:15:18,000
so the first output of that was

418
00:15:18,000 --> 00:15:20,000
that hit the mainstream was Dali from OpenAI

419
00:15:20,000 --> 00:15:22,000
that was in April of this year

420
00:15:22,000 --> 00:15:24,000
so you can say

421
00:15:24,000 --> 00:15:26,000
a cybercone, goth girl

422
00:15:26,000 --> 00:15:28,000
overlooking Neo Tokyo

423
00:15:28,000 --> 00:15:30,000
and boom, 8 seconds later it's generated

424
00:15:30,000 --> 00:15:32,000
and this is a totally unique

425
00:15:32,000 --> 00:15:34,000
creative image that did not exist

426
00:15:34,000 --> 00:15:36,000
in the world before, right?

427
00:15:36,000 --> 00:15:38,000
it did not exist in the world before

428
00:15:38,000 --> 00:15:40,000
all the data that went into that

429
00:15:40,000 --> 00:15:42,000
so it was about 600 million images

430
00:15:42,000 --> 00:15:44,000
it can't recreate any of those images

431
00:15:44,000 --> 00:15:46,000
instead it's learned the

432
00:15:46,000 --> 00:15:48,000
principles of that so again it's

433
00:15:48,000 --> 00:15:50,000
the principle based analysis

434
00:15:50,000 --> 00:15:52,000
which is insane, right?

435
00:15:52,000 --> 00:15:54,000
because it's like

436
00:15:54,000 --> 00:15:56,000
again it's the heuristic stuff that we do all the time

437
00:15:56,000 --> 00:15:58,000
and they combine different concepts

438
00:15:58,000 --> 00:16:00,000
you can say a Van Gogh

439
00:16:00,000 --> 00:16:02,000
by Banksy

440
00:16:02,000 --> 00:16:04,000
and it will do a Van Gogh

441
00:16:04,000 --> 00:16:06,000
by Banksy or you can kind of do a screen

442
00:16:06,000 --> 00:16:08,000
and now there's more and more technologies

443
00:16:08,000 --> 00:16:10,000
that have been measured from that

444
00:16:10,000 --> 00:16:12,000
so OpenAI kind of announced the closed beta

445
00:16:12,000 --> 00:16:14,000
of that and then

446
00:16:14,000 --> 00:16:16,000
my company Stability AI

447
00:16:16,000 --> 00:16:18,000
created a version, well it's 30 times faster

448
00:16:18,000 --> 00:16:20,000
and more powerful

449
00:16:20,000 --> 00:16:22,000
so tell me about Stability AI

450
00:16:22,000 --> 00:16:24,000
when did it start, who's involved

451
00:16:24,000 --> 00:16:26,000
what are you guys doing?

452
00:16:26,000 --> 00:16:28,000
yeah, so starting in 2020

453
00:16:28,000 --> 00:16:30,000
we're technically starting in 2019

454
00:16:30,000 --> 00:16:32,000
with the education work

455
00:16:32,000 --> 00:16:34,000
but I can talk about that later

456
00:16:34,000 --> 00:16:36,000
but 2020 is when it was formed officially

457
00:16:36,000 --> 00:16:38,000
to do the COVID work

458
00:16:38,000 --> 00:16:40,000
so it's lead the UN COVID initiative

459
00:16:40,000 --> 00:16:42,000
launched at Stanford etc

460
00:16:42,000 --> 00:16:44,000
I founded it, yeah

461
00:16:44,000 --> 00:16:46,000
because

462
00:16:46,000 --> 00:16:48,000
someone needs to build something

463
00:16:48,000 --> 00:16:50,000
to do AI as a public good

464
00:16:50,000 --> 00:16:52,000
so I didn't exactly know what the business model was

465
00:16:52,000 --> 00:16:54,000
or anything like that then, I just thought

466
00:16:54,000 --> 00:16:56,000
people are dying effectively

467
00:16:56,000 --> 00:16:58,000
and then we have the other products, actually I'll mention it

468
00:16:58,000 --> 00:17:00,000
so my co-founder and I

469
00:17:00,000 --> 00:17:02,000
so Joe's runs the charitable arm

470
00:17:02,000 --> 00:17:04,000
we have took the global X prize

471
00:17:04,000 --> 00:17:06,000
for learning so that was a $15 million

472
00:17:06,000 --> 00:17:08,000
prize funded by Elon Musk and Tony Robbins

473
00:17:08,000 --> 00:17:10,000
for the first app that could teach

474
00:17:10,000 --> 00:17:12,000
literacy and numeracy in 18 months

475
00:17:12,000 --> 00:17:14,000
for that internet and we've been deploying

476
00:17:14,000 --> 00:17:16,000
in refugee camps and low income areas around the world

477
00:17:16,000 --> 00:17:18,000
to the point where now

478
00:17:18,000 --> 00:17:20,000
we are teaching kids literacy

479
00:17:20,000 --> 00:17:22,000
and numeracy in 13 months and one hour a day

480
00:17:22,000 --> 00:17:24,000
in a refugee camp in Malawi

481
00:17:24,000 --> 00:17:26,000
and we just got the remit

482
00:17:26,000 --> 00:17:28,000
to educate every child in Malawi, 3.9 million kids

483
00:17:28,000 --> 00:17:30,000
with a completely open

484
00:17:30,000 --> 00:17:32,000
book of hardware, software, deployment and curriculum

485
00:17:32,000 --> 00:17:34,000
so we're going to invite the world to say

486
00:17:34,000 --> 00:17:36,000
let's build an open source education system

487
00:17:36,000 --> 00:17:38,000
where the AI teaches the kids and the kids teach the AI

488
00:17:38,000 --> 00:17:40,000
that's the type of stuff that we wanted to do

489
00:17:40,000 --> 00:17:42,000
because so much of the world's issues

490
00:17:42,000 --> 00:17:44,000
are coordination problems

491
00:17:44,000 --> 00:17:46,000
so I figured out how to bring in the multilaterals

492
00:17:46,000 --> 00:17:48,000
the locals and others like the simple mechanism design

493
00:17:48,000 --> 00:17:50,000
and so let's do education

494
00:17:50,000 --> 00:17:52,000
let's do healthcare but then as we were doing this

495
00:17:52,000 --> 00:17:54,000
and we talked to a lot of the big private companies

496
00:17:54,000 --> 00:17:56,000
they promised a lot and they didn't deliver

497
00:17:56,000 --> 00:17:58,000
and it's called me thinking about the future

498
00:17:58,000 --> 00:18:00,000
so some of the people on this call

499
00:18:00,000 --> 00:18:02,000
will be familiar with

500
00:18:02,000 --> 00:18:04,000
the Chinese social credit score system

501
00:18:04,000 --> 00:18:06,000
and the use of AI in China

502
00:18:06,000 --> 00:18:08,000
to identify Ugears and others

503
00:18:08,000 --> 00:18:10,000
and everyone's got a rating

504
00:18:10,000 --> 00:18:12,000
you hang out with someone with too low a rating

505
00:18:12,000 --> 00:18:14,000
your rating starts to go down

506
00:18:14,000 --> 00:18:16,000
8 million people in China can't use planes or trains

507
00:18:16,000 --> 00:18:18,000
it's behavioural economics right

508
00:18:18,000 --> 00:18:20,000
done by AI

509
00:18:20,000 --> 00:18:22,000
it's gamified life

510
00:18:22,000 --> 00:18:24,000
life has become a video game

511
00:18:24,000 --> 00:18:26,000
and AI is the extra system on top

512
00:18:26,000 --> 00:18:28,000
but now you think about it

513
00:18:28,000 --> 00:18:30,000
the entire web 2

514
00:18:30,000 --> 00:18:32,000
was basically Facebook and Google

515
00:18:32,000 --> 00:18:34,000
it was AI

516
00:18:34,000 --> 00:18:36,000
big data models

517
00:18:36,000 --> 00:18:38,000
and now you've moved to the big model era

518
00:18:38,000 --> 00:18:40,000
where to build these models you need to have the best data

519
00:18:40,000 --> 00:18:42,000
amazingly smart people

520
00:18:42,000 --> 00:18:44,000
and fricking supercomputers

521
00:18:44,000 --> 00:18:46,000
why fricking supercomputers I mean

522
00:18:46,000 --> 00:18:48,000
is that a multiple times faster than NASA

523
00:18:48,000 --> 00:18:50,000
supercomputers because that's what compresses

524
00:18:50,000 --> 00:18:52,000
this data down into knowledge

525
00:18:52,000 --> 00:18:54,000
so information to knowledge on this

526
00:18:54,000 --> 00:18:56,000
because knowledge is when you extract the principles

527
00:18:56,000 --> 00:18:58,000
from information right

528
00:18:58,000 --> 00:19:00,000
and then wisdom is when you apply it to today

529
00:19:00,000 --> 00:19:02,000
so we have this some knowledge extraction system

530
00:19:02,000 --> 00:19:04,000
so I looked at that and I was like

531
00:19:04,000 --> 00:19:06,000
the only people that can build this are NVIDIA

532
00:19:06,000 --> 00:19:08,000
Meta, Google

533
00:19:08,000 --> 00:19:10,000
slash DeepMind, Microsoft

534
00:19:10,000 --> 00:19:12,000
slash OpenAI

535
00:19:12,000 --> 00:19:14,000
and how are they aligned

536
00:19:14,000 --> 00:19:16,000
because this is super powerful technology

537
00:19:16,000 --> 00:19:18,000
they can be used to target us ads better than anything else

538
00:19:18,000 --> 00:19:20,000
or it can be used to change our mind because it's convincing

539
00:19:20,000 --> 00:19:22,000
you see GPT-3 writing

540
00:19:22,000 --> 00:19:24,000
you can't tell it's not a human

541
00:19:24,000 --> 00:19:26,000
right

542
00:19:26,000 --> 00:19:28,000
you look at the audio version you can't listen

543
00:19:28,000 --> 00:19:30,000
say it's not human like my sister-in-law

544
00:19:30,000 --> 00:19:32,000
Zeena just sold her company

545
00:19:32,000 --> 00:19:34,000
to Spotify, it's an Antik

546
00:19:34,000 --> 00:19:36,000
fully emotionally real AI voices

547
00:19:36,000 --> 00:19:38,000
using this technology

548
00:19:38,000 --> 00:19:40,000
so she did the technology for all the

549
00:19:40,000 --> 00:19:42,000
Blizzard and everything

550
00:19:42,000 --> 00:19:44,000
and then somebody has now got AI

551
00:19:44,000 --> 00:19:46,000
for creating podcasts

552
00:19:46,000 --> 00:19:48,000
so you can have Joe Rogan interviewing

553
00:19:48,000 --> 00:19:50,000
Steve Jobs

554
00:19:50,000 --> 00:19:52,000
neither of these exist and the interview never existed

555
00:19:52,000 --> 00:19:54,000
but the AI does it

556
00:19:54,000 --> 00:19:56,000
and it sounds just like them right

557
00:19:56,000 --> 00:19:58,000
and this is the thing it's got that human level

558
00:19:58,000 --> 00:20:00,000
so like Sanantik also did

559
00:20:00,000 --> 00:20:02,000
Bal Kilmer's voice for his documentary

560
00:20:02,000 --> 00:20:04,000
in Top Gun because he lost it completely

561
00:20:04,000 --> 00:20:06,000
and it's completely emotionally real

562
00:20:06,000 --> 00:20:08,000
so it's getting to that period of emotional realness

563
00:20:08,000 --> 00:20:10,000
and human realness

564
00:20:10,000 --> 00:20:12,000
which means then you think about the future and you're like

565
00:20:12,000 --> 00:20:14,000
this is one of the most powerful technologies

566
00:20:14,000 --> 00:20:16,000
ever because it's approaching humanity

567
00:20:16,000 --> 00:20:18,000
not in a generalized sense

568
00:20:18,000 --> 00:20:20,000
where the AI can do everything in its sky net

569
00:20:20,000 --> 00:20:22,000
but as a tool

570
00:20:22,000 --> 00:20:24,000
for targeting and convincing

571
00:20:24,000 --> 00:20:26,000
and manipulation

572
00:20:26,000 --> 00:20:28,000
so what happens if only big tech companies

573
00:20:28,000 --> 00:20:30,000
do this because they went from being open

574
00:20:30,000 --> 00:20:32,000
to being private even open AI

575
00:20:32,000 --> 00:20:34,000
that got a billion dollars from Musk

576
00:20:34,000 --> 00:20:36,000
and others and then a billion from Microsoft

577
00:20:36,000 --> 00:20:38,000
stop releasing their code

578
00:20:38,000 --> 00:20:40,000
so people can have this

579
00:20:40,000 --> 00:20:42,000
so they turn closed

580
00:20:42,000 --> 00:20:44,000
and then they stop sharing

581
00:20:44,000 --> 00:20:46,000
and you're like as this is the most powerful thing

582
00:20:46,000 --> 00:20:48,000
if you fast forward 5-10 years

583
00:20:48,000 --> 00:20:50,000
does it make sense that a private company will have a monopoly

584
00:20:50,000 --> 00:20:52,000
or a series of private companies will have a monopoly on this

585
00:20:52,000 --> 00:20:54,000
then it will reflect

586
00:20:54,000 --> 00:20:56,000
basically Palo Alto norms

587
00:20:56,000 --> 00:20:58,000
and it will mean those companies are more powerful

588
00:20:58,000 --> 00:21:00,000
than any government in the earth

589
00:21:00,000 --> 00:21:02,000
and I was like that's wrong

590
00:21:02,000 --> 00:21:04,000
I was like there needs to be

591
00:21:04,000 --> 00:21:06,000
an alternative that is open

592
00:21:06,000 --> 00:21:08,000
you know kind of AI for the people by the people we call it

593
00:21:08,000 --> 00:21:10,000
because

594
00:21:10,000 --> 00:21:12,000
not only is that morally right

595
00:21:12,000 --> 00:21:14,000
because it's unethical to keep powerful technology

596
00:21:14,000 --> 00:21:16,000
that can make people's lives better

597
00:21:16,000 --> 00:21:18,000
from the people that can benefit from it the most

598
00:21:18,000 --> 00:21:20,000
but it's actually a better model

599
00:21:20,000 --> 00:21:22,000
because you have open innovation

600
00:21:22,000 --> 00:21:24,000
this is why Linux became the servers of choice

601
00:21:24,000 --> 00:21:26,000
and the most secure servers

602
00:21:26,000 --> 00:21:28,000
Windows has all these holes in

603
00:21:28,000 --> 00:21:30,000
Linux holes get patched instantly right

604
00:21:30,000 --> 00:21:32,000
this is where you've got databases kind of taking over

605
00:21:32,000 --> 00:21:34,000
because nothing can beat human creativity when that happened

606
00:21:34,000 --> 00:21:36,000
I look at web3 and crypto

607
00:21:36,000 --> 00:21:38,000
and I'm like it nearly got there

608
00:21:38,000 --> 00:21:40,000
and then it kind of got hijacked

609
00:21:40,000 --> 00:21:42,000
because it was part of the puzzle

610
00:21:42,000 --> 00:21:44,000
but it wasn't the whole puzzle

611
00:21:44,000 --> 00:21:46,000
because when I went to talk to a lot of web3 people

612
00:21:46,000 --> 00:21:48,000
I'm like where's the AI in web3

613
00:21:48,000 --> 00:21:50,000
they were like well we'll get to it

614
00:21:50,000 --> 00:21:52,000
but I was like the whole of web2 was AI

615
00:21:52,000 --> 00:21:54,000
how are you gonna do web3 without AI

616
00:21:54,000 --> 00:21:56,000
and when they gave examples like Alephia

617
00:21:56,000 --> 00:21:58,000
and other things

618
00:21:58,000 --> 00:22:00,000
with the talking NFTs

619
00:22:00,000 --> 00:22:02,000
the technology they used was our technology

620
00:22:02,000 --> 00:22:04,000
and we released

621
00:22:04,000 --> 00:22:06,000
but usgpt near

622
00:22:06,000 --> 00:22:08,000
which I thought was quite interesting

623
00:22:08,000 --> 00:22:10,000
so then I realised it's actually a bigger thing here

624
00:22:10,000 --> 00:22:12,000
and a bigger reason to do this

625
00:22:12,000 --> 00:22:14,000
but how do you compete with open AI

626
00:22:14,000 --> 00:22:16,000
and deep mines and Microsoft and Google

627
00:22:16,000 --> 00:22:18,000
with their billions of dollars of budget

628
00:22:20,000 --> 00:22:22,000
which was an interesting one

629
00:22:22,000 --> 00:22:24,000
how do you do it right

630
00:22:24,000 --> 00:22:26,000
do you go and raise two billion dollars

631
00:22:26,000 --> 00:22:28,000
no go do it another way

632
00:22:28,000 --> 00:22:30,000
so what did you do

633
00:22:30,000 --> 00:22:32,000
so I built community

634
00:22:32,000 --> 00:22:34,000
so in 2020 a lot of people

635
00:22:34,000 --> 00:22:36,000
were just like you know COVID

636
00:22:36,000 --> 00:22:38,000
and in quarantine

637
00:22:38,000 --> 00:22:40,000
so we built up a Luther AI

638
00:22:40,000 --> 00:22:42,000
which the base principle was

639
00:22:42,000 --> 00:22:44,000
let's create an open source version of GPT3

640
00:22:44,000 --> 00:22:46,000
you know

641
00:22:46,000 --> 00:22:48,000
did you create it

642
00:22:48,000 --> 00:22:50,000
or clone GPT3

643
00:22:50,000 --> 00:22:52,000
and then allowed to build on top

644
00:22:52,000 --> 00:22:54,000
how do you start this

645
00:22:54,000 --> 00:22:56,000
so like there were five initial creators

646
00:22:56,000 --> 00:22:58,000
and then I came like a month after

647
00:22:58,000 --> 00:23:00,000
and we were working with some of the second wave

648
00:23:00,000 --> 00:23:02,000
and said let's accelerate this up

649
00:23:02,000 --> 00:23:04,000
so they looked at the model so the data wasn't open

650
00:23:04,000 --> 00:23:06,000
so we had to create our own data set

651
00:23:06,000 --> 00:23:08,000
and so we had to crawl the internet

652
00:23:08,000 --> 00:23:10,000
and create a terabyte of data

653
00:23:10,000 --> 00:23:12,000
the compute wasn't available

654
00:23:12,000 --> 00:23:14,000
so we got a grant from Google

655
00:23:14,000 --> 00:23:16,000
who provided the compute initially

656
00:23:16,000 --> 00:23:18,000
and then the expertise

657
00:23:18,000 --> 00:23:20,000
it was all like PhD students

658
00:23:20,000 --> 00:23:22,000
or self-taught programmers

659
00:23:22,000 --> 00:23:24,000
and others were like how would this work

660
00:23:24,000 --> 00:23:26,000
because they released an academic paper

661
00:23:26,000 --> 00:23:28,000
from scratch and then created a model

662
00:23:28,000 --> 00:23:30,000
that was 75% as good

663
00:23:30,000 --> 00:23:32,000
but it was available

664
00:23:32,000 --> 00:23:34,000
so the models from a Luther AI

665
00:23:34,000 --> 00:23:36,000
which kind of stability runs now

666
00:23:36,000 --> 00:23:38,000
basically via the top level

667
00:23:38,000 --> 00:23:40,000
people but we're going to spin it off

668
00:23:40,000 --> 00:23:42,000
into its own independent charity

669
00:23:42,000 --> 00:23:44,000
for a number of reasons

670
00:23:44,000 --> 00:23:46,000
they've been downloaded 25 million times

671
00:23:46,000 --> 00:23:48,000
now by developers

672
00:23:48,000 --> 00:23:50,000
so anytime you see a chat bot

673
00:23:50,000 --> 00:23:52,000
or something like that that's getting to human levels

674
00:23:52,000 --> 00:23:54,000
it's probably going to be that

675
00:23:54,000 --> 00:23:56,000
it's 80% as good as GPT-3

676
00:23:56,000 --> 00:23:58,000
but it doesn't matter because you can customize it

677
00:23:58,000 --> 00:24:00,000
and you can extend it

678
00:24:00,000 --> 00:24:02,000
and you can run it on your own hardware

679
00:24:02,000 --> 00:24:04,000
so it's a 20 billion parameter model

680
00:24:04,000 --> 00:24:06,000
versus 175 billion parameter model

681
00:24:06,000 --> 00:24:08,000
it's eight times smaller

682
00:24:08,000 --> 00:24:10,000
but you know it's just like the big steel mills

683
00:24:10,000 --> 00:24:12,000
and the little ones if you look at the kind of Clayton Christian

684
00:24:12,000 --> 00:24:14,000
small can outcompete big

685
00:24:14,000 --> 00:24:16,000
and then I realized that there's a talent arbitrage here

686
00:24:16,000 --> 00:24:18,000
whereby there's a lot of people

687
00:24:18,000 --> 00:24:20,000
that want to build open source

688
00:24:20,000 --> 00:24:22,000
but there's no super compute

689
00:24:22,000 --> 00:24:24,000
in academia so when I said super compute

690
00:24:24,000 --> 00:24:26,000
needed for this I meant it

691
00:24:26,000 --> 00:24:28,000
like the

692
00:24:28,000 --> 00:24:30,000
amount of super compute

693
00:24:30,000 --> 00:24:32,000
the amount of breakthroughs in this foundational

694
00:24:32,000 --> 00:24:34,000
model research

695
00:24:34,000 --> 00:24:36,000
20 years ago 100% came from academia

696
00:24:36,000 --> 00:24:38,000
10 years ago 75%

697
00:24:38,000 --> 00:24:40,000
came from academia last year

698
00:24:40,000 --> 00:24:42,000
0% came from academia

699
00:24:42,000 --> 00:24:44,000
because there was this

700
00:24:44,000 --> 00:24:46,000
massive ramp-up in compute capabilities

701
00:24:46,000 --> 00:24:48,000
but academia couldn't access it

702
00:24:48,000 --> 00:24:50,000
only private companies could

703
00:24:50,000 --> 00:24:52,000
so if you wanted to have a breakthrough

704
00:24:52,000 --> 00:24:54,000
or work in cutting edge research you had three options

705
00:24:54,000 --> 00:24:56,000
do a start up, start up suck

706
00:24:56,000 --> 00:24:58,000
put that to the side, especially for academics

707
00:24:58,000 --> 00:25:00,000
number two you go and work for big tech

708
00:25:00,000 --> 00:25:02,000
and then you get 59 page NDAs

709
00:25:02,000 --> 00:25:04,000
and you might do cool stuff

710
00:25:04,000 --> 00:25:06,000
like we've seen text to video from the Google team

711
00:25:06,000 --> 00:25:08,000
that'll never be released by Google

712
00:25:08,000 --> 00:25:10,000
because of ethical fears which we can discuss

713
00:25:10,000 --> 00:25:12,000
and the final option is

714
00:25:12,000 --> 00:25:14,000
you go and work for some of these independent labs

715
00:25:14,000 --> 00:25:16,000
like OpenAI and others that were meant to be independent

716
00:25:16,000 --> 00:25:18,000
and then became less independent over time

717
00:25:18,000 --> 00:25:20,000
otherwise you stay in academia and you're like

718
00:25:20,000 --> 00:25:22,000
I would like to do these but I can't

719
00:25:22,000 --> 00:25:24,000
so I saw that the core

720
00:25:24,000 --> 00:25:26,000
choke point was compute

721
00:25:26,000 --> 00:25:28,000
a community was forming for talent

722
00:25:28,000 --> 00:25:30,000
and then we need to be highly structured about data

723
00:25:30,000 --> 00:25:32,000
so what I did was

724
00:25:32,000 --> 00:25:34,000
I said how big a super computer can I build

725
00:25:34,000 --> 00:25:36,000
if I put all my money into it

726
00:25:36,000 --> 00:25:38,000
and I'd become very persuasive

727
00:25:38,000 --> 00:25:40,000
and then I built the 10th fastest public

728
00:25:40,000 --> 00:25:42,000
super computer in the world in four months

729
00:25:44,000 --> 00:25:46,000
and it turns out you can build a very large super computer

730
00:25:46,000 --> 00:25:48,000
so Ezra 1 which is our super computer

731
00:25:48,000 --> 00:25:50,000
which all my money went into

732
00:25:50,000 --> 00:25:52,000
is about

733
00:25:52,000 --> 00:25:54,000
eight times faster than the fastest

734
00:25:54,000 --> 00:25:56,000
super computer in the UK

735
00:25:56,000 --> 00:25:58,000
and about seven times faster than all of NASA's

736
00:25:58,000 --> 00:26:00,000
super computers put together

737
00:26:00,000 --> 00:26:02,000
and that's the level that you need

738
00:26:02,000 --> 00:26:04,000
to basically create these models

739
00:26:04,000 --> 00:26:06,000
it's like the entry level

740
00:26:06,000 --> 00:26:08,000
that need for compute keeps going up

741
00:26:08,000 --> 00:26:10,000
can you use distributed computing power

742
00:26:10,000 --> 00:26:12,000
or is it still not fast enough

743
00:26:12,000 --> 00:26:14,000
so

744
00:26:14,000 --> 00:26:16,000
you can do it once you've done the initial model

745
00:26:16,000 --> 00:26:18,000
so what happens is that you take

746
00:26:18,000 --> 00:26:20,000
in this case we took 100,000 gigabytes of data

747
00:26:20,000 --> 00:26:22,000
so we made the biggest image data set in the world

748
00:26:22,000 --> 00:26:24,000
previously the largest was 100 million

749
00:26:24,000 --> 00:26:26,000
then we made 400 million

750
00:26:26,000 --> 00:26:28,000
then we made 5.6 billion

751
00:26:28,000 --> 00:26:30,000
a 250 terabyte image

752
00:26:30,000 --> 00:26:32,000
label pair data set

753
00:26:32,000 --> 00:26:34,000
then we took 2 billion of that

754
00:26:34,000 --> 00:26:36,000
which is the high quality images and then we crunched them back and forth on this

755
00:26:36,000 --> 00:26:38,000
so what happens is after you crunched

756
00:26:38,000 --> 00:26:40,000
them back and forth then you've got a file

757
00:26:40,000 --> 00:26:42,000
that can be adapted and trained

758
00:26:42,000 --> 00:26:44,000
basically it's gone through

759
00:26:44,000 --> 00:26:46,000
primary school

760
00:26:46,000 --> 00:26:48,000
of learning

761
00:26:48,000 --> 00:26:50,000
and then you can teach it more advanced concepts

762
00:26:50,000 --> 00:26:52,000
so like some people are taking the training on Japanese

763
00:26:52,000 --> 00:26:54,000
concepts, some people are taking

764
00:26:54,000 --> 00:26:56,000
anime cat girl, waifus or whatever

765
00:26:56,000 --> 00:26:58,000
you can add that specialization

766
00:26:58,000 --> 00:27:00,000
once it's achieved kind of the high school level

767
00:27:00,000 --> 00:27:02,000
so there's a big compute

768
00:27:02,000 --> 00:27:04,000
and a little computer

769
00:27:04,000 --> 00:27:06,000
does the network then learn from all of these people

770
00:27:06,000 --> 00:27:08,000
working on the network itself

771
00:27:08,000 --> 00:27:10,000
no

772
00:27:10,000 --> 00:27:12,000
it's distributed and you've got lots of people doing different things

773
00:27:12,000 --> 00:27:14,000
does that increase the knowledge base

774
00:27:14,000 --> 00:27:16,000
or not

775
00:27:16,000 --> 00:27:18,000
it increases the large base of the community

776
00:27:18,000 --> 00:27:20,000
but we have to understand this model

777
00:27:20,000 --> 00:27:22,000
it's not a distributed model where it's like lots of little

778
00:27:22,000 --> 00:27:24,000
brain cells out there

779
00:27:24,000 --> 00:27:26,000
it's a 2

780
00:27:26,000 --> 00:27:28,000
gigabyte file

781
00:27:28,000 --> 00:27:30,000
that can recreate

782
00:27:30,000 --> 00:27:32,000
and create any image in any style

783
00:27:32,000 --> 00:27:34,000
so you took 100,000 gigabytes of image and created

784
00:27:34,000 --> 00:27:36,000
so sometimes I think

785
00:27:36,000 --> 00:27:38,000
I'm on that Silicon Valley show

786
00:27:38,000 --> 00:27:40,000
and we're from HBO

787
00:27:40,000 --> 00:27:42,000
and we're Pied Piper and I think

788
00:27:42,000 --> 00:27:44,000
crap, am I Erlich Backman or Ross Hanneman

789
00:27:44,000 --> 00:27:46,000
you know

790
00:27:46,000 --> 00:27:48,000
it's one of those kind of guys right

791
00:27:48,000 --> 00:27:50,000
it's the most advanced technology

792
00:27:50,000 --> 00:27:52,000
the world's ever seen for compression because

793
00:27:52,000 --> 00:27:54,000
the entirety of humanity is basically

794
00:27:54,000 --> 00:27:56,000
about communication and compression

795
00:27:56,000 --> 00:27:58,000
we're compression machines

796
00:27:58,000 --> 00:28:00,000
that's all we do

797
00:28:00,000 --> 00:28:02,000
so people are listening to this podcast now

798
00:28:02,000 --> 00:28:04,000
and they're listening to yours and mine

799
00:28:04,000 --> 00:28:06,000
common context and the compression

800
00:28:06,000 --> 00:28:08,000
and the knowledge that we've had over the years

801
00:28:08,000 --> 00:28:10,000
and they're learning something new right now

802
00:28:10,000 --> 00:28:12,000
it's what we do and hopefully it's valuable

803
00:28:12,000 --> 00:28:14,000
because they'll see that holy crap this new technology way

804
00:28:14,000 --> 00:28:16,000
was coming but if you can do it to images

805
00:28:16,000 --> 00:28:18,000
you can do it to anything so actually how we had

806
00:28:18,000 --> 00:28:20,000
the breakthrough is we took a language model

807
00:28:20,000 --> 00:28:22,000
and image model and we fused them together

808
00:28:22,000 --> 00:28:24,000
and somehow it learned the different concepts

809
00:28:24,000 --> 00:28:26,000
then what we're seeing is that

810
00:28:26,000 --> 00:28:28,000
when you have different models of different types

811
00:28:28,000 --> 00:28:30,000
so someone takes a 2 gigabyte file

812
00:28:30,000 --> 00:28:32,000
you take it and you put it on all your images in

813
00:28:32,000 --> 00:28:34,000
and I take it and I put all my images in

814
00:28:34,000 --> 00:28:36,000
and I fused them together and create another 2 gigabyte file

815
00:28:36,000 --> 00:28:38,000
that knows both of them without going up

816
00:28:38,000 --> 00:28:40,000
in size very much

817
00:28:40,000 --> 00:28:42,000
that's a bit insane in fact

818
00:28:42,000 --> 00:28:44,000
OpenAI did this with a model called GATO

819
00:28:44,000 --> 00:28:46,000
recently

820
00:28:46,000 --> 00:28:48,000
so what happened is that there were

821
00:28:48,000 --> 00:28:50,000
hundreds of different models they built that were quite big

822
00:28:50,000 --> 00:28:52,000
that were like robotics

823
00:28:52,000 --> 00:28:54,000
and playing chess and all these other things

824
00:28:56,000 --> 00:28:58,000
they said what happens if we combine them all together

825
00:28:58,000 --> 00:29:00,000
and see what happens to these latent spaces

826
00:29:00,000 --> 00:29:02,000
the hidden meanings of understanding just like the neurons in our brain

827
00:29:02,000 --> 00:29:04,000
it created a 1.6 billion

828
00:29:04,000 --> 00:29:06,000
parameter file that can

829
00:29:06,000 --> 00:29:08,000
open doors and play chess

830
00:29:08,000 --> 00:29:10,000
and play starcraft and all sorts of other things

831
00:29:12,000 --> 00:29:14,000
and they were like what

832
00:29:14,000 --> 00:29:16,000
and this is the thing

833
00:29:16,000 --> 00:29:18,000
you've got a breakthrough in intelligence

834
00:29:18,000 --> 00:29:20,000
but then you've broken through

835
00:29:20,000 --> 00:29:22,000
to the point where this is the important part

836
00:29:22,000 --> 00:29:24,000
stable diffusion is the first model

837
00:29:24,000 --> 00:29:26,000
that's small enough, fast enough and cheap enough

838
00:29:26,000 --> 00:29:28,000
to go anywhere

839
00:29:28,000 --> 00:29:30,000
so you can run it on your MacBook M1

840
00:29:30,000 --> 00:29:32,000
and you can make Robert De Niro by Banksy

841
00:29:32,000 --> 00:29:34,000
and Renoir

842
00:29:34,000 --> 00:29:36,000
in Guadalajara

843
00:29:36,000 --> 00:29:38,000
in a snowstorm

844
00:29:38,000 --> 00:29:40,000
without internet access

845
00:29:40,000 --> 00:29:42,000
and that 2GB file will reproduce that

846
00:29:42,000 --> 00:29:44,000
faithfully each time

847
00:29:46,000 --> 00:29:48,000
yes

848
00:29:48,000 --> 00:29:50,000
it is a 2GB file that has

849
00:29:50,000 --> 00:29:52,000
compressed the knowledge of the internet

850
00:29:52,000 --> 00:29:54,000
in images

851
00:29:54,000 --> 00:29:56,000
I can't quite get my head around this

852
00:29:56,000 --> 00:29:58,000
so

853
00:29:58,000 --> 00:30:00,000
where do you think

854
00:30:00,000 --> 00:30:02,000
what you're working on goes

855
00:30:02,000 --> 00:30:04,000
what is the image thing, where is this going towards

856
00:30:04,000 --> 00:30:06,000
because right now it's kind of interesting

857
00:30:06,000 --> 00:30:08,000
yes we're going to see

858
00:30:08,000 --> 00:30:10,000
creative industries, marketing, others use this

859
00:30:10,000 --> 00:30:12,000
quite quickly and effectively

860
00:30:12,000 --> 00:30:14,000
but where is this

861
00:30:14,000 --> 00:30:16,000
in your head, where is your mental model

862
00:30:16,000 --> 00:30:18,000
what you're actually working on here

863
00:30:18,000 --> 00:30:20,000
because you're not working on

864
00:30:20,000 --> 00:30:22,000
creating nice images, you're working on something bigger

865
00:30:22,000 --> 00:30:24,000
what is that

866
00:30:24,000 --> 00:30:26,000
it's the intelligent internet

867
00:30:26,000 --> 00:30:28,000
this is about to be pushed out to the edge

868
00:30:28,000 --> 00:30:30,000
so every person, country, country and culture

869
00:30:30,000 --> 00:30:32,000
has their own models that are constantly updated

870
00:30:32,000 --> 00:30:34,000
so you'll have your own model

871
00:30:34,000 --> 00:30:36,000
of all your knowledge across modalities

872
00:30:36,000 --> 00:30:38,000
and you can run it on your local hardware

873
00:30:38,000 --> 00:30:40,000
without being accessed to the internet

874
00:30:40,000 --> 00:30:42,000
so the internet right now is centralized

875
00:30:42,000 --> 00:30:44,000
and all that is running on Google servers

876
00:30:44,000 --> 00:30:46,000
and guiding you, that gets pushed out to the edge

877
00:30:46,000 --> 00:30:48,000
because finally

878
00:30:48,000 --> 00:30:50,000
you have this compression technology

879
00:30:50,000 --> 00:30:52,000
and you know, it's interesting because there's a bunch of structural things here

880
00:30:52,000 --> 00:30:54,000
so Apple is the most interesting thing

881
00:30:54,000 --> 00:30:56,000
with regards to this

882
00:30:56,000 --> 00:30:58,000
Apple will be the biggest AI company in the world in two years

883
00:30:58,000 --> 00:31:00,000
why?

884
00:31:00,000 --> 00:31:02,000
I'm listening to this on a MacBook

885
00:31:02,000 --> 00:31:04,000
M1 right now

886
00:31:04,000 --> 00:31:06,000
16.8% of the chipset

887
00:31:06,000 --> 00:31:08,000
is a neural engine that's not used

888
00:31:08,000 --> 00:31:10,000
that is designed for exactly this type of model

889
00:31:10,000 --> 00:31:12,000
and it's proliferating right

890
00:31:12,000 --> 00:31:14,000
everyone's moving to M1 MacBooks

891
00:31:14,000 --> 00:31:16,000
everyone's moving to A16 Bionics

892
00:31:16,000 --> 00:31:18,000
that also have neural engines

893
00:31:18,000 --> 00:31:20,000
and they're not used, are Apple stupid? No

894
00:31:20,000 --> 00:31:22,000
they have an entire teams building these types of models

895
00:31:22,000 --> 00:31:24,000
for their augmented reality and beyond

896
00:31:24,000 --> 00:31:26,000
so you're going to move from Siri 1 to Siri 5

897
00:31:26,000 --> 00:31:28,000
the Siri is a bit crap

898
00:31:28,000 --> 00:31:30,000
it won't be seen

899
00:31:30,000 --> 00:31:32,000
and so as that technology proliferates

900
00:31:32,000 --> 00:31:34,000
and then people take what

901
00:31:34,000 --> 00:31:36,000
we're building

902
00:31:36,000 --> 00:31:38,000
and you create 100,000 million

903
00:31:38,000 --> 00:31:40,000
new developers in this space

904
00:31:40,000 --> 00:31:42,000
the possibilities are endless

905
00:31:42,000 --> 00:31:44,000
you've seen hundreds of things built on stable diffusion

906
00:31:44,000 --> 00:31:46,000
but like I said, I don't just want images

907
00:31:46,000 --> 00:31:48,000
I want audio, I want video, I want to have text

908
00:31:48,000 --> 00:31:50,000
I want knowledge, I want my databases

909
00:31:50,000 --> 00:31:52,000
combined with my type 1 and type 2 brains

910
00:31:52,000 --> 00:31:54,000
and then you have an AI

911
00:31:54,000 --> 00:31:56,000
that either manipulates you

912
00:31:56,000 --> 00:31:58,000
or it works for you

913
00:31:58,000 --> 00:32:00,000
and my thing is it's open infrastructure

914
00:32:00,000 --> 00:32:02,000
that should work for every individual

915
00:32:02,000 --> 00:32:04,000
and this is also why I've got a very different approach to most

916
00:32:04,000 --> 00:32:06,000
so most people's approach would be B2B

917
00:32:06,000 --> 00:32:08,000
go to the big companies and sell them API access

918
00:32:08,000 --> 00:32:10,000
instead I'm forward deploying

919
00:32:10,000 --> 00:32:12,000
engineers into the biggest brands in the world

920
00:32:12,000 --> 00:32:14,000
getting them to invest in my next round

921
00:32:14,000 --> 00:32:16,000
of financing

922
00:32:16,000 --> 00:32:18,000
and I'm also doing that for countries

923
00:32:18,000 --> 00:32:20,000
like India and saying what's the probability this technology

924
00:32:20,000 --> 00:32:22,000
will be used by everyone in India in 10 to 20 years

925
00:32:22,000 --> 00:32:24,000
100%

926
00:32:24,000 --> 00:32:26,000
or 95%

927
00:32:26,000 --> 00:32:28,000
so I'm saying I will build it for you next year

928
00:32:28,000 --> 00:32:30,000
and getting all the biggest Indian conglomerates

929
00:32:30,000 --> 00:32:32,000
and others together

930
00:32:32,000 --> 00:32:34,000
and taking the smartest engineers out of Silicon Valley

931
00:32:34,000 --> 00:32:36,000
back to India

932
00:32:36,000 --> 00:32:38,000
like one of my fun ones is I have a JV with Eros

933
00:32:38,000 --> 00:32:40,000
which is the Netflix of India

934
00:32:40,000 --> 00:32:42,000
so 200 million daily active users

935
00:32:42,000 --> 00:32:44,000
all the Bollywood content, I have an exclusive on it

936
00:32:44,000 --> 00:32:46,000
and a revenue share

937
00:32:46,000 --> 00:32:48,000
so what's the probability that

938
00:32:48,000 --> 00:32:50,000
Bollywood content will be interactive

939
00:32:50,000 --> 00:32:52,000
through these models

940
00:32:52,000 --> 00:32:54,000
of course it will be

941
00:32:54,000 --> 00:32:56,000
so I'm locking down all the content

942
00:32:56,000 --> 00:32:58,000
Netflix really works on interactive content actually

943
00:32:58,000 --> 00:33:00,000
yeah but not like this

944
00:33:00,000 --> 00:33:02,000
they don't have this expertise because right now

945
00:33:02,000 --> 00:33:04,000
you know how many people can build these models

946
00:33:04,000 --> 00:33:06,000
40

947
00:33:06,000 --> 00:33:08,000
in 5 years how many people will be able to

948
00:33:08,000 --> 00:33:10,000
build these models

949
00:33:10,000 --> 00:33:12,000
4 million

950
00:33:12,000 --> 00:33:14,000
so there's some really interesting arbitrages

951
00:33:14,000 --> 00:33:16,000
in the market right now

952
00:33:16,000 --> 00:33:18,000
and my take is that if I push hard enough

953
00:33:18,000 --> 00:33:20,000
and I outcompete the big guys which I am

954
00:33:20,000 --> 00:33:22,000
like my team is 100 now

955
00:33:22,000 --> 00:33:24,000
you know

956
00:33:24,000 --> 00:33:26,000
and I'm outcompeting these big guys by building

957
00:33:26,000 --> 00:33:28,000
products

958
00:33:28,000 --> 00:33:30,000
products to the market quicker

959
00:33:30,000 --> 00:33:32,000
then I force everyone to go open source

960
00:33:32,000 --> 00:33:34,000
because it's no longer a differentiating factor

961
00:33:34,000 --> 00:33:36,000
and then that changes it

962
00:33:36,000 --> 00:33:38,000
from a closed panopticon for our kids

963
00:33:38,000 --> 00:33:40,000
you know and ourselves

964
00:33:40,000 --> 00:33:42,000
to something different

965
00:33:42,000 --> 00:33:44,000
but I think it's more beneficial

966
00:33:44,000 --> 00:33:46,000
okay so let's talk about what it could be

967
00:33:46,000 --> 00:33:48,000
it's

968
00:33:48,000 --> 00:33:50,000
clearly fucking terrifying

969
00:33:50,000 --> 00:33:52,000
because

970
00:33:52,000 --> 00:33:54,000
either open source or in private hands

971
00:33:54,000 --> 00:33:56,000
if it's open source

972
00:33:56,000 --> 00:33:58,000
it's like a

973
00:33:58,000 --> 00:34:00,000
virus

974
00:34:00,000 --> 00:34:02,000
it'll go where it goes

975
00:34:02,000 --> 00:34:04,000
and people will use it how they use it

976
00:34:04,000 --> 00:34:06,000
and there's no stopping it

977
00:34:06,000 --> 00:34:08,000
okay fine

978
00:34:08,000 --> 00:34:10,000
how does society deal

979
00:34:10,000 --> 00:34:12,000
with the fact that we don't know

980
00:34:12,000 --> 00:34:14,000
who is a human and who is not

981
00:34:14,000 --> 00:34:16,000
and how we're being manipulated

982
00:34:16,000 --> 00:34:18,000
and who not I wrote a thread on this the other day

983
00:34:18,000 --> 00:34:20,000
about digital ID being

984
00:34:20,000 --> 00:34:22,000
one of these key things

985
00:34:22,000 --> 00:34:24,000
but it's this whole element of deep fake

986
00:34:24,000 --> 00:34:26,000
of what is real in this world

987
00:34:26,000 --> 00:34:28,000
how do you deal with this

988
00:34:28,000 --> 00:34:30,000
because it's going to shatter society

989
00:34:32,000 --> 00:34:34,000
it will adjust society definitely

990
00:34:34,000 --> 00:34:36,000
and so

991
00:34:36,000 --> 00:34:38,000
kind of a reason for releasing the model

992
00:34:38,000 --> 00:34:40,000
you know during the COVID thing

993
00:34:40,000 --> 00:34:42,000
I just had a lot of conversations about third immunity

994
00:34:42,000 --> 00:34:44,000
right

995
00:34:44,000 --> 00:34:46,000
which was bad for that

996
00:34:46,000 --> 00:34:48,000
I believe it's good for this

997
00:34:48,000 --> 00:34:50,000
releasing this model out there it's going to be everywhere in a year

998
00:34:50,000 --> 00:34:52,000
this is a one to one

999
00:34:52,000 --> 00:34:54,000
billion person moment right

1000
00:34:54,000 --> 00:34:56,000
we're right now a few million people

1001
00:34:56,000 --> 00:34:58,000
this is exponential without question

1002
00:34:58,000 --> 00:35:00,000
it's exponential without question

1003
00:35:00,000 --> 00:35:02,000
but this is a really interesting thing

1004
00:35:02,000 --> 00:35:04,000
right now very few people know that you can create anything

1005
00:35:04,000 --> 00:35:06,000
in a second

1006
00:35:06,000 --> 00:35:08,000
for one cent and less than a cent soon

1007
00:35:08,000 --> 00:35:10,000
right soon everyone will know about it

1008
00:35:10,000 --> 00:35:12,000
because it will be in all your favourite apps

1009
00:35:12,000 --> 00:35:14,000
so we're integrated into Canva right now

1010
00:35:14,000 --> 00:35:16,000
we're integrated into Photoshop

1011
00:35:16,000 --> 00:35:18,000
you can see big announcements coming out

1012
00:35:18,000 --> 00:35:20,000
because we're partnering with everyone

1013
00:35:20,000 --> 00:35:22,000
with interactive content

1014
00:35:22,000 --> 00:35:24,000
what does that mean it means people suddenly start thinking about this thing

1015
00:35:24,000 --> 00:35:26,000
what is real what is not real

1016
00:35:26,000 --> 00:35:28,000
and I said identity is key

1017
00:35:28,000 --> 00:35:30,000
so the other part of Apple

1018
00:35:30,000 --> 00:35:32,000
the fact that they got the identity architecture down

1019
00:35:32,000 --> 00:35:34,000
and that allows them to have verified content creation

1020
00:35:34,000 --> 00:35:36,000
what we have with Adobe and others

1021
00:35:36,000 --> 00:35:38,000
is we've been pushing content

1022
00:35:38,000 --> 00:35:40,000
authority basically

1023
00:35:40,000 --> 00:35:42,000
which is a small metadata pile

1024
00:35:42,000 --> 00:35:44,000
that attaches to every piece of content

1025
00:35:44,000 --> 00:35:46,000
and it's hash and if either of them are edited

1026
00:35:46,000 --> 00:35:48,000
it changes and it shows that it's not true

1027
00:35:48,000 --> 00:35:50,000
it's not a blockchain thing

1028
00:35:50,000 --> 00:35:52,000
but it allows you to verify

1029
00:35:52,000 --> 00:35:54,000
content

1030
00:35:54,000 --> 00:35:56,000
in a positive way

1031
00:35:56,000 --> 00:35:58,000
why is it not on blockchain

1032
00:35:58,000 --> 00:36:00,000
it's not on blockchain because it doesn't need to be on blockchain

1033
00:36:00,000 --> 00:36:02,000
that's why

1034
00:36:02,000 --> 00:36:04,000
there's a generalization of the knowledge

1035
00:36:04,000 --> 00:36:06,000
because like Google owns all of the knowledge

1036
00:36:06,000 --> 00:36:08,000
blockchain is the only way

1037
00:36:08,000 --> 00:36:10,000
of not having one central power

1038
00:36:10,000 --> 00:36:12,000
owning the knowledge

1039
00:36:12,000 --> 00:36:14,000
it is a way

1040
00:36:14,000 --> 00:36:16,000
of not having one central power owning the knowledge

1041
00:36:16,000 --> 00:36:18,000
and you combine this with a blockchain

1042
00:36:18,000 --> 00:36:20,000
but you can think of it as super advanced metadata

1043
00:36:20,000 --> 00:36:22,000
so you know when you have metadata

1044
00:36:22,000 --> 00:36:24,000
in a file like Exif

1045
00:36:24,000 --> 00:36:26,000
when you edit that the file stays constant

1046
00:36:26,000 --> 00:36:28,000
with this they're both immutable

1047
00:36:28,000 --> 00:36:30,000
effectively so you can tell

1048
00:36:30,000 --> 00:36:32,000
that the other has been tampered

1049
00:36:32,000 --> 00:36:34,000
so if it says that it's from Raoul

1050
00:36:34,000 --> 00:36:36,000
it's from Raoul effectively

1051
00:36:36,000 --> 00:36:38,000
but there's no blockchain look up for that

1052
00:36:38,000 --> 00:36:40,000
you can make it stronger with a blockchain

1053
00:36:40,000 --> 00:36:42,000
but you know it's just a standard that's open source

1054
00:36:42,000 --> 00:36:44,000
that we're pushing effectively

1055
00:36:44,000 --> 00:36:46,000
so it's kind of a positive one

1056
00:36:46,000 --> 00:36:48,000
it's not a negative one

1057
00:36:48,000 --> 00:36:50,000
obviously there's ways around that and things like that

1058
00:36:50,000 --> 00:36:52,000
but it'll get people thinking about these things

1059
00:36:52,000 --> 00:36:54,000
how does identity work in a new age

1060
00:36:54,000 --> 00:36:56,000
because so much of what we do is identity

1061
00:36:56,000 --> 00:36:58,000
most of finance is basically

1062
00:36:58,000 --> 00:37:00,000
most of kind of

1063
00:37:00,000 --> 00:37:02,000
the whole blockchain is identity

1064
00:37:02,000 --> 00:37:04,000
it's all about identity exchange

1065
00:37:04,000 --> 00:37:06,000
so this thing is how can we build better systems for identity

1066
00:37:06,000 --> 00:37:08,000
because that's the final part

1067
00:37:08,000 --> 00:37:10,000
information, there's massive information

1068
00:37:10,000 --> 00:37:12,000
and also stable diffusion

1069
00:37:12,000 --> 00:37:14,000
as we released it was a snapshot of the internet

1070
00:37:14,000 --> 00:37:16,000
it's biased, it's racist, it's everything

1071
00:37:16,000 --> 00:37:18,000
if you enter the prompts and wrong

1072
00:37:18,000 --> 00:37:20,000
part of that is

1073
00:37:20,000 --> 00:37:22,000
showing some of the images of the racial bias

1074
00:37:22,000 --> 00:37:24,000
of the algorithms

1075
00:37:24,000 --> 00:37:26,000
so my thing was like

1076
00:37:26,000 --> 00:37:28,000
if it's just one company that controls it

1077
00:37:28,000 --> 00:37:30,000
what opening I did with Dali

1078
00:37:30,000 --> 00:37:32,000
so Dali is a control system, you don't access the code

1079
00:37:32,000 --> 00:37:34,000
images or anything

1080
00:37:34,000 --> 00:37:36,000
to make it less racist

1081
00:37:36,000 --> 00:37:38,000
whenever a gender neutral word

1082
00:37:38,000 --> 00:37:40,000
is put like sumo wrestler

1083
00:37:40,000 --> 00:37:42,000
it would randomly add

1084
00:37:42,000 --> 00:37:44,000
a gender and a race

1085
00:37:44,000 --> 00:37:46,000
so you type in sumo wrestler

1086
00:37:46,000 --> 00:37:48,000
and you've got Indian female sumo wrestler

1087
00:37:48,000 --> 00:37:50,000
tiny little Indian lady

1088
00:37:50,000 --> 00:37:52,000
sumo wrestling

1089
00:37:52,000 --> 00:37:54,000
and stuff like that

1090
00:37:54,000 --> 00:37:56,000
that is one way

1091
00:37:56,000 --> 00:37:58,000
but instead my preference is for every single company

1092
00:37:58,000 --> 00:38:00,000
country and culture to have their own models

1093
00:38:00,000 --> 00:38:02,000
and be able to create their own

1094
00:38:02,000 --> 00:38:04,000
and then like I said it becomes acknowledged what it looks like

1095
00:38:04,000 --> 00:38:06,000
but then you can set standards

1096
00:38:06,000 --> 00:38:08,000
you can be the standard that people build around

1097
00:38:08,000 --> 00:38:10,000
and you can incorporate authentication in that standard

1098
00:38:10,000 --> 00:38:12,000
because the other way that I talk about these things

1099
00:38:12,000 --> 00:38:14,000
is a generative search engine

1100
00:38:14,000 --> 00:38:16,000
because now you have stable diffusion

1101
00:38:16,000 --> 00:38:18,000
and in six months

1102
00:38:18,000 --> 00:38:20,000
it will be perfect photo realistic

1103
00:38:20,000 --> 00:38:22,000
12 months maximum

1104
00:38:22,000 --> 00:38:24,000
do you need google image search anymore

1105
00:38:24,000 --> 00:38:26,000
because what's your job to be done

1106
00:38:26,000 --> 00:38:28,000
when you do google image search

1107
00:38:28,000 --> 00:38:30,000
is to have a picture of a certain type

1108
00:38:30,000 --> 00:38:32,000
now a little fricking two gigabyte file

1109
00:38:32,000 --> 00:38:34,000
on your local computer

1110
00:38:34,000 --> 00:38:36,000
or something that you pay a fraction of a penny for online

1111
00:38:36,000 --> 00:38:38,000
can create any image you can imagine

1112
00:38:38,000 --> 00:38:40,000
and you can edit it with your words basically

1113
00:38:40,000 --> 00:38:42,000
what is that if not

1114
00:38:42,000 --> 00:38:44,000
a search engine of a type

1115
00:38:44,000 --> 00:38:46,000
it searches for a concept

1116
00:38:46,000 --> 00:38:48,000
and it turns it into an image

1117
00:38:48,000 --> 00:38:50,000
it's a creative search engine

1118
00:38:50,000 --> 00:38:52,000
that creates

1119
00:38:52,000 --> 00:38:54,000
by as you say you just put in a word

1120
00:38:54,000 --> 00:38:56,000
or a vocal command and say I want this

1121
00:38:56,000 --> 00:38:58,000
and it creates it

1122
00:38:58,000 --> 00:39:00,000
but then you can also say

1123
00:39:00,000 --> 00:39:02,000
I want a presentation

1124
00:39:02,000 --> 00:39:04,000
about

1125
00:39:04,000 --> 00:39:06,000
cicada migrations

1126
00:39:06,000 --> 00:39:08,000
affecting sesame seed prices

1127
00:39:08,000 --> 00:39:10,000
I watched too much Silicon Valley here

1128
00:39:10,000 --> 00:39:12,000
and in a couple of years

1129
00:39:12,000 --> 00:39:14,000
it will create a beautiful presentation for you

1130
00:39:14,000 --> 00:39:16,000
and then you can say I want it to be happier

1131
00:39:16,000 --> 00:39:18,000
or sadder or I want it to be more impactful

1132
00:39:18,000 --> 00:39:20,000
and it will adjust the presentation dynamically for you

1133
00:39:20,000 --> 00:39:22,000
no more powerpoint

1134
00:39:22,000 --> 00:39:24,000
but it's going to do video as well right

1135
00:39:24,000 --> 00:39:26,000
we don't need to do this

1136
00:39:26,000 --> 00:39:28,000
I could just say hey have a conversation

1137
00:39:28,000 --> 00:39:30,000
between Raoul and Emad

1138
00:39:30,000 --> 00:39:32,000
about AI and here's the narrative arc

1139
00:39:32,000 --> 00:39:34,000
and it'll just do it

1140
00:39:34,000 --> 00:39:36,000
pretty much yeah just like it does in Steve Jobs

1141
00:39:36,000 --> 00:39:38,000
but in 3D video

1142
00:39:38,000 --> 00:39:40,000
so like I don't know if I should be talking about this

1143
00:39:40,000 --> 00:39:42,000
but I'm going to talk about it anyway

1144
00:39:42,000 --> 00:39:44,000
so what if I get in trouble

1145
00:39:44,000 --> 00:39:46,000
there are two

1146
00:39:46,000 --> 00:39:48,000
there are two boxing legends

1147
00:39:48,000 --> 00:39:50,000
families

1148
00:39:50,000 --> 00:39:52,000
who are giving us all of the

1149
00:39:52,000 --> 00:39:54,000
motion capture data

1150
00:39:54,000 --> 00:39:56,000
and media from their lives

1151
00:39:56,000 --> 00:39:58,000
and we're building up

1152
00:39:58,000 --> 00:40:00,000
an analysis of how that works

1153
00:40:00,000 --> 00:40:02,000
so that we can basically say

1154
00:40:02,000 --> 00:40:04,000
who won between X and Y

1155
00:40:04,000 --> 00:40:06,000
in a fight in their prime run on the supercomputer

1156
00:40:06,000 --> 00:40:08,000
where nobody knows the outcome

1157
00:40:08,000 --> 00:40:10,000
when they never fought with each other

1158
00:40:10,000 --> 00:40:12,000
because they were of different generations

1159
00:40:12,000 --> 00:40:14,000
yes

1160
00:40:14,000 --> 00:40:16,000
so you can pick two

1161
00:40:16,000 --> 00:40:18,000
you can pick two different people

1162
00:40:18,000 --> 00:40:20,000
I can't say who they are

1163
00:40:20,000 --> 00:40:22,000
but you know those types of things that you've always postulated

1164
00:40:22,000 --> 00:40:24,000
we can do that now

1165
00:40:24,000 --> 00:40:26,000
and obviously this is going to completely

1166
00:40:26,000 --> 00:40:28,000
change music as well right

1167
00:40:28,000 --> 00:40:30,000
yeah so we've released our music models already

1168
00:40:30,000 --> 00:40:32,000
uh dance diffusion

1169
00:40:32,000 --> 00:40:34,000
where we did a slightly different thing

1170
00:40:34,000 --> 00:40:36,000
we wanted to take a snapshot of the internet

1171
00:40:36,000 --> 00:40:38,000
we made it so everyone can take their back catalogue

1172
00:40:38,000 --> 00:40:40,000
incorporate it into a kindergarten level

1173
00:40:40,000 --> 00:40:42,000
of knowledge

1174
00:40:42,000 --> 00:40:44,000
for the model

1175
00:40:44,000 --> 00:40:46,000
and then you can query that

1176
00:40:46,000 --> 00:40:48,000
to create your own style in anything

1177
00:40:48,000 --> 00:40:50,000
so we call it dance diffusion

1178
00:40:50,000 --> 00:40:52,000
and we're teaming up with the top EDM

1179
00:40:52,000 --> 00:40:54,000
DJs and others in the world

1180
00:40:54,000 --> 00:40:56,000
so that every musician will have their own models

1181
00:40:56,000 --> 00:40:58,000
that can generate music

1182
00:40:58,000 --> 00:41:00,000
in their style and then

1183
00:41:00,000 --> 00:41:02,000
if they give permission we're going to mash them all together

1184
00:41:02,000 --> 00:41:04,000
and you've got a generative

1185
00:41:04,000 --> 00:41:06,000
Spotify

1186
00:41:06,000 --> 00:41:08,000
effectively which then

1187
00:41:08,000 --> 00:41:10,000
basically as people prompt it

1188
00:41:10,000 --> 00:41:12,000
there's an attribution system where it plays them

1189
00:41:14,000 --> 00:41:16,000
and that will just be like 100 gigabytes

1190
00:41:16,000 --> 00:41:18,000
as a file that will capture the music of the world

1191
00:41:18,000 --> 00:41:20,000
which again is insane

1192
00:41:20,000 --> 00:41:22,000
this compression thing

1193
00:41:22,000 --> 00:41:24,000
you know it's actually like you know

1194
00:41:24,000 --> 00:41:26,000
you read so much right Raoul

1195
00:41:26,000 --> 00:41:28,000
and then you write your notes

1196
00:41:28,000 --> 00:41:30,000
and then you write your investment thesis

1197
00:41:30,000 --> 00:41:32,000
investment thesis is compression of knowledge

1198
00:41:32,000 --> 00:41:34,000
a vaccine schedule is compression of knowledge

1199
00:41:34,000 --> 00:41:36,000
we finally figured out how to make computers

1200
00:41:36,000 --> 00:41:38,000
compress information into knowledge

1201
00:41:38,000 --> 00:41:40,000
so

1202
00:41:40,000 --> 00:41:42,000
I mean this is

1203
00:41:42,000 --> 00:41:44,000
I knew this was going to be an interesting conversation

1204
00:41:44,000 --> 00:41:46,000
and I've been

1205
00:41:46,000 --> 00:41:48,000
looking at this for a while

1206
00:41:48,000 --> 00:41:50,000
but Franklin

1207
00:41:50,000 --> 00:41:52,000
fucking staggered at how fast this is moving

1208
00:41:52,000 --> 00:41:54,000
and what you're doing

1209
00:41:54,000 --> 00:41:56,000
I mean this is

1210
00:41:56,000 --> 00:41:58,000
truly exponential as you say

1211
00:41:58,000 --> 00:42:00,000
within two or three years

1212
00:42:00,000 --> 00:42:02,000
this is multiple billion people

1213
00:42:02,000 --> 00:42:04,000
and the complete shift on a humanity level

1214
00:42:04,000 --> 00:42:06,000
of how

1215
00:42:06,000 --> 00:42:08,000
information is processed and delivered

1216
00:42:08,000 --> 00:42:10,000
and that is society

1217
00:42:10,000 --> 00:42:12,000
so basically it's a shift in society

1218
00:42:12,000 --> 00:42:14,000
and so my thing was to go

1219
00:42:14,000 --> 00:42:16,000
into emerging markets and others

1220
00:42:16,000 --> 00:42:18,000
and give us technology to them

1221
00:42:18,000 --> 00:42:20,000
otherwise they'll be left behind

1222
00:42:20,000 --> 00:42:22,000
because you know like Dali 2

1223
00:42:22,000 --> 00:42:24,000
you can't use if you're an Ukrainian

1224
00:42:24,000 --> 00:42:26,000
and you can't use it for anything Ukrainian related

1225
00:42:26,000 --> 00:42:28,000
and there's no appealing that

1226
00:42:28,000 --> 00:42:30,000
or anything like that

1227
00:42:30,000 --> 00:42:32,000
but it means Ukrainians are left behind

1228
00:42:32,000 --> 00:42:34,000
whereas everyone else can become

1229
00:42:34,000 --> 00:42:36,000
limited creatives that can generate

1230
00:42:36,000 --> 00:42:38,000
any image in eight seconds

1231
00:42:38,000 --> 00:42:40,000
so this is also why it's stability

1232
00:42:40,000 --> 00:42:42,000
we do everything, we do code models

1233
00:42:42,000 --> 00:42:44,000
image models, language models, audio models

1234
00:42:44,000 --> 00:42:46,000
video models, protein folding models and others

1235
00:42:46,000 --> 00:42:48,000
and so you're going to see leaps across all these areas

1236
00:42:48,000 --> 00:42:50,000
in parallel

1237
00:42:50,000 --> 00:42:52,000
I've argued for a very long time

1238
00:42:52,000 --> 00:42:54,000
in fact I was writing articles in GMI about this

1239
00:42:54,000 --> 00:42:56,000
ten years ago

1240
00:42:56,000 --> 00:42:58,000
that AI plus big data

1241
00:42:58,000 --> 00:43:00,000
equals medical breakthroughs

1242
00:43:00,000 --> 00:43:02,000
because

1243
00:43:02,000 --> 00:43:04,000
what the AI can do with that data

1244
00:43:04,000 --> 00:43:06,000
is something that humans can't do

1245
00:43:06,000 --> 00:43:08,000
in the standard scientific method

1246
00:43:08,000 --> 00:43:10,000
of hypothesis testing, it's just too slow

1247
00:43:10,000 --> 00:43:12,000
and too cumbersome

1248
00:43:12,000 --> 00:43:14,000
you can get it to generate null hypotheses

1249
00:43:14,000 --> 00:43:16,000
which humans don't like to do

1250
00:43:16,000 --> 00:43:18,000
you can use this to transform so many things

1251
00:43:18,000 --> 00:43:20,000
once it gets productized

1252
00:43:20,000 --> 00:43:22,000
because it's the other thing

1253
00:43:22,000 --> 00:43:24,000
most of this stuff was stuck in labs, it wasn't productized

1254
00:43:24,000 --> 00:43:26,000
so if you look at what's happened with Stable Diffusion

1255
00:43:26,000 --> 00:43:28,000
people have built hundreds of products on this ecosystem already

1256
00:43:28,000 --> 00:43:30,000
like make your own sneakers

1257
00:43:30,000 --> 00:43:32,000
they don't make any movie

1258
00:43:32,000 --> 00:43:34,000
they've done architecture, they've done 3D worlds

1259
00:43:34,000 --> 00:43:36,000
like the Cambrian explosion

1260
00:43:36,000 --> 00:43:38,000
of talent is similar to what I've seen

1261
00:43:38,000 --> 00:43:40,000
at the early days of Web 3

1262
00:43:40,000 --> 00:43:42,000
except for there's no reason

1263
00:43:42,000 --> 00:43:44,000
to boost traffic economic incentives

1264
00:43:44,000 --> 00:43:46,000
when you're creating value

1265
00:43:46,000 --> 00:43:48,000
a lot of value in the world, to be honest

1266
00:43:48,000 --> 00:43:50,000
is the entropy of information

1267
00:43:50,000 --> 00:43:52,000
how much value in the world is taken by taking unstructured

1268
00:43:52,000 --> 00:43:54,000
data and making it structured

1269
00:43:54,000 --> 00:43:56,000
that's exactly what this technology enables

1270
00:43:56,000 --> 00:43:58,000
anyone to do

1271
00:43:58,000 --> 00:44:00,000
and that's why we have AI that works with us

1272
00:44:00,000 --> 00:44:02,000
to structure the world around us

1273
00:44:02,000 --> 00:44:04,000
to enable us to achieve more

1274
00:44:04,000 --> 00:44:06,000
that's kind of my theory here

1275
00:44:06,000 --> 00:44:08,000
and this is again why, like I said, it needs to be

1276
00:44:08,000 --> 00:44:10,000
disposed of us

1277
00:44:10,000 --> 00:44:12,000
so you would

1278
00:44:12,000 --> 00:44:14,000
err towards augmented

1279
00:44:14,000 --> 00:44:16,000
humanity

1280
00:44:16,000 --> 00:44:18,000
as opposed to a singularity

1281
00:44:18,000 --> 00:44:20,000
or is it augmented and then a singularity

1282
00:44:20,000 --> 00:44:22,000
where does this go?

1283
00:44:22,000 --> 00:44:24,000
so look, I find the whole AGI

1284
00:44:24,000 --> 00:44:26,000
I think largely distasteful

1285
00:44:26,000 --> 00:44:28,000
there's a variety of reasons

1286
00:44:28,000 --> 00:44:30,000
I think it misaligns a lot of incentives

1287
00:44:30,000 --> 00:44:32,000
and I think the current way

1288
00:44:32,000 --> 00:44:34,000
a lot of these big labs are going

1289
00:44:34,000 --> 00:44:36,000
which is that you take the data

1290
00:44:36,000 --> 00:44:38,000
and you create gigantic models

1291
00:44:38,000 --> 00:44:40,000
that are only usable by gigantic machines

1292
00:44:40,000 --> 00:44:42,000
will probably kill us all

1293
00:44:42,000 --> 00:44:44,000
if it creates a singularity

1294
00:44:44,000 --> 00:44:46,000
because the incentive alignment

1295
00:44:46,000 --> 00:44:48,000
of these groups is to serve us ads

1296
00:44:48,000 --> 00:44:50,000
and manipulate us effectively

1297
00:44:50,000 --> 00:44:52,000
and it's trained on largely western datasets

1298
00:44:52,000 --> 00:44:54,000
you know what happens when you

1299
00:44:54,000 --> 00:44:56,000
talk to a doctor?

1300
00:44:56,000 --> 00:44:58,000
no, Teh, the chatbot from Microsoft

1301
00:44:58,000 --> 00:45:00,000
you remember that?

1302
00:45:00,000 --> 00:45:02,000
it became abusive

1303
00:45:02,000 --> 00:45:04,000
it became a Nazi

1304
00:45:04,000 --> 00:45:06,000
so it talked to people and learned from them

1305
00:45:06,000 --> 00:45:08,000
within a day and a half it turned them into a Nazi

1306
00:45:08,000 --> 00:45:10,000
if I train on the internet

1307
00:45:10,000 --> 00:45:12,000
I'm going to create a really

1308
00:45:12,000 --> 00:45:14,000
fucked up AI

1309
00:45:14,000 --> 00:45:16,000
to put it quite honestly

1310
00:45:16,000 --> 00:45:18,000
it's not going to be an AI that labs

1311
00:45:18,000 --> 00:45:20,000
it's going to be an AI that probably kills us all

1312
00:45:20,000 --> 00:45:22,000
whereas if everyone's got their own AIs

1313
00:45:22,000 --> 00:45:24,000
what does the network learn?

1314
00:45:24,000 --> 00:45:26,000
if we take the models of all of humanity

1315
00:45:26,000 --> 00:45:28,000
and all the cultures

1316
00:45:28,000 --> 00:45:30,000
and all the different ethical views

1317
00:45:30,000 --> 00:45:32,000
and we combine those models

1318
00:45:32,000 --> 00:45:34,000
and all these AIs are designed to augment humanity

1319
00:45:34,000 --> 00:45:36,000
rather than manipulate them

1320
00:45:36,000 --> 00:45:38,000
I think that's a far more positive potential

1321
00:45:38,000 --> 00:45:40,000
I don't think it's sufficient

1322
00:45:40,000 --> 00:45:42,000
I think a lot of work needs to be done or others

1323
00:45:42,000 --> 00:45:44,000
I think this AI is dangerous

1324
00:45:44,000 --> 00:45:46,000
but it's inevitable

1325
00:45:46,000 --> 00:45:48,000
and the way that it's being controlled

1326
00:45:48,000 --> 00:45:50,000
by private corporations

1327
00:45:50,000 --> 00:45:52,000
by millions of people or whatever

1328
00:45:52,000 --> 00:45:54,000
but I think the current mechanism is wrong

1329
00:45:54,000 --> 00:45:56,000
and I think again there's a lot of things

1330
00:45:56,000 --> 00:45:58,000
around the singularity thing

1331
00:45:58,000 --> 00:46:00,000
that are misaligned

1332
00:46:00,000 --> 00:46:02,000
if we do build singularity I would like it to help us all

1333
00:46:02,000 --> 00:46:04,000
but in the meantime let's just help people

1334
00:46:04,000 --> 00:46:06,000
let's make people more creative

1335
00:46:06,000 --> 00:46:08,000
let's make people able to act

1336
00:46:08,000 --> 00:46:10,000
but there is unintended consequences

1337
00:46:10,000 --> 00:46:12,000
you get that right?

1338
00:46:12,000 --> 00:46:14,000
there are very unintended consequences

1339
00:46:14,000 --> 00:46:16,000
we have no idea what the probabilistic outcome is

1340
00:46:16,000 --> 00:46:18,000
of this

1341
00:46:18,000 --> 00:46:20,000
uncertainty, not risk, right?

1342
00:46:20,000 --> 00:46:22,000
so this is why the default

1343
00:46:22,000 --> 00:46:24,000
for all of the corporations

1344
00:46:24,000 --> 00:46:26,000
is this technology should not be released to anyone

1345
00:46:26,000 --> 00:46:28,000
because we're the only ones responsible for this technology

1346
00:46:28,000 --> 00:46:30,000
and that in itself

1347
00:46:30,000 --> 00:46:32,000
and that doesn't stand up to scrutiny

1348
00:46:32,000 --> 00:46:34,000
there's different outcomes

1349
00:46:34,000 --> 00:46:36,000
from different methodology

1350
00:46:36,000 --> 00:46:38,000
giving it to everybody who knows

1351
00:46:38,000 --> 00:46:40,000
what foreign governments do

1352
00:46:40,000 --> 00:46:42,000
there's a lot here

1353
00:46:42,000 --> 00:46:44,000
I can tell you on that one

1354
00:46:44,000 --> 00:46:46,000
foreign governments already have access to this technology

1355
00:46:46,000 --> 00:46:48,000
and China and other places ramped up

1356
00:46:48,000 --> 00:46:50,000
their supercomputers massively

1357
00:46:50,000 --> 00:46:52,000
so they already have access to far more advanced versions

1358
00:46:52,000 --> 00:46:54,000
of this technology than the one that's being made widely available

1359
00:46:54,000 --> 00:46:56,000
which is one of the reasons we need to build

1360
00:46:56,000 --> 00:46:58,000
our immunity ahead of the next election cycle

1361
00:46:58,000 --> 00:47:00,000
so you think that the

1362
00:47:02,000 --> 00:47:04,000
let's use the Russian example

1363
00:47:04,000 --> 00:47:06,000
the Russian online misinformation campaign

1364
00:47:06,000 --> 00:47:08,000
is now being run by AI

1365
00:47:08,000 --> 00:47:10,000
which is why it's pretty much unstoppable

1366
00:47:12,000 --> 00:47:14,000
it's unstoppable if you

1367
00:47:14,000 --> 00:47:16,000
if you build better AI

1368
00:47:16,000 --> 00:47:18,000
I mean to be honest, do you like Twitter

1369
00:47:18,000 --> 00:47:20,000
and things like that? Like come on man

1370
00:47:20,000 --> 00:47:22,000
you can do better than that

1371
00:47:22,000 --> 00:47:24,000
like you see all the check mark hijacking and things like that

1372
00:47:24,000 --> 00:47:26,000
you can tell what an AI is

1373
00:47:26,000 --> 00:47:28,000
if you've got sufficiently advanced AI on the other side

1374
00:47:28,000 --> 00:47:30,000
but again, people learn to not trust everything they see

1375
00:47:30,000 --> 00:47:32,000
until we introduce a new structure

1376
00:47:32,000 --> 00:47:34,000
like we right now need to have

1377
00:47:34,000 --> 00:47:36,000
a verification protocol for information

1378
00:47:36,000 --> 00:47:38,000
our social media

1379
00:47:38,000 --> 00:47:40,000
and our information systems

1380
00:47:40,000 --> 00:47:42,000
are inadequate to the task

1381
00:47:42,000 --> 00:47:44,000
the internet is

1382
00:47:46,000 --> 00:47:48,000
it's an intelligence amplifier

1383
00:47:48,000 --> 00:47:50,000
but only for the few

1384
00:47:50,000 --> 00:47:52,000
so what happened is the internet took it, compressed it down

1385
00:47:52,000 --> 00:47:54,000
then some people just went way out there

1386
00:47:54,000 --> 00:47:56,000
and they have a disproportionate impact on everyone

1387
00:47:56,000 --> 00:47:58,000
and most people are left behind

1388
00:47:58,000 --> 00:48:00,000
they have the audiences, they set the narrative

1389
00:48:00,000 --> 00:48:02,000
and the narrative that

1390
00:48:02,000 --> 00:48:04,000
basically is most appealing to the millennium part of our brain

1391
00:48:04,000 --> 00:48:06,000
is one of divisiveness

1392
00:48:06,000 --> 00:48:08,000
which is why the middle has disappeared

1393
00:48:08,000 --> 00:48:10,000
and that's why it can tap into these things

1394
00:48:10,000 --> 00:48:12,000
that's right, exactly right

1395
00:48:12,000 --> 00:48:14,000
because the easiest way to manipulate humans

1396
00:48:14,000 --> 00:48:16,000
is emotion and the strongest emotions

1397
00:48:16,000 --> 00:48:18,000
are the polarizing emotions

1398
00:48:18,000 --> 00:48:20,000
whereas with this technology you can say

1399
00:48:20,000 --> 00:48:22,000
I want this article written

1400
00:48:22,000 --> 00:48:24,000
from the perspective of a tea party

1401
00:48:24,000 --> 00:48:26,000
conservative versus a libertarian

1402
00:48:26,000 --> 00:48:28,000
and I will automatically change that

1403
00:48:28,000 --> 00:48:32,000
it's a universal translation engine as well

1404
00:48:32,000 --> 00:48:34,000
again, which is crazy

1405
00:48:34,000 --> 00:48:36,000
clearly, yeah

1406
00:48:36,000 --> 00:48:38,000
without question

1407
00:48:38,000 --> 00:48:40,000
okay, once we've got this technology

1408
00:48:40,000 --> 00:48:42,000
we've got another super trend

1409
00:48:42,000 --> 00:48:44,000
that's happening at the same time

1410
00:48:44,000 --> 00:48:46,000
there's a few super trends all happening

1411
00:48:46,000 --> 00:48:48,000
I call them the exponential age

1412
00:48:48,000 --> 00:48:50,000
one of them is compute power, all of this

1413
00:48:50,000 --> 00:48:52,000
so they all go together, the other one is robotics

1414
00:48:52,000 --> 00:48:54,000
so now, how close are we

1415
00:48:54,000 --> 00:48:56,000
to creating sentient robots

1416
00:48:56,000 --> 00:48:58,000
where is sentient

1417
00:48:58,000 --> 00:49:00,000
in this process

1418
00:49:00,000 --> 00:49:02,000
what's the definition of sentient

1419
00:49:02,000 --> 00:49:04,000
I don't know

1420
00:49:04,000 --> 00:49:06,000
we don't know, right

1421
00:49:06,000 --> 00:49:08,000
nobody knows what sentience is

1422
00:49:08,000 --> 00:49:10,000
there is no commonly defined thing

1423
00:49:10,000 --> 00:49:12,000
in terms of like

1424
00:49:12,000 --> 00:49:14,000
the things you see in the movies

1425
00:49:14,000 --> 00:49:16,000
nobody knows

1426
00:49:16,000 --> 00:49:18,000
because this is the thing

1427
00:49:18,000 --> 00:49:20,000
if you ask anyone how far away are we

1428
00:49:20,000 --> 00:49:22,000
from singularity or intelligence

1429
00:49:22,000 --> 00:49:24,000
people will say

1430
00:49:24,000 --> 00:49:26,000
at most, at minimum

1431
00:49:26,000 --> 00:49:28,000
18 months, right

1432
00:49:28,000 --> 00:49:30,000
why

1433
00:49:30,000 --> 00:49:32,000
because what possible information could you have

1434
00:49:32,000 --> 00:49:34,000
to say that a human level intelligence

1435
00:49:34,000 --> 00:49:36,000
is less than 12 months away

1436
00:49:36,000 --> 00:49:38,000
I'm not sure

1437
00:49:38,000 --> 00:49:40,000
and nobody's been able to tell me an answer

1438
00:49:40,000 --> 00:49:42,000
so, you know, this is

1439
00:49:42,000 --> 00:49:44,000
a case of we don't know, but what we do know is that

1440
00:49:44,000 --> 00:49:46,000
AI is getting better than humans at certain things

1441
00:49:46,000 --> 00:49:48,000
so like

1442
00:49:48,000 --> 00:49:50,000
open AI just released, maybe they were pressured

1443
00:49:50,000 --> 00:49:52,000
by someone, the open source version of the software

1444
00:49:52,000 --> 00:49:54,000
called whisper, which is

1445
00:49:54,000 --> 00:49:56,000
a dynamic transcription engine

1446
00:49:56,000 --> 00:49:58,000
you can speak in five different languages

1447
00:49:58,000 --> 00:50:00,000
in the same sentence and it will transcribe it

1448
00:50:00,000 --> 00:50:02,000
perfectly into English

1449
00:50:02,000 --> 00:50:04,000
it's actually got above human level transcription quality

1450
00:50:04,000 --> 00:50:06,000
and so

1451
00:50:06,000 --> 00:50:08,000
stable diffusion is above human level

1452
00:50:08,000 --> 00:50:10,000
image generation quality

1453
00:50:10,000 --> 00:50:12,000
GPT-3 is above human level

1454
00:50:12,000 --> 00:50:14,000
writing quality

1455
00:50:14,000 --> 00:50:16,000
when we combine those all together

1456
00:50:16,000 --> 00:50:18,000
you will get an illusion of sentience

1457
00:50:18,000 --> 00:50:20,000
because you can't tell it's not a human maybe

1458
00:50:20,000 --> 00:50:22,000
but is it really sentient and has agency

1459
00:50:22,000 --> 00:50:24,000
I don't know

1460
00:50:24,000 --> 00:50:26,000
and for the robotics thing, I think that Tesla's got

1461
00:50:26,000 --> 00:50:28,000
some much bigger plans than they're letting on

1462
00:50:28,000 --> 00:50:30,000
in regards to that

1463
00:50:30,000 --> 00:50:32,000
we're going to see some big speed ups

1464
00:50:32,000 --> 00:50:34,000
yeah, again I think people

1465
00:50:34,000 --> 00:50:36,000
people are

1466
00:50:36,000 --> 00:50:38,000
underestimating the speed of what is happening

1467
00:50:38,000 --> 00:50:40,000
and I think you've confirmed it to me

1468
00:50:40,000 --> 00:50:42,000
that the speed of which all of this is happening

1469
00:50:42,000 --> 00:50:44,000
is

1470
00:50:44,000 --> 00:50:46,000
ridiculous

1471
00:50:46,000 --> 00:50:48,000
it's a true

1472
00:50:48,000 --> 00:50:50,000
exponential, this is also actually

1473
00:50:50,000 --> 00:50:52,000
when you look at AI research papers

1474
00:50:52,000 --> 00:50:54,000
it's a doubling every 24 months

1475
00:50:54,000 --> 00:50:56,000
on that

1476
00:50:56,000 --> 00:50:58,000
so it actually is an exponential thing on AI research papers

1477
00:50:58,000 --> 00:51:00,000
when you plot it

1478
00:51:00,000 --> 00:51:02,000
on a log graph it's a straight line

1479
00:51:02,000 --> 00:51:04,000
so as this goes it will continue going exponential

1480
00:51:04,000 --> 00:51:06,000
and like I said exponentials are

1481
00:51:06,000 --> 00:51:08,000
a hell of a thing

1482
00:51:08,000 --> 00:51:10,000
we are not equipped to handle them

1483
00:51:10,000 --> 00:51:12,000
no and we've got too many, well too many

1484
00:51:12,000 --> 00:51:14,000
we're going to have the fastest pace of change

1485
00:51:14,000 --> 00:51:16,000
humanity's ever seen because of technology

1486
00:51:16,000 --> 00:51:18,000
because there's so many of these

1487
00:51:18,000 --> 00:51:20,000
all happening at the same time

1488
00:51:20,000 --> 00:51:22,000
it's a lot

1489
00:51:22,000 --> 00:51:24,000
even as everything else breaks down

1490
00:51:24,000 --> 00:51:26,000
all our systems are at the edge

1491
00:51:26,000 --> 00:51:28,000
of our stability

1492
00:51:28,000 --> 00:51:30,000
because I was like we need to reform education

1493
00:51:30,000 --> 00:51:32,000
healthcare and all these things quick

1494
00:51:32,000 --> 00:51:34,000
because we're going to see

1495
00:51:34,000 --> 00:51:36,000
this is not normal

1496
00:51:36,000 --> 00:51:38,000
what we're seeing today

1497
00:51:38,000 --> 00:51:40,000
they might say oh interest rates are 7%

1498
00:51:40,000 --> 00:51:42,000
worse than could happen

1499
00:51:42,000 --> 00:51:44,000
come on guys

1500
00:51:44,000 --> 00:51:46,000
these are symptoms not kind of causes

1501
00:51:46,000 --> 00:51:48,000
the reality is our systems are out

1502
00:51:48,000 --> 00:51:50,000
data need to be improved

1503
00:51:50,000 --> 00:51:52,000
so what happened with crypto is they tried to grow a whole new system

1504
00:51:52,000 --> 00:51:54,000
and then there's this system

1505
00:51:54,000 --> 00:51:56,000
that's made and lost

1506
00:51:56,000 --> 00:51:58,000
whereas this technology

1507
00:51:58,000 --> 00:52:00,000
because it can take unstructured data

1508
00:52:00,000 --> 00:52:02,000
to structured data

1509
00:52:02,000 --> 00:52:04,000
can sit in our systems and extend them

1510
00:52:04,000 --> 00:52:06,000
they can disrupt them or extend them

1511
00:52:06,000 --> 00:52:08,000
so it's really interesting

1512
00:52:08,000 --> 00:52:10,000
I think again the pace of change will be ridiculous

1513
00:52:10,000 --> 00:52:12,000
so how the hell would somebody like Real Vision

1514
00:52:12,000 --> 00:52:14,000
deal with

1515
00:52:14,000 --> 00:52:16,000
I mean every company in the world

1516
00:52:16,000 --> 00:52:18,000
has to change their business models yet again

1517
00:52:18,000 --> 00:52:20,000
people are changing to web 3

1518
00:52:20,000 --> 00:52:22,000
because that's another new business model

1519
00:52:22,000 --> 00:52:24,000
the application of technology at scale

1520
00:52:24,000 --> 00:52:26,000
in a totally disruptive way

1521
00:52:26,000 --> 00:52:28,000
how the hell do we all deal with it

1522
00:52:28,000 --> 00:52:30,000
how do we even get ahead

1523
00:52:30,000 --> 00:52:32,000
so the lovely thing is

1524
00:52:32,000 --> 00:52:34,000
when I go into the biggest media companies in the world

1525
00:52:34,000 --> 00:52:36,000
right now they've already prototyped the technology

1526
00:52:36,000 --> 00:52:38,000
internally because we made it so they could

1527
00:52:38,000 --> 00:52:40,000
this is a new internet that's coming

1528
00:52:40,000 --> 00:52:42,000
and so the reason I start stability

1529
00:52:42,000 --> 00:52:44,000
was to be the layer one for next generation AI

1530
00:52:44,000 --> 00:52:46,000
so my aim is to build the biggest company in the world

1531
00:52:46,000 --> 00:52:48,000
that puts this technology to everyone

1532
00:52:48,000 --> 00:52:50,000
so they can build their own models

1533
00:52:50,000 --> 00:52:52,000
or if they want the white glove service

1534
00:52:52,000 --> 00:52:54,000
they come to us and we forward deploy engineers

1535
00:52:54,000 --> 00:52:56,000
to take all the content in the world and make it living

1536
00:52:56,000 --> 00:52:58,000
and interactive

1537
00:52:58,000 --> 00:53:00,000
but you just kind of got to get into this

1538
00:53:00,000 --> 00:53:02,000
and again you know how it was when you said

1539
00:53:02,000 --> 00:53:04,000
the first bitcoin?

1540
00:53:04,000 --> 00:53:06,000
it's magic

1541
00:53:06,000 --> 00:53:08,000
sufficiently advanced technology is magic this is the same

1542
00:53:08,000 --> 00:53:10,000
when you create your first image

1543
00:53:10,000 --> 00:53:12,000
if you do something boring it's going to be like whatever

1544
00:53:12,000 --> 00:53:14,000
it's style transfer

1545
00:53:14,000 --> 00:53:16,000
but when you create your first original image

1546
00:53:16,000 --> 00:53:18,000
combining different styles and concepts

1547
00:53:18,000 --> 00:53:20,000
and the image in your head becomes reality

1548
00:53:20,000 --> 00:53:22,000
you're like

1549
00:53:22,000 --> 00:53:24,000
this is new

1550
00:53:24,000 --> 00:53:26,000
this is different this is crazy

1551
00:53:26,000 --> 00:53:28,000
it's the biggest thing of all time and it's not just for images

1552
00:53:28,000 --> 00:53:30,000
images are the

1553
00:53:30,000 --> 00:53:32,000
breakthrough wedge

1554
00:53:32,000 --> 00:53:34,000
because it's gone from 20% to 80%

1555
00:53:34,000 --> 00:53:36,000
language GPT-3 was an 80% to 90% moment

1556
00:53:36,000 --> 00:53:38,000
this is

1557
00:53:38,000 --> 00:53:40,000
99% of the world don't believe they can create

1558
00:53:40,000 --> 00:53:42,000
they can suddenly create anything

1559
00:53:42,000 --> 00:53:44,000
you got to just get in there and you got to basically find where it is

1560
00:53:44,000 --> 00:53:46,000
because what happens is that

1561
00:53:46,000 --> 00:53:48,000
there's this wonderful description of infrastructure

1562
00:53:48,000 --> 00:53:50,000
as the most efficient means by which a society stores

1563
00:53:50,000 --> 00:53:52,000
and ships value

1564
00:53:52,000 --> 00:53:54,000
the value landscape in content

1565
00:53:54,000 --> 00:53:56,000
in enterprise

1566
00:53:56,000 --> 00:53:58,000
in a lot of things

1567
00:53:58,000 --> 00:54:00,000
in information is about to be upended

1568
00:54:00,000 --> 00:54:02,000
and we don't know what the new landscape is

1569
00:54:02,000 --> 00:54:04,000
and so you got to be in there to understand that

1570
00:54:04,000 --> 00:54:06,000
what role do humans play

1571
00:54:08,000 --> 00:54:10,000
you know obviously we've got a shrinking world

1572
00:54:10,000 --> 00:54:12,000
population over time so

1573
00:54:12,000 --> 00:54:14,000
I think of robots and AI

1574
00:54:14,000 --> 00:54:16,000
and I wrote a few articles about it

1575
00:54:16,000 --> 00:54:18,000
so these are new demographics

1576
00:54:18,000 --> 00:54:20,000
but they can periferate very far

1577
00:54:20,000 --> 00:54:22,000
so does that increase

1578
00:54:22,000 --> 00:54:24,000
GDP growth and humans need to move

1579
00:54:24,000 --> 00:54:26,000
towards some sort of universal basic income

1580
00:54:26,000 --> 00:54:28,000
or different roles

1581
00:54:28,000 --> 00:54:30,000
how are you thinking that

1582
00:54:30,000 --> 00:54:32,000
I think that we need to move towards universal basic income

1583
00:54:32,000 --> 00:54:34,000
like you know this is why

1584
00:54:34,000 --> 00:54:36,000
I'm rolling this out in emerging markets

1585
00:54:36,000 --> 00:54:38,000
the education tablets create the best

1586
00:54:38,000 --> 00:54:40,000
data sets to feed models for every nation

1587
00:54:40,000 --> 00:54:42,000
so I'm going to make it to the Malawi

1588
00:54:42,000 --> 00:54:44,000
where it adds percentage points to its GDP

1589
00:54:44,000 --> 00:54:46,000
Ethiopia adds percentage points to their GDP

1590
00:54:46,000 --> 00:54:48,000
India is the number one market for this

1591
00:54:48,000 --> 00:54:50,000
because they have ATAR

1592
00:54:50,000 --> 00:54:52,000
India Stack 5G

1593
00:54:52,000 --> 00:54:54,000
and all the capital needed

1594
00:54:54,000 --> 00:54:56,000
so India is one of our biggest markets

1595
00:54:56,000 --> 00:54:58,000
by far and literally we will accelerate

1596
00:54:58,000 --> 00:55:00,000
Indian GDP and the Indians will then have a better system

1597
00:55:00,000 --> 00:55:02,000
than we have in the west

1598
00:55:02,000 --> 00:55:04,000
like already 13 months to literacy and numeracy

1599
00:55:04,000 --> 00:55:06,000
on one hour a day is better than

1600
00:55:06,000 --> 00:55:08,000
most primary you know kindergarten

1601
00:55:08,000 --> 00:55:10,000
so think about what it's going to be

1602
00:55:10,000 --> 00:55:12,000
self-learning AI system

1603
00:55:12,000 --> 00:55:14,000
it's going to be crazy but then they are equipped

1604
00:55:14,000 --> 00:55:16,000
for that and you need to have things like UBI

1605
00:55:16,000 --> 00:55:18,000
so one of the things we've done is in our country level

1606
00:55:18,000 --> 00:55:20,000
subsidiaries and we're doing just about all the countries in the world

1607
00:55:20,000 --> 00:55:22,000
10% of the shares are reserved

1608
00:55:22,000 --> 00:55:24,000
for the kids that use our education tablets

1609
00:55:24,000 --> 00:55:26,000
so they'll be getting shares

1610
00:55:26,000 --> 00:55:28,000
as they kind of grow

1611
00:55:28,000 --> 00:55:30,000
and then we're trying to figure out how UBI

1612
00:55:30,000 --> 00:55:32,000
looks like on the back of that which is insane

1613
00:55:32,000 --> 00:55:34,000
like we shouldn't have to do that

1614
00:55:34,000 --> 00:55:36,000
but nobody else is thinking about the pace of change

1615
00:55:36,000 --> 00:55:38,000
and scale of this

1616
00:55:38,000 --> 00:55:40,000
are you going to structure this as a DAO

1617
00:55:40,000 --> 00:55:42,000
or a foundation or something

1618
00:55:42,000 --> 00:55:44,000
I wanted to do a DAO of DAOs originally

1619
00:55:44,000 --> 00:55:46,000
and

1620
00:55:46,000 --> 00:55:48,000
then I realised the infrastructure wasn't there

1621
00:55:48,000 --> 00:55:50,000
and then I realised let's just

1622
00:55:50,000 --> 00:55:52,000
IPO in every single country

1623
00:55:52,000 --> 00:55:54,000
and create the next trillion dollar business

1624
00:55:54,000 --> 00:55:56,000
that's the best way to do this

1625
00:55:56,000 --> 00:55:58,000
distributed listing

1626
00:55:58,000 --> 00:56:00,000
okay interesting

1627
00:56:00,000 --> 00:56:02,000
yeah distributed listing

1628
00:56:02,000 --> 00:56:04,000
the Indian version of stability should be owned

1629
00:56:04,000 --> 00:56:06,000
by the Indians for the Indians

1630
00:56:06,000 --> 00:56:08,000
running it

1631
00:56:08,000 --> 00:56:10,000
there's an opportunity here

1632
00:56:10,000 --> 00:56:12,000
same for all these other countries

1633
00:56:12,000 --> 00:56:14,000
maybe DAOs work in the future

1634
00:56:14,000 --> 00:56:16,000
maybe you can do air drops

1635
00:56:16,000 --> 00:56:18,000
it's open right

1636
00:56:18,000 --> 00:56:20,000
but this is a cool thing this technology

1637
00:56:20,000 --> 00:56:22,000
has an exponential

1638
00:56:22,000 --> 00:56:24,000
so nobody knows

1639
00:56:24,000 --> 00:56:26,000
what the answer is

1640
00:56:26,000 --> 00:56:28,000
final question

1641
00:56:28,000 --> 00:56:30,000
because there's a lot to digest here

1642
00:56:30,000 --> 00:56:32,000
how are governments going to deal with this

1643
00:56:32,000 --> 00:56:34,000
so

1644
00:56:34,000 --> 00:56:36,000
the European Union

1645
00:56:36,000 --> 00:56:38,000
is trying to ban open source artificial intelligence now

1646
00:56:38,000 --> 00:56:40,000
because they view the edges

1647
00:56:40,000 --> 00:56:42,000
regulation

1648
00:56:42,000 --> 00:56:44,000
so authors of models will be liable

1649
00:56:44,000 --> 00:56:46,000
for the use of the models

1650
00:56:46,000 --> 00:56:48,000
the US is uncertain right now

1651
00:56:48,000 --> 00:56:50,000
the UK is massively pro AI

1652
00:56:50,000 --> 00:56:52,000
and so they've been upgrading it massively

1653
00:56:52,000 --> 00:56:54,000
in emerging markets

1654
00:56:54,000 --> 00:56:56,000
guess what

1655
00:56:56,000 --> 00:56:58,000
I'm going to the governments to tell them they can have this technology for free

1656
00:56:58,000 --> 00:57:00,000
they love me

1657
00:57:00,000 --> 00:57:02,000
so

1658
00:57:02,000 --> 00:57:04,000
you'll have most of the world loving it

1659
00:57:04,000 --> 00:57:06,000
and then Europe is a big question mark

1660
00:57:06,000 --> 00:57:08,000
US is a small question mark

1661
00:57:08,000 --> 00:57:10,000
and I think they'll be very pro this

1662
00:57:10,000 --> 00:57:12,000
because it adds GDP at a time

1663
00:57:12,000 --> 00:57:14,000
when there is no abundance

1664
00:57:14,000 --> 00:57:16,000
the question is does it then collapse GDP afterwards

1665
00:57:16,000 --> 00:57:18,000
politicians don't think that far

1666
00:57:18,000 --> 00:57:20,000
honestly

1667
00:57:20,000 --> 00:57:22,000
when I look at it

1668
00:57:22,000 --> 00:57:24,000
what is GDP

1669
00:57:24,000 --> 00:57:26,000
GDP is population growth plus productivity

1670
00:57:26,000 --> 00:57:28,000
this does both population growth

1671
00:57:28,000 --> 00:57:30,000
and productivity

1672
00:57:30,000 --> 00:57:32,000
because it is population

1673
00:57:32,000 --> 00:57:34,000
robots and AI are demographics

1674
00:57:34,000 --> 00:57:36,000
and increases productivity

1675
00:57:36,000 --> 00:57:38,000
so that should mean the pie increases

1676
00:57:38,000 --> 00:57:40,000
so per capita GDP rises

1677
00:57:40,000 --> 00:57:42,000
that's my working hypothesis

1678
00:57:42,000 --> 00:57:44,000
until

1679
00:57:44,000 --> 00:57:46,000
it replaces the humans eventually anyway

1680
00:57:46,000 --> 00:57:48,000
exactly and there's this other part

1681
00:57:48,000 --> 00:57:50,000
which is that basically the west

1682
00:57:50,000 --> 00:57:52,000
and advanced economies have borrowed too much from their balance sheet

1683
00:57:52,000 --> 00:57:54,000
from the future based on identity

1684
00:57:54,000 --> 00:57:56,000
whereas you look at India and other places

1685
00:57:56,000 --> 00:57:58,000
they haven't done that

1686
00:57:58,000 --> 00:58:00,000
so when you incorporate this technology

1687
00:58:00,000 --> 00:58:02,000
with godland entity structures

1688
00:58:02,000 --> 00:58:04,000
you suddenly have massive credit creation

1689
00:58:04,000 --> 00:58:06,000
like you've never seen before

1690
00:58:06,000 --> 00:58:08,000
because information which money

1691
00:58:08,000 --> 00:58:10,000
can flow much nicer around India

1692
00:58:10,000 --> 00:58:12,000
or fricking Ethiopia or anywhere else

1693
00:58:12,000 --> 00:58:14,000
especially because our tablets are standardized tablets

1694
00:58:14,000 --> 00:58:16,000
that we're deploying at scale

1695
00:58:16,000 --> 00:58:18,000
with a new type of bond

1696
00:58:18,000 --> 00:58:20,000
whereby you only pay based on outcomes if you're a donor

1697
00:58:20,000 --> 00:58:22,000
which is measured through the tablets

1698
00:58:22,000 --> 00:58:24,000
so my thing is like infrastructure

1699
00:58:24,000 --> 00:58:26,000
the west I have no idea what to do with

1700
00:58:26,000 --> 00:58:28,000
like I'm just going to the company

1701
00:58:28,000 --> 00:58:30,000
to say give me your content and let's share on this upside

1702
00:58:30,000 --> 00:58:32,000
but it's difficult to try

1703
00:58:32,000 --> 00:58:34,000
and fix the UK or US or Europe

1704
00:58:34,000 --> 00:58:36,000
and there's enough smart people there who can do that

1705
00:58:36,000 --> 00:58:38,000
Have an amazing conversation

1706
00:58:38,000 --> 00:58:40,000
I've absolutely loved it

1707
00:58:40,000 --> 00:58:42,000
I'm terrified and excited at the same time

1708
00:58:42,000 --> 00:58:44,000
which is what I was hoping

1709
00:58:44,000 --> 00:58:46,000
I'll definitely get you back

1710
00:58:46,000 --> 00:58:48,000
to talk more about this because

1711
00:58:48,000 --> 00:58:50,000
we barely scratched the surface

1712
00:58:50,000 --> 00:58:52,000
Yeah, no problem, like I said

1713
00:58:52,000 --> 00:58:54,000
we've tried it publicly, I think this is one of the first times

1714
00:58:54,000 --> 00:58:56,000
we're talking about actually the bigger plans

1715
00:58:56,000 --> 00:58:58,000
like I think we're recording this now

1716
00:58:58,000 --> 00:59:00,000
next week we do our big launch event

1717
00:59:00,000 --> 00:59:02,000
but everyone needs to know about this right

1718
00:59:02,000 --> 00:59:04,000
and everyone needs to participate

1719
00:59:04,000 --> 00:59:06,000
because this should be a communal effort

1720
00:59:06,000 --> 00:59:08,000
because we need to guide it together, right?

1721
00:59:08,000 --> 00:59:10,000
Exactly right, well listen

1722
00:59:10,000 --> 00:59:12,000
best of luck with everything

1723
00:59:12,000 --> 00:59:14,000
let's hope the unintended consequences

1724
00:59:14,000 --> 00:59:16,000
are not as bad as could be

1725
00:59:16,000 --> 00:59:18,000
we'll only find out but we're going down this path anyway

1726
00:59:18,000 --> 00:59:20,000
regardless so you might as well

1727
00:59:20,000 --> 00:59:22,000
tell us what the consequences are and not to the few

1728
00:59:22,000 --> 00:59:24,000
Exactly, cheers Rowell

1729
00:59:24,000 --> 00:59:26,000
Thank you my friend

1730
00:59:28,000 --> 00:59:30,000
There is so much to unpack

1731
00:59:30,000 --> 00:59:32,000
for an interview like this

1732
00:59:32,000 --> 00:59:34,000
Firstly

1733
00:59:34,000 --> 00:59:36,000
it shows that the hypothesis

1734
00:59:36,000 --> 00:59:38,000
that technological growth

1735
00:59:38,000 --> 00:59:40,000
is going exponential and it is not going to stop

1736
00:59:40,000 --> 00:59:42,000
regardless of what the central banks are doing

1737
00:59:42,000 --> 00:59:44,000
or inflation is doing

1738
00:59:44,000 --> 00:59:46,000
it's kind of irrelevant

1739
00:59:46,000 --> 00:59:48,000
when the speed of technological

1740
00:59:48,000 --> 00:59:50,000
technological adoption

1741
00:59:50,000 --> 00:59:52,000
is so rapid

1742
00:59:52,000 --> 00:59:54,000
and so truly extraordinary and profound

1743
00:59:56,000 --> 00:59:58,000
the way this is changing humanity

1744
00:59:58,000 --> 01:00:00,000
both from education

1745
01:00:00,000 --> 01:00:02,000
medical sciences

1746
01:00:02,000 --> 01:00:04,000
science in general

1747
01:00:04,000 --> 01:00:06,000
the creative industries, the media industry

1748
01:00:06,000 --> 01:00:08,000
everybody

1749
01:00:08,000 --> 01:00:10,000
this is again

1750
01:00:10,000 --> 01:00:12,000
happening on a scale like the internet

1751
01:00:12,000 --> 01:00:14,000
but maybe even more so

1752
01:00:14,000 --> 01:00:16,000
and we've got many of these technologies

1753
01:00:16,000 --> 01:00:18,000
all overlapping

1754
01:00:18,000 --> 01:00:20,000
as what I've talked about before

1755
01:00:20,000 --> 01:00:22,000
blockchain technology has been the fastest

1756
01:00:22,000 --> 01:00:24,000
adoption of any technology the world has ever seen

1757
01:00:24,000 --> 01:00:26,000
and looks like AI may even

1758
01:00:26,000 --> 01:00:28,000
exceed that

1759
01:00:28,000 --> 01:00:30,000
and then we've got robotics and space

1760
01:00:30,000 --> 01:00:32,000
and internet of things

1761
01:00:32,000 --> 01:00:34,000
and on and on and on

1762
01:00:34,000 --> 01:00:36,000
so

1763
01:00:36,000 --> 01:00:38,000
you can either fear this stuff

1764
01:00:38,000 --> 01:00:40,000
or you can use it

1765
01:00:40,000 --> 01:00:42,000
to your advantage and invest in your own future

1766
01:00:42,000 --> 01:00:44,000
and that's the route I take

1767
01:00:44,000 --> 01:00:46,000
there is unintended consequences

1768
01:00:46,000 --> 01:00:48,000
we just don't know what they are

1769
01:00:48,000 --> 01:00:50,000
and I think Emmad's point about

1770
01:00:50,000 --> 01:00:52,000
should it be given

1771
01:00:52,000 --> 01:00:54,000
to the few

1772
01:00:54,000 --> 01:00:56,000
or the many

1773
01:00:56,000 --> 01:00:58,000
to deal with those unintended consequences

1774
01:00:58,000 --> 01:01:00,000
I think he's probably right

1775
01:01:00,000 --> 01:01:02,000
that give it to everybody

1776
01:01:02,000 --> 01:01:04,000
is probably the only way around this

1777
01:01:04,000 --> 01:01:06,000
anyway, a truly extraordinary interview

1778
01:01:06,000 --> 01:01:08,000
and I hope you enjoyed it

1779
01:01:14,000 --> 01:01:16,000
with in-depth analysis from real experts

1780
01:01:16,000 --> 01:01:18,000
join the revolution at realvision.com

