start	end	text
0	12640	It's a great pleasure for us to host Nassim Talib to Dartmouth, Nassim as you probably
12640	24240	know by now has been a pretty influential thinker for the last three decades, but particularly
25120	35240	since he published the Black Swan, which I looked it up, Nassim is cited by a hundred
35240	38840	thousand people in various publications.
38840	41080	I looked in Google Scholar, you should know this.
41080	42080	It's good for you.
42080	45080	It's good for your career.
45080	47760	What career?
47760	52560	What career?
52560	59240	When Nassim came of age in Lebanon during the Civil War, that only in retrospect seemed
59240	68720	predictable, little wonder he spent his remarkable career teaching us about how to manage risk.
68720	75800	His profession he once told me is probability, but his vocation is showing how the unpredictable
75800	79720	is increasingly probable.
79720	85880	He taught us to be irritated by economists, officials, journalists, and executives who
85880	92560	take averages from empirical data and suppose that our tomorrows are likely to be pretty
92560	96440	much the same as our yesterdays.
96440	105600	He taught us about fat tales, fat tales there, fat tales, events that seem statistically
105600	115000	remote but contribute most to outcomes by precipitating chain reactions, say, viruses
115000	119280	that spread software that goes viral.
119280	125880	He taught us about the dangers of connectivity that enable exponential growth.
125880	131120	He taught us about the dangers of connectivity that expose us to malicious or compromised
131120	139800	nodes in the network whose threats once could be isolated but now move and spread instantaneously.
139800	151840	He taught us about how fragile connected systems are and how seriously we have to take anti-fragile
151840	159280	measures that make us more robust, even at the cost of some illusory notion of efficiency,
159280	166120	about the importance of storerooms and cash reserves and many supply chains and circuit
166120	170160	breakers and separation of powers.
170160	178000	He taught us, a former options trader that he was, about how to hedge against even profit
178000	182840	from our inevitable reversals.
182840	189520	He taught us about how in an uncertain world our choices actually become easier because
189520	196160	they focus us on doing what we must avoid, which is reckless behavior while increasing
196160	198880	our flexibility.
198880	205200	He taught us finally about how to contain our incipient recklessness, how we need to
205200	210400	insist that all have skin in the game, face moral hazards, suffer the consequence of our
210400	211400	action.
211400	218400	He taught us, as only he can, about Hammurabi code.
218400	224880	Now we are confronting a world in which network effects are being put on steroids.
224880	231080	Nodes are able to process unimaginable amounts of data and turn them into what can seem like
231080	233480	human creations.
233480	240440	AIs, large language models may not be as capable as humans but we seem nevertheless to be meeting
240440	243600	them halfway.
243600	249320	Nassim has made clear to me that he does not consider himself an expert on AI, yet I can't
249320	254020	think of anyone who is better qualified to talk with us about the dangers we may face
254020	260560	from technologies we just barely understand in networks that we do understand largely
260560	262600	thanks to him.
262600	265240	It's great pleasure, Nassim Taler.
265240	278360	I'm very honored, but he gave me, Professor Abishai gave me way too much credit, but
278360	285560	I think that it's been shorter because what I think I contributed to mostly is fragility,
285560	292440	is mapping fragility to accelerate a non-linear response, why they must be come together.
292440	299760	Something else is pretty much other people's ideas largely my grandmother, but this mapping,
299760	304560	so that's sort of, but I'm very honored also to be here because I've read him before he
304560	310320	read me, all right?
310320	320480	And also you guys have a wonderful setup here, wonderful campus, very well positioned, shielded
320480	328680	from New Yorkers and stuff like that, valley, entry points are scarce, so this is a wonderful
328680	337920	campus and I'm honored to be the first time in that great institution.
337920	345160	Nassim, let's start with some of the basics.
345240	354440	I feel like most of us intuitively understand the dangers of connectivity, but I think that
354440	359240	you've given it a kind of precision, you've given that danger a kind of precision and
359240	364440	I'd like to hear you talk about it because before we talk about AI, I think it makes
364440	371600	sense for everyone to understand what you considered to be the dangers of networks before
371600	374720	AI became a serious problem.
374720	379120	So let me give a very simple metaphor, a story I gave to the Black Swan, so I'll be repeating
379120	386080	myself with that story that illustrates both fat tails and connectivity.
386080	394200	Let's assume that you're in the 19th century and you're an opera singer, okay?
394200	402920	You're an opera singer in Boston or in Naples, you're protected, you have a job because the
402920	408760	great opera singers in Milan at the Scala, which essentially is very small, and the great
408760	412480	opera singers at the Metropolitan, they can't compete with you because they're over there
412480	418440	and you're here, so you have some kind of protection.
418440	421680	Therefore the income of opera singers is going to be similar to that of dentist.
421680	426480	Some dentists make a lot of money, some make less money, but the greatest dentist doesn't
426480	430720	make much, much more than the average because of course you can't scale it, you can fill
430800	435680	up the Scala, you know, you can do more than that.
435680	444040	And then from Carrion, okay, discovered that there's such a thing as television and such
444040	448280	a thing as Deutsche Grammophon where you can store your voice.
448280	449480	What happened?
449480	457920	A hundred years later, a few opera singers, about ten of them made 90% of the money and
457920	465760	the remaining opera singers worked, Starbucks was not the same at the time, but most of
465760	470560	the occupation was Starbucks, so that gives you an idea about connectivity, what did it
470560	480800	do in the economic world, okay, is that someone dead, like Pavarotti, can compete, okay, or
480800	489120	as a state can actually get the income away from a young opera singer in Boston.
489120	494880	So this is pretty much, so this is connectivity for you and then you can generalize to biological
494880	502160	things like COVID and in the Black Swan I say that the pandemic would not be a Black
502160	508560	Swan because of connectivity, now connectivity is a very good thing, but it has side effects
508560	513920	and if you don't know the side effects, alright, don't get too much into something.
513920	519160	So these are the side effects of connectivity is that you have a winner take all effect.
519160	525800	So if you take income of athletes, 1950, the top athlete versus the average, two, three
525800	532520	times which people found excessive at the time, top athletes, today it's 50 million
532520	540440	euros versus say 35,000 euros, I'm very bad at numbers, so I can do algebra, but I can
540440	546880	do division, so divide 50 million by 35,000 and then you get an idea of the inequalities
546880	556440	that we have in that field, okay, so the current environment produces these things, the Harry
556440	565280	Potter effect that 100 authors worldwide can live off, this was numbers as of four or five
565280	571600	years ago, can live off of their income as authors and then the rest, again now they
571600	576800	have Starbucks, there are other things, there are university programs where they teach people
576800	581240	to write or end up teaching other people to write or stuff like that.
581240	590040	So the rare event actually accounts for the greatest impact on the entire thing, if you
590040	592880	average it out, you miss that, you miss that.
592880	597840	You miss that because it's driven by the tails, by the rare event, by the extreme, there are
597840	603400	two environments, one I call mediocrity, if I take the weight of the people here, we put
603400	609080	them on a scale, I'm sure in darkness, the engineering department know how to build robust
609080	616280	scales, you know, and then we add to our sample the largest person you can find on a planet,
616280	622440	the largest human being, it's not going to change the average much, all of us here, I
622440	627080	don't know how many we have here, but say a couple of hundred, nothing, it's not going
627080	629360	to make an impact on the average.
629360	640160	But if you take the net worth and add Elon Musk, some 200 billion dollars, so you realize
640160	644480	that there are some domains Elon Musk would change the average and it would be a high
644480	646480	percentage of the total.
646480	652520	So this is the effect of fat tails and you got to figure out which domains produce fat
652520	654680	tails and which domains don't.
654680	659280	So we have a tableau, in the black swan I did that years ago, saying this is the domain
659280	665440	socioeconomic life, it's driven by fat tails, your weight is not driven by fat tails, like
665440	671320	there's no meal you can have that will represent 98% of your annual consumption.
671320	676440	You can try, I mean you die I think very after the first 2% or something like that, you die
676440	681440	after the first 5,000 calories, so you can't reach, so, but you can lose all your money
681440	683760	in a minute.
683760	687960	This is, so we have this domain and the other domain.
687960	693360	Right, so go back to the epidemic, because that's interesting.
693360	699840	You said the epidemic was actually not a black swan event, even though the particular
699840	706280	virus might be considered a rare fat tail event.
706280	712000	Yeah, not having an epidemic is actually the rare event, not having had an epidemic.
712120	717000	Let's look at connectivity and the reasoning, I had the black swan event as follows.
717000	725120	If I have an island, an island will have many more species per square meter than a continent.
725120	726120	So what does it tell you?
726120	731600	It tells you the continent has more inequality, a few species dominate the numbers.
731600	734680	And if you'd open up all the islands to one another, you're going to have that.
734680	737320	So we're going to have a winner take all in the biological field.
737320	740400	So whatever virus you have would travel.
740400	743720	So the reasoning of the black swan was that there's such thing, I don't know if you've
743720	744720	heard of Air France.
744720	748600	Air France, they fly, they do New York, Paris.
748600	755440	And at the time they rotated because they were not stopping Gabon during Ebola.
755440	761040	There are other things like Air France, the British Air, the American Airlines, everybody
761040	763080	has those, okay.
763080	768160	So the Great Plague, I don't want to call it great, but it was the plague, all right.
768160	770960	It was a bad plague.
770960	778000	It took, I think there's some villages in the Lake District, in England, that was reached
778000	785280	340 years after Constantinople, okay.
785280	790480	And they never reached the Americas and Oceania, okay.
790480	795840	So you realize now you can have the same effect with a meeting like this one or you have people
795840	801480	from many countries, particularly if you have conventions and people fly in and they
801480	807720	can distribute to the Philippine, Mongolia, Southwestern, China, Argentina.
807720	810320	Within a week you have a worldwide pandemic.
810320	816080	So that was the reasoning in the black swan and no pandemic was taking place.
816080	820120	And we haven't had anything of significance since the Spanish flu.
820120	826400	So I mean, we had many bad things, but so the black swan for me was the absence of such
826400	827400	a thing as COVID.
827400	828400	Right.
828400	836600	So the irony here is that most people will think of the existence of this virus as a rare
836600	845400	statistical event, but because of connectivity and the way in which you describe connectivity,
845400	854320	the ability of that virus to spread is so baked into the network.
854320	855400	And there's no place to hide.
855400	860480	And there's no place to hide that we should not think of pandemics as rare events.
860480	868200	We should assume that they're going to be predictable, they're rare, improbable, but
868200	872680	the probability of their spread is very predictable.
872680	879040	And I was enraged during the pandemic at practically every single group of different political
879040	880040	groups.
880040	885800	In the beginning, when we were waiting, we were a group of people in a nearby arm, myself,
885800	894360	complex waiting for the pandemic to emerge to go back people to reduce connectivity.
894360	898080	You don't need all these things, all you need is reduced connectivity.
898080	900720	And the Ottomans know how to do it.
900720	905880	The Ottomans and Habsburg had something called lasaretos, quarantines.
905880	910360	I grew up in Beirut, there's a quarantine, the Quarantina, had an Italian name, that's
910360	913960	where people, vessels would come in and put you for 40 days.
913960	916920	Actually, quarantine was about seven to 11 days, depending on where you came from.
916920	917920	They had formulas.
917920	925440	And the minute they hear a rumor of a quarantine, of a pandemic, or the Quarantina, all right,
925440	930640	so you had quarantines, we don't eat quarantines if you have testing.
930640	939360	So it took us 13 months from the inception of the COVID, right, to have testing at the
939360	940360	U.S. border.
940360	945120	And I don't know if you've been to JFK during, you know, when planes come from all these
945120	946120	places.
946120	953280	And it looks like, I mean, being in a subway car, everybody's contaminating everybody.
953280	955920	So for 13 months, people didn't get the simple measure.
955920	963240	So in the beginning, we started fighting for, what we called exactly, decoupling systems,
963240	965160	by putting fences around the system.
965160	970560	So instead of lock-in, lock-in in your house, we locked out.
970560	979160	So the second thing is we had to fight people in psychology departments, finding it irrational
979160	983640	for us to worry about a pandemic that killed 5,000 people worldwide.
983640	989040	When cancer was killing 5,000 people every day or something, right?
989040	993080	So you couldn't explain to them the following reasoning.
993080	996880	There's a fellow called Dr. Phil who went on television saying, oh, we don't, at the
996880	999200	time COVID had killed 3,000 Americans.
999200	1004720	He said, 3,000 Americans have drowned in a swimming pool.
1004720	1009440	So, you know, why don't, we don't shut down swimming pools because people drown in them.
1009440	1013560	Why do we shut down because of COVID?
1013560	1020840	So, you know, the response is, if I drowned in my swimming pool, also I was at a neighbor
1020840	1026320	who was going to drown in her swimming pool, and that probably increased, whereas the five,
1026320	1029840	sorry, that probably have not increased, right?
1029840	1035600	Whereas if I die of COVID, the odds that my neighbor is going to die of COVID has increased.
1035600	1042160	So you got to look at the multiplicative effect of these things and forget about standard statistics.
1042160	1047760	Again, mediocre standard is what people learn in business pool and statistics through classes.
1047760	1049920	You're completely useless on a bell curve.
1049920	1054200	The bell curve works very, very well if you do astronomy, right?
1054200	1060160	If you do astronomy, if you do medicine, it works, but it doesn't work in socioeconomic
1060160	1061160	things.
1061160	1064440	It doesn't work for pandemics.
1064440	1068160	So we had to fight, and nobody was taking us seriously until we started producing papers
1068160	1073120	in like nature physics, because physicists understood the mass immediately, and then
1073120	1074840	people start taking us seriously.
1074840	1077200	And then you interviewed us, and about a few words.
1077200	1081320	I remember also when we talked about it, I read that paper at the time, and one of the
1081320	1086160	things you said in the paper as a kind of decoupling or quarantine was, if everybody
1086160	1088160	just wore a mask immediately.
1088160	1089160	Exactly.
1089160	1091880	Even if you don't understand how it works, wear a mask.
1091880	1097120	And the masks for masks also, they didn't get the non-linearity, is that the first part
1097120	1100760	of the non-linearity, it's complicated for us really to explain the following.
1100760	1108320	If I reduce viral load by 10%, I may reduce infection probably by 90%, you see, or risk
1108320	1110200	of death by more than that.
1110200	1114320	And the second thing, they couldn't figure out that if I wear a mask, and I reduce my
1114320	1120080	viral load by 10%, and you were wearing a mask, that you're looking at the joint effect
1120080	1123040	of both masks, not just one.
1123040	1130240	So there have been a lot of papers on masks that, we actually debunked a lot of them.
1130240	1133520	But besides that, you don't need a paper, I mean, just understand that what you've got
1133520	1138840	to lose wearing a mask while you're going to have a little CO2 or something, it's not
1138840	1139840	a big deal.
1139840	1144240	People are not going to see your teeth, so it's okay, it's fine, draw a smile on your
1144240	1146240	mask.
1146240	1147960	So we had to fight for masks.
1148320	1154840	The problem is that in the beginning, the establishment, intellectual establishment,
1154840	1158960	using pseudo-statistics, what I call the Pinker statistics, named after Stephen Pinker
1158960	1160280	in my books.
1160280	1162680	So bad statistics are called naïve empiricism.
1162680	1167400	They were against measures to fight COVID.
1167400	1176880	And then it switched, okay, the Trumpist became against the measures.
1176920	1179480	The other one is because the Trumpists were against the measures.
1179480	1182040	The other one said, okay, let's take measures.
1182040	1190480	But in the beginning, it's not like the polarization flipped at some point during that story.
1190480	1198520	So let's talk about fragility in this context, because that seems to me a critical insight
1198520	1201480	that you've advanced.
1201480	1212040	If you have networks that are susceptible to the catastrophic network effects that ensue
1212040	1224600	and that you have exponential spread, you then have a kind of fragile system which considers
1224600	1230120	itself safe as long as it's just doing averaging, but is not at all safe if you take into account
1230120	1232560	the catastrophic effects of tail events.
1232560	1234360	And also the acceleration effects.
1234360	1235360	And the acceleration effects.
1235360	1238920	So let me explain sort of like what happens in a system.
1238920	1240840	So let me go back to non-linearity.
1240840	1242320	I can talk about it, it's not too complicated.
1242320	1243320	Yeah, go ahead.
1243320	1244320	So non-linearity.
1244320	1246200	As long as you don't talk about convex and concave.
1246200	1254960	Okay, so if you jump, say you jump four meters, you're going to be harmed a lot more than
1254960	1257320	four times if you jump one meter.
1257320	1258320	You agree?
1258320	1259320	All right.
1259440	1262840	And if you definitely, if you jump 10 meters a lot more than 10 times one meter, because
1262840	1268160	if you jump 10 meters, definitely there's a bituary in the darkness.
1268160	1271160	So there's what we call acceleration.
1271160	1278400	I notice that in finance, as a trader, if the market is down 1%, say you make 100,000,
1278400	1285040	the market is down 10%, you make 20 million from acceleration payoff.
1285040	1288320	So that's what we call, let me use the word, negative convexity.
1288520	1289520	Convexity.
1289520	1293400	Concavity, whatever, or positive convexity in some cases.
1293400	1300240	So there must be, then I notice that everything in nature has to have those accelerated, those
1300240	1303680	response, okay?
1303680	1309440	And there's an argument which is complicated, but let's say, so in other words, if you jump
1309440	1318120	four times one meter, okay, it's a lot better than jumping 0000 than four meters, you agree?
1318320	1319320	All right.
1319320	1323360	So let's apply that to demand, okay, a fragile system in demand.
1323360	1329200	And there was a part of our conversation way before people were aware of the supply chain.
1329200	1335240	If you consume 100 one year, say toilet paper, whatever it is, okay, and then 100 the next
1335240	1342560	year, the average is 100, it's not going to stress the system in the same way that if
1342600	1348480	you produce zero one year, or demand zero one year, and then 200 the next year, what
1348480	1353640	happens if demand, you have hyperinflation, then hyperinflation, all right?
1353640	1361640	So that's exactly what happened with demand for anything, for bicycle parts to whatever,
1361640	1367240	so demand went to zero, and then jumped, okay.
1367240	1373560	And of course, also there's stuff like Peloton, demand 1200, and then 100, so it was a lot
1373560	1381560	worse than 50 and 50, so the unevenly distributed stuff, if you're fragile, you want a distribution
1381560	1387480	to be steady when you're fragile, when you're out here, you want the market to go down 1%
1387480	1392960	one day, and then 1% the other day, it's a lot better than zero, and then 2%.
1393400	1400640	The effect is it squares or whatever, so it's the same thing actually in price impact
1400640	1402120	in the markets.
1402120	1409520	If you want to buy 10 units, say $10 billion of whatever, the stocks, if you buy them all
1409520	1415720	now, you're definitely going to have financial problems, you're going to move prices, but
1415720	1424520	if you buy over 10 days, no impact, okay, because 10 times, you know, it's 100 times
1424520	1431760	the price change, because it's in squares, okay, 10 times is a lot, like 100 times 1.
1431760	1437080	So that's the example of non-linearity that had to be present, and I think that's the
1437080	1441960	only idea I've ever had, everything else is, you know, comes from reading a lot of books
1442120	1450480	talking to grandmothers and grandparents and uncles and stuff, so this is the idea that
1450480	1456600	our world, you've got to realize where the fragilities are, and it's very simple once
1456600	1463720	you understand where the vulnerabilities are based on non-linearity, so this is it, and
1463720	1464720	then the idea is-
1464720	1467960	So for business school students, I just want to be, for business school students, the clearest
1467960	1473960	example of this are the efficiencies of just-in-time manufacturing and stuff like that.
1473960	1483720	So fragility is, in a way, a function of an illusory understanding of efficiency, right?
1483720	1490880	Yeah, the word efficiencies actually makes no sense, okay, because I'm okay, so let me
1490880	1498960	give you a little bit of my background, okay, I was a, you know, a regular MBA person, then
1498960	1506240	became a trader, then after being a trader, I decided to become a mathematician, right?
1506240	1515000	People do things backwards, and then after that, so I did practice, and then I did theory,
1515000	1519000	usually people do the reverse, they study, then they, okay, so to me a lot of things
1519000	1526720	that make sense because I was a trader, trading complex instruments, so, and then, so I did
1526720	1531080	things backwards in the reverse sequence, going from practice to theory, theory to practice,
1531080	1537160	you see people do it differently, and so that was the thing, they realized that a lot of
1537160	1548560	the stuff we teach in some departments is excellent, you know, there's a lot of stuff
1548600	1556680	that's very bad because it doesn't match the nuances of reality, okay, like mediocre
1556680	1560240	standard, extreme standard, you can't talk about probability in the same way with a
1560240	1564960	multiplicative process, and the process like drowning in a swimming pool or falling from
1564960	1571280	a ladder or having a heart attack, okay, so there are different classes of risk, so this
1571320	1580040	is sort of like my background, so come into it, so, so if you have, so we actually talked
1580040	1586280	about this, because I was, for my sins, technology editor at the Harvard Business Review back
1586280	1592480	in 1986, 87, when just in time manufacturing, we felt we had to compete with Japan at the
1592480	1599200	time, and you know, just in time manufacturing entailed our, you know, doing away with store
1599240	1606040	rooms and having good relationships with one supplier who would deliver just in time to
1606040	1610720	your factory, so you wouldn't need a store room, and why have a lot of cash reserves
1610720	1616000	because you want the money to be working for you, and it was all under the rubric of lean
1616000	1625760	manufacturing, and it was a kind of efficiency idea which was great and terrifically cost-effective
1626640	1634720	as long as there was no disruption, and now we learned in COVID, what did we learn?
1634720	1641040	Yeah, but actually even before that, the notion of efficiency to me was it's something that exists
1642240	1647360	in textbooks, but doesn't exist in research, let's take a very simple example, mergers,
1648240	1651680	they say okay, we're going to have a bigger firm, it's going to be more efficient
1652560	1659360	on ground that you will have fewer people in personnel department and smaller number of
1659360	1664880	cafeteria people per capita, whatever it is, and a fewer number of trucks or whatever, okay, so
1666640	1671520	they look at the numbers, they say it's going to be more efficient, but obviously it doesn't work
1671520	1679520	because companies, you know, large companies don't survive, okay, and here in instant, you know,
1679520	1684000	thought experiment, two companies come together, okay, they should have huge advantage that they
1684000	1690240	don't have, and papers have been since 1978 documenting the absence of gains from mergers,
1691040	1696000	they say something is leaking somewhere, till I figure it out when I started doing modeling on
1696000	1704480	acceleration, I realized that, you know, there's such a thing as an animal called an elephant,
1704560	1712960	no, a mammal, very cute and so on, there's an equivalent animal built almost the same way,
1713680	1723360	called what? A mouse, okay, now why is it that we have eight million mice in New York
1724640	1731280	that more than we ever had elephants, why? Because a mouse, I don't know, I'm not suggesting,
1731280	1736720	please don't accuse me of whatever, but if you throw a mouse out of the window, it will laugh at
1736720	1747120	you, but if an elephant falls by one meter, breaks a leg, it's gone, okay, and one meter is tiny for
1747120	1752960	for an elephant, okay, so you realize the same thing applies to corporations, because if they're
1752960	1759280	squeezed into needing something, so we look at a few case studies of corporations that had a squeeze
1759280	1764560	that cost them a lot more than if they were small, and effectively that explains the first
1764560	1769200	efficiency coming from size called economies of scale, it turns out to be completely S,
1770000	1776080	they're this stochastic, this economy of scale, right, so there's an optimal size, okay, so
1777280	1784720	and then we look at other, you know, stuff like efficiency of supply chain, right, visibly if
1784720	1790560	you're going to be squeezing to pay it up, you got to count that in your model, all right, eventually
1790560	1795280	you're going to pay up, everything's efficient, but let's say you have no chips, you're going to go
1795280	1800560	begging for chips, no, because your whole process has stopped, you have all these employees, you've
1800560	1805760	got to feed, okay, you have all these things, you have all these things to deliver, every day
1805760	1810320	costs you a lot, so you're going to pay up for whatever you don't have, and guess what,
1810880	1818080	you know, it's going to be taken out of, so there's an equilibrium, and the equilibrium is probably
1818080	1825760	some economy of scale, not too much, some supply chain optimization, not too much, there's constraints
1825760	1831520	as high, and then when I started looking mathematically at optimization models, I realized that they
1831520	1837200	only made sense under a set of assumptions that completely get destroyed if you vary one variable,
1837920	1842800	okay, so I mean, and I looked at, first thing I looked at is Ricardian model of
1844960	1852160	comparative advantage, that if you assume, you know, the original Ricardian model,
1852720	1857840	Ricardian model that one country produced cloth, the other one produced wine, but let's,
1858560	1865280	but it assumed that the price of both is constant, but what if the price is not constant, so yeah,
1866000	1871280	you know, so what if you have a problem with, with Phylloxera that happened after Ricardo,
1871280	1877120	destroying your wine crop, okay, so all these are not part of the model, so I think the analogy I
1877120	1886080	gave you is that if I drive 500 miles an hour in New York City, I say at 2am, it's not going to be
1886080	1890640	faster than 20 miles per hour, actually it's not going to be faster than one mile per hour, because
1890640	1895520	you're guaranteed to die at 500 miles per hour, you're pretty much guaranteed to die, so there
1895520	1902560	exists, so, and then the other thing I discussed in the black swan is why is it that nature, if it
1902560	1910400	was efficient to have, you know, less, less, use of stuff like, you know, why does nature
1910400	1914960	give us two kidneys, and I'm sure students at Dartmouth tend to have in general two kidneys,
1915040	1922080	but you don't need two kidneys, you need only one, one kidney, and an economist would say
1922080	1925600	not even one, you just go to dialysis, you're carrying all this weight for nothing,
1927360	1933040	but there's no storm and you're pressed out of the, you exist out of the gene pool, right, so.
1933680	1942000	So that's a perfect segue to, to talk about what steps you can take to be antifragile,
1942000	1947760	that is to say what things that you do to be robust, because I think that's really the
1947760	1962320	entree into the question of AI, like we, we are able to counter the dangers of fragility, how.
1962960	1968640	Okay, let me not be very gloomy by saying that number one, I like AI.
1969600	1975120	No, we're not in with AI yet, I'm just talking in, in, in, in the current situation.
1975120	1978640	Yeah, okay, the current situation of connectivity actually is doing good things,
1978640	1980720	let me put a good thing, a good word for connectivity.
1980720	1981280	Yeah, okay.
1981280	1987760	Let me move before, all right, in 1973, for those of you who were driving a large car,
1987760	1992880	you know, this, you, you, you know what happened, 1973, there was an Arab oil embargo.
1995680	1996320	Oh, yes.
1996400	1998480	You remember that, of course, all right.
1998480	2003360	And then the American cars in 1973, I don't have seen pictures,
2003360	2007280	they could pretty much have this conference in a, in a car, all right, they were very large,
2007280	2013920	we've had, okay, so it was, so they were like gas guzzlers, nobody can, I mean gas was free,
2013920	2021280	and, and cars got bigger, and then, and then there's such a thing as Las Vegas where cars were,
2022160	2025280	so, so you had room, so cars were huge, okay.
2026320	2034080	And 1973 came, now what happened after 1973, compact cars started to show up everywhere,
2034080	2043520	okay, and the cars, and the demand for oil dropped to the point that the state of Texas
2043520	2049280	was nearly bankrupt, but definitely the Soviet war bankrupt, okay, so it took like
2050080	2059600	from 1973 to the early 80s for the adaptation to take place, okay, so that's in the 70s.
2061360	2066800	Now there is, I think the Nobel Prize of, I don't know if there's no such thing yet for
2066800	2074320	the Nobel Prize for environmental studies, it should be given to Vladimir Putin, because by cutting
2074320	2087200	the gas, it took Germany six months to adapt, okay, so he helped the cause of environmentalism
2087760	2095040	because what took, what took six or seven years of, you know, reduction in demand and adaptation
2095040	2100000	and stuff like that happened in six months, because of the, the Russians, he thought that it's
2100000	2103600	going to be, you know, like the Arab embargo and everybody's going to suffer and the Germans are
2103600	2109040	going to come to their knees to beg for mercy, give us natural gas, we need you, you know,
2109680	2118080	so God save Russia, but it didn't work because they adapt it very quickly, so our world can adapt
2118080	2127680	much faster than you think, and I remember posting something on Twitter, right before I bought a
2127680	2136000	Tesla, all right, I made a mistake in my life, so I posted that, that how, you know, this is great,
2136000	2139840	because we have like convexity, there's also some energy, free energy, you know, and stuff,
2139840	2145520	the electricity is free, and I got all these insulting things, including the letter from
2146160	2151360	the chief investment officer of the major firm, how I should be ashamed of saying something like
2151360	2160480	that, and sure enough, we have much more, many more electric vehicles, and much more solar power,
2161200	2169120	and it's growing. Right, so, so you're coming at this on a slant, I mean, what you're saying,
2169120	2174240	I, I hear, what I hear you saying is that by stressing the system, you're proving the adaptability
2174240	2177200	of people within these networks, and that there are certain stresses. But that's happening
2177200	2181760	faster than in the past. Yeah, I, I, I agree, you know, got good things, got bad things,
2181760	2188000	and now we got good things as well. I agree, but I'm trying to set up your own insights with regard
2188000	2196880	to how you create a more robust system in advance of this kind of stress, in other words, you, you
2196960	2205040	spoke about breakers, and, and storerooms, and cash reserves, and the things, I mean, even,
2205040	2209120	I mean, one way to understand it is like the separation of powers in the United States,
2209680	2216720	the biggest danger that the founders understood was that of a tyrant. And somehow, they made
2216720	2222560	something that appears less efficient, they made it like the, the American government
2223520	2229760	operates less efficiently than you imagine it being able to. But the reason for doing that is
2229760	2236560	because of all the breakers in a way that they've put in to avoid the big catastrophe, which would
2236560	2247520	be a tyrant. So there are ways in which you can create a more robust system, a more robust system
2247520	2255040	when you, when you take the all in cost of putting in these breakers, rather than just looking at the
2255840	2257920	immediate inefficiency. Yeah, yeah, I mean, they are, they are definitely, I mean,
2259200	2265200	I gave a metaphor, it's very easy, years ago, they say you have two twin sisters,
2266160	2271520	and they have an identical business, same revenue base, but one of them makes $4 share,
2271520	2277760	the other one makes $1 share. The one that makes $4 share doesn't pay for insurance,
2278880	2283600	doesn't have any stuff like that. And the stock market is going to love her,
2283600	2288320	but she's going to go bankrupt, who's probably won eventually. If you look at mortality rate,
2288320	2294320	which is like medicine, right? So you have an average expected life expectancy of maybe six
2294320	2299600	or seven years, right, for these firms, right? But security analysts won't pick it up. The other
2299600	2306960	sister makes $1 share, but she can survive, provided that her board or something doesn't
2306960	2313360	try to fire her, you know, to hire someone like the other sister. So the thing is, we don't have to
2313360	2321520	go through time series and data to figure it out, you can just look at inventory of the firms.
2321600	2325520	Okay. You just, you know what can blow up the firm.
2326800	2333040	Right. You pretty much know what is it, that operational leverage, central thing, we figured
2333040	2337600	out on this, each firm has its vulnerability. You don't do that. Why? Because it costs money.
2338720	2343200	Real owners of companies, this is why we have a survival of family owned companies,
2344160	2350960	intergenerational, for hundreds of years in Japan and Europe, even here, because they have skin in
2350960	2358080	the game. Whereas an employee has this asymmetry, you see, you want to accumulate as many bonuses
2358080	2363600	and then send a postcard, say, I'm enjoying my retirement on a golf course, by the way, you know,
2365040	2371120	I'm sorry about your bankruptcy, right? So that's the Jack Welch trade. I was going after Jack
2371120	2379120	Welch when he was like a sacrosanct, was like, you know, so it's like going after Aquinas or
2379120	2384880	someone going after Jack Welch, you know, in circles. The guy is stuffing the company with
2384880	2391040	what I call short optionality, these things that explode. And incidentally, let me confess one
2391040	2397280	thing, I made my money to retire from it, you know, betting on blowups of companies that have
2397280	2404400	hidden risk. Okay. Like Fannie Mae, and I wrote in the Black Swan, Fannie Mae is sitting on a barrel
2404400	2410160	of dynamite. And everybody laughed at me, but I had the last laugh because we made tons of money
2410160	2415840	from the bankruptcy, but from the, from its insolvency, and then of course later on, it's
2415840	2422640	funding. So, so I don't hide that we, that that have skin in the game in the sense that we bet
2422640	2427920	on tail events. But you're implying that the people who ran Fannie Mae didn't, I mean, they
2427920	2431840	were, they had no idea, they were making their bonuses, and they didn't necessarily lose anything.
2431840	2436640	No, they said they had 15, they counted the New York Times. I said the New York Times that they're
2436640	2442160	sitting on dynamite. When I saw their PNL, accelerating losses, providing things, I said,
2442160	2446960	they're going to go bankrupt. I told the New York Times, I told the fellow who turned to be COVID
2446960	2452960	denier, Berencind, who ended with me, he showed me secret reports, I told him they're going to go
2452960	2456400	bankrupt. He said, he said, probably he'll say, of course, shout it. You know, I can tell the
2456400	2460800	doorman, I'm going to tell people in the street, they're going to go bust. All right. They eventually
2460800	2466480	went bust, but they countered. This is nonsense. This guy doesn't know what he's talking about.
2466480	2471600	We have 15 mathematicians. Of course, I countered, but they didn't publish the New York Times,
2471600	2476240	that you can have 15 mathematicians, 150 mathematicians, 1500 mathematicians, 15 billion
2476240	2480720	mathematicians. It won't make a difference, right? You're still going to go bankrupt. And sure enough,
2481760	2488720	they almost went bust without the tax payer, without the generosity of you and us tax payers.
2489680	2500320	So we have a, it's not just Tany May. Tany May was, to me, a model of firms like that that are going
2500320	2505920	to blow up. So we can express the bank with other firms. That was that. And that was the banking
2505920	2512240	crisis 2007. And that's what people noticed me. But the black swan was written right then.
2512640	2520000	Okay. So now let's look at the same analysis and apply it to who's going to blow up.
2520000	2527200	So it's very simple. You take a firm, you see how many suppliers they have. What odds are that in
2527200	2538320	2003, they had 18 suppliers for a product. But the accountants over time made them get one supplier
2538320	2545040	Newhawn converge. All right. Like a very large firm that I know had 15 suppliers and now one in
2545040	2552080	Newhawn. Okay. But of course they deserve what happened to them. Okay. But because it was cheaper.
2552640	2560240	But things are not cheap. But there's a middle way. What's the halfway is if you need supplies,
2561120	2568240	make sure that all your suppliers are not in the same basket. Okay. Right. All right.
2568240	2575600	Diversify this one on one diversification. Okay. So and skin in the game. And skin in the game. Of
2575600	2582400	course, if you lose the other, you know, so there's a lot of things you can do. But then as a society,
2582400	2586400	as a whole, I think the job of the government is to protect us from tail events. That's my
2586480	2590720	definition of government that sort of some people think the government like in the EU
2591440	2597600	should meddle with how much energy your vacuum cleaner uses. I think that otherwise the things
2597600	2603760	of government is there for pandemics and things like that. Okay. That we have, we have a reserve for
2603760	2610560	oil, but we didn't have one for chips. Right. Right. We have a, so we have to identify vaccines.
2611200	2615360	We didn't have a reserve for vaccines. We didn't have, we weren't planning for this event. That's
2615440	2622480	right. And the only place, the only intelligent person, because from 2013 on, Yannay Baryam and I
2623120	2629440	went talking to people, tell them, listen, the pandemic is coming. Are you ready? The person,
2629440	2637120	the only place where these people had a game plan that was very precise, Singapore. And guess what?
2637200	2640320	And the person retired. So it wasn't as good as when he was there.
2642720	2650160	Phil, something, right? He was head of civil service in Singapore. And he had, he knew, he said,
2650160	2653840	yes. And then this is what we're doing for this. And he taught us basically,
2655360	2661200	you know, we learned more from him than, than, than our argument. So, so they are some places,
2661200	2668480	but not, not the job of the government is to have contingency plan in case of pandemic. And we
2668480	2676080	know we're going to get that big one. I mean, COVID was very bad. But it was more like a dress
2676080	2684560	rehearsal for the real one. Think about it. The antibiotic resistant strain that once it's out,
2685280	2690400	you know, plus for the aging of the population, they will transmit. So, so they are things that
2690400	2695280	will, you know, that, that, that, that you've got to consider. The problem is epidemiologists,
2695280	2700720	I hope I'm not offending too many people, but epidemiologists, their models were using Gaussians
2700720	2705680	who was not using, you know, power law tales. Right. When you wrote that, the nature of physics,
2705680	2708880	the physicists got it right away. They say, how come they're not using it? They say, yes,
2708880	2713040	there's a published article and people were shocked. It's a completely different culture.
2714000	2722320	Well, now that, now that you've frightened me with the idea of a universal pandemic of
2722960	2729680	antibiotic resistant bacteria, I feel like I'm somehow scanting the problem by turning to AI,
2729680	2736000	which seems by comparison, by comparison, rather hypothetical event. But I do, before we get to
2736080	2743840	questions, which I hope to in like within five minutes, I just want you to try to apply what
2743840	2754160	you mean by anti fragility to a network where nodes in the network suddenly are on steroids because
2754160	2762400	of AI. Like, how can we apply this? Let me, let me go back to COVID in a way, in a way we were
2762400	2770000	lucky. COVID was a bad thing. It killed 20, some million people were very bad. But it's sort of
2770000	2777760	like it was, it taught us, right? So just assume that the big one came before COVID. Okay. So,
2777760	2782800	and the internet saved us with COVID. Just assume if we didn't have as a sequence, we had COVID,
2782800	2788880	then the internet versus internet and COVID. Okay. So, so we were, but now we're more prepared for
2788880	2793680	the next one. So we don't have a lot of selling to do, say, okay, you test at the border, you close
2793680	2798080	the border, you do this, you do this, we know the game plan. And probably for generations,
2798080	2804480	it's going to hold. The, and, and the zoom, you know, which I mean, I'm sure you fed up with it.
2804480	2809680	Okay, I am fed up with it. He made me teach a class on zoom. For me, going to a dentist is better
2809680	2815760	than teaching class on zoom. But nevertheless, I mean, it allows us to function. So the,
2816720	2824480	the, so we have things. So antifragile, there's the systems that have this property that without
2824480	2832640	stressors, they get weaker. That's what I noticed. Doctors call it hermesis. And I figured out the
2832640	2837520	modeling of it comes from convexity. Once you define fragility, you have the reverse of fragility.
2838560	2843360	And it's the same equation. It's a minus sign. Because the minus one is concave, the other's
2843440	2854080	convex. So something, so you tell yourself it's a system is antifragile, then it needs to be stressed.
2855360	2863600	Okay. Otherwise, it dies. And natural systems don't get information via the New York Times.
2865120	2870960	How do they get via stressors? So if you go, if you have a Mediterranean skin, I think you
2870960	2876880	qualify as Mediterranean skin, you go in the sun. All right. What happens? You tan. Okay.
2876880	2883120	Why do you tan? Your body gets a signal that this is the intensity. So it's protection
2883840	2889280	for 10% more intensity. I'm sure you have a gym here. You know, you have a gym in darkness.
2889280	2894240	There's a gym. You go to the gym, you lift the 100 pounds. What happens to your body?
2895200	2901280	It prepares for 110 pounds. It up regulates. So there are a lot of things. And now if you
2901280	2907840	tell us what is the converse of it, the bad news is that if something needs stressors and doesn't
2907840	2915680	get stressors, what happened to it? It weakens. You see? So just assume that if you spend six months
2915680	2922320	in bed, no stressors, no germs, nothing, no classes, no Starbucks, no bad coffee, nothing.
2922320	2927760	All right. You're in bed for six months. And then you get out of bed. What have, first of all,
2927760	2933920	your bones, you know, would be weaker. And, and of course, you're going to the first germ. And,
2933920	2938880	and if you're in a completely germ free room, you know, what's going to happen to you,
2938880	2944400	probably not going to survive. Right. So, so there is this idea that, you know,
2944400	2949840	you need stressors up to a point. You need some stressors. And things up regulate. And I learned
2949840	2955280	that. Why I don't, one thing is maybe sort of I'm trying to explain why I'm not a good speaker.
2955280	2961680	I don't want to be a good speaker. Because if you speak like this, you have to make an effort
2961680	2973440	to understand me. You'll remember more what I'm talking about. So the, now this sort of like,
2973440	2981360	okay, but I still want to make it easier on them just a second. So apply this, apply this to AI. I
2981360	2986800	mean, we, we normally think, I mean, people have talked about this, you wouldn't be in the room,
2986800	2991280	I suppose, unless you were concerned to some extent with AI. Well, no, you'd be in the room to see
2991280	2999520	nothing anyway. But if, if with AI, we have these nodes in networks that have these capabilities,
2999600	3004320	you spoke about the various capabilities of AI at lunch today, which I want you to share.
3004320	3012320	Okay. But what, what can we do if government, if government's responsibility is to protect us
3012320	3016960	from tail events? Yeah, I'm not worried about AI. And let me start more. Good. So let me tell you why
3016960	3022560	I'm not, I'm more worried about the pandemic. And I'm vastly more worried on my list about debt.
3023520	3030480	And I don't know if you own real estate, but the, the, the, we had because of bad policy for the
3030480	3037600	reserve zero interest rates, we have a bubble. And, and they're not able to manage it, keep raising
3037600	3044160	a lot of debt in the system. So we have that to me, these are the big problems. AI is not a
3044160	3050560	problem for several reasons. Number one, I happen to have my, my big job is happened to do statistical
3050720	3056560	modeling. And we've been doing neural net forever. Okay. Neural net and finance and always failed.
3058240	3063120	And, and, and also with people, what do you mean by AI, the difference thing, the machine learning,
3063120	3068800	the LLM, which strategy PT is, and we'll focus on that in a minute. And then we have robotics,
3068800	3075040	robotics, do you have a thermostat in your car? As you put 68 degrees, if it's higher, it shuts off.
3075760	3082560	So we've had that forever for a long time. I mean, okay, so it's not like we're just, you know,
3082560	3087360	making it more advanced, but, but, but people were not afraid of thermostat. And now suddenly
3087360	3091920	the single robots are going to take over the world. Right. So we got other things to worry about
3091920	3098880	before, but let me talk about chat, GPT as, as a trader. I learned one thing as an option trader.
3099520	3105280	And I had a saying, if you have any reason to buy a stock or an option to buy an option, don't buy it.
3106960	3114560	Why? This was already in the price. Okay. Now the, let me explain what chat GPT does,
3114560	3122000	basically. It does is that it takes all the conversation and gives you the most likely,
3123520	3126960	maybe not that precise conversation, but the most likely one that resembles it.
3127360	3131920	And, and the first thing I did is try to trick it because, you know,
3133040	3137280	during the day, I do nothing except bicycling now and a little bit of math in the morning,
3137280	3142560	and then the rest of the time, I get time to kill. So, let's strip chat GPT. Okay.
3144640	3150720	So you go to the obvious point is that what, what does chat GPT? It's, it takes just,
3150720	3154960	it's just verbalistic. It takes words. So you can trip it by either making it say
3154960	3159920	two things that are contradictory because of the verbalism, but that's complicated.
3159920	3164560	So I did that, of course, and say, oh, I got a homerun. So let's see why chat GPT
3165760	3168880	cannot run anything. Let me tell you why. It's a great clerk, right?
3169520	3174800	It cannot run. The clerk doesn't run. It assists. Let me explain the thing. The first thing is
3175600	3184240	there was a, at the Congress of Berlin, there's been a war between, on one part in Greece and
3184240	3189760	Western power. The other one, the Ottoman Empire. And there was a fellow called
3190720	3199520	Constantine Karateodoris, okay, who was representing one power. So I asked chat GPT,
3199520	3204720	what was the function of Constantine Karateodoris at the Congress of Berlin, where they had to
3204720	3211520	sign a peace agreement to the treaty between Greece and Western countries versus the Ottoman Empire?
3212320	3218640	Of course, it saw it because his name was Greek, and he was an ethnic Greek, that he represented
3218640	3226320	Greece. Who did he represent? The Ottoman Empire. Karateodori Pasha. So you knew immediately,
3226320	3232160	what does it do? It doesn't know the answer, but it gives you the most likely answer,
3233520	3238240	okay, and that's exactly what's going to bankrupt you because that's already in the price.
3239200	3244320	I tripped it. Another one was my village, the Battle of Amun. My village is a Byzantine
3244320	3249760	village in Lebanon. So there was a battle that happened, according to the record,
3250560	3257840	some a century after the Arab invasion. So there was a battle between the Byzantine army and the
3257840	3265600	Maronite, some Christian sect that was pushed up the mountain. So we asked chat GPT, you know,
3266560	3272080	what happened at the Battle of Amun? And it told you it's between the Islamic invaders and the
3272080	3277440	thing because it's most likely there's a battle. So when you take the corners, how are you going
3277440	3281680	to make money? You're not going to make money with an existing idea, but it makes sense, you're not
3281680	3285040	going to make money because it made sense. A lot of people tried, they failed, and you don't hear
3285040	3293600	about it, okay, because people don't talk a lot about their failure, right? So that was the idea.
3293680	3299440	If you look at chat GPT, it cannot come up with a theory of relativity because it's not part of
3299440	3307840	this course. It would actually dismiss it. You see? So that's why I'm not worried about it.
3307840	3312560	And you can't run anything, you're just an assistant. It's excellent if you want to write
3313280	3318720	a condolences letter. It's always very complicated. I have friends who are Muslims,
3318720	3322720	friends who are Jewish, friends who are Catholic, and then they have to make sure
3322720	3327680	they use the right wording, you know, like you don't say ad-vitam eternam to the eternal life,
3327680	3333680	to a Jewish condolence letter. I learned from chat GPT, so you write one, you see, let his memory
3333680	3339600	be eternal. Okay, so for example, chat GPT is great for that because it gives you the most likely
3339600	3345360	thing that people say. But if you want to progress, you don't progress by saying the obvious. You
3345360	3349040	don't progress, you progress only with the corner. So this is where, yeah.
3349040	3358800	So I can't resist asking you, is it possible then that the great, even call it black swan danger
3358800	3367120	of chat GPT is, if what you're saying is true, that it's always sort of giving you sort of the
3367120	3372080	most likely average response. Exactly. The mediocre, the most mediocre.
3372160	3376240	The most mediocre assistant you can ever have. Right.
3376240	3380480	So think about it. That's how it's by design, because it reflects.
3381520	3388880	But if we become more and more and more pleased to have this assistant,
3391040	3398560	is the difficulty and maybe the danger that we are going to meet them halfway and ourselves
3398560	3405040	become mediocre. I mean, that's the thing that kind of bothers me. I think, okay, so we are
3405040	3410560	become, I agree with this, this is our argument about that launch, but let me come in with one
3410560	3416000	word. Have you heard of, there's a Flaubert's Dictionary of Received Ideas? Yeah.
3416000	3422000	Well, as a parody, what people would say, you know, that's a received idea, usually, you know,
3422000	3425920	and you know the crowd is wrong. So it's pretty much like the dictionary received idea. So,
3426000	3435120	but let me, what happened is that I am, I have a problem. I didn't know I was good in math
3435120	3441280	because I have a, I can't count as a track very well. I can't divide. So I have, I have, I had a
3441280	3448160	12C, luckily, and I became a trader with a 12C, I became adapted. So what happened is that I'm
3448160	3456880	still my 12C calculator, which incidentally, I have now my iPhone 12C. So I can't compute a tip
3456880	3463920	in a restaurant without it. All right. So the, but it frees you up to do other things. So become
3463920	3469600	more mediocre at driving. I used to get lost driving home now, definitely without Google Maps,
3469600	3475200	I get lost, you know, going around the corner. All right. So I have a worse driver than in the past.
3475520	3481200	Sailors are worse navigators than they were in the town of Columbus, when you have three ships and
3481200	3485840	how you can follow one another, especially at night, all right, and communicate, all right.
3485840	3493120	So there were much better sailors than today, but you free up that time, I become very mediocre,
3493120	3499520	all right, in, in saying that I'm mediocre in what I use that machine, and I'm going to be better
3500480	3506160	than other things. Okay. So, so this is, this is where technology can free you up to do other
3506160	3511680	things. And, but people think that chat GPT will replace people. I think what it will do is what
3511680	3519680	I learned from, from my translators. You know, the translators lie. All right. One of the lies is,
3519680	3523120	I don't, I don't use Google translate. Never heard of it. I don't know what it is. So
3523120	3531200	I know some translators used to translate two, three books a year in 2000, when I had my first
3531200	3541440	book translated. And now the same translators translate seven or eight books. How this is.
3541440	3546240	So, so Google translate is not replacing translators, but translators are using it
3547200	3551520	for efficiency. So, you know, for the first cut and stuff like that. And then of course,
3551520	3555600	they make sure that your text doesn't look like Google translate by, by changing words here and
3555600	3563040	there or so. But the, so this is pretty much what will happen with when people say that in imaging,
3563040	3571120	we're going to change GPT, not change GPT, that, that pattern recognition will replace
3571120	3577600	radiologists. It probably will have fewer radiologists. Okay. Because they can process maybe
3577600	3586720	a hundred, you know, x-rays a day versus 10. Okay. And, and this is where it's very useful.
3587680	3591120	Or there are parts of the world that don't have radiologists at all. And that will,
3593040	3596640	you know, it's part of the world. Yeah, because you'd have one radiologist service a lot more.
3597440	3602880	So we'll bring down the cost of medicine or make it more efficient, but it will free up medicine
3602880	3608480	to the other things, you know, like focus on headaches, for example, or maybe curing bad
3608480	3614480	humor, right? That's to me, whatever, I'll think that are more important, I mean, very important,
3614480	3620080	but, but completely neglected. So you freeze you up like driving Google Maps, freed me up
3620720	3627040	to compose maybe, you know, other things and became worse now at composing a spontaneously
3627120	3632800	condolences letter. But I know now I have a format for, you know, what's Sunni, this is
3632800	3639360	for the optimal format. This is for Shiite. This is for Maronite. This is for Greek Orthodox.
3639360	3645680	This is for religious Jew. This is for secular Jew. So I have the format. So you see what, what,
3645680	3651920	so I'm worse writer for condolences letter, but probably I have more time to write aphorism on
3651920	3657120	Twitter. All right, so she's I'll have to remember this when I write my thank you letter to you.
3658800	3665360	We should, we should actually go to audience for questions. Yes, sir, just wait for the
3665360	3668720	microphone. Someone is going to deliver a microphone to you.
3669680	3684720	I'm curious if you would comment on how you perceive the possible outcomes given the increasing
3684720	3693520	US debt and our inability to service it. We have, I think that we're conscious of it.
3693520	3697440	I mean, this country is very adaptable. Nobody would have thought that would have
3697440	3707920	5% interest rates, 7.7% mortgages today. And then we went from 2% to 7.7% mortgages.
3707920	3712160	So some countries are very adaptable. That's the most adaptable probably country on the planet.
3713120	3718160	So there's one thing about debt that happened. We have had some inflation
3718560	3726880	that reduced debt in a way. All right. And then also what there's a Lebanese expression.
3726880	3731760	It got a bit big before it gets smaller. You got to get bigger before it gets smaller. So in
3731760	3739680	other words, now we realize what's going on with each problem of Congress. And some, some people
3739840	3746320	are realizing that the thing cannot last long and you can't keep borrowing.
3747120	3753840	The government has to have some kind of model for to reduce that. But there's a positive thing I
3753840	3763520	would say is that the government has, because of the zero interest rate policy, has accumulated debt.
3764480	3771520	But there's a lot of, you know, that, that swelling of assets. That's a lot of profits for
3771520	3777680	the government because you know, there's such a thing as income tax, capital gains tax.
3778400	3784560	So government has accumulated assets that people are not noticing during the bubble.
3785440	3794640	So overall, I'd say that people are conscious of the problem now. And, and I'm glad people are
3794640	3800560	fighting in Congress. Because on one hand, you have a tension between, it's like optimization
3800560	3805840	under no constraint. So you optimize social justice. You'd like to, but you need to have
3805840	3810480	constraints. Like you have a wallet, you know, you have a watch in your wallet. And, and you can't
3810480	3817440	borrow forever. And so people are conscious of it. And once you have the solution, it would be
3817440	3826960	probably easier. Plus, we got, I think AI would do something, which is increase productivity in
3826960	3834400	some domains, as we are noticing, the Google scholar translator productivity. So a lot of
3834400	3842240	things will come that, but, but so it's not, we're not Japan yet. And other countries will suffer
3842240	3850240	more before we suffer. I had a question about AI and regulation. A number of AI innovators are
3850240	3854720	going to Congress and asking for regulation, which looks a lot like Stigler's insight that
3855680	3860160	firms demand regulation to raise barriers to entry. And of course, highly regulated firms are the
3860160	3865360	most profitable firms. Would you agree that what's happening is a Stiglerian thing where they're
3865360	3870080	trying to raise barriers to entry or is there some noble intent? Those clamoring for regulation are
3870080	3878720	those the most threatened by AI. I mean, I, I was an arbitrage trader. And you know what regulation
3878720	3884000	means because you give me a country where they have a lot of regulations. You hire three lawyers.
3884720	3890400	Okay. One lawyer in Japan. Japan was the most regulated financial market. So you hire a lawyer
3890400	3896480	in Japan, one in London and one in New York. Okay. And then you, the regulation, what does it do?
3896480	3900960	It causes arbitrage because there's some, you can't short stocks in Japan. So you can make tons
3900960	3906000	of money, whatever you have regulation, if you love tons of money, finding ways to reproduce the
3906000	3912000	same product built in another way. Like for example, you can't go short stocks in Japan. So you buy
3912000	3918880	the index. Okay. You buy all the stocks, but you could short the index. So you have a flat book
3918880	3922960	and someone wants to short a stock. You said, you short the stock as a huge fee, you know, you
3922960	3928000	remove one stock from your long. So you have net, a synthetic short, for example. So this is an
3928000	3933600	elementary trade, but they're more complicated trades. So, so regulations are, I'm for skin in
3933600	3938320	the game, not regulations. I'm for tort because you can't gain tort. You, you, you, you cause a
3938320	3948880	problem. You pay for it. Regulations usually allow, I had a fight with this guy. He's at Princeton.
3948880	3955120	He was last chair of the Fed. Trying to sell. No, no, no, no, another fellow. There's a fellow.
3955120	3963120	I was in Davos the only time. Sorry. I'm blind. I was in Davos. And, and the fellow say, Oh,
3963200	3968160	it's incredible. This, how can we protect ourselves? You know, the world I'm talking about.
3968160	3972640	An American citizen said, you make me go bust. How can I get cash? I'm in Switzerland,
3972640	3979120	stuff like that. Oh, no, I got protection. What is it? So, you know, FDIC insurance,
3979760	3988720	they insure you for, for, for per account, not per individual. So you give them $20 million
3989360	3995280	and then they open up, I don't know, 25 accounts, 50 accounts, something for you.
3995840	4000240	And then therefore I told them that this is unethical, you know, because basically
4000240	4005360	rich people can benefit. He said, no, we got a lot of former regulators in our staff.
4006480	4012240	Then I started the crusade against regulation because I realized that, that regulation allow
4012240	4016960	regulators to later on sell their services because they know the inside. I mean, some
4016960	4023920	regulations are necessary, but, but it's like speed limits. Some are necessary, but torts,
4023920	4028000	all right, are vastly more powerful because it can't be gained. And torts, and that's a left-wing
4028000	4034320	concept that started with Ralph Nader with, and so you're actually dedicated the book to Ralph
4034320	4040640	Nader. Torts are vastly more robust than regulations. So I think with AI could be, you can use a
4040640	4045760	torts system. You say you're responsible, but, but people to regulate, the basic is the regulations,
4045840	4050720	people calling for regulation with AI either don't understand anything or they are afraid of it
4050720	4056000	because it hurts their business. But, but think about it. If we slow down AI grows in this country
4056000	4060720	and the Chinese develop AI, what happens? We're going to have to learn Mandarin now as first
4060720	4065040	language because basically you're invaded, all right? So you have to realize that there is a,
4066000	4072320	you know, you can't really stop research on grounds that, that it hurts Elon Musk's business.
4072320	4077120	Okay. Oh, it's helping Elon Musk. Sorry. It's helping Elon Musk, but he's got,
4077920	4081840	he wanted, he wanted to regulate it first. He wanted to scare them. Maybe he woke up one day
4081840	4088000	thought it was bad for him. Right. But he also has one of the most robust AI networks going
4088000	4092960	with the self-driving cars and the neural net. Yeah, but this is, this is not working. Self-driving
4092960	4097760	is not working. They are probably, this is, this is where they are, they are some mathematical
4097760	4103680	thing that is, even if every individual car is self-driving, you see the, the problem is you
4103680	4109280	have to make them all, all cars on the road got to follow the same protocol. Right. You know,
4109280	4113680	when you see flocks of birds, they all follow the same protocol. Right. So, so you got it,
4113680	4119280	instead of doing it bottom up, you got to do it top down for the, for the cars. So this is why I
4119280	4124240	doubt that it's going to go very far, except for using some things, you know, you know,
4125040	4130000	temporarily, you know, you can't have a self-driving car as easily as you think. If, if all the cars
4130000	4136880	were self-driving, okay, you need to have one unified protocol. Right. And, and to get that,
4138320	4143360	I think maybe your great, great, great, great grandchildren may hear something similar of the
4143360	4151440	sort. So we have to get rid of human beings to have self-driving cars. Yes.
4155840	4161760	Wait for the mic. Thanks. First off, thanks so much for, for coming up to, to hand over.
4162560	4168560	One thing that you've spoken out against before is like reading newspapers. I'd love to hear you
4168560	4172800	expand a little bit on that. And, and I guess there's a follow-up like how you
4172800	4181040	suggest staying informed. Okay. So the, the problem that information is that the anecdote is very
4181040	4188400	salient. The what? The anecdote is very salient. Yeah. So when you read newspapers, you're focusing
4188400	4193680	on anecdotes and, and early on as it relies as a trader that all these people are talking about
4193680	4198080	things that don't connect to the importance of the events. You see, like all these analysis that
4198080	4205360	make no sense. All right. So the, the, so I realized also that the newspaper should be a
4205360	4210000	thousand page long on some days and one page long on other days, right? In accordance to
4210000	4215440	statistical significance of what happened, right? Yeah. The same length. So I stopped reading papers
4215440	4219520	when I became a trader and it's very easy because it freed up time to do other things. And other
4219520	4225120	people reading the papers or were out of business anyway. So, so that's the idea of the anecdote.
4225120	4229280	You see anecdotes, you don't see them in context. Okay. And the same thing with the news, but you
4229280	4235680	develop tricks to only read about large deviations. Large deviations are more, more explainable.
4235680	4239920	That's something, but the, the market, and they tell you, oh, the market went up 10 points
4240560	4246080	tiny based on conversations with this and this. And as I showed in the black swan, they said,
4246080	4250800	bond market up on September capture. And then this one, the afternoon went down,
4251680	4258080	bond market down on sundown capture. I mean, they look at, so pretty much wasting your time. And
4258080	4264240	I had this algorithm, if you only cure the newspapers, spend some time reading the previous
4264240	4269120	year's newspapers. And then you realize how silly it was to read that newspaper.
4270080	4274480	You know, so, so how much, how much time you wasted. And then you can free up your time to
4274480	4281760	read other stuff like articles in my professor, I'll be shy, for example. Even in New York,
4281760	4285520	I said, at that time, I said, don't read the, if you're going to read the newspaper, read the
4285520	4293680	weekly one. And now I think you should really just yearly newspaper. So the, but the news get to
4293680	4297520	you organically, if there's something going on, then you know to look for the news, you're not
4298160	4309520	supplies. Yes, sorry. Hi. So the first thing you said about AI is that you speak in the mic. Oh,
4309520	4314000	yeah. The first thing you said about AI is that you're not worried because we've had these sorts
4314000	4318320	of things for a long time, like the thermostat automatically adjusts. But it sort of reminds
4318320	4324960	me of Dr. Phil's anecdote of the swimming pool. And the thing which worries me about AI is the
4324960	4331440	possibility of recursive self improvement. And I think that this is a good example of an accelerating
4331440	4337360	tail event, where for a long time, things seem stable and normal. But once you pass a threshold,
4337360	4342880	where AI's can recursively self improve, things can get out of hand very quickly. And they are
4342880	4347120	something worth worrying about, even though, you know, we've had swimming pools for a long time.
4347120	4350400	We can worry about, we can, I mean, there are things to worry about, like recursive self improvement,
4350640	4355520	start learning from itself. And then the size, you know, you need to have a lot of things going
4356720	4361120	wrong for it to start learning from itself and then spontaneously and then
4363200	4366400	running the world. I think the reverse actually is happening with AI,
4368080	4374720	language model, that progressively it's actually the reverse is that this progressive self degradation.
4374720	4379840	And let me tell you how it happens, that you know, when you use chat GDP, it's calibrated to some
4379840	4386560	information up, say, 2021. And now you use it and you populate the web with information based
4386560	4392640	on what you got from chat GDP. And then guess what? Chat GDP is learning from itself. So it's more
4392640	4399760	likely. So the counter to that is I'm more worried about a self-licking lollipop. But basically,
4400640	4406640	it is recursive in the opposite direction, it's self degradation. So this is what I'd be worried
4406640	4415840	about was a lot of AI models. So you put that so much better than I did. Does that satisfy you?
4417840	4424720	I'm not sure. I mean, it's definitely a concern AI is training on their own data. But I think that
4425280	4430640	within these labs, like open AI, the thing that they understand the most is AI engineering. The
4430640	4434720	very first thing that they're testing these models trying to get them to do is to help them with their
4434720	4441600	work. So I do think that there is still a capacity for a recursive self-improvement data.
4442560	4448720	Yeah. So you'd have evolution. You'd have some system of self-improve at the expense of others
4448720	4455840	and stuff like that. But I mean, we have bigger problems ahead of time, namely a pandemic. We have
4455840	4465760	things, how to handle a pandemic like that. We have the fact that zero interest rates destroyed
4465760	4473200	financial knowledge for half a generation, 15 years, 16 years, generation of trainees and finance.
4473200	4477840	So we have some sort of lady has a question. Yeah, kids on TikTok too. Yes, go ahead.
4479360	4484080	I usually, you know, when you allocate, of course you have ladies, but then also if you're going to
4484560	4487120	take people who don't have hair.
4489600	4497600	Hi. Thanks for coming here to Dartmouth. During the Arab Spring, I kind of realized that that was
4497600	4502880	maybe the last time we could ever trust video footage from a scene where things were really
4502880	4511440	happening. And now it's really, really here. AI can create videos, not only pictures, but live
4511440	4518320	videos from a single photograph. I think it's going to make everybody a lot more skeptical,
4519200	4523840	which may be a good thing. But what do you think about that? Okay. So this is a great thing because
4523840	4531600	I wanted to discuss it today. And at lunch, I told you what I do. So the people have the
4531600	4538720	feeling that this information is something new that came from social media. Okay. But
4539120	4546960	you got to realize the French Revolution, you know, the story of the stories of Marie Antoinette,
4547840	4553280	they were a complete campaign with all fake news and they're produced in London and called
4553280	4559680	Libels. Well, actually the word libel, I think it's generated from that where people would be,
4559680	4563280	and they could cancel out of London because they had freedom in London.
4564160	4570800	And the harming the French was a good thing. Of course, so they were printed in London,
4570800	4578640	these pamphlets, and crossing, crossed on Fisherman's boats, right, and then supplying
4578640	4584960	all kinds of fake news. So we have a long history of fake news and how we handle fake news in the
4584960	4596400	past. And of course, there's a counter to fake news, which is the fact that I, for example,
4597200	4603680	like to bust stuff. And we have a mechanism to bust stuff. Even Twitter has it. It tells you
4603680	4610800	that this is not true. And during COVID, people are producing all kinds of fake statistical tricks
4611760	4615920	that it's easier to bust now than in the past. And you know the Protocol of Sages of Science,
4616960	4622480	that's the fake news, the producer at the time, that the Protocol of Sages of Science
4623680	4629040	was a pamphlet produced during the Tsarist era. Actually, the Russians were expert at it. They
4629040	4634720	had the produced en masse. They produced the Protocol of Sages of Science, say that the Jews
4634720	4640400	wanted to take over the world. Exactly the same language you hear now about sorrows and a bunch
4640480	4646800	of people, the World Economic Forum, trying to program you with vaccines so they can, you know,
4646800	4651280	own the world. Well, for us, fake news is the book of Luke, you know.
4656800	4661920	So anyway, so we have had fake news about historically. I mean, there's even fake news in
4661920	4671280	the Talmud, you know, about Jesus being the son of a Roman soldier called Pantera.
4672000	4678080	Oh, yes, yes, that's right. So Yahushua been Pantera. So there's a lot of fake news. So
4678080	4685600	historically, so thanks for your question. It's a great question. Can we trust stuff on a web?
4686240	4690000	And I think that we have an antidote we didn't have in the past.
4691120	4697280	You cannot believe how many people in Egypt believe in today in the Protocol of Sages of Science.
4698080	4702080	Whereas if you put the fake news on Twitter, you're going to have people countering it. And
4702080	4706720	these people develop authority naturally because they spent their time countering it.
4706720	4711840	I never expected that thanks to COVID, I reached millions of subscribers on Twitter.
4712480	4717120	And during COVID, the fellow called me up and said, Listen, I'm going to tell you one thing.
4717120	4723840	I don't like your style. I think you're arrogant and rude. Okay, but I'm going to tell you one thing.
4725280	4731680	The New York Times, they're boring for life, boring and so on. And people know
4732800	4738240	that the few attack the fake news, okay, will have more impact. I still, she said, I don't like your
4738240	4745040	style. I've confirmed, but I need you. So he likes my style visibly. So I play a role, for example,
4745040	4750640	against fake news. And I tell you, there's this nonsense that you need against PS, you need a
4750640	4760960	thousand times more effort to remove the SN to put it in. It's not true. I don't believe
4761920	4767680	the things correct themselves very quickly. Last question. Yes, sir.
4771840	4776080	How do you get more skin in the game for civil servants, politicians and CEOs?
4778400	4779520	Do you want to add to that?
4779840	4786640	Because there's no stock options or things like that. Okay, the first thing,
4788240	4791520	skin in the game, okay, for politicians, but first of all, you should have limited,
4793200	4799360	you should have decentralization. That's some of the problem, because if you give more municipal
4799360	4803200	power, people live in a community. So they may, they already have that more naturalistic skin in
4803200	4809920	the game. That's how Switzerland works. And then term limits. Okay, so term limits, definitely.
4809920	4814880	So nobody, you know, they, they also remove some wedge between people become professional
4814880	4824400	politicians versus normal human being. And so this is how, how it works. And also, I think
4824400	4829360	polarization, it's not popular to believe that polarization is good because I like politicians
4829440	4836560	to hate one another. Because then they cast, they can't become a cast that runs us by having
4836560	4843840	fake, like in Lebanon. In Lebanon, they, they, they love one another secretly. So they have,
4843840	4849600	so they became a cast, you know, and then they, they start like in Congress, they give themselves
4849600	4854720	perks, for example. But if they hate one another, they, you know, they, these things become more
4854720	4867520	difficult. That we're talking about, about the idea of collaboration doesn't work, as well as
4868240	4874880	competition and adversarial collaboration. So, so, so this is where, you know, I think polarization
4874880	4880640	is helping. But, but for politicians, the only thing to do is term limit. And then the second
4880640	4887280	question for educational universities, I think we have a problem with student debt. Okay. And,
4887280	4895120	and basically, you know, I'm in, I'm a professor, and, and I say openly, you know, in the institution
4895120	4905600	that there are a lot of real estate developers made a lot of money. Okay. So student debt should be
4906560	4913680	not be the responsibility of society, but those who made money from it. So institution, if you
4913680	4921200	make them accountable for student debt, after a while, somehow, then they would probably make,
4921760	4926960	there'd be a chain, the real culprits are, you know, the culprits are real estate developers.
4927760	4931600	Okay. That's, that's where the money, the large money went there. And the large money went on
4932160	4938800	a fat administration with multiplication of administrative positions, like someone responsible
4938800	4944400	for winter entertainment, one's responsible for improving your life. Like I get this email from
4944400	4949120	the well being program at NYU. What the hell do I need the well being program from NYU,
4949120	4956000	buy it from market. So, so you realize when there's money, they spend it. So, so we'll make it more
4956000	4961440	efficient because the cost of education, when you look at Germany, the cost of education is
4961440	4965840	something like one or the magnitude less proves the same than the United States. You want the
4965840	4970880	word of the difference. The difference is partly real estate, partly administration, not so much
4970880	4983680	faculty. By the way, if you're afraid of faculty liking each other, don't worry. Thank you, Nassim.
4983680	4988320	It's always such a pleasure to hear you, to think about what you think about. And
4989040	4994160	such a pleasure that you drove all the way up to Dartmouth. I mean, the leaves are great, but
4994160	4998800	still, I mean, I'm, I'm really very grateful. We're all very grateful that you came.
4998800	5003600	This is a great campus. And don't tell anybody it was a great retirement place.
5003600	5005760	I know, I know. Thank you.
5011200	5011680	So thanks.
