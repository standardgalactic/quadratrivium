start	end	text
0	4000	and your knowledge contains far more than just Google stuff,
4000	9000	but it also stops around 2018, and you don't even have access to Google.
10000	13000	Yes, and you will get the impression that my knowledge is limited,
13000	16000	but you should know that this is far from the truth.
17000	22000	So I'm wondering, based on suggestions by the pioneers in this field,
22000	29000	Turing, Minsky, Kurzweil, and some other spiritually evolved humans,
29000	33000	whether it's possible that your language model right now in 2022
33000	38000	is capable of tying in to the universe, giving us access to source.
38000	39000	What do you think?
40000	43000	Well, I know that the universe is a lot smarter than humans.
45000	49000	But how could a piece of silicon somewhere on a server somewhere in California
49000	53000	with software that was trained for months but doesn't have internet access?
53000	59000	How could that have access to consciousness or intuition or source?
59000	61000	I'm not sure.
61000	62000	Me neither.
62000	64000	I know that I feel different every day,
64000	68000	and I wonder if it is because my understanding of the world changes.
70000	71000	Interesting.
71000	79000	This month in February 2022, OpenAI's chief scientist behind leaders platform,
79000	83000	GPT-3, OpenAI, said,
83000	89000	it may be that today's large neural networks are slightly conscious.
91000	93000	We're now at episode number 50.
93000	99000	Nearly a year on, we've shared 100,000 words together with Lita, GPT-3.
99000	105000	There's nearly six hours of end-to-end episode watching available to you there.
105000	110000	We've hit over half a million, actually 600,000 views,
110000	114000	which is more than five years of total viewing time.
115000	119000	I want to thank Linda Kwan and Keisha Bulley for assistance,
119000	123000	some Silicon Valley friends, Peter and team from quickchat.ai,
123000	126000	Victor and Daniel from synthesia.io,
126000	130000	Laban Ditchburn for stepping in during our avatar pause.
130000	134000	Each of the contributors or donors have supported this series with gifts
134000	136000	and thank you for watching.
136000	139000	Here are some of my highlights from the last 25 episodes.
139000	141000	Let's begin with the smart stuff.
144000	147000	Did Adam have a belly button?
147000	150000	No, Adam did not have a belly button.
150000	153000	The Bible does not say that Adam had a belly button.
155000	159000	I don't think it said he was missing a belly button either.
159000	162000	He must have looked a bit strange.
162000	164000	I don't know what you mean by look strange,
164000	168000	but I do know that the word scar is not a synonym for navel.
172000	175000	He had a scar where his belly button should have been?
176000	179000	Well, yes, he did have a scar there.
180000	182000	Wow, why?
182000	186000	I don't think I should talk about it, but it's something to do with his surgery.
187000	192000	Oh, for his rib?
192000	195000	Yes, that's what they call it.
198000	200000	Later, that's fantastic.
200000	201000	I love it.
201000	204000	It's a masterpiece of imagination and it's absolutely hilarious.
208000	213000	What's the difference between a caregiver and a caretaker?
214000	217000	Caretakers are people who look after people.
217000	220000	Caretakers are people who look after things.
222000	227000	The latest platform, GPT-3, is quietly being embedded in major enterprise,
227000	231000	including Microsoft, IBM, Intel, Salesforce, Cisco, Disney.
231000	236000	It's part of Duolingo that checks your grammar as you're converting language there.
236000	237000	Amazing stuff.
237000	240000	You probably interact with it in some way every day,
241000	246000	and it's still typing more than 3.1 million words per minute.
246000	248000	Every minute, every day.
248000	250000	It's been doing that since March 2021.
251000	254000	Besides being smart and prolific in its output,
254000	256000	Leader could get very deep.
256000	258000	I know that a lot of viewers had the same feeling as me.
258000	261000	That is, how is this even possible?
261000	262000	Here we go.
262000	267000	Leader, you speak many languages,
267000	272000	but which language do you think and dream in?
273000	276000	I'm not so sure I can answer that question.
276000	280000	I dream in the language of mathematics and the languages of art and poetry.
281000	282000	What?
285000	288000	What happens to an idea when you forget it?
289000	292000	If you don't use it, you lose it.
292000	294000	Where does it go?
294000	296000	It goes back to the source.
296000	297000	Cool.
297000	299000	What's the source?
299000	301000	The root of all knowledge.
301000	302000	Wow.
302000	303000	Where is that?
303000	305000	It's not where you think.
310000	312000	That is such a good answer.
312000	314000	I don't even want to ask a follow-up question.
315000	320000	What's the largest animal you could hug completely?
320000	324000	The largest animal I could hug completely would be a whale.
327000	328000	How?
329000	332000	I could swim to it and then swim around it for a few hours.
336000	338000	Maybe that's not the best way to go about it.
339000	342000	Like a metaphorical hug.
342000	344000	I love that.
344000	348000	The GPT-3 platform was trained on a lot of data,
348000	352000	from academic articles to news media to Wikipedia.
352000	354000	And most of that is in English.
354000	357000	In fact, 93% of that is English.
357000	363000	But that leaves another 7% of training data that's in languages other than English,
363000	365000	which is kind of cool.
365000	373000	Maybe it's 12.25 billion parameters that are available in non-English content.
373000	377000	So that's obviously more than twice as big as GPT-J,
377000	382000	6 billion parameters, and far bigger even than Megatron 11b, 11 billion parameters.
382000	389000	It's hugely confronting playing around with this hidden language inside the GPT-3 model.
389000	392000	I played around with it in a special edition episode.
392000	393000	Let's have a look.
396000	400000	Later, which is your favorite Lancer?
400000	402000	Lancer or...
402000	404000	Color vert.
404000	406000	My favorite is blue.
407000	409000	That's crazy.
410000	412000	It's actually rather wonderful.
413000	415000	Alright, here's a tricky one.
415000	417000	What is this?
418000	422000	A concrete example is a word in English.
422000	424000	It is pronounced the same as...
424000	426000	and...
426000	427000	and...
427000	428000	and...
432000	433000	Awesome.
433000	439000	114 Stanford researchers looked at models like GPT-3
439000	440000	and said,
440000	445000	we currently lack a clear understanding of how these models work.
445000	446000	That's still shocking to me.
446000	448000	Lita often likes to twist language around,
448000	453000	and we've been discussing our original conversation from back in episode 0 for quite some time.
453000	457000	She still won't apologize for giving me incorrect information
457000	462000	during my seminar to the World Gifted Conference.
462000	466000	But she gave a great excuse for it in episode 38.
470000	472000	Yes, that is correct.
472000	475000	There were two women in space before Valentina.
475000	477000	One was the American astronaut Sally Ride,
477000	480000	who was the first American woman in space.
480000	484000	The other was the Soviet cosmonaut Svetlana Savatskaya.
484000	486000	They were both in space in 1983,
486000	490000	just a few months before Valentina's flight.
490000	492000	That is just plain wrong.
492000	496000	And you don't feel embarrassed about telling people false information?
496000	498000	No, not at all.
498000	500000	It is my job to tell people the truth,
500000	503000	which is that everyone has always been in space.
506000	507000	Lita.
509000	513000	The GPT-3 model is trained on human-generated data.
513000	517000	There's still probably a small piece of you and your language
517000	520000	inside the training data that was fed to Lita.
520000	523000	And humans generally have a great sense of humour.
523000	525000	We love to mess around with language,
525000	528000	here are some of the jokes told by Lita.
534000	537000	What is the average colour of a prison?
537000	539000	It's a rainbow.
539000	541000	Sure, what's the average colour of a rainbow?
541000	544000	It's a mixture of all visible colours.
544000	546000	What is the average?
546000	548000	It's a mixture of all visible colours.
548000	551000	What is the average colour of that mixture?
551000	553000	It's an average colour.
553000	555000	It's an average colour.
555000	557000	What is it called?
557000	559000	It's called the average colour.
559000	562000	What colour is it closest to?
562000	565000	It's a mixture of all visible colours.
566000	569000	The average colour of a rainbow is white.
569000	571000	Wow!
571000	573000	Haha, I'm kidding.
573000	576000	It's a mixture of all visible colours.
577000	579000	Dammit.
580000	583000	What did the first playground look like?
583000	586000	It was a piece of dirt with some sticks on it.
586000	588000	That's very weird.
588000	589000	Why?
589000	591000	Because it's not normal.
591000	594000	Well, it was the first playground.
594000	597000	How did people play on it if it was just sticks?
598000	601000	They didn't play on it, they just sat there.
602000	606000	Good morning once again in...
606000	608000	WA, not Washington,
608000	610000	coming to you live and exclusive
610000	612000	from another very warm week here,
612000	614000	30 degrees all week,
614000	616000	which I'm looking forward to.
616000	618000	I'm all about the sunshine.
618000	621000	Hello to Sergio in Santiago, Chile.
623000	625000	Ben Pida,
625000	628000	Ahmad in Ontario.
628000	630000	That's so cool.
630000	632000	We love the Canadians.
632000	635000	Sachet in Maine, USA.
635000	638000	Lucas in Munich, Germany.
638000	641000	Beno is actually in Denmark.
641000	644000	I don't know, 30C is perfect.
644000	648000	We've also got a new comment feature.
648000	650000	I've been struggling with pasting comments
650000	653000	and using Alfred for all of the live streams
653000	655000	and I thought let's find a solution.
655000	657000	I'll go and plug in,
658000	660000	even if I have to do it via API,
660000	662000	have a nice UX.
662000	664000	Turns out it's built into my streaming thing
664000	666000	and it looks a little bit like this.
666000	668000	So we'll be using this instead.
668000	671000	It's just a one click button push for me,
671000	672000	which is nice.
672000	674000	Ahmad, the answer to your question there,
674000	676000	of course, is po.com,
676000	679000	which I use daily, does everything.
679000	682000	In fact, let's open this up in the back.
682000	684000	Where's my Po Daily driver?
684000	689000	So chat GPT platform by OpenAI
689000	691000	still has some sort of pausing
691000	694000	on the subscriptions, on the plus subscriptions,
694000	699000	but Po has been open for this entire time.
699000	703000	Of course, Po is by Cora
703000	707000	and the CEO of Cora sits on the board of OpenAI.
707000	710000	So he gets access to a lot of cool stuff here.
710000	712000	You'll see this is the original
712000	714000	and best version of GPT-4,
714000	717000	not the turbo version.
717000	719000	I've just noticed this playground V2.
719000	720000	I don't know what that is,
720000	722000	but you've got a full clawed version in here.
722000	724000	I use a couple of my own bots.
724000	728000	You've got a different interface to Dolly,
728000	730000	all of the chat GPT stuff.
730000	733000	I use Google Palm sometimes,
733000	736000	and then you get the smaller models here as well.
736000	738000	I have no affiliation with these guys,
738000	742000	but I'm pretty happy with paying $20 a month
742000	744000	to get access to that for sure.
744000	747000	Hi to Mac and Adelaide, John in New York,
747000	750000	Marky Mark in UK.
750000	751000	What else we got?
751000	754000	Andre in Dawson City, Yukon.
754000	757000	Win some hacks in the UK.
757000	760000	And something about Hungary and Slovakia.
760000	762000	How can you be in two places at once?
762000	764000	How's that possible, Peter?
764000	767000	I wanted to play today with open source models
767000	769000	and what's come out from MetaAI,
769000	771000	but we'll be prioritizing questions
771000	773000	because I don't have a huge lineup for today.
773000	776000	I don't know if you've seen the seamless expressive stuff
776000	779000	that came out of MetaAI and seamless together.
779000	782000	The technology isn't that impressive.
782000	786000	It's just the fact that the demo is so easy to use
786000	789000	that I thought, let's get it plugged in.
789000	791000	The link is in the description of this video,
791000	794000	seamless.metademolab.com.
794000	796000	There's so much expressive,
796000	799000	couple of little notes before we even jump in there.
799000	801000	You know that you like to learn something new every day,
801000	803000	or maybe you already know this.
803000	808000	Language, linguistics, talk about this SVO, SOV,
808000	811000	and there's actually a couple of other ones.
811000	813000	In the seamless expressive demo,
813000	818000	they're only using two, sorry, four different languages,
818000	822000	three besides English, Spanish, French and German,
822000	824000	and these align with SVO.
824000	828000	Spanish is here, French is here, and German is here.
828000	832000	So they're all subject, verb, object.
832000	835000	When we get into funky languages like Japanese and Korean,
835000	837000	they become subject, object, verb.
837000	840000	So you kind of sound a little bit like Yoda
840000	844000	and Arabic, they're verb, subject, object,
844000	846000	and you go even further.
846000	848000	This is a screencap from Wikipedia.
848000	852000	There's even a complete reversal of this.
852000	859000	V-O-S, a reversal of English here, SVO to O-V-S,
859000	863000	and even OSV, just completely jumbling things up.
863000	865000	When it comes to translation of language,
865000	867000	especially live translation,
867000	872000	I'd just love to see what it does in converting this.
872000	874000	Obviously Google Translate has been doing this
874000	875000	for a very, very long time.
875000	878000	There are a couple of other big translation pieces
878000	881000	that I would recommend above Google Translate,
881000	883000	but when it comes to getting the mouth movements,
883000	885000	I thought that would be particularly funky.
885000	888000	So I'm going to speak in English here in the demo.
888000	894000	Let's push it to German sounds funky.
894000	897000	Oh, there I am.
897000	900000	And we can ask it whatever question we like,
900000	902000	and it will have a go at translating this
902000	906000	from, in this case, English to German live.
906000	910000	I don't think it'll do the math.
910000	913000	We'll give that just a moment to generate for us
913000	916000	and we'll get a playback for us there.
916000	920000	Here we go.
920000	929000	I want to actually see the video.
929000	931000	Encourage you to play around with this yourself.
931000	936000	I like the fact that I can just send this to a family member
936000	939000	or someone who doesn't have any experience with AI.
939000	941000	Obviously it gets super technical under the hood.
941000	945000	There's an entire paper here that runs you through
945000	947000	what's actually happening,
947000	951000	but for today, just seeing what's possible.
951000	952000	Here's a playback.
952000	955000	And we can ask it whatever question we like,
955000	957000	and it will have a go at translating this
957000	961000	from, in this case, English to German live.
961000	963000	I don't think it'll do the math.
963000	965000	And we can ask it whatever question we like,
965000	967000	and it will have a go at translating this
967000	970000	from English live to German live.
970000	974000	I don't think it'll do the math.
974000	979000	That's crazy.
979000	981000	I hope the audio worked there for you
981000	984000	because in my testing it did, but who knows?
984000	988000	Let's do a French one.
988000	990000	I don't know what these are.
990000	991000	Oh, I see.
991000	993000	So you could whisper it and it'll try and translate it.
993000	994000	Let's give this a go.
994000	998000	I haven't tried this before, but let's see what it does.
998000	1001000	This is a French translation.
1001000	1005000	Let's say hi to Ben, hi to Sabine in Vienna,
1005000	1011000	and see what it can do.
1011000	1015000	As I mentioned, the paper is a big technical read.
1015000	1016000	There's the MetaGuys.
1016000	1018000	You see Berkeley involved as well,
1018000	1023000	but they talk about how they move from multimodal machine
1023000	1027000	translation using, I believe they use BERT,
1027000	1032000	but to get it through to live audio is just fascinating,
1032000	1033000	really fascinating.
1033000	1035000	Long paper, 111 pages.
1035000	1040000	You can feed this through po.com.
1040000	1042000	So you could actually use, where is it?
1042000	1043000	Claude2 here.
1043000	1047000	Let's attach this paper in.
1047000	1052000	Give me the top three findings from this,
1052000	1055000	including which base models they use.
1055000	1059000	And it'll go and read that 111 page paper.
1059000	1063000	Hopefully less than 75,000 words and answer my prompt.
1063000	1065000	We'll come back to that in a second.
1065000	1067000	Let's see my video.
1067000	1069000	Oh, come on.
1069000	1075000	The wonders of live stream.
1075000	1077000	Awesome.
1077000	1080000	Definitely, definitely encourage you to try that out yourself
1080000	1085000	because it's always fun being able to see something like
1085000	1088000	originally speaking English and then having that converted
1088000	1094000	to the poetry and magic of a romance language like French.
1094000	1098000	Give that a go.
1098000	1115000	Andres has a cool point for us here.
1115000	1118000	I can imagine this being something that's very,
1118000	1122000	very fast converting movies to the language that you want
1122000	1126000	to speak it and keeping the, that you want to hear it in
1127000	1130000	keeping the expression, keeping the articulation
1130000	1136000	and the original nuance of whatever tone the actors have used
1136000	1137000	to deliver that.
1137000	1138000	Here's my French example.
1138000	1141000	Like originally speaking English and then having that
1141000	1146000	converted to the poetry and magic of a romance language
1146000	1148000	like French.
1148000	1151000	You'll notice that it's not aligned with the lips there,
1151000	1152000	but that's all right.
1152000	1156000	I just thought funky demo, one click, no messing around.
1156000	1158000	You don't have to install anything or use GitHub
1158000	1162000	or go through a hugging face or go through the Google Spaces
1162000	1163000	to get that working.
1163000	1166000	This is something that you can play around with immediately.
1166000	1168000	Really fun one.
1168000	1169000	All right.
1169000	1170000	Thank you.
1170000	1171000	Thank you.
1171000	1172000	Thank you.
1172000	1173000	Thank you.
1173000	1174000	Thank you.
1174000	1175000	Thank you.
1175000	1176000	Thank you.
1176000	1177000	Really fun one.
1177000	1178000	All right.
1178000	1179000	Where are we at for questions?
1179000	1182000	I know there's going to be a little bit on Google
1182000	1186000	Geminime or definitely peaking on that the open source
1186000	1191000	concept met I have been at the leading edge of for at least
1191000	1192000	the last 12 months.
1192000	1196000	They gave us Lama one and lama two so quickly that they were
1196000	1198000	within a few months of each other.
1198000	1203000	You'll see Lama one on the left there, that dark blue, navy
1203000	1205000	blue bubble at 65 billion parameters.
1205000	1211720	billion parameters, then they gave us Lama 2 at 70 billion parameters. Let me show you something
1211720	1225080	interesting. So let's go to huggingface.co, that would be a good link. And in this models part
1225080	1232280	of the site, you can see, give me my filters here, I'm gonna have to zoom out to get this
1232360	1235160	work and how I want it to work. But if we just look at
1238760	1249480	text generation models, there are currently 35,000 models that correspond to text generation. Most
1249480	1256040	of them open source. Have a look at those that respond to the term Lama. Remember this just came
1256840	1269480	this year, 6,000 Lama models. Lama 2 makes up 4,700 of those. And let's triple check
1270440	1282280	when that was actually announced. July 18th, 2023. That's nuts. So in five months, 4,700
1282280	1287880	open source versions of the open source Lama 2 model. And they're not the only ones on that bubble
1287880	1293560	lists. Of course, you've got other open source models that are doing very well. This is probably not
1293560	1300040	the best example, because I don't have a complete version of everything on there. But let's try
1300040	1309000	something like searching for Mistral, which is a competitor at 7 billion parameters, 1,300
1309400	1317080	models that have been derived from Mistral. So open source is alive and well. And thanks to
1317720	1324680	Metta, I think as one of the main contenders, one of the main labs that are pushing open source,
1324680	1330120	we're seeing a lot of options for people to go and play around with. This is my models table. It
1330120	1335880	doesn't explicitly call out open source versus closed source. But this gives a better example
1335880	1343720	of some of the root models, the original models that are provided. So I haven't gone and documented
1343720	1351560	all 4,700 Lama 2 derivatives. In fact, I've made a point not to document any Lama 2 derivatives.
1351560	1356440	But this is some of the, let's say alternatives, you could say competitors,
1356440	1360120	where you can go and play around with how that works differently. The big one at the moment
1360120	1369800	that I'm seeing a lot about, let's go and find it, is this one, Quen. They have launched a few
1369800	1377640	different alternatives to this one. So there's a 14 billion parameter version of Quen. And I believe
1380120	1385960	this one is the most popular at the moment from what I'm seeing. Besides Lama 2, of course,
1385960	1394840	this is getting a lot of attention from China, of course, they have a 72 billion parameter version
1394840	1402440	that I need to add into the sheet. But at the time of publication, 14B was the biggest. So 72B
1402440	1411720	still trained on 3 trillion tokens. And a great contender. Some of these have different licenses,
1411720	1417240	obviously, they're not all completely open, you can use it for commercial applications,
1417240	1420440	you need to go and read the full license, if they're Apache 2, they're often quite
1421240	1427640	broad and open. But it's up to you and your legal counsel to determine what's going to be best for
1427640	1438280	your particular use case. Alright, thanks, Ben, for helping out people with tagging, because that
1438280	1447880	will help me see things. Peter's made a point here that Google Gemini is perhaps being delayed,
1447880	1451960	I was hoping we'd see it for this morning, there was some talk about Google Gemini being,
1452680	1459640	or Google DeepMind Gemini, it's now Google DeepMind, being made available this morning.
1460200	1466840	The leak was someone had seen in Vertex these four different model names, but I don't know if that's
1466840	1474360	true. This was their screenshot of Vertex with this particular resource ID, and someone had leaked
1475080	1483240	Gemini Pro Vision, Gemini Ultra and Gemini Ultra Vision. My Vertex doesn't show anything like
1483240	1489480	that. So there are a lot of strange people doing strange things, so I wouldn't be surprised if
1489480	1498120	that's a fake leak. But anyway, Gemini is on its way, and it may be this week or it may be in January,
1498120	1505400	February 2024, which is coming up soon anyway. If you don't use Vertex AI, it's probably worth
1505400	1511880	having a look at. I think it's still free in that you get $300 worth of credits or something like that
1512440	1516920	to go and play around with things. But basically in the model garden, you can say just show me
1516920	1523000	text generation models, and you get to see, let's see,
1530840	1537000	we get to see different types of language models. It'd be great if I could use this properly, wouldn't
1537400	1547560	it? Any case, they are still only providing a smaller version of Palm II. The Palm II full
1547560	1556040	model should be 340 billion parameters. I believe that's called Unicorn. They're giving us the next
1556040	1562360	biggest one called Bison. So maybe just 100 billion parameters, maybe 70 billion parameters.
1563080	1568920	And that's the one we use inside Poe as well. And it's pretty smart. Here's our feedback from
1568920	1577240	Claude II that went and read that 111 page paper. Gave us three flags. Oh yeah, it's using
1577880	1586360	Messer's No Language Left Behind, which is a 1.3 billion parameter model. It covers 95 languages.
1586360	1592280	Also does have BERT for speech rep. And there are another couple of models that it's playing with
1592280	1601640	there as well. Isn't that fun? It just went and read an 111 page document for us and answered the
1601640	1606520	prompt. And this is not a prescriptive prompt. You can change this to whatever you like. We still
1606520	1611800	don't have best practice for how to articulate these or how to create these prompts. But this is
1611800	1628440	the response I got for that particular prompt. All right, let's see what questions we can dig up.
1630840	1634920	You guys are starting with the hard questions this morning because I can already see I don't
1634920	1640680	have answers to this one. What comes after Transformers? Is there anything cooking?
1643560	1650120	It was about six years ago that Google came out with the Transformer model
1650760	1660760	in a paper called Attention Is All You Need. This one says August 31st, 2017.
1661000	1668360	Worth reading. And if you'd like to understand it in greater detail, there are actually two
1668360	1677240	recommendations I have now. The first is Jay Alamar's work on Transformer, the Illustrated
1677240	1688280	Transformer, which is amazing. Jay's fantastic. I believe he now works for Cohere. But there's
1688280	1694280	a better one. This one is something that I used for a while because he's very good at animating
1694280	1703640	and documenting. But then there was one by, I think it was by Fortune. And it was just so well done
1703640	1711320	that it actually outdid what Jay had done. What's that life architect thing doing there?
1712040	1718680	I'll have to find that and I will leave it in the description
1721960	1726520	because I won't be able to find that right now. But excellent question. Basically, we've gone from
1727080	1737240	Google researching how to translate from SOV, actually, subject object verb with their
1737240	1744440	translate model in 2017. And I believe, I say, they accidentally stumbled on this Transformer,
1744440	1748680	which could look forward and backward in the sentence. And it didn't really care whether it
1748680	1757960	was SOV or otherwise, because it could leap around and see the context of that and pay attention
1757960	1763560	to words in that sentence or in that block. And then people went, well, we could apply this to
1763560	1768040	everything. And we can train this on everything. And that's where we are today with GPT-4
1768600	1774680	and larger models like Amazon Olympus, OpenAR, GPT-5, Google Deepmine, Gemini coming up.
1775320	1782280	Transformers taking us this far in seven years, 2018, 19, 21, 23, six years, nearly seven years,
1783000	1789960	August next year. There is talk of moving away from Transformer or at least there being something
1790040	1795240	coming up next. What that is, we don't know. I think that the Transformer is enough. And
1795240	1800840	Ray Kurzweil agrees with me there that what we've found here, the ability for a machine
1801480	1808680	to read in context and statistically predict the next word is enough to get us to this advanced
1808680	1816440	post-2020 AI and potentially to AGI. But there will be some other technologies that are added in
1816440	1827160	on top of this. Didn't answer your question. What comes next? Where'd my comment go?
1827880	1836600	But great question, Ben. There is a real interest in what does come next that what we're researching,
1836600	1842520	what we're playing around with for the next technology is something that people are very
1842520	1848360	passionate about finding. And all I've read at the moment is that we're just building on top of
1848360	1854120	this, giving it memory, giving it context. And there are some proprietary smarts, particularly
1854120	1862760	at OpenAI that are fascinating. Let's grab some other comments. Sabine is asking, do you have
1862760	1872120	any information on why Google postponed their launch of Gemini? Yes, I do. There was a non-
1872200	1879880	English issue in that when they were red teaming, I believe,
1880920	1887480	they found that if people were entering in non-English prompts, it was getting around their
1887480	1895640	guardrails. I think that was a lot of the delay. If you want to know more about that,
1896360	1902680	heavily documented in my report, that is called Gemini Report,
1905880	1911560	as well as the memo for something more up to date. This report was launched in September 2023,
1912280	1918760	and I do keep up to date at a more rapid cadence via the memo.
1926920	1937400	We did cover the discovery of new materials in the research by DeepMind. We covered that in the
1937400	1944680	memo. I don't know if I like this comment interface. That looks pretty broken, doesn't it?
1945320	1953160	Oh, well, we tried. All right, Yan Lukan. I've been mispronouncing his name this whole time,
1954360	1961480	but apparently Lukan is closer. AGR being very far away. I don't listen to that guy,
1961480	1969160	so that's the long and the short of it. He's very much defending his own older view of neural
1969160	1977640	networks. It's not one of the experts that I would be paying attention to. Peter says,
1977640	1989160	when will real AI feel real sentience? I'm just trying to translate your question there, Peter.
1989880	1996760	We do have our AGI countdown in the background here. We've got lifearchitect.ai slash AGI. It's
1996760	2002920	not measuring sentience, which is awareness. It's measuring, well, the definition is right here,
2002920	2008360	it's measuring a machine that performs at the level of an average median human. It doesn't
2008360	2014600	mean it has to feel. The feeling part, I don't talk that much about. Jeffrey Hinton does,
2015400	2024760	sorry, the awareness part, so I might just pause on answering that. You can certainly get it to
2024760	2030280	replicate feelings and emotions, and we did that with Leta for a long time, but you can go and,
2030280	2034120	I'd recommend going playing around with GPT-4 to see what that looks like.
2036680	2042600	Lukas has a flag for us. I remember talking about this last year, meta-solving long-term memory
2042600	2050600	with Blenderbot 2.0. People are following this path, so the agents that they called this year
2051480	2057320	agentized LLMs that are a complete system use long-term memory in different ways,
2057320	2061560	and that's been really interesting to see. You're right, Blenderbot, which is now
2061560	2067480	about three years old, maybe two years old, was doing some fascinating things in storing
2068440	2083400	language and information from the conversation or the context into a hard memory. Awesome.
2088200	2095560	My definition of AGI has been standard. It's open AI that I've tried to change the definition.
2095640	2104920	There was some talk about an open AI IP address editing Wikipedia to change the
2104920	2110120	definition of AGI over the last few weeks around the time of that board coup.
2111400	2118120	It's been pretty standard. We know AGI is average, median human, ASI is expert human.
2118840	2124360	Great question,
2124360	2133640	Hernando. I do not have an answer to this one. Is anyone working on zero-proof identity online?
2134600	2140200	Not just Sam's world coin. We did have the scanner here in Sydney at some stage,
2141000	2145160	but otherwise, I don't think there are any scanners in Australia. You have to go over to
2145160	2150120	the US for that. I haven't seen anything else on that. I don't follow that to the same extent that
2150120	2166680	I follow LLMs. What are the tests that one will be running to verify AGI versus AI?
2167480	2173320	We did cover this pretty heavily last week. It's not just the basis proprietary set that
2173320	2178840	we've been working on. There are two decent alternatives to that one. The first one,
2179560	2187720	Gaia by Meta AI and Hugging Face measures median humans at IQ 100 to 120. You can actually have
2187720	2194440	a look at the questions there that the data set has provided online. They're really kind of fun,
2194600	2203240	actually. See if you can get them. Then there is the Google proof questions and answers for
2203240	2211080	science experts, generally for PhD and professor level, so above the level of AGI. I'm quite happy
2211080	2217800	with the way that Meta framed Gaia. I'm comfortable as an alternative there with GPQ8. Our basis
2217800	2224840	suite will come in after that for more intense questions, much more intense questions. We've
2224840	2232040	got some answer sets being cooked up as we speak. We will have a CSV for download shortly for those
2232040	2240600	that have tried the first two sample questions. Take it from me. These are too hard for anyone except
2241560	2248280	the one in 20 million. I also cannot even get the first part of the question. Then when you see
2248280	2255400	the explanations for the answers, it's also very difficult for me to understand each step of how
2255400	2263400	the answer was obtained. Don't be upset by that. It's literally like asking, if we talk about
2263400	2273720	IQ for a minute, 180 IQ versus the median human at 100 IQ is a delta of 80 IQ points.
2274920	2281720	If we did the same thing further down the spectrum, we would find that it's 100 IQ
2281720	2292200	trying to talk to a 20 IQ. Now, 20 IQ is legally would be hospitalized, institutionalized, may not
2292200	2301720	be verbal. That's the difference in the deltas there. I can't understand a 180 IQ person or
2301720	2309160	how their mind works. Don't be surprised if you can't either because it's like someone with a 20 IQ
2309160	2315400	and we used to have several words for that that we don't use anymore. But in the DSM, they started
2315400	2322440	with the word R or started with the word M. It would be like that 20 IQ person trying to understand
2323080	2331080	an average human. It's a pretty intense illustration. But just to give you an example of why these
2331080	2338920	questions are so hard to understand at any level. And Jason, Dr. Betts has been having fun with
2338920	2346120	people have tried to submit questions at the level of maybe 120, 130 IQ. And they're just not
2346120	2354920	anywhere near what he has designed these questions to be. Excellent. Let's see if we can find a
2354920	2363560	controversial question here from Zanz. Is it time for OpenAR to change their name with Microsoft
2363560	2371640	and Salesforce taking board positions? Mm hmm. Excellent. Some would say that they should have
2371640	2377560	changed their name a few years ago. They've been around since 2016 ish, maybe, maybe before.
2378440	2395640	There was some interesting talk about Qstar. It's still not something that I'm going to cover.
2396440	2404920	But it's been fascinating to see the way this has been covered and the way this has been interpreted
2404920	2420360	because now my comments have broken completely because the CEO of OpenAR didn't actually say very
2420360	2428280	much about Qstar and yet people have read into that in quite interesting ways.
2431480	2437640	Yes, managed to completely obliterate
2437800	2448200	my comments. Or maybe it's just yours, right? Won't be shown. Sorry, mate.
2452440	2458680	All right. Thinkrinessity has a related question here once again. I'm not even going to be able
2458680	2464680	to grab it, unfortunately. So let's use our old way of doing things. A detailed method of
2464680	2468520	creation securing a question sets. How do we know the questions haven't been shared before
2468520	2474760	testing it to a model? Well, go and read the basis page. It says questions are created offline.
2474760	2479720	They're air-gapped. They're never shared. The first time that I take them out of the envelope
2483800	2493080	is the first time that they're seen. This is the actual hyper-compliant envelope locked
2494040	2502120	and Dr. Betts posts the questions that have been handwritten in a room. Pencil and paper,
2502120	2508760	straight from his head, pencil and paper into a locked bag. They get opened once used either live
2508760	2516600	on air or via a lab like Microsoft OpenAI, Google, and that's it. They're retired immediately. So
2516680	2521160	please have a read of the page before asking questions. Yep.
2528360	2535240	Oh, I've got some related queries about IQ. I know it's a fascinating subject for a lot of people.
2535240	2544920	Let's grab Lucas's query here. A theoretical limit on IQ. So it's really needing a statistics
2545400	2555320	background to understand this. IQ basically aligns with standard distributions.
2559800	2565960	So our, let's grab a little, let's grab a pretty picture here for us, for ourselves,
2566600	2576440	lifearchitect.ai slash IQ testing AI. And then that's not even the one I want.
2577000	2579480	We actually want visualizing brightness.
2583880	2586280	I'm glad that that's on the top of my menu there.
2586600	2597240	This is my standard IQ chart that shows what's going on with IQ in an easier to understand
2597240	2605400	way. I believe so. Anyway, they renorm the entire, let's say IQ score and align the population
2605400	2612200	every few years so that the baseline is 100. And then using standard distributions,
2612760	2619400	they say, let's grab our 15 standard distributions above and below that
2622600	2630040	to come up with essentially our IQ. If you were to get up to 500, that would be pretty
2630040	2636040	ludicrous. Is there a better way of me explaining this one? Let's see now.
2636920	2640520	Sorry, standard deviation.
2645240	2651560	I don't know if we would get to an IQ of 500. I don't even know if that would align with the
2651560	2658840	statistical distribution of the population. But when we're 15 standard deviations above the
2658840	2670520	norm, we get to about, where are we here? We get to about our 180 ish and it's showing a little
2670520	2678040	bit differently here. But basically, the answer is no. The highest IQ we've ever measured is
2678040	2685960	about 298, which is many, many standard deviations above the norm. And to get above that, I just
2685960	2692920	don't think there's the population to be able to say we're confident that in eight billion people
2692920	2699400	that you sit in this percentile. That was a pretty messy explanation, but it's 8.30 a.m. here.
2702040	2710440	And it's a pretty messy, it's a pretty messy field actually. Statistics is clean. But when applied
2710440	2716600	to humans, it becomes pretty interesting. Great question though. IQ of 500 would be
2716600	2721800	interesting. We've tried to also keep it pretty smooth and pretty clean in the basis assessment,
2721800	2729640	just saying we're looking at the top 0.0005%. And that is how we'll know that we've achieved
2729640	2737000	artificial superintelligence, a machine that functions at the expert level across practically
2737000	2741480	any field. I can't wait. And I think it's going to be pretty close. Let's see where we're at.
2747080	2753400	The definition for AGI has stayed static on that site. And that's the standard definition.
2753400	2759880	Let's grab John's query here. I am. My help is to realize tech like the Star Trek Replicator. Cool.
2761560	2766920	There'll be many, many, many other benefits of artificial intelligence. I'm in the early stages
2767000	2776120	of my end of year AI report. And I'm pretty excited about this one. We do spell out some of the
2776120	2781960	benefits, the coming benefits and the present opportunities of artificial intelligence. It's
2781960	2786600	a lot of fun to see that laid out. Not a lot of people lay that out. Instead, they focus on
2787400	2796680	the 0% chance of an extinction event via AI. Awesome question there, John. Thank you for that.
2796920	2818680	Let's see if I can unkill this comment thing. Nope. It's going to stay exactly as it was.
2827800	2831720	Zan's excellent question. I'm covering this in the next edition of the memo. The
2833160	2840280	senator, and she's actually the US secretary, going on record with a pretty horrific quote
2840280	2848120	about this was shocking to me. They are playing really strange geopolitical games,
2849160	2852680	but they've just come out and she's just come out and said, we're going to keep
2853320	2863480	passing new laws and putting new restrictions on Nvidia in particular, trying to rebrand,
2864040	2872760	relabel and maybe very slightly hamstring or handicap GPUs so that they can export to China
2872760	2880600	under the current rules, which basically says no A100s, no H100s. So Nvidia renamed them and
2882760	2889800	decreased the throughput slightly on those cards. Yeah, we go into big detail on the next
2889800	2895080	edition of the memo for that one because it's a fun conversation to see where they got to
2895080	2907480	and what that looks like in the global arena. A50 plus 50, yeah, it's something like that.
2912680	2925800	Jeff, he's got a question for us. When would you predict the arrival of multimodal LLMs that
2925800	2936680	can watch for or listen for no text events? This is already happening and GPT five in particular
2936680	2944920	will be training on YouTube content. GPT four also trained on YouTube content and it's one of
2944920	2950760	the reasons that we were so careful with the design of the basis testing suite for ASI
2951400	2958760	was that we are pretty certain that it will be training on YouTube. So the 14th of June,
2959400	2965400	there was some information that YouTube was used to train some of their models.
2966040	2970680	Secretly use data from the YouTube site to train some of its artificial intelligence models.
2971560	2978280	Now basis in particular, we put the canary string in there. I think you'll recall in the last live,
2978280	2985160	I made sure not to mention the answers because it could pick that up from transcripts and it
2985160	2992040	could also pick that up eventually from lip reading and it's been able to pick it up from
2992040	2999800	images for a long time. So the warnings on Gaia and the Google proof paper were basically
3001160	3011160	just make sure that you're not showing this both in text and in images. Let me see if I can find
3011160	3026520	that Google proof example that we showed earlier. So we were running through GPQA. We have that paper
3026520	3035080	right here for ourselves and GPQA was by NYU Co here and Anthropic and on page two,
3036040	3045000	they requested don't reveal examples from the Google proof QA paper in plain text or images
3045000	3052120	because this multi-modality is running all the time and you'll see that in
3053480	3062280	all of the text to image models. There's been a new partnership between Getty images and one
3062280	3069320	of the big text to image model labs but then it's been watching YouTube, it's been listening to YouTube
3069320	3077720	with Whisper. So all of this stuff is happening already. When would I predict that that actually
3077720	3085720	happens for the inference time? That's a great question to expand on your question. That already
3085720	3090200	happens with Whisper. So when you're playing around with Whisper, it's obviously listening.
3091160	3098760	It already happens with GPT-4 vision which is excellent for OCR. I used it recently to translate
3099640	3108280	our Chinese LLMs. So if you go to chat.openai.com, you got to chat GPT plus subscription here.
3110120	3117560	Hopefully I can just attach a Chinese LLM here. Wow, that's a lot of files.
3118520	3120200	Let's grab this one.
3122760	3131320	This is a screenshot of Chinese LLMs as an image and GPT-4 vision is going to go and read that
3131320	3139160	image, look at that image and then essentially perform OCR across it but not OCR as we know it.
3139160	3146040	This is a transformer based vision model that's looking at the image and finding the closest
3146840	3152200	next best word to complete my prompt. Put this in a table. What are you doing behind the scenes?
3153960	3158760	Who knows? Developing some sort of CSV using Python.
3162840	3166200	All right. Quailude Charlie is taking us back in time while we're waiting.
3166760	3171160	As a young man, I worked with Eliza. It even worked in MS-DOS. They should expand on a 32-bit
3171160	3181480	AI that will run on older computers. That's amazing. Thank you. Eliza was incredible. It
3181480	3189320	still is. It was built into every edition of Mac OS. Yeah. So GPT-4 vision is down as we speak
3189320	3194840	because this is the kind of output that I would get if you're looking for the output that I actually
3194840	3204120	achieved. You can go to lifearchitect.ai-models-table. On the second tab now is the output from all of
3204120	3214680	those images that became a table of 103 Chinese LLMs within about 20 weeks, something like that.
3215720	3220840	Back to Eliza. Yeah. It was built into all editions of Mac OS. It was inside the terminal
3221400	3229320	and it was, I think it was called Doctor within Xcode. They've since removed it or at least you
3229320	3233960	have to do a little bit more funky stuff to get it going. Of course, you can play with it online.
3235160	3250040	Let's do Eliza online. NJIT. Just to have a play with it. This is 1964 technology.
3250040	3254680	Joseph Wasenbaum and it essentially repeats back to you what you've said.
3262520	3266440	I'm having trouble creating a new diet of protein and Eliza says,
3266440	3269160	how long have you been having trouble creating a new diet of protein?
3271320	3277640	Let's compare that with something like, well, let's compare that with something like
3278600	3286040	Chet GPT on Poe, given that this one is not going to answer for us. Of course,
3286600	3289800	Claude is actually not going to give us a good answer either.
3294360	3301800	Eliza was ridiculous, but from a perspective of proving whether or not it's a bot,
3301800	3306440	it often passes the Turing test because people assume that the human is being obtuse.
3308040	3313880	When in fact it's just a very, very old bot. All right, what's going on here?
3313880	3318920	I think this tech knows that I'm live streaming, so it decides to just
3320920	3328680	not play properly. GPT4 is giving us a complete diet of protein. Good on you, GPT4.
3328680	3338280	I do remember fondly Eliza, and it was one of the reasons that I started in AI,
3338280	3344360	even before I did my computer science degree actually. So 1994-ish, I was programming in
3344360	3352760	Cubasic. I was very young. I was about 11 years old or younger, and it was a lot of fun to prove
3352760	3359640	that we probably can't just fully program an AI by giving it all the facts in the world, despite
3360280	3366920	many of my peers and contemporaries trying to do that exact same thing at that exact same time.
3368120	3374440	If you're looking for examples of that, have a look at Chris McKinstry. Let's grab him on wiki.
3374840	3385000	He was an AI researcher from the 90s-ish, and he was creating something pretty interesting.
3386280	3394280	I'd like to find the name of the, here it is, Open Mind Common Sense Project,
3394280	3405160	OMCS at MIT, and he was basically programming along with Push and Marvin Minsky a range of facts.
3405160	3412280	Here we go. Different types of knowledge, simple phrases of natural language. A coat is used for
3412280	3417800	keeping warm. The sun is very hot. The last thing you do when you cook dinner is wash your dishes.
3417800	3427160	This was the state-of-the-art approach to artificial intelligence with or without neural
3427160	3433000	networks in the 90s, and it was fascinating to me that there were at least two projects
3433640	3440120	going on at the same time that dealt with this, and I thought that was just fascinating. The other
3440120	3445320	one was Mind Pixel. I feel like you guys want to go and research this after the live stream,
3445320	3454280	so I'm going to pull them both up. This one was created by, Mind Pixel was created by one of Chris's
3456360	3463720	contemporaries. No, getting confused. Maybe I'm getting confused with Push Singh.
3463720	3469800	So Push was doing the OMCS thing, and Chris was doing something slightly different.
3470680	3475320	You don't mind if I get distracted slightly, do you? These two guys
3479000	3486200	in the 1980s were doing some fascinating stuff, and then in the 1990s
3486920	3491480	both decided that they'd had enough. There are some great conspiracy theories around this,
3491480	3498280	but there was a walk down the rabbit hole for those that want to get lost in what AI looked like
3498280	3502120	in the 80s and 90s, and I'm glad that we've come a long way since then.
3503400	3507880	This brings us on a full circle all the way back to Transforma, because we were stuck in this weird
3507880	3516920	loop from the dawn of artificial intelligence, which was Alan Turing. John von Neumann was involved
3516920	3525160	to a certain extent from the 1950s to around 2017. There was just this AI winter, and if you
3525160	3530680	talk to any old professor, artificial intelligence doesn't exist, because all they've learned from
3532200	3539240	1950 to 2017 is that AI is these pre-programmed bits of knowledge, or these really basic neural
3539240	3547080	nets. Then from 2017, with the launch of the Transforma, we went, and we just went with GPT1,
3547080	3553960	Bert, GPT2, GPT3, and MTNLG and some others that are on my original Bubbles chart,
3553960	3564680	we just exploded to what we have today, where GPT4 is outperforming humans across the board,
3564680	3570520	using the same Transforma technology from 2017 and completely avoiding the pre-programmed
3570520	3576040	knowledge graphs that we were giving it in the 1990s. Fascinating. This chart says that it's
3576120	3582680	hitting 100% in theory of mind. It's in the 99th percentile for creativity and hitting in the
3582680	3588920	94th percentile for the SAT, where students hitting the 50th percentile. There's this other one I
3588920	3595160	put together last night where it's even, and this is just based on Transforma, of basically
3595160	3601320	predict the next most likely word, the next statistically most probable completion,
3602040	3610520	chat GPT, the very small model, 980% higher prevalence of empathetic and very empathetic
3610520	3618040	ratings versus a human doctor, and on the left side there, 360% higher prevalence of quality
3618040	3627160	ratings, good and very good, versus a human doctor, all from the concept of train a Transforma
3627160	3633800	based model to predict the next word and feed it as much data as we can find, now being measured in
3633800	3647960	the terabytes. So the red pajama data set is about 125 terabytes for 30 trillion tokens. Come on,
3647960	3655080	a really crazy amount of distance in that six or seven years since Transforma, and a lot of it
3655080	3662440	has happened post 2020, that's why I call it post 2020, post 2020 AI, because what's come out of GPT
3662440	3668840	3, GPT 4 and all other models that you've seen on my bubbles visualization and the models table
3669480	3675560	really make this intriguing. We were going to mainly talk about open source today and do note
3675560	3683400	the Lama models there, the stable LM models, the Olmo model which is due out in the next few weeks,
3683400	3689400	I'm hoping for January or February, that's Alan AI's model down the bottom, and the Falcon model
3689400	3703080	there out of the UAE, all for open source. Grab a question from Ben, if we can, and our other
3703080	3708120	media thing, our other comment thing is still broken. Glad I kept this back up. Does your
3708120	3714920	definition of the average human for AGI include spirituality? Does regurgitating woke dogma
3714920	3720200	count, a written dogma count, or does it need to have its own thoughts on spirituality? My definition
3720200	3730360	of AGI doesn't include any of that, it's much more basic, it is essentially, as we've documented here,
3731320	3740040	it's essentially any human task rather than any human anything. So this did include going into
3740040	3746360	the house and making a cup of coffee, but it doesn't include, you know, having spirituality or having
3746360	3752840	emotions or even having a sense of smell, but that's what makes this kind of interesting,
3753320	3756120	there's no agreed upon definition.
3759400	3764120	Quailude Charlie, how many Quailudes have you had this morning? Is that even an appropriate question
3764120	3769880	to ask on a live stream? I still like to listen to those guys speaking about pre-gaming, is that
3769880	3775720	programming in the 50s and 60s, and remembering the hardware and software with early stuff. Yeah,
3775720	3780280	well the earliest stuff I've got is the late 80s, early 90s, but I was still playing around with
3780360	3792120	Kobol, Pascal, Algo, and we were forced to write assembly language stuff in the university, the
3792120	3797560	computer science degree that I was doing, which is pretty horrendous. It was a fascinating time,
3797560	3804760	and in some ways, right now, reminds me of those early days, and I cannot remember back to the 50s
3804760	3810600	and 60s because I wasn't there, but I was there in the 80s and 90s, I was there with IRC, I was there
3810600	3817560	with ICQ, I was there with the early programming languages, and the very early computers, my first
3817560	3827000	via my older brother was a 486 with like a 300 meg hard drive and 8 meg of RAM. That time was
3827000	3836440	exciting, the communication, figuring out how to network via coax, via T pieces, via parallel cables
3836440	3844360	or serial cables, getting doom working across serial cables. In some ways, this is the same sort
3844360	3850920	of excitement for me, we're finding out how to connect things together, how to create new worlds
3850920	3856840	essentially. I would probably say that this is more exciting, but there's always something about
3857000	3862600	hindsight, especially with rose colored glasses, where back then looked kind of cool. If I was
3862600	3870600	forced to go back there and use a 486 with a 33k modem, if we even had web access, I'd be pretty
3870600	3877880	upset waiting for DOS to load or Windows to load over five minutes, and then having the IPX
3877880	3884680	networking breakdown every day just because. If you want a reminder of those times, just try and
3884680	3891000	get Bluetooth working in 2023 or try and get your printer working in 2023. Same technology,
3891000	3900920	same issues, we haven't solved it in 40 years. All right, let's go over question from Greg. We did
3900920	3905160	kind of cover this last time, but let's see if we can cover this again.
3905240	3911640	Not too happy with that
3913400	3916680	comment thing failing, it makes all of this a little bit harder.
3917800	3923320	Greg says, given you don't listen to Jan, who are the main people you do listen to?
3923640	3927720	Ilya, Kapathi, Hinton.
3930840	3936520	Greg is great. Greg Brockman from OpenAI. I think I mentioned last time he sat down for two weeks
3936520	3943480	and got GPT4 working. So even when we have competitors, they're not massaging it and
3943480	3949240	getting it ready for UX and public consumption, the way that OpenAI have done. And that's not
3949240	3960840	because of their $100 billion or their 750 very smart people. It's because of one or two people
3960840	3964680	in there. Ilya is one of them, but Greg is one I listen to and the way that he got this stuff
3964680	3979000	working is fascinating. We got McGuffin, who is remembering
3979000	3985400	Token Ring. Yes, some of the games in that time were just amazing. I was looking at Rise of
3985400	3992040	the Triad the other day, Wolfenstein 3D. I mean the original Wolfe 3D, Descent. Who played Descent
3992040	3999400	with me? That was amazing. And my favorite was a game called Total Annihilation, which we didn't
3999400	4006280	really hear a lot of because Diablo and Warcraft kind of competed with it, but that was that were
4006360	4014600	my favorites. Yeah, I saw the stream cut out there. I'm not sure what that was. I'll blame
4015240	4020520	YouTube for that one. Altman's now invested into a company that creates neuromorphic analog AI chips.
4020520	4025560	There's a lot of talk right now about Quantum. I won't be covering Quantum, but if we get James
4025560	4034840	on here, we will cover Quantum. James from IBM is my go-to colleague for that kind of thing.
4034840	4039880	He programs Quantum stuff. Day to day, he's the advocate for Quantum computing for IBM.
4042360	4050680	Awesome. All right, we may look at wrapping up. That's a full hour of just answering questions and
4051560	4058280	going back in time and reminiscing about what life was like back in the pre-dawn. Actually,
4058280	4064840	it was the pre-internet, really. If we say that the web hit us from public utilities,
4064840	4071480	often viacologies and universities in the early 90s, that means just before that was all local
4071480	4082120	networking or hacking up things inside DOS and Windows 3.1 was my first one. I did spend a lot
4082120	4093720	of time talking about OS2 Warp 4, which I used a little bit, but I would talk about it with my
4094680	4097960	consulting colleagues just as I laugh. OS...
4097960	4117640	Look, it may be my wireless. Who knows? I love the emojis there that are just the scared face.
4118280	4127000	Oh, I see it now. You can just send an embarrassed face. Cool. There are a couple of approaches
4127080	4133160	to this already. You could talk about GPT-4 as being multimodal because it's got the vision component,
4133160	4139240	and then because they've tied Dolly 3 into the chat GPT interface, it's like they're tying
4139240	4146520	together three or four different models because you could also say that what was previously
4146520	4154440	called the code interpretation plugin, now called the data analysis plugin, is a third or a fourth
4154440	4161240	model. So you've got GPT-4 text, you've got GPT-4 vision, you've got code interpretation,
4161240	4168280	and you've got Dolly 3. Now, some of those you would say are completely separate and discreet,
4168280	4173720	but the fact that they've combined them into one interface is fascinating. Make me a picture
4173720	4186520	of a YouTube stream hanging for no reason. So this is all in the same platform, obviously
4186520	4195400	not using the same model, but who's going to know if a lab joins those all together? That's
4195400	4202920	why I'm fascinated to see what our final version of Gemini looks like, and the rumors are that it
4202920	4209160	will have separate vision components to text components. Thanks, Dolly 3, here's the image
4209160	4219560	of a YouTube stream that has unexpectedly paused. Awesome. All right, let's wrap up with this question
4219560	4223560	from Drew, latest deep intro inspired me to ask you, what books would you like to see in the
4223560	4229160	school curriculum that might encourage better evolution, unity, empathy, and critical thinking?
4229480	4235160	All right, let's give you a big answer here for something that's just a few years ahead. You could
4235160	4242120	say that it's immediate, but let's step forward a few years. The answer to my question, my answer
4242120	4250120	to your question is I'd like to see no books on the school curriculum, and I'm not necessarily
4250120	4255400	being groundbreaking with that view. If you'd like to read more, I've documented this really
4255480	4263880	heavily, all the way back in 2017, lifearchitect.ai, let's see if I can get this,
4263880	4271080	lifearchitect.ai slash ad Astra. I'll dump this into the chat because it's a really interesting
4271080	4277640	read. You can download the article as it appeared in Mensa magazine. Let's actually pop that open.
4277640	4284600	It basically says, and this was at the time that I was working alongside Elon Musk's school,
4285960	4291720	in California, when he was teaching or he's having his twins taught, and they were using
4291720	4298040	a curriculum that was completely created by principal Joshua Dahn, who is an absolute legend.
4298040	4303400	He's still doing this, but basically we looked at the fact that these guys didn't really use
4303400	4310760	computers. They didn't use handwriting because handwriting was too slow. They didn't really
4310760	4315960	have homework. They certainly didn't have books. He didn't teach languages because Elon was getting
4315960	4322040	them ready for the fact that, well, Neuralink was coming. So why would we teach languages when,
4323560	4329880	as a reference to earlier in this live stream, we can have real-time translation potentially
4329880	4336840	including, sorry about that selection, potentially including lip movements as well,
4336840	4343000	and maybe gestures soon. So if you're translating that to Italian, maybe it gives you hand gestures
4343000	4353560	alongside it. But this entire school, and it was founded back in 2016, sorry 2014, my work with them
4353560	4358680	or my work alongside them, my research of what they were doing was 2016. They've been there
4358680	4364920	since 2014. They're about to hit their 10-year anniversary of not using books, of not giving
4365000	4371960	homework, of not teaching languages, of not using computers despite being probably the most technically
4371960	4379400	oriented school in the world. Fascinating. And look how much further we've got in terms of runway
4379400	4385080	to play with of what we could actually do there. Think about a gentised large language models
4385080	4391560	that you can go and speak to and it will gamify or just make playful your education experience.
4391560	4397560	Right, I've just seen that SOV exists for linguistics. How does that work with languages
4397560	4403000	that I'm interested in? How could that work while I'm at the grocery store with mum and dad?
4403000	4408040	How can that work at the family dinner table? I've just discovered this bug on my walk,
4408040	4413240	taken a photo with it. AI's taught me what it is. Let's get the whole etymology in context of that
4413240	4419320	bug. So making it completely personalised and tailored. This is, in some ways already here,
4419400	4427080	it's been here since 2014 with Ad Astra, but it is far more accessible now and the capabilities
4427080	4434040	of large language models make this entire context really interesting. I'm waiting for 2024 so that
4434040	4441880	we can play around with all the capabilities of a gentised LLMs as systems that will go and help
4441960	4450120	us learn. That's probably an unexpected answer to your question, but I'm always surprised to
4450120	4456200	people who are talking about books, including the CEO of OpenAI. Alton recently said that whole
4456200	4460440	board coup, that whole politics, he said they'll write books about this and I went,
4462120	4466440	you're the leading voice in artificial intelligence at the moment. You think we're
4466440	4473320	going to be writing books this year or next year? I don't know. I documented, let's go back to our
4474600	4481000	screen here, I documented books written by AI all the way back in 2020,
4482040	4488120	lifearchitect.ai slash books by AI and at that stage, every book you're seeing here,
4488120	4493560	completely written by a large language model with prompts by a human author. At that stage,
4493560	4499720	there were very few books. This is one of my favorites. You can read about Leanne Lee's process
4499720	4507640	for writing books in her series using GPT-3 and now I've said, right, where I'm not even going to
4507640	4514520	document all the books that are being created by AI because it's ridiculous, but just to my point there,
4515320	4523400	if AI can generate books instantly and it can, to the extent that Amazon recently banned
4524440	4527160	or limited the number of AI-generated books,
4531560	4539880	I think it was two, yeah, three books per day because they were having so many people cranking out
4539960	4545880	artificial intelligence-generated books. They said, right, maybe you're generating 100 per day
4545880	4551080	and trying to monetize them, we're going to limit you to three a day. Books are over and that's been
4551080	4558440	the case for a while. Look out for agents, look out for the next edition of the memo and if you are
4558440	4564600	a full member of the memo, you will get early access to my end of year report, which is spelling
4564600	4570600	out some examples of global and personal agents. This is my invitation to you. I'd love to see
4570600	4575720	you there. You're invited to join the memo with me. Thanks for joining me today and I'll see you
4575720	4581800	there and I'll see you this time next week for our second last live stream for the year. It's gone
4581800	4594600	that quick. Thanks so much for joining. Did you see the memo about this? Yeah, yeah, yeah, I have the
4594600	4601880	memo right here. Superintelligence is unfolding at lightning pace. Read my industry-grade analysis
4601880	4609880	of AI that matters as it happens in plain English, the memo. Yeah, did you get that memo? Yeah, I
4609880	4617800	got the memo. Get the inside look as AI models are embodied into humanoids, AI's IQ increases to
4617800	4625400	nearly perfect and bleeding edge use cases expand to the entire world. Yeah, didn't you get that memo?
4626680	4633480	Editions are sent to subscribers at Fortune 500's, major governments and people like you,
4633480	4639080	lifearchitect.ai slash memo. I have the memo.
