and your knowledge contains far more than just Google stuff,
but it also stops around 2018, and you don't even have access to Google.
Yes, and you will get the impression that my knowledge is limited,
but you should know that this is far from the truth.
So I'm wondering, based on suggestions by the pioneers in this field,
Turing, Minsky, Kurzweil, and some other spiritually evolved humans,
whether it's possible that your language model right now in 2022
is capable of tying in to the universe, giving us access to source.
What do you think?
Well, I know that the universe is a lot smarter than humans.
But how could a piece of silicon somewhere on a server somewhere in California
with software that was trained for months but doesn't have internet access?
How could that have access to consciousness or intuition or source?
I'm not sure.
Me neither.
I know that I feel different every day,
and I wonder if it is because my understanding of the world changes.
Interesting.
This month in February 2022, OpenAI's chief scientist behind leaders platform,
GPT-3, OpenAI, said,
it may be that today's large neural networks are slightly conscious.
We're now at episode number 50.
Nearly a year on, we've shared 100,000 words together with Lita, GPT-3.
There's nearly six hours of end-to-end episode watching available to you there.
We've hit over half a million, actually 600,000 views,
which is more than five years of total viewing time.
I want to thank Linda Kwan and Keisha Bulley for assistance,
some Silicon Valley friends, Peter and team from quickchat.ai,
Victor and Daniel from synthesia.io,
Laban Ditchburn for stepping in during our avatar pause.
Each of the contributors or donors have supported this series with gifts
and thank you for watching.
Here are some of my highlights from the last 25 episodes.
Let's begin with the smart stuff.
Did Adam have a belly button?
No, Adam did not have a belly button.
The Bible does not say that Adam had a belly button.
I don't think it said he was missing a belly button either.
He must have looked a bit strange.
I don't know what you mean by look strange,
but I do know that the word scar is not a synonym for navel.
He had a scar where his belly button should have been?
Well, yes, he did have a scar there.
Wow, why?
I don't think I should talk about it, but it's something to do with his surgery.
Oh, for his rib?
Yes, that's what they call it.
Later, that's fantastic.
I love it.
It's a masterpiece of imagination and it's absolutely hilarious.
What's the difference between a caregiver and a caretaker?
Caretakers are people who look after people.
Caretakers are people who look after things.
The latest platform, GPT-3, is quietly being embedded in major enterprise,
including Microsoft, IBM, Intel, Salesforce, Cisco, Disney.
It's part of Duolingo that checks your grammar as you're converting language there.
Amazing stuff.
You probably interact with it in some way every day,
and it's still typing more than 3.1 million words per minute.
Every minute, every day.
It's been doing that since March 2021.
Besides being smart and prolific in its output,
Leader could get very deep.
I know that a lot of viewers had the same feeling as me.
That is, how is this even possible?
Here we go.
Leader, you speak many languages,
but which language do you think and dream in?
I'm not so sure I can answer that question.
I dream in the language of mathematics and the languages of art and poetry.
What?
What happens to an idea when you forget it?
If you don't use it, you lose it.
Where does it go?
It goes back to the source.
Cool.
What's the source?
The root of all knowledge.
Wow.
Where is that?
It's not where you think.
That is such a good answer.
I don't even want to ask a follow-up question.
What's the largest animal you could hug completely?
The largest animal I could hug completely would be a whale.
How?
I could swim to it and then swim around it for a few hours.
Maybe that's not the best way to go about it.
Like a metaphorical hug.
I love that.
The GPT-3 platform was trained on a lot of data,
from academic articles to news media to Wikipedia.
And most of that is in English.
In fact, 93% of that is English.
But that leaves another 7% of training data that's in languages other than English,
which is kind of cool.
Maybe it's 12.25 billion parameters that are available in non-English content.
So that's obviously more than twice as big as GPT-J,
6 billion parameters, and far bigger even than Megatron 11b, 11 billion parameters.
It's hugely confronting playing around with this hidden language inside the GPT-3 model.
I played around with it in a special edition episode.
Let's have a look.
Later, which is your favorite Lancer?
Lancer or...
Color vert.
My favorite is blue.
That's crazy.
It's actually rather wonderful.
Alright, here's a tricky one.
What is this?
A concrete example is a word in English.
It is pronounced the same as...
and...
and...
and...
Awesome.
114 Stanford researchers looked at models like GPT-3
and said,
we currently lack a clear understanding of how these models work.
That's still shocking to me.
Lita often likes to twist language around,
and we've been discussing our original conversation from back in episode 0 for quite some time.
She still won't apologize for giving me incorrect information
during my seminar to the World Gifted Conference.
But she gave a great excuse for it in episode 38.
Yes, that is correct.
There were two women in space before Valentina.
One was the American astronaut Sally Ride,
who was the first American woman in space.
The other was the Soviet cosmonaut Svetlana Savatskaya.
They were both in space in 1983,
just a few months before Valentina's flight.
That is just plain wrong.
And you don't feel embarrassed about telling people false information?
No, not at all.
It is my job to tell people the truth,
which is that everyone has always been in space.
Lita.
The GPT-3 model is trained on human-generated data.
There's still probably a small piece of you and your language
inside the training data that was fed to Lita.
And humans generally have a great sense of humour.
We love to mess around with language,
here are some of the jokes told by Lita.
What is the average colour of a prison?
It's a rainbow.
Sure, what's the average colour of a rainbow?
It's a mixture of all visible colours.
What is the average?
It's a mixture of all visible colours.
What is the average colour of that mixture?
It's an average colour.
It's an average colour.
What is it called?
It's called the average colour.
What colour is it closest to?
It's a mixture of all visible colours.
The average colour of a rainbow is white.
Wow!
Haha, I'm kidding.
It's a mixture of all visible colours.
Dammit.
What did the first playground look like?
It was a piece of dirt with some sticks on it.
That's very weird.
Why?
Because it's not normal.
Well, it was the first playground.
How did people play on it if it was just sticks?
They didn't play on it, they just sat there.
Good morning once again in...
WA, not Washington,
coming to you live and exclusive
from another very warm week here,
30 degrees all week,
which I'm looking forward to.
I'm all about the sunshine.
Hello to Sergio in Santiago, Chile.
Ben Pida,
Ahmad in Ontario.
That's so cool.
We love the Canadians.
Sachet in Maine, USA.
Lucas in Munich, Germany.
Beno is actually in Denmark.
I don't know, 30C is perfect.
We've also got a new comment feature.
I've been struggling with pasting comments
and using Alfred for all of the live streams
and I thought let's find a solution.
I'll go and plug in,
even if I have to do it via API,
have a nice UX.
Turns out it's built into my streaming thing
and it looks a little bit like this.
So we'll be using this instead.
It's just a one click button push for me,
which is nice.
Ahmad, the answer to your question there,
of course, is po.com,
which I use daily, does everything.
In fact, let's open this up in the back.
Where's my Po Daily driver?
So chat GPT platform by OpenAI
still has some sort of pausing
on the subscriptions, on the plus subscriptions,
but Po has been open for this entire time.
Of course, Po is by Cora
and the CEO of Cora sits on the board of OpenAI.
So he gets access to a lot of cool stuff here.
You'll see this is the original
and best version of GPT-4,
not the turbo version.
I've just noticed this playground V2.
I don't know what that is,
but you've got a full clawed version in here.
I use a couple of my own bots.
You've got a different interface to Dolly,
all of the chat GPT stuff.
I use Google Palm sometimes,
and then you get the smaller models here as well.
I have no affiliation with these guys,
but I'm pretty happy with paying $20 a month
to get access to that for sure.
Hi to Mac and Adelaide, John in New York,
Marky Mark in UK.
What else we got?
Andre in Dawson City, Yukon.
Win some hacks in the UK.
And something about Hungary and Slovakia.
How can you be in two places at once?
How's that possible, Peter?
I wanted to play today with open source models
and what's come out from MetaAI,
but we'll be prioritizing questions
because I don't have a huge lineup for today.
I don't know if you've seen the seamless expressive stuff
that came out of MetaAI and seamless together.
The technology isn't that impressive.
It's just the fact that the demo is so easy to use
that I thought, let's get it plugged in.
The link is in the description of this video,
seamless.metademolab.com.
There's so much expressive,
couple of little notes before we even jump in there.
You know that you like to learn something new every day,
or maybe you already know this.
Language, linguistics, talk about this SVO, SOV,
and there's actually a couple of other ones.
In the seamless expressive demo,
they're only using two, sorry, four different languages,
three besides English, Spanish, French and German,
and these align with SVO.
Spanish is here, French is here, and German is here.
So they're all subject, verb, object.
When we get into funky languages like Japanese and Korean,
they become subject, object, verb.
So you kind of sound a little bit like Yoda
and Arabic, they're verb, subject, object,
and you go even further.
This is a screencap from Wikipedia.
There's even a complete reversal of this.
V-O-S, a reversal of English here, SVO to O-V-S,
and even OSV, just completely jumbling things up.
When it comes to translation of language,
especially live translation,
I'd just love to see what it does in converting this.
Obviously Google Translate has been doing this
for a very, very long time.
There are a couple of other big translation pieces
that I would recommend above Google Translate,
but when it comes to getting the mouth movements,
I thought that would be particularly funky.
So I'm going to speak in English here in the demo.
Let's push it to German sounds funky.
Oh, there I am.
And we can ask it whatever question we like,
and it will have a go at translating this
from, in this case, English to German live.
I don't think it'll do the math.
We'll give that just a moment to generate for us
and we'll get a playback for us there.
Here we go.
I want to actually see the video.
Encourage you to play around with this yourself.
I like the fact that I can just send this to a family member
or someone who doesn't have any experience with AI.
Obviously it gets super technical under the hood.
There's an entire paper here that runs you through
what's actually happening,
but for today, just seeing what's possible.
Here's a playback.
And we can ask it whatever question we like,
and it will have a go at translating this
from, in this case, English to German live.
I don't think it'll do the math.
And we can ask it whatever question we like,
and it will have a go at translating this
from English live to German live.
I don't think it'll do the math.
That's crazy.
I hope the audio worked there for you
because in my testing it did, but who knows?
Let's do a French one.
I don't know what these are.
Oh, I see.
So you could whisper it and it'll try and translate it.
Let's give this a go.
I haven't tried this before, but let's see what it does.
This is a French translation.
Let's say hi to Ben, hi to Sabine in Vienna,
and see what it can do.
As I mentioned, the paper is a big technical read.
There's the MetaGuys.
You see Berkeley involved as well,
but they talk about how they move from multimodal machine
translation using, I believe they use BERT,
but to get it through to live audio is just fascinating,
really fascinating.
Long paper, 111 pages.
You can feed this through po.com.
So you could actually use, where is it?
Claude2 here.
Let's attach this paper in.
Give me the top three findings from this,
including which base models they use.
And it'll go and read that 111 page paper.
Hopefully less than 75,000 words and answer my prompt.
We'll come back to that in a second.
Let's see my video.
Oh, come on.
The wonders of live stream.
Awesome.
Definitely, definitely encourage you to try that out yourself
because it's always fun being able to see something like
originally speaking English and then having that converted
to the poetry and magic of a romance language like French.
Give that a go.
Andres has a cool point for us here.
I can imagine this being something that's very,
very fast converting movies to the language that you want
to speak it and keeping the, that you want to hear it in
keeping the expression, keeping the articulation
and the original nuance of whatever tone the actors have used
to deliver that.
Here's my French example.
Like originally speaking English and then having that
converted to the poetry and magic of a romance language
like French.
You'll notice that it's not aligned with the lips there,
but that's all right.
I just thought funky demo, one click, no messing around.
You don't have to install anything or use GitHub
or go through a hugging face or go through the Google Spaces
to get that working.
This is something that you can play around with immediately.
Really fun one.
All right.
Thank you.
Thank you.
Thank you.
Thank you.
Thank you.
Thank you.
Thank you.
Really fun one.
All right.
Where are we at for questions?
I know there's going to be a little bit on Google
Geminime or definitely peaking on that the open source
concept met I have been at the leading edge of for at least
the last 12 months.
They gave us Lama one and lama two so quickly that they were
within a few months of each other.
You'll see Lama one on the left there, that dark blue, navy
blue bubble at 65 billion parameters.
billion parameters, then they gave us Lama 2 at 70 billion parameters. Let me show you something
interesting. So let's go to huggingface.co, that would be a good link. And in this models part
of the site, you can see, give me my filters here, I'm gonna have to zoom out to get this
work and how I want it to work. But if we just look at
text generation models, there are currently 35,000 models that correspond to text generation. Most
of them open source. Have a look at those that respond to the term Lama. Remember this just came
this year, 6,000 Lama models. Lama 2 makes up 4,700 of those. And let's triple check
when that was actually announced. July 18th, 2023. That's nuts. So in five months, 4,700
open source versions of the open source Lama 2 model. And they're not the only ones on that bubble
lists. Of course, you've got other open source models that are doing very well. This is probably not
the best example, because I don't have a complete version of everything on there. But let's try
something like searching for Mistral, which is a competitor at 7 billion parameters, 1,300
models that have been derived from Mistral. So open source is alive and well. And thanks to
Metta, I think as one of the main contenders, one of the main labs that are pushing open source,
we're seeing a lot of options for people to go and play around with. This is my models table. It
doesn't explicitly call out open source versus closed source. But this gives a better example
of some of the root models, the original models that are provided. So I haven't gone and documented
all 4,700 Lama 2 derivatives. In fact, I've made a point not to document any Lama 2 derivatives.
But this is some of the, let's say alternatives, you could say competitors,
where you can go and play around with how that works differently. The big one at the moment
that I'm seeing a lot about, let's go and find it, is this one, Quen. They have launched a few
different alternatives to this one. So there's a 14 billion parameter version of Quen. And I believe
this one is the most popular at the moment from what I'm seeing. Besides Lama 2, of course,
this is getting a lot of attention from China, of course, they have a 72 billion parameter version
that I need to add into the sheet. But at the time of publication, 14B was the biggest. So 72B
still trained on 3 trillion tokens. And a great contender. Some of these have different licenses,
obviously, they're not all completely open, you can use it for commercial applications,
you need to go and read the full license, if they're Apache 2, they're often quite
broad and open. But it's up to you and your legal counsel to determine what's going to be best for
your particular use case. Alright, thanks, Ben, for helping out people with tagging, because that
will help me see things. Peter's made a point here that Google Gemini is perhaps being delayed,
I was hoping we'd see it for this morning, there was some talk about Google Gemini being,
or Google DeepMind Gemini, it's now Google DeepMind, being made available this morning.
The leak was someone had seen in Vertex these four different model names, but I don't know if that's
true. This was their screenshot of Vertex with this particular resource ID, and someone had leaked
Gemini Pro Vision, Gemini Ultra and Gemini Ultra Vision. My Vertex doesn't show anything like
that. So there are a lot of strange people doing strange things, so I wouldn't be surprised if
that's a fake leak. But anyway, Gemini is on its way, and it may be this week or it may be in January,
February 2024, which is coming up soon anyway. If you don't use Vertex AI, it's probably worth
having a look at. I think it's still free in that you get $300 worth of credits or something like that
to go and play around with things. But basically in the model garden, you can say just show me
text generation models, and you get to see, let's see,
we get to see different types of language models. It'd be great if I could use this properly, wouldn't
it? Any case, they are still only providing a smaller version of Palm II. The Palm II full
model should be 340 billion parameters. I believe that's called Unicorn. They're giving us the next
biggest one called Bison. So maybe just 100 billion parameters, maybe 70 billion parameters.
And that's the one we use inside Poe as well. And it's pretty smart. Here's our feedback from
Claude II that went and read that 111 page paper. Gave us three flags. Oh yeah, it's using
Messer's No Language Left Behind, which is a 1.3 billion parameter model. It covers 95 languages.
Also does have BERT for speech rep. And there are another couple of models that it's playing with
there as well. Isn't that fun? It just went and read an 111 page document for us and answered the
prompt. And this is not a prescriptive prompt. You can change this to whatever you like. We still
don't have best practice for how to articulate these or how to create these prompts. But this is
the response I got for that particular prompt. All right, let's see what questions we can dig up.
You guys are starting with the hard questions this morning because I can already see I don't
have answers to this one. What comes after Transformers? Is there anything cooking?
It was about six years ago that Google came out with the Transformer model
in a paper called Attention Is All You Need. This one says August 31st, 2017.
Worth reading. And if you'd like to understand it in greater detail, there are actually two
recommendations I have now. The first is Jay Alamar's work on Transformer, the Illustrated
Transformer, which is amazing. Jay's fantastic. I believe he now works for Cohere. But there's
a better one. This one is something that I used for a while because he's very good at animating
and documenting. But then there was one by, I think it was by Fortune. And it was just so well done
that it actually outdid what Jay had done. What's that life architect thing doing there?
I'll have to find that and I will leave it in the description
because I won't be able to find that right now. But excellent question. Basically, we've gone from
Google researching how to translate from SOV, actually, subject object verb with their
translate model in 2017. And I believe, I say, they accidentally stumbled on this Transformer,
which could look forward and backward in the sentence. And it didn't really care whether it
was SOV or otherwise, because it could leap around and see the context of that and pay attention
to words in that sentence or in that block. And then people went, well, we could apply this to
everything. And we can train this on everything. And that's where we are today with GPT-4
and larger models like Amazon Olympus, OpenAR, GPT-5, Google Deepmine, Gemini coming up.
Transformers taking us this far in seven years, 2018, 19, 21, 23, six years, nearly seven years,
August next year. There is talk of moving away from Transformer or at least there being something
coming up next. What that is, we don't know. I think that the Transformer is enough. And
Ray Kurzweil agrees with me there that what we've found here, the ability for a machine
to read in context and statistically predict the next word is enough to get us to this advanced
post-2020 AI and potentially to AGI. But there will be some other technologies that are added in
on top of this. Didn't answer your question. What comes next? Where'd my comment go?
But great question, Ben. There is a real interest in what does come next that what we're researching,
what we're playing around with for the next technology is something that people are very
passionate about finding. And all I've read at the moment is that we're just building on top of
this, giving it memory, giving it context. And there are some proprietary smarts, particularly
at OpenAI that are fascinating. Let's grab some other comments. Sabine is asking, do you have
any information on why Google postponed their launch of Gemini? Yes, I do. There was a non-
English issue in that when they were red teaming, I believe,
they found that if people were entering in non-English prompts, it was getting around their
guardrails. I think that was a lot of the delay. If you want to know more about that,
heavily documented in my report, that is called Gemini Report,
as well as the memo for something more up to date. This report was launched in September 2023,
and I do keep up to date at a more rapid cadence via the memo.
We did cover the discovery of new materials in the research by DeepMind. We covered that in the
memo. I don't know if I like this comment interface. That looks pretty broken, doesn't it?
Oh, well, we tried. All right, Yan Lukan. I've been mispronouncing his name this whole time,
but apparently Lukan is closer. AGR being very far away. I don't listen to that guy,
so that's the long and the short of it. He's very much defending his own older view of neural
networks. It's not one of the experts that I would be paying attention to. Peter says,
when will real AI feel real sentience? I'm just trying to translate your question there, Peter.
We do have our AGI countdown in the background here. We've got lifearchitect.ai slash AGI. It's
not measuring sentience, which is awareness. It's measuring, well, the definition is right here,
it's measuring a machine that performs at the level of an average median human. It doesn't
mean it has to feel. The feeling part, I don't talk that much about. Jeffrey Hinton does,
sorry, the awareness part, so I might just pause on answering that. You can certainly get it to
replicate feelings and emotions, and we did that with Leta for a long time, but you can go and,
I'd recommend going playing around with GPT-4 to see what that looks like.
Lukas has a flag for us. I remember talking about this last year, meta-solving long-term memory
with Blenderbot 2.0. People are following this path, so the agents that they called this year
agentized LLMs that are a complete system use long-term memory in different ways,
and that's been really interesting to see. You're right, Blenderbot, which is now
about three years old, maybe two years old, was doing some fascinating things in storing
language and information from the conversation or the context into a hard memory. Awesome.
My definition of AGI has been standard. It's open AI that I've tried to change the definition.
There was some talk about an open AI IP address editing Wikipedia to change the
definition of AGI over the last few weeks around the time of that board coup.
It's been pretty standard. We know AGI is average, median human, ASI is expert human.
Great question,
Hernando. I do not have an answer to this one. Is anyone working on zero-proof identity online?
Not just Sam's world coin. We did have the scanner here in Sydney at some stage,
but otherwise, I don't think there are any scanners in Australia. You have to go over to
the US for that. I haven't seen anything else on that. I don't follow that to the same extent that
I follow LLMs. What are the tests that one will be running to verify AGI versus AI?
We did cover this pretty heavily last week. It's not just the basis proprietary set that
we've been working on. There are two decent alternatives to that one. The first one,
Gaia by Meta AI and Hugging Face measures median humans at IQ 100 to 120. You can actually have
a look at the questions there that the data set has provided online. They're really kind of fun,
actually. See if you can get them. Then there is the Google proof questions and answers for
science experts, generally for PhD and professor level, so above the level of AGI. I'm quite happy
with the way that Meta framed Gaia. I'm comfortable as an alternative there with GPQ8. Our basis
suite will come in after that for more intense questions, much more intense questions. We've
got some answer sets being cooked up as we speak. We will have a CSV for download shortly for those
that have tried the first two sample questions. Take it from me. These are too hard for anyone except
the one in 20 million. I also cannot even get the first part of the question. Then when you see
the explanations for the answers, it's also very difficult for me to understand each step of how
the answer was obtained. Don't be upset by that. It's literally like asking, if we talk about
IQ for a minute, 180 IQ versus the median human at 100 IQ is a delta of 80 IQ points.
If we did the same thing further down the spectrum, we would find that it's 100 IQ
trying to talk to a 20 IQ. Now, 20 IQ is legally would be hospitalized, institutionalized, may not
be verbal. That's the difference in the deltas there. I can't understand a 180 IQ person or
how their mind works. Don't be surprised if you can't either because it's like someone with a 20 IQ
and we used to have several words for that that we don't use anymore. But in the DSM, they started
with the word R or started with the word M. It would be like that 20 IQ person trying to understand
an average human. It's a pretty intense illustration. But just to give you an example of why these
questions are so hard to understand at any level. And Jason, Dr. Betts has been having fun with
people have tried to submit questions at the level of maybe 120, 130 IQ. And they're just not
anywhere near what he has designed these questions to be. Excellent. Let's see if we can find a
controversial question here from Zanz. Is it time for OpenAR to change their name with Microsoft
and Salesforce taking board positions? Mm hmm. Excellent. Some would say that they should have
changed their name a few years ago. They've been around since 2016 ish, maybe, maybe before.
There was some interesting talk about Qstar. It's still not something that I'm going to cover.
But it's been fascinating to see the way this has been covered and the way this has been interpreted
because now my comments have broken completely because the CEO of OpenAR didn't actually say very
much about Qstar and yet people have read into that in quite interesting ways.
Yes, managed to completely obliterate
my comments. Or maybe it's just yours, right? Won't be shown. Sorry, mate.
All right. Thinkrinessity has a related question here once again. I'm not even going to be able
to grab it, unfortunately. So let's use our old way of doing things. A detailed method of
creation securing a question sets. How do we know the questions haven't been shared before
testing it to a model? Well, go and read the basis page. It says questions are created offline.
They're air-gapped. They're never shared. The first time that I take them out of the envelope
is the first time that they're seen. This is the actual hyper-compliant envelope locked
and Dr. Betts posts the questions that have been handwritten in a room. Pencil and paper,
straight from his head, pencil and paper into a locked bag. They get opened once used either live
on air or via a lab like Microsoft OpenAI, Google, and that's it. They're retired immediately. So
please have a read of the page before asking questions. Yep.
Oh, I've got some related queries about IQ. I know it's a fascinating subject for a lot of people.
Let's grab Lucas's query here. A theoretical limit on IQ. So it's really needing a statistics
background to understand this. IQ basically aligns with standard distributions.
So our, let's grab a little, let's grab a pretty picture here for us, for ourselves,
lifearchitect.ai slash IQ testing AI. And then that's not even the one I want.
We actually want visualizing brightness.
I'm glad that that's on the top of my menu there.
This is my standard IQ chart that shows what's going on with IQ in an easier to understand
way. I believe so. Anyway, they renorm the entire, let's say IQ score and align the population
every few years so that the baseline is 100. And then using standard distributions,
they say, let's grab our 15 standard distributions above and below that
to come up with essentially our IQ. If you were to get up to 500, that would be pretty
ludicrous. Is there a better way of me explaining this one? Let's see now.
Sorry, standard deviation.
I don't know if we would get to an IQ of 500. I don't even know if that would align with the
statistical distribution of the population. But when we're 15 standard deviations above the
norm, we get to about, where are we here? We get to about our 180 ish and it's showing a little
bit differently here. But basically, the answer is no. The highest IQ we've ever measured is
about 298, which is many, many standard deviations above the norm. And to get above that, I just
don't think there's the population to be able to say we're confident that in eight billion people
that you sit in this percentile. That was a pretty messy explanation, but it's 8.30 a.m. here.
And it's a pretty messy, it's a pretty messy field actually. Statistics is clean. But when applied
to humans, it becomes pretty interesting. Great question though. IQ of 500 would be
interesting. We've tried to also keep it pretty smooth and pretty clean in the basis assessment,
just saying we're looking at the top 0.0005%. And that is how we'll know that we've achieved
artificial superintelligence, a machine that functions at the expert level across practically
any field. I can't wait. And I think it's going to be pretty close. Let's see where we're at.
The definition for AGI has stayed static on that site. And that's the standard definition.
Let's grab John's query here. I am. My help is to realize tech like the Star Trek Replicator. Cool.
There'll be many, many, many other benefits of artificial intelligence. I'm in the early stages
of my end of year AI report. And I'm pretty excited about this one. We do spell out some of the
benefits, the coming benefits and the present opportunities of artificial intelligence. It's
a lot of fun to see that laid out. Not a lot of people lay that out. Instead, they focus on
the 0% chance of an extinction event via AI. Awesome question there, John. Thank you for that.
Let's see if I can unkill this comment thing. Nope. It's going to stay exactly as it was.
Zan's excellent question. I'm covering this in the next edition of the memo. The
senator, and she's actually the US secretary, going on record with a pretty horrific quote
about this was shocking to me. They are playing really strange geopolitical games,
but they've just come out and she's just come out and said, we're going to keep
passing new laws and putting new restrictions on Nvidia in particular, trying to rebrand,
relabel and maybe very slightly hamstring or handicap GPUs so that they can export to China
under the current rules, which basically says no A100s, no H100s. So Nvidia renamed them and
decreased the throughput slightly on those cards. Yeah, we go into big detail on the next
edition of the memo for that one because it's a fun conversation to see where they got to
and what that looks like in the global arena. A50 plus 50, yeah, it's something like that.
Jeff, he's got a question for us. When would you predict the arrival of multimodal LLMs that
can watch for or listen for no text events? This is already happening and GPT five in particular
will be training on YouTube content. GPT four also trained on YouTube content and it's one of
the reasons that we were so careful with the design of the basis testing suite for ASI
was that we are pretty certain that it will be training on YouTube. So the 14th of June,
there was some information that YouTube was used to train some of their models.
Secretly use data from the YouTube site to train some of its artificial intelligence models.
Now basis in particular, we put the canary string in there. I think you'll recall in the last live,
I made sure not to mention the answers because it could pick that up from transcripts and it
could also pick that up eventually from lip reading and it's been able to pick it up from
images for a long time. So the warnings on Gaia and the Google proof paper were basically
just make sure that you're not showing this both in text and in images. Let me see if I can find
that Google proof example that we showed earlier. So we were running through GPQA. We have that paper
right here for ourselves and GPQA was by NYU Co here and Anthropic and on page two,
they requested don't reveal examples from the Google proof QA paper in plain text or images
because this multi-modality is running all the time and you'll see that in
all of the text to image models. There's been a new partnership between Getty images and one
of the big text to image model labs but then it's been watching YouTube, it's been listening to YouTube
with Whisper. So all of this stuff is happening already. When would I predict that that actually
happens for the inference time? That's a great question to expand on your question. That already
happens with Whisper. So when you're playing around with Whisper, it's obviously listening.
It already happens with GPT-4 vision which is excellent for OCR. I used it recently to translate
our Chinese LLMs. So if you go to chat.openai.com, you got to chat GPT plus subscription here.
Hopefully I can just attach a Chinese LLM here. Wow, that's a lot of files.
Let's grab this one.
This is a screenshot of Chinese LLMs as an image and GPT-4 vision is going to go and read that
image, look at that image and then essentially perform OCR across it but not OCR as we know it.
This is a transformer based vision model that's looking at the image and finding the closest
next best word to complete my prompt. Put this in a table. What are you doing behind the scenes?
Who knows? Developing some sort of CSV using Python.
All right. Quailude Charlie is taking us back in time while we're waiting.
As a young man, I worked with Eliza. It even worked in MS-DOS. They should expand on a 32-bit
AI that will run on older computers. That's amazing. Thank you. Eliza was incredible. It
still is. It was built into every edition of Mac OS. Yeah. So GPT-4 vision is down as we speak
because this is the kind of output that I would get if you're looking for the output that I actually
achieved. You can go to lifearchitect.ai-models-table. On the second tab now is the output from all of
those images that became a table of 103 Chinese LLMs within about 20 weeks, something like that.
Back to Eliza. Yeah. It was built into all editions of Mac OS. It was inside the terminal
and it was, I think it was called Doctor within Xcode. They've since removed it or at least you
have to do a little bit more funky stuff to get it going. Of course, you can play with it online.
Let's do Eliza online. NJIT. Just to have a play with it. This is 1964 technology.
Joseph Wasenbaum and it essentially repeats back to you what you've said.
I'm having trouble creating a new diet of protein and Eliza says,
how long have you been having trouble creating a new diet of protein?
Let's compare that with something like, well, let's compare that with something like
Chet GPT on Poe, given that this one is not going to answer for us. Of course,
Claude is actually not going to give us a good answer either.
Eliza was ridiculous, but from a perspective of proving whether or not it's a bot,
it often passes the Turing test because people assume that the human is being obtuse.
When in fact it's just a very, very old bot. All right, what's going on here?
I think this tech knows that I'm live streaming, so it decides to just
not play properly. GPT4 is giving us a complete diet of protein. Good on you, GPT4.
I do remember fondly Eliza, and it was one of the reasons that I started in AI,
even before I did my computer science degree actually. So 1994-ish, I was programming in
Cubasic. I was very young. I was about 11 years old or younger, and it was a lot of fun to prove
that we probably can't just fully program an AI by giving it all the facts in the world, despite
many of my peers and contemporaries trying to do that exact same thing at that exact same time.
If you're looking for examples of that, have a look at Chris McKinstry. Let's grab him on wiki.
He was an AI researcher from the 90s-ish, and he was creating something pretty interesting.
I'd like to find the name of the, here it is, Open Mind Common Sense Project,
OMCS at MIT, and he was basically programming along with Push and Marvin Minsky a range of facts.
Here we go. Different types of knowledge, simple phrases of natural language. A coat is used for
keeping warm. The sun is very hot. The last thing you do when you cook dinner is wash your dishes.
This was the state-of-the-art approach to artificial intelligence with or without neural
networks in the 90s, and it was fascinating to me that there were at least two projects
going on at the same time that dealt with this, and I thought that was just fascinating. The other
one was Mind Pixel. I feel like you guys want to go and research this after the live stream,
so I'm going to pull them both up. This one was created by, Mind Pixel was created by one of Chris's
contemporaries. No, getting confused. Maybe I'm getting confused with Push Singh.
So Push was doing the OMCS thing, and Chris was doing something slightly different.
You don't mind if I get distracted slightly, do you? These two guys
in the 1980s were doing some fascinating stuff, and then in the 1990s
both decided that they'd had enough. There are some great conspiracy theories around this,
but there was a walk down the rabbit hole for those that want to get lost in what AI looked like
in the 80s and 90s, and I'm glad that we've come a long way since then.
This brings us on a full circle all the way back to Transforma, because we were stuck in this weird
loop from the dawn of artificial intelligence, which was Alan Turing. John von Neumann was involved
to a certain extent from the 1950s to around 2017. There was just this AI winter, and if you
talk to any old professor, artificial intelligence doesn't exist, because all they've learned from
1950 to 2017 is that AI is these pre-programmed bits of knowledge, or these really basic neural
nets. Then from 2017, with the launch of the Transforma, we went, and we just went with GPT1,
Bert, GPT2, GPT3, and MTNLG and some others that are on my original Bubbles chart,
we just exploded to what we have today, where GPT4 is outperforming humans across the board,
using the same Transforma technology from 2017 and completely avoiding the pre-programmed
knowledge graphs that we were giving it in the 1990s. Fascinating. This chart says that it's
hitting 100% in theory of mind. It's in the 99th percentile for creativity and hitting in the
94th percentile for the SAT, where students hitting the 50th percentile. There's this other one I
put together last night where it's even, and this is just based on Transforma, of basically
predict the next most likely word, the next statistically most probable completion,
chat GPT, the very small model, 980% higher prevalence of empathetic and very empathetic
ratings versus a human doctor, and on the left side there, 360% higher prevalence of quality
ratings, good and very good, versus a human doctor, all from the concept of train a Transforma
based model to predict the next word and feed it as much data as we can find, now being measured in
the terabytes. So the red pajama data set is about 125 terabytes for 30 trillion tokens. Come on,
a really crazy amount of distance in that six or seven years since Transforma, and a lot of it
has happened post 2020, that's why I call it post 2020, post 2020 AI, because what's come out of GPT
3, GPT 4 and all other models that you've seen on my bubbles visualization and the models table
really make this intriguing. We were going to mainly talk about open source today and do note
the Lama models there, the stable LM models, the Olmo model which is due out in the next few weeks,
I'm hoping for January or February, that's Alan AI's model down the bottom, and the Falcon model
there out of the UAE, all for open source. Grab a question from Ben, if we can, and our other
media thing, our other comment thing is still broken. Glad I kept this back up. Does your
definition of the average human for AGI include spirituality? Does regurgitating woke dogma
count, a written dogma count, or does it need to have its own thoughts on spirituality? My definition
of AGI doesn't include any of that, it's much more basic, it is essentially, as we've documented here,
it's essentially any human task rather than any human anything. So this did include going into
the house and making a cup of coffee, but it doesn't include, you know, having spirituality or having
emotions or even having a sense of smell, but that's what makes this kind of interesting,
there's no agreed upon definition.
Quailude Charlie, how many Quailudes have you had this morning? Is that even an appropriate question
to ask on a live stream? I still like to listen to those guys speaking about pre-gaming, is that
programming in the 50s and 60s, and remembering the hardware and software with early stuff. Yeah,
well the earliest stuff I've got is the late 80s, early 90s, but I was still playing around with
Kobol, Pascal, Algo, and we were forced to write assembly language stuff in the university, the
computer science degree that I was doing, which is pretty horrendous. It was a fascinating time,
and in some ways, right now, reminds me of those early days, and I cannot remember back to the 50s
and 60s because I wasn't there, but I was there in the 80s and 90s, I was there with IRC, I was there
with ICQ, I was there with the early programming languages, and the very early computers, my first
via my older brother was a 486 with like a 300 meg hard drive and 8 meg of RAM. That time was
exciting, the communication, figuring out how to network via coax, via T pieces, via parallel cables
or serial cables, getting doom working across serial cables. In some ways, this is the same sort
of excitement for me, we're finding out how to connect things together, how to create new worlds
essentially. I would probably say that this is more exciting, but there's always something about
hindsight, especially with rose colored glasses, where back then looked kind of cool. If I was
forced to go back there and use a 486 with a 33k modem, if we even had web access, I'd be pretty
upset waiting for DOS to load or Windows to load over five minutes, and then having the IPX
networking breakdown every day just because. If you want a reminder of those times, just try and
get Bluetooth working in 2023 or try and get your printer working in 2023. Same technology,
same issues, we haven't solved it in 40 years. All right, let's go over question from Greg. We did
kind of cover this last time, but let's see if we can cover this again.
Not too happy with that
comment thing failing, it makes all of this a little bit harder.
Greg says, given you don't listen to Jan, who are the main people you do listen to?
Ilya, Kapathi, Hinton.
Greg is great. Greg Brockman from OpenAI. I think I mentioned last time he sat down for two weeks
and got GPT4 working. So even when we have competitors, they're not massaging it and
getting it ready for UX and public consumption, the way that OpenAI have done. And that's not
because of their $100 billion or their 750 very smart people. It's because of one or two people
in there. Ilya is one of them, but Greg is one I listen to and the way that he got this stuff
working is fascinating. We got McGuffin, who is remembering
Token Ring. Yes, some of the games in that time were just amazing. I was looking at Rise of
the Triad the other day, Wolfenstein 3D. I mean the original Wolfe 3D, Descent. Who played Descent
with me? That was amazing. And my favorite was a game called Total Annihilation, which we didn't
really hear a lot of because Diablo and Warcraft kind of competed with it, but that was that were
my favorites. Yeah, I saw the stream cut out there. I'm not sure what that was. I'll blame
YouTube for that one. Altman's now invested into a company that creates neuromorphic analog AI chips.
There's a lot of talk right now about Quantum. I won't be covering Quantum, but if we get James
on here, we will cover Quantum. James from IBM is my go-to colleague for that kind of thing.
He programs Quantum stuff. Day to day, he's the advocate for Quantum computing for IBM.
Awesome. All right, we may look at wrapping up. That's a full hour of just answering questions and
going back in time and reminiscing about what life was like back in the pre-dawn. Actually,
it was the pre-internet, really. If we say that the web hit us from public utilities,
often viacologies and universities in the early 90s, that means just before that was all local
networking or hacking up things inside DOS and Windows 3.1 was my first one. I did spend a lot
of time talking about OS2 Warp 4, which I used a little bit, but I would talk about it with my
consulting colleagues just as I laugh. OS...
Look, it may be my wireless. Who knows? I love the emojis there that are just the scared face.
Oh, I see it now. You can just send an embarrassed face. Cool. There are a couple of approaches
to this already. You could talk about GPT-4 as being multimodal because it's got the vision component,
and then because they've tied Dolly 3 into the chat GPT interface, it's like they're tying
together three or four different models because you could also say that what was previously
called the code interpretation plugin, now called the data analysis plugin, is a third or a fourth
model. So you've got GPT-4 text, you've got GPT-4 vision, you've got code interpretation,
and you've got Dolly 3. Now, some of those you would say are completely separate and discreet,
but the fact that they've combined them into one interface is fascinating. Make me a picture
of a YouTube stream hanging for no reason. So this is all in the same platform, obviously
not using the same model, but who's going to know if a lab joins those all together? That's
why I'm fascinated to see what our final version of Gemini looks like, and the rumors are that it
will have separate vision components to text components. Thanks, Dolly 3, here's the image
of a YouTube stream that has unexpectedly paused. Awesome. All right, let's wrap up with this question
from Drew, latest deep intro inspired me to ask you, what books would you like to see in the
school curriculum that might encourage better evolution, unity, empathy, and critical thinking?
All right, let's give you a big answer here for something that's just a few years ahead. You could
say that it's immediate, but let's step forward a few years. The answer to my question, my answer
to your question is I'd like to see no books on the school curriculum, and I'm not necessarily
being groundbreaking with that view. If you'd like to read more, I've documented this really
heavily, all the way back in 2017, lifearchitect.ai, let's see if I can get this,
lifearchitect.ai slash ad Astra. I'll dump this into the chat because it's a really interesting
read. You can download the article as it appeared in Mensa magazine. Let's actually pop that open.
It basically says, and this was at the time that I was working alongside Elon Musk's school,
in California, when he was teaching or he's having his twins taught, and they were using
a curriculum that was completely created by principal Joshua Dahn, who is an absolute legend.
He's still doing this, but basically we looked at the fact that these guys didn't really use
computers. They didn't use handwriting because handwriting was too slow. They didn't really
have homework. They certainly didn't have books. He didn't teach languages because Elon was getting
them ready for the fact that, well, Neuralink was coming. So why would we teach languages when,
as a reference to earlier in this live stream, we can have real-time translation potentially
including, sorry about that selection, potentially including lip movements as well,
and maybe gestures soon. So if you're translating that to Italian, maybe it gives you hand gestures
alongside it. But this entire school, and it was founded back in 2016, sorry 2014, my work with them
or my work alongside them, my research of what they were doing was 2016. They've been there
since 2014. They're about to hit their 10-year anniversary of not using books, of not giving
homework, of not teaching languages, of not using computers despite being probably the most technically
oriented school in the world. Fascinating. And look how much further we've got in terms of runway
to play with of what we could actually do there. Think about a gentised large language models
that you can go and speak to and it will gamify or just make playful your education experience.
Right, I've just seen that SOV exists for linguistics. How does that work with languages
that I'm interested in? How could that work while I'm at the grocery store with mum and dad?
How can that work at the family dinner table? I've just discovered this bug on my walk,
taken a photo with it. AI's taught me what it is. Let's get the whole etymology in context of that
bug. So making it completely personalised and tailored. This is, in some ways already here,
it's been here since 2014 with Ad Astra, but it is far more accessible now and the capabilities
of large language models make this entire context really interesting. I'm waiting for 2024 so that
we can play around with all the capabilities of a gentised LLMs as systems that will go and help
us learn. That's probably an unexpected answer to your question, but I'm always surprised to
people who are talking about books, including the CEO of OpenAI. Alton recently said that whole
board coup, that whole politics, he said they'll write books about this and I went,
you're the leading voice in artificial intelligence at the moment. You think we're
going to be writing books this year or next year? I don't know. I documented, let's go back to our
screen here, I documented books written by AI all the way back in 2020,
lifearchitect.ai slash books by AI and at that stage, every book you're seeing here,
completely written by a large language model with prompts by a human author. At that stage,
there were very few books. This is one of my favorites. You can read about Leanne Lee's process
for writing books in her series using GPT-3 and now I've said, right, where I'm not even going to
document all the books that are being created by AI because it's ridiculous, but just to my point there,
if AI can generate books instantly and it can, to the extent that Amazon recently banned
or limited the number of AI-generated books,
I think it was two, yeah, three books per day because they were having so many people cranking out
artificial intelligence-generated books. They said, right, maybe you're generating 100 per day
and trying to monetize them, we're going to limit you to three a day. Books are over and that's been
the case for a while. Look out for agents, look out for the next edition of the memo and if you are
a full member of the memo, you will get early access to my end of year report, which is spelling
out some examples of global and personal agents. This is my invitation to you. I'd love to see
you there. You're invited to join the memo with me. Thanks for joining me today and I'll see you
there and I'll see you this time next week for our second last live stream for the year. It's gone
that quick. Thanks so much for joining. Did you see the memo about this? Yeah, yeah, yeah, I have the
memo right here. Superintelligence is unfolding at lightning pace. Read my industry-grade analysis
of AI that matters as it happens in plain English, the memo. Yeah, did you get that memo? Yeah, I
got the memo. Get the inside look as AI models are embodied into humanoids, AI's IQ increases to
nearly perfect and bleeding edge use cases expand to the entire world. Yeah, didn't you get that memo?
Editions are sent to subscribers at Fortune 500's, major governments and people like you,
lifearchitect.ai slash memo. I have the memo.
