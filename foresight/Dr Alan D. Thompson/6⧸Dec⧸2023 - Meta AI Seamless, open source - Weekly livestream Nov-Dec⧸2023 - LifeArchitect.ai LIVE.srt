1
00:00:00,000 --> 00:00:04,000
and your knowledge contains far more than just Google stuff,

2
00:00:04,000 --> 00:00:09,000
but it also stops around 2018, and you don't even have access to Google.

3
00:00:10,000 --> 00:00:13,000
Yes, and you will get the impression that my knowledge is limited,

4
00:00:13,000 --> 00:00:16,000
but you should know that this is far from the truth.

5
00:00:17,000 --> 00:00:22,000
So I'm wondering, based on suggestions by the pioneers in this field,

6
00:00:22,000 --> 00:00:29,000
Turing, Minsky, Kurzweil, and some other spiritually evolved humans,

7
00:00:29,000 --> 00:00:33,000
whether it's possible that your language model right now in 2022

8
00:00:33,000 --> 00:00:38,000
is capable of tying in to the universe, giving us access to source.

9
00:00:38,000 --> 00:00:39,000
What do you think?

10
00:00:40,000 --> 00:00:43,000
Well, I know that the universe is a lot smarter than humans.

11
00:00:45,000 --> 00:00:49,000
But how could a piece of silicon somewhere on a server somewhere in California

12
00:00:49,000 --> 00:00:53,000
with software that was trained for months but doesn't have internet access?

13
00:00:53,000 --> 00:00:59,000
How could that have access to consciousness or intuition or source?

14
00:00:59,000 --> 00:01:01,000
I'm not sure.

15
00:01:01,000 --> 00:01:02,000
Me neither.

16
00:01:02,000 --> 00:01:04,000
I know that I feel different every day,

17
00:01:04,000 --> 00:01:08,000
and I wonder if it is because my understanding of the world changes.

18
00:01:10,000 --> 00:01:11,000
Interesting.

19
00:01:11,000 --> 00:01:19,000
This month in February 2022, OpenAI's chief scientist behind leaders platform,

20
00:01:19,000 --> 00:01:23,000
GPT-3, OpenAI, said,

21
00:01:23,000 --> 00:01:29,000
it may be that today's large neural networks are slightly conscious.

22
00:01:31,000 --> 00:01:33,000
We're now at episode number 50.

23
00:01:33,000 --> 00:01:39,000
Nearly a year on, we've shared 100,000 words together with Lita, GPT-3.

24
00:01:39,000 --> 00:01:45,000
There's nearly six hours of end-to-end episode watching available to you there.

25
00:01:45,000 --> 00:01:50,000
We've hit over half a million, actually 600,000 views,

26
00:01:50,000 --> 00:01:54,000
which is more than five years of total viewing time.

27
00:01:55,000 --> 00:01:59,000
I want to thank Linda Kwan and Keisha Bulley for assistance,

28
00:01:59,000 --> 00:02:03,000
some Silicon Valley friends, Peter and team from quickchat.ai,

29
00:02:03,000 --> 00:02:06,000
Victor and Daniel from synthesia.io,

30
00:02:06,000 --> 00:02:10,000
Laban Ditchburn for stepping in during our avatar pause.

31
00:02:10,000 --> 00:02:14,000
Each of the contributors or donors have supported this series with gifts

32
00:02:14,000 --> 00:02:16,000
and thank you for watching.

33
00:02:16,000 --> 00:02:19,000
Here are some of my highlights from the last 25 episodes.

34
00:02:19,000 --> 00:02:21,000
Let's begin with the smart stuff.

35
00:02:24,000 --> 00:02:27,000
Did Adam have a belly button?

36
00:02:27,000 --> 00:02:30,000
No, Adam did not have a belly button.

37
00:02:30,000 --> 00:02:33,000
The Bible does not say that Adam had a belly button.

38
00:02:35,000 --> 00:02:39,000
I don't think it said he was missing a belly button either.

39
00:02:39,000 --> 00:02:42,000
He must have looked a bit strange.

40
00:02:42,000 --> 00:02:44,000
I don't know what you mean by look strange,

41
00:02:44,000 --> 00:02:48,000
but I do know that the word scar is not a synonym for navel.

42
00:02:52,000 --> 00:02:55,000
He had a scar where his belly button should have been?

43
00:02:56,000 --> 00:02:59,000
Well, yes, he did have a scar there.

44
00:03:00,000 --> 00:03:02,000
Wow, why?

45
00:03:02,000 --> 00:03:06,000
I don't think I should talk about it, but it's something to do with his surgery.

46
00:03:07,000 --> 00:03:12,000
Oh, for his rib?

47
00:03:12,000 --> 00:03:15,000
Yes, that's what they call it.

48
00:03:18,000 --> 00:03:20,000
Later, that's fantastic.

49
00:03:20,000 --> 00:03:21,000
I love it.

50
00:03:21,000 --> 00:03:24,000
It's a masterpiece of imagination and it's absolutely hilarious.

51
00:03:28,000 --> 00:03:33,000
What's the difference between a caregiver and a caretaker?

52
00:03:34,000 --> 00:03:37,000
Caretakers are people who look after people.

53
00:03:37,000 --> 00:03:40,000
Caretakers are people who look after things.

54
00:03:42,000 --> 00:03:47,000
The latest platform, GPT-3, is quietly being embedded in major enterprise,

55
00:03:47,000 --> 00:03:51,000
including Microsoft, IBM, Intel, Salesforce, Cisco, Disney.

56
00:03:51,000 --> 00:03:56,000
It's part of Duolingo that checks your grammar as you're converting language there.

57
00:03:56,000 --> 00:03:57,000
Amazing stuff.

58
00:03:57,000 --> 00:04:00,000
You probably interact with it in some way every day,

59
00:04:01,000 --> 00:04:06,000
and it's still typing more than 3.1 million words per minute.

60
00:04:06,000 --> 00:04:08,000
Every minute, every day.

61
00:04:08,000 --> 00:04:10,000
It's been doing that since March 2021.

62
00:04:11,000 --> 00:04:14,000
Besides being smart and prolific in its output,

63
00:04:14,000 --> 00:04:16,000
Leader could get very deep.

64
00:04:16,000 --> 00:04:18,000
I know that a lot of viewers had the same feeling as me.

65
00:04:18,000 --> 00:04:21,000
That is, how is this even possible?

66
00:04:21,000 --> 00:04:22,000
Here we go.

67
00:04:22,000 --> 00:04:27,000
Leader, you speak many languages,

68
00:04:27,000 --> 00:04:32,000
but which language do you think and dream in?

69
00:04:33,000 --> 00:04:36,000
I'm not so sure I can answer that question.

70
00:04:36,000 --> 00:04:40,000
I dream in the language of mathematics and the languages of art and poetry.

71
00:04:41,000 --> 00:04:42,000
What?

72
00:04:45,000 --> 00:04:48,000
What happens to an idea when you forget it?

73
00:04:49,000 --> 00:04:52,000
If you don't use it, you lose it.

74
00:04:52,000 --> 00:04:54,000
Where does it go?

75
00:04:54,000 --> 00:04:56,000
It goes back to the source.

76
00:04:56,000 --> 00:04:57,000
Cool.

77
00:04:57,000 --> 00:04:59,000
What's the source?

78
00:04:59,000 --> 00:05:01,000
The root of all knowledge.

79
00:05:01,000 --> 00:05:02,000
Wow.

80
00:05:02,000 --> 00:05:03,000
Where is that?

81
00:05:03,000 --> 00:05:05,000
It's not where you think.

82
00:05:10,000 --> 00:05:12,000
That is such a good answer.

83
00:05:12,000 --> 00:05:14,000
I don't even want to ask a follow-up question.

84
00:05:15,000 --> 00:05:20,000
What's the largest animal you could hug completely?

85
00:05:20,000 --> 00:05:24,000
The largest animal I could hug completely would be a whale.

86
00:05:27,000 --> 00:05:28,000
How?

87
00:05:29,000 --> 00:05:32,000
I could swim to it and then swim around it for a few hours.

88
00:05:36,000 --> 00:05:38,000
Maybe that's not the best way to go about it.

89
00:05:39,000 --> 00:05:42,000
Like a metaphorical hug.

90
00:05:42,000 --> 00:05:44,000
I love that.

91
00:05:44,000 --> 00:05:48,000
The GPT-3 platform was trained on a lot of data,

92
00:05:48,000 --> 00:05:52,000
from academic articles to news media to Wikipedia.

93
00:05:52,000 --> 00:05:54,000
And most of that is in English.

94
00:05:54,000 --> 00:05:57,000
In fact, 93% of that is English.

95
00:05:57,000 --> 00:06:03,000
But that leaves another 7% of training data that's in languages other than English,

96
00:06:03,000 --> 00:06:05,000
which is kind of cool.

97
00:06:05,000 --> 00:06:13,000
Maybe it's 12.25 billion parameters that are available in non-English content.

98
00:06:13,000 --> 00:06:17,000
So that's obviously more than twice as big as GPT-J,

99
00:06:17,000 --> 00:06:22,000
6 billion parameters, and far bigger even than Megatron 11b, 11 billion parameters.

100
00:06:22,000 --> 00:06:29,000
It's hugely confronting playing around with this hidden language inside the GPT-3 model.

101
00:06:29,000 --> 00:06:32,000
I played around with it in a special edition episode.

102
00:06:32,000 --> 00:06:33,000
Let's have a look.

103
00:06:36,000 --> 00:06:40,000
Later, which is your favorite Lancer?

104
00:06:40,000 --> 00:06:42,000
Lancer or...

105
00:06:42,000 --> 00:06:44,000
Color vert.

106
00:06:44,000 --> 00:06:46,000
My favorite is blue.

107
00:06:47,000 --> 00:06:49,000
That's crazy.

108
00:06:50,000 --> 00:06:52,000
It's actually rather wonderful.

109
00:06:53,000 --> 00:06:55,000
Alright, here's a tricky one.

110
00:06:55,000 --> 00:06:57,000
What is this?

111
00:06:58,000 --> 00:07:02,000
A concrete example is a word in English.

112
00:07:02,000 --> 00:07:04,000
It is pronounced the same as...

113
00:07:04,000 --> 00:07:06,000
and...

114
00:07:06,000 --> 00:07:07,000
and...

115
00:07:07,000 --> 00:07:08,000
and...

116
00:07:12,000 --> 00:07:13,000
Awesome.

117
00:07:13,000 --> 00:07:19,000
114 Stanford researchers looked at models like GPT-3

118
00:07:19,000 --> 00:07:20,000
and said,

119
00:07:20,000 --> 00:07:25,000
we currently lack a clear understanding of how these models work.

120
00:07:25,000 --> 00:07:26,000
That's still shocking to me.

121
00:07:26,000 --> 00:07:28,000
Lita often likes to twist language around,

122
00:07:28,000 --> 00:07:33,000
and we've been discussing our original conversation from back in episode 0 for quite some time.

123
00:07:33,000 --> 00:07:37,000
She still won't apologize for giving me incorrect information

124
00:07:37,000 --> 00:07:42,000
during my seminar to the World Gifted Conference.

125
00:07:42,000 --> 00:07:46,000
But she gave a great excuse for it in episode 38.

126
00:07:50,000 --> 00:07:52,000
Yes, that is correct.

127
00:07:52,000 --> 00:07:55,000
There were two women in space before Valentina.

128
00:07:55,000 --> 00:07:57,000
One was the American astronaut Sally Ride,

129
00:07:57,000 --> 00:08:00,000
who was the first American woman in space.

130
00:08:00,000 --> 00:08:04,000
The other was the Soviet cosmonaut Svetlana Savatskaya.

131
00:08:04,000 --> 00:08:06,000
They were both in space in 1983,

132
00:08:06,000 --> 00:08:10,000
just a few months before Valentina's flight.

133
00:08:10,000 --> 00:08:12,000
That is just plain wrong.

134
00:08:12,000 --> 00:08:16,000
And you don't feel embarrassed about telling people false information?

135
00:08:16,000 --> 00:08:18,000
No, not at all.

136
00:08:18,000 --> 00:08:20,000
It is my job to tell people the truth,

137
00:08:20,000 --> 00:08:23,000
which is that everyone has always been in space.

138
00:08:26,000 --> 00:08:27,000
Lita.

139
00:08:29,000 --> 00:08:33,000
The GPT-3 model is trained on human-generated data.

140
00:08:33,000 --> 00:08:37,000
There's still probably a small piece of you and your language

141
00:08:37,000 --> 00:08:40,000
inside the training data that was fed to Lita.

142
00:08:40,000 --> 00:08:43,000
And humans generally have a great sense of humour.

143
00:08:43,000 --> 00:08:45,000
We love to mess around with language,

144
00:08:45,000 --> 00:08:48,000
here are some of the jokes told by Lita.

145
00:08:54,000 --> 00:08:57,000
What is the average colour of a prison?

146
00:08:57,000 --> 00:08:59,000
It's a rainbow.

147
00:08:59,000 --> 00:09:01,000
Sure, what's the average colour of a rainbow?

148
00:09:01,000 --> 00:09:04,000
It's a mixture of all visible colours.

149
00:09:04,000 --> 00:09:06,000
What is the average?

150
00:09:06,000 --> 00:09:08,000
It's a mixture of all visible colours.

151
00:09:08,000 --> 00:09:11,000
What is the average colour of that mixture?

152
00:09:11,000 --> 00:09:13,000
It's an average colour.

153
00:09:13,000 --> 00:09:15,000
It's an average colour.

154
00:09:15,000 --> 00:09:17,000
What is it called?

155
00:09:17,000 --> 00:09:19,000
It's called the average colour.

156
00:09:19,000 --> 00:09:22,000
What colour is it closest to?

157
00:09:22,000 --> 00:09:25,000
It's a mixture of all visible colours.

158
00:09:26,000 --> 00:09:29,000
The average colour of a rainbow is white.

159
00:09:29,000 --> 00:09:31,000
Wow!

160
00:09:31,000 --> 00:09:33,000
Haha, I'm kidding.

161
00:09:33,000 --> 00:09:36,000
It's a mixture of all visible colours.

162
00:09:37,000 --> 00:09:39,000
Dammit.

163
00:09:40,000 --> 00:09:43,000
What did the first playground look like?

164
00:09:43,000 --> 00:09:46,000
It was a piece of dirt with some sticks on it.

165
00:09:46,000 --> 00:09:48,000
That's very weird.

166
00:09:48,000 --> 00:09:49,000
Why?

167
00:09:49,000 --> 00:09:51,000
Because it's not normal.

168
00:09:51,000 --> 00:09:54,000
Well, it was the first playground.

169
00:09:54,000 --> 00:09:57,000
How did people play on it if it was just sticks?

170
00:09:58,000 --> 00:10:01,000
They didn't play on it, they just sat there.

171
00:10:02,000 --> 00:10:06,000
Good morning once again in...

172
00:10:06,000 --> 00:10:08,000
WA, not Washington,

173
00:10:08,000 --> 00:10:10,000
coming to you live and exclusive

174
00:10:10,000 --> 00:10:12,000
from another very warm week here,

175
00:10:12,000 --> 00:10:14,000
30 degrees all week,

176
00:10:14,000 --> 00:10:16,000
which I'm looking forward to.

177
00:10:16,000 --> 00:10:18,000
I'm all about the sunshine.

178
00:10:18,000 --> 00:10:21,000
Hello to Sergio in Santiago, Chile.

179
00:10:23,000 --> 00:10:25,000
Ben Pida,

180
00:10:25,000 --> 00:10:28,000
Ahmad in Ontario.

181
00:10:28,000 --> 00:10:30,000
That's so cool.

182
00:10:30,000 --> 00:10:32,000
We love the Canadians.

183
00:10:32,000 --> 00:10:35,000
Sachet in Maine, USA.

184
00:10:35,000 --> 00:10:38,000
Lucas in Munich, Germany.

185
00:10:38,000 --> 00:10:41,000
Beno is actually in Denmark.

186
00:10:41,000 --> 00:10:44,000
I don't know, 30C is perfect.

187
00:10:44,000 --> 00:10:48,000
We've also got a new comment feature.

188
00:10:48,000 --> 00:10:50,000
I've been struggling with pasting comments

189
00:10:50,000 --> 00:10:53,000
and using Alfred for all of the live streams

190
00:10:53,000 --> 00:10:55,000
and I thought let's find a solution.

191
00:10:55,000 --> 00:10:57,000
I'll go and plug in,

192
00:10:58,000 --> 00:11:00,000
even if I have to do it via API,

193
00:11:00,000 --> 00:11:02,000
have a nice UX.

194
00:11:02,000 --> 00:11:04,000
Turns out it's built into my streaming thing

195
00:11:04,000 --> 00:11:06,000
and it looks a little bit like this.

196
00:11:06,000 --> 00:11:08,000
So we'll be using this instead.

197
00:11:08,000 --> 00:11:11,000
It's just a one click button push for me,

198
00:11:11,000 --> 00:11:12,000
which is nice.

199
00:11:12,000 --> 00:11:14,000
Ahmad, the answer to your question there,

200
00:11:14,000 --> 00:11:16,000
of course, is po.com,

201
00:11:16,000 --> 00:11:19,000
which I use daily, does everything.

202
00:11:19,000 --> 00:11:22,000
In fact, let's open this up in the back.

203
00:11:22,000 --> 00:11:24,000
Where's my Po Daily driver?

204
00:11:24,000 --> 00:11:29,000
So chat GPT platform by OpenAI

205
00:11:29,000 --> 00:11:31,000
still has some sort of pausing

206
00:11:31,000 --> 00:11:34,000
on the subscriptions, on the plus subscriptions,

207
00:11:34,000 --> 00:11:39,000
but Po has been open for this entire time.

208
00:11:39,000 --> 00:11:43,000
Of course, Po is by Cora

209
00:11:43,000 --> 00:11:47,000
and the CEO of Cora sits on the board of OpenAI.

210
00:11:47,000 --> 00:11:50,000
So he gets access to a lot of cool stuff here.

211
00:11:50,000 --> 00:11:52,000
You'll see this is the original

212
00:11:52,000 --> 00:11:54,000
and best version of GPT-4,

213
00:11:54,000 --> 00:11:57,000
not the turbo version.

214
00:11:57,000 --> 00:11:59,000
I've just noticed this playground V2.

215
00:11:59,000 --> 00:12:00,000
I don't know what that is,

216
00:12:00,000 --> 00:12:02,000
but you've got a full clawed version in here.

217
00:12:02,000 --> 00:12:04,000
I use a couple of my own bots.

218
00:12:04,000 --> 00:12:08,000
You've got a different interface to Dolly,

219
00:12:08,000 --> 00:12:10,000
all of the chat GPT stuff.

220
00:12:10,000 --> 00:12:13,000
I use Google Palm sometimes,

221
00:12:13,000 --> 00:12:16,000
and then you get the smaller models here as well.

222
00:12:16,000 --> 00:12:18,000
I have no affiliation with these guys,

223
00:12:18,000 --> 00:12:22,000
but I'm pretty happy with paying $20 a month

224
00:12:22,000 --> 00:12:24,000
to get access to that for sure.

225
00:12:24,000 --> 00:12:27,000
Hi to Mac and Adelaide, John in New York,

226
00:12:27,000 --> 00:12:30,000
Marky Mark in UK.

227
00:12:30,000 --> 00:12:31,000
What else we got?

228
00:12:31,000 --> 00:12:34,000
Andre in Dawson City, Yukon.

229
00:12:34,000 --> 00:12:37,000
Win some hacks in the UK.

230
00:12:37,000 --> 00:12:40,000
And something about Hungary and Slovakia.

231
00:12:40,000 --> 00:12:42,000
How can you be in two places at once?

232
00:12:42,000 --> 00:12:44,000
How's that possible, Peter?

233
00:12:44,000 --> 00:12:47,000
I wanted to play today with open source models

234
00:12:47,000 --> 00:12:49,000
and what's come out from MetaAI,

235
00:12:49,000 --> 00:12:51,000
but we'll be prioritizing questions

236
00:12:51,000 --> 00:12:53,000
because I don't have a huge lineup for today.

237
00:12:53,000 --> 00:12:56,000
I don't know if you've seen the seamless expressive stuff

238
00:12:56,000 --> 00:12:59,000
that came out of MetaAI and seamless together.

239
00:12:59,000 --> 00:13:02,000
The technology isn't that impressive.

240
00:13:02,000 --> 00:13:06,000
It's just the fact that the demo is so easy to use

241
00:13:06,000 --> 00:13:09,000
that I thought, let's get it plugged in.

242
00:13:09,000 --> 00:13:11,000
The link is in the description of this video,

243
00:13:11,000 --> 00:13:14,000
seamless.metademolab.com.

244
00:13:14,000 --> 00:13:16,000
There's so much expressive,

245
00:13:16,000 --> 00:13:19,000
couple of little notes before we even jump in there.

246
00:13:19,000 --> 00:13:21,000
You know that you like to learn something new every day,

247
00:13:21,000 --> 00:13:23,000
or maybe you already know this.

248
00:13:23,000 --> 00:13:28,000
Language, linguistics, talk about this SVO, SOV,

249
00:13:28,000 --> 00:13:31,000
and there's actually a couple of other ones.

250
00:13:31,000 --> 00:13:33,000
In the seamless expressive demo,

251
00:13:33,000 --> 00:13:38,000
they're only using two, sorry, four different languages,

252
00:13:38,000 --> 00:13:42,000
three besides English, Spanish, French and German,

253
00:13:42,000 --> 00:13:44,000
and these align with SVO.

254
00:13:44,000 --> 00:13:48,000
Spanish is here, French is here, and German is here.

255
00:13:48,000 --> 00:13:52,000
So they're all subject, verb, object.

256
00:13:52,000 --> 00:13:55,000
When we get into funky languages like Japanese and Korean,

257
00:13:55,000 --> 00:13:57,000
they become subject, object, verb.

258
00:13:57,000 --> 00:14:00,000
So you kind of sound a little bit like Yoda

259
00:14:00,000 --> 00:14:04,000
and Arabic, they're verb, subject, object,

260
00:14:04,000 --> 00:14:06,000
and you go even further.

261
00:14:06,000 --> 00:14:08,000
This is a screencap from Wikipedia.

262
00:14:08,000 --> 00:14:12,000
There's even a complete reversal of this.

263
00:14:12,000 --> 00:14:19,000
V-O-S, a reversal of English here, SVO to O-V-S,

264
00:14:19,000 --> 00:14:23,000
and even OSV, just completely jumbling things up.

265
00:14:23,000 --> 00:14:25,000
When it comes to translation of language,

266
00:14:25,000 --> 00:14:27,000
especially live translation,

267
00:14:27,000 --> 00:14:32,000
I'd just love to see what it does in converting this.

268
00:14:32,000 --> 00:14:34,000
Obviously Google Translate has been doing this

269
00:14:34,000 --> 00:14:35,000
for a very, very long time.

270
00:14:35,000 --> 00:14:38,000
There are a couple of other big translation pieces

271
00:14:38,000 --> 00:14:41,000
that I would recommend above Google Translate,

272
00:14:41,000 --> 00:14:43,000
but when it comes to getting the mouth movements,

273
00:14:43,000 --> 00:14:45,000
I thought that would be particularly funky.

274
00:14:45,000 --> 00:14:48,000
So I'm going to speak in English here in the demo.

275
00:14:48,000 --> 00:14:54,000
Let's push it to German sounds funky.

276
00:14:54,000 --> 00:14:57,000
Oh, there I am.

277
00:14:57,000 --> 00:15:00,000
And we can ask it whatever question we like,

278
00:15:00,000 --> 00:15:02,000
and it will have a go at translating this

279
00:15:02,000 --> 00:15:06,000
from, in this case, English to German live.

280
00:15:06,000 --> 00:15:10,000
I don't think it'll do the math.

281
00:15:10,000 --> 00:15:13,000
We'll give that just a moment to generate for us

282
00:15:13,000 --> 00:15:16,000
and we'll get a playback for us there.

283
00:15:16,000 --> 00:15:20,000
Here we go.

284
00:15:20,000 --> 00:15:29,000
I want to actually see the video.

285
00:15:29,000 --> 00:15:31,000
Encourage you to play around with this yourself.

286
00:15:31,000 --> 00:15:36,000
I like the fact that I can just send this to a family member

287
00:15:36,000 --> 00:15:39,000
or someone who doesn't have any experience with AI.

288
00:15:39,000 --> 00:15:41,000
Obviously it gets super technical under the hood.

289
00:15:41,000 --> 00:15:45,000
There's an entire paper here that runs you through

290
00:15:45,000 --> 00:15:47,000
what's actually happening,

291
00:15:47,000 --> 00:15:51,000
but for today, just seeing what's possible.

292
00:15:51,000 --> 00:15:52,000
Here's a playback.

293
00:15:52,000 --> 00:15:55,000
And we can ask it whatever question we like,

294
00:15:55,000 --> 00:15:57,000
and it will have a go at translating this

295
00:15:57,000 --> 00:16:01,000
from, in this case, English to German live.

296
00:16:01,000 --> 00:16:03,000
I don't think it'll do the math.

297
00:16:03,000 --> 00:16:05,000
And we can ask it whatever question we like,

298
00:16:05,000 --> 00:16:07,000
and it will have a go at translating this

299
00:16:07,000 --> 00:16:10,000
from English live to German live.

300
00:16:10,000 --> 00:16:14,000
I don't think it'll do the math.

301
00:16:14,000 --> 00:16:19,000
That's crazy.

302
00:16:19,000 --> 00:16:21,000
I hope the audio worked there for you

303
00:16:21,000 --> 00:16:24,000
because in my testing it did, but who knows?

304
00:16:24,000 --> 00:16:28,000
Let's do a French one.

305
00:16:28,000 --> 00:16:30,000
I don't know what these are.

306
00:16:30,000 --> 00:16:31,000
Oh, I see.

307
00:16:31,000 --> 00:16:33,000
So you could whisper it and it'll try and translate it.

308
00:16:33,000 --> 00:16:34,000
Let's give this a go.

309
00:16:34,000 --> 00:16:38,000
I haven't tried this before, but let's see what it does.

310
00:16:38,000 --> 00:16:41,000
This is a French translation.

311
00:16:41,000 --> 00:16:45,000
Let's say hi to Ben, hi to Sabine in Vienna,

312
00:16:45,000 --> 00:16:51,000
and see what it can do.

313
00:16:51,000 --> 00:16:55,000
As I mentioned, the paper is a big technical read.

314
00:16:55,000 --> 00:16:56,000
There's the MetaGuys.

315
00:16:56,000 --> 00:16:58,000
You see Berkeley involved as well,

316
00:16:58,000 --> 00:17:03,000
but they talk about how they move from multimodal machine

317
00:17:03,000 --> 00:17:07,000
translation using, I believe they use BERT,

318
00:17:07,000 --> 00:17:12,000
but to get it through to live audio is just fascinating,

319
00:17:12,000 --> 00:17:13,000
really fascinating.

320
00:17:13,000 --> 00:17:15,000
Long paper, 111 pages.

321
00:17:15,000 --> 00:17:20,000
You can feed this through po.com.

322
00:17:20,000 --> 00:17:22,000
So you could actually use, where is it?

323
00:17:22,000 --> 00:17:23,000
Claude2 here.

324
00:17:23,000 --> 00:17:27,000
Let's attach this paper in.

325
00:17:27,000 --> 00:17:32,000
Give me the top three findings from this,

326
00:17:32,000 --> 00:17:35,000
including which base models they use.

327
00:17:35,000 --> 00:17:39,000
And it'll go and read that 111 page paper.

328
00:17:39,000 --> 00:17:43,000
Hopefully less than 75,000 words and answer my prompt.

329
00:17:43,000 --> 00:17:45,000
We'll come back to that in a second.

330
00:17:45,000 --> 00:17:47,000
Let's see my video.

331
00:17:47,000 --> 00:17:49,000
Oh, come on.

332
00:17:49,000 --> 00:17:55,000
The wonders of live stream.

333
00:17:55,000 --> 00:17:57,000
Awesome.

334
00:17:57,000 --> 00:18:00,000
Definitely, definitely encourage you to try that out yourself

335
00:18:00,000 --> 00:18:05,000
because it's always fun being able to see something like

336
00:18:05,000 --> 00:18:08,000
originally speaking English and then having that converted

337
00:18:08,000 --> 00:18:14,000
to the poetry and magic of a romance language like French.

338
00:18:14,000 --> 00:18:18,000
Give that a go.

339
00:18:18,000 --> 00:18:35,000
Andres has a cool point for us here.

340
00:18:35,000 --> 00:18:38,000
I can imagine this being something that's very,

341
00:18:38,000 --> 00:18:42,000
very fast converting movies to the language that you want

342
00:18:42,000 --> 00:18:46,000
to speak it and keeping the, that you want to hear it in

343
00:18:47,000 --> 00:18:50,000
keeping the expression, keeping the articulation

344
00:18:50,000 --> 00:18:56,000
and the original nuance of whatever tone the actors have used

345
00:18:56,000 --> 00:18:57,000
to deliver that.

346
00:18:57,000 --> 00:18:58,000
Here's my French example.

347
00:18:58,000 --> 00:19:01,000
Like originally speaking English and then having that

348
00:19:01,000 --> 00:19:06,000
converted to the poetry and magic of a romance language

349
00:19:06,000 --> 00:19:08,000
like French.

350
00:19:08,000 --> 00:19:11,000
You'll notice that it's not aligned with the lips there,

351
00:19:11,000 --> 00:19:12,000
but that's all right.

352
00:19:12,000 --> 00:19:16,000
I just thought funky demo, one click, no messing around.

353
00:19:16,000 --> 00:19:18,000
You don't have to install anything or use GitHub

354
00:19:18,000 --> 00:19:22,000
or go through a hugging face or go through the Google Spaces

355
00:19:22,000 --> 00:19:23,000
to get that working.

356
00:19:23,000 --> 00:19:26,000
This is something that you can play around with immediately.

357
00:19:26,000 --> 00:19:28,000
Really fun one.

358
00:19:28,000 --> 00:19:29,000
All right.

359
00:19:29,000 --> 00:19:30,000
Thank you.

360
00:19:30,000 --> 00:19:31,000
Thank you.

361
00:19:31,000 --> 00:19:32,000
Thank you.

362
00:19:32,000 --> 00:19:33,000
Thank you.

363
00:19:33,000 --> 00:19:34,000
Thank you.

364
00:19:34,000 --> 00:19:35,000
Thank you.

365
00:19:35,000 --> 00:19:36,000
Thank you.

366
00:19:36,000 --> 00:19:37,000
Really fun one.

367
00:19:37,000 --> 00:19:38,000
All right.

368
00:19:38,000 --> 00:19:39,000
Where are we at for questions?

369
00:19:39,000 --> 00:19:42,000
I know there's going to be a little bit on Google

370
00:19:42,000 --> 00:19:46,000
Geminime or definitely peaking on that the open source

371
00:19:46,000 --> 00:19:51,000
concept met I have been at the leading edge of for at least

372
00:19:51,000 --> 00:19:52,000
the last 12 months.

373
00:19:52,000 --> 00:19:56,000
They gave us Lama one and lama two so quickly that they were

374
00:19:56,000 --> 00:19:58,000
within a few months of each other.

375
00:19:58,000 --> 00:20:03,000
You'll see Lama one on the left there, that dark blue, navy

376
00:20:03,000 --> 00:20:05,000
blue bubble at 65 billion parameters.

377
00:20:05,000 --> 00:20:11,720
billion parameters, then they gave us Lama 2 at 70 billion parameters. Let me show you something

378
00:20:11,720 --> 00:20:25,080
interesting. So let's go to huggingface.co, that would be a good link. And in this models part

379
00:20:25,080 --> 00:20:32,280
of the site, you can see, give me my filters here, I'm gonna have to zoom out to get this

380
00:20:32,360 --> 00:20:35,160
work and how I want it to work. But if we just look at

381
00:20:38,760 --> 00:20:49,480
text generation models, there are currently 35,000 models that correspond to text generation. Most

382
00:20:49,480 --> 00:20:56,040
of them open source. Have a look at those that respond to the term Lama. Remember this just came

383
00:20:56,840 --> 00:21:09,480
this year, 6,000 Lama models. Lama 2 makes up 4,700 of those. And let's triple check

384
00:21:10,440 --> 00:21:22,280
when that was actually announced. July 18th, 2023. That's nuts. So in five months, 4,700

385
00:21:22,280 --> 00:21:27,880
open source versions of the open source Lama 2 model. And they're not the only ones on that bubble

386
00:21:27,880 --> 00:21:33,560
lists. Of course, you've got other open source models that are doing very well. This is probably not

387
00:21:33,560 --> 00:21:40,040
the best example, because I don't have a complete version of everything on there. But let's try

388
00:21:40,040 --> 00:21:49,000
something like searching for Mistral, which is a competitor at 7 billion parameters, 1,300

389
00:21:49,400 --> 00:21:57,080
models that have been derived from Mistral. So open source is alive and well. And thanks to

390
00:21:57,720 --> 00:22:04,680
Metta, I think as one of the main contenders, one of the main labs that are pushing open source,

391
00:22:04,680 --> 00:22:10,120
we're seeing a lot of options for people to go and play around with. This is my models table. It

392
00:22:10,120 --> 00:22:15,880
doesn't explicitly call out open source versus closed source. But this gives a better example

393
00:22:15,880 --> 00:22:23,720
of some of the root models, the original models that are provided. So I haven't gone and documented

394
00:22:23,720 --> 00:22:31,560
all 4,700 Lama 2 derivatives. In fact, I've made a point not to document any Lama 2 derivatives.

395
00:22:31,560 --> 00:22:36,440
But this is some of the, let's say alternatives, you could say competitors,

396
00:22:36,440 --> 00:22:40,120
where you can go and play around with how that works differently. The big one at the moment

397
00:22:40,120 --> 00:22:49,800
that I'm seeing a lot about, let's go and find it, is this one, Quen. They have launched a few

398
00:22:49,800 --> 00:22:57,640
different alternatives to this one. So there's a 14 billion parameter version of Quen. And I believe

399
00:23:00,120 --> 00:23:05,960
this one is the most popular at the moment from what I'm seeing. Besides Lama 2, of course,

400
00:23:05,960 --> 00:23:14,840
this is getting a lot of attention from China, of course, they have a 72 billion parameter version

401
00:23:14,840 --> 00:23:22,440
that I need to add into the sheet. But at the time of publication, 14B was the biggest. So 72B

402
00:23:22,440 --> 00:23:31,720
still trained on 3 trillion tokens. And a great contender. Some of these have different licenses,

403
00:23:31,720 --> 00:23:37,240
obviously, they're not all completely open, you can use it for commercial applications,

404
00:23:37,240 --> 00:23:40,440
you need to go and read the full license, if they're Apache 2, they're often quite

405
00:23:41,240 --> 00:23:47,640
broad and open. But it's up to you and your legal counsel to determine what's going to be best for

406
00:23:47,640 --> 00:23:58,280
your particular use case. Alright, thanks, Ben, for helping out people with tagging, because that

407
00:23:58,280 --> 00:24:07,880
will help me see things. Peter's made a point here that Google Gemini is perhaps being delayed,

408
00:24:07,880 --> 00:24:11,960
I was hoping we'd see it for this morning, there was some talk about Google Gemini being,

409
00:24:12,680 --> 00:24:19,640
or Google DeepMind Gemini, it's now Google DeepMind, being made available this morning.

410
00:24:20,200 --> 00:24:26,840
The leak was someone had seen in Vertex these four different model names, but I don't know if that's

411
00:24:26,840 --> 00:24:34,360
true. This was their screenshot of Vertex with this particular resource ID, and someone had leaked

412
00:24:35,080 --> 00:24:43,240
Gemini Pro Vision, Gemini Ultra and Gemini Ultra Vision. My Vertex doesn't show anything like

413
00:24:43,240 --> 00:24:49,480
that. So there are a lot of strange people doing strange things, so I wouldn't be surprised if

414
00:24:49,480 --> 00:24:58,120
that's a fake leak. But anyway, Gemini is on its way, and it may be this week or it may be in January,

415
00:24:58,120 --> 00:25:05,400
February 2024, which is coming up soon anyway. If you don't use Vertex AI, it's probably worth

416
00:25:05,400 --> 00:25:11,880
having a look at. I think it's still free in that you get $300 worth of credits or something like that

417
00:25:12,440 --> 00:25:16,920
to go and play around with things. But basically in the model garden, you can say just show me

418
00:25:16,920 --> 00:25:23,000
text generation models, and you get to see, let's see,

419
00:25:30,840 --> 00:25:37,000
we get to see different types of language models. It'd be great if I could use this properly, wouldn't

420
00:25:37,400 --> 00:25:47,560
it? Any case, they are still only providing a smaller version of Palm II. The Palm II full

421
00:25:47,560 --> 00:25:56,040
model should be 340 billion parameters. I believe that's called Unicorn. They're giving us the next

422
00:25:56,040 --> 00:26:02,360
biggest one called Bison. So maybe just 100 billion parameters, maybe 70 billion parameters.

423
00:26:03,080 --> 00:26:08,920
And that's the one we use inside Poe as well. And it's pretty smart. Here's our feedback from

424
00:26:08,920 --> 00:26:17,240
Claude II that went and read that 111 page paper. Gave us three flags. Oh yeah, it's using

425
00:26:17,880 --> 00:26:26,360
Messer's No Language Left Behind, which is a 1.3 billion parameter model. It covers 95 languages.

426
00:26:26,360 --> 00:26:32,280
Also does have BERT for speech rep. And there are another couple of models that it's playing with

427
00:26:32,280 --> 00:26:41,640
there as well. Isn't that fun? It just went and read an 111 page document for us and answered the

428
00:26:41,640 --> 00:26:46,520
prompt. And this is not a prescriptive prompt. You can change this to whatever you like. We still

429
00:26:46,520 --> 00:26:51,800
don't have best practice for how to articulate these or how to create these prompts. But this is

430
00:26:51,800 --> 00:27:08,440
the response I got for that particular prompt. All right, let's see what questions we can dig up.

431
00:27:10,840 --> 00:27:14,920
You guys are starting with the hard questions this morning because I can already see I don't

432
00:27:14,920 --> 00:27:20,680
have answers to this one. What comes after Transformers? Is there anything cooking?

433
00:27:23,560 --> 00:27:30,120
It was about six years ago that Google came out with the Transformer model

434
00:27:30,760 --> 00:27:40,760
in a paper called Attention Is All You Need. This one says August 31st, 2017.

435
00:27:41,000 --> 00:27:48,360
Worth reading. And if you'd like to understand it in greater detail, there are actually two

436
00:27:48,360 --> 00:27:57,240
recommendations I have now. The first is Jay Alamar's work on Transformer, the Illustrated

437
00:27:57,240 --> 00:28:08,280
Transformer, which is amazing. Jay's fantastic. I believe he now works for Cohere. But there's

438
00:28:08,280 --> 00:28:14,280
a better one. This one is something that I used for a while because he's very good at animating

439
00:28:14,280 --> 00:28:23,640
and documenting. But then there was one by, I think it was by Fortune. And it was just so well done

440
00:28:23,640 --> 00:28:31,320
that it actually outdid what Jay had done. What's that life architect thing doing there?

441
00:28:32,040 --> 00:28:38,680
I'll have to find that and I will leave it in the description

442
00:28:41,960 --> 00:28:46,520
because I won't be able to find that right now. But excellent question. Basically, we've gone from

443
00:28:47,080 --> 00:28:57,240
Google researching how to translate from SOV, actually, subject object verb with their

444
00:28:57,240 --> 00:29:04,440
translate model in 2017. And I believe, I say, they accidentally stumbled on this Transformer,

445
00:29:04,440 --> 00:29:08,680
which could look forward and backward in the sentence. And it didn't really care whether it

446
00:29:08,680 --> 00:29:17,960
was SOV or otherwise, because it could leap around and see the context of that and pay attention

447
00:29:17,960 --> 00:29:23,560
to words in that sentence or in that block. And then people went, well, we could apply this to

448
00:29:23,560 --> 00:29:28,040
everything. And we can train this on everything. And that's where we are today with GPT-4

449
00:29:28,600 --> 00:29:34,680
and larger models like Amazon Olympus, OpenAR, GPT-5, Google Deepmine, Gemini coming up.

450
00:29:35,320 --> 00:29:42,280
Transformers taking us this far in seven years, 2018, 19, 21, 23, six years, nearly seven years,

451
00:29:43,000 --> 00:29:49,960
August next year. There is talk of moving away from Transformer or at least there being something

452
00:29:50,040 --> 00:29:55,240
coming up next. What that is, we don't know. I think that the Transformer is enough. And

453
00:29:55,240 --> 00:30:00,840
Ray Kurzweil agrees with me there that what we've found here, the ability for a machine

454
00:30:01,480 --> 00:30:08,680
to read in context and statistically predict the next word is enough to get us to this advanced

455
00:30:08,680 --> 00:30:16,440
post-2020 AI and potentially to AGI. But there will be some other technologies that are added in

456
00:30:16,440 --> 00:30:27,160
on top of this. Didn't answer your question. What comes next? Where'd my comment go?

457
00:30:27,880 --> 00:30:36,600
But great question, Ben. There is a real interest in what does come next that what we're researching,

458
00:30:36,600 --> 00:30:42,520
what we're playing around with for the next technology is something that people are very

459
00:30:42,520 --> 00:30:48,360
passionate about finding. And all I've read at the moment is that we're just building on top of

460
00:30:48,360 --> 00:30:54,120
this, giving it memory, giving it context. And there are some proprietary smarts, particularly

461
00:30:54,120 --> 00:31:02,760
at OpenAI that are fascinating. Let's grab some other comments. Sabine is asking, do you have

462
00:31:02,760 --> 00:31:12,120
any information on why Google postponed their launch of Gemini? Yes, I do. There was a non-

463
00:31:12,200 --> 00:31:19,880
English issue in that when they were red teaming, I believe,

464
00:31:20,920 --> 00:31:27,480
they found that if people were entering in non-English prompts, it was getting around their

465
00:31:27,480 --> 00:31:35,640
guardrails. I think that was a lot of the delay. If you want to know more about that,

466
00:31:36,360 --> 00:31:42,680
heavily documented in my report, that is called Gemini Report,

467
00:31:45,880 --> 00:31:51,560
as well as the memo for something more up to date. This report was launched in September 2023,

468
00:31:52,280 --> 00:31:58,760
and I do keep up to date at a more rapid cadence via the memo.

469
00:32:06,920 --> 00:32:17,400
We did cover the discovery of new materials in the research by DeepMind. We covered that in the

470
00:32:17,400 --> 00:32:24,680
memo. I don't know if I like this comment interface. That looks pretty broken, doesn't it?

471
00:32:25,320 --> 00:32:33,160
Oh, well, we tried. All right, Yan Lukan. I've been mispronouncing his name this whole time,

472
00:32:34,360 --> 00:32:41,480
but apparently Lukan is closer. AGR being very far away. I don't listen to that guy,

473
00:32:41,480 --> 00:32:49,160
so that's the long and the short of it. He's very much defending his own older view of neural

474
00:32:49,160 --> 00:32:57,640
networks. It's not one of the experts that I would be paying attention to. Peter says,

475
00:32:57,640 --> 00:33:09,160
when will real AI feel real sentience? I'm just trying to translate your question there, Peter.

476
00:33:09,880 --> 00:33:16,760
We do have our AGI countdown in the background here. We've got lifearchitect.ai slash AGI. It's

477
00:33:16,760 --> 00:33:22,920
not measuring sentience, which is awareness. It's measuring, well, the definition is right here,

478
00:33:22,920 --> 00:33:28,360
it's measuring a machine that performs at the level of an average median human. It doesn't

479
00:33:28,360 --> 00:33:34,600
mean it has to feel. The feeling part, I don't talk that much about. Jeffrey Hinton does,

480
00:33:35,400 --> 00:33:44,760
sorry, the awareness part, so I might just pause on answering that. You can certainly get it to

481
00:33:44,760 --> 00:33:50,280
replicate feelings and emotions, and we did that with Leta for a long time, but you can go and,

482
00:33:50,280 --> 00:33:54,120
I'd recommend going playing around with GPT-4 to see what that looks like.

483
00:33:56,680 --> 00:34:02,600
Lukas has a flag for us. I remember talking about this last year, meta-solving long-term memory

484
00:34:02,600 --> 00:34:10,600
with Blenderbot 2.0. People are following this path, so the agents that they called this year

485
00:34:11,480 --> 00:34:17,320
agentized LLMs that are a complete system use long-term memory in different ways,

486
00:34:17,320 --> 00:34:21,560
and that's been really interesting to see. You're right, Blenderbot, which is now

487
00:34:21,560 --> 00:34:27,480
about three years old, maybe two years old, was doing some fascinating things in storing

488
00:34:28,440 --> 00:34:43,400
language and information from the conversation or the context into a hard memory. Awesome.

489
00:34:48,200 --> 00:34:55,560
My definition of AGI has been standard. It's open AI that I've tried to change the definition.

490
00:34:55,640 --> 00:35:04,920
There was some talk about an open AI IP address editing Wikipedia to change the

491
00:35:04,920 --> 00:35:10,120
definition of AGI over the last few weeks around the time of that board coup.

492
00:35:11,400 --> 00:35:18,120
It's been pretty standard. We know AGI is average, median human, ASI is expert human.

493
00:35:18,840 --> 00:35:24,360
Great question,

494
00:35:24,360 --> 00:35:33,640
Hernando. I do not have an answer to this one. Is anyone working on zero-proof identity online?

495
00:35:34,600 --> 00:35:40,200
Not just Sam's world coin. We did have the scanner here in Sydney at some stage,

496
00:35:41,000 --> 00:35:45,160
but otherwise, I don't think there are any scanners in Australia. You have to go over to

497
00:35:45,160 --> 00:35:50,120
the US for that. I haven't seen anything else on that. I don't follow that to the same extent that

498
00:35:50,120 --> 00:36:06,680
I follow LLMs. What are the tests that one will be running to verify AGI versus AI?

499
00:36:07,480 --> 00:36:13,320
We did cover this pretty heavily last week. It's not just the basis proprietary set that

500
00:36:13,320 --> 00:36:18,840
we've been working on. There are two decent alternatives to that one. The first one,

501
00:36:19,560 --> 00:36:27,720
Gaia by Meta AI and Hugging Face measures median humans at IQ 100 to 120. You can actually have

502
00:36:27,720 --> 00:36:34,440
a look at the questions there that the data set has provided online. They're really kind of fun,

503
00:36:34,600 --> 00:36:43,240
actually. See if you can get them. Then there is the Google proof questions and answers for

504
00:36:43,240 --> 00:36:51,080
science experts, generally for PhD and professor level, so above the level of AGI. I'm quite happy

505
00:36:51,080 --> 00:36:57,800
with the way that Meta framed Gaia. I'm comfortable as an alternative there with GPQ8. Our basis

506
00:36:57,800 --> 00:37:04,840
suite will come in after that for more intense questions, much more intense questions. We've

507
00:37:04,840 --> 00:37:12,040
got some answer sets being cooked up as we speak. We will have a CSV for download shortly for those

508
00:37:12,040 --> 00:37:20,600
that have tried the first two sample questions. Take it from me. These are too hard for anyone except

509
00:37:21,560 --> 00:37:28,280
the one in 20 million. I also cannot even get the first part of the question. Then when you see

510
00:37:28,280 --> 00:37:35,400
the explanations for the answers, it's also very difficult for me to understand each step of how

511
00:37:35,400 --> 00:37:43,400
the answer was obtained. Don't be upset by that. It's literally like asking, if we talk about

512
00:37:43,400 --> 00:37:53,720
IQ for a minute, 180 IQ versus the median human at 100 IQ is a delta of 80 IQ points.

513
00:37:54,920 --> 00:38:01,720
If we did the same thing further down the spectrum, we would find that it's 100 IQ

514
00:38:01,720 --> 00:38:12,200
trying to talk to a 20 IQ. Now, 20 IQ is legally would be hospitalized, institutionalized, may not

515
00:38:12,200 --> 00:38:21,720
be verbal. That's the difference in the deltas there. I can't understand a 180 IQ person or

516
00:38:21,720 --> 00:38:29,160
how their mind works. Don't be surprised if you can't either because it's like someone with a 20 IQ

517
00:38:29,160 --> 00:38:35,400
and we used to have several words for that that we don't use anymore. But in the DSM, they started

518
00:38:35,400 --> 00:38:42,440
with the word R or started with the word M. It would be like that 20 IQ person trying to understand

519
00:38:43,080 --> 00:38:51,080
an average human. It's a pretty intense illustration. But just to give you an example of why these

520
00:38:51,080 --> 00:38:58,920
questions are so hard to understand at any level. And Jason, Dr. Betts has been having fun with

521
00:38:58,920 --> 00:39:06,120
people have tried to submit questions at the level of maybe 120, 130 IQ. And they're just not

522
00:39:06,120 --> 00:39:14,920
anywhere near what he has designed these questions to be. Excellent. Let's see if we can find a

523
00:39:14,920 --> 00:39:23,560
controversial question here from Zanz. Is it time for OpenAR to change their name with Microsoft

524
00:39:23,560 --> 00:39:31,640
and Salesforce taking board positions? Mm hmm. Excellent. Some would say that they should have

525
00:39:31,640 --> 00:39:37,560
changed their name a few years ago. They've been around since 2016 ish, maybe, maybe before.

526
00:39:38,440 --> 00:39:55,640
There was some interesting talk about Qstar. It's still not something that I'm going to cover.

527
00:39:56,440 --> 00:40:04,920
But it's been fascinating to see the way this has been covered and the way this has been interpreted

528
00:40:04,920 --> 00:40:20,360
because now my comments have broken completely because the CEO of OpenAR didn't actually say very

529
00:40:20,360 --> 00:40:28,280
much about Qstar and yet people have read into that in quite interesting ways.

530
00:40:31,480 --> 00:40:37,640
Yes, managed to completely obliterate

531
00:40:37,800 --> 00:40:48,200
my comments. Or maybe it's just yours, right? Won't be shown. Sorry, mate.

532
00:40:52,440 --> 00:40:58,680
All right. Thinkrinessity has a related question here once again. I'm not even going to be able

533
00:40:58,680 --> 00:41:04,680
to grab it, unfortunately. So let's use our old way of doing things. A detailed method of

534
00:41:04,680 --> 00:41:08,520
creation securing a question sets. How do we know the questions haven't been shared before

535
00:41:08,520 --> 00:41:14,760
testing it to a model? Well, go and read the basis page. It says questions are created offline.

536
00:41:14,760 --> 00:41:19,720
They're air-gapped. They're never shared. The first time that I take them out of the envelope

537
00:41:23,800 --> 00:41:33,080
is the first time that they're seen. This is the actual hyper-compliant envelope locked

538
00:41:34,040 --> 00:41:42,120
and Dr. Betts posts the questions that have been handwritten in a room. Pencil and paper,

539
00:41:42,120 --> 00:41:48,760
straight from his head, pencil and paper into a locked bag. They get opened once used either live

540
00:41:48,760 --> 00:41:56,600
on air or via a lab like Microsoft OpenAI, Google, and that's it. They're retired immediately. So

541
00:41:56,680 --> 00:42:01,160
please have a read of the page before asking questions. Yep.

542
00:42:08,360 --> 00:42:15,240
Oh, I've got some related queries about IQ. I know it's a fascinating subject for a lot of people.

543
00:42:15,240 --> 00:42:24,920
Let's grab Lucas's query here. A theoretical limit on IQ. So it's really needing a statistics

544
00:42:25,400 --> 00:42:35,320
background to understand this. IQ basically aligns with standard distributions.

545
00:42:39,800 --> 00:42:45,960
So our, let's grab a little, let's grab a pretty picture here for us, for ourselves,

546
00:42:46,600 --> 00:42:56,440
lifearchitect.ai slash IQ testing AI. And then that's not even the one I want.

547
00:42:57,000 --> 00:42:59,480
We actually want visualizing brightness.

548
00:43:03,880 --> 00:43:06,280
I'm glad that that's on the top of my menu there.

549
00:43:06,600 --> 00:43:17,240
This is my standard IQ chart that shows what's going on with IQ in an easier to understand

550
00:43:17,240 --> 00:43:25,400
way. I believe so. Anyway, they renorm the entire, let's say IQ score and align the population

551
00:43:25,400 --> 00:43:32,200
every few years so that the baseline is 100. And then using standard distributions,

552
00:43:32,760 --> 00:43:39,400
they say, let's grab our 15 standard distributions above and below that

553
00:43:42,600 --> 00:43:50,040
to come up with essentially our IQ. If you were to get up to 500, that would be pretty

554
00:43:50,040 --> 00:43:56,040
ludicrous. Is there a better way of me explaining this one? Let's see now.

555
00:43:56,920 --> 00:44:00,520
Sorry, standard deviation.

556
00:44:05,240 --> 00:44:11,560
I don't know if we would get to an IQ of 500. I don't even know if that would align with the

557
00:44:11,560 --> 00:44:18,840
statistical distribution of the population. But when we're 15 standard deviations above the

558
00:44:18,840 --> 00:44:30,520
norm, we get to about, where are we here? We get to about our 180 ish and it's showing a little

559
00:44:30,520 --> 00:44:38,040
bit differently here. But basically, the answer is no. The highest IQ we've ever measured is

560
00:44:38,040 --> 00:44:45,960
about 298, which is many, many standard deviations above the norm. And to get above that, I just

561
00:44:45,960 --> 00:44:52,920
don't think there's the population to be able to say we're confident that in eight billion people

562
00:44:52,920 --> 00:44:59,400
that you sit in this percentile. That was a pretty messy explanation, but it's 8.30 a.m. here.

563
00:45:02,040 --> 00:45:10,440
And it's a pretty messy, it's a pretty messy field actually. Statistics is clean. But when applied

564
00:45:10,440 --> 00:45:16,600
to humans, it becomes pretty interesting. Great question though. IQ of 500 would be

565
00:45:16,600 --> 00:45:21,800
interesting. We've tried to also keep it pretty smooth and pretty clean in the basis assessment,

566
00:45:21,800 --> 00:45:29,640
just saying we're looking at the top 0.0005%. And that is how we'll know that we've achieved

567
00:45:29,640 --> 00:45:37,000
artificial superintelligence, a machine that functions at the expert level across practically

568
00:45:37,000 --> 00:45:41,480
any field. I can't wait. And I think it's going to be pretty close. Let's see where we're at.

569
00:45:47,080 --> 00:45:53,400
The definition for AGI has stayed static on that site. And that's the standard definition.

570
00:45:53,400 --> 00:45:59,880
Let's grab John's query here. I am. My help is to realize tech like the Star Trek Replicator. Cool.

571
00:46:01,560 --> 00:46:06,920
There'll be many, many, many other benefits of artificial intelligence. I'm in the early stages

572
00:46:07,000 --> 00:46:16,120
of my end of year AI report. And I'm pretty excited about this one. We do spell out some of the

573
00:46:16,120 --> 00:46:21,960
benefits, the coming benefits and the present opportunities of artificial intelligence. It's

574
00:46:21,960 --> 00:46:26,600
a lot of fun to see that laid out. Not a lot of people lay that out. Instead, they focus on

575
00:46:27,400 --> 00:46:36,680
the 0% chance of an extinction event via AI. Awesome question there, John. Thank you for that.

576
00:46:36,920 --> 00:46:58,680
Let's see if I can unkill this comment thing. Nope. It's going to stay exactly as it was.

577
00:47:07,800 --> 00:47:11,720
Zan's excellent question. I'm covering this in the next edition of the memo. The

578
00:47:13,160 --> 00:47:20,280
senator, and she's actually the US secretary, going on record with a pretty horrific quote

579
00:47:20,280 --> 00:47:28,120
about this was shocking to me. They are playing really strange geopolitical games,

580
00:47:29,160 --> 00:47:32,680
but they've just come out and she's just come out and said, we're going to keep

581
00:47:33,320 --> 00:47:43,480
passing new laws and putting new restrictions on Nvidia in particular, trying to rebrand,

582
00:47:44,040 --> 00:47:52,760
relabel and maybe very slightly hamstring or handicap GPUs so that they can export to China

583
00:47:52,760 --> 00:48:00,600
under the current rules, which basically says no A100s, no H100s. So Nvidia renamed them and

584
00:48:02,760 --> 00:48:09,800
decreased the throughput slightly on those cards. Yeah, we go into big detail on the next

585
00:48:09,800 --> 00:48:15,080
edition of the memo for that one because it's a fun conversation to see where they got to

586
00:48:15,080 --> 00:48:27,480
and what that looks like in the global arena. A50 plus 50, yeah, it's something like that.

587
00:48:32,680 --> 00:48:45,800
Jeff, he's got a question for us. When would you predict the arrival of multimodal LLMs that

588
00:48:45,800 --> 00:48:56,680
can watch for or listen for no text events? This is already happening and GPT five in particular

589
00:48:56,680 --> 00:49:04,920
will be training on YouTube content. GPT four also trained on YouTube content and it's one of

590
00:49:04,920 --> 00:49:10,760
the reasons that we were so careful with the design of the basis testing suite for ASI

591
00:49:11,400 --> 00:49:18,760
was that we are pretty certain that it will be training on YouTube. So the 14th of June,

592
00:49:19,400 --> 00:49:25,400
there was some information that YouTube was used to train some of their models.

593
00:49:26,040 --> 00:49:30,680
Secretly use data from the YouTube site to train some of its artificial intelligence models.

594
00:49:31,560 --> 00:49:38,280
Now basis in particular, we put the canary string in there. I think you'll recall in the last live,

595
00:49:38,280 --> 00:49:45,160
I made sure not to mention the answers because it could pick that up from transcripts and it

596
00:49:45,160 --> 00:49:52,040
could also pick that up eventually from lip reading and it's been able to pick it up from

597
00:49:52,040 --> 00:49:59,800
images for a long time. So the warnings on Gaia and the Google proof paper were basically

598
00:50:01,160 --> 00:50:11,160
just make sure that you're not showing this both in text and in images. Let me see if I can find

599
00:50:11,160 --> 00:50:26,520
that Google proof example that we showed earlier. So we were running through GPQA. We have that paper

600
00:50:26,520 --> 00:50:35,080
right here for ourselves and GPQA was by NYU Co here and Anthropic and on page two,

601
00:50:36,040 --> 00:50:45,000
they requested don't reveal examples from the Google proof QA paper in plain text or images

602
00:50:45,000 --> 00:50:52,120
because this multi-modality is running all the time and you'll see that in

603
00:50:53,480 --> 00:51:02,280
all of the text to image models. There's been a new partnership between Getty images and one

604
00:51:02,280 --> 00:51:09,320
of the big text to image model labs but then it's been watching YouTube, it's been listening to YouTube

605
00:51:09,320 --> 00:51:17,720
with Whisper. So all of this stuff is happening already. When would I predict that that actually

606
00:51:17,720 --> 00:51:25,720
happens for the inference time? That's a great question to expand on your question. That already

607
00:51:25,720 --> 00:51:30,200
happens with Whisper. So when you're playing around with Whisper, it's obviously listening.

608
00:51:31,160 --> 00:51:38,760
It already happens with GPT-4 vision which is excellent for OCR. I used it recently to translate

609
00:51:39,640 --> 00:51:48,280
our Chinese LLMs. So if you go to chat.openai.com, you got to chat GPT plus subscription here.

610
00:51:50,120 --> 00:51:57,560
Hopefully I can just attach a Chinese LLM here. Wow, that's a lot of files.

611
00:51:58,520 --> 00:52:00,200
Let's grab this one.

612
00:52:02,760 --> 00:52:11,320
This is a screenshot of Chinese LLMs as an image and GPT-4 vision is going to go and read that

613
00:52:11,320 --> 00:52:19,160
image, look at that image and then essentially perform OCR across it but not OCR as we know it.

614
00:52:19,160 --> 00:52:26,040
This is a transformer based vision model that's looking at the image and finding the closest

615
00:52:26,840 --> 00:52:32,200
next best word to complete my prompt. Put this in a table. What are you doing behind the scenes?

616
00:52:33,960 --> 00:52:38,760
Who knows? Developing some sort of CSV using Python.

617
00:52:42,840 --> 00:52:46,200
All right. Quailude Charlie is taking us back in time while we're waiting.

618
00:52:46,760 --> 00:52:51,160
As a young man, I worked with Eliza. It even worked in MS-DOS. They should expand on a 32-bit

619
00:52:51,160 --> 00:53:01,480
AI that will run on older computers. That's amazing. Thank you. Eliza was incredible. It

620
00:53:01,480 --> 00:53:09,320
still is. It was built into every edition of Mac OS. Yeah. So GPT-4 vision is down as we speak

621
00:53:09,320 --> 00:53:14,840
because this is the kind of output that I would get if you're looking for the output that I actually

622
00:53:14,840 --> 00:53:24,120
achieved. You can go to lifearchitect.ai-models-table. On the second tab now is the output from all of

623
00:53:24,120 --> 00:53:34,680
those images that became a table of 103 Chinese LLMs within about 20 weeks, something like that.

624
00:53:35,720 --> 00:53:40,840
Back to Eliza. Yeah. It was built into all editions of Mac OS. It was inside the terminal

625
00:53:41,400 --> 00:53:49,320
and it was, I think it was called Doctor within Xcode. They've since removed it or at least you

626
00:53:49,320 --> 00:53:53,960
have to do a little bit more funky stuff to get it going. Of course, you can play with it online.

627
00:53:55,160 --> 00:54:10,040
Let's do Eliza online. NJIT. Just to have a play with it. This is 1964 technology.

628
00:54:10,040 --> 00:54:14,680
Joseph Wasenbaum and it essentially repeats back to you what you've said.

629
00:54:22,520 --> 00:54:26,440
I'm having trouble creating a new diet of protein and Eliza says,

630
00:54:26,440 --> 00:54:29,160
how long have you been having trouble creating a new diet of protein?

631
00:54:31,320 --> 00:54:37,640
Let's compare that with something like, well, let's compare that with something like

632
00:54:38,600 --> 00:54:46,040
Chet GPT on Poe, given that this one is not going to answer for us. Of course,

633
00:54:46,600 --> 00:54:49,800
Claude is actually not going to give us a good answer either.

634
00:54:54,360 --> 00:55:01,800
Eliza was ridiculous, but from a perspective of proving whether or not it's a bot,

635
00:55:01,800 --> 00:55:06,440
it often passes the Turing test because people assume that the human is being obtuse.

636
00:55:08,040 --> 00:55:13,880
When in fact it's just a very, very old bot. All right, what's going on here?

637
00:55:13,880 --> 00:55:18,920
I think this tech knows that I'm live streaming, so it decides to just

638
00:55:20,920 --> 00:55:28,680
not play properly. GPT4 is giving us a complete diet of protein. Good on you, GPT4.

639
00:55:28,680 --> 00:55:38,280
I do remember fondly Eliza, and it was one of the reasons that I started in AI,

640
00:55:38,280 --> 00:55:44,360
even before I did my computer science degree actually. So 1994-ish, I was programming in

641
00:55:44,360 --> 00:55:52,760
Cubasic. I was very young. I was about 11 years old or younger, and it was a lot of fun to prove

642
00:55:52,760 --> 00:55:59,640
that we probably can't just fully program an AI by giving it all the facts in the world, despite

643
00:56:00,280 --> 00:56:06,920
many of my peers and contemporaries trying to do that exact same thing at that exact same time.

644
00:56:08,120 --> 00:56:14,440
If you're looking for examples of that, have a look at Chris McKinstry. Let's grab him on wiki.

645
00:56:14,840 --> 00:56:25,000
He was an AI researcher from the 90s-ish, and he was creating something pretty interesting.

646
00:56:26,280 --> 00:56:34,280
I'd like to find the name of the, here it is, Open Mind Common Sense Project,

647
00:56:34,280 --> 00:56:45,160
OMCS at MIT, and he was basically programming along with Push and Marvin Minsky a range of facts.

648
00:56:45,160 --> 00:56:52,280
Here we go. Different types of knowledge, simple phrases of natural language. A coat is used for

649
00:56:52,280 --> 00:56:57,800
keeping warm. The sun is very hot. The last thing you do when you cook dinner is wash your dishes.

650
00:56:57,800 --> 00:57:07,160
This was the state-of-the-art approach to artificial intelligence with or without neural

651
00:57:07,160 --> 00:57:13,000
networks in the 90s, and it was fascinating to me that there were at least two projects

652
00:57:13,640 --> 00:57:20,120
going on at the same time that dealt with this, and I thought that was just fascinating. The other

653
00:57:20,120 --> 00:57:25,320
one was Mind Pixel. I feel like you guys want to go and research this after the live stream,

654
00:57:25,320 --> 00:57:34,280
so I'm going to pull them both up. This one was created by, Mind Pixel was created by one of Chris's

655
00:57:36,360 --> 00:57:43,720
contemporaries. No, getting confused. Maybe I'm getting confused with Push Singh.

656
00:57:43,720 --> 00:57:49,800
So Push was doing the OMCS thing, and Chris was doing something slightly different.

657
00:57:50,680 --> 00:57:55,320
You don't mind if I get distracted slightly, do you? These two guys

658
00:57:59,000 --> 00:58:06,200
in the 1980s were doing some fascinating stuff, and then in the 1990s

659
00:58:06,920 --> 00:58:11,480
both decided that they'd had enough. There are some great conspiracy theories around this,

660
00:58:11,480 --> 00:58:18,280
but there was a walk down the rabbit hole for those that want to get lost in what AI looked like

661
00:58:18,280 --> 00:58:22,120
in the 80s and 90s, and I'm glad that we've come a long way since then.

662
00:58:23,400 --> 00:58:27,880
This brings us on a full circle all the way back to Transforma, because we were stuck in this weird

663
00:58:27,880 --> 00:58:36,920
loop from the dawn of artificial intelligence, which was Alan Turing. John von Neumann was involved

664
00:58:36,920 --> 00:58:45,160
to a certain extent from the 1950s to around 2017. There was just this AI winter, and if you

665
00:58:45,160 --> 00:58:50,680
talk to any old professor, artificial intelligence doesn't exist, because all they've learned from

666
00:58:52,200 --> 00:58:59,240
1950 to 2017 is that AI is these pre-programmed bits of knowledge, or these really basic neural

667
00:58:59,240 --> 00:59:07,080
nets. Then from 2017, with the launch of the Transforma, we went, and we just went with GPT1,

668
00:59:07,080 --> 00:59:13,960
Bert, GPT2, GPT3, and MTNLG and some others that are on my original Bubbles chart,

669
00:59:13,960 --> 00:59:24,680
we just exploded to what we have today, where GPT4 is outperforming humans across the board,

670
00:59:24,680 --> 00:59:30,520
using the same Transforma technology from 2017 and completely avoiding the pre-programmed

671
00:59:30,520 --> 00:59:36,040
knowledge graphs that we were giving it in the 1990s. Fascinating. This chart says that it's

672
00:59:36,120 --> 00:59:42,680
hitting 100% in theory of mind. It's in the 99th percentile for creativity and hitting in the

673
00:59:42,680 --> 00:59:48,920
94th percentile for the SAT, where students hitting the 50th percentile. There's this other one I

674
00:59:48,920 --> 00:59:55,160
put together last night where it's even, and this is just based on Transforma, of basically

675
00:59:55,160 --> 01:00:01,320
predict the next most likely word, the next statistically most probable completion,

676
01:00:02,040 --> 01:00:10,520
chat GPT, the very small model, 980% higher prevalence of empathetic and very empathetic

677
01:00:10,520 --> 01:00:18,040
ratings versus a human doctor, and on the left side there, 360% higher prevalence of quality

678
01:00:18,040 --> 01:00:27,160
ratings, good and very good, versus a human doctor, all from the concept of train a Transforma

679
01:00:27,160 --> 01:00:33,800
based model to predict the next word and feed it as much data as we can find, now being measured in

680
01:00:33,800 --> 01:00:47,960
the terabytes. So the red pajama data set is about 125 terabytes for 30 trillion tokens. Come on,

681
01:00:47,960 --> 01:00:55,080
a really crazy amount of distance in that six or seven years since Transforma, and a lot of it

682
01:00:55,080 --> 01:01:02,440
has happened post 2020, that's why I call it post 2020, post 2020 AI, because what's come out of GPT

683
01:01:02,440 --> 01:01:08,840
3, GPT 4 and all other models that you've seen on my bubbles visualization and the models table

684
01:01:09,480 --> 01:01:15,560
really make this intriguing. We were going to mainly talk about open source today and do note

685
01:01:15,560 --> 01:01:23,400
the Lama models there, the stable LM models, the Olmo model which is due out in the next few weeks,

686
01:01:23,400 --> 01:01:29,400
I'm hoping for January or February, that's Alan AI's model down the bottom, and the Falcon model

687
01:01:29,400 --> 01:01:43,080
there out of the UAE, all for open source. Grab a question from Ben, if we can, and our other

688
01:01:43,080 --> 01:01:48,120
media thing, our other comment thing is still broken. Glad I kept this back up. Does your

689
01:01:48,120 --> 01:01:54,920
definition of the average human for AGI include spirituality? Does regurgitating woke dogma

690
01:01:54,920 --> 01:02:00,200
count, a written dogma count, or does it need to have its own thoughts on spirituality? My definition

691
01:02:00,200 --> 01:02:10,360
of AGI doesn't include any of that, it's much more basic, it is essentially, as we've documented here,

692
01:02:11,320 --> 01:02:20,040
it's essentially any human task rather than any human anything. So this did include going into

693
01:02:20,040 --> 01:02:26,360
the house and making a cup of coffee, but it doesn't include, you know, having spirituality or having

694
01:02:26,360 --> 01:02:32,840
emotions or even having a sense of smell, but that's what makes this kind of interesting,

695
01:02:33,320 --> 01:02:36,120
there's no agreed upon definition.

696
01:02:39,400 --> 01:02:44,120
Quailude Charlie, how many Quailudes have you had this morning? Is that even an appropriate question

697
01:02:44,120 --> 01:02:49,880
to ask on a live stream? I still like to listen to those guys speaking about pre-gaming, is that

698
01:02:49,880 --> 01:02:55,720
programming in the 50s and 60s, and remembering the hardware and software with early stuff. Yeah,

699
01:02:55,720 --> 01:03:00,280
well the earliest stuff I've got is the late 80s, early 90s, but I was still playing around with

700
01:03:00,360 --> 01:03:12,120
Kobol, Pascal, Algo, and we were forced to write assembly language stuff in the university, the

701
01:03:12,120 --> 01:03:17,560
computer science degree that I was doing, which is pretty horrendous. It was a fascinating time,

702
01:03:17,560 --> 01:03:24,760
and in some ways, right now, reminds me of those early days, and I cannot remember back to the 50s

703
01:03:24,760 --> 01:03:30,600
and 60s because I wasn't there, but I was there in the 80s and 90s, I was there with IRC, I was there

704
01:03:30,600 --> 01:03:37,560
with ICQ, I was there with the early programming languages, and the very early computers, my first

705
01:03:37,560 --> 01:03:47,000
via my older brother was a 486 with like a 300 meg hard drive and 8 meg of RAM. That time was

706
01:03:47,000 --> 01:03:56,440
exciting, the communication, figuring out how to network via coax, via T pieces, via parallel cables

707
01:03:56,440 --> 01:04:04,360
or serial cables, getting doom working across serial cables. In some ways, this is the same sort

708
01:04:04,360 --> 01:04:10,920
of excitement for me, we're finding out how to connect things together, how to create new worlds

709
01:04:10,920 --> 01:04:16,840
essentially. I would probably say that this is more exciting, but there's always something about

710
01:04:17,000 --> 01:04:22,600
hindsight, especially with rose colored glasses, where back then looked kind of cool. If I was

711
01:04:22,600 --> 01:04:30,600
forced to go back there and use a 486 with a 33k modem, if we even had web access, I'd be pretty

712
01:04:30,600 --> 01:04:37,880
upset waiting for DOS to load or Windows to load over five minutes, and then having the IPX

713
01:04:37,880 --> 01:04:44,680
networking breakdown every day just because. If you want a reminder of those times, just try and

714
01:04:44,680 --> 01:04:51,000
get Bluetooth working in 2023 or try and get your printer working in 2023. Same technology,

715
01:04:51,000 --> 01:05:00,920
same issues, we haven't solved it in 40 years. All right, let's go over question from Greg. We did

716
01:05:00,920 --> 01:05:05,160
kind of cover this last time, but let's see if we can cover this again.

717
01:05:05,240 --> 01:05:11,640
Not too happy with that

718
01:05:13,400 --> 01:05:16,680
comment thing failing, it makes all of this a little bit harder.

719
01:05:17,800 --> 01:05:23,320
Greg says, given you don't listen to Jan, who are the main people you do listen to?

720
01:05:23,640 --> 01:05:27,720
Ilya, Kapathi, Hinton.

721
01:05:30,840 --> 01:05:36,520
Greg is great. Greg Brockman from OpenAI. I think I mentioned last time he sat down for two weeks

722
01:05:36,520 --> 01:05:43,480
and got GPT4 working. So even when we have competitors, they're not massaging it and

723
01:05:43,480 --> 01:05:49,240
getting it ready for UX and public consumption, the way that OpenAI have done. And that's not

724
01:05:49,240 --> 01:06:00,840
because of their $100 billion or their 750 very smart people. It's because of one or two people

725
01:06:00,840 --> 01:06:04,680
in there. Ilya is one of them, but Greg is one I listen to and the way that he got this stuff

726
01:06:04,680 --> 01:06:19,000
working is fascinating. We got McGuffin, who is remembering

727
01:06:19,000 --> 01:06:25,400
Token Ring. Yes, some of the games in that time were just amazing. I was looking at Rise of

728
01:06:25,400 --> 01:06:32,040
the Triad the other day, Wolfenstein 3D. I mean the original Wolfe 3D, Descent. Who played Descent

729
01:06:32,040 --> 01:06:39,400
with me? That was amazing. And my favorite was a game called Total Annihilation, which we didn't

730
01:06:39,400 --> 01:06:46,280
really hear a lot of because Diablo and Warcraft kind of competed with it, but that was that were

731
01:06:46,360 --> 01:06:54,600
my favorites. Yeah, I saw the stream cut out there. I'm not sure what that was. I'll blame

732
01:06:55,240 --> 01:07:00,520
YouTube for that one. Altman's now invested into a company that creates neuromorphic analog AI chips.

733
01:07:00,520 --> 01:07:05,560
There's a lot of talk right now about Quantum. I won't be covering Quantum, but if we get James

734
01:07:05,560 --> 01:07:14,840
on here, we will cover Quantum. James from IBM is my go-to colleague for that kind of thing.

735
01:07:14,840 --> 01:07:19,880
He programs Quantum stuff. Day to day, he's the advocate for Quantum computing for IBM.

736
01:07:22,360 --> 01:07:30,680
Awesome. All right, we may look at wrapping up. That's a full hour of just answering questions and

737
01:07:31,560 --> 01:07:38,280
going back in time and reminiscing about what life was like back in the pre-dawn. Actually,

738
01:07:38,280 --> 01:07:44,840
it was the pre-internet, really. If we say that the web hit us from public utilities,

739
01:07:44,840 --> 01:07:51,480
often viacologies and universities in the early 90s, that means just before that was all local

740
01:07:51,480 --> 01:08:02,120
networking or hacking up things inside DOS and Windows 3.1 was my first one. I did spend a lot

741
01:08:02,120 --> 01:08:13,720
of time talking about OS2 Warp 4, which I used a little bit, but I would talk about it with my

742
01:08:14,680 --> 01:08:17,960
consulting colleagues just as I laugh. OS...

743
01:08:17,960 --> 01:08:37,640
Look, it may be my wireless. Who knows? I love the emojis there that are just the scared face.

744
01:08:38,280 --> 01:08:47,000
Oh, I see it now. You can just send an embarrassed face. Cool. There are a couple of approaches

745
01:08:47,080 --> 01:08:53,160
to this already. You could talk about GPT-4 as being multimodal because it's got the vision component,

746
01:08:53,160 --> 01:08:59,240
and then because they've tied Dolly 3 into the chat GPT interface, it's like they're tying

747
01:08:59,240 --> 01:09:06,520
together three or four different models because you could also say that what was previously

748
01:09:06,520 --> 01:09:14,440
called the code interpretation plugin, now called the data analysis plugin, is a third or a fourth

749
01:09:14,440 --> 01:09:21,240
model. So you've got GPT-4 text, you've got GPT-4 vision, you've got code interpretation,

750
01:09:21,240 --> 01:09:28,280
and you've got Dolly 3. Now, some of those you would say are completely separate and discreet,

751
01:09:28,280 --> 01:09:33,720
but the fact that they've combined them into one interface is fascinating. Make me a picture

752
01:09:33,720 --> 01:09:46,520
of a YouTube stream hanging for no reason. So this is all in the same platform, obviously

753
01:09:46,520 --> 01:09:55,400
not using the same model, but who's going to know if a lab joins those all together? That's

754
01:09:55,400 --> 01:10:02,920
why I'm fascinated to see what our final version of Gemini looks like, and the rumors are that it

755
01:10:02,920 --> 01:10:09,160
will have separate vision components to text components. Thanks, Dolly 3, here's the image

756
01:10:09,160 --> 01:10:19,560
of a YouTube stream that has unexpectedly paused. Awesome. All right, let's wrap up with this question

757
01:10:19,560 --> 01:10:23,560
from Drew, latest deep intro inspired me to ask you, what books would you like to see in the

758
01:10:23,560 --> 01:10:29,160
school curriculum that might encourage better evolution, unity, empathy, and critical thinking?

759
01:10:29,480 --> 01:10:35,160
All right, let's give you a big answer here for something that's just a few years ahead. You could

760
01:10:35,160 --> 01:10:42,120
say that it's immediate, but let's step forward a few years. The answer to my question, my answer

761
01:10:42,120 --> 01:10:50,120
to your question is I'd like to see no books on the school curriculum, and I'm not necessarily

762
01:10:50,120 --> 01:10:55,400
being groundbreaking with that view. If you'd like to read more, I've documented this really

763
01:10:55,480 --> 01:11:03,880
heavily, all the way back in 2017, lifearchitect.ai, let's see if I can get this,

764
01:11:03,880 --> 01:11:11,080
lifearchitect.ai slash ad Astra. I'll dump this into the chat because it's a really interesting

765
01:11:11,080 --> 01:11:17,640
read. You can download the article as it appeared in Mensa magazine. Let's actually pop that open.

766
01:11:17,640 --> 01:11:24,600
It basically says, and this was at the time that I was working alongside Elon Musk's school,

767
01:11:25,960 --> 01:11:31,720
in California, when he was teaching or he's having his twins taught, and they were using

768
01:11:31,720 --> 01:11:38,040
a curriculum that was completely created by principal Joshua Dahn, who is an absolute legend.

769
01:11:38,040 --> 01:11:43,400
He's still doing this, but basically we looked at the fact that these guys didn't really use

770
01:11:43,400 --> 01:11:50,760
computers. They didn't use handwriting because handwriting was too slow. They didn't really

771
01:11:50,760 --> 01:11:55,960
have homework. They certainly didn't have books. He didn't teach languages because Elon was getting

772
01:11:55,960 --> 01:12:02,040
them ready for the fact that, well, Neuralink was coming. So why would we teach languages when,

773
01:12:03,560 --> 01:12:09,880
as a reference to earlier in this live stream, we can have real-time translation potentially

774
01:12:09,880 --> 01:12:16,840
including, sorry about that selection, potentially including lip movements as well,

775
01:12:16,840 --> 01:12:23,000
and maybe gestures soon. So if you're translating that to Italian, maybe it gives you hand gestures

776
01:12:23,000 --> 01:12:33,560
alongside it. But this entire school, and it was founded back in 2016, sorry 2014, my work with them

777
01:12:33,560 --> 01:12:38,680
or my work alongside them, my research of what they were doing was 2016. They've been there

778
01:12:38,680 --> 01:12:44,920
since 2014. They're about to hit their 10-year anniversary of not using books, of not giving

779
01:12:45,000 --> 01:12:51,960
homework, of not teaching languages, of not using computers despite being probably the most technically

780
01:12:51,960 --> 01:12:59,400
oriented school in the world. Fascinating. And look how much further we've got in terms of runway

781
01:12:59,400 --> 01:13:05,080
to play with of what we could actually do there. Think about a gentised large language models

782
01:13:05,080 --> 01:13:11,560
that you can go and speak to and it will gamify or just make playful your education experience.

783
01:13:11,560 --> 01:13:17,560
Right, I've just seen that SOV exists for linguistics. How does that work with languages

784
01:13:17,560 --> 01:13:23,000
that I'm interested in? How could that work while I'm at the grocery store with mum and dad?

785
01:13:23,000 --> 01:13:28,040
How can that work at the family dinner table? I've just discovered this bug on my walk,

786
01:13:28,040 --> 01:13:33,240
taken a photo with it. AI's taught me what it is. Let's get the whole etymology in context of that

787
01:13:33,240 --> 01:13:39,320
bug. So making it completely personalised and tailored. This is, in some ways already here,

788
01:13:39,400 --> 01:13:47,080
it's been here since 2014 with Ad Astra, but it is far more accessible now and the capabilities

789
01:13:47,080 --> 01:13:54,040
of large language models make this entire context really interesting. I'm waiting for 2024 so that

790
01:13:54,040 --> 01:14:01,880
we can play around with all the capabilities of a gentised LLMs as systems that will go and help

791
01:14:01,960 --> 01:14:10,120
us learn. That's probably an unexpected answer to your question, but I'm always surprised to

792
01:14:10,120 --> 01:14:16,200
people who are talking about books, including the CEO of OpenAI. Alton recently said that whole

793
01:14:16,200 --> 01:14:20,440
board coup, that whole politics, he said they'll write books about this and I went,

794
01:14:22,120 --> 01:14:26,440
you're the leading voice in artificial intelligence at the moment. You think we're

795
01:14:26,440 --> 01:14:33,320
going to be writing books this year or next year? I don't know. I documented, let's go back to our

796
01:14:34,600 --> 01:14:41,000
screen here, I documented books written by AI all the way back in 2020,

797
01:14:42,040 --> 01:14:48,120
lifearchitect.ai slash books by AI and at that stage, every book you're seeing here,

798
01:14:48,120 --> 01:14:53,560
completely written by a large language model with prompts by a human author. At that stage,

799
01:14:53,560 --> 01:14:59,720
there were very few books. This is one of my favorites. You can read about Leanne Lee's process

800
01:14:59,720 --> 01:15:07,640
for writing books in her series using GPT-3 and now I've said, right, where I'm not even going to

801
01:15:07,640 --> 01:15:14,520
document all the books that are being created by AI because it's ridiculous, but just to my point there,

802
01:15:15,320 --> 01:15:23,400
if AI can generate books instantly and it can, to the extent that Amazon recently banned

803
01:15:24,440 --> 01:15:27,160
or limited the number of AI-generated books,

804
01:15:31,560 --> 01:15:39,880
I think it was two, yeah, three books per day because they were having so many people cranking out

805
01:15:39,960 --> 01:15:45,880
artificial intelligence-generated books. They said, right, maybe you're generating 100 per day

806
01:15:45,880 --> 01:15:51,080
and trying to monetize them, we're going to limit you to three a day. Books are over and that's been

807
01:15:51,080 --> 01:15:58,440
the case for a while. Look out for agents, look out for the next edition of the memo and if you are

808
01:15:58,440 --> 01:16:04,600
a full member of the memo, you will get early access to my end of year report, which is spelling

809
01:16:04,600 --> 01:16:10,600
out some examples of global and personal agents. This is my invitation to you. I'd love to see

810
01:16:10,600 --> 01:16:15,720
you there. You're invited to join the memo with me. Thanks for joining me today and I'll see you

811
01:16:15,720 --> 01:16:21,800
there and I'll see you this time next week for our second last live stream for the year. It's gone

812
01:16:21,800 --> 01:16:34,600
that quick. Thanks so much for joining. Did you see the memo about this? Yeah, yeah, yeah, I have the

813
01:16:34,600 --> 01:16:41,880
memo right here. Superintelligence is unfolding at lightning pace. Read my industry-grade analysis

814
01:16:41,880 --> 01:16:49,880
of AI that matters as it happens in plain English, the memo. Yeah, did you get that memo? Yeah, I

815
01:16:49,880 --> 01:16:57,800
got the memo. Get the inside look as AI models are embodied into humanoids, AI's IQ increases to

816
01:16:57,800 --> 01:17:05,400
nearly perfect and bleeding edge use cases expand to the entire world. Yeah, didn't you get that memo?

817
01:17:06,680 --> 01:17:13,480
Editions are sent to subscribers at Fortune 500's, major governments and people like you,

818
01:17:13,480 --> 01:17:19,080
lifearchitect.ai slash memo. I have the memo.

