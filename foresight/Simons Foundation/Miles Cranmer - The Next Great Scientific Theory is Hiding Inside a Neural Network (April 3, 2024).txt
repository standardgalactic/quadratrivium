So I'm very excited today to talk to you about this idea of interpreting neural networks
to get physical insight, which I view as kind of a new, really kind of a new paradigm of
doing science.
So this is a work with a huge number of people.
I can't individually mention them all, but many of them are here at the Flutter Institute.
So I'm going to split this up.
I'm going to do two parts.
The first one, I'm going to talk about kind of how we go from a neural network to insights,
how we should get insights out of a neural network.
The second part, I'm going to talk about this polymathic AI thing, which is about basically
building massive neural networks for science.
So my motivation for this line of work is examples like the following.
So there was this paper led by Kimberly Stakkenfeld at DeepMind a couple years ago on learning
fast subgrid models for fluid turbulence.
So what you see here is the ground truth.
So this is kind of some box of a fluid.
The bottom row is the learned kind of subgrid model, essentially, for this simulation.
The really interesting thing about this is that this model was only trained on 16 simulations,
but it actually learned to be more accurate than all traditional subgrid models at that
resolution for fluid dynamics.
So I think it's really exciting kind of to figure out how did the model do that and kind
of what can we learn about science from this neural network.
Another example is, so this is a work that I worked on with Dan Twio and others on predicting
instability in planetary systems.
So this is a centuries old problem.
You have some, you know, this compact planetary system and you want to figure out when does
it go unstable.
There are literally, I mean, people have literally worked on this for centuries.
It's a fundamental problem in chaos, but this neural network trained on, I think it was
maybe 20,000 simulations.
It's not only more accurate at predicting instability, but it also seems to generalize
better to kind of different types of systems.
So it's really interesting to think about, okay, these neural networks, they've seemed
to have learned something new.
How can we actually use that to advance our own understanding?
So that's my motivation here.
So the traditional approach to science has been kind of, you have some low dimensional
data set or some kind of summary statistic and you build theories to describe that low
dimensional data, which might be kind of a summary statistic.
So you can look throughout the history of science, so maybe Kepler's law is an empirical
fit to data and then of course Newton's law of gravitation was required to explain this.
Another example is like Plank's law.
So this was actually an empirical fit to data and quantum mechanics was required, partially
motivated by this to explain it.
So this is kind of the normal approach to building theories.
And of course some of these, they've kind of, I mean it's not only this, it also involves
many other things.
But I think it's really exciting to think about how we can involve interpretation of
data driven models in this process, very generally.
So that's what I'm going to talk about today.
I'm going to conjecture that in this era of AI, where we have these massive neural networks
that kind of seem to outperform all of our traditional theory, we might want to consider
this approach where we use a neural network as essentially compression tool or some kind
of tool that pulls apart common patterns in a data set.
And we build theories not to describe the data directly, but really kind of to describe
the neural network and what the neural network has learned.
So I think this is kind of an exciting new approach to, I mean really science in general,
I think especially the physical sciences.
So the key point here is neural networks trained on massive amounts of data with very flexible
functions, they seem to find new things that are not in our existing theory.
So I showed you the example with turbulence, you know we can find better sub-grid models
just from data, and we can also do this with planetary dynamics.
So I think our challenge as scientists for those problems is distilling those insights
into our language, kind of incorporating it in our theory.
I think this is a really exciting way to kind of look at these models.
So I'm going to break this down a bit.
The first thing I would like to do is just go through kind of what machine learning is,
how it works, and then talk about this, kind of how you apply them to different data sets.
Okay so just going back to the very fundamentals, linear regression in 1D, this is, I would
argue if you don't really have physical meaning to these parameters yet, it is a kind of type
of machine learning.
And so this is, these are scalars, right, X and Y, those are scalars, FI0, FI1, scalar
parameters, linear model.
You go one step beyond that, and you get this shallow network.
So again this has 1D input, X, 1D output, Y, but now we've introduced this layer.
So we have these linear models, so we have three hidden neurons here, and they pass
through this function A, so this is called an activation function.
And what this does is it gives the model a way of including some non-linearity.
So these are called activation functions.
The one that most people would reach for first is the rectified linear unit, or RELU.
Essentially what this does is it says if the input is less than zero, drop it at zero,
greater than zero, leave it.
This is a very simple way of adding some kind of non-linearity to my flexible curve that
I'm going to fit to my data, right?
The next thing I do is I have these different activation functions.
They have this kind of joint here at different points, which depends on the parameters.
And I'm going to multiply the outputs of these activations by a number.
That's kind of the output of my kind of a layer of the neural network.
And this is going to maybe change the direction of it, change the slope of it.
The next thing I'm going to do is I'm going to sum these up.
I'm going to superimpose them, and I get this is the output of one layer in my network.
So this is a shallow network.
Basically what it is, it's a piecewise linear model.
And the joints here, the parts where it kind of switches from one linear region to another,
those are determined by the inputs to the first layer's activations.
So it's basically a piecewise linear model.
It's a piecewise linear model.
And the one cool thing about it is you can use this piecewise linear model to approximate
any 1D function to arbitrary accuracy.
So if I want to model this function with five joints, I can get an approximation like this
with 10 joints like this, 20 like that, and I can just keep increasing the number of these
neurons.
And that gives me better and better approximations.
So this is called the universal approximation theorem.
So it's that my shallow neural network just has one kind of layer of activations.
I can describe any continuous function to arbitrary precision.
Now that's not, I mean this alone is not that exciting because I can do that with polynomials.
I don't need, the neural network's not the only thing that does that.
I think the exciting part about neural networks is when you start making them deeper.
So first let's look at what if we had two inputs?
What would it look like?
We had two inputs.
Now these activations, they are activated along planes, not points, they're activated
along planes.
So for, this is my, maybe my input plane, I'm basically chopping it along the zero part
and now I have these 2D planes in space.
And the next thing I'm going to do, I'm going to scale these and then I'm going to superimpose
them.
And this gives me ways of representing kind of arbitrary functions in now a 2D space rather
than just a 1D space.
So it gives me a way of expressing arbitrary continuous functions.
Now the cool part, oops, the cool part here is when I want to do two layers.
So now I have two layers, so I have this, this is my first neural network, this is my
second neural network and my first neural network looks like this.
If I consider it alone, it looks like this.
My second neural network, it looks like this.
If I just like, I cut this neural network out, it looks like this, okay.
When I compose them together, I get this, this, this shared kind of behavior where,
so I'm composing these functions together and essentially what happens is, it's almost
like you fold the functions together so that I experience that function in this linear
region and kind of backwards and then again.
So you can see there's, there's kind of like that function is mirrored here, right?
It goes, goes back and forth.
So you can make this analogy to folding a piece of paper.
So if I consider my first neural network like this on a piece of paper, I could essentially
fold it, draw my second neural network, the function over that, that first one and then
expand it and essentially now I have this, this function.
So the, the cool part about this is that I'm sharing, I'm kind of sharing computation because
I'm sharing neurons in my neural network.
So this is going to come up again, this is kind of a theme, we're, we're doing efficient
computation in neural networks by sharing neurons and it's, it's useful to think about
it in this, this, this way, kind of folding paper, drawing curves over it and expanding
it.
Okay, so let's go back to the physics.
Now neural networks, right, they're efficient universal function approximators.
You can think of them as kind of like a type of data compression.
The same neurons can be used for different calculations in the same network and a common
use case in, in physical sciences, especially what I work on, is emulating physical processes.
So if I have some, my, my simulator is kind of too expensive or I have like real world
data and my simulator is not good at describing it.
I can build a neural network that maybe emulates it.
So like I have a neural network that looks at kind of the initial conditions in this
model and it predicts when it's going to go unstable.
So this is a, this is a good use case for them.
And once I have that, so maybe I have this, I have this trained piecewise linear model
that kind of emulates some physical process.
Now how do I take that and go to interpret it?
How do I actually get insight out of it?
So this is where I'm going to talk about symbolic regression.
So this is one of my favorite things.
So a lot of the interpretability work in industry, especially like computer vision language,
there's not really like, there's not a good modeling language.
Like if I have a, if I have a model that classifies cats and dogs, there's not really like, there's
not a language for describing every possible cat.
There's not like a mathematical framework for that.
But in science, we do have that.
We do have, oops, we do have a very good mathematical framework.
Let me see if this works.
So in science, right, so we have this, you know, in science we have this very good understanding
of the universe and we have this language for it.
We have mathematics, which describes the universe very well.
And I think when we want to interpret these data-driven models, we should use this language
because that will give us results that are interpretable.
If I have some piecewise linear model with different, you know, like millions of parameters,
it's not, it's not really useful for me, right?
I want to, I want to express it in the language that I'm familiar with, which is mathematics.
So you can look at like any cheat sheet and it's a lot of, you know, simple algebra.
This is the language of science.
So symbolic regression is a machine learning task where the objective is to find analytic
expressions that optimize some objective.
So maybe I, maybe I want to fit that data set.
And what I could do is basically try different trees.
So these are like expression trees, right?
So this equation is that tree.
I basically find different expression trees that match that data.
So the point of symbolic regression, I want to find equations that fit the data set.
So the symbolic and the parameters rather than just optimizing parameters in some model.
So the, the, the current way to do this, the, the state of the art way is a genetic algorithm.
So it's, it's kind of, it's not really like a clever algorithm.
It's, it's a, I can say that because I work on it.
It's a, it's, it's pretty close to brute force.
Basically what you do is you treat your equation like a DNA sequence and you basically evolve
it.
So you do like mutations, you swap one operator to another, maybe, maybe you cross-breed them.
So you have like two expressions which are okay.
You literally breed those together.
I mean, not literally, but you conceptually breed those together, get a new expression
until you fit the data set.
So yeah, so this is a genetic algorithm based search for symbolic regression.
Now the, the point of this is to find simple models in our language of mathematics that
describe a given data set.
So, so I've spent a lot of time working on these frameworks.
So pyser, symbolic regression.jl, they, they work like this.
So if I have this expression, I want to model that data set, essentially what I'm going
to do is just search over all possible expressions until I find one that gets me closer to this
ground truth expression.
You see it's kind of testing different, different branches in evolutionary space.
I'm going to play that again until it reaches this ground truth data set.
So this is, this is pretty close to how it works.
You're essentially finding simple expressions that fit some data set accurately.
So, what I'm going to show you how to do is this symbolic regression idea is about
fitting, kind of finding models, symbolic models that I can use to describe a data set.
I want to use that to build surrogate models of my neural network.
So this is, this is kind of a way of translating my model into my language.
You could, you could also think of it as like polynomial or like a Taylor expansion in some
ways.
The way this works is as follows.
If I have some neural network that I've trained on my data set, whatever, I'm going to train
it normally, freeze the parameters.
Then what I do is I record the inputs and outputs.
I kind of treat it like a data generating process.
I, I try to see like, okay, what's the behavior for this input, this input, and so on.
Then I stick those inputs and outputs into pyser, for example, and I, I find some equation
that models that neural network, or maybe it's like a piece of my neural network.
So this is a, this is building a surrogate model for my neural network that is kind of
approximates the same behavior.
Now you wouldn't just do this for like a standalone neural network.
This, this would typically be part of like a larger model, and it would give you a way
of interpreting exactly what it's doing for different inputs.
So what I might have is maybe I have like two, two pieces, like two neural networks
here.
Maybe I think the first neural network is like learning features, or it's learning
some kind of coordinate transform.
The second one is doing something in that space.
It's using those features for calculation.
And so I can, using symbolic regression, which we call symbolic distillation, I can, I can
distill this model into equations.
So that's, that's the basic idea of this.
I replace neural networks, so I replace them with my surrogate model, which is now an equation.
You would typically do this for G as well.
And now I have equations that describe my model.
And this is kind of a interpretable approximation of my original neural network.
Now the reason you wouldn't want to do this for like just directly on the data is because
it's a harder search problem.
If you break it into pieces, like kind of interpreting pieces of a neural network, it's easier.
Because you're only searching for two n expressions rather than n squared.
So it's a, it's a bit easier.
And you're kind of using the neural network as a way of factoring, factorizing the system
into different pieces that you then interpret.
So we've, we've used this in, in different papers.
So this is one led by Pablo Lemos on rediscovering Newton's law of gravity from data.
So this was a, this was a cool paper because we didn't tell it the masses of the bodies
in the solar system.
It had to simultaneously find the masses of every, all of these 30 bodies we gave it.
And it also found the law.
So we kind of train this neural network to do this.
And then we interpret that neural network.
And it gives us Newton's law of gravity.
Now that's a rediscovery.
And of course, like we know that.
So I think the discoveries are also cool.
So these are not my papers.
These are other people's papers.
I thought they were really exciting.
So this is one, a recent one by Ben Davis and Jehow Jin, where they discover this new
black hole mass scaling relationship.
So it's, it relates the, I think it's the spirality or something in a galaxy in the
velocity with the mass of a black hole.
So they, they found this with this technique, which is exciting.
And I saw this other cool one recently.
They found this cloud cover model with this technique using Picer.
So they, it kind of gets you this point where it's a, it's a fairly simple model and it's
also pretty accurate.
But again, the, the point of this is to find a model that you can understand, right?
It's not this black box neural network with, with billions of parameters.
It's a simple model that you can have a handle on.
Okay.
So that's part one.
Now part two, I want to talk about polymathic AI.
So this is kind of like the complete opposite end.
We're going to go from small models in the first part.
Now we're going to do the biggest possible models.
And I'm going to also talk about the meaning of simplicity, what it actually means.
So the past few years, you may have noticed there's been this shift in industry, industrial
machine learning to favor foundation models.
So like chat GPT is an example of this.
A foundation model is a machine learning model that serves as the foundation for other models.
These models are trained by basically taking massive amounts of general diverse data and,
and training this flexible model on that data and then fine tuning them to some specific
task.
So you could think of it as maybe teaching this machine learning model English and French
before teaching it to do translation between the two.
So it often gives you better performance on downstream tax.
I mean you can also see that, I mean chat GPT is, I've heard that it's trained on GitHub
and that kind of teaches it to reason a bit better.
And so the, I mean basically these models are trained on massive amounts of data and
they form this idea called a foundation model.
So the general idea is you, you collect, you know, you collect your massive amounts
of data, you have this very flexible model and then you train it on, you might train
it to do self supervised learning which is kind of like you mask parts of the data and
then the model tries to fill it back in.
That's a, that's a common way you train that.
So like for example, GPT style models, those are basically trained on the entire internet
and they're trained to predict the next word.
That's their only task.
You get an input sequence of words, you predict the next one and you just repeat that for
massive amounts of text.
And then just by doing that they get really good at general language understanding.
And they are fine tuned to be a chatbot essentially.
So they're, they're given a little bit of extra data on this is how you talk to someone
and be friendly and so on and, and that's much better than just training a model just
to do that.
So it's this idea of pre-training models.
So I mean once you have this model, I think like kind of the, the, the cool part about
these models is they're really trained in a way that gives them general priors for data.
So if I have like some, maybe I have like some artwork generation model, it's trained
on different images and it kind of generates different art.
I can fine tune this model on like Studio Ghibli artwork and it doesn't need much training
data because it already knows what a face looks like.
Like it's already seen tons of different faces.
So just by fine tuning it on some small number of examples, it can, it can kind of pick up
this task much quicker.
That's essentially the idea.
Now this is, I mean the same thing is true in language, right?
Like if I, if I train a model on, if I train a model just to do language translation, right?
Like I just teach it that.
It's kind of, I start from scratch and I just train it English to French.
It's going to struggle.
Whereas if I teach it English and French, kind of I teach it about the languages first
and then I specialize it on translation, it's going to do much better.
So this brings us to science.
So in, in science we also have this.
We also have this idea where there are shared concepts, right?
Like different languages have shared, there's a shared concept of grammar in different languages.
In science we also have shared concepts.
You could kind of draw a big circle around many areas of science and causality is a shared
concept.
If you zoom in to say dynamical systems, you could think about like multi-scale dynamics
is, is shared in many different disciplines, chaos is another shared concept.
So maybe if we train a general model, you know, over many, many different data sets,
the same way chat, GPT is trained on many, many different languages and text databases.
Maybe you'll pick up general concepts and then when we finally make it specialize to
our particular problem, maybe it'll do it, it'll find it easier to learn.
So that's essentially the idea.
So you can, you can really actually see this for a particular system.
So one example is the reaction diffusion equation.
This is a type of PDE and the shallow water equations, another type of PDE.
Different fields, different PDEs, but both have waves.
So they, they both have wave-like behavior.
So I mean maybe if we train this massive flexible model on both of these systems, it's going
to kind of learn a general prior for what a wave looks like.
And then if I have like some, you know, some small data set I only have a couple examples
of, maybe it'll immediately identify, oh, that's a wave, I know how to do that.
It's almost like, I mean, I kind of feel like in science today, what we often do is, I mean
we train machine learning models from scratch.
It's almost like we're taking toddlers and we're teaching them to do pattern matching
on like really advanced problems.
Like we, we have a toddler and we're showing them this is a, you know, this is a spiral
galaxy, this is an elliptical galaxy, and it kind of has to just do pattern matching.
Whereas maybe a foundation model that's trained on broad classes of problems, it's, it's kind
of like a general science graduate, maybe.
So it has a prior for how the world works.
It has seen many different phenomena before, and so when it, when you finally give it that
data set to kind of pick up, it's already seen a lot of that phenomena.
That's really the pitch of this.
That's why we think this will work well.
Okay, so we, we created this collaboration last year.
So this started at Flatiron Institute, led by Shirley Ho, to build this thing, a foundation
model for science.
So this, this is across disciplines, so we want to, you know, build these models to incorporate
data across many different disciplines, across institutions, and so we're currently working
on kind of scaling up these models right now.
The final, I think the final goal of this collaboration is that we would release these
open source foundation models so that people could download them and fine tune them to different
tasks.
So it's really kind of like a different paradigm of doing machine learning, right?
Like rather than the current paradigm where we take a model, randomly initialize it, it's
kind of like a, like a toddler, doesn't know how the world works, and we train that.
This paradigm is we have this generalist science model, and you start from that.
It's kind of a better initialization of a model.
That's, that's the, that's the pitch of Polymathic.
Okay, so we have results, so this year we're kind of scaling up, but last year we had a
couple of papers, so this is one led by Mike McCabe called Multiple Physics Pre-Training.
This paper looked at what if we have this general PDE simulator, this, this model that
learns to essentially run fluid dynamic simulations, and we train it on many different PDEs.
Will it do better on new PDEs or will it do worse?
So what we found is that a single, so a single model is not only able to match single models
trained on specific tasks, it can actually outperform them in many cases.
So it does seem like if you take a more flexible model, you train it on more diverse data,
it will do better in a lot of cases.
I mean it's, it's not unexpected because we do see this with language and vision, but
I think it's still really cool to, to see this.
So I'll skip through some of these.
So this is like, this is the ground truth data, and this is the reconstruction.
Essentially what it's doing is it's predicting the next step, right?
It's predicting the next velocity, the next density, and pressure, and so on.
And you're taking that prediction and running it back through the model, and you get this,
this rule out simulation.
So this is a, this is a task people work on in machine learning.
I'm gonna skip through these.
And essentially what we found is that most of the time by using this multiple physics
pre-training.
By training on many different PDEs, you do get better performance.
So the ones at the right side are the multiple physics pre-trained models, those seem to
do better in many cases.
And it's really because, I mean I think because they've seen, you know, so many different
PDEs, it's like they have a better prior for physics.
I'll skip this as well.
So okay, this is a funny thing that we observed is that, so during talks like this, one thing
that we get asked is, how similar do the PDEs need to be?
Like do the PDEs need to be, you know, like navier stokes but a different parameterization?
Or can they be like completely different physical systems?
So what we found is really hilarious is that, okay, so the bottom line here, this is the
error of the model over a different number of training examples.
So this model was trained on a bunch of different PDEs and then it was introduced to this new
PDE problem and it's given that amount of data.
So that does the best.
This model, it already knows some physics, that one does the best.
The one at the top is the worst.
This is the model that's trained from scratch.
It's never seen anything, this is like your toddler, right, like it doesn't know how the
physical world works.
It was just randomly initialized and it has to learn physics.
Okay, the middle models, those are pre-trained on general video data, a lot of which is cat
videos.
So even pre-training this model on cat videos actually helps you do much better than this
very sophisticated transformer architecture that just has never seen any data.
And it's really because, I mean, we think it's because of shared concepts of spatiotemporal
continuity, right, like videos of cats, there's a spatiotemporal continuity, like the cat
does not teleport across the video unless it's a very fast cat.
There's related concepts, right?
So I mean, that's what we think.
But it's really interesting that pre-training on completely unrelated systems still seems
to help.
And so the takeaway from this is that you should always pre-train your model, even if
the physical system is not that related, you still see benefit of it.
Now obviously, if you pre-train on related data, that helps you more, but anything is
basically better than nothing.
You could basically think of this as the default initialization for neural networks is garbage,
right, like just randomly initializing a neural network, that's a bad starting point.
It's a bad prior for physics.
You should always pre-train your model.
That's the takeaway of this.
OK, so I want to finish up here with kind of rhetorical questions.
So I started the talk about interpretability and kind of like how do we extract insights
from our model.
Now we've kind of gone into this regime of these very large, very flexible foundation
models that seem to learn general principles.
So OK, my question for you, you don't have to answer, but just think it over, is do you
think 1 plus 1 is simple?
It's not a trick question.
Do you think 1 plus 1 is simple?
So I think most people would say yes, 1 plus 1 is simple.
And if you break that down into y, it's simple.
You say, OK, so x plus y is simple for like x and y integers.
That's a simple relationship.
OK, y.
y is x plus y is simple.
And you break that down, it's because plus is simple.
Like plus is a simple operator.
OK, y.
y is plus simple.
It's a very abstract concept, OK?
It's we don't necessarily have plus kind of built into our brains.
It's kind of, I mean, it's really, so I'm going to show this.
This might be controversial, but I think that simplicity is based on familiarity.
We are used to plus as a concept.
We are used to adding numbers as a concept.
Therefore, we call it simple.
You can go back another step further.
The reason we're familiar with addition is because it's useful.
Adding numbers is useful for describing the world.
I count things.
That's useful to live in our universe.
It's useful to count things, to measure things.
Addition is useful.
And it's really one of the most useful things.
So that is why we are familiar with it.
And I would argue that's why we think it's simple.
But the simplicity we have often argued is if it's simple,
it's more likely to be useful.
I think that is actually not a statement about simplicity.
It's actually a statement that if something
is useful for problems like A, B, and C,
then it seems it will also be useful for another problem.
The world is compositional.
If I have a model that works for this set of problems,
it's probably also going to work for this one.
So that's the argument I would like to make.
So when we interpret these models,
I think it's important to keep this in mind and really probe
what is simple, what is interpretable.
So I think this is really exciting for polymathic EI
because these models that are trained on many, many systems,
they will find broadly useful algorithms.
They'll have these neurons that share calculations
across many different disciplines.
So you could argue that that is the utility.
And I mean, maybe we'll discover new kind of operators
and be familiar with those, and we'll
start calling those simple.
So it's not necessarily that all of the things
we discover in machine learning will be simple.
It's kind of that, by definition,
the polymathic EI models will be broadly useful.
And if we know they're broadly useful,
we might get familiar with those.
And that might kind of drive the simplicity of them.
So that's my note in simplicity.
And so the takeaways here are that I
think interpreting a neural network trained on some data
sets offers new ways of discovering
scientific insights from that data.
And I think foundation models like polymathic EI,
I think that is a very exciting way
of discovering new broadly applicable scientific models.
So I'm really excited about this direction.
And thank you for listening to me today.
OK.
Thank you.
Thank you.
Thank you.
Thank you.
Thank you.
Thank you.
Thank you.
Thank you.
Thank you.
Thank you.
Thank you for the question, which was great.
So three short questions.
One.
What was the cost of running polymathic AI
in this kind of a way?
And how to cost it, right?
Yeah.
Two.
When the story built out, it really destroyed you.
I got to help you.
Yeah.
Please use your C mic.
Right in front of you.
We'll put it back.
Yeah.
I'll call them.
And three.
You're putting your back.
Thank you guys for that.
I was trying to answer three.
OK, so I'll try to compartmentalize those.
OK, so the first question was the scale of training.
This is really an open research question.
We don't have the scaling law for science yet.
We have scaling laws for language.
We know that if you have this many GPUs,
you have this size data set, this
is going to be your performance.
We don't have that yet for science
because nobody's built this scale of model.
So that's something we're looking at right now,
is what is the trade-off of scale?
And if I want to train this model on many, many GPUs,
is it worth it?
So that's an open research question.
I do think it'll be large.
Probably order hundreds of GPUs trained for maybe
a couple months.
So it's going to be a very large model.
That's kind of assuming the scale of language models.
Now, the model is going to be free, definitely.
We're all very pro-open source.
And I think that's really the point,
is we want to open source this model so people can download it
and use it in science.
I think that's really the most exciting part about this.
And then I guess the third question you had
was about the future and how it changes how we teach.
I mean, I guess are you asking about teaching science
or teaching machine learning?
Teaching science.
I see.
I mean, yeah, I mean, I don't know.
It depends if it works.
I think if it works, it might very well
change how science is taught.
Yeah, I mean, so I don't know the impact of language models
on computational linguistics.
I'm assuming they've had a big impact.
I don't know if that's affected the teaching of it yet.
But if scientific foundation models had a similar impact,
I'm sure it would impact.
I don't know how much.
Probably depends on the success of the models.
I have a question about your foundation models also.
So in different branches of science,
the data sets are pretty different.
In molecular biology or genetics,
the data sets is a sequence of DNA
versus astrophysics where it's images of stars.
So how do you plan to use the same model
for different form of data sets, input data sets?
So you mean how to pose the objective?
Yes.
So I think the most general objective
is self-supervised learning where you basically
mask parts of the data and you predict the missing part.
If you can optimize that problem,
then you can solve tons of different ones.
You can do regression, predict parameters,
or go the other way and predict rollouts of the model.
It's a really general problem to mask data
and then fill it back in.
That kind of is a superset of many different prediction
problems.
And I think that's why language models are so broadly useful,
even though they're trained just on next-word prediction
or like BERT is a masked model.
Thanks.
Can you hear me?
All right, so that was a great talk.
I'm Victor.
So I'm actually a little bit worried
and this is a little bit of a question.
Whenever you have models like this,
you say that you train these on many examples, right?
So imagine you have already embedded
the laws of physics here somehow,
like let's say the law of repetition.
But when you think about discovering new physics,
we always have this question of whether we are actually
reinventing the wheel or like the network is kind of really
giving us something new or is it something giving us
or is it giving us something that it learned
but it's kind of wrong.
So sometimes we have the answer to know which one is which.
But if you don't have that, let's say for instance,
you're trying to discover what dark matter is,
which is something I'm working on.
How would you know that the network is actually giving you
something new and not just trying to set this
into one of the many parameters it has?
So, okay, so if you wanna test the model
by letting it rediscover something,
then I don't think you should use this.
I think you should use the scratch model,
like from scratch and train it.
Because if you use a pre-train model,
it's probably already seen that physics.
So it's biased towards it in some ways.
So if you're rediscovering something,
I don't think you should use this.
If you're discovering something new,
I do think this is more useful.
So I think a misconception of,
I think machine learning in general
is that scientists view machine learning
for uninitialized models,
like randomly initialized weights as a neutral prior.
But it's not, it's a very explicit prior.
And it happens to be a bad prior.
So if you train from a randomly initialized model,
it's kind of always gonna be a worse prior
than training from a pre-train model,
which has seen many different types of physics.
I think we can kind of make that statement.
So if you're trying to discover new physics,
I mean, like if you train it on some data set,
I guess you can always verify
that the predictions are accurate.
So that would be, I guess, one way to verify it.
But I do think like the fine tuning here,
so like taking this model and training it on the task,
I think that's very important.
I think in language models, it's not as emphasized.
Like people will just take a language model
and tweak the prompt to get a better result.
I think for science, I think the prompt is,
I mean, I think like the equivalent of the prompt
would be important, but I think the fine tuning
is much more important,
because our data sets are so much different across science.
Chris, in the back, thank you.
In my course, the main course,
all the students, I don't think there's one.
Please note, Steph, the symbolic.
It's still limited in the complexity
and dimensionality of the system.
So are you, based on your saying,
also the fine tuning and transfer learning
as a way of enhancing the symbolic regression?
And what does that mean?
Yeah, so the symbolic regression,
I mean, I would consider that it's not used
inside the foundation model part.
I think it's interesting to interpret the foundation model
and see if there's kind of more general physical frameworks
that it comes up with.
I think, yeah, symbolic regression is very limited
in that it's bad at high dimensional problems.
I think that might be because of the choice of operators.
I think if you can consider maybe high dimensional operators,
you might be a bit better off.
I mean, symbolic regression,
it's an active area of research,
and I think the hardest, the biggest hurdle right now
is it's not good at finding very complex symbolic models.
So I guess you could, it depends on the dimensionality
of the data.
I guess if it's very high dimensional data,
you're always kind of, symbolic regression is not good
at high dimensional data unless you can have
kind of some operators that aggregate
to lower dimensional spaces.
Yeah, I don't know if I'm answering your question.
OK.
I wanted to ask a little bit.
So when you were showing the construction of these trees
of each generation and the different operators,
I think this is related to kind of general themes
at the top and other questions.
But often in doing science, when you're writing it,
you're presented with kind of an algorithmic self-prompt.
It's like, you know, diagonalize the Hamiltonian
or something like that.
How do you calculate that aspect of doing science
that is kind of the algorithmic side of solving
the problem rather than just going up and down?
Right.
These, do you see my?
Yeah.
Yeah, so the question was about how
do you incorporate kind of more general, not
analytic operators, but kind of more general algorithms
like a Hamiltonian operator.
I think that, I mean, like in principle,
symbolic regression is it's part of a larger family
of an algorithm called program synthesis,
where the objective is to find a program, you know,
like code that describes a given data set, for example.
So if you can write your operators
into your symbolic regression approach
and your symbolic regression approach
has that ground truth model in there somewhere,
then I think it's totally possible.
I think, like, it's harder to do.
I think, like, even symbolic regression with scalars
is, it's fairly difficult to actually set up an algorithm.
I think, I don't know, I think it's really
like an engineering problem.
But the conceptual part is totally like there for this.
Yeah.
Thanks.
Oh, sorry.
This claim that random initial weights are always bad
or pre-training is always good.
I don't know if they're always bad,
but it seems like from our experiments,
we've never seen a case where pre-training
on some kind of physical data hurts.
Like the cap video is an example.
We thought that would hurt the model.
It didn't.
That is a cute example.
I'm sure there's cases where some pre-training hurts.
Yeah, so that's essentially my question.
So we're aware of, like, adversarial examples.
For example, you train on MNIST, add a bit of noise.
It does terrible compared to what a human would do.
What do you think adversarial examples look like in science?
Yeah, I mean, I don't know what those are,
but I'm sure they exist somewhere,
where pre-training on certain data types kind of
messes with training a bit.
We don't know those yet, but yeah, it'll be interesting.
Do you think it's a pick fall, though, of the approach?
Because I have a model of the sun and a model of DNA.
Yeah, I mean, I don't know.
I guess we'll see.
Yeah, it's hard to know.
I guess from language, we've seen you can pre-train
like a language model on video data,
and it helps the language, which is really weird.
But it does seem like if there's any kind of concepts,
it does, if it's flexible enough,
it can kind of transfer those in some ways.
So we'll see.
I mean, presumably, we'll find some adversarial examples there.
So far, we haven't.
We thought the cat was one, but it wasn't.
It helped.
