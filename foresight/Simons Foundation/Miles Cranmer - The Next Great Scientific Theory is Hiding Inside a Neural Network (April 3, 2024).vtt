WEBVTT

00:00.000 --> 00:19.440
So I'm very excited today to talk to you about this idea of interpreting neural networks

00:19.440 --> 00:26.960
to get physical insight, which I view as kind of a new, really kind of a new paradigm of

00:26.960 --> 00:29.440
doing science.

00:29.440 --> 00:32.600
So this is a work with a huge number of people.

00:32.600 --> 00:38.000
I can't individually mention them all, but many of them are here at the Flutter Institute.

00:38.000 --> 00:39.960
So I'm going to split this up.

00:39.960 --> 00:41.200
I'm going to do two parts.

00:41.200 --> 00:46.400
The first one, I'm going to talk about kind of how we go from a neural network to insights,

00:46.400 --> 00:49.160
how we should get insights out of a neural network.

00:49.160 --> 00:55.560
The second part, I'm going to talk about this polymathic AI thing, which is about basically

00:55.560 --> 00:59.800
building massive neural networks for science.

00:59.800 --> 01:09.840
So my motivation for this line of work is examples like the following.

01:09.840 --> 01:17.800
So there was this paper led by Kimberly Stakkenfeld at DeepMind a couple years ago on learning

01:17.800 --> 01:23.080
fast subgrid models for fluid turbulence.

01:23.080 --> 01:25.200
So what you see here is the ground truth.

01:25.200 --> 01:28.640
So this is kind of some box of a fluid.

01:28.640 --> 01:36.440
The bottom row is the learned kind of subgrid model, essentially, for this simulation.

01:36.440 --> 01:45.960
The really interesting thing about this is that this model was only trained on 16 simulations,

01:45.960 --> 01:52.160
but it actually learned to be more accurate than all traditional subgrid models at that

01:52.160 --> 01:55.000
resolution for fluid dynamics.

01:55.000 --> 02:01.560
So I think it's really exciting kind of to figure out how did the model do that and kind

02:01.560 --> 02:08.440
of what can we learn about science from this neural network.

02:08.440 --> 02:14.680
Another example is, so this is a work that I worked on with Dan Twio and others on predicting

02:14.680 --> 02:17.000
instability in planetary systems.

02:17.000 --> 02:20.160
So this is a centuries old problem.

02:20.160 --> 02:25.040
You have some, you know, this compact planetary system and you want to figure out when does

02:25.040 --> 02:27.680
it go unstable.

02:27.680 --> 02:33.240
There are literally, I mean, people have literally worked on this for centuries.

02:33.240 --> 02:38.720
It's a fundamental problem in chaos, but this neural network trained on, I think it was

02:38.720 --> 02:43.080
maybe 20,000 simulations.

02:43.080 --> 02:47.480
It's not only more accurate at predicting instability, but it also seems to generalize

02:47.480 --> 02:51.080
better to kind of different types of systems.

02:51.080 --> 02:57.200
So it's really interesting to think about, okay, these neural networks, they've seemed

02:57.200 --> 02:59.600
to have learned something new.

02:59.600 --> 03:03.480
How can we actually use that to advance our own understanding?

03:03.480 --> 03:06.600
So that's my motivation here.

03:06.600 --> 03:11.300
So the traditional approach to science has been kind of, you have some low dimensional

03:11.300 --> 03:18.260
data set or some kind of summary statistic and you build theories to describe that low

03:18.260 --> 03:23.760
dimensional data, which might be kind of a summary statistic.

03:23.760 --> 03:28.500
So you can look throughout the history of science, so maybe Kepler's law is an empirical

03:28.500 --> 03:35.340
fit to data and then of course Newton's law of gravitation was required to explain this.

03:35.340 --> 03:36.740
Another example is like Plank's law.

03:36.740 --> 03:44.340
So this was actually an empirical fit to data and quantum mechanics was required, partially

03:44.340 --> 03:47.340
motivated by this to explain it.

03:47.340 --> 03:57.420
So this is kind of the normal approach to building theories.

03:57.420 --> 04:02.980
And of course some of these, they've kind of, I mean it's not only this, it also involves

04:02.980 --> 04:03.980
many other things.

04:03.980 --> 04:12.060
But I think it's really exciting to think about how we can involve interpretation of

04:12.060 --> 04:16.020
data driven models in this process, very generally.

04:16.020 --> 04:19.140
So that's what I'm going to talk about today.

04:19.140 --> 04:26.460
I'm going to conjecture that in this era of AI, where we have these massive neural networks

04:26.460 --> 04:33.060
that kind of seem to outperform all of our traditional theory, we might want to consider

04:33.060 --> 04:40.620
this approach where we use a neural network as essentially compression tool or some kind

04:40.620 --> 04:49.620
of tool that pulls apart common patterns in a data set.

04:49.620 --> 04:54.100
And we build theories not to describe the data directly, but really kind of to describe

04:54.100 --> 04:57.620
the neural network and what the neural network has learned.

04:57.620 --> 05:03.340
So I think this is kind of an exciting new approach to, I mean really science in general,

05:03.340 --> 05:06.420
I think especially the physical sciences.

05:06.420 --> 05:13.300
So the key point here is neural networks trained on massive amounts of data with very flexible

05:13.300 --> 05:19.780
functions, they seem to find new things that are not in our existing theory.

05:19.780 --> 05:23.780
So I showed you the example with turbulence, you know we can find better sub-grid models

05:23.780 --> 05:29.500
just from data, and we can also do this with planetary dynamics.

05:29.500 --> 05:36.300
So I think our challenge as scientists for those problems is distilling those insights

05:36.300 --> 05:39.580
into our language, kind of incorporating it in our theory.

05:39.580 --> 05:46.220
I think this is a really exciting way to kind of look at these models.

05:46.220 --> 05:49.300
So I'm going to break this down a bit.

05:49.300 --> 05:54.700
The first thing I would like to do is just go through kind of what machine learning is,

05:54.700 --> 06:01.700
how it works, and then talk about this, kind of how you apply them to different data sets.

06:01.700 --> 06:12.260
Okay so just going back to the very fundamentals, linear regression in 1D, this is, I would

06:12.260 --> 06:19.700
argue if you don't really have physical meaning to these parameters yet, it is a kind of type

06:19.700 --> 06:22.260
of machine learning.

06:22.260 --> 06:28.020
And so this is, these are scalars, right, X and Y, those are scalars, FI0, FI1, scalar

06:28.020 --> 06:31.860
parameters, linear model.

06:31.860 --> 06:36.380
You go one step beyond that, and you get this shallow network.

06:36.380 --> 06:46.940
So again this has 1D input, X, 1D output, Y, but now we've introduced this layer.

06:46.940 --> 06:55.260
So we have these linear models, so we have three hidden neurons here, and they pass

06:55.260 --> 07:00.100
through this function A, so this is called an activation function.

07:00.100 --> 07:07.900
And what this does is it gives the model a way of including some non-linearity.

07:07.900 --> 07:11.860
So these are called activation functions.

07:11.860 --> 07:19.300
The one that most people would reach for first is the rectified linear unit, or RELU.

07:19.300 --> 07:25.220
Essentially what this does is it says if the input is less than zero, drop it at zero,

07:25.220 --> 07:27.980
greater than zero, leave it.

07:27.980 --> 07:34.780
This is a very simple way of adding some kind of non-linearity to my flexible curve that

07:34.780 --> 07:41.060
I'm going to fit to my data, right?

07:41.060 --> 07:47.380
The next thing I do is I have these different activation functions.

07:47.380 --> 07:55.100
They have this kind of joint here at different points, which depends on the parameters.

07:55.100 --> 08:01.540
And I'm going to multiply the outputs of these activations by a number.

08:01.540 --> 08:08.380
That's kind of the output of my kind of a layer of the neural network.

08:08.380 --> 08:13.740
And this is going to maybe change the direction of it, change the slope of it.

08:13.740 --> 08:15.740
The next thing I'm going to do is I'm going to sum these up.

08:15.740 --> 08:22.120
I'm going to superimpose them, and I get this is the output of one layer in my network.

08:22.120 --> 08:24.500
So this is a shallow network.

08:24.500 --> 08:29.260
Basically what it is, it's a piecewise linear model.

08:29.260 --> 08:35.660
And the joints here, the parts where it kind of switches from one linear region to another,

08:35.660 --> 08:40.500
those are determined by the inputs to the first layer's activations.

08:40.500 --> 08:43.940
So it's basically a piecewise linear model.

08:43.940 --> 08:47.780
It's a piecewise linear model.

08:47.780 --> 08:56.420
And the one cool thing about it is you can use this piecewise linear model to approximate

08:56.420 --> 08:59.660
any 1D function to arbitrary accuracy.

08:59.660 --> 09:05.300
So if I want to model this function with five joints, I can get an approximation like this

09:05.300 --> 09:11.340
with 10 joints like this, 20 like that, and I can just keep increasing the number of these

09:11.340 --> 09:12.340
neurons.

09:12.340 --> 09:16.540
And that gives me better and better approximations.

09:16.540 --> 09:20.500
So this is called the universal approximation theorem.

09:20.500 --> 09:29.020
So it's that my shallow neural network just has one kind of layer of activations.

09:29.020 --> 09:33.780
I can describe any continuous function to arbitrary precision.

09:33.780 --> 09:41.220
Now that's not, I mean this alone is not that exciting because I can do that with polynomials.

09:41.220 --> 09:44.820
I don't need, the neural network's not the only thing that does that.

09:44.820 --> 09:50.020
I think the exciting part about neural networks is when you start making them deeper.

09:50.020 --> 09:54.020
So first let's look at what if we had two inputs?

09:54.020 --> 09:55.020
What would it look like?

09:55.020 --> 09:56.820
We had two inputs.

09:56.820 --> 10:04.580
Now these activations, they are activated along planes, not points, they're activated

10:04.580 --> 10:06.100
along planes.

10:06.100 --> 10:15.420
So for, this is my, maybe my input plane, I'm basically chopping it along the zero part

10:15.420 --> 10:19.700
and now I have these 2D planes in space.

10:19.700 --> 10:24.740
And the next thing I'm going to do, I'm going to scale these and then I'm going to superimpose

10:24.740 --> 10:26.300
them.

10:26.300 --> 10:34.620
And this gives me ways of representing kind of arbitrary functions in now a 2D space rather

10:34.620 --> 10:35.740
than just a 1D space.

10:35.740 --> 10:44.900
So it gives me a way of expressing arbitrary continuous functions.

10:44.900 --> 10:54.820
Now the cool part, oops, the cool part here is when I want to do two layers.

10:54.820 --> 11:00.700
So now I have two layers, so I have this, this is my first neural network, this is my

11:00.700 --> 11:05.620
second neural network and my first neural network looks like this.

11:05.620 --> 11:08.660
If I consider it alone, it looks like this.

11:08.660 --> 11:12.820
My second neural network, it looks like this.

11:12.820 --> 11:17.500
If I just like, I cut this neural network out, it looks like this, okay.

11:17.500 --> 11:26.820
When I compose them together, I get this, this, this shared kind of behavior where,

11:26.820 --> 11:33.580
so I'm composing these functions together and essentially what happens is, it's almost

11:33.580 --> 11:41.100
like you fold the functions together so that I experience that function in this linear

11:41.100 --> 11:44.140
region and kind of backwards and then again.

11:44.140 --> 11:48.620
So you can see there's, there's kind of like that function is mirrored here, right?

11:48.620 --> 11:51.780
It goes, goes back and forth.

11:51.780 --> 11:56.660
So you can make this analogy to folding a piece of paper.

11:56.660 --> 12:02.300
So if I consider my first neural network like this on a piece of paper, I could essentially

12:02.300 --> 12:09.420
fold it, draw my second neural network, the function over that, that first one and then

12:09.420 --> 12:15.100
expand it and essentially now I have this, this function.

12:15.100 --> 12:23.340
So the, the cool part about this is that I'm sharing, I'm kind of sharing computation because

12:23.340 --> 12:27.820
I'm sharing neurons in my neural network.

12:27.820 --> 12:32.940
So this is going to come up again, this is kind of a theme, we're, we're doing efficient

12:32.940 --> 12:38.460
computation in neural networks by sharing neurons and it's, it's useful to think about

12:38.460 --> 12:44.700
it in this, this, this way, kind of folding paper, drawing curves over it and expanding

12:44.700 --> 12:45.700
it.

12:45.700 --> 12:50.500
Okay, so let's go back to the physics.

12:50.500 --> 12:57.540
Now neural networks, right, they're efficient universal function approximators.

12:57.540 --> 13:02.180
You can think of them as kind of like a type of data compression.

13:02.180 --> 13:10.180
The same neurons can be used for different calculations in the same network and a common

13:10.180 --> 13:18.500
use case in, in physical sciences, especially what I work on, is emulating physical processes.

13:18.500 --> 13:22.980
So if I have some, my, my simulator is kind of too expensive or I have like real world

13:22.980 --> 13:26.660
data and my simulator is not good at describing it.

13:26.660 --> 13:31.740
I can build a neural network that maybe emulates it.

13:31.740 --> 13:36.260
So like I have a neural network that looks at kind of the initial conditions in this

13:36.260 --> 13:40.500
model and it predicts when it's going to go unstable.

13:40.500 --> 13:44.500
So this is a, this is a good use case for them.

13:44.500 --> 13:52.020
And once I have that, so maybe I have this, I have this trained piecewise linear model

13:52.020 --> 13:55.500
that kind of emulates some physical process.

13:55.500 --> 14:00.740
Now how do I take that and go to interpret it?

14:00.740 --> 14:04.300
How do I actually get insight out of it?

14:04.300 --> 14:08.580
So this is where I'm going to talk about symbolic regression.

14:08.580 --> 14:11.460
So this is one of my favorite things.

14:11.460 --> 14:18.500
So a lot of the interpretability work in industry, especially like computer vision language,

14:18.500 --> 14:21.940
there's not really like, there's not a good modeling language.

14:21.940 --> 14:27.180
Like if I have a, if I have a model that classifies cats and dogs, there's not really like, there's

14:27.180 --> 14:31.660
not a language for describing every possible cat.

14:31.660 --> 14:33.940
There's not like a mathematical framework for that.

14:33.940 --> 14:35.060
But in science, we do have that.

14:35.060 --> 14:46.060
We do have, oops, we do have a very good mathematical framework.

14:46.060 --> 14:53.220
Let me see if this works.

14:53.220 --> 14:59.180
So in science, right, so we have this, you know, in science we have this very good understanding

14:59.180 --> 15:05.540
of the universe and we have this language for it.

15:05.540 --> 15:10.660
We have mathematics, which describes the universe very well.

15:10.660 --> 15:17.100
And I think when we want to interpret these data-driven models, we should use this language

15:17.100 --> 15:20.940
because that will give us results that are interpretable.

15:20.940 --> 15:25.940
If I have some piecewise linear model with different, you know, like millions of parameters,

15:25.940 --> 15:28.020
it's not, it's not really useful for me, right?

15:28.020 --> 15:33.820
I want to, I want to express it in the language that I'm familiar with, which is mathematics.

15:34.820 --> 15:40.900
So you can look at like any cheat sheet and it's a lot of, you know, simple algebra.

15:40.900 --> 15:43.860
This is the language of science.

15:43.860 --> 15:51.420
So symbolic regression is a machine learning task where the objective is to find analytic

15:51.420 --> 15:55.980
expressions that optimize some objective.

15:55.980 --> 16:00.580
So maybe I, maybe I want to fit that data set.

16:00.580 --> 16:06.140
And what I could do is basically try different trees.

16:06.140 --> 16:09.300
So these are like expression trees, right?

16:09.300 --> 16:11.300
So this equation is that tree.

16:11.300 --> 16:17.620
I basically find different expression trees that match that data.

16:17.620 --> 16:23.740
So the point of symbolic regression, I want to find equations that fit the data set.

16:23.740 --> 16:31.580
So the symbolic and the parameters rather than just optimizing parameters in some model.

16:31.580 --> 16:37.540
So the, the, the current way to do this, the, the state of the art way is a genetic algorithm.

16:37.540 --> 16:43.300
So it's, it's kind of, it's not really like a clever algorithm.

16:43.300 --> 16:46.860
It's, it's a, I can say that because I work on it.

16:46.860 --> 16:50.580
It's a, it's, it's pretty close to brute force.

16:50.580 --> 16:58.180
Basically what you do is you treat your equation like a DNA sequence and you basically evolve

16:58.180 --> 16:59.180
it.

16:59.180 --> 17:05.900
So you do like mutations, you swap one operator to another, maybe, maybe you cross-breed them.

17:05.900 --> 17:08.540
So you have like two expressions which are okay.

17:08.540 --> 17:10.540
You literally breed those together.

17:10.540 --> 17:16.060
I mean, not literally, but you conceptually breed those together, get a new expression

17:16.060 --> 17:21.300
until you fit the data set.

17:21.300 --> 17:28.420
So yeah, so this is a genetic algorithm based search for symbolic regression.

17:28.420 --> 17:38.060
Now the, the point of this is to find simple models in our language of mathematics that

17:38.060 --> 17:42.140
describe a given data set.

17:42.140 --> 17:46.220
So, so I've spent a lot of time working on these frameworks.

17:46.220 --> 17:53.180
So pyser, symbolic regression.jl, they, they work like this.

17:53.180 --> 17:57.500
So if I have this expression, I want to model that data set, essentially what I'm going

17:57.500 --> 18:06.340
to do is just search over all possible expressions until I find one that gets me closer to this

18:06.340 --> 18:07.780
ground truth expression.

18:08.460 --> 18:12.780
You see it's kind of testing different, different branches in evolutionary space.

18:12.780 --> 18:20.780
I'm going to play that again until it reaches this ground truth data set.

18:20.780 --> 18:24.820
So this is, this is pretty close to how it works.

18:24.820 --> 18:36.380
You're essentially finding simple expressions that fit some data set accurately.

18:36.380 --> 18:45.700
So, what I'm going to show you how to do is this symbolic regression idea is about

18:45.700 --> 18:53.420
fitting, kind of finding models, symbolic models that I can use to describe a data set.

18:53.420 --> 19:00.700
I want to use that to build surrogate models of my neural network.

19:00.700 --> 19:07.020
So this is, this is kind of a way of translating my model into my language.

19:07.020 --> 19:12.940
You could, you could also think of it as like polynomial or like a Taylor expansion in some

19:12.940 --> 19:15.140
ways.

19:15.140 --> 19:17.860
The way this works is as follows.

19:17.860 --> 19:23.940
If I have some neural network that I've trained on my data set, whatever, I'm going to train

19:23.940 --> 19:27.220
it normally, freeze the parameters.

19:27.220 --> 19:31.460
Then what I do is I record the inputs and outputs.

19:31.460 --> 19:33.660
I kind of treat it like a data generating process.

19:33.660 --> 19:39.300
I, I try to see like, okay, what's the behavior for this input, this input, and so on.

19:39.300 --> 19:45.940
Then I stick those inputs and outputs into pyser, for example, and I, I find some equation

19:45.940 --> 19:52.380
that models that neural network, or maybe it's like a piece of my neural network.

19:52.380 --> 19:58.340
So this is a, this is building a surrogate model for my neural network that is kind of

19:58.340 --> 20:00.340
approximates the same behavior.

20:00.340 --> 20:04.820
Now you wouldn't just do this for like a standalone neural network.

20:04.820 --> 20:10.540
This, this would typically be part of like a larger model, and it would give you a way

20:10.540 --> 20:16.420
of interpreting exactly what it's doing for different inputs.

20:16.420 --> 20:22.820
So what I might have is maybe I have like two, two pieces, like two neural networks

20:22.820 --> 20:24.420
here.

20:24.420 --> 20:28.780
Maybe I think the first neural network is like learning features, or it's learning

20:28.780 --> 20:31.140
some kind of coordinate transform.

20:31.140 --> 20:34.620
The second one is doing something in that space.

20:34.620 --> 20:38.500
It's using those features for calculation.

20:38.500 --> 20:44.780
And so I can, using symbolic regression, which we call symbolic distillation, I can, I can

20:44.780 --> 20:48.740
distill this model into equations.

20:48.740 --> 20:52.140
So that's, that's the basic idea of this.

20:52.140 --> 20:59.540
I replace neural networks, so I replace them with my surrogate model, which is now an equation.

20:59.540 --> 21:03.300
You would typically do this for G as well.

21:03.300 --> 21:08.620
And now I have equations that describe my model.

21:08.620 --> 21:14.100
And this is kind of a interpretable approximation of my original neural network.

21:14.100 --> 21:18.300
Now the reason you wouldn't want to do this for like just directly on the data is because

21:18.300 --> 21:20.860
it's a harder search problem.

21:20.860 --> 21:27.300
If you break it into pieces, like kind of interpreting pieces of a neural network, it's easier.

21:27.300 --> 21:32.900
Because you're only searching for two n expressions rather than n squared.

21:32.900 --> 21:34.620
So it's a, it's a bit easier.

21:34.620 --> 21:40.900
And you're kind of using the neural network as a way of factoring, factorizing the system

21:40.900 --> 21:46.560
into different pieces that you then interpret.

21:46.560 --> 21:48.860
So we've, we've used this in, in different papers.

21:48.860 --> 21:59.300
So this is one led by Pablo Lemos on rediscovering Newton's law of gravity from data.

21:59.300 --> 22:05.380
So this was a, this was a cool paper because we didn't tell it the masses of the bodies

22:05.380 --> 22:06.380
in the solar system.

22:06.380 --> 22:13.940
It had to simultaneously find the masses of every, all of these 30 bodies we gave it.

22:13.940 --> 22:15.460
And it also found the law.

22:15.460 --> 22:18.460
So we kind of train this neural network to do this.

22:18.460 --> 22:20.660
And then we interpret that neural network.

22:20.660 --> 22:24.860
And it gives us Newton's law of gravity.

22:24.860 --> 22:26.360
Now that's a rediscovery.

22:26.360 --> 22:28.260
And of course, like we know that.

22:28.260 --> 22:31.420
So I think the discoveries are also cool.

22:31.420 --> 22:32.820
So these are not my papers.

22:32.820 --> 22:33.820
These are other people's papers.

22:33.820 --> 22:35.740
I thought they were really exciting.

22:35.740 --> 22:43.820
So this is one, a recent one by Ben Davis and Jehow Jin, where they discover this new

22:43.820 --> 22:47.700
black hole mass scaling relationship.

22:47.700 --> 22:55.180
So it's, it relates the, I think it's the spirality or something in a galaxy in the

22:55.180 --> 22:57.660
velocity with the mass of a black hole.

22:57.660 --> 23:02.340
So they, they found this with this technique, which is exciting.

23:02.340 --> 23:05.620
And I saw this other cool one recently.

23:05.620 --> 23:12.140
They found this cloud cover model with this technique using Picer.

23:12.140 --> 23:16.820
So they, it kind of gets you this point where it's a, it's a fairly simple model and it's

23:16.820 --> 23:20.380
also pretty accurate.

23:20.380 --> 23:25.380
But again, the, the point of this is to find a model that you can understand, right?

23:25.380 --> 23:30.260
It's not this black box neural network with, with billions of parameters.

23:30.260 --> 23:34.980
It's a simple model that you can have a handle on.

23:34.980 --> 23:36.380
Okay.

23:36.380 --> 23:38.580
So that's part one.

23:38.580 --> 23:43.980
Now part two, I want to talk about polymathic AI.

23:43.980 --> 23:47.020
So this is kind of like the complete opposite end.

23:47.020 --> 23:50.180
We're going to go from small models in the first part.

23:50.180 --> 23:52.820
Now we're going to do the biggest possible models.

23:52.820 --> 23:58.140
And I'm going to also talk about the meaning of simplicity, what it actually means.

23:58.140 --> 24:06.300
So the past few years, you may have noticed there's been this shift in industry, industrial

24:06.300 --> 24:10.460
machine learning to favor foundation models.

24:10.460 --> 24:14.500
So like chat GPT is an example of this.

24:14.500 --> 24:23.660
A foundation model is a machine learning model that serves as the foundation for other models.

24:23.660 --> 24:30.060
These models are trained by basically taking massive amounts of general diverse data and,

24:30.060 --> 24:37.020
and training this flexible model on that data and then fine tuning them to some specific

24:37.020 --> 24:38.220
task.

24:38.220 --> 24:46.100
So you could think of it as maybe teaching this machine learning model English and French

24:46.100 --> 24:50.820
before teaching it to do translation between the two.

24:50.820 --> 24:55.980
So it often gives you better performance on downstream tax.

24:55.980 --> 25:05.980
I mean you can also see that, I mean chat GPT is, I've heard that it's trained on GitHub

25:05.980 --> 25:11.460
and that kind of teaches it to reason a bit better.

25:11.460 --> 25:17.020
And so the, I mean basically these models are trained on massive amounts of data and

25:17.020 --> 25:20.700
they form this idea called a foundation model.

25:20.700 --> 25:25.980
So the general idea is you, you collect, you know, you collect your massive amounts

25:25.980 --> 25:33.180
of data, you have this very flexible model and then you train it on, you might train

25:33.180 --> 25:40.460
it to do self supervised learning which is kind of like you mask parts of the data and

25:40.460 --> 25:43.220
then the model tries to fill it back in.

25:43.220 --> 25:44.900
That's a, that's a common way you train that.

25:44.900 --> 25:51.420
So like for example, GPT style models, those are basically trained on the entire internet

25:51.420 --> 25:54.700
and they're trained to predict the next word.

25:54.700 --> 25:56.140
That's their only task.

25:56.140 --> 26:01.740
You get an input sequence of words, you predict the next one and you just repeat that for

26:01.740 --> 26:04.180
massive amounts of text.

26:04.180 --> 26:12.380
And then just by doing that they get really good at general language understanding.

26:12.380 --> 26:16.180
And they are fine tuned to be a chatbot essentially.

26:16.180 --> 26:21.500
So they're, they're given a little bit of extra data on this is how you talk to someone

26:21.500 --> 26:27.580
and be friendly and so on and, and that's much better than just training a model just

26:27.580 --> 26:28.580
to do that.

26:28.580 --> 26:33.420
So it's this idea of pre-training models.

26:33.420 --> 26:40.100
So I mean once you have this model, I think like kind of the, the, the cool part about

26:40.100 --> 26:49.420
these models is they're really trained in a way that gives them general priors for data.

26:49.420 --> 26:55.140
So if I have like some, maybe I have like some artwork generation model, it's trained

26:55.140 --> 26:59.020
on different images and it kind of generates different art.

26:59.020 --> 27:06.140
I can fine tune this model on like Studio Ghibli artwork and it doesn't need much training

27:06.140 --> 27:09.860
data because it already knows what a face looks like.

27:09.860 --> 27:13.260
Like it's already seen tons of different faces.

27:13.260 --> 27:18.660
So just by fine tuning it on some small number of examples, it can, it can kind of pick up

27:18.660 --> 27:22.020
this task much quicker.

27:22.020 --> 27:24.140
That's essentially the idea.

27:24.140 --> 27:29.300
Now this is, I mean the same thing is true in language, right?

27:29.300 --> 27:36.060
Like if I, if I train a model on, if I train a model just to do language translation, right?

27:36.060 --> 27:37.660
Like I just teach it that.

27:37.660 --> 27:43.220
It's kind of, I start from scratch and I just train it English to French.

27:43.220 --> 27:44.860
It's going to struggle.

27:44.860 --> 27:51.100
Whereas if I teach it English and French, kind of I teach it about the languages first

27:51.100 --> 27:57.660
and then I specialize it on translation, it's going to do much better.

27:57.660 --> 28:00.660
So this brings us to science.

28:00.660 --> 28:06.260
So in, in science we also have this.

28:06.260 --> 28:11.180
We also have this idea where there are shared concepts, right?

28:11.180 --> 28:17.180
Like different languages have shared, there's a shared concept of grammar in different languages.

28:17.180 --> 28:19.820
In science we also have shared concepts.

28:19.820 --> 28:26.220
You could kind of draw a big circle around many areas of science and causality is a shared

28:26.220 --> 28:27.820
concept.

28:27.820 --> 28:34.940
If you zoom in to say dynamical systems, you could think about like multi-scale dynamics

28:34.940 --> 28:41.460
is, is shared in many different disciplines, chaos is another shared concept.

28:41.460 --> 28:51.420
So maybe if we train a general model, you know, over many, many different data sets,

28:51.420 --> 28:56.580
the same way chat, GPT is trained on many, many different languages and text databases.

28:57.540 --> 29:03.180
Maybe you'll pick up general concepts and then when we finally make it specialize to

29:03.180 --> 29:10.740
our particular problem, maybe it'll do it, it'll find it easier to learn.

29:10.740 --> 29:13.660
So that's essentially the idea.

29:13.660 --> 29:18.020
So you can, you can really actually see this for a particular system.

29:18.020 --> 29:21.660
So one example is the reaction diffusion equation.

29:21.660 --> 29:28.340
This is a type of PDE and the shallow water equations, another type of PDE.

29:28.340 --> 29:32.980
Different fields, different PDEs, but both have waves.

29:32.980 --> 29:37.460
So they, they both have wave-like behavior.

29:37.460 --> 29:44.900
So I mean maybe if we train this massive flexible model on both of these systems, it's going

29:44.900 --> 29:51.780
to kind of learn a general prior for what a wave looks like.

29:51.780 --> 29:55.900
And then if I have like some, you know, some small data set I only have a couple examples

29:55.900 --> 30:02.220
of, maybe it'll immediately identify, oh, that's a wave, I know how to do that.

30:02.220 --> 30:11.340
It's almost like, I mean, I kind of feel like in science today, what we often do is, I mean

30:11.340 --> 30:14.500
we train machine learning models from scratch.

30:14.500 --> 30:20.540
It's almost like we're taking toddlers and we're teaching them to do pattern matching

30:20.540 --> 30:23.420
on like really advanced problems.

30:23.420 --> 30:27.420
Like we, we have a toddler and we're showing them this is a, you know, this is a spiral

30:27.420 --> 30:33.100
galaxy, this is an elliptical galaxy, and it kind of has to just do pattern matching.

30:33.100 --> 30:38.860
Whereas maybe a foundation model that's trained on broad classes of problems, it's, it's kind

30:38.860 --> 30:43.620
of like a general science graduate, maybe.

30:43.620 --> 30:47.480
So it has a prior for how the world works.

30:47.480 --> 30:52.460
It has seen many different phenomena before, and so when it, when you finally give it that

30:52.460 --> 30:57.460
data set to kind of pick up, it's already seen a lot of that phenomena.

30:57.460 --> 30:59.860
That's really the pitch of this.

30:59.860 --> 31:02.500
That's why we think this will work well.

31:02.500 --> 31:08.380
Okay, so we, we created this collaboration last year.

31:08.380 --> 31:16.660
So this started at Flatiron Institute, led by Shirley Ho, to build this thing, a foundation

31:16.660 --> 31:19.780
model for science.

31:19.780 --> 31:27.980
So this, this is across disciplines, so we want to, you know, build these models to incorporate

31:27.980 --> 31:35.900
data across many different disciplines, across institutions, and so we're currently working

31:35.900 --> 31:38.740
on kind of scaling up these models right now.

31:38.740 --> 31:45.540
The final, I think the final goal of this collaboration is that we would release these

31:45.540 --> 31:52.100
open source foundation models so that people could download them and fine tune them to different

31:52.100 --> 31:53.100
tasks.

31:53.100 --> 31:57.860
So it's really kind of like a different paradigm of doing machine learning, right?

31:57.860 --> 32:03.660
Like rather than the current paradigm where we take a model, randomly initialize it, it's

32:03.660 --> 32:09.820
kind of like a, like a toddler, doesn't know how the world works, and we train that.

32:09.820 --> 32:16.780
This paradigm is we have this generalist science model, and you start from that.

32:16.780 --> 32:21.500
It's kind of a better initialization of a model.

32:21.500 --> 32:25.020
That's, that's the, that's the pitch of Polymathic.

32:25.020 --> 32:31.340
Okay, so we have results, so this year we're kind of scaling up, but last year we had a

32:31.340 --> 32:40.060
couple of papers, so this is one led by Mike McCabe called Multiple Physics Pre-Training.

32:40.060 --> 32:47.980
This paper looked at what if we have this general PDE simulator, this, this model that

32:47.980 --> 32:54.940
learns to essentially run fluid dynamic simulations, and we train it on many different PDEs.

32:54.940 --> 32:59.380
Will it do better on new PDEs or will it do worse?

32:59.380 --> 33:12.980
So what we found is that a single, so a single model is not only able to match single models

33:12.980 --> 33:18.140
trained on specific tasks, it can actually outperform them in many cases.

33:18.140 --> 33:26.860
So it does seem like if you take a more flexible model, you train it on more diverse data,

33:26.860 --> 33:29.220
it will do better in a lot of cases.

33:29.220 --> 33:36.140
I mean it's, it's not unexpected because we do see this with language and vision, but

33:36.140 --> 33:41.380
I think it's still really cool to, to see this.

33:41.380 --> 33:45.340
So I'll skip through some of these.

33:45.340 --> 33:52.060
So this is like, this is the ground truth data, and this is the reconstruction.

33:52.060 --> 33:55.980
Essentially what it's doing is it's predicting the next step, right?

33:55.980 --> 33:59.900
It's predicting the next velocity, the next density, and pressure, and so on.

33:59.900 --> 34:04.540
And you're taking that prediction and running it back through the model, and you get this,

34:04.540 --> 34:07.820
this rule out simulation.

34:07.820 --> 34:12.740
So this is a, this is a task people work on in machine learning.

34:12.740 --> 34:15.940
I'm gonna skip through these.

34:15.940 --> 34:23.820
And essentially what we found is that most of the time by using this multiple physics

34:23.820 --> 34:24.820
pre-training.

34:24.820 --> 34:30.260
By training on many different PDEs, you do get better performance.

34:30.260 --> 34:35.740
So the ones at the right side are the multiple physics pre-trained models, those seem to

34:35.740 --> 34:37.740
do better in many cases.

34:37.740 --> 34:43.420
And it's really because, I mean I think because they've seen, you know, so many different

34:43.420 --> 34:48.540
PDEs, it's like they have a better prior for physics.

34:48.540 --> 34:52.220
I'll skip this as well.

34:52.220 --> 35:00.100
So okay, this is a funny thing that we observed is that, so during talks like this, one thing

35:00.100 --> 35:04.980
that we get asked is, how similar do the PDEs need to be?

35:04.980 --> 35:10.780
Like do the PDEs need to be, you know, like navier stokes but a different parameterization?

35:10.780 --> 35:14.740
Or can they be like completely different physical systems?

35:14.740 --> 35:25.660
So what we found is really hilarious is that, okay, so the bottom line here, this is the

35:25.660 --> 35:32.140
error of the model over a different number of training examples.

35:32.140 --> 35:36.980
So this model was trained on a bunch of different PDEs and then it was introduced to this new

35:36.980 --> 35:41.340
PDE problem and it's given that amount of data.

35:41.340 --> 35:43.300
So that does the best.

35:43.300 --> 35:47.740
This model, it already knows some physics, that one does the best.

35:47.740 --> 35:50.100
The one at the top is the worst.

35:50.100 --> 35:52.900
This is the model that's trained from scratch.

35:52.900 --> 35:58.940
It's never seen anything, this is like your toddler, right, like it doesn't know how the

35:58.940 --> 36:01.540
physical world works.

36:01.540 --> 36:05.100
It was just randomly initialized and it has to learn physics.

36:05.580 --> 36:14.820
Okay, the middle models, those are pre-trained on general video data, a lot of which is cat

36:14.820 --> 36:16.260
videos.

36:16.260 --> 36:26.780
So even pre-training this model on cat videos actually helps you do much better than this

36:26.780 --> 36:32.500
very sophisticated transformer architecture that just has never seen any data.

36:32.500 --> 36:38.820
And it's really because, I mean, we think it's because of shared concepts of spatiotemporal

36:38.820 --> 36:48.700
continuity, right, like videos of cats, there's a spatiotemporal continuity, like the cat

36:48.700 --> 36:53.860
does not teleport across the video unless it's a very fast cat.

36:53.860 --> 36:56.220
There's related concepts, right?

36:56.220 --> 36:58.340
So I mean, that's what we think.

36:58.340 --> 37:06.540
But it's really interesting that pre-training on completely unrelated systems still seems

37:06.540 --> 37:09.820
to help.

37:09.820 --> 37:15.620
And so the takeaway from this is that you should always pre-train your model, even if

37:15.620 --> 37:22.260
the physical system is not that related, you still see benefit of it.

37:23.260 --> 37:29.060
Now obviously, if you pre-train on related data, that helps you more, but anything is

37:29.060 --> 37:32.060
basically better than nothing.

37:32.060 --> 37:39.020
You could basically think of this as the default initialization for neural networks is garbage,

37:39.020 --> 37:44.140
right, like just randomly initializing a neural network, that's a bad starting point.

37:44.140 --> 37:46.580
It's a bad prior for physics.

37:46.580 --> 37:48.980
You should always pre-train your model.

37:48.980 --> 37:50.580
That's the takeaway of this.

37:50.900 --> 37:57.700
OK, so I want to finish up here with kind of rhetorical questions.

37:57.700 --> 38:05.620
So I started the talk about interpretability and kind of like how do we extract insights

38:05.620 --> 38:07.300
from our model.

38:07.300 --> 38:12.700
Now we've kind of gone into this regime of these very large, very flexible foundation

38:12.700 --> 38:17.420
models that seem to learn general principles.

38:17.420 --> 38:25.740
So OK, my question for you, you don't have to answer, but just think it over, is do you

38:25.740 --> 38:29.460
think 1 plus 1 is simple?

38:29.460 --> 38:31.100
It's not a trick question.

38:31.100 --> 38:33.420
Do you think 1 plus 1 is simple?

38:33.420 --> 38:38.580
So I think most people would say yes, 1 plus 1 is simple.

38:38.580 --> 38:42.300
And if you break that down into y, it's simple.

38:42.300 --> 38:46.180
You say, OK, so x plus y is simple for like x and y integers.

38:46.220 --> 38:48.140
That's a simple relationship.

38:48.140 --> 38:49.660
OK, y.

38:49.660 --> 38:52.580
y is x plus y is simple.

38:52.580 --> 38:55.380
And you break that down, it's because plus is simple.

38:55.380 --> 38:57.500
Like plus is a simple operator.

38:57.500 --> 38:59.260
OK, y.

38:59.260 --> 39:01.100
y is plus simple.

39:01.100 --> 39:05.180
It's a very abstract concept, OK?

39:05.180 --> 39:12.900
It's we don't necessarily have plus kind of built into our brains.

39:12.940 --> 39:20.100
It's kind of, I mean, it's really, so I'm going to show this.

39:20.100 --> 39:28.380
This might be controversial, but I think that simplicity is based on familiarity.

39:28.380 --> 39:31.740
We are used to plus as a concept.

39:31.740 --> 39:35.220
We are used to adding numbers as a concept.

39:35.220 --> 39:38.220
Therefore, we call it simple.

39:38.220 --> 39:41.220
You can go back another step further.

39:41.220 --> 39:46.060
The reason we're familiar with addition is because it's useful.

39:46.060 --> 39:49.460
Adding numbers is useful for describing the world.

39:49.460 --> 39:51.340
I count things.

39:51.340 --> 39:53.980
That's useful to live in our universe.

39:53.980 --> 39:57.660
It's useful to count things, to measure things.

39:57.660 --> 40:00.260
Addition is useful.

40:00.260 --> 40:03.060
And it's really one of the most useful things.

40:03.060 --> 40:06.460
So that is why we are familiar with it.

40:06.460 --> 40:09.700
And I would argue that's why we think it's simple.

40:09.700 --> 40:17.820
But the simplicity we have often argued is if it's simple,

40:17.820 --> 40:20.820
it's more likely to be useful.

40:20.820 --> 40:24.620
I think that is actually not a statement about simplicity.

40:24.620 --> 40:28.580
It's actually a statement that if something

40:28.580 --> 40:32.140
is useful for problems like A, B, and C,

40:32.140 --> 40:36.660
then it seems it will also be useful for another problem.

40:36.660 --> 40:38.620
The world is compositional.

40:38.660 --> 40:41.780
If I have a model that works for this set of problems,

40:41.780 --> 40:44.300
it's probably also going to work for this one.

40:44.300 --> 40:47.220
So that's the argument I would like to make.

40:47.220 --> 40:51.580
So when we interpret these models,

40:51.580 --> 40:58.020
I think it's important to keep this in mind and really probe

40:58.020 --> 41:02.340
what is simple, what is interpretable.

41:02.340 --> 41:09.620
So I think this is really exciting for polymathic EI

41:09.620 --> 41:15.020
because these models that are trained on many, many systems,

41:15.020 --> 41:20.100
they will find broadly useful algorithms.

41:20.100 --> 41:23.380
They'll have these neurons that share calculations

41:23.380 --> 41:25.620
across many different disciplines.

41:25.620 --> 41:30.660
So you could argue that that is the utility.

41:30.660 --> 41:34.420
And I mean, maybe we'll discover new kind of operators

41:34.420 --> 41:37.060
and be familiar with those, and we'll

41:37.060 --> 41:38.460
start calling those simple.

41:38.460 --> 41:42.980
So it's not necessarily that all of the things

41:42.980 --> 41:46.620
we discover in machine learning will be simple.

41:46.620 --> 41:49.540
It's kind of that, by definition,

41:49.540 --> 41:53.860
the polymathic EI models will be broadly useful.

41:53.860 --> 41:56.700
And if we know they're broadly useful,

41:56.700 --> 41:58.980
we might get familiar with those.

41:58.980 --> 42:03.860
And that might kind of drive the simplicity of them.

42:03.860 --> 42:06.860
So that's my note in simplicity.

42:06.860 --> 42:09.940
And so the takeaways here are that I

42:09.940 --> 42:14.660
think interpreting a neural network trained on some data

42:14.660 --> 42:18.820
sets offers new ways of discovering

42:18.820 --> 42:21.220
scientific insights from that data.

42:21.220 --> 42:24.580
And I think foundation models like polymathic EI,

42:24.580 --> 42:26.580
I think that is a very exciting way

42:26.580 --> 42:30.820
of discovering new broadly applicable scientific models.

42:30.820 --> 42:33.540
So I'm really excited about this direction.

42:33.540 --> 42:36.340
And thank you for listening to me today.

42:36.340 --> 42:36.840
OK.

42:36.840 --> 42:37.340
Thank you.

42:37.340 --> 42:37.840
Thank you.

42:37.840 --> 42:38.340
Thank you.

42:38.340 --> 42:38.840
Thank you.

42:38.840 --> 42:39.340
Thank you.

42:39.340 --> 42:39.840
Thank you.

42:39.840 --> 42:40.340
Thank you.

42:40.340 --> 42:40.840
Thank you.

42:40.840 --> 42:41.340
Thank you.

42:41.340 --> 42:41.840
Thank you.

42:49.340 --> 42:52.820
Thank you for the question, which was great.

42:52.820 --> 42:55.300
So three short questions.

42:55.300 --> 42:55.800
One.

42:55.800 --> 43:00.560
What was the cost of running polymathic AI

43:00.560 --> 43:02.040
in this kind of a way?

43:02.040 --> 43:03.520
And how to cost it, right?

43:03.520 --> 43:04.020
Yeah.

43:07.520 --> 43:08.020
Two.

43:08.020 --> 43:12.520
When the story built out, it really destroyed you.

43:12.520 --> 43:13.840
I got to help you.

43:13.840 --> 43:16.520
Yeah.

43:16.520 --> 43:18.960
Please use your C mic.

43:18.960 --> 43:19.960
Right in front of you.

43:19.960 --> 43:20.960
We'll put it back.

43:23.960 --> 43:24.460
Yeah.

43:25.460 --> 43:26.460
I'll call them.

43:26.460 --> 43:28.460
And three.

43:28.460 --> 43:30.460
You're putting your back.

43:30.460 --> 43:34.460
Thank you guys for that.

43:34.460 --> 43:38.260
I was trying to answer three.

43:38.260 --> 43:40.980
OK, so I'll try to compartmentalize those.

43:40.980 --> 43:46.220
OK, so the first question was the scale of training.

43:46.220 --> 43:49.060
This is really an open research question.

43:49.060 --> 43:52.740
We don't have the scaling law for science yet.

43:52.740 --> 43:54.740
We have scaling laws for language.

43:54.740 --> 43:56.500
We know that if you have this many GPUs,

43:56.500 --> 43:58.300
you have this size data set, this

43:58.300 --> 43:59.860
is going to be your performance.

43:59.860 --> 44:01.580
We don't have that yet for science

44:01.580 --> 44:04.820
because nobody's built this scale of model.

44:04.820 --> 44:07.660
So that's something we're looking at right now,

44:07.660 --> 44:10.380
is what is the trade-off of scale?

44:10.380 --> 44:13.740
And if I want to train this model on many, many GPUs,

44:13.740 --> 44:16.140
is it worth it?

44:16.140 --> 44:18.660
So that's an open research question.

44:18.660 --> 44:21.620
I do think it'll be large.

44:21.620 --> 44:28.340
Probably order hundreds of GPUs trained for maybe

44:28.340 --> 44:29.740
a couple months.

44:29.740 --> 44:32.300
So it's going to be a very large model.

44:32.300 --> 44:36.900
That's kind of assuming the scale of language models.

44:36.900 --> 44:40.500
Now, the model is going to be free, definitely.

44:40.500 --> 44:43.820
We're all very pro-open source.

44:43.820 --> 44:46.140
And I think that's really the point,

44:46.140 --> 44:49.060
is we want to open source this model so people can download it

44:49.060 --> 44:50.100
and use it in science.

44:50.100 --> 44:55.340
I think that's really the most exciting part about this.

44:55.340 --> 44:57.780
And then I guess the third question you had

44:57.780 --> 45:07.060
was about the future and how it changes how we teach.

45:07.060 --> 45:10.860
I mean, I guess are you asking about teaching science

45:10.860 --> 45:12.380
or teaching machine learning?

45:12.380 --> 45:13.580
Teaching science.

45:13.580 --> 45:16.140
I see.

45:16.140 --> 45:18.100
I mean, yeah, I mean, I don't know.

45:18.100 --> 45:20.100
It depends if it works.

45:20.100 --> 45:22.460
I think if it works, it might very well

45:22.460 --> 45:26.180
change how science is taught.

45:26.180 --> 45:31.700
Yeah, I mean, so I don't know the impact of language models

45:31.700 --> 45:33.260
on computational linguistics.

45:33.260 --> 45:34.860
I'm assuming they've had a big impact.

45:34.860 --> 45:39.460
I don't know if that's affected the teaching of it yet.

45:39.460 --> 45:43.460
But if scientific foundation models had a similar impact,

45:43.460 --> 45:45.740
I'm sure it would impact.

45:45.740 --> 45:46.500
I don't know how much.

45:46.500 --> 45:48.900
Probably depends on the success of the models.

45:55.540 --> 45:58.180
I have a question about your foundation models also.

45:58.180 --> 45:59.980
So in different branches of science,

45:59.980 --> 46:01.500
the data sets are pretty different.

46:01.500 --> 46:03.340
In molecular biology or genetics,

46:03.340 --> 46:05.340
the data sets is a sequence of DNA

46:05.340 --> 46:08.460
versus astrophysics where it's images of stars.

46:08.460 --> 46:11.980
So how do you plan to use the same model

46:11.980 --> 46:15.820
for different form of data sets, input data sets?

46:15.820 --> 46:18.660
So you mean how to pose the objective?

46:18.660 --> 46:19.300
Yes.

46:19.300 --> 46:23.340
So I think the most general objective

46:23.340 --> 46:25.740
is self-supervised learning where you basically

46:25.740 --> 46:29.860
mask parts of the data and you predict the missing part.

46:29.860 --> 46:33.220
If you can optimize that problem,

46:33.220 --> 46:34.820
then you can solve tons of different ones.

46:34.820 --> 46:37.740
You can do regression, predict parameters,

46:37.740 --> 46:41.220
or go the other way and predict rollouts of the model.

46:41.260 --> 46:45.300
It's a really general problem to mask data

46:45.300 --> 46:46.820
and then fill it back in.

46:46.820 --> 46:51.620
That kind of is a superset of many different prediction

46:51.620 --> 46:53.060
problems.

46:53.060 --> 46:56.500
And I think that's why language models are so broadly useful,

46:56.500 --> 46:59.500
even though they're trained just on next-word prediction

46:59.500 --> 47:02.620
or like BERT is a masked model.

47:07.780 --> 47:08.660
Thanks.

47:08.660 --> 47:09.980
Can you hear me?

47:10.020 --> 47:12.540
All right, so that was a great talk.

47:12.540 --> 47:13.780
I'm Victor.

47:13.780 --> 47:17.620
So I'm actually a little bit worried

47:17.620 --> 47:20.260
and this is a little bit of a question.

47:20.260 --> 47:23.700
Whenever you have models like this,

47:23.700 --> 47:26.460
you say that you train these on many examples, right?

47:26.460 --> 47:29.220
So imagine you have already embedded

47:29.220 --> 47:30.940
the laws of physics here somehow,

47:30.940 --> 47:32.980
like let's say the law of repetition.

47:32.980 --> 47:36.060
But when you think about discovering new physics,

47:36.060 --> 47:38.940
we always have this question of whether we are actually

47:38.980 --> 47:42.980
reinventing the wheel or like the network is kind of really

47:42.980 --> 47:46.300
giving us something new or is it something giving us

47:46.300 --> 47:49.020
or is it giving us something that it learned

47:49.020 --> 47:50.340
but it's kind of wrong.

47:50.340 --> 47:55.100
So sometimes we have the answer to know which one is which.

47:55.100 --> 47:57.340
But if you don't have that, let's say for instance,

47:57.340 --> 47:58.980
you're trying to discover what dark matter is,

47:58.980 --> 48:01.620
which is something I'm working on.

48:01.620 --> 48:04.260
How would you know that the network is actually giving you

48:04.260 --> 48:07.700
something new and not just trying to set this

48:07.700 --> 48:10.180
into one of the many parameters it has?

48:11.380 --> 48:16.380
So, okay, so if you wanna test the model

48:17.100 --> 48:19.380
by letting it rediscover something,

48:19.380 --> 48:21.140
then I don't think you should use this.

48:21.140 --> 48:23.340
I think you should use the scratch model,

48:23.340 --> 48:25.460
like from scratch and train it.

48:25.460 --> 48:27.580
Because if you use a pre-train model,

48:27.580 --> 48:30.060
it's probably already seen that physics.

48:30.060 --> 48:32.500
So it's biased towards it in some ways.

48:32.500 --> 48:33.900
So if you're rediscovering something,

48:33.900 --> 48:35.380
I don't think you should use this.

48:35.380 --> 48:37.180
If you're discovering something new,

48:38.460 --> 48:42.060
I do think this is more useful.

48:42.060 --> 48:47.060
So I think a misconception of,

48:48.860 --> 48:50.100
I think machine learning in general

48:50.100 --> 48:52.460
is that scientists view machine learning

48:53.420 --> 48:54.780
for uninitialized models,

48:54.780 --> 48:58.660
like randomly initialized weights as a neutral prior.

48:58.660 --> 49:03.660
But it's not, it's a very explicit prior.

49:04.660 --> 49:06.740
And it happens to be a bad prior.

49:07.940 --> 49:12.940
So if you train from a randomly initialized model,

49:12.940 --> 49:16.660
it's kind of always gonna be a worse prior

49:16.660 --> 49:18.020
than training from a pre-train model,

49:18.020 --> 49:20.540
which has seen many different types of physics.

49:20.540 --> 49:23.060
I think we can kind of make that statement.

49:24.740 --> 49:26.740
So if you're trying to discover new physics,

49:26.740 --> 49:31.740
I mean, like if you train it on some data set,

49:32.740 --> 49:34.140
I guess you can always verify

49:34.140 --> 49:36.500
that the predictions are accurate.

49:36.500 --> 49:40.580
So that would be, I guess, one way to verify it.

49:41.900 --> 49:44.220
But I do think like the fine tuning here,

49:44.220 --> 49:47.300
so like taking this model and training it on the task,

49:47.300 --> 49:49.300
I think that's very important.

49:49.300 --> 49:52.940
I think in language models, it's not as emphasized.

49:52.940 --> 49:54.820
Like people will just take a language model

49:54.820 --> 49:58.100
and tweak the prompt to get a better result.

49:58.300 --> 50:02.980
I think for science, I think the prompt is,

50:02.980 --> 50:04.980
I mean, I think like the equivalent of the prompt

50:04.980 --> 50:06.700
would be important, but I think the fine tuning

50:06.700 --> 50:07.820
is much more important,

50:07.820 --> 50:10.780
because our data sets are so much different across science.

50:13.780 --> 50:15.780
Chris, in the back, thank you.

50:17.660 --> 50:19.100
In my course, the main course,

50:19.100 --> 50:21.700
all the students, I don't think there's one.

50:21.700 --> 50:23.540
Please note, Steph, the symbolic.

50:24.540 --> 50:26.860
It's still limited in the complexity

50:26.860 --> 50:29.580
and dimensionality of the system.

50:29.580 --> 50:32.180
So are you, based on your saying,

50:32.180 --> 50:35.940
also the fine tuning and transfer learning

50:35.940 --> 50:40.940
as a way of enhancing the symbolic regression?

50:40.940 --> 50:42.940
And what does that mean?

50:45.220 --> 50:48.300
Yeah, so the symbolic regression,

50:48.300 --> 50:51.140
I mean, I would consider that it's not used

50:51.140 --> 50:54.100
inside the foundation model part.

50:54.100 --> 50:58.540
I think it's interesting to interpret the foundation model

50:58.540 --> 51:02.860
and see if there's kind of more general physical frameworks

51:02.860 --> 51:03.980
that it comes up with.

51:06.860 --> 51:09.660
I think, yeah, symbolic regression is very limited

51:09.660 --> 51:12.820
in that it's bad at high dimensional problems.

51:12.820 --> 51:17.820
I think that might be because of the choice of operators.

51:18.820 --> 51:22.900
I think if you can consider maybe high dimensional operators,

51:22.900 --> 51:25.140
you might be a bit better off.

51:25.140 --> 51:26.420
I mean, symbolic regression,

51:26.420 --> 51:29.580
it's an active area of research,

51:29.580 --> 51:32.620
and I think the hardest, the biggest hurdle right now

51:32.620 --> 51:37.620
is it's not good at finding very complex symbolic models.

51:37.620 --> 51:55.620
So I guess you could, it depends on the dimensionality

51:55.620 --> 51:56.620
of the data.

51:56.620 --> 52:00.100
I guess if it's very high dimensional data,

52:00.100 --> 52:06.500
you're always kind of, symbolic regression is not good

52:06.500 --> 52:08.780
at high dimensional data unless you can have

52:08.780 --> 52:13.180
kind of some operators that aggregate

52:13.180 --> 52:15.580
to lower dimensional spaces.

52:18.380 --> 52:22.100
Yeah, I don't know if I'm answering your question.

52:22.100 --> 52:23.100
OK.

52:23.100 --> 52:24.780
I wanted to ask a little bit.

52:24.780 --> 52:28.780
So when you were showing the construction of these trees

52:28.780 --> 52:31.540
of each generation and the different operators,

52:31.540 --> 52:33.380
I think this is related to kind of general themes

52:33.380 --> 52:34.660
at the top and other questions.

52:34.660 --> 52:37.100
But often in doing science, when you're writing it,

52:37.100 --> 52:39.780
you're presented with kind of an algorithmic self-prompt.

52:39.780 --> 52:42.020
It's like, you know, diagonalize the Hamiltonian

52:42.020 --> 52:44.020
or something like that.

52:44.020 --> 52:46.140
How do you calculate that aspect of doing science

52:46.140 --> 52:48.820
that is kind of the algorithmic side of solving

52:48.820 --> 52:52.260
the problem rather than just going up and down?

52:52.260 --> 52:52.780
Right.

52:52.780 --> 52:55.420
These, do you see my?

52:55.420 --> 52:56.540
Yeah.

52:56.540 --> 52:58.940
Yeah, so the question was about how

52:58.940 --> 53:03.100
do you incorporate kind of more general, not

53:03.180 --> 53:05.820
analytic operators, but kind of more general algorithms

53:05.820 --> 53:08.500
like a Hamiltonian operator.

53:08.500 --> 53:11.460
I think that, I mean, like in principle,

53:11.460 --> 53:15.100
symbolic regression is it's part of a larger family

53:15.100 --> 53:17.900
of an algorithm called program synthesis,

53:17.900 --> 53:21.220
where the objective is to find a program, you know,

53:21.220 --> 53:26.140
like code that describes a given data set, for example.

53:26.140 --> 53:30.100
So if you can write your operators

53:30.100 --> 53:32.660
into your symbolic regression approach

53:32.780 --> 53:34.980
and your symbolic regression approach

53:34.980 --> 53:39.300
has that ground truth model in there somewhere,

53:39.300 --> 53:40.780
then I think it's totally possible.

53:40.780 --> 53:44.900
I think, like, it's harder to do.

53:44.900 --> 53:47.060
I think, like, even symbolic regression with scalars

53:47.060 --> 53:53.580
is, it's fairly difficult to actually set up an algorithm.

53:53.580 --> 53:54.780
I think, I don't know, I think it's really

53:54.780 --> 53:55.860
like an engineering problem.

53:55.860 --> 54:02.020
But the conceptual part is totally like there for this.

54:02.020 --> 54:03.020
Yeah.

54:07.500 --> 54:08.620
Thanks.

54:08.620 --> 54:09.140
Oh, sorry.

54:13.540 --> 54:17.500
This claim that random initial weights are always bad

54:17.500 --> 54:18.820
or pre-training is always good.

54:18.820 --> 54:20.300
I don't know if they're always bad,

54:20.300 --> 54:25.460
but it seems like from our experiments,

54:25.460 --> 54:29.980
we've never seen a case where pre-training

54:29.980 --> 54:32.180
on some kind of physical data hurts.

54:32.180 --> 54:34.340
Like the cap video is an example.

54:34.340 --> 54:35.980
We thought that would hurt the model.

54:35.980 --> 54:36.940
It didn't.

54:36.940 --> 54:39.260
That is a cute example.

54:39.260 --> 54:42.100
I'm sure there's cases where some pre-training hurts.

54:42.100 --> 54:43.620
Yeah, so that's essentially my question.

54:43.620 --> 54:45.540
So we're aware of, like, adversarial examples.

54:45.540 --> 54:47.540
For example, you train on MNIST, add a bit of noise.

54:47.540 --> 54:49.860
It does terrible compared to what a human would do.

54:49.860 --> 54:53.220
What do you think adversarial examples look like in science?

54:53.220 --> 54:54.860
Yeah, I mean, I don't know what those are,

54:54.860 --> 54:57.660
but I'm sure they exist somewhere,

54:57.700 --> 55:00.780
where pre-training on certain data types kind of

55:00.780 --> 55:03.500
messes with training a bit.

55:03.500 --> 55:07.180
We don't know those yet, but yeah, it'll be interesting.

55:07.180 --> 55:09.180
Do you think it's a pick fall, though, of the approach?

55:09.180 --> 55:12.140
Because I have a model of the sun and a model of DNA.

55:14.580 --> 55:18.620
Yeah, I mean, I don't know.

55:18.620 --> 55:21.180
I guess we'll see.

55:21.180 --> 55:23.260
Yeah, it's hard to know.

55:23.260 --> 55:27.420
I guess from language, we've seen you can pre-train

55:27.420 --> 55:29.820
like a language model on video data,

55:29.820 --> 55:32.860
and it helps the language, which is really weird.

55:32.860 --> 55:35.940
But it does seem like if there's any kind of concepts,

55:35.940 --> 55:38.260
it does, if it's flexible enough,

55:38.260 --> 55:41.100
it can kind of transfer those in some ways.

55:41.100 --> 55:41.940
So we'll see.

55:41.940 --> 55:45.860
I mean, presumably, we'll find some adversarial examples there.

55:45.860 --> 55:47.140
So far, we haven't.

55:47.140 --> 55:49.900
We thought the cat was one, but it wasn't.

55:49.900 --> 55:50.740
It helped.

