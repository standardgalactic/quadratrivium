start	end	text
0	19440	So I'm very excited today to talk to you about this idea of interpreting neural networks
19440	26960	to get physical insight, which I view as kind of a new, really kind of a new paradigm of
26960	29440	doing science.
29440	32600	So this is a work with a huge number of people.
32600	38000	I can't individually mention them all, but many of them are here at the Flutter Institute.
38000	39960	So I'm going to split this up.
39960	41200	I'm going to do two parts.
41200	46400	The first one, I'm going to talk about kind of how we go from a neural network to insights,
46400	49160	how we should get insights out of a neural network.
49160	55560	The second part, I'm going to talk about this polymathic AI thing, which is about basically
55560	59800	building massive neural networks for science.
59800	69840	So my motivation for this line of work is examples like the following.
69840	77800	So there was this paper led by Kimberly Stakkenfeld at DeepMind a couple years ago on learning
77800	83080	fast subgrid models for fluid turbulence.
83080	85200	So what you see here is the ground truth.
85200	88640	So this is kind of some box of a fluid.
88640	96440	The bottom row is the learned kind of subgrid model, essentially, for this simulation.
96440	105960	The really interesting thing about this is that this model was only trained on 16 simulations,
105960	112160	but it actually learned to be more accurate than all traditional subgrid models at that
112160	115000	resolution for fluid dynamics.
115000	121560	So I think it's really exciting kind of to figure out how did the model do that and kind
121560	128440	of what can we learn about science from this neural network.
128440	134680	Another example is, so this is a work that I worked on with Dan Twio and others on predicting
134680	137000	instability in planetary systems.
137000	140160	So this is a centuries old problem.
140160	145040	You have some, you know, this compact planetary system and you want to figure out when does
145040	147680	it go unstable.
147680	153240	There are literally, I mean, people have literally worked on this for centuries.
153240	158720	It's a fundamental problem in chaos, but this neural network trained on, I think it was
158720	163080	maybe 20,000 simulations.
163080	167480	It's not only more accurate at predicting instability, but it also seems to generalize
167480	171080	better to kind of different types of systems.
171080	177200	So it's really interesting to think about, okay, these neural networks, they've seemed
177200	179600	to have learned something new.
179600	183480	How can we actually use that to advance our own understanding?
183480	186600	So that's my motivation here.
186600	191300	So the traditional approach to science has been kind of, you have some low dimensional
191300	198260	data set or some kind of summary statistic and you build theories to describe that low
198260	203760	dimensional data, which might be kind of a summary statistic.
203760	208500	So you can look throughout the history of science, so maybe Kepler's law is an empirical
208500	215340	fit to data and then of course Newton's law of gravitation was required to explain this.
215340	216740	Another example is like Plank's law.
216740	224340	So this was actually an empirical fit to data and quantum mechanics was required, partially
224340	227340	motivated by this to explain it.
227340	237420	So this is kind of the normal approach to building theories.
237420	242980	And of course some of these, they've kind of, I mean it's not only this, it also involves
242980	243980	many other things.
243980	252060	But I think it's really exciting to think about how we can involve interpretation of
252060	256020	data driven models in this process, very generally.
256020	259140	So that's what I'm going to talk about today.
259140	266460	I'm going to conjecture that in this era of AI, where we have these massive neural networks
266460	273060	that kind of seem to outperform all of our traditional theory, we might want to consider
273060	280620	this approach where we use a neural network as essentially compression tool or some kind
280620	289620	of tool that pulls apart common patterns in a data set.
289620	294100	And we build theories not to describe the data directly, but really kind of to describe
294100	297620	the neural network and what the neural network has learned.
297620	303340	So I think this is kind of an exciting new approach to, I mean really science in general,
303340	306420	I think especially the physical sciences.
306420	313300	So the key point here is neural networks trained on massive amounts of data with very flexible
313300	319780	functions, they seem to find new things that are not in our existing theory.
319780	323780	So I showed you the example with turbulence, you know we can find better sub-grid models
323780	329500	just from data, and we can also do this with planetary dynamics.
329500	336300	So I think our challenge as scientists for those problems is distilling those insights
336300	339580	into our language, kind of incorporating it in our theory.
339580	346220	I think this is a really exciting way to kind of look at these models.
346220	349300	So I'm going to break this down a bit.
349300	354700	The first thing I would like to do is just go through kind of what machine learning is,
354700	361700	how it works, and then talk about this, kind of how you apply them to different data sets.
361700	372260	Okay so just going back to the very fundamentals, linear regression in 1D, this is, I would
372260	379700	argue if you don't really have physical meaning to these parameters yet, it is a kind of type
379700	382260	of machine learning.
382260	388020	And so this is, these are scalars, right, X and Y, those are scalars, FI0, FI1, scalar
388020	391860	parameters, linear model.
391860	396380	You go one step beyond that, and you get this shallow network.
396380	406940	So again this has 1D input, X, 1D output, Y, but now we've introduced this layer.
406940	415260	So we have these linear models, so we have three hidden neurons here, and they pass
415260	420100	through this function A, so this is called an activation function.
420100	427900	And what this does is it gives the model a way of including some non-linearity.
427900	431860	So these are called activation functions.
431860	439300	The one that most people would reach for first is the rectified linear unit, or RELU.
439300	445220	Essentially what this does is it says if the input is less than zero, drop it at zero,
445220	447980	greater than zero, leave it.
447980	454780	This is a very simple way of adding some kind of non-linearity to my flexible curve that
454780	461060	I'm going to fit to my data, right?
461060	467380	The next thing I do is I have these different activation functions.
467380	475100	They have this kind of joint here at different points, which depends on the parameters.
475100	481540	And I'm going to multiply the outputs of these activations by a number.
481540	488380	That's kind of the output of my kind of a layer of the neural network.
488380	493740	And this is going to maybe change the direction of it, change the slope of it.
493740	495740	The next thing I'm going to do is I'm going to sum these up.
495740	502120	I'm going to superimpose them, and I get this is the output of one layer in my network.
502120	504500	So this is a shallow network.
504500	509260	Basically what it is, it's a piecewise linear model.
509260	515660	And the joints here, the parts where it kind of switches from one linear region to another,
515660	520500	those are determined by the inputs to the first layer's activations.
520500	523940	So it's basically a piecewise linear model.
523940	527780	It's a piecewise linear model.
527780	536420	And the one cool thing about it is you can use this piecewise linear model to approximate
536420	539660	any 1D function to arbitrary accuracy.
539660	545300	So if I want to model this function with five joints, I can get an approximation like this
545300	551340	with 10 joints like this, 20 like that, and I can just keep increasing the number of these
551340	552340	neurons.
552340	556540	And that gives me better and better approximations.
556540	560500	So this is called the universal approximation theorem.
560500	569020	So it's that my shallow neural network just has one kind of layer of activations.
569020	573780	I can describe any continuous function to arbitrary precision.
573780	581220	Now that's not, I mean this alone is not that exciting because I can do that with polynomials.
581220	584820	I don't need, the neural network's not the only thing that does that.
584820	590020	I think the exciting part about neural networks is when you start making them deeper.
590020	594020	So first let's look at what if we had two inputs?
594020	595020	What would it look like?
595020	596820	We had two inputs.
596820	604580	Now these activations, they are activated along planes, not points, they're activated
604580	606100	along planes.
606100	615420	So for, this is my, maybe my input plane, I'm basically chopping it along the zero part
615420	619700	and now I have these 2D planes in space.
619700	624740	And the next thing I'm going to do, I'm going to scale these and then I'm going to superimpose
624740	626300	them.
626300	634620	And this gives me ways of representing kind of arbitrary functions in now a 2D space rather
634620	635740	than just a 1D space.
635740	644900	So it gives me a way of expressing arbitrary continuous functions.
644900	654820	Now the cool part, oops, the cool part here is when I want to do two layers.
654820	660700	So now I have two layers, so I have this, this is my first neural network, this is my
660700	665620	second neural network and my first neural network looks like this.
665620	668660	If I consider it alone, it looks like this.
668660	672820	My second neural network, it looks like this.
672820	677500	If I just like, I cut this neural network out, it looks like this, okay.
677500	686820	When I compose them together, I get this, this, this shared kind of behavior where,
686820	693580	so I'm composing these functions together and essentially what happens is, it's almost
693580	701100	like you fold the functions together so that I experience that function in this linear
701100	704140	region and kind of backwards and then again.
704140	708620	So you can see there's, there's kind of like that function is mirrored here, right?
708620	711780	It goes, goes back and forth.
711780	716660	So you can make this analogy to folding a piece of paper.
716660	722300	So if I consider my first neural network like this on a piece of paper, I could essentially
722300	729420	fold it, draw my second neural network, the function over that, that first one and then
729420	735100	expand it and essentially now I have this, this function.
735100	743340	So the, the cool part about this is that I'm sharing, I'm kind of sharing computation because
743340	747820	I'm sharing neurons in my neural network.
747820	752940	So this is going to come up again, this is kind of a theme, we're, we're doing efficient
752940	758460	computation in neural networks by sharing neurons and it's, it's useful to think about
758460	764700	it in this, this, this way, kind of folding paper, drawing curves over it and expanding
764700	765700	it.
765700	770500	Okay, so let's go back to the physics.
770500	777540	Now neural networks, right, they're efficient universal function approximators.
777540	782180	You can think of them as kind of like a type of data compression.
782180	790180	The same neurons can be used for different calculations in the same network and a common
790180	798500	use case in, in physical sciences, especially what I work on, is emulating physical processes.
798500	802980	So if I have some, my, my simulator is kind of too expensive or I have like real world
802980	806660	data and my simulator is not good at describing it.
806660	811740	I can build a neural network that maybe emulates it.
811740	816260	So like I have a neural network that looks at kind of the initial conditions in this
816260	820500	model and it predicts when it's going to go unstable.
820500	824500	So this is a, this is a good use case for them.
824500	832020	And once I have that, so maybe I have this, I have this trained piecewise linear model
832020	835500	that kind of emulates some physical process.
835500	840740	Now how do I take that and go to interpret it?
840740	844300	How do I actually get insight out of it?
844300	848580	So this is where I'm going to talk about symbolic regression.
848580	851460	So this is one of my favorite things.
851460	858500	So a lot of the interpretability work in industry, especially like computer vision language,
858500	861940	there's not really like, there's not a good modeling language.
861940	867180	Like if I have a, if I have a model that classifies cats and dogs, there's not really like, there's
867180	871660	not a language for describing every possible cat.
871660	873940	There's not like a mathematical framework for that.
873940	875060	But in science, we do have that.
875060	886060	We do have, oops, we do have a very good mathematical framework.
886060	893220	Let me see if this works.
893220	899180	So in science, right, so we have this, you know, in science we have this very good understanding
899180	905540	of the universe and we have this language for it.
905540	910660	We have mathematics, which describes the universe very well.
910660	917100	And I think when we want to interpret these data-driven models, we should use this language
917100	920940	because that will give us results that are interpretable.
920940	925940	If I have some piecewise linear model with different, you know, like millions of parameters,
925940	928020	it's not, it's not really useful for me, right?
928020	933820	I want to, I want to express it in the language that I'm familiar with, which is mathematics.
934820	940900	So you can look at like any cheat sheet and it's a lot of, you know, simple algebra.
940900	943860	This is the language of science.
943860	951420	So symbolic regression is a machine learning task where the objective is to find analytic
951420	955980	expressions that optimize some objective.
955980	960580	So maybe I, maybe I want to fit that data set.
960580	966140	And what I could do is basically try different trees.
966140	969300	So these are like expression trees, right?
969300	971300	So this equation is that tree.
971300	977620	I basically find different expression trees that match that data.
977620	983740	So the point of symbolic regression, I want to find equations that fit the data set.
983740	991580	So the symbolic and the parameters rather than just optimizing parameters in some model.
991580	997540	So the, the, the current way to do this, the, the state of the art way is a genetic algorithm.
997540	1003300	So it's, it's kind of, it's not really like a clever algorithm.
1003300	1006860	It's, it's a, I can say that because I work on it.
1006860	1010580	It's a, it's, it's pretty close to brute force.
1010580	1018180	Basically what you do is you treat your equation like a DNA sequence and you basically evolve
1018180	1019180	it.
1019180	1025900	So you do like mutations, you swap one operator to another, maybe, maybe you cross-breed them.
1025900	1028540	So you have like two expressions which are okay.
1028540	1030540	You literally breed those together.
1030540	1036060	I mean, not literally, but you conceptually breed those together, get a new expression
1036060	1041300	until you fit the data set.
1041300	1048420	So yeah, so this is a genetic algorithm based search for symbolic regression.
1048420	1058060	Now the, the point of this is to find simple models in our language of mathematics that
1058060	1062140	describe a given data set.
1062140	1066220	So, so I've spent a lot of time working on these frameworks.
1066220	1073180	So pyser, symbolic regression.jl, they, they work like this.
1073180	1077500	So if I have this expression, I want to model that data set, essentially what I'm going
1077500	1086340	to do is just search over all possible expressions until I find one that gets me closer to this
1086340	1087780	ground truth expression.
1088460	1092780	You see it's kind of testing different, different branches in evolutionary space.
1092780	1100780	I'm going to play that again until it reaches this ground truth data set.
1100780	1104820	So this is, this is pretty close to how it works.
1104820	1116380	You're essentially finding simple expressions that fit some data set accurately.
1116380	1125700	So, what I'm going to show you how to do is this symbolic regression idea is about
1125700	1133420	fitting, kind of finding models, symbolic models that I can use to describe a data set.
1133420	1140700	I want to use that to build surrogate models of my neural network.
1140700	1147020	So this is, this is kind of a way of translating my model into my language.
1147020	1152940	You could, you could also think of it as like polynomial or like a Taylor expansion in some
1152940	1155140	ways.
1155140	1157860	The way this works is as follows.
1157860	1163940	If I have some neural network that I've trained on my data set, whatever, I'm going to train
1163940	1167220	it normally, freeze the parameters.
1167220	1171460	Then what I do is I record the inputs and outputs.
1171460	1173660	I kind of treat it like a data generating process.
1173660	1179300	I, I try to see like, okay, what's the behavior for this input, this input, and so on.
1179300	1185940	Then I stick those inputs and outputs into pyser, for example, and I, I find some equation
1185940	1192380	that models that neural network, or maybe it's like a piece of my neural network.
1192380	1198340	So this is a, this is building a surrogate model for my neural network that is kind of
1198340	1200340	approximates the same behavior.
1200340	1204820	Now you wouldn't just do this for like a standalone neural network.
1204820	1210540	This, this would typically be part of like a larger model, and it would give you a way
1210540	1216420	of interpreting exactly what it's doing for different inputs.
1216420	1222820	So what I might have is maybe I have like two, two pieces, like two neural networks
1222820	1224420	here.
1224420	1228780	Maybe I think the first neural network is like learning features, or it's learning
1228780	1231140	some kind of coordinate transform.
1231140	1234620	The second one is doing something in that space.
1234620	1238500	It's using those features for calculation.
1238500	1244780	And so I can, using symbolic regression, which we call symbolic distillation, I can, I can
1244780	1248740	distill this model into equations.
1248740	1252140	So that's, that's the basic idea of this.
1252140	1259540	I replace neural networks, so I replace them with my surrogate model, which is now an equation.
1259540	1263300	You would typically do this for G as well.
1263300	1268620	And now I have equations that describe my model.
1268620	1274100	And this is kind of a interpretable approximation of my original neural network.
1274100	1278300	Now the reason you wouldn't want to do this for like just directly on the data is because
1278300	1280860	it's a harder search problem.
1280860	1287300	If you break it into pieces, like kind of interpreting pieces of a neural network, it's easier.
1287300	1292900	Because you're only searching for two n expressions rather than n squared.
1292900	1294620	So it's a, it's a bit easier.
1294620	1300900	And you're kind of using the neural network as a way of factoring, factorizing the system
1300900	1306560	into different pieces that you then interpret.
1306560	1308860	So we've, we've used this in, in different papers.
1308860	1319300	So this is one led by Pablo Lemos on rediscovering Newton's law of gravity from data.
1319300	1325380	So this was a, this was a cool paper because we didn't tell it the masses of the bodies
1325380	1326380	in the solar system.
1326380	1333940	It had to simultaneously find the masses of every, all of these 30 bodies we gave it.
1333940	1335460	And it also found the law.
1335460	1338460	So we kind of train this neural network to do this.
1338460	1340660	And then we interpret that neural network.
1340660	1344860	And it gives us Newton's law of gravity.
1344860	1346360	Now that's a rediscovery.
1346360	1348260	And of course, like we know that.
1348260	1351420	So I think the discoveries are also cool.
1351420	1352820	So these are not my papers.
1352820	1353820	These are other people's papers.
1353820	1355740	I thought they were really exciting.
1355740	1363820	So this is one, a recent one by Ben Davis and Jehow Jin, where they discover this new
1363820	1367700	black hole mass scaling relationship.
1367700	1375180	So it's, it relates the, I think it's the spirality or something in a galaxy in the
1375180	1377660	velocity with the mass of a black hole.
1377660	1382340	So they, they found this with this technique, which is exciting.
1382340	1385620	And I saw this other cool one recently.
1385620	1392140	They found this cloud cover model with this technique using Picer.
1392140	1396820	So they, it kind of gets you this point where it's a, it's a fairly simple model and it's
1396820	1400380	also pretty accurate.
1400380	1405380	But again, the, the point of this is to find a model that you can understand, right?
1405380	1410260	It's not this black box neural network with, with billions of parameters.
1410260	1414980	It's a simple model that you can have a handle on.
1414980	1416380	Okay.
1416380	1418580	So that's part one.
1418580	1423980	Now part two, I want to talk about polymathic AI.
1423980	1427020	So this is kind of like the complete opposite end.
1427020	1430180	We're going to go from small models in the first part.
1430180	1432820	Now we're going to do the biggest possible models.
1432820	1438140	And I'm going to also talk about the meaning of simplicity, what it actually means.
1438140	1446300	So the past few years, you may have noticed there's been this shift in industry, industrial
1446300	1450460	machine learning to favor foundation models.
1450460	1454500	So like chat GPT is an example of this.
1454500	1463660	A foundation model is a machine learning model that serves as the foundation for other models.
1463660	1470060	These models are trained by basically taking massive amounts of general diverse data and,
1470060	1477020	and training this flexible model on that data and then fine tuning them to some specific
1477020	1478220	task.
1478220	1486100	So you could think of it as maybe teaching this machine learning model English and French
1486100	1490820	before teaching it to do translation between the two.
1490820	1495980	So it often gives you better performance on downstream tax.
1495980	1505980	I mean you can also see that, I mean chat GPT is, I've heard that it's trained on GitHub
1505980	1511460	and that kind of teaches it to reason a bit better.
1511460	1517020	And so the, I mean basically these models are trained on massive amounts of data and
1517020	1520700	they form this idea called a foundation model.
1520700	1525980	So the general idea is you, you collect, you know, you collect your massive amounts
1525980	1533180	of data, you have this very flexible model and then you train it on, you might train
1533180	1540460	it to do self supervised learning which is kind of like you mask parts of the data and
1540460	1543220	then the model tries to fill it back in.
1543220	1544900	That's a, that's a common way you train that.
1544900	1551420	So like for example, GPT style models, those are basically trained on the entire internet
1551420	1554700	and they're trained to predict the next word.
1554700	1556140	That's their only task.
1556140	1561740	You get an input sequence of words, you predict the next one and you just repeat that for
1561740	1564180	massive amounts of text.
1564180	1572380	And then just by doing that they get really good at general language understanding.
1572380	1576180	And they are fine tuned to be a chatbot essentially.
1576180	1581500	So they're, they're given a little bit of extra data on this is how you talk to someone
1581500	1587580	and be friendly and so on and, and that's much better than just training a model just
1587580	1588580	to do that.
1588580	1593420	So it's this idea of pre-training models.
1593420	1600100	So I mean once you have this model, I think like kind of the, the, the cool part about
1600100	1609420	these models is they're really trained in a way that gives them general priors for data.
1609420	1615140	So if I have like some, maybe I have like some artwork generation model, it's trained
1615140	1619020	on different images and it kind of generates different art.
1619020	1626140	I can fine tune this model on like Studio Ghibli artwork and it doesn't need much training
1626140	1629860	data because it already knows what a face looks like.
1629860	1633260	Like it's already seen tons of different faces.
1633260	1638660	So just by fine tuning it on some small number of examples, it can, it can kind of pick up
1638660	1642020	this task much quicker.
1642020	1644140	That's essentially the idea.
1644140	1649300	Now this is, I mean the same thing is true in language, right?
1649300	1656060	Like if I, if I train a model on, if I train a model just to do language translation, right?
1656060	1657660	Like I just teach it that.
1657660	1663220	It's kind of, I start from scratch and I just train it English to French.
1663220	1664860	It's going to struggle.
1664860	1671100	Whereas if I teach it English and French, kind of I teach it about the languages first
1671100	1677660	and then I specialize it on translation, it's going to do much better.
1677660	1680660	So this brings us to science.
1680660	1686260	So in, in science we also have this.
1686260	1691180	We also have this idea where there are shared concepts, right?
1691180	1697180	Like different languages have shared, there's a shared concept of grammar in different languages.
1697180	1699820	In science we also have shared concepts.
1699820	1706220	You could kind of draw a big circle around many areas of science and causality is a shared
1706220	1707820	concept.
1707820	1714940	If you zoom in to say dynamical systems, you could think about like multi-scale dynamics
1714940	1721460	is, is shared in many different disciplines, chaos is another shared concept.
1721460	1731420	So maybe if we train a general model, you know, over many, many different data sets,
1731420	1736580	the same way chat, GPT is trained on many, many different languages and text databases.
1737540	1743180	Maybe you'll pick up general concepts and then when we finally make it specialize to
1743180	1750740	our particular problem, maybe it'll do it, it'll find it easier to learn.
1750740	1753660	So that's essentially the idea.
1753660	1758020	So you can, you can really actually see this for a particular system.
1758020	1761660	So one example is the reaction diffusion equation.
1761660	1768340	This is a type of PDE and the shallow water equations, another type of PDE.
1768340	1772980	Different fields, different PDEs, but both have waves.
1772980	1777460	So they, they both have wave-like behavior.
1777460	1784900	So I mean maybe if we train this massive flexible model on both of these systems, it's going
1784900	1791780	to kind of learn a general prior for what a wave looks like.
1791780	1795900	And then if I have like some, you know, some small data set I only have a couple examples
1795900	1802220	of, maybe it'll immediately identify, oh, that's a wave, I know how to do that.
1802220	1811340	It's almost like, I mean, I kind of feel like in science today, what we often do is, I mean
1811340	1814500	we train machine learning models from scratch.
1814500	1820540	It's almost like we're taking toddlers and we're teaching them to do pattern matching
1820540	1823420	on like really advanced problems.
1823420	1827420	Like we, we have a toddler and we're showing them this is a, you know, this is a spiral
1827420	1833100	galaxy, this is an elliptical galaxy, and it kind of has to just do pattern matching.
1833100	1838860	Whereas maybe a foundation model that's trained on broad classes of problems, it's, it's kind
1838860	1843620	of like a general science graduate, maybe.
1843620	1847480	So it has a prior for how the world works.
1847480	1852460	It has seen many different phenomena before, and so when it, when you finally give it that
1852460	1857460	data set to kind of pick up, it's already seen a lot of that phenomena.
1857460	1859860	That's really the pitch of this.
1859860	1862500	That's why we think this will work well.
1862500	1868380	Okay, so we, we created this collaboration last year.
1868380	1876660	So this started at Flatiron Institute, led by Shirley Ho, to build this thing, a foundation
1876660	1879780	model for science.
1879780	1887980	So this, this is across disciplines, so we want to, you know, build these models to incorporate
1887980	1895900	data across many different disciplines, across institutions, and so we're currently working
1895900	1898740	on kind of scaling up these models right now.
1898740	1905540	The final, I think the final goal of this collaboration is that we would release these
1905540	1912100	open source foundation models so that people could download them and fine tune them to different
1912100	1913100	tasks.
1913100	1917860	So it's really kind of like a different paradigm of doing machine learning, right?
1917860	1923660	Like rather than the current paradigm where we take a model, randomly initialize it, it's
1923660	1929820	kind of like a, like a toddler, doesn't know how the world works, and we train that.
1929820	1936780	This paradigm is we have this generalist science model, and you start from that.
1936780	1941500	It's kind of a better initialization of a model.
1941500	1945020	That's, that's the, that's the pitch of Polymathic.
1945020	1951340	Okay, so we have results, so this year we're kind of scaling up, but last year we had a
1951340	1960060	couple of papers, so this is one led by Mike McCabe called Multiple Physics Pre-Training.
1960060	1967980	This paper looked at what if we have this general PDE simulator, this, this model that
1967980	1974940	learns to essentially run fluid dynamic simulations, and we train it on many different PDEs.
1974940	1979380	Will it do better on new PDEs or will it do worse?
1979380	1992980	So what we found is that a single, so a single model is not only able to match single models
1992980	1998140	trained on specific tasks, it can actually outperform them in many cases.
1998140	2006860	So it does seem like if you take a more flexible model, you train it on more diverse data,
2006860	2009220	it will do better in a lot of cases.
2009220	2016140	I mean it's, it's not unexpected because we do see this with language and vision, but
2016140	2021380	I think it's still really cool to, to see this.
2021380	2025340	So I'll skip through some of these.
2025340	2032060	So this is like, this is the ground truth data, and this is the reconstruction.
2032060	2035980	Essentially what it's doing is it's predicting the next step, right?
2035980	2039900	It's predicting the next velocity, the next density, and pressure, and so on.
2039900	2044540	And you're taking that prediction and running it back through the model, and you get this,
2044540	2047820	this rule out simulation.
2047820	2052740	So this is a, this is a task people work on in machine learning.
2052740	2055940	I'm gonna skip through these.
2055940	2063820	And essentially what we found is that most of the time by using this multiple physics
2063820	2064820	pre-training.
2064820	2070260	By training on many different PDEs, you do get better performance.
2070260	2075740	So the ones at the right side are the multiple physics pre-trained models, those seem to
2075740	2077740	do better in many cases.
2077740	2083420	And it's really because, I mean I think because they've seen, you know, so many different
2083420	2088540	PDEs, it's like they have a better prior for physics.
2088540	2092220	I'll skip this as well.
2092220	2100100	So okay, this is a funny thing that we observed is that, so during talks like this, one thing
2100100	2104980	that we get asked is, how similar do the PDEs need to be?
2104980	2110780	Like do the PDEs need to be, you know, like navier stokes but a different parameterization?
2110780	2114740	Or can they be like completely different physical systems?
2114740	2125660	So what we found is really hilarious is that, okay, so the bottom line here, this is the
2125660	2132140	error of the model over a different number of training examples.
2132140	2136980	So this model was trained on a bunch of different PDEs and then it was introduced to this new
2136980	2141340	PDE problem and it's given that amount of data.
2141340	2143300	So that does the best.
2143300	2147740	This model, it already knows some physics, that one does the best.
2147740	2150100	The one at the top is the worst.
2150100	2152900	This is the model that's trained from scratch.
2152900	2158940	It's never seen anything, this is like your toddler, right, like it doesn't know how the
2158940	2161540	physical world works.
2161540	2165100	It was just randomly initialized and it has to learn physics.
2165580	2174820	Okay, the middle models, those are pre-trained on general video data, a lot of which is cat
2174820	2176260	videos.
2176260	2186780	So even pre-training this model on cat videos actually helps you do much better than this
2186780	2192500	very sophisticated transformer architecture that just has never seen any data.
2192500	2198820	And it's really because, I mean, we think it's because of shared concepts of spatiotemporal
2198820	2208700	continuity, right, like videos of cats, there's a spatiotemporal continuity, like the cat
2208700	2213860	does not teleport across the video unless it's a very fast cat.
2213860	2216220	There's related concepts, right?
2216220	2218340	So I mean, that's what we think.
2218340	2226540	But it's really interesting that pre-training on completely unrelated systems still seems
2226540	2229820	to help.
2229820	2235620	And so the takeaway from this is that you should always pre-train your model, even if
2235620	2242260	the physical system is not that related, you still see benefit of it.
2243260	2249060	Now obviously, if you pre-train on related data, that helps you more, but anything is
2249060	2252060	basically better than nothing.
2252060	2259020	You could basically think of this as the default initialization for neural networks is garbage,
2259020	2264140	right, like just randomly initializing a neural network, that's a bad starting point.
2264140	2266580	It's a bad prior for physics.
2266580	2268980	You should always pre-train your model.
2268980	2270580	That's the takeaway of this.
2270900	2277700	OK, so I want to finish up here with kind of rhetorical questions.
2277700	2285620	So I started the talk about interpretability and kind of like how do we extract insights
2285620	2287300	from our model.
2287300	2292700	Now we've kind of gone into this regime of these very large, very flexible foundation
2292700	2297420	models that seem to learn general principles.
2297420	2305740	So OK, my question for you, you don't have to answer, but just think it over, is do you
2305740	2309460	think 1 plus 1 is simple?
2309460	2311100	It's not a trick question.
2311100	2313420	Do you think 1 plus 1 is simple?
2313420	2318580	So I think most people would say yes, 1 plus 1 is simple.
2318580	2322300	And if you break that down into y, it's simple.
2322300	2326180	You say, OK, so x plus y is simple for like x and y integers.
2326220	2328140	That's a simple relationship.
2328140	2329660	OK, y.
2329660	2332580	y is x plus y is simple.
2332580	2335380	And you break that down, it's because plus is simple.
2335380	2337500	Like plus is a simple operator.
2337500	2339260	OK, y.
2339260	2341100	y is plus simple.
2341100	2345180	It's a very abstract concept, OK?
2345180	2352900	It's we don't necessarily have plus kind of built into our brains.
2352940	2360100	It's kind of, I mean, it's really, so I'm going to show this.
2360100	2368380	This might be controversial, but I think that simplicity is based on familiarity.
2368380	2371740	We are used to plus as a concept.
2371740	2375220	We are used to adding numbers as a concept.
2375220	2378220	Therefore, we call it simple.
2378220	2381220	You can go back another step further.
2381220	2386060	The reason we're familiar with addition is because it's useful.
2386060	2389460	Adding numbers is useful for describing the world.
2389460	2391340	I count things.
2391340	2393980	That's useful to live in our universe.
2393980	2397660	It's useful to count things, to measure things.
2397660	2400260	Addition is useful.
2400260	2403060	And it's really one of the most useful things.
2403060	2406460	So that is why we are familiar with it.
2406460	2409700	And I would argue that's why we think it's simple.
2409700	2417820	But the simplicity we have often argued is if it's simple,
2417820	2420820	it's more likely to be useful.
2420820	2424620	I think that is actually not a statement about simplicity.
2424620	2428580	It's actually a statement that if something
2428580	2432140	is useful for problems like A, B, and C,
2432140	2436660	then it seems it will also be useful for another problem.
2436660	2438620	The world is compositional.
2438660	2441780	If I have a model that works for this set of problems,
2441780	2444300	it's probably also going to work for this one.
2444300	2447220	So that's the argument I would like to make.
2447220	2451580	So when we interpret these models,
2451580	2458020	I think it's important to keep this in mind and really probe
2458020	2462340	what is simple, what is interpretable.
2462340	2469620	So I think this is really exciting for polymathic EI
2469620	2475020	because these models that are trained on many, many systems,
2475020	2480100	they will find broadly useful algorithms.
2480100	2483380	They'll have these neurons that share calculations
2483380	2485620	across many different disciplines.
2485620	2490660	So you could argue that that is the utility.
2490660	2494420	And I mean, maybe we'll discover new kind of operators
2494420	2497060	and be familiar with those, and we'll
2497060	2498460	start calling those simple.
2498460	2502980	So it's not necessarily that all of the things
2502980	2506620	we discover in machine learning will be simple.
2506620	2509540	It's kind of that, by definition,
2509540	2513860	the polymathic EI models will be broadly useful.
2513860	2516700	And if we know they're broadly useful,
2516700	2518980	we might get familiar with those.
2518980	2523860	And that might kind of drive the simplicity of them.
2523860	2526860	So that's my note in simplicity.
2526860	2529940	And so the takeaways here are that I
2529940	2534660	think interpreting a neural network trained on some data
2534660	2538820	sets offers new ways of discovering
2538820	2541220	scientific insights from that data.
2541220	2544580	And I think foundation models like polymathic EI,
2544580	2546580	I think that is a very exciting way
2546580	2550820	of discovering new broadly applicable scientific models.
2550820	2553540	So I'm really excited about this direction.
2553540	2556340	And thank you for listening to me today.
2556340	2556840	OK.
2556840	2557340	Thank you.
2557340	2557840	Thank you.
2557840	2558340	Thank you.
2558340	2558840	Thank you.
2558840	2559340	Thank you.
2559340	2559840	Thank you.
2559840	2560340	Thank you.
2560340	2560840	Thank you.
2560840	2561340	Thank you.
2561340	2561840	Thank you.
2569340	2572820	Thank you for the question, which was great.
2572820	2575300	So three short questions.
2575300	2575800	One.
2575800	2580560	What was the cost of running polymathic AI
2580560	2582040	in this kind of a way?
2582040	2583520	And how to cost it, right?
2583520	2584020	Yeah.
2587520	2588020	Two.
2588020	2592520	When the story built out, it really destroyed you.
2592520	2593840	I got to help you.
2593840	2596520	Yeah.
2596520	2598960	Please use your C mic.
2598960	2599960	Right in front of you.
2599960	2600960	We'll put it back.
2603960	2604460	Yeah.
2605460	2606460	I'll call them.
2606460	2608460	And three.
2608460	2610460	You're putting your back.
2610460	2614460	Thank you guys for that.
2614460	2618260	I was trying to answer three.
2618260	2620980	OK, so I'll try to compartmentalize those.
2620980	2626220	OK, so the first question was the scale of training.
2626220	2629060	This is really an open research question.
2629060	2632740	We don't have the scaling law for science yet.
2632740	2634740	We have scaling laws for language.
2634740	2636500	We know that if you have this many GPUs,
2636500	2638300	you have this size data set, this
2638300	2639860	is going to be your performance.
2639860	2641580	We don't have that yet for science
2641580	2644820	because nobody's built this scale of model.
2644820	2647660	So that's something we're looking at right now,
2647660	2650380	is what is the trade-off of scale?
2650380	2653740	And if I want to train this model on many, many GPUs,
2653740	2656140	is it worth it?
2656140	2658660	So that's an open research question.
2658660	2661620	I do think it'll be large.
2661620	2668340	Probably order hundreds of GPUs trained for maybe
2668340	2669740	a couple months.
2669740	2672300	So it's going to be a very large model.
2672300	2676900	That's kind of assuming the scale of language models.
2676900	2680500	Now, the model is going to be free, definitely.
2680500	2683820	We're all very pro-open source.
2683820	2686140	And I think that's really the point,
2686140	2689060	is we want to open source this model so people can download it
2689060	2690100	and use it in science.
2690100	2695340	I think that's really the most exciting part about this.
2695340	2697780	And then I guess the third question you had
2697780	2707060	was about the future and how it changes how we teach.
2707060	2710860	I mean, I guess are you asking about teaching science
2710860	2712380	or teaching machine learning?
2712380	2713580	Teaching science.
2713580	2716140	I see.
2716140	2718100	I mean, yeah, I mean, I don't know.
2718100	2720100	It depends if it works.
2720100	2722460	I think if it works, it might very well
2722460	2726180	change how science is taught.
2726180	2731700	Yeah, I mean, so I don't know the impact of language models
2731700	2733260	on computational linguistics.
2733260	2734860	I'm assuming they've had a big impact.
2734860	2739460	I don't know if that's affected the teaching of it yet.
2739460	2743460	But if scientific foundation models had a similar impact,
2743460	2745740	I'm sure it would impact.
2745740	2746500	I don't know how much.
2746500	2748900	Probably depends on the success of the models.
2755540	2758180	I have a question about your foundation models also.
2758180	2759980	So in different branches of science,
2759980	2761500	the data sets are pretty different.
2761500	2763340	In molecular biology or genetics,
2763340	2765340	the data sets is a sequence of DNA
2765340	2768460	versus astrophysics where it's images of stars.
2768460	2771980	So how do you plan to use the same model
2771980	2775820	for different form of data sets, input data sets?
2775820	2778660	So you mean how to pose the objective?
2778660	2779300	Yes.
2779300	2783340	So I think the most general objective
2783340	2785740	is self-supervised learning where you basically
2785740	2789860	mask parts of the data and you predict the missing part.
2789860	2793220	If you can optimize that problem,
2793220	2794820	then you can solve tons of different ones.
2794820	2797740	You can do regression, predict parameters,
2797740	2801220	or go the other way and predict rollouts of the model.
2801260	2805300	It's a really general problem to mask data
2805300	2806820	and then fill it back in.
2806820	2811620	That kind of is a superset of many different prediction
2811620	2813060	problems.
2813060	2816500	And I think that's why language models are so broadly useful,
2816500	2819500	even though they're trained just on next-word prediction
2819500	2822620	or like BERT is a masked model.
2827780	2828660	Thanks.
2828660	2829980	Can you hear me?
2830020	2832540	All right, so that was a great talk.
2832540	2833780	I'm Victor.
2833780	2837620	So I'm actually a little bit worried
2837620	2840260	and this is a little bit of a question.
2840260	2843700	Whenever you have models like this,
2843700	2846460	you say that you train these on many examples, right?
2846460	2849220	So imagine you have already embedded
2849220	2850940	the laws of physics here somehow,
2850940	2852980	like let's say the law of repetition.
2852980	2856060	But when you think about discovering new physics,
2856060	2858940	we always have this question of whether we are actually
2858980	2862980	reinventing the wheel or like the network is kind of really
2862980	2866300	giving us something new or is it something giving us
2866300	2869020	or is it giving us something that it learned
2869020	2870340	but it's kind of wrong.
2870340	2875100	So sometimes we have the answer to know which one is which.
2875100	2877340	But if you don't have that, let's say for instance,
2877340	2878980	you're trying to discover what dark matter is,
2878980	2881620	which is something I'm working on.
2881620	2884260	How would you know that the network is actually giving you
2884260	2887700	something new and not just trying to set this
2887700	2890180	into one of the many parameters it has?
2891380	2896380	So, okay, so if you wanna test the model
2897100	2899380	by letting it rediscover something,
2899380	2901140	then I don't think you should use this.
2901140	2903340	I think you should use the scratch model,
2903340	2905460	like from scratch and train it.
2905460	2907580	Because if you use a pre-train model,
2907580	2910060	it's probably already seen that physics.
2910060	2912500	So it's biased towards it in some ways.
2912500	2913900	So if you're rediscovering something,
2913900	2915380	I don't think you should use this.
2915380	2917180	If you're discovering something new,
2918460	2922060	I do think this is more useful.
2922060	2927060	So I think a misconception of,
2928860	2930100	I think machine learning in general
2930100	2932460	is that scientists view machine learning
2933420	2934780	for uninitialized models,
2934780	2938660	like randomly initialized weights as a neutral prior.
2938660	2943660	But it's not, it's a very explicit prior.
2944660	2946740	And it happens to be a bad prior.
2947940	2952940	So if you train from a randomly initialized model,
2952940	2956660	it's kind of always gonna be a worse prior
2956660	2958020	than training from a pre-train model,
2958020	2960540	which has seen many different types of physics.
2960540	2963060	I think we can kind of make that statement.
2964740	2966740	So if you're trying to discover new physics,
2966740	2971740	I mean, like if you train it on some data set,
2972740	2974140	I guess you can always verify
2974140	2976500	that the predictions are accurate.
2976500	2980580	So that would be, I guess, one way to verify it.
2981900	2984220	But I do think like the fine tuning here,
2984220	2987300	so like taking this model and training it on the task,
2987300	2989300	I think that's very important.
2989300	2992940	I think in language models, it's not as emphasized.
2992940	2994820	Like people will just take a language model
2994820	2998100	and tweak the prompt to get a better result.
2998300	3002980	I think for science, I think the prompt is,
3002980	3004980	I mean, I think like the equivalent of the prompt
3004980	3006700	would be important, but I think the fine tuning
3006700	3007820	is much more important,
3007820	3010780	because our data sets are so much different across science.
3013780	3015780	Chris, in the back, thank you.
3017660	3019100	In my course, the main course,
3019100	3021700	all the students, I don't think there's one.
3021700	3023540	Please note, Steph, the symbolic.
3024540	3026860	It's still limited in the complexity
3026860	3029580	and dimensionality of the system.
3029580	3032180	So are you, based on your saying,
3032180	3035940	also the fine tuning and transfer learning
3035940	3040940	as a way of enhancing the symbolic regression?
3040940	3042940	And what does that mean?
3045220	3048300	Yeah, so the symbolic regression,
3048300	3051140	I mean, I would consider that it's not used
3051140	3054100	inside the foundation model part.
3054100	3058540	I think it's interesting to interpret the foundation model
3058540	3062860	and see if there's kind of more general physical frameworks
3062860	3063980	that it comes up with.
3066860	3069660	I think, yeah, symbolic regression is very limited
3069660	3072820	in that it's bad at high dimensional problems.
3072820	3077820	I think that might be because of the choice of operators.
3078820	3082900	I think if you can consider maybe high dimensional operators,
3082900	3085140	you might be a bit better off.
3085140	3086420	I mean, symbolic regression,
3086420	3089580	it's an active area of research,
3089580	3092620	and I think the hardest, the biggest hurdle right now
3092620	3097620	is it's not good at finding very complex symbolic models.
3097620	3115620	So I guess you could, it depends on the dimensionality
3115620	3116620	of the data.
3116620	3120100	I guess if it's very high dimensional data,
3120100	3126500	you're always kind of, symbolic regression is not good
3126500	3128780	at high dimensional data unless you can have
3128780	3133180	kind of some operators that aggregate
3133180	3135580	to lower dimensional spaces.
3138380	3142100	Yeah, I don't know if I'm answering your question.
3142100	3143100	OK.
3143100	3144780	I wanted to ask a little bit.
3144780	3148780	So when you were showing the construction of these trees
3148780	3151540	of each generation and the different operators,
3151540	3153380	I think this is related to kind of general themes
3153380	3154660	at the top and other questions.
3154660	3157100	But often in doing science, when you're writing it,
3157100	3159780	you're presented with kind of an algorithmic self-prompt.
3159780	3162020	It's like, you know, diagonalize the Hamiltonian
3162020	3164020	or something like that.
3164020	3166140	How do you calculate that aspect of doing science
3166140	3168820	that is kind of the algorithmic side of solving
3168820	3172260	the problem rather than just going up and down?
3172260	3172780	Right.
3172780	3175420	These, do you see my?
3175420	3176540	Yeah.
3176540	3178940	Yeah, so the question was about how
3178940	3183100	do you incorporate kind of more general, not
3183180	3185820	analytic operators, but kind of more general algorithms
3185820	3188500	like a Hamiltonian operator.
3188500	3191460	I think that, I mean, like in principle,
3191460	3195100	symbolic regression is it's part of a larger family
3195100	3197900	of an algorithm called program synthesis,
3197900	3201220	where the objective is to find a program, you know,
3201220	3206140	like code that describes a given data set, for example.
3206140	3210100	So if you can write your operators
3210100	3212660	into your symbolic regression approach
3212780	3214980	and your symbolic regression approach
3214980	3219300	has that ground truth model in there somewhere,
3219300	3220780	then I think it's totally possible.
3220780	3224900	I think, like, it's harder to do.
3224900	3227060	I think, like, even symbolic regression with scalars
3227060	3233580	is, it's fairly difficult to actually set up an algorithm.
3233580	3234780	I think, I don't know, I think it's really
3234780	3235860	like an engineering problem.
3235860	3242020	But the conceptual part is totally like there for this.
3242020	3243020	Yeah.
3247500	3248620	Thanks.
3248620	3249140	Oh, sorry.
3253540	3257500	This claim that random initial weights are always bad
3257500	3258820	or pre-training is always good.
3258820	3260300	I don't know if they're always bad,
3260300	3265460	but it seems like from our experiments,
3265460	3269980	we've never seen a case where pre-training
3269980	3272180	on some kind of physical data hurts.
3272180	3274340	Like the cap video is an example.
3274340	3275980	We thought that would hurt the model.
3275980	3276940	It didn't.
3276940	3279260	That is a cute example.
3279260	3282100	I'm sure there's cases where some pre-training hurts.
3282100	3283620	Yeah, so that's essentially my question.
3283620	3285540	So we're aware of, like, adversarial examples.
3285540	3287540	For example, you train on MNIST, add a bit of noise.
3287540	3289860	It does terrible compared to what a human would do.
3289860	3293220	What do you think adversarial examples look like in science?
3293220	3294860	Yeah, I mean, I don't know what those are,
3294860	3297660	but I'm sure they exist somewhere,
3297700	3300780	where pre-training on certain data types kind of
3300780	3303500	messes with training a bit.
3303500	3307180	We don't know those yet, but yeah, it'll be interesting.
3307180	3309180	Do you think it's a pick fall, though, of the approach?
3309180	3312140	Because I have a model of the sun and a model of DNA.
3314580	3318620	Yeah, I mean, I don't know.
3318620	3321180	I guess we'll see.
3321180	3323260	Yeah, it's hard to know.
3323260	3327420	I guess from language, we've seen you can pre-train
3327420	3329820	like a language model on video data,
3329820	3332860	and it helps the language, which is really weird.
3332860	3335940	But it does seem like if there's any kind of concepts,
3335940	3338260	it does, if it's flexible enough,
3338260	3341100	it can kind of transfer those in some ways.
3341100	3341940	So we'll see.
3341940	3345860	I mean, presumably, we'll find some adversarial examples there.
3345860	3347140	So far, we haven't.
3347140	3349900	We thought the cat was one, but it wasn't.
3349900	3350740	It helped.
