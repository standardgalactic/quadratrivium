1
00:00:00,000 --> 00:00:19,440
So I'm very excited today to talk to you about this idea of interpreting neural networks

2
00:00:19,440 --> 00:00:26,960
to get physical insight, which I view as kind of a new, really kind of a new paradigm of

3
00:00:26,960 --> 00:00:29,440
doing science.

4
00:00:29,440 --> 00:00:32,600
So this is a work with a huge number of people.

5
00:00:32,600 --> 00:00:38,000
I can't individually mention them all, but many of them are here at the Flutter Institute.

6
00:00:38,000 --> 00:00:39,960
So I'm going to split this up.

7
00:00:39,960 --> 00:00:41,200
I'm going to do two parts.

8
00:00:41,200 --> 00:00:46,400
The first one, I'm going to talk about kind of how we go from a neural network to insights,

9
00:00:46,400 --> 00:00:49,160
how we should get insights out of a neural network.

10
00:00:49,160 --> 00:00:55,560
The second part, I'm going to talk about this polymathic AI thing, which is about basically

11
00:00:55,560 --> 00:00:59,800
building massive neural networks for science.

12
00:00:59,800 --> 00:01:09,840
So my motivation for this line of work is examples like the following.

13
00:01:09,840 --> 00:01:17,800
So there was this paper led by Kimberly Stakkenfeld at DeepMind a couple years ago on learning

14
00:01:17,800 --> 00:01:23,080
fast subgrid models for fluid turbulence.

15
00:01:23,080 --> 00:01:25,200
So what you see here is the ground truth.

16
00:01:25,200 --> 00:01:28,640
So this is kind of some box of a fluid.

17
00:01:28,640 --> 00:01:36,440
The bottom row is the learned kind of subgrid model, essentially, for this simulation.

18
00:01:36,440 --> 00:01:45,960
The really interesting thing about this is that this model was only trained on 16 simulations,

19
00:01:45,960 --> 00:01:52,160
but it actually learned to be more accurate than all traditional subgrid models at that

20
00:01:52,160 --> 00:01:55,000
resolution for fluid dynamics.

21
00:01:55,000 --> 00:02:01,560
So I think it's really exciting kind of to figure out how did the model do that and kind

22
00:02:01,560 --> 00:02:08,440
of what can we learn about science from this neural network.

23
00:02:08,440 --> 00:02:14,680
Another example is, so this is a work that I worked on with Dan Twio and others on predicting

24
00:02:14,680 --> 00:02:17,000
instability in planetary systems.

25
00:02:17,000 --> 00:02:20,160
So this is a centuries old problem.

26
00:02:20,160 --> 00:02:25,040
You have some, you know, this compact planetary system and you want to figure out when does

27
00:02:25,040 --> 00:02:27,680
it go unstable.

28
00:02:27,680 --> 00:02:33,240
There are literally, I mean, people have literally worked on this for centuries.

29
00:02:33,240 --> 00:02:38,720
It's a fundamental problem in chaos, but this neural network trained on, I think it was

30
00:02:38,720 --> 00:02:43,080
maybe 20,000 simulations.

31
00:02:43,080 --> 00:02:47,480
It's not only more accurate at predicting instability, but it also seems to generalize

32
00:02:47,480 --> 00:02:51,080
better to kind of different types of systems.

33
00:02:51,080 --> 00:02:57,200
So it's really interesting to think about, okay, these neural networks, they've seemed

34
00:02:57,200 --> 00:02:59,600
to have learned something new.

35
00:02:59,600 --> 00:03:03,480
How can we actually use that to advance our own understanding?

36
00:03:03,480 --> 00:03:06,600
So that's my motivation here.

37
00:03:06,600 --> 00:03:11,300
So the traditional approach to science has been kind of, you have some low dimensional

38
00:03:11,300 --> 00:03:18,260
data set or some kind of summary statistic and you build theories to describe that low

39
00:03:18,260 --> 00:03:23,760
dimensional data, which might be kind of a summary statistic.

40
00:03:23,760 --> 00:03:28,500
So you can look throughout the history of science, so maybe Kepler's law is an empirical

41
00:03:28,500 --> 00:03:35,340
fit to data and then of course Newton's law of gravitation was required to explain this.

42
00:03:35,340 --> 00:03:36,740
Another example is like Plank's law.

43
00:03:36,740 --> 00:03:44,340
So this was actually an empirical fit to data and quantum mechanics was required, partially

44
00:03:44,340 --> 00:03:47,340
motivated by this to explain it.

45
00:03:47,340 --> 00:03:57,420
So this is kind of the normal approach to building theories.

46
00:03:57,420 --> 00:04:02,980
And of course some of these, they've kind of, I mean it's not only this, it also involves

47
00:04:02,980 --> 00:04:03,980
many other things.

48
00:04:03,980 --> 00:04:12,060
But I think it's really exciting to think about how we can involve interpretation of

49
00:04:12,060 --> 00:04:16,020
data driven models in this process, very generally.

50
00:04:16,020 --> 00:04:19,140
So that's what I'm going to talk about today.

51
00:04:19,140 --> 00:04:26,460
I'm going to conjecture that in this era of AI, where we have these massive neural networks

52
00:04:26,460 --> 00:04:33,060
that kind of seem to outperform all of our traditional theory, we might want to consider

53
00:04:33,060 --> 00:04:40,620
this approach where we use a neural network as essentially compression tool or some kind

54
00:04:40,620 --> 00:04:49,620
of tool that pulls apart common patterns in a data set.

55
00:04:49,620 --> 00:04:54,100
And we build theories not to describe the data directly, but really kind of to describe

56
00:04:54,100 --> 00:04:57,620
the neural network and what the neural network has learned.

57
00:04:57,620 --> 00:05:03,340
So I think this is kind of an exciting new approach to, I mean really science in general,

58
00:05:03,340 --> 00:05:06,420
I think especially the physical sciences.

59
00:05:06,420 --> 00:05:13,300
So the key point here is neural networks trained on massive amounts of data with very flexible

60
00:05:13,300 --> 00:05:19,780
functions, they seem to find new things that are not in our existing theory.

61
00:05:19,780 --> 00:05:23,780
So I showed you the example with turbulence, you know we can find better sub-grid models

62
00:05:23,780 --> 00:05:29,500
just from data, and we can also do this with planetary dynamics.

63
00:05:29,500 --> 00:05:36,300
So I think our challenge as scientists for those problems is distilling those insights

64
00:05:36,300 --> 00:05:39,580
into our language, kind of incorporating it in our theory.

65
00:05:39,580 --> 00:05:46,220
I think this is a really exciting way to kind of look at these models.

66
00:05:46,220 --> 00:05:49,300
So I'm going to break this down a bit.

67
00:05:49,300 --> 00:05:54,700
The first thing I would like to do is just go through kind of what machine learning is,

68
00:05:54,700 --> 00:06:01,700
how it works, and then talk about this, kind of how you apply them to different data sets.

69
00:06:01,700 --> 00:06:12,260
Okay so just going back to the very fundamentals, linear regression in 1D, this is, I would

70
00:06:12,260 --> 00:06:19,700
argue if you don't really have physical meaning to these parameters yet, it is a kind of type

71
00:06:19,700 --> 00:06:22,260
of machine learning.

72
00:06:22,260 --> 00:06:28,020
And so this is, these are scalars, right, X and Y, those are scalars, FI0, FI1, scalar

73
00:06:28,020 --> 00:06:31,860
parameters, linear model.

74
00:06:31,860 --> 00:06:36,380
You go one step beyond that, and you get this shallow network.

75
00:06:36,380 --> 00:06:46,940
So again this has 1D input, X, 1D output, Y, but now we've introduced this layer.

76
00:06:46,940 --> 00:06:55,260
So we have these linear models, so we have three hidden neurons here, and they pass

77
00:06:55,260 --> 00:07:00,100
through this function A, so this is called an activation function.

78
00:07:00,100 --> 00:07:07,900
And what this does is it gives the model a way of including some non-linearity.

79
00:07:07,900 --> 00:07:11,860
So these are called activation functions.

80
00:07:11,860 --> 00:07:19,300
The one that most people would reach for first is the rectified linear unit, or RELU.

81
00:07:19,300 --> 00:07:25,220
Essentially what this does is it says if the input is less than zero, drop it at zero,

82
00:07:25,220 --> 00:07:27,980
greater than zero, leave it.

83
00:07:27,980 --> 00:07:34,780
This is a very simple way of adding some kind of non-linearity to my flexible curve that

84
00:07:34,780 --> 00:07:41,060
I'm going to fit to my data, right?

85
00:07:41,060 --> 00:07:47,380
The next thing I do is I have these different activation functions.

86
00:07:47,380 --> 00:07:55,100
They have this kind of joint here at different points, which depends on the parameters.

87
00:07:55,100 --> 00:08:01,540
And I'm going to multiply the outputs of these activations by a number.

88
00:08:01,540 --> 00:08:08,380
That's kind of the output of my kind of a layer of the neural network.

89
00:08:08,380 --> 00:08:13,740
And this is going to maybe change the direction of it, change the slope of it.

90
00:08:13,740 --> 00:08:15,740
The next thing I'm going to do is I'm going to sum these up.

91
00:08:15,740 --> 00:08:22,120
I'm going to superimpose them, and I get this is the output of one layer in my network.

92
00:08:22,120 --> 00:08:24,500
So this is a shallow network.

93
00:08:24,500 --> 00:08:29,260
Basically what it is, it's a piecewise linear model.

94
00:08:29,260 --> 00:08:35,660
And the joints here, the parts where it kind of switches from one linear region to another,

95
00:08:35,660 --> 00:08:40,500
those are determined by the inputs to the first layer's activations.

96
00:08:40,500 --> 00:08:43,940
So it's basically a piecewise linear model.

97
00:08:43,940 --> 00:08:47,780
It's a piecewise linear model.

98
00:08:47,780 --> 00:08:56,420
And the one cool thing about it is you can use this piecewise linear model to approximate

99
00:08:56,420 --> 00:08:59,660
any 1D function to arbitrary accuracy.

100
00:08:59,660 --> 00:09:05,300
So if I want to model this function with five joints, I can get an approximation like this

101
00:09:05,300 --> 00:09:11,340
with 10 joints like this, 20 like that, and I can just keep increasing the number of these

102
00:09:11,340 --> 00:09:12,340
neurons.

103
00:09:12,340 --> 00:09:16,540
And that gives me better and better approximations.

104
00:09:16,540 --> 00:09:20,500
So this is called the universal approximation theorem.

105
00:09:20,500 --> 00:09:29,020
So it's that my shallow neural network just has one kind of layer of activations.

106
00:09:29,020 --> 00:09:33,780
I can describe any continuous function to arbitrary precision.

107
00:09:33,780 --> 00:09:41,220
Now that's not, I mean this alone is not that exciting because I can do that with polynomials.

108
00:09:41,220 --> 00:09:44,820
I don't need, the neural network's not the only thing that does that.

109
00:09:44,820 --> 00:09:50,020
I think the exciting part about neural networks is when you start making them deeper.

110
00:09:50,020 --> 00:09:54,020
So first let's look at what if we had two inputs?

111
00:09:54,020 --> 00:09:55,020
What would it look like?

112
00:09:55,020 --> 00:09:56,820
We had two inputs.

113
00:09:56,820 --> 00:10:04,580
Now these activations, they are activated along planes, not points, they're activated

114
00:10:04,580 --> 00:10:06,100
along planes.

115
00:10:06,100 --> 00:10:15,420
So for, this is my, maybe my input plane, I'm basically chopping it along the zero part

116
00:10:15,420 --> 00:10:19,700
and now I have these 2D planes in space.

117
00:10:19,700 --> 00:10:24,740
And the next thing I'm going to do, I'm going to scale these and then I'm going to superimpose

118
00:10:24,740 --> 00:10:26,300
them.

119
00:10:26,300 --> 00:10:34,620
And this gives me ways of representing kind of arbitrary functions in now a 2D space rather

120
00:10:34,620 --> 00:10:35,740
than just a 1D space.

121
00:10:35,740 --> 00:10:44,900
So it gives me a way of expressing arbitrary continuous functions.

122
00:10:44,900 --> 00:10:54,820
Now the cool part, oops, the cool part here is when I want to do two layers.

123
00:10:54,820 --> 00:11:00,700
So now I have two layers, so I have this, this is my first neural network, this is my

124
00:11:00,700 --> 00:11:05,620
second neural network and my first neural network looks like this.

125
00:11:05,620 --> 00:11:08,660
If I consider it alone, it looks like this.

126
00:11:08,660 --> 00:11:12,820
My second neural network, it looks like this.

127
00:11:12,820 --> 00:11:17,500
If I just like, I cut this neural network out, it looks like this, okay.

128
00:11:17,500 --> 00:11:26,820
When I compose them together, I get this, this, this shared kind of behavior where,

129
00:11:26,820 --> 00:11:33,580
so I'm composing these functions together and essentially what happens is, it's almost

130
00:11:33,580 --> 00:11:41,100
like you fold the functions together so that I experience that function in this linear

131
00:11:41,100 --> 00:11:44,140
region and kind of backwards and then again.

132
00:11:44,140 --> 00:11:48,620
So you can see there's, there's kind of like that function is mirrored here, right?

133
00:11:48,620 --> 00:11:51,780
It goes, goes back and forth.

134
00:11:51,780 --> 00:11:56,660
So you can make this analogy to folding a piece of paper.

135
00:11:56,660 --> 00:12:02,300
So if I consider my first neural network like this on a piece of paper, I could essentially

136
00:12:02,300 --> 00:12:09,420
fold it, draw my second neural network, the function over that, that first one and then

137
00:12:09,420 --> 00:12:15,100
expand it and essentially now I have this, this function.

138
00:12:15,100 --> 00:12:23,340
So the, the cool part about this is that I'm sharing, I'm kind of sharing computation because

139
00:12:23,340 --> 00:12:27,820
I'm sharing neurons in my neural network.

140
00:12:27,820 --> 00:12:32,940
So this is going to come up again, this is kind of a theme, we're, we're doing efficient

141
00:12:32,940 --> 00:12:38,460
computation in neural networks by sharing neurons and it's, it's useful to think about

142
00:12:38,460 --> 00:12:44,700
it in this, this, this way, kind of folding paper, drawing curves over it and expanding

143
00:12:44,700 --> 00:12:45,700
it.

144
00:12:45,700 --> 00:12:50,500
Okay, so let's go back to the physics.

145
00:12:50,500 --> 00:12:57,540
Now neural networks, right, they're efficient universal function approximators.

146
00:12:57,540 --> 00:13:02,180
You can think of them as kind of like a type of data compression.

147
00:13:02,180 --> 00:13:10,180
The same neurons can be used for different calculations in the same network and a common

148
00:13:10,180 --> 00:13:18,500
use case in, in physical sciences, especially what I work on, is emulating physical processes.

149
00:13:18,500 --> 00:13:22,980
So if I have some, my, my simulator is kind of too expensive or I have like real world

150
00:13:22,980 --> 00:13:26,660
data and my simulator is not good at describing it.

151
00:13:26,660 --> 00:13:31,740
I can build a neural network that maybe emulates it.

152
00:13:31,740 --> 00:13:36,260
So like I have a neural network that looks at kind of the initial conditions in this

153
00:13:36,260 --> 00:13:40,500
model and it predicts when it's going to go unstable.

154
00:13:40,500 --> 00:13:44,500
So this is a, this is a good use case for them.

155
00:13:44,500 --> 00:13:52,020
And once I have that, so maybe I have this, I have this trained piecewise linear model

156
00:13:52,020 --> 00:13:55,500
that kind of emulates some physical process.

157
00:13:55,500 --> 00:14:00,740
Now how do I take that and go to interpret it?

158
00:14:00,740 --> 00:14:04,300
How do I actually get insight out of it?

159
00:14:04,300 --> 00:14:08,580
So this is where I'm going to talk about symbolic regression.

160
00:14:08,580 --> 00:14:11,460
So this is one of my favorite things.

161
00:14:11,460 --> 00:14:18,500
So a lot of the interpretability work in industry, especially like computer vision language,

162
00:14:18,500 --> 00:14:21,940
there's not really like, there's not a good modeling language.

163
00:14:21,940 --> 00:14:27,180
Like if I have a, if I have a model that classifies cats and dogs, there's not really like, there's

164
00:14:27,180 --> 00:14:31,660
not a language for describing every possible cat.

165
00:14:31,660 --> 00:14:33,940
There's not like a mathematical framework for that.

166
00:14:33,940 --> 00:14:35,060
But in science, we do have that.

167
00:14:35,060 --> 00:14:46,060
We do have, oops, we do have a very good mathematical framework.

168
00:14:46,060 --> 00:14:53,220
Let me see if this works.

169
00:14:53,220 --> 00:14:59,180
So in science, right, so we have this, you know, in science we have this very good understanding

170
00:14:59,180 --> 00:15:05,540
of the universe and we have this language for it.

171
00:15:05,540 --> 00:15:10,660
We have mathematics, which describes the universe very well.

172
00:15:10,660 --> 00:15:17,100
And I think when we want to interpret these data-driven models, we should use this language

173
00:15:17,100 --> 00:15:20,940
because that will give us results that are interpretable.

174
00:15:20,940 --> 00:15:25,940
If I have some piecewise linear model with different, you know, like millions of parameters,

175
00:15:25,940 --> 00:15:28,020
it's not, it's not really useful for me, right?

176
00:15:28,020 --> 00:15:33,820
I want to, I want to express it in the language that I'm familiar with, which is mathematics.

177
00:15:34,820 --> 00:15:40,900
So you can look at like any cheat sheet and it's a lot of, you know, simple algebra.

178
00:15:40,900 --> 00:15:43,860
This is the language of science.

179
00:15:43,860 --> 00:15:51,420
So symbolic regression is a machine learning task where the objective is to find analytic

180
00:15:51,420 --> 00:15:55,980
expressions that optimize some objective.

181
00:15:55,980 --> 00:16:00,580
So maybe I, maybe I want to fit that data set.

182
00:16:00,580 --> 00:16:06,140
And what I could do is basically try different trees.

183
00:16:06,140 --> 00:16:09,300
So these are like expression trees, right?

184
00:16:09,300 --> 00:16:11,300
So this equation is that tree.

185
00:16:11,300 --> 00:16:17,620
I basically find different expression trees that match that data.

186
00:16:17,620 --> 00:16:23,740
So the point of symbolic regression, I want to find equations that fit the data set.

187
00:16:23,740 --> 00:16:31,580
So the symbolic and the parameters rather than just optimizing parameters in some model.

188
00:16:31,580 --> 00:16:37,540
So the, the, the current way to do this, the, the state of the art way is a genetic algorithm.

189
00:16:37,540 --> 00:16:43,300
So it's, it's kind of, it's not really like a clever algorithm.

190
00:16:43,300 --> 00:16:46,860
It's, it's a, I can say that because I work on it.

191
00:16:46,860 --> 00:16:50,580
It's a, it's, it's pretty close to brute force.

192
00:16:50,580 --> 00:16:58,180
Basically what you do is you treat your equation like a DNA sequence and you basically evolve

193
00:16:58,180 --> 00:16:59,180
it.

194
00:16:59,180 --> 00:17:05,900
So you do like mutations, you swap one operator to another, maybe, maybe you cross-breed them.

195
00:17:05,900 --> 00:17:08,540
So you have like two expressions which are okay.

196
00:17:08,540 --> 00:17:10,540
You literally breed those together.

197
00:17:10,540 --> 00:17:16,060
I mean, not literally, but you conceptually breed those together, get a new expression

198
00:17:16,060 --> 00:17:21,300
until you fit the data set.

199
00:17:21,300 --> 00:17:28,420
So yeah, so this is a genetic algorithm based search for symbolic regression.

200
00:17:28,420 --> 00:17:38,060
Now the, the point of this is to find simple models in our language of mathematics that

201
00:17:38,060 --> 00:17:42,140
describe a given data set.

202
00:17:42,140 --> 00:17:46,220
So, so I've spent a lot of time working on these frameworks.

203
00:17:46,220 --> 00:17:53,180
So pyser, symbolic regression.jl, they, they work like this.

204
00:17:53,180 --> 00:17:57,500
So if I have this expression, I want to model that data set, essentially what I'm going

205
00:17:57,500 --> 00:18:06,340
to do is just search over all possible expressions until I find one that gets me closer to this

206
00:18:06,340 --> 00:18:07,780
ground truth expression.

207
00:18:08,460 --> 00:18:12,780
You see it's kind of testing different, different branches in evolutionary space.

208
00:18:12,780 --> 00:18:20,780
I'm going to play that again until it reaches this ground truth data set.

209
00:18:20,780 --> 00:18:24,820
So this is, this is pretty close to how it works.

210
00:18:24,820 --> 00:18:36,380
You're essentially finding simple expressions that fit some data set accurately.

211
00:18:36,380 --> 00:18:45,700
So, what I'm going to show you how to do is this symbolic regression idea is about

212
00:18:45,700 --> 00:18:53,420
fitting, kind of finding models, symbolic models that I can use to describe a data set.

213
00:18:53,420 --> 00:19:00,700
I want to use that to build surrogate models of my neural network.

214
00:19:00,700 --> 00:19:07,020
So this is, this is kind of a way of translating my model into my language.

215
00:19:07,020 --> 00:19:12,940
You could, you could also think of it as like polynomial or like a Taylor expansion in some

216
00:19:12,940 --> 00:19:15,140
ways.

217
00:19:15,140 --> 00:19:17,860
The way this works is as follows.

218
00:19:17,860 --> 00:19:23,940
If I have some neural network that I've trained on my data set, whatever, I'm going to train

219
00:19:23,940 --> 00:19:27,220
it normally, freeze the parameters.

220
00:19:27,220 --> 00:19:31,460
Then what I do is I record the inputs and outputs.

221
00:19:31,460 --> 00:19:33,660
I kind of treat it like a data generating process.

222
00:19:33,660 --> 00:19:39,300
I, I try to see like, okay, what's the behavior for this input, this input, and so on.

223
00:19:39,300 --> 00:19:45,940
Then I stick those inputs and outputs into pyser, for example, and I, I find some equation

224
00:19:45,940 --> 00:19:52,380
that models that neural network, or maybe it's like a piece of my neural network.

225
00:19:52,380 --> 00:19:58,340
So this is a, this is building a surrogate model for my neural network that is kind of

226
00:19:58,340 --> 00:20:00,340
approximates the same behavior.

227
00:20:00,340 --> 00:20:04,820
Now you wouldn't just do this for like a standalone neural network.

228
00:20:04,820 --> 00:20:10,540
This, this would typically be part of like a larger model, and it would give you a way

229
00:20:10,540 --> 00:20:16,420
of interpreting exactly what it's doing for different inputs.

230
00:20:16,420 --> 00:20:22,820
So what I might have is maybe I have like two, two pieces, like two neural networks

231
00:20:22,820 --> 00:20:24,420
here.

232
00:20:24,420 --> 00:20:28,780
Maybe I think the first neural network is like learning features, or it's learning

233
00:20:28,780 --> 00:20:31,140
some kind of coordinate transform.

234
00:20:31,140 --> 00:20:34,620
The second one is doing something in that space.

235
00:20:34,620 --> 00:20:38,500
It's using those features for calculation.

236
00:20:38,500 --> 00:20:44,780
And so I can, using symbolic regression, which we call symbolic distillation, I can, I can

237
00:20:44,780 --> 00:20:48,740
distill this model into equations.

238
00:20:48,740 --> 00:20:52,140
So that's, that's the basic idea of this.

239
00:20:52,140 --> 00:20:59,540
I replace neural networks, so I replace them with my surrogate model, which is now an equation.

240
00:20:59,540 --> 00:21:03,300
You would typically do this for G as well.

241
00:21:03,300 --> 00:21:08,620
And now I have equations that describe my model.

242
00:21:08,620 --> 00:21:14,100
And this is kind of a interpretable approximation of my original neural network.

243
00:21:14,100 --> 00:21:18,300
Now the reason you wouldn't want to do this for like just directly on the data is because

244
00:21:18,300 --> 00:21:20,860
it's a harder search problem.

245
00:21:20,860 --> 00:21:27,300
If you break it into pieces, like kind of interpreting pieces of a neural network, it's easier.

246
00:21:27,300 --> 00:21:32,900
Because you're only searching for two n expressions rather than n squared.

247
00:21:32,900 --> 00:21:34,620
So it's a, it's a bit easier.

248
00:21:34,620 --> 00:21:40,900
And you're kind of using the neural network as a way of factoring, factorizing the system

249
00:21:40,900 --> 00:21:46,560
into different pieces that you then interpret.

250
00:21:46,560 --> 00:21:48,860
So we've, we've used this in, in different papers.

251
00:21:48,860 --> 00:21:59,300
So this is one led by Pablo Lemos on rediscovering Newton's law of gravity from data.

252
00:21:59,300 --> 00:22:05,380
So this was a, this was a cool paper because we didn't tell it the masses of the bodies

253
00:22:05,380 --> 00:22:06,380
in the solar system.

254
00:22:06,380 --> 00:22:13,940
It had to simultaneously find the masses of every, all of these 30 bodies we gave it.

255
00:22:13,940 --> 00:22:15,460
And it also found the law.

256
00:22:15,460 --> 00:22:18,460
So we kind of train this neural network to do this.

257
00:22:18,460 --> 00:22:20,660
And then we interpret that neural network.

258
00:22:20,660 --> 00:22:24,860
And it gives us Newton's law of gravity.

259
00:22:24,860 --> 00:22:26,360
Now that's a rediscovery.

260
00:22:26,360 --> 00:22:28,260
And of course, like we know that.

261
00:22:28,260 --> 00:22:31,420
So I think the discoveries are also cool.

262
00:22:31,420 --> 00:22:32,820
So these are not my papers.

263
00:22:32,820 --> 00:22:33,820
These are other people's papers.

264
00:22:33,820 --> 00:22:35,740
I thought they were really exciting.

265
00:22:35,740 --> 00:22:43,820
So this is one, a recent one by Ben Davis and Jehow Jin, where they discover this new

266
00:22:43,820 --> 00:22:47,700
black hole mass scaling relationship.

267
00:22:47,700 --> 00:22:55,180
So it's, it relates the, I think it's the spirality or something in a galaxy in the

268
00:22:55,180 --> 00:22:57,660
velocity with the mass of a black hole.

269
00:22:57,660 --> 00:23:02,340
So they, they found this with this technique, which is exciting.

270
00:23:02,340 --> 00:23:05,620
And I saw this other cool one recently.

271
00:23:05,620 --> 00:23:12,140
They found this cloud cover model with this technique using Picer.

272
00:23:12,140 --> 00:23:16,820
So they, it kind of gets you this point where it's a, it's a fairly simple model and it's

273
00:23:16,820 --> 00:23:20,380
also pretty accurate.

274
00:23:20,380 --> 00:23:25,380
But again, the, the point of this is to find a model that you can understand, right?

275
00:23:25,380 --> 00:23:30,260
It's not this black box neural network with, with billions of parameters.

276
00:23:30,260 --> 00:23:34,980
It's a simple model that you can have a handle on.

277
00:23:34,980 --> 00:23:36,380
Okay.

278
00:23:36,380 --> 00:23:38,580
So that's part one.

279
00:23:38,580 --> 00:23:43,980
Now part two, I want to talk about polymathic AI.

280
00:23:43,980 --> 00:23:47,020
So this is kind of like the complete opposite end.

281
00:23:47,020 --> 00:23:50,180
We're going to go from small models in the first part.

282
00:23:50,180 --> 00:23:52,820
Now we're going to do the biggest possible models.

283
00:23:52,820 --> 00:23:58,140
And I'm going to also talk about the meaning of simplicity, what it actually means.

284
00:23:58,140 --> 00:24:06,300
So the past few years, you may have noticed there's been this shift in industry, industrial

285
00:24:06,300 --> 00:24:10,460
machine learning to favor foundation models.

286
00:24:10,460 --> 00:24:14,500
So like chat GPT is an example of this.

287
00:24:14,500 --> 00:24:23,660
A foundation model is a machine learning model that serves as the foundation for other models.

288
00:24:23,660 --> 00:24:30,060
These models are trained by basically taking massive amounts of general diverse data and,

289
00:24:30,060 --> 00:24:37,020
and training this flexible model on that data and then fine tuning them to some specific

290
00:24:37,020 --> 00:24:38,220
task.

291
00:24:38,220 --> 00:24:46,100
So you could think of it as maybe teaching this machine learning model English and French

292
00:24:46,100 --> 00:24:50,820
before teaching it to do translation between the two.

293
00:24:50,820 --> 00:24:55,980
So it often gives you better performance on downstream tax.

294
00:24:55,980 --> 00:25:05,980
I mean you can also see that, I mean chat GPT is, I've heard that it's trained on GitHub

295
00:25:05,980 --> 00:25:11,460
and that kind of teaches it to reason a bit better.

296
00:25:11,460 --> 00:25:17,020
And so the, I mean basically these models are trained on massive amounts of data and

297
00:25:17,020 --> 00:25:20,700
they form this idea called a foundation model.

298
00:25:20,700 --> 00:25:25,980
So the general idea is you, you collect, you know, you collect your massive amounts

299
00:25:25,980 --> 00:25:33,180
of data, you have this very flexible model and then you train it on, you might train

300
00:25:33,180 --> 00:25:40,460
it to do self supervised learning which is kind of like you mask parts of the data and

301
00:25:40,460 --> 00:25:43,220
then the model tries to fill it back in.

302
00:25:43,220 --> 00:25:44,900
That's a, that's a common way you train that.

303
00:25:44,900 --> 00:25:51,420
So like for example, GPT style models, those are basically trained on the entire internet

304
00:25:51,420 --> 00:25:54,700
and they're trained to predict the next word.

305
00:25:54,700 --> 00:25:56,140
That's their only task.

306
00:25:56,140 --> 00:26:01,740
You get an input sequence of words, you predict the next one and you just repeat that for

307
00:26:01,740 --> 00:26:04,180
massive amounts of text.

308
00:26:04,180 --> 00:26:12,380
And then just by doing that they get really good at general language understanding.

309
00:26:12,380 --> 00:26:16,180
And they are fine tuned to be a chatbot essentially.

310
00:26:16,180 --> 00:26:21,500
So they're, they're given a little bit of extra data on this is how you talk to someone

311
00:26:21,500 --> 00:26:27,580
and be friendly and so on and, and that's much better than just training a model just

312
00:26:27,580 --> 00:26:28,580
to do that.

313
00:26:28,580 --> 00:26:33,420
So it's this idea of pre-training models.

314
00:26:33,420 --> 00:26:40,100
So I mean once you have this model, I think like kind of the, the, the cool part about

315
00:26:40,100 --> 00:26:49,420
these models is they're really trained in a way that gives them general priors for data.

316
00:26:49,420 --> 00:26:55,140
So if I have like some, maybe I have like some artwork generation model, it's trained

317
00:26:55,140 --> 00:26:59,020
on different images and it kind of generates different art.

318
00:26:59,020 --> 00:27:06,140
I can fine tune this model on like Studio Ghibli artwork and it doesn't need much training

319
00:27:06,140 --> 00:27:09,860
data because it already knows what a face looks like.

320
00:27:09,860 --> 00:27:13,260
Like it's already seen tons of different faces.

321
00:27:13,260 --> 00:27:18,660
So just by fine tuning it on some small number of examples, it can, it can kind of pick up

322
00:27:18,660 --> 00:27:22,020
this task much quicker.

323
00:27:22,020 --> 00:27:24,140
That's essentially the idea.

324
00:27:24,140 --> 00:27:29,300
Now this is, I mean the same thing is true in language, right?

325
00:27:29,300 --> 00:27:36,060
Like if I, if I train a model on, if I train a model just to do language translation, right?

326
00:27:36,060 --> 00:27:37,660
Like I just teach it that.

327
00:27:37,660 --> 00:27:43,220
It's kind of, I start from scratch and I just train it English to French.

328
00:27:43,220 --> 00:27:44,860
It's going to struggle.

329
00:27:44,860 --> 00:27:51,100
Whereas if I teach it English and French, kind of I teach it about the languages first

330
00:27:51,100 --> 00:27:57,660
and then I specialize it on translation, it's going to do much better.

331
00:27:57,660 --> 00:28:00,660
So this brings us to science.

332
00:28:00,660 --> 00:28:06,260
So in, in science we also have this.

333
00:28:06,260 --> 00:28:11,180
We also have this idea where there are shared concepts, right?

334
00:28:11,180 --> 00:28:17,180
Like different languages have shared, there's a shared concept of grammar in different languages.

335
00:28:17,180 --> 00:28:19,820
In science we also have shared concepts.

336
00:28:19,820 --> 00:28:26,220
You could kind of draw a big circle around many areas of science and causality is a shared

337
00:28:26,220 --> 00:28:27,820
concept.

338
00:28:27,820 --> 00:28:34,940
If you zoom in to say dynamical systems, you could think about like multi-scale dynamics

339
00:28:34,940 --> 00:28:41,460
is, is shared in many different disciplines, chaos is another shared concept.

340
00:28:41,460 --> 00:28:51,420
So maybe if we train a general model, you know, over many, many different data sets,

341
00:28:51,420 --> 00:28:56,580
the same way chat, GPT is trained on many, many different languages and text databases.

342
00:28:57,540 --> 00:29:03,180
Maybe you'll pick up general concepts and then when we finally make it specialize to

343
00:29:03,180 --> 00:29:10,740
our particular problem, maybe it'll do it, it'll find it easier to learn.

344
00:29:10,740 --> 00:29:13,660
So that's essentially the idea.

345
00:29:13,660 --> 00:29:18,020
So you can, you can really actually see this for a particular system.

346
00:29:18,020 --> 00:29:21,660
So one example is the reaction diffusion equation.

347
00:29:21,660 --> 00:29:28,340
This is a type of PDE and the shallow water equations, another type of PDE.

348
00:29:28,340 --> 00:29:32,980
Different fields, different PDEs, but both have waves.

349
00:29:32,980 --> 00:29:37,460
So they, they both have wave-like behavior.

350
00:29:37,460 --> 00:29:44,900
So I mean maybe if we train this massive flexible model on both of these systems, it's going

351
00:29:44,900 --> 00:29:51,780
to kind of learn a general prior for what a wave looks like.

352
00:29:51,780 --> 00:29:55,900
And then if I have like some, you know, some small data set I only have a couple examples

353
00:29:55,900 --> 00:30:02,220
of, maybe it'll immediately identify, oh, that's a wave, I know how to do that.

354
00:30:02,220 --> 00:30:11,340
It's almost like, I mean, I kind of feel like in science today, what we often do is, I mean

355
00:30:11,340 --> 00:30:14,500
we train machine learning models from scratch.

356
00:30:14,500 --> 00:30:20,540
It's almost like we're taking toddlers and we're teaching them to do pattern matching

357
00:30:20,540 --> 00:30:23,420
on like really advanced problems.

358
00:30:23,420 --> 00:30:27,420
Like we, we have a toddler and we're showing them this is a, you know, this is a spiral

359
00:30:27,420 --> 00:30:33,100
galaxy, this is an elliptical galaxy, and it kind of has to just do pattern matching.

360
00:30:33,100 --> 00:30:38,860
Whereas maybe a foundation model that's trained on broad classes of problems, it's, it's kind

361
00:30:38,860 --> 00:30:43,620
of like a general science graduate, maybe.

362
00:30:43,620 --> 00:30:47,480
So it has a prior for how the world works.

363
00:30:47,480 --> 00:30:52,460
It has seen many different phenomena before, and so when it, when you finally give it that

364
00:30:52,460 --> 00:30:57,460
data set to kind of pick up, it's already seen a lot of that phenomena.

365
00:30:57,460 --> 00:30:59,860
That's really the pitch of this.

366
00:30:59,860 --> 00:31:02,500
That's why we think this will work well.

367
00:31:02,500 --> 00:31:08,380
Okay, so we, we created this collaboration last year.

368
00:31:08,380 --> 00:31:16,660
So this started at Flatiron Institute, led by Shirley Ho, to build this thing, a foundation

369
00:31:16,660 --> 00:31:19,780
model for science.

370
00:31:19,780 --> 00:31:27,980
So this, this is across disciplines, so we want to, you know, build these models to incorporate

371
00:31:27,980 --> 00:31:35,900
data across many different disciplines, across institutions, and so we're currently working

372
00:31:35,900 --> 00:31:38,740
on kind of scaling up these models right now.

373
00:31:38,740 --> 00:31:45,540
The final, I think the final goal of this collaboration is that we would release these

374
00:31:45,540 --> 00:31:52,100
open source foundation models so that people could download them and fine tune them to different

375
00:31:52,100 --> 00:31:53,100
tasks.

376
00:31:53,100 --> 00:31:57,860
So it's really kind of like a different paradigm of doing machine learning, right?

377
00:31:57,860 --> 00:32:03,660
Like rather than the current paradigm where we take a model, randomly initialize it, it's

378
00:32:03,660 --> 00:32:09,820
kind of like a, like a toddler, doesn't know how the world works, and we train that.

379
00:32:09,820 --> 00:32:16,780
This paradigm is we have this generalist science model, and you start from that.

380
00:32:16,780 --> 00:32:21,500
It's kind of a better initialization of a model.

381
00:32:21,500 --> 00:32:25,020
That's, that's the, that's the pitch of Polymathic.

382
00:32:25,020 --> 00:32:31,340
Okay, so we have results, so this year we're kind of scaling up, but last year we had a

383
00:32:31,340 --> 00:32:40,060
couple of papers, so this is one led by Mike McCabe called Multiple Physics Pre-Training.

384
00:32:40,060 --> 00:32:47,980
This paper looked at what if we have this general PDE simulator, this, this model that

385
00:32:47,980 --> 00:32:54,940
learns to essentially run fluid dynamic simulations, and we train it on many different PDEs.

386
00:32:54,940 --> 00:32:59,380
Will it do better on new PDEs or will it do worse?

387
00:32:59,380 --> 00:33:12,980
So what we found is that a single, so a single model is not only able to match single models

388
00:33:12,980 --> 00:33:18,140
trained on specific tasks, it can actually outperform them in many cases.

389
00:33:18,140 --> 00:33:26,860
So it does seem like if you take a more flexible model, you train it on more diverse data,

390
00:33:26,860 --> 00:33:29,220
it will do better in a lot of cases.

391
00:33:29,220 --> 00:33:36,140
I mean it's, it's not unexpected because we do see this with language and vision, but

392
00:33:36,140 --> 00:33:41,380
I think it's still really cool to, to see this.

393
00:33:41,380 --> 00:33:45,340
So I'll skip through some of these.

394
00:33:45,340 --> 00:33:52,060
So this is like, this is the ground truth data, and this is the reconstruction.

395
00:33:52,060 --> 00:33:55,980
Essentially what it's doing is it's predicting the next step, right?

396
00:33:55,980 --> 00:33:59,900
It's predicting the next velocity, the next density, and pressure, and so on.

397
00:33:59,900 --> 00:34:04,540
And you're taking that prediction and running it back through the model, and you get this,

398
00:34:04,540 --> 00:34:07,820
this rule out simulation.

399
00:34:07,820 --> 00:34:12,740
So this is a, this is a task people work on in machine learning.

400
00:34:12,740 --> 00:34:15,940
I'm gonna skip through these.

401
00:34:15,940 --> 00:34:23,820
And essentially what we found is that most of the time by using this multiple physics

402
00:34:23,820 --> 00:34:24,820
pre-training.

403
00:34:24,820 --> 00:34:30,260
By training on many different PDEs, you do get better performance.

404
00:34:30,260 --> 00:34:35,740
So the ones at the right side are the multiple physics pre-trained models, those seem to

405
00:34:35,740 --> 00:34:37,740
do better in many cases.

406
00:34:37,740 --> 00:34:43,420
And it's really because, I mean I think because they've seen, you know, so many different

407
00:34:43,420 --> 00:34:48,540
PDEs, it's like they have a better prior for physics.

408
00:34:48,540 --> 00:34:52,220
I'll skip this as well.

409
00:34:52,220 --> 00:35:00,100
So okay, this is a funny thing that we observed is that, so during talks like this, one thing

410
00:35:00,100 --> 00:35:04,980
that we get asked is, how similar do the PDEs need to be?

411
00:35:04,980 --> 00:35:10,780
Like do the PDEs need to be, you know, like navier stokes but a different parameterization?

412
00:35:10,780 --> 00:35:14,740
Or can they be like completely different physical systems?

413
00:35:14,740 --> 00:35:25,660
So what we found is really hilarious is that, okay, so the bottom line here, this is the

414
00:35:25,660 --> 00:35:32,140
error of the model over a different number of training examples.

415
00:35:32,140 --> 00:35:36,980
So this model was trained on a bunch of different PDEs and then it was introduced to this new

416
00:35:36,980 --> 00:35:41,340
PDE problem and it's given that amount of data.

417
00:35:41,340 --> 00:35:43,300
So that does the best.

418
00:35:43,300 --> 00:35:47,740
This model, it already knows some physics, that one does the best.

419
00:35:47,740 --> 00:35:50,100
The one at the top is the worst.

420
00:35:50,100 --> 00:35:52,900
This is the model that's trained from scratch.

421
00:35:52,900 --> 00:35:58,940
It's never seen anything, this is like your toddler, right, like it doesn't know how the

422
00:35:58,940 --> 00:36:01,540
physical world works.

423
00:36:01,540 --> 00:36:05,100
It was just randomly initialized and it has to learn physics.

424
00:36:05,580 --> 00:36:14,820
Okay, the middle models, those are pre-trained on general video data, a lot of which is cat

425
00:36:14,820 --> 00:36:16,260
videos.

426
00:36:16,260 --> 00:36:26,780
So even pre-training this model on cat videos actually helps you do much better than this

427
00:36:26,780 --> 00:36:32,500
very sophisticated transformer architecture that just has never seen any data.

428
00:36:32,500 --> 00:36:38,820
And it's really because, I mean, we think it's because of shared concepts of spatiotemporal

429
00:36:38,820 --> 00:36:48,700
continuity, right, like videos of cats, there's a spatiotemporal continuity, like the cat

430
00:36:48,700 --> 00:36:53,860
does not teleport across the video unless it's a very fast cat.

431
00:36:53,860 --> 00:36:56,220
There's related concepts, right?

432
00:36:56,220 --> 00:36:58,340
So I mean, that's what we think.

433
00:36:58,340 --> 00:37:06,540
But it's really interesting that pre-training on completely unrelated systems still seems

434
00:37:06,540 --> 00:37:09,820
to help.

435
00:37:09,820 --> 00:37:15,620
And so the takeaway from this is that you should always pre-train your model, even if

436
00:37:15,620 --> 00:37:22,260
the physical system is not that related, you still see benefit of it.

437
00:37:23,260 --> 00:37:29,060
Now obviously, if you pre-train on related data, that helps you more, but anything is

438
00:37:29,060 --> 00:37:32,060
basically better than nothing.

439
00:37:32,060 --> 00:37:39,020
You could basically think of this as the default initialization for neural networks is garbage,

440
00:37:39,020 --> 00:37:44,140
right, like just randomly initializing a neural network, that's a bad starting point.

441
00:37:44,140 --> 00:37:46,580
It's a bad prior for physics.

442
00:37:46,580 --> 00:37:48,980
You should always pre-train your model.

443
00:37:48,980 --> 00:37:50,580
That's the takeaway of this.

444
00:37:50,900 --> 00:37:57,700
OK, so I want to finish up here with kind of rhetorical questions.

445
00:37:57,700 --> 00:38:05,620
So I started the talk about interpretability and kind of like how do we extract insights

446
00:38:05,620 --> 00:38:07,300
from our model.

447
00:38:07,300 --> 00:38:12,700
Now we've kind of gone into this regime of these very large, very flexible foundation

448
00:38:12,700 --> 00:38:17,420
models that seem to learn general principles.

449
00:38:17,420 --> 00:38:25,740
So OK, my question for you, you don't have to answer, but just think it over, is do you

450
00:38:25,740 --> 00:38:29,460
think 1 plus 1 is simple?

451
00:38:29,460 --> 00:38:31,100
It's not a trick question.

452
00:38:31,100 --> 00:38:33,420
Do you think 1 plus 1 is simple?

453
00:38:33,420 --> 00:38:38,580
So I think most people would say yes, 1 plus 1 is simple.

454
00:38:38,580 --> 00:38:42,300
And if you break that down into y, it's simple.

455
00:38:42,300 --> 00:38:46,180
You say, OK, so x plus y is simple for like x and y integers.

456
00:38:46,220 --> 00:38:48,140
That's a simple relationship.

457
00:38:48,140 --> 00:38:49,660
OK, y.

458
00:38:49,660 --> 00:38:52,580
y is x plus y is simple.

459
00:38:52,580 --> 00:38:55,380
And you break that down, it's because plus is simple.

460
00:38:55,380 --> 00:38:57,500
Like plus is a simple operator.

461
00:38:57,500 --> 00:38:59,260
OK, y.

462
00:38:59,260 --> 00:39:01,100
y is plus simple.

463
00:39:01,100 --> 00:39:05,180
It's a very abstract concept, OK?

464
00:39:05,180 --> 00:39:12,900
It's we don't necessarily have plus kind of built into our brains.

465
00:39:12,940 --> 00:39:20,100
It's kind of, I mean, it's really, so I'm going to show this.

466
00:39:20,100 --> 00:39:28,380
This might be controversial, but I think that simplicity is based on familiarity.

467
00:39:28,380 --> 00:39:31,740
We are used to plus as a concept.

468
00:39:31,740 --> 00:39:35,220
We are used to adding numbers as a concept.

469
00:39:35,220 --> 00:39:38,220
Therefore, we call it simple.

470
00:39:38,220 --> 00:39:41,220
You can go back another step further.

471
00:39:41,220 --> 00:39:46,060
The reason we're familiar with addition is because it's useful.

472
00:39:46,060 --> 00:39:49,460
Adding numbers is useful for describing the world.

473
00:39:49,460 --> 00:39:51,340
I count things.

474
00:39:51,340 --> 00:39:53,980
That's useful to live in our universe.

475
00:39:53,980 --> 00:39:57,660
It's useful to count things, to measure things.

476
00:39:57,660 --> 00:40:00,260
Addition is useful.

477
00:40:00,260 --> 00:40:03,060
And it's really one of the most useful things.

478
00:40:03,060 --> 00:40:06,460
So that is why we are familiar with it.

479
00:40:06,460 --> 00:40:09,700
And I would argue that's why we think it's simple.

480
00:40:09,700 --> 00:40:17,820
But the simplicity we have often argued is if it's simple,

481
00:40:17,820 --> 00:40:20,820
it's more likely to be useful.

482
00:40:20,820 --> 00:40:24,620
I think that is actually not a statement about simplicity.

483
00:40:24,620 --> 00:40:28,580
It's actually a statement that if something

484
00:40:28,580 --> 00:40:32,140
is useful for problems like A, B, and C,

485
00:40:32,140 --> 00:40:36,660
then it seems it will also be useful for another problem.

486
00:40:36,660 --> 00:40:38,620
The world is compositional.

487
00:40:38,660 --> 00:40:41,780
If I have a model that works for this set of problems,

488
00:40:41,780 --> 00:40:44,300
it's probably also going to work for this one.

489
00:40:44,300 --> 00:40:47,220
So that's the argument I would like to make.

490
00:40:47,220 --> 00:40:51,580
So when we interpret these models,

491
00:40:51,580 --> 00:40:58,020
I think it's important to keep this in mind and really probe

492
00:40:58,020 --> 00:41:02,340
what is simple, what is interpretable.

493
00:41:02,340 --> 00:41:09,620
So I think this is really exciting for polymathic EI

494
00:41:09,620 --> 00:41:15,020
because these models that are trained on many, many systems,

495
00:41:15,020 --> 00:41:20,100
they will find broadly useful algorithms.

496
00:41:20,100 --> 00:41:23,380
They'll have these neurons that share calculations

497
00:41:23,380 --> 00:41:25,620
across many different disciplines.

498
00:41:25,620 --> 00:41:30,660
So you could argue that that is the utility.

499
00:41:30,660 --> 00:41:34,420
And I mean, maybe we'll discover new kind of operators

500
00:41:34,420 --> 00:41:37,060
and be familiar with those, and we'll

501
00:41:37,060 --> 00:41:38,460
start calling those simple.

502
00:41:38,460 --> 00:41:42,980
So it's not necessarily that all of the things

503
00:41:42,980 --> 00:41:46,620
we discover in machine learning will be simple.

504
00:41:46,620 --> 00:41:49,540
It's kind of that, by definition,

505
00:41:49,540 --> 00:41:53,860
the polymathic EI models will be broadly useful.

506
00:41:53,860 --> 00:41:56,700
And if we know they're broadly useful,

507
00:41:56,700 --> 00:41:58,980
we might get familiar with those.

508
00:41:58,980 --> 00:42:03,860
And that might kind of drive the simplicity of them.

509
00:42:03,860 --> 00:42:06,860
So that's my note in simplicity.

510
00:42:06,860 --> 00:42:09,940
And so the takeaways here are that I

511
00:42:09,940 --> 00:42:14,660
think interpreting a neural network trained on some data

512
00:42:14,660 --> 00:42:18,820
sets offers new ways of discovering

513
00:42:18,820 --> 00:42:21,220
scientific insights from that data.

514
00:42:21,220 --> 00:42:24,580
And I think foundation models like polymathic EI,

515
00:42:24,580 --> 00:42:26,580
I think that is a very exciting way

516
00:42:26,580 --> 00:42:30,820
of discovering new broadly applicable scientific models.

517
00:42:30,820 --> 00:42:33,540
So I'm really excited about this direction.

518
00:42:33,540 --> 00:42:36,340
And thank you for listening to me today.

519
00:42:36,340 --> 00:42:36,840
OK.

520
00:42:36,840 --> 00:42:37,340
Thank you.

521
00:42:37,340 --> 00:42:37,840
Thank you.

522
00:42:37,840 --> 00:42:38,340
Thank you.

523
00:42:38,340 --> 00:42:38,840
Thank you.

524
00:42:38,840 --> 00:42:39,340
Thank you.

525
00:42:39,340 --> 00:42:39,840
Thank you.

526
00:42:39,840 --> 00:42:40,340
Thank you.

527
00:42:40,340 --> 00:42:40,840
Thank you.

528
00:42:40,840 --> 00:42:41,340
Thank you.

529
00:42:41,340 --> 00:42:41,840
Thank you.

530
00:42:49,340 --> 00:42:52,820
Thank you for the question, which was great.

531
00:42:52,820 --> 00:42:55,300
So three short questions.

532
00:42:55,300 --> 00:42:55,800
One.

533
00:42:55,800 --> 00:43:00,560
What was the cost of running polymathic AI

534
00:43:00,560 --> 00:43:02,040
in this kind of a way?

535
00:43:02,040 --> 00:43:03,520
And how to cost it, right?

536
00:43:03,520 --> 00:43:04,020
Yeah.

537
00:43:07,520 --> 00:43:08,020
Two.

538
00:43:08,020 --> 00:43:12,520
When the story built out, it really destroyed you.

539
00:43:12,520 --> 00:43:13,840
I got to help you.

540
00:43:13,840 --> 00:43:16,520
Yeah.

541
00:43:16,520 --> 00:43:18,960
Please use your C mic.

542
00:43:18,960 --> 00:43:19,960
Right in front of you.

543
00:43:19,960 --> 00:43:20,960
We'll put it back.

544
00:43:23,960 --> 00:43:24,460
Yeah.

545
00:43:25,460 --> 00:43:26,460
I'll call them.

546
00:43:26,460 --> 00:43:28,460
And three.

547
00:43:28,460 --> 00:43:30,460
You're putting your back.

548
00:43:30,460 --> 00:43:34,460
Thank you guys for that.

549
00:43:34,460 --> 00:43:38,260
I was trying to answer three.

550
00:43:38,260 --> 00:43:40,980
OK, so I'll try to compartmentalize those.

551
00:43:40,980 --> 00:43:46,220
OK, so the first question was the scale of training.

552
00:43:46,220 --> 00:43:49,060
This is really an open research question.

553
00:43:49,060 --> 00:43:52,740
We don't have the scaling law for science yet.

554
00:43:52,740 --> 00:43:54,740
We have scaling laws for language.

555
00:43:54,740 --> 00:43:56,500
We know that if you have this many GPUs,

556
00:43:56,500 --> 00:43:58,300
you have this size data set, this

557
00:43:58,300 --> 00:43:59,860
is going to be your performance.

558
00:43:59,860 --> 00:44:01,580
We don't have that yet for science

559
00:44:01,580 --> 00:44:04,820
because nobody's built this scale of model.

560
00:44:04,820 --> 00:44:07,660
So that's something we're looking at right now,

561
00:44:07,660 --> 00:44:10,380
is what is the trade-off of scale?

562
00:44:10,380 --> 00:44:13,740
And if I want to train this model on many, many GPUs,

563
00:44:13,740 --> 00:44:16,140
is it worth it?

564
00:44:16,140 --> 00:44:18,660
So that's an open research question.

565
00:44:18,660 --> 00:44:21,620
I do think it'll be large.

566
00:44:21,620 --> 00:44:28,340
Probably order hundreds of GPUs trained for maybe

567
00:44:28,340 --> 00:44:29,740
a couple months.

568
00:44:29,740 --> 00:44:32,300
So it's going to be a very large model.

569
00:44:32,300 --> 00:44:36,900
That's kind of assuming the scale of language models.

570
00:44:36,900 --> 00:44:40,500
Now, the model is going to be free, definitely.

571
00:44:40,500 --> 00:44:43,820
We're all very pro-open source.

572
00:44:43,820 --> 00:44:46,140
And I think that's really the point,

573
00:44:46,140 --> 00:44:49,060
is we want to open source this model so people can download it

574
00:44:49,060 --> 00:44:50,100
and use it in science.

575
00:44:50,100 --> 00:44:55,340
I think that's really the most exciting part about this.

576
00:44:55,340 --> 00:44:57,780
And then I guess the third question you had

577
00:44:57,780 --> 00:45:07,060
was about the future and how it changes how we teach.

578
00:45:07,060 --> 00:45:10,860
I mean, I guess are you asking about teaching science

579
00:45:10,860 --> 00:45:12,380
or teaching machine learning?

580
00:45:12,380 --> 00:45:13,580
Teaching science.

581
00:45:13,580 --> 00:45:16,140
I see.

582
00:45:16,140 --> 00:45:18,100
I mean, yeah, I mean, I don't know.

583
00:45:18,100 --> 00:45:20,100
It depends if it works.

584
00:45:20,100 --> 00:45:22,460
I think if it works, it might very well

585
00:45:22,460 --> 00:45:26,180
change how science is taught.

586
00:45:26,180 --> 00:45:31,700
Yeah, I mean, so I don't know the impact of language models

587
00:45:31,700 --> 00:45:33,260
on computational linguistics.

588
00:45:33,260 --> 00:45:34,860
I'm assuming they've had a big impact.

589
00:45:34,860 --> 00:45:39,460
I don't know if that's affected the teaching of it yet.

590
00:45:39,460 --> 00:45:43,460
But if scientific foundation models had a similar impact,

591
00:45:43,460 --> 00:45:45,740
I'm sure it would impact.

592
00:45:45,740 --> 00:45:46,500
I don't know how much.

593
00:45:46,500 --> 00:45:48,900
Probably depends on the success of the models.

594
00:45:55,540 --> 00:45:58,180
I have a question about your foundation models also.

595
00:45:58,180 --> 00:45:59,980
So in different branches of science,

596
00:45:59,980 --> 00:46:01,500
the data sets are pretty different.

597
00:46:01,500 --> 00:46:03,340
In molecular biology or genetics,

598
00:46:03,340 --> 00:46:05,340
the data sets is a sequence of DNA

599
00:46:05,340 --> 00:46:08,460
versus astrophysics where it's images of stars.

600
00:46:08,460 --> 00:46:11,980
So how do you plan to use the same model

601
00:46:11,980 --> 00:46:15,820
for different form of data sets, input data sets?

602
00:46:15,820 --> 00:46:18,660
So you mean how to pose the objective?

603
00:46:18,660 --> 00:46:19,300
Yes.

604
00:46:19,300 --> 00:46:23,340
So I think the most general objective

605
00:46:23,340 --> 00:46:25,740
is self-supervised learning where you basically

606
00:46:25,740 --> 00:46:29,860
mask parts of the data and you predict the missing part.

607
00:46:29,860 --> 00:46:33,220
If you can optimize that problem,

608
00:46:33,220 --> 00:46:34,820
then you can solve tons of different ones.

609
00:46:34,820 --> 00:46:37,740
You can do regression, predict parameters,

610
00:46:37,740 --> 00:46:41,220
or go the other way and predict rollouts of the model.

611
00:46:41,260 --> 00:46:45,300
It's a really general problem to mask data

612
00:46:45,300 --> 00:46:46,820
and then fill it back in.

613
00:46:46,820 --> 00:46:51,620
That kind of is a superset of many different prediction

614
00:46:51,620 --> 00:46:53,060
problems.

615
00:46:53,060 --> 00:46:56,500
And I think that's why language models are so broadly useful,

616
00:46:56,500 --> 00:46:59,500
even though they're trained just on next-word prediction

617
00:46:59,500 --> 00:47:02,620
or like BERT is a masked model.

618
00:47:07,780 --> 00:47:08,660
Thanks.

619
00:47:08,660 --> 00:47:09,980
Can you hear me?

620
00:47:10,020 --> 00:47:12,540
All right, so that was a great talk.

621
00:47:12,540 --> 00:47:13,780
I'm Victor.

622
00:47:13,780 --> 00:47:17,620
So I'm actually a little bit worried

623
00:47:17,620 --> 00:47:20,260
and this is a little bit of a question.

624
00:47:20,260 --> 00:47:23,700
Whenever you have models like this,

625
00:47:23,700 --> 00:47:26,460
you say that you train these on many examples, right?

626
00:47:26,460 --> 00:47:29,220
So imagine you have already embedded

627
00:47:29,220 --> 00:47:30,940
the laws of physics here somehow,

628
00:47:30,940 --> 00:47:32,980
like let's say the law of repetition.

629
00:47:32,980 --> 00:47:36,060
But when you think about discovering new physics,

630
00:47:36,060 --> 00:47:38,940
we always have this question of whether we are actually

631
00:47:38,980 --> 00:47:42,980
reinventing the wheel or like the network is kind of really

632
00:47:42,980 --> 00:47:46,300
giving us something new or is it something giving us

633
00:47:46,300 --> 00:47:49,020
or is it giving us something that it learned

634
00:47:49,020 --> 00:47:50,340
but it's kind of wrong.

635
00:47:50,340 --> 00:47:55,100
So sometimes we have the answer to know which one is which.

636
00:47:55,100 --> 00:47:57,340
But if you don't have that, let's say for instance,

637
00:47:57,340 --> 00:47:58,980
you're trying to discover what dark matter is,

638
00:47:58,980 --> 00:48:01,620
which is something I'm working on.

639
00:48:01,620 --> 00:48:04,260
How would you know that the network is actually giving you

640
00:48:04,260 --> 00:48:07,700
something new and not just trying to set this

641
00:48:07,700 --> 00:48:10,180
into one of the many parameters it has?

642
00:48:11,380 --> 00:48:16,380
So, okay, so if you wanna test the model

643
00:48:17,100 --> 00:48:19,380
by letting it rediscover something,

644
00:48:19,380 --> 00:48:21,140
then I don't think you should use this.

645
00:48:21,140 --> 00:48:23,340
I think you should use the scratch model,

646
00:48:23,340 --> 00:48:25,460
like from scratch and train it.

647
00:48:25,460 --> 00:48:27,580
Because if you use a pre-train model,

648
00:48:27,580 --> 00:48:30,060
it's probably already seen that physics.

649
00:48:30,060 --> 00:48:32,500
So it's biased towards it in some ways.

650
00:48:32,500 --> 00:48:33,900
So if you're rediscovering something,

651
00:48:33,900 --> 00:48:35,380
I don't think you should use this.

652
00:48:35,380 --> 00:48:37,180
If you're discovering something new,

653
00:48:38,460 --> 00:48:42,060
I do think this is more useful.

654
00:48:42,060 --> 00:48:47,060
So I think a misconception of,

655
00:48:48,860 --> 00:48:50,100
I think machine learning in general

656
00:48:50,100 --> 00:48:52,460
is that scientists view machine learning

657
00:48:53,420 --> 00:48:54,780
for uninitialized models,

658
00:48:54,780 --> 00:48:58,660
like randomly initialized weights as a neutral prior.

659
00:48:58,660 --> 00:49:03,660
But it's not, it's a very explicit prior.

660
00:49:04,660 --> 00:49:06,740
And it happens to be a bad prior.

661
00:49:07,940 --> 00:49:12,940
So if you train from a randomly initialized model,

662
00:49:12,940 --> 00:49:16,660
it's kind of always gonna be a worse prior

663
00:49:16,660 --> 00:49:18,020
than training from a pre-train model,

664
00:49:18,020 --> 00:49:20,540
which has seen many different types of physics.

665
00:49:20,540 --> 00:49:23,060
I think we can kind of make that statement.

666
00:49:24,740 --> 00:49:26,740
So if you're trying to discover new physics,

667
00:49:26,740 --> 00:49:31,740
I mean, like if you train it on some data set,

668
00:49:32,740 --> 00:49:34,140
I guess you can always verify

669
00:49:34,140 --> 00:49:36,500
that the predictions are accurate.

670
00:49:36,500 --> 00:49:40,580
So that would be, I guess, one way to verify it.

671
00:49:41,900 --> 00:49:44,220
But I do think like the fine tuning here,

672
00:49:44,220 --> 00:49:47,300
so like taking this model and training it on the task,

673
00:49:47,300 --> 00:49:49,300
I think that's very important.

674
00:49:49,300 --> 00:49:52,940
I think in language models, it's not as emphasized.

675
00:49:52,940 --> 00:49:54,820
Like people will just take a language model

676
00:49:54,820 --> 00:49:58,100
and tweak the prompt to get a better result.

677
00:49:58,300 --> 00:50:02,980
I think for science, I think the prompt is,

678
00:50:02,980 --> 00:50:04,980
I mean, I think like the equivalent of the prompt

679
00:50:04,980 --> 00:50:06,700
would be important, but I think the fine tuning

680
00:50:06,700 --> 00:50:07,820
is much more important,

681
00:50:07,820 --> 00:50:10,780
because our data sets are so much different across science.

682
00:50:13,780 --> 00:50:15,780
Chris, in the back, thank you.

683
00:50:17,660 --> 00:50:19,100
In my course, the main course,

684
00:50:19,100 --> 00:50:21,700
all the students, I don't think there's one.

685
00:50:21,700 --> 00:50:23,540
Please note, Steph, the symbolic.

686
00:50:24,540 --> 00:50:26,860
It's still limited in the complexity

687
00:50:26,860 --> 00:50:29,580
and dimensionality of the system.

688
00:50:29,580 --> 00:50:32,180
So are you, based on your saying,

689
00:50:32,180 --> 00:50:35,940
also the fine tuning and transfer learning

690
00:50:35,940 --> 00:50:40,940
as a way of enhancing the symbolic regression?

691
00:50:40,940 --> 00:50:42,940
And what does that mean?

692
00:50:45,220 --> 00:50:48,300
Yeah, so the symbolic regression,

693
00:50:48,300 --> 00:50:51,140
I mean, I would consider that it's not used

694
00:50:51,140 --> 00:50:54,100
inside the foundation model part.

695
00:50:54,100 --> 00:50:58,540
I think it's interesting to interpret the foundation model

696
00:50:58,540 --> 00:51:02,860
and see if there's kind of more general physical frameworks

697
00:51:02,860 --> 00:51:03,980
that it comes up with.

698
00:51:06,860 --> 00:51:09,660
I think, yeah, symbolic regression is very limited

699
00:51:09,660 --> 00:51:12,820
in that it's bad at high dimensional problems.

700
00:51:12,820 --> 00:51:17,820
I think that might be because of the choice of operators.

701
00:51:18,820 --> 00:51:22,900
I think if you can consider maybe high dimensional operators,

702
00:51:22,900 --> 00:51:25,140
you might be a bit better off.

703
00:51:25,140 --> 00:51:26,420
I mean, symbolic regression,

704
00:51:26,420 --> 00:51:29,580
it's an active area of research,

705
00:51:29,580 --> 00:51:32,620
and I think the hardest, the biggest hurdle right now

706
00:51:32,620 --> 00:51:37,620
is it's not good at finding very complex symbolic models.

707
00:51:37,620 --> 00:51:55,620
So I guess you could, it depends on the dimensionality

708
00:51:55,620 --> 00:51:56,620
of the data.

709
00:51:56,620 --> 00:52:00,100
I guess if it's very high dimensional data,

710
00:52:00,100 --> 00:52:06,500
you're always kind of, symbolic regression is not good

711
00:52:06,500 --> 00:52:08,780
at high dimensional data unless you can have

712
00:52:08,780 --> 00:52:13,180
kind of some operators that aggregate

713
00:52:13,180 --> 00:52:15,580
to lower dimensional spaces.

714
00:52:18,380 --> 00:52:22,100
Yeah, I don't know if I'm answering your question.

715
00:52:22,100 --> 00:52:23,100
OK.

716
00:52:23,100 --> 00:52:24,780
I wanted to ask a little bit.

717
00:52:24,780 --> 00:52:28,780
So when you were showing the construction of these trees

718
00:52:28,780 --> 00:52:31,540
of each generation and the different operators,

719
00:52:31,540 --> 00:52:33,380
I think this is related to kind of general themes

720
00:52:33,380 --> 00:52:34,660
at the top and other questions.

721
00:52:34,660 --> 00:52:37,100
But often in doing science, when you're writing it,

722
00:52:37,100 --> 00:52:39,780
you're presented with kind of an algorithmic self-prompt.

723
00:52:39,780 --> 00:52:42,020
It's like, you know, diagonalize the Hamiltonian

724
00:52:42,020 --> 00:52:44,020
or something like that.

725
00:52:44,020 --> 00:52:46,140
How do you calculate that aspect of doing science

726
00:52:46,140 --> 00:52:48,820
that is kind of the algorithmic side of solving

727
00:52:48,820 --> 00:52:52,260
the problem rather than just going up and down?

728
00:52:52,260 --> 00:52:52,780
Right.

729
00:52:52,780 --> 00:52:55,420
These, do you see my?

730
00:52:55,420 --> 00:52:56,540
Yeah.

731
00:52:56,540 --> 00:52:58,940
Yeah, so the question was about how

732
00:52:58,940 --> 00:53:03,100
do you incorporate kind of more general, not

733
00:53:03,180 --> 00:53:05,820
analytic operators, but kind of more general algorithms

734
00:53:05,820 --> 00:53:08,500
like a Hamiltonian operator.

735
00:53:08,500 --> 00:53:11,460
I think that, I mean, like in principle,

736
00:53:11,460 --> 00:53:15,100
symbolic regression is it's part of a larger family

737
00:53:15,100 --> 00:53:17,900
of an algorithm called program synthesis,

738
00:53:17,900 --> 00:53:21,220
where the objective is to find a program, you know,

739
00:53:21,220 --> 00:53:26,140
like code that describes a given data set, for example.

740
00:53:26,140 --> 00:53:30,100
So if you can write your operators

741
00:53:30,100 --> 00:53:32,660
into your symbolic regression approach

742
00:53:32,780 --> 00:53:34,980
and your symbolic regression approach

743
00:53:34,980 --> 00:53:39,300
has that ground truth model in there somewhere,

744
00:53:39,300 --> 00:53:40,780
then I think it's totally possible.

745
00:53:40,780 --> 00:53:44,900
I think, like, it's harder to do.

746
00:53:44,900 --> 00:53:47,060
I think, like, even symbolic regression with scalars

747
00:53:47,060 --> 00:53:53,580
is, it's fairly difficult to actually set up an algorithm.

748
00:53:53,580 --> 00:53:54,780
I think, I don't know, I think it's really

749
00:53:54,780 --> 00:53:55,860
like an engineering problem.

750
00:53:55,860 --> 00:54:02,020
But the conceptual part is totally like there for this.

751
00:54:02,020 --> 00:54:03,020
Yeah.

752
00:54:07,500 --> 00:54:08,620
Thanks.

753
00:54:08,620 --> 00:54:09,140
Oh, sorry.

754
00:54:13,540 --> 00:54:17,500
This claim that random initial weights are always bad

755
00:54:17,500 --> 00:54:18,820
or pre-training is always good.

756
00:54:18,820 --> 00:54:20,300
I don't know if they're always bad,

757
00:54:20,300 --> 00:54:25,460
but it seems like from our experiments,

758
00:54:25,460 --> 00:54:29,980
we've never seen a case where pre-training

759
00:54:29,980 --> 00:54:32,180
on some kind of physical data hurts.

760
00:54:32,180 --> 00:54:34,340
Like the cap video is an example.

761
00:54:34,340 --> 00:54:35,980
We thought that would hurt the model.

762
00:54:35,980 --> 00:54:36,940
It didn't.

763
00:54:36,940 --> 00:54:39,260
That is a cute example.

764
00:54:39,260 --> 00:54:42,100
I'm sure there's cases where some pre-training hurts.

765
00:54:42,100 --> 00:54:43,620
Yeah, so that's essentially my question.

766
00:54:43,620 --> 00:54:45,540
So we're aware of, like, adversarial examples.

767
00:54:45,540 --> 00:54:47,540
For example, you train on MNIST, add a bit of noise.

768
00:54:47,540 --> 00:54:49,860
It does terrible compared to what a human would do.

769
00:54:49,860 --> 00:54:53,220
What do you think adversarial examples look like in science?

770
00:54:53,220 --> 00:54:54,860
Yeah, I mean, I don't know what those are,

771
00:54:54,860 --> 00:54:57,660
but I'm sure they exist somewhere,

772
00:54:57,700 --> 00:55:00,780
where pre-training on certain data types kind of

773
00:55:00,780 --> 00:55:03,500
messes with training a bit.

774
00:55:03,500 --> 00:55:07,180
We don't know those yet, but yeah, it'll be interesting.

775
00:55:07,180 --> 00:55:09,180
Do you think it's a pick fall, though, of the approach?

776
00:55:09,180 --> 00:55:12,140
Because I have a model of the sun and a model of DNA.

777
00:55:14,580 --> 00:55:18,620
Yeah, I mean, I don't know.

778
00:55:18,620 --> 00:55:21,180
I guess we'll see.

779
00:55:21,180 --> 00:55:23,260
Yeah, it's hard to know.

780
00:55:23,260 --> 00:55:27,420
I guess from language, we've seen you can pre-train

781
00:55:27,420 --> 00:55:29,820
like a language model on video data,

782
00:55:29,820 --> 00:55:32,860
and it helps the language, which is really weird.

783
00:55:32,860 --> 00:55:35,940
But it does seem like if there's any kind of concepts,

784
00:55:35,940 --> 00:55:38,260
it does, if it's flexible enough,

785
00:55:38,260 --> 00:55:41,100
it can kind of transfer those in some ways.

786
00:55:41,100 --> 00:55:41,940
So we'll see.

787
00:55:41,940 --> 00:55:45,860
I mean, presumably, we'll find some adversarial examples there.

788
00:55:45,860 --> 00:55:47,140
So far, we haven't.

789
00:55:47,140 --> 00:55:49,900
We thought the cat was one, but it wasn't.

790
00:55:49,900 --> 00:55:50,740
It helped.

