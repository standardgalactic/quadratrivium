start	end	text
0	12960	Our computer scientists at the University of Louisville and specialized in behavioral biometrics,
12960	18320	security of cyber worlds and AI safety. And indeed, one of my personal first steps in the
18320	22880	world of existential risk was to read the number of your publications. And I think you may have
22880	28160	inspired many like me to start working on either AI safety or existential risk or related fields.
29120	34160	And I think you also have always interesting and thought-provoking comments available on each
34160	38480	topic to social accounts, for example. So I really enjoyed reading those and I can recommend them
38480	43680	to everyone. So we are now honored and it is a great pleasure, I can say, to be able to learn
43680	47360	from you today. And we'll give the stage to you now, Dr. Romer Jampolsky.
48080	54400	Thank you so much. Wonderful introduction. I really appreciate that. I think the plan is that
54400	62720	I'll give about a 10-minute intro to my latest work and then we'll let me set up the slides.
66000	72080	Where is it? Where did it go? Yeah, that's right. We can take a little bit longer as well if you
72080	76720	want to. And then we can move to the fireside chat and then the Q&A with audience. Can you see my
76720	85680	slides? Yes. Excellent. So my name is Dr. Jampolsky. I'm a faculty member at University of Louisville.
85680	94000	I've been doing work on AI safety for about 10 years now. Give or take. And my latest work covers
94560	101040	what I will call impossibility results. Problems we encounter with actually
101040	109840	accomplishing what we think is necessary for us to do not just development work for AI,
109840	116480	but also work in terms of control and safety. If you would like to learn more about my work
117120	122320	after this talk, you are definitely welcome to follow me. You can follow me on Twitter,
122320	126880	follow me on Facebook. I always encourage you not to follow me home. It's very important.
127760	137760	So let's start with the big problem right away. If you heard previous talks at this conference,
137760	144960	I had a chance to hear a little bit. You know about concerns a lot of people have with advanced
144960	153920	artificial intelligence, usually called AI safety, AI alignment problem, sometimes AI control problem.
154160	160560	The question is the problem we're trying to solve is how can humanity remain safely in control while
160560	168240	benefiting from superior form of intelligence? So we want this very capable agent to do work for us,
168240	174640	to be helpful, but we want to be in charge. We want to be able to undo any changes if we don't
174640	183040	like them. We want to be final decision makers as to what's going to happen. And the good news
184000	189440	is after 10 years of building up the movement, there is a lot of people working on this now.
189440	197200	There is a lot of research labs, a lot of PhD programs, but I think the main question of
197760	203360	control problem has not been addressed sufficiently. And that is, is the problem actually solvable?
203360	209520	So everyone kind of works on it, but I haven't seen much in terms of proofs or even rigorous
209520	215200	argumentation for why we think we can do this. Why is this problem solvable? Is it solvable?
215200	220960	Is it partially solvable? Maybe it's not solvable. Maybe it's a silly question. It's not even decidable.
221600	228800	So that's essentially what I've been looking at for the last couple years. And I started by
228800	234560	formalizing a little bit what I mean by control problem. So we can talk about different types of
234640	242240	control. We have explicit control where you give orders and the system follows. And this is the
242240	247680	kind of standard Gini problem, right? You wish for things, then you realize that's not what I meant
247680	254320	and you hope you get to undo the damage. Then there is implicit control. So you have a system with
254320	259760	a little more common sense. It doesn't take your words literally, it tries to kind of parse things
259760	266240	in the right way. And there are intermediate steps between explicit and delegated control.
266240	272400	Delegated control is the other extreme where you have this superintelligent system very friendly.
272400	278000	It knows what needs to happen better than you do. And you might be even very happy with the
278000	284320	decisions it makes, but you're not in charge. The system is completely in control. Aligned control
284320	291120	is another intermediate option where the system kind of understands your values, human values,
291120	300400	and tries to do the best it can to fulfill those values, even if your spoken directions are not
300400	313440	exactly what maps onto those ideal values. So it seems at least from now that some sort of intermediate
314720	323280	value between total control and total autonomy for the system is necessary for us to be happy with
323280	330480	the results. If a system is completely autonomous, we have no control over it. By definition, we
330480	336400	will lose control. If a system has no autonomy, it's completely deterministic. We decide ahead
336400	341040	of time what's going to happen. It's not very useful. It cannot be generally intelligent. It's a
341040	349200	great way to do simple tasks for a narrow AI, but it's not something we can utilize to solve
350160	356080	completely new problems and new domains, help us with science, help us cure diseases and things
356080	362240	like that. So what I try to do is kind of break down the bigger problem of control
362240	367600	into all the ingredients we need, all the tools we would need to make controllability possible.
368560	373840	So what do you need? You can start thinking for yourself. If you want to be in control,
373840	379200	well, you kind of have to understand the situation. What is the system doing? Can the system explain
379200	384720	what it's doing? Can you comprehend the explanation? That's another very important one. Can you predict
384720	390000	what the system is likely to do? Maybe not just direction in which it is going, but specifics.
390000	396960	What steps will it take? Can you verify that whatever it is you want the system to do and
397040	404160	program it to do is actually going to happen? So can you verify the implementation versus the model?
404160	410320	Can you verify the model against your goals and data and so on? That is also a need for
410880	417120	general ability to govern AI research, AI systems, so governability of that. And of course,
417120	424000	we communicate with systems in human language and we need to make sure that communication we use
424000	431360	is unambiguous. There is no way to misinterpret commands in a potentially dangerous way.
431360	436880	And there is many, many more such limitations. And what I've been doing, kind of looking at each
436880	442960	one and trying to publish those results. So I'll go over some of the publications I have on that
443600	450000	so far. One paper talks about limits to explainability and incomprehensibility.
450000	458720	So essentially, for very complex systems, large neural networks, it is impossible for the system
458720	465600	to provide an exact explanation of what it is doing or why without simplifying it to the point of
465600	471920	where it is like you explaining something to a child. So a lot of important details are removed
471920	476480	and then a very simplified version is given to you because if a full version is given to you,
476480	482880	you'll simply not be able to comprehend it. And if you want to learn more and see the kind of
482880	488160	argumentation, in some cases, proofs, just go to the paper. I provide all the information you need
488160	493520	to get access to those papers. There are also available as preprints on my Google scholar account.
495120	500960	Another impossibility result is unpredictability and that's our inability to
501360	508400	precisely predict all decisions, all intermediate steps, a much smarter agent will take.
508400	513680	Of course, if we could predict all the decisions of a smarter agent, we ourselves would be that
513680	520640	smart and by definition, there wouldn't be much of an intelligence gap, cognitive gap between us.
521840	528560	That is also problems with verifiability. We know for a fact that with software,
529120	536880	with mathematical proofs, we can only get to a certain degree of confidence, but never 100%
536880	544160	confidence in the fact that we have no errors in the proof. So with more resources, we can
544160	551200	increase safety and security, but we're never able to guarantee something with 100% accuracy,
551200	556880	which is a problem for a superintelligence system, which makes potentially billions of
556880	563120	decisions every minute. Even if one in 100 million creates a problem, you're guaranteed to have a
563120	570880	huge problem within a minute or so. There are also problems with governance. We have history of
570880	578880	trying to govern technology, things like spam and computer viruses. We have laws against those
579600	585360	malevolent software products, but they don't seem to be doing much. So it's not obvious how much
585360	591520	benefit is actually added and other negative consequences from trying to control research,
591520	600320	control what is allowed and not allowed to be experimented with. Even the orders we give to
600320	606720	the system, the communication channel through English is very ambiguous and you're almost
606720	613840	guaranteed to run into situations where your orders will be misinterpreted at multiple levels
613840	621840	due to how imprecise human languages are. So what we did with our colleague, we surveyed a lot of
621840	627920	those impossibility results. Those I looked at and those other people have looked at, I'm not going
627920	635040	to go into details of all of them. I can just tell you there is a lot of them and some purely
635040	640720	software problems, mathematical problems, many problems with physics of the universe,
641360	648160	impossibility results from physics. But if you think even a small subset of all those tools
648160	654000	is necessary to solve the control problem, you have to come to the conclusion that control
654000	661280	is not possible. At least not 100% guaranteed safe, secure control we all dream about. And I
661280	668880	have a very lengthy paper about that, about 70 pages. I now have a few subsections of it
668880	675360	published coming out in conferences, those should be a lot more readable. But I think I
676160	681360	bring up a lot of interesting questions and additional directions for research and hopefully
681360	690640	in the next half hour or 40 minutes we can talk about what all this means and how we can move
690640	702640	forward from where we are right now. Thank you very much, Roman, for this introduction already.
703760	708800	Yes, we can now have a chat about indeed where does it leave us and where should we go from now
708800	714560	and also a couple of related questions. And towards the end, after about 15 to 20 minutes,
714560	719440	we will also take questions from the audience. So please, if you have any questions then please
719440	725920	type them into the chat. We're in the questions section rather. So first, you're spending quite a
725920	730800	lot of time researching AI existential risk, but I don't think it's already obvious for everyone in
730800	737600	the call why AI would be a danger at all. And I don't think everyone is perhaps 100% convinced
737600	743120	that this is actually an issue or an existential danger, at least that is. Could you please recap
743120	749040	how exactly AI could become an existential risk according to you? Right, so there is a lot of
749040	756080	ways to get to that conclusion. I have a few papers where I simply collect examples of accidents,
756080	761840	AI failures throughout history. And if you look at that progression, it's kind of same exponential
761840	766800	chart you see with development. We get more problems, the problems become more severe,
767520	774960	and our ability to anticipate and predict them seems to be very limited. So basically the conclusion
774960	780000	is something like, if you have a system of service to do X, eventually it fails to X.
780000	785120	Frequently it does so very quickly and you go, hmm, okay, my self-driving car just killed a bunch
785120	791200	of pedestrians, that's a problem. And then it's a narrow system, the damage is limited, right? So
791200	796960	self-driving car, okay, the worst it can do is run through some pedestrians. But if a system
796960	802240	becomes general, and it's now controlling not just a single car, but networks of cars,
802320	807200	nuclear response, airline industry, stock market, the damage is proportioned.
809040	815200	I think it's also not the best way to assume that I have to prove that this service or product is
815200	820240	dangerous. Whoever is developing and releasing it has to prove that it is safe, that it's standard
820240	827280	liability law for any product. Show me that this system, which is smarter than me, smarter than you,
827280	832480	smarter than all of us, will never do something within anticipate, something dangerous, something
832480	838160	we don't want it to do. Is this proof could at any point be possible or is it within the
838160	845280	impossibility realm of your theory? Well, I think I'm arguing that it's impossible to do so,
845280	850080	and not just because it's impossible to prove that, but it's impossible to get to that level
850080	855200	of performance. You can get progressively safer and more secure because you can look at specific
855200	861440	accept domains. You can limit what the system can do in certain situations, but you have an
861440	867760	infinite space of possible problems. So it's very hard to prove deterministically that you
867760	874320	can sit at all of them. So if AI safety would indeed be impossible, what does that imply for
874320	878880	AI safety research? Does it imply anything for AI safety research? Is that would that be a waste
878880	884560	of time or is it still something that we should pursue? Not at all. I'm doing it more than ever.
884560	890000	So think about mathematics. We know in mathematics there are many impossibility results. You cannot
890000	896480	prove certain things in general. You cannot have proofs with 100 percent confidence. It doesn't
896480	902000	stop mathematicians from discovering new beautiful mathematics. We know in physics there are limits
902000	907840	to, for example, speed, fundamental limit, you know, speed of light. That doesn't limit us from
907840	915760	doing great work on faster cars and faster rockets. It just tells you that there are limits to what
915760	920160	can be done. And so you should a, not waste your resources trying to accomplish that. Like
920160	924400	knowing that perpetual motion machines are not possible is helpful result in physics.
925200	929920	Same here. We need to understand what we can achieve and then concentrate on what is actually
929920	934160	solvable instead of trying to create magical devices which cannot work.
934160	941280	Next one. For AI to become an existential risk, it's commonly thought that it should
941280	946320	first outsmart humans. How big do you personally think the chances that this will happen at all
946960	950640	and which probabilities do your fellow AI scientists assign to this?
951520	957040	That AI will ever become as smart as humans? Exactly. Well, it's a guarantee. I mean,
958160	963760	we have proof by existence, right? If you just copy human system, you got same level.
964240	972400	We also kind of give a lot of credit to humans because we tend to think about Einstein and
972400	978960	similar type humans as typical examples. Every human is quite dumb. So it's not that hard to get
978960	991040	to that level. And how are you on timelines? Of course, you hear quite values that are quite
991040	997760	far apart. I think Elon Musk said that there could be a five-year timeline up until AGI that's
997760	1002240	on the progressive side. On the other side, there are people that claim it would take hundreds of
1002240	1009440	years. Where are you on this line and how certain are you? That's a very hard question. No one knows
1009440	1017360	for sure and no one can accurately predict something like that. But if our current theory is about
1018000	1024000	how systems scale, all right, meaning if you just add more compute, add more data,
1024000	1028880	you keep making progress, then it becomes a question of cost. How much compute are you
1028880	1037600	willing to purchase to get to that level? Do you have finite resources or what is here? $200
1037600	1043120	billion now. So maybe at that level, seven years is a reasonable estimate. With my budget, it might
1043120	1052800	be 2040. It depends on what type of resources you have. If it's also as difficult as, let's say,
1052800	1058560	Manhattan Project was, right? You need resources of a whole country to get there. It's one question.
1058560	1063200	If we discover, okay, there is a simplifying assumption, so we need a lot less resources to
1063200	1070160	drain this type of engine, a lot less compute. Maybe you can do it with a laptop in a garage and then
1070160	1076320	it becomes a lot more affordable and takes less time. So I don't have specific dates.
1076320	1083200	I would be surprised in maybe 2045 if I don't see something at human level. But
1084160	1089440	that's not important to the argument at all. Whatever it's 2045, 2070, it's still
1090320	1095280	something we need to worry about today, control and work on safety aspects of it.
1095920	1102240	I've read somewhere that there are about 70 research projects explicitly aiming for AGI at this
1102240	1108800	point. I guess the most famous two ones are DeepMind and OpenAI, at least the ones I know best.
1108800	1113920	Do you know a project that we've never heard of, but actually has a fair chance of beating those two?
1115280	1122640	Well, there could be many secret projects by secret agencies. I'm sure NSA is very interested
1122640	1127680	in processing your data more efficiently. So I'd be surprised if they don't have something good
1127680	1135760	happening. Usually, if you look at the history of what they publicly released and what we later
1135760	1140800	learned they had, I think they had public ecryptography like 30 years ahead of everyone. So
1141840	1152320	maybe already. Interesting thought. A week ago, you posted on the Facebook timeline that
1152400	1157360	I referred to already, which is quite interesting. A quote from that helped me to understand
1157360	1161920	where is the number of highly respected people who, one, argued that advanced AI is dangerous to
1161920	1166800	humanity, and two, work as fast as they can on developing advanced AI. And there were, I believe,
1166800	1171760	116 comments under your post. Have you come any closer to understanding this personally?
1173440	1179360	No, I had some good explanations and the best one, and I think that's the one Elon actually
1179360	1184720	gave himself was saying, okay, if we can't control it, I might as well be the one to get there and
1184720	1190800	I have the best chance of controlling it. People who don't care about safety have less of a chance.
1193360	1200640	But it is interesting. So a lot of very big names in arguing that AI is extremely dangerous are also
1200640	1204960	people who invested the most time and money in making it as fast as they can.
1205280	1210640	Yeah, and on a more serious note, some people might say, if AI is so dangerous, can't we just
1210640	1215680	not build it? You said something about regulation in your talk, but what would you say to them as a
1215680	1221600	general response? You can't stop progress on something so useful and so fuzzy in terms of
1221600	1227360	separation between narrow and general AI. If we could make it where, okay, you only can work on
1227360	1233200	narrow AI, but not allowed to work in general, it would be a good moratorium to have for a few years.
1233200	1237600	But the dividing line is meaningless. If you're using neural networks, they're general. If you're
1237600	1243680	using a lot of those latest evolutionary techniques, they are leading you to general solutions. So it's
1243680	1249200	simply impossible. If you make all computer science illegal, you're killing your economy,
1249200	1255600	you're shifting research to other countries. So I think I'll add another impossibility result of
1255600	1261040	unburnability of AI. You cannot ban it. You can maybe delay it at best.
1262000	1266240	It would be very interesting if you could either include or exclude it from the impossibility
1266240	1273680	space indeed, but I'm afraid that goes more into Simon Friedrich's chaos theorem, so to say.
1275440	1278960	You and I both agree, I think that AI is a significant existential risk,
1279680	1284800	but some AI researchers don't agree. And do you think there will ever be a scientific consensus
1284880	1289280	about this? And can we hope to achieve that at some point? And why could that be
1289920	1298080	either so or not? Well, I have a recent paper about AI risk skepticism, and I do a review of both
1298080	1305600	why would someone not accept the risks as real and kind of specific arguments they make for it.
1305600	1311760	I think it ended up with about 100 citations, and I have another 400 unprocessed ones. If anyone's
1311760	1319200	interested, it could be a nice survey. The most common explanation I see is just bias. If you
1320000	1327120	get your funding, your prestige, your reputation, everything from developing faster AI, it's very
1327120	1331040	hard for you to say, I'm working on the most dangerous thing in the world that will kill
1331040	1338560	everyone. So there seems to be this conflict of interest in any other domain. We wouldn't allow
1339520	1344560	for this to happen. If you are working for a tobacco company, you wouldn't be deciding if
1344560	1349360	smoking is dangerous. If you work for an oil company, we don't really trust your assessment
1349360	1357760	of impact on climate. But somehow here, it's fine. And interestingly, AI is a very large umbrella
1357760	1363920	term for lots of research sub-domains. Some people do natural language processing, some do vision.
1363920	1369040	Not everyone does safety and security, but we feel that anyone with a label of AI researcher
1369040	1375200	is qualified to pass judgment on the state of AI safety in software development. Not everyone
1375200	1380320	is a cybersecurity expert. If you're working on backend, GUI, something else, you're not going
1380320	1387840	to be consulted on how to do encryption. Why is this somehow different here? I don't fully understand.
1388800	1394800	It would be interesting indeed also to find out. I'm also kind of puzzled, but perhaps it could
1394800	1399760	have something to do with the fact that it's not a trial and error risk as one of the few
1401200	1407360	areas. I think mostly, of course, you're first developing something and then later you regulate
1407360	1411520	it, but it's only at the phase of application. So at this phase, it's much more obvious to have
1412080	1418720	separate controlling agencies, perhaps. But when you're creating something, of course,
1419920	1427920	that's not that obvious. Maybe one more question about also impossibility of AI safety, but I'm
1427920	1433760	from a different angle. I don't know if you are aware of the work of Anthony Burglas. He has
1433760	1439600	written a book about evolutionary arguments applied to AI, and it roughly goes as follows.
1440320	1445280	For superintelligence, being friendly to people is a necessary baggage. Because of evolution,
1445280	1449680	we should expect only the most efficient superintelligence to survive, and this is probably
1449680	1455280	not the friendliest one. Would you agree to this evolutionary argument applied to AI,
1455280	1459920	or what are your thoughts about this idea? I haven't read the books, so I'm trying to get the
1459920	1464720	argument from your question. So the argument is that it's more efficient to be friendly to humans,
1464720	1468480	and so it's a survival advantage. And the other way around, it's more efficient to be
1468480	1472720	unfriendly to humans, so that would be a survival advantage. Because the friendliness
1473600	1478160	would just be baggage according to him. Oh, in terms of his overhead and development,
1478160	1483600	being friendly limits your space of possibilities. Yeah, I think there is a lot to be said about
1483600	1488400	the trash rest turn option. It starts very friendly, gets the resources and help early on,
1488400	1495040	and once it's capable, it turns on us and removes all restrictions. So sounds like a good book.
1495440	1501120	I think it is, you should read it. All right, I'll put it on my list of 600 books to read,
1501120	1507520	excellent. It was marketed very poorly, so I'm not surprised that you did read it,
1507520	1513440	but I think the idea is interesting indeed. Are you more on the slow takeover or on the
1513440	1519360	fast take-off sides, and why would that be? Very fast. Once we get to human level, it goes super
1519360	1524400	intelligent almost immediately, just adding existing capabilities like infinite memory,
1524400	1529120	access to all the human knowledge, since they are already super intelligent, if you just take
1529120	1536960	human plus internet. And why do you think, I think the slow take-off side kind of
1536960	1542720	gains momentum the last years, I don't know if you agree. And I've heard that this is just because
1542720	1548000	it's more easy to write papers about this, or there are more possible stories that you could
1548480	1555680	tell about this, but do you around you see a shift there? Do you see more people going
1555680	1562880	towards the slow take-off side, or is that not true? I haven't surveyed, I honestly don't know
1562880	1568400	if you think there is a shift bias by ability to publish about it, I believe you.
1569680	1576560	I wouldn't make that claim too strictly. Okay, let's say that you're a non-AI expert and you
1576560	1582640	still want to do something about this existential risk, such as we are kind of. What action do you
1582640	1589600	think would be the best to take? So you're not an AI researcher, but you want to do something about...
1590560	1596320	Yes. Is there anything at all, or would you just say, okay, just leave it to the experts,
1596320	1602240	because there's not much you can do? I mean, in general, I think it's good if citizens are well
1602240	1607840	informed about the world and problems, and so the next time you vote, you don't vote for someone you
1607840	1617200	like visually, but actually picking better policies. It seems like based on age and experience of people
1617200	1622400	were elected, at least in the US, they're not experts on most advanced technology. I hear many
1622400	1629200	of them don't use computers, so I'm skeptical that they can keep up with crypto economics and
1629280	1635760	cryptography and synthetic biology and other interesting questions. So your job as a citizen
1635760	1640560	is to be informed and make sure your views, your informed views, have all represented.
1642800	1649440	Right, some questions from the audience to not finish off yet, but we're getting to the end of
1649440	1654800	the conference already. First one, who do you believe is responsible for the safety of AI? The
1654800	1658400	consumers, governments, or developers, or some other stakeholders?
1661040	1665680	So that's another interesting question. The ownership of AI itself is very difficult,
1665680	1671440	right? If it's self-improving, it changes, it's not even obvious who has any control
1671440	1677040	or possession over it. Obviously, the person to make it and release it has a lot of responsibility,
1677040	1682880	but if it's out there and now you are upgrading it, supplying it with goals, giving it data,
1683120	1688800	it feels like responsibility may shift to you. All of it for systems below human level
1688800	1693280	performance. It's a tool, you are in charge. The moment it's human level or beyond,
1694160	1698720	it's an independent agent. You are as responsible as you are for your adult children.
1702320	1706320	Another one that's also very interesting, I think, is it possible to program in a programming
1706320	1711520	language, not based on human language, to remove the ambiguity? Or would it be possible to have an
1711520	1716800	AI create a language without ambiguity? If the AI could create such a language, would humans be
1716800	1722720	able to learn it, or would we then also have to trust the AI to program in it? That's an
1722720	1727680	excellent question. So there is a lot of effort. First of all, every programming language is an
1727680	1732640	attempt to get away from English and into less ambiguous languages, but we know languages,
1732640	1738080	programming languages have lots of bugs. There are logical languages developed to remove
1738160	1744480	ambiguity. And I think Stephen Wolfram has a nice article about communicating with AI. And he, of
1744480	1751360	course, uses his Mathematica and models he creates in language, Wolfram language he developed as
1751360	1757920	possible solution. I think you can do way better than human language in terms of ambiguity. I'm
1757920	1766400	skeptical about bug-free communication. It relies on your existing cognitive models, your
1766400	1774080	understanding, and if you have different priors, even using well-defined terms may lead to problems.
1774080	1778320	But it's a very interesting area to do additional research. If you have
1778320	1782400	background in linguistics, I definitely invite you to look into that.
1784320	1789440	Another interesting one from Simon Friedrich, actually a previous speaker. Do you think AGI
1789440	1793520	could help overcome their global collective action problems that are at the roots of basically
1793520	1796400	all the existential risks, including those of AI itself?
1798880	1805760	So that's another great question. I see AI as a meta problem and meta solution. If we get it right,
1805760	1811680	if I'm wrong and you can make friendly superintelligence well-aligned, everything I said is just a
1811680	1817200	mistake, then it solves all the other existential problems trivially. Whatever is climate change,
1817200	1823440	synthetic bio, you have a godlike tool for solving those problems. If I'm right and it's
1823440	1829040	a terrible risk and it comes before, then it solves it by either killing all of us. We don't
1829040	1834880	have to worry about it or it comes before again. So if it takes a hundred years for climate change
1834880	1839600	to build up to boiling point, this happens in 20 years. It kind of dominates the risk.
1840480	1846720	I'm not sure about applying AI to solve the AI problem. That's a bit of a catch-22. There are
1846720	1854960	those solutions where you have a supervisory agent, AI, AGI, Nyanee, which looks after the world,
1854960	1863200	making sure no one creates dangerous AIs. I'm very skeptical of such super agents with a lot of
1864080	1870720	government control powers. I think they may be worse than what the system we're protecting against
1871360	1882400	gives us. Great answer, I think. One final question from the audience now. So if we cannot stop AI
1882400	1887120	development and we cannot totally ensure that it is safe, do we just need to accept that it is a
1887120	1891360	risk or even a big risk? Or is there anything we can do, for example, policy-wise?
1893680	1898400	So I think we need to do more research. I published those papers about a year ago and I
1898400	1903200	haven't seen a strong response from a community addressing those. If somebody just published
1903200	1907920	a paper saying, this is why you're wrong, then be very happy, but I haven't seen it. So I have to
1907920	1913040	assume that there is some merit to what I'm saying. The question is then, what do we do with our
1913040	1919360	lives? How do we update based on that? What do we change? For most people, I don't know if it makes
1919360	1924560	any difference. Before you were told, okay, you're definitely dying in 60 years. Now you may be dying
1924560	1932480	in 40. Not a big update. Figure out what to do with your 401k plan. You spend it on something now
1932480	1939920	or wait for it to become worthless later. I don't have any magical solutions or answers. I am curious
1940640	1947280	in case of successful alignment what happens to economy, what happens to work, what happens to
1947280	1955280	people's social interactions. I do have a paper which kind of assumes that progress in virtual
1955280	1963200	reality will be as good as progress in AI. And so each one of us gets what I call a personal
1963200	1968720	universe, where you basically get to do whatever you want and you don't have to negotiate with
1968720	1974560	others. There is no need for consensus. You basically have independence. So at least the
1974560	1980000	difficult part of value alignment problem is not aligning with me or you. It's hard. It's hard,
1980000	1985440	but it's not impossible. It's getting 8 billion people plus all the squirrels and whatnot to agree
1985440	1991360	on something. And this is where the personal universe solution reduces it to just now we need
1991360	1996560	to control the substrate. If you can get control of computational substrate and everyone gets
1996560	2003200	the resources to run their personal universe, okay, we're doing well. We have this virtual agreement.
2004560	2013760	I think that's not a good point again. I'm wondering also on a more personal level,
2013760	2019200	like when did you start to think about AI safety yourself and when did you move into this
2019200	2025280	research field? Was there anything that inspired you to do this and also what were the responses
2025280	2031360	that you got from fellow scientists in moving in this direction? Well, it was a very gradual
2031360	2036720	process. So I was doing research and behavioral biometrics. I was profiling poker players to see
2036720	2042880	if, you know, accounts get hacked and tell the things like that. And at the time, I realized
2042880	2048480	majority of online players are now bots. So my work started to be about detecting bots,
2049440	2054400	preventing bots from participating. But the question was, as bots get smarter and better,
2054400	2058960	can we keep up? And not just in poker, but in general online bots and automation.
2059680	2066560	I did that type of work for a while. I went to what was at the time Singularity Institute for
2066560	2071680	artificial intelligence, which was fighting hard against artificial intelligence. But
2071680	2077920	they had a lot of great ideas, which I still work on. And I've been back as a fellow and a
2077920	2083680	research advisor for Machine Intelligence Research Institute. I think they're doing excellent theoretical
2083680	2094800	work. Yeah, I think perhaps one more, like some AI researchers, I think, might be hesitant to talk
2094800	2099360	about existential risk in the public debate, like you already quickly mentioned, for example, in the
2099360	2106000	media. Do you agree that they are hesitant to do that? And why do you think that is so?
2108160	2112320	I think I lost a few words. So with media, what's the concern?
2112400	2117440	Sorry, I'll just repeat the whole thing. Some AI researchers might be hesitant to talk about
2117440	2122960	existential risk in the public debates, for example, in the media. Do you agree that this is so, that
2122960	2130240	they are hesitant to do this? And if so, why do you think that is? Well, it's a personal decision
2130240	2135520	based on your situation. So some people, before they get tenure, follow a very good advice of
2135520	2141840	be quiet. After you get tenure, never shut up again. But that's not a bad idea. You'll definitely
2141840	2147200	get someone disappointed in you. And that doesn't help your tenure case. I'm tenured, so I've been
2147200	2155040	saying stupid things for years now. What do you think about an initiative such as the existential
2155040	2162480	risk of territory? Is it useful to communicate this to more people in general, or to a certain
2162480	2168400	subset of people? Or do you think it's basically something that should be solved among researchers?
2169040	2174640	Well, if you think about developing a GI, working on superintelligence, you're really running an
2174640	2178960	experiment on all the humans, right? You've got eight billion subjects, none of them consented to
2178960	2188160	that work. The least you can do is tell them about it. That's actually great now to end this talk.
2188160	2191920	If there's no more questions from the audience, and I think we've covered those.
2191920	2204560	Yeah, and then I think we'll leave it here. It was super nice talking to you, and super nice to
2204560	2211600	listen to your short presentation. And I hope that you will also enjoy the rest of the conference
2211600	2216240	maybe tomorrow, and that will definitely be in touch and to cooperate more on this
2217120	2219840	quite hairy problem, but still very interesting one to think about.
2219840	2224080	Absolutely, and hopefully we'll meet in person one day. Likewise.
