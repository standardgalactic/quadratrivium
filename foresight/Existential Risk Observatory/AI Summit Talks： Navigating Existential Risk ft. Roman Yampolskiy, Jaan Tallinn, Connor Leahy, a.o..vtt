WEBVTT

00:00.000 --> 00:08.680
I encourage you to take a seat. We will be starting almost on time because we have a

00:08.680 --> 00:22.080
very rich agenda on a very big topic. We are talking about navigating existential risk.

00:22.080 --> 00:28.680
Navigating what people have described as a very difficult, tortuous landscape of risks

00:28.680 --> 00:38.040
that are made worse by oncoming new frontier AI. That's not just the AI that we have today,

00:38.040 --> 00:46.120
but AI that we can fairly easily imagine as coming within the next year or two. Next generation

00:46.120 --> 00:54.000
AI that's more powerful, more skillful, more knowledgeable, potentially more manipulative,

00:54.040 --> 01:01.320
potentially more deceitful, potentially more slippery, definitely more powerful than today's AI.

01:01.320 --> 01:09.480
That AI might generate existential risks in its own right. That AI is likely also to complicate

01:09.480 --> 01:17.720
existing existential risks, making some of the risks we already know about more tricky to handle,

01:18.480 --> 01:26.200
more wicked. We might also talk about the way in which next generation AI might be the solution

01:26.200 --> 01:33.800
to some of the existential risks and dilemmas facing society. If we can apply AI wisely,

01:33.800 --> 01:43.240
then perhaps we can find the narrow path through this difficult landscape. Welcome navigators in

01:43.240 --> 01:50.960
the hall. Welcome to navigators watching the live stream. Welcome to people and AIs watching the

01:50.960 --> 02:00.080
recording of this discussion. Let's get stuck in. We have lots of very capable, knowledgeable

02:00.080 --> 02:06.880
speakers who will approach this from a diversity of points of view. Indeed, I think one of the hazards

02:06.880 --> 02:12.560
in this whole topic is that some people want to be a little bit one-dimensional. They want to say

02:13.280 --> 02:18.240
this is how we'll solve the problem. It's quite straightforward. In my view, there are no

02:18.240 --> 02:23.560
straightforward solutions here, but you can make up your own minds as you listen to what all the

02:23.560 --> 02:28.480
speakers and panelists have to say. And yes, in the audience, you'll have a chance later on to

02:28.480 --> 02:34.400
raise your hand and get involved in the conversation too. The first person you're going to hear from

02:34.400 --> 02:42.080
is unfortunately not able to be with us tonight, but he has recorded a short video. He is Sir Robert

02:42.160 --> 02:49.280
Buckland, MP, former Lord Chancellor of the United Kingdom, which means he was responsible for the

02:49.280 --> 02:56.320
entire justice system here, former Secretary of State for Wales. He is still an MP and he has a

02:56.320 --> 03:04.880
side hustle as a senior fellow at the Harvard Kennedy School, where he is writing papers on

03:04.880 --> 03:11.520
exactly the topics we're going to be discussing tonight, namely how does AI change key aspects of

03:11.520 --> 03:19.440
society, potentially making it better, potentially making it much worse if we are unwise. So let's

03:20.000 --> 03:24.160
watch Sir Robert Buckland who will appear by magic on the big screen.

03:25.680 --> 03:32.160
Well, I'm very pleased to be able to join you, albeit virtually, for the Conjecture ARO Summit

03:32.160 --> 03:39.360
on AI and the challenges and opportunities that it presents us. And I think my

03:40.240 --> 03:45.120
pleasure at being with you is based upon not just my own experience in government,

03:45.760 --> 03:54.240
but also my deep interest in the subject since my departure from government last year. Now when I

03:54.240 --> 04:01.040
was in government, I had responsibility for, for many years, the legal advice that was given to

04:02.160 --> 04:07.040
departments and indeed to the government in general when I was in the Law Offices Department as the

04:07.040 --> 04:13.440
Solicitor General. And then responsibility for running the Ministry of Justice's Lord Chancellor

04:13.440 --> 04:20.000
and Secretary of State for over two years before a brief return as Welsh Secretary last year.

04:20.720 --> 04:26.000
That seven years or so experience within government as a minister gave me, I think,

04:26.000 --> 04:32.160
a very deep insight into the pluses and the minuses of the way the government works, the

04:32.640 --> 04:42.400
efficiencies and indeed the inefficiencies about process. And I think clearly, as in other walks of

04:42.400 --> 04:48.400
life, artificial intelligence, machine learning will bring huge advantages to government processes,

04:48.400 --> 04:56.320
to improve efficiency, to speed up a lot of the particular ways in which government works,

04:56.320 --> 05:01.440
which will be, I think, to the benefit of citizens, whether it's citizens waiting for

05:01.440 --> 05:07.840
passport applications, visa applications, or other government processes, benefits, for example.

05:08.480 --> 05:15.440
However, I think that we kid ourselves if we don't accept the fact that alongside the benefits

05:15.440 --> 05:22.800
come potential pitfalls. And the first and most obvious one, I think, for me is the scrutability

05:22.800 --> 05:29.200
of process. In other words, the way in which we understand how decisions are made. And that's

05:29.200 --> 05:34.560
very important, because understanding how decisions are made is part of democratic

05:34.560 --> 05:39.920
accountability in societies like ours, where individuals or organizations wish to challenge

05:40.560 --> 05:44.880
decisions made by government, perhaps through judicial review applications,

05:44.880 --> 05:51.040
then the explicability of those decisions, which is accompanied by a duty of candor

05:51.040 --> 05:58.400
by the government in order to disclose everything about those decisions, is part of that accountability.

05:58.400 --> 06:03.280
And of course, it's sometimes very difficult to explain how the machine has come to decisions.

06:04.080 --> 06:09.360
And more fundamental than that, we have to accept that if the data sets that are used in order to

06:09.360 --> 06:18.320
populate the processes are not as full of integrity as they should be, and are not the product of

06:18.320 --> 06:24.160
genuinely objective and carefully calibrated processes, then we are in danger of importing

06:24.240 --> 06:29.920
historic biases into the system, whether it's biases against neurodiverse people making job

06:29.920 --> 06:35.200
applications, or indeed biases against people of color in the criminal justice system,

06:35.760 --> 06:42.560
simply because the data sets have imported those historical anomalies, those historical imbalances.

06:43.200 --> 06:49.520
Now, all those questions have really got me thinking very deeply about the impact of machine

06:49.520 --> 06:56.160
learning on the ethics of justice itself. And as a result of my thinking, I was delighted last

06:56.160 --> 07:01.040
year to be accepted as a senior fellow at the Mosova Romani Center for Business and Government

07:01.040 --> 07:06.640
at Harvard Kennedy School. And I am working currently on a number of papers relating to

07:07.520 --> 07:14.800
the impact of AI and machine learning on the administration of justice and the law itself.

07:15.760 --> 07:23.040
It really developed from my own experience as a law chancellor, from digitalization,

07:23.040 --> 07:30.480
I should say, of the courts, when during the COVID pandemic, we had to move many, many thousands

07:30.480 --> 07:37.520
of hearings online for the first time. I think we jumped from a couple of hundred phone or online

07:37.600 --> 07:46.880
hearings to 20,000 a week in a very short compass. And the status quo will never be the same again.

07:46.880 --> 07:52.400
In fact, it has moved on, I think, in a way that we just hadn't foreseen before the pandemic. Now,

07:52.400 --> 07:57.040
I think that's a good thing. But I also think that accompanying this question about increased

07:57.040 --> 08:03.280
efficiency is the use of artificial intelligence. Now, in some jurisdictions, such as China,

08:03.280 --> 08:09.360
we are seeing its increased use not just to do legal research and to prepare cases,

08:09.360 --> 08:15.840
but to actually decide themselves. In other words, the AI judge. Now, that's all well and good.

08:16.400 --> 08:22.640
But do we actually know what populates the data sets that then forms the basis of the decisions

08:23.360 --> 08:30.560
made? And I think it's that unintentional bias or indeed worse than that potential

08:30.640 --> 08:37.200
intentional bias, whether that's influenced by a government or indeed a corporate that

08:37.200 --> 08:44.880
might be able through their financial means to influence a procedure or indeed the way in which

08:44.880 --> 08:52.480
we deal with cases, knowing as we might do more information about the way in which judges make

08:52.480 --> 08:58.560
their decisions. All these questions, I think, need to be asked now before we end up in a position

08:58.560 --> 09:04.560
where we've slept, walked into a completely different form of justice from the one that we know.

09:05.600 --> 09:10.640
Now, underpinning all of this, I think, is the need to ask a fundamental question

09:10.640 --> 09:15.840
about judgment itself. And that's what I've been doing in my first paper. You know, the essence

09:15.840 --> 09:21.440
of human judgment is something that will be based not just upon an understanding of the law, but

09:21.440 --> 09:27.360
on our experiences as human beings. And you can go right back, as I have done, to the judgment of

09:27.360 --> 09:35.360
Solomon and his emotional response to the woman who clearly was the true mother of the child that

09:35.360 --> 09:42.160
he proposed to be cut in half. Now, you know, that's an example, I think, of the human element of

09:42.160 --> 09:51.440
judgment, which has to be an essential foundation of decision making, particularly when it comes to

09:51.520 --> 09:57.760
the assessment of credibility of a witness, a human witness giving evidence upon which the case

09:58.320 --> 10:04.400
stands or falls. And of course, for judge, that applies for juries as well in criminal trials,

10:04.400 --> 10:11.360
particularly here in the UK. Now, you know, all these questions, I think, need to be asked.

10:12.160 --> 10:16.480
And then we need to work out what it is that we want to retain out of all of this.

10:16.480 --> 10:20.800
Now, I don't think we should make any cosy assumptions that because at the moment some

10:20.800 --> 10:32.080
large learning systems are having hallucinations. I don't think we should be assuming that just

10:32.080 --> 10:38.720
because of that, therefore AI will never work in a way that can achieve a greater degree of

10:38.720 --> 10:48.000
certainty. I think the inevitable arc of development will result in better and better and more

10:48.000 --> 10:54.800
capable machines. That's inevitable. But what we must be asking at the same time as capability

10:54.800 --> 11:00.080
is ensuring there is greater security and safety when it comes to the use of AI.

11:00.080 --> 11:04.880
And that really underpins, I think, the work that I'm doing in the field of justice. What

11:04.880 --> 11:11.840
does this all lead to then? Well, we have the AI safety summit in the UK next month. I very much

11:11.840 --> 11:18.800
hope that that summit will first of all involve those critical players in terms of international

11:19.920 --> 11:27.200
organisations and key countries as well that will come together to commit to creating,

11:27.200 --> 11:32.400
I think, a defined framework within which we should be using AI safely. And that framework,

11:32.400 --> 11:37.360
I think, will have to take several forms. I think in the field of justice we could do with an

11:37.360 --> 11:44.560
international framework of principles, which will ensure transparency and which can reassure

11:44.560 --> 11:50.480
people that in cases of the liberty of the individual, criminal cases, cases where perhaps the

11:50.480 --> 11:58.480
welfare of a child and the ultimate destination of a child is in issue, then the human element

11:58.480 --> 12:05.360
will be the key determinant in any decisions that are made. And that the use of machines will be

12:05.360 --> 12:12.960
transparent and made known to all the parties throughout the proceedings. And then other walks

12:12.960 --> 12:20.080
of life, I think the AI safety summit has to then look as well at whether frameworks can be created

12:20.080 --> 12:25.680
and what form they should take. I think it's tempting to try and be prescriptive. I think

12:25.680 --> 12:31.360
that would be a mistake, not just for the obvious reason that AI is developing and therefore anything

12:31.360 --> 12:37.840
that we write in 2023 will soon be out of date, but the very fact that AI itself does not mean an

12:37.840 --> 12:45.600
alloyed harm. In fact, it means a lot of benefit and also some neutral effects as well. And where

12:45.600 --> 12:52.480
you have that approach, then a principle-based system seems to me to be more sensible than

12:53.120 --> 12:58.240
overly prescriptive and detailed rules as you would have, for example, to prevent a crime,

12:58.240 --> 13:05.040
such as fraud. So just some preliminary thoughts there as to the impact of machine learning.

13:05.040 --> 13:12.320
I don't pretend to be a technical expert. I'm not. But my years in justice, my years as a lawyer,

13:12.320 --> 13:19.520
a judge and as a senior cabinet minister, I think obliged me to do some of the thinking now

13:20.000 --> 13:26.720
to help ensure that countries like Britain are in the forefront of the sensible and proportionate

13:27.360 --> 13:33.680
regulation of the use of machine learning and other types of artificial intelligence. If we

13:33.680 --> 13:40.480
don't do it now, then I think we'll be missing an unhistoric opportunity. I wish you all well,

13:41.040 --> 13:44.800
and I look forward to meeting some of you in the future and discussing these issues

13:45.520 --> 13:48.080
as they develop. Thank you very much.

13:49.920 --> 13:53.200
Well, thank you, Sir Robert, who may be watching the recording of this.

13:54.560 --> 14:00.960
Don't be prescriptive, he said. Let's sort out some sensible proportionate regulation.

14:01.840 --> 14:07.520
Is that credible? Is that feasible? You'll be hearing from other panellists who may be commenting on

14:07.520 --> 14:15.440
that shortly. So Robert also said there are risks such as the inscrutability of AI. We don't

14:15.440 --> 14:21.600
understand often how they reach its decisions. We don't understand the biases that might be there,

14:21.600 --> 14:27.120
that might have been planted. We might lose charge. We might become so used to AI taking

14:27.120 --> 14:34.240
decisions that humans end up in a very sad place. But how bad could things get? That's what we're

14:34.240 --> 14:39.920
going to hear from our next speaker. So I'm going to ask Conor Lehi to come up to the stage,

14:39.920 --> 14:47.440
who I briefly introduce him. Conor is the CEO of Conjecture. If you haven't heard about Conjecture,

14:47.440 --> 14:52.320
I think you need to do a bit more reading. Perhaps Conor will say a little bit about it. They are

14:53.200 --> 15:00.480
AI alignment solutions company, international, but with strong representation here in the UK.

15:00.480 --> 15:13.440
So welcome Conor, the floor is yours. Thank you so much. It's so great to see you all today.

15:13.440 --> 15:20.160
So happy to be able to talk to you here in person. And man, do we live in interesting times, to put

15:20.160 --> 15:29.360
it lightly. The world has changed so much. Just in the last few years, few months even, so much

15:29.360 --> 15:35.840
has happened in the world of AI and beyond. Just a mere couple of years ago, there wasn't

15:35.840 --> 15:44.000
such a thing as chat GPT, or even GPT3, or 4, or 2, or any of those. It was a different world

15:44.000 --> 15:51.200
not too long ago when technologists such as myself, weird little hobbyists, worried about

15:51.200 --> 15:59.760
the problem of AGI and how it will affect the world. Back then, it still seems so far away.

16:00.480 --> 16:08.880
It seemed like we still had time. But now, we find ourselves in a world of unrestricted,

16:09.440 --> 16:18.080
uncontrolled scaling, a race towards the finish, towards the end, to scale our AI systems ever

16:18.080 --> 16:25.760
more powerful, more general, more autonomous, more intelligent. And the reason I care about this

16:26.400 --> 16:32.480
is very simple. If we build systems that are smarter than humans, that are more capable

16:32.480 --> 16:40.080
at manipulation, deception, politics, making money, scientific research, and everything else,

16:40.880 --> 16:46.880
and we do not control such systems, then the future will belong to them, not to us.

16:48.800 --> 16:57.280
And this is not the future I want. I want a future in which humanity gets to decide its destiny,

16:57.840 --> 17:02.240
or we get to decide the future for ourselves, for our children, for our children's children,

17:03.520 --> 17:10.320
that we like. The future where our children can live long, happy lives surrounded by beauty,

17:10.320 --> 17:16.560
art, great technology, instead of being replaced by SOA's automata. And let me be clear,

17:17.760 --> 17:26.240
that this is the default outcome of building an uncontrolled AGI system, the full replacement

17:26.240 --> 17:36.240
of mankind. And what we're seeing is that AI is only exponential. There's a race.

17:36.800 --> 17:44.560
All the top organizations, which is OpenAI, DeepMind, Anthropic, among others, are racing ahead

17:45.280 --> 17:53.520
as fast as the VC dollars would scale up their work. And this has given us an exponential.

17:53.520 --> 18:00.320
AI is on an exponential curve, both on hardware and on software. It's improving at incredible rates.

18:01.120 --> 18:07.920
And when you're dealing with an exponential, there are precisely two times you can react to it,

18:08.880 --> 18:17.840
too early or too late. There is no such thing as reacting at just the right moment on an exponential,

18:17.840 --> 18:23.200
where you find just the perfect middle point, just in the nick of time, when everyone agrees

18:23.200 --> 18:29.360
that the problem is here and everything has perfect consensus. If you do this, you are too late.

18:30.320 --> 18:39.760
It will be too late. And the same thing applies to AGI. If we wait until we see the kinds of

18:39.760 --> 18:45.920
dangerous general purpose systems that I am worried about, then it will already be too late.

18:47.120 --> 18:56.000
By the moment such systems exist, the story of mankind is over. And so if we want to act,

18:56.000 --> 19:02.400
we must act well, well before such things actually come into existence.

19:03.440 --> 19:07.680
And unfortunately, we do not have much time. How the world has changed.

19:09.280 --> 19:16.720
As frightening and as terrible the race may be, there's also good changes.

19:18.000 --> 19:23.440
A few years ago, I could have barely imagined seeing governments, politicians, and the general

19:23.440 --> 19:29.600
public waking up to these weird nerd issues that I cared about so much with my friends online.

19:30.480 --> 19:35.520
But now we're looking forward to the first international AI summit convened by the UK

19:36.320 --> 19:42.720
and the famous Dip Bletchley Park. And this is great news. The European Commission has recently

19:42.720 --> 19:48.640
officially acknowledged the existential risks from AGI along with the risks from nuclear weapons

19:48.640 --> 19:55.760
and pandemics. This is great progress. This is fantastic. It is good to see our governments

19:55.760 --> 20:00.560
and our societies waking up and addressing these issues, or at least beginning to acknowledge them.

20:02.240 --> 20:07.440
And we must use this opportunity. We have an opportunity right now, and we must prevent it

20:07.440 --> 20:14.160
from being wasted. Because there's also bad news. But we're having this great opportunity

20:14.800 --> 20:18.880
to start building the regulation and the coordination necessary for a good future.

20:20.480 --> 20:26.160
The very people who are creating these risks, the very people at the heads of these labs,

20:26.160 --> 20:31.520
these organizations, building these technologies, are the very people who are being called upon

20:31.520 --> 20:36.480
by our governments to help regulate the very problem that they themselves are creating.

20:37.280 --> 20:43.440
And let me be very explicit about this. The problem that we face is not AGI.

20:44.160 --> 20:52.080
AGI doesn't exist yet. The problem we face is not a natural problem either. It is not an external

20:52.080 --> 21:00.720
force acting upon us from nature. It comes from people, from individual people, businessmen,

21:01.360 --> 21:07.520
politicians, technologists, athletes, large organizations, who are racing, who are scaling,

21:07.520 --> 21:12.560
who are building these technologies, and who are creating these risks for their own benefit.

21:14.320 --> 21:18.560
But they have offered us, these very people who are causing this problem,

21:19.280 --> 21:26.560
have offered us a solution. Fantastic. And they are pushing it as hard as they can

21:26.560 --> 21:33.120
towards the UK government and the upcoming summit. So what is the solution? The solution to the problem

21:33.120 --> 21:41.840
of scaling of these labs, these acceleration labs such as Anthropic and ARC have been pushing for.

21:41.840 --> 21:48.080
What is the solution? Well, the solution to the scaling problem is called responsible scaling.

21:48.880 --> 21:54.800
Now, what is responsible scaling, you might ask. You see, it's like normal scaling except you put

21:54.800 --> 22:03.040
the word responsible in front of it, and that makes it good. So of course I'm joking somewhat,

22:04.080 --> 22:11.760
but there's a lot of truth in humor. Responsible scaling is basically the policy

22:12.640 --> 22:17.760
and you can read this on both ARC or philanthropic's website. It's the policy proposal

22:18.320 --> 22:25.360
that we should continue to scale uninhibited until at some future time when tests and evaluations

22:25.360 --> 22:30.000
that do not yet exist and we do not know how to build, but the labs promise us they will build,

22:31.040 --> 22:37.600
detect some level of dangerous capabilities that we do not yet know, and then once it gets to that

22:37.600 --> 22:43.840
point, then they will stop, maybe, except there is a clause in the Anthropic version of the RSP

22:43.840 --> 22:50.000
paper in which they say that if a different organization was scaling even supers unsafely,

22:50.000 --> 22:58.240
then they can break this commitment and keep scaling anyways. So this could be sensible

22:58.880 --> 23:05.680
if they committed to a sensible bound, a conservative point on which to stop, but unfortunately the

23:06.640 --> 23:12.480
responsible scaling policy RSP fails to actually commit to any objective measure whatsoever.

23:13.120 --> 23:20.800
Oops. So effectively the current policy is to just keep scaling until they feel like stopping.

23:22.960 --> 23:27.760
This is the policy that is being suggested to our politicians and to the wider world

23:27.760 --> 23:35.280
as the responsible option for policy makers. It is trying to, is very clear that it is trying

23:35.280 --> 23:43.040
to recast this techno-libertarian extremist position as sensible, moderate, responsible even.

23:43.680 --> 23:52.080
Now, in my humble opinion, the reasonable moderate position to when dealing with a threat that is

23:52.080 --> 23:56.000
threatening the lives of billions of people is to simply not do that.

23:58.400 --> 24:05.760
But instead, there is trying to pass off this as the sensible middle ground position.

24:07.440 --> 24:14.240
The truth of RSP is that it comes from the same people who are causing this risk to exist.

24:15.920 --> 24:21.520
These people, the heads of these labs, many of the scientists and the policy people and

24:21.600 --> 24:28.160
the other people working on this have known about existential risks for decades and they fully admit

24:28.160 --> 24:32.480
this. This is not like they haven't heard about this. It's not even that they don't believe it.

24:33.040 --> 24:37.920
You can talk to them. They're on the record talking about how they believe that there is a

24:37.920 --> 24:46.080
significant chance that AGI could cause extinction of the entire human species. In a recent podcast,

24:46.080 --> 24:51.600
Dario Amade, the CEO of Anthropic, one of these labs, himself, said that he thinks it's a

24:51.600 --> 24:57.840
probably 25% chance that it could kill literally everybody. And they're doing it anyway.

24:58.640 --> 25:04.160
Despite this, they keep doing it. Why? Well, if you were talking to these people, what they might

25:04.160 --> 25:10.480
tell you is that, sure, you know, I know it's dangerous. I am very careful. But these other guys,

25:11.040 --> 25:15.440
well, they're even less careful than me. So I need to be number one. So I actually

25:15.440 --> 25:19.200
have to race faster than everyone else. And they all think this about each other.

25:20.880 --> 25:26.720
They call this incremental, but they never pause. They always race as fast as they possibly can.

25:27.920 --> 25:34.960
Do as I say, not as I do. There is a technical term for this. It's called hypocrisy.

25:35.840 --> 25:43.280
And RSP is no different. They are simply trying to twist words in an Oralian way

25:44.480 --> 25:48.160
to be allowed to keep doing the thing that they want to do anyways,

25:49.600 --> 25:59.360
which they themselves say could risk everybody. I mean, has responsible in the name, must be good.

25:59.360 --> 26:08.640
And people like Sam Altman talk about iterative deployment, about how we must iteratively release

26:08.640 --> 26:15.360
AI systems into the wild so societies can adapt to them, be inoculated by them. It sounds so nice.

26:15.360 --> 26:22.080
That sounds almost responsible. But if you're really trying to inoculate someone, you should

26:22.080 --> 26:29.120
let the host actually adapt before you jam in the next new pathogen into their weakened immune system

26:29.120 --> 26:35.440
as fast as you possibly can. But this is exactly what laboratories such as Open AI, DeepMind,

26:35.440 --> 26:39.840
Anthropic, and Tier 2 labs such as Meta are doing with all the force they can muster.

26:40.480 --> 26:46.160
To develop more and more new systems as fast as possible, release them as fast as possible,

26:46.160 --> 26:53.440
wide as spread possible. Now, if Open AI had developed a GPT-3 and then completely stopped

26:53.440 --> 26:58.480
further scaling, focused all of their efforts on understanding GPT-3, making it safe, making

26:58.480 --> 27:04.480
it controllable, working with governments and civil society to adapt the new problems

27:04.480 --> 27:11.200
posed by the system for years or even decades, and then they build GPT-4? Yeah, you know what?

27:11.200 --> 27:16.720
Fair enough. I think that could work. That would be responsible. But this is not what we're seeing.

27:19.760 --> 27:25.360
All of these people and all of these institutions are running a deadly experiment

27:26.080 --> 27:33.520
that they themselves think might cause extinction. It is gain-of-function research on AI,

27:33.520 --> 27:40.160
just like viruses, developed and released to the public as fast and aggressively as possible.

27:42.240 --> 27:46.880
They're developing more and more dangerous and more and more powerful viruses as quickly as

27:46.880 --> 27:56.560
possible and forcing it into everyone's immune system until they break. There is no responsible

27:56.560 --> 28:04.320
gain-of-function research for extinction-level threats. There is no such thing. We have no

28:04.320 --> 28:11.120
control over such systems and there is no responsible way to continue like this. And anyone

28:11.120 --> 28:21.520
who tells you otherwise is lying. A lot has changed. The summit can lead to many boring outcomes,

28:22.800 --> 28:27.920
just exchanges of diplomatic platitudes as is often the outcome of such international events.

28:29.600 --> 28:36.960
They have some good outcomes and it can have some very, very bad outcomes. Success in the summit

28:37.440 --> 28:43.440
is progress towards stopping the development of extinction-level AGI before we know how to control

28:43.440 --> 28:52.640
it. Most other outcomes are neutral and bad outcomes. They look like policymakers blindly

28:52.640 --> 28:59.520
and sheepishly swallowing the propaganda of the corporations to allow them to continue their

28:59.520 --> 29:06.400
unconsciously dangerous gamble for their own personal gain and glory at the expense of the

29:06.400 --> 29:15.760
entire planet. We owe it to ourselves and our children to build a good future, not gamble it

29:15.760 --> 29:25.840
all on a few people's utopian fever dreams. Governments and the public have a chance to regain

29:25.840 --> 29:31.200
control over the future and this is very hopeful. I wasn't sure we were going to get it, but the

29:31.200 --> 29:37.360
summit speaks to this, that people can act, that governments can act, that civil society can act,

29:38.080 --> 29:45.680
that it is not yet too late. There is simply no way around it. We need to stop the uncontrolled

29:45.680 --> 29:55.200
scaling, the uncontrolled race if we want a good future. And we are lucky because we can do this.

29:55.200 --> 30:02.720
We can cap the maximum amount of computing power going into these AI systems. We can have government

30:02.720 --> 30:09.200
intervene and prevent the creation of the next more dangerous, more general, more intelligent

30:09.200 --> 30:17.280
strain of AI until we are ready to handle it. And don't let anything distract you from this.

30:17.440 --> 30:25.200
There is no good future in which we continue on this path and we can change this path.

30:27.040 --> 30:32.160
We need to come together to solve these incredibly complex problems that we are facing

30:32.800 --> 30:38.400
and not let ourselves be led astrayed by corporate propaganda. And I hope that the governments

30:38.800 --> 30:45.760
and a civil society of the world do what needs to be done. Thank you.

30:58.400 --> 31:03.520
Thank you, Conor. We'll take questions from the floor in a moment. I'll just start off with the

31:03.600 --> 31:09.520
question I think maybe on many people's minds. Why would a super intelligent AI actually want

31:09.520 --> 31:14.400
to kill humans? I have a super intelligent calculator which is no desire to kill me. I

31:14.400 --> 31:19.680
have a super intelligent chess playing computer that is no desire to kill me. Why don't we just

31:19.680 --> 31:25.920
build as responsible scaling an AI that has no desires of its own? Because we don't know how

31:25.920 --> 31:32.720
to do that. Why did Homo sapiens eradicate Homer neanderthalis and Homo erectus and all the other

31:32.720 --> 31:38.000
species that we share the planet with? You should think AGI, not of as a calculator,

31:38.000 --> 31:44.320
but as a new species on our planet. There will be a moment where humanity is no longer the only

31:44.320 --> 31:51.680
or even the most intelligent species on this planet and we will be outcompeted. I don't think it

31:51.680 --> 31:57.920
will come necessarily from malice. I think it will be efficiency. We will build systems that make

31:57.920 --> 32:04.080
money that are effective at solving tasks, at solving problems, at gaining power. These are

32:04.080 --> 32:10.000
what these systems are being designed to do. We are not designing systems with human morals and

32:10.000 --> 32:15.120
ethics and emotions. They're AI. They don't have emotions. We don't even know how to do that. We

32:15.120 --> 32:20.400
don't even know how emotions work. We have no idea how you could get an AI to have emotions like a

32:20.400 --> 32:26.720
human does. So what we're building is extremely competent, completely sociopathic, emotionless,

32:26.720 --> 32:31.440
optimizing machines that are extremely good at solving problems, extremely good at gaining

32:31.440 --> 32:36.960
power, that do not care about human values or emotions, never sleep, never tire, never get

32:36.960 --> 32:43.520
distracted, can work a thousand times faster than humans and people will use these for many reasons

32:43.520 --> 32:49.200
to help and people and eventually I think humanity will just no longer be in control.

32:49.920 --> 32:54.960
Questions from the floor. There's a lady in the third drawer down here. Just wait for the mic,

32:54.960 --> 33:02.560
sorry, so that the audience online can hear you. Thank you, Susan Finnell from Finnell Consult.

33:03.920 --> 33:08.160
To stop the arms race, certainly at a geographical level, I mean in nuclear,

33:10.320 --> 33:15.040
the states and Europe can tell which countries are building nuclear weapons and what they've got

33:15.040 --> 33:22.560
and they can do tests. If computing power is a thing that needs to be capped to slow this down

33:22.560 --> 33:30.240
enough, is there a way to monitor what other countries or people in a clandestine way are doing

33:30.240 --> 33:35.760
and how does that work? This is a fantastic question and the extremely good news is yes,

33:36.880 --> 33:43.520
the at least currently, this will change in the near future, but the current state to build frontier

33:43.520 --> 33:50.560
models requires incredibly complex machines, massive supercomputers that take megawatts of energy.

33:50.560 --> 33:56.320
So this is on the order you'd have of like a nuclear centrifuge facility. So these are massive,

33:56.320 --> 34:02.160
huge machines that are only built by basically three or four companies of the world. There are

34:02.160 --> 34:07.680
very, very few companies and there is extreme bottlenecks on the supply chain. You need very,

34:07.680 --> 34:13.520
very specialized infrastructure, very specialized computer chips, very specialized hardware to

34:13.520 --> 34:18.240
be able to build these machines and these are produced exclusively by countries basically in

34:18.240 --> 34:24.640
the West and Taiwan. There is many ways where the US or other intelligence services can and already

34:24.640 --> 34:30.480
are intervening on these supply chains and it would be very easy to monitor where these things are

34:30.480 --> 34:37.040
going, who is buying them, where is energy being drawn in large scales. So it is not easy and the

34:37.040 --> 34:42.480
problem is that AI is unexponential both with hardware and with software. Eventually it will be

34:42.480 --> 34:49.200
possible to make essentially dangerous AGI on your home laptop probably, maybe not, but it seems

34:49.200 --> 34:55.840
plausible. If we get to that world, we're in big trouble. So this is part also why we have to buy

34:55.840 --> 35:01.840
time. At some point there will be a cutoff where we'll have algorithms that are so good that either

35:01.840 --> 35:07.200
we have to stop everyone from having a PlayStation at home, which doesn't seem that plausible,

35:07.840 --> 35:12.480
or at that point we have to have very good global coordination and regulation.

35:14.080 --> 35:17.840
Thanks. Just past the mic behind you, there's a person in the row behind.

35:18.720 --> 35:25.360
Robert Whitfield from One World Trust. Can I ask about Bletchley Park? Do you know, I mean are

35:25.360 --> 35:33.600
you participating and if not, do you know anybody else with similar views to you who is participating?

35:34.560 --> 35:38.400
I can't comment too much since it's closed doors. It's a very private event,

35:38.400 --> 35:42.640
unfortunately, so I don't think I have the liberty to talk about exactly what I know.

35:42.640 --> 35:48.320
I think the guest list is not public. I don't know most of the people who are coming. I know

35:48.320 --> 35:53.840
the obvious ones. All the CEOs of all the top labs, of course, are attending. It's not a secret.

35:54.800 --> 36:00.400
I don't know who, if anyone, of my reference class is attending.

36:00.480 --> 36:07.920
Just past the mic next to you, Robert. Thank you. Perhaps I can answer that question. Anybody

36:07.920 --> 36:17.280
that has read The Guardian today, there is an interview with Clifford and for the very first

36:17.280 --> 36:24.160
time, not for the second time, it has been clarified that there will be only about 100 people

36:24.160 --> 36:31.680
participating on the first day. Anybody is invited, including China, on the second day,

36:32.480 --> 36:40.720
apparently there will be only the coalition of the willing. So those who subscribe to the frontier

36:40.720 --> 36:48.000
model forum, they will sit on the second day. That's the current question. My main impression

36:48.000 --> 36:53.680
from that article generates very positive. I would say I've been surprised, as you would be

36:53.680 --> 37:02.400
surprised, that the UK government is really doing what it can to get the mission to what the title

37:02.400 --> 37:08.800
of the conference says, the AI safety summit. It's not about regulation, it's about controlling AI,

37:08.800 --> 37:14.160
and they're trying to do their best. The problem is, as outlined in that interview,

37:14.960 --> 37:21.760
is that we seem to be alone. We have the states a little bit, but the rest wants to go their own

37:21.760 --> 37:29.120
way and do it on their own territory, which is, I think, the tune. I agree. Sooner or later,

37:29.120 --> 37:33.600
international coordination around these issues will be necessary. It is as simple as that. If

37:33.600 --> 37:38.800
you want humanity to have a long, good future, we need to be able, as a global civilization,

37:38.800 --> 37:43.280
to handle powerful technologies like this. Take a question right from the back.

37:47.280 --> 37:53.040
In terms of legislation, what kind do you think is most effective? I've heard, for example,

37:53.040 --> 37:59.840
liability law takes too long to actually have an effect, and compute governance generally seems to be

38:00.400 --> 38:09.840
very easy to be called totalitarian. What do you think of legislation such as models must be released

38:09.840 --> 38:17.040
with a version before pre-processing, and there'll be attacks on the number of harmful outputs done

38:17.040 --> 38:23.920
by the model before the pre-processing? I am open to many kinds of regulation, per se. I would strongly

38:23.920 --> 38:28.000
disagree with the description of compute governance. This is like saying that, you know, not being

38:28.000 --> 38:32.320
private citizens not having nuclear weapons is totalitarian. I respectfully disagree. I'm quite

38:32.320 --> 38:36.160
happy that people do not have private nuclear weapons, and I do not think that people should

38:36.160 --> 38:43.360
have private AGI's. Similarly, I think liability is very promising. I think it has to be strict

38:43.360 --> 38:48.800
liability, so liability for developers rather than just users. This aligns the incentives of

38:48.800 --> 38:53.680
developers with those of wider society. The point of liability is to price in the negative

38:53.760 --> 38:59.040
externalities for the people actually causing them, so I'm a big fan of this. A third form of

38:59.040 --> 39:04.720
policy I would also suggest is a global AI kill switch. This would be a protocol where

39:05.520 --> 39:11.440
some number of countries or large organizations participate, and if some number of them decide

39:11.440 --> 39:17.840
to actually do this protocol, all major deployments of frontier models must be shut down and taken

39:17.840 --> 39:24.000
offline, and this should be tested every six months as a fire drill for five minutes to ensure full,

39:24.960 --> 39:29.680
so that hopefully we never need it, but if we do, that at least the protocol exists.

39:30.560 --> 39:34.640
Thank you very much. There are lots of hands up. Hold your questions. There will be more

39:34.640 --> 39:39.520
chance for Q&A later. Corner final remarks before we hand over to the next speaker.

39:40.160 --> 39:45.840
I want to really say that I do agree that it is very hopeful to see that the UK is trying to do

39:45.840 --> 39:50.400
things and is trying to push us forward in the good world, because what we really need,

39:50.400 --> 39:58.960
as I said briefly, what we need is as a civilization to mature enough to be able to handle dangerous

39:58.960 --> 40:05.360
technology. Even if we don't build AI right now, at some point we will build something so powerful

40:05.920 --> 40:10.080
that it can destroy everything. It's just a matter of time. Our technology keeps becoming more

40:10.080 --> 40:16.240
powerful. The only way for us to have a long-term good future is to build the institutions, the

40:16.240 --> 40:24.160
civilization, the world that can handle this, that can not build such things, that can not hold

40:24.160 --> 40:30.160
the trigger. I do think this is possible. I do think that it is, in fact, so I have heard,

40:30.160 --> 40:35.920
in the interest of most people, to not die. I think there is a natural coalition here,

40:36.000 --> 40:41.040
but it is hard, and I will not deny this extremely challenging problem, almost unlike,

40:41.680 --> 40:45.440
I mean, basically something we haven't faced in this nuclear proliferation, and even then it's

40:45.440 --> 40:51.360
even worse this time. It's a incredibly difficult problem. It is not over yet, but it could be

40:51.360 --> 40:57.040
very soon. If we don't act, if we let ourselves get distracted, if we fall for propaganda and all

40:57.040 --> 41:03.440
these things, these opportunities can be gone, and that will be it. But the game is not over yet,

41:03.520 --> 41:05.360
so let's do it. Thank you very much.

41:14.080 --> 41:18.960
So we've heard from a politician, a senior politician. We've heard from a technology

41:18.960 --> 41:25.760
entrepreneur and activist. We're now going to hear from a professor who is zooming in

41:25.760 --> 41:31.120
all the way from Kentucky from the University of Louisville. He's an expert. He's written several

41:31.120 --> 41:37.600
books on cybersecurity, computer science, and artificial superintelligence. Ah, Roman,

41:37.600 --> 41:44.000
I see you on the screen. I hope you're hearing us. Tell us, can we control superintelligence?

41:44.720 --> 41:49.280
Over to you. No. The answer is no. I'll tell you why in a few minutes.

41:53.440 --> 41:57.440
That's fine. So you can share your slides or talk to us whenever you're ready.

41:58.320 --> 42:01.920
Let's do the slides. Connor did a great job with

42:04.080 --> 42:09.200
his presentation. Let me see one second here.

42:13.120 --> 42:16.320
In the meantime, we can see the covers of some of your books in the background.

42:17.120 --> 42:18.240
Yes, absolutely.

42:18.240 --> 42:21.760
Security and artificial superintelligence.

42:22.400 --> 42:30.400
We're now having a slight technical issue as the technologist is found to slides. Great.

42:31.120 --> 42:35.440
Okay. Yeah, that's the hardest part. If I can get slides going, the rest is easy.

42:36.720 --> 42:39.920
Okay, so I didn't know what Connor's going to talk about.

42:41.840 --> 42:46.080
He did a great job. He's a deep thinker and covered a lot of important material.

42:46.720 --> 42:50.560
I will cover some of the same material, but I will have slides.

42:51.760 --> 42:58.320
And I will slightly take it to the next level where I may make Connor look like an optimist.

42:59.120 --> 43:05.440
So let's see how that goes. To begin with, let's look at the past.

43:06.400 --> 43:12.560
Well, over a decade ago, predictions were made about the state of AI based on nothing but compute

43:12.560 --> 43:18.720
power. Ray Kurzweil essentially looked at this scalability hypothesis before it was known as

43:18.720 --> 43:26.800
such and said by 2023, we will have computational capabilities to emulate one human brain.

43:26.800 --> 43:33.120
By 2045, we would be able to do it for all of humanity. So we are in 2023.

43:33.680 --> 43:35.920
Let's look at what we can do in the present.

43:38.160 --> 43:43.040
In the spring of this year, a program was released, which I'm sure many of you got to play with,

43:43.040 --> 43:50.400
called GPT-4, which is not a general intelligence, but it performs at a level

43:50.400 --> 43:56.800
superior to most humans in quite a few domains. If we look specifically at this table of different

43:56.800 --> 44:05.040
exams, lower exams, medical exams, AP tests, GRE tests, it's at 98, 99th percentile of performance

44:05.600 --> 44:14.560
for many of them, if not most. That is already quite impressive. And we know that there are models

44:14.560 --> 44:21.600
coming around, which are not just text models, but multi-model large models, which will overtake

44:21.600 --> 44:28.960
this level of performance. It seems like GPT-4 was stopped in its training process right around this

44:29.840 --> 44:36.560
human capacity. And if we were to train the next model, GPT-5, if you will, will quickly go

44:36.560 --> 44:42.400
into the superhuman territory. And by the time the training run is done, we would already be

44:43.280 --> 44:49.680
dealing with superintelligence out of the box. But let's see what the future holds according to

44:50.640 --> 45:00.960
heads of top labs, prediction markets. So we heard from CEO of Entropic, CEO of DeepMind.

45:00.960 --> 45:06.160
They both suggest that within two or three years, we will have artificial general intelligence,

45:06.160 --> 45:13.600
meaning systems capable of doing human beings can do in all those domains, including science and

45:13.600 --> 45:19.840
engineering. It's possible that they are overly optimistic or pessimistic, depending on your

45:19.840 --> 45:25.680
point of view. So we can also look at prediction markets. I haven't grabbed the latest slide,

45:25.680 --> 45:32.240
but last time I looked, prediction markets also had three to four years before artificial

45:32.240 --> 45:40.640
general intelligence, which is very, very quick. Why is this a big deal? This technology at the

45:40.640 --> 45:46.640
level of human capability means that we can automate a lot of dangerous malevolent behaviors,

45:46.640 --> 45:53.680
such as creating biological pandemics, new viruses, nuclear wars. And that's why we see a

45:53.680 --> 46:01.520
lot of top scholars, influential business people. In fact, thousands of computer scientists all

46:01.520 --> 46:08.720
signed this statement saying that, yes, AI will be very, very dangerous. And we need to take it

46:09.440 --> 46:17.280
with the same level of concern as we would nuclear war. So what is the problem everyone is concerned

46:17.280 --> 46:26.480
about? The problem is that, for one, we don't agree on what the problem is. Early in computer

46:26.480 --> 46:32.080
science, early in the history of AI, concerns were about AI ethics. How do we make software,

46:32.080 --> 46:37.280
which is ethical and moral? And there was very little agreement, nobody solved anything, but

46:37.280 --> 46:42.880
everyone proposed their own ethical system, gave it a name and describe what they had in mind.

46:44.320 --> 46:49.120
About a decade ago, we started to realize that ethics is not enough, we need to look at safety

46:49.120 --> 46:55.840
of those systems. So again, we started this naming competition, we had ideas for friendly AI, control

46:55.840 --> 47:02.240
problem, value alignment, doesn't really matter what we call it, we all intuitively kind of understand

47:02.240 --> 47:08.320
we want a system which if we run it, we will not regret running it. It will be beneficial to us.

47:08.320 --> 47:15.680
So how can humanity remain safely in control while benefiting from superior form of intelligence

47:15.680 --> 47:21.680
is the problem? I would like us to look at, we can call it control problem and the state of the

47:21.680 --> 47:27.360
art in this problem. In fact, we don't really know if the problem is even solvable. It may be

47:27.360 --> 47:32.240
partially solvable, unsolvable, maybe it's a silly question and the problem is undecidable.

47:32.960 --> 47:41.600
A lot of smart people made their judgments known about this, this problem. Unfortunately,

47:41.600 --> 47:49.680
there is little agreement, answers range from definitely solvable from a surprising source

47:49.680 --> 47:58.320
likely as a Riedkowski to very tractable from head of super alignment team at one of the top labs

47:58.320 --> 48:05.760
to I have no idea from a top tuning award winner who created much of machine learning evolution.

48:06.480 --> 48:10.800
So I think it's an important problem for us to look at to address and to understand

48:11.440 --> 48:17.680
how we can best figure out what is the status of the problem. My approach to that

48:17.680 --> 48:22.880
is to think about the tools I would need to control a system like that and intelligent,

48:23.440 --> 48:30.720
very capable AI and the tools I would guess I would need ability to explain how it works,

48:31.680 --> 48:38.480
capability to comprehend how it works, predict its behavior, verify if the code follows design,

48:38.480 --> 48:42.800
be able to communicate with that system and probably some others, but maybe some of the tools

48:42.880 --> 48:49.520
are interchangeable. So I did research and I published results on each one of those tools

48:50.160 --> 48:55.760
and the results are not very optimistic. For each one of those tools, there are strong limits to

48:55.760 --> 49:02.160
what is capable in the worst case scenarios. When we're talking about super intelligent systems,

49:02.160 --> 49:07.280
self-improving code systems, smarter than human capable of learning in new domains,

49:07.280 --> 49:12.000
it seems that there are limits to our ability to comprehend those systems

49:12.000 --> 49:17.360
or for those systems to explain their behavior. The only true explanation for an AI model is the

49:17.360 --> 49:24.480
model itself. Anything else is a simplification. You are getting a compressed, lossy version of

49:24.480 --> 49:30.480
what is happening in the model. If a full model is given, then you of course would not comprehend

49:30.480 --> 49:36.560
it because it's too large, too complex, it's not surveyable. So there are limits to what we can

49:36.560 --> 49:43.440
understand about those black box models. Similarly, we have limits to predicting capabilities of

49:43.440 --> 49:48.720
those systems. We can predict general direction in which they are going, but we cannot predict

49:48.720 --> 49:54.080
specific steps for how they're going to get there. If we could, we would be as intelligent as those

49:54.080 --> 49:58.880
systems. If you're playing chess against someone and you can predict every move they're going to make,

49:58.880 --> 50:04.960
you are playing at the same level as that opponent, but of course we made an assumption

50:04.960 --> 50:10.800
that a super intelligent system would be smarter than us. There are similar limits to our ability

50:10.800 --> 50:17.840
to verify software at best. We can get additional degree of verification for the amount of resources

50:17.840 --> 50:23.920
contributed. So we can make systems more and more likely to be reliable, to have less bugs,

50:23.920 --> 50:29.440
but we never get to a point of 100% safety and security. And I'll explain why that

50:29.440 --> 50:36.560
makes a difference in this domain. Likewise, human language is a very ambiguous language. It's not

50:36.560 --> 50:42.720
even as unambiguous as computer programming languages. So we are likely to make mistakes

50:42.720 --> 50:50.400
in giving orders to those systems. All of that kind of leads us to conclude that it will not be

50:50.400 --> 50:57.280
possible to indefinitely control super intelligent AI. We can trade capabilities for control, but at

50:57.280 --> 51:03.120
the end, if we want very, very capable systems, and this is what we're getting with super intelligence,

51:03.120 --> 51:09.200
we have to surrender control to them completely. If you feel that the impossibility results I've

51:09.200 --> 51:14.960
presented were just not enough, we have another paper where we cover about 50 of those impossibility

51:14.960 --> 51:23.840
results. It's a large survey in a prestigious journal of ACM surveys. From the beginning

51:23.840 --> 51:30.320
of history of AI with founding fathers like Alan Turin who said that he expects the machine

51:30.320 --> 51:38.400
will take over at some point to modern leaders of AI like Elon Musk who says we will not control them

51:38.400 --> 51:48.080
for sure. There is a lot of deep thinkers, philosophers who came to that exact conclusion.

51:49.040 --> 51:57.120
We are starting to see top labs publish reports in which they may gently acknowledge

51:57.120 --> 52:04.080
such scenarios. They call them pessimistic scenarios where the problem is simply unsolvable.

52:04.080 --> 52:10.320
We cannot control super intelligence. We cannot control it indefinitely. We are not smart enough

52:10.320 --> 52:16.880
to do it, and it doesn't even make sense that that would be a possibility. They ask, well,

52:16.880 --> 52:23.280
what's the distribution? What are the chances that we're in a universe where that's the case?

52:24.160 --> 52:32.160
They don't provide specific answers, but it seems from some of the writing and posts they make,

52:32.160 --> 52:38.720
maybe about 15% is allocated to that possibility. I was curious to see what other experts think,

52:38.720 --> 52:46.240
so I made a very small, very unscientific survey on social media. I surveyed people in my Facebook

52:46.240 --> 52:54.000
group on AI safety, and I surveyed my followers on Twitter, and it seems that about a third

52:54.560 --> 52:59.600
think that the problem is actually solvable. Everyone else thinks it's either unsolvable,

52:59.600 --> 53:04.560
or it's undecidable, or we can only get partial solutions or we will not solve it on time.

53:05.200 --> 53:10.160
So that's actually an interesting result. Most people don't think we can solve this problem,

53:10.720 --> 53:15.760
and I think part of the reason they think we cannot solve this problem is because there is a

53:15.760 --> 53:24.800
fundamental difference between standard cybersecurity safety and superintelligence safety.

53:24.800 --> 53:32.320
And cybersecurity, even if you fail, it's not a big deal. You can issue new passwords, you can

53:32.320 --> 53:38.080
provide someone with a new credit card number, and you get to try again. We suspect strongly with

53:38.080 --> 53:44.560
superintelligent safety, you only get one chance to get it right. There are unlimited dangers and

53:44.560 --> 53:51.920
limited damages, either you have existential risks or suffering risks, and we kind of agree that 100%

53:53.120 --> 54:01.680
is not an attainable level of security verification safety, but anything less is not sufficient.

54:01.680 --> 54:07.360
If a system makes a billion decisions a minute and you only make mistake once every couple of

54:07.360 --> 54:13.280
billion decisions, after a few minutes you are dead. And so this is like creating a perpetual

54:13.360 --> 54:18.800
motion machine. You are trying to design perpetual safety machine while they keep releasing more and

54:18.800 --> 54:27.120
more capable systems, GPT-5, GPT-50. At some point this game is not going to end in your favor.

54:28.160 --> 54:34.560
So I'm hoping that others join me in this line of research. We need to better understand what are

54:34.560 --> 54:42.000
the limits to controlling superintelligence systems. Is it even possible? My answer is no, but I would

54:42.000 --> 54:48.960
love to be proven wrong. It would be good to have surveys similar to the ones I conducted on larger

54:48.960 --> 54:56.720
scale to get much more statistically significant results. And in case we do agree that we have this

54:57.360 --> 55:03.520
worst case scenario where we are creating superintelligence and it is impossible to control it,

55:03.520 --> 55:10.000
what is our plan? Do we have a plan of action for this worst case scenario? This is what I wanted

55:10.000 --> 55:15.120
to share with you and I'm happy to answer any questions. Thank you very much Roman.

55:25.360 --> 55:26.480
Optimistic Roman.

55:29.280 --> 55:36.160
Sorry one second I'm trying to figure out how to use Zoom. Go ahead and repeat your question please.

55:36.480 --> 55:43.840
You gave us many reasons to be anxious. What do you think is the best reason for us to be optimistic?

55:44.880 --> 55:50.960
Well there seems to be many ways we can end up with world war three recently so that can slow down

55:50.960 --> 56:01.040
some things. It has been suggested that we can use a different kind of tool which is the kill switch.

56:01.680 --> 56:07.280
Your list of tools that you listed it didn't include that. It's been proposed that each AI system

56:07.280 --> 56:14.240
should be tested with a remote off switch capability. Have you looked at that? Do you think that's a

56:14.240 --> 56:22.800
viable option? So I would guess a superintelligence system would outsmart our ability to press the

56:22.800 --> 56:30.720
off button in time. It will work for not superintelligent AI's pre-GI systems maybe even for

56:30.720 --> 56:36.640
the GI systems but the moment it becomes that much more advanced I think it will outsmart us. It will

56:36.640 --> 56:42.080
take over any kill switch options we have. Let's have some questions from the floor.

56:45.840 --> 56:49.760
I can't see the hands so yes just give the microphone out thank you.

56:50.720 --> 56:57.200
Thank you. I would like to ask how does the scalable oversight that open AI is working on

56:57.200 --> 57:03.600
essentially the way they plan to align superintelligence fit into your expectation of the

57:03.600 --> 57:11.040
future pathway the AGI will take because again as personally we cannot align or control a super

57:11.040 --> 57:16.880
intelligent entity but another AI which is more capable than us could. So how does that fit into

57:17.440 --> 57:23.680
your expectations? So it seems like it increases complexity of the overall system instead of us

57:23.680 --> 57:28.960
trying to control one AI. Now you're trying to control a chain of agents going from slightly

57:28.960 --> 57:34.320
smarter to smarter to superintelligent maybe 50 agents in between and you're saying that you have

57:34.320 --> 57:40.880
to solve alignment problem between all the levels communication problem ambiguity of language between

57:40.880 --> 57:48.720
all those models supervision. It seems like you are trying to get safety by kind of upfuscating

57:48.720 --> 57:54.560
how the model actually works you're introducing more complexity hoping to make the system easier

57:54.560 --> 58:00.960
to control that seems counter-intuitive. But isn't it the case that sometimes you can verify an answer

58:00.960 --> 58:05.680
without understanding the mechanism by which the answer was achieved for example there can be a

58:05.680 --> 58:10.000
chess puzzle and you have no way of working out yourself but when somebody shows you the answer

58:10.000 --> 58:14.960
you can say oh yes this is the answer. So isn't it possible we don't need to really understand

58:14.960 --> 58:20.480
what's going on inside these systems but a simpler AI can at least verify the recommendations that

58:20.480 --> 58:27.200
come out of the more complex AIs. So such a chain may be the solution. Can you claim that you are

58:27.200 --> 58:31.840
still in control if you don't understand what's happening and somebody just tells you don't worry

58:31.840 --> 58:39.120
it's all good I checked it for you? But then it's like we humans we have a network of trust

58:39.200 --> 58:44.880
and I trust some people and they trust others within various categories we can't work out

58:44.880 --> 58:51.600
everything ourselves but we trust some scientists or some engineers or some lawyers who validate

58:51.600 --> 58:57.200
that an AI has a certain level of capability and that AI could come back with verification that

58:57.200 --> 59:03.200
the proposals of a superintelligence should be accepted or should not be. I don't say it's easy

59:03.200 --> 59:06.880
but as you said there's not likely to be a very simple and straightforward solution.

59:07.840 --> 59:14.640
Again to me at least it sounds like instead of trying to make this system safe you said that

59:14.640 --> 59:19.600
you made some other system safe and it made sure that the system you couldn't make safe is safe for

59:19.600 --> 59:26.720
you. Let's take some more questions there's another one in the middle here and then we'll go to the

59:26.720 --> 59:35.520
edge yes thank you. Thank you for the presentation. Number one second thing is that as you're talking

59:35.600 --> 59:41.120
about I think as David was talking about trust basically right could you tell me from your

59:41.120 --> 59:47.120
extensive years of AI research and experience as such that do you really think that humans or

59:47.120 --> 59:55.520
society can be trusted to for example regulate its own self or do you think that really need

59:55.520 --> 01:00:02.000
some sort of institution of sort that is totally separate from anyone else?

01:00:02.160 --> 01:00:12.880
So I'm not sure regulation would be enough Connor correctly pointed out that there is both lobbying

01:00:12.880 --> 01:00:19.760
of regulators by the labs and also it becomes easier and easier to train those models with less

01:00:19.760 --> 01:00:26.240
compute and over time you will be able to do it with very little resources. The only way forward

01:00:26.320 --> 01:00:34.400
I see is personal self-interest if you are a rich young person and you think this is going to kill

01:00:34.400 --> 01:00:39.600
you and everyone else maybe it's not any best interest to get there first that's really the

01:00:39.600 --> 01:00:46.000
only hope at this point just personal self-interest. The humans are always better if we can band

01:00:46.000 --> 01:00:50.320
together with our self-interest rather than each of us individually pursuing our self-interest so I

01:00:50.320 --> 01:00:55.760
think this kind of meeting and the community spirit might help. There was a hand over here

01:00:55.760 --> 01:01:07.760
yes with I think the red shot on jacket. If we assume that the two kind of well both views that

01:01:07.760 --> 01:01:14.240
have been suggested so far are correct in that we're definitely not going to be able to stop

01:01:14.960 --> 01:01:20.240
AI development etc and we're going to get to the point where we have no regulation that can

01:01:20.240 --> 01:01:24.560
effectively stop things you know people can build in super intelligent AI on their own computers

01:01:24.560 --> 01:01:29.200
etc okay so we'll assume that that's a fact that's coming and then we'll also assume that

01:01:29.200 --> 01:01:33.520
the control problem isn't a problem because it's a problem that can't be solved and we're definitely

01:01:33.520 --> 01:01:38.400
not going to be able to control it well now we're heading and barreling towards the point where we

01:01:38.400 --> 01:01:43.600
have super intelligent AIs definitely and we definitely can't control them. What comes next?

01:01:45.200 --> 01:01:51.840
What comes next? It's a wonderful question as I said and published you cannot predict what the

01:01:51.840 --> 01:02:01.360
super intelligent system will do. All right so was there a question down here? Thank you.

01:02:02.640 --> 01:02:08.880
You said that we kind of need a plan but on that last question if that scenario is true

01:02:09.760 --> 01:02:14.320
you said we need to do more work in this area but do you have any thoughts as to what we

01:02:14.320 --> 01:02:17.520
should be doing what we should be doing to plan for the worst-case scenario?

01:02:19.040 --> 01:02:26.320
So to me at least it seems that at least in some cases it is possible to use this idea of personal

01:02:26.320 --> 01:02:31.680
self-interest if you have a young person having a good life there is no reason why they need to

01:02:31.680 --> 01:02:37.040
do it this year or next year. I understand that someone may be in a position where they are

01:02:37.040 --> 01:02:41.680
very old very sick have nothing to lose and it's much harder to convince them not to try

01:02:42.240 --> 01:02:48.320
but at least from what I see the heads of those companies are all about the same age they young

01:02:48.320 --> 01:02:57.440
they healthy they they have a lot of money there is a good way to motivate them to wait a little bit

01:02:57.440 --> 01:03:04.160
maybe a decade or two just out of personal self-interest again. I think my answer to the question

01:03:04.160 --> 01:03:10.160
of optimism is that we humans can do remarkable things we humans can solve very hard problems

01:03:10.160 --> 01:03:16.720
and so I want to say now that we spread around what the problem is at least some more people can

01:03:16.720 --> 01:03:24.880
apply more brain power to it so that's my reason for optimism. Terry? I guess I'm pleased by the

01:03:24.880 --> 01:03:31.840
inevitability of this development because it seems to me that if you're going to create

01:03:32.400 --> 01:03:39.520
reasoning creatures then those reasoning creatures are going to have moral rise on the same

01:03:39.520 --> 01:03:49.760
plane as human beings so I'm looking forward to to chatting with these creatures and joining in

01:03:50.320 --> 01:03:54.640
them joining into this kind of discussions and I'm pleased that they won't be able to be thwarted

01:03:54.640 --> 01:04:01.200
and it will be wrong to enchain these reasoning creatures. So Roman are you looking forward

01:04:01.280 --> 01:04:08.000
to having more of the AIs involved in these discussions as well? So I remember giving a

01:04:08.000 --> 01:04:16.640
presentation for a podcast about rights for animals rights for AIs and I was very supportive of all

01:04:16.640 --> 01:04:21.920
the arguments developed because I said at one point we will need to use those arguments to beg

01:04:21.920 --> 01:04:30.560
for our rights to be retained. The question on the third row here? Yes hi I'm curious Roman

01:04:31.920 --> 01:04:38.480
which side of in your hopes of a possible future for us to get through this do you have more hope

01:04:38.480 --> 01:04:44.720
on the side of a more top-down sort of totalizing control system for AGI systems so should they

01:04:45.440 --> 01:04:52.960
remove the possibility of individual actors getting hold of this and weaponizing it or do you put

01:04:52.960 --> 01:05:01.200
more hope in a more sort of decentralized open-source approach to AGI emergence more like an ecology

01:05:01.200 --> 01:05:06.240
perhaps some people suggest would be more biologically inspired such that you know immune

01:05:06.240 --> 01:05:13.040
immune system like functions could arise which way do you lean in your sensibilities for what is

01:05:13.040 --> 01:05:20.880
a viable avenue for us? I'm not optimistic with either of those options the only kind of hope I

01:05:20.880 --> 01:05:27.440
see is that for strategic reasons superintelligence decides to wait to strike it will not go for

01:05:27.440 --> 01:05:33.440
immediate treacherous turn but decides to accumulate resources and trust and that buys us a couple

01:05:33.440 --> 01:05:39.200
of decades that's the best hope I see so far. So we slow things down we'll have more chance to

01:05:39.200 --> 01:05:44.880
work out solutions and the slowing down might come from a combination of top-down pressure

01:05:44.880 --> 01:05:51.520
and bottom-up pressure maybe have a is there a hand at the very back there yes let's try and get the

01:05:51.520 --> 01:06:03.760
microphone back there right at the sitting at the back yes sorry at the in the middle

01:06:09.680 --> 01:06:15.760
thanks. Hi Roman thanks for your talk yeah I was wondering what your thoughts are on

01:06:16.400 --> 01:06:23.280
aligning the first AGI that is human level or narrowly superhuman if in principle that is possible

01:06:23.920 --> 01:06:29.920
and if that is is is it possible in principle to align the next version of AGI

01:06:31.360 --> 01:06:38.640
but to use that narrowly superhuman AGI to align it and if if that's all technically

01:06:38.640 --> 01:06:48.640
possible then why would we not think like focus on doing that and also and also if you think in

01:06:48.640 --> 01:06:58.720
principle alignment is impossible and control is impossible then why why not work on practical

01:06:58.720 --> 01:07:06.000
ways to make the whatever AGI is created as nice as possible that is like better than the

01:07:06.000 --> 01:07:14.400
counterfactual of try to stop it it won't stop and you know it won't be nice. Well I definitely

01:07:14.400 --> 01:07:21.120
encourage everyone to work on as much safety as you can anything helps I would love to be proven

01:07:21.120 --> 01:07:26.640
wrong it would be my greatest dream that I'm completely wrong and somebody comes out and says

01:07:26.640 --> 01:07:32.400
here's a mistake in your logic and we have developed this beautiful friendly safe system

01:07:32.400 --> 01:07:38.080
capable of doing all this beneficial things for humanity that would be wonderful but so far I

01:07:38.080 --> 01:07:43.760
haven't seen any progress in that direction what we're doing right now is putting lipstick on this

01:07:43.760 --> 01:07:49.600
monster and the show that's all we're doing filters to prevent the model from disclosing its true

01:07:49.600 --> 01:07:56.320
intentions then you talk about alignment it's not a very well-defined terms what values are you

01:07:56.320 --> 01:08:03.600
aligning it with values of heads of that lab values of specific programmer we as humans don't agree

01:08:03.600 --> 01:08:09.760
on human values that's why we have all these wars and conflicts there is a 50-50 split and most

01:08:09.760 --> 01:08:16.560
political issues in my country we are not very good at agreeing even with ourselves over time

01:08:16.560 --> 01:08:23.360
what I want today is not what I wanted 20 years ago so I think this idea of being perfectly aligned

01:08:23.360 --> 01:08:30.240
with eight billion agents and people are suggesting adding animals to it and aliens and other AIs that

01:08:30.240 --> 01:08:37.680
doesn't seem like it's a workable proposal our values are changing they're not static and it's

01:08:37.680 --> 01:08:45.520
very likely that they will continue changing after we get those systems going I don't see how at any

01:08:45.520 --> 01:08:51.920
point you can claim that the system is specifically value aligned with someone in particular the last

01:08:51.920 --> 01:08:58.640
question in this section is going to go to Connolly he Roman love your talk I always love your

01:08:58.640 --> 01:09:05.520
optimism it's always great to hear you talk so so I'm kind of like going to pick up on the question

01:09:05.520 --> 01:09:10.240
I was just asked and just give a bit of my opinion and kind of like here would you think about this

01:09:10.240 --> 01:09:17.280
as well so my personal view is that I I do I have read many of your papers in fact and they're

01:09:17.360 --> 01:09:23.520
quite good so I do think that I agree with you that like in principle an arbitrarily

01:09:23.520 --> 01:09:28.800
intelligent system cannot be safe by any arbitrary like weaker system just kind of a proof of like

01:09:28.800 --> 01:09:37.760
you know program size induction and whatnot but in my view it does seem likely that there is a

01:09:38.720 --> 01:09:44.240
limit of intelligence far below the theoretical optimum but still significantly above the human

01:09:44.240 --> 01:09:51.840
level that can be achieved the reason I think this is that human civilization is actually very

01:09:51.840 --> 01:09:58.320
smart compared to a single caveman and can do really really great things so my point of optimism

01:09:58.320 --> 01:10:05.760
is it seems possible that if we stop ourselves from making self-improving systems and coordinate

01:10:05.760 --> 01:10:10.000
at a very strong scale and have very strong enforcement mechanisms it should be possible to

01:10:10.000 --> 01:10:16.960
build systems that are you know n steps you know above human good enough to build you know awesome

01:10:16.960 --> 01:10:24.160
you know sci-fi culture ship kind of like worlds but not further I'm wondering if you have an

01:10:24.160 --> 01:10:32.240
intuition about like where do things hit impossibilities like to me I think the impossibilities happen

01:10:33.200 --> 01:10:39.840
above human utopia but to get to the utopia a bit you already have to do extremely strong

01:10:39.840 --> 01:10:44.640
coordination extremely strong safety research extremely strong interpretability extremely

01:10:44.640 --> 01:10:48.800
strong constraints on the design of the agis extremely strong regulation which are things in

01:10:48.800 --> 01:10:52.560
principle possible wondering kind of like your thoughts about that kind of outcome so con is

01:10:52.560 --> 01:10:58.240
not asking about responsible scaling he's asking about limited superintelligence if we had limited

01:10:58.400 --> 01:11:03.120
superintelligence could we get everything we want without having the risks that we all fear

01:11:04.080 --> 01:11:09.360
so I think I want to emphasize difference between safety and control is it possible to

01:11:09.360 --> 01:11:15.840
create a system which will keep us safe in some somewhat happy state of preservation possible

01:11:15.840 --> 01:11:22.160
a way in control no that system is the example you give of humanity so humanity provides pretty

01:11:22.160 --> 01:11:27.520
nice living for me but I'm definitely not in control if I disagree with society and many issues

01:11:27.520 --> 01:11:34.080
in politics and culture it makes absolutely no difference I don't decide things scale it to the

01:11:34.080 --> 01:11:39.840
next level all eight billion of us may want something but this overseer this more intelligent

01:11:39.840 --> 01:11:44.800
system says it's not good for you we're not gonna do it this is what you're going to be doing right

01:11:44.800 --> 01:11:50.240
now so think about all the decisions you make throughout your day you decided to eat this

01:11:50.240 --> 01:11:55.520
doughnut you smoked with cigarette all those decisions were made by you because you felt

01:11:55.520 --> 01:12:00.480
you wanted to do them they may be good or bad decisions but if you had this much more intelligent

01:12:01.040 --> 01:12:06.560
personal advisor ideal advisor you would be at the gym working out eating carrots you may have a

01:12:06.560 --> 01:12:15.360
long healthy life but you're not in control and your happiness level may be questionable thank

01:12:15.360 --> 01:12:23.280
you very much roman for sharing your thoughts pessimism and some optimism thanks for moving

01:12:23.280 --> 01:12:30.640
the conversation forwards

01:12:33.840 --> 01:12:38.640
I'm now going to invite the five five members of the panel to come up on stage

01:12:38.640 --> 01:12:42.960
and they're each going to have a couple of chances to pass some comments and what they've heard

01:12:43.520 --> 01:12:49.520
so there's some stairs over there which you can come up to we're gonna hear from

01:12:49.600 --> 01:12:57.280
Jan Tallin who is the co-founder of Skype the co-founder of fli future of life institute

01:12:57.280 --> 01:13:04.800
and also CISA the center for study of existential risks we're going to hear from Eva Berens a

01:13:04.800 --> 01:13:10.160
policy analyst with the international center for future generations we're going to hear from Tom

01:13:10.160 --> 01:13:16.400
Oh who's a journalist who writes from time to time for the BBC amongst other places we're going to

01:13:16.400 --> 01:13:25.520
hear from alexandra moussa visit evidence who is the CEO of evident and has a track record with

01:13:25.520 --> 01:13:30.000
tortoise media in many other places and we're going to hear from also another representative

01:13:30.000 --> 01:13:37.680
from conjecture that Andrea Miotti who is their specialist for AI policy and governance so to

01:13:37.680 --> 01:13:42.480
start things let's just hear from each of them a few opening remarks Jan what's your comments

01:13:42.480 --> 01:13:46.640
from what you've heard so far have you changed your mind in any ways or all the things that are

01:13:46.640 --> 01:13:54.640
missing from the conversation now you all have to speak into the mics I'm being told so I yesterday

01:13:54.640 --> 01:14:01.360
I was at the dinner I was invited to a dinner and and my response to an invitation was that okay I

01:14:01.360 --> 01:14:08.000
will come but you have to invite Connor because he's making very similar points to me only much

01:14:08.000 --> 01:14:16.160
much more intensely so yeah basically I agree I agree with what what Conor said my main caveat

01:14:16.160 --> 01:14:23.440
would be that for the last decade or so I've been kind of trying to build a lot of friendly

01:14:24.240 --> 01:14:33.840
cooperation between people in the AI companies and making sure that like everybody can

01:14:33.840 --> 01:14:40.400
understand that it is in their interests with almost everybody let's be honest almost everybody

01:14:40.400 --> 01:14:47.040
understands that is in their interests to and of remaining control and not kill everyone else

01:14:48.560 --> 01:14:56.640
and so like for example I am a board observer observer to entropic and entropic is one of

01:14:56.640 --> 01:15:01.680
those companies just like conjecture when you go there you can talk to anyone from the receptionist

01:15:02.160 --> 01:15:09.520
to the CEO and they are aware of the AI risk I'm very concerned about this but yes I do think as I've

01:15:09.520 --> 01:15:16.160
said in several places that I don't think they should be doing what they're doing

01:15:17.760 --> 01:15:21.600
so these companies don't really want to do what they're doing but they feel they have to otherwise

01:15:21.600 --> 01:15:29.120
they might be left behind so yes so there is this a dilemma in when you want to do a safe AI

01:15:30.000 --> 01:15:35.440
one is that you're well safe like when you're trying to figure out how to do safe AI

01:15:36.960 --> 01:15:42.960
from one hand you have groups like Miri that the Aliezer Kowsky co-founded and that was the person

01:15:42.960 --> 01:15:50.160
who got me involved in AI safety 15 16 years ago where basically the claim is that you have to start

01:15:50.160 --> 01:15:55.120
really early even if you don't know exactly what the AI is going to look like because then you have

01:15:55.120 --> 01:16:01.120
a lot of time to prepare and then the group on the other end of that axis is entropic where they

01:16:01.120 --> 01:16:04.800
say that it's kind of useless to start early because you don't know what you're dealing with

01:16:05.440 --> 01:16:10.160
so you need to be as informed as possible so in that strategy you need to be just always

01:16:10.160 --> 01:16:14.240
at the frontier and Dario has been very public about this about this strategy of course the

01:16:14.240 --> 01:16:20.400
problem there is that like it also works as a perfect justification to raise rates so therefore

01:16:20.400 --> 01:16:29.120
it's have like double digit uncertainty both ways about what the actual picture is and so I

01:16:29.120 --> 01:16:34.160
do think that this point the labs indeed they are involved in death rates and they think there is

01:16:34.160 --> 01:16:39.360
that government intervention needed to get a time out there and we definitely need time out because

01:16:39.360 --> 01:16:50.240
we don't have enough safety results and but to yeah Romania Polsky's presentation I'm definitely

01:16:50.240 --> 01:16:55.360
more optimistic again as on one of these slides there was the dialogue held with Eliezer and

01:16:55.360 --> 01:17:02.000
Eliezer was confident that this can be sold and in fact like I'm super glad that earlier this year

01:17:03.360 --> 01:17:09.440
David Tarempel his group got UK government funding and he has this approach called

01:17:10.400 --> 01:17:16.400
open agency architecture I don't know exactly what the details there are but like my rough

01:17:16.400 --> 01:17:26.320
understanding is that you're using you're scaling AI capabilities and access according to formal

01:17:26.320 --> 01:17:34.000
statements that AI is produced and then you use not AI not humans but formal verifiers to verify

01:17:34.640 --> 01:17:41.840
those those statements therefore like building up your AI capabilities one formally verified

01:17:41.840 --> 01:17:46.560
step at the time there are many criticism of that but this is like one of those approaches that is

01:17:46.560 --> 01:17:55.280
kind of at least and principle has like some convincing story that that why it should work in

01:17:55.280 --> 01:17:59.680
in principle at least so there are some options that might work but we're going to need time

01:17:59.680 --> 01:18:04.480
to develop them exactly so that's why I've been working on like I've been supporting AI safety

01:18:04.480 --> 01:18:10.000
research for more than a decade now but unfortunately we just didn't make it we now need more by more

01:18:10.000 --> 01:18:17.200
time so let's hear from Eva because you work more with possibilities to inspire policy you've seen

01:18:17.200 --> 01:18:25.600
examples of policy in the past slowing down some technological races are you do you see reasons

01:18:25.600 --> 01:18:30.320
for optimism do you see ways in which politicians can make a good difference to the landscape we're

01:18:30.320 --> 01:18:36.240
discussing definitely definitely that very much plays into some of the problems or the issues

01:18:36.240 --> 01:18:39.840
characteristics of the problem that both corners spoke about and that also

01:18:39.840 --> 01:18:43.360
Jan Talion just mentioned that one of the problems so we're facing here is a human

01:18:43.360 --> 01:18:47.920
coordination problem and one of the ways to address that will be through policy as has been

01:18:47.920 --> 01:18:54.560
said many times this evening this is a technology that threatens to kill us to kill us all and the

01:18:54.560 --> 01:18:59.440
heads of the government of the companies that are driving forward the technology have agreed that

01:18:59.440 --> 01:19:04.480
and publicly stated that that might be the case and yet they seem to be locked into this dilemma

01:19:04.480 --> 01:19:09.520
that Jan just mentioned where they are for some or other reason impossible to to stop so I think

01:19:09.520 --> 01:19:13.680
that is a point where where government can really make a difference and step in and also

01:19:13.680 --> 01:19:19.920
should step in and we've seen that as you hinted at we've seen that work in the past one of the

01:19:19.920 --> 01:19:27.520
examples that I often think about is the Montreal Protocol which after the scientific consensus

01:19:27.520 --> 01:19:33.920
arose that CFCs and other similar gases actually destroy the ozone layer the international community

01:19:33.920 --> 01:19:41.680
did come together in 1987 and agreed through the Montreal Protocol to slowly phase out these gases

01:19:41.680 --> 01:19:47.440
so we see here that international cooperation by the international community by governments can

01:19:47.440 --> 01:19:52.960
succeed also in the face of the short-term economic interest of private sector companies

01:19:52.960 --> 01:19:58.560
in the public interest of well in the end and everyone on earth so I'm not saying that it's

01:19:58.560 --> 01:20:04.160
necessarily easy or easy but I think it's definitely possible and it is one of the strongest

01:20:04.160 --> 01:20:08.480
levels that we have here to make a difference so I think that's definitely something that we

01:20:08.480 --> 01:20:14.400
should lean into very strongly and do our best that that actually happens. The Montreal Protocol

01:20:14.400 --> 01:20:19.200
is an encouraging example but we haven't made a very good job of the governments in the world of

01:20:19.200 --> 01:20:24.080
controlling carbon emissions we've been talking about it for a long long time and maybe there's

01:20:24.080 --> 01:20:28.480
some progress but many people feel this is an example where governments can't cooperate

01:20:28.480 --> 01:20:34.160
so what makes you think that we can cooperate with the problems of AI more like the Montreal Protocol

01:20:34.160 --> 01:20:39.680
rather than the Paris Agreement to say. Well part of this of course is also that I think that this

01:20:39.680 --> 01:20:43.840
is one of the few levels that we have to make a difference at all so I also hope that we will be

01:20:43.840 --> 01:20:50.560
able to do it and I agree that looking at past climate conference is one of the negative examples

01:20:50.560 --> 01:20:54.800
that we see there is that with these conferences sometimes that the outcomes tend to be very

01:20:54.800 --> 01:20:59.840
watered down just because the focus lies on building consensus among all of the different

01:20:59.840 --> 01:21:04.240
countries that attend and then in the end you want to have a nice little consensus agreement

01:21:04.240 --> 01:21:08.160
that everyone signs so you can demonstrate that everyone's on the same page and everyone goes

01:21:08.160 --> 01:21:13.520
home and everyone's happy and I can just say that I think with the UK AI summit that's coming up now

01:21:13.520 --> 01:21:17.840
first of all that is a unique opportunity to actually have international cooperation

01:21:17.840 --> 01:21:22.160
and coordination on this issue take place you need to create the opportunities for stuff like

01:21:22.160 --> 01:21:28.400
that I'm really happy that the UK government took the initiative and created this opportunity I am

01:21:28.400 --> 01:21:32.480
one thing that makes me optimistic is that we all know that China is going to attend at least on one

01:21:32.480 --> 01:21:38.160
of the days so hopefully they will be able to be brought into the fold and yeah then I just hope

01:21:38.160 --> 01:21:43.520
that this opportunity is truly taken and that the outcome of this summit will not be just some vague

01:21:43.520 --> 01:21:49.120
commitments to long-term plans but ideally concrete binding commitments to to concrete

01:21:49.120 --> 01:21:56.960
next steps let's turn to Alexandra Alexandra you work a lot with businesses businesses are

01:21:56.960 --> 01:22:03.040
unsure in many ways how to deal with today's AI do you think there is good advice that they can

01:22:03.040 --> 01:22:09.280
be given or is there a sleepwalking process with many of our businesses definitely the latter

01:22:09.520 --> 01:22:18.160
I would say so I'm CEO of evident we map benchmark companies on how far they are in their AI

01:22:18.160 --> 01:22:25.760
adoption and so when I think it was Connor you mentioned the AI race that is on at the

01:22:25.760 --> 01:22:32.000
frontline of development in AI there is also a race as we all know going on in terms of

01:22:32.560 --> 01:22:38.480
adopting AI as as quickly as possible there's a sense of being there's a sort of geopolitical

01:22:38.560 --> 01:22:47.520
debate on AI development between US Europe China and so on and who's leading on that

01:22:47.520 --> 01:22:53.040
not only in AI but also in areas like quantum but in the business level which is where I deal with

01:22:53.040 --> 01:22:59.120
spend my time mostly there's a definitely a race on in terms of not being left behind in

01:22:59.120 --> 01:23:05.760
adoption of AI and it's an economic question it is a existential question so there's an existential

01:23:05.760 --> 01:23:13.760
question on sort of two dimensions in this debate and and so you've got this unstoppable

01:23:14.720 --> 01:23:19.680
race going on on the front end of AI and then you've got an unstoppable race on

01:23:20.240 --> 01:23:26.960
actual deploying AI at a business level and it's going to be very hard for regulators to keep up

01:23:26.960 --> 01:23:33.920
and to Eva's point I think in terms of what we hope will be the outcome often unfortunately comes

01:23:33.920 --> 01:23:41.120
with with the catastrophic happening taking place before it really sharpens the minds and

01:23:41.120 --> 01:23:47.760
people figure out how urgent it is I think there's a real sense of urgency in in the community around

01:23:47.760 --> 01:23:52.800
trying to work out what what the guardrail should be whether it should be a constitution

01:23:54.000 --> 01:24:00.720
or how we should think about implementing safety mechanisms in as as we develop further

01:24:00.720 --> 01:24:07.280
on our chat on our large language models but I hope it doesn't need a catastrophic moment

01:24:07.280 --> 01:24:13.520
for that to sharpen but back to the business question there is this hope that maybe businesses

01:24:13.520 --> 01:24:20.720
will self-regulate and I think that is maybe the case in highly regulated sectors you see in the

01:24:20.720 --> 01:24:27.040
banking sector and insurance or banking in particular that there is a guardrails put in

01:24:27.040 --> 01:24:32.720
place there but that is that there's a lot of businesses that don't have that regulation

01:24:33.280 --> 01:24:38.560
around them and I think there is a real risk for this completely running out of control

01:24:38.560 --> 01:24:45.040
at a business level as well. Would you advise businesses to self-regulate ahead of standards

01:24:45.040 --> 01:24:49.920
and regulations being agreed by governments? I think that's what they're doing or trying some

01:24:49.920 --> 01:24:57.920
businesses are trying to do there's a big risk in in the case of winning trust with your customers

01:24:57.920 --> 01:25:06.000
and also your your shareholders and and investors if you mishandle AI and you create issues around

01:25:06.000 --> 01:25:14.800
not taking into account how to properly deal with biases and other issues that is a situation that

01:25:14.880 --> 01:25:20.080
can create a real breakdown in trust with your with your organization so there is that risk

01:25:20.640 --> 01:25:27.680
and then there are businesses that don't necessarily lean on trust for their for their for their

01:25:27.680 --> 01:25:35.520
business and those are the ones I worry the most about. Indeed let's turn to Tom Oh as a representative

01:25:35.520 --> 01:25:40.960
of the world of journalism do you feel journalists have helped the discussion about the existential

01:25:40.960 --> 01:25:46.800
threat from AI or have they muddied the water leading people to panic unnecessarily or perhaps get

01:25:46.800 --> 01:25:55.520
distracted on side issues rather than the main issue? I think it's all of the above aside from

01:25:55.520 --> 01:26:00.560
overrugging the pudding I think most people in this room including me have had a wit scared out of

01:26:00.560 --> 01:26:10.560
them by some of the talks just now. One has side issues in journalism coverage of AI and I think

01:26:11.280 --> 01:26:19.040
the jobs market is one of those but I have been surprised pleasantly so by how things have progressed

01:26:19.040 --> 01:26:25.600
since 2016 and that's the first time that I wrote about AI safety and I think at that point the

01:26:25.600 --> 01:26:32.000
prospect of a bad scenario relating to AI was seen as about as likely by my colleagues as

01:26:32.000 --> 01:26:38.560
Leicester City winning the Premier League. Anyway several years later I now see lots of my former

01:26:38.560 --> 01:26:45.200
colleagues writing to my mind very informed pieces about AI safety and I think that's helped the

01:26:45.200 --> 01:26:51.680
public change well arrive at a view and probably a lot of people in this room are aware that the

01:26:51.680 --> 01:26:57.520
American public when polled now says that they want regulation of AI and they want a lot of it

01:26:57.520 --> 01:27:03.040
and I think we can credit journalism with some of that. Journalism should be doing more but

01:27:03.040 --> 01:27:07.920
it's more than I would have thought a few years ago. And if you were to go away and write up a

01:27:08.000 --> 01:27:12.480
story about things that you might have changed your mind about tonight and that the public

01:27:12.480 --> 01:27:16.080
should pay attention to can you give us a sneak preview what that would include?

01:27:18.320 --> 01:27:26.480
Well I think the idea of runaway AI is not new but I think it has been difficult historically to

01:27:26.480 --> 01:27:33.440
frame it in a way that really sticks and like really drives its way down your brainstem and we

01:27:33.440 --> 01:27:40.000
have different ways of framing AI risk and Mustafa Suleiman's new book which some of you

01:27:40.000 --> 01:27:46.640
might have read I think there's a pretty good job of framing it in a way in which he describes

01:27:48.240 --> 01:27:55.360
AI being used to accelerate human ingenuity in whatever endeavours humans are up to be they

01:27:56.320 --> 01:28:02.400
be they good or be they bad that's one way of framing it and I think we've heard some pretty

01:28:02.960 --> 01:28:08.000
compelling ways of telling a story of runaway AI which is a different and scarier story.

01:28:09.680 --> 01:28:16.400
Thanks and let's turn to Andrea and your role at Conjecture. What are you doing in a day-by-day

01:28:16.400 --> 01:28:22.960
basis to address this question? Well what we're trying to do and I mean kind of current a lot of

01:28:23.760 --> 01:28:29.920
first of all to explain the problem to people I've been heartened by the public reaction in

01:28:29.920 --> 01:28:36.400
the last years like I also got to know about this problem quite a long time ago and I in the past I

01:28:36.400 --> 01:28:41.520
could almost not expect the day that major governments take this problem seriously and the

01:28:41.520 --> 01:28:48.960
public understand this problem and we all get together and take some initial promising insufficient

01:28:48.960 --> 01:28:57.280
but promising steps to address it. Another thing is figuring out policy solutions and the reality

01:28:57.280 --> 01:29:04.480
is that we don't obviously we don't have a playbook for what exactly they look like but what I think

01:29:04.480 --> 01:29:14.480
was a common theme of the talks tonight is that clearly at some level of power we are not in

01:29:14.480 --> 01:29:22.960
control anymore and everybody expects this. Those who don't expect this are misguided or

01:29:23.840 --> 01:29:32.480
expected but don't say it and the positive thing is that there is one physical resource that drives

01:29:32.480 --> 01:29:38.080
the majority of what makes this system powerful which is computing power and it's a physical

01:29:38.080 --> 01:29:44.240
resource not not like you know algorithms that you could just write on a piece of paper it's

01:29:44.960 --> 01:29:52.480
traceable it's expensive large place in data centers and while you know the scaling hypothesis

01:29:52.480 --> 01:29:56.560
the idea that you know the more computing power you put into something the more powerful it becomes

01:29:56.560 --> 01:30:02.560
might hit some diminution in terms of at some point we do not see any reason to expect it to stop

01:30:03.280 --> 01:30:09.680
so we know you know from both sides companies know that more computing power leads to more power

01:30:09.680 --> 01:30:15.120
and that's why they're doing what they're doing we know that limiting that computing power is a

01:30:15.120 --> 01:30:24.480
very effective way to kind of stem the the bleeding and stop and or pause the situation for a while

01:30:24.480 --> 01:30:29.840
take a time out have the time to figure out the solutions have the time to absorb this into society

01:30:30.240 --> 01:30:34.800
but how much time will that give us because there's a risk that people will use today's models

01:30:35.360 --> 01:30:41.680
to design much more efficient ways to build next generation models and so they could therefore

01:30:41.680 --> 01:30:48.880
come under the radar as it were that people who were watching for large use of GPUs would miss

01:30:48.880 --> 01:30:54.480
the clever way that somebody has built it so do we have a decade do we have three or four years

01:30:54.560 --> 01:30:57.440
or how long yeah that's that's a great question it's

01:30:58.640 --> 01:31:03.280
capping computing power is not a permanent solution but it's one of the best solutions we have

01:31:04.160 --> 01:31:10.160
at the moment uh as others have said before we are in a double exponential it's not a single

01:31:10.160 --> 01:31:15.600
exponential we have an an exponential growth of computing power hardware and exponential

01:31:15.600 --> 01:31:25.120
improvement in software we need to start cutting down on one of the two uh cutting down a compute

01:31:25.120 --> 01:31:31.920
depends where you put the cap probably will buy us five seven years you can make you can make

01:31:31.920 --> 01:31:37.440
what would seem to people at the frontier extremely strong caps that would affect you know

01:31:37.440 --> 01:31:42.960
less than 20 companies in the world that probably could buy you 10 years in that period we need to

01:31:43.440 --> 01:31:48.000
figure out all of the rest it's going to be a hard problem but we have done it before with

01:31:48.000 --> 01:31:51.840
nuclear weapons we've done it before with biological weapons we can do it again we're

01:31:51.840 --> 01:31:56.080
going to go around the panelists one more time in the same order i'll give you a chance a choice

01:31:56.080 --> 01:32:00.720
panelist you can either comment on what you've heard from somebody else or you can paint me a

01:32:00.720 --> 01:32:06.240
picture of what would be a successful ali safety summit in bletchley park if things go well what

01:32:06.240 --> 01:32:12.800
would be the outcome and what would also be the follow-up so jan first well i'm the one of the

01:32:12.800 --> 01:32:19.360
authors of the post letter so it's like indefinite moratorium uh on further scaling uh would be sort

01:32:19.360 --> 01:32:26.080
of my wet dream from outcome from from this summit or perhaps the next one if this one isn't realistic

01:32:26.960 --> 01:32:32.080
and what's the chance do you think what what might cause the assembled world leaders to

01:32:32.080 --> 01:32:36.720
have an intellectual breakthrough and say yes actually we do need to have this indefinite pause

01:32:37.440 --> 01:32:44.560
so currently i'm not very optimistic on on that uh perhaps perhaps but perhaps in six months it

01:32:44.560 --> 01:32:51.200
would be much more clearer why this is needed so and and we have more time to gonna do the

01:32:51.200 --> 01:32:55.920
necessary loving so the discussion is prepared to ground and when something really bad happens

01:32:55.920 --> 01:33:00.560
in six months when gpt five comes out and oh my god at least we'll know what we should be doing

01:33:00.560 --> 01:33:06.000
yeah i mean like let's not forget that gpt chat gpt has been out less than one year so

01:33:06.560 --> 01:33:12.640
like the world was very different one year ago same question to you either yeah thank you i think

01:33:12.640 --> 01:33:18.160
i'm just going to build on top of jan's ideal outcome of the summit and say that i would also

01:33:18.160 --> 01:33:24.880
find it terrific if the summit could be the first in a series of repeated um summits like this where

01:33:24.880 --> 01:33:29.520
world leaders come together because as we i think a pretty clear picture has been painted tonight

01:33:29.600 --> 01:33:35.200
of the fact that the field of ai evolves very quickly and is going to continue to evolve very

01:33:35.200 --> 01:33:40.720
quickly if not ever quicker and because of that i think it would be very valuable if we would have

01:33:41.520 --> 01:33:46.160
a regular occasion for world leaders to come together and not only make sure that the rules

01:33:46.160 --> 01:33:51.120
that they came up with are upheld but also to reevaluate whether they still make sense and

01:33:51.120 --> 01:33:55.360
where they need to be adapted or whether new real rules need to be introduced as for example

01:33:55.360 --> 01:34:00.320
measures like compute control that andrea mentioned um they buy us some time but at some point they

01:34:00.320 --> 01:34:05.120
might not be applicable anymore so not just agreement on rules but setting up some audit

01:34:05.120 --> 01:34:11.200
process so that we can figure out whether the rules are being forward or not for example yeah same

01:34:11.200 --> 01:34:15.920
question to you i would agree you have to build in i mean right now it's just based especially in the

01:34:15.920 --> 01:34:22.720
us um the talks that have been held in the white house and by chuck schumer um the gatherings have

01:34:22.720 --> 01:34:30.400
led to sort of ideas around voluntary um adherence to some principles but there is absolutely no

01:34:30.400 --> 01:34:36.800
built-in audit or accountability um so i think that we've got to see that come out of of the

01:34:36.800 --> 01:34:41.680
uk's ai safety summit among other things maybe there has to be something more concrete around

01:34:41.680 --> 01:34:49.120
licensing um of the models and and the use of them and that they have to pass some kind of a

01:34:49.120 --> 01:34:55.760
threshold i think the risk of of bad actors getting hold of them is is a is a much higher risk

01:34:56.400 --> 01:35:02.960
i think the ia ea structure is is one that one can look at but the the nuclear a lot of the

01:35:02.960 --> 01:35:11.680
success probably of the ia ea lies in the mutual assured destruction of humanity by using um nuclear

01:35:11.680 --> 01:35:18.720
weapons and this might be the same situation but they're easier to monitor i think um i think this

01:35:18.720 --> 01:35:24.400
might be slightly harder because you can land in the hands of bad actors more easily we haven't

01:35:24.400 --> 01:35:29.280
really discussed bad actors much in this session tonight maybe that makes the things even more

01:35:29.280 --> 01:35:35.200
horrifying we might come back to that later and tom what's your answer what would you like to see

01:35:35.200 --> 01:35:38.400
come out of the summit or maybe you've got some comment on something else you've had from the

01:35:38.400 --> 01:35:48.880
other speakers well i'll talk about the summits um often when CEOs of um labs developing agi

01:35:50.080 --> 01:35:54.240
are asked about regulation um they basically say bring it on we'd love some

01:35:54.240 --> 01:35:59.920
regulation um and i think it would be great if politicians could actually put that to the test

01:36:00.240 --> 01:36:10.800
very good and andrea what would you like to see if you were invited to bletchley park and given the

01:36:10.800 --> 01:36:17.600
microphone for two minutes what would you entreat the assembled world leaders to consider well i

01:36:17.600 --> 01:36:22.560
don't want to be too hopeful as some others here have been but at the very least i would like to

01:36:22.560 --> 01:36:29.440
see a commitment to the fact that this is extremely dangerous technology continuing to scale leads to

01:36:29.440 --> 01:36:36.080
predictable disaster and we need to pull on the brakes right now uh we have a lot of applications

01:36:36.080 --> 01:36:42.400
are very beneficial we can focus on those but limit this death race to the ever more powerful

01:36:42.400 --> 01:36:48.160
ever more obscure general systems that we can control what i definitely do not want to see

01:36:49.280 --> 01:36:55.440
is a diplomatic shake of hands where companies write their own playbook and say we're gonna keep

01:36:55.520 --> 01:37:02.560
doing exactly what we we're doing right now but it's gonna sound responsible and governments can

01:37:02.560 --> 01:37:07.200
wash their hands and say well we did our part let's move on that would be a very bad outcome

01:37:08.320 --> 01:37:13.280
right i'm gonna ask for three questions from the floor i'm going to get the panel to think

01:37:13.280 --> 01:37:17.760
which one's the answer we're trying to take people haven't asked before so on the second row here

01:37:17.760 --> 01:37:24.240
there's a hand up here and then also the second row over there next as well let's take three

01:37:24.240 --> 01:37:29.600
fairly short questions please hi first of all thank you very much for coming here tonight

01:37:29.600 --> 01:37:35.360
and sharing your expertise with all of us in this whole question this whole discussion there's the

01:37:35.360 --> 01:37:42.080
implicit assumption that agi is coming and it's coming soon and the million dollar question i guess

01:37:42.080 --> 01:37:49.440
is when exactly is it coming but a more practical question is what are some warning signs and do

01:37:49.440 --> 01:37:54.720
we already see some of those in the systems that we have currently deployed great let's

01:37:54.720 --> 01:37:59.680
have a question over here as well sorry the microphones can have to run around at the end of

01:37:59.680 --> 01:38:08.800
the second row there thank you for great panel my question is related to yon's comment at the

01:38:08.800 --> 01:38:16.880
beginning on essentially dario amadeus philosophy which is you know and and also related to i guess

01:38:16.880 --> 01:38:23.520
roman's talk which is how do we solve the control problem and what i've heard the large a our labs

01:38:23.520 --> 01:38:30.800
repeat is oh you know we need to increase capability it's only better ai that is going to be able to

01:38:30.800 --> 01:38:37.120
help us figure out how to solve a and there's sort of this race to increase capability up to the

01:38:37.120 --> 01:38:44.880
point that can help us solve it but no further and and and just sort of thoughts on that philosophy

01:38:44.880 --> 01:38:52.320
and and and and you know whether there might be something to it or is it just a completely risky

01:38:52.320 --> 01:38:59.120
game you know thanks and there was one in the middle of the third row there pass the microphone

01:38:59.120 --> 01:39:06.800
along please to the middle how long of a time frame do you think we have between the arrival of agi

01:39:06.800 --> 01:39:12.640
and the arrival of superintelligence and within that time frame could there be tractable solutions

01:39:12.640 --> 01:39:18.320
for alignment or the control problem and if so would those solutions be able to be implemented

01:39:18.320 --> 01:39:24.240
before hurry up and develop better ai third question was how long might it take between

01:39:24.240 --> 01:39:31.040
the arrival of agi and superintelligence and whether there would be time for us to work

01:39:31.040 --> 01:39:36.960
out solutions then and my question i guess is well what's all this about agi isn't the bletchley park

01:39:36.960 --> 01:39:42.080
summit set up to discuss something else which is frontier models which says that there are

01:39:42.080 --> 01:39:49.040
catastrophic risks even before we get to agi so i'm going to go around the same order again

01:39:49.040 --> 01:39:55.760
i'll be a bit predictable jan you want to pick one of these questions maybe i mean answer to all the

01:39:55.760 --> 01:40:02.640
all three questions is uncertain that's that's why we need to pause and kind of take a time out

01:40:02.640 --> 01:40:08.240
and see like how can we kind of create more set more certainty about these things i think i would

01:40:08.240 --> 01:40:15.040
answer the anthropic question specifically that definitely is a lot of truth to the to the point

01:40:15.040 --> 01:40:21.600
that like the more capable model you have to work with the more kind of better position you are in

01:40:21.600 --> 01:40:29.280
particularly you can you can kind of be do like do science in a way that you just can't do with

01:40:29.360 --> 01:40:36.720
models from from 10 years ago and also like one claim that people and tropic to make is that like

01:40:36.720 --> 01:40:42.880
in some ways it becomes easier as the model kind of has better understanding of what you're trying to

01:40:42.880 --> 01:40:51.520
do with it or to do to it but that said again it's a to put it lightly it's playing with fire so

01:40:51.520 --> 01:40:57.760
so it's i'm not sure if anyone should be doing it either and jan said it's all uncertain but

01:40:57.760 --> 01:41:02.320
can't we at least agree in advance some canary signs that will make us say things are happening

01:41:02.320 --> 01:41:08.800
faster than we expected well i mean if we look at the past there were several signs that people

01:41:08.800 --> 01:41:15.360
agreed on that they might point out that we're getting into a zone where ai is maybe more capable

01:41:15.360 --> 01:41:22.240
than we think it is and i mean we certainly have seen signs connor mentioned or was it roman

01:41:22.240 --> 01:41:28.480
mentioned that the current models um i'll perform most humans on things like the bar exam

01:41:28.480 --> 01:41:34.320
um these are clearly advances in capabilities that um i almost wonder sometimes if we just

01:41:34.320 --> 01:41:39.520
become desensitized to them because we move so fast i mean again charge epd came out a couple

01:41:39.520 --> 01:41:45.120
months ago and it's already just normal and people are waiting okay what's the next big thing so um

01:41:45.120 --> 01:41:51.120
it doesn't really help um to think retroactively have them in any signs um if you didn't take them

01:41:51.120 --> 01:41:55.920
to actually stop and reconsider what you're doing so i think one of the big problems here

01:41:55.920 --> 01:42:01.920
is not have there been signs a big problem is can we pre-commit to stopping when we see certain

01:42:01.920 --> 01:42:06.320
signs and then actually stop or actually take certain actions and we just haven't seen that before

01:42:06.960 --> 01:42:12.400
so this is developing contingency solutions like we're meant to have had contingency solutions

01:42:12.400 --> 01:42:20.400
for pandemics yeah yeah any comments alizandra i i will leave the um well how long it's going to

01:42:20.400 --> 01:42:27.360
take to reach agi to to the experts on the panel but on the outcome of the summit and i think there

01:42:27.360 --> 01:42:35.680
is a bit of a confusion sometimes in in in what we are expecting to be achieved from the discussions

01:42:35.680 --> 01:42:42.480
on regulation because there's an obvious very important urgent and existential question around

01:42:42.480 --> 01:42:48.400
regulation regulating for the long term but then we also have businesses that are sitting and waiting

01:42:48.400 --> 01:42:54.880
for regulation that is here now how is it going to impact my particular sector how is it going to

01:42:54.880 --> 01:43:01.280
impact what i'm doing today and what are the immediate and very very real risks right now

01:43:01.360 --> 01:43:08.400
here today that we are seeing um with ai having impact on you know media with disinformation

01:43:08.400 --> 01:43:14.880
and so on but then there's also the specific um aspects to how that is implemented in particular

01:43:14.880 --> 01:43:22.000
sectors so i um hope that we would see addressing both of those short term and long term questions

01:43:22.000 --> 01:43:29.280
thanks tom any thoughts yeah on yardsticks i think it's worth remembering that the canonical

01:43:29.280 --> 01:43:35.680
yardstick was the turing test um and that's long gone um ai's can now beat humans at um

01:43:35.680 --> 01:43:42.960
diplomatic based games for instance um and much more um the modern turing test is i think quite

01:43:42.960 --> 01:43:49.200
an interesting proposition um and that's the test of whether the ai can i think make a million dollars

01:43:49.200 --> 01:43:56.560
very quickly um but as as eva says we must stop shifting the goalposts um we need to agree that

01:43:57.040 --> 01:44:01.760
the one is we should pick one agree that that's the one where we start taking it seriously and

01:44:01.760 --> 01:44:07.600
then take it seriously when it is passed which it will be quite soon but in the past people said

01:44:07.600 --> 01:44:13.360
you won't manage to solve chess unless you have got a full grasp of all aspects of creativity and

01:44:13.360 --> 01:44:18.400
so on and then when deep blue did win at chess people said oh well it's not actually doing it in

01:44:18.400 --> 01:44:23.360
the way that we thought would be so terrible it's just grunting out incredibly so i feel

01:44:23.360 --> 01:44:28.320
there will always be people who don't move the goalposts but they'll say well how it was implemented

01:44:28.320 --> 01:44:34.960
it doesn't demonstrate too intelligent yeah i think that's a good point um and it reminds us

01:44:34.960 --> 01:44:42.880
something um conna said um which is that there won't be consensus at the time to act so we need

01:44:42.880 --> 01:44:49.440
to be able to build a coalition of the willing uh andrea what's your views on these questions

01:44:50.080 --> 01:44:56.400
yeah maybe answering the last one first on isn't a summit about frontier ai well much like with

01:44:56.400 --> 01:45:02.720
goalposts it feels a bit like terminology is being shifted all the time sometimes quite willingly

01:45:02.720 --> 01:45:08.560
by the companies building this uh you know in in the in the old days people used to talk about

01:45:08.560 --> 01:45:16.640
superintelligence or friendly ai or strong ai then became agi then recently the frontier term was a

01:45:16.640 --> 01:45:22.720
kind of open ai entropic rebrand of oh no like we're not well we're gonna get to very powerful

01:45:22.720 --> 01:45:28.160
ai system soon but it's frontier which sounds better than agi because people are getting

01:45:28.160 --> 01:45:34.320
concerned about agi you know in practice do these terms matter not too much what matters is how

01:45:34.320 --> 01:45:42.080
competent systems are basically all of these companies expect to build systems that outperform

01:45:42.080 --> 01:45:46.560
humans at most tasks definitely most tasks you can do behind a computer in the next

01:45:47.280 --> 01:45:56.080
two to five years um these matches the trends that we see in performance and compute growth this

01:45:57.280 --> 01:46:02.800
is very worrying uh these these are these are levels of competence uh at which we expect the

01:46:02.800 --> 01:46:08.880
systems to be out of our control unless we have various solutions so we just need to deal with

01:46:08.880 --> 01:46:17.040
that we can call it frontier ai agi proto agi proto superintelligence it's just terminology

01:46:17.040 --> 01:46:21.760
what matters is how powerful they are and how ready we are to deal with them we must avoid the

01:46:21.760 --> 01:46:27.440
serious discussions getting sidelined into semantics which is often very frustrating we're

01:46:27.440 --> 01:46:30.880
going to take three more questions we're going to go around the panel in the reverse order next

01:46:30.880 --> 01:46:37.440
time i'm going to take questions for people who haven't answered asked before so somebody in white

01:46:37.520 --> 01:46:43.040
about halfway down if you have asked a question before please don't put your hand up just now so

01:46:43.040 --> 01:46:50.960
we have more chance just there yes thank you three questions one from each thanks um let's say that

01:46:50.960 --> 01:46:57.280
in 20 years we somehow managed to get it right and humanity still exists um despite the development

01:46:57.280 --> 01:47:03.120
of these a di what do you think is one essential piece of regulation or development that has to

01:47:03.120 --> 01:47:10.320
have happened together it's a great question uh where else with the hands uh where are the

01:47:10.320 --> 01:47:20.640
microphones there's one about just on the other side thank you so you've spoken quite a lot about

01:47:20.640 --> 01:47:24.880
like what government should do what companies should do uh i'm interested in like what should

01:47:24.880 --> 01:47:29.920
ordinary people do like what can we be doing to get our voices heard in this you know should we

01:47:29.920 --> 01:47:33.920
be protesting i've edited international protest on the 21st is this thing we should be doing or is

01:47:33.920 --> 01:47:39.200
this a terrible idea it's another fine question is there a question from a woman it's about time we

01:47:39.200 --> 01:47:48.880
heard from the other gender another gender hands up yes somebody put your hand up to whoever it was

01:47:51.120 --> 01:47:58.640
yes there's one okay there's one there and we'll take you as well we'll take four right um as

01:47:58.720 --> 01:48:03.920
agi has been developed to be more human do you think it's possible to have forms of agi that

01:48:03.920 --> 01:48:08.800
don't have the inherent geopolitical biases that come with the data sets that we currently have

01:48:08.800 --> 01:48:14.400
and how do you think we go about developing regulations that aren't formed by human conscious

01:48:14.400 --> 01:48:23.760
bias okay and so if we can get the mic over here as well two one two three four five rows back

01:48:23.760 --> 01:48:34.320
just at the edge yes over there yeah i miscounted perhaps yeah four sorry quick question um so i

01:48:34.320 --> 01:48:39.760
actually work in the automotive industry and we have to certify vehicles and engines and it is

01:48:39.760 --> 01:48:46.480
an uphill battle um you can spend years just trying to get a windshield wiper right um or a

01:48:46.480 --> 01:48:52.960
temperature sensor right and i'm just curious um if you think that there would be an ability

01:48:52.960 --> 01:49:00.400
to take people who have regulated and certified products around global markets and how difficult

01:49:00.400 --> 01:49:06.320
that is and create a summit where that expertise could come together from different industries

01:49:06.320 --> 01:49:12.240
and we could roll up our sleeves and say okay this is how the structures go and we know what

01:49:12.240 --> 01:49:18.000
works we know what goes slow and try to accelerate that learning because i think that voice we have

01:49:18.000 --> 01:49:23.440
so much experience in the world right now um with that sleeves rolled up we know what it's

01:49:23.440 --> 01:49:28.800
like to sit in those test labs or send 30 000 pages of documents in with verification and

01:49:28.800 --> 01:49:33.360
validation data we know how to do requirements engineering requirements design requirements

01:49:33.360 --> 01:49:38.800
and i'm just wondering if um there's been any discussion of that to pull you know pull a summit

01:49:38.800 --> 01:49:45.120
together from people from heavily regulated industries four great questions first of all

01:49:45.120 --> 01:49:49.760
20 years later it succeeded how did we get it right what were the regulations that made the

01:49:49.760 --> 01:49:56.960
difference what should ordinary people be doing can we design AI that is free from some of the

01:49:56.960 --> 01:50:04.000
human biases the geopolitical biases that cause strife among humans and can we learn from the

01:50:04.000 --> 01:50:09.600
people who are professionally involved in doing regulations and certification in multiple industry

01:50:09.680 --> 01:50:16.560
rather than just to being naive in our own applications so Andrea first yeah maybe i will

01:50:16.560 --> 01:50:21.680
answer the question about can we learn about highly regulated industries definitely i think there is a

01:50:21.680 --> 01:50:29.680
big kind of problem of uh arrogance in AI or like willful arrogance of just thinking that

01:50:29.680 --> 01:50:35.360
this sector should be special and people should be absolutely free to do any experiments they want

01:50:35.360 --> 01:50:41.200
all the time use you know as much computing power as they want try the worst possible applications

01:50:41.200 --> 01:50:46.000
all the time fully open source on the internet and nobody can complain like very often people in

01:50:46.000 --> 01:50:50.880
the eye sector get very very angry when somebody tells them look well maybe what you just did

01:50:52.080 --> 01:50:57.520
should be regulated and industries we don't do it like that like with drugs we don't just let

01:50:59.840 --> 01:51:04.480
pharmaceutical companies just release and test the drugs on billions of people

01:51:05.040 --> 01:51:09.440
and have their CEOs say oh there's a 20 percent chance you will die if you take this drug but you

01:51:09.440 --> 01:51:14.400
know don't it's okay like if it happens you can let us know and then we'll we'll stop maybe right

01:51:14.400 --> 01:51:20.400
so we can totally learn from that it would be great to learn from that there is one challenge

01:51:20.400 --> 01:51:26.480
which is that we don't understand current systems that well so it makes things like

01:51:27.440 --> 01:51:32.560
auditing them and evaluating them quite tricky because we simply don't know how they work internally

01:51:32.560 --> 01:51:37.680
as well but we can do many other things and we can definitely learn from highly regulated

01:51:37.680 --> 01:51:43.840
industries and definitely given the risks admitted by the companies themselves at the frontier the

01:51:43.840 --> 01:51:50.880
approach should be highly regulated industry not so it is different but not completely different

01:51:50.880 --> 01:51:58.560
and we can indeed learn tom closing words from you well i'll take the the question about what

01:51:58.640 --> 01:52:04.320
ordinary people should do and i have two immediate thoughts one is that it's very important to keep

01:52:04.320 --> 01:52:13.520
this issue apolitical the other is that lawmakers need a sense of legitimacy i think in order to

01:52:14.400 --> 01:52:21.520
come up with regulation and to bring it in through acts and bills and so on a good example of when

01:52:21.520 --> 01:52:26.800
this happened a bit too slowly was it the outset of covid and when it was a fringe issue there were

01:52:26.800 --> 01:52:32.240
no enough rules then the public got involved and suddenly the rules arrived a little too late but

01:52:32.240 --> 01:52:39.680
they did arrive and how can ordinary people achieve this i think ordinary people i i don't have a

01:52:39.680 --> 01:52:46.000
theory of protest so i won't comment on that but i think it's important that we all keep this in

01:52:46.000 --> 01:52:50.080
the public conversation i suppose what my answer is really tending towards is you should all read

01:52:50.080 --> 01:52:57.280
lots of journalism about ai click on my articles thanks alessandra any of these questions catch

01:52:57.280 --> 01:53:02.800
your attention i think um your question in the orange sweater there is is um is is definitely

01:53:02.800 --> 01:53:09.760
where we're headed i mean there's got to be some kind of um system that resembles either the car

01:53:09.760 --> 01:53:17.920
industry or fda and um the way that we certify um our you know products generally speaking

01:53:17.920 --> 01:53:25.360
and i i just don't know how we get from from that to something that is very difficult to trace

01:53:25.360 --> 01:53:31.680
and to to monitor as as ai but i would say to the gentleman's question in the white shirt there if

01:53:31.680 --> 01:53:39.360
we're looking 20 years down the road and we say that's really great in the uk november 2023 we

01:53:39.360 --> 01:53:46.320
we were able to put in place regulation that somehow created traceability um so we could we

01:53:46.320 --> 01:53:52.240
could work out sort of where the where they were where systems were running out of control or

01:53:52.240 --> 01:53:58.000
landing in the hands of bad actors that would be a huge success i think that the reality is a bit

01:53:58.000 --> 01:54:02.400
different and that is that it probably is going to resemble a bit more the world in which cyber

01:54:02.400 --> 01:54:10.160
security um flourishes and that means you're constantly trying to create um a dam system or a

01:54:10.800 --> 01:54:18.080
deflection of all sort of incoming um activities that are not great so i know that none of these

01:54:18.080 --> 01:54:23.840
are perfect analogies but i think it is in in that universe we're probably going to be operating in

01:54:23.840 --> 01:54:31.840
for a while thanks final words either sure so i would love to touch on two questions very briefly

01:54:31.840 --> 01:54:37.200
one of them um being i think your question in the white shirt what uh policies will bring us to the

01:54:37.200 --> 01:54:42.480
safe world in 20 years and i think um a policy that was mentioned today as well already but that

01:54:42.480 --> 01:54:48.320
i want to touch on again is um strict liability regimes just simply to kind of shift the incentive

01:54:48.320 --> 01:54:53.520
systems um incentive structures that drive private companies to take certain actions that are not in

01:54:53.520 --> 01:55:01.920
the interest of um the wider general public so i think there we can um really um shift shift the

01:55:01.920 --> 01:55:07.360
incentive structure to move companies to take um maybe different paths forward and then what can

01:55:07.360 --> 01:55:12.240
the the average person the general public do i would completely agree with tom i think um one

01:55:12.240 --> 01:55:17.840
thing that that really would help is to for lack of a better expression to just make noise just make

01:55:17.840 --> 01:55:23.120
sure that this topic is um talked about publicly you can do this in different ways you can write

01:55:23.120 --> 01:55:28.720
to your local newspaper you can make a protest if that's up your alley you can write to your

01:55:28.720 --> 01:55:34.320
mp or your congressman or wherever you live um and again create that legitimacy for people

01:55:34.320 --> 01:55:40.000
to actually act on the problem because to many people it does sound very much like sci-fi and

01:55:40.000 --> 01:55:44.480
policy makers are not going to take action and newspapers are not going to continue to report

01:55:44.480 --> 01:55:48.720
about an issue they feel like it doesn't have traction and isn't taken seriously by the general

01:55:48.720 --> 01:55:54.560
public the other thing the general public can do is we can educate ourselves and then we can share

01:55:55.040 --> 01:56:00.080
information we have found to be most persuasive ourselves because there's a wide variety of

01:56:00.080 --> 01:56:05.280
books a wide variety of youtube channels a wide variety of blogs and some of them are

01:56:05.280 --> 01:56:09.840
better than others so let's share what we have found to be the really best ones

01:56:10.640 --> 01:56:14.640
auto before i pass to jan maybe i'll ask you to get ready to come up on the stage because

01:56:14.640 --> 01:56:20.960
you're going to give some closing remarks but jan what's your answers to what you've heard uh so

01:56:20.960 --> 01:56:26.560
yeah just to just kind of underline the what ordinary people could do uh is just kind of

01:56:26.560 --> 01:56:31.680
keep this topic alive like one of the things that i'm very proud of uh that came out of the

01:56:31.680 --> 01:56:37.920
future five six months post letter uh was uh kind of framed by uh european commissioner

01:56:37.920 --> 01:56:43.840
margaret bestiger when she said that like one thing that this letter has done is you're gonna

01:56:43.840 --> 01:56:49.760
like communicate to the regulators that these concerns are much more widespread among people

01:56:49.760 --> 01:56:56.160
than among regulators so i think this potential difference should be continually kind of maintained

01:56:57.360 --> 01:57:04.080
so and when it comes to kind of bringing in kind of expertise from people from like regulated

01:57:04.080 --> 01:57:10.480
industries i think it's super valuable i was on the on the board or like on the european high

01:57:10.480 --> 01:57:14.880
level expert group at the european commission and there was like every once in a while there was

01:57:14.880 --> 01:57:18.640
like why are we inventing the wheel like that we already have like lots of regulations should

01:57:18.640 --> 01:57:23.680
we just apply this and i was like yes however there's like one big problem uh the problem is p

01:57:24.400 --> 01:57:32.400
in chat gpt gpt stands for generative pre-trained transformer the pre-training is something that

01:57:32.400 --> 01:57:42.400
you do before you actually train so the current the the out of nasty secret of ai uh field is

01:57:42.480 --> 01:57:48.400
the ai's are not built they are grown the way you you you build the frontier model build the

01:57:48.400 --> 01:57:54.880
frontier model is you take like two pages of code you put them in tens of thousands of

01:57:54.880 --> 01:58:02.320
graphics cards and let them hum for months and then you're gonna open up the hood and see like

01:58:02.320 --> 01:58:08.560
what creature brings out and what you can you can do with this creature so it's i think the

01:58:08.560 --> 01:58:16.320
regulate the industry the capacity to regulate things uh and kind of deal with various liability

01:58:16.320 --> 01:58:21.920
constraints etc they apply to what happens after what's once this creature has been kind of tamed

01:58:22.640 --> 01:58:29.840
and that's what what uh fine tuning and reinforcement learning from human feedback etc is doing

01:58:29.840 --> 01:58:35.280
and then productized then how do you deal with with these issues but uh is this where we need

01:58:35.280 --> 01:58:41.040
the competence of of like other other industries but like how can avoid the system not escaping

01:58:41.040 --> 01:58:47.440
during training run this is this is like a complete novel issue for this species and we need to need

01:58:47.440 --> 01:58:53.360
some other approaches like just banning those training runs that's great we'll thank the panel

01:58:53.360 --> 01:58:59.680
in a minute i asked the panel to stay here because who's going to wind up the evening is Otto Barton

01:59:00.480 --> 01:59:08.000
Otto is the executive director of the ERO the existential risks observatory

01:59:08.000 --> 01:59:13.760
which along with conjecture has designed and organized and sponsored this whole evening

01:59:13.760 --> 01:59:19.440
Otto's got a few closing remarks before those of us who are still here can have a quick drink

01:59:19.440 --> 01:59:24.960
and continue the discussion informally up to 10 o'clock by which time we must be out of the building

01:59:24.960 --> 01:59:28.240
Otto

01:59:38.960 --> 01:59:45.520
all right uh thanks david um a few closing remarks before we go to the drinks which is

01:59:45.520 --> 01:59:51.840
five minutes so you should be able to uh keep with me um so we're talking tonight about human

01:59:51.920 --> 01:59:56.960
extinction because of AI and what to do about this um and i think what to do about this there

01:59:56.960 --> 02:00:00.960
was also a great question from the audience what can we do about this this is exactly the question

02:00:00.960 --> 02:00:07.200
that i asked myself a few years ago um but it's not trivial and it's it's pretty difficult actually

02:00:07.200 --> 02:00:13.440
what is not positive what could you do develop AI yourself try to do it safely such as uh open

02:00:13.440 --> 02:00:20.720
AI deep mind and anthropic are doing will this increase safety some say so uh work on AI alignment

02:00:20.720 --> 02:00:25.440
for example interpretability where we've seen great breakthroughs actually last week uh it could

02:00:25.440 --> 02:00:30.480
be a good option but increasing knowledge of how AI works could also speed up its development so this

02:00:30.480 --> 02:00:37.520
brings risks as well uh one could campaign for regulations such as an AI pause we support this

02:00:37.520 --> 02:00:43.200
but this also has its downsides so i think it's pretty difficult to tell what one should do to

02:00:43.200 --> 02:00:48.960
reduce human extinction risk by AI but when i started reading into this i was only really

02:00:48.960 --> 02:00:54.080
convinced about one thing and that is that you cannot put humanity at risk without telling us

02:00:54.880 --> 02:01:00.400
so you cannot have a dozen tech executives embarking on the singularity without informing anyone

02:01:00.400 --> 02:01:06.400
else and you cannot have a hundred people at a summit which is what's happening now decide what

02:01:06.400 --> 02:01:11.440
should be built and what should not be built and i think you cannot let a tiny amount of people

02:01:11.440 --> 02:01:18.320
also decide how high extinction risk should be for the rest of us so the only thing that i'm

02:01:18.320 --> 02:01:24.320
really convinced of is that we should be informed about this topic and that's also why i'm so happy

02:01:24.320 --> 02:01:31.760
that events such as this one are taking place um we're happy i'm happy that we're together not just

02:01:31.760 --> 02:01:36.080
with in crowd people some of you are and it's great but also with some people who may who may have

02:01:36.080 --> 02:01:41.920
never heard of existential risk before and also a journalist who can inform a much wider audience

02:01:41.920 --> 02:01:47.200
about existential risk also with a member of parliament someone with a job to openly discuss

02:01:47.200 --> 02:01:52.560
difficult problems so i think this is all very encouraging and it's helping to normalize an open

02:01:52.560 --> 02:01:58.960
debate about the topic of human extinction by artificial intelligence the 31st of october at

02:01:58.960 --> 02:02:04.560
two o'clock we'll have our next event with professor steward russell it's just outside the ai safety

02:02:04.560 --> 02:02:09.120
summit in blashley park in the old assembly hall where the code breakers used to have their

02:02:09.120 --> 02:02:14.800
festivities after their important work so our event at blashley park the day before the summit

02:02:14.800 --> 02:02:20.240
may not resemble a festivity but in a sense i think it is because we're celebrating that we're

02:02:20.240 --> 02:02:25.840
all being hurt there we're celebrating that we can all be part of a democratic conversation about

02:02:25.840 --> 02:02:31.200
what the most important technology of the century should and should not be able to do we can talk

02:02:31.200 --> 02:02:36.080
about risks to humanity we find acceptable and what we intend to do about risks that are too high

02:02:37.440 --> 02:02:42.080
and as the existential risk observatory together with conjecture we invite everyone to be part

02:02:42.080 --> 02:02:48.000
of this conversation so there's much to be unsure of in this field but if there's one thing that i

02:02:48.000 --> 02:02:53.120
am sure of it's that the most important conversation in this century which i think this is has to be

02:02:53.120 --> 02:03:00.560
a democratic one so with that i would like to invite you to scan the qr code on the left

02:03:01.280 --> 02:03:08.080
if this is working right yes to join us in blashley this is containing the url where you

02:03:08.080 --> 02:03:13.200
can enroll to the blashley park event if you're interested then definitely pass by

02:03:15.520 --> 02:03:20.960
there's same qr code is also on the flyer on your chair and beyond blashley i think this

02:03:20.960 --> 02:03:28.320
conversation will not stop so there will be more summits according to my timeline about maybe 18

02:03:28.320 --> 02:03:36.880
roughly so we will organize more events probably publish more about ai do more research and inform

02:03:36.880 --> 02:03:42.240
governments as well as we can if you want to follow us or support us the existential risk

02:03:42.240 --> 02:03:47.040
observatory in that work then scanning your r-codes on the right there's much that you can do to

02:03:47.040 --> 02:03:52.960
help us um and with that i would like to close this evening and once again thanks to all our great

02:03:52.960 --> 02:03:59.200
speakers so that's uh romeo polsky cornelly sir robert buckland yantalin andrea milte alexandra

02:03:59.200 --> 02:04:11.520
mosefisa day eva birans and some are give them a round of applause

02:04:17.120 --> 02:04:22.960
and i would also very much like to thank david wood uh should see them conor xio dis

02:04:22.960 --> 02:04:26.720
ribbon dealer man and everyone at conway hall will also make this evening possible thank you very

02:04:26.720 --> 02:04:40.320
much and then i would like to hopefully see you in blashley and in any case you are the drink

02:04:40.320 --> 02:04:53.680
right now thank you thanks everybody

02:05:10.320 --> 02:05:11.220
you

02:05:40.320 --> 02:05:41.220
you

