start	end	text
0	2060	you
30000	32060	you
60000	62060	you
90000	92060	you
360000	387760	welcome everybody. I encourage you to take a seat. We will be starting almost on time
387760	396920	because we have a very rich agenda on a very big topic. We are talking about
396920	404720	navigating existential risk. Navigating what people have described as a very
404720	413640	difficult, tortuous landscape of risks that are made worse by oncoming new
413640	420400	frontier AI. That's not just the AI that we have today, but AI that we can
420400	426600	fairly easily imagine as coming within the next year or two. Next generation AI
426600	433160	that's more powerful, more skillful, more knowledgeable, potentially more
433160	439880	manipulative, potentially more deceitful, potentially more slippery, definitely
439880	445840	more powerful than today's AI. That AI might generate existential risks in its
445840	452680	own right. That AI is likely also to complicate existing existential risks,
452680	459840	making some of the risks we already know about more tricky to handle, more wicked.
459840	465840	And we might also talk about the way in which next generation AI might be the
465840	471560	solution to some of the existential risks and dilemmas facing society. If we can
471560	480000	apply AI wisely, then perhaps we can find the narrow path through this difficult
480000	486320	landscape. So welcome navigators in the hall. Welcome to navigators watching the
486320	492360	live stream. Welcome to people and AI's watching the recording of this
492680	500360	discussion. Let's get stuck in. We have lots of very capable, knowledgeable
500360	506040	speakers who will approach this from a diversity of points of view. Indeed, I
506040	510280	think one of the hazards in this whole topic is that some people want to be a
510280	515360	little bit one dimensional. They want to say, this is how we'll solve the problem.
515360	519600	It's quite straightforward. In my view, there are no straightforward solutions
519600	524480	here, but you can make up your own minds as you listen to what all the speakers
524480	528280	and panelists have to say. And yes, in the audience, you'll have a chance later
528280	533960	on to raise your hand and get involved in the conversation too. The first person
533960	537920	you're going to hear from is unfortunately not able to be with us
537920	544680	tonight, but he has recorded a short video. He is Sir Robert Buckland, MP,
544680	549360	former Lord Chancellor of the United Kingdom, which means he was responsible
549400	554400	for the entire justice system here, former Secretary of State for Wales. He is
554400	562200	still an MP and he has a side hustle as a senior fellow at the Harvard Kennedy
562200	567120	School, where he is writing papers on exactly the topics we're going to be
567120	573200	discussing tonight, namely how does AI change key aspects of society, potentially
573240	579960	making it better, potentially making it much worse if we are unwise. So let's
579960	584720	watch Sir Robert Buckland who will appear by magic on the big screen.
586240	591120	Well, I'm very pleased to be able to join you, albeit virtually, for the Conjecture
591120	599160	ARO Summit on AI and the challenges and opportunities that it presents us. And I
599160	605040	think my pleasure at being with you is based upon not just my own experience in
605040	612000	government, but also my deep interest in the subject since my departure from
612000	617720	government last year. Now, when I was in government, I had responsibility for
618480	623680	for many years the legal advice that was given to departments and indeed to the
623680	627920	government in general when I was in the Law Offices Department as the Solicitor
627920	633200	General. And then responsibility for running the Ministry of Justice as
633200	638120	Lord Chancellor and Secretary of State for over two years before a brief return as
638120	645000	Wales Secretary last year. That seven years or so experience within government as a
645000	650720	minister gave me, I think, a very deep insight into the pluses and the minuses
650720	656520	of the way the government works, the efficiencies and indeed the inefficiencies
656760	663440	about process. And I think clearly, as in other walks of life, artificial
663440	668840	intelligence, machine learning will bring huge advantages to government processes to
668840	676040	improve efficiency, to speed up a lot of the particular ways in which government
676040	681040	works, which will be, I think, to the benefit of citizens, whether it's citizens
681080	686920	waiting for passport applications, visa applications or other government processes
686920	693080	benefits, for example. However, I think that we kid ourselves if we don't accept
693080	699160	the fact that alongside the benefits come potential pitfalls. And the first and
699160	704680	most obvious one, I think, for me is the scrutability of process. In other words,
705320	709800	the way in which we understand how decisions are made. And that's very
709800	714920	important. Because understanding how decisions are made is part of democratic
714920	719560	accountability in societies like ours, where individuals or organizations wish
719560	724040	to challenge decisions made by government, perhaps through judicial review
724040	729720	applications, then the explicability of those decisions, which is accompanied by
730000	734600	a duty of candor by the government in order to disclose everything about those
734600	740360	decisions is part of that accountability. And of course, it's sometimes very
740360	745440	difficult to explain how the machine has come to decisions. And more fundamental
745440	749640	than that, we have to accept that if the data sets that are used in order to
749640	757400	populate the processes are not as full of integrity as they should be, and are
757400	763000	not the product of genuinely objective and carefully calibrated processes, then
763000	767720	we are in danger of importing historic biases into the system, whether it's
767720	772360	biases against neurodiverse people making job applications, or deep biases
772360	777160	against people of color in the criminal justice system, simply because the data
777160	783000	sets have imported those historical anomalies, those historical imbalances.
783560	788160	Now, all those questions have really got me thinking very deeply about the
788160	794280	impact of machine learning on the ethics of justice itself. And as a result of
794280	798480	my thinking, I was delighted last year to be accepted as a senior fellow at the
798480	803640	Moserfa-Romani Center for Business and Government at Harvard Kennedy School, and
803640	809960	I am working currently on a number of papers relating to the impact of AI and
809960	816760	machine learning on the administration of justice and the law itself. It really
816760	823560	developed from my own experience as law chancellor, from digitalization, I
823560	830080	should say, of the courts when, during the Covid pandemic, we had to move many,
830080	834680	many thousands of hearings online for the first time. You know, I think we jumped
834680	840960	from a couple of hundred phone or online hearings to 20,000 a week in a very
841000	847920	short compass. And the status quo will never be the same again. In fact, it has
847920	852720	moved on, I think, in a way that we just hadn't foreseen before the pandemic. Now,
852720	856600	I think that's a good thing, but I also think that accompanying this question
856600	861720	about increased efficiency is the use of artificial intelligence. Now, in some
861720	867280	jurisdictions, such as China, we are seeing its increased use, not just to do
867320	873160	legal research and to prepare cases, but to actually decide themselves. In other
873160	878200	words, the AI judge. Now, that's all well and good. But do we actually know what
878200	884720	populates the data sets that then forms the basis of the decisions made? And I
884720	890080	think it's that intentional, unintentional bias, or indeed worse than that
890080	895400	potential intentional bias, whether that's influenced by a government or
895400	900360	indeed a corporate that might be able through their financial means to
901080	908560	influence a procedure or indeed the way in which we deal with cases, knowing as
908560	913480	we might do more information about the way in which judges make the decisions.
913880	918280	All these questions, I think, need to be asked now before we end up in a
918280	924000	position where we've slept, walked into a completely different form of justice
924000	928880	than the one that we know. Now, underpinning all of this, I think, is the
928880	933680	need to ask a fundamental question about judgment itself. And that's what I've
933680	937320	been doing in my first paper. You know, the essence of human judgment is
937320	942000	something that will be based not just upon an understanding of the law, but on
942000	947080	our experiences as human beings. And you can go right back, as I have done, to
947080	953400	the judgment of Solomon and his emotional response to the woman who
953400	958000	clearly was the true mother of the child that he proposed to be cut in half.
958440	962480	Now, you know, that's an example, I think, of the human element of
962480	970800	judgment, which has to be an essential foundation of decision making,
970800	974160	particularly when it comes to the assessment of credibility of a witness,
974880	980760	human witness giving evidence upon which the case stands all falls. And of
980760	984520	course, for judge, that applies for juries as well in criminal trials,
984760	990480	particularly here in the UK. Now, you know, all these questions, I think, need
990480	995720	to be asked. And then we need to work out what it is that we want to retain out
995720	999320	of all of this. Now, I don't think we should make any cosy assumptions that
999320	1006680	because at the moment some large learning systems are having hallucinations.
1006680	1014000	I don't think we should be assuming that just because of that, therefore AI
1014000	1019520	will never work in a way that can achieve a greater degree of certainty.
1019520	1027480	I think the inevitable arc of development will result in better and
1027480	1032800	better and more capable machines. That's inevitable. And what we must be
1032800	1037480	asking at the same time as capability is ensuring there is greater security and
1037480	1042160	safety when it comes to the use of AI. And that really underpins, I think,
1042400	1046280	the work that I'm doing in the field of justice. What does this all lead to
1046280	1052120	then? Well, we have the AI Safety Summit in the UK next month. I very much
1052120	1057800	hope that that summit will first of all involve those critical players in
1057800	1065200	terms of international organisations and key countries as well that will come
1065200	1069760	together to commit to creating, I think, a defined framework within which we
1069760	1073960	should be using AI safely. And that framework, I think, will have to take
1074120	1078200	several forms. I think in the field of justice, we could do with an international
1078200	1084080	framework of principles, which will ensure transparency and which can
1084120	1088800	reassure people that in cases of the liberty of the individual criminal
1088800	1093280	cases, cases where perhaps the welfare of a child and the ultimate destination of
1093280	1101200	a child is in issue, then the human element will be the key determinant in
1101200	1107320	any decisions that are made. And that the use of machines will be transparent and
1107320	1112960	made known to all the parties throughout the proceedings. And then other
1113000	1118120	walks of life, I think the AI Safety Summit has to then look as well at
1118520	1123400	whether frameworks can be created and what form they should take. I think
1123400	1127240	it's tempting to try and be prescriptive. I think that would be a mistake, not
1127240	1131840	just for the obvious reason that AI is developing and therefore anything that
1131840	1136760	we write in 2023 will soon be out of date. But the very fact that AI itself
1137160	1143920	does not mean an alloyed harm. In fact, it means a lot of benefit and also
1143920	1147920	some neutral effects as well. And where you have that approach, then a
1147920	1153920	principle based system seems to me to be more sensible than overly
1153920	1157880	prescriptive and detailed rules as you would have, for example, to prevent a
1157880	1163720	crime such as fraud. So just some preliminary thoughts there as to the
1163720	1168000	impact of machine learning. I don't pretend to be a technical expert. I'm
1168000	1174000	not. But my years in justice, my years as a lawyer, a judge and as a senior
1174520	1180920	cabinet minister, I think obliged me to do some of the thinking now to help
1180920	1186160	ensure that countries like Britain are in the forefront of the sensible and
1186160	1192120	proportionate regulation of the use of machine learning and other types of
1192160	1197360	artificial intelligence. If we don't do it now, then I think we'll be missing
1197360	1202760	an unhistoric opportunity. I wish you all well and I look forward to meeting
1202760	1208000	some of you in future and discussing these issues as they develop. Thank you
1208000	1208520	very much.
1210320	1215360	Well, thank you, Sir Robert, who may be watching the recording of this. Don't be
1215360	1222360	prescriptive, he said. Let's sort out some sensible proportionate regulation. Is
1222360	1227120	that credible? Is that feasible? You'll be hearing from other panelists who may
1227120	1233320	be commenting on that shortly. So Robert also said there are risks such as the
1233360	1239040	inscrutability of AI. We don't understand often how they reach its decisions. We
1239040	1244000	don't understand the biases that might be there that might have been planted. We
1244000	1248840	might lose charge. We might become so used to AI taking decisions that humans
1248840	1254560	end up in a very sad place. But how bad could things get? That's what we're
1254560	1259520	going to hear from our next speaker. So I'm going to ask Conor Leahy to come up
1259520	1266360	to the stage while I briefly introduce him. Conor is the CEO of Conjecture. If
1266360	1270080	you haven't heard about Conjecture, I think you need to do a bit more reading.
1270080	1275600	Perhaps Conor will say a little bit about it. They are AI alignment solutions
1275600	1281600	company, international, but with strong representation here in the UK. So welcome
1281600	1283000	Conor, the floor is yours.
1291000	1294800	Thank you so much. It's so great to see you all today. So happy to be able to talk
1294880	1300640	to you here in person. And man, do we live in interesting times, to put it
1300640	1308640	lightly. The world has changed so much. Just in the last few years, a few months
1308640	1315080	even, so much has happened in the world of AI and beyond. Just a couple of years
1315080	1322240	ago, there wasn't such a thing as chat GPT, or even GPT 3, or 4, or 2, or any of
1322240	1327200	those. It was a different world not too long ago when technologists such as myself,
1327200	1335960	weird little hobbyists, worried about the problem of AGI and how it will affect
1335960	1342120	the world. Back then, it still seemed so far away. It seemed like we still had time.
1343120	1352120	But now we find ourselves in a world of unrestricted, uncontrolled scaling, a
1352120	1358560	race towards the finish, towards the end, to scale our AI systems ever more
1358560	1365600	powerful, more general, more autonomous, more intelligent. And the reason I care
1365600	1371880	about this is very simple. If we build systems that are smarter than humans, that
1371960	1378400	are more capable at manipulation, deception, politics, making money, scientific
1378400	1384560	research, and everything else, and we do not control such systems, then the future
1384560	1394440	will belong to them, not to us. And this is not the future I want. I want a future
1394440	1400000	in which humanity gets to decide its destiny. We get to decide the future for
1400040	1405320	ourselves, for our children, for our children's children, that we like. The
1405320	1410880	future where our children can live long, happy lives surrounded by beauty, art,
1410880	1416640	great technology, instead of being replaced by souless automata. And let me be
1416640	1424440	clear that this is the default outcome of building an uncontrolled AGI system,
1425160	1434920	the full replacement of mankind. And what we're seeing is that AI is on an
1434920	1441440	exponential. There's a race. All the top organizations, which is open AI, deep
1441440	1447600	mind, anthropic, among others, are racing ahead as fast as the VC dollars
1447680	1455480	scale up their work. And this has given us an exponential. AI is on an
1455480	1459760	exponential curve, both on hardware and on software. It's improving at
1459760	1464600	incredible rates. And when you're dealing with an exponential, there are
1464600	1473680	precisely two times you can react to it too early or too late. There is no such
1473760	1478440	thing as reacting at just the right moment on an exponential, where you
1478440	1483040	find just the perfect middle point just in the nick of time when everyone
1483040	1487760	agrees that the problem is here and everything has perfect consensus. If you
1487760	1495600	do this, you are too late. It will be too late. And the same thing applies to
1495600	1502440	AGI. If we wait until we see the kinds of dangerous general purpose systems that
1502440	1508760	I am worried about, then it will already be too late. By the moment such systems
1508760	1518760	exist, the story of mankind is over. And so if we want to act, we must act well,
1518760	1524760	well before such things actually come into existence. And unfortunately, we do
1524760	1532400	not have much time. How the world has changed. As frightening and as
1532400	1539680	terrible the race may be, there's also good changes. A few years ago, I could
1539680	1544440	have barely imagined seeing governments, politicians, and the general public
1544440	1549080	waking up to these weird nerd issues that I cared about so much with my
1549080	1553840	friends online. But now we're looking forward to the first international AI
1553880	1559840	summit convened by the UK and the famous Tip Bletchley Park. And this is great
1559840	1564840	news. The European Commission has recently officially acknowledged the
1564840	1569080	existential risks from AGI along with the risks from nuclear weapons and
1569080	1575320	pandemics. This is great progress. This is fantastic. It is good to see our
1575320	1579720	governments and our societies waking up and addressing these issues or at least
1579800	1584960	beginning to acknowledge them. And we must use this opportunity. We have an
1584960	1590960	opportunity right now and we must prevent it from being wasted. Because
1590960	1596360	there's also bad news. We're having this great opportunity to start building
1596360	1601240	the regulation and the coordination necessary for a good future. The very
1601240	1605720	people who are creating these risks, the very people at the heads of these
1605720	1610480	labs, these organizations, buildings technologies, are the very people who are
1610480	1615520	being called upon by our governments to help regulate the very problem that
1615520	1620200	they themselves are creating. And let me be very explicit about this. The
1620200	1628440	problem that we face is not AGI. AGI doesn't exist yet. The problem we
1628440	1634120	face is not a natural problem either. It is not an external force acting
1634120	1641000	upon us from nature. It comes from people, from individual people, businessmen,
1641000	1647160	politicians, technologists, athletes, large organizations who are racing, who are
1647160	1650520	skilling, who are building these technologies and who are creating these
1650520	1658280	risks for their own benefit. But they have offered us these very people who are
1658360	1664760	causing this problem, have offered us a solution. Fantastic. And they are pushing
1664760	1670760	it as hard as they can towards the UK government and the upcoming summit. So
1670760	1677800	what is the solution? The solution to the problem of scaling these labs, these
1677800	1682600	accelerations labs such as Anthropic and ARC have been pushing for. What is the
1682600	1687880	solution? Well, the solution to the scaling problem is called responsible
1687880	1693560	scaling. Now, what is responsible scaling? You might ask, you see, it's like
1693560	1697400	normal scaling except you put the word responsible in front of it. And that
1697400	1705880	makes it good. So, of course, I'm joking somewhat, but there's a lot of truth in
1705880	1714600	humor. Responsible scaling is basically the policy, and you can read this on
1714600	1719640	both ARC or Anthropics website, is the policy proposal that we should continue
1719640	1725720	to scale uninhibited until at some future time when tests and evaluations that
1725720	1729800	do not yet exist and we do not know how to build, but the labs promise us they
1729800	1735800	will build, detect some level of dangerous capabilities that we do not yet know,
1735800	1741480	and then once it gets to that point, then they will stop, maybe, except there is
1741560	1746120	a clause in the Anthropic version of the RSP paper in which they say that if a
1746120	1751560	different organization was scaling even super unsafely, then they can break this
1751560	1759880	commitment and keep scaling anyways. So, this could be sensible if they committed
1759880	1764920	to, you know, a sensible bound, a conservative point on which to stop, but
1764920	1771160	unfortunately the responsible scaling policy RSP fails to actually commit to
1771240	1778120	any objective measure whatsoever. So, effectively, the current policy is to
1778120	1785080	just keep scaling until they feel like stopping. This is the policy that is
1785080	1789720	being suggested to our politicians and to the wider world as the responsible
1789720	1795720	option for policy makers. It is trying to, is very clear that it is trying to
1795720	1802120	recast this techno-libertarian extremist position as sensible, moderate,
1802120	1810120	responsible even. Now, in my humble opinion, the reasonable, moderate position
1810120	1813960	to when dealing with a threat that is threatening the lives of billions of
1813960	1823560	people is to simply not do that. But instead, this is trying to pass off this
1823560	1831000	as the sensible, middle-ground position. The truth of RSP is that it comes from
1831640	1838360	the same people who are causing this risk to exist. These people, the heads of
1838360	1842360	these labs, many of the scientists and the policy people and the other people
1842360	1847880	working on this, have known about existential risks for decades. And they
1847880	1851960	fully admit this. This is not like they haven't heard about this. It's not even
1851960	1856200	that they don't believe it. You can talk to them. They're on the record talking
1856200	1861800	about how they believe that there is a significant chance that AGI could cause
1861800	1868120	extinction of the entire human species. In a recent podcast, Dario Amade, the CEO
1868120	1871880	of Anthropic, one of these labs, himself, said that he thinks it's a
1871880	1877720	probably 25% chance that it could kill literally everybody. And they're doing it
1877720	1883400	anyway. Despite this, they keep doing it. Why? Well, if you were talking to these
1883400	1888440	people, what they might tell you is that, sure, you know, I know it's dangerous. I
1888440	1893640	am very careful. But these other guys, well, they're even less careful than me. So
1893640	1897960	I need to be number one. So I actually have to race faster than everyone else.
1897960	1903160	And they all think this about each other. They call this incremental, but they
1903240	1910040	never pause. They always race as fast as they possibly can. Do as I say, not as I
1910040	1915160	do. There is a technical term for this. It's called hypocrisy.
1917240	1923560	And RSP is no different. They are simply trying to twist words in an Oralian way
1924760	1928360	to be allowed to keep doing the thing that they want to do anyways,
1928680	1939640	which they themselves say could risk everybody. I mean, has responsible in the name, must be good.
1941960	1948520	When people like Sam Altman talk about iterative deployment, about how we must iteratively
1948520	1954200	release AI systems into the wild so societies can adapt to them, be inoculated by them,
1954280	1960920	it sounds so nice. It sounds almost responsible. But if you're really trying to inoculate
1960920	1967800	someone, you should let the host actually adapt before you jam in the next new pathogen
1967800	1973240	into their weakened immune system as fast as you possibly can. But this is exactly what
1973240	1979160	laboratories such as OpenAI, DeepMind, Anthropic, and Tier 2 labs such as Meta are doing with all
1979160	1984440	the force they can muster to develop more and more new systems as fast as possible,
1984440	1991320	release them as fast as possible, wide as spread possible. Now, if OpenAI had developed a GPT-3
1991960	1997560	and then completely stop further scaling, focused all of the efforts on understanding GPT-3,
1997560	2003960	making it safe, making it controllable, working with governments and civil society to adapt the new
2004040	2009800	problems posed by a system for years or even decades, and then they build GPT-4?
2010680	2015000	Yeah, you know what? Fair enough. I think that could work. That would be responsible.
2015720	2017000	But this is not what we were seeing.
2020040	2025640	All of these people at all of these institutions are running a deadly experiment
2026360	2033560	that they themselves think might cause extinction. It is gain of function research on AI
2033960	2040520	just like viruses, developed and released to the public as fast and aggressively as possible.
2042520	2047160	They're developing more and more dangerous and more and more powerful viruses as quickly as
2047160	2056840	possible and forcing it into everyone's immune system until they break. There is no responsible
2056840	2062920	gain of function research for extinction level threats. There is no such thing.
2063960	2068440	We have no control over such systems and there is no responsible way to continue
2069160	2073880	like this. And anyone who tells you otherwise is lying.
2076600	2084600	A lot has changed. The summit can lead to many boring outcomes, just exchanges of diplomatic
2084600	2090600	platitudes as is often the outcome of such international events. They have some good
2090600	2098680	outcomes and can have some very, very bad outcomes. Success in the summit is progress
2098680	2103800	towards stopping the development of extinction level AGI before we know how to control it.
2104760	2113960	Most other outcomes are neutral and bad outcomes. They look like policymakers blindly and sheepishly
2114040	2120760	swallowing the propaganda of the corporations to allow them to continue their unconscionably
2120760	2127320	dangerous gamble for their own personal gain and glory at the expense of the entire planet.
2128680	2136280	We owe it to ourselves and our children to build a good future, not gamble it all
2137160	2145320	on a few people's utopian fever dreams. Governments and the public have a chance
2145320	2150520	to regain control over the future, and this is very hopeful. I wasn't sure we were going to get it,
2151080	2156520	but the summit speaks to this, that people can act, that governments can act, that civil society
2157080	2164680	can act, that it is not yet too late. There is simply no way around it. We need to stop
2164680	2170360	the uncontrolled scaling, the uncontrolled race, if we want a good future.
2172120	2179800	And we are lucky because we can do this. We can cap the maximum amount of computing power
2179800	2185080	going into these AI systems. We can have government intervene and prevent the creation
2185080	2192600	of the next more dangerous, more general, more intelligent strain of AI until we are ready to
2192600	2202040	handle it. And don't let anything distract you from this. There is no good future in which we
2202040	2209880	continue on this path, and we can change this path. We need to come together to solve these
2209880	2215480	incredibly complex problems that we are facing, and not let ourselves be led astrayed by corporate
2215480	2224520	propaganda. And I hope that the governments and the civil society of the world do what needs to be
2224520	2241960	done. Thank you. Thank you, Conor. We will take questions from the floor in a moment.
2242520	2245720	I'll just start off with the question, I think, maybe on many people's minds.
2246280	2253160	Why would a super-intelligent AI actually want to kill humans? I have a super-intelligent calculator
2253160	2257880	which is no desire to kill me. I have a super-intelligent chess-playing computer that is no
2257880	2264040	desire to kill me. Why don't we just build, as responsible scaling, an AI that has no desires
2264040	2270600	of its own? Because we don't know how to do that. Why did Homo sapiens eradicate Homer Neanderthalis
2271160	2276920	and Homo erectus and all the other species that we share the planet with? You should think AGI,
2276920	2283160	not of as a calculator, but as a new species on our planet. There will be a moment where humanity
2283160	2290600	is no longer the only or even the most intelligent species on this planet, and we will be outcompeted.
2291400	2297240	I don't think it will come necessarily from malice. I think it will be efficiency. We will build
2297240	2303320	systems that make money, that are effective at solving tasks, at solving problems, at gaining
2303320	2308600	power. These are what these systems are being designed to do. We are not designing systems
2308600	2314760	with human morals and ethics and emotions. They're AI. They don't have emotions. We don't even know
2314760	2319800	how to do that. We don't even know how emotions work. We have no idea how you could get an AI to
2319800	2325080	have emotions like a human does. So what we're building is extremely competent, completely
2325160	2330600	sociopathic, emotionless, optimizing machines that are extremely good at solving problems,
2330600	2335080	extremely good at gaining power, that do not care about human values or emotions,
2335080	2340040	never sleep, never tire, never get distracted, can work a thousand times faster than humans,
2340760	2347800	and people will use these for many reasons. And eventually, I think humanity will just
2348440	2354360	no longer be in control. Questions from the floor? There's a lady in the third drawer down here.
2354360	2360200	Just wait for the mic, sorry, so that the audience online can hear you.
2361160	2367560	Susan Finnell from Finnell Consult. To stop the arms race, certainly at a geographical level,
2367560	2374600	I mean, in nuclear, the states and Europe can tell which countries are building nuclear weapons
2374600	2379960	and what they've got, and they can do tests. If computing power is a thing that needs to be
2380040	2388120	capped to slow this down enough, is there a way to monitor what other countries or
2388120	2392040	people in a clandestine way are doing, and how does that work?
2392040	2399480	This is a fantastic question, and the extremely good news is yes. At least currently,
2399480	2405160	this will change in the near future, but the current state to build frontier models requires
2405240	2410760	incredibly complex machines, massive supercomputers that take megawatts of energy.
2410760	2416600	So this is on the order you'd have of a nuclear centrifuge facility. So these are massive,
2416600	2422440	huge machines that are only built by basically three or four companies of the world. There are
2422440	2427880	very, very few companies, and there is extreme bottlenecks on the supply chain. You need very,
2427880	2433800	very specialized infrastructure, very specialized computer chips, very specialized hardware to
2433800	2438520	be able to build these machines, and these are produced exclusively by countries basically in
2438520	2444440	the West and Taiwan. There are many ways where the US or other intelligence services can and
2444440	2450360	already are intervening on these supply chains, and it would be very easy to monitor where these
2450360	2456440	things are going, who is buying them, where is energy being drawn on large scales. So it is not
2456440	2462280	easy, and the problem is that AI is unexponential both with hardware and with software. Eventually,
2462280	2468280	it will be possible to make essentially dangerous AGI on your home laptop, probably,
2468280	2475080	maybe not, but it seems plausible. If we get to that world, we're in big trouble. So this is part
2475080	2479800	also why we have to buy time. We have, at some point, there will be a cutoff where we'll have
2479800	2485400	algorithms that are so good that either we have to stop everyone from having a PlayStation at home,
2485400	2492040	which doesn't seem that plausible, or at that point we have to have very good global coordination
2492040	2498120	and regulation. Thanks. Just past the mic behind you, there's a person in the row behind.
2499000	2506280	Robert Whitfield from One World Trust. Can I ask about Bletchley Park? Do you know, I mean are you
2506280	2513880	participating, and if not, do you know anybody else with similar views to you who is participating?
2515080	2518760	I can't comment too much, since it's closed doors, it's a very private event,
2518760	2523240	unfortunately, so I don't think I have my liberty to talk about exactly what I know. I think the
2523240	2530120	guest list is not public. I don't know most of the people who are coming. I know the obvious ones,
2530120	2535720	all the CEOs of all the top labs, of course, are attending, is not a secret. I don't know
2536600	2540760	who, if anyone, of my reference class is attending.
2543000	2547640	And just past the mic next to you, Robert. Thank you. Perhaps I can answer that question.
2547720	2555080	Anybody that has read The Guardian today, there is an interview with Clifford,
2556040	2562680	and for the very first time, not for the second time, it has been clarified that there will be
2562680	2570840	only about 100 people participating on the first day. Anybody is invited, including China,
2570840	2575320	on the second day, apparently there will be only the coalition of the willing.
2576120	2584920	So those who subscribe to the Frontier Model Forum, they will sit on the second day,
2584920	2591080	that's the current question. My main impression from that article is generally it's very positive,
2591080	2597000	and I would say I've been surprised, as you would be surprised, that the UK government is really
2597000	2605640	doing what it can to get the mission to what the title of the conference says, the AI safety
2605640	2610520	summit. It's not about regulation, it's about controlling AI, and they're trying to do their best.
2611240	2619080	The problem is, as outlined in that interview, is that we seem to be alone. We have the states
2619080	2625720	a little bit, but the rest wants to go their own way and do it on their own territory, which is,
2625720	2631480	I think, the tune. I agree. Sooner or later, international coordination around these issues
2631480	2636440	will be necessary. It's as simple as that. If you want humanity to have a long, good future,
2636440	2641320	we need to be able, as a global civilization, to handle powerful technologies like this.
2642040	2643560	Take a question right from the back.
2647240	2653080	In terms of legislation, what kind do you think was most effective? I've heard, for example,
2653320	2659080	liability law takes too long to actually have an effect, and compute governance
2659080	2666760	generally seems to be very easy to be called totalitarian. What do you think of legislation such
2666760	2675320	as models must be released with a version before pre-processing, and there'd be attacks on the
2675320	2679000	number of harmful outputs done by the model before the pre-processing?
2679960	2685400	I am open to many kinds of regulation per se. I would strongly disagree with this description
2685400	2689960	of compute governance. This is like saying that not the private citizens not having nuclear weapons
2689960	2694760	is totalitarian. I respectfully disagree. I'm quite happy that people do not have private
2694760	2700440	nuclear weapons, and I do not think that people should have private AGI's. Similarly, I think
2700440	2705560	liability is very promising. I think it has to be strict liability, so liability for developers
2705560	2710280	rather than just users. This aligns the incentives of developers with those of wider
2710280	2715320	society. The point of liability is to price in the negative externalities for the people actually
2715320	2721800	causing them, so I'm a big fan of this. A third form of policy I would also suggest is a global
2721800	2729000	AI kill switch. This would be a protocol where some number of countries or large organizations
2729000	2735240	participate, and if some number of them decide to actually do this protocol, all major deployments
2735240	2740760	of frontier models must be shut down and taken offline, and this should be tested every six
2740760	2747960	months as a fire drill for five minutes to ensure full, so that hopefully we never need it, but if
2747960	2753640	we do, that at least the protocol exists. Thank you very much. There are lots of hands up. Hold
2753640	2759000	your questions. There will be more chance for Q&A later. Corner final remarks before we hand over
2759000	2764360	to the next speaker. I want to really say that I do agree that it is very hopeful to see that
2764440	2769960	the UK is trying to do things and is trying to push us forward into the good world, because what
2769960	2778040	we really need, as I said briefly, what we need is as a civilization to mature enough to be able
2778040	2783960	to handle dangerous technology. Even if we don't build AI right now, at some point we will build
2783960	2789640	something so powerful that it can destroy everything. It's just a matter of time. Our technology
2789640	2795240	keeps becoming more powerful. The only way for us to have a long-term good future is to build
2795240	2803160	the institutions, the civilization, the world that can handle this, that cannot build such things,
2803160	2808920	that cannot hold the trigger. I do think this is possible. I do think that it is, in fact,
2809480	2816200	so I have heard, in the interest of most people to not die. I think there is a natural coalition here,
2816200	2820600	but it is hard, and I will not deny this is an extremely challenging problem,
2820600	2825080	almost unlike, I mean, basically something we haven't faced in this nuclear proliferation,
2825080	2829880	and even then it's even worse this time. It's an incredibly difficult problem. It is not over yet,
2830760	2836040	but it could be very soon. If we don't act, if we let ourselves get distracted, if we fall for
2836040	2842600	propaganda, and all these things, these opportunities can be gone, and that will be it. But the game
2842680	2845640	is not over yet, so let's do it. Thank you very much.
2854360	2859240	So we've heard from a politician, a senior politician. We've heard from a technology
2859240	2866040	entrepreneur and activist. We're now going to hear from a professor who is zooming in
2866040	2871400	all the way from Kentucky from the University of Louisville. He's an expert. He's written several
2871400	2877880	books on cybersecurity, computer science, and artificial superintelligence. Ah, Roman,
2877880	2884280	I see you on the screen. I hope you're hearing us. Tell us, can we control superintelligence?
2885000	2889560	Over to you. No. The answer is no. I'll tell you why in a few minutes.
2893720	2897720	That's fine. So you can share your slides or talk to us whenever you're ready.
2898600	2904600	Let's do the slides. Connor did a great job with his
2906680	2909480	presentation. Let me see one second here.
2913400	2916520	In the meantime, we can see the covers of some of your books in the background.
2917400	2917960	Yes, absolutely.
2918680	2922040	Safety and security and artificial superintelligence.
2925000	2927880	We're now having a slight technical issue as the
2928600	2934840	technologist is found to slides. Great. Okay. Yeah, that's the hardest part. If I can get slides going,
2934840	2940200	the rest is easy. Okay, so I didn't know what Connor's going to talk about.
2942120	2946360	He did a great job. He's a deep thinker and covered a lot of important material.
2947000	2954600	I will cover some of the same material, but I will have slides. And I will slightly
2954600	2960680	take it to the next level where I may make Connor look like an optimist. So let's see how that goes.
2962840	2969640	To begin with, let's look at the past. Well over a decade ago, predictions were made
2969640	2976040	about the state of AI based on nothing but compute power. Ray Kurzweil essentially looked at this
2976440	2983000	scalability hypothesis before it was known as such and said by 2023, we will have
2983000	2989240	computational capabilities to emulate one human brain. By 2045, we would be able to
2989240	2996200	do it for all of humanity. So we are in 2023. Let's look at what we can do in the present.
2998440	3002680	In the spring of this year, a program was released, which I'm sure many of you got to
3002680	3010600	play with called GPT-4, which is not a general intelligence, but it performs at a level
3010600	3017000	superior to most humans in quite a few domains. If we look specifically at this table of different
3017000	3025240	exams, lower exams, medical exams, AP tests, GRE tests, it's at 98, 99th percentile of performance
3025800	3034760	for many of them, if not most. That is already quite impressive and we know that there are models
3034760	3041800	coming around, which are not just text models, but multi-model large models, which will overtake
3041800	3049160	this level of performance. It seems like GPT-4 was stopped in its training process right around this
3050120	3057080	human capacity and if we were to train the next model, GPT-5, if you will, will quickly go into
3057080	3062680	the superhuman territory and by the time the training run is done, we would already be
3063560	3069960	dealing with superintelligence out of the box. But let's see what the future holds according to
3070360	3079880	heads of top labs prediction markets. So we heard from CEO of Entropic, CEO of DeepMind.
3079880	3085160	They both suggest that within two or three years we will have artificial general intelligence,
3085160	3092680	meaning systems capable of doing human beings can do in all those domains, including science and
3092760	3100120	engineering. It's possible that they are overly optimistic or pessimistic, depending on your
3100120	3105960	point of view. So we can also look at prediction markets. I haven't grabbed the latest slide,
3105960	3113160	but last time I looked, prediction markets also had three to four years before artificial general
3113160	3121640	intelligence, which is very, very quick. Why is this a big deal? This technology at the level of
3121720	3126920	human capability means that we can automate a lot of dangerous malevolent behaviors,
3126920	3133880	such as creating biological pandemics, new viruses, nuclear wars. And that's why we see
3133880	3141800	a lot of top scholars, influential business people, in fact, thousands of computer scientists all
3141800	3149000	signed this statement saying that, yes, AI will be very, very dangerous and we need to take it
3149720	3157560	with the same level of concern as we would nuclear war. So what is the problem everyone is concerned
3157560	3167160	about? The problem is that, for one, we don't agree on what the problem is. Early in computer science,
3167160	3173240	early in the history of AI, concerns were about AI ethics. How do we make software which is ethical
3173240	3178520	and moral? And there was very little agreement. Nobody solved anything, but everyone proposed
3178520	3183160	their own ethical system, gave it a name and described what they had in mind.
3184600	3189400	About a decade ago, we started to realize that ethics is not enough. We need to look at safety
3189400	3196120	of those systems. So again, we started this naming competition. We had ideas for friendly AI, control
3196120	3201960	problem, value alignment. It doesn't really matter what we call it. We all intuitively kind of
3201960	3207640	understand we want a system which, if we run it, we will not regret running it. It will be beneficial
3207640	3214760	to us. So how can humanity remain safely in control while benefiting from superior form
3214760	3220600	of intelligence is the problem? I would like us to look at. We can call it control problem
3220600	3226040	and the state of the art in this problem. In fact, we don't really know if the problem is even
3226040	3231800	solvable. It may be partially solvable, unsolvable. Maybe it's a silly question and the problem is
3231800	3241880	undecidable. A lot of smart people made their judgments known about this problem. Unfortunately,
3241880	3249960	there is little agreement. Answers range from definitely solvable, from a surprising source,
3249960	3258600	like Eliezer Ytkovsky, to very tractable, from head of superalignment team at one of the top labs,
3258600	3265400	to, I have no idea, from a top tuning award winner who created much of machine learning
3265400	3271080	evolution. So I think it's an important problem for us to look at, to address, and to understand
3271640	3278920	how we can best figure out what is the status of the problem. My approach to that is to think
3278920	3285960	about the tools I would need to control a system like that, an intelligent, very capable AI. And
3286680	3293400	the tools I would guess I would need, ability to explain how it works, capability to comprehend
3293400	3299960	how it works, predict its behavior, verify if the code follows design, be able to communicate
3299960	3304440	with that system. And probably some others, but maybe some of the tools are interchangeable.
3305560	3311720	So I did the research and I published results on each one of those tools. And the results are not
3311720	3317400	very optimistic. For each one of those tools, there are strong limits to what is capable
3317400	3323400	in the worst case scenarios. When we're talking about superintelligent systems, self-improving
3323400	3328840	code systems, smarter than human, capable of learning in new domains, it seems that there are
3328840	3334920	limits to our ability to comprehend those systems, offer those systems to explain their behavior.
3334920	3341080	The only true explanation for an AI model is the model itself. Anything else is a simplification.
3341080	3348440	You are getting a compressed, lossy version of what is happening in a model. If a full model is
3348440	3353000	given, then you of course would not comprehend it because it's too large, too complex, it's not
3353000	3359400	surveyable. So there are limits to what we can understand about those black box models.
3360120	3365800	Similarly, we have limits to predicting capabilities of those systems. We can predict
3365800	3370360	general direction in which they are going, but we cannot predict specific steps for how
3370360	3375480	they're going to get there. If we could, we would be as intelligent as those systems. If you're playing
3375480	3380040	chess against someone and you can predict every move they're going to make, you're playing at the
3380040	3386600	same level as that opponent. But of course, we made an assumption that a superintelligent system
3386600	3394040	would be smarter than us. There are similar limits to our ability to verify software. At best, we can
3394040	3399560	get additional degree of verification for the amount of resources contributed. So we can make
3399560	3405800	systems more and more likely to be reliable, to have less bugs, but we never get to a point of
3405800	3411320	100% safety and security. And I'll explain why that makes a difference in this domain.
3411960	3419000	Likewise, human language is a very ambiguous language. It's not even as unambiguous as computer
3419000	3426120	programming languages. So we are likely to make mistakes in giving orders to those systems.
3427080	3432840	All of it kind of leads us to conclude that it will not be possible to indefinitely control
3432840	3438920	superintelligent AI. We can trade capabilities for control, but at the end, if we want very,
3438920	3443320	very capable systems, and this is what we're getting with superintelligence,
3443320	3449400	we have to surrender control to them completely. If you feel that the impossibility results I
3449400	3455160	presented were just not enough, we have another paper where we cover about 50 of those impossibility
3455240	3461160	results. It's a large survey in a prestigious journal of ACM surveys.
3462920	3468520	From the beginning of history of AI with founding fathers like Alan Turing who said
3469160	3475400	that he expects the machine will take over at some point to modern leaders of AI like Elon
3475400	3486120	Musk who says we will not control them for sure. There is a lot of deep thinkers, philosophers,
3486120	3494200	who came to that exact conclusion. We are starting to see top labs publish reports
3494200	3501000	in which they may gently acknowledge such scenarios. They call them pessimistic scenarios
3501000	3508360	where the problem is simply unsolvable. We cannot control superintelligence. We cannot
3508360	3513480	control it indefinitely. We are not smart enough to do it, and it doesn't even make sense that
3513480	3521080	that would be a possibility. They ask, well, what's the distribution? What are the chances
3521080	3528280	that we're in a universe where that's the case? They don't provide specific answers, but it seems
3529000	3534680	from some of the writing and posts they make, maybe about 15 percent is allocated to that
3534680	3541080	possibility. I was curious to see what other experts think, so I made a very small, very
3541080	3548200	unscientific survey on social media. I surveyed people in my Facebook group on AI safety,
3548200	3555960	and I surveyed my followers on Twitter, and it seems that about a third think that the problem
3555960	3561240	is actually solvable. Everyone else thinks it's either unsolvable or it's undecidable,
3561240	3567000	or we can only get partial solutions or we will not solve it on time. So that's actually an
3567000	3572120	interesting result. Most people don't think we can solve this problem, and I think part of the
3572120	3577720	reason they think we cannot solve this problem is because there is a fundamental difference between
3578600	3586680	kind of standard cybersecurity safety and superintelligence safety. And cybersecurity,
3586680	3593400	even if you fail, it's not a big deal. You can issue new passwords, you can provide someone with
3593400	3599160	a new credit card number, and you get to try again. We suspect strongly with superintelligent
3599160	3605720	safety, you only get one chance to get it right. There are unlimited dangers and limited damages,
3605720	3613960	either you have existential risks or suffering risks. And we kind of agree that 100% is not an
3613960	3621960	attainable level of security verification safety, but anything less is not sufficient.
3621960	3627560	If a system makes a billion decisions a minute and you only make mistake once every couple
3627560	3633560	billion decisions, after a few minutes you are dead. And so this is like creating a perpetual
3633560	3639000	motion machine. You are trying to design perpetual safety machine while they keep releasing more and
3639000	3647320	more capable systems, GPT-5, GPT-50. At some point this game is not going to end in your favor.
3648360	3654760	So I'm hoping that others join me in this line of research. We need to better understand what are
3654760	3661240	the limits to controlling superintelligence systems. Is it even possible? My answer is no,
3661240	3667800	but I would love to be proven wrong. It would be good to have surveys similar to the ones I
3667800	3673080	conducted on a larger scale to get much more statistically significant results.
3673800	3680520	And in case we do agree that we have this worst case scenario where we are creating
3680520	3686520	superintelligence and it is impossible to control it, what is our plan? Do we have a plan of action
3686520	3692520	for this worst case scenario? This is what I wanted to share with you and I'm happy to answer
3692520	3695480	any questions. Thank you very much, Roman.
3705640	3706760	Optimistic, Roman.
3709560	3716200	Sorry, one second. I'm trying to figure out how to use Zoom. Go ahead and repeat your question
3716200	3723400	please. You gave us many reasons to be anxious. What do you think is the best reason for us to be
3723400	3729880	optimistic? Well, there seems to be many ways we can end up with World War 3 recently, so
3729880	3731720	that can slow down some things.
3735400	3742200	It has been suggested that we can use a different kind of tool, which is the kill switch. Your
3742200	3747800	list of tools that you listed, it didn't include that. It's been proposed that each AI system should
3747800	3755080	be tested with a remote off switch capability. Have you looked at that? Do you think that's a viable
3755080	3763800	option? So I would guess a superintelligent system would outsmart our ability to press the off button
3763880	3772280	in time. It will work for not superintelligent AI, pre-GI systems, maybe even for the GI systems,
3772280	3777880	but the moment it becomes that much more advanced, I think it will outsmart us. It will take over
3777880	3782360	any kill switch options we have. Let's have some questions from the floor.
3786120	3790040	I can't see the hands, so yes, just give the microphone out. Thank you.
3791000	3797480	Thank you. I would like to ask, how does the scalable oversight that open AI is working on,
3797480	3803880	essentially the way they plan to align superintelligence, fit into your expectation of the
3803880	3811000	future pathway the AGI will take? Because again, as personally, we cannot align or control a
3811000	3816840	superintelligent entity, but another AI which is more capable than us could. So how does that fit
3816840	3823080	into your expectations? So it seems like it increases complexity of the overall system,
3823080	3828760	instead of us trying to control one AI. Now you're trying to control a chain of agents going from
3828760	3834360	slightly smarter to smarter to superintelligent, maybe 50 agents in between, and you're saying that
3834360	3840280	you have to solve alignment problem between all the levels, communication problem, ambiguity of
3840280	3847240	language between all those models, supervision. It seems like you're trying to get safety by
3847240	3852520	kind of upthuscating how the model actually works. You're introducing more complexity,
3852520	3857000	hoping to make the system easier to control that seems counterintuitive.
3857880	3862360	But isn't it the case that sometimes you can verify an answer without understanding the
3862360	3866920	mechanism by which the answer was achieved? For example, there can be a chess puzzle,
3866920	3870680	and you know a way of working out yourself, but when somebody shows you the answer, you can say,
3870680	3875720	oh yes, this is the answer. Isn't it possible? We don't need to really understand what's going
3875720	3881160	on inside these systems, but a simpler AI can at least verify the recommendations that come out of
3881160	3888280	the more complex AIs. So such a chain may be the solution. Can you claim that you are still in control
3888280	3892440	if you don't understand what's happening and somebody just tells you don't worry, it's all
3892440	3900200	good I checked it for you? But then it's like we humans, we have a network of trust. When I trust
3900200	3906120	some people and they trust others within various categories, we can't work out everything ourselves,
3906120	3913240	but we trust some scientists or some engineers or some lawyers who validate that an AI has a certain
3913240	3919000	level of capability and that AI could come back with verification that the proposals of a super
3919000	3924520	intelligence should be accepted or should not be. I don't say it's easy, but as you said there's
3924520	3930680	not likely to be a very simple and straightforward solution. Again, to me at least it sounds like
3930680	3936920	instead of trying to make this system safe, you said that you made some other system safe and
3936920	3943480	it made sure that the system you could make safe is safe for you. Let's take some more questions.
3943720	3948360	There's another one in the middle here. Then we'll go to the edge. Yes, thank you.
3950520	3956200	Thank you for the presentation. Number one, second thing is that as you're talking about,
3956200	3961400	I think as David was talking about trust basically, could you tell me from your
3961400	3967160	extensive years of AI research and experience as such that do you really think that humans
3967160	3974760	or society can be trusted to, for example, regulate its own self or do you think that
3974760	3982280	really need some sort of institution of sort that is totally separate from anyone else?
3984760	3991800	So I'm not sure regulation would be enough. Connor correctly pointed out that there is
3991800	3999560	both lobbying of regulators by the labs and also it becomes easy and easier to train those models
3999560	4005880	with less compute and over time you will be able to do it with very little resources. The only
4005880	4013960	way forward I see is personal self-interest. If you are a rich young person and you think this is
4013960	4019000	going to kill you and everyone else, maybe it's not in your best interest to get there first.
4019000	4023160	That's really the only hope at this point, just personal self-interest.
4024360	4028200	The humans are always better if we can band together with our self-interest rather than each
4028200	4032440	of us individually pursuing our self-interest. So I think this kind of meeting and the community
4032440	4039800	spirit might help. There was a hand over here, yes, with I think the red shirt on jacket.
4039800	4050760	Yeah, if we assume that the two kind of well both views that have been suggested so far are
4050760	4057640	correct in that we're definitely not going to be able to stop AI development etc and we're going
4057640	4061880	to get to the point where we have no regulation that can effectively stop things. You know people
4061880	4066520	can build in super intelligent AI on their own computers etc okay so we'll assume that that's a
4066520	4072040	fact that's coming and then we'll also assume that the control problem isn't a problem because
4072040	4075000	it's a problem that can't be solved and we're definitely not going to be able to control it.
4075000	4079880	Well now we're heading and barreling towards the point where we have super intelligent
4079880	4086440	AIs definitely and we definitely can't control them. What comes next? What comes next?
4088200	4092440	It's a wonderful question. As I said and published you cannot predict what the super
4092520	4101640	intelligent system will do. All right so was there a question down here? Thank you.
4104200	4109160	You said that we kind of need a plan but on that last question if that scenario is true.
4110040	4113320	You said we need to do more work in this area but do you have any thoughts as to
4113880	4117800	what we should be doing? What we should be doing to plan for the worst-case scenario?
4118120	4126600	So to me at least it seems that at least in some cases it is possible to use this idea of personal
4126600	4131960	self-interest. If you have a young person having a good life there is no reason why they need to
4131960	4137640	do it this year or next year. I understand that someone may be in a position where they are very
4137640	4143240	old very sick have nothing to lose and it's much harder to convince them not to try but at least
4143240	4150600	from what I see the heads of those companies are all about the same age they young they healthy they
4151320	4158440	they have a lot of money there is a good way to motivate them to wait a little bit maybe a decade
4158440	4165240	or two just out of personal self-interest again. I think my answer to the question of optimism is
4165240	4171800	that we humans can do remarkable things we humans can solve very hard problems and so I want to say
4172760	4178040	now that we spread around what the problem is at least some more people can apply more brain power
4178040	4187000	to it so that's my reason for optimism. Terry? I guess I'm pleased by the inevitability of this
4187000	4193800	development because it seems to me that if you're going to create reasoning creatures
4194680	4201480	then those reasoning creatures are going to have moral rights on the same plane as human beings
4202200	4211560	so I'm looking forward to chatting with these creatures and joining in them joining into this
4211560	4215800	kind of discussions and I'm pleased that they won't be able to be thwarted and it will be wrong
4215800	4223160	to enchain these reasoning creatures. Sir Roman are you looking forward to having more of the AIs
4223160	4231400	involved in these discussions as well? I remember giving a presentation for a podcast about
4232040	4238360	rights for animals rights for AIs and I was very supportive of all the arguments developed because
4238360	4243640	I said at one point we will need to use those arguments to beg for our rights to be retained.
4245880	4255400	The question on the third row here? Yes hi I'm curious Roman which side of in your hopes of a
4255400	4260760	possible future for us to get through this do you have more hope on the side of a more top down
4260840	4268520	sort of totalizing control system for AGI systems such that they remove the possibility of
4268520	4274440	individual actors getting hold of this and weaponizing it or do you put more hope in a more
4275080	4282120	sort of decentralized open source approach to AGI emergence more like an ecology perhaps some
4282120	4287960	people suggest would be more biologically inspired such that you know immune system like functions
4287960	4295400	could arise. Which way do you lean in your sensibilities for what is a viable avenue for us?
4296200	4302600	I'm not optimistic with either of those options the only kind of hope I see is that for strategic
4302600	4308920	reasons superintelligence decides to wait to strike it will not go for immediate treacherous
4308920	4314680	turn but decides to accumulate resources and trust and that buys us a couple of decades
4314680	4319960	that's the best hope I see so far. So we slow things down we'll have more chance to work out
4319960	4325880	solutions and the slowing down might come from a combination of top down pressure and bottom up
4325880	4332520	pressure. Maybe have a is there a hand at the very back there yes let's try and get the microphone back
4332520	4338840	there. Right at the sitting at the back yes.
4342840	4344040	Sorry at the in the middle.
4349880	4356040	Thanks. Hi Roman thanks for your talk. Yeah I was wondering what your thoughts are on
4356600	4363560	aligning the first AGI that is human level or narrowly superhuman if in principle that is possible
4364120	4372920	and if that is is is it possible in principle to align the next version of AGI but to use that
4372920	4382200	narrowly superhuman AGI to align it and if if that's all technically possible then why would we
4382200	4391320	not think like focus on doing that and also and also if you think in principle alignment is
4391320	4400440	impossible and control is impossible then why why not work on practical ways to make the
4401560	4407160	to make whatever AGI is created as nice as possible that is like better than the counterfactual of
4408120	4415560	try to stop it it won't stop and you know it won't be nice. Well I definitely encourage everyone
4415560	4422360	to work on as much safety as you can anything helps I would love to be proven wrong it would
4422360	4427960	be my greatest dream that I'm completely wrong and somebody comes out and says here's a mistake in
4427960	4433880	your logic and we have developed this beautiful friendly safe system capable of doing all this
4433880	4439800	beneficial things for humanity that would be wonderful but so far I haven't seen any progress
4439800	4444760	in that direction what we're doing right now is putting lipstick on this monster and the
4444760	4450840	shadow that's all we're doing filters to prevent the model from disclosing its true intentions
4450840	4457000	when you talk about alignment it's not a very well-defined terms what values are you aligning
4457000	4463960	it with values of heads of that lab values of specific programmer we as humans don't agree on
4463960	4469960	human values that's why we have all these wars and conflicts there is a 50-50 split and most
4469960	4476760	political issues in my country we are not very good at agreeing even with ourselves over time
4476760	4483160	what I want today is not what I wanted 20 years ago so I think this idea of being perfectly
4483160	4489080	aligned with 8 billion agents and people are suggesting adding animals to it and aliens and
4489080	4494680	other AIs that doesn't seem like it's a workable proposal our values are changing
4495400	4502120	they're not static and it's very likely that they will continue changing after we get those
4502120	4508760	systems going I don't see how at any point you can claim that the system is specifically value
4508760	4515000	aligned with someone in particular the last question in this section is going to go to Connolly
4517000	4521640	Roman love your talk I always love your optimism it's always great to hear you talk so
4523800	4528440	so I'm kind of like gonna pick up on the question I was just asked and just give a bit of my opinion
4528440	4535080	and kind of like hear what you think about this as well so my personal view is that I do I have
4535080	4541720	read many of your papers in fact and they're quite good so I do think that I agree with you that like
4541720	4547720	in principle an arbitrarily intelligent system cannot be safe by any arbitrary like weaker system
4547720	4554840	just kind of by proof of like you know program size induction and whatnot but in my view it does
4554840	4563000	seem likely that there is a limit of intelligence far below the theoretical optimum but still
4563000	4569560	significantly above the human level that can be achieved the reason I think this is that
4570280	4575880	human civilization is actually very smart compared to a single caveman and can do really really great
4575880	4583160	things so my point of optimism is it seems possible that if we stop ourselves from making
4583160	4588200	self-improving systems and coordinate at a very strong scale and have very strong enforcement
4588200	4594600	mechanisms it should be possible to build systems that are you know end steps you know above human
4594600	4601320	good enough to build you know awesome you know sci-fi culture ship kind of like worlds but not
4601320	4610600	further I'm wondering if you have an intuition about like where do things hit impossibilities
4610600	4617880	like to me I think the impossibilities happen above human utopia but to get to the utopia put
4618200	4623160	you already have to do extremely strong coordination extremely strong safety research
4623160	4627400	extremely strong interpretability extremely strong constraint on the design of the agis
4627400	4630840	extremely strong regulation which I think is in principle possible wondering kind of like your
4630840	4635880	thoughts about that kind of outcome so Conor's not asking about responsible scaling he's asking
4635880	4640760	about limited superintelligence if we had limited superintelligence could we get everything we want
4641400	4646920	without having the risks that we all fear so I think I want to emphasize difference between
4646920	4652280	safety and control is it possible to create a system which will keep us safe and some
4652920	4659400	somewhat happy state of preservation possible a way in control no that system is the example you
4659400	4664920	give of humanity so humanity provides pretty nice living for me but I'm definitely not in control
4664920	4671080	if I disagree with society and many issues in politics and culture it makes absolutely no
4671080	4677880	difference I don't decide things scale it to the next level all 8 billion of us may want something
4677880	4682680	but this overseer this more intelligent system says it's not good for you we're not going to do it
4683640	4688280	this is what you're going to be doing right now so think about all the decisions you make throughout
4688280	4694600	your day you decided to eat this donut you smoke this cigarette all those decisions were made by
4694600	4699560	you because you felt you wanted to do them they may be good or bad decisions but if you had this
4699560	4705400	much more intelligent personal advisor ideal advisor you would be at the gym working out eating
4705400	4711640	carrots you may have a long healthy life but you're not in control and your happiness level
4712280	4719720	may be questionable thank you very much roman for sharing your thoughts pessimism and some
4719720	4724680	optimism thanks for moving the conversation forwards
4734040	4739560	I'm now going to invite the five members of the panel to come up on stage and they're each going
4739560	4745320	to have a couple of chances to pass some comments on what they've heard so there's some stairs over
4745320	4754360	there that you can come up to we're going to hear from jan tallan who is the co-founder of skype
4754360	4761240	the co-founder of fli future of life institute and also cesar the center for study of existential
4761240	4768040	risks we're going to hear from eva barons a policy analyst with the international center for future
4768040	4773400	generations we're going to hear from tom ohl who's a journalist who writes from time to time for the
4773400	4783480	bbc amongst other places we're going to hear from alexandra musa visadievic who is the ceo of evident
4783480	4789240	and has a track record with tortois media in many other places and we're going to hear from also
4789240	4796200	another representative from conjecture that's andrea miotti who is their specialist for ai policy
4796200	4801480	and governance so to start things let's just hear from each of them if you if you're opening remarks
4801560	4806200	jan what's your comments from what you've heard so far have you changed your mind in any ways or
4806200	4811080	other things that are missing from the conversation yeah you all have to speak into the mics i'm
4811080	4819480	being told so i yesterday i was at the dinner i was invited to a dinner and and my response to
4819480	4826600	an invitation was that okay i will come but you have to invite connor because he's making very
4826600	4833800	similar points to me only much much more intensely so yeah i basically agree i agree with what what
4833800	4840280	connor said uh my main caveat would be that for the last decade or so i've been kind of trying to build
4841960	4852680	a lot of friendly cooperation between people in the kind of ai companies and making sure that
4853480	4857400	everybody can understand that it isn't in their interests with almost everybody
4859320	4863240	let's be honest almost everybody understands that is in their interests to
4863880	4870360	and of remaining control and and not kill everyone else and so
4871960	4877880	like for example i am a board observer observer to entropic and entropic is one of those companies
4877880	4881960	just like conjecture when you go there you can talk to anyone from the receptionist
4882360	4887080	to the to the ceo and they are aware of the ai risk i'm very concerned about this
4887800	4896120	but yes i do think as i've said in several places that i don't think they should be doing what they're
4896120	4901320	doing right so these companies don't really want to do what they're doing but they feel they have
4901320	4907480	to otherwise they might be left behind so yes so there is this uh product basically dilemma in
4907480	4914520	when you want to do you know safe ai one is that you're safe like when you're trying to figure
4914520	4921480	out how to do safe ai from one hand you have groups like miri uh that the aliases with kowski
4921480	4928280	co-founded and that was the person who got me involved in ai safety uh 15 16 years ago uh
4928280	4932360	where basically the claim is that you have to start really early even if you don't know exactly
4932360	4938600	what the ai is going to look like because then you have a lot of time to prepare and then the group
4938600	4943640	on the other end of that axis is entropic where they say that it's kind of useless to start early
4943640	4948040	because you don't know what you're dealing with uh so you need to be as informed as possible
4948040	4952600	so in that strategy you need to be just always at the frontier and and dario has been very public
4952600	4957400	about this about this strategy of course the problem there is that like it also works as a
4957400	4964200	perfect justification to race right so so uh therefore it's uh i have like double digit uncertainty
4964200	4970520	both ways uh about what is what the actual picture is and so i do think that this point
4970520	4975000	the labs indeed they are involved in death race and they they there is the government
4975000	4979960	intervention needed uh to to get a time out there and we definitely need time out because we don't
4979960	4990440	have uh enough safety results uh and but to yeah romanial boski's uh presentation i call i'm definitely
4990440	4995640	more optimistic again as as on one of these slides there was the dialogue here with eliezer and
4995640	5002280	the eliezer was confident that this can be sold and in fact like i'm super glad that earlier this year
5002440	5009640	uh david tarimble uh his group got uk government funding and he has this approach called
5010600	5016680	open agency architecture i don't know exactly uh what the details there are but like my rough
5016680	5025480	understanding is that you you're using uh you're scaling ai capabilities and access uh according
5025480	5033240	to formal statements that the ai's produce and then you use not ai not humans but formal verifiers
5033240	5040920	to to verify uh those those statements therefore like building up your kind of ai capabilities
5040920	5045400	one formally verified step at the time uh there are many criticism of that but it's like one of
5045400	5053880	those approaches that is kind of at least and principle uh has like some convincing story
5053880	5058440	that that why it should work in in principle at least so there are some options that might work
5058440	5062920	but we're going to need time to develop them exactly so that's why like i've been working on
5062920	5067560	like i've been supporting ai i've taken research for more than a decade now but unfortunately we
5067560	5072920	just didn't make it uh we only need more by more time so let's hear from either because you work
5072920	5080920	more with possibilities to inspire policy you've seen examples of policy in the past slowing down
5080920	5087400	some technological races are you do you see reasons for optimism do you see ways in which
5087400	5092920	politicians can make a good difference to the landscape we're discussing definitely definitely
5092920	5097960	that very much plays into some of the problems or the issues characteristics of the problem that
5097960	5102680	both corner spoke about and that also yantan you just mentioned that one of the problems that we're
5102680	5107000	facing here is a human coordination problem and one of the ways to address that will be through
5107000	5112040	policy as has been said many times this evening this is a technology that threatens to kill us
5112040	5117800	to kill us all um and the the heads of the government of the companies that are driving
5117800	5122280	forward the technology have agreed that and publicly stated that that might be the case
5122280	5127160	and yet they seem to be locked into this dilemma that yantan just mentioned where they are for some
5127160	5132200	or other reason impossible to to stop so i think that is a point where where government can really
5132200	5138520	make a difference and step in and also should step in and we've seen that as you hinted at we've
5138520	5144120	seen that work in the past one of the examples that i often think about is the montreal protocol
5144120	5151880	which after the scientific consensus arose that cfcs and other similar gases actually destroy
5151880	5158200	the ozone layer the international community did come together in 1987 and agreed through the
5158200	5165240	montreal protocol to slowly phase out these gases so we see here that international cooperation
5165240	5170520	by the international community by governments can succeed also in the face of the short-term
5170520	5176520	economic interest of private sector companies in the public interest of well in the end in
5176520	5181560	everyone on earth so i'm not saying that it's necessarily easy but i think it's definitely
5181560	5186760	possible and it is one of the strongest levers that we have here to make a difference so i think
5186760	5191880	that's definitely something that we should lean into very strongly and do our best that
5191880	5197960	that actually happens the montreal protocol is an encouraging example but we haven't made a very good
5197960	5202360	job of the governments in the world of controlling carbon emissions we've been talking about it for
5202360	5207080	a long long time and maybe there's some progress but many people feel this is an example where
5207080	5212520	governments can't cooperate so what makes you think that we can cooperate with the problems of AI
5212520	5218760	more like the montreal protocol rather than the paris agreement to say well part of this of course
5218760	5222280	is also that i think that this is one of the few levers that we have to make a difference
5222280	5227400	at all so i also hope that we will be able to do it and i agree that looking at past climate
5227400	5232760	conference is one of the negative examples that we see there is that with these conferences
5232760	5237800	sometimes that the outcomes tend to be very watered down just because the the focus lies on
5237800	5242440	building consensus among all of the different countries that attend and then in the end you
5242440	5246680	want to have a nice little consensus agreement that everyone signs so you can demonstrate that
5246680	5251160	everyone's on the same page and everyone goes home and everyone's happy happy and i can just say that
5251160	5255880	i think with the ukai summit that's coming up now first of all that is a unique opportunity to
5255880	5260680	actually have international cooperation and coordination on this issue take place you need
5260680	5264920	to create the opportunities for stuff like that i'm really happy that the uk government took
5265480	5270200	the initiative and created this opportunity i am one thing that makes me optimistic is
5270200	5274840	that we all know that china is going to attend at least on one of the days so hopefully they
5274840	5279880	will be able to be brought into the fold and yeah then i just hope that this opportunity is truly
5279880	5284840	taken and that the outcome of this summit will not be just some vague commitments to long-term
5284840	5293320	plans but ideally concrete binding commitments to to concrete next steps let's turn to alexandra
5294040	5300840	you work a lot with businesses businesses are unsure in many ways how to deal with today's ai
5300840	5306280	do you think there is good advice that they can be given or is there a sleepwalking process with
5306280	5315000	many of our businesses definitely the latter i would say um so i i'm seo of evident we map
5315720	5323240	benchmark companies on how far they are in their ai adoption and so when um i think it was kind of
5323320	5330280	you mentioned the ai race that is on the front line of development um in ai there is also a race
5330280	5337400	as we all know going on in terms of adopting ai as as quickly as possible there's a sense of being
5337400	5346520	there's a sort of geopolitical debate on ai development between us europe china and so on um
5346520	5351640	and who's leading on that not only in ai but also in areas like quantum but in the business level
5351640	5357720	which is where i deal with um spend my time uh mostly there's a definitely a race on in terms of
5357720	5364920	not being left behind in adoption of ai and it's an economic question it is a existential question
5364920	5371800	so there's an existential question on sort of two dimensions in this debate and um and so you've got
5371800	5379960	this um unstoppable race going on on the front end of ai and then you've got an unstoppable race on
5380520	5387240	actual deploying ai at a business level and um it's going to be very hard for regulators to keep up
5387240	5393800	and to eva's point i think um in terms of what we hope will be the outcome often unfortunately
5393800	5400680	comes with with the catastrophic um happening taking place before it really sharpens the minds
5400680	5407560	and people figure out um how urgent it is i think there's a real sense of urgency in in the community
5407560	5413080	around trying to work out uh what what the guardrail should be whether it should be a constitution
5413640	5420920	um or or how we should think about implementing safety mechanisms in as as we develop um further
5420920	5427640	on our chat on our large language models but um i hope it doesn't need a catastrophic moment for
5427640	5433960	that to sharpen but back to the business question there is this um hope that maybe businesses will
5433960	5441400	self-regulate and i think that is maybe the case in highly regulated sectors you see in the banking
5442600	5448120	sector and insurance or banking in particular that there is a guardrail is put in place there but
5448120	5455000	that is um that there's a lot of businesses that don't have that regulation around them and i think
5455000	5460040	there is a real risk for this completely running out of control at a business level as well
5460760	5466920	would you advise businesses to self-regulate ahead of standards and regulations being agreed
5466920	5471640	by governments i think that's what they're doing or trying some businesses are trying to do there's
5471640	5480360	a big risk in um in the case of winning trust with your customers and also your your shareholders
5480360	5489160	and and investors if you mishandle ai and you create issues around not taking into account
5489160	5496360	how to properly deal with biases and other issues that is a situation that can create a real
5496360	5503000	breakdown and trust with your with your organization so there is that risk and then there are businesses
5503000	5508920	that don't necessarily lean on trust for their for their for their business and those are the
5508920	5516440	ones i worry the most about indeed let's turn to tom o is a representative of the world of
5516440	5521800	journalism do you feel journalists have helped the discussion about the existential threat from
5521800	5527880	ai or have they muddied the water leading people to panic unnecessarily or perhaps get distracted on
5527880	5536600	side issues rather than the main issue i think it's um all of the above aside from overrugging
5536600	5541000	the pudding i think most people in this room including me have had a wit scared out of them
5541880	5550840	by some of the talks just now one has side issues in journalism coverage of ai and i think
5551560	5558760	the jobs market is one of those but i have been surprised pleasantly so by how things
5558760	5564600	have progressed since 2016 and that's the first time that i wrote about ai safety and i think at
5564600	5571400	that point the prospects of a bad scenario relating to ai was seen as about as likely by my
5571400	5578120	colleagues as lester city winning the premier league um anyway several years later um i now see
5578120	5584360	lots of my former colleagues writing to my mind um very informed pieces about ai safety and i think
5584360	5591160	that's helped the public change well arrive at a view um and probably a lot of people in this room
5591240	5596440	aware that the american public when polled now says that they want regulation of ai
5596440	5601480	and they want a lot of it and i think we can credit journalism with some of that journalism
5601480	5606680	should be doing more but it's more than i would have thought a few years ago and if you were to
5606680	5611560	go away and write up a story about things that you might have changed your mind about tonight
5611560	5616360	and that the public should pay attention to can you give us a sneak preview what that would include
5616360	5625880	well i think um the idea of runaway ai is is not new um but i think it has been difficult
5625880	5631000	historically to frame it in a way that really sticks and like really drives its way down your
5631000	5639880	brainstem um and we have different ways of framing ai risk um and mr fosuliman's new book um which
5639880	5645480	some of you might have read i think there's a pretty good job of framing it in a in a in a way
5645480	5653240	in which he describes um ai being used to accelerate human ingenuity in whatever endeavors
5653240	5660920	humans are up to um be they um be they good or be they bad that's one way of framing it um and i
5660920	5666840	think um we've heard some pretty compelling ways of telling a story of runaway ai which is a
5666840	5674760	different and scarier story thanks and let's turn to andrea and your role at conjecture
5674840	5682200	what are you doing in a day by day basis to address this question well what we're trying to do and
5682200	5688120	i mean kind of covered a lot of it is first of all to explain the problem to people uh i've been
5688120	5693400	heartened by the public reaction in the last years like i also got to know about this problem
5693400	5700520	quite a long time ago and i in the past i could almost not expect the day that major governments
5700520	5705960	take this problem seriously and the public understand this problem and we all get together
5705960	5713560	and take some initial promising insufficient but promising steps to address it uh and i think
5713560	5721320	is figuring out policy solutions and the reality is that we don't obviously we don't have a playbook
5721320	5728920	for what exactly they look like but what i think was a common theme of the of the talks tonight
5728920	5737880	is that clearly at some level of power we are not in control anymore and everybody expects this
5739400	5746120	those who don't expect this are misguided or expected but don't don't say it and
5747160	5754200	the positive thing is that there is one physical resource that drives the majority of what makes
5754200	5761080	these systems powerful which is computing power and it's a physical resource not not like you know
5761080	5768680	algorithms that you could just write on a piece of paper it's traceable it's expensive large place
5768680	5773800	in data centers and while you know the scaling hypothesis the idea that you know the more
5773800	5777960	computing power you put into something the more powerful it becomes might head some
5778600	5783720	diminishing returns at some point we do not see any reason to expect it to stop so
5784680	5789960	we know you know from both sides companies know that more computing power leads to more power
5789960	5795400	and that's why they're doing what they're doing we know that limiting that computing power is a
5795400	5804760	very effective way to kind of stem the the bleeding and stop and our positive situation for a while
5804760	5810120	take a time out have the time to figure out the solutions have the time to absorb this into society
5810120	5815080	and how much time will that give us because there's a risk that people will use today's models
5815640	5821960	to design much more efficient ways to build next generation models and so they could therefore
5821960	5829080	come under the radar as it were that people who were watching for a large use of GPUs would miss
5829080	5834440	the clever way that somebody has built it so do we have a decade do we have three or four years
5834760	5840760	or how long yeah that's that's a great question it's captain computing power is not a permanent
5840760	5848360	solution but it's one of the best solutions we have at the moment uh as others have said before
5848360	5853080	we are in a double exponential it's not a single exponential we have an an exponential growth of
5853080	5859800	computing power hardware and exponential improvement in software we need to start
5860680	5866520	cutting down on one of the two uh cutting down on a compute depends where you put the cap
5867160	5873800	probably will buy us five seven years you can make you can make what would seem to people at
5873800	5878920	the frontier extremely strong caps that would affect you know less than 20 companies in the
5878920	5884680	world that probably could buy you 10 years in that period we need to figure out all of the rest
5884680	5889480	it's gonna be a hard problem but we have done it before with nuclear weapons we've done it before
5889480	5893800	with biological weapons we can do it again we're gonna go around the panelists one more time in
5893800	5897960	the same order i'll give you a chance a choice panelist you can either comment on what you've
5897960	5903960	heard from somebody else or you can paint me a picture of what would be a successful ali safety
5903960	5909000	summit in bletchley park if things go well what would be the outcome and what would also be the
5909000	5915400	follow-up so yarn first well i'm the one of the authors of the post letter so it's like
5915400	5922280	indefinite moratorium uh on further scaling uh would be sort of my wet dream from outcome from
5923000	5928520	from this summit or perhaps the next one if this one isn't realistic and what's the chance do you
5928520	5934680	think what might cause the assembled world leaders to have an intellectual breakthrough and say yes
5934680	5941240	actually we do need to have this indefinite pause so currently i'm not very optimistic on on that
5942040	5945960	perhaps perhaps but perhaps in six months it would be much more clearer
5946600	5952680	why this is needed so and and we have more time to kind of do the necessary loving
5952680	5957080	so the discussion is prepared to ground and when something really bad happens in six months when
5957080	5962520	gpd 5 comes out and oh my god at least we'll know what we should be doing yeah i mean let's not forget
5962520	5969000	that gpd chat gpd has been out less than one year so like the world was very different one year ago
5969960	5975640	same question to you either yeah thank you i think i'm just going to build on top of jan's ideal
5976200	5981400	outcome of the summit and say that i would also find it terrific if the summit could be the first
5981400	5987720	in a series of repeated um summits like this where world leaders come together because as we i think
5987720	5992920	a pretty clear picture has been painted tonight of the fact that the field of ai evolves very quickly
5993000	5999080	and is going to continue to evolve very quickly if not ever quicker and because of that i think it
5999080	6005160	would be very valuable if we would have a regular occasion for world leaders to come together and
6005160	6009880	not only make sure that the rules that they came up with are upheld but also to reevaluate
6009880	6013880	whether they still make sense and where they need to be adapted or whether new real rules need to
6013880	6019640	be introduced as for example measures like compute control that andrea mentioned they buy us some
6019640	6024280	time but at some point they might not be applicable anymore so not just agreement on rules but setting
6024280	6029640	up some audit process so that we can figure out whether the rules are being forward or not for
6029640	6034920	example yeah same question to you i would agree you have to build in i mean right now it's just
6034920	6041080	based especially in the us um the talks that have been held in the white house and by chuck schumer
6042120	6049080	the gatherings have led to sort of ideas around voluntary um adherence to some principles but
6049080	6055240	there is absolutely no built-in audit or accountability um so i think that we've got
6055240	6060680	to see that come out of of the uk's ai safety summit among other things maybe there has to be
6060680	6067800	something more concrete around licensing um of the models and and the use of them and that they
6067800	6074360	have to pass some kind of a threshold i think the risk of of bad actors getting hold of them is
6074920	6081320	is a is a much higher risk i think the ia ea structure is is one that one can look at but the
6081320	6088600	the nuclear a lot of the success probably of the ia ea lies in the mutual assured destruction
6088600	6096520	of humanity by using um nuclear weapons and this might be the same situation but they're easier to
6096520	6103240	monitor i think um i think this might be slightly harder because you can land in the hands of bad
6103240	6107880	actors more easily we haven't really discussed bad actors much in this session tonight maybe that
6107880	6114520	makes the things even more horrifying we might come back to that later and tom what's your answer
6114520	6118040	what would you like to see come out of the summit or maybe you've got some comment on something else
6118040	6128040	you've heard from the other speakers well i'll talk about the summits um often when CEOs of um
6128040	6134440	labs developing agi are asked about regulation um they basically say bring it on we love some
6134440	6140200	regulation um and i think it would be great if politicians could actually put that to the test
6142840	6150520	very good and andrea what would you like to see if you were invited to bletchley park and given
6150520	6156520	their microphone for two minutes what would you entreat the assembled world leaders to consider
6157400	6162440	well i i don't want to be too hopeful as some others here have been but at the very least i
6162440	6167000	would like to see a commitment to the fact that this is extremely dangerous technology
6167960	6174440	continuing to scale leads to predictable disaster and we need to pull on the brakes right now
6175160	6180680	we have a lot of applications are very beneficial we can focus on those but limit this death
6180680	6187160	phrase to the ever more powerful ever more obscure general systems that we can control what they
6187160	6194360	definitely do not want to see is a diplomatic shake of hands where companies write their own
6194360	6200280	playbook and say we're gonna keep doing exactly what we we're doing right now but it's gonna sound
6200280	6206120	responsible and governments can wash their hands and say well we did our part let's move on that
6206120	6211960	would be a very bad outcome right i'm gonna ask for three questions from the floor i'm gonna get
6211960	6216600	the panel to think which ones to answer but i'm trying to take people haven't asked before so on
6216600	6223560	the second row here there's a hand up here and then also the second row over there next as well
6223560	6229320	let's take three fairly short questions please hi first of all thank you very much for coming
6229320	6235160	here tonight and sharing your expertise with all of us in this whole question this whole discussion
6235160	6241560	there's the implicit assumption that agi is coming and it's coming soon and the million dollar
6241560	6248040	question i guess is when exactly is it coming but a more practical question is what are some
6248040	6253480	warning signs and do we already see some of those in the systems that we have currently deployed
6254120	6258040	great let's have a question over here as well sorry the microphone's gonna have to
6258680	6261000	run around at the end of the second row there
6261000	6270040	thank you for great panel um my question's related to yon's comment at the beginning
6270040	6277160	on essentially dario amadeus philosophy which is you know and and also related to i guess
6277160	6283080	roman's uh talk which is how do we solve the control problem and what i've heard the large
6283080	6290360	our labs repeat is oh you know we need to increase capability that's only better AI that is going to
6290360	6297000	be able to help us figure out how to solve AI and there's sort of this race to increase capability
6297000	6303720	up to the point that can help us solve it but no further and and and just sort of thoughts on that
6304280	6311720	philosophy and and and and you know uh whether there might be something to it or is it just a
6311720	6317800	completely risky game you know thanks and there was one in the middle of the third row there
6318360	6320760	pass the microphone along please to the middle
6322680	6327640	how long of a time frame do you think we have between the arrival of agi and the arrival
6327640	6333800	of superintelligence and within that time frame could there be tractable solutions for alignment
6333800	6339000	or the control problem and if so would those solutions be able to be implemented before
6339000	6346280	hurry up and develop better AI third question was how long might it take between the arrival of agi
6346280	6353160	and superintelligence and whether there would be time for us to work out solutions then and my
6353160	6358120	question i guess is well what's all this about agi isn't the bletchley park summit set up to
6358120	6363480	discuss something else which is frontier models which says that there are catastrophic risks
6363480	6370920	even before we get to agi so i'm gonna go around the same order again i'll be a bit predictable
6370920	6376360	jan you want to pick one of these questions maybe i mean the answer to all the all three
6376360	6383240	questions is uncertain and that's that's why we need to pause and kind of take a time out and see
6383240	6388440	like how can we kind of create more set more certainty about these things i think i would
6388440	6395320	answer the entropic question specifically that definitely is a lot of truth to the to the point
6395320	6401800	that like the more capable model you have to work with the more kind of better position you are
6401800	6409560	empirically you can you can kind of be too like too science in a way that you just can't do with
6409560	6417160	models from from 10 years ago and also like one claim that people on topic to make is that like in
6417160	6423320	some ways it becomes easier as the model kind of has better understanding of what you're trying to do
6423880	6432040	with it or to do to it but that said again it's a to put it lightly it's playing with fire so so
6432040	6438200	it's i'm not sure if anyone should be doing it either and jan said it's all uncertain but can't
6438200	6442920	we at least agree in advance some canary signs that will make us say things are happening faster
6442920	6449560	than we expected well i mean if we look at the past there were several signs that people agreed on
6449560	6456120	that they might point out that we're getting into a zone where ai is maybe more capable than we think
6456120	6463640	it is and i mean we certainly have seen signs connor mentioned or was it roman mentioned that
6463640	6470520	current models um outperform most humans on things like the bar exam um these are clearly
6470520	6475960	advances in capabilities that um i almost wonder sometimes if we just become desensitized to them
6475960	6480840	because we move so fast i mean again charge upt came out a couple months ago and it's already
6480840	6486360	just normal and people are waiting okay what's the next big thing so um it doesn't really help
6487080	6492840	to think retroactively have there been any signs um if you didn't take them to actually stop and
6492840	6497880	reconsider what you're doing so i think one of the big problems here is not have there been signs
6497880	6503640	a big problem is can we pre-commit to stopping when we see certain signs and then actually stop
6503800	6509640	actually take certain actions and we just haven't seen that before so this is developing contingency
6509640	6516440	solutions like we're meant to have had contingency solutions for pandemics yeah yeah any comments
6516440	6523160	alizandra i i will leave the um well how long it's going to take to reach agi to to the experts on
6523160	6529560	the panel but on the outcome of the summit and i think that there is a bit of a confusion sometimes
6529560	6537080	and in in what we are expecting to be achieved from the discussions on regulation because there's
6537080	6544680	an obvious very important urgent and existential question around regulation regulating for the
6544680	6551160	long term but then we also have businesses that are sitting and waiting for regulation that is here
6551160	6556600	and now how is it going to impact my particular sector how is it going to impact what i'm doing
6556600	6563560	today and what are the immediate and very very real risks right now here today that we are seeing
6564360	6569960	with ai having impact on you know media with disinformation and so on but then there's also
6569960	6578040	the specific um aspects to how that is implemented in particular sectors so i um hope that we would
6578040	6584280	see addressing both of those short term and long term questions thanks tom any thoughts
6584360	6590840	yeah on yardsticks i think it's worth remembering that the canonical yardstick was the Turing test
6591720	6599080	and that's long gone um ai's can now beat humans at um diplomatic based games for instance um and
6599080	6606760	much more um the modern Turing test is i think quite an interesting proposition um and that's
6606760	6613720	the test of whether the ai can i think make a million dollars very quickly um but as as eva says
6613720	6619080	we must stop shifting the gold posts um we need to agree that the one is we should pick one
6619720	6623400	agree that that's the one where we start taking it seriously and then take it seriously when it
6623400	6629400	is passed which it will be quite soon but in the past people said you won't manage to solve chess
6629400	6636120	unless you have got a full grasp of all aspects of creativity and so on and then when deep blue did
6636120	6639800	win at chess people said oh well it's not actually doing it in the way that we thought would be so
6639800	6645640	terrible it's just grunting out incredibly so i feel there will always be people who
6645640	6650040	don't move the gold post but they'll say well how it was implemented it doesn't demonstrate
6650040	6657960	too intelligent yeah i think that's a good point um and it reminds us something um conna said um
6657960	6664520	which is that there won't be consensus at the time to act so we need to be able to build a
6664520	6671320	coalition of the willing uh andrea what's your views on these questions yeah maybe answering the
6671880	6677880	last one first on isn't a summit about frontier ai well much like with gold posts it feels a bit
6677880	6684120	like terminology is being shifted all the time sometimes quite willingly by the companies
6684120	6689880	building this uh you know in in the in the old days people used to talk about superintelligence or
6690600	6697480	friendly ai or strong ai then became agi then recently the frontier term was a kind of open
6697480	6703880	ai entropic rebrand of oh no like we're not well we're gonna get to very powerful ai system soon
6703880	6709320	but it's frontier which sounds better than agi because people are getting concerned about agi
6709320	6715560	you know in practice do these terms matter not too much what matters is how competent systems are
6716360	6723800	basically all of these companies expect to build systems that outperform humans at most tasks
6723800	6728440	definitely most tasks you can do behind a computer in the next two to five years
6730120	6736360	there's these matches the trends that we see in performance and compute growth this
6737560	6743080	is very worrying uh these these are these are levels of competence uh at which we expect the
6743080	6749400	systems to be out of our control unless we have various solutions so we just need to deal with that
6749400	6757560	we can call it frontier ai agi proto agi proto superintelligence it's just terminology what
6757560	6762440	matters is how powerful they are and how ready we are to deal with them we must avoid the serious
6762440	6768200	discussions getting sidelined into semantics which is often very frustrating we're gonna take three
6768200	6771880	more questions we're gonna go around the panel in the reverse order next time i'm gonna take
6771880	6778840	questions from people who haven't answered asked before so somebody in white about halfway down
6779560	6784280	if you have asked a question before please don't put your hand up just now so we have more chance
6784280	6792520	just there yes thank you three questions one from each thanks um let's say that in 20 years
6792520	6797960	we somehow managed to get it right and humanity still exists um despite the development of these
6797960	6803480	adi what do you think is one essential piece of regulation or development that has to have
6803480	6811320	happened together it's a great question uh where else with the hands uh where are the microphones
6811320	6821320	there's one about just on the other side thank you so you've spoken quite a lot about like what
6821320	6825960	government should do what companies should do uh i'm interested in like what should ordinary people
6825960	6830840	do like what can we be doing to get our voices heard in this you know should we be protesting
6830840	6834760	i've edited international protest on the 21st is this thing we should be doing or is this a terrible
6834760	6840200	idea it's another fine question is there a question from a woman it's about time we heard from the
6840840	6849160	other gender another gender hands up yes somebody put your hand up to whoever it was
6849160	6856680	yeah there's one okay there's one there and we'll take you as well we'll take four right
6857880	6864040	um as agi has been developed to be more human do you think it's possible to have forms of agi
6864040	6869080	that don't have the inherent geopolitical biases that come with the data sets that we currently have
6869080	6875160	and how do you think we go about developing regulations that aren't formed by human conscious bias
6875560	6884040	okay and so if we can get the mic over here as well too one two three four five rows back
6884040	6894680	just at the edge yes over there yeah i miscounted perhaps yeah four sorry quick question um so i
6894680	6900040	actually work in the automotive industry and we have to certify vehicles and engines and it is
6900040	6906680	an uphill battle um you can spend years just trying to get a windshield wiper right um or a
6906680	6913800	temperature sensor right and i'm just curious um if you think that there would be an ability to
6913800	6920680	take people who have regulated and certified products around global markets and how difficult
6920680	6926600	that is and create a summit where that expertise could come together from different industries
6926600	6932440	and we could roll up our sleeves and say okay this is how the structures go and we know what
6932440	6938280	works we know what goes slow and try to accelerate that learning because i think that voice we have
6938280	6943960	so much experience in the world right now um with that sleeves rolled up we know what it's like to
6943960	6949720	sit in those test labs or send 30 000 pages of documents in with verification and validation
6949720	6954120	data we know how to do requirements engineering requirements design requirements and i'm just
6954120	6959800	wondering if um there's been any discussion of that to pull you know pull a summit together from
6959800	6966440	people from heavily regulated industries four great questions first of all 20 years later it
6966440	6971640	succeeded how did we get it right what were the regulations that made the difference what should
6971640	6979240	ordinary people be doing can we design AI that is free from some of the human biases the geopolitical
6979240	6985560	biases that cause strife among humans and can we learn from the people who are professionally
6985560	6991400	involved in doing regulations and certification in multiple industry rather than just to being
6991400	6998040	naive in our own applications so Andrea first yeah maybe i will answer the question about can we
6998040	7005240	learn about highly regulated industries definitely i think there is a big kind of problem of uh
7005240	7011640	arrogance in AI or like willful arrogance of just thinking that this sector should be special and
7012520	7017880	people should be absolutely free to do any experiments they want all the time uh use you
7017880	7022280	know as much computing power as they want try the worst possible applications all the time
7022280	7026360	fully open source on the internet and nobody can complain like very often people in the
7026360	7031160	eye sector get very very angry when somebody tells them look well maybe what you just did
7031160	7037720	should be regulated in other industries we don't do it like that like with drugs we don't just let
7040120	7046040	pharmaceutical companies just release and test the drugs on billions of people and have their
7046040	7050680	CEOs say oh there's a 20 percent chance you will die if you take this drug but you know it's okay
7050680	7055960	like if it happens you can let us know and then we'll we'll stop maybe right so we can totally
7055960	7062440	learn from that we'll be great to learn from that um there is one challenge which is that we don't
7062440	7069480	understand current systems that well so it makes things like auditing them and evaluating them
7069480	7074840	quite tricky because we simply don't know how they work internally as well but we can do many other
7074840	7079800	things and we can definitely learn from highly regulated industries and definitely given the
7080520	7084760	risks admitted by the companies themselves other frontier the approach should be
7086040	7092360	highly regulated industry not so it is different but not completely different and we can indeed learn
7093000	7099720	tom closing words from you um well i'll take the um the question about what ordinary people
7099720	7105080	should do um and have two immediate thoughts one is that it's very important to keep this issue
7105640	7113720	apolitical um the other is that lawmakers need a sense of legitimacy i think um in order to
7114680	7120760	come up with regulation um and to bring it in through acts and bills and so on um a good example
7120760	7126520	of um when this happened a bit too slowly was it the outset of covid um and when it was a fringe
7126520	7131800	issue there were no enough rules then the public got involved um and suddenly the rules arrived a
7131800	7138600	little too late but they did arrive um and how can ordinary people achieve this um i think ordinary
7138600	7143960	people i i don't have a theory of of protest so i won't comment on that um but i think it's
7143960	7149000	important um that we all keep this in the public conversation i suppose what my answer is really
7149000	7155640	tending towards is you should all read lots of journalism about ai click on my articles thanks
7155640	7159880	alessandra any of these questions catch your attention i think i think um your question
7159880	7164280	in the orange sweater there is is um is is definitely where we're headed i mean there's
7164280	7173320	got to be some kind of um system that resembles either the car industry or fda and um the way that
7173320	7181480	we certify um our you know products generally speaking and i i just don't know how we get from
7182040	7189480	from that to something that is very difficult to trace and to to monitor as as ai but i would say
7189480	7193560	to the gentleman's question in the white shirt there if we're looking 20 years down the road
7194120	7202840	and we say that's really great in the uk november 2023 we we were able to put in place regulation
7202840	7209320	that somehow created traceability um so we could we could work out sort of where the where they were
7209320	7214280	where systems were running out of control or landing in the hands of bad actors that would
7214280	7219960	be a huge success i think that the reality is a bit different and that is that it probably is
7219960	7226120	going to resemble a bit more the world in which cyber security um flourishes and that means you're
7226120	7234040	constantly trying to create um a dam system or a a um deflection of all sort of incoming um
7234840	7240600	activities that are not great so i know that none of these are perfect analogies but i think it is
7240600	7248840	and in that universe we're probably going to be operating in for a while thanks final words either
7248840	7254360	sure so i would love to touch on two questions very briefly one of them um being i think your
7254360	7260200	question the white shirt what uh policies will bring us to a safe world in 20 years and i think um
7260200	7264760	a policy that was mentioned today as well already but that i want to touch on again is um strict
7264760	7270520	liability regimes just simply to kind of shift the incentive systems um incentive structures that
7270600	7276360	drive private companies to take certain actions that are not in the interest of um the wider
7276360	7283720	general public so i think there we can um really um shift shift the incentive structure to move
7283720	7288760	companies to take um maybe different paths forward and then what can the the average person the
7288760	7293960	general public do i would completely agree with tom i think um one thing that that really would help
7293960	7300280	us to for lack of a better expression to just make noise just make sure that this topic is um
7300280	7304760	talked about publicly you can do this in different ways you can write to your local newspaper you
7304760	7311080	can make a protest if that's up your alley you can write to your mp or your congressman or
7311080	7315880	wherever you live um and again create that legitimacy for people to actually act on the
7315880	7321320	problem because to many people it does sound very much like sci-fi and policy makers i'm not going
7321320	7325960	to take action and newspapers are not going to continue to report about an issue if you're like
7325960	7331000	it doesn't have traction and isn't taken seriously by the general public the other thing the general
7331000	7336520	public can do is we can educate ourselves and then we can share what information we have found
7336520	7341880	to be most persuasive ourselves because there's a wide variety of books a wide variety of youtube
7341880	7347800	channels a wide variety of blogs and some of them are better than others so let's share what we have
7347800	7353800	found to be the really best ones auto before i pass to you and maybe i'll ask you to get ready to
7353800	7359000	come up on the stage because you're going to give some closing remarks but jan what's your answers to
7359000	7364840	what you've heard uh so yeah just to just kind of underline the what ordinary people could do
7366120	7371080	is just kind of keep this topic alive like one of the things that i'm very proud of uh that came out
7371080	7378520	of the future of life six months post letter uh was uh framed by uh european commissioner Margaret
7378520	7385160	best agar when she said that like one thing that this letter has done is to kind of like communicate
7385160	7391400	to the regulators that these concerns are much more widespread among people than among regulators
7391400	7396360	so i think this uh potential difference should be continually kind of maintained
7397560	7403880	so and when it comes to uh kind of bringing in uh kind of expertise from people from like
7403880	7410600	regulated industries i think it's super valuable i was on the on the board or like on the european
7410600	7415160	high-level expert group at the european commission and there was like every once in a while there was
7415160	7419080	like why are we inventing the wheel like that we already have like lots of regulations should we
7419080	7425640	just apply this and i was like yes however there's like one big problem uh the problem is p in chat gpt
7426680	7433480	gpt stands for generative pre-trained transformer the pre-training is something that you do before
7433960	7444440	you actually train so the current the nasty secret of ai field is the ai's are not built
7444440	7450760	they are grown the way you you build the frontier model build the frontier model is you take like
7450760	7458040	two pages of code you put them in tens of thousands of graphics cards and let them hum for months
7458600	7464440	and then you're gonna open up the hood and see like what creature brings out and what you can
7464440	7471800	can do with this creature so it's i think the regulate the industry the capacity to regulate
7471800	7479400	things uh and kind of deal with various liability constraints etc they apply to what happens after
7479400	7485960	what's once this creature has been kind of tamed and that's what what uh fine tuning and
7486120	7491960	uh reinforcement learning from human feedback etc is doing and then productized then how do you
7491960	7497480	deal with with these issues but uh is this where we need the competence of of like other other
7497480	7503640	industries but like how can avoid the system not escaping during training run this is this is
7503640	7508840	like a completely novel issue for this species and we need to need to need some other approaches
7508840	7514920	like just banning those training runs that's great we'll thank the panel in a minute i'll ask
7514920	7522360	the panel to stay here because who's going to wind up the evening is auto bartend auto is the
7522360	7530440	executive director of the ero the existential risks observatory which along with conjecture has
7530440	7537160	designed and organized and sponsored this whole evening auto has got a few closing remarks before
7537160	7542200	those of us are still here can have a quick drink and continue the discussion informally
7542200	7545640	up to 10 o'clock by which time we must be out of the building auto
7559240	7565800	all right uh thanks david um a few closing remarks before we go to the drinks which is
7565800	7572520	five minutes so you should be able to keep with me um so we're talking tonight about human extinction
7572520	7577640	because of ai and what to do about this um and i think what to do about this there was also a
7577640	7582120	great question from the audience what can we do about this this is exactly the question that i
7582120	7587400	asked myself a few years ago um but it's not trivial and it's it's pretty difficult actually
7587400	7593960	what is net positive what could you do develop ai yourself try to do it safely such as open ai
7593960	7601000	deep mines and entropic are doing will this increase safety some say so work on ai alignment
7601000	7605960	for example interpretability where we've seen great breakthroughs actually last week it could be a
7605960	7610760	good option but increasing knowledge of how ai works could also speed up its development so this
7610760	7617800	brings risks as well one could campaign for regulations such as an ai pause we support this
7617800	7623480	but this also has its downsides so i think it's pretty difficult to tell what one should do to
7623480	7629240	reduce human extinction risk by ai but when i started reading into this i was only really
7629240	7634360	convinced about one thing and that is that you cannot put humanity at risk without telling us
7635080	7640280	so you cannot have a dozen tech executives embarking on the singularity without informing
7640280	7646440	anyone else and you cannot have a hundred people at a summit which is what's happening now decide
7646440	7651720	what should be built and what should not be built and i think you cannot let a tiny amount of people
7651720	7658600	also decide how high extinction risk should be for the rest of us so the only thing that i am
7658600	7664520	really convinced of is that we should be informed about this topic and that's also why i'm so happy
7664520	7671960	that events such as this one are taking place we're happy i'm happy that we're together not just
7671960	7677080	with in crowd people some of you are and it's great but also with some people who may have never heard
7677160	7682440	of existential risk before and also with journalists who can inform a much wider audience about
7682440	7687880	existential risk also with a member of parliament someone with a job to openly discuss difficult
7687880	7693480	problems so i think this is all very encouraging and it's helping to normalize an open debate about
7693480	7699880	the topic of human extinction by artificial intelligence the 31st of october at two o'clock
7699880	7705240	we'll have our next event with professor steward russell it's just outside the ai safety summit
7705240	7709400	in bletchley park in the old assembly hall where the code breakers used to have their
7709400	7715080	festivities after their important work so our event at bletchley park the day before the summit
7715080	7720680	may not resemble a festivity but in a sense i think it is because we're celebrating that we're all
7720680	7726280	being hurt here we're celebrating that we can all be part of a democratic conversation about what
7726280	7731480	the most important technology of the century should and should not be able to do we can talk
7731480	7736360	about risks to humanity we find acceptable and what we intend to do about risks that are too high
7737720	7742360	and as the existential risk observatory together with conjecture we invite everyone to be part
7742360	7748280	of this conversation so there's much to be unsure of in this field but if there's one thing that i
7748280	7753400	am sure of it's that the most important conversation in this century which i think this is has to be
7753400	7760760	a democratic one so with that i would like to invite you to scan the qr code on the left
7761560	7768360	if this is working right yes to join us in bletchley this is containing the url where you
7768360	7773560	can enroll to the bletchley park event if you're interested then definitely pass by
7775800	7781240	there's same qr code is also on the flyer on your chair and beyond bletchley i think this
7781240	7786600	conversation will not stop so there will be more summits according to my timeline about
7787800	7796120	maybe 18 roughly um so we will organize more events probably publish more about ai do more
7796120	7801720	research and inform governments as well as we can if you want to follow us or support us the
7801720	7806440	existential risk observatory in that work then scan the qr code on the right there's much that you
7806440	7812760	can do to to help us um and with that i would like to close this evening and once again thanks to
7812760	7818440	all the great speakers so that's uh romeo polsky cornelly sir robert buckland yantalin andrea
7818440	7830440	milti alexandra mosefisa day ava birans and some are give them a round of applause
7837400	7843160	and i would also very much like to thank david wood uh should see them conor xio dis
7843240	7853880	even diluman and everyone at conway hall will also make this evening possible thank you very much
7856360	7860600	and then i would like to hopefully see you in bletchley and in any case you are the drink
7860600	7865880	right now thank you thanks everybody
7873160	7873880	you
7903160	7905160	You
