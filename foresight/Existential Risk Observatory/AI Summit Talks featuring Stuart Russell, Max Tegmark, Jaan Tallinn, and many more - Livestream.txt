and to me it's not clear right are we at the right brother stage or are we at
the Mongolfi a stage where we have a lot of hot air and so my current view is no
we have not succeeded and the models that people are excited about the large
language models and their extensions into multimodal models that take in video
and can actually operate robots and so on that these are a piece of the puzzle
and and CNN made this lovely animated gif here to to illustrate this idea that
we don't really know what shape the piece of the puzzle is and we don't know
what other pieces are needed and how it fits together to make general purpose
intelligence we may discover what's going on inside the large language models
we may figure out what source of power they're drawing on to to create the
kinds of surprisingly capable behaviors that they do exhibit but at the moment
that remains a mystery and there are some gaps right one of the achievements of
modern AI that people were most proud of and also most certain of was the defeat
of human go champions by alpha go and then alpha zero in the 2016 to 2018
period so in go for those of you don't know there's a board you put pieces on
and your goal is to surround territory and to surround your opponents pieces
and capture them and since AI systems beat the world champion in 2017 they've
gone on to leave human race in the dust so the highest rank program is catago
and its rating is about five thousand two hundred compared to the human world
champion at three thousand eight hundred and the human world champion leaves our
colleague kelin pelrin who's a grad student a decent amateur go player his
ratings about 2300 and now I'll show you a game between kelin and catago where
kelin actually gives catago a nine stone handicap so catago is black and
starts with nine stones on the board right if you're an adult go player and
you're teaching a five-year-old how to play go you give them a nine stone
handicap so that at least they can stay in the game for a few minutes right so
here we are treating treating catago as if it's a baby okay despite the fact that
it's massively superhuman and and here's the game so it's speeded up a little bit
but watch what happens in the bottom right corner so white the human being is
going to start building a little group of stones there they go and then black
very quickly surrounds that group to make sure that it can't grow and also to
actually have a pretty good chance of capturing that group but now white starts
to surround the black stones and interestingly black doesn't seem to pay
any attention to this it doesn't understand that the black stones are in
danger of being captured which is a very basic thing right you have to
understand when your opponent is going to capture your pieces and black just
pays no attention and loses all of those pieces and that's the end of the game
so something weird happens there right where an ordinary human amateur go
player can beat a go program that stratospherically better than any human
being has ever been in history right and in fact the go programs do not
correctly understand what it means for a group of stones to be alive or dead
which is the most basic concept in the game of go they have only a limited
fragmentary approximation to the definition of life and death and that's
actually a symptom of one of the weaknesses of training circuits to learn
these concepts circuits are a terrible representation for concepts such as
life and death which can be written down in python in a couple of lines can be
written in logic in a couple of lines but in circuit form you can't actually
write a correct definition of life and death at all you can only write finite
approximations to it and the systems are not learning a very good approximation
and so they are very vulnerable and this turns out to be applicable not just a
category but to all the other leading go programs which are trained by completely
different teams on completely different data using different training regimes
but they all fail against this very simple strategy so this suggests that
actually the systems that we have been building are we are overrating them in a
real sense and I think that's important to understand and human beings right
another way to make this argument is look at things that humans can do for
example we can build the large interferometric gravitational observatory
so these are black holes colliding on the other side of the universe this is
the the LIGO detector and which is several kilometers long it's full of
physics and is able to detect distortions of space down to 18 the 18th
decimal place and was able to actually measure exactly what the physicists
predicted would be the shape of the waveform arriving from the collision of
two black holes and was even able to measure the masses of the black holes
on the other side of the universe when they collided so could chat gpt do
this could any deep learning system do this given that there are exactly zero
training examples of a gravitational wave detector I think at the moment there
is still a long way to go on the other hand people are extremely ingenious and
people are working on hybrids of large language models with reasoning and
planning engines that could start to exhibit these capabilities quite soon
so people I respect a great deal think we might only have five years until this
happens almost everyone has now gone from 30 to 50 years which was the
estimate a decade ago to five to 20 years which is the estimate right now so
unlike fusion this is getting closer and closer and closer rather than further
and further into the future so we have to ask what happens if we actually
succeed in creating general purpose AI and the reason we are trying to do it
is because it could be so transformative to human civilization very crudely our
civilization results from our intelligence if we have access to a lot
more we could have a lot better civilization one thing we could do is
simply deliver what we already know how to deliver which is a nice middle class
standard living if you want to think of it that way we could deliver that to
everyone on earth at almost no cost and that would be about a tenfold increase
in GDP and the net present value of that is 13.5 quadrillion dollars so that's a
lower bound on the cash value of creating general purpose AI so if you want
to understand why we're investing hundreds of billions of pounds in it
it's because the value is millions of times larger than that and so that creates
a magnet in the future that is pulling us forward inexorably my friend Jan
Tallinn here likes to call this mollock right this sort of ineluctable force
that draws people towards something even though they know that it could be their
own destruction and we could actually have an even better civilization right
we could have we could one day have a clicker that works we could have health
care that's a lot better than we do now we could have education that could be
brought to every child on earth that would exceed what we can get from even a
professional human tutor this I think is the thing that is most feasible for us
to do that would benefit the world in this decade and I think this is entirely
possible health care is actually a lot more difficult for all kinds of reasons
but education is a digital good that can be delivered successfully and we could
also have much better progress in science and so on so on the other hand AI
amplifies a lot of difficult issues that policymakers have been facing for quite
a while so one is its ability to magnify the pollution of our information
ecosystem with disinformation what some people call truth decay and this is
happening at speed but if we thought about it really hard I could actually
help in the other direction it could help clean up the information ecosystem
it could be used as a detector of misinformation as something that
assembled consensus truth and made it available to people we're not using it
in that way but we could ditto with democracy is it being suppressed by
surveillance and control mechanisms or could we use AI systems to strengthen it
to allow people to deliberate cooperate and reach consensus on what to do could
it be that individuals are empowered or the current trajectory that we're on
individuals being enfeebled as we gradually take over more and more of the
functions of civilization and and humans lose the ability to even run their own
civilization as individuals right these are important questions that we have to
address while we're considering all of the safety issues that I'll be getting
to soon there's inequality right now we're on the path of magnifying it with
AI but it doesn't have to be that way and so on so let me I won't go through all
of these issues because they're they're all each of them worthy of an entire
talk in themselves so the I would say the sort of the mid term question is what
are humans going to be doing right if we have general purpose AI that can do all
the tasks or nearly all the tasks that human beings get paid for right now what
will humans do and this is not a new issue Aristotle talked about it in 350
BC Keynes since we're in Milton it's odd that we pronounce it milk Milton Keynes
but he his name is pronounced Keynes even though the town is named after him so so
Keynes in 1930 said thus for the first time since his creation man will be
faced with Israel his permanent problem how to use his freedom from pressing
economic cares which science will have one for him to live wisely and agreeably
and well so this is a really important problem and again this is one that
policymakers are misunderstanding I would say that the default answer in most
governments around the world is will retrain everyone to be a data scientist
as if somehow the world needs three and a half four billion data scientists I
think that's probably not the answer but this is again you know the default path
is one of enfeeblement which is illustrated really well by by Wally so
my my answer to this question is that in the future if we are successful in
building AI that is safe that does a lot of the tasks that we want done for us
most human beings are going to be in these interpersonal roles and for those
roles to be effective they have to be based on understanding right why is a
surgeon effective at fixing a broken leg because we have done centuries of
research in medicine and surgery to make that a very effective and in some
countries very highly paid and very prestigious but most into personal roles
for example think about childcare or elder care not highly paid not highly
prestigious because they are based on no science whatsoever despite the fact
that our children are our most precious possessions as people politicians like
to say a lot in fact we don't understand how to look after and we don't
understand how to make people's lives better so this is a a very different
direction for science much more focused on the human than on the physical
world okay so now let me move on if I can get the next light up to to Alan
Shuring's view of all this what happens if we succeed he said that it seems
parable the ones machine thinking method has started it would not take long to
outstrip our feeble powers at some stage therefore we should have to expect the
machines to take control so he said this in 1951 and to a first approximation for
the next 70 odd years we paid very little attention to what his warning was
and I used to illustrate this with the following imaginary email conversation
so an alien civilization sends email to the human race humanity at un.org be
warned we shall arrive in 30 to 50 years that was what most AI people thought back
then now we would say maybe 10 to 20 years and humanity replies humanity is
currently out of the office who will respond to your message when we return
and then there should be a smiley face there it is okay so that's now changed
unfortunately that slide wasn't supposed to come up like that let me see if we can
oh well can't fix it now so I think early on this year three things happened in
very quick succession so GPT-4 was released and then Microsoft which had been working
with GPT-4 for several months at that point published a paper saying that GPT-4
exhibited sparks of artificial general intelligence exactly what Turing warned us about
and then FLI released the open letter asking for a pause on giant AI experiments
and I think at that point very clearly humanity returned to the office and they saw the emails
from the aliens and the reaction since then I think has been somewhat similar to what would
happen if we really did get an email from the aliens there have been a global calls for action
the very next day UNESCO responded directly to the open letter asking all its member governments
which is all the countries on earth to immediately implement the AI principles in legislation
in particular principles that talk about robustness safety predictability and so on
and then you know there's China's AI regulations the US got into the act very quickly the White
House called emergency meeting of AI CEOs open AI calling for governments to regulate AI and so on
and I ran out of room on the slide on June 7th with Rishi Sunak announcing the global summit on
AI safety which is happening tomorrow so lots of other stuff has happened since then
but it's really I would say to to the credit of governments around the world how quickly
they have changed their position on this for the most part governments were saying you know
regulation stifles innovation you know if someone did mention risk it was either dismissed
or or viewed as something that was easily taken care of by the market by liability and so on
so I would say that the the the view the understanding has changed dramatically and that could not have
happened without the fact that politicians started to use chat GBT and they saw it for themselves
and I think that changed people's minds
so the question we have to face then is this one right how do we retain power over entities more
powerful than ourselves forever right and I think this is the question that Turing asked himself
and gave that answer we would have to expect them to take control so in other words
this question doesn't have an answer but I think there's another version of the question
which works somewhat more to our advantage
it should appear any second if right and it has to do with how we define what we're trying to do
what is the system that we're building what problem is it solving and we want a problem
such that we we set up an AI system to solve that problem so the standard model that I gave you
earlier was systems whose actions can be expected to achieve their objectives and that's exactly
where things go wrong that systems are pursuing objectives that are not aligned with what humans
want the future to be like and then you're setting up a chess match between humanity
and a machine that's pursuing a misaligned objective so instead we want to figure out
a problem whose solution is such that we're happy for AI systems to instantiate that solution
okay and it's not imitating human behavior which is what we're training LLMs to do that's actually
the fundamental and basic error and that's essentially why we can't make LLMs safe
because we have trained them to not be safe and trying to put
trying to put sticking plasters on all all the problems after the fact is never going to work
so instead I think we have to build systems that are provably beneficial to humans and
the way I'm thinking about that currently is that the system should act in the best interests of
humans but be explicitly uncertain about what those best interests are and this this I'm just
telling you in English and it can be written in a formal framework called an assistance game
so what we do is we build assistance game solvers we don't build objective maximizers
which is what we have been doing up to now we build assistance game solvers this is a different
kind of AI system when we've only been able to build very simple ones so far so we have a long
way to go but when you build those systems and look at the solutions they exhibit the properties
that we want from AI systems they will defer to human beings and in the extreme case they will
allow themselves to be switched off in fact they want to be switched off if we want to switch them
off because they want to avoid doing whatever it is that is making us upset they don't know what it is
because they're uncertain about our preferences but they want to avoid upsetting us and so they
are happy to be switched off in fact this is a mathematical theorem they have a positive
incentive to allow themselves to be switched off and that incentive is connected directly
to number two right the uncertainty about human preferences
so there's a long way to go as I said and we're not ready to say okay everyone in all these
companies stop doing what you're doing and start building these things instead right that probably
is not going to go down too well because we don't really know how to build these things at scale
and to deliver economic value but in the long run this is the right way to build AI systems
so in between what should we do and this is a lot about what's going to be discussed tomorrow
and there's a lot so this is in a small font I apologize to those of you at the back there's
a lot to put on this slide we need first of all a cooperation on AI safety research it's got to
stop being a cottage industry with a few little academic centers here and there it's also got to
stop being what a cynic might describe as a kind of whitewashing operation in companies
where they try to avoid the worst public relations disasters like you know the language model used
a bad word or something like that but in fact those efforts have not yielded any real safety
whatsoever so there's a great deal of research to do on alignment which is what I just described
on containment how do you get systems that are restricted in their capabilities that are not
directly connected to email and bank accounts and credit cards and social media and all those
things and if there are I think there are probably ways of building restricted capability systems
that are provably safe because they are restricted to only operate
uh provably sound reasoning engines for example um but the the bigger point is stop thinking about
making AI safe start thinking about making safe AI right these are just two different mindsets
the making AI safe says we build the AI and then we have a safety team whose job it is
to stop it from behaving badly that hasn't worked and it's never going to work we have
got to have AI systems that are safe by design and without that we are lost
we also need I think some international regulatory level to coordinate the regulations
that are going to be in place across the various national regimes so we have to start
probably with national regulation but and we can coordinate very easily for example we could
start coordinating tomorrow to agree on what would be a baseline for regulation I put a couple of
other things there that went by too quickly so I actually want to go back okay too far all right
um so the the light blue line transparent explainable analytical substrate is really
important uh the moment we're building AI systems that are black boxes we have no idea how they work
we have no idea what they're going to do and we have no idea how to get them to behave themselves
properly uh so my guess is that if we define regulations appropriately so that companies
have to build AI systems that they understand and predict and control successfully those
AI systems are going to be based on a very different technology not giant black box circuits
that are trained on vast quantities of data but actually well understood component based
systems that build on centuries of research in logic and probability where we can actually
prove that these systems are going to behave in certain ways the second thing the dark blue
secure PCC based digital ecosystem what is that uh so PCC is proof carrying code
and what we need here is a way of preventing bad actors from deploying unsafe systems so it's one
thing to say here's how you build safe systems and everyone has to do that it's another thing to
say how do you stop people from deploying unsafe systems who don't want safe AI systems they want
whatever they want this is probably even more difficult policing software is I think impossible
is I think impossible so the the place where we do have control is at the hardware level because
hardware uh first of all to build your own hardware costs about a hundred billion dollars
and tens of thousands of highly trained engineers so it provides a control point that's very difficult
for bad actors to get around and what the hardware should do is basically check the proof
of a software object before it's run
and check that in fact this is a safe piece of software to run
and proof carrying code is a technology that allows hardware to check proofs very efficiently
but of course the owners then is on the developer to provide a proof that their system is in fact
safe and so that's a prerequisite for this approach okay let me talk a little bit about
regulations so a number of acts already in in the in the works for example the european AI act
has a hard ban on the impersonation of human beings so you have a right to know if you're
interacting with a machine or a human this to me is the easiest the lowest hanging fruit
that every jurisdiction in the world could implement pretty much tomorrow if they so decided
and I believe that this is how legislators wake up those long unused muscles that have
lain dormant for decades while technology has just moved ahead unregulated so this is the place to
start but we also need some regulations on the design of AI systems specifically so
a provably operable kill switch is a really important and basic thing if your system is
misbehaving there has to be a way to turn it off and this has to apply not just to the system that
you made but if it's an open source system any copy of that system and that means that the kill
switch has got to be remotely operable and it's got to be non-removable
so that's a technological requirement on open source systems and in fact if you want to be in
the open source business you're going to have to figure this out you're actually going to subject
yourself to more regulatory controls than people who operate on closed source and that's exactly
as it should be imagine if we had open source enriched uranium right and the purveyor of enriched
uranium was responsible for all the enriched uranium that they pervade to anybody around the
world they're going to have a higher regulatory burden because that's a blinking stupid thing to
do right and so you would expect there to be a higher burden if you're going to do blinking stupid
things um and then red lines this is probably the most important thing so we don't know how to
define safety so i can't write a law saying your system has to be provably safe because it's very
hard to write the dividing line between safe and unsafe you know if you uh asimov's law you can't
harm human beings well what does harm mean that's very hard to define but we can scoop out
very specific forms of harm that are absolutely unacceptable so self-replication of computer
systems would absolutely be unacceptable that would be basically a harbinger of losing human
control if the system can copy itself onto other computers or break into other computer systems
absolutely systems should not be advising terrorists on building biological weapons
and so on so these red lines are things that any normal person would think well obviously
the software system should not be doing that and the developers are going to say oh well this is
really unfair because it's really hard to make our systems not do this
and their response is well tough right really you're spending hundreds of billions of pounds
on this system and you can't stop it from advising terrorists on building bioweapons
well then you shouldn't be in business at all right this is not hard and legislatures by uh by
implementing these red lines would put the onus on the developer to understand how their own systems
work and to be able to predict and control their behavior which is an absolute minimum we should
ask from any industry let alone one that could have such a massive impact and is hoping for
quadrillions of dollars in profits thank you thank you very much professor russell quick
question maybe before we move on to the next speaker there was some good news in there it is
that we have ideas on how to make safe ai but how long do you think we're going to need
how long is it going to take by default that we have these ideas worked out and how long
might it take if we had all the smart people in the world give up their current focus and instead
work on this uh i think these are really important questions because the um the political dynamic is
going to depend to some extent on how the ai safety community responds to this challenge
uh because if the ai safety community fails to make progress on any of this stuff the developers
can point and say look you know you guys are asking for stuff that isn't really possible
and we should be allowed to just do what we want um but if you look at the nuclear industry right
how does that work the regulator says to the nuclear plant operator show me that your plant
has a meantime to failure of 10 million years or more and the operator has to give them a full
analysis with fault trees and a probabilistic uh calculations and the regulator can push back and
say you know i don't agree with that independence assumption you know these components come from
the same manufacturer so not independent and come back with a better analysis and so on
uh at the moment there is nothing like that in the ai industry there is no logical connection
between any of the evidence that people are providing and the claim that the system is
actually going to be safe right that argument is just missing um now the nuclear industry
probably spends more than 90 percent of its r&d budget on safety
one way you can tell i i got the statistic from one of my nuclear engineering colleagues
that for the typical nuclear plant in the u.s for every kilogram of nuclear plant
there are seven kilograms of regulatory paperwork i kid you not
so that tells you something about how much of an emphasis that has been on safety in that industry
and also you know why is there to a first approximation no nuclear industry today
is because of Chernobyl and because of a failure in safety actually deliberately bypassing
safety measures that they knew were necessary in order to save money
we'll take one question from the audience provided it's a quick question
i see a hand over there let me dash down
hi uh thanks very much for your talks here um my name is Charlie i'm a senior at UCL
um one of the big reasons i think why there's so much regulation on nuclear power
is widespread public opinion and protests against nuclear power from within the environmental
movement so i wondered whether you uh thought if there's a similar role for public pressure or
protests uh for ai as well thanks uh i think that's a very important question
my sense is i i'm not really historian of the nuclear industry per se uh obviously nuclear
physicists thought about safety from the beginning uh in fact so Leo Leo Zillard was the one who
invented the basic idea of the nuclear chain reaction uh and he instantly thought about
a physical mechanism that could keep the reaction from going supercritical and becoming a bomb
right so he thought about this you know negative feedback control system
with moderators that would somehow keep the reaction subcritical
people in ai are not at that stage right or they just have their eyes on you know we can
generate energy and they're not even thinking you know is that energy going to be in the form
of a bomb or electricity right they haven't got to that stage yet so we are very much at the
preliminary stage i do worry that ai should not be politicized and at the moment there's
a precarious bipartisan agreement in the us and to some extent in europe i worry about that
breaking down in the uk uh i think it's really important that the political message be very
straightforward you can be on the side of humans or you can be on the side of our ai overlords
which do you want to be on um and so let's try to keep it a unified message around uh
developing technology in a way that's safe and beneficial for humans um so we can raise
but we shouldn't do it in a partisan way yes and and what i i i totally sympathize with the idea
that people have a right to be very upset that you know that multibillionaires are playing
you know playing poker with the future of the human race um it's entirely reasonable
but what i worry is exactly that uh certain types of of protest end up getting aligned
in a way that's unhealthy uh it sort of becomes anti technology and we can look back at what happened
with with uh gm uh organisms for example uh which which most scientists think didn't go the way
uh it should have and we we lost benefits uh without gaining any safety
watch to think about there thank you very much professor stewart russell
we may give you the microphone again a bit later on but there's lots of other people we want to
hear from now so the next speaker is kona lehi who is the ceo of conjecture many of us got a shock
with gp24 or 3.5 my goodness what's going on here kona was ahead of the curve when he saw gpg2
with all its warts and weaknesses he said my gosh this is going to change the world so he has been
thinking about some of these issues probably for longer than the rest of us so let's hear from kona
what would you like to say thank you so much so unfortunately uh professor russell has stole
my favorite allen turing quote so you're going to be hearing that one again but i guess there couldn't
be a more appropriate time because many years ago there lived a man named allen turing he was the
godfather of computer science a titan in his field and a hero of world war two and was here
at bletchley park that he did his most seminal work during the world war two and cracking the
codes that the germans were using and as a very early step into the field of computer science
and allen was ahead of his time in more way than one in 1951 in manchester he gave a lecture
entitled intelligent machinery a heretical theory and in this lecture he said for it seems probable
but once the machine thinking method had started it would not take long to outstrip
our feeble powers there would be no question of the machine's dying and it would be able to
converse with each other to sharpen their wits at some stage therefore we should have to expect
the machines to take control and here we are 72 years later where it all began
and a lot has changed since the days of allen turing computers have improved in incredible rates
i'm holding my hands right now a computer of such incredible power that it would be barely
imaginable to turing in his contemporaries barely one human lifetime hints and while computers have
advanced a lot in many ways since the days of turing i like to believe that there would be a lot
he would recognize he would recognize the basic functions of computers their memory their instructions
programming code ideas that go all the way back to his seminal work on turing machines
while he might not be familiar with the exact tooling he would be familiar with the general
concepts around modern programming where a programmer writes code instructions the computer
then executes but there is something that i'm not so sure he would so easily recognize
and that is a i or in particular the neural networks that power them now we have all seen
ai do truly amazing and things over the last couple of years in particular solving all these
problems that previously we barely knew how to approach and you might think when you look at
all these ai systems running on your phone on your computer that this is software like any other
written by very clever programmers to do the useful and the marvelous things that they do
and you would be wrong because ai is very different from normal software
it is not written so much as it is grown so while in the traditional software you'd have
a programmer sit down and write out the instruction with ai's you take huge supercomputers and massive
data sets and you use these supercomputers to grow a program on your data to solve your problem
and this works really well for many for many issues it has improved our ability to solve many
very useful tasks and do many things that we did not know how to do before and our ability to
grow these ai's continues to improve and get better and better while at the same time though
our ability to understand our ai's has not because these ai's are not like well written code
that a human could read they're more like giant blobs of numbers and we know if we execute them
they work but we have no idea why and only quite recently have we discovered that as we scale up
these systems and as we build bigger computers and bigger ai systems something quite remarkable
happens they become more intelligent more capable now of course there are many details
that have to be gotten right there are many parameters you have to set correctly you have
to have enough data you have to make sure your computer is set up correctly but fundamentally
is a stability in this prediction sometimes also called the scaling laws and that as our
systems become bigger as our computers become more powerful the systems learn higher and
higher order patterns more and more complex skills knowledge abilities and as they become more powerful
and more capable they are also becoming even harder to understand and to control
and this is why we are all here today back to where it all began we have now returned
Bletchley Park because as Turingell really realized it is really quite simple if we build
machines that are more competent than us at manipulation perception politics business science
and everything else and we do not control them then the future will belong to the machines
not to humans and the machines are unlikely to feel particularly sentimental about keeping us
around for very long and so here we are face with an exponentially increasing more powerful
by the day ai by the day as we learned with covid there are exactly two times one can react
to an exponential too early or too late if we wait until agi if we wait until we see the
self-improving powerful general purpose systems it will be too late far far too late and this is
why i am so happy to see the uk government take leadership in the first of many important steps
towards the necessary international coordination to address this extinction level threat that is
facing us all and the very first step as so many academics industry leaders and even governments
have already taken is to firmly acknowledge the reality of what we face the potential extinction
of our species by ai private ai companies are scaling their ai systems as we speak
and they will not ask for permission and they will not stop unless we make them they are already
lobbying our governments for with ineffective policies such as responsible scaling in attempt
to prevent actually effective policy like the oil ceo is a past trying to lobby against climate
change regulation that would hurt their bottom lines the good news is that is not yet too late
to stop this to prevent the building of such deadly machines until we know how to build them
safely that is why there is nothing more important than for people to know the truth
that a small group of unelected unaccountable private companies are running a deadly experiment
on you on your family and on everyone on earth without your consent or even knowledge despite
they themselves admitting that these risks are real at this point all of us agree that there is
that we are playing Russian roulette with the entire planet and we're only quibbling about
how many poles are left until the bullet now in my personal opinion if you ever find yourself
playing Russian roulette i suggest you put down the gun and so we have to speak up and demand
action if you want a future for us our children and our species it's not yet too late but it will
be soon we stand at a historic moment today at where it all began
let's leave park well let's not waste it
thank you corner let's take a couple of questions from the audience
where am i seeing there's one at yeah if you take the microphone in there thanks
hi very nice presentation by the way i wanted to ask
with the current situation that's going on with ai currently do you really think if you were to be
a philosopher maybe for five minutes do you really think that currently society is really ready for
it i mean sure we can adapt in somewhat but as things are uh dripping around us it doesn't seem
like we're really anywhere near to i mean accepting it we're just a such fair and all
and other factors coming yes so the simple answer is no we are absolutely ready
we should be playing with nuclear fire or worse our civilization does not have the level of maturity
to be able to handle technology like this and this is why i'm not extremely optimistic about the
future the truth is is that whether it's ai or something else ai technology is becoming more
and more powerful this is just how it is this is how the technology works and our society
has to adapt to this if we as a society do not find a way to as an entire civilization as an
international civilization work together in a way that we can responsibly steward technology so powerful
that it can destroy anything then humanity is on a timer whether it's ai or whatever comes after
that we need to improve our society or that's it sometimes people grow up in a hurry sometimes
people are a bit childish and suddenly there's a big threat ahead and my goodness we grow up
is that what you see happening with humanity now we're not ready for ai but as we understand the
risks we will change our mode of operation i sure hope so and if it happens it will not happen as
because something about it is because people like the people in this room actually do something
about it stand up and make a difference in our institutions and our society it is there is no
law of physics that forbids us from having a good future and taking control of our future
and building wonderful safe technology for all but there is also no law that mandates it
i saw one more hand up yes if you take give the mic to the the woman in the glasses thank you
oh and i was gonna say i know that one of your policies is that you want to cap compute and
and i'm just wondering whether you are gonna suggest that at the summit and what you think
the government's response to that will be compute caps are absolutely the most sensible direct
policy for us as a species to follow the main reason for this is is that it is the bottleneck
towards building the actually existential dangerous systems just explain what these view caps are
yes so compute is basically limiting the maximum size of the supercomputers i talked about the limit
the maximum size our ai is and our computers are allowed to be and so we can limit hypothetically
how intelligent they will be things actually get dangerous because we don't know what pops out of
our experiments until we run them so it might already be that we're always too late our computers
might already be big enough to end the world we don't know but hopefully not in that case the first
as i say with russian roulette if you pull the trigger once and there was no bullet the correct
move is not to pull it again the first thing you do is don't pull it again until you know if there's
a bullet and where it is and if you know there's one definitely don't pull it so this is my opinion
on this i will definitely be open to talking and would like to suggest this to all policy makers
of all nations and leave this to say i think there is extremely strong resistance to this
for the obvious reason that this cuts into the bottom lines of very powerful big tech companies
who have extreme lobbying power and control over governments this is well i mean it's very simple
there's a lot of people who gain a lot of benefits from continuing to pull that trigger and we have
to make them stop and they are going to fight us every step of the way it's just how it works
thank you very much Connolly
and i'd like to invite the eight members of the panel to come up on stage
and we're going to continue the conversation please self-organize on the seats
stewart i don't think we've got a seat for you at this stage we can either find another seat for
you or we'll let you come back on the stage later so sit down in whichever system you like
and we will hear from each of these panelists what they think has been missing from the
conversation so far maybe they've got an alternative view maybe they don't think we're playing
russian roulette and a different metaphors appropriate maybe they would like to express
what they think the politicians they're closest to should be saying maybe they'd like to comment on
some of the other issues of safety so shall we start at the far end there and let's just move
along the panel i'll give you two minutes each to contribute what you'd like into this conversation
and then we'll hear from the audience so and let me introduce you as well so
i'm sure i can do that my name is mark brackle i'm the director of policy at the future of life
institute um and truly support what stewart and corner have been saying i think when we
looked at the summit about six seven weeks ago we put out a set of recommendations ahead of time
i think there were three traps that we identified that we were worried the summit would fall into
the summit potentially not addressing the full range of risks all the way from
bias and discrimination up to extinction it not being inclusive namely china not being invited
and it being a setting where the big ceo's would sort of run the show and there would be maybe
some token academics uh at a panel in a room the night before so i think if we sort of assess
what the summit is looking like now the night before the actual event i think
sort of we can be reasonably happy i saw this morning that china will in fact be invited
and that it will be an inclusive summit in that nation of the world will get a seat at the table
so i think that's progress so that's very good if we think about the harms and the range of harms
that are being discussed one of the things fli recommended for the summit was grounding it in
examples of large-scale ai harm that we've already seen such as the australian robo debt scandal
or the dutch benefit scandal from the netherlands myself and it's it's a political scandal really
dominating the national scene to to show that very simple algorithms can already have a very large
impact in countries that were rushing towards adoption and to show the beginning of the trend
line and that hasn't happened i think that's potentially a missed opportunity but i think
it's really good where the uk has overall focused the summit and i think i'm sort of least optimistic
when it comes to the role of companies at the summit and i think corner's done a great job at
highlighting concerns that fli that many of us have around responsible scaling and this narrative
being pushed by many companies as an excuse to keep going rather than making sure that whatever
they put out onto the market is actually safe and i think that's a message that i hope we collectively
and the people in this room that are going to the summit can still take to the participants
and to the governments that are there making sure that we put the onus of what is safe and what isn't
safe on the companies and they need to prove to us that what they're putting on the market is safe
rather than the other way around where the default is they keep on scaling and it's up to the regulator
to prove that what is safe so that i think is a key message to take so you're giving at least two
cheers if not three cheers to the organisers for what you see happening already so from one
dutch man to another ron rusendal is the direct deputy director general of the netherlands ministry
for the interior lots of other roles what would you like to add to the conversation um well thank
you very much and i'm glad to be here um um first point is that we regulate cars and we
regulate pharmaceuticals um and we do so to mitigate risks of today and the risks of tomorrow
um so we have to act upon risks of today like bias and risks of tomorrow
um and those are global risks so we welcome the initiative of the summit but we also welcome
the initiative of the tech and foyer of the un starting a high-level advisory board um and we
have offered the tech and foyer to host a european um um meeting of the high-level advisory board
in the hake for example in the peace palace because we support the work that we all do
internationally to mitigate the risks um secondly we need some form of um early warning whatever
the risks are um and whatever whether they will occur or not we have to have early warning and
a rapid response mechanism uh or whatever goes uh will happen in the future and and therefore we need
to um operate in a failure-driven way and we need to participate we need to coordinate but
we also need to cooperate with industry with civil society with citizens with um uh industry and
with governments agreement on early warning seems like something that both sides of the debate should
be able to give because the people think things will go wrong and the people think things won't
go wrong should be able to agree well if this happens we should all be paying more attention
let's pass the microphone on to Hal Hodson who's a journalist at the Economist who has written
a lot about existential risks and AI Hal thanks David um yes so my name's Hal Hodson I'm a special
projects writer with the Economist I've been writing about AI for 10 years I have a degree in
astrophysics and that meant that I spent a lot of time looking at a thing called the archive
long before it was cool and uh papers from Facebook and Google would just turn up on the
archive with no PR whatsoever and this is journalistic gold and that's how I got into it um I
guess my sort of view is inherently going to be journalistic I think it is a very difficult point
to make very clear decisions about what anybody ought to do about any of this I think there's
from my perspective there's a huge amount of uncertainty I'm now I've now been writing about
it long enough to know that there's also a lot of hype and it's not the first time there's been a
huge amount of hype and I think making very clear decisions about important systems at a time that
is hype filled is a difficult thing to do I think the thing that I can agree on the consensus that
I can come to with probably most of the people in the room and the organizers of the summit
is that there's a huge amount of science to do and it both in terms of existential risks and these
sort of lower tier algorithmic risks I think there's two examples that show us that this is a perfectly
plausible thing to do the first is that there was a time in the 90s when everybody was very worried
about impact of bodies in the solar system to earth existential risks from asteroids and things
like this and congress mandated a large amount of money to go to NASA to map all of the asteroids in
the solar system and to figure out ways to nudge them off course if they come towards us and if
you look at the risks as they were assessed in the 90s and the risks as they are assessed today
they are massively massively dramatically lower and so that to me is a very strong case for doing
science on these risks I don't know and I'd be fascinated to talk to people who do know what
doing science on AI systems really looks like but it brings me to the next comparison which is
Facebook about five years ago there was also a big panic that Facebook was determining the results
of elections or you know hacking democracy essentially that is somewhat subsided now but
one of the most sensible responses to that concern that I saw was also that you need to do science
on Facebook just like you needed to do science on the solar system you need to start measuring
things and it actually took years to force Facebook to give access to data to people like
social science one it eventually sort of worked and I think there's a reason that you don't hear
a huge amount about it it's because the science that's been done so far has not determined that
Facebook destroyed democracy we still have at least a version of it and so I guess I would end
just by a plea to you know and perhaps in the same way you were saying Stuart politically neutral
science to the extent that that's possible a more of a goal than a thing that exists
but do you have no whether the US focus on asteroid risk in the 90s was that bipartisan
or was that a partisan issue I don't know if it was bipartisan but it went through congress so it
must have been a bit bipartisan so maybe it wasn't as bad back then actually now that I think of it
there's some encouraging examples there next we're going to hear from Anika Braak who's the CEO of
the International Center for Future Generations tell us about your views Anika yes thank you very
much and first of all I'd like to commend the UK on two things first of all their sense of humor
for setting up a summit on the darkest corners of AI on the night of Halloween or the nights after
and I'm surprised nobody made that joke so far and secondly for really bringing this to the
attention of leaders media and the public actually I don't think Frontier AI has ever been discussed
so much and the number of communicates the number of you know the the executive order the communications
leaders European leaders meeting ahead of the summit negotiating late night to get to bring
something here is already a measure of success and we could actually leave it here with this
stellar panel and and say I think it's very important that the the civil society is meeting
here this is maybe the element that is missing in the room I'd like to say we have two major
challenges here one is a coordination challenge we have corporates looking at the topic they
they're racing over competitive edge and we have governments who have serious geo-strategic
interest and when those two come together that doesn't help collaboration so we have to think
about how we get people around the table and secondly there's a democratic challenge democracy
is by its very nature a slow and patient regulator and that's important I will argue that actually
democracy is perfectly adapted to the society through these uncharted waters that we're experiencing
at the moment but we need to make sure that the sailors of this big ship are prepared that they
are well informed and that they have the tools to deal with this change and that's what the
international center for future generations set out to do in Brussels that's why we moved our
headquarters to Brussels to make sure that EU decision makers are well prepared because we
set our best hope in the EU in this international race for governance I will leave it here for now
are the EU decision makers paying attention to what you say I think they do you do here
already a lot of signs that they have also recognized that we have to look at advanced
artificial intelligence that regulation doesn't stop with the artificial intelligence act it's
only the very start of the beginning or the first piece of the puzzle to come back to Stuart's presentation
thanks next we're going to hear from Jan Tallinn who is the co-founder of Skype
fli Caesar that's the center for the study of existential risks and he is also one of the
advisors on the committee created by the tech and envoy for the UN so Jan what would you like
to say based on what you've heard so far yeah thank you very much sometimes people ask me
because I've been in this kind of existential risk and AI safety community and effort for more
than a decade now sometimes people ask like so how's it going and my standard answer is well it's
great progress against an unknown deadline and indeed it's kind of special this year it's just
like there's like a plethora of things to point to us as great progress and obviously the most
obvious one to point to at this point is the summit that starts tomorrow I do think it's
UK deserves a great credit for for pulling this together and I really wish
kind of best of luck to the organizers of this and the prime minister as well and the team
now when it comes to this like unknown deadline recently I've kind of pivoted away
to some degree from basically funding research towards just buying us more time which is kind
of has to be has to deal with something like less research side and more kind of an action side more
on the policy side so I do think it's kind of valuable now to really think through the policy
that would make the future a little bit less sound and the deadlines a little bit less
unknown another and final thing I wanted to say that there is I want to kind of caution against
if you're sailing to like uncharted waters there's like a temptation to
use something familiar and say that oh like the future is going to be just like this
like the most common one is that oh AI is just the technology it's just going to be
just another like electricity or something like that
when we're talking about risks the the way the model risks is by the you know reference class
that you cannot rule out so as long as there is like reference classes like viruses self-replicating
things or another species as long as you kind of rule out rule them out you have to like prepare
that this might be an instance of such thing so I think it's important to not make dismiss
AI it's always just another technology or like as one prominent VC recently said oh it's just
much of math thanks Jan next we're going to hear from Max Teckmark he might describe what he's got
on his chest I happen to know he has released a very interesting TED talk which I strongly
recommend all of you watch and Max might give an abbreviated form of that TED talk now or whatever
else you'd like to put in the conversation thank you thank you yeah so I'm Max Teckmark I've been
doing AI research at MIT as a professor there for many years focusing on safety related stuff I'm
also the president of the future of life institute and I'm a huge fan of this guy who you guys have
the wisdom to put on your 50 quid note Alan Turi who's come up many times and it's really remarkable
that the argument he made 72 years ago that when machines greatly outsmart us by default they're
going to take control that that argument has not been convincingly refuted in the 72 years
since he said it so I think we have to take it very seriously and people who think of AI is
just a new technology like steam engines or electricity tend to not take it so seriously
Alan Turi himself clearly thought about AI more as a new species and with that framing it's very
natural that we would lose control to them just like the Neanderthals lost control to us etc
so so what are we going to do about about this great threat first of all having conversations
like here and what happens tomorrow is great so a huge thank you to the British government for
really putting this on and for standing up to all the lobbying pressure from companies who wanted to
water it down into just talking into just a big blessing of responsible scaling or whatever
thanks also to the US government for standing up to also the weird pressures to turn this into a
geopolitical pissing contest by excluding China I'm really proud of the Brits for recognizing
that this is a global challenge and what do we actually do about it well I think there's a remarkable
consensus actually emerging from all the civil society and academic groups that don't directly
profit the way companies do about what we should do about this we put out maybe
Andrew creature Richard Muller can just hold up in the air with this thing you can if you go to
future you'll find the alternative to the responsible scaling policy called called the
safety standards policy where the idea is as we heard from Stuart Russell you should simply shift
the responsibility to companies to prove that things are safe instead of as responsible scaling
policy you you have the responsibility on the government regulators to prove that things are
unsafe more or less in order to stop them and there's there's a whole set of very concrete ideas
out there for what the safety standards should be to start with and some of them were mentioned
very eloquently by Stuart you can insist on quantitative safety bounds or provable safety
beginning with uncontroversial stuff that you should not be able to demonstrate that nobody
can hack the servers that these super large systems are on that they won't advise on how
to make bio weapons etc and this will very naturally accomplish something quite wonderful
where we sort of have the cake and eat it as a species because most people I talked to don't
realize that there are two almost there's two very different kinds of AI that they keep conflating
there is the AI that has current commercial value for curing cancer making self or better
safer cars and all sorts of wonderful things which have very little risk associated with them but
some which we need to address but 99 percent of the things that most people are excited about
do not require playing Russian roulette with AGI and super intelligence and then there is this
lunatic frame just try to build the machines that outsmart humans in all ways where almost all
the risk is coming for very little benefit so if we can put safety standards in place we can I think
quickly get into a situation where we have a long future with these wonderful benefits
that are quite safe to get from AI and then just take your time with with the really risky stuff
maybe one day humanity will or will not want to build more powerful machines but only when we can
figure out you know how to control them so that would end with just a bit of wisdom from ancient
Greece if I may so raise your hand if you remember the story of Icarus don't get hubris right you know
so artificial intelligence is giving humanity these incredible intellectual wings with which
we can accomplish things beyond their wildest dreams if we stop obsessively trying to fly into
the sun thank you so you're not saying pause AI you're saying let's keep using AI but you're
saying pause the rush to AGI let's not pause AI in fact let's continue almost everything that
people are excited about doing but pause this compulsive obsession about training ever more
ginormous models that we just don't understand thanks Andrea Miotti is the head of policy and
governance for AI at Conjecture are you in agreement with what you've heard or you have
different things to emphasize absolutely I'm very much in agreement with both the speakers and many
other members of the panel I think there are two big positives from the summit to highlight
one is that we're also echoed by the panel one is it's role in building common knowledge
making it clear and explicit at the highest levels of government that this is a big risk
that this is a extension level threat that we face as a species and number two coordination
not getting lost in a geopolitical pissing contest as Max has said or in other of these
things and realizing this is a again a threat we all face together it's a global security problem
it's not a national security problem or at least it's not only a national security problem and
to solve these problems we need coordination even during the heights of the Cold War there were
open lines between the US and the Union to deal closing the door on cooperation before it has
been tried is a surefire way for all of us to lose and so I was very very pleased to see that the
UK government the Prime Minister Rishi Sunak have already acknowledged the risks very explicitly
in the Prime Minister's speech last week are setting up the summit are inviting a diverse
group of countries to discuss this risk together the part where I think we can go
further and we can do better is in the measures I share the concern of some of the other panelists
on a focus of simply enabling the default to continue and the reality is that the default
is bad the default is bad we by now all understand it is bad and we all understand we need something
else even the companies racing towards its default admit that it's bad admit that it's a
one in four one in ten unacceptably high chance for all of us to be wiped out and so the concrete
measures they will need to take cannot look like continuing on the default path cannot look like
systems are safe until proven dangerous by external auditors that are strapped for resources and they
don't even have the the tools or the tests to do these tests they look like provably safe systems
they look like burden of proof on developers developing systems that they admit could wipe
everyone out to demonstrate ahead of time of running critical experiments that they are safe
if they cannot do that that's fine they can just build something else or they can move to a different
sector that's the standard we utilize in all high risk sectors there is no reason to not
utilize it in a sector where the risks are the literal extinction of your mighty thank you
and last but not least we have a trained economist alexandra musavi sadeh who is the
CEO of evident what would you like to add to the conversation alexandra it's a hard it's a it's a
great panel to follow it so it's um i think i have a different time horizon alexandra musavi sadeh
i'm the founder and CEO of evident and we actually do a lot of measurement uh we specialize in
benchmarking businesses on the option of ai so what i focus on is very near term so looking at
the here and now and the race is on at that level as well so the race is on by all businesses in
all sectors to take the capabilities that ai offers today and to implement it as fast as possible
and really not thinking about any of the risks so thinking about growing market share um
upping revenue cutting costs and all of that and continuing the um sort of digital transformation
which is now more and more an ai transformation and so with that um we we are observing this ai
race at a business level and one of the things that we see that some businesses that are highly
regulated really think about um how they can implement the oversight and implementation of
safe ai so while very impressed with what the uk government is doing and i think the right thing
is to focus on the long term because that is where we um should have our eyes at um at the stage but
there's also a near term risk and i think um if there was one thing i would suggest is that as
much as we need to focus on the long term we also need to look at the here and now and that businesses
are barreling ahead with ai adoption without any particular guard rails in that and so while we
need to put the um the burden on the development of safe ai we could also maybe in the meantime put
the burden on the businesses that are using ai to prove that they're doing it in a safe uh and
constructive way thanks so you've heard from all the panelists i'm sure there's lots and lots of
questions in your mind so i'm going to come to the audience and take maybe three or four questions
and then let the panelists pick what they want and my question to be would be do you agree with
this division between near term and far and far future some people say that the risks from
existential risk should not be considered to be far long term they are potentially here and now
but maybe you have a different way of framing it let's see some hands let's take uh one back over
there in the far corner it's a bit some running around if you can say who you are if you want
to remain anonymous that's fine too hi um i'm Matthew Kilcoin i looked into how the banking
industry turns short term into long term by sort of senior management risk and associated penalties
and clawbacks to force the change okay question on learning from the banking industry so let me
give the microphone down here just a second thanks a lot everyone yolanda lancas from the
future society what do we do about open source ai professor russell mentioned one idea which was
kill switches i think this is in terms of policy approaches such an important question
for us to all grapple and again um as yan was saying oh technology people assume that paradigms
continue open source has been valuable for software but with ai we're seeing new risks
and paradigms and how could maybe academia and others my name is oliver graves um my question is
what do you think the biggest hurdles are towards getting the general public to recognize this as an
existential risk and to take that risk seriously because it still seems to me like it's all well
and good everyone here at the summit and in this room being aware of that risk but it doesn't seem
to me like we're anywhere close to a level of majority of the general public grappling with it
probably
thank you so i'm i'm father pete vignaschi i'm i'm engaged in looking at how the catholic church
can respond to existential risks so it's a slightly different question here just in the most general
way what does it look like from your side of the table for religious groups to play their parts
in achieving existential security thank you i'm oliver chamberlain i'm a student studying
a master's in science in in ai uh one of my concerns is although like the regulation
is going to involve limiting like supply chains making sure that gpu's aren't going off to places
that we don't know about how do we stop um the advancement of algorithms which allows older
systems to be more powerful so like alpha tensor um i wonder uh in my mind like the only way around
something like this is a future which is like super draconian um how do we prevent gpu's that
already accessible already out there from being used in ways which are way more powerful
question on open source to what extent is it possible to control open source a question on
what are the biggest hurdles changing their minds in the public or indeed one of the other big hurdles
yeah question from the point of view of what might religious organizations contribute to
this conversation and do we need to have super draconian surveillance and policing systems if
we're going to stop these gpu's and algorithms they're potentially doing things that we didn't
want them to so max hand up first yeah religious organizations i hope can
remind us all of the importance of not play god and get hubris remember the moral angle
i'm only going to comment on the timeline one even though i have opinions about all the others
it's not so i don't talk too much the timeline one from alan turing's perspective when he said this
he said that when it eventually basically passed the turing test he expected to go very fast
so then it was a long-term risk now according to yosha bengio gpt4 passes a turing test so
he would probably if he were still with us in the room predict short timeline it's quite remarkable
what's happened on on the prediction market metaculous.com for those of you who are nerdy enough
to go there where the timeline how many years we have left artificial general intelligence
outsmarting us has plummeted from 20 years away to three years away just in the last 18 months
as a direct result of of this recent tech progress and dario amode has openly said one of the tech
ceo's here that he thinks you have two or three years left and others other tech ceos told me
that individually so i think we just have to stop calling this artificial general intelligence
risk long term or people are going to laugh at us and call us dinosaurs stuck in 2021
andrea i'd like to answer the question about the public actually the pub seems to really understand
i recently ran polling as part of a campaign i'm running called control ai and the british public
is extremely concerned about disempowerment and extinction risk from ai they seem to be
aware of it um a they seem to be aware of it um a whopping 60 percent a global ban on smart and
human ai period with only i believe 14 percent against and like quite a few undersized um
nearly i believe almost nearly 90 percent would be very very happy with a full ban on deepfakes
right now people understand very very well that full impersonation revenge pornography and like use
of their likeness against their will is not good is destabilizing is a threat that exists right
now with systems over here right now and they don't want that um and similarly uh there is
i believe 78 percent of the public it would want an international watchdog with real teeth more like
an i a a and there are basically across the board like i was personally surprised as he all of the
answers come up with such overwhelming uh support uh we might ask whether that mood is shallow that
it might be adjusted we might ask whether that mood is shallow that it might be adjusted again
in the future let's hear from anika and then from how and then from yana uh yeah i just wanted to
say that we should stay away from predictions with regard to timelines with regard to sectors
how they're going to be affected i think if we have learned one thing that it's really difficult
to assess that but we do know that there will be dislocations there will be impacts and as they
grow and as we see the more the public will be more informed and more aware i think it's really
our role here as civil society academia religious leaders to increase that awareness and to also
keep that conversation going um as we go when you drive a car you don't just look ahead right
you also look in the rear view uh mirror you look in the side mirrors for signals of change but
what i want to say when you look back that's something we are not doing enough we're trying
to predict in the future but we should also look back and look at you know stuff we've put out
there regulation we're putting out there and how it's actually being enforced um if it's effective
if um it's yeah effective regulation we discussed about it um before and this will be key going
forward how i'll take the question oh yeah on yeah i'll take the question on open source
and draconianism because i think they're related i don't really see a good way of regulating and
controlling open source code that is not deeply draconian that does not involve a massive expansion
of surveillance uh if you need to know what code is running on what chips you have to have access
to the computer in which those chips are running and for a sense of how well this is going to go
look at america's attempts to put export controls on chinese ai development um it it works to an
extent but it only works if you pick these very narrow bottlenecks and i guess in terms of existential
risk it depends on how powerful sort of lower tier open source models end up being i don't really have
a good answer to that question i just say one more thing on metaculous just as a little hint of
not relying on it too much if you just if you remember the lk 99 superconductivity thing over
the summer metaculous at one point was completely certain that that was real and uh the metaculous
thought it was real for a while and and then it dived again so just you can't rely on metaculous
we're gonna do some real time checking on this right so once that's going on yeah so i also
wanted to say a few words about open source i think it's as i mentioned earlier i think it's
important to just like not do this kind of categorical thinking that you have like some one particular
you know look at that too that you put things in and then like you reason about this pocket
my friend andrew gritch who is in the audience like he he observes that's like when people
say that they're really gonna prove open source it's valuable to try to understand what they
actually want what is what is the thing that they're trying to protect and quite often it's
just like they don't want this like massive centralization and power in the hands of people
that they don't trust now the question is like if you think about open source as like irreversible
deployment of things that we potentially don't want to irreversible deploy other other ways
to protect what the open source allocates want and for example there is like there's like in blockchain
community there is quite a lot of advancement in cryptographic techniques techniques like
zero noise proofs perhaps there are enough like ways how we can kind of eat our cake can keep it
keep it too by instead of having nosy people literally looking around in a computer you just
computer automatically producing things like zero noise proofs that you haven't been up to no good
things like that so there's lots of possibilities to explore there are this andra i'm actually
curious about what max and how we're talking about ai secrets did they know or did they not know
peak enthusiasm they thought it was 60 chance which i would say interpreted that they were
not saying for sure okay but prediction is very real time checking here prediction is hard canary
signals are more important in my view let's agree the canary signals okay if i can just add
something here we we ask if something is a near term or long term risk we have to remember
we're not asking if we know for sure that we're going to get super intelligent soon
if we think there's a three a 10 percent chance that something like this might happen in four years
but then it's still in the risk is near term even though i hope as much as anyone that it actually
won't happen for a long time so alessandra and then ron and then mark yeah i just wanted to
respond to the question on the banks um if i understood it correctly is like was that maybe a
model of regulation for ai is that what you meant okay i mean as we cover that sector very deeply
it is it is an area where i mean and it's it's um it's a sector that is heavily regulated and because
they heavily regulated the way that they are developing and deploying and implementing ai
today is that even as they develop there's a lot of oversight in the models themselves and then they
have they go through first and second and third lines of defense where there is oversight again
and then they submit to the regulators and in a way i mean i can't believe i'm saying this but
in a way the banks could be a model with which if you are to impose a oversight at a company level
for the ai that they're using the banking model is not a bad one because the way that they
assess the risks as they go from development to deployment into production and output so
that could be a blueprint or something to to look at for businesses themselves to regulate
themselves is that if that's where we end up so there might be something to learn but bearing
in mind agi is different from everything that's ever been before ron what would you like to add
and well i'd like to come back to the discussion about open source versus closed source and i'm
a bit surprised that here at the table there is a strong belief in closed lots of companies
that might contain lots of zero days we don't know of instead of trusting civil society
and and on regulation we therefore not only need to regulate development we also need to
regulate use and there and that's what the ai does for example sounds like none of the
old traditional models are going to work sounds like we need something that has a variety of
different approaches we have to transcend some of the previous systems and mark and then we'll
come back to pick up a few more comments um yeah i just wanted to pick up on sort of a question of
near term or existing harms that we see in sort of harms in future because i think what you saw
yesterday with president biden coming out with an executive order in the united states
as well as with uai act which has been a long time in the making you see sort of an attempt and i
think a successful attempt by policy makers to tackle both ai and bias that you see in current
day applications and some of the risks that are like that are agi related and i think that shows
that it's perfectly possible to do both and i know a lot of people in this audience are potentially
driven by existential risk i mean that's why we come to an existential risk observatory event
panel um but i think there is a lot of alliances and bridges that can be built across that space
and i think it's often not helpful to look at both of these things um maybe just on banking um i mean
we saw we seen the 2008 financial crisis where i think lots of people were justifiably angry
because ceo's got away with whatever they were wanting to do uh and governments build out the
banks um there was briefly a lot of regulation that was then rolled back over the past few years
and again we saw a few small banks collapse in california so i think we need to learn some
lessons there around liability and making sure that as we build a liability regime for ai companies
ceo's are also individually and criminally liable if they are indeed negligent or if there are
sort of safety risks that they're they're ignoring so maybe just to add on those two points so half
the panel have got their hands up wanting to speak well i'm going to ignore them briefly and
give the microphone very quickly to three people in the audience but you have to be quick because
we're out of time already thanks uh richard barker and ron introduced the analogy of the
pharmaceutical industry which is not perfect by any means but i spent most of my career in it so
i think there's still a few lessons from that right the first is you regulate not the underlying
technology but the application of the technology so it turns out felidomide is the terrible thing
to give to pregnant women but it actually cures people with multiple myeloma so you i can't imagine
how we're going to actually deeply regulate the the internal workings it's it's how they're used
and it may not be existential risk that is most uh relevant uh to actually harnessing public opinion
it will be some of the things that's already happening that affect them personally uh they're
not just experts but panels um come back to uh legislators and say this is what i saw and this
is what i like and don't like um terry rabie former risk manager um guys you really need some pushback
uh i was deeply appalled at stewart's example of the nuclear industry the regulation of the
nuclear industry in the united states essentially is anti-human it's prevented um the gifts of
energy that's not polluting by regulation we have another anti-human example of regulation in the
u which is a regulation of biology destructive of the advantages that we could get from genetic
regulation of biology destructive of the advantages that we could get from genetically modified
organisms so look it won't do the needs to be a little bit more pushback to you guys so you get
your story straight okay who's behind her here so earlier thank you don't show me there's key
substances professor russer make a very important distinction about as a i to be safe for humans
and a i uh safe for use as a tool the first one is a new type of intelligence the second one
is a just used as a tool so that's where the regulation comes in context we can regulate
a i as safe to use and we must have a control of our development so it doesn't become an
existential risk my question to the panel is the following one is it possible or shouldn't be possible
in order to avoid uh open sourcing problems to develop just one super intelligence program
that will beat any uh small guys developments and in that way make us safer and the second
question is what will follow this summit deliverable which thanks so plenty to talk about there
learning from the pharmaceutical industry regulating apps not platforms we had terry
pushing back quite hard saying goodness look at the mess of regulation in the nuclear industry
and in gmo's we had tony asking about a unified approach with one research and development program
and also what's going to happen next so 30 seconds each max's hand up again i have a good friend
in the american nuclear industry who told me that what really killed it wasn't regulation but it was
fukushima and then three mile island i for open source 20 seconds on that i actually think i i love
open source almost as much as yon lakoon mit is the cradle kind of open source but obviously we
don't open source plutonium in this geranium and similarly here we should get away from this childish
debate about whether your open source nothing or everything and just ask where the line goes
finally there's a technical solution i think to this which is not creepy but still works
which steve omohandro and i wrote a paper about where where you actually have control of your own
hardware chips you own it no the government doesn't see what you run but it's not going to run
certain kinds of really creepy code because the hardware itself won't don't it's like a
virus checker in reverse where if your code can't prove that it's not making bioweapons
it just won't run so you can find out more about that proposal if you watch max's ted talk how
all i was just gonna ask doesn't that make it a backdoor in kind of the same way that
csam detection on i message it makes it a backdoor no it's completely decentralized no one has
access to your chip it's just if you want to chip that'll run the harmful code you have to make your
own chip we need standards for hardware which is what stew was saying who's who's wants to jump in next
andrea just a quick reply to the ipcc model uh i very much hope that we will not have an international
agency model after the ipcc here uh we are in crunch time and we have now knowledge of the risks
and we have common knowledge about the risks uh the role of the pcc was a great organization to
build over decades uh essentially expertise and information to governments to deliberate on how
to act we do not have decades and we know what the problems are governments are already acknowledging
what the problems are we need action not a yearly report alessandra i would i would agree with that
i think we're at a point where and also i don't see how it's practically gonna gonna work i mean
are the chinese in the u.s gonna open the kimono and submit to a uk body that wants it to um and
no accountability and no repercussions if they don't so i really don't see it that how that would
work but um there's so much to say and so much to respond to uh on this but i think uh gentlemen
in the in the red jumper they'll very much agree with the the fact that at least until
something has um taken place on the regulation and we've agreed to what that might look like for the
near term big risks but for the here and now um make the companies make the sectors accountable
for how they use it make the buck stop there first and then we can figure out or in parallel figure
out how to regulate it's sort of the bigger bigger questions and the the things that are um giving us
pause but i think the world will submit to a body that's run by the uk but the world might
cooperate with a body that the uk helps to inspire and get off the board mark your hand was up
yeah i also just wanted to pick on uh pick up on the question the gentleman in the red jumper
raised on parallels with the pharmaceutical industry uh yeah you're just sort of shining
beacon here in the audience um i mean i think on the one hand like the time where everyone
just could produce whatever potion they wanted to and put it out on the market that has disappeared
and i think thankfully disappeared and i think that there is lessons we can learn in that in
terms of potentially licensing or making sure that you guarantee that something's safe when it comes to
regulating the application of ai i think i'm significantly more skeptical uh this has been
for example in the senate hearing this was what uh christine amon gomri from ibm was pushing quite
heavily we see on both sides of the atlantic big tech really pushing for application based
regulation because that often means that the underlying big systems that they are building
won't be regulated right because how do you regulate regulate gpt4 if if you only regulate
applications and it's only the hospital that then integrates it into a chatbot for patient contact
contact that actually has to deal with the regulatory burden so i think you do need to force
these big tech companies to do risk identification and mitigation even if they can't particularly
specify oh it goes into that application or or this other one so i think you need a bit of a
combination of both anik are you a fan of the ipc model ipcc or would you prefer the i c fg
model i c fc or the i e uh ai ea i think there are a couple of models proposed i think what's
important is to not um put it in one hand uh not in the hand of a few corporates but also not
in the hands of one state so the current race is not healthy and we need to think about how
we get them back to the table i know you you have some ideas about this and uh yeah we we as i cfg
are trying to show and build the scenarios to explain what it means to look at a future where
emerging tech is governed and what it means when you look at a future where emerging tech is not
governed and that this will hopefully help decision makers come together and work together because
there are many other emerging technologies that might disrupt our society in many other ways
too just around the corner absolutely so we had this discussion shortly before the panel because
yeah uh ai is of course a turbo charger for a number of technologies that are being developed
at a fast pace so we are also looking at mirror technology and quantum and biotech and i mean a
lot of here in the room are looking at different technologies but the power come from the combination
of those and they also round the corner ron final remarks yes yes it works um first i agree that we
should both look into both the models and the applications not one of them but um what i think
is that we yes we need science but we do not have decades um so we need some sort of form of a
rapid response mechanism and in that we need a credible helix we need both government civil
society we need science um and and we need all of them at the table thanks closing words yana
okay one thing i would say about the regulation issue i think uh friends we most of it's like
he has this concept of dial-up progress uh that a lot of conversations end up in like
do we need more progress or less progress uh which is kind of like way too black and white
way of looking at things what you actually want to do is like look look like there are different
ways uh where we want more progress and different places where we want want less progress so it's
actually completely consistent to believe as i believe that yes we have over like over-regulated
a lot of things in a way that is kind of detrimental uh for us but that doesn't mean that we really
should stop regulating things and new things as they come up thanks so please stay on the
stage for a moment we're going to have a few closing words from Otto who is the head of
ero which is part of the organization that has made this happen Otto are you here
yes and by the way this discussion is a prelude to an even more important discussion which is
going to be taking place in the pub in the good old british tradition afterwards some of you might
want to join us where we can get around to all of you who had hand ups and i unfortunately couldn't
take your question Otto thank you David sorry um yeah thanks and thank you all of so so much
for being present here today um as some of us has already mentioned we're here in wilton hall
this was built in 1943 as an assembly hall for the world for two codebreakers and while deciphering
they have progressed beyond imagination and borrow multiple exponential curves here hardware
data quantity algorithm capabilities are all growing with tens of percentage points per year
so i think we all or at least a lot in this room will suspect where this leads uh which is a i that
has the capability to do mental tasks much better than we can and of course this presents amazing
opportunities but according to most existential risk experts we also risk nothing short of human
extinction here and it does mean that our species is on the line so when i turned on the radio last
weekend the bbc was discussing human extinction by ai and i think that this was dramatic but also
hopeful at the same time so i thought this was dramatic since human extinction caused by our
own actions is now officially a possibility and it never ceases to amaze me that we have been stupid
enough to let it get this far but hearing this discussed on national radio for me was also extremely
hopeful because up until now attempts to reduce the real human extinction risks were minor and
world leaders were not paying attention and with the summits that's starting tomorrow i think this
is really changing so i think it's hopeful that after the uk's prime minister speech on ai last
week in which he explicitly warned of human extinction risks the questions that followed
from the press were no longer about prime minister is this a real concern shouldn't you be concerned
about the bills of the of your people instead of this but instead at least some of the questions
were about are you addressing this problem seriously enough and shouldn't we consider instead
posing ai a moratorium or are you doing the right thing with backing responsible scaling
and i think this is exactly the debate that we need so i think it's now important that we
continue in this direction so we must organize ai's safety summits much more often we must open
them up so everybody gets to say we must have societal debates about this and i think in general
we must come together to coordinate and if we do that we are confident at the extension risk
observatory that we can implement the measures that are needed and this is why we have organized
this event and i think it's a huge privilege that we're able to do this together with conjecture
so thank you so much for co-organizing this event and we also want to continue organizing events
like this one but it's impossible without the support of all of you so if you want to support us
doing this there was a flyer that you got handed at the beginning please scan the QR code there
and there's possibilities to support us could be with funding could be with volunteering
could be with just following and sharing our content so this is enormously appreciated
and with that can i please get some applause for all our amazing speakers professor Stuart Russell
Connolly Professor Max Teckmark
Jan Tellin Annika Brack Mark Brackle Ron Rosendell
Alexandra Moussevisidae Andrea Mariotti and Helhotzen please don't stop clapping
um and finally a special thanks as well to David Wood a moderator Ruben Dileman, Katrina Joslyn,
Connor Axiotis, Sue Chisholm, Tillman Schepke, Niky Drogdowski, Mark van der Waal and Joep Soeren
and everyone here at Wilton Hall who made this all possible thanks a lot for helping us out
and please join us for drinks at Three Trees which is about 10 to 15 minutes
more from here so i hope to see you all there thank you
and some people believe in the future there's going to be a wedding in here shortly
so we all need to get out unfortunately unless we're part of that wedding crowd
so biome is chat but chat whilst moving out thank you
