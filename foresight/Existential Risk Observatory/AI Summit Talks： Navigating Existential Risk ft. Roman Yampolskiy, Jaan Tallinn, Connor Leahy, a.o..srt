1
00:00:00,000 --> 00:00:08,680
I encourage you to take a seat. We will be starting almost on time because we have a

2
00:00:08,680 --> 00:00:22,080
very rich agenda on a very big topic. We are talking about navigating existential risk.

3
00:00:22,080 --> 00:00:28,680
Navigating what people have described as a very difficult, tortuous landscape of risks

4
00:00:28,680 --> 00:00:38,040
that are made worse by oncoming new frontier AI. That's not just the AI that we have today,

5
00:00:38,040 --> 00:00:46,120
but AI that we can fairly easily imagine as coming within the next year or two. Next generation

6
00:00:46,120 --> 00:00:54,000
AI that's more powerful, more skillful, more knowledgeable, potentially more manipulative,

7
00:00:54,040 --> 00:01:01,320
potentially more deceitful, potentially more slippery, definitely more powerful than today's AI.

8
00:01:01,320 --> 00:01:09,480
That AI might generate existential risks in its own right. That AI is likely also to complicate

9
00:01:09,480 --> 00:01:17,720
existing existential risks, making some of the risks we already know about more tricky to handle,

10
00:01:18,480 --> 00:01:26,200
more wicked. We might also talk about the way in which next generation AI might be the solution

11
00:01:26,200 --> 00:01:33,800
to some of the existential risks and dilemmas facing society. If we can apply AI wisely,

12
00:01:33,800 --> 00:01:43,240
then perhaps we can find the narrow path through this difficult landscape. Welcome navigators in

13
00:01:43,240 --> 00:01:50,960
the hall. Welcome to navigators watching the live stream. Welcome to people and AIs watching the

14
00:01:50,960 --> 00:02:00,080
recording of this discussion. Let's get stuck in. We have lots of very capable, knowledgeable

15
00:02:00,080 --> 00:02:06,880
speakers who will approach this from a diversity of points of view. Indeed, I think one of the hazards

16
00:02:06,880 --> 00:02:12,560
in this whole topic is that some people want to be a little bit one-dimensional. They want to say

17
00:02:13,280 --> 00:02:18,240
this is how we'll solve the problem. It's quite straightforward. In my view, there are no

18
00:02:18,240 --> 00:02:23,560
straightforward solutions here, but you can make up your own minds as you listen to what all the

19
00:02:23,560 --> 00:02:28,480
speakers and panelists have to say. And yes, in the audience, you'll have a chance later on to

20
00:02:28,480 --> 00:02:34,400
raise your hand and get involved in the conversation too. The first person you're going to hear from

21
00:02:34,400 --> 00:02:42,080
is unfortunately not able to be with us tonight, but he has recorded a short video. He is Sir Robert

22
00:02:42,160 --> 00:02:49,280
Buckland, MP, former Lord Chancellor of the United Kingdom, which means he was responsible for the

23
00:02:49,280 --> 00:02:56,320
entire justice system here, former Secretary of State for Wales. He is still an MP and he has a

24
00:02:56,320 --> 00:03:04,880
side hustle as a senior fellow at the Harvard Kennedy School, where he is writing papers on

25
00:03:04,880 --> 00:03:11,520
exactly the topics we're going to be discussing tonight, namely how does AI change key aspects of

26
00:03:11,520 --> 00:03:19,440
society, potentially making it better, potentially making it much worse if we are unwise. So let's

27
00:03:20,000 --> 00:03:24,160
watch Sir Robert Buckland who will appear by magic on the big screen.

28
00:03:25,680 --> 00:03:32,160
Well, I'm very pleased to be able to join you, albeit virtually, for the Conjecture ARO Summit

29
00:03:32,160 --> 00:03:39,360
on AI and the challenges and opportunities that it presents us. And I think my

30
00:03:40,240 --> 00:03:45,120
pleasure at being with you is based upon not just my own experience in government,

31
00:03:45,760 --> 00:03:54,240
but also my deep interest in the subject since my departure from government last year. Now when I

32
00:03:54,240 --> 00:04:01,040
was in government, I had responsibility for, for many years, the legal advice that was given to

33
00:04:02,160 --> 00:04:07,040
departments and indeed to the government in general when I was in the Law Offices Department as the

34
00:04:07,040 --> 00:04:13,440
Solicitor General. And then responsibility for running the Ministry of Justice's Lord Chancellor

35
00:04:13,440 --> 00:04:20,000
and Secretary of State for over two years before a brief return as Welsh Secretary last year.

36
00:04:20,720 --> 00:04:26,000
That seven years or so experience within government as a minister gave me, I think,

37
00:04:26,000 --> 00:04:32,160
a very deep insight into the pluses and the minuses of the way the government works, the

38
00:04:32,640 --> 00:04:42,400
efficiencies and indeed the inefficiencies about process. And I think clearly, as in other walks of

39
00:04:42,400 --> 00:04:48,400
life, artificial intelligence, machine learning will bring huge advantages to government processes,

40
00:04:48,400 --> 00:04:56,320
to improve efficiency, to speed up a lot of the particular ways in which government works,

41
00:04:56,320 --> 00:05:01,440
which will be, I think, to the benefit of citizens, whether it's citizens waiting for

42
00:05:01,440 --> 00:05:07,840
passport applications, visa applications, or other government processes, benefits, for example.

43
00:05:08,480 --> 00:05:15,440
However, I think that we kid ourselves if we don't accept the fact that alongside the benefits

44
00:05:15,440 --> 00:05:22,800
come potential pitfalls. And the first and most obvious one, I think, for me is the scrutability

45
00:05:22,800 --> 00:05:29,200
of process. In other words, the way in which we understand how decisions are made. And that's

46
00:05:29,200 --> 00:05:34,560
very important, because understanding how decisions are made is part of democratic

47
00:05:34,560 --> 00:05:39,920
accountability in societies like ours, where individuals or organizations wish to challenge

48
00:05:40,560 --> 00:05:44,880
decisions made by government, perhaps through judicial review applications,

49
00:05:44,880 --> 00:05:51,040
then the explicability of those decisions, which is accompanied by a duty of candor

50
00:05:51,040 --> 00:05:58,400
by the government in order to disclose everything about those decisions, is part of that accountability.

51
00:05:58,400 --> 00:06:03,280
And of course, it's sometimes very difficult to explain how the machine has come to decisions.

52
00:06:04,080 --> 00:06:09,360
And more fundamental than that, we have to accept that if the data sets that are used in order to

53
00:06:09,360 --> 00:06:18,320
populate the processes are not as full of integrity as they should be, and are not the product of

54
00:06:18,320 --> 00:06:24,160
genuinely objective and carefully calibrated processes, then we are in danger of importing

55
00:06:24,240 --> 00:06:29,920
historic biases into the system, whether it's biases against neurodiverse people making job

56
00:06:29,920 --> 00:06:35,200
applications, or indeed biases against people of color in the criminal justice system,

57
00:06:35,760 --> 00:06:42,560
simply because the data sets have imported those historical anomalies, those historical imbalances.

58
00:06:43,200 --> 00:06:49,520
Now, all those questions have really got me thinking very deeply about the impact of machine

59
00:06:49,520 --> 00:06:56,160
learning on the ethics of justice itself. And as a result of my thinking, I was delighted last

60
00:06:56,160 --> 00:07:01,040
year to be accepted as a senior fellow at the Mosova Romani Center for Business and Government

61
00:07:01,040 --> 00:07:06,640
at Harvard Kennedy School. And I am working currently on a number of papers relating to

62
00:07:07,520 --> 00:07:14,800
the impact of AI and machine learning on the administration of justice and the law itself.

63
00:07:15,760 --> 00:07:23,040
It really developed from my own experience as a law chancellor, from digitalization,

64
00:07:23,040 --> 00:07:30,480
I should say, of the courts, when during the COVID pandemic, we had to move many, many thousands

65
00:07:30,480 --> 00:07:37,520
of hearings online for the first time. I think we jumped from a couple of hundred phone or online

66
00:07:37,600 --> 00:07:46,880
hearings to 20,000 a week in a very short compass. And the status quo will never be the same again.

67
00:07:46,880 --> 00:07:52,400
In fact, it has moved on, I think, in a way that we just hadn't foreseen before the pandemic. Now,

68
00:07:52,400 --> 00:07:57,040
I think that's a good thing. But I also think that accompanying this question about increased

69
00:07:57,040 --> 00:08:03,280
efficiency is the use of artificial intelligence. Now, in some jurisdictions, such as China,

70
00:08:03,280 --> 00:08:09,360
we are seeing its increased use not just to do legal research and to prepare cases,

71
00:08:09,360 --> 00:08:15,840
but to actually decide themselves. In other words, the AI judge. Now, that's all well and good.

72
00:08:16,400 --> 00:08:22,640
But do we actually know what populates the data sets that then forms the basis of the decisions

73
00:08:23,360 --> 00:08:30,560
made? And I think it's that unintentional bias or indeed worse than that potential

74
00:08:30,640 --> 00:08:37,200
intentional bias, whether that's influenced by a government or indeed a corporate that

75
00:08:37,200 --> 00:08:44,880
might be able through their financial means to influence a procedure or indeed the way in which

76
00:08:44,880 --> 00:08:52,480
we deal with cases, knowing as we might do more information about the way in which judges make

77
00:08:52,480 --> 00:08:58,560
their decisions. All these questions, I think, need to be asked now before we end up in a position

78
00:08:58,560 --> 00:09:04,560
where we've slept, walked into a completely different form of justice from the one that we know.

79
00:09:05,600 --> 00:09:10,640
Now, underpinning all of this, I think, is the need to ask a fundamental question

80
00:09:10,640 --> 00:09:15,840
about judgment itself. And that's what I've been doing in my first paper. You know, the essence

81
00:09:15,840 --> 00:09:21,440
of human judgment is something that will be based not just upon an understanding of the law, but

82
00:09:21,440 --> 00:09:27,360
on our experiences as human beings. And you can go right back, as I have done, to the judgment of

83
00:09:27,360 --> 00:09:35,360
Solomon and his emotional response to the woman who clearly was the true mother of the child that

84
00:09:35,360 --> 00:09:42,160
he proposed to be cut in half. Now, you know, that's an example, I think, of the human element of

85
00:09:42,160 --> 00:09:51,440
judgment, which has to be an essential foundation of decision making, particularly when it comes to

86
00:09:51,520 --> 00:09:57,760
the assessment of credibility of a witness, a human witness giving evidence upon which the case

87
00:09:58,320 --> 00:10:04,400
stands or falls. And of course, for judge, that applies for juries as well in criminal trials,

88
00:10:04,400 --> 00:10:11,360
particularly here in the UK. Now, you know, all these questions, I think, need to be asked.

89
00:10:12,160 --> 00:10:16,480
And then we need to work out what it is that we want to retain out of all of this.

90
00:10:16,480 --> 00:10:20,800
Now, I don't think we should make any cosy assumptions that because at the moment some

91
00:10:20,800 --> 00:10:32,080
large learning systems are having hallucinations. I don't think we should be assuming that just

92
00:10:32,080 --> 00:10:38,720
because of that, therefore AI will never work in a way that can achieve a greater degree of

93
00:10:38,720 --> 00:10:48,000
certainty. I think the inevitable arc of development will result in better and better and more

94
00:10:48,000 --> 00:10:54,800
capable machines. That's inevitable. But what we must be asking at the same time as capability

95
00:10:54,800 --> 00:11:00,080
is ensuring there is greater security and safety when it comes to the use of AI.

96
00:11:00,080 --> 00:11:04,880
And that really underpins, I think, the work that I'm doing in the field of justice. What

97
00:11:04,880 --> 00:11:11,840
does this all lead to then? Well, we have the AI safety summit in the UK next month. I very much

98
00:11:11,840 --> 00:11:18,800
hope that that summit will first of all involve those critical players in terms of international

99
00:11:19,920 --> 00:11:27,200
organisations and key countries as well that will come together to commit to creating,

100
00:11:27,200 --> 00:11:32,400
I think, a defined framework within which we should be using AI safely. And that framework,

101
00:11:32,400 --> 00:11:37,360
I think, will have to take several forms. I think in the field of justice we could do with an

102
00:11:37,360 --> 00:11:44,560
international framework of principles, which will ensure transparency and which can reassure

103
00:11:44,560 --> 00:11:50,480
people that in cases of the liberty of the individual, criminal cases, cases where perhaps the

104
00:11:50,480 --> 00:11:58,480
welfare of a child and the ultimate destination of a child is in issue, then the human element

105
00:11:58,480 --> 00:12:05,360
will be the key determinant in any decisions that are made. And that the use of machines will be

106
00:12:05,360 --> 00:12:12,960
transparent and made known to all the parties throughout the proceedings. And then other walks

107
00:12:12,960 --> 00:12:20,080
of life, I think the AI safety summit has to then look as well at whether frameworks can be created

108
00:12:20,080 --> 00:12:25,680
and what form they should take. I think it's tempting to try and be prescriptive. I think

109
00:12:25,680 --> 00:12:31,360
that would be a mistake, not just for the obvious reason that AI is developing and therefore anything

110
00:12:31,360 --> 00:12:37,840
that we write in 2023 will soon be out of date, but the very fact that AI itself does not mean an

111
00:12:37,840 --> 00:12:45,600
alloyed harm. In fact, it means a lot of benefit and also some neutral effects as well. And where

112
00:12:45,600 --> 00:12:52,480
you have that approach, then a principle-based system seems to me to be more sensible than

113
00:12:53,120 --> 00:12:58,240
overly prescriptive and detailed rules as you would have, for example, to prevent a crime,

114
00:12:58,240 --> 00:13:05,040
such as fraud. So just some preliminary thoughts there as to the impact of machine learning.

115
00:13:05,040 --> 00:13:12,320
I don't pretend to be a technical expert. I'm not. But my years in justice, my years as a lawyer,

116
00:13:12,320 --> 00:13:19,520
a judge and as a senior cabinet minister, I think obliged me to do some of the thinking now

117
00:13:20,000 --> 00:13:26,720
to help ensure that countries like Britain are in the forefront of the sensible and proportionate

118
00:13:27,360 --> 00:13:33,680
regulation of the use of machine learning and other types of artificial intelligence. If we

119
00:13:33,680 --> 00:13:40,480
don't do it now, then I think we'll be missing an unhistoric opportunity. I wish you all well,

120
00:13:41,040 --> 00:13:44,800
and I look forward to meeting some of you in the future and discussing these issues

121
00:13:45,520 --> 00:13:48,080
as they develop. Thank you very much.

122
00:13:49,920 --> 00:13:53,200
Well, thank you, Sir Robert, who may be watching the recording of this.

123
00:13:54,560 --> 00:14:00,960
Don't be prescriptive, he said. Let's sort out some sensible proportionate regulation.

124
00:14:01,840 --> 00:14:07,520
Is that credible? Is that feasible? You'll be hearing from other panellists who may be commenting on

125
00:14:07,520 --> 00:14:15,440
that shortly. So Robert also said there are risks such as the inscrutability of AI. We don't

126
00:14:15,440 --> 00:14:21,600
understand often how they reach its decisions. We don't understand the biases that might be there,

127
00:14:21,600 --> 00:14:27,120
that might have been planted. We might lose charge. We might become so used to AI taking

128
00:14:27,120 --> 00:14:34,240
decisions that humans end up in a very sad place. But how bad could things get? That's what we're

129
00:14:34,240 --> 00:14:39,920
going to hear from our next speaker. So I'm going to ask Conor Lehi to come up to the stage,

130
00:14:39,920 --> 00:14:47,440
who I briefly introduce him. Conor is the CEO of Conjecture. If you haven't heard about Conjecture,

131
00:14:47,440 --> 00:14:52,320
I think you need to do a bit more reading. Perhaps Conor will say a little bit about it. They are

132
00:14:53,200 --> 00:15:00,480
AI alignment solutions company, international, but with strong representation here in the UK.

133
00:15:00,480 --> 00:15:13,440
So welcome Conor, the floor is yours. Thank you so much. It's so great to see you all today.

134
00:15:13,440 --> 00:15:20,160
So happy to be able to talk to you here in person. And man, do we live in interesting times, to put

135
00:15:20,160 --> 00:15:29,360
it lightly. The world has changed so much. Just in the last few years, few months even, so much

136
00:15:29,360 --> 00:15:35,840
has happened in the world of AI and beyond. Just a mere couple of years ago, there wasn't

137
00:15:35,840 --> 00:15:44,000
such a thing as chat GPT, or even GPT3, or 4, or 2, or any of those. It was a different world

138
00:15:44,000 --> 00:15:51,200
not too long ago when technologists such as myself, weird little hobbyists, worried about

139
00:15:51,200 --> 00:15:59,760
the problem of AGI and how it will affect the world. Back then, it still seems so far away.

140
00:16:00,480 --> 00:16:08,880
It seemed like we still had time. But now, we find ourselves in a world of unrestricted,

141
00:16:09,440 --> 00:16:18,080
uncontrolled scaling, a race towards the finish, towards the end, to scale our AI systems ever

142
00:16:18,080 --> 00:16:25,760
more powerful, more general, more autonomous, more intelligent. And the reason I care about this

143
00:16:26,400 --> 00:16:32,480
is very simple. If we build systems that are smarter than humans, that are more capable

144
00:16:32,480 --> 00:16:40,080
at manipulation, deception, politics, making money, scientific research, and everything else,

145
00:16:40,880 --> 00:16:46,880
and we do not control such systems, then the future will belong to them, not to us.

146
00:16:48,800 --> 00:16:57,280
And this is not the future I want. I want a future in which humanity gets to decide its destiny,

147
00:16:57,840 --> 00:17:02,240
or we get to decide the future for ourselves, for our children, for our children's children,

148
00:17:03,520 --> 00:17:10,320
that we like. The future where our children can live long, happy lives surrounded by beauty,

149
00:17:10,320 --> 00:17:16,560
art, great technology, instead of being replaced by SOA's automata. And let me be clear,

150
00:17:17,760 --> 00:17:26,240
that this is the default outcome of building an uncontrolled AGI system, the full replacement

151
00:17:26,240 --> 00:17:36,240
of mankind. And what we're seeing is that AI is only exponential. There's a race.

152
00:17:36,800 --> 00:17:44,560
All the top organizations, which is OpenAI, DeepMind, Anthropic, among others, are racing ahead

153
00:17:45,280 --> 00:17:53,520
as fast as the VC dollars would scale up their work. And this has given us an exponential.

154
00:17:53,520 --> 00:18:00,320
AI is on an exponential curve, both on hardware and on software. It's improving at incredible rates.

155
00:18:01,120 --> 00:18:07,920
And when you're dealing with an exponential, there are precisely two times you can react to it,

156
00:18:08,880 --> 00:18:17,840
too early or too late. There is no such thing as reacting at just the right moment on an exponential,

157
00:18:17,840 --> 00:18:23,200
where you find just the perfect middle point, just in the nick of time, when everyone agrees

158
00:18:23,200 --> 00:18:29,360
that the problem is here and everything has perfect consensus. If you do this, you are too late.

159
00:18:30,320 --> 00:18:39,760
It will be too late. And the same thing applies to AGI. If we wait until we see the kinds of

160
00:18:39,760 --> 00:18:45,920
dangerous general purpose systems that I am worried about, then it will already be too late.

161
00:18:47,120 --> 00:18:56,000
By the moment such systems exist, the story of mankind is over. And so if we want to act,

162
00:18:56,000 --> 00:19:02,400
we must act well, well before such things actually come into existence.

163
00:19:03,440 --> 00:19:07,680
And unfortunately, we do not have much time. How the world has changed.

164
00:19:09,280 --> 00:19:16,720
As frightening and as terrible the race may be, there's also good changes.

165
00:19:18,000 --> 00:19:23,440
A few years ago, I could have barely imagined seeing governments, politicians, and the general

166
00:19:23,440 --> 00:19:29,600
public waking up to these weird nerd issues that I cared about so much with my friends online.

167
00:19:30,480 --> 00:19:35,520
But now we're looking forward to the first international AI summit convened by the UK

168
00:19:36,320 --> 00:19:42,720
and the famous Dip Bletchley Park. And this is great news. The European Commission has recently

169
00:19:42,720 --> 00:19:48,640
officially acknowledged the existential risks from AGI along with the risks from nuclear weapons

170
00:19:48,640 --> 00:19:55,760
and pandemics. This is great progress. This is fantastic. It is good to see our governments

171
00:19:55,760 --> 00:20:00,560
and our societies waking up and addressing these issues, or at least beginning to acknowledge them.

172
00:20:02,240 --> 00:20:07,440
And we must use this opportunity. We have an opportunity right now, and we must prevent it

173
00:20:07,440 --> 00:20:14,160
from being wasted. Because there's also bad news. But we're having this great opportunity

174
00:20:14,800 --> 00:20:18,880
to start building the regulation and the coordination necessary for a good future.

175
00:20:20,480 --> 00:20:26,160
The very people who are creating these risks, the very people at the heads of these labs,

176
00:20:26,160 --> 00:20:31,520
these organizations, building these technologies, are the very people who are being called upon

177
00:20:31,520 --> 00:20:36,480
by our governments to help regulate the very problem that they themselves are creating.

178
00:20:37,280 --> 00:20:43,440
And let me be very explicit about this. The problem that we face is not AGI.

179
00:20:44,160 --> 00:20:52,080
AGI doesn't exist yet. The problem we face is not a natural problem either. It is not an external

180
00:20:52,080 --> 00:21:00,720
force acting upon us from nature. It comes from people, from individual people, businessmen,

181
00:21:01,360 --> 00:21:07,520
politicians, technologists, athletes, large organizations, who are racing, who are scaling,

182
00:21:07,520 --> 00:21:12,560
who are building these technologies, and who are creating these risks for their own benefit.

183
00:21:14,320 --> 00:21:18,560
But they have offered us, these very people who are causing this problem,

184
00:21:19,280 --> 00:21:26,560
have offered us a solution. Fantastic. And they are pushing it as hard as they can

185
00:21:26,560 --> 00:21:33,120
towards the UK government and the upcoming summit. So what is the solution? The solution to the problem

186
00:21:33,120 --> 00:21:41,840
of scaling of these labs, these acceleration labs such as Anthropic and ARC have been pushing for.

187
00:21:41,840 --> 00:21:48,080
What is the solution? Well, the solution to the scaling problem is called responsible scaling.

188
00:21:48,880 --> 00:21:54,800
Now, what is responsible scaling, you might ask. You see, it's like normal scaling except you put

189
00:21:54,800 --> 00:22:03,040
the word responsible in front of it, and that makes it good. So of course I'm joking somewhat,

190
00:22:04,080 --> 00:22:11,760
but there's a lot of truth in humor. Responsible scaling is basically the policy

191
00:22:12,640 --> 00:22:17,760
and you can read this on both ARC or philanthropic's website. It's the policy proposal

192
00:22:18,320 --> 00:22:25,360
that we should continue to scale uninhibited until at some future time when tests and evaluations

193
00:22:25,360 --> 00:22:30,000
that do not yet exist and we do not know how to build, but the labs promise us they will build,

194
00:22:31,040 --> 00:22:37,600
detect some level of dangerous capabilities that we do not yet know, and then once it gets to that

195
00:22:37,600 --> 00:22:43,840
point, then they will stop, maybe, except there is a clause in the Anthropic version of the RSP

196
00:22:43,840 --> 00:22:50,000
paper in which they say that if a different organization was scaling even supers unsafely,

197
00:22:50,000 --> 00:22:58,240
then they can break this commitment and keep scaling anyways. So this could be sensible

198
00:22:58,880 --> 00:23:05,680
if they committed to a sensible bound, a conservative point on which to stop, but unfortunately the

199
00:23:06,640 --> 00:23:12,480
responsible scaling policy RSP fails to actually commit to any objective measure whatsoever.

200
00:23:13,120 --> 00:23:20,800
Oops. So effectively the current policy is to just keep scaling until they feel like stopping.

201
00:23:22,960 --> 00:23:27,760
This is the policy that is being suggested to our politicians and to the wider world

202
00:23:27,760 --> 00:23:35,280
as the responsible option for policy makers. It is trying to, is very clear that it is trying

203
00:23:35,280 --> 00:23:43,040
to recast this techno-libertarian extremist position as sensible, moderate, responsible even.

204
00:23:43,680 --> 00:23:52,080
Now, in my humble opinion, the reasonable moderate position to when dealing with a threat that is

205
00:23:52,080 --> 00:23:56,000
threatening the lives of billions of people is to simply not do that.

206
00:23:58,400 --> 00:24:05,760
But instead, there is trying to pass off this as the sensible middle ground position.

207
00:24:07,440 --> 00:24:14,240
The truth of RSP is that it comes from the same people who are causing this risk to exist.

208
00:24:15,920 --> 00:24:21,520
These people, the heads of these labs, many of the scientists and the policy people and

209
00:24:21,600 --> 00:24:28,160
the other people working on this have known about existential risks for decades and they fully admit

210
00:24:28,160 --> 00:24:32,480
this. This is not like they haven't heard about this. It's not even that they don't believe it.

211
00:24:33,040 --> 00:24:37,920
You can talk to them. They're on the record talking about how they believe that there is a

212
00:24:37,920 --> 00:24:46,080
significant chance that AGI could cause extinction of the entire human species. In a recent podcast,

213
00:24:46,080 --> 00:24:51,600
Dario Amade, the CEO of Anthropic, one of these labs, himself, said that he thinks it's a

214
00:24:51,600 --> 00:24:57,840
probably 25% chance that it could kill literally everybody. And they're doing it anyway.

215
00:24:58,640 --> 00:25:04,160
Despite this, they keep doing it. Why? Well, if you were talking to these people, what they might

216
00:25:04,160 --> 00:25:10,480
tell you is that, sure, you know, I know it's dangerous. I am very careful. But these other guys,

217
00:25:11,040 --> 00:25:15,440
well, they're even less careful than me. So I need to be number one. So I actually

218
00:25:15,440 --> 00:25:19,200
have to race faster than everyone else. And they all think this about each other.

219
00:25:20,880 --> 00:25:26,720
They call this incremental, but they never pause. They always race as fast as they possibly can.

220
00:25:27,920 --> 00:25:34,960
Do as I say, not as I do. There is a technical term for this. It's called hypocrisy.

221
00:25:35,840 --> 00:25:43,280
And RSP is no different. They are simply trying to twist words in an Oralian way

222
00:25:44,480 --> 00:25:48,160
to be allowed to keep doing the thing that they want to do anyways,

223
00:25:49,600 --> 00:25:59,360
which they themselves say could risk everybody. I mean, has responsible in the name, must be good.

224
00:25:59,360 --> 00:26:08,640
And people like Sam Altman talk about iterative deployment, about how we must iteratively release

225
00:26:08,640 --> 00:26:15,360
AI systems into the wild so societies can adapt to them, be inoculated by them. It sounds so nice.

226
00:26:15,360 --> 00:26:22,080
That sounds almost responsible. But if you're really trying to inoculate someone, you should

227
00:26:22,080 --> 00:26:29,120
let the host actually adapt before you jam in the next new pathogen into their weakened immune system

228
00:26:29,120 --> 00:26:35,440
as fast as you possibly can. But this is exactly what laboratories such as Open AI, DeepMind,

229
00:26:35,440 --> 00:26:39,840
Anthropic, and Tier 2 labs such as Meta are doing with all the force they can muster.

230
00:26:40,480 --> 00:26:46,160
To develop more and more new systems as fast as possible, release them as fast as possible,

231
00:26:46,160 --> 00:26:53,440
wide as spread possible. Now, if Open AI had developed a GPT-3 and then completely stopped

232
00:26:53,440 --> 00:26:58,480
further scaling, focused all of their efforts on understanding GPT-3, making it safe, making

233
00:26:58,480 --> 00:27:04,480
it controllable, working with governments and civil society to adapt the new problems

234
00:27:04,480 --> 00:27:11,200
posed by the system for years or even decades, and then they build GPT-4? Yeah, you know what?

235
00:27:11,200 --> 00:27:16,720
Fair enough. I think that could work. That would be responsible. But this is not what we're seeing.

236
00:27:19,760 --> 00:27:25,360
All of these people and all of these institutions are running a deadly experiment

237
00:27:26,080 --> 00:27:33,520
that they themselves think might cause extinction. It is gain-of-function research on AI,

238
00:27:33,520 --> 00:27:40,160
just like viruses, developed and released to the public as fast and aggressively as possible.

239
00:27:42,240 --> 00:27:46,880
They're developing more and more dangerous and more and more powerful viruses as quickly as

240
00:27:46,880 --> 00:27:56,560
possible and forcing it into everyone's immune system until they break. There is no responsible

241
00:27:56,560 --> 00:28:04,320
gain-of-function research for extinction-level threats. There is no such thing. We have no

242
00:28:04,320 --> 00:28:11,120
control over such systems and there is no responsible way to continue like this. And anyone

243
00:28:11,120 --> 00:28:21,520
who tells you otherwise is lying. A lot has changed. The summit can lead to many boring outcomes,

244
00:28:22,800 --> 00:28:27,920
just exchanges of diplomatic platitudes as is often the outcome of such international events.

245
00:28:29,600 --> 00:28:36,960
They have some good outcomes and it can have some very, very bad outcomes. Success in the summit

246
00:28:37,440 --> 00:28:43,440
is progress towards stopping the development of extinction-level AGI before we know how to control

247
00:28:43,440 --> 00:28:52,640
it. Most other outcomes are neutral and bad outcomes. They look like policymakers blindly

248
00:28:52,640 --> 00:28:59,520
and sheepishly swallowing the propaganda of the corporations to allow them to continue their

249
00:28:59,520 --> 00:29:06,400
unconsciously dangerous gamble for their own personal gain and glory at the expense of the

250
00:29:06,400 --> 00:29:15,760
entire planet. We owe it to ourselves and our children to build a good future, not gamble it

251
00:29:15,760 --> 00:29:25,840
all on a few people's utopian fever dreams. Governments and the public have a chance to regain

252
00:29:25,840 --> 00:29:31,200
control over the future and this is very hopeful. I wasn't sure we were going to get it, but the

253
00:29:31,200 --> 00:29:37,360
summit speaks to this, that people can act, that governments can act, that civil society can act,

254
00:29:38,080 --> 00:29:45,680
that it is not yet too late. There is simply no way around it. We need to stop the uncontrolled

255
00:29:45,680 --> 00:29:55,200
scaling, the uncontrolled race if we want a good future. And we are lucky because we can do this.

256
00:29:55,200 --> 00:30:02,720
We can cap the maximum amount of computing power going into these AI systems. We can have government

257
00:30:02,720 --> 00:30:09,200
intervene and prevent the creation of the next more dangerous, more general, more intelligent

258
00:30:09,200 --> 00:30:17,280
strain of AI until we are ready to handle it. And don't let anything distract you from this.

259
00:30:17,440 --> 00:30:25,200
There is no good future in which we continue on this path and we can change this path.

260
00:30:27,040 --> 00:30:32,160
We need to come together to solve these incredibly complex problems that we are facing

261
00:30:32,800 --> 00:30:38,400
and not let ourselves be led astrayed by corporate propaganda. And I hope that the governments

262
00:30:38,800 --> 00:30:45,760
and a civil society of the world do what needs to be done. Thank you.

263
00:30:58,400 --> 00:31:03,520
Thank you, Conor. We'll take questions from the floor in a moment. I'll just start off with the

264
00:31:03,600 --> 00:31:09,520
question I think maybe on many people's minds. Why would a super intelligent AI actually want

265
00:31:09,520 --> 00:31:14,400
to kill humans? I have a super intelligent calculator which is no desire to kill me. I

266
00:31:14,400 --> 00:31:19,680
have a super intelligent chess playing computer that is no desire to kill me. Why don't we just

267
00:31:19,680 --> 00:31:25,920
build as responsible scaling an AI that has no desires of its own? Because we don't know how

268
00:31:25,920 --> 00:31:32,720
to do that. Why did Homo sapiens eradicate Homer neanderthalis and Homo erectus and all the other

269
00:31:32,720 --> 00:31:38,000
species that we share the planet with? You should think AGI, not of as a calculator,

270
00:31:38,000 --> 00:31:44,320
but as a new species on our planet. There will be a moment where humanity is no longer the only

271
00:31:44,320 --> 00:31:51,680
or even the most intelligent species on this planet and we will be outcompeted. I don't think it

272
00:31:51,680 --> 00:31:57,920
will come necessarily from malice. I think it will be efficiency. We will build systems that make

273
00:31:57,920 --> 00:32:04,080
money that are effective at solving tasks, at solving problems, at gaining power. These are

274
00:32:04,080 --> 00:32:10,000
what these systems are being designed to do. We are not designing systems with human morals and

275
00:32:10,000 --> 00:32:15,120
ethics and emotions. They're AI. They don't have emotions. We don't even know how to do that. We

276
00:32:15,120 --> 00:32:20,400
don't even know how emotions work. We have no idea how you could get an AI to have emotions like a

277
00:32:20,400 --> 00:32:26,720
human does. So what we're building is extremely competent, completely sociopathic, emotionless,

278
00:32:26,720 --> 00:32:31,440
optimizing machines that are extremely good at solving problems, extremely good at gaining

279
00:32:31,440 --> 00:32:36,960
power, that do not care about human values or emotions, never sleep, never tire, never get

280
00:32:36,960 --> 00:32:43,520
distracted, can work a thousand times faster than humans and people will use these for many reasons

281
00:32:43,520 --> 00:32:49,200
to help and people and eventually I think humanity will just no longer be in control.

282
00:32:49,920 --> 00:32:54,960
Questions from the floor. There's a lady in the third drawer down here. Just wait for the mic,

283
00:32:54,960 --> 00:33:02,560
sorry, so that the audience online can hear you. Thank you, Susan Finnell from Finnell Consult.

284
00:33:03,920 --> 00:33:08,160
To stop the arms race, certainly at a geographical level, I mean in nuclear,

285
00:33:10,320 --> 00:33:15,040
the states and Europe can tell which countries are building nuclear weapons and what they've got

286
00:33:15,040 --> 00:33:22,560
and they can do tests. If computing power is a thing that needs to be capped to slow this down

287
00:33:22,560 --> 00:33:30,240
enough, is there a way to monitor what other countries or people in a clandestine way are doing

288
00:33:30,240 --> 00:33:35,760
and how does that work? This is a fantastic question and the extremely good news is yes,

289
00:33:36,880 --> 00:33:43,520
the at least currently, this will change in the near future, but the current state to build frontier

290
00:33:43,520 --> 00:33:50,560
models requires incredibly complex machines, massive supercomputers that take megawatts of energy.

291
00:33:50,560 --> 00:33:56,320
So this is on the order you'd have of like a nuclear centrifuge facility. So these are massive,

292
00:33:56,320 --> 00:34:02,160
huge machines that are only built by basically three or four companies of the world. There are

293
00:34:02,160 --> 00:34:07,680
very, very few companies and there is extreme bottlenecks on the supply chain. You need very,

294
00:34:07,680 --> 00:34:13,520
very specialized infrastructure, very specialized computer chips, very specialized hardware to

295
00:34:13,520 --> 00:34:18,240
be able to build these machines and these are produced exclusively by countries basically in

296
00:34:18,240 --> 00:34:24,640
the West and Taiwan. There is many ways where the US or other intelligence services can and already

297
00:34:24,640 --> 00:34:30,480
are intervening on these supply chains and it would be very easy to monitor where these things are

298
00:34:30,480 --> 00:34:37,040
going, who is buying them, where is energy being drawn in large scales. So it is not easy and the

299
00:34:37,040 --> 00:34:42,480
problem is that AI is unexponential both with hardware and with software. Eventually it will be

300
00:34:42,480 --> 00:34:49,200
possible to make essentially dangerous AGI on your home laptop probably, maybe not, but it seems

301
00:34:49,200 --> 00:34:55,840
plausible. If we get to that world, we're in big trouble. So this is part also why we have to buy

302
00:34:55,840 --> 00:35:01,840
time. At some point there will be a cutoff where we'll have algorithms that are so good that either

303
00:35:01,840 --> 00:35:07,200
we have to stop everyone from having a PlayStation at home, which doesn't seem that plausible,

304
00:35:07,840 --> 00:35:12,480
or at that point we have to have very good global coordination and regulation.

305
00:35:14,080 --> 00:35:17,840
Thanks. Just past the mic behind you, there's a person in the row behind.

306
00:35:18,720 --> 00:35:25,360
Robert Whitfield from One World Trust. Can I ask about Bletchley Park? Do you know, I mean are

307
00:35:25,360 --> 00:35:33,600
you participating and if not, do you know anybody else with similar views to you who is participating?

308
00:35:34,560 --> 00:35:38,400
I can't comment too much since it's closed doors. It's a very private event,

309
00:35:38,400 --> 00:35:42,640
unfortunately, so I don't think I have the liberty to talk about exactly what I know.

310
00:35:42,640 --> 00:35:48,320
I think the guest list is not public. I don't know most of the people who are coming. I know

311
00:35:48,320 --> 00:35:53,840
the obvious ones. All the CEOs of all the top labs, of course, are attending. It's not a secret.

312
00:35:54,800 --> 00:36:00,400
I don't know who, if anyone, of my reference class is attending.

313
00:36:00,480 --> 00:36:07,920
Just past the mic next to you, Robert. Thank you. Perhaps I can answer that question. Anybody

314
00:36:07,920 --> 00:36:17,280
that has read The Guardian today, there is an interview with Clifford and for the very first

315
00:36:17,280 --> 00:36:24,160
time, not for the second time, it has been clarified that there will be only about 100 people

316
00:36:24,160 --> 00:36:31,680
participating on the first day. Anybody is invited, including China, on the second day,

317
00:36:32,480 --> 00:36:40,720
apparently there will be only the coalition of the willing. So those who subscribe to the frontier

318
00:36:40,720 --> 00:36:48,000
model forum, they will sit on the second day. That's the current question. My main impression

319
00:36:48,000 --> 00:36:53,680
from that article generates very positive. I would say I've been surprised, as you would be

320
00:36:53,680 --> 00:37:02,400
surprised, that the UK government is really doing what it can to get the mission to what the title

321
00:37:02,400 --> 00:37:08,800
of the conference says, the AI safety summit. It's not about regulation, it's about controlling AI,

322
00:37:08,800 --> 00:37:14,160
and they're trying to do their best. The problem is, as outlined in that interview,

323
00:37:14,960 --> 00:37:21,760
is that we seem to be alone. We have the states a little bit, but the rest wants to go their own

324
00:37:21,760 --> 00:37:29,120
way and do it on their own territory, which is, I think, the tune. I agree. Sooner or later,

325
00:37:29,120 --> 00:37:33,600
international coordination around these issues will be necessary. It is as simple as that. If

326
00:37:33,600 --> 00:37:38,800
you want humanity to have a long, good future, we need to be able, as a global civilization,

327
00:37:38,800 --> 00:37:43,280
to handle powerful technologies like this. Take a question right from the back.

328
00:37:47,280 --> 00:37:53,040
In terms of legislation, what kind do you think is most effective? I've heard, for example,

329
00:37:53,040 --> 00:37:59,840
liability law takes too long to actually have an effect, and compute governance generally seems to be

330
00:38:00,400 --> 00:38:09,840
very easy to be called totalitarian. What do you think of legislation such as models must be released

331
00:38:09,840 --> 00:38:17,040
with a version before pre-processing, and there'll be attacks on the number of harmful outputs done

332
00:38:17,040 --> 00:38:23,920
by the model before the pre-processing? I am open to many kinds of regulation, per se. I would strongly

333
00:38:23,920 --> 00:38:28,000
disagree with the description of compute governance. This is like saying that, you know, not being

334
00:38:28,000 --> 00:38:32,320
private citizens not having nuclear weapons is totalitarian. I respectfully disagree. I'm quite

335
00:38:32,320 --> 00:38:36,160
happy that people do not have private nuclear weapons, and I do not think that people should

336
00:38:36,160 --> 00:38:43,360
have private AGI's. Similarly, I think liability is very promising. I think it has to be strict

337
00:38:43,360 --> 00:38:48,800
liability, so liability for developers rather than just users. This aligns the incentives of

338
00:38:48,800 --> 00:38:53,680
developers with those of wider society. The point of liability is to price in the negative

339
00:38:53,760 --> 00:38:59,040
externalities for the people actually causing them, so I'm a big fan of this. A third form of

340
00:38:59,040 --> 00:39:04,720
policy I would also suggest is a global AI kill switch. This would be a protocol where

341
00:39:05,520 --> 00:39:11,440
some number of countries or large organizations participate, and if some number of them decide

342
00:39:11,440 --> 00:39:17,840
to actually do this protocol, all major deployments of frontier models must be shut down and taken

343
00:39:17,840 --> 00:39:24,000
offline, and this should be tested every six months as a fire drill for five minutes to ensure full,

344
00:39:24,960 --> 00:39:29,680
so that hopefully we never need it, but if we do, that at least the protocol exists.

345
00:39:30,560 --> 00:39:34,640
Thank you very much. There are lots of hands up. Hold your questions. There will be more

346
00:39:34,640 --> 00:39:39,520
chance for Q&A later. Corner final remarks before we hand over to the next speaker.

347
00:39:40,160 --> 00:39:45,840
I want to really say that I do agree that it is very hopeful to see that the UK is trying to do

348
00:39:45,840 --> 00:39:50,400
things and is trying to push us forward in the good world, because what we really need,

349
00:39:50,400 --> 00:39:58,960
as I said briefly, what we need is as a civilization to mature enough to be able to handle dangerous

350
00:39:58,960 --> 00:40:05,360
technology. Even if we don't build AI right now, at some point we will build something so powerful

351
00:40:05,920 --> 00:40:10,080
that it can destroy everything. It's just a matter of time. Our technology keeps becoming more

352
00:40:10,080 --> 00:40:16,240
powerful. The only way for us to have a long-term good future is to build the institutions, the

353
00:40:16,240 --> 00:40:24,160
civilization, the world that can handle this, that can not build such things, that can not hold

354
00:40:24,160 --> 00:40:30,160
the trigger. I do think this is possible. I do think that it is, in fact, so I have heard,

355
00:40:30,160 --> 00:40:35,920
in the interest of most people, to not die. I think there is a natural coalition here,

356
00:40:36,000 --> 00:40:41,040
but it is hard, and I will not deny this extremely challenging problem, almost unlike,

357
00:40:41,680 --> 00:40:45,440
I mean, basically something we haven't faced in this nuclear proliferation, and even then it's

358
00:40:45,440 --> 00:40:51,360
even worse this time. It's a incredibly difficult problem. It is not over yet, but it could be

359
00:40:51,360 --> 00:40:57,040
very soon. If we don't act, if we let ourselves get distracted, if we fall for propaganda and all

360
00:40:57,040 --> 00:41:03,440
these things, these opportunities can be gone, and that will be it. But the game is not over yet,

361
00:41:03,520 --> 00:41:05,360
so let's do it. Thank you very much.

362
00:41:14,080 --> 00:41:18,960
So we've heard from a politician, a senior politician. We've heard from a technology

363
00:41:18,960 --> 00:41:25,760
entrepreneur and activist. We're now going to hear from a professor who is zooming in

364
00:41:25,760 --> 00:41:31,120
all the way from Kentucky from the University of Louisville. He's an expert. He's written several

365
00:41:31,120 --> 00:41:37,600
books on cybersecurity, computer science, and artificial superintelligence. Ah, Roman,

366
00:41:37,600 --> 00:41:44,000
I see you on the screen. I hope you're hearing us. Tell us, can we control superintelligence?

367
00:41:44,720 --> 00:41:49,280
Over to you. No. The answer is no. I'll tell you why in a few minutes.

368
00:41:53,440 --> 00:41:57,440
That's fine. So you can share your slides or talk to us whenever you're ready.

369
00:41:58,320 --> 00:42:01,920
Let's do the slides. Connor did a great job with

370
00:42:04,080 --> 00:42:09,200
his presentation. Let me see one second here.

371
00:42:13,120 --> 00:42:16,320
In the meantime, we can see the covers of some of your books in the background.

372
00:42:17,120 --> 00:42:18,240
Yes, absolutely.

373
00:42:18,240 --> 00:42:21,760
Security and artificial superintelligence.

374
00:42:22,400 --> 00:42:30,400
We're now having a slight technical issue as the technologist is found to slides. Great.

375
00:42:31,120 --> 00:42:35,440
Okay. Yeah, that's the hardest part. If I can get slides going, the rest is easy.

376
00:42:36,720 --> 00:42:39,920
Okay, so I didn't know what Connor's going to talk about.

377
00:42:41,840 --> 00:42:46,080
He did a great job. He's a deep thinker and covered a lot of important material.

378
00:42:46,720 --> 00:42:50,560
I will cover some of the same material, but I will have slides.

379
00:42:51,760 --> 00:42:58,320
And I will slightly take it to the next level where I may make Connor look like an optimist.

380
00:42:59,120 --> 00:43:05,440
So let's see how that goes. To begin with, let's look at the past.

381
00:43:06,400 --> 00:43:12,560
Well, over a decade ago, predictions were made about the state of AI based on nothing but compute

382
00:43:12,560 --> 00:43:18,720
power. Ray Kurzweil essentially looked at this scalability hypothesis before it was known as

383
00:43:18,720 --> 00:43:26,800
such and said by 2023, we will have computational capabilities to emulate one human brain.

384
00:43:26,800 --> 00:43:33,120
By 2045, we would be able to do it for all of humanity. So we are in 2023.

385
00:43:33,680 --> 00:43:35,920
Let's look at what we can do in the present.

386
00:43:38,160 --> 00:43:43,040
In the spring of this year, a program was released, which I'm sure many of you got to play with,

387
00:43:43,040 --> 00:43:50,400
called GPT-4, which is not a general intelligence, but it performs at a level

388
00:43:50,400 --> 00:43:56,800
superior to most humans in quite a few domains. If we look specifically at this table of different

389
00:43:56,800 --> 00:44:05,040
exams, lower exams, medical exams, AP tests, GRE tests, it's at 98, 99th percentile of performance

390
00:44:05,600 --> 00:44:14,560
for many of them, if not most. That is already quite impressive. And we know that there are models

391
00:44:14,560 --> 00:44:21,600
coming around, which are not just text models, but multi-model large models, which will overtake

392
00:44:21,600 --> 00:44:28,960
this level of performance. It seems like GPT-4 was stopped in its training process right around this

393
00:44:29,840 --> 00:44:36,560
human capacity. And if we were to train the next model, GPT-5, if you will, will quickly go

394
00:44:36,560 --> 00:44:42,400
into the superhuman territory. And by the time the training run is done, we would already be

395
00:44:43,280 --> 00:44:49,680
dealing with superintelligence out of the box. But let's see what the future holds according to

396
00:44:50,640 --> 00:45:00,960
heads of top labs, prediction markets. So we heard from CEO of Entropic, CEO of DeepMind.

397
00:45:00,960 --> 00:45:06,160
They both suggest that within two or three years, we will have artificial general intelligence,

398
00:45:06,160 --> 00:45:13,600
meaning systems capable of doing human beings can do in all those domains, including science and

399
00:45:13,600 --> 00:45:19,840
engineering. It's possible that they are overly optimistic or pessimistic, depending on your

400
00:45:19,840 --> 00:45:25,680
point of view. So we can also look at prediction markets. I haven't grabbed the latest slide,

401
00:45:25,680 --> 00:45:32,240
but last time I looked, prediction markets also had three to four years before artificial

402
00:45:32,240 --> 00:45:40,640
general intelligence, which is very, very quick. Why is this a big deal? This technology at the

403
00:45:40,640 --> 00:45:46,640
level of human capability means that we can automate a lot of dangerous malevolent behaviors,

404
00:45:46,640 --> 00:45:53,680
such as creating biological pandemics, new viruses, nuclear wars. And that's why we see a

405
00:45:53,680 --> 00:46:01,520
lot of top scholars, influential business people. In fact, thousands of computer scientists all

406
00:46:01,520 --> 00:46:08,720
signed this statement saying that, yes, AI will be very, very dangerous. And we need to take it

407
00:46:09,440 --> 00:46:17,280
with the same level of concern as we would nuclear war. So what is the problem everyone is concerned

408
00:46:17,280 --> 00:46:26,480
about? The problem is that, for one, we don't agree on what the problem is. Early in computer

409
00:46:26,480 --> 00:46:32,080
science, early in the history of AI, concerns were about AI ethics. How do we make software,

410
00:46:32,080 --> 00:46:37,280
which is ethical and moral? And there was very little agreement, nobody solved anything, but

411
00:46:37,280 --> 00:46:42,880
everyone proposed their own ethical system, gave it a name and describe what they had in mind.

412
00:46:44,320 --> 00:46:49,120
About a decade ago, we started to realize that ethics is not enough, we need to look at safety

413
00:46:49,120 --> 00:46:55,840
of those systems. So again, we started this naming competition, we had ideas for friendly AI, control

414
00:46:55,840 --> 00:47:02,240
problem, value alignment, doesn't really matter what we call it, we all intuitively kind of understand

415
00:47:02,240 --> 00:47:08,320
we want a system which if we run it, we will not regret running it. It will be beneficial to us.

416
00:47:08,320 --> 00:47:15,680
So how can humanity remain safely in control while benefiting from superior form of intelligence

417
00:47:15,680 --> 00:47:21,680
is the problem? I would like us to look at, we can call it control problem and the state of the

418
00:47:21,680 --> 00:47:27,360
art in this problem. In fact, we don't really know if the problem is even solvable. It may be

419
00:47:27,360 --> 00:47:32,240
partially solvable, unsolvable, maybe it's a silly question and the problem is undecidable.

420
00:47:32,960 --> 00:47:41,600
A lot of smart people made their judgments known about this, this problem. Unfortunately,

421
00:47:41,600 --> 00:47:49,680
there is little agreement, answers range from definitely solvable from a surprising source

422
00:47:49,680 --> 00:47:58,320
likely as a Riedkowski to very tractable from head of super alignment team at one of the top labs

423
00:47:58,320 --> 00:48:05,760
to I have no idea from a top tuning award winner who created much of machine learning evolution.

424
00:48:06,480 --> 00:48:10,800
So I think it's an important problem for us to look at to address and to understand

425
00:48:11,440 --> 00:48:17,680
how we can best figure out what is the status of the problem. My approach to that

426
00:48:17,680 --> 00:48:22,880
is to think about the tools I would need to control a system like that and intelligent,

427
00:48:23,440 --> 00:48:30,720
very capable AI and the tools I would guess I would need ability to explain how it works,

428
00:48:31,680 --> 00:48:38,480
capability to comprehend how it works, predict its behavior, verify if the code follows design,

429
00:48:38,480 --> 00:48:42,800
be able to communicate with that system and probably some others, but maybe some of the tools

430
00:48:42,880 --> 00:48:49,520
are interchangeable. So I did research and I published results on each one of those tools

431
00:48:50,160 --> 00:48:55,760
and the results are not very optimistic. For each one of those tools, there are strong limits to

432
00:48:55,760 --> 00:49:02,160
what is capable in the worst case scenarios. When we're talking about super intelligent systems,

433
00:49:02,160 --> 00:49:07,280
self-improving code systems, smarter than human capable of learning in new domains,

434
00:49:07,280 --> 00:49:12,000
it seems that there are limits to our ability to comprehend those systems

435
00:49:12,000 --> 00:49:17,360
or for those systems to explain their behavior. The only true explanation for an AI model is the

436
00:49:17,360 --> 00:49:24,480
model itself. Anything else is a simplification. You are getting a compressed, lossy version of

437
00:49:24,480 --> 00:49:30,480
what is happening in the model. If a full model is given, then you of course would not comprehend

438
00:49:30,480 --> 00:49:36,560
it because it's too large, too complex, it's not surveyable. So there are limits to what we can

439
00:49:36,560 --> 00:49:43,440
understand about those black box models. Similarly, we have limits to predicting capabilities of

440
00:49:43,440 --> 00:49:48,720
those systems. We can predict general direction in which they are going, but we cannot predict

441
00:49:48,720 --> 00:49:54,080
specific steps for how they're going to get there. If we could, we would be as intelligent as those

442
00:49:54,080 --> 00:49:58,880
systems. If you're playing chess against someone and you can predict every move they're going to make,

443
00:49:58,880 --> 00:50:04,960
you are playing at the same level as that opponent, but of course we made an assumption

444
00:50:04,960 --> 00:50:10,800
that a super intelligent system would be smarter than us. There are similar limits to our ability

445
00:50:10,800 --> 00:50:17,840
to verify software at best. We can get additional degree of verification for the amount of resources

446
00:50:17,840 --> 00:50:23,920
contributed. So we can make systems more and more likely to be reliable, to have less bugs,

447
00:50:23,920 --> 00:50:29,440
but we never get to a point of 100% safety and security. And I'll explain why that

448
00:50:29,440 --> 00:50:36,560
makes a difference in this domain. Likewise, human language is a very ambiguous language. It's not

449
00:50:36,560 --> 00:50:42,720
even as unambiguous as computer programming languages. So we are likely to make mistakes

450
00:50:42,720 --> 00:50:50,400
in giving orders to those systems. All of that kind of leads us to conclude that it will not be

451
00:50:50,400 --> 00:50:57,280
possible to indefinitely control super intelligent AI. We can trade capabilities for control, but at

452
00:50:57,280 --> 00:51:03,120
the end, if we want very, very capable systems, and this is what we're getting with super intelligence,

453
00:51:03,120 --> 00:51:09,200
we have to surrender control to them completely. If you feel that the impossibility results I've

454
00:51:09,200 --> 00:51:14,960
presented were just not enough, we have another paper where we cover about 50 of those impossibility

455
00:51:14,960 --> 00:51:23,840
results. It's a large survey in a prestigious journal of ACM surveys. From the beginning

456
00:51:23,840 --> 00:51:30,320
of history of AI with founding fathers like Alan Turin who said that he expects the machine

457
00:51:30,320 --> 00:51:38,400
will take over at some point to modern leaders of AI like Elon Musk who says we will not control them

458
00:51:38,400 --> 00:51:48,080
for sure. There is a lot of deep thinkers, philosophers who came to that exact conclusion.

459
00:51:49,040 --> 00:51:57,120
We are starting to see top labs publish reports in which they may gently acknowledge

460
00:51:57,120 --> 00:52:04,080
such scenarios. They call them pessimistic scenarios where the problem is simply unsolvable.

461
00:52:04,080 --> 00:52:10,320
We cannot control super intelligence. We cannot control it indefinitely. We are not smart enough

462
00:52:10,320 --> 00:52:16,880
to do it, and it doesn't even make sense that that would be a possibility. They ask, well,

463
00:52:16,880 --> 00:52:23,280
what's the distribution? What are the chances that we're in a universe where that's the case?

464
00:52:24,160 --> 00:52:32,160
They don't provide specific answers, but it seems from some of the writing and posts they make,

465
00:52:32,160 --> 00:52:38,720
maybe about 15% is allocated to that possibility. I was curious to see what other experts think,

466
00:52:38,720 --> 00:52:46,240
so I made a very small, very unscientific survey on social media. I surveyed people in my Facebook

467
00:52:46,240 --> 00:52:54,000
group on AI safety, and I surveyed my followers on Twitter, and it seems that about a third

468
00:52:54,560 --> 00:52:59,600
think that the problem is actually solvable. Everyone else thinks it's either unsolvable,

469
00:52:59,600 --> 00:53:04,560
or it's undecidable, or we can only get partial solutions or we will not solve it on time.

470
00:53:05,200 --> 00:53:10,160
So that's actually an interesting result. Most people don't think we can solve this problem,

471
00:53:10,720 --> 00:53:15,760
and I think part of the reason they think we cannot solve this problem is because there is a

472
00:53:15,760 --> 00:53:24,800
fundamental difference between standard cybersecurity safety and superintelligence safety.

473
00:53:24,800 --> 00:53:32,320
And cybersecurity, even if you fail, it's not a big deal. You can issue new passwords, you can

474
00:53:32,320 --> 00:53:38,080
provide someone with a new credit card number, and you get to try again. We suspect strongly with

475
00:53:38,080 --> 00:53:44,560
superintelligent safety, you only get one chance to get it right. There are unlimited dangers and

476
00:53:44,560 --> 00:53:51,920
limited damages, either you have existential risks or suffering risks, and we kind of agree that 100%

477
00:53:53,120 --> 00:54:01,680
is not an attainable level of security verification safety, but anything less is not sufficient.

478
00:54:01,680 --> 00:54:07,360
If a system makes a billion decisions a minute and you only make mistake once every couple of

479
00:54:07,360 --> 00:54:13,280
billion decisions, after a few minutes you are dead. And so this is like creating a perpetual

480
00:54:13,360 --> 00:54:18,800
motion machine. You are trying to design perpetual safety machine while they keep releasing more and

481
00:54:18,800 --> 00:54:27,120
more capable systems, GPT-5, GPT-50. At some point this game is not going to end in your favor.

482
00:54:28,160 --> 00:54:34,560
So I'm hoping that others join me in this line of research. We need to better understand what are

483
00:54:34,560 --> 00:54:42,000
the limits to controlling superintelligence systems. Is it even possible? My answer is no, but I would

484
00:54:42,000 --> 00:54:48,960
love to be proven wrong. It would be good to have surveys similar to the ones I conducted on larger

485
00:54:48,960 --> 00:54:56,720
scale to get much more statistically significant results. And in case we do agree that we have this

486
00:54:57,360 --> 00:55:03,520
worst case scenario where we are creating superintelligence and it is impossible to control it,

487
00:55:03,520 --> 00:55:10,000
what is our plan? Do we have a plan of action for this worst case scenario? This is what I wanted

488
00:55:10,000 --> 00:55:15,120
to share with you and I'm happy to answer any questions. Thank you very much Roman.

489
00:55:25,360 --> 00:55:26,480
Optimistic Roman.

490
00:55:29,280 --> 00:55:36,160
Sorry one second I'm trying to figure out how to use Zoom. Go ahead and repeat your question please.

491
00:55:36,480 --> 00:55:43,840
You gave us many reasons to be anxious. What do you think is the best reason for us to be optimistic?

492
00:55:44,880 --> 00:55:50,960
Well there seems to be many ways we can end up with world war three recently so that can slow down

493
00:55:50,960 --> 00:56:01,040
some things. It has been suggested that we can use a different kind of tool which is the kill switch.

494
00:56:01,680 --> 00:56:07,280
Your list of tools that you listed it didn't include that. It's been proposed that each AI system

495
00:56:07,280 --> 00:56:14,240
should be tested with a remote off switch capability. Have you looked at that? Do you think that's a

496
00:56:14,240 --> 00:56:22,800
viable option? So I would guess a superintelligence system would outsmart our ability to press the

497
00:56:22,800 --> 00:56:30,720
off button in time. It will work for not superintelligent AI's pre-GI systems maybe even for

498
00:56:30,720 --> 00:56:36,640
the GI systems but the moment it becomes that much more advanced I think it will outsmart us. It will

499
00:56:36,640 --> 00:56:42,080
take over any kill switch options we have. Let's have some questions from the floor.

500
00:56:45,840 --> 00:56:49,760
I can't see the hands so yes just give the microphone out thank you.

501
00:56:50,720 --> 00:56:57,200
Thank you. I would like to ask how does the scalable oversight that open AI is working on

502
00:56:57,200 --> 00:57:03,600
essentially the way they plan to align superintelligence fit into your expectation of the

503
00:57:03,600 --> 00:57:11,040
future pathway the AGI will take because again as personally we cannot align or control a super

504
00:57:11,040 --> 00:57:16,880
intelligent entity but another AI which is more capable than us could. So how does that fit into

505
00:57:17,440 --> 00:57:23,680
your expectations? So it seems like it increases complexity of the overall system instead of us

506
00:57:23,680 --> 00:57:28,960
trying to control one AI. Now you're trying to control a chain of agents going from slightly

507
00:57:28,960 --> 00:57:34,320
smarter to smarter to superintelligent maybe 50 agents in between and you're saying that you have

508
00:57:34,320 --> 00:57:40,880
to solve alignment problem between all the levels communication problem ambiguity of language between

509
00:57:40,880 --> 00:57:48,720
all those models supervision. It seems like you are trying to get safety by kind of upfuscating

510
00:57:48,720 --> 00:57:54,560
how the model actually works you're introducing more complexity hoping to make the system easier

511
00:57:54,560 --> 00:58:00,960
to control that seems counter-intuitive. But isn't it the case that sometimes you can verify an answer

512
00:58:00,960 --> 00:58:05,680
without understanding the mechanism by which the answer was achieved for example there can be a

513
00:58:05,680 --> 00:58:10,000
chess puzzle and you have no way of working out yourself but when somebody shows you the answer

514
00:58:10,000 --> 00:58:14,960
you can say oh yes this is the answer. So isn't it possible we don't need to really understand

515
00:58:14,960 --> 00:58:20,480
what's going on inside these systems but a simpler AI can at least verify the recommendations that

516
00:58:20,480 --> 00:58:27,200
come out of the more complex AIs. So such a chain may be the solution. Can you claim that you are

517
00:58:27,200 --> 00:58:31,840
still in control if you don't understand what's happening and somebody just tells you don't worry

518
00:58:31,840 --> 00:58:39,120
it's all good I checked it for you? But then it's like we humans we have a network of trust

519
00:58:39,200 --> 00:58:44,880
and I trust some people and they trust others within various categories we can't work out

520
00:58:44,880 --> 00:58:51,600
everything ourselves but we trust some scientists or some engineers or some lawyers who validate

521
00:58:51,600 --> 00:58:57,200
that an AI has a certain level of capability and that AI could come back with verification that

522
00:58:57,200 --> 00:59:03,200
the proposals of a superintelligence should be accepted or should not be. I don't say it's easy

523
00:59:03,200 --> 00:59:06,880
but as you said there's not likely to be a very simple and straightforward solution.

524
00:59:07,840 --> 00:59:14,640
Again to me at least it sounds like instead of trying to make this system safe you said that

525
00:59:14,640 --> 00:59:19,600
you made some other system safe and it made sure that the system you couldn't make safe is safe for

526
00:59:19,600 --> 00:59:26,720
you. Let's take some more questions there's another one in the middle here and then we'll go to the

527
00:59:26,720 --> 00:59:35,520
edge yes thank you. Thank you for the presentation. Number one second thing is that as you're talking

528
00:59:35,600 --> 00:59:41,120
about I think as David was talking about trust basically right could you tell me from your

529
00:59:41,120 --> 00:59:47,120
extensive years of AI research and experience as such that do you really think that humans or

530
00:59:47,120 --> 00:59:55,520
society can be trusted to for example regulate its own self or do you think that really need

531
00:59:55,520 --> 01:00:02,000
some sort of institution of sort that is totally separate from anyone else?

532
01:00:02,160 --> 01:00:12,880
So I'm not sure regulation would be enough Connor correctly pointed out that there is both lobbying

533
01:00:12,880 --> 01:00:19,760
of regulators by the labs and also it becomes easier and easier to train those models with less

534
01:00:19,760 --> 01:00:26,240
compute and over time you will be able to do it with very little resources. The only way forward

535
01:00:26,320 --> 01:00:34,400
I see is personal self-interest if you are a rich young person and you think this is going to kill

536
01:00:34,400 --> 01:00:39,600
you and everyone else maybe it's not any best interest to get there first that's really the

537
01:00:39,600 --> 01:00:46,000
only hope at this point just personal self-interest. The humans are always better if we can band

538
01:00:46,000 --> 01:00:50,320
together with our self-interest rather than each of us individually pursuing our self-interest so I

539
01:00:50,320 --> 01:00:55,760
think this kind of meeting and the community spirit might help. There was a hand over here

540
01:00:55,760 --> 01:01:07,760
yes with I think the red shot on jacket. If we assume that the two kind of well both views that

541
01:01:07,760 --> 01:01:14,240
have been suggested so far are correct in that we're definitely not going to be able to stop

542
01:01:14,960 --> 01:01:20,240
AI development etc and we're going to get to the point where we have no regulation that can

543
01:01:20,240 --> 01:01:24,560
effectively stop things you know people can build in super intelligent AI on their own computers

544
01:01:24,560 --> 01:01:29,200
etc okay so we'll assume that that's a fact that's coming and then we'll also assume that

545
01:01:29,200 --> 01:01:33,520
the control problem isn't a problem because it's a problem that can't be solved and we're definitely

546
01:01:33,520 --> 01:01:38,400
not going to be able to control it well now we're heading and barreling towards the point where we

547
01:01:38,400 --> 01:01:43,600
have super intelligent AIs definitely and we definitely can't control them. What comes next?

548
01:01:45,200 --> 01:01:51,840
What comes next? It's a wonderful question as I said and published you cannot predict what the

549
01:01:51,840 --> 01:02:01,360
super intelligent system will do. All right so was there a question down here? Thank you.

550
01:02:02,640 --> 01:02:08,880
You said that we kind of need a plan but on that last question if that scenario is true

551
01:02:09,760 --> 01:02:14,320
you said we need to do more work in this area but do you have any thoughts as to what we

552
01:02:14,320 --> 01:02:17,520
should be doing what we should be doing to plan for the worst-case scenario?

553
01:02:19,040 --> 01:02:26,320
So to me at least it seems that at least in some cases it is possible to use this idea of personal

554
01:02:26,320 --> 01:02:31,680
self-interest if you have a young person having a good life there is no reason why they need to

555
01:02:31,680 --> 01:02:37,040
do it this year or next year. I understand that someone may be in a position where they are

556
01:02:37,040 --> 01:02:41,680
very old very sick have nothing to lose and it's much harder to convince them not to try

557
01:02:42,240 --> 01:02:48,320
but at least from what I see the heads of those companies are all about the same age they young

558
01:02:48,320 --> 01:02:57,440
they healthy they they have a lot of money there is a good way to motivate them to wait a little bit

559
01:02:57,440 --> 01:03:04,160
maybe a decade or two just out of personal self-interest again. I think my answer to the question

560
01:03:04,160 --> 01:03:10,160
of optimism is that we humans can do remarkable things we humans can solve very hard problems

561
01:03:10,160 --> 01:03:16,720
and so I want to say now that we spread around what the problem is at least some more people can

562
01:03:16,720 --> 01:03:24,880
apply more brain power to it so that's my reason for optimism. Terry? I guess I'm pleased by the

563
01:03:24,880 --> 01:03:31,840
inevitability of this development because it seems to me that if you're going to create

564
01:03:32,400 --> 01:03:39,520
reasoning creatures then those reasoning creatures are going to have moral rise on the same

565
01:03:39,520 --> 01:03:49,760
plane as human beings so I'm looking forward to to chatting with these creatures and joining in

566
01:03:50,320 --> 01:03:54,640
them joining into this kind of discussions and I'm pleased that they won't be able to be thwarted

567
01:03:54,640 --> 01:04:01,200
and it will be wrong to enchain these reasoning creatures. So Roman are you looking forward

568
01:04:01,280 --> 01:04:08,000
to having more of the AIs involved in these discussions as well? So I remember giving a

569
01:04:08,000 --> 01:04:16,640
presentation for a podcast about rights for animals rights for AIs and I was very supportive of all

570
01:04:16,640 --> 01:04:21,920
the arguments developed because I said at one point we will need to use those arguments to beg

571
01:04:21,920 --> 01:04:30,560
for our rights to be retained. The question on the third row here? Yes hi I'm curious Roman

572
01:04:31,920 --> 01:04:38,480
which side of in your hopes of a possible future for us to get through this do you have more hope

573
01:04:38,480 --> 01:04:44,720
on the side of a more top-down sort of totalizing control system for AGI systems so should they

574
01:04:45,440 --> 01:04:52,960
remove the possibility of individual actors getting hold of this and weaponizing it or do you put

575
01:04:52,960 --> 01:05:01,200
more hope in a more sort of decentralized open-source approach to AGI emergence more like an ecology

576
01:05:01,200 --> 01:05:06,240
perhaps some people suggest would be more biologically inspired such that you know immune

577
01:05:06,240 --> 01:05:13,040
immune system like functions could arise which way do you lean in your sensibilities for what is

578
01:05:13,040 --> 01:05:20,880
a viable avenue for us? I'm not optimistic with either of those options the only kind of hope I

579
01:05:20,880 --> 01:05:27,440
see is that for strategic reasons superintelligence decides to wait to strike it will not go for

580
01:05:27,440 --> 01:05:33,440
immediate treacherous turn but decides to accumulate resources and trust and that buys us a couple

581
01:05:33,440 --> 01:05:39,200
of decades that's the best hope I see so far. So we slow things down we'll have more chance to

582
01:05:39,200 --> 01:05:44,880
work out solutions and the slowing down might come from a combination of top-down pressure

583
01:05:44,880 --> 01:05:51,520
and bottom-up pressure maybe have a is there a hand at the very back there yes let's try and get the

584
01:05:51,520 --> 01:06:03,760
microphone back there right at the sitting at the back yes sorry at the in the middle

585
01:06:09,680 --> 01:06:15,760
thanks. Hi Roman thanks for your talk yeah I was wondering what your thoughts are on

586
01:06:16,400 --> 01:06:23,280
aligning the first AGI that is human level or narrowly superhuman if in principle that is possible

587
01:06:23,920 --> 01:06:29,920
and if that is is is it possible in principle to align the next version of AGI

588
01:06:31,360 --> 01:06:38,640
but to use that narrowly superhuman AGI to align it and if if that's all technically

589
01:06:38,640 --> 01:06:48,640
possible then why would we not think like focus on doing that and also and also if you think in

590
01:06:48,640 --> 01:06:58,720
principle alignment is impossible and control is impossible then why why not work on practical

591
01:06:58,720 --> 01:07:06,000
ways to make the whatever AGI is created as nice as possible that is like better than the

592
01:07:06,000 --> 01:07:14,400
counterfactual of try to stop it it won't stop and you know it won't be nice. Well I definitely

593
01:07:14,400 --> 01:07:21,120
encourage everyone to work on as much safety as you can anything helps I would love to be proven

594
01:07:21,120 --> 01:07:26,640
wrong it would be my greatest dream that I'm completely wrong and somebody comes out and says

595
01:07:26,640 --> 01:07:32,400
here's a mistake in your logic and we have developed this beautiful friendly safe system

596
01:07:32,400 --> 01:07:38,080
capable of doing all this beneficial things for humanity that would be wonderful but so far I

597
01:07:38,080 --> 01:07:43,760
haven't seen any progress in that direction what we're doing right now is putting lipstick on this

598
01:07:43,760 --> 01:07:49,600
monster and the show that's all we're doing filters to prevent the model from disclosing its true

599
01:07:49,600 --> 01:07:56,320
intentions then you talk about alignment it's not a very well-defined terms what values are you

600
01:07:56,320 --> 01:08:03,600
aligning it with values of heads of that lab values of specific programmer we as humans don't agree

601
01:08:03,600 --> 01:08:09,760
on human values that's why we have all these wars and conflicts there is a 50-50 split and most

602
01:08:09,760 --> 01:08:16,560
political issues in my country we are not very good at agreeing even with ourselves over time

603
01:08:16,560 --> 01:08:23,360
what I want today is not what I wanted 20 years ago so I think this idea of being perfectly aligned

604
01:08:23,360 --> 01:08:30,240
with eight billion agents and people are suggesting adding animals to it and aliens and other AIs that

605
01:08:30,240 --> 01:08:37,680
doesn't seem like it's a workable proposal our values are changing they're not static and it's

606
01:08:37,680 --> 01:08:45,520
very likely that they will continue changing after we get those systems going I don't see how at any

607
01:08:45,520 --> 01:08:51,920
point you can claim that the system is specifically value aligned with someone in particular the last

608
01:08:51,920 --> 01:08:58,640
question in this section is going to go to Connolly he Roman love your talk I always love your

609
01:08:58,640 --> 01:09:05,520
optimism it's always great to hear you talk so so I'm kind of like going to pick up on the question

610
01:09:05,520 --> 01:09:10,240
I was just asked and just give a bit of my opinion and kind of like here would you think about this

611
01:09:10,240 --> 01:09:17,280
as well so my personal view is that I I do I have read many of your papers in fact and they're

612
01:09:17,360 --> 01:09:23,520
quite good so I do think that I agree with you that like in principle an arbitrarily

613
01:09:23,520 --> 01:09:28,800
intelligent system cannot be safe by any arbitrary like weaker system just kind of a proof of like

614
01:09:28,800 --> 01:09:37,760
you know program size induction and whatnot but in my view it does seem likely that there is a

615
01:09:38,720 --> 01:09:44,240
limit of intelligence far below the theoretical optimum but still significantly above the human

616
01:09:44,240 --> 01:09:51,840
level that can be achieved the reason I think this is that human civilization is actually very

617
01:09:51,840 --> 01:09:58,320
smart compared to a single caveman and can do really really great things so my point of optimism

618
01:09:58,320 --> 01:10:05,760
is it seems possible that if we stop ourselves from making self-improving systems and coordinate

619
01:10:05,760 --> 01:10:10,000
at a very strong scale and have very strong enforcement mechanisms it should be possible to

620
01:10:10,000 --> 01:10:16,960
build systems that are you know n steps you know above human good enough to build you know awesome

621
01:10:16,960 --> 01:10:24,160
you know sci-fi culture ship kind of like worlds but not further I'm wondering if you have an

622
01:10:24,160 --> 01:10:32,240
intuition about like where do things hit impossibilities like to me I think the impossibilities happen

623
01:10:33,200 --> 01:10:39,840
above human utopia but to get to the utopia a bit you already have to do extremely strong

624
01:10:39,840 --> 01:10:44,640
coordination extremely strong safety research extremely strong interpretability extremely

625
01:10:44,640 --> 01:10:48,800
strong constraints on the design of the agis extremely strong regulation which are things in

626
01:10:48,800 --> 01:10:52,560
principle possible wondering kind of like your thoughts about that kind of outcome so con is

627
01:10:52,560 --> 01:10:58,240
not asking about responsible scaling he's asking about limited superintelligence if we had limited

628
01:10:58,400 --> 01:11:03,120
superintelligence could we get everything we want without having the risks that we all fear

629
01:11:04,080 --> 01:11:09,360
so I think I want to emphasize difference between safety and control is it possible to

630
01:11:09,360 --> 01:11:15,840
create a system which will keep us safe in some somewhat happy state of preservation possible

631
01:11:15,840 --> 01:11:22,160
a way in control no that system is the example you give of humanity so humanity provides pretty

632
01:11:22,160 --> 01:11:27,520
nice living for me but I'm definitely not in control if I disagree with society and many issues

633
01:11:27,520 --> 01:11:34,080
in politics and culture it makes absolutely no difference I don't decide things scale it to the

634
01:11:34,080 --> 01:11:39,840
next level all eight billion of us may want something but this overseer this more intelligent

635
01:11:39,840 --> 01:11:44,800
system says it's not good for you we're not gonna do it this is what you're going to be doing right

636
01:11:44,800 --> 01:11:50,240
now so think about all the decisions you make throughout your day you decided to eat this

637
01:11:50,240 --> 01:11:55,520
doughnut you smoked with cigarette all those decisions were made by you because you felt

638
01:11:55,520 --> 01:12:00,480
you wanted to do them they may be good or bad decisions but if you had this much more intelligent

639
01:12:01,040 --> 01:12:06,560
personal advisor ideal advisor you would be at the gym working out eating carrots you may have a

640
01:12:06,560 --> 01:12:15,360
long healthy life but you're not in control and your happiness level may be questionable thank

641
01:12:15,360 --> 01:12:23,280
you very much roman for sharing your thoughts pessimism and some optimism thanks for moving

642
01:12:23,280 --> 01:12:30,640
the conversation forwards

643
01:12:33,840 --> 01:12:38,640
I'm now going to invite the five five members of the panel to come up on stage

644
01:12:38,640 --> 01:12:42,960
and they're each going to have a couple of chances to pass some comments and what they've heard

645
01:12:43,520 --> 01:12:49,520
so there's some stairs over there which you can come up to we're gonna hear from

646
01:12:49,600 --> 01:12:57,280
Jan Tallin who is the co-founder of Skype the co-founder of fli future of life institute

647
01:12:57,280 --> 01:13:04,800
and also CISA the center for study of existential risks we're going to hear from Eva Berens a

648
01:13:04,800 --> 01:13:10,160
policy analyst with the international center for future generations we're going to hear from Tom

649
01:13:10,160 --> 01:13:16,400
Oh who's a journalist who writes from time to time for the BBC amongst other places we're going to

650
01:13:16,400 --> 01:13:25,520
hear from alexandra moussa visit evidence who is the CEO of evident and has a track record with

651
01:13:25,520 --> 01:13:30,000
tortoise media in many other places and we're going to hear from also another representative

652
01:13:30,000 --> 01:13:37,680
from conjecture that Andrea Miotti who is their specialist for AI policy and governance so to

653
01:13:37,680 --> 01:13:42,480
start things let's just hear from each of them a few opening remarks Jan what's your comments

654
01:13:42,480 --> 01:13:46,640
from what you've heard so far have you changed your mind in any ways or all the things that are

655
01:13:46,640 --> 01:13:54,640
missing from the conversation now you all have to speak into the mics I'm being told so I yesterday

656
01:13:54,640 --> 01:14:01,360
I was at the dinner I was invited to a dinner and and my response to an invitation was that okay I

657
01:14:01,360 --> 01:14:08,000
will come but you have to invite Connor because he's making very similar points to me only much

658
01:14:08,000 --> 01:14:16,160
much more intensely so yeah basically I agree I agree with what what Conor said my main caveat

659
01:14:16,160 --> 01:14:23,440
would be that for the last decade or so I've been kind of trying to build a lot of friendly

660
01:14:24,240 --> 01:14:33,840
cooperation between people in the AI companies and making sure that like everybody can

661
01:14:33,840 --> 01:14:40,400
understand that it is in their interests with almost everybody let's be honest almost everybody

662
01:14:40,400 --> 01:14:47,040
understands that is in their interests to and of remaining control and not kill everyone else

663
01:14:48,560 --> 01:14:56,640
and so like for example I am a board observer observer to entropic and entropic is one of

664
01:14:56,640 --> 01:15:01,680
those companies just like conjecture when you go there you can talk to anyone from the receptionist

665
01:15:02,160 --> 01:15:09,520
to the CEO and they are aware of the AI risk I'm very concerned about this but yes I do think as I've

666
01:15:09,520 --> 01:15:16,160
said in several places that I don't think they should be doing what they're doing

667
01:15:17,760 --> 01:15:21,600
so these companies don't really want to do what they're doing but they feel they have to otherwise

668
01:15:21,600 --> 01:15:29,120
they might be left behind so yes so there is this a dilemma in when you want to do a safe AI

669
01:15:30,000 --> 01:15:35,440
one is that you're well safe like when you're trying to figure out how to do safe AI

670
01:15:36,960 --> 01:15:42,960
from one hand you have groups like Miri that the Aliezer Kowsky co-founded and that was the person

671
01:15:42,960 --> 01:15:50,160
who got me involved in AI safety 15 16 years ago where basically the claim is that you have to start

672
01:15:50,160 --> 01:15:55,120
really early even if you don't know exactly what the AI is going to look like because then you have

673
01:15:55,120 --> 01:16:01,120
a lot of time to prepare and then the group on the other end of that axis is entropic where they

674
01:16:01,120 --> 01:16:04,800
say that it's kind of useless to start early because you don't know what you're dealing with

675
01:16:05,440 --> 01:16:10,160
so you need to be as informed as possible so in that strategy you need to be just always

676
01:16:10,160 --> 01:16:14,240
at the frontier and Dario has been very public about this about this strategy of course the

677
01:16:14,240 --> 01:16:20,400
problem there is that like it also works as a perfect justification to raise rates so therefore

678
01:16:20,400 --> 01:16:29,120
it's have like double digit uncertainty both ways about what the actual picture is and so I

679
01:16:29,120 --> 01:16:34,160
do think that this point the labs indeed they are involved in death rates and they think there is

680
01:16:34,160 --> 01:16:39,360
that government intervention needed to get a time out there and we definitely need time out because

681
01:16:39,360 --> 01:16:50,240
we don't have enough safety results and but to yeah Romania Polsky's presentation I'm definitely

682
01:16:50,240 --> 01:16:55,360
more optimistic again as on one of these slides there was the dialogue held with Eliezer and

683
01:16:55,360 --> 01:17:02,000
Eliezer was confident that this can be sold and in fact like I'm super glad that earlier this year

684
01:17:03,360 --> 01:17:09,440
David Tarempel his group got UK government funding and he has this approach called

685
01:17:10,400 --> 01:17:16,400
open agency architecture I don't know exactly what the details there are but like my rough

686
01:17:16,400 --> 01:17:26,320
understanding is that you're using you're scaling AI capabilities and access according to formal

687
01:17:26,320 --> 01:17:34,000
statements that AI is produced and then you use not AI not humans but formal verifiers to verify

688
01:17:34,640 --> 01:17:41,840
those those statements therefore like building up your AI capabilities one formally verified

689
01:17:41,840 --> 01:17:46,560
step at the time there are many criticism of that but this is like one of those approaches that is

690
01:17:46,560 --> 01:17:55,280
kind of at least and principle has like some convincing story that that why it should work in

691
01:17:55,280 --> 01:17:59,680
in principle at least so there are some options that might work but we're going to need time

692
01:17:59,680 --> 01:18:04,480
to develop them exactly so that's why I've been working on like I've been supporting AI safety

693
01:18:04,480 --> 01:18:10,000
research for more than a decade now but unfortunately we just didn't make it we now need more by more

694
01:18:10,000 --> 01:18:17,200
time so let's hear from Eva because you work more with possibilities to inspire policy you've seen

695
01:18:17,200 --> 01:18:25,600
examples of policy in the past slowing down some technological races are you do you see reasons

696
01:18:25,600 --> 01:18:30,320
for optimism do you see ways in which politicians can make a good difference to the landscape we're

697
01:18:30,320 --> 01:18:36,240
discussing definitely definitely that very much plays into some of the problems or the issues

698
01:18:36,240 --> 01:18:39,840
characteristics of the problem that both corners spoke about and that also

699
01:18:39,840 --> 01:18:43,360
Jan Talion just mentioned that one of the problems so we're facing here is a human

700
01:18:43,360 --> 01:18:47,920
coordination problem and one of the ways to address that will be through policy as has been

701
01:18:47,920 --> 01:18:54,560
said many times this evening this is a technology that threatens to kill us to kill us all and the

702
01:18:54,560 --> 01:18:59,440
heads of the government of the companies that are driving forward the technology have agreed that

703
01:18:59,440 --> 01:19:04,480
and publicly stated that that might be the case and yet they seem to be locked into this dilemma

704
01:19:04,480 --> 01:19:09,520
that Jan just mentioned where they are for some or other reason impossible to to stop so I think

705
01:19:09,520 --> 01:19:13,680
that is a point where where government can really make a difference and step in and also

706
01:19:13,680 --> 01:19:19,920
should step in and we've seen that as you hinted at we've seen that work in the past one of the

707
01:19:19,920 --> 01:19:27,520
examples that I often think about is the Montreal Protocol which after the scientific consensus

708
01:19:27,520 --> 01:19:33,920
arose that CFCs and other similar gases actually destroy the ozone layer the international community

709
01:19:33,920 --> 01:19:41,680
did come together in 1987 and agreed through the Montreal Protocol to slowly phase out these gases

710
01:19:41,680 --> 01:19:47,440
so we see here that international cooperation by the international community by governments can

711
01:19:47,440 --> 01:19:52,960
succeed also in the face of the short-term economic interest of private sector companies

712
01:19:52,960 --> 01:19:58,560
in the public interest of well in the end and everyone on earth so I'm not saying that it's

713
01:19:58,560 --> 01:20:04,160
necessarily easy or easy but I think it's definitely possible and it is one of the strongest

714
01:20:04,160 --> 01:20:08,480
levels that we have here to make a difference so I think that's definitely something that we

715
01:20:08,480 --> 01:20:14,400
should lean into very strongly and do our best that that actually happens. The Montreal Protocol

716
01:20:14,400 --> 01:20:19,200
is an encouraging example but we haven't made a very good job of the governments in the world of

717
01:20:19,200 --> 01:20:24,080
controlling carbon emissions we've been talking about it for a long long time and maybe there's

718
01:20:24,080 --> 01:20:28,480
some progress but many people feel this is an example where governments can't cooperate

719
01:20:28,480 --> 01:20:34,160
so what makes you think that we can cooperate with the problems of AI more like the Montreal Protocol

720
01:20:34,160 --> 01:20:39,680
rather than the Paris Agreement to say. Well part of this of course is also that I think that this

721
01:20:39,680 --> 01:20:43,840
is one of the few levels that we have to make a difference at all so I also hope that we will be

722
01:20:43,840 --> 01:20:50,560
able to do it and I agree that looking at past climate conference is one of the negative examples

723
01:20:50,560 --> 01:20:54,800
that we see there is that with these conferences sometimes that the outcomes tend to be very

724
01:20:54,800 --> 01:20:59,840
watered down just because the focus lies on building consensus among all of the different

725
01:20:59,840 --> 01:21:04,240
countries that attend and then in the end you want to have a nice little consensus agreement

726
01:21:04,240 --> 01:21:08,160
that everyone signs so you can demonstrate that everyone's on the same page and everyone goes

727
01:21:08,160 --> 01:21:13,520
home and everyone's happy and I can just say that I think with the UK AI summit that's coming up now

728
01:21:13,520 --> 01:21:17,840
first of all that is a unique opportunity to actually have international cooperation

729
01:21:17,840 --> 01:21:22,160
and coordination on this issue take place you need to create the opportunities for stuff like

730
01:21:22,160 --> 01:21:28,400
that I'm really happy that the UK government took the initiative and created this opportunity I am

731
01:21:28,400 --> 01:21:32,480
one thing that makes me optimistic is that we all know that China is going to attend at least on one

732
01:21:32,480 --> 01:21:38,160
of the days so hopefully they will be able to be brought into the fold and yeah then I just hope

733
01:21:38,160 --> 01:21:43,520
that this opportunity is truly taken and that the outcome of this summit will not be just some vague

734
01:21:43,520 --> 01:21:49,120
commitments to long-term plans but ideally concrete binding commitments to to concrete

735
01:21:49,120 --> 01:21:56,960
next steps let's turn to Alexandra Alexandra you work a lot with businesses businesses are

736
01:21:56,960 --> 01:22:03,040
unsure in many ways how to deal with today's AI do you think there is good advice that they can

737
01:22:03,040 --> 01:22:09,280
be given or is there a sleepwalking process with many of our businesses definitely the latter

738
01:22:09,520 --> 01:22:18,160
I would say so I'm CEO of evident we map benchmark companies on how far they are in their AI

739
01:22:18,160 --> 01:22:25,760
adoption and so when I think it was Connor you mentioned the AI race that is on at the

740
01:22:25,760 --> 01:22:32,000
frontline of development in AI there is also a race as we all know going on in terms of

741
01:22:32,560 --> 01:22:38,480
adopting AI as as quickly as possible there's a sense of being there's a sort of geopolitical

742
01:22:38,560 --> 01:22:47,520
debate on AI development between US Europe China and so on and who's leading on that

743
01:22:47,520 --> 01:22:53,040
not only in AI but also in areas like quantum but in the business level which is where I deal with

744
01:22:53,040 --> 01:22:59,120
spend my time mostly there's a definitely a race on in terms of not being left behind in

745
01:22:59,120 --> 01:23:05,760
adoption of AI and it's an economic question it is a existential question so there's an existential

746
01:23:05,760 --> 01:23:13,760
question on sort of two dimensions in this debate and and so you've got this unstoppable

747
01:23:14,720 --> 01:23:19,680
race going on on the front end of AI and then you've got an unstoppable race on

748
01:23:20,240 --> 01:23:26,960
actual deploying AI at a business level and it's going to be very hard for regulators to keep up

749
01:23:26,960 --> 01:23:33,920
and to Eva's point I think in terms of what we hope will be the outcome often unfortunately comes

750
01:23:33,920 --> 01:23:41,120
with with the catastrophic happening taking place before it really sharpens the minds and

751
01:23:41,120 --> 01:23:47,760
people figure out how urgent it is I think there's a real sense of urgency in in the community around

752
01:23:47,760 --> 01:23:52,800
trying to work out what what the guardrail should be whether it should be a constitution

753
01:23:54,000 --> 01:24:00,720
or how we should think about implementing safety mechanisms in as as we develop further

754
01:24:00,720 --> 01:24:07,280
on our chat on our large language models but I hope it doesn't need a catastrophic moment

755
01:24:07,280 --> 01:24:13,520
for that to sharpen but back to the business question there is this hope that maybe businesses

756
01:24:13,520 --> 01:24:20,720
will self-regulate and I think that is maybe the case in highly regulated sectors you see in the

757
01:24:20,720 --> 01:24:27,040
banking sector and insurance or banking in particular that there is a guardrails put in

758
01:24:27,040 --> 01:24:32,720
place there but that is that there's a lot of businesses that don't have that regulation

759
01:24:33,280 --> 01:24:38,560
around them and I think there is a real risk for this completely running out of control

760
01:24:38,560 --> 01:24:45,040
at a business level as well. Would you advise businesses to self-regulate ahead of standards

761
01:24:45,040 --> 01:24:49,920
and regulations being agreed by governments? I think that's what they're doing or trying some

762
01:24:49,920 --> 01:24:57,920
businesses are trying to do there's a big risk in in the case of winning trust with your customers

763
01:24:57,920 --> 01:25:06,000
and also your your shareholders and and investors if you mishandle AI and you create issues around

764
01:25:06,000 --> 01:25:14,800
not taking into account how to properly deal with biases and other issues that is a situation that

765
01:25:14,880 --> 01:25:20,080
can create a real breakdown in trust with your with your organization so there is that risk

766
01:25:20,640 --> 01:25:27,680
and then there are businesses that don't necessarily lean on trust for their for their for their

767
01:25:27,680 --> 01:25:35,520
business and those are the ones I worry the most about. Indeed let's turn to Tom Oh as a representative

768
01:25:35,520 --> 01:25:40,960
of the world of journalism do you feel journalists have helped the discussion about the existential

769
01:25:40,960 --> 01:25:46,800
threat from AI or have they muddied the water leading people to panic unnecessarily or perhaps get

770
01:25:46,800 --> 01:25:55,520
distracted on side issues rather than the main issue? I think it's all of the above aside from

771
01:25:55,520 --> 01:26:00,560
overrugging the pudding I think most people in this room including me have had a wit scared out of

772
01:26:00,560 --> 01:26:10,560
them by some of the talks just now. One has side issues in journalism coverage of AI and I think

773
01:26:11,280 --> 01:26:19,040
the jobs market is one of those but I have been surprised pleasantly so by how things have progressed

774
01:26:19,040 --> 01:26:25,600
since 2016 and that's the first time that I wrote about AI safety and I think at that point the

775
01:26:25,600 --> 01:26:32,000
prospect of a bad scenario relating to AI was seen as about as likely by my colleagues as

776
01:26:32,000 --> 01:26:38,560
Leicester City winning the Premier League. Anyway several years later I now see lots of my former

777
01:26:38,560 --> 01:26:45,200
colleagues writing to my mind very informed pieces about AI safety and I think that's helped the

778
01:26:45,200 --> 01:26:51,680
public change well arrive at a view and probably a lot of people in this room are aware that the

779
01:26:51,680 --> 01:26:57,520
American public when polled now says that they want regulation of AI and they want a lot of it

780
01:26:57,520 --> 01:27:03,040
and I think we can credit journalism with some of that. Journalism should be doing more but

781
01:27:03,040 --> 01:27:07,920
it's more than I would have thought a few years ago. And if you were to go away and write up a

782
01:27:08,000 --> 01:27:12,480
story about things that you might have changed your mind about tonight and that the public

783
01:27:12,480 --> 01:27:16,080
should pay attention to can you give us a sneak preview what that would include?

784
01:27:18,320 --> 01:27:26,480
Well I think the idea of runaway AI is not new but I think it has been difficult historically to

785
01:27:26,480 --> 01:27:33,440
frame it in a way that really sticks and like really drives its way down your brainstem and we

786
01:27:33,440 --> 01:27:40,000
have different ways of framing AI risk and Mustafa Suleiman's new book which some of you

787
01:27:40,000 --> 01:27:46,640
might have read I think there's a pretty good job of framing it in a way in which he describes

788
01:27:48,240 --> 01:27:55,360
AI being used to accelerate human ingenuity in whatever endeavours humans are up to be they

789
01:27:56,320 --> 01:28:02,400
be they good or be they bad that's one way of framing it and I think we've heard some pretty

790
01:28:02,960 --> 01:28:08,000
compelling ways of telling a story of runaway AI which is a different and scarier story.

791
01:28:09,680 --> 01:28:16,400
Thanks and let's turn to Andrea and your role at Conjecture. What are you doing in a day-by-day

792
01:28:16,400 --> 01:28:22,960
basis to address this question? Well what we're trying to do and I mean kind of current a lot of

793
01:28:23,760 --> 01:28:29,920
first of all to explain the problem to people I've been heartened by the public reaction in

794
01:28:29,920 --> 01:28:36,400
the last years like I also got to know about this problem quite a long time ago and I in the past I

795
01:28:36,400 --> 01:28:41,520
could almost not expect the day that major governments take this problem seriously and the

796
01:28:41,520 --> 01:28:48,960
public understand this problem and we all get together and take some initial promising insufficient

797
01:28:48,960 --> 01:28:57,280
but promising steps to address it. Another thing is figuring out policy solutions and the reality

798
01:28:57,280 --> 01:29:04,480
is that we don't obviously we don't have a playbook for what exactly they look like but what I think

799
01:29:04,480 --> 01:29:14,480
was a common theme of the talks tonight is that clearly at some level of power we are not in

800
01:29:14,480 --> 01:29:22,960
control anymore and everybody expects this. Those who don't expect this are misguided or

801
01:29:23,840 --> 01:29:32,480
expected but don't say it and the positive thing is that there is one physical resource that drives

802
01:29:32,480 --> 01:29:38,080
the majority of what makes this system powerful which is computing power and it's a physical

803
01:29:38,080 --> 01:29:44,240
resource not not like you know algorithms that you could just write on a piece of paper it's

804
01:29:44,960 --> 01:29:52,480
traceable it's expensive large place in data centers and while you know the scaling hypothesis

805
01:29:52,480 --> 01:29:56,560
the idea that you know the more computing power you put into something the more powerful it becomes

806
01:29:56,560 --> 01:30:02,560
might hit some diminution in terms of at some point we do not see any reason to expect it to stop

807
01:30:03,280 --> 01:30:09,680
so we know you know from both sides companies know that more computing power leads to more power

808
01:30:09,680 --> 01:30:15,120
and that's why they're doing what they're doing we know that limiting that computing power is a

809
01:30:15,120 --> 01:30:24,480
very effective way to kind of stem the the bleeding and stop and or pause the situation for a while

810
01:30:24,480 --> 01:30:29,840
take a time out have the time to figure out the solutions have the time to absorb this into society

811
01:30:30,240 --> 01:30:34,800
but how much time will that give us because there's a risk that people will use today's models

812
01:30:35,360 --> 01:30:41,680
to design much more efficient ways to build next generation models and so they could therefore

813
01:30:41,680 --> 01:30:48,880
come under the radar as it were that people who were watching for large use of GPUs would miss

814
01:30:48,880 --> 01:30:54,480
the clever way that somebody has built it so do we have a decade do we have three or four years

815
01:30:54,560 --> 01:30:57,440
or how long yeah that's that's a great question it's

816
01:30:58,640 --> 01:31:03,280
capping computing power is not a permanent solution but it's one of the best solutions we have

817
01:31:04,160 --> 01:31:10,160
at the moment uh as others have said before we are in a double exponential it's not a single

818
01:31:10,160 --> 01:31:15,600
exponential we have an an exponential growth of computing power hardware and exponential

819
01:31:15,600 --> 01:31:25,120
improvement in software we need to start cutting down on one of the two uh cutting down a compute

820
01:31:25,120 --> 01:31:31,920
depends where you put the cap probably will buy us five seven years you can make you can make

821
01:31:31,920 --> 01:31:37,440
what would seem to people at the frontier extremely strong caps that would affect you know

822
01:31:37,440 --> 01:31:42,960
less than 20 companies in the world that probably could buy you 10 years in that period we need to

823
01:31:43,440 --> 01:31:48,000
figure out all of the rest it's going to be a hard problem but we have done it before with

824
01:31:48,000 --> 01:31:51,840
nuclear weapons we've done it before with biological weapons we can do it again we're

825
01:31:51,840 --> 01:31:56,080
going to go around the panelists one more time in the same order i'll give you a chance a choice

826
01:31:56,080 --> 01:32:00,720
panelist you can either comment on what you've heard from somebody else or you can paint me a

827
01:32:00,720 --> 01:32:06,240
picture of what would be a successful ali safety summit in bletchley park if things go well what

828
01:32:06,240 --> 01:32:12,800
would be the outcome and what would also be the follow-up so jan first well i'm the one of the

829
01:32:12,800 --> 01:32:19,360
authors of the post letter so it's like indefinite moratorium uh on further scaling uh would be sort

830
01:32:19,360 --> 01:32:26,080
of my wet dream from outcome from from this summit or perhaps the next one if this one isn't realistic

831
01:32:26,960 --> 01:32:32,080
and what's the chance do you think what what might cause the assembled world leaders to

832
01:32:32,080 --> 01:32:36,720
have an intellectual breakthrough and say yes actually we do need to have this indefinite pause

833
01:32:37,440 --> 01:32:44,560
so currently i'm not very optimistic on on that uh perhaps perhaps but perhaps in six months it

834
01:32:44,560 --> 01:32:51,200
would be much more clearer why this is needed so and and we have more time to gonna do the

835
01:32:51,200 --> 01:32:55,920
necessary loving so the discussion is prepared to ground and when something really bad happens

836
01:32:55,920 --> 01:33:00,560
in six months when gpt five comes out and oh my god at least we'll know what we should be doing

837
01:33:00,560 --> 01:33:06,000
yeah i mean like let's not forget that gpt chat gpt has been out less than one year so

838
01:33:06,560 --> 01:33:12,640
like the world was very different one year ago same question to you either yeah thank you i think

839
01:33:12,640 --> 01:33:18,160
i'm just going to build on top of jan's ideal outcome of the summit and say that i would also

840
01:33:18,160 --> 01:33:24,880
find it terrific if the summit could be the first in a series of repeated um summits like this where

841
01:33:24,880 --> 01:33:29,520
world leaders come together because as we i think a pretty clear picture has been painted tonight

842
01:33:29,600 --> 01:33:35,200
of the fact that the field of ai evolves very quickly and is going to continue to evolve very

843
01:33:35,200 --> 01:33:40,720
quickly if not ever quicker and because of that i think it would be very valuable if we would have

844
01:33:41,520 --> 01:33:46,160
a regular occasion for world leaders to come together and not only make sure that the rules

845
01:33:46,160 --> 01:33:51,120
that they came up with are upheld but also to reevaluate whether they still make sense and

846
01:33:51,120 --> 01:33:55,360
where they need to be adapted or whether new real rules need to be introduced as for example

847
01:33:55,360 --> 01:34:00,320
measures like compute control that andrea mentioned um they buy us some time but at some point they

848
01:34:00,320 --> 01:34:05,120
might not be applicable anymore so not just agreement on rules but setting up some audit

849
01:34:05,120 --> 01:34:11,200
process so that we can figure out whether the rules are being forward or not for example yeah same

850
01:34:11,200 --> 01:34:15,920
question to you i would agree you have to build in i mean right now it's just based especially in the

851
01:34:15,920 --> 01:34:22,720
us um the talks that have been held in the white house and by chuck schumer um the gatherings have

852
01:34:22,720 --> 01:34:30,400
led to sort of ideas around voluntary um adherence to some principles but there is absolutely no

853
01:34:30,400 --> 01:34:36,800
built-in audit or accountability um so i think that we've got to see that come out of of the

854
01:34:36,800 --> 01:34:41,680
uk's ai safety summit among other things maybe there has to be something more concrete around

855
01:34:41,680 --> 01:34:49,120
licensing um of the models and and the use of them and that they have to pass some kind of a

856
01:34:49,120 --> 01:34:55,760
threshold i think the risk of of bad actors getting hold of them is is a is a much higher risk

857
01:34:56,400 --> 01:35:02,960
i think the ia ea structure is is one that one can look at but the the nuclear a lot of the

858
01:35:02,960 --> 01:35:11,680
success probably of the ia ea lies in the mutual assured destruction of humanity by using um nuclear

859
01:35:11,680 --> 01:35:18,720
weapons and this might be the same situation but they're easier to monitor i think um i think this

860
01:35:18,720 --> 01:35:24,400
might be slightly harder because you can land in the hands of bad actors more easily we haven't

861
01:35:24,400 --> 01:35:29,280
really discussed bad actors much in this session tonight maybe that makes the things even more

862
01:35:29,280 --> 01:35:35,200
horrifying we might come back to that later and tom what's your answer what would you like to see

863
01:35:35,200 --> 01:35:38,400
come out of the summit or maybe you've got some comment on something else you've had from the

864
01:35:38,400 --> 01:35:48,880
other speakers well i'll talk about the summits um often when CEOs of um labs developing agi

865
01:35:50,080 --> 01:35:54,240
are asked about regulation um they basically say bring it on we'd love some

866
01:35:54,240 --> 01:35:59,920
regulation um and i think it would be great if politicians could actually put that to the test

867
01:36:00,240 --> 01:36:10,800
very good and andrea what would you like to see if you were invited to bletchley park and given the

868
01:36:10,800 --> 01:36:17,600
microphone for two minutes what would you entreat the assembled world leaders to consider well i

869
01:36:17,600 --> 01:36:22,560
don't want to be too hopeful as some others here have been but at the very least i would like to

870
01:36:22,560 --> 01:36:29,440
see a commitment to the fact that this is extremely dangerous technology continuing to scale leads to

871
01:36:29,440 --> 01:36:36,080
predictable disaster and we need to pull on the brakes right now uh we have a lot of applications

872
01:36:36,080 --> 01:36:42,400
are very beneficial we can focus on those but limit this death race to the ever more powerful

873
01:36:42,400 --> 01:36:48,160
ever more obscure general systems that we can control what i definitely do not want to see

874
01:36:49,280 --> 01:36:55,440
is a diplomatic shake of hands where companies write their own playbook and say we're gonna keep

875
01:36:55,520 --> 01:37:02,560
doing exactly what we we're doing right now but it's gonna sound responsible and governments can

876
01:37:02,560 --> 01:37:07,200
wash their hands and say well we did our part let's move on that would be a very bad outcome

877
01:37:08,320 --> 01:37:13,280
right i'm gonna ask for three questions from the floor i'm going to get the panel to think

878
01:37:13,280 --> 01:37:17,760
which one's the answer we're trying to take people haven't asked before so on the second row here

879
01:37:17,760 --> 01:37:24,240
there's a hand up here and then also the second row over there next as well let's take three

880
01:37:24,240 --> 01:37:29,600
fairly short questions please hi first of all thank you very much for coming here tonight

881
01:37:29,600 --> 01:37:35,360
and sharing your expertise with all of us in this whole question this whole discussion there's the

882
01:37:35,360 --> 01:37:42,080
implicit assumption that agi is coming and it's coming soon and the million dollar question i guess

883
01:37:42,080 --> 01:37:49,440
is when exactly is it coming but a more practical question is what are some warning signs and do

884
01:37:49,440 --> 01:37:54,720
we already see some of those in the systems that we have currently deployed great let's

885
01:37:54,720 --> 01:37:59,680
have a question over here as well sorry the microphones can have to run around at the end of

886
01:37:59,680 --> 01:38:08,800
the second row there thank you for great panel my question is related to yon's comment at the

887
01:38:08,800 --> 01:38:16,880
beginning on essentially dario amadeus philosophy which is you know and and also related to i guess

888
01:38:16,880 --> 01:38:23,520
roman's talk which is how do we solve the control problem and what i've heard the large a our labs

889
01:38:23,520 --> 01:38:30,800
repeat is oh you know we need to increase capability it's only better ai that is going to be able to

890
01:38:30,800 --> 01:38:37,120
help us figure out how to solve a and there's sort of this race to increase capability up to the

891
01:38:37,120 --> 01:38:44,880
point that can help us solve it but no further and and and just sort of thoughts on that philosophy

892
01:38:44,880 --> 01:38:52,320
and and and and you know whether there might be something to it or is it just a completely risky

893
01:38:52,320 --> 01:38:59,120
game you know thanks and there was one in the middle of the third row there pass the microphone

894
01:38:59,120 --> 01:39:06,800
along please to the middle how long of a time frame do you think we have between the arrival of agi

895
01:39:06,800 --> 01:39:12,640
and the arrival of superintelligence and within that time frame could there be tractable solutions

896
01:39:12,640 --> 01:39:18,320
for alignment or the control problem and if so would those solutions be able to be implemented

897
01:39:18,320 --> 01:39:24,240
before hurry up and develop better ai third question was how long might it take between

898
01:39:24,240 --> 01:39:31,040
the arrival of agi and superintelligence and whether there would be time for us to work

899
01:39:31,040 --> 01:39:36,960
out solutions then and my question i guess is well what's all this about agi isn't the bletchley park

900
01:39:36,960 --> 01:39:42,080
summit set up to discuss something else which is frontier models which says that there are

901
01:39:42,080 --> 01:39:49,040
catastrophic risks even before we get to agi so i'm going to go around the same order again

902
01:39:49,040 --> 01:39:55,760
i'll be a bit predictable jan you want to pick one of these questions maybe i mean answer to all the

903
01:39:55,760 --> 01:40:02,640
all three questions is uncertain that's that's why we need to pause and kind of take a time out

904
01:40:02,640 --> 01:40:08,240
and see like how can we kind of create more set more certainty about these things i think i would

905
01:40:08,240 --> 01:40:15,040
answer the anthropic question specifically that definitely is a lot of truth to the to the point

906
01:40:15,040 --> 01:40:21,600
that like the more capable model you have to work with the more kind of better position you are in

907
01:40:21,600 --> 01:40:29,280
particularly you can you can kind of be do like do science in a way that you just can't do with

908
01:40:29,360 --> 01:40:36,720
models from from 10 years ago and also like one claim that people and tropic to make is that like

909
01:40:36,720 --> 01:40:42,880
in some ways it becomes easier as the model kind of has better understanding of what you're trying to

910
01:40:42,880 --> 01:40:51,520
do with it or to do to it but that said again it's a to put it lightly it's playing with fire so

911
01:40:51,520 --> 01:40:57,760
so it's i'm not sure if anyone should be doing it either and jan said it's all uncertain but

912
01:40:57,760 --> 01:41:02,320
can't we at least agree in advance some canary signs that will make us say things are happening

913
01:41:02,320 --> 01:41:08,800
faster than we expected well i mean if we look at the past there were several signs that people

914
01:41:08,800 --> 01:41:15,360
agreed on that they might point out that we're getting into a zone where ai is maybe more capable

915
01:41:15,360 --> 01:41:22,240
than we think it is and i mean we certainly have seen signs connor mentioned or was it roman

916
01:41:22,240 --> 01:41:28,480
mentioned that the current models um i'll perform most humans on things like the bar exam

917
01:41:28,480 --> 01:41:34,320
um these are clearly advances in capabilities that um i almost wonder sometimes if we just

918
01:41:34,320 --> 01:41:39,520
become desensitized to them because we move so fast i mean again charge epd came out a couple

919
01:41:39,520 --> 01:41:45,120
months ago and it's already just normal and people are waiting okay what's the next big thing so um

920
01:41:45,120 --> 01:41:51,120
it doesn't really help um to think retroactively have them in any signs um if you didn't take them

921
01:41:51,120 --> 01:41:55,920
to actually stop and reconsider what you're doing so i think one of the big problems here

922
01:41:55,920 --> 01:42:01,920
is not have there been signs a big problem is can we pre-commit to stopping when we see certain

923
01:42:01,920 --> 01:42:06,320
signs and then actually stop or actually take certain actions and we just haven't seen that before

924
01:42:06,960 --> 01:42:12,400
so this is developing contingency solutions like we're meant to have had contingency solutions

925
01:42:12,400 --> 01:42:20,400
for pandemics yeah yeah any comments alizandra i i will leave the um well how long it's going to

926
01:42:20,400 --> 01:42:27,360
take to reach agi to to the experts on the panel but on the outcome of the summit and i think there

927
01:42:27,360 --> 01:42:35,680
is a bit of a confusion sometimes in in in what we are expecting to be achieved from the discussions

928
01:42:35,680 --> 01:42:42,480
on regulation because there's an obvious very important urgent and existential question around

929
01:42:42,480 --> 01:42:48,400
regulation regulating for the long term but then we also have businesses that are sitting and waiting

930
01:42:48,400 --> 01:42:54,880
for regulation that is here now how is it going to impact my particular sector how is it going to

931
01:42:54,880 --> 01:43:01,280
impact what i'm doing today and what are the immediate and very very real risks right now

932
01:43:01,360 --> 01:43:08,400
here today that we are seeing um with ai having impact on you know media with disinformation

933
01:43:08,400 --> 01:43:14,880
and so on but then there's also the specific um aspects to how that is implemented in particular

934
01:43:14,880 --> 01:43:22,000
sectors so i um hope that we would see addressing both of those short term and long term questions

935
01:43:22,000 --> 01:43:29,280
thanks tom any thoughts yeah on yardsticks i think it's worth remembering that the canonical

936
01:43:29,280 --> 01:43:35,680
yardstick was the turing test um and that's long gone um ai's can now beat humans at um

937
01:43:35,680 --> 01:43:42,960
diplomatic based games for instance um and much more um the modern turing test is i think quite

938
01:43:42,960 --> 01:43:49,200
an interesting proposition um and that's the test of whether the ai can i think make a million dollars

939
01:43:49,200 --> 01:43:56,560
very quickly um but as as eva says we must stop shifting the goalposts um we need to agree that

940
01:43:57,040 --> 01:44:01,760
the one is we should pick one agree that that's the one where we start taking it seriously and

941
01:44:01,760 --> 01:44:07,600
then take it seriously when it is passed which it will be quite soon but in the past people said

942
01:44:07,600 --> 01:44:13,360
you won't manage to solve chess unless you have got a full grasp of all aspects of creativity and

943
01:44:13,360 --> 01:44:18,400
so on and then when deep blue did win at chess people said oh well it's not actually doing it in

944
01:44:18,400 --> 01:44:23,360
the way that we thought would be so terrible it's just grunting out incredibly so i feel

945
01:44:23,360 --> 01:44:28,320
there will always be people who don't move the goalposts but they'll say well how it was implemented

946
01:44:28,320 --> 01:44:34,960
it doesn't demonstrate too intelligent yeah i think that's a good point um and it reminds us

947
01:44:34,960 --> 01:44:42,880
something um conna said um which is that there won't be consensus at the time to act so we need

948
01:44:42,880 --> 01:44:49,440
to be able to build a coalition of the willing uh andrea what's your views on these questions

949
01:44:50,080 --> 01:44:56,400
yeah maybe answering the last one first on isn't a summit about frontier ai well much like with

950
01:44:56,400 --> 01:45:02,720
goalposts it feels a bit like terminology is being shifted all the time sometimes quite willingly

951
01:45:02,720 --> 01:45:08,560
by the companies building this uh you know in in the in the old days people used to talk about

952
01:45:08,560 --> 01:45:16,640
superintelligence or friendly ai or strong ai then became agi then recently the frontier term was a

953
01:45:16,640 --> 01:45:22,720
kind of open ai entropic rebrand of oh no like we're not well we're gonna get to very powerful

954
01:45:22,720 --> 01:45:28,160
ai system soon but it's frontier which sounds better than agi because people are getting

955
01:45:28,160 --> 01:45:34,320
concerned about agi you know in practice do these terms matter not too much what matters is how

956
01:45:34,320 --> 01:45:42,080
competent systems are basically all of these companies expect to build systems that outperform

957
01:45:42,080 --> 01:45:46,560
humans at most tasks definitely most tasks you can do behind a computer in the next

958
01:45:47,280 --> 01:45:56,080
two to five years um these matches the trends that we see in performance and compute growth this

959
01:45:57,280 --> 01:46:02,800
is very worrying uh these these are these are levels of competence uh at which we expect the

960
01:46:02,800 --> 01:46:08,880
systems to be out of our control unless we have various solutions so we just need to deal with

961
01:46:08,880 --> 01:46:17,040
that we can call it frontier ai agi proto agi proto superintelligence it's just terminology

962
01:46:17,040 --> 01:46:21,760
what matters is how powerful they are and how ready we are to deal with them we must avoid the

963
01:46:21,760 --> 01:46:27,440
serious discussions getting sidelined into semantics which is often very frustrating we're

964
01:46:27,440 --> 01:46:30,880
going to take three more questions we're going to go around the panel in the reverse order next

965
01:46:30,880 --> 01:46:37,440
time i'm going to take questions for people who haven't answered asked before so somebody in white

966
01:46:37,520 --> 01:46:43,040
about halfway down if you have asked a question before please don't put your hand up just now so

967
01:46:43,040 --> 01:46:50,960
we have more chance just there yes thank you three questions one from each thanks um let's say that

968
01:46:50,960 --> 01:46:57,280
in 20 years we somehow managed to get it right and humanity still exists um despite the development

969
01:46:57,280 --> 01:47:03,120
of these a di what do you think is one essential piece of regulation or development that has to

970
01:47:03,120 --> 01:47:10,320
have happened together it's a great question uh where else with the hands uh where are the

971
01:47:10,320 --> 01:47:20,640
microphones there's one about just on the other side thank you so you've spoken quite a lot about

972
01:47:20,640 --> 01:47:24,880
like what government should do what companies should do uh i'm interested in like what should

973
01:47:24,880 --> 01:47:29,920
ordinary people do like what can we be doing to get our voices heard in this you know should we

974
01:47:29,920 --> 01:47:33,920
be protesting i've edited international protest on the 21st is this thing we should be doing or is

975
01:47:33,920 --> 01:47:39,200
this a terrible idea it's another fine question is there a question from a woman it's about time we

976
01:47:39,200 --> 01:47:48,880
heard from the other gender another gender hands up yes somebody put your hand up to whoever it was

977
01:47:51,120 --> 01:47:58,640
yes there's one okay there's one there and we'll take you as well we'll take four right um as

978
01:47:58,720 --> 01:48:03,920
agi has been developed to be more human do you think it's possible to have forms of agi that

979
01:48:03,920 --> 01:48:08,800
don't have the inherent geopolitical biases that come with the data sets that we currently have

980
01:48:08,800 --> 01:48:14,400
and how do you think we go about developing regulations that aren't formed by human conscious

981
01:48:14,400 --> 01:48:23,760
bias okay and so if we can get the mic over here as well two one two three four five rows back

982
01:48:23,760 --> 01:48:34,320
just at the edge yes over there yeah i miscounted perhaps yeah four sorry quick question um so i

983
01:48:34,320 --> 01:48:39,760
actually work in the automotive industry and we have to certify vehicles and engines and it is

984
01:48:39,760 --> 01:48:46,480
an uphill battle um you can spend years just trying to get a windshield wiper right um or a

985
01:48:46,480 --> 01:48:52,960
temperature sensor right and i'm just curious um if you think that there would be an ability

986
01:48:52,960 --> 01:49:00,400
to take people who have regulated and certified products around global markets and how difficult

987
01:49:00,400 --> 01:49:06,320
that is and create a summit where that expertise could come together from different industries

988
01:49:06,320 --> 01:49:12,240
and we could roll up our sleeves and say okay this is how the structures go and we know what

989
01:49:12,240 --> 01:49:18,000
works we know what goes slow and try to accelerate that learning because i think that voice we have

990
01:49:18,000 --> 01:49:23,440
so much experience in the world right now um with that sleeves rolled up we know what it's

991
01:49:23,440 --> 01:49:28,800
like to sit in those test labs or send 30 000 pages of documents in with verification and

992
01:49:28,800 --> 01:49:33,360
validation data we know how to do requirements engineering requirements design requirements

993
01:49:33,360 --> 01:49:38,800
and i'm just wondering if um there's been any discussion of that to pull you know pull a summit

994
01:49:38,800 --> 01:49:45,120
together from people from heavily regulated industries four great questions first of all

995
01:49:45,120 --> 01:49:49,760
20 years later it succeeded how did we get it right what were the regulations that made the

996
01:49:49,760 --> 01:49:56,960
difference what should ordinary people be doing can we design AI that is free from some of the

997
01:49:56,960 --> 01:50:04,000
human biases the geopolitical biases that cause strife among humans and can we learn from the

998
01:50:04,000 --> 01:50:09,600
people who are professionally involved in doing regulations and certification in multiple industry

999
01:50:09,680 --> 01:50:16,560
rather than just to being naive in our own applications so Andrea first yeah maybe i will

1000
01:50:16,560 --> 01:50:21,680
answer the question about can we learn about highly regulated industries definitely i think there is a

1001
01:50:21,680 --> 01:50:29,680
big kind of problem of uh arrogance in AI or like willful arrogance of just thinking that

1002
01:50:29,680 --> 01:50:35,360
this sector should be special and people should be absolutely free to do any experiments they want

1003
01:50:35,360 --> 01:50:41,200
all the time use you know as much computing power as they want try the worst possible applications

1004
01:50:41,200 --> 01:50:46,000
all the time fully open source on the internet and nobody can complain like very often people in

1005
01:50:46,000 --> 01:50:50,880
the eye sector get very very angry when somebody tells them look well maybe what you just did

1006
01:50:52,080 --> 01:50:57,520
should be regulated and industries we don't do it like that like with drugs we don't just let

1007
01:50:59,840 --> 01:51:04,480
pharmaceutical companies just release and test the drugs on billions of people

1008
01:51:05,040 --> 01:51:09,440
and have their CEOs say oh there's a 20 percent chance you will die if you take this drug but you

1009
01:51:09,440 --> 01:51:14,400
know don't it's okay like if it happens you can let us know and then we'll we'll stop maybe right

1010
01:51:14,400 --> 01:51:20,400
so we can totally learn from that it would be great to learn from that there is one challenge

1011
01:51:20,400 --> 01:51:26,480
which is that we don't understand current systems that well so it makes things like

1012
01:51:27,440 --> 01:51:32,560
auditing them and evaluating them quite tricky because we simply don't know how they work internally

1013
01:51:32,560 --> 01:51:37,680
as well but we can do many other things and we can definitely learn from highly regulated

1014
01:51:37,680 --> 01:51:43,840
industries and definitely given the risks admitted by the companies themselves at the frontier the

1015
01:51:43,840 --> 01:51:50,880
approach should be highly regulated industry not so it is different but not completely different

1016
01:51:50,880 --> 01:51:58,560
and we can indeed learn tom closing words from you well i'll take the the question about what

1017
01:51:58,640 --> 01:52:04,320
ordinary people should do and i have two immediate thoughts one is that it's very important to keep

1018
01:52:04,320 --> 01:52:13,520
this issue apolitical the other is that lawmakers need a sense of legitimacy i think in order to

1019
01:52:14,400 --> 01:52:21,520
come up with regulation and to bring it in through acts and bills and so on a good example of when

1020
01:52:21,520 --> 01:52:26,800
this happened a bit too slowly was it the outset of covid and when it was a fringe issue there were

1021
01:52:26,800 --> 01:52:32,240
no enough rules then the public got involved and suddenly the rules arrived a little too late but

1022
01:52:32,240 --> 01:52:39,680
they did arrive and how can ordinary people achieve this i think ordinary people i i don't have a

1023
01:52:39,680 --> 01:52:46,000
theory of protest so i won't comment on that but i think it's important that we all keep this in

1024
01:52:46,000 --> 01:52:50,080
the public conversation i suppose what my answer is really tending towards is you should all read

1025
01:52:50,080 --> 01:52:57,280
lots of journalism about ai click on my articles thanks alessandra any of these questions catch

1026
01:52:57,280 --> 01:53:02,800
your attention i think um your question in the orange sweater there is is um is is definitely

1027
01:53:02,800 --> 01:53:09,760
where we're headed i mean there's got to be some kind of um system that resembles either the car

1028
01:53:09,760 --> 01:53:17,920
industry or fda and um the way that we certify um our you know products generally speaking

1029
01:53:17,920 --> 01:53:25,360
and i i just don't know how we get from from that to something that is very difficult to trace

1030
01:53:25,360 --> 01:53:31,680
and to to monitor as as ai but i would say to the gentleman's question in the white shirt there if

1031
01:53:31,680 --> 01:53:39,360
we're looking 20 years down the road and we say that's really great in the uk november 2023 we

1032
01:53:39,360 --> 01:53:46,320
we were able to put in place regulation that somehow created traceability um so we could we

1033
01:53:46,320 --> 01:53:52,240
could work out sort of where the where they were where systems were running out of control or

1034
01:53:52,240 --> 01:53:58,000
landing in the hands of bad actors that would be a huge success i think that the reality is a bit

1035
01:53:58,000 --> 01:54:02,400
different and that is that it probably is going to resemble a bit more the world in which cyber

1036
01:54:02,400 --> 01:54:10,160
security um flourishes and that means you're constantly trying to create um a dam system or a

1037
01:54:10,800 --> 01:54:18,080
deflection of all sort of incoming um activities that are not great so i know that none of these

1038
01:54:18,080 --> 01:54:23,840
are perfect analogies but i think it is in in that universe we're probably going to be operating in

1039
01:54:23,840 --> 01:54:31,840
for a while thanks final words either sure so i would love to touch on two questions very briefly

1040
01:54:31,840 --> 01:54:37,200
one of them um being i think your question in the white shirt what uh policies will bring us to the

1041
01:54:37,200 --> 01:54:42,480
safe world in 20 years and i think um a policy that was mentioned today as well already but that

1042
01:54:42,480 --> 01:54:48,320
i want to touch on again is um strict liability regimes just simply to kind of shift the incentive

1043
01:54:48,320 --> 01:54:53,520
systems um incentive structures that drive private companies to take certain actions that are not in

1044
01:54:53,520 --> 01:55:01,920
the interest of um the wider general public so i think there we can um really um shift shift the

1045
01:55:01,920 --> 01:55:07,360
incentive structure to move companies to take um maybe different paths forward and then what can

1046
01:55:07,360 --> 01:55:12,240
the the average person the general public do i would completely agree with tom i think um one

1047
01:55:12,240 --> 01:55:17,840
thing that that really would help is to for lack of a better expression to just make noise just make

1048
01:55:17,840 --> 01:55:23,120
sure that this topic is um talked about publicly you can do this in different ways you can write

1049
01:55:23,120 --> 01:55:28,720
to your local newspaper you can make a protest if that's up your alley you can write to your

1050
01:55:28,720 --> 01:55:34,320
mp or your congressman or wherever you live um and again create that legitimacy for people

1051
01:55:34,320 --> 01:55:40,000
to actually act on the problem because to many people it does sound very much like sci-fi and

1052
01:55:40,000 --> 01:55:44,480
policy makers are not going to take action and newspapers are not going to continue to report

1053
01:55:44,480 --> 01:55:48,720
about an issue they feel like it doesn't have traction and isn't taken seriously by the general

1054
01:55:48,720 --> 01:55:54,560
public the other thing the general public can do is we can educate ourselves and then we can share

1055
01:55:55,040 --> 01:56:00,080
information we have found to be most persuasive ourselves because there's a wide variety of

1056
01:56:00,080 --> 01:56:05,280
books a wide variety of youtube channels a wide variety of blogs and some of them are

1057
01:56:05,280 --> 01:56:09,840
better than others so let's share what we have found to be the really best ones

1058
01:56:10,640 --> 01:56:14,640
auto before i pass to jan maybe i'll ask you to get ready to come up on the stage because

1059
01:56:14,640 --> 01:56:20,960
you're going to give some closing remarks but jan what's your answers to what you've heard uh so

1060
01:56:20,960 --> 01:56:26,560
yeah just to just kind of underline the what ordinary people could do uh is just kind of

1061
01:56:26,560 --> 01:56:31,680
keep this topic alive like one of the things that i'm very proud of uh that came out of the

1062
01:56:31,680 --> 01:56:37,920
future five six months post letter uh was uh kind of framed by uh european commissioner

1063
01:56:37,920 --> 01:56:43,840
margaret bestiger when she said that like one thing that this letter has done is you're gonna

1064
01:56:43,840 --> 01:56:49,760
like communicate to the regulators that these concerns are much more widespread among people

1065
01:56:49,760 --> 01:56:56,160
than among regulators so i think this potential difference should be continually kind of maintained

1066
01:56:57,360 --> 01:57:04,080
so and when it comes to kind of bringing in kind of expertise from people from like regulated

1067
01:57:04,080 --> 01:57:10,480
industries i think it's super valuable i was on the on the board or like on the european high

1068
01:57:10,480 --> 01:57:14,880
level expert group at the european commission and there was like every once in a while there was

1069
01:57:14,880 --> 01:57:18,640
like why are we inventing the wheel like that we already have like lots of regulations should

1070
01:57:18,640 --> 01:57:23,680
we just apply this and i was like yes however there's like one big problem uh the problem is p

1071
01:57:24,400 --> 01:57:32,400
in chat gpt gpt stands for generative pre-trained transformer the pre-training is something that

1072
01:57:32,400 --> 01:57:42,400
you do before you actually train so the current the the out of nasty secret of ai uh field is

1073
01:57:42,480 --> 01:57:48,400
the ai's are not built they are grown the way you you you build the frontier model build the

1074
01:57:48,400 --> 01:57:54,880
frontier model is you take like two pages of code you put them in tens of thousands of

1075
01:57:54,880 --> 01:58:02,320
graphics cards and let them hum for months and then you're gonna open up the hood and see like

1076
01:58:02,320 --> 01:58:08,560
what creature brings out and what you can you can do with this creature so it's i think the

1077
01:58:08,560 --> 01:58:16,320
regulate the industry the capacity to regulate things uh and kind of deal with various liability

1078
01:58:16,320 --> 01:58:21,920
constraints etc they apply to what happens after what's once this creature has been kind of tamed

1079
01:58:22,640 --> 01:58:29,840
and that's what what uh fine tuning and reinforcement learning from human feedback etc is doing

1080
01:58:29,840 --> 01:58:35,280
and then productized then how do you deal with with these issues but uh is this where we need

1081
01:58:35,280 --> 01:58:41,040
the competence of of like other other industries but like how can avoid the system not escaping

1082
01:58:41,040 --> 01:58:47,440
during training run this is this is like a complete novel issue for this species and we need to need

1083
01:58:47,440 --> 01:58:53,360
some other approaches like just banning those training runs that's great we'll thank the panel

1084
01:58:53,360 --> 01:58:59,680
in a minute i asked the panel to stay here because who's going to wind up the evening is Otto Barton

1085
01:59:00,480 --> 01:59:08,000
Otto is the executive director of the ERO the existential risks observatory

1086
01:59:08,000 --> 01:59:13,760
which along with conjecture has designed and organized and sponsored this whole evening

1087
01:59:13,760 --> 01:59:19,440
Otto's got a few closing remarks before those of us who are still here can have a quick drink

1088
01:59:19,440 --> 01:59:24,960
and continue the discussion informally up to 10 o'clock by which time we must be out of the building

1089
01:59:24,960 --> 01:59:28,240
Otto

1090
01:59:38,960 --> 01:59:45,520
all right uh thanks david um a few closing remarks before we go to the drinks which is

1091
01:59:45,520 --> 01:59:51,840
five minutes so you should be able to uh keep with me um so we're talking tonight about human

1092
01:59:51,920 --> 01:59:56,960
extinction because of AI and what to do about this um and i think what to do about this there

1093
01:59:56,960 --> 02:00:00,960
was also a great question from the audience what can we do about this this is exactly the question

1094
02:00:00,960 --> 02:00:07,200
that i asked myself a few years ago um but it's not trivial and it's it's pretty difficult actually

1095
02:00:07,200 --> 02:00:13,440
what is not positive what could you do develop AI yourself try to do it safely such as uh open

1096
02:00:13,440 --> 02:00:20,720
AI deep mind and anthropic are doing will this increase safety some say so uh work on AI alignment

1097
02:00:20,720 --> 02:00:25,440
for example interpretability where we've seen great breakthroughs actually last week uh it could

1098
02:00:25,440 --> 02:00:30,480
be a good option but increasing knowledge of how AI works could also speed up its development so this

1099
02:00:30,480 --> 02:00:37,520
brings risks as well uh one could campaign for regulations such as an AI pause we support this

1100
02:00:37,520 --> 02:00:43,200
but this also has its downsides so i think it's pretty difficult to tell what one should do to

1101
02:00:43,200 --> 02:00:48,960
reduce human extinction risk by AI but when i started reading into this i was only really

1102
02:00:48,960 --> 02:00:54,080
convinced about one thing and that is that you cannot put humanity at risk without telling us

1103
02:00:54,880 --> 02:01:00,400
so you cannot have a dozen tech executives embarking on the singularity without informing anyone

1104
02:01:00,400 --> 02:01:06,400
else and you cannot have a hundred people at a summit which is what's happening now decide what

1105
02:01:06,400 --> 02:01:11,440
should be built and what should not be built and i think you cannot let a tiny amount of people

1106
02:01:11,440 --> 02:01:18,320
also decide how high extinction risk should be for the rest of us so the only thing that i'm

1107
02:01:18,320 --> 02:01:24,320
really convinced of is that we should be informed about this topic and that's also why i'm so happy

1108
02:01:24,320 --> 02:01:31,760
that events such as this one are taking place um we're happy i'm happy that we're together not just

1109
02:01:31,760 --> 02:01:36,080
with in crowd people some of you are and it's great but also with some people who may who may have

1110
02:01:36,080 --> 02:01:41,920
never heard of existential risk before and also a journalist who can inform a much wider audience

1111
02:01:41,920 --> 02:01:47,200
about existential risk also with a member of parliament someone with a job to openly discuss

1112
02:01:47,200 --> 02:01:52,560
difficult problems so i think this is all very encouraging and it's helping to normalize an open

1113
02:01:52,560 --> 02:01:58,960
debate about the topic of human extinction by artificial intelligence the 31st of october at

1114
02:01:58,960 --> 02:02:04,560
two o'clock we'll have our next event with professor steward russell it's just outside the ai safety

1115
02:02:04,560 --> 02:02:09,120
summit in blashley park in the old assembly hall where the code breakers used to have their

1116
02:02:09,120 --> 02:02:14,800
festivities after their important work so our event at blashley park the day before the summit

1117
02:02:14,800 --> 02:02:20,240
may not resemble a festivity but in a sense i think it is because we're celebrating that we're

1118
02:02:20,240 --> 02:02:25,840
all being hurt there we're celebrating that we can all be part of a democratic conversation about

1119
02:02:25,840 --> 02:02:31,200
what the most important technology of the century should and should not be able to do we can talk

1120
02:02:31,200 --> 02:02:36,080
about risks to humanity we find acceptable and what we intend to do about risks that are too high

1121
02:02:37,440 --> 02:02:42,080
and as the existential risk observatory together with conjecture we invite everyone to be part

1122
02:02:42,080 --> 02:02:48,000
of this conversation so there's much to be unsure of in this field but if there's one thing that i

1123
02:02:48,000 --> 02:02:53,120
am sure of it's that the most important conversation in this century which i think this is has to be

1124
02:02:53,120 --> 02:03:00,560
a democratic one so with that i would like to invite you to scan the qr code on the left

1125
02:03:01,280 --> 02:03:08,080
if this is working right yes to join us in blashley this is containing the url where you

1126
02:03:08,080 --> 02:03:13,200
can enroll to the blashley park event if you're interested then definitely pass by

1127
02:03:15,520 --> 02:03:20,960
there's same qr code is also on the flyer on your chair and beyond blashley i think this

1128
02:03:20,960 --> 02:03:28,320
conversation will not stop so there will be more summits according to my timeline about maybe 18

1129
02:03:28,320 --> 02:03:36,880
roughly so we will organize more events probably publish more about ai do more research and inform

1130
02:03:36,880 --> 02:03:42,240
governments as well as we can if you want to follow us or support us the existential risk

1131
02:03:42,240 --> 02:03:47,040
observatory in that work then scanning your r-codes on the right there's much that you can do to

1132
02:03:47,040 --> 02:03:52,960
help us um and with that i would like to close this evening and once again thanks to all our great

1133
02:03:52,960 --> 02:03:59,200
speakers so that's uh romeo polsky cornelly sir robert buckland yantalin andrea milte alexandra

1134
02:03:59,200 --> 02:04:11,520
mosefisa day eva birans and some are give them a round of applause

1135
02:04:17,120 --> 02:04:22,960
and i would also very much like to thank david wood uh should see them conor xio dis

1136
02:04:22,960 --> 02:04:26,720
ribbon dealer man and everyone at conway hall will also make this evening possible thank you very

1137
02:04:26,720 --> 02:04:40,320
much and then i would like to hopefully see you in blashley and in any case you are the drink

1138
02:04:40,320 --> 02:04:53,680
right now thank you thanks everybody

1139
02:05:10,320 --> 02:05:11,220
you

1140
02:05:40,320 --> 02:05:41,220
you

