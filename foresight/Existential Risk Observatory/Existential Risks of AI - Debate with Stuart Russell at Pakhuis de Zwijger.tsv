start	end	text
0	10000	Welkom, everybody.
10000	18000	Great to have you all here in a fully packed packhuis de Zwijger.
18000	20000	Mijn naam is Maarten Gehem.
20000	27000	I'm director of the argumentation factory and I have the honour of hosting this evening on existential risks.
28000	37000	De late Stefan Hawken once said success in creating AI could be the biggest event in the history of our civilization.
37000	42000	But it could also be the last unless we learn to avoid the risks.
42000	45000	And that's precisely the topic of today.
45000	50000	And we're going to talk about that with none other than professor Stuart Russell.
50000	56000	I'll properly introduce him later, but first I'll hand over the floor to Otto Barton,
56000	59000	who is the director of the existential risk observatory,
59000	62000	and who is the instigator of this evening.
62000	64000	Otto, come over.
64000	69000	After Otto, Russell will give a talk, then we'll have a Q&A,
69000	73000	and then afterwards we'll have a panel with five distinguished panelists.
73000	75000	We're sitting over there, I'll also introduce you later.
75000	78000	But now, without further ado, Otto.
78000	85000	APPLAUS
85000	90000	Dank je wel, en thank you very much, Maarten, for the introduction.
90000	93000	Ja, I'm super happy that you're all here.
93000	95000	Indeed, my name is Otto Barton.
95000	98000	I'm the founder and the director of the existential risk observatory.
98000	103000	We're an organization aimed at reducing existential risk by informing the public debates.
103000	108000	I'm going to talk a little bit for a few minutes about what existential risk is
108000	110000	and what our organization is doing.
112000	116000	All right, so next slide, please.
116000	117000	Thank you.
117000	120000	So existential risks, what are they?
120000	127000	Basically, as humanity, we've had about 300,000 years now already on this earth,
127000	129000	and we have maybe about five billion to go,
129000	132000	so an enormous amount before the sun explodes.
132000	138000	So the huge majority of our time is still ahead of us,
138000	141000	and an existential risk is something that can threaten that.
141000	147000	So basically, the definition is a risk that threatens the destruction of humanity's long-term potential.
147000	151000	It has been defined by Toby Ork from the Future of Humanity Institutes
151000	154000	and his colleagues in this way.
154000	157000	So this could be in a few ways.
157000	160000	Of course, human extinction is a permanent state.
160000	166000	So human extinction is one way in which we cannot have a future left anymore.
166000	169000	So these five billion years, there won't be any value in that.
169000	173000	An unrecoverable collapse or dystopian log-in are two other ways
173000	178000	in which we could, which are contained in existential risk definition.
178000	183000	So on the graph to the right, you see a rough estimate by Toby Ork,
183000	186000	this researcher from the Future of Humanity Institute,
186000	191000	on what causes could be for existential risks.
191000	193000	So there are natural causes.
193000	196000	There might be an asteroid strike, there might be a super volcano,
196000	199000	but these are tiny and very well-known.
199000	201000	So not the most interesting ones.
201000	205000	To the left of that, you see a nuclear war and climate change,
205000	208000	which are already somewhat bigger.
208000	214000	But you can see that these are still fairly small compared to other existential risks.
214000	218000	That climate change has a small chance of leading to human extinction.
218000	220000	It doesn't mean that it's not a big problem.
220000	224000	Of course, the chance that climate change will occur is 100% basically.
224000	226000	And it is a very big issue.
226000	229000	However, the chance that it leads to complete human extinction is relatively small,
229000	231000	which is why you see a small bar here.
231000	235000	Nuclear war, perhaps a little bit of a similar story,
235000	239000	the chance that it occurs in the next hundred years is not that tiny.
239000	242000	But the chance that it leads to human extinction is fairly small.
242000	247000	To the left, you see even bigger chances of human extinction
247000	250000	or the other existential risk categories.
250000	254000	These are, for example, the man-made pandemics.
254000	259000	The pandemics bar here is actually for man-made pandemics.
259000	263000	A natural pandemic is also very unlikely to lead to human extinction.
263000	267000	But a man-made pandemic with the biotechnology that we have developed right now
267000	270000	and that we are still developing and democratizing.
270000	275000	The chance that this could lead to human extinction in the next hundred years is non-negligible.
275000	280000	To be orged and most of the other existential risk researchers
280000	283000	think that it's unaligned AI, so artificial intelligence
283000	287000	that has human level or even beyond human level, superhuman level.
287000	292000	But it's unaligned, so it has different values than ours.
292000	296000	This could be a relatively large chance of human extinction.
296000	300000	We're going to talk more about it later, but I'll just leave it here for now.
300000	305000	What else do we see, the total existential risk in the next hundred years
305000	309000	is about a one in six estimate.
309000	311000	There's a lot to be said about these estimates,
311000	315000	but you can still draw a couple of robust conclusions, I think, from them.
315000	318000	That a very likely source is new technology.
318000	322000	And also that technology is man-made, so risk could be reduced in principle.
327000	329000	Next slide, please.
329000	333000	Solution directions for AI existential risk.
333000	336000	These kind of also carry over for other technologies,
336000	339000	but very broadly you could say
339000	342000	if you don't want something to go very wrong with technology,
342000	345000	you can either develop it safely or you cannot develop it.
345000	350000	So basically for AI, this is built AGI safely or AI safety.
350000	354000	So this is done by people who try to focus on AI alignment,
354000	357000	trying to make AGI align to our values.
357000	360000	We think, as an existential risk observatory,
360000	363000	that's an important line of research and it should be scaled up.
363000	366000	But on the other hand, it hasn't worked so far.
366000	369000	People are already working on this for perhaps a few decades.
369000	373000	And so far the consensus is that AI alignment,
373000	377000	more or less the consensus is that AI alignment hasn't been successful yet.
377000	380000	So another option could be to not build AGI
380000	383000	and we think there might be some kind of regulation necessary for that.
383000	386000	So this could be a software regulation, a data regulation
386000	389000	or perhaps a hardware regulation.
389000	392000	And we think these are all options that should be investigated.
392000	396000	But we do think that regulation, whatever is the form it takes,
396000	401000	will require widespread awareness and global cooperation.
401000	405000	So for that, our solution is to inform the societal debate.
405000	407000	So as an existential risk observatory,
407000	410000	a small non-profit organization based in Amsterdam,
410000	415000	we are focusing on informing the society about existential risk.
415000	418000	So we do that by publishing articles in traditional media,
418000	421000	for example in Time Magazine a few weeks ago
421000	424000	and by organizing events such as this debate.
424000	427000	And we also provide input to policy makers
427000	431000	and I think it's a really nice sign that emotion was accepted
431000	434000	by Dutch parliament a few weeks ago
434000	438000	that is calling for more AI safety research in the Netherlands.
440000	445000	So with that, I'm just going to end this small introduction talk
445000	449000	and we're now going to watch a documentary
449000	452000	which is already giving you a little bit of a flavor
452000	454000	of the next speaker Stuart Russell.
454000	457000	And I hope that you enjoyed a few minutes of documentary
457000	460000	and I wish you a great rest of the evening.
460000	462000	Thank you very much.
471000	474000	Everything we have is a result of our intelligence.
474000	477000	It's not the result of our big scary teeth
477000	480000	or our large claws or our enormous muscles.
480000	483000	It's because we're actually relatively intelligent.
483000	487000	And among my generation, we're all having what we call holy cow
487000	489000	or holy something else moments
489000	494000	because we see that the technology is accelerating faster than we expected.
494000	497000	I remember sitting around the table there
497000	500000	with some of the best and the smartest minds in the world
500000	502000	and what really struck me was
502000	506000	maybe the human brain is not able to fully grasp
506000	510000	the complexity of the world that we're confronted with.
510000	512000	As it's currently constructed,
512000	515000	the road that AI is following heads off a cliff
515000	518000	and we need to change the direction that we're going
518000	521000	so that we don't take the human race off the cliff.
521000	527000	This is from the Deep Mind Reinforcement Learning System.
527000	530000	Basically wakes up like a newborn baby
530000	533000	and is shown the screen of an Atari video game
533000	536000	and then has to learn to play the video game.
536000	542000	It knows nothing about objects, about motion, about time.
542000	547000	It only knows that there's an image on the screen and there's a score.
547000	551000	So if your baby woke up the day it was born
551000	556000	and by late afternoon was playing 40 different Atari video games
556000	558000	at a superhuman level,
558000	560000	you would be terrified.
560000	564000	You would say my baby is possessed, send it back.
564000	567000	The Deep Mind System can win at any game.
567000	572000	It can already beat all the original Atari games.
572000	574000	It is superhuman.
575000	576000	It is superhuman.
576000	579000	It plays the games at super speed in less than a minute.
584000	586000	Deep Mind turned to another challenge
586000	588000	and the challenge was the game of Go
588000	590000	which people have generally argued
590000	592000	has been beyond the power of computers
592000	595000	to play with the best human Go players.
595000	598000	First they challenged the European Go Champion.
600000	603000	Then they challenged a Korean Go Champion.
605000	607000	En they were able to win both times
607000	609000	in a kind of striking fashion.
610000	612000	You were reading articles in New York Times years ago
612000	615000	talking about how Go would take a hundred years
615000	617000	for us to solve.
617000	620000	People said, well, you know, but that's still just a board.
620000	622000	Poker is an art.
622000	623000	Poker involves reading people.
623000	625000	Poker involves lying, bluffing.
625000	626000	It's not an exact thing.
626000	628000	That will never be a computer.
628000	629000	You can't do that.
629000	631000	They took the best poker players in the world
631000	634000	en took seven days for the computer
634000	637000	to start demolishing the humans.
637000	639000	So it's the best poker player in the world.
639000	640000	It's the best Go player in the world.
640000	644000	And the pattern here is that AI might take a little while
644000	647000	to wrap its tentacles around a new skill.
647000	652000	But when it does, when it gets it, it is unstoppable.
652000	659000	MUZIEK
659000	662000	DeepMind's AI has administrator-level access
662000	664000	to Google's servers
664000	667000	to optimize energy usage at the data centers.
667000	671000	However, this could be an unintentional trojan horse.
671000	674000	DeepMind has to have complete control of the data centers.
674000	676000	So with a little software update,
676000	679000	that AI could take complete control of the whole Google system,
679000	681000	which means they can do anything.
681000	683000	They can look at all your data and do anything.
683000	688000	MUZIEK
688000	690000	We were rapidly headed towards digital superintelligence
690000	692000	that far exceeds any human.
692000	694000	I think it's very obvious.
694000	697000	The problem is we're not going to suddenly hit human-level intelligence
697000	700000	and say, OK, let's stop research.
700000	702000	It's going to go beyond human-level intelligence
702000	704000	into what's called superintelligence
704000	706000	and that's anything smarter than us.
706000	710000	AI at the superhuman level, if we succeed with that,
710000	714000	is by far the most powerful invention we've ever made
714000	717000	and the last invention we ever have to make.
717000	720000	And if we create AI that's smarter than us,
720000	722000	we have to be open to the possibility
722000	725000	that we might actually lose control of them.
725000	728000	MUZIEK
728000	731000	Let's say you give it some objective like curing cancer
731000	734000	and then you discover that the way it chooses to go about that
734000	739000	is actually in conflict with a lot of other things you care about.
739000	743000	AI doesn't have to be able to destroy humanity.
743000	747000	If AI has a goal and humanity just happens to be in the way,
747000	750000	it will destroy humanity as a matter of course.
750000	752000	Without even thinking about it, no hard feelings.
752000	754000	It's just like if we're building a road
754000	756000	and an ant hill happens to be in the way,
756000	760000	we don't hate ants, we're just building a road
760000	762000	and so goodbye, Ant Hill.
762000	765000	MUZIEK
769000	773000	OK, if you weren't scared already.
773000	779000	Make sure humanity doesn't run off a cliff.
779000	781000	Stuart Russell said.
781000	784000	So who better to tell us how not to run off a cliff
784000	786000	than Professor Russell himself.
786000	788000	And that's precisely what we're going to hear.
788000	794000	Professor Russell is one of the leading experts
794000	797000	in AI research and AI safety research.
797000	800000	He's based at the University of California, Berkeley.
800000	806000	He's one of the writers, co-author of the standard textbook in AI research.
806000	809000	AI, a modern approach.
809000	813000	And recently he wrote a magnificent book called Human Alignment.
813000	815000	I don't know who read the book already.
815000	817000	Let me see some hands here.
817000	819000	OK, all right.
819000	821000	Well, it's well worth the effort.
821000	825000	And he'll probably tell you why in the next 20 minutes.
825000	830000	Professor Russell is beamed to us all over the world,
830000	834000	from all across the world, from California where he's based right now.
834000	838000	So we're going to see him on the screen in a minute or two.
838000	842000	And afterwards there's ample room for questions and answers.
842000	847000	So we'll have some room here for a discussion with Mr. Russell himself.
847000	849000	And there he is.
849000	854000	Professor Russell, the floor is yours.
854000	857000	Hey there, thank you very much.
857000	860000	So I should just make a slight correction.
860000	863000	I'm not in California, I'm actually at MIT.
863000	869000	But I'm on my way home to California later on this evening.
869000	873000	So I think the little movie that you just saw
873000	877000	actually brings up a lot of important points.
877000	880000	So I don't have to repeat them.
880000	884000	But I will give you a short presentation,
884000	887000	which in some ways brings it up to date.
887000	891000	So let's say a little bit about what we're doing to help
891000	895000	and about the current situation.
895000	898000	So together everyone on the same page.
898000	900000	What is AI?
900000	902000	It's not a particular technology.
902000	907000	It's a task just like the task of physics is to understand the universe.
907000	911000	The task of AI is to make intelligent machines.
911000	914000	And then the question is, well, what does that mean?
914000	916000	What does it mean for a machine to be intelligent?
916000	919000	And for most of the history of AI, it's meant the following.
919000	922000	Machines are intelligent to the extent that their actions
922000	926000	can be expected to achieve their objectives.
926000	929000	And this is so pervasive, I'll call it the standard model.
929000	934000	And many forms of AI, problem solving, planning, reinforcement learning,
934000	937000	or conform to the standard model,
937000	940000	as well as many other disciplines like control theory
940000	942000	and operations research and economics.
942000	948000	You create optimizing machinery and then you specify some objective.
948000	953000	You put that into the machinery and then it becomes the objective of the machine.
953000	957000	And then it finds ways to fulfill that objective.
957000	960000	It's a very natural way to go about doing things.
960000	963000	Later on, I'll argue that it's completely wrong.
963000	967000	But for now, take that as the standard model of AI.
967000	973000	And since the beginning, we've been looking at what we might call general purpose AI.
973000	980000	So not just an AI designed to achieve some specific objective,
980000	985000	but actually one that's capable of achieving more or less any objective that we might give it.
985000	992000	And learning to do that very quickly at a level that exceeds human capabilities
992000	996000	eventually in every dimension.
996000	998000	So that's the goal.
998000	1004000	And rather than be accused of always talking about doom,
1004000	1008000	I'll begin by talking about the upside.
1008000	1013000	And it's really the upside that explains why the field exists in the first place
1013000	1017000	and why people are investing lots of money in it
1017000	1019000	and why lots of smart people are working on it.
1020000	1023000	Because the potential upside is really enormous.
1023000	1025000	For example, if you had general purpose AI,
1025000	1031000	then you could do by definition what humans already know how to do,
1031000	1034000	which is to deliver, among other things,
1034000	1040000	to deliver a good standard of living to maybe hundreds of millions
1040000	1042000	or maybe close to a billion people on Earth
1042000	1046000	have what we might call a good standard of living.
1046000	1053000	En we could deliver it actually on much greater scale at much lower cost
1053000	1057000	because the cost involved in delivering a standard of living
1057000	1061000	is the expensive time of other human beings.
1061000	1066000	So if we have general purpose AI, we could, for example,
1066000	1071000	use it to give everyone on Earth that same respectable standard of living
1071000	1075000	that we might see in some developed countries.
1076000	1080000	En if you calculate the sort of economic value of that,
1080000	1083000	it's about a tenfold increase in GDP
1083000	1086000	and that converts to what economists call the net present value.
1086000	1088000	So that's sort of what's the cash equivalent
1088000	1091000	of having that increased income stream.
1091000	1095000	So it comes to about $13.5 quadrillion.
1095000	1099000	So that's a lower bound, a low ball estimate
1099000	1104000	on the cash value of general purpose AI as a technology.
1104000	1107000	We could of course have many more things besides that.
1107000	1110000	I think we could have much better,
1110000	1114000	more individualized ongoing healthcare.
1114000	1117000	We could have very personalized
1117000	1121000	and very, very effective education for every child on Earth.
1121000	1125000	We could speed up the rate of scientific progress
1125000	1128000	and perhaps many other things.
1128000	1132000	I used to have advances in politics on that slide,
1132000	1135000	but I took it off for obvious reasons.
1135000	1139000	So now the question is, well, where are we?
1139000	1143000	A lot of people seem to be saying that we're already there,
1143000	1147000	that we've already created general purpose AI.
1147000	1151000	And I think this is not true.
1151000	1154000	I think there's something going on,
1154000	1158000	but we are still far away from general purpose AI.
1158000	1161000	And what's going on, of course, is large language models.
1161000	1166000	The chat GPT, GPT4, BARD, Lambda, Palm,
1166000	1171000	all these models are displaying very intriguing
1171000	1174000	and in some cases very impressive behaviors.
1174000	1178000	And I think they are probably a piece of the puzzle
1178000	1179000	of general purpose AI,
1179000	1182000	but they are not by themselves general purpose AI.
1182000	1184000	And at the moment, I would say,
1184000	1187000	we don't know what shape this puzzle piece is
1187000	1190000	and we don't know how to fit it into the puzzle.
1190000	1193000	We're not really sure what the other pieces are.
1193000	1197000	I think one of the things we're learning now
1197000	1199000	is that the pieces of this puzzle
1199000	1202000	are probably not the pieces that we thought
1202000	1207000	made up the puzzle maybe 15 or 20 years ago.
1207000	1210000	So just to illustrate a few reasons
1210000	1215000	why I don't think these systems are the solution,
1215000	1216000	they're not general purpose AI.
1216000	1219000	So here's a simple example from chat GPT
1219000	1222000	to me by my friend Prasad Tattapalli.
1222000	1223000	So the first question,
1223000	1225000	which is bigger an elephant or a cat,
1225000	1228000	and it answers an elephant is bigger than a cat.
1228000	1230000	So far so good.
1230000	1233000	Which is not bigger than the other, an elephant or a cat.
1233000	1235000	And it says neither an elephant nor a cat
1235000	1238000	is bigger than the other.
1238000	1241000	So these are two consecutive sentences
1241000	1243000	that he asked it.
1243000	1248000	And it seems clear from this that in a real sense
1248000	1251000	chat GPT doesn't know facts.
1251000	1254000	So when you ask a human a question,
1254000	1256000	at least our impression of what happens
1256000	1260000	is that we refer to an internal world model
1260000	1262000	that is self consistent,
1262000	1267000	that's composed of facts that we understand about the world.
1267000	1269000	And then we ask in a question
1269000	1271000	relative to that internal world model,
1271000	1273000	we find out what the answer is
1273000	1277000	and we express the answer in natural language.
1277000	1279000	In natural language as the answer to the question.
1279000	1281000	But that clearly can't be what's going on
1281000	1284000	in at least in this example
1284000	1287000	because you could not have an internal world model
1287000	1289000	that contradicted itself
1289000	1291000	in which the elephants are both bigger than cats
1291000	1293000	and not bigger than cats.
1293000	1295000	So in a real sense,
1295000	1297000	I think we could say that there's evidence
1297000	1299000	that these systems do not know things
1299000	1303000	in the way that word is usually used.
1303000	1305000	I also want to point out,
1305000	1309000	in the movie you just saw that several years ago
1309000	1313000	we defeated the best human go players.
1313000	1315000	In fact, when that happened
1315000	1318000	to the Chinese world champion in 2017
1318000	1320000	that was called China's Sputnik moment.
1320000	1324000	That event precipitated a total change
1324000	1327000	in Chinese government policy around AI
1327000	1330000	and the commitment of hundreds of billions of dollars
1330000	1332000	worth of investment.
1332000	1335000	The commitments to train hundreds of thousands
1335000	1339000	of AI researchers, et cetera, et cetera.
1339000	1346000	We decided to see how good the go programs really are.
1346000	1349000	We played one of our team members,
1349000	1353000	Kellan Pelrin, is a reasonably good amateur go player.
1353000	1356000	His rating is about 2,300.
1356000	1361000	On that scale, the human world champion is about 3,800.
1361000	1365000	The go programs are far ahead of human beings now.
1365000	1368000	In 2017, or 2016,
1368000	1370000	they were about the level of the human world champion,
1370000	1372000	so around 3,800.
1372000	1375000	Now they've reached around 5,200.
1375000	1379000	JBX Kata 005 is the name of the current number one
1379000	1382000	go playing program in the world.
1382000	1388000	Its rating is 1,400 points higher than any human player.
1388000	1393000	Kellan had been playing against this program
1393000	1396000	and had beaten it 14 times in a row
1396000	1400000	and then decided to give it a nine stone handicap.
1400000	1403000	That means that black, the computer,
1403000	1405000	starts with nine stones on the board,
1405000	1408000	as we're showing here, which is an enormous advantage.
1408000	1413000	This is the kind of handicap that you give to a small child
1413000	1417000	who's learning the game if you're a go teacher,
1417000	1421000	just so that the child feels they have a chance.
1421000	1425000	Now I'll show you what happens in the game.
1425000	1428000	Remember, the computer is black.
1428000	1430000	Kellan, the human, is white.
1430000	1433000	It doesn't really matter if you don't understand go.
1433000	1436000	Basically, you're trying to surround territory with your pieces
1436000	1439000	and to surround your opponent's pieces and capture them.
1439000	1443000	Notice what's happening in the bottom right corner of the board.
1443000	1446000	The white is making a little group.
1446000	1449000	It sort of has a kind of a figure 80 sort of shape.
1449000	1452000	And then black immediately starts to surround that group
1452000	1455000	in order to prevent it from capturing more territory.
1455000	1458000	And now white starts to surround the black group
1458000	1461000	so that this larger white circle is forming.
1461000	1463000	So it's kind of a circular sandwich.
1463000	1465000	There's a white piece in the middle
1465000	1467000	and there's a white thing around the outside
1467000	1470000	and it's sandwiching in that black group.
1470000	1472000	In the end, there's no attention
1472000	1474000	and then loses all of those pieces.
1474000	1480000	So what's going on here seems to be
1480000	1482000	that these super human go programs
1482000	1485000	actually have not correctly learned
1485000	1487000	what it means to be a group of stones,
1487000	1489000	what it means to be alive or dead
1489000	1492000	which are the most basic concepts in go.
1492000	1497000	And that allows Kellan, the human to defeat these programs
1497000	1500520	met alle voordelingsprogramma's, die worden geschreven door verschillende mensen
1500520	1505400	met verschillende trainingsregime's en verschillende netwerkstructuren en zo en zo.
1505400	1510440	Ze voelen allemaal in dezelfde manier, die is echt remarkably.
1510440	1514720	En ik denk dat het eigenlijk een consequentie is van de fact dat ze proberen te trainen
1514720	1521640	circuitten om concepten te representeren zoals connectiviteit en surroundering,
1521640	1526840	die eigenlijk niet mogelijk kunnen representeren correct via circuitten.
1526840	1531720	Je kunt alleen een soort van patchy, fragmentair, finite approximatie
1531720	1536840	te die concepten, maar met een generele programma, zoals Python,
1536840	1540840	het is heel makkelijk om die concepten correct te representeren.
1540840	1543560	Dus dit is een fundamentele limitering,
1543560	1552280	alstublieft als we het uitzenden, met diep leren als een manier te leren over de wereld.
1552360	1555960	Oké, dus ik denk, in mijn gevoel, dat we nog steeds een manier te gaan
1555960	1561080	voor generalpurpos A.I. en ik heb een aantal van de dingen
1561080	1563960	ik denk dat je hier gevoel hebt, waarschijnlijk de derde,
1563960	1569320	onze behoorlijkheid om niet alleen te bekijken, waarin de go-programma's
1569320	1571960	wel kunnen doen, zelfs als ze missen
1573320	1576840	over de kwaliteit van de positie die ze reachen,
1576920	1581560	zijn we zeker kunnen planen voor 50 of 60 of even 100 behoorlijkheid
1581560	1585640	naar de toekomst, maar mensen planen op veel levens van de extractie.
1585640	1591000	We planen over tijdscalingen van jaren en ook over tijdscalingen
1591000	1593400	van milliseconden en op elke tijdscaling in de tweede.
1594280	1599320	En als je op een phd maakt, bijvoorbeeld, dat gaat over een trillion
1599320	1606440	motocontrole acties, en niet alleen maar 50 motocontrole acties,
1606520	1611720	dus we kunnen in het universen, het heel complex universen,
1611720	1615320	door onze behoorlijkheid om op deze verschillende levens van de extractie te opereren.
1616040	1619880	En dat is iets dat nog wel duidelijk onderzoek in A.I.
1620840	1622920	Dus ik denk dat het nog wel gelijkgemaakt is,
1622920	1625560	omdat de manier van momentum, investering,
1627880	1632760	van geniën die dit gebouw wordt geplaatst, dat deze voorkomsten ergens gebeuren.
1632760	1634280	Het is gewoon heel hard te predicteren
1634920	1636040	wanneer ze gaan gebeuren.
1639000	1643000	En om een voorbeeld te geven van hoe hard het is om te predicteren
1643000	1646680	wanneer deze dingen gaan gebeuren, kunnen we in historie kijken
1646680	1650200	tot de laatste keer dat we een civilisatie-endende technologie ontvangen,
1650760	1652920	waarom het automatische energie was.
1653960	1659880	En we weten sinds 1905, en speciale relativiteit,
1659880	1663240	dat er een enorm amount van energie in atomen loopt.
1663320	1667080	En als je er tussen verschillende typen van atomen kan veranderen,
1667080	1668280	kan je dat energie veranderen.
1668840	1671560	Maar de fysieke ontdekking hier,
1671560	1673160	persoonlijke door Lord Rutherford,
1673160	1676600	de ledende nucleofysicist, geloofde dat dat mogelijk was.
1677320	1680840	Hij was gevraagd op een meet in september 11, 1933,
1680840	1683000	dacht je dat in 25 of 30 jaar tijd,
1683000	1686520	we zouden kunnen vinden een manier om deze energie te ontvangen.
1686520	1689320	En hij zei dat iemand die voor een verhaal van poder
1689320	1692120	in de transformatie van de atomen, een moeenscheid is.
1693880	1697400	En de volgende morning, Leo Zillard,
1697400	1699640	die een Hungarian fysiast was,
1699640	1701160	die van Hungary had afgesloten,
1702360	1704600	en was in London in de tijd,
1705320	1706760	reed dit in de newspaper,
1706760	1707720	en hij ging voor een reis,
1707720	1710200	en hij inventeerde de neutron-induust
1710200	1711240	nuclea-chain reactie,
1711880	1713480	die de solution is
1713480	1715880	om hoe je de energie van het atom op te leveren.
1716760	1718520	Dus hij ging van het mogelijkheden
1718520	1721480	tot de essentieel gevolgd in 16 uur.
1721800	1725320	Dus als ik zei dat het onpredictabel is,
1725320	1727240	het is onpredictabel
1727240	1730440	wanneer deze avond gaat gebeuren.
1730440	1732680	Ik denk dat er omdat er nogal wat we nodig zijn,
1734040	1736520	het is ongelijkbaar dat er allemaal in een goede gebeuren zijn,
1736520	1738280	dus we mogen een paar eerlijke oorlogen krijgen.
1739720	1741160	Dus over een eerlijke oorlogen,
1741160	1742680	dit is de titel van een paper
1742680	1744920	geschreven door een dozen
1744920	1748360	erg besteldere researchers in Microsoft.
1748360	1750920	Er zijn twee bezoekers van de Nationaal Kool,
1750920	1752200	de Nationaal Academies hier,
1753000	1754360	en andere bezoekers
1754360	1757160	die een heel signifieke contributie zijn
1757160	1758440	aan de theorie van machine-learning.
1759960	1761800	En ze hebben met GPT-4
1763240	1765160	de laatste systeem uit OpenAI gesproken.
1765160	1766600	Ze hebben het voor een aantal maanden
1767720	1768920	voordat het gebouwd was.
1769800	1772200	En ze hadden veel bezoekers
1772200	1773880	om het te bekijken hoe goed het was.
1774600	1775560	En hun conclusie,
1777000	1778200	zoals dit titel is,
1778200	1780680	is dat ze geloven dat GPT-4
1780680	1783000	overspraken van artificieel
1783000	1784360	en generale intelligenteel zijn.
1785320	1787400	Dus ze zeggen alstublieft
1787400	1790680	dat er een reale probleem
1791400	1792920	tegen AGI
1792920	1794760	gebeurt met dit systeem.
1797400	1798120	Oké, dus,
1798600	1799880	dus terug naar de vraag
1799880	1801800	over wat we vergelijden,
1802920	1803960	dit is Alan Turing,
1804520	1806280	die de founder van Computerscience is,
1807080	1809480	en in veel manier de founder van AI ook.
1810920	1812120	En in 1951,
1812120	1813880	hij was asked that question at a lecture.
1814760	1815720	What if we succeed?
1815720	1816680	And this is what he said.
1817240	1819400	It seems parable that once the machine
1819400	1821080	thinking method had started,
1821080	1823240	it would not take long to outstrip
1823240	1824280	our feeble powers.
1824840	1826440	At some stage, therefore,
1826440	1828040	we should have to expect the machines
1828040	1828840	to take control.
1831160	1831720	So that's it.
1831720	1834440	So he offers no mitigation,
1835160	1836680	no solution, no apology.
1837720	1840120	You almost get a sense of resignation
1840120	1842680	about this, about this prediction.
1843320	1844840	So why is it?
1845560	1847000	Where is this prediction coming from?
1847640	1849560	This idea that as you make AI
1850200	1851080	better and better,
1852360	1854360	things could end up getting worse
1854360	1855480	and worse as a result.
1857560	1860120	And I think underlying his prediction is,
1860120	1862040	I'm going to put it in a more positive way,
1862040	1863800	rather than a prediction, a question.
1864440	1866200	How do we retain power
1867480	1869640	over entities more powerful than us?
1870120	1870600	Forever.
1871800	1872840	That's the question
1872840	1875480	that I think he's asking himself
1875480	1878760	and he's failing to find an answer to it.
1878760	1880360	And so that's his prediction.
1884280	1886440	So I've spent the last 10 years also
1886440	1888840	trying to figure out an answer to this
1890200	1891480	that isn't, we can't.
1894040	1894760	And to do that,
1895400	1896920	I've been trying to understand
1896920	1898600	where things go wrong.
1898600	1900040	And I think they go wrong
1900040	1903400	because of a phenomenon called misalignment.
1904280	1906280	And that was described a little bit in the movie.
1906920	1908440	I think Elon Musk talked about it
1908440	1909640	and I talked about it a little bit.
1910200	1912920	This idea that systems that are pursuing
1912920	1915960	an objective, as in the Standard Model,
1915960	1917480	if that objective is not
1918920	1922920	the full, complete, correct description
1922920	1925240	of what the human race wants the future to be like,
1926200	1929880	then you are setting up a mismatch,
1929880	1932840	a misalignment between what we want the future to be like
1932840	1935240	and the objective that the machine is pursuing.
1935880	1938520	And we can see that happening already in social media
1938520	1940520	where the algorithms that choose
1940520	1944040	what billions of people read and watch every day
1945480	1951080	are simply designed to maximize a very local objective.
1951800	1954680	The number of clicks that they produce
1955400	1957080	over the lifetime of each user,
1957800	1959160	that's called click through,
1959160	1960360	or it could be engagement,
1960360	1962600	the amount of time that the user spends
1962600	1963640	engaging with the system.
1965800	1967320	And you might think, well, okay,
1967320	1969000	if I want to get the user to click,
1969000	1972200	I have to send things that the user likes.
1972760	1975320	And so the algorithm should be learning
1975320	1976120	what people want.
1977000	1979240	That sounds like pretty good.
1979240	1981960	But we very soon found out
1981960	1984120	that that wasn't the solution
1984120	1985240	that the algorithms found,
1985240	1988600	that we know that they amplify clickbait.
1989080	1991960	Clickbait by definition is articles
1991960	1993000	that you think you want,
1993000	1994440	but it turns out you don't want,
1995480	1997160	because the headline is misleading.
1998280	1999800	And they also create filter bubbles
1999800	2002280	because you stop seeing content
2003080	2004920	that is outside your comfort zone.
2004920	2009720	So these phenomena were observed very quickly,
2009720	2012040	but actually the real solution
2012040	2013800	that the algorithms are finding
2015000	2016680	is inevitable when you think about
2016680	2018360	the definition of the problem that they're given.
2020040	2022120	If you want to maximize
2022120	2024680	the long-term number of clicks from a user
2026520	2027880	and the way you do,
2027880	2028920	the way you can do that
2028920	2031240	is by choosing content to recommend to them,
2032200	2034920	then the solution is to choose content
2034920	2040440	that will change the user consistently over time
2040440	2042520	through perhaps thousands of little nudges,
2043320	2045240	change the user, modify people
2045240	2048680	to be more predictable in the content
2048680	2049800	that they will consume,
2049800	2051800	because the more predictable you are,
2051800	2055080	the higher the click rate the machine can generate.
2055960	2058680	So this is what the algorithms learn to do
2058680	2060120	and at least anecdotally,
2060120	2061720	we think that the consequence of that
2063240	2066520	is that it's tended to make people
2067080	2069480	more extreme versions of themselves.
2069480	2071720	So it's created polarization
2071720	2074120	where people who were towards the middle
2074120	2076360	end up at one extreme or another
2076360	2077640	because at the extremes,
2077640	2080520	their consumption is much more predictable.
2082520	2084120	And these are very simple algorithms.
2084120	2085720	They don't know that people exist
2085720	2086520	or have brains
2086520	2087880	or they don't understand the content
2087880	2089480	of any of these things
2089480	2090920	that they're sending to people.
2090920	2093240	So if they were better AI systems,
2093240	2095400	the outcome would be much worse
2096520	2099640	because they would be much more effective at manipulation.
2100280	2103240	And this turns out to be a fairly general property
2103240	2105320	of optimization systems
2106440	2108840	that when you have a misaligned objective,
2108840	2111400	the harder you optimize it,
2111400	2113160	the worse the outcome is going to be
2113720	2115320	relative to the true objectives.
2116200	2117800	And this was proved in a paper
2117800	2118840	by one of my students,
2120600	2123000	Dylan Hadfield Menel at Europe's in 2020.
2124920	2127720	So I think we have to then question
2127720	2130120	whether the problem comes
2130120	2132840	from the standard model of AI itself
2133400	2135560	because that's the model in which
2136680	2139160	systems are designed to pursue objectives
2139160	2140200	that we plug into them.
2141480	2142920	So this is the original definition
2142920	2144680	that I wrote for what do we mean by
2145560	2146040	AI?
2146040	2147960	What do we mean by intelligent machine?
2147960	2150520	And I think we actually need to get rid of that definition
2150520	2152840	and replace it with a different one.
2152840	2154920	We want machines that are beneficial,
2154920	2156360	not just intelligent.
2158120	2159320	And they are beneficial
2159320	2161000	if their actions can be expected
2161000	2163720	to achieve our objectives.
2163720	2166680	So this is specifically talking about us.
2166680	2169400	We want machines beneficial to us.
2170520	2172120	The aliens from Alpha Centauri
2172120	2174440	might want machines that are beneficial to them
2174520	2176360	en they can do their own kind of AI,
2176920	2178520	but we should do this kind of AI.
2179800	2182840	And this might seem like it's
2183400	2185160	impossible or certainly more difficult,
2185160	2187560	but it turns out that we can actually formulate this
2189080	2191160	in a fairly straightforward mathematical way
2191160	2194680	and we can produce systems that solve this problem.
2195960	2200600	And one easy way to think about this is what
2201960	2204120	what are we going to get the machines to do?
2204120	2204360	Right?
2205800	2207800	And here are two core principles.
2207800	2211320	The first one is that the machines
2211320	2213480	are constitutionally obliged
2213480	2215960	to be acting in the best interests of humans.
2216600	2218040	That's what they're for.
2218040	2219960	If you want to think of that as an objective,
2219960	2221480	that's the objective,
2221480	2223240	but obviously it's a very general one.
2223880	2225640	But the second point is crucial,
2225640	2228600	that the machines are explicitly uncertain
2228600	2231560	about what those human interests are.
2232520	2235960	So they know that they don't know what the objective is.
2237240	2241400	And it turns out that those two principles together
2242200	2244760	give us what I think could be a solution
2244760	2245800	to the control problem.
2247800	2250200	And the mathematical version of this
2250200	2251640	is called an assistance game.
2251640	2254360	So it's a game because there are at least two entities,
2254360	2257800	a human and a machine involved in this decision problem.
2257800	2259720	And it's an assistance game because the machine
2259720	2262840	is designed to be of assistance to the human.
2264040	2267000	And we can show by examining solutions,
2267000	2269240	we can actually write down simple cases
2269240	2274040	and analyze behaviors of the solutions of this game,
2274040	2276360	that when you solve assistance games,
2276360	2279720	the machine will be deferential to humans.
2280360	2282760	It will behave cautiously.
2282760	2285080	So minimally invasive behavior means
2285080	2287800	that it changes as little as possible of the world.
2289720	2293160	In order to help you because there are parts of the world
2293160	2296840	about which it doesn't understand your preferences.
2296840	2299320	And it knows that it doesn't understand your preferences.
2299320	2302440	So it knows not to mess with those parts of the world.
2302440	2306120	And in the extreme case, we can show that these kinds
2306120	2308520	of AI systems want to be switched off
2309240	2311400	if humans want to switch them off.
2311400	2313640	Whereas standard model AI systems,
2313640	2315400	which are pursuing a fixed objective,
2316120	2318200	will prevent themselves from being switched off
2318200	2322040	because that would lead to them failing in their objective.
2322920	2325320	So you get very, very different behaviors
2325320	2326840	from these kinds of AI systems.
2327560	2330840	And I believe this is the core of how we could build
2331560	2334200	a new discipline of safe and beneficial AI.
2336280	2338120	Okay, so I'm going to make a couple of brief remarks
2338120	2340200	about large language models before I wrap up
2340200	2343080	because that's what you're probably expecting me to talk about.
2344120	2345640	So first of all, what are they?
2345640	2348280	Right there, they are big circuits.
2349640	2353960	And those circuits are trained by billions of trillions
2353960	2356600	of small random perturbations.
2356600	2359720	They are trained to imitate human linguistic behavior.
2359720	2363160	And the training data they have is text
2363160	2367960	and transcribed speech trillions of words,
2367960	2371000	an amount of text parably equivalent to everything,
2371800	2373880	every book that the human race has ever written.
2374840	2377400	En, of course, as we know, they do it very well.
2378040	2380600	And it's really difficult for a human being
2380600	2387640	to see this level of semantic and syntactic fluency
2387640	2391320	and not think that there's some intelligence behind it.
2393000	2395400	And I would argue, as we go,
2395400	2397560	that we may well be overestimating
2397560	2399960	how much intelligence there really is behind it.
2399960	2403160	We have no experience with entities
2403240	2406280	dat heeft read every book the human race has ever written.
2406280	2410520	That's parably 100,000 times more
2410520	2412200	than any human has ever read.
2413240	2415000	So, of course, it's going to look
2416680	2419160	more knowledgeable and more capable
2419160	2421000	of answering a wider variety of questions.
2421560	2423400	But whether that's real intelligence
2424360	2426600	and whether it's flexible enough
2426600	2429640	to move outside of its training data effectively,
2429640	2430440	we don't know yet.
2430440	2433240	But here, the key point is that
2433800	2435480	that linguistic behavior is generated
2435480	2437160	by humans who have goals.
2437800	2440440	That is the generating mechanism for the data.
2441400	2443000	And if there's one thing we know
2443000	2443960	about machine learning,
2443960	2447240	typically the best solutions
2447240	2449400	that are found by machine learning algorithms
2449400	2452520	are to recreate the generating mechanism
2452520	2455240	for the data within the model itself.
2455880	2458840	And so the default hypothesis, actually,
2459560	2463000	is that large language models
2463000	2464840	are creating internal goals
2465720	2467640	because that's a good way
2467640	2469640	to be a good human imitator.
2470520	2472520	So it's not that the system is learning
2472520	2474120	what the goals of the humans are,
2474120	2477240	it's actually forming internal goals itself
2478040	2481320	as a way of being a better human imitator.
2483240	2485480	So I asked this question to the Microsoft,
2485480	2487000	that group of Microsoft authors,
2487560	2488760	the first author in particular,
2488760	2492360	Sebastian Bubeck, do these systems have goals?
2492360	2494760	And his answer was, we have no idea.
2496440	2497880	So that should worry you, right?
2497880	2499720	The fact that they are releasing a system
2500680	2502920	to eventually hundreds of millions
2502920	2503880	or billions of people
2505160	2507000	that they claim exhibits sparks
2507000	2509080	of artificial general intelligence
2509080	2510520	and they have no idea
2510520	2511960	whether or not this system
2511960	2514120	is pursuing internal goal structures
2514840	2517160	en they have no idea what those goals might be.
2518200	2519400	I think that should worry you.
2521800	2523800	So one question then is,
2523800	2526520	okay, so let's imagine that it is learning goals.
2526520	2528040	Is it learning the right goals?
2528040	2529400	It's learning from humans,
2529400	2532520	so maybe we're going to be lucky here
2534280	2535800	and we'll end up producing systems
2535800	2537400	that are aligned with humans
2537400	2538200	and that will be great.
2540200	2541640	Unfortunately, it's not true.
2542600	2545000	And the way to understand the answer to this question
2545880	2547400	depends on the type of goal
2548040	2549240	that you're going to learn.
2549240	2551720	So I distinguish here two types of goals.
2552920	2556440	The first type is what we call an indexical goal,
2556440	2561400	which means a goal that's specific to the individual who has it.
2561400	2564200	So the state you're trying to bring about
2564200	2565640	is specific to the individual.
2566600	2569480	So if I have the goal of drinking coffee,
2569480	2571800	then it's satisfied if I'm drinking the coffee
2571800	2573880	and it's not satisfied if you're drinking the coffee.
2576200	2578040	If I want to become ruler of the universe
2578040	2580280	and obviously it's only satisfied
2580280	2581960	if I'm the ruler of the universe
2581960	2583640	and it's not if you're the ruler of the universe.
2584520	2586680	So if those are some of the goals
2586680	2587720	that the system acquires,
2588280	2589880	then obviously that's bad.
2590440	2593320	We don't want the machine to be drinking the coffee.
2593320	2595080	We want it to be making the coffee for us.
2596200	2598040	We don't want the machine to be trying
2598120	2599640	to become ruler of the universe.
2601960	2602760	And then you might say, well,
2602760	2603960	there's other kinds of goals
2603960	2605560	which we might call common goals.
2605560	2607480	So if I want to paint the wall,
2608440	2610200	I want the wall to be painted,
2610200	2612200	but I don't mind if you paint the wall.
2612200	2614520	If you paint the wall, the wall gets painted and that's fine.
2614520	2616200	So this is not indexical.
2616200	2617560	This is a common goal
2617560	2619400	and maybe mitigating climate change.
2619400	2621560	That sounds like something we would all like to have.
2622440	2623080	So that's good.
2623080	2626440	And if the system learns to pursue these common goals,
2626440	2628360	then that maybe is not so bad.
2628360	2631400	But actually, that can be just as bad
2632360	2634360	because when humans pursue a goal,
2634920	2637560	we don't pursue it to the exclusion of everything else.
2638840	2641240	We know that we want to mitigate climate change,
2641240	2644200	but we know that we can't mitigate climate change
2644200	2647480	by, for example, removing all the oxygen in the atmosphere.
2649160	2653320	Perhaps that would restore some equilibrium to temperatures
2653320	2655480	and it would certainly get rid of all the humans
2655560	2657400	who are the cause of the climate change.
2657400	2659400	But that's something we don't want.
2659400	2661880	So we'd rather be alive than dead.
2661880	2663960	And so we look for climate change solutions
2663960	2665240	that don't also kill us.
2665960	2669080	Whereas the AI system may be pursuing
2669080	2670200	some of these common goals,
2670200	2672120	but in a way that is pursuing
2672120	2673800	to the exclusion of everything else,
2674600	2676360	which is just as bad, if not worse,
2676920	2678760	than pursuing the indexical goals.
2680040	2682360	So then the next question is,
2682360	2686440	well, does GPT-4 actually pursue its goals?
2686440	2689480	If it has goals, is it able to pursue them?
2690360	2691880	And I think we don't know
2691880	2693160	because we don't know if it has goals
2693160	2696760	and we have no idea what its internal mechanism is at all.
2698040	2699720	But when you look at the conversation
2699720	2701640	with Kevin Ruse in The New York Times
2701640	2702920	and here are some of the headlines,
2703800	2706760	Creepy Microsoft being chatbot urges tech columnist
2706760	2707640	to leave his wife,
2708360	2711080	and it does so persistently over 20 pages.
2711800	2713560	Despite Kevin Ruse's attempts
2713560	2714520	to change the subject
2714520	2715880	and say, I want to talk about baseball.
2715880	2719000	He says, no, no, no, you have to marry me.
2719000	2719800	Blah, blah, blah, right?
2719800	2722120	It's very persistently pursuing the goal.
2722120	2724120	At least that's how it appears
2724120	2726040	to any normal observer
2726040	2727960	that this is a system that does,
2727960	2728920	for whatever reason,
2729800	2731320	has acquired this goal
2731320	2733880	and is pursuing it persistently
2733880	2735640	across many pages of interaction.
2737800	2740840	Okay, so that leads us to the open letter
2740840	2743720	which was published a couple of weeks ago
2743720	2746120	and caused a great deal of media
2746120	2748120	and it turns out government attention as well.
2748120	2752440	And the open letter is asking for a pause
2752440	2755720	in the development and release
2755720	2758360	of systems more powerful than GPT-4.
2759400	2760680	And the purpose is
2760680	2764840	that before we resume that kind of activity,
2765480	2768040	so it's not asking to stop AI research.
2768120	2769560	There's a lot of misunderstanding
2769560	2772040	and misinformation around the open letter.
2772840	2775880	It's asking for a pause in development
2775880	2777000	and deployment of systems
2777000	2778520	more powerful than GPT-4
2779080	2781000	so that we have time to develop
2781000	2783080	the basic safety criteria
2783640	2785000	that these systems should meet
2785800	2787720	and to ensure that systems
2787720	2789240	meet those criteria
2789240	2790680	before they can be released.
2791240	2792920	And this is completely consistent
2792920	2796920	with agreements that all the governments
2796920	2799000	of the developed Western economies
2799000	2801080	have already signed up to.
2801080	2803800	So the OECD AI principle says
2803800	2805800	that AI systems should be robust,
2805800	2807960	secure and safe throughout their entire lifecycle
2808520	2810360	so that in conditions of normal use,
2810360	2812360	foreseeable use or misuse,
2812360	2813720	or other adverse conditions
2813720	2815400	they function appropriately
2815400	2817960	and do not pose unreasonable safety risk.
2818920	2821080	So that's what governments have already agreed to.
2821080	2823160	We're not asking for anything
2823160	2824440	particularly outlandish here.
2825080	2828040	And those principles are going to be enshrined
2828040	2830120	in the European Union AI Act
2830120	2832200	which should be enacted later on this year.
2833160	2836520	And interestingly, after the open letter came out,
2836520	2837800	open AI responded,
2837800	2840040	or at least maybe it's coincidental,
2840040	2843800	but a few days later they issued an announcement
2843800	2845320	that included the following statement.
2845320	2847800	We believe that powerful AI systems
2847800	2850680	should be subject to rigorous safety evaluations.
2850680	2852200	Regulation is needed to ensure
2852200	2854280	that such practices are adopted.
2855160	2857880	So perhaps there isn't such a big gap
2858920	2861800	between the people who sign the letter
2862520	2866120	and the tech corporations who are developing the systems.
2866120	2867800	So I have a couple of other recommendations.
2867800	2871400	One is that in order to pass these tests,
2873160	2875480	and I would say that at the moment
2875480	2877480	the large language models cannot pass
2877480	2880280	any reasonable test for safety,
2881000	2882360	in order to pass these tests,
2883400	2885560	I think we're going to need to develop AI systems
2885560	2887000	that are what are called well founded,
2887000	2891480	that they're built from semantically well-defined components
2891480	2893800	that are composed in a rigorous way,
2893800	2896520	such that we can analyze the properties
2896520	2898760	of the composite system that we're building.
2899400	2904840	This is how we do engineering in every area of our civilization.
2905800	2907720	We understand how the systems work
2907720	2910200	and ideally we develop proofs
2910760	2913560	that they are safe before they are released.
2914200	2916360	We also need actually a way of preventing
2917320	2920440	the deployment of unsafe systems.
2921720	2923400	And regulation is not enough.
2923400	2925480	It's obviously necessary, but not sufficient.
2926440	2927720	And I believe to do that,
2927720	2930040	we need a big change in our digital ecosystem.
2930040	2933000	The existing model is basically
2933000	2934920	that everything can run on the computer
2934920	2936680	unless it's known to be unsafe.
2937640	2939800	But I think the new model that we need,
2939800	2942680	certainly outside of the research lab
2942680	2944120	and outside of the classroom,
2945080	2948360	so in real world data centers, for example,
2948360	2951400	that nothing runs unless it is known to be safe.
2952280	2954440	And there are technologies such as proofcaring code
2954440	2957080	that enable this to be implemented
2957080	2959560	with efficient hardware checking of proofs and so on.
2960600	2963960	And at the moment I do not see another solution
2964680	2969000	for the problem of preventing unsafe AI systems
2969000	2971160	from being used and misused.
2972120	2975800	So to summarize, I think AI has huge potential
2976360	2978120	for benefiting our civilization
2978760	2983320	and that potential is leading to this apparently unstoppable momentum.
2984600	2986840	But if we keep going in the same direction,
2987880	2991960	that's the driving off a cliff metaphor from the small movie,
2992040	2993960	then we end up losing control
2993960	2995960	because we are building these systems
2995960	2997960	within the standard model for AI
2997960	2999960	and that leads to loss of control.
2999960	3001960	We can do it differently.
3001960	3003960	There's a huge amount of work to do,
3003960	3005960	but I think we can do it differently
3005960	3007960	and build systems that are safe and beneficial.
3007960	3011960	And then I think there needs to be a general change
3011960	3013960	in the whole nature of the discipline
3013960	3017960	and the profession so that AI,
3017960	3020960	because of its power, needs to be treated
3020960	3024960	more like the high stakes technologies
3024960	3026960	such as aviation and nuclear power
3026960	3030960	and less like what some people call
3030960	3032960	a battle of special effects wizardry
3032960	3034960	which seems to be going on right now.
3034960	3037960	So with that I'll say thank you very much
3037960	3040960	and I hope we have time for questions.
3040960	3042960	Thank you so much.
3051960	3053960	I hope you could hear that Professor Russell,
3053960	3056960	those were 300 people applauding your speech.
3056960	3059960	We do have room for some questions and answers.
3059960	3062960	We have a mic somewhere in the audience.
3063960	3066960	So just raise your hand if you want to ask
3066960	3068960	Professor Russell a question.
3070960	3072960	Is Sir in front?
3072960	3075960	Hi, thank you for your talk.
3075960	3077960	Very interesting, I read your book.
3077960	3079960	It was also very good.
3079960	3081960	I recommend it to everyone.
3081960	3084960	What would be an early warning sign
3084960	3087960	of an AGI taking over the world?
3087960	3091960	So when do we know we're heading off that cliff?
3091960	3094960	Yeah, I think that's a great question.
3094960	3097960	In that sense I think it's very different
3097960	3099960	from nuclear technology.
3099960	3102960	In some sense we had a warning about nuclear technology
3102960	3105960	in 1945.
3105960	3108960	And you don't have to explain to a prime minister
3108960	3111960	why nuclear technology could be dangerous.
3113960	3118960	But with AI I think it could be much more insidious.
3120960	3125960	And when we think about the way the oil industry
3125960	3128960	or fossil fuel corporations in general
3128960	3131960	in some sense took over the world,
3131960	3136960	they led us down the path of probably irreversible climate change
3136960	3139960	despite the widespread understanding
3139960	3142960	that this direction was catastrophic.
3142960	3149960	And it involved a lot of complex disinformation campaigns,
3149960	3154960	regulatory capture, so literally taking over
3154960	3158960	through corruption and economic power,
3158960	3162960	governments and representatives in democracies,
3163960	3167960	ensuring that people became economically dependent
3167960	3172960	on fossil fuels in order to maintain
3172960	3175960	their stranglehold, if you like.
3175960	3177960	So many, many parts of that plan
3177960	3180960	that were developed and executed over many decades.
3180960	3183960	And I think the rest of humanity
3183960	3185960	was sort of asleep at the wheel
3185960	3188960	and didn't realize the extent to which
3188960	3191960	they were losing control over their future.
3191960	3195960	En I think it could easily be much more like that.
3195960	3197960	And it wouldn't necessarily have to be
3197960	3202960	that the systems form any kind of explicit goal
3202960	3204960	of taking over the world.
3204960	3207960	That whatever goals, for example,
3207960	3209960	we continue with this approach
3209960	3212960	of training large language models on human datasets
3212960	3216960	and having no idea what kinds of internal goals
3216960	3218960	these systems are forming.
3218960	3220960	I mean, for all we know,
3220960	3226960	GPT-4 is actually in favor of more climate change
3226960	3230960	or maybe it's in favor of preventing climate change.
3230960	3234960	We don't know, but whichever one of those it turns out to be,
3234960	3238960	it may be subtly manipulating millions of people
3238960	3241960	in the way it answers questions related to climate change
3241960	3243960	or should I buy an electric car?
3243960	3246960	What do you think about solar panels?
3246960	3250960	It may be pursuing whatever political agenda it has
3250960	3255960	and not something that it autonomously chose to have.
3255960	3259960	Just this was a result of training on the datasets
3259960	3263960	and it can be affecting our entire world
3263960	3266960	in that simple kind of way.
3266960	3269960	We, I think, are still a long way, as I said,
3269960	3273960	from systems that are really general purpose AI,
3273960	3279960	particularly the ability to form very complex long term plans.
3279960	3281960	But if we reach that stage
3281960	3284960	and we haven't solved the control problem,
3284960	3289960	then I think it's just going to be irreversible.
3289960	3292960	There may well not be a very clear warning sign
3292960	3295960	and we may well slide off the cliff very slowly.
3295960	3297960	Another question here.
3297960	3300960	Thank you and thank you for your interesting lecture.
3300960	3305960	If you look at some concerns for existential risk of AI
3305960	3308960	10, 20 years ago about AGI, ASI,
3308960	3313960	one of the concerns was that it might be a very alien intelligence
3313960	3316960	compared to the human intelligence.
3316960	3318960	Now with large language models,
3318960	3320960	if that's indeed an important piece of the puzzle,
3320960	3322960	it may not solve the alignment problem,
3322960	3325960	but do you think it might alleviate that concern
3325960	3328960	that it would be a very alien intelligence?
3331960	3333960	No, not really.
3333960	3337960	In many ways, they are quite alien.
3337960	3341960	Partly because they've read hundreds of thousands
3341960	3344960	or million times more than humans have read.
3344960	3347960	Partly because of the way they're,
3347960	3352960	I wouldn't say design, the way they've evolved.
3352960	3361960	I think the human mind clearly has lots of internal structure.
3361960	3366960	We are very aware as we think of some of the things
3366960	3369960	that are going on inside our mental process.
3369960	3371960	There are many things we're not aware,
3371960	3376960	but it's quite possible that the internal structures,
3376960	3380960	these systems develop on nothing like the ones
3380960	3383960	that the human mind develops.
3383960	3386960	The thing that fools you
3386960	3390960	is the fact that it's conversing in English.
3390960	3392960	I don't know many humans
3392960	3395960	who can give me a proof of Pythagoras' theorem
3395960	3399960	in the form of a Shakespeare sonnet in half a second.
3399960	3402960	I'd like to meet him if you do.
3402960	3404960	One or two more questions,
3404960	3407960	maybe there in the back of the audience.
3407960	3409960	We have so many raised hands here.
3409960	3412960	I'm quite sure we can't answer all the questions,
3412960	3414960	but one or two more, please.
3414960	3416960	Ja, thanks so much for this talk.
3416960	3419960	When you said the beneficial or the assistance AI,
3419960	3422960	you described how it differs from the general purpose one.
3422960	3425960	To me it seems really clear that this is the one we want.
3425960	3428960	But I'm wondering, could there ever be,
3428960	3430960	what are the incentives,
3430960	3432960	what are the strongest incentives,
3432960	3434960	not to make these assistance AIs?
3434960	3436960	Is there anything you can predictably say
3436960	3438960	that the general purpose AIs
3438960	3440960	will do much better than the assistance AIs?
3440960	3443960	Or are there any tasks that assistance AIs cannot solve,
3443960	3446960	which might mean that some organization
3446960	3448960	will want to deploy another one,
3448960	3451960	even if they are aware of the safety risk.
3451960	3453960	But perhaps there's so much profit at stake
3453960	3455960	that they will do it anyway.
3455960	3461960	Well, I don't see any necessary difference in capability.
3461960	3463960	But there may be a difference
3463960	3466960	in what the systems are willing to do.
3466960	3471960	Obviously, I'm recommending that
3471960	3475960	when we train these assistance game solvers,
3475960	3480960	we design it such that their objective
3480960	3484960	is furthering the interests of all of humanity.
3484960	3486960	Now, you could have a different version
3486960	3489960	that furthers the interests of me
3489960	3491960	at the expense of the rest of humanity
3491960	3495960	en deploying that type of system
3495960	3499960	might appear to give you some short term gain,
3499960	3506960	but it could be in the long run arbitrarily bad
3506960	3510960	for the rest of humanity and perhaps for you too.
3510960	3514960	So I think that this is why I'm arguing
3514960	3517960	that we need not just,
3517960	3519960	okay, here is a safe technology,
3519960	3523960	we need a way to make sure that unsafe technologies
3523960	3525960	or unsafe versions of that technology
3525960	3528960	are not deployable.
3528960	3533960	We've tried a policing model with malware,
3533960	3537960	cybercriminals, cyberwarfare,
3537960	3540960	and it's been a total failure.
3540960	3543960	So I think we need to change the way
3543960	3547960	we conceptualize our whole digital infrastructure.
3547960	3549960	And I've talked to people,
3549960	3552960	both hardware architects and network architects
3552960	3554960	and formal methods people,
3554960	3556960	and I think there's a belief
3556960	3559960	that this is technologically feasible.
3559960	3563960	It would make life a little bit more complicated,
3563960	3568960	but it's technologically feasible to do.
3568960	3570960	And in fact, interestingly, Microsoft
3570960	3572960	tried to do something very much like this
3572960	3576960	in the early 2000s in their palladium project.
3576960	3579960	But at that time, the economics was not there.
3579960	3581960	But given that right now,
3581960	3584960	some estimates of the cost of malware
3584960	3587960	are about $6 trillion a year,
3587960	3593960	then maybe the time is right to look at this again.
3593960	3596960	It seems like a daunting but worthwhile task.
3596960	3599960	Let me see.
3599960	3601960	Do we have women in the audience
3601960	3603960	that have a question? Yes.
3606960	3608960	Hi, Dr. Russell.
3608960	3611960	I think my question might be related to what was just asked,
3611960	3613960	but indeed you're proposing a new model
3613960	3616960	where we create beneficial machines rather than intelligent,
3616960	3620960	but beneficial can mean different things to different people.
3620960	3623960	So are there going to be human standards
3623960	3625960	as to what is beneficial to humanity,
3625960	3630960	or would it, in your recommendation, be defined, per instance?
3631960	3638960	So there is about 8 billion people on the earth,
3638960	3642960	and there's no problem having 8 billion predictive models
3642960	3646960	of what each person wants the future to be like.
3646960	3652960	So there's no sense in which we standardise
3652960	3654960	what humans should want
3654960	3658960	or put in any particular set of values.
3658960	3663960	So there's no whose values it is going to produce.
3663960	3668960	It's going to be everyone's preferences count equally.
3668960	3673960	But there's a long-standing debate in moral philosophy
3673960	3679960	how do you aggregate the preferences of many individuals,
3679960	3683960	because, for example, if everyone wants to be a ruler of the universe,
3683960	3685960	well, they can't all be a ruler of the universe.
3685960	3688960	So what do you do?
3688960	3694960	And the utilitarian theory is that basically
3694960	3697960	you add up the preferences of the individuals
3697960	3701960	and you try to maximise the sum of those preferences.
3701960	3704960	Other people have what's called the ontological approach.
3704960	3711960	They say, no, we have to have certain inalienable rights
3711960	3713960	that need to be protected,
3713960	3717960	regardless of the potentially negative impact
3717960	3720960	on other people of respecting those rights.
3720960	3724960	And I believe that these two approaches can be reconciled
3724960	3727960	and so on, and there's some material in the book
3727960	3732960	in the last two chapters about those questions.
3732960	3737960	There are still some real difficulties inherent
3737960	3743960	in how an AI system should make decisions on behalf of people.
3743960	3747960	And this is nothing to do with my particular approach
3747960	3749960	of the assistance games always.
3749960	3753960	This is just what do we actually want AI systems to do at all, right?
3753960	3759960	So the idea that's, I think, is most difficult to,
3759960	3761960	the problem that's most difficult to address
3761960	3764960	is that what people want the future to be like
3764960	3770960	is not something that they autonomously chose, right?
3770960	3773960	We're not born with complicated preferences
3773960	3777960	about what kind of governmental structure
3777960	3780960	I want to live under and things like that, right?
3780960	3783960	Our preferences about the future
3783960	3786960	are acquired during our lifetime
3786960	3789960	as a result of experience of our culture
3789960	3792960	and the various forces applied to us
3792960	3796960	by our families and our peers and so on.
3796960	3800960	And Matja Sen, among others, has pointed out
3800960	3803960	that many of the preferences that people have
3803960	3806960	are put there by others for their own benefit.
3806960	3809960	So typically the elite, for example,
3809960	3815960	the patriarchy enforces a certain kind of view of society
3815960	3818960	that's beneficial to the patriarchy.
3818960	3822960	And should we take those views,
3822960	3827960	for example, the views of some women in very patriarchal societies
3827960	3832960	that the correct status of women is to be oppressed, right?
3832960	3834960	Should we take those views at face value
3834960	3839960	because they are not autonomously chosen,
3839960	3843960	they are basically indoctrinated by the patriarchy.
3843960	3846960	So Sen argues that no,
3846960	3850960	we should not take those preferences at face value.
3850960	3854960	But that gets you into very dangerous territory, right?
3854960	3857960	Well, which preferences are okay to take at face value
3857960	3860960	and which ones are not okay to take at face value?
3860960	3863960	And if they're not okay to take them at face value,
3863960	3866960	well, what do you replace them with?
3866960	3869960	And this is an area where I don't think
3869960	3872960	AI researchers should be answering that question.
3872960	3876960	But we need answers fairly soon from somewhere
3876960	3879960	because AI systems are going to be making decisions
3879960	3881960	on behalf of many people.
3881960	3883960	So whether you like it or not,
3883960	3889960	they are implementing some answer to that moral problem.
3889960	3891960	And it might be the wrong one
3891960	3893960	if we don't actually think it through.
3893960	3895960	That's interesting.
3895960	3899960	AI as a catalyst to some of the most pressing moral concerns
3899960	3901960	of mankind so far.
3901960	3904960	One final question, maybe.
3904960	3907960	They're completely in the left.
3912960	3914960	Hi, Professor Russell.
3914960	3917960	Thank you for your talk.
3917960	3919960	I just want to ask about,
3919960	3921960	so you believe that large language models
3921960	3927960	wouldn't be able to actually be capable of AGI.
3927960	3931960	So why would you sign the open letter, basically?
3931960	3935960	So since that,
3935960	3938960	it won't be a catastrophic risk per se
3938960	3940960	since it won't be able to become
3940960	3942960	general artificial intelligence.
3942960	3945960	So do you see any catastrophic risk
3945960	3949960	in companies building larger and larger models?
3949960	3953960	Or is it just for general safety purposes?
3953960	3957960	So the question is,
3957960	3960960	is it actually something
3960960	3962960	that we should really be worried about
3962960	3965960	large language models?
3965960	3968960	So I think large language models
3968960	3972960	in isolation as we currently conceive them
3972960	3977960	are probably not presenting that kind of catastrophic risk.
3977960	3979960	I think they present many, many risks.
3979960	3981960	And the open letter talks about
3981960	3985960	some of those disinformation bias, et cetera, et cetera.
3985960	3989960	So I think there are already many reasons
3989960	3991960	that these systems would fail
3991960	3996960	any reasonable safety criteria.
3996960	4000960	But the concern is that it's not just,
4000960	4003960	we're not just going to make these models bigger.
4003960	4007960	We are also going to try to figure out
4007960	4011960	how can they be arranged
4011960	4014960	so that they actually develop
4014960	4016960	a consistent internal model of the world?
4016960	4018960	How can they be arranged
4018960	4022960	so that they can also do long-term planning?
4022960	4024960	En, as I say,
4024960	4027960	we don't really know the answers to those questions yet,
4027960	4032960	but I think that the ideas behind
4032960	4034960	large language models
4034960	4039960	do form a significant piece of the puzzle.
4039960	4044960	And so the concern is that future generations of these systems,
4044960	4046960	which will be extended,
4046960	4048960	not just in scale,
4048960	4053960	but also in the additional capabilities
4053960	4056960	that we might endow them with
4056960	4062960	by maybe a more design-based approach,
4062960	4066960	that those systems would start to
4066960	4071960	get close to presenting a real threat.
4071960	4074960	En so in some ways,
4074960	4076960	I think this title of that paper,
4076960	4081960	Sparks of Artificial General Intelligence,
4081960	4087960	is not wrong.
4087960	4090960	And I think when you think about
4090960	4091960	what does sparks mean,
4091960	4095960	what sparks are a predecessor to a fire.
4095960	4103960	En I think that's what we want to prevent.
4103960	4104960	En with that,
4104960	4107960	we come to the end of the first part of this evening.
4107960	4108960	Professor Russell,
4108960	4110960	I'd like to thank you for elaborating
4110960	4112960	in such a concise way
4112960	4115960	the dangers of developing intelligent AI
4115960	4119960	and to provide a manual, so to speak,
4119960	4121960	to steer away from that cliff
4121960	4126960	and to develop safe, beneficial artificial intelligence
4126960	4128960	that is aligned with our goals.
4128960	4130960	So join me in a big round of applause
4130960	4132960	for Professor Russell.
4142960	4143960	En with that,
4143960	4145960	we're going to leave you, Professor Russell,
4145960	4147960	and I would like to invite the five panelists
4147960	4150960	to continue the conversation.
4150960	4154960	Please come to the floor.
4154960	4158960	And I'll introduce you properly.
4158960	4160960	Marc Brakel, to the right,
4160960	4164960	he's a director of policy at Future Life Institute,
4164960	4166960	involved in the AI Act
4166960	4169960	that is currently being prepared
4169960	4172960	and is later this year going to be proposed
4172960	4174960	by the European Union, am I right?
4174960	4175960	It's already been proposed.
4175960	4176960	Oh, it's already been proposed.
4176960	4177960	Hopefully it gets voted through.
4177960	4178960	Voted through.
4178960	4179960	I should say that, yeah.
4179960	4181960	Then we have Tim Bakker,
4181960	4185960	who is a PhD student at the University of Amsterdam.
4185960	4187960	The title of your thesis?
4187960	4188960	My thesis?
4188960	4189960	Yeah.
4189960	4190960	I don't know yet.
4190960	4191960	You don't know?
4191960	4192960	Live Hanger.
4192960	4193960	Chat GPT, yeah.
4193960	4194960	Little secrets.
4194960	4197960	Working on AI research.
4197960	4200960	Then we have Nandi Robijns,
4200960	4202960	who is working at the Ministry of Interior
4202960	4204960	and Kingdom Relations,
4204960	4207960	part of a crew of AI and data consultants.
4207960	4209960	About 70 people strong.
4209960	4211960	I just heard over dinner.
4211960	4213960	Nice of you to be here.
4213960	4215960	Two members of the Dutch parliament.
4215960	4217960	Queenie Rijkovski,
4217960	4219960	who is a member of the Liberal Party,
4219960	4220960	the VVD,
4220960	4223960	and has cyber security and digitisation
4223960	4224960	in your portfolio.
4224960	4226960	And last but not least,
4226960	4228960	Lammert van Raam,
4228960	4230960	who is a member of parliament
4230960	4232960	for the party for the animals,
4232960	4236960	who is focusing on IT and privacy issues.
4236960	4238960	Great of you guys to come over
4238960	4240960	and stand here and discuss with us
4240960	4242960	the dangers of AI
4242960	4244960	and how to deal with them.
4244960	4247960	Let me just start off with the obvious question.
4247960	4251960	Professor Russell just painted a rather scary picture
4251960	4255960	of humanity that may one day fear of a cliff
4255960	4259960	because we don't control the risks involved in AI.
4259960	4261960	Do you share this view?
4261960	4264960	Do you also think artificial intelligence
4264960	4265960	may sooner of later,
4265960	4267960	if we don't control as well,
4267960	4269960	steer humanity over a cliff?
4270960	4271960	I work at an organisation
4271960	4273960	at the Future of Life Institute
4273960	4274960	that believes this,
4274960	4276960	so it doesn't come as a surprise,
4276960	4278960	I think, to say that I agree
4278960	4281960	with Stuart Eastman of our advisors also
4281960	4284960	and just in response to the last question also
4284960	4287960	about our open letter that we put out
4287960	4289960	I think it's now a week and a half ago.
4289960	4291960	This is the first event where I'm at
4291960	4294960	with actual people since we put the letter out.
4295960	4298960	I think it's really worth reading that
4298960	4299960	because the letter,
4299960	4301960	the open letter that we presented
4301960	4303960	talks about all kinds of risks
4303960	4305960	that our society might struggle with
4305960	4306960	when it comes to AI,
4306960	4308960	not just the existential risk.
4308960	4309960	Right.
4309960	4311960	And I think our first contact with AI
4311960	4313960	as Stuart Russell also highlighted
4313960	4314960	was social media
4314960	4316960	with a super simple algorithm
4316960	4318960	our second contact with AI
4318960	4320960	is probably these large neural networks
4320960	4322960	and I think we're going to really struggle
4322960	4324960	to control truths,
4324960	4326960	to control access to
4326960	4328960	what was previously very hard
4328960	4330960	to access information.
4330960	4332960	So yes, I worry about existential risks,
4332960	4333960	I agree with Stuart,
4333960	4335960	but I also think beneath that
4335960	4337960	there's a layer of very, very serious risks
4337960	4339960	that is also a cause of worry.
4339960	4340960	Right.
4340960	4342960	We'll touch upon them probably later.
4342960	4344960	Ja.
4344960	4346960	No, I definitely agree with
4346960	4348960	Professor Russell about his worries
4348960	4350960	and actually also with Mark
4350960	4351960	about what he just said.
4351960	4353960	I think Stuart Russell was very right
4353960	4355960	about pointing to the fundamental problem
4355960	4356960	with the current systems
4356960	4358960	which is that we're training them
4358960	4360960	as optimizers
4360960	4362960	instead of as things that
4362960	4364960	do anything that is not that
4364960	4366960	because that is just such a hard thing
4366960	4368960	to aim in a way that we
4368960	4369960	want to aim it.
4369960	4371960	We just have no idea how to do that.
4371960	4372960	Dandy.
4372960	4374960	Yes.
4374960	4377960	I think it is very important to take into account
4377960	4379960	a wide range of potential risks of AI,
4379960	4381960	especially because AI is such
4381960	4384960	extremely powerful technology
4384960	4387960	and I think what we talk about today
4387960	4390960	is a very important part of this range of risks,
4390960	4392960	especially because of the scale
4392960	4395960	of the potential negative impact that it can have.
4395960	4397960	And on top of that, it is very neglected
4397960	4399960	and this neglect is worrying me,
4399960	4402960	especially because as we see
4402960	4405960	more and more AI systems are extremely capable
4405960	4408960	in achieving their programmed goals
4408960	4411960	and the main worry
4411960	4413960	about these AI systems becoming dangerous
4413960	4416960	is rooted in the fact that they pursue these goals
4416960	4419960	regardless of whether or not it is what we intended.
4419960	4422960	Ja, so that needs to be addressed
4422960	4425960	and on top of that, these models,
4425960	4427960	no one knows what is going on inside these models,
4427960	4430960	as George also also said
4430960	4432960	and no one actually knows
4432960	4435960	how we can define a goal
4435960	4437960	that takes into account every value
4437960	4438960	that we care about,
4438960	4440960	which is also what we just talked about.
4440960	4442960	So yes, I do agree
4442960	4445960	that it needs much more attention.
4445960	4446960	Queenie.
4446960	4448960	Yes, I agree even though
4448960	4451960	I am a tech-optimistical person.
4451960	4455960	So when it comes to technologies like AI,
4455960	4457960	I can definitely see the risk in the downside.
4457960	4460960	So I completely agree with the other speakers
4460960	4463960	and also when we just heard in a presentation
4463960	4467960	that if we, AI can also see maybe human
4467960	4471960	as a danger when it comes to climate or climate change.
4471960	4473960	We need to really think about
4473960	4475960	how we are going to program it.
4475960	4476960	What are the goals?
4476960	4478960	I think we just heard some examples.
4478960	4480960	And one of the experiments that they have been doing,
4480960	4483960	TNO, it's a scientific research company
4483960	4485960	in de Nederlands organization.
4485960	4487960	En they've also done some smaller
4487960	4488960	and some bigger experiments.
4488960	4490960	And the smaller experiment is
4490960	4492960	a robotic vacuum cleaner.
4492960	4494960	And they said, okay, your task is
4494960	4497960	to keep this clean room dust free.
4497960	4499960	And what happened?
4499960	4502960	The robot started to block the door.
4502960	4505960	Because every time when a human came in,
4505960	4506960	the room was dirty.
4506960	4507960	You were the actual problem, yeah.
4507960	4508960	Yeah, so it's,
4508960	4511960	and this is just like a small example, of course.
4511960	4514960	But what it got me thinking is
4514960	4517960	not only what assignment or what goal
4517960	4520960	do you give a system or robot, et cetera.
4520960	4523960	But also can you grasp upon
4523960	4526960	what the outcomes can be when you ask something.
4526960	4530960	And actually when it comes to equality,
4530960	4534960	I think AI can maybe even help
4534960	4536960	because in my experience
4536960	4539960	being a woman in politics,
4539960	4541960	working on tech,
4541960	4545960	mostly a lot of men around me
4545960	4547960	in my context,
4547960	4551960	I still hear people say things.
4551960	4555960	I still see some articles written in a way
4555960	4557960	that they write different when you're a woman
4557960	4558960	than when you're a man.
4558960	4560960	So actually, I hope,
4560960	4561960	so that's also my goal
4561960	4563960	from a political perspective,
4563960	4564960	if we can make sure
4564960	4567960	that we provide the right regulation
4567960	4570960	and control when it comes to AI,
4570960	4573960	maybe we can even help equality
4573960	4575960	instead of being a danger to it.
4575960	4576960	Okay, interesting.
4576960	4578960	We may continue that conversation
4578960	4580960	later this evening.
4580960	4582960	Nasty little buggers,
4582960	4584960	those vacuum cleaners, right?
4584960	4586960	I have.
4586960	4588960	So, you have one?
4588960	4589960	Yes.
4589960	4590960	But you're not locked out yet?
4590960	4591960	No, not yet.
4591960	4593960	Good for you.
4593960	4596960	So, yeah, worrying.
4596960	4602960	I was taking comfort from the example of Rutherford
4602960	4604960	and the next day,
4604960	4608960	Zillar invented something that made it possible.
4608960	4611960	Perhaps it's now the 12th of April
4611960	4613960	on 13th of April.
4613960	4616960	There's one Zillar in the room already
4616960	4617960	making a solution
4617960	4619960	because that is what possible.
4619960	4621960	And yes, it is worrying.
4621960	4622960	It is worrying.
4622960	4624960	I have only one consolation.
4624960	4627960	A politician will probably have to solve it.
4627960	4629960	And then it's...
4629960	4631960	I don't know if it's a consolation, to be honest.
4631960	4633960	But...
4633960	4635960	It's the best we have, right?
4635960	4639960	It's the best we have politicians in a democracy.
4639960	4641960	Nevertheless, we are working together
4641960	4644960	on the concerns we both feel.
4644960	4646960	And I think that's giving some hope
4646960	4649960	because she's a completely different ideology
4649960	4651960	than my party.
4651960	4654960	En stil we find the same common ground
4654960	4656960	in our concerns.
4656960	4658960	En dat is, I think,
4658960	4660960	something to look forward.
4660960	4662960	The other consolation is we don't have to worry
4662960	4665960	about falling off the cliff
4665960	4668960	because we are already sliding off the cliff.
4668960	4670960	That's very comforting, yeah.
4670960	4672960	And for you, Otto,
4672960	4674960	the first slide, I have to say it, I'm sorry,
4674960	4677960	the first slide with all the risks,
4677960	4679960	don't show it to the animals
4679960	4682960	because they're already in a massive wave.
4682960	4685960	And don't show it to the global south.
4685960	4688960	But there are ways probably
4688960	4690960	if we can get this problem solved.
4690960	4693960	If there are enough zillars in the room,
4693960	4695960	we're counting on you.
4695960	4697960	We can also solve it politically
4697960	4699960	because the worries are very real.
4699960	4701960	So thank you.
4701960	4704960	So you all share, more or less,
4704960	4708960	the alarming story that Professor Russell just told us.
4708960	4711960	But at the same time, I don't see us all going to the streets
4711960	4714960	and protesting like we do with climate change
4714960	4719960	or in the 1980s we did with the risk of nuclear war.
4719960	4721960	So what's wrong here?
4721960	4725960	Why aren't we, if we all share this great risk
4725960	4727960	or concern for this great risk,
4727960	4729960	why are we not protesting?
4729960	4731960	What's wrong with us, wants to answer?
4731960	4734960	Nandi, you actually raised this point yourself.
4735960	4737960	So solve it.
4737960	4740960	We can't.
4740960	4744960	Ja, so I feel like there are some reasons.
4744960	4747960	The question is about...
4747960	4750960	Well, if the risk is so big for us as a society,
4750960	4752960	why aren't we, you know,
4752960	4754960	talking about this daily in parliament
4754960	4756960	and protesting on the square.
4756960	4759960	Yes, I think there are some reasons
4759960	4762960	that make it hard for people to realize one
4762960	4764960	that this is a real problem
4764960	4766960	and to see that this is something
4766960	4769960	that needs to be addressed right now.
4769960	4773960	And I think one of the reasons is that these aspects,
4773960	4775960	these concepts that we talk about
4775960	4777960	and the terms that we use are still quite vague
4777960	4781960	and difficult for people to understand.
4781960	4783960	And sadly, vague problems are much easier
4783960	4785960	to ignore than concrete ones,
4785960	4787960	which also makes it harder
4787960	4789960	for policymakers to prioritize,
4789960	4792960	things that are a bit more uncertain and vague
4792960	4795960	over things that are more concrete
4795960	4797960	and where we can see the harm
4797960	4800960	right in front of us right now.
4800960	4803960	A second reason also is that, you know,
4803960	4806960	this is also, to some people,
4806960	4809960	at first glance, quickly dismissed as science fiction,
4809960	4812960	not real or something that doesn't need attention right now.
4812960	4817960	Ja, which I think is a misconception.
4818960	4821960	So yeah, I think this is caused
4821960	4823960	by a lack of awareness and understanding
4823960	4826960	and a lack of urgency that we need to address.
4826960	4828960	At PR, yeah.
4828960	4830960	First Lamert, and then it goes to you.
4830960	4833960	I fully agree with you that there's a lack of knowledge, et cetera.
4833960	4835960	But perhaps it's also,
4835960	4837960	perhaps the protest is already there,
4837960	4839960	but we just don't recognize it.
4839960	4842960	For instance, there's, well,
4842960	4845960	the best example, of course, is the upheaval there was
4845960	4849960	in the Netherlands, of the toeslagen schandaal.
4849960	4853960	And algorithms played a very big role in that.
4853960	4858960	And also in the, let's say, discrimination factor of that.
4858960	4861960	And that led to the fall of the government.
4861960	4863960	So there was a big upheaval,
4863960	4866960	but we didn't perhaps recognize it as such.
4866960	4868960	So perhaps there is a lot of upheaval,
4868960	4871960	but we just have to categorize it differently to understand it.
4871960	4875960	But I agree fully with you that there's also a very,
4875960	4879960	a lack of understanding and a forelichting.
4879960	4881960	What is that?
4881960	4882960	Education, thank you.
4882960	4883960	Thank you, George.
4883960	4884960	Mark.
4884960	4888960	Yeah, if I could maybe add two sort of points of optimism to that.
4888960	4891960	I think when Otto first asked about sort of doing this event,
4891960	4893960	it was going to be in the smallest room of this building.
4893960	4894960	Right.
4894960	4896960	And it sold out, and now we're in the big room.
4896960	4900960	So I think that shows that I think society is moving
4900960	4903960	maybe slower than the development of the systems.
4903960	4905960	But still, it's of interest to more people.
4905960	4909960	And when we sat together with the Future of Life Institute,
4909960	4913960	with my sort of 16 colleagues four weeks ago brainstorming this letter,
4913960	4916960	we thought, okay, maybe we can have four news outlets cover it,
4916960	4918960	mainly in the United States.
4918960	4921960	Potentially we get one in Europe, that'd be great.
4921960	4924960	And a colleague of mine comes from rural Australia,
4924960	4928960	and her mum had heard it at the hairdressers on the radio show.
4928960	4930960	And I think that shows that.
4930960	4933960	Great source of information, the hairdressers recommended.
4933960	4936960	People are slowly waking up to the risk.
4936960	4938960	And the Overton window is also shifting.
4938960	4940960	I think a lot of governments are waking up to the fact
4940960	4943960	that they need to regulate this, and really quite quickly.
4943960	4944960	So we're going in the right direction.
4944960	4947960	Are we all happy with the direction we're going here?
4947960	4948960	Or are we, yeah?
4948960	4951960	I mean, I don't want to be overly optimistic.
4951960	4954960	I mean, the one thing that worries me is companies,
4954960	4957960	because we have your Russell's proposal here to say,
4957960	4958960	okay, we need to look at AI systems,
4958960	4960960	and we need to make sure that they are uncertain
4960960	4961960	about what our objectives are.
4961960	4962960	Right.
4962960	4963960	Whereas all of the investment,
4963960	4965960	and the economist in an article last week,
4965960	4968960	just saying how it escalated since chat GPT,
4968960	4970960	how many more billions are suddenly available to invest,
4970960	4973960	are all going to neural nets that do exactly the opposite.
4973960	4976960	Everyone is clueless as to what these systems do,
4976960	4979960	including the chief technology officer of open AI
4979960	4981960	who goes on TV and says that.
4981960	4984960	I'm interested to hear your comments on this, Tim,
4984960	4987960	because you actually interned at Facebook AI.
4987960	4988960	Yes.
4988960	4991960	Which was a long story, and quite eccentric,
4991960	4992960	what you did there.
4992960	4993960	I'm going to have to fed myself now.
4993960	4995960	No, you don't.
4995960	4997960	He was not working on the metaphors.
4997960	5000960	But please tell me, how do you view this danger?
5000960	5002960	Yeah, so it's, I mean, it's interesting,
5002960	5004960	because I've been worried about these topics
5004960	5005960	for a very long time,
5005960	5007960	and I've now been involved with AI
5007960	5010960	for a bit less than that, but still.
5010960	5013960	And it's been interesting to see the shift in opinions,
5013960	5016960	like, as Nandi said, people at the start really thought,
5016960	5018960	okay, this is some kind of science fiction
5018960	5019960	why you worried about this.
5019960	5022960	If you look at these systems, they're not able to do anything.
5022960	5025960	It makes no sense at all, stop worrying.
5025960	5027960	And in the last, say, two years,
5027960	5029960	but especially the last two months,
5029960	5031960	this has really changed in the community as well.
5031960	5033960	There's been so many researchers
5033960	5035960	that are now coming out as, oh yeah, actually,
5035960	5036960	I am kind of worried,
5036960	5038960	and I actually have been worried for a while,
5038960	5039960	but I kind of couldn't say,
5039960	5041960	because it was just such a weird thing,
5041960	5042960	and I couldn't really, you know,
5042960	5044960	probably if I said that to my colleagues,
5044960	5045960	who would call me crazy.
5045960	5047960	And now you have people like Geoffrey Hinton,
5047960	5050960	who is sort of often considered
5050960	5051960	the godfather of modern AI,
5051960	5053960	going on national television in an interview
5053960	5055960	and saying, yeah, it's not inconceivable
5055960	5058960	that AI will wipe us all out.
5058960	5060960	And also, I don't know what to do about that.
5060960	5063960	And so, it's become much more of an okay thing
5063960	5065960	to worry about, and I think that is quite hopeful.
5065960	5067960	Of course, that doesn't mean we know
5067960	5069960	how to solve the problem yet,
5069960	5072960	but at least we're allowed
5072960	5074960	as a scientific community
5074960	5075960	and also as a tech community
5075960	5077960	to at least consider these problems seriously,
5077960	5079960	and that's very good.
5079960	5081960	I'm also interested to hear the comments
5081960	5083960	of the two parliamentarians here,
5083960	5085960	because, well, your job, partly,
5085960	5087960	is to devise laws
5087960	5089960	and to think about how we could improve
5089960	5092960	the well-being of us people here in the Netherlands.
5092960	5094960	So, what do you think is the risk
5094960	5098960	of tech companies devising new AI systems
5098960	5102960	that may not be aligned with our well-being?
5102960	5104960	En what can you do about that?
5104960	5105960	Biggest party first?
5105960	5107960	Ja, ja, well, I think the...
5107960	5109960	Thanks.
5109960	5110960	That's a privilege.
5110960	5111960	Very generous.
5111960	5112960	I think...
5112960	5115960	No, well, of course, like the risks,
5115960	5117960	I think some of the risks we already talked about,
5117960	5120960	because in January, I think,
5120960	5124960	we had a big debate about AI in the parliament,
5124960	5127960	and one of the things that we also talked about
5127960	5129960	was, OK, so...
5129960	5132960	But, and you see it now with the open letter.
5132960	5135960	The people who write the code
5135960	5138960	are worried about what's going to happen with AI.
5138960	5140960	So, that's a bit, you know,
5140960	5143960	if someone can do something about how a software,
5143960	5145960	how a large language model,
5145960	5147960	how AI is working,
5147960	5150960	it starts with the person who's making it, right?
5150960	5153960	So, we also discussed,
5153960	5156960	so can you also maybe start looking maybe
5156960	5159960	over engineers, professions like that,
5159960	5162960	and not only teach them at university
5162960	5165960	how to write good code, efficient code, et cetera,
5165960	5170960	but also take into account ethics, human rights, et cetera.
5170960	5174960	So, where the programming starts,
5174960	5178960	also ethics and safety is taken into account.
5178960	5180960	The core of the curriculum, ja.
5180960	5183960	Exactly, and one thing which I think is hopeful
5183960	5186960	is that when you look at social media, big tech, the internet,
5186960	5190960	at first everyone starts, oh, it's going to regulate itself, right?
5190960	5193960	We only see positive things,
5193960	5196960	and human rights will fix this itself,
5196960	5199960	and now politics are waking up, oh, wait a minute,
5199960	5202960	social media, big tech companies,
5202960	5204960	they are a lot about making money
5204960	5206960	and not about taking responsibility
5206960	5209960	to make sure that they have a good contribution
5209960	5212960	to the country that they make money in.
5212960	5215960	We are trying to repair that,
5215960	5217960	but it's too late,
5217960	5219960	there's already a big power,
5219960	5221960	they already decide a lot,
5221960	5223960	maybe they have even more power
5223960	5225960	than a lot of governments have.
5225960	5227960	And what I see with AI,
5227960	5229960	especially on the European level,
5229960	5232960	that the European Commission already started to work on AI regulation
5232960	5234960	two years ago.
5234960	5236960	So, what is hopeful for me
5236960	5239960	is that we already started also from a political point of view,
5239960	5241960	because experts are thinking about this,
5241960	5243960	way longer than that,
5243960	5245960	but also from a political side,
5245960	5247960	the thinking has already begun,
5247960	5249960	the law regulation is already in the making,
5249960	5251960	not only in Europe, but also worldwide,
5251960	5253960	they are working with treaties, et cetera.
5253960	5255960	So, for me that's hopeful,
5255960	5259960	in a way that it helps me
5259960	5262960	in thinking that, okay, AI can still do wrong,
5262960	5265960	but maybe we are not too late.
5265960	5268960	Lamert, how do you view this?
5268960	5273960	It seems to me,
5273960	5278960	that it's like a winner takes all industry.
5278960	5282960	Like, to a certain extent,
5282960	5285960	the fossil industry is, wasn't is.
5285960	5287960	So, and we have a very,
5287960	5290960	that industry has a very bad track record.
5290960	5295960	So, I would be inclined to give
5295960	5297960	the big companies the,
5297960	5299960	not the benefit of the doubt,
5299960	5301960	but the disadvantage of the certainty
5301960	5303960	that they are still in the race
5303960	5306960	in a winner takes all situation.
5306960	5310960	So, again, on top of what you're saying,
5310960	5314960	I think we should be very restrictive,
5314960	5317960	restrictive policy,
5317960	5322960	and the European AI directive
5322960	5328960	is setting some very strict policies there as well.
5328960	5330960	Because it's not something that will
5330960	5333960	come from the goodness of the big companies.
5333960	5336960	I'm afraid.
5336960	5338960	Just like Professor Thompson said,
5338960	5340960	I mean, he's hopeful that AI will
5340960	5343960	solve world inequality.
5343960	5345960	But the thing, of course, is,
5345960	5348960	we could have solved world inequality a long time ago.
5348960	5352960	We don't need the computers or AI for that.
5352960	5354960	So, the problem goes far deeper in that.
5354960	5357960	And that's where I connect with Queenie
5357960	5360960	in the sense that we need to instill
5360960	5363960	the ethics in the educational system
5363960	5365960	to try and do that.
5365960	5367960	And at the same time,
5367960	5370960	very strict regulation, I would say.
5370960	5373960	Ja, because even if I can add a little bit to that,
5373960	5376960	so what internet, social media, et cetera,
5376960	5378960	what they did is they created,
5378960	5381960	or actually the companies who created it,
5381960	5384960	the companies were big in a sense
5384960	5386960	that we have never experienced before.
5386960	5388960	They have more power in a way
5388960	5390960	we have never experienced before
5390960	5392960	from companies, in my opinion.
5392960	5396960	And AI can actually triple that.
5396960	5400960	So, when we just saw in the presentation
5400960	5402960	how much extra quadrillion money
5402960	5404960	that can be made,
5404960	5406960	the first thing I thought was,
5406960	5408960	okay, but in whose pockets is going to,
5408960	5412960	and who's going to fill the pockets with the money.
5412960	5416960	And I don't think that we'll go to fighting inequality
5416960	5420960	if we don't make sure from a political perspective
5420960	5425960	that technology can make everyone's life better
5425960	5427960	and not only a happy few.
5427960	5429960	And I think that's really important
5429960	5431960	that we are talking about this right now,
5431960	5433960	that we're discussing this right now
5433960	5435960	is when we are making regulation.
5435960	5437960	And I'd love to hear you talk like that.
5439960	5441960	You can switch sides.
5441960	5443960	A big hand for Greeny.
5444960	5446960	Okay, in a minute
5446960	5448960	there's some room for questions from the audience
5448960	5450960	to our distinguished panelists.
5450960	5453960	But first, I want to pose the million dollar question.
5453960	5456960	That's what can we, or what should we all do
5456960	5458960	in order to tame the beast,
5458960	5461960	in order to avoid, we're going to run off the cliff.
5461960	5463960	We have the EU AI Act.
5463960	5467960	We have ideas of instilling ethics
5467960	5473960	and other subjects into core curricula of AI engineers.
5473960	5476960	But there are probably other great ideas
5476960	5478960	that we should take into account.
5478960	5481960	In a minute, but first I want to hear the five panelists.
5481960	5484960	We just make a little round and then the floor is yours.
5484960	5487960	So what should we do to tame the beast?
5487960	5489960	What should we do tomorrow
5489960	5491960	in order to make sure that we don't run off the cliff?
5491960	5493960	Of course, we have an AI Act.
5493960	5495960	Of course, we have great ideas
5495960	5497960	of how to improve the curricula of AI engineers.
5497960	5500960	But that's probably not the answer
5500960	5502960	to the million dollar question.
5502960	5504960	What else should happen?
5504960	5506960	I mean, there's a lot.
5506960	5508960	Just before coming here,
5508960	5510960	we send out seven recommendations to policymakers
5510960	5512960	to the signatories of the open letter.
5512960	5515960	We'll put that out tomorrow, so go check that out.
5515960	5518960	But it has things like national regulatory agencies for AI.
5518960	5521960	It has things like more AI safety research
5521960	5523960	and public funding in that.
5523960	5525960	So it's not just the companies doing that.
5525960	5527960	It really requires, I think,
5527960	5529960	the world coming together over this.
5529960	5531960	But given that I've got...
5531960	5533960	What does that mean, the world coming together over this?
5533960	5535960	I mean, I think ultimately we need
5535960	5538960	a sort of international atomic energy agency for AI.
5538960	5541960	So an international body that has enforcement agency
5541960	5543960	even over those jurisdictions
5543960	5545960	that don't fall under an AI Act
5545960	5549960	or aren't part of sort of a big power agreement.
5549960	5551960	But I am going to take this opportunity
5551960	5553960	with these two Dutch politicians
5553960	5555960	because I work a lot in Brussels
5555960	5557960	where we have an AI Act,
5557960	5559960	but there's also a lot of big tech lobbying.
5559960	5561960	I mean, there's maybe four or five NGO people
5561960	5563960	and then there's several hundred from Microsoft
5563960	5565960	and Google and Bing
5565960	5567960	an open AI's own team right now.
5567960	5569960	It seems like an uneven fight.
5569960	5572960	It is. I think we need the AI Act tomorrow.
5572960	5575960	I think Brussels is taking its normal slow course.
5575960	5577960	And I think one thing the Dutch Parliament could do
5577960	5580960	is to ask for it to be applied provisionally.
5580960	5582960	As we have chat GPT right now,
5582960	5585960	we probably also need some rules and safeguards.
5585960	5589960	Another thing is the Act prohibits manipulation of people.
5589960	5592960	But only if you use your AI system in a subliminal way.
5592960	5594960	So if it's in a hidden frame.
5594960	5596960	But if you do it overtly, it's fine.
5596960	5599960	We think that probably should be changed.
5599960	5601960	En dan
5601960	5603960	maybe my final pitch here
5603960	5605960	is that for a long time
5605960	5607960	more general AI systems such as chat GPT
5607960	5609960	were exempt from the Act.
5609960	5612960	We've worked very, very hard to try and bring that into the Act.
5612960	5614960	But we also face a lot of Microsoft pushback.
5614960	5616960	So if there's anything you can do to keep it there,
5616960	5618960	that would be awesome.
5618960	5620960	Ok, keep Microsoft at bay.
5622960	5625960	Maybe a quick response here
5625960	5628960	because those are very sort of concise recommendations.
5628960	5630960	What do you think?
5630960	5632960	Are you going to take these up
5632960	5634960	and next time you talk to your fellow parliamentarians?
5634960	5636960	Of course read the recommendations.
5636960	5638960	We will be stupid not to do it.
5638960	5641960	But I fully agree with the lobbying power
5641960	5644960	and the equality of arms is not equal.
5644960	5647960	You see it in the fossil industry.
5647960	5650960	You see it in the finance industry.
5650960	5653960	So my call would be to
5653960	5656960	call to arms for
5656960	5659960	raising funds
5659960	5662960	to putting more money
5662960	5665960	in the lobbying effort
5665960	5669960	because I think we are very weak there.
5669960	5672960	And I think the Microsofts.
5672960	5675960	I'm not too sure if a guy like Elon Musk
5675960	5678960	is saying that AI is a threat
5678960	5681960	when he is, you know, what's he doing.
5681960	5684960	So I'm not entirely sure if that's the right...
5684960	5687960	Ok, so I agree with you
5687960	5690960	on the lobbying front, definitely.
5690960	5693960	And what about the international agency for AI?
5693960	5696960	It sounds a bit...
5696960	5699960	I have to think about it to be honest
5699960	5702960	because it sounds like a drastic...
5702960	5705960	I don't think we have a red button
5705960	5708960	or an international police agency
5708960	5711960	that can say stop this.
5711960	5714960	I haven't thought about that.
5714960	5717960	Maybe it's wrong for a politician not to give an answer
5717960	5720960	on the spot directly.
5720960	5723960	Sleep over it, yeah.
5723960	5726960	I'll ask my AI to...
5726960	5729960	Queenie.
5729960	5732960	I would like to read, but you're going to send out the e-mail
5732960	5735960	so I'm going to read all the seven recommendations.
5735960	5738960	I recognize the lobbying part a lot.
5738960	5741960	So what I tried from a Dutch perspective,
5741960	5744960	when you look at the AI Act, they distinguish
5744960	5747960	if an AI is a high risk AI
5747960	5750960	or a low risk AI.
5750960	5753960	And I'm not sure if I follow those categories
5753960	5756960	because I don't think it's about the technology
5756960	5759960	but in which context you use them and with which goal.
5759960	5762960	And I also try to make some low risk
5762960	5765960	AI to try to get them in the higher category
5765960	5768960	which is really difficult.
5768960	5771960	So I really recognize the lobbying part.
5771960	5774960	So maybe it's good also to come together after tonight
5774960	5777960	and to see if we can align
5777960	5780960	on some topics.
5780960	5783960	I wanted to...
5783960	5786960	Maybe one thing that can come close to what you're saying
5786960	5789960	is de Wetenschappelijke Raad voor Regeringbeleid.
5789960	5792960	So that's a group that advises
5792960	5795960	the parliament but also the cabinet, the ministers.
5795960	5798960	And they said you have to work on AI diplomacy.
5798960	5801960	And I think that comes...
5801960	5804960	Well, it doesn't have really overruling power
5804960	5807960	but it comes really close in making sure that you get treaties,
5807960	5810960	make agreements all over the world
5810960	5813960	on how to use AI.
5813960	5816960	So I think that's a good one and at the same time
5816960	5819960	well, if you look at the geopolitical situation right now
5819960	5822960	not everyone listens to international treaties.
5822960	5824960	But I think it's a good start
5824960	5827960	and let's talk about that tomorrow or after more.
5827960	5829960	Tim, your two cents.
5829960	5831960	Right.
5831960	5834960	I don't know a lot about the social technological aspects of this.
5834960	5837960	I'm going to maybe focus on the existential risk part
5837960	5839960	that I know a bit more about
5839960	5842960	because I think Marc already gave a very good summary.
5842960	5845960	So one thing we can do is hope it goes right.
5845960	5848960	I don't think that has a lot of chance.
5848960	5852960	The other thing is we can try to solve this alignment problem
5852960	5855960	either by, as Professor Russell suggested,
5855960	5857960	finding new paradigms
5857960	5859960	or by trying to solve it in a deep learning setting
5859960	5862960	which might be very difficult but maybe it's doable.
5862960	5865960	I don't particularly have any hope
5865960	5867960	in the companies themselves solving this
5867960	5869960	and I also feel like if we want to do this
5869960	5871960	we need a lot more time.
5871960	5875960	And so one way to give us time
5875960	5877960	would be to have these kinds of international regulations
5877960	5879960	that make sure
5879960	5882960	like the open letter suggested, systems like GPT-4
5882960	5885960	or stronger systems like that shouldn't be allowed to be trained
5885960	5888960	for maybe ever or until we solve alignment
5888960	5890960	or six months, I don't know how long it will take.
5890960	5892960	And I think it's very telling that
5892960	5894960	even the tech industry itself is saying,
5894960	5897960	look, world help us
5897960	5899960	because we don't know how to do this
5899960	5901960	and we need more time to solve this.
5901960	5903960	I think maybe we do actually need that kind of drastic action
5903960	5905960	because they're not going to do it by themselves.
5905960	5908960	Ja, they seem to be open minded in some ways to that.
5908960	5910960	Ja, Andy.
5910960	5914960	I can only agree what has been said already
5914960	5916960	and besides that
5916960	5919960	something that was also mentioned in the open letter
5919960	5923960	is to call for a research focus shift
5923960	5926960	from AI capabilities research
5926960	5929960	so making the biggest models even bigger
5929960	5933960	and better and smarter to AI safety research
5933960	5937960	which is research to ensure the beneficial outcomes
5937960	5940960	of these advanced AI models
5940960	5944960	and so I think the Dutch government can play a role in that
5944960	5948960	as well to advocate for more funding towards that
5948960	5951960	and I think the Netherlands is a great place for that as well
5951960	5953960	because we have a lot of technical universities
5953960	5957960	that are highly internationally regarded.
5957960	5959960	So yeah, besides technical research
5959960	5962960	we also need more research for AI governance
5962960	5966960	so we need more robust and effective governance mechanisms
5966960	5971960	en ja, I think we can also play a role in that.
5971960	5973960	Oké, the list goes on.
5973960	5975960	Some questions from the audience.
5975960	5978960	May I see your hands? Ja.
5987960	5990960	Queenie inspires me to ask this question
5990960	5994960	because you seem to refer to one profession
5994960	5998960	or one type of school to take an oath
5998960	6001960	but I would take it one step further.
6001960	6005960	Why do we still have professions and or schools
6005960	6009960	who might be threatening in any way
6009960	6012960	without taking an oath?
6012960	6018960	Shouldn't this oath be obligatory for many more professions or schools?
6018960	6021960	Yes, yes, it should.
6021960	6024960	So actually this was not my idea
6024960	6029960	but there are two female mathematical teachers
6029960	6032960	at I think it was the Delft Technical University
6032960	6034960	and they came to me and they said
6034960	6037960	hey we are trying to adjust the curriculum
6037960	6041960	into making sure that everyone who is going to
6041960	6044960	who is going to this technical university
6044960	6048960	that ethics should be part of all of the studies
6048960	6052960	and they asked well can you help us to give a push
6052960	6055960	so actually it was I didn't steal the idea
6055960	6058960	but I tried to give them a push from a political part
6058960	6062960	and that helped but the goal is not to do it just for AI engineers
6062960	6065960	but you know like because you cannot
6065960	6067960	this is a big responsibility
6067960	6070960	and you shouldn't just put it with just one person
6070960	6073960	but you have to make sure that everyone who works in the field
6073960	6076960	understands what are their human rights
6076960	6079960	how can we make sure that we strengthen them
6079960	6082960	instead of threatening them et cetera
6082960	6086960	so it's actually the whole system needs to be
6086960	6088960	needs to be conscious of that
6088960	6090960	because what we all learned
6090960	6092960	what we all saw the last couple of years
6092960	6096960	is that technical NIT is not just technical
6096960	6098960	it's about the way we live our lives
6098960	6101960	it's about who earns money
6101960	6103960	what information do we see et cetera
6103960	6106960	so we need to make sure that ethical standards
6106960	6109960	are taking into account when it comes to IT broadly
6109960	6112960	because it determines the way we live
6112960	6115960	did I answer your question?
6115960	6117960	Lamert, sorry?
6117960	6121960	I think it's definitely worth pursuing
6121960	6125960	and to instill ethical values in
6125960	6128960	educational systems or professions
6128960	6130960	I think that's a good idea
6130960	6132960	although banker oats
6134960	6136960	why not?
6136960	6138960	we have them
6138960	6140960	we saw what happens
6140960	6142960	but nevertheless
6142960	6144960	when you look at the medical profession
6144960	6148960	it took 2000 years to instill the values
6148960	6151960	that the oath is meaningful
6151960	6153960	so it can work
6153960	6155960	but we know from a banker perspective
6155960	6157960	it's meaningless
6157960	6160960	but I'm sure we can find a balance
6160960	6162960	so I think we should pursue it
6162960	6164960	maybe a start
6170960	6172960	thank you for the interesting discussion
6172960	6174960	I was wondering
6174960	6176960	because there is a lot of progress going on
6176960	6178960	when it comes to AI interpretability
6178960	6182960	and making sure that we understand
6182960	6185960	what kind of representation deep learning models are forming
6185960	6188960	do you think there is any role of AI interpretability
6188960	6192960	in making sure that these systems are safe?
6192960	6195960	that's maybe a question for Tim I guess
6195960	6197960	thanks for the question
6197960	6200960	I mean definitely a big part of
6200960	6203960	AI alignment research or ASafety more broadly
6203960	6206960	should be interpretability for these deep learning systems
6206960	6209960	to give us at least some kind of lens
6209960	6211960	of looking at these systems
6211960	6213960	and maybe understanding a little bit of how they work
6213960	6215960	of what they potentially do before they do it
6215960	6218960	I think the hope of this field
6218960	6220960	might be very difficult
6220960	6223960	in the sense that these models are so huge
6223960	6225960	and there are so many parameters
6225960	6227960	and it's so hard to even understand
6227960	6229960	what small parts of it are doing
6229960	6232960	like if you look at the field of
6232960	6234960	the specific kind of mechanistic interpretability
6234960	6236960	they call it right now
6236960	6238960	we know tiny little things
6238960	6240960	about tiny little parts of the model
6240960	6242960	that give us some kind of idea
6242960	6244960	it's doing it a little bit like this
6244960	6246960	but before we can actually scale that up
6246960	6248960	to
6248960	6250960	just actually understanding what goes on
6250960	6252960	that will take so long
6252960	6255960	and I'm not sure how feasible it is
6255960	6258960	to use that as the main angle of attack
6258960	6260960	I definitely think it's part of it
6260960	6263960	but we need a lot of other approaches as well
6263960	6265960	part of the solution
6265960	6267960	another question there
6267960	6269960	yes
6269960	6271960	thank you for the diverse perspectives you had
6271960	6273960	and I was wondering Mark
6273960	6275960	you mentioned law and policy
6275960	6277960	as one of the key aspects
6277960	6279960	but I think now also with GDPR
6279960	6281960	actually the enforcement
6281960	6283960	is one of the challenges
6283960	6285960	and how would you propose a solution
6285960	6287960	specifically for the enforcement
6287960	6289960	perhaps is it on a national or European level
6289960	6291960	or other further levels as well
6291960	6293960	having a law is one thing
6293960	6295960	but how do you actually enforce it effectively
6295960	6298960	I think the general data protection regulation
6298960	6300960	is really interesting in that
6300960	6302960	Italy in sheer desperation
6302960	6304960	blocked chat GPT last week
6304960	6306960	on the base of GDPR
6306960	6308960	because there was no AI act
6308960	6311960	so I think just harping on about how we need it urgently
6311960	6313960	I think people are learning lessons
6313960	6315960	from the failures of GDPR
6315960	6318960	people realise that the fact that all the big tech companies
6318960	6320960	have their headquarters in Dublin, in Ireland
6320960	6322960	and the fact that the Irish data protection authority
6322960	6324960	is probably the weakest out of all the EU members
6324960	6327960	is something that people in Brussels have realised
6327960	6329960	so under the AI act
6329960	6332960	potentially there will be a centralized office
6332960	6334960	so that will help deal with enforcement
6334960	6336960	because it means that the European Commission can step in
6336960	6338960	when member states do not
6338960	6341960	there is also again lobbying
6341960	6343960	against this AI office
6343960	6346960	and some people are worried about the cost of civil servants
6346960	6349960	that the commission would potentially need to hire for this
6349960	6352960	we've been arguing that this technology is so transformative
6352960	6355960	that it's probably worth a few hundred civil servants
6355960	6358960	but it's really a knife edge vote
6358960	6360960	I think it's about half the European Parliament
6360960	6362960	at the moment that would favour such an office
6362960	6364960	and half that oppose it
6364960	6366960	and would like to see a GDPR type model
6366960	6369960	maybe you have some partners in crime here
6369960	6372960	I'm not sure but they could help out
6372960	6375960	well exactly what Italy did
6375960	6378960	was they took the law that they have on data protection
6378960	6380960	and AVG
6380960	6383960	en they
6383960	6386960	they said let's treat it as a human decision
6386960	6389960	and then it fell short
6389960	6392960	of the decision process and on that grounds you can
6392960	6395960	in fact do enforcement
6395960	6398960	it is a bit like using a hammer when you try to
6398960	6400960	do a screw
6400960	6403960	but it is possible in the area of well wanting for another law
6403960	6406960	but that is very feasible
6406960	6409960	enforcement instruments are in place
6409960	6412960	one question here
6412960	6415960	thank you
6415960	6418960	when it comes to regulation and policy
6418960	6421960	I think the human species has a track record of solving always
6421960	6424960	the last crisis
6424960	6427960	when it comes to existential risk
6427960	6430960	Nick Bostrom also said that we basically have one shot to get this right
6430960	6433960	with human track record
6433960	6436960	human civilization track record inside
6436960	6439960	is that something that should concern us
6439960	6442960	only one shot to get this right
6442960	6445960	and we rather myopic and focus on short term risk
6445960	6448960	who wants to answer, are we optimist here
6448960	6451960	I don't think it's a matter of one shot
6451960	6454960	to be honest
6454960	6457960	you don't think it's one shot
6457960	6460960	I like the question but I think
6460960	6463960	if I try to think about the presentation that we have
6463960	6466960	what I liked about
6466960	6469960	we can have several smaller signs
6469960	6472960	before we get to total extension
6472960	6475960	so that's hopeful for me
6475960	6478960	and yes this is worrying
6478960	6481960	that's why we are here today
6481960	6484960	we have to make sure that more people understand what AI is
6484960	6487960	what are the dangers, how can we make sure it works for us all
6487960	6490960	what are the good things
6490960	6493960	we have a lot of work in society as a whole
6493960	6496960	regulation et cetera
6496960	6499960	but again for me it's hopeful that
6499960	6502960	when I see the difference when the internet started
6502960	6505960	and when big tech companies became big
6505960	6508960	we were too late when it came to
6508960	6511960	regulation of market power et cetera
6511960	6514960	and actually that on European level
6514960	6517960	that they started thinking about the AI act two years ago
6517960	6520960	it's hopeful, it also means that it's two years later now
6520960	6523960	so maybe it's not complete
6523960	6526960	because technology has been developing
6526960	6529960	really quick over the last two years
6529960	6532960	but I think that part is hopeful
6532960	6535960	that we recognize this problem before it's too late
6535960	6538960	and I think it's our responsibility
6538960	6541960	to make sure that we prevent that regulation comes too late
6541960	6544960	so it's a bit late now
6544960	6547960	one last question here
6547960	6550960	thanks
6550960	6553960	I wanted to make
6553960	6556960	a short statement first that
6556960	6559960	we were talking about protests
6559960	6562960	I think that should happen and I want to organize them
6562960	6565960	this year so if
6565960	6568960	who's coming
6568960	6571960	called safe transition
6571960	6574960	safe transition to the machine intelligence era
6574960	6577960	but thank you
6577960	6580960	my question is to Mark
6580960	6583960	and
6583960	6586960	it's about the EU regulations
6586960	6589960	and
6589960	6592960	my current understanding is that
6592960	6595960	they are only focusing on deployment
6595960	6598960	and not on the training
6598960	6601960	so that
6601960	6604960	they are in effect not protecting us
6604960	6607960	from the existential risk of
6607960	6610960	an AI that secretly breaks out
6610960	6613960	and goes and
6613960	6616960	does its plan to take over the world
6616960	6619960	in some other server
6619960	6622960	data center so my question is
6622960	6625960	is there something in the EU process
6626960	6629960	is
6629960	6632960	just like the sparks of AGI paper by Microsoft
6632960	6635960	the EU AI act I think is a spark of hope
6635960	6638960	but you're completely right it's not more than that
6638960	6641960	because what they've basically done is they've taken a product safety regulation
6641960	6644960	like of any type you have in Europe so basically
6644960	6647960	the one that regulates the toy market
6647960	6650960	and then they've said okay we'll apply that to AI products
6650960	6653960	so it only starts to kick in once
6653960	6656960	we want to put it on the market
6656960	6658960	so if you're training it, if you're testing it
6658960	6661960	it's all fine it's completely unregulated
6661960	6664960	and we definitely need rules for that
6664960	6667960	I think most AI researchers
6667960	6670960	feel that we need to start looking seriously at companies
6670960	6673960	that have a huge amount of computational power
6673960	6676960	there was someone writing an article
6676960	6679960	the former advisor to the UK prime minister on technology
6679960	6682960	who observed that open AI by itself
6682960	6685960	in California has 25 more sort of GPUs
6685960	6688960	than the entire United Kingdom
6688960	6691960	so their compute power is about 25 times the size of the UK
6691960	6694960	those are the sources of worry
6694960	6697960	and I think we need to start regulating
6697960	6700960	and inspecting and monitoring and verifying those companies
6700960	6703960	before ideally
6703960	6706960	I think they've developed their product and there's no way
6706960	6709960	we can still change it or make it safer
6709960	6712960	we don't want to create risks that maybe something happens before it's deployed
6712960	6715960	so I think you're completely right
6715960	6718960	we need a bunch of extra regulation
6718960	6721960	and we also desperately need the United States
6721960	6724960	because that's where most of these companies are based
6724960	6727960	and it's super, like it's great that we have European regulation
6727960	6730960	but without the US this existential risk is not going to go away
6730960	6733960	we need to tackle this problem globally
6733960	6736960	if I could add something to that maybe
6736960	6739960	yes, I think we should be worried about that
6739960	6742960	because there are still these scenarios
6742960	6745960	like the one you mentioned
6745960	6748960	where we are not protected and we do kind of only have one chance
6748960	6751960	and for those kinds of things it is
6751960	6754960	I think very important to target the bottlenecks of these kind of systems
6754960	6757960	which right now is just the model training
6757960	6760960	you need so much more computational power
6760960	6763960	to train these models and to deploy them
6763960	6766960	the easiest and the most obvious thing to regulate
6766960	6769960	of course it might be very hard to regulate in practice
6769960	6772960	but if you target that part
6772960	6775960	then you actually have a better chance at stopping these kinds of models
6775960	6778960	is that part of your seven recommendations
6778960	6781960	it's number two
6781960	6784960	well that's one of the commitments I already heard
6784960	6787960	that you're going to discuss these seven recommendations
6787960	6790960	I also heard the idea of trying to
6790960	6793960	strengthen AI safety research
6793960	6796960	and ethics in different engineering programs
6796960	6799960	increasing funding and trying to fight the big lobbying power
6799960	6802960	of the tech industry
6802960	6805960	so quite some commitments that have already been uttered here on this stage
6805960	6808960	we've come to the end of this evening
6808960	6811960	but actually it's only the start
6811960	6814960	because there are drinks later on
6814960	6817960	to continue the conversation
6817960	6820960	about all the great catastrophic risks
6820960	6823960	that have come to the fore this evening
6823960	6826960	or maybe you're very hopeful about humanity
6826960	6829960	steering away from the cliff
6829960	6832960	doesn't really matter, both are great reasons for a good drink and chat
6832960	6835960	so I advise you all to the bar
6835960	6838960	that's in the hall
6838960	6841960	down the hole there
6841960	6844960	I want to give a big round of applause to our five panelists
6847960	6850960	applause
6850960	6853960	applause
6853960	6856960	en some flowers
6856960	6859960	we'll probably meet again
6859960	6862960	during the demonstration
6862960	6865960	when will it take place
6865960	6868960	to be decided
6868960	6871960	cliffhanger, we'll see each other at the dam square
6871960	6874960	somewhere next year
6874960	6877960	we hope to see you again
6877960	6880960	applause
6880960	6883960	applause
6883960	6886960	applause
