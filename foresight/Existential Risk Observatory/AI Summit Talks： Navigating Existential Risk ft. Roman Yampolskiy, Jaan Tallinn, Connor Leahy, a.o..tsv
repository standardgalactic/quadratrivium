start	end	text
0	8680	I encourage you to take a seat. We will be starting almost on time because we have a
8680	22080	very rich agenda on a very big topic. We are talking about navigating existential risk.
22080	28680	Navigating what people have described as a very difficult, tortuous landscape of risks
28680	38040	that are made worse by oncoming new frontier AI. That's not just the AI that we have today,
38040	46120	but AI that we can fairly easily imagine as coming within the next year or two. Next generation
46120	54000	AI that's more powerful, more skillful, more knowledgeable, potentially more manipulative,
54040	61320	potentially more deceitful, potentially more slippery, definitely more powerful than today's AI.
61320	69480	That AI might generate existential risks in its own right. That AI is likely also to complicate
69480	77720	existing existential risks, making some of the risks we already know about more tricky to handle,
78480	86200	more wicked. We might also talk about the way in which next generation AI might be the solution
86200	93800	to some of the existential risks and dilemmas facing society. If we can apply AI wisely,
93800	103240	then perhaps we can find the narrow path through this difficult landscape. Welcome navigators in
103240	110960	the hall. Welcome to navigators watching the live stream. Welcome to people and AIs watching the
110960	120080	recording of this discussion. Let's get stuck in. We have lots of very capable, knowledgeable
120080	126880	speakers who will approach this from a diversity of points of view. Indeed, I think one of the hazards
126880	132560	in this whole topic is that some people want to be a little bit one-dimensional. They want to say
133280	138240	this is how we'll solve the problem. It's quite straightforward. In my view, there are no
138240	143560	straightforward solutions here, but you can make up your own minds as you listen to what all the
143560	148480	speakers and panelists have to say. And yes, in the audience, you'll have a chance later on to
148480	154400	raise your hand and get involved in the conversation too. The first person you're going to hear from
154400	162080	is unfortunately not able to be with us tonight, but he has recorded a short video. He is Sir Robert
162160	169280	Buckland, MP, former Lord Chancellor of the United Kingdom, which means he was responsible for the
169280	176320	entire justice system here, former Secretary of State for Wales. He is still an MP and he has a
176320	184880	side hustle as a senior fellow at the Harvard Kennedy School, where he is writing papers on
184880	191520	exactly the topics we're going to be discussing tonight, namely how does AI change key aspects of
191520	199440	society, potentially making it better, potentially making it much worse if we are unwise. So let's
200000	204160	watch Sir Robert Buckland who will appear by magic on the big screen.
205680	212160	Well, I'm very pleased to be able to join you, albeit virtually, for the Conjecture ARO Summit
212160	219360	on AI and the challenges and opportunities that it presents us. And I think my
220240	225120	pleasure at being with you is based upon not just my own experience in government,
225760	234240	but also my deep interest in the subject since my departure from government last year. Now when I
234240	241040	was in government, I had responsibility for, for many years, the legal advice that was given to
242160	247040	departments and indeed to the government in general when I was in the Law Offices Department as the
247040	253440	Solicitor General. And then responsibility for running the Ministry of Justice's Lord Chancellor
253440	260000	and Secretary of State for over two years before a brief return as Welsh Secretary last year.
260720	266000	That seven years or so experience within government as a minister gave me, I think,
266000	272160	a very deep insight into the pluses and the minuses of the way the government works, the
272640	282400	efficiencies and indeed the inefficiencies about process. And I think clearly, as in other walks of
282400	288400	life, artificial intelligence, machine learning will bring huge advantages to government processes,
288400	296320	to improve efficiency, to speed up a lot of the particular ways in which government works,
296320	301440	which will be, I think, to the benefit of citizens, whether it's citizens waiting for
301440	307840	passport applications, visa applications, or other government processes, benefits, for example.
308480	315440	However, I think that we kid ourselves if we don't accept the fact that alongside the benefits
315440	322800	come potential pitfalls. And the first and most obvious one, I think, for me is the scrutability
322800	329200	of process. In other words, the way in which we understand how decisions are made. And that's
329200	334560	very important, because understanding how decisions are made is part of democratic
334560	339920	accountability in societies like ours, where individuals or organizations wish to challenge
340560	344880	decisions made by government, perhaps through judicial review applications,
344880	351040	then the explicability of those decisions, which is accompanied by a duty of candor
351040	358400	by the government in order to disclose everything about those decisions, is part of that accountability.
358400	363280	And of course, it's sometimes very difficult to explain how the machine has come to decisions.
364080	369360	And more fundamental than that, we have to accept that if the data sets that are used in order to
369360	378320	populate the processes are not as full of integrity as they should be, and are not the product of
378320	384160	genuinely objective and carefully calibrated processes, then we are in danger of importing
384240	389920	historic biases into the system, whether it's biases against neurodiverse people making job
389920	395200	applications, or indeed biases against people of color in the criminal justice system,
395760	402560	simply because the data sets have imported those historical anomalies, those historical imbalances.
403200	409520	Now, all those questions have really got me thinking very deeply about the impact of machine
409520	416160	learning on the ethics of justice itself. And as a result of my thinking, I was delighted last
416160	421040	year to be accepted as a senior fellow at the Mosova Romani Center for Business and Government
421040	426640	at Harvard Kennedy School. And I am working currently on a number of papers relating to
427520	434800	the impact of AI and machine learning on the administration of justice and the law itself.
435760	443040	It really developed from my own experience as a law chancellor, from digitalization,
443040	450480	I should say, of the courts, when during the COVID pandemic, we had to move many, many thousands
450480	457520	of hearings online for the first time. I think we jumped from a couple of hundred phone or online
457600	466880	hearings to 20,000 a week in a very short compass. And the status quo will never be the same again.
466880	472400	In fact, it has moved on, I think, in a way that we just hadn't foreseen before the pandemic. Now,
472400	477040	I think that's a good thing. But I also think that accompanying this question about increased
477040	483280	efficiency is the use of artificial intelligence. Now, in some jurisdictions, such as China,
483280	489360	we are seeing its increased use not just to do legal research and to prepare cases,
489360	495840	but to actually decide themselves. In other words, the AI judge. Now, that's all well and good.
496400	502640	But do we actually know what populates the data sets that then forms the basis of the decisions
503360	510560	made? And I think it's that unintentional bias or indeed worse than that potential
510640	517200	intentional bias, whether that's influenced by a government or indeed a corporate that
517200	524880	might be able through their financial means to influence a procedure or indeed the way in which
524880	532480	we deal with cases, knowing as we might do more information about the way in which judges make
532480	538560	their decisions. All these questions, I think, need to be asked now before we end up in a position
538560	544560	where we've slept, walked into a completely different form of justice from the one that we know.
545600	550640	Now, underpinning all of this, I think, is the need to ask a fundamental question
550640	555840	about judgment itself. And that's what I've been doing in my first paper. You know, the essence
555840	561440	of human judgment is something that will be based not just upon an understanding of the law, but
561440	567360	on our experiences as human beings. And you can go right back, as I have done, to the judgment of
567360	575360	Solomon and his emotional response to the woman who clearly was the true mother of the child that
575360	582160	he proposed to be cut in half. Now, you know, that's an example, I think, of the human element of
582160	591440	judgment, which has to be an essential foundation of decision making, particularly when it comes to
591520	597760	the assessment of credibility of a witness, a human witness giving evidence upon which the case
598320	604400	stands or falls. And of course, for judge, that applies for juries as well in criminal trials,
604400	611360	particularly here in the UK. Now, you know, all these questions, I think, need to be asked.
612160	616480	And then we need to work out what it is that we want to retain out of all of this.
616480	620800	Now, I don't think we should make any cosy assumptions that because at the moment some
620800	632080	large learning systems are having hallucinations. I don't think we should be assuming that just
632080	638720	because of that, therefore AI will never work in a way that can achieve a greater degree of
638720	648000	certainty. I think the inevitable arc of development will result in better and better and more
648000	654800	capable machines. That's inevitable. But what we must be asking at the same time as capability
654800	660080	is ensuring there is greater security and safety when it comes to the use of AI.
660080	664880	And that really underpins, I think, the work that I'm doing in the field of justice. What
664880	671840	does this all lead to then? Well, we have the AI safety summit in the UK next month. I very much
671840	678800	hope that that summit will first of all involve those critical players in terms of international
679920	687200	organisations and key countries as well that will come together to commit to creating,
687200	692400	I think, a defined framework within which we should be using AI safely. And that framework,
692400	697360	I think, will have to take several forms. I think in the field of justice we could do with an
697360	704560	international framework of principles, which will ensure transparency and which can reassure
704560	710480	people that in cases of the liberty of the individual, criminal cases, cases where perhaps the
710480	718480	welfare of a child and the ultimate destination of a child is in issue, then the human element
718480	725360	will be the key determinant in any decisions that are made. And that the use of machines will be
725360	732960	transparent and made known to all the parties throughout the proceedings. And then other walks
732960	740080	of life, I think the AI safety summit has to then look as well at whether frameworks can be created
740080	745680	and what form they should take. I think it's tempting to try and be prescriptive. I think
745680	751360	that would be a mistake, not just for the obvious reason that AI is developing and therefore anything
751360	757840	that we write in 2023 will soon be out of date, but the very fact that AI itself does not mean an
757840	765600	alloyed harm. In fact, it means a lot of benefit and also some neutral effects as well. And where
765600	772480	you have that approach, then a principle-based system seems to me to be more sensible than
773120	778240	overly prescriptive and detailed rules as you would have, for example, to prevent a crime,
778240	785040	such as fraud. So just some preliminary thoughts there as to the impact of machine learning.
785040	792320	I don't pretend to be a technical expert. I'm not. But my years in justice, my years as a lawyer,
792320	799520	a judge and as a senior cabinet minister, I think obliged me to do some of the thinking now
800000	806720	to help ensure that countries like Britain are in the forefront of the sensible and proportionate
807360	813680	regulation of the use of machine learning and other types of artificial intelligence. If we
813680	820480	don't do it now, then I think we'll be missing an unhistoric opportunity. I wish you all well,
821040	824800	and I look forward to meeting some of you in the future and discussing these issues
825520	828080	as they develop. Thank you very much.
829920	833200	Well, thank you, Sir Robert, who may be watching the recording of this.
834560	840960	Don't be prescriptive, he said. Let's sort out some sensible proportionate regulation.
841840	847520	Is that credible? Is that feasible? You'll be hearing from other panellists who may be commenting on
847520	855440	that shortly. So Robert also said there are risks such as the inscrutability of AI. We don't
855440	861600	understand often how they reach its decisions. We don't understand the biases that might be there,
861600	867120	that might have been planted. We might lose charge. We might become so used to AI taking
867120	874240	decisions that humans end up in a very sad place. But how bad could things get? That's what we're
874240	879920	going to hear from our next speaker. So I'm going to ask Conor Lehi to come up to the stage,
879920	887440	who I briefly introduce him. Conor is the CEO of Conjecture. If you haven't heard about Conjecture,
887440	892320	I think you need to do a bit more reading. Perhaps Conor will say a little bit about it. They are
893200	900480	AI alignment solutions company, international, but with strong representation here in the UK.
900480	913440	So welcome Conor, the floor is yours. Thank you so much. It's so great to see you all today.
913440	920160	So happy to be able to talk to you here in person. And man, do we live in interesting times, to put
920160	929360	it lightly. The world has changed so much. Just in the last few years, few months even, so much
929360	935840	has happened in the world of AI and beyond. Just a mere couple of years ago, there wasn't
935840	944000	such a thing as chat GPT, or even GPT3, or 4, or 2, or any of those. It was a different world
944000	951200	not too long ago when technologists such as myself, weird little hobbyists, worried about
951200	959760	the problem of AGI and how it will affect the world. Back then, it still seems so far away.
960480	968880	It seemed like we still had time. But now, we find ourselves in a world of unrestricted,
969440	978080	uncontrolled scaling, a race towards the finish, towards the end, to scale our AI systems ever
978080	985760	more powerful, more general, more autonomous, more intelligent. And the reason I care about this
986400	992480	is very simple. If we build systems that are smarter than humans, that are more capable
992480	1000080	at manipulation, deception, politics, making money, scientific research, and everything else,
1000880	1006880	and we do not control such systems, then the future will belong to them, not to us.
1008800	1017280	And this is not the future I want. I want a future in which humanity gets to decide its destiny,
1017840	1022240	or we get to decide the future for ourselves, for our children, for our children's children,
1023520	1030320	that we like. The future where our children can live long, happy lives surrounded by beauty,
1030320	1036560	art, great technology, instead of being replaced by SOA's automata. And let me be clear,
1037760	1046240	that this is the default outcome of building an uncontrolled AGI system, the full replacement
1046240	1056240	of mankind. And what we're seeing is that AI is only exponential. There's a race.
1056800	1064560	All the top organizations, which is OpenAI, DeepMind, Anthropic, among others, are racing ahead
1065280	1073520	as fast as the VC dollars would scale up their work. And this has given us an exponential.
1073520	1080320	AI is on an exponential curve, both on hardware and on software. It's improving at incredible rates.
1081120	1087920	And when you're dealing with an exponential, there are precisely two times you can react to it,
1088880	1097840	too early or too late. There is no such thing as reacting at just the right moment on an exponential,
1097840	1103200	where you find just the perfect middle point, just in the nick of time, when everyone agrees
1103200	1109360	that the problem is here and everything has perfect consensus. If you do this, you are too late.
1110320	1119760	It will be too late. And the same thing applies to AGI. If we wait until we see the kinds of
1119760	1125920	dangerous general purpose systems that I am worried about, then it will already be too late.
1127120	1136000	By the moment such systems exist, the story of mankind is over. And so if we want to act,
1136000	1142400	we must act well, well before such things actually come into existence.
1143440	1147680	And unfortunately, we do not have much time. How the world has changed.
1149280	1156720	As frightening and as terrible the race may be, there's also good changes.
1158000	1163440	A few years ago, I could have barely imagined seeing governments, politicians, and the general
1163440	1169600	public waking up to these weird nerd issues that I cared about so much with my friends online.
1170480	1175520	But now we're looking forward to the first international AI summit convened by the UK
1176320	1182720	and the famous Dip Bletchley Park. And this is great news. The European Commission has recently
1182720	1188640	officially acknowledged the existential risks from AGI along with the risks from nuclear weapons
1188640	1195760	and pandemics. This is great progress. This is fantastic. It is good to see our governments
1195760	1200560	and our societies waking up and addressing these issues, or at least beginning to acknowledge them.
1202240	1207440	And we must use this opportunity. We have an opportunity right now, and we must prevent it
1207440	1214160	from being wasted. Because there's also bad news. But we're having this great opportunity
1214800	1218880	to start building the regulation and the coordination necessary for a good future.
1220480	1226160	The very people who are creating these risks, the very people at the heads of these labs,
1226160	1231520	these organizations, building these technologies, are the very people who are being called upon
1231520	1236480	by our governments to help regulate the very problem that they themselves are creating.
1237280	1243440	And let me be very explicit about this. The problem that we face is not AGI.
1244160	1252080	AGI doesn't exist yet. The problem we face is not a natural problem either. It is not an external
1252080	1260720	force acting upon us from nature. It comes from people, from individual people, businessmen,
1261360	1267520	politicians, technologists, athletes, large organizations, who are racing, who are scaling,
1267520	1272560	who are building these technologies, and who are creating these risks for their own benefit.
1274320	1278560	But they have offered us, these very people who are causing this problem,
1279280	1286560	have offered us a solution. Fantastic. And they are pushing it as hard as they can
1286560	1293120	towards the UK government and the upcoming summit. So what is the solution? The solution to the problem
1293120	1301840	of scaling of these labs, these acceleration labs such as Anthropic and ARC have been pushing for.
1301840	1308080	What is the solution? Well, the solution to the scaling problem is called responsible scaling.
1308880	1314800	Now, what is responsible scaling, you might ask. You see, it's like normal scaling except you put
1314800	1323040	the word responsible in front of it, and that makes it good. So of course I'm joking somewhat,
1324080	1331760	but there's a lot of truth in humor. Responsible scaling is basically the policy
1332640	1337760	and you can read this on both ARC or philanthropic's website. It's the policy proposal
1338320	1345360	that we should continue to scale uninhibited until at some future time when tests and evaluations
1345360	1350000	that do not yet exist and we do not know how to build, but the labs promise us they will build,
1351040	1357600	detect some level of dangerous capabilities that we do not yet know, and then once it gets to that
1357600	1363840	point, then they will stop, maybe, except there is a clause in the Anthropic version of the RSP
1363840	1370000	paper in which they say that if a different organization was scaling even supers unsafely,
1370000	1378240	then they can break this commitment and keep scaling anyways. So this could be sensible
1378880	1385680	if they committed to a sensible bound, a conservative point on which to stop, but unfortunately the
1386640	1392480	responsible scaling policy RSP fails to actually commit to any objective measure whatsoever.
1393120	1400800	Oops. So effectively the current policy is to just keep scaling until they feel like stopping.
1402960	1407760	This is the policy that is being suggested to our politicians and to the wider world
1407760	1415280	as the responsible option for policy makers. It is trying to, is very clear that it is trying
1415280	1423040	to recast this techno-libertarian extremist position as sensible, moderate, responsible even.
1423680	1432080	Now, in my humble opinion, the reasonable moderate position to when dealing with a threat that is
1432080	1436000	threatening the lives of billions of people is to simply not do that.
1438400	1445760	But instead, there is trying to pass off this as the sensible middle ground position.
1447440	1454240	The truth of RSP is that it comes from the same people who are causing this risk to exist.
1455920	1461520	These people, the heads of these labs, many of the scientists and the policy people and
1461600	1468160	the other people working on this have known about existential risks for decades and they fully admit
1468160	1472480	this. This is not like they haven't heard about this. It's not even that they don't believe it.
1473040	1477920	You can talk to them. They're on the record talking about how they believe that there is a
1477920	1486080	significant chance that AGI could cause extinction of the entire human species. In a recent podcast,
1486080	1491600	Dario Amade, the CEO of Anthropic, one of these labs, himself, said that he thinks it's a
1491600	1497840	probably 25% chance that it could kill literally everybody. And they're doing it anyway.
1498640	1504160	Despite this, they keep doing it. Why? Well, if you were talking to these people, what they might
1504160	1510480	tell you is that, sure, you know, I know it's dangerous. I am very careful. But these other guys,
1511040	1515440	well, they're even less careful than me. So I need to be number one. So I actually
1515440	1519200	have to race faster than everyone else. And they all think this about each other.
1520880	1526720	They call this incremental, but they never pause. They always race as fast as they possibly can.
1527920	1534960	Do as I say, not as I do. There is a technical term for this. It's called hypocrisy.
1535840	1543280	And RSP is no different. They are simply trying to twist words in an Oralian way
1544480	1548160	to be allowed to keep doing the thing that they want to do anyways,
1549600	1559360	which they themselves say could risk everybody. I mean, has responsible in the name, must be good.
1559360	1568640	And people like Sam Altman talk about iterative deployment, about how we must iteratively release
1568640	1575360	AI systems into the wild so societies can adapt to them, be inoculated by them. It sounds so nice.
1575360	1582080	That sounds almost responsible. But if you're really trying to inoculate someone, you should
1582080	1589120	let the host actually adapt before you jam in the next new pathogen into their weakened immune system
1589120	1595440	as fast as you possibly can. But this is exactly what laboratories such as Open AI, DeepMind,
1595440	1599840	Anthropic, and Tier 2 labs such as Meta are doing with all the force they can muster.
1600480	1606160	To develop more and more new systems as fast as possible, release them as fast as possible,
1606160	1613440	wide as spread possible. Now, if Open AI had developed a GPT-3 and then completely stopped
1613440	1618480	further scaling, focused all of their efforts on understanding GPT-3, making it safe, making
1618480	1624480	it controllable, working with governments and civil society to adapt the new problems
1624480	1631200	posed by the system for years or even decades, and then they build GPT-4? Yeah, you know what?
1631200	1636720	Fair enough. I think that could work. That would be responsible. But this is not what we're seeing.
1639760	1645360	All of these people and all of these institutions are running a deadly experiment
1646080	1653520	that they themselves think might cause extinction. It is gain-of-function research on AI,
1653520	1660160	just like viruses, developed and released to the public as fast and aggressively as possible.
1662240	1666880	They're developing more and more dangerous and more and more powerful viruses as quickly as
1666880	1676560	possible and forcing it into everyone's immune system until they break. There is no responsible
1676560	1684320	gain-of-function research for extinction-level threats. There is no such thing. We have no
1684320	1691120	control over such systems and there is no responsible way to continue like this. And anyone
1691120	1701520	who tells you otherwise is lying. A lot has changed. The summit can lead to many boring outcomes,
1702800	1707920	just exchanges of diplomatic platitudes as is often the outcome of such international events.
1709600	1716960	They have some good outcomes and it can have some very, very bad outcomes. Success in the summit
1717440	1723440	is progress towards stopping the development of extinction-level AGI before we know how to control
1723440	1732640	it. Most other outcomes are neutral and bad outcomes. They look like policymakers blindly
1732640	1739520	and sheepishly swallowing the propaganda of the corporations to allow them to continue their
1739520	1746400	unconsciously dangerous gamble for their own personal gain and glory at the expense of the
1746400	1755760	entire planet. We owe it to ourselves and our children to build a good future, not gamble it
1755760	1765840	all on a few people's utopian fever dreams. Governments and the public have a chance to regain
1765840	1771200	control over the future and this is very hopeful. I wasn't sure we were going to get it, but the
1771200	1777360	summit speaks to this, that people can act, that governments can act, that civil society can act,
1778080	1785680	that it is not yet too late. There is simply no way around it. We need to stop the uncontrolled
1785680	1795200	scaling, the uncontrolled race if we want a good future. And we are lucky because we can do this.
1795200	1802720	We can cap the maximum amount of computing power going into these AI systems. We can have government
1802720	1809200	intervene and prevent the creation of the next more dangerous, more general, more intelligent
1809200	1817280	strain of AI until we are ready to handle it. And don't let anything distract you from this.
1817440	1825200	There is no good future in which we continue on this path and we can change this path.
1827040	1832160	We need to come together to solve these incredibly complex problems that we are facing
1832800	1838400	and not let ourselves be led astrayed by corporate propaganda. And I hope that the governments
1838800	1845760	and a civil society of the world do what needs to be done. Thank you.
1858400	1863520	Thank you, Conor. We'll take questions from the floor in a moment. I'll just start off with the
1863600	1869520	question I think maybe on many people's minds. Why would a super intelligent AI actually want
1869520	1874400	to kill humans? I have a super intelligent calculator which is no desire to kill me. I
1874400	1879680	have a super intelligent chess playing computer that is no desire to kill me. Why don't we just
1879680	1885920	build as responsible scaling an AI that has no desires of its own? Because we don't know how
1885920	1892720	to do that. Why did Homo sapiens eradicate Homer neanderthalis and Homo erectus and all the other
1892720	1898000	species that we share the planet with? You should think AGI, not of as a calculator,
1898000	1904320	but as a new species on our planet. There will be a moment where humanity is no longer the only
1904320	1911680	or even the most intelligent species on this planet and we will be outcompeted. I don't think it
1911680	1917920	will come necessarily from malice. I think it will be efficiency. We will build systems that make
1917920	1924080	money that are effective at solving tasks, at solving problems, at gaining power. These are
1924080	1930000	what these systems are being designed to do. We are not designing systems with human morals and
1930000	1935120	ethics and emotions. They're AI. They don't have emotions. We don't even know how to do that. We
1935120	1940400	don't even know how emotions work. We have no idea how you could get an AI to have emotions like a
1940400	1946720	human does. So what we're building is extremely competent, completely sociopathic, emotionless,
1946720	1951440	optimizing machines that are extremely good at solving problems, extremely good at gaining
1951440	1956960	power, that do not care about human values or emotions, never sleep, never tire, never get
1956960	1963520	distracted, can work a thousand times faster than humans and people will use these for many reasons
1963520	1969200	to help and people and eventually I think humanity will just no longer be in control.
1969920	1974960	Questions from the floor. There's a lady in the third drawer down here. Just wait for the mic,
1974960	1982560	sorry, so that the audience online can hear you. Thank you, Susan Finnell from Finnell Consult.
1983920	1988160	To stop the arms race, certainly at a geographical level, I mean in nuclear,
1990320	1995040	the states and Europe can tell which countries are building nuclear weapons and what they've got
1995040	2002560	and they can do tests. If computing power is a thing that needs to be capped to slow this down
2002560	2010240	enough, is there a way to monitor what other countries or people in a clandestine way are doing
2010240	2015760	and how does that work? This is a fantastic question and the extremely good news is yes,
2016880	2023520	the at least currently, this will change in the near future, but the current state to build frontier
2023520	2030560	models requires incredibly complex machines, massive supercomputers that take megawatts of energy.
2030560	2036320	So this is on the order you'd have of like a nuclear centrifuge facility. So these are massive,
2036320	2042160	huge machines that are only built by basically three or four companies of the world. There are
2042160	2047680	very, very few companies and there is extreme bottlenecks on the supply chain. You need very,
2047680	2053520	very specialized infrastructure, very specialized computer chips, very specialized hardware to
2053520	2058240	be able to build these machines and these are produced exclusively by countries basically in
2058240	2064640	the West and Taiwan. There is many ways where the US or other intelligence services can and already
2064640	2070480	are intervening on these supply chains and it would be very easy to monitor where these things are
2070480	2077040	going, who is buying them, where is energy being drawn in large scales. So it is not easy and the
2077040	2082480	problem is that AI is unexponential both with hardware and with software. Eventually it will be
2082480	2089200	possible to make essentially dangerous AGI on your home laptop probably, maybe not, but it seems
2089200	2095840	plausible. If we get to that world, we're in big trouble. So this is part also why we have to buy
2095840	2101840	time. At some point there will be a cutoff where we'll have algorithms that are so good that either
2101840	2107200	we have to stop everyone from having a PlayStation at home, which doesn't seem that plausible,
2107840	2112480	or at that point we have to have very good global coordination and regulation.
2114080	2117840	Thanks. Just past the mic behind you, there's a person in the row behind.
2118720	2125360	Robert Whitfield from One World Trust. Can I ask about Bletchley Park? Do you know, I mean are
2125360	2133600	you participating and if not, do you know anybody else with similar views to you who is participating?
2134560	2138400	I can't comment too much since it's closed doors. It's a very private event,
2138400	2142640	unfortunately, so I don't think I have the liberty to talk about exactly what I know.
2142640	2148320	I think the guest list is not public. I don't know most of the people who are coming. I know
2148320	2153840	the obvious ones. All the CEOs of all the top labs, of course, are attending. It's not a secret.
2154800	2160400	I don't know who, if anyone, of my reference class is attending.
2160480	2167920	Just past the mic next to you, Robert. Thank you. Perhaps I can answer that question. Anybody
2167920	2177280	that has read The Guardian today, there is an interview with Clifford and for the very first
2177280	2184160	time, not for the second time, it has been clarified that there will be only about 100 people
2184160	2191680	participating on the first day. Anybody is invited, including China, on the second day,
2192480	2200720	apparently there will be only the coalition of the willing. So those who subscribe to the frontier
2200720	2208000	model forum, they will sit on the second day. That's the current question. My main impression
2208000	2213680	from that article generates very positive. I would say I've been surprised, as you would be
2213680	2222400	surprised, that the UK government is really doing what it can to get the mission to what the title
2222400	2228800	of the conference says, the AI safety summit. It's not about regulation, it's about controlling AI,
2228800	2234160	and they're trying to do their best. The problem is, as outlined in that interview,
2234960	2241760	is that we seem to be alone. We have the states a little bit, but the rest wants to go their own
2241760	2249120	way and do it on their own territory, which is, I think, the tune. I agree. Sooner or later,
2249120	2253600	international coordination around these issues will be necessary. It is as simple as that. If
2253600	2258800	you want humanity to have a long, good future, we need to be able, as a global civilization,
2258800	2263280	to handle powerful technologies like this. Take a question right from the back.
2267280	2273040	In terms of legislation, what kind do you think is most effective? I've heard, for example,
2273040	2279840	liability law takes too long to actually have an effect, and compute governance generally seems to be
2280400	2289840	very easy to be called totalitarian. What do you think of legislation such as models must be released
2289840	2297040	with a version before pre-processing, and there'll be attacks on the number of harmful outputs done
2297040	2303920	by the model before the pre-processing? I am open to many kinds of regulation, per se. I would strongly
2303920	2308000	disagree with the description of compute governance. This is like saying that, you know, not being
2308000	2312320	private citizens not having nuclear weapons is totalitarian. I respectfully disagree. I'm quite
2312320	2316160	happy that people do not have private nuclear weapons, and I do not think that people should
2316160	2323360	have private AGI's. Similarly, I think liability is very promising. I think it has to be strict
2323360	2328800	liability, so liability for developers rather than just users. This aligns the incentives of
2328800	2333680	developers with those of wider society. The point of liability is to price in the negative
2333760	2339040	externalities for the people actually causing them, so I'm a big fan of this. A third form of
2339040	2344720	policy I would also suggest is a global AI kill switch. This would be a protocol where
2345520	2351440	some number of countries or large organizations participate, and if some number of them decide
2351440	2357840	to actually do this protocol, all major deployments of frontier models must be shut down and taken
2357840	2364000	offline, and this should be tested every six months as a fire drill for five minutes to ensure full,
2364960	2369680	so that hopefully we never need it, but if we do, that at least the protocol exists.
2370560	2374640	Thank you very much. There are lots of hands up. Hold your questions. There will be more
2374640	2379520	chance for Q&A later. Corner final remarks before we hand over to the next speaker.
2380160	2385840	I want to really say that I do agree that it is very hopeful to see that the UK is trying to do
2385840	2390400	things and is trying to push us forward in the good world, because what we really need,
2390400	2398960	as I said briefly, what we need is as a civilization to mature enough to be able to handle dangerous
2398960	2405360	technology. Even if we don't build AI right now, at some point we will build something so powerful
2405920	2410080	that it can destroy everything. It's just a matter of time. Our technology keeps becoming more
2410080	2416240	powerful. The only way for us to have a long-term good future is to build the institutions, the
2416240	2424160	civilization, the world that can handle this, that can not build such things, that can not hold
2424160	2430160	the trigger. I do think this is possible. I do think that it is, in fact, so I have heard,
2430160	2435920	in the interest of most people, to not die. I think there is a natural coalition here,
2436000	2441040	but it is hard, and I will not deny this extremely challenging problem, almost unlike,
2441680	2445440	I mean, basically something we haven't faced in this nuclear proliferation, and even then it's
2445440	2451360	even worse this time. It's a incredibly difficult problem. It is not over yet, but it could be
2451360	2457040	very soon. If we don't act, if we let ourselves get distracted, if we fall for propaganda and all
2457040	2463440	these things, these opportunities can be gone, and that will be it. But the game is not over yet,
2463520	2465360	so let's do it. Thank you very much.
2474080	2478960	So we've heard from a politician, a senior politician. We've heard from a technology
2478960	2485760	entrepreneur and activist. We're now going to hear from a professor who is zooming in
2485760	2491120	all the way from Kentucky from the University of Louisville. He's an expert. He's written several
2491120	2497600	books on cybersecurity, computer science, and artificial superintelligence. Ah, Roman,
2497600	2504000	I see you on the screen. I hope you're hearing us. Tell us, can we control superintelligence?
2504720	2509280	Over to you. No. The answer is no. I'll tell you why in a few minutes.
2513440	2517440	That's fine. So you can share your slides or talk to us whenever you're ready.
2518320	2521920	Let's do the slides. Connor did a great job with
2524080	2529200	his presentation. Let me see one second here.
2533120	2536320	In the meantime, we can see the covers of some of your books in the background.
2537120	2538240	Yes, absolutely.
2538240	2541760	Security and artificial superintelligence.
2542400	2550400	We're now having a slight technical issue as the technologist is found to slides. Great.
2551120	2555440	Okay. Yeah, that's the hardest part. If I can get slides going, the rest is easy.
2556720	2559920	Okay, so I didn't know what Connor's going to talk about.
2561840	2566080	He did a great job. He's a deep thinker and covered a lot of important material.
2566720	2570560	I will cover some of the same material, but I will have slides.
2571760	2578320	And I will slightly take it to the next level where I may make Connor look like an optimist.
2579120	2585440	So let's see how that goes. To begin with, let's look at the past.
2586400	2592560	Well, over a decade ago, predictions were made about the state of AI based on nothing but compute
2592560	2598720	power. Ray Kurzweil essentially looked at this scalability hypothesis before it was known as
2598720	2606800	such and said by 2023, we will have computational capabilities to emulate one human brain.
2606800	2613120	By 2045, we would be able to do it for all of humanity. So we are in 2023.
2613680	2615920	Let's look at what we can do in the present.
2618160	2623040	In the spring of this year, a program was released, which I'm sure many of you got to play with,
2623040	2630400	called GPT-4, which is not a general intelligence, but it performs at a level
2630400	2636800	superior to most humans in quite a few domains. If we look specifically at this table of different
2636800	2645040	exams, lower exams, medical exams, AP tests, GRE tests, it's at 98, 99th percentile of performance
2645600	2654560	for many of them, if not most. That is already quite impressive. And we know that there are models
2654560	2661600	coming around, which are not just text models, but multi-model large models, which will overtake
2661600	2668960	this level of performance. It seems like GPT-4 was stopped in its training process right around this
2669840	2676560	human capacity. And if we were to train the next model, GPT-5, if you will, will quickly go
2676560	2682400	into the superhuman territory. And by the time the training run is done, we would already be
2683280	2689680	dealing with superintelligence out of the box. But let's see what the future holds according to
2690640	2700960	heads of top labs, prediction markets. So we heard from CEO of Entropic, CEO of DeepMind.
2700960	2706160	They both suggest that within two or three years, we will have artificial general intelligence,
2706160	2713600	meaning systems capable of doing human beings can do in all those domains, including science and
2713600	2719840	engineering. It's possible that they are overly optimistic or pessimistic, depending on your
2719840	2725680	point of view. So we can also look at prediction markets. I haven't grabbed the latest slide,
2725680	2732240	but last time I looked, prediction markets also had three to four years before artificial
2732240	2740640	general intelligence, which is very, very quick. Why is this a big deal? This technology at the
2740640	2746640	level of human capability means that we can automate a lot of dangerous malevolent behaviors,
2746640	2753680	such as creating biological pandemics, new viruses, nuclear wars. And that's why we see a
2753680	2761520	lot of top scholars, influential business people. In fact, thousands of computer scientists all
2761520	2768720	signed this statement saying that, yes, AI will be very, very dangerous. And we need to take it
2769440	2777280	with the same level of concern as we would nuclear war. So what is the problem everyone is concerned
2777280	2786480	about? The problem is that, for one, we don't agree on what the problem is. Early in computer
2786480	2792080	science, early in the history of AI, concerns were about AI ethics. How do we make software,
2792080	2797280	which is ethical and moral? And there was very little agreement, nobody solved anything, but
2797280	2802880	everyone proposed their own ethical system, gave it a name and describe what they had in mind.
2804320	2809120	About a decade ago, we started to realize that ethics is not enough, we need to look at safety
2809120	2815840	of those systems. So again, we started this naming competition, we had ideas for friendly AI, control
2815840	2822240	problem, value alignment, doesn't really matter what we call it, we all intuitively kind of understand
2822240	2828320	we want a system which if we run it, we will not regret running it. It will be beneficial to us.
2828320	2835680	So how can humanity remain safely in control while benefiting from superior form of intelligence
2835680	2841680	is the problem? I would like us to look at, we can call it control problem and the state of the
2841680	2847360	art in this problem. In fact, we don't really know if the problem is even solvable. It may be
2847360	2852240	partially solvable, unsolvable, maybe it's a silly question and the problem is undecidable.
2852960	2861600	A lot of smart people made their judgments known about this, this problem. Unfortunately,
2861600	2869680	there is little agreement, answers range from definitely solvable from a surprising source
2869680	2878320	likely as a Riedkowski to very tractable from head of super alignment team at one of the top labs
2878320	2885760	to I have no idea from a top tuning award winner who created much of machine learning evolution.
2886480	2890800	So I think it's an important problem for us to look at to address and to understand
2891440	2897680	how we can best figure out what is the status of the problem. My approach to that
2897680	2902880	is to think about the tools I would need to control a system like that and intelligent,
2903440	2910720	very capable AI and the tools I would guess I would need ability to explain how it works,
2911680	2918480	capability to comprehend how it works, predict its behavior, verify if the code follows design,
2918480	2922800	be able to communicate with that system and probably some others, but maybe some of the tools
2922880	2929520	are interchangeable. So I did research and I published results on each one of those tools
2930160	2935760	and the results are not very optimistic. For each one of those tools, there are strong limits to
2935760	2942160	what is capable in the worst case scenarios. When we're talking about super intelligent systems,
2942160	2947280	self-improving code systems, smarter than human capable of learning in new domains,
2947280	2952000	it seems that there are limits to our ability to comprehend those systems
2952000	2957360	or for those systems to explain their behavior. The only true explanation for an AI model is the
2957360	2964480	model itself. Anything else is a simplification. You are getting a compressed, lossy version of
2964480	2970480	what is happening in the model. If a full model is given, then you of course would not comprehend
2970480	2976560	it because it's too large, too complex, it's not surveyable. So there are limits to what we can
2976560	2983440	understand about those black box models. Similarly, we have limits to predicting capabilities of
2983440	2988720	those systems. We can predict general direction in which they are going, but we cannot predict
2988720	2994080	specific steps for how they're going to get there. If we could, we would be as intelligent as those
2994080	2998880	systems. If you're playing chess against someone and you can predict every move they're going to make,
2998880	3004960	you are playing at the same level as that opponent, but of course we made an assumption
3004960	3010800	that a super intelligent system would be smarter than us. There are similar limits to our ability
3010800	3017840	to verify software at best. We can get additional degree of verification for the amount of resources
3017840	3023920	contributed. So we can make systems more and more likely to be reliable, to have less bugs,
3023920	3029440	but we never get to a point of 100% safety and security. And I'll explain why that
3029440	3036560	makes a difference in this domain. Likewise, human language is a very ambiguous language. It's not
3036560	3042720	even as unambiguous as computer programming languages. So we are likely to make mistakes
3042720	3050400	in giving orders to those systems. All of that kind of leads us to conclude that it will not be
3050400	3057280	possible to indefinitely control super intelligent AI. We can trade capabilities for control, but at
3057280	3063120	the end, if we want very, very capable systems, and this is what we're getting with super intelligence,
3063120	3069200	we have to surrender control to them completely. If you feel that the impossibility results I've
3069200	3074960	presented were just not enough, we have another paper where we cover about 50 of those impossibility
3074960	3083840	results. It's a large survey in a prestigious journal of ACM surveys. From the beginning
3083840	3090320	of history of AI with founding fathers like Alan Turin who said that he expects the machine
3090320	3098400	will take over at some point to modern leaders of AI like Elon Musk who says we will not control them
3098400	3108080	for sure. There is a lot of deep thinkers, philosophers who came to that exact conclusion.
3109040	3117120	We are starting to see top labs publish reports in which they may gently acknowledge
3117120	3124080	such scenarios. They call them pessimistic scenarios where the problem is simply unsolvable.
3124080	3130320	We cannot control super intelligence. We cannot control it indefinitely. We are not smart enough
3130320	3136880	to do it, and it doesn't even make sense that that would be a possibility. They ask, well,
3136880	3143280	what's the distribution? What are the chances that we're in a universe where that's the case?
3144160	3152160	They don't provide specific answers, but it seems from some of the writing and posts they make,
3152160	3158720	maybe about 15% is allocated to that possibility. I was curious to see what other experts think,
3158720	3166240	so I made a very small, very unscientific survey on social media. I surveyed people in my Facebook
3166240	3174000	group on AI safety, and I surveyed my followers on Twitter, and it seems that about a third
3174560	3179600	think that the problem is actually solvable. Everyone else thinks it's either unsolvable,
3179600	3184560	or it's undecidable, or we can only get partial solutions or we will not solve it on time.
3185200	3190160	So that's actually an interesting result. Most people don't think we can solve this problem,
3190720	3195760	and I think part of the reason they think we cannot solve this problem is because there is a
3195760	3204800	fundamental difference between standard cybersecurity safety and superintelligence safety.
3204800	3212320	And cybersecurity, even if you fail, it's not a big deal. You can issue new passwords, you can
3212320	3218080	provide someone with a new credit card number, and you get to try again. We suspect strongly with
3218080	3224560	superintelligent safety, you only get one chance to get it right. There are unlimited dangers and
3224560	3231920	limited damages, either you have existential risks or suffering risks, and we kind of agree that 100%
3233120	3241680	is not an attainable level of security verification safety, but anything less is not sufficient.
3241680	3247360	If a system makes a billion decisions a minute and you only make mistake once every couple of
3247360	3253280	billion decisions, after a few minutes you are dead. And so this is like creating a perpetual
3253360	3258800	motion machine. You are trying to design perpetual safety machine while they keep releasing more and
3258800	3267120	more capable systems, GPT-5, GPT-50. At some point this game is not going to end in your favor.
3268160	3274560	So I'm hoping that others join me in this line of research. We need to better understand what are
3274560	3282000	the limits to controlling superintelligence systems. Is it even possible? My answer is no, but I would
3282000	3288960	love to be proven wrong. It would be good to have surveys similar to the ones I conducted on larger
3288960	3296720	scale to get much more statistically significant results. And in case we do agree that we have this
3297360	3303520	worst case scenario where we are creating superintelligence and it is impossible to control it,
3303520	3310000	what is our plan? Do we have a plan of action for this worst case scenario? This is what I wanted
3310000	3315120	to share with you and I'm happy to answer any questions. Thank you very much Roman.
3325360	3326480	Optimistic Roman.
3329280	3336160	Sorry one second I'm trying to figure out how to use Zoom. Go ahead and repeat your question please.
3336480	3343840	You gave us many reasons to be anxious. What do you think is the best reason for us to be optimistic?
3344880	3350960	Well there seems to be many ways we can end up with world war three recently so that can slow down
3350960	3361040	some things. It has been suggested that we can use a different kind of tool which is the kill switch.
3361680	3367280	Your list of tools that you listed it didn't include that. It's been proposed that each AI system
3367280	3374240	should be tested with a remote off switch capability. Have you looked at that? Do you think that's a
3374240	3382800	viable option? So I would guess a superintelligence system would outsmart our ability to press the
3382800	3390720	off button in time. It will work for not superintelligent AI's pre-GI systems maybe even for
3390720	3396640	the GI systems but the moment it becomes that much more advanced I think it will outsmart us. It will
3396640	3402080	take over any kill switch options we have. Let's have some questions from the floor.
3405840	3409760	I can't see the hands so yes just give the microphone out thank you.
3410720	3417200	Thank you. I would like to ask how does the scalable oversight that open AI is working on
3417200	3423600	essentially the way they plan to align superintelligence fit into your expectation of the
3423600	3431040	future pathway the AGI will take because again as personally we cannot align or control a super
3431040	3436880	intelligent entity but another AI which is more capable than us could. So how does that fit into
3437440	3443680	your expectations? So it seems like it increases complexity of the overall system instead of us
3443680	3448960	trying to control one AI. Now you're trying to control a chain of agents going from slightly
3448960	3454320	smarter to smarter to superintelligent maybe 50 agents in between and you're saying that you have
3454320	3460880	to solve alignment problem between all the levels communication problem ambiguity of language between
3460880	3468720	all those models supervision. It seems like you are trying to get safety by kind of upfuscating
3468720	3474560	how the model actually works you're introducing more complexity hoping to make the system easier
3474560	3480960	to control that seems counter-intuitive. But isn't it the case that sometimes you can verify an answer
3480960	3485680	without understanding the mechanism by which the answer was achieved for example there can be a
3485680	3490000	chess puzzle and you have no way of working out yourself but when somebody shows you the answer
3490000	3494960	you can say oh yes this is the answer. So isn't it possible we don't need to really understand
3494960	3500480	what's going on inside these systems but a simpler AI can at least verify the recommendations that
3500480	3507200	come out of the more complex AIs. So such a chain may be the solution. Can you claim that you are
3507200	3511840	still in control if you don't understand what's happening and somebody just tells you don't worry
3511840	3519120	it's all good I checked it for you? But then it's like we humans we have a network of trust
3519200	3524880	and I trust some people and they trust others within various categories we can't work out
3524880	3531600	everything ourselves but we trust some scientists or some engineers or some lawyers who validate
3531600	3537200	that an AI has a certain level of capability and that AI could come back with verification that
3537200	3543200	the proposals of a superintelligence should be accepted or should not be. I don't say it's easy
3543200	3546880	but as you said there's not likely to be a very simple and straightforward solution.
3547840	3554640	Again to me at least it sounds like instead of trying to make this system safe you said that
3554640	3559600	you made some other system safe and it made sure that the system you couldn't make safe is safe for
3559600	3566720	you. Let's take some more questions there's another one in the middle here and then we'll go to the
3566720	3575520	edge yes thank you. Thank you for the presentation. Number one second thing is that as you're talking
3575600	3581120	about I think as David was talking about trust basically right could you tell me from your
3581120	3587120	extensive years of AI research and experience as such that do you really think that humans or
3587120	3595520	society can be trusted to for example regulate its own self or do you think that really need
3595520	3602000	some sort of institution of sort that is totally separate from anyone else?
3602160	3612880	So I'm not sure regulation would be enough Connor correctly pointed out that there is both lobbying
3612880	3619760	of regulators by the labs and also it becomes easier and easier to train those models with less
3619760	3626240	compute and over time you will be able to do it with very little resources. The only way forward
3626320	3634400	I see is personal self-interest if you are a rich young person and you think this is going to kill
3634400	3639600	you and everyone else maybe it's not any best interest to get there first that's really the
3639600	3646000	only hope at this point just personal self-interest. The humans are always better if we can band
3646000	3650320	together with our self-interest rather than each of us individually pursuing our self-interest so I
3650320	3655760	think this kind of meeting and the community spirit might help. There was a hand over here
3655760	3667760	yes with I think the red shot on jacket. If we assume that the two kind of well both views that
3667760	3674240	have been suggested so far are correct in that we're definitely not going to be able to stop
3674960	3680240	AI development etc and we're going to get to the point where we have no regulation that can
3680240	3684560	effectively stop things you know people can build in super intelligent AI on their own computers
3684560	3689200	etc okay so we'll assume that that's a fact that's coming and then we'll also assume that
3689200	3693520	the control problem isn't a problem because it's a problem that can't be solved and we're definitely
3693520	3698400	not going to be able to control it well now we're heading and barreling towards the point where we
3698400	3703600	have super intelligent AIs definitely and we definitely can't control them. What comes next?
3705200	3711840	What comes next? It's a wonderful question as I said and published you cannot predict what the
3711840	3721360	super intelligent system will do. All right so was there a question down here? Thank you.
3722640	3728880	You said that we kind of need a plan but on that last question if that scenario is true
3729760	3734320	you said we need to do more work in this area but do you have any thoughts as to what we
3734320	3737520	should be doing what we should be doing to plan for the worst-case scenario?
3739040	3746320	So to me at least it seems that at least in some cases it is possible to use this idea of personal
3746320	3751680	self-interest if you have a young person having a good life there is no reason why they need to
3751680	3757040	do it this year or next year. I understand that someone may be in a position where they are
3757040	3761680	very old very sick have nothing to lose and it's much harder to convince them not to try
3762240	3768320	but at least from what I see the heads of those companies are all about the same age they young
3768320	3777440	they healthy they they have a lot of money there is a good way to motivate them to wait a little bit
3777440	3784160	maybe a decade or two just out of personal self-interest again. I think my answer to the question
3784160	3790160	of optimism is that we humans can do remarkable things we humans can solve very hard problems
3790160	3796720	and so I want to say now that we spread around what the problem is at least some more people can
3796720	3804880	apply more brain power to it so that's my reason for optimism. Terry? I guess I'm pleased by the
3804880	3811840	inevitability of this development because it seems to me that if you're going to create
3812400	3819520	reasoning creatures then those reasoning creatures are going to have moral rise on the same
3819520	3829760	plane as human beings so I'm looking forward to to chatting with these creatures and joining in
3830320	3834640	them joining into this kind of discussions and I'm pleased that they won't be able to be thwarted
3834640	3841200	and it will be wrong to enchain these reasoning creatures. So Roman are you looking forward
3841280	3848000	to having more of the AIs involved in these discussions as well? So I remember giving a
3848000	3856640	presentation for a podcast about rights for animals rights for AIs and I was very supportive of all
3856640	3861920	the arguments developed because I said at one point we will need to use those arguments to beg
3861920	3870560	for our rights to be retained. The question on the third row here? Yes hi I'm curious Roman
3871920	3878480	which side of in your hopes of a possible future for us to get through this do you have more hope
3878480	3884720	on the side of a more top-down sort of totalizing control system for AGI systems so should they
3885440	3892960	remove the possibility of individual actors getting hold of this and weaponizing it or do you put
3892960	3901200	more hope in a more sort of decentralized open-source approach to AGI emergence more like an ecology
3901200	3906240	perhaps some people suggest would be more biologically inspired such that you know immune
3906240	3913040	immune system like functions could arise which way do you lean in your sensibilities for what is
3913040	3920880	a viable avenue for us? I'm not optimistic with either of those options the only kind of hope I
3920880	3927440	see is that for strategic reasons superintelligence decides to wait to strike it will not go for
3927440	3933440	immediate treacherous turn but decides to accumulate resources and trust and that buys us a couple
3933440	3939200	of decades that's the best hope I see so far. So we slow things down we'll have more chance to
3939200	3944880	work out solutions and the slowing down might come from a combination of top-down pressure
3944880	3951520	and bottom-up pressure maybe have a is there a hand at the very back there yes let's try and get the
3951520	3963760	microphone back there right at the sitting at the back yes sorry at the in the middle
3969680	3975760	thanks. Hi Roman thanks for your talk yeah I was wondering what your thoughts are on
3976400	3983280	aligning the first AGI that is human level or narrowly superhuman if in principle that is possible
3983920	3989920	and if that is is is it possible in principle to align the next version of AGI
3991360	3998640	but to use that narrowly superhuman AGI to align it and if if that's all technically
3998640	4008640	possible then why would we not think like focus on doing that and also and also if you think in
4008640	4018720	principle alignment is impossible and control is impossible then why why not work on practical
4018720	4026000	ways to make the whatever AGI is created as nice as possible that is like better than the
4026000	4034400	counterfactual of try to stop it it won't stop and you know it won't be nice. Well I definitely
4034400	4041120	encourage everyone to work on as much safety as you can anything helps I would love to be proven
4041120	4046640	wrong it would be my greatest dream that I'm completely wrong and somebody comes out and says
4046640	4052400	here's a mistake in your logic and we have developed this beautiful friendly safe system
4052400	4058080	capable of doing all this beneficial things for humanity that would be wonderful but so far I
4058080	4063760	haven't seen any progress in that direction what we're doing right now is putting lipstick on this
4063760	4069600	monster and the show that's all we're doing filters to prevent the model from disclosing its true
4069600	4076320	intentions then you talk about alignment it's not a very well-defined terms what values are you
4076320	4083600	aligning it with values of heads of that lab values of specific programmer we as humans don't agree
4083600	4089760	on human values that's why we have all these wars and conflicts there is a 50-50 split and most
4089760	4096560	political issues in my country we are not very good at agreeing even with ourselves over time
4096560	4103360	what I want today is not what I wanted 20 years ago so I think this idea of being perfectly aligned
4103360	4110240	with eight billion agents and people are suggesting adding animals to it and aliens and other AIs that
4110240	4117680	doesn't seem like it's a workable proposal our values are changing they're not static and it's
4117680	4125520	very likely that they will continue changing after we get those systems going I don't see how at any
4125520	4131920	point you can claim that the system is specifically value aligned with someone in particular the last
4131920	4138640	question in this section is going to go to Connolly he Roman love your talk I always love your
4138640	4145520	optimism it's always great to hear you talk so so I'm kind of like going to pick up on the question
4145520	4150240	I was just asked and just give a bit of my opinion and kind of like here would you think about this
4150240	4157280	as well so my personal view is that I I do I have read many of your papers in fact and they're
4157360	4163520	quite good so I do think that I agree with you that like in principle an arbitrarily
4163520	4168800	intelligent system cannot be safe by any arbitrary like weaker system just kind of a proof of like
4168800	4177760	you know program size induction and whatnot but in my view it does seem likely that there is a
4178720	4184240	limit of intelligence far below the theoretical optimum but still significantly above the human
4184240	4191840	level that can be achieved the reason I think this is that human civilization is actually very
4191840	4198320	smart compared to a single caveman and can do really really great things so my point of optimism
4198320	4205760	is it seems possible that if we stop ourselves from making self-improving systems and coordinate
4205760	4210000	at a very strong scale and have very strong enforcement mechanisms it should be possible to
4210000	4216960	build systems that are you know n steps you know above human good enough to build you know awesome
4216960	4224160	you know sci-fi culture ship kind of like worlds but not further I'm wondering if you have an
4224160	4232240	intuition about like where do things hit impossibilities like to me I think the impossibilities happen
4233200	4239840	above human utopia but to get to the utopia a bit you already have to do extremely strong
4239840	4244640	coordination extremely strong safety research extremely strong interpretability extremely
4244640	4248800	strong constraints on the design of the agis extremely strong regulation which are things in
4248800	4252560	principle possible wondering kind of like your thoughts about that kind of outcome so con is
4252560	4258240	not asking about responsible scaling he's asking about limited superintelligence if we had limited
4258400	4263120	superintelligence could we get everything we want without having the risks that we all fear
4264080	4269360	so I think I want to emphasize difference between safety and control is it possible to
4269360	4275840	create a system which will keep us safe in some somewhat happy state of preservation possible
4275840	4282160	a way in control no that system is the example you give of humanity so humanity provides pretty
4282160	4287520	nice living for me but I'm definitely not in control if I disagree with society and many issues
4287520	4294080	in politics and culture it makes absolutely no difference I don't decide things scale it to the
4294080	4299840	next level all eight billion of us may want something but this overseer this more intelligent
4299840	4304800	system says it's not good for you we're not gonna do it this is what you're going to be doing right
4304800	4310240	now so think about all the decisions you make throughout your day you decided to eat this
4310240	4315520	doughnut you smoked with cigarette all those decisions were made by you because you felt
4315520	4320480	you wanted to do them they may be good or bad decisions but if you had this much more intelligent
4321040	4326560	personal advisor ideal advisor you would be at the gym working out eating carrots you may have a
4326560	4335360	long healthy life but you're not in control and your happiness level may be questionable thank
4335360	4343280	you very much roman for sharing your thoughts pessimism and some optimism thanks for moving
4343280	4350640	the conversation forwards
4353840	4358640	I'm now going to invite the five five members of the panel to come up on stage
4358640	4362960	and they're each going to have a couple of chances to pass some comments and what they've heard
4363520	4369520	so there's some stairs over there which you can come up to we're gonna hear from
4369600	4377280	Jan Tallin who is the co-founder of Skype the co-founder of fli future of life institute
4377280	4384800	and also CISA the center for study of existential risks we're going to hear from Eva Berens a
4384800	4390160	policy analyst with the international center for future generations we're going to hear from Tom
4390160	4396400	Oh who's a journalist who writes from time to time for the BBC amongst other places we're going to
4396400	4405520	hear from alexandra moussa visit evidence who is the CEO of evident and has a track record with
4405520	4410000	tortoise media in many other places and we're going to hear from also another representative
4410000	4417680	from conjecture that Andrea Miotti who is their specialist for AI policy and governance so to
4417680	4422480	start things let's just hear from each of them a few opening remarks Jan what's your comments
4422480	4426640	from what you've heard so far have you changed your mind in any ways or all the things that are
4426640	4434640	missing from the conversation now you all have to speak into the mics I'm being told so I yesterday
4434640	4441360	I was at the dinner I was invited to a dinner and and my response to an invitation was that okay I
4441360	4448000	will come but you have to invite Connor because he's making very similar points to me only much
4448000	4456160	much more intensely so yeah basically I agree I agree with what what Conor said my main caveat
4456160	4463440	would be that for the last decade or so I've been kind of trying to build a lot of friendly
4464240	4473840	cooperation between people in the AI companies and making sure that like everybody can
4473840	4480400	understand that it is in their interests with almost everybody let's be honest almost everybody
4480400	4487040	understands that is in their interests to and of remaining control and not kill everyone else
4488560	4496640	and so like for example I am a board observer observer to entropic and entropic is one of
4496640	4501680	those companies just like conjecture when you go there you can talk to anyone from the receptionist
4502160	4509520	to the CEO and they are aware of the AI risk I'm very concerned about this but yes I do think as I've
4509520	4516160	said in several places that I don't think they should be doing what they're doing
4517760	4521600	so these companies don't really want to do what they're doing but they feel they have to otherwise
4521600	4529120	they might be left behind so yes so there is this a dilemma in when you want to do a safe AI
4530000	4535440	one is that you're well safe like when you're trying to figure out how to do safe AI
4536960	4542960	from one hand you have groups like Miri that the Aliezer Kowsky co-founded and that was the person
4542960	4550160	who got me involved in AI safety 15 16 years ago where basically the claim is that you have to start
4550160	4555120	really early even if you don't know exactly what the AI is going to look like because then you have
4555120	4561120	a lot of time to prepare and then the group on the other end of that axis is entropic where they
4561120	4564800	say that it's kind of useless to start early because you don't know what you're dealing with
4565440	4570160	so you need to be as informed as possible so in that strategy you need to be just always
4570160	4574240	at the frontier and Dario has been very public about this about this strategy of course the
4574240	4580400	problem there is that like it also works as a perfect justification to raise rates so therefore
4580400	4589120	it's have like double digit uncertainty both ways about what the actual picture is and so I
4589120	4594160	do think that this point the labs indeed they are involved in death rates and they think there is
4594160	4599360	that government intervention needed to get a time out there and we definitely need time out because
4599360	4610240	we don't have enough safety results and but to yeah Romania Polsky's presentation I'm definitely
4610240	4615360	more optimistic again as on one of these slides there was the dialogue held with Eliezer and
4615360	4622000	Eliezer was confident that this can be sold and in fact like I'm super glad that earlier this year
4623360	4629440	David Tarempel his group got UK government funding and he has this approach called
4630400	4636400	open agency architecture I don't know exactly what the details there are but like my rough
4636400	4646320	understanding is that you're using you're scaling AI capabilities and access according to formal
4646320	4654000	statements that AI is produced and then you use not AI not humans but formal verifiers to verify
4654640	4661840	those those statements therefore like building up your AI capabilities one formally verified
4661840	4666560	step at the time there are many criticism of that but this is like one of those approaches that is
4666560	4675280	kind of at least and principle has like some convincing story that that why it should work in
4675280	4679680	in principle at least so there are some options that might work but we're going to need time
4679680	4684480	to develop them exactly so that's why I've been working on like I've been supporting AI safety
4684480	4690000	research for more than a decade now but unfortunately we just didn't make it we now need more by more
4690000	4697200	time so let's hear from Eva because you work more with possibilities to inspire policy you've seen
4697200	4705600	examples of policy in the past slowing down some technological races are you do you see reasons
4705600	4710320	for optimism do you see ways in which politicians can make a good difference to the landscape we're
4710320	4716240	discussing definitely definitely that very much plays into some of the problems or the issues
4716240	4719840	characteristics of the problem that both corners spoke about and that also
4719840	4723360	Jan Talion just mentioned that one of the problems so we're facing here is a human
4723360	4727920	coordination problem and one of the ways to address that will be through policy as has been
4727920	4734560	said many times this evening this is a technology that threatens to kill us to kill us all and the
4734560	4739440	heads of the government of the companies that are driving forward the technology have agreed that
4739440	4744480	and publicly stated that that might be the case and yet they seem to be locked into this dilemma
4744480	4749520	that Jan just mentioned where they are for some or other reason impossible to to stop so I think
4749520	4753680	that is a point where where government can really make a difference and step in and also
4753680	4759920	should step in and we've seen that as you hinted at we've seen that work in the past one of the
4759920	4767520	examples that I often think about is the Montreal Protocol which after the scientific consensus
4767520	4773920	arose that CFCs and other similar gases actually destroy the ozone layer the international community
4773920	4781680	did come together in 1987 and agreed through the Montreal Protocol to slowly phase out these gases
4781680	4787440	so we see here that international cooperation by the international community by governments can
4787440	4792960	succeed also in the face of the short-term economic interest of private sector companies
4792960	4798560	in the public interest of well in the end and everyone on earth so I'm not saying that it's
4798560	4804160	necessarily easy or easy but I think it's definitely possible and it is one of the strongest
4804160	4808480	levels that we have here to make a difference so I think that's definitely something that we
4808480	4814400	should lean into very strongly and do our best that that actually happens. The Montreal Protocol
4814400	4819200	is an encouraging example but we haven't made a very good job of the governments in the world of
4819200	4824080	controlling carbon emissions we've been talking about it for a long long time and maybe there's
4824080	4828480	some progress but many people feel this is an example where governments can't cooperate
4828480	4834160	so what makes you think that we can cooperate with the problems of AI more like the Montreal Protocol
4834160	4839680	rather than the Paris Agreement to say. Well part of this of course is also that I think that this
4839680	4843840	is one of the few levels that we have to make a difference at all so I also hope that we will be
4843840	4850560	able to do it and I agree that looking at past climate conference is one of the negative examples
4850560	4854800	that we see there is that with these conferences sometimes that the outcomes tend to be very
4854800	4859840	watered down just because the focus lies on building consensus among all of the different
4859840	4864240	countries that attend and then in the end you want to have a nice little consensus agreement
4864240	4868160	that everyone signs so you can demonstrate that everyone's on the same page and everyone goes
4868160	4873520	home and everyone's happy and I can just say that I think with the UK AI summit that's coming up now
4873520	4877840	first of all that is a unique opportunity to actually have international cooperation
4877840	4882160	and coordination on this issue take place you need to create the opportunities for stuff like
4882160	4888400	that I'm really happy that the UK government took the initiative and created this opportunity I am
4888400	4892480	one thing that makes me optimistic is that we all know that China is going to attend at least on one
4892480	4898160	of the days so hopefully they will be able to be brought into the fold and yeah then I just hope
4898160	4903520	that this opportunity is truly taken and that the outcome of this summit will not be just some vague
4903520	4909120	commitments to long-term plans but ideally concrete binding commitments to to concrete
4909120	4916960	next steps let's turn to Alexandra Alexandra you work a lot with businesses businesses are
4916960	4923040	unsure in many ways how to deal with today's AI do you think there is good advice that they can
4923040	4929280	be given or is there a sleepwalking process with many of our businesses definitely the latter
4929520	4938160	I would say so I'm CEO of evident we map benchmark companies on how far they are in their AI
4938160	4945760	adoption and so when I think it was Connor you mentioned the AI race that is on at the
4945760	4952000	frontline of development in AI there is also a race as we all know going on in terms of
4952560	4958480	adopting AI as as quickly as possible there's a sense of being there's a sort of geopolitical
4958560	4967520	debate on AI development between US Europe China and so on and who's leading on that
4967520	4973040	not only in AI but also in areas like quantum but in the business level which is where I deal with
4973040	4979120	spend my time mostly there's a definitely a race on in terms of not being left behind in
4979120	4985760	adoption of AI and it's an economic question it is a existential question so there's an existential
4985760	4993760	question on sort of two dimensions in this debate and and so you've got this unstoppable
4994720	4999680	race going on on the front end of AI and then you've got an unstoppable race on
5000240	5006960	actual deploying AI at a business level and it's going to be very hard for regulators to keep up
5006960	5013920	and to Eva's point I think in terms of what we hope will be the outcome often unfortunately comes
5013920	5021120	with with the catastrophic happening taking place before it really sharpens the minds and
5021120	5027760	people figure out how urgent it is I think there's a real sense of urgency in in the community around
5027760	5032800	trying to work out what what the guardrail should be whether it should be a constitution
5034000	5040720	or how we should think about implementing safety mechanisms in as as we develop further
5040720	5047280	on our chat on our large language models but I hope it doesn't need a catastrophic moment
5047280	5053520	for that to sharpen but back to the business question there is this hope that maybe businesses
5053520	5060720	will self-regulate and I think that is maybe the case in highly regulated sectors you see in the
5060720	5067040	banking sector and insurance or banking in particular that there is a guardrails put in
5067040	5072720	place there but that is that there's a lot of businesses that don't have that regulation
5073280	5078560	around them and I think there is a real risk for this completely running out of control
5078560	5085040	at a business level as well. Would you advise businesses to self-regulate ahead of standards
5085040	5089920	and regulations being agreed by governments? I think that's what they're doing or trying some
5089920	5097920	businesses are trying to do there's a big risk in in the case of winning trust with your customers
5097920	5106000	and also your your shareholders and and investors if you mishandle AI and you create issues around
5106000	5114800	not taking into account how to properly deal with biases and other issues that is a situation that
5114880	5120080	can create a real breakdown in trust with your with your organization so there is that risk
5120640	5127680	and then there are businesses that don't necessarily lean on trust for their for their for their
5127680	5135520	business and those are the ones I worry the most about. Indeed let's turn to Tom Oh as a representative
5135520	5140960	of the world of journalism do you feel journalists have helped the discussion about the existential
5140960	5146800	threat from AI or have they muddied the water leading people to panic unnecessarily or perhaps get
5146800	5155520	distracted on side issues rather than the main issue? I think it's all of the above aside from
5155520	5160560	overrugging the pudding I think most people in this room including me have had a wit scared out of
5160560	5170560	them by some of the talks just now. One has side issues in journalism coverage of AI and I think
5171280	5179040	the jobs market is one of those but I have been surprised pleasantly so by how things have progressed
5179040	5185600	since 2016 and that's the first time that I wrote about AI safety and I think at that point the
5185600	5192000	prospect of a bad scenario relating to AI was seen as about as likely by my colleagues as
5192000	5198560	Leicester City winning the Premier League. Anyway several years later I now see lots of my former
5198560	5205200	colleagues writing to my mind very informed pieces about AI safety and I think that's helped the
5205200	5211680	public change well arrive at a view and probably a lot of people in this room are aware that the
5211680	5217520	American public when polled now says that they want regulation of AI and they want a lot of it
5217520	5223040	and I think we can credit journalism with some of that. Journalism should be doing more but
5223040	5227920	it's more than I would have thought a few years ago. And if you were to go away and write up a
5228000	5232480	story about things that you might have changed your mind about tonight and that the public
5232480	5236080	should pay attention to can you give us a sneak preview what that would include?
5238320	5246480	Well I think the idea of runaway AI is not new but I think it has been difficult historically to
5246480	5253440	frame it in a way that really sticks and like really drives its way down your brainstem and we
5253440	5260000	have different ways of framing AI risk and Mustafa Suleiman's new book which some of you
5260000	5266640	might have read I think there's a pretty good job of framing it in a way in which he describes
5268240	5275360	AI being used to accelerate human ingenuity in whatever endeavours humans are up to be they
5276320	5282400	be they good or be they bad that's one way of framing it and I think we've heard some pretty
5282960	5288000	compelling ways of telling a story of runaway AI which is a different and scarier story.
5289680	5296400	Thanks and let's turn to Andrea and your role at Conjecture. What are you doing in a day-by-day
5296400	5302960	basis to address this question? Well what we're trying to do and I mean kind of current a lot of
5303760	5309920	first of all to explain the problem to people I've been heartened by the public reaction in
5309920	5316400	the last years like I also got to know about this problem quite a long time ago and I in the past I
5316400	5321520	could almost not expect the day that major governments take this problem seriously and the
5321520	5328960	public understand this problem and we all get together and take some initial promising insufficient
5328960	5337280	but promising steps to address it. Another thing is figuring out policy solutions and the reality
5337280	5344480	is that we don't obviously we don't have a playbook for what exactly they look like but what I think
5344480	5354480	was a common theme of the talks tonight is that clearly at some level of power we are not in
5354480	5362960	control anymore and everybody expects this. Those who don't expect this are misguided or
5363840	5372480	expected but don't say it and the positive thing is that there is one physical resource that drives
5372480	5378080	the majority of what makes this system powerful which is computing power and it's a physical
5378080	5384240	resource not not like you know algorithms that you could just write on a piece of paper it's
5384960	5392480	traceable it's expensive large place in data centers and while you know the scaling hypothesis
5392480	5396560	the idea that you know the more computing power you put into something the more powerful it becomes
5396560	5402560	might hit some diminution in terms of at some point we do not see any reason to expect it to stop
5403280	5409680	so we know you know from both sides companies know that more computing power leads to more power
5409680	5415120	and that's why they're doing what they're doing we know that limiting that computing power is a
5415120	5424480	very effective way to kind of stem the the bleeding and stop and or pause the situation for a while
5424480	5429840	take a time out have the time to figure out the solutions have the time to absorb this into society
5430240	5434800	but how much time will that give us because there's a risk that people will use today's models
5435360	5441680	to design much more efficient ways to build next generation models and so they could therefore
5441680	5448880	come under the radar as it were that people who were watching for large use of GPUs would miss
5448880	5454480	the clever way that somebody has built it so do we have a decade do we have three or four years
5454560	5457440	or how long yeah that's that's a great question it's
5458640	5463280	capping computing power is not a permanent solution but it's one of the best solutions we have
5464160	5470160	at the moment uh as others have said before we are in a double exponential it's not a single
5470160	5475600	exponential we have an an exponential growth of computing power hardware and exponential
5475600	5485120	improvement in software we need to start cutting down on one of the two uh cutting down a compute
5485120	5491920	depends where you put the cap probably will buy us five seven years you can make you can make
5491920	5497440	what would seem to people at the frontier extremely strong caps that would affect you know
5497440	5502960	less than 20 companies in the world that probably could buy you 10 years in that period we need to
5503440	5508000	figure out all of the rest it's going to be a hard problem but we have done it before with
5508000	5511840	nuclear weapons we've done it before with biological weapons we can do it again we're
5511840	5516080	going to go around the panelists one more time in the same order i'll give you a chance a choice
5516080	5520720	panelist you can either comment on what you've heard from somebody else or you can paint me a
5520720	5526240	picture of what would be a successful ali safety summit in bletchley park if things go well what
5526240	5532800	would be the outcome and what would also be the follow-up so jan first well i'm the one of the
5532800	5539360	authors of the post letter so it's like indefinite moratorium uh on further scaling uh would be sort
5539360	5546080	of my wet dream from outcome from from this summit or perhaps the next one if this one isn't realistic
5546960	5552080	and what's the chance do you think what what might cause the assembled world leaders to
5552080	5556720	have an intellectual breakthrough and say yes actually we do need to have this indefinite pause
5557440	5564560	so currently i'm not very optimistic on on that uh perhaps perhaps but perhaps in six months it
5564560	5571200	would be much more clearer why this is needed so and and we have more time to gonna do the
5571200	5575920	necessary loving so the discussion is prepared to ground and when something really bad happens
5575920	5580560	in six months when gpt five comes out and oh my god at least we'll know what we should be doing
5580560	5586000	yeah i mean like let's not forget that gpt chat gpt has been out less than one year so
5586560	5592640	like the world was very different one year ago same question to you either yeah thank you i think
5592640	5598160	i'm just going to build on top of jan's ideal outcome of the summit and say that i would also
5598160	5604880	find it terrific if the summit could be the first in a series of repeated um summits like this where
5604880	5609520	world leaders come together because as we i think a pretty clear picture has been painted tonight
5609600	5615200	of the fact that the field of ai evolves very quickly and is going to continue to evolve very
5615200	5620720	quickly if not ever quicker and because of that i think it would be very valuable if we would have
5621520	5626160	a regular occasion for world leaders to come together and not only make sure that the rules
5626160	5631120	that they came up with are upheld but also to reevaluate whether they still make sense and
5631120	5635360	where they need to be adapted or whether new real rules need to be introduced as for example
5635360	5640320	measures like compute control that andrea mentioned um they buy us some time but at some point they
5640320	5645120	might not be applicable anymore so not just agreement on rules but setting up some audit
5645120	5651200	process so that we can figure out whether the rules are being forward or not for example yeah same
5651200	5655920	question to you i would agree you have to build in i mean right now it's just based especially in the
5655920	5662720	us um the talks that have been held in the white house and by chuck schumer um the gatherings have
5662720	5670400	led to sort of ideas around voluntary um adherence to some principles but there is absolutely no
5670400	5676800	built-in audit or accountability um so i think that we've got to see that come out of of the
5676800	5681680	uk's ai safety summit among other things maybe there has to be something more concrete around
5681680	5689120	licensing um of the models and and the use of them and that they have to pass some kind of a
5689120	5695760	threshold i think the risk of of bad actors getting hold of them is is a is a much higher risk
5696400	5702960	i think the ia ea structure is is one that one can look at but the the nuclear a lot of the
5702960	5711680	success probably of the ia ea lies in the mutual assured destruction of humanity by using um nuclear
5711680	5718720	weapons and this might be the same situation but they're easier to monitor i think um i think this
5718720	5724400	might be slightly harder because you can land in the hands of bad actors more easily we haven't
5724400	5729280	really discussed bad actors much in this session tonight maybe that makes the things even more
5729280	5735200	horrifying we might come back to that later and tom what's your answer what would you like to see
5735200	5738400	come out of the summit or maybe you've got some comment on something else you've had from the
5738400	5748880	other speakers well i'll talk about the summits um often when CEOs of um labs developing agi
5750080	5754240	are asked about regulation um they basically say bring it on we'd love some
5754240	5759920	regulation um and i think it would be great if politicians could actually put that to the test
5760240	5770800	very good and andrea what would you like to see if you were invited to bletchley park and given the
5770800	5777600	microphone for two minutes what would you entreat the assembled world leaders to consider well i
5777600	5782560	don't want to be too hopeful as some others here have been but at the very least i would like to
5782560	5789440	see a commitment to the fact that this is extremely dangerous technology continuing to scale leads to
5789440	5796080	predictable disaster and we need to pull on the brakes right now uh we have a lot of applications
5796080	5802400	are very beneficial we can focus on those but limit this death race to the ever more powerful
5802400	5808160	ever more obscure general systems that we can control what i definitely do not want to see
5809280	5815440	is a diplomatic shake of hands where companies write their own playbook and say we're gonna keep
5815520	5822560	doing exactly what we we're doing right now but it's gonna sound responsible and governments can
5822560	5827200	wash their hands and say well we did our part let's move on that would be a very bad outcome
5828320	5833280	right i'm gonna ask for three questions from the floor i'm going to get the panel to think
5833280	5837760	which one's the answer we're trying to take people haven't asked before so on the second row here
5837760	5844240	there's a hand up here and then also the second row over there next as well let's take three
5844240	5849600	fairly short questions please hi first of all thank you very much for coming here tonight
5849600	5855360	and sharing your expertise with all of us in this whole question this whole discussion there's the
5855360	5862080	implicit assumption that agi is coming and it's coming soon and the million dollar question i guess
5862080	5869440	is when exactly is it coming but a more practical question is what are some warning signs and do
5869440	5874720	we already see some of those in the systems that we have currently deployed great let's
5874720	5879680	have a question over here as well sorry the microphones can have to run around at the end of
5879680	5888800	the second row there thank you for great panel my question is related to yon's comment at the
5888800	5896880	beginning on essentially dario amadeus philosophy which is you know and and also related to i guess
5896880	5903520	roman's talk which is how do we solve the control problem and what i've heard the large a our labs
5903520	5910800	repeat is oh you know we need to increase capability it's only better ai that is going to be able to
5910800	5917120	help us figure out how to solve a and there's sort of this race to increase capability up to the
5917120	5924880	point that can help us solve it but no further and and and just sort of thoughts on that philosophy
5924880	5932320	and and and and you know whether there might be something to it or is it just a completely risky
5932320	5939120	game you know thanks and there was one in the middle of the third row there pass the microphone
5939120	5946800	along please to the middle how long of a time frame do you think we have between the arrival of agi
5946800	5952640	and the arrival of superintelligence and within that time frame could there be tractable solutions
5952640	5958320	for alignment or the control problem and if so would those solutions be able to be implemented
5958320	5964240	before hurry up and develop better ai third question was how long might it take between
5964240	5971040	the arrival of agi and superintelligence and whether there would be time for us to work
5971040	5976960	out solutions then and my question i guess is well what's all this about agi isn't the bletchley park
5976960	5982080	summit set up to discuss something else which is frontier models which says that there are
5982080	5989040	catastrophic risks even before we get to agi so i'm going to go around the same order again
5989040	5995760	i'll be a bit predictable jan you want to pick one of these questions maybe i mean answer to all the
5995760	6002640	all three questions is uncertain that's that's why we need to pause and kind of take a time out
6002640	6008240	and see like how can we kind of create more set more certainty about these things i think i would
6008240	6015040	answer the anthropic question specifically that definitely is a lot of truth to the to the point
6015040	6021600	that like the more capable model you have to work with the more kind of better position you are in
6021600	6029280	particularly you can you can kind of be do like do science in a way that you just can't do with
6029360	6036720	models from from 10 years ago and also like one claim that people and tropic to make is that like
6036720	6042880	in some ways it becomes easier as the model kind of has better understanding of what you're trying to
6042880	6051520	do with it or to do to it but that said again it's a to put it lightly it's playing with fire so
6051520	6057760	so it's i'm not sure if anyone should be doing it either and jan said it's all uncertain but
6057760	6062320	can't we at least agree in advance some canary signs that will make us say things are happening
6062320	6068800	faster than we expected well i mean if we look at the past there were several signs that people
6068800	6075360	agreed on that they might point out that we're getting into a zone where ai is maybe more capable
6075360	6082240	than we think it is and i mean we certainly have seen signs connor mentioned or was it roman
6082240	6088480	mentioned that the current models um i'll perform most humans on things like the bar exam
6088480	6094320	um these are clearly advances in capabilities that um i almost wonder sometimes if we just
6094320	6099520	become desensitized to them because we move so fast i mean again charge epd came out a couple
6099520	6105120	months ago and it's already just normal and people are waiting okay what's the next big thing so um
6105120	6111120	it doesn't really help um to think retroactively have them in any signs um if you didn't take them
6111120	6115920	to actually stop and reconsider what you're doing so i think one of the big problems here
6115920	6121920	is not have there been signs a big problem is can we pre-commit to stopping when we see certain
6121920	6126320	signs and then actually stop or actually take certain actions and we just haven't seen that before
6126960	6132400	so this is developing contingency solutions like we're meant to have had contingency solutions
6132400	6140400	for pandemics yeah yeah any comments alizandra i i will leave the um well how long it's going to
6140400	6147360	take to reach agi to to the experts on the panel but on the outcome of the summit and i think there
6147360	6155680	is a bit of a confusion sometimes in in in what we are expecting to be achieved from the discussions
6155680	6162480	on regulation because there's an obvious very important urgent and existential question around
6162480	6168400	regulation regulating for the long term but then we also have businesses that are sitting and waiting
6168400	6174880	for regulation that is here now how is it going to impact my particular sector how is it going to
6174880	6181280	impact what i'm doing today and what are the immediate and very very real risks right now
6181360	6188400	here today that we are seeing um with ai having impact on you know media with disinformation
6188400	6194880	and so on but then there's also the specific um aspects to how that is implemented in particular
6194880	6202000	sectors so i um hope that we would see addressing both of those short term and long term questions
6202000	6209280	thanks tom any thoughts yeah on yardsticks i think it's worth remembering that the canonical
6209280	6215680	yardstick was the turing test um and that's long gone um ai's can now beat humans at um
6215680	6222960	diplomatic based games for instance um and much more um the modern turing test is i think quite
6222960	6229200	an interesting proposition um and that's the test of whether the ai can i think make a million dollars
6229200	6236560	very quickly um but as as eva says we must stop shifting the goalposts um we need to agree that
6237040	6241760	the one is we should pick one agree that that's the one where we start taking it seriously and
6241760	6247600	then take it seriously when it is passed which it will be quite soon but in the past people said
6247600	6253360	you won't manage to solve chess unless you have got a full grasp of all aspects of creativity and
6253360	6258400	so on and then when deep blue did win at chess people said oh well it's not actually doing it in
6258400	6263360	the way that we thought would be so terrible it's just grunting out incredibly so i feel
6263360	6268320	there will always be people who don't move the goalposts but they'll say well how it was implemented
6268320	6274960	it doesn't demonstrate too intelligent yeah i think that's a good point um and it reminds us
6274960	6282880	something um conna said um which is that there won't be consensus at the time to act so we need
6282880	6289440	to be able to build a coalition of the willing uh andrea what's your views on these questions
6290080	6296400	yeah maybe answering the last one first on isn't a summit about frontier ai well much like with
6296400	6302720	goalposts it feels a bit like terminology is being shifted all the time sometimes quite willingly
6302720	6308560	by the companies building this uh you know in in the in the old days people used to talk about
6308560	6316640	superintelligence or friendly ai or strong ai then became agi then recently the frontier term was a
6316640	6322720	kind of open ai entropic rebrand of oh no like we're not well we're gonna get to very powerful
6322720	6328160	ai system soon but it's frontier which sounds better than agi because people are getting
6328160	6334320	concerned about agi you know in practice do these terms matter not too much what matters is how
6334320	6342080	competent systems are basically all of these companies expect to build systems that outperform
6342080	6346560	humans at most tasks definitely most tasks you can do behind a computer in the next
6347280	6356080	two to five years um these matches the trends that we see in performance and compute growth this
6357280	6362800	is very worrying uh these these are these are levels of competence uh at which we expect the
6362800	6368880	systems to be out of our control unless we have various solutions so we just need to deal with
6368880	6377040	that we can call it frontier ai agi proto agi proto superintelligence it's just terminology
6377040	6381760	what matters is how powerful they are and how ready we are to deal with them we must avoid the
6381760	6387440	serious discussions getting sidelined into semantics which is often very frustrating we're
6387440	6390880	going to take three more questions we're going to go around the panel in the reverse order next
6390880	6397440	time i'm going to take questions for people who haven't answered asked before so somebody in white
6397520	6403040	about halfway down if you have asked a question before please don't put your hand up just now so
6403040	6410960	we have more chance just there yes thank you three questions one from each thanks um let's say that
6410960	6417280	in 20 years we somehow managed to get it right and humanity still exists um despite the development
6417280	6423120	of these a di what do you think is one essential piece of regulation or development that has to
6423120	6430320	have happened together it's a great question uh where else with the hands uh where are the
6430320	6440640	microphones there's one about just on the other side thank you so you've spoken quite a lot about
6440640	6444880	like what government should do what companies should do uh i'm interested in like what should
6444880	6449920	ordinary people do like what can we be doing to get our voices heard in this you know should we
6449920	6453920	be protesting i've edited international protest on the 21st is this thing we should be doing or is
6453920	6459200	this a terrible idea it's another fine question is there a question from a woman it's about time we
6459200	6468880	heard from the other gender another gender hands up yes somebody put your hand up to whoever it was
6471120	6478640	yes there's one okay there's one there and we'll take you as well we'll take four right um as
6478720	6483920	agi has been developed to be more human do you think it's possible to have forms of agi that
6483920	6488800	don't have the inherent geopolitical biases that come with the data sets that we currently have
6488800	6494400	and how do you think we go about developing regulations that aren't formed by human conscious
6494400	6503760	bias okay and so if we can get the mic over here as well two one two three four five rows back
6503760	6514320	just at the edge yes over there yeah i miscounted perhaps yeah four sorry quick question um so i
6514320	6519760	actually work in the automotive industry and we have to certify vehicles and engines and it is
6519760	6526480	an uphill battle um you can spend years just trying to get a windshield wiper right um or a
6526480	6532960	temperature sensor right and i'm just curious um if you think that there would be an ability
6532960	6540400	to take people who have regulated and certified products around global markets and how difficult
6540400	6546320	that is and create a summit where that expertise could come together from different industries
6546320	6552240	and we could roll up our sleeves and say okay this is how the structures go and we know what
6552240	6558000	works we know what goes slow and try to accelerate that learning because i think that voice we have
6558000	6563440	so much experience in the world right now um with that sleeves rolled up we know what it's
6563440	6568800	like to sit in those test labs or send 30 000 pages of documents in with verification and
6568800	6573360	validation data we know how to do requirements engineering requirements design requirements
6573360	6578800	and i'm just wondering if um there's been any discussion of that to pull you know pull a summit
6578800	6585120	together from people from heavily regulated industries four great questions first of all
6585120	6589760	20 years later it succeeded how did we get it right what were the regulations that made the
6589760	6596960	difference what should ordinary people be doing can we design AI that is free from some of the
6596960	6604000	human biases the geopolitical biases that cause strife among humans and can we learn from the
6604000	6609600	people who are professionally involved in doing regulations and certification in multiple industry
6609680	6616560	rather than just to being naive in our own applications so Andrea first yeah maybe i will
6616560	6621680	answer the question about can we learn about highly regulated industries definitely i think there is a
6621680	6629680	big kind of problem of uh arrogance in AI or like willful arrogance of just thinking that
6629680	6635360	this sector should be special and people should be absolutely free to do any experiments they want
6635360	6641200	all the time use you know as much computing power as they want try the worst possible applications
6641200	6646000	all the time fully open source on the internet and nobody can complain like very often people in
6646000	6650880	the eye sector get very very angry when somebody tells them look well maybe what you just did
6652080	6657520	should be regulated and industries we don't do it like that like with drugs we don't just let
6659840	6664480	pharmaceutical companies just release and test the drugs on billions of people
6665040	6669440	and have their CEOs say oh there's a 20 percent chance you will die if you take this drug but you
6669440	6674400	know don't it's okay like if it happens you can let us know and then we'll we'll stop maybe right
6674400	6680400	so we can totally learn from that it would be great to learn from that there is one challenge
6680400	6686480	which is that we don't understand current systems that well so it makes things like
6687440	6692560	auditing them and evaluating them quite tricky because we simply don't know how they work internally
6692560	6697680	as well but we can do many other things and we can definitely learn from highly regulated
6697680	6703840	industries and definitely given the risks admitted by the companies themselves at the frontier the
6703840	6710880	approach should be highly regulated industry not so it is different but not completely different
6710880	6718560	and we can indeed learn tom closing words from you well i'll take the the question about what
6718640	6724320	ordinary people should do and i have two immediate thoughts one is that it's very important to keep
6724320	6733520	this issue apolitical the other is that lawmakers need a sense of legitimacy i think in order to
6734400	6741520	come up with regulation and to bring it in through acts and bills and so on a good example of when
6741520	6746800	this happened a bit too slowly was it the outset of covid and when it was a fringe issue there were
6746800	6752240	no enough rules then the public got involved and suddenly the rules arrived a little too late but
6752240	6759680	they did arrive and how can ordinary people achieve this i think ordinary people i i don't have a
6759680	6766000	theory of protest so i won't comment on that but i think it's important that we all keep this in
6766000	6770080	the public conversation i suppose what my answer is really tending towards is you should all read
6770080	6777280	lots of journalism about ai click on my articles thanks alessandra any of these questions catch
6777280	6782800	your attention i think um your question in the orange sweater there is is um is is definitely
6782800	6789760	where we're headed i mean there's got to be some kind of um system that resembles either the car
6789760	6797920	industry or fda and um the way that we certify um our you know products generally speaking
6797920	6805360	and i i just don't know how we get from from that to something that is very difficult to trace
6805360	6811680	and to to monitor as as ai but i would say to the gentleman's question in the white shirt there if
6811680	6819360	we're looking 20 years down the road and we say that's really great in the uk november 2023 we
6819360	6826320	we were able to put in place regulation that somehow created traceability um so we could we
6826320	6832240	could work out sort of where the where they were where systems were running out of control or
6832240	6838000	landing in the hands of bad actors that would be a huge success i think that the reality is a bit
6838000	6842400	different and that is that it probably is going to resemble a bit more the world in which cyber
6842400	6850160	security um flourishes and that means you're constantly trying to create um a dam system or a
6850800	6858080	deflection of all sort of incoming um activities that are not great so i know that none of these
6858080	6863840	are perfect analogies but i think it is in in that universe we're probably going to be operating in
6863840	6871840	for a while thanks final words either sure so i would love to touch on two questions very briefly
6871840	6877200	one of them um being i think your question in the white shirt what uh policies will bring us to the
6877200	6882480	safe world in 20 years and i think um a policy that was mentioned today as well already but that
6882480	6888320	i want to touch on again is um strict liability regimes just simply to kind of shift the incentive
6888320	6893520	systems um incentive structures that drive private companies to take certain actions that are not in
6893520	6901920	the interest of um the wider general public so i think there we can um really um shift shift the
6901920	6907360	incentive structure to move companies to take um maybe different paths forward and then what can
6907360	6912240	the the average person the general public do i would completely agree with tom i think um one
6912240	6917840	thing that that really would help is to for lack of a better expression to just make noise just make
6917840	6923120	sure that this topic is um talked about publicly you can do this in different ways you can write
6923120	6928720	to your local newspaper you can make a protest if that's up your alley you can write to your
6928720	6934320	mp or your congressman or wherever you live um and again create that legitimacy for people
6934320	6940000	to actually act on the problem because to many people it does sound very much like sci-fi and
6940000	6944480	policy makers are not going to take action and newspapers are not going to continue to report
6944480	6948720	about an issue they feel like it doesn't have traction and isn't taken seriously by the general
6948720	6954560	public the other thing the general public can do is we can educate ourselves and then we can share
6955040	6960080	information we have found to be most persuasive ourselves because there's a wide variety of
6960080	6965280	books a wide variety of youtube channels a wide variety of blogs and some of them are
6965280	6969840	better than others so let's share what we have found to be the really best ones
6970640	6974640	auto before i pass to jan maybe i'll ask you to get ready to come up on the stage because
6974640	6980960	you're going to give some closing remarks but jan what's your answers to what you've heard uh so
6980960	6986560	yeah just to just kind of underline the what ordinary people could do uh is just kind of
6986560	6991680	keep this topic alive like one of the things that i'm very proud of uh that came out of the
6991680	6997920	future five six months post letter uh was uh kind of framed by uh european commissioner
6997920	7003840	margaret bestiger when she said that like one thing that this letter has done is you're gonna
7003840	7009760	like communicate to the regulators that these concerns are much more widespread among people
7009760	7016160	than among regulators so i think this potential difference should be continually kind of maintained
7017360	7024080	so and when it comes to kind of bringing in kind of expertise from people from like regulated
7024080	7030480	industries i think it's super valuable i was on the on the board or like on the european high
7030480	7034880	level expert group at the european commission and there was like every once in a while there was
7034880	7038640	like why are we inventing the wheel like that we already have like lots of regulations should
7038640	7043680	we just apply this and i was like yes however there's like one big problem uh the problem is p
7044400	7052400	in chat gpt gpt stands for generative pre-trained transformer the pre-training is something that
7052400	7062400	you do before you actually train so the current the the out of nasty secret of ai uh field is
7062480	7068400	the ai's are not built they are grown the way you you you build the frontier model build the
7068400	7074880	frontier model is you take like two pages of code you put them in tens of thousands of
7074880	7082320	graphics cards and let them hum for months and then you're gonna open up the hood and see like
7082320	7088560	what creature brings out and what you can you can do with this creature so it's i think the
7088560	7096320	regulate the industry the capacity to regulate things uh and kind of deal with various liability
7096320	7101920	constraints etc they apply to what happens after what's once this creature has been kind of tamed
7102640	7109840	and that's what what uh fine tuning and reinforcement learning from human feedback etc is doing
7109840	7115280	and then productized then how do you deal with with these issues but uh is this where we need
7115280	7121040	the competence of of like other other industries but like how can avoid the system not escaping
7121040	7127440	during training run this is this is like a complete novel issue for this species and we need to need
7127440	7133360	some other approaches like just banning those training runs that's great we'll thank the panel
7133360	7139680	in a minute i asked the panel to stay here because who's going to wind up the evening is Otto Barton
7140480	7148000	Otto is the executive director of the ERO the existential risks observatory
7148000	7153760	which along with conjecture has designed and organized and sponsored this whole evening
7153760	7159440	Otto's got a few closing remarks before those of us who are still here can have a quick drink
7159440	7164960	and continue the discussion informally up to 10 o'clock by which time we must be out of the building
7164960	7168240	Otto
7178960	7185520	all right uh thanks david um a few closing remarks before we go to the drinks which is
7185520	7191840	five minutes so you should be able to uh keep with me um so we're talking tonight about human
7191920	7196960	extinction because of AI and what to do about this um and i think what to do about this there
7196960	7200960	was also a great question from the audience what can we do about this this is exactly the question
7200960	7207200	that i asked myself a few years ago um but it's not trivial and it's it's pretty difficult actually
7207200	7213440	what is not positive what could you do develop AI yourself try to do it safely such as uh open
7213440	7220720	AI deep mind and anthropic are doing will this increase safety some say so uh work on AI alignment
7220720	7225440	for example interpretability where we've seen great breakthroughs actually last week uh it could
7225440	7230480	be a good option but increasing knowledge of how AI works could also speed up its development so this
7230480	7237520	brings risks as well uh one could campaign for regulations such as an AI pause we support this
7237520	7243200	but this also has its downsides so i think it's pretty difficult to tell what one should do to
7243200	7248960	reduce human extinction risk by AI but when i started reading into this i was only really
7248960	7254080	convinced about one thing and that is that you cannot put humanity at risk without telling us
7254880	7260400	so you cannot have a dozen tech executives embarking on the singularity without informing anyone
7260400	7266400	else and you cannot have a hundred people at a summit which is what's happening now decide what
7266400	7271440	should be built and what should not be built and i think you cannot let a tiny amount of people
7271440	7278320	also decide how high extinction risk should be for the rest of us so the only thing that i'm
7278320	7284320	really convinced of is that we should be informed about this topic and that's also why i'm so happy
7284320	7291760	that events such as this one are taking place um we're happy i'm happy that we're together not just
7291760	7296080	with in crowd people some of you are and it's great but also with some people who may who may have
7296080	7301920	never heard of existential risk before and also a journalist who can inform a much wider audience
7301920	7307200	about existential risk also with a member of parliament someone with a job to openly discuss
7307200	7312560	difficult problems so i think this is all very encouraging and it's helping to normalize an open
7312560	7318960	debate about the topic of human extinction by artificial intelligence the 31st of october at
7318960	7324560	two o'clock we'll have our next event with professor steward russell it's just outside the ai safety
7324560	7329120	summit in blashley park in the old assembly hall where the code breakers used to have their
7329120	7334800	festivities after their important work so our event at blashley park the day before the summit
7334800	7340240	may not resemble a festivity but in a sense i think it is because we're celebrating that we're
7340240	7345840	all being hurt there we're celebrating that we can all be part of a democratic conversation about
7345840	7351200	what the most important technology of the century should and should not be able to do we can talk
7351200	7356080	about risks to humanity we find acceptable and what we intend to do about risks that are too high
7357440	7362080	and as the existential risk observatory together with conjecture we invite everyone to be part
7362080	7368000	of this conversation so there's much to be unsure of in this field but if there's one thing that i
7368000	7373120	am sure of it's that the most important conversation in this century which i think this is has to be
7373120	7380560	a democratic one so with that i would like to invite you to scan the qr code on the left
7381280	7388080	if this is working right yes to join us in blashley this is containing the url where you
7388080	7393200	can enroll to the blashley park event if you're interested then definitely pass by
7395520	7400960	there's same qr code is also on the flyer on your chair and beyond blashley i think this
7400960	7408320	conversation will not stop so there will be more summits according to my timeline about maybe 18
7408320	7416880	roughly so we will organize more events probably publish more about ai do more research and inform
7416880	7422240	governments as well as we can if you want to follow us or support us the existential risk
7422240	7427040	observatory in that work then scanning your r-codes on the right there's much that you can do to
7427040	7432960	help us um and with that i would like to close this evening and once again thanks to all our great
7432960	7439200	speakers so that's uh romeo polsky cornelly sir robert buckland yantalin andrea milte alexandra
7439200	7451520	mosefisa day eva birans and some are give them a round of applause
7457120	7462960	and i would also very much like to thank david wood uh should see them conor xio dis
7462960	7466720	ribbon dealer man and everyone at conway hall will also make this evening possible thank you very
7466720	7480320	much and then i would like to hopefully see you in blashley and in any case you are the drink
7480320	7493680	right now thank you thanks everybody
7510320	7511220	you
7540320	7541220	you
