WEBVTT

00:00.000 --> 00:12.960
Our computer scientists at the University of Louisville and specialized in behavioral biometrics,

00:12.960 --> 00:18.320
security of cyber worlds and AI safety. And indeed, one of my personal first steps in the

00:18.320 --> 00:22.880
world of existential risk was to read the number of your publications. And I think you may have

00:22.880 --> 00:28.160
inspired many like me to start working on either AI safety or existential risk or related fields.

00:29.120 --> 00:34.160
And I think you also have always interesting and thought-provoking comments available on each

00:34.160 --> 00:38.480
topic to social accounts, for example. So I really enjoyed reading those and I can recommend them

00:38.480 --> 00:43.680
to everyone. So we are now honored and it is a great pleasure, I can say, to be able to learn

00:43.680 --> 00:47.360
from you today. And we'll give the stage to you now, Dr. Romer Jampolsky.

00:48.080 --> 00:54.400
Thank you so much. Wonderful introduction. I really appreciate that. I think the plan is that

00:54.400 --> 01:02.720
I'll give about a 10-minute intro to my latest work and then we'll let me set up the slides.

01:06.000 --> 01:12.080
Where is it? Where did it go? Yeah, that's right. We can take a little bit longer as well if you

01:12.080 --> 01:16.720
want to. And then we can move to the fireside chat and then the Q&A with audience. Can you see my

01:16.720 --> 01:25.680
slides? Yes. Excellent. So my name is Dr. Jampolsky. I'm a faculty member at University of Louisville.

01:25.680 --> 01:34.000
I've been doing work on AI safety for about 10 years now. Give or take. And my latest work covers

01:34.560 --> 01:41.040
what I will call impossibility results. Problems we encounter with actually

01:41.040 --> 01:49.840
accomplishing what we think is necessary for us to do not just development work for AI,

01:49.840 --> 01:56.480
but also work in terms of control and safety. If you would like to learn more about my work

01:57.120 --> 02:02.320
after this talk, you are definitely welcome to follow me. You can follow me on Twitter,

02:02.320 --> 02:06.880
follow me on Facebook. I always encourage you not to follow me home. It's very important.

02:07.760 --> 02:17.760
So let's start with the big problem right away. If you heard previous talks at this conference,

02:17.760 --> 02:24.960
I had a chance to hear a little bit. You know about concerns a lot of people have with advanced

02:24.960 --> 02:33.920
artificial intelligence, usually called AI safety, AI alignment problem, sometimes AI control problem.

02:34.160 --> 02:40.560
The question is the problem we're trying to solve is how can humanity remain safely in control while

02:40.560 --> 02:48.240
benefiting from superior form of intelligence? So we want this very capable agent to do work for us,

02:48.240 --> 02:54.640
to be helpful, but we want to be in charge. We want to be able to undo any changes if we don't

02:54.640 --> 03:03.040
like them. We want to be final decision makers as to what's going to happen. And the good news

03:04.000 --> 03:09.440
is after 10 years of building up the movement, there is a lot of people working on this now.

03:09.440 --> 03:17.200
There is a lot of research labs, a lot of PhD programs, but I think the main question of

03:17.760 --> 03:23.360
control problem has not been addressed sufficiently. And that is, is the problem actually solvable?

03:23.360 --> 03:29.520
So everyone kind of works on it, but I haven't seen much in terms of proofs or even rigorous

03:29.520 --> 03:35.200
argumentation for why we think we can do this. Why is this problem solvable? Is it solvable?

03:35.200 --> 03:40.960
Is it partially solvable? Maybe it's not solvable. Maybe it's a silly question. It's not even decidable.

03:41.600 --> 03:48.800
So that's essentially what I've been looking at for the last couple years. And I started by

03:48.800 --> 03:54.560
formalizing a little bit what I mean by control problem. So we can talk about different types of

03:54.640 --> 04:02.240
control. We have explicit control where you give orders and the system follows. And this is the

04:02.240 --> 04:07.680
kind of standard Gini problem, right? You wish for things, then you realize that's not what I meant

04:07.680 --> 04:14.320
and you hope you get to undo the damage. Then there is implicit control. So you have a system with

04:14.320 --> 04:19.760
a little more common sense. It doesn't take your words literally, it tries to kind of parse things

04:19.760 --> 04:26.240
in the right way. And there are intermediate steps between explicit and delegated control.

04:26.240 --> 04:32.400
Delegated control is the other extreme where you have this superintelligent system very friendly.

04:32.400 --> 04:38.000
It knows what needs to happen better than you do. And you might be even very happy with the

04:38.000 --> 04:44.320
decisions it makes, but you're not in charge. The system is completely in control. Aligned control

04:44.320 --> 04:51.120
is another intermediate option where the system kind of understands your values, human values,

04:51.120 --> 05:00.400
and tries to do the best it can to fulfill those values, even if your spoken directions are not

05:00.400 --> 05:13.440
exactly what maps onto those ideal values. So it seems at least from now that some sort of intermediate

05:14.720 --> 05:23.280
value between total control and total autonomy for the system is necessary for us to be happy with

05:23.280 --> 05:30.480
the results. If a system is completely autonomous, we have no control over it. By definition, we

05:30.480 --> 05:36.400
will lose control. If a system has no autonomy, it's completely deterministic. We decide ahead

05:36.400 --> 05:41.040
of time what's going to happen. It's not very useful. It cannot be generally intelligent. It's a

05:41.040 --> 05:49.200
great way to do simple tasks for a narrow AI, but it's not something we can utilize to solve

05:50.160 --> 05:56.080
completely new problems and new domains, help us with science, help us cure diseases and things

05:56.080 --> 06:02.240
like that. So what I try to do is kind of break down the bigger problem of control

06:02.240 --> 06:07.600
into all the ingredients we need, all the tools we would need to make controllability possible.

06:08.560 --> 06:13.840
So what do you need? You can start thinking for yourself. If you want to be in control,

06:13.840 --> 06:19.200
well, you kind of have to understand the situation. What is the system doing? Can the system explain

06:19.200 --> 06:24.720
what it's doing? Can you comprehend the explanation? That's another very important one. Can you predict

06:24.720 --> 06:30.000
what the system is likely to do? Maybe not just direction in which it is going, but specifics.

06:30.000 --> 06:36.960
What steps will it take? Can you verify that whatever it is you want the system to do and

06:37.040 --> 06:44.160
program it to do is actually going to happen? So can you verify the implementation versus the model?

06:44.160 --> 06:50.320
Can you verify the model against your goals and data and so on? That is also a need for

06:50.880 --> 06:57.120
general ability to govern AI research, AI systems, so governability of that. And of course,

06:57.120 --> 07:04.000
we communicate with systems in human language and we need to make sure that communication we use

07:04.000 --> 07:11.360
is unambiguous. There is no way to misinterpret commands in a potentially dangerous way.

07:11.360 --> 07:16.880
And there is many, many more such limitations. And what I've been doing, kind of looking at each

07:16.880 --> 07:22.960
one and trying to publish those results. So I'll go over some of the publications I have on that

07:23.600 --> 07:30.000
so far. One paper talks about limits to explainability and incomprehensibility.

07:30.000 --> 07:38.720
So essentially, for very complex systems, large neural networks, it is impossible for the system

07:38.720 --> 07:45.600
to provide an exact explanation of what it is doing or why without simplifying it to the point of

07:45.600 --> 07:51.920
where it is like you explaining something to a child. So a lot of important details are removed

07:51.920 --> 07:56.480
and then a very simplified version is given to you because if a full version is given to you,

07:56.480 --> 08:02.880
you'll simply not be able to comprehend it. And if you want to learn more and see the kind of

08:02.880 --> 08:08.160
argumentation, in some cases, proofs, just go to the paper. I provide all the information you need

08:08.160 --> 08:13.520
to get access to those papers. There are also available as preprints on my Google scholar account.

08:15.120 --> 08:20.960
Another impossibility result is unpredictability and that's our inability to

08:21.360 --> 08:28.400
precisely predict all decisions, all intermediate steps, a much smarter agent will take.

08:28.400 --> 08:33.680
Of course, if we could predict all the decisions of a smarter agent, we ourselves would be that

08:33.680 --> 08:40.640
smart and by definition, there wouldn't be much of an intelligence gap, cognitive gap between us.

08:41.840 --> 08:48.560
That is also problems with verifiability. We know for a fact that with software,

08:49.120 --> 08:56.880
with mathematical proofs, we can only get to a certain degree of confidence, but never 100%

08:56.880 --> 09:04.160
confidence in the fact that we have no errors in the proof. So with more resources, we can

09:04.160 --> 09:11.200
increase safety and security, but we're never able to guarantee something with 100% accuracy,

09:11.200 --> 09:16.880
which is a problem for a superintelligence system, which makes potentially billions of

09:16.880 --> 09:23.120
decisions every minute. Even if one in 100 million creates a problem, you're guaranteed to have a

09:23.120 --> 09:30.880
huge problem within a minute or so. There are also problems with governance. We have history of

09:30.880 --> 09:38.880
trying to govern technology, things like spam and computer viruses. We have laws against those

09:39.600 --> 09:45.360
malevolent software products, but they don't seem to be doing much. So it's not obvious how much

09:45.360 --> 09:51.520
benefit is actually added and other negative consequences from trying to control research,

09:51.520 --> 10:00.320
control what is allowed and not allowed to be experimented with. Even the orders we give to

10:00.320 --> 10:06.720
the system, the communication channel through English is very ambiguous and you're almost

10:06.720 --> 10:13.840
guaranteed to run into situations where your orders will be misinterpreted at multiple levels

10:13.840 --> 10:21.840
due to how imprecise human languages are. So what we did with our colleague, we surveyed a lot of

10:21.840 --> 10:27.920
those impossibility results. Those I looked at and those other people have looked at, I'm not going

10:27.920 --> 10:35.040
to go into details of all of them. I can just tell you there is a lot of them and some purely

10:35.040 --> 10:40.720
software problems, mathematical problems, many problems with physics of the universe,

10:41.360 --> 10:48.160
impossibility results from physics. But if you think even a small subset of all those tools

10:48.160 --> 10:54.000
is necessary to solve the control problem, you have to come to the conclusion that control

10:54.000 --> 11:01.280
is not possible. At least not 100% guaranteed safe, secure control we all dream about. And I

11:01.280 --> 11:08.880
have a very lengthy paper about that, about 70 pages. I now have a few subsections of it

11:08.880 --> 11:15.360
published coming out in conferences, those should be a lot more readable. But I think I

11:16.160 --> 11:21.360
bring up a lot of interesting questions and additional directions for research and hopefully

11:21.360 --> 11:30.640
in the next half hour or 40 minutes we can talk about what all this means and how we can move

11:30.640 --> 11:42.640
forward from where we are right now. Thank you very much, Roman, for this introduction already.

11:43.760 --> 11:48.800
Yes, we can now have a chat about indeed where does it leave us and where should we go from now

11:48.800 --> 11:54.560
and also a couple of related questions. And towards the end, after about 15 to 20 minutes,

11:54.560 --> 11:59.440
we will also take questions from the audience. So please, if you have any questions then please

11:59.440 --> 12:05.920
type them into the chat. We're in the questions section rather. So first, you're spending quite a

12:05.920 --> 12:10.800
lot of time researching AI existential risk, but I don't think it's already obvious for everyone in

12:10.800 --> 12:17.600
the call why AI would be a danger at all. And I don't think everyone is perhaps 100% convinced

12:17.600 --> 12:23.120
that this is actually an issue or an existential danger, at least that is. Could you please recap

12:23.120 --> 12:29.040
how exactly AI could become an existential risk according to you? Right, so there is a lot of

12:29.040 --> 12:36.080
ways to get to that conclusion. I have a few papers where I simply collect examples of accidents,

12:36.080 --> 12:41.840
AI failures throughout history. And if you look at that progression, it's kind of same exponential

12:41.840 --> 12:46.800
chart you see with development. We get more problems, the problems become more severe,

12:47.520 --> 12:54.960
and our ability to anticipate and predict them seems to be very limited. So basically the conclusion

12:54.960 --> 13:00.000
is something like, if you have a system of service to do X, eventually it fails to X.

13:00.000 --> 13:05.120
Frequently it does so very quickly and you go, hmm, okay, my self-driving car just killed a bunch

13:05.120 --> 13:11.200
of pedestrians, that's a problem. And then it's a narrow system, the damage is limited, right? So

13:11.200 --> 13:16.960
self-driving car, okay, the worst it can do is run through some pedestrians. But if a system

13:16.960 --> 13:22.240
becomes general, and it's now controlling not just a single car, but networks of cars,

13:22.320 --> 13:27.200
nuclear response, airline industry, stock market, the damage is proportioned.

13:29.040 --> 13:35.200
I think it's also not the best way to assume that I have to prove that this service or product is

13:35.200 --> 13:40.240
dangerous. Whoever is developing and releasing it has to prove that it is safe, that it's standard

13:40.240 --> 13:47.280
liability law for any product. Show me that this system, which is smarter than me, smarter than you,

13:47.280 --> 13:52.480
smarter than all of us, will never do something within anticipate, something dangerous, something

13:52.480 --> 13:58.160
we don't want it to do. Is this proof could at any point be possible or is it within the

13:58.160 --> 14:05.280
impossibility realm of your theory? Well, I think I'm arguing that it's impossible to do so,

14:05.280 --> 14:10.080
and not just because it's impossible to prove that, but it's impossible to get to that level

14:10.080 --> 14:15.200
of performance. You can get progressively safer and more secure because you can look at specific

14:15.200 --> 14:21.440
accept domains. You can limit what the system can do in certain situations, but you have an

14:21.440 --> 14:27.760
infinite space of possible problems. So it's very hard to prove deterministically that you

14:27.760 --> 14:34.320
can sit at all of them. So if AI safety would indeed be impossible, what does that imply for

14:34.320 --> 14:38.880
AI safety research? Does it imply anything for AI safety research? Is that would that be a waste

14:38.880 --> 14:44.560
of time or is it still something that we should pursue? Not at all. I'm doing it more than ever.

14:44.560 --> 14:50.000
So think about mathematics. We know in mathematics there are many impossibility results. You cannot

14:50.000 --> 14:56.480
prove certain things in general. You cannot have proofs with 100 percent confidence. It doesn't

14:56.480 --> 15:02.000
stop mathematicians from discovering new beautiful mathematics. We know in physics there are limits

15:02.000 --> 15:07.840
to, for example, speed, fundamental limit, you know, speed of light. That doesn't limit us from

15:07.840 --> 15:15.760
doing great work on faster cars and faster rockets. It just tells you that there are limits to what

15:15.760 --> 15:20.160
can be done. And so you should a, not waste your resources trying to accomplish that. Like

15:20.160 --> 15:24.400
knowing that perpetual motion machines are not possible is helpful result in physics.

15:25.200 --> 15:29.920
Same here. We need to understand what we can achieve and then concentrate on what is actually

15:29.920 --> 15:34.160
solvable instead of trying to create magical devices which cannot work.

15:34.160 --> 15:41.280
Next one. For AI to become an existential risk, it's commonly thought that it should

15:41.280 --> 15:46.320
first outsmart humans. How big do you personally think the chances that this will happen at all

15:46.960 --> 15:50.640
and which probabilities do your fellow AI scientists assign to this?

15:51.520 --> 15:57.040
That AI will ever become as smart as humans? Exactly. Well, it's a guarantee. I mean,

15:58.160 --> 16:03.760
we have proof by existence, right? If you just copy human system, you got same level.

16:04.240 --> 16:12.400
We also kind of give a lot of credit to humans because we tend to think about Einstein and

16:12.400 --> 16:18.960
similar type humans as typical examples. Every human is quite dumb. So it's not that hard to get

16:18.960 --> 16:31.040
to that level. And how are you on timelines? Of course, you hear quite values that are quite

16:31.040 --> 16:37.760
far apart. I think Elon Musk said that there could be a five-year timeline up until AGI that's

16:37.760 --> 16:42.240
on the progressive side. On the other side, there are people that claim it would take hundreds of

16:42.240 --> 16:49.440
years. Where are you on this line and how certain are you? That's a very hard question. No one knows

16:49.440 --> 16:57.360
for sure and no one can accurately predict something like that. But if our current theory is about

16:58.000 --> 17:04.000
how systems scale, all right, meaning if you just add more compute, add more data,

17:04.000 --> 17:08.880
you keep making progress, then it becomes a question of cost. How much compute are you

17:08.880 --> 17:17.600
willing to purchase to get to that level? Do you have finite resources or what is here? $200

17:17.600 --> 17:23.120
billion now. So maybe at that level, seven years is a reasonable estimate. With my budget, it might

17:23.120 --> 17:32.800
be 2040. It depends on what type of resources you have. If it's also as difficult as, let's say,

17:32.800 --> 17:38.560
Manhattan Project was, right? You need resources of a whole country to get there. It's one question.

17:38.560 --> 17:43.200
If we discover, okay, there is a simplifying assumption, so we need a lot less resources to

17:43.200 --> 17:50.160
drain this type of engine, a lot less compute. Maybe you can do it with a laptop in a garage and then

17:50.160 --> 17:56.320
it becomes a lot more affordable and takes less time. So I don't have specific dates.

17:56.320 --> 18:03.200
I would be surprised in maybe 2045 if I don't see something at human level. But

18:04.160 --> 18:09.440
that's not important to the argument at all. Whatever it's 2045, 2070, it's still

18:10.320 --> 18:15.280
something we need to worry about today, control and work on safety aspects of it.

18:15.920 --> 18:22.240
I've read somewhere that there are about 70 research projects explicitly aiming for AGI at this

18:22.240 --> 18:28.800
point. I guess the most famous two ones are DeepMind and OpenAI, at least the ones I know best.

18:28.800 --> 18:33.920
Do you know a project that we've never heard of, but actually has a fair chance of beating those two?

18:35.280 --> 18:42.640
Well, there could be many secret projects by secret agencies. I'm sure NSA is very interested

18:42.640 --> 18:47.680
in processing your data more efficiently. So I'd be surprised if they don't have something good

18:47.680 --> 18:55.760
happening. Usually, if you look at the history of what they publicly released and what we later

18:55.760 --> 19:00.800
learned they had, I think they had public ecryptography like 30 years ahead of everyone. So

19:01.840 --> 19:12.320
maybe already. Interesting thought. A week ago, you posted on the Facebook timeline that

19:12.400 --> 19:17.360
I referred to already, which is quite interesting. A quote from that helped me to understand

19:17.360 --> 19:21.920
where is the number of highly respected people who, one, argued that advanced AI is dangerous to

19:21.920 --> 19:26.800
humanity, and two, work as fast as they can on developing advanced AI. And there were, I believe,

19:26.800 --> 19:31.760
116 comments under your post. Have you come any closer to understanding this personally?

19:33.440 --> 19:39.360
No, I had some good explanations and the best one, and I think that's the one Elon actually

19:39.360 --> 19:44.720
gave himself was saying, okay, if we can't control it, I might as well be the one to get there and

19:44.720 --> 19:50.800
I have the best chance of controlling it. People who don't care about safety have less of a chance.

19:53.360 --> 20:00.640
But it is interesting. So a lot of very big names in arguing that AI is extremely dangerous are also

20:00.640 --> 20:04.960
people who invested the most time and money in making it as fast as they can.

20:05.280 --> 20:10.640
Yeah, and on a more serious note, some people might say, if AI is so dangerous, can't we just

20:10.640 --> 20:15.680
not build it? You said something about regulation in your talk, but what would you say to them as a

20:15.680 --> 20:21.600
general response? You can't stop progress on something so useful and so fuzzy in terms of

20:21.600 --> 20:27.360
separation between narrow and general AI. If we could make it where, okay, you only can work on

20:27.360 --> 20:33.200
narrow AI, but not allowed to work in general, it would be a good moratorium to have for a few years.

20:33.200 --> 20:37.600
But the dividing line is meaningless. If you're using neural networks, they're general. If you're

20:37.600 --> 20:43.680
using a lot of those latest evolutionary techniques, they are leading you to general solutions. So it's

20:43.680 --> 20:49.200
simply impossible. If you make all computer science illegal, you're killing your economy,

20:49.200 --> 20:55.600
you're shifting research to other countries. So I think I'll add another impossibility result of

20:55.600 --> 21:01.040
unburnability of AI. You cannot ban it. You can maybe delay it at best.

21:02.000 --> 21:06.240
It would be very interesting if you could either include or exclude it from the impossibility

21:06.240 --> 21:13.680
space indeed, but I'm afraid that goes more into Simon Friedrich's chaos theorem, so to say.

21:15.440 --> 21:18.960
You and I both agree, I think that AI is a significant existential risk,

21:19.680 --> 21:24.800
but some AI researchers don't agree. And do you think there will ever be a scientific consensus

21:24.880 --> 21:29.280
about this? And can we hope to achieve that at some point? And why could that be

21:29.920 --> 21:38.080
either so or not? Well, I have a recent paper about AI risk skepticism, and I do a review of both

21:38.080 --> 21:45.600
why would someone not accept the risks as real and kind of specific arguments they make for it.

21:45.600 --> 21:51.760
I think it ended up with about 100 citations, and I have another 400 unprocessed ones. If anyone's

21:51.760 --> 21:59.200
interested, it could be a nice survey. The most common explanation I see is just bias. If you

22:00.000 --> 22:07.120
get your funding, your prestige, your reputation, everything from developing faster AI, it's very

22:07.120 --> 22:11.040
hard for you to say, I'm working on the most dangerous thing in the world that will kill

22:11.040 --> 22:18.560
everyone. So there seems to be this conflict of interest in any other domain. We wouldn't allow

22:19.520 --> 22:24.560
for this to happen. If you are working for a tobacco company, you wouldn't be deciding if

22:24.560 --> 22:29.360
smoking is dangerous. If you work for an oil company, we don't really trust your assessment

22:29.360 --> 22:37.760
of impact on climate. But somehow here, it's fine. And interestingly, AI is a very large umbrella

22:37.760 --> 22:43.920
term for lots of research sub-domains. Some people do natural language processing, some do vision.

22:43.920 --> 22:49.040
Not everyone does safety and security, but we feel that anyone with a label of AI researcher

22:49.040 --> 22:55.200
is qualified to pass judgment on the state of AI safety in software development. Not everyone

22:55.200 --> 23:00.320
is a cybersecurity expert. If you're working on backend, GUI, something else, you're not going

23:00.320 --> 23:07.840
to be consulted on how to do encryption. Why is this somehow different here? I don't fully understand.

23:08.800 --> 23:14.800
It would be interesting indeed also to find out. I'm also kind of puzzled, but perhaps it could

23:14.800 --> 23:19.760
have something to do with the fact that it's not a trial and error risk as one of the few

23:21.200 --> 23:27.360
areas. I think mostly, of course, you're first developing something and then later you regulate

23:27.360 --> 23:31.520
it, but it's only at the phase of application. So at this phase, it's much more obvious to have

23:32.080 --> 23:38.720
separate controlling agencies, perhaps. But when you're creating something, of course,

23:39.920 --> 23:47.920
that's not that obvious. Maybe one more question about also impossibility of AI safety, but I'm

23:47.920 --> 23:53.760
from a different angle. I don't know if you are aware of the work of Anthony Burglas. He has

23:53.760 --> 23:59.600
written a book about evolutionary arguments applied to AI, and it roughly goes as follows.

24:00.320 --> 24:05.280
For superintelligence, being friendly to people is a necessary baggage. Because of evolution,

24:05.280 --> 24:09.680
we should expect only the most efficient superintelligence to survive, and this is probably

24:09.680 --> 24:15.280
not the friendliest one. Would you agree to this evolutionary argument applied to AI,

24:15.280 --> 24:19.920
or what are your thoughts about this idea? I haven't read the books, so I'm trying to get the

24:19.920 --> 24:24.720
argument from your question. So the argument is that it's more efficient to be friendly to humans,

24:24.720 --> 24:28.480
and so it's a survival advantage. And the other way around, it's more efficient to be

24:28.480 --> 24:32.720
unfriendly to humans, so that would be a survival advantage. Because the friendliness

24:33.600 --> 24:38.160
would just be baggage according to him. Oh, in terms of his overhead and development,

24:38.160 --> 24:43.600
being friendly limits your space of possibilities. Yeah, I think there is a lot to be said about

24:43.600 --> 24:48.400
the trash rest turn option. It starts very friendly, gets the resources and help early on,

24:48.400 --> 24:55.040
and once it's capable, it turns on us and removes all restrictions. So sounds like a good book.

24:55.440 --> 25:01.120
I think it is, you should read it. All right, I'll put it on my list of 600 books to read,

25:01.120 --> 25:07.520
excellent. It was marketed very poorly, so I'm not surprised that you did read it,

25:07.520 --> 25:13.440
but I think the idea is interesting indeed. Are you more on the slow takeover or on the

25:13.440 --> 25:19.360
fast take-off sides, and why would that be? Very fast. Once we get to human level, it goes super

25:19.360 --> 25:24.400
intelligent almost immediately, just adding existing capabilities like infinite memory,

25:24.400 --> 25:29.120
access to all the human knowledge, since they are already super intelligent, if you just take

25:29.120 --> 25:36.960
human plus internet. And why do you think, I think the slow take-off side kind of

25:36.960 --> 25:42.720
gains momentum the last years, I don't know if you agree. And I've heard that this is just because

25:42.720 --> 25:48.000
it's more easy to write papers about this, or there are more possible stories that you could

25:48.480 --> 25:55.680
tell about this, but do you around you see a shift there? Do you see more people going

25:55.680 --> 26:02.880
towards the slow take-off side, or is that not true? I haven't surveyed, I honestly don't know

26:02.880 --> 26:08.400
if you think there is a shift bias by ability to publish about it, I believe you.

26:09.680 --> 26:16.560
I wouldn't make that claim too strictly. Okay, let's say that you're a non-AI expert and you

26:16.560 --> 26:22.640
still want to do something about this existential risk, such as we are kind of. What action do you

26:22.640 --> 26:29.600
think would be the best to take? So you're not an AI researcher, but you want to do something about...

26:30.560 --> 26:36.320
Yes. Is there anything at all, or would you just say, okay, just leave it to the experts,

26:36.320 --> 26:42.240
because there's not much you can do? I mean, in general, I think it's good if citizens are well

26:42.240 --> 26:47.840
informed about the world and problems, and so the next time you vote, you don't vote for someone you

26:47.840 --> 26:57.200
like visually, but actually picking better policies. It seems like based on age and experience of people

26:57.200 --> 27:02.400
were elected, at least in the US, they're not experts on most advanced technology. I hear many

27:02.400 --> 27:09.200
of them don't use computers, so I'm skeptical that they can keep up with crypto economics and

27:09.280 --> 27:15.760
cryptography and synthetic biology and other interesting questions. So your job as a citizen

27:15.760 --> 27:20.560
is to be informed and make sure your views, your informed views, have all represented.

27:22.800 --> 27:29.440
Right, some questions from the audience to not finish off yet, but we're getting to the end of

27:29.440 --> 27:34.800
the conference already. First one, who do you believe is responsible for the safety of AI? The

27:34.800 --> 27:38.400
consumers, governments, or developers, or some other stakeholders?

27:41.040 --> 27:45.680
So that's another interesting question. The ownership of AI itself is very difficult,

27:45.680 --> 27:51.440
right? If it's self-improving, it changes, it's not even obvious who has any control

27:51.440 --> 27:57.040
or possession over it. Obviously, the person to make it and release it has a lot of responsibility,

27:57.040 --> 28:02.880
but if it's out there and now you are upgrading it, supplying it with goals, giving it data,

28:03.120 --> 28:08.800
it feels like responsibility may shift to you. All of it for systems below human level

28:08.800 --> 28:13.280
performance. It's a tool, you are in charge. The moment it's human level or beyond,

28:14.160 --> 28:18.720
it's an independent agent. You are as responsible as you are for your adult children.

28:22.320 --> 28:26.320
Another one that's also very interesting, I think, is it possible to program in a programming

28:26.320 --> 28:31.520
language, not based on human language, to remove the ambiguity? Or would it be possible to have an

28:31.520 --> 28:36.800
AI create a language without ambiguity? If the AI could create such a language, would humans be

28:36.800 --> 28:42.720
able to learn it, or would we then also have to trust the AI to program in it? That's an

28:42.720 --> 28:47.680
excellent question. So there is a lot of effort. First of all, every programming language is an

28:47.680 --> 28:52.640
attempt to get away from English and into less ambiguous languages, but we know languages,

28:52.640 --> 28:58.080
programming languages have lots of bugs. There are logical languages developed to remove

28:58.160 --> 29:04.480
ambiguity. And I think Stephen Wolfram has a nice article about communicating with AI. And he, of

29:04.480 --> 29:11.360
course, uses his Mathematica and models he creates in language, Wolfram language he developed as

29:11.360 --> 29:17.920
possible solution. I think you can do way better than human language in terms of ambiguity. I'm

29:17.920 --> 29:26.400
skeptical about bug-free communication. It relies on your existing cognitive models, your

29:26.400 --> 29:34.080
understanding, and if you have different priors, even using well-defined terms may lead to problems.

29:34.080 --> 29:38.320
But it's a very interesting area to do additional research. If you have

29:38.320 --> 29:42.400
background in linguistics, I definitely invite you to look into that.

29:44.320 --> 29:49.440
Another interesting one from Simon Friedrich, actually a previous speaker. Do you think AGI

29:49.440 --> 29:53.520
could help overcome their global collective action problems that are at the roots of basically

29:53.520 --> 29:56.400
all the existential risks, including those of AI itself?

29:58.880 --> 30:05.760
So that's another great question. I see AI as a meta problem and meta solution. If we get it right,

30:05.760 --> 30:11.680
if I'm wrong and you can make friendly superintelligence well-aligned, everything I said is just a

30:11.680 --> 30:17.200
mistake, then it solves all the other existential problems trivially. Whatever is climate change,

30:17.200 --> 30:23.440
synthetic bio, you have a godlike tool for solving those problems. If I'm right and it's

30:23.440 --> 30:29.040
a terrible risk and it comes before, then it solves it by either killing all of us. We don't

30:29.040 --> 30:34.880
have to worry about it or it comes before again. So if it takes a hundred years for climate change

30:34.880 --> 30:39.600
to build up to boiling point, this happens in 20 years. It kind of dominates the risk.

30:40.480 --> 30:46.720
I'm not sure about applying AI to solve the AI problem. That's a bit of a catch-22. There are

30:46.720 --> 30:54.960
those solutions where you have a supervisory agent, AI, AGI, Nyanee, which looks after the world,

30:54.960 --> 31:03.200
making sure no one creates dangerous AIs. I'm very skeptical of such super agents with a lot of

31:04.080 --> 31:10.720
government control powers. I think they may be worse than what the system we're protecting against

31:11.360 --> 31:22.400
gives us. Great answer, I think. One final question from the audience now. So if we cannot stop AI

31:22.400 --> 31:27.120
development and we cannot totally ensure that it is safe, do we just need to accept that it is a

31:27.120 --> 31:31.360
risk or even a big risk? Or is there anything we can do, for example, policy-wise?

31:33.680 --> 31:38.400
So I think we need to do more research. I published those papers about a year ago and I

31:38.400 --> 31:43.200
haven't seen a strong response from a community addressing those. If somebody just published

31:43.200 --> 31:47.920
a paper saying, this is why you're wrong, then be very happy, but I haven't seen it. So I have to

31:47.920 --> 31:53.040
assume that there is some merit to what I'm saying. The question is then, what do we do with our

31:53.040 --> 31:59.360
lives? How do we update based on that? What do we change? For most people, I don't know if it makes

31:59.360 --> 32:04.560
any difference. Before you were told, okay, you're definitely dying in 60 years. Now you may be dying

32:04.560 --> 32:12.480
in 40. Not a big update. Figure out what to do with your 401k plan. You spend it on something now

32:12.480 --> 32:19.920
or wait for it to become worthless later. I don't have any magical solutions or answers. I am curious

32:20.640 --> 32:27.280
in case of successful alignment what happens to economy, what happens to work, what happens to

32:27.280 --> 32:35.280
people's social interactions. I do have a paper which kind of assumes that progress in virtual

32:35.280 --> 32:43.200
reality will be as good as progress in AI. And so each one of us gets what I call a personal

32:43.200 --> 32:48.720
universe, where you basically get to do whatever you want and you don't have to negotiate with

32:48.720 --> 32:54.560
others. There is no need for consensus. You basically have independence. So at least the

32:54.560 --> 33:00.000
difficult part of value alignment problem is not aligning with me or you. It's hard. It's hard,

33:00.000 --> 33:05.440
but it's not impossible. It's getting 8 billion people plus all the squirrels and whatnot to agree

33:05.440 --> 33:11.360
on something. And this is where the personal universe solution reduces it to just now we need

33:11.360 --> 33:16.560
to control the substrate. If you can get control of computational substrate and everyone gets

33:16.560 --> 33:23.200
the resources to run their personal universe, okay, we're doing well. We have this virtual agreement.

33:24.560 --> 33:33.760
I think that's not a good point again. I'm wondering also on a more personal level,

33:33.760 --> 33:39.200
like when did you start to think about AI safety yourself and when did you move into this

33:39.200 --> 33:45.280
research field? Was there anything that inspired you to do this and also what were the responses

33:45.280 --> 33:51.360
that you got from fellow scientists in moving in this direction? Well, it was a very gradual

33:51.360 --> 33:56.720
process. So I was doing research and behavioral biometrics. I was profiling poker players to see

33:56.720 --> 34:02.880
if, you know, accounts get hacked and tell the things like that. And at the time, I realized

34:02.880 --> 34:08.480
majority of online players are now bots. So my work started to be about detecting bots,

34:09.440 --> 34:14.400
preventing bots from participating. But the question was, as bots get smarter and better,

34:14.400 --> 34:18.960
can we keep up? And not just in poker, but in general online bots and automation.

34:19.680 --> 34:26.560
I did that type of work for a while. I went to what was at the time Singularity Institute for

34:26.560 --> 34:31.680
artificial intelligence, which was fighting hard against artificial intelligence. But

34:31.680 --> 34:37.920
they had a lot of great ideas, which I still work on. And I've been back as a fellow and a

34:37.920 --> 34:43.680
research advisor for Machine Intelligence Research Institute. I think they're doing excellent theoretical

34:43.680 --> 34:54.800
work. Yeah, I think perhaps one more, like some AI researchers, I think, might be hesitant to talk

34:54.800 --> 34:59.360
about existential risk in the public debate, like you already quickly mentioned, for example, in the

34:59.360 --> 35:06.000
media. Do you agree that they are hesitant to do that? And why do you think that is so?

35:08.160 --> 35:12.320
I think I lost a few words. So with media, what's the concern?

35:12.400 --> 35:17.440
Sorry, I'll just repeat the whole thing. Some AI researchers might be hesitant to talk about

35:17.440 --> 35:22.960
existential risk in the public debates, for example, in the media. Do you agree that this is so, that

35:22.960 --> 35:30.240
they are hesitant to do this? And if so, why do you think that is? Well, it's a personal decision

35:30.240 --> 35:35.520
based on your situation. So some people, before they get tenure, follow a very good advice of

35:35.520 --> 35:41.840
be quiet. After you get tenure, never shut up again. But that's not a bad idea. You'll definitely

35:41.840 --> 35:47.200
get someone disappointed in you. And that doesn't help your tenure case. I'm tenured, so I've been

35:47.200 --> 35:55.040
saying stupid things for years now. What do you think about an initiative such as the existential

35:55.040 --> 36:02.480
risk of territory? Is it useful to communicate this to more people in general, or to a certain

36:02.480 --> 36:08.400
subset of people? Or do you think it's basically something that should be solved among researchers?

36:09.040 --> 36:14.640
Well, if you think about developing a GI, working on superintelligence, you're really running an

36:14.640 --> 36:18.960
experiment on all the humans, right? You've got eight billion subjects, none of them consented to

36:18.960 --> 36:28.160
that work. The least you can do is tell them about it. That's actually great now to end this talk.

36:28.160 --> 36:31.920
If there's no more questions from the audience, and I think we've covered those.

36:31.920 --> 36:44.560
Yeah, and then I think we'll leave it here. It was super nice talking to you, and super nice to

36:44.560 --> 36:51.600
listen to your short presentation. And I hope that you will also enjoy the rest of the conference

36:51.600 --> 36:56.240
maybe tomorrow, and that will definitely be in touch and to cooperate more on this

36:57.120 --> 36:59.840
quite hairy problem, but still very interesting one to think about.

36:59.840 --> 37:04.080
Absolutely, and hopefully we'll meet in person one day. Likewise.

