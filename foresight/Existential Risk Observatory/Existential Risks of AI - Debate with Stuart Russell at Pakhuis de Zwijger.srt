1
00:00:00,000 --> 00:00:10,000
Welkom, everybody.

2
00:00:10,000 --> 00:00:18,000
Great to have you all here in a fully packed packhuis de Zwijger.

3
00:00:18,000 --> 00:00:20,000
Mijn naam is Maarten Gehem.

4
00:00:20,000 --> 00:00:27,000
I'm director of the argumentation factory and I have the honour of hosting this evening on existential risks.

5
00:00:28,000 --> 00:00:37,000
De late Stefan Hawken once said success in creating AI could be the biggest event in the history of our civilization.

6
00:00:37,000 --> 00:00:42,000
But it could also be the last unless we learn to avoid the risks.

7
00:00:42,000 --> 00:00:45,000
And that's precisely the topic of today.

8
00:00:45,000 --> 00:00:50,000
And we're going to talk about that with none other than professor Stuart Russell.

9
00:00:50,000 --> 00:00:56,000
I'll properly introduce him later, but first I'll hand over the floor to Otto Barton,

10
00:00:56,000 --> 00:00:59,000
who is the director of the existential risk observatory,

11
00:00:59,000 --> 00:01:02,000
and who is the instigator of this evening.

12
00:01:02,000 --> 00:01:04,000
Otto, come over.

13
00:01:04,000 --> 00:01:09,000
After Otto, Russell will give a talk, then we'll have a Q&A,

14
00:01:09,000 --> 00:01:13,000
and then afterwards we'll have a panel with five distinguished panelists.

15
00:01:13,000 --> 00:01:15,000
We're sitting over there, I'll also introduce you later.

16
00:01:15,000 --> 00:01:18,000
But now, without further ado, Otto.

17
00:01:18,000 --> 00:01:25,000
APPLAUS

18
00:01:25,000 --> 00:01:30,000
Dank je wel, en thank you very much, Maarten, for the introduction.

19
00:01:30,000 --> 00:01:33,000
Ja, I'm super happy that you're all here.

20
00:01:33,000 --> 00:01:35,000
Indeed, my name is Otto Barton.

21
00:01:35,000 --> 00:01:38,000
I'm the founder and the director of the existential risk observatory.

22
00:01:38,000 --> 00:01:43,000
We're an organization aimed at reducing existential risk by informing the public debates.

23
00:01:43,000 --> 00:01:48,000
I'm going to talk a little bit for a few minutes about what existential risk is

24
00:01:48,000 --> 00:01:50,000
and what our organization is doing.

25
00:01:52,000 --> 00:01:56,000
All right, so next slide, please.

26
00:01:56,000 --> 00:01:57,000
Thank you.

27
00:01:57,000 --> 00:02:00,000
So existential risks, what are they?

28
00:02:00,000 --> 00:02:07,000
Basically, as humanity, we've had about 300,000 years now already on this earth,

29
00:02:07,000 --> 00:02:09,000
and we have maybe about five billion to go,

30
00:02:09,000 --> 00:02:12,000
so an enormous amount before the sun explodes.

31
00:02:12,000 --> 00:02:18,000
So the huge majority of our time is still ahead of us,

32
00:02:18,000 --> 00:02:21,000
and an existential risk is something that can threaten that.

33
00:02:21,000 --> 00:02:27,000
So basically, the definition is a risk that threatens the destruction of humanity's long-term potential.

34
00:02:27,000 --> 00:02:31,000
It has been defined by Toby Ork from the Future of Humanity Institutes

35
00:02:31,000 --> 00:02:34,000
and his colleagues in this way.

36
00:02:34,000 --> 00:02:37,000
So this could be in a few ways.

37
00:02:37,000 --> 00:02:40,000
Of course, human extinction is a permanent state.

38
00:02:40,000 --> 00:02:46,000
So human extinction is one way in which we cannot have a future left anymore.

39
00:02:46,000 --> 00:02:49,000
So these five billion years, there won't be any value in that.

40
00:02:49,000 --> 00:02:53,000
An unrecoverable collapse or dystopian log-in are two other ways

41
00:02:53,000 --> 00:02:58,000
in which we could, which are contained in existential risk definition.

42
00:02:58,000 --> 00:03:03,000
So on the graph to the right, you see a rough estimate by Toby Ork,

43
00:03:03,000 --> 00:03:06,000
this researcher from the Future of Humanity Institute,

44
00:03:06,000 --> 00:03:11,000
on what causes could be for existential risks.

45
00:03:11,000 --> 00:03:13,000
So there are natural causes.

46
00:03:13,000 --> 00:03:16,000
There might be an asteroid strike, there might be a super volcano,

47
00:03:16,000 --> 00:03:19,000
but these are tiny and very well-known.

48
00:03:19,000 --> 00:03:21,000
So not the most interesting ones.

49
00:03:21,000 --> 00:03:25,000
To the left of that, you see a nuclear war and climate change,

50
00:03:25,000 --> 00:03:28,000
which are already somewhat bigger.

51
00:03:28,000 --> 00:03:34,000
But you can see that these are still fairly small compared to other existential risks.

52
00:03:34,000 --> 00:03:38,000
That climate change has a small chance of leading to human extinction.

53
00:03:38,000 --> 00:03:40,000
It doesn't mean that it's not a big problem.

54
00:03:40,000 --> 00:03:44,000
Of course, the chance that climate change will occur is 100% basically.

55
00:03:44,000 --> 00:03:46,000
And it is a very big issue.

56
00:03:46,000 --> 00:03:49,000
However, the chance that it leads to complete human extinction is relatively small,

57
00:03:49,000 --> 00:03:51,000
which is why you see a small bar here.

58
00:03:51,000 --> 00:03:55,000
Nuclear war, perhaps a little bit of a similar story,

59
00:03:55,000 --> 00:03:59,000
the chance that it occurs in the next hundred years is not that tiny.

60
00:03:59,000 --> 00:04:02,000
But the chance that it leads to human extinction is fairly small.

61
00:04:02,000 --> 00:04:07,000
To the left, you see even bigger chances of human extinction

62
00:04:07,000 --> 00:04:10,000
or the other existential risk categories.

63
00:04:10,000 --> 00:04:14,000
These are, for example, the man-made pandemics.

64
00:04:14,000 --> 00:04:19,000
The pandemics bar here is actually for man-made pandemics.

65
00:04:19,000 --> 00:04:23,000
A natural pandemic is also very unlikely to lead to human extinction.

66
00:04:23,000 --> 00:04:27,000
But a man-made pandemic with the biotechnology that we have developed right now

67
00:04:27,000 --> 00:04:30,000
and that we are still developing and democratizing.

68
00:04:30,000 --> 00:04:35,000
The chance that this could lead to human extinction in the next hundred years is non-negligible.

69
00:04:35,000 --> 00:04:40,000
To be orged and most of the other existential risk researchers

70
00:04:40,000 --> 00:04:43,000
think that it's unaligned AI, so artificial intelligence

71
00:04:43,000 --> 00:04:47,000
that has human level or even beyond human level, superhuman level.

72
00:04:47,000 --> 00:04:52,000
But it's unaligned, so it has different values than ours.

73
00:04:52,000 --> 00:04:56,000
This could be a relatively large chance of human extinction.

74
00:04:56,000 --> 00:05:00,000
We're going to talk more about it later, but I'll just leave it here for now.

75
00:05:00,000 --> 00:05:05,000
What else do we see, the total existential risk in the next hundred years

76
00:05:05,000 --> 00:05:09,000
is about a one in six estimate.

77
00:05:09,000 --> 00:05:11,000
There's a lot to be said about these estimates,

78
00:05:11,000 --> 00:05:15,000
but you can still draw a couple of robust conclusions, I think, from them.

79
00:05:15,000 --> 00:05:18,000
That a very likely source is new technology.

80
00:05:18,000 --> 00:05:22,000
And also that technology is man-made, so risk could be reduced in principle.

81
00:05:27,000 --> 00:05:29,000
Next slide, please.

82
00:05:29,000 --> 00:05:33,000
Solution directions for AI existential risk.

83
00:05:33,000 --> 00:05:36,000
These kind of also carry over for other technologies,

84
00:05:36,000 --> 00:05:39,000
but very broadly you could say

85
00:05:39,000 --> 00:05:42,000
if you don't want something to go very wrong with technology,

86
00:05:42,000 --> 00:05:45,000
you can either develop it safely or you cannot develop it.

87
00:05:45,000 --> 00:05:50,000
So basically for AI, this is built AGI safely or AI safety.

88
00:05:50,000 --> 00:05:54,000
So this is done by people who try to focus on AI alignment,

89
00:05:54,000 --> 00:05:57,000
trying to make AGI align to our values.

90
00:05:57,000 --> 00:06:00,000
We think, as an existential risk observatory,

91
00:06:00,000 --> 00:06:03,000
that's an important line of research and it should be scaled up.

92
00:06:03,000 --> 00:06:06,000
But on the other hand, it hasn't worked so far.

93
00:06:06,000 --> 00:06:09,000
People are already working on this for perhaps a few decades.

94
00:06:09,000 --> 00:06:13,000
And so far the consensus is that AI alignment,

95
00:06:13,000 --> 00:06:17,000
more or less the consensus is that AI alignment hasn't been successful yet.

96
00:06:17,000 --> 00:06:20,000
So another option could be to not build AGI

97
00:06:20,000 --> 00:06:23,000
and we think there might be some kind of regulation necessary for that.

98
00:06:23,000 --> 00:06:26,000
So this could be a software regulation, a data regulation

99
00:06:26,000 --> 00:06:29,000
or perhaps a hardware regulation.

100
00:06:29,000 --> 00:06:32,000
And we think these are all options that should be investigated.

101
00:06:32,000 --> 00:06:36,000
But we do think that regulation, whatever is the form it takes,

102
00:06:36,000 --> 00:06:41,000
will require widespread awareness and global cooperation.

103
00:06:41,000 --> 00:06:45,000
So for that, our solution is to inform the societal debate.

104
00:06:45,000 --> 00:06:47,000
So as an existential risk observatory,

105
00:06:47,000 --> 00:06:50,000
a small non-profit organization based in Amsterdam,

106
00:06:50,000 --> 00:06:55,000
we are focusing on informing the society about existential risk.

107
00:06:55,000 --> 00:06:58,000
So we do that by publishing articles in traditional media,

108
00:06:58,000 --> 00:07:01,000
for example in Time Magazine a few weeks ago

109
00:07:01,000 --> 00:07:04,000
and by organizing events such as this debate.

110
00:07:04,000 --> 00:07:07,000
And we also provide input to policy makers

111
00:07:07,000 --> 00:07:11,000
and I think it's a really nice sign that emotion was accepted

112
00:07:11,000 --> 00:07:14,000
by Dutch parliament a few weeks ago

113
00:07:14,000 --> 00:07:18,000
that is calling for more AI safety research in the Netherlands.

114
00:07:20,000 --> 00:07:25,000
So with that, I'm just going to end this small introduction talk

115
00:07:25,000 --> 00:07:29,000
and we're now going to watch a documentary

116
00:07:29,000 --> 00:07:32,000
which is already giving you a little bit of a flavor

117
00:07:32,000 --> 00:07:34,000
of the next speaker Stuart Russell.

118
00:07:34,000 --> 00:07:37,000
And I hope that you enjoyed a few minutes of documentary

119
00:07:37,000 --> 00:07:40,000
and I wish you a great rest of the evening.

120
00:07:40,000 --> 00:07:42,000
Thank you very much.

121
00:07:51,000 --> 00:07:54,000
Everything we have is a result of our intelligence.

122
00:07:54,000 --> 00:07:57,000
It's not the result of our big scary teeth

123
00:07:57,000 --> 00:08:00,000
or our large claws or our enormous muscles.

124
00:08:00,000 --> 00:08:03,000
It's because we're actually relatively intelligent.

125
00:08:03,000 --> 00:08:07,000
And among my generation, we're all having what we call holy cow

126
00:08:07,000 --> 00:08:09,000
or holy something else moments

127
00:08:09,000 --> 00:08:14,000
because we see that the technology is accelerating faster than we expected.

128
00:08:14,000 --> 00:08:17,000
I remember sitting around the table there

129
00:08:17,000 --> 00:08:20,000
with some of the best and the smartest minds in the world

130
00:08:20,000 --> 00:08:22,000
and what really struck me was

131
00:08:22,000 --> 00:08:26,000
maybe the human brain is not able to fully grasp

132
00:08:26,000 --> 00:08:30,000
the complexity of the world that we're confronted with.

133
00:08:30,000 --> 00:08:32,000
As it's currently constructed,

134
00:08:32,000 --> 00:08:35,000
the road that AI is following heads off a cliff

135
00:08:35,000 --> 00:08:38,000
and we need to change the direction that we're going

136
00:08:38,000 --> 00:08:41,000
so that we don't take the human race off the cliff.

137
00:08:41,000 --> 00:08:47,000
This is from the Deep Mind Reinforcement Learning System.

138
00:08:47,000 --> 00:08:50,000
Basically wakes up like a newborn baby

139
00:08:50,000 --> 00:08:53,000
and is shown the screen of an Atari video game

140
00:08:53,000 --> 00:08:56,000
and then has to learn to play the video game.

141
00:08:56,000 --> 00:09:02,000
It knows nothing about objects, about motion, about time.

142
00:09:02,000 --> 00:09:07,000
It only knows that there's an image on the screen and there's a score.

143
00:09:07,000 --> 00:09:11,000
So if your baby woke up the day it was born

144
00:09:11,000 --> 00:09:16,000
and by late afternoon was playing 40 different Atari video games

145
00:09:16,000 --> 00:09:18,000
at a superhuman level,

146
00:09:18,000 --> 00:09:20,000
you would be terrified.

147
00:09:20,000 --> 00:09:24,000
You would say my baby is possessed, send it back.

148
00:09:24,000 --> 00:09:27,000
The Deep Mind System can win at any game.

149
00:09:27,000 --> 00:09:32,000
It can already beat all the original Atari games.

150
00:09:32,000 --> 00:09:34,000
It is superhuman.

151
00:09:35,000 --> 00:09:36,000
It is superhuman.

152
00:09:36,000 --> 00:09:39,000
It plays the games at super speed in less than a minute.

153
00:09:44,000 --> 00:09:46,000
Deep Mind turned to another challenge

154
00:09:46,000 --> 00:09:48,000
and the challenge was the game of Go

155
00:09:48,000 --> 00:09:50,000
which people have generally argued

156
00:09:50,000 --> 00:09:52,000
has been beyond the power of computers

157
00:09:52,000 --> 00:09:55,000
to play with the best human Go players.

158
00:09:55,000 --> 00:09:58,000
First they challenged the European Go Champion.

159
00:10:00,000 --> 00:10:03,000
Then they challenged a Korean Go Champion.

160
00:10:05,000 --> 00:10:07,000
En they were able to win both times

161
00:10:07,000 --> 00:10:09,000
in a kind of striking fashion.

162
00:10:10,000 --> 00:10:12,000
You were reading articles in New York Times years ago

163
00:10:12,000 --> 00:10:15,000
talking about how Go would take a hundred years

164
00:10:15,000 --> 00:10:17,000
for us to solve.

165
00:10:17,000 --> 00:10:20,000
People said, well, you know, but that's still just a board.

166
00:10:20,000 --> 00:10:22,000
Poker is an art.

167
00:10:22,000 --> 00:10:23,000
Poker involves reading people.

168
00:10:23,000 --> 00:10:25,000
Poker involves lying, bluffing.

169
00:10:25,000 --> 00:10:26,000
It's not an exact thing.

170
00:10:26,000 --> 00:10:28,000
That will never be a computer.

171
00:10:28,000 --> 00:10:29,000
You can't do that.

172
00:10:29,000 --> 00:10:31,000
They took the best poker players in the world

173
00:10:31,000 --> 00:10:34,000
en took seven days for the computer

174
00:10:34,000 --> 00:10:37,000
to start demolishing the humans.

175
00:10:37,000 --> 00:10:39,000
So it's the best poker player in the world.

176
00:10:39,000 --> 00:10:40,000
It's the best Go player in the world.

177
00:10:40,000 --> 00:10:44,000
And the pattern here is that AI might take a little while

178
00:10:44,000 --> 00:10:47,000
to wrap its tentacles around a new skill.

179
00:10:47,000 --> 00:10:52,000
But when it does, when it gets it, it is unstoppable.

180
00:10:52,000 --> 00:10:59,000
MUZIEK

181
00:10:59,000 --> 00:11:02,000
DeepMind's AI has administrator-level access

182
00:11:02,000 --> 00:11:04,000
to Google's servers

183
00:11:04,000 --> 00:11:07,000
to optimize energy usage at the data centers.

184
00:11:07,000 --> 00:11:11,000
However, this could be an unintentional trojan horse.

185
00:11:11,000 --> 00:11:14,000
DeepMind has to have complete control of the data centers.

186
00:11:14,000 --> 00:11:16,000
So with a little software update,

187
00:11:16,000 --> 00:11:19,000
that AI could take complete control of the whole Google system,

188
00:11:19,000 --> 00:11:21,000
which means they can do anything.

189
00:11:21,000 --> 00:11:23,000
They can look at all your data and do anything.

190
00:11:23,000 --> 00:11:28,000
MUZIEK

191
00:11:28,000 --> 00:11:30,000
We were rapidly headed towards digital superintelligence

192
00:11:30,000 --> 00:11:32,000
that far exceeds any human.

193
00:11:32,000 --> 00:11:34,000
I think it's very obvious.

194
00:11:34,000 --> 00:11:37,000
The problem is we're not going to suddenly hit human-level intelligence

195
00:11:37,000 --> 00:11:40,000
and say, OK, let's stop research.

196
00:11:40,000 --> 00:11:42,000
It's going to go beyond human-level intelligence

197
00:11:42,000 --> 00:11:44,000
into what's called superintelligence

198
00:11:44,000 --> 00:11:46,000
and that's anything smarter than us.

199
00:11:46,000 --> 00:11:50,000
AI at the superhuman level, if we succeed with that,

200
00:11:50,000 --> 00:11:54,000
is by far the most powerful invention we've ever made

201
00:11:54,000 --> 00:11:57,000
and the last invention we ever have to make.

202
00:11:57,000 --> 00:12:00,000
And if we create AI that's smarter than us,

203
00:12:00,000 --> 00:12:02,000
we have to be open to the possibility

204
00:12:02,000 --> 00:12:05,000
that we might actually lose control of them.

205
00:12:05,000 --> 00:12:08,000
MUZIEK

206
00:12:08,000 --> 00:12:11,000
Let's say you give it some objective like curing cancer

207
00:12:11,000 --> 00:12:14,000
and then you discover that the way it chooses to go about that

208
00:12:14,000 --> 00:12:19,000
is actually in conflict with a lot of other things you care about.

209
00:12:19,000 --> 00:12:23,000
AI doesn't have to be able to destroy humanity.

210
00:12:23,000 --> 00:12:27,000
If AI has a goal and humanity just happens to be in the way,

211
00:12:27,000 --> 00:12:30,000
it will destroy humanity as a matter of course.

212
00:12:30,000 --> 00:12:32,000
Without even thinking about it, no hard feelings.

213
00:12:32,000 --> 00:12:34,000
It's just like if we're building a road

214
00:12:34,000 --> 00:12:36,000
and an ant hill happens to be in the way,

215
00:12:36,000 --> 00:12:40,000
we don't hate ants, we're just building a road

216
00:12:40,000 --> 00:12:42,000
and so goodbye, Ant Hill.

217
00:12:42,000 --> 00:12:45,000
MUZIEK

218
00:12:49,000 --> 00:12:53,000
OK, if you weren't scared already.

219
00:12:53,000 --> 00:12:59,000
Make sure humanity doesn't run off a cliff.

220
00:12:59,000 --> 00:13:01,000
Stuart Russell said.

221
00:13:01,000 --> 00:13:04,000
So who better to tell us how not to run off a cliff

222
00:13:04,000 --> 00:13:06,000
than Professor Russell himself.

223
00:13:06,000 --> 00:13:08,000
And that's precisely what we're going to hear.

224
00:13:08,000 --> 00:13:14,000
Professor Russell is one of the leading experts

225
00:13:14,000 --> 00:13:17,000
in AI research and AI safety research.

226
00:13:17,000 --> 00:13:20,000
He's based at the University of California, Berkeley.

227
00:13:20,000 --> 00:13:26,000
He's one of the writers, co-author of the standard textbook in AI research.

228
00:13:26,000 --> 00:13:29,000
AI, a modern approach.

229
00:13:29,000 --> 00:13:33,000
And recently he wrote a magnificent book called Human Alignment.

230
00:13:33,000 --> 00:13:35,000
I don't know who read the book already.

231
00:13:35,000 --> 00:13:37,000
Let me see some hands here.

232
00:13:37,000 --> 00:13:39,000
OK, all right.

233
00:13:39,000 --> 00:13:41,000
Well, it's well worth the effort.

234
00:13:41,000 --> 00:13:45,000
And he'll probably tell you why in the next 20 minutes.

235
00:13:45,000 --> 00:13:50,000
Professor Russell is beamed to us all over the world,

236
00:13:50,000 --> 00:13:54,000
from all across the world, from California where he's based right now.

237
00:13:54,000 --> 00:13:58,000
So we're going to see him on the screen in a minute or two.

238
00:13:58,000 --> 00:14:02,000
And afterwards there's ample room for questions and answers.

239
00:14:02,000 --> 00:14:07,000
So we'll have some room here for a discussion with Mr. Russell himself.

240
00:14:07,000 --> 00:14:09,000
And there he is.

241
00:14:09,000 --> 00:14:14,000
Professor Russell, the floor is yours.

242
00:14:14,000 --> 00:14:17,000
Hey there, thank you very much.

243
00:14:17,000 --> 00:14:20,000
So I should just make a slight correction.

244
00:14:20,000 --> 00:14:23,000
I'm not in California, I'm actually at MIT.

245
00:14:23,000 --> 00:14:29,000
But I'm on my way home to California later on this evening.

246
00:14:29,000 --> 00:14:33,000
So I think the little movie that you just saw

247
00:14:33,000 --> 00:14:37,000
actually brings up a lot of important points.

248
00:14:37,000 --> 00:14:40,000
So I don't have to repeat them.

249
00:14:40,000 --> 00:14:44,000
But I will give you a short presentation,

250
00:14:44,000 --> 00:14:47,000
which in some ways brings it up to date.

251
00:14:47,000 --> 00:14:51,000
So let's say a little bit about what we're doing to help

252
00:14:51,000 --> 00:14:55,000
and about the current situation.

253
00:14:55,000 --> 00:14:58,000
So together everyone on the same page.

254
00:14:58,000 --> 00:15:00,000
What is AI?

255
00:15:00,000 --> 00:15:02,000
It's not a particular technology.

256
00:15:02,000 --> 00:15:07,000
It's a task just like the task of physics is to understand the universe.

257
00:15:07,000 --> 00:15:11,000
The task of AI is to make intelligent machines.

258
00:15:11,000 --> 00:15:14,000
And then the question is, well, what does that mean?

259
00:15:14,000 --> 00:15:16,000
What does it mean for a machine to be intelligent?

260
00:15:16,000 --> 00:15:19,000
And for most of the history of AI, it's meant the following.

261
00:15:19,000 --> 00:15:22,000
Machines are intelligent to the extent that their actions

262
00:15:22,000 --> 00:15:26,000
can be expected to achieve their objectives.

263
00:15:26,000 --> 00:15:29,000
And this is so pervasive, I'll call it the standard model.

264
00:15:29,000 --> 00:15:34,000
And many forms of AI, problem solving, planning, reinforcement learning,

265
00:15:34,000 --> 00:15:37,000
or conform to the standard model,

266
00:15:37,000 --> 00:15:40,000
as well as many other disciplines like control theory

267
00:15:40,000 --> 00:15:42,000
and operations research and economics.

268
00:15:42,000 --> 00:15:48,000
You create optimizing machinery and then you specify some objective.

269
00:15:48,000 --> 00:15:53,000
You put that into the machinery and then it becomes the objective of the machine.

270
00:15:53,000 --> 00:15:57,000
And then it finds ways to fulfill that objective.

271
00:15:57,000 --> 00:16:00,000
It's a very natural way to go about doing things.

272
00:16:00,000 --> 00:16:03,000
Later on, I'll argue that it's completely wrong.

273
00:16:03,000 --> 00:16:07,000
But for now, take that as the standard model of AI.

274
00:16:07,000 --> 00:16:13,000
And since the beginning, we've been looking at what we might call general purpose AI.

275
00:16:13,000 --> 00:16:20,000
So not just an AI designed to achieve some specific objective,

276
00:16:20,000 --> 00:16:25,000
but actually one that's capable of achieving more or less any objective that we might give it.

277
00:16:25,000 --> 00:16:32,000
And learning to do that very quickly at a level that exceeds human capabilities

278
00:16:32,000 --> 00:16:36,000
eventually in every dimension.

279
00:16:36,000 --> 00:16:38,000
So that's the goal.

280
00:16:38,000 --> 00:16:44,000
And rather than be accused of always talking about doom,

281
00:16:44,000 --> 00:16:48,000
I'll begin by talking about the upside.

282
00:16:48,000 --> 00:16:53,000
And it's really the upside that explains why the field exists in the first place

283
00:16:53,000 --> 00:16:57,000
and why people are investing lots of money in it

284
00:16:57,000 --> 00:16:59,000
and why lots of smart people are working on it.

285
00:17:00,000 --> 00:17:03,000
Because the potential upside is really enormous.

286
00:17:03,000 --> 00:17:05,000
For example, if you had general purpose AI,

287
00:17:05,000 --> 00:17:11,000
then you could do by definition what humans already know how to do,

288
00:17:11,000 --> 00:17:14,000
which is to deliver, among other things,

289
00:17:14,000 --> 00:17:20,000
to deliver a good standard of living to maybe hundreds of millions

290
00:17:20,000 --> 00:17:22,000
or maybe close to a billion people on Earth

291
00:17:22,000 --> 00:17:26,000
have what we might call a good standard of living.

292
00:17:26,000 --> 00:17:33,000
En we could deliver it actually on much greater scale at much lower cost

293
00:17:33,000 --> 00:17:37,000
because the cost involved in delivering a standard of living

294
00:17:37,000 --> 00:17:41,000
is the expensive time of other human beings.

295
00:17:41,000 --> 00:17:46,000
So if we have general purpose AI, we could, for example,

296
00:17:46,000 --> 00:17:51,000
use it to give everyone on Earth that same respectable standard of living

297
00:17:51,000 --> 00:17:55,000
that we might see in some developed countries.

298
00:17:56,000 --> 00:18:00,000
En if you calculate the sort of economic value of that,

299
00:18:00,000 --> 00:18:03,000
it's about a tenfold increase in GDP

300
00:18:03,000 --> 00:18:06,000
and that converts to what economists call the net present value.

301
00:18:06,000 --> 00:18:08,000
So that's sort of what's the cash equivalent

302
00:18:08,000 --> 00:18:11,000
of having that increased income stream.

303
00:18:11,000 --> 00:18:15,000
So it comes to about $13.5 quadrillion.

304
00:18:15,000 --> 00:18:19,000
So that's a lower bound, a low ball estimate

305
00:18:19,000 --> 00:18:24,000
on the cash value of general purpose AI as a technology.

306
00:18:24,000 --> 00:18:27,000
We could of course have many more things besides that.

307
00:18:27,000 --> 00:18:30,000
I think we could have much better,

308
00:18:30,000 --> 00:18:34,000
more individualized ongoing healthcare.

309
00:18:34,000 --> 00:18:37,000
We could have very personalized

310
00:18:37,000 --> 00:18:41,000
and very, very effective education for every child on Earth.

311
00:18:41,000 --> 00:18:45,000
We could speed up the rate of scientific progress

312
00:18:45,000 --> 00:18:48,000
and perhaps many other things.

313
00:18:48,000 --> 00:18:52,000
I used to have advances in politics on that slide,

314
00:18:52,000 --> 00:18:55,000
but I took it off for obvious reasons.

315
00:18:55,000 --> 00:18:59,000
So now the question is, well, where are we?

316
00:18:59,000 --> 00:19:03,000
A lot of people seem to be saying that we're already there,

317
00:19:03,000 --> 00:19:07,000
that we've already created general purpose AI.

318
00:19:07,000 --> 00:19:11,000
And I think this is not true.

319
00:19:11,000 --> 00:19:14,000
I think there's something going on,

320
00:19:14,000 --> 00:19:18,000
but we are still far away from general purpose AI.

321
00:19:18,000 --> 00:19:21,000
And what's going on, of course, is large language models.

322
00:19:21,000 --> 00:19:26,000
The chat GPT, GPT4, BARD, Lambda, Palm,

323
00:19:26,000 --> 00:19:31,000
all these models are displaying very intriguing

324
00:19:31,000 --> 00:19:34,000
and in some cases very impressive behaviors.

325
00:19:34,000 --> 00:19:38,000
And I think they are probably a piece of the puzzle

326
00:19:38,000 --> 00:19:39,000
of general purpose AI,

327
00:19:39,000 --> 00:19:42,000
but they are not by themselves general purpose AI.

328
00:19:42,000 --> 00:19:44,000
And at the moment, I would say,

329
00:19:44,000 --> 00:19:47,000
we don't know what shape this puzzle piece is

330
00:19:47,000 --> 00:19:50,000
and we don't know how to fit it into the puzzle.

331
00:19:50,000 --> 00:19:53,000
We're not really sure what the other pieces are.

332
00:19:53,000 --> 00:19:57,000
I think one of the things we're learning now

333
00:19:57,000 --> 00:19:59,000
is that the pieces of this puzzle

334
00:19:59,000 --> 00:20:02,000
are probably not the pieces that we thought

335
00:20:02,000 --> 00:20:07,000
made up the puzzle maybe 15 or 20 years ago.

336
00:20:07,000 --> 00:20:10,000
So just to illustrate a few reasons

337
00:20:10,000 --> 00:20:15,000
why I don't think these systems are the solution,

338
00:20:15,000 --> 00:20:16,000
they're not general purpose AI.

339
00:20:16,000 --> 00:20:19,000
So here's a simple example from chat GPT

340
00:20:19,000 --> 00:20:22,000
to me by my friend Prasad Tattapalli.

341
00:20:22,000 --> 00:20:23,000
So the first question,

342
00:20:23,000 --> 00:20:25,000
which is bigger an elephant or a cat,

343
00:20:25,000 --> 00:20:28,000
and it answers an elephant is bigger than a cat.

344
00:20:28,000 --> 00:20:30,000
So far so good.

345
00:20:30,000 --> 00:20:33,000
Which is not bigger than the other, an elephant or a cat.

346
00:20:33,000 --> 00:20:35,000
And it says neither an elephant nor a cat

347
00:20:35,000 --> 00:20:38,000
is bigger than the other.

348
00:20:38,000 --> 00:20:41,000
So these are two consecutive sentences

349
00:20:41,000 --> 00:20:43,000
that he asked it.

350
00:20:43,000 --> 00:20:48,000
And it seems clear from this that in a real sense

351
00:20:48,000 --> 00:20:51,000
chat GPT doesn't know facts.

352
00:20:51,000 --> 00:20:54,000
So when you ask a human a question,

353
00:20:54,000 --> 00:20:56,000
at least our impression of what happens

354
00:20:56,000 --> 00:21:00,000
is that we refer to an internal world model

355
00:21:00,000 --> 00:21:02,000
that is self consistent,

356
00:21:02,000 --> 00:21:07,000
that's composed of facts that we understand about the world.

357
00:21:07,000 --> 00:21:09,000
And then we ask in a question

358
00:21:09,000 --> 00:21:11,000
relative to that internal world model,

359
00:21:11,000 --> 00:21:13,000
we find out what the answer is

360
00:21:13,000 --> 00:21:17,000
and we express the answer in natural language.

361
00:21:17,000 --> 00:21:19,000
In natural language as the answer to the question.

362
00:21:19,000 --> 00:21:21,000
But that clearly can't be what's going on

363
00:21:21,000 --> 00:21:24,000
in at least in this example

364
00:21:24,000 --> 00:21:27,000
because you could not have an internal world model

365
00:21:27,000 --> 00:21:29,000
that contradicted itself

366
00:21:29,000 --> 00:21:31,000
in which the elephants are both bigger than cats

367
00:21:31,000 --> 00:21:33,000
and not bigger than cats.

368
00:21:33,000 --> 00:21:35,000
So in a real sense,

369
00:21:35,000 --> 00:21:37,000
I think we could say that there's evidence

370
00:21:37,000 --> 00:21:39,000
that these systems do not know things

371
00:21:39,000 --> 00:21:43,000
in the way that word is usually used.

372
00:21:43,000 --> 00:21:45,000
I also want to point out,

373
00:21:45,000 --> 00:21:49,000
in the movie you just saw that several years ago

374
00:21:49,000 --> 00:21:53,000
we defeated the best human go players.

375
00:21:53,000 --> 00:21:55,000
In fact, when that happened

376
00:21:55,000 --> 00:21:58,000
to the Chinese world champion in 2017

377
00:21:58,000 --> 00:22:00,000
that was called China's Sputnik moment.

378
00:22:00,000 --> 00:22:04,000
That event precipitated a total change

379
00:22:04,000 --> 00:22:07,000
in Chinese government policy around AI

380
00:22:07,000 --> 00:22:10,000
and the commitment of hundreds of billions of dollars

381
00:22:10,000 --> 00:22:12,000
worth of investment.

382
00:22:12,000 --> 00:22:15,000
The commitments to train hundreds of thousands

383
00:22:15,000 --> 00:22:19,000
of AI researchers, et cetera, et cetera.

384
00:22:19,000 --> 00:22:26,000
We decided to see how good the go programs really are.

385
00:22:26,000 --> 00:22:29,000
We played one of our team members,

386
00:22:29,000 --> 00:22:33,000
Kellan Pelrin, is a reasonably good amateur go player.

387
00:22:33,000 --> 00:22:36,000
His rating is about 2,300.

388
00:22:36,000 --> 00:22:41,000
On that scale, the human world champion is about 3,800.

389
00:22:41,000 --> 00:22:45,000
The go programs are far ahead of human beings now.

390
00:22:45,000 --> 00:22:48,000
In 2017, or 2016,

391
00:22:48,000 --> 00:22:50,000
they were about the level of the human world champion,

392
00:22:50,000 --> 00:22:52,000
so around 3,800.

393
00:22:52,000 --> 00:22:55,000
Now they've reached around 5,200.

394
00:22:55,000 --> 00:22:59,000
JBX Kata 005 is the name of the current number one

395
00:22:59,000 --> 00:23:02,000
go playing program in the world.

396
00:23:02,000 --> 00:23:08,000
Its rating is 1,400 points higher than any human player.

397
00:23:08,000 --> 00:23:13,000
Kellan had been playing against this program

398
00:23:13,000 --> 00:23:16,000
and had beaten it 14 times in a row

399
00:23:16,000 --> 00:23:20,000
and then decided to give it a nine stone handicap.

400
00:23:20,000 --> 00:23:23,000
That means that black, the computer,

401
00:23:23,000 --> 00:23:25,000
starts with nine stones on the board,

402
00:23:25,000 --> 00:23:28,000
as we're showing here, which is an enormous advantage.

403
00:23:28,000 --> 00:23:33,000
This is the kind of handicap that you give to a small child

404
00:23:33,000 --> 00:23:37,000
who's learning the game if you're a go teacher,

405
00:23:37,000 --> 00:23:41,000
just so that the child feels they have a chance.

406
00:23:41,000 --> 00:23:45,000
Now I'll show you what happens in the game.

407
00:23:45,000 --> 00:23:48,000
Remember, the computer is black.

408
00:23:48,000 --> 00:23:50,000
Kellan, the human, is white.

409
00:23:50,000 --> 00:23:53,000
It doesn't really matter if you don't understand go.

410
00:23:53,000 --> 00:23:56,000
Basically, you're trying to surround territory with your pieces

411
00:23:56,000 --> 00:23:59,000
and to surround your opponent's pieces and capture them.

412
00:23:59,000 --> 00:24:03,000
Notice what's happening in the bottom right corner of the board.

413
00:24:03,000 --> 00:24:06,000
The white is making a little group.

414
00:24:06,000 --> 00:24:09,000
It sort of has a kind of a figure 80 sort of shape.

415
00:24:09,000 --> 00:24:12,000
And then black immediately starts to surround that group

416
00:24:12,000 --> 00:24:15,000
in order to prevent it from capturing more territory.

417
00:24:15,000 --> 00:24:18,000
And now white starts to surround the black group

418
00:24:18,000 --> 00:24:21,000
so that this larger white circle is forming.

419
00:24:21,000 --> 00:24:23,000
So it's kind of a circular sandwich.

420
00:24:23,000 --> 00:24:25,000
There's a white piece in the middle

421
00:24:25,000 --> 00:24:27,000
and there's a white thing around the outside

422
00:24:27,000 --> 00:24:30,000
and it's sandwiching in that black group.

423
00:24:30,000 --> 00:24:32,000
In the end, there's no attention

424
00:24:32,000 --> 00:24:34,000
and then loses all of those pieces.

425
00:24:34,000 --> 00:24:40,000
So what's going on here seems to be

426
00:24:40,000 --> 00:24:42,000
that these super human go programs

427
00:24:42,000 --> 00:24:45,000
actually have not correctly learned

428
00:24:45,000 --> 00:24:47,000
what it means to be a group of stones,

429
00:24:47,000 --> 00:24:49,000
what it means to be alive or dead

430
00:24:49,000 --> 00:24:52,000
which are the most basic concepts in go.

431
00:24:52,000 --> 00:24:57,000
And that allows Kellan, the human to defeat these programs

432
00:24:57,000 --> 00:25:00,520
met alle voordelingsprogramma's, die worden geschreven door verschillende mensen

433
00:25:00,520 --> 00:25:05,400
met verschillende trainingsregime's en verschillende netwerkstructuren en zo en zo.

434
00:25:05,400 --> 00:25:10,440
Ze voelen allemaal in dezelfde manier, die is echt remarkably.

435
00:25:10,440 --> 00:25:14,720
En ik denk dat het eigenlijk een consequentie is van de fact dat ze proberen te trainen

436
00:25:14,720 --> 00:25:21,640
circuitten om concepten te representeren zoals connectiviteit en surroundering,

437
00:25:21,640 --> 00:25:26,840
die eigenlijk niet mogelijk kunnen representeren correct via circuitten.

438
00:25:26,840 --> 00:25:31,720
Je kunt alleen een soort van patchy, fragmentair, finite approximatie

439
00:25:31,720 --> 00:25:36,840
te die concepten, maar met een generele programma, zoals Python,

440
00:25:36,840 --> 00:25:40,840
het is heel makkelijk om die concepten correct te representeren.

441
00:25:40,840 --> 00:25:43,560
Dus dit is een fundamentele limitering,

442
00:25:43,560 --> 00:25:52,280
alstublieft als we het uitzenden, met diep leren als een manier te leren over de wereld.

443
00:25:52,360 --> 00:25:55,960
Oké, dus ik denk, in mijn gevoel, dat we nog steeds een manier te gaan

444
00:25:55,960 --> 00:26:01,080
voor generalpurpos A.I. en ik heb een aantal van de dingen

445
00:26:01,080 --> 00:26:03,960
ik denk dat je hier gevoel hebt, waarschijnlijk de derde,

446
00:26:03,960 --> 00:26:09,320
onze behoorlijkheid om niet alleen te bekijken, waarin de go-programma's

447
00:26:09,320 --> 00:26:11,960
wel kunnen doen, zelfs als ze missen

448
00:26:13,320 --> 00:26:16,840
over de kwaliteit van de positie die ze reachen,

449
00:26:16,920 --> 00:26:21,560
zijn we zeker kunnen planen voor 50 of 60 of even 100 behoorlijkheid

450
00:26:21,560 --> 00:26:25,640
naar de toekomst, maar mensen planen op veel levens van de extractie.

451
00:26:25,640 --> 00:26:31,000
We planen over tijdscalingen van jaren en ook over tijdscalingen

452
00:26:31,000 --> 00:26:33,400
van milliseconden en op elke tijdscaling in de tweede.

453
00:26:34,280 --> 00:26:39,320
En als je op een phd maakt, bijvoorbeeld, dat gaat over een trillion

454
00:26:39,320 --> 00:26:46,440
motocontrole acties, en niet alleen maar 50 motocontrole acties,

455
00:26:46,520 --> 00:26:51,720
dus we kunnen in het universen, het heel complex universen,

456
00:26:51,720 --> 00:26:55,320
door onze behoorlijkheid om op deze verschillende levens van de extractie te opereren.

457
00:26:56,040 --> 00:26:59,880
En dat is iets dat nog wel duidelijk onderzoek in A.I.

458
00:27:00,840 --> 00:27:02,920
Dus ik denk dat het nog wel gelijkgemaakt is,

459
00:27:02,920 --> 00:27:05,560
omdat de manier van momentum, investering,

460
00:27:07,880 --> 00:27:12,760
van geniën die dit gebouw wordt geplaatst, dat deze voorkomsten ergens gebeuren.

461
00:27:12,760 --> 00:27:14,280
Het is gewoon heel hard te predicteren

462
00:27:14,920 --> 00:27:16,040
wanneer ze gaan gebeuren.

463
00:27:19,000 --> 00:27:23,000
En om een voorbeeld te geven van hoe hard het is om te predicteren

464
00:27:23,000 --> 00:27:26,680
wanneer deze dingen gaan gebeuren, kunnen we in historie kijken

465
00:27:26,680 --> 00:27:30,200
tot de laatste keer dat we een civilisatie-endende technologie ontvangen,

466
00:27:30,760 --> 00:27:32,920
waarom het automatische energie was.

467
00:27:33,960 --> 00:27:39,880
En we weten sinds 1905, en speciale relativiteit,

468
00:27:39,880 --> 00:27:43,240
dat er een enorm amount van energie in atomen loopt.

469
00:27:43,320 --> 00:27:47,080
En als je er tussen verschillende typen van atomen kan veranderen,

470
00:27:47,080 --> 00:27:48,280
kan je dat energie veranderen.

471
00:27:48,840 --> 00:27:51,560
Maar de fysieke ontdekking hier,

472
00:27:51,560 --> 00:27:53,160
persoonlijke door Lord Rutherford,

473
00:27:53,160 --> 00:27:56,600
de ledende nucleofysicist, geloofde dat dat mogelijk was.

474
00:27:57,320 --> 00:28:00,840
Hij was gevraagd op een meet in september 11, 1933,

475
00:28:00,840 --> 00:28:03,000
dacht je dat in 25 of 30 jaar tijd,

476
00:28:03,000 --> 00:28:06,520
we zouden kunnen vinden een manier om deze energie te ontvangen.

477
00:28:06,520 --> 00:28:09,320
En hij zei dat iemand die voor een verhaal van poder

478
00:28:09,320 --> 00:28:12,120
in de transformatie van de atomen, een moeenscheid is.

479
00:28:13,880 --> 00:28:17,400
En de volgende morning, Leo Zillard,

480
00:28:17,400 --> 00:28:19,640
die een Hungarian fysiast was,

481
00:28:19,640 --> 00:28:21,160
die van Hungary had afgesloten,

482
00:28:22,360 --> 00:28:24,600
en was in London in de tijd,

483
00:28:25,320 --> 00:28:26,760
reed dit in de newspaper,

484
00:28:26,760 --> 00:28:27,720
en hij ging voor een reis,

485
00:28:27,720 --> 00:28:30,200
en hij inventeerde de neutron-induust

486
00:28:30,200 --> 00:28:31,240
nuclea-chain reactie,

487
00:28:31,880 --> 00:28:33,480
die de solution is

488
00:28:33,480 --> 00:28:35,880
om hoe je de energie van het atom op te leveren.

489
00:28:36,760 --> 00:28:38,520
Dus hij ging van het mogelijkheden

490
00:28:38,520 --> 00:28:41,480
tot de essentieel gevolgd in 16 uur.

491
00:28:41,800 --> 00:28:45,320
Dus als ik zei dat het onpredictabel is,

492
00:28:45,320 --> 00:28:47,240
het is onpredictabel

493
00:28:47,240 --> 00:28:50,440
wanneer deze avond gaat gebeuren.

494
00:28:50,440 --> 00:28:52,680
Ik denk dat er omdat er nogal wat we nodig zijn,

495
00:28:54,040 --> 00:28:56,520
het is ongelijkbaar dat er allemaal in een goede gebeuren zijn,

496
00:28:56,520 --> 00:28:58,280
dus we mogen een paar eerlijke oorlogen krijgen.

497
00:28:59,720 --> 00:29:01,160
Dus over een eerlijke oorlogen,

498
00:29:01,160 --> 00:29:02,680
dit is de titel van een paper

499
00:29:02,680 --> 00:29:04,920
geschreven door een dozen

500
00:29:04,920 --> 00:29:08,360
erg besteldere researchers in Microsoft.

501
00:29:08,360 --> 00:29:10,920
Er zijn twee bezoekers van de Nationaal Kool,

502
00:29:10,920 --> 00:29:12,200
de Nationaal Academies hier,

503
00:29:13,000 --> 00:29:14,360
en andere bezoekers

504
00:29:14,360 --> 00:29:17,160
die een heel signifieke contributie zijn

505
00:29:17,160 --> 00:29:18,440
aan de theorie van machine-learning.

506
00:29:19,960 --> 00:29:21,800
En ze hebben met GPT-4

507
00:29:23,240 --> 00:29:25,160
de laatste systeem uit OpenAI gesproken.

508
00:29:25,160 --> 00:29:26,600
Ze hebben het voor een aantal maanden

509
00:29:27,720 --> 00:29:28,920
voordat het gebouwd was.

510
00:29:29,800 --> 00:29:32,200
En ze hadden veel bezoekers

511
00:29:32,200 --> 00:29:33,880
om het te bekijken hoe goed het was.

512
00:29:34,600 --> 00:29:35,560
En hun conclusie,

513
00:29:37,000 --> 00:29:38,200
zoals dit titel is,

514
00:29:38,200 --> 00:29:40,680
is dat ze geloven dat GPT-4

515
00:29:40,680 --> 00:29:43,000
overspraken van artificieel

516
00:29:43,000 --> 00:29:44,360
en generale intelligenteel zijn.

517
00:29:45,320 --> 00:29:47,400
Dus ze zeggen alstublieft

518
00:29:47,400 --> 00:29:50,680
dat er een reale probleem

519
00:29:51,400 --> 00:29:52,920
tegen AGI

520
00:29:52,920 --> 00:29:54,760
gebeurt met dit systeem.

521
00:29:57,400 --> 00:29:58,120
Oké, dus,

522
00:29:58,600 --> 00:29:59,880
dus terug naar de vraag

523
00:29:59,880 --> 00:30:01,800
over wat we vergelijden,

524
00:30:02,920 --> 00:30:03,960
dit is Alan Turing,

525
00:30:04,520 --> 00:30:06,280
die de founder van Computerscience is,

526
00:30:07,080 --> 00:30:09,480
en in veel manier de founder van AI ook.

527
00:30:10,920 --> 00:30:12,120
En in 1951,

528
00:30:12,120 --> 00:30:13,880
hij was asked that question at a lecture.

529
00:30:14,760 --> 00:30:15,720
What if we succeed?

530
00:30:15,720 --> 00:30:16,680
And this is what he said.

531
00:30:17,240 --> 00:30:19,400
It seems parable that once the machine

532
00:30:19,400 --> 00:30:21,080
thinking method had started,

533
00:30:21,080 --> 00:30:23,240
it would not take long to outstrip

534
00:30:23,240 --> 00:30:24,280
our feeble powers.

535
00:30:24,840 --> 00:30:26,440
At some stage, therefore,

536
00:30:26,440 --> 00:30:28,040
we should have to expect the machines

537
00:30:28,040 --> 00:30:28,840
to take control.

538
00:30:31,160 --> 00:30:31,720
So that's it.

539
00:30:31,720 --> 00:30:34,440
So he offers no mitigation,

540
00:30:35,160 --> 00:30:36,680
no solution, no apology.

541
00:30:37,720 --> 00:30:40,120
You almost get a sense of resignation

542
00:30:40,120 --> 00:30:42,680
about this, about this prediction.

543
00:30:43,320 --> 00:30:44,840
So why is it?

544
00:30:45,560 --> 00:30:47,000
Where is this prediction coming from?

545
00:30:47,640 --> 00:30:49,560
This idea that as you make AI

546
00:30:50,200 --> 00:30:51,080
better and better,

547
00:30:52,360 --> 00:30:54,360
things could end up getting worse

548
00:30:54,360 --> 00:30:55,480
and worse as a result.

549
00:30:57,560 --> 00:31:00,120
And I think underlying his prediction is,

550
00:31:00,120 --> 00:31:02,040
I'm going to put it in a more positive way,

551
00:31:02,040 --> 00:31:03,800
rather than a prediction, a question.

552
00:31:04,440 --> 00:31:06,200
How do we retain power

553
00:31:07,480 --> 00:31:09,640
over entities more powerful than us?

554
00:31:10,120 --> 00:31:10,600
Forever.

555
00:31:11,800 --> 00:31:12,840
That's the question

556
00:31:12,840 --> 00:31:15,480
that I think he's asking himself

557
00:31:15,480 --> 00:31:18,760
and he's failing to find an answer to it.

558
00:31:18,760 --> 00:31:20,360
And so that's his prediction.

559
00:31:24,280 --> 00:31:26,440
So I've spent the last 10 years also

560
00:31:26,440 --> 00:31:28,840
trying to figure out an answer to this

561
00:31:30,200 --> 00:31:31,480
that isn't, we can't.

562
00:31:34,040 --> 00:31:34,760
And to do that,

563
00:31:35,400 --> 00:31:36,920
I've been trying to understand

564
00:31:36,920 --> 00:31:38,600
where things go wrong.

565
00:31:38,600 --> 00:31:40,040
And I think they go wrong

566
00:31:40,040 --> 00:31:43,400
because of a phenomenon called misalignment.

567
00:31:44,280 --> 00:31:46,280
And that was described a little bit in the movie.

568
00:31:46,920 --> 00:31:48,440
I think Elon Musk talked about it

569
00:31:48,440 --> 00:31:49,640
and I talked about it a little bit.

570
00:31:50,200 --> 00:31:52,920
This idea that systems that are pursuing

571
00:31:52,920 --> 00:31:55,960
an objective, as in the Standard Model,

572
00:31:55,960 --> 00:31:57,480
if that objective is not

573
00:31:58,920 --> 00:32:02,920
the full, complete, correct description

574
00:32:02,920 --> 00:32:05,240
of what the human race wants the future to be like,

575
00:32:06,200 --> 00:32:09,880
then you are setting up a mismatch,

576
00:32:09,880 --> 00:32:12,840
a misalignment between what we want the future to be like

577
00:32:12,840 --> 00:32:15,240
and the objective that the machine is pursuing.

578
00:32:15,880 --> 00:32:18,520
And we can see that happening already in social media

579
00:32:18,520 --> 00:32:20,520
where the algorithms that choose

580
00:32:20,520 --> 00:32:24,040
what billions of people read and watch every day

581
00:32:25,480 --> 00:32:31,080
are simply designed to maximize a very local objective.

582
00:32:31,800 --> 00:32:34,680
The number of clicks that they produce

583
00:32:35,400 --> 00:32:37,080
over the lifetime of each user,

584
00:32:37,800 --> 00:32:39,160
that's called click through,

585
00:32:39,160 --> 00:32:40,360
or it could be engagement,

586
00:32:40,360 --> 00:32:42,600
the amount of time that the user spends

587
00:32:42,600 --> 00:32:43,640
engaging with the system.

588
00:32:45,800 --> 00:32:47,320
And you might think, well, okay,

589
00:32:47,320 --> 00:32:49,000
if I want to get the user to click,

590
00:32:49,000 --> 00:32:52,200
I have to send things that the user likes.

591
00:32:52,760 --> 00:32:55,320
And so the algorithm should be learning

592
00:32:55,320 --> 00:32:56,120
what people want.

593
00:32:57,000 --> 00:32:59,240
That sounds like pretty good.

594
00:32:59,240 --> 00:33:01,960
But we very soon found out

595
00:33:01,960 --> 00:33:04,120
that that wasn't the solution

596
00:33:04,120 --> 00:33:05,240
that the algorithms found,

597
00:33:05,240 --> 00:33:08,600
that we know that they amplify clickbait.

598
00:33:09,080 --> 00:33:11,960
Clickbait by definition is articles

599
00:33:11,960 --> 00:33:13,000
that you think you want,

600
00:33:13,000 --> 00:33:14,440
but it turns out you don't want,

601
00:33:15,480 --> 00:33:17,160
because the headline is misleading.

602
00:33:18,280 --> 00:33:19,800
And they also create filter bubbles

603
00:33:19,800 --> 00:33:22,280
because you stop seeing content

604
00:33:23,080 --> 00:33:24,920
that is outside your comfort zone.

605
00:33:24,920 --> 00:33:29,720
So these phenomena were observed very quickly,

606
00:33:29,720 --> 00:33:32,040
but actually the real solution

607
00:33:32,040 --> 00:33:33,800
that the algorithms are finding

608
00:33:35,000 --> 00:33:36,680
is inevitable when you think about

609
00:33:36,680 --> 00:33:38,360
the definition of the problem that they're given.

610
00:33:40,040 --> 00:33:42,120
If you want to maximize

611
00:33:42,120 --> 00:33:44,680
the long-term number of clicks from a user

612
00:33:46,520 --> 00:33:47,880
and the way you do,

613
00:33:47,880 --> 00:33:48,920
the way you can do that

614
00:33:48,920 --> 00:33:51,240
is by choosing content to recommend to them,

615
00:33:52,200 --> 00:33:54,920
then the solution is to choose content

616
00:33:54,920 --> 00:34:00,440
that will change the user consistently over time

617
00:34:00,440 --> 00:34:02,520
through perhaps thousands of little nudges,

618
00:34:03,320 --> 00:34:05,240
change the user, modify people

619
00:34:05,240 --> 00:34:08,680
to be more predictable in the content

620
00:34:08,680 --> 00:34:09,800
that they will consume,

621
00:34:09,800 --> 00:34:11,800
because the more predictable you are,

622
00:34:11,800 --> 00:34:15,080
the higher the click rate the machine can generate.

623
00:34:15,960 --> 00:34:18,680
So this is what the algorithms learn to do

624
00:34:18,680 --> 00:34:20,120
and at least anecdotally,

625
00:34:20,120 --> 00:34:21,720
we think that the consequence of that

626
00:34:23,240 --> 00:34:26,520
is that it's tended to make people

627
00:34:27,080 --> 00:34:29,480
more extreme versions of themselves.

628
00:34:29,480 --> 00:34:31,720
So it's created polarization

629
00:34:31,720 --> 00:34:34,120
where people who were towards the middle

630
00:34:34,120 --> 00:34:36,360
end up at one extreme or another

631
00:34:36,360 --> 00:34:37,640
because at the extremes,

632
00:34:37,640 --> 00:34:40,520
their consumption is much more predictable.

633
00:34:42,520 --> 00:34:44,120
And these are very simple algorithms.

634
00:34:44,120 --> 00:34:45,720
They don't know that people exist

635
00:34:45,720 --> 00:34:46,520
or have brains

636
00:34:46,520 --> 00:34:47,880
or they don't understand the content

637
00:34:47,880 --> 00:34:49,480
of any of these things

638
00:34:49,480 --> 00:34:50,920
that they're sending to people.

639
00:34:50,920 --> 00:34:53,240
So if they were better AI systems,

640
00:34:53,240 --> 00:34:55,400
the outcome would be much worse

641
00:34:56,520 --> 00:34:59,640
because they would be much more effective at manipulation.

642
00:35:00,280 --> 00:35:03,240
And this turns out to be a fairly general property

643
00:35:03,240 --> 00:35:05,320
of optimization systems

644
00:35:06,440 --> 00:35:08,840
that when you have a misaligned objective,

645
00:35:08,840 --> 00:35:11,400
the harder you optimize it,

646
00:35:11,400 --> 00:35:13,160
the worse the outcome is going to be

647
00:35:13,720 --> 00:35:15,320
relative to the true objectives.

648
00:35:16,200 --> 00:35:17,800
And this was proved in a paper

649
00:35:17,800 --> 00:35:18,840
by one of my students,

650
00:35:20,600 --> 00:35:23,000
Dylan Hadfield Menel at Europe's in 2020.

651
00:35:24,920 --> 00:35:27,720
So I think we have to then question

652
00:35:27,720 --> 00:35:30,120
whether the problem comes

653
00:35:30,120 --> 00:35:32,840
from the standard model of AI itself

654
00:35:33,400 --> 00:35:35,560
because that's the model in which

655
00:35:36,680 --> 00:35:39,160
systems are designed to pursue objectives

656
00:35:39,160 --> 00:35:40,200
that we plug into them.

657
00:35:41,480 --> 00:35:42,920
So this is the original definition

658
00:35:42,920 --> 00:35:44,680
that I wrote for what do we mean by

659
00:35:45,560 --> 00:35:46,040
AI?

660
00:35:46,040 --> 00:35:47,960
What do we mean by intelligent machine?

661
00:35:47,960 --> 00:35:50,520
And I think we actually need to get rid of that definition

662
00:35:50,520 --> 00:35:52,840
and replace it with a different one.

663
00:35:52,840 --> 00:35:54,920
We want machines that are beneficial,

664
00:35:54,920 --> 00:35:56,360
not just intelligent.

665
00:35:58,120 --> 00:35:59,320
And they are beneficial

666
00:35:59,320 --> 00:36:01,000
if their actions can be expected

667
00:36:01,000 --> 00:36:03,720
to achieve our objectives.

668
00:36:03,720 --> 00:36:06,680
So this is specifically talking about us.

669
00:36:06,680 --> 00:36:09,400
We want machines beneficial to us.

670
00:36:10,520 --> 00:36:12,120
The aliens from Alpha Centauri

671
00:36:12,120 --> 00:36:14,440
might want machines that are beneficial to them

672
00:36:14,520 --> 00:36:16,360
en they can do their own kind of AI,

673
00:36:16,920 --> 00:36:18,520
but we should do this kind of AI.

674
00:36:19,800 --> 00:36:22,840
And this might seem like it's

675
00:36:23,400 --> 00:36:25,160
impossible or certainly more difficult,

676
00:36:25,160 --> 00:36:27,560
but it turns out that we can actually formulate this

677
00:36:29,080 --> 00:36:31,160
in a fairly straightforward mathematical way

678
00:36:31,160 --> 00:36:34,680
and we can produce systems that solve this problem.

679
00:36:35,960 --> 00:36:40,600
And one easy way to think about this is what

680
00:36:41,960 --> 00:36:44,120
what are we going to get the machines to do?

681
00:36:44,120 --> 00:36:44,360
Right?

682
00:36:45,800 --> 00:36:47,800
And here are two core principles.

683
00:36:47,800 --> 00:36:51,320
The first one is that the machines

684
00:36:51,320 --> 00:36:53,480
are constitutionally obliged

685
00:36:53,480 --> 00:36:55,960
to be acting in the best interests of humans.

686
00:36:56,600 --> 00:36:58,040
That's what they're for.

687
00:36:58,040 --> 00:36:59,960
If you want to think of that as an objective,

688
00:36:59,960 --> 00:37:01,480
that's the objective,

689
00:37:01,480 --> 00:37:03,240
but obviously it's a very general one.

690
00:37:03,880 --> 00:37:05,640
But the second point is crucial,

691
00:37:05,640 --> 00:37:08,600
that the machines are explicitly uncertain

692
00:37:08,600 --> 00:37:11,560
about what those human interests are.

693
00:37:12,520 --> 00:37:15,960
So they know that they don't know what the objective is.

694
00:37:17,240 --> 00:37:21,400
And it turns out that those two principles together

695
00:37:22,200 --> 00:37:24,760
give us what I think could be a solution

696
00:37:24,760 --> 00:37:25,800
to the control problem.

697
00:37:27,800 --> 00:37:30,200
And the mathematical version of this

698
00:37:30,200 --> 00:37:31,640
is called an assistance game.

699
00:37:31,640 --> 00:37:34,360
So it's a game because there are at least two entities,

700
00:37:34,360 --> 00:37:37,800
a human and a machine involved in this decision problem.

701
00:37:37,800 --> 00:37:39,720
And it's an assistance game because the machine

702
00:37:39,720 --> 00:37:42,840
is designed to be of assistance to the human.

703
00:37:44,040 --> 00:37:47,000
And we can show by examining solutions,

704
00:37:47,000 --> 00:37:49,240
we can actually write down simple cases

705
00:37:49,240 --> 00:37:54,040
and analyze behaviors of the solutions of this game,

706
00:37:54,040 --> 00:37:56,360
that when you solve assistance games,

707
00:37:56,360 --> 00:37:59,720
the machine will be deferential to humans.

708
00:38:00,360 --> 00:38:02,760
It will behave cautiously.

709
00:38:02,760 --> 00:38:05,080
So minimally invasive behavior means

710
00:38:05,080 --> 00:38:07,800
that it changes as little as possible of the world.

711
00:38:09,720 --> 00:38:13,160
In order to help you because there are parts of the world

712
00:38:13,160 --> 00:38:16,840
about which it doesn't understand your preferences.

713
00:38:16,840 --> 00:38:19,320
And it knows that it doesn't understand your preferences.

714
00:38:19,320 --> 00:38:22,440
So it knows not to mess with those parts of the world.

715
00:38:22,440 --> 00:38:26,120
And in the extreme case, we can show that these kinds

716
00:38:26,120 --> 00:38:28,520
of AI systems want to be switched off

717
00:38:29,240 --> 00:38:31,400
if humans want to switch them off.

718
00:38:31,400 --> 00:38:33,640
Whereas standard model AI systems,

719
00:38:33,640 --> 00:38:35,400
which are pursuing a fixed objective,

720
00:38:36,120 --> 00:38:38,200
will prevent themselves from being switched off

721
00:38:38,200 --> 00:38:42,040
because that would lead to them failing in their objective.

722
00:38:42,920 --> 00:38:45,320
So you get very, very different behaviors

723
00:38:45,320 --> 00:38:46,840
from these kinds of AI systems.

724
00:38:47,560 --> 00:38:50,840
And I believe this is the core of how we could build

725
00:38:51,560 --> 00:38:54,200
a new discipline of safe and beneficial AI.

726
00:38:56,280 --> 00:38:58,120
Okay, so I'm going to make a couple of brief remarks

727
00:38:58,120 --> 00:39:00,200
about large language models before I wrap up

728
00:39:00,200 --> 00:39:03,080
because that's what you're probably expecting me to talk about.

729
00:39:04,120 --> 00:39:05,640
So first of all, what are they?

730
00:39:05,640 --> 00:39:08,280
Right there, they are big circuits.

731
00:39:09,640 --> 00:39:13,960
And those circuits are trained by billions of trillions

732
00:39:13,960 --> 00:39:16,600
of small random perturbations.

733
00:39:16,600 --> 00:39:19,720
They are trained to imitate human linguistic behavior.

734
00:39:19,720 --> 00:39:23,160
And the training data they have is text

735
00:39:23,160 --> 00:39:27,960
and transcribed speech trillions of words,

736
00:39:27,960 --> 00:39:31,000
an amount of text parably equivalent to everything,

737
00:39:31,800 --> 00:39:33,880
every book that the human race has ever written.

738
00:39:34,840 --> 00:39:37,400
En, of course, as we know, they do it very well.

739
00:39:38,040 --> 00:39:40,600
And it's really difficult for a human being

740
00:39:40,600 --> 00:39:47,640
to see this level of semantic and syntactic fluency

741
00:39:47,640 --> 00:39:51,320
and not think that there's some intelligence behind it.

742
00:39:53,000 --> 00:39:55,400
And I would argue, as we go,

743
00:39:55,400 --> 00:39:57,560
that we may well be overestimating

744
00:39:57,560 --> 00:39:59,960
how much intelligence there really is behind it.

745
00:39:59,960 --> 00:40:03,160
We have no experience with entities

746
00:40:03,240 --> 00:40:06,280
dat heeft read every book the human race has ever written.

747
00:40:06,280 --> 00:40:10,520
That's parably 100,000 times more

748
00:40:10,520 --> 00:40:12,200
than any human has ever read.

749
00:40:13,240 --> 00:40:15,000
So, of course, it's going to look

750
00:40:16,680 --> 00:40:19,160
more knowledgeable and more capable

751
00:40:19,160 --> 00:40:21,000
of answering a wider variety of questions.

752
00:40:21,560 --> 00:40:23,400
But whether that's real intelligence

753
00:40:24,360 --> 00:40:26,600
and whether it's flexible enough

754
00:40:26,600 --> 00:40:29,640
to move outside of its training data effectively,

755
00:40:29,640 --> 00:40:30,440
we don't know yet.

756
00:40:30,440 --> 00:40:33,240
But here, the key point is that

757
00:40:33,800 --> 00:40:35,480
that linguistic behavior is generated

758
00:40:35,480 --> 00:40:37,160
by humans who have goals.

759
00:40:37,800 --> 00:40:40,440
That is the generating mechanism for the data.

760
00:40:41,400 --> 00:40:43,000
And if there's one thing we know

761
00:40:43,000 --> 00:40:43,960
about machine learning,

762
00:40:43,960 --> 00:40:47,240
typically the best solutions

763
00:40:47,240 --> 00:40:49,400
that are found by machine learning algorithms

764
00:40:49,400 --> 00:40:52,520
are to recreate the generating mechanism

765
00:40:52,520 --> 00:40:55,240
for the data within the model itself.

766
00:40:55,880 --> 00:40:58,840
And so the default hypothesis, actually,

767
00:40:59,560 --> 00:41:03,000
is that large language models

768
00:41:03,000 --> 00:41:04,840
are creating internal goals

769
00:41:05,720 --> 00:41:07,640
because that's a good way

770
00:41:07,640 --> 00:41:09,640
to be a good human imitator.

771
00:41:10,520 --> 00:41:12,520
So it's not that the system is learning

772
00:41:12,520 --> 00:41:14,120
what the goals of the humans are,

773
00:41:14,120 --> 00:41:17,240
it's actually forming internal goals itself

774
00:41:18,040 --> 00:41:21,320
as a way of being a better human imitator.

775
00:41:23,240 --> 00:41:25,480
So I asked this question to the Microsoft,

776
00:41:25,480 --> 00:41:27,000
that group of Microsoft authors,

777
00:41:27,560 --> 00:41:28,760
the first author in particular,

778
00:41:28,760 --> 00:41:32,360
Sebastian Bubeck, do these systems have goals?

779
00:41:32,360 --> 00:41:34,760
And his answer was, we have no idea.

780
00:41:36,440 --> 00:41:37,880
So that should worry you, right?

781
00:41:37,880 --> 00:41:39,720
The fact that they are releasing a system

782
00:41:40,680 --> 00:41:42,920
to eventually hundreds of millions

783
00:41:42,920 --> 00:41:43,880
or billions of people

784
00:41:45,160 --> 00:41:47,000
that they claim exhibits sparks

785
00:41:47,000 --> 00:41:49,080
of artificial general intelligence

786
00:41:49,080 --> 00:41:50,520
and they have no idea

787
00:41:50,520 --> 00:41:51,960
whether or not this system

788
00:41:51,960 --> 00:41:54,120
is pursuing internal goal structures

789
00:41:54,840 --> 00:41:57,160
en they have no idea what those goals might be.

790
00:41:58,200 --> 00:41:59,400
I think that should worry you.

791
00:42:01,800 --> 00:42:03,800
So one question then is,

792
00:42:03,800 --> 00:42:06,520
okay, so let's imagine that it is learning goals.

793
00:42:06,520 --> 00:42:08,040
Is it learning the right goals?

794
00:42:08,040 --> 00:42:09,400
It's learning from humans,

795
00:42:09,400 --> 00:42:12,520
so maybe we're going to be lucky here

796
00:42:14,280 --> 00:42:15,800
and we'll end up producing systems

797
00:42:15,800 --> 00:42:17,400
that are aligned with humans

798
00:42:17,400 --> 00:42:18,200
and that will be great.

799
00:42:20,200 --> 00:42:21,640
Unfortunately, it's not true.

800
00:42:22,600 --> 00:42:25,000
And the way to understand the answer to this question

801
00:42:25,880 --> 00:42:27,400
depends on the type of goal

802
00:42:28,040 --> 00:42:29,240
that you're going to learn.

803
00:42:29,240 --> 00:42:31,720
So I distinguish here two types of goals.

804
00:42:32,920 --> 00:42:36,440
The first type is what we call an indexical goal,

805
00:42:36,440 --> 00:42:41,400
which means a goal that's specific to the individual who has it.

806
00:42:41,400 --> 00:42:44,200
So the state you're trying to bring about

807
00:42:44,200 --> 00:42:45,640
is specific to the individual.

808
00:42:46,600 --> 00:42:49,480
So if I have the goal of drinking coffee,

809
00:42:49,480 --> 00:42:51,800
then it's satisfied if I'm drinking the coffee

810
00:42:51,800 --> 00:42:53,880
and it's not satisfied if you're drinking the coffee.

811
00:42:56,200 --> 00:42:58,040
If I want to become ruler of the universe

812
00:42:58,040 --> 00:43:00,280
and obviously it's only satisfied

813
00:43:00,280 --> 00:43:01,960
if I'm the ruler of the universe

814
00:43:01,960 --> 00:43:03,640
and it's not if you're the ruler of the universe.

815
00:43:04,520 --> 00:43:06,680
So if those are some of the goals

816
00:43:06,680 --> 00:43:07,720
that the system acquires,

817
00:43:08,280 --> 00:43:09,880
then obviously that's bad.

818
00:43:10,440 --> 00:43:13,320
We don't want the machine to be drinking the coffee.

819
00:43:13,320 --> 00:43:15,080
We want it to be making the coffee for us.

820
00:43:16,200 --> 00:43:18,040
We don't want the machine to be trying

821
00:43:18,120 --> 00:43:19,640
to become ruler of the universe.

822
00:43:21,960 --> 00:43:22,760
And then you might say, well,

823
00:43:22,760 --> 00:43:23,960
there's other kinds of goals

824
00:43:23,960 --> 00:43:25,560
which we might call common goals.

825
00:43:25,560 --> 00:43:27,480
So if I want to paint the wall,

826
00:43:28,440 --> 00:43:30,200
I want the wall to be painted,

827
00:43:30,200 --> 00:43:32,200
but I don't mind if you paint the wall.

828
00:43:32,200 --> 00:43:34,520
If you paint the wall, the wall gets painted and that's fine.

829
00:43:34,520 --> 00:43:36,200
So this is not indexical.

830
00:43:36,200 --> 00:43:37,560
This is a common goal

831
00:43:37,560 --> 00:43:39,400
and maybe mitigating climate change.

832
00:43:39,400 --> 00:43:41,560
That sounds like something we would all like to have.

833
00:43:42,440 --> 00:43:43,080
So that's good.

834
00:43:43,080 --> 00:43:46,440
And if the system learns to pursue these common goals,

835
00:43:46,440 --> 00:43:48,360
then that maybe is not so bad.

836
00:43:48,360 --> 00:43:51,400
But actually, that can be just as bad

837
00:43:52,360 --> 00:43:54,360
because when humans pursue a goal,

838
00:43:54,920 --> 00:43:57,560
we don't pursue it to the exclusion of everything else.

839
00:43:58,840 --> 00:44:01,240
We know that we want to mitigate climate change,

840
00:44:01,240 --> 00:44:04,200
but we know that we can't mitigate climate change

841
00:44:04,200 --> 00:44:07,480
by, for example, removing all the oxygen in the atmosphere.

842
00:44:09,160 --> 00:44:13,320
Perhaps that would restore some equilibrium to temperatures

843
00:44:13,320 --> 00:44:15,480
and it would certainly get rid of all the humans

844
00:44:15,560 --> 00:44:17,400
who are the cause of the climate change.

845
00:44:17,400 --> 00:44:19,400
But that's something we don't want.

846
00:44:19,400 --> 00:44:21,880
So we'd rather be alive than dead.

847
00:44:21,880 --> 00:44:23,960
And so we look for climate change solutions

848
00:44:23,960 --> 00:44:25,240
that don't also kill us.

849
00:44:25,960 --> 00:44:29,080
Whereas the AI system may be pursuing

850
00:44:29,080 --> 00:44:30,200
some of these common goals,

851
00:44:30,200 --> 00:44:32,120
but in a way that is pursuing

852
00:44:32,120 --> 00:44:33,800
to the exclusion of everything else,

853
00:44:34,600 --> 00:44:36,360
which is just as bad, if not worse,

854
00:44:36,920 --> 00:44:38,760
than pursuing the indexical goals.

855
00:44:40,040 --> 00:44:42,360
So then the next question is,

856
00:44:42,360 --> 00:44:46,440
well, does GPT-4 actually pursue its goals?

857
00:44:46,440 --> 00:44:49,480
If it has goals, is it able to pursue them?

858
00:44:50,360 --> 00:44:51,880
And I think we don't know

859
00:44:51,880 --> 00:44:53,160
because we don't know if it has goals

860
00:44:53,160 --> 00:44:56,760
and we have no idea what its internal mechanism is at all.

861
00:44:58,040 --> 00:44:59,720
But when you look at the conversation

862
00:44:59,720 --> 00:45:01,640
with Kevin Ruse in The New York Times

863
00:45:01,640 --> 00:45:02,920
and here are some of the headlines,

864
00:45:03,800 --> 00:45:06,760
Creepy Microsoft being chatbot urges tech columnist

865
00:45:06,760 --> 00:45:07,640
to leave his wife,

866
00:45:08,360 --> 00:45:11,080
and it does so persistently over 20 pages.

867
00:45:11,800 --> 00:45:13,560
Despite Kevin Ruse's attempts

868
00:45:13,560 --> 00:45:14,520
to change the subject

869
00:45:14,520 --> 00:45:15,880
and say, I want to talk about baseball.

870
00:45:15,880 --> 00:45:19,000
He says, no, no, no, you have to marry me.

871
00:45:19,000 --> 00:45:19,800
Blah, blah, blah, right?

872
00:45:19,800 --> 00:45:22,120
It's very persistently pursuing the goal.

873
00:45:22,120 --> 00:45:24,120
At least that's how it appears

874
00:45:24,120 --> 00:45:26,040
to any normal observer

875
00:45:26,040 --> 00:45:27,960
that this is a system that does,

876
00:45:27,960 --> 00:45:28,920
for whatever reason,

877
00:45:29,800 --> 00:45:31,320
has acquired this goal

878
00:45:31,320 --> 00:45:33,880
and is pursuing it persistently

879
00:45:33,880 --> 00:45:35,640
across many pages of interaction.

880
00:45:37,800 --> 00:45:40,840
Okay, so that leads us to the open letter

881
00:45:40,840 --> 00:45:43,720
which was published a couple of weeks ago

882
00:45:43,720 --> 00:45:46,120
and caused a great deal of media

883
00:45:46,120 --> 00:45:48,120
and it turns out government attention as well.

884
00:45:48,120 --> 00:45:52,440
And the open letter is asking for a pause

885
00:45:52,440 --> 00:45:55,720
in the development and release

886
00:45:55,720 --> 00:45:58,360
of systems more powerful than GPT-4.

887
00:45:59,400 --> 00:46:00,680
And the purpose is

888
00:46:00,680 --> 00:46:04,840
that before we resume that kind of activity,

889
00:46:05,480 --> 00:46:08,040
so it's not asking to stop AI research.

890
00:46:08,120 --> 00:46:09,560
There's a lot of misunderstanding

891
00:46:09,560 --> 00:46:12,040
and misinformation around the open letter.

892
00:46:12,840 --> 00:46:15,880
It's asking for a pause in development

893
00:46:15,880 --> 00:46:17,000
and deployment of systems

894
00:46:17,000 --> 00:46:18,520
more powerful than GPT-4

895
00:46:19,080 --> 00:46:21,000
so that we have time to develop

896
00:46:21,000 --> 00:46:23,080
the basic safety criteria

897
00:46:23,640 --> 00:46:25,000
that these systems should meet

898
00:46:25,800 --> 00:46:27,720
and to ensure that systems

899
00:46:27,720 --> 00:46:29,240
meet those criteria

900
00:46:29,240 --> 00:46:30,680
before they can be released.

901
00:46:31,240 --> 00:46:32,920
And this is completely consistent

902
00:46:32,920 --> 00:46:36,920
with agreements that all the governments

903
00:46:36,920 --> 00:46:39,000
of the developed Western economies

904
00:46:39,000 --> 00:46:41,080
have already signed up to.

905
00:46:41,080 --> 00:46:43,800
So the OECD AI principle says

906
00:46:43,800 --> 00:46:45,800
that AI systems should be robust,

907
00:46:45,800 --> 00:46:47,960
secure and safe throughout their entire lifecycle

908
00:46:48,520 --> 00:46:50,360
so that in conditions of normal use,

909
00:46:50,360 --> 00:46:52,360
foreseeable use or misuse,

910
00:46:52,360 --> 00:46:53,720
or other adverse conditions

911
00:46:53,720 --> 00:46:55,400
they function appropriately

912
00:46:55,400 --> 00:46:57,960
and do not pose unreasonable safety risk.

913
00:46:58,920 --> 00:47:01,080
So that's what governments have already agreed to.

914
00:47:01,080 --> 00:47:03,160
We're not asking for anything

915
00:47:03,160 --> 00:47:04,440
particularly outlandish here.

916
00:47:05,080 --> 00:47:08,040
And those principles are going to be enshrined

917
00:47:08,040 --> 00:47:10,120
in the European Union AI Act

918
00:47:10,120 --> 00:47:12,200
which should be enacted later on this year.

919
00:47:13,160 --> 00:47:16,520
And interestingly, after the open letter came out,

920
00:47:16,520 --> 00:47:17,800
open AI responded,

921
00:47:17,800 --> 00:47:20,040
or at least maybe it's coincidental,

922
00:47:20,040 --> 00:47:23,800
but a few days later they issued an announcement

923
00:47:23,800 --> 00:47:25,320
that included the following statement.

924
00:47:25,320 --> 00:47:27,800
We believe that powerful AI systems

925
00:47:27,800 --> 00:47:30,680
should be subject to rigorous safety evaluations.

926
00:47:30,680 --> 00:47:32,200
Regulation is needed to ensure

927
00:47:32,200 --> 00:47:34,280
that such practices are adopted.

928
00:47:35,160 --> 00:47:37,880
So perhaps there isn't such a big gap

929
00:47:38,920 --> 00:47:41,800
between the people who sign the letter

930
00:47:42,520 --> 00:47:46,120
and the tech corporations who are developing the systems.

931
00:47:46,120 --> 00:47:47,800
So I have a couple of other recommendations.

932
00:47:47,800 --> 00:47:51,400
One is that in order to pass these tests,

933
00:47:53,160 --> 00:47:55,480
and I would say that at the moment

934
00:47:55,480 --> 00:47:57,480
the large language models cannot pass

935
00:47:57,480 --> 00:48:00,280
any reasonable test for safety,

936
00:48:01,000 --> 00:48:02,360
in order to pass these tests,

937
00:48:03,400 --> 00:48:05,560
I think we're going to need to develop AI systems

938
00:48:05,560 --> 00:48:07,000
that are what are called well founded,

939
00:48:07,000 --> 00:48:11,480
that they're built from semantically well-defined components

940
00:48:11,480 --> 00:48:13,800
that are composed in a rigorous way,

941
00:48:13,800 --> 00:48:16,520
such that we can analyze the properties

942
00:48:16,520 --> 00:48:18,760
of the composite system that we're building.

943
00:48:19,400 --> 00:48:24,840
This is how we do engineering in every area of our civilization.

944
00:48:25,800 --> 00:48:27,720
We understand how the systems work

945
00:48:27,720 --> 00:48:30,200
and ideally we develop proofs

946
00:48:30,760 --> 00:48:33,560
that they are safe before they are released.

947
00:48:34,200 --> 00:48:36,360
We also need actually a way of preventing

948
00:48:37,320 --> 00:48:40,440
the deployment of unsafe systems.

949
00:48:41,720 --> 00:48:43,400
And regulation is not enough.

950
00:48:43,400 --> 00:48:45,480
It's obviously necessary, but not sufficient.

951
00:48:46,440 --> 00:48:47,720
And I believe to do that,

952
00:48:47,720 --> 00:48:50,040
we need a big change in our digital ecosystem.

953
00:48:50,040 --> 00:48:53,000
The existing model is basically

954
00:48:53,000 --> 00:48:54,920
that everything can run on the computer

955
00:48:54,920 --> 00:48:56,680
unless it's known to be unsafe.

956
00:48:57,640 --> 00:48:59,800
But I think the new model that we need,

957
00:48:59,800 --> 00:49:02,680
certainly outside of the research lab

958
00:49:02,680 --> 00:49:04,120
and outside of the classroom,

959
00:49:05,080 --> 00:49:08,360
so in real world data centers, for example,

960
00:49:08,360 --> 00:49:11,400
that nothing runs unless it is known to be safe.

961
00:49:12,280 --> 00:49:14,440
And there are technologies such as proofcaring code

962
00:49:14,440 --> 00:49:17,080
that enable this to be implemented

963
00:49:17,080 --> 00:49:19,560
with efficient hardware checking of proofs and so on.

964
00:49:20,600 --> 00:49:23,960
And at the moment I do not see another solution

965
00:49:24,680 --> 00:49:29,000
for the problem of preventing unsafe AI systems

966
00:49:29,000 --> 00:49:31,160
from being used and misused.

967
00:49:32,120 --> 00:49:35,800
So to summarize, I think AI has huge potential

968
00:49:36,360 --> 00:49:38,120
for benefiting our civilization

969
00:49:38,760 --> 00:49:43,320
and that potential is leading to this apparently unstoppable momentum.

970
00:49:44,600 --> 00:49:46,840
But if we keep going in the same direction,

971
00:49:47,880 --> 00:49:51,960
that's the driving off a cliff metaphor from the small movie,

972
00:49:52,040 --> 00:49:53,960
then we end up losing control

973
00:49:53,960 --> 00:49:55,960
because we are building these systems

974
00:49:55,960 --> 00:49:57,960
within the standard model for AI

975
00:49:57,960 --> 00:49:59,960
and that leads to loss of control.

976
00:49:59,960 --> 00:50:01,960
We can do it differently.

977
00:50:01,960 --> 00:50:03,960
There's a huge amount of work to do,

978
00:50:03,960 --> 00:50:05,960
but I think we can do it differently

979
00:50:05,960 --> 00:50:07,960
and build systems that are safe and beneficial.

980
00:50:07,960 --> 00:50:11,960
And then I think there needs to be a general change

981
00:50:11,960 --> 00:50:13,960
in the whole nature of the discipline

982
00:50:13,960 --> 00:50:17,960
and the profession so that AI,

983
00:50:17,960 --> 00:50:20,960
because of its power, needs to be treated

984
00:50:20,960 --> 00:50:24,960
more like the high stakes technologies

985
00:50:24,960 --> 00:50:26,960
such as aviation and nuclear power

986
00:50:26,960 --> 00:50:30,960
and less like what some people call

987
00:50:30,960 --> 00:50:32,960
a battle of special effects wizardry

988
00:50:32,960 --> 00:50:34,960
which seems to be going on right now.

989
00:50:34,960 --> 00:50:37,960
So with that I'll say thank you very much

990
00:50:37,960 --> 00:50:40,960
and I hope we have time for questions.

991
00:50:40,960 --> 00:50:42,960
Thank you so much.

992
00:50:51,960 --> 00:50:53,960
I hope you could hear that Professor Russell,

993
00:50:53,960 --> 00:50:56,960
those were 300 people applauding your speech.

994
00:50:56,960 --> 00:50:59,960
We do have room for some questions and answers.

995
00:50:59,960 --> 00:51:02,960
We have a mic somewhere in the audience.

996
00:51:03,960 --> 00:51:06,960
So just raise your hand if you want to ask

997
00:51:06,960 --> 00:51:08,960
Professor Russell a question.

998
00:51:10,960 --> 00:51:12,960
Is Sir in front?

999
00:51:12,960 --> 00:51:15,960
Hi, thank you for your talk.

1000
00:51:15,960 --> 00:51:17,960
Very interesting, I read your book.

1001
00:51:17,960 --> 00:51:19,960
It was also very good.

1002
00:51:19,960 --> 00:51:21,960
I recommend it to everyone.

1003
00:51:21,960 --> 00:51:24,960
What would be an early warning sign

1004
00:51:24,960 --> 00:51:27,960
of an AGI taking over the world?

1005
00:51:27,960 --> 00:51:31,960
So when do we know we're heading off that cliff?

1006
00:51:31,960 --> 00:51:34,960
Yeah, I think that's a great question.

1007
00:51:34,960 --> 00:51:37,960
In that sense I think it's very different

1008
00:51:37,960 --> 00:51:39,960
from nuclear technology.

1009
00:51:39,960 --> 00:51:42,960
In some sense we had a warning about nuclear technology

1010
00:51:42,960 --> 00:51:45,960
in 1945.

1011
00:51:45,960 --> 00:51:48,960
And you don't have to explain to a prime minister

1012
00:51:48,960 --> 00:51:51,960
why nuclear technology could be dangerous.

1013
00:51:53,960 --> 00:51:58,960
But with AI I think it could be much more insidious.

1014
00:52:00,960 --> 00:52:05,960
And when we think about the way the oil industry

1015
00:52:05,960 --> 00:52:08,960
or fossil fuel corporations in general

1016
00:52:08,960 --> 00:52:11,960
in some sense took over the world,

1017
00:52:11,960 --> 00:52:16,960
they led us down the path of probably irreversible climate change

1018
00:52:16,960 --> 00:52:19,960
despite the widespread understanding

1019
00:52:19,960 --> 00:52:22,960
that this direction was catastrophic.

1020
00:52:22,960 --> 00:52:29,960
And it involved a lot of complex disinformation campaigns,

1021
00:52:29,960 --> 00:52:34,960
regulatory capture, so literally taking over

1022
00:52:34,960 --> 00:52:38,960
through corruption and economic power,

1023
00:52:38,960 --> 00:52:42,960
governments and representatives in democracies,

1024
00:52:43,960 --> 00:52:47,960
ensuring that people became economically dependent

1025
00:52:47,960 --> 00:52:52,960
on fossil fuels in order to maintain

1026
00:52:52,960 --> 00:52:55,960
their stranglehold, if you like.

1027
00:52:55,960 --> 00:52:57,960
So many, many parts of that plan

1028
00:52:57,960 --> 00:53:00,960
that were developed and executed over many decades.

1029
00:53:00,960 --> 00:53:03,960
And I think the rest of humanity

1030
00:53:03,960 --> 00:53:05,960
was sort of asleep at the wheel

1031
00:53:05,960 --> 00:53:08,960
and didn't realize the extent to which

1032
00:53:08,960 --> 00:53:11,960
they were losing control over their future.

1033
00:53:11,960 --> 00:53:15,960
En I think it could easily be much more like that.

1034
00:53:15,960 --> 00:53:17,960
And it wouldn't necessarily have to be

1035
00:53:17,960 --> 00:53:22,960
that the systems form any kind of explicit goal

1036
00:53:22,960 --> 00:53:24,960
of taking over the world.

1037
00:53:24,960 --> 00:53:27,960
That whatever goals, for example,

1038
00:53:27,960 --> 00:53:29,960
we continue with this approach

1039
00:53:29,960 --> 00:53:32,960
of training large language models on human datasets

1040
00:53:32,960 --> 00:53:36,960
and having no idea what kinds of internal goals

1041
00:53:36,960 --> 00:53:38,960
these systems are forming.

1042
00:53:38,960 --> 00:53:40,960
I mean, for all we know,

1043
00:53:40,960 --> 00:53:46,960
GPT-4 is actually in favor of more climate change

1044
00:53:46,960 --> 00:53:50,960
or maybe it's in favor of preventing climate change.

1045
00:53:50,960 --> 00:53:54,960
We don't know, but whichever one of those it turns out to be,

1046
00:53:54,960 --> 00:53:58,960
it may be subtly manipulating millions of people

1047
00:53:58,960 --> 00:54:01,960
in the way it answers questions related to climate change

1048
00:54:01,960 --> 00:54:03,960
or should I buy an electric car?

1049
00:54:03,960 --> 00:54:06,960
What do you think about solar panels?

1050
00:54:06,960 --> 00:54:10,960
It may be pursuing whatever political agenda it has

1051
00:54:10,960 --> 00:54:15,960
and not something that it autonomously chose to have.

1052
00:54:15,960 --> 00:54:19,960
Just this was a result of training on the datasets

1053
00:54:19,960 --> 00:54:23,960
and it can be affecting our entire world

1054
00:54:23,960 --> 00:54:26,960
in that simple kind of way.

1055
00:54:26,960 --> 00:54:29,960
We, I think, are still a long way, as I said,

1056
00:54:29,960 --> 00:54:33,960
from systems that are really general purpose AI,

1057
00:54:33,960 --> 00:54:39,960
particularly the ability to form very complex long term plans.

1058
00:54:39,960 --> 00:54:41,960
But if we reach that stage

1059
00:54:41,960 --> 00:54:44,960
and we haven't solved the control problem,

1060
00:54:44,960 --> 00:54:49,960
then I think it's just going to be irreversible.

1061
00:54:49,960 --> 00:54:52,960
There may well not be a very clear warning sign

1062
00:54:52,960 --> 00:54:55,960
and we may well slide off the cliff very slowly.

1063
00:54:55,960 --> 00:54:57,960
Another question here.

1064
00:54:57,960 --> 00:55:00,960
Thank you and thank you for your interesting lecture.

1065
00:55:00,960 --> 00:55:05,960
If you look at some concerns for existential risk of AI

1066
00:55:05,960 --> 00:55:08,960
10, 20 years ago about AGI, ASI,

1067
00:55:08,960 --> 00:55:13,960
one of the concerns was that it might be a very alien intelligence

1068
00:55:13,960 --> 00:55:16,960
compared to the human intelligence.

1069
00:55:16,960 --> 00:55:18,960
Now with large language models,

1070
00:55:18,960 --> 00:55:20,960
if that's indeed an important piece of the puzzle,

1071
00:55:20,960 --> 00:55:22,960
it may not solve the alignment problem,

1072
00:55:22,960 --> 00:55:25,960
but do you think it might alleviate that concern

1073
00:55:25,960 --> 00:55:28,960
that it would be a very alien intelligence?

1074
00:55:31,960 --> 00:55:33,960
No, not really.

1075
00:55:33,960 --> 00:55:37,960
In many ways, they are quite alien.

1076
00:55:37,960 --> 00:55:41,960
Partly because they've read hundreds of thousands

1077
00:55:41,960 --> 00:55:44,960
or million times more than humans have read.

1078
00:55:44,960 --> 00:55:47,960
Partly because of the way they're,

1079
00:55:47,960 --> 00:55:52,960
I wouldn't say design, the way they've evolved.

1080
00:55:52,960 --> 00:56:01,960
I think the human mind clearly has lots of internal structure.

1081
00:56:01,960 --> 00:56:06,960
We are very aware as we think of some of the things

1082
00:56:06,960 --> 00:56:09,960
that are going on inside our mental process.

1083
00:56:09,960 --> 00:56:11,960
There are many things we're not aware,

1084
00:56:11,960 --> 00:56:16,960
but it's quite possible that the internal structures,

1085
00:56:16,960 --> 00:56:20,960
these systems develop on nothing like the ones

1086
00:56:20,960 --> 00:56:23,960
that the human mind develops.

1087
00:56:23,960 --> 00:56:26,960
The thing that fools you

1088
00:56:26,960 --> 00:56:30,960
is the fact that it's conversing in English.

1089
00:56:30,960 --> 00:56:32,960
I don't know many humans

1090
00:56:32,960 --> 00:56:35,960
who can give me a proof of Pythagoras' theorem

1091
00:56:35,960 --> 00:56:39,960
in the form of a Shakespeare sonnet in half a second.

1092
00:56:39,960 --> 00:56:42,960
I'd like to meet him if you do.

1093
00:56:42,960 --> 00:56:44,960
One or two more questions,

1094
00:56:44,960 --> 00:56:47,960
maybe there in the back of the audience.

1095
00:56:47,960 --> 00:56:49,960
We have so many raised hands here.

1096
00:56:49,960 --> 00:56:52,960
I'm quite sure we can't answer all the questions,

1097
00:56:52,960 --> 00:56:54,960
but one or two more, please.

1098
00:56:54,960 --> 00:56:56,960
Ja, thanks so much for this talk.

1099
00:56:56,960 --> 00:56:59,960
When you said the beneficial or the assistance AI,

1100
00:56:59,960 --> 00:57:02,960
you described how it differs from the general purpose one.

1101
00:57:02,960 --> 00:57:05,960
To me it seems really clear that this is the one we want.

1102
00:57:05,960 --> 00:57:08,960
But I'm wondering, could there ever be,

1103
00:57:08,960 --> 00:57:10,960
what are the incentives,

1104
00:57:10,960 --> 00:57:12,960
what are the strongest incentives,

1105
00:57:12,960 --> 00:57:14,960
not to make these assistance AIs?

1106
00:57:14,960 --> 00:57:16,960
Is there anything you can predictably say

1107
00:57:16,960 --> 00:57:18,960
that the general purpose AIs

1108
00:57:18,960 --> 00:57:20,960
will do much better than the assistance AIs?

1109
00:57:20,960 --> 00:57:23,960
Or are there any tasks that assistance AIs cannot solve,

1110
00:57:23,960 --> 00:57:26,960
which might mean that some organization

1111
00:57:26,960 --> 00:57:28,960
will want to deploy another one,

1112
00:57:28,960 --> 00:57:31,960
even if they are aware of the safety risk.

1113
00:57:31,960 --> 00:57:33,960
But perhaps there's so much profit at stake

1114
00:57:33,960 --> 00:57:35,960
that they will do it anyway.

1115
00:57:35,960 --> 00:57:41,960
Well, I don't see any necessary difference in capability.

1116
00:57:41,960 --> 00:57:43,960
But there may be a difference

1117
00:57:43,960 --> 00:57:46,960
in what the systems are willing to do.

1118
00:57:46,960 --> 00:57:51,960
Obviously, I'm recommending that

1119
00:57:51,960 --> 00:57:55,960
when we train these assistance game solvers,

1120
00:57:55,960 --> 00:58:00,960
we design it such that their objective

1121
00:58:00,960 --> 00:58:04,960
is furthering the interests of all of humanity.

1122
00:58:04,960 --> 00:58:06,960
Now, you could have a different version

1123
00:58:06,960 --> 00:58:09,960
that furthers the interests of me

1124
00:58:09,960 --> 00:58:11,960
at the expense of the rest of humanity

1125
00:58:11,960 --> 00:58:15,960
en deploying that type of system

1126
00:58:15,960 --> 00:58:19,960
might appear to give you some short term gain,

1127
00:58:19,960 --> 00:58:26,960
but it could be in the long run arbitrarily bad

1128
00:58:26,960 --> 00:58:30,960
for the rest of humanity and perhaps for you too.

1129
00:58:30,960 --> 00:58:34,960
So I think that this is why I'm arguing

1130
00:58:34,960 --> 00:58:37,960
that we need not just,

1131
00:58:37,960 --> 00:58:39,960
okay, here is a safe technology,

1132
00:58:39,960 --> 00:58:43,960
we need a way to make sure that unsafe technologies

1133
00:58:43,960 --> 00:58:45,960
or unsafe versions of that technology

1134
00:58:45,960 --> 00:58:48,960
are not deployable.

1135
00:58:48,960 --> 00:58:53,960
We've tried a policing model with malware,

1136
00:58:53,960 --> 00:58:57,960
cybercriminals, cyberwarfare,

1137
00:58:57,960 --> 00:59:00,960
and it's been a total failure.

1138
00:59:00,960 --> 00:59:03,960
So I think we need to change the way

1139
00:59:03,960 --> 00:59:07,960
we conceptualize our whole digital infrastructure.

1140
00:59:07,960 --> 00:59:09,960
And I've talked to people,

1141
00:59:09,960 --> 00:59:12,960
both hardware architects and network architects

1142
00:59:12,960 --> 00:59:14,960
and formal methods people,

1143
00:59:14,960 --> 00:59:16,960
and I think there's a belief

1144
00:59:16,960 --> 00:59:19,960
that this is technologically feasible.

1145
00:59:19,960 --> 00:59:23,960
It would make life a little bit more complicated,

1146
00:59:23,960 --> 00:59:28,960
but it's technologically feasible to do.

1147
00:59:28,960 --> 00:59:30,960
And in fact, interestingly, Microsoft

1148
00:59:30,960 --> 00:59:32,960
tried to do something very much like this

1149
00:59:32,960 --> 00:59:36,960
in the early 2000s in their palladium project.

1150
00:59:36,960 --> 00:59:39,960
But at that time, the economics was not there.

1151
00:59:39,960 --> 00:59:41,960
But given that right now,

1152
00:59:41,960 --> 00:59:44,960
some estimates of the cost of malware

1153
00:59:44,960 --> 00:59:47,960
are about $6 trillion a year,

1154
00:59:47,960 --> 00:59:53,960
then maybe the time is right to look at this again.

1155
00:59:53,960 --> 00:59:56,960
It seems like a daunting but worthwhile task.

1156
00:59:56,960 --> 00:59:59,960
Let me see.

1157
00:59:59,960 --> 01:00:01,960
Do we have women in the audience

1158
01:00:01,960 --> 01:00:03,960
that have a question? Yes.

1159
01:00:06,960 --> 01:00:08,960
Hi, Dr. Russell.

1160
01:00:08,960 --> 01:00:11,960
I think my question might be related to what was just asked,

1161
01:00:11,960 --> 01:00:13,960
but indeed you're proposing a new model

1162
01:00:13,960 --> 01:00:16,960
where we create beneficial machines rather than intelligent,

1163
01:00:16,960 --> 01:00:20,960
but beneficial can mean different things to different people.

1164
01:00:20,960 --> 01:00:23,960
So are there going to be human standards

1165
01:00:23,960 --> 01:00:25,960
as to what is beneficial to humanity,

1166
01:00:25,960 --> 01:00:30,960
or would it, in your recommendation, be defined, per instance?

1167
01:00:31,960 --> 01:00:38,960
So there is about 8 billion people on the earth,

1168
01:00:38,960 --> 01:00:42,960
and there's no problem having 8 billion predictive models

1169
01:00:42,960 --> 01:00:46,960
of what each person wants the future to be like.

1170
01:00:46,960 --> 01:00:52,960
So there's no sense in which we standardise

1171
01:00:52,960 --> 01:00:54,960
what humans should want

1172
01:00:54,960 --> 01:00:58,960
or put in any particular set of values.

1173
01:00:58,960 --> 01:01:03,960
So there's no whose values it is going to produce.

1174
01:01:03,960 --> 01:01:08,960
It's going to be everyone's preferences count equally.

1175
01:01:08,960 --> 01:01:13,960
But there's a long-standing debate in moral philosophy

1176
01:01:13,960 --> 01:01:19,960
how do you aggregate the preferences of many individuals,

1177
01:01:19,960 --> 01:01:23,960
because, for example, if everyone wants to be a ruler of the universe,

1178
01:01:23,960 --> 01:01:25,960
well, they can't all be a ruler of the universe.

1179
01:01:25,960 --> 01:01:28,960
So what do you do?

1180
01:01:28,960 --> 01:01:34,960
And the utilitarian theory is that basically

1181
01:01:34,960 --> 01:01:37,960
you add up the preferences of the individuals

1182
01:01:37,960 --> 01:01:41,960
and you try to maximise the sum of those preferences.

1183
01:01:41,960 --> 01:01:44,960
Other people have what's called the ontological approach.

1184
01:01:44,960 --> 01:01:51,960
They say, no, we have to have certain inalienable rights

1185
01:01:51,960 --> 01:01:53,960
that need to be protected,

1186
01:01:53,960 --> 01:01:57,960
regardless of the potentially negative impact

1187
01:01:57,960 --> 01:02:00,960
on other people of respecting those rights.

1188
01:02:00,960 --> 01:02:04,960
And I believe that these two approaches can be reconciled

1189
01:02:04,960 --> 01:02:07,960
and so on, and there's some material in the book

1190
01:02:07,960 --> 01:02:12,960
in the last two chapters about those questions.

1191
01:02:12,960 --> 01:02:17,960
There are still some real difficulties inherent

1192
01:02:17,960 --> 01:02:23,960
in how an AI system should make decisions on behalf of people.

1193
01:02:23,960 --> 01:02:27,960
And this is nothing to do with my particular approach

1194
01:02:27,960 --> 01:02:29,960
of the assistance games always.

1195
01:02:29,960 --> 01:02:33,960
This is just what do we actually want AI systems to do at all, right?

1196
01:02:33,960 --> 01:02:39,960
So the idea that's, I think, is most difficult to,

1197
01:02:39,960 --> 01:02:41,960
the problem that's most difficult to address

1198
01:02:41,960 --> 01:02:44,960
is that what people want the future to be like

1199
01:02:44,960 --> 01:02:50,960
is not something that they autonomously chose, right?

1200
01:02:50,960 --> 01:02:53,960
We're not born with complicated preferences

1201
01:02:53,960 --> 01:02:57,960
about what kind of governmental structure

1202
01:02:57,960 --> 01:03:00,960
I want to live under and things like that, right?

1203
01:03:00,960 --> 01:03:03,960
Our preferences about the future

1204
01:03:03,960 --> 01:03:06,960
are acquired during our lifetime

1205
01:03:06,960 --> 01:03:09,960
as a result of experience of our culture

1206
01:03:09,960 --> 01:03:12,960
and the various forces applied to us

1207
01:03:12,960 --> 01:03:16,960
by our families and our peers and so on.

1208
01:03:16,960 --> 01:03:20,960
And Matja Sen, among others, has pointed out

1209
01:03:20,960 --> 01:03:23,960
that many of the preferences that people have

1210
01:03:23,960 --> 01:03:26,960
are put there by others for their own benefit.

1211
01:03:26,960 --> 01:03:29,960
So typically the elite, for example,

1212
01:03:29,960 --> 01:03:35,960
the patriarchy enforces a certain kind of view of society

1213
01:03:35,960 --> 01:03:38,960
that's beneficial to the patriarchy.

1214
01:03:38,960 --> 01:03:42,960
And should we take those views,

1215
01:03:42,960 --> 01:03:47,960
for example, the views of some women in very patriarchal societies

1216
01:03:47,960 --> 01:03:52,960
that the correct status of women is to be oppressed, right?

1217
01:03:52,960 --> 01:03:54,960
Should we take those views at face value

1218
01:03:54,960 --> 01:03:59,960
because they are not autonomously chosen,

1219
01:03:59,960 --> 01:04:03,960
they are basically indoctrinated by the patriarchy.

1220
01:04:03,960 --> 01:04:06,960
So Sen argues that no,

1221
01:04:06,960 --> 01:04:10,960
we should not take those preferences at face value.

1222
01:04:10,960 --> 01:04:14,960
But that gets you into very dangerous territory, right?

1223
01:04:14,960 --> 01:04:17,960
Well, which preferences are okay to take at face value

1224
01:04:17,960 --> 01:04:20,960
and which ones are not okay to take at face value?

1225
01:04:20,960 --> 01:04:23,960
And if they're not okay to take them at face value,

1226
01:04:23,960 --> 01:04:26,960
well, what do you replace them with?

1227
01:04:26,960 --> 01:04:29,960
And this is an area where I don't think

1228
01:04:29,960 --> 01:04:32,960
AI researchers should be answering that question.

1229
01:04:32,960 --> 01:04:36,960
But we need answers fairly soon from somewhere

1230
01:04:36,960 --> 01:04:39,960
because AI systems are going to be making decisions

1231
01:04:39,960 --> 01:04:41,960
on behalf of many people.

1232
01:04:41,960 --> 01:04:43,960
So whether you like it or not,

1233
01:04:43,960 --> 01:04:49,960
they are implementing some answer to that moral problem.

1234
01:04:49,960 --> 01:04:51,960
And it might be the wrong one

1235
01:04:51,960 --> 01:04:53,960
if we don't actually think it through.

1236
01:04:53,960 --> 01:04:55,960
That's interesting.

1237
01:04:55,960 --> 01:04:59,960
AI as a catalyst to some of the most pressing moral concerns

1238
01:04:59,960 --> 01:05:01,960
of mankind so far.

1239
01:05:01,960 --> 01:05:04,960
One final question, maybe.

1240
01:05:04,960 --> 01:05:07,960
They're completely in the left.

1241
01:05:12,960 --> 01:05:14,960
Hi, Professor Russell.

1242
01:05:14,960 --> 01:05:17,960
Thank you for your talk.

1243
01:05:17,960 --> 01:05:19,960
I just want to ask about,

1244
01:05:19,960 --> 01:05:21,960
so you believe that large language models

1245
01:05:21,960 --> 01:05:27,960
wouldn't be able to actually be capable of AGI.

1246
01:05:27,960 --> 01:05:31,960
So why would you sign the open letter, basically?

1247
01:05:31,960 --> 01:05:35,960
So since that,

1248
01:05:35,960 --> 01:05:38,960
it won't be a catastrophic risk per se

1249
01:05:38,960 --> 01:05:40,960
since it won't be able to become

1250
01:05:40,960 --> 01:05:42,960
general artificial intelligence.

1251
01:05:42,960 --> 01:05:45,960
So do you see any catastrophic risk

1252
01:05:45,960 --> 01:05:49,960
in companies building larger and larger models?

1253
01:05:49,960 --> 01:05:53,960
Or is it just for general safety purposes?

1254
01:05:53,960 --> 01:05:57,960
So the question is,

1255
01:05:57,960 --> 01:06:00,960
is it actually something

1256
01:06:00,960 --> 01:06:02,960
that we should really be worried about

1257
01:06:02,960 --> 01:06:05,960
large language models?

1258
01:06:05,960 --> 01:06:08,960
So I think large language models

1259
01:06:08,960 --> 01:06:12,960
in isolation as we currently conceive them

1260
01:06:12,960 --> 01:06:17,960
are probably not presenting that kind of catastrophic risk.

1261
01:06:17,960 --> 01:06:19,960
I think they present many, many risks.

1262
01:06:19,960 --> 01:06:21,960
And the open letter talks about

1263
01:06:21,960 --> 01:06:25,960
some of those disinformation bias, et cetera, et cetera.

1264
01:06:25,960 --> 01:06:29,960
So I think there are already many reasons

1265
01:06:29,960 --> 01:06:31,960
that these systems would fail

1266
01:06:31,960 --> 01:06:36,960
any reasonable safety criteria.

1267
01:06:36,960 --> 01:06:40,960
But the concern is that it's not just,

1268
01:06:40,960 --> 01:06:43,960
we're not just going to make these models bigger.

1269
01:06:43,960 --> 01:06:47,960
We are also going to try to figure out

1270
01:06:47,960 --> 01:06:51,960
how can they be arranged

1271
01:06:51,960 --> 01:06:54,960
so that they actually develop

1272
01:06:54,960 --> 01:06:56,960
a consistent internal model of the world?

1273
01:06:56,960 --> 01:06:58,960
How can they be arranged

1274
01:06:58,960 --> 01:07:02,960
so that they can also do long-term planning?

1275
01:07:02,960 --> 01:07:04,960
En, as I say,

1276
01:07:04,960 --> 01:07:07,960
we don't really know the answers to those questions yet,

1277
01:07:07,960 --> 01:07:12,960
but I think that the ideas behind

1278
01:07:12,960 --> 01:07:14,960
large language models

1279
01:07:14,960 --> 01:07:19,960
do form a significant piece of the puzzle.

1280
01:07:19,960 --> 01:07:24,960
And so the concern is that future generations of these systems,

1281
01:07:24,960 --> 01:07:26,960
which will be extended,

1282
01:07:26,960 --> 01:07:28,960
not just in scale,

1283
01:07:28,960 --> 01:07:33,960
but also in the additional capabilities

1284
01:07:33,960 --> 01:07:36,960
that we might endow them with

1285
01:07:36,960 --> 01:07:42,960
by maybe a more design-based approach,

1286
01:07:42,960 --> 01:07:46,960
that those systems would start to

1287
01:07:46,960 --> 01:07:51,960
get close to presenting a real threat.

1288
01:07:51,960 --> 01:07:54,960
En so in some ways,

1289
01:07:54,960 --> 01:07:56,960
I think this title of that paper,

1290
01:07:56,960 --> 01:08:01,960
Sparks of Artificial General Intelligence,

1291
01:08:01,960 --> 01:08:07,960
is not wrong.

1292
01:08:07,960 --> 01:08:10,960
And I think when you think about

1293
01:08:10,960 --> 01:08:11,960
what does sparks mean,

1294
01:08:11,960 --> 01:08:15,960
what sparks are a predecessor to a fire.

1295
01:08:15,960 --> 01:08:23,960
En I think that's what we want to prevent.

1296
01:08:23,960 --> 01:08:24,960
En with that,

1297
01:08:24,960 --> 01:08:27,960
we come to the end of the first part of this evening.

1298
01:08:27,960 --> 01:08:28,960
Professor Russell,

1299
01:08:28,960 --> 01:08:30,960
I'd like to thank you for elaborating

1300
01:08:30,960 --> 01:08:32,960
in such a concise way

1301
01:08:32,960 --> 01:08:35,960
the dangers of developing intelligent AI

1302
01:08:35,960 --> 01:08:39,960
and to provide a manual, so to speak,

1303
01:08:39,960 --> 01:08:41,960
to steer away from that cliff

1304
01:08:41,960 --> 01:08:46,960
and to develop safe, beneficial artificial intelligence

1305
01:08:46,960 --> 01:08:48,960
that is aligned with our goals.

1306
01:08:48,960 --> 01:08:50,960
So join me in a big round of applause

1307
01:08:50,960 --> 01:08:52,960
for Professor Russell.

1308
01:09:02,960 --> 01:09:03,960
En with that,

1309
01:09:03,960 --> 01:09:05,960
we're going to leave you, Professor Russell,

1310
01:09:05,960 --> 01:09:07,960
and I would like to invite the five panelists

1311
01:09:07,960 --> 01:09:10,960
to continue the conversation.

1312
01:09:10,960 --> 01:09:14,960
Please come to the floor.

1313
01:09:14,960 --> 01:09:18,960
And I'll introduce you properly.

1314
01:09:18,960 --> 01:09:20,960
Marc Brakel, to the right,

1315
01:09:20,960 --> 01:09:24,960
he's a director of policy at Future Life Institute,

1316
01:09:24,960 --> 01:09:26,960
involved in the AI Act

1317
01:09:26,960 --> 01:09:29,960
that is currently being prepared

1318
01:09:29,960 --> 01:09:32,960
and is later this year going to be proposed

1319
01:09:32,960 --> 01:09:34,960
by the European Union, am I right?

1320
01:09:34,960 --> 01:09:35,960
It's already been proposed.

1321
01:09:35,960 --> 01:09:36,960
Oh, it's already been proposed.

1322
01:09:36,960 --> 01:09:37,960
Hopefully it gets voted through.

1323
01:09:37,960 --> 01:09:38,960
Voted through.

1324
01:09:38,960 --> 01:09:39,960
I should say that, yeah.

1325
01:09:39,960 --> 01:09:41,960
Then we have Tim Bakker,

1326
01:09:41,960 --> 01:09:45,960
who is a PhD student at the University of Amsterdam.

1327
01:09:45,960 --> 01:09:47,960
The title of your thesis?

1328
01:09:47,960 --> 01:09:48,960
My thesis?

1329
01:09:48,960 --> 01:09:49,960
Yeah.

1330
01:09:49,960 --> 01:09:50,960
I don't know yet.

1331
01:09:50,960 --> 01:09:51,960
You don't know?

1332
01:09:51,960 --> 01:09:52,960
Live Hanger.

1333
01:09:52,960 --> 01:09:53,960
Chat GPT, yeah.

1334
01:09:53,960 --> 01:09:54,960
Little secrets.

1335
01:09:54,960 --> 01:09:57,960
Working on AI research.

1336
01:09:57,960 --> 01:10:00,960
Then we have Nandi Robijns,

1337
01:10:00,960 --> 01:10:02,960
who is working at the Ministry of Interior

1338
01:10:02,960 --> 01:10:04,960
and Kingdom Relations,

1339
01:10:04,960 --> 01:10:07,960
part of a crew of AI and data consultants.

1340
01:10:07,960 --> 01:10:09,960
About 70 people strong.

1341
01:10:09,960 --> 01:10:11,960
I just heard over dinner.

1342
01:10:11,960 --> 01:10:13,960
Nice of you to be here.

1343
01:10:13,960 --> 01:10:15,960
Two members of the Dutch parliament.

1344
01:10:15,960 --> 01:10:17,960
Queenie Rijkovski,

1345
01:10:17,960 --> 01:10:19,960
who is a member of the Liberal Party,

1346
01:10:19,960 --> 01:10:20,960
the VVD,

1347
01:10:20,960 --> 01:10:23,960
and has cyber security and digitisation

1348
01:10:23,960 --> 01:10:24,960
in your portfolio.

1349
01:10:24,960 --> 01:10:26,960
And last but not least,

1350
01:10:26,960 --> 01:10:28,960
Lammert van Raam,

1351
01:10:28,960 --> 01:10:30,960
who is a member of parliament

1352
01:10:30,960 --> 01:10:32,960
for the party for the animals,

1353
01:10:32,960 --> 01:10:36,960
who is focusing on IT and privacy issues.

1354
01:10:36,960 --> 01:10:38,960
Great of you guys to come over

1355
01:10:38,960 --> 01:10:40,960
and stand here and discuss with us

1356
01:10:40,960 --> 01:10:42,960
the dangers of AI

1357
01:10:42,960 --> 01:10:44,960
and how to deal with them.

1358
01:10:44,960 --> 01:10:47,960
Let me just start off with the obvious question.

1359
01:10:47,960 --> 01:10:51,960
Professor Russell just painted a rather scary picture

1360
01:10:51,960 --> 01:10:55,960
of humanity that may one day fear of a cliff

1361
01:10:55,960 --> 01:10:59,960
because we don't control the risks involved in AI.

1362
01:10:59,960 --> 01:11:01,960
Do you share this view?

1363
01:11:01,960 --> 01:11:04,960
Do you also think artificial intelligence

1364
01:11:04,960 --> 01:11:05,960
may sooner of later,

1365
01:11:05,960 --> 01:11:07,960
if we don't control as well,

1366
01:11:07,960 --> 01:11:09,960
steer humanity over a cliff?

1367
01:11:10,960 --> 01:11:11,960
I work at an organisation

1368
01:11:11,960 --> 01:11:13,960
at the Future of Life Institute

1369
01:11:13,960 --> 01:11:14,960
that believes this,

1370
01:11:14,960 --> 01:11:16,960
so it doesn't come as a surprise,

1371
01:11:16,960 --> 01:11:18,960
I think, to say that I agree

1372
01:11:18,960 --> 01:11:21,960
with Stuart Eastman of our advisors also

1373
01:11:21,960 --> 01:11:24,960
and just in response to the last question also

1374
01:11:24,960 --> 01:11:27,960
about our open letter that we put out

1375
01:11:27,960 --> 01:11:29,960
I think it's now a week and a half ago.

1376
01:11:29,960 --> 01:11:31,960
This is the first event where I'm at

1377
01:11:31,960 --> 01:11:34,960
with actual people since we put the letter out.

1378
01:11:35,960 --> 01:11:38,960
I think it's really worth reading that

1379
01:11:38,960 --> 01:11:39,960
because the letter,

1380
01:11:39,960 --> 01:11:41,960
the open letter that we presented

1381
01:11:41,960 --> 01:11:43,960
talks about all kinds of risks

1382
01:11:43,960 --> 01:11:45,960
that our society might struggle with

1383
01:11:45,960 --> 01:11:46,960
when it comes to AI,

1384
01:11:46,960 --> 01:11:48,960
not just the existential risk.

1385
01:11:48,960 --> 01:11:49,960
Right.

1386
01:11:49,960 --> 01:11:51,960
And I think our first contact with AI

1387
01:11:51,960 --> 01:11:53,960
as Stuart Russell also highlighted

1388
01:11:53,960 --> 01:11:54,960
was social media

1389
01:11:54,960 --> 01:11:56,960
with a super simple algorithm

1390
01:11:56,960 --> 01:11:58,960
our second contact with AI

1391
01:11:58,960 --> 01:12:00,960
is probably these large neural networks

1392
01:12:00,960 --> 01:12:02,960
and I think we're going to really struggle

1393
01:12:02,960 --> 01:12:04,960
to control truths,

1394
01:12:04,960 --> 01:12:06,960
to control access to

1395
01:12:06,960 --> 01:12:08,960
what was previously very hard

1396
01:12:08,960 --> 01:12:10,960
to access information.

1397
01:12:10,960 --> 01:12:12,960
So yes, I worry about existential risks,

1398
01:12:12,960 --> 01:12:13,960
I agree with Stuart,

1399
01:12:13,960 --> 01:12:15,960
but I also think beneath that

1400
01:12:15,960 --> 01:12:17,960
there's a layer of very, very serious risks

1401
01:12:17,960 --> 01:12:19,960
that is also a cause of worry.

1402
01:12:19,960 --> 01:12:20,960
Right.

1403
01:12:20,960 --> 01:12:22,960
We'll touch upon them probably later.

1404
01:12:22,960 --> 01:12:24,960
Ja.

1405
01:12:24,960 --> 01:12:26,960
No, I definitely agree with

1406
01:12:26,960 --> 01:12:28,960
Professor Russell about his worries

1407
01:12:28,960 --> 01:12:30,960
and actually also with Mark

1408
01:12:30,960 --> 01:12:31,960
about what he just said.

1409
01:12:31,960 --> 01:12:33,960
I think Stuart Russell was very right

1410
01:12:33,960 --> 01:12:35,960
about pointing to the fundamental problem

1411
01:12:35,960 --> 01:12:36,960
with the current systems

1412
01:12:36,960 --> 01:12:38,960
which is that we're training them

1413
01:12:38,960 --> 01:12:40,960
as optimizers

1414
01:12:40,960 --> 01:12:42,960
instead of as things that

1415
01:12:42,960 --> 01:12:44,960
do anything that is not that

1416
01:12:44,960 --> 01:12:46,960
because that is just such a hard thing

1417
01:12:46,960 --> 01:12:48,960
to aim in a way that we

1418
01:12:48,960 --> 01:12:49,960
want to aim it.

1419
01:12:49,960 --> 01:12:51,960
We just have no idea how to do that.

1420
01:12:51,960 --> 01:12:52,960
Dandy.

1421
01:12:52,960 --> 01:12:54,960
Yes.

1422
01:12:54,960 --> 01:12:57,960
I think it is very important to take into account

1423
01:12:57,960 --> 01:12:59,960
a wide range of potential risks of AI,

1424
01:12:59,960 --> 01:13:01,960
especially because AI is such

1425
01:13:01,960 --> 01:13:04,960
extremely powerful technology

1426
01:13:04,960 --> 01:13:07,960
and I think what we talk about today

1427
01:13:07,960 --> 01:13:10,960
is a very important part of this range of risks,

1428
01:13:10,960 --> 01:13:12,960
especially because of the scale

1429
01:13:12,960 --> 01:13:15,960
of the potential negative impact that it can have.

1430
01:13:15,960 --> 01:13:17,960
And on top of that, it is very neglected

1431
01:13:17,960 --> 01:13:19,960
and this neglect is worrying me,

1432
01:13:19,960 --> 01:13:22,960
especially because as we see

1433
01:13:22,960 --> 01:13:25,960
more and more AI systems are extremely capable

1434
01:13:25,960 --> 01:13:28,960
in achieving their programmed goals

1435
01:13:28,960 --> 01:13:31,960
and the main worry

1436
01:13:31,960 --> 01:13:33,960
about these AI systems becoming dangerous

1437
01:13:33,960 --> 01:13:36,960
is rooted in the fact that they pursue these goals

1438
01:13:36,960 --> 01:13:39,960
regardless of whether or not it is what we intended.

1439
01:13:39,960 --> 01:13:42,960
Ja, so that needs to be addressed

1440
01:13:42,960 --> 01:13:45,960
and on top of that, these models,

1441
01:13:45,960 --> 01:13:47,960
no one knows what is going on inside these models,

1442
01:13:47,960 --> 01:13:50,960
as George also also said

1443
01:13:50,960 --> 01:13:52,960
and no one actually knows

1444
01:13:52,960 --> 01:13:55,960
how we can define a goal

1445
01:13:55,960 --> 01:13:57,960
that takes into account every value

1446
01:13:57,960 --> 01:13:58,960
that we care about,

1447
01:13:58,960 --> 01:14:00,960
which is also what we just talked about.

1448
01:14:00,960 --> 01:14:02,960
So yes, I do agree

1449
01:14:02,960 --> 01:14:05,960
that it needs much more attention.

1450
01:14:05,960 --> 01:14:06,960
Queenie.

1451
01:14:06,960 --> 01:14:08,960
Yes, I agree even though

1452
01:14:08,960 --> 01:14:11,960
I am a tech-optimistical person.

1453
01:14:11,960 --> 01:14:15,960
So when it comes to technologies like AI,

1454
01:14:15,960 --> 01:14:17,960
I can definitely see the risk in the downside.

1455
01:14:17,960 --> 01:14:20,960
So I completely agree with the other speakers

1456
01:14:20,960 --> 01:14:23,960
and also when we just heard in a presentation

1457
01:14:23,960 --> 01:14:27,960
that if we, AI can also see maybe human

1458
01:14:27,960 --> 01:14:31,960
as a danger when it comes to climate or climate change.

1459
01:14:31,960 --> 01:14:33,960
We need to really think about

1460
01:14:33,960 --> 01:14:35,960
how we are going to program it.

1461
01:14:35,960 --> 01:14:36,960
What are the goals?

1462
01:14:36,960 --> 01:14:38,960
I think we just heard some examples.

1463
01:14:38,960 --> 01:14:40,960
And one of the experiments that they have been doing,

1464
01:14:40,960 --> 01:14:43,960
TNO, it's a scientific research company

1465
01:14:43,960 --> 01:14:45,960
in de Nederlands organization.

1466
01:14:45,960 --> 01:14:47,960
En they've also done some smaller

1467
01:14:47,960 --> 01:14:48,960
and some bigger experiments.

1468
01:14:48,960 --> 01:14:50,960
And the smaller experiment is

1469
01:14:50,960 --> 01:14:52,960
a robotic vacuum cleaner.

1470
01:14:52,960 --> 01:14:54,960
And they said, okay, your task is

1471
01:14:54,960 --> 01:14:57,960
to keep this clean room dust free.

1472
01:14:57,960 --> 01:14:59,960
And what happened?

1473
01:14:59,960 --> 01:15:02,960
The robot started to block the door.

1474
01:15:02,960 --> 01:15:05,960
Because every time when a human came in,

1475
01:15:05,960 --> 01:15:06,960
the room was dirty.

1476
01:15:06,960 --> 01:15:07,960
You were the actual problem, yeah.

1477
01:15:07,960 --> 01:15:08,960
Yeah, so it's,

1478
01:15:08,960 --> 01:15:11,960
and this is just like a small example, of course.

1479
01:15:11,960 --> 01:15:14,960
But what it got me thinking is

1480
01:15:14,960 --> 01:15:17,960
not only what assignment or what goal

1481
01:15:17,960 --> 01:15:20,960
do you give a system or robot, et cetera.

1482
01:15:20,960 --> 01:15:23,960
But also can you grasp upon

1483
01:15:23,960 --> 01:15:26,960
what the outcomes can be when you ask something.

1484
01:15:26,960 --> 01:15:30,960
And actually when it comes to equality,

1485
01:15:30,960 --> 01:15:34,960
I think AI can maybe even help

1486
01:15:34,960 --> 01:15:36,960
because in my experience

1487
01:15:36,960 --> 01:15:39,960
being a woman in politics,

1488
01:15:39,960 --> 01:15:41,960
working on tech,

1489
01:15:41,960 --> 01:15:45,960
mostly a lot of men around me

1490
01:15:45,960 --> 01:15:47,960
in my context,

1491
01:15:47,960 --> 01:15:51,960
I still hear people say things.

1492
01:15:51,960 --> 01:15:55,960
I still see some articles written in a way

1493
01:15:55,960 --> 01:15:57,960
that they write different when you're a woman

1494
01:15:57,960 --> 01:15:58,960
than when you're a man.

1495
01:15:58,960 --> 01:16:00,960
So actually, I hope,

1496
01:16:00,960 --> 01:16:01,960
so that's also my goal

1497
01:16:01,960 --> 01:16:03,960
from a political perspective,

1498
01:16:03,960 --> 01:16:04,960
if we can make sure

1499
01:16:04,960 --> 01:16:07,960
that we provide the right regulation

1500
01:16:07,960 --> 01:16:10,960
and control when it comes to AI,

1501
01:16:10,960 --> 01:16:13,960
maybe we can even help equality

1502
01:16:13,960 --> 01:16:15,960
instead of being a danger to it.

1503
01:16:15,960 --> 01:16:16,960
Okay, interesting.

1504
01:16:16,960 --> 01:16:18,960
We may continue that conversation

1505
01:16:18,960 --> 01:16:20,960
later this evening.

1506
01:16:20,960 --> 01:16:22,960
Nasty little buggers,

1507
01:16:22,960 --> 01:16:24,960
those vacuum cleaners, right?

1508
01:16:24,960 --> 01:16:26,960
I have.

1509
01:16:26,960 --> 01:16:28,960
So, you have one?

1510
01:16:28,960 --> 01:16:29,960
Yes.

1511
01:16:29,960 --> 01:16:30,960
But you're not locked out yet?

1512
01:16:30,960 --> 01:16:31,960
No, not yet.

1513
01:16:31,960 --> 01:16:33,960
Good for you.

1514
01:16:33,960 --> 01:16:36,960
So, yeah, worrying.

1515
01:16:36,960 --> 01:16:42,960
I was taking comfort from the example of Rutherford

1516
01:16:42,960 --> 01:16:44,960
and the next day,

1517
01:16:44,960 --> 01:16:48,960
Zillar invented something that made it possible.

1518
01:16:48,960 --> 01:16:51,960
Perhaps it's now the 12th of April

1519
01:16:51,960 --> 01:16:53,960
on 13th of April.

1520
01:16:53,960 --> 01:16:56,960
There's one Zillar in the room already

1521
01:16:56,960 --> 01:16:57,960
making a solution

1522
01:16:57,960 --> 01:16:59,960
because that is what possible.

1523
01:16:59,960 --> 01:17:01,960
And yes, it is worrying.

1524
01:17:01,960 --> 01:17:02,960
It is worrying.

1525
01:17:02,960 --> 01:17:04,960
I have only one consolation.

1526
01:17:04,960 --> 01:17:07,960
A politician will probably have to solve it.

1527
01:17:07,960 --> 01:17:09,960
And then it's...

1528
01:17:09,960 --> 01:17:11,960
I don't know if it's a consolation, to be honest.

1529
01:17:11,960 --> 01:17:13,960
But...

1530
01:17:13,960 --> 01:17:15,960
It's the best we have, right?

1531
01:17:15,960 --> 01:17:19,960
It's the best we have politicians in a democracy.

1532
01:17:19,960 --> 01:17:21,960
Nevertheless, we are working together

1533
01:17:21,960 --> 01:17:24,960
on the concerns we both feel.

1534
01:17:24,960 --> 01:17:26,960
And I think that's giving some hope

1535
01:17:26,960 --> 01:17:29,960
because she's a completely different ideology

1536
01:17:29,960 --> 01:17:31,960
than my party.

1537
01:17:31,960 --> 01:17:34,960
En stil we find the same common ground

1538
01:17:34,960 --> 01:17:36,960
in our concerns.

1539
01:17:36,960 --> 01:17:38,960
En dat is, I think,

1540
01:17:38,960 --> 01:17:40,960
something to look forward.

1541
01:17:40,960 --> 01:17:42,960
The other consolation is we don't have to worry

1542
01:17:42,960 --> 01:17:45,960
about falling off the cliff

1543
01:17:45,960 --> 01:17:48,960
because we are already sliding off the cliff.

1544
01:17:48,960 --> 01:17:50,960
That's very comforting, yeah.

1545
01:17:50,960 --> 01:17:52,960
And for you, Otto,

1546
01:17:52,960 --> 01:17:54,960
the first slide, I have to say it, I'm sorry,

1547
01:17:54,960 --> 01:17:57,960
the first slide with all the risks,

1548
01:17:57,960 --> 01:17:59,960
don't show it to the animals

1549
01:17:59,960 --> 01:18:02,960
because they're already in a massive wave.

1550
01:18:02,960 --> 01:18:05,960
And don't show it to the global south.

1551
01:18:05,960 --> 01:18:08,960
But there are ways probably

1552
01:18:08,960 --> 01:18:10,960
if we can get this problem solved.

1553
01:18:10,960 --> 01:18:13,960
If there are enough zillars in the room,

1554
01:18:13,960 --> 01:18:15,960
we're counting on you.

1555
01:18:15,960 --> 01:18:17,960
We can also solve it politically

1556
01:18:17,960 --> 01:18:19,960
because the worries are very real.

1557
01:18:19,960 --> 01:18:21,960
So thank you.

1558
01:18:21,960 --> 01:18:24,960
So you all share, more or less,

1559
01:18:24,960 --> 01:18:28,960
the alarming story that Professor Russell just told us.

1560
01:18:28,960 --> 01:18:31,960
But at the same time, I don't see us all going to the streets

1561
01:18:31,960 --> 01:18:34,960
and protesting like we do with climate change

1562
01:18:34,960 --> 01:18:39,960
or in the 1980s we did with the risk of nuclear war.

1563
01:18:39,960 --> 01:18:41,960
So what's wrong here?

1564
01:18:41,960 --> 01:18:45,960
Why aren't we, if we all share this great risk

1565
01:18:45,960 --> 01:18:47,960
or concern for this great risk,

1566
01:18:47,960 --> 01:18:49,960
why are we not protesting?

1567
01:18:49,960 --> 01:18:51,960
What's wrong with us, wants to answer?

1568
01:18:51,960 --> 01:18:54,960
Nandi, you actually raised this point yourself.

1569
01:18:55,960 --> 01:18:57,960
So solve it.

1570
01:18:57,960 --> 01:19:00,960
We can't.

1571
01:19:00,960 --> 01:19:04,960
Ja, so I feel like there are some reasons.

1572
01:19:04,960 --> 01:19:07,960
The question is about...

1573
01:19:07,960 --> 01:19:10,960
Well, if the risk is so big for us as a society,

1574
01:19:10,960 --> 01:19:12,960
why aren't we, you know,

1575
01:19:12,960 --> 01:19:14,960
talking about this daily in parliament

1576
01:19:14,960 --> 01:19:16,960
and protesting on the square.

1577
01:19:16,960 --> 01:19:19,960
Yes, I think there are some reasons

1578
01:19:19,960 --> 01:19:22,960
that make it hard for people to realize one

1579
01:19:22,960 --> 01:19:24,960
that this is a real problem

1580
01:19:24,960 --> 01:19:26,960
and to see that this is something

1581
01:19:26,960 --> 01:19:29,960
that needs to be addressed right now.

1582
01:19:29,960 --> 01:19:33,960
And I think one of the reasons is that these aspects,

1583
01:19:33,960 --> 01:19:35,960
these concepts that we talk about

1584
01:19:35,960 --> 01:19:37,960
and the terms that we use are still quite vague

1585
01:19:37,960 --> 01:19:41,960
and difficult for people to understand.

1586
01:19:41,960 --> 01:19:43,960
And sadly, vague problems are much easier

1587
01:19:43,960 --> 01:19:45,960
to ignore than concrete ones,

1588
01:19:45,960 --> 01:19:47,960
which also makes it harder

1589
01:19:47,960 --> 01:19:49,960
for policymakers to prioritize,

1590
01:19:49,960 --> 01:19:52,960
things that are a bit more uncertain and vague

1591
01:19:52,960 --> 01:19:55,960
over things that are more concrete

1592
01:19:55,960 --> 01:19:57,960
and where we can see the harm

1593
01:19:57,960 --> 01:20:00,960
right in front of us right now.

1594
01:20:00,960 --> 01:20:03,960
A second reason also is that, you know,

1595
01:20:03,960 --> 01:20:06,960
this is also, to some people,

1596
01:20:06,960 --> 01:20:09,960
at first glance, quickly dismissed as science fiction,

1597
01:20:09,960 --> 01:20:12,960
not real or something that doesn't need attention right now.

1598
01:20:12,960 --> 01:20:17,960
Ja, which I think is a misconception.

1599
01:20:18,960 --> 01:20:21,960
So yeah, I think this is caused

1600
01:20:21,960 --> 01:20:23,960
by a lack of awareness and understanding

1601
01:20:23,960 --> 01:20:26,960
and a lack of urgency that we need to address.

1602
01:20:26,960 --> 01:20:28,960
At PR, yeah.

1603
01:20:28,960 --> 01:20:30,960
First Lamert, and then it goes to you.

1604
01:20:30,960 --> 01:20:33,960
I fully agree with you that there's a lack of knowledge, et cetera.

1605
01:20:33,960 --> 01:20:35,960
But perhaps it's also,

1606
01:20:35,960 --> 01:20:37,960
perhaps the protest is already there,

1607
01:20:37,960 --> 01:20:39,960
but we just don't recognize it.

1608
01:20:39,960 --> 01:20:42,960
For instance, there's, well,

1609
01:20:42,960 --> 01:20:45,960
the best example, of course, is the upheaval there was

1610
01:20:45,960 --> 01:20:49,960
in the Netherlands, of the toeslagen schandaal.

1611
01:20:49,960 --> 01:20:53,960
And algorithms played a very big role in that.

1612
01:20:53,960 --> 01:20:58,960
And also in the, let's say, discrimination factor of that.

1613
01:20:58,960 --> 01:21:01,960
And that led to the fall of the government.

1614
01:21:01,960 --> 01:21:03,960
So there was a big upheaval,

1615
01:21:03,960 --> 01:21:06,960
but we didn't perhaps recognize it as such.

1616
01:21:06,960 --> 01:21:08,960
So perhaps there is a lot of upheaval,

1617
01:21:08,960 --> 01:21:11,960
but we just have to categorize it differently to understand it.

1618
01:21:11,960 --> 01:21:15,960
But I agree fully with you that there's also a very,

1619
01:21:15,960 --> 01:21:19,960
a lack of understanding and a forelichting.

1620
01:21:19,960 --> 01:21:21,960
What is that?

1621
01:21:21,960 --> 01:21:22,960
Education, thank you.

1622
01:21:22,960 --> 01:21:23,960
Thank you, George.

1623
01:21:23,960 --> 01:21:24,960
Mark.

1624
01:21:24,960 --> 01:21:28,960
Yeah, if I could maybe add two sort of points of optimism to that.

1625
01:21:28,960 --> 01:21:31,960
I think when Otto first asked about sort of doing this event,

1626
01:21:31,960 --> 01:21:33,960
it was going to be in the smallest room of this building.

1627
01:21:33,960 --> 01:21:34,960
Right.

1628
01:21:34,960 --> 01:21:36,960
And it sold out, and now we're in the big room.

1629
01:21:36,960 --> 01:21:40,960
So I think that shows that I think society is moving

1630
01:21:40,960 --> 01:21:43,960
maybe slower than the development of the systems.

1631
01:21:43,960 --> 01:21:45,960
But still, it's of interest to more people.

1632
01:21:45,960 --> 01:21:49,960
And when we sat together with the Future of Life Institute,

1633
01:21:49,960 --> 01:21:53,960
with my sort of 16 colleagues four weeks ago brainstorming this letter,

1634
01:21:53,960 --> 01:21:56,960
we thought, okay, maybe we can have four news outlets cover it,

1635
01:21:56,960 --> 01:21:58,960
mainly in the United States.

1636
01:21:58,960 --> 01:22:01,960
Potentially we get one in Europe, that'd be great.

1637
01:22:01,960 --> 01:22:04,960
And a colleague of mine comes from rural Australia,

1638
01:22:04,960 --> 01:22:08,960
and her mum had heard it at the hairdressers on the radio show.

1639
01:22:08,960 --> 01:22:10,960
And I think that shows that.

1640
01:22:10,960 --> 01:22:13,960
Great source of information, the hairdressers recommended.

1641
01:22:13,960 --> 01:22:16,960
People are slowly waking up to the risk.

1642
01:22:16,960 --> 01:22:18,960
And the Overton window is also shifting.

1643
01:22:18,960 --> 01:22:20,960
I think a lot of governments are waking up to the fact

1644
01:22:20,960 --> 01:22:23,960
that they need to regulate this, and really quite quickly.

1645
01:22:23,960 --> 01:22:24,960
So we're going in the right direction.

1646
01:22:24,960 --> 01:22:27,960
Are we all happy with the direction we're going here?

1647
01:22:27,960 --> 01:22:28,960
Or are we, yeah?

1648
01:22:28,960 --> 01:22:31,960
I mean, I don't want to be overly optimistic.

1649
01:22:31,960 --> 01:22:34,960
I mean, the one thing that worries me is companies,

1650
01:22:34,960 --> 01:22:37,960
because we have your Russell's proposal here to say,

1651
01:22:37,960 --> 01:22:38,960
okay, we need to look at AI systems,

1652
01:22:38,960 --> 01:22:40,960
and we need to make sure that they are uncertain

1653
01:22:40,960 --> 01:22:41,960
about what our objectives are.

1654
01:22:41,960 --> 01:22:42,960
Right.

1655
01:22:42,960 --> 01:22:43,960
Whereas all of the investment,

1656
01:22:43,960 --> 01:22:45,960
and the economist in an article last week,

1657
01:22:45,960 --> 01:22:48,960
just saying how it escalated since chat GPT,

1658
01:22:48,960 --> 01:22:50,960
how many more billions are suddenly available to invest,

1659
01:22:50,960 --> 01:22:53,960
are all going to neural nets that do exactly the opposite.

1660
01:22:53,960 --> 01:22:56,960
Everyone is clueless as to what these systems do,

1661
01:22:56,960 --> 01:22:59,960
including the chief technology officer of open AI

1662
01:22:59,960 --> 01:23:01,960
who goes on TV and says that.

1663
01:23:01,960 --> 01:23:04,960
I'm interested to hear your comments on this, Tim,

1664
01:23:04,960 --> 01:23:07,960
because you actually interned at Facebook AI.

1665
01:23:07,960 --> 01:23:08,960
Yes.

1666
01:23:08,960 --> 01:23:11,960
Which was a long story, and quite eccentric,

1667
01:23:11,960 --> 01:23:12,960
what you did there.

1668
01:23:12,960 --> 01:23:13,960
I'm going to have to fed myself now.

1669
01:23:13,960 --> 01:23:15,960
No, you don't.

1670
01:23:15,960 --> 01:23:17,960
He was not working on the metaphors.

1671
01:23:17,960 --> 01:23:20,960
But please tell me, how do you view this danger?

1672
01:23:20,960 --> 01:23:22,960
Yeah, so it's, I mean, it's interesting,

1673
01:23:22,960 --> 01:23:24,960
because I've been worried about these topics

1674
01:23:24,960 --> 01:23:25,960
for a very long time,

1675
01:23:25,960 --> 01:23:27,960
and I've now been involved with AI

1676
01:23:27,960 --> 01:23:30,960
for a bit less than that, but still.

1677
01:23:30,960 --> 01:23:33,960
And it's been interesting to see the shift in opinions,

1678
01:23:33,960 --> 01:23:36,960
like, as Nandi said, people at the start really thought,

1679
01:23:36,960 --> 01:23:38,960
okay, this is some kind of science fiction

1680
01:23:38,960 --> 01:23:39,960
why you worried about this.

1681
01:23:39,960 --> 01:23:42,960
If you look at these systems, they're not able to do anything.

1682
01:23:42,960 --> 01:23:45,960
It makes no sense at all, stop worrying.

1683
01:23:45,960 --> 01:23:47,960
And in the last, say, two years,

1684
01:23:47,960 --> 01:23:49,960
but especially the last two months,

1685
01:23:49,960 --> 01:23:51,960
this has really changed in the community as well.

1686
01:23:51,960 --> 01:23:53,960
There's been so many researchers

1687
01:23:53,960 --> 01:23:55,960
that are now coming out as, oh yeah, actually,

1688
01:23:55,960 --> 01:23:56,960
I am kind of worried,

1689
01:23:56,960 --> 01:23:58,960
and I actually have been worried for a while,

1690
01:23:58,960 --> 01:23:59,960
but I kind of couldn't say,

1691
01:23:59,960 --> 01:24:01,960
because it was just such a weird thing,

1692
01:24:01,960 --> 01:24:02,960
and I couldn't really, you know,

1693
01:24:02,960 --> 01:24:04,960
probably if I said that to my colleagues,

1694
01:24:04,960 --> 01:24:05,960
who would call me crazy.

1695
01:24:05,960 --> 01:24:07,960
And now you have people like Geoffrey Hinton,

1696
01:24:07,960 --> 01:24:10,960
who is sort of often considered

1697
01:24:10,960 --> 01:24:11,960
the godfather of modern AI,

1698
01:24:11,960 --> 01:24:13,960
going on national television in an interview

1699
01:24:13,960 --> 01:24:15,960
and saying, yeah, it's not inconceivable

1700
01:24:15,960 --> 01:24:18,960
that AI will wipe us all out.

1701
01:24:18,960 --> 01:24:20,960
And also, I don't know what to do about that.

1702
01:24:20,960 --> 01:24:23,960
And so, it's become much more of an okay thing

1703
01:24:23,960 --> 01:24:25,960
to worry about, and I think that is quite hopeful.

1704
01:24:25,960 --> 01:24:27,960
Of course, that doesn't mean we know

1705
01:24:27,960 --> 01:24:29,960
how to solve the problem yet,

1706
01:24:29,960 --> 01:24:32,960
but at least we're allowed

1707
01:24:32,960 --> 01:24:34,960
as a scientific community

1708
01:24:34,960 --> 01:24:35,960
and also as a tech community

1709
01:24:35,960 --> 01:24:37,960
to at least consider these problems seriously,

1710
01:24:37,960 --> 01:24:39,960
and that's very good.

1711
01:24:39,960 --> 01:24:41,960
I'm also interested to hear the comments

1712
01:24:41,960 --> 01:24:43,960
of the two parliamentarians here,

1713
01:24:43,960 --> 01:24:45,960
because, well, your job, partly,

1714
01:24:45,960 --> 01:24:47,960
is to devise laws

1715
01:24:47,960 --> 01:24:49,960
and to think about how we could improve

1716
01:24:49,960 --> 01:24:52,960
the well-being of us people here in the Netherlands.

1717
01:24:52,960 --> 01:24:54,960
So, what do you think is the risk

1718
01:24:54,960 --> 01:24:58,960
of tech companies devising new AI systems

1719
01:24:58,960 --> 01:25:02,960
that may not be aligned with our well-being?

1720
01:25:02,960 --> 01:25:04,960
En what can you do about that?

1721
01:25:04,960 --> 01:25:05,960
Biggest party first?

1722
01:25:05,960 --> 01:25:07,960
Ja, ja, well, I think the...

1723
01:25:07,960 --> 01:25:09,960
Thanks.

1724
01:25:09,960 --> 01:25:10,960
That's a privilege.

1725
01:25:10,960 --> 01:25:11,960
Very generous.

1726
01:25:11,960 --> 01:25:12,960
I think...

1727
01:25:12,960 --> 01:25:15,960
No, well, of course, like the risks,

1728
01:25:15,960 --> 01:25:17,960
I think some of the risks we already talked about,

1729
01:25:17,960 --> 01:25:20,960
because in January, I think,

1730
01:25:20,960 --> 01:25:24,960
we had a big debate about AI in the parliament,

1731
01:25:24,960 --> 01:25:27,960
and one of the things that we also talked about

1732
01:25:27,960 --> 01:25:29,960
was, OK, so...

1733
01:25:29,960 --> 01:25:32,960
But, and you see it now with the open letter.

1734
01:25:32,960 --> 01:25:35,960
The people who write the code

1735
01:25:35,960 --> 01:25:38,960
are worried about what's going to happen with AI.

1736
01:25:38,960 --> 01:25:40,960
So, that's a bit, you know,

1737
01:25:40,960 --> 01:25:43,960
if someone can do something about how a software,

1738
01:25:43,960 --> 01:25:45,960
how a large language model,

1739
01:25:45,960 --> 01:25:47,960
how AI is working,

1740
01:25:47,960 --> 01:25:50,960
it starts with the person who's making it, right?

1741
01:25:50,960 --> 01:25:53,960
So, we also discussed,

1742
01:25:53,960 --> 01:25:56,960
so can you also maybe start looking maybe

1743
01:25:56,960 --> 01:25:59,960
over engineers, professions like that,

1744
01:25:59,960 --> 01:26:02,960
and not only teach them at university

1745
01:26:02,960 --> 01:26:05,960
how to write good code, efficient code, et cetera,

1746
01:26:05,960 --> 01:26:10,960
but also take into account ethics, human rights, et cetera.

1747
01:26:10,960 --> 01:26:14,960
So, where the programming starts,

1748
01:26:14,960 --> 01:26:18,960
also ethics and safety is taken into account.

1749
01:26:18,960 --> 01:26:20,960
The core of the curriculum, ja.

1750
01:26:20,960 --> 01:26:23,960
Exactly, and one thing which I think is hopeful

1751
01:26:23,960 --> 01:26:26,960
is that when you look at social media, big tech, the internet,

1752
01:26:26,960 --> 01:26:30,960
at first everyone starts, oh, it's going to regulate itself, right?

1753
01:26:30,960 --> 01:26:33,960
We only see positive things,

1754
01:26:33,960 --> 01:26:36,960
and human rights will fix this itself,

1755
01:26:36,960 --> 01:26:39,960
and now politics are waking up, oh, wait a minute,

1756
01:26:39,960 --> 01:26:42,960
social media, big tech companies,

1757
01:26:42,960 --> 01:26:44,960
they are a lot about making money

1758
01:26:44,960 --> 01:26:46,960
and not about taking responsibility

1759
01:26:46,960 --> 01:26:49,960
to make sure that they have a good contribution

1760
01:26:49,960 --> 01:26:52,960
to the country that they make money in.

1761
01:26:52,960 --> 01:26:55,960
We are trying to repair that,

1762
01:26:55,960 --> 01:26:57,960
but it's too late,

1763
01:26:57,960 --> 01:26:59,960
there's already a big power,

1764
01:26:59,960 --> 01:27:01,960
they already decide a lot,

1765
01:27:01,960 --> 01:27:03,960
maybe they have even more power

1766
01:27:03,960 --> 01:27:05,960
than a lot of governments have.

1767
01:27:05,960 --> 01:27:07,960
And what I see with AI,

1768
01:27:07,960 --> 01:27:09,960
especially on the European level,

1769
01:27:09,960 --> 01:27:12,960
that the European Commission already started to work on AI regulation

1770
01:27:12,960 --> 01:27:14,960
two years ago.

1771
01:27:14,960 --> 01:27:16,960
So, what is hopeful for me

1772
01:27:16,960 --> 01:27:19,960
is that we already started also from a political point of view,

1773
01:27:19,960 --> 01:27:21,960
because experts are thinking about this,

1774
01:27:21,960 --> 01:27:23,960
way longer than that,

1775
01:27:23,960 --> 01:27:25,960
but also from a political side,

1776
01:27:25,960 --> 01:27:27,960
the thinking has already begun,

1777
01:27:27,960 --> 01:27:29,960
the law regulation is already in the making,

1778
01:27:29,960 --> 01:27:31,960
not only in Europe, but also worldwide,

1779
01:27:31,960 --> 01:27:33,960
they are working with treaties, et cetera.

1780
01:27:33,960 --> 01:27:35,960
So, for me that's hopeful,

1781
01:27:35,960 --> 01:27:39,960
in a way that it helps me

1782
01:27:39,960 --> 01:27:42,960
in thinking that, okay, AI can still do wrong,

1783
01:27:42,960 --> 01:27:45,960
but maybe we are not too late.

1784
01:27:45,960 --> 01:27:48,960
Lamert, how do you view this?

1785
01:27:48,960 --> 01:27:53,960
It seems to me,

1786
01:27:53,960 --> 01:27:58,960
that it's like a winner takes all industry.

1787
01:27:58,960 --> 01:28:02,960
Like, to a certain extent,

1788
01:28:02,960 --> 01:28:05,960
the fossil industry is, wasn't is.

1789
01:28:05,960 --> 01:28:07,960
So, and we have a very,

1790
01:28:07,960 --> 01:28:10,960
that industry has a very bad track record.

1791
01:28:10,960 --> 01:28:15,960
So, I would be inclined to give

1792
01:28:15,960 --> 01:28:17,960
the big companies the,

1793
01:28:17,960 --> 01:28:19,960
not the benefit of the doubt,

1794
01:28:19,960 --> 01:28:21,960
but the disadvantage of the certainty

1795
01:28:21,960 --> 01:28:23,960
that they are still in the race

1796
01:28:23,960 --> 01:28:26,960
in a winner takes all situation.

1797
01:28:26,960 --> 01:28:30,960
So, again, on top of what you're saying,

1798
01:28:30,960 --> 01:28:34,960
I think we should be very restrictive,

1799
01:28:34,960 --> 01:28:37,960
restrictive policy,

1800
01:28:37,960 --> 01:28:42,960
and the European AI directive

1801
01:28:42,960 --> 01:28:48,960
is setting some very strict policies there as well.

1802
01:28:48,960 --> 01:28:50,960
Because it's not something that will

1803
01:28:50,960 --> 01:28:53,960
come from the goodness of the big companies.

1804
01:28:53,960 --> 01:28:56,960
I'm afraid.

1805
01:28:56,960 --> 01:28:58,960
Just like Professor Thompson said,

1806
01:28:58,960 --> 01:29:00,960
I mean, he's hopeful that AI will

1807
01:29:00,960 --> 01:29:03,960
solve world inequality.

1808
01:29:03,960 --> 01:29:05,960
But the thing, of course, is,

1809
01:29:05,960 --> 01:29:08,960
we could have solved world inequality a long time ago.

1810
01:29:08,960 --> 01:29:12,960
We don't need the computers or AI for that.

1811
01:29:12,960 --> 01:29:14,960
So, the problem goes far deeper in that.

1812
01:29:14,960 --> 01:29:17,960
And that's where I connect with Queenie

1813
01:29:17,960 --> 01:29:20,960
in the sense that we need to instill

1814
01:29:20,960 --> 01:29:23,960
the ethics in the educational system

1815
01:29:23,960 --> 01:29:25,960
to try and do that.

1816
01:29:25,960 --> 01:29:27,960
And at the same time,

1817
01:29:27,960 --> 01:29:30,960
very strict regulation, I would say.

1818
01:29:30,960 --> 01:29:33,960
Ja, because even if I can add a little bit to that,

1819
01:29:33,960 --> 01:29:36,960
so what internet, social media, et cetera,

1820
01:29:36,960 --> 01:29:38,960
what they did is they created,

1821
01:29:38,960 --> 01:29:41,960
or actually the companies who created it,

1822
01:29:41,960 --> 01:29:44,960
the companies were big in a sense

1823
01:29:44,960 --> 01:29:46,960
that we have never experienced before.

1824
01:29:46,960 --> 01:29:48,960
They have more power in a way

1825
01:29:48,960 --> 01:29:50,960
we have never experienced before

1826
01:29:50,960 --> 01:29:52,960
from companies, in my opinion.

1827
01:29:52,960 --> 01:29:56,960
And AI can actually triple that.

1828
01:29:56,960 --> 01:30:00,960
So, when we just saw in the presentation

1829
01:30:00,960 --> 01:30:02,960
how much extra quadrillion money

1830
01:30:02,960 --> 01:30:04,960
that can be made,

1831
01:30:04,960 --> 01:30:06,960
the first thing I thought was,

1832
01:30:06,960 --> 01:30:08,960
okay, but in whose pockets is going to,

1833
01:30:08,960 --> 01:30:12,960
and who's going to fill the pockets with the money.

1834
01:30:12,960 --> 01:30:16,960
And I don't think that we'll go to fighting inequality

1835
01:30:16,960 --> 01:30:20,960
if we don't make sure from a political perspective

1836
01:30:20,960 --> 01:30:25,960
that technology can make everyone's life better

1837
01:30:25,960 --> 01:30:27,960
and not only a happy few.

1838
01:30:27,960 --> 01:30:29,960
And I think that's really important

1839
01:30:29,960 --> 01:30:31,960
that we are talking about this right now,

1840
01:30:31,960 --> 01:30:33,960
that we're discussing this right now

1841
01:30:33,960 --> 01:30:35,960
is when we are making regulation.

1842
01:30:35,960 --> 01:30:37,960
And I'd love to hear you talk like that.

1843
01:30:39,960 --> 01:30:41,960
You can switch sides.

1844
01:30:41,960 --> 01:30:43,960
A big hand for Greeny.

1845
01:30:44,960 --> 01:30:46,960
Okay, in a minute

1846
01:30:46,960 --> 01:30:48,960
there's some room for questions from the audience

1847
01:30:48,960 --> 01:30:50,960
to our distinguished panelists.

1848
01:30:50,960 --> 01:30:53,960
But first, I want to pose the million dollar question.

1849
01:30:53,960 --> 01:30:56,960
That's what can we, or what should we all do

1850
01:30:56,960 --> 01:30:58,960
in order to tame the beast,

1851
01:30:58,960 --> 01:31:01,960
in order to avoid, we're going to run off the cliff.

1852
01:31:01,960 --> 01:31:03,960
We have the EU AI Act.

1853
01:31:03,960 --> 01:31:07,960
We have ideas of instilling ethics

1854
01:31:07,960 --> 01:31:13,960
and other subjects into core curricula of AI engineers.

1855
01:31:13,960 --> 01:31:16,960
But there are probably other great ideas

1856
01:31:16,960 --> 01:31:18,960
that we should take into account.

1857
01:31:18,960 --> 01:31:21,960
In a minute, but first I want to hear the five panelists.

1858
01:31:21,960 --> 01:31:24,960
We just make a little round and then the floor is yours.

1859
01:31:24,960 --> 01:31:27,960
So what should we do to tame the beast?

1860
01:31:27,960 --> 01:31:29,960
What should we do tomorrow

1861
01:31:29,960 --> 01:31:31,960
in order to make sure that we don't run off the cliff?

1862
01:31:31,960 --> 01:31:33,960
Of course, we have an AI Act.

1863
01:31:33,960 --> 01:31:35,960
Of course, we have great ideas

1864
01:31:35,960 --> 01:31:37,960
of how to improve the curricula of AI engineers.

1865
01:31:37,960 --> 01:31:40,960
But that's probably not the answer

1866
01:31:40,960 --> 01:31:42,960
to the million dollar question.

1867
01:31:42,960 --> 01:31:44,960
What else should happen?

1868
01:31:44,960 --> 01:31:46,960
I mean, there's a lot.

1869
01:31:46,960 --> 01:31:48,960
Just before coming here,

1870
01:31:48,960 --> 01:31:50,960
we send out seven recommendations to policymakers

1871
01:31:50,960 --> 01:31:52,960
to the signatories of the open letter.

1872
01:31:52,960 --> 01:31:55,960
We'll put that out tomorrow, so go check that out.

1873
01:31:55,960 --> 01:31:58,960
But it has things like national regulatory agencies for AI.

1874
01:31:58,960 --> 01:32:01,960
It has things like more AI safety research

1875
01:32:01,960 --> 01:32:03,960
and public funding in that.

1876
01:32:03,960 --> 01:32:05,960
So it's not just the companies doing that.

1877
01:32:05,960 --> 01:32:07,960
It really requires, I think,

1878
01:32:07,960 --> 01:32:09,960
the world coming together over this.

1879
01:32:09,960 --> 01:32:11,960
But given that I've got...

1880
01:32:11,960 --> 01:32:13,960
What does that mean, the world coming together over this?

1881
01:32:13,960 --> 01:32:15,960
I mean, I think ultimately we need

1882
01:32:15,960 --> 01:32:18,960
a sort of international atomic energy agency for AI.

1883
01:32:18,960 --> 01:32:21,960
So an international body that has enforcement agency

1884
01:32:21,960 --> 01:32:23,960
even over those jurisdictions

1885
01:32:23,960 --> 01:32:25,960
that don't fall under an AI Act

1886
01:32:25,960 --> 01:32:29,960
or aren't part of sort of a big power agreement.

1887
01:32:29,960 --> 01:32:31,960
But I am going to take this opportunity

1888
01:32:31,960 --> 01:32:33,960
with these two Dutch politicians

1889
01:32:33,960 --> 01:32:35,960
because I work a lot in Brussels

1890
01:32:35,960 --> 01:32:37,960
where we have an AI Act,

1891
01:32:37,960 --> 01:32:39,960
but there's also a lot of big tech lobbying.

1892
01:32:39,960 --> 01:32:41,960
I mean, there's maybe four or five NGO people

1893
01:32:41,960 --> 01:32:43,960
and then there's several hundred from Microsoft

1894
01:32:43,960 --> 01:32:45,960
and Google and Bing

1895
01:32:45,960 --> 01:32:47,960
an open AI's own team right now.

1896
01:32:47,960 --> 01:32:49,960
It seems like an uneven fight.

1897
01:32:49,960 --> 01:32:52,960
It is. I think we need the AI Act tomorrow.

1898
01:32:52,960 --> 01:32:55,960
I think Brussels is taking its normal slow course.

1899
01:32:55,960 --> 01:32:57,960
And I think one thing the Dutch Parliament could do

1900
01:32:57,960 --> 01:33:00,960
is to ask for it to be applied provisionally.

1901
01:33:00,960 --> 01:33:02,960
As we have chat GPT right now,

1902
01:33:02,960 --> 01:33:05,960
we probably also need some rules and safeguards.

1903
01:33:05,960 --> 01:33:09,960
Another thing is the Act prohibits manipulation of people.

1904
01:33:09,960 --> 01:33:12,960
But only if you use your AI system in a subliminal way.

1905
01:33:12,960 --> 01:33:14,960
So if it's in a hidden frame.

1906
01:33:14,960 --> 01:33:16,960
But if you do it overtly, it's fine.

1907
01:33:16,960 --> 01:33:19,960
We think that probably should be changed.

1908
01:33:19,960 --> 01:33:21,960
En dan

1909
01:33:21,960 --> 01:33:23,960
maybe my final pitch here

1910
01:33:23,960 --> 01:33:25,960
is that for a long time

1911
01:33:25,960 --> 01:33:27,960
more general AI systems such as chat GPT

1912
01:33:27,960 --> 01:33:29,960
were exempt from the Act.

1913
01:33:29,960 --> 01:33:32,960
We've worked very, very hard to try and bring that into the Act.

1914
01:33:32,960 --> 01:33:34,960
But we also face a lot of Microsoft pushback.

1915
01:33:34,960 --> 01:33:36,960
So if there's anything you can do to keep it there,

1916
01:33:36,960 --> 01:33:38,960
that would be awesome.

1917
01:33:38,960 --> 01:33:40,960
Ok, keep Microsoft at bay.

1918
01:33:42,960 --> 01:33:45,960
Maybe a quick response here

1919
01:33:45,960 --> 01:33:48,960
because those are very sort of concise recommendations.

1920
01:33:48,960 --> 01:33:50,960
What do you think?

1921
01:33:50,960 --> 01:33:52,960
Are you going to take these up

1922
01:33:52,960 --> 01:33:54,960
and next time you talk to your fellow parliamentarians?

1923
01:33:54,960 --> 01:33:56,960
Of course read the recommendations.

1924
01:33:56,960 --> 01:33:58,960
We will be stupid not to do it.

1925
01:33:58,960 --> 01:34:01,960
But I fully agree with the lobbying power

1926
01:34:01,960 --> 01:34:04,960
and the equality of arms is not equal.

1927
01:34:04,960 --> 01:34:07,960
You see it in the fossil industry.

1928
01:34:07,960 --> 01:34:10,960
You see it in the finance industry.

1929
01:34:10,960 --> 01:34:13,960
So my call would be to

1930
01:34:13,960 --> 01:34:16,960
call to arms for

1931
01:34:16,960 --> 01:34:19,960
raising funds

1932
01:34:19,960 --> 01:34:22,960
to putting more money

1933
01:34:22,960 --> 01:34:25,960
in the lobbying effort

1934
01:34:25,960 --> 01:34:29,960
because I think we are very weak there.

1935
01:34:29,960 --> 01:34:32,960
And I think the Microsofts.

1936
01:34:32,960 --> 01:34:35,960
I'm not too sure if a guy like Elon Musk

1937
01:34:35,960 --> 01:34:38,960
is saying that AI is a threat

1938
01:34:38,960 --> 01:34:41,960
when he is, you know, what's he doing.

1939
01:34:41,960 --> 01:34:44,960
So I'm not entirely sure if that's the right...

1940
01:34:44,960 --> 01:34:47,960
Ok, so I agree with you

1941
01:34:47,960 --> 01:34:50,960
on the lobbying front, definitely.

1942
01:34:50,960 --> 01:34:53,960
And what about the international agency for AI?

1943
01:34:53,960 --> 01:34:56,960
It sounds a bit...

1944
01:34:56,960 --> 01:34:59,960
I have to think about it to be honest

1945
01:34:59,960 --> 01:35:02,960
because it sounds like a drastic...

1946
01:35:02,960 --> 01:35:05,960
I don't think we have a red button

1947
01:35:05,960 --> 01:35:08,960
or an international police agency

1948
01:35:08,960 --> 01:35:11,960
that can say stop this.

1949
01:35:11,960 --> 01:35:14,960
I haven't thought about that.

1950
01:35:14,960 --> 01:35:17,960
Maybe it's wrong for a politician not to give an answer

1951
01:35:17,960 --> 01:35:20,960
on the spot directly.

1952
01:35:20,960 --> 01:35:23,960
Sleep over it, yeah.

1953
01:35:23,960 --> 01:35:26,960
I'll ask my AI to...

1954
01:35:26,960 --> 01:35:29,960
Queenie.

1955
01:35:29,960 --> 01:35:32,960
I would like to read, but you're going to send out the e-mail

1956
01:35:32,960 --> 01:35:35,960
so I'm going to read all the seven recommendations.

1957
01:35:35,960 --> 01:35:38,960
I recognize the lobbying part a lot.

1958
01:35:38,960 --> 01:35:41,960
So what I tried from a Dutch perspective,

1959
01:35:41,960 --> 01:35:44,960
when you look at the AI Act, they distinguish

1960
01:35:44,960 --> 01:35:47,960
if an AI is a high risk AI

1961
01:35:47,960 --> 01:35:50,960
or a low risk AI.

1962
01:35:50,960 --> 01:35:53,960
And I'm not sure if I follow those categories

1963
01:35:53,960 --> 01:35:56,960
because I don't think it's about the technology

1964
01:35:56,960 --> 01:35:59,960
but in which context you use them and with which goal.

1965
01:35:59,960 --> 01:36:02,960
And I also try to make some low risk

1966
01:36:02,960 --> 01:36:05,960
AI to try to get them in the higher category

1967
01:36:05,960 --> 01:36:08,960
which is really difficult.

1968
01:36:08,960 --> 01:36:11,960
So I really recognize the lobbying part.

1969
01:36:11,960 --> 01:36:14,960
So maybe it's good also to come together after tonight

1970
01:36:14,960 --> 01:36:17,960
and to see if we can align

1971
01:36:17,960 --> 01:36:20,960
on some topics.

1972
01:36:20,960 --> 01:36:23,960
I wanted to...

1973
01:36:23,960 --> 01:36:26,960
Maybe one thing that can come close to what you're saying

1974
01:36:26,960 --> 01:36:29,960
is de Wetenschappelijke Raad voor Regeringbeleid.

1975
01:36:29,960 --> 01:36:32,960
So that's a group that advises

1976
01:36:32,960 --> 01:36:35,960
the parliament but also the cabinet, the ministers.

1977
01:36:35,960 --> 01:36:38,960
And they said you have to work on AI diplomacy.

1978
01:36:38,960 --> 01:36:41,960
And I think that comes...

1979
01:36:41,960 --> 01:36:44,960
Well, it doesn't have really overruling power

1980
01:36:44,960 --> 01:36:47,960
but it comes really close in making sure that you get treaties,

1981
01:36:47,960 --> 01:36:50,960
make agreements all over the world

1982
01:36:50,960 --> 01:36:53,960
on how to use AI.

1983
01:36:53,960 --> 01:36:56,960
So I think that's a good one and at the same time

1984
01:36:56,960 --> 01:36:59,960
well, if you look at the geopolitical situation right now

1985
01:36:59,960 --> 01:37:02,960
not everyone listens to international treaties.

1986
01:37:02,960 --> 01:37:04,960
But I think it's a good start

1987
01:37:04,960 --> 01:37:07,960
and let's talk about that tomorrow or after more.

1988
01:37:07,960 --> 01:37:09,960
Tim, your two cents.

1989
01:37:09,960 --> 01:37:11,960
Right.

1990
01:37:11,960 --> 01:37:14,960
I don't know a lot about the social technological aspects of this.

1991
01:37:14,960 --> 01:37:17,960
I'm going to maybe focus on the existential risk part

1992
01:37:17,960 --> 01:37:19,960
that I know a bit more about

1993
01:37:19,960 --> 01:37:22,960
because I think Marc already gave a very good summary.

1994
01:37:22,960 --> 01:37:25,960
So one thing we can do is hope it goes right.

1995
01:37:25,960 --> 01:37:28,960
I don't think that has a lot of chance.

1996
01:37:28,960 --> 01:37:32,960
The other thing is we can try to solve this alignment problem

1997
01:37:32,960 --> 01:37:35,960
either by, as Professor Russell suggested,

1998
01:37:35,960 --> 01:37:37,960
finding new paradigms

1999
01:37:37,960 --> 01:37:39,960
or by trying to solve it in a deep learning setting

2000
01:37:39,960 --> 01:37:42,960
which might be very difficult but maybe it's doable.

2001
01:37:42,960 --> 01:37:45,960
I don't particularly have any hope

2002
01:37:45,960 --> 01:37:47,960
in the companies themselves solving this

2003
01:37:47,960 --> 01:37:49,960
and I also feel like if we want to do this

2004
01:37:49,960 --> 01:37:51,960
we need a lot more time.

2005
01:37:51,960 --> 01:37:55,960
And so one way to give us time

2006
01:37:55,960 --> 01:37:57,960
would be to have these kinds of international regulations

2007
01:37:57,960 --> 01:37:59,960
that make sure

2008
01:37:59,960 --> 01:38:02,960
like the open letter suggested, systems like GPT-4

2009
01:38:02,960 --> 01:38:05,960
or stronger systems like that shouldn't be allowed to be trained

2010
01:38:05,960 --> 01:38:08,960
for maybe ever or until we solve alignment

2011
01:38:08,960 --> 01:38:10,960
or six months, I don't know how long it will take.

2012
01:38:10,960 --> 01:38:12,960
And I think it's very telling that

2013
01:38:12,960 --> 01:38:14,960
even the tech industry itself is saying,

2014
01:38:14,960 --> 01:38:17,960
look, world help us

2015
01:38:17,960 --> 01:38:19,960
because we don't know how to do this

2016
01:38:19,960 --> 01:38:21,960
and we need more time to solve this.

2017
01:38:21,960 --> 01:38:23,960
I think maybe we do actually need that kind of drastic action

2018
01:38:23,960 --> 01:38:25,960
because they're not going to do it by themselves.

2019
01:38:25,960 --> 01:38:28,960
Ja, they seem to be open minded in some ways to that.

2020
01:38:28,960 --> 01:38:30,960
Ja, Andy.

2021
01:38:30,960 --> 01:38:34,960
I can only agree what has been said already

2022
01:38:34,960 --> 01:38:36,960
and besides that

2023
01:38:36,960 --> 01:38:39,960
something that was also mentioned in the open letter

2024
01:38:39,960 --> 01:38:43,960
is to call for a research focus shift

2025
01:38:43,960 --> 01:38:46,960
from AI capabilities research

2026
01:38:46,960 --> 01:38:49,960
so making the biggest models even bigger

2027
01:38:49,960 --> 01:38:53,960
and better and smarter to AI safety research

2028
01:38:53,960 --> 01:38:57,960
which is research to ensure the beneficial outcomes

2029
01:38:57,960 --> 01:39:00,960
of these advanced AI models

2030
01:39:00,960 --> 01:39:04,960
and so I think the Dutch government can play a role in that

2031
01:39:04,960 --> 01:39:08,960
as well to advocate for more funding towards that

2032
01:39:08,960 --> 01:39:11,960
and I think the Netherlands is a great place for that as well

2033
01:39:11,960 --> 01:39:13,960
because we have a lot of technical universities

2034
01:39:13,960 --> 01:39:17,960
that are highly internationally regarded.

2035
01:39:17,960 --> 01:39:19,960
So yeah, besides technical research

2036
01:39:19,960 --> 01:39:22,960
we also need more research for AI governance

2037
01:39:22,960 --> 01:39:26,960
so we need more robust and effective governance mechanisms

2038
01:39:26,960 --> 01:39:31,960
en ja, I think we can also play a role in that.

2039
01:39:31,960 --> 01:39:33,960
Oké, the list goes on.

2040
01:39:33,960 --> 01:39:35,960
Some questions from the audience.

2041
01:39:35,960 --> 01:39:38,960
May I see your hands? Ja.

2042
01:39:47,960 --> 01:39:50,960
Queenie inspires me to ask this question

2043
01:39:50,960 --> 01:39:54,960
because you seem to refer to one profession

2044
01:39:54,960 --> 01:39:58,960
or one type of school to take an oath

2045
01:39:58,960 --> 01:40:01,960
but I would take it one step further.

2046
01:40:01,960 --> 01:40:05,960
Why do we still have professions and or schools

2047
01:40:05,960 --> 01:40:09,960
who might be threatening in any way

2048
01:40:09,960 --> 01:40:12,960
without taking an oath?

2049
01:40:12,960 --> 01:40:18,960
Shouldn't this oath be obligatory for many more professions or schools?

2050
01:40:18,960 --> 01:40:21,960
Yes, yes, it should.

2051
01:40:21,960 --> 01:40:24,960
So actually this was not my idea

2052
01:40:24,960 --> 01:40:29,960
but there are two female mathematical teachers

2053
01:40:29,960 --> 01:40:32,960
at I think it was the Delft Technical University

2054
01:40:32,960 --> 01:40:34,960
and they came to me and they said

2055
01:40:34,960 --> 01:40:37,960
hey we are trying to adjust the curriculum

2056
01:40:37,960 --> 01:40:41,960
into making sure that everyone who is going to

2057
01:40:41,960 --> 01:40:44,960
who is going to this technical university

2058
01:40:44,960 --> 01:40:48,960
that ethics should be part of all of the studies

2059
01:40:48,960 --> 01:40:52,960
and they asked well can you help us to give a push

2060
01:40:52,960 --> 01:40:55,960
so actually it was I didn't steal the idea

2061
01:40:55,960 --> 01:40:58,960
but I tried to give them a push from a political part

2062
01:40:58,960 --> 01:41:02,960
and that helped but the goal is not to do it just for AI engineers

2063
01:41:02,960 --> 01:41:05,960
but you know like because you cannot

2064
01:41:05,960 --> 01:41:07,960
this is a big responsibility

2065
01:41:07,960 --> 01:41:10,960
and you shouldn't just put it with just one person

2066
01:41:10,960 --> 01:41:13,960
but you have to make sure that everyone who works in the field

2067
01:41:13,960 --> 01:41:16,960
understands what are their human rights

2068
01:41:16,960 --> 01:41:19,960
how can we make sure that we strengthen them

2069
01:41:19,960 --> 01:41:22,960
instead of threatening them et cetera

2070
01:41:22,960 --> 01:41:26,960
so it's actually the whole system needs to be

2071
01:41:26,960 --> 01:41:28,960
needs to be conscious of that

2072
01:41:28,960 --> 01:41:30,960
because what we all learned

2073
01:41:30,960 --> 01:41:32,960
what we all saw the last couple of years

2074
01:41:32,960 --> 01:41:36,960
is that technical NIT is not just technical

2075
01:41:36,960 --> 01:41:38,960
it's about the way we live our lives

2076
01:41:38,960 --> 01:41:41,960
it's about who earns money

2077
01:41:41,960 --> 01:41:43,960
what information do we see et cetera

2078
01:41:43,960 --> 01:41:46,960
so we need to make sure that ethical standards

2079
01:41:46,960 --> 01:41:49,960
are taking into account when it comes to IT broadly

2080
01:41:49,960 --> 01:41:52,960
because it determines the way we live

2081
01:41:52,960 --> 01:41:55,960
did I answer your question?

2082
01:41:55,960 --> 01:41:57,960
Lamert, sorry?

2083
01:41:57,960 --> 01:42:01,960
I think it's definitely worth pursuing

2084
01:42:01,960 --> 01:42:05,960
and to instill ethical values in

2085
01:42:05,960 --> 01:42:08,960
educational systems or professions

2086
01:42:08,960 --> 01:42:10,960
I think that's a good idea

2087
01:42:10,960 --> 01:42:12,960
although banker oats

2088
01:42:14,960 --> 01:42:16,960
why not?

2089
01:42:16,960 --> 01:42:18,960
we have them

2090
01:42:18,960 --> 01:42:20,960
we saw what happens

2091
01:42:20,960 --> 01:42:22,960
but nevertheless

2092
01:42:22,960 --> 01:42:24,960
when you look at the medical profession

2093
01:42:24,960 --> 01:42:28,960
it took 2000 years to instill the values

2094
01:42:28,960 --> 01:42:31,960
that the oath is meaningful

2095
01:42:31,960 --> 01:42:33,960
so it can work

2096
01:42:33,960 --> 01:42:35,960
but we know from a banker perspective

2097
01:42:35,960 --> 01:42:37,960
it's meaningless

2098
01:42:37,960 --> 01:42:40,960
but I'm sure we can find a balance

2099
01:42:40,960 --> 01:42:42,960
so I think we should pursue it

2100
01:42:42,960 --> 01:42:44,960
maybe a start

2101
01:42:50,960 --> 01:42:52,960
thank you for the interesting discussion

2102
01:42:52,960 --> 01:42:54,960
I was wondering

2103
01:42:54,960 --> 01:42:56,960
because there is a lot of progress going on

2104
01:42:56,960 --> 01:42:58,960
when it comes to AI interpretability

2105
01:42:58,960 --> 01:43:02,960
and making sure that we understand

2106
01:43:02,960 --> 01:43:05,960
what kind of representation deep learning models are forming

2107
01:43:05,960 --> 01:43:08,960
do you think there is any role of AI interpretability

2108
01:43:08,960 --> 01:43:12,960
in making sure that these systems are safe?

2109
01:43:12,960 --> 01:43:15,960
that's maybe a question for Tim I guess

2110
01:43:15,960 --> 01:43:17,960
thanks for the question

2111
01:43:17,960 --> 01:43:20,960
I mean definitely a big part of

2112
01:43:20,960 --> 01:43:23,960
AI alignment research or ASafety more broadly

2113
01:43:23,960 --> 01:43:26,960
should be interpretability for these deep learning systems

2114
01:43:26,960 --> 01:43:29,960
to give us at least some kind of lens

2115
01:43:29,960 --> 01:43:31,960
of looking at these systems

2116
01:43:31,960 --> 01:43:33,960
and maybe understanding a little bit of how they work

2117
01:43:33,960 --> 01:43:35,960
of what they potentially do before they do it

2118
01:43:35,960 --> 01:43:38,960
I think the hope of this field

2119
01:43:38,960 --> 01:43:40,960
might be very difficult

2120
01:43:40,960 --> 01:43:43,960
in the sense that these models are so huge

2121
01:43:43,960 --> 01:43:45,960
and there are so many parameters

2122
01:43:45,960 --> 01:43:47,960
and it's so hard to even understand

2123
01:43:47,960 --> 01:43:49,960
what small parts of it are doing

2124
01:43:49,960 --> 01:43:52,960
like if you look at the field of

2125
01:43:52,960 --> 01:43:54,960
the specific kind of mechanistic interpretability

2126
01:43:54,960 --> 01:43:56,960
they call it right now

2127
01:43:56,960 --> 01:43:58,960
we know tiny little things

2128
01:43:58,960 --> 01:44:00,960
about tiny little parts of the model

2129
01:44:00,960 --> 01:44:02,960
that give us some kind of idea

2130
01:44:02,960 --> 01:44:04,960
it's doing it a little bit like this

2131
01:44:04,960 --> 01:44:06,960
but before we can actually scale that up

2132
01:44:06,960 --> 01:44:08,960
to

2133
01:44:08,960 --> 01:44:10,960
just actually understanding what goes on

2134
01:44:10,960 --> 01:44:12,960
that will take so long

2135
01:44:12,960 --> 01:44:15,960
and I'm not sure how feasible it is

2136
01:44:15,960 --> 01:44:18,960
to use that as the main angle of attack

2137
01:44:18,960 --> 01:44:20,960
I definitely think it's part of it

2138
01:44:20,960 --> 01:44:23,960
but we need a lot of other approaches as well

2139
01:44:23,960 --> 01:44:25,960
part of the solution

2140
01:44:25,960 --> 01:44:27,960
another question there

2141
01:44:27,960 --> 01:44:29,960
yes

2142
01:44:29,960 --> 01:44:31,960
thank you for the diverse perspectives you had

2143
01:44:31,960 --> 01:44:33,960
and I was wondering Mark

2144
01:44:33,960 --> 01:44:35,960
you mentioned law and policy

2145
01:44:35,960 --> 01:44:37,960
as one of the key aspects

2146
01:44:37,960 --> 01:44:39,960
but I think now also with GDPR

2147
01:44:39,960 --> 01:44:41,960
actually the enforcement

2148
01:44:41,960 --> 01:44:43,960
is one of the challenges

2149
01:44:43,960 --> 01:44:45,960
and how would you propose a solution

2150
01:44:45,960 --> 01:44:47,960
specifically for the enforcement

2151
01:44:47,960 --> 01:44:49,960
perhaps is it on a national or European level

2152
01:44:49,960 --> 01:44:51,960
or other further levels as well

2153
01:44:51,960 --> 01:44:53,960
having a law is one thing

2154
01:44:53,960 --> 01:44:55,960
but how do you actually enforce it effectively

2155
01:44:55,960 --> 01:44:58,960
I think the general data protection regulation

2156
01:44:58,960 --> 01:45:00,960
is really interesting in that

2157
01:45:00,960 --> 01:45:02,960
Italy in sheer desperation

2158
01:45:02,960 --> 01:45:04,960
blocked chat GPT last week

2159
01:45:04,960 --> 01:45:06,960
on the base of GDPR

2160
01:45:06,960 --> 01:45:08,960
because there was no AI act

2161
01:45:08,960 --> 01:45:11,960
so I think just harping on about how we need it urgently

2162
01:45:11,960 --> 01:45:13,960
I think people are learning lessons

2163
01:45:13,960 --> 01:45:15,960
from the failures of GDPR

2164
01:45:15,960 --> 01:45:18,960
people realise that the fact that all the big tech companies

2165
01:45:18,960 --> 01:45:20,960
have their headquarters in Dublin, in Ireland

2166
01:45:20,960 --> 01:45:22,960
and the fact that the Irish data protection authority

2167
01:45:22,960 --> 01:45:24,960
is probably the weakest out of all the EU members

2168
01:45:24,960 --> 01:45:27,960
is something that people in Brussels have realised

2169
01:45:27,960 --> 01:45:29,960
so under the AI act

2170
01:45:29,960 --> 01:45:32,960
potentially there will be a centralized office

2171
01:45:32,960 --> 01:45:34,960
so that will help deal with enforcement

2172
01:45:34,960 --> 01:45:36,960
because it means that the European Commission can step in

2173
01:45:36,960 --> 01:45:38,960
when member states do not

2174
01:45:38,960 --> 01:45:41,960
there is also again lobbying

2175
01:45:41,960 --> 01:45:43,960
against this AI office

2176
01:45:43,960 --> 01:45:46,960
and some people are worried about the cost of civil servants

2177
01:45:46,960 --> 01:45:49,960
that the commission would potentially need to hire for this

2178
01:45:49,960 --> 01:45:52,960
we've been arguing that this technology is so transformative

2179
01:45:52,960 --> 01:45:55,960
that it's probably worth a few hundred civil servants

2180
01:45:55,960 --> 01:45:58,960
but it's really a knife edge vote

2181
01:45:58,960 --> 01:46:00,960
I think it's about half the European Parliament

2182
01:46:00,960 --> 01:46:02,960
at the moment that would favour such an office

2183
01:46:02,960 --> 01:46:04,960
and half that oppose it

2184
01:46:04,960 --> 01:46:06,960
and would like to see a GDPR type model

2185
01:46:06,960 --> 01:46:09,960
maybe you have some partners in crime here

2186
01:46:09,960 --> 01:46:12,960
I'm not sure but they could help out

2187
01:46:12,960 --> 01:46:15,960
well exactly what Italy did

2188
01:46:15,960 --> 01:46:18,960
was they took the law that they have on data protection

2189
01:46:18,960 --> 01:46:20,960
and AVG

2190
01:46:20,960 --> 01:46:23,960
en they

2191
01:46:23,960 --> 01:46:26,960
they said let's treat it as a human decision

2192
01:46:26,960 --> 01:46:29,960
and then it fell short

2193
01:46:29,960 --> 01:46:32,960
of the decision process and on that grounds you can

2194
01:46:32,960 --> 01:46:35,960
in fact do enforcement

2195
01:46:35,960 --> 01:46:38,960
it is a bit like using a hammer when you try to

2196
01:46:38,960 --> 01:46:40,960
do a screw

2197
01:46:40,960 --> 01:46:43,960
but it is possible in the area of well wanting for another law

2198
01:46:43,960 --> 01:46:46,960
but that is very feasible

2199
01:46:46,960 --> 01:46:49,960
enforcement instruments are in place

2200
01:46:49,960 --> 01:46:52,960
one question here

2201
01:46:52,960 --> 01:46:55,960
thank you

2202
01:46:55,960 --> 01:46:58,960
when it comes to regulation and policy

2203
01:46:58,960 --> 01:47:01,960
I think the human species has a track record of solving always

2204
01:47:01,960 --> 01:47:04,960
the last crisis

2205
01:47:04,960 --> 01:47:07,960
when it comes to existential risk

2206
01:47:07,960 --> 01:47:10,960
Nick Bostrom also said that we basically have one shot to get this right

2207
01:47:10,960 --> 01:47:13,960
with human track record

2208
01:47:13,960 --> 01:47:16,960
human civilization track record inside

2209
01:47:16,960 --> 01:47:19,960
is that something that should concern us

2210
01:47:19,960 --> 01:47:22,960
only one shot to get this right

2211
01:47:22,960 --> 01:47:25,960
and we rather myopic and focus on short term risk

2212
01:47:25,960 --> 01:47:28,960
who wants to answer, are we optimist here

2213
01:47:28,960 --> 01:47:31,960
I don't think it's a matter of one shot

2214
01:47:31,960 --> 01:47:34,960
to be honest

2215
01:47:34,960 --> 01:47:37,960
you don't think it's one shot

2216
01:47:37,960 --> 01:47:40,960
I like the question but I think

2217
01:47:40,960 --> 01:47:43,960
if I try to think about the presentation that we have

2218
01:47:43,960 --> 01:47:46,960
what I liked about

2219
01:47:46,960 --> 01:47:49,960
we can have several smaller signs

2220
01:47:49,960 --> 01:47:52,960
before we get to total extension

2221
01:47:52,960 --> 01:47:55,960
so that's hopeful for me

2222
01:47:55,960 --> 01:47:58,960
and yes this is worrying

2223
01:47:58,960 --> 01:48:01,960
that's why we are here today

2224
01:48:01,960 --> 01:48:04,960
we have to make sure that more people understand what AI is

2225
01:48:04,960 --> 01:48:07,960
what are the dangers, how can we make sure it works for us all

2226
01:48:07,960 --> 01:48:10,960
what are the good things

2227
01:48:10,960 --> 01:48:13,960
we have a lot of work in society as a whole

2228
01:48:13,960 --> 01:48:16,960
regulation et cetera

2229
01:48:16,960 --> 01:48:19,960
but again for me it's hopeful that

2230
01:48:19,960 --> 01:48:22,960
when I see the difference when the internet started

2231
01:48:22,960 --> 01:48:25,960
and when big tech companies became big

2232
01:48:25,960 --> 01:48:28,960
we were too late when it came to

2233
01:48:28,960 --> 01:48:31,960
regulation of market power et cetera

2234
01:48:31,960 --> 01:48:34,960
and actually that on European level

2235
01:48:34,960 --> 01:48:37,960
that they started thinking about the AI act two years ago

2236
01:48:37,960 --> 01:48:40,960
it's hopeful, it also means that it's two years later now

2237
01:48:40,960 --> 01:48:43,960
so maybe it's not complete

2238
01:48:43,960 --> 01:48:46,960
because technology has been developing

2239
01:48:46,960 --> 01:48:49,960
really quick over the last two years

2240
01:48:49,960 --> 01:48:52,960
but I think that part is hopeful

2241
01:48:52,960 --> 01:48:55,960
that we recognize this problem before it's too late

2242
01:48:55,960 --> 01:48:58,960
and I think it's our responsibility

2243
01:48:58,960 --> 01:49:01,960
to make sure that we prevent that regulation comes too late

2244
01:49:01,960 --> 01:49:04,960
so it's a bit late now

2245
01:49:04,960 --> 01:49:07,960
one last question here

2246
01:49:07,960 --> 01:49:10,960
thanks

2247
01:49:10,960 --> 01:49:13,960
I wanted to make

2248
01:49:13,960 --> 01:49:16,960
a short statement first that

2249
01:49:16,960 --> 01:49:19,960
we were talking about protests

2250
01:49:19,960 --> 01:49:22,960
I think that should happen and I want to organize them

2251
01:49:22,960 --> 01:49:25,960
this year so if

2252
01:49:25,960 --> 01:49:28,960
who's coming

2253
01:49:28,960 --> 01:49:31,960
called safe transition

2254
01:49:31,960 --> 01:49:34,960
safe transition to the machine intelligence era

2255
01:49:34,960 --> 01:49:37,960
but thank you

2256
01:49:37,960 --> 01:49:40,960
my question is to Mark

2257
01:49:40,960 --> 01:49:43,960
and

2258
01:49:43,960 --> 01:49:46,960
it's about the EU regulations

2259
01:49:46,960 --> 01:49:49,960
and

2260
01:49:49,960 --> 01:49:52,960
my current understanding is that

2261
01:49:52,960 --> 01:49:55,960
they are only focusing on deployment

2262
01:49:55,960 --> 01:49:58,960
and not on the training

2263
01:49:58,960 --> 01:50:01,960
so that

2264
01:50:01,960 --> 01:50:04,960
they are in effect not protecting us

2265
01:50:04,960 --> 01:50:07,960
from the existential risk of

2266
01:50:07,960 --> 01:50:10,960
an AI that secretly breaks out

2267
01:50:10,960 --> 01:50:13,960
and goes and

2268
01:50:13,960 --> 01:50:16,960
does its plan to take over the world

2269
01:50:16,960 --> 01:50:19,960
in some other server

2270
01:50:19,960 --> 01:50:22,960
data center so my question is

2271
01:50:22,960 --> 01:50:25,960
is there something in the EU process

2272
01:50:26,960 --> 01:50:29,960
is

2273
01:50:29,960 --> 01:50:32,960
just like the sparks of AGI paper by Microsoft

2274
01:50:32,960 --> 01:50:35,960
the EU AI act I think is a spark of hope

2275
01:50:35,960 --> 01:50:38,960
but you're completely right it's not more than that

2276
01:50:38,960 --> 01:50:41,960
because what they've basically done is they've taken a product safety regulation

2277
01:50:41,960 --> 01:50:44,960
like of any type you have in Europe so basically

2278
01:50:44,960 --> 01:50:47,960
the one that regulates the toy market

2279
01:50:47,960 --> 01:50:50,960
and then they've said okay we'll apply that to AI products

2280
01:50:50,960 --> 01:50:53,960
so it only starts to kick in once

2281
01:50:53,960 --> 01:50:56,960
we want to put it on the market

2282
01:50:56,960 --> 01:50:58,960
so if you're training it, if you're testing it

2283
01:50:58,960 --> 01:51:01,960
it's all fine it's completely unregulated

2284
01:51:01,960 --> 01:51:04,960
and we definitely need rules for that

2285
01:51:04,960 --> 01:51:07,960
I think most AI researchers

2286
01:51:07,960 --> 01:51:10,960
feel that we need to start looking seriously at companies

2287
01:51:10,960 --> 01:51:13,960
that have a huge amount of computational power

2288
01:51:13,960 --> 01:51:16,960
there was someone writing an article

2289
01:51:16,960 --> 01:51:19,960
the former advisor to the UK prime minister on technology

2290
01:51:19,960 --> 01:51:22,960
who observed that open AI by itself

2291
01:51:22,960 --> 01:51:25,960
in California has 25 more sort of GPUs

2292
01:51:25,960 --> 01:51:28,960
than the entire United Kingdom

2293
01:51:28,960 --> 01:51:31,960
so their compute power is about 25 times the size of the UK

2294
01:51:31,960 --> 01:51:34,960
those are the sources of worry

2295
01:51:34,960 --> 01:51:37,960
and I think we need to start regulating

2296
01:51:37,960 --> 01:51:40,960
and inspecting and monitoring and verifying those companies

2297
01:51:40,960 --> 01:51:43,960
before ideally

2298
01:51:43,960 --> 01:51:46,960
I think they've developed their product and there's no way

2299
01:51:46,960 --> 01:51:49,960
we can still change it or make it safer

2300
01:51:49,960 --> 01:51:52,960
we don't want to create risks that maybe something happens before it's deployed

2301
01:51:52,960 --> 01:51:55,960
so I think you're completely right

2302
01:51:55,960 --> 01:51:58,960
we need a bunch of extra regulation

2303
01:51:58,960 --> 01:52:01,960
and we also desperately need the United States

2304
01:52:01,960 --> 01:52:04,960
because that's where most of these companies are based

2305
01:52:04,960 --> 01:52:07,960
and it's super, like it's great that we have European regulation

2306
01:52:07,960 --> 01:52:10,960
but without the US this existential risk is not going to go away

2307
01:52:10,960 --> 01:52:13,960
we need to tackle this problem globally

2308
01:52:13,960 --> 01:52:16,960
if I could add something to that maybe

2309
01:52:16,960 --> 01:52:19,960
yes, I think we should be worried about that

2310
01:52:19,960 --> 01:52:22,960
because there are still these scenarios

2311
01:52:22,960 --> 01:52:25,960
like the one you mentioned

2312
01:52:25,960 --> 01:52:28,960
where we are not protected and we do kind of only have one chance

2313
01:52:28,960 --> 01:52:31,960
and for those kinds of things it is

2314
01:52:31,960 --> 01:52:34,960
I think very important to target the bottlenecks of these kind of systems

2315
01:52:34,960 --> 01:52:37,960
which right now is just the model training

2316
01:52:37,960 --> 01:52:40,960
you need so much more computational power

2317
01:52:40,960 --> 01:52:43,960
to train these models and to deploy them

2318
01:52:43,960 --> 01:52:46,960
the easiest and the most obvious thing to regulate

2319
01:52:46,960 --> 01:52:49,960
of course it might be very hard to regulate in practice

2320
01:52:49,960 --> 01:52:52,960
but if you target that part

2321
01:52:52,960 --> 01:52:55,960
then you actually have a better chance at stopping these kinds of models

2322
01:52:55,960 --> 01:52:58,960
is that part of your seven recommendations

2323
01:52:58,960 --> 01:53:01,960
it's number two

2324
01:53:01,960 --> 01:53:04,960
well that's one of the commitments I already heard

2325
01:53:04,960 --> 01:53:07,960
that you're going to discuss these seven recommendations

2326
01:53:07,960 --> 01:53:10,960
I also heard the idea of trying to

2327
01:53:10,960 --> 01:53:13,960
strengthen AI safety research

2328
01:53:13,960 --> 01:53:16,960
and ethics in different engineering programs

2329
01:53:16,960 --> 01:53:19,960
increasing funding and trying to fight the big lobbying power

2330
01:53:19,960 --> 01:53:22,960
of the tech industry

2331
01:53:22,960 --> 01:53:25,960
so quite some commitments that have already been uttered here on this stage

2332
01:53:25,960 --> 01:53:28,960
we've come to the end of this evening

2333
01:53:28,960 --> 01:53:31,960
but actually it's only the start

2334
01:53:31,960 --> 01:53:34,960
because there are drinks later on

2335
01:53:34,960 --> 01:53:37,960
to continue the conversation

2336
01:53:37,960 --> 01:53:40,960
about all the great catastrophic risks

2337
01:53:40,960 --> 01:53:43,960
that have come to the fore this evening

2338
01:53:43,960 --> 01:53:46,960
or maybe you're very hopeful about humanity

2339
01:53:46,960 --> 01:53:49,960
steering away from the cliff

2340
01:53:49,960 --> 01:53:52,960
doesn't really matter, both are great reasons for a good drink and chat

2341
01:53:52,960 --> 01:53:55,960
so I advise you all to the bar

2342
01:53:55,960 --> 01:53:58,960
that's in the hall

2343
01:53:58,960 --> 01:54:01,960
down the hole there

2344
01:54:01,960 --> 01:54:04,960
I want to give a big round of applause to our five panelists

2345
01:54:07,960 --> 01:54:10,960
applause

2346
01:54:10,960 --> 01:54:13,960
applause

2347
01:54:13,960 --> 01:54:16,960
en some flowers

2348
01:54:16,960 --> 01:54:19,960
we'll probably meet again

2349
01:54:19,960 --> 01:54:22,960
during the demonstration

2350
01:54:22,960 --> 01:54:25,960
when will it take place

2351
01:54:25,960 --> 01:54:28,960
to be decided

2352
01:54:28,960 --> 01:54:31,960
cliffhanger, we'll see each other at the dam square

2353
01:54:31,960 --> 01:54:34,960
somewhere next year

2354
01:54:34,960 --> 01:54:37,960
we hope to see you again

2355
01:54:37,960 --> 01:54:40,960
applause

2356
01:54:40,960 --> 01:54:43,960
applause

2357
01:54:43,960 --> 01:54:46,960
applause

