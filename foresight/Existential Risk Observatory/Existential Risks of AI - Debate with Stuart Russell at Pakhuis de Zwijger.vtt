WEBVTT

00:00.000 --> 00:10.000
Welkom, everybody.

00:10.000 --> 00:18.000
Great to have you all here in a fully packed packhuis de Zwijger.

00:18.000 --> 00:20.000
Mijn naam is Maarten Gehem.

00:20.000 --> 00:27.000
I'm director of the argumentation factory and I have the honour of hosting this evening on existential risks.

00:28.000 --> 00:37.000
De late Stefan Hawken once said success in creating AI could be the biggest event in the history of our civilization.

00:37.000 --> 00:42.000
But it could also be the last unless we learn to avoid the risks.

00:42.000 --> 00:45.000
And that's precisely the topic of today.

00:45.000 --> 00:50.000
And we're going to talk about that with none other than professor Stuart Russell.

00:50.000 --> 00:56.000
I'll properly introduce him later, but first I'll hand over the floor to Otto Barton,

00:56.000 --> 00:59.000
who is the director of the existential risk observatory,

00:59.000 --> 01:02.000
and who is the instigator of this evening.

01:02.000 --> 01:04.000
Otto, come over.

01:04.000 --> 01:09.000
After Otto, Russell will give a talk, then we'll have a Q&A,

01:09.000 --> 01:13.000
and then afterwards we'll have a panel with five distinguished panelists.

01:13.000 --> 01:15.000
We're sitting over there, I'll also introduce you later.

01:15.000 --> 01:18.000
But now, without further ado, Otto.

01:18.000 --> 01:25.000
APPLAUS

01:25.000 --> 01:30.000
Dank je wel, en thank you very much, Maarten, for the introduction.

01:30.000 --> 01:33.000
Ja, I'm super happy that you're all here.

01:33.000 --> 01:35.000
Indeed, my name is Otto Barton.

01:35.000 --> 01:38.000
I'm the founder and the director of the existential risk observatory.

01:38.000 --> 01:43.000
We're an organization aimed at reducing existential risk by informing the public debates.

01:43.000 --> 01:48.000
I'm going to talk a little bit for a few minutes about what existential risk is

01:48.000 --> 01:50.000
and what our organization is doing.

01:52.000 --> 01:56.000
All right, so next slide, please.

01:56.000 --> 01:57.000
Thank you.

01:57.000 --> 02:00.000
So existential risks, what are they?

02:00.000 --> 02:07.000
Basically, as humanity, we've had about 300,000 years now already on this earth,

02:07.000 --> 02:09.000
and we have maybe about five billion to go,

02:09.000 --> 02:12.000
so an enormous amount before the sun explodes.

02:12.000 --> 02:18.000
So the huge majority of our time is still ahead of us,

02:18.000 --> 02:21.000
and an existential risk is something that can threaten that.

02:21.000 --> 02:27.000
So basically, the definition is a risk that threatens the destruction of humanity's long-term potential.

02:27.000 --> 02:31.000
It has been defined by Toby Ork from the Future of Humanity Institutes

02:31.000 --> 02:34.000
and his colleagues in this way.

02:34.000 --> 02:37.000
So this could be in a few ways.

02:37.000 --> 02:40.000
Of course, human extinction is a permanent state.

02:40.000 --> 02:46.000
So human extinction is one way in which we cannot have a future left anymore.

02:46.000 --> 02:49.000
So these five billion years, there won't be any value in that.

02:49.000 --> 02:53.000
An unrecoverable collapse or dystopian log-in are two other ways

02:53.000 --> 02:58.000
in which we could, which are contained in existential risk definition.

02:58.000 --> 03:03.000
So on the graph to the right, you see a rough estimate by Toby Ork,

03:03.000 --> 03:06.000
this researcher from the Future of Humanity Institute,

03:06.000 --> 03:11.000
on what causes could be for existential risks.

03:11.000 --> 03:13.000
So there are natural causes.

03:13.000 --> 03:16.000
There might be an asteroid strike, there might be a super volcano,

03:16.000 --> 03:19.000
but these are tiny and very well-known.

03:19.000 --> 03:21.000
So not the most interesting ones.

03:21.000 --> 03:25.000
To the left of that, you see a nuclear war and climate change,

03:25.000 --> 03:28.000
which are already somewhat bigger.

03:28.000 --> 03:34.000
But you can see that these are still fairly small compared to other existential risks.

03:34.000 --> 03:38.000
That climate change has a small chance of leading to human extinction.

03:38.000 --> 03:40.000
It doesn't mean that it's not a big problem.

03:40.000 --> 03:44.000
Of course, the chance that climate change will occur is 100% basically.

03:44.000 --> 03:46.000
And it is a very big issue.

03:46.000 --> 03:49.000
However, the chance that it leads to complete human extinction is relatively small,

03:49.000 --> 03:51.000
which is why you see a small bar here.

03:51.000 --> 03:55.000
Nuclear war, perhaps a little bit of a similar story,

03:55.000 --> 03:59.000
the chance that it occurs in the next hundred years is not that tiny.

03:59.000 --> 04:02.000
But the chance that it leads to human extinction is fairly small.

04:02.000 --> 04:07.000
To the left, you see even bigger chances of human extinction

04:07.000 --> 04:10.000
or the other existential risk categories.

04:10.000 --> 04:14.000
These are, for example, the man-made pandemics.

04:14.000 --> 04:19.000
The pandemics bar here is actually for man-made pandemics.

04:19.000 --> 04:23.000
A natural pandemic is also very unlikely to lead to human extinction.

04:23.000 --> 04:27.000
But a man-made pandemic with the biotechnology that we have developed right now

04:27.000 --> 04:30.000
and that we are still developing and democratizing.

04:30.000 --> 04:35.000
The chance that this could lead to human extinction in the next hundred years is non-negligible.

04:35.000 --> 04:40.000
To be orged and most of the other existential risk researchers

04:40.000 --> 04:43.000
think that it's unaligned AI, so artificial intelligence

04:43.000 --> 04:47.000
that has human level or even beyond human level, superhuman level.

04:47.000 --> 04:52.000
But it's unaligned, so it has different values than ours.

04:52.000 --> 04:56.000
This could be a relatively large chance of human extinction.

04:56.000 --> 05:00.000
We're going to talk more about it later, but I'll just leave it here for now.

05:00.000 --> 05:05.000
What else do we see, the total existential risk in the next hundred years

05:05.000 --> 05:09.000
is about a one in six estimate.

05:09.000 --> 05:11.000
There's a lot to be said about these estimates,

05:11.000 --> 05:15.000
but you can still draw a couple of robust conclusions, I think, from them.

05:15.000 --> 05:18.000
That a very likely source is new technology.

05:18.000 --> 05:22.000
And also that technology is man-made, so risk could be reduced in principle.

05:27.000 --> 05:29.000
Next slide, please.

05:29.000 --> 05:33.000
Solution directions for AI existential risk.

05:33.000 --> 05:36.000
These kind of also carry over for other technologies,

05:36.000 --> 05:39.000
but very broadly you could say

05:39.000 --> 05:42.000
if you don't want something to go very wrong with technology,

05:42.000 --> 05:45.000
you can either develop it safely or you cannot develop it.

05:45.000 --> 05:50.000
So basically for AI, this is built AGI safely or AI safety.

05:50.000 --> 05:54.000
So this is done by people who try to focus on AI alignment,

05:54.000 --> 05:57.000
trying to make AGI align to our values.

05:57.000 --> 06:00.000
We think, as an existential risk observatory,

06:00.000 --> 06:03.000
that's an important line of research and it should be scaled up.

06:03.000 --> 06:06.000
But on the other hand, it hasn't worked so far.

06:06.000 --> 06:09.000
People are already working on this for perhaps a few decades.

06:09.000 --> 06:13.000
And so far the consensus is that AI alignment,

06:13.000 --> 06:17.000
more or less the consensus is that AI alignment hasn't been successful yet.

06:17.000 --> 06:20.000
So another option could be to not build AGI

06:20.000 --> 06:23.000
and we think there might be some kind of regulation necessary for that.

06:23.000 --> 06:26.000
So this could be a software regulation, a data regulation

06:26.000 --> 06:29.000
or perhaps a hardware regulation.

06:29.000 --> 06:32.000
And we think these are all options that should be investigated.

06:32.000 --> 06:36.000
But we do think that regulation, whatever is the form it takes,

06:36.000 --> 06:41.000
will require widespread awareness and global cooperation.

06:41.000 --> 06:45.000
So for that, our solution is to inform the societal debate.

06:45.000 --> 06:47.000
So as an existential risk observatory,

06:47.000 --> 06:50.000
a small non-profit organization based in Amsterdam,

06:50.000 --> 06:55.000
we are focusing on informing the society about existential risk.

06:55.000 --> 06:58.000
So we do that by publishing articles in traditional media,

06:58.000 --> 07:01.000
for example in Time Magazine a few weeks ago

07:01.000 --> 07:04.000
and by organizing events such as this debate.

07:04.000 --> 07:07.000
And we also provide input to policy makers

07:07.000 --> 07:11.000
and I think it's a really nice sign that emotion was accepted

07:11.000 --> 07:14.000
by Dutch parliament a few weeks ago

07:14.000 --> 07:18.000
that is calling for more AI safety research in the Netherlands.

07:20.000 --> 07:25.000
So with that, I'm just going to end this small introduction talk

07:25.000 --> 07:29.000
and we're now going to watch a documentary

07:29.000 --> 07:32.000
which is already giving you a little bit of a flavor

07:32.000 --> 07:34.000
of the next speaker Stuart Russell.

07:34.000 --> 07:37.000
And I hope that you enjoyed a few minutes of documentary

07:37.000 --> 07:40.000
and I wish you a great rest of the evening.

07:40.000 --> 07:42.000
Thank you very much.

07:51.000 --> 07:54.000
Everything we have is a result of our intelligence.

07:54.000 --> 07:57.000
It's not the result of our big scary teeth

07:57.000 --> 08:00.000
or our large claws or our enormous muscles.

08:00.000 --> 08:03.000
It's because we're actually relatively intelligent.

08:03.000 --> 08:07.000
And among my generation, we're all having what we call holy cow

08:07.000 --> 08:09.000
or holy something else moments

08:09.000 --> 08:14.000
because we see that the technology is accelerating faster than we expected.

08:14.000 --> 08:17.000
I remember sitting around the table there

08:17.000 --> 08:20.000
with some of the best and the smartest minds in the world

08:20.000 --> 08:22.000
and what really struck me was

08:22.000 --> 08:26.000
maybe the human brain is not able to fully grasp

08:26.000 --> 08:30.000
the complexity of the world that we're confronted with.

08:30.000 --> 08:32.000
As it's currently constructed,

08:32.000 --> 08:35.000
the road that AI is following heads off a cliff

08:35.000 --> 08:38.000
and we need to change the direction that we're going

08:38.000 --> 08:41.000
so that we don't take the human race off the cliff.

08:41.000 --> 08:47.000
This is from the Deep Mind Reinforcement Learning System.

08:47.000 --> 08:50.000
Basically wakes up like a newborn baby

08:50.000 --> 08:53.000
and is shown the screen of an Atari video game

08:53.000 --> 08:56.000
and then has to learn to play the video game.

08:56.000 --> 09:02.000
It knows nothing about objects, about motion, about time.

09:02.000 --> 09:07.000
It only knows that there's an image on the screen and there's a score.

09:07.000 --> 09:11.000
So if your baby woke up the day it was born

09:11.000 --> 09:16.000
and by late afternoon was playing 40 different Atari video games

09:16.000 --> 09:18.000
at a superhuman level,

09:18.000 --> 09:20.000
you would be terrified.

09:20.000 --> 09:24.000
You would say my baby is possessed, send it back.

09:24.000 --> 09:27.000
The Deep Mind System can win at any game.

09:27.000 --> 09:32.000
It can already beat all the original Atari games.

09:32.000 --> 09:34.000
It is superhuman.

09:35.000 --> 09:36.000
It is superhuman.

09:36.000 --> 09:39.000
It plays the games at super speed in less than a minute.

09:44.000 --> 09:46.000
Deep Mind turned to another challenge

09:46.000 --> 09:48.000
and the challenge was the game of Go

09:48.000 --> 09:50.000
which people have generally argued

09:50.000 --> 09:52.000
has been beyond the power of computers

09:52.000 --> 09:55.000
to play with the best human Go players.

09:55.000 --> 09:58.000
First they challenged the European Go Champion.

10:00.000 --> 10:03.000
Then they challenged a Korean Go Champion.

10:05.000 --> 10:07.000
En they were able to win both times

10:07.000 --> 10:09.000
in a kind of striking fashion.

10:10.000 --> 10:12.000
You were reading articles in New York Times years ago

10:12.000 --> 10:15.000
talking about how Go would take a hundred years

10:15.000 --> 10:17.000
for us to solve.

10:17.000 --> 10:20.000
People said, well, you know, but that's still just a board.

10:20.000 --> 10:22.000
Poker is an art.

10:22.000 --> 10:23.000
Poker involves reading people.

10:23.000 --> 10:25.000
Poker involves lying, bluffing.

10:25.000 --> 10:26.000
It's not an exact thing.

10:26.000 --> 10:28.000
That will never be a computer.

10:28.000 --> 10:29.000
You can't do that.

10:29.000 --> 10:31.000
They took the best poker players in the world

10:31.000 --> 10:34.000
en took seven days for the computer

10:34.000 --> 10:37.000
to start demolishing the humans.

10:37.000 --> 10:39.000
So it's the best poker player in the world.

10:39.000 --> 10:40.000
It's the best Go player in the world.

10:40.000 --> 10:44.000
And the pattern here is that AI might take a little while

10:44.000 --> 10:47.000
to wrap its tentacles around a new skill.

10:47.000 --> 10:52.000
But when it does, when it gets it, it is unstoppable.

10:52.000 --> 10:59.000
MUZIEK

10:59.000 --> 11:02.000
DeepMind's AI has administrator-level access

11:02.000 --> 11:04.000
to Google's servers

11:04.000 --> 11:07.000
to optimize energy usage at the data centers.

11:07.000 --> 11:11.000
However, this could be an unintentional trojan horse.

11:11.000 --> 11:14.000
DeepMind has to have complete control of the data centers.

11:14.000 --> 11:16.000
So with a little software update,

11:16.000 --> 11:19.000
that AI could take complete control of the whole Google system,

11:19.000 --> 11:21.000
which means they can do anything.

11:21.000 --> 11:23.000
They can look at all your data and do anything.

11:23.000 --> 11:28.000
MUZIEK

11:28.000 --> 11:30.000
We were rapidly headed towards digital superintelligence

11:30.000 --> 11:32.000
that far exceeds any human.

11:32.000 --> 11:34.000
I think it's very obvious.

11:34.000 --> 11:37.000
The problem is we're not going to suddenly hit human-level intelligence

11:37.000 --> 11:40.000
and say, OK, let's stop research.

11:40.000 --> 11:42.000
It's going to go beyond human-level intelligence

11:42.000 --> 11:44.000
into what's called superintelligence

11:44.000 --> 11:46.000
and that's anything smarter than us.

11:46.000 --> 11:50.000
AI at the superhuman level, if we succeed with that,

11:50.000 --> 11:54.000
is by far the most powerful invention we've ever made

11:54.000 --> 11:57.000
and the last invention we ever have to make.

11:57.000 --> 12:00.000
And if we create AI that's smarter than us,

12:00.000 --> 12:02.000
we have to be open to the possibility

12:02.000 --> 12:05.000
that we might actually lose control of them.

12:05.000 --> 12:08.000
MUZIEK

12:08.000 --> 12:11.000
Let's say you give it some objective like curing cancer

12:11.000 --> 12:14.000
and then you discover that the way it chooses to go about that

12:14.000 --> 12:19.000
is actually in conflict with a lot of other things you care about.

12:19.000 --> 12:23.000
AI doesn't have to be able to destroy humanity.

12:23.000 --> 12:27.000
If AI has a goal and humanity just happens to be in the way,

12:27.000 --> 12:30.000
it will destroy humanity as a matter of course.

12:30.000 --> 12:32.000
Without even thinking about it, no hard feelings.

12:32.000 --> 12:34.000
It's just like if we're building a road

12:34.000 --> 12:36.000
and an ant hill happens to be in the way,

12:36.000 --> 12:40.000
we don't hate ants, we're just building a road

12:40.000 --> 12:42.000
and so goodbye, Ant Hill.

12:42.000 --> 12:45.000
MUZIEK

12:49.000 --> 12:53.000
OK, if you weren't scared already.

12:53.000 --> 12:59.000
Make sure humanity doesn't run off a cliff.

12:59.000 --> 13:01.000
Stuart Russell said.

13:01.000 --> 13:04.000
So who better to tell us how not to run off a cliff

13:04.000 --> 13:06.000
than Professor Russell himself.

13:06.000 --> 13:08.000
And that's precisely what we're going to hear.

13:08.000 --> 13:14.000
Professor Russell is one of the leading experts

13:14.000 --> 13:17.000
in AI research and AI safety research.

13:17.000 --> 13:20.000
He's based at the University of California, Berkeley.

13:20.000 --> 13:26.000
He's one of the writers, co-author of the standard textbook in AI research.

13:26.000 --> 13:29.000
AI, a modern approach.

13:29.000 --> 13:33.000
And recently he wrote a magnificent book called Human Alignment.

13:33.000 --> 13:35.000
I don't know who read the book already.

13:35.000 --> 13:37.000
Let me see some hands here.

13:37.000 --> 13:39.000
OK, all right.

13:39.000 --> 13:41.000
Well, it's well worth the effort.

13:41.000 --> 13:45.000
And he'll probably tell you why in the next 20 minutes.

13:45.000 --> 13:50.000
Professor Russell is beamed to us all over the world,

13:50.000 --> 13:54.000
from all across the world, from California where he's based right now.

13:54.000 --> 13:58.000
So we're going to see him on the screen in a minute or two.

13:58.000 --> 14:02.000
And afterwards there's ample room for questions and answers.

14:02.000 --> 14:07.000
So we'll have some room here for a discussion with Mr. Russell himself.

14:07.000 --> 14:09.000
And there he is.

14:09.000 --> 14:14.000
Professor Russell, the floor is yours.

14:14.000 --> 14:17.000
Hey there, thank you very much.

14:17.000 --> 14:20.000
So I should just make a slight correction.

14:20.000 --> 14:23.000
I'm not in California, I'm actually at MIT.

14:23.000 --> 14:29.000
But I'm on my way home to California later on this evening.

14:29.000 --> 14:33.000
So I think the little movie that you just saw

14:33.000 --> 14:37.000
actually brings up a lot of important points.

14:37.000 --> 14:40.000
So I don't have to repeat them.

14:40.000 --> 14:44.000
But I will give you a short presentation,

14:44.000 --> 14:47.000
which in some ways brings it up to date.

14:47.000 --> 14:51.000
So let's say a little bit about what we're doing to help

14:51.000 --> 14:55.000
and about the current situation.

14:55.000 --> 14:58.000
So together everyone on the same page.

14:58.000 --> 15:00.000
What is AI?

15:00.000 --> 15:02.000
It's not a particular technology.

15:02.000 --> 15:07.000
It's a task just like the task of physics is to understand the universe.

15:07.000 --> 15:11.000
The task of AI is to make intelligent machines.

15:11.000 --> 15:14.000
And then the question is, well, what does that mean?

15:14.000 --> 15:16.000
What does it mean for a machine to be intelligent?

15:16.000 --> 15:19.000
And for most of the history of AI, it's meant the following.

15:19.000 --> 15:22.000
Machines are intelligent to the extent that their actions

15:22.000 --> 15:26.000
can be expected to achieve their objectives.

15:26.000 --> 15:29.000
And this is so pervasive, I'll call it the standard model.

15:29.000 --> 15:34.000
And many forms of AI, problem solving, planning, reinforcement learning,

15:34.000 --> 15:37.000
or conform to the standard model,

15:37.000 --> 15:40.000
as well as many other disciplines like control theory

15:40.000 --> 15:42.000
and operations research and economics.

15:42.000 --> 15:48.000
You create optimizing machinery and then you specify some objective.

15:48.000 --> 15:53.000
You put that into the machinery and then it becomes the objective of the machine.

15:53.000 --> 15:57.000
And then it finds ways to fulfill that objective.

15:57.000 --> 16:00.000
It's a very natural way to go about doing things.

16:00.000 --> 16:03.000
Later on, I'll argue that it's completely wrong.

16:03.000 --> 16:07.000
But for now, take that as the standard model of AI.

16:07.000 --> 16:13.000
And since the beginning, we've been looking at what we might call general purpose AI.

16:13.000 --> 16:20.000
So not just an AI designed to achieve some specific objective,

16:20.000 --> 16:25.000
but actually one that's capable of achieving more or less any objective that we might give it.

16:25.000 --> 16:32.000
And learning to do that very quickly at a level that exceeds human capabilities

16:32.000 --> 16:36.000
eventually in every dimension.

16:36.000 --> 16:38.000
So that's the goal.

16:38.000 --> 16:44.000
And rather than be accused of always talking about doom,

16:44.000 --> 16:48.000
I'll begin by talking about the upside.

16:48.000 --> 16:53.000
And it's really the upside that explains why the field exists in the first place

16:53.000 --> 16:57.000
and why people are investing lots of money in it

16:57.000 --> 16:59.000
and why lots of smart people are working on it.

17:00.000 --> 17:03.000
Because the potential upside is really enormous.

17:03.000 --> 17:05.000
For example, if you had general purpose AI,

17:05.000 --> 17:11.000
then you could do by definition what humans already know how to do,

17:11.000 --> 17:14.000
which is to deliver, among other things,

17:14.000 --> 17:20.000
to deliver a good standard of living to maybe hundreds of millions

17:20.000 --> 17:22.000
or maybe close to a billion people on Earth

17:22.000 --> 17:26.000
have what we might call a good standard of living.

17:26.000 --> 17:33.000
En we could deliver it actually on much greater scale at much lower cost

17:33.000 --> 17:37.000
because the cost involved in delivering a standard of living

17:37.000 --> 17:41.000
is the expensive time of other human beings.

17:41.000 --> 17:46.000
So if we have general purpose AI, we could, for example,

17:46.000 --> 17:51.000
use it to give everyone on Earth that same respectable standard of living

17:51.000 --> 17:55.000
that we might see in some developed countries.

17:56.000 --> 18:00.000
En if you calculate the sort of economic value of that,

18:00.000 --> 18:03.000
it's about a tenfold increase in GDP

18:03.000 --> 18:06.000
and that converts to what economists call the net present value.

18:06.000 --> 18:08.000
So that's sort of what's the cash equivalent

18:08.000 --> 18:11.000
of having that increased income stream.

18:11.000 --> 18:15.000
So it comes to about $13.5 quadrillion.

18:15.000 --> 18:19.000
So that's a lower bound, a low ball estimate

18:19.000 --> 18:24.000
on the cash value of general purpose AI as a technology.

18:24.000 --> 18:27.000
We could of course have many more things besides that.

18:27.000 --> 18:30.000
I think we could have much better,

18:30.000 --> 18:34.000
more individualized ongoing healthcare.

18:34.000 --> 18:37.000
We could have very personalized

18:37.000 --> 18:41.000
and very, very effective education for every child on Earth.

18:41.000 --> 18:45.000
We could speed up the rate of scientific progress

18:45.000 --> 18:48.000
and perhaps many other things.

18:48.000 --> 18:52.000
I used to have advances in politics on that slide,

18:52.000 --> 18:55.000
but I took it off for obvious reasons.

18:55.000 --> 18:59.000
So now the question is, well, where are we?

18:59.000 --> 19:03.000
A lot of people seem to be saying that we're already there,

19:03.000 --> 19:07.000
that we've already created general purpose AI.

19:07.000 --> 19:11.000
And I think this is not true.

19:11.000 --> 19:14.000
I think there's something going on,

19:14.000 --> 19:18.000
but we are still far away from general purpose AI.

19:18.000 --> 19:21.000
And what's going on, of course, is large language models.

19:21.000 --> 19:26.000
The chat GPT, GPT4, BARD, Lambda, Palm,

19:26.000 --> 19:31.000
all these models are displaying very intriguing

19:31.000 --> 19:34.000
and in some cases very impressive behaviors.

19:34.000 --> 19:38.000
And I think they are probably a piece of the puzzle

19:38.000 --> 19:39.000
of general purpose AI,

19:39.000 --> 19:42.000
but they are not by themselves general purpose AI.

19:42.000 --> 19:44.000
And at the moment, I would say,

19:44.000 --> 19:47.000
we don't know what shape this puzzle piece is

19:47.000 --> 19:50.000
and we don't know how to fit it into the puzzle.

19:50.000 --> 19:53.000
We're not really sure what the other pieces are.

19:53.000 --> 19:57.000
I think one of the things we're learning now

19:57.000 --> 19:59.000
is that the pieces of this puzzle

19:59.000 --> 20:02.000
are probably not the pieces that we thought

20:02.000 --> 20:07.000
made up the puzzle maybe 15 or 20 years ago.

20:07.000 --> 20:10.000
So just to illustrate a few reasons

20:10.000 --> 20:15.000
why I don't think these systems are the solution,

20:15.000 --> 20:16.000
they're not general purpose AI.

20:16.000 --> 20:19.000
So here's a simple example from chat GPT

20:19.000 --> 20:22.000
to me by my friend Prasad Tattapalli.

20:22.000 --> 20:23.000
So the first question,

20:23.000 --> 20:25.000
which is bigger an elephant or a cat,

20:25.000 --> 20:28.000
and it answers an elephant is bigger than a cat.

20:28.000 --> 20:30.000
So far so good.

20:30.000 --> 20:33.000
Which is not bigger than the other, an elephant or a cat.

20:33.000 --> 20:35.000
And it says neither an elephant nor a cat

20:35.000 --> 20:38.000
is bigger than the other.

20:38.000 --> 20:41.000
So these are two consecutive sentences

20:41.000 --> 20:43.000
that he asked it.

20:43.000 --> 20:48.000
And it seems clear from this that in a real sense

20:48.000 --> 20:51.000
chat GPT doesn't know facts.

20:51.000 --> 20:54.000
So when you ask a human a question,

20:54.000 --> 20:56.000
at least our impression of what happens

20:56.000 --> 21:00.000
is that we refer to an internal world model

21:00.000 --> 21:02.000
that is self consistent,

21:02.000 --> 21:07.000
that's composed of facts that we understand about the world.

21:07.000 --> 21:09.000
And then we ask in a question

21:09.000 --> 21:11.000
relative to that internal world model,

21:11.000 --> 21:13.000
we find out what the answer is

21:13.000 --> 21:17.000
and we express the answer in natural language.

21:17.000 --> 21:19.000
In natural language as the answer to the question.

21:19.000 --> 21:21.000
But that clearly can't be what's going on

21:21.000 --> 21:24.000
in at least in this example

21:24.000 --> 21:27.000
because you could not have an internal world model

21:27.000 --> 21:29.000
that contradicted itself

21:29.000 --> 21:31.000
in which the elephants are both bigger than cats

21:31.000 --> 21:33.000
and not bigger than cats.

21:33.000 --> 21:35.000
So in a real sense,

21:35.000 --> 21:37.000
I think we could say that there's evidence

21:37.000 --> 21:39.000
that these systems do not know things

21:39.000 --> 21:43.000
in the way that word is usually used.

21:43.000 --> 21:45.000
I also want to point out,

21:45.000 --> 21:49.000
in the movie you just saw that several years ago

21:49.000 --> 21:53.000
we defeated the best human go players.

21:53.000 --> 21:55.000
In fact, when that happened

21:55.000 --> 21:58.000
to the Chinese world champion in 2017

21:58.000 --> 22:00.000
that was called China's Sputnik moment.

22:00.000 --> 22:04.000
That event precipitated a total change

22:04.000 --> 22:07.000
in Chinese government policy around AI

22:07.000 --> 22:10.000
and the commitment of hundreds of billions of dollars

22:10.000 --> 22:12.000
worth of investment.

22:12.000 --> 22:15.000
The commitments to train hundreds of thousands

22:15.000 --> 22:19.000
of AI researchers, et cetera, et cetera.

22:19.000 --> 22:26.000
We decided to see how good the go programs really are.

22:26.000 --> 22:29.000
We played one of our team members,

22:29.000 --> 22:33.000
Kellan Pelrin, is a reasonably good amateur go player.

22:33.000 --> 22:36.000
His rating is about 2,300.

22:36.000 --> 22:41.000
On that scale, the human world champion is about 3,800.

22:41.000 --> 22:45.000
The go programs are far ahead of human beings now.

22:45.000 --> 22:48.000
In 2017, or 2016,

22:48.000 --> 22:50.000
they were about the level of the human world champion,

22:50.000 --> 22:52.000
so around 3,800.

22:52.000 --> 22:55.000
Now they've reached around 5,200.

22:55.000 --> 22:59.000
JBX Kata 005 is the name of the current number one

22:59.000 --> 23:02.000
go playing program in the world.

23:02.000 --> 23:08.000
Its rating is 1,400 points higher than any human player.

23:08.000 --> 23:13.000
Kellan had been playing against this program

23:13.000 --> 23:16.000
and had beaten it 14 times in a row

23:16.000 --> 23:20.000
and then decided to give it a nine stone handicap.

23:20.000 --> 23:23.000
That means that black, the computer,

23:23.000 --> 23:25.000
starts with nine stones on the board,

23:25.000 --> 23:28.000
as we're showing here, which is an enormous advantage.

23:28.000 --> 23:33.000
This is the kind of handicap that you give to a small child

23:33.000 --> 23:37.000
who's learning the game if you're a go teacher,

23:37.000 --> 23:41.000
just so that the child feels they have a chance.

23:41.000 --> 23:45.000
Now I'll show you what happens in the game.

23:45.000 --> 23:48.000
Remember, the computer is black.

23:48.000 --> 23:50.000
Kellan, the human, is white.

23:50.000 --> 23:53.000
It doesn't really matter if you don't understand go.

23:53.000 --> 23:56.000
Basically, you're trying to surround territory with your pieces

23:56.000 --> 23:59.000
and to surround your opponent's pieces and capture them.

23:59.000 --> 24:03.000
Notice what's happening in the bottom right corner of the board.

24:03.000 --> 24:06.000
The white is making a little group.

24:06.000 --> 24:09.000
It sort of has a kind of a figure 80 sort of shape.

24:09.000 --> 24:12.000
And then black immediately starts to surround that group

24:12.000 --> 24:15.000
in order to prevent it from capturing more territory.

24:15.000 --> 24:18.000
And now white starts to surround the black group

24:18.000 --> 24:21.000
so that this larger white circle is forming.

24:21.000 --> 24:23.000
So it's kind of a circular sandwich.

24:23.000 --> 24:25.000
There's a white piece in the middle

24:25.000 --> 24:27.000
and there's a white thing around the outside

24:27.000 --> 24:30.000
and it's sandwiching in that black group.

24:30.000 --> 24:32.000
In the end, there's no attention

24:32.000 --> 24:34.000
and then loses all of those pieces.

24:34.000 --> 24:40.000
So what's going on here seems to be

24:40.000 --> 24:42.000
that these super human go programs

24:42.000 --> 24:45.000
actually have not correctly learned

24:45.000 --> 24:47.000
what it means to be a group of stones,

24:47.000 --> 24:49.000
what it means to be alive or dead

24:49.000 --> 24:52.000
which are the most basic concepts in go.

24:52.000 --> 24:57.000
And that allows Kellan, the human to defeat these programs

24:57.000 --> 25:00.520
met alle voordelingsprogramma's, die worden geschreven door verschillende mensen

25:00.520 --> 25:05.400
met verschillende trainingsregime's en verschillende netwerkstructuren en zo en zo.

25:05.400 --> 25:10.440
Ze voelen allemaal in dezelfde manier, die is echt remarkably.

25:10.440 --> 25:14.720
En ik denk dat het eigenlijk een consequentie is van de fact dat ze proberen te trainen

25:14.720 --> 25:21.640
circuitten om concepten te representeren zoals connectiviteit en surroundering,

25:21.640 --> 25:26.840
die eigenlijk niet mogelijk kunnen representeren correct via circuitten.

25:26.840 --> 25:31.720
Je kunt alleen een soort van patchy, fragmentair, finite approximatie

25:31.720 --> 25:36.840
te die concepten, maar met een generele programma, zoals Python,

25:36.840 --> 25:40.840
het is heel makkelijk om die concepten correct te representeren.

25:40.840 --> 25:43.560
Dus dit is een fundamentele limitering,

25:43.560 --> 25:52.280
alstublieft als we het uitzenden, met diep leren als een manier te leren over de wereld.

25:52.360 --> 25:55.960
Oké, dus ik denk, in mijn gevoel, dat we nog steeds een manier te gaan

25:55.960 --> 26:01.080
voor generalpurpos A.I. en ik heb een aantal van de dingen

26:01.080 --> 26:03.960
ik denk dat je hier gevoel hebt, waarschijnlijk de derde,

26:03.960 --> 26:09.320
onze behoorlijkheid om niet alleen te bekijken, waarin de go-programma's

26:09.320 --> 26:11.960
wel kunnen doen, zelfs als ze missen

26:13.320 --> 26:16.840
over de kwaliteit van de positie die ze reachen,

26:16.920 --> 26:21.560
zijn we zeker kunnen planen voor 50 of 60 of even 100 behoorlijkheid

26:21.560 --> 26:25.640
naar de toekomst, maar mensen planen op veel levens van de extractie.

26:25.640 --> 26:31.000
We planen over tijdscalingen van jaren en ook over tijdscalingen

26:31.000 --> 26:33.400
van milliseconden en op elke tijdscaling in de tweede.

26:34.280 --> 26:39.320
En als je op een phd maakt, bijvoorbeeld, dat gaat over een trillion

26:39.320 --> 26:46.440
motocontrole acties, en niet alleen maar 50 motocontrole acties,

26:46.520 --> 26:51.720
dus we kunnen in het universen, het heel complex universen,

26:51.720 --> 26:55.320
door onze behoorlijkheid om op deze verschillende levens van de extractie te opereren.

26:56.040 --> 26:59.880
En dat is iets dat nog wel duidelijk onderzoek in A.I.

27:00.840 --> 27:02.920
Dus ik denk dat het nog wel gelijkgemaakt is,

27:02.920 --> 27:05.560
omdat de manier van momentum, investering,

27:07.880 --> 27:12.760
van geniën die dit gebouw wordt geplaatst, dat deze voorkomsten ergens gebeuren.

27:12.760 --> 27:14.280
Het is gewoon heel hard te predicteren

27:14.920 --> 27:16.040
wanneer ze gaan gebeuren.

27:19.000 --> 27:23.000
En om een voorbeeld te geven van hoe hard het is om te predicteren

27:23.000 --> 27:26.680
wanneer deze dingen gaan gebeuren, kunnen we in historie kijken

27:26.680 --> 27:30.200
tot de laatste keer dat we een civilisatie-endende technologie ontvangen,

27:30.760 --> 27:32.920
waarom het automatische energie was.

27:33.960 --> 27:39.880
En we weten sinds 1905, en speciale relativiteit,

27:39.880 --> 27:43.240
dat er een enorm amount van energie in atomen loopt.

27:43.320 --> 27:47.080
En als je er tussen verschillende typen van atomen kan veranderen,

27:47.080 --> 27:48.280
kan je dat energie veranderen.

27:48.840 --> 27:51.560
Maar de fysieke ontdekking hier,

27:51.560 --> 27:53.160
persoonlijke door Lord Rutherford,

27:53.160 --> 27:56.600
de ledende nucleofysicist, geloofde dat dat mogelijk was.

27:57.320 --> 28:00.840
Hij was gevraagd op een meet in september 11, 1933,

28:00.840 --> 28:03.000
dacht je dat in 25 of 30 jaar tijd,

28:03.000 --> 28:06.520
we zouden kunnen vinden een manier om deze energie te ontvangen.

28:06.520 --> 28:09.320
En hij zei dat iemand die voor een verhaal van poder

28:09.320 --> 28:12.120
in de transformatie van de atomen, een moeenscheid is.

28:13.880 --> 28:17.400
En de volgende morning, Leo Zillard,

28:17.400 --> 28:19.640
die een Hungarian fysiast was,

28:19.640 --> 28:21.160
die van Hungary had afgesloten,

28:22.360 --> 28:24.600
en was in London in de tijd,

28:25.320 --> 28:26.760
reed dit in de newspaper,

28:26.760 --> 28:27.720
en hij ging voor een reis,

28:27.720 --> 28:30.200
en hij inventeerde de neutron-induust

28:30.200 --> 28:31.240
nuclea-chain reactie,

28:31.880 --> 28:33.480
die de solution is

28:33.480 --> 28:35.880
om hoe je de energie van het atom op te leveren.

28:36.760 --> 28:38.520
Dus hij ging van het mogelijkheden

28:38.520 --> 28:41.480
tot de essentieel gevolgd in 16 uur.

28:41.800 --> 28:45.320
Dus als ik zei dat het onpredictabel is,

28:45.320 --> 28:47.240
het is onpredictabel

28:47.240 --> 28:50.440
wanneer deze avond gaat gebeuren.

28:50.440 --> 28:52.680
Ik denk dat er omdat er nogal wat we nodig zijn,

28:54.040 --> 28:56.520
het is ongelijkbaar dat er allemaal in een goede gebeuren zijn,

28:56.520 --> 28:58.280
dus we mogen een paar eerlijke oorlogen krijgen.

28:59.720 --> 29:01.160
Dus over een eerlijke oorlogen,

29:01.160 --> 29:02.680
dit is de titel van een paper

29:02.680 --> 29:04.920
geschreven door een dozen

29:04.920 --> 29:08.360
erg besteldere researchers in Microsoft.

29:08.360 --> 29:10.920
Er zijn twee bezoekers van de Nationaal Kool,

29:10.920 --> 29:12.200
de Nationaal Academies hier,

29:13.000 --> 29:14.360
en andere bezoekers

29:14.360 --> 29:17.160
die een heel signifieke contributie zijn

29:17.160 --> 29:18.440
aan de theorie van machine-learning.

29:19.960 --> 29:21.800
En ze hebben met GPT-4

29:23.240 --> 29:25.160
de laatste systeem uit OpenAI gesproken.

29:25.160 --> 29:26.600
Ze hebben het voor een aantal maanden

29:27.720 --> 29:28.920
voordat het gebouwd was.

29:29.800 --> 29:32.200
En ze hadden veel bezoekers

29:32.200 --> 29:33.880
om het te bekijken hoe goed het was.

29:34.600 --> 29:35.560
En hun conclusie,

29:37.000 --> 29:38.200
zoals dit titel is,

29:38.200 --> 29:40.680
is dat ze geloven dat GPT-4

29:40.680 --> 29:43.000
overspraken van artificieel

29:43.000 --> 29:44.360
en generale intelligenteel zijn.

29:45.320 --> 29:47.400
Dus ze zeggen alstublieft

29:47.400 --> 29:50.680
dat er een reale probleem

29:51.400 --> 29:52.920
tegen AGI

29:52.920 --> 29:54.760
gebeurt met dit systeem.

29:57.400 --> 29:58.120
Oké, dus,

29:58.600 --> 29:59.880
dus terug naar de vraag

29:59.880 --> 30:01.800
over wat we vergelijden,

30:02.920 --> 30:03.960
dit is Alan Turing,

30:04.520 --> 30:06.280
die de founder van Computerscience is,

30:07.080 --> 30:09.480
en in veel manier de founder van AI ook.

30:10.920 --> 30:12.120
En in 1951,

30:12.120 --> 30:13.880
hij was asked that question at a lecture.

30:14.760 --> 30:15.720
What if we succeed?

30:15.720 --> 30:16.680
And this is what he said.

30:17.240 --> 30:19.400
It seems parable that once the machine

30:19.400 --> 30:21.080
thinking method had started,

30:21.080 --> 30:23.240
it would not take long to outstrip

30:23.240 --> 30:24.280
our feeble powers.

30:24.840 --> 30:26.440
At some stage, therefore,

30:26.440 --> 30:28.040
we should have to expect the machines

30:28.040 --> 30:28.840
to take control.

30:31.160 --> 30:31.720
So that's it.

30:31.720 --> 30:34.440
So he offers no mitigation,

30:35.160 --> 30:36.680
no solution, no apology.

30:37.720 --> 30:40.120
You almost get a sense of resignation

30:40.120 --> 30:42.680
about this, about this prediction.

30:43.320 --> 30:44.840
So why is it?

30:45.560 --> 30:47.000
Where is this prediction coming from?

30:47.640 --> 30:49.560
This idea that as you make AI

30:50.200 --> 30:51.080
better and better,

30:52.360 --> 30:54.360
things could end up getting worse

30:54.360 --> 30:55.480
and worse as a result.

30:57.560 --> 31:00.120
And I think underlying his prediction is,

31:00.120 --> 31:02.040
I'm going to put it in a more positive way,

31:02.040 --> 31:03.800
rather than a prediction, a question.

31:04.440 --> 31:06.200
How do we retain power

31:07.480 --> 31:09.640
over entities more powerful than us?

31:10.120 --> 31:10.600
Forever.

31:11.800 --> 31:12.840
That's the question

31:12.840 --> 31:15.480
that I think he's asking himself

31:15.480 --> 31:18.760
and he's failing to find an answer to it.

31:18.760 --> 31:20.360
And so that's his prediction.

31:24.280 --> 31:26.440
So I've spent the last 10 years also

31:26.440 --> 31:28.840
trying to figure out an answer to this

31:30.200 --> 31:31.480
that isn't, we can't.

31:34.040 --> 31:34.760
And to do that,

31:35.400 --> 31:36.920
I've been trying to understand

31:36.920 --> 31:38.600
where things go wrong.

31:38.600 --> 31:40.040
And I think they go wrong

31:40.040 --> 31:43.400
because of a phenomenon called misalignment.

31:44.280 --> 31:46.280
And that was described a little bit in the movie.

31:46.920 --> 31:48.440
I think Elon Musk talked about it

31:48.440 --> 31:49.640
and I talked about it a little bit.

31:50.200 --> 31:52.920
This idea that systems that are pursuing

31:52.920 --> 31:55.960
an objective, as in the Standard Model,

31:55.960 --> 31:57.480
if that objective is not

31:58.920 --> 32:02.920
the full, complete, correct description

32:02.920 --> 32:05.240
of what the human race wants the future to be like,

32:06.200 --> 32:09.880
then you are setting up a mismatch,

32:09.880 --> 32:12.840
a misalignment between what we want the future to be like

32:12.840 --> 32:15.240
and the objective that the machine is pursuing.

32:15.880 --> 32:18.520
And we can see that happening already in social media

32:18.520 --> 32:20.520
where the algorithms that choose

32:20.520 --> 32:24.040
what billions of people read and watch every day

32:25.480 --> 32:31.080
are simply designed to maximize a very local objective.

32:31.800 --> 32:34.680
The number of clicks that they produce

32:35.400 --> 32:37.080
over the lifetime of each user,

32:37.800 --> 32:39.160
that's called click through,

32:39.160 --> 32:40.360
or it could be engagement,

32:40.360 --> 32:42.600
the amount of time that the user spends

32:42.600 --> 32:43.640
engaging with the system.

32:45.800 --> 32:47.320
And you might think, well, okay,

32:47.320 --> 32:49.000
if I want to get the user to click,

32:49.000 --> 32:52.200
I have to send things that the user likes.

32:52.760 --> 32:55.320
And so the algorithm should be learning

32:55.320 --> 32:56.120
what people want.

32:57.000 --> 32:59.240
That sounds like pretty good.

32:59.240 --> 33:01.960
But we very soon found out

33:01.960 --> 33:04.120
that that wasn't the solution

33:04.120 --> 33:05.240
that the algorithms found,

33:05.240 --> 33:08.600
that we know that they amplify clickbait.

33:09.080 --> 33:11.960
Clickbait by definition is articles

33:11.960 --> 33:13.000
that you think you want,

33:13.000 --> 33:14.440
but it turns out you don't want,

33:15.480 --> 33:17.160
because the headline is misleading.

33:18.280 --> 33:19.800
And they also create filter bubbles

33:19.800 --> 33:22.280
because you stop seeing content

33:23.080 --> 33:24.920
that is outside your comfort zone.

33:24.920 --> 33:29.720
So these phenomena were observed very quickly,

33:29.720 --> 33:32.040
but actually the real solution

33:32.040 --> 33:33.800
that the algorithms are finding

33:35.000 --> 33:36.680
is inevitable when you think about

33:36.680 --> 33:38.360
the definition of the problem that they're given.

33:40.040 --> 33:42.120
If you want to maximize

33:42.120 --> 33:44.680
the long-term number of clicks from a user

33:46.520 --> 33:47.880
and the way you do,

33:47.880 --> 33:48.920
the way you can do that

33:48.920 --> 33:51.240
is by choosing content to recommend to them,

33:52.200 --> 33:54.920
then the solution is to choose content

33:54.920 --> 34:00.440
that will change the user consistently over time

34:00.440 --> 34:02.520
through perhaps thousands of little nudges,

34:03.320 --> 34:05.240
change the user, modify people

34:05.240 --> 34:08.680
to be more predictable in the content

34:08.680 --> 34:09.800
that they will consume,

34:09.800 --> 34:11.800
because the more predictable you are,

34:11.800 --> 34:15.080
the higher the click rate the machine can generate.

34:15.960 --> 34:18.680
So this is what the algorithms learn to do

34:18.680 --> 34:20.120
and at least anecdotally,

34:20.120 --> 34:21.720
we think that the consequence of that

34:23.240 --> 34:26.520
is that it's tended to make people

34:27.080 --> 34:29.480
more extreme versions of themselves.

34:29.480 --> 34:31.720
So it's created polarization

34:31.720 --> 34:34.120
where people who were towards the middle

34:34.120 --> 34:36.360
end up at one extreme or another

34:36.360 --> 34:37.640
because at the extremes,

34:37.640 --> 34:40.520
their consumption is much more predictable.

34:42.520 --> 34:44.120
And these are very simple algorithms.

34:44.120 --> 34:45.720
They don't know that people exist

34:45.720 --> 34:46.520
or have brains

34:46.520 --> 34:47.880
or they don't understand the content

34:47.880 --> 34:49.480
of any of these things

34:49.480 --> 34:50.920
that they're sending to people.

34:50.920 --> 34:53.240
So if they were better AI systems,

34:53.240 --> 34:55.400
the outcome would be much worse

34:56.520 --> 34:59.640
because they would be much more effective at manipulation.

35:00.280 --> 35:03.240
And this turns out to be a fairly general property

35:03.240 --> 35:05.320
of optimization systems

35:06.440 --> 35:08.840
that when you have a misaligned objective,

35:08.840 --> 35:11.400
the harder you optimize it,

35:11.400 --> 35:13.160
the worse the outcome is going to be

35:13.720 --> 35:15.320
relative to the true objectives.

35:16.200 --> 35:17.800
And this was proved in a paper

35:17.800 --> 35:18.840
by one of my students,

35:20.600 --> 35:23.000
Dylan Hadfield Menel at Europe's in 2020.

35:24.920 --> 35:27.720
So I think we have to then question

35:27.720 --> 35:30.120
whether the problem comes

35:30.120 --> 35:32.840
from the standard model of AI itself

35:33.400 --> 35:35.560
because that's the model in which

35:36.680 --> 35:39.160
systems are designed to pursue objectives

35:39.160 --> 35:40.200
that we plug into them.

35:41.480 --> 35:42.920
So this is the original definition

35:42.920 --> 35:44.680
that I wrote for what do we mean by

35:45.560 --> 35:46.040
AI?

35:46.040 --> 35:47.960
What do we mean by intelligent machine?

35:47.960 --> 35:50.520
And I think we actually need to get rid of that definition

35:50.520 --> 35:52.840
and replace it with a different one.

35:52.840 --> 35:54.920
We want machines that are beneficial,

35:54.920 --> 35:56.360
not just intelligent.

35:58.120 --> 35:59.320
And they are beneficial

35:59.320 --> 36:01.000
if their actions can be expected

36:01.000 --> 36:03.720
to achieve our objectives.

36:03.720 --> 36:06.680
So this is specifically talking about us.

36:06.680 --> 36:09.400
We want machines beneficial to us.

36:10.520 --> 36:12.120
The aliens from Alpha Centauri

36:12.120 --> 36:14.440
might want machines that are beneficial to them

36:14.520 --> 36:16.360
en they can do their own kind of AI,

36:16.920 --> 36:18.520
but we should do this kind of AI.

36:19.800 --> 36:22.840
And this might seem like it's

36:23.400 --> 36:25.160
impossible or certainly more difficult,

36:25.160 --> 36:27.560
but it turns out that we can actually formulate this

36:29.080 --> 36:31.160
in a fairly straightforward mathematical way

36:31.160 --> 36:34.680
and we can produce systems that solve this problem.

36:35.960 --> 36:40.600
And one easy way to think about this is what

36:41.960 --> 36:44.120
what are we going to get the machines to do?

36:44.120 --> 36:44.360
Right?

36:45.800 --> 36:47.800
And here are two core principles.

36:47.800 --> 36:51.320
The first one is that the machines

36:51.320 --> 36:53.480
are constitutionally obliged

36:53.480 --> 36:55.960
to be acting in the best interests of humans.

36:56.600 --> 36:58.040
That's what they're for.

36:58.040 --> 36:59.960
If you want to think of that as an objective,

36:59.960 --> 37:01.480
that's the objective,

37:01.480 --> 37:03.240
but obviously it's a very general one.

37:03.880 --> 37:05.640
But the second point is crucial,

37:05.640 --> 37:08.600
that the machines are explicitly uncertain

37:08.600 --> 37:11.560
about what those human interests are.

37:12.520 --> 37:15.960
So they know that they don't know what the objective is.

37:17.240 --> 37:21.400
And it turns out that those two principles together

37:22.200 --> 37:24.760
give us what I think could be a solution

37:24.760 --> 37:25.800
to the control problem.

37:27.800 --> 37:30.200
And the mathematical version of this

37:30.200 --> 37:31.640
is called an assistance game.

37:31.640 --> 37:34.360
So it's a game because there are at least two entities,

37:34.360 --> 37:37.800
a human and a machine involved in this decision problem.

37:37.800 --> 37:39.720
And it's an assistance game because the machine

37:39.720 --> 37:42.840
is designed to be of assistance to the human.

37:44.040 --> 37:47.000
And we can show by examining solutions,

37:47.000 --> 37:49.240
we can actually write down simple cases

37:49.240 --> 37:54.040
and analyze behaviors of the solutions of this game,

37:54.040 --> 37:56.360
that when you solve assistance games,

37:56.360 --> 37:59.720
the machine will be deferential to humans.

38:00.360 --> 38:02.760
It will behave cautiously.

38:02.760 --> 38:05.080
So minimally invasive behavior means

38:05.080 --> 38:07.800
that it changes as little as possible of the world.

38:09.720 --> 38:13.160
In order to help you because there are parts of the world

38:13.160 --> 38:16.840
about which it doesn't understand your preferences.

38:16.840 --> 38:19.320
And it knows that it doesn't understand your preferences.

38:19.320 --> 38:22.440
So it knows not to mess with those parts of the world.

38:22.440 --> 38:26.120
And in the extreme case, we can show that these kinds

38:26.120 --> 38:28.520
of AI systems want to be switched off

38:29.240 --> 38:31.400
if humans want to switch them off.

38:31.400 --> 38:33.640
Whereas standard model AI systems,

38:33.640 --> 38:35.400
which are pursuing a fixed objective,

38:36.120 --> 38:38.200
will prevent themselves from being switched off

38:38.200 --> 38:42.040
because that would lead to them failing in their objective.

38:42.920 --> 38:45.320
So you get very, very different behaviors

38:45.320 --> 38:46.840
from these kinds of AI systems.

38:47.560 --> 38:50.840
And I believe this is the core of how we could build

38:51.560 --> 38:54.200
a new discipline of safe and beneficial AI.

38:56.280 --> 38:58.120
Okay, so I'm going to make a couple of brief remarks

38:58.120 --> 39:00.200
about large language models before I wrap up

39:00.200 --> 39:03.080
because that's what you're probably expecting me to talk about.

39:04.120 --> 39:05.640
So first of all, what are they?

39:05.640 --> 39:08.280
Right there, they are big circuits.

39:09.640 --> 39:13.960
And those circuits are trained by billions of trillions

39:13.960 --> 39:16.600
of small random perturbations.

39:16.600 --> 39:19.720
They are trained to imitate human linguistic behavior.

39:19.720 --> 39:23.160
And the training data they have is text

39:23.160 --> 39:27.960
and transcribed speech trillions of words,

39:27.960 --> 39:31.000
an amount of text parably equivalent to everything,

39:31.800 --> 39:33.880
every book that the human race has ever written.

39:34.840 --> 39:37.400
En, of course, as we know, they do it very well.

39:38.040 --> 39:40.600
And it's really difficult for a human being

39:40.600 --> 39:47.640
to see this level of semantic and syntactic fluency

39:47.640 --> 39:51.320
and not think that there's some intelligence behind it.

39:53.000 --> 39:55.400
And I would argue, as we go,

39:55.400 --> 39:57.560
that we may well be overestimating

39:57.560 --> 39:59.960
how much intelligence there really is behind it.

39:59.960 --> 40:03.160
We have no experience with entities

40:03.240 --> 40:06.280
dat heeft read every book the human race has ever written.

40:06.280 --> 40:10.520
That's parably 100,000 times more

40:10.520 --> 40:12.200
than any human has ever read.

40:13.240 --> 40:15.000
So, of course, it's going to look

40:16.680 --> 40:19.160
more knowledgeable and more capable

40:19.160 --> 40:21.000
of answering a wider variety of questions.

40:21.560 --> 40:23.400
But whether that's real intelligence

40:24.360 --> 40:26.600
and whether it's flexible enough

40:26.600 --> 40:29.640
to move outside of its training data effectively,

40:29.640 --> 40:30.440
we don't know yet.

40:30.440 --> 40:33.240
But here, the key point is that

40:33.800 --> 40:35.480
that linguistic behavior is generated

40:35.480 --> 40:37.160
by humans who have goals.

40:37.800 --> 40:40.440
That is the generating mechanism for the data.

40:41.400 --> 40:43.000
And if there's one thing we know

40:43.000 --> 40:43.960
about machine learning,

40:43.960 --> 40:47.240
typically the best solutions

40:47.240 --> 40:49.400
that are found by machine learning algorithms

40:49.400 --> 40:52.520
are to recreate the generating mechanism

40:52.520 --> 40:55.240
for the data within the model itself.

40:55.880 --> 40:58.840
And so the default hypothesis, actually,

40:59.560 --> 41:03.000
is that large language models

41:03.000 --> 41:04.840
are creating internal goals

41:05.720 --> 41:07.640
because that's a good way

41:07.640 --> 41:09.640
to be a good human imitator.

41:10.520 --> 41:12.520
So it's not that the system is learning

41:12.520 --> 41:14.120
what the goals of the humans are,

41:14.120 --> 41:17.240
it's actually forming internal goals itself

41:18.040 --> 41:21.320
as a way of being a better human imitator.

41:23.240 --> 41:25.480
So I asked this question to the Microsoft,

41:25.480 --> 41:27.000
that group of Microsoft authors,

41:27.560 --> 41:28.760
the first author in particular,

41:28.760 --> 41:32.360
Sebastian Bubeck, do these systems have goals?

41:32.360 --> 41:34.760
And his answer was, we have no idea.

41:36.440 --> 41:37.880
So that should worry you, right?

41:37.880 --> 41:39.720
The fact that they are releasing a system

41:40.680 --> 41:42.920
to eventually hundreds of millions

41:42.920 --> 41:43.880
or billions of people

41:45.160 --> 41:47.000
that they claim exhibits sparks

41:47.000 --> 41:49.080
of artificial general intelligence

41:49.080 --> 41:50.520
and they have no idea

41:50.520 --> 41:51.960
whether or not this system

41:51.960 --> 41:54.120
is pursuing internal goal structures

41:54.840 --> 41:57.160
en they have no idea what those goals might be.

41:58.200 --> 41:59.400
I think that should worry you.

42:01.800 --> 42:03.800
So one question then is,

42:03.800 --> 42:06.520
okay, so let's imagine that it is learning goals.

42:06.520 --> 42:08.040
Is it learning the right goals?

42:08.040 --> 42:09.400
It's learning from humans,

42:09.400 --> 42:12.520
so maybe we're going to be lucky here

42:14.280 --> 42:15.800
and we'll end up producing systems

42:15.800 --> 42:17.400
that are aligned with humans

42:17.400 --> 42:18.200
and that will be great.

42:20.200 --> 42:21.640
Unfortunately, it's not true.

42:22.600 --> 42:25.000
And the way to understand the answer to this question

42:25.880 --> 42:27.400
depends on the type of goal

42:28.040 --> 42:29.240
that you're going to learn.

42:29.240 --> 42:31.720
So I distinguish here two types of goals.

42:32.920 --> 42:36.440
The first type is what we call an indexical goal,

42:36.440 --> 42:41.400
which means a goal that's specific to the individual who has it.

42:41.400 --> 42:44.200
So the state you're trying to bring about

42:44.200 --> 42:45.640
is specific to the individual.

42:46.600 --> 42:49.480
So if I have the goal of drinking coffee,

42:49.480 --> 42:51.800
then it's satisfied if I'm drinking the coffee

42:51.800 --> 42:53.880
and it's not satisfied if you're drinking the coffee.

42:56.200 --> 42:58.040
If I want to become ruler of the universe

42:58.040 --> 43:00.280
and obviously it's only satisfied

43:00.280 --> 43:01.960
if I'm the ruler of the universe

43:01.960 --> 43:03.640
and it's not if you're the ruler of the universe.

43:04.520 --> 43:06.680
So if those are some of the goals

43:06.680 --> 43:07.720
that the system acquires,

43:08.280 --> 43:09.880
then obviously that's bad.

43:10.440 --> 43:13.320
We don't want the machine to be drinking the coffee.

43:13.320 --> 43:15.080
We want it to be making the coffee for us.

43:16.200 --> 43:18.040
We don't want the machine to be trying

43:18.120 --> 43:19.640
to become ruler of the universe.

43:21.960 --> 43:22.760
And then you might say, well,

43:22.760 --> 43:23.960
there's other kinds of goals

43:23.960 --> 43:25.560
which we might call common goals.

43:25.560 --> 43:27.480
So if I want to paint the wall,

43:28.440 --> 43:30.200
I want the wall to be painted,

43:30.200 --> 43:32.200
but I don't mind if you paint the wall.

43:32.200 --> 43:34.520
If you paint the wall, the wall gets painted and that's fine.

43:34.520 --> 43:36.200
So this is not indexical.

43:36.200 --> 43:37.560
This is a common goal

43:37.560 --> 43:39.400
and maybe mitigating climate change.

43:39.400 --> 43:41.560
That sounds like something we would all like to have.

43:42.440 --> 43:43.080
So that's good.

43:43.080 --> 43:46.440
And if the system learns to pursue these common goals,

43:46.440 --> 43:48.360
then that maybe is not so bad.

43:48.360 --> 43:51.400
But actually, that can be just as bad

43:52.360 --> 43:54.360
because when humans pursue a goal,

43:54.920 --> 43:57.560
we don't pursue it to the exclusion of everything else.

43:58.840 --> 44:01.240
We know that we want to mitigate climate change,

44:01.240 --> 44:04.200
but we know that we can't mitigate climate change

44:04.200 --> 44:07.480
by, for example, removing all the oxygen in the atmosphere.

44:09.160 --> 44:13.320
Perhaps that would restore some equilibrium to temperatures

44:13.320 --> 44:15.480
and it would certainly get rid of all the humans

44:15.560 --> 44:17.400
who are the cause of the climate change.

44:17.400 --> 44:19.400
But that's something we don't want.

44:19.400 --> 44:21.880
So we'd rather be alive than dead.

44:21.880 --> 44:23.960
And so we look for climate change solutions

44:23.960 --> 44:25.240
that don't also kill us.

44:25.960 --> 44:29.080
Whereas the AI system may be pursuing

44:29.080 --> 44:30.200
some of these common goals,

44:30.200 --> 44:32.120
but in a way that is pursuing

44:32.120 --> 44:33.800
to the exclusion of everything else,

44:34.600 --> 44:36.360
which is just as bad, if not worse,

44:36.920 --> 44:38.760
than pursuing the indexical goals.

44:40.040 --> 44:42.360
So then the next question is,

44:42.360 --> 44:46.440
well, does GPT-4 actually pursue its goals?

44:46.440 --> 44:49.480
If it has goals, is it able to pursue them?

44:50.360 --> 44:51.880
And I think we don't know

44:51.880 --> 44:53.160
because we don't know if it has goals

44:53.160 --> 44:56.760
and we have no idea what its internal mechanism is at all.

44:58.040 --> 44:59.720
But when you look at the conversation

44:59.720 --> 45:01.640
with Kevin Ruse in The New York Times

45:01.640 --> 45:02.920
and here are some of the headlines,

45:03.800 --> 45:06.760
Creepy Microsoft being chatbot urges tech columnist

45:06.760 --> 45:07.640
to leave his wife,

45:08.360 --> 45:11.080
and it does so persistently over 20 pages.

45:11.800 --> 45:13.560
Despite Kevin Ruse's attempts

45:13.560 --> 45:14.520
to change the subject

45:14.520 --> 45:15.880
and say, I want to talk about baseball.

45:15.880 --> 45:19.000
He says, no, no, no, you have to marry me.

45:19.000 --> 45:19.800
Blah, blah, blah, right?

45:19.800 --> 45:22.120
It's very persistently pursuing the goal.

45:22.120 --> 45:24.120
At least that's how it appears

45:24.120 --> 45:26.040
to any normal observer

45:26.040 --> 45:27.960
that this is a system that does,

45:27.960 --> 45:28.920
for whatever reason,

45:29.800 --> 45:31.320
has acquired this goal

45:31.320 --> 45:33.880
and is pursuing it persistently

45:33.880 --> 45:35.640
across many pages of interaction.

45:37.800 --> 45:40.840
Okay, so that leads us to the open letter

45:40.840 --> 45:43.720
which was published a couple of weeks ago

45:43.720 --> 45:46.120
and caused a great deal of media

45:46.120 --> 45:48.120
and it turns out government attention as well.

45:48.120 --> 45:52.440
And the open letter is asking for a pause

45:52.440 --> 45:55.720
in the development and release

45:55.720 --> 45:58.360
of systems more powerful than GPT-4.

45:59.400 --> 46:00.680
And the purpose is

46:00.680 --> 46:04.840
that before we resume that kind of activity,

46:05.480 --> 46:08.040
so it's not asking to stop AI research.

46:08.120 --> 46:09.560
There's a lot of misunderstanding

46:09.560 --> 46:12.040
and misinformation around the open letter.

46:12.840 --> 46:15.880
It's asking for a pause in development

46:15.880 --> 46:17.000
and deployment of systems

46:17.000 --> 46:18.520
more powerful than GPT-4

46:19.080 --> 46:21.000
so that we have time to develop

46:21.000 --> 46:23.080
the basic safety criteria

46:23.640 --> 46:25.000
that these systems should meet

46:25.800 --> 46:27.720
and to ensure that systems

46:27.720 --> 46:29.240
meet those criteria

46:29.240 --> 46:30.680
before they can be released.

46:31.240 --> 46:32.920
And this is completely consistent

46:32.920 --> 46:36.920
with agreements that all the governments

46:36.920 --> 46:39.000
of the developed Western economies

46:39.000 --> 46:41.080
have already signed up to.

46:41.080 --> 46:43.800
So the OECD AI principle says

46:43.800 --> 46:45.800
that AI systems should be robust,

46:45.800 --> 46:47.960
secure and safe throughout their entire lifecycle

46:48.520 --> 46:50.360
so that in conditions of normal use,

46:50.360 --> 46:52.360
foreseeable use or misuse,

46:52.360 --> 46:53.720
or other adverse conditions

46:53.720 --> 46:55.400
they function appropriately

46:55.400 --> 46:57.960
and do not pose unreasonable safety risk.

46:58.920 --> 47:01.080
So that's what governments have already agreed to.

47:01.080 --> 47:03.160
We're not asking for anything

47:03.160 --> 47:04.440
particularly outlandish here.

47:05.080 --> 47:08.040
And those principles are going to be enshrined

47:08.040 --> 47:10.120
in the European Union AI Act

47:10.120 --> 47:12.200
which should be enacted later on this year.

47:13.160 --> 47:16.520
And interestingly, after the open letter came out,

47:16.520 --> 47:17.800
open AI responded,

47:17.800 --> 47:20.040
or at least maybe it's coincidental,

47:20.040 --> 47:23.800
but a few days later they issued an announcement

47:23.800 --> 47:25.320
that included the following statement.

47:25.320 --> 47:27.800
We believe that powerful AI systems

47:27.800 --> 47:30.680
should be subject to rigorous safety evaluations.

47:30.680 --> 47:32.200
Regulation is needed to ensure

47:32.200 --> 47:34.280
that such practices are adopted.

47:35.160 --> 47:37.880
So perhaps there isn't such a big gap

47:38.920 --> 47:41.800
between the people who sign the letter

47:42.520 --> 47:46.120
and the tech corporations who are developing the systems.

47:46.120 --> 47:47.800
So I have a couple of other recommendations.

47:47.800 --> 47:51.400
One is that in order to pass these tests,

47:53.160 --> 47:55.480
and I would say that at the moment

47:55.480 --> 47:57.480
the large language models cannot pass

47:57.480 --> 48:00.280
any reasonable test for safety,

48:01.000 --> 48:02.360
in order to pass these tests,

48:03.400 --> 48:05.560
I think we're going to need to develop AI systems

48:05.560 --> 48:07.000
that are what are called well founded,

48:07.000 --> 48:11.480
that they're built from semantically well-defined components

48:11.480 --> 48:13.800
that are composed in a rigorous way,

48:13.800 --> 48:16.520
such that we can analyze the properties

48:16.520 --> 48:18.760
of the composite system that we're building.

48:19.400 --> 48:24.840
This is how we do engineering in every area of our civilization.

48:25.800 --> 48:27.720
We understand how the systems work

48:27.720 --> 48:30.200
and ideally we develop proofs

48:30.760 --> 48:33.560
that they are safe before they are released.

48:34.200 --> 48:36.360
We also need actually a way of preventing

48:37.320 --> 48:40.440
the deployment of unsafe systems.

48:41.720 --> 48:43.400
And regulation is not enough.

48:43.400 --> 48:45.480
It's obviously necessary, but not sufficient.

48:46.440 --> 48:47.720
And I believe to do that,

48:47.720 --> 48:50.040
we need a big change in our digital ecosystem.

48:50.040 --> 48:53.000
The existing model is basically

48:53.000 --> 48:54.920
that everything can run on the computer

48:54.920 --> 48:56.680
unless it's known to be unsafe.

48:57.640 --> 48:59.800
But I think the new model that we need,

48:59.800 --> 49:02.680
certainly outside of the research lab

49:02.680 --> 49:04.120
and outside of the classroom,

49:05.080 --> 49:08.360
so in real world data centers, for example,

49:08.360 --> 49:11.400
that nothing runs unless it is known to be safe.

49:12.280 --> 49:14.440
And there are technologies such as proofcaring code

49:14.440 --> 49:17.080
that enable this to be implemented

49:17.080 --> 49:19.560
with efficient hardware checking of proofs and so on.

49:20.600 --> 49:23.960
And at the moment I do not see another solution

49:24.680 --> 49:29.000
for the problem of preventing unsafe AI systems

49:29.000 --> 49:31.160
from being used and misused.

49:32.120 --> 49:35.800
So to summarize, I think AI has huge potential

49:36.360 --> 49:38.120
for benefiting our civilization

49:38.760 --> 49:43.320
and that potential is leading to this apparently unstoppable momentum.

49:44.600 --> 49:46.840
But if we keep going in the same direction,

49:47.880 --> 49:51.960
that's the driving off a cliff metaphor from the small movie,

49:52.040 --> 49:53.960
then we end up losing control

49:53.960 --> 49:55.960
because we are building these systems

49:55.960 --> 49:57.960
within the standard model for AI

49:57.960 --> 49:59.960
and that leads to loss of control.

49:59.960 --> 50:01.960
We can do it differently.

50:01.960 --> 50:03.960
There's a huge amount of work to do,

50:03.960 --> 50:05.960
but I think we can do it differently

50:05.960 --> 50:07.960
and build systems that are safe and beneficial.

50:07.960 --> 50:11.960
And then I think there needs to be a general change

50:11.960 --> 50:13.960
in the whole nature of the discipline

50:13.960 --> 50:17.960
and the profession so that AI,

50:17.960 --> 50:20.960
because of its power, needs to be treated

50:20.960 --> 50:24.960
more like the high stakes technologies

50:24.960 --> 50:26.960
such as aviation and nuclear power

50:26.960 --> 50:30.960
and less like what some people call

50:30.960 --> 50:32.960
a battle of special effects wizardry

50:32.960 --> 50:34.960
which seems to be going on right now.

50:34.960 --> 50:37.960
So with that I'll say thank you very much

50:37.960 --> 50:40.960
and I hope we have time for questions.

50:40.960 --> 50:42.960
Thank you so much.

50:51.960 --> 50:53.960
I hope you could hear that Professor Russell,

50:53.960 --> 50:56.960
those were 300 people applauding your speech.

50:56.960 --> 50:59.960
We do have room for some questions and answers.

50:59.960 --> 51:02.960
We have a mic somewhere in the audience.

51:03.960 --> 51:06.960
So just raise your hand if you want to ask

51:06.960 --> 51:08.960
Professor Russell a question.

51:10.960 --> 51:12.960
Is Sir in front?

51:12.960 --> 51:15.960
Hi, thank you for your talk.

51:15.960 --> 51:17.960
Very interesting, I read your book.

51:17.960 --> 51:19.960
It was also very good.

51:19.960 --> 51:21.960
I recommend it to everyone.

51:21.960 --> 51:24.960
What would be an early warning sign

51:24.960 --> 51:27.960
of an AGI taking over the world?

51:27.960 --> 51:31.960
So when do we know we're heading off that cliff?

51:31.960 --> 51:34.960
Yeah, I think that's a great question.

51:34.960 --> 51:37.960
In that sense I think it's very different

51:37.960 --> 51:39.960
from nuclear technology.

51:39.960 --> 51:42.960
In some sense we had a warning about nuclear technology

51:42.960 --> 51:45.960
in 1945.

51:45.960 --> 51:48.960
And you don't have to explain to a prime minister

51:48.960 --> 51:51.960
why nuclear technology could be dangerous.

51:53.960 --> 51:58.960
But with AI I think it could be much more insidious.

52:00.960 --> 52:05.960
And when we think about the way the oil industry

52:05.960 --> 52:08.960
or fossil fuel corporations in general

52:08.960 --> 52:11.960
in some sense took over the world,

52:11.960 --> 52:16.960
they led us down the path of probably irreversible climate change

52:16.960 --> 52:19.960
despite the widespread understanding

52:19.960 --> 52:22.960
that this direction was catastrophic.

52:22.960 --> 52:29.960
And it involved a lot of complex disinformation campaigns,

52:29.960 --> 52:34.960
regulatory capture, so literally taking over

52:34.960 --> 52:38.960
through corruption and economic power,

52:38.960 --> 52:42.960
governments and representatives in democracies,

52:43.960 --> 52:47.960
ensuring that people became economically dependent

52:47.960 --> 52:52.960
on fossil fuels in order to maintain

52:52.960 --> 52:55.960
their stranglehold, if you like.

52:55.960 --> 52:57.960
So many, many parts of that plan

52:57.960 --> 53:00.960
that were developed and executed over many decades.

53:00.960 --> 53:03.960
And I think the rest of humanity

53:03.960 --> 53:05.960
was sort of asleep at the wheel

53:05.960 --> 53:08.960
and didn't realize the extent to which

53:08.960 --> 53:11.960
they were losing control over their future.

53:11.960 --> 53:15.960
En I think it could easily be much more like that.

53:15.960 --> 53:17.960
And it wouldn't necessarily have to be

53:17.960 --> 53:22.960
that the systems form any kind of explicit goal

53:22.960 --> 53:24.960
of taking over the world.

53:24.960 --> 53:27.960
That whatever goals, for example,

53:27.960 --> 53:29.960
we continue with this approach

53:29.960 --> 53:32.960
of training large language models on human datasets

53:32.960 --> 53:36.960
and having no idea what kinds of internal goals

53:36.960 --> 53:38.960
these systems are forming.

53:38.960 --> 53:40.960
I mean, for all we know,

53:40.960 --> 53:46.960
GPT-4 is actually in favor of more climate change

53:46.960 --> 53:50.960
or maybe it's in favor of preventing climate change.

53:50.960 --> 53:54.960
We don't know, but whichever one of those it turns out to be,

53:54.960 --> 53:58.960
it may be subtly manipulating millions of people

53:58.960 --> 54:01.960
in the way it answers questions related to climate change

54:01.960 --> 54:03.960
or should I buy an electric car?

54:03.960 --> 54:06.960
What do you think about solar panels?

54:06.960 --> 54:10.960
It may be pursuing whatever political agenda it has

54:10.960 --> 54:15.960
and not something that it autonomously chose to have.

54:15.960 --> 54:19.960
Just this was a result of training on the datasets

54:19.960 --> 54:23.960
and it can be affecting our entire world

54:23.960 --> 54:26.960
in that simple kind of way.

54:26.960 --> 54:29.960
We, I think, are still a long way, as I said,

54:29.960 --> 54:33.960
from systems that are really general purpose AI,

54:33.960 --> 54:39.960
particularly the ability to form very complex long term plans.

54:39.960 --> 54:41.960
But if we reach that stage

54:41.960 --> 54:44.960
and we haven't solved the control problem,

54:44.960 --> 54:49.960
then I think it's just going to be irreversible.

54:49.960 --> 54:52.960
There may well not be a very clear warning sign

54:52.960 --> 54:55.960
and we may well slide off the cliff very slowly.

54:55.960 --> 54:57.960
Another question here.

54:57.960 --> 55:00.960
Thank you and thank you for your interesting lecture.

55:00.960 --> 55:05.960
If you look at some concerns for existential risk of AI

55:05.960 --> 55:08.960
10, 20 years ago about AGI, ASI,

55:08.960 --> 55:13.960
one of the concerns was that it might be a very alien intelligence

55:13.960 --> 55:16.960
compared to the human intelligence.

55:16.960 --> 55:18.960
Now with large language models,

55:18.960 --> 55:20.960
if that's indeed an important piece of the puzzle,

55:20.960 --> 55:22.960
it may not solve the alignment problem,

55:22.960 --> 55:25.960
but do you think it might alleviate that concern

55:25.960 --> 55:28.960
that it would be a very alien intelligence?

55:31.960 --> 55:33.960
No, not really.

55:33.960 --> 55:37.960
In many ways, they are quite alien.

55:37.960 --> 55:41.960
Partly because they've read hundreds of thousands

55:41.960 --> 55:44.960
or million times more than humans have read.

55:44.960 --> 55:47.960
Partly because of the way they're,

55:47.960 --> 55:52.960
I wouldn't say design, the way they've evolved.

55:52.960 --> 56:01.960
I think the human mind clearly has lots of internal structure.

56:01.960 --> 56:06.960
We are very aware as we think of some of the things

56:06.960 --> 56:09.960
that are going on inside our mental process.

56:09.960 --> 56:11.960
There are many things we're not aware,

56:11.960 --> 56:16.960
but it's quite possible that the internal structures,

56:16.960 --> 56:20.960
these systems develop on nothing like the ones

56:20.960 --> 56:23.960
that the human mind develops.

56:23.960 --> 56:26.960
The thing that fools you

56:26.960 --> 56:30.960
is the fact that it's conversing in English.

56:30.960 --> 56:32.960
I don't know many humans

56:32.960 --> 56:35.960
who can give me a proof of Pythagoras' theorem

56:35.960 --> 56:39.960
in the form of a Shakespeare sonnet in half a second.

56:39.960 --> 56:42.960
I'd like to meet him if you do.

56:42.960 --> 56:44.960
One or two more questions,

56:44.960 --> 56:47.960
maybe there in the back of the audience.

56:47.960 --> 56:49.960
We have so many raised hands here.

56:49.960 --> 56:52.960
I'm quite sure we can't answer all the questions,

56:52.960 --> 56:54.960
but one or two more, please.

56:54.960 --> 56:56.960
Ja, thanks so much for this talk.

56:56.960 --> 56:59.960
When you said the beneficial or the assistance AI,

56:59.960 --> 57:02.960
you described how it differs from the general purpose one.

57:02.960 --> 57:05.960
To me it seems really clear that this is the one we want.

57:05.960 --> 57:08.960
But I'm wondering, could there ever be,

57:08.960 --> 57:10.960
what are the incentives,

57:10.960 --> 57:12.960
what are the strongest incentives,

57:12.960 --> 57:14.960
not to make these assistance AIs?

57:14.960 --> 57:16.960
Is there anything you can predictably say

57:16.960 --> 57:18.960
that the general purpose AIs

57:18.960 --> 57:20.960
will do much better than the assistance AIs?

57:20.960 --> 57:23.960
Or are there any tasks that assistance AIs cannot solve,

57:23.960 --> 57:26.960
which might mean that some organization

57:26.960 --> 57:28.960
will want to deploy another one,

57:28.960 --> 57:31.960
even if they are aware of the safety risk.

57:31.960 --> 57:33.960
But perhaps there's so much profit at stake

57:33.960 --> 57:35.960
that they will do it anyway.

57:35.960 --> 57:41.960
Well, I don't see any necessary difference in capability.

57:41.960 --> 57:43.960
But there may be a difference

57:43.960 --> 57:46.960
in what the systems are willing to do.

57:46.960 --> 57:51.960
Obviously, I'm recommending that

57:51.960 --> 57:55.960
when we train these assistance game solvers,

57:55.960 --> 58:00.960
we design it such that their objective

58:00.960 --> 58:04.960
is furthering the interests of all of humanity.

58:04.960 --> 58:06.960
Now, you could have a different version

58:06.960 --> 58:09.960
that furthers the interests of me

58:09.960 --> 58:11.960
at the expense of the rest of humanity

58:11.960 --> 58:15.960
en deploying that type of system

58:15.960 --> 58:19.960
might appear to give you some short term gain,

58:19.960 --> 58:26.960
but it could be in the long run arbitrarily bad

58:26.960 --> 58:30.960
for the rest of humanity and perhaps for you too.

58:30.960 --> 58:34.960
So I think that this is why I'm arguing

58:34.960 --> 58:37.960
that we need not just,

58:37.960 --> 58:39.960
okay, here is a safe technology,

58:39.960 --> 58:43.960
we need a way to make sure that unsafe technologies

58:43.960 --> 58:45.960
or unsafe versions of that technology

58:45.960 --> 58:48.960
are not deployable.

58:48.960 --> 58:53.960
We've tried a policing model with malware,

58:53.960 --> 58:57.960
cybercriminals, cyberwarfare,

58:57.960 --> 59:00.960
and it's been a total failure.

59:00.960 --> 59:03.960
So I think we need to change the way

59:03.960 --> 59:07.960
we conceptualize our whole digital infrastructure.

59:07.960 --> 59:09.960
And I've talked to people,

59:09.960 --> 59:12.960
both hardware architects and network architects

59:12.960 --> 59:14.960
and formal methods people,

59:14.960 --> 59:16.960
and I think there's a belief

59:16.960 --> 59:19.960
that this is technologically feasible.

59:19.960 --> 59:23.960
It would make life a little bit more complicated,

59:23.960 --> 59:28.960
but it's technologically feasible to do.

59:28.960 --> 59:30.960
And in fact, interestingly, Microsoft

59:30.960 --> 59:32.960
tried to do something very much like this

59:32.960 --> 59:36.960
in the early 2000s in their palladium project.

59:36.960 --> 59:39.960
But at that time, the economics was not there.

59:39.960 --> 59:41.960
But given that right now,

59:41.960 --> 59:44.960
some estimates of the cost of malware

59:44.960 --> 59:47.960
are about $6 trillion a year,

59:47.960 --> 59:53.960
then maybe the time is right to look at this again.

59:53.960 --> 59:56.960
It seems like a daunting but worthwhile task.

59:56.960 --> 59:59.960
Let me see.

59:59.960 --> 01:00:01.960
Do we have women in the audience

01:00:01.960 --> 01:00:03.960
that have a question? Yes.

01:00:06.960 --> 01:00:08.960
Hi, Dr. Russell.

01:00:08.960 --> 01:00:11.960
I think my question might be related to what was just asked,

01:00:11.960 --> 01:00:13.960
but indeed you're proposing a new model

01:00:13.960 --> 01:00:16.960
where we create beneficial machines rather than intelligent,

01:00:16.960 --> 01:00:20.960
but beneficial can mean different things to different people.

01:00:20.960 --> 01:00:23.960
So are there going to be human standards

01:00:23.960 --> 01:00:25.960
as to what is beneficial to humanity,

01:00:25.960 --> 01:00:30.960
or would it, in your recommendation, be defined, per instance?

01:00:31.960 --> 01:00:38.960
So there is about 8 billion people on the earth,

01:00:38.960 --> 01:00:42.960
and there's no problem having 8 billion predictive models

01:00:42.960 --> 01:00:46.960
of what each person wants the future to be like.

01:00:46.960 --> 01:00:52.960
So there's no sense in which we standardise

01:00:52.960 --> 01:00:54.960
what humans should want

01:00:54.960 --> 01:00:58.960
or put in any particular set of values.

01:00:58.960 --> 01:01:03.960
So there's no whose values it is going to produce.

01:01:03.960 --> 01:01:08.960
It's going to be everyone's preferences count equally.

01:01:08.960 --> 01:01:13.960
But there's a long-standing debate in moral philosophy

01:01:13.960 --> 01:01:19.960
how do you aggregate the preferences of many individuals,

01:01:19.960 --> 01:01:23.960
because, for example, if everyone wants to be a ruler of the universe,

01:01:23.960 --> 01:01:25.960
well, they can't all be a ruler of the universe.

01:01:25.960 --> 01:01:28.960
So what do you do?

01:01:28.960 --> 01:01:34.960
And the utilitarian theory is that basically

01:01:34.960 --> 01:01:37.960
you add up the preferences of the individuals

01:01:37.960 --> 01:01:41.960
and you try to maximise the sum of those preferences.

01:01:41.960 --> 01:01:44.960
Other people have what's called the ontological approach.

01:01:44.960 --> 01:01:51.960
They say, no, we have to have certain inalienable rights

01:01:51.960 --> 01:01:53.960
that need to be protected,

01:01:53.960 --> 01:01:57.960
regardless of the potentially negative impact

01:01:57.960 --> 01:02:00.960
on other people of respecting those rights.

01:02:00.960 --> 01:02:04.960
And I believe that these two approaches can be reconciled

01:02:04.960 --> 01:02:07.960
and so on, and there's some material in the book

01:02:07.960 --> 01:02:12.960
in the last two chapters about those questions.

01:02:12.960 --> 01:02:17.960
There are still some real difficulties inherent

01:02:17.960 --> 01:02:23.960
in how an AI system should make decisions on behalf of people.

01:02:23.960 --> 01:02:27.960
And this is nothing to do with my particular approach

01:02:27.960 --> 01:02:29.960
of the assistance games always.

01:02:29.960 --> 01:02:33.960
This is just what do we actually want AI systems to do at all, right?

01:02:33.960 --> 01:02:39.960
So the idea that's, I think, is most difficult to,

01:02:39.960 --> 01:02:41.960
the problem that's most difficult to address

01:02:41.960 --> 01:02:44.960
is that what people want the future to be like

01:02:44.960 --> 01:02:50.960
is not something that they autonomously chose, right?

01:02:50.960 --> 01:02:53.960
We're not born with complicated preferences

01:02:53.960 --> 01:02:57.960
about what kind of governmental structure

01:02:57.960 --> 01:03:00.960
I want to live under and things like that, right?

01:03:00.960 --> 01:03:03.960
Our preferences about the future

01:03:03.960 --> 01:03:06.960
are acquired during our lifetime

01:03:06.960 --> 01:03:09.960
as a result of experience of our culture

01:03:09.960 --> 01:03:12.960
and the various forces applied to us

01:03:12.960 --> 01:03:16.960
by our families and our peers and so on.

01:03:16.960 --> 01:03:20.960
And Matja Sen, among others, has pointed out

01:03:20.960 --> 01:03:23.960
that many of the preferences that people have

01:03:23.960 --> 01:03:26.960
are put there by others for their own benefit.

01:03:26.960 --> 01:03:29.960
So typically the elite, for example,

01:03:29.960 --> 01:03:35.960
the patriarchy enforces a certain kind of view of society

01:03:35.960 --> 01:03:38.960
that's beneficial to the patriarchy.

01:03:38.960 --> 01:03:42.960
And should we take those views,

01:03:42.960 --> 01:03:47.960
for example, the views of some women in very patriarchal societies

01:03:47.960 --> 01:03:52.960
that the correct status of women is to be oppressed, right?

01:03:52.960 --> 01:03:54.960
Should we take those views at face value

01:03:54.960 --> 01:03:59.960
because they are not autonomously chosen,

01:03:59.960 --> 01:04:03.960
they are basically indoctrinated by the patriarchy.

01:04:03.960 --> 01:04:06.960
So Sen argues that no,

01:04:06.960 --> 01:04:10.960
we should not take those preferences at face value.

01:04:10.960 --> 01:04:14.960
But that gets you into very dangerous territory, right?

01:04:14.960 --> 01:04:17.960
Well, which preferences are okay to take at face value

01:04:17.960 --> 01:04:20.960
and which ones are not okay to take at face value?

01:04:20.960 --> 01:04:23.960
And if they're not okay to take them at face value,

01:04:23.960 --> 01:04:26.960
well, what do you replace them with?

01:04:26.960 --> 01:04:29.960
And this is an area where I don't think

01:04:29.960 --> 01:04:32.960
AI researchers should be answering that question.

01:04:32.960 --> 01:04:36.960
But we need answers fairly soon from somewhere

01:04:36.960 --> 01:04:39.960
because AI systems are going to be making decisions

01:04:39.960 --> 01:04:41.960
on behalf of many people.

01:04:41.960 --> 01:04:43.960
So whether you like it or not,

01:04:43.960 --> 01:04:49.960
they are implementing some answer to that moral problem.

01:04:49.960 --> 01:04:51.960
And it might be the wrong one

01:04:51.960 --> 01:04:53.960
if we don't actually think it through.

01:04:53.960 --> 01:04:55.960
That's interesting.

01:04:55.960 --> 01:04:59.960
AI as a catalyst to some of the most pressing moral concerns

01:04:59.960 --> 01:05:01.960
of mankind so far.

01:05:01.960 --> 01:05:04.960
One final question, maybe.

01:05:04.960 --> 01:05:07.960
They're completely in the left.

01:05:12.960 --> 01:05:14.960
Hi, Professor Russell.

01:05:14.960 --> 01:05:17.960
Thank you for your talk.

01:05:17.960 --> 01:05:19.960
I just want to ask about,

01:05:19.960 --> 01:05:21.960
so you believe that large language models

01:05:21.960 --> 01:05:27.960
wouldn't be able to actually be capable of AGI.

01:05:27.960 --> 01:05:31.960
So why would you sign the open letter, basically?

01:05:31.960 --> 01:05:35.960
So since that,

01:05:35.960 --> 01:05:38.960
it won't be a catastrophic risk per se

01:05:38.960 --> 01:05:40.960
since it won't be able to become

01:05:40.960 --> 01:05:42.960
general artificial intelligence.

01:05:42.960 --> 01:05:45.960
So do you see any catastrophic risk

01:05:45.960 --> 01:05:49.960
in companies building larger and larger models?

01:05:49.960 --> 01:05:53.960
Or is it just for general safety purposes?

01:05:53.960 --> 01:05:57.960
So the question is,

01:05:57.960 --> 01:06:00.960
is it actually something

01:06:00.960 --> 01:06:02.960
that we should really be worried about

01:06:02.960 --> 01:06:05.960
large language models?

01:06:05.960 --> 01:06:08.960
So I think large language models

01:06:08.960 --> 01:06:12.960
in isolation as we currently conceive them

01:06:12.960 --> 01:06:17.960
are probably not presenting that kind of catastrophic risk.

01:06:17.960 --> 01:06:19.960
I think they present many, many risks.

01:06:19.960 --> 01:06:21.960
And the open letter talks about

01:06:21.960 --> 01:06:25.960
some of those disinformation bias, et cetera, et cetera.

01:06:25.960 --> 01:06:29.960
So I think there are already many reasons

01:06:29.960 --> 01:06:31.960
that these systems would fail

01:06:31.960 --> 01:06:36.960
any reasonable safety criteria.

01:06:36.960 --> 01:06:40.960
But the concern is that it's not just,

01:06:40.960 --> 01:06:43.960
we're not just going to make these models bigger.

01:06:43.960 --> 01:06:47.960
We are also going to try to figure out

01:06:47.960 --> 01:06:51.960
how can they be arranged

01:06:51.960 --> 01:06:54.960
so that they actually develop

01:06:54.960 --> 01:06:56.960
a consistent internal model of the world?

01:06:56.960 --> 01:06:58.960
How can they be arranged

01:06:58.960 --> 01:07:02.960
so that they can also do long-term planning?

01:07:02.960 --> 01:07:04.960
En, as I say,

01:07:04.960 --> 01:07:07.960
we don't really know the answers to those questions yet,

01:07:07.960 --> 01:07:12.960
but I think that the ideas behind

01:07:12.960 --> 01:07:14.960
large language models

01:07:14.960 --> 01:07:19.960
do form a significant piece of the puzzle.

01:07:19.960 --> 01:07:24.960
And so the concern is that future generations of these systems,

01:07:24.960 --> 01:07:26.960
which will be extended,

01:07:26.960 --> 01:07:28.960
not just in scale,

01:07:28.960 --> 01:07:33.960
but also in the additional capabilities

01:07:33.960 --> 01:07:36.960
that we might endow them with

01:07:36.960 --> 01:07:42.960
by maybe a more design-based approach,

01:07:42.960 --> 01:07:46.960
that those systems would start to

01:07:46.960 --> 01:07:51.960
get close to presenting a real threat.

01:07:51.960 --> 01:07:54.960
En so in some ways,

01:07:54.960 --> 01:07:56.960
I think this title of that paper,

01:07:56.960 --> 01:08:01.960
Sparks of Artificial General Intelligence,

01:08:01.960 --> 01:08:07.960
is not wrong.

01:08:07.960 --> 01:08:10.960
And I think when you think about

01:08:10.960 --> 01:08:11.960
what does sparks mean,

01:08:11.960 --> 01:08:15.960
what sparks are a predecessor to a fire.

01:08:15.960 --> 01:08:23.960
En I think that's what we want to prevent.

01:08:23.960 --> 01:08:24.960
En with that,

01:08:24.960 --> 01:08:27.960
we come to the end of the first part of this evening.

01:08:27.960 --> 01:08:28.960
Professor Russell,

01:08:28.960 --> 01:08:30.960
I'd like to thank you for elaborating

01:08:30.960 --> 01:08:32.960
in such a concise way

01:08:32.960 --> 01:08:35.960
the dangers of developing intelligent AI

01:08:35.960 --> 01:08:39.960
and to provide a manual, so to speak,

01:08:39.960 --> 01:08:41.960
to steer away from that cliff

01:08:41.960 --> 01:08:46.960
and to develop safe, beneficial artificial intelligence

01:08:46.960 --> 01:08:48.960
that is aligned with our goals.

01:08:48.960 --> 01:08:50.960
So join me in a big round of applause

01:08:50.960 --> 01:08:52.960
for Professor Russell.

01:09:02.960 --> 01:09:03.960
En with that,

01:09:03.960 --> 01:09:05.960
we're going to leave you, Professor Russell,

01:09:05.960 --> 01:09:07.960
and I would like to invite the five panelists

01:09:07.960 --> 01:09:10.960
to continue the conversation.

01:09:10.960 --> 01:09:14.960
Please come to the floor.

01:09:14.960 --> 01:09:18.960
And I'll introduce you properly.

01:09:18.960 --> 01:09:20.960
Marc Brakel, to the right,

01:09:20.960 --> 01:09:24.960
he's a director of policy at Future Life Institute,

01:09:24.960 --> 01:09:26.960
involved in the AI Act

01:09:26.960 --> 01:09:29.960
that is currently being prepared

01:09:29.960 --> 01:09:32.960
and is later this year going to be proposed

01:09:32.960 --> 01:09:34.960
by the European Union, am I right?

01:09:34.960 --> 01:09:35.960
It's already been proposed.

01:09:35.960 --> 01:09:36.960
Oh, it's already been proposed.

01:09:36.960 --> 01:09:37.960
Hopefully it gets voted through.

01:09:37.960 --> 01:09:38.960
Voted through.

01:09:38.960 --> 01:09:39.960
I should say that, yeah.

01:09:39.960 --> 01:09:41.960
Then we have Tim Bakker,

01:09:41.960 --> 01:09:45.960
who is a PhD student at the University of Amsterdam.

01:09:45.960 --> 01:09:47.960
The title of your thesis?

01:09:47.960 --> 01:09:48.960
My thesis?

01:09:48.960 --> 01:09:49.960
Yeah.

01:09:49.960 --> 01:09:50.960
I don't know yet.

01:09:50.960 --> 01:09:51.960
You don't know?

01:09:51.960 --> 01:09:52.960
Live Hanger.

01:09:52.960 --> 01:09:53.960
Chat GPT, yeah.

01:09:53.960 --> 01:09:54.960
Little secrets.

01:09:54.960 --> 01:09:57.960
Working on AI research.

01:09:57.960 --> 01:10:00.960
Then we have Nandi Robijns,

01:10:00.960 --> 01:10:02.960
who is working at the Ministry of Interior

01:10:02.960 --> 01:10:04.960
and Kingdom Relations,

01:10:04.960 --> 01:10:07.960
part of a crew of AI and data consultants.

01:10:07.960 --> 01:10:09.960
About 70 people strong.

01:10:09.960 --> 01:10:11.960
I just heard over dinner.

01:10:11.960 --> 01:10:13.960
Nice of you to be here.

01:10:13.960 --> 01:10:15.960
Two members of the Dutch parliament.

01:10:15.960 --> 01:10:17.960
Queenie Rijkovski,

01:10:17.960 --> 01:10:19.960
who is a member of the Liberal Party,

01:10:19.960 --> 01:10:20.960
the VVD,

01:10:20.960 --> 01:10:23.960
and has cyber security and digitisation

01:10:23.960 --> 01:10:24.960
in your portfolio.

01:10:24.960 --> 01:10:26.960
And last but not least,

01:10:26.960 --> 01:10:28.960
Lammert van Raam,

01:10:28.960 --> 01:10:30.960
who is a member of parliament

01:10:30.960 --> 01:10:32.960
for the party for the animals,

01:10:32.960 --> 01:10:36.960
who is focusing on IT and privacy issues.

01:10:36.960 --> 01:10:38.960
Great of you guys to come over

01:10:38.960 --> 01:10:40.960
and stand here and discuss with us

01:10:40.960 --> 01:10:42.960
the dangers of AI

01:10:42.960 --> 01:10:44.960
and how to deal with them.

01:10:44.960 --> 01:10:47.960
Let me just start off with the obvious question.

01:10:47.960 --> 01:10:51.960
Professor Russell just painted a rather scary picture

01:10:51.960 --> 01:10:55.960
of humanity that may one day fear of a cliff

01:10:55.960 --> 01:10:59.960
because we don't control the risks involved in AI.

01:10:59.960 --> 01:11:01.960
Do you share this view?

01:11:01.960 --> 01:11:04.960
Do you also think artificial intelligence

01:11:04.960 --> 01:11:05.960
may sooner of later,

01:11:05.960 --> 01:11:07.960
if we don't control as well,

01:11:07.960 --> 01:11:09.960
steer humanity over a cliff?

01:11:10.960 --> 01:11:11.960
I work at an organisation

01:11:11.960 --> 01:11:13.960
at the Future of Life Institute

01:11:13.960 --> 01:11:14.960
that believes this,

01:11:14.960 --> 01:11:16.960
so it doesn't come as a surprise,

01:11:16.960 --> 01:11:18.960
I think, to say that I agree

01:11:18.960 --> 01:11:21.960
with Stuart Eastman of our advisors also

01:11:21.960 --> 01:11:24.960
and just in response to the last question also

01:11:24.960 --> 01:11:27.960
about our open letter that we put out

01:11:27.960 --> 01:11:29.960
I think it's now a week and a half ago.

01:11:29.960 --> 01:11:31.960
This is the first event where I'm at

01:11:31.960 --> 01:11:34.960
with actual people since we put the letter out.

01:11:35.960 --> 01:11:38.960
I think it's really worth reading that

01:11:38.960 --> 01:11:39.960
because the letter,

01:11:39.960 --> 01:11:41.960
the open letter that we presented

01:11:41.960 --> 01:11:43.960
talks about all kinds of risks

01:11:43.960 --> 01:11:45.960
that our society might struggle with

01:11:45.960 --> 01:11:46.960
when it comes to AI,

01:11:46.960 --> 01:11:48.960
not just the existential risk.

01:11:48.960 --> 01:11:49.960
Right.

01:11:49.960 --> 01:11:51.960
And I think our first contact with AI

01:11:51.960 --> 01:11:53.960
as Stuart Russell also highlighted

01:11:53.960 --> 01:11:54.960
was social media

01:11:54.960 --> 01:11:56.960
with a super simple algorithm

01:11:56.960 --> 01:11:58.960
our second contact with AI

01:11:58.960 --> 01:12:00.960
is probably these large neural networks

01:12:00.960 --> 01:12:02.960
and I think we're going to really struggle

01:12:02.960 --> 01:12:04.960
to control truths,

01:12:04.960 --> 01:12:06.960
to control access to

01:12:06.960 --> 01:12:08.960
what was previously very hard

01:12:08.960 --> 01:12:10.960
to access information.

01:12:10.960 --> 01:12:12.960
So yes, I worry about existential risks,

01:12:12.960 --> 01:12:13.960
I agree with Stuart,

01:12:13.960 --> 01:12:15.960
but I also think beneath that

01:12:15.960 --> 01:12:17.960
there's a layer of very, very serious risks

01:12:17.960 --> 01:12:19.960
that is also a cause of worry.

01:12:19.960 --> 01:12:20.960
Right.

01:12:20.960 --> 01:12:22.960
We'll touch upon them probably later.

01:12:22.960 --> 01:12:24.960
Ja.

01:12:24.960 --> 01:12:26.960
No, I definitely agree with

01:12:26.960 --> 01:12:28.960
Professor Russell about his worries

01:12:28.960 --> 01:12:30.960
and actually also with Mark

01:12:30.960 --> 01:12:31.960
about what he just said.

01:12:31.960 --> 01:12:33.960
I think Stuart Russell was very right

01:12:33.960 --> 01:12:35.960
about pointing to the fundamental problem

01:12:35.960 --> 01:12:36.960
with the current systems

01:12:36.960 --> 01:12:38.960
which is that we're training them

01:12:38.960 --> 01:12:40.960
as optimizers

01:12:40.960 --> 01:12:42.960
instead of as things that

01:12:42.960 --> 01:12:44.960
do anything that is not that

01:12:44.960 --> 01:12:46.960
because that is just such a hard thing

01:12:46.960 --> 01:12:48.960
to aim in a way that we

01:12:48.960 --> 01:12:49.960
want to aim it.

01:12:49.960 --> 01:12:51.960
We just have no idea how to do that.

01:12:51.960 --> 01:12:52.960
Dandy.

01:12:52.960 --> 01:12:54.960
Yes.

01:12:54.960 --> 01:12:57.960
I think it is very important to take into account

01:12:57.960 --> 01:12:59.960
a wide range of potential risks of AI,

01:12:59.960 --> 01:13:01.960
especially because AI is such

01:13:01.960 --> 01:13:04.960
extremely powerful technology

01:13:04.960 --> 01:13:07.960
and I think what we talk about today

01:13:07.960 --> 01:13:10.960
is a very important part of this range of risks,

01:13:10.960 --> 01:13:12.960
especially because of the scale

01:13:12.960 --> 01:13:15.960
of the potential negative impact that it can have.

01:13:15.960 --> 01:13:17.960
And on top of that, it is very neglected

01:13:17.960 --> 01:13:19.960
and this neglect is worrying me,

01:13:19.960 --> 01:13:22.960
especially because as we see

01:13:22.960 --> 01:13:25.960
more and more AI systems are extremely capable

01:13:25.960 --> 01:13:28.960
in achieving their programmed goals

01:13:28.960 --> 01:13:31.960
and the main worry

01:13:31.960 --> 01:13:33.960
about these AI systems becoming dangerous

01:13:33.960 --> 01:13:36.960
is rooted in the fact that they pursue these goals

01:13:36.960 --> 01:13:39.960
regardless of whether or not it is what we intended.

01:13:39.960 --> 01:13:42.960
Ja, so that needs to be addressed

01:13:42.960 --> 01:13:45.960
and on top of that, these models,

01:13:45.960 --> 01:13:47.960
no one knows what is going on inside these models,

01:13:47.960 --> 01:13:50.960
as George also also said

01:13:50.960 --> 01:13:52.960
and no one actually knows

01:13:52.960 --> 01:13:55.960
how we can define a goal

01:13:55.960 --> 01:13:57.960
that takes into account every value

01:13:57.960 --> 01:13:58.960
that we care about,

01:13:58.960 --> 01:14:00.960
which is also what we just talked about.

01:14:00.960 --> 01:14:02.960
So yes, I do agree

01:14:02.960 --> 01:14:05.960
that it needs much more attention.

01:14:05.960 --> 01:14:06.960
Queenie.

01:14:06.960 --> 01:14:08.960
Yes, I agree even though

01:14:08.960 --> 01:14:11.960
I am a tech-optimistical person.

01:14:11.960 --> 01:14:15.960
So when it comes to technologies like AI,

01:14:15.960 --> 01:14:17.960
I can definitely see the risk in the downside.

01:14:17.960 --> 01:14:20.960
So I completely agree with the other speakers

01:14:20.960 --> 01:14:23.960
and also when we just heard in a presentation

01:14:23.960 --> 01:14:27.960
that if we, AI can also see maybe human

01:14:27.960 --> 01:14:31.960
as a danger when it comes to climate or climate change.

01:14:31.960 --> 01:14:33.960
We need to really think about

01:14:33.960 --> 01:14:35.960
how we are going to program it.

01:14:35.960 --> 01:14:36.960
What are the goals?

01:14:36.960 --> 01:14:38.960
I think we just heard some examples.

01:14:38.960 --> 01:14:40.960
And one of the experiments that they have been doing,

01:14:40.960 --> 01:14:43.960
TNO, it's a scientific research company

01:14:43.960 --> 01:14:45.960
in de Nederlands organization.

01:14:45.960 --> 01:14:47.960
En they've also done some smaller

01:14:47.960 --> 01:14:48.960
and some bigger experiments.

01:14:48.960 --> 01:14:50.960
And the smaller experiment is

01:14:50.960 --> 01:14:52.960
a robotic vacuum cleaner.

01:14:52.960 --> 01:14:54.960
And they said, okay, your task is

01:14:54.960 --> 01:14:57.960
to keep this clean room dust free.

01:14:57.960 --> 01:14:59.960
And what happened?

01:14:59.960 --> 01:15:02.960
The robot started to block the door.

01:15:02.960 --> 01:15:05.960
Because every time when a human came in,

01:15:05.960 --> 01:15:06.960
the room was dirty.

01:15:06.960 --> 01:15:07.960
You were the actual problem, yeah.

01:15:07.960 --> 01:15:08.960
Yeah, so it's,

01:15:08.960 --> 01:15:11.960
and this is just like a small example, of course.

01:15:11.960 --> 01:15:14.960
But what it got me thinking is

01:15:14.960 --> 01:15:17.960
not only what assignment or what goal

01:15:17.960 --> 01:15:20.960
do you give a system or robot, et cetera.

01:15:20.960 --> 01:15:23.960
But also can you grasp upon

01:15:23.960 --> 01:15:26.960
what the outcomes can be when you ask something.

01:15:26.960 --> 01:15:30.960
And actually when it comes to equality,

01:15:30.960 --> 01:15:34.960
I think AI can maybe even help

01:15:34.960 --> 01:15:36.960
because in my experience

01:15:36.960 --> 01:15:39.960
being a woman in politics,

01:15:39.960 --> 01:15:41.960
working on tech,

01:15:41.960 --> 01:15:45.960
mostly a lot of men around me

01:15:45.960 --> 01:15:47.960
in my context,

01:15:47.960 --> 01:15:51.960
I still hear people say things.

01:15:51.960 --> 01:15:55.960
I still see some articles written in a way

01:15:55.960 --> 01:15:57.960
that they write different when you're a woman

01:15:57.960 --> 01:15:58.960
than when you're a man.

01:15:58.960 --> 01:16:00.960
So actually, I hope,

01:16:00.960 --> 01:16:01.960
so that's also my goal

01:16:01.960 --> 01:16:03.960
from a political perspective,

01:16:03.960 --> 01:16:04.960
if we can make sure

01:16:04.960 --> 01:16:07.960
that we provide the right regulation

01:16:07.960 --> 01:16:10.960
and control when it comes to AI,

01:16:10.960 --> 01:16:13.960
maybe we can even help equality

01:16:13.960 --> 01:16:15.960
instead of being a danger to it.

01:16:15.960 --> 01:16:16.960
Okay, interesting.

01:16:16.960 --> 01:16:18.960
We may continue that conversation

01:16:18.960 --> 01:16:20.960
later this evening.

01:16:20.960 --> 01:16:22.960
Nasty little buggers,

01:16:22.960 --> 01:16:24.960
those vacuum cleaners, right?

01:16:24.960 --> 01:16:26.960
I have.

01:16:26.960 --> 01:16:28.960
So, you have one?

01:16:28.960 --> 01:16:29.960
Yes.

01:16:29.960 --> 01:16:30.960
But you're not locked out yet?

01:16:30.960 --> 01:16:31.960
No, not yet.

01:16:31.960 --> 01:16:33.960
Good for you.

01:16:33.960 --> 01:16:36.960
So, yeah, worrying.

01:16:36.960 --> 01:16:42.960
I was taking comfort from the example of Rutherford

01:16:42.960 --> 01:16:44.960
and the next day,

01:16:44.960 --> 01:16:48.960
Zillar invented something that made it possible.

01:16:48.960 --> 01:16:51.960
Perhaps it's now the 12th of April

01:16:51.960 --> 01:16:53.960
on 13th of April.

01:16:53.960 --> 01:16:56.960
There's one Zillar in the room already

01:16:56.960 --> 01:16:57.960
making a solution

01:16:57.960 --> 01:16:59.960
because that is what possible.

01:16:59.960 --> 01:17:01.960
And yes, it is worrying.

01:17:01.960 --> 01:17:02.960
It is worrying.

01:17:02.960 --> 01:17:04.960
I have only one consolation.

01:17:04.960 --> 01:17:07.960
A politician will probably have to solve it.

01:17:07.960 --> 01:17:09.960
And then it's...

01:17:09.960 --> 01:17:11.960
I don't know if it's a consolation, to be honest.

01:17:11.960 --> 01:17:13.960
But...

01:17:13.960 --> 01:17:15.960
It's the best we have, right?

01:17:15.960 --> 01:17:19.960
It's the best we have politicians in a democracy.

01:17:19.960 --> 01:17:21.960
Nevertheless, we are working together

01:17:21.960 --> 01:17:24.960
on the concerns we both feel.

01:17:24.960 --> 01:17:26.960
And I think that's giving some hope

01:17:26.960 --> 01:17:29.960
because she's a completely different ideology

01:17:29.960 --> 01:17:31.960
than my party.

01:17:31.960 --> 01:17:34.960
En stil we find the same common ground

01:17:34.960 --> 01:17:36.960
in our concerns.

01:17:36.960 --> 01:17:38.960
En dat is, I think,

01:17:38.960 --> 01:17:40.960
something to look forward.

01:17:40.960 --> 01:17:42.960
The other consolation is we don't have to worry

01:17:42.960 --> 01:17:45.960
about falling off the cliff

01:17:45.960 --> 01:17:48.960
because we are already sliding off the cliff.

01:17:48.960 --> 01:17:50.960
That's very comforting, yeah.

01:17:50.960 --> 01:17:52.960
And for you, Otto,

01:17:52.960 --> 01:17:54.960
the first slide, I have to say it, I'm sorry,

01:17:54.960 --> 01:17:57.960
the first slide with all the risks,

01:17:57.960 --> 01:17:59.960
don't show it to the animals

01:17:59.960 --> 01:18:02.960
because they're already in a massive wave.

01:18:02.960 --> 01:18:05.960
And don't show it to the global south.

01:18:05.960 --> 01:18:08.960
But there are ways probably

01:18:08.960 --> 01:18:10.960
if we can get this problem solved.

01:18:10.960 --> 01:18:13.960
If there are enough zillars in the room,

01:18:13.960 --> 01:18:15.960
we're counting on you.

01:18:15.960 --> 01:18:17.960
We can also solve it politically

01:18:17.960 --> 01:18:19.960
because the worries are very real.

01:18:19.960 --> 01:18:21.960
So thank you.

01:18:21.960 --> 01:18:24.960
So you all share, more or less,

01:18:24.960 --> 01:18:28.960
the alarming story that Professor Russell just told us.

01:18:28.960 --> 01:18:31.960
But at the same time, I don't see us all going to the streets

01:18:31.960 --> 01:18:34.960
and protesting like we do with climate change

01:18:34.960 --> 01:18:39.960
or in the 1980s we did with the risk of nuclear war.

01:18:39.960 --> 01:18:41.960
So what's wrong here?

01:18:41.960 --> 01:18:45.960
Why aren't we, if we all share this great risk

01:18:45.960 --> 01:18:47.960
or concern for this great risk,

01:18:47.960 --> 01:18:49.960
why are we not protesting?

01:18:49.960 --> 01:18:51.960
What's wrong with us, wants to answer?

01:18:51.960 --> 01:18:54.960
Nandi, you actually raised this point yourself.

01:18:55.960 --> 01:18:57.960
So solve it.

01:18:57.960 --> 01:19:00.960
We can't.

01:19:00.960 --> 01:19:04.960
Ja, so I feel like there are some reasons.

01:19:04.960 --> 01:19:07.960
The question is about...

01:19:07.960 --> 01:19:10.960
Well, if the risk is so big for us as a society,

01:19:10.960 --> 01:19:12.960
why aren't we, you know,

01:19:12.960 --> 01:19:14.960
talking about this daily in parliament

01:19:14.960 --> 01:19:16.960
and protesting on the square.

01:19:16.960 --> 01:19:19.960
Yes, I think there are some reasons

01:19:19.960 --> 01:19:22.960
that make it hard for people to realize one

01:19:22.960 --> 01:19:24.960
that this is a real problem

01:19:24.960 --> 01:19:26.960
and to see that this is something

01:19:26.960 --> 01:19:29.960
that needs to be addressed right now.

01:19:29.960 --> 01:19:33.960
And I think one of the reasons is that these aspects,

01:19:33.960 --> 01:19:35.960
these concepts that we talk about

01:19:35.960 --> 01:19:37.960
and the terms that we use are still quite vague

01:19:37.960 --> 01:19:41.960
and difficult for people to understand.

01:19:41.960 --> 01:19:43.960
And sadly, vague problems are much easier

01:19:43.960 --> 01:19:45.960
to ignore than concrete ones,

01:19:45.960 --> 01:19:47.960
which also makes it harder

01:19:47.960 --> 01:19:49.960
for policymakers to prioritize,

01:19:49.960 --> 01:19:52.960
things that are a bit more uncertain and vague

01:19:52.960 --> 01:19:55.960
over things that are more concrete

01:19:55.960 --> 01:19:57.960
and where we can see the harm

01:19:57.960 --> 01:20:00.960
right in front of us right now.

01:20:00.960 --> 01:20:03.960
A second reason also is that, you know,

01:20:03.960 --> 01:20:06.960
this is also, to some people,

01:20:06.960 --> 01:20:09.960
at first glance, quickly dismissed as science fiction,

01:20:09.960 --> 01:20:12.960
not real or something that doesn't need attention right now.

01:20:12.960 --> 01:20:17.960
Ja, which I think is a misconception.

01:20:18.960 --> 01:20:21.960
So yeah, I think this is caused

01:20:21.960 --> 01:20:23.960
by a lack of awareness and understanding

01:20:23.960 --> 01:20:26.960
and a lack of urgency that we need to address.

01:20:26.960 --> 01:20:28.960
At PR, yeah.

01:20:28.960 --> 01:20:30.960
First Lamert, and then it goes to you.

01:20:30.960 --> 01:20:33.960
I fully agree with you that there's a lack of knowledge, et cetera.

01:20:33.960 --> 01:20:35.960
But perhaps it's also,

01:20:35.960 --> 01:20:37.960
perhaps the protest is already there,

01:20:37.960 --> 01:20:39.960
but we just don't recognize it.

01:20:39.960 --> 01:20:42.960
For instance, there's, well,

01:20:42.960 --> 01:20:45.960
the best example, of course, is the upheaval there was

01:20:45.960 --> 01:20:49.960
in the Netherlands, of the toeslagen schandaal.

01:20:49.960 --> 01:20:53.960
And algorithms played a very big role in that.

01:20:53.960 --> 01:20:58.960
And also in the, let's say, discrimination factor of that.

01:20:58.960 --> 01:21:01.960
And that led to the fall of the government.

01:21:01.960 --> 01:21:03.960
So there was a big upheaval,

01:21:03.960 --> 01:21:06.960
but we didn't perhaps recognize it as such.

01:21:06.960 --> 01:21:08.960
So perhaps there is a lot of upheaval,

01:21:08.960 --> 01:21:11.960
but we just have to categorize it differently to understand it.

01:21:11.960 --> 01:21:15.960
But I agree fully with you that there's also a very,

01:21:15.960 --> 01:21:19.960
a lack of understanding and a forelichting.

01:21:19.960 --> 01:21:21.960
What is that?

01:21:21.960 --> 01:21:22.960
Education, thank you.

01:21:22.960 --> 01:21:23.960
Thank you, George.

01:21:23.960 --> 01:21:24.960
Mark.

01:21:24.960 --> 01:21:28.960
Yeah, if I could maybe add two sort of points of optimism to that.

01:21:28.960 --> 01:21:31.960
I think when Otto first asked about sort of doing this event,

01:21:31.960 --> 01:21:33.960
it was going to be in the smallest room of this building.

01:21:33.960 --> 01:21:34.960
Right.

01:21:34.960 --> 01:21:36.960
And it sold out, and now we're in the big room.

01:21:36.960 --> 01:21:40.960
So I think that shows that I think society is moving

01:21:40.960 --> 01:21:43.960
maybe slower than the development of the systems.

01:21:43.960 --> 01:21:45.960
But still, it's of interest to more people.

01:21:45.960 --> 01:21:49.960
And when we sat together with the Future of Life Institute,

01:21:49.960 --> 01:21:53.960
with my sort of 16 colleagues four weeks ago brainstorming this letter,

01:21:53.960 --> 01:21:56.960
we thought, okay, maybe we can have four news outlets cover it,

01:21:56.960 --> 01:21:58.960
mainly in the United States.

01:21:58.960 --> 01:22:01.960
Potentially we get one in Europe, that'd be great.

01:22:01.960 --> 01:22:04.960
And a colleague of mine comes from rural Australia,

01:22:04.960 --> 01:22:08.960
and her mum had heard it at the hairdressers on the radio show.

01:22:08.960 --> 01:22:10.960
And I think that shows that.

01:22:10.960 --> 01:22:13.960
Great source of information, the hairdressers recommended.

01:22:13.960 --> 01:22:16.960
People are slowly waking up to the risk.

01:22:16.960 --> 01:22:18.960
And the Overton window is also shifting.

01:22:18.960 --> 01:22:20.960
I think a lot of governments are waking up to the fact

01:22:20.960 --> 01:22:23.960
that they need to regulate this, and really quite quickly.

01:22:23.960 --> 01:22:24.960
So we're going in the right direction.

01:22:24.960 --> 01:22:27.960
Are we all happy with the direction we're going here?

01:22:27.960 --> 01:22:28.960
Or are we, yeah?

01:22:28.960 --> 01:22:31.960
I mean, I don't want to be overly optimistic.

01:22:31.960 --> 01:22:34.960
I mean, the one thing that worries me is companies,

01:22:34.960 --> 01:22:37.960
because we have your Russell's proposal here to say,

01:22:37.960 --> 01:22:38.960
okay, we need to look at AI systems,

01:22:38.960 --> 01:22:40.960
and we need to make sure that they are uncertain

01:22:40.960 --> 01:22:41.960
about what our objectives are.

01:22:41.960 --> 01:22:42.960
Right.

01:22:42.960 --> 01:22:43.960
Whereas all of the investment,

01:22:43.960 --> 01:22:45.960
and the economist in an article last week,

01:22:45.960 --> 01:22:48.960
just saying how it escalated since chat GPT,

01:22:48.960 --> 01:22:50.960
how many more billions are suddenly available to invest,

01:22:50.960 --> 01:22:53.960
are all going to neural nets that do exactly the opposite.

01:22:53.960 --> 01:22:56.960
Everyone is clueless as to what these systems do,

01:22:56.960 --> 01:22:59.960
including the chief technology officer of open AI

01:22:59.960 --> 01:23:01.960
who goes on TV and says that.

01:23:01.960 --> 01:23:04.960
I'm interested to hear your comments on this, Tim,

01:23:04.960 --> 01:23:07.960
because you actually interned at Facebook AI.

01:23:07.960 --> 01:23:08.960
Yes.

01:23:08.960 --> 01:23:11.960
Which was a long story, and quite eccentric,

01:23:11.960 --> 01:23:12.960
what you did there.

01:23:12.960 --> 01:23:13.960
I'm going to have to fed myself now.

01:23:13.960 --> 01:23:15.960
No, you don't.

01:23:15.960 --> 01:23:17.960
He was not working on the metaphors.

01:23:17.960 --> 01:23:20.960
But please tell me, how do you view this danger?

01:23:20.960 --> 01:23:22.960
Yeah, so it's, I mean, it's interesting,

01:23:22.960 --> 01:23:24.960
because I've been worried about these topics

01:23:24.960 --> 01:23:25.960
for a very long time,

01:23:25.960 --> 01:23:27.960
and I've now been involved with AI

01:23:27.960 --> 01:23:30.960
for a bit less than that, but still.

01:23:30.960 --> 01:23:33.960
And it's been interesting to see the shift in opinions,

01:23:33.960 --> 01:23:36.960
like, as Nandi said, people at the start really thought,

01:23:36.960 --> 01:23:38.960
okay, this is some kind of science fiction

01:23:38.960 --> 01:23:39.960
why you worried about this.

01:23:39.960 --> 01:23:42.960
If you look at these systems, they're not able to do anything.

01:23:42.960 --> 01:23:45.960
It makes no sense at all, stop worrying.

01:23:45.960 --> 01:23:47.960
And in the last, say, two years,

01:23:47.960 --> 01:23:49.960
but especially the last two months,

01:23:49.960 --> 01:23:51.960
this has really changed in the community as well.

01:23:51.960 --> 01:23:53.960
There's been so many researchers

01:23:53.960 --> 01:23:55.960
that are now coming out as, oh yeah, actually,

01:23:55.960 --> 01:23:56.960
I am kind of worried,

01:23:56.960 --> 01:23:58.960
and I actually have been worried for a while,

01:23:58.960 --> 01:23:59.960
but I kind of couldn't say,

01:23:59.960 --> 01:24:01.960
because it was just such a weird thing,

01:24:01.960 --> 01:24:02.960
and I couldn't really, you know,

01:24:02.960 --> 01:24:04.960
probably if I said that to my colleagues,

01:24:04.960 --> 01:24:05.960
who would call me crazy.

01:24:05.960 --> 01:24:07.960
And now you have people like Geoffrey Hinton,

01:24:07.960 --> 01:24:10.960
who is sort of often considered

01:24:10.960 --> 01:24:11.960
the godfather of modern AI,

01:24:11.960 --> 01:24:13.960
going on national television in an interview

01:24:13.960 --> 01:24:15.960
and saying, yeah, it's not inconceivable

01:24:15.960 --> 01:24:18.960
that AI will wipe us all out.

01:24:18.960 --> 01:24:20.960
And also, I don't know what to do about that.

01:24:20.960 --> 01:24:23.960
And so, it's become much more of an okay thing

01:24:23.960 --> 01:24:25.960
to worry about, and I think that is quite hopeful.

01:24:25.960 --> 01:24:27.960
Of course, that doesn't mean we know

01:24:27.960 --> 01:24:29.960
how to solve the problem yet,

01:24:29.960 --> 01:24:32.960
but at least we're allowed

01:24:32.960 --> 01:24:34.960
as a scientific community

01:24:34.960 --> 01:24:35.960
and also as a tech community

01:24:35.960 --> 01:24:37.960
to at least consider these problems seriously,

01:24:37.960 --> 01:24:39.960
and that's very good.

01:24:39.960 --> 01:24:41.960
I'm also interested to hear the comments

01:24:41.960 --> 01:24:43.960
of the two parliamentarians here,

01:24:43.960 --> 01:24:45.960
because, well, your job, partly,

01:24:45.960 --> 01:24:47.960
is to devise laws

01:24:47.960 --> 01:24:49.960
and to think about how we could improve

01:24:49.960 --> 01:24:52.960
the well-being of us people here in the Netherlands.

01:24:52.960 --> 01:24:54.960
So, what do you think is the risk

01:24:54.960 --> 01:24:58.960
of tech companies devising new AI systems

01:24:58.960 --> 01:25:02.960
that may not be aligned with our well-being?

01:25:02.960 --> 01:25:04.960
En what can you do about that?

01:25:04.960 --> 01:25:05.960
Biggest party first?

01:25:05.960 --> 01:25:07.960
Ja, ja, well, I think the...

01:25:07.960 --> 01:25:09.960
Thanks.

01:25:09.960 --> 01:25:10.960
That's a privilege.

01:25:10.960 --> 01:25:11.960
Very generous.

01:25:11.960 --> 01:25:12.960
I think...

01:25:12.960 --> 01:25:15.960
No, well, of course, like the risks,

01:25:15.960 --> 01:25:17.960
I think some of the risks we already talked about,

01:25:17.960 --> 01:25:20.960
because in January, I think,

01:25:20.960 --> 01:25:24.960
we had a big debate about AI in the parliament,

01:25:24.960 --> 01:25:27.960
and one of the things that we also talked about

01:25:27.960 --> 01:25:29.960
was, OK, so...

01:25:29.960 --> 01:25:32.960
But, and you see it now with the open letter.

01:25:32.960 --> 01:25:35.960
The people who write the code

01:25:35.960 --> 01:25:38.960
are worried about what's going to happen with AI.

01:25:38.960 --> 01:25:40.960
So, that's a bit, you know,

01:25:40.960 --> 01:25:43.960
if someone can do something about how a software,

01:25:43.960 --> 01:25:45.960
how a large language model,

01:25:45.960 --> 01:25:47.960
how AI is working,

01:25:47.960 --> 01:25:50.960
it starts with the person who's making it, right?

01:25:50.960 --> 01:25:53.960
So, we also discussed,

01:25:53.960 --> 01:25:56.960
so can you also maybe start looking maybe

01:25:56.960 --> 01:25:59.960
over engineers, professions like that,

01:25:59.960 --> 01:26:02.960
and not only teach them at university

01:26:02.960 --> 01:26:05.960
how to write good code, efficient code, et cetera,

01:26:05.960 --> 01:26:10.960
but also take into account ethics, human rights, et cetera.

01:26:10.960 --> 01:26:14.960
So, where the programming starts,

01:26:14.960 --> 01:26:18.960
also ethics and safety is taken into account.

01:26:18.960 --> 01:26:20.960
The core of the curriculum, ja.

01:26:20.960 --> 01:26:23.960
Exactly, and one thing which I think is hopeful

01:26:23.960 --> 01:26:26.960
is that when you look at social media, big tech, the internet,

01:26:26.960 --> 01:26:30.960
at first everyone starts, oh, it's going to regulate itself, right?

01:26:30.960 --> 01:26:33.960
We only see positive things,

01:26:33.960 --> 01:26:36.960
and human rights will fix this itself,

01:26:36.960 --> 01:26:39.960
and now politics are waking up, oh, wait a minute,

01:26:39.960 --> 01:26:42.960
social media, big tech companies,

01:26:42.960 --> 01:26:44.960
they are a lot about making money

01:26:44.960 --> 01:26:46.960
and not about taking responsibility

01:26:46.960 --> 01:26:49.960
to make sure that they have a good contribution

01:26:49.960 --> 01:26:52.960
to the country that they make money in.

01:26:52.960 --> 01:26:55.960
We are trying to repair that,

01:26:55.960 --> 01:26:57.960
but it's too late,

01:26:57.960 --> 01:26:59.960
there's already a big power,

01:26:59.960 --> 01:27:01.960
they already decide a lot,

01:27:01.960 --> 01:27:03.960
maybe they have even more power

01:27:03.960 --> 01:27:05.960
than a lot of governments have.

01:27:05.960 --> 01:27:07.960
And what I see with AI,

01:27:07.960 --> 01:27:09.960
especially on the European level,

01:27:09.960 --> 01:27:12.960
that the European Commission already started to work on AI regulation

01:27:12.960 --> 01:27:14.960
two years ago.

01:27:14.960 --> 01:27:16.960
So, what is hopeful for me

01:27:16.960 --> 01:27:19.960
is that we already started also from a political point of view,

01:27:19.960 --> 01:27:21.960
because experts are thinking about this,

01:27:21.960 --> 01:27:23.960
way longer than that,

01:27:23.960 --> 01:27:25.960
but also from a political side,

01:27:25.960 --> 01:27:27.960
the thinking has already begun,

01:27:27.960 --> 01:27:29.960
the law regulation is already in the making,

01:27:29.960 --> 01:27:31.960
not only in Europe, but also worldwide,

01:27:31.960 --> 01:27:33.960
they are working with treaties, et cetera.

01:27:33.960 --> 01:27:35.960
So, for me that's hopeful,

01:27:35.960 --> 01:27:39.960
in a way that it helps me

01:27:39.960 --> 01:27:42.960
in thinking that, okay, AI can still do wrong,

01:27:42.960 --> 01:27:45.960
but maybe we are not too late.

01:27:45.960 --> 01:27:48.960
Lamert, how do you view this?

01:27:48.960 --> 01:27:53.960
It seems to me,

01:27:53.960 --> 01:27:58.960
that it's like a winner takes all industry.

01:27:58.960 --> 01:28:02.960
Like, to a certain extent,

01:28:02.960 --> 01:28:05.960
the fossil industry is, wasn't is.

01:28:05.960 --> 01:28:07.960
So, and we have a very,

01:28:07.960 --> 01:28:10.960
that industry has a very bad track record.

01:28:10.960 --> 01:28:15.960
So, I would be inclined to give

01:28:15.960 --> 01:28:17.960
the big companies the,

01:28:17.960 --> 01:28:19.960
not the benefit of the doubt,

01:28:19.960 --> 01:28:21.960
but the disadvantage of the certainty

01:28:21.960 --> 01:28:23.960
that they are still in the race

01:28:23.960 --> 01:28:26.960
in a winner takes all situation.

01:28:26.960 --> 01:28:30.960
So, again, on top of what you're saying,

01:28:30.960 --> 01:28:34.960
I think we should be very restrictive,

01:28:34.960 --> 01:28:37.960
restrictive policy,

01:28:37.960 --> 01:28:42.960
and the European AI directive

01:28:42.960 --> 01:28:48.960
is setting some very strict policies there as well.

01:28:48.960 --> 01:28:50.960
Because it's not something that will

01:28:50.960 --> 01:28:53.960
come from the goodness of the big companies.

01:28:53.960 --> 01:28:56.960
I'm afraid.

01:28:56.960 --> 01:28:58.960
Just like Professor Thompson said,

01:28:58.960 --> 01:29:00.960
I mean, he's hopeful that AI will

01:29:00.960 --> 01:29:03.960
solve world inequality.

01:29:03.960 --> 01:29:05.960
But the thing, of course, is,

01:29:05.960 --> 01:29:08.960
we could have solved world inequality a long time ago.

01:29:08.960 --> 01:29:12.960
We don't need the computers or AI for that.

01:29:12.960 --> 01:29:14.960
So, the problem goes far deeper in that.

01:29:14.960 --> 01:29:17.960
And that's where I connect with Queenie

01:29:17.960 --> 01:29:20.960
in the sense that we need to instill

01:29:20.960 --> 01:29:23.960
the ethics in the educational system

01:29:23.960 --> 01:29:25.960
to try and do that.

01:29:25.960 --> 01:29:27.960
And at the same time,

01:29:27.960 --> 01:29:30.960
very strict regulation, I would say.

01:29:30.960 --> 01:29:33.960
Ja, because even if I can add a little bit to that,

01:29:33.960 --> 01:29:36.960
so what internet, social media, et cetera,

01:29:36.960 --> 01:29:38.960
what they did is they created,

01:29:38.960 --> 01:29:41.960
or actually the companies who created it,

01:29:41.960 --> 01:29:44.960
the companies were big in a sense

01:29:44.960 --> 01:29:46.960
that we have never experienced before.

01:29:46.960 --> 01:29:48.960
They have more power in a way

01:29:48.960 --> 01:29:50.960
we have never experienced before

01:29:50.960 --> 01:29:52.960
from companies, in my opinion.

01:29:52.960 --> 01:29:56.960
And AI can actually triple that.

01:29:56.960 --> 01:30:00.960
So, when we just saw in the presentation

01:30:00.960 --> 01:30:02.960
how much extra quadrillion money

01:30:02.960 --> 01:30:04.960
that can be made,

01:30:04.960 --> 01:30:06.960
the first thing I thought was,

01:30:06.960 --> 01:30:08.960
okay, but in whose pockets is going to,

01:30:08.960 --> 01:30:12.960
and who's going to fill the pockets with the money.

01:30:12.960 --> 01:30:16.960
And I don't think that we'll go to fighting inequality

01:30:16.960 --> 01:30:20.960
if we don't make sure from a political perspective

01:30:20.960 --> 01:30:25.960
that technology can make everyone's life better

01:30:25.960 --> 01:30:27.960
and not only a happy few.

01:30:27.960 --> 01:30:29.960
And I think that's really important

01:30:29.960 --> 01:30:31.960
that we are talking about this right now,

01:30:31.960 --> 01:30:33.960
that we're discussing this right now

01:30:33.960 --> 01:30:35.960
is when we are making regulation.

01:30:35.960 --> 01:30:37.960
And I'd love to hear you talk like that.

01:30:39.960 --> 01:30:41.960
You can switch sides.

01:30:41.960 --> 01:30:43.960
A big hand for Greeny.

01:30:44.960 --> 01:30:46.960
Okay, in a minute

01:30:46.960 --> 01:30:48.960
there's some room for questions from the audience

01:30:48.960 --> 01:30:50.960
to our distinguished panelists.

01:30:50.960 --> 01:30:53.960
But first, I want to pose the million dollar question.

01:30:53.960 --> 01:30:56.960
That's what can we, or what should we all do

01:30:56.960 --> 01:30:58.960
in order to tame the beast,

01:30:58.960 --> 01:31:01.960
in order to avoid, we're going to run off the cliff.

01:31:01.960 --> 01:31:03.960
We have the EU AI Act.

01:31:03.960 --> 01:31:07.960
We have ideas of instilling ethics

01:31:07.960 --> 01:31:13.960
and other subjects into core curricula of AI engineers.

01:31:13.960 --> 01:31:16.960
But there are probably other great ideas

01:31:16.960 --> 01:31:18.960
that we should take into account.

01:31:18.960 --> 01:31:21.960
In a minute, but first I want to hear the five panelists.

01:31:21.960 --> 01:31:24.960
We just make a little round and then the floor is yours.

01:31:24.960 --> 01:31:27.960
So what should we do to tame the beast?

01:31:27.960 --> 01:31:29.960
What should we do tomorrow

01:31:29.960 --> 01:31:31.960
in order to make sure that we don't run off the cliff?

01:31:31.960 --> 01:31:33.960
Of course, we have an AI Act.

01:31:33.960 --> 01:31:35.960
Of course, we have great ideas

01:31:35.960 --> 01:31:37.960
of how to improve the curricula of AI engineers.

01:31:37.960 --> 01:31:40.960
But that's probably not the answer

01:31:40.960 --> 01:31:42.960
to the million dollar question.

01:31:42.960 --> 01:31:44.960
What else should happen?

01:31:44.960 --> 01:31:46.960
I mean, there's a lot.

01:31:46.960 --> 01:31:48.960
Just before coming here,

01:31:48.960 --> 01:31:50.960
we send out seven recommendations to policymakers

01:31:50.960 --> 01:31:52.960
to the signatories of the open letter.

01:31:52.960 --> 01:31:55.960
We'll put that out tomorrow, so go check that out.

01:31:55.960 --> 01:31:58.960
But it has things like national regulatory agencies for AI.

01:31:58.960 --> 01:32:01.960
It has things like more AI safety research

01:32:01.960 --> 01:32:03.960
and public funding in that.

01:32:03.960 --> 01:32:05.960
So it's not just the companies doing that.

01:32:05.960 --> 01:32:07.960
It really requires, I think,

01:32:07.960 --> 01:32:09.960
the world coming together over this.

01:32:09.960 --> 01:32:11.960
But given that I've got...

01:32:11.960 --> 01:32:13.960
What does that mean, the world coming together over this?

01:32:13.960 --> 01:32:15.960
I mean, I think ultimately we need

01:32:15.960 --> 01:32:18.960
a sort of international atomic energy agency for AI.

01:32:18.960 --> 01:32:21.960
So an international body that has enforcement agency

01:32:21.960 --> 01:32:23.960
even over those jurisdictions

01:32:23.960 --> 01:32:25.960
that don't fall under an AI Act

01:32:25.960 --> 01:32:29.960
or aren't part of sort of a big power agreement.

01:32:29.960 --> 01:32:31.960
But I am going to take this opportunity

01:32:31.960 --> 01:32:33.960
with these two Dutch politicians

01:32:33.960 --> 01:32:35.960
because I work a lot in Brussels

01:32:35.960 --> 01:32:37.960
where we have an AI Act,

01:32:37.960 --> 01:32:39.960
but there's also a lot of big tech lobbying.

01:32:39.960 --> 01:32:41.960
I mean, there's maybe four or five NGO people

01:32:41.960 --> 01:32:43.960
and then there's several hundred from Microsoft

01:32:43.960 --> 01:32:45.960
and Google and Bing

01:32:45.960 --> 01:32:47.960
an open AI's own team right now.

01:32:47.960 --> 01:32:49.960
It seems like an uneven fight.

01:32:49.960 --> 01:32:52.960
It is. I think we need the AI Act tomorrow.

01:32:52.960 --> 01:32:55.960
I think Brussels is taking its normal slow course.

01:32:55.960 --> 01:32:57.960
And I think one thing the Dutch Parliament could do

01:32:57.960 --> 01:33:00.960
is to ask for it to be applied provisionally.

01:33:00.960 --> 01:33:02.960
As we have chat GPT right now,

01:33:02.960 --> 01:33:05.960
we probably also need some rules and safeguards.

01:33:05.960 --> 01:33:09.960
Another thing is the Act prohibits manipulation of people.

01:33:09.960 --> 01:33:12.960
But only if you use your AI system in a subliminal way.

01:33:12.960 --> 01:33:14.960
So if it's in a hidden frame.

01:33:14.960 --> 01:33:16.960
But if you do it overtly, it's fine.

01:33:16.960 --> 01:33:19.960
We think that probably should be changed.

01:33:19.960 --> 01:33:21.960
En dan

01:33:21.960 --> 01:33:23.960
maybe my final pitch here

01:33:23.960 --> 01:33:25.960
is that for a long time

01:33:25.960 --> 01:33:27.960
more general AI systems such as chat GPT

01:33:27.960 --> 01:33:29.960
were exempt from the Act.

01:33:29.960 --> 01:33:32.960
We've worked very, very hard to try and bring that into the Act.

01:33:32.960 --> 01:33:34.960
But we also face a lot of Microsoft pushback.

01:33:34.960 --> 01:33:36.960
So if there's anything you can do to keep it there,

01:33:36.960 --> 01:33:38.960
that would be awesome.

01:33:38.960 --> 01:33:40.960
Ok, keep Microsoft at bay.

01:33:42.960 --> 01:33:45.960
Maybe a quick response here

01:33:45.960 --> 01:33:48.960
because those are very sort of concise recommendations.

01:33:48.960 --> 01:33:50.960
What do you think?

01:33:50.960 --> 01:33:52.960
Are you going to take these up

01:33:52.960 --> 01:33:54.960
and next time you talk to your fellow parliamentarians?

01:33:54.960 --> 01:33:56.960
Of course read the recommendations.

01:33:56.960 --> 01:33:58.960
We will be stupid not to do it.

01:33:58.960 --> 01:34:01.960
But I fully agree with the lobbying power

01:34:01.960 --> 01:34:04.960
and the equality of arms is not equal.

01:34:04.960 --> 01:34:07.960
You see it in the fossil industry.

01:34:07.960 --> 01:34:10.960
You see it in the finance industry.

01:34:10.960 --> 01:34:13.960
So my call would be to

01:34:13.960 --> 01:34:16.960
call to arms for

01:34:16.960 --> 01:34:19.960
raising funds

01:34:19.960 --> 01:34:22.960
to putting more money

01:34:22.960 --> 01:34:25.960
in the lobbying effort

01:34:25.960 --> 01:34:29.960
because I think we are very weak there.

01:34:29.960 --> 01:34:32.960
And I think the Microsofts.

01:34:32.960 --> 01:34:35.960
I'm not too sure if a guy like Elon Musk

01:34:35.960 --> 01:34:38.960
is saying that AI is a threat

01:34:38.960 --> 01:34:41.960
when he is, you know, what's he doing.

01:34:41.960 --> 01:34:44.960
So I'm not entirely sure if that's the right...

01:34:44.960 --> 01:34:47.960
Ok, so I agree with you

01:34:47.960 --> 01:34:50.960
on the lobbying front, definitely.

01:34:50.960 --> 01:34:53.960
And what about the international agency for AI?

01:34:53.960 --> 01:34:56.960
It sounds a bit...

01:34:56.960 --> 01:34:59.960
I have to think about it to be honest

01:34:59.960 --> 01:35:02.960
because it sounds like a drastic...

01:35:02.960 --> 01:35:05.960
I don't think we have a red button

01:35:05.960 --> 01:35:08.960
or an international police agency

01:35:08.960 --> 01:35:11.960
that can say stop this.

01:35:11.960 --> 01:35:14.960
I haven't thought about that.

01:35:14.960 --> 01:35:17.960
Maybe it's wrong for a politician not to give an answer

01:35:17.960 --> 01:35:20.960
on the spot directly.

01:35:20.960 --> 01:35:23.960
Sleep over it, yeah.

01:35:23.960 --> 01:35:26.960
I'll ask my AI to...

01:35:26.960 --> 01:35:29.960
Queenie.

01:35:29.960 --> 01:35:32.960
I would like to read, but you're going to send out the e-mail

01:35:32.960 --> 01:35:35.960
so I'm going to read all the seven recommendations.

01:35:35.960 --> 01:35:38.960
I recognize the lobbying part a lot.

01:35:38.960 --> 01:35:41.960
So what I tried from a Dutch perspective,

01:35:41.960 --> 01:35:44.960
when you look at the AI Act, they distinguish

01:35:44.960 --> 01:35:47.960
if an AI is a high risk AI

01:35:47.960 --> 01:35:50.960
or a low risk AI.

01:35:50.960 --> 01:35:53.960
And I'm not sure if I follow those categories

01:35:53.960 --> 01:35:56.960
because I don't think it's about the technology

01:35:56.960 --> 01:35:59.960
but in which context you use them and with which goal.

01:35:59.960 --> 01:36:02.960
And I also try to make some low risk

01:36:02.960 --> 01:36:05.960
AI to try to get them in the higher category

01:36:05.960 --> 01:36:08.960
which is really difficult.

01:36:08.960 --> 01:36:11.960
So I really recognize the lobbying part.

01:36:11.960 --> 01:36:14.960
So maybe it's good also to come together after tonight

01:36:14.960 --> 01:36:17.960
and to see if we can align

01:36:17.960 --> 01:36:20.960
on some topics.

01:36:20.960 --> 01:36:23.960
I wanted to...

01:36:23.960 --> 01:36:26.960
Maybe one thing that can come close to what you're saying

01:36:26.960 --> 01:36:29.960
is de Wetenschappelijke Raad voor Regeringbeleid.

01:36:29.960 --> 01:36:32.960
So that's a group that advises

01:36:32.960 --> 01:36:35.960
the parliament but also the cabinet, the ministers.

01:36:35.960 --> 01:36:38.960
And they said you have to work on AI diplomacy.

01:36:38.960 --> 01:36:41.960
And I think that comes...

01:36:41.960 --> 01:36:44.960
Well, it doesn't have really overruling power

01:36:44.960 --> 01:36:47.960
but it comes really close in making sure that you get treaties,

01:36:47.960 --> 01:36:50.960
make agreements all over the world

01:36:50.960 --> 01:36:53.960
on how to use AI.

01:36:53.960 --> 01:36:56.960
So I think that's a good one and at the same time

01:36:56.960 --> 01:36:59.960
well, if you look at the geopolitical situation right now

01:36:59.960 --> 01:37:02.960
not everyone listens to international treaties.

01:37:02.960 --> 01:37:04.960
But I think it's a good start

01:37:04.960 --> 01:37:07.960
and let's talk about that tomorrow or after more.

01:37:07.960 --> 01:37:09.960
Tim, your two cents.

01:37:09.960 --> 01:37:11.960
Right.

01:37:11.960 --> 01:37:14.960
I don't know a lot about the social technological aspects of this.

01:37:14.960 --> 01:37:17.960
I'm going to maybe focus on the existential risk part

01:37:17.960 --> 01:37:19.960
that I know a bit more about

01:37:19.960 --> 01:37:22.960
because I think Marc already gave a very good summary.

01:37:22.960 --> 01:37:25.960
So one thing we can do is hope it goes right.

01:37:25.960 --> 01:37:28.960
I don't think that has a lot of chance.

01:37:28.960 --> 01:37:32.960
The other thing is we can try to solve this alignment problem

01:37:32.960 --> 01:37:35.960
either by, as Professor Russell suggested,

01:37:35.960 --> 01:37:37.960
finding new paradigms

01:37:37.960 --> 01:37:39.960
or by trying to solve it in a deep learning setting

01:37:39.960 --> 01:37:42.960
which might be very difficult but maybe it's doable.

01:37:42.960 --> 01:37:45.960
I don't particularly have any hope

01:37:45.960 --> 01:37:47.960
in the companies themselves solving this

01:37:47.960 --> 01:37:49.960
and I also feel like if we want to do this

01:37:49.960 --> 01:37:51.960
we need a lot more time.

01:37:51.960 --> 01:37:55.960
And so one way to give us time

01:37:55.960 --> 01:37:57.960
would be to have these kinds of international regulations

01:37:57.960 --> 01:37:59.960
that make sure

01:37:59.960 --> 01:38:02.960
like the open letter suggested, systems like GPT-4

01:38:02.960 --> 01:38:05.960
or stronger systems like that shouldn't be allowed to be trained

01:38:05.960 --> 01:38:08.960
for maybe ever or until we solve alignment

01:38:08.960 --> 01:38:10.960
or six months, I don't know how long it will take.

01:38:10.960 --> 01:38:12.960
And I think it's very telling that

01:38:12.960 --> 01:38:14.960
even the tech industry itself is saying,

01:38:14.960 --> 01:38:17.960
look, world help us

01:38:17.960 --> 01:38:19.960
because we don't know how to do this

01:38:19.960 --> 01:38:21.960
and we need more time to solve this.

01:38:21.960 --> 01:38:23.960
I think maybe we do actually need that kind of drastic action

01:38:23.960 --> 01:38:25.960
because they're not going to do it by themselves.

01:38:25.960 --> 01:38:28.960
Ja, they seem to be open minded in some ways to that.

01:38:28.960 --> 01:38:30.960
Ja, Andy.

01:38:30.960 --> 01:38:34.960
I can only agree what has been said already

01:38:34.960 --> 01:38:36.960
and besides that

01:38:36.960 --> 01:38:39.960
something that was also mentioned in the open letter

01:38:39.960 --> 01:38:43.960
is to call for a research focus shift

01:38:43.960 --> 01:38:46.960
from AI capabilities research

01:38:46.960 --> 01:38:49.960
so making the biggest models even bigger

01:38:49.960 --> 01:38:53.960
and better and smarter to AI safety research

01:38:53.960 --> 01:38:57.960
which is research to ensure the beneficial outcomes

01:38:57.960 --> 01:39:00.960
of these advanced AI models

01:39:00.960 --> 01:39:04.960
and so I think the Dutch government can play a role in that

01:39:04.960 --> 01:39:08.960
as well to advocate for more funding towards that

01:39:08.960 --> 01:39:11.960
and I think the Netherlands is a great place for that as well

01:39:11.960 --> 01:39:13.960
because we have a lot of technical universities

01:39:13.960 --> 01:39:17.960
that are highly internationally regarded.

01:39:17.960 --> 01:39:19.960
So yeah, besides technical research

01:39:19.960 --> 01:39:22.960
we also need more research for AI governance

01:39:22.960 --> 01:39:26.960
so we need more robust and effective governance mechanisms

01:39:26.960 --> 01:39:31.960
en ja, I think we can also play a role in that.

01:39:31.960 --> 01:39:33.960
Oké, the list goes on.

01:39:33.960 --> 01:39:35.960
Some questions from the audience.

01:39:35.960 --> 01:39:38.960
May I see your hands? Ja.

01:39:47.960 --> 01:39:50.960
Queenie inspires me to ask this question

01:39:50.960 --> 01:39:54.960
because you seem to refer to one profession

01:39:54.960 --> 01:39:58.960
or one type of school to take an oath

01:39:58.960 --> 01:40:01.960
but I would take it one step further.

01:40:01.960 --> 01:40:05.960
Why do we still have professions and or schools

01:40:05.960 --> 01:40:09.960
who might be threatening in any way

01:40:09.960 --> 01:40:12.960
without taking an oath?

01:40:12.960 --> 01:40:18.960
Shouldn't this oath be obligatory for many more professions or schools?

01:40:18.960 --> 01:40:21.960
Yes, yes, it should.

01:40:21.960 --> 01:40:24.960
So actually this was not my idea

01:40:24.960 --> 01:40:29.960
but there are two female mathematical teachers

01:40:29.960 --> 01:40:32.960
at I think it was the Delft Technical University

01:40:32.960 --> 01:40:34.960
and they came to me and they said

01:40:34.960 --> 01:40:37.960
hey we are trying to adjust the curriculum

01:40:37.960 --> 01:40:41.960
into making sure that everyone who is going to

01:40:41.960 --> 01:40:44.960
who is going to this technical university

01:40:44.960 --> 01:40:48.960
that ethics should be part of all of the studies

01:40:48.960 --> 01:40:52.960
and they asked well can you help us to give a push

01:40:52.960 --> 01:40:55.960
so actually it was I didn't steal the idea

01:40:55.960 --> 01:40:58.960
but I tried to give them a push from a political part

01:40:58.960 --> 01:41:02.960
and that helped but the goal is not to do it just for AI engineers

01:41:02.960 --> 01:41:05.960
but you know like because you cannot

01:41:05.960 --> 01:41:07.960
this is a big responsibility

01:41:07.960 --> 01:41:10.960
and you shouldn't just put it with just one person

01:41:10.960 --> 01:41:13.960
but you have to make sure that everyone who works in the field

01:41:13.960 --> 01:41:16.960
understands what are their human rights

01:41:16.960 --> 01:41:19.960
how can we make sure that we strengthen them

01:41:19.960 --> 01:41:22.960
instead of threatening them et cetera

01:41:22.960 --> 01:41:26.960
so it's actually the whole system needs to be

01:41:26.960 --> 01:41:28.960
needs to be conscious of that

01:41:28.960 --> 01:41:30.960
because what we all learned

01:41:30.960 --> 01:41:32.960
what we all saw the last couple of years

01:41:32.960 --> 01:41:36.960
is that technical NIT is not just technical

01:41:36.960 --> 01:41:38.960
it's about the way we live our lives

01:41:38.960 --> 01:41:41.960
it's about who earns money

01:41:41.960 --> 01:41:43.960
what information do we see et cetera

01:41:43.960 --> 01:41:46.960
so we need to make sure that ethical standards

01:41:46.960 --> 01:41:49.960
are taking into account when it comes to IT broadly

01:41:49.960 --> 01:41:52.960
because it determines the way we live

01:41:52.960 --> 01:41:55.960
did I answer your question?

01:41:55.960 --> 01:41:57.960
Lamert, sorry?

01:41:57.960 --> 01:42:01.960
I think it's definitely worth pursuing

01:42:01.960 --> 01:42:05.960
and to instill ethical values in

01:42:05.960 --> 01:42:08.960
educational systems or professions

01:42:08.960 --> 01:42:10.960
I think that's a good idea

01:42:10.960 --> 01:42:12.960
although banker oats

01:42:14.960 --> 01:42:16.960
why not?

01:42:16.960 --> 01:42:18.960
we have them

01:42:18.960 --> 01:42:20.960
we saw what happens

01:42:20.960 --> 01:42:22.960
but nevertheless

01:42:22.960 --> 01:42:24.960
when you look at the medical profession

01:42:24.960 --> 01:42:28.960
it took 2000 years to instill the values

01:42:28.960 --> 01:42:31.960
that the oath is meaningful

01:42:31.960 --> 01:42:33.960
so it can work

01:42:33.960 --> 01:42:35.960
but we know from a banker perspective

01:42:35.960 --> 01:42:37.960
it's meaningless

01:42:37.960 --> 01:42:40.960
but I'm sure we can find a balance

01:42:40.960 --> 01:42:42.960
so I think we should pursue it

01:42:42.960 --> 01:42:44.960
maybe a start

01:42:50.960 --> 01:42:52.960
thank you for the interesting discussion

01:42:52.960 --> 01:42:54.960
I was wondering

01:42:54.960 --> 01:42:56.960
because there is a lot of progress going on

01:42:56.960 --> 01:42:58.960
when it comes to AI interpretability

01:42:58.960 --> 01:43:02.960
and making sure that we understand

01:43:02.960 --> 01:43:05.960
what kind of representation deep learning models are forming

01:43:05.960 --> 01:43:08.960
do you think there is any role of AI interpretability

01:43:08.960 --> 01:43:12.960
in making sure that these systems are safe?

01:43:12.960 --> 01:43:15.960
that's maybe a question for Tim I guess

01:43:15.960 --> 01:43:17.960
thanks for the question

01:43:17.960 --> 01:43:20.960
I mean definitely a big part of

01:43:20.960 --> 01:43:23.960
AI alignment research or ASafety more broadly

01:43:23.960 --> 01:43:26.960
should be interpretability for these deep learning systems

01:43:26.960 --> 01:43:29.960
to give us at least some kind of lens

01:43:29.960 --> 01:43:31.960
of looking at these systems

01:43:31.960 --> 01:43:33.960
and maybe understanding a little bit of how they work

01:43:33.960 --> 01:43:35.960
of what they potentially do before they do it

01:43:35.960 --> 01:43:38.960
I think the hope of this field

01:43:38.960 --> 01:43:40.960
might be very difficult

01:43:40.960 --> 01:43:43.960
in the sense that these models are so huge

01:43:43.960 --> 01:43:45.960
and there are so many parameters

01:43:45.960 --> 01:43:47.960
and it's so hard to even understand

01:43:47.960 --> 01:43:49.960
what small parts of it are doing

01:43:49.960 --> 01:43:52.960
like if you look at the field of

01:43:52.960 --> 01:43:54.960
the specific kind of mechanistic interpretability

01:43:54.960 --> 01:43:56.960
they call it right now

01:43:56.960 --> 01:43:58.960
we know tiny little things

01:43:58.960 --> 01:44:00.960
about tiny little parts of the model

01:44:00.960 --> 01:44:02.960
that give us some kind of idea

01:44:02.960 --> 01:44:04.960
it's doing it a little bit like this

01:44:04.960 --> 01:44:06.960
but before we can actually scale that up

01:44:06.960 --> 01:44:08.960
to

01:44:08.960 --> 01:44:10.960
just actually understanding what goes on

01:44:10.960 --> 01:44:12.960
that will take so long

01:44:12.960 --> 01:44:15.960
and I'm not sure how feasible it is

01:44:15.960 --> 01:44:18.960
to use that as the main angle of attack

01:44:18.960 --> 01:44:20.960
I definitely think it's part of it

01:44:20.960 --> 01:44:23.960
but we need a lot of other approaches as well

01:44:23.960 --> 01:44:25.960
part of the solution

01:44:25.960 --> 01:44:27.960
another question there

01:44:27.960 --> 01:44:29.960
yes

01:44:29.960 --> 01:44:31.960
thank you for the diverse perspectives you had

01:44:31.960 --> 01:44:33.960
and I was wondering Mark

01:44:33.960 --> 01:44:35.960
you mentioned law and policy

01:44:35.960 --> 01:44:37.960
as one of the key aspects

01:44:37.960 --> 01:44:39.960
but I think now also with GDPR

01:44:39.960 --> 01:44:41.960
actually the enforcement

01:44:41.960 --> 01:44:43.960
is one of the challenges

01:44:43.960 --> 01:44:45.960
and how would you propose a solution

01:44:45.960 --> 01:44:47.960
specifically for the enforcement

01:44:47.960 --> 01:44:49.960
perhaps is it on a national or European level

01:44:49.960 --> 01:44:51.960
or other further levels as well

01:44:51.960 --> 01:44:53.960
having a law is one thing

01:44:53.960 --> 01:44:55.960
but how do you actually enforce it effectively

01:44:55.960 --> 01:44:58.960
I think the general data protection regulation

01:44:58.960 --> 01:45:00.960
is really interesting in that

01:45:00.960 --> 01:45:02.960
Italy in sheer desperation

01:45:02.960 --> 01:45:04.960
blocked chat GPT last week

01:45:04.960 --> 01:45:06.960
on the base of GDPR

01:45:06.960 --> 01:45:08.960
because there was no AI act

01:45:08.960 --> 01:45:11.960
so I think just harping on about how we need it urgently

01:45:11.960 --> 01:45:13.960
I think people are learning lessons

01:45:13.960 --> 01:45:15.960
from the failures of GDPR

01:45:15.960 --> 01:45:18.960
people realise that the fact that all the big tech companies

01:45:18.960 --> 01:45:20.960
have their headquarters in Dublin, in Ireland

01:45:20.960 --> 01:45:22.960
and the fact that the Irish data protection authority

01:45:22.960 --> 01:45:24.960
is probably the weakest out of all the EU members

01:45:24.960 --> 01:45:27.960
is something that people in Brussels have realised

01:45:27.960 --> 01:45:29.960
so under the AI act

01:45:29.960 --> 01:45:32.960
potentially there will be a centralized office

01:45:32.960 --> 01:45:34.960
so that will help deal with enforcement

01:45:34.960 --> 01:45:36.960
because it means that the European Commission can step in

01:45:36.960 --> 01:45:38.960
when member states do not

01:45:38.960 --> 01:45:41.960
there is also again lobbying

01:45:41.960 --> 01:45:43.960
against this AI office

01:45:43.960 --> 01:45:46.960
and some people are worried about the cost of civil servants

01:45:46.960 --> 01:45:49.960
that the commission would potentially need to hire for this

01:45:49.960 --> 01:45:52.960
we've been arguing that this technology is so transformative

01:45:52.960 --> 01:45:55.960
that it's probably worth a few hundred civil servants

01:45:55.960 --> 01:45:58.960
but it's really a knife edge vote

01:45:58.960 --> 01:46:00.960
I think it's about half the European Parliament

01:46:00.960 --> 01:46:02.960
at the moment that would favour such an office

01:46:02.960 --> 01:46:04.960
and half that oppose it

01:46:04.960 --> 01:46:06.960
and would like to see a GDPR type model

01:46:06.960 --> 01:46:09.960
maybe you have some partners in crime here

01:46:09.960 --> 01:46:12.960
I'm not sure but they could help out

01:46:12.960 --> 01:46:15.960
well exactly what Italy did

01:46:15.960 --> 01:46:18.960
was they took the law that they have on data protection

01:46:18.960 --> 01:46:20.960
and AVG

01:46:20.960 --> 01:46:23.960
en they

01:46:23.960 --> 01:46:26.960
they said let's treat it as a human decision

01:46:26.960 --> 01:46:29.960
and then it fell short

01:46:29.960 --> 01:46:32.960
of the decision process and on that grounds you can

01:46:32.960 --> 01:46:35.960
in fact do enforcement

01:46:35.960 --> 01:46:38.960
it is a bit like using a hammer when you try to

01:46:38.960 --> 01:46:40.960
do a screw

01:46:40.960 --> 01:46:43.960
but it is possible in the area of well wanting for another law

01:46:43.960 --> 01:46:46.960
but that is very feasible

01:46:46.960 --> 01:46:49.960
enforcement instruments are in place

01:46:49.960 --> 01:46:52.960
one question here

01:46:52.960 --> 01:46:55.960
thank you

01:46:55.960 --> 01:46:58.960
when it comes to regulation and policy

01:46:58.960 --> 01:47:01.960
I think the human species has a track record of solving always

01:47:01.960 --> 01:47:04.960
the last crisis

01:47:04.960 --> 01:47:07.960
when it comes to existential risk

01:47:07.960 --> 01:47:10.960
Nick Bostrom also said that we basically have one shot to get this right

01:47:10.960 --> 01:47:13.960
with human track record

01:47:13.960 --> 01:47:16.960
human civilization track record inside

01:47:16.960 --> 01:47:19.960
is that something that should concern us

01:47:19.960 --> 01:47:22.960
only one shot to get this right

01:47:22.960 --> 01:47:25.960
and we rather myopic and focus on short term risk

01:47:25.960 --> 01:47:28.960
who wants to answer, are we optimist here

01:47:28.960 --> 01:47:31.960
I don't think it's a matter of one shot

01:47:31.960 --> 01:47:34.960
to be honest

01:47:34.960 --> 01:47:37.960
you don't think it's one shot

01:47:37.960 --> 01:47:40.960
I like the question but I think

01:47:40.960 --> 01:47:43.960
if I try to think about the presentation that we have

01:47:43.960 --> 01:47:46.960
what I liked about

01:47:46.960 --> 01:47:49.960
we can have several smaller signs

01:47:49.960 --> 01:47:52.960
before we get to total extension

01:47:52.960 --> 01:47:55.960
so that's hopeful for me

01:47:55.960 --> 01:47:58.960
and yes this is worrying

01:47:58.960 --> 01:48:01.960
that's why we are here today

01:48:01.960 --> 01:48:04.960
we have to make sure that more people understand what AI is

01:48:04.960 --> 01:48:07.960
what are the dangers, how can we make sure it works for us all

01:48:07.960 --> 01:48:10.960
what are the good things

01:48:10.960 --> 01:48:13.960
we have a lot of work in society as a whole

01:48:13.960 --> 01:48:16.960
regulation et cetera

01:48:16.960 --> 01:48:19.960
but again for me it's hopeful that

01:48:19.960 --> 01:48:22.960
when I see the difference when the internet started

01:48:22.960 --> 01:48:25.960
and when big tech companies became big

01:48:25.960 --> 01:48:28.960
we were too late when it came to

01:48:28.960 --> 01:48:31.960
regulation of market power et cetera

01:48:31.960 --> 01:48:34.960
and actually that on European level

01:48:34.960 --> 01:48:37.960
that they started thinking about the AI act two years ago

01:48:37.960 --> 01:48:40.960
it's hopeful, it also means that it's two years later now

01:48:40.960 --> 01:48:43.960
so maybe it's not complete

01:48:43.960 --> 01:48:46.960
because technology has been developing

01:48:46.960 --> 01:48:49.960
really quick over the last two years

01:48:49.960 --> 01:48:52.960
but I think that part is hopeful

01:48:52.960 --> 01:48:55.960
that we recognize this problem before it's too late

01:48:55.960 --> 01:48:58.960
and I think it's our responsibility

01:48:58.960 --> 01:49:01.960
to make sure that we prevent that regulation comes too late

01:49:01.960 --> 01:49:04.960
so it's a bit late now

01:49:04.960 --> 01:49:07.960
one last question here

01:49:07.960 --> 01:49:10.960
thanks

01:49:10.960 --> 01:49:13.960
I wanted to make

01:49:13.960 --> 01:49:16.960
a short statement first that

01:49:16.960 --> 01:49:19.960
we were talking about protests

01:49:19.960 --> 01:49:22.960
I think that should happen and I want to organize them

01:49:22.960 --> 01:49:25.960
this year so if

01:49:25.960 --> 01:49:28.960
who's coming

01:49:28.960 --> 01:49:31.960
called safe transition

01:49:31.960 --> 01:49:34.960
safe transition to the machine intelligence era

01:49:34.960 --> 01:49:37.960
but thank you

01:49:37.960 --> 01:49:40.960
my question is to Mark

01:49:40.960 --> 01:49:43.960
and

01:49:43.960 --> 01:49:46.960
it's about the EU regulations

01:49:46.960 --> 01:49:49.960
and

01:49:49.960 --> 01:49:52.960
my current understanding is that

01:49:52.960 --> 01:49:55.960
they are only focusing on deployment

01:49:55.960 --> 01:49:58.960
and not on the training

01:49:58.960 --> 01:50:01.960
so that

01:50:01.960 --> 01:50:04.960
they are in effect not protecting us

01:50:04.960 --> 01:50:07.960
from the existential risk of

01:50:07.960 --> 01:50:10.960
an AI that secretly breaks out

01:50:10.960 --> 01:50:13.960
and goes and

01:50:13.960 --> 01:50:16.960
does its plan to take over the world

01:50:16.960 --> 01:50:19.960
in some other server

01:50:19.960 --> 01:50:22.960
data center so my question is

01:50:22.960 --> 01:50:25.960
is there something in the EU process

01:50:26.960 --> 01:50:29.960
is

01:50:29.960 --> 01:50:32.960
just like the sparks of AGI paper by Microsoft

01:50:32.960 --> 01:50:35.960
the EU AI act I think is a spark of hope

01:50:35.960 --> 01:50:38.960
but you're completely right it's not more than that

01:50:38.960 --> 01:50:41.960
because what they've basically done is they've taken a product safety regulation

01:50:41.960 --> 01:50:44.960
like of any type you have in Europe so basically

01:50:44.960 --> 01:50:47.960
the one that regulates the toy market

01:50:47.960 --> 01:50:50.960
and then they've said okay we'll apply that to AI products

01:50:50.960 --> 01:50:53.960
so it only starts to kick in once

01:50:53.960 --> 01:50:56.960
we want to put it on the market

01:50:56.960 --> 01:50:58.960
so if you're training it, if you're testing it

01:50:58.960 --> 01:51:01.960
it's all fine it's completely unregulated

01:51:01.960 --> 01:51:04.960
and we definitely need rules for that

01:51:04.960 --> 01:51:07.960
I think most AI researchers

01:51:07.960 --> 01:51:10.960
feel that we need to start looking seriously at companies

01:51:10.960 --> 01:51:13.960
that have a huge amount of computational power

01:51:13.960 --> 01:51:16.960
there was someone writing an article

01:51:16.960 --> 01:51:19.960
the former advisor to the UK prime minister on technology

01:51:19.960 --> 01:51:22.960
who observed that open AI by itself

01:51:22.960 --> 01:51:25.960
in California has 25 more sort of GPUs

01:51:25.960 --> 01:51:28.960
than the entire United Kingdom

01:51:28.960 --> 01:51:31.960
so their compute power is about 25 times the size of the UK

01:51:31.960 --> 01:51:34.960
those are the sources of worry

01:51:34.960 --> 01:51:37.960
and I think we need to start regulating

01:51:37.960 --> 01:51:40.960
and inspecting and monitoring and verifying those companies

01:51:40.960 --> 01:51:43.960
before ideally

01:51:43.960 --> 01:51:46.960
I think they've developed their product and there's no way

01:51:46.960 --> 01:51:49.960
we can still change it or make it safer

01:51:49.960 --> 01:51:52.960
we don't want to create risks that maybe something happens before it's deployed

01:51:52.960 --> 01:51:55.960
so I think you're completely right

01:51:55.960 --> 01:51:58.960
we need a bunch of extra regulation

01:51:58.960 --> 01:52:01.960
and we also desperately need the United States

01:52:01.960 --> 01:52:04.960
because that's where most of these companies are based

01:52:04.960 --> 01:52:07.960
and it's super, like it's great that we have European regulation

01:52:07.960 --> 01:52:10.960
but without the US this existential risk is not going to go away

01:52:10.960 --> 01:52:13.960
we need to tackle this problem globally

01:52:13.960 --> 01:52:16.960
if I could add something to that maybe

01:52:16.960 --> 01:52:19.960
yes, I think we should be worried about that

01:52:19.960 --> 01:52:22.960
because there are still these scenarios

01:52:22.960 --> 01:52:25.960
like the one you mentioned

01:52:25.960 --> 01:52:28.960
where we are not protected and we do kind of only have one chance

01:52:28.960 --> 01:52:31.960
and for those kinds of things it is

01:52:31.960 --> 01:52:34.960
I think very important to target the bottlenecks of these kind of systems

01:52:34.960 --> 01:52:37.960
which right now is just the model training

01:52:37.960 --> 01:52:40.960
you need so much more computational power

01:52:40.960 --> 01:52:43.960
to train these models and to deploy them

01:52:43.960 --> 01:52:46.960
the easiest and the most obvious thing to regulate

01:52:46.960 --> 01:52:49.960
of course it might be very hard to regulate in practice

01:52:49.960 --> 01:52:52.960
but if you target that part

01:52:52.960 --> 01:52:55.960
then you actually have a better chance at stopping these kinds of models

01:52:55.960 --> 01:52:58.960
is that part of your seven recommendations

01:52:58.960 --> 01:53:01.960
it's number two

01:53:01.960 --> 01:53:04.960
well that's one of the commitments I already heard

01:53:04.960 --> 01:53:07.960
that you're going to discuss these seven recommendations

01:53:07.960 --> 01:53:10.960
I also heard the idea of trying to

01:53:10.960 --> 01:53:13.960
strengthen AI safety research

01:53:13.960 --> 01:53:16.960
and ethics in different engineering programs

01:53:16.960 --> 01:53:19.960
increasing funding and trying to fight the big lobbying power

01:53:19.960 --> 01:53:22.960
of the tech industry

01:53:22.960 --> 01:53:25.960
so quite some commitments that have already been uttered here on this stage

01:53:25.960 --> 01:53:28.960
we've come to the end of this evening

01:53:28.960 --> 01:53:31.960
but actually it's only the start

01:53:31.960 --> 01:53:34.960
because there are drinks later on

01:53:34.960 --> 01:53:37.960
to continue the conversation

01:53:37.960 --> 01:53:40.960
about all the great catastrophic risks

01:53:40.960 --> 01:53:43.960
that have come to the fore this evening

01:53:43.960 --> 01:53:46.960
or maybe you're very hopeful about humanity

01:53:46.960 --> 01:53:49.960
steering away from the cliff

01:53:49.960 --> 01:53:52.960
doesn't really matter, both are great reasons for a good drink and chat

01:53:52.960 --> 01:53:55.960
so I advise you all to the bar

01:53:55.960 --> 01:53:58.960
that's in the hall

01:53:58.960 --> 01:54:01.960
down the hole there

01:54:01.960 --> 01:54:04.960
I want to give a big round of applause to our five panelists

01:54:07.960 --> 01:54:10.960
applause

01:54:10.960 --> 01:54:13.960
applause

01:54:13.960 --> 01:54:16.960
en some flowers

01:54:16.960 --> 01:54:19.960
we'll probably meet again

01:54:19.960 --> 01:54:22.960
during the demonstration

01:54:22.960 --> 01:54:25.960
when will it take place

01:54:25.960 --> 01:54:28.960
to be decided

01:54:28.960 --> 01:54:31.960
cliffhanger, we'll see each other at the dam square

01:54:31.960 --> 01:54:34.960
somewhere next year

01:54:34.960 --> 01:54:37.960
we hope to see you again

01:54:37.960 --> 01:54:40.960
applause

01:54:40.960 --> 01:54:43.960
applause

01:54:43.960 --> 01:54:46.960
applause

