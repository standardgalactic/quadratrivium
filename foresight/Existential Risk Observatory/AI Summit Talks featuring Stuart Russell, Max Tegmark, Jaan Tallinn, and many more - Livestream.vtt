WEBVTT

00:00.000 --> 00:07.120
and to me it's not clear right are we at the right brother stage or are we at

00:07.120 --> 00:15.560
the Mongolfi a stage where we have a lot of hot air and so my current view is no

00:15.560 --> 00:25.480
we have not succeeded and the models that people are excited about the large

00:25.480 --> 00:31.320
language models and their extensions into multimodal models that take in video

00:31.320 --> 00:37.480
and can actually operate robots and so on that these are a piece of the puzzle

00:37.480 --> 00:44.200
and and CNN made this lovely animated gif here to to illustrate this idea that

00:44.200 --> 00:49.200
we don't really know what shape the piece of the puzzle is and we don't know

00:49.200 --> 00:54.160
what other pieces are needed and how it fits together to make general purpose

00:54.160 --> 01:00.560
intelligence we may discover what's going on inside the large language models

01:00.560 --> 01:06.720
we may figure out what source of power they're drawing on to to create the

01:06.720 --> 01:11.920
kinds of surprisingly capable behaviors that they do exhibit but at the moment

01:11.920 --> 01:20.200
that remains a mystery and there are some gaps right one of the achievements of

01:20.200 --> 01:26.320
modern AI that people were most proud of and also most certain of was the defeat

01:26.320 --> 01:36.320
of human go champions by alpha go and then alpha zero in the 2016 to 2018

01:36.320 --> 01:43.840
period so in go for those of you don't know there's a board you put pieces on

01:43.840 --> 01:48.600
and your goal is to surround territory and to surround your opponents pieces

01:48.600 --> 01:56.840
and capture them and since AI systems beat the world champion in 2017 they've

01:56.840 --> 02:03.160
gone on to leave human race in the dust so the highest rank program is catago

02:03.160 --> 02:08.480
and its rating is about five thousand two hundred compared to the human world

02:08.480 --> 02:15.120
champion at three thousand eight hundred and the human world champion leaves our

02:15.160 --> 02:22.960
colleague kelin pelrin who's a grad student a decent amateur go player his

02:22.960 --> 02:30.680
ratings about 2300 and now I'll show you a game between kelin and catago where

02:30.680 --> 02:35.040
kelin actually gives catago a nine stone handicap so catago is black and

02:35.040 --> 02:40.160
starts with nine stones on the board right if you're an adult go player and

02:40.160 --> 02:43.280
you're teaching a five-year-old how to play go you give them a nine stone

02:43.320 --> 02:47.560
handicap so that at least they can stay in the game for a few minutes right so

02:47.560 --> 02:55.760
here we are treating treating catago as if it's a baby okay despite the fact that

02:55.760 --> 03:03.760
it's massively superhuman and and here's the game so it's speeded up a little bit

03:03.760 --> 03:10.000
but watch what happens in the bottom right corner so white the human being is

03:10.040 --> 03:15.080
going to start building a little group of stones there they go and then black

03:15.080 --> 03:20.880
very quickly surrounds that group to make sure that it can't grow and also to

03:20.880 --> 03:25.280
actually have a pretty good chance of capturing that group but now white starts

03:25.280 --> 03:30.240
to surround the black stones and interestingly black doesn't seem to pay

03:30.240 --> 03:34.640
any attention to this it doesn't understand that the black stones are in

03:34.640 --> 03:38.240
danger of being captured which is a very basic thing right you have to

03:38.280 --> 03:42.440
understand when your opponent is going to capture your pieces and black just

03:42.440 --> 03:48.040
pays no attention and loses all of those pieces and that's the end of the game

03:48.040 --> 03:55.400
so something weird happens there right where an ordinary human amateur go

03:55.400 --> 04:00.640
player can beat a go program that stratospherically better than any human

04:00.640 --> 04:05.880
being has ever been in history right and in fact the go programs do not

04:06.240 --> 04:10.960
correctly understand what it means for a group of stones to be alive or dead

04:10.960 --> 04:18.320
which is the most basic concept in the game of go they have only a limited

04:18.320 --> 04:25.160
fragmentary approximation to the definition of life and death and that's

04:25.160 --> 04:30.840
actually a symptom of one of the weaknesses of training circuits to learn

04:30.840 --> 04:35.640
these concepts circuits are a terrible representation for concepts such as

04:35.720 --> 04:40.040
life and death which can be written down in python in a couple of lines can be

04:40.040 --> 04:45.480
written in logic in a couple of lines but in circuit form you can't actually

04:45.480 --> 04:49.800
write a correct definition of life and death at all you can only write finite

04:49.800 --> 04:55.640
approximations to it and the systems are not learning a very good approximation

04:55.640 --> 05:00.840
and so they are very vulnerable and this turns out to be applicable not just a

05:00.840 --> 05:05.360
category but to all the other leading go programs which are trained by completely

05:05.360 --> 05:09.600
different teams on completely different data using different training regimes

05:09.600 --> 05:16.400
but they all fail against this very simple strategy so this suggests that

05:16.400 --> 05:23.120
actually the systems that we have been building are we are overrating them in a

05:23.120 --> 05:29.520
real sense and I think that's important to understand and human beings right

05:29.520 --> 05:35.320
another way to make this argument is look at things that humans can do for

05:35.360 --> 05:41.480
example we can build the large interferometric gravitational observatory

05:41.480 --> 05:46.360
so these are black holes colliding on the other side of the universe this is

05:46.360 --> 05:54.400
the the LIGO detector and which is several kilometers long it's full of

05:54.400 --> 06:00.800
physics and is able to detect distortions of space down to 18 the 18th

06:00.880 --> 06:07.640
decimal place and was able to actually measure exactly what the physicists

06:07.640 --> 06:13.040
predicted would be the shape of the waveform arriving from the collision of

06:13.040 --> 06:16.840
two black holes and was even able to measure the masses of the black holes

06:16.840 --> 06:25.360
on the other side of the universe when they collided so could chat gpt do

06:25.400 --> 06:33.080
this could any deep learning system do this given that there are exactly zero

06:33.080 --> 06:40.200
training examples of a gravitational wave detector I think at the moment there

06:40.200 --> 06:48.640
is still a long way to go on the other hand people are extremely ingenious and

06:48.640 --> 06:53.440
people are working on hybrids of large language models with reasoning and

06:53.440 --> 06:59.080
planning engines that could start to exhibit these capabilities quite soon

06:59.080 --> 07:05.840
so people I respect a great deal think we might only have five years until this

07:05.840 --> 07:12.360
happens almost everyone has now gone from 30 to 50 years which was the

07:12.360 --> 07:19.840
estimate a decade ago to five to 20 years which is the estimate right now so

07:19.880 --> 07:24.880
unlike fusion this is getting closer and closer and closer rather than further

07:24.880 --> 07:29.560
and further into the future so we have to ask what happens if we actually

07:29.560 --> 07:36.760
succeed in creating general purpose AI and the reason we are trying to do it

07:36.760 --> 07:42.160
is because it could be so transformative to human civilization very crudely our

07:42.160 --> 07:46.560
civilization results from our intelligence if we have access to a lot

07:46.560 --> 07:51.440
more we could have a lot better civilization one thing we could do is

07:51.440 --> 07:57.520
simply deliver what we already know how to deliver which is a nice middle class

07:57.520 --> 08:00.880
standard living if you want to think of it that way we could deliver that to

08:00.880 --> 08:07.000
everyone on earth at almost no cost and that would be about a tenfold increase

08:07.160 --> 08:17.280
in GDP and the net present value of that is 13.5 quadrillion dollars so that's a

08:17.280 --> 08:23.200
lower bound on the cash value of creating general purpose AI so if you want

08:23.200 --> 08:26.880
to understand why we're investing hundreds of billions of pounds in it

08:26.880 --> 08:36.960
it's because the value is millions of times larger than that and so that creates

08:37.000 --> 08:43.320
a magnet in the future that is pulling us forward inexorably my friend Jan

08:43.320 --> 08:48.600
Tallinn here likes to call this mollock right this sort of ineluctable force

08:48.600 --> 08:54.040
that draws people towards something even though they know that it could be their

08:54.040 --> 08:59.840
own destruction and we could actually have an even better civilization right

08:59.840 --> 09:07.680
we could have we could one day have a clicker that works we could have health

09:07.680 --> 09:12.520
care that's a lot better than we do now we could have education that could be

09:12.520 --> 09:19.360
brought to every child on earth that would exceed what we can get from even a

09:19.360 --> 09:25.200
professional human tutor this I think is the thing that is most feasible for us

09:25.280 --> 09:30.280
to do that would benefit the world in this decade and I think this is entirely

09:30.280 --> 09:34.640
possible health care is actually a lot more difficult for all kinds of reasons

09:34.640 --> 09:41.160
but education is a digital good that can be delivered successfully and we could

09:41.160 --> 09:49.200
also have much better progress in science and so on so on the other hand AI

09:49.200 --> 09:55.080
amplifies a lot of difficult issues that policymakers have been facing for quite

09:55.080 --> 10:05.480
a while so one is its ability to magnify the pollution of our information

10:05.480 --> 10:11.960
ecosystem with disinformation what some people call truth decay and this is

10:11.960 --> 10:17.560
happening at speed but if we thought about it really hard I could actually

10:17.560 --> 10:21.960
help in the other direction it could help clean up the information ecosystem

10:21.960 --> 10:26.760
it could be used as a detector of misinformation as something that

10:26.760 --> 10:32.040
assembled consensus truth and made it available to people we're not using it

10:32.040 --> 10:38.600
in that way but we could ditto with democracy is it being suppressed by

10:38.600 --> 10:44.400
surveillance and control mechanisms or could we use AI systems to strengthen it

10:44.400 --> 10:51.720
to allow people to deliberate cooperate and reach consensus on what to do could

10:51.720 --> 10:58.040
it be that individuals are empowered or the current trajectory that we're on

10:58.040 --> 11:03.120
individuals being enfeebled as we gradually take over more and more of the

11:03.120 --> 11:09.040
functions of civilization and and humans lose the ability to even run their own

11:09.040 --> 11:15.480
civilization as individuals right these are important questions that we have to

11:15.480 --> 11:19.120
address while we're considering all of the safety issues that I'll be getting

11:19.120 --> 11:24.520
to soon there's inequality right now we're on the path of magnifying it with

11:24.520 --> 11:34.840
AI but it doesn't have to be that way and so on so let me I won't go through all

11:34.840 --> 11:39.600
of these issues because they're they're all each of them worthy of an entire

11:39.640 --> 11:51.360
talk in themselves so the I would say the sort of the mid term question is what

11:51.360 --> 11:56.760
are humans going to be doing right if we have general purpose AI that can do all

11:56.760 --> 12:02.800
the tasks or nearly all the tasks that human beings get paid for right now what

12:02.800 --> 12:08.480
will humans do and this is not a new issue Aristotle talked about it in 350

12:08.480 --> 12:15.080
BC Keynes since we're in Milton it's odd that we pronounce it milk Milton Keynes

12:15.080 --> 12:21.080
but he his name is pronounced Keynes even though the town is named after him so so

12:21.080 --> 12:25.600
Keynes in 1930 said thus for the first time since his creation man will be

12:25.600 --> 12:30.240
faced with Israel his permanent problem how to use his freedom from pressing

12:30.240 --> 12:34.400
economic cares which science will have one for him to live wisely and agreeably

12:34.400 --> 12:40.640
and well so this is a really important problem and again this is one that

12:40.640 --> 12:47.960
policymakers are misunderstanding I would say that the default answer in most

12:47.960 --> 12:54.120
governments around the world is will retrain everyone to be a data scientist

12:54.120 --> 12:58.960
as if somehow the world needs three and a half four billion data scientists I

12:58.960 --> 13:04.880
think that's probably not the answer but this is again you know the default path

13:04.880 --> 13:12.520
is one of enfeeblement which is illustrated really well by by Wally so

13:12.520 --> 13:19.240
my my answer to this question is that in the future if we are successful in

13:19.240 --> 13:25.240
building AI that is safe that does a lot of the tasks that we want done for us

13:25.320 --> 13:30.600
most human beings are going to be in these interpersonal roles and for those

13:30.600 --> 13:37.720
roles to be effective they have to be based on understanding right why is a

13:37.720 --> 13:43.440
surgeon effective at fixing a broken leg because we have done centuries of

13:43.440 --> 13:50.960
research in medicine and surgery to make that a very effective and in some

13:51.000 --> 13:56.880
countries very highly paid and very prestigious but most into personal roles

13:56.880 --> 14:02.160
for example think about childcare or elder care not highly paid not highly

14:02.160 --> 14:06.600
prestigious because they are based on no science whatsoever despite the fact

14:06.600 --> 14:10.760
that our children are our most precious possessions as people politicians like

14:10.760 --> 14:16.200
to say a lot in fact we don't understand how to look after and we don't

14:16.200 --> 14:21.000
understand how to make people's lives better so this is a a very different

14:21.000 --> 14:26.280
direction for science much more focused on the human than on the physical

14:26.280 --> 14:38.920
world okay so now let me move on if I can get the next light up to to Alan

14:38.920 --> 14:45.080
Shuring's view of all this what happens if we succeed he said that it seems

14:45.080 --> 14:48.920
parable the ones machine thinking method has started it would not take long to

14:48.920 --> 14:54.040
outstrip our feeble powers at some stage therefore we should have to expect the

14:54.040 --> 15:02.680
machines to take control so he said this in 1951 and to a first approximation for

15:02.680 --> 15:10.280
the next 70 odd years we paid very little attention to what his warning was

15:11.160 --> 15:16.520
and I used to illustrate this with the following imaginary email conversation

15:18.600 --> 15:25.480
so an alien civilization sends email to the human race humanity at un.org be

15:25.480 --> 15:30.040
warned we shall arrive in 30 to 50 years that was what most AI people thought back

15:30.040 --> 15:37.720
then now we would say maybe 10 to 20 years and humanity replies humanity is

15:37.720 --> 15:41.400
currently out of the office who will respond to your message when we return

15:42.520 --> 15:49.080
and then there should be a smiley face there it is okay so that's now changed

15:51.000 --> 15:54.840
unfortunately that slide wasn't supposed to come up like that let me see if we can

15:54.840 --> 16:00.600
oh well can't fix it now so I think early on this year three things happened in

16:00.600 --> 16:06.120
very quick succession so GPT-4 was released and then Microsoft which had been working

16:06.120 --> 16:11.720
with GPT-4 for several months at that point published a paper saying that GPT-4

16:11.720 --> 16:17.320
exhibited sparks of artificial general intelligence exactly what Turing warned us about

16:18.440 --> 16:25.240
and then FLI released the open letter asking for a pause on giant AI experiments

16:26.200 --> 16:33.800
and I think at that point very clearly humanity returned to the office and they saw the emails

16:33.800 --> 16:40.920
from the aliens and the reaction since then I think has been somewhat similar to what would

16:40.920 --> 16:48.120
happen if we really did get an email from the aliens there have been a global calls for action

16:48.120 --> 16:54.840
the very next day UNESCO responded directly to the open letter asking all its member governments

16:54.840 --> 17:01.400
which is all the countries on earth to immediately implement the AI principles in legislation

17:02.120 --> 17:07.560
in particular principles that talk about robustness safety predictability and so on

17:08.760 --> 17:15.320
and then you know there's China's AI regulations the US got into the act very quickly the White

17:15.320 --> 17:23.560
House called emergency meeting of AI CEOs open AI calling for governments to regulate AI and so on

17:24.120 --> 17:29.240
and I ran out of room on the slide on June 7th with Rishi Sunak announcing the global summit on

17:29.320 --> 17:35.160
AI safety which is happening tomorrow so lots of other stuff has happened since then

17:36.040 --> 17:42.360
but it's really I would say to to the credit of governments around the world how quickly

17:43.080 --> 17:50.680
they have changed their position on this for the most part governments were saying you know

17:50.680 --> 17:59.080
regulation stifles innovation you know if someone did mention risk it was either dismissed

17:59.720 --> 18:05.720
or or viewed as something that was easily taken care of by the market by liability and so on

18:06.360 --> 18:12.280
so I would say that the the the view the understanding has changed dramatically and that could not have

18:12.280 --> 18:21.400
happened without the fact that politicians started to use chat GBT and they saw it for themselves

18:22.440 --> 18:25.000
and I think that changed people's minds

18:27.720 --> 18:36.120
so the question we have to face then is this one right how do we retain power over entities more

18:36.120 --> 18:41.240
powerful than ourselves forever right and I think this is the question that Turing asked himself

18:42.840 --> 18:46.680
and gave that answer we would have to expect them to take control so in other words

18:47.400 --> 18:53.000
this question doesn't have an answer but I think there's another version of the question

18:53.000 --> 18:56.520
which works somewhat more to our advantage

18:58.920 --> 19:07.720
it should appear any second if right and it has to do with how we define what we're trying to do

19:08.280 --> 19:16.440
what is the system that we're building what problem is it solving and we want a problem

19:16.440 --> 19:22.680
such that we we set up an AI system to solve that problem so the standard model that I gave you

19:22.680 --> 19:29.640
earlier was systems whose actions can be expected to achieve their objectives and that's exactly

19:29.640 --> 19:35.480
where things go wrong that systems are pursuing objectives that are not aligned with what humans

19:35.480 --> 19:40.360
want the future to be like and then you're setting up a chess match between humanity

19:40.360 --> 19:47.720
and a machine that's pursuing a misaligned objective so instead we want to figure out

19:48.520 --> 19:55.800
a problem whose solution is such that we're happy for AI systems to instantiate that solution

19:57.240 --> 20:04.200
okay and it's not imitating human behavior which is what we're training LLMs to do that's actually

20:04.200 --> 20:13.000
the fundamental and basic error and that's essentially why we can't make LLMs safe

20:13.800 --> 20:17.160
because we have trained them to not be safe and trying to put

20:19.480 --> 20:24.920
trying to put sticking plasters on all all the problems after the fact is never going to work

20:25.800 --> 20:31.160
so instead I think we have to build systems that are provably beneficial to humans and

20:31.720 --> 20:37.320
the way I'm thinking about that currently is that the system should act in the best interests of

20:37.320 --> 20:45.640
humans but be explicitly uncertain about what those best interests are and this this I'm just

20:45.640 --> 20:52.120
telling you in English and it can be written in a formal framework called an assistance game

20:52.840 --> 21:00.280
so what we do is we build assistance game solvers we don't build objective maximizers

21:00.280 --> 21:05.480
which is what we have been doing up to now we build assistance game solvers this is a different

21:05.480 --> 21:10.440
kind of AI system when we've only been able to build very simple ones so far so we have a long

21:10.440 --> 21:17.320
way to go but when you build those systems and look at the solutions they exhibit the properties

21:17.320 --> 21:23.000
that we want from AI systems they will defer to human beings and in the extreme case they will

21:23.000 --> 21:28.280
allow themselves to be switched off in fact they want to be switched off if we want to switch them

21:28.280 --> 21:34.040
off because they want to avoid doing whatever it is that is making us upset they don't know what it is

21:34.040 --> 21:39.480
because they're uncertain about our preferences but they want to avoid upsetting us and so they

21:39.480 --> 21:44.680
are happy to be switched off in fact this is a mathematical theorem they have a positive

21:44.680 --> 21:51.080
incentive to allow themselves to be switched off and that incentive is connected directly

21:51.080 --> 21:54.280
to number two right the uncertainty about human preferences

21:56.360 --> 22:03.000
so there's a long way to go as I said and we're not ready to say okay everyone in all these

22:03.000 --> 22:08.200
companies stop doing what you're doing and start building these things instead right that probably

22:08.200 --> 22:12.520
is not going to go down too well because we don't really know how to build these things at scale

22:13.400 --> 22:18.920
and to deliver economic value but in the long run this is the right way to build AI systems

22:19.400 --> 22:25.880
so in between what should we do and this is a lot about what's going to be discussed tomorrow

22:27.800 --> 22:31.720
and there's a lot so this is in a small font I apologize to those of you at the back there's

22:31.720 --> 22:38.680
a lot to put on this slide we need first of all a cooperation on AI safety research it's got to

22:38.680 --> 22:45.240
stop being a cottage industry with a few little academic centers here and there it's also got to

22:45.240 --> 22:53.240
stop being what a cynic might describe as a kind of whitewashing operation in companies

22:53.240 --> 22:59.400
where they try to avoid the worst public relations disasters like you know the language model used

22:59.400 --> 23:07.960
a bad word or something like that but in fact those efforts have not yielded any real safety

23:07.960 --> 23:13.640
whatsoever so there's a great deal of research to do on alignment which is what I just described

23:13.640 --> 23:19.240
on containment how do you get systems that are restricted in their capabilities that are not

23:19.800 --> 23:24.920
directly connected to email and bank accounts and credit cards and social media and all those

23:24.920 --> 23:31.960
things and if there are I think there are probably ways of building restricted capability systems

23:32.600 --> 23:37.960
that are provably safe because they are restricted to only operate

23:38.200 --> 23:47.400
uh provably sound reasoning engines for example um but the the bigger point is stop thinking about

23:47.400 --> 23:58.680
making AI safe start thinking about making safe AI right these are just two different mindsets

23:58.680 --> 24:04.840
the making AI safe says we build the AI and then we have a safety team whose job it is

24:04.920 --> 24:10.680
to stop it from behaving badly that hasn't worked and it's never going to work we have

24:10.680 --> 24:16.680
got to have AI systems that are safe by design and without that we are lost

24:19.400 --> 24:26.040
we also need I think some international regulatory level to coordinate the regulations

24:26.040 --> 24:31.800
that are going to be in place across the various national regimes so we have to start

24:32.360 --> 24:38.600
probably with national regulation but and we can coordinate very easily for example we could

24:38.600 --> 24:46.520
start coordinating tomorrow to agree on what would be a baseline for regulation I put a couple of

24:46.520 --> 24:57.560
other things there that went by too quickly so I actually want to go back okay too far all right

24:58.040 --> 25:05.560
um so the the light blue line transparent explainable analytical substrate is really

25:05.560 --> 25:11.800
important uh the moment we're building AI systems that are black boxes we have no idea how they work

25:11.800 --> 25:16.760
we have no idea what they're going to do and we have no idea how to get them to behave themselves

25:16.760 --> 25:26.600
properly uh so my guess is that if we define regulations appropriately so that companies

25:27.320 --> 25:34.040
have to build AI systems that they understand and predict and control successfully those

25:34.040 --> 25:40.200
AI systems are going to be based on a very different technology not giant black box circuits

25:40.200 --> 25:47.000
that are trained on vast quantities of data but actually well understood component based

25:47.000 --> 25:53.320
systems that build on centuries of research in logic and probability where we can actually

25:53.320 --> 25:59.720
prove that these systems are going to behave in certain ways the second thing the dark blue

25:59.720 --> 26:05.400
secure PCC based digital ecosystem what is that uh so PCC is proof carrying code

26:05.640 --> 26:12.120
and what we need here is a way of preventing bad actors from deploying unsafe systems so it's one

26:12.120 --> 26:18.120
thing to say here's how you build safe systems and everyone has to do that it's another thing to

26:18.120 --> 26:24.200
say how do you stop people from deploying unsafe systems who don't want safe AI systems they want

26:24.200 --> 26:32.600
whatever they want this is probably even more difficult policing software is I think impossible

26:32.600 --> 26:41.080
is I think impossible so the the place where we do have control is at the hardware level because

26:41.080 --> 26:46.440
hardware uh first of all to build your own hardware costs about a hundred billion dollars

26:47.080 --> 26:54.520
and tens of thousands of highly trained engineers so it provides a control point that's very difficult

26:54.520 --> 27:00.280
for bad actors to get around and what the hardware should do is basically check the proof

27:01.080 --> 27:03.400
of a software object before it's run

27:05.560 --> 27:08.920
and check that in fact this is a safe piece of software to run

27:09.880 --> 27:15.560
and proof carrying code is a technology that allows hardware to check proofs very efficiently

27:16.280 --> 27:21.640
but of course the owners then is on the developer to provide a proof that their system is in fact

27:21.640 --> 27:30.920
safe and so that's a prerequisite for this approach okay let me talk a little bit about

27:30.920 --> 27:41.960
regulations so a number of acts already in in the in the works for example the european AI act

27:42.680 --> 27:48.200
has a hard ban on the impersonation of human beings so you have a right to know if you're

27:48.200 --> 27:53.080
interacting with a machine or a human this to me is the easiest the lowest hanging fruit

27:53.800 --> 27:59.960
that every jurisdiction in the world could implement pretty much tomorrow if they so decided

28:00.680 --> 28:08.280
and I believe that this is how legislators wake up those long unused muscles that have

28:08.280 --> 28:14.120
lain dormant for decades while technology has just moved ahead unregulated so this is the place to

28:14.200 --> 28:23.080
start but we also need some regulations on the design of AI systems specifically so

28:23.800 --> 28:29.800
a provably operable kill switch is a really important and basic thing if your system is

28:29.800 --> 28:36.600
misbehaving there has to be a way to turn it off and this has to apply not just to the system that

28:36.600 --> 28:43.320
you made but if it's an open source system any copy of that system and that means that the kill

28:43.320 --> 28:48.440
switch has got to be remotely operable and it's got to be non-removable

28:51.400 --> 28:55.880
so that's a technological requirement on open source systems and in fact if you want to be in

28:55.880 --> 29:01.000
the open source business you're going to have to figure this out you're actually going to subject

29:01.000 --> 29:08.680
yourself to more regulatory controls than people who operate on closed source and that's exactly

29:08.680 --> 29:15.720
as it should be imagine if we had open source enriched uranium right and the purveyor of enriched

29:15.720 --> 29:21.800
uranium was responsible for all the enriched uranium that they pervade to anybody around the

29:21.800 --> 29:27.080
world they're going to have a higher regulatory burden because that's a blinking stupid thing to

29:27.080 --> 29:32.280
do right and so you would expect there to be a higher burden if you're going to do blinking stupid

29:32.280 --> 29:39.960
things um and then red lines this is probably the most important thing so we don't know how to

29:39.960 --> 29:46.360
define safety so i can't write a law saying your system has to be provably safe because it's very

29:46.360 --> 29:53.000
hard to write the dividing line between safe and unsafe you know if you uh asimov's law you can't

29:53.800 --> 30:00.520
harm human beings well what does harm mean that's very hard to define but we can scoop out

30:01.320 --> 30:08.280
very specific forms of harm that are absolutely unacceptable so self-replication of computer

30:08.280 --> 30:14.040
systems would absolutely be unacceptable that would be basically a harbinger of losing human

30:14.040 --> 30:20.840
control if the system can copy itself onto other computers or break into other computer systems

30:21.480 --> 30:26.120
absolutely systems should not be advising terrorists on building biological weapons

30:26.120 --> 30:31.400
and so on so these red lines are things that any normal person would think well obviously

30:33.000 --> 30:39.480
the software system should not be doing that and the developers are going to say oh well this is

30:39.480 --> 30:43.880
really unfair because it's really hard to make our systems not do this

30:46.040 --> 30:54.040
and their response is well tough right really you're spending hundreds of billions of pounds

30:54.040 --> 30:58.840
on this system and you can't stop it from advising terrorists on building bioweapons

30:59.400 --> 31:07.640
well then you shouldn't be in business at all right this is not hard and legislatures by uh by

31:07.640 --> 31:14.600
implementing these red lines would put the onus on the developer to understand how their own systems

31:14.600 --> 31:20.520
work and to be able to predict and control their behavior which is an absolute minimum we should

31:20.520 --> 31:28.440
ask from any industry let alone one that could have such a massive impact and is hoping for

31:28.440 --> 31:40.600
quadrillions of dollars in profits thank you thank you very much professor russell quick

31:40.600 --> 31:46.200
question maybe before we move on to the next speaker there was some good news in there it is

31:46.200 --> 31:51.000
that we have ideas on how to make safe ai but how long do you think we're going to need

31:51.640 --> 31:55.240
how long is it going to take by default that we have these ideas worked out and how long

31:55.240 --> 32:00.920
might it take if we had all the smart people in the world give up their current focus and instead

32:00.920 --> 32:08.840
work on this uh i think these are really important questions because the um the political dynamic is

32:08.840 --> 32:14.760
going to depend to some extent on how the ai safety community responds to this challenge

32:14.760 --> 32:21.000
uh because if the ai safety community fails to make progress on any of this stuff the developers

32:21.000 --> 32:27.240
can point and say look you know you guys are asking for stuff that isn't really possible

32:27.240 --> 32:32.600
and we should be allowed to just do what we want um but if you look at the nuclear industry right

32:32.600 --> 32:39.160
how does that work the regulator says to the nuclear plant operator show me that your plant

32:39.160 --> 32:45.960
has a meantime to failure of 10 million years or more and the operator has to give them a full

32:45.960 --> 32:53.240
analysis with fault trees and a probabilistic uh calculations and the regulator can push back and

32:53.240 --> 32:57.720
say you know i don't agree with that independence assumption you know these components come from

32:57.720 --> 33:02.920
the same manufacturer so not independent and come back with a better analysis and so on

33:03.560 --> 33:10.840
uh at the moment there is nothing like that in the ai industry there is no logical connection

33:11.560 --> 33:16.840
between any of the evidence that people are providing and the claim that the system is

33:16.840 --> 33:25.160
actually going to be safe right that argument is just missing um now the nuclear industry

33:25.560 --> 33:33.080
probably spends more than 90 percent of its r&d budget on safety

33:35.080 --> 33:39.880
one way you can tell i i got the statistic from one of my nuclear engineering colleagues

33:40.440 --> 33:46.680
that for the typical nuclear plant in the u.s for every kilogram of nuclear plant

33:47.240 --> 33:52.120
there are seven kilograms of regulatory paperwork i kid you not

33:52.200 --> 34:00.840
so that tells you something about how much of an emphasis that has been on safety in that industry

34:02.360 --> 34:09.880
and also you know why is there to a first approximation no nuclear industry today

34:10.520 --> 34:17.800
is because of Chernobyl and because of a failure in safety actually deliberately bypassing

34:18.760 --> 34:22.600
safety measures that they knew were necessary in order to save money

34:23.720 --> 34:27.080
we'll take one question from the audience provided it's a quick question

34:27.800 --> 34:32.440
i see a hand over there let me dash down

34:36.040 --> 34:40.280
hi uh thanks very much for your talks here um my name is Charlie i'm a senior at UCL

34:40.920 --> 34:45.400
um one of the big reasons i think why there's so much regulation on nuclear power

34:45.880 --> 34:52.360
is widespread public opinion and protests against nuclear power from within the environmental

34:52.360 --> 34:58.040
movement so i wondered whether you uh thought if there's a similar role for public pressure or

34:58.040 --> 35:06.520
protests uh for ai as well thanks uh i think that's a very important question

35:07.160 --> 35:18.360
my sense is i i'm not really historian of the nuclear industry per se uh obviously nuclear

35:18.360 --> 35:24.360
physicists thought about safety from the beginning uh in fact so Leo Leo Zillard was the one who

35:24.920 --> 35:30.840
invented the basic idea of the nuclear chain reaction uh and he instantly thought about

35:30.840 --> 35:35.640
a physical mechanism that could keep the reaction from going supercritical and becoming a bomb

35:36.360 --> 35:39.880
right so he thought about this you know negative feedback control system

35:40.680 --> 35:45.560
with moderators that would somehow keep the reaction subcritical

35:48.040 --> 35:54.760
people in ai are not at that stage right or they just have their eyes on you know we can

35:54.760 --> 35:58.760
generate energy and they're not even thinking you know is that energy going to be in the form

35:58.760 --> 36:04.680
of a bomb or electricity right they haven't got to that stage yet so we are very much at the

36:04.680 --> 36:17.080
preliminary stage i do worry that ai should not be politicized and at the moment there's

36:17.080 --> 36:24.680
a precarious bipartisan agreement in the us and to some extent in europe i worry about that

36:24.680 --> 36:31.400
breaking down in the uk uh i think it's really important that the political message be very

36:31.400 --> 36:38.200
straightforward you can be on the side of humans or you can be on the side of our ai overlords

36:39.560 --> 36:48.520
which do you want to be on um and so let's try to keep it a unified message around uh

36:49.720 --> 36:56.200
developing technology in a way that's safe and beneficial for humans um so we can raise

36:56.760 --> 37:04.520
but we shouldn't do it in a partisan way yes and and what i i i totally sympathize with the idea

37:04.520 --> 37:11.240
that people have a right to be very upset that you know that multibillionaires are playing

37:12.600 --> 37:17.720
you know playing poker with the future of the human race um it's entirely reasonable

37:17.960 --> 37:28.600
but what i worry is exactly that uh certain types of of protest end up getting aligned

37:29.320 --> 37:36.840
in a way that's unhealthy uh it sort of becomes anti technology and we can look back at what happened

37:36.840 --> 37:45.640
with with uh gm uh organisms for example uh which which most scientists think didn't go the way

37:46.360 --> 37:51.080
uh it should have and we we lost benefits uh without gaining any safety

37:52.600 --> 37:55.320
watch to think about there thank you very much professor stewart russell

37:58.120 --> 38:01.800
we may give you the microphone again a bit later on but there's lots of other people we want to

38:01.800 --> 38:08.840
hear from now so the next speaker is kona lehi who is the ceo of conjecture many of us got a shock

38:08.840 --> 38:16.280
with gp24 or 3.5 my goodness what's going on here kona was ahead of the curve when he saw gpg2

38:16.280 --> 38:21.080
with all its warts and weaknesses he said my gosh this is going to change the world so he has been

38:21.080 --> 38:25.880
thinking about some of these issues probably for longer than the rest of us so let's hear from kona

38:25.880 --> 38:37.240
what would you like to say thank you so much so unfortunately uh professor russell has stole

38:37.240 --> 38:42.840
my favorite allen turing quote so you're going to be hearing that one again but i guess there couldn't

38:42.840 --> 38:51.800
be a more appropriate time because many years ago there lived a man named allen turing he was the

38:51.800 --> 39:00.680
godfather of computer science a titan in his field and a hero of world war two and was here

39:00.680 --> 39:06.680
at bletchley park that he did his most seminal work during the world war two and cracking the

39:06.680 --> 39:13.000
codes that the germans were using and as a very early step into the field of computer science

39:15.160 --> 39:24.760
and allen was ahead of his time in more way than one in 1951 in manchester he gave a lecture

39:24.760 --> 39:36.200
entitled intelligent machinery a heretical theory and in this lecture he said for it seems probable

39:36.200 --> 39:41.000
but once the machine thinking method had started it would not take long to outstrip

39:41.000 --> 39:46.120
our feeble powers there would be no question of the machine's dying and it would be able to

39:46.120 --> 39:52.920
converse with each other to sharpen their wits at some stage therefore we should have to expect

39:52.920 --> 40:01.880
the machines to take control and here we are 72 years later where it all began

40:02.520 --> 40:10.760
and a lot has changed since the days of allen turing computers have improved in incredible rates

40:11.400 --> 40:18.120
i'm holding my hands right now a computer of such incredible power that it would be barely

40:18.120 --> 40:27.240
imaginable to turing in his contemporaries barely one human lifetime hints and while computers have

40:27.240 --> 40:33.080
advanced a lot in many ways since the days of turing i like to believe that there would be a lot

40:33.080 --> 40:42.760
he would recognize he would recognize the basic functions of computers their memory their instructions

40:42.760 --> 40:48.680
programming code ideas that go all the way back to his seminal work on turing machines

40:49.400 --> 40:54.360
while he might not be familiar with the exact tooling he would be familiar with the general

40:54.360 --> 41:02.120
concepts around modern programming where a programmer writes code instructions the computer

41:02.120 --> 41:09.800
then executes but there is something that i'm not so sure he would so easily recognize

41:11.400 --> 41:21.160
and that is a i or in particular the neural networks that power them now we have all seen

41:21.160 --> 41:26.760
ai do truly amazing and things over the last couple of years in particular solving all these

41:26.760 --> 41:32.600
problems that previously we barely knew how to approach and you might think when you look at

41:32.600 --> 41:38.360
all these ai systems running on your phone on your computer that this is software like any other

41:38.360 --> 41:43.480
written by very clever programmers to do the useful and the marvelous things that they do

41:44.440 --> 41:52.120
and you would be wrong because ai is very different from normal software

41:53.160 --> 41:59.480
it is not written so much as it is grown so while in the traditional software you'd have

41:59.480 --> 42:07.640
a programmer sit down and write out the instruction with ai's you take huge supercomputers and massive

42:07.800 --> 42:15.160
data sets and you use these supercomputers to grow a program on your data to solve your problem

42:16.200 --> 42:22.040
and this works really well for many for many issues it has improved our ability to solve many

42:22.040 --> 42:28.360
very useful tasks and do many things that we did not know how to do before and our ability to

42:28.920 --> 42:34.520
grow these ai's continues to improve and get better and better while at the same time though

42:34.520 --> 42:42.840
our ability to understand our ai's has not because these ai's are not like well written code

42:42.840 --> 42:49.960
that a human could read they're more like giant blobs of numbers and we know if we execute them

42:49.960 --> 43:02.280
they work but we have no idea why and only quite recently have we discovered that as we scale up

43:02.280 --> 43:10.280
these systems and as we build bigger computers and bigger ai systems something quite remarkable

43:10.280 --> 43:20.600
happens they become more intelligent more capable now of course there are many details

43:20.600 --> 43:25.720
that have to be gotten right there are many parameters you have to set correctly you have

43:25.720 --> 43:29.800
to have enough data you have to make sure your computer is set up correctly but fundamentally

43:30.760 --> 43:36.120
is a stability in this prediction sometimes also called the scaling laws and that as our

43:36.120 --> 43:42.040
systems become bigger as our computers become more powerful the systems learn higher and

43:42.040 --> 43:52.600
higher order patterns more and more complex skills knowledge abilities and as they become more powerful

43:52.600 --> 43:59.400
and more capable they are also becoming even harder to understand and to control

44:01.960 --> 44:08.760
and this is why we are all here today back to where it all began we have now returned

44:09.400 --> 44:18.840
Bletchley Park because as Turingell really realized it is really quite simple if we build

44:18.840 --> 44:27.560
machines that are more competent than us at manipulation perception politics business science

44:28.200 --> 44:36.040
and everything else and we do not control them then the future will belong to the machines

44:37.400 --> 44:44.120
not to humans and the machines are unlikely to feel particularly sentimental about keeping us

44:44.120 --> 44:52.600
around for very long and so here we are face with an exponentially increasing more powerful

44:52.600 --> 44:59.880
by the day ai by the day as we learned with covid there are exactly two times one can react

44:59.880 --> 45:09.480
to an exponential too early or too late if we wait until agi if we wait until we see the

45:09.480 --> 45:18.920
self-improving powerful general purpose systems it will be too late far far too late and this is

45:18.920 --> 45:25.480
why i am so happy to see the uk government take leadership in the first of many important steps

45:25.480 --> 45:32.440
towards the necessary international coordination to address this extinction level threat that is

45:32.440 --> 45:41.240
facing us all and the very first step as so many academics industry leaders and even governments

45:41.240 --> 45:49.560
have already taken is to firmly acknowledge the reality of what we face the potential extinction

45:49.560 --> 45:59.080
of our species by ai private ai companies are scaling their ai systems as we speak

45:59.880 --> 46:06.920
and they will not ask for permission and they will not stop unless we make them they are already

46:06.920 --> 46:13.960
lobbying our governments for with ineffective policies such as responsible scaling in attempt

46:13.960 --> 46:21.240
to prevent actually effective policy like the oil ceo is a past trying to lobby against climate

46:21.240 --> 46:27.880
change regulation that would hurt their bottom lines the good news is that is not yet too late

46:28.520 --> 46:34.360
to stop this to prevent the building of such deadly machines until we know how to build them

46:34.360 --> 46:42.360
safely that is why there is nothing more important than for people to know the truth

46:43.480 --> 46:50.760
that a small group of unelected unaccountable private companies are running a deadly experiment

46:50.760 --> 46:59.320
on you on your family and on everyone on earth without your consent or even knowledge despite

46:59.320 --> 47:10.520
they themselves admitting that these risks are real at this point all of us agree that there is

47:10.520 --> 47:16.760
that we are playing Russian roulette with the entire planet and we're only quibbling about

47:16.760 --> 47:22.440
how many poles are left until the bullet now in my personal opinion if you ever find yourself

47:22.440 --> 47:31.000
playing Russian roulette i suggest you put down the gun and so we have to speak up and demand

47:31.000 --> 47:39.480
action if you want a future for us our children and our species it's not yet too late but it will

47:39.480 --> 47:46.040
be soon we stand at a historic moment today at where it all began

47:47.080 --> 47:50.200
let's leave park well let's not waste it

47:58.680 --> 48:01.880
thank you corner let's take a couple of questions from the audience

48:03.400 --> 48:07.880
where am i seeing there's one at yeah if you take the microphone in there thanks

48:09.720 --> 48:16.440
hi very nice presentation by the way i wanted to ask

48:18.200 --> 48:23.160
with the current situation that's going on with ai currently do you really think if you were to be

48:23.160 --> 48:29.320
a philosopher maybe for five minutes do you really think that currently society is really ready for

48:29.320 --> 48:37.560
it i mean sure we can adapt in somewhat but as things are uh dripping around us it doesn't seem

48:37.640 --> 48:44.840
like we're really anywhere near to i mean accepting it we're just a such fair and all

48:45.560 --> 48:52.440
and other factors coming yes so the simple answer is no we are absolutely ready

48:53.240 --> 48:58.360
we should be playing with nuclear fire or worse our civilization does not have the level of maturity

48:58.360 --> 49:02.200
to be able to handle technology like this and this is why i'm not extremely optimistic about the

49:02.200 --> 49:08.040
future the truth is is that whether it's ai or something else ai technology is becoming more

49:08.040 --> 49:13.320
and more powerful this is just how it is this is how the technology works and our society

49:13.320 --> 49:19.880
has to adapt to this if we as a society do not find a way to as an entire civilization as an

49:19.880 --> 49:26.600
international civilization work together in a way that we can responsibly steward technology so powerful

49:27.240 --> 49:33.800
that it can destroy anything then humanity is on a timer whether it's ai or whatever comes after

49:33.800 --> 49:41.720
that we need to improve our society or that's it sometimes people grow up in a hurry sometimes

49:41.720 --> 49:46.120
people are a bit childish and suddenly there's a big threat ahead and my goodness we grow up

49:46.120 --> 49:50.840
is that what you see happening with humanity now we're not ready for ai but as we understand the

49:50.840 --> 49:57.880
risks we will change our mode of operation i sure hope so and if it happens it will not happen as

50:00.280 --> 50:04.280
because something about it is because people like the people in this room actually do something

50:04.280 --> 50:09.560
about it stand up and make a difference in our institutions and our society it is there is no

50:09.560 --> 50:14.760
law of physics that forbids us from having a good future and taking control of our future

50:14.760 --> 50:20.520
and building wonderful safe technology for all but there is also no law that mandates it

50:20.920 --> 50:27.560
i saw one more hand up yes if you take give the mic to the the woman in the glasses thank you

50:32.120 --> 50:37.400
oh and i was gonna say i know that one of your policies is that you want to cap compute and

50:37.400 --> 50:41.400
and i'm just wondering whether you are gonna suggest that at the summit and what you think

50:41.400 --> 50:47.800
the government's response to that will be compute caps are absolutely the most sensible direct

50:47.800 --> 50:52.760
policy for us as a species to follow the main reason for this is is that it is the bottleneck

50:52.760 --> 50:58.040
towards building the actually existential dangerous systems just explain what these view caps are

50:58.040 --> 51:03.880
yes so compute is basically limiting the maximum size of the supercomputers i talked about the limit

51:03.880 --> 51:09.640
the maximum size our ai is and our computers are allowed to be and so we can limit hypothetically

51:09.640 --> 51:13.960
how intelligent they will be things actually get dangerous because we don't know what pops out of

51:13.960 --> 51:18.440
our experiments until we run them so it might already be that we're always too late our computers

51:18.440 --> 51:24.280
might already be big enough to end the world we don't know but hopefully not in that case the first

51:24.280 --> 51:28.840
as i say with russian roulette if you pull the trigger once and there was no bullet the correct

51:28.840 --> 51:35.080
move is not to pull it again the first thing you do is don't pull it again until you know if there's

51:35.080 --> 51:40.280
a bullet and where it is and if you know there's one definitely don't pull it so this is my opinion

51:40.360 --> 51:44.920
on this i will definitely be open to talking and would like to suggest this to all policy makers

51:44.920 --> 51:49.800
of all nations and leave this to say i think there is extremely strong resistance to this

51:49.800 --> 51:54.200
for the obvious reason that this cuts into the bottom lines of very powerful big tech companies

51:54.200 --> 52:00.520
who have extreme lobbying power and control over governments this is well i mean it's very simple

52:00.520 --> 52:05.480
there's a lot of people who gain a lot of benefits from continuing to pull that trigger and we have

52:05.480 --> 52:09.640
to make them stop and they are going to fight us every step of the way it's just how it works

52:10.840 --> 52:12.840
thank you very much Connolly

52:18.760 --> 52:23.080
and i'd like to invite the eight members of the panel to come up on stage

52:23.080 --> 52:27.800
and we're going to continue the conversation please self-organize on the seats

52:28.520 --> 52:32.360
stewart i don't think we've got a seat for you at this stage we can either find another seat for

52:32.360 --> 52:39.960
you or we'll let you come back on the stage later so sit down in whichever system you like

52:39.960 --> 52:45.320
and we will hear from each of these panelists what they think has been missing from the

52:45.320 --> 52:49.560
conversation so far maybe they've got an alternative view maybe they don't think we're playing

52:49.560 --> 52:54.360
russian roulette and a different metaphors appropriate maybe they would like to express

52:54.360 --> 52:59.960
what they think the politicians they're closest to should be saying maybe they'd like to comment on

52:59.960 --> 53:06.520
some of the other issues of safety so shall we start at the far end there and let's just move

53:06.520 --> 53:11.000
along the panel i'll give you two minutes each to contribute what you'd like into this conversation

53:11.000 --> 53:16.600
and then we'll hear from the audience so and let me introduce you as well so

53:18.440 --> 53:23.480
i'm sure i can do that my name is mark brackle i'm the director of policy at the future of life

53:23.480 --> 53:31.480
institute um and truly support what stewart and corner have been saying i think when we

53:31.480 --> 53:36.040
looked at the summit about six seven weeks ago we put out a set of recommendations ahead of time

53:36.520 --> 53:40.920
i think there were three traps that we identified that we were worried the summit would fall into

53:42.040 --> 53:45.880
the summit potentially not addressing the full range of risks all the way from

53:45.880 --> 53:51.880
bias and discrimination up to extinction it not being inclusive namely china not being invited

53:52.520 --> 53:57.720
and it being a setting where the big ceo's would sort of run the show and there would be maybe

53:57.720 --> 54:04.440
some token academics uh at a panel in a room the night before so i think if we sort of assess

54:04.440 --> 54:11.000
what the summit is looking like now the night before the actual event i think

54:11.720 --> 54:17.720
sort of we can be reasonably happy i saw this morning that china will in fact be invited

54:18.520 --> 54:26.040
and that it will be an inclusive summit in that nation of the world will get a seat at the table

54:26.040 --> 54:32.680
so i think that's progress so that's very good if we think about the harms and the range of harms

54:32.680 --> 54:38.280
that are being discussed one of the things fli recommended for the summit was grounding it in

54:38.280 --> 54:43.800
examples of large-scale ai harm that we've already seen such as the australian robo debt scandal

54:43.800 --> 54:49.080
or the dutch benefit scandal from the netherlands myself and it's it's a political scandal really

54:49.080 --> 54:54.840
dominating the national scene to to show that very simple algorithms can already have a very large

54:54.840 --> 54:59.640
impact in countries that were rushing towards adoption and to show the beginning of the trend

54:59.640 --> 55:03.960
line and that hasn't happened i think that's potentially a missed opportunity but i think

55:03.960 --> 55:10.920
it's really good where the uk has overall focused the summit and i think i'm sort of least optimistic

55:10.920 --> 55:15.800
when it comes to the role of companies at the summit and i think corner's done a great job at

55:15.800 --> 55:21.000
highlighting concerns that fli that many of us have around responsible scaling and this narrative

55:21.000 --> 55:25.800
being pushed by many companies as an excuse to keep going rather than making sure that whatever

55:25.800 --> 55:31.560
they put out onto the market is actually safe and i think that's a message that i hope we collectively

55:31.560 --> 55:35.800
and the people in this room that are going to the summit can still take to the participants

55:35.800 --> 55:41.000
and to the governments that are there making sure that we put the onus of what is safe and what isn't

55:41.000 --> 55:46.040
safe on the companies and they need to prove to us that what they're putting on the market is safe

55:46.040 --> 55:51.320
rather than the other way around where the default is they keep on scaling and it's up to the regulator

55:51.320 --> 55:56.920
to prove that what is safe so that i think is a key message to take so you're giving at least two

55:56.920 --> 56:02.040
cheers if not three cheers to the organisers for what you see happening already so from one

56:02.040 --> 56:08.680
dutch man to another ron rusendal is the direct deputy director general of the netherlands ministry

56:08.680 --> 56:13.560
for the interior lots of other roles what would you like to add to the conversation um well thank

56:13.560 --> 56:21.480
you very much and i'm glad to be here um um first point is that we regulate cars and we

56:21.480 --> 56:29.240
regulate pharmaceuticals um and we do so to mitigate risks of today and the risks of tomorrow

56:30.040 --> 56:37.640
um so we have to act upon risks of today like bias and risks of tomorrow

56:38.280 --> 56:46.360
um and those are global risks so we welcome the initiative of the summit but we also welcome

56:46.360 --> 56:55.160
the initiative of the tech and foyer of the un starting a high-level advisory board um and we

56:55.160 --> 57:03.960
have offered the tech and foyer to host a european um um meeting of the high-level advisory board

57:04.040 --> 57:08.760
in the hake for example in the peace palace because we support the work that we all do

57:08.760 --> 57:17.320
internationally to mitigate the risks um secondly we need some form of um early warning whatever

57:17.320 --> 57:25.080
the risks are um and whatever whether they will occur or not we have to have early warning and

57:25.080 --> 57:32.840
a rapid response mechanism uh or whatever goes uh will happen in the future and and therefore we need

57:32.840 --> 57:38.840
to um operate in a failure-driven way and we need to participate we need to coordinate but

57:38.840 --> 57:47.400
we also need to cooperate with industry with civil society with citizens with um uh industry and

57:47.400 --> 57:52.040
with governments agreement on early warning seems like something that both sides of the debate should

57:52.040 --> 57:57.080
be able to give because the people think things will go wrong and the people think things won't

57:57.080 --> 58:01.480
go wrong should be able to agree well if this happens we should all be paying more attention

58:02.440 --> 58:07.080
let's pass the microphone on to Hal Hodson who's a journalist at the Economist who has written

58:07.080 --> 58:12.840
a lot about existential risks and AI Hal thanks David um yes so my name's Hal Hodson I'm a special

58:12.840 --> 58:16.920
projects writer with the Economist I've been writing about AI for 10 years I have a degree in

58:16.920 --> 58:21.000
astrophysics and that meant that I spent a lot of time looking at a thing called the archive

58:21.000 --> 58:25.480
long before it was cool and uh papers from Facebook and Google would just turn up on the

58:25.480 --> 58:31.000
archive with no PR whatsoever and this is journalistic gold and that's how I got into it um I

58:31.000 --> 58:37.000
guess my sort of view is inherently going to be journalistic I think it is a very difficult point

58:37.000 --> 58:42.600
to make very clear decisions about what anybody ought to do about any of this I think there's

58:42.600 --> 58:46.600
from my perspective there's a huge amount of uncertainty I'm now I've now been writing about

58:46.600 --> 58:49.880
it long enough to know that there's also a lot of hype and it's not the first time there's been a

58:49.880 --> 58:56.360
huge amount of hype and I think making very clear decisions about important systems at a time that

58:56.360 --> 59:01.640
is hype filled is a difficult thing to do I think the thing that I can agree on the consensus that

59:01.640 --> 59:05.720
I can come to with probably most of the people in the room and the organizers of the summit

59:05.720 --> 59:10.840
is that there's a huge amount of science to do and it both in terms of existential risks and these

59:10.840 --> 59:15.720
sort of lower tier algorithmic risks I think there's two examples that show us that this is a perfectly

59:15.720 --> 59:21.080
plausible thing to do the first is that there was a time in the 90s when everybody was very worried

59:21.080 --> 59:28.280
about impact of bodies in the solar system to earth existential risks from asteroids and things

59:28.280 --> 59:33.400
like this and congress mandated a large amount of money to go to NASA to map all of the asteroids in

59:33.400 --> 59:38.680
the solar system and to figure out ways to nudge them off course if they come towards us and if

59:38.680 --> 59:43.800
you look at the risks as they were assessed in the 90s and the risks as they are assessed today

59:43.800 --> 59:49.400
they are massively massively dramatically lower and so that to me is a very strong case for doing

59:49.480 --> 59:54.200
science on these risks I don't know and I'd be fascinated to talk to people who do know what

59:54.200 --> 59:59.560
doing science on AI systems really looks like but it brings me to the next comparison which is

59:59.560 --> 01:00:04.520
Facebook about five years ago there was also a big panic that Facebook was determining the results

01:00:04.520 --> 01:00:09.320
of elections or you know hacking democracy essentially that is somewhat subsided now but

01:00:09.320 --> 01:00:14.680
one of the most sensible responses to that concern that I saw was also that you need to do science

01:00:14.680 --> 01:00:19.000
on Facebook just like you needed to do science on the solar system you need to start measuring

01:00:19.000 --> 01:00:24.520
things and it actually took years to force Facebook to give access to data to people like

01:00:24.520 --> 01:00:29.800
social science one it eventually sort of worked and I think there's a reason that you don't hear

01:00:29.800 --> 01:00:33.880
a huge amount about it it's because the science that's been done so far has not determined that

01:00:33.880 --> 01:00:39.720
Facebook destroyed democracy we still have at least a version of it and so I guess I would end

01:00:39.720 --> 01:00:45.880
just by a plea to you know and perhaps in the same way you were saying Stuart politically neutral

01:00:45.880 --> 01:00:50.120
science to the extent that that's possible a more of a goal than a thing that exists

01:00:50.120 --> 01:00:56.520
but do you have no whether the US focus on asteroid risk in the 90s was that bipartisan

01:00:56.520 --> 01:01:01.720
or was that a partisan issue I don't know if it was bipartisan but it went through congress so it

01:01:01.720 --> 01:01:06.600
must have been a bit bipartisan so maybe it wasn't as bad back then actually now that I think of it

01:01:06.600 --> 01:01:11.560
there's some encouraging examples there next we're going to hear from Anika Braak who's the CEO of

01:01:11.560 --> 01:01:18.600
the International Center for Future Generations tell us about your views Anika yes thank you very

01:01:18.600 --> 01:01:23.800
much and first of all I'd like to commend the UK on two things first of all their sense of humor

01:01:23.800 --> 01:01:30.200
for setting up a summit on the darkest corners of AI on the night of Halloween or the nights after

01:01:31.080 --> 01:01:37.800
and I'm surprised nobody made that joke so far and secondly for really bringing this to the

01:01:37.800 --> 01:01:45.000
attention of leaders media and the public actually I don't think Frontier AI has ever been discussed

01:01:45.000 --> 01:01:53.240
so much and the number of communicates the number of you know the the executive order the communications

01:01:53.240 --> 01:01:59.880
leaders European leaders meeting ahead of the summit negotiating late night to get to bring

01:01:59.880 --> 01:02:06.040
something here is already a measure of success and we could actually leave it here with this

01:02:06.040 --> 01:02:11.320
stellar panel and and say I think it's very important that the the civil society is meeting

01:02:11.320 --> 01:02:17.320
here this is maybe the element that is missing in the room I'd like to say we have two major

01:02:17.320 --> 01:02:22.200
challenges here one is a coordination challenge we have corporates looking at the topic they

01:02:22.920 --> 01:02:28.840
they're racing over competitive edge and we have governments who have serious geo-strategic

01:02:28.840 --> 01:02:34.680
interest and when those two come together that doesn't help collaboration so we have to think

01:02:34.680 --> 01:02:40.120
about how we get people around the table and secondly there's a democratic challenge democracy

01:02:40.120 --> 01:02:46.440
is by its very nature a slow and patient regulator and that's important I will argue that actually

01:02:47.720 --> 01:02:53.560
democracy is perfectly adapted to the society through these uncharted waters that we're experiencing

01:02:53.560 --> 01:02:59.480
at the moment but we need to make sure that the sailors of this big ship are prepared that they

01:02:59.480 --> 01:03:05.800
are well informed and that they have the tools to deal with this change and that's what the

01:03:05.800 --> 01:03:11.000
international center for future generations set out to do in Brussels that's why we moved our

01:03:11.000 --> 01:03:16.680
headquarters to Brussels to make sure that EU decision makers are well prepared because we

01:03:16.680 --> 01:03:22.120
set our best hope in the EU in this international race for governance I will leave it here for now

01:03:23.000 --> 01:03:28.760
are the EU decision makers paying attention to what you say I think they do you do here

01:03:28.840 --> 01:03:34.600
already a lot of signs that they have also recognized that we have to look at advanced

01:03:34.600 --> 01:03:41.640
artificial intelligence that regulation doesn't stop with the artificial intelligence act it's

01:03:41.640 --> 01:03:48.280
only the very start of the beginning or the first piece of the puzzle to come back to Stuart's presentation

01:03:49.080 --> 01:03:52.840
thanks next we're going to hear from Jan Tallinn who is the co-founder of Skype

01:03:53.320 --> 01:04:00.360
fli Caesar that's the center for the study of existential risks and he is also one of the

01:04:00.360 --> 01:04:05.960
advisors on the committee created by the tech and envoy for the UN so Jan what would you like

01:04:05.960 --> 01:04:12.920
to say based on what you've heard so far yeah thank you very much sometimes people ask me

01:04:13.480 --> 01:04:20.280
because I've been in this kind of existential risk and AI safety community and effort for more

01:04:20.280 --> 01:04:25.880
than a decade now sometimes people ask like so how's it going and my standard answer is well it's

01:04:26.680 --> 01:04:35.880
great progress against an unknown deadline and indeed it's kind of special this year it's just

01:04:35.880 --> 01:04:42.280
like there's like a plethora of things to point to us as great progress and obviously the most

01:04:42.280 --> 01:04:48.520
obvious one to point to at this point is the summit that starts tomorrow I do think it's

01:04:49.480 --> 01:04:55.160
UK deserves a great credit for for pulling this together and I really wish

01:04:56.040 --> 01:05:01.880
kind of best of luck to the organizers of this and the prime minister as well and the team

01:05:03.720 --> 01:05:13.560
now when it comes to this like unknown deadline recently I've kind of pivoted away

01:05:14.360 --> 01:05:23.880
to some degree from basically funding research towards just buying us more time which is kind

01:05:23.880 --> 01:05:31.080
of has to be has to deal with something like less research side and more kind of an action side more

01:05:31.080 --> 01:05:38.680
on the policy side so I do think it's kind of valuable now to really think through the policy

01:05:38.680 --> 01:05:46.760
that would make the future a little bit less sound and the deadlines a little bit less

01:05:46.760 --> 01:05:51.880
unknown another and final thing I wanted to say that there is I want to kind of caution against

01:05:54.040 --> 01:05:58.200
if you're sailing to like uncharted waters there's like a temptation to

01:05:59.400 --> 01:06:05.400
use something familiar and say that oh like the future is going to be just like this

01:06:05.480 --> 01:06:09.560
like the most common one is that oh AI is just the technology it's just going to be

01:06:10.200 --> 01:06:13.080
just another like electricity or something like that

01:06:15.800 --> 01:06:24.280
when we're talking about risks the the way the model risks is by the you know reference class

01:06:24.280 --> 01:06:31.080
that you cannot rule out so as long as there is like reference classes like viruses self-replicating

01:06:31.080 --> 01:06:36.360
things or another species as long as you kind of rule out rule them out you have to like prepare

01:06:36.360 --> 01:06:44.040
that this might be an instance of such thing so I think it's important to not make dismiss

01:06:44.040 --> 01:06:48.920
AI it's always just another technology or like as one prominent VC recently said oh it's just

01:06:48.920 --> 01:06:55.880
much of math thanks Jan next we're going to hear from Max Teckmark he might describe what he's got

01:06:55.880 --> 01:07:02.280
on his chest I happen to know he has released a very interesting TED talk which I strongly

01:07:02.280 --> 01:07:07.960
recommend all of you watch and Max might give an abbreviated form of that TED talk now or whatever

01:07:07.960 --> 01:07:13.240
else you'd like to put in the conversation thank you thank you yeah so I'm Max Teckmark I've been

01:07:13.240 --> 01:07:21.880
doing AI research at MIT as a professor there for many years focusing on safety related stuff I'm

01:07:21.880 --> 01:07:30.920
also the president of the future of life institute and I'm a huge fan of this guy who you guys have

01:07:30.920 --> 01:07:36.760
the wisdom to put on your 50 quid note Alan Turi who's come up many times and it's really remarkable

01:07:36.760 --> 01:07:43.720
that the argument he made 72 years ago that when machines greatly outsmart us by default they're

01:07:43.720 --> 01:07:48.520
going to take control that that argument has not been convincingly refuted in the 72 years

01:07:49.320 --> 01:07:53.880
since he said it so I think we have to take it very seriously and people who think of AI is

01:07:53.880 --> 01:07:59.320
just a new technology like steam engines or electricity tend to not take it so seriously

01:08:00.200 --> 01:08:06.520
Alan Turi himself clearly thought about AI more as a new species and with that framing it's very

01:08:06.520 --> 01:08:13.000
natural that we would lose control to them just like the Neanderthals lost control to us etc

01:08:13.000 --> 01:08:19.880
so so what are we going to do about about this great threat first of all having conversations

01:08:19.880 --> 01:08:25.160
like here and what happens tomorrow is great so a huge thank you to the British government for

01:08:25.160 --> 01:08:30.680
really putting this on and for standing up to all the lobbying pressure from companies who wanted to

01:08:31.480 --> 01:08:38.920
water it down into just talking into just a big blessing of responsible scaling or whatever

01:08:38.920 --> 01:08:45.880
thanks also to the US government for standing up to also the weird pressures to turn this into a

01:08:45.880 --> 01:08:51.800
geopolitical pissing contest by excluding China I'm really proud of the Brits for recognizing

01:08:51.800 --> 01:09:01.000
that this is a global challenge and what do we actually do about it well I think there's a remarkable

01:09:01.000 --> 01:09:07.160
consensus actually emerging from all the civil society and academic groups that don't directly

01:09:07.160 --> 01:09:13.400
profit the way companies do about what we should do about this we put out maybe

01:09:14.680 --> 01:09:19.080
Andrew creature Richard Muller can just hold up in the air with this thing you can if you go to

01:09:19.080 --> 01:09:28.280
future you'll find the alternative to the responsible scaling policy called called the

01:09:30.200 --> 01:09:35.800
safety standards policy where the idea is as we heard from Stuart Russell you should simply shift

01:09:35.800 --> 01:09:43.320
the responsibility to companies to prove that things are safe instead of as responsible scaling

01:09:43.320 --> 01:09:47.640
policy you you have the responsibility on the government regulators to prove that things are

01:09:47.640 --> 01:09:56.520
unsafe more or less in order to stop them and there's there's a whole set of very concrete ideas

01:09:56.520 --> 01:10:04.200
out there for what the safety standards should be to start with and some of them were mentioned

01:10:04.200 --> 01:10:10.200
very eloquently by Stuart you can insist on quantitative safety bounds or provable safety

01:10:10.200 --> 01:10:15.320
beginning with uncontroversial stuff that you should not be able to demonstrate that nobody

01:10:15.320 --> 01:10:23.080
can hack the servers that these super large systems are on that they won't advise on how

01:10:23.080 --> 01:10:27.880
to make bio weapons etc and this will very naturally accomplish something quite wonderful

01:10:27.880 --> 01:10:32.920
where we sort of have the cake and eat it as a species because most people I talked to don't

01:10:32.920 --> 01:10:39.320
realize that there are two almost there's two very different kinds of AI that they keep conflating

01:10:40.200 --> 01:10:46.040
there is the AI that has current commercial value for curing cancer making self or better

01:10:46.040 --> 01:10:52.280
safer cars and all sorts of wonderful things which have very little risk associated with them but

01:10:52.280 --> 01:10:58.600
some which we need to address but 99 percent of the things that most people are excited about

01:10:58.600 --> 01:11:03.960
do not require playing Russian roulette with AGI and super intelligence and then there is this

01:11:03.960 --> 01:11:12.760
lunatic frame just try to build the machines that outsmart humans in all ways where almost all

01:11:12.760 --> 01:11:18.440
the risk is coming for very little benefit so if we can put safety standards in place we can I think

01:11:18.440 --> 01:11:22.680
quickly get into a situation where we have a long future with these wonderful benefits

01:11:23.320 --> 01:11:29.720
that are quite safe to get from AI and then just take your time with with the really risky stuff

01:11:29.720 --> 01:11:34.600
maybe one day humanity will or will not want to build more powerful machines but only when we can

01:11:34.600 --> 01:11:38.920
figure out you know how to control them so that would end with just a bit of wisdom from ancient

01:11:38.920 --> 01:11:45.960
Greece if I may so raise your hand if you remember the story of Icarus don't get hubris right you know

01:11:46.040 --> 01:11:53.720
so artificial intelligence is giving humanity these incredible intellectual wings with which

01:11:53.720 --> 01:12:01.160
we can accomplish things beyond their wildest dreams if we stop obsessively trying to fly into

01:12:01.160 --> 01:12:08.520
the sun thank you so you're not saying pause AI you're saying let's keep using AI but you're

01:12:08.520 --> 01:12:16.280
saying pause the rush to AGI let's not pause AI in fact let's continue almost everything that

01:12:16.280 --> 01:12:23.240
people are excited about doing but pause this compulsive obsession about training ever more

01:12:23.240 --> 01:12:31.320
ginormous models that we just don't understand thanks Andrea Miotti is the head of policy and

01:12:31.320 --> 01:12:35.400
governance for AI at Conjecture are you in agreement with what you've heard or you have

01:12:35.480 --> 01:12:40.680
different things to emphasize absolutely I'm very much in agreement with both the speakers and many

01:12:40.680 --> 01:12:45.640
other members of the panel I think there are two big positives from the summit to highlight

01:12:47.000 --> 01:12:53.160
one is that we're also echoed by the panel one is it's role in building common knowledge

01:12:53.880 --> 01:12:59.400
making it clear and explicit at the highest levels of government that this is a big risk

01:12:59.400 --> 01:13:05.720
that this is a extension level threat that we face as a species and number two coordination

01:13:05.720 --> 01:13:12.280
not getting lost in a geopolitical pissing contest as Max has said or in other of these

01:13:12.280 --> 01:13:18.920
things and realizing this is a again a threat we all face together it's a global security problem

01:13:18.920 --> 01:13:23.400
it's not a national security problem or at least it's not only a national security problem and

01:13:24.120 --> 01:13:29.720
to solve these problems we need coordination even during the heights of the Cold War there were

01:13:29.720 --> 01:13:38.200
open lines between the US and the Union to deal closing the door on cooperation before it has

01:13:38.200 --> 01:13:46.840
been tried is a surefire way for all of us to lose and so I was very very pleased to see that the

01:13:47.560 --> 01:13:54.120
UK government the Prime Minister Rishi Sunak have already acknowledged the risks very explicitly

01:13:54.120 --> 01:14:00.920
in the Prime Minister's speech last week are setting up the summit are inviting a diverse

01:14:00.920 --> 01:14:08.760
group of countries to discuss this risk together the part where I think we can go

01:14:09.400 --> 01:14:15.160
further and we can do better is in the measures I share the concern of some of the other panelists

01:14:15.160 --> 01:14:22.600
on a focus of simply enabling the default to continue and the reality is that the default

01:14:23.400 --> 01:14:30.680
is bad the default is bad we by now all understand it is bad and we all understand we need something

01:14:30.680 --> 01:14:38.120
else even the companies racing towards its default admit that it's bad admit that it's a

01:14:38.120 --> 01:14:46.440
one in four one in ten unacceptably high chance for all of us to be wiped out and so the concrete

01:14:46.440 --> 01:14:52.120
measures they will need to take cannot look like continuing on the default path cannot look like

01:14:52.680 --> 01:15:00.680
systems are safe until proven dangerous by external auditors that are strapped for resources and they

01:15:00.680 --> 01:15:06.920
don't even have the the tools or the tests to do these tests they look like provably safe systems

01:15:06.920 --> 01:15:12.200
they look like burden of proof on developers developing systems that they admit could wipe

01:15:12.200 --> 01:15:17.640
everyone out to demonstrate ahead of time of running critical experiments that they are safe

01:15:18.200 --> 01:15:22.280
if they cannot do that that's fine they can just build something else or they can move to a different

01:15:22.280 --> 01:15:29.240
sector that's the standard we utilize in all high risk sectors there is no reason to not

01:15:29.240 --> 01:15:34.600
utilize it in a sector where the risks are the literal extinction of your mighty thank you

01:15:35.240 --> 01:15:41.400
and last but not least we have a trained economist alexandra musavi sadeh who is the

01:15:41.400 --> 01:15:47.000
CEO of evident what would you like to add to the conversation alexandra it's a hard it's a it's a

01:15:47.000 --> 01:15:55.080
great panel to follow it so it's um i think i have a different time horizon alexandra musavi sadeh

01:15:55.080 --> 01:16:02.920
i'm the founder and CEO of evident and we actually do a lot of measurement uh we specialize in

01:16:02.920 --> 01:16:10.440
benchmarking businesses on the option of ai so what i focus on is very near term so looking at

01:16:10.440 --> 01:16:17.480
the here and now and the race is on at that level as well so the race is on by all businesses in

01:16:17.480 --> 01:16:25.240
all sectors to take the capabilities that ai offers today and to implement it as fast as possible

01:16:25.240 --> 01:16:30.120
and really not thinking about any of the risks so thinking about growing market share um

01:16:30.680 --> 01:16:37.320
upping revenue cutting costs and all of that and continuing the um sort of digital transformation

01:16:37.320 --> 01:16:43.960
which is now more and more an ai transformation and so with that um we we are observing this ai

01:16:43.960 --> 01:16:49.240
race at a business level and one of the things that we see that some businesses that are highly

01:16:49.240 --> 01:16:55.880
regulated really think about um how they can implement the oversight and implementation of

01:16:55.880 --> 01:17:04.200
safe ai so while very impressed with what the uk government is doing and i think the right thing

01:17:04.200 --> 01:17:11.800
is to focus on the long term because that is where we um should have our eyes at um at the stage but

01:17:11.800 --> 01:17:17.000
there's also a near term risk and i think um if there was one thing i would suggest is that as

01:17:17.000 --> 01:17:21.960
much as we need to focus on the long term we also need to look at the here and now and that businesses

01:17:22.040 --> 01:17:28.200
are barreling ahead with ai adoption without any particular guard rails in that and so while we

01:17:28.200 --> 01:17:35.560
need to put the um the burden on the development of safe ai we could also maybe in the meantime put

01:17:35.560 --> 01:17:43.320
the burden on the businesses that are using ai to prove that they're doing it in a safe uh and

01:17:43.320 --> 01:17:48.600
constructive way thanks so you've heard from all the panelists i'm sure there's lots and lots of

01:17:48.600 --> 01:17:52.920
questions in your mind so i'm going to come to the audience and take maybe three or four questions

01:17:52.920 --> 01:17:58.120
and then let the panelists pick what they want and my question to be would be do you agree with

01:17:58.120 --> 01:18:03.160
this division between near term and far and far future some people say that the risks from

01:18:03.160 --> 01:18:08.840
existential risk should not be considered to be far long term they are potentially here and now

01:18:08.840 --> 01:18:14.840
but maybe you have a different way of framing it let's see some hands let's take uh one back over

01:18:14.840 --> 01:18:21.960
there in the far corner it's a bit some running around if you can say who you are if you want

01:18:21.960 --> 01:18:28.200
to remain anonymous that's fine too hi um i'm Matthew Kilcoin i looked into how the banking

01:18:28.200 --> 01:18:34.520
industry turns short term into long term by sort of senior management risk and associated penalties

01:18:34.520 --> 01:18:40.920
and clawbacks to force the change okay question on learning from the banking industry so let me

01:18:40.920 --> 01:18:49.400
give the microphone down here just a second thanks a lot everyone yolanda lancas from the

01:18:49.400 --> 01:18:55.160
future society what do we do about open source ai professor russell mentioned one idea which was

01:18:55.800 --> 01:19:00.520
kill switches i think this is in terms of policy approaches such an important question

01:19:00.520 --> 01:19:06.680
for us to all grapple and again um as yan was saying oh technology people assume that paradigms

01:19:06.680 --> 01:19:12.840
continue open source has been valuable for software but with ai we're seeing new risks

01:19:12.840 --> 01:19:25.720
and paradigms and how could maybe academia and others my name is oliver graves um my question is

01:19:25.720 --> 01:19:31.000
what do you think the biggest hurdles are towards getting the general public to recognize this as an

01:19:31.000 --> 01:19:35.640
existential risk and to take that risk seriously because it still seems to me like it's all well

01:19:35.720 --> 01:19:40.360
and good everyone here at the summit and in this room being aware of that risk but it doesn't seem

01:19:40.360 --> 01:19:46.280
to me like we're anywhere close to a level of majority of the general public grappling with it

01:19:46.280 --> 01:19:48.120
probably

01:19:52.280 --> 01:19:56.440
thank you so i'm i'm father pete vignaschi i'm i'm engaged in looking at how the catholic church

01:19:56.440 --> 01:20:01.880
can respond to existential risks so it's a slightly different question here just in the most general

01:20:01.880 --> 01:20:07.160
way what does it look like from your side of the table for religious groups to play their parts

01:20:07.160 --> 01:20:15.720
in achieving existential security thank you i'm oliver chamberlain i'm a student studying

01:20:17.000 --> 01:20:23.480
a master's in science in in ai uh one of my concerns is although like the regulation

01:20:23.480 --> 01:20:31.000
is going to involve limiting like supply chains making sure that gpu's aren't going off to places

01:20:31.000 --> 01:20:37.640
that we don't know about how do we stop um the advancement of algorithms which allows older

01:20:37.640 --> 01:20:46.680
systems to be more powerful so like alpha tensor um i wonder uh in my mind like the only way around

01:20:46.680 --> 01:20:54.520
something like this is a future which is like super draconian um how do we prevent gpu's that

01:20:54.520 --> 01:20:58.520
already accessible already out there from being used in ways which are way more powerful

01:21:12.760 --> 01:21:17.800
question on open source to what extent is it possible to control open source a question on

01:21:17.800 --> 01:21:23.240
what are the biggest hurdles changing their minds in the public or indeed one of the other big hurdles

01:21:25.080 --> 01:21:32.840
yeah question from the point of view of what might religious organizations contribute to

01:21:32.840 --> 01:21:39.720
this conversation and do we need to have super draconian surveillance and policing systems if

01:21:39.720 --> 01:21:45.880
we're going to stop these gpu's and algorithms they're potentially doing things that we didn't

01:21:45.880 --> 01:21:52.200
want them to so max hand up first yeah religious organizations i hope can

01:21:53.160 --> 01:21:59.560
remind us all of the importance of not play god and get hubris remember the moral angle

01:22:00.600 --> 01:22:05.080
i'm only going to comment on the timeline one even though i have opinions about all the others

01:22:05.080 --> 01:22:10.680
it's not so i don't talk too much the timeline one from alan turing's perspective when he said this

01:22:10.680 --> 01:22:17.320
he said that when it eventually basically passed the turing test he expected to go very fast

01:22:18.040 --> 01:22:23.800
so then it was a long-term risk now according to yosha bengio gpt4 passes a turing test so

01:22:23.800 --> 01:22:29.960
he would probably if he were still with us in the room predict short timeline it's quite remarkable

01:22:29.960 --> 01:22:34.840
what's happened on on the prediction market metaculous.com for those of you who are nerdy enough

01:22:35.640 --> 01:22:41.320
to go there where the timeline how many years we have left artificial general intelligence

01:22:41.400 --> 01:22:47.800
outsmarting us has plummeted from 20 years away to three years away just in the last 18 months

01:22:48.440 --> 01:22:54.520
as a direct result of of this recent tech progress and dario amode has openly said one of the tech

01:22:54.520 --> 01:22:59.640
ceo's here that he thinks you have two or three years left and others other tech ceos told me

01:22:59.640 --> 01:23:05.640
that individually so i think we just have to stop calling this artificial general intelligence

01:23:05.640 --> 01:23:11.640
risk long term or people are going to laugh at us and call us dinosaurs stuck in 2021

01:23:13.720 --> 01:23:21.800
andrea i'd like to answer the question about the public actually the pub seems to really understand

01:23:22.440 --> 01:23:28.280
i recently ran polling as part of a campaign i'm running called control ai and the british public

01:23:28.280 --> 01:23:34.600
is extremely concerned about disempowerment and extinction risk from ai they seem to be

01:23:34.680 --> 01:23:45.320
aware of it um a they seem to be aware of it um a whopping 60 percent a global ban on smart and

01:23:45.320 --> 01:23:52.200
human ai period with only i believe 14 percent against and like quite a few undersized um

01:23:53.320 --> 01:24:00.040
nearly i believe almost nearly 90 percent would be very very happy with a full ban on deepfakes

01:24:00.040 --> 01:24:08.040
right now people understand very very well that full impersonation revenge pornography and like use

01:24:08.040 --> 01:24:13.560
of their likeness against their will is not good is destabilizing is a threat that exists right

01:24:13.560 --> 01:24:19.960
now with systems over here right now and they don't want that um and similarly uh there is

01:24:19.960 --> 01:24:25.880
i believe 78 percent of the public it would want an international watchdog with real teeth more like

01:24:25.880 --> 01:24:34.200
an i a a and there are basically across the board like i was personally surprised as he all of the

01:24:34.200 --> 01:24:42.440
answers come up with such overwhelming uh support uh we might ask whether that mood is shallow that

01:24:42.440 --> 01:24:48.360
it might be adjusted we might ask whether that mood is shallow that it might be adjusted again

01:24:48.360 --> 01:24:54.840
in the future let's hear from anika and then from how and then from yana uh yeah i just wanted to

01:24:54.840 --> 01:25:00.360
say that we should stay away from predictions with regard to timelines with regard to sectors

01:25:00.360 --> 01:25:04.840
how they're going to be affected i think if we have learned one thing that it's really difficult

01:25:04.840 --> 01:25:10.520
to assess that but we do know that there will be dislocations there will be impacts and as they

01:25:10.520 --> 01:25:16.200
grow and as we see the more the public will be more informed and more aware i think it's really

01:25:16.200 --> 01:25:23.880
our role here as civil society academia religious leaders to increase that awareness and to also

01:25:23.880 --> 01:25:30.120
keep that conversation going um as we go when you drive a car you don't just look ahead right

01:25:30.120 --> 01:25:36.520
you also look in the rear view uh mirror you look in the side mirrors for signals of change but

01:25:36.520 --> 01:25:41.240
what i want to say when you look back that's something we are not doing enough we're trying

01:25:41.240 --> 01:25:46.520
to predict in the future but we should also look back and look at you know stuff we've put out

01:25:46.520 --> 01:25:51.720
there regulation we're putting out there and how it's actually being enforced um if it's effective

01:25:51.800 --> 01:25:58.600
if um it's yeah effective regulation we discussed about it um before and this will be key going

01:25:58.600 --> 01:26:05.240
forward how i'll take the question oh yeah on yeah i'll take the question on open source

01:26:05.240 --> 01:26:10.440
and draconianism because i think they're related i don't really see a good way of regulating and

01:26:10.440 --> 01:26:15.720
controlling open source code that is not deeply draconian that does not involve a massive expansion

01:26:15.720 --> 01:26:21.480
of surveillance uh if you need to know what code is running on what chips you have to have access

01:26:21.560 --> 01:26:26.920
to the computer in which those chips are running and for a sense of how well this is going to go

01:26:26.920 --> 01:26:33.480
look at america's attempts to put export controls on chinese ai development um it it works to an

01:26:33.480 --> 01:26:40.920
extent but it only works if you pick these very narrow bottlenecks and i guess in terms of existential

01:26:40.920 --> 01:26:46.840
risk it depends on how powerful sort of lower tier open source models end up being i don't really have

01:26:46.840 --> 01:26:51.480
a good answer to that question i just say one more thing on metaculous just as a little hint of

01:26:51.480 --> 01:26:56.200
not relying on it too much if you just if you remember the lk 99 superconductivity thing over

01:26:56.200 --> 01:27:03.400
the summer metaculous at one point was completely certain that that was real and uh the metaculous

01:27:03.400 --> 01:27:08.760
thought it was real for a while and and then it dived again so just you can't rely on metaculous

01:27:08.760 --> 01:27:16.120
we're gonna do some real time checking on this right so once that's going on yeah so i also

01:27:16.120 --> 01:27:21.160
wanted to say a few words about open source i think it's as i mentioned earlier i think it's

01:27:21.160 --> 01:27:27.560
important to just like not do this kind of categorical thinking that you have like some one particular

01:27:27.560 --> 01:27:32.600
you know look at that too that you put things in and then like you reason about this pocket

01:27:34.040 --> 01:27:39.800
my friend andrew gritch who is in the audience like he he observes that's like when people

01:27:40.520 --> 01:27:45.720
say that they're really gonna prove open source it's valuable to try to understand what they

01:27:45.720 --> 01:27:50.760
actually want what is what is the thing that they're trying to protect and quite often it's

01:27:50.760 --> 01:27:55.320
just like they don't want this like massive centralization and power in the hands of people

01:27:55.320 --> 01:28:00.840
that they don't trust now the question is like if you think about open source as like irreversible

01:28:00.840 --> 01:28:05.880
deployment of things that we potentially don't want to irreversible deploy other other ways

01:28:05.880 --> 01:28:12.360
to protect what the open source allocates want and for example there is like there's like in blockchain

01:28:12.360 --> 01:28:21.320
community there is quite a lot of advancement in cryptographic techniques techniques like

01:28:21.320 --> 01:28:26.520
zero noise proofs perhaps there are enough like ways how we can kind of eat our cake can keep it

01:28:27.240 --> 01:28:33.800
keep it too by instead of having nosy people literally looking around in a computer you just

01:28:33.800 --> 01:28:38.440
computer automatically producing things like zero noise proofs that you haven't been up to no good

01:28:38.440 --> 01:28:44.120
things like that so there's lots of possibilities to explore there are this andra i'm actually

01:28:44.120 --> 01:28:51.720
curious about what max and how we're talking about ai secrets did they know or did they not know

01:28:53.640 --> 01:28:58.360
peak enthusiasm they thought it was 60 chance which i would say interpreted that they were

01:28:58.360 --> 01:29:05.640
not saying for sure okay but prediction is very real time checking here prediction is hard canary

01:29:05.640 --> 01:29:10.360
signals are more important in my view let's agree the canary signals okay if i can just add

01:29:10.360 --> 01:29:14.760
something here we we ask if something is a near term or long term risk we have to remember

01:29:14.760 --> 01:29:19.080
we're not asking if we know for sure that we're going to get super intelligent soon

01:29:19.080 --> 01:29:24.360
if we think there's a three a 10 percent chance that something like this might happen in four years

01:29:24.360 --> 01:29:29.240
but then it's still in the risk is near term even though i hope as much as anyone that it actually

01:29:29.240 --> 01:29:35.560
won't happen for a long time so alessandra and then ron and then mark yeah i just wanted to

01:29:35.560 --> 01:29:40.760
respond to the question on the banks um if i understood it correctly is like was that maybe a

01:29:40.760 --> 01:29:48.360
model of regulation for ai is that what you meant okay i mean as we cover that sector very deeply

01:29:48.360 --> 01:29:54.840
it is it is an area where i mean and it's it's um it's a sector that is heavily regulated and because

01:29:54.840 --> 01:29:59.560
they heavily regulated the way that they are developing and deploying and implementing ai

01:29:59.560 --> 01:30:04.440
today is that even as they develop there's a lot of oversight in the models themselves and then they

01:30:04.440 --> 01:30:10.360
have they go through first and second and third lines of defense where there is oversight again

01:30:10.360 --> 01:30:16.040
and then they submit to the regulators and in a way i mean i can't believe i'm saying this but

01:30:16.040 --> 01:30:24.280
in a way the banks could be a model with which if you are to impose a oversight at a company level

01:30:25.240 --> 01:30:30.040
for the ai that they're using the banking model is not a bad one because the way that they

01:30:30.600 --> 01:30:36.280
assess the risks as they go from development to deployment into production and output so

01:30:37.320 --> 01:30:43.160
that could be a blueprint or something to to look at for businesses themselves to regulate

01:30:43.160 --> 01:30:47.160
themselves is that if that's where we end up so there might be something to learn but bearing

01:30:47.160 --> 01:30:52.360
in mind agi is different from everything that's ever been before ron what would you like to add

01:30:52.360 --> 01:30:57.400
and well i'd like to come back to the discussion about open source versus closed source and i'm

01:30:57.400 --> 01:31:03.720
a bit surprised that here at the table there is a strong belief in closed lots of companies

01:31:04.440 --> 01:31:11.880
that might contain lots of zero days we don't know of instead of trusting civil society

01:31:13.160 --> 01:31:19.560
and and on regulation we therefore not only need to regulate development we also need to

01:31:19.560 --> 01:31:27.720
regulate use and there and that's what the ai does for example sounds like none of the

01:31:27.720 --> 01:31:31.800
old traditional models are going to work sounds like we need something that has a variety of

01:31:31.800 --> 01:31:37.320
different approaches we have to transcend some of the previous systems and mark and then we'll

01:31:37.320 --> 01:31:43.000
come back to pick up a few more comments um yeah i just wanted to pick up on sort of a question of

01:31:43.000 --> 01:31:47.240
near term or existing harms that we see in sort of harms in future because i think what you saw

01:31:47.240 --> 01:31:50.920
yesterday with president biden coming out with an executive order in the united states

01:31:51.560 --> 01:31:56.920
as well as with uai act which has been a long time in the making you see sort of an attempt and i

01:31:56.920 --> 01:32:02.600
think a successful attempt by policy makers to tackle both ai and bias that you see in current

01:32:02.600 --> 01:32:09.560
day applications and some of the risks that are like that are agi related and i think that shows

01:32:09.560 --> 01:32:15.240
that it's perfectly possible to do both and i know a lot of people in this audience are potentially

01:32:15.240 --> 01:32:19.320
driven by existential risk i mean that's why we come to an existential risk observatory event

01:32:19.320 --> 01:32:24.360
panel um but i think there is a lot of alliances and bridges that can be built across that space

01:32:24.360 --> 01:32:30.120
and i think it's often not helpful to look at both of these things um maybe just on banking um i mean

01:32:30.120 --> 01:32:36.200
we saw we seen the 2008 financial crisis where i think lots of people were justifiably angry

01:32:36.200 --> 01:32:41.000
because ceo's got away with whatever they were wanting to do uh and governments build out the

01:32:41.000 --> 01:32:46.360
banks um there was briefly a lot of regulation that was then rolled back over the past few years

01:32:46.360 --> 01:32:51.320
and again we saw a few small banks collapse in california so i think we need to learn some

01:32:51.320 --> 01:32:56.920
lessons there around liability and making sure that as we build a liability regime for ai companies

01:32:57.960 --> 01:33:03.400
ceo's are also individually and criminally liable if they are indeed negligent or if there are

01:33:03.400 --> 01:33:07.320
sort of safety risks that they're they're ignoring so maybe just to add on those two points so half

01:33:07.400 --> 01:33:10.920
the panel have got their hands up wanting to speak well i'm going to ignore them briefly and

01:33:10.920 --> 01:33:14.760
give the microphone very quickly to three people in the audience but you have to be quick because

01:33:14.760 --> 01:33:20.920
we're out of time already thanks uh richard barker and ron introduced the analogy of the

01:33:20.920 --> 01:33:24.920
pharmaceutical industry which is not perfect by any means but i spent most of my career in it so

01:33:24.920 --> 01:33:30.920
i think there's still a few lessons from that right the first is you regulate not the underlying

01:33:30.920 --> 01:33:36.200
technology but the application of the technology so it turns out felidomide is the terrible thing

01:33:36.200 --> 01:33:41.720
to give to pregnant women but it actually cures people with multiple myeloma so you i can't imagine

01:33:41.720 --> 01:33:47.560
how we're going to actually deeply regulate the the internal workings it's it's how they're used

01:33:47.560 --> 01:33:55.160
and it may not be existential risk that is most uh relevant uh to actually harnessing public opinion

01:33:55.160 --> 01:34:00.680
it will be some of the things that's already happening that affect them personally uh they're

01:34:00.680 --> 01:34:08.440
not just experts but panels um come back to uh legislators and say this is what i saw and this

01:34:08.440 --> 01:34:21.960
is what i like and don't like um terry rabie former risk manager um guys you really need some pushback

01:34:22.920 --> 01:34:30.280
uh i was deeply appalled at stewart's example of the nuclear industry the regulation of the

01:34:30.280 --> 01:34:39.800
nuclear industry in the united states essentially is anti-human it's prevented um the gifts of

01:34:39.800 --> 01:34:48.440
energy that's not polluting by regulation we have another anti-human example of regulation in the

01:34:48.440 --> 01:34:56.200
u which is a regulation of biology destructive of the advantages that we could get from genetic

01:34:56.200 --> 01:35:03.320
regulation of biology destructive of the advantages that we could get from genetically modified

01:35:03.320 --> 01:35:11.320
organisms so look it won't do the needs to be a little bit more pushback to you guys so you get

01:35:11.320 --> 01:35:22.360
your story straight okay who's behind her here so earlier thank you don't show me there's key

01:35:22.360 --> 01:35:30.520
substances professor russer make a very important distinction about as a i to be safe for humans

01:35:31.160 --> 01:35:39.400
and a i uh safe for use as a tool the first one is a new type of intelligence the second one

01:35:39.400 --> 01:35:48.440
is a just used as a tool so that's where the regulation comes in context we can regulate

01:35:48.440 --> 01:35:56.280
a i as safe to use and we must have a control of our development so it doesn't become an

01:35:56.280 --> 01:36:03.080
existential risk my question to the panel is the following one is it possible or shouldn't be possible

01:36:03.160 --> 01:36:10.760
in order to avoid uh open sourcing problems to develop just one super intelligence program

01:36:10.760 --> 01:36:18.120
that will beat any uh small guys developments and in that way make us safer and the second

01:36:18.120 --> 01:36:25.880
question is what will follow this summit deliverable which thanks so plenty to talk about there

01:36:25.880 --> 01:36:30.600
learning from the pharmaceutical industry regulating apps not platforms we had terry

01:36:30.600 --> 01:36:35.320
pushing back quite hard saying goodness look at the mess of regulation in the nuclear industry

01:36:35.320 --> 01:36:43.320
and in gmo's we had tony asking about a unified approach with one research and development program

01:36:43.320 --> 01:36:50.360
and also what's going to happen next so 30 seconds each max's hand up again i have a good friend

01:36:50.360 --> 01:36:55.080
in the american nuclear industry who told me that what really killed it wasn't regulation but it was

01:36:55.080 --> 01:37:03.080
fukushima and then three mile island i for open source 20 seconds on that i actually think i i love

01:37:03.080 --> 01:37:08.280
open source almost as much as yon lakoon mit is the cradle kind of open source but obviously we

01:37:08.280 --> 01:37:14.200
don't open source plutonium in this geranium and similarly here we should get away from this childish

01:37:14.200 --> 01:37:17.800
debate about whether your open source nothing or everything and just ask where the line goes

01:37:18.680 --> 01:37:23.720
finally there's a technical solution i think to this which is not creepy but still works

01:37:23.720 --> 01:37:31.560
which steve omohandro and i wrote a paper about where where you actually have control of your own

01:37:31.560 --> 01:37:35.800
hardware chips you own it no the government doesn't see what you run but it's not going to run

01:37:35.800 --> 01:37:40.040
certain kinds of really creepy code because the hardware itself won't don't it's like a

01:37:40.040 --> 01:37:44.840
virus checker in reverse where if your code can't prove that it's not making bioweapons

01:37:44.840 --> 01:37:50.520
it just won't run so you can find out more about that proposal if you watch max's ted talk how

01:37:51.400 --> 01:37:55.160
all i was just gonna ask doesn't that make it a backdoor in kind of the same way that

01:37:55.160 --> 01:38:01.240
csam detection on i message it makes it a backdoor no it's completely decentralized no one has

01:38:01.240 --> 01:38:05.480
access to your chip it's just if you want to chip that'll run the harmful code you have to make your

01:38:05.480 --> 01:38:13.000
own chip we need standards for hardware which is what stew was saying who's who's wants to jump in next

01:38:13.000 --> 01:38:22.920
andrea just a quick reply to the ipcc model uh i very much hope that we will not have an international

01:38:22.920 --> 01:38:30.360
agency model after the ipcc here uh we are in crunch time and we have now knowledge of the risks

01:38:30.360 --> 01:38:36.120
and we have common knowledge about the risks uh the role of the pcc was a great organization to

01:38:36.120 --> 01:38:42.280
build over decades uh essentially expertise and information to governments to deliberate on how

01:38:42.280 --> 01:38:49.000
to act we do not have decades and we know what the problems are governments are already acknowledging

01:38:49.000 --> 01:38:56.280
what the problems are we need action not a yearly report alessandra i would i would agree with that

01:38:56.280 --> 01:39:00.680
i think we're at a point where and also i don't see how it's practically gonna gonna work i mean

01:39:01.320 --> 01:39:09.240
are the chinese in the u.s gonna open the kimono and submit to a uk body that wants it to um and

01:39:09.240 --> 01:39:13.400
no accountability and no repercussions if they don't so i really don't see it that how that would

01:39:13.400 --> 01:39:19.800
work but um there's so much to say and so much to respond to uh on this but i think uh gentlemen

01:39:19.800 --> 01:39:25.560
in the in the red jumper they'll very much agree with the the fact that at least until

01:39:26.360 --> 01:39:32.280
something has um taken place on the regulation and we've agreed to what that might look like for the

01:39:32.280 --> 01:39:39.560
near term big risks but for the here and now um make the companies make the sectors accountable

01:39:39.560 --> 01:39:45.800
for how they use it make the buck stop there first and then we can figure out or in parallel figure

01:39:45.800 --> 01:39:52.040
out how to regulate it's sort of the bigger bigger questions and the the things that are um giving us

01:39:52.040 --> 01:39:56.200
pause but i think the world will submit to a body that's run by the uk but the world might

01:39:56.200 --> 01:40:02.120
cooperate with a body that the uk helps to inspire and get off the board mark your hand was up

01:40:03.080 --> 01:40:07.560
yeah i also just wanted to pick on uh pick up on the question the gentleman in the red jumper

01:40:07.560 --> 01:40:11.880
raised on parallels with the pharmaceutical industry uh yeah you're just sort of shining

01:40:11.880 --> 01:40:17.880
beacon here in the audience um i mean i think on the one hand like the time where everyone

01:40:17.880 --> 01:40:22.200
just could produce whatever potion they wanted to and put it out on the market that has disappeared

01:40:22.200 --> 01:40:26.040
and i think thankfully disappeared and i think that there is lessons we can learn in that in

01:40:26.040 --> 01:40:30.760
terms of potentially licensing or making sure that you guarantee that something's safe when it comes to

01:40:31.320 --> 01:40:36.280
regulating the application of ai i think i'm significantly more skeptical uh this has been

01:40:36.840 --> 01:40:42.360
for example in the senate hearing this was what uh christine amon gomri from ibm was pushing quite

01:40:42.360 --> 01:40:46.920
heavily we see on both sides of the atlantic big tech really pushing for application based

01:40:46.920 --> 01:40:51.800
regulation because that often means that the underlying big systems that they are building

01:40:51.800 --> 01:40:57.800
won't be regulated right because how do you regulate regulate gpt4 if if you only regulate

01:40:57.800 --> 01:41:03.000
applications and it's only the hospital that then integrates it into a chatbot for patient contact

01:41:03.000 --> 01:41:07.880
contact that actually has to deal with the regulatory burden so i think you do need to force

01:41:07.880 --> 01:41:13.240
these big tech companies to do risk identification and mitigation even if they can't particularly

01:41:13.240 --> 01:41:18.040
specify oh it goes into that application or or this other one so i think you need a bit of a

01:41:18.040 --> 01:41:25.880
combination of both anik are you a fan of the ipc model ipcc or would you prefer the i c fg

01:41:25.880 --> 01:41:32.440
model i c fc or the i e uh ai ea i think there are a couple of models proposed i think what's

01:41:32.440 --> 01:41:38.680
important is to not um put it in one hand uh not in the hand of a few corporates but also not

01:41:38.680 --> 01:41:43.080
in the hands of one state so the current race is not healthy and we need to think about how

01:41:43.080 --> 01:41:49.880
we get them back to the table i know you you have some ideas about this and uh yeah we we as i cfg

01:41:49.880 --> 01:41:56.920
are trying to show and build the scenarios to explain what it means to look at a future where

01:41:56.920 --> 01:42:01.800
emerging tech is governed and what it means when you look at a future where emerging tech is not

01:42:01.800 --> 01:42:07.720
governed and that this will hopefully help decision makers come together and work together because

01:42:07.720 --> 01:42:12.040
there are many other emerging technologies that might disrupt our society in many other ways

01:42:12.040 --> 01:42:17.640
too just around the corner absolutely so we had this discussion shortly before the panel because

01:42:18.600 --> 01:42:24.840
yeah uh ai is of course a turbo charger for a number of technologies that are being developed

01:42:24.840 --> 01:42:31.480
at a fast pace so we are also looking at mirror technology and quantum and biotech and i mean a

01:42:31.480 --> 01:42:36.200
lot of here in the room are looking at different technologies but the power come from the combination

01:42:36.200 --> 01:42:45.480
of those and they also round the corner ron final remarks yes yes it works um first i agree that we

01:42:45.480 --> 01:42:50.440
should both look into both the models and the applications not one of them but um what i think

01:42:50.440 --> 01:42:56.120
is that we yes we need science but we do not have decades um so we need some sort of form of a

01:42:56.120 --> 01:43:01.320
rapid response mechanism and in that we need a credible helix we need both government civil

01:43:01.320 --> 01:43:10.280
society we need science um and and we need all of them at the table thanks closing words yana

01:43:14.120 --> 01:43:19.480
okay one thing i would say about the regulation issue i think uh friends we most of it's like

01:43:19.480 --> 01:43:24.760
he has this concept of dial-up progress uh that a lot of conversations end up in like

01:43:24.760 --> 01:43:29.800
do we need more progress or less progress uh which is kind of like way too black and white

01:43:30.600 --> 01:43:34.920
way of looking at things what you actually want to do is like look look like there are different

01:43:34.920 --> 01:43:39.800
ways uh where we want more progress and different places where we want want less progress so it's

01:43:39.800 --> 01:43:44.440
actually completely consistent to believe as i believe that yes we have over like over-regulated

01:43:44.440 --> 01:43:51.480
a lot of things in a way that is kind of detrimental uh for us but that doesn't mean that we really

01:43:51.480 --> 01:43:56.520
should stop regulating things and new things as they come up thanks so please stay on the

01:43:56.520 --> 01:44:02.760
stage for a moment we're going to have a few closing words from Otto who is the head of

01:44:02.760 --> 01:44:09.880
ero which is part of the organization that has made this happen Otto are you here

01:44:10.840 --> 01:44:17.800
yes and by the way this discussion is a prelude to an even more important discussion which is

01:44:17.800 --> 01:44:21.880
going to be taking place in the pub in the good old british tradition afterwards some of you might

01:44:21.880 --> 01:44:27.320
want to join us where we can get around to all of you who had hand ups and i unfortunately couldn't

01:44:27.320 --> 01:44:42.920
take your question Otto thank you David sorry um yeah thanks and thank you all of so so much

01:44:42.920 --> 01:44:49.960
for being present here today um as some of us has already mentioned we're here in wilton hall

01:44:49.960 --> 01:44:55.320
this was built in 1943 as an assembly hall for the world for two codebreakers and while deciphering

01:44:55.320 --> 01:45:01.400
they have progressed beyond imagination and borrow multiple exponential curves here hardware

01:45:01.400 --> 01:45:06.840
data quantity algorithm capabilities are all growing with tens of percentage points per year

01:45:07.720 --> 01:45:14.760
so i think we all or at least a lot in this room will suspect where this leads uh which is a i that

01:45:14.760 --> 01:45:20.440
has the capability to do mental tasks much better than we can and of course this presents amazing

01:45:20.440 --> 01:45:26.200
opportunities but according to most existential risk experts we also risk nothing short of human

01:45:26.200 --> 01:45:33.240
extinction here and it does mean that our species is on the line so when i turned on the radio last

01:45:33.240 --> 01:45:39.000
weekend the bbc was discussing human extinction by ai and i think that this was dramatic but also

01:45:39.000 --> 01:45:45.400
hopeful at the same time so i thought this was dramatic since human extinction caused by our

01:45:45.400 --> 01:45:52.040
own actions is now officially a possibility and it never ceases to amaze me that we have been stupid

01:45:52.040 --> 01:45:58.440
enough to let it get this far but hearing this discussed on national radio for me was also extremely

01:45:58.440 --> 01:46:05.160
hopeful because up until now attempts to reduce the real human extinction risks were minor and

01:46:05.160 --> 01:46:10.680
world leaders were not paying attention and with the summits that's starting tomorrow i think this

01:46:10.680 --> 01:46:17.160
is really changing so i think it's hopeful that after the uk's prime minister speech on ai last

01:46:17.160 --> 01:46:22.040
week in which he explicitly warned of human extinction risks the questions that followed

01:46:22.040 --> 01:46:28.360
from the press were no longer about prime minister is this a real concern shouldn't you be concerned

01:46:28.440 --> 01:46:36.280
about the bills of the of your people instead of this but instead at least some of the questions

01:46:36.280 --> 01:46:41.480
were about are you addressing this problem seriously enough and shouldn't we consider instead

01:46:41.480 --> 01:46:47.880
posing ai a moratorium or are you doing the right thing with backing responsible scaling

01:46:49.000 --> 01:46:54.040
and i think this is exactly the debate that we need so i think it's now important that we

01:46:54.040 --> 01:47:00.280
continue in this direction so we must organize ai's safety summits much more often we must open

01:47:00.280 --> 01:47:06.200
them up so everybody gets to say we must have societal debates about this and i think in general

01:47:06.200 --> 01:47:11.960
we must come together to coordinate and if we do that we are confident at the extension risk

01:47:11.960 --> 01:47:16.840
observatory that we can implement the measures that are needed and this is why we have organized

01:47:16.840 --> 01:47:22.760
this event and i think it's a huge privilege that we're able to do this together with conjecture

01:47:22.760 --> 01:47:29.000
so thank you so much for co-organizing this event and we also want to continue organizing events

01:47:29.000 --> 01:47:34.680
like this one but it's impossible without the support of all of you so if you want to support us

01:47:34.680 --> 01:47:41.000
doing this there was a flyer that you got handed at the beginning please scan the QR code there

01:47:41.000 --> 01:47:46.520
and there's possibilities to support us could be with funding could be with volunteering

01:47:46.520 --> 01:47:50.840
could be with just following and sharing our content so this is enormously appreciated

01:47:51.800 --> 01:47:56.920
and with that can i please get some applause for all our amazing speakers professor Stuart Russell

01:47:59.400 --> 01:48:02.600
Connolly Professor Max Teckmark

01:48:04.120 --> 01:48:07.880
Jan Tellin Annika Brack Mark Brackle Ron Rosendell

01:48:07.880 --> 01:48:11.640
Alexandra Moussevisidae Andrea Mariotti and Helhotzen please don't stop clapping

01:48:11.640 --> 01:48:25.800
um and finally a special thanks as well to David Wood a moderator Ruben Dileman, Katrina Joslyn,

01:48:25.800 --> 01:48:31.240
Connor Axiotis, Sue Chisholm, Tillman Schepke, Niky Drogdowski, Mark van der Waal and Joep Soeren

01:48:31.240 --> 01:48:34.760
and everyone here at Wilton Hall who made this all possible thanks a lot for helping us out

01:48:41.640 --> 01:48:48.680
and please join us for drinks at Three Trees which is about 10 to 15 minutes

01:48:48.680 --> 01:48:51.160
more from here so i hope to see you all there thank you

01:48:55.080 --> 01:48:59.160
and some people believe in the future there's going to be a wedding in here shortly

01:48:59.160 --> 01:49:03.320
so we all need to get out unfortunately unless we're part of that wedding crowd

01:49:03.320 --> 01:49:14.280
so biome is chat but chat whilst moving out thank you

