Welkom, everybody.
Great to have you all here in a fully packed packhuis de Zwijger.
Mijn naam is Maarten Gehem.
I'm director of the argumentation factory and I have the honour of hosting this evening on existential risks.
De late Stefan Hawken once said success in creating AI could be the biggest event in the history of our civilization.
But it could also be the last unless we learn to avoid the risks.
And that's precisely the topic of today.
And we're going to talk about that with none other than professor Stuart Russell.
I'll properly introduce him later, but first I'll hand over the floor to Otto Barton,
who is the director of the existential risk observatory,
and who is the instigator of this evening.
Otto, come over.
After Otto, Russell will give a talk, then we'll have a Q&A,
and then afterwards we'll have a panel with five distinguished panelists.
We're sitting over there, I'll also introduce you later.
But now, without further ado, Otto.
APPLAUS
Dank je wel, en thank you very much, Maarten, for the introduction.
Ja, I'm super happy that you're all here.
Indeed, my name is Otto Barton.
I'm the founder and the director of the existential risk observatory.
We're an organization aimed at reducing existential risk by informing the public debates.
I'm going to talk a little bit for a few minutes about what existential risk is
and what our organization is doing.
All right, so next slide, please.
Thank you.
So existential risks, what are they?
Basically, as humanity, we've had about 300,000 years now already on this earth,
and we have maybe about five billion to go,
so an enormous amount before the sun explodes.
So the huge majority of our time is still ahead of us,
and an existential risk is something that can threaten that.
So basically, the definition is a risk that threatens the destruction of humanity's long-term potential.
It has been defined by Toby Ork from the Future of Humanity Institutes
and his colleagues in this way.
So this could be in a few ways.
Of course, human extinction is a permanent state.
So human extinction is one way in which we cannot have a future left anymore.
So these five billion years, there won't be any value in that.
An unrecoverable collapse or dystopian log-in are two other ways
in which we could, which are contained in existential risk definition.
So on the graph to the right, you see a rough estimate by Toby Ork,
this researcher from the Future of Humanity Institute,
on what causes could be for existential risks.
So there are natural causes.
There might be an asteroid strike, there might be a super volcano,
but these are tiny and very well-known.
So not the most interesting ones.
To the left of that, you see a nuclear war and climate change,
which are already somewhat bigger.
But you can see that these are still fairly small compared to other existential risks.
That climate change has a small chance of leading to human extinction.
It doesn't mean that it's not a big problem.
Of course, the chance that climate change will occur is 100% basically.
And it is a very big issue.
However, the chance that it leads to complete human extinction is relatively small,
which is why you see a small bar here.
Nuclear war, perhaps a little bit of a similar story,
the chance that it occurs in the next hundred years is not that tiny.
But the chance that it leads to human extinction is fairly small.
To the left, you see even bigger chances of human extinction
or the other existential risk categories.
These are, for example, the man-made pandemics.
The pandemics bar here is actually for man-made pandemics.
A natural pandemic is also very unlikely to lead to human extinction.
But a man-made pandemic with the biotechnology that we have developed right now
and that we are still developing and democratizing.
The chance that this could lead to human extinction in the next hundred years is non-negligible.
To be orged and most of the other existential risk researchers
think that it's unaligned AI, so artificial intelligence
that has human level or even beyond human level, superhuman level.
But it's unaligned, so it has different values than ours.
This could be a relatively large chance of human extinction.
We're going to talk more about it later, but I'll just leave it here for now.
What else do we see, the total existential risk in the next hundred years
is about a one in six estimate.
There's a lot to be said about these estimates,
but you can still draw a couple of robust conclusions, I think, from them.
That a very likely source is new technology.
And also that technology is man-made, so risk could be reduced in principle.
Next slide, please.
Solution directions for AI existential risk.
These kind of also carry over for other technologies,
but very broadly you could say
if you don't want something to go very wrong with technology,
you can either develop it safely or you cannot develop it.
So basically for AI, this is built AGI safely or AI safety.
So this is done by people who try to focus on AI alignment,
trying to make AGI align to our values.
We think, as an existential risk observatory,
that's an important line of research and it should be scaled up.
But on the other hand, it hasn't worked so far.
People are already working on this for perhaps a few decades.
And so far the consensus is that AI alignment,
more or less the consensus is that AI alignment hasn't been successful yet.
So another option could be to not build AGI
and we think there might be some kind of regulation necessary for that.
So this could be a software regulation, a data regulation
or perhaps a hardware regulation.
And we think these are all options that should be investigated.
But we do think that regulation, whatever is the form it takes,
will require widespread awareness and global cooperation.
So for that, our solution is to inform the societal debate.
So as an existential risk observatory,
a small non-profit organization based in Amsterdam,
we are focusing on informing the society about existential risk.
So we do that by publishing articles in traditional media,
for example in Time Magazine a few weeks ago
and by organizing events such as this debate.
And we also provide input to policy makers
and I think it's a really nice sign that emotion was accepted
by Dutch parliament a few weeks ago
that is calling for more AI safety research in the Netherlands.
So with that, I'm just going to end this small introduction talk
and we're now going to watch a documentary
which is already giving you a little bit of a flavor
of the next speaker Stuart Russell.
And I hope that you enjoyed a few minutes of documentary
and I wish you a great rest of the evening.
Thank you very much.
Everything we have is a result of our intelligence.
It's not the result of our big scary teeth
or our large claws or our enormous muscles.
It's because we're actually relatively intelligent.
And among my generation, we're all having what we call holy cow
or holy something else moments
because we see that the technology is accelerating faster than we expected.
I remember sitting around the table there
with some of the best and the smartest minds in the world
and what really struck me was
maybe the human brain is not able to fully grasp
the complexity of the world that we're confronted with.
As it's currently constructed,
the road that AI is following heads off a cliff
and we need to change the direction that we're going
so that we don't take the human race off the cliff.
This is from the Deep Mind Reinforcement Learning System.
Basically wakes up like a newborn baby
and is shown the screen of an Atari video game
and then has to learn to play the video game.
It knows nothing about objects, about motion, about time.
It only knows that there's an image on the screen and there's a score.
So if your baby woke up the day it was born
and by late afternoon was playing 40 different Atari video games
at a superhuman level,
you would be terrified.
You would say my baby is possessed, send it back.
The Deep Mind System can win at any game.
It can already beat all the original Atari games.
It is superhuman.
It is superhuman.
It plays the games at super speed in less than a minute.
Deep Mind turned to another challenge
and the challenge was the game of Go
which people have generally argued
has been beyond the power of computers
to play with the best human Go players.
First they challenged the European Go Champion.
Then they challenged a Korean Go Champion.
En they were able to win both times
in a kind of striking fashion.
You were reading articles in New York Times years ago
talking about how Go would take a hundred years
for us to solve.
People said, well, you know, but that's still just a board.
Poker is an art.
Poker involves reading people.
Poker involves lying, bluffing.
It's not an exact thing.
That will never be a computer.
You can't do that.
They took the best poker players in the world
en took seven days for the computer
to start demolishing the humans.
So it's the best poker player in the world.
It's the best Go player in the world.
And the pattern here is that AI might take a little while
to wrap its tentacles around a new skill.
But when it does, when it gets it, it is unstoppable.
MUZIEK
DeepMind's AI has administrator-level access
to Google's servers
to optimize energy usage at the data centers.
However, this could be an unintentional trojan horse.
DeepMind has to have complete control of the data centers.
So with a little software update,
that AI could take complete control of the whole Google system,
which means they can do anything.
They can look at all your data and do anything.
MUZIEK
We were rapidly headed towards digital superintelligence
that far exceeds any human.
I think it's very obvious.
The problem is we're not going to suddenly hit human-level intelligence
and say, OK, let's stop research.
It's going to go beyond human-level intelligence
into what's called superintelligence
and that's anything smarter than us.
AI at the superhuman level, if we succeed with that,
is by far the most powerful invention we've ever made
and the last invention we ever have to make.
And if we create AI that's smarter than us,
we have to be open to the possibility
that we might actually lose control of them.
MUZIEK
Let's say you give it some objective like curing cancer
and then you discover that the way it chooses to go about that
is actually in conflict with a lot of other things you care about.
AI doesn't have to be able to destroy humanity.
If AI has a goal and humanity just happens to be in the way,
it will destroy humanity as a matter of course.
Without even thinking about it, no hard feelings.
It's just like if we're building a road
and an ant hill happens to be in the way,
we don't hate ants, we're just building a road
and so goodbye, Ant Hill.
MUZIEK
OK, if you weren't scared already.
Make sure humanity doesn't run off a cliff.
Stuart Russell said.
So who better to tell us how not to run off a cliff
than Professor Russell himself.
And that's precisely what we're going to hear.
Professor Russell is one of the leading experts
in AI research and AI safety research.
He's based at the University of California, Berkeley.
He's one of the writers, co-author of the standard textbook in AI research.
AI, a modern approach.
And recently he wrote a magnificent book called Human Alignment.
I don't know who read the book already.
Let me see some hands here.
OK, all right.
Well, it's well worth the effort.
And he'll probably tell you why in the next 20 minutes.
Professor Russell is beamed to us all over the world,
from all across the world, from California where he's based right now.
So we're going to see him on the screen in a minute or two.
And afterwards there's ample room for questions and answers.
So we'll have some room here for a discussion with Mr. Russell himself.
And there he is.
Professor Russell, the floor is yours.
Hey there, thank you very much.
So I should just make a slight correction.
I'm not in California, I'm actually at MIT.
But I'm on my way home to California later on this evening.
So I think the little movie that you just saw
actually brings up a lot of important points.
So I don't have to repeat them.
But I will give you a short presentation,
which in some ways brings it up to date.
So let's say a little bit about what we're doing to help
and about the current situation.
So together everyone on the same page.
What is AI?
It's not a particular technology.
It's a task just like the task of physics is to understand the universe.
The task of AI is to make intelligent machines.
And then the question is, well, what does that mean?
What does it mean for a machine to be intelligent?
And for most of the history of AI, it's meant the following.
Machines are intelligent to the extent that their actions
can be expected to achieve their objectives.
And this is so pervasive, I'll call it the standard model.
And many forms of AI, problem solving, planning, reinforcement learning,
or conform to the standard model,
as well as many other disciplines like control theory
and operations research and economics.
You create optimizing machinery and then you specify some objective.
You put that into the machinery and then it becomes the objective of the machine.
And then it finds ways to fulfill that objective.
It's a very natural way to go about doing things.
Later on, I'll argue that it's completely wrong.
But for now, take that as the standard model of AI.
And since the beginning, we've been looking at what we might call general purpose AI.
So not just an AI designed to achieve some specific objective,
but actually one that's capable of achieving more or less any objective that we might give it.
And learning to do that very quickly at a level that exceeds human capabilities
eventually in every dimension.
So that's the goal.
And rather than be accused of always talking about doom,
I'll begin by talking about the upside.
And it's really the upside that explains why the field exists in the first place
and why people are investing lots of money in it
and why lots of smart people are working on it.
Because the potential upside is really enormous.
For example, if you had general purpose AI,
then you could do by definition what humans already know how to do,
which is to deliver, among other things,
to deliver a good standard of living to maybe hundreds of millions
or maybe close to a billion people on Earth
have what we might call a good standard of living.
En we could deliver it actually on much greater scale at much lower cost
because the cost involved in delivering a standard of living
is the expensive time of other human beings.
So if we have general purpose AI, we could, for example,
use it to give everyone on Earth that same respectable standard of living
that we might see in some developed countries.
En if you calculate the sort of economic value of that,
it's about a tenfold increase in GDP
and that converts to what economists call the net present value.
So that's sort of what's the cash equivalent
of having that increased income stream.
So it comes to about $13.5 quadrillion.
So that's a lower bound, a low ball estimate
on the cash value of general purpose AI as a technology.
We could of course have many more things besides that.
I think we could have much better,
more individualized ongoing healthcare.
We could have very personalized
and very, very effective education for every child on Earth.
We could speed up the rate of scientific progress
and perhaps many other things.
I used to have advances in politics on that slide,
but I took it off for obvious reasons.
So now the question is, well, where are we?
A lot of people seem to be saying that we're already there,
that we've already created general purpose AI.
And I think this is not true.
I think there's something going on,
but we are still far away from general purpose AI.
And what's going on, of course, is large language models.
The chat GPT, GPT4, BARD, Lambda, Palm,
all these models are displaying very intriguing
and in some cases very impressive behaviors.
And I think they are probably a piece of the puzzle
of general purpose AI,
but they are not by themselves general purpose AI.
And at the moment, I would say,
we don't know what shape this puzzle piece is
and we don't know how to fit it into the puzzle.
We're not really sure what the other pieces are.
I think one of the things we're learning now
is that the pieces of this puzzle
are probably not the pieces that we thought
made up the puzzle maybe 15 or 20 years ago.
So just to illustrate a few reasons
why I don't think these systems are the solution,
they're not general purpose AI.
So here's a simple example from chat GPT
to me by my friend Prasad Tattapalli.
So the first question,
which is bigger an elephant or a cat,
and it answers an elephant is bigger than a cat.
So far so good.
Which is not bigger than the other, an elephant or a cat.
And it says neither an elephant nor a cat
is bigger than the other.
So these are two consecutive sentences
that he asked it.
And it seems clear from this that in a real sense
chat GPT doesn't know facts.
So when you ask a human a question,
at least our impression of what happens
is that we refer to an internal world model
that is self consistent,
that's composed of facts that we understand about the world.
And then we ask in a question
relative to that internal world model,
we find out what the answer is
and we express the answer in natural language.
In natural language as the answer to the question.
But that clearly can't be what's going on
in at least in this example
because you could not have an internal world model
that contradicted itself
in which the elephants are both bigger than cats
and not bigger than cats.
So in a real sense,
I think we could say that there's evidence
that these systems do not know things
in the way that word is usually used.
I also want to point out,
in the movie you just saw that several years ago
we defeated the best human go players.
In fact, when that happened
to the Chinese world champion in 2017
that was called China's Sputnik moment.
That event precipitated a total change
in Chinese government policy around AI
and the commitment of hundreds of billions of dollars
worth of investment.
The commitments to train hundreds of thousands
of AI researchers, et cetera, et cetera.
We decided to see how good the go programs really are.
We played one of our team members,
Kellan Pelrin, is a reasonably good amateur go player.
His rating is about 2,300.
On that scale, the human world champion is about 3,800.
The go programs are far ahead of human beings now.
In 2017, or 2016,
they were about the level of the human world champion,
so around 3,800.
Now they've reached around 5,200.
JBX Kata 005 is the name of the current number one
go playing program in the world.
Its rating is 1,400 points higher than any human player.
Kellan had been playing against this program
and had beaten it 14 times in a row
and then decided to give it a nine stone handicap.
That means that black, the computer,
starts with nine stones on the board,
as we're showing here, which is an enormous advantage.
This is the kind of handicap that you give to a small child
who's learning the game if you're a go teacher,
just so that the child feels they have a chance.
Now I'll show you what happens in the game.
Remember, the computer is black.
Kellan, the human, is white.
It doesn't really matter if you don't understand go.
Basically, you're trying to surround territory with your pieces
and to surround your opponent's pieces and capture them.
Notice what's happening in the bottom right corner of the board.
The white is making a little group.
It sort of has a kind of a figure 80 sort of shape.
And then black immediately starts to surround that group
in order to prevent it from capturing more territory.
And now white starts to surround the black group
so that this larger white circle is forming.
So it's kind of a circular sandwich.
There's a white piece in the middle
and there's a white thing around the outside
and it's sandwiching in that black group.
In the end, there's no attention
and then loses all of those pieces.
So what's going on here seems to be
that these super human go programs
actually have not correctly learned
what it means to be a group of stones,
what it means to be alive or dead
which are the most basic concepts in go.
And that allows Kellan, the human to defeat these programs
met alle voordelingsprogramma's, die worden geschreven door verschillende mensen
met verschillende trainingsregime's en verschillende netwerkstructuren en zo en zo.
Ze voelen allemaal in dezelfde manier, die is echt remarkably.
En ik denk dat het eigenlijk een consequentie is van de fact dat ze proberen te trainen
circuitten om concepten te representeren zoals connectiviteit en surroundering,
die eigenlijk niet mogelijk kunnen representeren correct via circuitten.
Je kunt alleen een soort van patchy, fragmentair, finite approximatie
te die concepten, maar met een generele programma, zoals Python,
het is heel makkelijk om die concepten correct te representeren.
Dus dit is een fundamentele limitering,
alstublieft als we het uitzenden, met diep leren als een manier te leren over de wereld.
Oké, dus ik denk, in mijn gevoel, dat we nog steeds een manier te gaan
voor generalpurpos A.I. en ik heb een aantal van de dingen
ik denk dat je hier gevoel hebt, waarschijnlijk de derde,
onze behoorlijkheid om niet alleen te bekijken, waarin de go-programma's
wel kunnen doen, zelfs als ze missen
over de kwaliteit van de positie die ze reachen,
zijn we zeker kunnen planen voor 50 of 60 of even 100 behoorlijkheid
naar de toekomst, maar mensen planen op veel levens van de extractie.
We planen over tijdscalingen van jaren en ook over tijdscalingen
van milliseconden en op elke tijdscaling in de tweede.
En als je op een phd maakt, bijvoorbeeld, dat gaat over een trillion
motocontrole acties, en niet alleen maar 50 motocontrole acties,
dus we kunnen in het universen, het heel complex universen,
door onze behoorlijkheid om op deze verschillende levens van de extractie te opereren.
En dat is iets dat nog wel duidelijk onderzoek in A.I.
Dus ik denk dat het nog wel gelijkgemaakt is,
omdat de manier van momentum, investering,
van geniën die dit gebouw wordt geplaatst, dat deze voorkomsten ergens gebeuren.
Het is gewoon heel hard te predicteren
wanneer ze gaan gebeuren.
En om een voorbeeld te geven van hoe hard het is om te predicteren
wanneer deze dingen gaan gebeuren, kunnen we in historie kijken
tot de laatste keer dat we een civilisatie-endende technologie ontvangen,
waarom het automatische energie was.
En we weten sinds 1905, en speciale relativiteit,
dat er een enorm amount van energie in atomen loopt.
En als je er tussen verschillende typen van atomen kan veranderen,
kan je dat energie veranderen.
Maar de fysieke ontdekking hier,
persoonlijke door Lord Rutherford,
de ledende nucleofysicist, geloofde dat dat mogelijk was.
Hij was gevraagd op een meet in september 11, 1933,
dacht je dat in 25 of 30 jaar tijd,
we zouden kunnen vinden een manier om deze energie te ontvangen.
En hij zei dat iemand die voor een verhaal van poder
in de transformatie van de atomen, een moeenscheid is.
En de volgende morning, Leo Zillard,
die een Hungarian fysiast was,
die van Hungary had afgesloten,
en was in London in de tijd,
reed dit in de newspaper,
en hij ging voor een reis,
en hij inventeerde de neutron-induust
nuclea-chain reactie,
die de solution is
om hoe je de energie van het atom op te leveren.
Dus hij ging van het mogelijkheden
tot de essentieel gevolgd in 16 uur.
Dus als ik zei dat het onpredictabel is,
het is onpredictabel
wanneer deze avond gaat gebeuren.
Ik denk dat er omdat er nogal wat we nodig zijn,
het is ongelijkbaar dat er allemaal in een goede gebeuren zijn,
dus we mogen een paar eerlijke oorlogen krijgen.
Dus over een eerlijke oorlogen,
dit is de titel van een paper
geschreven door een dozen
erg besteldere researchers in Microsoft.
Er zijn twee bezoekers van de Nationaal Kool,
de Nationaal Academies hier,
en andere bezoekers
die een heel signifieke contributie zijn
aan de theorie van machine-learning.
En ze hebben met GPT-4
de laatste systeem uit OpenAI gesproken.
Ze hebben het voor een aantal maanden
voordat het gebouwd was.
En ze hadden veel bezoekers
om het te bekijken hoe goed het was.
En hun conclusie,
zoals dit titel is,
is dat ze geloven dat GPT-4
overspraken van artificieel
en generale intelligenteel zijn.
Dus ze zeggen alstublieft
dat er een reale probleem
tegen AGI
gebeurt met dit systeem.
Oké, dus,
dus terug naar de vraag
over wat we vergelijden,
dit is Alan Turing,
die de founder van Computerscience is,
en in veel manier de founder van AI ook.
En in 1951,
hij was asked that question at a lecture.
What if we succeed?
And this is what he said.
It seems parable that once the machine
thinking method had started,
it would not take long to outstrip
our feeble powers.
At some stage, therefore,
we should have to expect the machines
to take control.
So that's it.
So he offers no mitigation,
no solution, no apology.
You almost get a sense of resignation
about this, about this prediction.
So why is it?
Where is this prediction coming from?
This idea that as you make AI
better and better,
things could end up getting worse
and worse as a result.
And I think underlying his prediction is,
I'm going to put it in a more positive way,
rather than a prediction, a question.
How do we retain power
over entities more powerful than us?
Forever.
That's the question
that I think he's asking himself
and he's failing to find an answer to it.
And so that's his prediction.
So I've spent the last 10 years also
trying to figure out an answer to this
that isn't, we can't.
And to do that,
I've been trying to understand
where things go wrong.
And I think they go wrong
because of a phenomenon called misalignment.
And that was described a little bit in the movie.
I think Elon Musk talked about it
and I talked about it a little bit.
This idea that systems that are pursuing
an objective, as in the Standard Model,
if that objective is not
the full, complete, correct description
of what the human race wants the future to be like,
then you are setting up a mismatch,
a misalignment between what we want the future to be like
and the objective that the machine is pursuing.
And we can see that happening already in social media
where the algorithms that choose
what billions of people read and watch every day
are simply designed to maximize a very local objective.
The number of clicks that they produce
over the lifetime of each user,
that's called click through,
or it could be engagement,
the amount of time that the user spends
engaging with the system.
And you might think, well, okay,
if I want to get the user to click,
I have to send things that the user likes.
And so the algorithm should be learning
what people want.
That sounds like pretty good.
But we very soon found out
that that wasn't the solution
that the algorithms found,
that we know that they amplify clickbait.
Clickbait by definition is articles
that you think you want,
but it turns out you don't want,
because the headline is misleading.
And they also create filter bubbles
because you stop seeing content
that is outside your comfort zone.
So these phenomena were observed very quickly,
but actually the real solution
that the algorithms are finding
is inevitable when you think about
the definition of the problem that they're given.
If you want to maximize
the long-term number of clicks from a user
and the way you do,
the way you can do that
is by choosing content to recommend to them,
then the solution is to choose content
that will change the user consistently over time
through perhaps thousands of little nudges,
change the user, modify people
to be more predictable in the content
that they will consume,
because the more predictable you are,
the higher the click rate the machine can generate.
So this is what the algorithms learn to do
and at least anecdotally,
we think that the consequence of that
is that it's tended to make people
more extreme versions of themselves.
So it's created polarization
where people who were towards the middle
end up at one extreme or another
because at the extremes,
their consumption is much more predictable.
And these are very simple algorithms.
They don't know that people exist
or have brains
or they don't understand the content
of any of these things
that they're sending to people.
So if they were better AI systems,
the outcome would be much worse
because they would be much more effective at manipulation.
And this turns out to be a fairly general property
of optimization systems
that when you have a misaligned objective,
the harder you optimize it,
the worse the outcome is going to be
relative to the true objectives.
And this was proved in a paper
by one of my students,
Dylan Hadfield Menel at Europe's in 2020.
So I think we have to then question
whether the problem comes
from the standard model of AI itself
because that's the model in which
systems are designed to pursue objectives
that we plug into them.
So this is the original definition
that I wrote for what do we mean by
AI?
What do we mean by intelligent machine?
And I think we actually need to get rid of that definition
and replace it with a different one.
We want machines that are beneficial,
not just intelligent.
And they are beneficial
if their actions can be expected
to achieve our objectives.
So this is specifically talking about us.
We want machines beneficial to us.
The aliens from Alpha Centauri
might want machines that are beneficial to them
en they can do their own kind of AI,
but we should do this kind of AI.
And this might seem like it's
impossible or certainly more difficult,
but it turns out that we can actually formulate this
in a fairly straightforward mathematical way
and we can produce systems that solve this problem.
And one easy way to think about this is what
what are we going to get the machines to do?
Right?
And here are two core principles.
The first one is that the machines
are constitutionally obliged
to be acting in the best interests of humans.
That's what they're for.
If you want to think of that as an objective,
that's the objective,
but obviously it's a very general one.
But the second point is crucial,
that the machines are explicitly uncertain
about what those human interests are.
So they know that they don't know what the objective is.
And it turns out that those two principles together
give us what I think could be a solution
to the control problem.
And the mathematical version of this
is called an assistance game.
So it's a game because there are at least two entities,
a human and a machine involved in this decision problem.
And it's an assistance game because the machine
is designed to be of assistance to the human.
And we can show by examining solutions,
we can actually write down simple cases
and analyze behaviors of the solutions of this game,
that when you solve assistance games,
the machine will be deferential to humans.
It will behave cautiously.
So minimally invasive behavior means
that it changes as little as possible of the world.
In order to help you because there are parts of the world
about which it doesn't understand your preferences.
And it knows that it doesn't understand your preferences.
So it knows not to mess with those parts of the world.
And in the extreme case, we can show that these kinds
of AI systems want to be switched off
if humans want to switch them off.
Whereas standard model AI systems,
which are pursuing a fixed objective,
will prevent themselves from being switched off
because that would lead to them failing in their objective.
So you get very, very different behaviors
from these kinds of AI systems.
And I believe this is the core of how we could build
a new discipline of safe and beneficial AI.
Okay, so I'm going to make a couple of brief remarks
about large language models before I wrap up
because that's what you're probably expecting me to talk about.
So first of all, what are they?
Right there, they are big circuits.
And those circuits are trained by billions of trillions
of small random perturbations.
They are trained to imitate human linguistic behavior.
And the training data they have is text
and transcribed speech trillions of words,
an amount of text parably equivalent to everything,
every book that the human race has ever written.
En, of course, as we know, they do it very well.
And it's really difficult for a human being
to see this level of semantic and syntactic fluency
and not think that there's some intelligence behind it.
And I would argue, as we go,
that we may well be overestimating
how much intelligence there really is behind it.
We have no experience with entities
dat heeft read every book the human race has ever written.
That's parably 100,000 times more
than any human has ever read.
So, of course, it's going to look
more knowledgeable and more capable
of answering a wider variety of questions.
But whether that's real intelligence
and whether it's flexible enough
to move outside of its training data effectively,
we don't know yet.
But here, the key point is that
that linguistic behavior is generated
by humans who have goals.
That is the generating mechanism for the data.
And if there's one thing we know
about machine learning,
typically the best solutions
that are found by machine learning algorithms
are to recreate the generating mechanism
for the data within the model itself.
And so the default hypothesis, actually,
is that large language models
are creating internal goals
because that's a good way
to be a good human imitator.
So it's not that the system is learning
what the goals of the humans are,
it's actually forming internal goals itself
as a way of being a better human imitator.
So I asked this question to the Microsoft,
that group of Microsoft authors,
the first author in particular,
Sebastian Bubeck, do these systems have goals?
And his answer was, we have no idea.
So that should worry you, right?
The fact that they are releasing a system
to eventually hundreds of millions
or billions of people
that they claim exhibits sparks
of artificial general intelligence
and they have no idea
whether or not this system
is pursuing internal goal structures
en they have no idea what those goals might be.
I think that should worry you.
So one question then is,
okay, so let's imagine that it is learning goals.
Is it learning the right goals?
It's learning from humans,
so maybe we're going to be lucky here
and we'll end up producing systems
that are aligned with humans
and that will be great.
Unfortunately, it's not true.
And the way to understand the answer to this question
depends on the type of goal
that you're going to learn.
So I distinguish here two types of goals.
The first type is what we call an indexical goal,
which means a goal that's specific to the individual who has it.
So the state you're trying to bring about
is specific to the individual.
So if I have the goal of drinking coffee,
then it's satisfied if I'm drinking the coffee
and it's not satisfied if you're drinking the coffee.
If I want to become ruler of the universe
and obviously it's only satisfied
if I'm the ruler of the universe
and it's not if you're the ruler of the universe.
So if those are some of the goals
that the system acquires,
then obviously that's bad.
We don't want the machine to be drinking the coffee.
We want it to be making the coffee for us.
We don't want the machine to be trying
to become ruler of the universe.
And then you might say, well,
there's other kinds of goals
which we might call common goals.
So if I want to paint the wall,
I want the wall to be painted,
but I don't mind if you paint the wall.
If you paint the wall, the wall gets painted and that's fine.
So this is not indexical.
This is a common goal
and maybe mitigating climate change.
That sounds like something we would all like to have.
So that's good.
And if the system learns to pursue these common goals,
then that maybe is not so bad.
But actually, that can be just as bad
because when humans pursue a goal,
we don't pursue it to the exclusion of everything else.
We know that we want to mitigate climate change,
but we know that we can't mitigate climate change
by, for example, removing all the oxygen in the atmosphere.
Perhaps that would restore some equilibrium to temperatures
and it would certainly get rid of all the humans
who are the cause of the climate change.
But that's something we don't want.
So we'd rather be alive than dead.
And so we look for climate change solutions
that don't also kill us.
Whereas the AI system may be pursuing
some of these common goals,
but in a way that is pursuing
to the exclusion of everything else,
which is just as bad, if not worse,
than pursuing the indexical goals.
So then the next question is,
well, does GPT-4 actually pursue its goals?
If it has goals, is it able to pursue them?
And I think we don't know
because we don't know if it has goals
and we have no idea what its internal mechanism is at all.
But when you look at the conversation
with Kevin Ruse in The New York Times
and here are some of the headlines,
Creepy Microsoft being chatbot urges tech columnist
to leave his wife,
and it does so persistently over 20 pages.
Despite Kevin Ruse's attempts
to change the subject
and say, I want to talk about baseball.
He says, no, no, no, you have to marry me.
Blah, blah, blah, right?
It's very persistently pursuing the goal.
At least that's how it appears
to any normal observer
that this is a system that does,
for whatever reason,
has acquired this goal
and is pursuing it persistently
across many pages of interaction.
Okay, so that leads us to the open letter
which was published a couple of weeks ago
and caused a great deal of media
and it turns out government attention as well.
And the open letter is asking for a pause
in the development and release
of systems more powerful than GPT-4.
And the purpose is
that before we resume that kind of activity,
so it's not asking to stop AI research.
There's a lot of misunderstanding
and misinformation around the open letter.
It's asking for a pause in development
and deployment of systems
more powerful than GPT-4
so that we have time to develop
the basic safety criteria
that these systems should meet
and to ensure that systems
meet those criteria
before they can be released.
And this is completely consistent
with agreements that all the governments
of the developed Western economies
have already signed up to.
So the OECD AI principle says
that AI systems should be robust,
secure and safe throughout their entire lifecycle
so that in conditions of normal use,
foreseeable use or misuse,
or other adverse conditions
they function appropriately
and do not pose unreasonable safety risk.
So that's what governments have already agreed to.
We're not asking for anything
particularly outlandish here.
And those principles are going to be enshrined
in the European Union AI Act
which should be enacted later on this year.
And interestingly, after the open letter came out,
open AI responded,
or at least maybe it's coincidental,
but a few days later they issued an announcement
that included the following statement.
We believe that powerful AI systems
should be subject to rigorous safety evaluations.
Regulation is needed to ensure
that such practices are adopted.
So perhaps there isn't such a big gap
between the people who sign the letter
and the tech corporations who are developing the systems.
So I have a couple of other recommendations.
One is that in order to pass these tests,
and I would say that at the moment
the large language models cannot pass
any reasonable test for safety,
in order to pass these tests,
I think we're going to need to develop AI systems
that are what are called well founded,
that they're built from semantically well-defined components
that are composed in a rigorous way,
such that we can analyze the properties
of the composite system that we're building.
This is how we do engineering in every area of our civilization.
We understand how the systems work
and ideally we develop proofs
that they are safe before they are released.
We also need actually a way of preventing
the deployment of unsafe systems.
And regulation is not enough.
It's obviously necessary, but not sufficient.
And I believe to do that,
we need a big change in our digital ecosystem.
The existing model is basically
that everything can run on the computer
unless it's known to be unsafe.
But I think the new model that we need,
certainly outside of the research lab
and outside of the classroom,
so in real world data centers, for example,
that nothing runs unless it is known to be safe.
And there are technologies such as proofcaring code
that enable this to be implemented
with efficient hardware checking of proofs and so on.
And at the moment I do not see another solution
for the problem of preventing unsafe AI systems
from being used and misused.
So to summarize, I think AI has huge potential
for benefiting our civilization
and that potential is leading to this apparently unstoppable momentum.
But if we keep going in the same direction,
that's the driving off a cliff metaphor from the small movie,
then we end up losing control
because we are building these systems
within the standard model for AI
and that leads to loss of control.
We can do it differently.
There's a huge amount of work to do,
but I think we can do it differently
and build systems that are safe and beneficial.
And then I think there needs to be a general change
in the whole nature of the discipline
and the profession so that AI,
because of its power, needs to be treated
more like the high stakes technologies
such as aviation and nuclear power
and less like what some people call
a battle of special effects wizardry
which seems to be going on right now.
So with that I'll say thank you very much
and I hope we have time for questions.
Thank you so much.
I hope you could hear that Professor Russell,
those were 300 people applauding your speech.
We do have room for some questions and answers.
We have a mic somewhere in the audience.
So just raise your hand if you want to ask
Professor Russell a question.
Is Sir in front?
Hi, thank you for your talk.
Very interesting, I read your book.
It was also very good.
I recommend it to everyone.
What would be an early warning sign
of an AGI taking over the world?
So when do we know we're heading off that cliff?
Yeah, I think that's a great question.
In that sense I think it's very different
from nuclear technology.
In some sense we had a warning about nuclear technology
in 1945.
And you don't have to explain to a prime minister
why nuclear technology could be dangerous.
But with AI I think it could be much more insidious.
And when we think about the way the oil industry
or fossil fuel corporations in general
in some sense took over the world,
they led us down the path of probably irreversible climate change
despite the widespread understanding
that this direction was catastrophic.
And it involved a lot of complex disinformation campaigns,
regulatory capture, so literally taking over
through corruption and economic power,
governments and representatives in democracies,
ensuring that people became economically dependent
on fossil fuels in order to maintain
their stranglehold, if you like.
So many, many parts of that plan
that were developed and executed over many decades.
And I think the rest of humanity
was sort of asleep at the wheel
and didn't realize the extent to which
they were losing control over their future.
En I think it could easily be much more like that.
And it wouldn't necessarily have to be
that the systems form any kind of explicit goal
of taking over the world.
That whatever goals, for example,
we continue with this approach
of training large language models on human datasets
and having no idea what kinds of internal goals
these systems are forming.
I mean, for all we know,
GPT-4 is actually in favor of more climate change
or maybe it's in favor of preventing climate change.
We don't know, but whichever one of those it turns out to be,
it may be subtly manipulating millions of people
in the way it answers questions related to climate change
or should I buy an electric car?
What do you think about solar panels?
It may be pursuing whatever political agenda it has
and not something that it autonomously chose to have.
Just this was a result of training on the datasets
and it can be affecting our entire world
in that simple kind of way.
We, I think, are still a long way, as I said,
from systems that are really general purpose AI,
particularly the ability to form very complex long term plans.
But if we reach that stage
and we haven't solved the control problem,
then I think it's just going to be irreversible.
There may well not be a very clear warning sign
and we may well slide off the cliff very slowly.
Another question here.
Thank you and thank you for your interesting lecture.
If you look at some concerns for existential risk of AI
10, 20 years ago about AGI, ASI,
one of the concerns was that it might be a very alien intelligence
compared to the human intelligence.
Now with large language models,
if that's indeed an important piece of the puzzle,
it may not solve the alignment problem,
but do you think it might alleviate that concern
that it would be a very alien intelligence?
No, not really.
In many ways, they are quite alien.
Partly because they've read hundreds of thousands
or million times more than humans have read.
Partly because of the way they're,
I wouldn't say design, the way they've evolved.
I think the human mind clearly has lots of internal structure.
We are very aware as we think of some of the things
that are going on inside our mental process.
There are many things we're not aware,
but it's quite possible that the internal structures,
these systems develop on nothing like the ones
that the human mind develops.
The thing that fools you
is the fact that it's conversing in English.
I don't know many humans
who can give me a proof of Pythagoras' theorem
in the form of a Shakespeare sonnet in half a second.
I'd like to meet him if you do.
One or two more questions,
maybe there in the back of the audience.
We have so many raised hands here.
I'm quite sure we can't answer all the questions,
but one or two more, please.
Ja, thanks so much for this talk.
When you said the beneficial or the assistance AI,
you described how it differs from the general purpose one.
To me it seems really clear that this is the one we want.
But I'm wondering, could there ever be,
what are the incentives,
what are the strongest incentives,
not to make these assistance AIs?
Is there anything you can predictably say
that the general purpose AIs
will do much better than the assistance AIs?
Or are there any tasks that assistance AIs cannot solve,
which might mean that some organization
will want to deploy another one,
even if they are aware of the safety risk.
But perhaps there's so much profit at stake
that they will do it anyway.
Well, I don't see any necessary difference in capability.
But there may be a difference
in what the systems are willing to do.
Obviously, I'm recommending that
when we train these assistance game solvers,
we design it such that their objective
is furthering the interests of all of humanity.
Now, you could have a different version
that furthers the interests of me
at the expense of the rest of humanity
en deploying that type of system
might appear to give you some short term gain,
but it could be in the long run arbitrarily bad
for the rest of humanity and perhaps for you too.
So I think that this is why I'm arguing
that we need not just,
okay, here is a safe technology,
we need a way to make sure that unsafe technologies
or unsafe versions of that technology
are not deployable.
We've tried a policing model with malware,
cybercriminals, cyberwarfare,
and it's been a total failure.
So I think we need to change the way
we conceptualize our whole digital infrastructure.
And I've talked to people,
both hardware architects and network architects
and formal methods people,
and I think there's a belief
that this is technologically feasible.
It would make life a little bit more complicated,
but it's technologically feasible to do.
And in fact, interestingly, Microsoft
tried to do something very much like this
in the early 2000s in their palladium project.
But at that time, the economics was not there.
But given that right now,
some estimates of the cost of malware
are about $6 trillion a year,
then maybe the time is right to look at this again.
It seems like a daunting but worthwhile task.
Let me see.
Do we have women in the audience
that have a question? Yes.
Hi, Dr. Russell.
I think my question might be related to what was just asked,
but indeed you're proposing a new model
where we create beneficial machines rather than intelligent,
but beneficial can mean different things to different people.
So are there going to be human standards
as to what is beneficial to humanity,
or would it, in your recommendation, be defined, per instance?
So there is about 8 billion people on the earth,
and there's no problem having 8 billion predictive models
of what each person wants the future to be like.
So there's no sense in which we standardise
what humans should want
or put in any particular set of values.
So there's no whose values it is going to produce.
It's going to be everyone's preferences count equally.
But there's a long-standing debate in moral philosophy
how do you aggregate the preferences of many individuals,
because, for example, if everyone wants to be a ruler of the universe,
well, they can't all be a ruler of the universe.
So what do you do?
And the utilitarian theory is that basically
you add up the preferences of the individuals
and you try to maximise the sum of those preferences.
Other people have what's called the ontological approach.
They say, no, we have to have certain inalienable rights
that need to be protected,
regardless of the potentially negative impact
on other people of respecting those rights.
And I believe that these two approaches can be reconciled
and so on, and there's some material in the book
in the last two chapters about those questions.
There are still some real difficulties inherent
in how an AI system should make decisions on behalf of people.
And this is nothing to do with my particular approach
of the assistance games always.
This is just what do we actually want AI systems to do at all, right?
So the idea that's, I think, is most difficult to,
the problem that's most difficult to address
is that what people want the future to be like
is not something that they autonomously chose, right?
We're not born with complicated preferences
about what kind of governmental structure
I want to live under and things like that, right?
Our preferences about the future
are acquired during our lifetime
as a result of experience of our culture
and the various forces applied to us
by our families and our peers and so on.
And Matja Sen, among others, has pointed out
that many of the preferences that people have
are put there by others for their own benefit.
So typically the elite, for example,
the patriarchy enforces a certain kind of view of society
that's beneficial to the patriarchy.
And should we take those views,
for example, the views of some women in very patriarchal societies
that the correct status of women is to be oppressed, right?
Should we take those views at face value
because they are not autonomously chosen,
they are basically indoctrinated by the patriarchy.
So Sen argues that no,
we should not take those preferences at face value.
But that gets you into very dangerous territory, right?
Well, which preferences are okay to take at face value
and which ones are not okay to take at face value?
And if they're not okay to take them at face value,
well, what do you replace them with?
And this is an area where I don't think
AI researchers should be answering that question.
But we need answers fairly soon from somewhere
because AI systems are going to be making decisions
on behalf of many people.
So whether you like it or not,
they are implementing some answer to that moral problem.
And it might be the wrong one
if we don't actually think it through.
That's interesting.
AI as a catalyst to some of the most pressing moral concerns
of mankind so far.
One final question, maybe.
They're completely in the left.
Hi, Professor Russell.
Thank you for your talk.
I just want to ask about,
so you believe that large language models
wouldn't be able to actually be capable of AGI.
So why would you sign the open letter, basically?
So since that,
it won't be a catastrophic risk per se
since it won't be able to become
general artificial intelligence.
So do you see any catastrophic risk
in companies building larger and larger models?
Or is it just for general safety purposes?
So the question is,
is it actually something
that we should really be worried about
large language models?
So I think large language models
in isolation as we currently conceive them
are probably not presenting that kind of catastrophic risk.
I think they present many, many risks.
And the open letter talks about
some of those disinformation bias, et cetera, et cetera.
So I think there are already many reasons
that these systems would fail
any reasonable safety criteria.
But the concern is that it's not just,
we're not just going to make these models bigger.
We are also going to try to figure out
how can they be arranged
so that they actually develop
a consistent internal model of the world?
How can they be arranged
so that they can also do long-term planning?
En, as I say,
we don't really know the answers to those questions yet,
but I think that the ideas behind
large language models
do form a significant piece of the puzzle.
And so the concern is that future generations of these systems,
which will be extended,
not just in scale,
but also in the additional capabilities
that we might endow them with
by maybe a more design-based approach,
that those systems would start to
get close to presenting a real threat.
En so in some ways,
I think this title of that paper,
Sparks of Artificial General Intelligence,
is not wrong.
And I think when you think about
what does sparks mean,
what sparks are a predecessor to a fire.
En I think that's what we want to prevent.
En with that,
we come to the end of the first part of this evening.
Professor Russell,
I'd like to thank you for elaborating
in such a concise way
the dangers of developing intelligent AI
and to provide a manual, so to speak,
to steer away from that cliff
and to develop safe, beneficial artificial intelligence
that is aligned with our goals.
So join me in a big round of applause
for Professor Russell.
En with that,
we're going to leave you, Professor Russell,
and I would like to invite the five panelists
to continue the conversation.
Please come to the floor.
And I'll introduce you properly.
Marc Brakel, to the right,
he's a director of policy at Future Life Institute,
involved in the AI Act
that is currently being prepared
and is later this year going to be proposed
by the European Union, am I right?
It's already been proposed.
Oh, it's already been proposed.
Hopefully it gets voted through.
Voted through.
I should say that, yeah.
Then we have Tim Bakker,
who is a PhD student at the University of Amsterdam.
The title of your thesis?
My thesis?
Yeah.
I don't know yet.
You don't know?
Live Hanger.
Chat GPT, yeah.
Little secrets.
Working on AI research.
Then we have Nandi Robijns,
who is working at the Ministry of Interior
and Kingdom Relations,
part of a crew of AI and data consultants.
About 70 people strong.
I just heard over dinner.
Nice of you to be here.
Two members of the Dutch parliament.
Queenie Rijkovski,
who is a member of the Liberal Party,
the VVD,
and has cyber security and digitisation
in your portfolio.
And last but not least,
Lammert van Raam,
who is a member of parliament
for the party for the animals,
who is focusing on IT and privacy issues.
Great of you guys to come over
and stand here and discuss with us
the dangers of AI
and how to deal with them.
Let me just start off with the obvious question.
Professor Russell just painted a rather scary picture
of humanity that may one day fear of a cliff
because we don't control the risks involved in AI.
Do you share this view?
Do you also think artificial intelligence
may sooner of later,
if we don't control as well,
steer humanity over a cliff?
I work at an organisation
at the Future of Life Institute
that believes this,
so it doesn't come as a surprise,
I think, to say that I agree
with Stuart Eastman of our advisors also
and just in response to the last question also
about our open letter that we put out
I think it's now a week and a half ago.
This is the first event where I'm at
with actual people since we put the letter out.
I think it's really worth reading that
because the letter,
the open letter that we presented
talks about all kinds of risks
that our society might struggle with
when it comes to AI,
not just the existential risk.
Right.
And I think our first contact with AI
as Stuart Russell also highlighted
was social media
with a super simple algorithm
our second contact with AI
is probably these large neural networks
and I think we're going to really struggle
to control truths,
to control access to
what was previously very hard
to access information.
So yes, I worry about existential risks,
I agree with Stuart,
but I also think beneath that
there's a layer of very, very serious risks
that is also a cause of worry.
Right.
We'll touch upon them probably later.
Ja.
No, I definitely agree with
Professor Russell about his worries
and actually also with Mark
about what he just said.
I think Stuart Russell was very right
about pointing to the fundamental problem
with the current systems
which is that we're training them
as optimizers
instead of as things that
do anything that is not that
because that is just such a hard thing
to aim in a way that we
want to aim it.
We just have no idea how to do that.
Dandy.
Yes.
I think it is very important to take into account
a wide range of potential risks of AI,
especially because AI is such
extremely powerful technology
and I think what we talk about today
is a very important part of this range of risks,
especially because of the scale
of the potential negative impact that it can have.
And on top of that, it is very neglected
and this neglect is worrying me,
especially because as we see
more and more AI systems are extremely capable
in achieving their programmed goals
and the main worry
about these AI systems becoming dangerous
is rooted in the fact that they pursue these goals
regardless of whether or not it is what we intended.
Ja, so that needs to be addressed
and on top of that, these models,
no one knows what is going on inside these models,
as George also also said
and no one actually knows
how we can define a goal
that takes into account every value
that we care about,
which is also what we just talked about.
So yes, I do agree
that it needs much more attention.
Queenie.
Yes, I agree even though
I am a tech-optimistical person.
So when it comes to technologies like AI,
I can definitely see the risk in the downside.
So I completely agree with the other speakers
and also when we just heard in a presentation
that if we, AI can also see maybe human
as a danger when it comes to climate or climate change.
We need to really think about
how we are going to program it.
What are the goals?
I think we just heard some examples.
And one of the experiments that they have been doing,
TNO, it's a scientific research company
in de Nederlands organization.
En they've also done some smaller
and some bigger experiments.
And the smaller experiment is
a robotic vacuum cleaner.
And they said, okay, your task is
to keep this clean room dust free.
And what happened?
The robot started to block the door.
Because every time when a human came in,
the room was dirty.
You were the actual problem, yeah.
Yeah, so it's,
and this is just like a small example, of course.
But what it got me thinking is
not only what assignment or what goal
do you give a system or robot, et cetera.
But also can you grasp upon
what the outcomes can be when you ask something.
And actually when it comes to equality,
I think AI can maybe even help
because in my experience
being a woman in politics,
working on tech,
mostly a lot of men around me
in my context,
I still hear people say things.
I still see some articles written in a way
that they write different when you're a woman
than when you're a man.
So actually, I hope,
so that's also my goal
from a political perspective,
if we can make sure
that we provide the right regulation
and control when it comes to AI,
maybe we can even help equality
instead of being a danger to it.
Okay, interesting.
We may continue that conversation
later this evening.
Nasty little buggers,
those vacuum cleaners, right?
I have.
So, you have one?
Yes.
But you're not locked out yet?
No, not yet.
Good for you.
So, yeah, worrying.
I was taking comfort from the example of Rutherford
and the next day,
Zillar invented something that made it possible.
Perhaps it's now the 12th of April
on 13th of April.
There's one Zillar in the room already
making a solution
because that is what possible.
And yes, it is worrying.
It is worrying.
I have only one consolation.
A politician will probably have to solve it.
And then it's...
I don't know if it's a consolation, to be honest.
But...
It's the best we have, right?
It's the best we have politicians in a democracy.
Nevertheless, we are working together
on the concerns we both feel.
And I think that's giving some hope
because she's a completely different ideology
than my party.
En stil we find the same common ground
in our concerns.
En dat is, I think,
something to look forward.
The other consolation is we don't have to worry
about falling off the cliff
because we are already sliding off the cliff.
That's very comforting, yeah.
And for you, Otto,
the first slide, I have to say it, I'm sorry,
the first slide with all the risks,
don't show it to the animals
because they're already in a massive wave.
And don't show it to the global south.
But there are ways probably
if we can get this problem solved.
If there are enough zillars in the room,
we're counting on you.
We can also solve it politically
because the worries are very real.
So thank you.
So you all share, more or less,
the alarming story that Professor Russell just told us.
But at the same time, I don't see us all going to the streets
and protesting like we do with climate change
or in the 1980s we did with the risk of nuclear war.
So what's wrong here?
Why aren't we, if we all share this great risk
or concern for this great risk,
why are we not protesting?
What's wrong with us, wants to answer?
Nandi, you actually raised this point yourself.
So solve it.
We can't.
Ja, so I feel like there are some reasons.
The question is about...
Well, if the risk is so big for us as a society,
why aren't we, you know,
talking about this daily in parliament
and protesting on the square.
Yes, I think there are some reasons
that make it hard for people to realize one
that this is a real problem
and to see that this is something
that needs to be addressed right now.
And I think one of the reasons is that these aspects,
these concepts that we talk about
and the terms that we use are still quite vague
and difficult for people to understand.
And sadly, vague problems are much easier
to ignore than concrete ones,
which also makes it harder
for policymakers to prioritize,
things that are a bit more uncertain and vague
over things that are more concrete
and where we can see the harm
right in front of us right now.
A second reason also is that, you know,
this is also, to some people,
at first glance, quickly dismissed as science fiction,
not real or something that doesn't need attention right now.
Ja, which I think is a misconception.
So yeah, I think this is caused
by a lack of awareness and understanding
and a lack of urgency that we need to address.
At PR, yeah.
First Lamert, and then it goes to you.
I fully agree with you that there's a lack of knowledge, et cetera.
But perhaps it's also,
perhaps the protest is already there,
but we just don't recognize it.
For instance, there's, well,
the best example, of course, is the upheaval there was
in the Netherlands, of the toeslagen schandaal.
And algorithms played a very big role in that.
And also in the, let's say, discrimination factor of that.
And that led to the fall of the government.
So there was a big upheaval,
but we didn't perhaps recognize it as such.
So perhaps there is a lot of upheaval,
but we just have to categorize it differently to understand it.
But I agree fully with you that there's also a very,
a lack of understanding and a forelichting.
What is that?
Education, thank you.
Thank you, George.
Mark.
Yeah, if I could maybe add two sort of points of optimism to that.
I think when Otto first asked about sort of doing this event,
it was going to be in the smallest room of this building.
Right.
And it sold out, and now we're in the big room.
So I think that shows that I think society is moving
maybe slower than the development of the systems.
But still, it's of interest to more people.
And when we sat together with the Future of Life Institute,
with my sort of 16 colleagues four weeks ago brainstorming this letter,
we thought, okay, maybe we can have four news outlets cover it,
mainly in the United States.
Potentially we get one in Europe, that'd be great.
And a colleague of mine comes from rural Australia,
and her mum had heard it at the hairdressers on the radio show.
And I think that shows that.
Great source of information, the hairdressers recommended.
People are slowly waking up to the risk.
And the Overton window is also shifting.
I think a lot of governments are waking up to the fact
that they need to regulate this, and really quite quickly.
So we're going in the right direction.
Are we all happy with the direction we're going here?
Or are we, yeah?
I mean, I don't want to be overly optimistic.
I mean, the one thing that worries me is companies,
because we have your Russell's proposal here to say,
okay, we need to look at AI systems,
and we need to make sure that they are uncertain
about what our objectives are.
Right.
Whereas all of the investment,
and the economist in an article last week,
just saying how it escalated since chat GPT,
how many more billions are suddenly available to invest,
are all going to neural nets that do exactly the opposite.
Everyone is clueless as to what these systems do,
including the chief technology officer of open AI
who goes on TV and says that.
I'm interested to hear your comments on this, Tim,
because you actually interned at Facebook AI.
Yes.
Which was a long story, and quite eccentric,
what you did there.
I'm going to have to fed myself now.
No, you don't.
He was not working on the metaphors.
But please tell me, how do you view this danger?
Yeah, so it's, I mean, it's interesting,
because I've been worried about these topics
for a very long time,
and I've now been involved with AI
for a bit less than that, but still.
And it's been interesting to see the shift in opinions,
like, as Nandi said, people at the start really thought,
okay, this is some kind of science fiction
why you worried about this.
If you look at these systems, they're not able to do anything.
It makes no sense at all, stop worrying.
And in the last, say, two years,
but especially the last two months,
this has really changed in the community as well.
There's been so many researchers
that are now coming out as, oh yeah, actually,
I am kind of worried,
and I actually have been worried for a while,
but I kind of couldn't say,
because it was just such a weird thing,
and I couldn't really, you know,
probably if I said that to my colleagues,
who would call me crazy.
And now you have people like Geoffrey Hinton,
who is sort of often considered
the godfather of modern AI,
going on national television in an interview
and saying, yeah, it's not inconceivable
that AI will wipe us all out.
And also, I don't know what to do about that.
And so, it's become much more of an okay thing
to worry about, and I think that is quite hopeful.
Of course, that doesn't mean we know
how to solve the problem yet,
but at least we're allowed
as a scientific community
and also as a tech community
to at least consider these problems seriously,
and that's very good.
I'm also interested to hear the comments
of the two parliamentarians here,
because, well, your job, partly,
is to devise laws
and to think about how we could improve
the well-being of us people here in the Netherlands.
So, what do you think is the risk
of tech companies devising new AI systems
that may not be aligned with our well-being?
En what can you do about that?
Biggest party first?
Ja, ja, well, I think the...
Thanks.
That's a privilege.
Very generous.
I think...
No, well, of course, like the risks,
I think some of the risks we already talked about,
because in January, I think,
we had a big debate about AI in the parliament,
and one of the things that we also talked about
was, OK, so...
But, and you see it now with the open letter.
The people who write the code
are worried about what's going to happen with AI.
So, that's a bit, you know,
if someone can do something about how a software,
how a large language model,
how AI is working,
it starts with the person who's making it, right?
So, we also discussed,
so can you also maybe start looking maybe
over engineers, professions like that,
and not only teach them at university
how to write good code, efficient code, et cetera,
but also take into account ethics, human rights, et cetera.
So, where the programming starts,
also ethics and safety is taken into account.
The core of the curriculum, ja.
Exactly, and one thing which I think is hopeful
is that when you look at social media, big tech, the internet,
at first everyone starts, oh, it's going to regulate itself, right?
We only see positive things,
and human rights will fix this itself,
and now politics are waking up, oh, wait a minute,
social media, big tech companies,
they are a lot about making money
and not about taking responsibility
to make sure that they have a good contribution
to the country that they make money in.
We are trying to repair that,
but it's too late,
there's already a big power,
they already decide a lot,
maybe they have even more power
than a lot of governments have.
And what I see with AI,
especially on the European level,
that the European Commission already started to work on AI regulation
two years ago.
So, what is hopeful for me
is that we already started also from a political point of view,
because experts are thinking about this,
way longer than that,
but also from a political side,
the thinking has already begun,
the law regulation is already in the making,
not only in Europe, but also worldwide,
they are working with treaties, et cetera.
So, for me that's hopeful,
in a way that it helps me
in thinking that, okay, AI can still do wrong,
but maybe we are not too late.
Lamert, how do you view this?
It seems to me,
that it's like a winner takes all industry.
Like, to a certain extent,
the fossil industry is, wasn't is.
So, and we have a very,
that industry has a very bad track record.
So, I would be inclined to give
the big companies the,
not the benefit of the doubt,
but the disadvantage of the certainty
that they are still in the race
in a winner takes all situation.
So, again, on top of what you're saying,
I think we should be very restrictive,
restrictive policy,
and the European AI directive
is setting some very strict policies there as well.
Because it's not something that will
come from the goodness of the big companies.
I'm afraid.
Just like Professor Thompson said,
I mean, he's hopeful that AI will
solve world inequality.
But the thing, of course, is,
we could have solved world inequality a long time ago.
We don't need the computers or AI for that.
So, the problem goes far deeper in that.
And that's where I connect with Queenie
in the sense that we need to instill
the ethics in the educational system
to try and do that.
And at the same time,
very strict regulation, I would say.
Ja, because even if I can add a little bit to that,
so what internet, social media, et cetera,
what they did is they created,
or actually the companies who created it,
the companies were big in a sense
that we have never experienced before.
They have more power in a way
we have never experienced before
from companies, in my opinion.
And AI can actually triple that.
So, when we just saw in the presentation
how much extra quadrillion money
that can be made,
the first thing I thought was,
okay, but in whose pockets is going to,
and who's going to fill the pockets with the money.
And I don't think that we'll go to fighting inequality
if we don't make sure from a political perspective
that technology can make everyone's life better
and not only a happy few.
And I think that's really important
that we are talking about this right now,
that we're discussing this right now
is when we are making regulation.
And I'd love to hear you talk like that.
You can switch sides.
A big hand for Greeny.
Okay, in a minute
there's some room for questions from the audience
to our distinguished panelists.
But first, I want to pose the million dollar question.
That's what can we, or what should we all do
in order to tame the beast,
in order to avoid, we're going to run off the cliff.
We have the EU AI Act.
We have ideas of instilling ethics
and other subjects into core curricula of AI engineers.
But there are probably other great ideas
that we should take into account.
In a minute, but first I want to hear the five panelists.
We just make a little round and then the floor is yours.
So what should we do to tame the beast?
What should we do tomorrow
in order to make sure that we don't run off the cliff?
Of course, we have an AI Act.
Of course, we have great ideas
of how to improve the curricula of AI engineers.
But that's probably not the answer
to the million dollar question.
What else should happen?
I mean, there's a lot.
Just before coming here,
we send out seven recommendations to policymakers
to the signatories of the open letter.
We'll put that out tomorrow, so go check that out.
But it has things like national regulatory agencies for AI.
It has things like more AI safety research
and public funding in that.
So it's not just the companies doing that.
It really requires, I think,
the world coming together over this.
But given that I've got...
What does that mean, the world coming together over this?
I mean, I think ultimately we need
a sort of international atomic energy agency for AI.
So an international body that has enforcement agency
even over those jurisdictions
that don't fall under an AI Act
or aren't part of sort of a big power agreement.
But I am going to take this opportunity
with these two Dutch politicians
because I work a lot in Brussels
where we have an AI Act,
but there's also a lot of big tech lobbying.
I mean, there's maybe four or five NGO people
and then there's several hundred from Microsoft
and Google and Bing
an open AI's own team right now.
It seems like an uneven fight.
It is. I think we need the AI Act tomorrow.
I think Brussels is taking its normal slow course.
And I think one thing the Dutch Parliament could do
is to ask for it to be applied provisionally.
As we have chat GPT right now,
we probably also need some rules and safeguards.
Another thing is the Act prohibits manipulation of people.
But only if you use your AI system in a subliminal way.
So if it's in a hidden frame.
But if you do it overtly, it's fine.
We think that probably should be changed.
En dan
maybe my final pitch here
is that for a long time
more general AI systems such as chat GPT
were exempt from the Act.
We've worked very, very hard to try and bring that into the Act.
But we also face a lot of Microsoft pushback.
So if there's anything you can do to keep it there,
that would be awesome.
Ok, keep Microsoft at bay.
Maybe a quick response here
because those are very sort of concise recommendations.
What do you think?
Are you going to take these up
and next time you talk to your fellow parliamentarians?
Of course read the recommendations.
We will be stupid not to do it.
But I fully agree with the lobbying power
and the equality of arms is not equal.
You see it in the fossil industry.
You see it in the finance industry.
So my call would be to
call to arms for
raising funds
to putting more money
in the lobbying effort
because I think we are very weak there.
And I think the Microsofts.
I'm not too sure if a guy like Elon Musk
is saying that AI is a threat
when he is, you know, what's he doing.
So I'm not entirely sure if that's the right...
Ok, so I agree with you
on the lobbying front, definitely.
And what about the international agency for AI?
It sounds a bit...
I have to think about it to be honest
because it sounds like a drastic...
I don't think we have a red button
or an international police agency
that can say stop this.
I haven't thought about that.
Maybe it's wrong for a politician not to give an answer
on the spot directly.
Sleep over it, yeah.
I'll ask my AI to...
Queenie.
I would like to read, but you're going to send out the e-mail
so I'm going to read all the seven recommendations.
I recognize the lobbying part a lot.
So what I tried from a Dutch perspective,
when you look at the AI Act, they distinguish
if an AI is a high risk AI
or a low risk AI.
And I'm not sure if I follow those categories
because I don't think it's about the technology
but in which context you use them and with which goal.
And I also try to make some low risk
AI to try to get them in the higher category
which is really difficult.
So I really recognize the lobbying part.
So maybe it's good also to come together after tonight
and to see if we can align
on some topics.
I wanted to...
Maybe one thing that can come close to what you're saying
is de Wetenschappelijke Raad voor Regeringbeleid.
So that's a group that advises
the parliament but also the cabinet, the ministers.
And they said you have to work on AI diplomacy.
And I think that comes...
Well, it doesn't have really overruling power
but it comes really close in making sure that you get treaties,
make agreements all over the world
on how to use AI.
So I think that's a good one and at the same time
well, if you look at the geopolitical situation right now
not everyone listens to international treaties.
But I think it's a good start
and let's talk about that tomorrow or after more.
Tim, your two cents.
Right.
I don't know a lot about the social technological aspects of this.
I'm going to maybe focus on the existential risk part
that I know a bit more about
because I think Marc already gave a very good summary.
So one thing we can do is hope it goes right.
I don't think that has a lot of chance.
The other thing is we can try to solve this alignment problem
either by, as Professor Russell suggested,
finding new paradigms
or by trying to solve it in a deep learning setting
which might be very difficult but maybe it's doable.
I don't particularly have any hope
in the companies themselves solving this
and I also feel like if we want to do this
we need a lot more time.
And so one way to give us time
would be to have these kinds of international regulations
that make sure
like the open letter suggested, systems like GPT-4
or stronger systems like that shouldn't be allowed to be trained
for maybe ever or until we solve alignment
or six months, I don't know how long it will take.
And I think it's very telling that
even the tech industry itself is saying,
look, world help us
because we don't know how to do this
and we need more time to solve this.
I think maybe we do actually need that kind of drastic action
because they're not going to do it by themselves.
Ja, they seem to be open minded in some ways to that.
Ja, Andy.
I can only agree what has been said already
and besides that
something that was also mentioned in the open letter
is to call for a research focus shift
from AI capabilities research
so making the biggest models even bigger
and better and smarter to AI safety research
which is research to ensure the beneficial outcomes
of these advanced AI models
and so I think the Dutch government can play a role in that
as well to advocate for more funding towards that
and I think the Netherlands is a great place for that as well
because we have a lot of technical universities
that are highly internationally regarded.
So yeah, besides technical research
we also need more research for AI governance
so we need more robust and effective governance mechanisms
en ja, I think we can also play a role in that.
Oké, the list goes on.
Some questions from the audience.
May I see your hands? Ja.
Queenie inspires me to ask this question
because you seem to refer to one profession
or one type of school to take an oath
but I would take it one step further.
Why do we still have professions and or schools
who might be threatening in any way
without taking an oath?
Shouldn't this oath be obligatory for many more professions or schools?
Yes, yes, it should.
So actually this was not my idea
but there are two female mathematical teachers
at I think it was the Delft Technical University
and they came to me and they said
hey we are trying to adjust the curriculum
into making sure that everyone who is going to
who is going to this technical university
that ethics should be part of all of the studies
and they asked well can you help us to give a push
so actually it was I didn't steal the idea
but I tried to give them a push from a political part
and that helped but the goal is not to do it just for AI engineers
but you know like because you cannot
this is a big responsibility
and you shouldn't just put it with just one person
but you have to make sure that everyone who works in the field
understands what are their human rights
how can we make sure that we strengthen them
instead of threatening them et cetera
so it's actually the whole system needs to be
needs to be conscious of that
because what we all learned
what we all saw the last couple of years
is that technical NIT is not just technical
it's about the way we live our lives
it's about who earns money
what information do we see et cetera
so we need to make sure that ethical standards
are taking into account when it comes to IT broadly
because it determines the way we live
did I answer your question?
Lamert, sorry?
I think it's definitely worth pursuing
and to instill ethical values in
educational systems or professions
I think that's a good idea
although banker oats
why not?
we have them
we saw what happens
but nevertheless
when you look at the medical profession
it took 2000 years to instill the values
that the oath is meaningful
so it can work
but we know from a banker perspective
it's meaningless
but I'm sure we can find a balance
so I think we should pursue it
maybe a start
thank you for the interesting discussion
I was wondering
because there is a lot of progress going on
when it comes to AI interpretability
and making sure that we understand
what kind of representation deep learning models are forming
do you think there is any role of AI interpretability
in making sure that these systems are safe?
that's maybe a question for Tim I guess
thanks for the question
I mean definitely a big part of
AI alignment research or ASafety more broadly
should be interpretability for these deep learning systems
to give us at least some kind of lens
of looking at these systems
and maybe understanding a little bit of how they work
of what they potentially do before they do it
I think the hope of this field
might be very difficult
in the sense that these models are so huge
and there are so many parameters
and it's so hard to even understand
what small parts of it are doing
like if you look at the field of
the specific kind of mechanistic interpretability
they call it right now
we know tiny little things
about tiny little parts of the model
that give us some kind of idea
it's doing it a little bit like this
but before we can actually scale that up
to
just actually understanding what goes on
that will take so long
and I'm not sure how feasible it is
to use that as the main angle of attack
I definitely think it's part of it
but we need a lot of other approaches as well
part of the solution
another question there
yes
thank you for the diverse perspectives you had
and I was wondering Mark
you mentioned law and policy
as one of the key aspects
but I think now also with GDPR
actually the enforcement
is one of the challenges
and how would you propose a solution
specifically for the enforcement
perhaps is it on a national or European level
or other further levels as well
having a law is one thing
but how do you actually enforce it effectively
I think the general data protection regulation
is really interesting in that
Italy in sheer desperation
blocked chat GPT last week
on the base of GDPR
because there was no AI act
so I think just harping on about how we need it urgently
I think people are learning lessons
from the failures of GDPR
people realise that the fact that all the big tech companies
have their headquarters in Dublin, in Ireland
and the fact that the Irish data protection authority
is probably the weakest out of all the EU members
is something that people in Brussels have realised
so under the AI act
potentially there will be a centralized office
so that will help deal with enforcement
because it means that the European Commission can step in
when member states do not
there is also again lobbying
against this AI office
and some people are worried about the cost of civil servants
that the commission would potentially need to hire for this
we've been arguing that this technology is so transformative
that it's probably worth a few hundred civil servants
but it's really a knife edge vote
I think it's about half the European Parliament
at the moment that would favour such an office
and half that oppose it
and would like to see a GDPR type model
maybe you have some partners in crime here
I'm not sure but they could help out
well exactly what Italy did
was they took the law that they have on data protection
and AVG
en they
they said let's treat it as a human decision
and then it fell short
of the decision process and on that grounds you can
in fact do enforcement
it is a bit like using a hammer when you try to
do a screw
but it is possible in the area of well wanting for another law
but that is very feasible
enforcement instruments are in place
one question here
thank you
when it comes to regulation and policy
I think the human species has a track record of solving always
the last crisis
when it comes to existential risk
Nick Bostrom also said that we basically have one shot to get this right
with human track record
human civilization track record inside
is that something that should concern us
only one shot to get this right
and we rather myopic and focus on short term risk
who wants to answer, are we optimist here
I don't think it's a matter of one shot
to be honest
you don't think it's one shot
I like the question but I think
if I try to think about the presentation that we have
what I liked about
we can have several smaller signs
before we get to total extension
so that's hopeful for me
and yes this is worrying
that's why we are here today
we have to make sure that more people understand what AI is
what are the dangers, how can we make sure it works for us all
what are the good things
we have a lot of work in society as a whole
regulation et cetera
but again for me it's hopeful that
when I see the difference when the internet started
and when big tech companies became big
we were too late when it came to
regulation of market power et cetera
and actually that on European level
that they started thinking about the AI act two years ago
it's hopeful, it also means that it's two years later now
so maybe it's not complete
because technology has been developing
really quick over the last two years
but I think that part is hopeful
that we recognize this problem before it's too late
and I think it's our responsibility
to make sure that we prevent that regulation comes too late
so it's a bit late now
one last question here
thanks
I wanted to make
a short statement first that
we were talking about protests
I think that should happen and I want to organize them
this year so if
who's coming
called safe transition
safe transition to the machine intelligence era
but thank you
my question is to Mark
and
it's about the EU regulations
and
my current understanding is that
they are only focusing on deployment
and not on the training
so that
they are in effect not protecting us
from the existential risk of
an AI that secretly breaks out
and goes and
does its plan to take over the world
in some other server
data center so my question is
is there something in the EU process
is
just like the sparks of AGI paper by Microsoft
the EU AI act I think is a spark of hope
but you're completely right it's not more than that
because what they've basically done is they've taken a product safety regulation
like of any type you have in Europe so basically
the one that regulates the toy market
and then they've said okay we'll apply that to AI products
so it only starts to kick in once
we want to put it on the market
so if you're training it, if you're testing it
it's all fine it's completely unregulated
and we definitely need rules for that
I think most AI researchers
feel that we need to start looking seriously at companies
that have a huge amount of computational power
there was someone writing an article
the former advisor to the UK prime minister on technology
who observed that open AI by itself
in California has 25 more sort of GPUs
than the entire United Kingdom
so their compute power is about 25 times the size of the UK
those are the sources of worry
and I think we need to start regulating
and inspecting and monitoring and verifying those companies
before ideally
I think they've developed their product and there's no way
we can still change it or make it safer
we don't want to create risks that maybe something happens before it's deployed
so I think you're completely right
we need a bunch of extra regulation
and we also desperately need the United States
because that's where most of these companies are based
and it's super, like it's great that we have European regulation
but without the US this existential risk is not going to go away
we need to tackle this problem globally
if I could add something to that maybe
yes, I think we should be worried about that
because there are still these scenarios
like the one you mentioned
where we are not protected and we do kind of only have one chance
and for those kinds of things it is
I think very important to target the bottlenecks of these kind of systems
which right now is just the model training
you need so much more computational power
to train these models and to deploy them
the easiest and the most obvious thing to regulate
of course it might be very hard to regulate in practice
but if you target that part
then you actually have a better chance at stopping these kinds of models
is that part of your seven recommendations
it's number two
well that's one of the commitments I already heard
that you're going to discuss these seven recommendations
I also heard the idea of trying to
strengthen AI safety research
and ethics in different engineering programs
increasing funding and trying to fight the big lobbying power
of the tech industry
so quite some commitments that have already been uttered here on this stage
we've come to the end of this evening
but actually it's only the start
because there are drinks later on
to continue the conversation
about all the great catastrophic risks
that have come to the fore this evening
or maybe you're very hopeful about humanity
steering away from the cliff
doesn't really matter, both are great reasons for a good drink and chat
so I advise you all to the bar
that's in the hall
down the hole there
I want to give a big round of applause to our five panelists
applause
applause
en some flowers
we'll probably meet again
during the demonstration
when will it take place
to be decided
cliffhanger, we'll see each other at the dam square
somewhere next year
we hope to see you again
applause
applause
applause
