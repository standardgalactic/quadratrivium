1
00:00:00,000 --> 00:00:07,120
and to me it's not clear right are we at the right brother stage or are we at

2
00:00:07,120 --> 00:00:15,560
the Mongolfi a stage where we have a lot of hot air and so my current view is no

3
00:00:15,560 --> 00:00:25,480
we have not succeeded and the models that people are excited about the large

4
00:00:25,480 --> 00:00:31,320
language models and their extensions into multimodal models that take in video

5
00:00:31,320 --> 00:00:37,480
and can actually operate robots and so on that these are a piece of the puzzle

6
00:00:37,480 --> 00:00:44,200
and and CNN made this lovely animated gif here to to illustrate this idea that

7
00:00:44,200 --> 00:00:49,200
we don't really know what shape the piece of the puzzle is and we don't know

8
00:00:49,200 --> 00:00:54,160
what other pieces are needed and how it fits together to make general purpose

9
00:00:54,160 --> 00:01:00,560
intelligence we may discover what's going on inside the large language models

10
00:01:00,560 --> 00:01:06,720
we may figure out what source of power they're drawing on to to create the

11
00:01:06,720 --> 00:01:11,920
kinds of surprisingly capable behaviors that they do exhibit but at the moment

12
00:01:11,920 --> 00:01:20,200
that remains a mystery and there are some gaps right one of the achievements of

13
00:01:20,200 --> 00:01:26,320
modern AI that people were most proud of and also most certain of was the defeat

14
00:01:26,320 --> 00:01:36,320
of human go champions by alpha go and then alpha zero in the 2016 to 2018

15
00:01:36,320 --> 00:01:43,840
period so in go for those of you don't know there's a board you put pieces on

16
00:01:43,840 --> 00:01:48,600
and your goal is to surround territory and to surround your opponents pieces

17
00:01:48,600 --> 00:01:56,840
and capture them and since AI systems beat the world champion in 2017 they've

18
00:01:56,840 --> 00:02:03,160
gone on to leave human race in the dust so the highest rank program is catago

19
00:02:03,160 --> 00:02:08,480
and its rating is about five thousand two hundred compared to the human world

20
00:02:08,480 --> 00:02:15,120
champion at three thousand eight hundred and the human world champion leaves our

21
00:02:15,160 --> 00:02:22,960
colleague kelin pelrin who's a grad student a decent amateur go player his

22
00:02:22,960 --> 00:02:30,680
ratings about 2300 and now I'll show you a game between kelin and catago where

23
00:02:30,680 --> 00:02:35,040
kelin actually gives catago a nine stone handicap so catago is black and

24
00:02:35,040 --> 00:02:40,160
starts with nine stones on the board right if you're an adult go player and

25
00:02:40,160 --> 00:02:43,280
you're teaching a five-year-old how to play go you give them a nine stone

26
00:02:43,320 --> 00:02:47,560
handicap so that at least they can stay in the game for a few minutes right so

27
00:02:47,560 --> 00:02:55,760
here we are treating treating catago as if it's a baby okay despite the fact that

28
00:02:55,760 --> 00:03:03,760
it's massively superhuman and and here's the game so it's speeded up a little bit

29
00:03:03,760 --> 00:03:10,000
but watch what happens in the bottom right corner so white the human being is

30
00:03:10,040 --> 00:03:15,080
going to start building a little group of stones there they go and then black

31
00:03:15,080 --> 00:03:20,880
very quickly surrounds that group to make sure that it can't grow and also to

32
00:03:20,880 --> 00:03:25,280
actually have a pretty good chance of capturing that group but now white starts

33
00:03:25,280 --> 00:03:30,240
to surround the black stones and interestingly black doesn't seem to pay

34
00:03:30,240 --> 00:03:34,640
any attention to this it doesn't understand that the black stones are in

35
00:03:34,640 --> 00:03:38,240
danger of being captured which is a very basic thing right you have to

36
00:03:38,280 --> 00:03:42,440
understand when your opponent is going to capture your pieces and black just

37
00:03:42,440 --> 00:03:48,040
pays no attention and loses all of those pieces and that's the end of the game

38
00:03:48,040 --> 00:03:55,400
so something weird happens there right where an ordinary human amateur go

39
00:03:55,400 --> 00:04:00,640
player can beat a go program that stratospherically better than any human

40
00:04:00,640 --> 00:04:05,880
being has ever been in history right and in fact the go programs do not

41
00:04:06,240 --> 00:04:10,960
correctly understand what it means for a group of stones to be alive or dead

42
00:04:10,960 --> 00:04:18,320
which is the most basic concept in the game of go they have only a limited

43
00:04:18,320 --> 00:04:25,160
fragmentary approximation to the definition of life and death and that's

44
00:04:25,160 --> 00:04:30,840
actually a symptom of one of the weaknesses of training circuits to learn

45
00:04:30,840 --> 00:04:35,640
these concepts circuits are a terrible representation for concepts such as

46
00:04:35,720 --> 00:04:40,040
life and death which can be written down in python in a couple of lines can be

47
00:04:40,040 --> 00:04:45,480
written in logic in a couple of lines but in circuit form you can't actually

48
00:04:45,480 --> 00:04:49,800
write a correct definition of life and death at all you can only write finite

49
00:04:49,800 --> 00:04:55,640
approximations to it and the systems are not learning a very good approximation

50
00:04:55,640 --> 00:05:00,840
and so they are very vulnerable and this turns out to be applicable not just a

51
00:05:00,840 --> 00:05:05,360
category but to all the other leading go programs which are trained by completely

52
00:05:05,360 --> 00:05:09,600
different teams on completely different data using different training regimes

53
00:05:09,600 --> 00:05:16,400
but they all fail against this very simple strategy so this suggests that

54
00:05:16,400 --> 00:05:23,120
actually the systems that we have been building are we are overrating them in a

55
00:05:23,120 --> 00:05:29,520
real sense and I think that's important to understand and human beings right

56
00:05:29,520 --> 00:05:35,320
another way to make this argument is look at things that humans can do for

57
00:05:35,360 --> 00:05:41,480
example we can build the large interferometric gravitational observatory

58
00:05:41,480 --> 00:05:46,360
so these are black holes colliding on the other side of the universe this is

59
00:05:46,360 --> 00:05:54,400
the the LIGO detector and which is several kilometers long it's full of

60
00:05:54,400 --> 00:06:00,800
physics and is able to detect distortions of space down to 18 the 18th

61
00:06:00,880 --> 00:06:07,640
decimal place and was able to actually measure exactly what the physicists

62
00:06:07,640 --> 00:06:13,040
predicted would be the shape of the waveform arriving from the collision of

63
00:06:13,040 --> 00:06:16,840
two black holes and was even able to measure the masses of the black holes

64
00:06:16,840 --> 00:06:25,360
on the other side of the universe when they collided so could chat gpt do

65
00:06:25,400 --> 00:06:33,080
this could any deep learning system do this given that there are exactly zero

66
00:06:33,080 --> 00:06:40,200
training examples of a gravitational wave detector I think at the moment there

67
00:06:40,200 --> 00:06:48,640
is still a long way to go on the other hand people are extremely ingenious and

68
00:06:48,640 --> 00:06:53,440
people are working on hybrids of large language models with reasoning and

69
00:06:53,440 --> 00:06:59,080
planning engines that could start to exhibit these capabilities quite soon

70
00:06:59,080 --> 00:07:05,840
so people I respect a great deal think we might only have five years until this

71
00:07:05,840 --> 00:07:12,360
happens almost everyone has now gone from 30 to 50 years which was the

72
00:07:12,360 --> 00:07:19,840
estimate a decade ago to five to 20 years which is the estimate right now so

73
00:07:19,880 --> 00:07:24,880
unlike fusion this is getting closer and closer and closer rather than further

74
00:07:24,880 --> 00:07:29,560
and further into the future so we have to ask what happens if we actually

75
00:07:29,560 --> 00:07:36,760
succeed in creating general purpose AI and the reason we are trying to do it

76
00:07:36,760 --> 00:07:42,160
is because it could be so transformative to human civilization very crudely our

77
00:07:42,160 --> 00:07:46,560
civilization results from our intelligence if we have access to a lot

78
00:07:46,560 --> 00:07:51,440
more we could have a lot better civilization one thing we could do is

79
00:07:51,440 --> 00:07:57,520
simply deliver what we already know how to deliver which is a nice middle class

80
00:07:57,520 --> 00:08:00,880
standard living if you want to think of it that way we could deliver that to

81
00:08:00,880 --> 00:08:07,000
everyone on earth at almost no cost and that would be about a tenfold increase

82
00:08:07,160 --> 00:08:17,280
in GDP and the net present value of that is 13.5 quadrillion dollars so that's a

83
00:08:17,280 --> 00:08:23,200
lower bound on the cash value of creating general purpose AI so if you want

84
00:08:23,200 --> 00:08:26,880
to understand why we're investing hundreds of billions of pounds in it

85
00:08:26,880 --> 00:08:36,960
it's because the value is millions of times larger than that and so that creates

86
00:08:37,000 --> 00:08:43,320
a magnet in the future that is pulling us forward inexorably my friend Jan

87
00:08:43,320 --> 00:08:48,600
Tallinn here likes to call this mollock right this sort of ineluctable force

88
00:08:48,600 --> 00:08:54,040
that draws people towards something even though they know that it could be their

89
00:08:54,040 --> 00:08:59,840
own destruction and we could actually have an even better civilization right

90
00:08:59,840 --> 00:09:07,680
we could have we could one day have a clicker that works we could have health

91
00:09:07,680 --> 00:09:12,520
care that's a lot better than we do now we could have education that could be

92
00:09:12,520 --> 00:09:19,360
brought to every child on earth that would exceed what we can get from even a

93
00:09:19,360 --> 00:09:25,200
professional human tutor this I think is the thing that is most feasible for us

94
00:09:25,280 --> 00:09:30,280
to do that would benefit the world in this decade and I think this is entirely

95
00:09:30,280 --> 00:09:34,640
possible health care is actually a lot more difficult for all kinds of reasons

96
00:09:34,640 --> 00:09:41,160
but education is a digital good that can be delivered successfully and we could

97
00:09:41,160 --> 00:09:49,200
also have much better progress in science and so on so on the other hand AI

98
00:09:49,200 --> 00:09:55,080
amplifies a lot of difficult issues that policymakers have been facing for quite

99
00:09:55,080 --> 00:10:05,480
a while so one is its ability to magnify the pollution of our information

100
00:10:05,480 --> 00:10:11,960
ecosystem with disinformation what some people call truth decay and this is

101
00:10:11,960 --> 00:10:17,560
happening at speed but if we thought about it really hard I could actually

102
00:10:17,560 --> 00:10:21,960
help in the other direction it could help clean up the information ecosystem

103
00:10:21,960 --> 00:10:26,760
it could be used as a detector of misinformation as something that

104
00:10:26,760 --> 00:10:32,040
assembled consensus truth and made it available to people we're not using it

105
00:10:32,040 --> 00:10:38,600
in that way but we could ditto with democracy is it being suppressed by

106
00:10:38,600 --> 00:10:44,400
surveillance and control mechanisms or could we use AI systems to strengthen it

107
00:10:44,400 --> 00:10:51,720
to allow people to deliberate cooperate and reach consensus on what to do could

108
00:10:51,720 --> 00:10:58,040
it be that individuals are empowered or the current trajectory that we're on

109
00:10:58,040 --> 00:11:03,120
individuals being enfeebled as we gradually take over more and more of the

110
00:11:03,120 --> 00:11:09,040
functions of civilization and and humans lose the ability to even run their own

111
00:11:09,040 --> 00:11:15,480
civilization as individuals right these are important questions that we have to

112
00:11:15,480 --> 00:11:19,120
address while we're considering all of the safety issues that I'll be getting

113
00:11:19,120 --> 00:11:24,520
to soon there's inequality right now we're on the path of magnifying it with

114
00:11:24,520 --> 00:11:34,840
AI but it doesn't have to be that way and so on so let me I won't go through all

115
00:11:34,840 --> 00:11:39,600
of these issues because they're they're all each of them worthy of an entire

116
00:11:39,640 --> 00:11:51,360
talk in themselves so the I would say the sort of the mid term question is what

117
00:11:51,360 --> 00:11:56,760
are humans going to be doing right if we have general purpose AI that can do all

118
00:11:56,760 --> 00:12:02,800
the tasks or nearly all the tasks that human beings get paid for right now what

119
00:12:02,800 --> 00:12:08,480
will humans do and this is not a new issue Aristotle talked about it in 350

120
00:12:08,480 --> 00:12:15,080
BC Keynes since we're in Milton it's odd that we pronounce it milk Milton Keynes

121
00:12:15,080 --> 00:12:21,080
but he his name is pronounced Keynes even though the town is named after him so so

122
00:12:21,080 --> 00:12:25,600
Keynes in 1930 said thus for the first time since his creation man will be

123
00:12:25,600 --> 00:12:30,240
faced with Israel his permanent problem how to use his freedom from pressing

124
00:12:30,240 --> 00:12:34,400
economic cares which science will have one for him to live wisely and agreeably

125
00:12:34,400 --> 00:12:40,640
and well so this is a really important problem and again this is one that

126
00:12:40,640 --> 00:12:47,960
policymakers are misunderstanding I would say that the default answer in most

127
00:12:47,960 --> 00:12:54,120
governments around the world is will retrain everyone to be a data scientist

128
00:12:54,120 --> 00:12:58,960
as if somehow the world needs three and a half four billion data scientists I

129
00:12:58,960 --> 00:13:04,880
think that's probably not the answer but this is again you know the default path

130
00:13:04,880 --> 00:13:12,520
is one of enfeeblement which is illustrated really well by by Wally so

131
00:13:12,520 --> 00:13:19,240
my my answer to this question is that in the future if we are successful in

132
00:13:19,240 --> 00:13:25,240
building AI that is safe that does a lot of the tasks that we want done for us

133
00:13:25,320 --> 00:13:30,600
most human beings are going to be in these interpersonal roles and for those

134
00:13:30,600 --> 00:13:37,720
roles to be effective they have to be based on understanding right why is a

135
00:13:37,720 --> 00:13:43,440
surgeon effective at fixing a broken leg because we have done centuries of

136
00:13:43,440 --> 00:13:50,960
research in medicine and surgery to make that a very effective and in some

137
00:13:51,000 --> 00:13:56,880
countries very highly paid and very prestigious but most into personal roles

138
00:13:56,880 --> 00:14:02,160
for example think about childcare or elder care not highly paid not highly

139
00:14:02,160 --> 00:14:06,600
prestigious because they are based on no science whatsoever despite the fact

140
00:14:06,600 --> 00:14:10,760
that our children are our most precious possessions as people politicians like

141
00:14:10,760 --> 00:14:16,200
to say a lot in fact we don't understand how to look after and we don't

142
00:14:16,200 --> 00:14:21,000
understand how to make people's lives better so this is a a very different

143
00:14:21,000 --> 00:14:26,280
direction for science much more focused on the human than on the physical

144
00:14:26,280 --> 00:14:38,920
world okay so now let me move on if I can get the next light up to to Alan

145
00:14:38,920 --> 00:14:45,080
Shuring's view of all this what happens if we succeed he said that it seems

146
00:14:45,080 --> 00:14:48,920
parable the ones machine thinking method has started it would not take long to

147
00:14:48,920 --> 00:14:54,040
outstrip our feeble powers at some stage therefore we should have to expect the

148
00:14:54,040 --> 00:15:02,680
machines to take control so he said this in 1951 and to a first approximation for

149
00:15:02,680 --> 00:15:10,280
the next 70 odd years we paid very little attention to what his warning was

150
00:15:11,160 --> 00:15:16,520
and I used to illustrate this with the following imaginary email conversation

151
00:15:18,600 --> 00:15:25,480
so an alien civilization sends email to the human race humanity at un.org be

152
00:15:25,480 --> 00:15:30,040
warned we shall arrive in 30 to 50 years that was what most AI people thought back

153
00:15:30,040 --> 00:15:37,720
then now we would say maybe 10 to 20 years and humanity replies humanity is

154
00:15:37,720 --> 00:15:41,400
currently out of the office who will respond to your message when we return

155
00:15:42,520 --> 00:15:49,080
and then there should be a smiley face there it is okay so that's now changed

156
00:15:51,000 --> 00:15:54,840
unfortunately that slide wasn't supposed to come up like that let me see if we can

157
00:15:54,840 --> 00:16:00,600
oh well can't fix it now so I think early on this year three things happened in

158
00:16:00,600 --> 00:16:06,120
very quick succession so GPT-4 was released and then Microsoft which had been working

159
00:16:06,120 --> 00:16:11,720
with GPT-4 for several months at that point published a paper saying that GPT-4

160
00:16:11,720 --> 00:16:17,320
exhibited sparks of artificial general intelligence exactly what Turing warned us about

161
00:16:18,440 --> 00:16:25,240
and then FLI released the open letter asking for a pause on giant AI experiments

162
00:16:26,200 --> 00:16:33,800
and I think at that point very clearly humanity returned to the office and they saw the emails

163
00:16:33,800 --> 00:16:40,920
from the aliens and the reaction since then I think has been somewhat similar to what would

164
00:16:40,920 --> 00:16:48,120
happen if we really did get an email from the aliens there have been a global calls for action

165
00:16:48,120 --> 00:16:54,840
the very next day UNESCO responded directly to the open letter asking all its member governments

166
00:16:54,840 --> 00:17:01,400
which is all the countries on earth to immediately implement the AI principles in legislation

167
00:17:02,120 --> 00:17:07,560
in particular principles that talk about robustness safety predictability and so on

168
00:17:08,760 --> 00:17:15,320
and then you know there's China's AI regulations the US got into the act very quickly the White

169
00:17:15,320 --> 00:17:23,560
House called emergency meeting of AI CEOs open AI calling for governments to regulate AI and so on

170
00:17:24,120 --> 00:17:29,240
and I ran out of room on the slide on June 7th with Rishi Sunak announcing the global summit on

171
00:17:29,320 --> 00:17:35,160
AI safety which is happening tomorrow so lots of other stuff has happened since then

172
00:17:36,040 --> 00:17:42,360
but it's really I would say to to the credit of governments around the world how quickly

173
00:17:43,080 --> 00:17:50,680
they have changed their position on this for the most part governments were saying you know

174
00:17:50,680 --> 00:17:59,080
regulation stifles innovation you know if someone did mention risk it was either dismissed

175
00:17:59,720 --> 00:18:05,720
or or viewed as something that was easily taken care of by the market by liability and so on

176
00:18:06,360 --> 00:18:12,280
so I would say that the the the view the understanding has changed dramatically and that could not have

177
00:18:12,280 --> 00:18:21,400
happened without the fact that politicians started to use chat GBT and they saw it for themselves

178
00:18:22,440 --> 00:18:25,000
and I think that changed people's minds

179
00:18:27,720 --> 00:18:36,120
so the question we have to face then is this one right how do we retain power over entities more

180
00:18:36,120 --> 00:18:41,240
powerful than ourselves forever right and I think this is the question that Turing asked himself

181
00:18:42,840 --> 00:18:46,680
and gave that answer we would have to expect them to take control so in other words

182
00:18:47,400 --> 00:18:53,000
this question doesn't have an answer but I think there's another version of the question

183
00:18:53,000 --> 00:18:56,520
which works somewhat more to our advantage

184
00:18:58,920 --> 00:19:07,720
it should appear any second if right and it has to do with how we define what we're trying to do

185
00:19:08,280 --> 00:19:16,440
what is the system that we're building what problem is it solving and we want a problem

186
00:19:16,440 --> 00:19:22,680
such that we we set up an AI system to solve that problem so the standard model that I gave you

187
00:19:22,680 --> 00:19:29,640
earlier was systems whose actions can be expected to achieve their objectives and that's exactly

188
00:19:29,640 --> 00:19:35,480
where things go wrong that systems are pursuing objectives that are not aligned with what humans

189
00:19:35,480 --> 00:19:40,360
want the future to be like and then you're setting up a chess match between humanity

190
00:19:40,360 --> 00:19:47,720
and a machine that's pursuing a misaligned objective so instead we want to figure out

191
00:19:48,520 --> 00:19:55,800
a problem whose solution is such that we're happy for AI systems to instantiate that solution

192
00:19:57,240 --> 00:20:04,200
okay and it's not imitating human behavior which is what we're training LLMs to do that's actually

193
00:20:04,200 --> 00:20:13,000
the fundamental and basic error and that's essentially why we can't make LLMs safe

194
00:20:13,800 --> 00:20:17,160
because we have trained them to not be safe and trying to put

195
00:20:19,480 --> 00:20:24,920
trying to put sticking plasters on all all the problems after the fact is never going to work

196
00:20:25,800 --> 00:20:31,160
so instead I think we have to build systems that are provably beneficial to humans and

197
00:20:31,720 --> 00:20:37,320
the way I'm thinking about that currently is that the system should act in the best interests of

198
00:20:37,320 --> 00:20:45,640
humans but be explicitly uncertain about what those best interests are and this this I'm just

199
00:20:45,640 --> 00:20:52,120
telling you in English and it can be written in a formal framework called an assistance game

200
00:20:52,840 --> 00:21:00,280
so what we do is we build assistance game solvers we don't build objective maximizers

201
00:21:00,280 --> 00:21:05,480
which is what we have been doing up to now we build assistance game solvers this is a different

202
00:21:05,480 --> 00:21:10,440
kind of AI system when we've only been able to build very simple ones so far so we have a long

203
00:21:10,440 --> 00:21:17,320
way to go but when you build those systems and look at the solutions they exhibit the properties

204
00:21:17,320 --> 00:21:23,000
that we want from AI systems they will defer to human beings and in the extreme case they will

205
00:21:23,000 --> 00:21:28,280
allow themselves to be switched off in fact they want to be switched off if we want to switch them

206
00:21:28,280 --> 00:21:34,040
off because they want to avoid doing whatever it is that is making us upset they don't know what it is

207
00:21:34,040 --> 00:21:39,480
because they're uncertain about our preferences but they want to avoid upsetting us and so they

208
00:21:39,480 --> 00:21:44,680
are happy to be switched off in fact this is a mathematical theorem they have a positive

209
00:21:44,680 --> 00:21:51,080
incentive to allow themselves to be switched off and that incentive is connected directly

210
00:21:51,080 --> 00:21:54,280
to number two right the uncertainty about human preferences

211
00:21:56,360 --> 00:22:03,000
so there's a long way to go as I said and we're not ready to say okay everyone in all these

212
00:22:03,000 --> 00:22:08,200
companies stop doing what you're doing and start building these things instead right that probably

213
00:22:08,200 --> 00:22:12,520
is not going to go down too well because we don't really know how to build these things at scale

214
00:22:13,400 --> 00:22:18,920
and to deliver economic value but in the long run this is the right way to build AI systems

215
00:22:19,400 --> 00:22:25,880
so in between what should we do and this is a lot about what's going to be discussed tomorrow

216
00:22:27,800 --> 00:22:31,720
and there's a lot so this is in a small font I apologize to those of you at the back there's

217
00:22:31,720 --> 00:22:38,680
a lot to put on this slide we need first of all a cooperation on AI safety research it's got to

218
00:22:38,680 --> 00:22:45,240
stop being a cottage industry with a few little academic centers here and there it's also got to

219
00:22:45,240 --> 00:22:53,240
stop being what a cynic might describe as a kind of whitewashing operation in companies

220
00:22:53,240 --> 00:22:59,400
where they try to avoid the worst public relations disasters like you know the language model used

221
00:22:59,400 --> 00:23:07,960
a bad word or something like that but in fact those efforts have not yielded any real safety

222
00:23:07,960 --> 00:23:13,640
whatsoever so there's a great deal of research to do on alignment which is what I just described

223
00:23:13,640 --> 00:23:19,240
on containment how do you get systems that are restricted in their capabilities that are not

224
00:23:19,800 --> 00:23:24,920
directly connected to email and bank accounts and credit cards and social media and all those

225
00:23:24,920 --> 00:23:31,960
things and if there are I think there are probably ways of building restricted capability systems

226
00:23:32,600 --> 00:23:37,960
that are provably safe because they are restricted to only operate

227
00:23:38,200 --> 00:23:47,400
uh provably sound reasoning engines for example um but the the bigger point is stop thinking about

228
00:23:47,400 --> 00:23:58,680
making AI safe start thinking about making safe AI right these are just two different mindsets

229
00:23:58,680 --> 00:24:04,840
the making AI safe says we build the AI and then we have a safety team whose job it is

230
00:24:04,920 --> 00:24:10,680
to stop it from behaving badly that hasn't worked and it's never going to work we have

231
00:24:10,680 --> 00:24:16,680
got to have AI systems that are safe by design and without that we are lost

232
00:24:19,400 --> 00:24:26,040
we also need I think some international regulatory level to coordinate the regulations

233
00:24:26,040 --> 00:24:31,800
that are going to be in place across the various national regimes so we have to start

234
00:24:32,360 --> 00:24:38,600
probably with national regulation but and we can coordinate very easily for example we could

235
00:24:38,600 --> 00:24:46,520
start coordinating tomorrow to agree on what would be a baseline for regulation I put a couple of

236
00:24:46,520 --> 00:24:57,560
other things there that went by too quickly so I actually want to go back okay too far all right

237
00:24:58,040 --> 00:25:05,560
um so the the light blue line transparent explainable analytical substrate is really

238
00:25:05,560 --> 00:25:11,800
important uh the moment we're building AI systems that are black boxes we have no idea how they work

239
00:25:11,800 --> 00:25:16,760
we have no idea what they're going to do and we have no idea how to get them to behave themselves

240
00:25:16,760 --> 00:25:26,600
properly uh so my guess is that if we define regulations appropriately so that companies

241
00:25:27,320 --> 00:25:34,040
have to build AI systems that they understand and predict and control successfully those

242
00:25:34,040 --> 00:25:40,200
AI systems are going to be based on a very different technology not giant black box circuits

243
00:25:40,200 --> 00:25:47,000
that are trained on vast quantities of data but actually well understood component based

244
00:25:47,000 --> 00:25:53,320
systems that build on centuries of research in logic and probability where we can actually

245
00:25:53,320 --> 00:25:59,720
prove that these systems are going to behave in certain ways the second thing the dark blue

246
00:25:59,720 --> 00:26:05,400
secure PCC based digital ecosystem what is that uh so PCC is proof carrying code

247
00:26:05,640 --> 00:26:12,120
and what we need here is a way of preventing bad actors from deploying unsafe systems so it's one

248
00:26:12,120 --> 00:26:18,120
thing to say here's how you build safe systems and everyone has to do that it's another thing to

249
00:26:18,120 --> 00:26:24,200
say how do you stop people from deploying unsafe systems who don't want safe AI systems they want

250
00:26:24,200 --> 00:26:32,600
whatever they want this is probably even more difficult policing software is I think impossible

251
00:26:32,600 --> 00:26:41,080
is I think impossible so the the place where we do have control is at the hardware level because

252
00:26:41,080 --> 00:26:46,440
hardware uh first of all to build your own hardware costs about a hundred billion dollars

253
00:26:47,080 --> 00:26:54,520
and tens of thousands of highly trained engineers so it provides a control point that's very difficult

254
00:26:54,520 --> 00:27:00,280
for bad actors to get around and what the hardware should do is basically check the proof

255
00:27:01,080 --> 00:27:03,400
of a software object before it's run

256
00:27:05,560 --> 00:27:08,920
and check that in fact this is a safe piece of software to run

257
00:27:09,880 --> 00:27:15,560
and proof carrying code is a technology that allows hardware to check proofs very efficiently

258
00:27:16,280 --> 00:27:21,640
but of course the owners then is on the developer to provide a proof that their system is in fact

259
00:27:21,640 --> 00:27:30,920
safe and so that's a prerequisite for this approach okay let me talk a little bit about

260
00:27:30,920 --> 00:27:41,960
regulations so a number of acts already in in the in the works for example the european AI act

261
00:27:42,680 --> 00:27:48,200
has a hard ban on the impersonation of human beings so you have a right to know if you're

262
00:27:48,200 --> 00:27:53,080
interacting with a machine or a human this to me is the easiest the lowest hanging fruit

263
00:27:53,800 --> 00:27:59,960
that every jurisdiction in the world could implement pretty much tomorrow if they so decided

264
00:28:00,680 --> 00:28:08,280
and I believe that this is how legislators wake up those long unused muscles that have

265
00:28:08,280 --> 00:28:14,120
lain dormant for decades while technology has just moved ahead unregulated so this is the place to

266
00:28:14,200 --> 00:28:23,080
start but we also need some regulations on the design of AI systems specifically so

267
00:28:23,800 --> 00:28:29,800
a provably operable kill switch is a really important and basic thing if your system is

268
00:28:29,800 --> 00:28:36,600
misbehaving there has to be a way to turn it off and this has to apply not just to the system that

269
00:28:36,600 --> 00:28:43,320
you made but if it's an open source system any copy of that system and that means that the kill

270
00:28:43,320 --> 00:28:48,440
switch has got to be remotely operable and it's got to be non-removable

271
00:28:51,400 --> 00:28:55,880
so that's a technological requirement on open source systems and in fact if you want to be in

272
00:28:55,880 --> 00:29:01,000
the open source business you're going to have to figure this out you're actually going to subject

273
00:29:01,000 --> 00:29:08,680
yourself to more regulatory controls than people who operate on closed source and that's exactly

274
00:29:08,680 --> 00:29:15,720
as it should be imagine if we had open source enriched uranium right and the purveyor of enriched

275
00:29:15,720 --> 00:29:21,800
uranium was responsible for all the enriched uranium that they pervade to anybody around the

276
00:29:21,800 --> 00:29:27,080
world they're going to have a higher regulatory burden because that's a blinking stupid thing to

277
00:29:27,080 --> 00:29:32,280
do right and so you would expect there to be a higher burden if you're going to do blinking stupid

278
00:29:32,280 --> 00:29:39,960
things um and then red lines this is probably the most important thing so we don't know how to

279
00:29:39,960 --> 00:29:46,360
define safety so i can't write a law saying your system has to be provably safe because it's very

280
00:29:46,360 --> 00:29:53,000
hard to write the dividing line between safe and unsafe you know if you uh asimov's law you can't

281
00:29:53,800 --> 00:30:00,520
harm human beings well what does harm mean that's very hard to define but we can scoop out

282
00:30:01,320 --> 00:30:08,280
very specific forms of harm that are absolutely unacceptable so self-replication of computer

283
00:30:08,280 --> 00:30:14,040
systems would absolutely be unacceptable that would be basically a harbinger of losing human

284
00:30:14,040 --> 00:30:20,840
control if the system can copy itself onto other computers or break into other computer systems

285
00:30:21,480 --> 00:30:26,120
absolutely systems should not be advising terrorists on building biological weapons

286
00:30:26,120 --> 00:30:31,400
and so on so these red lines are things that any normal person would think well obviously

287
00:30:33,000 --> 00:30:39,480
the software system should not be doing that and the developers are going to say oh well this is

288
00:30:39,480 --> 00:30:43,880
really unfair because it's really hard to make our systems not do this

289
00:30:46,040 --> 00:30:54,040
and their response is well tough right really you're spending hundreds of billions of pounds

290
00:30:54,040 --> 00:30:58,840
on this system and you can't stop it from advising terrorists on building bioweapons

291
00:30:59,400 --> 00:31:07,640
well then you shouldn't be in business at all right this is not hard and legislatures by uh by

292
00:31:07,640 --> 00:31:14,600
implementing these red lines would put the onus on the developer to understand how their own systems

293
00:31:14,600 --> 00:31:20,520
work and to be able to predict and control their behavior which is an absolute minimum we should

294
00:31:20,520 --> 00:31:28,440
ask from any industry let alone one that could have such a massive impact and is hoping for

295
00:31:28,440 --> 00:31:40,600
quadrillions of dollars in profits thank you thank you very much professor russell quick

296
00:31:40,600 --> 00:31:46,200
question maybe before we move on to the next speaker there was some good news in there it is

297
00:31:46,200 --> 00:31:51,000
that we have ideas on how to make safe ai but how long do you think we're going to need

298
00:31:51,640 --> 00:31:55,240
how long is it going to take by default that we have these ideas worked out and how long

299
00:31:55,240 --> 00:32:00,920
might it take if we had all the smart people in the world give up their current focus and instead

300
00:32:00,920 --> 00:32:08,840
work on this uh i think these are really important questions because the um the political dynamic is

301
00:32:08,840 --> 00:32:14,760
going to depend to some extent on how the ai safety community responds to this challenge

302
00:32:14,760 --> 00:32:21,000
uh because if the ai safety community fails to make progress on any of this stuff the developers

303
00:32:21,000 --> 00:32:27,240
can point and say look you know you guys are asking for stuff that isn't really possible

304
00:32:27,240 --> 00:32:32,600
and we should be allowed to just do what we want um but if you look at the nuclear industry right

305
00:32:32,600 --> 00:32:39,160
how does that work the regulator says to the nuclear plant operator show me that your plant

306
00:32:39,160 --> 00:32:45,960
has a meantime to failure of 10 million years or more and the operator has to give them a full

307
00:32:45,960 --> 00:32:53,240
analysis with fault trees and a probabilistic uh calculations and the regulator can push back and

308
00:32:53,240 --> 00:32:57,720
say you know i don't agree with that independence assumption you know these components come from

309
00:32:57,720 --> 00:33:02,920
the same manufacturer so not independent and come back with a better analysis and so on

310
00:33:03,560 --> 00:33:10,840
uh at the moment there is nothing like that in the ai industry there is no logical connection

311
00:33:11,560 --> 00:33:16,840
between any of the evidence that people are providing and the claim that the system is

312
00:33:16,840 --> 00:33:25,160
actually going to be safe right that argument is just missing um now the nuclear industry

313
00:33:25,560 --> 00:33:33,080
probably spends more than 90 percent of its r&d budget on safety

314
00:33:35,080 --> 00:33:39,880
one way you can tell i i got the statistic from one of my nuclear engineering colleagues

315
00:33:40,440 --> 00:33:46,680
that for the typical nuclear plant in the u.s for every kilogram of nuclear plant

316
00:33:47,240 --> 00:33:52,120
there are seven kilograms of regulatory paperwork i kid you not

317
00:33:52,200 --> 00:34:00,840
so that tells you something about how much of an emphasis that has been on safety in that industry

318
00:34:02,360 --> 00:34:09,880
and also you know why is there to a first approximation no nuclear industry today

319
00:34:10,520 --> 00:34:17,800
is because of Chernobyl and because of a failure in safety actually deliberately bypassing

320
00:34:18,760 --> 00:34:22,600
safety measures that they knew were necessary in order to save money

321
00:34:23,720 --> 00:34:27,080
we'll take one question from the audience provided it's a quick question

322
00:34:27,800 --> 00:34:32,440
i see a hand over there let me dash down

323
00:34:36,040 --> 00:34:40,280
hi uh thanks very much for your talks here um my name is Charlie i'm a senior at UCL

324
00:34:40,920 --> 00:34:45,400
um one of the big reasons i think why there's so much regulation on nuclear power

325
00:34:45,880 --> 00:34:52,360
is widespread public opinion and protests against nuclear power from within the environmental

326
00:34:52,360 --> 00:34:58,040
movement so i wondered whether you uh thought if there's a similar role for public pressure or

327
00:34:58,040 --> 00:35:06,520
protests uh for ai as well thanks uh i think that's a very important question

328
00:35:07,160 --> 00:35:18,360
my sense is i i'm not really historian of the nuclear industry per se uh obviously nuclear

329
00:35:18,360 --> 00:35:24,360
physicists thought about safety from the beginning uh in fact so Leo Leo Zillard was the one who

330
00:35:24,920 --> 00:35:30,840
invented the basic idea of the nuclear chain reaction uh and he instantly thought about

331
00:35:30,840 --> 00:35:35,640
a physical mechanism that could keep the reaction from going supercritical and becoming a bomb

332
00:35:36,360 --> 00:35:39,880
right so he thought about this you know negative feedback control system

333
00:35:40,680 --> 00:35:45,560
with moderators that would somehow keep the reaction subcritical

334
00:35:48,040 --> 00:35:54,760
people in ai are not at that stage right or they just have their eyes on you know we can

335
00:35:54,760 --> 00:35:58,760
generate energy and they're not even thinking you know is that energy going to be in the form

336
00:35:58,760 --> 00:36:04,680
of a bomb or electricity right they haven't got to that stage yet so we are very much at the

337
00:36:04,680 --> 00:36:17,080
preliminary stage i do worry that ai should not be politicized and at the moment there's

338
00:36:17,080 --> 00:36:24,680
a precarious bipartisan agreement in the us and to some extent in europe i worry about that

339
00:36:24,680 --> 00:36:31,400
breaking down in the uk uh i think it's really important that the political message be very

340
00:36:31,400 --> 00:36:38,200
straightforward you can be on the side of humans or you can be on the side of our ai overlords

341
00:36:39,560 --> 00:36:48,520
which do you want to be on um and so let's try to keep it a unified message around uh

342
00:36:49,720 --> 00:36:56,200
developing technology in a way that's safe and beneficial for humans um so we can raise

343
00:36:56,760 --> 00:37:04,520
but we shouldn't do it in a partisan way yes and and what i i i totally sympathize with the idea

344
00:37:04,520 --> 00:37:11,240
that people have a right to be very upset that you know that multibillionaires are playing

345
00:37:12,600 --> 00:37:17,720
you know playing poker with the future of the human race um it's entirely reasonable

346
00:37:17,960 --> 00:37:28,600
but what i worry is exactly that uh certain types of of protest end up getting aligned

347
00:37:29,320 --> 00:37:36,840
in a way that's unhealthy uh it sort of becomes anti technology and we can look back at what happened

348
00:37:36,840 --> 00:37:45,640
with with uh gm uh organisms for example uh which which most scientists think didn't go the way

349
00:37:46,360 --> 00:37:51,080
uh it should have and we we lost benefits uh without gaining any safety

350
00:37:52,600 --> 00:37:55,320
watch to think about there thank you very much professor stewart russell

351
00:37:58,120 --> 00:38:01,800
we may give you the microphone again a bit later on but there's lots of other people we want to

352
00:38:01,800 --> 00:38:08,840
hear from now so the next speaker is kona lehi who is the ceo of conjecture many of us got a shock

353
00:38:08,840 --> 00:38:16,280
with gp24 or 3.5 my goodness what's going on here kona was ahead of the curve when he saw gpg2

354
00:38:16,280 --> 00:38:21,080
with all its warts and weaknesses he said my gosh this is going to change the world so he has been

355
00:38:21,080 --> 00:38:25,880
thinking about some of these issues probably for longer than the rest of us so let's hear from kona

356
00:38:25,880 --> 00:38:37,240
what would you like to say thank you so much so unfortunately uh professor russell has stole

357
00:38:37,240 --> 00:38:42,840
my favorite allen turing quote so you're going to be hearing that one again but i guess there couldn't

358
00:38:42,840 --> 00:38:51,800
be a more appropriate time because many years ago there lived a man named allen turing he was the

359
00:38:51,800 --> 00:39:00,680
godfather of computer science a titan in his field and a hero of world war two and was here

360
00:39:00,680 --> 00:39:06,680
at bletchley park that he did his most seminal work during the world war two and cracking the

361
00:39:06,680 --> 00:39:13,000
codes that the germans were using and as a very early step into the field of computer science

362
00:39:15,160 --> 00:39:24,760
and allen was ahead of his time in more way than one in 1951 in manchester he gave a lecture

363
00:39:24,760 --> 00:39:36,200
entitled intelligent machinery a heretical theory and in this lecture he said for it seems probable

364
00:39:36,200 --> 00:39:41,000
but once the machine thinking method had started it would not take long to outstrip

365
00:39:41,000 --> 00:39:46,120
our feeble powers there would be no question of the machine's dying and it would be able to

366
00:39:46,120 --> 00:39:52,920
converse with each other to sharpen their wits at some stage therefore we should have to expect

367
00:39:52,920 --> 00:40:01,880
the machines to take control and here we are 72 years later where it all began

368
00:40:02,520 --> 00:40:10,760
and a lot has changed since the days of allen turing computers have improved in incredible rates

369
00:40:11,400 --> 00:40:18,120
i'm holding my hands right now a computer of such incredible power that it would be barely

370
00:40:18,120 --> 00:40:27,240
imaginable to turing in his contemporaries barely one human lifetime hints and while computers have

371
00:40:27,240 --> 00:40:33,080
advanced a lot in many ways since the days of turing i like to believe that there would be a lot

372
00:40:33,080 --> 00:40:42,760
he would recognize he would recognize the basic functions of computers their memory their instructions

373
00:40:42,760 --> 00:40:48,680
programming code ideas that go all the way back to his seminal work on turing machines

374
00:40:49,400 --> 00:40:54,360
while he might not be familiar with the exact tooling he would be familiar with the general

375
00:40:54,360 --> 00:41:02,120
concepts around modern programming where a programmer writes code instructions the computer

376
00:41:02,120 --> 00:41:09,800
then executes but there is something that i'm not so sure he would so easily recognize

377
00:41:11,400 --> 00:41:21,160
and that is a i or in particular the neural networks that power them now we have all seen

378
00:41:21,160 --> 00:41:26,760
ai do truly amazing and things over the last couple of years in particular solving all these

379
00:41:26,760 --> 00:41:32,600
problems that previously we barely knew how to approach and you might think when you look at

380
00:41:32,600 --> 00:41:38,360
all these ai systems running on your phone on your computer that this is software like any other

381
00:41:38,360 --> 00:41:43,480
written by very clever programmers to do the useful and the marvelous things that they do

382
00:41:44,440 --> 00:41:52,120
and you would be wrong because ai is very different from normal software

383
00:41:53,160 --> 00:41:59,480
it is not written so much as it is grown so while in the traditional software you'd have

384
00:41:59,480 --> 00:42:07,640
a programmer sit down and write out the instruction with ai's you take huge supercomputers and massive

385
00:42:07,800 --> 00:42:15,160
data sets and you use these supercomputers to grow a program on your data to solve your problem

386
00:42:16,200 --> 00:42:22,040
and this works really well for many for many issues it has improved our ability to solve many

387
00:42:22,040 --> 00:42:28,360
very useful tasks and do many things that we did not know how to do before and our ability to

388
00:42:28,920 --> 00:42:34,520
grow these ai's continues to improve and get better and better while at the same time though

389
00:42:34,520 --> 00:42:42,840
our ability to understand our ai's has not because these ai's are not like well written code

390
00:42:42,840 --> 00:42:49,960
that a human could read they're more like giant blobs of numbers and we know if we execute them

391
00:42:49,960 --> 00:43:02,280
they work but we have no idea why and only quite recently have we discovered that as we scale up

392
00:43:02,280 --> 00:43:10,280
these systems and as we build bigger computers and bigger ai systems something quite remarkable

393
00:43:10,280 --> 00:43:20,600
happens they become more intelligent more capable now of course there are many details

394
00:43:20,600 --> 00:43:25,720
that have to be gotten right there are many parameters you have to set correctly you have

395
00:43:25,720 --> 00:43:29,800
to have enough data you have to make sure your computer is set up correctly but fundamentally

396
00:43:30,760 --> 00:43:36,120
is a stability in this prediction sometimes also called the scaling laws and that as our

397
00:43:36,120 --> 00:43:42,040
systems become bigger as our computers become more powerful the systems learn higher and

398
00:43:42,040 --> 00:43:52,600
higher order patterns more and more complex skills knowledge abilities and as they become more powerful

399
00:43:52,600 --> 00:43:59,400
and more capable they are also becoming even harder to understand and to control

400
00:44:01,960 --> 00:44:08,760
and this is why we are all here today back to where it all began we have now returned

401
00:44:09,400 --> 00:44:18,840
Bletchley Park because as Turingell really realized it is really quite simple if we build

402
00:44:18,840 --> 00:44:27,560
machines that are more competent than us at manipulation perception politics business science

403
00:44:28,200 --> 00:44:36,040
and everything else and we do not control them then the future will belong to the machines

404
00:44:37,400 --> 00:44:44,120
not to humans and the machines are unlikely to feel particularly sentimental about keeping us

405
00:44:44,120 --> 00:44:52,600
around for very long and so here we are face with an exponentially increasing more powerful

406
00:44:52,600 --> 00:44:59,880
by the day ai by the day as we learned with covid there are exactly two times one can react

407
00:44:59,880 --> 00:45:09,480
to an exponential too early or too late if we wait until agi if we wait until we see the

408
00:45:09,480 --> 00:45:18,920
self-improving powerful general purpose systems it will be too late far far too late and this is

409
00:45:18,920 --> 00:45:25,480
why i am so happy to see the uk government take leadership in the first of many important steps

410
00:45:25,480 --> 00:45:32,440
towards the necessary international coordination to address this extinction level threat that is

411
00:45:32,440 --> 00:45:41,240
facing us all and the very first step as so many academics industry leaders and even governments

412
00:45:41,240 --> 00:45:49,560
have already taken is to firmly acknowledge the reality of what we face the potential extinction

413
00:45:49,560 --> 00:45:59,080
of our species by ai private ai companies are scaling their ai systems as we speak

414
00:45:59,880 --> 00:46:06,920
and they will not ask for permission and they will not stop unless we make them they are already

415
00:46:06,920 --> 00:46:13,960
lobbying our governments for with ineffective policies such as responsible scaling in attempt

416
00:46:13,960 --> 00:46:21,240
to prevent actually effective policy like the oil ceo is a past trying to lobby against climate

417
00:46:21,240 --> 00:46:27,880
change regulation that would hurt their bottom lines the good news is that is not yet too late

418
00:46:28,520 --> 00:46:34,360
to stop this to prevent the building of such deadly machines until we know how to build them

419
00:46:34,360 --> 00:46:42,360
safely that is why there is nothing more important than for people to know the truth

420
00:46:43,480 --> 00:46:50,760
that a small group of unelected unaccountable private companies are running a deadly experiment

421
00:46:50,760 --> 00:46:59,320
on you on your family and on everyone on earth without your consent or even knowledge despite

422
00:46:59,320 --> 00:47:10,520
they themselves admitting that these risks are real at this point all of us agree that there is

423
00:47:10,520 --> 00:47:16,760
that we are playing Russian roulette with the entire planet and we're only quibbling about

424
00:47:16,760 --> 00:47:22,440
how many poles are left until the bullet now in my personal opinion if you ever find yourself

425
00:47:22,440 --> 00:47:31,000
playing Russian roulette i suggest you put down the gun and so we have to speak up and demand

426
00:47:31,000 --> 00:47:39,480
action if you want a future for us our children and our species it's not yet too late but it will

427
00:47:39,480 --> 00:47:46,040
be soon we stand at a historic moment today at where it all began

428
00:47:47,080 --> 00:47:50,200
let's leave park well let's not waste it

429
00:47:58,680 --> 00:48:01,880
thank you corner let's take a couple of questions from the audience

430
00:48:03,400 --> 00:48:07,880
where am i seeing there's one at yeah if you take the microphone in there thanks

431
00:48:09,720 --> 00:48:16,440
hi very nice presentation by the way i wanted to ask

432
00:48:18,200 --> 00:48:23,160
with the current situation that's going on with ai currently do you really think if you were to be

433
00:48:23,160 --> 00:48:29,320
a philosopher maybe for five minutes do you really think that currently society is really ready for

434
00:48:29,320 --> 00:48:37,560
it i mean sure we can adapt in somewhat but as things are uh dripping around us it doesn't seem

435
00:48:37,640 --> 00:48:44,840
like we're really anywhere near to i mean accepting it we're just a such fair and all

436
00:48:45,560 --> 00:48:52,440
and other factors coming yes so the simple answer is no we are absolutely ready

437
00:48:53,240 --> 00:48:58,360
we should be playing with nuclear fire or worse our civilization does not have the level of maturity

438
00:48:58,360 --> 00:49:02,200
to be able to handle technology like this and this is why i'm not extremely optimistic about the

439
00:49:02,200 --> 00:49:08,040
future the truth is is that whether it's ai or something else ai technology is becoming more

440
00:49:08,040 --> 00:49:13,320
and more powerful this is just how it is this is how the technology works and our society

441
00:49:13,320 --> 00:49:19,880
has to adapt to this if we as a society do not find a way to as an entire civilization as an

442
00:49:19,880 --> 00:49:26,600
international civilization work together in a way that we can responsibly steward technology so powerful

443
00:49:27,240 --> 00:49:33,800
that it can destroy anything then humanity is on a timer whether it's ai or whatever comes after

444
00:49:33,800 --> 00:49:41,720
that we need to improve our society or that's it sometimes people grow up in a hurry sometimes

445
00:49:41,720 --> 00:49:46,120
people are a bit childish and suddenly there's a big threat ahead and my goodness we grow up

446
00:49:46,120 --> 00:49:50,840
is that what you see happening with humanity now we're not ready for ai but as we understand the

447
00:49:50,840 --> 00:49:57,880
risks we will change our mode of operation i sure hope so and if it happens it will not happen as

448
00:50:00,280 --> 00:50:04,280
because something about it is because people like the people in this room actually do something

449
00:50:04,280 --> 00:50:09,560
about it stand up and make a difference in our institutions and our society it is there is no

450
00:50:09,560 --> 00:50:14,760
law of physics that forbids us from having a good future and taking control of our future

451
00:50:14,760 --> 00:50:20,520
and building wonderful safe technology for all but there is also no law that mandates it

452
00:50:20,920 --> 00:50:27,560
i saw one more hand up yes if you take give the mic to the the woman in the glasses thank you

453
00:50:32,120 --> 00:50:37,400
oh and i was gonna say i know that one of your policies is that you want to cap compute and

454
00:50:37,400 --> 00:50:41,400
and i'm just wondering whether you are gonna suggest that at the summit and what you think

455
00:50:41,400 --> 00:50:47,800
the government's response to that will be compute caps are absolutely the most sensible direct

456
00:50:47,800 --> 00:50:52,760
policy for us as a species to follow the main reason for this is is that it is the bottleneck

457
00:50:52,760 --> 00:50:58,040
towards building the actually existential dangerous systems just explain what these view caps are

458
00:50:58,040 --> 00:51:03,880
yes so compute is basically limiting the maximum size of the supercomputers i talked about the limit

459
00:51:03,880 --> 00:51:09,640
the maximum size our ai is and our computers are allowed to be and so we can limit hypothetically

460
00:51:09,640 --> 00:51:13,960
how intelligent they will be things actually get dangerous because we don't know what pops out of

461
00:51:13,960 --> 00:51:18,440
our experiments until we run them so it might already be that we're always too late our computers

462
00:51:18,440 --> 00:51:24,280
might already be big enough to end the world we don't know but hopefully not in that case the first

463
00:51:24,280 --> 00:51:28,840
as i say with russian roulette if you pull the trigger once and there was no bullet the correct

464
00:51:28,840 --> 00:51:35,080
move is not to pull it again the first thing you do is don't pull it again until you know if there's

465
00:51:35,080 --> 00:51:40,280
a bullet and where it is and if you know there's one definitely don't pull it so this is my opinion

466
00:51:40,360 --> 00:51:44,920
on this i will definitely be open to talking and would like to suggest this to all policy makers

467
00:51:44,920 --> 00:51:49,800
of all nations and leave this to say i think there is extremely strong resistance to this

468
00:51:49,800 --> 00:51:54,200
for the obvious reason that this cuts into the bottom lines of very powerful big tech companies

469
00:51:54,200 --> 00:52:00,520
who have extreme lobbying power and control over governments this is well i mean it's very simple

470
00:52:00,520 --> 00:52:05,480
there's a lot of people who gain a lot of benefits from continuing to pull that trigger and we have

471
00:52:05,480 --> 00:52:09,640
to make them stop and they are going to fight us every step of the way it's just how it works

472
00:52:10,840 --> 00:52:12,840
thank you very much Connolly

473
00:52:18,760 --> 00:52:23,080
and i'd like to invite the eight members of the panel to come up on stage

474
00:52:23,080 --> 00:52:27,800
and we're going to continue the conversation please self-organize on the seats

475
00:52:28,520 --> 00:52:32,360
stewart i don't think we've got a seat for you at this stage we can either find another seat for

476
00:52:32,360 --> 00:52:39,960
you or we'll let you come back on the stage later so sit down in whichever system you like

477
00:52:39,960 --> 00:52:45,320
and we will hear from each of these panelists what they think has been missing from the

478
00:52:45,320 --> 00:52:49,560
conversation so far maybe they've got an alternative view maybe they don't think we're playing

479
00:52:49,560 --> 00:52:54,360
russian roulette and a different metaphors appropriate maybe they would like to express

480
00:52:54,360 --> 00:52:59,960
what they think the politicians they're closest to should be saying maybe they'd like to comment on

481
00:52:59,960 --> 00:53:06,520
some of the other issues of safety so shall we start at the far end there and let's just move

482
00:53:06,520 --> 00:53:11,000
along the panel i'll give you two minutes each to contribute what you'd like into this conversation

483
00:53:11,000 --> 00:53:16,600
and then we'll hear from the audience so and let me introduce you as well so

484
00:53:18,440 --> 00:53:23,480
i'm sure i can do that my name is mark brackle i'm the director of policy at the future of life

485
00:53:23,480 --> 00:53:31,480
institute um and truly support what stewart and corner have been saying i think when we

486
00:53:31,480 --> 00:53:36,040
looked at the summit about six seven weeks ago we put out a set of recommendations ahead of time

487
00:53:36,520 --> 00:53:40,920
i think there were three traps that we identified that we were worried the summit would fall into

488
00:53:42,040 --> 00:53:45,880
the summit potentially not addressing the full range of risks all the way from

489
00:53:45,880 --> 00:53:51,880
bias and discrimination up to extinction it not being inclusive namely china not being invited

490
00:53:52,520 --> 00:53:57,720
and it being a setting where the big ceo's would sort of run the show and there would be maybe

491
00:53:57,720 --> 00:54:04,440
some token academics uh at a panel in a room the night before so i think if we sort of assess

492
00:54:04,440 --> 00:54:11,000
what the summit is looking like now the night before the actual event i think

493
00:54:11,720 --> 00:54:17,720
sort of we can be reasonably happy i saw this morning that china will in fact be invited

494
00:54:18,520 --> 00:54:26,040
and that it will be an inclusive summit in that nation of the world will get a seat at the table

495
00:54:26,040 --> 00:54:32,680
so i think that's progress so that's very good if we think about the harms and the range of harms

496
00:54:32,680 --> 00:54:38,280
that are being discussed one of the things fli recommended for the summit was grounding it in

497
00:54:38,280 --> 00:54:43,800
examples of large-scale ai harm that we've already seen such as the australian robo debt scandal

498
00:54:43,800 --> 00:54:49,080
or the dutch benefit scandal from the netherlands myself and it's it's a political scandal really

499
00:54:49,080 --> 00:54:54,840
dominating the national scene to to show that very simple algorithms can already have a very large

500
00:54:54,840 --> 00:54:59,640
impact in countries that were rushing towards adoption and to show the beginning of the trend

501
00:54:59,640 --> 00:55:03,960
line and that hasn't happened i think that's potentially a missed opportunity but i think

502
00:55:03,960 --> 00:55:10,920
it's really good where the uk has overall focused the summit and i think i'm sort of least optimistic

503
00:55:10,920 --> 00:55:15,800
when it comes to the role of companies at the summit and i think corner's done a great job at

504
00:55:15,800 --> 00:55:21,000
highlighting concerns that fli that many of us have around responsible scaling and this narrative

505
00:55:21,000 --> 00:55:25,800
being pushed by many companies as an excuse to keep going rather than making sure that whatever

506
00:55:25,800 --> 00:55:31,560
they put out onto the market is actually safe and i think that's a message that i hope we collectively

507
00:55:31,560 --> 00:55:35,800
and the people in this room that are going to the summit can still take to the participants

508
00:55:35,800 --> 00:55:41,000
and to the governments that are there making sure that we put the onus of what is safe and what isn't

509
00:55:41,000 --> 00:55:46,040
safe on the companies and they need to prove to us that what they're putting on the market is safe

510
00:55:46,040 --> 00:55:51,320
rather than the other way around where the default is they keep on scaling and it's up to the regulator

511
00:55:51,320 --> 00:55:56,920
to prove that what is safe so that i think is a key message to take so you're giving at least two

512
00:55:56,920 --> 00:56:02,040
cheers if not three cheers to the organisers for what you see happening already so from one

513
00:56:02,040 --> 00:56:08,680
dutch man to another ron rusendal is the direct deputy director general of the netherlands ministry

514
00:56:08,680 --> 00:56:13,560
for the interior lots of other roles what would you like to add to the conversation um well thank

515
00:56:13,560 --> 00:56:21,480
you very much and i'm glad to be here um um first point is that we regulate cars and we

516
00:56:21,480 --> 00:56:29,240
regulate pharmaceuticals um and we do so to mitigate risks of today and the risks of tomorrow

517
00:56:30,040 --> 00:56:37,640
um so we have to act upon risks of today like bias and risks of tomorrow

518
00:56:38,280 --> 00:56:46,360
um and those are global risks so we welcome the initiative of the summit but we also welcome

519
00:56:46,360 --> 00:56:55,160
the initiative of the tech and foyer of the un starting a high-level advisory board um and we

520
00:56:55,160 --> 00:57:03,960
have offered the tech and foyer to host a european um um meeting of the high-level advisory board

521
00:57:04,040 --> 00:57:08,760
in the hake for example in the peace palace because we support the work that we all do

522
00:57:08,760 --> 00:57:17,320
internationally to mitigate the risks um secondly we need some form of um early warning whatever

523
00:57:17,320 --> 00:57:25,080
the risks are um and whatever whether they will occur or not we have to have early warning and

524
00:57:25,080 --> 00:57:32,840
a rapid response mechanism uh or whatever goes uh will happen in the future and and therefore we need

525
00:57:32,840 --> 00:57:38,840
to um operate in a failure-driven way and we need to participate we need to coordinate but

526
00:57:38,840 --> 00:57:47,400
we also need to cooperate with industry with civil society with citizens with um uh industry and

527
00:57:47,400 --> 00:57:52,040
with governments agreement on early warning seems like something that both sides of the debate should

528
00:57:52,040 --> 00:57:57,080
be able to give because the people think things will go wrong and the people think things won't

529
00:57:57,080 --> 00:58:01,480
go wrong should be able to agree well if this happens we should all be paying more attention

530
00:58:02,440 --> 00:58:07,080
let's pass the microphone on to Hal Hodson who's a journalist at the Economist who has written

531
00:58:07,080 --> 00:58:12,840
a lot about existential risks and AI Hal thanks David um yes so my name's Hal Hodson I'm a special

532
00:58:12,840 --> 00:58:16,920
projects writer with the Economist I've been writing about AI for 10 years I have a degree in

533
00:58:16,920 --> 00:58:21,000
astrophysics and that meant that I spent a lot of time looking at a thing called the archive

534
00:58:21,000 --> 00:58:25,480
long before it was cool and uh papers from Facebook and Google would just turn up on the

535
00:58:25,480 --> 00:58:31,000
archive with no PR whatsoever and this is journalistic gold and that's how I got into it um I

536
00:58:31,000 --> 00:58:37,000
guess my sort of view is inherently going to be journalistic I think it is a very difficult point

537
00:58:37,000 --> 00:58:42,600
to make very clear decisions about what anybody ought to do about any of this I think there's

538
00:58:42,600 --> 00:58:46,600
from my perspective there's a huge amount of uncertainty I'm now I've now been writing about

539
00:58:46,600 --> 00:58:49,880
it long enough to know that there's also a lot of hype and it's not the first time there's been a

540
00:58:49,880 --> 00:58:56,360
huge amount of hype and I think making very clear decisions about important systems at a time that

541
00:58:56,360 --> 00:59:01,640
is hype filled is a difficult thing to do I think the thing that I can agree on the consensus that

542
00:59:01,640 --> 00:59:05,720
I can come to with probably most of the people in the room and the organizers of the summit

543
00:59:05,720 --> 00:59:10,840
is that there's a huge amount of science to do and it both in terms of existential risks and these

544
00:59:10,840 --> 00:59:15,720
sort of lower tier algorithmic risks I think there's two examples that show us that this is a perfectly

545
00:59:15,720 --> 00:59:21,080
plausible thing to do the first is that there was a time in the 90s when everybody was very worried

546
00:59:21,080 --> 00:59:28,280
about impact of bodies in the solar system to earth existential risks from asteroids and things

547
00:59:28,280 --> 00:59:33,400
like this and congress mandated a large amount of money to go to NASA to map all of the asteroids in

548
00:59:33,400 --> 00:59:38,680
the solar system and to figure out ways to nudge them off course if they come towards us and if

549
00:59:38,680 --> 00:59:43,800
you look at the risks as they were assessed in the 90s and the risks as they are assessed today

550
00:59:43,800 --> 00:59:49,400
they are massively massively dramatically lower and so that to me is a very strong case for doing

551
00:59:49,480 --> 00:59:54,200
science on these risks I don't know and I'd be fascinated to talk to people who do know what

552
00:59:54,200 --> 00:59:59,560
doing science on AI systems really looks like but it brings me to the next comparison which is

553
00:59:59,560 --> 01:00:04,520
Facebook about five years ago there was also a big panic that Facebook was determining the results

554
01:00:04,520 --> 01:00:09,320
of elections or you know hacking democracy essentially that is somewhat subsided now but

555
01:00:09,320 --> 01:00:14,680
one of the most sensible responses to that concern that I saw was also that you need to do science

556
01:00:14,680 --> 01:00:19,000
on Facebook just like you needed to do science on the solar system you need to start measuring

557
01:00:19,000 --> 01:00:24,520
things and it actually took years to force Facebook to give access to data to people like

558
01:00:24,520 --> 01:00:29,800
social science one it eventually sort of worked and I think there's a reason that you don't hear

559
01:00:29,800 --> 01:00:33,880
a huge amount about it it's because the science that's been done so far has not determined that

560
01:00:33,880 --> 01:00:39,720
Facebook destroyed democracy we still have at least a version of it and so I guess I would end

561
01:00:39,720 --> 01:00:45,880
just by a plea to you know and perhaps in the same way you were saying Stuart politically neutral

562
01:00:45,880 --> 01:00:50,120
science to the extent that that's possible a more of a goal than a thing that exists

563
01:00:50,120 --> 01:00:56,520
but do you have no whether the US focus on asteroid risk in the 90s was that bipartisan

564
01:00:56,520 --> 01:01:01,720
or was that a partisan issue I don't know if it was bipartisan but it went through congress so it

565
01:01:01,720 --> 01:01:06,600
must have been a bit bipartisan so maybe it wasn't as bad back then actually now that I think of it

566
01:01:06,600 --> 01:01:11,560
there's some encouraging examples there next we're going to hear from Anika Braak who's the CEO of

567
01:01:11,560 --> 01:01:18,600
the International Center for Future Generations tell us about your views Anika yes thank you very

568
01:01:18,600 --> 01:01:23,800
much and first of all I'd like to commend the UK on two things first of all their sense of humor

569
01:01:23,800 --> 01:01:30,200
for setting up a summit on the darkest corners of AI on the night of Halloween or the nights after

570
01:01:31,080 --> 01:01:37,800
and I'm surprised nobody made that joke so far and secondly for really bringing this to the

571
01:01:37,800 --> 01:01:45,000
attention of leaders media and the public actually I don't think Frontier AI has ever been discussed

572
01:01:45,000 --> 01:01:53,240
so much and the number of communicates the number of you know the the executive order the communications

573
01:01:53,240 --> 01:01:59,880
leaders European leaders meeting ahead of the summit negotiating late night to get to bring

574
01:01:59,880 --> 01:02:06,040
something here is already a measure of success and we could actually leave it here with this

575
01:02:06,040 --> 01:02:11,320
stellar panel and and say I think it's very important that the the civil society is meeting

576
01:02:11,320 --> 01:02:17,320
here this is maybe the element that is missing in the room I'd like to say we have two major

577
01:02:17,320 --> 01:02:22,200
challenges here one is a coordination challenge we have corporates looking at the topic they

578
01:02:22,920 --> 01:02:28,840
they're racing over competitive edge and we have governments who have serious geo-strategic

579
01:02:28,840 --> 01:02:34,680
interest and when those two come together that doesn't help collaboration so we have to think

580
01:02:34,680 --> 01:02:40,120
about how we get people around the table and secondly there's a democratic challenge democracy

581
01:02:40,120 --> 01:02:46,440
is by its very nature a slow and patient regulator and that's important I will argue that actually

582
01:02:47,720 --> 01:02:53,560
democracy is perfectly adapted to the society through these uncharted waters that we're experiencing

583
01:02:53,560 --> 01:02:59,480
at the moment but we need to make sure that the sailors of this big ship are prepared that they

584
01:02:59,480 --> 01:03:05,800
are well informed and that they have the tools to deal with this change and that's what the

585
01:03:05,800 --> 01:03:11,000
international center for future generations set out to do in Brussels that's why we moved our

586
01:03:11,000 --> 01:03:16,680
headquarters to Brussels to make sure that EU decision makers are well prepared because we

587
01:03:16,680 --> 01:03:22,120
set our best hope in the EU in this international race for governance I will leave it here for now

588
01:03:23,000 --> 01:03:28,760
are the EU decision makers paying attention to what you say I think they do you do here

589
01:03:28,840 --> 01:03:34,600
already a lot of signs that they have also recognized that we have to look at advanced

590
01:03:34,600 --> 01:03:41,640
artificial intelligence that regulation doesn't stop with the artificial intelligence act it's

591
01:03:41,640 --> 01:03:48,280
only the very start of the beginning or the first piece of the puzzle to come back to Stuart's presentation

592
01:03:49,080 --> 01:03:52,840
thanks next we're going to hear from Jan Tallinn who is the co-founder of Skype

593
01:03:53,320 --> 01:04:00,360
fli Caesar that's the center for the study of existential risks and he is also one of the

594
01:04:00,360 --> 01:04:05,960
advisors on the committee created by the tech and envoy for the UN so Jan what would you like

595
01:04:05,960 --> 01:04:12,920
to say based on what you've heard so far yeah thank you very much sometimes people ask me

596
01:04:13,480 --> 01:04:20,280
because I've been in this kind of existential risk and AI safety community and effort for more

597
01:04:20,280 --> 01:04:25,880
than a decade now sometimes people ask like so how's it going and my standard answer is well it's

598
01:04:26,680 --> 01:04:35,880
great progress against an unknown deadline and indeed it's kind of special this year it's just

599
01:04:35,880 --> 01:04:42,280
like there's like a plethora of things to point to us as great progress and obviously the most

600
01:04:42,280 --> 01:04:48,520
obvious one to point to at this point is the summit that starts tomorrow I do think it's

601
01:04:49,480 --> 01:04:55,160
UK deserves a great credit for for pulling this together and I really wish

602
01:04:56,040 --> 01:05:01,880
kind of best of luck to the organizers of this and the prime minister as well and the team

603
01:05:03,720 --> 01:05:13,560
now when it comes to this like unknown deadline recently I've kind of pivoted away

604
01:05:14,360 --> 01:05:23,880
to some degree from basically funding research towards just buying us more time which is kind

605
01:05:23,880 --> 01:05:31,080
of has to be has to deal with something like less research side and more kind of an action side more

606
01:05:31,080 --> 01:05:38,680
on the policy side so I do think it's kind of valuable now to really think through the policy

607
01:05:38,680 --> 01:05:46,760
that would make the future a little bit less sound and the deadlines a little bit less

608
01:05:46,760 --> 01:05:51,880
unknown another and final thing I wanted to say that there is I want to kind of caution against

609
01:05:54,040 --> 01:05:58,200
if you're sailing to like uncharted waters there's like a temptation to

610
01:05:59,400 --> 01:06:05,400
use something familiar and say that oh like the future is going to be just like this

611
01:06:05,480 --> 01:06:09,560
like the most common one is that oh AI is just the technology it's just going to be

612
01:06:10,200 --> 01:06:13,080
just another like electricity or something like that

613
01:06:15,800 --> 01:06:24,280
when we're talking about risks the the way the model risks is by the you know reference class

614
01:06:24,280 --> 01:06:31,080
that you cannot rule out so as long as there is like reference classes like viruses self-replicating

615
01:06:31,080 --> 01:06:36,360
things or another species as long as you kind of rule out rule them out you have to like prepare

616
01:06:36,360 --> 01:06:44,040
that this might be an instance of such thing so I think it's important to not make dismiss

617
01:06:44,040 --> 01:06:48,920
AI it's always just another technology or like as one prominent VC recently said oh it's just

618
01:06:48,920 --> 01:06:55,880
much of math thanks Jan next we're going to hear from Max Teckmark he might describe what he's got

619
01:06:55,880 --> 01:07:02,280
on his chest I happen to know he has released a very interesting TED talk which I strongly

620
01:07:02,280 --> 01:07:07,960
recommend all of you watch and Max might give an abbreviated form of that TED talk now or whatever

621
01:07:07,960 --> 01:07:13,240
else you'd like to put in the conversation thank you thank you yeah so I'm Max Teckmark I've been

622
01:07:13,240 --> 01:07:21,880
doing AI research at MIT as a professor there for many years focusing on safety related stuff I'm

623
01:07:21,880 --> 01:07:30,920
also the president of the future of life institute and I'm a huge fan of this guy who you guys have

624
01:07:30,920 --> 01:07:36,760
the wisdom to put on your 50 quid note Alan Turi who's come up many times and it's really remarkable

625
01:07:36,760 --> 01:07:43,720
that the argument he made 72 years ago that when machines greatly outsmart us by default they're

626
01:07:43,720 --> 01:07:48,520
going to take control that that argument has not been convincingly refuted in the 72 years

627
01:07:49,320 --> 01:07:53,880
since he said it so I think we have to take it very seriously and people who think of AI is

628
01:07:53,880 --> 01:07:59,320
just a new technology like steam engines or electricity tend to not take it so seriously

629
01:08:00,200 --> 01:08:06,520
Alan Turi himself clearly thought about AI more as a new species and with that framing it's very

630
01:08:06,520 --> 01:08:13,000
natural that we would lose control to them just like the Neanderthals lost control to us etc

631
01:08:13,000 --> 01:08:19,880
so so what are we going to do about about this great threat first of all having conversations

632
01:08:19,880 --> 01:08:25,160
like here and what happens tomorrow is great so a huge thank you to the British government for

633
01:08:25,160 --> 01:08:30,680
really putting this on and for standing up to all the lobbying pressure from companies who wanted to

634
01:08:31,480 --> 01:08:38,920
water it down into just talking into just a big blessing of responsible scaling or whatever

635
01:08:38,920 --> 01:08:45,880
thanks also to the US government for standing up to also the weird pressures to turn this into a

636
01:08:45,880 --> 01:08:51,800
geopolitical pissing contest by excluding China I'm really proud of the Brits for recognizing

637
01:08:51,800 --> 01:09:01,000
that this is a global challenge and what do we actually do about it well I think there's a remarkable

638
01:09:01,000 --> 01:09:07,160
consensus actually emerging from all the civil society and academic groups that don't directly

639
01:09:07,160 --> 01:09:13,400
profit the way companies do about what we should do about this we put out maybe

640
01:09:14,680 --> 01:09:19,080
Andrew creature Richard Muller can just hold up in the air with this thing you can if you go to

641
01:09:19,080 --> 01:09:28,280
future you'll find the alternative to the responsible scaling policy called called the

642
01:09:30,200 --> 01:09:35,800
safety standards policy where the idea is as we heard from Stuart Russell you should simply shift

643
01:09:35,800 --> 01:09:43,320
the responsibility to companies to prove that things are safe instead of as responsible scaling

644
01:09:43,320 --> 01:09:47,640
policy you you have the responsibility on the government regulators to prove that things are

645
01:09:47,640 --> 01:09:56,520
unsafe more or less in order to stop them and there's there's a whole set of very concrete ideas

646
01:09:56,520 --> 01:10:04,200
out there for what the safety standards should be to start with and some of them were mentioned

647
01:10:04,200 --> 01:10:10,200
very eloquently by Stuart you can insist on quantitative safety bounds or provable safety

648
01:10:10,200 --> 01:10:15,320
beginning with uncontroversial stuff that you should not be able to demonstrate that nobody

649
01:10:15,320 --> 01:10:23,080
can hack the servers that these super large systems are on that they won't advise on how

650
01:10:23,080 --> 01:10:27,880
to make bio weapons etc and this will very naturally accomplish something quite wonderful

651
01:10:27,880 --> 01:10:32,920
where we sort of have the cake and eat it as a species because most people I talked to don't

652
01:10:32,920 --> 01:10:39,320
realize that there are two almost there's two very different kinds of AI that they keep conflating

653
01:10:40,200 --> 01:10:46,040
there is the AI that has current commercial value for curing cancer making self or better

654
01:10:46,040 --> 01:10:52,280
safer cars and all sorts of wonderful things which have very little risk associated with them but

655
01:10:52,280 --> 01:10:58,600
some which we need to address but 99 percent of the things that most people are excited about

656
01:10:58,600 --> 01:11:03,960
do not require playing Russian roulette with AGI and super intelligence and then there is this

657
01:11:03,960 --> 01:11:12,760
lunatic frame just try to build the machines that outsmart humans in all ways where almost all

658
01:11:12,760 --> 01:11:18,440
the risk is coming for very little benefit so if we can put safety standards in place we can I think

659
01:11:18,440 --> 01:11:22,680
quickly get into a situation where we have a long future with these wonderful benefits

660
01:11:23,320 --> 01:11:29,720
that are quite safe to get from AI and then just take your time with with the really risky stuff

661
01:11:29,720 --> 01:11:34,600
maybe one day humanity will or will not want to build more powerful machines but only when we can

662
01:11:34,600 --> 01:11:38,920
figure out you know how to control them so that would end with just a bit of wisdom from ancient

663
01:11:38,920 --> 01:11:45,960
Greece if I may so raise your hand if you remember the story of Icarus don't get hubris right you know

664
01:11:46,040 --> 01:11:53,720
so artificial intelligence is giving humanity these incredible intellectual wings with which

665
01:11:53,720 --> 01:12:01,160
we can accomplish things beyond their wildest dreams if we stop obsessively trying to fly into

666
01:12:01,160 --> 01:12:08,520
the sun thank you so you're not saying pause AI you're saying let's keep using AI but you're

667
01:12:08,520 --> 01:12:16,280
saying pause the rush to AGI let's not pause AI in fact let's continue almost everything that

668
01:12:16,280 --> 01:12:23,240
people are excited about doing but pause this compulsive obsession about training ever more

669
01:12:23,240 --> 01:12:31,320
ginormous models that we just don't understand thanks Andrea Miotti is the head of policy and

670
01:12:31,320 --> 01:12:35,400
governance for AI at Conjecture are you in agreement with what you've heard or you have

671
01:12:35,480 --> 01:12:40,680
different things to emphasize absolutely I'm very much in agreement with both the speakers and many

672
01:12:40,680 --> 01:12:45,640
other members of the panel I think there are two big positives from the summit to highlight

673
01:12:47,000 --> 01:12:53,160
one is that we're also echoed by the panel one is it's role in building common knowledge

674
01:12:53,880 --> 01:12:59,400
making it clear and explicit at the highest levels of government that this is a big risk

675
01:12:59,400 --> 01:13:05,720
that this is a extension level threat that we face as a species and number two coordination

676
01:13:05,720 --> 01:13:12,280
not getting lost in a geopolitical pissing contest as Max has said or in other of these

677
01:13:12,280 --> 01:13:18,920
things and realizing this is a again a threat we all face together it's a global security problem

678
01:13:18,920 --> 01:13:23,400
it's not a national security problem or at least it's not only a national security problem and

679
01:13:24,120 --> 01:13:29,720
to solve these problems we need coordination even during the heights of the Cold War there were

680
01:13:29,720 --> 01:13:38,200
open lines between the US and the Union to deal closing the door on cooperation before it has

681
01:13:38,200 --> 01:13:46,840
been tried is a surefire way for all of us to lose and so I was very very pleased to see that the

682
01:13:47,560 --> 01:13:54,120
UK government the Prime Minister Rishi Sunak have already acknowledged the risks very explicitly

683
01:13:54,120 --> 01:14:00,920
in the Prime Minister's speech last week are setting up the summit are inviting a diverse

684
01:14:00,920 --> 01:14:08,760
group of countries to discuss this risk together the part where I think we can go

685
01:14:09,400 --> 01:14:15,160
further and we can do better is in the measures I share the concern of some of the other panelists

686
01:14:15,160 --> 01:14:22,600
on a focus of simply enabling the default to continue and the reality is that the default

687
01:14:23,400 --> 01:14:30,680
is bad the default is bad we by now all understand it is bad and we all understand we need something

688
01:14:30,680 --> 01:14:38,120
else even the companies racing towards its default admit that it's bad admit that it's a

689
01:14:38,120 --> 01:14:46,440
one in four one in ten unacceptably high chance for all of us to be wiped out and so the concrete

690
01:14:46,440 --> 01:14:52,120
measures they will need to take cannot look like continuing on the default path cannot look like

691
01:14:52,680 --> 01:15:00,680
systems are safe until proven dangerous by external auditors that are strapped for resources and they

692
01:15:00,680 --> 01:15:06,920
don't even have the the tools or the tests to do these tests they look like provably safe systems

693
01:15:06,920 --> 01:15:12,200
they look like burden of proof on developers developing systems that they admit could wipe

694
01:15:12,200 --> 01:15:17,640
everyone out to demonstrate ahead of time of running critical experiments that they are safe

695
01:15:18,200 --> 01:15:22,280
if they cannot do that that's fine they can just build something else or they can move to a different

696
01:15:22,280 --> 01:15:29,240
sector that's the standard we utilize in all high risk sectors there is no reason to not

697
01:15:29,240 --> 01:15:34,600
utilize it in a sector where the risks are the literal extinction of your mighty thank you

698
01:15:35,240 --> 01:15:41,400
and last but not least we have a trained economist alexandra musavi sadeh who is the

699
01:15:41,400 --> 01:15:47,000
CEO of evident what would you like to add to the conversation alexandra it's a hard it's a it's a

700
01:15:47,000 --> 01:15:55,080
great panel to follow it so it's um i think i have a different time horizon alexandra musavi sadeh

701
01:15:55,080 --> 01:16:02,920
i'm the founder and CEO of evident and we actually do a lot of measurement uh we specialize in

702
01:16:02,920 --> 01:16:10,440
benchmarking businesses on the option of ai so what i focus on is very near term so looking at

703
01:16:10,440 --> 01:16:17,480
the here and now and the race is on at that level as well so the race is on by all businesses in

704
01:16:17,480 --> 01:16:25,240
all sectors to take the capabilities that ai offers today and to implement it as fast as possible

705
01:16:25,240 --> 01:16:30,120
and really not thinking about any of the risks so thinking about growing market share um

706
01:16:30,680 --> 01:16:37,320
upping revenue cutting costs and all of that and continuing the um sort of digital transformation

707
01:16:37,320 --> 01:16:43,960
which is now more and more an ai transformation and so with that um we we are observing this ai

708
01:16:43,960 --> 01:16:49,240
race at a business level and one of the things that we see that some businesses that are highly

709
01:16:49,240 --> 01:16:55,880
regulated really think about um how they can implement the oversight and implementation of

710
01:16:55,880 --> 01:17:04,200
safe ai so while very impressed with what the uk government is doing and i think the right thing

711
01:17:04,200 --> 01:17:11,800
is to focus on the long term because that is where we um should have our eyes at um at the stage but

712
01:17:11,800 --> 01:17:17,000
there's also a near term risk and i think um if there was one thing i would suggest is that as

713
01:17:17,000 --> 01:17:21,960
much as we need to focus on the long term we also need to look at the here and now and that businesses

714
01:17:22,040 --> 01:17:28,200
are barreling ahead with ai adoption without any particular guard rails in that and so while we

715
01:17:28,200 --> 01:17:35,560
need to put the um the burden on the development of safe ai we could also maybe in the meantime put

716
01:17:35,560 --> 01:17:43,320
the burden on the businesses that are using ai to prove that they're doing it in a safe uh and

717
01:17:43,320 --> 01:17:48,600
constructive way thanks so you've heard from all the panelists i'm sure there's lots and lots of

718
01:17:48,600 --> 01:17:52,920
questions in your mind so i'm going to come to the audience and take maybe three or four questions

719
01:17:52,920 --> 01:17:58,120
and then let the panelists pick what they want and my question to be would be do you agree with

720
01:17:58,120 --> 01:18:03,160
this division between near term and far and far future some people say that the risks from

721
01:18:03,160 --> 01:18:08,840
existential risk should not be considered to be far long term they are potentially here and now

722
01:18:08,840 --> 01:18:14,840
but maybe you have a different way of framing it let's see some hands let's take uh one back over

723
01:18:14,840 --> 01:18:21,960
there in the far corner it's a bit some running around if you can say who you are if you want

724
01:18:21,960 --> 01:18:28,200
to remain anonymous that's fine too hi um i'm Matthew Kilcoin i looked into how the banking

725
01:18:28,200 --> 01:18:34,520
industry turns short term into long term by sort of senior management risk and associated penalties

726
01:18:34,520 --> 01:18:40,920
and clawbacks to force the change okay question on learning from the banking industry so let me

727
01:18:40,920 --> 01:18:49,400
give the microphone down here just a second thanks a lot everyone yolanda lancas from the

728
01:18:49,400 --> 01:18:55,160
future society what do we do about open source ai professor russell mentioned one idea which was

729
01:18:55,800 --> 01:19:00,520
kill switches i think this is in terms of policy approaches such an important question

730
01:19:00,520 --> 01:19:06,680
for us to all grapple and again um as yan was saying oh technology people assume that paradigms

731
01:19:06,680 --> 01:19:12,840
continue open source has been valuable for software but with ai we're seeing new risks

732
01:19:12,840 --> 01:19:25,720
and paradigms and how could maybe academia and others my name is oliver graves um my question is

733
01:19:25,720 --> 01:19:31,000
what do you think the biggest hurdles are towards getting the general public to recognize this as an

734
01:19:31,000 --> 01:19:35,640
existential risk and to take that risk seriously because it still seems to me like it's all well

735
01:19:35,720 --> 01:19:40,360
and good everyone here at the summit and in this room being aware of that risk but it doesn't seem

736
01:19:40,360 --> 01:19:46,280
to me like we're anywhere close to a level of majority of the general public grappling with it

737
01:19:46,280 --> 01:19:48,120
probably

738
01:19:52,280 --> 01:19:56,440
thank you so i'm i'm father pete vignaschi i'm i'm engaged in looking at how the catholic church

739
01:19:56,440 --> 01:20:01,880
can respond to existential risks so it's a slightly different question here just in the most general

740
01:20:01,880 --> 01:20:07,160
way what does it look like from your side of the table for religious groups to play their parts

741
01:20:07,160 --> 01:20:15,720
in achieving existential security thank you i'm oliver chamberlain i'm a student studying

742
01:20:17,000 --> 01:20:23,480
a master's in science in in ai uh one of my concerns is although like the regulation

743
01:20:23,480 --> 01:20:31,000
is going to involve limiting like supply chains making sure that gpu's aren't going off to places

744
01:20:31,000 --> 01:20:37,640
that we don't know about how do we stop um the advancement of algorithms which allows older

745
01:20:37,640 --> 01:20:46,680
systems to be more powerful so like alpha tensor um i wonder uh in my mind like the only way around

746
01:20:46,680 --> 01:20:54,520
something like this is a future which is like super draconian um how do we prevent gpu's that

747
01:20:54,520 --> 01:20:58,520
already accessible already out there from being used in ways which are way more powerful

748
01:21:12,760 --> 01:21:17,800
question on open source to what extent is it possible to control open source a question on

749
01:21:17,800 --> 01:21:23,240
what are the biggest hurdles changing their minds in the public or indeed one of the other big hurdles

750
01:21:25,080 --> 01:21:32,840
yeah question from the point of view of what might religious organizations contribute to

751
01:21:32,840 --> 01:21:39,720
this conversation and do we need to have super draconian surveillance and policing systems if

752
01:21:39,720 --> 01:21:45,880
we're going to stop these gpu's and algorithms they're potentially doing things that we didn't

753
01:21:45,880 --> 01:21:52,200
want them to so max hand up first yeah religious organizations i hope can

754
01:21:53,160 --> 01:21:59,560
remind us all of the importance of not play god and get hubris remember the moral angle

755
01:22:00,600 --> 01:22:05,080
i'm only going to comment on the timeline one even though i have opinions about all the others

756
01:22:05,080 --> 01:22:10,680
it's not so i don't talk too much the timeline one from alan turing's perspective when he said this

757
01:22:10,680 --> 01:22:17,320
he said that when it eventually basically passed the turing test he expected to go very fast

758
01:22:18,040 --> 01:22:23,800
so then it was a long-term risk now according to yosha bengio gpt4 passes a turing test so

759
01:22:23,800 --> 01:22:29,960
he would probably if he were still with us in the room predict short timeline it's quite remarkable

760
01:22:29,960 --> 01:22:34,840
what's happened on on the prediction market metaculous.com for those of you who are nerdy enough

761
01:22:35,640 --> 01:22:41,320
to go there where the timeline how many years we have left artificial general intelligence

762
01:22:41,400 --> 01:22:47,800
outsmarting us has plummeted from 20 years away to three years away just in the last 18 months

763
01:22:48,440 --> 01:22:54,520
as a direct result of of this recent tech progress and dario amode has openly said one of the tech

764
01:22:54,520 --> 01:22:59,640
ceo's here that he thinks you have two or three years left and others other tech ceos told me

765
01:22:59,640 --> 01:23:05,640
that individually so i think we just have to stop calling this artificial general intelligence

766
01:23:05,640 --> 01:23:11,640
risk long term or people are going to laugh at us and call us dinosaurs stuck in 2021

767
01:23:13,720 --> 01:23:21,800
andrea i'd like to answer the question about the public actually the pub seems to really understand

768
01:23:22,440 --> 01:23:28,280
i recently ran polling as part of a campaign i'm running called control ai and the british public

769
01:23:28,280 --> 01:23:34,600
is extremely concerned about disempowerment and extinction risk from ai they seem to be

770
01:23:34,680 --> 01:23:45,320
aware of it um a they seem to be aware of it um a whopping 60 percent a global ban on smart and

771
01:23:45,320 --> 01:23:52,200
human ai period with only i believe 14 percent against and like quite a few undersized um

772
01:23:53,320 --> 01:24:00,040
nearly i believe almost nearly 90 percent would be very very happy with a full ban on deepfakes

773
01:24:00,040 --> 01:24:08,040
right now people understand very very well that full impersonation revenge pornography and like use

774
01:24:08,040 --> 01:24:13,560
of their likeness against their will is not good is destabilizing is a threat that exists right

775
01:24:13,560 --> 01:24:19,960
now with systems over here right now and they don't want that um and similarly uh there is

776
01:24:19,960 --> 01:24:25,880
i believe 78 percent of the public it would want an international watchdog with real teeth more like

777
01:24:25,880 --> 01:24:34,200
an i a a and there are basically across the board like i was personally surprised as he all of the

778
01:24:34,200 --> 01:24:42,440
answers come up with such overwhelming uh support uh we might ask whether that mood is shallow that

779
01:24:42,440 --> 01:24:48,360
it might be adjusted we might ask whether that mood is shallow that it might be adjusted again

780
01:24:48,360 --> 01:24:54,840
in the future let's hear from anika and then from how and then from yana uh yeah i just wanted to

781
01:24:54,840 --> 01:25:00,360
say that we should stay away from predictions with regard to timelines with regard to sectors

782
01:25:00,360 --> 01:25:04,840
how they're going to be affected i think if we have learned one thing that it's really difficult

783
01:25:04,840 --> 01:25:10,520
to assess that but we do know that there will be dislocations there will be impacts and as they

784
01:25:10,520 --> 01:25:16,200
grow and as we see the more the public will be more informed and more aware i think it's really

785
01:25:16,200 --> 01:25:23,880
our role here as civil society academia religious leaders to increase that awareness and to also

786
01:25:23,880 --> 01:25:30,120
keep that conversation going um as we go when you drive a car you don't just look ahead right

787
01:25:30,120 --> 01:25:36,520
you also look in the rear view uh mirror you look in the side mirrors for signals of change but

788
01:25:36,520 --> 01:25:41,240
what i want to say when you look back that's something we are not doing enough we're trying

789
01:25:41,240 --> 01:25:46,520
to predict in the future but we should also look back and look at you know stuff we've put out

790
01:25:46,520 --> 01:25:51,720
there regulation we're putting out there and how it's actually being enforced um if it's effective

791
01:25:51,800 --> 01:25:58,600
if um it's yeah effective regulation we discussed about it um before and this will be key going

792
01:25:58,600 --> 01:26:05,240
forward how i'll take the question oh yeah on yeah i'll take the question on open source

793
01:26:05,240 --> 01:26:10,440
and draconianism because i think they're related i don't really see a good way of regulating and

794
01:26:10,440 --> 01:26:15,720
controlling open source code that is not deeply draconian that does not involve a massive expansion

795
01:26:15,720 --> 01:26:21,480
of surveillance uh if you need to know what code is running on what chips you have to have access

796
01:26:21,560 --> 01:26:26,920
to the computer in which those chips are running and for a sense of how well this is going to go

797
01:26:26,920 --> 01:26:33,480
look at america's attempts to put export controls on chinese ai development um it it works to an

798
01:26:33,480 --> 01:26:40,920
extent but it only works if you pick these very narrow bottlenecks and i guess in terms of existential

799
01:26:40,920 --> 01:26:46,840
risk it depends on how powerful sort of lower tier open source models end up being i don't really have

800
01:26:46,840 --> 01:26:51,480
a good answer to that question i just say one more thing on metaculous just as a little hint of

801
01:26:51,480 --> 01:26:56,200
not relying on it too much if you just if you remember the lk 99 superconductivity thing over

802
01:26:56,200 --> 01:27:03,400
the summer metaculous at one point was completely certain that that was real and uh the metaculous

803
01:27:03,400 --> 01:27:08,760
thought it was real for a while and and then it dived again so just you can't rely on metaculous

804
01:27:08,760 --> 01:27:16,120
we're gonna do some real time checking on this right so once that's going on yeah so i also

805
01:27:16,120 --> 01:27:21,160
wanted to say a few words about open source i think it's as i mentioned earlier i think it's

806
01:27:21,160 --> 01:27:27,560
important to just like not do this kind of categorical thinking that you have like some one particular

807
01:27:27,560 --> 01:27:32,600
you know look at that too that you put things in and then like you reason about this pocket

808
01:27:34,040 --> 01:27:39,800
my friend andrew gritch who is in the audience like he he observes that's like when people

809
01:27:40,520 --> 01:27:45,720
say that they're really gonna prove open source it's valuable to try to understand what they

810
01:27:45,720 --> 01:27:50,760
actually want what is what is the thing that they're trying to protect and quite often it's

811
01:27:50,760 --> 01:27:55,320
just like they don't want this like massive centralization and power in the hands of people

812
01:27:55,320 --> 01:28:00,840
that they don't trust now the question is like if you think about open source as like irreversible

813
01:28:00,840 --> 01:28:05,880
deployment of things that we potentially don't want to irreversible deploy other other ways

814
01:28:05,880 --> 01:28:12,360
to protect what the open source allocates want and for example there is like there's like in blockchain

815
01:28:12,360 --> 01:28:21,320
community there is quite a lot of advancement in cryptographic techniques techniques like

816
01:28:21,320 --> 01:28:26,520
zero noise proofs perhaps there are enough like ways how we can kind of eat our cake can keep it

817
01:28:27,240 --> 01:28:33,800
keep it too by instead of having nosy people literally looking around in a computer you just

818
01:28:33,800 --> 01:28:38,440
computer automatically producing things like zero noise proofs that you haven't been up to no good

819
01:28:38,440 --> 01:28:44,120
things like that so there's lots of possibilities to explore there are this andra i'm actually

820
01:28:44,120 --> 01:28:51,720
curious about what max and how we're talking about ai secrets did they know or did they not know

821
01:28:53,640 --> 01:28:58,360
peak enthusiasm they thought it was 60 chance which i would say interpreted that they were

822
01:28:58,360 --> 01:29:05,640
not saying for sure okay but prediction is very real time checking here prediction is hard canary

823
01:29:05,640 --> 01:29:10,360
signals are more important in my view let's agree the canary signals okay if i can just add

824
01:29:10,360 --> 01:29:14,760
something here we we ask if something is a near term or long term risk we have to remember

825
01:29:14,760 --> 01:29:19,080
we're not asking if we know for sure that we're going to get super intelligent soon

826
01:29:19,080 --> 01:29:24,360
if we think there's a three a 10 percent chance that something like this might happen in four years

827
01:29:24,360 --> 01:29:29,240
but then it's still in the risk is near term even though i hope as much as anyone that it actually

828
01:29:29,240 --> 01:29:35,560
won't happen for a long time so alessandra and then ron and then mark yeah i just wanted to

829
01:29:35,560 --> 01:29:40,760
respond to the question on the banks um if i understood it correctly is like was that maybe a

830
01:29:40,760 --> 01:29:48,360
model of regulation for ai is that what you meant okay i mean as we cover that sector very deeply

831
01:29:48,360 --> 01:29:54,840
it is it is an area where i mean and it's it's um it's a sector that is heavily regulated and because

832
01:29:54,840 --> 01:29:59,560
they heavily regulated the way that they are developing and deploying and implementing ai

833
01:29:59,560 --> 01:30:04,440
today is that even as they develop there's a lot of oversight in the models themselves and then they

834
01:30:04,440 --> 01:30:10,360
have they go through first and second and third lines of defense where there is oversight again

835
01:30:10,360 --> 01:30:16,040
and then they submit to the regulators and in a way i mean i can't believe i'm saying this but

836
01:30:16,040 --> 01:30:24,280
in a way the banks could be a model with which if you are to impose a oversight at a company level

837
01:30:25,240 --> 01:30:30,040
for the ai that they're using the banking model is not a bad one because the way that they

838
01:30:30,600 --> 01:30:36,280
assess the risks as they go from development to deployment into production and output so

839
01:30:37,320 --> 01:30:43,160
that could be a blueprint or something to to look at for businesses themselves to regulate

840
01:30:43,160 --> 01:30:47,160
themselves is that if that's where we end up so there might be something to learn but bearing

841
01:30:47,160 --> 01:30:52,360
in mind agi is different from everything that's ever been before ron what would you like to add

842
01:30:52,360 --> 01:30:57,400
and well i'd like to come back to the discussion about open source versus closed source and i'm

843
01:30:57,400 --> 01:31:03,720
a bit surprised that here at the table there is a strong belief in closed lots of companies

844
01:31:04,440 --> 01:31:11,880
that might contain lots of zero days we don't know of instead of trusting civil society

845
01:31:13,160 --> 01:31:19,560
and and on regulation we therefore not only need to regulate development we also need to

846
01:31:19,560 --> 01:31:27,720
regulate use and there and that's what the ai does for example sounds like none of the

847
01:31:27,720 --> 01:31:31,800
old traditional models are going to work sounds like we need something that has a variety of

848
01:31:31,800 --> 01:31:37,320
different approaches we have to transcend some of the previous systems and mark and then we'll

849
01:31:37,320 --> 01:31:43,000
come back to pick up a few more comments um yeah i just wanted to pick up on sort of a question of

850
01:31:43,000 --> 01:31:47,240
near term or existing harms that we see in sort of harms in future because i think what you saw

851
01:31:47,240 --> 01:31:50,920
yesterday with president biden coming out with an executive order in the united states

852
01:31:51,560 --> 01:31:56,920
as well as with uai act which has been a long time in the making you see sort of an attempt and i

853
01:31:56,920 --> 01:32:02,600
think a successful attempt by policy makers to tackle both ai and bias that you see in current

854
01:32:02,600 --> 01:32:09,560
day applications and some of the risks that are like that are agi related and i think that shows

855
01:32:09,560 --> 01:32:15,240
that it's perfectly possible to do both and i know a lot of people in this audience are potentially

856
01:32:15,240 --> 01:32:19,320
driven by existential risk i mean that's why we come to an existential risk observatory event

857
01:32:19,320 --> 01:32:24,360
panel um but i think there is a lot of alliances and bridges that can be built across that space

858
01:32:24,360 --> 01:32:30,120
and i think it's often not helpful to look at both of these things um maybe just on banking um i mean

859
01:32:30,120 --> 01:32:36,200
we saw we seen the 2008 financial crisis where i think lots of people were justifiably angry

860
01:32:36,200 --> 01:32:41,000
because ceo's got away with whatever they were wanting to do uh and governments build out the

861
01:32:41,000 --> 01:32:46,360
banks um there was briefly a lot of regulation that was then rolled back over the past few years

862
01:32:46,360 --> 01:32:51,320
and again we saw a few small banks collapse in california so i think we need to learn some

863
01:32:51,320 --> 01:32:56,920
lessons there around liability and making sure that as we build a liability regime for ai companies

864
01:32:57,960 --> 01:33:03,400
ceo's are also individually and criminally liable if they are indeed negligent or if there are

865
01:33:03,400 --> 01:33:07,320
sort of safety risks that they're they're ignoring so maybe just to add on those two points so half

866
01:33:07,400 --> 01:33:10,920
the panel have got their hands up wanting to speak well i'm going to ignore them briefly and

867
01:33:10,920 --> 01:33:14,760
give the microphone very quickly to three people in the audience but you have to be quick because

868
01:33:14,760 --> 01:33:20,920
we're out of time already thanks uh richard barker and ron introduced the analogy of the

869
01:33:20,920 --> 01:33:24,920
pharmaceutical industry which is not perfect by any means but i spent most of my career in it so

870
01:33:24,920 --> 01:33:30,920
i think there's still a few lessons from that right the first is you regulate not the underlying

871
01:33:30,920 --> 01:33:36,200
technology but the application of the technology so it turns out felidomide is the terrible thing

872
01:33:36,200 --> 01:33:41,720
to give to pregnant women but it actually cures people with multiple myeloma so you i can't imagine

873
01:33:41,720 --> 01:33:47,560
how we're going to actually deeply regulate the the internal workings it's it's how they're used

874
01:33:47,560 --> 01:33:55,160
and it may not be existential risk that is most uh relevant uh to actually harnessing public opinion

875
01:33:55,160 --> 01:34:00,680
it will be some of the things that's already happening that affect them personally uh they're

876
01:34:00,680 --> 01:34:08,440
not just experts but panels um come back to uh legislators and say this is what i saw and this

877
01:34:08,440 --> 01:34:21,960
is what i like and don't like um terry rabie former risk manager um guys you really need some pushback

878
01:34:22,920 --> 01:34:30,280
uh i was deeply appalled at stewart's example of the nuclear industry the regulation of the

879
01:34:30,280 --> 01:34:39,800
nuclear industry in the united states essentially is anti-human it's prevented um the gifts of

880
01:34:39,800 --> 01:34:48,440
energy that's not polluting by regulation we have another anti-human example of regulation in the

881
01:34:48,440 --> 01:34:56,200
u which is a regulation of biology destructive of the advantages that we could get from genetic

882
01:34:56,200 --> 01:35:03,320
regulation of biology destructive of the advantages that we could get from genetically modified

883
01:35:03,320 --> 01:35:11,320
organisms so look it won't do the needs to be a little bit more pushback to you guys so you get

884
01:35:11,320 --> 01:35:22,360
your story straight okay who's behind her here so earlier thank you don't show me there's key

885
01:35:22,360 --> 01:35:30,520
substances professor russer make a very important distinction about as a i to be safe for humans

886
01:35:31,160 --> 01:35:39,400
and a i uh safe for use as a tool the first one is a new type of intelligence the second one

887
01:35:39,400 --> 01:35:48,440
is a just used as a tool so that's where the regulation comes in context we can regulate

888
01:35:48,440 --> 01:35:56,280
a i as safe to use and we must have a control of our development so it doesn't become an

889
01:35:56,280 --> 01:36:03,080
existential risk my question to the panel is the following one is it possible or shouldn't be possible

890
01:36:03,160 --> 01:36:10,760
in order to avoid uh open sourcing problems to develop just one super intelligence program

891
01:36:10,760 --> 01:36:18,120
that will beat any uh small guys developments and in that way make us safer and the second

892
01:36:18,120 --> 01:36:25,880
question is what will follow this summit deliverable which thanks so plenty to talk about there

893
01:36:25,880 --> 01:36:30,600
learning from the pharmaceutical industry regulating apps not platforms we had terry

894
01:36:30,600 --> 01:36:35,320
pushing back quite hard saying goodness look at the mess of regulation in the nuclear industry

895
01:36:35,320 --> 01:36:43,320
and in gmo's we had tony asking about a unified approach with one research and development program

896
01:36:43,320 --> 01:36:50,360
and also what's going to happen next so 30 seconds each max's hand up again i have a good friend

897
01:36:50,360 --> 01:36:55,080
in the american nuclear industry who told me that what really killed it wasn't regulation but it was

898
01:36:55,080 --> 01:37:03,080
fukushima and then three mile island i for open source 20 seconds on that i actually think i i love

899
01:37:03,080 --> 01:37:08,280
open source almost as much as yon lakoon mit is the cradle kind of open source but obviously we

900
01:37:08,280 --> 01:37:14,200
don't open source plutonium in this geranium and similarly here we should get away from this childish

901
01:37:14,200 --> 01:37:17,800
debate about whether your open source nothing or everything and just ask where the line goes

902
01:37:18,680 --> 01:37:23,720
finally there's a technical solution i think to this which is not creepy but still works

903
01:37:23,720 --> 01:37:31,560
which steve omohandro and i wrote a paper about where where you actually have control of your own

904
01:37:31,560 --> 01:37:35,800
hardware chips you own it no the government doesn't see what you run but it's not going to run

905
01:37:35,800 --> 01:37:40,040
certain kinds of really creepy code because the hardware itself won't don't it's like a

906
01:37:40,040 --> 01:37:44,840
virus checker in reverse where if your code can't prove that it's not making bioweapons

907
01:37:44,840 --> 01:37:50,520
it just won't run so you can find out more about that proposal if you watch max's ted talk how

908
01:37:51,400 --> 01:37:55,160
all i was just gonna ask doesn't that make it a backdoor in kind of the same way that

909
01:37:55,160 --> 01:38:01,240
csam detection on i message it makes it a backdoor no it's completely decentralized no one has

910
01:38:01,240 --> 01:38:05,480
access to your chip it's just if you want to chip that'll run the harmful code you have to make your

911
01:38:05,480 --> 01:38:13,000
own chip we need standards for hardware which is what stew was saying who's who's wants to jump in next

912
01:38:13,000 --> 01:38:22,920
andrea just a quick reply to the ipcc model uh i very much hope that we will not have an international

913
01:38:22,920 --> 01:38:30,360
agency model after the ipcc here uh we are in crunch time and we have now knowledge of the risks

914
01:38:30,360 --> 01:38:36,120
and we have common knowledge about the risks uh the role of the pcc was a great organization to

915
01:38:36,120 --> 01:38:42,280
build over decades uh essentially expertise and information to governments to deliberate on how

916
01:38:42,280 --> 01:38:49,000
to act we do not have decades and we know what the problems are governments are already acknowledging

917
01:38:49,000 --> 01:38:56,280
what the problems are we need action not a yearly report alessandra i would i would agree with that

918
01:38:56,280 --> 01:39:00,680
i think we're at a point where and also i don't see how it's practically gonna gonna work i mean

919
01:39:01,320 --> 01:39:09,240
are the chinese in the u.s gonna open the kimono and submit to a uk body that wants it to um and

920
01:39:09,240 --> 01:39:13,400
no accountability and no repercussions if they don't so i really don't see it that how that would

921
01:39:13,400 --> 01:39:19,800
work but um there's so much to say and so much to respond to uh on this but i think uh gentlemen

922
01:39:19,800 --> 01:39:25,560
in the in the red jumper they'll very much agree with the the fact that at least until

923
01:39:26,360 --> 01:39:32,280
something has um taken place on the regulation and we've agreed to what that might look like for the

924
01:39:32,280 --> 01:39:39,560
near term big risks but for the here and now um make the companies make the sectors accountable

925
01:39:39,560 --> 01:39:45,800
for how they use it make the buck stop there first and then we can figure out or in parallel figure

926
01:39:45,800 --> 01:39:52,040
out how to regulate it's sort of the bigger bigger questions and the the things that are um giving us

927
01:39:52,040 --> 01:39:56,200
pause but i think the world will submit to a body that's run by the uk but the world might

928
01:39:56,200 --> 01:40:02,120
cooperate with a body that the uk helps to inspire and get off the board mark your hand was up

929
01:40:03,080 --> 01:40:07,560
yeah i also just wanted to pick on uh pick up on the question the gentleman in the red jumper

930
01:40:07,560 --> 01:40:11,880
raised on parallels with the pharmaceutical industry uh yeah you're just sort of shining

931
01:40:11,880 --> 01:40:17,880
beacon here in the audience um i mean i think on the one hand like the time where everyone

932
01:40:17,880 --> 01:40:22,200
just could produce whatever potion they wanted to and put it out on the market that has disappeared

933
01:40:22,200 --> 01:40:26,040
and i think thankfully disappeared and i think that there is lessons we can learn in that in

934
01:40:26,040 --> 01:40:30,760
terms of potentially licensing or making sure that you guarantee that something's safe when it comes to

935
01:40:31,320 --> 01:40:36,280
regulating the application of ai i think i'm significantly more skeptical uh this has been

936
01:40:36,840 --> 01:40:42,360
for example in the senate hearing this was what uh christine amon gomri from ibm was pushing quite

937
01:40:42,360 --> 01:40:46,920
heavily we see on both sides of the atlantic big tech really pushing for application based

938
01:40:46,920 --> 01:40:51,800
regulation because that often means that the underlying big systems that they are building

939
01:40:51,800 --> 01:40:57,800
won't be regulated right because how do you regulate regulate gpt4 if if you only regulate

940
01:40:57,800 --> 01:41:03,000
applications and it's only the hospital that then integrates it into a chatbot for patient contact

941
01:41:03,000 --> 01:41:07,880
contact that actually has to deal with the regulatory burden so i think you do need to force

942
01:41:07,880 --> 01:41:13,240
these big tech companies to do risk identification and mitigation even if they can't particularly

943
01:41:13,240 --> 01:41:18,040
specify oh it goes into that application or or this other one so i think you need a bit of a

944
01:41:18,040 --> 01:41:25,880
combination of both anik are you a fan of the ipc model ipcc or would you prefer the i c fg

945
01:41:25,880 --> 01:41:32,440
model i c fc or the i e uh ai ea i think there are a couple of models proposed i think what's

946
01:41:32,440 --> 01:41:38,680
important is to not um put it in one hand uh not in the hand of a few corporates but also not

947
01:41:38,680 --> 01:41:43,080
in the hands of one state so the current race is not healthy and we need to think about how

948
01:41:43,080 --> 01:41:49,880
we get them back to the table i know you you have some ideas about this and uh yeah we we as i cfg

949
01:41:49,880 --> 01:41:56,920
are trying to show and build the scenarios to explain what it means to look at a future where

950
01:41:56,920 --> 01:42:01,800
emerging tech is governed and what it means when you look at a future where emerging tech is not

951
01:42:01,800 --> 01:42:07,720
governed and that this will hopefully help decision makers come together and work together because

952
01:42:07,720 --> 01:42:12,040
there are many other emerging technologies that might disrupt our society in many other ways

953
01:42:12,040 --> 01:42:17,640
too just around the corner absolutely so we had this discussion shortly before the panel because

954
01:42:18,600 --> 01:42:24,840
yeah uh ai is of course a turbo charger for a number of technologies that are being developed

955
01:42:24,840 --> 01:42:31,480
at a fast pace so we are also looking at mirror technology and quantum and biotech and i mean a

956
01:42:31,480 --> 01:42:36,200
lot of here in the room are looking at different technologies but the power come from the combination

957
01:42:36,200 --> 01:42:45,480
of those and they also round the corner ron final remarks yes yes it works um first i agree that we

958
01:42:45,480 --> 01:42:50,440
should both look into both the models and the applications not one of them but um what i think

959
01:42:50,440 --> 01:42:56,120
is that we yes we need science but we do not have decades um so we need some sort of form of a

960
01:42:56,120 --> 01:43:01,320
rapid response mechanism and in that we need a credible helix we need both government civil

961
01:43:01,320 --> 01:43:10,280
society we need science um and and we need all of them at the table thanks closing words yana

962
01:43:14,120 --> 01:43:19,480
okay one thing i would say about the regulation issue i think uh friends we most of it's like

963
01:43:19,480 --> 01:43:24,760
he has this concept of dial-up progress uh that a lot of conversations end up in like

964
01:43:24,760 --> 01:43:29,800
do we need more progress or less progress uh which is kind of like way too black and white

965
01:43:30,600 --> 01:43:34,920
way of looking at things what you actually want to do is like look look like there are different

966
01:43:34,920 --> 01:43:39,800
ways uh where we want more progress and different places where we want want less progress so it's

967
01:43:39,800 --> 01:43:44,440
actually completely consistent to believe as i believe that yes we have over like over-regulated

968
01:43:44,440 --> 01:43:51,480
a lot of things in a way that is kind of detrimental uh for us but that doesn't mean that we really

969
01:43:51,480 --> 01:43:56,520
should stop regulating things and new things as they come up thanks so please stay on the

970
01:43:56,520 --> 01:44:02,760
stage for a moment we're going to have a few closing words from Otto who is the head of

971
01:44:02,760 --> 01:44:09,880
ero which is part of the organization that has made this happen Otto are you here

972
01:44:10,840 --> 01:44:17,800
yes and by the way this discussion is a prelude to an even more important discussion which is

973
01:44:17,800 --> 01:44:21,880
going to be taking place in the pub in the good old british tradition afterwards some of you might

974
01:44:21,880 --> 01:44:27,320
want to join us where we can get around to all of you who had hand ups and i unfortunately couldn't

975
01:44:27,320 --> 01:44:42,920
take your question Otto thank you David sorry um yeah thanks and thank you all of so so much

976
01:44:42,920 --> 01:44:49,960
for being present here today um as some of us has already mentioned we're here in wilton hall

977
01:44:49,960 --> 01:44:55,320
this was built in 1943 as an assembly hall for the world for two codebreakers and while deciphering

978
01:44:55,320 --> 01:45:01,400
they have progressed beyond imagination and borrow multiple exponential curves here hardware

979
01:45:01,400 --> 01:45:06,840
data quantity algorithm capabilities are all growing with tens of percentage points per year

980
01:45:07,720 --> 01:45:14,760
so i think we all or at least a lot in this room will suspect where this leads uh which is a i that

981
01:45:14,760 --> 01:45:20,440
has the capability to do mental tasks much better than we can and of course this presents amazing

982
01:45:20,440 --> 01:45:26,200
opportunities but according to most existential risk experts we also risk nothing short of human

983
01:45:26,200 --> 01:45:33,240
extinction here and it does mean that our species is on the line so when i turned on the radio last

984
01:45:33,240 --> 01:45:39,000
weekend the bbc was discussing human extinction by ai and i think that this was dramatic but also

985
01:45:39,000 --> 01:45:45,400
hopeful at the same time so i thought this was dramatic since human extinction caused by our

986
01:45:45,400 --> 01:45:52,040
own actions is now officially a possibility and it never ceases to amaze me that we have been stupid

987
01:45:52,040 --> 01:45:58,440
enough to let it get this far but hearing this discussed on national radio for me was also extremely

988
01:45:58,440 --> 01:46:05,160
hopeful because up until now attempts to reduce the real human extinction risks were minor and

989
01:46:05,160 --> 01:46:10,680
world leaders were not paying attention and with the summits that's starting tomorrow i think this

990
01:46:10,680 --> 01:46:17,160
is really changing so i think it's hopeful that after the uk's prime minister speech on ai last

991
01:46:17,160 --> 01:46:22,040
week in which he explicitly warned of human extinction risks the questions that followed

992
01:46:22,040 --> 01:46:28,360
from the press were no longer about prime minister is this a real concern shouldn't you be concerned

993
01:46:28,440 --> 01:46:36,280
about the bills of the of your people instead of this but instead at least some of the questions

994
01:46:36,280 --> 01:46:41,480
were about are you addressing this problem seriously enough and shouldn't we consider instead

995
01:46:41,480 --> 01:46:47,880
posing ai a moratorium or are you doing the right thing with backing responsible scaling

996
01:46:49,000 --> 01:46:54,040
and i think this is exactly the debate that we need so i think it's now important that we

997
01:46:54,040 --> 01:47:00,280
continue in this direction so we must organize ai's safety summits much more often we must open

998
01:47:00,280 --> 01:47:06,200
them up so everybody gets to say we must have societal debates about this and i think in general

999
01:47:06,200 --> 01:47:11,960
we must come together to coordinate and if we do that we are confident at the extension risk

1000
01:47:11,960 --> 01:47:16,840
observatory that we can implement the measures that are needed and this is why we have organized

1001
01:47:16,840 --> 01:47:22,760
this event and i think it's a huge privilege that we're able to do this together with conjecture

1002
01:47:22,760 --> 01:47:29,000
so thank you so much for co-organizing this event and we also want to continue organizing events

1003
01:47:29,000 --> 01:47:34,680
like this one but it's impossible without the support of all of you so if you want to support us

1004
01:47:34,680 --> 01:47:41,000
doing this there was a flyer that you got handed at the beginning please scan the QR code there

1005
01:47:41,000 --> 01:47:46,520
and there's possibilities to support us could be with funding could be with volunteering

1006
01:47:46,520 --> 01:47:50,840
could be with just following and sharing our content so this is enormously appreciated

1007
01:47:51,800 --> 01:47:56,920
and with that can i please get some applause for all our amazing speakers professor Stuart Russell

1008
01:47:59,400 --> 01:48:02,600
Connolly Professor Max Teckmark

1009
01:48:04,120 --> 01:48:07,880
Jan Tellin Annika Brack Mark Brackle Ron Rosendell

1010
01:48:07,880 --> 01:48:11,640
Alexandra Moussevisidae Andrea Mariotti and Helhotzen please don't stop clapping

1011
01:48:11,640 --> 01:48:25,800
um and finally a special thanks as well to David Wood a moderator Ruben Dileman, Katrina Joslyn,

1012
01:48:25,800 --> 01:48:31,240
Connor Axiotis, Sue Chisholm, Tillman Schepke, Niky Drogdowski, Mark van der Waal and Joep Soeren

1013
01:48:31,240 --> 01:48:34,760
and everyone here at Wilton Hall who made this all possible thanks a lot for helping us out

1014
01:48:41,640 --> 01:48:48,680
and please join us for drinks at Three Trees which is about 10 to 15 minutes

1015
01:48:48,680 --> 01:48:51,160
more from here so i hope to see you all there thank you

1016
01:48:55,080 --> 01:48:59,160
and some people believe in the future there's going to be a wedding in here shortly

1017
01:48:59,160 --> 01:49:03,320
so we all need to get out unfortunately unless we're part of that wedding crowd

1018
01:49:03,320 --> 01:49:14,280
so biome is chat but chat whilst moving out thank you

