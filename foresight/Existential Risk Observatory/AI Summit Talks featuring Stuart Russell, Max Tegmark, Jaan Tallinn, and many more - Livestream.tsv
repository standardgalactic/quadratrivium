start	end	text
0	7120	and to me it's not clear right are we at the right brother stage or are we at
7120	15560	the Mongolfi a stage where we have a lot of hot air and so my current view is no
15560	25480	we have not succeeded and the models that people are excited about the large
25480	31320	language models and their extensions into multimodal models that take in video
31320	37480	and can actually operate robots and so on that these are a piece of the puzzle
37480	44200	and and CNN made this lovely animated gif here to to illustrate this idea that
44200	49200	we don't really know what shape the piece of the puzzle is and we don't know
49200	54160	what other pieces are needed and how it fits together to make general purpose
54160	60560	intelligence we may discover what's going on inside the large language models
60560	66720	we may figure out what source of power they're drawing on to to create the
66720	71920	kinds of surprisingly capable behaviors that they do exhibit but at the moment
71920	80200	that remains a mystery and there are some gaps right one of the achievements of
80200	86320	modern AI that people were most proud of and also most certain of was the defeat
86320	96320	of human go champions by alpha go and then alpha zero in the 2016 to 2018
96320	103840	period so in go for those of you don't know there's a board you put pieces on
103840	108600	and your goal is to surround territory and to surround your opponents pieces
108600	116840	and capture them and since AI systems beat the world champion in 2017 they've
116840	123160	gone on to leave human race in the dust so the highest rank program is catago
123160	128480	and its rating is about five thousand two hundred compared to the human world
128480	135120	champion at three thousand eight hundred and the human world champion leaves our
135160	142960	colleague kelin pelrin who's a grad student a decent amateur go player his
142960	150680	ratings about 2300 and now I'll show you a game between kelin and catago where
150680	155040	kelin actually gives catago a nine stone handicap so catago is black and
155040	160160	starts with nine stones on the board right if you're an adult go player and
160160	163280	you're teaching a five-year-old how to play go you give them a nine stone
163320	167560	handicap so that at least they can stay in the game for a few minutes right so
167560	175760	here we are treating treating catago as if it's a baby okay despite the fact that
175760	183760	it's massively superhuman and and here's the game so it's speeded up a little bit
183760	190000	but watch what happens in the bottom right corner so white the human being is
190040	195080	going to start building a little group of stones there they go and then black
195080	200880	very quickly surrounds that group to make sure that it can't grow and also to
200880	205280	actually have a pretty good chance of capturing that group but now white starts
205280	210240	to surround the black stones and interestingly black doesn't seem to pay
210240	214640	any attention to this it doesn't understand that the black stones are in
214640	218240	danger of being captured which is a very basic thing right you have to
218280	222440	understand when your opponent is going to capture your pieces and black just
222440	228040	pays no attention and loses all of those pieces and that's the end of the game
228040	235400	so something weird happens there right where an ordinary human amateur go
235400	240640	player can beat a go program that stratospherically better than any human
240640	245880	being has ever been in history right and in fact the go programs do not
246240	250960	correctly understand what it means for a group of stones to be alive or dead
250960	258320	which is the most basic concept in the game of go they have only a limited
258320	265160	fragmentary approximation to the definition of life and death and that's
265160	270840	actually a symptom of one of the weaknesses of training circuits to learn
270840	275640	these concepts circuits are a terrible representation for concepts such as
275720	280040	life and death which can be written down in python in a couple of lines can be
280040	285480	written in logic in a couple of lines but in circuit form you can't actually
285480	289800	write a correct definition of life and death at all you can only write finite
289800	295640	approximations to it and the systems are not learning a very good approximation
295640	300840	and so they are very vulnerable and this turns out to be applicable not just a
300840	305360	category but to all the other leading go programs which are trained by completely
305360	309600	different teams on completely different data using different training regimes
309600	316400	but they all fail against this very simple strategy so this suggests that
316400	323120	actually the systems that we have been building are we are overrating them in a
323120	329520	real sense and I think that's important to understand and human beings right
329520	335320	another way to make this argument is look at things that humans can do for
335360	341480	example we can build the large interferometric gravitational observatory
341480	346360	so these are black holes colliding on the other side of the universe this is
346360	354400	the the LIGO detector and which is several kilometers long it's full of
354400	360800	physics and is able to detect distortions of space down to 18 the 18th
360880	367640	decimal place and was able to actually measure exactly what the physicists
367640	373040	predicted would be the shape of the waveform arriving from the collision of
373040	376840	two black holes and was even able to measure the masses of the black holes
376840	385360	on the other side of the universe when they collided so could chat gpt do
385400	393080	this could any deep learning system do this given that there are exactly zero
393080	400200	training examples of a gravitational wave detector I think at the moment there
400200	408640	is still a long way to go on the other hand people are extremely ingenious and
408640	413440	people are working on hybrids of large language models with reasoning and
413440	419080	planning engines that could start to exhibit these capabilities quite soon
419080	425840	so people I respect a great deal think we might only have five years until this
425840	432360	happens almost everyone has now gone from 30 to 50 years which was the
432360	439840	estimate a decade ago to five to 20 years which is the estimate right now so
439880	444880	unlike fusion this is getting closer and closer and closer rather than further
444880	449560	and further into the future so we have to ask what happens if we actually
449560	456760	succeed in creating general purpose AI and the reason we are trying to do it
456760	462160	is because it could be so transformative to human civilization very crudely our
462160	466560	civilization results from our intelligence if we have access to a lot
466560	471440	more we could have a lot better civilization one thing we could do is
471440	477520	simply deliver what we already know how to deliver which is a nice middle class
477520	480880	standard living if you want to think of it that way we could deliver that to
480880	487000	everyone on earth at almost no cost and that would be about a tenfold increase
487160	497280	in GDP and the net present value of that is 13.5 quadrillion dollars so that's a
497280	503200	lower bound on the cash value of creating general purpose AI so if you want
503200	506880	to understand why we're investing hundreds of billions of pounds in it
506880	516960	it's because the value is millions of times larger than that and so that creates
517000	523320	a magnet in the future that is pulling us forward inexorably my friend Jan
523320	528600	Tallinn here likes to call this mollock right this sort of ineluctable force
528600	534040	that draws people towards something even though they know that it could be their
534040	539840	own destruction and we could actually have an even better civilization right
539840	547680	we could have we could one day have a clicker that works we could have health
547680	552520	care that's a lot better than we do now we could have education that could be
552520	559360	brought to every child on earth that would exceed what we can get from even a
559360	565200	professional human tutor this I think is the thing that is most feasible for us
565280	570280	to do that would benefit the world in this decade and I think this is entirely
570280	574640	possible health care is actually a lot more difficult for all kinds of reasons
574640	581160	but education is a digital good that can be delivered successfully and we could
581160	589200	also have much better progress in science and so on so on the other hand AI
589200	595080	amplifies a lot of difficult issues that policymakers have been facing for quite
595080	605480	a while so one is its ability to magnify the pollution of our information
605480	611960	ecosystem with disinformation what some people call truth decay and this is
611960	617560	happening at speed but if we thought about it really hard I could actually
617560	621960	help in the other direction it could help clean up the information ecosystem
621960	626760	it could be used as a detector of misinformation as something that
626760	632040	assembled consensus truth and made it available to people we're not using it
632040	638600	in that way but we could ditto with democracy is it being suppressed by
638600	644400	surveillance and control mechanisms or could we use AI systems to strengthen it
644400	651720	to allow people to deliberate cooperate and reach consensus on what to do could
651720	658040	it be that individuals are empowered or the current trajectory that we're on
658040	663120	individuals being enfeebled as we gradually take over more and more of the
663120	669040	functions of civilization and and humans lose the ability to even run their own
669040	675480	civilization as individuals right these are important questions that we have to
675480	679120	address while we're considering all of the safety issues that I'll be getting
679120	684520	to soon there's inequality right now we're on the path of magnifying it with
684520	694840	AI but it doesn't have to be that way and so on so let me I won't go through all
694840	699600	of these issues because they're they're all each of them worthy of an entire
699640	711360	talk in themselves so the I would say the sort of the mid term question is what
711360	716760	are humans going to be doing right if we have general purpose AI that can do all
716760	722800	the tasks or nearly all the tasks that human beings get paid for right now what
722800	728480	will humans do and this is not a new issue Aristotle talked about it in 350
728480	735080	BC Keynes since we're in Milton it's odd that we pronounce it milk Milton Keynes
735080	741080	but he his name is pronounced Keynes even though the town is named after him so so
741080	745600	Keynes in 1930 said thus for the first time since his creation man will be
745600	750240	faced with Israel his permanent problem how to use his freedom from pressing
750240	754400	economic cares which science will have one for him to live wisely and agreeably
754400	760640	and well so this is a really important problem and again this is one that
760640	767960	policymakers are misunderstanding I would say that the default answer in most
767960	774120	governments around the world is will retrain everyone to be a data scientist
774120	778960	as if somehow the world needs three and a half four billion data scientists I
778960	784880	think that's probably not the answer but this is again you know the default path
784880	792520	is one of enfeeblement which is illustrated really well by by Wally so
792520	799240	my my answer to this question is that in the future if we are successful in
799240	805240	building AI that is safe that does a lot of the tasks that we want done for us
805320	810600	most human beings are going to be in these interpersonal roles and for those
810600	817720	roles to be effective they have to be based on understanding right why is a
817720	823440	surgeon effective at fixing a broken leg because we have done centuries of
823440	830960	research in medicine and surgery to make that a very effective and in some
831000	836880	countries very highly paid and very prestigious but most into personal roles
836880	842160	for example think about childcare or elder care not highly paid not highly
842160	846600	prestigious because they are based on no science whatsoever despite the fact
846600	850760	that our children are our most precious possessions as people politicians like
850760	856200	to say a lot in fact we don't understand how to look after and we don't
856200	861000	understand how to make people's lives better so this is a a very different
861000	866280	direction for science much more focused on the human than on the physical
866280	878920	world okay so now let me move on if I can get the next light up to to Alan
878920	885080	Shuring's view of all this what happens if we succeed he said that it seems
885080	888920	parable the ones machine thinking method has started it would not take long to
888920	894040	outstrip our feeble powers at some stage therefore we should have to expect the
894040	902680	machines to take control so he said this in 1951 and to a first approximation for
902680	910280	the next 70 odd years we paid very little attention to what his warning was
911160	916520	and I used to illustrate this with the following imaginary email conversation
918600	925480	so an alien civilization sends email to the human race humanity at un.org be
925480	930040	warned we shall arrive in 30 to 50 years that was what most AI people thought back
930040	937720	then now we would say maybe 10 to 20 years and humanity replies humanity is
937720	941400	currently out of the office who will respond to your message when we return
942520	949080	and then there should be a smiley face there it is okay so that's now changed
951000	954840	unfortunately that slide wasn't supposed to come up like that let me see if we can
954840	960600	oh well can't fix it now so I think early on this year three things happened in
960600	966120	very quick succession so GPT-4 was released and then Microsoft which had been working
966120	971720	with GPT-4 for several months at that point published a paper saying that GPT-4
971720	977320	exhibited sparks of artificial general intelligence exactly what Turing warned us about
978440	985240	and then FLI released the open letter asking for a pause on giant AI experiments
986200	993800	and I think at that point very clearly humanity returned to the office and they saw the emails
993800	1000920	from the aliens and the reaction since then I think has been somewhat similar to what would
1000920	1008120	happen if we really did get an email from the aliens there have been a global calls for action
1008120	1014840	the very next day UNESCO responded directly to the open letter asking all its member governments
1014840	1021400	which is all the countries on earth to immediately implement the AI principles in legislation
1022120	1027560	in particular principles that talk about robustness safety predictability and so on
1028760	1035320	and then you know there's China's AI regulations the US got into the act very quickly the White
1035320	1043560	House called emergency meeting of AI CEOs open AI calling for governments to regulate AI and so on
1044120	1049240	and I ran out of room on the slide on June 7th with Rishi Sunak announcing the global summit on
1049320	1055160	AI safety which is happening tomorrow so lots of other stuff has happened since then
1056040	1062360	but it's really I would say to to the credit of governments around the world how quickly
1063080	1070680	they have changed their position on this for the most part governments were saying you know
1070680	1079080	regulation stifles innovation you know if someone did mention risk it was either dismissed
1079720	1085720	or or viewed as something that was easily taken care of by the market by liability and so on
1086360	1092280	so I would say that the the the view the understanding has changed dramatically and that could not have
1092280	1101400	happened without the fact that politicians started to use chat GBT and they saw it for themselves
1102440	1105000	and I think that changed people's minds
1107720	1116120	so the question we have to face then is this one right how do we retain power over entities more
1116120	1121240	powerful than ourselves forever right and I think this is the question that Turing asked himself
1122840	1126680	and gave that answer we would have to expect them to take control so in other words
1127400	1133000	this question doesn't have an answer but I think there's another version of the question
1133000	1136520	which works somewhat more to our advantage
1138920	1147720	it should appear any second if right and it has to do with how we define what we're trying to do
1148280	1156440	what is the system that we're building what problem is it solving and we want a problem
1156440	1162680	such that we we set up an AI system to solve that problem so the standard model that I gave you
1162680	1169640	earlier was systems whose actions can be expected to achieve their objectives and that's exactly
1169640	1175480	where things go wrong that systems are pursuing objectives that are not aligned with what humans
1175480	1180360	want the future to be like and then you're setting up a chess match between humanity
1180360	1187720	and a machine that's pursuing a misaligned objective so instead we want to figure out
1188520	1195800	a problem whose solution is such that we're happy for AI systems to instantiate that solution
1197240	1204200	okay and it's not imitating human behavior which is what we're training LLMs to do that's actually
1204200	1213000	the fundamental and basic error and that's essentially why we can't make LLMs safe
1213800	1217160	because we have trained them to not be safe and trying to put
1219480	1224920	trying to put sticking plasters on all all the problems after the fact is never going to work
1225800	1231160	so instead I think we have to build systems that are provably beneficial to humans and
1231720	1237320	the way I'm thinking about that currently is that the system should act in the best interests of
1237320	1245640	humans but be explicitly uncertain about what those best interests are and this this I'm just
1245640	1252120	telling you in English and it can be written in a formal framework called an assistance game
1252840	1260280	so what we do is we build assistance game solvers we don't build objective maximizers
1260280	1265480	which is what we have been doing up to now we build assistance game solvers this is a different
1265480	1270440	kind of AI system when we've only been able to build very simple ones so far so we have a long
1270440	1277320	way to go but when you build those systems and look at the solutions they exhibit the properties
1277320	1283000	that we want from AI systems they will defer to human beings and in the extreme case they will
1283000	1288280	allow themselves to be switched off in fact they want to be switched off if we want to switch them
1288280	1294040	off because they want to avoid doing whatever it is that is making us upset they don't know what it is
1294040	1299480	because they're uncertain about our preferences but they want to avoid upsetting us and so they
1299480	1304680	are happy to be switched off in fact this is a mathematical theorem they have a positive
1304680	1311080	incentive to allow themselves to be switched off and that incentive is connected directly
1311080	1314280	to number two right the uncertainty about human preferences
1316360	1323000	so there's a long way to go as I said and we're not ready to say okay everyone in all these
1323000	1328200	companies stop doing what you're doing and start building these things instead right that probably
1328200	1332520	is not going to go down too well because we don't really know how to build these things at scale
1333400	1338920	and to deliver economic value but in the long run this is the right way to build AI systems
1339400	1345880	so in between what should we do and this is a lot about what's going to be discussed tomorrow
1347800	1351720	and there's a lot so this is in a small font I apologize to those of you at the back there's
1351720	1358680	a lot to put on this slide we need first of all a cooperation on AI safety research it's got to
1358680	1365240	stop being a cottage industry with a few little academic centers here and there it's also got to
1365240	1373240	stop being what a cynic might describe as a kind of whitewashing operation in companies
1373240	1379400	where they try to avoid the worst public relations disasters like you know the language model used
1379400	1387960	a bad word or something like that but in fact those efforts have not yielded any real safety
1387960	1393640	whatsoever so there's a great deal of research to do on alignment which is what I just described
1393640	1399240	on containment how do you get systems that are restricted in their capabilities that are not
1399800	1404920	directly connected to email and bank accounts and credit cards and social media and all those
1404920	1411960	things and if there are I think there are probably ways of building restricted capability systems
1412600	1417960	that are provably safe because they are restricted to only operate
1418200	1427400	uh provably sound reasoning engines for example um but the the bigger point is stop thinking about
1427400	1438680	making AI safe start thinking about making safe AI right these are just two different mindsets
1438680	1444840	the making AI safe says we build the AI and then we have a safety team whose job it is
1444920	1450680	to stop it from behaving badly that hasn't worked and it's never going to work we have
1450680	1456680	got to have AI systems that are safe by design and without that we are lost
1459400	1466040	we also need I think some international regulatory level to coordinate the regulations
1466040	1471800	that are going to be in place across the various national regimes so we have to start
1472360	1478600	probably with national regulation but and we can coordinate very easily for example we could
1478600	1486520	start coordinating tomorrow to agree on what would be a baseline for regulation I put a couple of
1486520	1497560	other things there that went by too quickly so I actually want to go back okay too far all right
1498040	1505560	um so the the light blue line transparent explainable analytical substrate is really
1505560	1511800	important uh the moment we're building AI systems that are black boxes we have no idea how they work
1511800	1516760	we have no idea what they're going to do and we have no idea how to get them to behave themselves
1516760	1526600	properly uh so my guess is that if we define regulations appropriately so that companies
1527320	1534040	have to build AI systems that they understand and predict and control successfully those
1534040	1540200	AI systems are going to be based on a very different technology not giant black box circuits
1540200	1547000	that are trained on vast quantities of data but actually well understood component based
1547000	1553320	systems that build on centuries of research in logic and probability where we can actually
1553320	1559720	prove that these systems are going to behave in certain ways the second thing the dark blue
1559720	1565400	secure PCC based digital ecosystem what is that uh so PCC is proof carrying code
1565640	1572120	and what we need here is a way of preventing bad actors from deploying unsafe systems so it's one
1572120	1578120	thing to say here's how you build safe systems and everyone has to do that it's another thing to
1578120	1584200	say how do you stop people from deploying unsafe systems who don't want safe AI systems they want
1584200	1592600	whatever they want this is probably even more difficult policing software is I think impossible
1592600	1601080	is I think impossible so the the place where we do have control is at the hardware level because
1601080	1606440	hardware uh first of all to build your own hardware costs about a hundred billion dollars
1607080	1614520	and tens of thousands of highly trained engineers so it provides a control point that's very difficult
1614520	1620280	for bad actors to get around and what the hardware should do is basically check the proof
1621080	1623400	of a software object before it's run
1625560	1628920	and check that in fact this is a safe piece of software to run
1629880	1635560	and proof carrying code is a technology that allows hardware to check proofs very efficiently
1636280	1641640	but of course the owners then is on the developer to provide a proof that their system is in fact
1641640	1650920	safe and so that's a prerequisite for this approach okay let me talk a little bit about
1650920	1661960	regulations so a number of acts already in in the in the works for example the european AI act
1662680	1668200	has a hard ban on the impersonation of human beings so you have a right to know if you're
1668200	1673080	interacting with a machine or a human this to me is the easiest the lowest hanging fruit
1673800	1679960	that every jurisdiction in the world could implement pretty much tomorrow if they so decided
1680680	1688280	and I believe that this is how legislators wake up those long unused muscles that have
1688280	1694120	lain dormant for decades while technology has just moved ahead unregulated so this is the place to
1694200	1703080	start but we also need some regulations on the design of AI systems specifically so
1703800	1709800	a provably operable kill switch is a really important and basic thing if your system is
1709800	1716600	misbehaving there has to be a way to turn it off and this has to apply not just to the system that
1716600	1723320	you made but if it's an open source system any copy of that system and that means that the kill
1723320	1728440	switch has got to be remotely operable and it's got to be non-removable
1731400	1735880	so that's a technological requirement on open source systems and in fact if you want to be in
1735880	1741000	the open source business you're going to have to figure this out you're actually going to subject
1741000	1748680	yourself to more regulatory controls than people who operate on closed source and that's exactly
1748680	1755720	as it should be imagine if we had open source enriched uranium right and the purveyor of enriched
1755720	1761800	uranium was responsible for all the enriched uranium that they pervade to anybody around the
1761800	1767080	world they're going to have a higher regulatory burden because that's a blinking stupid thing to
1767080	1772280	do right and so you would expect there to be a higher burden if you're going to do blinking stupid
1772280	1779960	things um and then red lines this is probably the most important thing so we don't know how to
1779960	1786360	define safety so i can't write a law saying your system has to be provably safe because it's very
1786360	1793000	hard to write the dividing line between safe and unsafe you know if you uh asimov's law you can't
1793800	1800520	harm human beings well what does harm mean that's very hard to define but we can scoop out
1801320	1808280	very specific forms of harm that are absolutely unacceptable so self-replication of computer
1808280	1814040	systems would absolutely be unacceptable that would be basically a harbinger of losing human
1814040	1820840	control if the system can copy itself onto other computers or break into other computer systems
1821480	1826120	absolutely systems should not be advising terrorists on building biological weapons
1826120	1831400	and so on so these red lines are things that any normal person would think well obviously
1833000	1839480	the software system should not be doing that and the developers are going to say oh well this is
1839480	1843880	really unfair because it's really hard to make our systems not do this
1846040	1854040	and their response is well tough right really you're spending hundreds of billions of pounds
1854040	1858840	on this system and you can't stop it from advising terrorists on building bioweapons
1859400	1867640	well then you shouldn't be in business at all right this is not hard and legislatures by uh by
1867640	1874600	implementing these red lines would put the onus on the developer to understand how their own systems
1874600	1880520	work and to be able to predict and control their behavior which is an absolute minimum we should
1880520	1888440	ask from any industry let alone one that could have such a massive impact and is hoping for
1888440	1900600	quadrillions of dollars in profits thank you thank you very much professor russell quick
1900600	1906200	question maybe before we move on to the next speaker there was some good news in there it is
1906200	1911000	that we have ideas on how to make safe ai but how long do you think we're going to need
1911640	1915240	how long is it going to take by default that we have these ideas worked out and how long
1915240	1920920	might it take if we had all the smart people in the world give up their current focus and instead
1920920	1928840	work on this uh i think these are really important questions because the um the political dynamic is
1928840	1934760	going to depend to some extent on how the ai safety community responds to this challenge
1934760	1941000	uh because if the ai safety community fails to make progress on any of this stuff the developers
1941000	1947240	can point and say look you know you guys are asking for stuff that isn't really possible
1947240	1952600	and we should be allowed to just do what we want um but if you look at the nuclear industry right
1952600	1959160	how does that work the regulator says to the nuclear plant operator show me that your plant
1959160	1965960	has a meantime to failure of 10 million years or more and the operator has to give them a full
1965960	1973240	analysis with fault trees and a probabilistic uh calculations and the regulator can push back and
1973240	1977720	say you know i don't agree with that independence assumption you know these components come from
1977720	1982920	the same manufacturer so not independent and come back with a better analysis and so on
1983560	1990840	uh at the moment there is nothing like that in the ai industry there is no logical connection
1991560	1996840	between any of the evidence that people are providing and the claim that the system is
1996840	2005160	actually going to be safe right that argument is just missing um now the nuclear industry
2005560	2013080	probably spends more than 90 percent of its r&d budget on safety
2015080	2019880	one way you can tell i i got the statistic from one of my nuclear engineering colleagues
2020440	2026680	that for the typical nuclear plant in the u.s for every kilogram of nuclear plant
2027240	2032120	there are seven kilograms of regulatory paperwork i kid you not
2032200	2040840	so that tells you something about how much of an emphasis that has been on safety in that industry
2042360	2049880	and also you know why is there to a first approximation no nuclear industry today
2050520	2057800	is because of Chernobyl and because of a failure in safety actually deliberately bypassing
2058760	2062600	safety measures that they knew were necessary in order to save money
2063720	2067080	we'll take one question from the audience provided it's a quick question
2067800	2072440	i see a hand over there let me dash down
2076040	2080280	hi uh thanks very much for your talks here um my name is Charlie i'm a senior at UCL
2080920	2085400	um one of the big reasons i think why there's so much regulation on nuclear power
2085880	2092360	is widespread public opinion and protests against nuclear power from within the environmental
2092360	2098040	movement so i wondered whether you uh thought if there's a similar role for public pressure or
2098040	2106520	protests uh for ai as well thanks uh i think that's a very important question
2107160	2118360	my sense is i i'm not really historian of the nuclear industry per se uh obviously nuclear
2118360	2124360	physicists thought about safety from the beginning uh in fact so Leo Leo Zillard was the one who
2124920	2130840	invented the basic idea of the nuclear chain reaction uh and he instantly thought about
2130840	2135640	a physical mechanism that could keep the reaction from going supercritical and becoming a bomb
2136360	2139880	right so he thought about this you know negative feedback control system
2140680	2145560	with moderators that would somehow keep the reaction subcritical
2148040	2154760	people in ai are not at that stage right or they just have their eyes on you know we can
2154760	2158760	generate energy and they're not even thinking you know is that energy going to be in the form
2158760	2164680	of a bomb or electricity right they haven't got to that stage yet so we are very much at the
2164680	2177080	preliminary stage i do worry that ai should not be politicized and at the moment there's
2177080	2184680	a precarious bipartisan agreement in the us and to some extent in europe i worry about that
2184680	2191400	breaking down in the uk uh i think it's really important that the political message be very
2191400	2198200	straightforward you can be on the side of humans or you can be on the side of our ai overlords
2199560	2208520	which do you want to be on um and so let's try to keep it a unified message around uh
2209720	2216200	developing technology in a way that's safe and beneficial for humans um so we can raise
2216760	2224520	but we shouldn't do it in a partisan way yes and and what i i i totally sympathize with the idea
2224520	2231240	that people have a right to be very upset that you know that multibillionaires are playing
2232600	2237720	you know playing poker with the future of the human race um it's entirely reasonable
2237960	2248600	but what i worry is exactly that uh certain types of of protest end up getting aligned
2249320	2256840	in a way that's unhealthy uh it sort of becomes anti technology and we can look back at what happened
2256840	2265640	with with uh gm uh organisms for example uh which which most scientists think didn't go the way
2266360	2271080	uh it should have and we we lost benefits uh without gaining any safety
2272600	2275320	watch to think about there thank you very much professor stewart russell
2278120	2281800	we may give you the microphone again a bit later on but there's lots of other people we want to
2281800	2288840	hear from now so the next speaker is kona lehi who is the ceo of conjecture many of us got a shock
2288840	2296280	with gp24 or 3.5 my goodness what's going on here kona was ahead of the curve when he saw gpg2
2296280	2301080	with all its warts and weaknesses he said my gosh this is going to change the world so he has been
2301080	2305880	thinking about some of these issues probably for longer than the rest of us so let's hear from kona
2305880	2317240	what would you like to say thank you so much so unfortunately uh professor russell has stole
2317240	2322840	my favorite allen turing quote so you're going to be hearing that one again but i guess there couldn't
2322840	2331800	be a more appropriate time because many years ago there lived a man named allen turing he was the
2331800	2340680	godfather of computer science a titan in his field and a hero of world war two and was here
2340680	2346680	at bletchley park that he did his most seminal work during the world war two and cracking the
2346680	2353000	codes that the germans were using and as a very early step into the field of computer science
2355160	2364760	and allen was ahead of his time in more way than one in 1951 in manchester he gave a lecture
2364760	2376200	entitled intelligent machinery a heretical theory and in this lecture he said for it seems probable
2376200	2381000	but once the machine thinking method had started it would not take long to outstrip
2381000	2386120	our feeble powers there would be no question of the machine's dying and it would be able to
2386120	2392920	converse with each other to sharpen their wits at some stage therefore we should have to expect
2392920	2401880	the machines to take control and here we are 72 years later where it all began
2402520	2410760	and a lot has changed since the days of allen turing computers have improved in incredible rates
2411400	2418120	i'm holding my hands right now a computer of such incredible power that it would be barely
2418120	2427240	imaginable to turing in his contemporaries barely one human lifetime hints and while computers have
2427240	2433080	advanced a lot in many ways since the days of turing i like to believe that there would be a lot
2433080	2442760	he would recognize he would recognize the basic functions of computers their memory their instructions
2442760	2448680	programming code ideas that go all the way back to his seminal work on turing machines
2449400	2454360	while he might not be familiar with the exact tooling he would be familiar with the general
2454360	2462120	concepts around modern programming where a programmer writes code instructions the computer
2462120	2469800	then executes but there is something that i'm not so sure he would so easily recognize
2471400	2481160	and that is a i or in particular the neural networks that power them now we have all seen
2481160	2486760	ai do truly amazing and things over the last couple of years in particular solving all these
2486760	2492600	problems that previously we barely knew how to approach and you might think when you look at
2492600	2498360	all these ai systems running on your phone on your computer that this is software like any other
2498360	2503480	written by very clever programmers to do the useful and the marvelous things that they do
2504440	2512120	and you would be wrong because ai is very different from normal software
2513160	2519480	it is not written so much as it is grown so while in the traditional software you'd have
2519480	2527640	a programmer sit down and write out the instruction with ai's you take huge supercomputers and massive
2527800	2535160	data sets and you use these supercomputers to grow a program on your data to solve your problem
2536200	2542040	and this works really well for many for many issues it has improved our ability to solve many
2542040	2548360	very useful tasks and do many things that we did not know how to do before and our ability to
2548920	2554520	grow these ai's continues to improve and get better and better while at the same time though
2554520	2562840	our ability to understand our ai's has not because these ai's are not like well written code
2562840	2569960	that a human could read they're more like giant blobs of numbers and we know if we execute them
2569960	2582280	they work but we have no idea why and only quite recently have we discovered that as we scale up
2582280	2590280	these systems and as we build bigger computers and bigger ai systems something quite remarkable
2590280	2600600	happens they become more intelligent more capable now of course there are many details
2600600	2605720	that have to be gotten right there are many parameters you have to set correctly you have
2605720	2609800	to have enough data you have to make sure your computer is set up correctly but fundamentally
2610760	2616120	is a stability in this prediction sometimes also called the scaling laws and that as our
2616120	2622040	systems become bigger as our computers become more powerful the systems learn higher and
2622040	2632600	higher order patterns more and more complex skills knowledge abilities and as they become more powerful
2632600	2639400	and more capable they are also becoming even harder to understand and to control
2641960	2648760	and this is why we are all here today back to where it all began we have now returned
2649400	2658840	Bletchley Park because as Turingell really realized it is really quite simple if we build
2658840	2667560	machines that are more competent than us at manipulation perception politics business science
2668200	2676040	and everything else and we do not control them then the future will belong to the machines
2677400	2684120	not to humans and the machines are unlikely to feel particularly sentimental about keeping us
2684120	2692600	around for very long and so here we are face with an exponentially increasing more powerful
2692600	2699880	by the day ai by the day as we learned with covid there are exactly two times one can react
2699880	2709480	to an exponential too early or too late if we wait until agi if we wait until we see the
2709480	2718920	self-improving powerful general purpose systems it will be too late far far too late and this is
2718920	2725480	why i am so happy to see the uk government take leadership in the first of many important steps
2725480	2732440	towards the necessary international coordination to address this extinction level threat that is
2732440	2741240	facing us all and the very first step as so many academics industry leaders and even governments
2741240	2749560	have already taken is to firmly acknowledge the reality of what we face the potential extinction
2749560	2759080	of our species by ai private ai companies are scaling their ai systems as we speak
2759880	2766920	and they will not ask for permission and they will not stop unless we make them they are already
2766920	2773960	lobbying our governments for with ineffective policies such as responsible scaling in attempt
2773960	2781240	to prevent actually effective policy like the oil ceo is a past trying to lobby against climate
2781240	2787880	change regulation that would hurt their bottom lines the good news is that is not yet too late
2788520	2794360	to stop this to prevent the building of such deadly machines until we know how to build them
2794360	2802360	safely that is why there is nothing more important than for people to know the truth
2803480	2810760	that a small group of unelected unaccountable private companies are running a deadly experiment
2810760	2819320	on you on your family and on everyone on earth without your consent or even knowledge despite
2819320	2830520	they themselves admitting that these risks are real at this point all of us agree that there is
2830520	2836760	that we are playing Russian roulette with the entire planet and we're only quibbling about
2836760	2842440	how many poles are left until the bullet now in my personal opinion if you ever find yourself
2842440	2851000	playing Russian roulette i suggest you put down the gun and so we have to speak up and demand
2851000	2859480	action if you want a future for us our children and our species it's not yet too late but it will
2859480	2866040	be soon we stand at a historic moment today at where it all began
2867080	2870200	let's leave park well let's not waste it
2878680	2881880	thank you corner let's take a couple of questions from the audience
2883400	2887880	where am i seeing there's one at yeah if you take the microphone in there thanks
2889720	2896440	hi very nice presentation by the way i wanted to ask
2898200	2903160	with the current situation that's going on with ai currently do you really think if you were to be
2903160	2909320	a philosopher maybe for five minutes do you really think that currently society is really ready for
2909320	2917560	it i mean sure we can adapt in somewhat but as things are uh dripping around us it doesn't seem
2917640	2924840	like we're really anywhere near to i mean accepting it we're just a such fair and all
2925560	2932440	and other factors coming yes so the simple answer is no we are absolutely ready
2933240	2938360	we should be playing with nuclear fire or worse our civilization does not have the level of maturity
2938360	2942200	to be able to handle technology like this and this is why i'm not extremely optimistic about the
2942200	2948040	future the truth is is that whether it's ai or something else ai technology is becoming more
2948040	2953320	and more powerful this is just how it is this is how the technology works and our society
2953320	2959880	has to adapt to this if we as a society do not find a way to as an entire civilization as an
2959880	2966600	international civilization work together in a way that we can responsibly steward technology so powerful
2967240	2973800	that it can destroy anything then humanity is on a timer whether it's ai or whatever comes after
2973800	2981720	that we need to improve our society or that's it sometimes people grow up in a hurry sometimes
2981720	2986120	people are a bit childish and suddenly there's a big threat ahead and my goodness we grow up
2986120	2990840	is that what you see happening with humanity now we're not ready for ai but as we understand the
2990840	2997880	risks we will change our mode of operation i sure hope so and if it happens it will not happen as
3000280	3004280	because something about it is because people like the people in this room actually do something
3004280	3009560	about it stand up and make a difference in our institutions and our society it is there is no
3009560	3014760	law of physics that forbids us from having a good future and taking control of our future
3014760	3020520	and building wonderful safe technology for all but there is also no law that mandates it
3020920	3027560	i saw one more hand up yes if you take give the mic to the the woman in the glasses thank you
3032120	3037400	oh and i was gonna say i know that one of your policies is that you want to cap compute and
3037400	3041400	and i'm just wondering whether you are gonna suggest that at the summit and what you think
3041400	3047800	the government's response to that will be compute caps are absolutely the most sensible direct
3047800	3052760	policy for us as a species to follow the main reason for this is is that it is the bottleneck
3052760	3058040	towards building the actually existential dangerous systems just explain what these view caps are
3058040	3063880	yes so compute is basically limiting the maximum size of the supercomputers i talked about the limit
3063880	3069640	the maximum size our ai is and our computers are allowed to be and so we can limit hypothetically
3069640	3073960	how intelligent they will be things actually get dangerous because we don't know what pops out of
3073960	3078440	our experiments until we run them so it might already be that we're always too late our computers
3078440	3084280	might already be big enough to end the world we don't know but hopefully not in that case the first
3084280	3088840	as i say with russian roulette if you pull the trigger once and there was no bullet the correct
3088840	3095080	move is not to pull it again the first thing you do is don't pull it again until you know if there's
3095080	3100280	a bullet and where it is and if you know there's one definitely don't pull it so this is my opinion
3100360	3104920	on this i will definitely be open to talking and would like to suggest this to all policy makers
3104920	3109800	of all nations and leave this to say i think there is extremely strong resistance to this
3109800	3114200	for the obvious reason that this cuts into the bottom lines of very powerful big tech companies
3114200	3120520	who have extreme lobbying power and control over governments this is well i mean it's very simple
3120520	3125480	there's a lot of people who gain a lot of benefits from continuing to pull that trigger and we have
3125480	3129640	to make them stop and they are going to fight us every step of the way it's just how it works
3130840	3132840	thank you very much Connolly
3138760	3143080	and i'd like to invite the eight members of the panel to come up on stage
3143080	3147800	and we're going to continue the conversation please self-organize on the seats
3148520	3152360	stewart i don't think we've got a seat for you at this stage we can either find another seat for
3152360	3159960	you or we'll let you come back on the stage later so sit down in whichever system you like
3159960	3165320	and we will hear from each of these panelists what they think has been missing from the
3165320	3169560	conversation so far maybe they've got an alternative view maybe they don't think we're playing
3169560	3174360	russian roulette and a different metaphors appropriate maybe they would like to express
3174360	3179960	what they think the politicians they're closest to should be saying maybe they'd like to comment on
3179960	3186520	some of the other issues of safety so shall we start at the far end there and let's just move
3186520	3191000	along the panel i'll give you two minutes each to contribute what you'd like into this conversation
3191000	3196600	and then we'll hear from the audience so and let me introduce you as well so
3198440	3203480	i'm sure i can do that my name is mark brackle i'm the director of policy at the future of life
3203480	3211480	institute um and truly support what stewart and corner have been saying i think when we
3211480	3216040	looked at the summit about six seven weeks ago we put out a set of recommendations ahead of time
3216520	3220920	i think there were three traps that we identified that we were worried the summit would fall into
3222040	3225880	the summit potentially not addressing the full range of risks all the way from
3225880	3231880	bias and discrimination up to extinction it not being inclusive namely china not being invited
3232520	3237720	and it being a setting where the big ceo's would sort of run the show and there would be maybe
3237720	3244440	some token academics uh at a panel in a room the night before so i think if we sort of assess
3244440	3251000	what the summit is looking like now the night before the actual event i think
3251720	3257720	sort of we can be reasonably happy i saw this morning that china will in fact be invited
3258520	3266040	and that it will be an inclusive summit in that nation of the world will get a seat at the table
3266040	3272680	so i think that's progress so that's very good if we think about the harms and the range of harms
3272680	3278280	that are being discussed one of the things fli recommended for the summit was grounding it in
3278280	3283800	examples of large-scale ai harm that we've already seen such as the australian robo debt scandal
3283800	3289080	or the dutch benefit scandal from the netherlands myself and it's it's a political scandal really
3289080	3294840	dominating the national scene to to show that very simple algorithms can already have a very large
3294840	3299640	impact in countries that were rushing towards adoption and to show the beginning of the trend
3299640	3303960	line and that hasn't happened i think that's potentially a missed opportunity but i think
3303960	3310920	it's really good where the uk has overall focused the summit and i think i'm sort of least optimistic
3310920	3315800	when it comes to the role of companies at the summit and i think corner's done a great job at
3315800	3321000	highlighting concerns that fli that many of us have around responsible scaling and this narrative
3321000	3325800	being pushed by many companies as an excuse to keep going rather than making sure that whatever
3325800	3331560	they put out onto the market is actually safe and i think that's a message that i hope we collectively
3331560	3335800	and the people in this room that are going to the summit can still take to the participants
3335800	3341000	and to the governments that are there making sure that we put the onus of what is safe and what isn't
3341000	3346040	safe on the companies and they need to prove to us that what they're putting on the market is safe
3346040	3351320	rather than the other way around where the default is they keep on scaling and it's up to the regulator
3351320	3356920	to prove that what is safe so that i think is a key message to take so you're giving at least two
3356920	3362040	cheers if not three cheers to the organisers for what you see happening already so from one
3362040	3368680	dutch man to another ron rusendal is the direct deputy director general of the netherlands ministry
3368680	3373560	for the interior lots of other roles what would you like to add to the conversation um well thank
3373560	3381480	you very much and i'm glad to be here um um first point is that we regulate cars and we
3381480	3389240	regulate pharmaceuticals um and we do so to mitigate risks of today and the risks of tomorrow
3390040	3397640	um so we have to act upon risks of today like bias and risks of tomorrow
3398280	3406360	um and those are global risks so we welcome the initiative of the summit but we also welcome
3406360	3415160	the initiative of the tech and foyer of the un starting a high-level advisory board um and we
3415160	3423960	have offered the tech and foyer to host a european um um meeting of the high-level advisory board
3424040	3428760	in the hake for example in the peace palace because we support the work that we all do
3428760	3437320	internationally to mitigate the risks um secondly we need some form of um early warning whatever
3437320	3445080	the risks are um and whatever whether they will occur or not we have to have early warning and
3445080	3452840	a rapid response mechanism uh or whatever goes uh will happen in the future and and therefore we need
3452840	3458840	to um operate in a failure-driven way and we need to participate we need to coordinate but
3458840	3467400	we also need to cooperate with industry with civil society with citizens with um uh industry and
3467400	3472040	with governments agreement on early warning seems like something that both sides of the debate should
3472040	3477080	be able to give because the people think things will go wrong and the people think things won't
3477080	3481480	go wrong should be able to agree well if this happens we should all be paying more attention
3482440	3487080	let's pass the microphone on to Hal Hodson who's a journalist at the Economist who has written
3487080	3492840	a lot about existential risks and AI Hal thanks David um yes so my name's Hal Hodson I'm a special
3492840	3496920	projects writer with the Economist I've been writing about AI for 10 years I have a degree in
3496920	3501000	astrophysics and that meant that I spent a lot of time looking at a thing called the archive
3501000	3505480	long before it was cool and uh papers from Facebook and Google would just turn up on the
3505480	3511000	archive with no PR whatsoever and this is journalistic gold and that's how I got into it um I
3511000	3517000	guess my sort of view is inherently going to be journalistic I think it is a very difficult point
3517000	3522600	to make very clear decisions about what anybody ought to do about any of this I think there's
3522600	3526600	from my perspective there's a huge amount of uncertainty I'm now I've now been writing about
3526600	3529880	it long enough to know that there's also a lot of hype and it's not the first time there's been a
3529880	3536360	huge amount of hype and I think making very clear decisions about important systems at a time that
3536360	3541640	is hype filled is a difficult thing to do I think the thing that I can agree on the consensus that
3541640	3545720	I can come to with probably most of the people in the room and the organizers of the summit
3545720	3550840	is that there's a huge amount of science to do and it both in terms of existential risks and these
3550840	3555720	sort of lower tier algorithmic risks I think there's two examples that show us that this is a perfectly
3555720	3561080	plausible thing to do the first is that there was a time in the 90s when everybody was very worried
3561080	3568280	about impact of bodies in the solar system to earth existential risks from asteroids and things
3568280	3573400	like this and congress mandated a large amount of money to go to NASA to map all of the asteroids in
3573400	3578680	the solar system and to figure out ways to nudge them off course if they come towards us and if
3578680	3583800	you look at the risks as they were assessed in the 90s and the risks as they are assessed today
3583800	3589400	they are massively massively dramatically lower and so that to me is a very strong case for doing
3589480	3594200	science on these risks I don't know and I'd be fascinated to talk to people who do know what
3594200	3599560	doing science on AI systems really looks like but it brings me to the next comparison which is
3599560	3604520	Facebook about five years ago there was also a big panic that Facebook was determining the results
3604520	3609320	of elections or you know hacking democracy essentially that is somewhat subsided now but
3609320	3614680	one of the most sensible responses to that concern that I saw was also that you need to do science
3614680	3619000	on Facebook just like you needed to do science on the solar system you need to start measuring
3619000	3624520	things and it actually took years to force Facebook to give access to data to people like
3624520	3629800	social science one it eventually sort of worked and I think there's a reason that you don't hear
3629800	3633880	a huge amount about it it's because the science that's been done so far has not determined that
3633880	3639720	Facebook destroyed democracy we still have at least a version of it and so I guess I would end
3639720	3645880	just by a plea to you know and perhaps in the same way you were saying Stuart politically neutral
3645880	3650120	science to the extent that that's possible a more of a goal than a thing that exists
3650120	3656520	but do you have no whether the US focus on asteroid risk in the 90s was that bipartisan
3656520	3661720	or was that a partisan issue I don't know if it was bipartisan but it went through congress so it
3661720	3666600	must have been a bit bipartisan so maybe it wasn't as bad back then actually now that I think of it
3666600	3671560	there's some encouraging examples there next we're going to hear from Anika Braak who's the CEO of
3671560	3678600	the International Center for Future Generations tell us about your views Anika yes thank you very
3678600	3683800	much and first of all I'd like to commend the UK on two things first of all their sense of humor
3683800	3690200	for setting up a summit on the darkest corners of AI on the night of Halloween or the nights after
3691080	3697800	and I'm surprised nobody made that joke so far and secondly for really bringing this to the
3697800	3705000	attention of leaders media and the public actually I don't think Frontier AI has ever been discussed
3705000	3713240	so much and the number of communicates the number of you know the the executive order the communications
3713240	3719880	leaders European leaders meeting ahead of the summit negotiating late night to get to bring
3719880	3726040	something here is already a measure of success and we could actually leave it here with this
3726040	3731320	stellar panel and and say I think it's very important that the the civil society is meeting
3731320	3737320	here this is maybe the element that is missing in the room I'd like to say we have two major
3737320	3742200	challenges here one is a coordination challenge we have corporates looking at the topic they
3742920	3748840	they're racing over competitive edge and we have governments who have serious geo-strategic
3748840	3754680	interest and when those two come together that doesn't help collaboration so we have to think
3754680	3760120	about how we get people around the table and secondly there's a democratic challenge democracy
3760120	3766440	is by its very nature a slow and patient regulator and that's important I will argue that actually
3767720	3773560	democracy is perfectly adapted to the society through these uncharted waters that we're experiencing
3773560	3779480	at the moment but we need to make sure that the sailors of this big ship are prepared that they
3779480	3785800	are well informed and that they have the tools to deal with this change and that's what the
3785800	3791000	international center for future generations set out to do in Brussels that's why we moved our
3791000	3796680	headquarters to Brussels to make sure that EU decision makers are well prepared because we
3796680	3802120	set our best hope in the EU in this international race for governance I will leave it here for now
3803000	3808760	are the EU decision makers paying attention to what you say I think they do you do here
3808840	3814600	already a lot of signs that they have also recognized that we have to look at advanced
3814600	3821640	artificial intelligence that regulation doesn't stop with the artificial intelligence act it's
3821640	3828280	only the very start of the beginning or the first piece of the puzzle to come back to Stuart's presentation
3829080	3832840	thanks next we're going to hear from Jan Tallinn who is the co-founder of Skype
3833320	3840360	fli Caesar that's the center for the study of existential risks and he is also one of the
3840360	3845960	advisors on the committee created by the tech and envoy for the UN so Jan what would you like
3845960	3852920	to say based on what you've heard so far yeah thank you very much sometimes people ask me
3853480	3860280	because I've been in this kind of existential risk and AI safety community and effort for more
3860280	3865880	than a decade now sometimes people ask like so how's it going and my standard answer is well it's
3866680	3875880	great progress against an unknown deadline and indeed it's kind of special this year it's just
3875880	3882280	like there's like a plethora of things to point to us as great progress and obviously the most
3882280	3888520	obvious one to point to at this point is the summit that starts tomorrow I do think it's
3889480	3895160	UK deserves a great credit for for pulling this together and I really wish
3896040	3901880	kind of best of luck to the organizers of this and the prime minister as well and the team
3903720	3913560	now when it comes to this like unknown deadline recently I've kind of pivoted away
3914360	3923880	to some degree from basically funding research towards just buying us more time which is kind
3923880	3931080	of has to be has to deal with something like less research side and more kind of an action side more
3931080	3938680	on the policy side so I do think it's kind of valuable now to really think through the policy
3938680	3946760	that would make the future a little bit less sound and the deadlines a little bit less
3946760	3951880	unknown another and final thing I wanted to say that there is I want to kind of caution against
3954040	3958200	if you're sailing to like uncharted waters there's like a temptation to
3959400	3965400	use something familiar and say that oh like the future is going to be just like this
3965480	3969560	like the most common one is that oh AI is just the technology it's just going to be
3970200	3973080	just another like electricity or something like that
3975800	3984280	when we're talking about risks the the way the model risks is by the you know reference class
3984280	3991080	that you cannot rule out so as long as there is like reference classes like viruses self-replicating
3991080	3996360	things or another species as long as you kind of rule out rule them out you have to like prepare
3996360	4004040	that this might be an instance of such thing so I think it's important to not make dismiss
4004040	4008920	AI it's always just another technology or like as one prominent VC recently said oh it's just
4008920	4015880	much of math thanks Jan next we're going to hear from Max Teckmark he might describe what he's got
4015880	4022280	on his chest I happen to know he has released a very interesting TED talk which I strongly
4022280	4027960	recommend all of you watch and Max might give an abbreviated form of that TED talk now or whatever
4027960	4033240	else you'd like to put in the conversation thank you thank you yeah so I'm Max Teckmark I've been
4033240	4041880	doing AI research at MIT as a professor there for many years focusing on safety related stuff I'm
4041880	4050920	also the president of the future of life institute and I'm a huge fan of this guy who you guys have
4050920	4056760	the wisdom to put on your 50 quid note Alan Turi who's come up many times and it's really remarkable
4056760	4063720	that the argument he made 72 years ago that when machines greatly outsmart us by default they're
4063720	4068520	going to take control that that argument has not been convincingly refuted in the 72 years
4069320	4073880	since he said it so I think we have to take it very seriously and people who think of AI is
4073880	4079320	just a new technology like steam engines or electricity tend to not take it so seriously
4080200	4086520	Alan Turi himself clearly thought about AI more as a new species and with that framing it's very
4086520	4093000	natural that we would lose control to them just like the Neanderthals lost control to us etc
4093000	4099880	so so what are we going to do about about this great threat first of all having conversations
4099880	4105160	like here and what happens tomorrow is great so a huge thank you to the British government for
4105160	4110680	really putting this on and for standing up to all the lobbying pressure from companies who wanted to
4111480	4118920	water it down into just talking into just a big blessing of responsible scaling or whatever
4118920	4125880	thanks also to the US government for standing up to also the weird pressures to turn this into a
4125880	4131800	geopolitical pissing contest by excluding China I'm really proud of the Brits for recognizing
4131800	4141000	that this is a global challenge and what do we actually do about it well I think there's a remarkable
4141000	4147160	consensus actually emerging from all the civil society and academic groups that don't directly
4147160	4153400	profit the way companies do about what we should do about this we put out maybe
4154680	4159080	Andrew creature Richard Muller can just hold up in the air with this thing you can if you go to
4159080	4168280	future you'll find the alternative to the responsible scaling policy called called the
4170200	4175800	safety standards policy where the idea is as we heard from Stuart Russell you should simply shift
4175800	4183320	the responsibility to companies to prove that things are safe instead of as responsible scaling
4183320	4187640	policy you you have the responsibility on the government regulators to prove that things are
4187640	4196520	unsafe more or less in order to stop them and there's there's a whole set of very concrete ideas
4196520	4204200	out there for what the safety standards should be to start with and some of them were mentioned
4204200	4210200	very eloquently by Stuart you can insist on quantitative safety bounds or provable safety
4210200	4215320	beginning with uncontroversial stuff that you should not be able to demonstrate that nobody
4215320	4223080	can hack the servers that these super large systems are on that they won't advise on how
4223080	4227880	to make bio weapons etc and this will very naturally accomplish something quite wonderful
4227880	4232920	where we sort of have the cake and eat it as a species because most people I talked to don't
4232920	4239320	realize that there are two almost there's two very different kinds of AI that they keep conflating
4240200	4246040	there is the AI that has current commercial value for curing cancer making self or better
4246040	4252280	safer cars and all sorts of wonderful things which have very little risk associated with them but
4252280	4258600	some which we need to address but 99 percent of the things that most people are excited about
4258600	4263960	do not require playing Russian roulette with AGI and super intelligence and then there is this
4263960	4272760	lunatic frame just try to build the machines that outsmart humans in all ways where almost all
4272760	4278440	the risk is coming for very little benefit so if we can put safety standards in place we can I think
4278440	4282680	quickly get into a situation where we have a long future with these wonderful benefits
4283320	4289720	that are quite safe to get from AI and then just take your time with with the really risky stuff
4289720	4294600	maybe one day humanity will or will not want to build more powerful machines but only when we can
4294600	4298920	figure out you know how to control them so that would end with just a bit of wisdom from ancient
4298920	4305960	Greece if I may so raise your hand if you remember the story of Icarus don't get hubris right you know
4306040	4313720	so artificial intelligence is giving humanity these incredible intellectual wings with which
4313720	4321160	we can accomplish things beyond their wildest dreams if we stop obsessively trying to fly into
4321160	4328520	the sun thank you so you're not saying pause AI you're saying let's keep using AI but you're
4328520	4336280	saying pause the rush to AGI let's not pause AI in fact let's continue almost everything that
4336280	4343240	people are excited about doing but pause this compulsive obsession about training ever more
4343240	4351320	ginormous models that we just don't understand thanks Andrea Miotti is the head of policy and
4351320	4355400	governance for AI at Conjecture are you in agreement with what you've heard or you have
4355480	4360680	different things to emphasize absolutely I'm very much in agreement with both the speakers and many
4360680	4365640	other members of the panel I think there are two big positives from the summit to highlight
4367000	4373160	one is that we're also echoed by the panel one is it's role in building common knowledge
4373880	4379400	making it clear and explicit at the highest levels of government that this is a big risk
4379400	4385720	that this is a extension level threat that we face as a species and number two coordination
4385720	4392280	not getting lost in a geopolitical pissing contest as Max has said or in other of these
4392280	4398920	things and realizing this is a again a threat we all face together it's a global security problem
4398920	4403400	it's not a national security problem or at least it's not only a national security problem and
4404120	4409720	to solve these problems we need coordination even during the heights of the Cold War there were
4409720	4418200	open lines between the US and the Union to deal closing the door on cooperation before it has
4418200	4426840	been tried is a surefire way for all of us to lose and so I was very very pleased to see that the
4427560	4434120	UK government the Prime Minister Rishi Sunak have already acknowledged the risks very explicitly
4434120	4440920	in the Prime Minister's speech last week are setting up the summit are inviting a diverse
4440920	4448760	group of countries to discuss this risk together the part where I think we can go
4449400	4455160	further and we can do better is in the measures I share the concern of some of the other panelists
4455160	4462600	on a focus of simply enabling the default to continue and the reality is that the default
4463400	4470680	is bad the default is bad we by now all understand it is bad and we all understand we need something
4470680	4478120	else even the companies racing towards its default admit that it's bad admit that it's a
4478120	4486440	one in four one in ten unacceptably high chance for all of us to be wiped out and so the concrete
4486440	4492120	measures they will need to take cannot look like continuing on the default path cannot look like
4492680	4500680	systems are safe until proven dangerous by external auditors that are strapped for resources and they
4500680	4506920	don't even have the the tools or the tests to do these tests they look like provably safe systems
4506920	4512200	they look like burden of proof on developers developing systems that they admit could wipe
4512200	4517640	everyone out to demonstrate ahead of time of running critical experiments that they are safe
4518200	4522280	if they cannot do that that's fine they can just build something else or they can move to a different
4522280	4529240	sector that's the standard we utilize in all high risk sectors there is no reason to not
4529240	4534600	utilize it in a sector where the risks are the literal extinction of your mighty thank you
4535240	4541400	and last but not least we have a trained economist alexandra musavi sadeh who is the
4541400	4547000	CEO of evident what would you like to add to the conversation alexandra it's a hard it's a it's a
4547000	4555080	great panel to follow it so it's um i think i have a different time horizon alexandra musavi sadeh
4555080	4562920	i'm the founder and CEO of evident and we actually do a lot of measurement uh we specialize in
4562920	4570440	benchmarking businesses on the option of ai so what i focus on is very near term so looking at
4570440	4577480	the here and now and the race is on at that level as well so the race is on by all businesses in
4577480	4585240	all sectors to take the capabilities that ai offers today and to implement it as fast as possible
4585240	4590120	and really not thinking about any of the risks so thinking about growing market share um
4590680	4597320	upping revenue cutting costs and all of that and continuing the um sort of digital transformation
4597320	4603960	which is now more and more an ai transformation and so with that um we we are observing this ai
4603960	4609240	race at a business level and one of the things that we see that some businesses that are highly
4609240	4615880	regulated really think about um how they can implement the oversight and implementation of
4615880	4624200	safe ai so while very impressed with what the uk government is doing and i think the right thing
4624200	4631800	is to focus on the long term because that is where we um should have our eyes at um at the stage but
4631800	4637000	there's also a near term risk and i think um if there was one thing i would suggest is that as
4637000	4641960	much as we need to focus on the long term we also need to look at the here and now and that businesses
4642040	4648200	are barreling ahead with ai adoption without any particular guard rails in that and so while we
4648200	4655560	need to put the um the burden on the development of safe ai we could also maybe in the meantime put
4655560	4663320	the burden on the businesses that are using ai to prove that they're doing it in a safe uh and
4663320	4668600	constructive way thanks so you've heard from all the panelists i'm sure there's lots and lots of
4668600	4672920	questions in your mind so i'm going to come to the audience and take maybe three or four questions
4672920	4678120	and then let the panelists pick what they want and my question to be would be do you agree with
4678120	4683160	this division between near term and far and far future some people say that the risks from
4683160	4688840	existential risk should not be considered to be far long term they are potentially here and now
4688840	4694840	but maybe you have a different way of framing it let's see some hands let's take uh one back over
4694840	4701960	there in the far corner it's a bit some running around if you can say who you are if you want
4701960	4708200	to remain anonymous that's fine too hi um i'm Matthew Kilcoin i looked into how the banking
4708200	4714520	industry turns short term into long term by sort of senior management risk and associated penalties
4714520	4720920	and clawbacks to force the change okay question on learning from the banking industry so let me
4720920	4729400	give the microphone down here just a second thanks a lot everyone yolanda lancas from the
4729400	4735160	future society what do we do about open source ai professor russell mentioned one idea which was
4735800	4740520	kill switches i think this is in terms of policy approaches such an important question
4740520	4746680	for us to all grapple and again um as yan was saying oh technology people assume that paradigms
4746680	4752840	continue open source has been valuable for software but with ai we're seeing new risks
4752840	4765720	and paradigms and how could maybe academia and others my name is oliver graves um my question is
4765720	4771000	what do you think the biggest hurdles are towards getting the general public to recognize this as an
4771000	4775640	existential risk and to take that risk seriously because it still seems to me like it's all well
4775720	4780360	and good everyone here at the summit and in this room being aware of that risk but it doesn't seem
4780360	4786280	to me like we're anywhere close to a level of majority of the general public grappling with it
4786280	4788120	probably
4792280	4796440	thank you so i'm i'm father pete vignaschi i'm i'm engaged in looking at how the catholic church
4796440	4801880	can respond to existential risks so it's a slightly different question here just in the most general
4801880	4807160	way what does it look like from your side of the table for religious groups to play their parts
4807160	4815720	in achieving existential security thank you i'm oliver chamberlain i'm a student studying
4817000	4823480	a master's in science in in ai uh one of my concerns is although like the regulation
4823480	4831000	is going to involve limiting like supply chains making sure that gpu's aren't going off to places
4831000	4837640	that we don't know about how do we stop um the advancement of algorithms which allows older
4837640	4846680	systems to be more powerful so like alpha tensor um i wonder uh in my mind like the only way around
4846680	4854520	something like this is a future which is like super draconian um how do we prevent gpu's that
4854520	4858520	already accessible already out there from being used in ways which are way more powerful
4872760	4877800	question on open source to what extent is it possible to control open source a question on
4877800	4883240	what are the biggest hurdles changing their minds in the public or indeed one of the other big hurdles
4885080	4892840	yeah question from the point of view of what might religious organizations contribute to
4892840	4899720	this conversation and do we need to have super draconian surveillance and policing systems if
4899720	4905880	we're going to stop these gpu's and algorithms they're potentially doing things that we didn't
4905880	4912200	want them to so max hand up first yeah religious organizations i hope can
4913160	4919560	remind us all of the importance of not play god and get hubris remember the moral angle
4920600	4925080	i'm only going to comment on the timeline one even though i have opinions about all the others
4925080	4930680	it's not so i don't talk too much the timeline one from alan turing's perspective when he said this
4930680	4937320	he said that when it eventually basically passed the turing test he expected to go very fast
4938040	4943800	so then it was a long-term risk now according to yosha bengio gpt4 passes a turing test so
4943800	4949960	he would probably if he were still with us in the room predict short timeline it's quite remarkable
4949960	4954840	what's happened on on the prediction market metaculous.com for those of you who are nerdy enough
4955640	4961320	to go there where the timeline how many years we have left artificial general intelligence
4961400	4967800	outsmarting us has plummeted from 20 years away to three years away just in the last 18 months
4968440	4974520	as a direct result of of this recent tech progress and dario amode has openly said one of the tech
4974520	4979640	ceo's here that he thinks you have two or three years left and others other tech ceos told me
4979640	4985640	that individually so i think we just have to stop calling this artificial general intelligence
4985640	4991640	risk long term or people are going to laugh at us and call us dinosaurs stuck in 2021
4993720	5001800	andrea i'd like to answer the question about the public actually the pub seems to really understand
5002440	5008280	i recently ran polling as part of a campaign i'm running called control ai and the british public
5008280	5014600	is extremely concerned about disempowerment and extinction risk from ai they seem to be
5014680	5025320	aware of it um a they seem to be aware of it um a whopping 60 percent a global ban on smart and
5025320	5032200	human ai period with only i believe 14 percent against and like quite a few undersized um
5033320	5040040	nearly i believe almost nearly 90 percent would be very very happy with a full ban on deepfakes
5040040	5048040	right now people understand very very well that full impersonation revenge pornography and like use
5048040	5053560	of their likeness against their will is not good is destabilizing is a threat that exists right
5053560	5059960	now with systems over here right now and they don't want that um and similarly uh there is
5059960	5065880	i believe 78 percent of the public it would want an international watchdog with real teeth more like
5065880	5074200	an i a a and there are basically across the board like i was personally surprised as he all of the
5074200	5082440	answers come up with such overwhelming uh support uh we might ask whether that mood is shallow that
5082440	5088360	it might be adjusted we might ask whether that mood is shallow that it might be adjusted again
5088360	5094840	in the future let's hear from anika and then from how and then from yana uh yeah i just wanted to
5094840	5100360	say that we should stay away from predictions with regard to timelines with regard to sectors
5100360	5104840	how they're going to be affected i think if we have learned one thing that it's really difficult
5104840	5110520	to assess that but we do know that there will be dislocations there will be impacts and as they
5110520	5116200	grow and as we see the more the public will be more informed and more aware i think it's really
5116200	5123880	our role here as civil society academia religious leaders to increase that awareness and to also
5123880	5130120	keep that conversation going um as we go when you drive a car you don't just look ahead right
5130120	5136520	you also look in the rear view uh mirror you look in the side mirrors for signals of change but
5136520	5141240	what i want to say when you look back that's something we are not doing enough we're trying
5141240	5146520	to predict in the future but we should also look back and look at you know stuff we've put out
5146520	5151720	there regulation we're putting out there and how it's actually being enforced um if it's effective
5151800	5158600	if um it's yeah effective regulation we discussed about it um before and this will be key going
5158600	5165240	forward how i'll take the question oh yeah on yeah i'll take the question on open source
5165240	5170440	and draconianism because i think they're related i don't really see a good way of regulating and
5170440	5175720	controlling open source code that is not deeply draconian that does not involve a massive expansion
5175720	5181480	of surveillance uh if you need to know what code is running on what chips you have to have access
5181560	5186920	to the computer in which those chips are running and for a sense of how well this is going to go
5186920	5193480	look at america's attempts to put export controls on chinese ai development um it it works to an
5193480	5200920	extent but it only works if you pick these very narrow bottlenecks and i guess in terms of existential
5200920	5206840	risk it depends on how powerful sort of lower tier open source models end up being i don't really have
5206840	5211480	a good answer to that question i just say one more thing on metaculous just as a little hint of
5211480	5216200	not relying on it too much if you just if you remember the lk 99 superconductivity thing over
5216200	5223400	the summer metaculous at one point was completely certain that that was real and uh the metaculous
5223400	5228760	thought it was real for a while and and then it dived again so just you can't rely on metaculous
5228760	5236120	we're gonna do some real time checking on this right so once that's going on yeah so i also
5236120	5241160	wanted to say a few words about open source i think it's as i mentioned earlier i think it's
5241160	5247560	important to just like not do this kind of categorical thinking that you have like some one particular
5247560	5252600	you know look at that too that you put things in and then like you reason about this pocket
5254040	5259800	my friend andrew gritch who is in the audience like he he observes that's like when people
5260520	5265720	say that they're really gonna prove open source it's valuable to try to understand what they
5265720	5270760	actually want what is what is the thing that they're trying to protect and quite often it's
5270760	5275320	just like they don't want this like massive centralization and power in the hands of people
5275320	5280840	that they don't trust now the question is like if you think about open source as like irreversible
5280840	5285880	deployment of things that we potentially don't want to irreversible deploy other other ways
5285880	5292360	to protect what the open source allocates want and for example there is like there's like in blockchain
5292360	5301320	community there is quite a lot of advancement in cryptographic techniques techniques like
5301320	5306520	zero noise proofs perhaps there are enough like ways how we can kind of eat our cake can keep it
5307240	5313800	keep it too by instead of having nosy people literally looking around in a computer you just
5313800	5318440	computer automatically producing things like zero noise proofs that you haven't been up to no good
5318440	5324120	things like that so there's lots of possibilities to explore there are this andra i'm actually
5324120	5331720	curious about what max and how we're talking about ai secrets did they know or did they not know
5333640	5338360	peak enthusiasm they thought it was 60 chance which i would say interpreted that they were
5338360	5345640	not saying for sure okay but prediction is very real time checking here prediction is hard canary
5345640	5350360	signals are more important in my view let's agree the canary signals okay if i can just add
5350360	5354760	something here we we ask if something is a near term or long term risk we have to remember
5354760	5359080	we're not asking if we know for sure that we're going to get super intelligent soon
5359080	5364360	if we think there's a three a 10 percent chance that something like this might happen in four years
5364360	5369240	but then it's still in the risk is near term even though i hope as much as anyone that it actually
5369240	5375560	won't happen for a long time so alessandra and then ron and then mark yeah i just wanted to
5375560	5380760	respond to the question on the banks um if i understood it correctly is like was that maybe a
5380760	5388360	model of regulation for ai is that what you meant okay i mean as we cover that sector very deeply
5388360	5394840	it is it is an area where i mean and it's it's um it's a sector that is heavily regulated and because
5394840	5399560	they heavily regulated the way that they are developing and deploying and implementing ai
5399560	5404440	today is that even as they develop there's a lot of oversight in the models themselves and then they
5404440	5410360	have they go through first and second and third lines of defense where there is oversight again
5410360	5416040	and then they submit to the regulators and in a way i mean i can't believe i'm saying this but
5416040	5424280	in a way the banks could be a model with which if you are to impose a oversight at a company level
5425240	5430040	for the ai that they're using the banking model is not a bad one because the way that they
5430600	5436280	assess the risks as they go from development to deployment into production and output so
5437320	5443160	that could be a blueprint or something to to look at for businesses themselves to regulate
5443160	5447160	themselves is that if that's where we end up so there might be something to learn but bearing
5447160	5452360	in mind agi is different from everything that's ever been before ron what would you like to add
5452360	5457400	and well i'd like to come back to the discussion about open source versus closed source and i'm
5457400	5463720	a bit surprised that here at the table there is a strong belief in closed lots of companies
5464440	5471880	that might contain lots of zero days we don't know of instead of trusting civil society
5473160	5479560	and and on regulation we therefore not only need to regulate development we also need to
5479560	5487720	regulate use and there and that's what the ai does for example sounds like none of the
5487720	5491800	old traditional models are going to work sounds like we need something that has a variety of
5491800	5497320	different approaches we have to transcend some of the previous systems and mark and then we'll
5497320	5503000	come back to pick up a few more comments um yeah i just wanted to pick up on sort of a question of
5503000	5507240	near term or existing harms that we see in sort of harms in future because i think what you saw
5507240	5510920	yesterday with president biden coming out with an executive order in the united states
5511560	5516920	as well as with uai act which has been a long time in the making you see sort of an attempt and i
5516920	5522600	think a successful attempt by policy makers to tackle both ai and bias that you see in current
5522600	5529560	day applications and some of the risks that are like that are agi related and i think that shows
5529560	5535240	that it's perfectly possible to do both and i know a lot of people in this audience are potentially
5535240	5539320	driven by existential risk i mean that's why we come to an existential risk observatory event
5539320	5544360	panel um but i think there is a lot of alliances and bridges that can be built across that space
5544360	5550120	and i think it's often not helpful to look at both of these things um maybe just on banking um i mean
5550120	5556200	we saw we seen the 2008 financial crisis where i think lots of people were justifiably angry
5556200	5561000	because ceo's got away with whatever they were wanting to do uh and governments build out the
5561000	5566360	banks um there was briefly a lot of regulation that was then rolled back over the past few years
5566360	5571320	and again we saw a few small banks collapse in california so i think we need to learn some
5571320	5576920	lessons there around liability and making sure that as we build a liability regime for ai companies
5577960	5583400	ceo's are also individually and criminally liable if they are indeed negligent or if there are
5583400	5587320	sort of safety risks that they're they're ignoring so maybe just to add on those two points so half
5587400	5590920	the panel have got their hands up wanting to speak well i'm going to ignore them briefly and
5590920	5594760	give the microphone very quickly to three people in the audience but you have to be quick because
5594760	5600920	we're out of time already thanks uh richard barker and ron introduced the analogy of the
5600920	5604920	pharmaceutical industry which is not perfect by any means but i spent most of my career in it so
5604920	5610920	i think there's still a few lessons from that right the first is you regulate not the underlying
5610920	5616200	technology but the application of the technology so it turns out felidomide is the terrible thing
5616200	5621720	to give to pregnant women but it actually cures people with multiple myeloma so you i can't imagine
5621720	5627560	how we're going to actually deeply regulate the the internal workings it's it's how they're used
5627560	5635160	and it may not be existential risk that is most uh relevant uh to actually harnessing public opinion
5635160	5640680	it will be some of the things that's already happening that affect them personally uh they're
5640680	5648440	not just experts but panels um come back to uh legislators and say this is what i saw and this
5648440	5661960	is what i like and don't like um terry rabie former risk manager um guys you really need some pushback
5662920	5670280	uh i was deeply appalled at stewart's example of the nuclear industry the regulation of the
5670280	5679800	nuclear industry in the united states essentially is anti-human it's prevented um the gifts of
5679800	5688440	energy that's not polluting by regulation we have another anti-human example of regulation in the
5688440	5696200	u which is a regulation of biology destructive of the advantages that we could get from genetic
5696200	5703320	regulation of biology destructive of the advantages that we could get from genetically modified
5703320	5711320	organisms so look it won't do the needs to be a little bit more pushback to you guys so you get
5711320	5722360	your story straight okay who's behind her here so earlier thank you don't show me there's key
5722360	5730520	substances professor russer make a very important distinction about as a i to be safe for humans
5731160	5739400	and a i uh safe for use as a tool the first one is a new type of intelligence the second one
5739400	5748440	is a just used as a tool so that's where the regulation comes in context we can regulate
5748440	5756280	a i as safe to use and we must have a control of our development so it doesn't become an
5756280	5763080	existential risk my question to the panel is the following one is it possible or shouldn't be possible
5763160	5770760	in order to avoid uh open sourcing problems to develop just one super intelligence program
5770760	5778120	that will beat any uh small guys developments and in that way make us safer and the second
5778120	5785880	question is what will follow this summit deliverable which thanks so plenty to talk about there
5785880	5790600	learning from the pharmaceutical industry regulating apps not platforms we had terry
5790600	5795320	pushing back quite hard saying goodness look at the mess of regulation in the nuclear industry
5795320	5803320	and in gmo's we had tony asking about a unified approach with one research and development program
5803320	5810360	and also what's going to happen next so 30 seconds each max's hand up again i have a good friend
5810360	5815080	in the american nuclear industry who told me that what really killed it wasn't regulation but it was
5815080	5823080	fukushima and then three mile island i for open source 20 seconds on that i actually think i i love
5823080	5828280	open source almost as much as yon lakoon mit is the cradle kind of open source but obviously we
5828280	5834200	don't open source plutonium in this geranium and similarly here we should get away from this childish
5834200	5837800	debate about whether your open source nothing or everything and just ask where the line goes
5838680	5843720	finally there's a technical solution i think to this which is not creepy but still works
5843720	5851560	which steve omohandro and i wrote a paper about where where you actually have control of your own
5851560	5855800	hardware chips you own it no the government doesn't see what you run but it's not going to run
5855800	5860040	certain kinds of really creepy code because the hardware itself won't don't it's like a
5860040	5864840	virus checker in reverse where if your code can't prove that it's not making bioweapons
5864840	5870520	it just won't run so you can find out more about that proposal if you watch max's ted talk how
5871400	5875160	all i was just gonna ask doesn't that make it a backdoor in kind of the same way that
5875160	5881240	csam detection on i message it makes it a backdoor no it's completely decentralized no one has
5881240	5885480	access to your chip it's just if you want to chip that'll run the harmful code you have to make your
5885480	5893000	own chip we need standards for hardware which is what stew was saying who's who's wants to jump in next
5893000	5902920	andrea just a quick reply to the ipcc model uh i very much hope that we will not have an international
5902920	5910360	agency model after the ipcc here uh we are in crunch time and we have now knowledge of the risks
5910360	5916120	and we have common knowledge about the risks uh the role of the pcc was a great organization to
5916120	5922280	build over decades uh essentially expertise and information to governments to deliberate on how
5922280	5929000	to act we do not have decades and we know what the problems are governments are already acknowledging
5929000	5936280	what the problems are we need action not a yearly report alessandra i would i would agree with that
5936280	5940680	i think we're at a point where and also i don't see how it's practically gonna gonna work i mean
5941320	5949240	are the chinese in the u.s gonna open the kimono and submit to a uk body that wants it to um and
5949240	5953400	no accountability and no repercussions if they don't so i really don't see it that how that would
5953400	5959800	work but um there's so much to say and so much to respond to uh on this but i think uh gentlemen
5959800	5965560	in the in the red jumper they'll very much agree with the the fact that at least until
5966360	5972280	something has um taken place on the regulation and we've agreed to what that might look like for the
5972280	5979560	near term big risks but for the here and now um make the companies make the sectors accountable
5979560	5985800	for how they use it make the buck stop there first and then we can figure out or in parallel figure
5985800	5992040	out how to regulate it's sort of the bigger bigger questions and the the things that are um giving us
5992040	5996200	pause but i think the world will submit to a body that's run by the uk but the world might
5996200	6002120	cooperate with a body that the uk helps to inspire and get off the board mark your hand was up
6003080	6007560	yeah i also just wanted to pick on uh pick up on the question the gentleman in the red jumper
6007560	6011880	raised on parallels with the pharmaceutical industry uh yeah you're just sort of shining
6011880	6017880	beacon here in the audience um i mean i think on the one hand like the time where everyone
6017880	6022200	just could produce whatever potion they wanted to and put it out on the market that has disappeared
6022200	6026040	and i think thankfully disappeared and i think that there is lessons we can learn in that in
6026040	6030760	terms of potentially licensing or making sure that you guarantee that something's safe when it comes to
6031320	6036280	regulating the application of ai i think i'm significantly more skeptical uh this has been
6036840	6042360	for example in the senate hearing this was what uh christine amon gomri from ibm was pushing quite
6042360	6046920	heavily we see on both sides of the atlantic big tech really pushing for application based
6046920	6051800	regulation because that often means that the underlying big systems that they are building
6051800	6057800	won't be regulated right because how do you regulate regulate gpt4 if if you only regulate
6057800	6063000	applications and it's only the hospital that then integrates it into a chatbot for patient contact
6063000	6067880	contact that actually has to deal with the regulatory burden so i think you do need to force
6067880	6073240	these big tech companies to do risk identification and mitigation even if they can't particularly
6073240	6078040	specify oh it goes into that application or or this other one so i think you need a bit of a
6078040	6085880	combination of both anik are you a fan of the ipc model ipcc or would you prefer the i c fg
6085880	6092440	model i c fc or the i e uh ai ea i think there are a couple of models proposed i think what's
6092440	6098680	important is to not um put it in one hand uh not in the hand of a few corporates but also not
6098680	6103080	in the hands of one state so the current race is not healthy and we need to think about how
6103080	6109880	we get them back to the table i know you you have some ideas about this and uh yeah we we as i cfg
6109880	6116920	are trying to show and build the scenarios to explain what it means to look at a future where
6116920	6121800	emerging tech is governed and what it means when you look at a future where emerging tech is not
6121800	6127720	governed and that this will hopefully help decision makers come together and work together because
6127720	6132040	there are many other emerging technologies that might disrupt our society in many other ways
6132040	6137640	too just around the corner absolutely so we had this discussion shortly before the panel because
6138600	6144840	yeah uh ai is of course a turbo charger for a number of technologies that are being developed
6144840	6151480	at a fast pace so we are also looking at mirror technology and quantum and biotech and i mean a
6151480	6156200	lot of here in the room are looking at different technologies but the power come from the combination
6156200	6165480	of those and they also round the corner ron final remarks yes yes it works um first i agree that we
6165480	6170440	should both look into both the models and the applications not one of them but um what i think
6170440	6176120	is that we yes we need science but we do not have decades um so we need some sort of form of a
6176120	6181320	rapid response mechanism and in that we need a credible helix we need both government civil
6181320	6190280	society we need science um and and we need all of them at the table thanks closing words yana
6194120	6199480	okay one thing i would say about the regulation issue i think uh friends we most of it's like
6199480	6204760	he has this concept of dial-up progress uh that a lot of conversations end up in like
6204760	6209800	do we need more progress or less progress uh which is kind of like way too black and white
6210600	6214920	way of looking at things what you actually want to do is like look look like there are different
6214920	6219800	ways uh where we want more progress and different places where we want want less progress so it's
6219800	6224440	actually completely consistent to believe as i believe that yes we have over like over-regulated
6224440	6231480	a lot of things in a way that is kind of detrimental uh for us but that doesn't mean that we really
6231480	6236520	should stop regulating things and new things as they come up thanks so please stay on the
6236520	6242760	stage for a moment we're going to have a few closing words from Otto who is the head of
6242760	6249880	ero which is part of the organization that has made this happen Otto are you here
6250840	6257800	yes and by the way this discussion is a prelude to an even more important discussion which is
6257800	6261880	going to be taking place in the pub in the good old british tradition afterwards some of you might
6261880	6267320	want to join us where we can get around to all of you who had hand ups and i unfortunately couldn't
6267320	6282920	take your question Otto thank you David sorry um yeah thanks and thank you all of so so much
6282920	6289960	for being present here today um as some of us has already mentioned we're here in wilton hall
6289960	6295320	this was built in 1943 as an assembly hall for the world for two codebreakers and while deciphering
6295320	6301400	they have progressed beyond imagination and borrow multiple exponential curves here hardware
6301400	6306840	data quantity algorithm capabilities are all growing with tens of percentage points per year
6307720	6314760	so i think we all or at least a lot in this room will suspect where this leads uh which is a i that
6314760	6320440	has the capability to do mental tasks much better than we can and of course this presents amazing
6320440	6326200	opportunities but according to most existential risk experts we also risk nothing short of human
6326200	6333240	extinction here and it does mean that our species is on the line so when i turned on the radio last
6333240	6339000	weekend the bbc was discussing human extinction by ai and i think that this was dramatic but also
6339000	6345400	hopeful at the same time so i thought this was dramatic since human extinction caused by our
6345400	6352040	own actions is now officially a possibility and it never ceases to amaze me that we have been stupid
6352040	6358440	enough to let it get this far but hearing this discussed on national radio for me was also extremely
6358440	6365160	hopeful because up until now attempts to reduce the real human extinction risks were minor and
6365160	6370680	world leaders were not paying attention and with the summits that's starting tomorrow i think this
6370680	6377160	is really changing so i think it's hopeful that after the uk's prime minister speech on ai last
6377160	6382040	week in which he explicitly warned of human extinction risks the questions that followed
6382040	6388360	from the press were no longer about prime minister is this a real concern shouldn't you be concerned
6388440	6396280	about the bills of the of your people instead of this but instead at least some of the questions
6396280	6401480	were about are you addressing this problem seriously enough and shouldn't we consider instead
6401480	6407880	posing ai a moratorium or are you doing the right thing with backing responsible scaling
6409000	6414040	and i think this is exactly the debate that we need so i think it's now important that we
6414040	6420280	continue in this direction so we must organize ai's safety summits much more often we must open
6420280	6426200	them up so everybody gets to say we must have societal debates about this and i think in general
6426200	6431960	we must come together to coordinate and if we do that we are confident at the extension risk
6431960	6436840	observatory that we can implement the measures that are needed and this is why we have organized
6436840	6442760	this event and i think it's a huge privilege that we're able to do this together with conjecture
6442760	6449000	so thank you so much for co-organizing this event and we also want to continue organizing events
6449000	6454680	like this one but it's impossible without the support of all of you so if you want to support us
6454680	6461000	doing this there was a flyer that you got handed at the beginning please scan the QR code there
6461000	6466520	and there's possibilities to support us could be with funding could be with volunteering
6466520	6470840	could be with just following and sharing our content so this is enormously appreciated
6471800	6476920	and with that can i please get some applause for all our amazing speakers professor Stuart Russell
6479400	6482600	Connolly Professor Max Teckmark
6484120	6487880	Jan Tellin Annika Brack Mark Brackle Ron Rosendell
6487880	6491640	Alexandra Moussevisidae Andrea Mariotti and Helhotzen please don't stop clapping
6491640	6505800	um and finally a special thanks as well to David Wood a moderator Ruben Dileman, Katrina Joslyn,
6505800	6511240	Connor Axiotis, Sue Chisholm, Tillman Schepke, Niky Drogdowski, Mark van der Waal and Joep Soeren
6511240	6514760	and everyone here at Wilton Hall who made this all possible thanks a lot for helping us out
6521640	6528680	and please join us for drinks at Three Trees which is about 10 to 15 minutes
6528680	6531160	more from here so i hope to see you all there thank you
6535080	6539160	and some people believe in the future there's going to be a wedding in here shortly
6539160	6543320	so we all need to get out unfortunately unless we're part of that wedding crowd
6543320	6554280	so biome is chat but chat whilst moving out thank you
