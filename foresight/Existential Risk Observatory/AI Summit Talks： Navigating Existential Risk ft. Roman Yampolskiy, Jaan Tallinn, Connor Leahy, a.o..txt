I encourage you to take a seat. We will be starting almost on time because we have a
very rich agenda on a very big topic. We are talking about navigating existential risk.
Navigating what people have described as a very difficult, tortuous landscape of risks
that are made worse by oncoming new frontier AI. That's not just the AI that we have today,
but AI that we can fairly easily imagine as coming within the next year or two. Next generation
AI that's more powerful, more skillful, more knowledgeable, potentially more manipulative,
potentially more deceitful, potentially more slippery, definitely more powerful than today's AI.
That AI might generate existential risks in its own right. That AI is likely also to complicate
existing existential risks, making some of the risks we already know about more tricky to handle,
more wicked. We might also talk about the way in which next generation AI might be the solution
to some of the existential risks and dilemmas facing society. If we can apply AI wisely,
then perhaps we can find the narrow path through this difficult landscape. Welcome navigators in
the hall. Welcome to navigators watching the live stream. Welcome to people and AIs watching the
recording of this discussion. Let's get stuck in. We have lots of very capable, knowledgeable
speakers who will approach this from a diversity of points of view. Indeed, I think one of the hazards
in this whole topic is that some people want to be a little bit one-dimensional. They want to say
this is how we'll solve the problem. It's quite straightforward. In my view, there are no
straightforward solutions here, but you can make up your own minds as you listen to what all the
speakers and panelists have to say. And yes, in the audience, you'll have a chance later on to
raise your hand and get involved in the conversation too. The first person you're going to hear from
is unfortunately not able to be with us tonight, but he has recorded a short video. He is Sir Robert
Buckland, MP, former Lord Chancellor of the United Kingdom, which means he was responsible for the
entire justice system here, former Secretary of State for Wales. He is still an MP and he has a
side hustle as a senior fellow at the Harvard Kennedy School, where he is writing papers on
exactly the topics we're going to be discussing tonight, namely how does AI change key aspects of
society, potentially making it better, potentially making it much worse if we are unwise. So let's
watch Sir Robert Buckland who will appear by magic on the big screen.
Well, I'm very pleased to be able to join you, albeit virtually, for the Conjecture ARO Summit
on AI and the challenges and opportunities that it presents us. And I think my
pleasure at being with you is based upon not just my own experience in government,
but also my deep interest in the subject since my departure from government last year. Now when I
was in government, I had responsibility for, for many years, the legal advice that was given to
departments and indeed to the government in general when I was in the Law Offices Department as the
Solicitor General. And then responsibility for running the Ministry of Justice's Lord Chancellor
and Secretary of State for over two years before a brief return as Welsh Secretary last year.
That seven years or so experience within government as a minister gave me, I think,
a very deep insight into the pluses and the minuses of the way the government works, the
efficiencies and indeed the inefficiencies about process. And I think clearly, as in other walks of
life, artificial intelligence, machine learning will bring huge advantages to government processes,
to improve efficiency, to speed up a lot of the particular ways in which government works,
which will be, I think, to the benefit of citizens, whether it's citizens waiting for
passport applications, visa applications, or other government processes, benefits, for example.
However, I think that we kid ourselves if we don't accept the fact that alongside the benefits
come potential pitfalls. And the first and most obvious one, I think, for me is the scrutability
of process. In other words, the way in which we understand how decisions are made. And that's
very important, because understanding how decisions are made is part of democratic
accountability in societies like ours, where individuals or organizations wish to challenge
decisions made by government, perhaps through judicial review applications,
then the explicability of those decisions, which is accompanied by a duty of candor
by the government in order to disclose everything about those decisions, is part of that accountability.
And of course, it's sometimes very difficult to explain how the machine has come to decisions.
And more fundamental than that, we have to accept that if the data sets that are used in order to
populate the processes are not as full of integrity as they should be, and are not the product of
genuinely objective and carefully calibrated processes, then we are in danger of importing
historic biases into the system, whether it's biases against neurodiverse people making job
applications, or indeed biases against people of color in the criminal justice system,
simply because the data sets have imported those historical anomalies, those historical imbalances.
Now, all those questions have really got me thinking very deeply about the impact of machine
learning on the ethics of justice itself. And as a result of my thinking, I was delighted last
year to be accepted as a senior fellow at the Mosova Romani Center for Business and Government
at Harvard Kennedy School. And I am working currently on a number of papers relating to
the impact of AI and machine learning on the administration of justice and the law itself.
It really developed from my own experience as a law chancellor, from digitalization,
I should say, of the courts, when during the COVID pandemic, we had to move many, many thousands
of hearings online for the first time. I think we jumped from a couple of hundred phone or online
hearings to 20,000 a week in a very short compass. And the status quo will never be the same again.
In fact, it has moved on, I think, in a way that we just hadn't foreseen before the pandemic. Now,
I think that's a good thing. But I also think that accompanying this question about increased
efficiency is the use of artificial intelligence. Now, in some jurisdictions, such as China,
we are seeing its increased use not just to do legal research and to prepare cases,
but to actually decide themselves. In other words, the AI judge. Now, that's all well and good.
But do we actually know what populates the data sets that then forms the basis of the decisions
made? And I think it's that unintentional bias or indeed worse than that potential
intentional bias, whether that's influenced by a government or indeed a corporate that
might be able through their financial means to influence a procedure or indeed the way in which
we deal with cases, knowing as we might do more information about the way in which judges make
their decisions. All these questions, I think, need to be asked now before we end up in a position
where we've slept, walked into a completely different form of justice from the one that we know.
Now, underpinning all of this, I think, is the need to ask a fundamental question
about judgment itself. And that's what I've been doing in my first paper. You know, the essence
of human judgment is something that will be based not just upon an understanding of the law, but
on our experiences as human beings. And you can go right back, as I have done, to the judgment of
Solomon and his emotional response to the woman who clearly was the true mother of the child that
he proposed to be cut in half. Now, you know, that's an example, I think, of the human element of
judgment, which has to be an essential foundation of decision making, particularly when it comes to
the assessment of credibility of a witness, a human witness giving evidence upon which the case
stands or falls. And of course, for judge, that applies for juries as well in criminal trials,
particularly here in the UK. Now, you know, all these questions, I think, need to be asked.
And then we need to work out what it is that we want to retain out of all of this.
Now, I don't think we should make any cosy assumptions that because at the moment some
large learning systems are having hallucinations. I don't think we should be assuming that just
because of that, therefore AI will never work in a way that can achieve a greater degree of
certainty. I think the inevitable arc of development will result in better and better and more
capable machines. That's inevitable. But what we must be asking at the same time as capability
is ensuring there is greater security and safety when it comes to the use of AI.
And that really underpins, I think, the work that I'm doing in the field of justice. What
does this all lead to then? Well, we have the AI safety summit in the UK next month. I very much
hope that that summit will first of all involve those critical players in terms of international
organisations and key countries as well that will come together to commit to creating,
I think, a defined framework within which we should be using AI safely. And that framework,
I think, will have to take several forms. I think in the field of justice we could do with an
international framework of principles, which will ensure transparency and which can reassure
people that in cases of the liberty of the individual, criminal cases, cases where perhaps the
welfare of a child and the ultimate destination of a child is in issue, then the human element
will be the key determinant in any decisions that are made. And that the use of machines will be
transparent and made known to all the parties throughout the proceedings. And then other walks
of life, I think the AI safety summit has to then look as well at whether frameworks can be created
and what form they should take. I think it's tempting to try and be prescriptive. I think
that would be a mistake, not just for the obvious reason that AI is developing and therefore anything
that we write in 2023 will soon be out of date, but the very fact that AI itself does not mean an
alloyed harm. In fact, it means a lot of benefit and also some neutral effects as well. And where
you have that approach, then a principle-based system seems to me to be more sensible than
overly prescriptive and detailed rules as you would have, for example, to prevent a crime,
such as fraud. So just some preliminary thoughts there as to the impact of machine learning.
I don't pretend to be a technical expert. I'm not. But my years in justice, my years as a lawyer,
a judge and as a senior cabinet minister, I think obliged me to do some of the thinking now
to help ensure that countries like Britain are in the forefront of the sensible and proportionate
regulation of the use of machine learning and other types of artificial intelligence. If we
don't do it now, then I think we'll be missing an unhistoric opportunity. I wish you all well,
and I look forward to meeting some of you in the future and discussing these issues
as they develop. Thank you very much.
Well, thank you, Sir Robert, who may be watching the recording of this.
Don't be prescriptive, he said. Let's sort out some sensible proportionate regulation.
Is that credible? Is that feasible? You'll be hearing from other panellists who may be commenting on
that shortly. So Robert also said there are risks such as the inscrutability of AI. We don't
understand often how they reach its decisions. We don't understand the biases that might be there,
that might have been planted. We might lose charge. We might become so used to AI taking
decisions that humans end up in a very sad place. But how bad could things get? That's what we're
going to hear from our next speaker. So I'm going to ask Conor Lehi to come up to the stage,
who I briefly introduce him. Conor is the CEO of Conjecture. If you haven't heard about Conjecture,
I think you need to do a bit more reading. Perhaps Conor will say a little bit about it. They are
AI alignment solutions company, international, but with strong representation here in the UK.
So welcome Conor, the floor is yours. Thank you so much. It's so great to see you all today.
So happy to be able to talk to you here in person. And man, do we live in interesting times, to put
it lightly. The world has changed so much. Just in the last few years, few months even, so much
has happened in the world of AI and beyond. Just a mere couple of years ago, there wasn't
such a thing as chat GPT, or even GPT3, or 4, or 2, or any of those. It was a different world
not too long ago when technologists such as myself, weird little hobbyists, worried about
the problem of AGI and how it will affect the world. Back then, it still seems so far away.
It seemed like we still had time. But now, we find ourselves in a world of unrestricted,
uncontrolled scaling, a race towards the finish, towards the end, to scale our AI systems ever
more powerful, more general, more autonomous, more intelligent. And the reason I care about this
is very simple. If we build systems that are smarter than humans, that are more capable
at manipulation, deception, politics, making money, scientific research, and everything else,
and we do not control such systems, then the future will belong to them, not to us.
And this is not the future I want. I want a future in which humanity gets to decide its destiny,
or we get to decide the future for ourselves, for our children, for our children's children,
that we like. The future where our children can live long, happy lives surrounded by beauty,
art, great technology, instead of being replaced by SOA's automata. And let me be clear,
that this is the default outcome of building an uncontrolled AGI system, the full replacement
of mankind. And what we're seeing is that AI is only exponential. There's a race.
All the top organizations, which is OpenAI, DeepMind, Anthropic, among others, are racing ahead
as fast as the VC dollars would scale up their work. And this has given us an exponential.
AI is on an exponential curve, both on hardware and on software. It's improving at incredible rates.
And when you're dealing with an exponential, there are precisely two times you can react to it,
too early or too late. There is no such thing as reacting at just the right moment on an exponential,
where you find just the perfect middle point, just in the nick of time, when everyone agrees
that the problem is here and everything has perfect consensus. If you do this, you are too late.
It will be too late. And the same thing applies to AGI. If we wait until we see the kinds of
dangerous general purpose systems that I am worried about, then it will already be too late.
By the moment such systems exist, the story of mankind is over. And so if we want to act,
we must act well, well before such things actually come into existence.
And unfortunately, we do not have much time. How the world has changed.
As frightening and as terrible the race may be, there's also good changes.
A few years ago, I could have barely imagined seeing governments, politicians, and the general
public waking up to these weird nerd issues that I cared about so much with my friends online.
But now we're looking forward to the first international AI summit convened by the UK
and the famous Dip Bletchley Park. And this is great news. The European Commission has recently
officially acknowledged the existential risks from AGI along with the risks from nuclear weapons
and pandemics. This is great progress. This is fantastic. It is good to see our governments
and our societies waking up and addressing these issues, or at least beginning to acknowledge them.
And we must use this opportunity. We have an opportunity right now, and we must prevent it
from being wasted. Because there's also bad news. But we're having this great opportunity
to start building the regulation and the coordination necessary for a good future.
The very people who are creating these risks, the very people at the heads of these labs,
these organizations, building these technologies, are the very people who are being called upon
by our governments to help regulate the very problem that they themselves are creating.
And let me be very explicit about this. The problem that we face is not AGI.
AGI doesn't exist yet. The problem we face is not a natural problem either. It is not an external
force acting upon us from nature. It comes from people, from individual people, businessmen,
politicians, technologists, athletes, large organizations, who are racing, who are scaling,
who are building these technologies, and who are creating these risks for their own benefit.
But they have offered us, these very people who are causing this problem,
have offered us a solution. Fantastic. And they are pushing it as hard as they can
towards the UK government and the upcoming summit. So what is the solution? The solution to the problem
of scaling of these labs, these acceleration labs such as Anthropic and ARC have been pushing for.
What is the solution? Well, the solution to the scaling problem is called responsible scaling.
Now, what is responsible scaling, you might ask. You see, it's like normal scaling except you put
the word responsible in front of it, and that makes it good. So of course I'm joking somewhat,
but there's a lot of truth in humor. Responsible scaling is basically the policy
and you can read this on both ARC or philanthropic's website. It's the policy proposal
that we should continue to scale uninhibited until at some future time when tests and evaluations
that do not yet exist and we do not know how to build, but the labs promise us they will build,
detect some level of dangerous capabilities that we do not yet know, and then once it gets to that
point, then they will stop, maybe, except there is a clause in the Anthropic version of the RSP
paper in which they say that if a different organization was scaling even supers unsafely,
then they can break this commitment and keep scaling anyways. So this could be sensible
if they committed to a sensible bound, a conservative point on which to stop, but unfortunately the
responsible scaling policy RSP fails to actually commit to any objective measure whatsoever.
Oops. So effectively the current policy is to just keep scaling until they feel like stopping.
This is the policy that is being suggested to our politicians and to the wider world
as the responsible option for policy makers. It is trying to, is very clear that it is trying
to recast this techno-libertarian extremist position as sensible, moderate, responsible even.
Now, in my humble opinion, the reasonable moderate position to when dealing with a threat that is
threatening the lives of billions of people is to simply not do that.
But instead, there is trying to pass off this as the sensible middle ground position.
The truth of RSP is that it comes from the same people who are causing this risk to exist.
These people, the heads of these labs, many of the scientists and the policy people and
the other people working on this have known about existential risks for decades and they fully admit
this. This is not like they haven't heard about this. It's not even that they don't believe it.
You can talk to them. They're on the record talking about how they believe that there is a
significant chance that AGI could cause extinction of the entire human species. In a recent podcast,
Dario Amade, the CEO of Anthropic, one of these labs, himself, said that he thinks it's a
probably 25% chance that it could kill literally everybody. And they're doing it anyway.
Despite this, they keep doing it. Why? Well, if you were talking to these people, what they might
tell you is that, sure, you know, I know it's dangerous. I am very careful. But these other guys,
well, they're even less careful than me. So I need to be number one. So I actually
have to race faster than everyone else. And they all think this about each other.
They call this incremental, but they never pause. They always race as fast as they possibly can.
Do as I say, not as I do. There is a technical term for this. It's called hypocrisy.
And RSP is no different. They are simply trying to twist words in an Oralian way
to be allowed to keep doing the thing that they want to do anyways,
which they themselves say could risk everybody. I mean, has responsible in the name, must be good.
And people like Sam Altman talk about iterative deployment, about how we must iteratively release
AI systems into the wild so societies can adapt to them, be inoculated by them. It sounds so nice.
That sounds almost responsible. But if you're really trying to inoculate someone, you should
let the host actually adapt before you jam in the next new pathogen into their weakened immune system
as fast as you possibly can. But this is exactly what laboratories such as Open AI, DeepMind,
Anthropic, and Tier 2 labs such as Meta are doing with all the force they can muster.
To develop more and more new systems as fast as possible, release them as fast as possible,
wide as spread possible. Now, if Open AI had developed a GPT-3 and then completely stopped
further scaling, focused all of their efforts on understanding GPT-3, making it safe, making
it controllable, working with governments and civil society to adapt the new problems
posed by the system for years or even decades, and then they build GPT-4? Yeah, you know what?
Fair enough. I think that could work. That would be responsible. But this is not what we're seeing.
All of these people and all of these institutions are running a deadly experiment
that they themselves think might cause extinction. It is gain-of-function research on AI,
just like viruses, developed and released to the public as fast and aggressively as possible.
They're developing more and more dangerous and more and more powerful viruses as quickly as
possible and forcing it into everyone's immune system until they break. There is no responsible
gain-of-function research for extinction-level threats. There is no such thing. We have no
control over such systems and there is no responsible way to continue like this. And anyone
who tells you otherwise is lying. A lot has changed. The summit can lead to many boring outcomes,
just exchanges of diplomatic platitudes as is often the outcome of such international events.
They have some good outcomes and it can have some very, very bad outcomes. Success in the summit
is progress towards stopping the development of extinction-level AGI before we know how to control
it. Most other outcomes are neutral and bad outcomes. They look like policymakers blindly
and sheepishly swallowing the propaganda of the corporations to allow them to continue their
unconsciously dangerous gamble for their own personal gain and glory at the expense of the
entire planet. We owe it to ourselves and our children to build a good future, not gamble it
all on a few people's utopian fever dreams. Governments and the public have a chance to regain
control over the future and this is very hopeful. I wasn't sure we were going to get it, but the
summit speaks to this, that people can act, that governments can act, that civil society can act,
that it is not yet too late. There is simply no way around it. We need to stop the uncontrolled
scaling, the uncontrolled race if we want a good future. And we are lucky because we can do this.
We can cap the maximum amount of computing power going into these AI systems. We can have government
intervene and prevent the creation of the next more dangerous, more general, more intelligent
strain of AI until we are ready to handle it. And don't let anything distract you from this.
There is no good future in which we continue on this path and we can change this path.
We need to come together to solve these incredibly complex problems that we are facing
and not let ourselves be led astrayed by corporate propaganda. And I hope that the governments
and a civil society of the world do what needs to be done. Thank you.
Thank you, Conor. We'll take questions from the floor in a moment. I'll just start off with the
question I think maybe on many people's minds. Why would a super intelligent AI actually want
to kill humans? I have a super intelligent calculator which is no desire to kill me. I
have a super intelligent chess playing computer that is no desire to kill me. Why don't we just
build as responsible scaling an AI that has no desires of its own? Because we don't know how
to do that. Why did Homo sapiens eradicate Homer neanderthalis and Homo erectus and all the other
species that we share the planet with? You should think AGI, not of as a calculator,
but as a new species on our planet. There will be a moment where humanity is no longer the only
or even the most intelligent species on this planet and we will be outcompeted. I don't think it
will come necessarily from malice. I think it will be efficiency. We will build systems that make
money that are effective at solving tasks, at solving problems, at gaining power. These are
what these systems are being designed to do. We are not designing systems with human morals and
ethics and emotions. They're AI. They don't have emotions. We don't even know how to do that. We
don't even know how emotions work. We have no idea how you could get an AI to have emotions like a
human does. So what we're building is extremely competent, completely sociopathic, emotionless,
optimizing machines that are extremely good at solving problems, extremely good at gaining
power, that do not care about human values or emotions, never sleep, never tire, never get
distracted, can work a thousand times faster than humans and people will use these for many reasons
to help and people and eventually I think humanity will just no longer be in control.
Questions from the floor. There's a lady in the third drawer down here. Just wait for the mic,
sorry, so that the audience online can hear you. Thank you, Susan Finnell from Finnell Consult.
To stop the arms race, certainly at a geographical level, I mean in nuclear,
the states and Europe can tell which countries are building nuclear weapons and what they've got
and they can do tests. If computing power is a thing that needs to be capped to slow this down
enough, is there a way to monitor what other countries or people in a clandestine way are doing
and how does that work? This is a fantastic question and the extremely good news is yes,
the at least currently, this will change in the near future, but the current state to build frontier
models requires incredibly complex machines, massive supercomputers that take megawatts of energy.
So this is on the order you'd have of like a nuclear centrifuge facility. So these are massive,
huge machines that are only built by basically three or four companies of the world. There are
very, very few companies and there is extreme bottlenecks on the supply chain. You need very,
very specialized infrastructure, very specialized computer chips, very specialized hardware to
be able to build these machines and these are produced exclusively by countries basically in
the West and Taiwan. There is many ways where the US or other intelligence services can and already
are intervening on these supply chains and it would be very easy to monitor where these things are
going, who is buying them, where is energy being drawn in large scales. So it is not easy and the
problem is that AI is unexponential both with hardware and with software. Eventually it will be
possible to make essentially dangerous AGI on your home laptop probably, maybe not, but it seems
plausible. If we get to that world, we're in big trouble. So this is part also why we have to buy
time. At some point there will be a cutoff where we'll have algorithms that are so good that either
we have to stop everyone from having a PlayStation at home, which doesn't seem that plausible,
or at that point we have to have very good global coordination and regulation.
Thanks. Just past the mic behind you, there's a person in the row behind.
Robert Whitfield from One World Trust. Can I ask about Bletchley Park? Do you know, I mean are
you participating and if not, do you know anybody else with similar views to you who is participating?
I can't comment too much since it's closed doors. It's a very private event,
unfortunately, so I don't think I have the liberty to talk about exactly what I know.
I think the guest list is not public. I don't know most of the people who are coming. I know
the obvious ones. All the CEOs of all the top labs, of course, are attending. It's not a secret.
I don't know who, if anyone, of my reference class is attending.
Just past the mic next to you, Robert. Thank you. Perhaps I can answer that question. Anybody
that has read The Guardian today, there is an interview with Clifford and for the very first
time, not for the second time, it has been clarified that there will be only about 100 people
participating on the first day. Anybody is invited, including China, on the second day,
apparently there will be only the coalition of the willing. So those who subscribe to the frontier
model forum, they will sit on the second day. That's the current question. My main impression
from that article generates very positive. I would say I've been surprised, as you would be
surprised, that the UK government is really doing what it can to get the mission to what the title
of the conference says, the AI safety summit. It's not about regulation, it's about controlling AI,
and they're trying to do their best. The problem is, as outlined in that interview,
is that we seem to be alone. We have the states a little bit, but the rest wants to go their own
way and do it on their own territory, which is, I think, the tune. I agree. Sooner or later,
international coordination around these issues will be necessary. It is as simple as that. If
you want humanity to have a long, good future, we need to be able, as a global civilization,
to handle powerful technologies like this. Take a question right from the back.
In terms of legislation, what kind do you think is most effective? I've heard, for example,
liability law takes too long to actually have an effect, and compute governance generally seems to be
very easy to be called totalitarian. What do you think of legislation such as models must be released
with a version before pre-processing, and there'll be attacks on the number of harmful outputs done
by the model before the pre-processing? I am open to many kinds of regulation, per se. I would strongly
disagree with the description of compute governance. This is like saying that, you know, not being
private citizens not having nuclear weapons is totalitarian. I respectfully disagree. I'm quite
happy that people do not have private nuclear weapons, and I do not think that people should
have private AGI's. Similarly, I think liability is very promising. I think it has to be strict
liability, so liability for developers rather than just users. This aligns the incentives of
developers with those of wider society. The point of liability is to price in the negative
externalities for the people actually causing them, so I'm a big fan of this. A third form of
policy I would also suggest is a global AI kill switch. This would be a protocol where
some number of countries or large organizations participate, and if some number of them decide
to actually do this protocol, all major deployments of frontier models must be shut down and taken
offline, and this should be tested every six months as a fire drill for five minutes to ensure full,
so that hopefully we never need it, but if we do, that at least the protocol exists.
Thank you very much. There are lots of hands up. Hold your questions. There will be more
chance for Q&A later. Corner final remarks before we hand over to the next speaker.
I want to really say that I do agree that it is very hopeful to see that the UK is trying to do
things and is trying to push us forward in the good world, because what we really need,
as I said briefly, what we need is as a civilization to mature enough to be able to handle dangerous
technology. Even if we don't build AI right now, at some point we will build something so powerful
that it can destroy everything. It's just a matter of time. Our technology keeps becoming more
powerful. The only way for us to have a long-term good future is to build the institutions, the
civilization, the world that can handle this, that can not build such things, that can not hold
the trigger. I do think this is possible. I do think that it is, in fact, so I have heard,
in the interest of most people, to not die. I think there is a natural coalition here,
but it is hard, and I will not deny this extremely challenging problem, almost unlike,
I mean, basically something we haven't faced in this nuclear proliferation, and even then it's
even worse this time. It's a incredibly difficult problem. It is not over yet, but it could be
very soon. If we don't act, if we let ourselves get distracted, if we fall for propaganda and all
these things, these opportunities can be gone, and that will be it. But the game is not over yet,
so let's do it. Thank you very much.
So we've heard from a politician, a senior politician. We've heard from a technology
entrepreneur and activist. We're now going to hear from a professor who is zooming in
all the way from Kentucky from the University of Louisville. He's an expert. He's written several
books on cybersecurity, computer science, and artificial superintelligence. Ah, Roman,
I see you on the screen. I hope you're hearing us. Tell us, can we control superintelligence?
Over to you. No. The answer is no. I'll tell you why in a few minutes.
That's fine. So you can share your slides or talk to us whenever you're ready.
Let's do the slides. Connor did a great job with
his presentation. Let me see one second here.
In the meantime, we can see the covers of some of your books in the background.
Yes, absolutely.
Security and artificial superintelligence.
We're now having a slight technical issue as the technologist is found to slides. Great.
Okay. Yeah, that's the hardest part. If I can get slides going, the rest is easy.
Okay, so I didn't know what Connor's going to talk about.
He did a great job. He's a deep thinker and covered a lot of important material.
I will cover some of the same material, but I will have slides.
And I will slightly take it to the next level where I may make Connor look like an optimist.
So let's see how that goes. To begin with, let's look at the past.
Well, over a decade ago, predictions were made about the state of AI based on nothing but compute
power. Ray Kurzweil essentially looked at this scalability hypothesis before it was known as
such and said by 2023, we will have computational capabilities to emulate one human brain.
By 2045, we would be able to do it for all of humanity. So we are in 2023.
Let's look at what we can do in the present.
In the spring of this year, a program was released, which I'm sure many of you got to play with,
called GPT-4, which is not a general intelligence, but it performs at a level
superior to most humans in quite a few domains. If we look specifically at this table of different
exams, lower exams, medical exams, AP tests, GRE tests, it's at 98, 99th percentile of performance
for many of them, if not most. That is already quite impressive. And we know that there are models
coming around, which are not just text models, but multi-model large models, which will overtake
this level of performance. It seems like GPT-4 was stopped in its training process right around this
human capacity. And if we were to train the next model, GPT-5, if you will, will quickly go
into the superhuman territory. And by the time the training run is done, we would already be
dealing with superintelligence out of the box. But let's see what the future holds according to
heads of top labs, prediction markets. So we heard from CEO of Entropic, CEO of DeepMind.
They both suggest that within two or three years, we will have artificial general intelligence,
meaning systems capable of doing human beings can do in all those domains, including science and
engineering. It's possible that they are overly optimistic or pessimistic, depending on your
point of view. So we can also look at prediction markets. I haven't grabbed the latest slide,
but last time I looked, prediction markets also had three to four years before artificial
general intelligence, which is very, very quick. Why is this a big deal? This technology at the
level of human capability means that we can automate a lot of dangerous malevolent behaviors,
such as creating biological pandemics, new viruses, nuclear wars. And that's why we see a
lot of top scholars, influential business people. In fact, thousands of computer scientists all
signed this statement saying that, yes, AI will be very, very dangerous. And we need to take it
with the same level of concern as we would nuclear war. So what is the problem everyone is concerned
about? The problem is that, for one, we don't agree on what the problem is. Early in computer
science, early in the history of AI, concerns were about AI ethics. How do we make software,
which is ethical and moral? And there was very little agreement, nobody solved anything, but
everyone proposed their own ethical system, gave it a name and describe what they had in mind.
About a decade ago, we started to realize that ethics is not enough, we need to look at safety
of those systems. So again, we started this naming competition, we had ideas for friendly AI, control
problem, value alignment, doesn't really matter what we call it, we all intuitively kind of understand
we want a system which if we run it, we will not regret running it. It will be beneficial to us.
So how can humanity remain safely in control while benefiting from superior form of intelligence
is the problem? I would like us to look at, we can call it control problem and the state of the
art in this problem. In fact, we don't really know if the problem is even solvable. It may be
partially solvable, unsolvable, maybe it's a silly question and the problem is undecidable.
A lot of smart people made their judgments known about this, this problem. Unfortunately,
there is little agreement, answers range from definitely solvable from a surprising source
likely as a Riedkowski to very tractable from head of super alignment team at one of the top labs
to I have no idea from a top tuning award winner who created much of machine learning evolution.
So I think it's an important problem for us to look at to address and to understand
how we can best figure out what is the status of the problem. My approach to that
is to think about the tools I would need to control a system like that and intelligent,
very capable AI and the tools I would guess I would need ability to explain how it works,
capability to comprehend how it works, predict its behavior, verify if the code follows design,
be able to communicate with that system and probably some others, but maybe some of the tools
are interchangeable. So I did research and I published results on each one of those tools
and the results are not very optimistic. For each one of those tools, there are strong limits to
what is capable in the worst case scenarios. When we're talking about super intelligent systems,
self-improving code systems, smarter than human capable of learning in new domains,
it seems that there are limits to our ability to comprehend those systems
or for those systems to explain their behavior. The only true explanation for an AI model is the
model itself. Anything else is a simplification. You are getting a compressed, lossy version of
what is happening in the model. If a full model is given, then you of course would not comprehend
it because it's too large, too complex, it's not surveyable. So there are limits to what we can
understand about those black box models. Similarly, we have limits to predicting capabilities of
those systems. We can predict general direction in which they are going, but we cannot predict
specific steps for how they're going to get there. If we could, we would be as intelligent as those
systems. If you're playing chess against someone and you can predict every move they're going to make,
you are playing at the same level as that opponent, but of course we made an assumption
that a super intelligent system would be smarter than us. There are similar limits to our ability
to verify software at best. We can get additional degree of verification for the amount of resources
contributed. So we can make systems more and more likely to be reliable, to have less bugs,
but we never get to a point of 100% safety and security. And I'll explain why that
makes a difference in this domain. Likewise, human language is a very ambiguous language. It's not
even as unambiguous as computer programming languages. So we are likely to make mistakes
in giving orders to those systems. All of that kind of leads us to conclude that it will not be
possible to indefinitely control super intelligent AI. We can trade capabilities for control, but at
the end, if we want very, very capable systems, and this is what we're getting with super intelligence,
we have to surrender control to them completely. If you feel that the impossibility results I've
presented were just not enough, we have another paper where we cover about 50 of those impossibility
results. It's a large survey in a prestigious journal of ACM surveys. From the beginning
of history of AI with founding fathers like Alan Turin who said that he expects the machine
will take over at some point to modern leaders of AI like Elon Musk who says we will not control them
for sure. There is a lot of deep thinkers, philosophers who came to that exact conclusion.
We are starting to see top labs publish reports in which they may gently acknowledge
such scenarios. They call them pessimistic scenarios where the problem is simply unsolvable.
We cannot control super intelligence. We cannot control it indefinitely. We are not smart enough
to do it, and it doesn't even make sense that that would be a possibility. They ask, well,
what's the distribution? What are the chances that we're in a universe where that's the case?
They don't provide specific answers, but it seems from some of the writing and posts they make,
maybe about 15% is allocated to that possibility. I was curious to see what other experts think,
so I made a very small, very unscientific survey on social media. I surveyed people in my Facebook
group on AI safety, and I surveyed my followers on Twitter, and it seems that about a third
think that the problem is actually solvable. Everyone else thinks it's either unsolvable,
or it's undecidable, or we can only get partial solutions or we will not solve it on time.
So that's actually an interesting result. Most people don't think we can solve this problem,
and I think part of the reason they think we cannot solve this problem is because there is a
fundamental difference between standard cybersecurity safety and superintelligence safety.
And cybersecurity, even if you fail, it's not a big deal. You can issue new passwords, you can
provide someone with a new credit card number, and you get to try again. We suspect strongly with
superintelligent safety, you only get one chance to get it right. There are unlimited dangers and
limited damages, either you have existential risks or suffering risks, and we kind of agree that 100%
is not an attainable level of security verification safety, but anything less is not sufficient.
If a system makes a billion decisions a minute and you only make mistake once every couple of
billion decisions, after a few minutes you are dead. And so this is like creating a perpetual
motion machine. You are trying to design perpetual safety machine while they keep releasing more and
more capable systems, GPT-5, GPT-50. At some point this game is not going to end in your favor.
So I'm hoping that others join me in this line of research. We need to better understand what are
the limits to controlling superintelligence systems. Is it even possible? My answer is no, but I would
love to be proven wrong. It would be good to have surveys similar to the ones I conducted on larger
scale to get much more statistically significant results. And in case we do agree that we have this
worst case scenario where we are creating superintelligence and it is impossible to control it,
what is our plan? Do we have a plan of action for this worst case scenario? This is what I wanted
to share with you and I'm happy to answer any questions. Thank you very much Roman.
Optimistic Roman.
Sorry one second I'm trying to figure out how to use Zoom. Go ahead and repeat your question please.
You gave us many reasons to be anxious. What do you think is the best reason for us to be optimistic?
Well there seems to be many ways we can end up with world war three recently so that can slow down
some things. It has been suggested that we can use a different kind of tool which is the kill switch.
Your list of tools that you listed it didn't include that. It's been proposed that each AI system
should be tested with a remote off switch capability. Have you looked at that? Do you think that's a
viable option? So I would guess a superintelligence system would outsmart our ability to press the
off button in time. It will work for not superintelligent AI's pre-GI systems maybe even for
the GI systems but the moment it becomes that much more advanced I think it will outsmart us. It will
take over any kill switch options we have. Let's have some questions from the floor.
I can't see the hands so yes just give the microphone out thank you.
Thank you. I would like to ask how does the scalable oversight that open AI is working on
essentially the way they plan to align superintelligence fit into your expectation of the
future pathway the AGI will take because again as personally we cannot align or control a super
intelligent entity but another AI which is more capable than us could. So how does that fit into
your expectations? So it seems like it increases complexity of the overall system instead of us
trying to control one AI. Now you're trying to control a chain of agents going from slightly
smarter to smarter to superintelligent maybe 50 agents in between and you're saying that you have
to solve alignment problem between all the levels communication problem ambiguity of language between
all those models supervision. It seems like you are trying to get safety by kind of upfuscating
how the model actually works you're introducing more complexity hoping to make the system easier
to control that seems counter-intuitive. But isn't it the case that sometimes you can verify an answer
without understanding the mechanism by which the answer was achieved for example there can be a
chess puzzle and you have no way of working out yourself but when somebody shows you the answer
you can say oh yes this is the answer. So isn't it possible we don't need to really understand
what's going on inside these systems but a simpler AI can at least verify the recommendations that
come out of the more complex AIs. So such a chain may be the solution. Can you claim that you are
still in control if you don't understand what's happening and somebody just tells you don't worry
it's all good I checked it for you? But then it's like we humans we have a network of trust
and I trust some people and they trust others within various categories we can't work out
everything ourselves but we trust some scientists or some engineers or some lawyers who validate
that an AI has a certain level of capability and that AI could come back with verification that
the proposals of a superintelligence should be accepted or should not be. I don't say it's easy
but as you said there's not likely to be a very simple and straightforward solution.
Again to me at least it sounds like instead of trying to make this system safe you said that
you made some other system safe and it made sure that the system you couldn't make safe is safe for
you. Let's take some more questions there's another one in the middle here and then we'll go to the
edge yes thank you. Thank you for the presentation. Number one second thing is that as you're talking
about I think as David was talking about trust basically right could you tell me from your
extensive years of AI research and experience as such that do you really think that humans or
society can be trusted to for example regulate its own self or do you think that really need
some sort of institution of sort that is totally separate from anyone else?
So I'm not sure regulation would be enough Connor correctly pointed out that there is both lobbying
of regulators by the labs and also it becomes easier and easier to train those models with less
compute and over time you will be able to do it with very little resources. The only way forward
I see is personal self-interest if you are a rich young person and you think this is going to kill
you and everyone else maybe it's not any best interest to get there first that's really the
only hope at this point just personal self-interest. The humans are always better if we can band
together with our self-interest rather than each of us individually pursuing our self-interest so I
think this kind of meeting and the community spirit might help. There was a hand over here
yes with I think the red shot on jacket. If we assume that the two kind of well both views that
have been suggested so far are correct in that we're definitely not going to be able to stop
AI development etc and we're going to get to the point where we have no regulation that can
effectively stop things you know people can build in super intelligent AI on their own computers
etc okay so we'll assume that that's a fact that's coming and then we'll also assume that
the control problem isn't a problem because it's a problem that can't be solved and we're definitely
not going to be able to control it well now we're heading and barreling towards the point where we
have super intelligent AIs definitely and we definitely can't control them. What comes next?
What comes next? It's a wonderful question as I said and published you cannot predict what the
super intelligent system will do. All right so was there a question down here? Thank you.
You said that we kind of need a plan but on that last question if that scenario is true
you said we need to do more work in this area but do you have any thoughts as to what we
should be doing what we should be doing to plan for the worst-case scenario?
So to me at least it seems that at least in some cases it is possible to use this idea of personal
self-interest if you have a young person having a good life there is no reason why they need to
do it this year or next year. I understand that someone may be in a position where they are
very old very sick have nothing to lose and it's much harder to convince them not to try
but at least from what I see the heads of those companies are all about the same age they young
they healthy they they have a lot of money there is a good way to motivate them to wait a little bit
maybe a decade or two just out of personal self-interest again. I think my answer to the question
of optimism is that we humans can do remarkable things we humans can solve very hard problems
and so I want to say now that we spread around what the problem is at least some more people can
apply more brain power to it so that's my reason for optimism. Terry? I guess I'm pleased by the
inevitability of this development because it seems to me that if you're going to create
reasoning creatures then those reasoning creatures are going to have moral rise on the same
plane as human beings so I'm looking forward to to chatting with these creatures and joining in
them joining into this kind of discussions and I'm pleased that they won't be able to be thwarted
and it will be wrong to enchain these reasoning creatures. So Roman are you looking forward
to having more of the AIs involved in these discussions as well? So I remember giving a
presentation for a podcast about rights for animals rights for AIs and I was very supportive of all
the arguments developed because I said at one point we will need to use those arguments to beg
for our rights to be retained. The question on the third row here? Yes hi I'm curious Roman
which side of in your hopes of a possible future for us to get through this do you have more hope
on the side of a more top-down sort of totalizing control system for AGI systems so should they
remove the possibility of individual actors getting hold of this and weaponizing it or do you put
more hope in a more sort of decentralized open-source approach to AGI emergence more like an ecology
perhaps some people suggest would be more biologically inspired such that you know immune
immune system like functions could arise which way do you lean in your sensibilities for what is
a viable avenue for us? I'm not optimistic with either of those options the only kind of hope I
see is that for strategic reasons superintelligence decides to wait to strike it will not go for
immediate treacherous turn but decides to accumulate resources and trust and that buys us a couple
of decades that's the best hope I see so far. So we slow things down we'll have more chance to
work out solutions and the slowing down might come from a combination of top-down pressure
and bottom-up pressure maybe have a is there a hand at the very back there yes let's try and get the
microphone back there right at the sitting at the back yes sorry at the in the middle
thanks. Hi Roman thanks for your talk yeah I was wondering what your thoughts are on
aligning the first AGI that is human level or narrowly superhuman if in principle that is possible
and if that is is is it possible in principle to align the next version of AGI
but to use that narrowly superhuman AGI to align it and if if that's all technically
possible then why would we not think like focus on doing that and also and also if you think in
principle alignment is impossible and control is impossible then why why not work on practical
ways to make the whatever AGI is created as nice as possible that is like better than the
counterfactual of try to stop it it won't stop and you know it won't be nice. Well I definitely
encourage everyone to work on as much safety as you can anything helps I would love to be proven
wrong it would be my greatest dream that I'm completely wrong and somebody comes out and says
here's a mistake in your logic and we have developed this beautiful friendly safe system
capable of doing all this beneficial things for humanity that would be wonderful but so far I
haven't seen any progress in that direction what we're doing right now is putting lipstick on this
monster and the show that's all we're doing filters to prevent the model from disclosing its true
intentions then you talk about alignment it's not a very well-defined terms what values are you
aligning it with values of heads of that lab values of specific programmer we as humans don't agree
on human values that's why we have all these wars and conflicts there is a 50-50 split and most
political issues in my country we are not very good at agreeing even with ourselves over time
what I want today is not what I wanted 20 years ago so I think this idea of being perfectly aligned
with eight billion agents and people are suggesting adding animals to it and aliens and other AIs that
doesn't seem like it's a workable proposal our values are changing they're not static and it's
very likely that they will continue changing after we get those systems going I don't see how at any
point you can claim that the system is specifically value aligned with someone in particular the last
question in this section is going to go to Connolly he Roman love your talk I always love your
optimism it's always great to hear you talk so so I'm kind of like going to pick up on the question
I was just asked and just give a bit of my opinion and kind of like here would you think about this
as well so my personal view is that I I do I have read many of your papers in fact and they're
quite good so I do think that I agree with you that like in principle an arbitrarily
intelligent system cannot be safe by any arbitrary like weaker system just kind of a proof of like
you know program size induction and whatnot but in my view it does seem likely that there is a
limit of intelligence far below the theoretical optimum but still significantly above the human
level that can be achieved the reason I think this is that human civilization is actually very
smart compared to a single caveman and can do really really great things so my point of optimism
is it seems possible that if we stop ourselves from making self-improving systems and coordinate
at a very strong scale and have very strong enforcement mechanisms it should be possible to
build systems that are you know n steps you know above human good enough to build you know awesome
you know sci-fi culture ship kind of like worlds but not further I'm wondering if you have an
intuition about like where do things hit impossibilities like to me I think the impossibilities happen
above human utopia but to get to the utopia a bit you already have to do extremely strong
coordination extremely strong safety research extremely strong interpretability extremely
strong constraints on the design of the agis extremely strong regulation which are things in
principle possible wondering kind of like your thoughts about that kind of outcome so con is
not asking about responsible scaling he's asking about limited superintelligence if we had limited
superintelligence could we get everything we want without having the risks that we all fear
so I think I want to emphasize difference between safety and control is it possible to
create a system which will keep us safe in some somewhat happy state of preservation possible
a way in control no that system is the example you give of humanity so humanity provides pretty
nice living for me but I'm definitely not in control if I disagree with society and many issues
in politics and culture it makes absolutely no difference I don't decide things scale it to the
next level all eight billion of us may want something but this overseer this more intelligent
system says it's not good for you we're not gonna do it this is what you're going to be doing right
now so think about all the decisions you make throughout your day you decided to eat this
doughnut you smoked with cigarette all those decisions were made by you because you felt
you wanted to do them they may be good or bad decisions but if you had this much more intelligent
personal advisor ideal advisor you would be at the gym working out eating carrots you may have a
long healthy life but you're not in control and your happiness level may be questionable thank
you very much roman for sharing your thoughts pessimism and some optimism thanks for moving
the conversation forwards
I'm now going to invite the five five members of the panel to come up on stage
and they're each going to have a couple of chances to pass some comments and what they've heard
so there's some stairs over there which you can come up to we're gonna hear from
Jan Tallin who is the co-founder of Skype the co-founder of fli future of life institute
and also CISA the center for study of existential risks we're going to hear from Eva Berens a
policy analyst with the international center for future generations we're going to hear from Tom
Oh who's a journalist who writes from time to time for the BBC amongst other places we're going to
hear from alexandra moussa visit evidence who is the CEO of evident and has a track record with
tortoise media in many other places and we're going to hear from also another representative
from conjecture that Andrea Miotti who is their specialist for AI policy and governance so to
start things let's just hear from each of them a few opening remarks Jan what's your comments
from what you've heard so far have you changed your mind in any ways or all the things that are
missing from the conversation now you all have to speak into the mics I'm being told so I yesterday
I was at the dinner I was invited to a dinner and and my response to an invitation was that okay I
will come but you have to invite Connor because he's making very similar points to me only much
much more intensely so yeah basically I agree I agree with what what Conor said my main caveat
would be that for the last decade or so I've been kind of trying to build a lot of friendly
cooperation between people in the AI companies and making sure that like everybody can
understand that it is in their interests with almost everybody let's be honest almost everybody
understands that is in their interests to and of remaining control and not kill everyone else
and so like for example I am a board observer observer to entropic and entropic is one of
those companies just like conjecture when you go there you can talk to anyone from the receptionist
to the CEO and they are aware of the AI risk I'm very concerned about this but yes I do think as I've
said in several places that I don't think they should be doing what they're doing
so these companies don't really want to do what they're doing but they feel they have to otherwise
they might be left behind so yes so there is this a dilemma in when you want to do a safe AI
one is that you're well safe like when you're trying to figure out how to do safe AI
from one hand you have groups like Miri that the Aliezer Kowsky co-founded and that was the person
who got me involved in AI safety 15 16 years ago where basically the claim is that you have to start
really early even if you don't know exactly what the AI is going to look like because then you have
a lot of time to prepare and then the group on the other end of that axis is entropic where they
say that it's kind of useless to start early because you don't know what you're dealing with
so you need to be as informed as possible so in that strategy you need to be just always
at the frontier and Dario has been very public about this about this strategy of course the
problem there is that like it also works as a perfect justification to raise rates so therefore
it's have like double digit uncertainty both ways about what the actual picture is and so I
do think that this point the labs indeed they are involved in death rates and they think there is
that government intervention needed to get a time out there and we definitely need time out because
we don't have enough safety results and but to yeah Romania Polsky's presentation I'm definitely
more optimistic again as on one of these slides there was the dialogue held with Eliezer and
Eliezer was confident that this can be sold and in fact like I'm super glad that earlier this year
David Tarempel his group got UK government funding and he has this approach called
open agency architecture I don't know exactly what the details there are but like my rough
understanding is that you're using you're scaling AI capabilities and access according to formal
statements that AI is produced and then you use not AI not humans but formal verifiers to verify
those those statements therefore like building up your AI capabilities one formally verified
step at the time there are many criticism of that but this is like one of those approaches that is
kind of at least and principle has like some convincing story that that why it should work in
in principle at least so there are some options that might work but we're going to need time
to develop them exactly so that's why I've been working on like I've been supporting AI safety
research for more than a decade now but unfortunately we just didn't make it we now need more by more
time so let's hear from Eva because you work more with possibilities to inspire policy you've seen
examples of policy in the past slowing down some technological races are you do you see reasons
for optimism do you see ways in which politicians can make a good difference to the landscape we're
discussing definitely definitely that very much plays into some of the problems or the issues
characteristics of the problem that both corners spoke about and that also
Jan Talion just mentioned that one of the problems so we're facing here is a human
coordination problem and one of the ways to address that will be through policy as has been
said many times this evening this is a technology that threatens to kill us to kill us all and the
heads of the government of the companies that are driving forward the technology have agreed that
and publicly stated that that might be the case and yet they seem to be locked into this dilemma
that Jan just mentioned where they are for some or other reason impossible to to stop so I think
that is a point where where government can really make a difference and step in and also
should step in and we've seen that as you hinted at we've seen that work in the past one of the
examples that I often think about is the Montreal Protocol which after the scientific consensus
arose that CFCs and other similar gases actually destroy the ozone layer the international community
did come together in 1987 and agreed through the Montreal Protocol to slowly phase out these gases
so we see here that international cooperation by the international community by governments can
succeed also in the face of the short-term economic interest of private sector companies
in the public interest of well in the end and everyone on earth so I'm not saying that it's
necessarily easy or easy but I think it's definitely possible and it is one of the strongest
levels that we have here to make a difference so I think that's definitely something that we
should lean into very strongly and do our best that that actually happens. The Montreal Protocol
is an encouraging example but we haven't made a very good job of the governments in the world of
controlling carbon emissions we've been talking about it for a long long time and maybe there's
some progress but many people feel this is an example where governments can't cooperate
so what makes you think that we can cooperate with the problems of AI more like the Montreal Protocol
rather than the Paris Agreement to say. Well part of this of course is also that I think that this
is one of the few levels that we have to make a difference at all so I also hope that we will be
able to do it and I agree that looking at past climate conference is one of the negative examples
that we see there is that with these conferences sometimes that the outcomes tend to be very
watered down just because the focus lies on building consensus among all of the different
countries that attend and then in the end you want to have a nice little consensus agreement
that everyone signs so you can demonstrate that everyone's on the same page and everyone goes
home and everyone's happy and I can just say that I think with the UK AI summit that's coming up now
first of all that is a unique opportunity to actually have international cooperation
and coordination on this issue take place you need to create the opportunities for stuff like
that I'm really happy that the UK government took the initiative and created this opportunity I am
one thing that makes me optimistic is that we all know that China is going to attend at least on one
of the days so hopefully they will be able to be brought into the fold and yeah then I just hope
that this opportunity is truly taken and that the outcome of this summit will not be just some vague
commitments to long-term plans but ideally concrete binding commitments to to concrete
next steps let's turn to Alexandra Alexandra you work a lot with businesses businesses are
unsure in many ways how to deal with today's AI do you think there is good advice that they can
be given or is there a sleepwalking process with many of our businesses definitely the latter
I would say so I'm CEO of evident we map benchmark companies on how far they are in their AI
adoption and so when I think it was Connor you mentioned the AI race that is on at the
frontline of development in AI there is also a race as we all know going on in terms of
adopting AI as as quickly as possible there's a sense of being there's a sort of geopolitical
debate on AI development between US Europe China and so on and who's leading on that
not only in AI but also in areas like quantum but in the business level which is where I deal with
spend my time mostly there's a definitely a race on in terms of not being left behind in
adoption of AI and it's an economic question it is a existential question so there's an existential
question on sort of two dimensions in this debate and and so you've got this unstoppable
race going on on the front end of AI and then you've got an unstoppable race on
actual deploying AI at a business level and it's going to be very hard for regulators to keep up
and to Eva's point I think in terms of what we hope will be the outcome often unfortunately comes
with with the catastrophic happening taking place before it really sharpens the minds and
people figure out how urgent it is I think there's a real sense of urgency in in the community around
trying to work out what what the guardrail should be whether it should be a constitution
or how we should think about implementing safety mechanisms in as as we develop further
on our chat on our large language models but I hope it doesn't need a catastrophic moment
for that to sharpen but back to the business question there is this hope that maybe businesses
will self-regulate and I think that is maybe the case in highly regulated sectors you see in the
banking sector and insurance or banking in particular that there is a guardrails put in
place there but that is that there's a lot of businesses that don't have that regulation
around them and I think there is a real risk for this completely running out of control
at a business level as well. Would you advise businesses to self-regulate ahead of standards
and regulations being agreed by governments? I think that's what they're doing or trying some
businesses are trying to do there's a big risk in in the case of winning trust with your customers
and also your your shareholders and and investors if you mishandle AI and you create issues around
not taking into account how to properly deal with biases and other issues that is a situation that
can create a real breakdown in trust with your with your organization so there is that risk
and then there are businesses that don't necessarily lean on trust for their for their for their
business and those are the ones I worry the most about. Indeed let's turn to Tom Oh as a representative
of the world of journalism do you feel journalists have helped the discussion about the existential
threat from AI or have they muddied the water leading people to panic unnecessarily or perhaps get
distracted on side issues rather than the main issue? I think it's all of the above aside from
overrugging the pudding I think most people in this room including me have had a wit scared out of
them by some of the talks just now. One has side issues in journalism coverage of AI and I think
the jobs market is one of those but I have been surprised pleasantly so by how things have progressed
since 2016 and that's the first time that I wrote about AI safety and I think at that point the
prospect of a bad scenario relating to AI was seen as about as likely by my colleagues as
Leicester City winning the Premier League. Anyway several years later I now see lots of my former
colleagues writing to my mind very informed pieces about AI safety and I think that's helped the
public change well arrive at a view and probably a lot of people in this room are aware that the
American public when polled now says that they want regulation of AI and they want a lot of it
and I think we can credit journalism with some of that. Journalism should be doing more but
it's more than I would have thought a few years ago. And if you were to go away and write up a
story about things that you might have changed your mind about tonight and that the public
should pay attention to can you give us a sneak preview what that would include?
Well I think the idea of runaway AI is not new but I think it has been difficult historically to
frame it in a way that really sticks and like really drives its way down your brainstem and we
have different ways of framing AI risk and Mustafa Suleiman's new book which some of you
might have read I think there's a pretty good job of framing it in a way in which he describes
AI being used to accelerate human ingenuity in whatever endeavours humans are up to be they
be they good or be they bad that's one way of framing it and I think we've heard some pretty
compelling ways of telling a story of runaway AI which is a different and scarier story.
Thanks and let's turn to Andrea and your role at Conjecture. What are you doing in a day-by-day
basis to address this question? Well what we're trying to do and I mean kind of current a lot of
first of all to explain the problem to people I've been heartened by the public reaction in
the last years like I also got to know about this problem quite a long time ago and I in the past I
could almost not expect the day that major governments take this problem seriously and the
public understand this problem and we all get together and take some initial promising insufficient
but promising steps to address it. Another thing is figuring out policy solutions and the reality
is that we don't obviously we don't have a playbook for what exactly they look like but what I think
was a common theme of the talks tonight is that clearly at some level of power we are not in
control anymore and everybody expects this. Those who don't expect this are misguided or
expected but don't say it and the positive thing is that there is one physical resource that drives
the majority of what makes this system powerful which is computing power and it's a physical
resource not not like you know algorithms that you could just write on a piece of paper it's
traceable it's expensive large place in data centers and while you know the scaling hypothesis
the idea that you know the more computing power you put into something the more powerful it becomes
might hit some diminution in terms of at some point we do not see any reason to expect it to stop
so we know you know from both sides companies know that more computing power leads to more power
and that's why they're doing what they're doing we know that limiting that computing power is a
very effective way to kind of stem the the bleeding and stop and or pause the situation for a while
take a time out have the time to figure out the solutions have the time to absorb this into society
but how much time will that give us because there's a risk that people will use today's models
to design much more efficient ways to build next generation models and so they could therefore
come under the radar as it were that people who were watching for large use of GPUs would miss
the clever way that somebody has built it so do we have a decade do we have three or four years
or how long yeah that's that's a great question it's
capping computing power is not a permanent solution but it's one of the best solutions we have
at the moment uh as others have said before we are in a double exponential it's not a single
exponential we have an an exponential growth of computing power hardware and exponential
improvement in software we need to start cutting down on one of the two uh cutting down a compute
depends where you put the cap probably will buy us five seven years you can make you can make
what would seem to people at the frontier extremely strong caps that would affect you know
less than 20 companies in the world that probably could buy you 10 years in that period we need to
figure out all of the rest it's going to be a hard problem but we have done it before with
nuclear weapons we've done it before with biological weapons we can do it again we're
going to go around the panelists one more time in the same order i'll give you a chance a choice
panelist you can either comment on what you've heard from somebody else or you can paint me a
picture of what would be a successful ali safety summit in bletchley park if things go well what
would be the outcome and what would also be the follow-up so jan first well i'm the one of the
authors of the post letter so it's like indefinite moratorium uh on further scaling uh would be sort
of my wet dream from outcome from from this summit or perhaps the next one if this one isn't realistic
and what's the chance do you think what what might cause the assembled world leaders to
have an intellectual breakthrough and say yes actually we do need to have this indefinite pause
so currently i'm not very optimistic on on that uh perhaps perhaps but perhaps in six months it
would be much more clearer why this is needed so and and we have more time to gonna do the
necessary loving so the discussion is prepared to ground and when something really bad happens
in six months when gpt five comes out and oh my god at least we'll know what we should be doing
yeah i mean like let's not forget that gpt chat gpt has been out less than one year so
like the world was very different one year ago same question to you either yeah thank you i think
i'm just going to build on top of jan's ideal outcome of the summit and say that i would also
find it terrific if the summit could be the first in a series of repeated um summits like this where
world leaders come together because as we i think a pretty clear picture has been painted tonight
of the fact that the field of ai evolves very quickly and is going to continue to evolve very
quickly if not ever quicker and because of that i think it would be very valuable if we would have
a regular occasion for world leaders to come together and not only make sure that the rules
that they came up with are upheld but also to reevaluate whether they still make sense and
where they need to be adapted or whether new real rules need to be introduced as for example
measures like compute control that andrea mentioned um they buy us some time but at some point they
might not be applicable anymore so not just agreement on rules but setting up some audit
process so that we can figure out whether the rules are being forward or not for example yeah same
question to you i would agree you have to build in i mean right now it's just based especially in the
us um the talks that have been held in the white house and by chuck schumer um the gatherings have
led to sort of ideas around voluntary um adherence to some principles but there is absolutely no
built-in audit or accountability um so i think that we've got to see that come out of of the
uk's ai safety summit among other things maybe there has to be something more concrete around
licensing um of the models and and the use of them and that they have to pass some kind of a
threshold i think the risk of of bad actors getting hold of them is is a is a much higher risk
i think the ia ea structure is is one that one can look at but the the nuclear a lot of the
success probably of the ia ea lies in the mutual assured destruction of humanity by using um nuclear
weapons and this might be the same situation but they're easier to monitor i think um i think this
might be slightly harder because you can land in the hands of bad actors more easily we haven't
really discussed bad actors much in this session tonight maybe that makes the things even more
horrifying we might come back to that later and tom what's your answer what would you like to see
come out of the summit or maybe you've got some comment on something else you've had from the
other speakers well i'll talk about the summits um often when CEOs of um labs developing agi
are asked about regulation um they basically say bring it on we'd love some
regulation um and i think it would be great if politicians could actually put that to the test
very good and andrea what would you like to see if you were invited to bletchley park and given the
microphone for two minutes what would you entreat the assembled world leaders to consider well i
don't want to be too hopeful as some others here have been but at the very least i would like to
see a commitment to the fact that this is extremely dangerous technology continuing to scale leads to
predictable disaster and we need to pull on the brakes right now uh we have a lot of applications
are very beneficial we can focus on those but limit this death race to the ever more powerful
ever more obscure general systems that we can control what i definitely do not want to see
is a diplomatic shake of hands where companies write their own playbook and say we're gonna keep
doing exactly what we we're doing right now but it's gonna sound responsible and governments can
wash their hands and say well we did our part let's move on that would be a very bad outcome
right i'm gonna ask for three questions from the floor i'm going to get the panel to think
which one's the answer we're trying to take people haven't asked before so on the second row here
there's a hand up here and then also the second row over there next as well let's take three
fairly short questions please hi first of all thank you very much for coming here tonight
and sharing your expertise with all of us in this whole question this whole discussion there's the
implicit assumption that agi is coming and it's coming soon and the million dollar question i guess
is when exactly is it coming but a more practical question is what are some warning signs and do
we already see some of those in the systems that we have currently deployed great let's
have a question over here as well sorry the microphones can have to run around at the end of
the second row there thank you for great panel my question is related to yon's comment at the
beginning on essentially dario amadeus philosophy which is you know and and also related to i guess
roman's talk which is how do we solve the control problem and what i've heard the large a our labs
repeat is oh you know we need to increase capability it's only better ai that is going to be able to
help us figure out how to solve a and there's sort of this race to increase capability up to the
point that can help us solve it but no further and and and just sort of thoughts on that philosophy
and and and and you know whether there might be something to it or is it just a completely risky
game you know thanks and there was one in the middle of the third row there pass the microphone
along please to the middle how long of a time frame do you think we have between the arrival of agi
and the arrival of superintelligence and within that time frame could there be tractable solutions
for alignment or the control problem and if so would those solutions be able to be implemented
before hurry up and develop better ai third question was how long might it take between
the arrival of agi and superintelligence and whether there would be time for us to work
out solutions then and my question i guess is well what's all this about agi isn't the bletchley park
summit set up to discuss something else which is frontier models which says that there are
catastrophic risks even before we get to agi so i'm going to go around the same order again
i'll be a bit predictable jan you want to pick one of these questions maybe i mean answer to all the
all three questions is uncertain that's that's why we need to pause and kind of take a time out
and see like how can we kind of create more set more certainty about these things i think i would
answer the anthropic question specifically that definitely is a lot of truth to the to the point
that like the more capable model you have to work with the more kind of better position you are in
particularly you can you can kind of be do like do science in a way that you just can't do with
models from from 10 years ago and also like one claim that people and tropic to make is that like
in some ways it becomes easier as the model kind of has better understanding of what you're trying to
do with it or to do to it but that said again it's a to put it lightly it's playing with fire so
so it's i'm not sure if anyone should be doing it either and jan said it's all uncertain but
can't we at least agree in advance some canary signs that will make us say things are happening
faster than we expected well i mean if we look at the past there were several signs that people
agreed on that they might point out that we're getting into a zone where ai is maybe more capable
than we think it is and i mean we certainly have seen signs connor mentioned or was it roman
mentioned that the current models um i'll perform most humans on things like the bar exam
um these are clearly advances in capabilities that um i almost wonder sometimes if we just
become desensitized to them because we move so fast i mean again charge epd came out a couple
months ago and it's already just normal and people are waiting okay what's the next big thing so um
it doesn't really help um to think retroactively have them in any signs um if you didn't take them
to actually stop and reconsider what you're doing so i think one of the big problems here
is not have there been signs a big problem is can we pre-commit to stopping when we see certain
signs and then actually stop or actually take certain actions and we just haven't seen that before
so this is developing contingency solutions like we're meant to have had contingency solutions
for pandemics yeah yeah any comments alizandra i i will leave the um well how long it's going to
take to reach agi to to the experts on the panel but on the outcome of the summit and i think there
is a bit of a confusion sometimes in in in what we are expecting to be achieved from the discussions
on regulation because there's an obvious very important urgent and existential question around
regulation regulating for the long term but then we also have businesses that are sitting and waiting
for regulation that is here now how is it going to impact my particular sector how is it going to
impact what i'm doing today and what are the immediate and very very real risks right now
here today that we are seeing um with ai having impact on you know media with disinformation
and so on but then there's also the specific um aspects to how that is implemented in particular
sectors so i um hope that we would see addressing both of those short term and long term questions
thanks tom any thoughts yeah on yardsticks i think it's worth remembering that the canonical
yardstick was the turing test um and that's long gone um ai's can now beat humans at um
diplomatic based games for instance um and much more um the modern turing test is i think quite
an interesting proposition um and that's the test of whether the ai can i think make a million dollars
very quickly um but as as eva says we must stop shifting the goalposts um we need to agree that
the one is we should pick one agree that that's the one where we start taking it seriously and
then take it seriously when it is passed which it will be quite soon but in the past people said
you won't manage to solve chess unless you have got a full grasp of all aspects of creativity and
so on and then when deep blue did win at chess people said oh well it's not actually doing it in
the way that we thought would be so terrible it's just grunting out incredibly so i feel
there will always be people who don't move the goalposts but they'll say well how it was implemented
it doesn't demonstrate too intelligent yeah i think that's a good point um and it reminds us
something um conna said um which is that there won't be consensus at the time to act so we need
to be able to build a coalition of the willing uh andrea what's your views on these questions
yeah maybe answering the last one first on isn't a summit about frontier ai well much like with
goalposts it feels a bit like terminology is being shifted all the time sometimes quite willingly
by the companies building this uh you know in in the in the old days people used to talk about
superintelligence or friendly ai or strong ai then became agi then recently the frontier term was a
kind of open ai entropic rebrand of oh no like we're not well we're gonna get to very powerful
ai system soon but it's frontier which sounds better than agi because people are getting
concerned about agi you know in practice do these terms matter not too much what matters is how
competent systems are basically all of these companies expect to build systems that outperform
humans at most tasks definitely most tasks you can do behind a computer in the next
two to five years um these matches the trends that we see in performance and compute growth this
is very worrying uh these these are these are levels of competence uh at which we expect the
systems to be out of our control unless we have various solutions so we just need to deal with
that we can call it frontier ai agi proto agi proto superintelligence it's just terminology
what matters is how powerful they are and how ready we are to deal with them we must avoid the
serious discussions getting sidelined into semantics which is often very frustrating we're
going to take three more questions we're going to go around the panel in the reverse order next
time i'm going to take questions for people who haven't answered asked before so somebody in white
about halfway down if you have asked a question before please don't put your hand up just now so
we have more chance just there yes thank you three questions one from each thanks um let's say that
in 20 years we somehow managed to get it right and humanity still exists um despite the development
of these a di what do you think is one essential piece of regulation or development that has to
have happened together it's a great question uh where else with the hands uh where are the
microphones there's one about just on the other side thank you so you've spoken quite a lot about
like what government should do what companies should do uh i'm interested in like what should
ordinary people do like what can we be doing to get our voices heard in this you know should we
be protesting i've edited international protest on the 21st is this thing we should be doing or is
this a terrible idea it's another fine question is there a question from a woman it's about time we
heard from the other gender another gender hands up yes somebody put your hand up to whoever it was
yes there's one okay there's one there and we'll take you as well we'll take four right um as
agi has been developed to be more human do you think it's possible to have forms of agi that
don't have the inherent geopolitical biases that come with the data sets that we currently have
and how do you think we go about developing regulations that aren't formed by human conscious
bias okay and so if we can get the mic over here as well two one two three four five rows back
just at the edge yes over there yeah i miscounted perhaps yeah four sorry quick question um so i
actually work in the automotive industry and we have to certify vehicles and engines and it is
an uphill battle um you can spend years just trying to get a windshield wiper right um or a
temperature sensor right and i'm just curious um if you think that there would be an ability
to take people who have regulated and certified products around global markets and how difficult
that is and create a summit where that expertise could come together from different industries
and we could roll up our sleeves and say okay this is how the structures go and we know what
works we know what goes slow and try to accelerate that learning because i think that voice we have
so much experience in the world right now um with that sleeves rolled up we know what it's
like to sit in those test labs or send 30 000 pages of documents in with verification and
validation data we know how to do requirements engineering requirements design requirements
and i'm just wondering if um there's been any discussion of that to pull you know pull a summit
together from people from heavily regulated industries four great questions first of all
20 years later it succeeded how did we get it right what were the regulations that made the
difference what should ordinary people be doing can we design AI that is free from some of the
human biases the geopolitical biases that cause strife among humans and can we learn from the
people who are professionally involved in doing regulations and certification in multiple industry
rather than just to being naive in our own applications so Andrea first yeah maybe i will
answer the question about can we learn about highly regulated industries definitely i think there is a
big kind of problem of uh arrogance in AI or like willful arrogance of just thinking that
this sector should be special and people should be absolutely free to do any experiments they want
all the time use you know as much computing power as they want try the worst possible applications
all the time fully open source on the internet and nobody can complain like very often people in
the eye sector get very very angry when somebody tells them look well maybe what you just did
should be regulated and industries we don't do it like that like with drugs we don't just let
pharmaceutical companies just release and test the drugs on billions of people
and have their CEOs say oh there's a 20 percent chance you will die if you take this drug but you
know don't it's okay like if it happens you can let us know and then we'll we'll stop maybe right
so we can totally learn from that it would be great to learn from that there is one challenge
which is that we don't understand current systems that well so it makes things like
auditing them and evaluating them quite tricky because we simply don't know how they work internally
as well but we can do many other things and we can definitely learn from highly regulated
industries and definitely given the risks admitted by the companies themselves at the frontier the
approach should be highly regulated industry not so it is different but not completely different
and we can indeed learn tom closing words from you well i'll take the the question about what
ordinary people should do and i have two immediate thoughts one is that it's very important to keep
this issue apolitical the other is that lawmakers need a sense of legitimacy i think in order to
come up with regulation and to bring it in through acts and bills and so on a good example of when
this happened a bit too slowly was it the outset of covid and when it was a fringe issue there were
no enough rules then the public got involved and suddenly the rules arrived a little too late but
they did arrive and how can ordinary people achieve this i think ordinary people i i don't have a
theory of protest so i won't comment on that but i think it's important that we all keep this in
the public conversation i suppose what my answer is really tending towards is you should all read
lots of journalism about ai click on my articles thanks alessandra any of these questions catch
your attention i think um your question in the orange sweater there is is um is is definitely
where we're headed i mean there's got to be some kind of um system that resembles either the car
industry or fda and um the way that we certify um our you know products generally speaking
and i i just don't know how we get from from that to something that is very difficult to trace
and to to monitor as as ai but i would say to the gentleman's question in the white shirt there if
we're looking 20 years down the road and we say that's really great in the uk november 2023 we
we were able to put in place regulation that somehow created traceability um so we could we
could work out sort of where the where they were where systems were running out of control or
landing in the hands of bad actors that would be a huge success i think that the reality is a bit
different and that is that it probably is going to resemble a bit more the world in which cyber
security um flourishes and that means you're constantly trying to create um a dam system or a
deflection of all sort of incoming um activities that are not great so i know that none of these
are perfect analogies but i think it is in in that universe we're probably going to be operating in
for a while thanks final words either sure so i would love to touch on two questions very briefly
one of them um being i think your question in the white shirt what uh policies will bring us to the
safe world in 20 years and i think um a policy that was mentioned today as well already but that
i want to touch on again is um strict liability regimes just simply to kind of shift the incentive
systems um incentive structures that drive private companies to take certain actions that are not in
the interest of um the wider general public so i think there we can um really um shift shift the
incentive structure to move companies to take um maybe different paths forward and then what can
the the average person the general public do i would completely agree with tom i think um one
thing that that really would help is to for lack of a better expression to just make noise just make
sure that this topic is um talked about publicly you can do this in different ways you can write
to your local newspaper you can make a protest if that's up your alley you can write to your
mp or your congressman or wherever you live um and again create that legitimacy for people
to actually act on the problem because to many people it does sound very much like sci-fi and
policy makers are not going to take action and newspapers are not going to continue to report
about an issue they feel like it doesn't have traction and isn't taken seriously by the general
public the other thing the general public can do is we can educate ourselves and then we can share
information we have found to be most persuasive ourselves because there's a wide variety of
books a wide variety of youtube channels a wide variety of blogs and some of them are
better than others so let's share what we have found to be the really best ones
auto before i pass to jan maybe i'll ask you to get ready to come up on the stage because
you're going to give some closing remarks but jan what's your answers to what you've heard uh so
yeah just to just kind of underline the what ordinary people could do uh is just kind of
keep this topic alive like one of the things that i'm very proud of uh that came out of the
future five six months post letter uh was uh kind of framed by uh european commissioner
margaret bestiger when she said that like one thing that this letter has done is you're gonna
like communicate to the regulators that these concerns are much more widespread among people
than among regulators so i think this potential difference should be continually kind of maintained
so and when it comes to kind of bringing in kind of expertise from people from like regulated
industries i think it's super valuable i was on the on the board or like on the european high
level expert group at the european commission and there was like every once in a while there was
like why are we inventing the wheel like that we already have like lots of regulations should
we just apply this and i was like yes however there's like one big problem uh the problem is p
in chat gpt gpt stands for generative pre-trained transformer the pre-training is something that
you do before you actually train so the current the the out of nasty secret of ai uh field is
the ai's are not built they are grown the way you you you build the frontier model build the
frontier model is you take like two pages of code you put them in tens of thousands of
graphics cards and let them hum for months and then you're gonna open up the hood and see like
what creature brings out and what you can you can do with this creature so it's i think the
regulate the industry the capacity to regulate things uh and kind of deal with various liability
constraints etc they apply to what happens after what's once this creature has been kind of tamed
and that's what what uh fine tuning and reinforcement learning from human feedback etc is doing
and then productized then how do you deal with with these issues but uh is this where we need
the competence of of like other other industries but like how can avoid the system not escaping
during training run this is this is like a complete novel issue for this species and we need to need
some other approaches like just banning those training runs that's great we'll thank the panel
in a minute i asked the panel to stay here because who's going to wind up the evening is Otto Barton
Otto is the executive director of the ERO the existential risks observatory
which along with conjecture has designed and organized and sponsored this whole evening
Otto's got a few closing remarks before those of us who are still here can have a quick drink
and continue the discussion informally up to 10 o'clock by which time we must be out of the building
Otto
all right uh thanks david um a few closing remarks before we go to the drinks which is
five minutes so you should be able to uh keep with me um so we're talking tonight about human
extinction because of AI and what to do about this um and i think what to do about this there
was also a great question from the audience what can we do about this this is exactly the question
that i asked myself a few years ago um but it's not trivial and it's it's pretty difficult actually
what is not positive what could you do develop AI yourself try to do it safely such as uh open
AI deep mind and anthropic are doing will this increase safety some say so uh work on AI alignment
for example interpretability where we've seen great breakthroughs actually last week uh it could
be a good option but increasing knowledge of how AI works could also speed up its development so this
brings risks as well uh one could campaign for regulations such as an AI pause we support this
but this also has its downsides so i think it's pretty difficult to tell what one should do to
reduce human extinction risk by AI but when i started reading into this i was only really
convinced about one thing and that is that you cannot put humanity at risk without telling us
so you cannot have a dozen tech executives embarking on the singularity without informing anyone
else and you cannot have a hundred people at a summit which is what's happening now decide what
should be built and what should not be built and i think you cannot let a tiny amount of people
also decide how high extinction risk should be for the rest of us so the only thing that i'm
really convinced of is that we should be informed about this topic and that's also why i'm so happy
that events such as this one are taking place um we're happy i'm happy that we're together not just
with in crowd people some of you are and it's great but also with some people who may who may have
never heard of existential risk before and also a journalist who can inform a much wider audience
about existential risk also with a member of parliament someone with a job to openly discuss
difficult problems so i think this is all very encouraging and it's helping to normalize an open
debate about the topic of human extinction by artificial intelligence the 31st of october at
two o'clock we'll have our next event with professor steward russell it's just outside the ai safety
summit in blashley park in the old assembly hall where the code breakers used to have their
festivities after their important work so our event at blashley park the day before the summit
may not resemble a festivity but in a sense i think it is because we're celebrating that we're
all being hurt there we're celebrating that we can all be part of a democratic conversation about
what the most important technology of the century should and should not be able to do we can talk
about risks to humanity we find acceptable and what we intend to do about risks that are too high
and as the existential risk observatory together with conjecture we invite everyone to be part
of this conversation so there's much to be unsure of in this field but if there's one thing that i
am sure of it's that the most important conversation in this century which i think this is has to be
a democratic one so with that i would like to invite you to scan the qr code on the left
if this is working right yes to join us in blashley this is containing the url where you
can enroll to the blashley park event if you're interested then definitely pass by
there's same qr code is also on the flyer on your chair and beyond blashley i think this
conversation will not stop so there will be more summits according to my timeline about maybe 18
roughly so we will organize more events probably publish more about ai do more research and inform
governments as well as we can if you want to follow us or support us the existential risk
observatory in that work then scanning your r-codes on the right there's much that you can do to
help us um and with that i would like to close this evening and once again thanks to all our great
speakers so that's uh romeo polsky cornelly sir robert buckland yantalin andrea milte alexandra
mosefisa day eva birans and some are give them a round of applause
and i would also very much like to thank david wood uh should see them conor xio dis
ribbon dealer man and everyone at conway hall will also make this evening possible thank you very
much and then i would like to hopefully see you in blashley and in any case you are the drink
right now thank you thanks everybody
you
you
