1
00:00:00,000 --> 00:00:12,960
Our computer scientists at the University of Louisville and specialized in behavioral biometrics,

2
00:00:12,960 --> 00:00:18,320
security of cyber worlds and AI safety. And indeed, one of my personal first steps in the

3
00:00:18,320 --> 00:00:22,880
world of existential risk was to read the number of your publications. And I think you may have

4
00:00:22,880 --> 00:00:28,160
inspired many like me to start working on either AI safety or existential risk or related fields.

5
00:00:29,120 --> 00:00:34,160
And I think you also have always interesting and thought-provoking comments available on each

6
00:00:34,160 --> 00:00:38,480
topic to social accounts, for example. So I really enjoyed reading those and I can recommend them

7
00:00:38,480 --> 00:00:43,680
to everyone. So we are now honored and it is a great pleasure, I can say, to be able to learn

8
00:00:43,680 --> 00:00:47,360
from you today. And we'll give the stage to you now, Dr. Romer Jampolsky.

9
00:00:48,080 --> 00:00:54,400
Thank you so much. Wonderful introduction. I really appreciate that. I think the plan is that

10
00:00:54,400 --> 00:01:02,720
I'll give about a 10-minute intro to my latest work and then we'll let me set up the slides.

11
00:01:06,000 --> 00:01:12,080
Where is it? Where did it go? Yeah, that's right. We can take a little bit longer as well if you

12
00:01:12,080 --> 00:01:16,720
want to. And then we can move to the fireside chat and then the Q&A with audience. Can you see my

13
00:01:16,720 --> 00:01:25,680
slides? Yes. Excellent. So my name is Dr. Jampolsky. I'm a faculty member at University of Louisville.

14
00:01:25,680 --> 00:01:34,000
I've been doing work on AI safety for about 10 years now. Give or take. And my latest work covers

15
00:01:34,560 --> 00:01:41,040
what I will call impossibility results. Problems we encounter with actually

16
00:01:41,040 --> 00:01:49,840
accomplishing what we think is necessary for us to do not just development work for AI,

17
00:01:49,840 --> 00:01:56,480
but also work in terms of control and safety. If you would like to learn more about my work

18
00:01:57,120 --> 00:02:02,320
after this talk, you are definitely welcome to follow me. You can follow me on Twitter,

19
00:02:02,320 --> 00:02:06,880
follow me on Facebook. I always encourage you not to follow me home. It's very important.

20
00:02:07,760 --> 00:02:17,760
So let's start with the big problem right away. If you heard previous talks at this conference,

21
00:02:17,760 --> 00:02:24,960
I had a chance to hear a little bit. You know about concerns a lot of people have with advanced

22
00:02:24,960 --> 00:02:33,920
artificial intelligence, usually called AI safety, AI alignment problem, sometimes AI control problem.

23
00:02:34,160 --> 00:02:40,560
The question is the problem we're trying to solve is how can humanity remain safely in control while

24
00:02:40,560 --> 00:02:48,240
benefiting from superior form of intelligence? So we want this very capable agent to do work for us,

25
00:02:48,240 --> 00:02:54,640
to be helpful, but we want to be in charge. We want to be able to undo any changes if we don't

26
00:02:54,640 --> 00:03:03,040
like them. We want to be final decision makers as to what's going to happen. And the good news

27
00:03:04,000 --> 00:03:09,440
is after 10 years of building up the movement, there is a lot of people working on this now.

28
00:03:09,440 --> 00:03:17,200
There is a lot of research labs, a lot of PhD programs, but I think the main question of

29
00:03:17,760 --> 00:03:23,360
control problem has not been addressed sufficiently. And that is, is the problem actually solvable?

30
00:03:23,360 --> 00:03:29,520
So everyone kind of works on it, but I haven't seen much in terms of proofs or even rigorous

31
00:03:29,520 --> 00:03:35,200
argumentation for why we think we can do this. Why is this problem solvable? Is it solvable?

32
00:03:35,200 --> 00:03:40,960
Is it partially solvable? Maybe it's not solvable. Maybe it's a silly question. It's not even decidable.

33
00:03:41,600 --> 00:03:48,800
So that's essentially what I've been looking at for the last couple years. And I started by

34
00:03:48,800 --> 00:03:54,560
formalizing a little bit what I mean by control problem. So we can talk about different types of

35
00:03:54,640 --> 00:04:02,240
control. We have explicit control where you give orders and the system follows. And this is the

36
00:04:02,240 --> 00:04:07,680
kind of standard Gini problem, right? You wish for things, then you realize that's not what I meant

37
00:04:07,680 --> 00:04:14,320
and you hope you get to undo the damage. Then there is implicit control. So you have a system with

38
00:04:14,320 --> 00:04:19,760
a little more common sense. It doesn't take your words literally, it tries to kind of parse things

39
00:04:19,760 --> 00:04:26,240
in the right way. And there are intermediate steps between explicit and delegated control.

40
00:04:26,240 --> 00:04:32,400
Delegated control is the other extreme where you have this superintelligent system very friendly.

41
00:04:32,400 --> 00:04:38,000
It knows what needs to happen better than you do. And you might be even very happy with the

42
00:04:38,000 --> 00:04:44,320
decisions it makes, but you're not in charge. The system is completely in control. Aligned control

43
00:04:44,320 --> 00:04:51,120
is another intermediate option where the system kind of understands your values, human values,

44
00:04:51,120 --> 00:05:00,400
and tries to do the best it can to fulfill those values, even if your spoken directions are not

45
00:05:00,400 --> 00:05:13,440
exactly what maps onto those ideal values. So it seems at least from now that some sort of intermediate

46
00:05:14,720 --> 00:05:23,280
value between total control and total autonomy for the system is necessary for us to be happy with

47
00:05:23,280 --> 00:05:30,480
the results. If a system is completely autonomous, we have no control over it. By definition, we

48
00:05:30,480 --> 00:05:36,400
will lose control. If a system has no autonomy, it's completely deterministic. We decide ahead

49
00:05:36,400 --> 00:05:41,040
of time what's going to happen. It's not very useful. It cannot be generally intelligent. It's a

50
00:05:41,040 --> 00:05:49,200
great way to do simple tasks for a narrow AI, but it's not something we can utilize to solve

51
00:05:50,160 --> 00:05:56,080
completely new problems and new domains, help us with science, help us cure diseases and things

52
00:05:56,080 --> 00:06:02,240
like that. So what I try to do is kind of break down the bigger problem of control

53
00:06:02,240 --> 00:06:07,600
into all the ingredients we need, all the tools we would need to make controllability possible.

54
00:06:08,560 --> 00:06:13,840
So what do you need? You can start thinking for yourself. If you want to be in control,

55
00:06:13,840 --> 00:06:19,200
well, you kind of have to understand the situation. What is the system doing? Can the system explain

56
00:06:19,200 --> 00:06:24,720
what it's doing? Can you comprehend the explanation? That's another very important one. Can you predict

57
00:06:24,720 --> 00:06:30,000
what the system is likely to do? Maybe not just direction in which it is going, but specifics.

58
00:06:30,000 --> 00:06:36,960
What steps will it take? Can you verify that whatever it is you want the system to do and

59
00:06:37,040 --> 00:06:44,160
program it to do is actually going to happen? So can you verify the implementation versus the model?

60
00:06:44,160 --> 00:06:50,320
Can you verify the model against your goals and data and so on? That is also a need for

61
00:06:50,880 --> 00:06:57,120
general ability to govern AI research, AI systems, so governability of that. And of course,

62
00:06:57,120 --> 00:07:04,000
we communicate with systems in human language and we need to make sure that communication we use

63
00:07:04,000 --> 00:07:11,360
is unambiguous. There is no way to misinterpret commands in a potentially dangerous way.

64
00:07:11,360 --> 00:07:16,880
And there is many, many more such limitations. And what I've been doing, kind of looking at each

65
00:07:16,880 --> 00:07:22,960
one and trying to publish those results. So I'll go over some of the publications I have on that

66
00:07:23,600 --> 00:07:30,000
so far. One paper talks about limits to explainability and incomprehensibility.

67
00:07:30,000 --> 00:07:38,720
So essentially, for very complex systems, large neural networks, it is impossible for the system

68
00:07:38,720 --> 00:07:45,600
to provide an exact explanation of what it is doing or why without simplifying it to the point of

69
00:07:45,600 --> 00:07:51,920
where it is like you explaining something to a child. So a lot of important details are removed

70
00:07:51,920 --> 00:07:56,480
and then a very simplified version is given to you because if a full version is given to you,

71
00:07:56,480 --> 00:08:02,880
you'll simply not be able to comprehend it. And if you want to learn more and see the kind of

72
00:08:02,880 --> 00:08:08,160
argumentation, in some cases, proofs, just go to the paper. I provide all the information you need

73
00:08:08,160 --> 00:08:13,520
to get access to those papers. There are also available as preprints on my Google scholar account.

74
00:08:15,120 --> 00:08:20,960
Another impossibility result is unpredictability and that's our inability to

75
00:08:21,360 --> 00:08:28,400
precisely predict all decisions, all intermediate steps, a much smarter agent will take.

76
00:08:28,400 --> 00:08:33,680
Of course, if we could predict all the decisions of a smarter agent, we ourselves would be that

77
00:08:33,680 --> 00:08:40,640
smart and by definition, there wouldn't be much of an intelligence gap, cognitive gap between us.

78
00:08:41,840 --> 00:08:48,560
That is also problems with verifiability. We know for a fact that with software,

79
00:08:49,120 --> 00:08:56,880
with mathematical proofs, we can only get to a certain degree of confidence, but never 100%

80
00:08:56,880 --> 00:09:04,160
confidence in the fact that we have no errors in the proof. So with more resources, we can

81
00:09:04,160 --> 00:09:11,200
increase safety and security, but we're never able to guarantee something with 100% accuracy,

82
00:09:11,200 --> 00:09:16,880
which is a problem for a superintelligence system, which makes potentially billions of

83
00:09:16,880 --> 00:09:23,120
decisions every minute. Even if one in 100 million creates a problem, you're guaranteed to have a

84
00:09:23,120 --> 00:09:30,880
huge problem within a minute or so. There are also problems with governance. We have history of

85
00:09:30,880 --> 00:09:38,880
trying to govern technology, things like spam and computer viruses. We have laws against those

86
00:09:39,600 --> 00:09:45,360
malevolent software products, but they don't seem to be doing much. So it's not obvious how much

87
00:09:45,360 --> 00:09:51,520
benefit is actually added and other negative consequences from trying to control research,

88
00:09:51,520 --> 00:10:00,320
control what is allowed and not allowed to be experimented with. Even the orders we give to

89
00:10:00,320 --> 00:10:06,720
the system, the communication channel through English is very ambiguous and you're almost

90
00:10:06,720 --> 00:10:13,840
guaranteed to run into situations where your orders will be misinterpreted at multiple levels

91
00:10:13,840 --> 00:10:21,840
due to how imprecise human languages are. So what we did with our colleague, we surveyed a lot of

92
00:10:21,840 --> 00:10:27,920
those impossibility results. Those I looked at and those other people have looked at, I'm not going

93
00:10:27,920 --> 00:10:35,040
to go into details of all of them. I can just tell you there is a lot of them and some purely

94
00:10:35,040 --> 00:10:40,720
software problems, mathematical problems, many problems with physics of the universe,

95
00:10:41,360 --> 00:10:48,160
impossibility results from physics. But if you think even a small subset of all those tools

96
00:10:48,160 --> 00:10:54,000
is necessary to solve the control problem, you have to come to the conclusion that control

97
00:10:54,000 --> 00:11:01,280
is not possible. At least not 100% guaranteed safe, secure control we all dream about. And I

98
00:11:01,280 --> 00:11:08,880
have a very lengthy paper about that, about 70 pages. I now have a few subsections of it

99
00:11:08,880 --> 00:11:15,360
published coming out in conferences, those should be a lot more readable. But I think I

100
00:11:16,160 --> 00:11:21,360
bring up a lot of interesting questions and additional directions for research and hopefully

101
00:11:21,360 --> 00:11:30,640
in the next half hour or 40 minutes we can talk about what all this means and how we can move

102
00:11:30,640 --> 00:11:42,640
forward from where we are right now. Thank you very much, Roman, for this introduction already.

103
00:11:43,760 --> 00:11:48,800
Yes, we can now have a chat about indeed where does it leave us and where should we go from now

104
00:11:48,800 --> 00:11:54,560
and also a couple of related questions. And towards the end, after about 15 to 20 minutes,

105
00:11:54,560 --> 00:11:59,440
we will also take questions from the audience. So please, if you have any questions then please

106
00:11:59,440 --> 00:12:05,920
type them into the chat. We're in the questions section rather. So first, you're spending quite a

107
00:12:05,920 --> 00:12:10,800
lot of time researching AI existential risk, but I don't think it's already obvious for everyone in

108
00:12:10,800 --> 00:12:17,600
the call why AI would be a danger at all. And I don't think everyone is perhaps 100% convinced

109
00:12:17,600 --> 00:12:23,120
that this is actually an issue or an existential danger, at least that is. Could you please recap

110
00:12:23,120 --> 00:12:29,040
how exactly AI could become an existential risk according to you? Right, so there is a lot of

111
00:12:29,040 --> 00:12:36,080
ways to get to that conclusion. I have a few papers where I simply collect examples of accidents,

112
00:12:36,080 --> 00:12:41,840
AI failures throughout history. And if you look at that progression, it's kind of same exponential

113
00:12:41,840 --> 00:12:46,800
chart you see with development. We get more problems, the problems become more severe,

114
00:12:47,520 --> 00:12:54,960
and our ability to anticipate and predict them seems to be very limited. So basically the conclusion

115
00:12:54,960 --> 00:13:00,000
is something like, if you have a system of service to do X, eventually it fails to X.

116
00:13:00,000 --> 00:13:05,120
Frequently it does so very quickly and you go, hmm, okay, my self-driving car just killed a bunch

117
00:13:05,120 --> 00:13:11,200
of pedestrians, that's a problem. And then it's a narrow system, the damage is limited, right? So

118
00:13:11,200 --> 00:13:16,960
self-driving car, okay, the worst it can do is run through some pedestrians. But if a system

119
00:13:16,960 --> 00:13:22,240
becomes general, and it's now controlling not just a single car, but networks of cars,

120
00:13:22,320 --> 00:13:27,200
nuclear response, airline industry, stock market, the damage is proportioned.

121
00:13:29,040 --> 00:13:35,200
I think it's also not the best way to assume that I have to prove that this service or product is

122
00:13:35,200 --> 00:13:40,240
dangerous. Whoever is developing and releasing it has to prove that it is safe, that it's standard

123
00:13:40,240 --> 00:13:47,280
liability law for any product. Show me that this system, which is smarter than me, smarter than you,

124
00:13:47,280 --> 00:13:52,480
smarter than all of us, will never do something within anticipate, something dangerous, something

125
00:13:52,480 --> 00:13:58,160
we don't want it to do. Is this proof could at any point be possible or is it within the

126
00:13:58,160 --> 00:14:05,280
impossibility realm of your theory? Well, I think I'm arguing that it's impossible to do so,

127
00:14:05,280 --> 00:14:10,080
and not just because it's impossible to prove that, but it's impossible to get to that level

128
00:14:10,080 --> 00:14:15,200
of performance. You can get progressively safer and more secure because you can look at specific

129
00:14:15,200 --> 00:14:21,440
accept domains. You can limit what the system can do in certain situations, but you have an

130
00:14:21,440 --> 00:14:27,760
infinite space of possible problems. So it's very hard to prove deterministically that you

131
00:14:27,760 --> 00:14:34,320
can sit at all of them. So if AI safety would indeed be impossible, what does that imply for

132
00:14:34,320 --> 00:14:38,880
AI safety research? Does it imply anything for AI safety research? Is that would that be a waste

133
00:14:38,880 --> 00:14:44,560
of time or is it still something that we should pursue? Not at all. I'm doing it more than ever.

134
00:14:44,560 --> 00:14:50,000
So think about mathematics. We know in mathematics there are many impossibility results. You cannot

135
00:14:50,000 --> 00:14:56,480
prove certain things in general. You cannot have proofs with 100 percent confidence. It doesn't

136
00:14:56,480 --> 00:15:02,000
stop mathematicians from discovering new beautiful mathematics. We know in physics there are limits

137
00:15:02,000 --> 00:15:07,840
to, for example, speed, fundamental limit, you know, speed of light. That doesn't limit us from

138
00:15:07,840 --> 00:15:15,760
doing great work on faster cars and faster rockets. It just tells you that there are limits to what

139
00:15:15,760 --> 00:15:20,160
can be done. And so you should a, not waste your resources trying to accomplish that. Like

140
00:15:20,160 --> 00:15:24,400
knowing that perpetual motion machines are not possible is helpful result in physics.

141
00:15:25,200 --> 00:15:29,920
Same here. We need to understand what we can achieve and then concentrate on what is actually

142
00:15:29,920 --> 00:15:34,160
solvable instead of trying to create magical devices which cannot work.

143
00:15:34,160 --> 00:15:41,280
Next one. For AI to become an existential risk, it's commonly thought that it should

144
00:15:41,280 --> 00:15:46,320
first outsmart humans. How big do you personally think the chances that this will happen at all

145
00:15:46,960 --> 00:15:50,640
and which probabilities do your fellow AI scientists assign to this?

146
00:15:51,520 --> 00:15:57,040
That AI will ever become as smart as humans? Exactly. Well, it's a guarantee. I mean,

147
00:15:58,160 --> 00:16:03,760
we have proof by existence, right? If you just copy human system, you got same level.

148
00:16:04,240 --> 00:16:12,400
We also kind of give a lot of credit to humans because we tend to think about Einstein and

149
00:16:12,400 --> 00:16:18,960
similar type humans as typical examples. Every human is quite dumb. So it's not that hard to get

150
00:16:18,960 --> 00:16:31,040
to that level. And how are you on timelines? Of course, you hear quite values that are quite

151
00:16:31,040 --> 00:16:37,760
far apart. I think Elon Musk said that there could be a five-year timeline up until AGI that's

152
00:16:37,760 --> 00:16:42,240
on the progressive side. On the other side, there are people that claim it would take hundreds of

153
00:16:42,240 --> 00:16:49,440
years. Where are you on this line and how certain are you? That's a very hard question. No one knows

154
00:16:49,440 --> 00:16:57,360
for sure and no one can accurately predict something like that. But if our current theory is about

155
00:16:58,000 --> 00:17:04,000
how systems scale, all right, meaning if you just add more compute, add more data,

156
00:17:04,000 --> 00:17:08,880
you keep making progress, then it becomes a question of cost. How much compute are you

157
00:17:08,880 --> 00:17:17,600
willing to purchase to get to that level? Do you have finite resources or what is here? $200

158
00:17:17,600 --> 00:17:23,120
billion now. So maybe at that level, seven years is a reasonable estimate. With my budget, it might

159
00:17:23,120 --> 00:17:32,800
be 2040. It depends on what type of resources you have. If it's also as difficult as, let's say,

160
00:17:32,800 --> 00:17:38,560
Manhattan Project was, right? You need resources of a whole country to get there. It's one question.

161
00:17:38,560 --> 00:17:43,200
If we discover, okay, there is a simplifying assumption, so we need a lot less resources to

162
00:17:43,200 --> 00:17:50,160
drain this type of engine, a lot less compute. Maybe you can do it with a laptop in a garage and then

163
00:17:50,160 --> 00:17:56,320
it becomes a lot more affordable and takes less time. So I don't have specific dates.

164
00:17:56,320 --> 00:18:03,200
I would be surprised in maybe 2045 if I don't see something at human level. But

165
00:18:04,160 --> 00:18:09,440
that's not important to the argument at all. Whatever it's 2045, 2070, it's still

166
00:18:10,320 --> 00:18:15,280
something we need to worry about today, control and work on safety aspects of it.

167
00:18:15,920 --> 00:18:22,240
I've read somewhere that there are about 70 research projects explicitly aiming for AGI at this

168
00:18:22,240 --> 00:18:28,800
point. I guess the most famous two ones are DeepMind and OpenAI, at least the ones I know best.

169
00:18:28,800 --> 00:18:33,920
Do you know a project that we've never heard of, but actually has a fair chance of beating those two?

170
00:18:35,280 --> 00:18:42,640
Well, there could be many secret projects by secret agencies. I'm sure NSA is very interested

171
00:18:42,640 --> 00:18:47,680
in processing your data more efficiently. So I'd be surprised if they don't have something good

172
00:18:47,680 --> 00:18:55,760
happening. Usually, if you look at the history of what they publicly released and what we later

173
00:18:55,760 --> 00:19:00,800
learned they had, I think they had public ecryptography like 30 years ahead of everyone. So

174
00:19:01,840 --> 00:19:12,320
maybe already. Interesting thought. A week ago, you posted on the Facebook timeline that

175
00:19:12,400 --> 00:19:17,360
I referred to already, which is quite interesting. A quote from that helped me to understand

176
00:19:17,360 --> 00:19:21,920
where is the number of highly respected people who, one, argued that advanced AI is dangerous to

177
00:19:21,920 --> 00:19:26,800
humanity, and two, work as fast as they can on developing advanced AI. And there were, I believe,

178
00:19:26,800 --> 00:19:31,760
116 comments under your post. Have you come any closer to understanding this personally?

179
00:19:33,440 --> 00:19:39,360
No, I had some good explanations and the best one, and I think that's the one Elon actually

180
00:19:39,360 --> 00:19:44,720
gave himself was saying, okay, if we can't control it, I might as well be the one to get there and

181
00:19:44,720 --> 00:19:50,800
I have the best chance of controlling it. People who don't care about safety have less of a chance.

182
00:19:53,360 --> 00:20:00,640
But it is interesting. So a lot of very big names in arguing that AI is extremely dangerous are also

183
00:20:00,640 --> 00:20:04,960
people who invested the most time and money in making it as fast as they can.

184
00:20:05,280 --> 00:20:10,640
Yeah, and on a more serious note, some people might say, if AI is so dangerous, can't we just

185
00:20:10,640 --> 00:20:15,680
not build it? You said something about regulation in your talk, but what would you say to them as a

186
00:20:15,680 --> 00:20:21,600
general response? You can't stop progress on something so useful and so fuzzy in terms of

187
00:20:21,600 --> 00:20:27,360
separation between narrow and general AI. If we could make it where, okay, you only can work on

188
00:20:27,360 --> 00:20:33,200
narrow AI, but not allowed to work in general, it would be a good moratorium to have for a few years.

189
00:20:33,200 --> 00:20:37,600
But the dividing line is meaningless. If you're using neural networks, they're general. If you're

190
00:20:37,600 --> 00:20:43,680
using a lot of those latest evolutionary techniques, they are leading you to general solutions. So it's

191
00:20:43,680 --> 00:20:49,200
simply impossible. If you make all computer science illegal, you're killing your economy,

192
00:20:49,200 --> 00:20:55,600
you're shifting research to other countries. So I think I'll add another impossibility result of

193
00:20:55,600 --> 00:21:01,040
unburnability of AI. You cannot ban it. You can maybe delay it at best.

194
00:21:02,000 --> 00:21:06,240
It would be very interesting if you could either include or exclude it from the impossibility

195
00:21:06,240 --> 00:21:13,680
space indeed, but I'm afraid that goes more into Simon Friedrich's chaos theorem, so to say.

196
00:21:15,440 --> 00:21:18,960
You and I both agree, I think that AI is a significant existential risk,

197
00:21:19,680 --> 00:21:24,800
but some AI researchers don't agree. And do you think there will ever be a scientific consensus

198
00:21:24,880 --> 00:21:29,280
about this? And can we hope to achieve that at some point? And why could that be

199
00:21:29,920 --> 00:21:38,080
either so or not? Well, I have a recent paper about AI risk skepticism, and I do a review of both

200
00:21:38,080 --> 00:21:45,600
why would someone not accept the risks as real and kind of specific arguments they make for it.

201
00:21:45,600 --> 00:21:51,760
I think it ended up with about 100 citations, and I have another 400 unprocessed ones. If anyone's

202
00:21:51,760 --> 00:21:59,200
interested, it could be a nice survey. The most common explanation I see is just bias. If you

203
00:22:00,000 --> 00:22:07,120
get your funding, your prestige, your reputation, everything from developing faster AI, it's very

204
00:22:07,120 --> 00:22:11,040
hard for you to say, I'm working on the most dangerous thing in the world that will kill

205
00:22:11,040 --> 00:22:18,560
everyone. So there seems to be this conflict of interest in any other domain. We wouldn't allow

206
00:22:19,520 --> 00:22:24,560
for this to happen. If you are working for a tobacco company, you wouldn't be deciding if

207
00:22:24,560 --> 00:22:29,360
smoking is dangerous. If you work for an oil company, we don't really trust your assessment

208
00:22:29,360 --> 00:22:37,760
of impact on climate. But somehow here, it's fine. And interestingly, AI is a very large umbrella

209
00:22:37,760 --> 00:22:43,920
term for lots of research sub-domains. Some people do natural language processing, some do vision.

210
00:22:43,920 --> 00:22:49,040
Not everyone does safety and security, but we feel that anyone with a label of AI researcher

211
00:22:49,040 --> 00:22:55,200
is qualified to pass judgment on the state of AI safety in software development. Not everyone

212
00:22:55,200 --> 00:23:00,320
is a cybersecurity expert. If you're working on backend, GUI, something else, you're not going

213
00:23:00,320 --> 00:23:07,840
to be consulted on how to do encryption. Why is this somehow different here? I don't fully understand.

214
00:23:08,800 --> 00:23:14,800
It would be interesting indeed also to find out. I'm also kind of puzzled, but perhaps it could

215
00:23:14,800 --> 00:23:19,760
have something to do with the fact that it's not a trial and error risk as one of the few

216
00:23:21,200 --> 00:23:27,360
areas. I think mostly, of course, you're first developing something and then later you regulate

217
00:23:27,360 --> 00:23:31,520
it, but it's only at the phase of application. So at this phase, it's much more obvious to have

218
00:23:32,080 --> 00:23:38,720
separate controlling agencies, perhaps. But when you're creating something, of course,

219
00:23:39,920 --> 00:23:47,920
that's not that obvious. Maybe one more question about also impossibility of AI safety, but I'm

220
00:23:47,920 --> 00:23:53,760
from a different angle. I don't know if you are aware of the work of Anthony Burglas. He has

221
00:23:53,760 --> 00:23:59,600
written a book about evolutionary arguments applied to AI, and it roughly goes as follows.

222
00:24:00,320 --> 00:24:05,280
For superintelligence, being friendly to people is a necessary baggage. Because of evolution,

223
00:24:05,280 --> 00:24:09,680
we should expect only the most efficient superintelligence to survive, and this is probably

224
00:24:09,680 --> 00:24:15,280
not the friendliest one. Would you agree to this evolutionary argument applied to AI,

225
00:24:15,280 --> 00:24:19,920
or what are your thoughts about this idea? I haven't read the books, so I'm trying to get the

226
00:24:19,920 --> 00:24:24,720
argument from your question. So the argument is that it's more efficient to be friendly to humans,

227
00:24:24,720 --> 00:24:28,480
and so it's a survival advantage. And the other way around, it's more efficient to be

228
00:24:28,480 --> 00:24:32,720
unfriendly to humans, so that would be a survival advantage. Because the friendliness

229
00:24:33,600 --> 00:24:38,160
would just be baggage according to him. Oh, in terms of his overhead and development,

230
00:24:38,160 --> 00:24:43,600
being friendly limits your space of possibilities. Yeah, I think there is a lot to be said about

231
00:24:43,600 --> 00:24:48,400
the trash rest turn option. It starts very friendly, gets the resources and help early on,

232
00:24:48,400 --> 00:24:55,040
and once it's capable, it turns on us and removes all restrictions. So sounds like a good book.

233
00:24:55,440 --> 00:25:01,120
I think it is, you should read it. All right, I'll put it on my list of 600 books to read,

234
00:25:01,120 --> 00:25:07,520
excellent. It was marketed very poorly, so I'm not surprised that you did read it,

235
00:25:07,520 --> 00:25:13,440
but I think the idea is interesting indeed. Are you more on the slow takeover or on the

236
00:25:13,440 --> 00:25:19,360
fast take-off sides, and why would that be? Very fast. Once we get to human level, it goes super

237
00:25:19,360 --> 00:25:24,400
intelligent almost immediately, just adding existing capabilities like infinite memory,

238
00:25:24,400 --> 00:25:29,120
access to all the human knowledge, since they are already super intelligent, if you just take

239
00:25:29,120 --> 00:25:36,960
human plus internet. And why do you think, I think the slow take-off side kind of

240
00:25:36,960 --> 00:25:42,720
gains momentum the last years, I don't know if you agree. And I've heard that this is just because

241
00:25:42,720 --> 00:25:48,000
it's more easy to write papers about this, or there are more possible stories that you could

242
00:25:48,480 --> 00:25:55,680
tell about this, but do you around you see a shift there? Do you see more people going

243
00:25:55,680 --> 00:26:02,880
towards the slow take-off side, or is that not true? I haven't surveyed, I honestly don't know

244
00:26:02,880 --> 00:26:08,400
if you think there is a shift bias by ability to publish about it, I believe you.

245
00:26:09,680 --> 00:26:16,560
I wouldn't make that claim too strictly. Okay, let's say that you're a non-AI expert and you

246
00:26:16,560 --> 00:26:22,640
still want to do something about this existential risk, such as we are kind of. What action do you

247
00:26:22,640 --> 00:26:29,600
think would be the best to take? So you're not an AI researcher, but you want to do something about...

248
00:26:30,560 --> 00:26:36,320
Yes. Is there anything at all, or would you just say, okay, just leave it to the experts,

249
00:26:36,320 --> 00:26:42,240
because there's not much you can do? I mean, in general, I think it's good if citizens are well

250
00:26:42,240 --> 00:26:47,840
informed about the world and problems, and so the next time you vote, you don't vote for someone you

251
00:26:47,840 --> 00:26:57,200
like visually, but actually picking better policies. It seems like based on age and experience of people

252
00:26:57,200 --> 00:27:02,400
were elected, at least in the US, they're not experts on most advanced technology. I hear many

253
00:27:02,400 --> 00:27:09,200
of them don't use computers, so I'm skeptical that they can keep up with crypto economics and

254
00:27:09,280 --> 00:27:15,760
cryptography and synthetic biology and other interesting questions. So your job as a citizen

255
00:27:15,760 --> 00:27:20,560
is to be informed and make sure your views, your informed views, have all represented.

256
00:27:22,800 --> 00:27:29,440
Right, some questions from the audience to not finish off yet, but we're getting to the end of

257
00:27:29,440 --> 00:27:34,800
the conference already. First one, who do you believe is responsible for the safety of AI? The

258
00:27:34,800 --> 00:27:38,400
consumers, governments, or developers, or some other stakeholders?

259
00:27:41,040 --> 00:27:45,680
So that's another interesting question. The ownership of AI itself is very difficult,

260
00:27:45,680 --> 00:27:51,440
right? If it's self-improving, it changes, it's not even obvious who has any control

261
00:27:51,440 --> 00:27:57,040
or possession over it. Obviously, the person to make it and release it has a lot of responsibility,

262
00:27:57,040 --> 00:28:02,880
but if it's out there and now you are upgrading it, supplying it with goals, giving it data,

263
00:28:03,120 --> 00:28:08,800
it feels like responsibility may shift to you. All of it for systems below human level

264
00:28:08,800 --> 00:28:13,280
performance. It's a tool, you are in charge. The moment it's human level or beyond,

265
00:28:14,160 --> 00:28:18,720
it's an independent agent. You are as responsible as you are for your adult children.

266
00:28:22,320 --> 00:28:26,320
Another one that's also very interesting, I think, is it possible to program in a programming

267
00:28:26,320 --> 00:28:31,520
language, not based on human language, to remove the ambiguity? Or would it be possible to have an

268
00:28:31,520 --> 00:28:36,800
AI create a language without ambiguity? If the AI could create such a language, would humans be

269
00:28:36,800 --> 00:28:42,720
able to learn it, or would we then also have to trust the AI to program in it? That's an

270
00:28:42,720 --> 00:28:47,680
excellent question. So there is a lot of effort. First of all, every programming language is an

271
00:28:47,680 --> 00:28:52,640
attempt to get away from English and into less ambiguous languages, but we know languages,

272
00:28:52,640 --> 00:28:58,080
programming languages have lots of bugs. There are logical languages developed to remove

273
00:28:58,160 --> 00:29:04,480
ambiguity. And I think Stephen Wolfram has a nice article about communicating with AI. And he, of

274
00:29:04,480 --> 00:29:11,360
course, uses his Mathematica and models he creates in language, Wolfram language he developed as

275
00:29:11,360 --> 00:29:17,920
possible solution. I think you can do way better than human language in terms of ambiguity. I'm

276
00:29:17,920 --> 00:29:26,400
skeptical about bug-free communication. It relies on your existing cognitive models, your

277
00:29:26,400 --> 00:29:34,080
understanding, and if you have different priors, even using well-defined terms may lead to problems.

278
00:29:34,080 --> 00:29:38,320
But it's a very interesting area to do additional research. If you have

279
00:29:38,320 --> 00:29:42,400
background in linguistics, I definitely invite you to look into that.

280
00:29:44,320 --> 00:29:49,440
Another interesting one from Simon Friedrich, actually a previous speaker. Do you think AGI

281
00:29:49,440 --> 00:29:53,520
could help overcome their global collective action problems that are at the roots of basically

282
00:29:53,520 --> 00:29:56,400
all the existential risks, including those of AI itself?

283
00:29:58,880 --> 00:30:05,760
So that's another great question. I see AI as a meta problem and meta solution. If we get it right,

284
00:30:05,760 --> 00:30:11,680
if I'm wrong and you can make friendly superintelligence well-aligned, everything I said is just a

285
00:30:11,680 --> 00:30:17,200
mistake, then it solves all the other existential problems trivially. Whatever is climate change,

286
00:30:17,200 --> 00:30:23,440
synthetic bio, you have a godlike tool for solving those problems. If I'm right and it's

287
00:30:23,440 --> 00:30:29,040
a terrible risk and it comes before, then it solves it by either killing all of us. We don't

288
00:30:29,040 --> 00:30:34,880
have to worry about it or it comes before again. So if it takes a hundred years for climate change

289
00:30:34,880 --> 00:30:39,600
to build up to boiling point, this happens in 20 years. It kind of dominates the risk.

290
00:30:40,480 --> 00:30:46,720
I'm not sure about applying AI to solve the AI problem. That's a bit of a catch-22. There are

291
00:30:46,720 --> 00:30:54,960
those solutions where you have a supervisory agent, AI, AGI, Nyanee, which looks after the world,

292
00:30:54,960 --> 00:31:03,200
making sure no one creates dangerous AIs. I'm very skeptical of such super agents with a lot of

293
00:31:04,080 --> 00:31:10,720
government control powers. I think they may be worse than what the system we're protecting against

294
00:31:11,360 --> 00:31:22,400
gives us. Great answer, I think. One final question from the audience now. So if we cannot stop AI

295
00:31:22,400 --> 00:31:27,120
development and we cannot totally ensure that it is safe, do we just need to accept that it is a

296
00:31:27,120 --> 00:31:31,360
risk or even a big risk? Or is there anything we can do, for example, policy-wise?

297
00:31:33,680 --> 00:31:38,400
So I think we need to do more research. I published those papers about a year ago and I

298
00:31:38,400 --> 00:31:43,200
haven't seen a strong response from a community addressing those. If somebody just published

299
00:31:43,200 --> 00:31:47,920
a paper saying, this is why you're wrong, then be very happy, but I haven't seen it. So I have to

300
00:31:47,920 --> 00:31:53,040
assume that there is some merit to what I'm saying. The question is then, what do we do with our

301
00:31:53,040 --> 00:31:59,360
lives? How do we update based on that? What do we change? For most people, I don't know if it makes

302
00:31:59,360 --> 00:32:04,560
any difference. Before you were told, okay, you're definitely dying in 60 years. Now you may be dying

303
00:32:04,560 --> 00:32:12,480
in 40. Not a big update. Figure out what to do with your 401k plan. You spend it on something now

304
00:32:12,480 --> 00:32:19,920
or wait for it to become worthless later. I don't have any magical solutions or answers. I am curious

305
00:32:20,640 --> 00:32:27,280
in case of successful alignment what happens to economy, what happens to work, what happens to

306
00:32:27,280 --> 00:32:35,280
people's social interactions. I do have a paper which kind of assumes that progress in virtual

307
00:32:35,280 --> 00:32:43,200
reality will be as good as progress in AI. And so each one of us gets what I call a personal

308
00:32:43,200 --> 00:32:48,720
universe, where you basically get to do whatever you want and you don't have to negotiate with

309
00:32:48,720 --> 00:32:54,560
others. There is no need for consensus. You basically have independence. So at least the

310
00:32:54,560 --> 00:33:00,000
difficult part of value alignment problem is not aligning with me or you. It's hard. It's hard,

311
00:33:00,000 --> 00:33:05,440
but it's not impossible. It's getting 8 billion people plus all the squirrels and whatnot to agree

312
00:33:05,440 --> 00:33:11,360
on something. And this is where the personal universe solution reduces it to just now we need

313
00:33:11,360 --> 00:33:16,560
to control the substrate. If you can get control of computational substrate and everyone gets

314
00:33:16,560 --> 00:33:23,200
the resources to run their personal universe, okay, we're doing well. We have this virtual agreement.

315
00:33:24,560 --> 00:33:33,760
I think that's not a good point again. I'm wondering also on a more personal level,

316
00:33:33,760 --> 00:33:39,200
like when did you start to think about AI safety yourself and when did you move into this

317
00:33:39,200 --> 00:33:45,280
research field? Was there anything that inspired you to do this and also what were the responses

318
00:33:45,280 --> 00:33:51,360
that you got from fellow scientists in moving in this direction? Well, it was a very gradual

319
00:33:51,360 --> 00:33:56,720
process. So I was doing research and behavioral biometrics. I was profiling poker players to see

320
00:33:56,720 --> 00:34:02,880
if, you know, accounts get hacked and tell the things like that. And at the time, I realized

321
00:34:02,880 --> 00:34:08,480
majority of online players are now bots. So my work started to be about detecting bots,

322
00:34:09,440 --> 00:34:14,400
preventing bots from participating. But the question was, as bots get smarter and better,

323
00:34:14,400 --> 00:34:18,960
can we keep up? And not just in poker, but in general online bots and automation.

324
00:34:19,680 --> 00:34:26,560
I did that type of work for a while. I went to what was at the time Singularity Institute for

325
00:34:26,560 --> 00:34:31,680
artificial intelligence, which was fighting hard against artificial intelligence. But

326
00:34:31,680 --> 00:34:37,920
they had a lot of great ideas, which I still work on. And I've been back as a fellow and a

327
00:34:37,920 --> 00:34:43,680
research advisor for Machine Intelligence Research Institute. I think they're doing excellent theoretical

328
00:34:43,680 --> 00:34:54,800
work. Yeah, I think perhaps one more, like some AI researchers, I think, might be hesitant to talk

329
00:34:54,800 --> 00:34:59,360
about existential risk in the public debate, like you already quickly mentioned, for example, in the

330
00:34:59,360 --> 00:35:06,000
media. Do you agree that they are hesitant to do that? And why do you think that is so?

331
00:35:08,160 --> 00:35:12,320
I think I lost a few words. So with media, what's the concern?

332
00:35:12,400 --> 00:35:17,440
Sorry, I'll just repeat the whole thing. Some AI researchers might be hesitant to talk about

333
00:35:17,440 --> 00:35:22,960
existential risk in the public debates, for example, in the media. Do you agree that this is so, that

334
00:35:22,960 --> 00:35:30,240
they are hesitant to do this? And if so, why do you think that is? Well, it's a personal decision

335
00:35:30,240 --> 00:35:35,520
based on your situation. So some people, before they get tenure, follow a very good advice of

336
00:35:35,520 --> 00:35:41,840
be quiet. After you get tenure, never shut up again. But that's not a bad idea. You'll definitely

337
00:35:41,840 --> 00:35:47,200
get someone disappointed in you. And that doesn't help your tenure case. I'm tenured, so I've been

338
00:35:47,200 --> 00:35:55,040
saying stupid things for years now. What do you think about an initiative such as the existential

339
00:35:55,040 --> 00:36:02,480
risk of territory? Is it useful to communicate this to more people in general, or to a certain

340
00:36:02,480 --> 00:36:08,400
subset of people? Or do you think it's basically something that should be solved among researchers?

341
00:36:09,040 --> 00:36:14,640
Well, if you think about developing a GI, working on superintelligence, you're really running an

342
00:36:14,640 --> 00:36:18,960
experiment on all the humans, right? You've got eight billion subjects, none of them consented to

343
00:36:18,960 --> 00:36:28,160
that work. The least you can do is tell them about it. That's actually great now to end this talk.

344
00:36:28,160 --> 00:36:31,920
If there's no more questions from the audience, and I think we've covered those.

345
00:36:31,920 --> 00:36:44,560
Yeah, and then I think we'll leave it here. It was super nice talking to you, and super nice to

346
00:36:44,560 --> 00:36:51,600
listen to your short presentation. And I hope that you will also enjoy the rest of the conference

347
00:36:51,600 --> 00:36:56,240
maybe tomorrow, and that will definitely be in touch and to cooperate more on this

348
00:36:57,120 --> 00:36:59,840
quite hairy problem, but still very interesting one to think about.

349
00:36:59,840 --> 00:37:04,080
Absolutely, and hopefully we'll meet in person one day. Likewise.

