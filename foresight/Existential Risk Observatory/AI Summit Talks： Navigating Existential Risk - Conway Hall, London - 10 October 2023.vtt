WEBVTT

00:00.000 --> 00:02.060
you

00:30.000 --> 00:32.060
you

01:00.000 --> 01:02.060
you

01:30.000 --> 01:32.060
you

06:00.000 --> 06:27.760
welcome everybody. I encourage you to take a seat. We will be starting almost on time

06:27.760 --> 06:36.920
because we have a very rich agenda on a very big topic. We are talking about

06:36.920 --> 06:44.720
navigating existential risk. Navigating what people have described as a very

06:44.720 --> 06:53.640
difficult, tortuous landscape of risks that are made worse by oncoming new

06:53.640 --> 07:00.400
frontier AI. That's not just the AI that we have today, but AI that we can

07:00.400 --> 07:06.600
fairly easily imagine as coming within the next year or two. Next generation AI

07:06.600 --> 07:13.160
that's more powerful, more skillful, more knowledgeable, potentially more

07:13.160 --> 07:19.880
manipulative, potentially more deceitful, potentially more slippery, definitely

07:19.880 --> 07:25.840
more powerful than today's AI. That AI might generate existential risks in its

07:25.840 --> 07:32.680
own right. That AI is likely also to complicate existing existential risks,

07:32.680 --> 07:39.840
making some of the risks we already know about more tricky to handle, more wicked.

07:39.840 --> 07:45.840
And we might also talk about the way in which next generation AI might be the

07:45.840 --> 07:51.560
solution to some of the existential risks and dilemmas facing society. If we can

07:51.560 --> 08:00.000
apply AI wisely, then perhaps we can find the narrow path through this difficult

08:00.000 --> 08:06.320
landscape. So welcome navigators in the hall. Welcome to navigators watching the

08:06.320 --> 08:12.360
live stream. Welcome to people and AI's watching the recording of this

08:12.680 --> 08:20.360
discussion. Let's get stuck in. We have lots of very capable, knowledgeable

08:20.360 --> 08:26.040
speakers who will approach this from a diversity of points of view. Indeed, I

08:26.040 --> 08:30.280
think one of the hazards in this whole topic is that some people want to be a

08:30.280 --> 08:35.360
little bit one dimensional. They want to say, this is how we'll solve the problem.

08:35.360 --> 08:39.600
It's quite straightforward. In my view, there are no straightforward solutions

08:39.600 --> 08:44.480
here, but you can make up your own minds as you listen to what all the speakers

08:44.480 --> 08:48.280
and panelists have to say. And yes, in the audience, you'll have a chance later

08:48.280 --> 08:53.960
on to raise your hand and get involved in the conversation too. The first person

08:53.960 --> 08:57.920
you're going to hear from is unfortunately not able to be with us

08:57.920 --> 09:04.680
tonight, but he has recorded a short video. He is Sir Robert Buckland, MP,

09:04.680 --> 09:09.360
former Lord Chancellor of the United Kingdom, which means he was responsible

09:09.400 --> 09:14.400
for the entire justice system here, former Secretary of State for Wales. He is

09:14.400 --> 09:22.200
still an MP and he has a side hustle as a senior fellow at the Harvard Kennedy

09:22.200 --> 09:27.120
School, where he is writing papers on exactly the topics we're going to be

09:27.120 --> 09:33.200
discussing tonight, namely how does AI change key aspects of society, potentially

09:33.240 --> 09:39.960
making it better, potentially making it much worse if we are unwise. So let's

09:39.960 --> 09:44.720
watch Sir Robert Buckland who will appear by magic on the big screen.

09:46.240 --> 09:51.120
Well, I'm very pleased to be able to join you, albeit virtually, for the Conjecture

09:51.120 --> 09:59.160
ARO Summit on AI and the challenges and opportunities that it presents us. And I

09:59.160 --> 10:05.040
think my pleasure at being with you is based upon not just my own experience in

10:05.040 --> 10:12.000
government, but also my deep interest in the subject since my departure from

10:12.000 --> 10:17.720
government last year. Now, when I was in government, I had responsibility for

10:18.480 --> 10:23.680
for many years the legal advice that was given to departments and indeed to the

10:23.680 --> 10:27.920
government in general when I was in the Law Offices Department as the Solicitor

10:27.920 --> 10:33.200
General. And then responsibility for running the Ministry of Justice as

10:33.200 --> 10:38.120
Lord Chancellor and Secretary of State for over two years before a brief return as

10:38.120 --> 10:45.000
Wales Secretary last year. That seven years or so experience within government as a

10:45.000 --> 10:50.720
minister gave me, I think, a very deep insight into the pluses and the minuses

10:50.720 --> 10:56.520
of the way the government works, the efficiencies and indeed the inefficiencies

10:56.760 --> 11:03.440
about process. And I think clearly, as in other walks of life, artificial

11:03.440 --> 11:08.840
intelligence, machine learning will bring huge advantages to government processes to

11:08.840 --> 11:16.040
improve efficiency, to speed up a lot of the particular ways in which government

11:16.040 --> 11:21.040
works, which will be, I think, to the benefit of citizens, whether it's citizens

11:21.080 --> 11:26.920
waiting for passport applications, visa applications or other government processes

11:26.920 --> 11:33.080
benefits, for example. However, I think that we kid ourselves if we don't accept

11:33.080 --> 11:39.160
the fact that alongside the benefits come potential pitfalls. And the first and

11:39.160 --> 11:44.680
most obvious one, I think, for me is the scrutability of process. In other words,

11:45.320 --> 11:49.800
the way in which we understand how decisions are made. And that's very

11:49.800 --> 11:54.920
important. Because understanding how decisions are made is part of democratic

11:54.920 --> 11:59.560
accountability in societies like ours, where individuals or organizations wish

11:59.560 --> 12:04.040
to challenge decisions made by government, perhaps through judicial review

12:04.040 --> 12:09.720
applications, then the explicability of those decisions, which is accompanied by

12:10.000 --> 12:14.600
a duty of candor by the government in order to disclose everything about those

12:14.600 --> 12:20.360
decisions is part of that accountability. And of course, it's sometimes very

12:20.360 --> 12:25.440
difficult to explain how the machine has come to decisions. And more fundamental

12:25.440 --> 12:29.640
than that, we have to accept that if the data sets that are used in order to

12:29.640 --> 12:37.400
populate the processes are not as full of integrity as they should be, and are

12:37.400 --> 12:43.000
not the product of genuinely objective and carefully calibrated processes, then

12:43.000 --> 12:47.720
we are in danger of importing historic biases into the system, whether it's

12:47.720 --> 12:52.360
biases against neurodiverse people making job applications, or deep biases

12:52.360 --> 12:57.160
against people of color in the criminal justice system, simply because the data

12:57.160 --> 13:03.000
sets have imported those historical anomalies, those historical imbalances.

13:03.560 --> 13:08.160
Now, all those questions have really got me thinking very deeply about the

13:08.160 --> 13:14.280
impact of machine learning on the ethics of justice itself. And as a result of

13:14.280 --> 13:18.480
my thinking, I was delighted last year to be accepted as a senior fellow at the

13:18.480 --> 13:23.640
Moserfa-Romani Center for Business and Government at Harvard Kennedy School, and

13:23.640 --> 13:29.960
I am working currently on a number of papers relating to the impact of AI and

13:29.960 --> 13:36.760
machine learning on the administration of justice and the law itself. It really

13:36.760 --> 13:43.560
developed from my own experience as law chancellor, from digitalization, I

13:43.560 --> 13:50.080
should say, of the courts when, during the Covid pandemic, we had to move many,

13:50.080 --> 13:54.680
many thousands of hearings online for the first time. You know, I think we jumped

13:54.680 --> 14:00.960
from a couple of hundred phone or online hearings to 20,000 a week in a very

14:01.000 --> 14:07.920
short compass. And the status quo will never be the same again. In fact, it has

14:07.920 --> 14:12.720
moved on, I think, in a way that we just hadn't foreseen before the pandemic. Now,

14:12.720 --> 14:16.600
I think that's a good thing, but I also think that accompanying this question

14:16.600 --> 14:21.720
about increased efficiency is the use of artificial intelligence. Now, in some

14:21.720 --> 14:27.280
jurisdictions, such as China, we are seeing its increased use, not just to do

14:27.320 --> 14:33.160
legal research and to prepare cases, but to actually decide themselves. In other

14:33.160 --> 14:38.200
words, the AI judge. Now, that's all well and good. But do we actually know what

14:38.200 --> 14:44.720
populates the data sets that then forms the basis of the decisions made? And I

14:44.720 --> 14:50.080
think it's that intentional, unintentional bias, or indeed worse than that

14:50.080 --> 14:55.400
potential intentional bias, whether that's influenced by a government or

14:55.400 --> 15:00.360
indeed a corporate that might be able through their financial means to

15:01.080 --> 15:08.560
influence a procedure or indeed the way in which we deal with cases, knowing as

15:08.560 --> 15:13.480
we might do more information about the way in which judges make the decisions.

15:13.880 --> 15:18.280
All these questions, I think, need to be asked now before we end up in a

15:18.280 --> 15:24.000
position where we've slept, walked into a completely different form of justice

15:24.000 --> 15:28.880
than the one that we know. Now, underpinning all of this, I think, is the

15:28.880 --> 15:33.680
need to ask a fundamental question about judgment itself. And that's what I've

15:33.680 --> 15:37.320
been doing in my first paper. You know, the essence of human judgment is

15:37.320 --> 15:42.000
something that will be based not just upon an understanding of the law, but on

15:42.000 --> 15:47.080
our experiences as human beings. And you can go right back, as I have done, to

15:47.080 --> 15:53.400
the judgment of Solomon and his emotional response to the woman who

15:53.400 --> 15:58.000
clearly was the true mother of the child that he proposed to be cut in half.

15:58.440 --> 16:02.480
Now, you know, that's an example, I think, of the human element of

16:02.480 --> 16:10.800
judgment, which has to be an essential foundation of decision making,

16:10.800 --> 16:14.160
particularly when it comes to the assessment of credibility of a witness,

16:14.880 --> 16:20.760
human witness giving evidence upon which the case stands all falls. And of

16:20.760 --> 16:24.520
course, for judge, that applies for juries as well in criminal trials,

16:24.760 --> 16:30.480
particularly here in the UK. Now, you know, all these questions, I think, need

16:30.480 --> 16:35.720
to be asked. And then we need to work out what it is that we want to retain out

16:35.720 --> 16:39.320
of all of this. Now, I don't think we should make any cosy assumptions that

16:39.320 --> 16:46.680
because at the moment some large learning systems are having hallucinations.

16:46.680 --> 16:54.000
I don't think we should be assuming that just because of that, therefore AI

16:54.000 --> 16:59.520
will never work in a way that can achieve a greater degree of certainty.

16:59.520 --> 17:07.480
I think the inevitable arc of development will result in better and

17:07.480 --> 17:12.800
better and more capable machines. That's inevitable. And what we must be

17:12.800 --> 17:17.480
asking at the same time as capability is ensuring there is greater security and

17:17.480 --> 17:22.160
safety when it comes to the use of AI. And that really underpins, I think,

17:22.400 --> 17:26.280
the work that I'm doing in the field of justice. What does this all lead to

17:26.280 --> 17:32.120
then? Well, we have the AI Safety Summit in the UK next month. I very much

17:32.120 --> 17:37.800
hope that that summit will first of all involve those critical players in

17:37.800 --> 17:45.200
terms of international organisations and key countries as well that will come

17:45.200 --> 17:49.760
together to commit to creating, I think, a defined framework within which we

17:49.760 --> 17:53.960
should be using AI safely. And that framework, I think, will have to take

17:54.120 --> 17:58.200
several forms. I think in the field of justice, we could do with an international

17:58.200 --> 18:04.080
framework of principles, which will ensure transparency and which can

18:04.120 --> 18:08.800
reassure people that in cases of the liberty of the individual criminal

18:08.800 --> 18:13.280
cases, cases where perhaps the welfare of a child and the ultimate destination of

18:13.280 --> 18:21.200
a child is in issue, then the human element will be the key determinant in

18:21.200 --> 18:27.320
any decisions that are made. And that the use of machines will be transparent and

18:27.320 --> 18:32.960
made known to all the parties throughout the proceedings. And then other

18:33.000 --> 18:38.120
walks of life, I think the AI Safety Summit has to then look as well at

18:38.520 --> 18:43.400
whether frameworks can be created and what form they should take. I think

18:43.400 --> 18:47.240
it's tempting to try and be prescriptive. I think that would be a mistake, not

18:47.240 --> 18:51.840
just for the obvious reason that AI is developing and therefore anything that

18:51.840 --> 18:56.760
we write in 2023 will soon be out of date. But the very fact that AI itself

18:57.160 --> 19:03.920
does not mean an alloyed harm. In fact, it means a lot of benefit and also

19:03.920 --> 19:07.920
some neutral effects as well. And where you have that approach, then a

19:07.920 --> 19:13.920
principle based system seems to me to be more sensible than overly

19:13.920 --> 19:17.880
prescriptive and detailed rules as you would have, for example, to prevent a

19:17.880 --> 19:23.720
crime such as fraud. So just some preliminary thoughts there as to the

19:23.720 --> 19:28.000
impact of machine learning. I don't pretend to be a technical expert. I'm

19:28.000 --> 19:34.000
not. But my years in justice, my years as a lawyer, a judge and as a senior

19:34.520 --> 19:40.920
cabinet minister, I think obliged me to do some of the thinking now to help

19:40.920 --> 19:46.160
ensure that countries like Britain are in the forefront of the sensible and

19:46.160 --> 19:52.120
proportionate regulation of the use of machine learning and other types of

19:52.160 --> 19:57.360
artificial intelligence. If we don't do it now, then I think we'll be missing

19:57.360 --> 20:02.760
an unhistoric opportunity. I wish you all well and I look forward to meeting

20:02.760 --> 20:08.000
some of you in future and discussing these issues as they develop. Thank you

20:08.000 --> 20:08.520
very much.

20:10.320 --> 20:15.360
Well, thank you, Sir Robert, who may be watching the recording of this. Don't be

20:15.360 --> 20:22.360
prescriptive, he said. Let's sort out some sensible proportionate regulation. Is

20:22.360 --> 20:27.120
that credible? Is that feasible? You'll be hearing from other panelists who may

20:27.120 --> 20:33.320
be commenting on that shortly. So Robert also said there are risks such as the

20:33.360 --> 20:39.040
inscrutability of AI. We don't understand often how they reach its decisions. We

20:39.040 --> 20:44.000
don't understand the biases that might be there that might have been planted. We

20:44.000 --> 20:48.840
might lose charge. We might become so used to AI taking decisions that humans

20:48.840 --> 20:54.560
end up in a very sad place. But how bad could things get? That's what we're

20:54.560 --> 20:59.520
going to hear from our next speaker. So I'm going to ask Conor Leahy to come up

20:59.520 --> 21:06.360
to the stage while I briefly introduce him. Conor is the CEO of Conjecture. If

21:06.360 --> 21:10.080
you haven't heard about Conjecture, I think you need to do a bit more reading.

21:10.080 --> 21:15.600
Perhaps Conor will say a little bit about it. They are AI alignment solutions

21:15.600 --> 21:21.600
company, international, but with strong representation here in the UK. So welcome

21:21.600 --> 21:23.000
Conor, the floor is yours.

21:31.000 --> 21:34.800
Thank you so much. It's so great to see you all today. So happy to be able to talk

21:34.880 --> 21:40.640
to you here in person. And man, do we live in interesting times, to put it

21:40.640 --> 21:48.640
lightly. The world has changed so much. Just in the last few years, a few months

21:48.640 --> 21:55.080
even, so much has happened in the world of AI and beyond. Just a couple of years

21:55.080 --> 22:02.240
ago, there wasn't such a thing as chat GPT, or even GPT 3, or 4, or 2, or any of

22:02.240 --> 22:07.200
those. It was a different world not too long ago when technologists such as myself,

22:07.200 --> 22:15.960
weird little hobbyists, worried about the problem of AGI and how it will affect

22:15.960 --> 22:22.120
the world. Back then, it still seemed so far away. It seemed like we still had time.

22:23.120 --> 22:32.120
But now we find ourselves in a world of unrestricted, uncontrolled scaling, a

22:32.120 --> 22:38.560
race towards the finish, towards the end, to scale our AI systems ever more

22:38.560 --> 22:45.600
powerful, more general, more autonomous, more intelligent. And the reason I care

22:45.600 --> 22:51.880
about this is very simple. If we build systems that are smarter than humans, that

22:51.960 --> 22:58.400
are more capable at manipulation, deception, politics, making money, scientific

22:58.400 --> 23:04.560
research, and everything else, and we do not control such systems, then the future

23:04.560 --> 23:14.440
will belong to them, not to us. And this is not the future I want. I want a future

23:14.440 --> 23:20.000
in which humanity gets to decide its destiny. We get to decide the future for

23:20.040 --> 23:25.320
ourselves, for our children, for our children's children, that we like. The

23:25.320 --> 23:30.880
future where our children can live long, happy lives surrounded by beauty, art,

23:30.880 --> 23:36.640
great technology, instead of being replaced by souless automata. And let me be

23:36.640 --> 23:44.440
clear that this is the default outcome of building an uncontrolled AGI system,

23:45.160 --> 23:54.920
the full replacement of mankind. And what we're seeing is that AI is on an

23:54.920 --> 24:01.440
exponential. There's a race. All the top organizations, which is open AI, deep

24:01.440 --> 24:07.600
mind, anthropic, among others, are racing ahead as fast as the VC dollars

24:07.680 --> 24:15.480
scale up their work. And this has given us an exponential. AI is on an

24:15.480 --> 24:19.760
exponential curve, both on hardware and on software. It's improving at

24:19.760 --> 24:24.600
incredible rates. And when you're dealing with an exponential, there are

24:24.600 --> 24:33.680
precisely two times you can react to it too early or too late. There is no such

24:33.760 --> 24:38.440
thing as reacting at just the right moment on an exponential, where you

24:38.440 --> 24:43.040
find just the perfect middle point just in the nick of time when everyone

24:43.040 --> 24:47.760
agrees that the problem is here and everything has perfect consensus. If you

24:47.760 --> 24:55.600
do this, you are too late. It will be too late. And the same thing applies to

24:55.600 --> 25:02.440
AGI. If we wait until we see the kinds of dangerous general purpose systems that

25:02.440 --> 25:08.760
I am worried about, then it will already be too late. By the moment such systems

25:08.760 --> 25:18.760
exist, the story of mankind is over. And so if we want to act, we must act well,

25:18.760 --> 25:24.760
well before such things actually come into existence. And unfortunately, we do

25:24.760 --> 25:32.400
not have much time. How the world has changed. As frightening and as

25:32.400 --> 25:39.680
terrible the race may be, there's also good changes. A few years ago, I could

25:39.680 --> 25:44.440
have barely imagined seeing governments, politicians, and the general public

25:44.440 --> 25:49.080
waking up to these weird nerd issues that I cared about so much with my

25:49.080 --> 25:53.840
friends online. But now we're looking forward to the first international AI

25:53.880 --> 25:59.840
summit convened by the UK and the famous Tip Bletchley Park. And this is great

25:59.840 --> 26:04.840
news. The European Commission has recently officially acknowledged the

26:04.840 --> 26:09.080
existential risks from AGI along with the risks from nuclear weapons and

26:09.080 --> 26:15.320
pandemics. This is great progress. This is fantastic. It is good to see our

26:15.320 --> 26:19.720
governments and our societies waking up and addressing these issues or at least

26:19.800 --> 26:24.960
beginning to acknowledge them. And we must use this opportunity. We have an

26:24.960 --> 26:30.960
opportunity right now and we must prevent it from being wasted. Because

26:30.960 --> 26:36.360
there's also bad news. We're having this great opportunity to start building

26:36.360 --> 26:41.240
the regulation and the coordination necessary for a good future. The very

26:41.240 --> 26:45.720
people who are creating these risks, the very people at the heads of these

26:45.720 --> 26:50.480
labs, these organizations, buildings technologies, are the very people who are

26:50.480 --> 26:55.520
being called upon by our governments to help regulate the very problem that

26:55.520 --> 27:00.200
they themselves are creating. And let me be very explicit about this. The

27:00.200 --> 27:08.440
problem that we face is not AGI. AGI doesn't exist yet. The problem we

27:08.440 --> 27:14.120
face is not a natural problem either. It is not an external force acting

27:14.120 --> 27:21.000
upon us from nature. It comes from people, from individual people, businessmen,

27:21.000 --> 27:27.160
politicians, technologists, athletes, large organizations who are racing, who are

27:27.160 --> 27:30.520
skilling, who are building these technologies and who are creating these

27:30.520 --> 27:38.280
risks for their own benefit. But they have offered us these very people who are

27:38.360 --> 27:44.760
causing this problem, have offered us a solution. Fantastic. And they are pushing

27:44.760 --> 27:50.760
it as hard as they can towards the UK government and the upcoming summit. So

27:50.760 --> 27:57.800
what is the solution? The solution to the problem of scaling these labs, these

27:57.800 --> 28:02.600
accelerations labs such as Anthropic and ARC have been pushing for. What is the

28:02.600 --> 28:07.880
solution? Well, the solution to the scaling problem is called responsible

28:07.880 --> 28:13.560
scaling. Now, what is responsible scaling? You might ask, you see, it's like

28:13.560 --> 28:17.400
normal scaling except you put the word responsible in front of it. And that

28:17.400 --> 28:25.880
makes it good. So, of course, I'm joking somewhat, but there's a lot of truth in

28:25.880 --> 28:34.600
humor. Responsible scaling is basically the policy, and you can read this on

28:34.600 --> 28:39.640
both ARC or Anthropics website, is the policy proposal that we should continue

28:39.640 --> 28:45.720
to scale uninhibited until at some future time when tests and evaluations that

28:45.720 --> 28:49.800
do not yet exist and we do not know how to build, but the labs promise us they

28:49.800 --> 28:55.800
will build, detect some level of dangerous capabilities that we do not yet know,

28:55.800 --> 29:01.480
and then once it gets to that point, then they will stop, maybe, except there is

29:01.560 --> 29:06.120
a clause in the Anthropic version of the RSP paper in which they say that if a

29:06.120 --> 29:11.560
different organization was scaling even super unsafely, then they can break this

29:11.560 --> 29:19.880
commitment and keep scaling anyways. So, this could be sensible if they committed

29:19.880 --> 29:24.920
to, you know, a sensible bound, a conservative point on which to stop, but

29:24.920 --> 29:31.160
unfortunately the responsible scaling policy RSP fails to actually commit to

29:31.240 --> 29:38.120
any objective measure whatsoever. So, effectively, the current policy is to

29:38.120 --> 29:45.080
just keep scaling until they feel like stopping. This is the policy that is

29:45.080 --> 29:49.720
being suggested to our politicians and to the wider world as the responsible

29:49.720 --> 29:55.720
option for policy makers. It is trying to, is very clear that it is trying to

29:55.720 --> 30:02.120
recast this techno-libertarian extremist position as sensible, moderate,

30:02.120 --> 30:10.120
responsible even. Now, in my humble opinion, the reasonable, moderate position

30:10.120 --> 30:13.960
to when dealing with a threat that is threatening the lives of billions of

30:13.960 --> 30:23.560
people is to simply not do that. But instead, this is trying to pass off this

30:23.560 --> 30:31.000
as the sensible, middle-ground position. The truth of RSP is that it comes from

30:31.640 --> 30:38.360
the same people who are causing this risk to exist. These people, the heads of

30:38.360 --> 30:42.360
these labs, many of the scientists and the policy people and the other people

30:42.360 --> 30:47.880
working on this, have known about existential risks for decades. And they

30:47.880 --> 30:51.960
fully admit this. This is not like they haven't heard about this. It's not even

30:51.960 --> 30:56.200
that they don't believe it. You can talk to them. They're on the record talking

30:56.200 --> 31:01.800
about how they believe that there is a significant chance that AGI could cause

31:01.800 --> 31:08.120
extinction of the entire human species. In a recent podcast, Dario Amade, the CEO

31:08.120 --> 31:11.880
of Anthropic, one of these labs, himself, said that he thinks it's a

31:11.880 --> 31:17.720
probably 25% chance that it could kill literally everybody. And they're doing it

31:17.720 --> 31:23.400
anyway. Despite this, they keep doing it. Why? Well, if you were talking to these

31:23.400 --> 31:28.440
people, what they might tell you is that, sure, you know, I know it's dangerous. I

31:28.440 --> 31:33.640
am very careful. But these other guys, well, they're even less careful than me. So

31:33.640 --> 31:37.960
I need to be number one. So I actually have to race faster than everyone else.

31:37.960 --> 31:43.160
And they all think this about each other. They call this incremental, but they

31:43.240 --> 31:50.040
never pause. They always race as fast as they possibly can. Do as I say, not as I

31:50.040 --> 31:55.160
do. There is a technical term for this. It's called hypocrisy.

31:57.240 --> 32:03.560
And RSP is no different. They are simply trying to twist words in an Oralian way

32:04.760 --> 32:08.360
to be allowed to keep doing the thing that they want to do anyways,

32:08.680 --> 32:19.640
which they themselves say could risk everybody. I mean, has responsible in the name, must be good.

32:21.960 --> 32:28.520
When people like Sam Altman talk about iterative deployment, about how we must iteratively

32:28.520 --> 32:34.200
release AI systems into the wild so societies can adapt to them, be inoculated by them,

32:34.280 --> 32:40.920
it sounds so nice. It sounds almost responsible. But if you're really trying to inoculate

32:40.920 --> 32:47.800
someone, you should let the host actually adapt before you jam in the next new pathogen

32:47.800 --> 32:53.240
into their weakened immune system as fast as you possibly can. But this is exactly what

32:53.240 --> 32:59.160
laboratories such as OpenAI, DeepMind, Anthropic, and Tier 2 labs such as Meta are doing with all

32:59.160 --> 33:04.440
the force they can muster to develop more and more new systems as fast as possible,

33:04.440 --> 33:11.320
release them as fast as possible, wide as spread possible. Now, if OpenAI had developed a GPT-3

33:11.960 --> 33:17.560
and then completely stop further scaling, focused all of the efforts on understanding GPT-3,

33:17.560 --> 33:23.960
making it safe, making it controllable, working with governments and civil society to adapt the new

33:24.040 --> 33:29.800
problems posed by a system for years or even decades, and then they build GPT-4?

33:30.680 --> 33:35.000
Yeah, you know what? Fair enough. I think that could work. That would be responsible.

33:35.720 --> 33:37.000
But this is not what we were seeing.

33:40.040 --> 33:45.640
All of these people at all of these institutions are running a deadly experiment

33:46.360 --> 33:53.560
that they themselves think might cause extinction. It is gain of function research on AI

33:53.960 --> 34:00.520
just like viruses, developed and released to the public as fast and aggressively as possible.

34:02.520 --> 34:07.160
They're developing more and more dangerous and more and more powerful viruses as quickly as

34:07.160 --> 34:16.840
possible and forcing it into everyone's immune system until they break. There is no responsible

34:16.840 --> 34:22.920
gain of function research for extinction level threats. There is no such thing.

34:23.960 --> 34:28.440
We have no control over such systems and there is no responsible way to continue

34:29.160 --> 34:33.880
like this. And anyone who tells you otherwise is lying.

34:36.600 --> 34:44.600
A lot has changed. The summit can lead to many boring outcomes, just exchanges of diplomatic

34:44.600 --> 34:50.600
platitudes as is often the outcome of such international events. They have some good

34:50.600 --> 34:58.680
outcomes and can have some very, very bad outcomes. Success in the summit is progress

34:58.680 --> 35:03.800
towards stopping the development of extinction level AGI before we know how to control it.

35:04.760 --> 35:13.960
Most other outcomes are neutral and bad outcomes. They look like policymakers blindly and sheepishly

35:14.040 --> 35:20.760
swallowing the propaganda of the corporations to allow them to continue their unconscionably

35:20.760 --> 35:27.320
dangerous gamble for their own personal gain and glory at the expense of the entire planet.

35:28.680 --> 35:36.280
We owe it to ourselves and our children to build a good future, not gamble it all

35:37.160 --> 35:45.320
on a few people's utopian fever dreams. Governments and the public have a chance

35:45.320 --> 35:50.520
to regain control over the future, and this is very hopeful. I wasn't sure we were going to get it,

35:51.080 --> 35:56.520
but the summit speaks to this, that people can act, that governments can act, that civil society

35:57.080 --> 36:04.680
can act, that it is not yet too late. There is simply no way around it. We need to stop

36:04.680 --> 36:10.360
the uncontrolled scaling, the uncontrolled race, if we want a good future.

36:12.120 --> 36:19.800
And we are lucky because we can do this. We can cap the maximum amount of computing power

36:19.800 --> 36:25.080
going into these AI systems. We can have government intervene and prevent the creation

36:25.080 --> 36:32.600
of the next more dangerous, more general, more intelligent strain of AI until we are ready to

36:32.600 --> 36:42.040
handle it. And don't let anything distract you from this. There is no good future in which we

36:42.040 --> 36:49.880
continue on this path, and we can change this path. We need to come together to solve these

36:49.880 --> 36:55.480
incredibly complex problems that we are facing, and not let ourselves be led astrayed by corporate

36:55.480 --> 37:04.520
propaganda. And I hope that the governments and the civil society of the world do what needs to be

37:04.520 --> 37:21.960
done. Thank you. Thank you, Conor. We will take questions from the floor in a moment.

37:22.520 --> 37:25.720
I'll just start off with the question, I think, maybe on many people's minds.

37:26.280 --> 37:33.160
Why would a super-intelligent AI actually want to kill humans? I have a super-intelligent calculator

37:33.160 --> 37:37.880
which is no desire to kill me. I have a super-intelligent chess-playing computer that is no

37:37.880 --> 37:44.040
desire to kill me. Why don't we just build, as responsible scaling, an AI that has no desires

37:44.040 --> 37:50.600
of its own? Because we don't know how to do that. Why did Homo sapiens eradicate Homer Neanderthalis

37:51.160 --> 37:56.920
and Homo erectus and all the other species that we share the planet with? You should think AGI,

37:56.920 --> 38:03.160
not of as a calculator, but as a new species on our planet. There will be a moment where humanity

38:03.160 --> 38:10.600
is no longer the only or even the most intelligent species on this planet, and we will be outcompeted.

38:11.400 --> 38:17.240
I don't think it will come necessarily from malice. I think it will be efficiency. We will build

38:17.240 --> 38:23.320
systems that make money, that are effective at solving tasks, at solving problems, at gaining

38:23.320 --> 38:28.600
power. These are what these systems are being designed to do. We are not designing systems

38:28.600 --> 38:34.760
with human morals and ethics and emotions. They're AI. They don't have emotions. We don't even know

38:34.760 --> 38:39.800
how to do that. We don't even know how emotions work. We have no idea how you could get an AI to

38:39.800 --> 38:45.080
have emotions like a human does. So what we're building is extremely competent, completely

38:45.160 --> 38:50.600
sociopathic, emotionless, optimizing machines that are extremely good at solving problems,

38:50.600 --> 38:55.080
extremely good at gaining power, that do not care about human values or emotions,

38:55.080 --> 39:00.040
never sleep, never tire, never get distracted, can work a thousand times faster than humans,

39:00.760 --> 39:07.800
and people will use these for many reasons. And eventually, I think humanity will just

39:08.440 --> 39:14.360
no longer be in control. Questions from the floor? There's a lady in the third drawer down here.

39:14.360 --> 39:20.200
Just wait for the mic, sorry, so that the audience online can hear you.

39:21.160 --> 39:27.560
Susan Finnell from Finnell Consult. To stop the arms race, certainly at a geographical level,

39:27.560 --> 39:34.600
I mean, in nuclear, the states and Europe can tell which countries are building nuclear weapons

39:34.600 --> 39:39.960
and what they've got, and they can do tests. If computing power is a thing that needs to be

39:40.040 --> 39:48.120
capped to slow this down enough, is there a way to monitor what other countries or

39:48.120 --> 39:52.040
people in a clandestine way are doing, and how does that work?

39:52.040 --> 39:59.480
This is a fantastic question, and the extremely good news is yes. At least currently,

39:59.480 --> 40:05.160
this will change in the near future, but the current state to build frontier models requires

40:05.240 --> 40:10.760
incredibly complex machines, massive supercomputers that take megawatts of energy.

40:10.760 --> 40:16.600
So this is on the order you'd have of a nuclear centrifuge facility. So these are massive,

40:16.600 --> 40:22.440
huge machines that are only built by basically three or four companies of the world. There are

40:22.440 --> 40:27.880
very, very few companies, and there is extreme bottlenecks on the supply chain. You need very,

40:27.880 --> 40:33.800
very specialized infrastructure, very specialized computer chips, very specialized hardware to

40:33.800 --> 40:38.520
be able to build these machines, and these are produced exclusively by countries basically in

40:38.520 --> 40:44.440
the West and Taiwan. There are many ways where the US or other intelligence services can and

40:44.440 --> 40:50.360
already are intervening on these supply chains, and it would be very easy to monitor where these

40:50.360 --> 40:56.440
things are going, who is buying them, where is energy being drawn on large scales. So it is not

40:56.440 --> 41:02.280
easy, and the problem is that AI is unexponential both with hardware and with software. Eventually,

41:02.280 --> 41:08.280
it will be possible to make essentially dangerous AGI on your home laptop, probably,

41:08.280 --> 41:15.080
maybe not, but it seems plausible. If we get to that world, we're in big trouble. So this is part

41:15.080 --> 41:19.800
also why we have to buy time. We have, at some point, there will be a cutoff where we'll have

41:19.800 --> 41:25.400
algorithms that are so good that either we have to stop everyone from having a PlayStation at home,

41:25.400 --> 41:32.040
which doesn't seem that plausible, or at that point we have to have very good global coordination

41:32.040 --> 41:38.120
and regulation. Thanks. Just past the mic behind you, there's a person in the row behind.

41:39.000 --> 41:46.280
Robert Whitfield from One World Trust. Can I ask about Bletchley Park? Do you know, I mean are you

41:46.280 --> 41:53.880
participating, and if not, do you know anybody else with similar views to you who is participating?

41:55.080 --> 41:58.760
I can't comment too much, since it's closed doors, it's a very private event,

41:58.760 --> 42:03.240
unfortunately, so I don't think I have my liberty to talk about exactly what I know. I think the

42:03.240 --> 42:10.120
guest list is not public. I don't know most of the people who are coming. I know the obvious ones,

42:10.120 --> 42:15.720
all the CEOs of all the top labs, of course, are attending, is not a secret. I don't know

42:16.600 --> 42:20.760
who, if anyone, of my reference class is attending.

42:23.000 --> 42:27.640
And just past the mic next to you, Robert. Thank you. Perhaps I can answer that question.

42:27.720 --> 42:35.080
Anybody that has read The Guardian today, there is an interview with Clifford,

42:36.040 --> 42:42.680
and for the very first time, not for the second time, it has been clarified that there will be

42:42.680 --> 42:50.840
only about 100 people participating on the first day. Anybody is invited, including China,

42:50.840 --> 42:55.320
on the second day, apparently there will be only the coalition of the willing.

42:56.120 --> 43:04.920
So those who subscribe to the Frontier Model Forum, they will sit on the second day,

43:04.920 --> 43:11.080
that's the current question. My main impression from that article is generally it's very positive,

43:11.080 --> 43:17.000
and I would say I've been surprised, as you would be surprised, that the UK government is really

43:17.000 --> 43:25.640
doing what it can to get the mission to what the title of the conference says, the AI safety

43:25.640 --> 43:30.520
summit. It's not about regulation, it's about controlling AI, and they're trying to do their best.

43:31.240 --> 43:39.080
The problem is, as outlined in that interview, is that we seem to be alone. We have the states

43:39.080 --> 43:45.720
a little bit, but the rest wants to go their own way and do it on their own territory, which is,

43:45.720 --> 43:51.480
I think, the tune. I agree. Sooner or later, international coordination around these issues

43:51.480 --> 43:56.440
will be necessary. It's as simple as that. If you want humanity to have a long, good future,

43:56.440 --> 44:01.320
we need to be able, as a global civilization, to handle powerful technologies like this.

44:02.040 --> 44:03.560
Take a question right from the back.

44:07.240 --> 44:13.080
In terms of legislation, what kind do you think was most effective? I've heard, for example,

44:13.320 --> 44:19.080
liability law takes too long to actually have an effect, and compute governance

44:19.080 --> 44:26.760
generally seems to be very easy to be called totalitarian. What do you think of legislation such

44:26.760 --> 44:35.320
as models must be released with a version before pre-processing, and there'd be attacks on the

44:35.320 --> 44:39.000
number of harmful outputs done by the model before the pre-processing?

44:39.960 --> 44:45.400
I am open to many kinds of regulation per se. I would strongly disagree with this description

44:45.400 --> 44:49.960
of compute governance. This is like saying that not the private citizens not having nuclear weapons

44:49.960 --> 44:54.760
is totalitarian. I respectfully disagree. I'm quite happy that people do not have private

44:54.760 --> 45:00.440
nuclear weapons, and I do not think that people should have private AGI's. Similarly, I think

45:00.440 --> 45:05.560
liability is very promising. I think it has to be strict liability, so liability for developers

45:05.560 --> 45:10.280
rather than just users. This aligns the incentives of developers with those of wider

45:10.280 --> 45:15.320
society. The point of liability is to price in the negative externalities for the people actually

45:15.320 --> 45:21.800
causing them, so I'm a big fan of this. A third form of policy I would also suggest is a global

45:21.800 --> 45:29.000
AI kill switch. This would be a protocol where some number of countries or large organizations

45:29.000 --> 45:35.240
participate, and if some number of them decide to actually do this protocol, all major deployments

45:35.240 --> 45:40.760
of frontier models must be shut down and taken offline, and this should be tested every six

45:40.760 --> 45:47.960
months as a fire drill for five minutes to ensure full, so that hopefully we never need it, but if

45:47.960 --> 45:53.640
we do, that at least the protocol exists. Thank you very much. There are lots of hands up. Hold

45:53.640 --> 45:59.000
your questions. There will be more chance for Q&A later. Corner final remarks before we hand over

45:59.000 --> 46:04.360
to the next speaker. I want to really say that I do agree that it is very hopeful to see that

46:04.440 --> 46:09.960
the UK is trying to do things and is trying to push us forward into the good world, because what

46:09.960 --> 46:18.040
we really need, as I said briefly, what we need is as a civilization to mature enough to be able

46:18.040 --> 46:23.960
to handle dangerous technology. Even if we don't build AI right now, at some point we will build

46:23.960 --> 46:29.640
something so powerful that it can destroy everything. It's just a matter of time. Our technology

46:29.640 --> 46:35.240
keeps becoming more powerful. The only way for us to have a long-term good future is to build

46:35.240 --> 46:43.160
the institutions, the civilization, the world that can handle this, that cannot build such things,

46:43.160 --> 46:48.920
that cannot hold the trigger. I do think this is possible. I do think that it is, in fact,

46:49.480 --> 46:56.200
so I have heard, in the interest of most people to not die. I think there is a natural coalition here,

46:56.200 --> 47:00.600
but it is hard, and I will not deny this is an extremely challenging problem,

47:00.600 --> 47:05.080
almost unlike, I mean, basically something we haven't faced in this nuclear proliferation,

47:05.080 --> 47:09.880
and even then it's even worse this time. It's an incredibly difficult problem. It is not over yet,

47:10.760 --> 47:16.040
but it could be very soon. If we don't act, if we let ourselves get distracted, if we fall for

47:16.040 --> 47:22.600
propaganda, and all these things, these opportunities can be gone, and that will be it. But the game

47:22.680 --> 47:25.640
is not over yet, so let's do it. Thank you very much.

47:34.360 --> 47:39.240
So we've heard from a politician, a senior politician. We've heard from a technology

47:39.240 --> 47:46.040
entrepreneur and activist. We're now going to hear from a professor who is zooming in

47:46.040 --> 47:51.400
all the way from Kentucky from the University of Louisville. He's an expert. He's written several

47:51.400 --> 47:57.880
books on cybersecurity, computer science, and artificial superintelligence. Ah, Roman,

47:57.880 --> 48:04.280
I see you on the screen. I hope you're hearing us. Tell us, can we control superintelligence?

48:05.000 --> 48:09.560
Over to you. No. The answer is no. I'll tell you why in a few minutes.

48:13.720 --> 48:17.720
That's fine. So you can share your slides or talk to us whenever you're ready.

48:18.600 --> 48:24.600
Let's do the slides. Connor did a great job with his

48:26.680 --> 48:29.480
presentation. Let me see one second here.

48:33.400 --> 48:36.520
In the meantime, we can see the covers of some of your books in the background.

48:37.400 --> 48:37.960
Yes, absolutely.

48:38.680 --> 48:42.040
Safety and security and artificial superintelligence.

48:45.000 --> 48:47.880
We're now having a slight technical issue as the

48:48.600 --> 48:54.840
technologist is found to slides. Great. Okay. Yeah, that's the hardest part. If I can get slides going,

48:54.840 --> 49:00.200
the rest is easy. Okay, so I didn't know what Connor's going to talk about.

49:02.120 --> 49:06.360
He did a great job. He's a deep thinker and covered a lot of important material.

49:07.000 --> 49:14.600
I will cover some of the same material, but I will have slides. And I will slightly

49:14.600 --> 49:20.680
take it to the next level where I may make Connor look like an optimist. So let's see how that goes.

49:22.840 --> 49:29.640
To begin with, let's look at the past. Well over a decade ago, predictions were made

49:29.640 --> 49:36.040
about the state of AI based on nothing but compute power. Ray Kurzweil essentially looked at this

49:36.440 --> 49:43.000
scalability hypothesis before it was known as such and said by 2023, we will have

49:43.000 --> 49:49.240
computational capabilities to emulate one human brain. By 2045, we would be able to

49:49.240 --> 49:56.200
do it for all of humanity. So we are in 2023. Let's look at what we can do in the present.

49:58.440 --> 50:02.680
In the spring of this year, a program was released, which I'm sure many of you got to

50:02.680 --> 50:10.600
play with called GPT-4, which is not a general intelligence, but it performs at a level

50:10.600 --> 50:17.000
superior to most humans in quite a few domains. If we look specifically at this table of different

50:17.000 --> 50:25.240
exams, lower exams, medical exams, AP tests, GRE tests, it's at 98, 99th percentile of performance

50:25.800 --> 50:34.760
for many of them, if not most. That is already quite impressive and we know that there are models

50:34.760 --> 50:41.800
coming around, which are not just text models, but multi-model large models, which will overtake

50:41.800 --> 50:49.160
this level of performance. It seems like GPT-4 was stopped in its training process right around this

50:50.120 --> 50:57.080
human capacity and if we were to train the next model, GPT-5, if you will, will quickly go into

50:57.080 --> 51:02.680
the superhuman territory and by the time the training run is done, we would already be

51:03.560 --> 51:09.960
dealing with superintelligence out of the box. But let's see what the future holds according to

51:10.360 --> 51:19.880
heads of top labs prediction markets. So we heard from CEO of Entropic, CEO of DeepMind.

51:19.880 --> 51:25.160
They both suggest that within two or three years we will have artificial general intelligence,

51:25.160 --> 51:32.680
meaning systems capable of doing human beings can do in all those domains, including science and

51:32.760 --> 51:40.120
engineering. It's possible that they are overly optimistic or pessimistic, depending on your

51:40.120 --> 51:45.960
point of view. So we can also look at prediction markets. I haven't grabbed the latest slide,

51:45.960 --> 51:53.160
but last time I looked, prediction markets also had three to four years before artificial general

51:53.160 --> 52:01.640
intelligence, which is very, very quick. Why is this a big deal? This technology at the level of

52:01.720 --> 52:06.920
human capability means that we can automate a lot of dangerous malevolent behaviors,

52:06.920 --> 52:13.880
such as creating biological pandemics, new viruses, nuclear wars. And that's why we see

52:13.880 --> 52:21.800
a lot of top scholars, influential business people, in fact, thousands of computer scientists all

52:21.800 --> 52:29.000
signed this statement saying that, yes, AI will be very, very dangerous and we need to take it

52:29.720 --> 52:37.560
with the same level of concern as we would nuclear war. So what is the problem everyone is concerned

52:37.560 --> 52:47.160
about? The problem is that, for one, we don't agree on what the problem is. Early in computer science,

52:47.160 --> 52:53.240
early in the history of AI, concerns were about AI ethics. How do we make software which is ethical

52:53.240 --> 52:58.520
and moral? And there was very little agreement. Nobody solved anything, but everyone proposed

52:58.520 --> 53:03.160
their own ethical system, gave it a name and described what they had in mind.

53:04.600 --> 53:09.400
About a decade ago, we started to realize that ethics is not enough. We need to look at safety

53:09.400 --> 53:16.120
of those systems. So again, we started this naming competition. We had ideas for friendly AI, control

53:16.120 --> 53:21.960
problem, value alignment. It doesn't really matter what we call it. We all intuitively kind of

53:21.960 --> 53:27.640
understand we want a system which, if we run it, we will not regret running it. It will be beneficial

53:27.640 --> 53:34.760
to us. So how can humanity remain safely in control while benefiting from superior form

53:34.760 --> 53:40.600
of intelligence is the problem? I would like us to look at. We can call it control problem

53:40.600 --> 53:46.040
and the state of the art in this problem. In fact, we don't really know if the problem is even

53:46.040 --> 53:51.800
solvable. It may be partially solvable, unsolvable. Maybe it's a silly question and the problem is

53:51.800 --> 54:01.880
undecidable. A lot of smart people made their judgments known about this problem. Unfortunately,

54:01.880 --> 54:09.960
there is little agreement. Answers range from definitely solvable, from a surprising source,

54:09.960 --> 54:18.600
like Eliezer Ytkovsky, to very tractable, from head of superalignment team at one of the top labs,

54:18.600 --> 54:25.400
to, I have no idea, from a top tuning award winner who created much of machine learning

54:25.400 --> 54:31.080
evolution. So I think it's an important problem for us to look at, to address, and to understand

54:31.640 --> 54:38.920
how we can best figure out what is the status of the problem. My approach to that is to think

54:38.920 --> 54:45.960
about the tools I would need to control a system like that, an intelligent, very capable AI. And

54:46.680 --> 54:53.400
the tools I would guess I would need, ability to explain how it works, capability to comprehend

54:53.400 --> 54:59.960
how it works, predict its behavior, verify if the code follows design, be able to communicate

54:59.960 --> 55:04.440
with that system. And probably some others, but maybe some of the tools are interchangeable.

55:05.560 --> 55:11.720
So I did the research and I published results on each one of those tools. And the results are not

55:11.720 --> 55:17.400
very optimistic. For each one of those tools, there are strong limits to what is capable

55:17.400 --> 55:23.400
in the worst case scenarios. When we're talking about superintelligent systems, self-improving

55:23.400 --> 55:28.840
code systems, smarter than human, capable of learning in new domains, it seems that there are

55:28.840 --> 55:34.920
limits to our ability to comprehend those systems, offer those systems to explain their behavior.

55:34.920 --> 55:41.080
The only true explanation for an AI model is the model itself. Anything else is a simplification.

55:41.080 --> 55:48.440
You are getting a compressed, lossy version of what is happening in a model. If a full model is

55:48.440 --> 55:53.000
given, then you of course would not comprehend it because it's too large, too complex, it's not

55:53.000 --> 55:59.400
surveyable. So there are limits to what we can understand about those black box models.

56:00.120 --> 56:05.800
Similarly, we have limits to predicting capabilities of those systems. We can predict

56:05.800 --> 56:10.360
general direction in which they are going, but we cannot predict specific steps for how

56:10.360 --> 56:15.480
they're going to get there. If we could, we would be as intelligent as those systems. If you're playing

56:15.480 --> 56:20.040
chess against someone and you can predict every move they're going to make, you're playing at the

56:20.040 --> 56:26.600
same level as that opponent. But of course, we made an assumption that a superintelligent system

56:26.600 --> 56:34.040
would be smarter than us. There are similar limits to our ability to verify software. At best, we can

56:34.040 --> 56:39.560
get additional degree of verification for the amount of resources contributed. So we can make

56:39.560 --> 56:45.800
systems more and more likely to be reliable, to have less bugs, but we never get to a point of

56:45.800 --> 56:51.320
100% safety and security. And I'll explain why that makes a difference in this domain.

56:51.960 --> 56:59.000
Likewise, human language is a very ambiguous language. It's not even as unambiguous as computer

56:59.000 --> 57:06.120
programming languages. So we are likely to make mistakes in giving orders to those systems.

57:07.080 --> 57:12.840
All of it kind of leads us to conclude that it will not be possible to indefinitely control

57:12.840 --> 57:18.920
superintelligent AI. We can trade capabilities for control, but at the end, if we want very,

57:18.920 --> 57:23.320
very capable systems, and this is what we're getting with superintelligence,

57:23.320 --> 57:29.400
we have to surrender control to them completely. If you feel that the impossibility results I

57:29.400 --> 57:35.160
presented were just not enough, we have another paper where we cover about 50 of those impossibility

57:35.240 --> 57:41.160
results. It's a large survey in a prestigious journal of ACM surveys.

57:42.920 --> 57:48.520
From the beginning of history of AI with founding fathers like Alan Turing who said

57:49.160 --> 57:55.400
that he expects the machine will take over at some point to modern leaders of AI like Elon

57:55.400 --> 58:06.120
Musk who says we will not control them for sure. There is a lot of deep thinkers, philosophers,

58:06.120 --> 58:14.200
who came to that exact conclusion. We are starting to see top labs publish reports

58:14.200 --> 58:21.000
in which they may gently acknowledge such scenarios. They call them pessimistic scenarios

58:21.000 --> 58:28.360
where the problem is simply unsolvable. We cannot control superintelligence. We cannot

58:28.360 --> 58:33.480
control it indefinitely. We are not smart enough to do it, and it doesn't even make sense that

58:33.480 --> 58:41.080
that would be a possibility. They ask, well, what's the distribution? What are the chances

58:41.080 --> 58:48.280
that we're in a universe where that's the case? They don't provide specific answers, but it seems

58:49.000 --> 58:54.680
from some of the writing and posts they make, maybe about 15 percent is allocated to that

58:54.680 --> 59:01.080
possibility. I was curious to see what other experts think, so I made a very small, very

59:01.080 --> 59:08.200
unscientific survey on social media. I surveyed people in my Facebook group on AI safety,

59:08.200 --> 59:15.960
and I surveyed my followers on Twitter, and it seems that about a third think that the problem

59:15.960 --> 59:21.240
is actually solvable. Everyone else thinks it's either unsolvable or it's undecidable,

59:21.240 --> 59:27.000
or we can only get partial solutions or we will not solve it on time. So that's actually an

59:27.000 --> 59:32.120
interesting result. Most people don't think we can solve this problem, and I think part of the

59:32.120 --> 59:37.720
reason they think we cannot solve this problem is because there is a fundamental difference between

59:38.600 --> 59:46.680
kind of standard cybersecurity safety and superintelligence safety. And cybersecurity,

59:46.680 --> 59:53.400
even if you fail, it's not a big deal. You can issue new passwords, you can provide someone with

59:53.400 --> 59:59.160
a new credit card number, and you get to try again. We suspect strongly with superintelligent

59:59.160 --> 01:00:05.720
safety, you only get one chance to get it right. There are unlimited dangers and limited damages,

01:00:05.720 --> 01:00:13.960
either you have existential risks or suffering risks. And we kind of agree that 100% is not an

01:00:13.960 --> 01:00:21.960
attainable level of security verification safety, but anything less is not sufficient.

01:00:21.960 --> 01:00:27.560
If a system makes a billion decisions a minute and you only make mistake once every couple

01:00:27.560 --> 01:00:33.560
billion decisions, after a few minutes you are dead. And so this is like creating a perpetual

01:00:33.560 --> 01:00:39.000
motion machine. You are trying to design perpetual safety machine while they keep releasing more and

01:00:39.000 --> 01:00:47.320
more capable systems, GPT-5, GPT-50. At some point this game is not going to end in your favor.

01:00:48.360 --> 01:00:54.760
So I'm hoping that others join me in this line of research. We need to better understand what are

01:00:54.760 --> 01:01:01.240
the limits to controlling superintelligence systems. Is it even possible? My answer is no,

01:01:01.240 --> 01:01:07.800
but I would love to be proven wrong. It would be good to have surveys similar to the ones I

01:01:07.800 --> 01:01:13.080
conducted on a larger scale to get much more statistically significant results.

01:01:13.800 --> 01:01:20.520
And in case we do agree that we have this worst case scenario where we are creating

01:01:20.520 --> 01:01:26.520
superintelligence and it is impossible to control it, what is our plan? Do we have a plan of action

01:01:26.520 --> 01:01:32.520
for this worst case scenario? This is what I wanted to share with you and I'm happy to answer

01:01:32.520 --> 01:01:35.480
any questions. Thank you very much, Roman.

01:01:45.640 --> 01:01:46.760
Optimistic, Roman.

01:01:49.560 --> 01:01:56.200
Sorry, one second. I'm trying to figure out how to use Zoom. Go ahead and repeat your question

01:01:56.200 --> 01:02:03.400
please. You gave us many reasons to be anxious. What do you think is the best reason for us to be

01:02:03.400 --> 01:02:09.880
optimistic? Well, there seems to be many ways we can end up with World War 3 recently, so

01:02:09.880 --> 01:02:11.720
that can slow down some things.

01:02:15.400 --> 01:02:22.200
It has been suggested that we can use a different kind of tool, which is the kill switch. Your

01:02:22.200 --> 01:02:27.800
list of tools that you listed, it didn't include that. It's been proposed that each AI system should

01:02:27.800 --> 01:02:35.080
be tested with a remote off switch capability. Have you looked at that? Do you think that's a viable

01:02:35.080 --> 01:02:43.800
option? So I would guess a superintelligent system would outsmart our ability to press the off button

01:02:43.880 --> 01:02:52.280
in time. It will work for not superintelligent AI, pre-GI systems, maybe even for the GI systems,

01:02:52.280 --> 01:02:57.880
but the moment it becomes that much more advanced, I think it will outsmart us. It will take over

01:02:57.880 --> 01:03:02.360
any kill switch options we have. Let's have some questions from the floor.

01:03:06.120 --> 01:03:10.040
I can't see the hands, so yes, just give the microphone out. Thank you.

01:03:11.000 --> 01:03:17.480
Thank you. I would like to ask, how does the scalable oversight that open AI is working on,

01:03:17.480 --> 01:03:23.880
essentially the way they plan to align superintelligence, fit into your expectation of the

01:03:23.880 --> 01:03:31.000
future pathway the AGI will take? Because again, as personally, we cannot align or control a

01:03:31.000 --> 01:03:36.840
superintelligent entity, but another AI which is more capable than us could. So how does that fit

01:03:36.840 --> 01:03:43.080
into your expectations? So it seems like it increases complexity of the overall system,

01:03:43.080 --> 01:03:48.760
instead of us trying to control one AI. Now you're trying to control a chain of agents going from

01:03:48.760 --> 01:03:54.360
slightly smarter to smarter to superintelligent, maybe 50 agents in between, and you're saying that

01:03:54.360 --> 01:04:00.280
you have to solve alignment problem between all the levels, communication problem, ambiguity of

01:04:00.280 --> 01:04:07.240
language between all those models, supervision. It seems like you're trying to get safety by

01:04:07.240 --> 01:04:12.520
kind of upthuscating how the model actually works. You're introducing more complexity,

01:04:12.520 --> 01:04:17.000
hoping to make the system easier to control that seems counterintuitive.

01:04:17.880 --> 01:04:22.360
But isn't it the case that sometimes you can verify an answer without understanding the

01:04:22.360 --> 01:04:26.920
mechanism by which the answer was achieved? For example, there can be a chess puzzle,

01:04:26.920 --> 01:04:30.680
and you know a way of working out yourself, but when somebody shows you the answer, you can say,

01:04:30.680 --> 01:04:35.720
oh yes, this is the answer. Isn't it possible? We don't need to really understand what's going

01:04:35.720 --> 01:04:41.160
on inside these systems, but a simpler AI can at least verify the recommendations that come out of

01:04:41.160 --> 01:04:48.280
the more complex AIs. So such a chain may be the solution. Can you claim that you are still in control

01:04:48.280 --> 01:04:52.440
if you don't understand what's happening and somebody just tells you don't worry, it's all

01:04:52.440 --> 01:05:00.200
good I checked it for you? But then it's like we humans, we have a network of trust. When I trust

01:05:00.200 --> 01:05:06.120
some people and they trust others within various categories, we can't work out everything ourselves,

01:05:06.120 --> 01:05:13.240
but we trust some scientists or some engineers or some lawyers who validate that an AI has a certain

01:05:13.240 --> 01:05:19.000
level of capability and that AI could come back with verification that the proposals of a super

01:05:19.000 --> 01:05:24.520
intelligence should be accepted or should not be. I don't say it's easy, but as you said there's

01:05:24.520 --> 01:05:30.680
not likely to be a very simple and straightforward solution. Again, to me at least it sounds like

01:05:30.680 --> 01:05:36.920
instead of trying to make this system safe, you said that you made some other system safe and

01:05:36.920 --> 01:05:43.480
it made sure that the system you could make safe is safe for you. Let's take some more questions.

01:05:43.720 --> 01:05:48.360
There's another one in the middle here. Then we'll go to the edge. Yes, thank you.

01:05:50.520 --> 01:05:56.200
Thank you for the presentation. Number one, second thing is that as you're talking about,

01:05:56.200 --> 01:06:01.400
I think as David was talking about trust basically, could you tell me from your

01:06:01.400 --> 01:06:07.160
extensive years of AI research and experience as such that do you really think that humans

01:06:07.160 --> 01:06:14.760
or society can be trusted to, for example, regulate its own self or do you think that

01:06:14.760 --> 01:06:22.280
really need some sort of institution of sort that is totally separate from anyone else?

01:06:24.760 --> 01:06:31.800
So I'm not sure regulation would be enough. Connor correctly pointed out that there is

01:06:31.800 --> 01:06:39.560
both lobbying of regulators by the labs and also it becomes easy and easier to train those models

01:06:39.560 --> 01:06:45.880
with less compute and over time you will be able to do it with very little resources. The only

01:06:45.880 --> 01:06:53.960
way forward I see is personal self-interest. If you are a rich young person and you think this is

01:06:53.960 --> 01:06:59.000
going to kill you and everyone else, maybe it's not in your best interest to get there first.

01:06:59.000 --> 01:07:03.160
That's really the only hope at this point, just personal self-interest.

01:07:04.360 --> 01:07:08.200
The humans are always better if we can band together with our self-interest rather than each

01:07:08.200 --> 01:07:12.440
of us individually pursuing our self-interest. So I think this kind of meeting and the community

01:07:12.440 --> 01:07:19.800
spirit might help. There was a hand over here, yes, with I think the red shirt on jacket.

01:07:19.800 --> 01:07:30.760
Yeah, if we assume that the two kind of well both views that have been suggested so far are

01:07:30.760 --> 01:07:37.640
correct in that we're definitely not going to be able to stop AI development etc and we're going

01:07:37.640 --> 01:07:41.880
to get to the point where we have no regulation that can effectively stop things. You know people

01:07:41.880 --> 01:07:46.520
can build in super intelligent AI on their own computers etc okay so we'll assume that that's a

01:07:46.520 --> 01:07:52.040
fact that's coming and then we'll also assume that the control problem isn't a problem because

01:07:52.040 --> 01:07:55.000
it's a problem that can't be solved and we're definitely not going to be able to control it.

01:07:55.000 --> 01:07:59.880
Well now we're heading and barreling towards the point where we have super intelligent

01:07:59.880 --> 01:08:06.440
AIs definitely and we definitely can't control them. What comes next? What comes next?

01:08:08.200 --> 01:08:12.440
It's a wonderful question. As I said and published you cannot predict what the super

01:08:12.520 --> 01:08:21.640
intelligent system will do. All right so was there a question down here? Thank you.

01:08:24.200 --> 01:08:29.160
You said that we kind of need a plan but on that last question if that scenario is true.

01:08:30.040 --> 01:08:33.320
You said we need to do more work in this area but do you have any thoughts as to

01:08:33.880 --> 01:08:37.800
what we should be doing? What we should be doing to plan for the worst-case scenario?

01:08:38.120 --> 01:08:46.600
So to me at least it seems that at least in some cases it is possible to use this idea of personal

01:08:46.600 --> 01:08:51.960
self-interest. If you have a young person having a good life there is no reason why they need to

01:08:51.960 --> 01:08:57.640
do it this year or next year. I understand that someone may be in a position where they are very

01:08:57.640 --> 01:09:03.240
old very sick have nothing to lose and it's much harder to convince them not to try but at least

01:09:03.240 --> 01:09:10.600
from what I see the heads of those companies are all about the same age they young they healthy they

01:09:11.320 --> 01:09:18.440
they have a lot of money there is a good way to motivate them to wait a little bit maybe a decade

01:09:18.440 --> 01:09:25.240
or two just out of personal self-interest again. I think my answer to the question of optimism is

01:09:25.240 --> 01:09:31.800
that we humans can do remarkable things we humans can solve very hard problems and so I want to say

01:09:32.760 --> 01:09:38.040
now that we spread around what the problem is at least some more people can apply more brain power

01:09:38.040 --> 01:09:47.000
to it so that's my reason for optimism. Terry? I guess I'm pleased by the inevitability of this

01:09:47.000 --> 01:09:53.800
development because it seems to me that if you're going to create reasoning creatures

01:09:54.680 --> 01:10:01.480
then those reasoning creatures are going to have moral rights on the same plane as human beings

01:10:02.200 --> 01:10:11.560
so I'm looking forward to chatting with these creatures and joining in them joining into this

01:10:11.560 --> 01:10:15.800
kind of discussions and I'm pleased that they won't be able to be thwarted and it will be wrong

01:10:15.800 --> 01:10:23.160
to enchain these reasoning creatures. Sir Roman are you looking forward to having more of the AIs

01:10:23.160 --> 01:10:31.400
involved in these discussions as well? I remember giving a presentation for a podcast about

01:10:32.040 --> 01:10:38.360
rights for animals rights for AIs and I was very supportive of all the arguments developed because

01:10:38.360 --> 01:10:43.640
I said at one point we will need to use those arguments to beg for our rights to be retained.

01:10:45.880 --> 01:10:55.400
The question on the third row here? Yes hi I'm curious Roman which side of in your hopes of a

01:10:55.400 --> 01:11:00.760
possible future for us to get through this do you have more hope on the side of a more top down

01:11:00.840 --> 01:11:08.520
sort of totalizing control system for AGI systems such that they remove the possibility of

01:11:08.520 --> 01:11:14.440
individual actors getting hold of this and weaponizing it or do you put more hope in a more

01:11:15.080 --> 01:11:22.120
sort of decentralized open source approach to AGI emergence more like an ecology perhaps some

01:11:22.120 --> 01:11:27.960
people suggest would be more biologically inspired such that you know immune system like functions

01:11:27.960 --> 01:11:35.400
could arise. Which way do you lean in your sensibilities for what is a viable avenue for us?

01:11:36.200 --> 01:11:42.600
I'm not optimistic with either of those options the only kind of hope I see is that for strategic

01:11:42.600 --> 01:11:48.920
reasons superintelligence decides to wait to strike it will not go for immediate treacherous

01:11:48.920 --> 01:11:54.680
turn but decides to accumulate resources and trust and that buys us a couple of decades

01:11:54.680 --> 01:11:59.960
that's the best hope I see so far. So we slow things down we'll have more chance to work out

01:11:59.960 --> 01:12:05.880
solutions and the slowing down might come from a combination of top down pressure and bottom up

01:12:05.880 --> 01:12:12.520
pressure. Maybe have a is there a hand at the very back there yes let's try and get the microphone back

01:12:12.520 --> 01:12:18.840
there. Right at the sitting at the back yes.

01:12:22.840 --> 01:12:24.040
Sorry at the in the middle.

01:12:29.880 --> 01:12:36.040
Thanks. Hi Roman thanks for your talk. Yeah I was wondering what your thoughts are on

01:12:36.600 --> 01:12:43.560
aligning the first AGI that is human level or narrowly superhuman if in principle that is possible

01:12:44.120 --> 01:12:52.920
and if that is is is it possible in principle to align the next version of AGI but to use that

01:12:52.920 --> 01:13:02.200
narrowly superhuman AGI to align it and if if that's all technically possible then why would we

01:13:02.200 --> 01:13:11.320
not think like focus on doing that and also and also if you think in principle alignment is

01:13:11.320 --> 01:13:20.440
impossible and control is impossible then why why not work on practical ways to make the

01:13:21.560 --> 01:13:27.160
to make whatever AGI is created as nice as possible that is like better than the counterfactual of

01:13:28.120 --> 01:13:35.560
try to stop it it won't stop and you know it won't be nice. Well I definitely encourage everyone

01:13:35.560 --> 01:13:42.360
to work on as much safety as you can anything helps I would love to be proven wrong it would

01:13:42.360 --> 01:13:47.960
be my greatest dream that I'm completely wrong and somebody comes out and says here's a mistake in

01:13:47.960 --> 01:13:53.880
your logic and we have developed this beautiful friendly safe system capable of doing all this

01:13:53.880 --> 01:13:59.800
beneficial things for humanity that would be wonderful but so far I haven't seen any progress

01:13:59.800 --> 01:14:04.760
in that direction what we're doing right now is putting lipstick on this monster and the

01:14:04.760 --> 01:14:10.840
shadow that's all we're doing filters to prevent the model from disclosing its true intentions

01:14:10.840 --> 01:14:17.000
when you talk about alignment it's not a very well-defined terms what values are you aligning

01:14:17.000 --> 01:14:23.960
it with values of heads of that lab values of specific programmer we as humans don't agree on

01:14:23.960 --> 01:14:29.960
human values that's why we have all these wars and conflicts there is a 50-50 split and most

01:14:29.960 --> 01:14:36.760
political issues in my country we are not very good at agreeing even with ourselves over time

01:14:36.760 --> 01:14:43.160
what I want today is not what I wanted 20 years ago so I think this idea of being perfectly

01:14:43.160 --> 01:14:49.080
aligned with 8 billion agents and people are suggesting adding animals to it and aliens and

01:14:49.080 --> 01:14:54.680
other AIs that doesn't seem like it's a workable proposal our values are changing

01:14:55.400 --> 01:15:02.120
they're not static and it's very likely that they will continue changing after we get those

01:15:02.120 --> 01:15:08.760
systems going I don't see how at any point you can claim that the system is specifically value

01:15:08.760 --> 01:15:15.000
aligned with someone in particular the last question in this section is going to go to Connolly

01:15:17.000 --> 01:15:21.640
Roman love your talk I always love your optimism it's always great to hear you talk so

01:15:23.800 --> 01:15:28.440
so I'm kind of like gonna pick up on the question I was just asked and just give a bit of my opinion

01:15:28.440 --> 01:15:35.080
and kind of like hear what you think about this as well so my personal view is that I do I have

01:15:35.080 --> 01:15:41.720
read many of your papers in fact and they're quite good so I do think that I agree with you that like

01:15:41.720 --> 01:15:47.720
in principle an arbitrarily intelligent system cannot be safe by any arbitrary like weaker system

01:15:47.720 --> 01:15:54.840
just kind of by proof of like you know program size induction and whatnot but in my view it does

01:15:54.840 --> 01:16:03.000
seem likely that there is a limit of intelligence far below the theoretical optimum but still

01:16:03.000 --> 01:16:09.560
significantly above the human level that can be achieved the reason I think this is that

01:16:10.280 --> 01:16:15.880
human civilization is actually very smart compared to a single caveman and can do really really great

01:16:15.880 --> 01:16:23.160
things so my point of optimism is it seems possible that if we stop ourselves from making

01:16:23.160 --> 01:16:28.200
self-improving systems and coordinate at a very strong scale and have very strong enforcement

01:16:28.200 --> 01:16:34.600
mechanisms it should be possible to build systems that are you know end steps you know above human

01:16:34.600 --> 01:16:41.320
good enough to build you know awesome you know sci-fi culture ship kind of like worlds but not

01:16:41.320 --> 01:16:50.600
further I'm wondering if you have an intuition about like where do things hit impossibilities

01:16:50.600 --> 01:16:57.880
like to me I think the impossibilities happen above human utopia but to get to the utopia put

01:16:58.200 --> 01:17:03.160
you already have to do extremely strong coordination extremely strong safety research

01:17:03.160 --> 01:17:07.400
extremely strong interpretability extremely strong constraint on the design of the agis

01:17:07.400 --> 01:17:10.840
extremely strong regulation which I think is in principle possible wondering kind of like your

01:17:10.840 --> 01:17:15.880
thoughts about that kind of outcome so Conor's not asking about responsible scaling he's asking

01:17:15.880 --> 01:17:20.760
about limited superintelligence if we had limited superintelligence could we get everything we want

01:17:21.400 --> 01:17:26.920
without having the risks that we all fear so I think I want to emphasize difference between

01:17:26.920 --> 01:17:32.280
safety and control is it possible to create a system which will keep us safe and some

01:17:32.920 --> 01:17:39.400
somewhat happy state of preservation possible a way in control no that system is the example you

01:17:39.400 --> 01:17:44.920
give of humanity so humanity provides pretty nice living for me but I'm definitely not in control

01:17:44.920 --> 01:17:51.080
if I disagree with society and many issues in politics and culture it makes absolutely no

01:17:51.080 --> 01:17:57.880
difference I don't decide things scale it to the next level all 8 billion of us may want something

01:17:57.880 --> 01:18:02.680
but this overseer this more intelligent system says it's not good for you we're not going to do it

01:18:03.640 --> 01:18:08.280
this is what you're going to be doing right now so think about all the decisions you make throughout

01:18:08.280 --> 01:18:14.600
your day you decided to eat this donut you smoke this cigarette all those decisions were made by

01:18:14.600 --> 01:18:19.560
you because you felt you wanted to do them they may be good or bad decisions but if you had this

01:18:19.560 --> 01:18:25.400
much more intelligent personal advisor ideal advisor you would be at the gym working out eating

01:18:25.400 --> 01:18:31.640
carrots you may have a long healthy life but you're not in control and your happiness level

01:18:32.280 --> 01:18:39.720
may be questionable thank you very much roman for sharing your thoughts pessimism and some

01:18:39.720 --> 01:18:44.680
optimism thanks for moving the conversation forwards

01:18:54.040 --> 01:18:59.560
I'm now going to invite the five members of the panel to come up on stage and they're each going

01:18:59.560 --> 01:19:05.320
to have a couple of chances to pass some comments on what they've heard so there's some stairs over

01:19:05.320 --> 01:19:14.360
there that you can come up to we're going to hear from jan tallan who is the co-founder of skype

01:19:14.360 --> 01:19:21.240
the co-founder of fli future of life institute and also cesar the center for study of existential

01:19:21.240 --> 01:19:28.040
risks we're going to hear from eva barons a policy analyst with the international center for future

01:19:28.040 --> 01:19:33.400
generations we're going to hear from tom ohl who's a journalist who writes from time to time for the

01:19:33.400 --> 01:19:43.480
bbc amongst other places we're going to hear from alexandra musa visadievic who is the ceo of evident

01:19:43.480 --> 01:19:49.240
and has a track record with tortois media in many other places and we're going to hear from also

01:19:49.240 --> 01:19:56.200
another representative from conjecture that's andrea miotti who is their specialist for ai policy

01:19:56.200 --> 01:20:01.480
and governance so to start things let's just hear from each of them if you if you're opening remarks

01:20:01.560 --> 01:20:06.200
jan what's your comments from what you've heard so far have you changed your mind in any ways or

01:20:06.200 --> 01:20:11.080
other things that are missing from the conversation yeah you all have to speak into the mics i'm

01:20:11.080 --> 01:20:19.480
being told so i yesterday i was at the dinner i was invited to a dinner and and my response to

01:20:19.480 --> 01:20:26.600
an invitation was that okay i will come but you have to invite connor because he's making very

01:20:26.600 --> 01:20:33.800
similar points to me only much much more intensely so yeah i basically agree i agree with what what

01:20:33.800 --> 01:20:40.280
connor said uh my main caveat would be that for the last decade or so i've been kind of trying to build

01:20:41.960 --> 01:20:52.680
a lot of friendly cooperation between people in the kind of ai companies and making sure that

01:20:53.480 --> 01:20:57.400
everybody can understand that it isn't in their interests with almost everybody

01:20:59.320 --> 01:21:03.240
let's be honest almost everybody understands that is in their interests to

01:21:03.880 --> 01:21:10.360
and of remaining control and and not kill everyone else and so

01:21:11.960 --> 01:21:17.880
like for example i am a board observer observer to entropic and entropic is one of those companies

01:21:17.880 --> 01:21:21.960
just like conjecture when you go there you can talk to anyone from the receptionist

01:21:22.360 --> 01:21:27.080
to the to the ceo and they are aware of the ai risk i'm very concerned about this

01:21:27.800 --> 01:21:36.120
but yes i do think as i've said in several places that i don't think they should be doing what they're

01:21:36.120 --> 01:21:41.320
doing right so these companies don't really want to do what they're doing but they feel they have

01:21:41.320 --> 01:21:47.480
to otherwise they might be left behind so yes so there is this uh product basically dilemma in

01:21:47.480 --> 01:21:54.520
when you want to do you know safe ai one is that you're safe like when you're trying to figure

01:21:54.520 --> 01:22:01.480
out how to do safe ai from one hand you have groups like miri uh that the aliases with kowski

01:22:01.480 --> 01:22:08.280
co-founded and that was the person who got me involved in ai safety uh 15 16 years ago uh

01:22:08.280 --> 01:22:12.360
where basically the claim is that you have to start really early even if you don't know exactly

01:22:12.360 --> 01:22:18.600
what the ai is going to look like because then you have a lot of time to prepare and then the group

01:22:18.600 --> 01:22:23.640
on the other end of that axis is entropic where they say that it's kind of useless to start early

01:22:23.640 --> 01:22:28.040
because you don't know what you're dealing with uh so you need to be as informed as possible

01:22:28.040 --> 01:22:32.600
so in that strategy you need to be just always at the frontier and and dario has been very public

01:22:32.600 --> 01:22:37.400
about this about this strategy of course the problem there is that like it also works as a

01:22:37.400 --> 01:22:44.200
perfect justification to race right so so uh therefore it's uh i have like double digit uncertainty

01:22:44.200 --> 01:22:50.520
both ways uh about what is what the actual picture is and so i do think that this point

01:22:50.520 --> 01:22:55.000
the labs indeed they are involved in death race and they they there is the government

01:22:55.000 --> 01:22:59.960
intervention needed uh to to get a time out there and we definitely need time out because we don't

01:22:59.960 --> 01:23:10.440
have uh enough safety results uh and but to yeah romanial boski's uh presentation i call i'm definitely

01:23:10.440 --> 01:23:15.640
more optimistic again as as on one of these slides there was the dialogue here with eliezer and

01:23:15.640 --> 01:23:22.280
the eliezer was confident that this can be sold and in fact like i'm super glad that earlier this year

01:23:22.440 --> 01:23:29.640
uh david tarimble uh his group got uk government funding and he has this approach called

01:23:30.600 --> 01:23:36.680
open agency architecture i don't know exactly uh what the details there are but like my rough

01:23:36.680 --> 01:23:45.480
understanding is that you you're using uh you're scaling ai capabilities and access uh according

01:23:45.480 --> 01:23:53.240
to formal statements that the ai's produce and then you use not ai not humans but formal verifiers

01:23:53.240 --> 01:24:00.920
to to verify uh those those statements therefore like building up your kind of ai capabilities

01:24:00.920 --> 01:24:05.400
one formally verified step at the time uh there are many criticism of that but it's like one of

01:24:05.400 --> 01:24:13.880
those approaches that is kind of at least and principle uh has like some convincing story

01:24:13.880 --> 01:24:18.440
that that why it should work in in principle at least so there are some options that might work

01:24:18.440 --> 01:24:22.920
but we're going to need time to develop them exactly so that's why like i've been working on

01:24:22.920 --> 01:24:27.560
like i've been supporting ai i've taken research for more than a decade now but unfortunately we

01:24:27.560 --> 01:24:32.920
just didn't make it uh we only need more by more time so let's hear from either because you work

01:24:32.920 --> 01:24:40.920
more with possibilities to inspire policy you've seen examples of policy in the past slowing down

01:24:40.920 --> 01:24:47.400
some technological races are you do you see reasons for optimism do you see ways in which

01:24:47.400 --> 01:24:52.920
politicians can make a good difference to the landscape we're discussing definitely definitely

01:24:52.920 --> 01:24:57.960
that very much plays into some of the problems or the issues characteristics of the problem that

01:24:57.960 --> 01:25:02.680
both corner spoke about and that also yantan you just mentioned that one of the problems that we're

01:25:02.680 --> 01:25:07.000
facing here is a human coordination problem and one of the ways to address that will be through

01:25:07.000 --> 01:25:12.040
policy as has been said many times this evening this is a technology that threatens to kill us

01:25:12.040 --> 01:25:17.800
to kill us all um and the the heads of the government of the companies that are driving

01:25:17.800 --> 01:25:22.280
forward the technology have agreed that and publicly stated that that might be the case

01:25:22.280 --> 01:25:27.160
and yet they seem to be locked into this dilemma that yantan just mentioned where they are for some

01:25:27.160 --> 01:25:32.200
or other reason impossible to to stop so i think that is a point where where government can really

01:25:32.200 --> 01:25:38.520
make a difference and step in and also should step in and we've seen that as you hinted at we've

01:25:38.520 --> 01:25:44.120
seen that work in the past one of the examples that i often think about is the montreal protocol

01:25:44.120 --> 01:25:51.880
which after the scientific consensus arose that cfcs and other similar gases actually destroy

01:25:51.880 --> 01:25:58.200
the ozone layer the international community did come together in 1987 and agreed through the

01:25:58.200 --> 01:26:05.240
montreal protocol to slowly phase out these gases so we see here that international cooperation

01:26:05.240 --> 01:26:10.520
by the international community by governments can succeed also in the face of the short-term

01:26:10.520 --> 01:26:16.520
economic interest of private sector companies in the public interest of well in the end in

01:26:16.520 --> 01:26:21.560
everyone on earth so i'm not saying that it's necessarily easy but i think it's definitely

01:26:21.560 --> 01:26:26.760
possible and it is one of the strongest levers that we have here to make a difference so i think

01:26:26.760 --> 01:26:31.880
that's definitely something that we should lean into very strongly and do our best that

01:26:31.880 --> 01:26:37.960
that actually happens the montreal protocol is an encouraging example but we haven't made a very good

01:26:37.960 --> 01:26:42.360
job of the governments in the world of controlling carbon emissions we've been talking about it for

01:26:42.360 --> 01:26:47.080
a long long time and maybe there's some progress but many people feel this is an example where

01:26:47.080 --> 01:26:52.520
governments can't cooperate so what makes you think that we can cooperate with the problems of AI

01:26:52.520 --> 01:26:58.760
more like the montreal protocol rather than the paris agreement to say well part of this of course

01:26:58.760 --> 01:27:02.280
is also that i think that this is one of the few levers that we have to make a difference

01:27:02.280 --> 01:27:07.400
at all so i also hope that we will be able to do it and i agree that looking at past climate

01:27:07.400 --> 01:27:12.760
conference is one of the negative examples that we see there is that with these conferences

01:27:12.760 --> 01:27:17.800
sometimes that the outcomes tend to be very watered down just because the the focus lies on

01:27:17.800 --> 01:27:22.440
building consensus among all of the different countries that attend and then in the end you

01:27:22.440 --> 01:27:26.680
want to have a nice little consensus agreement that everyone signs so you can demonstrate that

01:27:26.680 --> 01:27:31.160
everyone's on the same page and everyone goes home and everyone's happy happy and i can just say that

01:27:31.160 --> 01:27:35.880
i think with the ukai summit that's coming up now first of all that is a unique opportunity to

01:27:35.880 --> 01:27:40.680
actually have international cooperation and coordination on this issue take place you need

01:27:40.680 --> 01:27:44.920
to create the opportunities for stuff like that i'm really happy that the uk government took

01:27:45.480 --> 01:27:50.200
the initiative and created this opportunity i am one thing that makes me optimistic is

01:27:50.200 --> 01:27:54.840
that we all know that china is going to attend at least on one of the days so hopefully they

01:27:54.840 --> 01:27:59.880
will be able to be brought into the fold and yeah then i just hope that this opportunity is truly

01:27:59.880 --> 01:28:04.840
taken and that the outcome of this summit will not be just some vague commitments to long-term

01:28:04.840 --> 01:28:13.320
plans but ideally concrete binding commitments to to concrete next steps let's turn to alexandra

01:28:14.040 --> 01:28:20.840
you work a lot with businesses businesses are unsure in many ways how to deal with today's ai

01:28:20.840 --> 01:28:26.280
do you think there is good advice that they can be given or is there a sleepwalking process with

01:28:26.280 --> 01:28:35.000
many of our businesses definitely the latter i would say um so i i'm seo of evident we map

01:28:35.720 --> 01:28:43.240
benchmark companies on how far they are in their ai adoption and so when um i think it was kind of

01:28:43.320 --> 01:28:50.280
you mentioned the ai race that is on the front line of development um in ai there is also a race

01:28:50.280 --> 01:28:57.400
as we all know going on in terms of adopting ai as as quickly as possible there's a sense of being

01:28:57.400 --> 01:29:06.520
there's a sort of geopolitical debate on ai development between us europe china and so on um

01:29:06.520 --> 01:29:11.640
and who's leading on that not only in ai but also in areas like quantum but in the business level

01:29:11.640 --> 01:29:17.720
which is where i deal with um spend my time uh mostly there's a definitely a race on in terms of

01:29:17.720 --> 01:29:24.920
not being left behind in adoption of ai and it's an economic question it is a existential question

01:29:24.920 --> 01:29:31.800
so there's an existential question on sort of two dimensions in this debate and um and so you've got

01:29:31.800 --> 01:29:39.960
this um unstoppable race going on on the front end of ai and then you've got an unstoppable race on

01:29:40.520 --> 01:29:47.240
actual deploying ai at a business level and um it's going to be very hard for regulators to keep up

01:29:47.240 --> 01:29:53.800
and to eva's point i think um in terms of what we hope will be the outcome often unfortunately

01:29:53.800 --> 01:30:00.680
comes with with the catastrophic um happening taking place before it really sharpens the minds

01:30:00.680 --> 01:30:07.560
and people figure out um how urgent it is i think there's a real sense of urgency in in the community

01:30:07.560 --> 01:30:13.080
around trying to work out uh what what the guardrail should be whether it should be a constitution

01:30:13.640 --> 01:30:20.920
um or or how we should think about implementing safety mechanisms in as as we develop um further

01:30:20.920 --> 01:30:27.640
on our chat on our large language models but um i hope it doesn't need a catastrophic moment for

01:30:27.640 --> 01:30:33.960
that to sharpen but back to the business question there is this um hope that maybe businesses will

01:30:33.960 --> 01:30:41.400
self-regulate and i think that is maybe the case in highly regulated sectors you see in the banking

01:30:42.600 --> 01:30:48.120
sector and insurance or banking in particular that there is a guardrail is put in place there but

01:30:48.120 --> 01:30:55.000
that is um that there's a lot of businesses that don't have that regulation around them and i think

01:30:55.000 --> 01:31:00.040
there is a real risk for this completely running out of control at a business level as well

01:31:00.760 --> 01:31:06.920
would you advise businesses to self-regulate ahead of standards and regulations being agreed

01:31:06.920 --> 01:31:11.640
by governments i think that's what they're doing or trying some businesses are trying to do there's

01:31:11.640 --> 01:31:20.360
a big risk in um in the case of winning trust with your customers and also your your shareholders

01:31:20.360 --> 01:31:29.160
and and investors if you mishandle ai and you create issues around not taking into account

01:31:29.160 --> 01:31:36.360
how to properly deal with biases and other issues that is a situation that can create a real

01:31:36.360 --> 01:31:43.000
breakdown and trust with your with your organization so there is that risk and then there are businesses

01:31:43.000 --> 01:31:48.920
that don't necessarily lean on trust for their for their for their business and those are the

01:31:48.920 --> 01:31:56.440
ones i worry the most about indeed let's turn to tom o is a representative of the world of

01:31:56.440 --> 01:32:01.800
journalism do you feel journalists have helped the discussion about the existential threat from

01:32:01.800 --> 01:32:07.880
ai or have they muddied the water leading people to panic unnecessarily or perhaps get distracted on

01:32:07.880 --> 01:32:16.600
side issues rather than the main issue i think it's um all of the above aside from overrugging

01:32:16.600 --> 01:32:21.000
the pudding i think most people in this room including me have had a wit scared out of them

01:32:21.880 --> 01:32:30.840
by some of the talks just now one has side issues in journalism coverage of ai and i think

01:32:31.560 --> 01:32:38.760
the jobs market is one of those but i have been surprised pleasantly so by how things

01:32:38.760 --> 01:32:44.600
have progressed since 2016 and that's the first time that i wrote about ai safety and i think at

01:32:44.600 --> 01:32:51.400
that point the prospects of a bad scenario relating to ai was seen as about as likely by my

01:32:51.400 --> 01:32:58.120
colleagues as lester city winning the premier league um anyway several years later um i now see

01:32:58.120 --> 01:33:04.360
lots of my former colleagues writing to my mind um very informed pieces about ai safety and i think

01:33:04.360 --> 01:33:11.160
that's helped the public change well arrive at a view um and probably a lot of people in this room

01:33:11.240 --> 01:33:16.440
aware that the american public when polled now says that they want regulation of ai

01:33:16.440 --> 01:33:21.480
and they want a lot of it and i think we can credit journalism with some of that journalism

01:33:21.480 --> 01:33:26.680
should be doing more but it's more than i would have thought a few years ago and if you were to

01:33:26.680 --> 01:33:31.560
go away and write up a story about things that you might have changed your mind about tonight

01:33:31.560 --> 01:33:36.360
and that the public should pay attention to can you give us a sneak preview what that would include

01:33:36.360 --> 01:33:45.880
well i think um the idea of runaway ai is is not new um but i think it has been difficult

01:33:45.880 --> 01:33:51.000
historically to frame it in a way that really sticks and like really drives its way down your

01:33:51.000 --> 01:33:59.880
brainstem um and we have different ways of framing ai risk um and mr fosuliman's new book um which

01:33:59.880 --> 01:34:05.480
some of you might have read i think there's a pretty good job of framing it in a in a in a way

01:34:05.480 --> 01:34:13.240
in which he describes um ai being used to accelerate human ingenuity in whatever endeavors

01:34:13.240 --> 01:34:20.920
humans are up to um be they um be they good or be they bad that's one way of framing it um and i

01:34:20.920 --> 01:34:26.840
think um we've heard some pretty compelling ways of telling a story of runaway ai which is a

01:34:26.840 --> 01:34:34.760
different and scarier story thanks and let's turn to andrea and your role at conjecture

01:34:34.840 --> 01:34:42.200
what are you doing in a day by day basis to address this question well what we're trying to do and

01:34:42.200 --> 01:34:48.120
i mean kind of covered a lot of it is first of all to explain the problem to people uh i've been

01:34:48.120 --> 01:34:53.400
heartened by the public reaction in the last years like i also got to know about this problem

01:34:53.400 --> 01:35:00.520
quite a long time ago and i in the past i could almost not expect the day that major governments

01:35:00.520 --> 01:35:05.960
take this problem seriously and the public understand this problem and we all get together

01:35:05.960 --> 01:35:13.560
and take some initial promising insufficient but promising steps to address it uh and i think

01:35:13.560 --> 01:35:21.320
is figuring out policy solutions and the reality is that we don't obviously we don't have a playbook

01:35:21.320 --> 01:35:28.920
for what exactly they look like but what i think was a common theme of the of the talks tonight

01:35:28.920 --> 01:35:37.880
is that clearly at some level of power we are not in control anymore and everybody expects this

01:35:39.400 --> 01:35:46.120
those who don't expect this are misguided or expected but don't don't say it and

01:35:47.160 --> 01:35:54.200
the positive thing is that there is one physical resource that drives the majority of what makes

01:35:54.200 --> 01:36:01.080
these systems powerful which is computing power and it's a physical resource not not like you know

01:36:01.080 --> 01:36:08.680
algorithms that you could just write on a piece of paper it's traceable it's expensive large place

01:36:08.680 --> 01:36:13.800
in data centers and while you know the scaling hypothesis the idea that you know the more

01:36:13.800 --> 01:36:17.960
computing power you put into something the more powerful it becomes might head some

01:36:18.600 --> 01:36:23.720
diminishing returns at some point we do not see any reason to expect it to stop so

01:36:24.680 --> 01:36:29.960
we know you know from both sides companies know that more computing power leads to more power

01:36:29.960 --> 01:36:35.400
and that's why they're doing what they're doing we know that limiting that computing power is a

01:36:35.400 --> 01:36:44.760
very effective way to kind of stem the the bleeding and stop and our positive situation for a while

01:36:44.760 --> 01:36:50.120
take a time out have the time to figure out the solutions have the time to absorb this into society

01:36:50.120 --> 01:36:55.080
and how much time will that give us because there's a risk that people will use today's models

01:36:55.640 --> 01:37:01.960
to design much more efficient ways to build next generation models and so they could therefore

01:37:01.960 --> 01:37:09.080
come under the radar as it were that people who were watching for a large use of GPUs would miss

01:37:09.080 --> 01:37:14.440
the clever way that somebody has built it so do we have a decade do we have three or four years

01:37:14.760 --> 01:37:20.760
or how long yeah that's that's a great question it's captain computing power is not a permanent

01:37:20.760 --> 01:37:28.360
solution but it's one of the best solutions we have at the moment uh as others have said before

01:37:28.360 --> 01:37:33.080
we are in a double exponential it's not a single exponential we have an an exponential growth of

01:37:33.080 --> 01:37:39.800
computing power hardware and exponential improvement in software we need to start

01:37:40.680 --> 01:37:46.520
cutting down on one of the two uh cutting down on a compute depends where you put the cap

01:37:47.160 --> 01:37:53.800
probably will buy us five seven years you can make you can make what would seem to people at

01:37:53.800 --> 01:37:58.920
the frontier extremely strong caps that would affect you know less than 20 companies in the

01:37:58.920 --> 01:38:04.680
world that probably could buy you 10 years in that period we need to figure out all of the rest

01:38:04.680 --> 01:38:09.480
it's gonna be a hard problem but we have done it before with nuclear weapons we've done it before

01:38:09.480 --> 01:38:13.800
with biological weapons we can do it again we're gonna go around the panelists one more time in

01:38:13.800 --> 01:38:17.960
the same order i'll give you a chance a choice panelist you can either comment on what you've

01:38:17.960 --> 01:38:23.960
heard from somebody else or you can paint me a picture of what would be a successful ali safety

01:38:23.960 --> 01:38:29.000
summit in bletchley park if things go well what would be the outcome and what would also be the

01:38:29.000 --> 01:38:35.400
follow-up so yarn first well i'm the one of the authors of the post letter so it's like

01:38:35.400 --> 01:38:42.280
indefinite moratorium uh on further scaling uh would be sort of my wet dream from outcome from

01:38:43.000 --> 01:38:48.520
from this summit or perhaps the next one if this one isn't realistic and what's the chance do you

01:38:48.520 --> 01:38:54.680
think what might cause the assembled world leaders to have an intellectual breakthrough and say yes

01:38:54.680 --> 01:39:01.240
actually we do need to have this indefinite pause so currently i'm not very optimistic on on that

01:39:02.040 --> 01:39:05.960
perhaps perhaps but perhaps in six months it would be much more clearer

01:39:06.600 --> 01:39:12.680
why this is needed so and and we have more time to kind of do the necessary loving

01:39:12.680 --> 01:39:17.080
so the discussion is prepared to ground and when something really bad happens in six months when

01:39:17.080 --> 01:39:22.520
gpd 5 comes out and oh my god at least we'll know what we should be doing yeah i mean let's not forget

01:39:22.520 --> 01:39:29.000
that gpd chat gpd has been out less than one year so like the world was very different one year ago

01:39:29.960 --> 01:39:35.640
same question to you either yeah thank you i think i'm just going to build on top of jan's ideal

01:39:36.200 --> 01:39:41.400
outcome of the summit and say that i would also find it terrific if the summit could be the first

01:39:41.400 --> 01:39:47.720
in a series of repeated um summits like this where world leaders come together because as we i think

01:39:47.720 --> 01:39:52.920
a pretty clear picture has been painted tonight of the fact that the field of ai evolves very quickly

01:39:53.000 --> 01:39:59.080
and is going to continue to evolve very quickly if not ever quicker and because of that i think it

01:39:59.080 --> 01:40:05.160
would be very valuable if we would have a regular occasion for world leaders to come together and

01:40:05.160 --> 01:40:09.880
not only make sure that the rules that they came up with are upheld but also to reevaluate

01:40:09.880 --> 01:40:13.880
whether they still make sense and where they need to be adapted or whether new real rules need to

01:40:13.880 --> 01:40:19.640
be introduced as for example measures like compute control that andrea mentioned they buy us some

01:40:19.640 --> 01:40:24.280
time but at some point they might not be applicable anymore so not just agreement on rules but setting

01:40:24.280 --> 01:40:29.640
up some audit process so that we can figure out whether the rules are being forward or not for

01:40:29.640 --> 01:40:34.920
example yeah same question to you i would agree you have to build in i mean right now it's just

01:40:34.920 --> 01:40:41.080
based especially in the us um the talks that have been held in the white house and by chuck schumer

01:40:42.120 --> 01:40:49.080
the gatherings have led to sort of ideas around voluntary um adherence to some principles but

01:40:49.080 --> 01:40:55.240
there is absolutely no built-in audit or accountability um so i think that we've got

01:40:55.240 --> 01:41:00.680
to see that come out of of the uk's ai safety summit among other things maybe there has to be

01:41:00.680 --> 01:41:07.800
something more concrete around licensing um of the models and and the use of them and that they

01:41:07.800 --> 01:41:14.360
have to pass some kind of a threshold i think the risk of of bad actors getting hold of them is

01:41:14.920 --> 01:41:21.320
is a is a much higher risk i think the ia ea structure is is one that one can look at but the

01:41:21.320 --> 01:41:28.600
the nuclear a lot of the success probably of the ia ea lies in the mutual assured destruction

01:41:28.600 --> 01:41:36.520
of humanity by using um nuclear weapons and this might be the same situation but they're easier to

01:41:36.520 --> 01:41:43.240
monitor i think um i think this might be slightly harder because you can land in the hands of bad

01:41:43.240 --> 01:41:47.880
actors more easily we haven't really discussed bad actors much in this session tonight maybe that

01:41:47.880 --> 01:41:54.520
makes the things even more horrifying we might come back to that later and tom what's your answer

01:41:54.520 --> 01:41:58.040
what would you like to see come out of the summit or maybe you've got some comment on something else

01:41:58.040 --> 01:42:08.040
you've heard from the other speakers well i'll talk about the summits um often when CEOs of um

01:42:08.040 --> 01:42:14.440
labs developing agi are asked about regulation um they basically say bring it on we love some

01:42:14.440 --> 01:42:20.200
regulation um and i think it would be great if politicians could actually put that to the test

01:42:22.840 --> 01:42:30.520
very good and andrea what would you like to see if you were invited to bletchley park and given

01:42:30.520 --> 01:42:36.520
their microphone for two minutes what would you entreat the assembled world leaders to consider

01:42:37.400 --> 01:42:42.440
well i i don't want to be too hopeful as some others here have been but at the very least i

01:42:42.440 --> 01:42:47.000
would like to see a commitment to the fact that this is extremely dangerous technology

01:42:47.960 --> 01:42:54.440
continuing to scale leads to predictable disaster and we need to pull on the brakes right now

01:42:55.160 --> 01:43:00.680
we have a lot of applications are very beneficial we can focus on those but limit this death

01:43:00.680 --> 01:43:07.160
phrase to the ever more powerful ever more obscure general systems that we can control what they

01:43:07.160 --> 01:43:14.360
definitely do not want to see is a diplomatic shake of hands where companies write their own

01:43:14.360 --> 01:43:20.280
playbook and say we're gonna keep doing exactly what we we're doing right now but it's gonna sound

01:43:20.280 --> 01:43:26.120
responsible and governments can wash their hands and say well we did our part let's move on that

01:43:26.120 --> 01:43:31.960
would be a very bad outcome right i'm gonna ask for three questions from the floor i'm gonna get

01:43:31.960 --> 01:43:36.600
the panel to think which ones to answer but i'm trying to take people haven't asked before so on

01:43:36.600 --> 01:43:43.560
the second row here there's a hand up here and then also the second row over there next as well

01:43:43.560 --> 01:43:49.320
let's take three fairly short questions please hi first of all thank you very much for coming

01:43:49.320 --> 01:43:55.160
here tonight and sharing your expertise with all of us in this whole question this whole discussion

01:43:55.160 --> 01:44:01.560
there's the implicit assumption that agi is coming and it's coming soon and the million dollar

01:44:01.560 --> 01:44:08.040
question i guess is when exactly is it coming but a more practical question is what are some

01:44:08.040 --> 01:44:13.480
warning signs and do we already see some of those in the systems that we have currently deployed

01:44:14.120 --> 01:44:18.040
great let's have a question over here as well sorry the microphone's gonna have to

01:44:18.680 --> 01:44:21.000
run around at the end of the second row there

01:44:21.000 --> 01:44:30.040
thank you for great panel um my question's related to yon's comment at the beginning

01:44:30.040 --> 01:44:37.160
on essentially dario amadeus philosophy which is you know and and also related to i guess

01:44:37.160 --> 01:44:43.080
roman's uh talk which is how do we solve the control problem and what i've heard the large

01:44:43.080 --> 01:44:50.360
our labs repeat is oh you know we need to increase capability that's only better AI that is going to

01:44:50.360 --> 01:44:57.000
be able to help us figure out how to solve AI and there's sort of this race to increase capability

01:44:57.000 --> 01:45:03.720
up to the point that can help us solve it but no further and and and just sort of thoughts on that

01:45:04.280 --> 01:45:11.720
philosophy and and and and you know uh whether there might be something to it or is it just a

01:45:11.720 --> 01:45:17.800
completely risky game you know thanks and there was one in the middle of the third row there

01:45:18.360 --> 01:45:20.760
pass the microphone along please to the middle

01:45:22.680 --> 01:45:27.640
how long of a time frame do you think we have between the arrival of agi and the arrival

01:45:27.640 --> 01:45:33.800
of superintelligence and within that time frame could there be tractable solutions for alignment

01:45:33.800 --> 01:45:39.000
or the control problem and if so would those solutions be able to be implemented before

01:45:39.000 --> 01:45:46.280
hurry up and develop better AI third question was how long might it take between the arrival of agi

01:45:46.280 --> 01:45:53.160
and superintelligence and whether there would be time for us to work out solutions then and my

01:45:53.160 --> 01:45:58.120
question i guess is well what's all this about agi isn't the bletchley park summit set up to

01:45:58.120 --> 01:46:03.480
discuss something else which is frontier models which says that there are catastrophic risks

01:46:03.480 --> 01:46:10.920
even before we get to agi so i'm gonna go around the same order again i'll be a bit predictable

01:46:10.920 --> 01:46:16.360
jan you want to pick one of these questions maybe i mean the answer to all the all three

01:46:16.360 --> 01:46:23.240
questions is uncertain and that's that's why we need to pause and kind of take a time out and see

01:46:23.240 --> 01:46:28.440
like how can we kind of create more set more certainty about these things i think i would

01:46:28.440 --> 01:46:35.320
answer the entropic question specifically that definitely is a lot of truth to the to the point

01:46:35.320 --> 01:46:41.800
that like the more capable model you have to work with the more kind of better position you are

01:46:41.800 --> 01:46:49.560
empirically you can you can kind of be too like too science in a way that you just can't do with

01:46:49.560 --> 01:46:57.160
models from from 10 years ago and also like one claim that people on topic to make is that like in

01:46:57.160 --> 01:47:03.320
some ways it becomes easier as the model kind of has better understanding of what you're trying to do

01:47:03.880 --> 01:47:12.040
with it or to do to it but that said again it's a to put it lightly it's playing with fire so so

01:47:12.040 --> 01:47:18.200
it's i'm not sure if anyone should be doing it either and jan said it's all uncertain but can't

01:47:18.200 --> 01:47:22.920
we at least agree in advance some canary signs that will make us say things are happening faster

01:47:22.920 --> 01:47:29.560
than we expected well i mean if we look at the past there were several signs that people agreed on

01:47:29.560 --> 01:47:36.120
that they might point out that we're getting into a zone where ai is maybe more capable than we think

01:47:36.120 --> 01:47:43.640
it is and i mean we certainly have seen signs connor mentioned or was it roman mentioned that

01:47:43.640 --> 01:47:50.520
current models um outperform most humans on things like the bar exam um these are clearly

01:47:50.520 --> 01:47:55.960
advances in capabilities that um i almost wonder sometimes if we just become desensitized to them

01:47:55.960 --> 01:48:00.840
because we move so fast i mean again charge upt came out a couple months ago and it's already

01:48:00.840 --> 01:48:06.360
just normal and people are waiting okay what's the next big thing so um it doesn't really help

01:48:07.080 --> 01:48:12.840
to think retroactively have there been any signs um if you didn't take them to actually stop and

01:48:12.840 --> 01:48:17.880
reconsider what you're doing so i think one of the big problems here is not have there been signs

01:48:17.880 --> 01:48:23.640
a big problem is can we pre-commit to stopping when we see certain signs and then actually stop

01:48:23.800 --> 01:48:29.640
actually take certain actions and we just haven't seen that before so this is developing contingency

01:48:29.640 --> 01:48:36.440
solutions like we're meant to have had contingency solutions for pandemics yeah yeah any comments

01:48:36.440 --> 01:48:43.160
alizandra i i will leave the um well how long it's going to take to reach agi to to the experts on

01:48:43.160 --> 01:48:49.560
the panel but on the outcome of the summit and i think that there is a bit of a confusion sometimes

01:48:49.560 --> 01:48:57.080
and in in what we are expecting to be achieved from the discussions on regulation because there's

01:48:57.080 --> 01:49:04.680
an obvious very important urgent and existential question around regulation regulating for the

01:49:04.680 --> 01:49:11.160
long term but then we also have businesses that are sitting and waiting for regulation that is here

01:49:11.160 --> 01:49:16.600
and now how is it going to impact my particular sector how is it going to impact what i'm doing

01:49:16.600 --> 01:49:23.560
today and what are the immediate and very very real risks right now here today that we are seeing

01:49:24.360 --> 01:49:29.960
with ai having impact on you know media with disinformation and so on but then there's also

01:49:29.960 --> 01:49:38.040
the specific um aspects to how that is implemented in particular sectors so i um hope that we would

01:49:38.040 --> 01:49:44.280
see addressing both of those short term and long term questions thanks tom any thoughts

01:49:44.360 --> 01:49:50.840
yeah on yardsticks i think it's worth remembering that the canonical yardstick was the Turing test

01:49:51.720 --> 01:49:59.080
and that's long gone um ai's can now beat humans at um diplomatic based games for instance um and

01:49:59.080 --> 01:50:06.760
much more um the modern Turing test is i think quite an interesting proposition um and that's

01:50:06.760 --> 01:50:13.720
the test of whether the ai can i think make a million dollars very quickly um but as as eva says

01:50:13.720 --> 01:50:19.080
we must stop shifting the gold posts um we need to agree that the one is we should pick one

01:50:19.720 --> 01:50:23.400
agree that that's the one where we start taking it seriously and then take it seriously when it

01:50:23.400 --> 01:50:29.400
is passed which it will be quite soon but in the past people said you won't manage to solve chess

01:50:29.400 --> 01:50:36.120
unless you have got a full grasp of all aspects of creativity and so on and then when deep blue did

01:50:36.120 --> 01:50:39.800
win at chess people said oh well it's not actually doing it in the way that we thought would be so

01:50:39.800 --> 01:50:45.640
terrible it's just grunting out incredibly so i feel there will always be people who

01:50:45.640 --> 01:50:50.040
don't move the gold post but they'll say well how it was implemented it doesn't demonstrate

01:50:50.040 --> 01:50:57.960
too intelligent yeah i think that's a good point um and it reminds us something um conna said um

01:50:57.960 --> 01:51:04.520
which is that there won't be consensus at the time to act so we need to be able to build a

01:51:04.520 --> 01:51:11.320
coalition of the willing uh andrea what's your views on these questions yeah maybe answering the

01:51:11.880 --> 01:51:17.880
last one first on isn't a summit about frontier ai well much like with gold posts it feels a bit

01:51:17.880 --> 01:51:24.120
like terminology is being shifted all the time sometimes quite willingly by the companies

01:51:24.120 --> 01:51:29.880
building this uh you know in in the in the old days people used to talk about superintelligence or

01:51:30.600 --> 01:51:37.480
friendly ai or strong ai then became agi then recently the frontier term was a kind of open

01:51:37.480 --> 01:51:43.880
ai entropic rebrand of oh no like we're not well we're gonna get to very powerful ai system soon

01:51:43.880 --> 01:51:49.320
but it's frontier which sounds better than agi because people are getting concerned about agi

01:51:49.320 --> 01:51:55.560
you know in practice do these terms matter not too much what matters is how competent systems are

01:51:56.360 --> 01:52:03.800
basically all of these companies expect to build systems that outperform humans at most tasks

01:52:03.800 --> 01:52:08.440
definitely most tasks you can do behind a computer in the next two to five years

01:52:10.120 --> 01:52:16.360
there's these matches the trends that we see in performance and compute growth this

01:52:17.560 --> 01:52:23.080
is very worrying uh these these are these are levels of competence uh at which we expect the

01:52:23.080 --> 01:52:29.400
systems to be out of our control unless we have various solutions so we just need to deal with that

01:52:29.400 --> 01:52:37.560
we can call it frontier ai agi proto agi proto superintelligence it's just terminology what

01:52:37.560 --> 01:52:42.440
matters is how powerful they are and how ready we are to deal with them we must avoid the serious

01:52:42.440 --> 01:52:48.200
discussions getting sidelined into semantics which is often very frustrating we're gonna take three

01:52:48.200 --> 01:52:51.880
more questions we're gonna go around the panel in the reverse order next time i'm gonna take

01:52:51.880 --> 01:52:58.840
questions from people who haven't answered asked before so somebody in white about halfway down

01:52:59.560 --> 01:53:04.280
if you have asked a question before please don't put your hand up just now so we have more chance

01:53:04.280 --> 01:53:12.520
just there yes thank you three questions one from each thanks um let's say that in 20 years

01:53:12.520 --> 01:53:17.960
we somehow managed to get it right and humanity still exists um despite the development of these

01:53:17.960 --> 01:53:23.480
adi what do you think is one essential piece of regulation or development that has to have

01:53:23.480 --> 01:53:31.320
happened together it's a great question uh where else with the hands uh where are the microphones

01:53:31.320 --> 01:53:41.320
there's one about just on the other side thank you so you've spoken quite a lot about like what

01:53:41.320 --> 01:53:45.960
government should do what companies should do uh i'm interested in like what should ordinary people

01:53:45.960 --> 01:53:50.840
do like what can we be doing to get our voices heard in this you know should we be protesting

01:53:50.840 --> 01:53:54.760
i've edited international protest on the 21st is this thing we should be doing or is this a terrible

01:53:54.760 --> 01:54:00.200
idea it's another fine question is there a question from a woman it's about time we heard from the

01:54:00.840 --> 01:54:09.160
other gender another gender hands up yes somebody put your hand up to whoever it was

01:54:09.160 --> 01:54:16.680
yeah there's one okay there's one there and we'll take you as well we'll take four right

01:54:17.880 --> 01:54:24.040
um as agi has been developed to be more human do you think it's possible to have forms of agi

01:54:24.040 --> 01:54:29.080
that don't have the inherent geopolitical biases that come with the data sets that we currently have

01:54:29.080 --> 01:54:35.160
and how do you think we go about developing regulations that aren't formed by human conscious bias

01:54:35.560 --> 01:54:44.040
okay and so if we can get the mic over here as well too one two three four five rows back

01:54:44.040 --> 01:54:54.680
just at the edge yes over there yeah i miscounted perhaps yeah four sorry quick question um so i

01:54:54.680 --> 01:55:00.040
actually work in the automotive industry and we have to certify vehicles and engines and it is

01:55:00.040 --> 01:55:06.680
an uphill battle um you can spend years just trying to get a windshield wiper right um or a

01:55:06.680 --> 01:55:13.800
temperature sensor right and i'm just curious um if you think that there would be an ability to

01:55:13.800 --> 01:55:20.680
take people who have regulated and certified products around global markets and how difficult

01:55:20.680 --> 01:55:26.600
that is and create a summit where that expertise could come together from different industries

01:55:26.600 --> 01:55:32.440
and we could roll up our sleeves and say okay this is how the structures go and we know what

01:55:32.440 --> 01:55:38.280
works we know what goes slow and try to accelerate that learning because i think that voice we have

01:55:38.280 --> 01:55:43.960
so much experience in the world right now um with that sleeves rolled up we know what it's like to

01:55:43.960 --> 01:55:49.720
sit in those test labs or send 30 000 pages of documents in with verification and validation

01:55:49.720 --> 01:55:54.120
data we know how to do requirements engineering requirements design requirements and i'm just

01:55:54.120 --> 01:55:59.800
wondering if um there's been any discussion of that to pull you know pull a summit together from

01:55:59.800 --> 01:56:06.440
people from heavily regulated industries four great questions first of all 20 years later it

01:56:06.440 --> 01:56:11.640
succeeded how did we get it right what were the regulations that made the difference what should

01:56:11.640 --> 01:56:19.240
ordinary people be doing can we design AI that is free from some of the human biases the geopolitical

01:56:19.240 --> 01:56:25.560
biases that cause strife among humans and can we learn from the people who are professionally

01:56:25.560 --> 01:56:31.400
involved in doing regulations and certification in multiple industry rather than just to being

01:56:31.400 --> 01:56:38.040
naive in our own applications so Andrea first yeah maybe i will answer the question about can we

01:56:38.040 --> 01:56:45.240
learn about highly regulated industries definitely i think there is a big kind of problem of uh

01:56:45.240 --> 01:56:51.640
arrogance in AI or like willful arrogance of just thinking that this sector should be special and

01:56:52.520 --> 01:56:57.880
people should be absolutely free to do any experiments they want all the time uh use you

01:56:57.880 --> 01:57:02.280
know as much computing power as they want try the worst possible applications all the time

01:57:02.280 --> 01:57:06.360
fully open source on the internet and nobody can complain like very often people in the

01:57:06.360 --> 01:57:11.160
eye sector get very very angry when somebody tells them look well maybe what you just did

01:57:11.160 --> 01:57:17.720
should be regulated in other industries we don't do it like that like with drugs we don't just let

01:57:20.120 --> 01:57:26.040
pharmaceutical companies just release and test the drugs on billions of people and have their

01:57:26.040 --> 01:57:30.680
CEOs say oh there's a 20 percent chance you will die if you take this drug but you know it's okay

01:57:30.680 --> 01:57:35.960
like if it happens you can let us know and then we'll we'll stop maybe right so we can totally

01:57:35.960 --> 01:57:42.440
learn from that we'll be great to learn from that um there is one challenge which is that we don't

01:57:42.440 --> 01:57:49.480
understand current systems that well so it makes things like auditing them and evaluating them

01:57:49.480 --> 01:57:54.840
quite tricky because we simply don't know how they work internally as well but we can do many other

01:57:54.840 --> 01:57:59.800
things and we can definitely learn from highly regulated industries and definitely given the

01:58:00.520 --> 01:58:04.760
risks admitted by the companies themselves other frontier the approach should be

01:58:06.040 --> 01:58:12.360
highly regulated industry not so it is different but not completely different and we can indeed learn

01:58:13.000 --> 01:58:19.720
tom closing words from you um well i'll take the um the question about what ordinary people

01:58:19.720 --> 01:58:25.080
should do um and have two immediate thoughts one is that it's very important to keep this issue

01:58:25.640 --> 01:58:33.720
apolitical um the other is that lawmakers need a sense of legitimacy i think um in order to

01:58:34.680 --> 01:58:40.760
come up with regulation um and to bring it in through acts and bills and so on um a good example

01:58:40.760 --> 01:58:46.520
of um when this happened a bit too slowly was it the outset of covid um and when it was a fringe

01:58:46.520 --> 01:58:51.800
issue there were no enough rules then the public got involved um and suddenly the rules arrived a

01:58:51.800 --> 01:58:58.600
little too late but they did arrive um and how can ordinary people achieve this um i think ordinary

01:58:58.600 --> 01:59:03.960
people i i don't have a theory of of protest so i won't comment on that um but i think it's

01:59:03.960 --> 01:59:09.000
important um that we all keep this in the public conversation i suppose what my answer is really

01:59:09.000 --> 01:59:15.640
tending towards is you should all read lots of journalism about ai click on my articles thanks

01:59:15.640 --> 01:59:19.880
alessandra any of these questions catch your attention i think i think um your question

01:59:19.880 --> 01:59:24.280
in the orange sweater there is is um is is definitely where we're headed i mean there's

01:59:24.280 --> 01:59:33.320
got to be some kind of um system that resembles either the car industry or fda and um the way that

01:59:33.320 --> 01:59:41.480
we certify um our you know products generally speaking and i i just don't know how we get from

01:59:42.040 --> 01:59:49.480
from that to something that is very difficult to trace and to to monitor as as ai but i would say

01:59:49.480 --> 01:59:53.560
to the gentleman's question in the white shirt there if we're looking 20 years down the road

01:59:54.120 --> 02:00:02.840
and we say that's really great in the uk november 2023 we we were able to put in place regulation

02:00:02.840 --> 02:00:09.320
that somehow created traceability um so we could we could work out sort of where the where they were

02:00:09.320 --> 02:00:14.280
where systems were running out of control or landing in the hands of bad actors that would

02:00:14.280 --> 02:00:19.960
be a huge success i think that the reality is a bit different and that is that it probably is

02:00:19.960 --> 02:00:26.120
going to resemble a bit more the world in which cyber security um flourishes and that means you're

02:00:26.120 --> 02:00:34.040
constantly trying to create um a dam system or a a um deflection of all sort of incoming um

02:00:34.840 --> 02:00:40.600
activities that are not great so i know that none of these are perfect analogies but i think it is

02:00:40.600 --> 02:00:48.840
and in that universe we're probably going to be operating in for a while thanks final words either

02:00:48.840 --> 02:00:54.360
sure so i would love to touch on two questions very briefly one of them um being i think your

02:00:54.360 --> 02:01:00.200
question the white shirt what uh policies will bring us to a safe world in 20 years and i think um

02:01:00.200 --> 02:01:04.760
a policy that was mentioned today as well already but that i want to touch on again is um strict

02:01:04.760 --> 02:01:10.520
liability regimes just simply to kind of shift the incentive systems um incentive structures that

02:01:10.600 --> 02:01:16.360
drive private companies to take certain actions that are not in the interest of um the wider

02:01:16.360 --> 02:01:23.720
general public so i think there we can um really um shift shift the incentive structure to move

02:01:23.720 --> 02:01:28.760
companies to take um maybe different paths forward and then what can the the average person the

02:01:28.760 --> 02:01:33.960
general public do i would completely agree with tom i think um one thing that that really would help

02:01:33.960 --> 02:01:40.280
us to for lack of a better expression to just make noise just make sure that this topic is um

02:01:40.280 --> 02:01:44.760
talked about publicly you can do this in different ways you can write to your local newspaper you

02:01:44.760 --> 02:01:51.080
can make a protest if that's up your alley you can write to your mp or your congressman or

02:01:51.080 --> 02:01:55.880
wherever you live um and again create that legitimacy for people to actually act on the

02:01:55.880 --> 02:02:01.320
problem because to many people it does sound very much like sci-fi and policy makers i'm not going

02:02:01.320 --> 02:02:05.960
to take action and newspapers are not going to continue to report about an issue if you're like

02:02:05.960 --> 02:02:11.000
it doesn't have traction and isn't taken seriously by the general public the other thing the general

02:02:11.000 --> 02:02:16.520
public can do is we can educate ourselves and then we can share what information we have found

02:02:16.520 --> 02:02:21.880
to be most persuasive ourselves because there's a wide variety of books a wide variety of youtube

02:02:21.880 --> 02:02:27.800
channels a wide variety of blogs and some of them are better than others so let's share what we have

02:02:27.800 --> 02:02:33.800
found to be the really best ones auto before i pass to you and maybe i'll ask you to get ready to

02:02:33.800 --> 02:02:39.000
come up on the stage because you're going to give some closing remarks but jan what's your answers to

02:02:39.000 --> 02:02:44.840
what you've heard uh so yeah just to just kind of underline the what ordinary people could do

02:02:46.120 --> 02:02:51.080
is just kind of keep this topic alive like one of the things that i'm very proud of uh that came out

02:02:51.080 --> 02:02:58.520
of the future of life six months post letter uh was uh framed by uh european commissioner Margaret

02:02:58.520 --> 02:03:05.160
best agar when she said that like one thing that this letter has done is to kind of like communicate

02:03:05.160 --> 02:03:11.400
to the regulators that these concerns are much more widespread among people than among regulators

02:03:11.400 --> 02:03:16.360
so i think this uh potential difference should be continually kind of maintained

02:03:17.560 --> 02:03:23.880
so and when it comes to uh kind of bringing in uh kind of expertise from people from like

02:03:23.880 --> 02:03:30.600
regulated industries i think it's super valuable i was on the on the board or like on the european

02:03:30.600 --> 02:03:35.160
high-level expert group at the european commission and there was like every once in a while there was

02:03:35.160 --> 02:03:39.080
like why are we inventing the wheel like that we already have like lots of regulations should we

02:03:39.080 --> 02:03:45.640
just apply this and i was like yes however there's like one big problem uh the problem is p in chat gpt

02:03:46.680 --> 02:03:53.480
gpt stands for generative pre-trained transformer the pre-training is something that you do before

02:03:53.960 --> 02:04:04.440
you actually train so the current the nasty secret of ai field is the ai's are not built

02:04:04.440 --> 02:04:10.760
they are grown the way you you build the frontier model build the frontier model is you take like

02:04:10.760 --> 02:04:18.040
two pages of code you put them in tens of thousands of graphics cards and let them hum for months

02:04:18.600 --> 02:04:24.440
and then you're gonna open up the hood and see like what creature brings out and what you can

02:04:24.440 --> 02:04:31.800
can do with this creature so it's i think the regulate the industry the capacity to regulate

02:04:31.800 --> 02:04:39.400
things uh and kind of deal with various liability constraints etc they apply to what happens after

02:04:39.400 --> 02:04:45.960
what's once this creature has been kind of tamed and that's what what uh fine tuning and

02:04:46.120 --> 02:04:51.960
uh reinforcement learning from human feedback etc is doing and then productized then how do you

02:04:51.960 --> 02:04:57.480
deal with with these issues but uh is this where we need the competence of of like other other

02:04:57.480 --> 02:05:03.640
industries but like how can avoid the system not escaping during training run this is this is

02:05:03.640 --> 02:05:08.840
like a completely novel issue for this species and we need to need to need some other approaches

02:05:08.840 --> 02:05:14.920
like just banning those training runs that's great we'll thank the panel in a minute i'll ask

02:05:14.920 --> 02:05:22.360
the panel to stay here because who's going to wind up the evening is auto bartend auto is the

02:05:22.360 --> 02:05:30.440
executive director of the ero the existential risks observatory which along with conjecture has

02:05:30.440 --> 02:05:37.160
designed and organized and sponsored this whole evening auto has got a few closing remarks before

02:05:37.160 --> 02:05:42.200
those of us are still here can have a quick drink and continue the discussion informally

02:05:42.200 --> 02:05:45.640
up to 10 o'clock by which time we must be out of the building auto

02:05:59.240 --> 02:06:05.800
all right uh thanks david um a few closing remarks before we go to the drinks which is

02:06:05.800 --> 02:06:12.520
five minutes so you should be able to keep with me um so we're talking tonight about human extinction

02:06:12.520 --> 02:06:17.640
because of ai and what to do about this um and i think what to do about this there was also a

02:06:17.640 --> 02:06:22.120
great question from the audience what can we do about this this is exactly the question that i

02:06:22.120 --> 02:06:27.400
asked myself a few years ago um but it's not trivial and it's it's pretty difficult actually

02:06:27.400 --> 02:06:33.960
what is net positive what could you do develop ai yourself try to do it safely such as open ai

02:06:33.960 --> 02:06:41.000
deep mines and entropic are doing will this increase safety some say so work on ai alignment

02:06:41.000 --> 02:06:45.960
for example interpretability where we've seen great breakthroughs actually last week it could be a

02:06:45.960 --> 02:06:50.760
good option but increasing knowledge of how ai works could also speed up its development so this

02:06:50.760 --> 02:06:57.800
brings risks as well one could campaign for regulations such as an ai pause we support this

02:06:57.800 --> 02:07:03.480
but this also has its downsides so i think it's pretty difficult to tell what one should do to

02:07:03.480 --> 02:07:09.240
reduce human extinction risk by ai but when i started reading into this i was only really

02:07:09.240 --> 02:07:14.360
convinced about one thing and that is that you cannot put humanity at risk without telling us

02:07:15.080 --> 02:07:20.280
so you cannot have a dozen tech executives embarking on the singularity without informing

02:07:20.280 --> 02:07:26.440
anyone else and you cannot have a hundred people at a summit which is what's happening now decide

02:07:26.440 --> 02:07:31.720
what should be built and what should not be built and i think you cannot let a tiny amount of people

02:07:31.720 --> 02:07:38.600
also decide how high extinction risk should be for the rest of us so the only thing that i am

02:07:38.600 --> 02:07:44.520
really convinced of is that we should be informed about this topic and that's also why i'm so happy

02:07:44.520 --> 02:07:51.960
that events such as this one are taking place we're happy i'm happy that we're together not just

02:07:51.960 --> 02:07:57.080
with in crowd people some of you are and it's great but also with some people who may have never heard

02:07:57.160 --> 02:08:02.440
of existential risk before and also with journalists who can inform a much wider audience about

02:08:02.440 --> 02:08:07.880
existential risk also with a member of parliament someone with a job to openly discuss difficult

02:08:07.880 --> 02:08:13.480
problems so i think this is all very encouraging and it's helping to normalize an open debate about

02:08:13.480 --> 02:08:19.880
the topic of human extinction by artificial intelligence the 31st of october at two o'clock

02:08:19.880 --> 02:08:25.240
we'll have our next event with professor steward russell it's just outside the ai safety summit

02:08:25.240 --> 02:08:29.400
in bletchley park in the old assembly hall where the code breakers used to have their

02:08:29.400 --> 02:08:35.080
festivities after their important work so our event at bletchley park the day before the summit

02:08:35.080 --> 02:08:40.680
may not resemble a festivity but in a sense i think it is because we're celebrating that we're all

02:08:40.680 --> 02:08:46.280
being hurt here we're celebrating that we can all be part of a democratic conversation about what

02:08:46.280 --> 02:08:51.480
the most important technology of the century should and should not be able to do we can talk

02:08:51.480 --> 02:08:56.360
about risks to humanity we find acceptable and what we intend to do about risks that are too high

02:08:57.720 --> 02:09:02.360
and as the existential risk observatory together with conjecture we invite everyone to be part

02:09:02.360 --> 02:09:08.280
of this conversation so there's much to be unsure of in this field but if there's one thing that i

02:09:08.280 --> 02:09:13.400
am sure of it's that the most important conversation in this century which i think this is has to be

02:09:13.400 --> 02:09:20.760
a democratic one so with that i would like to invite you to scan the qr code on the left

02:09:21.560 --> 02:09:28.360
if this is working right yes to join us in bletchley this is containing the url where you

02:09:28.360 --> 02:09:33.560
can enroll to the bletchley park event if you're interested then definitely pass by

02:09:35.800 --> 02:09:41.240
there's same qr code is also on the flyer on your chair and beyond bletchley i think this

02:09:41.240 --> 02:09:46.600
conversation will not stop so there will be more summits according to my timeline about

02:09:47.800 --> 02:09:56.120
maybe 18 roughly um so we will organize more events probably publish more about ai do more

02:09:56.120 --> 02:10:01.720
research and inform governments as well as we can if you want to follow us or support us the

02:10:01.720 --> 02:10:06.440
existential risk observatory in that work then scan the qr code on the right there's much that you

02:10:06.440 --> 02:10:12.760
can do to to help us um and with that i would like to close this evening and once again thanks to

02:10:12.760 --> 02:10:18.440
all the great speakers so that's uh romeo polsky cornelly sir robert buckland yantalin andrea

02:10:18.440 --> 02:10:30.440
milti alexandra mosefisa day ava birans and some are give them a round of applause

02:10:37.400 --> 02:10:43.160
and i would also very much like to thank david wood uh should see them conor xio dis

02:10:43.240 --> 02:10:53.880
even diluman and everyone at conway hall will also make this evening possible thank you very much

02:10:56.360 --> 02:11:00.600
and then i would like to hopefully see you in bletchley and in any case you are the drink

02:11:00.600 --> 02:11:05.880
right now thank you thanks everybody

02:11:13.160 --> 02:11:13.880
you

02:11:43.160 --> 02:11:45.160
You

