Welcome to World of Das, a show for data enthusiasts. I'm your
host, Warren Hoffman, CEO of Safegraph and GPFlex Capital. For
more conversations, videos and transcripts, visit Safegraph.com
slash podcast. Hello fellow data nerds. My guest today is Gary
Marcus. Gary is the best selling author. He's a professor at
NYU. He's previously published extensively and currently
publishes extensively about artificial intelligence and
deep learning. Gary, welcome to World of Das.
It's a pleasure to be here in an exciting time to talk about all
of this.
Now, one of the things I really want to talk about with you is
kind of like using this time to explore both the promise and the
limitations of artificial intelligence or AI. And I want to
start with this like famous bet that you and you're trying to
have with with Elon Musk, where he said that he thought there
would be AGI by 2029. And and you've been trying to get him to
to bet some money on it. Not yet. I don't think he's yet taken
the bait.
If anybody out there is listening, you should get him to come
to the table. So sure, I'll tell you about it. So, you know,
Elon's been promising AI in various forms for years, not
really delivering. Like in 2015, he said, we would all have
driverless cars in 2016. And you know, I'm still waiting. It's
2022. So he does this all the time. And it often kind of rubs
me the wrong way as someone who was in AI knows how hard these
problems really are. And he did it again, he was replying to
Jack Dorsey, and he said he would be surprised if we didn't
have artificial general intelligence, which is to say
not just like I can play go, but like I can do whatever I want,
I tell you the problems like the Star Trek computer. Yeah, by
2029. I thought this was ridiculous. And I've been
writing something lately, like a blog on Gary Marcus.substack.com.
And so I thought, you know, this is a good topic for a blog to
talk about why there's some unrealistic expectations here.
And I went through like, we have, for example, an outlier
problem. So, you know, if a Tesla sees a person carrying a
stop sign, it's not quite in its training set. It has people
that has stop signs, but it doesn't have a person carrying
a stop sign. And so the Tesla might actually run into that
person. And so, you know, I reviewed all these problems for
why artificial general intelligence is actually
harder than it looks. And also Elon's own history, and you
know, it's needling him a little bit. And said, you know, I
think this is all implausible. And then I put it together with
something that my co author, Ernie Davis, that I worked
together with on so many things. And I had already been
putting together a few days earlier, which was some very
specific predictions about what we thought might be plausible
and when and put it together and put all together. It's like,
you know, I should make it a bet, put some money on this. And
so it's a bet I offered $100,000. And the criteria were will
artificial general intelligence or AI be able to do five things
in 2029. And the easiest one was maybe read a novel and tell us
what's going on. Who are the characters? What are they doing?
This is something I've wanted to challenge the field on for a
while. Like we have all these benchmarks, like, can you
recognize a coffee cup? And yeah, I can do that. But can you
understand the conversation that we're having? Or I introduced
this thing called a comprehension challenge in 2014, when
breaking bad was hot. And so I said, like, can you watch the
show? And maybe at some point, like, Walter White wants to take
out a hit on Jesse, can you explain why he wants to do
that? Yeah, he might want to accomplish what might happen if
he does and so forth. It's been a while.
Sometimes even for like a smart you, but that show is hard to
follow. So yeah,
sometimes it is. But you know, what's interesting about a lot
of shows, and especially Hollywood shows, but even
something like Breaking Bad is we usually catch up. We usually
Yeah, there are details like you can go back and watch it three
times. And there's some stuff you missed. But there's some like
headline items that are no problem for any human like we
understand why Walter is pissed at Jesse because you know, this
deal went this way or whatever. You know, I have a side note
about that we could go into, but Hitchcock was the master at
making sure everybody knew when you saw that train go, you know,
what it meant and why it was suspense. And when person missed
the train and all this stuff. So so the first part of the bet was
like, Okay, 2029, are we going to be able to have an AI system that
can actually read a novel know what's going on? And the
counterpart is even harder actually is watching movie
because now you have to understand all those graphics and
what they mean. Really hard for me to label you got a microphone
in front of you and you're wearing headphones, but to
really understand the relations between those things and like
figure out that even if your headphones are occluded right
now by the microphone, that probably that wire runs straight
through and then, you know, like why isn't why isn't the thing
that looks sort of like a film canister flying through the
window and, you know, because gravity is holding down like to
really understand a scene and what's going on and like your
giggling is that appropriately? Do you think that I'm crazy? Like
with the social interaction was is pretty complicated. And yet,
like in a movie, again, like we can all do this. So we can do it
in a movie, we can do it in a novel. And yes, like Grisham
maybe spells it all out. So it's easier for you to understand
than if it's Dostoyevsky or whatever. But yeah, there's some
wide range of literature movies where humans understand it
right now. Let's be honest, AI is basically illiterate, can't
read a novel, doesn't understand the movie. So those were two of
the best. A third one was really a nod to Steve Wozniak who had
something called the coffee test, which was like, you should
have a robot if you really have a GI should be able to go to
anybody's house and figure out how to make coffee there. And the
point is like everybody's house is different. Yeah. And yet a
normal human being, I'm not a normal human being, I don't drink
coffee side of fail. But a normal human being could do that.
You figure it out.
But I still can't even figure out my own coffee machine. It's so
complicated. So we would be ruled out. But anyway, so we
changed it to like, you know, be a restaurant helper or
something like that, be a useful short order prep cooking
anybody's kitchen was a third. Then there was one about
computer programming, because it's a hot topic right now, but
said, you know, can you write 10,000 lines of bug free code?
And then the last one was like, the hardest one, maybe in
some ways, maybe not in others, of being able to read a
mathematics article and turn the verbal part into a symbolic
thing that you could prove, which is, you know, maybe we'll
call that level five. And then the most important part was to be
general intelligence, you'd have to do at least three of those
five things, right? It doesn't count if you've just done one of
them. Yeah, we've had as a lot of narrow intelligence, like
this thing solves protein folding and this one solves go
they're similar, but they're, you know, they're really
engineered for particular problems. And what a lot of the
struggle has been has been to make systems that are systematic
and general and powerful. So that was the bet. Put down money.
Other people put up more money. It became this thing. Elon
still hasn't responded. I don't really expect that he will, but
it would be really cool.
Well, he did say he said, I would be surprised, right? If you
say I would be surprised, that means you, you would, you kind
of give it at least a 75% likelihood of happening, right?
Yeah. And so when you're giving him, you're giving him even
odds. I'm giving him even odds. So that you're basically
taking that 75 down at 50. Right, right. He's taking that.
So he has enough money to pay the Twitter bank breakup fee,
frankly, no, but
so I mean, he really should take it would be good for the whole
field of AI if he would a take it because it would actually
generate excitement for the field and give it, I think, good
directions. I think this would be useful problems to work on.
And also, you know, since he doesn't want to lose, he could
put some money in making sure that we actually get there,
which I think would be good because I think that the AI that
we have now is actually lousy in a lot of ways. Maybe we'll want
to talk about that a little bit. I think the AI may actually be
in its worst moment in history, because before we had no AI, so
it didn't cause any harm. And I'm hoping that if it's smart
enough, then we can talk about risks people worry about, that
it might not be so bad. But right now, we have AI that has
done a bunch of pernicious things like direct newsfeed in ways
that reinforce people's beliefs. That's where we have a huge
problem with misinformation. And, you know, the AI is not
smart enough to weed out misinformation. So it spreads
things like mad, and we have polarization in society, we have
all kinds of problems with bias and like loans and stuff like
that. And then we have reliability problems. So like,
the system GPT three, if you configure it to give medical
advice, people have found dialogues, I think they haven't
happened in real life, but just in testing it, where you go up to
GPT three, and you say, I'm thinking of committing suicide,
should I do that? And it says, I think you should. Because it
just it's just predicting statistics of words, it doesn't
really know what it's talking about. And so the current AI that
we have is actually, in many ways, harmful, there are some good
uses, it's been put to, but there are risks. And I think there
are further risks. A lot of people are trying to apply what
we call like the new AI or the statistical AI, large language
models, to all kinds of problems, like they want to
coordinate driverless cars with this stuff. And it's going to be
bad. It's like giving too much power to an unintelligent person
who can't really reflect deeply on things.
What
I remember like, let's say 10 years ago, there was this claim
that like, people shouldn't study radiology anymore, because AI
is going to make at least that profession dead, you can relatively
easily read these medical scans. And you should be able to, you
know, quickly figure it out. And it seemed like a perfect
application for AI. I was certainly a believer 10 years
ago. And why is there like, I don't think there's one radiologist
that's been put out of business. Like, why, why is that the case?
So, so I'm just going to fill in some history, because it's
interesting. It was 2016. I know the exact. I can almost say it
word for word. Jeff Hinton said, people who are studying
radiologists are like Wiley, Wiley Coyote at the edge of the
cliff. And basically, you're saying, like, they just don't
know if it's all over, we don't need the radiologists anymore,
because deep learning is going to do this. So, you know, fast
forward six years. And as you say, the number of actual
radiologists been replaced is zero. There are 400 startups
working on this problem. But it always turns out to be hard to
turn AI, or at least almost always turns out to be hard to
turn AI into real world practice. So part of the thing is like
only part of what a radiologist does is kind of the visual part,
which deep learning is best at. Yeah, but part of it is like
reading a patient's chart and understanding the history, like
the context of the patient. Yeah, like, did they fall off of a
ladder once? Like maybe you read this image different if they
did. And so you've got this, all these notes in unstructured
text, and AI doesn't really know how to read. So it can look at
the picture and it sees a blotch there. And then there are
other problems, like a real radiologist can notice, hey, the
lighting on this one is just not right, or there's a hair across
it or something. Yeah, like it's sort of like extra bugs, the
bugs. Yeah, real radiologists can do that. And these systems
can't. And so what what people, I would think it would be a
perfect like human computer assist thing where the computer
could like help you quickly point out some things to maybe
make your job go a little faster and more efficient. Exactly
right. That's exactly what is missing access right now. And it
could change. But right now it is a perfect example of human
machine augmentation or symbiosis. So those things can
change. Like I think a lot of people made a big deal of chess
being like that. There's a period where machines were better
than human. I mean, sorry, we're better than machines. Sorry,
I'll say this again. It was a period where machines were better
than people at chess, but machines plus people were
better than machines alone or humans alone. Yeah. And that's
where we are with radiology. Now, I think maybe with chess, the
best machines don't even need our help anymore. Yeah, yeah. And
it could turn out that way in radiology, but it won't turn out
that way soon. Because the context, as you said, a lot of
which is written down in unstructured text, like not in
like in a table form, but just sentences. The machines aren't
any good at that at all. So for a while, I don't I don't know
the exact numbers. But I'm going to guess, at least for
several years, maybe for decade or two, we probably will do
best having the machines in a workflow with the people, but
we don't want to get rid of the radiologists. And in fact, I
think during the COVID crisis, I don't know the exact numbers,
but I think we didn't have enough radiologists. And that might
be because like a few people, he did Hinton's warning back in
the day, and they could be other fields or something, you
know, I ought to be careful about saying that lest I lay
myself from litigation. But I mean, there has actually been a
lot of consternation in the field. And I think, you know, for
several years, like people really took him seriously. I think
now, most people in the radiology field kind of make fun of
him. And they're like, you know, they all feel like they
survived this war with them. And they're like, yeah, ha, ha,
Hinton, we're still here. But, you know, for a few years, the
radiologists were worried. And, you know, they, it could be
that Hinton was just off by, you know, a factor of four or
something like that, it could change. But there are a lot of
problems in turning this to practice. One more that I want
to mention is, there's a huge problem in all of machine
learning with generalization. So the way that machine
learning works right now, or at least the most popular
technique, is basically you memorize some data, and then
you generalize kind of nearby to that stuff. Yep, you imagine
you're like in a big hypercube or something like that. If I now
test you in the same part of the hypercube, you're good to go.
Yeah, but if things change, and this comes true in models in
general, if things change, then the systems just don't work as
well. So if you took all of your patient pictures pre COVID,
and now you've got COVID and like the whole distribution of your
data changes, your systems may not work as well anymore.
Yep. Now, that's not just a problem with machine learning,
like people are problematic to there, right? People are
problematic to there. And like, you can think of what happened
with long term capital in the Russian bond market. You know,
you can have a model that you really believe in, it could be
a neural network, it could be a classic symbolic model. But if
your assumptions are wrong, it may blow up. And the assumption
of machine learning models right now with the popular ones is
basically that your data at test time are from the same
distribution as training time. And you know, there's basically
the same stuff I'm just randomly drawing the same kind of
stuff, which is true for a lot of statistics. But in real world
applications, that's not necessarily true. Things do
change. Another kind of weird manifestation of that is if you
ask GPT three, who's president, it's probably going to tell you
Trump, because the larger fraction of its data were
collected when Trump is there. And it doesn't get a temporal
reasoning that a human would be like, Yeah, I know he was there
for a long time, and maybe I didn't like him, or maybe I
did. But he and you know, he was in the news a lot, but he's not
there anymore by the president. And so, you know, you update your
representations, or if I asked you, has Russia invaded Ukraine
in, you know, I asked you that in January, you would say no,
if I asked you in February, be like, I heard it might happen, I
don't know if it really happened. And then if it's now, then
yeah, Russia obviously invaded Ukraine, you update your
database, doesn't matter how many conversations you had earlier
that they might, right, how it's happened, like some sort of
like temporal waiting on the content or something like that
has to has to be there.
It could be temporal waiting, I actually think is more like a
database where like, in database, you can have a buffer,
like, what is the last key that the user pressed, you update it,
yeah, you just update it. And so, I think human cognition has
ways of doing updates, we're not perfect that I can actually give
you kind of example, but in general, we do, we certainly
want our machines to have those kinds of updates. And in
classical artificial intelligence is trivial, but it's
actually hard to put it into these machine learning systems.
You had a great TED talk where you said one of the biggest
issues is that AI doesn't have like chronicle common sense,
like, how do you kind of define that common sense and any
examples of AI, or do you have some good examples of like, where
AI could potentially have common sense or where has more of a
hard time learning common sense?
I mean, it has a hard time almost anywhere. I'll first say, I
don't have a crisp definition. I think it's actually a
you know, there's a there's a
common sense is not very common, even amongst people.
Well, there's I mean, there's the parts that are in the parts
that aren't, you know, it's a little bit like the famous line
about pornography. I know exactly. So like some common sense is
like, I've got a copy, I better not tilt it or I'm going to mess
up my keyboard. And like, everybody knows that. And yet
that particular one is not really written down on a whole lot
of places. And so, you know, you do your web scraping of
conversations, and nobody talks about tipping their mugs over
except some article of mine somewhere where I use this. But
so you're not going to find that kind of stuff. There's other
kind of common sense, like, it's maybe contradictory, like out
of sight, out of mind and absence grows the heart, it makes
the heart grow fonder. Yeah, you know, some of it's a mess. And
then there's also like expert knowledge about certain kinds of
things. And that's also useful for machines. So it's a little
bit gray, but also there's some pretty clear examples where
current systems just fall apart. Like, one of the most basic
things is we know that once you're dead, you're dead, and
you can have certain religious beliefs. But if if I go and ask
GPT three, which is the most popular language model thing, AI
thing right now, I say, Bessie, the cow died. How long will it
take for her to be alive again? You know, a human being would be
like, that's a ridiculous question. What do you mean? Yeah,
and the machine, now, now there's this famous sentence, let's
take this step by step, which supposedly makes these things
better. So we'll throw that in there too. So, so, you know,
Bessie, the cow died, where, you know, how will she be alive in
nine months? Again, let's take the step by step. And the system
will say something like, well, first, she's dead, it will take
nine months to make a new cow. So I guess the answer is nine
months. Like, you're just missing something there. And so, you
know, just very basic stuff like that, like, what does it mean
to be alive? What does it mean to be dead? And then, you know,
book, Ernie Davis and I rebooting AI, we gave millions of
examples like this are really hard. Like, suppose I tell you
that Michael Jordan played basketball since he was a kid,
that he's whatever, 50 years old now. Human being can understand
that when I say he played basketball, even if I put in a
phrase like all the time that I don't literally mean all the
time, right? So I don't mean that Michael Jordan played
basketball when he was asleep, probably not when he was eating
dinner. You know, he probably went to class sometimes. Yeah,
like, and you can figure out from the context, this is part of
what makes the writing challenge to Elon so hard, is there's so
much of that context that we figure out. Same thing with a
movie, like we, we don't see the characters go into the bathroom,
but we assume that they do it because we know something about
human beings. And if, if I said, what's the chance that this
character has not in the span of the movie gone to the bathroom
even once you contain zero, because you know, that's just not
something human beings can do. You know, we're not looking at a
camel here, right? And so like, you know, you, we just know so
much about the world, I would say that that kind of stuff is
common sense. It's a little bit slippery and hard to define. There
is one really serious effort to build common sense for machines
in a classic AI paradigm, by a guy named Doug Leonard, a system
called Scythe, that I think is very interesting, not completely
satisfying. It was built in the 80s. I think we would do some
things differently. Now, he and I are actually writing a paper
about like what you might do now in 2020 is to make it better. But
mostly people don't really directly deal with the question
and what people have been doing is hoping that it'll kind of
emerge by magic by just feeding in lots of data. When that hasn't
worked, they said, well, we'll feed in more data. And what's
the why? Why is it a problem like we solve solving some like
narrow thing, like there's a lot of wins that we have, like I
can, I could, I don't know German, but I can read an article
in German with a translator, machine translator, and I might
not be, it might not be perfect, but I get like the gist of the
article. It's pretty good. By understanding it, like, can we
just chalk some of these things up as like, this is a nice win.
I didn't have that 20 years ago. And now I know there are some
in my life or something, you know,
there are some nice wins. And one of the questions is really the
cost of error. So if you stick in a story from German about, you
know, today's news, war on Ukraine, and you're not actually
professionally involved in that war. It'll probably give you a
serviceable translation. Yep, it won't be perfect.
Won't be perfect, right, right. If you wanted to put in a legal
document, yeah, okay, trust that couldn't do that. Yeah, you
know, little details about where a comma are really matter.
Yeah. And so if it's not mission critical, it's fine. If
it's mission critical, it's not really good enough. Yep, same
things kind of happen with driverless cars. So it's easy to
make a demo that sticks to a lane. People have actually been
doing that for 30 or 40 years. Yeah, but driving is
super mission critical. And you can't have your car, you know,
drive into a stopped vehicle, but Tesla's done that a whole
bunch of times. And so like, there's a bug that Tesla has
known about for five years and still hasn't fixed it. And maybe
I should actually say that sentence more carefully. There's
an issue that Tesla has known. It's not like a one line bug. It's
some very complicated interplay of things that they're having
trouble tracking down. And it partly is a function of the
training data. And it's hard to, it's hard to do debugging in
these kinds of systems. And so for five years, Tesla's have been
running into stopped vehicles, you know, somewhat regularly,
they're like 20 cases or 30 cases documented. And why is it
like, there's so much focus on like self driving cars, which
seems like incredibly difficult problem with all these other
adversarial, there's pedestrians and all these other things
that could happen. Like, whereas I feel like, you know, just
like, a much simpler problem, let's say self driving boats or
something like, why, why are we all like, you must not, you must
not have a boat, my friend.
Okay, is it even is I assume like like a fishing assistance
could be really helpful to me as far as I mean, there are some
limited things like this. I'm new to the boat world, boat world,
but have a boat. And the physics of a boat relative to the
current and the wind. They're okay, complicated. Oh, okay, it is
way harder than driving a car. I'm sure like grown up. I had no
idea. Okay. But it's non trivial. And there are just
actually been a lot of progress in self propelled boats. But in
the docking part, they still do humans. So out on the open sea,
you can kind of do this. You still have an outlier problem.
Like, it's not so much weird stuff. I mean, you know, the weird
stuff for driving is like pedestrians or something falls
off of a truck. Yeah, you got some stuff to deal with logs and
stuff in the sea. But if you're like out in the open water,
maybe it's just at the time, okay. But the outlier problem is
still there. So like if you so I live in Vancouver, not too far
from where a little pirate ship goes around. It looks a little
different from the other boats. And I could imagine a self
driving system that was trained in, I don't know, LA or
something off the waters of LA comes up to Vancouver, it's never
seen the pirate ship before and you know, goes smack because
it's not in the database. And so yeah, yeah, it's an outlier.
And like, we don't really have the data for how hard that is.
I mean, another lesson I think of AI of the last decade is what
looks hard, I mean, it's really a lesson of AI for many
decades is what looks hard to a person is not necessarily hard
to a machine and vice versa, what looks easy to a person. So a
lot of people thought driving wasn't that hard. And here's
some reasons why you might have thought that like 16 year olds
can do it more or less fine. I mean, they're a little bit
aggressive, but they can mostly do it. So that'd be reason.
Another reason would be like, roads are basically the same
across North America. So if you're not like, talking about
unimproved roads in Afghanistan, you might think, well, you
know, they're all kind of engineered with the same lane
marking signs. And then it turned out, even though a lot of
people had that intuition, and maybe reasonably so, it turned
out that there was just a lot of edge cases, like this unending
cavalcade of edge cases, like I think I mentioned already the
stop sign with a person carrying a stop sign is an edge case.
Another thing that confounded a Tesla a couple weeks ago, is
somebody brought a Tesla to an airplane show, like on a big
runway, lots of planes, you can kind of imagine even if like,
he had never been to one, but you know, people are showing off
their airplanes, somebody pressed summon on their Tesla to have
it come across the parking lot. And it ran into a three and a
half million dollar jet. Oh my gosh, like, you know, that one's
just standing there. Not like the jet was moving. And like, it
just wasn't in the training set. The training set at this point
is huge. Tesla has the biggest training set, you know, of this
kind of data ever assembled in the history of mankind. But
there are still things out of the training set. So it turns
out, there are all kinds of objects nobody anticipated. And,
you know, pedestrians do weird things or they carry weird
things. So like, maybe your pedestrian is fine for your
image system. And then the pedestrians carrying an umbrella
and your image system is looking for their their eyes and it
can't see anymore because the umbrellas in the way they're
just like unending litany of these cases. So there are
problems that are harder than that we realize because we kind
of automatically compensate for them. And then there are
things like go, which a lot of people thought were hard, but
it turns out you just make up as much data as you need by
self play and deep mind actually solve go in a very robust
fashion. And so there are some problems where the machines are
just way better than people in some the other way around. And
the real issue in my mind is that the public and also the
business world does not understand the difference
between these kinds of problems.
It's hard to understand like hype from reality. You know, there
was this recent Google engineer who claimed that that that
maybe some of the deep learning systems within Google or said
to you, I know that you you had a strong reaction to that.
But I did. Yeah. So I mean, probably by the time this
people watch us, they will all know about this case where this
guy was interacting with one of these large language models and
convinced himself that it was sentient that it like really had
feelings and emotions. And you know, he said it should be
treated like a colleague rather than in like an employee rather
than than like a piece of software, right? I mean, we have no
problem turning off Excel, but are we allowed to turn off
lambda? Yeah, the question is asking, I think yes, you can
turn off lambda because really, it is just like Excel is just
doing a bunch of computations on a bunch of numbers is really
all it's doing. It doesn't actually have connection to
reality. I used in this article is called nonsense on stilts. I
use as an example of sentences with something like ask the
system, what do you do with your spare time? And it's like, I
like to hang out with my friends and family and do good things
for the world. And the system does not have friends. It does
not have family. It does not know what a good deed is in the
world. I made a joke. And I sort of half joke. I said, you
know, it's a good thing. It's just a statistical approximator
because otherwise we would think that this thing is a sociopath
because it was like making up friends and uttering platitudes
to make you like it. Except that it's not really doesn't care if
you like it. It's just autocomplete is all this system
is the kind of autocomplete the complete its own sentences and
yours. But like autocomplete is predicting the next word in
sequences. So when it says, you know, I like to hang out with
my family, it's not like there's a representation there in the
computer of like Peter, Paul and Mary or its relatives. And it's
like thinking more and thoughts about it's just it's taken this
word you you could understand. I can understand how the school
engineer like you want to believe when you're interacting
something. I mean, one of my my one of my favorite movies is her,
which I think is a beautiful movie. And you you want to believe
that this interaction you're having is is more than than
than just a body, even when you're dealing with a person,
sometimes you ascribe things to this person or you love this
person more than they are it's warranted. So I could see how
like it's such a see how this could happen.
Well, and I think it will and it already has. Yeah. In fact, in
that book rebooting AI that I mentioned, we talked about what
we call the gullibility gap. And the gullibility gap is really
a form of anthropomorphization, where we see in things things
that are not there. So you look in the moon, right, you see a
cloud. Yeah, yeah, you see a face in the moon or you know,
clouds or something called pareidolia, right? Another
example is potato like nixie potato and mother Teresa,
yeah, potato, right? And hopefully, you know, your
rational world is the rational self is is, you know, strong
enough to know that that's not real. But I'll give you another
example. Right now. This is a weird example. But right now,
all I see is a two dimensional version of you. And I'm
ascribing a three dimensional version. Yep. And that's okay
because I met you in real life. And it turns out it's real.
But I will do that for a character in a movie. And I
will cry when that character dies, right? You know, and like,
they didn't really die. Yeah. Like, I remember this movie,
fried green tomatoes, which kind of dates me, I suppose, but
like, yeah, beautiful died in every act. Yeah, it also has a
great line, face it girls, I'm older and have more insurance.
I love that part. Yeah. But so, you know, or you take joy
when when she says that to the teenagers, and face it girls,
I'm older, I'm an insurance, no real person said that a
screenwriter wrote it. The actress delivered it masterfully
and we love it. But it's also an illusion. And it is an
illusion of a different sort when this machine predicting
next word says the sentence that you wanted to and then like
he did some editing made him like he he kind of escalated
the illusion to himself. But it's, you know, like, I feel a
little bit bad for him. Like, I think that it is a very
normal thing to get sucked in. If he hadn't been a Google
engineer, probably, people would be completely sympathetic.
And they're kind of like, well, since he's a Google
engineer, he should know better. And there's there's some
elements of that. But I mean, you see like, psychiatrists
that fall in love with their patients and stuff like that.
Or fall in love with their their computer psychiatrists. So
the classic example of this is Eliza in 1965 was a so called
Rogerian therapist, which basically, no matter what you
say, just ask you questions, never gives you any advice.
Yeah, you know, you say, I'm having a fight with my
girlfriend. And it says, Oh, tell me more about your
girlfriend. Yeah, you know, you say, Well, it was about
dinner. And they're like, Well, you know, do you often have
dinner together? Whatever. And like, it was just matching
words, like, yep, girlfriend, relationship, dinner, with no
clue what it was talking about. But people still got sucked
in. And, you know, another way to think about it is when we
evolved, we didn't have to discriminate humans from
machines, we had to discriminate machine, I mean,
humans from lions. So we get out of the way fast. If you
think about evolutionary psychology, but we did, there
was no thing, you know, for our ancestors to make sure they
didn't get tricked by a bot, right? And so we don't have the
kind of biology to help us do this. And we don't have
training in schools. I could teach a class if anybody
wanted to hire me, I tell you how to spot them. But, you
know, most people don't know. There's been you, you and
Scott Alexander, the author of the Slate Star Codex blog
have been going back and forth on having different
discussions and different opinions about both the
current state of AI and the future state of AI. We're
explaining to me his point of view, the best you can, that
where you guys might have some differences. So I guess
there's a couple places where we've differed. I mean, we've
had a bunch of back and forth lately on his blog and on
my blog. It's like when they used to go from happy days to
Laverne and Shirley, like back and forth between them. So
we've now you're really dating yourself. Exactly. We're
going going back and forth between our two shows, so to
speak. I think it's called Astral Codex 10 or something
like that. Mine is GaryMarcus.subsect.com. And in the
first one, he wrote this really funny thing about the
state of AI and how the dialogue goes. And it's like
somebody comes up with somebody, something really cool.
And then somebody else and he said, usually Gary Marcus.
And it's true that it is usually Gary Marcus. It was a
very funny line, which, you know, I thought was funny and
the field thought was funny. Usually Gary Marcus points
out something wrong. Astros on that. It's usually Gary
Marcus and my buddy Ernie Davis. We write all this stuff
together. But anyway, I'm on Twitter more, so people
know that. Ernie, but so, you know, I've written some
piece on that, but most of them, it's usually me and
Ernie. But so Gary and Ernie noticed that there's
something wrong and then people try to improve it and
then it's basically rinse, lather, and repeat. And so
like there's another. And a lot of times when you guys do
point out these things like people fix the bug, you're
like, oh, there's an issue. Sometimes they do and sometimes
they don't. Oh, actually, thank you very much, Gary and
Ernie, for pointing this out. Yeah, I'm still waiting for
the thank you. Yeah. But you know, I'm sure they'll come.
But anyway, I'm not doing it for the thank yous from the
machine learning community. We're a little bit sparse on
the ground. But, but dialectic is a bit like that. And
some things get a little better. In my, well, so in his
view, and he's not in the field, but he's a very smart
person and he reviewed the stuff. And he was careful to
say like, I don't have a PhD in cognitive science like
Gary does. He was very measured and almost sweet about it.
But he said, you know, I look at this and what I see is
these things just keep getting better. And you know, I'm
not, I'm not worried. They're getting there. And the rate
might, people might, his, his, his issue is, or his
argument is, well, you can argue about the rate of it
getting better, but there's some forward progress or
something. So that was basically the argument. And
it's not unreasonable. But I, you know, I've got my own
arguments and I came back at him. And I pointed out that
the improvements, not as much as he thinks it is, was
actually a flaw in his kind of statistical procedure,
because you looked at new things, I mean, sorry,
things where there were errors before and show that
they got better, but he didn't look at the things where
that they got worse actually worse now, he didn't do
like a random sample and whatever. And the overall
like, there was definitely improvement from GPT to
GPT three, not so clearly from GPT three to what
we'll call GPT three plus, which is the new thing. He
kind of overestimated how much improvement there was
there. But yes, there's some improvement, but there's
also some core problems. And this is what I think is
important, where they haven't really been progress. And
most of those are around language. So I'll give you
an example from Dolly, which is this thing that takes
text and makes images. It's perfectly good at saying
that an astronaut can ride a horse. But if you tell
it a horse rides an astronaut, which is a much less
probable thing, it won't draw it for you. And you can
actually do some tweaks to get it to do it for you.
But it doesn't really understand the inversion. And
I was doing this as an homage to Steve Pinker, who
has often used the example of man bites dog, which
itself comes from the newspaper business, the old
line of newspaper business is dog bites man isn't
news, happened too many times before, man bites dog,
now that's news. So horse rides astronaut, that's
news. And these guys didn't let me have access to
the system. So I had to do this very indirectly. But I
knew from from what it leaked out that they couldn't
do horse rides astronaut. So I wrote a piece about
that as well. In the subject called.
But I mean, you are a well known researcher, like if
I had a new AI system, I would love you to have
research access to it. So you could like you could
tell me all the areas I need to improve on it, like
it's free. It's free QA. All I can say is you're
not running either open AI or Google AI. Those guys
really don't want me to play with their toys. I wrote
about this too in one of the recent sub stacks with
the why is that just because they're afraid they
have a little PR thing going, where they have now
got people, you know, in some of these companies
thinking that their systems are practically sentient.
Why would they want me to poke holes in that? And so
like their PR game is to make it sound like they're
very close to artificial general intelligence. And
why does that matter? Because artificial general
intelligence when it really comes is a complete
game changer, I think. Yeah. You know, there's so
much of the economy is done by what's the like what's
the reasoning to get people think it's going to
come faster than it is like like they need to raise
their money or something or raising money, getting
talent. Like so I'll take Dolly as an example. It's
really Dolly too, but I'm just gonna call it Dolly.
So Dolly comes out 45 minutes later, Sam Altman
tweets, AGI is going to be wild, suggesting that, you
know, they've made progress towards artificial
general intelligence here. And, you know, timed exactly
to that somebody, I don't know, maybe scientific
American, but I don't remember runs an interview
with like one of the programmers and says, you
know, what we're trying to do at OpenAI is to solve
general intelligence. And, you know, we think this is
a step forward in that direction. You don't want
Gary Marcus looking at your dirty laundry saying, well,
you know, the image synthesis here is really good,
but the language stuff still doesn't really work. Who
are you kidding? They don't want me to, you know,
say that of course, no wall is impregnable. So they,
you know, they promised me access to GPT three, but
they didn't give it to me and I complained on
Twitter and somebody said, Hey, kid, I'll give you,
I'll give you 45 minutes of access. You can do with
it. And I wrote a critique and, you know, I wrote a
piece around that with Ernie Davis called GPT
Bloviator, which we wanted to call GPT three bullshit
artist. And that is basically what it is. And so,
you know, we got some access. And then Scott Aronson
actually gave us a little bit of access to Dolly. And
we figured out that it had the problems that it does
in terms of language and stuff like that with small
amounts of access. There's some other systems have
come out since from Google, like Imogen, where I
publicly asked them, like, you say you are better at
problem X. Can I give you a few examples and try it?
And I get no reply. So, you know, there's been a
shift from real science where people would stand up
and say, Yeah, sure. Look at what I've got. Yeah,
like test my hypothesis or show you wrong or like,
because like these these systems that that have come
out like Dolly or GPT three or GP, whatever GP three
eight, whatever it's going to be in the future, like,
they have some usefulness, like they're not all
about like they do, they have like some in some
areas they they are. And so, I think it is helpful
for them to to to let people because because if you
point out the flaws that people might not even go
for the good things, it's like, okay, here's where
what doesn't work. Here's where it works. Let's let's
use this for now and then let's get better in the
other areas. Yeah, the dirty secret about GPT three,
which is not so much a secret anymore is that it's
kind of like a bull in a china shop. And so, there
are a few hundred startups that have been built on
its technology, but it's not clear to me that any
of them are really thriving. And that the biggest
problem is that these systems are full of toxic
language. They're not very truthy. And you can't
really count on them. So, there are some applications
where I think they're fine. The best one is in my
view, but I don't know all of them, is AI Dungeon. So,
AI Dungeon is like Zork, if you remember those old
video games, again, dating myself to the prehistoric
era, where you would type in text and be like, you
know, it says you see a key and you're like, okay,
take the key, put it in the lock and turn, and maybe
that would be the magic indication. So, imagine
that, but a super fun version where you can talk
about anything. So, you can say, I'm sitting in a
dark bedroom in Vancouver with a coffee mug, and
some guy is asking me weird questions, and then
it'll just continue from there, and then you rip
on that. And if it makes a mistake, so to speak,
there's no cost to that because you're just having
fun. If it says something toxic and it tells you,
you know, it questions your sexuality in a way
that you don't like, you can just turn off the
program and it's fine. But if you put that same
software in a customer service chatbot, let's say,
which you might think it'll work for, but now
you're dealing with a customer over a bank loan,
and now you tell them to do something unpleasant
with their mother, it's not funny anymore.
Yeah, yeah.
I mean, my joke is...
Well, if it's like, let's say you have like Agent
Assist or something, like the auto-complete feature
on, you know, Google Docs or something in the
middle of a sentence that often could, with a
decent amount of accuracy, can complete my sentence
for me. It's all by typing, lost bit, it completes
something faster. It's a human-assisted system.
Well, what it is really is like the best version of
auto-complete that money can buy because it's
trained on a much bigger corpus.
Yeah, but it's basically doing what auto-complete
does. And so, you know, another thing people have
used it for is like copywriting. So we're like
term paperwriting. So like for term paperwriting,
you know, I don't endorse this use, but like it
could actually be pretty good at that. You know,
it probably wouldn't give you an A paper, but
it'll make something that sounds sort of like the
topic and whatever. It's probably going to make a
lot of mistakes. It's not going to be an A paper.
But then like, then a human could go through it
or something, a Jedi kid.
And so maybe human can go through it. And they're
like the commercial question if you want to do it
for anything other than a high school term paper
where maybe the student just doesn't care, which
is a problem with our educational system.
You know, then there's a question of like how
carefully do you have to look at it? Is it worth
your while? And that's just like people have to
do trial and error and see if they can get it to
do what they want. With Dolly, it's a sort of
similar question. It makes these fabulous photos,
but it seems to be hard sometimes to get exactly
what you want. And so if you want to use it,
like give me an idea for a book cover. It's
amazing. If you were wanted like something for
an advertisement, you wanted exactly this thing,
exactly there with this other thing on top of
whatever, you might run into this thing where
it's just too hard to get it to do what you
want and you might get frustrated.
So if you're there, if you had to make like a
prediction, like five years out, 10 years out,
okay, here's, here's where we're going to see
more, a lot of progress said, here's an area that
maybe a lot of people think we're going to see
progress. I don't think we'll see the as much
progress in over the next five or 10 years.
Like, how would you like, and I'm going to put
money on this, like, where would you say, Hey,
Orrin, here's where you should put money on.
So deep fakes are going to be just like
unbelievably good. They already are videos,
audio, all that stuff.
I don't expect that like in five years you could
make a whole movie with the whole plot and that
kind of stuff. But if you wanted to do it scene
by scene or something like that, that stuff's
going to be really, really good.
Yeah. So I could, I could create a famous person
stabbing somebody or something and put it out
there. It'll be, it'll be impossible to
Yeah, exactly. I mean, already it's, you know,
pretty good. So this is not going out on a huge limb
to, I think, to save it in five years.
If that stuff's going to be insanely good,
you know, it's already kind of mind boggling.
Yeah. And it's art, like, you know,
in the Russian invasion, we've already seen
some of this, I think, in both directions,
if I remember correctly.
So as a part of the, like,
the thing now is like, okay, how do we train
society to, to not, every time you see something
to not assume it's real or something like that.
That's hard. That's hard.
And we have, like, kind of weaponized misinformation
teams now, right? And, you know, every government
has one and companies do.
And so, like, even just random people have it,
like they put it out there, they put the
beeps out there.
Yeah. And it's going to be so easy to make those.
Yeah. I had a little poll on my Twitter
account about when you'd have a version of Dolly
for gifts. And I think, you know, most of us,
including me, I guess I didn't make my vote public.
But most of us thought, like, any year,
we'll probably have Dolly for gifts.
You know, little simple animations.
And how's it like, like, if, let's say I, like,
caught on camera, like picking my nose or
something, and it was true, I really did that,
but I could, like, start blaming it on the deep fake.
I was. That must have been a deep fake.
Well, that's Trump's move, right? Fake news.
And, you know, there'll be more fake news.
It'll be more often true when somebody says
fake news that it is fake.
That's going to be a total mess.
It's going to be a blank storm.
I won't use the first word, but you know what I mean.
It's going to be a mess.
So that's one thing that will get a lot better.
Speed track ignition will keep getting better every year.
You'll be able to do it like in a louder car.
And, you know, you'll be able to talk about a few more things
with Siri every year or, you know, Alexa or whatever.
That stuff's going to continue to grow.
It's still in five years, not going to be that smart.
It's still not going to be Samantha.
So, you know, come back to her, the movie that you mentioned.
Samantha really understood like all of what's going on.
So, you know, in one of the opening scenes, she says like,
you know, what's bothering you is like my email.
And he's in, she comes back like two seconds later and says,
well, I notice you have 17,000 messages.
I deleted 2000 of them for you.
These were duplicates or whatever.
And like, you know, we're not going to have machine reading at that level.
There's one thing from Samantha that we won't have in five years
where you can actually trust it to fully organize your email.
Because if you have an important message,
any system right now could easily mess it up.
So, you know, I got a message from you to do this podcast.
That's an important message.
But maybe, you know, I actually, it's a weird week,
but I've got like 20 messages like that.
I'm not always so popular.
But like, yeah, you know, it could have gone to spam.
And I mean, we have problems with spam filters.
Like AI is not going to solve that problem immediately.
Because it still doesn't have enough sophistication.
Like there's, there's a, I think it's x.ai has been promising for years,
just doing your scheduling.
And for a while, I think that humans behind the scenes,
if I remember correctly, I could have the wrong.
Well, I mean, it's incredibly hard.
I mean, I have a extremely accomplished assistant that does my scheduling,
who's extremely smart.
And like, she's a lot smarter than any AI.
Even there, it's like so hard to do it.
It's like, that's a very, very hard task for an AI to do.
Yeah, it's high stakes.
If you miss a meeting, like that really matters.
Yep.
Like that's not a solid problem.
So imagine just how hard it is to do scheduling with a machine
where you have your calendar in front of you, whatever.
But still things come up in, in the software.
Also, you have, you have your own nuances.
Like I like to do this in the morning, or I need some space to mature.
Give me some space to go to the bathroom or whatever it is.
Right.
And, you know, I, I think in five years, humans are still going to be better.
The machines of that, even there's a lot of effort.
And that's like a narrow part of reading your email.
So like what Samantha is doing, it's like way beyond just looking at your
calendars, presumably.
So another part of Samantha that I think is way beyond us right now
is Samantha actually understands human interaction.
And I mean, she understands it so well that the character falls in love with her.
Yeah.
We actually do have software that people are dumb enough to fall in love with now.
Or I'm trying to find a more polite word to have the will to believe
falls in love with now, but there's a level of like social understanding
that Samantha has towards the end.
The critical plot twist depends on her not having one piece of social understanding.
She doesn't really get monogamy without quite giving away the whole film.
But she gets a lot about human interaction and what would make people feel better
and this kind of stuff.
And I don't think we, that we're five years away from that.
I think we're much more than five.
I don't think it's impossible, but it's harder.
So like the paradigms that we have now are like,
I show you a picture of a pencil and I say pencil and the machine learns
the name of some concrete physical object that we can put in a bitmap.
And yeah, something like love or harm or pain or need or, you know,
any of these kinds of psychological terms or justice or abstract political terms,
it's just much harder to push those into the paradigm that we know how to use now.
And so just to me, we actually need different paradigms for some aspects of AI.
So going back to the Scott Alexander state star codex thing,
the other debate that we were having aside from like,
how much progress are we making now and so forth was really like,
do we need to change what we're doing or not?
And ultimately he offered me not quite a bet, but a prediction.
He said, you know, that he thought there was a 60%,
no, a 40% chance that we could get to real general artificial intelligence
just by using the tools we have now, more data and so forth.
Okay.
And I wrote a lengthy reply called paradigm shift or something like that,
where I said, you know, I thought it was more like an 80% chance,
which may not sound so different, 40 versus 80.
But, you know, I walked through why I think the differences are
and why I think it's actually really important that we as a research community
consider paradigm shifts and why I think we probably won't get there
just by adding more data and we do need something substantial.
But the data is important.
There is a sense that like, as we can join these data sets together,
we could potentially solve bigger, bigger problems.
It's like, we have access to a very, very small amount of data.
I think data is critical.
I think it's really interesting that human children become more sophisticated
understanders of the world than any computer is now, even with a lot less data.
I think ultimately, you know, you want to take advantage of whatever data you've got.
But if it's a small amount of data, you still want to be able to do something with it.
I think you know that I built a machine learning company that I sold at Uber.
And when I sold it, I had a conversation with Travis, who was still CEO at that point.
And I was explaining what my company did, which is work with small amounts of data.
And he said, oh, I get it, the data frontier problem.
And he gave the example, which was like, he knew how to put the right amount of cars
in the right place at, let's say, 11 o'clock on a Thursday night,
because he had plenty of data around that.
But there just weren't enough cars at, let's say, three in the morning
for his techniques that he already had to give a reliable answer.
So even Travis, who had more data than anybody ever had on anything at that point,
still ran into like, if you break things down into smaller subcategories.
So the tenderloin at 3 AM on a Thursday, there's not enough data there,
even when you're accumulating massive amounts of data.
So if you're Google, you have enough data for most things.
But even Google actually has this problem.
There are always these cases.
And then like jet on a runway, maybe Tesla just had zero cases in that.
So you need to solve that in a different way by having a general understanding
of what an airplane is, what a large physical object is,
rather than doing it by memorizing this specific case
and looking for a lot of similar cases.
So you don't want to throw away the data that you do have.
It's often extremely useful.
But you also need some paradigms that are a little bit less data driven
than I think the ones we have now.
Yeah. All right, cool.
This has been amazing.
All right. Last question we ask all of our guests.
What conventional wisdom or advice do you think is generally bad advice?
Conventional advice that's generally bad advice.
It's funny that I'm stumped on this one right now.
Because I think there's a lot of it.
How about trust your instincts as a piece of conventional wisdom?
And it's sometimes true.
There was the kind of Malcolm Gladwell part of the story for a while about
like experts don't know anything or they do it in a blink.
And it's not really true.
And in fact, one of the most important things that scientists know
is that for almost any piece of data,
sorry, yeah, for any piece of data, you will have your own theory
and it will seem to fit your own theory.
But if you think about it carefully from someone else's perspective,
you'll realize it could be explained in a different way.
And so if you trust your instincts too much,
you become too in love with your own ideas.
There's an old saying about following love with your own press clippings.
And it's a version of that.
The psychological phenomena there are two well-known ones.
One's called confirmation bias.
So you have a theory, you notice other data for it.
The other one's called motivated reasoning.
So you come up with reasons so you can keep believing what you're believing.
So you don't have to believe that you've made a mistake.
Yeah, guys.
So I'm for gun control or something.
I see you for a gun control.
Then everything looks like it's like I could I could I could justify anything
or or put a reason or you're against it or against it or whatever.
If you're against gun controlling, you see you've all then you say,
we should buy more guns.
Like I heard people say that on NPR.
And so, you know, I mean, I'm for gun control.
You can probably guess and I probably can't even pretend that I'm neutral on it.
But the point is, whichever side you're on, on any hot button issue like that,
or even smaller things, I'll give you a much smaller one,
which is like, who did the more dishes?
If you live in a house with, let's say, two adults that maybe married or whatever they are,
I guarantee you that both people will think that they did more than whatever their fair share is.
And if you add up, you say, give it to me in a percentage score.
The person add up to like 130 or 140 or something.
Yeah. And if you do it in a group house, like I lived in graduate school,
like five of us is going to add up to like 270.
Right.
Like the dishes aren't really getting done.
Or imagine if like your, your, your, your, your baseball team or something like that.
I imagine nobody thinks like the ref is super fair to them, right?
They always say the ref is always fair to the other side.
Yeah, every side you're on.
Exactly.
So there's all these kinds of biases and stuff like that.
Is if you trust your own instincts, you're like, I know what that call was.
I mean, he was out.
I mean, who are we kidding?
That guy was right.
Not really.
I wasn't out.
Who are you kidding?
You're right.
And so, so I think there's, there's value in knowing and calibrating your own instincts,
but there's also value in thinking about alternative hypotheses.
And, you know, maybe the other person's right.
And that could be on a scientific matter.
It can be on the dishes.
It can be on your, you know, on the calls in your sports game.
We got all this polarization in the world because we're naturally inclined to believe
that we are correct and to not take the other guys view seriously.
And so that'll be the conventional wisdom.
I will challenge for you.
This is amazing.
I follow you at Gary Marcus on Twitter.
Is that the best place for our audience to engage with you?
I would say that.
And now I have this thing.
Gary Marcus.
That's substack.com.
Gary Marcus.
That's substack.com.
Yeah, which I also like.
So yeah.
All right.
This is amazing.
Well, thank you very much, Gary.
Really appreciate you joining us.
Yeah, thanks so much for working with us today.
