{"text": " Welcome to the first lecture of Machine Learning on Columbia X. I'm your professor, John Paisley. I'm a member of the Department of Electrical Engineering as well as the Data Science Institute at Columbia and my specialty is in machine learning and this is this class that we'll be working through in the following lectures is directly based on the machine learning course that I teach here at Columbia as part of the Data Science Institute. So in this first lecture I want to primarily focus on a general overview of the course and of what machine learning is and then go into a little bit of detail on a very specific problem working with multivariate Gaussians in order to kind of highlight the different aspects and the different components of what we'll be discussing in this course. So this course will cover model-based techniques for extracting information from data with some sort of an end task in mind. So these tasks could include for example predicting an unknown output given some corresponding input. We could simply want to uncover the information underlying our data set with the goal of better understanding what's contained in our data that we have or we could do things like data driven recommendation, grouping, classification, ranking, etc. So using the data to help us learn how to perform these sorts of end tasks. So in a course like this there's a few ways that the information can be presented, different orderings of the information. One example would be to partition it as in one half supervised learning, the other half unsupervised learning and I'll discuss that in a bit more detail in this lecture because that's the perspective that we're going to take. But we could also think in terms of probabilistic models where we are working with probability distributions versus non probabilistic models where we're learning from the data without any sort of probability probabilistic motivation. There's also a dichotomy between modeling approaches. So what is the model that we want to define for our data versus optimization approaches which is very tightly linked with modeling approaches. But with optimization now that we've defined a model, how do we learn the model? So these are two separate problems with various techniques in these two sub problems. So again we're going to partition this course into roughly two halves. The first half will focus on supervised learning and the second half will focus on unsupervised learning and these additional ideas such as probabilistic versus non probabilistic or modeling approaches versus optimization techniques will be sort of discussed as we go along as needed. So those will be interwoven throughout the course but the first part of this course will be strictly supervised learning and the second part will be on supervised learning. What do we mean when we say we want to perform supervised learning? In a nutshell what we're saying is that we want to take inputs and predict corresponding outputs. So for example if we do want to do regression we might have pairs of data in this case a one-dimensional value for x and a corresponding one-dimensional value for t and then we would want to learn some sort of a function so that we input x and we make a prediction for the output t. So for example here we have several data points as indicated by circles where the x-axis is the input for that particular point and the t-axis is the corresponding output and now that we have this data set of these several points we want to define some sort of a regression function for example this blue line that in some way interpolates the outputs as a function of the inputs and then the goal is given this smoothing function that we've learned for some new input x we want to predict the corresponding output t so we for future time points we obtain x we obtain an input and we want to predict the corresponding output so that's regression we say it's regression because the outputs are assumed to be real valued. Classification is another supervised learning problem that is slightly different the form or the structure is very similar we have pairs of inputs and outputs and we get this data set which has many of these pairs of inputs and outputs and we want to learn some functions so that in the future when we get a new input for which we don't have the output we can make a prediction of the output that's going to be accurate however the key difference here is that where with regression the output is a real valued output with classification it's a discrete valued thing so it's a category or a class so in this right plot what I'm showing are input output pairs except now the input is a two-dimensional vector so here the input would be this two-dimensional point and the output for this plot is being encoded by a color so the output could be one of two values either a blue value or a an orange value so in this case we want to take our data inputs and classify them into one of two outputs so we get a data set like this with all of these input locations and the corresponding color-coded output and now our goal is to learn some sort of a function a classifier so that we can partition the space such as is shown here where for a brand new point any of these points that we don't have the output we can evaluate the function at that point and make a prediction of the output so we might say for this data set we would partition this entire region here these two regions into the orange class and this region here into the blue class so any new points falling in this region will be assigned to the blue class so the key here with supervised learning is that we're learning an algorithm based on a function mapping from input to output we the outputs are basically going to be telling us how to map the inputs so that we have an accurate function so we have input output pairs so to look at a classic example we could think of spam detection given some set of inputs like these two chunks of text we would want to assign it a label plus one or minus one sometimes we would say plus one or zero but we would want to sign it one of two possible labels one label would correspond to an email that is spam and we would want to then you know automatically delete that email and the other class would be non spam emails emails that we want to put into our inbox and actually read so it's essentially a filtering problem so for example we might have a data set a data point like this containing this text and we would want to now input this into some sort of a function and say is it spam or not in this case most likely it's not or a data point like this this piece of text where we would input it into the same exact function with the same classifier and in this case that same classifier would say this email is a spam so we classify this email to spam and this email to non spam using the same classifier learned from examples of of labeled spam and labeled non spam emails so essentially the first half of this course is going to be all about learning different ways that we can define these functions to map inputs to outputs either regression models or classification models depending on the problem as well as algorithms or techniques for then learning the parameters of these models based on data so that will take up the first half of this course there are many very useful techniques very different techniques for performing these two tasks they'll entail different ways of thinking about the problems probabilistic versus non probabilistic the models that we define will require different ways or different techniques for learning them so we'll need different optimization techniques so the first half of this course will be all about supervised learning then in the second half of the course we'll transition to unsupervised learning and with unsupervised learning the goal is a bit more vague supervised learning is very nice because we know that we want to map an input to an output and honestly we don't necessarily even care how it's mapped we don't care whether we learn anything by mapping it in many cases we don't perhaps in some cases we do we simply want to say here's my input what should I map it to as an output and we measure the performance based purely on how well it does that task with unsupervised learning we don't have in most cases this sort of an input output mapping we want to perform more abstract or vague tasks such as understanding what is the information in our data set for example we don't have an infinite amount of time to read you know so many thousands or millions of documents so we want a fast algorithmic way for taking in information taking in data and extracting the information for us so for example with unsupervised learning we might want to do something like topic modeling where we have many texts data many documents provided to us we don't have any labels for these documents all we have is the text for each document and then we want to extract what is the underlying what are the underlying themes within these documents so that's the idea of topic modeling we also might want to do recommendations this would be where we have many users and many objects and users will give feedback or input on a subset of these objects either through a review or through some sort of a rating like a star rating for example with Netflix a user could rate a movie one to five stars and we want to take all of this information and learn some sort of a latent space where we can embed users and movies such that users who are close to each other share similar tastes movies that are close to users are somehow appropriate recommendations to be made to those users movies that are close to each other are similar in their content and things that are very far apart are very unlike each other so we want to learn this information simply from the data from the raw data and some model assumption that we have to make so for example one of the most well-known unsupervised learning tasks is the topic modeling problem and so what I'm showing here is an example of what a topic model will learn if you provide it with over a million documents from the New York Times over roughly 20 year period so what we have in these documents is simply a tag that says that this is a document and these are the words in the document and we have that repeated for all of the documents in our data set again it can be over a million of these documents we want to make some sort of a modeling assumption such that we find the words that should somehow cluster together these words would then define topics underlying our data set so for example by simply inputting the raw data from the New York Times making a modeling assumption that doesn't in advance tell us which words should go with which other words we can then run an algorithm to extract information like this that says that this set of words belongs together this set of words belongs together and so on so we can learn for example 10 or more of these what are called topics that tell us which words belong together and then not shown here is also how each document uses that topic so for example for a particular document we might say it's composed of these two topics and no other topics and so this is information that's extracted from the raw data we don't a priori tell the algorithm what it should learn we simply say there is this structure that we want to learn here's the data tell me the structure", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 5.48, "text": " Welcome to the first lecture of Machine Learning on Columbia X. I'm your", "tokens": [50364, 4027, 281, 264, 700, 7991, 295, 22155, 15205, 322, 17339, 1783, 13, 286, 478, 428, 50638], "temperature": 0.0, "avg_logprob": -0.1836628137632858, "compression_ratio": 1.6196581196581197, "no_speech_prob": 0.050144873559474945}, {"id": 1, "seek": 0, "start": 5.48, "end": 11.4, "text": " professor, John Paisley. I'm a member of the Department of Electrical Engineering", "tokens": [50638, 8304, 11, 2619, 430, 1527, 3420, 13, 286, 478, 257, 4006, 295, 264, 5982, 295, 12575, 15888, 16215, 50934], "temperature": 0.0, "avg_logprob": -0.1836628137632858, "compression_ratio": 1.6196581196581197, "no_speech_prob": 0.050144873559474945}, {"id": 2, "seek": 0, "start": 11.4, "end": 16.04, "text": " as well as the Data Science Institute at Columbia and my specialty is in", "tokens": [50934, 382, 731, 382, 264, 11888, 8976, 9446, 412, 17339, 293, 452, 22000, 307, 294, 51166], "temperature": 0.0, "avg_logprob": -0.1836628137632858, "compression_ratio": 1.6196581196581197, "no_speech_prob": 0.050144873559474945}, {"id": 3, "seek": 0, "start": 16.04, "end": 20.12, "text": " machine learning and this is this class that we'll be working through in the", "tokens": [51166, 3479, 2539, 293, 341, 307, 341, 1508, 300, 321, 603, 312, 1364, 807, 294, 264, 51370], "temperature": 0.0, "avg_logprob": -0.1836628137632858, "compression_ratio": 1.6196581196581197, "no_speech_prob": 0.050144873559474945}, {"id": 4, "seek": 0, "start": 20.12, "end": 24.12, "text": " following lectures is directly based on the machine learning course that I", "tokens": [51370, 3480, 16564, 307, 3838, 2361, 322, 264, 3479, 2539, 1164, 300, 286, 51570], "temperature": 0.0, "avg_logprob": -0.1836628137632858, "compression_ratio": 1.6196581196581197, "no_speech_prob": 0.050144873559474945}, {"id": 5, "seek": 2412, "start": 24.12, "end": 30.84, "text": " teach here at Columbia as part of the Data Science Institute. So in this first", "tokens": [50364, 2924, 510, 412, 17339, 382, 644, 295, 264, 11888, 8976, 9446, 13, 407, 294, 341, 700, 50700], "temperature": 0.0, "avg_logprob": -0.10533572327006947, "compression_ratio": 1.6008403361344539, "no_speech_prob": 0.003374942345544696}, {"id": 6, "seek": 2412, "start": 30.84, "end": 36.84, "text": " lecture I want to primarily focus on a general overview of the course and of", "tokens": [50700, 7991, 286, 528, 281, 10029, 1879, 322, 257, 2674, 12492, 295, 264, 1164, 293, 295, 51000], "temperature": 0.0, "avg_logprob": -0.10533572327006947, "compression_ratio": 1.6008403361344539, "no_speech_prob": 0.003374942345544696}, {"id": 7, "seek": 2412, "start": 36.84, "end": 42.6, "text": " what machine learning is and then go into a little bit of detail on a very", "tokens": [51000, 437, 3479, 2539, 307, 293, 550, 352, 666, 257, 707, 857, 295, 2607, 322, 257, 588, 51288], "temperature": 0.0, "avg_logprob": -0.10533572327006947, "compression_ratio": 1.6008403361344539, "no_speech_prob": 0.003374942345544696}, {"id": 8, "seek": 2412, "start": 42.6, "end": 48.52, "text": " specific problem working with multivariate Gaussians in order to kind of", "tokens": [51288, 2685, 1154, 1364, 365, 2120, 592, 3504, 473, 10384, 2023, 2567, 294, 1668, 281, 733, 295, 51584], "temperature": 0.0, "avg_logprob": -0.10533572327006947, "compression_ratio": 1.6008403361344539, "no_speech_prob": 0.003374942345544696}, {"id": 9, "seek": 2412, "start": 48.52, "end": 53.16, "text": " highlight the different aspects and the different components of what we'll be", "tokens": [51584, 5078, 264, 819, 7270, 293, 264, 819, 6677, 295, 437, 321, 603, 312, 51816], "temperature": 0.0, "avg_logprob": -0.10533572327006947, "compression_ratio": 1.6008403361344539, "no_speech_prob": 0.003374942345544696}, {"id": 10, "seek": 5316, "start": 53.16, "end": 59.72, "text": " discussing in this course. So this course will cover model-based techniques", "tokens": [50364, 10850, 294, 341, 1164, 13, 407, 341, 1164, 486, 2060, 2316, 12, 6032, 7512, 50692], "temperature": 0.0, "avg_logprob": -0.15566500406416636, "compression_ratio": 1.6358695652173914, "no_speech_prob": 0.0012444441672414541}, {"id": 11, "seek": 5316, "start": 59.72, "end": 65.92, "text": " for extracting information from data with some sort of an end task in mind. So", "tokens": [50692, 337, 49844, 1589, 490, 1412, 365, 512, 1333, 295, 364, 917, 5633, 294, 1575, 13, 407, 51002], "temperature": 0.0, "avg_logprob": -0.15566500406416636, "compression_ratio": 1.6358695652173914, "no_speech_prob": 0.0012444441672414541}, {"id": 12, "seek": 5316, "start": 65.92, "end": 72.24, "text": " these tasks could include for example predicting an unknown output given some", "tokens": [51002, 613, 9608, 727, 4090, 337, 1365, 32884, 364, 9841, 5598, 2212, 512, 51318], "temperature": 0.0, "avg_logprob": -0.15566500406416636, "compression_ratio": 1.6358695652173914, "no_speech_prob": 0.0012444441672414541}, {"id": 13, "seek": 5316, "start": 72.24, "end": 78.12, "text": " corresponding input. We could simply want to uncover the information", "tokens": [51318, 11760, 4846, 13, 492, 727, 2935, 528, 281, 21694, 264, 1589, 51612], "temperature": 0.0, "avg_logprob": -0.15566500406416636, "compression_ratio": 1.6358695652173914, "no_speech_prob": 0.0012444441672414541}, {"id": 14, "seek": 7812, "start": 78.16000000000001, "end": 84.52000000000001, "text": " underlying our data set with the goal of better understanding what's", "tokens": [50366, 14217, 527, 1412, 992, 365, 264, 3387, 295, 1101, 3701, 437, 311, 50684], "temperature": 0.0, "avg_logprob": -0.21579266919030082, "compression_ratio": 1.5906735751295338, "no_speech_prob": 0.008059362880885601}, {"id": 15, "seek": 7812, "start": 84.52000000000001, "end": 89.32000000000001, "text": " contained in our data that we have or we could do things like data driven", "tokens": [50684, 16212, 294, 527, 1412, 300, 321, 362, 420, 321, 727, 360, 721, 411, 1412, 9555, 50924], "temperature": 0.0, "avg_logprob": -0.21579266919030082, "compression_ratio": 1.5906735751295338, "no_speech_prob": 0.008059362880885601}, {"id": 16, "seek": 7812, "start": 89.32000000000001, "end": 96.88000000000001, "text": " recommendation, grouping, classification, ranking, etc. So using the data to help us", "tokens": [50924, 11879, 11, 40149, 11, 21538, 11, 17833, 11, 5183, 13, 407, 1228, 264, 1412, 281, 854, 505, 51302], "temperature": 0.0, "avg_logprob": -0.21579266919030082, "compression_ratio": 1.5906735751295338, "no_speech_prob": 0.008059362880885601}, {"id": 17, "seek": 7812, "start": 96.88000000000001, "end": 103.88000000000001, "text": " learn how to perform these sorts of end tasks. So in a course like this there's", "tokens": [51302, 1466, 577, 281, 2042, 613, 7527, 295, 917, 9608, 13, 407, 294, 257, 1164, 411, 341, 456, 311, 51652], "temperature": 0.0, "avg_logprob": -0.21579266919030082, "compression_ratio": 1.5906735751295338, "no_speech_prob": 0.008059362880885601}, {"id": 18, "seek": 10388, "start": 103.92, "end": 109.32, "text": " a few ways that the information can be presented, different orderings of the", "tokens": [50366, 257, 1326, 2098, 300, 264, 1589, 393, 312, 8212, 11, 819, 1668, 1109, 295, 264, 50636], "temperature": 0.0, "avg_logprob": -0.13026931848418846, "compression_ratio": 1.6812227074235808, "no_speech_prob": 0.0005883345729671419}, {"id": 19, "seek": 10388, "start": 109.32, "end": 115.96, "text": " information. One example would be to partition it as in one half supervised", "tokens": [50636, 1589, 13, 1485, 1365, 576, 312, 281, 24808, 309, 382, 294, 472, 1922, 46533, 50968], "temperature": 0.0, "avg_logprob": -0.13026931848418846, "compression_ratio": 1.6812227074235808, "no_speech_prob": 0.0005883345729671419}, {"id": 20, "seek": 10388, "start": 115.96, "end": 120.12, "text": " learning, the other half unsupervised learning and I'll discuss that in a bit", "tokens": [50968, 2539, 11, 264, 661, 1922, 2693, 12879, 24420, 2539, 293, 286, 603, 2248, 300, 294, 257, 857, 51176], "temperature": 0.0, "avg_logprob": -0.13026931848418846, "compression_ratio": 1.6812227074235808, "no_speech_prob": 0.0005883345729671419}, {"id": 21, "seek": 10388, "start": 120.12, "end": 124.03999999999999, "text": " more detail in this lecture because that's the perspective that we're going", "tokens": [51176, 544, 2607, 294, 341, 7991, 570, 300, 311, 264, 4585, 300, 321, 434, 516, 51372], "temperature": 0.0, "avg_logprob": -0.13026931848418846, "compression_ratio": 1.6812227074235808, "no_speech_prob": 0.0005883345729671419}, {"id": 22, "seek": 10388, "start": 124.03999999999999, "end": 129.88, "text": " to take. But we could also think in terms of probabilistic models where we are", "tokens": [51372, 281, 747, 13, 583, 321, 727, 611, 519, 294, 2115, 295, 31959, 3142, 5245, 689, 321, 366, 51664], "temperature": 0.0, "avg_logprob": -0.13026931848418846, "compression_ratio": 1.6812227074235808, "no_speech_prob": 0.0005883345729671419}, {"id": 23, "seek": 12988, "start": 129.88, "end": 134.44, "text": " working with probability distributions versus non probabilistic models where", "tokens": [50364, 1364, 365, 8482, 37870, 5717, 2107, 31959, 3142, 5245, 689, 50592], "temperature": 0.0, "avg_logprob": -0.13684770935460142, "compression_ratio": 1.7934272300469483, "no_speech_prob": 0.002631271257996559}, {"id": 24, "seek": 12988, "start": 134.44, "end": 140.88, "text": " we're learning from the data without any sort of probability probabilistic", "tokens": [50592, 321, 434, 2539, 490, 264, 1412, 1553, 604, 1333, 295, 8482, 31959, 3142, 50914], "temperature": 0.0, "avg_logprob": -0.13684770935460142, "compression_ratio": 1.7934272300469483, "no_speech_prob": 0.002631271257996559}, {"id": 25, "seek": 12988, "start": 140.88, "end": 146.28, "text": " motivation. There's also a dichotomy between modeling approaches. So what is", "tokens": [50914, 12335, 13, 821, 311, 611, 257, 10390, 310, 8488, 1296, 15983, 11587, 13, 407, 437, 307, 51184], "temperature": 0.0, "avg_logprob": -0.13684770935460142, "compression_ratio": 1.7934272300469483, "no_speech_prob": 0.002631271257996559}, {"id": 26, "seek": 12988, "start": 146.28, "end": 152.72, "text": " the model that we want to define for our data versus optimization approaches", "tokens": [51184, 264, 2316, 300, 321, 528, 281, 6964, 337, 527, 1412, 5717, 19618, 11587, 51506], "temperature": 0.0, "avg_logprob": -0.13684770935460142, "compression_ratio": 1.7934272300469483, "no_speech_prob": 0.002631271257996559}, {"id": 27, "seek": 12988, "start": 152.72, "end": 157.0, "text": " which is very tightly linked with modeling approaches. But with optimization", "tokens": [51506, 597, 307, 588, 21952, 9408, 365, 15983, 11587, 13, 583, 365, 19618, 51720], "temperature": 0.0, "avg_logprob": -0.13684770935460142, "compression_ratio": 1.7934272300469483, "no_speech_prob": 0.002631271257996559}, {"id": 28, "seek": 15700, "start": 157.28, "end": 162.32, "text": " now that we've defined a model, how do we learn the model? So these are two", "tokens": [50378, 586, 300, 321, 600, 7642, 257, 2316, 11, 577, 360, 321, 1466, 264, 2316, 30, 407, 613, 366, 732, 50630], "temperature": 0.0, "avg_logprob": -0.1387859712164086, "compression_ratio": 1.7242990654205608, "no_speech_prob": 0.0002453593770042062}, {"id": 29, "seek": 15700, "start": 162.32, "end": 168.24, "text": " separate problems with various techniques in these two sub problems. So again", "tokens": [50630, 4994, 2740, 365, 3683, 7512, 294, 613, 732, 1422, 2740, 13, 407, 797, 50926], "temperature": 0.0, "avg_logprob": -0.1387859712164086, "compression_ratio": 1.7242990654205608, "no_speech_prob": 0.0002453593770042062}, {"id": 30, "seek": 15700, "start": 168.24, "end": 172.96, "text": " we're going to partition this course into roughly two halves. The first half", "tokens": [50926, 321, 434, 516, 281, 24808, 341, 1164, 666, 9810, 732, 38490, 13, 440, 700, 1922, 51162], "temperature": 0.0, "avg_logprob": -0.1387859712164086, "compression_ratio": 1.7242990654205608, "no_speech_prob": 0.0002453593770042062}, {"id": 31, "seek": 15700, "start": 172.96, "end": 177.6, "text": " will focus on supervised learning and the second half will focus on", "tokens": [51162, 486, 1879, 322, 46533, 2539, 293, 264, 1150, 1922, 486, 1879, 322, 51394], "temperature": 0.0, "avg_logprob": -0.1387859712164086, "compression_ratio": 1.7242990654205608, "no_speech_prob": 0.0002453593770042062}, {"id": 32, "seek": 15700, "start": 177.6, "end": 182.68, "text": " unsupervised learning and these additional ideas such as probabilistic", "tokens": [51394, 2693, 12879, 24420, 2539, 293, 613, 4497, 3487, 1270, 382, 31959, 3142, 51648], "temperature": 0.0, "avg_logprob": -0.1387859712164086, "compression_ratio": 1.7242990654205608, "no_speech_prob": 0.0002453593770042062}, {"id": 33, "seek": 18268, "start": 182.72, "end": 187.56, "text": " versus non probabilistic or modeling approaches versus optimization techniques", "tokens": [50366, 5717, 2107, 31959, 3142, 420, 15983, 11587, 5717, 19618, 7512, 50608], "temperature": 0.0, "avg_logprob": -0.1445051312446594, "compression_ratio": 1.7666666666666666, "no_speech_prob": 0.00022340301075018942}, {"id": 34, "seek": 18268, "start": 187.56, "end": 194.36, "text": " will be sort of discussed as we go along as needed. So those will be", "tokens": [50608, 486, 312, 1333, 295, 7152, 382, 321, 352, 2051, 382, 2978, 13, 407, 729, 486, 312, 50948], "temperature": 0.0, "avg_logprob": -0.1445051312446594, "compression_ratio": 1.7666666666666666, "no_speech_prob": 0.00022340301075018942}, {"id": 35, "seek": 18268, "start": 194.36, "end": 197.92000000000002, "text": " interwoven throughout the course but the first part of this course will be", "tokens": [50948, 728, 6120, 553, 3710, 264, 1164, 457, 264, 700, 644, 295, 341, 1164, 486, 312, 51126], "temperature": 0.0, "avg_logprob": -0.1445051312446594, "compression_ratio": 1.7666666666666666, "no_speech_prob": 0.00022340301075018942}, {"id": 36, "seek": 18268, "start": 199.04000000000002, "end": 202.56, "text": " strictly supervised learning and the second part will be on supervised", "tokens": [51182, 20792, 46533, 2539, 293, 264, 1150, 644, 486, 312, 322, 46533, 51358], "temperature": 0.0, "avg_logprob": -0.1445051312446594, "compression_ratio": 1.7666666666666666, "no_speech_prob": 0.00022340301075018942}, {"id": 37, "seek": 18268, "start": 202.56, "end": 208.0, "text": " learning. What do we mean when we say we want to perform supervised learning?", "tokens": [51358, 2539, 13, 708, 360, 321, 914, 562, 321, 584, 321, 528, 281, 2042, 46533, 2539, 30, 51630], "temperature": 0.0, "avg_logprob": -0.1445051312446594, "compression_ratio": 1.7666666666666666, "no_speech_prob": 0.00022340301075018942}, {"id": 38, "seek": 20800, "start": 208.92, "end": 215.56, "text": " In a nutshell what we're saying is that we want to take inputs and predict", "tokens": [50410, 682, 257, 37711, 437, 321, 434, 1566, 307, 300, 321, 528, 281, 747, 15743, 293, 6069, 50742], "temperature": 0.0, "avg_logprob": -0.14518868923187256, "compression_ratio": 1.7085714285714286, "no_speech_prob": 0.0003920108138117939}, {"id": 39, "seek": 20800, "start": 215.56, "end": 221.52, "text": " corresponding outputs. So for example if we do want to do regression we might", "tokens": [50742, 11760, 23930, 13, 407, 337, 1365, 498, 321, 360, 528, 281, 360, 24590, 321, 1062, 51040], "temperature": 0.0, "avg_logprob": -0.14518868923187256, "compression_ratio": 1.7085714285714286, "no_speech_prob": 0.0003920108138117939}, {"id": 40, "seek": 20800, "start": 221.52, "end": 228.52, "text": " have pairs of data in this case a one-dimensional value for x and a", "tokens": [51040, 362, 15494, 295, 1412, 294, 341, 1389, 257, 472, 12, 18759, 2158, 337, 2031, 293, 257, 51390], "temperature": 0.0, "avg_logprob": -0.14518868923187256, "compression_ratio": 1.7085714285714286, "no_speech_prob": 0.0003920108138117939}, {"id": 41, "seek": 20800, "start": 228.52, "end": 235.0, "text": " corresponding one-dimensional value for t and then we would want to learn some", "tokens": [51390, 11760, 472, 12, 18759, 2158, 337, 256, 293, 550, 321, 576, 528, 281, 1466, 512, 51714], "temperature": 0.0, "avg_logprob": -0.14518868923187256, "compression_ratio": 1.7085714285714286, "no_speech_prob": 0.0003920108138117939}, {"id": 42, "seek": 23500, "start": 235.04, "end": 239.32, "text": " sort of a function so that we input x and we make a prediction for the output", "tokens": [50366, 1333, 295, 257, 2445, 370, 300, 321, 4846, 2031, 293, 321, 652, 257, 17630, 337, 264, 5598, 50580], "temperature": 0.0, "avg_logprob": -0.09664035373263889, "compression_ratio": 1.8269230769230769, "no_speech_prob": 0.001501012477092445}, {"id": 43, "seek": 23500, "start": 239.32, "end": 246.48, "text": " t. So for example here we have several data points as indicated by circles", "tokens": [50580, 256, 13, 407, 337, 1365, 510, 321, 362, 2940, 1412, 2793, 382, 16176, 538, 13040, 50938], "temperature": 0.0, "avg_logprob": -0.09664035373263889, "compression_ratio": 1.8269230769230769, "no_speech_prob": 0.001501012477092445}, {"id": 44, "seek": 23500, "start": 246.48, "end": 253.44, "text": " where the x-axis is the input for that particular point and the t-axis is the", "tokens": [50938, 689, 264, 2031, 12, 24633, 307, 264, 4846, 337, 300, 1729, 935, 293, 264, 256, 12, 24633, 307, 264, 51286], "temperature": 0.0, "avg_logprob": -0.09664035373263889, "compression_ratio": 1.8269230769230769, "no_speech_prob": 0.001501012477092445}, {"id": 45, "seek": 23500, "start": 253.44, "end": 258.72, "text": " corresponding output and now that we have this data set of these several", "tokens": [51286, 11760, 5598, 293, 586, 300, 321, 362, 341, 1412, 992, 295, 613, 2940, 51550], "temperature": 0.0, "avg_logprob": -0.09664035373263889, "compression_ratio": 1.8269230769230769, "no_speech_prob": 0.001501012477092445}, {"id": 46, "seek": 23500, "start": 258.72, "end": 264.72, "text": " points we want to define some sort of a regression function for example this", "tokens": [51550, 2793, 321, 528, 281, 6964, 512, 1333, 295, 257, 24590, 2445, 337, 1365, 341, 51850], "temperature": 0.0, "avg_logprob": -0.09664035373263889, "compression_ratio": 1.8269230769230769, "no_speech_prob": 0.001501012477092445}, {"id": 47, "seek": 26472, "start": 264.76000000000005, "end": 270.56, "text": " blue line that in some way interpolates the outputs as a function of the inputs", "tokens": [50366, 3344, 1622, 300, 294, 512, 636, 44902, 1024, 264, 23930, 382, 257, 2445, 295, 264, 15743, 50656], "temperature": 0.0, "avg_logprob": -0.1554814299491987, "compression_ratio": 1.7928994082840237, "no_speech_prob": 0.0003459547588136047}, {"id": 48, "seek": 26472, "start": 271.76000000000005, "end": 277.48, "text": " and then the goal is given this smoothing function that we've learned for some new", "tokens": [50716, 293, 550, 264, 3387, 307, 2212, 341, 899, 6259, 571, 2445, 300, 321, 600, 3264, 337, 512, 777, 51002], "temperature": 0.0, "avg_logprob": -0.1554814299491987, "compression_ratio": 1.7928994082840237, "no_speech_prob": 0.0003459547588136047}, {"id": 49, "seek": 26472, "start": 277.48, "end": 284.20000000000005, "text": " input x we want to predict the corresponding output t so we for future", "tokens": [51002, 4846, 2031, 321, 528, 281, 6069, 264, 11760, 5598, 256, 370, 321, 337, 2027, 51338], "temperature": 0.0, "avg_logprob": -0.1554814299491987, "compression_ratio": 1.7928994082840237, "no_speech_prob": 0.0003459547588136047}, {"id": 50, "seek": 26472, "start": 284.84000000000003, "end": 290.16, "text": " time points we obtain x we obtain an input and we want to predict the", "tokens": [51370, 565, 2793, 321, 12701, 2031, 321, 12701, 364, 4846, 293, 321, 528, 281, 6069, 264, 51636], "temperature": 0.0, "avg_logprob": -0.1554814299491987, "compression_ratio": 1.7928994082840237, "no_speech_prob": 0.0003459547588136047}, {"id": 51, "seek": 29016, "start": 290.20000000000005, "end": 295.24, "text": " corresponding output so that's regression we say it's regression because the", "tokens": [50366, 11760, 5598, 370, 300, 311, 24590, 321, 584, 309, 311, 24590, 570, 264, 50618], "temperature": 0.0, "avg_logprob": -0.15989260025966315, "compression_ratio": 1.7867298578199051, "no_speech_prob": 0.004467797465622425}, {"id": 52, "seek": 29016, "start": 295.24, "end": 301.88000000000005, "text": " outputs are assumed to be real valued. Classification is another supervised", "tokens": [50618, 23930, 366, 15895, 281, 312, 957, 22608, 13, 9471, 3774, 307, 1071, 46533, 50950], "temperature": 0.0, "avg_logprob": -0.15989260025966315, "compression_ratio": 1.7867298578199051, "no_speech_prob": 0.004467797465622425}, {"id": 53, "seek": 29016, "start": 301.88000000000005, "end": 307.36, "text": " learning problem that is slightly different the form or the structure is", "tokens": [50950, 2539, 1154, 300, 307, 4748, 819, 264, 1254, 420, 264, 3877, 307, 51224], "temperature": 0.0, "avg_logprob": -0.15989260025966315, "compression_ratio": 1.7867298578199051, "no_speech_prob": 0.004467797465622425}, {"id": 54, "seek": 29016, "start": 308.08000000000004, "end": 313.88, "text": " very similar we have pairs of inputs and outputs and we get this data set which", "tokens": [51260, 588, 2531, 321, 362, 15494, 295, 15743, 293, 23930, 293, 321, 483, 341, 1412, 992, 597, 51550], "temperature": 0.0, "avg_logprob": -0.15989260025966315, "compression_ratio": 1.7867298578199051, "no_speech_prob": 0.004467797465622425}, {"id": 55, "seek": 29016, "start": 313.88, "end": 317.32000000000005, "text": " has many of these pairs of inputs and outputs and we want to learn some", "tokens": [51550, 575, 867, 295, 613, 15494, 295, 15743, 293, 23930, 293, 321, 528, 281, 1466, 512, 51722], "temperature": 0.0, "avg_logprob": -0.15989260025966315, "compression_ratio": 1.7867298578199051, "no_speech_prob": 0.004467797465622425}, {"id": 56, "seek": 31732, "start": 317.36, "end": 321.71999999999997, "text": " functions so that in the future when we get a new input for which we don't have", "tokens": [50366, 6828, 370, 300, 294, 264, 2027, 562, 321, 483, 257, 777, 4846, 337, 597, 321, 500, 380, 362, 50584], "temperature": 0.0, "avg_logprob": -0.14451926687489386, "compression_ratio": 1.7792792792792793, "no_speech_prob": 0.0023963775020092726}, {"id": 57, "seek": 31732, "start": 321.71999999999997, "end": 325.56, "text": " the output we can make a prediction of the output that's going to be accurate", "tokens": [50584, 264, 5598, 321, 393, 652, 257, 17630, 295, 264, 5598, 300, 311, 516, 281, 312, 8559, 50776], "temperature": 0.0, "avg_logprob": -0.14451926687489386, "compression_ratio": 1.7792792792792793, "no_speech_prob": 0.0023963775020092726}, {"id": 58, "seek": 31732, "start": 327.04, "end": 331.64, "text": " however the key difference here is that where with regression the output is a", "tokens": [50850, 4461, 264, 2141, 2649, 510, 307, 300, 689, 365, 24590, 264, 5598, 307, 257, 51080], "temperature": 0.0, "avg_logprob": -0.14451926687489386, "compression_ratio": 1.7792792792792793, "no_speech_prob": 0.0023963775020092726}, {"id": 59, "seek": 31732, "start": 331.64, "end": 339.15999999999997, "text": " real valued output with classification it's a discrete valued thing so it's a", "tokens": [51080, 957, 22608, 5598, 365, 21538, 309, 311, 257, 27706, 22608, 551, 370, 309, 311, 257, 51456], "temperature": 0.0, "avg_logprob": -0.14451926687489386, "compression_ratio": 1.7792792792792793, "no_speech_prob": 0.0023963775020092726}, {"id": 60, "seek": 31732, "start": 339.15999999999997, "end": 345.28, "text": " category or a class so in this right plot what I'm showing are input output pairs", "tokens": [51456, 7719, 420, 257, 1508, 370, 294, 341, 558, 7542, 437, 286, 478, 4099, 366, 4846, 5598, 15494, 51762], "temperature": 0.0, "avg_logprob": -0.14451926687489386, "compression_ratio": 1.7792792792792793, "no_speech_prob": 0.0023963775020092726}, {"id": 61, "seek": 34528, "start": 346.11999999999995, "end": 351.64, "text": " except now the input is a two-dimensional vector so here the input would be this", "tokens": [50406, 3993, 586, 264, 4846, 307, 257, 732, 12, 18759, 8062, 370, 510, 264, 4846, 576, 312, 341, 50682], "temperature": 0.0, "avg_logprob": -0.13472810158362755, "compression_ratio": 1.7853107344632768, "no_speech_prob": 0.0002453525667078793}, {"id": 62, "seek": 34528, "start": 351.64, "end": 357.4, "text": " two-dimensional point and the output for this plot is being encoded by a color", "tokens": [50682, 732, 12, 18759, 935, 293, 264, 5598, 337, 341, 7542, 307, 885, 2058, 12340, 538, 257, 2017, 50970], "temperature": 0.0, "avg_logprob": -0.13472810158362755, "compression_ratio": 1.7853107344632768, "no_speech_prob": 0.0002453525667078793}, {"id": 63, "seek": 34528, "start": 357.4, "end": 364.84, "text": " so the output could be one of two values either a blue value or a an orange", "tokens": [50970, 370, 264, 5598, 727, 312, 472, 295, 732, 4190, 2139, 257, 3344, 2158, 420, 257, 364, 7671, 51342], "temperature": 0.0, "avg_logprob": -0.13472810158362755, "compression_ratio": 1.7853107344632768, "no_speech_prob": 0.0002453525667078793}, {"id": 64, "seek": 34528, "start": 364.84, "end": 370.23999999999995, "text": " value so in this case we want to take our data inputs and classify them into one", "tokens": [51342, 2158, 370, 294, 341, 1389, 321, 528, 281, 747, 527, 1412, 15743, 293, 33872, 552, 666, 472, 51612], "temperature": 0.0, "avg_logprob": -0.13472810158362755, "compression_ratio": 1.7853107344632768, "no_speech_prob": 0.0002453525667078793}, {"id": 65, "seek": 37024, "start": 370.24, "end": 377.68, "text": " of two outputs so we get a data set like this with all of these input locations", "tokens": [50364, 295, 732, 23930, 370, 321, 483, 257, 1412, 992, 411, 341, 365, 439, 295, 613, 4846, 9253, 50736], "temperature": 0.0, "avg_logprob": -0.1152339103894356, "compression_ratio": 1.6526315789473685, "no_speech_prob": 0.006487159989774227}, {"id": 66, "seek": 37024, "start": 377.68, "end": 383.36, "text": " and the corresponding color-coded output and now our goal is to learn some sort", "tokens": [50736, 293, 264, 11760, 2017, 12, 66, 12340, 5598, 293, 586, 527, 3387, 307, 281, 1466, 512, 1333, 51020], "temperature": 0.0, "avg_logprob": -0.1152339103894356, "compression_ratio": 1.6526315789473685, "no_speech_prob": 0.006487159989774227}, {"id": 67, "seek": 37024, "start": 383.36, "end": 389.92, "text": " of a function a classifier so that we can partition the space such as is shown", "tokens": [51020, 295, 257, 2445, 257, 1508, 9902, 370, 300, 321, 393, 24808, 264, 1901, 1270, 382, 307, 4898, 51348], "temperature": 0.0, "avg_logprob": -0.1152339103894356, "compression_ratio": 1.6526315789473685, "no_speech_prob": 0.006487159989774227}, {"id": 68, "seek": 37024, "start": 389.92, "end": 396.12, "text": " here where for a brand new point any of these points that we don't have the", "tokens": [51348, 510, 689, 337, 257, 3360, 777, 935, 604, 295, 613, 2793, 300, 321, 500, 380, 362, 264, 51658], "temperature": 0.0, "avg_logprob": -0.1152339103894356, "compression_ratio": 1.6526315789473685, "no_speech_prob": 0.006487159989774227}, {"id": 69, "seek": 39612, "start": 396.12, "end": 400.96, "text": " output we can evaluate the function at that point and make a prediction of the", "tokens": [50364, 5598, 321, 393, 13059, 264, 2445, 412, 300, 935, 293, 652, 257, 17630, 295, 264, 50606], "temperature": 0.0, "avg_logprob": -0.11559957776750837, "compression_ratio": 1.8304093567251463, "no_speech_prob": 0.005059388466179371}, {"id": 70, "seek": 39612, "start": 400.96, "end": 405.88, "text": " output so we might say for this data set we would partition this entire region", "tokens": [50606, 5598, 370, 321, 1062, 584, 337, 341, 1412, 992, 321, 576, 24808, 341, 2302, 4458, 50852], "temperature": 0.0, "avg_logprob": -0.11559957776750837, "compression_ratio": 1.8304093567251463, "no_speech_prob": 0.005059388466179371}, {"id": 71, "seek": 39612, "start": 405.88, "end": 414.08, "text": " here these two regions into the orange class and this region here into the blue", "tokens": [50852, 510, 613, 732, 10682, 666, 264, 7671, 1508, 293, 341, 4458, 510, 666, 264, 3344, 51262], "temperature": 0.0, "avg_logprob": -0.11559957776750837, "compression_ratio": 1.8304093567251463, "no_speech_prob": 0.005059388466179371}, {"id": 72, "seek": 39612, "start": 414.08, "end": 418.96, "text": " class so any new points falling in this region will be assigned to the blue", "tokens": [51262, 1508, 370, 604, 777, 2793, 7440, 294, 341, 4458, 486, 312, 13279, 281, 264, 3344, 51506], "temperature": 0.0, "avg_logprob": -0.11559957776750837, "compression_ratio": 1.8304093567251463, "no_speech_prob": 0.005059388466179371}, {"id": 73, "seek": 41896, "start": 418.96, "end": 426.12, "text": " class so the key here with supervised learning is that we're learning an", "tokens": [50364, 1508, 370, 264, 2141, 510, 365, 46533, 2539, 307, 300, 321, 434, 2539, 364, 50722], "temperature": 0.0, "avg_logprob": -0.12157653034597203, "compression_ratio": 1.7272727272727273, "no_speech_prob": 0.006289379205554724}, {"id": 74, "seek": 41896, "start": 426.12, "end": 434.4, "text": " algorithm based on a function mapping from input to output we the outputs are", "tokens": [50722, 9284, 2361, 322, 257, 2445, 18350, 490, 4846, 281, 5598, 321, 264, 23930, 366, 51136], "temperature": 0.0, "avg_logprob": -0.12157653034597203, "compression_ratio": 1.7272727272727273, "no_speech_prob": 0.006289379205554724}, {"id": 75, "seek": 41896, "start": 434.4, "end": 438.64, "text": " basically going to be telling us how to map the inputs so that we have an", "tokens": [51136, 1936, 516, 281, 312, 3585, 505, 577, 281, 4471, 264, 15743, 370, 300, 321, 362, 364, 51348], "temperature": 0.0, "avg_logprob": -0.12157653034597203, "compression_ratio": 1.7272727272727273, "no_speech_prob": 0.006289379205554724}, {"id": 76, "seek": 41896, "start": 438.64, "end": 447.32, "text": " accurate function so we have input output pairs so to look at a classic example", "tokens": [51348, 8559, 2445, 370, 321, 362, 4846, 5598, 15494, 370, 281, 574, 412, 257, 7230, 1365, 51782], "temperature": 0.0, "avg_logprob": -0.12157653034597203, "compression_ratio": 1.7272727272727273, "no_speech_prob": 0.006289379205554724}, {"id": 77, "seek": 44732, "start": 447.32, "end": 453.84, "text": " we could think of spam detection given some set of inputs like these two chunks", "tokens": [50364, 321, 727, 519, 295, 24028, 17784, 2212, 512, 992, 295, 15743, 411, 613, 732, 24004, 50690], "temperature": 0.0, "avg_logprob": -0.12835130586728946, "compression_ratio": 1.893719806763285, "no_speech_prob": 0.01132909208536148}, {"id": 78, "seek": 44732, "start": 453.84, "end": 459.0, "text": " of text we would want to assign it a label plus one or minus one sometimes", "tokens": [50690, 295, 2487, 321, 576, 528, 281, 6269, 309, 257, 7645, 1804, 472, 420, 3175, 472, 2171, 50948], "temperature": 0.0, "avg_logprob": -0.12835130586728946, "compression_ratio": 1.893719806763285, "no_speech_prob": 0.01132909208536148}, {"id": 79, "seek": 44732, "start": 459.0, "end": 463.15999999999997, "text": " we would say plus one or zero but we would want to sign it one of two possible", "tokens": [50948, 321, 576, 584, 1804, 472, 420, 4018, 457, 321, 576, 528, 281, 1465, 309, 472, 295, 732, 1944, 51156], "temperature": 0.0, "avg_logprob": -0.12835130586728946, "compression_ratio": 1.893719806763285, "no_speech_prob": 0.01132909208536148}, {"id": 80, "seek": 44732, "start": 463.15999999999997, "end": 469.4, "text": " labels one label would correspond to an email that is spam and we would want to", "tokens": [51156, 16949, 472, 7645, 576, 6805, 281, 364, 3796, 300, 307, 24028, 293, 321, 576, 528, 281, 51468], "temperature": 0.0, "avg_logprob": -0.12835130586728946, "compression_ratio": 1.893719806763285, "no_speech_prob": 0.01132909208536148}, {"id": 81, "seek": 44732, "start": 469.4, "end": 476.2, "text": " then you know automatically delete that email and the other class would be non", "tokens": [51468, 550, 291, 458, 6772, 12097, 300, 3796, 293, 264, 661, 1508, 576, 312, 2107, 51808], "temperature": 0.0, "avg_logprob": -0.12835130586728946, "compression_ratio": 1.893719806763285, "no_speech_prob": 0.01132909208536148}, {"id": 82, "seek": 47620, "start": 476.24, "end": 482.03999999999996, "text": " spam emails emails that we want to put into our inbox and actually read so it's", "tokens": [50366, 24028, 12524, 12524, 300, 321, 528, 281, 829, 666, 527, 35067, 293, 767, 1401, 370, 309, 311, 50656], "temperature": 0.0, "avg_logprob": -0.10542298377828395, "compression_ratio": 1.8436018957345972, "no_speech_prob": 0.015418772585690022}, {"id": 83, "seek": 47620, "start": 482.03999999999996, "end": 488.03999999999996, "text": " essentially a filtering problem so for example we might have a data set a data", "tokens": [50656, 4476, 257, 30822, 1154, 370, 337, 1365, 321, 1062, 362, 257, 1412, 992, 257, 1412, 50956], "temperature": 0.0, "avg_logprob": -0.10542298377828395, "compression_ratio": 1.8436018957345972, "no_speech_prob": 0.015418772585690022}, {"id": 84, "seek": 47620, "start": 488.03999999999996, "end": 492.24, "text": " point like this containing this text and we would want to now input this into", "tokens": [50956, 935, 411, 341, 19273, 341, 2487, 293, 321, 576, 528, 281, 586, 4846, 341, 666, 51166], "temperature": 0.0, "avg_logprob": -0.10542298377828395, "compression_ratio": 1.8436018957345972, "no_speech_prob": 0.015418772585690022}, {"id": 85, "seek": 47620, "start": 492.24, "end": 497.03999999999996, "text": " some sort of a function and say is it spam or not in this case most likely", "tokens": [51166, 512, 1333, 295, 257, 2445, 293, 584, 307, 309, 24028, 420, 406, 294, 341, 1389, 881, 3700, 51406], "temperature": 0.0, "avg_logprob": -0.10542298377828395, "compression_ratio": 1.8436018957345972, "no_speech_prob": 0.015418772585690022}, {"id": 86, "seek": 47620, "start": 497.03999999999996, "end": 503.59999999999997, "text": " it's not or a data point like this this piece of text where we would input it", "tokens": [51406, 309, 311, 406, 420, 257, 1412, 935, 411, 341, 341, 2522, 295, 2487, 689, 321, 576, 4846, 309, 51734], "temperature": 0.0, "avg_logprob": -0.10542298377828395, "compression_ratio": 1.8436018957345972, "no_speech_prob": 0.015418772585690022}, {"id": 87, "seek": 50360, "start": 503.6, "end": 509.36, "text": " into the same exact function with the same classifier and in this case that", "tokens": [50364, 666, 264, 912, 1900, 2445, 365, 264, 912, 1508, 9902, 293, 294, 341, 1389, 300, 50652], "temperature": 0.0, "avg_logprob": -0.1488695010332994, "compression_ratio": 2.0064516129032257, "no_speech_prob": 0.0009697007481008768}, {"id": 88, "seek": 50360, "start": 509.36, "end": 514.44, "text": " same classifier would say this email is a spam so we classify this email to spam", "tokens": [50652, 912, 1508, 9902, 576, 584, 341, 3796, 307, 257, 24028, 370, 321, 33872, 341, 3796, 281, 24028, 50906], "temperature": 0.0, "avg_logprob": -0.1488695010332994, "compression_ratio": 2.0064516129032257, "no_speech_prob": 0.0009697007481008768}, {"id": 89, "seek": 50360, "start": 514.44, "end": 521.84, "text": " and this email to non spam using the same classifier learned from examples of", "tokens": [50906, 293, 341, 3796, 281, 2107, 24028, 1228, 264, 912, 1508, 9902, 3264, 490, 5110, 295, 51276], "temperature": 0.0, "avg_logprob": -0.1488695010332994, "compression_ratio": 2.0064516129032257, "no_speech_prob": 0.0009697007481008768}, {"id": 90, "seek": 50360, "start": 521.84, "end": 528.08, "text": " of labeled spam and labeled non spam emails so essentially the first half of", "tokens": [51276, 295, 21335, 24028, 293, 21335, 2107, 24028, 12524, 370, 4476, 264, 700, 1922, 295, 51588], "temperature": 0.0, "avg_logprob": -0.1488695010332994, "compression_ratio": 2.0064516129032257, "no_speech_prob": 0.0009697007481008768}, {"id": 91, "seek": 52808, "start": 528.08, "end": 533.72, "text": " this course is going to be all about learning different ways that we can", "tokens": [50364, 341, 1164, 307, 516, 281, 312, 439, 466, 2539, 819, 2098, 300, 321, 393, 50646], "temperature": 0.0, "avg_logprob": -0.1341409471299913, "compression_ratio": 1.8589211618257262, "no_speech_prob": 0.019713113084435463}, {"id": 92, "seek": 52808, "start": 533.72, "end": 538.72, "text": " define these functions to map inputs to outputs either regression models or", "tokens": [50646, 6964, 613, 6828, 281, 4471, 15743, 281, 23930, 2139, 24590, 5245, 420, 50896], "temperature": 0.0, "avg_logprob": -0.1341409471299913, "compression_ratio": 1.8589211618257262, "no_speech_prob": 0.019713113084435463}, {"id": 93, "seek": 52808, "start": 538.72, "end": 544.0, "text": " classification models depending on the problem as well as algorithms or", "tokens": [50896, 21538, 5245, 5413, 322, 264, 1154, 382, 731, 382, 14642, 420, 51160], "temperature": 0.0, "avg_logprob": -0.1341409471299913, "compression_ratio": 1.8589211618257262, "no_speech_prob": 0.019713113084435463}, {"id": 94, "seek": 52808, "start": 544.0, "end": 547.96, "text": " techniques for then learning the parameters of these models based on data", "tokens": [51160, 7512, 337, 550, 2539, 264, 9834, 295, 613, 5245, 2361, 322, 1412, 51358], "temperature": 0.0, "avg_logprob": -0.1341409471299913, "compression_ratio": 1.8589211618257262, "no_speech_prob": 0.019713113084435463}, {"id": 95, "seek": 52808, "start": 547.96, "end": 552.96, "text": " so that will take up the first half of this course there are many very useful", "tokens": [51358, 370, 300, 486, 747, 493, 264, 700, 1922, 295, 341, 1164, 456, 366, 867, 588, 4420, 51608], "temperature": 0.0, "avg_logprob": -0.1341409471299913, "compression_ratio": 1.8589211618257262, "no_speech_prob": 0.019713113084435463}, {"id": 96, "seek": 52808, "start": 552.96, "end": 557.6400000000001, "text": " techniques very different techniques for performing these two tasks they'll", "tokens": [51608, 7512, 588, 819, 7512, 337, 10205, 613, 732, 9608, 436, 603, 51842], "temperature": 0.0, "avg_logprob": -0.1341409471299913, "compression_ratio": 1.8589211618257262, "no_speech_prob": 0.019713113084435463}, {"id": 97, "seek": 55764, "start": 557.72, "end": 561.28, "text": " entail different ways of thinking about the problems probabilistic versus", "tokens": [50368, 948, 864, 819, 2098, 295, 1953, 466, 264, 2740, 31959, 3142, 5717, 50546], "temperature": 0.0, "avg_logprob": -0.12287846431937269, "compression_ratio": 2.055299539170507, "no_speech_prob": 0.0010003913193941116}, {"id": 98, "seek": 55764, "start": 561.28, "end": 568.16, "text": " non probabilistic the models that we define will require different ways or", "tokens": [50546, 2107, 31959, 3142, 264, 5245, 300, 321, 6964, 486, 3651, 819, 2098, 420, 50890], "temperature": 0.0, "avg_logprob": -0.12287846431937269, "compression_ratio": 2.055299539170507, "no_speech_prob": 0.0010003913193941116}, {"id": 99, "seek": 55764, "start": 568.16, "end": 571.4399999999999, "text": " different techniques for learning them so we'll need different optimization", "tokens": [50890, 819, 7512, 337, 2539, 552, 370, 321, 603, 643, 819, 19618, 51054], "temperature": 0.0, "avg_logprob": -0.12287846431937269, "compression_ratio": 2.055299539170507, "no_speech_prob": 0.0010003913193941116}, {"id": 100, "seek": 55764, "start": 571.4399999999999, "end": 575.76, "text": " techniques so the first half of this course will be all about supervised", "tokens": [51054, 7512, 370, 264, 700, 1922, 295, 341, 1164, 486, 312, 439, 466, 46533, 51270], "temperature": 0.0, "avg_logprob": -0.12287846431937269, "compression_ratio": 2.055299539170507, "no_speech_prob": 0.0010003913193941116}, {"id": 101, "seek": 55764, "start": 575.76, "end": 579.96, "text": " learning then in the second half of the course we'll transition to unsupervised", "tokens": [51270, 2539, 550, 294, 264, 1150, 1922, 295, 264, 1164, 321, 603, 6034, 281, 2693, 12879, 24420, 51480], "temperature": 0.0, "avg_logprob": -0.12287846431937269, "compression_ratio": 2.055299539170507, "no_speech_prob": 0.0010003913193941116}, {"id": 102, "seek": 55764, "start": 579.96, "end": 585.28, "text": " learning and with unsupervised learning the goal is a bit more vague", "tokens": [51480, 2539, 293, 365, 2693, 12879, 24420, 2539, 264, 3387, 307, 257, 857, 544, 24247, 51746], "temperature": 0.0, "avg_logprob": -0.12287846431937269, "compression_ratio": 2.055299539170507, "no_speech_prob": 0.0010003913193941116}, {"id": 103, "seek": 58528, "start": 585.8, "end": 589.8399999999999, "text": " supervised learning is very nice because we know that we want to map an input to", "tokens": [50390, 46533, 2539, 307, 588, 1481, 570, 321, 458, 300, 321, 528, 281, 4471, 364, 4846, 281, 50592], "temperature": 0.0, "avg_logprob": -0.10507410316057103, "compression_ratio": 1.787037037037037, "no_speech_prob": 0.015419461764395237}, {"id": 104, "seek": 58528, "start": 589.8399999999999, "end": 596.04, "text": " an output and honestly we don't necessarily even care how it's mapped we", "tokens": [50592, 364, 5598, 293, 6095, 321, 500, 380, 4725, 754, 1127, 577, 309, 311, 33318, 321, 50902], "temperature": 0.0, "avg_logprob": -0.10507410316057103, "compression_ratio": 1.787037037037037, "no_speech_prob": 0.015419461764395237}, {"id": 105, "seek": 58528, "start": 596.04, "end": 601.0, "text": " don't care whether we learn anything by mapping it in many cases we don't", "tokens": [50902, 500, 380, 1127, 1968, 321, 1466, 1340, 538, 18350, 309, 294, 867, 3331, 321, 500, 380, 51150], "temperature": 0.0, "avg_logprob": -0.10507410316057103, "compression_ratio": 1.787037037037037, "no_speech_prob": 0.015419461764395237}, {"id": 106, "seek": 58528, "start": 601.0, "end": 605.8399999999999, "text": " perhaps in some cases we do we simply want to say here's my input what should", "tokens": [51150, 4317, 294, 512, 3331, 321, 360, 321, 2935, 528, 281, 584, 510, 311, 452, 4846, 437, 820, 51392], "temperature": 0.0, "avg_logprob": -0.10507410316057103, "compression_ratio": 1.787037037037037, "no_speech_prob": 0.015419461764395237}, {"id": 107, "seek": 58528, "start": 605.8399999999999, "end": 611.1999999999999, "text": " I map it to as an output and we measure the performance based purely on how well", "tokens": [51392, 286, 4471, 309, 281, 382, 364, 5598, 293, 321, 3481, 264, 3389, 2361, 17491, 322, 577, 731, 51660], "temperature": 0.0, "avg_logprob": -0.10507410316057103, "compression_ratio": 1.787037037037037, "no_speech_prob": 0.015419461764395237}, {"id": 108, "seek": 61120, "start": 611.32, "end": 618.0, "text": " it does that task with unsupervised learning we don't have in most cases", "tokens": [50370, 309, 775, 300, 5633, 365, 2693, 12879, 24420, 2539, 321, 500, 380, 362, 294, 881, 3331, 50704], "temperature": 0.0, "avg_logprob": -0.07961360974745317, "compression_ratio": 1.6964285714285714, "no_speech_prob": 0.00186724157538265}, {"id": 109, "seek": 61120, "start": 618.0, "end": 624.9200000000001, "text": " this sort of an input output mapping we want to perform more abstract or vague", "tokens": [50704, 341, 1333, 295, 364, 4846, 5598, 18350, 321, 528, 281, 2042, 544, 12649, 420, 24247, 51050], "temperature": 0.0, "avg_logprob": -0.07961360974745317, "compression_ratio": 1.6964285714285714, "no_speech_prob": 0.00186724157538265}, {"id": 110, "seek": 61120, "start": 624.9200000000001, "end": 629.4000000000001, "text": " tasks such as understanding what is the information in our data set for example", "tokens": [51050, 9608, 1270, 382, 3701, 437, 307, 264, 1589, 294, 527, 1412, 992, 337, 1365, 51274], "temperature": 0.0, "avg_logprob": -0.07961360974745317, "compression_ratio": 1.6964285714285714, "no_speech_prob": 0.00186724157538265}, {"id": 111, "seek": 61120, "start": 629.4000000000001, "end": 635.2, "text": " we don't have an infinite amount of time to read you know so many thousands or", "tokens": [51274, 321, 500, 380, 362, 364, 13785, 2372, 295, 565, 281, 1401, 291, 458, 370, 867, 5383, 420, 51564], "temperature": 0.0, "avg_logprob": -0.07961360974745317, "compression_ratio": 1.6964285714285714, "no_speech_prob": 0.00186724157538265}, {"id": 112, "seek": 61120, "start": 635.2, "end": 640.24, "text": " millions of documents so we want a fast algorithmic way for taking in", "tokens": [51564, 6803, 295, 8512, 370, 321, 528, 257, 2370, 9284, 299, 636, 337, 1940, 294, 51816], "temperature": 0.0, "avg_logprob": -0.07961360974745317, "compression_ratio": 1.6964285714285714, "no_speech_prob": 0.00186724157538265}, {"id": 113, "seek": 64024, "start": 640.24, "end": 645.72, "text": " information taking in data and extracting the information for us so for", "tokens": [50364, 1589, 1940, 294, 1412, 293, 49844, 264, 1589, 337, 505, 370, 337, 50638], "temperature": 0.0, "avg_logprob": -0.10692938362679831, "compression_ratio": 1.8529411764705883, "no_speech_prob": 0.0007320753647945821}, {"id": 114, "seek": 64024, "start": 645.72, "end": 649.5600000000001, "text": " example with unsupervised learning we might want to do something like topic", "tokens": [50638, 1365, 365, 2693, 12879, 24420, 2539, 321, 1062, 528, 281, 360, 746, 411, 4829, 50830], "temperature": 0.0, "avg_logprob": -0.10692938362679831, "compression_ratio": 1.8529411764705883, "no_speech_prob": 0.0007320753647945821}, {"id": 115, "seek": 64024, "start": 649.5600000000001, "end": 659.84, "text": " modeling where we have many texts data many documents provided to us we don't", "tokens": [50830, 15983, 689, 321, 362, 867, 15765, 1412, 867, 8512, 5649, 281, 505, 321, 500, 380, 51344], "temperature": 0.0, "avg_logprob": -0.10692938362679831, "compression_ratio": 1.8529411764705883, "no_speech_prob": 0.0007320753647945821}, {"id": 116, "seek": 64024, "start": 659.84, "end": 663.4, "text": " have any labels for these documents all we have is the text for each document", "tokens": [51344, 362, 604, 16949, 337, 613, 8512, 439, 321, 362, 307, 264, 2487, 337, 1184, 4166, 51522], "temperature": 0.0, "avg_logprob": -0.10692938362679831, "compression_ratio": 1.8529411764705883, "no_speech_prob": 0.0007320753647945821}, {"id": 117, "seek": 64024, "start": 663.4, "end": 668.96, "text": " and then we want to extract what is the underlying what are the underlying", "tokens": [51522, 293, 550, 321, 528, 281, 8947, 437, 307, 264, 14217, 437, 366, 264, 14217, 51800], "temperature": 0.0, "avg_logprob": -0.10692938362679831, "compression_ratio": 1.8529411764705883, "no_speech_prob": 0.0007320753647945821}, {"id": 118, "seek": 66896, "start": 669.0, "end": 674.52, "text": " themes within these documents so that's the idea of topic modeling we also might", "tokens": [50366, 13544, 1951, 613, 8512, 370, 300, 311, 264, 1558, 295, 4829, 15983, 321, 611, 1062, 50642], "temperature": 0.0, "avg_logprob": -0.12493607167447551, "compression_ratio": 1.724890829694323, "no_speech_prob": 0.0023227971978485584}, {"id": 119, "seek": 66896, "start": 674.52, "end": 680.52, "text": " want to do recommendations this would be where we have many users and many", "tokens": [50642, 528, 281, 360, 10434, 341, 576, 312, 689, 321, 362, 867, 5022, 293, 867, 50942], "temperature": 0.0, "avg_logprob": -0.12493607167447551, "compression_ratio": 1.724890829694323, "no_speech_prob": 0.0023227971978485584}, {"id": 120, "seek": 66896, "start": 680.52, "end": 687.32, "text": " objects and users will give feedback or input on a subset of these objects either", "tokens": [50942, 6565, 293, 5022, 486, 976, 5824, 420, 4846, 322, 257, 25993, 295, 613, 6565, 2139, 51282], "temperature": 0.0, "avg_logprob": -0.12493607167447551, "compression_ratio": 1.724890829694323, "no_speech_prob": 0.0023227971978485584}, {"id": 121, "seek": 66896, "start": 687.32, "end": 692.24, "text": " through a review or through some sort of a rating like a star rating for example", "tokens": [51282, 807, 257, 3131, 420, 807, 512, 1333, 295, 257, 10990, 411, 257, 3543, 10990, 337, 1365, 51528], "temperature": 0.0, "avg_logprob": -0.12493607167447551, "compression_ratio": 1.724890829694323, "no_speech_prob": 0.0023227971978485584}, {"id": 122, "seek": 66896, "start": 692.24, "end": 698.08, "text": " with Netflix a user could rate a movie one to five stars and we want to take", "tokens": [51528, 365, 12778, 257, 4195, 727, 3314, 257, 3169, 472, 281, 1732, 6105, 293, 321, 528, 281, 747, 51820], "temperature": 0.0, "avg_logprob": -0.12493607167447551, "compression_ratio": 1.724890829694323, "no_speech_prob": 0.0023227971978485584}, {"id": 123, "seek": 69808, "start": 698.08, "end": 703.5200000000001, "text": " all of this information and learn some sort of a latent space where we can", "tokens": [50364, 439, 295, 341, 1589, 293, 1466, 512, 1333, 295, 257, 48994, 1901, 689, 321, 393, 50636], "temperature": 0.0, "avg_logprob": -0.11876835057764877, "compression_ratio": 1.9430051813471503, "no_speech_prob": 0.006190793123096228}, {"id": 124, "seek": 69808, "start": 703.5200000000001, "end": 708.8000000000001, "text": " embed users and movies such that users who are close to each other share", "tokens": [50636, 12240, 5022, 293, 6233, 1270, 300, 5022, 567, 366, 1998, 281, 1184, 661, 2073, 50900], "temperature": 0.0, "avg_logprob": -0.11876835057764877, "compression_ratio": 1.9430051813471503, "no_speech_prob": 0.006190793123096228}, {"id": 125, "seek": 69808, "start": 708.8000000000001, "end": 714.5200000000001, "text": " similar tastes movies that are close to users are somehow appropriate", "tokens": [50900, 2531, 8666, 6233, 300, 366, 1998, 281, 5022, 366, 6063, 6854, 51186], "temperature": 0.0, "avg_logprob": -0.11876835057764877, "compression_ratio": 1.9430051813471503, "no_speech_prob": 0.006190793123096228}, {"id": 126, "seek": 69808, "start": 714.5200000000001, "end": 719.12, "text": " recommendations to be made to those users movies that are close to each other", "tokens": [51186, 10434, 281, 312, 1027, 281, 729, 5022, 6233, 300, 366, 1998, 281, 1184, 661, 51416], "temperature": 0.0, "avg_logprob": -0.11876835057764877, "compression_ratio": 1.9430051813471503, "no_speech_prob": 0.006190793123096228}, {"id": 127, "seek": 69808, "start": 719.12, "end": 723.76, "text": " are similar in their content and things that are very far apart are very unlike", "tokens": [51416, 366, 2531, 294, 641, 2701, 293, 721, 300, 366, 588, 1400, 4936, 366, 588, 8343, 51648], "temperature": 0.0, "avg_logprob": -0.11876835057764877, "compression_ratio": 1.9430051813471503, "no_speech_prob": 0.006190793123096228}, {"id": 128, "seek": 72376, "start": 723.76, "end": 730.4399999999999, "text": " each other so we want to learn this information simply from the data from", "tokens": [50364, 1184, 661, 370, 321, 528, 281, 1466, 341, 1589, 2935, 490, 264, 1412, 490, 50698], "temperature": 0.0, "avg_logprob": -0.11901027626461452, "compression_ratio": 1.664835164835165, "no_speech_prob": 0.0014102028217166662}, {"id": 129, "seek": 72376, "start": 730.4399999999999, "end": 737.4399999999999, "text": " the raw data and some model assumption that we have to make so for example one", "tokens": [50698, 264, 8936, 1412, 293, 512, 2316, 15302, 300, 321, 362, 281, 652, 370, 337, 1365, 472, 51048], "temperature": 0.0, "avg_logprob": -0.11901027626461452, "compression_ratio": 1.664835164835165, "no_speech_prob": 0.0014102028217166662}, {"id": 130, "seek": 72376, "start": 737.4399999999999, "end": 742.6, "text": " of the most well-known unsupervised learning tasks is the topic modeling", "tokens": [51048, 295, 264, 881, 731, 12, 6861, 2693, 12879, 24420, 2539, 9608, 307, 264, 4829, 15983, 51306], "temperature": 0.0, "avg_logprob": -0.11901027626461452, "compression_ratio": 1.664835164835165, "no_speech_prob": 0.0014102028217166662}, {"id": 131, "seek": 72376, "start": 742.6, "end": 748.16, "text": " problem and so what I'm showing here is an example of what a topic model will", "tokens": [51306, 1154, 293, 370, 437, 286, 478, 4099, 510, 307, 364, 1365, 295, 437, 257, 4829, 2316, 486, 51584], "temperature": 0.0, "avg_logprob": -0.11901027626461452, "compression_ratio": 1.664835164835165, "no_speech_prob": 0.0014102028217166662}, {"id": 132, "seek": 74816, "start": 748.1999999999999, "end": 753.56, "text": " learn if you provide it with over a million documents from the New York", "tokens": [50366, 1466, 498, 291, 2893, 309, 365, 670, 257, 2459, 8512, 490, 264, 1873, 3609, 50634], "temperature": 0.0, "avg_logprob": -0.09254501416133, "compression_ratio": 1.860655737704918, "no_speech_prob": 0.02095903642475605}, {"id": 133, "seek": 74816, "start": 753.56, "end": 759.56, "text": " Times over roughly 20 year period so what we have in these documents is", "tokens": [50634, 11366, 670, 9810, 945, 1064, 2896, 370, 437, 321, 362, 294, 613, 8512, 307, 50934], "temperature": 0.0, "avg_logprob": -0.09254501416133, "compression_ratio": 1.860655737704918, "no_speech_prob": 0.02095903642475605}, {"id": 134, "seek": 74816, "start": 759.56, "end": 764.28, "text": " simply a tag that says that this is a document and these are the words in the", "tokens": [50934, 2935, 257, 6162, 300, 1619, 300, 341, 307, 257, 4166, 293, 613, 366, 264, 2283, 294, 264, 51170], "temperature": 0.0, "avg_logprob": -0.09254501416133, "compression_ratio": 1.860655737704918, "no_speech_prob": 0.02095903642475605}, {"id": 135, "seek": 74816, "start": 764.28, "end": 768.48, "text": " document and we have that repeated for all of the documents in our data set", "tokens": [51170, 4166, 293, 321, 362, 300, 10477, 337, 439, 295, 264, 8512, 294, 527, 1412, 992, 51380], "temperature": 0.0, "avg_logprob": -0.09254501416133, "compression_ratio": 1.860655737704918, "no_speech_prob": 0.02095903642475605}, {"id": 136, "seek": 74816, "start": 768.48, "end": 773.4, "text": " again it can be over a million of these documents we want to make some sort of a", "tokens": [51380, 797, 309, 393, 312, 670, 257, 2459, 295, 613, 8512, 321, 528, 281, 652, 512, 1333, 295, 257, 51626], "temperature": 0.0, "avg_logprob": -0.09254501416133, "compression_ratio": 1.860655737704918, "no_speech_prob": 0.02095903642475605}, {"id": 137, "seek": 74816, "start": 773.4, "end": 777.0799999999999, "text": " modeling assumption such that we find the words that should somehow cluster", "tokens": [51626, 15983, 15302, 1270, 300, 321, 915, 264, 2283, 300, 820, 6063, 13630, 51810], "temperature": 0.0, "avg_logprob": -0.09254501416133, "compression_ratio": 1.860655737704918, "no_speech_prob": 0.02095903642475605}, {"id": 138, "seek": 77708, "start": 777.12, "end": 783.12, "text": " together these words would then define topics underlying our data set so for", "tokens": [50366, 1214, 613, 2283, 576, 550, 6964, 8378, 14217, 527, 1412, 992, 370, 337, 50666], "temperature": 0.0, "avg_logprob": -0.08960623800018687, "compression_ratio": 1.7136363636363636, "no_speech_prob": 0.0012446397449821234}, {"id": 139, "seek": 77708, "start": 783.12, "end": 788.76, "text": " example by simply inputting the raw data from the New York Times making a", "tokens": [50666, 1365, 538, 2935, 4846, 783, 264, 8936, 1412, 490, 264, 1873, 3609, 11366, 1455, 257, 50948], "temperature": 0.0, "avg_logprob": -0.08960623800018687, "compression_ratio": 1.7136363636363636, "no_speech_prob": 0.0012446397449821234}, {"id": 140, "seek": 77708, "start": 788.76, "end": 793.1600000000001, "text": " modeling assumption that doesn't in advance tell us which words should go", "tokens": [50948, 15983, 15302, 300, 1177, 380, 294, 7295, 980, 505, 597, 2283, 820, 352, 51168], "temperature": 0.0, "avg_logprob": -0.08960623800018687, "compression_ratio": 1.7136363636363636, "no_speech_prob": 0.0012446397449821234}, {"id": 141, "seek": 77708, "start": 793.1600000000001, "end": 799.0, "text": " with which other words we can then run an algorithm to extract information like", "tokens": [51168, 365, 597, 661, 2283, 321, 393, 550, 1190, 364, 9284, 281, 8947, 1589, 411, 51460], "temperature": 0.0, "avg_logprob": -0.08960623800018687, "compression_ratio": 1.7136363636363636, "no_speech_prob": 0.0012446397449821234}, {"id": 142, "seek": 77708, "start": 799.0, "end": 804.08, "text": " this that says that this set of words belongs together this set of words", "tokens": [51460, 341, 300, 1619, 300, 341, 992, 295, 2283, 12953, 1214, 341, 992, 295, 2283, 51714], "temperature": 0.0, "avg_logprob": -0.08960623800018687, "compression_ratio": 1.7136363636363636, "no_speech_prob": 0.0012446397449821234}, {"id": 143, "seek": 80408, "start": 804.08, "end": 809.0400000000001, "text": " belongs together and so on so we can learn for example 10 or more of these", "tokens": [50364, 12953, 1214, 293, 370, 322, 370, 321, 393, 1466, 337, 1365, 1266, 420, 544, 295, 613, 50612], "temperature": 0.0, "avg_logprob": -0.09320138801227916, "compression_ratio": 1.7731481481481481, "no_speech_prob": 0.005219250917434692}, {"id": 144, "seek": 80408, "start": 809.0400000000001, "end": 814.12, "text": " what are called topics that tell us which words belong together and then not", "tokens": [50612, 437, 366, 1219, 8378, 300, 980, 505, 597, 2283, 5784, 1214, 293, 550, 406, 50866], "temperature": 0.0, "avg_logprob": -0.09320138801227916, "compression_ratio": 1.7731481481481481, "no_speech_prob": 0.005219250917434692}, {"id": 145, "seek": 80408, "start": 814.12, "end": 819.2, "text": " shown here is also how each document uses that topic so for example for a", "tokens": [50866, 4898, 510, 307, 611, 577, 1184, 4166, 4960, 300, 4829, 370, 337, 1365, 337, 257, 51120], "temperature": 0.0, "avg_logprob": -0.09320138801227916, "compression_ratio": 1.7731481481481481, "no_speech_prob": 0.005219250917434692}, {"id": 146, "seek": 80408, "start": 819.2, "end": 823.84, "text": " particular document we might say it's composed of these two topics and no other", "tokens": [51120, 1729, 4166, 321, 1062, 584, 309, 311, 18204, 295, 613, 732, 8378, 293, 572, 661, 51352], "temperature": 0.0, "avg_logprob": -0.09320138801227916, "compression_ratio": 1.7731481481481481, "no_speech_prob": 0.005219250917434692}, {"id": 147, "seek": 80408, "start": 823.84, "end": 829.0400000000001, "text": " topics and so this is information that's extracted from the raw data we don't", "tokens": [51352, 8378, 293, 370, 341, 307, 1589, 300, 311, 34086, 490, 264, 8936, 1412, 321, 500, 380, 51612], "temperature": 0.0, "avg_logprob": -0.09320138801227916, "compression_ratio": 1.7731481481481481, "no_speech_prob": 0.005219250917434692}, {"id": 148, "seek": 82904, "start": 829.0799999999999, "end": 834.0799999999999, "text": " a priori tell the algorithm what it should learn we simply say there is this", "tokens": [50366, 257, 4059, 72, 980, 264, 9284, 437, 309, 820, 1466, 321, 2935, 584, 456, 307, 341, 50616], "temperature": 0.0, "avg_logprob": -0.22120553425380163, "compression_ratio": 1.4313725490196079, "no_speech_prob": 0.04269059747457504}, {"id": 149, "seek": 82904, "start": 834.0799999999999, "end": 840.0, "text": " structure that we want to learn here's the data tell me the structure", "tokens": [50616, 3877, 300, 321, 528, 281, 1466, 510, 311, 264, 1412, 980, 385, 264, 3877, 50912], "temperature": 0.0, "avg_logprob": -0.22120553425380163, "compression_ratio": 1.4313725490196079, "no_speech_prob": 0.04269059747457504}], "language": "en"}