WEBVTT

00:00.000 --> 00:05.480
Welcome to the first lecture of Machine Learning on Columbia X. I'm your

00:05.480 --> 00:11.400
professor, John Paisley. I'm a member of the Department of Electrical Engineering

00:11.400 --> 00:16.040
as well as the Data Science Institute at Columbia and my specialty is in

00:16.040 --> 00:20.120
machine learning and this is this class that we'll be working through in the

00:20.120 --> 00:24.120
following lectures is directly based on the machine learning course that I

00:24.120 --> 00:30.840
teach here at Columbia as part of the Data Science Institute. So in this first

00:30.840 --> 00:36.840
lecture I want to primarily focus on a general overview of the course and of

00:36.840 --> 00:42.600
what machine learning is and then go into a little bit of detail on a very

00:42.600 --> 00:48.520
specific problem working with multivariate Gaussians in order to kind of

00:48.520 --> 00:53.160
highlight the different aspects and the different components of what we'll be

00:53.160 --> 00:59.720
discussing in this course. So this course will cover model-based techniques

00:59.720 --> 01:05.920
for extracting information from data with some sort of an end task in mind. So

01:05.920 --> 01:12.240
these tasks could include for example predicting an unknown output given some

01:12.240 --> 01:18.120
corresponding input. We could simply want to uncover the information

01:18.160 --> 01:24.520
underlying our data set with the goal of better understanding what's

01:24.520 --> 01:29.320
contained in our data that we have or we could do things like data driven

01:29.320 --> 01:36.880
recommendation, grouping, classification, ranking, etc. So using the data to help us

01:36.880 --> 01:43.880
learn how to perform these sorts of end tasks. So in a course like this there's

01:43.920 --> 01:49.320
a few ways that the information can be presented, different orderings of the

01:49.320 --> 01:55.960
information. One example would be to partition it as in one half supervised

01:55.960 --> 02:00.120
learning, the other half unsupervised learning and I'll discuss that in a bit

02:00.120 --> 02:04.040
more detail in this lecture because that's the perspective that we're going

02:04.040 --> 02:09.880
to take. But we could also think in terms of probabilistic models where we are

02:09.880 --> 02:14.440
working with probability distributions versus non probabilistic models where

02:14.440 --> 02:20.880
we're learning from the data without any sort of probability probabilistic

02:20.880 --> 02:26.280
motivation. There's also a dichotomy between modeling approaches. So what is

02:26.280 --> 02:32.720
the model that we want to define for our data versus optimization approaches

02:32.720 --> 02:37.000
which is very tightly linked with modeling approaches. But with optimization

02:37.280 --> 02:42.320
now that we've defined a model, how do we learn the model? So these are two

02:42.320 --> 02:48.240
separate problems with various techniques in these two sub problems. So again

02:48.240 --> 02:52.960
we're going to partition this course into roughly two halves. The first half

02:52.960 --> 02:57.600
will focus on supervised learning and the second half will focus on

02:57.600 --> 03:02.680
unsupervised learning and these additional ideas such as probabilistic

03:02.720 --> 03:07.560
versus non probabilistic or modeling approaches versus optimization techniques

03:07.560 --> 03:14.360
will be sort of discussed as we go along as needed. So those will be

03:14.360 --> 03:17.920
interwoven throughout the course but the first part of this course will be

03:19.040 --> 03:22.560
strictly supervised learning and the second part will be on supervised

03:22.560 --> 03:28.000
learning. What do we mean when we say we want to perform supervised learning?

03:28.920 --> 03:35.560
In a nutshell what we're saying is that we want to take inputs and predict

03:35.560 --> 03:41.520
corresponding outputs. So for example if we do want to do regression we might

03:41.520 --> 03:48.520
have pairs of data in this case a one-dimensional value for x and a

03:48.520 --> 03:55.000
corresponding one-dimensional value for t and then we would want to learn some

03:55.040 --> 03:59.320
sort of a function so that we input x and we make a prediction for the output

03:59.320 --> 04:06.480
t. So for example here we have several data points as indicated by circles

04:06.480 --> 04:13.440
where the x-axis is the input for that particular point and the t-axis is the

04:13.440 --> 04:18.720
corresponding output and now that we have this data set of these several

04:18.720 --> 04:24.720
points we want to define some sort of a regression function for example this

04:24.760 --> 04:30.560
blue line that in some way interpolates the outputs as a function of the inputs

04:31.760 --> 04:37.480
and then the goal is given this smoothing function that we've learned for some new

04:37.480 --> 04:44.200
input x we want to predict the corresponding output t so we for future

04:44.840 --> 04:50.160
time points we obtain x we obtain an input and we want to predict the

04:50.200 --> 04:55.240
corresponding output so that's regression we say it's regression because the

04:55.240 --> 05:01.880
outputs are assumed to be real valued. Classification is another supervised

05:01.880 --> 05:07.360
learning problem that is slightly different the form or the structure is

05:08.080 --> 05:13.880
very similar we have pairs of inputs and outputs and we get this data set which

05:13.880 --> 05:17.320
has many of these pairs of inputs and outputs and we want to learn some

05:17.360 --> 05:21.720
functions so that in the future when we get a new input for which we don't have

05:21.720 --> 05:25.560
the output we can make a prediction of the output that's going to be accurate

05:27.040 --> 05:31.640
however the key difference here is that where with regression the output is a

05:31.640 --> 05:39.160
real valued output with classification it's a discrete valued thing so it's a

05:39.160 --> 05:45.280
category or a class so in this right plot what I'm showing are input output pairs

05:46.120 --> 05:51.640
except now the input is a two-dimensional vector so here the input would be this

05:51.640 --> 05:57.400
two-dimensional point and the output for this plot is being encoded by a color

05:57.400 --> 06:04.840
so the output could be one of two values either a blue value or a an orange

06:04.840 --> 06:10.240
value so in this case we want to take our data inputs and classify them into one

06:10.240 --> 06:17.680
of two outputs so we get a data set like this with all of these input locations

06:17.680 --> 06:23.360
and the corresponding color-coded output and now our goal is to learn some sort

06:23.360 --> 06:29.920
of a function a classifier so that we can partition the space such as is shown

06:29.920 --> 06:36.120
here where for a brand new point any of these points that we don't have the

06:36.120 --> 06:40.960
output we can evaluate the function at that point and make a prediction of the

06:40.960 --> 06:45.880
output so we might say for this data set we would partition this entire region

06:45.880 --> 06:54.080
here these two regions into the orange class and this region here into the blue

06:54.080 --> 06:58.960
class so any new points falling in this region will be assigned to the blue

06:58.960 --> 07:06.120
class so the key here with supervised learning is that we're learning an

07:06.120 --> 07:14.400
algorithm based on a function mapping from input to output we the outputs are

07:14.400 --> 07:18.640
basically going to be telling us how to map the inputs so that we have an

07:18.640 --> 07:27.320
accurate function so we have input output pairs so to look at a classic example

07:27.320 --> 07:33.840
we could think of spam detection given some set of inputs like these two chunks

07:33.840 --> 07:39.000
of text we would want to assign it a label plus one or minus one sometimes

07:39.000 --> 07:43.160
we would say plus one or zero but we would want to sign it one of two possible

07:43.160 --> 07:49.400
labels one label would correspond to an email that is spam and we would want to

07:49.400 --> 07:56.200
then you know automatically delete that email and the other class would be non

07:56.240 --> 08:02.040
spam emails emails that we want to put into our inbox and actually read so it's

08:02.040 --> 08:08.040
essentially a filtering problem so for example we might have a data set a data

08:08.040 --> 08:12.240
point like this containing this text and we would want to now input this into

08:12.240 --> 08:17.040
some sort of a function and say is it spam or not in this case most likely

08:17.040 --> 08:23.600
it's not or a data point like this this piece of text where we would input it

08:23.600 --> 08:29.360
into the same exact function with the same classifier and in this case that

08:29.360 --> 08:34.440
same classifier would say this email is a spam so we classify this email to spam

08:34.440 --> 08:41.840
and this email to non spam using the same classifier learned from examples of

08:41.840 --> 08:48.080
of labeled spam and labeled non spam emails so essentially the first half of

08:48.080 --> 08:53.720
this course is going to be all about learning different ways that we can

08:53.720 --> 08:58.720
define these functions to map inputs to outputs either regression models or

08:58.720 --> 09:04.000
classification models depending on the problem as well as algorithms or

09:04.000 --> 09:07.960
techniques for then learning the parameters of these models based on data

09:07.960 --> 09:12.960
so that will take up the first half of this course there are many very useful

09:12.960 --> 09:17.640
techniques very different techniques for performing these two tasks they'll

09:17.720 --> 09:21.280
entail different ways of thinking about the problems probabilistic versus

09:21.280 --> 09:28.160
non probabilistic the models that we define will require different ways or

09:28.160 --> 09:31.440
different techniques for learning them so we'll need different optimization

09:31.440 --> 09:35.760
techniques so the first half of this course will be all about supervised

09:35.760 --> 09:39.960
learning then in the second half of the course we'll transition to unsupervised

09:39.960 --> 09:45.280
learning and with unsupervised learning the goal is a bit more vague

09:45.800 --> 09:49.840
supervised learning is very nice because we know that we want to map an input to

09:49.840 --> 09:56.040
an output and honestly we don't necessarily even care how it's mapped we

09:56.040 --> 10:01.000
don't care whether we learn anything by mapping it in many cases we don't

10:01.000 --> 10:05.840
perhaps in some cases we do we simply want to say here's my input what should

10:05.840 --> 10:11.200
I map it to as an output and we measure the performance based purely on how well

10:11.320 --> 10:18.000
it does that task with unsupervised learning we don't have in most cases

10:18.000 --> 10:24.920
this sort of an input output mapping we want to perform more abstract or vague

10:24.920 --> 10:29.400
tasks such as understanding what is the information in our data set for example

10:29.400 --> 10:35.200
we don't have an infinite amount of time to read you know so many thousands or

10:35.200 --> 10:40.240
millions of documents so we want a fast algorithmic way for taking in

10:40.240 --> 10:45.720
information taking in data and extracting the information for us so for

10:45.720 --> 10:49.560
example with unsupervised learning we might want to do something like topic

10:49.560 --> 10:59.840
modeling where we have many texts data many documents provided to us we don't

10:59.840 --> 11:03.400
have any labels for these documents all we have is the text for each document

11:03.400 --> 11:08.960
and then we want to extract what is the underlying what are the underlying

11:09.000 --> 11:14.520
themes within these documents so that's the idea of topic modeling we also might

11:14.520 --> 11:20.520
want to do recommendations this would be where we have many users and many

11:20.520 --> 11:27.320
objects and users will give feedback or input on a subset of these objects either

11:27.320 --> 11:32.240
through a review or through some sort of a rating like a star rating for example

11:32.240 --> 11:38.080
with Netflix a user could rate a movie one to five stars and we want to take

11:38.080 --> 11:43.520
all of this information and learn some sort of a latent space where we can

11:43.520 --> 11:48.800
embed users and movies such that users who are close to each other share

11:48.800 --> 11:54.520
similar tastes movies that are close to users are somehow appropriate

11:54.520 --> 11:59.120
recommendations to be made to those users movies that are close to each other

11:59.120 --> 12:03.760
are similar in their content and things that are very far apart are very unlike

12:03.760 --> 12:10.440
each other so we want to learn this information simply from the data from

12:10.440 --> 12:17.440
the raw data and some model assumption that we have to make so for example one

12:17.440 --> 12:22.600
of the most well-known unsupervised learning tasks is the topic modeling

12:22.600 --> 12:28.160
problem and so what I'm showing here is an example of what a topic model will

12:28.200 --> 12:33.560
learn if you provide it with over a million documents from the New York

12:33.560 --> 12:39.560
Times over roughly 20 year period so what we have in these documents is

12:39.560 --> 12:44.280
simply a tag that says that this is a document and these are the words in the

12:44.280 --> 12:48.480
document and we have that repeated for all of the documents in our data set

12:48.480 --> 12:53.400
again it can be over a million of these documents we want to make some sort of a

12:53.400 --> 12:57.080
modeling assumption such that we find the words that should somehow cluster

12:57.120 --> 13:03.120
together these words would then define topics underlying our data set so for

13:03.120 --> 13:08.760
example by simply inputting the raw data from the New York Times making a

13:08.760 --> 13:13.160
modeling assumption that doesn't in advance tell us which words should go

13:13.160 --> 13:19.000
with which other words we can then run an algorithm to extract information like

13:19.000 --> 13:24.080
this that says that this set of words belongs together this set of words

13:24.080 --> 13:29.040
belongs together and so on so we can learn for example 10 or more of these

13:29.040 --> 13:34.120
what are called topics that tell us which words belong together and then not

13:34.120 --> 13:39.200
shown here is also how each document uses that topic so for example for a

13:39.200 --> 13:43.840
particular document we might say it's composed of these two topics and no other

13:43.840 --> 13:49.040
topics and so this is information that's extracted from the raw data we don't

13:49.080 --> 13:54.080
a priori tell the algorithm what it should learn we simply say there is this

13:54.080 --> 14:00.000
structure that we want to learn here's the data tell me the structure

