1
00:00:00,000 --> 00:00:05,480
Welcome to the first lecture of Machine Learning on Columbia X. I'm your

2
00:00:05,480 --> 00:00:11,400
professor, John Paisley. I'm a member of the Department of Electrical Engineering

3
00:00:11,400 --> 00:00:16,040
as well as the Data Science Institute at Columbia and my specialty is in

4
00:00:16,040 --> 00:00:20,120
machine learning and this is this class that we'll be working through in the

5
00:00:20,120 --> 00:00:24,120
following lectures is directly based on the machine learning course that I

6
00:00:24,120 --> 00:00:30,840
teach here at Columbia as part of the Data Science Institute. So in this first

7
00:00:30,840 --> 00:00:36,840
lecture I want to primarily focus on a general overview of the course and of

8
00:00:36,840 --> 00:00:42,600
what machine learning is and then go into a little bit of detail on a very

9
00:00:42,600 --> 00:00:48,520
specific problem working with multivariate Gaussians in order to kind of

10
00:00:48,520 --> 00:00:53,160
highlight the different aspects and the different components of what we'll be

11
00:00:53,160 --> 00:00:59,720
discussing in this course. So this course will cover model-based techniques

12
00:00:59,720 --> 00:01:05,920
for extracting information from data with some sort of an end task in mind. So

13
00:01:05,920 --> 00:01:12,240
these tasks could include for example predicting an unknown output given some

14
00:01:12,240 --> 00:01:18,120
corresponding input. We could simply want to uncover the information

15
00:01:18,160 --> 00:01:24,520
underlying our data set with the goal of better understanding what's

16
00:01:24,520 --> 00:01:29,320
contained in our data that we have or we could do things like data driven

17
00:01:29,320 --> 00:01:36,880
recommendation, grouping, classification, ranking, etc. So using the data to help us

18
00:01:36,880 --> 00:01:43,880
learn how to perform these sorts of end tasks. So in a course like this there's

19
00:01:43,920 --> 00:01:49,320
a few ways that the information can be presented, different orderings of the

20
00:01:49,320 --> 00:01:55,960
information. One example would be to partition it as in one half supervised

21
00:01:55,960 --> 00:02:00,120
learning, the other half unsupervised learning and I'll discuss that in a bit

22
00:02:00,120 --> 00:02:04,040
more detail in this lecture because that's the perspective that we're going

23
00:02:04,040 --> 00:02:09,880
to take. But we could also think in terms of probabilistic models where we are

24
00:02:09,880 --> 00:02:14,440
working with probability distributions versus non probabilistic models where

25
00:02:14,440 --> 00:02:20,880
we're learning from the data without any sort of probability probabilistic

26
00:02:20,880 --> 00:02:26,280
motivation. There's also a dichotomy between modeling approaches. So what is

27
00:02:26,280 --> 00:02:32,720
the model that we want to define for our data versus optimization approaches

28
00:02:32,720 --> 00:02:37,000
which is very tightly linked with modeling approaches. But with optimization

29
00:02:37,280 --> 00:02:42,320
now that we've defined a model, how do we learn the model? So these are two

30
00:02:42,320 --> 00:02:48,240
separate problems with various techniques in these two sub problems. So again

31
00:02:48,240 --> 00:02:52,960
we're going to partition this course into roughly two halves. The first half

32
00:02:52,960 --> 00:02:57,600
will focus on supervised learning and the second half will focus on

33
00:02:57,600 --> 00:03:02,680
unsupervised learning and these additional ideas such as probabilistic

34
00:03:02,720 --> 00:03:07,560
versus non probabilistic or modeling approaches versus optimization techniques

35
00:03:07,560 --> 00:03:14,360
will be sort of discussed as we go along as needed. So those will be

36
00:03:14,360 --> 00:03:17,920
interwoven throughout the course but the first part of this course will be

37
00:03:19,040 --> 00:03:22,560
strictly supervised learning and the second part will be on supervised

38
00:03:22,560 --> 00:03:28,000
learning. What do we mean when we say we want to perform supervised learning?

39
00:03:28,920 --> 00:03:35,560
In a nutshell what we're saying is that we want to take inputs and predict

40
00:03:35,560 --> 00:03:41,520
corresponding outputs. So for example if we do want to do regression we might

41
00:03:41,520 --> 00:03:48,520
have pairs of data in this case a one-dimensional value for x and a

42
00:03:48,520 --> 00:03:55,000
corresponding one-dimensional value for t and then we would want to learn some

43
00:03:55,040 --> 00:03:59,320
sort of a function so that we input x and we make a prediction for the output

44
00:03:59,320 --> 00:04:06,480
t. So for example here we have several data points as indicated by circles

45
00:04:06,480 --> 00:04:13,440
where the x-axis is the input for that particular point and the t-axis is the

46
00:04:13,440 --> 00:04:18,720
corresponding output and now that we have this data set of these several

47
00:04:18,720 --> 00:04:24,720
points we want to define some sort of a regression function for example this

48
00:04:24,760 --> 00:04:30,560
blue line that in some way interpolates the outputs as a function of the inputs

49
00:04:31,760 --> 00:04:37,480
and then the goal is given this smoothing function that we've learned for some new

50
00:04:37,480 --> 00:04:44,200
input x we want to predict the corresponding output t so we for future

51
00:04:44,840 --> 00:04:50,160
time points we obtain x we obtain an input and we want to predict the

52
00:04:50,200 --> 00:04:55,240
corresponding output so that's regression we say it's regression because the

53
00:04:55,240 --> 00:05:01,880
outputs are assumed to be real valued. Classification is another supervised

54
00:05:01,880 --> 00:05:07,360
learning problem that is slightly different the form or the structure is

55
00:05:08,080 --> 00:05:13,880
very similar we have pairs of inputs and outputs and we get this data set which

56
00:05:13,880 --> 00:05:17,320
has many of these pairs of inputs and outputs and we want to learn some

57
00:05:17,360 --> 00:05:21,720
functions so that in the future when we get a new input for which we don't have

58
00:05:21,720 --> 00:05:25,560
the output we can make a prediction of the output that's going to be accurate

59
00:05:27,040 --> 00:05:31,640
however the key difference here is that where with regression the output is a

60
00:05:31,640 --> 00:05:39,160
real valued output with classification it's a discrete valued thing so it's a

61
00:05:39,160 --> 00:05:45,280
category or a class so in this right plot what I'm showing are input output pairs

62
00:05:46,120 --> 00:05:51,640
except now the input is a two-dimensional vector so here the input would be this

63
00:05:51,640 --> 00:05:57,400
two-dimensional point and the output for this plot is being encoded by a color

64
00:05:57,400 --> 00:06:04,840
so the output could be one of two values either a blue value or a an orange

65
00:06:04,840 --> 00:06:10,240
value so in this case we want to take our data inputs and classify them into one

66
00:06:10,240 --> 00:06:17,680
of two outputs so we get a data set like this with all of these input locations

67
00:06:17,680 --> 00:06:23,360
and the corresponding color-coded output and now our goal is to learn some sort

68
00:06:23,360 --> 00:06:29,920
of a function a classifier so that we can partition the space such as is shown

69
00:06:29,920 --> 00:06:36,120
here where for a brand new point any of these points that we don't have the

70
00:06:36,120 --> 00:06:40,960
output we can evaluate the function at that point and make a prediction of the

71
00:06:40,960 --> 00:06:45,880
output so we might say for this data set we would partition this entire region

72
00:06:45,880 --> 00:06:54,080
here these two regions into the orange class and this region here into the blue

73
00:06:54,080 --> 00:06:58,960
class so any new points falling in this region will be assigned to the blue

74
00:06:58,960 --> 00:07:06,120
class so the key here with supervised learning is that we're learning an

75
00:07:06,120 --> 00:07:14,400
algorithm based on a function mapping from input to output we the outputs are

76
00:07:14,400 --> 00:07:18,640
basically going to be telling us how to map the inputs so that we have an

77
00:07:18,640 --> 00:07:27,320
accurate function so we have input output pairs so to look at a classic example

78
00:07:27,320 --> 00:07:33,840
we could think of spam detection given some set of inputs like these two chunks

79
00:07:33,840 --> 00:07:39,000
of text we would want to assign it a label plus one or minus one sometimes

80
00:07:39,000 --> 00:07:43,160
we would say plus one or zero but we would want to sign it one of two possible

81
00:07:43,160 --> 00:07:49,400
labels one label would correspond to an email that is spam and we would want to

82
00:07:49,400 --> 00:07:56,200
then you know automatically delete that email and the other class would be non

83
00:07:56,240 --> 00:08:02,040
spam emails emails that we want to put into our inbox and actually read so it's

84
00:08:02,040 --> 00:08:08,040
essentially a filtering problem so for example we might have a data set a data

85
00:08:08,040 --> 00:08:12,240
point like this containing this text and we would want to now input this into

86
00:08:12,240 --> 00:08:17,040
some sort of a function and say is it spam or not in this case most likely

87
00:08:17,040 --> 00:08:23,600
it's not or a data point like this this piece of text where we would input it

88
00:08:23,600 --> 00:08:29,360
into the same exact function with the same classifier and in this case that

89
00:08:29,360 --> 00:08:34,440
same classifier would say this email is a spam so we classify this email to spam

90
00:08:34,440 --> 00:08:41,840
and this email to non spam using the same classifier learned from examples of

91
00:08:41,840 --> 00:08:48,080
of labeled spam and labeled non spam emails so essentially the first half of

92
00:08:48,080 --> 00:08:53,720
this course is going to be all about learning different ways that we can

93
00:08:53,720 --> 00:08:58,720
define these functions to map inputs to outputs either regression models or

94
00:08:58,720 --> 00:09:04,000
classification models depending on the problem as well as algorithms or

95
00:09:04,000 --> 00:09:07,960
techniques for then learning the parameters of these models based on data

96
00:09:07,960 --> 00:09:12,960
so that will take up the first half of this course there are many very useful

97
00:09:12,960 --> 00:09:17,640
techniques very different techniques for performing these two tasks they'll

98
00:09:17,720 --> 00:09:21,280
entail different ways of thinking about the problems probabilistic versus

99
00:09:21,280 --> 00:09:28,160
non probabilistic the models that we define will require different ways or

100
00:09:28,160 --> 00:09:31,440
different techniques for learning them so we'll need different optimization

101
00:09:31,440 --> 00:09:35,760
techniques so the first half of this course will be all about supervised

102
00:09:35,760 --> 00:09:39,960
learning then in the second half of the course we'll transition to unsupervised

103
00:09:39,960 --> 00:09:45,280
learning and with unsupervised learning the goal is a bit more vague

104
00:09:45,800 --> 00:09:49,840
supervised learning is very nice because we know that we want to map an input to

105
00:09:49,840 --> 00:09:56,040
an output and honestly we don't necessarily even care how it's mapped we

106
00:09:56,040 --> 00:10:01,000
don't care whether we learn anything by mapping it in many cases we don't

107
00:10:01,000 --> 00:10:05,840
perhaps in some cases we do we simply want to say here's my input what should

108
00:10:05,840 --> 00:10:11,200
I map it to as an output and we measure the performance based purely on how well

109
00:10:11,320 --> 00:10:18,000
it does that task with unsupervised learning we don't have in most cases

110
00:10:18,000 --> 00:10:24,920
this sort of an input output mapping we want to perform more abstract or vague

111
00:10:24,920 --> 00:10:29,400
tasks such as understanding what is the information in our data set for example

112
00:10:29,400 --> 00:10:35,200
we don't have an infinite amount of time to read you know so many thousands or

113
00:10:35,200 --> 00:10:40,240
millions of documents so we want a fast algorithmic way for taking in

114
00:10:40,240 --> 00:10:45,720
information taking in data and extracting the information for us so for

115
00:10:45,720 --> 00:10:49,560
example with unsupervised learning we might want to do something like topic

116
00:10:49,560 --> 00:10:59,840
modeling where we have many texts data many documents provided to us we don't

117
00:10:59,840 --> 00:11:03,400
have any labels for these documents all we have is the text for each document

118
00:11:03,400 --> 00:11:08,960
and then we want to extract what is the underlying what are the underlying

119
00:11:09,000 --> 00:11:14,520
themes within these documents so that's the idea of topic modeling we also might

120
00:11:14,520 --> 00:11:20,520
want to do recommendations this would be where we have many users and many

121
00:11:20,520 --> 00:11:27,320
objects and users will give feedback or input on a subset of these objects either

122
00:11:27,320 --> 00:11:32,240
through a review or through some sort of a rating like a star rating for example

123
00:11:32,240 --> 00:11:38,080
with Netflix a user could rate a movie one to five stars and we want to take

124
00:11:38,080 --> 00:11:43,520
all of this information and learn some sort of a latent space where we can

125
00:11:43,520 --> 00:11:48,800
embed users and movies such that users who are close to each other share

126
00:11:48,800 --> 00:11:54,520
similar tastes movies that are close to users are somehow appropriate

127
00:11:54,520 --> 00:11:59,120
recommendations to be made to those users movies that are close to each other

128
00:11:59,120 --> 00:12:03,760
are similar in their content and things that are very far apart are very unlike

129
00:12:03,760 --> 00:12:10,440
each other so we want to learn this information simply from the data from

130
00:12:10,440 --> 00:12:17,440
the raw data and some model assumption that we have to make so for example one

131
00:12:17,440 --> 00:12:22,600
of the most well-known unsupervised learning tasks is the topic modeling

132
00:12:22,600 --> 00:12:28,160
problem and so what I'm showing here is an example of what a topic model will

133
00:12:28,200 --> 00:12:33,560
learn if you provide it with over a million documents from the New York

134
00:12:33,560 --> 00:12:39,560
Times over roughly 20 year period so what we have in these documents is

135
00:12:39,560 --> 00:12:44,280
simply a tag that says that this is a document and these are the words in the

136
00:12:44,280 --> 00:12:48,480
document and we have that repeated for all of the documents in our data set

137
00:12:48,480 --> 00:12:53,400
again it can be over a million of these documents we want to make some sort of a

138
00:12:53,400 --> 00:12:57,080
modeling assumption such that we find the words that should somehow cluster

139
00:12:57,120 --> 00:13:03,120
together these words would then define topics underlying our data set so for

140
00:13:03,120 --> 00:13:08,760
example by simply inputting the raw data from the New York Times making a

141
00:13:08,760 --> 00:13:13,160
modeling assumption that doesn't in advance tell us which words should go

142
00:13:13,160 --> 00:13:19,000
with which other words we can then run an algorithm to extract information like

143
00:13:19,000 --> 00:13:24,080
this that says that this set of words belongs together this set of words

144
00:13:24,080 --> 00:13:29,040
belongs together and so on so we can learn for example 10 or more of these

145
00:13:29,040 --> 00:13:34,120
what are called topics that tell us which words belong together and then not

146
00:13:34,120 --> 00:13:39,200
shown here is also how each document uses that topic so for example for a

147
00:13:39,200 --> 00:13:43,840
particular document we might say it's composed of these two topics and no other

148
00:13:43,840 --> 00:13:49,040
topics and so this is information that's extracted from the raw data we don't

149
00:13:49,080 --> 00:13:54,080
a priori tell the algorithm what it should learn we simply say there is this

150
00:13:54,080 --> 00:14:00,000
structure that we want to learn here's the data tell me the structure

