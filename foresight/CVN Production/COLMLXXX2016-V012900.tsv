start	end	text
0	5480	Welcome to the first lecture of Machine Learning on Columbia X. I'm your
5480	11400	professor, John Paisley. I'm a member of the Department of Electrical Engineering
11400	16040	as well as the Data Science Institute at Columbia and my specialty is in
16040	20120	machine learning and this is this class that we'll be working through in the
20120	24120	following lectures is directly based on the machine learning course that I
24120	30840	teach here at Columbia as part of the Data Science Institute. So in this first
30840	36840	lecture I want to primarily focus on a general overview of the course and of
36840	42600	what machine learning is and then go into a little bit of detail on a very
42600	48520	specific problem working with multivariate Gaussians in order to kind of
48520	53160	highlight the different aspects and the different components of what we'll be
53160	59720	discussing in this course. So this course will cover model-based techniques
59720	65920	for extracting information from data with some sort of an end task in mind. So
65920	72240	these tasks could include for example predicting an unknown output given some
72240	78120	corresponding input. We could simply want to uncover the information
78160	84520	underlying our data set with the goal of better understanding what's
84520	89320	contained in our data that we have or we could do things like data driven
89320	96880	recommendation, grouping, classification, ranking, etc. So using the data to help us
96880	103880	learn how to perform these sorts of end tasks. So in a course like this there's
103920	109320	a few ways that the information can be presented, different orderings of the
109320	115960	information. One example would be to partition it as in one half supervised
115960	120120	learning, the other half unsupervised learning and I'll discuss that in a bit
120120	124040	more detail in this lecture because that's the perspective that we're going
124040	129880	to take. But we could also think in terms of probabilistic models where we are
129880	134440	working with probability distributions versus non probabilistic models where
134440	140880	we're learning from the data without any sort of probability probabilistic
140880	146280	motivation. There's also a dichotomy between modeling approaches. So what is
146280	152720	the model that we want to define for our data versus optimization approaches
152720	157000	which is very tightly linked with modeling approaches. But with optimization
157280	162320	now that we've defined a model, how do we learn the model? So these are two
162320	168240	separate problems with various techniques in these two sub problems. So again
168240	172960	we're going to partition this course into roughly two halves. The first half
172960	177600	will focus on supervised learning and the second half will focus on
177600	182680	unsupervised learning and these additional ideas such as probabilistic
182720	187560	versus non probabilistic or modeling approaches versus optimization techniques
187560	194360	will be sort of discussed as we go along as needed. So those will be
194360	197920	interwoven throughout the course but the first part of this course will be
199040	202560	strictly supervised learning and the second part will be on supervised
202560	208000	learning. What do we mean when we say we want to perform supervised learning?
208920	215560	In a nutshell what we're saying is that we want to take inputs and predict
215560	221520	corresponding outputs. So for example if we do want to do regression we might
221520	228520	have pairs of data in this case a one-dimensional value for x and a
228520	235000	corresponding one-dimensional value for t and then we would want to learn some
235040	239320	sort of a function so that we input x and we make a prediction for the output
239320	246480	t. So for example here we have several data points as indicated by circles
246480	253440	where the x-axis is the input for that particular point and the t-axis is the
253440	258720	corresponding output and now that we have this data set of these several
258720	264720	points we want to define some sort of a regression function for example this
264760	270560	blue line that in some way interpolates the outputs as a function of the inputs
271760	277480	and then the goal is given this smoothing function that we've learned for some new
277480	284200	input x we want to predict the corresponding output t so we for future
284840	290160	time points we obtain x we obtain an input and we want to predict the
290200	295240	corresponding output so that's regression we say it's regression because the
295240	301880	outputs are assumed to be real valued. Classification is another supervised
301880	307360	learning problem that is slightly different the form or the structure is
308080	313880	very similar we have pairs of inputs and outputs and we get this data set which
313880	317320	has many of these pairs of inputs and outputs and we want to learn some
317360	321720	functions so that in the future when we get a new input for which we don't have
321720	325560	the output we can make a prediction of the output that's going to be accurate
327040	331640	however the key difference here is that where with regression the output is a
331640	339160	real valued output with classification it's a discrete valued thing so it's a
339160	345280	category or a class so in this right plot what I'm showing are input output pairs
346120	351640	except now the input is a two-dimensional vector so here the input would be this
351640	357400	two-dimensional point and the output for this plot is being encoded by a color
357400	364840	so the output could be one of two values either a blue value or a an orange
364840	370240	value so in this case we want to take our data inputs and classify them into one
370240	377680	of two outputs so we get a data set like this with all of these input locations
377680	383360	and the corresponding color-coded output and now our goal is to learn some sort
383360	389920	of a function a classifier so that we can partition the space such as is shown
389920	396120	here where for a brand new point any of these points that we don't have the
396120	400960	output we can evaluate the function at that point and make a prediction of the
400960	405880	output so we might say for this data set we would partition this entire region
405880	414080	here these two regions into the orange class and this region here into the blue
414080	418960	class so any new points falling in this region will be assigned to the blue
418960	426120	class so the key here with supervised learning is that we're learning an
426120	434400	algorithm based on a function mapping from input to output we the outputs are
434400	438640	basically going to be telling us how to map the inputs so that we have an
438640	447320	accurate function so we have input output pairs so to look at a classic example
447320	453840	we could think of spam detection given some set of inputs like these two chunks
453840	459000	of text we would want to assign it a label plus one or minus one sometimes
459000	463160	we would say plus one or zero but we would want to sign it one of two possible
463160	469400	labels one label would correspond to an email that is spam and we would want to
469400	476200	then you know automatically delete that email and the other class would be non
476240	482040	spam emails emails that we want to put into our inbox and actually read so it's
482040	488040	essentially a filtering problem so for example we might have a data set a data
488040	492240	point like this containing this text and we would want to now input this into
492240	497040	some sort of a function and say is it spam or not in this case most likely
497040	503600	it's not or a data point like this this piece of text where we would input it
503600	509360	into the same exact function with the same classifier and in this case that
509360	514440	same classifier would say this email is a spam so we classify this email to spam
514440	521840	and this email to non spam using the same classifier learned from examples of
521840	528080	of labeled spam and labeled non spam emails so essentially the first half of
528080	533720	this course is going to be all about learning different ways that we can
533720	538720	define these functions to map inputs to outputs either regression models or
538720	544000	classification models depending on the problem as well as algorithms or
544000	547960	techniques for then learning the parameters of these models based on data
547960	552960	so that will take up the first half of this course there are many very useful
552960	557640	techniques very different techniques for performing these two tasks they'll
557720	561280	entail different ways of thinking about the problems probabilistic versus
561280	568160	non probabilistic the models that we define will require different ways or
568160	571440	different techniques for learning them so we'll need different optimization
571440	575760	techniques so the first half of this course will be all about supervised
575760	579960	learning then in the second half of the course we'll transition to unsupervised
579960	585280	learning and with unsupervised learning the goal is a bit more vague
585800	589840	supervised learning is very nice because we know that we want to map an input to
589840	596040	an output and honestly we don't necessarily even care how it's mapped we
596040	601000	don't care whether we learn anything by mapping it in many cases we don't
601000	605840	perhaps in some cases we do we simply want to say here's my input what should
605840	611200	I map it to as an output and we measure the performance based purely on how well
611320	618000	it does that task with unsupervised learning we don't have in most cases
618000	624920	this sort of an input output mapping we want to perform more abstract or vague
624920	629400	tasks such as understanding what is the information in our data set for example
629400	635200	we don't have an infinite amount of time to read you know so many thousands or
635200	640240	millions of documents so we want a fast algorithmic way for taking in
640240	645720	information taking in data and extracting the information for us so for
645720	649560	example with unsupervised learning we might want to do something like topic
649560	659840	modeling where we have many texts data many documents provided to us we don't
659840	663400	have any labels for these documents all we have is the text for each document
663400	668960	and then we want to extract what is the underlying what are the underlying
669000	674520	themes within these documents so that's the idea of topic modeling we also might
674520	680520	want to do recommendations this would be where we have many users and many
680520	687320	objects and users will give feedback or input on a subset of these objects either
687320	692240	through a review or through some sort of a rating like a star rating for example
692240	698080	with Netflix a user could rate a movie one to five stars and we want to take
698080	703520	all of this information and learn some sort of a latent space where we can
703520	708800	embed users and movies such that users who are close to each other share
708800	714520	similar tastes movies that are close to users are somehow appropriate
714520	719120	recommendations to be made to those users movies that are close to each other
719120	723760	are similar in their content and things that are very far apart are very unlike
723760	730440	each other so we want to learn this information simply from the data from
730440	737440	the raw data and some model assumption that we have to make so for example one
737440	742600	of the most well-known unsupervised learning tasks is the topic modeling
742600	748160	problem and so what I'm showing here is an example of what a topic model will
748200	753560	learn if you provide it with over a million documents from the New York
753560	759560	Times over roughly 20 year period so what we have in these documents is
759560	764280	simply a tag that says that this is a document and these are the words in the
764280	768480	document and we have that repeated for all of the documents in our data set
768480	773400	again it can be over a million of these documents we want to make some sort of a
773400	777080	modeling assumption such that we find the words that should somehow cluster
777120	783120	together these words would then define topics underlying our data set so for
783120	788760	example by simply inputting the raw data from the New York Times making a
788760	793160	modeling assumption that doesn't in advance tell us which words should go
793160	799000	with which other words we can then run an algorithm to extract information like
799000	804080	this that says that this set of words belongs together this set of words
804080	809040	belongs together and so on so we can learn for example 10 or more of these
809040	814120	what are called topics that tell us which words belong together and then not
814120	819200	shown here is also how each document uses that topic so for example for a
819200	823840	particular document we might say it's composed of these two topics and no other
823840	829040	topics and so this is information that's extracted from the raw data we don't
829080	834080	a priori tell the algorithm what it should learn we simply say there is this
834080	840000	structure that we want to learn here's the data tell me the structure
