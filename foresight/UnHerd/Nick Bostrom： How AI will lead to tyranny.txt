Hello and welcome to Unheard, I'm Florence Read. It appears that the world has entered
the age of existential risk. Every week, a new threat to the future of humanity comes
barreling down the track towards us. Reading the news can be, frankly, quite depressing
and I do it for a living, but someone who seemingly never tires of thinking about the
end of civilization and how to avoid it is Swedish-born philosophy professor Nick Bostrom.
He coined the term existential risk is the founder of the Future of Humanity Institute
at the University of Oxford and is the author of many bestselling books on theoretical physics,
computational neuroscience, logic, artificial intelligence, and many other subjects of which
I have no understanding. Professor Bostrom is the most cited professional
philosopher in the world under the age of 50, which may also have something to do with
being one of the only professional philosophers in the world under the age of 50. He joins
me live from his office in Oxford for the humble task of explaining the end of the
world in around 45 minutes. Nick, a very warm welcome to unheard.
Well, thank you. Okay, so we've got our business cut out for us.
We do. It's good. I like to set the bar high to start. Let's begin by getting at a definition
of existential risk. What do we actually mean? Do we mean the total annihilation of humanity,
the end of the world as we know it, or do we actually mean something closer to civilizational
collapse?
The concept of existential risk basically means ways that the human story could end prematurely.
That might mean literal extinction. That's one way we could end, but it could also mean
getting ourselves permanently locked into some radically suboptimal state. That in turn
could either be collapse from which we never recover, or you could imagine some kind of
global totalitarian surveillance dystopia that you could never overthrow. And if it were
sufficiently bad, that could also count as an existential catastrophe.
As for collapse scenarios, many of those might not be existential catastrophes because you
might imagine civilizations have risen and fallen, empires have come and gone, but eventually
maybe even if our own contemporary civilization totally collapsed, perhaps out of the ashes
would rise eventually another civilization, hundreds of years from now or thousands of
years from now. So for something to be an literal existential catastrophe, it would not just
have to be bad, but the badness would have to be lasting indefinitely.
A state of kind of semi-anarchy does feel that it's almost already descended. Perhaps
that's too extreme, but it does feel a little like that.
Yeah, I think there is like a general sense of many people have recently in the last few
years that sort of the wheels are coming off a little bit, or maybe they haven't fallen off,
but they're sort of rickety and things like institutional processes and long-term trends
that were previously taken for granted, like this is the modern world. They're still worse,
but they're going to be fewer and fewer every year, and there's still a lot of ignorance,
but the education system is gradually improving. And I think the kind of faith people had in
those assumptions have been shaken over the last five years or so. Somebody plays Russian roulette
once and survives. You shouldn't draw the lesson that Russian roulette is not dangerous. They should
just thank their lucky star that they survived that first round and make sure they never played
again. But I don't think we have kind of invested in building robust global conflict
resolution institutions or developing norms, etc. That could help.
Trust in global institutions as well seems to have fallen significantly because of the handling
of things like the COVID pandemic by these global institutions, which was in many cases so disastrous
that the individual has lost all interest in following the guidance of a kind of world economic
forum or its equivalent. Yeah, I mean, the whole COVID thing was a big, I mean, they certainly
didn't cover themselves in glory, the World Health Organization and many other authorities,
I should say, also kind of burned a lot of their credibility. Your people did very well.
But then even more, okay, so we get this big pandemic, some big institutions who ought to know
better sort of scramble around and do a bunch of bad things. Okay, so it's a hard challenge,
but then you might at least think after it's happened that there would be some sort of lessons
learned and that we would, you know, prepare ourselves for another pandemic that could happen
at any time from natural causes or because of biotechnology making it easier for
evildoers to conquer. But it seems like as soon as this was over, people just lost interest. And so
there is still gain of function research going on without any more safeguards than that was before
in different places of the world, open publication of the most dangerous viral recipes.
Your books, you write a lot about the way in which it's going to become increasingly necessary
for us to learn from each existential threat as we move forward and to try and create mitigation
methods so that the next time when it becomes more severe or more intelligent or more sophisticated,
that we can actually cope with that. And that specifically, of course, relates to artificial
intelligence, which is something that, of course, we have to cover here. It must be astonishing to
you to have the subject that you've dedicated decades of your life to studying suddenly becoming
a household conversation topic, something people are debating over the dinner table.
But it's quite striking how radically the public discourse on this has shifted even just in the
last six, 12 months. I've been involved in the field for a long time. You know, there were people
working on it, but broadly in society it was more viewed as a sort of science fiction speculation,
like kind of, you know, not really part of mainstream concern or certainly nothing that
top level policymakers would be. And but now we've had, I mean, just here in the UK, this recent
global AI summit, you know, the White House just came out with executive orders. There's been
quite a lot of talk, including about existential potential existential risks from AI as well as
more near term issues that is kind of striking. I think that technical progress is really what
has been primarily responsible for this. People saw with, you know, GPT-3, then GPT-3 and a half,
and chat GPT-4, and like just the delta between these, like how much, even just within a couple
of years, this technology has improved. And it's so general purpose, like the same AI system can,
you know, write poems or program computers or answer questions about history or anything. And
so these very large transformer models seem to have this general capacity to learn. And if they
are big enough and have enough data, they also seem to be able to learn to reason more abstractly
about all this information. How close are we to something that you might consider a kind of
singularity or AGI that does actually in some way supersedes any human control over it? Is that
something that is in the near distance or is that a long way away? It's something that we can't
any longer be confident is not in the near distance. So as far as we can see now, there is no
obvious clear barrier that would necessarily prevent systems next year or the year after from
reaching this level. It doesn't mean that that's the most likely scenario, but
we don't know what happens as you scale GPT-4 to GPT-5 level. Because we know that when you scale
it from GPT-3 to GPT-4, for example, it unlocked new abilities. There's this phenomenon of grokking
so that initially you try to learn some task and it's too hard for the AI. Maybe it gets slightly,
slightly better because it sort of memorizes more and more specific instances of the problem. And
but it's like the hard sluggish way of learning to do something. And then at some point,
it kind of gets it. Like once it has enough neurons in its brain or has seen enough example,
it sort of sees the underlying principle or develops the right higher level concept that
enables it to sort of suddenly have a rapid spike in performance. And we've seen as we have scaled
up these large language models that sort of each new order of magnitude of scaling has unlocked new
capabilities that and we don't really know what will happen if we add orders of magnitude. It
might unlock further capabilities, perhaps enough capabilities to make these systems capable of
long-term planning or capable of really high quality research into AI that that could then
create feedback loops and so on. I thought grok was an interesting name for X or Elon Musk to choose
for their AGI considering that it has such a human implication. It's about intuition or gut
feeling to grok something is to really understand it in a human way. I wonder if they're personally
trying to align the machine here with an idea of human sentience or understanding. It feels like
in back rooms of places like X and at the University of Oxford, these two things are
much closer than we might think. I think probably closer than most people think or I mean,
you really need to think in terms of probabilities. But there should be more probability mass
on nearer dates than most people think. But I think perhaps the bigger difference between say
what I would think and what the average person would think might not so much be about
how many years between now and this thing are there, but more like when this thing happens,
like how radical would that be or how quickly would things unfold at that point? Like once you
get to something that can say substitute for humans across half of all human jobs or something
like that. I think some people model this test and then maybe the next year it will be like 52%
and then 54% and then over a lifetime or two you might gradually get to complete automation. I think
that second phase will be much compressed and that you like once it really starts to happen,
I think people will be surprised about just the speed at which this unfolds, absent deliberate
measures to slow it down. Right. And my assumption is that there will be a company, one of these
major companies, whether it's OpenAI or X, who makes the first leap and then the others will
follow very quickly. There's a bunch of different ways that this could go on. It partly depends on
what policy makers think should be done as well. So at the moment it's still a very open and
competitive field with some number of big players and then startups that are sort of
vying to also join the frontier, but it's possible that if what is required to get cutting-edge
performance is increasingly large data centers to run these. So we've seen a very rapid increase
in the amount of spending on compute required to train the system. So like five years ago maybe
you could get by with like $100,000 of computer or something. But now you're probably like in
you probably need like probably billions, I mean at least a billion dollars worth of
Nvidia chips or something to sort of be able to train a cutting like the next level of model.
And a couple of years from now maybe that will be 10 billion. And so at that level it's going to
decrease the number of players that could participate at the frontier. You have basically a small
number of really big tech companies and then potentially governments. And even governments,
for many that's got to be too big an expenditure and for others they are not going to necessarily
have access to the Nvidia chips because of export restrictions. Maybe it's not a government run
project, but the projects that the government has a lot of oversight into. Which for some
people would be deeply worrying and for others reassuring. So one current debate is whether it
is good or bad for these current models to be open source. So Facebook opensource the Llama 2
model which is a little bit behind the frontier. It's not quite as good as GPT4 but it's still a
powerful model. And others are saying that this is irresponsible or at least that it would be
irresponsible to sort of open source the next generation and the next generation after that.
Because it could fall into the wrong hands, is that the logic there?
Well at the most basic level so these big tech companies they want to limit what their models
are used for. So they try to prevent them from say assisting users to commit cyber crime or to
do or say various bad things. But once somebody has access to the parameters of the model they can
usually retrain it in such a way as to sort of eliminate whatever safeguards were in the original
model. This is where we come up against this misalignment of values that you write about the
idea that we have to begin to teach AI at the earliest stages a set of values by which it will
function if we have any hope of kind of maintaining its benefit for humanity in the long term.
One of the values the liberal values that has been called into question with
when it comes to AI is freedom of speech. There has been examples of AI chat GPT the early forms of
effectively censoring information, filtering information that is available on the platform.
Do you think that there is a genuine threat to freedom and a kind of totalitarian impulse
built into some of these systems that we're going to see extended and kind of exaggerated
further down the line? I think AI is likely to greatly increase the ability that I say
some central power would have of keeping track of what people are thinking and saying.
So right now I mean we've had for a couple of decades I guess the ability to collect huge
amounts of information. I mean you could sort of eavesdrop on people's phone calls or social
media postings and turns out governments do that. But what can you do with that information?
I mean so far not that much. You could sort of map out the network of who is talking to whom
and then if there is a particular individual of concern you could assign some analysts to sort
of read through their emails. With AI technology you could sort of simultaneously analyze everybody's
political opinions in a sophisticated way like sentiment analysis. You could probably form a
pretty good idea of what each citizen thinks of the government or the current leader
if you had access to their communications and with even present AI tools. You don't have to
have much imagination to imagine how that could be sort of useful to particular people or regimes.
And then on top of that you will then be able to customize responses. So you could have sort of mass
manipulation but instead of sending out sort of one campaign message to everybody you could have
tailor-made customized persuasion message to each individual.
Appetize is already a user message. And then of course you can combine that with sort of physical
surveillance systems like with facial recognition and gate recognition and all of this information
going into one and credit card information. And like if you imagine all of this information
and the communications feeding into one giant model I think you will be able to have a pretty
good idea of what each person is up to and not just what they have done but also what
and who they know but also like maybe what they are thinking and intending to do.
And so the upshot of this might well be that I mean it will take a while to shake out because
there's a lot of inertia in these and but like eventually it might just be there like some
scenarios in which you get the kind of lock-in so that current political systems become
imperturbable. Like like if you have some sufficiently powerful regime in place it might
then implement these measures and then perhaps make itself immune to overthrow.
That does feel a little like what we've seen in the last few years in China.
People have lived in sort of highly censored societies and often what develops is the kind
of folk feeling for well we know they are lying to us and we are not talking about it but wink
notch notch and there is like a kind of and I think people can become over time quite
sophisticated at sort of seeing through official propaganda narratives.
Do you think the rise in hyper realistic propaganda, deep fake videos, what AI is going
to make possible in the coming years will that coincide with the rise in skepticism,
generalized skepticism like you're talking about in the Chinese example. Do you think the average
American or English or Swedish person is going to become hyper skeptical?
I think that in principle society could adjust to that but I think it will come at the same time
as a whole bunch of other things with sort of these automated persuasion bots like social
companions built of these large language models and then with visual components that might be very
compelling and addictive perhaps like and compete with social companionship and then
these mass surveillance, mass potential censorship or propaganda and also mass education like you
could also have like individual tutors I'm just talking a lot about the negative. We know from
the past I mean going back all the way like when people invented writings so that then enabled
states to form because you could keep tax records etc and like a huge change in how human societies
were organized politically then like with a printing press another big change that you know
ultimately maybe enabled sort of modern democracy and stuff but also like a hundred years of religious
wars when people like started forming their own opinion and came to different conclusions.
With social media I think we're also seeing quite a lot of turbulence and then when this gets AI
powered it might kick it up a notch further. Is there a distinction between a bad use of AI so in
those examples we're talking about a tyrannical government who uses AI to surveil its citizens
versus a kind of innate moral component to the AI itself. Is there a chance that an AGI model could
in some way become a bad actor on its own without human intervention? Yeah so there are a bunch of
different concerns that one might have as we move towards increasingly powerful AI tools and like
one like completely unnecessary feud that people have had is like well I think concern X
is should be taken seriously and someone else think I should I think concern Y should be taken
seriously and then like well but X well what about Y and then it's like people love just to form
tribes and to beat one another like an excuse to just form a little tribe that you can but I mean
I think like both X, Y, Z and B and W like need to be taken it's just such a big thing that there
are going to be many aspects of this that need to be addressed so yeah so one we were talking about
sort of these tools of AI and how they might shape political systems and this applies even just with
current AI tools for surveillance etc but yeah so then you're right there's also this separate
alignment problem which is with an arbitrarily powerful AI system how could you make sure that
it does what the people building it intend for it to do and this is where it's about building in
certain principles or what you might call a kind of ethical code into the system from the off is
that the way of mitigating that risk? Well yeah or being able to steer it basically I mean it's a
separate question of where do you steer it if you could build in some principle or goal like which
goal or which principle but but even just having the ability to point it towards any particular
outcome you want or set of principles you want it to follow that is a technical problem
that is hard and in particular what is hard is to figure out the way that we would be able to do
that would continue to work even in these scenarios where the AI system becomes smarter than us and
perhaps eventually radically super intelligent and where we are no longer able to maybe at that
point understand what it is doing or why it is doing it what's going on inside its brain we still
want this original like scaling method to to keep working to arbitrarily high levels of
intelligence and and we might need to get that right on the first try so so no pressure no pressure
nope no pressure in western liberal democracies or what might be considered western liberal
democracies we have a lot of values that are in direct conflict with a kind of utilitarian
way of living we think about welfare caring for those who are at the bottom of society those who
might not be able to contribute for some reason or another children prior to working age if a model
was created to just maximize utility you can imagine already the kind of what the potential
outcomes of that might be and i think for many people that is the existential fear is that the
computer brain would have a naturally more utilitarian bent than the human brain which
in these liberal democracies likes to think about in some way not maximizing utility and
maximizing efficiency but rather thinking about how do we best live in a common society with a
common good which are quite wishy washy concepts when you actually dig into them i think there
could be like two ways in which this so on the one hand you might object in general to this
consequentialist maximizing approach which i think makes sense to object to i think that doesn't
capture everything that we should care about in ethics but then there's like an additional
problem is even if you did assume that it there's something that should be maximized
we might not be able to spell out exactly what that is let alone give that set of instructions
to a computer without sort of forgetting or leaving out a bunch of important things of course
like if you ask someone what value do you want to organize a society round they might say family
family well of course a machine that constantly spouted out children would not be a good family
but it would certainly be a creator of family if you were worried about antinatalism and lower
birth rates in your country like somewhere like japan you might ask your computer to create something
that maximizes for family but already you can see how the concept of family to us as humans is so
distinct from just an idea of pumping out babies all the time one aspect of that problem i think
will get easier more or less automatically like there is a set of failures that would result from
you know imagining the ai not really to get what it is that you were meaning and it's like fixating
on the letter of the instruction or something but there as the ai becomes more capable i think it will
be able to understand not just the words but the intentions behind the words
and things like what we would maybe have added if we had thought about it longer or if
and stuff like that and i mean at least at the level at which a human is able to do that but
probably much better um the question then is like will it be motivated to pursue
this understanding that it is maybe developing about what we truly would want on reflection
that can be hard to do i mean like just to make it very simple like you might sort of like a
typical way that you do things is you you train it to do some tasks and then you say it's like
give it thumbs up when when it did it the way we wanted and thumbs down when it did it some other
way and that can work well during a certain regime when we can tell whether the outcome was good or
bad and where we remain in control to give it the thumbs up and thumbs down but then you start to
wonder whether what it really learns is to do the thing that we want or it starts to learn the thing
that the things will result in getting thumbs up and now when the AI is limited this might make no
difference because we give it thumbs up in exactly those cases where it did what we intended because
we understand what's going on and we are in control of the reward mechanism but if you then try to
generalize this to a future situation where the AI is in control or is smarter than us or whether
when it's doing something so complicated that we can't tell whether we should give it the thumbs
up or thumbs down then it might matter a great deal whether it was kind of learning to do the
things that triggers humans to give it thumbs up or whether it was learning to do the thing that we
really had in mind and it can be really hard during training to make sure that you sort of
teach it one thing rather than the other so that's an example of how like there are many
techniques that work now perfectly well for aligning these AI systems and but that we have
reason to think well systematically fail as they get situationally aware or when when they
find themselves in a position where they no longer are dependent on human approval
where they would be able to sort of short circuit that process or delude us would this not inevitably
lead to a situation where we have an ubermench class at the top of society who access super
intelligence before everyone else and that compounds because the super intelligence allows them to
access even higher forms of super intelligence so none of the rest of us can keep up well yeah
and it might not require any implantations at all but just having access to the leading edge systems
when other people don't have access just for advice or like like asking for strategy to achieve
something initially probably some subset of people will have that and maybe it will be
some people in the alabs or maybe it will be the us military or intelligent like some sort of
group who have appointed themselves to because surely somebody needs to test it out a little
first before we can give everybody access but then there's an interval during which they will have
access to maybe super intelligent advice that would place a lot of weight on the quality of the
like the trustworthiness of these individuals or institutions whatever exactly is the form of that
that that that has kind of early access the grave existential risk that we're all facing this week
is is war of course in the middle east in that situation you do have a very stark contrast between
the almost medieval warfare tactics of Hamas and the incredibly hyper modern warfare of israel with
the iron dome and the incredible technological capabilities how is ai going to play into future
wars is it already playing a part in this conflict we're seeing in israel i certainly think military
will be very interested in developing ai powered weapon systems and integrating it in in their
operations so far i think they have been lagging in like the cutting edge ai is not in some
lockheed martin research group but it's probably a relief actually yeah but um but as i think as
the strategic significance of this technology becomes more obvious to people there will be
an increased hammering from from sort of military intelligence establishments to to
have more control over this i would expect that that will happen increasingly as time goes by
if it is true that it will have these tremendous strategic and security implications it's it's
hard to see that there would not be a kind of that that that would all be just left to some
non-profit or cap profit silicon valley group of a few hundred people to sort of do what they please
like i i i demand and um people who who currently hold more powerful positions in those uh
domains would would want to get their hands on this as well and have have oversight of it do
you think we're ready for the intelligence explosion are we properly prepared or are we
underprepared no no no and i mean i don't think we'll ever i mean i think it's a little bit like
we are in some plane and we realize there is no pilot or the pilot has had a heart attack
and died and so now we are the passengers here we're going to try to land this right and it's
harder because we don't have sort of a ground control that is like giving us instructions so we
have to we see a big big instrument panel right and uh there are like a few people in the cockpit
kind of looking at all of this and like you could look at the fuel gauge and we've got some time left
in the air um like a limited period of time before this arrives and we got to sort of figure out
how to bring this uh this this bird in for a safe landing how do we do that with such
incredible levels of dispute and ideological schism across the world it feels like without a
kind of central body who's dealing with the ethics of this we're going to very quickly get
into a situation where different polar axes of the world are developing this stuff in a kind of
space race style competition but with their own ideological messaging and values built into it
which feels like such an existent such a clearly existential threat how what would be the first
step in the kind of Nick Bostrom global program to so to mitigate the risk of that I mean I I think
there are like a few things on the margin and then like if what if wishes were horses or whatever
like the ideal world but on the margin I think even a toothless but like affirmation of the general
principle that ultimately AI should be for the benefit of all all sentient life at least um it's
too big to be uh like if we're talking about superintelligence like like obviously if somebody's
making a cool little app or something they could benefit from that in the way that they benefit
from any other consumer thing that they're doing but if we're talking about a transition to the
superintelligence era all humans would be exposed to some of the risk in this whether they want it
or not and so it should seem fair that all should also stand to have some slice of the upside if it
goes well um and I would say this principle should even go beyond all of currently existing humans
and also include for example animals that we are treating very badly in many cases today and and
also the some of the digital minds themselves that might become moral subjects and so I think as
of right now I think like all of we might hope for in that demand is like some general
big principle that the firm stays and then that can sort of be firmed up as we go along so that's
one thing um I think another ask uh is and that's it's got some recent progress has been made on
this as well is for the next generation systems to be tested prior to deployment to check that they
don't lend themselves to uh people who would want to make um biological weapons of mass destruction
or or massive cyber crime thing and so far AI companies have sort of done some voluntary work
on this open AI before releasing chat gpt 4 like had the technology for more or less have
half a year and like did red teaming exercises to and I think like making that more of a requirement
um uh seems quite robustly good research on technical alignment seems good
to solve the problem of scalable alignment before we have superintelligence
I think the whole area of the moral status of digital mind will require more attention and
it's going to be that's that's now where the the the the alignment problem was like 10 years ago
like outside the overturn window a few people are talking about it but it's like seems slightly
silly like if you imagine having a meeting with a PM or or like somebody like you would not
like oh well what if the AI has moral status and is like that that's it's like a kind of yeah
fun thing to think about but I actually think it needs to start to migrate from that kind of
philosophy seminar topic to to where alignment is now which is like a serious mainstream
that's going to be built into policy and politicians are gonna have to think about
how they're gonna deal with it yeah we don't want to have a future where the majority of
of sentient minds are digital minds and they're all horribly oppressed and like basically like pigs
in in animal forms or something like that that that would be one way of creating a dystopia and it
it's going to be a big challenge because it's already hard for us to extend empathy sufficiently
to to animals even though animals have eyes and faces and can squeak and are much more similar
to like if it were an invisible process in a big data center but a sentient invisible process
but yeah like I mean it's got to be harder like and I think incidentally there might be grounds
for moral status besides sentience like I think that might be a sufficient if somebody can suffer
that might be sufficient to give them moral status but I think even if you thought they were
not conscious but if they had goals extend as a conception of self as an entity persisting
through time the ability to enter into reciprocal relationships with with other beings and humans
I think that might also ground various forms of moral status often in this conversation it
turns back to the human we worry about are the risk to us the risk to our freedoms and liberties
I suppose there we have to ask the question of what would it be like to live in a world
where you coexist with sentient beings who are not human and what what might that be like I mean
the only thing I can think of it that would be akin to it would be an alien invasion in which
then we were living alongside an alien species perhaps even stranger than that yeah except in
in the case of AI we get to design the alien species that we're going to share the world with
so that is a potentially crucial difference I want to know I suppose on the flip side of this
do you have hope for we've talked about a lot about the risks for the rewards of this if we
can design these aliens who we're going to coexist with for the next hundreds thousands
millions of years how best could we do that what is the kind of upside of this what would be the
best case scenario I think the upside is enormous and in fact my view is it would be tragic if we
never developed advanced artificial intelligence I think I think it's a kind of a portal through which
humanity will at some point have to passage that all the paths to really great futures
like ultimately lead through the development of of machine superintelligence but that this actual
transition itself will be associated with major risks and we need to be super careful to get that
right but I've started slightly worrying now in the last year or so that we might overshoot with
this increase in attention to the risks and downsides which I think is welcome because before
that this was neglected for decades we could have used this time actually to be in a much better
position now but people didn't but anyway so it's starting to get more of the attention it deserves
which is great and it still seems unlikely but less unlikely than it did a year ago that we might
overshoot and get to the point of like a permafrost like some situation where I is never developed
like a kind of AI nihilism that would come from being so afraid I'm so stigmatized that it just
becomes impossible for anybody to say anything positive about it and then and then we get one
of these other locking effects like with the other AI tools from surveillance and propaganda and
censorship and whatever this sort of orthodoxy is you know at five years from now ten years from
now whatever that sort of gets locked in somehow and we then never take this next step I think
that would be very tragic and I still think it's unlikely but certainly more likely than like even
just six or 12 months ago if you just plot the change in public attitude and policymaker attitude
and you sort of think what's happened in the last year if that continues to happen next year and the
year after and the year after that then I mean we pretty much be there like as a kind of permanent
ban on AI and I think that could be very bad I still would think we need to move to a greater
level of concern than we currently have but I would want us to sort of reach the optimal
level of concern and then stop there rather than just kind of come we're getting to like a Goldilocks
level of fear about AI I'm afraid that it's like a big wrecking ball that you can't really
control in a fine-grained way like people are people like the moving herds and like they get
an idea and then you know how people are I worry a little bit about it become a big
sort of stampede to say negative things about AI and then it just running completely out of control
and sort of destroying the future in that way instead and we don't and then of course we go
extinct through some other method instead maybe synthetic biology without even ever getting at
least to roll the die. So it's a bit of a pick your poison it just so happens that this poison
might cure you or poison you and you just have to kind of roll the dice on it. Yeah well I mean
yeah so I think yeah I mean there's like a bunch of stuff we can do to improve the odds and the
sequence of different things and stuff like that which we should do all of those. Being a scholar
of existential risk though I suppose puts you in the category or the camp of people who are often
this show being an example asked to speak about the terrifying hypothetical futures that AI could
draw us to do do you regret that focus on risk? Yeah I could maybe because I think now that there
was this deficit for decades it was obvious to me at least but it should have been pretty obvious
that eventually AI was going to succeed and then we were going to be confronted with this problem
of how do we control them and what do we do with them and that that's going to be really hard and
therefore risky and that was just neglected like there were like 10,000 people building AI but like
five or something thinking about how we would control them if we actually succeeded and so but
now that that's changed and this is recognized so I think there's less
need now maybe to add more to the sort of concern bucket. The doomerous work is done
and now you can go into the more silver lining. It's hard because it's like it's always a wobbly
thing and different groups of people have different views and there are still people
dismissing the risks or not thinking about them and I would think that actually the optimal level
of concern is slightly greater than what we currently have and so I still think there should
still be more concern. It's more dangerous than most people have realized but I'm just
starting to worry about it then kind of overshooting that and the conclusion being you know well
let's wait for like a thousand years before we do that and then of course it's unlikely that our
civilization will remain on track for a thousand years and so we're damned if we do and we're damned
if we don't. I mean we will hopefully be fine either way but I think I would like the AI before
some radical biotech revolution so if you think about it if you first get some sort of super
advanced synthetic quality that might kill us but if we're lucky we survive it and then maybe you
invent some super advanced molecular nanotechnology and that might kill us but if we're lucky we
survive that and then you do the AI and then maybe that will kill us or if we're lucky we survive
that and we get Utopia. Well then you have to get through sort of three separate existential
risks like first the biotech risks and then plus the nanotech risks plus the AI risks
whereas if we get AI first well maybe that will kill us but if not we get through that then I think
that will like handle the biotech and nanotech risks and so the total amount of existential
risk on that second trajectory would sort of be less than on the former. Now it's more complicated
than that because we need some time to prepare for the AI actually so but you can start to think
about sort of optimal trajectories rather than like a very simplistic binary question of is
technology X good or bad? We might more think like on the margin which one should we try to accelerate
which one's retard you know and you get the more nuanced picture of the field of possible
interventions that way I think. Do you have existential angst? Does this play on your mind
late at night when you're sitting in bed? Well late at night I'm usually still in my office
working I'm a sort of a night dolly person. I could have guessed that I think I could have guessed
that. I mean it is weird to be I mean if this worldview is even remotely correct that we should
happen to be alive at this particular point in human history so close to this fulcrum or nexus
right on which the giant future of earth originating intelligent life might hinge
and out of all the different people that have lived throughout history or all the later times that
might be people that if things go well that one should sit so close to this critical juncture
that seems a bit too much to be a coincidence maybe and then you're led to these questions
about the simulation hypothesis etc. I think there is more in heaven and on earth than
than is dreamt of in our philosophy and that we understand quite little about how all of these
pieces fit together. With that I will leave you to get back to your very important work trying to
put some of these pieces together at least a few of them. Thank you Nick Bostrom very much.
That was Professor Nick Bostrom of the University of Oxford. I felt a little torn there towards the
end. Nick was espousing the value and virtue of a future with artificial intelligence after 45
minutes of telling me exactly how it could bring an end to humanity. It feels like a confusing future
but he thinks one that's worth digging into rather than turning away from he cautioned there
as well of herd mentality a revulsion at the idea of an artificially intelligent future
that we must keep our eyes on the prize which is a good version of this inevitable acceleration.
Thanks to Nick for joining me to you for watching. This was Unheard.
