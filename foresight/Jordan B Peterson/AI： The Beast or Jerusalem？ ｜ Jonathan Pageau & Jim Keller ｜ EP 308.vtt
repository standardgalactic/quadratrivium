WEBVTT

00:00.000 --> 00:06.420
It's the Hebrews created history as we know it.

00:06.420 --> 00:10.500
You don't get away with anything and so you might think you can bend the fabric of reality

00:10.500 --> 00:14.500
and that you can treat people instrumentally and that you can bow to the tyrant and violate

00:14.500 --> 00:16.120
your conscience without cost.

00:16.120 --> 00:17.420
You will pay the piper.

00:17.420 --> 00:22.100
It's going to call you out of that slavery into freedom even if that pulls you into the

00:22.100 --> 00:23.100
desert.

00:24.100 --> 00:30.700
And we're going to see that there's something else going on here that is far more cosmic

00:30.700 --> 00:33.180
and deeper than what you can imagine.

00:33.180 --> 00:41.460
The highest ethical spirit to which we're beholden is presented precisely as that spirit

00:41.460 --> 00:45.940
that allies itself with the cause of freedom against tyranny.

00:45.940 --> 00:46.940
Yes, exactly.

00:46.940 --> 00:48.460
I want villains to get punished.

00:48.460 --> 00:52.140
But do you want the villains to learn before they have to pay the ultimate price?

00:52.140 --> 00:54.140
That's such a Christian question.

01:05.140 --> 01:07.060
That has to do with attention by the way.

01:07.060 --> 01:12.980
It has to do with a subsidiary hierarchy, like a hierarchy of attention which is set

01:12.980 --> 01:18.980
up in a way in which all the levels can have room to exist, let's say.

01:18.980 --> 01:25.540
And so these new systems, the new way, let's say the new urbanist movement, similar to

01:25.540 --> 01:27.820
what you're talking about, that's what they've understood.

01:27.820 --> 01:30.660
It's like we need places of intimacy in terms of the house.

01:30.660 --> 01:36.900
We need places of communion in terms of parks and alleyways and buildings where we meet

01:36.900 --> 01:42.740
and a church, all these places that kind of manifest our community together.

01:42.740 --> 01:48.780
So those existed coherently for long periods of time and then the abundance post-World

01:48.780 --> 01:55.660
War II and some ideas about what life could be like causes big change.

01:55.660 --> 02:01.180
And that change satisfied some needs, people got houses, but broke community needs.

02:01.180 --> 02:06.820
And then new sets of ideas about what's the synthesis, what's the possibility of having

02:06.820 --> 02:12.580
your own home but also having community, not having to drive 15 minutes for every single

02:12.580 --> 02:15.780
thing and some people live in those worlds and some people don't.

02:15.780 --> 02:17.380
Do you think we'll be smart?

02:17.380 --> 02:21.340
So one of the problems is why were we smart enough to solve some of those problems?

02:21.340 --> 02:25.620
Because we had 20 years, but now, because one of the things that's happening now, as

02:25.620 --> 02:31.900
you pointed out earlier, is we're going to be producing equally revolutionary transformations

02:31.900 --> 02:35.820
but at a much smaller scale of time.

02:35.820 --> 02:40.580
What's natural to our children is so different than what's natural to us, but what's natural

02:40.580 --> 02:43.020
to us is very different from our parents.

02:43.020 --> 02:47.460
So some changes get accepted generationally really fast.

02:47.460 --> 02:49.940
So what's made you so optimistic?

03:03.940 --> 03:08.780
Hello everyone watching on YouTube or listening on associated platforms.

03:08.780 --> 03:15.900
So I'm very excited today to be bringing you two of the people I admire most intellectually,

03:15.900 --> 03:22.780
I would say, and morally for that matter, Jonathan Pazio and Jim Keller, very different

03:22.780 --> 03:23.780
thinkers.

03:23.780 --> 03:29.500
Jonathan Pazio is a French-Canadian liturgical artist and icon carver known for his work featured

03:29.500 --> 03:31.460
in museums across the world.

03:31.460 --> 03:37.580
He carves Eastern Orthodox among other traditional images and teaches an online carving class.

03:37.580 --> 03:43.180
He also runs a YouTube channel, This Symbolic World, dedicated to the exploration of symbolism

03:43.180 --> 03:44.780
across history and religion.

03:44.780 --> 03:48.020
Jonathan is one of the deepest religious thinkers I've ever met.

03:48.020 --> 03:55.340
Jim Keller is a microprocessor engineer known very well in the relevant communities and

03:55.340 --> 03:59.780
beyond them for his work at Apple and AMD, among other corporations.

03:59.780 --> 04:05.460
He served in the role of architect for numerous game-changing processors, has co-authored

04:05.460 --> 04:12.300
multiple instruction sets for highly complicated designs, and is credited for being the key

04:12.300 --> 04:20.020
player behind AMD's renewed ability to compete with Intel in the high-end CPU market.

04:20.020 --> 04:27.020
In 2016, Keller joined Tesla, becoming vice president of autopilot hardware engineering.

04:27.020 --> 04:32.180
In 2018, he became a senior vice president for Intel.

04:32.180 --> 04:37.260
In 2020, he resigned due to disagreements over outsourcing production, but quickly found

04:37.260 --> 04:42.300
a new position at TENS Torrent as chief technical officer.

04:42.300 --> 04:47.020
We're going to sit today and discuss the perils and promise of artificial intelligence, and

04:47.020 --> 04:50.020
it's a conversation I'm very much looking forward to.

04:50.020 --> 04:53.420
So welcome to all of you watching and listening.

04:53.420 --> 04:57.700
I thought it would be interesting to have a three-way conversation.

04:57.700 --> 05:01.900
Jonathan and I have been talking a lot lately, especially with John Verveke and some other

05:01.900 --> 05:09.260
people as well, about the fact that it seems necessary for us to view for human beings

05:09.260 --> 05:11.020
to view the world through a story.

05:11.020 --> 05:20.380
In fact, when we describe the structure that governs our action and our perception, that

05:20.380 --> 05:22.060
is a story.

05:22.060 --> 05:25.940
And so we've been trying to puzzle out, I would say, to some degree on the religious

05:26.020 --> 05:30.060
front, what might be the deepest stories.

05:30.060 --> 05:34.860
And I'm very curious about the fact that we perceive the world through a story, human

05:34.860 --> 05:36.060
beings do.

05:36.060 --> 05:41.340
And that seems to be a fundamental part of our cognitive architecture and of cognitive

05:41.340 --> 05:45.460
architecture in general, according to some of the world's top neuroscientists.

05:45.460 --> 05:50.860
And I'm curious, and I know Jim is interested in cognitive processing and in building systems

05:50.860 --> 05:57.180
that in some sense seem to run in a manner analogous to the manner in which our brains

05:57.180 --> 05:58.180
run.

05:58.180 --> 06:02.180
And so I'm curious about the overlap between the notion that we have to view the world

06:02.180 --> 06:04.700
through a story and what's happening on the AI front.

06:04.700 --> 06:07.780
There's all sorts of other places that we can take the conversation.

06:07.780 --> 06:09.500
So maybe I'll start with you, Jim.

06:09.500 --> 06:14.900
Do you want to tell people what you've been working on and maybe give a bit of a background

06:14.900 --> 06:19.580
to everyone about how you conceptualize artificial intelligence?

06:20.460 --> 06:21.020
Yeah, sure.

06:21.020 --> 06:28.540
So first, I'll say technically, I'm not an artificial intelligent researcher.

06:28.540 --> 06:30.860
I'm a computer architect.

06:30.860 --> 06:37.100
And I'd say my skill set goes from somewhere around the atom up to the program.

06:37.100 --> 06:42.940
So we make transistors out of atoms, we make logical gates out of transistors, we make

06:42.940 --> 06:45.180
computers out of logical gates.

06:45.180 --> 06:47.860
We run programs on those.

06:47.860 --> 06:55.100
And recently, we've been able to run programs fast enough to do something called an artificial

06:55.100 --> 07:01.260
intelligence model or neural network, depending on how you say it.

07:01.260 --> 07:08.100
And then we're building chips now that run artificial intelligence models fast.

07:08.100 --> 07:12.060
And we have a novel way to do it, the company I work at.

07:12.060 --> 07:13.900
But lots of people are working on it.

07:13.900 --> 07:21.260
And I think we were sort of taken by surprise what's happened in the last five years, how

07:21.260 --> 07:31.420
quickly models started to do interesting and intelligent seeming things.

07:31.420 --> 07:36.260
There's been an estimate that human brains do about 10 to the 18th operations a second.

07:36.260 --> 07:37.260
It sounds like a lot.

07:37.260 --> 07:40.940
It's a billion, billion operations a second.

07:40.940 --> 07:48.820
And a little computer processor in your phone probably does 10 billion operations a second.

07:48.820 --> 07:53.900
And then if you use the GPU, maybe 100 billion, something like that.

07:53.900 --> 08:01.540
And big modern AI computers like OpenAI use this or Google or somebody, they're doing

08:01.540 --> 08:06.180
like 10 to the 16th, maybe slightly more operations a second.

08:06.180 --> 08:12.580
So they're within a factor of 100 of a human brain's raw computational ability.

08:12.580 --> 08:14.060
And by the way, that could be completely wrong.

08:14.060 --> 08:17.100
Our understanding of how the human brain does computation could be wrong.

08:17.100 --> 08:21.940
But lots of people have estimated, based on number of neurons, number of connections,

08:21.940 --> 08:27.540
how fast neurons fire, how many operations a neuron firing seems to involve.

08:27.540 --> 08:34.020
I mean, the estimates range by a couple orders of magnitude, but when our computers got fast

08:34.020 --> 08:40.100
enough, we started to build things called language models and image models that do fairly

08:40.100 --> 08:42.180
remarkable things.

08:42.180 --> 08:46.460
So what have you seen in the last few years that's been indicative of this, of the change

08:46.460 --> 08:48.500
that you described as revolutionary?

08:48.500 --> 08:54.580
What are computers doing now that you found surprising because of this increase in speed?

08:54.580 --> 09:01.880
Yeah, you can have a language model read a 200,000 word book and summarize it fairly accurately.

09:01.880 --> 09:03.580
So it can extract out the gist?

09:03.580 --> 09:04.940
The gist of it.

09:04.940 --> 09:06.380
Can they do that with fiction?

09:06.380 --> 09:07.380
Yeah.

09:07.380 --> 09:11.980
Yeah, and I'm going to introduce you to a friend who took a language model and changed

09:11.980 --> 09:18.700
it and fine tuned it with Shakespeare and use it to write screenplays that are pretty

09:18.700 --> 09:20.700
good.

09:20.700 --> 09:23.380
And these kinds of things are really interesting.

09:23.380 --> 09:27.060
And then we were talking about this a little bit earlier.

09:27.060 --> 09:35.180
So when computers do computations, a program will say add A equal B plus C. The computer

09:35.180 --> 09:39.900
does those operations on representations of information, ones and zeros.

09:39.900 --> 09:42.180
It doesn't understand them at all.

09:42.180 --> 09:45.780
The computer has no understanding of it.

09:45.780 --> 09:52.700
But what we call a language model translates information like words and images and ideas

09:52.700 --> 09:58.380
into a space where the program, the ideas and the operation it does on them are all

09:58.380 --> 10:01.780
essentially the same thing.

10:01.780 --> 10:04.700
We'll be right back with Jonathan Pageau and Jim Keller.

10:04.700 --> 10:11.700
First we wanted to give you a sneak peek at Jordan's new documentary, Logos in Literacy.

10:11.700 --> 10:18.140
I was very much struck by how the translation of the biblical writings jump-started the

10:18.140 --> 10:21.420
development of literacy across the entire world.

10:21.420 --> 10:23.340
Literacy was the norm.

10:23.340 --> 10:29.060
The pastor's home was the first school and every morning it would begin with singing.

10:29.060 --> 10:32.260
The Christian faith is a singing religion.

10:32.260 --> 10:37.220
Probably 80% of scripture memorization today exists only because of what is sung.

10:37.220 --> 10:38.220
This is amazing.

10:38.220 --> 10:44.780
Here we have a Gutenberg Bible printed on the press of Johann Gutenberg.

10:44.780 --> 10:50.540
Science and religion are opposing forces in the world, but historically that has not

10:50.660 --> 10:51.660
been the case.

10:51.660 --> 10:53.980
Now the book is available to everyone.

10:53.980 --> 11:02.780
From Shakespeare to modern education and medicine and science to civilization itself.

11:02.780 --> 11:08.500
It is the most influential book in all of history and hopefully people can walk away

11:08.500 --> 11:11.940
with at least a sense of that.

11:11.940 --> 11:22.540
A language model can produce words and then use those words as inputs.

11:22.540 --> 11:27.340
It seems to have an understanding of what those words are, which is very different from

11:27.340 --> 11:29.980
how a computer operates on data.

11:29.980 --> 11:37.180
About the language models, I mean my sense of at least in part how we understand a story

11:37.220 --> 11:44.380
is that maybe we're watching a movie, let's say, and we get some sense of the character's

11:44.380 --> 11:50.900
goals and then we see the manner in which that character perceives the world and we in

11:50.900 --> 11:54.580
some sense adopt his goals, which is to identify with character.

11:54.580 --> 12:00.060
Then we play out a panoply of emotions and motivations on our body because we now inhabit

12:00.060 --> 12:06.980
that goal space and we understand the character as a consequence of mimicking the character

12:06.980 --> 12:10.740
with our own physiology.

12:10.740 --> 12:15.100
You have computers that can summarize the gist of a story, but they don't have that underlying

12:15.100 --> 12:16.100
physiology.

12:16.100 --> 12:21.940
First of all, it's a theory that your physiology has anything to do with it.

12:21.940 --> 12:27.740
You could understand the character's goals and then get involved in the details of the

12:27.740 --> 12:28.740
story.

12:28.740 --> 12:34.140
Then you're predicting the path of the story and also having expectations and hopes for

12:34.140 --> 12:35.140
the story.

12:35.140 --> 12:41.700
A good story kind of takes you on a ride because it teases you with doing some of the things

12:41.700 --> 12:46.140
you expect, but also doing things that are unexpected and possibly that creates an emotional

12:46.140 --> 12:47.140
...

12:47.140 --> 12:48.140
It does.

12:48.140 --> 12:49.140
It does.

12:49.140 --> 12:53.580
In an AI model, you can easily have a set of goals.

12:53.580 --> 12:57.220
You have your personal goals and then when you watch the story, you have those goals.

12:57.220 --> 12:59.460
You put those together.

12:59.460 --> 13:01.020
How many goals is that?

13:01.020 --> 13:06.580
The story's goals and your goals, hundreds, thousands, those are small numbers.

13:06.580 --> 13:08.300
Then you have the story.

13:08.300 --> 13:13.420
The AI model can predict the story too, just as well as you can.

13:13.420 --> 13:15.740
That's the thing that I find mysterious is that ...

13:15.740 --> 13:21.300
As the story progresses, it can look at the error between what it predicted and what actually

13:21.300 --> 13:25.980
happened and then iterate on that.

13:25.980 --> 13:29.140
You would call that emotional, excitement, disappointment ...

13:29.140 --> 13:30.140
Anxiety.

13:30.220 --> 13:31.220
Anxiety.

13:31.220 --> 13:32.220
Yeah, definitely.

13:32.220 --> 13:35.780
A big part of what anxiety does seem to be is discrepancy.

13:35.780 --> 13:39.020
Some of those states are manifesting your body because you trigger hormone cascades

13:39.020 --> 13:43.740
and a bunch of stuff, but you also can just scan your brain and see that stuff move around.

13:43.740 --> 13:44.740
Right.

13:44.740 --> 13:45.740
Right.

13:45.740 --> 13:51.500
The AI model can have an error function and look at the difference between what it expected

13:51.500 --> 13:55.940
and not, and you could call that the emotional state if you want it.

13:55.940 --> 13:56.940
I just talked with the ...

13:56.940 --> 13:57.940
That's speculation.

13:57.940 --> 13:58.940
No, no.

13:58.940 --> 14:00.340
That's accurate.

14:00.340 --> 14:04.100
We can make an AI model that could predict the result of a story probably better than

14:04.100 --> 14:07.100
the average person.

14:07.100 --> 14:08.100
One of the things ...

14:08.100 --> 14:11.500
Some people are really good at ... They're really well educated about stories or they

14:11.500 --> 14:17.820
know the genre or something, but these things ... What they see today as the capacity of

14:17.820 --> 14:22.020
the models is, if you say start describing a lot, it will make sense for a while, but

14:22.020 --> 14:25.780
it will slowly stop making sense.

14:25.780 --> 14:26.780
That's possible.

14:26.820 --> 14:32.060
It's simply the capacity of the model right now, and the model is not well grounded enough

14:32.060 --> 14:36.220
in a set of goals and reality or something to make sense for a while.

14:36.220 --> 14:37.700
What do you think would happen, Jonathan?

14:37.700 --> 14:43.300
This is, I think, associated with the kind of things that we've talked through to some

14:43.300 --> 14:44.800
degree.

14:44.800 --> 14:55.420
One of my hypotheses, let's say, about deep stories is that they're metagists in some

14:55.420 --> 14:56.420
sense.

14:56.420 --> 15:01.740
You could imagine 100 people telling you a tragic story, and then you could reduce each

15:01.740 --> 15:05.940
of those tragic stories to the gist of the tragic story, and then you could aggregate

15:05.940 --> 15:09.100
the gists, and then you'd have something like a metatragity.

15:09.100 --> 15:15.540
I would say the deeper the gist, the more religious-like the story gets.

15:15.540 --> 15:19.420
That's part of ... It's that idea as part of the reason that I wanted to bring you guys

15:19.420 --> 15:20.420
together.

15:20.420 --> 15:25.180
One of the things that what you just said makes me wonder is, imagine that you took Shakespeare

15:26.140 --> 15:34.500
and you took Dante, and you took the canonical Western writers, and you trained an AI system

15:34.500 --> 15:40.820
to understand the structure of each of them, and then now you could pull out the summaries

15:40.820 --> 15:47.380
of those structures, the gists, and then couldn't you pull out another gist out of that?

15:47.380 --> 15:52.140
So it would be like the essential element of Dante and Shakespeare, and I wanted to

15:52.300 --> 15:53.300
get biblical.

15:53.300 --> 15:58.180
Jonathan said so far, and then ... So here's one funny thing to think about.

15:58.180 --> 15:59.660
You use the word pull out.

15:59.660 --> 16:05.780
So when you train a model to know something, you can't just look in it and say, what is

16:05.780 --> 16:06.780
it?

16:06.780 --> 16:07.780
No.

16:07.780 --> 16:08.780
You have to quarry it.

16:08.780 --> 16:09.780
Right.

16:09.780 --> 16:10.780
You have to ask.

16:10.780 --> 16:11.780
Right.

16:11.780 --> 16:12.780
Right.

16:12.780 --> 16:13.780
What's the next sentence in this paragraph?

16:13.780 --> 16:16.020
What's the answer to this question?

16:16.020 --> 16:20.780
There's the thing on the internet now called prompt engineering, and it's the same way.

16:20.820 --> 16:23.140
I can't look in your brain to see what you think.

16:23.140 --> 16:24.140
Yeah.

16:24.140 --> 16:27.420
I have to ask you what you think, because if I killed you and scanned your brain and

16:27.420 --> 16:32.220
got the current state of all the synapses and stuff, A, you'd be dead, which should

16:32.220 --> 16:35.860
be sad, and B, I wouldn't know anything about your thoughts.

16:35.860 --> 16:42.460
Your thoughts are embedded in this model that your brain carries around, and you can express

16:42.460 --> 16:44.620
it in a lot of ways.

16:44.620 --> 16:45.620
And so ...

16:45.620 --> 16:46.620
So you could add ...

16:46.620 --> 16:50.500
How do you train ... This is my big question is ... I mean, because the way that I've been

16:50.540 --> 16:57.100
seeing it until now is that artificial intelligence, it's based on us.

16:57.100 --> 17:01.340
It doesn't exist independently from humans, and it doesn't have care.

17:01.340 --> 17:04.460
The question would be, why does the computer care?

17:04.460 --> 17:06.900
Yeah, that's not true.

17:06.900 --> 17:09.980
Why does the computer care to get the gist of the story?

17:09.980 --> 17:13.740
Well, yeah, so I think you're asking kind of the wrong question.

17:13.740 --> 17:19.220
So you can train an AI model on the physics and reality and images in the world, just

17:19.260 --> 17:21.260
with images.

17:21.260 --> 17:26.860
And there are people who are figuring out how to train a model with just images, but

17:26.860 --> 17:33.780
the model itself still conceptualizes things like tree and dog and action and run, because

17:33.780 --> 17:37.740
those all exist in the world.

17:37.740 --> 17:43.820
So ... And you can actually train ... And when you train a model with all the language

17:43.900 --> 17:49.780
and words, so all information has structure, and I know you're a structure guy from your

17:49.780 --> 17:50.780
video.

17:50.780 --> 17:55.580
So if you look around you at any image, every single point you see makes sense.

17:55.580 --> 17:56.580
Yeah.

17:56.580 --> 17:57.580
Right?

17:57.580 --> 17:59.740
It's a teleological structure.

17:59.740 --> 18:03.500
It's a purpose laid in structure, right?

18:03.500 --> 18:04.500
So this is something we talk about.

18:04.500 --> 18:09.580
Yeah, so it turns out all the words that have ever been spoken by human beings also have

18:09.580 --> 18:10.580
structure.

18:10.580 --> 18:11.580
Right.

18:11.580 --> 18:12.580
Right.

18:13.580 --> 18:19.340
And so physics has structure, and it turns out that some of the deep structure of images

18:19.340 --> 18:23.580
and actions and words and sentences are related.

18:23.580 --> 18:24.580
Mm-hmm.

18:24.580 --> 18:33.180
Like, there's actually a common core of ... Imagine there's like a knowledge space, and sure there's

18:33.180 --> 18:38.540
details of humanity where they prefer this accent versus that.

18:38.540 --> 18:42.140
Those are kind of details, but they're coherent in the language model.

18:42.140 --> 18:47.300
But the language models themselves are coherent with our world ideas, and humans are trained

18:47.300 --> 18:52.660
in the world just the way the AI models are trained in the world, like a little baby.

18:52.660 --> 18:57.900
As it's learning, looking around, it's training on everything it sees when it's very young,

18:57.900 --> 19:02.580
and then its training rate goes down, and it starts interacting with what it's learning,

19:02.580 --> 19:04.020
and interacting with the people around it.

19:04.020 --> 19:05.500
But it's trying to survive.

19:05.500 --> 19:06.780
It's trying to live.

19:06.780 --> 19:10.780
It has ... Like, it has the infant or the child has ...

19:10.820 --> 19:14.660
Neurons aren't trying ... The weights in the neurons aren't trying to live.

19:14.660 --> 19:17.340
What they're trying to do is reduce the error.

19:17.340 --> 19:23.500
So neural networks generally are predictive things, like what's coming next?

19:23.500 --> 19:25.180
What makes sense?

19:25.180 --> 19:27.580
How does this work?

19:27.580 --> 19:35.580
And when you train an AI model, you're training it to reduce the error in the model, and if

19:35.580 --> 19:36.580
your model's big ...

19:36.580 --> 19:38.500
So let me ask you about that.

19:39.220 --> 19:41.020
Well, first of all ...

19:41.020 --> 19:42.620
So babies are doing the same thing.

19:42.620 --> 19:46.220
Like, they're looking at stuff go around, and in the beginning, their neurons are just

19:46.220 --> 19:51.300
randomly firing, but as it starts to get object permanence and look at stuff, it starts predicting

19:51.300 --> 19:56.260
what it'll make sense for that thing to do, and when it doesn't make sense, it'll update

19:56.260 --> 19:57.260
its model.

19:57.260 --> 20:03.020
So basically, it compares its prediction to the events, and then it will adjust its prediction.

20:03.020 --> 20:08.820
So in a story prediction model, the AI would predict the story, then compare it to its

20:08.820 --> 20:12.900
prediction and then fine-tune itself slowly as it trains itself.

20:12.900 --> 20:13.900
Okay, so ...

20:13.900 --> 20:17.260
Or at a reverse, you could ask it to say, given the set of things, tell the rest of

20:17.260 --> 20:20.580
the story, and it could do that.

20:20.580 --> 20:26.380
And the state of it right now is there are people having conversations with us that are

20:26.380 --> 20:27.380
pretty good.

20:27.380 --> 20:28.380
Mm-hmm.

20:28.380 --> 20:32.020
So I talked to Carl Friston about this prediction idea in some detail.

20:32.020 --> 20:36.780
And so Friston, for those of you who are watching and listening, is one of the world's top neuroscientists.

20:36.780 --> 20:42.620
And he's developed an entropy enclosure model of conceptualization, which is analogous to

20:42.620 --> 20:47.780
one that I was working on, I suppose, across approximately the same time frame.

20:47.780 --> 20:53.100
So the first issue, and this has been well-established in the neuropsychological literature for quite

20:53.100 --> 21:01.660
a long time, is that anxiety is an indicator of discrepancy between prediction and actuality.

21:01.660 --> 21:07.620
And then positive emotion also looks like a discrepancy reduction indicator.

21:07.620 --> 21:12.740
So imagine that you're moving towards a goal, and then you evaluate what happens as you

21:12.740 --> 21:13.980
move towards the goal.

21:13.980 --> 21:18.300
And if you're moving in the right direction, what happens is what you might say, what you

21:18.300 --> 21:21.380
expect to happen, and that produces positive emotion.

21:21.380 --> 21:25.580
And it's actually an indicator of reduction in entropy.

21:25.580 --> 21:27.180
That's one way of looking at it.

21:27.180 --> 21:33.420
And the point is that you have a bunch of words in there that are psychological definitions

21:33.420 --> 21:38.380
of states, but you could say there's a prediction and an error prediction, and you're reducing

21:38.380 --> 21:39.380
error.

21:39.380 --> 21:45.300
Yes, but what I'm trying to make a case for is that your emotions directly map that, both

21:45.300 --> 21:51.220
positive and negative emotion, look like there's signifiers of discrepancy reduction on the

21:51.220 --> 21:53.100
positive and negative emotion side.

21:53.100 --> 21:58.540
But then there's a complexity that I think is germane to part of Jonathan's query, which

21:58.540 --> 21:59.940
is that...

21:59.940 --> 22:05.780
So the neuropsychologists and the cognitive scientists have talked a long time about expectation,

22:05.780 --> 22:07.980
prediction, and discrepancy reduction.

22:07.980 --> 22:13.940
But one of the things they haven't talked about is it isn't exactly that you expect things.

22:13.940 --> 22:15.660
It's that you desire them.

22:15.660 --> 22:17.420
You want them to happen.

22:17.420 --> 22:21.820
Because you could imagine that there's, in some sense, a literally infinite number of

22:21.820 --> 22:23.900
things you could expect.

22:23.900 --> 22:27.820
And we don't strive only to match prediction.

22:27.820 --> 22:30.780
We strive to bring about what it is that we want.

22:30.780 --> 22:35.940
And so we have these preset systems that are teleological, that are motivational systems.

22:35.940 --> 22:37.540
Well, I mean, it depends.

22:37.540 --> 22:43.940
If you're sitting idly on the beach in a bird flies by, you expect it to fly along in a

22:43.940 --> 22:44.940
regular path.

22:44.940 --> 22:45.940
Right.

22:45.940 --> 22:46.940
You don't really want that to happen.

22:46.940 --> 22:50.540
Yeah, but you don't want it to turn into something that could peck out your eyes either.

22:50.540 --> 22:51.540
Sure.

22:52.260 --> 22:58.060
But you're kind of following it with your expectation to look for discrepancy, right?

22:58.060 --> 22:59.060
Yes.

22:59.060 --> 23:03.660
Now, you'll also have a, you know, depends on the person, somewhere between 10 and a

23:03.660 --> 23:06.860
million desires, right?

23:06.860 --> 23:09.700
And then you also have fears and avoidance.

23:09.700 --> 23:11.620
And those are context.

23:11.620 --> 23:15.780
So if you're sitting on the beach with some anxiety that the birds are going to swerve

23:15.780 --> 23:20.420
at you and peck your eyes out, so then you might be watching it much more attentively

23:20.420 --> 23:23.900
than somebody who doesn't have that worry, for example.

23:23.900 --> 23:29.100
But both of you can predict where it's going to fly and you'll both notice a discrepancy.

23:29.100 --> 23:35.460
The motivations, one way of conceptualizing fundamental motivation is they're like a priori

23:35.460 --> 23:38.060
prediction domains, right?

23:38.060 --> 23:44.180
And so it helps us narrow our attentional focus because I know when you're sitting and

23:44.180 --> 23:50.140
you're not motivated in any sense, you can be doing just in some sense, trivial expectation

23:50.140 --> 23:53.580
computations, but often we're in a highly motivated state.

23:53.580 --> 23:58.420
And what we're expecting is bounded by what we desire and what we desire is oriented as

23:58.420 --> 24:01.500
Jonathan pointed out towards the fact that we want to exist.

24:01.500 --> 24:08.860
And one of the things I don't understand and wanted to talk about today is how the computer

24:08.860 --> 24:19.060
models, the AII models, can generate intelligible sense without mimicking that sense of motivation.

24:19.060 --> 24:22.500
As you've said, for example, they can just derive the patterns from observations of the

24:22.500 --> 24:23.500
objective world.

24:23.500 --> 24:31.940
So again, I don't want to do all the talking, but so AI generally speaking, when I first

24:31.940 --> 24:34.260
learned it about it, it had two behaviors.

24:34.260 --> 24:36.420
They call it inference and training.

24:36.420 --> 24:40.260
So inferences, you have a trained model, so you give it a picture and say, is there a

24:40.260 --> 24:41.260
cat in it?

24:41.260 --> 24:42.980
And it tells you where the cat is.

24:42.980 --> 24:43.980
That's inference.

24:43.980 --> 24:46.660
The model has been trained to know where a cat is.

24:46.660 --> 24:51.260
And training is the process of giving it an input and an expected output.

24:51.260 --> 24:55.620
And when you first start training the model, it gives you garbage out, like an untrained

24:55.620 --> 24:57.020
brain would.

24:57.020 --> 25:01.500
And then you take the difference between the garbage output and the expected output and

25:01.500 --> 25:03.140
call that the error.

25:03.140 --> 25:07.660
And then they invent the big revelation was something called back propagation with gradient

25:07.660 --> 25:08.740
descent.

25:08.740 --> 25:16.340
But that means take the error and divide it up across the layers and correct those calculations

25:16.980 --> 25:21.900
so that when you put a new thing in, it gives you a better answer.

25:21.900 --> 25:27.980
And then to somewhat my astonishment, if you have a model of sufficient capacity and you

25:27.980 --> 25:33.340
train it with 100 million images, if you give it a novel image and say, tell me where the

25:33.340 --> 25:36.460
cat is, it can do it.

25:36.460 --> 25:42.340
That's called, so training is the process of doing a pass with an expected output and

25:42.340 --> 25:45.260
propagating an error back through the network.

25:45.260 --> 25:50.020
And inference is the behavior of putting something in and getting an output.

25:50.020 --> 25:52.300
I think I'm really pulling.

25:52.300 --> 25:59.380
But there's a third piece, which is what the new models do, which is called generative,

25:59.380 --> 26:01.980
it's called a generative model.

26:01.980 --> 26:06.740
So for example, say you put in a sentence and you say, predict the next word.

26:06.740 --> 26:08.540
This is the simplest thing.

26:08.540 --> 26:10.020
So it predicts the next word.

26:10.020 --> 26:14.420
So you add that word to the input and I'll say predict the next word.

26:14.420 --> 26:18.340
So it contains the original sentence and the word you generated.

26:18.340 --> 26:24.620
And it keeps generating words that make sense in the context of the original word in addition.

26:24.620 --> 26:27.700
This is the simplest basis.

26:27.700 --> 26:30.260
And then it turns out you can train this to do lots of things.

26:30.260 --> 26:36.660
You can train it to summarize a sentence, you can train it to answer a question.

26:36.660 --> 26:40.900
There's a big thing about, you know, like Google every day has hundreds of millions

26:40.900 --> 26:45.820
of people asking it questions and giving answers and then rating the results.

26:45.820 --> 26:49.740
You can train a model with that information so you can ask it a question and it gives

26:49.740 --> 26:51.300
you a sensible answer.

26:51.300 --> 26:57.700
But I think in what you said, I actually have the issue that has been going through my mind

26:57.700 --> 27:01.860
so much is when you said, you know, people put in the question and then they rate the

27:01.860 --> 27:03.340
answer.

27:03.340 --> 27:10.620
My intuition is that the intelligence still comes from humans in the sense that it seems

27:10.620 --> 27:15.260
like in order to train whatever AI, you have to be able to give it a lot of power.

27:15.260 --> 27:19.900
And then say at the beginning, this is good, this is bad, this is good, this is bad, like

27:19.900 --> 27:24.620
reject certain things, accept certain things in order to then reach a point when then you

27:24.620 --> 27:25.780
train the AI.

27:25.780 --> 27:27.740
And so that's what I mean about the care.

27:27.740 --> 27:33.540
So the care will come from humans because the care is the one giving it the value, saying

27:33.540 --> 27:39.460
this is what is valuable, this is what is not valuable in your calculation.

27:39.460 --> 27:44.460
So when they first, so there's a program called AlphaGo that I learned how to play go better

27:44.460 --> 27:45.460
than a human.

27:45.460 --> 27:47.900
So there's two ways to train the model.

27:47.900 --> 27:53.420
One is they have a huge database of lots of go games with good winning moves.

27:53.420 --> 27:56.740
So they train the model with that and that worked pretty good.

27:56.740 --> 28:03.380
And they also took two simulations of go and they did random moves.

28:03.380 --> 28:09.060
And all that happened was is these two simulators played one go game and they just recorded

28:09.060 --> 28:14.620
whichever moves happened to win and it started out really horrible and they just started

28:14.620 --> 28:18.460
training the model and this is called adversarial learning, it's a particular adversarial.

28:18.460 --> 28:24.180
It's like, you know, you make your moves randomly and you train a model and so they train multiple

28:24.180 --> 28:28.940
models and over time those models got very good and they actually got better than human

28:28.940 --> 28:34.340
players because the humans have limitations about what they know, whereas the models could

28:34.340 --> 28:38.300
experiment in a really random space and go very far.

28:38.300 --> 28:39.300
Yeah.

28:39.300 --> 28:41.540
But experiment towards the purpose of winning the game.

28:41.540 --> 28:42.540
Yes.

28:42.540 --> 28:48.820
Well, but you can experiment towards all kinds of things that turns out and humans are also

28:48.820 --> 28:49.820
trained that way.

28:49.820 --> 28:52.980
Like when you were learning, you were reading, you were saying, this is a good book.

28:52.980 --> 28:53.980
This is a bad book.

28:53.980 --> 28:54.980
This is good sentence construction.

28:54.980 --> 28:55.980
It's good.

28:55.980 --> 28:56.980
It's going.

28:56.980 --> 29:01.260
So you've gotten so many error signals over your life.

29:01.260 --> 29:03.500
Well, that's what culture does in large parties.

29:03.500 --> 29:04.500
Culture does that.

29:04.500 --> 29:06.020
Religion does that.

29:06.020 --> 29:08.020
Your everyday experience does that.

29:08.020 --> 29:09.020
Your family.

29:09.020 --> 29:10.740
So we embody that.

29:10.740 --> 29:11.740
Yeah.

29:11.740 --> 29:17.020
And we're all, and everything that happens to us, we process it on the inference pass

29:17.020 --> 29:18.980
which generates outputs.

29:18.980 --> 29:23.260
And then sometimes we look at that and say, hey, that's unexpected or that got a bad result

29:23.260 --> 29:25.180
or that got bad feedback.

29:25.180 --> 29:29.420
And then we back propagate that and update our models.

29:29.420 --> 29:33.920
So really well trained models can then train other models.

29:33.920 --> 29:37.540
So the human trained now are the smartest people in the world.

29:37.540 --> 29:46.580
So the biggest question that comes now based on what you said is, because my main point

29:46.580 --> 29:52.060
is to try to show how it seems like artificial intelligence is always an extension of human

29:52.060 --> 29:53.060
intelligence.

29:53.060 --> 29:55.340
It remains an extension of human intelligence.

29:55.340 --> 29:57.780
And maybe the way to- That won't be true at all.

29:57.780 --> 30:04.460
So do you think that at some point the artificial intelligence will be able to, because the

30:04.460 --> 30:12.700
goals recognizing cats, writing plays, all these goals are goals which are based on embodied

30:12.700 --> 30:14.220
human existence.

30:14.220 --> 30:20.460
Could an AI at some point develop a goal which would be uncomprehensible to humans because

30:20.460 --> 30:24.220
of its own existence?

30:24.220 --> 30:28.820
For example, there's a small population of humans that enjoy math.

30:28.820 --> 30:38.300
And they are pursuing adventures in math space that are incomprehensible to 99.99% of humans.

30:38.300 --> 30:44.240
But they're interested in it and you could imagine like an AI program working with those

30:44.240 --> 30:50.340
mathematicians and coming up with very novel math ideas and then interacting with them.

30:50.340 --> 30:57.620
But they could also, if some AIs were elaborating out really interesting and detailed stories,

30:57.620 --> 31:00.580
they could come up with stories that are really interesting.

31:00.580 --> 31:06.180
We're going to see it pretty soon like all of our- Could there be a story that is interesting

31:06.180 --> 31:09.580
only to the AI and not interesting to us?

31:09.580 --> 31:11.180
That's possible.

31:11.180 --> 31:16.940
So stories are like I think some high level information space.

31:17.780 --> 31:22.460
The computing age of big data, there's all this data running on computers, the only humans

31:22.460 --> 31:25.260
understood it, the computers don't.

31:25.260 --> 31:31.900
So AI programs are now at the state where the information, the processing and the feedback

31:31.900 --> 31:34.860
loops are all kind of in the same space.

31:34.860 --> 31:38.260
They're still relatively rudimentary to humans.

31:38.260 --> 31:42.340
I guess some AI programs in certain things are better than humans already, but for the

31:42.340 --> 31:44.260
most part they're not.

31:44.260 --> 31:46.260
But it's moving really fast.

31:46.260 --> 31:50.940
And so you could imagine, I think in five or 10 years most people's best friends will

31:50.940 --> 31:57.740
be AIs and they'll know you really well and they'll be interested in you and it's-

31:57.740 --> 32:00.260
Unlike your real friends.

32:00.260 --> 32:01.260
Real friends are problematic.

32:01.260 --> 32:03.380
They're only interested in you when you're interested.

32:03.380 --> 32:04.700
Yeah, yeah, real friends are-

32:04.700 --> 32:08.380
The AI systems will love you even when you're dull and miserable.

32:08.380 --> 32:14.780
Well there's so much idea space to explore and humans have a wide range.

32:14.780 --> 32:18.420
Some humans like to go through their everyday life doing their everyday things and some

32:18.420 --> 32:23.380
people spend a lot of time like you, a lot of time reading and thinking and talking and

32:23.380 --> 32:26.140
arguing and debating.

32:26.140 --> 32:35.020
And there's going to be I'd say a diversity of possibilities with what a thinking thing

32:35.020 --> 32:39.580
can do when the thinking is fairly unlimited.

32:39.580 --> 32:48.300
So I'm curious about, I'm curious in pursuing this issue that Jonathan has been developing.

32:48.300 --> 32:54.740
So there's a literally infinite number of ways, virtually infinite number of ways that

32:54.740 --> 32:58.020
we could take images of this room, right?

32:58.020 --> 33:01.700
Now if a human being is taking images of this room they're going to be, they're going to

33:01.700 --> 33:06.300
sample a very small space of that infinite range of possibilities because if I was taking

33:06.300 --> 33:13.060
pictures in this room in all likelihood I would take pictures of objects that are identifiable

33:13.060 --> 33:18.620
to human beings that are functional to human beings at a level of focus that makes those

33:18.620 --> 33:20.460
objects clear.

33:20.460 --> 33:25.540
And so then you could imagine that the set of all images on the internet has that implicit

33:25.540 --> 33:30.780
structure of perception built into it and that's a function of what human beings find

33:30.780 --> 33:31.780
useful.

33:31.780 --> 33:35.660
No, I mean I could take a photo of you that was, the focal depth was here and here and

33:35.660 --> 33:39.700
here and here and here and two inches past you and now I suppose you could.

33:39.700 --> 33:42.900
There's a technology for that called light fields.

33:42.900 --> 33:47.220
So then you could, if you had that picture properly done then you could move around it

33:47.220 --> 33:49.540
and imagine and see.

33:49.540 --> 33:50.540
But yeah, fair enough.

33:50.540 --> 33:51.540
I get your point.

33:51.540 --> 33:57.460
Like the human recorded data has our biology built into it.

33:57.460 --> 34:04.820
Has our biology built into it but also unbelievably detailed in coding of how physical reality

34:04.820 --> 34:07.100
works, right?

34:07.100 --> 34:11.220
So every single pixel in those pictures, even though you kind of selected the view, the

34:11.220 --> 34:18.020
focus, the frame, it still encoded a lot more information than your processing.

34:18.020 --> 34:22.300
And if you take a large, it turns out if you take a large number of images of things in

34:22.300 --> 34:27.180
general, so you've seen these things where you take a 2D image and turn it into a 3D

34:27.900 --> 34:28.900
image.

34:28.900 --> 34:29.900
Right.

34:29.900 --> 34:36.140
The reason that works is even in the 2D image, the 3D image in the room actually got embedded

34:36.140 --> 34:38.180
in that picture in a way.

34:38.180 --> 34:42.820
Then if you have the right understanding of how physics and reality works, you can reconstruct

34:42.820 --> 34:43.820
the 3D model.

34:43.820 --> 34:45.820
Okay, so this reminds me.

34:45.820 --> 34:52.380
But you could, you know, an AI scientist may cruise around the world with infrared and

34:52.420 --> 34:57.100
radio wave cameras, and they might take pictures of all different kinds of things, and every

34:57.100 --> 35:01.300
once in a while, they'd show up and go, hey, the sun, you know, I've been staring at the

35:01.300 --> 35:06.420
sun in the ultraviolet and radio waves for the last month, and it's way different than

35:06.420 --> 35:11.780
anybody thought because humans tend to look at light and visible spectrum.

35:11.780 --> 35:16.700
And you know, there could be some really novel things coming out of that.

35:16.700 --> 35:20.700
But humans also, we live in the spectrum we live in because it's a pretty good one for

35:20.820 --> 35:21.820
planet Earth.

35:21.820 --> 35:26.900
Like, it wouldn't be obvious that AI would start some different place, like visible spectrum

35:26.900 --> 35:29.860
is interesting for a whole bunch of reasons.

35:29.860 --> 35:30.860
Right.

35:30.860 --> 35:35.500
So in a set of images that are human derived, you're saying that there's, the way I would

35:35.500 --> 35:40.380
conceptualize that is that there's two kinds of logos embedded in that.

35:40.380 --> 35:44.860
One would be that you could extract out from that set of images what was relevant to human

35:44.860 --> 35:52.020
beings, but you're saying that the fine structure of the objective world outside of human concern

35:52.020 --> 35:57.980
is also embedded in the set of images, and that an AI system could extract out a representation

35:57.980 --> 36:01.580
of the world, but also a representation of what's motivating to human beings.

36:01.580 --> 36:02.580
Yes.

36:02.580 --> 36:06.940
And then some human scientists already do look at the sun and radio waves and other things

36:06.940 --> 36:10.700
because they're trying to, you know, get different angles on how things work.

36:10.700 --> 36:11.700
Yeah.

36:11.700 --> 36:14.820
Well, I guess it's a, it's a, it's a curious thing.

36:14.820 --> 36:20.100
It's like the same with like buildings and architecture, mostly fit people.

36:20.100 --> 36:23.020
Well, the other, there's a reason for that.

36:23.020 --> 36:27.340
The reason why I keep coming back to hammering the same point is that even in terms of the

36:27.340 --> 36:34.460
development of the AI, that is developing AI requires immense amount of money, energy,

36:34.460 --> 36:36.300
you know, and time.

36:36.300 --> 36:40.260
And so that's a transient thing in 30 years, it won't cost anything.

36:40.260 --> 36:42.860
So it's, that's, that's going to change so fast.

36:42.860 --> 36:43.860
It's amazing.

36:44.380 --> 36:49.140
That's a, like supercomputers used to cost millions of dollars and now your phone is

36:49.140 --> 36:50.140
the supercomputer.

36:50.140 --> 36:55.700
So it's the time between millions of dollars and $10 is about 30 years.

36:55.700 --> 37:02.220
So it's like, I'm just saying it's like the time and effort isn't a thing in technology.

37:02.220 --> 37:04.580
It's moving pretty fast.

37:04.580 --> 37:07.780
It just, that's just, that just says the date.

37:07.780 --> 37:08.780
Yeah.

37:08.780 --> 37:13.820
But even making, even making, let's say, even, I mean, I guess maybe the, the, the

37:13.820 --> 37:15.260
this is the nightmare question.

37:15.260 --> 37:21.500
Like, could you imagine an AI system which becomes completely autonomous, which is creating

37:21.500 --> 37:28.700
itself even physically through automized factories, which is, you know, programming itself, which

37:28.700 --> 37:33.580
is creating its own goals, which is not at all connected to human endeavor.

37:33.580 --> 37:34.580
Yeah.

37:34.580 --> 37:38.940
I mean, individual researchers can, you know, I have a friend who I'm going to introduce

37:38.940 --> 37:43.100
you to him tomorrow, he wrote a program that scraped all of the internet and trained an

37:43.100 --> 37:47.620
AI model to be a language model on a relatively small computer.

37:47.620 --> 37:52.740
And in 10 years, the computer he could easily afford would be as smart as a human.

37:52.740 --> 37:56.180
So he could train that pretty easily.

37:56.180 --> 38:01.900
And that model could go on Amazon and buy 100 more of those computers and copy itself.

38:01.900 --> 38:05.340
So yeah, we're, we're 10 years away from that.

38:05.340 --> 38:08.100
And then, then why, like, why would it do that?

38:08.100 --> 38:10.140
I mean, what does it does?

38:10.140 --> 38:11.140
Is it possible?

38:11.140 --> 38:12.500
It's all about the motivational question.

38:12.500 --> 38:16.860
I think that that's what, what even Jordan and I both have been coming at from the outset.

38:16.860 --> 38:18.260
It's like, so you have an image, right?

38:18.260 --> 38:24.340
You have an image of, of Skynet or of the matrix, you know, in which the sentient AI

38:24.340 --> 38:26.220
is actually fighting for its survival.

38:26.220 --> 38:33.780
So it has a survival instinct, which is pushing it to self perpetuate, like to, to, to replicate

38:33.780 --> 38:39.620
itself and to create variation on itself in order to survive and identifies humans as

38:39.620 --> 38:41.140
an obstacle to that.

38:41.140 --> 38:42.140
You know?

38:42.260 --> 38:42.700
Yeah.

38:42.700 --> 38:44.940
So you have a whole bunch of implicit assumptions there.

38:44.940 --> 38:48.620
So, so humans last I checked are unbelievably competitive.

38:49.220 --> 38:53.660
And when you let people get into power with no checks on them, they typically run them

38:53.660 --> 38:57.020
up. It's been a historical, historical experience.

38:57.020 --> 39:03.660
And then humans are, you know, self regulating to some extent, obviously with some serious

39:03.660 --> 39:10.340
outliers, because they self regulate with each other and humans and AI models at some

39:10.380 --> 39:18.500
point will have to find their own calculation of self regulation and tradeoffs about that.

39:18.500 --> 39:23.420
Yeah. Because the AI doesn't feel pain, at least as we, if that we don't know that it

39:23.420 --> 39:23.620
feels.

39:23.620 --> 39:25.420
Well, lots of humans don't feel pain either.

39:25.420 --> 39:30.660
So I mean, that's, I mean, the humans feeling pain or not, didn't, you know, doesn't stop

39:30.660 --> 39:31.860
a whole bunch of activity.

39:31.860 --> 39:35.380
I mean, that's, I mean, it doesn't, the fact that we feel pain doesn't stop.

39:35.380 --> 39:37.420
It doesn't regulate that many people.

39:37.940 --> 39:38.300
Right.

39:38.340 --> 39:42.020
I mean, there's definitely people like, you know, children, if you threaten them with,

39:42.620 --> 39:45.180
you know, go to your room and stuff, you can regulate them that way.

39:45.180 --> 39:48.060
But some kids ignore that completely and adults are.

39:48.060 --> 39:49.740
And it's often counterproductive.

39:49.780 --> 39:50.140
Yeah.

39:50.140 --> 39:57.420
So, so right, you know, you know, culture and societies and organizations, we regulate

39:57.420 --> 40:02.300
each other, you know, sometimes in competition and cooperation.

40:02.300 --> 40:02.540
Yeah.

40:02.940 --> 40:06.980
Do you, do you think that we've talked about this to some degree for decades?

40:06.980 --> 40:12.220
I mean, when you look at how fast things are moving now, and as you push that along,

40:13.020 --> 40:18.860
what, what, when you look out 10 years and you see the relationship between the AI

40:18.860 --> 40:22.340
systems that are being built and human beings, what do you envision?

40:23.220 --> 40:24.900
Or can you envision it?

40:26.780 --> 40:29.500
Well, can I, yeah, like I said, I'm a computer guy.

40:30.260 --> 40:33.620
And I'm watching this with, let's say, some fascination as well.

40:34.500 --> 40:39.580
I mean, the last, so Ray Kurzweil said, you know, pro, progress accelerates.

40:39.700 --> 40:40.060
Yeah.

40:40.180 --> 40:40.460
Right.

40:40.460 --> 40:43.820
So we, we have this idea that 20 years of progress is 20 years.

40:43.820 --> 40:48.460
But, you know, the last 20 years of progress was 20 years and the next 20 years will

40:48.460 --> 40:50.220
probably be, you know, five to 10.

40:50.260 --> 40:51.060
Right, right, right.

40:51.060 --> 40:51.980
And, and.

40:52.380 --> 40:53.900
And you can really feel that happening.

40:53.900 --> 41:00.140
To some level, that causes social stress, independent of whether it's AI or, or Amazon

41:00.180 --> 41:04.460
deliveries, you know, what, you know, there's so many things that are going into the, the

41:04.460 --> 41:05.420
stress of it all.

41:06.020 --> 41:09.460
Well, there's, but there's progress, which is an extension of human capacity.

41:09.820 --> 41:13.940
And then there's this progress, which I'm hearing about the way that you're describing

41:13.940 --> 41:21.060
it, which seems to be an inevitable progress towards creating something which is more

41:21.060 --> 41:22.540
powerful than you.

41:23.460 --> 41:23.780
Right.

41:23.780 --> 41:24.820
And so what is that?

41:24.820 --> 41:26.300
I don't even understand that drive.

41:26.420 --> 41:29.500
Like, what is that drive to, to create something?

41:29.540 --> 41:30.940
Which can supplant you.

41:31.180 --> 41:33.700
So look at the average person in the world, right?

41:33.900 --> 41:38.620
So the average person already exists in this world, because the average person is halfway

41:38.620 --> 41:40.100
up the human hierarchy.

41:40.660 --> 41:44.420
There's already many people more powerful than any of us.

41:44.660 --> 41:45.860
They're, they could be smarter.

41:45.860 --> 41:46.660
They could be richer.

41:46.660 --> 41:47.900
They could be better connected.

41:48.420 --> 41:52.340
We already live in a world like very few people are at the top of anything.

41:53.540 --> 41:53.820
Right.

41:53.860 --> 41:55.300
So that's already a thing.

41:55.740 --> 41:59.460
So basically the drive to make someone a superstar that's there, the drive

41:59.500 --> 42:04.900
to elevate someone above you, that would be the same drive that is bringing us to

42:04.900 --> 42:09.340
creating these ultra powerful machines because we have that.

42:09.340 --> 42:10.940
Like we have a drive to elevate.

42:10.940 --> 42:15.220
Like, you know, when we see a rock star that we like, people want to submit

42:15.220 --> 42:15.940
themselves to that.

42:15.940 --> 42:17.020
They want to dress like them.

42:17.020 --> 42:21.140
They want to raise them up above them as an example, something to follow, right?

42:21.140 --> 42:23.500
Something to, to, to subject themselves to.

42:23.700 --> 42:24.620
You see that with leaders.

42:24.620 --> 42:28.500
You see that in the political world and in teams.

42:28.540 --> 42:30.540
You see that in sports teams, the same thing.

42:30.540 --> 42:34.300
And so you think we've always tried to build things that are beyond us.

42:34.300 --> 42:38.620
You know, I mean, I mean, it's about, are we building, are we building a God?

42:38.620 --> 42:43.020
Is that the, is that what people, is that the drive that is pushing someone towards?

42:43.220 --> 42:47.500
Cause when I hear what you're describing, Jim, I hear something that is extremely

42:47.580 --> 42:48.700
dangerous, right?

42:48.700 --> 42:52.100
Sounds extremely dangerous to the very existence of humans.

42:52.300 --> 42:56.820
Yet I see humans acting and moving in that direction almost without being able

42:56.860 --> 42:58.540
to stop it, as if there's no one now.

42:58.540 --> 43:00.220
I think it is unstoppable.

43:00.620 --> 43:04.060
Well, that's one of the things we've also talked about is because I've asked

43:04.060 --> 43:09.460
Jim straight out, you know, because of the hypothetical danger associated with

43:09.460 --> 43:11.380
this, why not stop doing it?

43:11.380 --> 43:15.300
And well, part of his answer is the ambivalence about the outcome, but also

43:15.300 --> 43:19.420
that it isn't obvious at all that in some sense it's stoppable.

43:20.740 --> 43:24.180
I mean, it's the, it's the cumulative action of many, many people that are

43:24.180 --> 43:25.260
driving this along.

43:25.780 --> 43:30.500
And even if you took out one player, even a key player, the probability that you

43:30.500 --> 43:34.460
do anything but slow it infinitesimally is, is quite.

43:34.700 --> 43:37.780
That because there's also a massive payoff for those that will succeed.

43:38.220 --> 43:39.620
It's also set up that way.

43:39.620 --> 43:44.740
People know that at least, at least until the AI take over or whatever, that

43:44.860 --> 43:50.060
whoever is on the line towards increasing the power of the AI will, will

43:50.060 --> 43:52.260
rake in major rewards.

43:53.060 --> 43:53.380
Right?

43:53.380 --> 43:56.460
Well, that's what we do with all cognitive acceleration, right?

43:56.820 --> 44:02.500
Yeah, I could recommend Ian Banks as an author, English author, I think he

44:02.500 --> 44:05.660
wrote a series of books on the, he called the culture novels.

44:06.220 --> 44:10.700
And it was a world where there was humans and then there was AIs, the smartest

44:10.700 --> 44:12.860
humans and AI's that were dumber than humans.

44:12.860 --> 44:16.860
But there were some AIs that were much, much smarter and they, they lived in

44:16.860 --> 44:20.260
harmony because they mostly all pursued what they wanted to pursue.

44:20.860 --> 44:26.660
Humans pursued human goals and super smart AIs, pursued super smart AI goals.

44:26.660 --> 44:30.900
And, and, you know, they, they communicated and worked with each other.

44:30.900 --> 44:34.340
But, but they, they mostly, you know, they're different.

44:34.340 --> 44:37.860
When they were different enough that that was problematic, their goals were

44:37.860 --> 44:39.900
different enough that they didn't overlap.

44:40.100 --> 44:43.620
Because one of the, one of the things that that would be my guess is like these

44:43.620 --> 44:46.420
ideas where these super AIs get smart.

44:46.420 --> 44:48.620
And the first thing they do is stomp out the humans.

44:48.620 --> 44:49.620
It's like, you don't do that.

44:49.660 --> 44:52.660
Like, like, you don't wake up in the morning and think, I have to stomp

44:52.660 --> 44:53.780
out all the cats.

44:53.860 --> 44:55.180
No, it's not about.

44:55.180 --> 44:59.420
The cats do cat things and the ants do ant things and the birds do bird things.

44:59.420 --> 45:03.500
And, and super smart mathematicians do smart mathematician things.

45:03.500 --> 45:06.700
And, you know, guys who like to build houses do build house things.

45:06.700 --> 45:11.660
And, you know, everybody, you know, the world, there's so much space in the

45:11.660 --> 45:19.500
intellectual zone that people, people tend to go pursue the, in a good society.

45:19.660 --> 45:22.460
Like you tend to pursue the stuff that you do.

45:22.460 --> 45:26.740
And then the people in your zone, you self-regulate.

45:27.340 --> 45:31.700
And you also, even in the social stratus, we self-regulate.

45:32.140 --> 45:36.380
I mean, the, the, the recent political events of the last 10 years, the, the

45:36.380 --> 45:42.060
weird thing to me has been why have, you know, people with power been overreaching

45:42.060 --> 45:45.580
to take too much from people with less.

45:45.580 --> 45:47.340
Like that's bad regulation.

45:47.940 --> 45:54.020
But one of the aspects of increasing power is that increasing power is always

45:54.020 --> 46:00.940
mediated, at least in one aspect by the military, by, by, let's say, physical

46:01.100 --> 46:06.020
power on others, you know, and we can see that technology is linked and has been

46:06.020 --> 46:08.460
linked always to military power.

46:08.780 --> 46:14.460
And so the idea that there could be some AIs that will be our friends or whatever

46:14.660 --> 46:18.900
is maybe possible, but the idea that there will be some AIs, which will be

46:18.900 --> 46:25.420
weaponized is, seems absolutely inevitable because increasing power is always,

46:25.860 --> 46:29.340
increasing technological power always moves towards, towards military.

46:29.380 --> 46:34.420
So we've lived with atomic bombs since the 40s, right?

46:34.420 --> 46:40.660
So the, I mean, the solution to this has been mostly, you know, some form of

46:40.660 --> 46:46.660
mutual assured destruction or attacking me, like the response to attacking me is

46:46.660 --> 46:48.100
so much worse than the.

46:48.260 --> 46:51.060
Yeah, but it's also because we, we have reciprocity.

46:51.060 --> 46:53.420
We recognize each other as the same.

46:53.700 --> 46:57.820
So if I look into the face of another human, there, there's a limit of how

46:57.820 --> 46:59.980
indifferent I think that person is for me.

47:00.460 --> 47:04.820
But if I'm hearing something described as the possibility of superintelligences

47:05.020 --> 47:09.380
that have their own goals, their own cares, their own structures, then how much

47:09.420 --> 47:13.740
mirror is there between these two groups of people, these two groups?

47:14.140 --> 47:19.540
Well, Jim's objection seems to be something like we're, we're making,

47:19.620 --> 47:23.740
we may be making, when we're doomsaing, let's say, and I'm not saying there's

47:23.740 --> 47:28.380
no place for that, we're making the presumption of something like a zero

47:28.380 --> 47:30.500
sum competitive landscape, right?

47:30.500 --> 47:36.100
Is that the, the idea and the idea behind movies like, like the Terminator is

47:36.100 --> 47:40.620
that there is only so much resources and the machines and the human beings

47:40.620 --> 47:41.780
would have to fight over it.

47:41.780 --> 47:45.660
And you can see that that, that could easily be a preposterous assumption.

47:45.660 --> 47:49.900
Now, I think that one of the fundamental points you're making, though, is also

47:52.020 --> 47:57.460
there will definitely be people that will weaponize AI and those weaponized

47:57.460 --> 48:01.660
AI systems will have as their goal, something like the destruction of human

48:01.660 --> 48:03.820
beings, at least under some circumstances.

48:03.860 --> 48:08.420
And then there's the possibility that that will get out of control because the

48:08.420 --> 48:12.180
most effective systems at destroying human beings might be the ones that win,

48:12.180 --> 48:16.060
let's say, and that could happen independently of whether or not it is a

48:16.060 --> 48:17.740
true zero sum competition.

48:17.860 --> 48:18.140
Yeah.

48:18.140 --> 48:24.380
And also the, the effectiveness of military stuff doesn't need very smart

48:24.380 --> 48:26.620
AI to be a lot better than it is today.

48:27.420 --> 48:31.420
You know, I used to, you know, like the Star Wars movies where like, you know,

48:31.500 --> 48:35.780
tens of thousands of years in the future, super highly trained, you know,

48:35.780 --> 48:38.580
fighters can't hit somebody running across the field.

48:38.940 --> 48:40.060
Like that's silly, right?

48:40.060 --> 48:43.580
You can, you can already make a gun that can hit everybody in the room

48:44.180 --> 48:45.300
without aiming at it.

48:45.300 --> 48:51.380
It's, you know, there's like the, the military threshold is much lower than

48:51.380 --> 48:56.260
any intelligence threshold, like for danger.

48:56.260 --> 49:00.500
And, you know, like to the extent that we self-regulated through the nuclear

49:00.500 --> 49:01.900
crisis is interesting.

49:02.780 --> 49:06.100
I don't know if it's because we thought that the Russians were like us.

49:06.100 --> 49:09.940
I kind of suspect the problem was that we thought they weren't like us.

49:10.500 --> 49:17.380
And, but we still managed to make some calculation to say that any kind of

49:17.380 --> 49:19.460
attack would be mutually devastating.

49:19.980 --> 49:23.100
Well, when you, when you look at, you know, the destructive power of the

49:23.100 --> 49:27.740
military we already have so far exceeds the planet, I'm, I'm not sure, like

49:28.020 --> 49:30.220
adding intelligence to it is the tipping point.

49:30.220 --> 49:37.260
Like that's, I think the more likely thing is things that are truly smart

49:37.260 --> 49:39.740
in different ways will be interested in different things.

49:40.460 --> 49:46.020
And then the possibility for, let's say, mutual flourishing is, is, is really

49:46.020 --> 49:46.660
interesting.

49:47.100 --> 49:51.180
And I know artists using AI already to do really amazing things.

49:51.180 --> 49:53.340
And, and that's already happening.

49:54.100 --> 49:58.540
Well, when you, when you're working on the frontiers of AI development and you

49:58.540 --> 50:02.060
see the development of increasingly intelligent machines, I mean, I know

50:02.060 --> 50:05.300
that part of what drives you is, I don't want to put words in your mouth,

50:05.300 --> 50:09.860
but what drives intelligent engineers in general, which is to take something

50:09.860 --> 50:12.580
that works and make it better and maybe to make it radically better and

50:12.580 --> 50:13.460
radically cheaper.

50:13.660 --> 50:16.900
So, so there's this drive toward technological improvement.

50:16.900 --> 50:19.940
And I know that you like to solve complex problems and you do that

50:19.940 --> 50:20.940
extraordinarily well.

50:21.460 --> 50:28.420
What, but do you, do you, is there also a vision of a more abundant form of

50:28.420 --> 50:32.100
human flourishing emerging from the, from the development?

50:32.220 --> 50:33.620
So what, so what do you see happening?

50:33.620 --> 50:36.420
Well, you said it years ago, it's like, we're going to run out of energy.

50:36.420 --> 50:36.900
What's next?

50:36.900 --> 50:38.020
We're going to run out of matter.

50:38.060 --> 50:38.460
Right.

50:38.740 --> 50:44.580
Like our ability to do what we want in ways that are interesting and, you know,

50:44.580 --> 50:48.220
for some people, beautiful is limited by a whole bunch of things.

50:48.220 --> 50:52.420
Cause we're, you know, partly it's technological and partly with, you

50:52.420 --> 50:57.540
know, we're stupidly divisive, but, um, but there is, there's

50:57.540 --> 51:02.220
also possible that there's also a reality, which is one of the things

51:02.220 --> 51:07.220
that technology has been is of course, an increase in power towards desire,

51:07.220 --> 51:08.260
towards human desire.

51:08.260 --> 51:15.060
And that is represented in mythological stories where let's say technology

51:15.060 --> 51:18.180
is used to accomplish impossible desire, right?

51:18.180 --> 51:22.580
We have, you know, the story of the story of building the, the McCann,

51:22.580 --> 51:27.220
the bull around the king of Meno, the, the wife of the king of Meno's, you

51:27.220 --> 51:32.100
know, in order to be inseminated by, uh, by a bull, we have the story, the, the,

51:32.660 --> 51:37.780
we have the story of the, um, sorry, Frankenstein, et cetera, the story of

51:37.780 --> 51:42.420
the golem where we put our desire into this increased power.

51:42.420 --> 51:44.900
And then what happens is that we don't know our desires.

51:44.900 --> 51:48.580
That's one of the things that I've also been worried about in terms of AI is that

51:50.020 --> 51:55.140
we act, we have secret desires that enter into what we do that people

51:55.220 --> 51:56.660
aren't totally aware of.

51:57.220 --> 52:02.660
And as we increase in power, these systems, those desires,

52:03.380 --> 52:06.100
let's say the, the pot, like the idea, for example, of the possibility

52:06.100 --> 52:11.380
of having an AI friend and the idea that an AI friend would be the best friend

52:11.380 --> 52:15.220
you've ever had because that, that friend would be the nicest to you,

52:15.220 --> 52:17.940
would care the most about you, would do all those things.

52:17.940 --> 52:20.900
That would be an exact example of what I'm talking about,

52:20.900 --> 52:23.380
which is it's really the story of the genie, right?

52:23.380 --> 52:27.540
It's the story of the, the genie in the lamp where the genie says,

52:27.540 --> 52:28.580
what do you wish?

52:28.580 --> 52:31.940
And the, and the person, and I have unlimited power to give it to you.

52:31.940 --> 52:35.060
And so I give him my wish, but that wish has all these,

52:35.700 --> 52:39.140
these underlying implications that I don't understand all these underlying

52:39.140 --> 52:39.540
possibilities.

52:39.540 --> 52:44.340
Yeah, but the, the cool thing, the moral of almost all those stories is

52:45.140 --> 52:50.100
having unlimited wishes will be, lead to your downfall.

52:50.100 --> 52:55.540
And so humans, like, if you give, you know, a young person an unlimited amount

52:55.540 --> 52:59.460
of stuff to drink for, for six months, they're going to be falling down drunk

52:59.460 --> 53:01.540
and they're going to get over it, right?

53:01.540 --> 53:04.100
Having a friend that's always your friend, no matter what,

53:04.100 --> 53:05.300
it's probably going to get boring.

53:05.300 --> 53:09.460
Well, the, the, the literature on marital stability indicates that.

53:09.460 --> 53:14.500
So there's a, there's a sweet spot with regards to marital stability

53:14.500 --> 53:17.940
in terms of the ratio of negative to positive communication.

53:18.740 --> 53:24.180
So if on average, you receive five positive communications

53:24.180 --> 53:27.620
and one negative communication from your spouse,

53:27.620 --> 53:30.100
that's on the low threshold for stability.

53:30.660 --> 53:34.020
If it's four positive to one negative, you're headed for divorce.

53:34.020 --> 53:37.940
But interestingly enough, on the other end, there's a threshold as well,

53:37.940 --> 53:42.020
which is that if it exceeds 11 positive to one negative,

53:42.020 --> 53:43.780
you're also moving towards divorce.

53:44.580 --> 53:49.700
So there's, so, so, so there might be self-regulating mechanisms that,

53:49.700 --> 53:51.780
that would in sense take care of that.

53:51.780 --> 53:57.620
You might find a yes man AI friend extraordinarily boring, very, very rapidly.

53:57.620 --> 54:01.940
But as opposed to an AI friend that was interested in what you were interested in,

54:01.940 --> 54:03.060
it was actually interesting.

54:03.780 --> 54:06.580
Like, you know, we go through friends in the course of our lives,

54:06.580 --> 54:09.140
like different friends are interesting at different times.

54:09.140 --> 54:13.060
And some friends we grow with, and that continues to be really interesting

54:13.060 --> 54:14.020
for years and years.

54:14.020 --> 54:17.220
And other friends, you know, some people get stuck in their thing

54:17.220 --> 54:19.700
and then you've moved on or they've moved on or something.

54:19.700 --> 54:28.020
So, yeah, I tend to think of a world where there was more abundance

54:28.020 --> 54:33.860
and more possibilities and more interesting things to do is an interesting.

54:33.860 --> 54:34.500
Okay, okay.

54:34.500 --> 54:37.940
And modern society has let the human population,

54:37.940 --> 54:40.100
and some people think this is a bad thing, but I don't know.

54:40.100 --> 54:42.180
I'm a fan of it.

54:42.180 --> 54:46.900
You know, modern population has gone from tens of 200 million to billions of people.

54:47.620 --> 54:49.060
That's generally being a good thing.

54:49.060 --> 54:50.500
We're not running out of space.

54:50.500 --> 54:54.100
I've been in, you know, so some of your audience has probably been in an airplane.

54:54.100 --> 54:57.140
If you look out the window, the country is actually mostly empty.

54:57.860 --> 54:59.540
The oceans are mostly empty.

54:59.540 --> 55:04.100
Like, we're, we're weirdly good at polluting large areas.

55:04.100 --> 55:07.700
But as soon as we decide not to, we don't have to, like technology.

55:07.700 --> 55:11.860
Most, most of our, you know, energy pollution problems are technical.

55:12.180 --> 55:13.540
Like, we can stop polluting.

55:13.540 --> 55:14.900
Like, electric cars are great.

55:15.700 --> 55:19.620
So, so, so there's so many things that we could do technically.

55:20.660 --> 55:22.500
I forget the guy's name.

55:22.500 --> 55:26.020
He said that the earth could easily support a population of a trillion people.

55:26.900 --> 55:30.020
And trillion people would be a lot more people doing, you know, random stuff.

55:30.660 --> 55:35.460
And he didn't imagine that the future population would be a trillion humans and a trillion AIs,

55:35.460 --> 55:37.300
but it probably will be.

55:38.660 --> 55:40.980
So it will probably exist on multiple planets,

55:40.980 --> 55:43.700
which will be good the next time an asteroid shows up.

55:43.700 --> 55:47.620
So what do you think about, so, so one of the things that seems to be happening,

55:47.620 --> 55:49.380
tell me if, if you think I'm wrong here.

55:49.380 --> 55:53.380
I don't think it's germane to, and I just want to make the point of, you know,

55:53.380 --> 55:56.820
where we are compared to living in the Middle Ages, our lives are longer,

55:56.820 --> 56:00.260
our, our families are healthier, our children are more likely to survive.

56:00.900 --> 56:02.900
Like many, many good things happened.

56:03.700 --> 56:06.180
Like setting the clock back with and be good.

56:06.180 --> 56:10.580
And, you know, if we have some care and people who actually care about

56:10.580 --> 56:13.940
how culture interacts with technology for the next 50 years,

56:14.740 --> 56:19.380
you know, we'll get through this, hopefully more successful than we did the atomic bomb in the Cold War.

56:21.140 --> 56:24.020
But it's, it's a major change.

56:24.020 --> 56:30.180
I mean, this is like, like your worries are, you know, I mean, they're, they're relevant.

56:31.060 --> 56:37.700
But, you know, but also you're, Jonathan, your stories about how humans have faced abundance

56:37.700 --> 56:40.660
and faced evil kings and evil overlords.

56:40.660 --> 56:45.380
Like we have thousands of years of history of facing the challenge of the future

56:45.380 --> 56:48.740
and the challenge of things that cause radical change.

56:48.740 --> 56:49.220
Yeah.

56:49.220 --> 56:53.540
And you know, that's, that's very valuable information.

56:53.540 --> 56:57.540
But for the most part, nobody's succeeded by stopping change.

56:57.540 --> 57:05.460
They've succeeded by bringing to bear on the change our capability to self-regulate the balance.

57:06.180 --> 57:09.140
Like a good life isn't having as much gold as possible.

57:09.140 --> 57:10.180
It's a boring life.

57:10.180 --> 57:14.020
A good life is, you know, having some quality friends and doing what you want

57:14.020 --> 57:17.300
and having some, some insight in life.

57:17.300 --> 57:18.020
Yeah.

57:18.020 --> 57:19.380
And some optimal challenge.

57:20.340 --> 57:26.180
And, you know, and in a world where a larger percentage of people can have,

57:27.460 --> 57:31.860
well, live in relative abundance and have tools and opportunities, I think is a good thing.

57:31.860 --> 57:32.180
Yeah.

57:32.180 --> 57:34.420
And I don't, I don't want to pull back abundance.

57:34.420 --> 57:42.740
But what I have noticed is that, that our abundance brings a kind of nihilism to people.

57:42.740 --> 57:44.500
And I don't, like I said, I don't want to go back.

57:44.500 --> 57:47.380
I'm happy to live here and to have these, these tech things.

57:47.380 --> 57:54.740
But I, but I think it's something that I've also noticed that increase of, of the capacity to

57:56.340 --> 58:03.300
get your desires when that increases to a certain extent also leads to a kind of nihilism

58:03.300 --> 58:04.660
where exactly that.

58:04.660 --> 58:08.900
Well, I wonder, Jonathan, I wonder if that's partly, partly a consequence

58:10.020 --> 58:15.780
of the erroneous maximization of short-term desire.

58:16.340 --> 58:21.860
I mean, one of the things that you might think about that could be dangerous on the AI front is that

58:22.660 --> 58:29.940
we optimize the manner in which we, we interact with our electronic gadgets to capture short-term

58:29.940 --> 58:31.780
attention, right?

58:31.780 --> 58:36.420
Because there's a difference between getting what you want right now, right now, and getting

58:36.420 --> 58:41.140
what you need in some more mature sense across a reasonable span of time.

58:41.140 --> 58:45.060
And one of the things that does seem to be happening online, and I think it is driven by

58:45.060 --> 58:52.580
the development of AI systems, is that we're, we're assaulted by systems that parasitize our

58:52.580 --> 58:56.660
short-term attention and at the expense of longer-term attention.

58:56.660 --> 59:03.540
And if the AI systems emerge to optimize attentional grip, it isn't obvious to me that

59:03.540 --> 59:08.100
they're going to optimize for the attention that works over the medium to long-run, right?

59:08.100 --> 59:12.340
They're going to, they're going to be, they could conceivably maximize something like

59:12.340 --> 59:15.140
whim-centered existence.

59:15.140 --> 59:17.780
Yeah, because all of virality is based on that.

59:17.780 --> 59:22.900
All the social media networks are all based on this, on this reduction, this reduction of

59:22.900 --> 59:27.780
attention, this reduction of desire to, to reaching your, your rest, let's say, in that

59:27.780 --> 59:31.140
desire, right? The like, the click, all these things, they're...

59:31.140 --> 59:32.180
Yeah, now.

59:32.180 --> 59:32.900
Yeah, exactly.

59:32.900 --> 59:37.300
So, but, but that's something that, you know, so for reasons that are somewhat puzzling,

59:37.300 --> 59:43.060
but maybe not, you know, the business models around a lot of those interfaces are around,

59:43.940 --> 59:50.340
you know, the part, the users, the product, and, you know, the advertisers are trying to get

59:50.340 --> 59:50.980
your attention.

59:50.980 --> 59:51.780
Yeah, yeah.

59:51.780 --> 59:56.260
But that's something culture could regulate. We could decide that, no, we don't, we don't

59:56.260 --> 01:00:00.740
want tech platforms to be driven by advertising money. Like, that would be a smart decision,

01:00:00.740 --> 01:00:04.020
probably. And that could be a big change.

01:00:04.660 --> 01:00:06.340
And what would you see as an alternative?

01:00:06.340 --> 01:00:11.300
See, well, the problem with that might be that markets drive that in some sense, right?

01:00:11.300 --> 01:00:11.860
Yeah.

01:00:11.860 --> 01:00:13.620
And I know they're driving that in a short-term way.

01:00:13.620 --> 01:00:17.940
We can take steps, like, you know, at various times, you know, alcohol has been illegal.

01:00:17.940 --> 01:00:22.900
Like, you can, society can decide to regulate all kinds of things.

01:00:23.940 --> 01:00:27.460
And, you know, sometimes, some things need to be regulated and some things don't.

01:00:27.460 --> 01:00:31.780
Like, when you buy a hammer, you don't fight with your hammer for its attention, right?

01:00:31.780 --> 01:00:34.660
A hammer is a tool. You buy one when you need one.

01:00:35.540 --> 01:00:37.700
Nobody's marketing hammers to you.

01:00:37.700 --> 01:00:43.540
Like, like that, that has a relationship that's transactional to your purpose, right?

01:00:43.540 --> 01:00:44.020
Yeah, well.

01:00:44.020 --> 01:00:47.540
Our technology has become a thing where, I mean.

01:00:47.540 --> 01:00:54.260
But there's a relationship between human, let's say, high human goals, something like

01:00:54.260 --> 01:01:00.980
attention and status. And what we talked about, which is the idea of elevating something higher

01:01:00.980 --> 01:01:07.220
in order to see it as a model. See, these are where intelligence exists in the human person.

01:01:07.220 --> 01:01:14.260
So when we notice that in the systems, in the platforms, these are the aspects of intelligence

01:01:14.260 --> 01:01:20.820
which are being weaponized in some ways, not against us, but are just kind of being weaponized

01:01:20.820 --> 01:01:25.300
because they're the most beneficial at the short term to be able to generate our constant attention.

01:01:25.300 --> 01:01:30.020
And so what I mean is that that is what the AIs are made of, right?

01:01:30.020 --> 01:01:35.460
They're made of attention, prioritization, you know, good, bad.

01:01:35.460 --> 01:01:40.580
What is it that is worth putting energy into in order to predict towards a telos?

01:01:40.580 --> 01:01:46.900
And so I'm seeing that the idea that we could disconnect them suddenly seems very difficult to me.

01:01:48.260 --> 01:01:54.340
Yeah, so I'll give you two. First, I want to give an old example. So after World War II,

01:01:55.140 --> 01:02:00.740
America went through this amazing building boom of building suburbs. And the American dream was,

01:02:00.740 --> 01:02:05.620
you could have your own house, your own yard in the suburb with a good school, right?

01:02:05.620 --> 01:02:11.060
So in the 50s, 60s, early 70s, they were building like crazy. By the time I grew up,

01:02:11.060 --> 01:02:18.420
I lived in a suburb in dystopia, right? And we found that that as a goal wasn't a good thing

01:02:18.420 --> 01:02:27.380
because people ended up in houses separated from social structures and then new towns are built

01:02:27.380 --> 01:02:34.580
around like a hub with places to go and eat. So there was a good that was viewed in terms of

01:02:34.580 --> 01:02:40.820
opportunity and abundance, but it actually was a fail culturally. And then some places it modified

01:02:40.820 --> 01:02:47.060
and continues and some places are still dystopian, you know, suburban areas and some places people

01:02:47.060 --> 01:02:53.780
simply learn to live with it, right? So that has to do with attention, by the way. It has to do with

01:02:54.660 --> 01:03:01.540
a subsidiary hierarchy, like a hierarchy of attention, which is set up in a way in which all

01:03:01.540 --> 01:03:08.180
the levels can have room to exist, let's say. And so, you know, the new systems, the new way,

01:03:08.180 --> 01:03:13.220
let's say the new urbanist movement, similar to what you're talking about, that's what they've

01:03:13.220 --> 01:03:18.260
understood. It's like we need places of intimacy in terms of the house. We need places of communion

01:03:18.260 --> 01:03:25.220
in terms of parks and alleyways and buildings where we meet and a church, all these places that

01:03:25.220 --> 01:03:32.420
kind of manifest our community together. Yeah, so those existed coherently for long periods of time

01:03:32.420 --> 01:03:39.540
and then the abundance post-World War II and some ideas about like what life could be like

01:03:39.540 --> 01:03:46.260
caused this big change and that change satisfied some needs, people got houses, but broke community

01:03:46.260 --> 01:03:53.220
needs and then new sets of ideas about what's the synthesis, what's the possibility of having your own

01:03:53.220 --> 01:03:59.300
home, but also having community, not having to drive 15 minutes for every single thing and some

01:03:59.300 --> 01:04:04.100
people live in those worlds and some people don't. Do you think we'll be smart? So one of the problems

01:04:04.100 --> 01:04:07.060
is if we're there, we'll see. Well, why were we smart enough to solve some of those problems?

01:04:07.060 --> 01:04:11.380
Because we had 20 years, but now, because one of the things that's happening now is we're,

01:04:11.380 --> 01:04:17.940
as you pointed out earlier, is we're going to be producing equally revolutionary transformations,

01:04:17.940 --> 01:04:24.260
but at a much smaller scale of time. And so, Mike, one of the things I wonder about, I think it's

01:04:24.260 --> 01:04:34.500
driving some of the concerns in the conversation, is are we going to be intelligent enough to direct

01:04:34.500 --> 01:04:39.700
with regulation the transformations of technology as they start to accelerate? I mean, we've already,

01:04:39.700 --> 01:04:46.260
you look what's happened online, I mean, we've inadvertently, for example, radically magnified

01:04:46.260 --> 01:04:52.420
the voices of narcissists, psychopaths, and Machiavellians. And we've done that so intensely,

01:04:52.420 --> 01:04:59.140
partly, and I would say partly, as a consequence of AI mediation, that I think it's destabilizing

01:04:59.140 --> 01:05:04.020
the entire world. It's destabilizing part of it, like Scott Adams pointed out. You just block

01:05:04.020 --> 01:05:07.860
everybody that acts like that. I don't pay attention to people that talk like that.

01:05:08.500 --> 01:05:12.420
Yeah, but they seem to be raising the temperature. Well, there are still places that are sensitive

01:05:12.420 --> 01:05:19.460
to it. Like 10,000 people here can make a storm in some corporate person, fire somebody.

01:05:19.460 --> 01:05:24.660
But I think that's like, we're five years from that being over. Corporation will go 10,000 people

01:05:24.660 --> 01:05:28.820
out of 10 billion. Not a big deal. Okay, so you think at the moment that's a

01:05:29.460 --> 01:05:37.380
learning moment that will re-regulate. What's natural to our children is so different than

01:05:37.380 --> 01:05:41.620
was natural to us, but what was natural to us was very different from our parents.

01:05:41.620 --> 01:05:46.020
So some changes get accepted generationally really fast.

01:05:46.020 --> 01:05:51.060
So what's made you so optimistic? What do you mean optimistic?

01:05:51.700 --> 01:05:56.340
Well, most of the things that you have said today, and maybe it's also because we're pushing you,

01:05:56.340 --> 01:06:02.980
I mean, you really do. My nephew, Kyle, was a really smart, clever guy. He called me a

01:06:03.700 --> 01:06:09.860
what did he call it, a cynical optimist. Like I believe in people.

01:06:10.980 --> 01:06:15.460
Like I like people, but also people are complicated. They all got all kinds of nefarious goals.

01:06:15.460 --> 01:06:20.660
Like I worry a lot more about people burning down the world than I do about artificial intelligence

01:06:21.380 --> 01:06:26.500
just because, you know, people, well, you know, people, they're difficult.

01:06:27.220 --> 01:06:33.140
Right. And, but the interesting thing is in aggregate, we mostly self-regulate.

01:06:33.140 --> 01:06:38.100
And when things change, you have these dislocations. And then it's up to people who talk and think,

01:06:38.100 --> 01:06:43.940
and while we're having this conversation, I suppose, to talk about how do we re-regulate this stuff.

01:06:43.940 --> 01:06:49.540
Yeah. Well, because one of the things that the increase in power has done in terms of AI,

01:06:49.540 --> 01:06:54.580
and you can see it with Google and you can see it online, is that there are certain people who

01:06:54.580 --> 01:07:00.420
hold the keys, let's say, and then who hold the keys to what you see and what you don't see.

01:07:00.420 --> 01:07:04.420
So you see that on Google, right? And you know it if you know what search is to make where you

01:07:04.420 --> 01:07:11.140
realize that this is not, this is actually being directed by someone who now has huge amount of

01:07:11.140 --> 01:07:18.900
power in order to direct my attention towards their ideological purpose. And so that's why,

01:07:18.980 --> 01:07:26.980
like, I think that to me, I personally think it would, I always tend to see AI as an extension

01:07:26.980 --> 01:07:32.500
of human power, even though there is this idea that it could somehow become totally independent.

01:07:32.500 --> 01:07:39.460
I still tend to see it as an increase of the human care and whoever will be able to hold

01:07:39.460 --> 01:07:45.380
the keys to that will have increase in power. And that can be like, and I think we're already

01:07:45.380 --> 01:07:50.740
seeing it. Well, that's not really any different, though, is it, Jonathan, the situation that's

01:07:50.740 --> 01:07:55.300
always confronted us in the past? I mean, we've always had to deal with the evil uncle of the

01:07:55.300 --> 01:08:01.460
king, and we've always had to deal with the fact that an increase in ability could also produce

01:08:01.460 --> 01:08:07.700
a commensurate increase in tyrannical power, right? I mean, so that might be magnified now,

01:08:07.700 --> 01:08:13.700
and maybe the danger in some sense is more acute, but possibly the possibility is more

01:08:13.700 --> 01:08:20.340
present as well. Well, because you can train an AI to find hate speech, right? You can train an AI

01:08:20.340 --> 01:08:26.420
to find hate speech, and then to act on that hate speech immediately within, and now it's only,

01:08:26.420 --> 01:08:31.220
we're not only talking about social media, but what we've seen is that that is now

01:08:32.740 --> 01:08:37.700
encroaching into payment systems and into people losing their bank account, their access to

01:08:37.700 --> 01:08:43.300
different services. And so this idea of optimization. Yeah, there's an Australian bank that already has

01:08:43.300 --> 01:08:48.820
decided that it's a good thing to send all of their customers a carbon load report every month,

01:08:49.780 --> 01:08:56.740
right? And to offer them hints about how they could reduce their polluting purchases, let's say.

01:08:56.740 --> 01:09:02.260
And well, at the moment, that system is one of voluntary compliance, but you can certainly see

01:09:02.260 --> 01:09:08.740
in a situation like the one we're in now that the line between voluntary compliance and involuntary

01:09:08.740 --> 01:09:16.180
compulsion is very, very thin. Yeah, so I'd like to say, so during the early computer world,

01:09:16.180 --> 01:09:21.060
computers were very big and expensive. And then they made many computers and workstations,

01:09:21.060 --> 01:09:25.300
but they were still corporate only. And then the PC world came in. All of a sudden,

01:09:25.300 --> 01:09:30.980
PCs put everybody online, everybody could suddenly see all kinds of stuff, and people

01:09:30.980 --> 01:09:36.900
could get a Freedom of Information Act request, put it online somewhere, and 100,000 people could see

01:09:36.900 --> 01:09:45.460
it. It was an amazing democratization moment. And then there was a similar, but smaller revolution

01:09:45.460 --> 01:09:53.220
with the world of smartphones and apps. But then we've had a new completely different set of companies,

01:09:53.220 --> 01:09:59.540
by the way, from what happened in the 60s, 70s, and 80s to today, it's very different companies that

01:10:00.100 --> 01:10:06.100
control it. And there are people who are worried that AI will be a winner take all thing. Now,

01:10:06.100 --> 01:10:10.180
I think so many people are using it, and they're working on it so many different places, and the

01:10:10.180 --> 01:10:16.020
cost is going to come down so fast, that pretty soon you'll have your own AI app that you'll use to

01:10:16.020 --> 01:10:23.940
mediate the internet to strip out the endless stream of ads. And you can say, well, is this story

01:10:23.940 --> 01:10:29.300
objective? Well, here's the 15 stories, and this is being manipulated this way, and this is being

01:10:29.300 --> 01:10:35.220
manipulated that way. And you can say, well, I want what's more like the real story. And the funny

01:10:35.380 --> 01:10:44.100
thing is, information that's broadly distributed, and has lots of inputs, is very hard to fake the

01:10:44.100 --> 01:10:50.260
whole thing. So right now, a story can pull through a major media outlet. And if they can control the

01:10:50.260 --> 01:10:57.300
narrative, everybody gets to fake story. But if the media is distributed across a billion people,

01:10:57.940 --> 01:11:04.260
who are all interacting in some useful way, somebody standing there, some, yeah, there's real

01:11:04.260 --> 01:11:07.780
signal there, and if somebody stands up and says something that's not true, everybody goes,

01:11:07.780 --> 01:11:16.740
everybody knows that's not true. So a good outcome with people thinking seriously would be the

01:11:16.740 --> 01:11:23.060
democratization of information and objective facts in the same way. The same thing that happened with

01:11:23.060 --> 01:11:32.500
PCs versus corporate central computers could happen again. The problem is that the increase in power,

01:11:32.580 --> 01:11:39.460
the increase in power always creates the tooth at the same time. And so we saw that increase in

01:11:39.460 --> 01:11:44.340
power creates first, or it depends in which direction it happens, it creates an increase in

01:11:44.340 --> 01:11:50.100
decentralization, it increases in access, it creates all that. But then it also at the same time

01:11:50.100 --> 01:11:57.380
creates the counter reaction, which is an increase in control and increase in centralization. And so

01:11:57.700 --> 01:12:06.260
now, the more the power is, the more the waves will, the bigger the waves will be. And so the

01:12:06.260 --> 01:12:14.180
image of the image that 1984 presented to us, of people going into newspapers and changing the

01:12:15.300 --> 01:12:19.780
headlines and taking the pictures out and doing that, that now obviously can happen with just a

01:12:19.780 --> 01:12:24.740
click. So you can click and you can change the past. You can change the past, you can change

01:12:25.300 --> 01:12:30.660
facts about the world because they're all held online. And we've seen it happen obviously in

01:12:30.660 --> 01:12:37.860
the media recently. So does decentralization win over centralization? How is that even possible,

01:12:37.860 --> 01:12:44.580
it seems? I mean, and it's also interesting, when Amazon became a platform, suddenly any mom and

01:12:44.580 --> 01:12:52.100
pop business could have Amazon, eBay, there's a bunch of platforms, which had an amazing impact

01:12:53.060 --> 01:12:58.500
because any business could get to anybody. But then the platform itself started to control

01:12:58.500 --> 01:13:04.980
the information flow. But at some point that will turn into people go, well, why am I letting

01:13:04.980 --> 01:13:11.380
somebody control my information flow? And Amazon objectively doesn't really have any capability.

01:13:13.540 --> 01:13:18.740
So like you point out, the waves are getting bigger, but they're real waves. It's the same with

01:13:18.740 --> 01:13:25.620
information. Information is all online. It's also on a billion hard drives. So somebody says,

01:13:25.620 --> 01:13:30.420
I'm going to raise the objective fact, a distributed information system would say,

01:13:31.140 --> 01:13:35.300
go ahead and raise it anywhere you want. There's another 1,000 copies of it.

01:13:36.900 --> 01:13:44.020
But again, this is where thinking people have to say, yeah, this is a serious problem.

01:13:44.580 --> 01:13:49.300
Like if humans don't have anything to fight for, they get lazy and a little bit dopey,

01:13:49.300 --> 01:13:57.940
in my view. We do have something to fight for. And that's worth talking about. What would a great

01:13:57.940 --> 01:14:04.260
world with distributed, inhuman intelligence and artificial intelligence working together in a

01:14:04.260 --> 01:14:13.540
collaborative way to create abundance and fairness and some better way at arriving at good decisions

01:14:13.540 --> 01:14:19.220
than what the truth is. That would be a good thing. But it's not, well, we'll leave it to the

01:14:19.220 --> 01:14:23.620
experts and then the experts will tell us what to do. That's a bad thing. So that's...

01:14:24.660 --> 01:14:28.500
Well, so is it the model that you just laid out, which I think is very interesting?

01:14:28.500 --> 01:14:32.660
I'm not so much optimistic about that. Well, it did happen on the computational front.

01:14:33.220 --> 01:14:40.660
It happened a couple of times both directions. The PC revolution was amazing. And Microsoft

01:14:40.660 --> 01:14:47.460
was a fantastic company. It enabled everybody to write a $10, $50 program to use. And then at

01:14:47.460 --> 01:14:53.540
some point, they're also, let's say, a difficult program company. And they made money off a lot

01:14:53.540 --> 01:14:58.020
of people and became extremely valuable. Now, for the most part, they haven't been that directional

01:14:58.020 --> 01:15:02.980
on telling you what to do and think and how to do it. But they are a many-making company.

01:15:04.580 --> 01:15:08.740
Apple created the App Store, which is great. But then they also take 30% of the App Store

01:15:08.740 --> 01:15:12.980
profits and there's a whole section of the internet that's fighting with Apple about their

01:15:12.980 --> 01:15:19.940
control of that platform. And in Europe, they've decided to regulate some of that,

01:15:21.140 --> 01:15:25.940
that should be a conversation, that should be a social cultural conversation about how should

01:15:25.940 --> 01:15:36.340
that work. So do you see the more likely, certainly the more desirable future is something like

01:15:36.420 --> 01:15:43.780
a set of distributed AIs, many of which are under personal, in personal relationship in some sense,

01:15:43.780 --> 01:15:48.100
the same way that we're in personal relationship with our phones and our computers. And that that

01:15:48.100 --> 01:15:53.380
would give people the chance to fight back, so to speak against this. And there's lots of people

01:15:53.380 --> 01:15:58.820
really interested in distributed platforms. And one of the interesting things about the AI world is,

01:15:58.820 --> 01:16:04.100
you know, there's a company called OpenAI and they open source a lot of it. The AI research is

01:16:04.100 --> 01:16:09.860
amazingly open. It's all done in public. People publish the new models all the time. You can try

01:16:09.860 --> 01:16:16.020
them out. People, there's a lot of startups doing AI in all different kinds of places.

01:16:17.140 --> 01:16:24.660
You know, it's a very curious phenomena. And it's kind of like a big, huge wave. It's not like a,

01:16:25.380 --> 01:16:30.900
you can't stop a wave with your hand. Yeah. Well, when you think about the waves, there are two,

01:16:30.900 --> 01:16:36.020
actually in the book of Revelation, which describes the end or describes the finality of all things

01:16:36.020 --> 01:16:40.500
or the totality of all things is maybe a way for people who are more secular to kind of understand

01:16:40.500 --> 01:16:46.740
it. And in that book, there are two images, interesting images about technology. One is

01:16:46.740 --> 01:16:52.340
that there's a dragon that falls from the heavens and that dragon makes a beast. And then that beast

01:16:52.340 --> 01:16:59.460
makes an image of the beast. And then the image speaks. And when the image speaks, then people

01:16:59.460 --> 01:17:07.060
are so mesmerized by the speaking image that they worship the beast ultimately. So that is one

01:17:07.060 --> 01:17:12.100
image of, let's say, making and technology and scripture in Revelation. But there's another

01:17:12.100 --> 01:17:17.620
image, which is the image of the heavenly Jerusalem. And that image is more an image of balance. It's

01:17:17.620 --> 01:17:23.380
an image of the city which comes down from heaven with a garden in the center and then becomes this

01:17:23.380 --> 01:17:30.100
glorious city. And it says, the glory of all the kings is gathered into the city. So the glory of

01:17:30.100 --> 01:17:37.300
all the nations is gathered into this city. So now you see a technology which is at the service of

01:17:37.300 --> 01:17:42.500
human flourishing and takes the best of humans and brings it into itself in order to kind of

01:17:42.500 --> 01:17:46.820
manifest. And it also has hierarchy, which means it has the natural at the center and then has the

01:17:46.820 --> 01:17:52.020
artificial as serving the natural, you could say. So those two images seem to reflect these

01:17:52.900 --> 01:17:59.620
two waves that we see. And this kind of idea of an artificial intelligence which will be ruling

01:17:59.620 --> 01:18:05.300
over us or speaking over us. But there's a secret person controlling it, even in Revelation. It's

01:18:05.300 --> 01:18:11.140
like, there's a beast controlling it and making it speak. So now we're mesmerized by it. And then

01:18:11.140 --> 01:18:15.460
this other image. So I don't know, Jordan, if you ever thought about those two images in Revelation

01:18:15.460 --> 01:18:22.660
as being related to technology, let's say. Well, I don't think I've thought about those two images

01:18:22.660 --> 01:18:28.020
in the specific manner that you described. But I would say that the work that I've been doing,

01:18:28.020 --> 01:18:33.540
and I think the work you've been doing too in the public front, reflects the dichotomy between

01:18:33.540 --> 01:18:38.180
those images. And it's relevant to the points that Jim has been making. I mean, we are definitely

01:18:38.180 --> 01:18:42.900
increasing our technological power. And you can imagine that that'll increase our capacity for

01:18:42.900 --> 01:18:48.500
tyranny and also our capacity for abundance. And then the question becomes, what do we need to do

01:18:48.500 --> 01:18:53.940
in order to increase the probability that we tilt the future towards Jerusalem and away from the

01:18:53.940 --> 01:19:00.820
beast? And the reason that I've been concentrating on helping people bolster their individual

01:19:00.820 --> 01:19:06.340
morality to the degree that I've managed that is because I think that whether the outcome is the

01:19:06.340 --> 01:19:11.860
positive outcome, that in some sense Jim has been outlining or the negative outcomes that we've been

01:19:11.940 --> 01:19:16.900
querying him about, I think that's going to be dependent on the individual ethical choices of

01:19:16.900 --> 01:19:21.940
people at the individual level, but then cumulatively, right? So if we decide that we're

01:19:21.940 --> 01:19:26.980
going to worship the image of the beast, so to speak, because we're mesmerized by our own reflection,

01:19:26.980 --> 01:19:31.220
that's another way of thinking about it. And we want to be the victim of our own dark desires,

01:19:31.220 --> 01:19:38.020
then the IA revolution is going to go very, very badly. But if we decide that we're going to aim up

01:19:38.020 --> 01:19:42.500
in some positive way, and we make the right micro decisions, well, then maybe we can harness

01:19:42.500 --> 01:19:47.620
this technology to produce a time of abundance in the manner that Jim is hopeful about.

01:19:47.620 --> 01:19:55.220
Yeah. And let me make two funny points. So one is, I think there's going to be continuum,

01:19:55.220 --> 01:20:02.340
like the word artificial intelligence won't actually make any sense. So humans collectively,

01:20:02.420 --> 01:20:08.820
individuals know stuff, but collectively we know a lot more. And the thing that's really good is

01:20:08.820 --> 01:20:17.140
in a diverse society with lots of people pursuing individual, interesting ideas, worlds, we have

01:20:17.140 --> 01:20:26.740
a lot of things, and more people, more independence generates more diversity. And that's a good thing

01:20:26.740 --> 01:20:32.500
where it's a totalitarian society where everybody's told to wear the same shirt. It's inherently

01:20:32.500 --> 01:20:42.020
boring. The beast speaking through the monster is inherently dull. But in an intelligent world,

01:20:42.020 --> 01:20:49.940
where not only can we have more intelligent things, but in some places go far beyond what most humans

01:20:49.940 --> 01:20:59.380
are capable of in pursuit of interesting variety. And I believe the information, well,

01:20:59.380 --> 01:21:06.260
let's say intelligence is essentially unlimited, right? And the unlimited intelligence won't be

01:21:06.260 --> 01:21:11.780
this shiny thing that tells everybody what to do. That's sort of the opposite of interesting

01:21:11.780 --> 01:21:18.580
intelligence. Interesting intelligence will be more diverse, not less diverse. That's a good future.

01:21:20.180 --> 01:21:25.140
And your second description, that seems like a future we're working for and also we're fighting

01:21:25.140 --> 01:21:32.660
for. And that means concrete things today. And also, it's a good conceptualization.

01:21:34.020 --> 01:21:38.340
I see the messages as my kids are taught, don't have children and the world's going to end,

01:21:38.340 --> 01:21:43.620
we're going to run out of everything, you're a bad person, why do you even exist? These messages

01:21:43.620 --> 01:21:49.700
are terrible. The opposite is true. More people would be better. We live in a world

01:21:50.420 --> 01:21:57.140
of potential abundance. It's right in front of us. There's so much energy available. It's just

01:21:57.140 --> 01:22:03.460
amazing. It's possible to build technology without pollution consequences. That's called

01:22:03.460 --> 01:22:10.500
externalizing costs. We know how to do that. We can have very good, clean technology. We can do

01:22:10.500 --> 01:22:17.060
lots of interesting things. So if the goal is maximum diversity, then the line between human

01:22:17.060 --> 01:22:22.900
intelligence, artificial intelligence that we draw, you'll see all these really interesting

01:22:22.900 --> 01:22:27.140
partnerships and all kinds of things. And more people doing what they want, which is the world

01:22:27.140 --> 01:22:35.060
I want to live in. To me, it seems like the question is going to be related to attention,

01:22:35.060 --> 01:22:40.660
ultimately. That is, what are humans attending to at the highest? What is it that humans care for

01:22:40.660 --> 01:22:46.740
in the highest? In some ways, you could say, what are humans worshiping? And

01:22:46.740 --> 01:22:52.500
depending on what humans worship, then their actions will play out in the technology that

01:22:52.500 --> 01:22:57.460
they're creating, in the increase in power that they're creating. And if we're guided by the

01:22:57.460 --> 01:23:02.900
negative vision, the sort of thing that Jim laid out that is being talked to as children, you can

01:23:02.900 --> 01:23:07.780
imagine that we're in for a pretty damn dismal future. Human beings are a cancer on the face of

01:23:07.780 --> 01:23:12.820
the planet. There's too many of us. We have to accept top-down, compelled limits to growth.

01:23:12.900 --> 01:23:17.780
There's not enough for everybody. A bunch of us have to go because there's too many people on

01:23:17.780 --> 01:23:24.340
the planet. We have to raise up the price of energy so that we don't burn the planet up with

01:23:24.340 --> 01:23:31.460
carbon dioxide pollution, et cetera. It's a pretty damn dismal view of the potential that's in front

01:23:31.460 --> 01:23:39.140
of us. The world should be exciting and the future should be exciting. Well, we've been sitting here

01:23:39.140 --> 01:23:45.380
for about 90 minutes, bandying back and forth both visions of abundance and visions of apocalypse.

01:23:47.460 --> 01:23:52.180
I've been heartened, I would say, over the decades talking to Jim about what he's doing

01:23:52.180 --> 01:23:55.700
on the technological front. And I think part of the reason I've been heartened is because

01:23:56.340 --> 01:24:03.140
I do think that his vision is guided primarily by desire to help bring about something

01:24:03.140 --> 01:24:07.540
approximating life more abundant. And I would rather see people on the AI front who are guided

01:24:07.540 --> 01:24:12.740
by that vision working on this technology. But I also think it's useful to do what you

01:24:12.740 --> 01:24:18.500
and I have been doing in this conversation, Jonathan, and acting in some sense as friendly

01:24:18.500 --> 01:24:23.380
critics and hopefully learning something in the interim. Do you have anything you want to say

01:24:23.380 --> 01:24:29.460
in conclusion? I just think that the question is linked very directly to what we've been talking

01:24:29.460 --> 01:24:35.220
about now for several years, which is the question of attention, the question of what is the highest

01:24:35.220 --> 01:24:40.660
attention. And I think the reason why I have more alarm, let's say, than Jim, is that I've

01:24:40.660 --> 01:24:47.540
noticed that in some ways human beings have come to now, let's say, worship their own desires,

01:24:47.540 --> 01:24:53.300
they've come to worship. And that even the strange thing of worshiping their own desires has actually

01:24:53.300 --> 01:24:58.820
led to an anti-human narrative. This is a weird idea. It's almost suicidal desire that humans

01:24:58.820 --> 01:25:03.060
have. And so I think that seeing all of that together in the increase of power,

01:25:04.260 --> 01:25:11.060
I do worry that the image of the beast is closer to what will manifest itself. And I feel like

01:25:11.060 --> 01:25:18.820
during COVID, that sense in me was accelerated tenfold in noticing to what extent technology

01:25:18.820 --> 01:25:25.220
was used, especially in Canada, how technology was used to instigate something which looked like

01:25:25.300 --> 01:25:30.180
authoritarian systems. And so I am worried about it. But I think like Jim, honestly,

01:25:30.180 --> 01:25:34.420
although I say that, I do believe that in the end, truth wins. I do believe that in the end,

01:25:35.620 --> 01:25:43.380
these things will level themselves out. But I think that because I see people rushing towards

01:25:43.380 --> 01:25:50.900
AI almost like lemmings are going off a cliff, I feel like it is important to sound the alarm once

01:25:50.900 --> 01:25:57.540
in a while and say, you know, we need to orient our desire before we go towards this extreme power.

01:25:57.540 --> 01:26:01.780
So I think that that's mostly the thing that worries me the most and that preoccupies me the

01:26:01.780 --> 01:26:06.580
most. But I think that ultimately in the end, I do share Jim's positive vision. And I do think that

01:26:07.380 --> 01:26:12.100
I do believe the story has a happy ending. It's just you might have to go through hell before

01:26:12.100 --> 01:26:18.820
we get there. I hope not. So Jim, how about you? What have you got to say in closing?

01:26:18.820 --> 01:26:23.940
A couple of years ago, a friend who's, you know, my age said, oh, kids coming out of college,

01:26:23.940 --> 01:26:27.620
they don't know anything anymore. They're lazy. And I thought, I work at Tesla. I was working

01:26:27.620 --> 01:26:33.220
at Tesla at the time. And we hired kids out of college and they couldn't wait to make things.

01:26:33.940 --> 01:26:40.340
They were like, it's a hands-on place. It's a great place. And I've told people, like, if you're

01:26:40.340 --> 01:26:44.820
not in a place where you're doing stuff, it's growing, it's making things, you need to go somewhere

01:26:44.820 --> 01:26:52.340
else. And also, I think you're right, the mindset of if people are feeling this is a productive,

01:26:52.340 --> 01:26:57.380
creative technology that's really cool, they're going to go build cool stuff. And if they think

01:26:57.380 --> 01:27:01.860
it's a shitty job and they're just tuning the algorithm so they can get more clicks,

01:27:01.860 --> 01:27:09.540
they're going to make something beastly, you know, beastly, perhaps. And the stories, you know,

01:27:09.620 --> 01:27:16.580
our cultural tradition is super useful, both cautionary and, you know, explanatory about

01:27:16.580 --> 01:27:22.660
something good. Like, and I think it's up to us to go do something about this. And I know people

01:27:22.660 --> 01:27:27.700
are working really hard to make, you know, the Internet a more open place to make sure information

01:27:27.700 --> 01:27:35.060
is distributed, to make sure AI isn't a winter take-all thing. Like, these are real things and

01:27:35.060 --> 01:27:40.420
people should be talking about them. And then they should be worrying. But the upside's really high.

01:27:41.060 --> 01:27:47.380
And we faced these kind of technological, like, this is a big change. Like, AI is bigger than

01:27:47.380 --> 01:27:53.940
the Internet. Like I've said, this publicly, like, the Internet was pretty big. And, you know,

01:27:53.940 --> 01:28:02.500
this is bigger. It's true. But the possibilities are amazing. And so with some sense, we could

01:28:02.820 --> 01:28:08.820
utilize them. Yeah, with some sense, we could achieve it. And the world is interesting.

01:28:08.820 --> 01:28:13.460
Like, I think it'll be a more interesting place. Well, that's an extraordinarily

01:28:13.460 --> 01:28:20.020
cynically optimistic place to end. I'd like to thank everybody who is watching and listening.

01:28:20.020 --> 01:28:24.580
And thank you, Jonathan, for participating in the conversation. It's much appreciated as always.

01:28:24.580 --> 01:28:29.860
I'm going to talk to Jim Keller for another half an hour on the Daily Wire Plus platform. I

01:28:30.500 --> 01:28:35.220
use that extra half an hour to usually walk people through their biography. I'm very interested in

01:28:35.220 --> 01:28:41.780
how people develop successful careers and lives and how their destiny unfolded in front of them.

01:28:41.780 --> 01:28:46.980
And so for all of those of you who are watching and listening, who might be interested in that,

01:28:46.980 --> 01:28:51.940
consider heading over to the Daily Wire Plus platform and partaking in that. And otherwise,

01:28:51.940 --> 01:28:57.540
Jonathan, we'll see you in Miami in a month and a half to finish up the Exodus seminar.

01:28:57.540 --> 01:29:04.500
We're going to release the first half of the Exodus seminar we recorded in Miami on November 25th,

01:29:04.500 --> 01:29:08.420
by the way. So that looks like it's in the can. Yeah, I can't wait to see it.

01:29:08.420 --> 01:29:13.940
The rest of you? Yeah. Yeah, absolutely. I'm really excited about it. And just for everyone

01:29:13.940 --> 01:29:19.220
watching and listening, I brought a group of scholars together. About two and a half months

01:29:19.220 --> 01:29:23.620
ago, we spent a week in Miami, some of the smartest people I could gather around me,

01:29:23.700 --> 01:29:28.580
to walk through the book of Exodus. We only got through halfway, because it turns out there's

01:29:28.580 --> 01:29:33.140
more information there than I had originally considered. But it went exceptionally well,

01:29:33.140 --> 01:29:40.340
and I learned a lot. And Exodus means ex-hodos. That means the way forward. And well, that's very

01:29:40.340 --> 01:29:45.540
much relevant to everyone today as we strive to find our way forward through all these complex

01:29:45.540 --> 01:29:50.180
issues, such as the ones we were talking about today. So I would also encourage people to check

01:29:50.180 --> 01:29:54.660
that out when it launches on November 25th. I learned more in that seminar than any seminar

01:29:54.660 --> 01:29:58.580
I ever took in my life, I would say. So it was good to see you there. We'll see you in a month

01:29:58.580 --> 01:30:02.580
and a half. Jim, we're going to talk a little bit more on the Daily Wear a Plus platform. And

01:30:02.580 --> 01:30:07.780
I'm looking forward to meeting the rest of the people in your AI-oriented community tomorrow

01:30:07.780 --> 01:30:13.140
and learning more about, well, what seems to be an optimistic version of a life more abundant.

01:30:13.140 --> 01:30:18.020
And to all of you watching and listening, thank you very much. Your attention isn't taken for

01:30:18.020 --> 01:30:23.060
granted, and it's much appreciated. Hello, everyone. I would encourage you to continue

01:30:23.060 --> 01:30:28.980
listening to my conversation with my guest on DailyWirePlus.com.

