start	end	text
0	6420	It's the Hebrews created history as we know it.
6420	10500	You don't get away with anything and so you might think you can bend the fabric of reality
10500	14500	and that you can treat people instrumentally and that you can bow to the tyrant and violate
14500	16120	your conscience without cost.
16120	17420	You will pay the piper.
17420	22100	It's going to call you out of that slavery into freedom even if that pulls you into the
22100	23100	desert.
24100	30700	And we're going to see that there's something else going on here that is far more cosmic
30700	33180	and deeper than what you can imagine.
33180	41460	The highest ethical spirit to which we're beholden is presented precisely as that spirit
41460	45940	that allies itself with the cause of freedom against tyranny.
45940	46940	Yes, exactly.
46940	48460	I want villains to get punished.
48460	52140	But do you want the villains to learn before they have to pay the ultimate price?
52140	54140	That's such a Christian question.
65140	67060	That has to do with attention by the way.
67060	72980	It has to do with a subsidiary hierarchy, like a hierarchy of attention which is set
72980	78980	up in a way in which all the levels can have room to exist, let's say.
78980	85540	And so these new systems, the new way, let's say the new urbanist movement, similar to
85540	87820	what you're talking about, that's what they've understood.
87820	90660	It's like we need places of intimacy in terms of the house.
90660	96900	We need places of communion in terms of parks and alleyways and buildings where we meet
96900	102740	and a church, all these places that kind of manifest our community together.
102740	108780	So those existed coherently for long periods of time and then the abundance post-World
108780	115660	War II and some ideas about what life could be like causes big change.
115660	121180	And that change satisfied some needs, people got houses, but broke community needs.
121180	126820	And then new sets of ideas about what's the synthesis, what's the possibility of having
126820	132580	your own home but also having community, not having to drive 15 minutes for every single
132580	135780	thing and some people live in those worlds and some people don't.
135780	137380	Do you think we'll be smart?
137380	141340	So one of the problems is why were we smart enough to solve some of those problems?
141340	145620	Because we had 20 years, but now, because one of the things that's happening now, as
145620	151900	you pointed out earlier, is we're going to be producing equally revolutionary transformations
151900	155820	but at a much smaller scale of time.
155820	160580	What's natural to our children is so different than what's natural to us, but what's natural
160580	163020	to us is very different from our parents.
163020	167460	So some changes get accepted generationally really fast.
167460	169940	So what's made you so optimistic?
183940	188780	Hello everyone watching on YouTube or listening on associated platforms.
188780	195900	So I'm very excited today to be bringing you two of the people I admire most intellectually,
195900	202780	I would say, and morally for that matter, Jonathan Pazio and Jim Keller, very different
202780	203780	thinkers.
203780	209500	Jonathan Pazio is a French-Canadian liturgical artist and icon carver known for his work featured
209500	211460	in museums across the world.
211460	217580	He carves Eastern Orthodox among other traditional images and teaches an online carving class.
217580	223180	He also runs a YouTube channel, This Symbolic World, dedicated to the exploration of symbolism
223180	224780	across history and religion.
224780	228020	Jonathan is one of the deepest religious thinkers I've ever met.
228020	235340	Jim Keller is a microprocessor engineer known very well in the relevant communities and
235340	239780	beyond them for his work at Apple and AMD, among other corporations.
239780	245460	He served in the role of architect for numerous game-changing processors, has co-authored
245460	252300	multiple instruction sets for highly complicated designs, and is credited for being the key
252300	260020	player behind AMD's renewed ability to compete with Intel in the high-end CPU market.
260020	267020	In 2016, Keller joined Tesla, becoming vice president of autopilot hardware engineering.
267020	272180	In 2018, he became a senior vice president for Intel.
272180	277260	In 2020, he resigned due to disagreements over outsourcing production, but quickly found
277260	282300	a new position at TENS Torrent as chief technical officer.
282300	287020	We're going to sit today and discuss the perils and promise of artificial intelligence, and
287020	290020	it's a conversation I'm very much looking forward to.
290020	293420	So welcome to all of you watching and listening.
293420	297700	I thought it would be interesting to have a three-way conversation.
297700	301900	Jonathan and I have been talking a lot lately, especially with John Verveke and some other
301900	309260	people as well, about the fact that it seems necessary for us to view for human beings
309260	311020	to view the world through a story.
311020	320380	In fact, when we describe the structure that governs our action and our perception, that
320380	322060	is a story.
322060	325940	And so we've been trying to puzzle out, I would say, to some degree on the religious
326020	330060	front, what might be the deepest stories.
330060	334860	And I'm very curious about the fact that we perceive the world through a story, human
334860	336060	beings do.
336060	341340	And that seems to be a fundamental part of our cognitive architecture and of cognitive
341340	345460	architecture in general, according to some of the world's top neuroscientists.
345460	350860	And I'm curious, and I know Jim is interested in cognitive processing and in building systems
350860	357180	that in some sense seem to run in a manner analogous to the manner in which our brains
357180	358180	run.
358180	362180	And so I'm curious about the overlap between the notion that we have to view the world
362180	364700	through a story and what's happening on the AI front.
364700	367780	There's all sorts of other places that we can take the conversation.
367780	369500	So maybe I'll start with you, Jim.
369500	374900	Do you want to tell people what you've been working on and maybe give a bit of a background
374900	379580	to everyone about how you conceptualize artificial intelligence?
380460	381020	Yeah, sure.
381020	388540	So first, I'll say technically, I'm not an artificial intelligent researcher.
388540	390860	I'm a computer architect.
390860	397100	And I'd say my skill set goes from somewhere around the atom up to the program.
397100	402940	So we make transistors out of atoms, we make logical gates out of transistors, we make
402940	405180	computers out of logical gates.
405180	407860	We run programs on those.
407860	415100	And recently, we've been able to run programs fast enough to do something called an artificial
415100	421260	intelligence model or neural network, depending on how you say it.
421260	428100	And then we're building chips now that run artificial intelligence models fast.
428100	432060	And we have a novel way to do it, the company I work at.
432060	433900	But lots of people are working on it.
433900	441260	And I think we were sort of taken by surprise what's happened in the last five years, how
441260	451420	quickly models started to do interesting and intelligent seeming things.
451420	456260	There's been an estimate that human brains do about 10 to the 18th operations a second.
456260	457260	It sounds like a lot.
457260	460940	It's a billion, billion operations a second.
460940	468820	And a little computer processor in your phone probably does 10 billion operations a second.
468820	473900	And then if you use the GPU, maybe 100 billion, something like that.
473900	481540	And big modern AI computers like OpenAI use this or Google or somebody, they're doing
481540	486180	like 10 to the 16th, maybe slightly more operations a second.
486180	492580	So they're within a factor of 100 of a human brain's raw computational ability.
492580	494060	And by the way, that could be completely wrong.
494060	497100	Our understanding of how the human brain does computation could be wrong.
497100	501940	But lots of people have estimated, based on number of neurons, number of connections,
501940	507540	how fast neurons fire, how many operations a neuron firing seems to involve.
507540	514020	I mean, the estimates range by a couple orders of magnitude, but when our computers got fast
514020	520100	enough, we started to build things called language models and image models that do fairly
520100	522180	remarkable things.
522180	526460	So what have you seen in the last few years that's been indicative of this, of the change
526460	528500	that you described as revolutionary?
528500	534580	What are computers doing now that you found surprising because of this increase in speed?
534580	541880	Yeah, you can have a language model read a 200,000 word book and summarize it fairly accurately.
541880	543580	So it can extract out the gist?
543580	544940	The gist of it.
544940	546380	Can they do that with fiction?
546380	547380	Yeah.
547380	551980	Yeah, and I'm going to introduce you to a friend who took a language model and changed
551980	558700	it and fine tuned it with Shakespeare and use it to write screenplays that are pretty
558700	560700	good.
560700	563380	And these kinds of things are really interesting.
563380	567060	And then we were talking about this a little bit earlier.
567060	575180	So when computers do computations, a program will say add A equal B plus C. The computer
575180	579900	does those operations on representations of information, ones and zeros.
579900	582180	It doesn't understand them at all.
582180	585780	The computer has no understanding of it.
585780	592700	But what we call a language model translates information like words and images and ideas
592700	598380	into a space where the program, the ideas and the operation it does on them are all
598380	601780	essentially the same thing.
601780	604700	We'll be right back with Jonathan Pageau and Jim Keller.
604700	611700	First we wanted to give you a sneak peek at Jordan's new documentary, Logos in Literacy.
611700	618140	I was very much struck by how the translation of the biblical writings jump-started the
618140	621420	development of literacy across the entire world.
621420	623340	Literacy was the norm.
623340	629060	The pastor's home was the first school and every morning it would begin with singing.
629060	632260	The Christian faith is a singing religion.
632260	637220	Probably 80% of scripture memorization today exists only because of what is sung.
637220	638220	This is amazing.
638220	644780	Here we have a Gutenberg Bible printed on the press of Johann Gutenberg.
644780	650540	Science and religion are opposing forces in the world, but historically that has not
650660	651660	been the case.
651660	653980	Now the book is available to everyone.
653980	662780	From Shakespeare to modern education and medicine and science to civilization itself.
662780	668500	It is the most influential book in all of history and hopefully people can walk away
668500	671940	with at least a sense of that.
671940	682540	A language model can produce words and then use those words as inputs.
682540	687340	It seems to have an understanding of what those words are, which is very different from
687340	689980	how a computer operates on data.
689980	697180	About the language models, I mean my sense of at least in part how we understand a story
697220	704380	is that maybe we're watching a movie, let's say, and we get some sense of the character's
704380	710900	goals and then we see the manner in which that character perceives the world and we in
710900	714580	some sense adopt his goals, which is to identify with character.
714580	720060	Then we play out a panoply of emotions and motivations on our body because we now inhabit
720060	726980	that goal space and we understand the character as a consequence of mimicking the character
726980	730740	with our own physiology.
730740	735100	You have computers that can summarize the gist of a story, but they don't have that underlying
735100	736100	physiology.
736100	741940	First of all, it's a theory that your physiology has anything to do with it.
741940	747740	You could understand the character's goals and then get involved in the details of the
747740	748740	story.
748740	754140	Then you're predicting the path of the story and also having expectations and hopes for
754140	755140	the story.
755140	761700	A good story kind of takes you on a ride because it teases you with doing some of the things
761700	766140	you expect, but also doing things that are unexpected and possibly that creates an emotional
766140	767140	...
767140	768140	It does.
768140	769140	It does.
769140	773580	In an AI model, you can easily have a set of goals.
773580	777220	You have your personal goals and then when you watch the story, you have those goals.
777220	779460	You put those together.
779460	781020	How many goals is that?
781020	786580	The story's goals and your goals, hundreds, thousands, those are small numbers.
786580	788300	Then you have the story.
788300	793420	The AI model can predict the story too, just as well as you can.
793420	795740	That's the thing that I find mysterious is that ...
795740	801300	As the story progresses, it can look at the error between what it predicted and what actually
801300	805980	happened and then iterate on that.
805980	809140	You would call that emotional, excitement, disappointment ...
809140	810140	Anxiety.
810220	811220	Anxiety.
811220	812220	Yeah, definitely.
812220	815780	A big part of what anxiety does seem to be is discrepancy.
815780	819020	Some of those states are manifesting your body because you trigger hormone cascades
819020	823740	and a bunch of stuff, but you also can just scan your brain and see that stuff move around.
823740	824740	Right.
824740	825740	Right.
825740	831500	The AI model can have an error function and look at the difference between what it expected
831500	835940	and not, and you could call that the emotional state if you want it.
835940	836940	I just talked with the ...
836940	837940	That's speculation.
837940	838940	No, no.
838940	840340	That's accurate.
840340	844100	We can make an AI model that could predict the result of a story probably better than
844100	847100	the average person.
847100	848100	One of the things ...
848100	851500	Some people are really good at ... They're really well educated about stories or they
851500	857820	know the genre or something, but these things ... What they see today as the capacity of
857820	862020	the models is, if you say start describing a lot, it will make sense for a while, but
862020	865780	it will slowly stop making sense.
865780	866780	That's possible.
866820	872060	It's simply the capacity of the model right now, and the model is not well grounded enough
872060	876220	in a set of goals and reality or something to make sense for a while.
876220	877700	What do you think would happen, Jonathan?
877700	883300	This is, I think, associated with the kind of things that we've talked through to some
883300	884800	degree.
884800	895420	One of my hypotheses, let's say, about deep stories is that they're metagists in some
895420	896420	sense.
896420	901740	You could imagine 100 people telling you a tragic story, and then you could reduce each
901740	905940	of those tragic stories to the gist of the tragic story, and then you could aggregate
905940	909100	the gists, and then you'd have something like a metatragity.
909100	915540	I would say the deeper the gist, the more religious-like the story gets.
915540	919420	That's part of ... It's that idea as part of the reason that I wanted to bring you guys
919420	920420	together.
920420	925180	One of the things that what you just said makes me wonder is, imagine that you took Shakespeare
926140	934500	and you took Dante, and you took the canonical Western writers, and you trained an AI system
934500	940820	to understand the structure of each of them, and then now you could pull out the summaries
940820	947380	of those structures, the gists, and then couldn't you pull out another gist out of that?
947380	952140	So it would be like the essential element of Dante and Shakespeare, and I wanted to
952300	953300	get biblical.
953300	958180	Jonathan said so far, and then ... So here's one funny thing to think about.
958180	959660	You use the word pull out.
959660	965780	So when you train a model to know something, you can't just look in it and say, what is
965780	966780	it?
966780	967780	No.
967780	968780	You have to quarry it.
968780	969780	Right.
969780	970780	You have to ask.
970780	971780	Right.
971780	972780	Right.
972780	973780	What's the next sentence in this paragraph?
973780	976020	What's the answer to this question?
976020	980780	There's the thing on the internet now called prompt engineering, and it's the same way.
980820	983140	I can't look in your brain to see what you think.
983140	984140	Yeah.
984140	987420	I have to ask you what you think, because if I killed you and scanned your brain and
987420	992220	got the current state of all the synapses and stuff, A, you'd be dead, which should
992220	995860	be sad, and B, I wouldn't know anything about your thoughts.
995860	1002460	Your thoughts are embedded in this model that your brain carries around, and you can express
1002460	1004620	it in a lot of ways.
1004620	1005620	And so ...
1005620	1006620	So you could add ...
1006620	1010500	How do you train ... This is my big question is ... I mean, because the way that I've been
1010540	1017100	seeing it until now is that artificial intelligence, it's based on us.
1017100	1021340	It doesn't exist independently from humans, and it doesn't have care.
1021340	1024460	The question would be, why does the computer care?
1024460	1026900	Yeah, that's not true.
1026900	1029980	Why does the computer care to get the gist of the story?
1029980	1033740	Well, yeah, so I think you're asking kind of the wrong question.
1033740	1039220	So you can train an AI model on the physics and reality and images in the world, just
1039260	1041260	with images.
1041260	1046860	And there are people who are figuring out how to train a model with just images, but
1046860	1053780	the model itself still conceptualizes things like tree and dog and action and run, because
1053780	1057740	those all exist in the world.
1057740	1063820	So ... And you can actually train ... And when you train a model with all the language
1063900	1069780	and words, so all information has structure, and I know you're a structure guy from your
1069780	1070780	video.
1070780	1075580	So if you look around you at any image, every single point you see makes sense.
1075580	1076580	Yeah.
1076580	1077580	Right?
1077580	1079740	It's a teleological structure.
1079740	1083500	It's a purpose laid in structure, right?
1083500	1084500	So this is something we talk about.
1084500	1089580	Yeah, so it turns out all the words that have ever been spoken by human beings also have
1089580	1090580	structure.
1090580	1091580	Right.
1091580	1092580	Right.
1093580	1099340	And so physics has structure, and it turns out that some of the deep structure of images
1099340	1103580	and actions and words and sentences are related.
1103580	1104580	Mm-hmm.
1104580	1113180	Like, there's actually a common core of ... Imagine there's like a knowledge space, and sure there's
1113180	1118540	details of humanity where they prefer this accent versus that.
1118540	1122140	Those are kind of details, but they're coherent in the language model.
1122140	1127300	But the language models themselves are coherent with our world ideas, and humans are trained
1127300	1132660	in the world just the way the AI models are trained in the world, like a little baby.
1132660	1137900	As it's learning, looking around, it's training on everything it sees when it's very young,
1137900	1142580	and then its training rate goes down, and it starts interacting with what it's learning,
1142580	1144020	and interacting with the people around it.
1144020	1145500	But it's trying to survive.
1145500	1146780	It's trying to live.
1146780	1150780	It has ... Like, it has the infant or the child has ...
1150820	1154660	Neurons aren't trying ... The weights in the neurons aren't trying to live.
1154660	1157340	What they're trying to do is reduce the error.
1157340	1163500	So neural networks generally are predictive things, like what's coming next?
1163500	1165180	What makes sense?
1165180	1167580	How does this work?
1167580	1175580	And when you train an AI model, you're training it to reduce the error in the model, and if
1175580	1176580	your model's big ...
1176580	1178500	So let me ask you about that.
1179220	1181020	Well, first of all ...
1181020	1182620	So babies are doing the same thing.
1182620	1186220	Like, they're looking at stuff go around, and in the beginning, their neurons are just
1186220	1191300	randomly firing, but as it starts to get object permanence and look at stuff, it starts predicting
1191300	1196260	what it'll make sense for that thing to do, and when it doesn't make sense, it'll update
1196260	1197260	its model.
1197260	1203020	So basically, it compares its prediction to the events, and then it will adjust its prediction.
1203020	1208820	So in a story prediction model, the AI would predict the story, then compare it to its
1208820	1212900	prediction and then fine-tune itself slowly as it trains itself.
1212900	1213900	Okay, so ...
1213900	1217260	Or at a reverse, you could ask it to say, given the set of things, tell the rest of
1217260	1220580	the story, and it could do that.
1220580	1226380	And the state of it right now is there are people having conversations with us that are
1226380	1227380	pretty good.
1227380	1228380	Mm-hmm.
1228380	1232020	So I talked to Carl Friston about this prediction idea in some detail.
1232020	1236780	And so Friston, for those of you who are watching and listening, is one of the world's top neuroscientists.
1236780	1242620	And he's developed an entropy enclosure model of conceptualization, which is analogous to
1242620	1247780	one that I was working on, I suppose, across approximately the same time frame.
1247780	1253100	So the first issue, and this has been well-established in the neuropsychological literature for quite
1253100	1261660	a long time, is that anxiety is an indicator of discrepancy between prediction and actuality.
1261660	1267620	And then positive emotion also looks like a discrepancy reduction indicator.
1267620	1272740	So imagine that you're moving towards a goal, and then you evaluate what happens as you
1272740	1273980	move towards the goal.
1273980	1278300	And if you're moving in the right direction, what happens is what you might say, what you
1278300	1281380	expect to happen, and that produces positive emotion.
1281380	1285580	And it's actually an indicator of reduction in entropy.
1285580	1287180	That's one way of looking at it.
1287180	1293420	And the point is that you have a bunch of words in there that are psychological definitions
1293420	1298380	of states, but you could say there's a prediction and an error prediction, and you're reducing
1298380	1299380	error.
1299380	1305300	Yes, but what I'm trying to make a case for is that your emotions directly map that, both
1305300	1311220	positive and negative emotion, look like there's signifiers of discrepancy reduction on the
1311220	1313100	positive and negative emotion side.
1313100	1318540	But then there's a complexity that I think is germane to part of Jonathan's query, which
1318540	1319940	is that...
1319940	1325780	So the neuropsychologists and the cognitive scientists have talked a long time about expectation,
1325780	1327980	prediction, and discrepancy reduction.
1327980	1333940	But one of the things they haven't talked about is it isn't exactly that you expect things.
1333940	1335660	It's that you desire them.
1335660	1337420	You want them to happen.
1337420	1341820	Because you could imagine that there's, in some sense, a literally infinite number of
1341820	1343900	things you could expect.
1343900	1347820	And we don't strive only to match prediction.
1347820	1350780	We strive to bring about what it is that we want.
1350780	1355940	And so we have these preset systems that are teleological, that are motivational systems.
1355940	1357540	Well, I mean, it depends.
1357540	1363940	If you're sitting idly on the beach in a bird flies by, you expect it to fly along in a
1363940	1364940	regular path.
1364940	1365940	Right.
1365940	1366940	You don't really want that to happen.
1366940	1370540	Yeah, but you don't want it to turn into something that could peck out your eyes either.
1370540	1371540	Sure.
1372260	1378060	But you're kind of following it with your expectation to look for discrepancy, right?
1378060	1379060	Yes.
1379060	1383660	Now, you'll also have a, you know, depends on the person, somewhere between 10 and a
1383660	1386860	million desires, right?
1386860	1389700	And then you also have fears and avoidance.
1389700	1391620	And those are context.
1391620	1395780	So if you're sitting on the beach with some anxiety that the birds are going to swerve
1395780	1400420	at you and peck your eyes out, so then you might be watching it much more attentively
1400420	1403900	than somebody who doesn't have that worry, for example.
1403900	1409100	But both of you can predict where it's going to fly and you'll both notice a discrepancy.
1409100	1415460	The motivations, one way of conceptualizing fundamental motivation is they're like a priori
1415460	1418060	prediction domains, right?
1418060	1424180	And so it helps us narrow our attentional focus because I know when you're sitting and
1424180	1430140	you're not motivated in any sense, you can be doing just in some sense, trivial expectation
1430140	1433580	computations, but often we're in a highly motivated state.
1433580	1438420	And what we're expecting is bounded by what we desire and what we desire is oriented as
1438420	1441500	Jonathan pointed out towards the fact that we want to exist.
1441500	1448860	And one of the things I don't understand and wanted to talk about today is how the computer
1448860	1459060	models, the AII models, can generate intelligible sense without mimicking that sense of motivation.
1459060	1462500	As you've said, for example, they can just derive the patterns from observations of the
1462500	1463500	objective world.
1463500	1471940	So again, I don't want to do all the talking, but so AI generally speaking, when I first
1471940	1474260	learned it about it, it had two behaviors.
1474260	1476420	They call it inference and training.
1476420	1480260	So inferences, you have a trained model, so you give it a picture and say, is there a
1480260	1481260	cat in it?
1481260	1482980	And it tells you where the cat is.
1482980	1483980	That's inference.
1483980	1486660	The model has been trained to know where a cat is.
1486660	1491260	And training is the process of giving it an input and an expected output.
1491260	1495620	And when you first start training the model, it gives you garbage out, like an untrained
1495620	1497020	brain would.
1497020	1501500	And then you take the difference between the garbage output and the expected output and
1501500	1503140	call that the error.
1503140	1507660	And then they invent the big revelation was something called back propagation with gradient
1507660	1508740	descent.
1508740	1516340	But that means take the error and divide it up across the layers and correct those calculations
1516980	1521900	so that when you put a new thing in, it gives you a better answer.
1521900	1527980	And then to somewhat my astonishment, if you have a model of sufficient capacity and you
1527980	1533340	train it with 100 million images, if you give it a novel image and say, tell me where the
1533340	1536460	cat is, it can do it.
1536460	1542340	That's called, so training is the process of doing a pass with an expected output and
1542340	1545260	propagating an error back through the network.
1545260	1550020	And inference is the behavior of putting something in and getting an output.
1550020	1552300	I think I'm really pulling.
1552300	1559380	But there's a third piece, which is what the new models do, which is called generative,
1559380	1561980	it's called a generative model.
1561980	1566740	So for example, say you put in a sentence and you say, predict the next word.
1566740	1568540	This is the simplest thing.
1568540	1570020	So it predicts the next word.
1570020	1574420	So you add that word to the input and I'll say predict the next word.
1574420	1578340	So it contains the original sentence and the word you generated.
1578340	1584620	And it keeps generating words that make sense in the context of the original word in addition.
1584620	1587700	This is the simplest basis.
1587700	1590260	And then it turns out you can train this to do lots of things.
1590260	1596660	You can train it to summarize a sentence, you can train it to answer a question.
1596660	1600900	There's a big thing about, you know, like Google every day has hundreds of millions
1600900	1605820	of people asking it questions and giving answers and then rating the results.
1605820	1609740	You can train a model with that information so you can ask it a question and it gives
1609740	1611300	you a sensible answer.
1611300	1617700	But I think in what you said, I actually have the issue that has been going through my mind
1617700	1621860	so much is when you said, you know, people put in the question and then they rate the
1621860	1623340	answer.
1623340	1630620	My intuition is that the intelligence still comes from humans in the sense that it seems
1630620	1635260	like in order to train whatever AI, you have to be able to give it a lot of power.
1635260	1639900	And then say at the beginning, this is good, this is bad, this is good, this is bad, like
1639900	1644620	reject certain things, accept certain things in order to then reach a point when then you
1644620	1645780	train the AI.
1645780	1647740	And so that's what I mean about the care.
1647740	1653540	So the care will come from humans because the care is the one giving it the value, saying
1653540	1659460	this is what is valuable, this is what is not valuable in your calculation.
1659460	1664460	So when they first, so there's a program called AlphaGo that I learned how to play go better
1664460	1665460	than a human.
1665460	1667900	So there's two ways to train the model.
1667900	1673420	One is they have a huge database of lots of go games with good winning moves.
1673420	1676740	So they train the model with that and that worked pretty good.
1676740	1683380	And they also took two simulations of go and they did random moves.
1683380	1689060	And all that happened was is these two simulators played one go game and they just recorded
1689060	1694620	whichever moves happened to win and it started out really horrible and they just started
1694620	1698460	training the model and this is called adversarial learning, it's a particular adversarial.
1698460	1704180	It's like, you know, you make your moves randomly and you train a model and so they train multiple
1704180	1708940	models and over time those models got very good and they actually got better than human
1708940	1714340	players because the humans have limitations about what they know, whereas the models could
1714340	1718300	experiment in a really random space and go very far.
1718300	1719300	Yeah.
1719300	1721540	But experiment towards the purpose of winning the game.
1721540	1722540	Yes.
1722540	1728820	Well, but you can experiment towards all kinds of things that turns out and humans are also
1728820	1729820	trained that way.
1729820	1732980	Like when you were learning, you were reading, you were saying, this is a good book.
1732980	1733980	This is a bad book.
1733980	1734980	This is good sentence construction.
1734980	1735980	It's good.
1735980	1736980	It's going.
1736980	1741260	So you've gotten so many error signals over your life.
1741260	1743500	Well, that's what culture does in large parties.
1743500	1744500	Culture does that.
1744500	1746020	Religion does that.
1746020	1748020	Your everyday experience does that.
1748020	1749020	Your family.
1749020	1750740	So we embody that.
1750740	1751740	Yeah.
1751740	1757020	And we're all, and everything that happens to us, we process it on the inference pass
1757020	1758980	which generates outputs.
1758980	1763260	And then sometimes we look at that and say, hey, that's unexpected or that got a bad result
1763260	1765180	or that got bad feedback.
1765180	1769420	And then we back propagate that and update our models.
1769420	1773920	So really well trained models can then train other models.
1773920	1777540	So the human trained now are the smartest people in the world.
1777540	1786580	So the biggest question that comes now based on what you said is, because my main point
1786580	1792060	is to try to show how it seems like artificial intelligence is always an extension of human
1792060	1793060	intelligence.
1793060	1795340	It remains an extension of human intelligence.
1795340	1797780	And maybe the way to- That won't be true at all.
1797780	1804460	So do you think that at some point the artificial intelligence will be able to, because the
1804460	1812700	goals recognizing cats, writing plays, all these goals are goals which are based on embodied
1812700	1814220	human existence.
1814220	1820460	Could an AI at some point develop a goal which would be uncomprehensible to humans because
1820460	1824220	of its own existence?
1824220	1828820	For example, there's a small population of humans that enjoy math.
1828820	1838300	And they are pursuing adventures in math space that are incomprehensible to 99.99% of humans.
1838300	1844240	But they're interested in it and you could imagine like an AI program working with those
1844240	1850340	mathematicians and coming up with very novel math ideas and then interacting with them.
1850340	1857620	But they could also, if some AIs were elaborating out really interesting and detailed stories,
1857620	1860580	they could come up with stories that are really interesting.
1860580	1866180	We're going to see it pretty soon like all of our- Could there be a story that is interesting
1866180	1869580	only to the AI and not interesting to us?
1869580	1871180	That's possible.
1871180	1876940	So stories are like I think some high level information space.
1877780	1882460	The computing age of big data, there's all this data running on computers, the only humans
1882460	1885260	understood it, the computers don't.
1885260	1891900	So AI programs are now at the state where the information, the processing and the feedback
1891900	1894860	loops are all kind of in the same space.
1894860	1898260	They're still relatively rudimentary to humans.
1898260	1902340	I guess some AI programs in certain things are better than humans already, but for the
1902340	1904260	most part they're not.
1904260	1906260	But it's moving really fast.
1906260	1910940	And so you could imagine, I think in five or 10 years most people's best friends will
1910940	1917740	be AIs and they'll know you really well and they'll be interested in you and it's-
1917740	1920260	Unlike your real friends.
1920260	1921260	Real friends are problematic.
1921260	1923380	They're only interested in you when you're interested.
1923380	1924700	Yeah, yeah, real friends are-
1924700	1928380	The AI systems will love you even when you're dull and miserable.
1928380	1934780	Well there's so much idea space to explore and humans have a wide range.
1934780	1938420	Some humans like to go through their everyday life doing their everyday things and some
1938420	1943380	people spend a lot of time like you, a lot of time reading and thinking and talking and
1943380	1946140	arguing and debating.
1946140	1955020	And there's going to be I'd say a diversity of possibilities with what a thinking thing
1955020	1959580	can do when the thinking is fairly unlimited.
1959580	1968300	So I'm curious about, I'm curious in pursuing this issue that Jonathan has been developing.
1968300	1974740	So there's a literally infinite number of ways, virtually infinite number of ways that
1974740	1978020	we could take images of this room, right?
1978020	1981700	Now if a human being is taking images of this room they're going to be, they're going to
1981700	1986300	sample a very small space of that infinite range of possibilities because if I was taking
1986300	1993060	pictures in this room in all likelihood I would take pictures of objects that are identifiable
1993060	1998620	to human beings that are functional to human beings at a level of focus that makes those
1998620	2000460	objects clear.
2000460	2005540	And so then you could imagine that the set of all images on the internet has that implicit
2005540	2010780	structure of perception built into it and that's a function of what human beings find
2010780	2011780	useful.
2011780	2015660	No, I mean I could take a photo of you that was, the focal depth was here and here and
2015660	2019700	here and here and here and two inches past you and now I suppose you could.
2019700	2022900	There's a technology for that called light fields.
2022900	2027220	So then you could, if you had that picture properly done then you could move around it
2027220	2029540	and imagine and see.
2029540	2030540	But yeah, fair enough.
2030540	2031540	I get your point.
2031540	2037460	Like the human recorded data has our biology built into it.
2037460	2044820	Has our biology built into it but also unbelievably detailed in coding of how physical reality
2044820	2047100	works, right?
2047100	2051220	So every single pixel in those pictures, even though you kind of selected the view, the
2051220	2058020	focus, the frame, it still encoded a lot more information than your processing.
2058020	2062300	And if you take a large, it turns out if you take a large number of images of things in
2062300	2067180	general, so you've seen these things where you take a 2D image and turn it into a 3D
2067900	2068900	image.
2068900	2069900	Right.
2069900	2076140	The reason that works is even in the 2D image, the 3D image in the room actually got embedded
2076140	2078180	in that picture in a way.
2078180	2082820	Then if you have the right understanding of how physics and reality works, you can reconstruct
2082820	2083820	the 3D model.
2083820	2085820	Okay, so this reminds me.
2085820	2092380	But you could, you know, an AI scientist may cruise around the world with infrared and
2092420	2097100	radio wave cameras, and they might take pictures of all different kinds of things, and every
2097100	2101300	once in a while, they'd show up and go, hey, the sun, you know, I've been staring at the
2101300	2106420	sun in the ultraviolet and radio waves for the last month, and it's way different than
2106420	2111780	anybody thought because humans tend to look at light and visible spectrum.
2111780	2116700	And you know, there could be some really novel things coming out of that.
2116700	2120700	But humans also, we live in the spectrum we live in because it's a pretty good one for
2120820	2121820	planet Earth.
2121820	2126900	Like, it wouldn't be obvious that AI would start some different place, like visible spectrum
2126900	2129860	is interesting for a whole bunch of reasons.
2129860	2130860	Right.
2130860	2135500	So in a set of images that are human derived, you're saying that there's, the way I would
2135500	2140380	conceptualize that is that there's two kinds of logos embedded in that.
2140380	2144860	One would be that you could extract out from that set of images what was relevant to human
2144860	2152020	beings, but you're saying that the fine structure of the objective world outside of human concern
2152020	2157980	is also embedded in the set of images, and that an AI system could extract out a representation
2157980	2161580	of the world, but also a representation of what's motivating to human beings.
2161580	2162580	Yes.
2162580	2166940	And then some human scientists already do look at the sun and radio waves and other things
2166940	2170700	because they're trying to, you know, get different angles on how things work.
2170700	2171700	Yeah.
2171700	2174820	Well, I guess it's a, it's a, it's a curious thing.
2174820	2180100	It's like the same with like buildings and architecture, mostly fit people.
2180100	2183020	Well, the other, there's a reason for that.
2183020	2187340	The reason why I keep coming back to hammering the same point is that even in terms of the
2187340	2194460	development of the AI, that is developing AI requires immense amount of money, energy,
2194460	2196300	you know, and time.
2196300	2200260	And so that's a transient thing in 30 years, it won't cost anything.
2200260	2202860	So it's, that's, that's going to change so fast.
2202860	2203860	It's amazing.
2204380	2209140	That's a, like supercomputers used to cost millions of dollars and now your phone is
2209140	2210140	the supercomputer.
2210140	2215700	So it's the time between millions of dollars and $10 is about 30 years.
2215700	2222220	So it's like, I'm just saying it's like the time and effort isn't a thing in technology.
2222220	2224580	It's moving pretty fast.
2224580	2227780	It just, that's just, that just says the date.
2227780	2228780	Yeah.
2228780	2233820	But even making, even making, let's say, even, I mean, I guess maybe the, the, the
2233820	2235260	this is the nightmare question.
2235260	2241500	Like, could you imagine an AI system which becomes completely autonomous, which is creating
2241500	2248700	itself even physically through automized factories, which is, you know, programming itself, which
2248700	2253580	is creating its own goals, which is not at all connected to human endeavor.
2253580	2254580	Yeah.
2254580	2258940	I mean, individual researchers can, you know, I have a friend who I'm going to introduce
2258940	2263100	you to him tomorrow, he wrote a program that scraped all of the internet and trained an
2263100	2267620	AI model to be a language model on a relatively small computer.
2267620	2272740	And in 10 years, the computer he could easily afford would be as smart as a human.
2272740	2276180	So he could train that pretty easily.
2276180	2281900	And that model could go on Amazon and buy 100 more of those computers and copy itself.
2281900	2285340	So yeah, we're, we're 10 years away from that.
2285340	2288100	And then, then why, like, why would it do that?
2288100	2290140	I mean, what does it does?
2290140	2291140	Is it possible?
2291140	2292500	It's all about the motivational question.
2292500	2296860	I think that that's what, what even Jordan and I both have been coming at from the outset.
2296860	2298260	It's like, so you have an image, right?
2298260	2304340	You have an image of, of Skynet or of the matrix, you know, in which the sentient AI
2304340	2306220	is actually fighting for its survival.
2306220	2313780	So it has a survival instinct, which is pushing it to self perpetuate, like to, to, to replicate
2313780	2319620	itself and to create variation on itself in order to survive and identifies humans as
2319620	2321140	an obstacle to that.
2321140	2322140	You know?
2322260	2322700	Yeah.
2322700	2324940	So you have a whole bunch of implicit assumptions there.
2324940	2328620	So, so humans last I checked are unbelievably competitive.
2329220	2333660	And when you let people get into power with no checks on them, they typically run them
2333660	2337020	up. It's been a historical, historical experience.
2337020	2343660	And then humans are, you know, self regulating to some extent, obviously with some serious
2343660	2350340	outliers, because they self regulate with each other and humans and AI models at some
2350380	2358500	point will have to find their own calculation of self regulation and tradeoffs about that.
2358500	2363420	Yeah. Because the AI doesn't feel pain, at least as we, if that we don't know that it
2363420	2363620	feels.
2363620	2365420	Well, lots of humans don't feel pain either.
2365420	2370660	So I mean, that's, I mean, the humans feeling pain or not, didn't, you know, doesn't stop
2370660	2371860	a whole bunch of activity.
2371860	2375380	I mean, that's, I mean, it doesn't, the fact that we feel pain doesn't stop.
2375380	2377420	It doesn't regulate that many people.
2377940	2378300	Right.
2378340	2382020	I mean, there's definitely people like, you know, children, if you threaten them with,
2382620	2385180	you know, go to your room and stuff, you can regulate them that way.
2385180	2388060	But some kids ignore that completely and adults are.
2388060	2389740	And it's often counterproductive.
2389780	2390140	Yeah.
2390140	2397420	So, so right, you know, you know, culture and societies and organizations, we regulate
2397420	2402300	each other, you know, sometimes in competition and cooperation.
2402300	2402540	Yeah.
2402940	2406980	Do you, do you think that we've talked about this to some degree for decades?
2406980	2412220	I mean, when you look at how fast things are moving now, and as you push that along,
2413020	2418860	what, what, when you look out 10 years and you see the relationship between the AI
2418860	2422340	systems that are being built and human beings, what do you envision?
2423220	2424900	Or can you envision it?
2426780	2429500	Well, can I, yeah, like I said, I'm a computer guy.
2430260	2433620	And I'm watching this with, let's say, some fascination as well.
2434500	2439580	I mean, the last, so Ray Kurzweil said, you know, pro, progress accelerates.
2439700	2440060	Yeah.
2440180	2440460	Right.
2440460	2443820	So we, we have this idea that 20 years of progress is 20 years.
2443820	2448460	But, you know, the last 20 years of progress was 20 years and the next 20 years will
2448460	2450220	probably be, you know, five to 10.
2450260	2451060	Right, right, right.
2451060	2451980	And, and.
2452380	2453900	And you can really feel that happening.
2453900	2460140	To some level, that causes social stress, independent of whether it's AI or, or Amazon
2460180	2464460	deliveries, you know, what, you know, there's so many things that are going into the, the
2464460	2465420	stress of it all.
2466020	2469460	Well, there's, but there's progress, which is an extension of human capacity.
2469820	2473940	And then there's this progress, which I'm hearing about the way that you're describing
2473940	2481060	it, which seems to be an inevitable progress towards creating something which is more
2481060	2482540	powerful than you.
2483460	2483780	Right.
2483780	2484820	And so what is that?
2484820	2486300	I don't even understand that drive.
2486420	2489500	Like, what is that drive to, to create something?
2489540	2490940	Which can supplant you.
2491180	2493700	So look at the average person in the world, right?
2493900	2498620	So the average person already exists in this world, because the average person is halfway
2498620	2500100	up the human hierarchy.
2500660	2504420	There's already many people more powerful than any of us.
2504660	2505860	They're, they could be smarter.
2505860	2506660	They could be richer.
2506660	2507900	They could be better connected.
2508420	2512340	We already live in a world like very few people are at the top of anything.
2513540	2513820	Right.
2513860	2515300	So that's already a thing.
2515740	2519460	So basically the drive to make someone a superstar that's there, the drive
2519500	2524900	to elevate someone above you, that would be the same drive that is bringing us to
2524900	2529340	creating these ultra powerful machines because we have that.
2529340	2530940	Like we have a drive to elevate.
2530940	2535220	Like, you know, when we see a rock star that we like, people want to submit
2535220	2535940	themselves to that.
2535940	2537020	They want to dress like them.
2537020	2541140	They want to raise them up above them as an example, something to follow, right?
2541140	2543500	Something to, to, to subject themselves to.
2543700	2544620	You see that with leaders.
2544620	2548500	You see that in the political world and in teams.
2548540	2550540	You see that in sports teams, the same thing.
2550540	2554300	And so you think we've always tried to build things that are beyond us.
2554300	2558620	You know, I mean, I mean, it's about, are we building, are we building a God?
2558620	2563020	Is that the, is that what people, is that the drive that is pushing someone towards?
2563220	2567500	Cause when I hear what you're describing, Jim, I hear something that is extremely
2567580	2568700	dangerous, right?
2568700	2572100	Sounds extremely dangerous to the very existence of humans.
2572300	2576820	Yet I see humans acting and moving in that direction almost without being able
2576860	2578540	to stop it, as if there's no one now.
2578540	2580220	I think it is unstoppable.
2580620	2584060	Well, that's one of the things we've also talked about is because I've asked
2584060	2589460	Jim straight out, you know, because of the hypothetical danger associated with
2589460	2591380	this, why not stop doing it?
2591380	2595300	And well, part of his answer is the ambivalence about the outcome, but also
2595300	2599420	that it isn't obvious at all that in some sense it's stoppable.
2600740	2604180	I mean, it's the, it's the cumulative action of many, many people that are
2604180	2605260	driving this along.
2605780	2610500	And even if you took out one player, even a key player, the probability that you
2610500	2614460	do anything but slow it infinitesimally is, is quite.
2614700	2617780	That because there's also a massive payoff for those that will succeed.
2618220	2619620	It's also set up that way.
2619620	2624740	People know that at least, at least until the AI take over or whatever, that
2624860	2630060	whoever is on the line towards increasing the power of the AI will, will
2630060	2632260	rake in major rewards.
2633060	2633380	Right?
2633380	2636460	Well, that's what we do with all cognitive acceleration, right?
2636820	2642500	Yeah, I could recommend Ian Banks as an author, English author, I think he
2642500	2645660	wrote a series of books on the, he called the culture novels.
2646220	2650700	And it was a world where there was humans and then there was AIs, the smartest
2650700	2652860	humans and AI's that were dumber than humans.
2652860	2656860	But there were some AIs that were much, much smarter and they, they lived in
2656860	2660260	harmony because they mostly all pursued what they wanted to pursue.
2660860	2666660	Humans pursued human goals and super smart AIs, pursued super smart AI goals.
2666660	2670900	And, and, you know, they, they communicated and worked with each other.
2670900	2674340	But, but they, they mostly, you know, they're different.
2674340	2677860	When they were different enough that that was problematic, their goals were
2677860	2679900	different enough that they didn't overlap.
2680100	2683620	Because one of the, one of the things that that would be my guess is like these
2683620	2686420	ideas where these super AIs get smart.
2686420	2688620	And the first thing they do is stomp out the humans.
2688620	2689620	It's like, you don't do that.
2689660	2692660	Like, like, you don't wake up in the morning and think, I have to stomp
2692660	2693780	out all the cats.
2693860	2695180	No, it's not about.
2695180	2699420	The cats do cat things and the ants do ant things and the birds do bird things.
2699420	2703500	And, and super smart mathematicians do smart mathematician things.
2703500	2706700	And, you know, guys who like to build houses do build house things.
2706700	2711660	And, you know, everybody, you know, the world, there's so much space in the
2711660	2719500	intellectual zone that people, people tend to go pursue the, in a good society.
2719660	2722460	Like you tend to pursue the stuff that you do.
2722460	2726740	And then the people in your zone, you self-regulate.
2727340	2731700	And you also, even in the social stratus, we self-regulate.
2732140	2736380	I mean, the, the, the recent political events of the last 10 years, the, the
2736380	2742060	weird thing to me has been why have, you know, people with power been overreaching
2742060	2745580	to take too much from people with less.
2745580	2747340	Like that's bad regulation.
2747940	2754020	But one of the aspects of increasing power is that increasing power is always
2754020	2760940	mediated, at least in one aspect by the military, by, by, let's say, physical
2761100	2766020	power on others, you know, and we can see that technology is linked and has been
2766020	2768460	linked always to military power.
2768780	2774460	And so the idea that there could be some AIs that will be our friends or whatever
2774660	2778900	is maybe possible, but the idea that there will be some AIs, which will be
2778900	2785420	weaponized is, seems absolutely inevitable because increasing power is always,
2785860	2789340	increasing technological power always moves towards, towards military.
2789380	2794420	So we've lived with atomic bombs since the 40s, right?
2794420	2800660	So the, I mean, the solution to this has been mostly, you know, some form of
2800660	2806660	mutual assured destruction or attacking me, like the response to attacking me is
2806660	2808100	so much worse than the.
2808260	2811060	Yeah, but it's also because we, we have reciprocity.
2811060	2813420	We recognize each other as the same.
2813700	2817820	So if I look into the face of another human, there, there's a limit of how
2817820	2819980	indifferent I think that person is for me.
2820460	2824820	But if I'm hearing something described as the possibility of superintelligences
2825020	2829380	that have their own goals, their own cares, their own structures, then how much
2829420	2833740	mirror is there between these two groups of people, these two groups?
2834140	2839540	Well, Jim's objection seems to be something like we're, we're making,
2839620	2843740	we may be making, when we're doomsaing, let's say, and I'm not saying there's
2843740	2848380	no place for that, we're making the presumption of something like a zero
2848380	2850500	sum competitive landscape, right?
2850500	2856100	Is that the, the idea and the idea behind movies like, like the Terminator is
2856100	2860620	that there is only so much resources and the machines and the human beings
2860620	2861780	would have to fight over it.
2861780	2865660	And you can see that that, that could easily be a preposterous assumption.
2865660	2869900	Now, I think that one of the fundamental points you're making, though, is also
2872020	2877460	there will definitely be people that will weaponize AI and those weaponized
2877460	2881660	AI systems will have as their goal, something like the destruction of human
2881660	2883820	beings, at least under some circumstances.
2883860	2888420	And then there's the possibility that that will get out of control because the
2888420	2892180	most effective systems at destroying human beings might be the ones that win,
2892180	2896060	let's say, and that could happen independently of whether or not it is a
2896060	2897740	true zero sum competition.
2897860	2898140	Yeah.
2898140	2904380	And also the, the effectiveness of military stuff doesn't need very smart
2904380	2906620	AI to be a lot better than it is today.
2907420	2911420	You know, I used to, you know, like the Star Wars movies where like, you know,
2911500	2915780	tens of thousands of years in the future, super highly trained, you know,
2915780	2918580	fighters can't hit somebody running across the field.
2918940	2920060	Like that's silly, right?
2920060	2923580	You can, you can already make a gun that can hit everybody in the room
2924180	2925300	without aiming at it.
2925300	2931380	It's, you know, there's like the, the military threshold is much lower than
2931380	2936260	any intelligence threshold, like for danger.
2936260	2940500	And, you know, like to the extent that we self-regulated through the nuclear
2940500	2941900	crisis is interesting.
2942780	2946100	I don't know if it's because we thought that the Russians were like us.
2946100	2949940	I kind of suspect the problem was that we thought they weren't like us.
2950500	2957380	And, but we still managed to make some calculation to say that any kind of
2957380	2959460	attack would be mutually devastating.
2959980	2963100	Well, when you, when you look at, you know, the destructive power of the
2963100	2967740	military we already have so far exceeds the planet, I'm, I'm not sure, like
2968020	2970220	adding intelligence to it is the tipping point.
2970220	2977260	Like that's, I think the more likely thing is things that are truly smart
2977260	2979740	in different ways will be interested in different things.
2980460	2986020	And then the possibility for, let's say, mutual flourishing is, is, is really
2986020	2986660	interesting.
2987100	2991180	And I know artists using AI already to do really amazing things.
2991180	2993340	And, and that's already happening.
2994100	2998540	Well, when you, when you're working on the frontiers of AI development and you
2998540	3002060	see the development of increasingly intelligent machines, I mean, I know
3002060	3005300	that part of what drives you is, I don't want to put words in your mouth,
3005300	3009860	but what drives intelligent engineers in general, which is to take something
3009860	3012580	that works and make it better and maybe to make it radically better and
3012580	3013460	radically cheaper.
3013660	3016900	So, so there's this drive toward technological improvement.
3016900	3019940	And I know that you like to solve complex problems and you do that
3019940	3020940	extraordinarily well.
3021460	3028420	What, but do you, do you, is there also a vision of a more abundant form of
3028420	3032100	human flourishing emerging from the, from the development?
3032220	3033620	So what, so what do you see happening?
3033620	3036420	Well, you said it years ago, it's like, we're going to run out of energy.
3036420	3036900	What's next?
3036900	3038020	We're going to run out of matter.
3038060	3038460	Right.
3038740	3044580	Like our ability to do what we want in ways that are interesting and, you know,
3044580	3048220	for some people, beautiful is limited by a whole bunch of things.
3048220	3052420	Cause we're, you know, partly it's technological and partly with, you
3052420	3057540	know, we're stupidly divisive, but, um, but there is, there's
3057540	3062220	also possible that there's also a reality, which is one of the things
3062220	3067220	that technology has been is of course, an increase in power towards desire,
3067220	3068260	towards human desire.
3068260	3075060	And that is represented in mythological stories where let's say technology
3075060	3078180	is used to accomplish impossible desire, right?
3078180	3082580	We have, you know, the story of the story of building the, the McCann,
3082580	3087220	the bull around the king of Meno, the, the wife of the king of Meno's, you
3087220	3092100	know, in order to be inseminated by, uh, by a bull, we have the story, the, the,
3092660	3097780	we have the story of the, um, sorry, Frankenstein, et cetera, the story of
3097780	3102420	the golem where we put our desire into this increased power.
3102420	3104900	And then what happens is that we don't know our desires.
3104900	3108580	That's one of the things that I've also been worried about in terms of AI is that
3110020	3115140	we act, we have secret desires that enter into what we do that people
3115220	3116660	aren't totally aware of.
3117220	3122660	And as we increase in power, these systems, those desires,
3123380	3126100	let's say the, the pot, like the idea, for example, of the possibility
3126100	3131380	of having an AI friend and the idea that an AI friend would be the best friend
3131380	3135220	you've ever had because that, that friend would be the nicest to you,
3135220	3137940	would care the most about you, would do all those things.
3137940	3140900	That would be an exact example of what I'm talking about,
3140900	3143380	which is it's really the story of the genie, right?
3143380	3147540	It's the story of the, the genie in the lamp where the genie says,
3147540	3148580	what do you wish?
3148580	3151940	And the, and the person, and I have unlimited power to give it to you.
3151940	3155060	And so I give him my wish, but that wish has all these,
3155700	3159140	these underlying implications that I don't understand all these underlying
3159140	3159540	possibilities.
3159540	3164340	Yeah, but the, the cool thing, the moral of almost all those stories is
3165140	3170100	having unlimited wishes will be, lead to your downfall.
3170100	3175540	And so humans, like, if you give, you know, a young person an unlimited amount
3175540	3179460	of stuff to drink for, for six months, they're going to be falling down drunk
3179460	3181540	and they're going to get over it, right?
3181540	3184100	Having a friend that's always your friend, no matter what,
3184100	3185300	it's probably going to get boring.
3185300	3189460	Well, the, the, the literature on marital stability indicates that.
3189460	3194500	So there's a, there's a sweet spot with regards to marital stability
3194500	3197940	in terms of the ratio of negative to positive communication.
3198740	3204180	So if on average, you receive five positive communications
3204180	3207620	and one negative communication from your spouse,
3207620	3210100	that's on the low threshold for stability.
3210660	3214020	If it's four positive to one negative, you're headed for divorce.
3214020	3217940	But interestingly enough, on the other end, there's a threshold as well,
3217940	3222020	which is that if it exceeds 11 positive to one negative,
3222020	3223780	you're also moving towards divorce.
3224580	3229700	So there's, so, so, so there might be self-regulating mechanisms that,
3229700	3231780	that would in sense take care of that.
3231780	3237620	You might find a yes man AI friend extraordinarily boring, very, very rapidly.
3237620	3241940	But as opposed to an AI friend that was interested in what you were interested in,
3241940	3243060	it was actually interesting.
3243780	3246580	Like, you know, we go through friends in the course of our lives,
3246580	3249140	like different friends are interesting at different times.
3249140	3253060	And some friends we grow with, and that continues to be really interesting
3253060	3254020	for years and years.
3254020	3257220	And other friends, you know, some people get stuck in their thing
3257220	3259700	and then you've moved on or they've moved on or something.
3259700	3268020	So, yeah, I tend to think of a world where there was more abundance
3268020	3273860	and more possibilities and more interesting things to do is an interesting.
3273860	3274500	Okay, okay.
3274500	3277940	And modern society has let the human population,
3277940	3280100	and some people think this is a bad thing, but I don't know.
3280100	3282180	I'm a fan of it.
3282180	3286900	You know, modern population has gone from tens of 200 million to billions of people.
3287620	3289060	That's generally being a good thing.
3289060	3290500	We're not running out of space.
3290500	3294100	I've been in, you know, so some of your audience has probably been in an airplane.
3294100	3297140	If you look out the window, the country is actually mostly empty.
3297860	3299540	The oceans are mostly empty.
3299540	3304100	Like, we're, we're weirdly good at polluting large areas.
3304100	3307700	But as soon as we decide not to, we don't have to, like technology.
3307700	3311860	Most, most of our, you know, energy pollution problems are technical.
3312180	3313540	Like, we can stop polluting.
3313540	3314900	Like, electric cars are great.
3315700	3319620	So, so, so there's so many things that we could do technically.
3320660	3322500	I forget the guy's name.
3322500	3326020	He said that the earth could easily support a population of a trillion people.
3326900	3330020	And trillion people would be a lot more people doing, you know, random stuff.
3330660	3335460	And he didn't imagine that the future population would be a trillion humans and a trillion AIs,
3335460	3337300	but it probably will be.
3338660	3340980	So it will probably exist on multiple planets,
3340980	3343700	which will be good the next time an asteroid shows up.
3343700	3347620	So what do you think about, so, so one of the things that seems to be happening,
3347620	3349380	tell me if, if you think I'm wrong here.
3349380	3353380	I don't think it's germane to, and I just want to make the point of, you know,
3353380	3356820	where we are compared to living in the Middle Ages, our lives are longer,
3356820	3360260	our, our families are healthier, our children are more likely to survive.
3360900	3362900	Like many, many good things happened.
3363700	3366180	Like setting the clock back with and be good.
3366180	3370580	And, you know, if we have some care and people who actually care about
3370580	3373940	how culture interacts with technology for the next 50 years,
3374740	3379380	you know, we'll get through this, hopefully more successful than we did the atomic bomb in the Cold War.
3381140	3384020	But it's, it's a major change.
3384020	3390180	I mean, this is like, like your worries are, you know, I mean, they're, they're relevant.
3391060	3397700	But, you know, but also you're, Jonathan, your stories about how humans have faced abundance
3397700	3400660	and faced evil kings and evil overlords.
3400660	3405380	Like we have thousands of years of history of facing the challenge of the future
3405380	3408740	and the challenge of things that cause radical change.
3408740	3409220	Yeah.
3409220	3413540	And you know, that's, that's very valuable information.
3413540	3417540	But for the most part, nobody's succeeded by stopping change.
3417540	3425460	They've succeeded by bringing to bear on the change our capability to self-regulate the balance.
3426180	3429140	Like a good life isn't having as much gold as possible.
3429140	3430180	It's a boring life.
3430180	3434020	A good life is, you know, having some quality friends and doing what you want
3434020	3437300	and having some, some insight in life.
3437300	3438020	Yeah.
3438020	3439380	And some optimal challenge.
3440340	3446180	And, you know, and in a world where a larger percentage of people can have,
3447460	3451860	well, live in relative abundance and have tools and opportunities, I think is a good thing.
3451860	3452180	Yeah.
3452180	3454420	And I don't, I don't want to pull back abundance.
3454420	3462740	But what I have noticed is that, that our abundance brings a kind of nihilism to people.
3462740	3464500	And I don't, like I said, I don't want to go back.
3464500	3467380	I'm happy to live here and to have these, these tech things.
3467380	3474740	But I, but I think it's something that I've also noticed that increase of, of the capacity to
3476340	3483300	get your desires when that increases to a certain extent also leads to a kind of nihilism
3483300	3484660	where exactly that.
3484660	3488900	Well, I wonder, Jonathan, I wonder if that's partly, partly a consequence
3490020	3495780	of the erroneous maximization of short-term desire.
3496340	3501860	I mean, one of the things that you might think about that could be dangerous on the AI front is that
3502660	3509940	we optimize the manner in which we, we interact with our electronic gadgets to capture short-term
3509940	3511780	attention, right?
3511780	3516420	Because there's a difference between getting what you want right now, right now, and getting
3516420	3521140	what you need in some more mature sense across a reasonable span of time.
3521140	3525060	And one of the things that does seem to be happening online, and I think it is driven by
3525060	3532580	the development of AI systems, is that we're, we're assaulted by systems that parasitize our
3532580	3536660	short-term attention and at the expense of longer-term attention.
3536660	3543540	And if the AI systems emerge to optimize attentional grip, it isn't obvious to me that
3543540	3548100	they're going to optimize for the attention that works over the medium to long-run, right?
3548100	3552340	They're going to, they're going to be, they could conceivably maximize something like
3552340	3555140	whim-centered existence.
3555140	3557780	Yeah, because all of virality is based on that.
3557780	3562900	All the social media networks are all based on this, on this reduction, this reduction of
3562900	3567780	attention, this reduction of desire to, to reaching your, your rest, let's say, in that
3567780	3571140	desire, right? The like, the click, all these things, they're...
3571140	3572180	Yeah, now.
3572180	3572900	Yeah, exactly.
3572900	3577300	So, but, but that's something that, you know, so for reasons that are somewhat puzzling,
3577300	3583060	but maybe not, you know, the business models around a lot of those interfaces are around,
3583940	3590340	you know, the part, the users, the product, and, you know, the advertisers are trying to get
3590340	3590980	your attention.
3590980	3591780	Yeah, yeah.
3591780	3596260	But that's something culture could regulate. We could decide that, no, we don't, we don't
3596260	3600740	want tech platforms to be driven by advertising money. Like, that would be a smart decision,
3600740	3604020	probably. And that could be a big change.
3604660	3606340	And what would you see as an alternative?
3606340	3611300	See, well, the problem with that might be that markets drive that in some sense, right?
3611300	3611860	Yeah.
3611860	3613620	And I know they're driving that in a short-term way.
3613620	3617940	We can take steps, like, you know, at various times, you know, alcohol has been illegal.
3617940	3622900	Like, you can, society can decide to regulate all kinds of things.
3623940	3627460	And, you know, sometimes, some things need to be regulated and some things don't.
3627460	3631780	Like, when you buy a hammer, you don't fight with your hammer for its attention, right?
3631780	3634660	A hammer is a tool. You buy one when you need one.
3635540	3637700	Nobody's marketing hammers to you.
3637700	3643540	Like, like that, that has a relationship that's transactional to your purpose, right?
3643540	3644020	Yeah, well.
3644020	3647540	Our technology has become a thing where, I mean.
3647540	3654260	But there's a relationship between human, let's say, high human goals, something like
3654260	3660980	attention and status. And what we talked about, which is the idea of elevating something higher
3660980	3667220	in order to see it as a model. See, these are where intelligence exists in the human person.
3667220	3674260	So when we notice that in the systems, in the platforms, these are the aspects of intelligence
3674260	3680820	which are being weaponized in some ways, not against us, but are just kind of being weaponized
3680820	3685300	because they're the most beneficial at the short term to be able to generate our constant attention.
3685300	3690020	And so what I mean is that that is what the AIs are made of, right?
3690020	3695460	They're made of attention, prioritization, you know, good, bad.
3695460	3700580	What is it that is worth putting energy into in order to predict towards a telos?
3700580	3706900	And so I'm seeing that the idea that we could disconnect them suddenly seems very difficult to me.
3708260	3714340	Yeah, so I'll give you two. First, I want to give an old example. So after World War II,
3715140	3720740	America went through this amazing building boom of building suburbs. And the American dream was,
3720740	3725620	you could have your own house, your own yard in the suburb with a good school, right?
3725620	3731060	So in the 50s, 60s, early 70s, they were building like crazy. By the time I grew up,
3731060	3738420	I lived in a suburb in dystopia, right? And we found that that as a goal wasn't a good thing
3738420	3747380	because people ended up in houses separated from social structures and then new towns are built
3747380	3754580	around like a hub with places to go and eat. So there was a good that was viewed in terms of
3754580	3760820	opportunity and abundance, but it actually was a fail culturally. And then some places it modified
3760820	3767060	and continues and some places are still dystopian, you know, suburban areas and some places people
3767060	3773780	simply learn to live with it, right? So that has to do with attention, by the way. It has to do with
3774660	3781540	a subsidiary hierarchy, like a hierarchy of attention, which is set up in a way in which all
3781540	3788180	the levels can have room to exist, let's say. And so, you know, the new systems, the new way,
3788180	3793220	let's say the new urbanist movement, similar to what you're talking about, that's what they've
3793220	3798260	understood. It's like we need places of intimacy in terms of the house. We need places of communion
3798260	3805220	in terms of parks and alleyways and buildings where we meet and a church, all these places that
3805220	3812420	kind of manifest our community together. Yeah, so those existed coherently for long periods of time
3812420	3819540	and then the abundance post-World War II and some ideas about like what life could be like
3819540	3826260	caused this big change and that change satisfied some needs, people got houses, but broke community
3826260	3833220	needs and then new sets of ideas about what's the synthesis, what's the possibility of having your own
3833220	3839300	home, but also having community, not having to drive 15 minutes for every single thing and some
3839300	3844100	people live in those worlds and some people don't. Do you think we'll be smart? So one of the problems
3844100	3847060	is if we're there, we'll see. Well, why were we smart enough to solve some of those problems?
3847060	3851380	Because we had 20 years, but now, because one of the things that's happening now is we're,
3851380	3857940	as you pointed out earlier, is we're going to be producing equally revolutionary transformations,
3857940	3864260	but at a much smaller scale of time. And so, Mike, one of the things I wonder about, I think it's
3864260	3874500	driving some of the concerns in the conversation, is are we going to be intelligent enough to direct
3874500	3879700	with regulation the transformations of technology as they start to accelerate? I mean, we've already,
3879700	3886260	you look what's happened online, I mean, we've inadvertently, for example, radically magnified
3886260	3892420	the voices of narcissists, psychopaths, and Machiavellians. And we've done that so intensely,
3892420	3899140	partly, and I would say partly, as a consequence of AI mediation, that I think it's destabilizing
3899140	3904020	the entire world. It's destabilizing part of it, like Scott Adams pointed out. You just block
3904020	3907860	everybody that acts like that. I don't pay attention to people that talk like that.
3908500	3912420	Yeah, but they seem to be raising the temperature. Well, there are still places that are sensitive
3912420	3919460	to it. Like 10,000 people here can make a storm in some corporate person, fire somebody.
3919460	3924660	But I think that's like, we're five years from that being over. Corporation will go 10,000 people
3924660	3928820	out of 10 billion. Not a big deal. Okay, so you think at the moment that's a
3929460	3937380	learning moment that will re-regulate. What's natural to our children is so different than
3937380	3941620	was natural to us, but what was natural to us was very different from our parents.
3941620	3946020	So some changes get accepted generationally really fast.
3946020	3951060	So what's made you so optimistic? What do you mean optimistic?
3951700	3956340	Well, most of the things that you have said today, and maybe it's also because we're pushing you,
3956340	3962980	I mean, you really do. My nephew, Kyle, was a really smart, clever guy. He called me a
3963700	3969860	what did he call it, a cynical optimist. Like I believe in people.
3970980	3975460	Like I like people, but also people are complicated. They all got all kinds of nefarious goals.
3975460	3980660	Like I worry a lot more about people burning down the world than I do about artificial intelligence
3981380	3986500	just because, you know, people, well, you know, people, they're difficult.
3987220	3993140	Right. And, but the interesting thing is in aggregate, we mostly self-regulate.
3993140	3998100	And when things change, you have these dislocations. And then it's up to people who talk and think,
3998100	4003940	and while we're having this conversation, I suppose, to talk about how do we re-regulate this stuff.
4003940	4009540	Yeah. Well, because one of the things that the increase in power has done in terms of AI,
4009540	4014580	and you can see it with Google and you can see it online, is that there are certain people who
4014580	4020420	hold the keys, let's say, and then who hold the keys to what you see and what you don't see.
4020420	4024420	So you see that on Google, right? And you know it if you know what search is to make where you
4024420	4031140	realize that this is not, this is actually being directed by someone who now has huge amount of
4031140	4038900	power in order to direct my attention towards their ideological purpose. And so that's why,
4038980	4046980	like, I think that to me, I personally think it would, I always tend to see AI as an extension
4046980	4052500	of human power, even though there is this idea that it could somehow become totally independent.
4052500	4059460	I still tend to see it as an increase of the human care and whoever will be able to hold
4059460	4065380	the keys to that will have increase in power. And that can be like, and I think we're already
4065380	4070740	seeing it. Well, that's not really any different, though, is it, Jonathan, the situation that's
4070740	4075300	always confronted us in the past? I mean, we've always had to deal with the evil uncle of the
4075300	4081460	king, and we've always had to deal with the fact that an increase in ability could also produce
4081460	4087700	a commensurate increase in tyrannical power, right? I mean, so that might be magnified now,
4087700	4093700	and maybe the danger in some sense is more acute, but possibly the possibility is more
4093700	4100340	present as well. Well, because you can train an AI to find hate speech, right? You can train an AI
4100340	4106420	to find hate speech, and then to act on that hate speech immediately within, and now it's only,
4106420	4111220	we're not only talking about social media, but what we've seen is that that is now
4112740	4117700	encroaching into payment systems and into people losing their bank account, their access to
4117700	4123300	different services. And so this idea of optimization. Yeah, there's an Australian bank that already has
4123300	4128820	decided that it's a good thing to send all of their customers a carbon load report every month,
4129780	4136740	right? And to offer them hints about how they could reduce their polluting purchases, let's say.
4136740	4142260	And well, at the moment, that system is one of voluntary compliance, but you can certainly see
4142260	4148740	in a situation like the one we're in now that the line between voluntary compliance and involuntary
4148740	4156180	compulsion is very, very thin. Yeah, so I'd like to say, so during the early computer world,
4156180	4161060	computers were very big and expensive. And then they made many computers and workstations,
4161060	4165300	but they were still corporate only. And then the PC world came in. All of a sudden,
4165300	4170980	PCs put everybody online, everybody could suddenly see all kinds of stuff, and people
4170980	4176900	could get a Freedom of Information Act request, put it online somewhere, and 100,000 people could see
4176900	4185460	it. It was an amazing democratization moment. And then there was a similar, but smaller revolution
4185460	4193220	with the world of smartphones and apps. But then we've had a new completely different set of companies,
4193220	4199540	by the way, from what happened in the 60s, 70s, and 80s to today, it's very different companies that
4200100	4206100	control it. And there are people who are worried that AI will be a winner take all thing. Now,
4206100	4210180	I think so many people are using it, and they're working on it so many different places, and the
4210180	4216020	cost is going to come down so fast, that pretty soon you'll have your own AI app that you'll use to
4216020	4223940	mediate the internet to strip out the endless stream of ads. And you can say, well, is this story
4223940	4229300	objective? Well, here's the 15 stories, and this is being manipulated this way, and this is being
4229300	4235220	manipulated that way. And you can say, well, I want what's more like the real story. And the funny
4235380	4244100	thing is, information that's broadly distributed, and has lots of inputs, is very hard to fake the
4244100	4250260	whole thing. So right now, a story can pull through a major media outlet. And if they can control the
4250260	4257300	narrative, everybody gets to fake story. But if the media is distributed across a billion people,
4257940	4264260	who are all interacting in some useful way, somebody standing there, some, yeah, there's real
4264260	4267780	signal there, and if somebody stands up and says something that's not true, everybody goes,
4267780	4276740	everybody knows that's not true. So a good outcome with people thinking seriously would be the
4276740	4283060	democratization of information and objective facts in the same way. The same thing that happened with
4283060	4292500	PCs versus corporate central computers could happen again. The problem is that the increase in power,
4292580	4299460	the increase in power always creates the tooth at the same time. And so we saw that increase in
4299460	4304340	power creates first, or it depends in which direction it happens, it creates an increase in
4304340	4310100	decentralization, it increases in access, it creates all that. But then it also at the same time
4310100	4317380	creates the counter reaction, which is an increase in control and increase in centralization. And so
4317700	4326260	now, the more the power is, the more the waves will, the bigger the waves will be. And so the
4326260	4334180	image of the image that 1984 presented to us, of people going into newspapers and changing the
4335300	4339780	headlines and taking the pictures out and doing that, that now obviously can happen with just a
4339780	4344740	click. So you can click and you can change the past. You can change the past, you can change
4345300	4350660	facts about the world because they're all held online. And we've seen it happen obviously in
4350660	4357860	the media recently. So does decentralization win over centralization? How is that even possible,
4357860	4364580	it seems? I mean, and it's also interesting, when Amazon became a platform, suddenly any mom and
4364580	4372100	pop business could have Amazon, eBay, there's a bunch of platforms, which had an amazing impact
4373060	4378500	because any business could get to anybody. But then the platform itself started to control
4378500	4384980	the information flow. But at some point that will turn into people go, well, why am I letting
4384980	4391380	somebody control my information flow? And Amazon objectively doesn't really have any capability.
4393540	4398740	So like you point out, the waves are getting bigger, but they're real waves. It's the same with
4398740	4405620	information. Information is all online. It's also on a billion hard drives. So somebody says,
4405620	4410420	I'm going to raise the objective fact, a distributed information system would say,
4411140	4415300	go ahead and raise it anywhere you want. There's another 1,000 copies of it.
4416900	4424020	But again, this is where thinking people have to say, yeah, this is a serious problem.
4424580	4429300	Like if humans don't have anything to fight for, they get lazy and a little bit dopey,
4429300	4437940	in my view. We do have something to fight for. And that's worth talking about. What would a great
4437940	4444260	world with distributed, inhuman intelligence and artificial intelligence working together in a
4444260	4453540	collaborative way to create abundance and fairness and some better way at arriving at good decisions
4453540	4459220	than what the truth is. That would be a good thing. But it's not, well, we'll leave it to the
4459220	4463620	experts and then the experts will tell us what to do. That's a bad thing. So that's...
4464660	4468500	Well, so is it the model that you just laid out, which I think is very interesting?
4468500	4472660	I'm not so much optimistic about that. Well, it did happen on the computational front.
4473220	4480660	It happened a couple of times both directions. The PC revolution was amazing. And Microsoft
4480660	4487460	was a fantastic company. It enabled everybody to write a $10, $50 program to use. And then at
4487460	4493540	some point, they're also, let's say, a difficult program company. And they made money off a lot
4493540	4498020	of people and became extremely valuable. Now, for the most part, they haven't been that directional
4498020	4502980	on telling you what to do and think and how to do it. But they are a many-making company.
4504580	4508740	Apple created the App Store, which is great. But then they also take 30% of the App Store
4508740	4512980	profits and there's a whole section of the internet that's fighting with Apple about their
4512980	4519940	control of that platform. And in Europe, they've decided to regulate some of that,
4521140	4525940	that should be a conversation, that should be a social cultural conversation about how should
4525940	4536340	that work. So do you see the more likely, certainly the more desirable future is something like
4536420	4543780	a set of distributed AIs, many of which are under personal, in personal relationship in some sense,
4543780	4548100	the same way that we're in personal relationship with our phones and our computers. And that that
4548100	4553380	would give people the chance to fight back, so to speak against this. And there's lots of people
4553380	4558820	really interested in distributed platforms. And one of the interesting things about the AI world is,
4558820	4564100	you know, there's a company called OpenAI and they open source a lot of it. The AI research is
4564100	4569860	amazingly open. It's all done in public. People publish the new models all the time. You can try
4569860	4576020	them out. People, there's a lot of startups doing AI in all different kinds of places.
4577140	4584660	You know, it's a very curious phenomena. And it's kind of like a big, huge wave. It's not like a,
4585380	4590900	you can't stop a wave with your hand. Yeah. Well, when you think about the waves, there are two,
4590900	4596020	actually in the book of Revelation, which describes the end or describes the finality of all things
4596020	4600500	or the totality of all things is maybe a way for people who are more secular to kind of understand
4600500	4606740	it. And in that book, there are two images, interesting images about technology. One is
4606740	4612340	that there's a dragon that falls from the heavens and that dragon makes a beast. And then that beast
4612340	4619460	makes an image of the beast. And then the image speaks. And when the image speaks, then people
4619460	4627060	are so mesmerized by the speaking image that they worship the beast ultimately. So that is one
4627060	4632100	image of, let's say, making and technology and scripture in Revelation. But there's another
4632100	4637620	image, which is the image of the heavenly Jerusalem. And that image is more an image of balance. It's
4637620	4643380	an image of the city which comes down from heaven with a garden in the center and then becomes this
4643380	4650100	glorious city. And it says, the glory of all the kings is gathered into the city. So the glory of
4650100	4657300	all the nations is gathered into this city. So now you see a technology which is at the service of
4657300	4662500	human flourishing and takes the best of humans and brings it into itself in order to kind of
4662500	4666820	manifest. And it also has hierarchy, which means it has the natural at the center and then has the
4666820	4672020	artificial as serving the natural, you could say. So those two images seem to reflect these
4672900	4679620	two waves that we see. And this kind of idea of an artificial intelligence which will be ruling
4679620	4685300	over us or speaking over us. But there's a secret person controlling it, even in Revelation. It's
4685300	4691140	like, there's a beast controlling it and making it speak. So now we're mesmerized by it. And then
4691140	4695460	this other image. So I don't know, Jordan, if you ever thought about those two images in Revelation
4695460	4702660	as being related to technology, let's say. Well, I don't think I've thought about those two images
4702660	4708020	in the specific manner that you described. But I would say that the work that I've been doing,
4708020	4713540	and I think the work you've been doing too in the public front, reflects the dichotomy between
4713540	4718180	those images. And it's relevant to the points that Jim has been making. I mean, we are definitely
4718180	4722900	increasing our technological power. And you can imagine that that'll increase our capacity for
4722900	4728500	tyranny and also our capacity for abundance. And then the question becomes, what do we need to do
4728500	4733940	in order to increase the probability that we tilt the future towards Jerusalem and away from the
4733940	4740820	beast? And the reason that I've been concentrating on helping people bolster their individual
4740820	4746340	morality to the degree that I've managed that is because I think that whether the outcome is the
4746340	4751860	positive outcome, that in some sense Jim has been outlining or the negative outcomes that we've been
4751940	4756900	querying him about, I think that's going to be dependent on the individual ethical choices of
4756900	4761940	people at the individual level, but then cumulatively, right? So if we decide that we're
4761940	4766980	going to worship the image of the beast, so to speak, because we're mesmerized by our own reflection,
4766980	4771220	that's another way of thinking about it. And we want to be the victim of our own dark desires,
4771220	4778020	then the IA revolution is going to go very, very badly. But if we decide that we're going to aim up
4778020	4782500	in some positive way, and we make the right micro decisions, well, then maybe we can harness
4782500	4787620	this technology to produce a time of abundance in the manner that Jim is hopeful about.
4787620	4795220	Yeah. And let me make two funny points. So one is, I think there's going to be continuum,
4795220	4802340	like the word artificial intelligence won't actually make any sense. So humans collectively,
4802420	4808820	individuals know stuff, but collectively we know a lot more. And the thing that's really good is
4808820	4817140	in a diverse society with lots of people pursuing individual, interesting ideas, worlds, we have
4817140	4826740	a lot of things, and more people, more independence generates more diversity. And that's a good thing
4826740	4832500	where it's a totalitarian society where everybody's told to wear the same shirt. It's inherently
4832500	4842020	boring. The beast speaking through the monster is inherently dull. But in an intelligent world,
4842020	4849940	where not only can we have more intelligent things, but in some places go far beyond what most humans
4849940	4859380	are capable of in pursuit of interesting variety. And I believe the information, well,
4859380	4866260	let's say intelligence is essentially unlimited, right? And the unlimited intelligence won't be
4866260	4871780	this shiny thing that tells everybody what to do. That's sort of the opposite of interesting
4871780	4878580	intelligence. Interesting intelligence will be more diverse, not less diverse. That's a good future.
4880180	4885140	And your second description, that seems like a future we're working for and also we're fighting
4885140	4892660	for. And that means concrete things today. And also, it's a good conceptualization.
4894020	4898340	I see the messages as my kids are taught, don't have children and the world's going to end,
4898340	4903620	we're going to run out of everything, you're a bad person, why do you even exist? These messages
4903620	4909700	are terrible. The opposite is true. More people would be better. We live in a world
4910420	4917140	of potential abundance. It's right in front of us. There's so much energy available. It's just
4917140	4923460	amazing. It's possible to build technology without pollution consequences. That's called
4923460	4930500	externalizing costs. We know how to do that. We can have very good, clean technology. We can do
4930500	4937060	lots of interesting things. So if the goal is maximum diversity, then the line between human
4937060	4942900	intelligence, artificial intelligence that we draw, you'll see all these really interesting
4942900	4947140	partnerships and all kinds of things. And more people doing what they want, which is the world
4947140	4955060	I want to live in. To me, it seems like the question is going to be related to attention,
4955060	4960660	ultimately. That is, what are humans attending to at the highest? What is it that humans care for
4960660	4966740	in the highest? In some ways, you could say, what are humans worshiping? And
4966740	4972500	depending on what humans worship, then their actions will play out in the technology that
4972500	4977460	they're creating, in the increase in power that they're creating. And if we're guided by the
4977460	4982900	negative vision, the sort of thing that Jim laid out that is being talked to as children, you can
4982900	4987780	imagine that we're in for a pretty damn dismal future. Human beings are a cancer on the face of
4987780	4992820	the planet. There's too many of us. We have to accept top-down, compelled limits to growth.
4992900	4997780	There's not enough for everybody. A bunch of us have to go because there's too many people on
4997780	5004340	the planet. We have to raise up the price of energy so that we don't burn the planet up with
5004340	5011460	carbon dioxide pollution, et cetera. It's a pretty damn dismal view of the potential that's in front
5011460	5019140	of us. The world should be exciting and the future should be exciting. Well, we've been sitting here
5019140	5025380	for about 90 minutes, bandying back and forth both visions of abundance and visions of apocalypse.
5027460	5032180	I've been heartened, I would say, over the decades talking to Jim about what he's doing
5032180	5035700	on the technological front. And I think part of the reason I've been heartened is because
5036340	5043140	I do think that his vision is guided primarily by desire to help bring about something
5043140	5047540	approximating life more abundant. And I would rather see people on the AI front who are guided
5047540	5052740	by that vision working on this technology. But I also think it's useful to do what you
5052740	5058500	and I have been doing in this conversation, Jonathan, and acting in some sense as friendly
5058500	5063380	critics and hopefully learning something in the interim. Do you have anything you want to say
5063380	5069460	in conclusion? I just think that the question is linked very directly to what we've been talking
5069460	5075220	about now for several years, which is the question of attention, the question of what is the highest
5075220	5080660	attention. And I think the reason why I have more alarm, let's say, than Jim, is that I've
5080660	5087540	noticed that in some ways human beings have come to now, let's say, worship their own desires,
5087540	5093300	they've come to worship. And that even the strange thing of worshiping their own desires has actually
5093300	5098820	led to an anti-human narrative. This is a weird idea. It's almost suicidal desire that humans
5098820	5103060	have. And so I think that seeing all of that together in the increase of power,
5104260	5111060	I do worry that the image of the beast is closer to what will manifest itself. And I feel like
5111060	5118820	during COVID, that sense in me was accelerated tenfold in noticing to what extent technology
5118820	5125220	was used, especially in Canada, how technology was used to instigate something which looked like
5125300	5130180	authoritarian systems. And so I am worried about it. But I think like Jim, honestly,
5130180	5134420	although I say that, I do believe that in the end, truth wins. I do believe that in the end,
5135620	5143380	these things will level themselves out. But I think that because I see people rushing towards
5143380	5150900	AI almost like lemmings are going off a cliff, I feel like it is important to sound the alarm once
5150900	5157540	in a while and say, you know, we need to orient our desire before we go towards this extreme power.
5157540	5161780	So I think that that's mostly the thing that worries me the most and that preoccupies me the
5161780	5166580	most. But I think that ultimately in the end, I do share Jim's positive vision. And I do think that
5167380	5172100	I do believe the story has a happy ending. It's just you might have to go through hell before
5172100	5178820	we get there. I hope not. So Jim, how about you? What have you got to say in closing?
5178820	5183940	A couple of years ago, a friend who's, you know, my age said, oh, kids coming out of college,
5183940	5187620	they don't know anything anymore. They're lazy. And I thought, I work at Tesla. I was working
5187620	5193220	at Tesla at the time. And we hired kids out of college and they couldn't wait to make things.
5193940	5200340	They were like, it's a hands-on place. It's a great place. And I've told people, like, if you're
5200340	5204820	not in a place where you're doing stuff, it's growing, it's making things, you need to go somewhere
5204820	5212340	else. And also, I think you're right, the mindset of if people are feeling this is a productive,
5212340	5217380	creative technology that's really cool, they're going to go build cool stuff. And if they think
5217380	5221860	it's a shitty job and they're just tuning the algorithm so they can get more clicks,
5221860	5229540	they're going to make something beastly, you know, beastly, perhaps. And the stories, you know,
5229620	5236580	our cultural tradition is super useful, both cautionary and, you know, explanatory about
5236580	5242660	something good. Like, and I think it's up to us to go do something about this. And I know people
5242660	5247700	are working really hard to make, you know, the Internet a more open place to make sure information
5247700	5255060	is distributed, to make sure AI isn't a winter take-all thing. Like, these are real things and
5255060	5260420	people should be talking about them. And then they should be worrying. But the upside's really high.
5261060	5267380	And we faced these kind of technological, like, this is a big change. Like, AI is bigger than
5267380	5273940	the Internet. Like I've said, this publicly, like, the Internet was pretty big. And, you know,
5273940	5282500	this is bigger. It's true. But the possibilities are amazing. And so with some sense, we could
5282820	5288820	utilize them. Yeah, with some sense, we could achieve it. And the world is interesting.
5288820	5293460	Like, I think it'll be a more interesting place. Well, that's an extraordinarily
5293460	5300020	cynically optimistic place to end. I'd like to thank everybody who is watching and listening.
5300020	5304580	And thank you, Jonathan, for participating in the conversation. It's much appreciated as always.
5304580	5309860	I'm going to talk to Jim Keller for another half an hour on the Daily Wire Plus platform. I
5310500	5315220	use that extra half an hour to usually walk people through their biography. I'm very interested in
5315220	5321780	how people develop successful careers and lives and how their destiny unfolded in front of them.
5321780	5326980	And so for all of those of you who are watching and listening, who might be interested in that,
5326980	5331940	consider heading over to the Daily Wire Plus platform and partaking in that. And otherwise,
5331940	5337540	Jonathan, we'll see you in Miami in a month and a half to finish up the Exodus seminar.
5337540	5344500	We're going to release the first half of the Exodus seminar we recorded in Miami on November 25th,
5344500	5348420	by the way. So that looks like it's in the can. Yeah, I can't wait to see it.
5348420	5353940	The rest of you? Yeah. Yeah, absolutely. I'm really excited about it. And just for everyone
5353940	5359220	watching and listening, I brought a group of scholars together. About two and a half months
5359220	5363620	ago, we spent a week in Miami, some of the smartest people I could gather around me,
5363700	5368580	to walk through the book of Exodus. We only got through halfway, because it turns out there's
5368580	5373140	more information there than I had originally considered. But it went exceptionally well,
5373140	5380340	and I learned a lot. And Exodus means ex-hodos. That means the way forward. And well, that's very
5380340	5385540	much relevant to everyone today as we strive to find our way forward through all these complex
5385540	5390180	issues, such as the ones we were talking about today. So I would also encourage people to check
5390180	5394660	that out when it launches on November 25th. I learned more in that seminar than any seminar
5394660	5398580	I ever took in my life, I would say. So it was good to see you there. We'll see you in a month
5398580	5402580	and a half. Jim, we're going to talk a little bit more on the Daily Wear a Plus platform. And
5402580	5407780	I'm looking forward to meeting the rest of the people in your AI-oriented community tomorrow
5407780	5413140	and learning more about, well, what seems to be an optimistic version of a life more abundant.
5413140	5418020	And to all of you watching and listening, thank you very much. Your attention isn't taken for
5418020	5423060	granted, and it's much appreciated. Hello, everyone. I would encourage you to continue
5423060	5428980	listening to my conversation with my guest on DailyWirePlus.com.
