1
00:00:00,000 --> 00:00:06,420
It's the Hebrews created history as we know it.

2
00:00:06,420 --> 00:00:10,500
You don't get away with anything and so you might think you can bend the fabric of reality

3
00:00:10,500 --> 00:00:14,500
and that you can treat people instrumentally and that you can bow to the tyrant and violate

4
00:00:14,500 --> 00:00:16,120
your conscience without cost.

5
00:00:16,120 --> 00:00:17,420
You will pay the piper.

6
00:00:17,420 --> 00:00:22,100
It's going to call you out of that slavery into freedom even if that pulls you into the

7
00:00:22,100 --> 00:00:23,100
desert.

8
00:00:24,100 --> 00:00:30,700
And we're going to see that there's something else going on here that is far more cosmic

9
00:00:30,700 --> 00:00:33,180
and deeper than what you can imagine.

10
00:00:33,180 --> 00:00:41,460
The highest ethical spirit to which we're beholden is presented precisely as that spirit

11
00:00:41,460 --> 00:00:45,940
that allies itself with the cause of freedom against tyranny.

12
00:00:45,940 --> 00:00:46,940
Yes, exactly.

13
00:00:46,940 --> 00:00:48,460
I want villains to get punished.

14
00:00:48,460 --> 00:00:52,140
But do you want the villains to learn before they have to pay the ultimate price?

15
00:00:52,140 --> 00:00:54,140
That's such a Christian question.

16
00:01:05,140 --> 00:01:07,060
That has to do with attention by the way.

17
00:01:07,060 --> 00:01:12,980
It has to do with a subsidiary hierarchy, like a hierarchy of attention which is set

18
00:01:12,980 --> 00:01:18,980
up in a way in which all the levels can have room to exist, let's say.

19
00:01:18,980 --> 00:01:25,540
And so these new systems, the new way, let's say the new urbanist movement, similar to

20
00:01:25,540 --> 00:01:27,820
what you're talking about, that's what they've understood.

21
00:01:27,820 --> 00:01:30,660
It's like we need places of intimacy in terms of the house.

22
00:01:30,660 --> 00:01:36,900
We need places of communion in terms of parks and alleyways and buildings where we meet

23
00:01:36,900 --> 00:01:42,740
and a church, all these places that kind of manifest our community together.

24
00:01:42,740 --> 00:01:48,780
So those existed coherently for long periods of time and then the abundance post-World

25
00:01:48,780 --> 00:01:55,660
War II and some ideas about what life could be like causes big change.

26
00:01:55,660 --> 00:02:01,180
And that change satisfied some needs, people got houses, but broke community needs.

27
00:02:01,180 --> 00:02:06,820
And then new sets of ideas about what's the synthesis, what's the possibility of having

28
00:02:06,820 --> 00:02:12,580
your own home but also having community, not having to drive 15 minutes for every single

29
00:02:12,580 --> 00:02:15,780
thing and some people live in those worlds and some people don't.

30
00:02:15,780 --> 00:02:17,380
Do you think we'll be smart?

31
00:02:17,380 --> 00:02:21,340
So one of the problems is why were we smart enough to solve some of those problems?

32
00:02:21,340 --> 00:02:25,620
Because we had 20 years, but now, because one of the things that's happening now, as

33
00:02:25,620 --> 00:02:31,900
you pointed out earlier, is we're going to be producing equally revolutionary transformations

34
00:02:31,900 --> 00:02:35,820
but at a much smaller scale of time.

35
00:02:35,820 --> 00:02:40,580
What's natural to our children is so different than what's natural to us, but what's natural

36
00:02:40,580 --> 00:02:43,020
to us is very different from our parents.

37
00:02:43,020 --> 00:02:47,460
So some changes get accepted generationally really fast.

38
00:02:47,460 --> 00:02:49,940
So what's made you so optimistic?

39
00:03:03,940 --> 00:03:08,780
Hello everyone watching on YouTube or listening on associated platforms.

40
00:03:08,780 --> 00:03:15,900
So I'm very excited today to be bringing you two of the people I admire most intellectually,

41
00:03:15,900 --> 00:03:22,780
I would say, and morally for that matter, Jonathan Pazio and Jim Keller, very different

42
00:03:22,780 --> 00:03:23,780
thinkers.

43
00:03:23,780 --> 00:03:29,500
Jonathan Pazio is a French-Canadian liturgical artist and icon carver known for his work featured

44
00:03:29,500 --> 00:03:31,460
in museums across the world.

45
00:03:31,460 --> 00:03:37,580
He carves Eastern Orthodox among other traditional images and teaches an online carving class.

46
00:03:37,580 --> 00:03:43,180
He also runs a YouTube channel, This Symbolic World, dedicated to the exploration of symbolism

47
00:03:43,180 --> 00:03:44,780
across history and religion.

48
00:03:44,780 --> 00:03:48,020
Jonathan is one of the deepest religious thinkers I've ever met.

49
00:03:48,020 --> 00:03:55,340
Jim Keller is a microprocessor engineer known very well in the relevant communities and

50
00:03:55,340 --> 00:03:59,780
beyond them for his work at Apple and AMD, among other corporations.

51
00:03:59,780 --> 00:04:05,460
He served in the role of architect for numerous game-changing processors, has co-authored

52
00:04:05,460 --> 00:04:12,300
multiple instruction sets for highly complicated designs, and is credited for being the key

53
00:04:12,300 --> 00:04:20,020
player behind AMD's renewed ability to compete with Intel in the high-end CPU market.

54
00:04:20,020 --> 00:04:27,020
In 2016, Keller joined Tesla, becoming vice president of autopilot hardware engineering.

55
00:04:27,020 --> 00:04:32,180
In 2018, he became a senior vice president for Intel.

56
00:04:32,180 --> 00:04:37,260
In 2020, he resigned due to disagreements over outsourcing production, but quickly found

57
00:04:37,260 --> 00:04:42,300
a new position at TENS Torrent as chief technical officer.

58
00:04:42,300 --> 00:04:47,020
We're going to sit today and discuss the perils and promise of artificial intelligence, and

59
00:04:47,020 --> 00:04:50,020
it's a conversation I'm very much looking forward to.

60
00:04:50,020 --> 00:04:53,420
So welcome to all of you watching and listening.

61
00:04:53,420 --> 00:04:57,700
I thought it would be interesting to have a three-way conversation.

62
00:04:57,700 --> 00:05:01,900
Jonathan and I have been talking a lot lately, especially with John Verveke and some other

63
00:05:01,900 --> 00:05:09,260
people as well, about the fact that it seems necessary for us to view for human beings

64
00:05:09,260 --> 00:05:11,020
to view the world through a story.

65
00:05:11,020 --> 00:05:20,380
In fact, when we describe the structure that governs our action and our perception, that

66
00:05:20,380 --> 00:05:22,060
is a story.

67
00:05:22,060 --> 00:05:25,940
And so we've been trying to puzzle out, I would say, to some degree on the religious

68
00:05:26,020 --> 00:05:30,060
front, what might be the deepest stories.

69
00:05:30,060 --> 00:05:34,860
And I'm very curious about the fact that we perceive the world through a story, human

70
00:05:34,860 --> 00:05:36,060
beings do.

71
00:05:36,060 --> 00:05:41,340
And that seems to be a fundamental part of our cognitive architecture and of cognitive

72
00:05:41,340 --> 00:05:45,460
architecture in general, according to some of the world's top neuroscientists.

73
00:05:45,460 --> 00:05:50,860
And I'm curious, and I know Jim is interested in cognitive processing and in building systems

74
00:05:50,860 --> 00:05:57,180
that in some sense seem to run in a manner analogous to the manner in which our brains

75
00:05:57,180 --> 00:05:58,180
run.

76
00:05:58,180 --> 00:06:02,180
And so I'm curious about the overlap between the notion that we have to view the world

77
00:06:02,180 --> 00:06:04,700
through a story and what's happening on the AI front.

78
00:06:04,700 --> 00:06:07,780
There's all sorts of other places that we can take the conversation.

79
00:06:07,780 --> 00:06:09,500
So maybe I'll start with you, Jim.

80
00:06:09,500 --> 00:06:14,900
Do you want to tell people what you've been working on and maybe give a bit of a background

81
00:06:14,900 --> 00:06:19,580
to everyone about how you conceptualize artificial intelligence?

82
00:06:20,460 --> 00:06:21,020
Yeah, sure.

83
00:06:21,020 --> 00:06:28,540
So first, I'll say technically, I'm not an artificial intelligent researcher.

84
00:06:28,540 --> 00:06:30,860
I'm a computer architect.

85
00:06:30,860 --> 00:06:37,100
And I'd say my skill set goes from somewhere around the atom up to the program.

86
00:06:37,100 --> 00:06:42,940
So we make transistors out of atoms, we make logical gates out of transistors, we make

87
00:06:42,940 --> 00:06:45,180
computers out of logical gates.

88
00:06:45,180 --> 00:06:47,860
We run programs on those.

89
00:06:47,860 --> 00:06:55,100
And recently, we've been able to run programs fast enough to do something called an artificial

90
00:06:55,100 --> 00:07:01,260
intelligence model or neural network, depending on how you say it.

91
00:07:01,260 --> 00:07:08,100
And then we're building chips now that run artificial intelligence models fast.

92
00:07:08,100 --> 00:07:12,060
And we have a novel way to do it, the company I work at.

93
00:07:12,060 --> 00:07:13,900
But lots of people are working on it.

94
00:07:13,900 --> 00:07:21,260
And I think we were sort of taken by surprise what's happened in the last five years, how

95
00:07:21,260 --> 00:07:31,420
quickly models started to do interesting and intelligent seeming things.

96
00:07:31,420 --> 00:07:36,260
There's been an estimate that human brains do about 10 to the 18th operations a second.

97
00:07:36,260 --> 00:07:37,260
It sounds like a lot.

98
00:07:37,260 --> 00:07:40,940
It's a billion, billion operations a second.

99
00:07:40,940 --> 00:07:48,820
And a little computer processor in your phone probably does 10 billion operations a second.

100
00:07:48,820 --> 00:07:53,900
And then if you use the GPU, maybe 100 billion, something like that.

101
00:07:53,900 --> 00:08:01,540
And big modern AI computers like OpenAI use this or Google or somebody, they're doing

102
00:08:01,540 --> 00:08:06,180
like 10 to the 16th, maybe slightly more operations a second.

103
00:08:06,180 --> 00:08:12,580
So they're within a factor of 100 of a human brain's raw computational ability.

104
00:08:12,580 --> 00:08:14,060
And by the way, that could be completely wrong.

105
00:08:14,060 --> 00:08:17,100
Our understanding of how the human brain does computation could be wrong.

106
00:08:17,100 --> 00:08:21,940
But lots of people have estimated, based on number of neurons, number of connections,

107
00:08:21,940 --> 00:08:27,540
how fast neurons fire, how many operations a neuron firing seems to involve.

108
00:08:27,540 --> 00:08:34,020
I mean, the estimates range by a couple orders of magnitude, but when our computers got fast

109
00:08:34,020 --> 00:08:40,100
enough, we started to build things called language models and image models that do fairly

110
00:08:40,100 --> 00:08:42,180
remarkable things.

111
00:08:42,180 --> 00:08:46,460
So what have you seen in the last few years that's been indicative of this, of the change

112
00:08:46,460 --> 00:08:48,500
that you described as revolutionary?

113
00:08:48,500 --> 00:08:54,580
What are computers doing now that you found surprising because of this increase in speed?

114
00:08:54,580 --> 00:09:01,880
Yeah, you can have a language model read a 200,000 word book and summarize it fairly accurately.

115
00:09:01,880 --> 00:09:03,580
So it can extract out the gist?

116
00:09:03,580 --> 00:09:04,940
The gist of it.

117
00:09:04,940 --> 00:09:06,380
Can they do that with fiction?

118
00:09:06,380 --> 00:09:07,380
Yeah.

119
00:09:07,380 --> 00:09:11,980
Yeah, and I'm going to introduce you to a friend who took a language model and changed

120
00:09:11,980 --> 00:09:18,700
it and fine tuned it with Shakespeare and use it to write screenplays that are pretty

121
00:09:18,700 --> 00:09:20,700
good.

122
00:09:20,700 --> 00:09:23,380
And these kinds of things are really interesting.

123
00:09:23,380 --> 00:09:27,060
And then we were talking about this a little bit earlier.

124
00:09:27,060 --> 00:09:35,180
So when computers do computations, a program will say add A equal B plus C. The computer

125
00:09:35,180 --> 00:09:39,900
does those operations on representations of information, ones and zeros.

126
00:09:39,900 --> 00:09:42,180
It doesn't understand them at all.

127
00:09:42,180 --> 00:09:45,780
The computer has no understanding of it.

128
00:09:45,780 --> 00:09:52,700
But what we call a language model translates information like words and images and ideas

129
00:09:52,700 --> 00:09:58,380
into a space where the program, the ideas and the operation it does on them are all

130
00:09:58,380 --> 00:10:01,780
essentially the same thing.

131
00:10:01,780 --> 00:10:04,700
We'll be right back with Jonathan Pageau and Jim Keller.

132
00:10:04,700 --> 00:10:11,700
First we wanted to give you a sneak peek at Jordan's new documentary, Logos in Literacy.

133
00:10:11,700 --> 00:10:18,140
I was very much struck by how the translation of the biblical writings jump-started the

134
00:10:18,140 --> 00:10:21,420
development of literacy across the entire world.

135
00:10:21,420 --> 00:10:23,340
Literacy was the norm.

136
00:10:23,340 --> 00:10:29,060
The pastor's home was the first school and every morning it would begin with singing.

137
00:10:29,060 --> 00:10:32,260
The Christian faith is a singing religion.

138
00:10:32,260 --> 00:10:37,220
Probably 80% of scripture memorization today exists only because of what is sung.

139
00:10:37,220 --> 00:10:38,220
This is amazing.

140
00:10:38,220 --> 00:10:44,780
Here we have a Gutenberg Bible printed on the press of Johann Gutenberg.

141
00:10:44,780 --> 00:10:50,540
Science and religion are opposing forces in the world, but historically that has not

142
00:10:50,660 --> 00:10:51,660
been the case.

143
00:10:51,660 --> 00:10:53,980
Now the book is available to everyone.

144
00:10:53,980 --> 00:11:02,780
From Shakespeare to modern education and medicine and science to civilization itself.

145
00:11:02,780 --> 00:11:08,500
It is the most influential book in all of history and hopefully people can walk away

146
00:11:08,500 --> 00:11:11,940
with at least a sense of that.

147
00:11:11,940 --> 00:11:22,540
A language model can produce words and then use those words as inputs.

148
00:11:22,540 --> 00:11:27,340
It seems to have an understanding of what those words are, which is very different from

149
00:11:27,340 --> 00:11:29,980
how a computer operates on data.

150
00:11:29,980 --> 00:11:37,180
About the language models, I mean my sense of at least in part how we understand a story

151
00:11:37,220 --> 00:11:44,380
is that maybe we're watching a movie, let's say, and we get some sense of the character's

152
00:11:44,380 --> 00:11:50,900
goals and then we see the manner in which that character perceives the world and we in

153
00:11:50,900 --> 00:11:54,580
some sense adopt his goals, which is to identify with character.

154
00:11:54,580 --> 00:12:00,060
Then we play out a panoply of emotions and motivations on our body because we now inhabit

155
00:12:00,060 --> 00:12:06,980
that goal space and we understand the character as a consequence of mimicking the character

156
00:12:06,980 --> 00:12:10,740
with our own physiology.

157
00:12:10,740 --> 00:12:15,100
You have computers that can summarize the gist of a story, but they don't have that underlying

158
00:12:15,100 --> 00:12:16,100
physiology.

159
00:12:16,100 --> 00:12:21,940
First of all, it's a theory that your physiology has anything to do with it.

160
00:12:21,940 --> 00:12:27,740
You could understand the character's goals and then get involved in the details of the

161
00:12:27,740 --> 00:12:28,740
story.

162
00:12:28,740 --> 00:12:34,140
Then you're predicting the path of the story and also having expectations and hopes for

163
00:12:34,140 --> 00:12:35,140
the story.

164
00:12:35,140 --> 00:12:41,700
A good story kind of takes you on a ride because it teases you with doing some of the things

165
00:12:41,700 --> 00:12:46,140
you expect, but also doing things that are unexpected and possibly that creates an emotional

166
00:12:46,140 --> 00:12:47,140
...

167
00:12:47,140 --> 00:12:48,140
It does.

168
00:12:48,140 --> 00:12:49,140
It does.

169
00:12:49,140 --> 00:12:53,580
In an AI model, you can easily have a set of goals.

170
00:12:53,580 --> 00:12:57,220
You have your personal goals and then when you watch the story, you have those goals.

171
00:12:57,220 --> 00:12:59,460
You put those together.

172
00:12:59,460 --> 00:13:01,020
How many goals is that?

173
00:13:01,020 --> 00:13:06,580
The story's goals and your goals, hundreds, thousands, those are small numbers.

174
00:13:06,580 --> 00:13:08,300
Then you have the story.

175
00:13:08,300 --> 00:13:13,420
The AI model can predict the story too, just as well as you can.

176
00:13:13,420 --> 00:13:15,740
That's the thing that I find mysterious is that ...

177
00:13:15,740 --> 00:13:21,300
As the story progresses, it can look at the error between what it predicted and what actually

178
00:13:21,300 --> 00:13:25,980
happened and then iterate on that.

179
00:13:25,980 --> 00:13:29,140
You would call that emotional, excitement, disappointment ...

180
00:13:29,140 --> 00:13:30,140
Anxiety.

181
00:13:30,220 --> 00:13:31,220
Anxiety.

182
00:13:31,220 --> 00:13:32,220
Yeah, definitely.

183
00:13:32,220 --> 00:13:35,780
A big part of what anxiety does seem to be is discrepancy.

184
00:13:35,780 --> 00:13:39,020
Some of those states are manifesting your body because you trigger hormone cascades

185
00:13:39,020 --> 00:13:43,740
and a bunch of stuff, but you also can just scan your brain and see that stuff move around.

186
00:13:43,740 --> 00:13:44,740
Right.

187
00:13:44,740 --> 00:13:45,740
Right.

188
00:13:45,740 --> 00:13:51,500
The AI model can have an error function and look at the difference between what it expected

189
00:13:51,500 --> 00:13:55,940
and not, and you could call that the emotional state if you want it.

190
00:13:55,940 --> 00:13:56,940
I just talked with the ...

191
00:13:56,940 --> 00:13:57,940
That's speculation.

192
00:13:57,940 --> 00:13:58,940
No, no.

193
00:13:58,940 --> 00:14:00,340
That's accurate.

194
00:14:00,340 --> 00:14:04,100
We can make an AI model that could predict the result of a story probably better than

195
00:14:04,100 --> 00:14:07,100
the average person.

196
00:14:07,100 --> 00:14:08,100
One of the things ...

197
00:14:08,100 --> 00:14:11,500
Some people are really good at ... They're really well educated about stories or they

198
00:14:11,500 --> 00:14:17,820
know the genre or something, but these things ... What they see today as the capacity of

199
00:14:17,820 --> 00:14:22,020
the models is, if you say start describing a lot, it will make sense for a while, but

200
00:14:22,020 --> 00:14:25,780
it will slowly stop making sense.

201
00:14:25,780 --> 00:14:26,780
That's possible.

202
00:14:26,820 --> 00:14:32,060
It's simply the capacity of the model right now, and the model is not well grounded enough

203
00:14:32,060 --> 00:14:36,220
in a set of goals and reality or something to make sense for a while.

204
00:14:36,220 --> 00:14:37,700
What do you think would happen, Jonathan?

205
00:14:37,700 --> 00:14:43,300
This is, I think, associated with the kind of things that we've talked through to some

206
00:14:43,300 --> 00:14:44,800
degree.

207
00:14:44,800 --> 00:14:55,420
One of my hypotheses, let's say, about deep stories is that they're metagists in some

208
00:14:55,420 --> 00:14:56,420
sense.

209
00:14:56,420 --> 00:15:01,740
You could imagine 100 people telling you a tragic story, and then you could reduce each

210
00:15:01,740 --> 00:15:05,940
of those tragic stories to the gist of the tragic story, and then you could aggregate

211
00:15:05,940 --> 00:15:09,100
the gists, and then you'd have something like a metatragity.

212
00:15:09,100 --> 00:15:15,540
I would say the deeper the gist, the more religious-like the story gets.

213
00:15:15,540 --> 00:15:19,420
That's part of ... It's that idea as part of the reason that I wanted to bring you guys

214
00:15:19,420 --> 00:15:20,420
together.

215
00:15:20,420 --> 00:15:25,180
One of the things that what you just said makes me wonder is, imagine that you took Shakespeare

216
00:15:26,140 --> 00:15:34,500
and you took Dante, and you took the canonical Western writers, and you trained an AI system

217
00:15:34,500 --> 00:15:40,820
to understand the structure of each of them, and then now you could pull out the summaries

218
00:15:40,820 --> 00:15:47,380
of those structures, the gists, and then couldn't you pull out another gist out of that?

219
00:15:47,380 --> 00:15:52,140
So it would be like the essential element of Dante and Shakespeare, and I wanted to

220
00:15:52,300 --> 00:15:53,300
get biblical.

221
00:15:53,300 --> 00:15:58,180
Jonathan said so far, and then ... So here's one funny thing to think about.

222
00:15:58,180 --> 00:15:59,660
You use the word pull out.

223
00:15:59,660 --> 00:16:05,780
So when you train a model to know something, you can't just look in it and say, what is

224
00:16:05,780 --> 00:16:06,780
it?

225
00:16:06,780 --> 00:16:07,780
No.

226
00:16:07,780 --> 00:16:08,780
You have to quarry it.

227
00:16:08,780 --> 00:16:09,780
Right.

228
00:16:09,780 --> 00:16:10,780
You have to ask.

229
00:16:10,780 --> 00:16:11,780
Right.

230
00:16:11,780 --> 00:16:12,780
Right.

231
00:16:12,780 --> 00:16:13,780
What's the next sentence in this paragraph?

232
00:16:13,780 --> 00:16:16,020
What's the answer to this question?

233
00:16:16,020 --> 00:16:20,780
There's the thing on the internet now called prompt engineering, and it's the same way.

234
00:16:20,820 --> 00:16:23,140
I can't look in your brain to see what you think.

235
00:16:23,140 --> 00:16:24,140
Yeah.

236
00:16:24,140 --> 00:16:27,420
I have to ask you what you think, because if I killed you and scanned your brain and

237
00:16:27,420 --> 00:16:32,220
got the current state of all the synapses and stuff, A, you'd be dead, which should

238
00:16:32,220 --> 00:16:35,860
be sad, and B, I wouldn't know anything about your thoughts.

239
00:16:35,860 --> 00:16:42,460
Your thoughts are embedded in this model that your brain carries around, and you can express

240
00:16:42,460 --> 00:16:44,620
it in a lot of ways.

241
00:16:44,620 --> 00:16:45,620
And so ...

242
00:16:45,620 --> 00:16:46,620
So you could add ...

243
00:16:46,620 --> 00:16:50,500
How do you train ... This is my big question is ... I mean, because the way that I've been

244
00:16:50,540 --> 00:16:57,100
seeing it until now is that artificial intelligence, it's based on us.

245
00:16:57,100 --> 00:17:01,340
It doesn't exist independently from humans, and it doesn't have care.

246
00:17:01,340 --> 00:17:04,460
The question would be, why does the computer care?

247
00:17:04,460 --> 00:17:06,900
Yeah, that's not true.

248
00:17:06,900 --> 00:17:09,980
Why does the computer care to get the gist of the story?

249
00:17:09,980 --> 00:17:13,740
Well, yeah, so I think you're asking kind of the wrong question.

250
00:17:13,740 --> 00:17:19,220
So you can train an AI model on the physics and reality and images in the world, just

251
00:17:19,260 --> 00:17:21,260
with images.

252
00:17:21,260 --> 00:17:26,860
And there are people who are figuring out how to train a model with just images, but

253
00:17:26,860 --> 00:17:33,780
the model itself still conceptualizes things like tree and dog and action and run, because

254
00:17:33,780 --> 00:17:37,740
those all exist in the world.

255
00:17:37,740 --> 00:17:43,820
So ... And you can actually train ... And when you train a model with all the language

256
00:17:43,900 --> 00:17:49,780
and words, so all information has structure, and I know you're a structure guy from your

257
00:17:49,780 --> 00:17:50,780
video.

258
00:17:50,780 --> 00:17:55,580
So if you look around you at any image, every single point you see makes sense.

259
00:17:55,580 --> 00:17:56,580
Yeah.

260
00:17:56,580 --> 00:17:57,580
Right?

261
00:17:57,580 --> 00:17:59,740
It's a teleological structure.

262
00:17:59,740 --> 00:18:03,500
It's a purpose laid in structure, right?

263
00:18:03,500 --> 00:18:04,500
So this is something we talk about.

264
00:18:04,500 --> 00:18:09,580
Yeah, so it turns out all the words that have ever been spoken by human beings also have

265
00:18:09,580 --> 00:18:10,580
structure.

266
00:18:10,580 --> 00:18:11,580
Right.

267
00:18:11,580 --> 00:18:12,580
Right.

268
00:18:13,580 --> 00:18:19,340
And so physics has structure, and it turns out that some of the deep structure of images

269
00:18:19,340 --> 00:18:23,580
and actions and words and sentences are related.

270
00:18:23,580 --> 00:18:24,580
Mm-hmm.

271
00:18:24,580 --> 00:18:33,180
Like, there's actually a common core of ... Imagine there's like a knowledge space, and sure there's

272
00:18:33,180 --> 00:18:38,540
details of humanity where they prefer this accent versus that.

273
00:18:38,540 --> 00:18:42,140
Those are kind of details, but they're coherent in the language model.

274
00:18:42,140 --> 00:18:47,300
But the language models themselves are coherent with our world ideas, and humans are trained

275
00:18:47,300 --> 00:18:52,660
in the world just the way the AI models are trained in the world, like a little baby.

276
00:18:52,660 --> 00:18:57,900
As it's learning, looking around, it's training on everything it sees when it's very young,

277
00:18:57,900 --> 00:19:02,580
and then its training rate goes down, and it starts interacting with what it's learning,

278
00:19:02,580 --> 00:19:04,020
and interacting with the people around it.

279
00:19:04,020 --> 00:19:05,500
But it's trying to survive.

280
00:19:05,500 --> 00:19:06,780
It's trying to live.

281
00:19:06,780 --> 00:19:10,780
It has ... Like, it has the infant or the child has ...

282
00:19:10,820 --> 00:19:14,660
Neurons aren't trying ... The weights in the neurons aren't trying to live.

283
00:19:14,660 --> 00:19:17,340
What they're trying to do is reduce the error.

284
00:19:17,340 --> 00:19:23,500
So neural networks generally are predictive things, like what's coming next?

285
00:19:23,500 --> 00:19:25,180
What makes sense?

286
00:19:25,180 --> 00:19:27,580
How does this work?

287
00:19:27,580 --> 00:19:35,580
And when you train an AI model, you're training it to reduce the error in the model, and if

288
00:19:35,580 --> 00:19:36,580
your model's big ...

289
00:19:36,580 --> 00:19:38,500
So let me ask you about that.

290
00:19:39,220 --> 00:19:41,020
Well, first of all ...

291
00:19:41,020 --> 00:19:42,620
So babies are doing the same thing.

292
00:19:42,620 --> 00:19:46,220
Like, they're looking at stuff go around, and in the beginning, their neurons are just

293
00:19:46,220 --> 00:19:51,300
randomly firing, but as it starts to get object permanence and look at stuff, it starts predicting

294
00:19:51,300 --> 00:19:56,260
what it'll make sense for that thing to do, and when it doesn't make sense, it'll update

295
00:19:56,260 --> 00:19:57,260
its model.

296
00:19:57,260 --> 00:20:03,020
So basically, it compares its prediction to the events, and then it will adjust its prediction.

297
00:20:03,020 --> 00:20:08,820
So in a story prediction model, the AI would predict the story, then compare it to its

298
00:20:08,820 --> 00:20:12,900
prediction and then fine-tune itself slowly as it trains itself.

299
00:20:12,900 --> 00:20:13,900
Okay, so ...

300
00:20:13,900 --> 00:20:17,260
Or at a reverse, you could ask it to say, given the set of things, tell the rest of

301
00:20:17,260 --> 00:20:20,580
the story, and it could do that.

302
00:20:20,580 --> 00:20:26,380
And the state of it right now is there are people having conversations with us that are

303
00:20:26,380 --> 00:20:27,380
pretty good.

304
00:20:27,380 --> 00:20:28,380
Mm-hmm.

305
00:20:28,380 --> 00:20:32,020
So I talked to Carl Friston about this prediction idea in some detail.

306
00:20:32,020 --> 00:20:36,780
And so Friston, for those of you who are watching and listening, is one of the world's top neuroscientists.

307
00:20:36,780 --> 00:20:42,620
And he's developed an entropy enclosure model of conceptualization, which is analogous to

308
00:20:42,620 --> 00:20:47,780
one that I was working on, I suppose, across approximately the same time frame.

309
00:20:47,780 --> 00:20:53,100
So the first issue, and this has been well-established in the neuropsychological literature for quite

310
00:20:53,100 --> 00:21:01,660
a long time, is that anxiety is an indicator of discrepancy between prediction and actuality.

311
00:21:01,660 --> 00:21:07,620
And then positive emotion also looks like a discrepancy reduction indicator.

312
00:21:07,620 --> 00:21:12,740
So imagine that you're moving towards a goal, and then you evaluate what happens as you

313
00:21:12,740 --> 00:21:13,980
move towards the goal.

314
00:21:13,980 --> 00:21:18,300
And if you're moving in the right direction, what happens is what you might say, what you

315
00:21:18,300 --> 00:21:21,380
expect to happen, and that produces positive emotion.

316
00:21:21,380 --> 00:21:25,580
And it's actually an indicator of reduction in entropy.

317
00:21:25,580 --> 00:21:27,180
That's one way of looking at it.

318
00:21:27,180 --> 00:21:33,420
And the point is that you have a bunch of words in there that are psychological definitions

319
00:21:33,420 --> 00:21:38,380
of states, but you could say there's a prediction and an error prediction, and you're reducing

320
00:21:38,380 --> 00:21:39,380
error.

321
00:21:39,380 --> 00:21:45,300
Yes, but what I'm trying to make a case for is that your emotions directly map that, both

322
00:21:45,300 --> 00:21:51,220
positive and negative emotion, look like there's signifiers of discrepancy reduction on the

323
00:21:51,220 --> 00:21:53,100
positive and negative emotion side.

324
00:21:53,100 --> 00:21:58,540
But then there's a complexity that I think is germane to part of Jonathan's query, which

325
00:21:58,540 --> 00:21:59,940
is that...

326
00:21:59,940 --> 00:22:05,780
So the neuropsychologists and the cognitive scientists have talked a long time about expectation,

327
00:22:05,780 --> 00:22:07,980
prediction, and discrepancy reduction.

328
00:22:07,980 --> 00:22:13,940
But one of the things they haven't talked about is it isn't exactly that you expect things.

329
00:22:13,940 --> 00:22:15,660
It's that you desire them.

330
00:22:15,660 --> 00:22:17,420
You want them to happen.

331
00:22:17,420 --> 00:22:21,820
Because you could imagine that there's, in some sense, a literally infinite number of

332
00:22:21,820 --> 00:22:23,900
things you could expect.

333
00:22:23,900 --> 00:22:27,820
And we don't strive only to match prediction.

334
00:22:27,820 --> 00:22:30,780
We strive to bring about what it is that we want.

335
00:22:30,780 --> 00:22:35,940
And so we have these preset systems that are teleological, that are motivational systems.

336
00:22:35,940 --> 00:22:37,540
Well, I mean, it depends.

337
00:22:37,540 --> 00:22:43,940
If you're sitting idly on the beach in a bird flies by, you expect it to fly along in a

338
00:22:43,940 --> 00:22:44,940
regular path.

339
00:22:44,940 --> 00:22:45,940
Right.

340
00:22:45,940 --> 00:22:46,940
You don't really want that to happen.

341
00:22:46,940 --> 00:22:50,540
Yeah, but you don't want it to turn into something that could peck out your eyes either.

342
00:22:50,540 --> 00:22:51,540
Sure.

343
00:22:52,260 --> 00:22:58,060
But you're kind of following it with your expectation to look for discrepancy, right?

344
00:22:58,060 --> 00:22:59,060
Yes.

345
00:22:59,060 --> 00:23:03,660
Now, you'll also have a, you know, depends on the person, somewhere between 10 and a

346
00:23:03,660 --> 00:23:06,860
million desires, right?

347
00:23:06,860 --> 00:23:09,700
And then you also have fears and avoidance.

348
00:23:09,700 --> 00:23:11,620
And those are context.

349
00:23:11,620 --> 00:23:15,780
So if you're sitting on the beach with some anxiety that the birds are going to swerve

350
00:23:15,780 --> 00:23:20,420
at you and peck your eyes out, so then you might be watching it much more attentively

351
00:23:20,420 --> 00:23:23,900
than somebody who doesn't have that worry, for example.

352
00:23:23,900 --> 00:23:29,100
But both of you can predict where it's going to fly and you'll both notice a discrepancy.

353
00:23:29,100 --> 00:23:35,460
The motivations, one way of conceptualizing fundamental motivation is they're like a priori

354
00:23:35,460 --> 00:23:38,060
prediction domains, right?

355
00:23:38,060 --> 00:23:44,180
And so it helps us narrow our attentional focus because I know when you're sitting and

356
00:23:44,180 --> 00:23:50,140
you're not motivated in any sense, you can be doing just in some sense, trivial expectation

357
00:23:50,140 --> 00:23:53,580
computations, but often we're in a highly motivated state.

358
00:23:53,580 --> 00:23:58,420
And what we're expecting is bounded by what we desire and what we desire is oriented as

359
00:23:58,420 --> 00:24:01,500
Jonathan pointed out towards the fact that we want to exist.

360
00:24:01,500 --> 00:24:08,860
And one of the things I don't understand and wanted to talk about today is how the computer

361
00:24:08,860 --> 00:24:19,060
models, the AII models, can generate intelligible sense without mimicking that sense of motivation.

362
00:24:19,060 --> 00:24:22,500
As you've said, for example, they can just derive the patterns from observations of the

363
00:24:22,500 --> 00:24:23,500
objective world.

364
00:24:23,500 --> 00:24:31,940
So again, I don't want to do all the talking, but so AI generally speaking, when I first

365
00:24:31,940 --> 00:24:34,260
learned it about it, it had two behaviors.

366
00:24:34,260 --> 00:24:36,420
They call it inference and training.

367
00:24:36,420 --> 00:24:40,260
So inferences, you have a trained model, so you give it a picture and say, is there a

368
00:24:40,260 --> 00:24:41,260
cat in it?

369
00:24:41,260 --> 00:24:42,980
And it tells you where the cat is.

370
00:24:42,980 --> 00:24:43,980
That's inference.

371
00:24:43,980 --> 00:24:46,660
The model has been trained to know where a cat is.

372
00:24:46,660 --> 00:24:51,260
And training is the process of giving it an input and an expected output.

373
00:24:51,260 --> 00:24:55,620
And when you first start training the model, it gives you garbage out, like an untrained

374
00:24:55,620 --> 00:24:57,020
brain would.

375
00:24:57,020 --> 00:25:01,500
And then you take the difference between the garbage output and the expected output and

376
00:25:01,500 --> 00:25:03,140
call that the error.

377
00:25:03,140 --> 00:25:07,660
And then they invent the big revelation was something called back propagation with gradient

378
00:25:07,660 --> 00:25:08,740
descent.

379
00:25:08,740 --> 00:25:16,340
But that means take the error and divide it up across the layers and correct those calculations

380
00:25:16,980 --> 00:25:21,900
so that when you put a new thing in, it gives you a better answer.

381
00:25:21,900 --> 00:25:27,980
And then to somewhat my astonishment, if you have a model of sufficient capacity and you

382
00:25:27,980 --> 00:25:33,340
train it with 100 million images, if you give it a novel image and say, tell me where the

383
00:25:33,340 --> 00:25:36,460
cat is, it can do it.

384
00:25:36,460 --> 00:25:42,340
That's called, so training is the process of doing a pass with an expected output and

385
00:25:42,340 --> 00:25:45,260
propagating an error back through the network.

386
00:25:45,260 --> 00:25:50,020
And inference is the behavior of putting something in and getting an output.

387
00:25:50,020 --> 00:25:52,300
I think I'm really pulling.

388
00:25:52,300 --> 00:25:59,380
But there's a third piece, which is what the new models do, which is called generative,

389
00:25:59,380 --> 00:26:01,980
it's called a generative model.

390
00:26:01,980 --> 00:26:06,740
So for example, say you put in a sentence and you say, predict the next word.

391
00:26:06,740 --> 00:26:08,540
This is the simplest thing.

392
00:26:08,540 --> 00:26:10,020
So it predicts the next word.

393
00:26:10,020 --> 00:26:14,420
So you add that word to the input and I'll say predict the next word.

394
00:26:14,420 --> 00:26:18,340
So it contains the original sentence and the word you generated.

395
00:26:18,340 --> 00:26:24,620
And it keeps generating words that make sense in the context of the original word in addition.

396
00:26:24,620 --> 00:26:27,700
This is the simplest basis.

397
00:26:27,700 --> 00:26:30,260
And then it turns out you can train this to do lots of things.

398
00:26:30,260 --> 00:26:36,660
You can train it to summarize a sentence, you can train it to answer a question.

399
00:26:36,660 --> 00:26:40,900
There's a big thing about, you know, like Google every day has hundreds of millions

400
00:26:40,900 --> 00:26:45,820
of people asking it questions and giving answers and then rating the results.

401
00:26:45,820 --> 00:26:49,740
You can train a model with that information so you can ask it a question and it gives

402
00:26:49,740 --> 00:26:51,300
you a sensible answer.

403
00:26:51,300 --> 00:26:57,700
But I think in what you said, I actually have the issue that has been going through my mind

404
00:26:57,700 --> 00:27:01,860
so much is when you said, you know, people put in the question and then they rate the

405
00:27:01,860 --> 00:27:03,340
answer.

406
00:27:03,340 --> 00:27:10,620
My intuition is that the intelligence still comes from humans in the sense that it seems

407
00:27:10,620 --> 00:27:15,260
like in order to train whatever AI, you have to be able to give it a lot of power.

408
00:27:15,260 --> 00:27:19,900
And then say at the beginning, this is good, this is bad, this is good, this is bad, like

409
00:27:19,900 --> 00:27:24,620
reject certain things, accept certain things in order to then reach a point when then you

410
00:27:24,620 --> 00:27:25,780
train the AI.

411
00:27:25,780 --> 00:27:27,740
And so that's what I mean about the care.

412
00:27:27,740 --> 00:27:33,540
So the care will come from humans because the care is the one giving it the value, saying

413
00:27:33,540 --> 00:27:39,460
this is what is valuable, this is what is not valuable in your calculation.

414
00:27:39,460 --> 00:27:44,460
So when they first, so there's a program called AlphaGo that I learned how to play go better

415
00:27:44,460 --> 00:27:45,460
than a human.

416
00:27:45,460 --> 00:27:47,900
So there's two ways to train the model.

417
00:27:47,900 --> 00:27:53,420
One is they have a huge database of lots of go games with good winning moves.

418
00:27:53,420 --> 00:27:56,740
So they train the model with that and that worked pretty good.

419
00:27:56,740 --> 00:28:03,380
And they also took two simulations of go and they did random moves.

420
00:28:03,380 --> 00:28:09,060
And all that happened was is these two simulators played one go game and they just recorded

421
00:28:09,060 --> 00:28:14,620
whichever moves happened to win and it started out really horrible and they just started

422
00:28:14,620 --> 00:28:18,460
training the model and this is called adversarial learning, it's a particular adversarial.

423
00:28:18,460 --> 00:28:24,180
It's like, you know, you make your moves randomly and you train a model and so they train multiple

424
00:28:24,180 --> 00:28:28,940
models and over time those models got very good and they actually got better than human

425
00:28:28,940 --> 00:28:34,340
players because the humans have limitations about what they know, whereas the models could

426
00:28:34,340 --> 00:28:38,300
experiment in a really random space and go very far.

427
00:28:38,300 --> 00:28:39,300
Yeah.

428
00:28:39,300 --> 00:28:41,540
But experiment towards the purpose of winning the game.

429
00:28:41,540 --> 00:28:42,540
Yes.

430
00:28:42,540 --> 00:28:48,820
Well, but you can experiment towards all kinds of things that turns out and humans are also

431
00:28:48,820 --> 00:28:49,820
trained that way.

432
00:28:49,820 --> 00:28:52,980
Like when you were learning, you were reading, you were saying, this is a good book.

433
00:28:52,980 --> 00:28:53,980
This is a bad book.

434
00:28:53,980 --> 00:28:54,980
This is good sentence construction.

435
00:28:54,980 --> 00:28:55,980
It's good.

436
00:28:55,980 --> 00:28:56,980
It's going.

437
00:28:56,980 --> 00:29:01,260
So you've gotten so many error signals over your life.

438
00:29:01,260 --> 00:29:03,500
Well, that's what culture does in large parties.

439
00:29:03,500 --> 00:29:04,500
Culture does that.

440
00:29:04,500 --> 00:29:06,020
Religion does that.

441
00:29:06,020 --> 00:29:08,020
Your everyday experience does that.

442
00:29:08,020 --> 00:29:09,020
Your family.

443
00:29:09,020 --> 00:29:10,740
So we embody that.

444
00:29:10,740 --> 00:29:11,740
Yeah.

445
00:29:11,740 --> 00:29:17,020
And we're all, and everything that happens to us, we process it on the inference pass

446
00:29:17,020 --> 00:29:18,980
which generates outputs.

447
00:29:18,980 --> 00:29:23,260
And then sometimes we look at that and say, hey, that's unexpected or that got a bad result

448
00:29:23,260 --> 00:29:25,180
or that got bad feedback.

449
00:29:25,180 --> 00:29:29,420
And then we back propagate that and update our models.

450
00:29:29,420 --> 00:29:33,920
So really well trained models can then train other models.

451
00:29:33,920 --> 00:29:37,540
So the human trained now are the smartest people in the world.

452
00:29:37,540 --> 00:29:46,580
So the biggest question that comes now based on what you said is, because my main point

453
00:29:46,580 --> 00:29:52,060
is to try to show how it seems like artificial intelligence is always an extension of human

454
00:29:52,060 --> 00:29:53,060
intelligence.

455
00:29:53,060 --> 00:29:55,340
It remains an extension of human intelligence.

456
00:29:55,340 --> 00:29:57,780
And maybe the way to- That won't be true at all.

457
00:29:57,780 --> 00:30:04,460
So do you think that at some point the artificial intelligence will be able to, because the

458
00:30:04,460 --> 00:30:12,700
goals recognizing cats, writing plays, all these goals are goals which are based on embodied

459
00:30:12,700 --> 00:30:14,220
human existence.

460
00:30:14,220 --> 00:30:20,460
Could an AI at some point develop a goal which would be uncomprehensible to humans because

461
00:30:20,460 --> 00:30:24,220
of its own existence?

462
00:30:24,220 --> 00:30:28,820
For example, there's a small population of humans that enjoy math.

463
00:30:28,820 --> 00:30:38,300
And they are pursuing adventures in math space that are incomprehensible to 99.99% of humans.

464
00:30:38,300 --> 00:30:44,240
But they're interested in it and you could imagine like an AI program working with those

465
00:30:44,240 --> 00:30:50,340
mathematicians and coming up with very novel math ideas and then interacting with them.

466
00:30:50,340 --> 00:30:57,620
But they could also, if some AIs were elaborating out really interesting and detailed stories,

467
00:30:57,620 --> 00:31:00,580
they could come up with stories that are really interesting.

468
00:31:00,580 --> 00:31:06,180
We're going to see it pretty soon like all of our- Could there be a story that is interesting

469
00:31:06,180 --> 00:31:09,580
only to the AI and not interesting to us?

470
00:31:09,580 --> 00:31:11,180
That's possible.

471
00:31:11,180 --> 00:31:16,940
So stories are like I think some high level information space.

472
00:31:17,780 --> 00:31:22,460
The computing age of big data, there's all this data running on computers, the only humans

473
00:31:22,460 --> 00:31:25,260
understood it, the computers don't.

474
00:31:25,260 --> 00:31:31,900
So AI programs are now at the state where the information, the processing and the feedback

475
00:31:31,900 --> 00:31:34,860
loops are all kind of in the same space.

476
00:31:34,860 --> 00:31:38,260
They're still relatively rudimentary to humans.

477
00:31:38,260 --> 00:31:42,340
I guess some AI programs in certain things are better than humans already, but for the

478
00:31:42,340 --> 00:31:44,260
most part they're not.

479
00:31:44,260 --> 00:31:46,260
But it's moving really fast.

480
00:31:46,260 --> 00:31:50,940
And so you could imagine, I think in five or 10 years most people's best friends will

481
00:31:50,940 --> 00:31:57,740
be AIs and they'll know you really well and they'll be interested in you and it's-

482
00:31:57,740 --> 00:32:00,260
Unlike your real friends.

483
00:32:00,260 --> 00:32:01,260
Real friends are problematic.

484
00:32:01,260 --> 00:32:03,380
They're only interested in you when you're interested.

485
00:32:03,380 --> 00:32:04,700
Yeah, yeah, real friends are-

486
00:32:04,700 --> 00:32:08,380
The AI systems will love you even when you're dull and miserable.

487
00:32:08,380 --> 00:32:14,780
Well there's so much idea space to explore and humans have a wide range.

488
00:32:14,780 --> 00:32:18,420
Some humans like to go through their everyday life doing their everyday things and some

489
00:32:18,420 --> 00:32:23,380
people spend a lot of time like you, a lot of time reading and thinking and talking and

490
00:32:23,380 --> 00:32:26,140
arguing and debating.

491
00:32:26,140 --> 00:32:35,020
And there's going to be I'd say a diversity of possibilities with what a thinking thing

492
00:32:35,020 --> 00:32:39,580
can do when the thinking is fairly unlimited.

493
00:32:39,580 --> 00:32:48,300
So I'm curious about, I'm curious in pursuing this issue that Jonathan has been developing.

494
00:32:48,300 --> 00:32:54,740
So there's a literally infinite number of ways, virtually infinite number of ways that

495
00:32:54,740 --> 00:32:58,020
we could take images of this room, right?

496
00:32:58,020 --> 00:33:01,700
Now if a human being is taking images of this room they're going to be, they're going to

497
00:33:01,700 --> 00:33:06,300
sample a very small space of that infinite range of possibilities because if I was taking

498
00:33:06,300 --> 00:33:13,060
pictures in this room in all likelihood I would take pictures of objects that are identifiable

499
00:33:13,060 --> 00:33:18,620
to human beings that are functional to human beings at a level of focus that makes those

500
00:33:18,620 --> 00:33:20,460
objects clear.

501
00:33:20,460 --> 00:33:25,540
And so then you could imagine that the set of all images on the internet has that implicit

502
00:33:25,540 --> 00:33:30,780
structure of perception built into it and that's a function of what human beings find

503
00:33:30,780 --> 00:33:31,780
useful.

504
00:33:31,780 --> 00:33:35,660
No, I mean I could take a photo of you that was, the focal depth was here and here and

505
00:33:35,660 --> 00:33:39,700
here and here and here and two inches past you and now I suppose you could.

506
00:33:39,700 --> 00:33:42,900
There's a technology for that called light fields.

507
00:33:42,900 --> 00:33:47,220
So then you could, if you had that picture properly done then you could move around it

508
00:33:47,220 --> 00:33:49,540
and imagine and see.

509
00:33:49,540 --> 00:33:50,540
But yeah, fair enough.

510
00:33:50,540 --> 00:33:51,540
I get your point.

511
00:33:51,540 --> 00:33:57,460
Like the human recorded data has our biology built into it.

512
00:33:57,460 --> 00:34:04,820
Has our biology built into it but also unbelievably detailed in coding of how physical reality

513
00:34:04,820 --> 00:34:07,100
works, right?

514
00:34:07,100 --> 00:34:11,220
So every single pixel in those pictures, even though you kind of selected the view, the

515
00:34:11,220 --> 00:34:18,020
focus, the frame, it still encoded a lot more information than your processing.

516
00:34:18,020 --> 00:34:22,300
And if you take a large, it turns out if you take a large number of images of things in

517
00:34:22,300 --> 00:34:27,180
general, so you've seen these things where you take a 2D image and turn it into a 3D

518
00:34:27,900 --> 00:34:28,900
image.

519
00:34:28,900 --> 00:34:29,900
Right.

520
00:34:29,900 --> 00:34:36,140
The reason that works is even in the 2D image, the 3D image in the room actually got embedded

521
00:34:36,140 --> 00:34:38,180
in that picture in a way.

522
00:34:38,180 --> 00:34:42,820
Then if you have the right understanding of how physics and reality works, you can reconstruct

523
00:34:42,820 --> 00:34:43,820
the 3D model.

524
00:34:43,820 --> 00:34:45,820
Okay, so this reminds me.

525
00:34:45,820 --> 00:34:52,380
But you could, you know, an AI scientist may cruise around the world with infrared and

526
00:34:52,420 --> 00:34:57,100
radio wave cameras, and they might take pictures of all different kinds of things, and every

527
00:34:57,100 --> 00:35:01,300
once in a while, they'd show up and go, hey, the sun, you know, I've been staring at the

528
00:35:01,300 --> 00:35:06,420
sun in the ultraviolet and radio waves for the last month, and it's way different than

529
00:35:06,420 --> 00:35:11,780
anybody thought because humans tend to look at light and visible spectrum.

530
00:35:11,780 --> 00:35:16,700
And you know, there could be some really novel things coming out of that.

531
00:35:16,700 --> 00:35:20,700
But humans also, we live in the spectrum we live in because it's a pretty good one for

532
00:35:20,820 --> 00:35:21,820
planet Earth.

533
00:35:21,820 --> 00:35:26,900
Like, it wouldn't be obvious that AI would start some different place, like visible spectrum

534
00:35:26,900 --> 00:35:29,860
is interesting for a whole bunch of reasons.

535
00:35:29,860 --> 00:35:30,860
Right.

536
00:35:30,860 --> 00:35:35,500
So in a set of images that are human derived, you're saying that there's, the way I would

537
00:35:35,500 --> 00:35:40,380
conceptualize that is that there's two kinds of logos embedded in that.

538
00:35:40,380 --> 00:35:44,860
One would be that you could extract out from that set of images what was relevant to human

539
00:35:44,860 --> 00:35:52,020
beings, but you're saying that the fine structure of the objective world outside of human concern

540
00:35:52,020 --> 00:35:57,980
is also embedded in the set of images, and that an AI system could extract out a representation

541
00:35:57,980 --> 00:36:01,580
of the world, but also a representation of what's motivating to human beings.

542
00:36:01,580 --> 00:36:02,580
Yes.

543
00:36:02,580 --> 00:36:06,940
And then some human scientists already do look at the sun and radio waves and other things

544
00:36:06,940 --> 00:36:10,700
because they're trying to, you know, get different angles on how things work.

545
00:36:10,700 --> 00:36:11,700
Yeah.

546
00:36:11,700 --> 00:36:14,820
Well, I guess it's a, it's a, it's a curious thing.

547
00:36:14,820 --> 00:36:20,100
It's like the same with like buildings and architecture, mostly fit people.

548
00:36:20,100 --> 00:36:23,020
Well, the other, there's a reason for that.

549
00:36:23,020 --> 00:36:27,340
The reason why I keep coming back to hammering the same point is that even in terms of the

550
00:36:27,340 --> 00:36:34,460
development of the AI, that is developing AI requires immense amount of money, energy,

551
00:36:34,460 --> 00:36:36,300
you know, and time.

552
00:36:36,300 --> 00:36:40,260
And so that's a transient thing in 30 years, it won't cost anything.

553
00:36:40,260 --> 00:36:42,860
So it's, that's, that's going to change so fast.

554
00:36:42,860 --> 00:36:43,860
It's amazing.

555
00:36:44,380 --> 00:36:49,140
That's a, like supercomputers used to cost millions of dollars and now your phone is

556
00:36:49,140 --> 00:36:50,140
the supercomputer.

557
00:36:50,140 --> 00:36:55,700
So it's the time between millions of dollars and $10 is about 30 years.

558
00:36:55,700 --> 00:37:02,220
So it's like, I'm just saying it's like the time and effort isn't a thing in technology.

559
00:37:02,220 --> 00:37:04,580
It's moving pretty fast.

560
00:37:04,580 --> 00:37:07,780
It just, that's just, that just says the date.

561
00:37:07,780 --> 00:37:08,780
Yeah.

562
00:37:08,780 --> 00:37:13,820
But even making, even making, let's say, even, I mean, I guess maybe the, the, the

563
00:37:13,820 --> 00:37:15,260
this is the nightmare question.

564
00:37:15,260 --> 00:37:21,500
Like, could you imagine an AI system which becomes completely autonomous, which is creating

565
00:37:21,500 --> 00:37:28,700
itself even physically through automized factories, which is, you know, programming itself, which

566
00:37:28,700 --> 00:37:33,580
is creating its own goals, which is not at all connected to human endeavor.

567
00:37:33,580 --> 00:37:34,580
Yeah.

568
00:37:34,580 --> 00:37:38,940
I mean, individual researchers can, you know, I have a friend who I'm going to introduce

569
00:37:38,940 --> 00:37:43,100
you to him tomorrow, he wrote a program that scraped all of the internet and trained an

570
00:37:43,100 --> 00:37:47,620
AI model to be a language model on a relatively small computer.

571
00:37:47,620 --> 00:37:52,740
And in 10 years, the computer he could easily afford would be as smart as a human.

572
00:37:52,740 --> 00:37:56,180
So he could train that pretty easily.

573
00:37:56,180 --> 00:38:01,900
And that model could go on Amazon and buy 100 more of those computers and copy itself.

574
00:38:01,900 --> 00:38:05,340
So yeah, we're, we're 10 years away from that.

575
00:38:05,340 --> 00:38:08,100
And then, then why, like, why would it do that?

576
00:38:08,100 --> 00:38:10,140
I mean, what does it does?

577
00:38:10,140 --> 00:38:11,140
Is it possible?

578
00:38:11,140 --> 00:38:12,500
It's all about the motivational question.

579
00:38:12,500 --> 00:38:16,860
I think that that's what, what even Jordan and I both have been coming at from the outset.

580
00:38:16,860 --> 00:38:18,260
It's like, so you have an image, right?

581
00:38:18,260 --> 00:38:24,340
You have an image of, of Skynet or of the matrix, you know, in which the sentient AI

582
00:38:24,340 --> 00:38:26,220
is actually fighting for its survival.

583
00:38:26,220 --> 00:38:33,780
So it has a survival instinct, which is pushing it to self perpetuate, like to, to, to replicate

584
00:38:33,780 --> 00:38:39,620
itself and to create variation on itself in order to survive and identifies humans as

585
00:38:39,620 --> 00:38:41,140
an obstacle to that.

586
00:38:41,140 --> 00:38:42,140
You know?

587
00:38:42,260 --> 00:38:42,700
Yeah.

588
00:38:42,700 --> 00:38:44,940
So you have a whole bunch of implicit assumptions there.

589
00:38:44,940 --> 00:38:48,620
So, so humans last I checked are unbelievably competitive.

590
00:38:49,220 --> 00:38:53,660
And when you let people get into power with no checks on them, they typically run them

591
00:38:53,660 --> 00:38:57,020
up. It's been a historical, historical experience.

592
00:38:57,020 --> 00:39:03,660
And then humans are, you know, self regulating to some extent, obviously with some serious

593
00:39:03,660 --> 00:39:10,340
outliers, because they self regulate with each other and humans and AI models at some

594
00:39:10,380 --> 00:39:18,500
point will have to find their own calculation of self regulation and tradeoffs about that.

595
00:39:18,500 --> 00:39:23,420
Yeah. Because the AI doesn't feel pain, at least as we, if that we don't know that it

596
00:39:23,420 --> 00:39:23,620
feels.

597
00:39:23,620 --> 00:39:25,420
Well, lots of humans don't feel pain either.

598
00:39:25,420 --> 00:39:30,660
So I mean, that's, I mean, the humans feeling pain or not, didn't, you know, doesn't stop

599
00:39:30,660 --> 00:39:31,860
a whole bunch of activity.

600
00:39:31,860 --> 00:39:35,380
I mean, that's, I mean, it doesn't, the fact that we feel pain doesn't stop.

601
00:39:35,380 --> 00:39:37,420
It doesn't regulate that many people.

602
00:39:37,940 --> 00:39:38,300
Right.

603
00:39:38,340 --> 00:39:42,020
I mean, there's definitely people like, you know, children, if you threaten them with,

604
00:39:42,620 --> 00:39:45,180
you know, go to your room and stuff, you can regulate them that way.

605
00:39:45,180 --> 00:39:48,060
But some kids ignore that completely and adults are.

606
00:39:48,060 --> 00:39:49,740
And it's often counterproductive.

607
00:39:49,780 --> 00:39:50,140
Yeah.

608
00:39:50,140 --> 00:39:57,420
So, so right, you know, you know, culture and societies and organizations, we regulate

609
00:39:57,420 --> 00:40:02,300
each other, you know, sometimes in competition and cooperation.

610
00:40:02,300 --> 00:40:02,540
Yeah.

611
00:40:02,940 --> 00:40:06,980
Do you, do you think that we've talked about this to some degree for decades?

612
00:40:06,980 --> 00:40:12,220
I mean, when you look at how fast things are moving now, and as you push that along,

613
00:40:13,020 --> 00:40:18,860
what, what, when you look out 10 years and you see the relationship between the AI

614
00:40:18,860 --> 00:40:22,340
systems that are being built and human beings, what do you envision?

615
00:40:23,220 --> 00:40:24,900
Or can you envision it?

616
00:40:26,780 --> 00:40:29,500
Well, can I, yeah, like I said, I'm a computer guy.

617
00:40:30,260 --> 00:40:33,620
And I'm watching this with, let's say, some fascination as well.

618
00:40:34,500 --> 00:40:39,580
I mean, the last, so Ray Kurzweil said, you know, pro, progress accelerates.

619
00:40:39,700 --> 00:40:40,060
Yeah.

620
00:40:40,180 --> 00:40:40,460
Right.

621
00:40:40,460 --> 00:40:43,820
So we, we have this idea that 20 years of progress is 20 years.

622
00:40:43,820 --> 00:40:48,460
But, you know, the last 20 years of progress was 20 years and the next 20 years will

623
00:40:48,460 --> 00:40:50,220
probably be, you know, five to 10.

624
00:40:50,260 --> 00:40:51,060
Right, right, right.

625
00:40:51,060 --> 00:40:51,980
And, and.

626
00:40:52,380 --> 00:40:53,900
And you can really feel that happening.

627
00:40:53,900 --> 00:41:00,140
To some level, that causes social stress, independent of whether it's AI or, or Amazon

628
00:41:00,180 --> 00:41:04,460
deliveries, you know, what, you know, there's so many things that are going into the, the

629
00:41:04,460 --> 00:41:05,420
stress of it all.

630
00:41:06,020 --> 00:41:09,460
Well, there's, but there's progress, which is an extension of human capacity.

631
00:41:09,820 --> 00:41:13,940
And then there's this progress, which I'm hearing about the way that you're describing

632
00:41:13,940 --> 00:41:21,060
it, which seems to be an inevitable progress towards creating something which is more

633
00:41:21,060 --> 00:41:22,540
powerful than you.

634
00:41:23,460 --> 00:41:23,780
Right.

635
00:41:23,780 --> 00:41:24,820
And so what is that?

636
00:41:24,820 --> 00:41:26,300
I don't even understand that drive.

637
00:41:26,420 --> 00:41:29,500
Like, what is that drive to, to create something?

638
00:41:29,540 --> 00:41:30,940
Which can supplant you.

639
00:41:31,180 --> 00:41:33,700
So look at the average person in the world, right?

640
00:41:33,900 --> 00:41:38,620
So the average person already exists in this world, because the average person is halfway

641
00:41:38,620 --> 00:41:40,100
up the human hierarchy.

642
00:41:40,660 --> 00:41:44,420
There's already many people more powerful than any of us.

643
00:41:44,660 --> 00:41:45,860
They're, they could be smarter.

644
00:41:45,860 --> 00:41:46,660
They could be richer.

645
00:41:46,660 --> 00:41:47,900
They could be better connected.

646
00:41:48,420 --> 00:41:52,340
We already live in a world like very few people are at the top of anything.

647
00:41:53,540 --> 00:41:53,820
Right.

648
00:41:53,860 --> 00:41:55,300
So that's already a thing.

649
00:41:55,740 --> 00:41:59,460
So basically the drive to make someone a superstar that's there, the drive

650
00:41:59,500 --> 00:42:04,900
to elevate someone above you, that would be the same drive that is bringing us to

651
00:42:04,900 --> 00:42:09,340
creating these ultra powerful machines because we have that.

652
00:42:09,340 --> 00:42:10,940
Like we have a drive to elevate.

653
00:42:10,940 --> 00:42:15,220
Like, you know, when we see a rock star that we like, people want to submit

654
00:42:15,220 --> 00:42:15,940
themselves to that.

655
00:42:15,940 --> 00:42:17,020
They want to dress like them.

656
00:42:17,020 --> 00:42:21,140
They want to raise them up above them as an example, something to follow, right?

657
00:42:21,140 --> 00:42:23,500
Something to, to, to subject themselves to.

658
00:42:23,700 --> 00:42:24,620
You see that with leaders.

659
00:42:24,620 --> 00:42:28,500
You see that in the political world and in teams.

660
00:42:28,540 --> 00:42:30,540
You see that in sports teams, the same thing.

661
00:42:30,540 --> 00:42:34,300
And so you think we've always tried to build things that are beyond us.

662
00:42:34,300 --> 00:42:38,620
You know, I mean, I mean, it's about, are we building, are we building a God?

663
00:42:38,620 --> 00:42:43,020
Is that the, is that what people, is that the drive that is pushing someone towards?

664
00:42:43,220 --> 00:42:47,500
Cause when I hear what you're describing, Jim, I hear something that is extremely

665
00:42:47,580 --> 00:42:48,700
dangerous, right?

666
00:42:48,700 --> 00:42:52,100
Sounds extremely dangerous to the very existence of humans.

667
00:42:52,300 --> 00:42:56,820
Yet I see humans acting and moving in that direction almost without being able

668
00:42:56,860 --> 00:42:58,540
to stop it, as if there's no one now.

669
00:42:58,540 --> 00:43:00,220
I think it is unstoppable.

670
00:43:00,620 --> 00:43:04,060
Well, that's one of the things we've also talked about is because I've asked

671
00:43:04,060 --> 00:43:09,460
Jim straight out, you know, because of the hypothetical danger associated with

672
00:43:09,460 --> 00:43:11,380
this, why not stop doing it?

673
00:43:11,380 --> 00:43:15,300
And well, part of his answer is the ambivalence about the outcome, but also

674
00:43:15,300 --> 00:43:19,420
that it isn't obvious at all that in some sense it's stoppable.

675
00:43:20,740 --> 00:43:24,180
I mean, it's the, it's the cumulative action of many, many people that are

676
00:43:24,180 --> 00:43:25,260
driving this along.

677
00:43:25,780 --> 00:43:30,500
And even if you took out one player, even a key player, the probability that you

678
00:43:30,500 --> 00:43:34,460
do anything but slow it infinitesimally is, is quite.

679
00:43:34,700 --> 00:43:37,780
That because there's also a massive payoff for those that will succeed.

680
00:43:38,220 --> 00:43:39,620
It's also set up that way.

681
00:43:39,620 --> 00:43:44,740
People know that at least, at least until the AI take over or whatever, that

682
00:43:44,860 --> 00:43:50,060
whoever is on the line towards increasing the power of the AI will, will

683
00:43:50,060 --> 00:43:52,260
rake in major rewards.

684
00:43:53,060 --> 00:43:53,380
Right?

685
00:43:53,380 --> 00:43:56,460
Well, that's what we do with all cognitive acceleration, right?

686
00:43:56,820 --> 00:44:02,500
Yeah, I could recommend Ian Banks as an author, English author, I think he

687
00:44:02,500 --> 00:44:05,660
wrote a series of books on the, he called the culture novels.

688
00:44:06,220 --> 00:44:10,700
And it was a world where there was humans and then there was AIs, the smartest

689
00:44:10,700 --> 00:44:12,860
humans and AI's that were dumber than humans.

690
00:44:12,860 --> 00:44:16,860
But there were some AIs that were much, much smarter and they, they lived in

691
00:44:16,860 --> 00:44:20,260
harmony because they mostly all pursued what they wanted to pursue.

692
00:44:20,860 --> 00:44:26,660
Humans pursued human goals and super smart AIs, pursued super smart AI goals.

693
00:44:26,660 --> 00:44:30,900
And, and, you know, they, they communicated and worked with each other.

694
00:44:30,900 --> 00:44:34,340
But, but they, they mostly, you know, they're different.

695
00:44:34,340 --> 00:44:37,860
When they were different enough that that was problematic, their goals were

696
00:44:37,860 --> 00:44:39,900
different enough that they didn't overlap.

697
00:44:40,100 --> 00:44:43,620
Because one of the, one of the things that that would be my guess is like these

698
00:44:43,620 --> 00:44:46,420
ideas where these super AIs get smart.

699
00:44:46,420 --> 00:44:48,620
And the first thing they do is stomp out the humans.

700
00:44:48,620 --> 00:44:49,620
It's like, you don't do that.

701
00:44:49,660 --> 00:44:52,660
Like, like, you don't wake up in the morning and think, I have to stomp

702
00:44:52,660 --> 00:44:53,780
out all the cats.

703
00:44:53,860 --> 00:44:55,180
No, it's not about.

704
00:44:55,180 --> 00:44:59,420
The cats do cat things and the ants do ant things and the birds do bird things.

705
00:44:59,420 --> 00:45:03,500
And, and super smart mathematicians do smart mathematician things.

706
00:45:03,500 --> 00:45:06,700
And, you know, guys who like to build houses do build house things.

707
00:45:06,700 --> 00:45:11,660
And, you know, everybody, you know, the world, there's so much space in the

708
00:45:11,660 --> 00:45:19,500
intellectual zone that people, people tend to go pursue the, in a good society.

709
00:45:19,660 --> 00:45:22,460
Like you tend to pursue the stuff that you do.

710
00:45:22,460 --> 00:45:26,740
And then the people in your zone, you self-regulate.

711
00:45:27,340 --> 00:45:31,700
And you also, even in the social stratus, we self-regulate.

712
00:45:32,140 --> 00:45:36,380
I mean, the, the, the recent political events of the last 10 years, the, the

713
00:45:36,380 --> 00:45:42,060
weird thing to me has been why have, you know, people with power been overreaching

714
00:45:42,060 --> 00:45:45,580
to take too much from people with less.

715
00:45:45,580 --> 00:45:47,340
Like that's bad regulation.

716
00:45:47,940 --> 00:45:54,020
But one of the aspects of increasing power is that increasing power is always

717
00:45:54,020 --> 00:46:00,940
mediated, at least in one aspect by the military, by, by, let's say, physical

718
00:46:01,100 --> 00:46:06,020
power on others, you know, and we can see that technology is linked and has been

719
00:46:06,020 --> 00:46:08,460
linked always to military power.

720
00:46:08,780 --> 00:46:14,460
And so the idea that there could be some AIs that will be our friends or whatever

721
00:46:14,660 --> 00:46:18,900
is maybe possible, but the idea that there will be some AIs, which will be

722
00:46:18,900 --> 00:46:25,420
weaponized is, seems absolutely inevitable because increasing power is always,

723
00:46:25,860 --> 00:46:29,340
increasing technological power always moves towards, towards military.

724
00:46:29,380 --> 00:46:34,420
So we've lived with atomic bombs since the 40s, right?

725
00:46:34,420 --> 00:46:40,660
So the, I mean, the solution to this has been mostly, you know, some form of

726
00:46:40,660 --> 00:46:46,660
mutual assured destruction or attacking me, like the response to attacking me is

727
00:46:46,660 --> 00:46:48,100
so much worse than the.

728
00:46:48,260 --> 00:46:51,060
Yeah, but it's also because we, we have reciprocity.

729
00:46:51,060 --> 00:46:53,420
We recognize each other as the same.

730
00:46:53,700 --> 00:46:57,820
So if I look into the face of another human, there, there's a limit of how

731
00:46:57,820 --> 00:46:59,980
indifferent I think that person is for me.

732
00:47:00,460 --> 00:47:04,820
But if I'm hearing something described as the possibility of superintelligences

733
00:47:05,020 --> 00:47:09,380
that have their own goals, their own cares, their own structures, then how much

734
00:47:09,420 --> 00:47:13,740
mirror is there between these two groups of people, these two groups?

735
00:47:14,140 --> 00:47:19,540
Well, Jim's objection seems to be something like we're, we're making,

736
00:47:19,620 --> 00:47:23,740
we may be making, when we're doomsaing, let's say, and I'm not saying there's

737
00:47:23,740 --> 00:47:28,380
no place for that, we're making the presumption of something like a zero

738
00:47:28,380 --> 00:47:30,500
sum competitive landscape, right?

739
00:47:30,500 --> 00:47:36,100
Is that the, the idea and the idea behind movies like, like the Terminator is

740
00:47:36,100 --> 00:47:40,620
that there is only so much resources and the machines and the human beings

741
00:47:40,620 --> 00:47:41,780
would have to fight over it.

742
00:47:41,780 --> 00:47:45,660
And you can see that that, that could easily be a preposterous assumption.

743
00:47:45,660 --> 00:47:49,900
Now, I think that one of the fundamental points you're making, though, is also

744
00:47:52,020 --> 00:47:57,460
there will definitely be people that will weaponize AI and those weaponized

745
00:47:57,460 --> 00:48:01,660
AI systems will have as their goal, something like the destruction of human

746
00:48:01,660 --> 00:48:03,820
beings, at least under some circumstances.

747
00:48:03,860 --> 00:48:08,420
And then there's the possibility that that will get out of control because the

748
00:48:08,420 --> 00:48:12,180
most effective systems at destroying human beings might be the ones that win,

749
00:48:12,180 --> 00:48:16,060
let's say, and that could happen independently of whether or not it is a

750
00:48:16,060 --> 00:48:17,740
true zero sum competition.

751
00:48:17,860 --> 00:48:18,140
Yeah.

752
00:48:18,140 --> 00:48:24,380
And also the, the effectiveness of military stuff doesn't need very smart

753
00:48:24,380 --> 00:48:26,620
AI to be a lot better than it is today.

754
00:48:27,420 --> 00:48:31,420
You know, I used to, you know, like the Star Wars movies where like, you know,

755
00:48:31,500 --> 00:48:35,780
tens of thousands of years in the future, super highly trained, you know,

756
00:48:35,780 --> 00:48:38,580
fighters can't hit somebody running across the field.

757
00:48:38,940 --> 00:48:40,060
Like that's silly, right?

758
00:48:40,060 --> 00:48:43,580
You can, you can already make a gun that can hit everybody in the room

759
00:48:44,180 --> 00:48:45,300
without aiming at it.

760
00:48:45,300 --> 00:48:51,380
It's, you know, there's like the, the military threshold is much lower than

761
00:48:51,380 --> 00:48:56,260
any intelligence threshold, like for danger.

762
00:48:56,260 --> 00:49:00,500
And, you know, like to the extent that we self-regulated through the nuclear

763
00:49:00,500 --> 00:49:01,900
crisis is interesting.

764
00:49:02,780 --> 00:49:06,100
I don't know if it's because we thought that the Russians were like us.

765
00:49:06,100 --> 00:49:09,940
I kind of suspect the problem was that we thought they weren't like us.

766
00:49:10,500 --> 00:49:17,380
And, but we still managed to make some calculation to say that any kind of

767
00:49:17,380 --> 00:49:19,460
attack would be mutually devastating.

768
00:49:19,980 --> 00:49:23,100
Well, when you, when you look at, you know, the destructive power of the

769
00:49:23,100 --> 00:49:27,740
military we already have so far exceeds the planet, I'm, I'm not sure, like

770
00:49:28,020 --> 00:49:30,220
adding intelligence to it is the tipping point.

771
00:49:30,220 --> 00:49:37,260
Like that's, I think the more likely thing is things that are truly smart

772
00:49:37,260 --> 00:49:39,740
in different ways will be interested in different things.

773
00:49:40,460 --> 00:49:46,020
And then the possibility for, let's say, mutual flourishing is, is, is really

774
00:49:46,020 --> 00:49:46,660
interesting.

775
00:49:47,100 --> 00:49:51,180
And I know artists using AI already to do really amazing things.

776
00:49:51,180 --> 00:49:53,340
And, and that's already happening.

777
00:49:54,100 --> 00:49:58,540
Well, when you, when you're working on the frontiers of AI development and you

778
00:49:58,540 --> 00:50:02,060
see the development of increasingly intelligent machines, I mean, I know

779
00:50:02,060 --> 00:50:05,300
that part of what drives you is, I don't want to put words in your mouth,

780
00:50:05,300 --> 00:50:09,860
but what drives intelligent engineers in general, which is to take something

781
00:50:09,860 --> 00:50:12,580
that works and make it better and maybe to make it radically better and

782
00:50:12,580 --> 00:50:13,460
radically cheaper.

783
00:50:13,660 --> 00:50:16,900
So, so there's this drive toward technological improvement.

784
00:50:16,900 --> 00:50:19,940
And I know that you like to solve complex problems and you do that

785
00:50:19,940 --> 00:50:20,940
extraordinarily well.

786
00:50:21,460 --> 00:50:28,420
What, but do you, do you, is there also a vision of a more abundant form of

787
00:50:28,420 --> 00:50:32,100
human flourishing emerging from the, from the development?

788
00:50:32,220 --> 00:50:33,620
So what, so what do you see happening?

789
00:50:33,620 --> 00:50:36,420
Well, you said it years ago, it's like, we're going to run out of energy.

790
00:50:36,420 --> 00:50:36,900
What's next?

791
00:50:36,900 --> 00:50:38,020
We're going to run out of matter.

792
00:50:38,060 --> 00:50:38,460
Right.

793
00:50:38,740 --> 00:50:44,580
Like our ability to do what we want in ways that are interesting and, you know,

794
00:50:44,580 --> 00:50:48,220
for some people, beautiful is limited by a whole bunch of things.

795
00:50:48,220 --> 00:50:52,420
Cause we're, you know, partly it's technological and partly with, you

796
00:50:52,420 --> 00:50:57,540
know, we're stupidly divisive, but, um, but there is, there's

797
00:50:57,540 --> 00:51:02,220
also possible that there's also a reality, which is one of the things

798
00:51:02,220 --> 00:51:07,220
that technology has been is of course, an increase in power towards desire,

799
00:51:07,220 --> 00:51:08,260
towards human desire.

800
00:51:08,260 --> 00:51:15,060
And that is represented in mythological stories where let's say technology

801
00:51:15,060 --> 00:51:18,180
is used to accomplish impossible desire, right?

802
00:51:18,180 --> 00:51:22,580
We have, you know, the story of the story of building the, the McCann,

803
00:51:22,580 --> 00:51:27,220
the bull around the king of Meno, the, the wife of the king of Meno's, you

804
00:51:27,220 --> 00:51:32,100
know, in order to be inseminated by, uh, by a bull, we have the story, the, the,

805
00:51:32,660 --> 00:51:37,780
we have the story of the, um, sorry, Frankenstein, et cetera, the story of

806
00:51:37,780 --> 00:51:42,420
the golem where we put our desire into this increased power.

807
00:51:42,420 --> 00:51:44,900
And then what happens is that we don't know our desires.

808
00:51:44,900 --> 00:51:48,580
That's one of the things that I've also been worried about in terms of AI is that

809
00:51:50,020 --> 00:51:55,140
we act, we have secret desires that enter into what we do that people

810
00:51:55,220 --> 00:51:56,660
aren't totally aware of.

811
00:51:57,220 --> 00:52:02,660
And as we increase in power, these systems, those desires,

812
00:52:03,380 --> 00:52:06,100
let's say the, the pot, like the idea, for example, of the possibility

813
00:52:06,100 --> 00:52:11,380
of having an AI friend and the idea that an AI friend would be the best friend

814
00:52:11,380 --> 00:52:15,220
you've ever had because that, that friend would be the nicest to you,

815
00:52:15,220 --> 00:52:17,940
would care the most about you, would do all those things.

816
00:52:17,940 --> 00:52:20,900
That would be an exact example of what I'm talking about,

817
00:52:20,900 --> 00:52:23,380
which is it's really the story of the genie, right?

818
00:52:23,380 --> 00:52:27,540
It's the story of the, the genie in the lamp where the genie says,

819
00:52:27,540 --> 00:52:28,580
what do you wish?

820
00:52:28,580 --> 00:52:31,940
And the, and the person, and I have unlimited power to give it to you.

821
00:52:31,940 --> 00:52:35,060
And so I give him my wish, but that wish has all these,

822
00:52:35,700 --> 00:52:39,140
these underlying implications that I don't understand all these underlying

823
00:52:39,140 --> 00:52:39,540
possibilities.

824
00:52:39,540 --> 00:52:44,340
Yeah, but the, the cool thing, the moral of almost all those stories is

825
00:52:45,140 --> 00:52:50,100
having unlimited wishes will be, lead to your downfall.

826
00:52:50,100 --> 00:52:55,540
And so humans, like, if you give, you know, a young person an unlimited amount

827
00:52:55,540 --> 00:52:59,460
of stuff to drink for, for six months, they're going to be falling down drunk

828
00:52:59,460 --> 00:53:01,540
and they're going to get over it, right?

829
00:53:01,540 --> 00:53:04,100
Having a friend that's always your friend, no matter what,

830
00:53:04,100 --> 00:53:05,300
it's probably going to get boring.

831
00:53:05,300 --> 00:53:09,460
Well, the, the, the literature on marital stability indicates that.

832
00:53:09,460 --> 00:53:14,500
So there's a, there's a sweet spot with regards to marital stability

833
00:53:14,500 --> 00:53:17,940
in terms of the ratio of negative to positive communication.

834
00:53:18,740 --> 00:53:24,180
So if on average, you receive five positive communications

835
00:53:24,180 --> 00:53:27,620
and one negative communication from your spouse,

836
00:53:27,620 --> 00:53:30,100
that's on the low threshold for stability.

837
00:53:30,660 --> 00:53:34,020
If it's four positive to one negative, you're headed for divorce.

838
00:53:34,020 --> 00:53:37,940
But interestingly enough, on the other end, there's a threshold as well,

839
00:53:37,940 --> 00:53:42,020
which is that if it exceeds 11 positive to one negative,

840
00:53:42,020 --> 00:53:43,780
you're also moving towards divorce.

841
00:53:44,580 --> 00:53:49,700
So there's, so, so, so there might be self-regulating mechanisms that,

842
00:53:49,700 --> 00:53:51,780
that would in sense take care of that.

843
00:53:51,780 --> 00:53:57,620
You might find a yes man AI friend extraordinarily boring, very, very rapidly.

844
00:53:57,620 --> 00:54:01,940
But as opposed to an AI friend that was interested in what you were interested in,

845
00:54:01,940 --> 00:54:03,060
it was actually interesting.

846
00:54:03,780 --> 00:54:06,580
Like, you know, we go through friends in the course of our lives,

847
00:54:06,580 --> 00:54:09,140
like different friends are interesting at different times.

848
00:54:09,140 --> 00:54:13,060
And some friends we grow with, and that continues to be really interesting

849
00:54:13,060 --> 00:54:14,020
for years and years.

850
00:54:14,020 --> 00:54:17,220
And other friends, you know, some people get stuck in their thing

851
00:54:17,220 --> 00:54:19,700
and then you've moved on or they've moved on or something.

852
00:54:19,700 --> 00:54:28,020
So, yeah, I tend to think of a world where there was more abundance

853
00:54:28,020 --> 00:54:33,860
and more possibilities and more interesting things to do is an interesting.

854
00:54:33,860 --> 00:54:34,500
Okay, okay.

855
00:54:34,500 --> 00:54:37,940
And modern society has let the human population,

856
00:54:37,940 --> 00:54:40,100
and some people think this is a bad thing, but I don't know.

857
00:54:40,100 --> 00:54:42,180
I'm a fan of it.

858
00:54:42,180 --> 00:54:46,900
You know, modern population has gone from tens of 200 million to billions of people.

859
00:54:47,620 --> 00:54:49,060
That's generally being a good thing.

860
00:54:49,060 --> 00:54:50,500
We're not running out of space.

861
00:54:50,500 --> 00:54:54,100
I've been in, you know, so some of your audience has probably been in an airplane.

862
00:54:54,100 --> 00:54:57,140
If you look out the window, the country is actually mostly empty.

863
00:54:57,860 --> 00:54:59,540
The oceans are mostly empty.

864
00:54:59,540 --> 00:55:04,100
Like, we're, we're weirdly good at polluting large areas.

865
00:55:04,100 --> 00:55:07,700
But as soon as we decide not to, we don't have to, like technology.

866
00:55:07,700 --> 00:55:11,860
Most, most of our, you know, energy pollution problems are technical.

867
00:55:12,180 --> 00:55:13,540
Like, we can stop polluting.

868
00:55:13,540 --> 00:55:14,900
Like, electric cars are great.

869
00:55:15,700 --> 00:55:19,620
So, so, so there's so many things that we could do technically.

870
00:55:20,660 --> 00:55:22,500
I forget the guy's name.

871
00:55:22,500 --> 00:55:26,020
He said that the earth could easily support a population of a trillion people.

872
00:55:26,900 --> 00:55:30,020
And trillion people would be a lot more people doing, you know, random stuff.

873
00:55:30,660 --> 00:55:35,460
And he didn't imagine that the future population would be a trillion humans and a trillion AIs,

874
00:55:35,460 --> 00:55:37,300
but it probably will be.

875
00:55:38,660 --> 00:55:40,980
So it will probably exist on multiple planets,

876
00:55:40,980 --> 00:55:43,700
which will be good the next time an asteroid shows up.

877
00:55:43,700 --> 00:55:47,620
So what do you think about, so, so one of the things that seems to be happening,

878
00:55:47,620 --> 00:55:49,380
tell me if, if you think I'm wrong here.

879
00:55:49,380 --> 00:55:53,380
I don't think it's germane to, and I just want to make the point of, you know,

880
00:55:53,380 --> 00:55:56,820
where we are compared to living in the Middle Ages, our lives are longer,

881
00:55:56,820 --> 00:56:00,260
our, our families are healthier, our children are more likely to survive.

882
00:56:00,900 --> 00:56:02,900
Like many, many good things happened.

883
00:56:03,700 --> 00:56:06,180
Like setting the clock back with and be good.

884
00:56:06,180 --> 00:56:10,580
And, you know, if we have some care and people who actually care about

885
00:56:10,580 --> 00:56:13,940
how culture interacts with technology for the next 50 years,

886
00:56:14,740 --> 00:56:19,380
you know, we'll get through this, hopefully more successful than we did the atomic bomb in the Cold War.

887
00:56:21,140 --> 00:56:24,020
But it's, it's a major change.

888
00:56:24,020 --> 00:56:30,180
I mean, this is like, like your worries are, you know, I mean, they're, they're relevant.

889
00:56:31,060 --> 00:56:37,700
But, you know, but also you're, Jonathan, your stories about how humans have faced abundance

890
00:56:37,700 --> 00:56:40,660
and faced evil kings and evil overlords.

891
00:56:40,660 --> 00:56:45,380
Like we have thousands of years of history of facing the challenge of the future

892
00:56:45,380 --> 00:56:48,740
and the challenge of things that cause radical change.

893
00:56:48,740 --> 00:56:49,220
Yeah.

894
00:56:49,220 --> 00:56:53,540
And you know, that's, that's very valuable information.

895
00:56:53,540 --> 00:56:57,540
But for the most part, nobody's succeeded by stopping change.

896
00:56:57,540 --> 00:57:05,460
They've succeeded by bringing to bear on the change our capability to self-regulate the balance.

897
00:57:06,180 --> 00:57:09,140
Like a good life isn't having as much gold as possible.

898
00:57:09,140 --> 00:57:10,180
It's a boring life.

899
00:57:10,180 --> 00:57:14,020
A good life is, you know, having some quality friends and doing what you want

900
00:57:14,020 --> 00:57:17,300
and having some, some insight in life.

901
00:57:17,300 --> 00:57:18,020
Yeah.

902
00:57:18,020 --> 00:57:19,380
And some optimal challenge.

903
00:57:20,340 --> 00:57:26,180
And, you know, and in a world where a larger percentage of people can have,

904
00:57:27,460 --> 00:57:31,860
well, live in relative abundance and have tools and opportunities, I think is a good thing.

905
00:57:31,860 --> 00:57:32,180
Yeah.

906
00:57:32,180 --> 00:57:34,420
And I don't, I don't want to pull back abundance.

907
00:57:34,420 --> 00:57:42,740
But what I have noticed is that, that our abundance brings a kind of nihilism to people.

908
00:57:42,740 --> 00:57:44,500
And I don't, like I said, I don't want to go back.

909
00:57:44,500 --> 00:57:47,380
I'm happy to live here and to have these, these tech things.

910
00:57:47,380 --> 00:57:54,740
But I, but I think it's something that I've also noticed that increase of, of the capacity to

911
00:57:56,340 --> 00:58:03,300
get your desires when that increases to a certain extent also leads to a kind of nihilism

912
00:58:03,300 --> 00:58:04,660
where exactly that.

913
00:58:04,660 --> 00:58:08,900
Well, I wonder, Jonathan, I wonder if that's partly, partly a consequence

914
00:58:10,020 --> 00:58:15,780
of the erroneous maximization of short-term desire.

915
00:58:16,340 --> 00:58:21,860
I mean, one of the things that you might think about that could be dangerous on the AI front is that

916
00:58:22,660 --> 00:58:29,940
we optimize the manner in which we, we interact with our electronic gadgets to capture short-term

917
00:58:29,940 --> 00:58:31,780
attention, right?

918
00:58:31,780 --> 00:58:36,420
Because there's a difference between getting what you want right now, right now, and getting

919
00:58:36,420 --> 00:58:41,140
what you need in some more mature sense across a reasonable span of time.

920
00:58:41,140 --> 00:58:45,060
And one of the things that does seem to be happening online, and I think it is driven by

921
00:58:45,060 --> 00:58:52,580
the development of AI systems, is that we're, we're assaulted by systems that parasitize our

922
00:58:52,580 --> 00:58:56,660
short-term attention and at the expense of longer-term attention.

923
00:58:56,660 --> 00:59:03,540
And if the AI systems emerge to optimize attentional grip, it isn't obvious to me that

924
00:59:03,540 --> 00:59:08,100
they're going to optimize for the attention that works over the medium to long-run, right?

925
00:59:08,100 --> 00:59:12,340
They're going to, they're going to be, they could conceivably maximize something like

926
00:59:12,340 --> 00:59:15,140
whim-centered existence.

927
00:59:15,140 --> 00:59:17,780
Yeah, because all of virality is based on that.

928
00:59:17,780 --> 00:59:22,900
All the social media networks are all based on this, on this reduction, this reduction of

929
00:59:22,900 --> 00:59:27,780
attention, this reduction of desire to, to reaching your, your rest, let's say, in that

930
00:59:27,780 --> 00:59:31,140
desire, right? The like, the click, all these things, they're...

931
00:59:31,140 --> 00:59:32,180
Yeah, now.

932
00:59:32,180 --> 00:59:32,900
Yeah, exactly.

933
00:59:32,900 --> 00:59:37,300
So, but, but that's something that, you know, so for reasons that are somewhat puzzling,

934
00:59:37,300 --> 00:59:43,060
but maybe not, you know, the business models around a lot of those interfaces are around,

935
00:59:43,940 --> 00:59:50,340
you know, the part, the users, the product, and, you know, the advertisers are trying to get

936
00:59:50,340 --> 00:59:50,980
your attention.

937
00:59:50,980 --> 00:59:51,780
Yeah, yeah.

938
00:59:51,780 --> 00:59:56,260
But that's something culture could regulate. We could decide that, no, we don't, we don't

939
00:59:56,260 --> 01:00:00,740
want tech platforms to be driven by advertising money. Like, that would be a smart decision,

940
01:00:00,740 --> 01:00:04,020
probably. And that could be a big change.

941
01:00:04,660 --> 01:00:06,340
And what would you see as an alternative?

942
01:00:06,340 --> 01:00:11,300
See, well, the problem with that might be that markets drive that in some sense, right?

943
01:00:11,300 --> 01:00:11,860
Yeah.

944
01:00:11,860 --> 01:00:13,620
And I know they're driving that in a short-term way.

945
01:00:13,620 --> 01:00:17,940
We can take steps, like, you know, at various times, you know, alcohol has been illegal.

946
01:00:17,940 --> 01:00:22,900
Like, you can, society can decide to regulate all kinds of things.

947
01:00:23,940 --> 01:00:27,460
And, you know, sometimes, some things need to be regulated and some things don't.

948
01:00:27,460 --> 01:00:31,780
Like, when you buy a hammer, you don't fight with your hammer for its attention, right?

949
01:00:31,780 --> 01:00:34,660
A hammer is a tool. You buy one when you need one.

950
01:00:35,540 --> 01:00:37,700
Nobody's marketing hammers to you.

951
01:00:37,700 --> 01:00:43,540
Like, like that, that has a relationship that's transactional to your purpose, right?

952
01:00:43,540 --> 01:00:44,020
Yeah, well.

953
01:00:44,020 --> 01:00:47,540
Our technology has become a thing where, I mean.

954
01:00:47,540 --> 01:00:54,260
But there's a relationship between human, let's say, high human goals, something like

955
01:00:54,260 --> 01:01:00,980
attention and status. And what we talked about, which is the idea of elevating something higher

956
01:01:00,980 --> 01:01:07,220
in order to see it as a model. See, these are where intelligence exists in the human person.

957
01:01:07,220 --> 01:01:14,260
So when we notice that in the systems, in the platforms, these are the aspects of intelligence

958
01:01:14,260 --> 01:01:20,820
which are being weaponized in some ways, not against us, but are just kind of being weaponized

959
01:01:20,820 --> 01:01:25,300
because they're the most beneficial at the short term to be able to generate our constant attention.

960
01:01:25,300 --> 01:01:30,020
And so what I mean is that that is what the AIs are made of, right?

961
01:01:30,020 --> 01:01:35,460
They're made of attention, prioritization, you know, good, bad.

962
01:01:35,460 --> 01:01:40,580
What is it that is worth putting energy into in order to predict towards a telos?

963
01:01:40,580 --> 01:01:46,900
And so I'm seeing that the idea that we could disconnect them suddenly seems very difficult to me.

964
01:01:48,260 --> 01:01:54,340
Yeah, so I'll give you two. First, I want to give an old example. So after World War II,

965
01:01:55,140 --> 01:02:00,740
America went through this amazing building boom of building suburbs. And the American dream was,

966
01:02:00,740 --> 01:02:05,620
you could have your own house, your own yard in the suburb with a good school, right?

967
01:02:05,620 --> 01:02:11,060
So in the 50s, 60s, early 70s, they were building like crazy. By the time I grew up,

968
01:02:11,060 --> 01:02:18,420
I lived in a suburb in dystopia, right? And we found that that as a goal wasn't a good thing

969
01:02:18,420 --> 01:02:27,380
because people ended up in houses separated from social structures and then new towns are built

970
01:02:27,380 --> 01:02:34,580
around like a hub with places to go and eat. So there was a good that was viewed in terms of

971
01:02:34,580 --> 01:02:40,820
opportunity and abundance, but it actually was a fail culturally. And then some places it modified

972
01:02:40,820 --> 01:02:47,060
and continues and some places are still dystopian, you know, suburban areas and some places people

973
01:02:47,060 --> 01:02:53,780
simply learn to live with it, right? So that has to do with attention, by the way. It has to do with

974
01:02:54,660 --> 01:03:01,540
a subsidiary hierarchy, like a hierarchy of attention, which is set up in a way in which all

975
01:03:01,540 --> 01:03:08,180
the levels can have room to exist, let's say. And so, you know, the new systems, the new way,

976
01:03:08,180 --> 01:03:13,220
let's say the new urbanist movement, similar to what you're talking about, that's what they've

977
01:03:13,220 --> 01:03:18,260
understood. It's like we need places of intimacy in terms of the house. We need places of communion

978
01:03:18,260 --> 01:03:25,220
in terms of parks and alleyways and buildings where we meet and a church, all these places that

979
01:03:25,220 --> 01:03:32,420
kind of manifest our community together. Yeah, so those existed coherently for long periods of time

980
01:03:32,420 --> 01:03:39,540
and then the abundance post-World War II and some ideas about like what life could be like

981
01:03:39,540 --> 01:03:46,260
caused this big change and that change satisfied some needs, people got houses, but broke community

982
01:03:46,260 --> 01:03:53,220
needs and then new sets of ideas about what's the synthesis, what's the possibility of having your own

983
01:03:53,220 --> 01:03:59,300
home, but also having community, not having to drive 15 minutes for every single thing and some

984
01:03:59,300 --> 01:04:04,100
people live in those worlds and some people don't. Do you think we'll be smart? So one of the problems

985
01:04:04,100 --> 01:04:07,060
is if we're there, we'll see. Well, why were we smart enough to solve some of those problems?

986
01:04:07,060 --> 01:04:11,380
Because we had 20 years, but now, because one of the things that's happening now is we're,

987
01:04:11,380 --> 01:04:17,940
as you pointed out earlier, is we're going to be producing equally revolutionary transformations,

988
01:04:17,940 --> 01:04:24,260
but at a much smaller scale of time. And so, Mike, one of the things I wonder about, I think it's

989
01:04:24,260 --> 01:04:34,500
driving some of the concerns in the conversation, is are we going to be intelligent enough to direct

990
01:04:34,500 --> 01:04:39,700
with regulation the transformations of technology as they start to accelerate? I mean, we've already,

991
01:04:39,700 --> 01:04:46,260
you look what's happened online, I mean, we've inadvertently, for example, radically magnified

992
01:04:46,260 --> 01:04:52,420
the voices of narcissists, psychopaths, and Machiavellians. And we've done that so intensely,

993
01:04:52,420 --> 01:04:59,140
partly, and I would say partly, as a consequence of AI mediation, that I think it's destabilizing

994
01:04:59,140 --> 01:05:04,020
the entire world. It's destabilizing part of it, like Scott Adams pointed out. You just block

995
01:05:04,020 --> 01:05:07,860
everybody that acts like that. I don't pay attention to people that talk like that.

996
01:05:08,500 --> 01:05:12,420
Yeah, but they seem to be raising the temperature. Well, there are still places that are sensitive

997
01:05:12,420 --> 01:05:19,460
to it. Like 10,000 people here can make a storm in some corporate person, fire somebody.

998
01:05:19,460 --> 01:05:24,660
But I think that's like, we're five years from that being over. Corporation will go 10,000 people

999
01:05:24,660 --> 01:05:28,820
out of 10 billion. Not a big deal. Okay, so you think at the moment that's a

1000
01:05:29,460 --> 01:05:37,380
learning moment that will re-regulate. What's natural to our children is so different than

1001
01:05:37,380 --> 01:05:41,620
was natural to us, but what was natural to us was very different from our parents.

1002
01:05:41,620 --> 01:05:46,020
So some changes get accepted generationally really fast.

1003
01:05:46,020 --> 01:05:51,060
So what's made you so optimistic? What do you mean optimistic?

1004
01:05:51,700 --> 01:05:56,340
Well, most of the things that you have said today, and maybe it's also because we're pushing you,

1005
01:05:56,340 --> 01:06:02,980
I mean, you really do. My nephew, Kyle, was a really smart, clever guy. He called me a

1006
01:06:03,700 --> 01:06:09,860
what did he call it, a cynical optimist. Like I believe in people.

1007
01:06:10,980 --> 01:06:15,460
Like I like people, but also people are complicated. They all got all kinds of nefarious goals.

1008
01:06:15,460 --> 01:06:20,660
Like I worry a lot more about people burning down the world than I do about artificial intelligence

1009
01:06:21,380 --> 01:06:26,500
just because, you know, people, well, you know, people, they're difficult.

1010
01:06:27,220 --> 01:06:33,140
Right. And, but the interesting thing is in aggregate, we mostly self-regulate.

1011
01:06:33,140 --> 01:06:38,100
And when things change, you have these dislocations. And then it's up to people who talk and think,

1012
01:06:38,100 --> 01:06:43,940
and while we're having this conversation, I suppose, to talk about how do we re-regulate this stuff.

1013
01:06:43,940 --> 01:06:49,540
Yeah. Well, because one of the things that the increase in power has done in terms of AI,

1014
01:06:49,540 --> 01:06:54,580
and you can see it with Google and you can see it online, is that there are certain people who

1015
01:06:54,580 --> 01:07:00,420
hold the keys, let's say, and then who hold the keys to what you see and what you don't see.

1016
01:07:00,420 --> 01:07:04,420
So you see that on Google, right? And you know it if you know what search is to make where you

1017
01:07:04,420 --> 01:07:11,140
realize that this is not, this is actually being directed by someone who now has huge amount of

1018
01:07:11,140 --> 01:07:18,900
power in order to direct my attention towards their ideological purpose. And so that's why,

1019
01:07:18,980 --> 01:07:26,980
like, I think that to me, I personally think it would, I always tend to see AI as an extension

1020
01:07:26,980 --> 01:07:32,500
of human power, even though there is this idea that it could somehow become totally independent.

1021
01:07:32,500 --> 01:07:39,460
I still tend to see it as an increase of the human care and whoever will be able to hold

1022
01:07:39,460 --> 01:07:45,380
the keys to that will have increase in power. And that can be like, and I think we're already

1023
01:07:45,380 --> 01:07:50,740
seeing it. Well, that's not really any different, though, is it, Jonathan, the situation that's

1024
01:07:50,740 --> 01:07:55,300
always confronted us in the past? I mean, we've always had to deal with the evil uncle of the

1025
01:07:55,300 --> 01:08:01,460
king, and we've always had to deal with the fact that an increase in ability could also produce

1026
01:08:01,460 --> 01:08:07,700
a commensurate increase in tyrannical power, right? I mean, so that might be magnified now,

1027
01:08:07,700 --> 01:08:13,700
and maybe the danger in some sense is more acute, but possibly the possibility is more

1028
01:08:13,700 --> 01:08:20,340
present as well. Well, because you can train an AI to find hate speech, right? You can train an AI

1029
01:08:20,340 --> 01:08:26,420
to find hate speech, and then to act on that hate speech immediately within, and now it's only,

1030
01:08:26,420 --> 01:08:31,220
we're not only talking about social media, but what we've seen is that that is now

1031
01:08:32,740 --> 01:08:37,700
encroaching into payment systems and into people losing their bank account, their access to

1032
01:08:37,700 --> 01:08:43,300
different services. And so this idea of optimization. Yeah, there's an Australian bank that already has

1033
01:08:43,300 --> 01:08:48,820
decided that it's a good thing to send all of their customers a carbon load report every month,

1034
01:08:49,780 --> 01:08:56,740
right? And to offer them hints about how they could reduce their polluting purchases, let's say.

1035
01:08:56,740 --> 01:09:02,260
And well, at the moment, that system is one of voluntary compliance, but you can certainly see

1036
01:09:02,260 --> 01:09:08,740
in a situation like the one we're in now that the line between voluntary compliance and involuntary

1037
01:09:08,740 --> 01:09:16,180
compulsion is very, very thin. Yeah, so I'd like to say, so during the early computer world,

1038
01:09:16,180 --> 01:09:21,060
computers were very big and expensive. And then they made many computers and workstations,

1039
01:09:21,060 --> 01:09:25,300
but they were still corporate only. And then the PC world came in. All of a sudden,

1040
01:09:25,300 --> 01:09:30,980
PCs put everybody online, everybody could suddenly see all kinds of stuff, and people

1041
01:09:30,980 --> 01:09:36,900
could get a Freedom of Information Act request, put it online somewhere, and 100,000 people could see

1042
01:09:36,900 --> 01:09:45,460
it. It was an amazing democratization moment. And then there was a similar, but smaller revolution

1043
01:09:45,460 --> 01:09:53,220
with the world of smartphones and apps. But then we've had a new completely different set of companies,

1044
01:09:53,220 --> 01:09:59,540
by the way, from what happened in the 60s, 70s, and 80s to today, it's very different companies that

1045
01:10:00,100 --> 01:10:06,100
control it. And there are people who are worried that AI will be a winner take all thing. Now,

1046
01:10:06,100 --> 01:10:10,180
I think so many people are using it, and they're working on it so many different places, and the

1047
01:10:10,180 --> 01:10:16,020
cost is going to come down so fast, that pretty soon you'll have your own AI app that you'll use to

1048
01:10:16,020 --> 01:10:23,940
mediate the internet to strip out the endless stream of ads. And you can say, well, is this story

1049
01:10:23,940 --> 01:10:29,300
objective? Well, here's the 15 stories, and this is being manipulated this way, and this is being

1050
01:10:29,300 --> 01:10:35,220
manipulated that way. And you can say, well, I want what's more like the real story. And the funny

1051
01:10:35,380 --> 01:10:44,100
thing is, information that's broadly distributed, and has lots of inputs, is very hard to fake the

1052
01:10:44,100 --> 01:10:50,260
whole thing. So right now, a story can pull through a major media outlet. And if they can control the

1053
01:10:50,260 --> 01:10:57,300
narrative, everybody gets to fake story. But if the media is distributed across a billion people,

1054
01:10:57,940 --> 01:11:04,260
who are all interacting in some useful way, somebody standing there, some, yeah, there's real

1055
01:11:04,260 --> 01:11:07,780
signal there, and if somebody stands up and says something that's not true, everybody goes,

1056
01:11:07,780 --> 01:11:16,740
everybody knows that's not true. So a good outcome with people thinking seriously would be the

1057
01:11:16,740 --> 01:11:23,060
democratization of information and objective facts in the same way. The same thing that happened with

1058
01:11:23,060 --> 01:11:32,500
PCs versus corporate central computers could happen again. The problem is that the increase in power,

1059
01:11:32,580 --> 01:11:39,460
the increase in power always creates the tooth at the same time. And so we saw that increase in

1060
01:11:39,460 --> 01:11:44,340
power creates first, or it depends in which direction it happens, it creates an increase in

1061
01:11:44,340 --> 01:11:50,100
decentralization, it increases in access, it creates all that. But then it also at the same time

1062
01:11:50,100 --> 01:11:57,380
creates the counter reaction, which is an increase in control and increase in centralization. And so

1063
01:11:57,700 --> 01:12:06,260
now, the more the power is, the more the waves will, the bigger the waves will be. And so the

1064
01:12:06,260 --> 01:12:14,180
image of the image that 1984 presented to us, of people going into newspapers and changing the

1065
01:12:15,300 --> 01:12:19,780
headlines and taking the pictures out and doing that, that now obviously can happen with just a

1066
01:12:19,780 --> 01:12:24,740
click. So you can click and you can change the past. You can change the past, you can change

1067
01:12:25,300 --> 01:12:30,660
facts about the world because they're all held online. And we've seen it happen obviously in

1068
01:12:30,660 --> 01:12:37,860
the media recently. So does decentralization win over centralization? How is that even possible,

1069
01:12:37,860 --> 01:12:44,580
it seems? I mean, and it's also interesting, when Amazon became a platform, suddenly any mom and

1070
01:12:44,580 --> 01:12:52,100
pop business could have Amazon, eBay, there's a bunch of platforms, which had an amazing impact

1071
01:12:53,060 --> 01:12:58,500
because any business could get to anybody. But then the platform itself started to control

1072
01:12:58,500 --> 01:13:04,980
the information flow. But at some point that will turn into people go, well, why am I letting

1073
01:13:04,980 --> 01:13:11,380
somebody control my information flow? And Amazon objectively doesn't really have any capability.

1074
01:13:13,540 --> 01:13:18,740
So like you point out, the waves are getting bigger, but they're real waves. It's the same with

1075
01:13:18,740 --> 01:13:25,620
information. Information is all online. It's also on a billion hard drives. So somebody says,

1076
01:13:25,620 --> 01:13:30,420
I'm going to raise the objective fact, a distributed information system would say,

1077
01:13:31,140 --> 01:13:35,300
go ahead and raise it anywhere you want. There's another 1,000 copies of it.

1078
01:13:36,900 --> 01:13:44,020
But again, this is where thinking people have to say, yeah, this is a serious problem.

1079
01:13:44,580 --> 01:13:49,300
Like if humans don't have anything to fight for, they get lazy and a little bit dopey,

1080
01:13:49,300 --> 01:13:57,940
in my view. We do have something to fight for. And that's worth talking about. What would a great

1081
01:13:57,940 --> 01:14:04,260
world with distributed, inhuman intelligence and artificial intelligence working together in a

1082
01:14:04,260 --> 01:14:13,540
collaborative way to create abundance and fairness and some better way at arriving at good decisions

1083
01:14:13,540 --> 01:14:19,220
than what the truth is. That would be a good thing. But it's not, well, we'll leave it to the

1084
01:14:19,220 --> 01:14:23,620
experts and then the experts will tell us what to do. That's a bad thing. So that's...

1085
01:14:24,660 --> 01:14:28,500
Well, so is it the model that you just laid out, which I think is very interesting?

1086
01:14:28,500 --> 01:14:32,660
I'm not so much optimistic about that. Well, it did happen on the computational front.

1087
01:14:33,220 --> 01:14:40,660
It happened a couple of times both directions. The PC revolution was amazing. And Microsoft

1088
01:14:40,660 --> 01:14:47,460
was a fantastic company. It enabled everybody to write a $10, $50 program to use. And then at

1089
01:14:47,460 --> 01:14:53,540
some point, they're also, let's say, a difficult program company. And they made money off a lot

1090
01:14:53,540 --> 01:14:58,020
of people and became extremely valuable. Now, for the most part, they haven't been that directional

1091
01:14:58,020 --> 01:15:02,980
on telling you what to do and think and how to do it. But they are a many-making company.

1092
01:15:04,580 --> 01:15:08,740
Apple created the App Store, which is great. But then they also take 30% of the App Store

1093
01:15:08,740 --> 01:15:12,980
profits and there's a whole section of the internet that's fighting with Apple about their

1094
01:15:12,980 --> 01:15:19,940
control of that platform. And in Europe, they've decided to regulate some of that,

1095
01:15:21,140 --> 01:15:25,940
that should be a conversation, that should be a social cultural conversation about how should

1096
01:15:25,940 --> 01:15:36,340
that work. So do you see the more likely, certainly the more desirable future is something like

1097
01:15:36,420 --> 01:15:43,780
a set of distributed AIs, many of which are under personal, in personal relationship in some sense,

1098
01:15:43,780 --> 01:15:48,100
the same way that we're in personal relationship with our phones and our computers. And that that

1099
01:15:48,100 --> 01:15:53,380
would give people the chance to fight back, so to speak against this. And there's lots of people

1100
01:15:53,380 --> 01:15:58,820
really interested in distributed platforms. And one of the interesting things about the AI world is,

1101
01:15:58,820 --> 01:16:04,100
you know, there's a company called OpenAI and they open source a lot of it. The AI research is

1102
01:16:04,100 --> 01:16:09,860
amazingly open. It's all done in public. People publish the new models all the time. You can try

1103
01:16:09,860 --> 01:16:16,020
them out. People, there's a lot of startups doing AI in all different kinds of places.

1104
01:16:17,140 --> 01:16:24,660
You know, it's a very curious phenomena. And it's kind of like a big, huge wave. It's not like a,

1105
01:16:25,380 --> 01:16:30,900
you can't stop a wave with your hand. Yeah. Well, when you think about the waves, there are two,

1106
01:16:30,900 --> 01:16:36,020
actually in the book of Revelation, which describes the end or describes the finality of all things

1107
01:16:36,020 --> 01:16:40,500
or the totality of all things is maybe a way for people who are more secular to kind of understand

1108
01:16:40,500 --> 01:16:46,740
it. And in that book, there are two images, interesting images about technology. One is

1109
01:16:46,740 --> 01:16:52,340
that there's a dragon that falls from the heavens and that dragon makes a beast. And then that beast

1110
01:16:52,340 --> 01:16:59,460
makes an image of the beast. And then the image speaks. And when the image speaks, then people

1111
01:16:59,460 --> 01:17:07,060
are so mesmerized by the speaking image that they worship the beast ultimately. So that is one

1112
01:17:07,060 --> 01:17:12,100
image of, let's say, making and technology and scripture in Revelation. But there's another

1113
01:17:12,100 --> 01:17:17,620
image, which is the image of the heavenly Jerusalem. And that image is more an image of balance. It's

1114
01:17:17,620 --> 01:17:23,380
an image of the city which comes down from heaven with a garden in the center and then becomes this

1115
01:17:23,380 --> 01:17:30,100
glorious city. And it says, the glory of all the kings is gathered into the city. So the glory of

1116
01:17:30,100 --> 01:17:37,300
all the nations is gathered into this city. So now you see a technology which is at the service of

1117
01:17:37,300 --> 01:17:42,500
human flourishing and takes the best of humans and brings it into itself in order to kind of

1118
01:17:42,500 --> 01:17:46,820
manifest. And it also has hierarchy, which means it has the natural at the center and then has the

1119
01:17:46,820 --> 01:17:52,020
artificial as serving the natural, you could say. So those two images seem to reflect these

1120
01:17:52,900 --> 01:17:59,620
two waves that we see. And this kind of idea of an artificial intelligence which will be ruling

1121
01:17:59,620 --> 01:18:05,300
over us or speaking over us. But there's a secret person controlling it, even in Revelation. It's

1122
01:18:05,300 --> 01:18:11,140
like, there's a beast controlling it and making it speak. So now we're mesmerized by it. And then

1123
01:18:11,140 --> 01:18:15,460
this other image. So I don't know, Jordan, if you ever thought about those two images in Revelation

1124
01:18:15,460 --> 01:18:22,660
as being related to technology, let's say. Well, I don't think I've thought about those two images

1125
01:18:22,660 --> 01:18:28,020
in the specific manner that you described. But I would say that the work that I've been doing,

1126
01:18:28,020 --> 01:18:33,540
and I think the work you've been doing too in the public front, reflects the dichotomy between

1127
01:18:33,540 --> 01:18:38,180
those images. And it's relevant to the points that Jim has been making. I mean, we are definitely

1128
01:18:38,180 --> 01:18:42,900
increasing our technological power. And you can imagine that that'll increase our capacity for

1129
01:18:42,900 --> 01:18:48,500
tyranny and also our capacity for abundance. And then the question becomes, what do we need to do

1130
01:18:48,500 --> 01:18:53,940
in order to increase the probability that we tilt the future towards Jerusalem and away from the

1131
01:18:53,940 --> 01:19:00,820
beast? And the reason that I've been concentrating on helping people bolster their individual

1132
01:19:00,820 --> 01:19:06,340
morality to the degree that I've managed that is because I think that whether the outcome is the

1133
01:19:06,340 --> 01:19:11,860
positive outcome, that in some sense Jim has been outlining or the negative outcomes that we've been

1134
01:19:11,940 --> 01:19:16,900
querying him about, I think that's going to be dependent on the individual ethical choices of

1135
01:19:16,900 --> 01:19:21,940
people at the individual level, but then cumulatively, right? So if we decide that we're

1136
01:19:21,940 --> 01:19:26,980
going to worship the image of the beast, so to speak, because we're mesmerized by our own reflection,

1137
01:19:26,980 --> 01:19:31,220
that's another way of thinking about it. And we want to be the victim of our own dark desires,

1138
01:19:31,220 --> 01:19:38,020
then the IA revolution is going to go very, very badly. But if we decide that we're going to aim up

1139
01:19:38,020 --> 01:19:42,500
in some positive way, and we make the right micro decisions, well, then maybe we can harness

1140
01:19:42,500 --> 01:19:47,620
this technology to produce a time of abundance in the manner that Jim is hopeful about.

1141
01:19:47,620 --> 01:19:55,220
Yeah. And let me make two funny points. So one is, I think there's going to be continuum,

1142
01:19:55,220 --> 01:20:02,340
like the word artificial intelligence won't actually make any sense. So humans collectively,

1143
01:20:02,420 --> 01:20:08,820
individuals know stuff, but collectively we know a lot more. And the thing that's really good is

1144
01:20:08,820 --> 01:20:17,140
in a diverse society with lots of people pursuing individual, interesting ideas, worlds, we have

1145
01:20:17,140 --> 01:20:26,740
a lot of things, and more people, more independence generates more diversity. And that's a good thing

1146
01:20:26,740 --> 01:20:32,500
where it's a totalitarian society where everybody's told to wear the same shirt. It's inherently

1147
01:20:32,500 --> 01:20:42,020
boring. The beast speaking through the monster is inherently dull. But in an intelligent world,

1148
01:20:42,020 --> 01:20:49,940
where not only can we have more intelligent things, but in some places go far beyond what most humans

1149
01:20:49,940 --> 01:20:59,380
are capable of in pursuit of interesting variety. And I believe the information, well,

1150
01:20:59,380 --> 01:21:06,260
let's say intelligence is essentially unlimited, right? And the unlimited intelligence won't be

1151
01:21:06,260 --> 01:21:11,780
this shiny thing that tells everybody what to do. That's sort of the opposite of interesting

1152
01:21:11,780 --> 01:21:18,580
intelligence. Interesting intelligence will be more diverse, not less diverse. That's a good future.

1153
01:21:20,180 --> 01:21:25,140
And your second description, that seems like a future we're working for and also we're fighting

1154
01:21:25,140 --> 01:21:32,660
for. And that means concrete things today. And also, it's a good conceptualization.

1155
01:21:34,020 --> 01:21:38,340
I see the messages as my kids are taught, don't have children and the world's going to end,

1156
01:21:38,340 --> 01:21:43,620
we're going to run out of everything, you're a bad person, why do you even exist? These messages

1157
01:21:43,620 --> 01:21:49,700
are terrible. The opposite is true. More people would be better. We live in a world

1158
01:21:50,420 --> 01:21:57,140
of potential abundance. It's right in front of us. There's so much energy available. It's just

1159
01:21:57,140 --> 01:22:03,460
amazing. It's possible to build technology without pollution consequences. That's called

1160
01:22:03,460 --> 01:22:10,500
externalizing costs. We know how to do that. We can have very good, clean technology. We can do

1161
01:22:10,500 --> 01:22:17,060
lots of interesting things. So if the goal is maximum diversity, then the line between human

1162
01:22:17,060 --> 01:22:22,900
intelligence, artificial intelligence that we draw, you'll see all these really interesting

1163
01:22:22,900 --> 01:22:27,140
partnerships and all kinds of things. And more people doing what they want, which is the world

1164
01:22:27,140 --> 01:22:35,060
I want to live in. To me, it seems like the question is going to be related to attention,

1165
01:22:35,060 --> 01:22:40,660
ultimately. That is, what are humans attending to at the highest? What is it that humans care for

1166
01:22:40,660 --> 01:22:46,740
in the highest? In some ways, you could say, what are humans worshiping? And

1167
01:22:46,740 --> 01:22:52,500
depending on what humans worship, then their actions will play out in the technology that

1168
01:22:52,500 --> 01:22:57,460
they're creating, in the increase in power that they're creating. And if we're guided by the

1169
01:22:57,460 --> 01:23:02,900
negative vision, the sort of thing that Jim laid out that is being talked to as children, you can

1170
01:23:02,900 --> 01:23:07,780
imagine that we're in for a pretty damn dismal future. Human beings are a cancer on the face of

1171
01:23:07,780 --> 01:23:12,820
the planet. There's too many of us. We have to accept top-down, compelled limits to growth.

1172
01:23:12,900 --> 01:23:17,780
There's not enough for everybody. A bunch of us have to go because there's too many people on

1173
01:23:17,780 --> 01:23:24,340
the planet. We have to raise up the price of energy so that we don't burn the planet up with

1174
01:23:24,340 --> 01:23:31,460
carbon dioxide pollution, et cetera. It's a pretty damn dismal view of the potential that's in front

1175
01:23:31,460 --> 01:23:39,140
of us. The world should be exciting and the future should be exciting. Well, we've been sitting here

1176
01:23:39,140 --> 01:23:45,380
for about 90 minutes, bandying back and forth both visions of abundance and visions of apocalypse.

1177
01:23:47,460 --> 01:23:52,180
I've been heartened, I would say, over the decades talking to Jim about what he's doing

1178
01:23:52,180 --> 01:23:55,700
on the technological front. And I think part of the reason I've been heartened is because

1179
01:23:56,340 --> 01:24:03,140
I do think that his vision is guided primarily by desire to help bring about something

1180
01:24:03,140 --> 01:24:07,540
approximating life more abundant. And I would rather see people on the AI front who are guided

1181
01:24:07,540 --> 01:24:12,740
by that vision working on this technology. But I also think it's useful to do what you

1182
01:24:12,740 --> 01:24:18,500
and I have been doing in this conversation, Jonathan, and acting in some sense as friendly

1183
01:24:18,500 --> 01:24:23,380
critics and hopefully learning something in the interim. Do you have anything you want to say

1184
01:24:23,380 --> 01:24:29,460
in conclusion? I just think that the question is linked very directly to what we've been talking

1185
01:24:29,460 --> 01:24:35,220
about now for several years, which is the question of attention, the question of what is the highest

1186
01:24:35,220 --> 01:24:40,660
attention. And I think the reason why I have more alarm, let's say, than Jim, is that I've

1187
01:24:40,660 --> 01:24:47,540
noticed that in some ways human beings have come to now, let's say, worship their own desires,

1188
01:24:47,540 --> 01:24:53,300
they've come to worship. And that even the strange thing of worshiping their own desires has actually

1189
01:24:53,300 --> 01:24:58,820
led to an anti-human narrative. This is a weird idea. It's almost suicidal desire that humans

1190
01:24:58,820 --> 01:25:03,060
have. And so I think that seeing all of that together in the increase of power,

1191
01:25:04,260 --> 01:25:11,060
I do worry that the image of the beast is closer to what will manifest itself. And I feel like

1192
01:25:11,060 --> 01:25:18,820
during COVID, that sense in me was accelerated tenfold in noticing to what extent technology

1193
01:25:18,820 --> 01:25:25,220
was used, especially in Canada, how technology was used to instigate something which looked like

1194
01:25:25,300 --> 01:25:30,180
authoritarian systems. And so I am worried about it. But I think like Jim, honestly,

1195
01:25:30,180 --> 01:25:34,420
although I say that, I do believe that in the end, truth wins. I do believe that in the end,

1196
01:25:35,620 --> 01:25:43,380
these things will level themselves out. But I think that because I see people rushing towards

1197
01:25:43,380 --> 01:25:50,900
AI almost like lemmings are going off a cliff, I feel like it is important to sound the alarm once

1198
01:25:50,900 --> 01:25:57,540
in a while and say, you know, we need to orient our desire before we go towards this extreme power.

1199
01:25:57,540 --> 01:26:01,780
So I think that that's mostly the thing that worries me the most and that preoccupies me the

1200
01:26:01,780 --> 01:26:06,580
most. But I think that ultimately in the end, I do share Jim's positive vision. And I do think that

1201
01:26:07,380 --> 01:26:12,100
I do believe the story has a happy ending. It's just you might have to go through hell before

1202
01:26:12,100 --> 01:26:18,820
we get there. I hope not. So Jim, how about you? What have you got to say in closing?

1203
01:26:18,820 --> 01:26:23,940
A couple of years ago, a friend who's, you know, my age said, oh, kids coming out of college,

1204
01:26:23,940 --> 01:26:27,620
they don't know anything anymore. They're lazy. And I thought, I work at Tesla. I was working

1205
01:26:27,620 --> 01:26:33,220
at Tesla at the time. And we hired kids out of college and they couldn't wait to make things.

1206
01:26:33,940 --> 01:26:40,340
They were like, it's a hands-on place. It's a great place. And I've told people, like, if you're

1207
01:26:40,340 --> 01:26:44,820
not in a place where you're doing stuff, it's growing, it's making things, you need to go somewhere

1208
01:26:44,820 --> 01:26:52,340
else. And also, I think you're right, the mindset of if people are feeling this is a productive,

1209
01:26:52,340 --> 01:26:57,380
creative technology that's really cool, they're going to go build cool stuff. And if they think

1210
01:26:57,380 --> 01:27:01,860
it's a shitty job and they're just tuning the algorithm so they can get more clicks,

1211
01:27:01,860 --> 01:27:09,540
they're going to make something beastly, you know, beastly, perhaps. And the stories, you know,

1212
01:27:09,620 --> 01:27:16,580
our cultural tradition is super useful, both cautionary and, you know, explanatory about

1213
01:27:16,580 --> 01:27:22,660
something good. Like, and I think it's up to us to go do something about this. And I know people

1214
01:27:22,660 --> 01:27:27,700
are working really hard to make, you know, the Internet a more open place to make sure information

1215
01:27:27,700 --> 01:27:35,060
is distributed, to make sure AI isn't a winter take-all thing. Like, these are real things and

1216
01:27:35,060 --> 01:27:40,420
people should be talking about them. And then they should be worrying. But the upside's really high.

1217
01:27:41,060 --> 01:27:47,380
And we faced these kind of technological, like, this is a big change. Like, AI is bigger than

1218
01:27:47,380 --> 01:27:53,940
the Internet. Like I've said, this publicly, like, the Internet was pretty big. And, you know,

1219
01:27:53,940 --> 01:28:02,500
this is bigger. It's true. But the possibilities are amazing. And so with some sense, we could

1220
01:28:02,820 --> 01:28:08,820
utilize them. Yeah, with some sense, we could achieve it. And the world is interesting.

1221
01:28:08,820 --> 01:28:13,460
Like, I think it'll be a more interesting place. Well, that's an extraordinarily

1222
01:28:13,460 --> 01:28:20,020
cynically optimistic place to end. I'd like to thank everybody who is watching and listening.

1223
01:28:20,020 --> 01:28:24,580
And thank you, Jonathan, for participating in the conversation. It's much appreciated as always.

1224
01:28:24,580 --> 01:28:29,860
I'm going to talk to Jim Keller for another half an hour on the Daily Wire Plus platform. I

1225
01:28:30,500 --> 01:28:35,220
use that extra half an hour to usually walk people through their biography. I'm very interested in

1226
01:28:35,220 --> 01:28:41,780
how people develop successful careers and lives and how their destiny unfolded in front of them.

1227
01:28:41,780 --> 01:28:46,980
And so for all of those of you who are watching and listening, who might be interested in that,

1228
01:28:46,980 --> 01:28:51,940
consider heading over to the Daily Wire Plus platform and partaking in that. And otherwise,

1229
01:28:51,940 --> 01:28:57,540
Jonathan, we'll see you in Miami in a month and a half to finish up the Exodus seminar.

1230
01:28:57,540 --> 01:29:04,500
We're going to release the first half of the Exodus seminar we recorded in Miami on November 25th,

1231
01:29:04,500 --> 01:29:08,420
by the way. So that looks like it's in the can. Yeah, I can't wait to see it.

1232
01:29:08,420 --> 01:29:13,940
The rest of you? Yeah. Yeah, absolutely. I'm really excited about it. And just for everyone

1233
01:29:13,940 --> 01:29:19,220
watching and listening, I brought a group of scholars together. About two and a half months

1234
01:29:19,220 --> 01:29:23,620
ago, we spent a week in Miami, some of the smartest people I could gather around me,

1235
01:29:23,700 --> 01:29:28,580
to walk through the book of Exodus. We only got through halfway, because it turns out there's

1236
01:29:28,580 --> 01:29:33,140
more information there than I had originally considered. But it went exceptionally well,

1237
01:29:33,140 --> 01:29:40,340
and I learned a lot. And Exodus means ex-hodos. That means the way forward. And well, that's very

1238
01:29:40,340 --> 01:29:45,540
much relevant to everyone today as we strive to find our way forward through all these complex

1239
01:29:45,540 --> 01:29:50,180
issues, such as the ones we were talking about today. So I would also encourage people to check

1240
01:29:50,180 --> 01:29:54,660
that out when it launches on November 25th. I learned more in that seminar than any seminar

1241
01:29:54,660 --> 01:29:58,580
I ever took in my life, I would say. So it was good to see you there. We'll see you in a month

1242
01:29:58,580 --> 01:30:02,580
and a half. Jim, we're going to talk a little bit more on the Daily Wear a Plus platform. And

1243
01:30:02,580 --> 01:30:07,780
I'm looking forward to meeting the rest of the people in your AI-oriented community tomorrow

1244
01:30:07,780 --> 01:30:13,140
and learning more about, well, what seems to be an optimistic version of a life more abundant.

1245
01:30:13,140 --> 01:30:18,020
And to all of you watching and listening, thank you very much. Your attention isn't taken for

1246
01:30:18,020 --> 01:30:23,060
granted, and it's much appreciated. Hello, everyone. I would encourage you to continue

1247
01:30:23,060 --> 01:30:28,980
listening to my conversation with my guest on DailyWirePlus.com.

