start	end	text
0	3520	The title does not mislead you. This is indeed a 12-step plan.
3520	8960	12 steps right here. You can count them 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, any with 12.
8960	12960	That starts from essentially basic machine learning and ends up with the goal of
13520	16560	basically the singularity, what they call intelligence amplification.
16560	19760	And gosh, was this a fun read and interesting to think about.
19760	23760	You might be wondering what random dude drank a bit too much one night and decided to write
23760	28160	down their brilliant plan for this. But as it turns out, this is not written by some random dude.
28160	32560	Rather, this is a new paper by Rich Sutton, Michael Bulling, and Patrick Polarski,
32560	36640	who are all well-established researchers that head the Alberta branch of D-Mind and
36640	41760	are also professors in the area. So yeah, I guess this is kind of unofficially a D-Mind paper.
41760	47440	And not only is this just some plan, it is now the plan for research at the Alberta branch of
47440	52400	D-Mind. Hence why it's called, you know, the Alberta plan. And it's just out there. It has some,
52400	56320	I think, really interesting ideas and no one's been talking about it. So today,
56320	61440	I hope to take you through the motivation for this and the plan itself. It is very interesting in
61440	66080	the sense that it's building back up from the foundations of machine learning and reinforcement
66080	71760	learning. And while it is very similar in most ways, it's also somewhat contrarian to many of
71760	76880	the approaches, especially in reinforcement learning that are popular today. I love this because if
76880	81600	you're a new graduate student or you can't find a research topic or you just want to get into research,
81600	86800	well, here's 12 research topics handed to you on a silver platter. You really can't ask for much
86800	90960	more. And it's also just, I personally think, interesting to know what some prominent researchers
90960	95120	are thinking about and how they're going about approaching these problems that lots of us are
95120	99600	very interested in. One last thing I'll mention before we dive into this is I cover lots of big
99600	103440	ideas on this channel to keep you up to date with what's going on in the field, but also smaller
103440	108320	ideas kind of like this kind of go under the radar to introduce you to some new interesting ideas. So
108320	111680	if that's the type of thing that interests you, sort of subscribing to the channel, it means a lot
111680	115600	and it really does help out. Anyway, let's dive into this. In this paper starts, we're going to
115600	118960	come back the first two paragraphs in a second. I just want to start with the third paragraph
118960	122560	because it really explains the core of what's going on here, what they're thinking. So starting
122560	128080	from here, following the Alberta plan, we seek to understand and create long lived computational
128080	134480	agents that interact with a vastly more complex world and come to predict and control their sensory
134480	139920	input signals and blah, blah, blah. They are as scalable as possible. If you've seen Rich Sutton's
139920	143200	better lesson, you know, maybe they're starting to look a little familiar and they have to adapt
143200	147920	and change the complexity, which means they have to continually learn. Big focus on this here. And
147920	151760	then another thing they mentioned is they must plan with a learned model of the world. Now I
151760	156080	highlighted these specific things, except for the word follow, but I highlighted these specific
156080	161040	things because these I think are things that are very core to this paper. So long lived, right,
161120	165920	and continuing learning. These are things that some people in our work on, but are not super
165920	171040	focused on that are really the sort of at the forefront of this paper. That means that agents
171040	175200	are going to have to be able to work with certain challenges that are present in continuing learning.
175200	180240	For example, if you try and take a neural network and have it continue to learn on a constantly
180240	185600	changing task, many people don't realize this, but what actually happens is neural networks get worse
185600	189600	and worse at performing or essentially learning new things, kind of like the, what do you say,
189600	194160	an old dog can't learn new tricks. There's actually some concept of that in I think machine
194160	198480	learning that lots of people are unfamiliar with. So things like that, those are issues that might
198480	203840	be important in this situation. Anything vastly more complex world predicting and controlling
203840	208160	sensory inputs. Some of these things are very like what you would assume is common, but you'll
208160	212160	see that everything works under these assumptions in this paper. And we'll get into some of the
212160	216880	more details of why I think some of these, you know, approaching these types of problems is
216880	221200	interesting. The one thing that does stick out here a little bit is I think planning with
221200	224960	a learning model of the world, because the other things are problems, right? Sort of having to
224960	228720	be long lived and continuing to learn these, you know, having a vastly more complex world,
228720	232720	these are statements about the problem. Whereas playing with a model is like a specific thing
232720	236800	they do. So I guess they just think it's that important, which I would agree. But anyway,
236800	241040	just some details about, you know, sort of the logistics of this plan before we maybe get into
241040	245760	it. This is a five to 10 year plan, as they mentioned here. They also, you know, some of this is
245760	249920	very like, you know, we think this will greatly affect the economy or society or individual
249920	254960	lives. I don't think we need to go through all the, all the philosophical stuff. We can kind of get
254960	259840	into the meat of this. So the purpose of giving this plan, which they write about here is, as they
259840	264320	say, it's twofold. The main one is, or one of the two is that they want to sort of give their vision
264320	268880	for AI. Well, lots of people have been working on these problems, like continue learning. They
268880	272080	think it should really be at the forefront. So some of those things are a little bit different. So
272080	276080	this is their vision, what problems they think should be focused on. And then also laying out,
276080	281920	of course, a research plan for the purpose, you know, of doing research. This is not a to-do list.
281920	287040	This is not like a things they completed. This is a research plan. They, they, as they very much
287040	291200	stay here, gaps and uncertainties, which are to be expected, right? When you're tackling these
291200	296560	big problems, when you have literally 12 steps, of course, it might not be enough. Maybe some
296560	299760	things aren't going to be necessary. Things are going to change throughout this plan. There are
299840	305040	some key points or some important things that they think sort of differentiates this Alberta plan
305040	309520	from lots of the work going on at other places. So that's a little bit what they talk about in
309520	314000	this next section. Ah, the other thing in this next section is this diagram that reminds me,
314000	319120	I forgot to say what RL is. So this is going to be just a five second explanation. If you don't
319120	323360	know what reinforcement learning is, just imagine you have some environment like the world, you have
323360	327440	an agent, you know, it sees things from the environment, gets observations, takes actions,
327440	332320	and it tries to maximize some reward. There's your five second explanation. Hopefully that keyed
332320	337040	anyone in it wasn't okay. Anyway, what are these core tenants that they think differentiate the
337040	342560	Alberta plan? So the first one, here it is, they say the first distinguishing feature is the emphasis
342560	349120	on ordinary experience as described above. So an ordinary experience, as they'll mention here,
349120	353840	this means that you can't do things like use specialized training sets. You can't use human,
353840	358640	like assistance, you know, you can't put human data into this, you can't access the internal
358640	362320	structure of the world, which lots of work and reinforcement learning does, right? Not everything,
362320	367520	but lots of it does. And I should say, maybe clarify, human assistance here does not mean that
367520	373280	an AI cannot interact with humans. It just means that once like the learning process has started,
373280	377120	you can't have a human like tweaking like different parameters and like, you know, maybe
377120	381200	training a bit like this and then a bit like this. It means that the learning process, you kind of
381200	386880	have this agent, it starts out and then it has to do everything by itself. And why is this so
386880	391360	important? And the idea here is that these types of methods that require human assistance, they
391360	396720	usually don't scale, where do they say this, they typically do not scale with computational resources,
396720	401680	and as such is not a research priority. Now, if you've read the bitter, the bitter lesson, that's
401680	406080	basically the, you know, this is the one paragraph version of this. So what's the next thing that
406080	410880	they put a focus on? The next thing is temporal uniformity. What does this mean? Essentially,
410960	415440	it means that at all time steps, so at all times, in terms of the algorithms running,
415440	419760	everything needs to be the same. You can't have, and this again, goes back to what I had mentioned
419760	425280	earlier, you can't have a separate set of training data and testing data like you would have in
425280	428960	supervised learning, or lots of people when they're evaluating reinforcement learning agents,
428960	433920	like on the Atari 100k benchmark, I think it's pretty common to train with 100,000 steps of data,
433920	437680	but then the actual testing is done on different data. So that would be a no, no here, right? And
437680	442240	you might be like, oh, that's heresy, no training and testing, no separation. Well, yeah, but you
442240	446080	have to remember, we're working in the continual learning setting, which means that you can just
446080	450640	keep simulating your environment and getting new experiences. And well, if all those experiences
450640	455200	are the same, well, then maybe your environment just isn't a very interesting environment at that
455200	458400	point. So this is essentially talked about in the paper here, you know, they say no special
458400	464800	training periods, if information is provided, then it has to be the same information on every
464880	470560	time step. Or if it learns to plan, then it has to plan on every time step. If the agent constructs
470560	475600	its own representations or subtasks, then the meta learned out are the meta algorithms for
475600	480640	constructing them and operating them operate on every time step. So you might be wondering like,
480640	485360	well, aren't on some time steps, maybe where things are more complicated, won't we maybe want to plan
485360	490320	more like look further into the future? Yeah, maybe we'll want to do that. But the idea here is that
490320	495600	should all be internal to the agent, as opposed for a human tweaking these parameters, right?
495600	500240	You could have a meta learning algorithm that learns those types of things. And in general,
500240	505360	I think this, why do they do this? I think they actually say it fairly well. Their focus on this
505360	511520	is to leads us to interesting non stationary continuing environments, and in algorithms
511520	516400	for continual learning and meta learning. So in one sense, I understand totally how this leads
516480	519680	them. I have way too much yellow on this page, I'll start using something else.
520560	524960	So algorithms for meta learning, this makes sense as I just mentioned, right? You need meta
524960	528080	learning when you can't have a human tweaking these things, you should have an algorithm to
528080	532560	that and meta learning is one way to do that. On the other hand, I don't see how this really leads
532560	536720	to continuing environments or like continual learning, I would think it would be the other way
536720	543120	around, right? You have continually like continual learning. And hence, you're going to want to focus
543120	546720	on temporal uniformity, but maybe there's something I'm not seeing here. And another reason
546720	550960	they mentioned this is that keeping everything temporarily uniform, it reduces the degrees of
550960	555520	freedom and shrinks the agent design space, which to be fair, they think there's a decent point,
555520	561840	really have plenty of things to look into. There's no reason to, you know, forcibly make that higher
561840	565840	when we could simplify the space. Then if we go down a bit more, this is one thing we touched on
565840	572160	earlier. And this is also from the bitter lesson. But the third distinguishing factor is it's
572240	578160	cognizance of computational considerations and Moore's law. I do think lots of other, and this
578160	582880	essentially, all this means, right, is compute computations getting stronger or more, we have
582880	587920	more of it, it's more efficient. So we want to be able to take, make use of that. So we can't have
587920	591840	anything that's not going to scale. And I think this has been getting better over the years,
591840	597040	my personal opinion, it looks like lots of people are working in areas that do or on methods that
597040	601840	do scale fairly well. Deep learning is one great example of this rate. And basically, everyone's
601840	607760	using deep learning now. Oh, they even namedropped the bitter lesson. Okay, lovely. So then let's
607760	611200	move on to the fourth. And I think this is the final one. The fourth distinguishing feature
611200	616800	is the focus on the special case, or is it the special case in which the environment includes
616800	622480	other intelligent agents. Now, what does this mean? It doesn't mean that there need to be other
622480	627280	intelligent agents, but rather that intelligent agents. So like if I'm, you know, modeling a
627280	631360	robot that's interacting with humans, you know, there's, there's nothing fancy that I do to model
631440	635040	the fact that there's humans in the world, rather, they're just part of the environment.
635040	639120	And I need to be cognizant, and I think they say this right here, you'd be cognizant that the
639120	642800	environment may behave differently. And this might sound weird, but the environment is essentially,
642800	647200	you can think of it as, you know, a living, breathing thing, or at least it could be in
647200	651920	response to your own actions, right? So what you do might make the environment behave differently.
651920	655520	And that's, you know, we shouldn't do anything special for these other agents. To be honest,
655520	658320	I don't think they even need to put this here. Because I think this follows under the bullet
658320	663680	point of using no human experience, right? The idea that we have other intelligent or like
663680	668080	conscious agents, that is something that we're sort of embedding our human biases into this,
668080	673760	right? Technically, we can't even prove other people are conscious. So I don't know, I don't
673760	678560	think this is so weird, but I guess lots of places that do work with multi-agent settings, they do
678560	683200	do this sort of thing where they're explicitly trying to model other agents. So that would be a
683200	687040	no-no in this sort of setting, or at least that's, that's what they want to work with.
687600	691840	So if we scroll down, we're almost to the research plan. I swear we're getting there.
691840	695280	We're going to spend a good bit of time on that once we get there. But the last thing we need to
695280	700560	touch on is the base agent. So I believe this is what they call the common model of the intelligent
700560	704720	agent, as you're looking at right here. And they say that this is used in a few things, like
705520	710560	areas like economics have similar ideas. But the idea is that we want to start from some base
710560	714400	that we can maybe agree on, not that it's necessarily the best or something like that,
714400	719920	but that this is a reasonable base that will inform the rest of the research. So what's
719920	722880	essentially happening here is if we step through this, we start with an observation from the
722880	727680	environment, pretty standard, then it goes into this perception. So perception, and this is going
727680	731920	to be a recurrent thing, right? You also take in the last action, and you can see the output
731920	736160	of perception feeds back into here. So this is recurrent. So this would be keeping what they
736160	740480	like to call the agent state, which is like the agents, some people call this the belief state.
740480	745360	It's maybe what the agent perceives and wants to remember about the world to do whatever it needs
745360	752000	to do. It has reactive policies. So the policies are what transform these observations into actions.
752000	756000	It has a value function. This is a normal value function in reinforcement learning, right?
756800	760720	It essentially assigns value to certain states, which helps you do things like credit assignment,
760720	765360	and it has a transition model, which allows you to do, as you can see, planning and planning
765360	769360	means that you can more efficiently, you know, imagine scenarios in your head, not have to play
769360	773920	through them in real life. And then that means that you can be a lot more sample efficient and
773920	778400	potentially have other benefits too. One question, and this is a question I used to have, is why have
778400	783120	these things? They are, they're one, two, three, four things. Why not other things, right? We could
783120	788720	have, for example, a different portion here saying we want to model like other agents. I mean, we just
788720	792560	said like this, this is not good, we shouldn't do this, but why not? Why are these other things
792560	797440	okay to have? And the difference here, I don't think they mentioned the paper. But the difference,
797440	802240	I believe they would say, is that these four things that they have here, they're not domain
802240	806320	specific. These should essentially work in any domain, they're general, they're very general,
806320	810320	whereas modeling other agents, well, you might not always need to do this in the way in which you
810320	814320	might do this will probably change drastically, depending on what types of agents you're working
814320	818720	with. So for that reason, these four things stay and other things don't quite make the cut. It has
818720	823680	to do with the generality of these four components. Another thing you might mention is that the year,
823760	828800	there are s's right here. So reactive policies, value functions, that is something we'll get into.
828800	832960	These don't necessarily need to have one policy or one value function, we can work with multiple.
832960	838000	And that is one of the steps of the plan that we'll get into. Speaking of the plan, we are finally
838000	843680	about there. So here we go. A roadmap to an AI prototype. So here are 12 steps. I'm going to
843680	847200	start off by just reading through them. And then they have bullet points for each of these that we
847200	851280	can go through and talk about all of them and more depth. Some of them I'll go over somewhat
851360	855120	quickly. Some of them I'll go over in a bit more depth because they have more written or I think
855120	858960	they're more interesting. But anyway, starting off with item number one, we have continual
858960	864560	supervised learning with given features, then two, supervised feature finding, three, continual
864560	868160	generalized value function prediction learning. If you don't know what these words mean, don't
868160	872880	worry, I'll go over all of it as we go through the individual points later. And four, continual
872880	879520	active credit control, five, average reward, GVF learning, six, continuing control problems, seven,
879600	885200	planning with average reward, eight, prototype AI one. Wow, incredible. One step model based
885200	890960	around with continual function approximation, nine, search control and exploration, 10, the stomp
890960	897280	progression, 11, oak and 12, intelligence amplification, or this is, I guess you could call
897280	902160	this the singularity, though to be fair, I guess in making AIs make themselves better does not
902160	906400	necessarily guarantee the singularity. So maybe that's a bit of a misnomer, but you know, it's
906400	912400	a bit more catchy. It's more of a buzzword, I guess. So you might notice that looking over these
912400	918080	points, Hey, this is just normal reinforcement learning. And to some extent, you wouldn't be
918080	922720	entirely wrong. These are very common things that we see in reinforcement learning, like
922720	927920	actor credit control. What's what's another thing model based around with continual function
927920	932320	approximation, like what this is supposed to be a generally I that can do all these things. And
932320	936320	that's what I meant when I was talking about at the beginning. These are very similar to what
936320	940160	lots of people are doing today. But what you'll notice as we go through the points is that sort of
940160	944720	the focus on the importance of what's important, that's what I think is a bit different, like the
944720	949200	importance, for example, on continual RL. So we'll get into this, this, this, I should say, this
949200	954160	isn't anything groundbreaking. It's more of just an interesting plan and an interesting way to go
954160	959120	about this or to think about going about it this way. So let's start with our first big step,
959120	964240	which is continual supervised learning with given features. So what let's start out with what this
964240	968320	means. This essentially means given features. So maybe you have cart pull, and cart pull is a problem
968320	972960	by the way, where you have this little cart, and it has a little pole on top of it. And the pole can
972960	977280	go back and forth. And you need to move the cart back and forth to bounce the pole, very simple
977280	981600	problem. So maybe for your features, you could have like the x, y values of this, and then the
981600	987200	angular velocity of this cart, or something like that. So those would be given features. And to
987200	991680	point out, you know, yes, we can already do things like this. But sort of the point I think of this
991760	997200	plan is to really revisit things in the simplest setting. So they split the explicitly say this,
997200	1002320	we want to go back to the simplest setting, and essentially try and make everything as good as
1002320	1006640	possible. Essentially try out everything that might have been overlooked, especially in these new
1006640	1011440	settings in the setting of continual learning, and say, can we do better? So some things they
1011440	1016000	mentioned that they might look into are how can we make things quicker? How can we train faster,
1016000	1020560	be more robust, be more efficient, while also continuing again, over long periods of time,
1020560	1026640	how can we do things like meta learn better representations? And that's be the most efficient.
1026640	1030000	So these are all questions that that would be asked, although I guess meta learning better
1030000	1034720	representations. I'm not sure if that's, I would think that would be in a different step. But,
1034720	1039280	but anyway, what are some examples of some ways, you know, you could look into this, like, what are
1039280	1042640	like, what can we really change when we have, you would think this is such a simple setting,
1042640	1045920	there's not really much we can do, but there actually are some things we can look at. So one
1045920	1049120	thing they mentioned is the global step size. So this is like your learning rate. Generally,
1049120	1052960	you have a global learning rate. Now, this is somewhat, but certain optimizers, like Adam,
1052960	1056640	that people have, some people have argued, I'm not sure, I don't know really the ins and outs of
1056640	1060080	this, but they've argued that those sorts of optimizers, while they're really good for supervised
1060080	1064000	learning, maybe aren't the best fit for reinforcement learning, because for reinforcement
1064000	1067920	learning, maybe you want to have different learning rates, depending on how important certain
1067920	1073120	features are for a certain task, or like what, you know, if you're trying to do different sorts
1073120	1076640	of tasks, there's different things you could think of. But this is like one thing you could
1076640	1082640	look at, right? Like, can you have a different step size for each single parameter? And how could
1082640	1086720	you do that in a way that makes sense for continual reinforcement learning and not, you know,
1086720	1091040	supervised learning, which is the context that most of these methods have been developed in.
1091040	1095120	And of course, I think I'm not going to find it right now in this, this C of text, but they also
1095120	1099200	mentioned that, you know, you don't want a human to be setting the learning rate. These are things
1099200	1104400	that should be meta learned. Again, right? We don't want really too much human experience. We don't
1104480	1108640	want that much fine tuning. We want the agent to be able to really do everything for itself. So
1108640	1113280	these are all things that yes, people are looking into them. But the idea here, again, is go back
1113280	1118080	to the simplest example, and get something, something working as best as possible. And then
1118080	1122480	we'll start to scale up and make things more complicated. So some other things they mentioned,
1122480	1126560	again, it's a bit too much here. Oh, here it is. So there's like normalizations of features, right?
1126560	1129920	Like, of course, we know that you can normalize features. And that tends to be a good thing in
1129920	1133760	machine learning. But when you're doing continual learning and reinforcement learning, does anything
1133760	1138240	change? You know, these are all things that could be reconsidered in this slightly different context
1138240	1143280	to maybe squeeze the most out of this that you can. So they go into this in a bit of depth.
1143280	1146720	Honestly, I'm going to skip over this because I don't think it's particularly important.
1146720	1150240	There's talking about examples of ways, you know, you can think about this, for example,
1150240	1154160	like the meta learning per weight step parameter. That's something I just mentioned. I think they
1154160	1158240	talk about this more in depth for this step, maybe because they already have some papers out about
1158240	1162960	this. But anyway, let's get on to step two. So this is supervised feature finding. So before
1162960	1167680	we had the features given to us, but now we actually want to find them, we want to generate
1167680	1172640	new features, right? And features, right, these are just, we get some observation, we have our
1172640	1177120	perception, remember from the base agent, now we want to use those to create new features to
1177120	1182160	essentially use things that will serve representations that will help us do whatever tasks we're
1182160	1186560	working on. Now, this is generally done via back propagation. And sorry, if you don't know what
1186560	1190240	back prop is, but it's a bit too much to explain this video, there's plenty of good things. There's
1190240	1195120	out there explaining it. And back prop, one issue actually with back prop is lots of people
1195120	1199840	aren't aware of this. It doesn't work super well for continual learning. I mean, maybe not super
1199840	1204800	well as in an overstatement, it does work. And it works fairly well, but it has an issue. It has
1204800	1210000	an issue that the more you train, if you train on and on and on and on and you just keep going,
1210000	1214320	it actually sort of does what they call sat and it gets saturated. I think they call like
1214400	1220960	interridger saturation. Individual nodes get saturated, which means they stop essentially
1220960	1225360	contributing. And over time, what you'll find is you'll see that maybe the performance of your
1225360	1229200	agent, it starts off low, it goes up and it gets good and it kind of blows out. Well, if the task
1229200	1233280	keeps changing, what you'll find is that the performance slowly starts to drop as the agent
1233280	1238080	is unable to adapt because the network has essentially gotten saturated. Or at least,
1238080	1242240	that's one explanation. I guess it's not fully explained yet. I actually explain this phenomenon
1242240	1247200	as well with there's a method called continue backprop or CBP for short. And this is from
1247200	1251760	Rich's lab, where they work on this problem. So this is one example of how supervised feature
1251760	1256960	finding actually needs to be adjusted to the given scenario, which is continual learning.
1256960	1260800	And this is one example of how although we have something that's really good already, we have
1260800	1265920	back propagation, it might not be the best thing to use, at least not this exact instance we use
1265920	1270560	of it right now, might not be the best thing for continual RL or something like that. One other
1270560	1274880	thing that I think is really interesting, and I found this to be always very enticing, but
1274880	1280080	sort of dangerous. I'm not dangerous and literally dangerous, but like dangerously enticing part of
1280080	1285520	the work that like Rich's lab does is the way they think about modeling, like the typical modeling
1285520	1290320	paradigm, their perspective is quite a bit different. So let me describe what I sort of think of as
1290320	1294960	the typical paradigm. So maybe I'll do that on the left here. So I think the typical paradigm of,
1294960	1298960	and this is not what all papers are about, but lots of them, one, you want to pick an architecture,
1299040	1301520	right? You have some problems. So you want to pick the architecture you're going to use. This
1301520	1305520	could be like a ResNet, right? You can use different ResNet layers. Maybe you want to use a
1305520	1310400	transformer. Maybe you want to use some sort of self attention. There's lots of different
1310400	1314880	architectures you could choose, right? You choose an architecture and then two, you choose an
1314880	1318880	objective function. So this objective function is going to be like your loss. Like what are you
1318880	1322960	trying to optimize here, especially in reinforcement learning, maybe this is usually going to be the
1322960	1327760	reward or the value or whatever. But this could be a number of things. It could be the MSC with
1328160	1331280	like an image. If you're trying to recreate an image or something like that. And then the third
1331280	1335840	and last thing you do is you pick an optimizer. So 99% of the time, this is going to be like
1335840	1341520	Adam or it's going to be RMS prop. But the thing is the way I find that within this paper, they talk
1341520	1345680	about these sorts of things and reading other papers from the same people, they tend to think
1345680	1350880	about these things a bit differently. Rather, they look at it from like a perspective of looking at
1350880	1355600	individual neurons and the interaction between these neurons. And what do I mean by this? Because
1355680	1360000	technically, like, you know, we're ending up using lots of the same things, but it is really just a
1360000	1364080	difference in perspective. So for example, we can say, how can the utility be assigned to all the
1364080	1369840	features that we're using? And then how could we make use of these utilities in the future, right?
1369840	1373680	So this is like a very much we're looking at a neuron neuron basis, trying to find the utility,
1373680	1377520	why might we want to do this sort of thing? Well, this could help us do things like evaluating
1377520	1382400	existent features and discarding less promising features, so as to make room for new ones. Well,
1382400	1386480	why don't we just keep expanding? Well, we have like a limited amount of computation here, right?
1386480	1390880	Even though we are constantly getting more, this is where like the big world hypothesis, as they
1390880	1395600	call it comes in, even though we have more and more compute, we also have just always a bigger
1395600	1399200	world so that we'll never be able to match it. So we are going to have to forget things, right?
1399200	1403360	So what do we forget? And that's kind of what this whole CBP thing I was just talking about.
1403360	1407680	This does something very similar to this. But beyond just dropping features, there are also
1407680	1411360	other things they talk about, you know, like initializing features. This is something you have
1411360	1414560	to do. Normally, you do it when you create your own network, you initialize all your features,
1414560	1418800	and then you just sort of train forever, right? But as it turns out, initialization, although
1418800	1422480	it's kind of brushed to the side, it's actually much more important than you might think. Oftentimes,
1422480	1426480	maybe not often, but occasionally, I'll implement a paper. And I've had this happen to me a couple
1426480	1430160	of times, we'll implement it, and it won't work. And I'll go back and read the paper very carefully
1430160	1434240	and realize, oh, they use this very specific form of initialization for their network,
1434240	1438240	and they try it, and suddenly everything's working. This has happened to me, especially
1438240	1442480	in papers that don't use backprop, but like use biologically inspired training. I found that
1442480	1446320	this is actually more important than you might expect. Another thing to talk about is like,
1446320	1451360	how should you adjust neurons or weights? One way to do this is obviously backprop. And I don't
1451360	1455040	think they're, they're definitely not proposing we just get rid of backprop. I think they realize
1455040	1460720	backprop scales very well. It works very well, but it might not be perfectly suited for a problem.
1460720	1465120	So how can we look at this sort of more, I don't know if I want to call like an individual neuron
1465120	1468800	approach, but something like that, more than neuron to neuron level, looking at them as features,
1468800	1473760	features with utilities and features that we want to drop or get rid of. How can we implement like
1473760	1478080	all these sorts of things? Like what, what would this look like in, in play? So maybe one example
1478080	1484000	of this, right? And this is very similar to what CBP does is say one, we would do an initialization
1484000	1488000	just as normal. We in it all our, our nodes, but this is one thing we could look into what are
1488000	1491760	better ways to in it for these types of things. The second thing we might want to do is use,
1491760	1495520	use backprop, right? We don't need to get rid of backprop. Backprop is strong, but as we use
1495520	1500880	backprop, maybe then we evaluate the utility. And then four, we remove, or maybe I should just say
1500880	1505040	we drop the nodes that are not super useful. And then the fifth thing we could do is we could
1505040	1510000	reinit, but instead of just re-init normally, maybe we could have some sort of meta learning
1510000	1514560	that figures out how to initialize nodes such that things will be learned faster, right?
1514560	1518160	So there's, there's some sort of meta stuff you could do here. So this would be one example.
1518160	1523120	And again, this is very similar. That's done what's done in CBP. I'll link my video to this
1523120	1527040	in the description. Actually, if you're interested in checking out, but this is like one alternative,
1527040	1530800	right? That still makes use of backprop, but might be better suited for something like
1530800	1535120	continue learning or reinforcement learning. And could give you more control over how you
1535120	1540320	go about developing these algorithms. As I just mentioned, I think this is a very exciting way
1540320	1545360	to think about how to model things, thinking of individual features. It's almost less limiting
1545360	1549760	the standard approach that I read out right here. But at the same time, it is dangerous because let's,
1549760	1555280	let's not kid ourselves, back propagation works very well. It scales very well. So when you do
1555280	1558640	want to do something different, you're going up against something that's already very well
1558640	1563200	established and is known to work very well. So that's why I say it's kind of dangerous. It's,
1563200	1568640	it's exciting, but hard to get working and not very explorative. It's a very underexplored area,
1568640	1573760	I think. Okay, step number three, continue GVF prediction learning. This is where we start getting
1573760	1578800	into non-IID data. So that's independent, individually distributed data. So when we're
1578800	1583680	working with reinforcement learning or in the real world, we are often, we're going to be working
1583680	1588320	with streams of data that come, you know, in, in a row. But one thing to note is that many
1588320	1595520	machine learning methods, the theory is entirely based upon data that is IID. Or in other words,
1595520	1599600	we are essentially breaking that when we move on to reinforcement learning most of the time.
1599600	1602880	That's why people like to use, or one of the reasons people like to use things like replay
1602880	1607840	buffers is because it helps you get this IID distribution, which can help you learn. Now,
1607840	1612240	we have methods of somewhat counteracting this, but it is nevertheless still an issue. But one
1612240	1616640	thing they also put in here is GVFs. I honestly don't know why they put this in here, but we can
1616640	1620880	talk about it because they do. I just don't know why they put them together. So GVFs, this is
1620880	1625200	generalized value function. And this is a fairly simple idea. If you're familiar with normal value
1625200	1630000	functions, essentially a value function measures like how good a state is, like what the expected
1630080	1634400	reward is. Whereas a generalized value function, well, it's essentially just a value function
1634400	1638720	for something other than the reward, or it could be the reward, or any other feature, right? So
1638720	1643280	you're predicting something. So they're just predictions or predictions about the world
1643280	1648160	that are not reward based, not necessarily reward based. The last thing I'll mention about this
1648160	1652320	step three is this is where perception comes in. And I think the reason they say this is where
1652320	1656720	perception really starts to come in is because of the non IID data. That means if we have like
1656720	1660560	sequential data, and we need to remember stuff that happened in the past, well, part of our
1660560	1664000	perception needs to be remembering the important thing we need to know what's important, like if
1664000	1668480	there's a key over there that I can't see anymore, I need to remember it's there so I can, you know,
1668480	1672080	go back and get it if I see a locked door or something like that. The fourth thing we've been
1672080	1677520	on now is continual actor critic control. And that is right up until this step. You might not
1677520	1681520	have noticed there was no control involved. We're not actually interacting with the environment,
1681520	1685200	but rather up to this point, we were just predicting what was happening. There's really
1685200	1688880	not much more to say here. Now that we're bringing control in the mix, we need to figure out how to
1688880	1694480	get that working with everything we've done so far. So then step five, average reward GVF learning.
1694480	1699120	Now, average reward is I think something lots of people are not very familiar with. Understandably,
1699120	1705920	so it's it's not very popular, I guess you could say. And if you've never thought about it,
1705920	1709520	using something like a discount factor and for those that aren't familiar with reinforcement
1709520	1714320	learning, usually you have this gamma parameter. And this is called the discount factor. And
1714320	1719600	essentially it weighs how important current rewards are versus future rewards. And it's,
1719600	1722720	if you think about it, it's kind of weird. The other thing that we could do, as you see right
1722720	1726720	here is just have an average reward and say we want to maximize the average reward, which would
1726720	1731440	essentially be in the same as saying we just want to maximize the reward total over everything.
1731440	1735440	The thing is, when you do something like having gamma, you're really, I actually don't want to
1735440	1740320	get into this too much. But if you've ever thought about it, it's kind of janky, right? And the argument
1740320	1744640	for why to use average reward instead is not just the jankiness, but I don't want to get into
1744640	1748000	it too much. So if you're interested, there is a whole section on this in the RL book that you
1748000	1753520	can check out. So step six, and this is going to be kind of cut us off at the first half of this
1753520	1759280	plan is the continuing control problems. Essentially, all this is going to say is we've
1759280	1765200	essentially created up until this point, a, a model free RL agent. So essentially the point
1765200	1769600	for this step is really, we just want to combine everything we've learned of the average reward
1769600	1773760	stuff, the generating new feature stuff. And we want to try it out in a bunch of continuing
1773760	1778320	environments, maybe make some more like new continuing environments, because most environments
1778320	1782640	are episodic, like the open AI gym stuff they mentioned, it's mostly episodic, but you could
1782640	1787760	convert it to continuing versions. And then we want to try the combination of everything we've
1787760	1793760	worked for so far and see how we're stacking up. Because remember, this is not like a one shot plan,
1793760	1798240	it will need to be adjusted as you go. So this would be a good point maybe to revise,
1798240	1802640	see how things are doing, a workout, anything that's maybe not working as expected. So then
1802640	1806240	when we're not going to jump just to the seventh step, this, this is kind of wraps up the first
1806240	1810880	part of this. Once we have this done, we have a, I love how they say it, a more continual, true,
1810880	1816560	it's a more continual model free learning method, which is good. We have like our first leg base.
1816560	1820400	This is where I guess after this point, we can say we have something good now. Now it's time to
1820400	1826480	start making it better. So what they focus on from here is primarily model based RL and things
1826480	1831360	start to get a little bit more complicated. Ideas, I will say from the seventh step onward,
1831360	1835600	tend to also be a little bit higher level without as many details, which I think is because, you
1835600	1838720	know, they're later in the plan less certain about how these things will go because they're
1838720	1845040	further out. So step seven is planning with average reward. So planning is very much done
1845040	1850960	in RL today. And there are lots of great examples of this. For example, you can look at Mu zero.
1850960	1854880	Mu zero is a great example of planning where the results are pretty incredible. There are some
1854880	1858800	differences here, right? It will again be in the continual setting will want to do this with
1858800	1864320	average reward. And I can say just right now, there are a lot of problems in planning that are
1864320	1868640	still unsolved. And it's very like not clear how to do things like what's what's the best way of
1868640	1873760	going about things. You also have like other alternatives to Mu zero, like dreamer dreamer is
1873760	1877440	dreamer v2 also works very well, you know, what's what's the best way to go about planning in this
1877440	1883360	setting? Who knows. So then step eight is prototype AI one. I love that. I got to love that prototype
1883440	1888000	AI one one step model based RL with continual function approximation. So this is actually
1888000	1893360	learning a model now, right? You can do planning if you have a pre given model, which I think is
1893360	1897520	what they entailed for step seven. But now that we have model based RL, that means we're going to
1897520	1902480	want to learn a model and then maybe do some sort of planning or something like that within that
1902480	1906880	model. So a one step planning model, they actually give a few steps down here that we can talk about.
1906880	1911520	So some things they want it to include. So one is a recursive update function or a perception
1911520	1916320	process. So right, we had that and this essentially makes everything more efficient. Instead of having
1916320	1921760	to like relearn the perception and the value function, the world model, the policy can have one
1921760	1926960	shared perception function that learns some sort of useful representation and knows how to store
1926960	1931920	memory of things that are important and that sort of thing. We also need a one step environment
1931920	1936960	model. They say this presumably be an expectation model or sample model or something in between.
1936960	1940960	I'm pretty sure it would not end up being an expectation model. This is I'm saying this because
1940960	1945120	this is what I do in my own work and expectation models are kind of awful. They're really easy
1945120	1949920	to learn, but they're they're not very good. So sample models are one thing they don't mention
1949920	1954160	a distribution model here. But I think that's another possibility, although there's no reason
1954160	1957680	to think a sample model couldn't also work. So and by the way, if you're unfamiliar with this,
1957680	1961680	this is just the idea that we want to predict the next step, what's going to happen next, right?
1961680	1965600	Because if we can predict what's going to happen next, then we can update based on our, you know,
1965600	1969840	sort of visions in our head, like our it's kind of like humans. I think when you go to sleep,
1969840	1974240	something like this happens, right? You get replayed memories and those can help you learn. So
1974240	1979680	so that's kind of what happens with world modeling and planning. What else is happening? So feature
1979680	1984640	finding as in step two, then importance feedback from the model. So this is now essentially tying
1984640	1989440	steps to and this planning together, right? If we're doing plain, we should be able to use that
1989440	1993440	plane to go back and change our actual features or how we're learning our features and improve them
1993440	1998160	in some way. The other idea, sorry, not other idea, but the final step here is a ranking of
1998160	2002560	features used for both feature finding and to determine which features are included in the
2002560	2008160	environment model. Since step D is essentially ranking features for feature finding, determining
2008160	2012000	what features are used in the environment model, because we might not need to use everything. If
2012000	2015840	you've looked at like what's been going on with this recently, instead of actually trying to predict
2015840	2019440	the future, lots of people are doing what's it called value equivalent models, where it doesn't
2019440	2022720	actually matter what the model predicts, so long as it's getting the right value. So it might be
2022720	2027440	predicting, you know, it might be leaving things out that are not important or stuff like that.
2027440	2031760	And then an influence of model learning and planning on feature ranking. So I think they
2031760	2036720	just kind of stated this. But anyway, I don't want to go maybe through these individual points.
2036720	2040720	They're kind of weird. Maybe tie in all these together. What are they talking about, especially
2040720	2045280	in these later few points? And the idea for this step eight is I think really that they want to
2045280	2052160	bring a full integration circle between the policy value function and the model components, right?
2052160	2056080	The idea that whenever you're doing any one of these things, when you're learning a value function,
2056080	2060720	when you're learning how to plan, the planning should essentially help you make better features,
2060720	2064960	which should then in turn help you make a better model. So all these different components, and
2064960	2069040	there's too many, there would be too many errors to draw this out in the like common model we saw
2069040	2072640	before. But the idea is that all these different parts should be affecting each other. And I do
2072640	2077840	think that this is one interesting thing that is not like too out there at all, but it's one thing
2077840	2083360	that is not fully done. Now in things like Mu zero, what they essentially do is they they have their
2083360	2087760	input state, they pass this through like a representation function. So they get there,
2087760	2092160	maybe I should call this the observation, this is like the state, then they do like the planning.
2092160	2096640	So this is called like the dynamics model to get the next state. And then from here, you go up and
2096640	2101440	you predict the policy and the value function. And then, and they do some Monte Carlo tree. So it
2101440	2106080	gets very complicated, right? But you have this whole system, and it's trained from end to end.
2106080	2112480	So when you actually train things go back like this, and you sort of update everything in one go.
2112480	2116640	So this is one type of feedback, but it's far from the only type of feedback that could be used,
2116640	2120400	right? There could be other types of feedback between these different components. Maybe again,
2120400	2124560	these are all things that could potentially be meta learned to. And those other types of
2124560	2128720	feedbacks might help these systems be more efficient, which would certainly always be great.
2128720	2134640	We head down to step nine, we have search control, and exploration, just to sum this up this,
2134640	2138800	there's not too much in this. But essentially, the idea here is that we want to make things
2138800	2143200	more efficient and a bit better when we're doing search. So search would be done in something
2143200	2147360	like planning, right? We're trying to look at the future, see what could happen. We want to see,
2147360	2150560	like, you know, what, if we take this action, what are all the different things that could happen?
2151280	2156560	Or also when we're updating over, like, you know, maybe we want to explore in a certain way that
2156560	2160800	helps us learn what we're missing, like fill out the gaps in our knowledge. So different ways you
2160800	2164560	could, you know, different things you could look into or like prioritize sweeping is one,
2164560	2167920	that's where you, I guess, like look at certain states where you have the least
2167920	2171120	knowledge of what will happen. But there's also a difference between, like, you know,
2171120	2174800	instead of using Monte Carlo tree search, there's also different types of heuristic search. Now,
2174800	2179360	I don't think they would just use heuristic search, because that seems like it uses a bit too much
2179360	2183840	human experience. But perhaps the model could, could meta learn, I say meta learn too much,
2183840	2187840	it could learn some heuristics for this, right? And that would be another way to go about planning.
2187840	2193280	So that's what that is all about. And next is the stump progression. So stump, I believe,
2193360	2199440	do they write it here? Yes, they do. So it stands for subtask, option, model, and planning. We've
2199440	2202640	already talked about most of these, you can probably infer what a subtask is, right? It's the
2202640	2207600	idea that you might want to have other tasks, other than the main task, the agent can, can work on.
2207600	2212240	And of course, the agent would learn these tasks themselves, as opposed to like a human, a human
2212240	2216720	doing these. So this is like one, one differentiation, one point they mentioned earlier, right? We don't
2216720	2222000	want, and this is what a lot of people do now is if they want to learn like skills. So a skill is
2222560	2226480	maybe, I don't know, you're going to drive a car and one skill is like turning the handle left
2226480	2229680	or right. You have to move a lot of muscles in your arms. So if you have the steering wheel,
2229680	2234240	how does, how do these normally look? I don't think this is close enough. If you want to move this
2234240	2238000	left and right, you actually have to move a lot of muscles in your arm to get that to work, right?
2238000	2242000	But for humans, it's very easy, because we've, we essentially know how to move our arms in
2242000	2247760	certain ways. We have skills or options are the more general form of this. That's when option is
2247760	2251760	in a subtask. If I'm learning how to live a good life, learning to drive is going to be one
2251760	2257120	subtask. So that's important for me to master, right? So how can the agent pose its own subtasks
2257120	2261520	and how can it learn options that solve those subtasks? And then how can we combine those?
2261520	2264880	Or sorry, I'm getting a bit ahead of myself. Combining them is the next step. What they
2264880	2268800	essentially propose, I'm not sure, I forget if they propose it here, but the idea is that they
2268800	2273520	want to have GVFs. So they want to be predicting things or have certain features, right? So feature
2273520	2278080	one, feature two, feature three, that tells things about the environment. And then they want to pose
2278080	2283280	GVFs. So being able to predict these things and also control them. So say, how can we maximize
2283280	2287280	feature one? How can we maximize feature two or feature three, and then learn options for each
2287280	2291920	of those. So essentially what you're doing is you're learning how to control your environment
2291920	2296000	in ways that are not just trying to maximize reward. And then maybe these options that you've
2296000	2300640	learned could be reused later. They actually have a paper on this called, I think, Stomp or
2300640	2305520	something like that. So if you're interested in that, I do encourage you to check that out. So then
2305520	2312080	step 11 is oak. So oak is, it stands for another thing. I get, I'm forgetting, I think options,
2312080	2316080	action knowledge or something like that. That might not be exactly right here. They actually
2316080	2320720	introduce option keyboard. Now I'm surprised they mentioned this because the other steps don't
2320720	2325360	really go in depth. Whereas the options keyboard is a very specific thing. There's a paper out
2325360	2329280	about it. They link to it. I think it's a really interesting paper. And the idea is that let's
2329280	2334640	say you have a set of options. So maybe option number one knows how to fish. And option number
2334640	2339840	two knows how to use a computer. Now, I don't know why you would ever want to do these two things
2339840	2344960	at, at the same time, but I know I hate fishing and I find it incredibly boring. So if I was going
2344960	2349360	to fish and I had a computer, well, honestly, I'd probably enjoy the nature, but you know, you could
2349360	2354240	do both at one time, right? There's no reason to say you can only do one fishing or using your
2354240	2359040	computer. Choose you could do both at the same time. If you want to weather or not, it's a good
2359040	2363520	idea. So and that's the idea of an option keyboard where you can essentially specify how much you
2363600	2367760	want to do this option or that option. And instead of having to learn a billion different
2367760	2372320	options to do all the different things you do, well, if you learn a good set of base options,
2372320	2377040	now you can combine them and get massively more expressive options that are just combinations
2377040	2381280	of others instead of having to learn those explicitly. And then we get to the last step,
2381280	2386640	intelligence amplification on how far we've come. So intelligence amplification is I think what most
2386640	2391520	people think of when they think of the singularity. It's essentially the idea that now we have our
2391600	2398240	prototype AI number two, and it should be able to do things very well. They describe this in
2398240	2402720	some interesting ways. So like there's an exo cerebellum, which is, you know, they talk about
2402720	2405760	these things, but essentially the core of what they're getting at is really at the end here,
2405760	2410160	where it says an intelligent application agent to perform policies and use planning to
2411840	2417360	multiplicatively enhance the intelligence of another partner agent or part of a single agent,
2417440	2421680	or I guess they could also change themselves, right? So we see these two versions being studied
2421680	2427040	in both human agents and agent to agent interaction settings. So essentially the idea that you have
2427040	2433440	this one agent right here, and then you have agent, agent two right here. And maybe agent two is
2433440	2439360	as much bigger brain, much smarter, you know, great brain drawing for me. So what agent two can do
2439360	2443440	is it can go to agent one and say, I'm gonna make you smarter. And then it makes agent one
2443440	2447840	better because remember it's a machine, it can edit its code. And then agent one is like, oh,
2447840	2452320	thanks for making me smarter. Now I'm going to go make you smarter. Or of course, maybe you could
2452320	2456720	just have a single agent, reperforming this on itself, or maybe there's some risk there because
2456720	2461040	it could mess itself up. Maybe that's why they talk about having two different agents. But anyway,
2461040	2465280	I mean, this is the idea, right, of how you get better and better agents. At some point,
2465280	2469280	you have an agent that is just better than humans at producing these sorts of, you know,
2469280	2475600	AI agents. And that's when we can get this sort of multiplicative scaling. Now, I think if you're
2475600	2480320	like me, you might be thinking, wait, wait, step 11 was talking about an options keeper. And in
2480320	2487440	step 12, we're talking about the singularity. Yeah, it's a bit of a jump, right? It's a bit weird.
2488000	2493280	But that's one things I'm going to be honest, I find interesting, but I'm uncertain about in this
2493280	2499520	paper. The fact that everything they outline up to step 11, for the most part, is within very
2499520	2504640	reasonable expectations. It's like what you would expect, but with a different focus. What I find
2504640	2511120	interesting is that they think that we can go from step 11 to step 12. I'm not sure how much
2511120	2515840	effort they think it will take, but essentially, that they think not much will be missing at that
2515840	2521120	point. And it is interesting to think about. On one hand, I'm very inclined to say, no, of course,
2521200	2524080	that's not going to be enough. We already have most of the things they talked about in these
2524080	2528240	previous steps. But on the other hand, we don't actually have the things they mentioned in these
2528240	2533840	previous steps. We have usually, for lots of these, we have specific instances, right? So for
2533840	2538320	like planning, we have something like Mu zero, but we still have so much more planning to explore.
2538320	2543360	There's so many different things we could try. And if everything below the planning level, like if
2543360	2547680	if we have incredibly good ways to learn representations, and if we have incredibly
2547680	2552720	more efficient ways to train your own networks for continuing learning problems, maybe things will
2552720	2558640	go a lot better. It's really hard to say. And to be honest, I would cut the authors of this
2558640	2561760	some slack. I don't think, you know, they don't think that they're just going to go through this
2561760	2565600	plan and suddenly hit step 12 and everything's going to work out. I think they're probably,
2565600	2568640	they probably realize, you know, they have to revise this. I think they even mentioned, yeah,
2568640	2573200	this is provisional. Oh, not crossing this out. It's not not provisional. It is provisional.
2573280	2578320	It's a draft and a working plan. It's going to be revised. But I do think it's interesting that
2578320	2581920	someone like Rich Sutton, who's worked in the field for a long time, has had some really good
2581920	2587200	ideas, thinks that this will be enough. And to be honest, I mean, I don't think I could come up
2587200	2591200	with a better plan myself. And I'm not sure quite what's missing here. I guess what's missing are
2591200	2595840	obviously the details that you have to fill out, right? Like meta learning, there's a million
2595840	2599280	bazillion ways to do meta learning. How are they going to do it? What's the way to, I don't know,
2599280	2604000	who knows? And that's, that's the thing, right? This is at the end of the day, it's a research plan.
2604000	2608160	It talks about the things they want to focus on, not how to do them. And it is very vague in that
2608160	2612800	sense. So overall, those are kind of my thoughts. I really like this. I think it's interesting to
2612800	2618560	read. I think it's very familiar, well, at the same time being somewhat fairly different from what
2618560	2622640	people, it's what, or rather, I should say, it's almost the same thing that people are working on,
2622640	2627440	but with a different problem in mind, different sort of problem setting. And I think interesting
2627440	2632080	differences could arise from that. I could certainly see people having a wide variety
2632080	2637680	of reactions to this. Some people saying this is completely useless. It's not detailed enough at all.
2637680	2641520	I could see other people saying, Oh, this is very interesting. I could see some people saying, Oh,
2641520	2646560	this is, this is the next step in the future. I really don't know. I'm very curious. So let me
2646560	2652000	know what you think in the comments. I'm, I think we'll have some very different opinions. I am
2652000	2656720	excited to say, if you've enjoyed this, do consider subscribing to the channel, maybe check it out
2656720	2660400	some of my other videos. I would really appreciate it. It really helps out. And hopefully you'll
2660400	2665040	find some other interesting content. Anyway, thank you so much for joining me and I hope to catch you
2665040	2665680	next time.
