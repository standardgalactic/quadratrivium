WEBVTT

00:00.000 --> 00:04.720
I think it's the dream of a lot of researchers, to have an agent where you can just give it a goal

00:04.720 --> 00:08.960
and take a step back from your computer, go grab a coffee or, you know, if you're a little bit

00:08.960 --> 00:10.800
more cultured like me, maybe some green tea.

00:14.640 --> 00:18.560
Oh, that was not planned. Anyway, maybe even throw a YouTube video in there.

00:18.560 --> 00:23.440
Then you come back after 10, 15 minutes and whabam. You have an agent that can do exactly

00:23.440 --> 00:28.240
what you tasked it to do, because within those 10 to 15 minutes, it was efficiently exploring,

00:28.320 --> 00:32.080
experimenting, and now it is honed in on a way to achieve the task you gave it.

00:32.080 --> 00:35.840
And unfortunately, right now, that's just kind of a pipe dream. That's because

00:35.840 --> 00:40.240
learning from scratch just takes way too long. It'd be kind of like getting a newborn to learn

00:40.240 --> 00:44.480
how to play chess. I mean, how can you learn how to play chess when you don't even have

00:44.480 --> 00:48.160
object permanence? Heck, first, you know, the baby would need to learn to move its arms.

00:48.160 --> 00:51.920
Heck, babies don't even know that their arms are their arms. So the point being,

00:51.920 --> 00:56.560
it just doesn't quite work. Or at least it didn't until very recently when DeepMind

00:56.560 --> 01:01.120
published a paper on ADA, where in this paper, authors train an agent to be able to learn new

01:01.120 --> 01:06.480
tasks most of the time faster than humans. And this is all taking place in a fairly complex

01:06.480 --> 01:11.280
3D environment. For example, here you can see the agent running around, efficiently testing out

01:11.280 --> 01:15.440
different ideas to see what will achieve its new goal. We'll dive deeper into this in a minute,

01:15.440 --> 01:19.440
but first I'd like to address something. If you're familiar with work in this area,

01:19.440 --> 01:24.080
you may be like, Eden, none of this is new. There's plenty of prior work on metal learning,

01:24.160 --> 01:27.920
view shot learning, the same kind of stuff that's going on here. And you would be right.

01:27.920 --> 01:33.280
ADA doesn't have any new groundbreaking ideas, but rather combines ideas that've worked pretty well

01:33.280 --> 01:37.760
and puts them together in a way that allows this method to scale to larger models. I think that's

01:37.760 --> 01:42.560
a pretty important contribution, primarily because ADA is a method for reinforcement learning. Or in

01:42.560 --> 01:46.960
other words, it's trained an agent to maximize their reward as its goal. And if you know anything

01:46.960 --> 01:52.160
about RL, you know, it can be pretty sample inefficient. And when you combine sample inefficiency

01:52.160 --> 01:56.640
with scale, well, all hell breaks loose. As a little sneak peek of what's coming up, they actually

01:56.640 --> 02:02.560
show this later down in this paper, where they prepare a 23 million and a 265 million parameter

02:02.560 --> 02:08.160
model and train both on 22 billion frames of total interactions. And without any changes,

02:08.160 --> 02:13.760
the bigger model actually performs significantly worse. In fact, it has a hard time learning at

02:13.760 --> 02:18.160
all. As we'll see, the authors propose a way to combine ideas in the field to scale this up,

02:18.160 --> 02:22.720
with the end goal being a framework for training an RL foundation model, because then instead of

02:22.720 --> 02:27.200
always having to start training from scratch, you could have a baseline to significantly speed

02:27.200 --> 02:31.760
up learning new tasks. You could think of this as the same way you can use GPT three to train

02:31.760 --> 02:36.720
something like a sentiment analysis model, and just a few examples instead of well, millions.

02:36.720 --> 02:41.520
Next, we're going to take a look at how ADA works. But before that, I want to thank clear ML for

02:41.520 --> 02:45.360
making it possible for me to spend so much time on videos like this by being a sponsor.

02:45.440 --> 02:50.640
Clear ML offers an end end platform for ML ops, where you can do everything from tracking experiments

02:50.640 --> 02:54.800
to automating an entire machine learning pipeline through to deployment. Here's the code for one

02:54.800 --> 02:58.640
of my current projects. And with just an extra several lines of code, I can integrate it with

02:58.640 --> 03:02.800
clear mail. And with that, we can now pull up a dashboard and see the progress of my experiment

03:02.800 --> 03:06.800
as it runs. Here, I'm just logging the loss because I wanted to get this done in a few seconds,

03:06.800 --> 03:10.880
but there is a lot more you can do with this. But really, this is just the tip of the iceberg.

03:10.880 --> 03:14.720
You can also create entire pipelines that pull in different versions of your data sets,

03:14.720 --> 03:19.520
run hyper parameter sweeps, automatically set up new environments. And because of that last point,

03:19.520 --> 03:24.000
clear mail has no problem with scaling. As I run many experiments in my own research,

03:24.000 --> 03:27.760
I personally love some of the features that help me with that. For example, I love how clear

03:27.760 --> 03:32.480
mail not only tracks code version, but also uncommitted get changes, which is very nice.

03:32.480 --> 03:36.400
And also how through the dashboard, I can inject configs into my experiments,

03:36.400 --> 03:40.880
which means I don't have to constantly be modifying my code to run a bunch of new experiments.

03:40.880 --> 03:44.640
If that sounds like something you're interested in, you can try out their product for free by

03:44.640 --> 03:48.880
following the link in the description below. And now let's get back into this paper and talk

03:48.880 --> 03:53.360
about how 80 works right off the bat. The authors get pretty straight to the point by listing the

03:53.360 --> 03:58.720
three key components that allow for adaptation to emerge. One is meta reinforcement learning

03:58.720 --> 04:04.560
across a vast, smooth and diverse task distribution. Two is a policy parameter prize as a large scale

04:04.560 --> 04:10.320
attention based memory architecture. And three is an effective automated curriculum that prioritizes

04:10.320 --> 04:14.800
tasks at the frontier of the agent's capabilities. If we scroll down a bit, we'll see a high level

04:14.800 --> 04:19.040
visualization of the whole training process. And we can use this as we talk about each of these

04:19.040 --> 04:24.480
steps. Going back to our first point, we have meta learning over a diverse set of tasks, where in

04:24.480 --> 04:29.120
our diagram here, we have that set of diverse tasks. And here we have the RL update that will

04:29.120 --> 04:33.360
allow us to do meta learning, perhaps the most interesting thing about the use of meta RL here

04:33.360 --> 04:37.520
is that the meta learning comes from how the environment tasks are structured and not the RL

04:37.520 --> 04:41.920
algorithm itself. So let's take a look at the environment they use here and this key structuring

04:41.920 --> 04:46.720
that I'm referring to the environment is called X land to, and as opposed to being a single environment,

04:46.720 --> 04:50.720
it's built to be completely customizable so that you can make your own suite of environments.

04:50.720 --> 04:55.680
Each different instance consists of one to two agents. Yes, they also do some multi agent learning

04:55.680 --> 05:00.080
here, which we'll get into. Then there are some initial objects that start out in the world rules

05:00.080 --> 05:05.280
for how those objects interact. For example, this rule right here specifies that when a black pyramid

05:05.280 --> 05:09.200
touches a yellow sphere, it generates a black cube in their place. There are different types

05:09.200 --> 05:13.520
of terrain. And then there is a goal for the agent to achieve, which in this case, the goal

05:13.520 --> 05:17.840
is to hold the black cube for as long as possible. And then there's also a pretty strict time limit

05:17.840 --> 05:22.080
for all of this. And if all of that wasn't hard enough already, several of these specifications

05:22.080 --> 05:26.320
can actually be hidden from the agent itself. Sorry, I've cut it off a little bit here. For

05:26.320 --> 05:30.720
each task instantiation, each of these components are randomized, which ends up giving a whopping

05:30.720 --> 05:35.600
total of 10 to the 40 unique task combinations. That's, that's a lot of tasks. When the authors

05:35.600 --> 05:39.840
said having diverse tasks was essential, they certainly were not skimping on this. But now

05:39.840 --> 05:44.240
we can take a look at the part of the task setup that makes this into a meta RL formulation. And

05:44.240 --> 05:48.880
that has to do with how they structure these trials and episodes. Typically in episodic

05:48.880 --> 05:53.840
reinforcement learning, everything is reset after each episode. And the same process repeats episode

05:53.840 --> 05:58.560
after episode. But here they use the RL squared algorithm for meta learning, which is a bit

05:58.560 --> 06:03.120
different here for each task, a number of trials is determined, which is generally between one and

06:03.120 --> 06:07.520
six while they're training their models. Each trial works kind of like an episode typically would

06:07.520 --> 06:11.600
the same task is used. That means the same environment, the same goal, the same everything.

06:11.600 --> 06:16.640
And every time a trial ends, everything is reset, except for one thing. And this is where it differs

06:16.640 --> 06:21.680
from an episode. The one thing that is not reset that the agent has is a memory module that memory

06:21.680 --> 06:27.200
module persists through trials. And it's only reset after all the trials are over at the end of an

06:27.200 --> 06:32.480
episode. Well, this may seem like not a huge deal. This is actually essential for how this works,

06:32.480 --> 06:37.040
because if the agent can remember previous trials, then it can use its previous exploration

06:37.040 --> 06:41.200
to improve in future trials. And this works because the agent has memory. So if it tries

06:41.200 --> 06:45.120
something in trial one, that doesn't work, and it still has that memory going into trial two,

06:45.120 --> 06:48.960
well, it's probably not going to try the same thing in trial two. And that's the idea here.

06:48.960 --> 06:52.560
What you're looking at right now is an example of this where the agent learns that if it picks up

06:52.560 --> 06:57.200
this object, it disappears. So in the next trial, it learns from that and pushes the object instead.

06:57.200 --> 07:01.280
Without making any changes to how the RL algorithm itself works, we've turned this

07:01.280 --> 07:05.680
into a meta reinforcement learning problem. And speaking of the RL algorithm, which one do they

07:05.680 --> 07:10.320
use here? Well, in theory, any solid reinforcement learning algorithm could work here. But for this,

07:10.320 --> 07:15.440
they decide to use Mooseley or Mooseley or I don't know, hopefully I'm not saying it too wrong.

07:15.440 --> 07:20.400
Mooseley is a model based reinforcement learning algorithm that's based off MPO, but adds a few

07:20.400 --> 07:24.720
bells and whistles, including a value equivalent model component. Looking at the Mooseley paper,

07:24.720 --> 07:30.000
we can see that it performs about the same as mu zero on Atari, but without the huge overhead

07:30.000 --> 07:34.800
of having to do planning. I think one key thing to note here is that Mooseley's model does not try

07:34.800 --> 07:39.760
to reconstruct observations or predict future observations, which is perfectly fine in and

07:39.760 --> 07:44.560
of itself. But other model based methods that have done this, like efficient zero and dreamer,

07:44.560 --> 07:49.280
tend to be significantly more sample efficient. As you'll see, the training in this paper happens

07:49.280 --> 07:53.920
over billions of environment steps. So perhaps that change would be a simple way to make this

07:53.920 --> 07:58.080
more sample efficient. But perhaps one thing that's even more important than sample efficiency in this

07:58.080 --> 08:03.280
case is subscribing to the channel because without subscribing, happy doggo may be reconstructed into

08:03.280 --> 08:07.600
mad doggo. And no one wants that. And if you want more machine learning content like this, you know,

08:07.600 --> 08:11.600
consider subscribing. Anyway, I'll link the paper in the description if you want more details,

08:11.600 --> 08:15.680
but it's not super important for understanding the takeaways of this paper. Awesome. With that,

08:15.760 --> 08:20.240
the way we can check meta reinforcement learning off the list and move on to part two, which is

08:20.240 --> 08:25.440
having a large scale attention based memory architecture, large architectures, attention.

08:26.240 --> 08:31.120
I wonder what that could be. It's right here. How could I miss this? A transformer who could

08:31.120 --> 08:35.360
have seen that one coming? No, but actually, it's not just transformers. In total, they try

08:35.360 --> 08:40.080
three different architectures. The first of them is this transformer XL architecture, which is a

08:40.080 --> 08:44.960
transformer that kind of has a sort of memory. The way it works is that each hidden unit in

08:44.960 --> 08:49.680
the transformer also gets input from the layer below it at the previous time step, which means

08:49.680 --> 08:54.240
that information from previous time steps are going into the future, which means that there is

08:54.240 --> 08:58.080
kind of some sort of memory. If I want to be more specific, what this really does is that it

08:58.080 --> 09:02.160
effectively extends the context length of the transformer. Though it's not written in this

09:02.160 --> 09:07.360
list, they also try a vanilla RNN, a GRU to be specific, is just a very vanilla memory based

09:07.360 --> 09:11.200
model where you have some sort of memory, and you have some gates that decide what gets remembered

09:11.200 --> 09:15.600
and what gets forgotten at each step. Then they also try one more type of RNN that is augmented

09:15.600 --> 09:21.200
with attention that attends over previous time step activations. So very similar to how a standard

09:21.200 --> 09:25.040
RNN works, but just with that extra little bit of attention thrown in there. We can take a look

09:25.040 --> 09:29.120
at how these methods compare to each other in this figure here, where the authors have graphed the

09:29.120 --> 09:34.560
normalized score of these agents based on the final trials of episodes in the test tasks. Or in

09:34.560 --> 09:39.200
Lehmann's terms, big number is good number. What we can see here is that both of the attention

09:39.200 --> 09:44.240
based models significantly outperform the vanilla RNN, despite the fact that they use about the same

09:44.240 --> 09:48.160
number of parameters. Clearly, the attention is doing something to help out quite a bit here. And

09:48.160 --> 09:52.560
due to these results, most of the experiments we'll look at a bit later, use the transformer XL

09:52.560 --> 09:56.320
architecture. So we've covered the architecture here, but there's actually one more portion of

09:56.320 --> 10:00.400
the paper I should talk about in this architecture segment. And that is this distillation update.

10:00.400 --> 10:04.320
If you remember what I showed you in the beginning about how this large model was not able to learn

10:04.320 --> 10:08.560
from this setup, you may be wondering how they end up getting that to work. And the answer here is

10:08.560 --> 10:13.280
kind of interesting. Nothing changes about the architecture, the RL training or any of that,

10:13.280 --> 10:17.680
but rather they add two steps to the start of training to essentially kickstart the large

10:17.680 --> 10:22.480
model's learning progress. And that step is where this idea of distillation comes into the picture.

10:22.480 --> 10:26.800
First, they train a smaller model called the teacher model, using the whole reinforcement

10:26.800 --> 10:30.800
learning training process we were just talking about. Once the smaller model has trained for

10:30.800 --> 10:35.200
several billion steps, then they create the larger model. And when they create the larger model,

10:35.200 --> 10:38.800
they don't just start training it with reinforcement learning from scratch, but rather

10:38.800 --> 10:42.880
for the first four billion steps of training, they use an additional distillation loss. And what

10:42.880 --> 10:47.200
this distillation loss does is essentially distills the teacher or the smaller model

10:47.200 --> 10:51.840
into the big model. Or in other words, they try to have the larger model imitate the smaller

10:51.840 --> 10:55.760
teacher model. The idea here is that because it's much easier to train a smaller model,

10:55.760 --> 10:59.680
even though it might not be as good, you can start by training a small model. Then by having

10:59.680 --> 11:03.680
the larger model imitate the small model, it gets a lot more signal that doesn't require

11:03.680 --> 11:08.320
interacting with the environment some unreasonable amount of times. The idea is once they've done

11:08.320 --> 11:12.640
this for the first four billion steps, the larger model will have already kickstarted its learning

11:12.640 --> 11:16.800
process. So then it will be much easier to learn on its own and it won't just stagnate without being

11:16.800 --> 11:21.280
able to learn anything at all. And now looking at this figure again, it should make a lot more sense

11:21.280 --> 11:26.000
when we train the large model from scratch, it performs pretty horribly. But when we add a

11:26.000 --> 11:30.640
distillation loss, it outperforms everything else. And this is the case for both tasks where the

11:30.640 --> 11:35.680
agent is achieving a median level score, and tasks where the agent is only performing a 20th

11:35.680 --> 11:40.160
percentile score. In other words, whether the agent is good or bad at this task, this approach

11:40.160 --> 11:44.720
works either way. And that explains this distillation loss. Now it's actually kind of funny, because

11:44.720 --> 11:49.280
most of the time when you see someone doing model distillation, it's being used to distill a large

11:49.280 --> 11:53.200
model into a smaller version of that model, so that you can be more efficient at inference time.

11:53.200 --> 11:56.800
But here they're actually doing the opposite in a successful attempt to train the large model

11:56.800 --> 12:00.400
faster, which isn't something I thought about before, but it certainly is an interesting

12:00.400 --> 12:05.520
use of model distillation. So now we know how to set up our tasks as a meta RL problem, we know

12:05.520 --> 12:09.360
what algorithm we're going to use for training, and we know how to get that to work with a large

12:09.360 --> 12:13.680
attention based model. But the point here is to build a foundation model that can explore and

12:13.680 --> 12:18.000
solve tasks efficiently, especially in environments where reward is somewhat sparse. And this is

12:18.000 --> 12:23.520
the case in XLAN 2. The issue is that if you throw a bunch of hard exploration tasks at a new agent,

12:23.600 --> 12:27.920
it's just going to bang its head into the wall over and over and over. And it's not really going

12:27.920 --> 12:31.920
to learn anything, because it's never going to get that first reward or enough reward in the

12:31.920 --> 12:36.560
beginning that gives an incentive or enough signal to learn from our 10 to the 40 tasks,

12:36.560 --> 12:40.800
we need to be able to choose ones that are not too hard, yet also not too easy for the agent at

12:40.800 --> 12:44.960
any given time. And that is where step three comes into play, auto curriculum learning, which we

12:44.960 --> 12:49.360
can see happening as these top steps right here. This brings us to the auto curriculum learning

12:49.360 --> 12:54.080
section, unsurprisingly, where we want to build some sort of curriculum of tasks for the agent

12:54.080 --> 12:58.320
based off its current skill level. And to do this, the authors try two different approaches.

12:58.320 --> 13:03.120
Approach number one is no op filtering. The idea here is that you start by sampling a new task,

13:03.120 --> 13:08.400
and you let both the ADA agent and the no op agent attempt the task. The no op agent takes no

13:08.400 --> 13:12.960
actions, hence the name no op, so it can be used as a baseline for comparison. Given the outcome,

13:12.960 --> 13:17.280
some heuristics are used to determine whether or not this task is at the right difficulty level.

13:17.280 --> 13:21.920
One example of a heuristic they use is that the agent should be able to at least get some reward,

13:21.920 --> 13:26.080
but it shouldn't be too good at the task either. And another criteria, for example,

13:26.080 --> 13:30.400
is that the agent has to achieve a score that's sufficiently different from the no op agent.

13:30.400 --> 13:34.480
And if all these defined heuristics are passed, then the task is used for training. The other

13:34.480 --> 13:39.520
method used for generating curriculum they try is prioritized level replay or PLR for short.

13:39.520 --> 13:43.280
Instead of essentially using a series of if statements like a no op filtering,

13:43.280 --> 13:47.840
PLR runs ADA through candidate tasks and estimates the agent's regret. And regret is

13:47.840 --> 13:52.640
essentially how much potential reward the agent failed to attain as estimated by its TD air.

13:52.640 --> 13:57.280
A fitness score is calculated from the grid, higher regret being better because that means the

13:57.280 --> 14:01.200
agent knows that it has more room to improve. And all the tasks with the highest fitness scores

14:01.200 --> 14:05.600
are kept in a buffer that is sampled from during training. In ablations, we can see that both of

14:05.600 --> 14:10.240
these methods, no op filtering and PLR were quite a bit better than uniform filtering,

14:10.240 --> 14:15.680
which is just random sampling of tasks. However, overall, PLR does perform a little bit better

14:15.680 --> 14:20.240
looking at how PLR and no op filtering changed the task difficulty over time.

14:20.240 --> 14:25.200
In the examples they give here, we can see PLR starts out with less rules, more trials,

14:25.200 --> 14:29.680
gives less dead end rules overall. And it also doesn't hide as many of the critical rules.

14:29.680 --> 14:33.840
And because of these results and PLR's overall high performance, it's used as the curriculum

14:33.840 --> 14:38.320
algorithm of choice for the experiments that we'll be looking at in a bit. So that was a lot,

14:38.320 --> 14:41.840
but we've finally gotten through all the components of how it worked. So now we should be

14:41.840 --> 14:47.280
able to understand this full diagram. We start with a massive task pool from XLAN2. We randomly

14:47.280 --> 14:52.000
sample a bunch of those environments. We use PLR to check which ones have the highest fitness,

14:52.000 --> 14:55.840
throw those into a training set that can be sampled from at any point during training. From

14:55.840 --> 15:01.200
there, these get fed into the actual ADA agent, which uses in the beginning distillation updates,

15:01.200 --> 15:06.240
and throughout the entire process, also RL updates to update this transformer based architecture.

15:06.240 --> 15:10.960
And because it has memory combined with this unique trial in episode format, we're able to

15:10.960 --> 15:16.320
meta-learn how to adapt on the fly. And now that we understand, or at least I hope you all understand,

15:16.320 --> 15:21.520
that works, we can finally dive in to the results. So let's get started with these results that give

15:21.520 --> 15:26.080
us sort of an overview of how the fully trained agent is performing. The score you see here on

15:26.080 --> 15:31.120
the Y axis is normalized based on a model that is fine-tuned on this test task, which means that a

15:31.120 --> 15:35.840
score of around one is very good because it means that the model that is just trying to adapt on

15:35.840 --> 15:40.560
the fly is just as good as something that has been fine-tuned on this task. And looking at this

15:40.560 --> 15:45.280
figure, I think there's two main takeaways. And one is that ADA is pretty effectively able to make

15:45.280 --> 15:51.600
use of its multiple trials. As we can see, there is a pretty large gap between having one and 13

15:51.600 --> 15:56.240
trials. Though after eight or so trials, those benefits do start to wear off as we can see the

15:56.240 --> 16:00.640
eight and the 13 curves are pretty close to each other. Though this should be kind of what we were

16:00.640 --> 16:05.200
expecting because ADA was only allowed up to six trials during training. So if it magically learned

16:05.200 --> 16:09.200
how to use more, well, that would be kind of crazy. And the second thing to take away, you know,

16:09.760 --> 16:16.640
ADA is doing pretty good here. It's able to get greater than an 80% score on 72% of these tasks.

16:16.640 --> 16:21.760
And it's only a mere about 10% of tasks where ADA doesn't get any reward at all. So overall,

16:21.760 --> 16:26.400
pretty good, especially considering that ADA only has a limited amount of time to solve these tasks

16:26.400 --> 16:31.280
that are pretty complicated and always going to be randomized. So the final model is doing pretty

16:31.280 --> 16:35.280
good. But how does this compare to humans? I said this is pretty good. These are pretty hard

16:35.280 --> 16:39.520
tasks. But maybe these are easier than I'm making them out to be. After all, maybe if we gave this

16:39.520 --> 16:44.400
to humans, they would be able to do this much faster, you might think. Luckily for us, the

16:44.400 --> 16:48.960
authors actually tested this idea with a number of human trials. Here you can see the scores of

16:48.960 --> 16:54.560
humans and then also of the ADA agent as it gets more trials. And what you immediately notice is

16:54.560 --> 17:00.160
what is that? Look at that gap right there. I'm not going to lie. I expected humans to kind of

17:00.160 --> 17:04.000
crush this task. It seems like kind of difficult, but you would think that, you know, a human would

17:04.000 --> 17:08.640
be able to figure this out. So one thing I thought looking at this at first was, you know, maybe humans

17:08.640 --> 17:13.200
because they don't have any prior experience with this specific environment, it takes some time to

17:13.200 --> 17:17.200
figure out the rules. There's all these rules. Some of them are hidden. There's different goals.

17:17.200 --> 17:21.760
Everything's randomized. But perhaps if a human, you know, maybe had some priming to this task,

17:21.760 --> 17:25.680
then they would be able to do a lot better, maybe on par with ADA. Turns out they thought of that.

17:26.640 --> 17:31.600
The humans that participated in these trials, actually before they participated in these trials,

17:31.600 --> 17:36.160
had to complete a bunch of test levels, not the ones that are shown in this graph,

17:36.160 --> 17:39.760
but a bunch of different test levels that would have familiarized them with the rules of how

17:39.760 --> 17:44.880
this works. So despite that, humans still kind of suck. Before we move on to this, I also want

17:44.880 --> 17:49.360
to show you this. What we just saw was averaged over all these different tasks. But here we can

17:49.360 --> 17:53.760
see the individual tasks. And one thing to note is that there are some tasks where humans are

17:53.760 --> 17:59.040
actually able to perform better, or ADA really just can't do anything at all. However, in the

17:59.040 --> 18:03.360
vast majority of these tasks, as you saw reflected in the previous figure, ADA still does perform

18:03.360 --> 18:07.280
better. So overall, there are some things that humans can do, but ADA can't do. And there are

18:07.280 --> 18:11.520
many things that ADA can do faster, but humans cannot do very well. But in the average, ADA is

18:11.520 --> 18:15.600
much faster to learn than humans. The next thing we'll take a look at is multi agent learning,

18:15.600 --> 18:19.200
because apparently it wasn't enough for the authors just to do single agents, they need to do multi

18:19.200 --> 18:23.840
agent learning too. It's pretty crazy. So multi agent learning is unsurprisingly, when you have

18:23.840 --> 18:27.680
multiple agents, but more interestingly, in these tasks, where there are multiple agents,

18:27.680 --> 18:31.760
these agents need to cooperate to get the maximum reward. For example, in this environment, you're

18:31.760 --> 18:36.400
seen right here, you have two agents one right here and one right here. But the goals for each of

18:36.400 --> 18:40.560
them, I think are to hold these objects on the other side of the wall that they cannot pass. So

18:40.560 --> 18:45.760
to maximize the total reward here, both of these agents need to throw their items to the other

18:45.760 --> 18:50.240
side of the wall. So this is just one example of what a multi agent environment might look like.

18:50.240 --> 18:53.120
At the figures up here, we can see what they're trying to measure. And what they're trying to

18:53.120 --> 18:58.000
measure for multi agent learning is is an ADA agent cooperating with another ADA agent. In other

18:58.000 --> 19:01.600
words, they have, you know, ADA, they can replicate it, they just have it work with itself. Is that

19:01.600 --> 19:05.920
better than cooperating with a random agent? Because if it is, that would mean that ADA is

19:05.920 --> 19:10.320
learning some sort of cooperation that's better than just random behavior. For all the tasks that

19:10.320 --> 19:13.920
ADA is being given, if we look at the one where it's achieving the median score, we can see that

19:14.000 --> 19:17.760
there's not actually a huge difference. But the reason there's not a huge difference is because

19:17.760 --> 19:22.560
they're both already near one, they're already near optimal performance. If however, we instead

19:22.560 --> 19:26.880
look at the 20th percentile, or in other words, tasks where ADA is not able to do quite as well

19:26.880 --> 19:32.240
that are harder, we can see that cooperative self play, which is where ADA is playing with itself

19:32.240 --> 19:37.040
instead of a random agent does decently outperform playing with a random agent. So this just shows

19:37.040 --> 19:41.760
us that ADA without any special multi agent objective can actually cooperate with other

19:41.760 --> 19:46.480
agents when those actions are in its favor. One reason I kind of like these results and I don't

19:46.480 --> 19:51.440
mean to be spin fire here, but when you go into sort of the multi agent universe of research

19:51.440 --> 19:55.680
being done, there are lots of approaches where they treat multi agent as this sort of separate

19:55.680 --> 19:59.840
setting from the single agent RL problem. And while that can be useful, I think what this shows is

19:59.840 --> 20:04.880
so long as it's beneficial to the single agent multi agent cooperation can arise as an emergent

20:04.880 --> 20:08.960
behavior in cases like these, at least there's actually no need to model them separately. Maybe

20:08.960 --> 20:12.480
it would get you better performance or not. I'm not entirely sure, but it's just some new good

20:12.480 --> 20:16.000
experiment showing that this kind of thing is also feasible. This next experiment we're taking

20:16.000 --> 20:20.560
a look at is a scaling experiment. I mean, come on, how can you publish a paper these days without

20:20.560 --> 20:27.760
a scaling experiment? Am I right? I wonder if I should add in some like laughing noises here.

20:33.280 --> 20:38.400
And unsurprisingly, as you might have guessed, the results here are bigger model do better. Now,

20:38.400 --> 20:42.080
I think we should, you know, maybe not take this for granted because it was shown

20:42.080 --> 20:45.760
without this whole distillation process that they're doing to get the bigger models working.

20:45.760 --> 20:49.680
The larger models actually couldn't really learn it all. So these scaling curves are really possible

20:49.680 --> 20:55.120
because of that. The x axis ranges from 6 million to 265 million parameters, which I don't know,

20:55.120 --> 20:58.800
maybe if you only look at large language model stuff that may seem small, but that's huge for

20:58.800 --> 21:02.640
reinforcement learning models. No one really does reinforcement learning models that big because

21:02.640 --> 21:06.560
it's incredibly hard to train them as as you've seen. And then again, they have the score on the

21:06.560 --> 21:10.960
y axis, but they're both log scaled. And in the paper, they say because these are log scaled and

21:10.960 --> 21:16.320
these look approximately linear, this is a roughly power scaling law. Maybe maybe this is a bit of

21:16.320 --> 21:20.160
a hot take. I'm sorry, I'm going to share a bit of a hot take. That's that I'm not sure if people

21:20.160 --> 21:24.320
are just like gung ho to say that everything's power scaling nowadays. But if there wasn't already

21:24.320 --> 21:29.280
this idea in everyone's heads that machine learning models scale via power law, I don't know, does this

21:29.280 --> 21:34.240
does this look linear to other people? I guess here right at the top for the 13 number of trials,

21:34.240 --> 21:38.080
it is tapering off because it's getting to a round maximum performance, right? But if we look

21:38.080 --> 21:41.280
on the right, where these are the scores for the 20th percentile tasks, like,

21:44.000 --> 21:47.920
is that linear? It doesn't look like it to me. It looks, you know, a little bit more like that.

21:47.920 --> 21:52.240
Anyway, maybe I'm being a little picky here. But another thing to mention is they are showing

21:52.240 --> 21:56.320
the median and the 20th percentile scores, which I have no problem with. That's perfectly fine.

21:56.320 --> 22:00.880
But you know, if they showed the mean, how would that look? Would it also look like power law scaling?

22:00.880 --> 22:05.600
Would it look linear? I don't know. I don't know. I can't imagine thinking this is a power law if I

22:05.600 --> 22:09.440
did this without already having that in my head. This isn't even like a flight. It's just me going

22:09.440 --> 22:13.440
on a stupid rant. That's a tangent. So I'll get over this. I'll go to the next results. Now I'm

22:13.440 --> 22:17.840
just happy that it does scale. It's pretty great results. These next results are again about scaling,

22:17.840 --> 22:22.240
but this time they're scaling the memory length instead of the size of the model. I think pretty

22:22.240 --> 22:27.360
unsurprisingly, as you get a larger memory length, you do get better results here. So that is great.

22:27.360 --> 22:31.440
One thing that is interesting to point out here, maybe two things. One is that memory seems to be

22:31.440 --> 22:36.400
much more important in cases where the agent is not doing as well. So again, these are the 20th

22:36.400 --> 22:40.160
percentile tasks. In other words, it's the tasks that are harder for the agent. And perhaps the

22:40.160 --> 22:44.960
reason memory is more helpful on these is because it can't do it like in one shot or in just a few

22:44.960 --> 22:49.120
trials. It needs more trials. Hence that longer memory comes more in handy. And the other thing

22:49.120 --> 22:53.120
that's interesting is that if we look at one case as an example here, say this case where we have

22:53.200 --> 23:00.400
five trials, there are 300 time steps in each trial. And five times 300 gives us 1,800 as a

23:00.400 --> 23:05.440
total number of time steps in all of these trials. However, if we look at this sort of greenish cyan

23:05.440 --> 23:11.440
line, we will see that once it gets to 1,800, it still continues to go up a little bit. I mean

23:11.440 --> 23:15.680
that even though the memory is getting longer than the total amount of steps it will actually see,

23:15.680 --> 23:19.840
it's still benefiting from that longer memory. And that's likely because even though it can keep

23:19.840 --> 23:24.960
memory for 1,800 steps, it's still going to remember something that's much more recent and

23:24.960 --> 23:28.880
actually still happening like in the present. The agent's probably going to remember that much

23:28.880 --> 23:33.280
better than something that happened 1,800 steps ago, even if it theoretically can. So even once

23:33.280 --> 23:36.800
you sort of hit that limit of the memory theoretically being able to contain everything

23:36.800 --> 23:40.880
that you need, expanding it still can help out a bit. And maybe one more thing I should point out,

23:40.880 --> 23:45.840
which is perhaps obvious, but important sort of as a sanity check is that the memory is the most

23:45.840 --> 23:50.160
helpful when going from one to two trials, which is exactly what we should expect, right? For one

23:50.160 --> 23:54.560
trial, we just need enough memory to contain the current episode. But when we go up and we add more

23:54.560 --> 23:58.880
trials, that memory suddenly becomes a lot more important, because not only are we just sort of

23:58.880 --> 24:03.440
remembering what our current trajectory is, we're remembering the fumbles we had in the past, and

24:03.440 --> 24:06.960
we're learning how to improve those we're trying to adopt. Hence, you know, we see the biggest gap

24:06.960 --> 24:10.880
here, which is sort of a sanity check that this is working how we would expect it to. And I swear

24:10.880 --> 24:14.640
that these are the last results that I'll show you that do some sort of scaling. Here we're looking

24:14.720 --> 24:19.600
at scaling, I guess not really scaling, but just the number of tasks use when we use 200 million

24:19.600 --> 24:25.440
tasks versus 25 billion tasks, you know, just small difference, 200 million, 25 billion, a wee bit

24:25.440 --> 24:31.040
more. And as you get more tasks unsurprisingly, the performance increases, I think this isn't too

24:31.040 --> 24:36.720
much of a surprise. If you think of the excellent tasks as a distribution over tasks, we have 200

24:36.720 --> 24:40.480
million tasks, you're of course sampling many different possibilities. But when you have 25

24:40.480 --> 24:43.760
billion, you're sampling more possibilities, which means you're kind of filling out your

24:43.760 --> 24:47.440
training distribution a bit more, you're also getting more diversity in what you learned. So

24:47.440 --> 24:51.040
I think it makes sense that we should expect to see this perform better. And one thing that's kind

24:51.040 --> 24:56.000
of cool is at least with two to three trials, we can actually have the smaller model, the 23

24:56.000 --> 25:00.880
million parameter model in blue here outperform the 75 million parameter model by a bit just by

25:00.880 --> 25:04.880
adding more tasks showing that you know, scaling parameters is not the only thing that's important.

25:04.880 --> 25:09.760
Having a diverse wider set of tasks is also very important. And I think this kind of mirrors what

25:09.760 --> 25:14.240
we've seen in NLP, where language models, you know, you need a bigger model to do better,

25:14.240 --> 25:18.000
but you also need better data and that can help quite a bit. And now we finally wrap it back around

25:18.000 --> 25:22.160
these distillation results. I know I already showed you how a large model without distillation

25:22.160 --> 25:26.800
feels miserably, but one interesting experiment they add on beneath this is what happens when they

25:26.800 --> 25:31.760
do distillation with two small models of the same size, you might expect that there would be no

25:31.760 --> 25:36.240
benefit, or at least that's what I thought. But as it turns out, even when these two models are

25:36.240 --> 25:40.800
the same size, the teacher and the student here, there is still some benefit. You can see that the

25:40.800 --> 25:45.040
student ends up with a bit higher score than the teacher, even though they are trained for the same

25:45.040 --> 25:49.200
amount of training steps. The reason they mentioned the paper for why this might be happening is due

25:49.200 --> 25:53.040
to what's called the primacy bias. Now they don't mention this name in the paper, but I'm pretty sure

25:53.040 --> 25:56.880
this is what they're referring to. There's a great paper all link in the description that talks about

25:56.880 --> 26:02.000
this idea. And in the paper, the authors show that representations implicitly learned early on during

26:02.080 --> 26:05.920
reinforcement learning can hinder an agent's later performance. Essentially, the early

26:05.920 --> 26:10.160
representations can be bad, and then it's hard for the agent to unlearn those. So here are the

26:10.160 --> 26:14.800
author's posture that perhaps the distillation process is helping to avoid these bad early

26:14.800 --> 26:19.440
representations. Honestly, I'm not entirely convinced though, because I think having an extra

26:19.440 --> 26:23.760
distillation loss alone could be the reason for the better peak performance here. That being said,

26:23.760 --> 26:28.480
I do think this would be an interesting avenue to explore further because it's certainly not 100%

26:28.480 --> 26:32.400
either way. I was actually taking some time to think about this and, you know, having to train

26:32.400 --> 26:37.520
multiple models for this whole distillation process gets kind of messy and takes a lot of memory.

26:37.520 --> 26:41.680
It would be really nice if we could get this to work with just one model. And I was wondering if

26:41.680 --> 26:45.680
that would be possible by training with a very high learning rate or maybe training multiple

26:45.680 --> 26:50.400
times in the beginning with these large models. One issue, of course, of training with a larger

26:50.400 --> 26:54.320
learning rate is that some of the representations could end up bad or the model could start jumping

26:54.320 --> 26:58.960
into some rough optimization territory. But if we randomly reset layers of hidden units,

26:58.960 --> 27:03.120
which is something they do in the primacy bias paper to unlearn bad representations,

27:03.120 --> 27:07.920
then maybe something like that would work. I really have no clue. This is a complete shot in

27:07.920 --> 27:11.600
the dark and it's been quite the tangent. So I'll get back to the results now, but you know,

27:11.600 --> 27:15.840
if anyone was looking for a research idea to try, there's an idea. We've covered what I think are

27:15.840 --> 27:20.480
the most interesting results from this paper. So with this, where do we now stand in the world of

27:20.480 --> 27:25.120
RL foundation models? I'll start by saying, I think this is really great. Several papers I think

27:25.120 --> 27:30.000
have been trying to get at this idea. For example, the Gatto paper from D-Mind and the VPT from

27:30.000 --> 27:33.680
Minecraft paper from OpenAI, both getting at something similar. And I've actually covered

27:33.680 --> 27:38.240
both on this channel if you are interested. But all of the other papers on this topic have shied away

27:38.240 --> 27:43.040
from actually using RL from start to finish. For example, the Gatto paper, which used supervised

27:43.040 --> 27:46.640
learning, I remember it kind of had a line in it. I'm going to be paraphrasing this because I don't

27:46.640 --> 27:50.720
remember it exactly, but it was something like, in theory, we could also do the same experiments

27:50.720 --> 27:54.400
with reinforcement learning. And surely it would just work fine because, you know, it's just kind

27:54.400 --> 28:00.240
of the same thing, but with RL instead of supervised learning. And no, hopefully I'm not

28:00.240 --> 28:04.880
butching what they said here. But looking at this paper now, I think it's obvious why doing this

28:04.880 --> 28:08.960
with supervised learning and doing it with reinforcement learning is really not the same.

28:08.960 --> 28:13.680
Reinforcement learning, it's just a lot harder. The problem itself is a harder problem. Getting

28:13.680 --> 28:17.840
the model to scale was not straightforward at all. I mean, the whole distillation thing is not

28:17.840 --> 28:21.600
something I would have thought of at least. Not to mention it took billions of training steps,

28:21.600 --> 28:26.000
plus millions and millions of unique tasks to get this working. But I think the really great thing

28:26.000 --> 28:29.920
here is that now that we have a paper looking at this, not shying away from the hard questions,

28:29.920 --> 28:33.840
is that we have a reference. And we can use this reference to start deciding the next important

28:33.840 --> 28:38.400
steps to make this feasible on more diverse sets of tasks. And the main one here definitely looks

28:38.400 --> 28:43.120
to be sample efficiency. Even if this is a foundation model requiring billions of steps,

28:43.120 --> 28:46.320
it's just not going to cut it for one, anyone that doesn't have, you know,

28:46.320 --> 28:51.760
an army of TPUs at their command. Everyone other than Google and OpenAI. Oh, sorry,

28:51.760 --> 28:57.680
what was that? And two, any tasks where simulation is difficult or costly, like,

28:57.680 --> 29:02.720
you know, the real world, this kind of data requirement just is not going to be enough.

29:02.720 --> 29:06.800
Luckily, I think that there are lots of promising avenues here. As I mentioned earlier,

29:06.800 --> 29:10.960
model based reinforcement learning methods like efficient zero and dreamer are, I think,

29:10.960 --> 29:15.200
on the order of magnitude astros there, I think they're about a hundred times more efficient

29:15.200 --> 29:19.920
than something like musli. So that alone would already be a huge win if it turned out to be true

29:19.920 --> 29:24.560
in this case. And then there are a lot of other really great methods for self supervised learning

29:24.560 --> 29:28.960
in computer vision that I would imagine could also come in handy. Continuing learning could also

29:28.960 --> 29:33.120
play a pretty big role here, especially when it comes to foundation model. The whole idea of

29:33.120 --> 29:37.920
foundation models is that you train them once, and then you fine tune them for many other tasks

29:37.920 --> 29:42.560
afterwards. But neural networks have a plasticity problem. The more you train them, the worse they

29:42.560 --> 29:46.400
get at learning new tasks, which is obviously not ideal. You can imagine that this is something

29:46.400 --> 29:50.240
we definitely want to avoid here. Maybe check out my video on continual backprop, if that's

29:50.240 --> 29:53.840
something you're interested in. But overall, this is really great work. Kudos to the team for

29:53.840 --> 29:57.520
getting this working. I'm incredibly excited to see where it goes. If you've enjoyed this,

29:57.520 --> 30:01.280
consider subscribing for more dagos and more machine learning stuff. Thank you so much for

30:01.280 --> 30:03.280
watching, and I hope to catch you next time.

