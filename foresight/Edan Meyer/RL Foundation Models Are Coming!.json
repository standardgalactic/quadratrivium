{"text": " I think it's the dream of a lot of researchers, to have an agent where you can just give it a goal and take a step back from your computer, go grab a coffee or, you know, if you're a little bit more cultured like me, maybe some green tea. Oh, that was not planned. Anyway, maybe even throw a YouTube video in there. Then you come back after 10, 15 minutes and whabam. You have an agent that can do exactly what you tasked it to do, because within those 10 to 15 minutes, it was efficiently exploring, experimenting, and now it is honed in on a way to achieve the task you gave it. And unfortunately, right now, that's just kind of a pipe dream. That's because learning from scratch just takes way too long. It'd be kind of like getting a newborn to learn how to play chess. I mean, how can you learn how to play chess when you don't even have object permanence? Heck, first, you know, the baby would need to learn to move its arms. Heck, babies don't even know that their arms are their arms. So the point being, it just doesn't quite work. Or at least it didn't until very recently when DeepMind published a paper on ADA, where in this paper, authors train an agent to be able to learn new tasks most of the time faster than humans. And this is all taking place in a fairly complex 3D environment. For example, here you can see the agent running around, efficiently testing out different ideas to see what will achieve its new goal. We'll dive deeper into this in a minute, but first I'd like to address something. If you're familiar with work in this area, you may be like, Eden, none of this is new. There's plenty of prior work on metal learning, view shot learning, the same kind of stuff that's going on here. And you would be right. ADA doesn't have any new groundbreaking ideas, but rather combines ideas that've worked pretty well and puts them together in a way that allows this method to scale to larger models. I think that's a pretty important contribution, primarily because ADA is a method for reinforcement learning. Or in other words, it's trained an agent to maximize their reward as its goal. And if you know anything about RL, you know, it can be pretty sample inefficient. And when you combine sample inefficiency with scale, well, all hell breaks loose. As a little sneak peek of what's coming up, they actually show this later down in this paper, where they prepare a 23 million and a 265 million parameter model and train both on 22 billion frames of total interactions. And without any changes, the bigger model actually performs significantly worse. In fact, it has a hard time learning at all. As we'll see, the authors propose a way to combine ideas in the field to scale this up, with the end goal being a framework for training an RL foundation model, because then instead of always having to start training from scratch, you could have a baseline to significantly speed up learning new tasks. You could think of this as the same way you can use GPT three to train something like a sentiment analysis model, and just a few examples instead of well, millions. Next, we're going to take a look at how ADA works. But before that, I want to thank clear ML for making it possible for me to spend so much time on videos like this by being a sponsor. Clear ML offers an end end platform for ML ops, where you can do everything from tracking experiments to automating an entire machine learning pipeline through to deployment. Here's the code for one of my current projects. And with just an extra several lines of code, I can integrate it with clear mail. And with that, we can now pull up a dashboard and see the progress of my experiment as it runs. Here, I'm just logging the loss because I wanted to get this done in a few seconds, but there is a lot more you can do with this. But really, this is just the tip of the iceberg. You can also create entire pipelines that pull in different versions of your data sets, run hyper parameter sweeps, automatically set up new environments. And because of that last point, clear mail has no problem with scaling. As I run many experiments in my own research, I personally love some of the features that help me with that. For example, I love how clear mail not only tracks code version, but also uncommitted get changes, which is very nice. And also how through the dashboard, I can inject configs into my experiments, which means I don't have to constantly be modifying my code to run a bunch of new experiments. If that sounds like something you're interested in, you can try out their product for free by following the link in the description below. And now let's get back into this paper and talk about how 80 works right off the bat. The authors get pretty straight to the point by listing the three key components that allow for adaptation to emerge. One is meta reinforcement learning across a vast, smooth and diverse task distribution. Two is a policy parameter prize as a large scale attention based memory architecture. And three is an effective automated curriculum that prioritizes tasks at the frontier of the agent's capabilities. If we scroll down a bit, we'll see a high level visualization of the whole training process. And we can use this as we talk about each of these steps. Going back to our first point, we have meta learning over a diverse set of tasks, where in our diagram here, we have that set of diverse tasks. And here we have the RL update that will allow us to do meta learning, perhaps the most interesting thing about the use of meta RL here is that the meta learning comes from how the environment tasks are structured and not the RL algorithm itself. So let's take a look at the environment they use here and this key structuring that I'm referring to the environment is called X land to, and as opposed to being a single environment, it's built to be completely customizable so that you can make your own suite of environments. Each different instance consists of one to two agents. Yes, they also do some multi agent learning here, which we'll get into. Then there are some initial objects that start out in the world rules for how those objects interact. For example, this rule right here specifies that when a black pyramid touches a yellow sphere, it generates a black cube in their place. There are different types of terrain. And then there is a goal for the agent to achieve, which in this case, the goal is to hold the black cube for as long as possible. And then there's also a pretty strict time limit for all of this. And if all of that wasn't hard enough already, several of these specifications can actually be hidden from the agent itself. Sorry, I've cut it off a little bit here. For each task instantiation, each of these components are randomized, which ends up giving a whopping total of 10 to the 40 unique task combinations. That's, that's a lot of tasks. When the authors said having diverse tasks was essential, they certainly were not skimping on this. But now we can take a look at the part of the task setup that makes this into a meta RL formulation. And that has to do with how they structure these trials and episodes. Typically in episodic reinforcement learning, everything is reset after each episode. And the same process repeats episode after episode. But here they use the RL squared algorithm for meta learning, which is a bit different here for each task, a number of trials is determined, which is generally between one and six while they're training their models. Each trial works kind of like an episode typically would the same task is used. That means the same environment, the same goal, the same everything. And every time a trial ends, everything is reset, except for one thing. And this is where it differs from an episode. The one thing that is not reset that the agent has is a memory module that memory module persists through trials. And it's only reset after all the trials are over at the end of an episode. Well, this may seem like not a huge deal. This is actually essential for how this works, because if the agent can remember previous trials, then it can use its previous exploration to improve in future trials. And this works because the agent has memory. So if it tries something in trial one, that doesn't work, and it still has that memory going into trial two, well, it's probably not going to try the same thing in trial two. And that's the idea here. What you're looking at right now is an example of this where the agent learns that if it picks up this object, it disappears. So in the next trial, it learns from that and pushes the object instead. Without making any changes to how the RL algorithm itself works, we've turned this into a meta reinforcement learning problem. And speaking of the RL algorithm, which one do they use here? Well, in theory, any solid reinforcement learning algorithm could work here. But for this, they decide to use Mooseley or Mooseley or I don't know, hopefully I'm not saying it too wrong. Mooseley is a model based reinforcement learning algorithm that's based off MPO, but adds a few bells and whistles, including a value equivalent model component. Looking at the Mooseley paper, we can see that it performs about the same as mu zero on Atari, but without the huge overhead of having to do planning. I think one key thing to note here is that Mooseley's model does not try to reconstruct observations or predict future observations, which is perfectly fine in and of itself. But other model based methods that have done this, like efficient zero and dreamer, tend to be significantly more sample efficient. As you'll see, the training in this paper happens over billions of environment steps. So perhaps that change would be a simple way to make this more sample efficient. But perhaps one thing that's even more important than sample efficiency in this case is subscribing to the channel because without subscribing, happy doggo may be reconstructed into mad doggo. And no one wants that. And if you want more machine learning content like this, you know, consider subscribing. Anyway, I'll link the paper in the description if you want more details, but it's not super important for understanding the takeaways of this paper. Awesome. With that, the way we can check meta reinforcement learning off the list and move on to part two, which is having a large scale attention based memory architecture, large architectures, attention. I wonder what that could be. It's right here. How could I miss this? A transformer who could have seen that one coming? No, but actually, it's not just transformers. In total, they try three different architectures. The first of them is this transformer XL architecture, which is a transformer that kind of has a sort of memory. The way it works is that each hidden unit in the transformer also gets input from the layer below it at the previous time step, which means that information from previous time steps are going into the future, which means that there is kind of some sort of memory. If I want to be more specific, what this really does is that it effectively extends the context length of the transformer. Though it's not written in this list, they also try a vanilla RNN, a GRU to be specific, is just a very vanilla memory based model where you have some sort of memory, and you have some gates that decide what gets remembered and what gets forgotten at each step. Then they also try one more type of RNN that is augmented with attention that attends over previous time step activations. So very similar to how a standard RNN works, but just with that extra little bit of attention thrown in there. We can take a look at how these methods compare to each other in this figure here, where the authors have graphed the normalized score of these agents based on the final trials of episodes in the test tasks. Or in Lehmann's terms, big number is good number. What we can see here is that both of the attention based models significantly outperform the vanilla RNN, despite the fact that they use about the same number of parameters. Clearly, the attention is doing something to help out quite a bit here. And due to these results, most of the experiments we'll look at a bit later, use the transformer XL architecture. So we've covered the architecture here, but there's actually one more portion of the paper I should talk about in this architecture segment. And that is this distillation update. If you remember what I showed you in the beginning about how this large model was not able to learn from this setup, you may be wondering how they end up getting that to work. And the answer here is kind of interesting. Nothing changes about the architecture, the RL training or any of that, but rather they add two steps to the start of training to essentially kickstart the large model's learning progress. And that step is where this idea of distillation comes into the picture. First, they train a smaller model called the teacher model, using the whole reinforcement learning training process we were just talking about. Once the smaller model has trained for several billion steps, then they create the larger model. And when they create the larger model, they don't just start training it with reinforcement learning from scratch, but rather for the first four billion steps of training, they use an additional distillation loss. And what this distillation loss does is essentially distills the teacher or the smaller model into the big model. Or in other words, they try to have the larger model imitate the smaller teacher model. The idea here is that because it's much easier to train a smaller model, even though it might not be as good, you can start by training a small model. Then by having the larger model imitate the small model, it gets a lot more signal that doesn't require interacting with the environment some unreasonable amount of times. The idea is once they've done this for the first four billion steps, the larger model will have already kickstarted its learning process. So then it will be much easier to learn on its own and it won't just stagnate without being able to learn anything at all. And now looking at this figure again, it should make a lot more sense when we train the large model from scratch, it performs pretty horribly. But when we add a distillation loss, it outperforms everything else. And this is the case for both tasks where the agent is achieving a median level score, and tasks where the agent is only performing a 20th percentile score. In other words, whether the agent is good or bad at this task, this approach works either way. And that explains this distillation loss. Now it's actually kind of funny, because most of the time when you see someone doing model distillation, it's being used to distill a large model into a smaller version of that model, so that you can be more efficient at inference time. But here they're actually doing the opposite in a successful attempt to train the large model faster, which isn't something I thought about before, but it certainly is an interesting use of model distillation. So now we know how to set up our tasks as a meta RL problem, we know what algorithm we're going to use for training, and we know how to get that to work with a large attention based model. But the point here is to build a foundation model that can explore and solve tasks efficiently, especially in environments where reward is somewhat sparse. And this is the case in XLAN 2. The issue is that if you throw a bunch of hard exploration tasks at a new agent, it's just going to bang its head into the wall over and over and over. And it's not really going to learn anything, because it's never going to get that first reward or enough reward in the beginning that gives an incentive or enough signal to learn from our 10 to the 40 tasks, we need to be able to choose ones that are not too hard, yet also not too easy for the agent at any given time. And that is where step three comes into play, auto curriculum learning, which we can see happening as these top steps right here. This brings us to the auto curriculum learning section, unsurprisingly, where we want to build some sort of curriculum of tasks for the agent based off its current skill level. And to do this, the authors try two different approaches. Approach number one is no op filtering. The idea here is that you start by sampling a new task, and you let both the ADA agent and the no op agent attempt the task. The no op agent takes no actions, hence the name no op, so it can be used as a baseline for comparison. Given the outcome, some heuristics are used to determine whether or not this task is at the right difficulty level. One example of a heuristic they use is that the agent should be able to at least get some reward, but it shouldn't be too good at the task either. And another criteria, for example, is that the agent has to achieve a score that's sufficiently different from the no op agent. And if all these defined heuristics are passed, then the task is used for training. The other method used for generating curriculum they try is prioritized level replay or PLR for short. Instead of essentially using a series of if statements like a no op filtering, PLR runs ADA through candidate tasks and estimates the agent's regret. And regret is essentially how much potential reward the agent failed to attain as estimated by its TD air. A fitness score is calculated from the grid, higher regret being better because that means the agent knows that it has more room to improve. And all the tasks with the highest fitness scores are kept in a buffer that is sampled from during training. In ablations, we can see that both of these methods, no op filtering and PLR were quite a bit better than uniform filtering, which is just random sampling of tasks. However, overall, PLR does perform a little bit better looking at how PLR and no op filtering changed the task difficulty over time. In the examples they give here, we can see PLR starts out with less rules, more trials, gives less dead end rules overall. And it also doesn't hide as many of the critical rules. And because of these results and PLR's overall high performance, it's used as the curriculum algorithm of choice for the experiments that we'll be looking at in a bit. So that was a lot, but we've finally gotten through all the components of how it worked. So now we should be able to understand this full diagram. We start with a massive task pool from XLAN2. We randomly sample a bunch of those environments. We use PLR to check which ones have the highest fitness, throw those into a training set that can be sampled from at any point during training. From there, these get fed into the actual ADA agent, which uses in the beginning distillation updates, and throughout the entire process, also RL updates to update this transformer based architecture. And because it has memory combined with this unique trial in episode format, we're able to meta-learn how to adapt on the fly. And now that we understand, or at least I hope you all understand, that works, we can finally dive in to the results. So let's get started with these results that give us sort of an overview of how the fully trained agent is performing. The score you see here on the Y axis is normalized based on a model that is fine-tuned on this test task, which means that a score of around one is very good because it means that the model that is just trying to adapt on the fly is just as good as something that has been fine-tuned on this task. And looking at this figure, I think there's two main takeaways. And one is that ADA is pretty effectively able to make use of its multiple trials. As we can see, there is a pretty large gap between having one and 13 trials. Though after eight or so trials, those benefits do start to wear off as we can see the eight and the 13 curves are pretty close to each other. Though this should be kind of what we were expecting because ADA was only allowed up to six trials during training. So if it magically learned how to use more, well, that would be kind of crazy. And the second thing to take away, you know, ADA is doing pretty good here. It's able to get greater than an 80% score on 72% of these tasks. And it's only a mere about 10% of tasks where ADA doesn't get any reward at all. So overall, pretty good, especially considering that ADA only has a limited amount of time to solve these tasks that are pretty complicated and always going to be randomized. So the final model is doing pretty good. But how does this compare to humans? I said this is pretty good. These are pretty hard tasks. But maybe these are easier than I'm making them out to be. After all, maybe if we gave this to humans, they would be able to do this much faster, you might think. Luckily for us, the authors actually tested this idea with a number of human trials. Here you can see the scores of humans and then also of the ADA agent as it gets more trials. And what you immediately notice is what is that? Look at that gap right there. I'm not going to lie. I expected humans to kind of crush this task. It seems like kind of difficult, but you would think that, you know, a human would be able to figure this out. So one thing I thought looking at this at first was, you know, maybe humans because they don't have any prior experience with this specific environment, it takes some time to figure out the rules. There's all these rules. Some of them are hidden. There's different goals. Everything's randomized. But perhaps if a human, you know, maybe had some priming to this task, then they would be able to do a lot better, maybe on par with ADA. Turns out they thought of that. The humans that participated in these trials, actually before they participated in these trials, had to complete a bunch of test levels, not the ones that are shown in this graph, but a bunch of different test levels that would have familiarized them with the rules of how this works. So despite that, humans still kind of suck. Before we move on to this, I also want to show you this. What we just saw was averaged over all these different tasks. But here we can see the individual tasks. And one thing to note is that there are some tasks where humans are actually able to perform better, or ADA really just can't do anything at all. However, in the vast majority of these tasks, as you saw reflected in the previous figure, ADA still does perform better. So overall, there are some things that humans can do, but ADA can't do. And there are many things that ADA can do faster, but humans cannot do very well. But in the average, ADA is much faster to learn than humans. The next thing we'll take a look at is multi agent learning, because apparently it wasn't enough for the authors just to do single agents, they need to do multi agent learning too. It's pretty crazy. So multi agent learning is unsurprisingly, when you have multiple agents, but more interestingly, in these tasks, where there are multiple agents, these agents need to cooperate to get the maximum reward. For example, in this environment, you're seen right here, you have two agents one right here and one right here. But the goals for each of them, I think are to hold these objects on the other side of the wall that they cannot pass. So to maximize the total reward here, both of these agents need to throw their items to the other side of the wall. So this is just one example of what a multi agent environment might look like. At the figures up here, we can see what they're trying to measure. And what they're trying to measure for multi agent learning is is an ADA agent cooperating with another ADA agent. In other words, they have, you know, ADA, they can replicate it, they just have it work with itself. Is that better than cooperating with a random agent? Because if it is, that would mean that ADA is learning some sort of cooperation that's better than just random behavior. For all the tasks that ADA is being given, if we look at the one where it's achieving the median score, we can see that there's not actually a huge difference. But the reason there's not a huge difference is because they're both already near one, they're already near optimal performance. If however, we instead look at the 20th percentile, or in other words, tasks where ADA is not able to do quite as well that are harder, we can see that cooperative self play, which is where ADA is playing with itself instead of a random agent does decently outperform playing with a random agent. So this just shows us that ADA without any special multi agent objective can actually cooperate with other agents when those actions are in its favor. One reason I kind of like these results and I don't mean to be spin fire here, but when you go into sort of the multi agent universe of research being done, there are lots of approaches where they treat multi agent as this sort of separate setting from the single agent RL problem. And while that can be useful, I think what this shows is so long as it's beneficial to the single agent multi agent cooperation can arise as an emergent behavior in cases like these, at least there's actually no need to model them separately. Maybe it would get you better performance or not. I'm not entirely sure, but it's just some new good experiment showing that this kind of thing is also feasible. This next experiment we're taking a look at is a scaling experiment. I mean, come on, how can you publish a paper these days without a scaling experiment? Am I right? I wonder if I should add in some like laughing noises here. And unsurprisingly, as you might have guessed, the results here are bigger model do better. Now, I think we should, you know, maybe not take this for granted because it was shown without this whole distillation process that they're doing to get the bigger models working. The larger models actually couldn't really learn it all. So these scaling curves are really possible because of that. The x axis ranges from 6 million to 265 million parameters, which I don't know, maybe if you only look at large language model stuff that may seem small, but that's huge for reinforcement learning models. No one really does reinforcement learning models that big because it's incredibly hard to train them as as you've seen. And then again, they have the score on the y axis, but they're both log scaled. And in the paper, they say because these are log scaled and these look approximately linear, this is a roughly power scaling law. Maybe maybe this is a bit of a hot take. I'm sorry, I'm going to share a bit of a hot take. That's that I'm not sure if people are just like gung ho to say that everything's power scaling nowadays. But if there wasn't already this idea in everyone's heads that machine learning models scale via power law, I don't know, does this does this look linear to other people? I guess here right at the top for the 13 number of trials, it is tapering off because it's getting to a round maximum performance, right? But if we look on the right, where these are the scores for the 20th percentile tasks, like, is that linear? It doesn't look like it to me. It looks, you know, a little bit more like that. Anyway, maybe I'm being a little picky here. But another thing to mention is they are showing the median and the 20th percentile scores, which I have no problem with. That's perfectly fine. But you know, if they showed the mean, how would that look? Would it also look like power law scaling? Would it look linear? I don't know. I don't know. I can't imagine thinking this is a power law if I did this without already having that in my head. This isn't even like a flight. It's just me going on a stupid rant. That's a tangent. So I'll get over this. I'll go to the next results. Now I'm just happy that it does scale. It's pretty great results. These next results are again about scaling, but this time they're scaling the memory length instead of the size of the model. I think pretty unsurprisingly, as you get a larger memory length, you do get better results here. So that is great. One thing that is interesting to point out here, maybe two things. One is that memory seems to be much more important in cases where the agent is not doing as well. So again, these are the 20th percentile tasks. In other words, it's the tasks that are harder for the agent. And perhaps the reason memory is more helpful on these is because it can't do it like in one shot or in just a few trials. It needs more trials. Hence that longer memory comes more in handy. And the other thing that's interesting is that if we look at one case as an example here, say this case where we have five trials, there are 300 time steps in each trial. And five times 300 gives us 1,800 as a total number of time steps in all of these trials. However, if we look at this sort of greenish cyan line, we will see that once it gets to 1,800, it still continues to go up a little bit. I mean that even though the memory is getting longer than the total amount of steps it will actually see, it's still benefiting from that longer memory. And that's likely because even though it can keep memory for 1,800 steps, it's still going to remember something that's much more recent and actually still happening like in the present. The agent's probably going to remember that much better than something that happened 1,800 steps ago, even if it theoretically can. So even once you sort of hit that limit of the memory theoretically being able to contain everything that you need, expanding it still can help out a bit. And maybe one more thing I should point out, which is perhaps obvious, but important sort of as a sanity check is that the memory is the most helpful when going from one to two trials, which is exactly what we should expect, right? For one trial, we just need enough memory to contain the current episode. But when we go up and we add more trials, that memory suddenly becomes a lot more important, because not only are we just sort of remembering what our current trajectory is, we're remembering the fumbles we had in the past, and we're learning how to improve those we're trying to adopt. Hence, you know, we see the biggest gap here, which is sort of a sanity check that this is working how we would expect it to. And I swear that these are the last results that I'll show you that do some sort of scaling. Here we're looking at scaling, I guess not really scaling, but just the number of tasks use when we use 200 million tasks versus 25 billion tasks, you know, just small difference, 200 million, 25 billion, a wee bit more. And as you get more tasks unsurprisingly, the performance increases, I think this isn't too much of a surprise. If you think of the excellent tasks as a distribution over tasks, we have 200 million tasks, you're of course sampling many different possibilities. But when you have 25 billion, you're sampling more possibilities, which means you're kind of filling out your training distribution a bit more, you're also getting more diversity in what you learned. So I think it makes sense that we should expect to see this perform better. And one thing that's kind of cool is at least with two to three trials, we can actually have the smaller model, the 23 million parameter model in blue here outperform the 75 million parameter model by a bit just by adding more tasks showing that you know, scaling parameters is not the only thing that's important. Having a diverse wider set of tasks is also very important. And I think this kind of mirrors what we've seen in NLP, where language models, you know, you need a bigger model to do better, but you also need better data and that can help quite a bit. And now we finally wrap it back around these distillation results. I know I already showed you how a large model without distillation feels miserably, but one interesting experiment they add on beneath this is what happens when they do distillation with two small models of the same size, you might expect that there would be no benefit, or at least that's what I thought. But as it turns out, even when these two models are the same size, the teacher and the student here, there is still some benefit. You can see that the student ends up with a bit higher score than the teacher, even though they are trained for the same amount of training steps. The reason they mentioned the paper for why this might be happening is due to what's called the primacy bias. Now they don't mention this name in the paper, but I'm pretty sure this is what they're referring to. There's a great paper all link in the description that talks about this idea. And in the paper, the authors show that representations implicitly learned early on during reinforcement learning can hinder an agent's later performance. Essentially, the early representations can be bad, and then it's hard for the agent to unlearn those. So here are the author's posture that perhaps the distillation process is helping to avoid these bad early representations. Honestly, I'm not entirely convinced though, because I think having an extra distillation loss alone could be the reason for the better peak performance here. That being said, I do think this would be an interesting avenue to explore further because it's certainly not 100% either way. I was actually taking some time to think about this and, you know, having to train multiple models for this whole distillation process gets kind of messy and takes a lot of memory. It would be really nice if we could get this to work with just one model. And I was wondering if that would be possible by training with a very high learning rate or maybe training multiple times in the beginning with these large models. One issue, of course, of training with a larger learning rate is that some of the representations could end up bad or the model could start jumping into some rough optimization territory. But if we randomly reset layers of hidden units, which is something they do in the primacy bias paper to unlearn bad representations, then maybe something like that would work. I really have no clue. This is a complete shot in the dark and it's been quite the tangent. So I'll get back to the results now, but you know, if anyone was looking for a research idea to try, there's an idea. We've covered what I think are the most interesting results from this paper. So with this, where do we now stand in the world of RL foundation models? I'll start by saying, I think this is really great. Several papers I think have been trying to get at this idea. For example, the Gatto paper from D-Mind and the VPT from Minecraft paper from OpenAI, both getting at something similar. And I've actually covered both on this channel if you are interested. But all of the other papers on this topic have shied away from actually using RL from start to finish. For example, the Gatto paper, which used supervised learning, I remember it kind of had a line in it. I'm going to be paraphrasing this because I don't remember it exactly, but it was something like, in theory, we could also do the same experiments with reinforcement learning. And surely it would just work fine because, you know, it's just kind of the same thing, but with RL instead of supervised learning. And no, hopefully I'm not butching what they said here. But looking at this paper now, I think it's obvious why doing this with supervised learning and doing it with reinforcement learning is really not the same. Reinforcement learning, it's just a lot harder. The problem itself is a harder problem. Getting the model to scale was not straightforward at all. I mean, the whole distillation thing is not something I would have thought of at least. Not to mention it took billions of training steps, plus millions and millions of unique tasks to get this working. But I think the really great thing here is that now that we have a paper looking at this, not shying away from the hard questions, is that we have a reference. And we can use this reference to start deciding the next important steps to make this feasible on more diverse sets of tasks. And the main one here definitely looks to be sample efficiency. Even if this is a foundation model requiring billions of steps, it's just not going to cut it for one, anyone that doesn't have, you know, an army of TPUs at their command. Everyone other than Google and OpenAI. Oh, sorry, what was that? And two, any tasks where simulation is difficult or costly, like, you know, the real world, this kind of data requirement just is not going to be enough. Luckily, I think that there are lots of promising avenues here. As I mentioned earlier, model based reinforcement learning methods like efficient zero and dreamer are, I think, on the order of magnitude astros there, I think they're about a hundred times more efficient than something like musli. So that alone would already be a huge win if it turned out to be true in this case. And then there are a lot of other really great methods for self supervised learning in computer vision that I would imagine could also come in handy. Continuing learning could also play a pretty big role here, especially when it comes to foundation model. The whole idea of foundation models is that you train them once, and then you fine tune them for many other tasks afterwards. But neural networks have a plasticity problem. The more you train them, the worse they get at learning new tasks, which is obviously not ideal. You can imagine that this is something we definitely want to avoid here. Maybe check out my video on continual backprop, if that's something you're interested in. But overall, this is really great work. Kudos to the team for getting this working. I'm incredibly excited to see where it goes. If you've enjoyed this, consider subscribing for more dagos and more machine learning stuff. Thank you so much for watching, and I hope to catch you next time.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 4.72, "text": " I think it's the dream of a lot of researchers, to have an agent where you can just give it a goal", "tokens": [50364, 286, 519, 309, 311, 264, 3055, 295, 257, 688, 295, 10309, 11, 281, 362, 364, 9461, 689, 291, 393, 445, 976, 309, 257, 3387, 50600], "temperature": 0.0, "avg_logprob": -0.17497393959446958, "compression_ratio": 1.6129032258064515, "no_speech_prob": 0.0182461179792881}, {"id": 1, "seek": 0, "start": 4.72, "end": 8.96, "text": " and take a step back from your computer, go grab a coffee or, you know, if you're a little bit", "tokens": [50600, 293, 747, 257, 1823, 646, 490, 428, 3820, 11, 352, 4444, 257, 4982, 420, 11, 291, 458, 11, 498, 291, 434, 257, 707, 857, 50812], "temperature": 0.0, "avg_logprob": -0.17497393959446958, "compression_ratio": 1.6129032258064515, "no_speech_prob": 0.0182461179792881}, {"id": 2, "seek": 0, "start": 8.96, "end": 10.8, "text": " more cultured like me, maybe some green tea.", "tokens": [50812, 544, 3713, 67, 411, 385, 11, 1310, 512, 3092, 5817, 13, 50904], "temperature": 0.0, "avg_logprob": -0.17497393959446958, "compression_ratio": 1.6129032258064515, "no_speech_prob": 0.0182461179792881}, {"id": 3, "seek": 0, "start": 14.64, "end": 18.56, "text": " Oh, that was not planned. Anyway, maybe even throw a YouTube video in there.", "tokens": [51096, 876, 11, 300, 390, 406, 8589, 13, 5684, 11, 1310, 754, 3507, 257, 3088, 960, 294, 456, 13, 51292], "temperature": 0.0, "avg_logprob": -0.17497393959446958, "compression_ratio": 1.6129032258064515, "no_speech_prob": 0.0182461179792881}, {"id": 4, "seek": 0, "start": 18.56, "end": 23.44, "text": " Then you come back after 10, 15 minutes and whabam. You have an agent that can do exactly", "tokens": [51292, 1396, 291, 808, 646, 934, 1266, 11, 2119, 2077, 293, 315, 455, 335, 13, 509, 362, 364, 9461, 300, 393, 360, 2293, 51536], "temperature": 0.0, "avg_logprob": -0.17497393959446958, "compression_ratio": 1.6129032258064515, "no_speech_prob": 0.0182461179792881}, {"id": 5, "seek": 0, "start": 23.44, "end": 28.240000000000002, "text": " what you tasked it to do, because within those 10 to 15 minutes, it was efficiently exploring,", "tokens": [51536, 437, 291, 38621, 309, 281, 360, 11, 570, 1951, 729, 1266, 281, 2119, 2077, 11, 309, 390, 19621, 12736, 11, 51776], "temperature": 0.0, "avg_logprob": -0.17497393959446958, "compression_ratio": 1.6129032258064515, "no_speech_prob": 0.0182461179792881}, {"id": 6, "seek": 2824, "start": 28.319999999999997, "end": 32.08, "text": " experimenting, and now it is honed in on a way to achieve the task you gave it.", "tokens": [50368, 29070, 11, 293, 586, 309, 307, 2157, 292, 294, 322, 257, 636, 281, 4584, 264, 5633, 291, 2729, 309, 13, 50556], "temperature": 0.0, "avg_logprob": -0.08737994758946122, "compression_ratio": 1.7196531791907514, "no_speech_prob": 0.027580372989177704}, {"id": 7, "seek": 2824, "start": 32.08, "end": 35.839999999999996, "text": " And unfortunately, right now, that's just kind of a pipe dream. That's because", "tokens": [50556, 400, 7015, 11, 558, 586, 11, 300, 311, 445, 733, 295, 257, 11240, 3055, 13, 663, 311, 570, 50744], "temperature": 0.0, "avg_logprob": -0.08737994758946122, "compression_ratio": 1.7196531791907514, "no_speech_prob": 0.027580372989177704}, {"id": 8, "seek": 2824, "start": 35.839999999999996, "end": 40.239999999999995, "text": " learning from scratch just takes way too long. It'd be kind of like getting a newborn to learn", "tokens": [50744, 2539, 490, 8459, 445, 2516, 636, 886, 938, 13, 467, 1116, 312, 733, 295, 411, 1242, 257, 32928, 281, 1466, 50964], "temperature": 0.0, "avg_logprob": -0.08737994758946122, "compression_ratio": 1.7196531791907514, "no_speech_prob": 0.027580372989177704}, {"id": 9, "seek": 2824, "start": 40.239999999999995, "end": 44.480000000000004, "text": " how to play chess. I mean, how can you learn how to play chess when you don't even have", "tokens": [50964, 577, 281, 862, 24122, 13, 286, 914, 11, 577, 393, 291, 1466, 577, 281, 862, 24122, 562, 291, 500, 380, 754, 362, 51176], "temperature": 0.0, "avg_logprob": -0.08737994758946122, "compression_ratio": 1.7196531791907514, "no_speech_prob": 0.027580372989177704}, {"id": 10, "seek": 2824, "start": 44.480000000000004, "end": 48.16, "text": " object permanence? Heck, first, you know, the baby would need to learn to move its arms.", "tokens": [51176, 2657, 8105, 655, 30, 41948, 11, 700, 11, 291, 458, 11, 264, 3186, 576, 643, 281, 1466, 281, 1286, 1080, 5812, 13, 51360], "temperature": 0.0, "avg_logprob": -0.08737994758946122, "compression_ratio": 1.7196531791907514, "no_speech_prob": 0.027580372989177704}, {"id": 11, "seek": 2824, "start": 48.16, "end": 51.92, "text": " Heck, babies don't even know that their arms are their arms. So the point being,", "tokens": [51360, 41948, 11, 10917, 500, 380, 754, 458, 300, 641, 5812, 366, 641, 5812, 13, 407, 264, 935, 885, 11, 51548], "temperature": 0.0, "avg_logprob": -0.08737994758946122, "compression_ratio": 1.7196531791907514, "no_speech_prob": 0.027580372989177704}, {"id": 12, "seek": 2824, "start": 51.92, "end": 56.56, "text": " it just doesn't quite work. Or at least it didn't until very recently when DeepMind", "tokens": [51548, 309, 445, 1177, 380, 1596, 589, 13, 1610, 412, 1935, 309, 994, 380, 1826, 588, 3938, 562, 14895, 44, 471, 51780], "temperature": 0.0, "avg_logprob": -0.08737994758946122, "compression_ratio": 1.7196531791907514, "no_speech_prob": 0.027580372989177704}, {"id": 13, "seek": 5656, "start": 56.56, "end": 61.120000000000005, "text": " published a paper on ADA, where in this paper, authors train an agent to be able to learn new", "tokens": [50364, 6572, 257, 3035, 322, 39354, 11, 689, 294, 341, 3035, 11, 16552, 3847, 364, 9461, 281, 312, 1075, 281, 1466, 777, 50592], "temperature": 0.0, "avg_logprob": -0.08883509215186625, "compression_ratio": 1.6409495548961424, "no_speech_prob": 0.018545253202319145}, {"id": 14, "seek": 5656, "start": 61.120000000000005, "end": 66.48, "text": " tasks most of the time faster than humans. And this is all taking place in a fairly complex", "tokens": [50592, 9608, 881, 295, 264, 565, 4663, 813, 6255, 13, 400, 341, 307, 439, 1940, 1081, 294, 257, 6457, 3997, 50860], "temperature": 0.0, "avg_logprob": -0.08883509215186625, "compression_ratio": 1.6409495548961424, "no_speech_prob": 0.018545253202319145}, {"id": 15, "seek": 5656, "start": 66.48, "end": 71.28, "text": " 3D environment. For example, here you can see the agent running around, efficiently testing out", "tokens": [50860, 805, 35, 2823, 13, 1171, 1365, 11, 510, 291, 393, 536, 264, 9461, 2614, 926, 11, 19621, 4997, 484, 51100], "temperature": 0.0, "avg_logprob": -0.08883509215186625, "compression_ratio": 1.6409495548961424, "no_speech_prob": 0.018545253202319145}, {"id": 16, "seek": 5656, "start": 71.28, "end": 75.44, "text": " different ideas to see what will achieve its new goal. We'll dive deeper into this in a minute,", "tokens": [51100, 819, 3487, 281, 536, 437, 486, 4584, 1080, 777, 3387, 13, 492, 603, 9192, 7731, 666, 341, 294, 257, 3456, 11, 51308], "temperature": 0.0, "avg_logprob": -0.08883509215186625, "compression_ratio": 1.6409495548961424, "no_speech_prob": 0.018545253202319145}, {"id": 17, "seek": 5656, "start": 75.44, "end": 79.44, "text": " but first I'd like to address something. If you're familiar with work in this area,", "tokens": [51308, 457, 700, 286, 1116, 411, 281, 2985, 746, 13, 759, 291, 434, 4963, 365, 589, 294, 341, 1859, 11, 51508], "temperature": 0.0, "avg_logprob": -0.08883509215186625, "compression_ratio": 1.6409495548961424, "no_speech_prob": 0.018545253202319145}, {"id": 18, "seek": 5656, "start": 79.44, "end": 84.08, "text": " you may be like, Eden, none of this is new. There's plenty of prior work on metal learning,", "tokens": [51508, 291, 815, 312, 411, 11, 35322, 11, 6022, 295, 341, 307, 777, 13, 821, 311, 7140, 295, 4059, 589, 322, 5760, 2539, 11, 51740], "temperature": 0.0, "avg_logprob": -0.08883509215186625, "compression_ratio": 1.6409495548961424, "no_speech_prob": 0.018545253202319145}, {"id": 19, "seek": 8408, "start": 84.16, "end": 87.92, "text": " view shot learning, the same kind of stuff that's going on here. And you would be right.", "tokens": [50368, 1910, 3347, 2539, 11, 264, 912, 733, 295, 1507, 300, 311, 516, 322, 510, 13, 400, 291, 576, 312, 558, 13, 50556], "temperature": 0.0, "avg_logprob": -0.09834280229152594, "compression_ratio": 1.71976401179941, "no_speech_prob": 0.013635465875267982}, {"id": 20, "seek": 8408, "start": 87.92, "end": 93.28, "text": " ADA doesn't have any new groundbreaking ideas, but rather combines ideas that've worked pretty well", "tokens": [50556, 39354, 1177, 380, 362, 604, 777, 42491, 3487, 11, 457, 2831, 29520, 3487, 300, 600, 2732, 1238, 731, 50824], "temperature": 0.0, "avg_logprob": -0.09834280229152594, "compression_ratio": 1.71976401179941, "no_speech_prob": 0.013635465875267982}, {"id": 21, "seek": 8408, "start": 93.28, "end": 97.75999999999999, "text": " and puts them together in a way that allows this method to scale to larger models. I think that's", "tokens": [50824, 293, 8137, 552, 1214, 294, 257, 636, 300, 4045, 341, 3170, 281, 4373, 281, 4833, 5245, 13, 286, 519, 300, 311, 51048], "temperature": 0.0, "avg_logprob": -0.09834280229152594, "compression_ratio": 1.71976401179941, "no_speech_prob": 0.013635465875267982}, {"id": 22, "seek": 8408, "start": 97.75999999999999, "end": 102.56, "text": " a pretty important contribution, primarily because ADA is a method for reinforcement learning. Or in", "tokens": [51048, 257, 1238, 1021, 13150, 11, 10029, 570, 39354, 307, 257, 3170, 337, 29280, 2539, 13, 1610, 294, 51288], "temperature": 0.0, "avg_logprob": -0.09834280229152594, "compression_ratio": 1.71976401179941, "no_speech_prob": 0.013635465875267982}, {"id": 23, "seek": 8408, "start": 102.56, "end": 106.96, "text": " other words, it's trained an agent to maximize their reward as its goal. And if you know anything", "tokens": [51288, 661, 2283, 11, 309, 311, 8895, 364, 9461, 281, 19874, 641, 7782, 382, 1080, 3387, 13, 400, 498, 291, 458, 1340, 51508], "temperature": 0.0, "avg_logprob": -0.09834280229152594, "compression_ratio": 1.71976401179941, "no_speech_prob": 0.013635465875267982}, {"id": 24, "seek": 8408, "start": 106.96, "end": 112.16, "text": " about RL, you know, it can be pretty sample inefficient. And when you combine sample inefficiency", "tokens": [51508, 466, 497, 43, 11, 291, 458, 11, 309, 393, 312, 1238, 6889, 43495, 13, 400, 562, 291, 10432, 6889, 7167, 49086, 51768], "temperature": 0.0, "avg_logprob": -0.09834280229152594, "compression_ratio": 1.71976401179941, "no_speech_prob": 0.013635465875267982}, {"id": 25, "seek": 11216, "start": 112.16, "end": 116.64, "text": " with scale, well, all hell breaks loose. As a little sneak peek of what's coming up, they actually", "tokens": [50364, 365, 4373, 11, 731, 11, 439, 4921, 9857, 9612, 13, 1018, 257, 707, 13164, 19604, 295, 437, 311, 1348, 493, 11, 436, 767, 50588], "temperature": 0.0, "avg_logprob": -0.0655706329683287, "compression_ratio": 1.6596491228070176, "no_speech_prob": 0.02096213772892952}, {"id": 26, "seek": 11216, "start": 116.64, "end": 122.56, "text": " show this later down in this paper, where they prepare a 23 million and a 265 million parameter", "tokens": [50588, 855, 341, 1780, 760, 294, 341, 3035, 11, 689, 436, 5940, 257, 6673, 2459, 293, 257, 7551, 20, 2459, 13075, 50884], "temperature": 0.0, "avg_logprob": -0.0655706329683287, "compression_ratio": 1.6596491228070176, "no_speech_prob": 0.02096213772892952}, {"id": 27, "seek": 11216, "start": 122.56, "end": 128.16, "text": " model and train both on 22 billion frames of total interactions. And without any changes,", "tokens": [50884, 2316, 293, 3847, 1293, 322, 5853, 5218, 12083, 295, 3217, 13280, 13, 400, 1553, 604, 2962, 11, 51164], "temperature": 0.0, "avg_logprob": -0.0655706329683287, "compression_ratio": 1.6596491228070176, "no_speech_prob": 0.02096213772892952}, {"id": 28, "seek": 11216, "start": 128.16, "end": 133.76, "text": " the bigger model actually performs significantly worse. In fact, it has a hard time learning at", "tokens": [51164, 264, 3801, 2316, 767, 26213, 10591, 5324, 13, 682, 1186, 11, 309, 575, 257, 1152, 565, 2539, 412, 51444], "temperature": 0.0, "avg_logprob": -0.0655706329683287, "compression_ratio": 1.6596491228070176, "no_speech_prob": 0.02096213772892952}, {"id": 29, "seek": 11216, "start": 133.76, "end": 138.16, "text": " all. As we'll see, the authors propose a way to combine ideas in the field to scale this up,", "tokens": [51444, 439, 13, 1018, 321, 603, 536, 11, 264, 16552, 17421, 257, 636, 281, 10432, 3487, 294, 264, 2519, 281, 4373, 341, 493, 11, 51664], "temperature": 0.0, "avg_logprob": -0.0655706329683287, "compression_ratio": 1.6596491228070176, "no_speech_prob": 0.02096213772892952}, {"id": 30, "seek": 13816, "start": 138.16, "end": 142.72, "text": " with the end goal being a framework for training an RL foundation model, because then instead of", "tokens": [50364, 365, 264, 917, 3387, 885, 257, 8388, 337, 3097, 364, 497, 43, 7030, 2316, 11, 570, 550, 2602, 295, 50592], "temperature": 0.0, "avg_logprob": -0.09369089370383356, "compression_ratio": 1.653958944281525, "no_speech_prob": 0.03732096776366234}, {"id": 31, "seek": 13816, "start": 142.72, "end": 147.2, "text": " always having to start training from scratch, you could have a baseline to significantly speed", "tokens": [50592, 1009, 1419, 281, 722, 3097, 490, 8459, 11, 291, 727, 362, 257, 20518, 281, 10591, 3073, 50816], "temperature": 0.0, "avg_logprob": -0.09369089370383356, "compression_ratio": 1.653958944281525, "no_speech_prob": 0.03732096776366234}, {"id": 32, "seek": 13816, "start": 147.2, "end": 151.76, "text": " up learning new tasks. You could think of this as the same way you can use GPT three to train", "tokens": [50816, 493, 2539, 777, 9608, 13, 509, 727, 519, 295, 341, 382, 264, 912, 636, 291, 393, 764, 26039, 51, 1045, 281, 3847, 51044], "temperature": 0.0, "avg_logprob": -0.09369089370383356, "compression_ratio": 1.653958944281525, "no_speech_prob": 0.03732096776366234}, {"id": 33, "seek": 13816, "start": 151.76, "end": 156.72, "text": " something like a sentiment analysis model, and just a few examples instead of well, millions.", "tokens": [51044, 746, 411, 257, 16149, 5215, 2316, 11, 293, 445, 257, 1326, 5110, 2602, 295, 731, 11, 6803, 13, 51292], "temperature": 0.0, "avg_logprob": -0.09369089370383356, "compression_ratio": 1.653958944281525, "no_speech_prob": 0.03732096776366234}, {"id": 34, "seek": 13816, "start": 156.72, "end": 161.51999999999998, "text": " Next, we're going to take a look at how ADA works. But before that, I want to thank clear ML for", "tokens": [51292, 3087, 11, 321, 434, 516, 281, 747, 257, 574, 412, 577, 39354, 1985, 13, 583, 949, 300, 11, 286, 528, 281, 1309, 1850, 21601, 337, 51532], "temperature": 0.0, "avg_logprob": -0.09369089370383356, "compression_ratio": 1.653958944281525, "no_speech_prob": 0.03732096776366234}, {"id": 35, "seek": 13816, "start": 161.51999999999998, "end": 165.35999999999999, "text": " making it possible for me to spend so much time on videos like this by being a sponsor.", "tokens": [51532, 1455, 309, 1944, 337, 385, 281, 3496, 370, 709, 565, 322, 2145, 411, 341, 538, 885, 257, 16198, 13, 51724], "temperature": 0.0, "avg_logprob": -0.09369089370383356, "compression_ratio": 1.653958944281525, "no_speech_prob": 0.03732096776366234}, {"id": 36, "seek": 16536, "start": 165.44000000000003, "end": 170.64000000000001, "text": " Clear ML offers an end end platform for ML ops, where you can do everything from tracking experiments", "tokens": [50368, 14993, 21601, 7736, 364, 917, 917, 3663, 337, 21601, 44663, 11, 689, 291, 393, 360, 1203, 490, 11603, 12050, 50628], "temperature": 0.0, "avg_logprob": -0.1104206374928921, "compression_ratio": 1.7598944591029024, "no_speech_prob": 0.005384635645896196}, {"id": 37, "seek": 16536, "start": 170.64000000000001, "end": 174.8, "text": " to automating an entire machine learning pipeline through to deployment. Here's the code for one", "tokens": [50628, 281, 3553, 990, 364, 2302, 3479, 2539, 15517, 807, 281, 19317, 13, 1692, 311, 264, 3089, 337, 472, 50836], "temperature": 0.0, "avg_logprob": -0.1104206374928921, "compression_ratio": 1.7598944591029024, "no_speech_prob": 0.005384635645896196}, {"id": 38, "seek": 16536, "start": 174.8, "end": 178.64000000000001, "text": " of my current projects. And with just an extra several lines of code, I can integrate it with", "tokens": [50836, 295, 452, 2190, 4455, 13, 400, 365, 445, 364, 2857, 2940, 3876, 295, 3089, 11, 286, 393, 13365, 309, 365, 51028], "temperature": 0.0, "avg_logprob": -0.1104206374928921, "compression_ratio": 1.7598944591029024, "no_speech_prob": 0.005384635645896196}, {"id": 39, "seek": 16536, "start": 178.64000000000001, "end": 182.8, "text": " clear mail. And with that, we can now pull up a dashboard and see the progress of my experiment", "tokens": [51028, 1850, 10071, 13, 400, 365, 300, 11, 321, 393, 586, 2235, 493, 257, 18342, 293, 536, 264, 4205, 295, 452, 5120, 51236], "temperature": 0.0, "avg_logprob": -0.1104206374928921, "compression_ratio": 1.7598944591029024, "no_speech_prob": 0.005384635645896196}, {"id": 40, "seek": 16536, "start": 182.8, "end": 186.8, "text": " as it runs. Here, I'm just logging the loss because I wanted to get this done in a few seconds,", "tokens": [51236, 382, 309, 6676, 13, 1692, 11, 286, 478, 445, 27991, 264, 4470, 570, 286, 1415, 281, 483, 341, 1096, 294, 257, 1326, 3949, 11, 51436], "temperature": 0.0, "avg_logprob": -0.1104206374928921, "compression_ratio": 1.7598944591029024, "no_speech_prob": 0.005384635645896196}, {"id": 41, "seek": 16536, "start": 186.8, "end": 190.88000000000002, "text": " but there is a lot more you can do with this. But really, this is just the tip of the iceberg.", "tokens": [51436, 457, 456, 307, 257, 688, 544, 291, 393, 360, 365, 341, 13, 583, 534, 11, 341, 307, 445, 264, 4125, 295, 264, 38880, 13, 51640], "temperature": 0.0, "avg_logprob": -0.1104206374928921, "compression_ratio": 1.7598944591029024, "no_speech_prob": 0.005384635645896196}, {"id": 42, "seek": 16536, "start": 190.88000000000002, "end": 194.72000000000003, "text": " You can also create entire pipelines that pull in different versions of your data sets,", "tokens": [51640, 509, 393, 611, 1884, 2302, 40168, 300, 2235, 294, 819, 9606, 295, 428, 1412, 6352, 11, 51832], "temperature": 0.0, "avg_logprob": -0.1104206374928921, "compression_ratio": 1.7598944591029024, "no_speech_prob": 0.005384635645896196}, {"id": 43, "seek": 19472, "start": 194.72, "end": 199.52, "text": " run hyper parameter sweeps, automatically set up new environments. And because of that last point,", "tokens": [50364, 1190, 9848, 13075, 2484, 10653, 11, 6772, 992, 493, 777, 12388, 13, 400, 570, 295, 300, 1036, 935, 11, 50604], "temperature": 0.0, "avg_logprob": -0.07178359418301969, "compression_ratio": 1.7247956403269755, "no_speech_prob": 0.001548710628412664}, {"id": 44, "seek": 19472, "start": 199.52, "end": 204.0, "text": " clear mail has no problem with scaling. As I run many experiments in my own research,", "tokens": [50604, 1850, 10071, 575, 572, 1154, 365, 21589, 13, 1018, 286, 1190, 867, 12050, 294, 452, 1065, 2132, 11, 50828], "temperature": 0.0, "avg_logprob": -0.07178359418301969, "compression_ratio": 1.7247956403269755, "no_speech_prob": 0.001548710628412664}, {"id": 45, "seek": 19472, "start": 204.0, "end": 207.76, "text": " I personally love some of the features that help me with that. For example, I love how clear", "tokens": [50828, 286, 5665, 959, 512, 295, 264, 4122, 300, 854, 385, 365, 300, 13, 1171, 1365, 11, 286, 959, 577, 1850, 51016], "temperature": 0.0, "avg_logprob": -0.07178359418301969, "compression_ratio": 1.7247956403269755, "no_speech_prob": 0.001548710628412664}, {"id": 46, "seek": 19472, "start": 207.76, "end": 212.48, "text": " mail not only tracks code version, but also uncommitted get changes, which is very nice.", "tokens": [51016, 10071, 406, 787, 10218, 3089, 3037, 11, 457, 611, 8585, 76, 3944, 483, 2962, 11, 597, 307, 588, 1481, 13, 51252], "temperature": 0.0, "avg_logprob": -0.07178359418301969, "compression_ratio": 1.7247956403269755, "no_speech_prob": 0.001548710628412664}, {"id": 47, "seek": 19472, "start": 212.48, "end": 216.4, "text": " And also how through the dashboard, I can inject configs into my experiments,", "tokens": [51252, 400, 611, 577, 807, 264, 18342, 11, 286, 393, 10711, 6662, 82, 666, 452, 12050, 11, 51448], "temperature": 0.0, "avg_logprob": -0.07178359418301969, "compression_ratio": 1.7247956403269755, "no_speech_prob": 0.001548710628412664}, {"id": 48, "seek": 19472, "start": 216.4, "end": 220.88, "text": " which means I don't have to constantly be modifying my code to run a bunch of new experiments.", "tokens": [51448, 597, 1355, 286, 500, 380, 362, 281, 6460, 312, 42626, 452, 3089, 281, 1190, 257, 3840, 295, 777, 12050, 13, 51672], "temperature": 0.0, "avg_logprob": -0.07178359418301969, "compression_ratio": 1.7247956403269755, "no_speech_prob": 0.001548710628412664}, {"id": 49, "seek": 19472, "start": 220.88, "end": 224.64, "text": " If that sounds like something you're interested in, you can try out their product for free by", "tokens": [51672, 759, 300, 3263, 411, 746, 291, 434, 3102, 294, 11, 291, 393, 853, 484, 641, 1674, 337, 1737, 538, 51860], "temperature": 0.0, "avg_logprob": -0.07178359418301969, "compression_ratio": 1.7247956403269755, "no_speech_prob": 0.001548710628412664}, {"id": 50, "seek": 22464, "start": 224.64, "end": 228.88, "text": " following the link in the description below. And now let's get back into this paper and talk", "tokens": [50364, 3480, 264, 2113, 294, 264, 3855, 2507, 13, 400, 586, 718, 311, 483, 646, 666, 341, 3035, 293, 751, 50576], "temperature": 0.0, "avg_logprob": -0.112908136726606, "compression_ratio": 1.625418060200669, "no_speech_prob": 0.004754579160362482}, {"id": 51, "seek": 22464, "start": 228.88, "end": 233.35999999999999, "text": " about how 80 works right off the bat. The authors get pretty straight to the point by listing the", "tokens": [50576, 466, 577, 4688, 1985, 558, 766, 264, 7362, 13, 440, 16552, 483, 1238, 2997, 281, 264, 935, 538, 22161, 264, 50800], "temperature": 0.0, "avg_logprob": -0.112908136726606, "compression_ratio": 1.625418060200669, "no_speech_prob": 0.004754579160362482}, {"id": 52, "seek": 22464, "start": 233.35999999999999, "end": 238.72, "text": " three key components that allow for adaptation to emerge. One is meta reinforcement learning", "tokens": [50800, 1045, 2141, 6677, 300, 2089, 337, 21549, 281, 21511, 13, 1485, 307, 19616, 29280, 2539, 51068], "temperature": 0.0, "avg_logprob": -0.112908136726606, "compression_ratio": 1.625418060200669, "no_speech_prob": 0.004754579160362482}, {"id": 53, "seek": 22464, "start": 238.72, "end": 244.56, "text": " across a vast, smooth and diverse task distribution. Two is a policy parameter prize as a large scale", "tokens": [51068, 2108, 257, 8369, 11, 5508, 293, 9521, 5633, 7316, 13, 4453, 307, 257, 3897, 13075, 12818, 382, 257, 2416, 4373, 51360], "temperature": 0.0, "avg_logprob": -0.112908136726606, "compression_ratio": 1.625418060200669, "no_speech_prob": 0.004754579160362482}, {"id": 54, "seek": 22464, "start": 244.56, "end": 250.32, "text": " attention based memory architecture. And three is an effective automated curriculum that prioritizes", "tokens": [51360, 3202, 2361, 4675, 9482, 13, 400, 1045, 307, 364, 4942, 18473, 14302, 300, 14846, 5660, 51648], "temperature": 0.0, "avg_logprob": -0.112908136726606, "compression_ratio": 1.625418060200669, "no_speech_prob": 0.004754579160362482}, {"id": 55, "seek": 25032, "start": 250.32, "end": 254.79999999999998, "text": " tasks at the frontier of the agent's capabilities. If we scroll down a bit, we'll see a high level", "tokens": [50364, 9608, 412, 264, 35853, 295, 264, 9461, 311, 10862, 13, 759, 321, 11369, 760, 257, 857, 11, 321, 603, 536, 257, 1090, 1496, 50588], "temperature": 0.0, "avg_logprob": -0.08705114972764168, "compression_ratio": 1.8164556962025316, "no_speech_prob": 0.025954831391572952}, {"id": 56, "seek": 25032, "start": 254.79999999999998, "end": 259.04, "text": " visualization of the whole training process. And we can use this as we talk about each of these", "tokens": [50588, 25801, 295, 264, 1379, 3097, 1399, 13, 400, 321, 393, 764, 341, 382, 321, 751, 466, 1184, 295, 613, 50800], "temperature": 0.0, "avg_logprob": -0.08705114972764168, "compression_ratio": 1.8164556962025316, "no_speech_prob": 0.025954831391572952}, {"id": 57, "seek": 25032, "start": 259.04, "end": 264.48, "text": " steps. Going back to our first point, we have meta learning over a diverse set of tasks, where in", "tokens": [50800, 4439, 13, 10963, 646, 281, 527, 700, 935, 11, 321, 362, 19616, 2539, 670, 257, 9521, 992, 295, 9608, 11, 689, 294, 51072], "temperature": 0.0, "avg_logprob": -0.08705114972764168, "compression_ratio": 1.8164556962025316, "no_speech_prob": 0.025954831391572952}, {"id": 58, "seek": 25032, "start": 264.48, "end": 269.12, "text": " our diagram here, we have that set of diverse tasks. And here we have the RL update that will", "tokens": [51072, 527, 10686, 510, 11, 321, 362, 300, 992, 295, 9521, 9608, 13, 400, 510, 321, 362, 264, 497, 43, 5623, 300, 486, 51304], "temperature": 0.0, "avg_logprob": -0.08705114972764168, "compression_ratio": 1.8164556962025316, "no_speech_prob": 0.025954831391572952}, {"id": 59, "seek": 25032, "start": 269.12, "end": 273.36, "text": " allow us to do meta learning, perhaps the most interesting thing about the use of meta RL here", "tokens": [51304, 2089, 505, 281, 360, 19616, 2539, 11, 4317, 264, 881, 1880, 551, 466, 264, 764, 295, 19616, 497, 43, 510, 51516], "temperature": 0.0, "avg_logprob": -0.08705114972764168, "compression_ratio": 1.8164556962025316, "no_speech_prob": 0.025954831391572952}, {"id": 60, "seek": 25032, "start": 273.36, "end": 277.52, "text": " is that the meta learning comes from how the environment tasks are structured and not the RL", "tokens": [51516, 307, 300, 264, 19616, 2539, 1487, 490, 577, 264, 2823, 9608, 366, 18519, 293, 406, 264, 497, 43, 51724], "temperature": 0.0, "avg_logprob": -0.08705114972764168, "compression_ratio": 1.8164556962025316, "no_speech_prob": 0.025954831391572952}, {"id": 61, "seek": 27752, "start": 277.52, "end": 281.91999999999996, "text": " algorithm itself. So let's take a look at the environment they use here and this key structuring", "tokens": [50364, 9284, 2564, 13, 407, 718, 311, 747, 257, 574, 412, 264, 2823, 436, 764, 510, 293, 341, 2141, 6594, 1345, 50584], "temperature": 0.0, "avg_logprob": -0.08299969915133804, "compression_ratio": 1.757396449704142, "no_speech_prob": 0.01065191999077797}, {"id": 62, "seek": 27752, "start": 281.91999999999996, "end": 286.71999999999997, "text": " that I'm referring to the environment is called X land to, and as opposed to being a single environment,", "tokens": [50584, 300, 286, 478, 13761, 281, 264, 2823, 307, 1219, 1783, 2117, 281, 11, 293, 382, 8851, 281, 885, 257, 2167, 2823, 11, 50824], "temperature": 0.0, "avg_logprob": -0.08299969915133804, "compression_ratio": 1.757396449704142, "no_speech_prob": 0.01065191999077797}, {"id": 63, "seek": 27752, "start": 286.71999999999997, "end": 290.71999999999997, "text": " it's built to be completely customizable so that you can make your own suite of environments.", "tokens": [50824, 309, 311, 3094, 281, 312, 2584, 47922, 370, 300, 291, 393, 652, 428, 1065, 14205, 295, 12388, 13, 51024], "temperature": 0.0, "avg_logprob": -0.08299969915133804, "compression_ratio": 1.757396449704142, "no_speech_prob": 0.01065191999077797}, {"id": 64, "seek": 27752, "start": 290.71999999999997, "end": 295.68, "text": " Each different instance consists of one to two agents. Yes, they also do some multi agent learning", "tokens": [51024, 6947, 819, 5197, 14689, 295, 472, 281, 732, 12554, 13, 1079, 11, 436, 611, 360, 512, 4825, 9461, 2539, 51272], "temperature": 0.0, "avg_logprob": -0.08299969915133804, "compression_ratio": 1.757396449704142, "no_speech_prob": 0.01065191999077797}, {"id": 65, "seek": 27752, "start": 295.68, "end": 300.08, "text": " here, which we'll get into. Then there are some initial objects that start out in the world rules", "tokens": [51272, 510, 11, 597, 321, 603, 483, 666, 13, 1396, 456, 366, 512, 5883, 6565, 300, 722, 484, 294, 264, 1002, 4474, 51492], "temperature": 0.0, "avg_logprob": -0.08299969915133804, "compression_ratio": 1.757396449704142, "no_speech_prob": 0.01065191999077797}, {"id": 66, "seek": 27752, "start": 300.08, "end": 305.28, "text": " for how those objects interact. For example, this rule right here specifies that when a black pyramid", "tokens": [51492, 337, 577, 729, 6565, 4648, 13, 1171, 1365, 11, 341, 4978, 558, 510, 1608, 11221, 300, 562, 257, 2211, 25950, 51752], "temperature": 0.0, "avg_logprob": -0.08299969915133804, "compression_ratio": 1.757396449704142, "no_speech_prob": 0.01065191999077797}, {"id": 67, "seek": 30528, "start": 305.28, "end": 309.2, "text": " touches a yellow sphere, it generates a black cube in their place. There are different types", "tokens": [50364, 17431, 257, 5566, 16687, 11, 309, 23815, 257, 2211, 13728, 294, 641, 1081, 13, 821, 366, 819, 3467, 50560], "temperature": 0.0, "avg_logprob": -0.046092450618743896, "compression_ratio": 1.7592592592592593, "no_speech_prob": 0.025175021961331367}, {"id": 68, "seek": 30528, "start": 309.2, "end": 313.52, "text": " of terrain. And then there is a goal for the agent to achieve, which in this case, the goal", "tokens": [50560, 295, 17674, 13, 400, 550, 456, 307, 257, 3387, 337, 264, 9461, 281, 4584, 11, 597, 294, 341, 1389, 11, 264, 3387, 50776], "temperature": 0.0, "avg_logprob": -0.046092450618743896, "compression_ratio": 1.7592592592592593, "no_speech_prob": 0.025175021961331367}, {"id": 69, "seek": 30528, "start": 313.52, "end": 317.84, "text": " is to hold the black cube for as long as possible. And then there's also a pretty strict time limit", "tokens": [50776, 307, 281, 1797, 264, 2211, 13728, 337, 382, 938, 382, 1944, 13, 400, 550, 456, 311, 611, 257, 1238, 10910, 565, 4948, 50992], "temperature": 0.0, "avg_logprob": -0.046092450618743896, "compression_ratio": 1.7592592592592593, "no_speech_prob": 0.025175021961331367}, {"id": 70, "seek": 30528, "start": 317.84, "end": 322.08, "text": " for all of this. And if all of that wasn't hard enough already, several of these specifications", "tokens": [50992, 337, 439, 295, 341, 13, 400, 498, 439, 295, 300, 2067, 380, 1152, 1547, 1217, 11, 2940, 295, 613, 29448, 51204], "temperature": 0.0, "avg_logprob": -0.046092450618743896, "compression_ratio": 1.7592592592592593, "no_speech_prob": 0.025175021961331367}, {"id": 71, "seek": 30528, "start": 322.08, "end": 326.32, "text": " can actually be hidden from the agent itself. Sorry, I've cut it off a little bit here. For", "tokens": [51204, 393, 767, 312, 7633, 490, 264, 9461, 2564, 13, 4919, 11, 286, 600, 1723, 309, 766, 257, 707, 857, 510, 13, 1171, 51416], "temperature": 0.0, "avg_logprob": -0.046092450618743896, "compression_ratio": 1.7592592592592593, "no_speech_prob": 0.025175021961331367}, {"id": 72, "seek": 30528, "start": 326.32, "end": 330.71999999999997, "text": " each task instantiation, each of these components are randomized, which ends up giving a whopping", "tokens": [51416, 1184, 5633, 9836, 6642, 11, 1184, 295, 613, 6677, 366, 38513, 11, 597, 5314, 493, 2902, 257, 50043, 51636], "temperature": 0.0, "avg_logprob": -0.046092450618743896, "compression_ratio": 1.7592592592592593, "no_speech_prob": 0.025175021961331367}, {"id": 73, "seek": 33072, "start": 330.72, "end": 335.6, "text": " total of 10 to the 40 unique task combinations. That's, that's a lot of tasks. When the authors", "tokens": [50364, 3217, 295, 1266, 281, 264, 3356, 3845, 5633, 21267, 13, 663, 311, 11, 300, 311, 257, 688, 295, 9608, 13, 1133, 264, 16552, 50608], "temperature": 0.0, "avg_logprob": -0.0846485947117661, "compression_ratio": 1.724770642201835, "no_speech_prob": 0.03209805116057396}, {"id": 74, "seek": 33072, "start": 335.6, "end": 339.84000000000003, "text": " said having diverse tasks was essential, they certainly were not skimping on this. But now", "tokens": [50608, 848, 1419, 9521, 9608, 390, 7115, 11, 436, 3297, 645, 406, 1110, 332, 3381, 322, 341, 13, 583, 586, 50820], "temperature": 0.0, "avg_logprob": -0.0846485947117661, "compression_ratio": 1.724770642201835, "no_speech_prob": 0.03209805116057396}, {"id": 75, "seek": 33072, "start": 339.84000000000003, "end": 344.24, "text": " we can take a look at the part of the task setup that makes this into a meta RL formulation. And", "tokens": [50820, 321, 393, 747, 257, 574, 412, 264, 644, 295, 264, 5633, 8657, 300, 1669, 341, 666, 257, 19616, 497, 43, 37642, 13, 400, 51040], "temperature": 0.0, "avg_logprob": -0.0846485947117661, "compression_ratio": 1.724770642201835, "no_speech_prob": 0.03209805116057396}, {"id": 76, "seek": 33072, "start": 344.24, "end": 348.88000000000005, "text": " that has to do with how they structure these trials and episodes. Typically in episodic", "tokens": [51040, 300, 575, 281, 360, 365, 577, 436, 3877, 613, 12450, 293, 9313, 13, 23129, 294, 39200, 299, 51272], "temperature": 0.0, "avg_logprob": -0.0846485947117661, "compression_ratio": 1.724770642201835, "no_speech_prob": 0.03209805116057396}, {"id": 77, "seek": 33072, "start": 348.88000000000005, "end": 353.84000000000003, "text": " reinforcement learning, everything is reset after each episode. And the same process repeats episode", "tokens": [51272, 29280, 2539, 11, 1203, 307, 14322, 934, 1184, 3500, 13, 400, 264, 912, 1399, 35038, 3500, 51520], "temperature": 0.0, "avg_logprob": -0.0846485947117661, "compression_ratio": 1.724770642201835, "no_speech_prob": 0.03209805116057396}, {"id": 78, "seek": 33072, "start": 353.84000000000003, "end": 358.56, "text": " after episode. But here they use the RL squared algorithm for meta learning, which is a bit", "tokens": [51520, 934, 3500, 13, 583, 510, 436, 764, 264, 497, 43, 8889, 9284, 337, 19616, 2539, 11, 597, 307, 257, 857, 51756], "temperature": 0.0, "avg_logprob": -0.0846485947117661, "compression_ratio": 1.724770642201835, "no_speech_prob": 0.03209805116057396}, {"id": 79, "seek": 35856, "start": 358.56, "end": 363.12, "text": " different here for each task, a number of trials is determined, which is generally between one and", "tokens": [50364, 819, 510, 337, 1184, 5633, 11, 257, 1230, 295, 12450, 307, 9540, 11, 597, 307, 5101, 1296, 472, 293, 50592], "temperature": 0.0, "avg_logprob": -0.0692677567986881, "compression_ratio": 1.8814102564102564, "no_speech_prob": 0.009708148427307606}, {"id": 80, "seek": 35856, "start": 363.12, "end": 367.52, "text": " six while they're training their models. Each trial works kind of like an episode typically would", "tokens": [50592, 2309, 1339, 436, 434, 3097, 641, 5245, 13, 6947, 7308, 1985, 733, 295, 411, 364, 3500, 5850, 576, 50812], "temperature": 0.0, "avg_logprob": -0.0692677567986881, "compression_ratio": 1.8814102564102564, "no_speech_prob": 0.009708148427307606}, {"id": 81, "seek": 35856, "start": 367.52, "end": 371.6, "text": " the same task is used. That means the same environment, the same goal, the same everything.", "tokens": [50812, 264, 912, 5633, 307, 1143, 13, 663, 1355, 264, 912, 2823, 11, 264, 912, 3387, 11, 264, 912, 1203, 13, 51016], "temperature": 0.0, "avg_logprob": -0.0692677567986881, "compression_ratio": 1.8814102564102564, "no_speech_prob": 0.009708148427307606}, {"id": 82, "seek": 35856, "start": 371.6, "end": 376.64, "text": " And every time a trial ends, everything is reset, except for one thing. And this is where it differs", "tokens": [51016, 400, 633, 565, 257, 7308, 5314, 11, 1203, 307, 14322, 11, 3993, 337, 472, 551, 13, 400, 341, 307, 689, 309, 37761, 51268], "temperature": 0.0, "avg_logprob": -0.0692677567986881, "compression_ratio": 1.8814102564102564, "no_speech_prob": 0.009708148427307606}, {"id": 83, "seek": 35856, "start": 376.64, "end": 381.68, "text": " from an episode. The one thing that is not reset that the agent has is a memory module that memory", "tokens": [51268, 490, 364, 3500, 13, 440, 472, 551, 300, 307, 406, 14322, 300, 264, 9461, 575, 307, 257, 4675, 10088, 300, 4675, 51520], "temperature": 0.0, "avg_logprob": -0.0692677567986881, "compression_ratio": 1.8814102564102564, "no_speech_prob": 0.009708148427307606}, {"id": 84, "seek": 35856, "start": 381.68, "end": 387.2, "text": " module persists through trials. And it's only reset after all the trials are over at the end of an", "tokens": [51520, 10088, 868, 1751, 807, 12450, 13, 400, 309, 311, 787, 14322, 934, 439, 264, 12450, 366, 670, 412, 264, 917, 295, 364, 51796], "temperature": 0.0, "avg_logprob": -0.0692677567986881, "compression_ratio": 1.8814102564102564, "no_speech_prob": 0.009708148427307606}, {"id": 85, "seek": 38720, "start": 387.2, "end": 392.47999999999996, "text": " episode. Well, this may seem like not a huge deal. This is actually essential for how this works,", "tokens": [50364, 3500, 13, 1042, 11, 341, 815, 1643, 411, 406, 257, 2603, 2028, 13, 639, 307, 767, 7115, 337, 577, 341, 1985, 11, 50628], "temperature": 0.0, "avg_logprob": -0.06647748806897331, "compression_ratio": 1.8366013071895424, "no_speech_prob": 0.004198480397462845}, {"id": 86, "seek": 38720, "start": 392.47999999999996, "end": 397.03999999999996, "text": " because if the agent can remember previous trials, then it can use its previous exploration", "tokens": [50628, 570, 498, 264, 9461, 393, 1604, 3894, 12450, 11, 550, 309, 393, 764, 1080, 3894, 16197, 50856], "temperature": 0.0, "avg_logprob": -0.06647748806897331, "compression_ratio": 1.8366013071895424, "no_speech_prob": 0.004198480397462845}, {"id": 87, "seek": 38720, "start": 397.03999999999996, "end": 401.2, "text": " to improve in future trials. And this works because the agent has memory. So if it tries", "tokens": [50856, 281, 3470, 294, 2027, 12450, 13, 400, 341, 1985, 570, 264, 9461, 575, 4675, 13, 407, 498, 309, 9898, 51064], "temperature": 0.0, "avg_logprob": -0.06647748806897331, "compression_ratio": 1.8366013071895424, "no_speech_prob": 0.004198480397462845}, {"id": 88, "seek": 38720, "start": 401.2, "end": 405.12, "text": " something in trial one, that doesn't work, and it still has that memory going into trial two,", "tokens": [51064, 746, 294, 7308, 472, 11, 300, 1177, 380, 589, 11, 293, 309, 920, 575, 300, 4675, 516, 666, 7308, 732, 11, 51260], "temperature": 0.0, "avg_logprob": -0.06647748806897331, "compression_ratio": 1.8366013071895424, "no_speech_prob": 0.004198480397462845}, {"id": 89, "seek": 38720, "start": 405.12, "end": 408.96, "text": " well, it's probably not going to try the same thing in trial two. And that's the idea here.", "tokens": [51260, 731, 11, 309, 311, 1391, 406, 516, 281, 853, 264, 912, 551, 294, 7308, 732, 13, 400, 300, 311, 264, 1558, 510, 13, 51452], "temperature": 0.0, "avg_logprob": -0.06647748806897331, "compression_ratio": 1.8366013071895424, "no_speech_prob": 0.004198480397462845}, {"id": 90, "seek": 38720, "start": 408.96, "end": 412.56, "text": " What you're looking at right now is an example of this where the agent learns that if it picks up", "tokens": [51452, 708, 291, 434, 1237, 412, 558, 586, 307, 364, 1365, 295, 341, 689, 264, 9461, 27152, 300, 498, 309, 16137, 493, 51632], "temperature": 0.0, "avg_logprob": -0.06647748806897331, "compression_ratio": 1.8366013071895424, "no_speech_prob": 0.004198480397462845}, {"id": 91, "seek": 41256, "start": 412.56, "end": 417.2, "text": " this object, it disappears. So in the next trial, it learns from that and pushes the object instead.", "tokens": [50364, 341, 2657, 11, 309, 25527, 13, 407, 294, 264, 958, 7308, 11, 309, 27152, 490, 300, 293, 21020, 264, 2657, 2602, 13, 50596], "temperature": 0.0, "avg_logprob": -0.09261778422764369, "compression_ratio": 1.8158730158730159, "no_speech_prob": 0.061864446848630905}, {"id": 92, "seek": 41256, "start": 417.2, "end": 421.28000000000003, "text": " Without making any changes to how the RL algorithm itself works, we've turned this", "tokens": [50596, 9129, 1455, 604, 2962, 281, 577, 264, 497, 43, 9284, 2564, 1985, 11, 321, 600, 3574, 341, 50800], "temperature": 0.0, "avg_logprob": -0.09261778422764369, "compression_ratio": 1.8158730158730159, "no_speech_prob": 0.061864446848630905}, {"id": 93, "seek": 41256, "start": 421.28000000000003, "end": 425.68, "text": " into a meta reinforcement learning problem. And speaking of the RL algorithm, which one do they", "tokens": [50800, 666, 257, 19616, 29280, 2539, 1154, 13, 400, 4124, 295, 264, 497, 43, 9284, 11, 597, 472, 360, 436, 51020], "temperature": 0.0, "avg_logprob": -0.09261778422764369, "compression_ratio": 1.8158730158730159, "no_speech_prob": 0.061864446848630905}, {"id": 94, "seek": 41256, "start": 425.68, "end": 430.32, "text": " use here? Well, in theory, any solid reinforcement learning algorithm could work here. But for this,", "tokens": [51020, 764, 510, 30, 1042, 11, 294, 5261, 11, 604, 5100, 29280, 2539, 9284, 727, 589, 510, 13, 583, 337, 341, 11, 51252], "temperature": 0.0, "avg_logprob": -0.09261778422764369, "compression_ratio": 1.8158730158730159, "no_speech_prob": 0.061864446848630905}, {"id": 95, "seek": 41256, "start": 430.32, "end": 435.44, "text": " they decide to use Mooseley or Mooseley or I don't know, hopefully I'm not saying it too wrong.", "tokens": [51252, 436, 4536, 281, 764, 3335, 541, 3420, 420, 3335, 541, 3420, 420, 286, 500, 380, 458, 11, 4696, 286, 478, 406, 1566, 309, 886, 2085, 13, 51508], "temperature": 0.0, "avg_logprob": -0.09261778422764369, "compression_ratio": 1.8158730158730159, "no_speech_prob": 0.061864446848630905}, {"id": 96, "seek": 41256, "start": 435.44, "end": 440.4, "text": " Mooseley is a model based reinforcement learning algorithm that's based off MPO, but adds a few", "tokens": [51508, 3335, 541, 3420, 307, 257, 2316, 2361, 29280, 2539, 9284, 300, 311, 2361, 766, 14146, 46, 11, 457, 10860, 257, 1326, 51756], "temperature": 0.0, "avg_logprob": -0.09261778422764369, "compression_ratio": 1.8158730158730159, "no_speech_prob": 0.061864446848630905}, {"id": 97, "seek": 44040, "start": 440.4, "end": 444.71999999999997, "text": " bells and whistles, including a value equivalent model component. Looking at the Mooseley paper,", "tokens": [50364, 25474, 293, 49282, 11, 3009, 257, 2158, 10344, 2316, 6542, 13, 11053, 412, 264, 3335, 541, 3420, 3035, 11, 50580], "temperature": 0.0, "avg_logprob": -0.0718705544104943, "compression_ratio": 1.7053571428571428, "no_speech_prob": 0.011331104673445225}, {"id": 98, "seek": 44040, "start": 444.71999999999997, "end": 450.0, "text": " we can see that it performs about the same as mu zero on Atari, but without the huge overhead", "tokens": [50580, 321, 393, 536, 300, 309, 26213, 466, 264, 912, 382, 2992, 4018, 322, 41381, 11, 457, 1553, 264, 2603, 19922, 50844], "temperature": 0.0, "avg_logprob": -0.0718705544104943, "compression_ratio": 1.7053571428571428, "no_speech_prob": 0.011331104673445225}, {"id": 99, "seek": 44040, "start": 450.0, "end": 454.79999999999995, "text": " of having to do planning. I think one key thing to note here is that Mooseley's model does not try", "tokens": [50844, 295, 1419, 281, 360, 5038, 13, 286, 519, 472, 2141, 551, 281, 3637, 510, 307, 300, 3335, 541, 3420, 311, 2316, 775, 406, 853, 51084], "temperature": 0.0, "avg_logprob": -0.0718705544104943, "compression_ratio": 1.7053571428571428, "no_speech_prob": 0.011331104673445225}, {"id": 100, "seek": 44040, "start": 454.79999999999995, "end": 459.76, "text": " to reconstruct observations or predict future observations, which is perfectly fine in and", "tokens": [51084, 281, 31499, 18163, 420, 6069, 2027, 18163, 11, 597, 307, 6239, 2489, 294, 293, 51332], "temperature": 0.0, "avg_logprob": -0.0718705544104943, "compression_ratio": 1.7053571428571428, "no_speech_prob": 0.011331104673445225}, {"id": 101, "seek": 44040, "start": 459.76, "end": 464.56, "text": " of itself. But other model based methods that have done this, like efficient zero and dreamer,", "tokens": [51332, 295, 2564, 13, 583, 661, 2316, 2361, 7150, 300, 362, 1096, 341, 11, 411, 7148, 4018, 293, 3055, 260, 11, 51572], "temperature": 0.0, "avg_logprob": -0.0718705544104943, "compression_ratio": 1.7053571428571428, "no_speech_prob": 0.011331104673445225}, {"id": 102, "seek": 44040, "start": 464.56, "end": 469.28, "text": " tend to be significantly more sample efficient. As you'll see, the training in this paper happens", "tokens": [51572, 3928, 281, 312, 10591, 544, 6889, 7148, 13, 1018, 291, 603, 536, 11, 264, 3097, 294, 341, 3035, 2314, 51808], "temperature": 0.0, "avg_logprob": -0.0718705544104943, "compression_ratio": 1.7053571428571428, "no_speech_prob": 0.011331104673445225}, {"id": 103, "seek": 46928, "start": 469.28, "end": 473.91999999999996, "text": " over billions of environment steps. So perhaps that change would be a simple way to make this", "tokens": [50364, 670, 17375, 295, 2823, 4439, 13, 407, 4317, 300, 1319, 576, 312, 257, 2199, 636, 281, 652, 341, 50596], "temperature": 0.0, "avg_logprob": -0.07628057186420147, "compression_ratio": 1.84375, "no_speech_prob": 0.006903463508933783}, {"id": 104, "seek": 46928, "start": 473.91999999999996, "end": 478.08, "text": " more sample efficient. But perhaps one thing that's even more important than sample efficiency in this", "tokens": [50596, 544, 6889, 7148, 13, 583, 4317, 472, 551, 300, 311, 754, 544, 1021, 813, 6889, 10493, 294, 341, 50804], "temperature": 0.0, "avg_logprob": -0.07628057186420147, "compression_ratio": 1.84375, "no_speech_prob": 0.006903463508933783}, {"id": 105, "seek": 46928, "start": 478.08, "end": 483.28, "text": " case is subscribing to the channel because without subscribing, happy doggo may be reconstructed into", "tokens": [50804, 1389, 307, 19981, 281, 264, 2269, 570, 1553, 19981, 11, 2055, 3000, 1571, 815, 312, 31499, 292, 666, 51064], "temperature": 0.0, "avg_logprob": -0.07628057186420147, "compression_ratio": 1.84375, "no_speech_prob": 0.006903463508933783}, {"id": 106, "seek": 46928, "start": 483.28, "end": 487.59999999999997, "text": " mad doggo. And no one wants that. And if you want more machine learning content like this, you know,", "tokens": [51064, 5244, 3000, 1571, 13, 400, 572, 472, 2738, 300, 13, 400, 498, 291, 528, 544, 3479, 2539, 2701, 411, 341, 11, 291, 458, 11, 51280], "temperature": 0.0, "avg_logprob": -0.07628057186420147, "compression_ratio": 1.84375, "no_speech_prob": 0.006903463508933783}, {"id": 107, "seek": 46928, "start": 487.59999999999997, "end": 491.59999999999997, "text": " consider subscribing. Anyway, I'll link the paper in the description if you want more details,", "tokens": [51280, 1949, 19981, 13, 5684, 11, 286, 603, 2113, 264, 3035, 294, 264, 3855, 498, 291, 528, 544, 4365, 11, 51480], "temperature": 0.0, "avg_logprob": -0.07628057186420147, "compression_ratio": 1.84375, "no_speech_prob": 0.006903463508933783}, {"id": 108, "seek": 46928, "start": 491.59999999999997, "end": 495.67999999999995, "text": " but it's not super important for understanding the takeaways of this paper. Awesome. With that,", "tokens": [51480, 457, 309, 311, 406, 1687, 1021, 337, 3701, 264, 45584, 295, 341, 3035, 13, 10391, 13, 2022, 300, 11, 51684], "temperature": 0.0, "avg_logprob": -0.07628057186420147, "compression_ratio": 1.84375, "no_speech_prob": 0.006903463508933783}, {"id": 109, "seek": 49568, "start": 495.76, "end": 500.24, "text": " the way we can check meta reinforcement learning off the list and move on to part two, which is", "tokens": [50368, 264, 636, 321, 393, 1520, 19616, 29280, 2539, 766, 264, 1329, 293, 1286, 322, 281, 644, 732, 11, 597, 307, 50592], "temperature": 0.0, "avg_logprob": -0.10107438492052483, "compression_ratio": 1.797427652733119, "no_speech_prob": 0.1293749213218689}, {"id": 110, "seek": 49568, "start": 500.24, "end": 505.44, "text": " having a large scale attention based memory architecture, large architectures, attention.", "tokens": [50592, 1419, 257, 2416, 4373, 3202, 2361, 4675, 9482, 11, 2416, 6331, 1303, 11, 3202, 13, 50852], "temperature": 0.0, "avg_logprob": -0.10107438492052483, "compression_ratio": 1.797427652733119, "no_speech_prob": 0.1293749213218689}, {"id": 111, "seek": 49568, "start": 506.24, "end": 511.12, "text": " I wonder what that could be. It's right here. How could I miss this? A transformer who could", "tokens": [50892, 286, 2441, 437, 300, 727, 312, 13, 467, 311, 558, 510, 13, 1012, 727, 286, 1713, 341, 30, 316, 31782, 567, 727, 51136], "temperature": 0.0, "avg_logprob": -0.10107438492052483, "compression_ratio": 1.797427652733119, "no_speech_prob": 0.1293749213218689}, {"id": 112, "seek": 49568, "start": 511.12, "end": 515.36, "text": " have seen that one coming? No, but actually, it's not just transformers. In total, they try", "tokens": [51136, 362, 1612, 300, 472, 1348, 30, 883, 11, 457, 767, 11, 309, 311, 406, 445, 4088, 433, 13, 682, 3217, 11, 436, 853, 51348], "temperature": 0.0, "avg_logprob": -0.10107438492052483, "compression_ratio": 1.797427652733119, "no_speech_prob": 0.1293749213218689}, {"id": 113, "seek": 49568, "start": 515.36, "end": 520.08, "text": " three different architectures. The first of them is this transformer XL architecture, which is a", "tokens": [51348, 1045, 819, 6331, 1303, 13, 440, 700, 295, 552, 307, 341, 31782, 37210, 9482, 11, 597, 307, 257, 51584], "temperature": 0.0, "avg_logprob": -0.10107438492052483, "compression_ratio": 1.797427652733119, "no_speech_prob": 0.1293749213218689}, {"id": 114, "seek": 49568, "start": 520.08, "end": 524.96, "text": " transformer that kind of has a sort of memory. The way it works is that each hidden unit in", "tokens": [51584, 31782, 300, 733, 295, 575, 257, 1333, 295, 4675, 13, 440, 636, 309, 1985, 307, 300, 1184, 7633, 4985, 294, 51828], "temperature": 0.0, "avg_logprob": -0.10107438492052483, "compression_ratio": 1.797427652733119, "no_speech_prob": 0.1293749213218689}, {"id": 115, "seek": 52496, "start": 524.96, "end": 529.6800000000001, "text": " the transformer also gets input from the layer below it at the previous time step, which means", "tokens": [50364, 264, 31782, 611, 2170, 4846, 490, 264, 4583, 2507, 309, 412, 264, 3894, 565, 1823, 11, 597, 1355, 50600], "temperature": 0.0, "avg_logprob": -0.06591328107393706, "compression_ratio": 1.8585526315789473, "no_speech_prob": 0.005554524715989828}, {"id": 116, "seek": 52496, "start": 529.6800000000001, "end": 534.24, "text": " that information from previous time steps are going into the future, which means that there is", "tokens": [50600, 300, 1589, 490, 3894, 565, 4439, 366, 516, 666, 264, 2027, 11, 597, 1355, 300, 456, 307, 50828], "temperature": 0.0, "avg_logprob": -0.06591328107393706, "compression_ratio": 1.8585526315789473, "no_speech_prob": 0.005554524715989828}, {"id": 117, "seek": 52496, "start": 534.24, "end": 538.08, "text": " kind of some sort of memory. If I want to be more specific, what this really does is that it", "tokens": [50828, 733, 295, 512, 1333, 295, 4675, 13, 759, 286, 528, 281, 312, 544, 2685, 11, 437, 341, 534, 775, 307, 300, 309, 51020], "temperature": 0.0, "avg_logprob": -0.06591328107393706, "compression_ratio": 1.8585526315789473, "no_speech_prob": 0.005554524715989828}, {"id": 118, "seek": 52496, "start": 538.08, "end": 542.1600000000001, "text": " effectively extends the context length of the transformer. Though it's not written in this", "tokens": [51020, 8659, 26448, 264, 4319, 4641, 295, 264, 31782, 13, 10404, 309, 311, 406, 3720, 294, 341, 51224], "temperature": 0.0, "avg_logprob": -0.06591328107393706, "compression_ratio": 1.8585526315789473, "no_speech_prob": 0.005554524715989828}, {"id": 119, "seek": 52496, "start": 542.1600000000001, "end": 547.36, "text": " list, they also try a vanilla RNN, a GRU to be specific, is just a very vanilla memory based", "tokens": [51224, 1329, 11, 436, 611, 853, 257, 17528, 45702, 45, 11, 257, 10903, 52, 281, 312, 2685, 11, 307, 445, 257, 588, 17528, 4675, 2361, 51484], "temperature": 0.0, "avg_logprob": -0.06591328107393706, "compression_ratio": 1.8585526315789473, "no_speech_prob": 0.005554524715989828}, {"id": 120, "seek": 52496, "start": 547.36, "end": 551.2, "text": " model where you have some sort of memory, and you have some gates that decide what gets remembered", "tokens": [51484, 2316, 689, 291, 362, 512, 1333, 295, 4675, 11, 293, 291, 362, 512, 19792, 300, 4536, 437, 2170, 13745, 51676], "temperature": 0.0, "avg_logprob": -0.06591328107393706, "compression_ratio": 1.8585526315789473, "no_speech_prob": 0.005554524715989828}, {"id": 121, "seek": 55120, "start": 551.2, "end": 555.6, "text": " and what gets forgotten at each step. Then they also try one more type of RNN that is augmented", "tokens": [50364, 293, 437, 2170, 11832, 412, 1184, 1823, 13, 1396, 436, 611, 853, 472, 544, 2010, 295, 45702, 45, 300, 307, 36155, 50584], "temperature": 0.0, "avg_logprob": -0.06891925839612084, "compression_ratio": 1.7313432835820894, "no_speech_prob": 0.015905240550637245}, {"id": 122, "seek": 55120, "start": 555.6, "end": 561.2, "text": " with attention that attends over previous time step activations. So very similar to how a standard", "tokens": [50584, 365, 3202, 300, 49837, 670, 3894, 565, 1823, 2430, 763, 13, 407, 588, 2531, 281, 577, 257, 3832, 50864], "temperature": 0.0, "avg_logprob": -0.06891925839612084, "compression_ratio": 1.7313432835820894, "no_speech_prob": 0.015905240550637245}, {"id": 123, "seek": 55120, "start": 561.2, "end": 565.0400000000001, "text": " RNN works, but just with that extra little bit of attention thrown in there. We can take a look", "tokens": [50864, 45702, 45, 1985, 11, 457, 445, 365, 300, 2857, 707, 857, 295, 3202, 11732, 294, 456, 13, 492, 393, 747, 257, 574, 51056], "temperature": 0.0, "avg_logprob": -0.06891925839612084, "compression_ratio": 1.7313432835820894, "no_speech_prob": 0.015905240550637245}, {"id": 124, "seek": 55120, "start": 565.0400000000001, "end": 569.12, "text": " at how these methods compare to each other in this figure here, where the authors have graphed the", "tokens": [51056, 412, 577, 613, 7150, 6794, 281, 1184, 661, 294, 341, 2573, 510, 11, 689, 264, 16552, 362, 4295, 292, 264, 51260], "temperature": 0.0, "avg_logprob": -0.06891925839612084, "compression_ratio": 1.7313432835820894, "no_speech_prob": 0.015905240550637245}, {"id": 125, "seek": 55120, "start": 569.12, "end": 574.5600000000001, "text": " normalized score of these agents based on the final trials of episodes in the test tasks. Or in", "tokens": [51260, 48704, 6175, 295, 613, 12554, 2361, 322, 264, 2572, 12450, 295, 9313, 294, 264, 1500, 9608, 13, 1610, 294, 51532], "temperature": 0.0, "avg_logprob": -0.06891925839612084, "compression_ratio": 1.7313432835820894, "no_speech_prob": 0.015905240550637245}, {"id": 126, "seek": 55120, "start": 574.5600000000001, "end": 579.2, "text": " Lehmann's terms, big number is good number. What we can see here is that both of the attention", "tokens": [51532, 1456, 8587, 969, 311, 2115, 11, 955, 1230, 307, 665, 1230, 13, 708, 321, 393, 536, 510, 307, 300, 1293, 295, 264, 3202, 51764], "temperature": 0.0, "avg_logprob": -0.06891925839612084, "compression_ratio": 1.7313432835820894, "no_speech_prob": 0.015905240550637245}, {"id": 127, "seek": 57920, "start": 579.2, "end": 584.24, "text": " based models significantly outperform the vanilla RNN, despite the fact that they use about the same", "tokens": [50364, 2361, 5245, 10591, 484, 26765, 264, 17528, 45702, 45, 11, 7228, 264, 1186, 300, 436, 764, 466, 264, 912, 50616], "temperature": 0.0, "avg_logprob": -0.07209024121684413, "compression_ratio": 1.763496143958869, "no_speech_prob": 0.020330676808953285}, {"id": 128, "seek": 57920, "start": 584.24, "end": 588.1600000000001, "text": " number of parameters. Clearly, the attention is doing something to help out quite a bit here. And", "tokens": [50616, 1230, 295, 9834, 13, 24120, 11, 264, 3202, 307, 884, 746, 281, 854, 484, 1596, 257, 857, 510, 13, 400, 50812], "temperature": 0.0, "avg_logprob": -0.07209024121684413, "compression_ratio": 1.763496143958869, "no_speech_prob": 0.020330676808953285}, {"id": 129, "seek": 57920, "start": 588.1600000000001, "end": 592.5600000000001, "text": " due to these results, most of the experiments we'll look at a bit later, use the transformer XL", "tokens": [50812, 3462, 281, 613, 3542, 11, 881, 295, 264, 12050, 321, 603, 574, 412, 257, 857, 1780, 11, 764, 264, 31782, 37210, 51032], "temperature": 0.0, "avg_logprob": -0.07209024121684413, "compression_ratio": 1.763496143958869, "no_speech_prob": 0.020330676808953285}, {"id": 130, "seek": 57920, "start": 592.5600000000001, "end": 596.32, "text": " architecture. So we've covered the architecture here, but there's actually one more portion of", "tokens": [51032, 9482, 13, 407, 321, 600, 5343, 264, 9482, 510, 11, 457, 456, 311, 767, 472, 544, 8044, 295, 51220], "temperature": 0.0, "avg_logprob": -0.07209024121684413, "compression_ratio": 1.763496143958869, "no_speech_prob": 0.020330676808953285}, {"id": 131, "seek": 57920, "start": 596.32, "end": 600.4000000000001, "text": " the paper I should talk about in this architecture segment. And that is this distillation update.", "tokens": [51220, 264, 3035, 286, 820, 751, 466, 294, 341, 9482, 9469, 13, 400, 300, 307, 341, 42923, 399, 5623, 13, 51424], "temperature": 0.0, "avg_logprob": -0.07209024121684413, "compression_ratio": 1.763496143958869, "no_speech_prob": 0.020330676808953285}, {"id": 132, "seek": 57920, "start": 600.4000000000001, "end": 604.32, "text": " If you remember what I showed you in the beginning about how this large model was not able to learn", "tokens": [51424, 759, 291, 1604, 437, 286, 4712, 291, 294, 264, 2863, 466, 577, 341, 2416, 2316, 390, 406, 1075, 281, 1466, 51620], "temperature": 0.0, "avg_logprob": -0.07209024121684413, "compression_ratio": 1.763496143958869, "no_speech_prob": 0.020330676808953285}, {"id": 133, "seek": 57920, "start": 604.32, "end": 608.5600000000001, "text": " from this setup, you may be wondering how they end up getting that to work. And the answer here is", "tokens": [51620, 490, 341, 8657, 11, 291, 815, 312, 6359, 577, 436, 917, 493, 1242, 300, 281, 589, 13, 400, 264, 1867, 510, 307, 51832], "temperature": 0.0, "avg_logprob": -0.07209024121684413, "compression_ratio": 1.763496143958869, "no_speech_prob": 0.020330676808953285}, {"id": 134, "seek": 60856, "start": 608.56, "end": 613.28, "text": " kind of interesting. Nothing changes about the architecture, the RL training or any of that,", "tokens": [50364, 733, 295, 1880, 13, 6693, 2962, 466, 264, 9482, 11, 264, 497, 43, 3097, 420, 604, 295, 300, 11, 50600], "temperature": 0.0, "avg_logprob": -0.07215921543846446, "compression_ratio": 1.8859060402684564, "no_speech_prob": 0.005384585354477167}, {"id": 135, "seek": 60856, "start": 613.28, "end": 617.68, "text": " but rather they add two steps to the start of training to essentially kickstart the large", "tokens": [50600, 457, 2831, 436, 909, 732, 4439, 281, 264, 722, 295, 3097, 281, 4476, 4437, 24419, 264, 2416, 50820], "temperature": 0.0, "avg_logprob": -0.07215921543846446, "compression_ratio": 1.8859060402684564, "no_speech_prob": 0.005384585354477167}, {"id": 136, "seek": 60856, "start": 617.68, "end": 622.4799999999999, "text": " model's learning progress. And that step is where this idea of distillation comes into the picture.", "tokens": [50820, 2316, 311, 2539, 4205, 13, 400, 300, 1823, 307, 689, 341, 1558, 295, 42923, 399, 1487, 666, 264, 3036, 13, 51060], "temperature": 0.0, "avg_logprob": -0.07215921543846446, "compression_ratio": 1.8859060402684564, "no_speech_prob": 0.005384585354477167}, {"id": 137, "seek": 60856, "start": 622.4799999999999, "end": 626.8, "text": " First, they train a smaller model called the teacher model, using the whole reinforcement", "tokens": [51060, 2386, 11, 436, 3847, 257, 4356, 2316, 1219, 264, 5027, 2316, 11, 1228, 264, 1379, 29280, 51276], "temperature": 0.0, "avg_logprob": -0.07215921543846446, "compression_ratio": 1.8859060402684564, "no_speech_prob": 0.005384585354477167}, {"id": 138, "seek": 60856, "start": 626.8, "end": 630.8, "text": " learning training process we were just talking about. Once the smaller model has trained for", "tokens": [51276, 2539, 3097, 1399, 321, 645, 445, 1417, 466, 13, 3443, 264, 4356, 2316, 575, 8895, 337, 51476], "temperature": 0.0, "avg_logprob": -0.07215921543846446, "compression_ratio": 1.8859060402684564, "no_speech_prob": 0.005384585354477167}, {"id": 139, "seek": 60856, "start": 630.8, "end": 635.1999999999999, "text": " several billion steps, then they create the larger model. And when they create the larger model,", "tokens": [51476, 2940, 5218, 4439, 11, 550, 436, 1884, 264, 4833, 2316, 13, 400, 562, 436, 1884, 264, 4833, 2316, 11, 51696], "temperature": 0.0, "avg_logprob": -0.07215921543846446, "compression_ratio": 1.8859060402684564, "no_speech_prob": 0.005384585354477167}, {"id": 140, "seek": 63520, "start": 635.2, "end": 638.8000000000001, "text": " they don't just start training it with reinforcement learning from scratch, but rather", "tokens": [50364, 436, 500, 380, 445, 722, 3097, 309, 365, 29280, 2539, 490, 8459, 11, 457, 2831, 50544], "temperature": 0.0, "avg_logprob": -0.055836920971637005, "compression_ratio": 1.971875, "no_speech_prob": 0.008315347135066986}, {"id": 141, "seek": 63520, "start": 638.8000000000001, "end": 642.88, "text": " for the first four billion steps of training, they use an additional distillation loss. And what", "tokens": [50544, 337, 264, 700, 1451, 5218, 4439, 295, 3097, 11, 436, 764, 364, 4497, 42923, 399, 4470, 13, 400, 437, 50748], "temperature": 0.0, "avg_logprob": -0.055836920971637005, "compression_ratio": 1.971875, "no_speech_prob": 0.008315347135066986}, {"id": 142, "seek": 63520, "start": 642.88, "end": 647.2, "text": " this distillation loss does is essentially distills the teacher or the smaller model", "tokens": [50748, 341, 42923, 399, 4470, 775, 307, 4476, 1483, 2565, 264, 5027, 420, 264, 4356, 2316, 50964], "temperature": 0.0, "avg_logprob": -0.055836920971637005, "compression_ratio": 1.971875, "no_speech_prob": 0.008315347135066986}, {"id": 143, "seek": 63520, "start": 647.2, "end": 651.84, "text": " into the big model. Or in other words, they try to have the larger model imitate the smaller", "tokens": [50964, 666, 264, 955, 2316, 13, 1610, 294, 661, 2283, 11, 436, 853, 281, 362, 264, 4833, 2316, 35556, 264, 4356, 51196], "temperature": 0.0, "avg_logprob": -0.055836920971637005, "compression_ratio": 1.971875, "no_speech_prob": 0.008315347135066986}, {"id": 144, "seek": 63520, "start": 651.84, "end": 655.76, "text": " teacher model. The idea here is that because it's much easier to train a smaller model,", "tokens": [51196, 5027, 2316, 13, 440, 1558, 510, 307, 300, 570, 309, 311, 709, 3571, 281, 3847, 257, 4356, 2316, 11, 51392], "temperature": 0.0, "avg_logprob": -0.055836920971637005, "compression_ratio": 1.971875, "no_speech_prob": 0.008315347135066986}, {"id": 145, "seek": 63520, "start": 655.76, "end": 659.6800000000001, "text": " even though it might not be as good, you can start by training a small model. Then by having", "tokens": [51392, 754, 1673, 309, 1062, 406, 312, 382, 665, 11, 291, 393, 722, 538, 3097, 257, 1359, 2316, 13, 1396, 538, 1419, 51588], "temperature": 0.0, "avg_logprob": -0.055836920971637005, "compression_ratio": 1.971875, "no_speech_prob": 0.008315347135066986}, {"id": 146, "seek": 63520, "start": 659.6800000000001, "end": 663.6800000000001, "text": " the larger model imitate the small model, it gets a lot more signal that doesn't require", "tokens": [51588, 264, 4833, 2316, 35556, 264, 1359, 2316, 11, 309, 2170, 257, 688, 544, 6358, 300, 1177, 380, 3651, 51788], "temperature": 0.0, "avg_logprob": -0.055836920971637005, "compression_ratio": 1.971875, "no_speech_prob": 0.008315347135066986}, {"id": 147, "seek": 66368, "start": 663.68, "end": 668.3199999999999, "text": " interacting with the environment some unreasonable amount of times. The idea is once they've done", "tokens": [50364, 18017, 365, 264, 2823, 512, 41730, 2372, 295, 1413, 13, 440, 1558, 307, 1564, 436, 600, 1096, 50596], "temperature": 0.0, "avg_logprob": -0.06084303679289641, "compression_ratio": 1.738872403560831, "no_speech_prob": 0.0018101517343893647}, {"id": 148, "seek": 66368, "start": 668.3199999999999, "end": 672.64, "text": " this for the first four billion steps, the larger model will have already kickstarted its learning", "tokens": [50596, 341, 337, 264, 700, 1451, 5218, 4439, 11, 264, 4833, 2316, 486, 362, 1217, 4437, 24419, 292, 1080, 2539, 50812], "temperature": 0.0, "avg_logprob": -0.06084303679289641, "compression_ratio": 1.738872403560831, "no_speech_prob": 0.0018101517343893647}, {"id": 149, "seek": 66368, "start": 672.64, "end": 676.8, "text": " process. So then it will be much easier to learn on its own and it won't just stagnate without being", "tokens": [50812, 1399, 13, 407, 550, 309, 486, 312, 709, 3571, 281, 1466, 322, 1080, 1065, 293, 309, 1582, 380, 445, 32853, 473, 1553, 885, 51020], "temperature": 0.0, "avg_logprob": -0.06084303679289641, "compression_ratio": 1.738872403560831, "no_speech_prob": 0.0018101517343893647}, {"id": 150, "seek": 66368, "start": 676.8, "end": 681.28, "text": " able to learn anything at all. And now looking at this figure again, it should make a lot more sense", "tokens": [51020, 1075, 281, 1466, 1340, 412, 439, 13, 400, 586, 1237, 412, 341, 2573, 797, 11, 309, 820, 652, 257, 688, 544, 2020, 51244], "temperature": 0.0, "avg_logprob": -0.06084303679289641, "compression_ratio": 1.738872403560831, "no_speech_prob": 0.0018101517343893647}, {"id": 151, "seek": 66368, "start": 681.28, "end": 686.0, "text": " when we train the large model from scratch, it performs pretty horribly. But when we add a", "tokens": [51244, 562, 321, 3847, 264, 2416, 2316, 490, 8459, 11, 309, 26213, 1238, 45028, 13, 583, 562, 321, 909, 257, 51480], "temperature": 0.0, "avg_logprob": -0.06084303679289641, "compression_ratio": 1.738872403560831, "no_speech_prob": 0.0018101517343893647}, {"id": 152, "seek": 66368, "start": 686.0, "end": 690.64, "text": " distillation loss, it outperforms everything else. And this is the case for both tasks where the", "tokens": [51480, 42923, 399, 4470, 11, 309, 484, 26765, 82, 1203, 1646, 13, 400, 341, 307, 264, 1389, 337, 1293, 9608, 689, 264, 51712], "temperature": 0.0, "avg_logprob": -0.06084303679289641, "compression_ratio": 1.738872403560831, "no_speech_prob": 0.0018101517343893647}, {"id": 153, "seek": 69064, "start": 690.64, "end": 695.68, "text": " agent is achieving a median level score, and tasks where the agent is only performing a 20th", "tokens": [50364, 9461, 307, 19626, 257, 26779, 1496, 6175, 11, 293, 9608, 689, 264, 9461, 307, 787, 10205, 257, 945, 392, 50616], "temperature": 0.0, "avg_logprob": -0.061392282184801604, "compression_ratio": 1.795031055900621, "no_speech_prob": 0.004609365016222}, {"id": 154, "seek": 69064, "start": 695.68, "end": 700.16, "text": " percentile score. In other words, whether the agent is good or bad at this task, this approach", "tokens": [50616, 3043, 794, 6175, 13, 682, 661, 2283, 11, 1968, 264, 9461, 307, 665, 420, 1578, 412, 341, 5633, 11, 341, 3109, 50840], "temperature": 0.0, "avg_logprob": -0.061392282184801604, "compression_ratio": 1.795031055900621, "no_speech_prob": 0.004609365016222}, {"id": 155, "seek": 69064, "start": 700.16, "end": 704.72, "text": " works either way. And that explains this distillation loss. Now it's actually kind of funny, because", "tokens": [50840, 1985, 2139, 636, 13, 400, 300, 13948, 341, 42923, 399, 4470, 13, 823, 309, 311, 767, 733, 295, 4074, 11, 570, 51068], "temperature": 0.0, "avg_logprob": -0.061392282184801604, "compression_ratio": 1.795031055900621, "no_speech_prob": 0.004609365016222}, {"id": 156, "seek": 69064, "start": 704.72, "end": 709.28, "text": " most of the time when you see someone doing model distillation, it's being used to distill a large", "tokens": [51068, 881, 295, 264, 565, 562, 291, 536, 1580, 884, 2316, 42923, 399, 11, 309, 311, 885, 1143, 281, 42923, 257, 2416, 51296], "temperature": 0.0, "avg_logprob": -0.061392282184801604, "compression_ratio": 1.795031055900621, "no_speech_prob": 0.004609365016222}, {"id": 157, "seek": 69064, "start": 709.28, "end": 713.1999999999999, "text": " model into a smaller version of that model, so that you can be more efficient at inference time.", "tokens": [51296, 2316, 666, 257, 4356, 3037, 295, 300, 2316, 11, 370, 300, 291, 393, 312, 544, 7148, 412, 38253, 565, 13, 51492], "temperature": 0.0, "avg_logprob": -0.061392282184801604, "compression_ratio": 1.795031055900621, "no_speech_prob": 0.004609365016222}, {"id": 158, "seek": 69064, "start": 713.1999999999999, "end": 716.8, "text": " But here they're actually doing the opposite in a successful attempt to train the large model", "tokens": [51492, 583, 510, 436, 434, 767, 884, 264, 6182, 294, 257, 4406, 5217, 281, 3847, 264, 2416, 2316, 51672], "temperature": 0.0, "avg_logprob": -0.061392282184801604, "compression_ratio": 1.795031055900621, "no_speech_prob": 0.004609365016222}, {"id": 159, "seek": 71680, "start": 716.8, "end": 720.4, "text": " faster, which isn't something I thought about before, but it certainly is an interesting", "tokens": [50364, 4663, 11, 597, 1943, 380, 746, 286, 1194, 466, 949, 11, 457, 309, 3297, 307, 364, 1880, 50544], "temperature": 0.0, "avg_logprob": -0.0791404834692029, "compression_ratio": 1.6608695652173913, "no_speech_prob": 0.058337073773145676}, {"id": 160, "seek": 71680, "start": 720.4, "end": 725.52, "text": " use of model distillation. So now we know how to set up our tasks as a meta RL problem, we know", "tokens": [50544, 764, 295, 2316, 42923, 399, 13, 407, 586, 321, 458, 577, 281, 992, 493, 527, 9608, 382, 257, 19616, 497, 43, 1154, 11, 321, 458, 50800], "temperature": 0.0, "avg_logprob": -0.0791404834692029, "compression_ratio": 1.6608695652173913, "no_speech_prob": 0.058337073773145676}, {"id": 161, "seek": 71680, "start": 725.52, "end": 729.3599999999999, "text": " what algorithm we're going to use for training, and we know how to get that to work with a large", "tokens": [50800, 437, 9284, 321, 434, 516, 281, 764, 337, 3097, 11, 293, 321, 458, 577, 281, 483, 300, 281, 589, 365, 257, 2416, 50992], "temperature": 0.0, "avg_logprob": -0.0791404834692029, "compression_ratio": 1.6608695652173913, "no_speech_prob": 0.058337073773145676}, {"id": 162, "seek": 71680, "start": 729.3599999999999, "end": 733.68, "text": " attention based model. But the point here is to build a foundation model that can explore and", "tokens": [50992, 3202, 2361, 2316, 13, 583, 264, 935, 510, 307, 281, 1322, 257, 7030, 2316, 300, 393, 6839, 293, 51208], "temperature": 0.0, "avg_logprob": -0.0791404834692029, "compression_ratio": 1.6608695652173913, "no_speech_prob": 0.058337073773145676}, {"id": 163, "seek": 71680, "start": 733.68, "end": 738.0, "text": " solve tasks efficiently, especially in environments where reward is somewhat sparse. And this is", "tokens": [51208, 5039, 9608, 19621, 11, 2318, 294, 12388, 689, 7782, 307, 8344, 637, 11668, 13, 400, 341, 307, 51424], "temperature": 0.0, "avg_logprob": -0.0791404834692029, "compression_ratio": 1.6608695652173913, "no_speech_prob": 0.058337073773145676}, {"id": 164, "seek": 71680, "start": 738.0, "end": 743.52, "text": " the case in XLAN 2. The issue is that if you throw a bunch of hard exploration tasks at a new agent,", "tokens": [51424, 264, 1389, 294, 1783, 36527, 568, 13, 440, 2734, 307, 300, 498, 291, 3507, 257, 3840, 295, 1152, 16197, 9608, 412, 257, 777, 9461, 11, 51700], "temperature": 0.0, "avg_logprob": -0.0791404834692029, "compression_ratio": 1.6608695652173913, "no_speech_prob": 0.058337073773145676}, {"id": 165, "seek": 74352, "start": 743.6, "end": 747.92, "text": " it's just going to bang its head into the wall over and over and over. And it's not really going", "tokens": [50368, 309, 311, 445, 516, 281, 8550, 1080, 1378, 666, 264, 2929, 670, 293, 670, 293, 670, 13, 400, 309, 311, 406, 534, 516, 50584], "temperature": 0.0, "avg_logprob": -0.06851480272081163, "compression_ratio": 1.859016393442623, "no_speech_prob": 0.006487986072897911}, {"id": 166, "seek": 74352, "start": 747.92, "end": 751.92, "text": " to learn anything, because it's never going to get that first reward or enough reward in the", "tokens": [50584, 281, 1466, 1340, 11, 570, 309, 311, 1128, 516, 281, 483, 300, 700, 7782, 420, 1547, 7782, 294, 264, 50784], "temperature": 0.0, "avg_logprob": -0.06851480272081163, "compression_ratio": 1.859016393442623, "no_speech_prob": 0.006487986072897911}, {"id": 167, "seek": 74352, "start": 751.92, "end": 756.56, "text": " beginning that gives an incentive or enough signal to learn from our 10 to the 40 tasks,", "tokens": [50784, 2863, 300, 2709, 364, 22346, 420, 1547, 6358, 281, 1466, 490, 527, 1266, 281, 264, 3356, 9608, 11, 51016], "temperature": 0.0, "avg_logprob": -0.06851480272081163, "compression_ratio": 1.859016393442623, "no_speech_prob": 0.006487986072897911}, {"id": 168, "seek": 74352, "start": 756.56, "end": 760.8, "text": " we need to be able to choose ones that are not too hard, yet also not too easy for the agent at", "tokens": [51016, 321, 643, 281, 312, 1075, 281, 2826, 2306, 300, 366, 406, 886, 1152, 11, 1939, 611, 406, 886, 1858, 337, 264, 9461, 412, 51228], "temperature": 0.0, "avg_logprob": -0.06851480272081163, "compression_ratio": 1.859016393442623, "no_speech_prob": 0.006487986072897911}, {"id": 169, "seek": 74352, "start": 760.8, "end": 764.96, "text": " any given time. And that is where step three comes into play, auto curriculum learning, which we", "tokens": [51228, 604, 2212, 565, 13, 400, 300, 307, 689, 1823, 1045, 1487, 666, 862, 11, 8399, 14302, 2539, 11, 597, 321, 51436], "temperature": 0.0, "avg_logprob": -0.06851480272081163, "compression_ratio": 1.859016393442623, "no_speech_prob": 0.006487986072897911}, {"id": 170, "seek": 74352, "start": 764.96, "end": 769.36, "text": " can see happening as these top steps right here. This brings us to the auto curriculum learning", "tokens": [51436, 393, 536, 2737, 382, 613, 1192, 4439, 558, 510, 13, 639, 5607, 505, 281, 264, 8399, 14302, 2539, 51656], "temperature": 0.0, "avg_logprob": -0.06851480272081163, "compression_ratio": 1.859016393442623, "no_speech_prob": 0.006487986072897911}, {"id": 171, "seek": 76936, "start": 769.36, "end": 774.08, "text": " section, unsurprisingly, where we want to build some sort of curriculum of tasks for the agent", "tokens": [50364, 3541, 11, 2693, 374, 34408, 11, 689, 321, 528, 281, 1322, 512, 1333, 295, 14302, 295, 9608, 337, 264, 9461, 50600], "temperature": 0.0, "avg_logprob": -0.08741997991289412, "compression_ratio": 1.7654320987654322, "no_speech_prob": 0.03114151768386364}, {"id": 172, "seek": 76936, "start": 774.08, "end": 778.32, "text": " based off its current skill level. And to do this, the authors try two different approaches.", "tokens": [50600, 2361, 766, 1080, 2190, 5389, 1496, 13, 400, 281, 360, 341, 11, 264, 16552, 853, 732, 819, 11587, 13, 50812], "temperature": 0.0, "avg_logprob": -0.08741997991289412, "compression_ratio": 1.7654320987654322, "no_speech_prob": 0.03114151768386364}, {"id": 173, "seek": 76936, "start": 778.32, "end": 783.12, "text": " Approach number one is no op filtering. The idea here is that you start by sampling a new task,", "tokens": [50812, 29551, 608, 1230, 472, 307, 572, 999, 30822, 13, 440, 1558, 510, 307, 300, 291, 722, 538, 21179, 257, 777, 5633, 11, 51052], "temperature": 0.0, "avg_logprob": -0.08741997991289412, "compression_ratio": 1.7654320987654322, "no_speech_prob": 0.03114151768386364}, {"id": 174, "seek": 76936, "start": 783.12, "end": 788.4, "text": " and you let both the ADA agent and the no op agent attempt the task. The no op agent takes no", "tokens": [51052, 293, 291, 718, 1293, 264, 39354, 9461, 293, 264, 572, 999, 9461, 5217, 264, 5633, 13, 440, 572, 999, 9461, 2516, 572, 51316], "temperature": 0.0, "avg_logprob": -0.08741997991289412, "compression_ratio": 1.7654320987654322, "no_speech_prob": 0.03114151768386364}, {"id": 175, "seek": 76936, "start": 788.4, "end": 792.96, "text": " actions, hence the name no op, so it can be used as a baseline for comparison. Given the outcome,", "tokens": [51316, 5909, 11, 16678, 264, 1315, 572, 999, 11, 370, 309, 393, 312, 1143, 382, 257, 20518, 337, 9660, 13, 18600, 264, 9700, 11, 51544], "temperature": 0.0, "avg_logprob": -0.08741997991289412, "compression_ratio": 1.7654320987654322, "no_speech_prob": 0.03114151768386364}, {"id": 176, "seek": 76936, "start": 792.96, "end": 797.28, "text": " some heuristics are used to determine whether or not this task is at the right difficulty level.", "tokens": [51544, 512, 415, 374, 6006, 366, 1143, 281, 6997, 1968, 420, 406, 341, 5633, 307, 412, 264, 558, 10360, 1496, 13, 51760], "temperature": 0.0, "avg_logprob": -0.08741997991289412, "compression_ratio": 1.7654320987654322, "no_speech_prob": 0.03114151768386364}, {"id": 177, "seek": 79728, "start": 797.28, "end": 801.92, "text": " One example of a heuristic they use is that the agent should be able to at least get some reward,", "tokens": [50364, 1485, 1365, 295, 257, 415, 374, 3142, 436, 764, 307, 300, 264, 9461, 820, 312, 1075, 281, 412, 1935, 483, 512, 7782, 11, 50596], "temperature": 0.0, "avg_logprob": -0.083788043768831, "compression_ratio": 1.7419354838709677, "no_speech_prob": 0.0004442096396815032}, {"id": 178, "seek": 79728, "start": 801.92, "end": 806.0799999999999, "text": " but it shouldn't be too good at the task either. And another criteria, for example,", "tokens": [50596, 457, 309, 4659, 380, 312, 886, 665, 412, 264, 5633, 2139, 13, 400, 1071, 11101, 11, 337, 1365, 11, 50804], "temperature": 0.0, "avg_logprob": -0.083788043768831, "compression_ratio": 1.7419354838709677, "no_speech_prob": 0.0004442096396815032}, {"id": 179, "seek": 79728, "start": 806.0799999999999, "end": 810.4, "text": " is that the agent has to achieve a score that's sufficiently different from the no op agent.", "tokens": [50804, 307, 300, 264, 9461, 575, 281, 4584, 257, 6175, 300, 311, 31868, 819, 490, 264, 572, 999, 9461, 13, 51020], "temperature": 0.0, "avg_logprob": -0.083788043768831, "compression_ratio": 1.7419354838709677, "no_speech_prob": 0.0004442096396815032}, {"id": 180, "seek": 79728, "start": 810.4, "end": 814.48, "text": " And if all these defined heuristics are passed, then the task is used for training. The other", "tokens": [51020, 400, 498, 439, 613, 7642, 415, 374, 6006, 366, 4678, 11, 550, 264, 5633, 307, 1143, 337, 3097, 13, 440, 661, 51224], "temperature": 0.0, "avg_logprob": -0.083788043768831, "compression_ratio": 1.7419354838709677, "no_speech_prob": 0.0004442096396815032}, {"id": 181, "seek": 79728, "start": 814.48, "end": 819.52, "text": " method used for generating curriculum they try is prioritized level replay or PLR for short.", "tokens": [51224, 3170, 1143, 337, 17746, 14302, 436, 853, 307, 14846, 1602, 1496, 23836, 420, 6999, 49, 337, 2099, 13, 51476], "temperature": 0.0, "avg_logprob": -0.083788043768831, "compression_ratio": 1.7419354838709677, "no_speech_prob": 0.0004442096396815032}, {"id": 182, "seek": 79728, "start": 819.52, "end": 823.28, "text": " Instead of essentially using a series of if statements like a no op filtering,", "tokens": [51476, 7156, 295, 4476, 1228, 257, 2638, 295, 498, 12363, 411, 257, 572, 999, 30822, 11, 51664], "temperature": 0.0, "avg_logprob": -0.083788043768831, "compression_ratio": 1.7419354838709677, "no_speech_prob": 0.0004442096396815032}, {"id": 183, "seek": 82328, "start": 823.28, "end": 827.8399999999999, "text": " PLR runs ADA through candidate tasks and estimates the agent's regret. And regret is", "tokens": [50364, 6999, 49, 6676, 39354, 807, 11532, 9608, 293, 20561, 264, 9461, 311, 10879, 13, 400, 10879, 307, 50592], "temperature": 0.0, "avg_logprob": -0.08381105604625884, "compression_ratio": 1.7468354430379747, "no_speech_prob": 0.001754578435793519}, {"id": 184, "seek": 82328, "start": 827.8399999999999, "end": 832.64, "text": " essentially how much potential reward the agent failed to attain as estimated by its TD air.", "tokens": [50592, 4476, 577, 709, 3995, 7782, 264, 9461, 7612, 281, 23766, 382, 14109, 538, 1080, 42606, 1988, 13, 50832], "temperature": 0.0, "avg_logprob": -0.08381105604625884, "compression_ratio": 1.7468354430379747, "no_speech_prob": 0.001754578435793519}, {"id": 185, "seek": 82328, "start": 832.64, "end": 837.28, "text": " A fitness score is calculated from the grid, higher regret being better because that means the", "tokens": [50832, 316, 15303, 6175, 307, 15598, 490, 264, 10748, 11, 2946, 10879, 885, 1101, 570, 300, 1355, 264, 51064], "temperature": 0.0, "avg_logprob": -0.08381105604625884, "compression_ratio": 1.7468354430379747, "no_speech_prob": 0.001754578435793519}, {"id": 186, "seek": 82328, "start": 837.28, "end": 841.1999999999999, "text": " agent knows that it has more room to improve. And all the tasks with the highest fitness scores", "tokens": [51064, 9461, 3255, 300, 309, 575, 544, 1808, 281, 3470, 13, 400, 439, 264, 9608, 365, 264, 6343, 15303, 13444, 51260], "temperature": 0.0, "avg_logprob": -0.08381105604625884, "compression_ratio": 1.7468354430379747, "no_speech_prob": 0.001754578435793519}, {"id": 187, "seek": 82328, "start": 841.1999999999999, "end": 845.6, "text": " are kept in a buffer that is sampled from during training. In ablations, we can see that both of", "tokens": [51260, 366, 4305, 294, 257, 21762, 300, 307, 3247, 15551, 490, 1830, 3097, 13, 682, 410, 75, 763, 11, 321, 393, 536, 300, 1293, 295, 51480], "temperature": 0.0, "avg_logprob": -0.08381105604625884, "compression_ratio": 1.7468354430379747, "no_speech_prob": 0.001754578435793519}, {"id": 188, "seek": 82328, "start": 845.6, "end": 850.24, "text": " these methods, no op filtering and PLR were quite a bit better than uniform filtering,", "tokens": [51480, 613, 7150, 11, 572, 999, 30822, 293, 6999, 49, 645, 1596, 257, 857, 1101, 813, 9452, 30822, 11, 51712], "temperature": 0.0, "avg_logprob": -0.08381105604625884, "compression_ratio": 1.7468354430379747, "no_speech_prob": 0.001754578435793519}, {"id": 189, "seek": 85024, "start": 850.24, "end": 855.6800000000001, "text": " which is just random sampling of tasks. However, overall, PLR does perform a little bit better", "tokens": [50364, 597, 307, 445, 4974, 21179, 295, 9608, 13, 2908, 11, 4787, 11, 6999, 49, 775, 2042, 257, 707, 857, 1101, 50636], "temperature": 0.0, "avg_logprob": -0.07933834104826956, "compression_ratio": 1.6971608832807572, "no_speech_prob": 0.05339353531599045}, {"id": 190, "seek": 85024, "start": 855.6800000000001, "end": 860.24, "text": " looking at how PLR and no op filtering changed the task difficulty over time.", "tokens": [50636, 1237, 412, 577, 6999, 49, 293, 572, 999, 30822, 3105, 264, 5633, 10360, 670, 565, 13, 50864], "temperature": 0.0, "avg_logprob": -0.07933834104826956, "compression_ratio": 1.6971608832807572, "no_speech_prob": 0.05339353531599045}, {"id": 191, "seek": 85024, "start": 860.24, "end": 865.2, "text": " In the examples they give here, we can see PLR starts out with less rules, more trials,", "tokens": [50864, 682, 264, 5110, 436, 976, 510, 11, 321, 393, 536, 6999, 49, 3719, 484, 365, 1570, 4474, 11, 544, 12450, 11, 51112], "temperature": 0.0, "avg_logprob": -0.07933834104826956, "compression_ratio": 1.6971608832807572, "no_speech_prob": 0.05339353531599045}, {"id": 192, "seek": 85024, "start": 865.2, "end": 869.6800000000001, "text": " gives less dead end rules overall. And it also doesn't hide as many of the critical rules.", "tokens": [51112, 2709, 1570, 3116, 917, 4474, 4787, 13, 400, 309, 611, 1177, 380, 6479, 382, 867, 295, 264, 4924, 4474, 13, 51336], "temperature": 0.0, "avg_logprob": -0.07933834104826956, "compression_ratio": 1.6971608832807572, "no_speech_prob": 0.05339353531599045}, {"id": 193, "seek": 85024, "start": 869.6800000000001, "end": 873.84, "text": " And because of these results and PLR's overall high performance, it's used as the curriculum", "tokens": [51336, 400, 570, 295, 613, 3542, 293, 6999, 49, 311, 4787, 1090, 3389, 11, 309, 311, 1143, 382, 264, 14302, 51544], "temperature": 0.0, "avg_logprob": -0.07933834104826956, "compression_ratio": 1.6971608832807572, "no_speech_prob": 0.05339353531599045}, {"id": 194, "seek": 85024, "start": 873.84, "end": 878.32, "text": " algorithm of choice for the experiments that we'll be looking at in a bit. So that was a lot,", "tokens": [51544, 9284, 295, 3922, 337, 264, 12050, 300, 321, 603, 312, 1237, 412, 294, 257, 857, 13, 407, 300, 390, 257, 688, 11, 51768], "temperature": 0.0, "avg_logprob": -0.07933834104826956, "compression_ratio": 1.6971608832807572, "no_speech_prob": 0.05339353531599045}, {"id": 195, "seek": 87832, "start": 878.32, "end": 881.84, "text": " but we've finally gotten through all the components of how it worked. So now we should be", "tokens": [50364, 457, 321, 600, 2721, 5768, 807, 439, 264, 6677, 295, 577, 309, 2732, 13, 407, 586, 321, 820, 312, 50540], "temperature": 0.0, "avg_logprob": -0.10202125402597281, "compression_ratio": 1.6511627906976745, "no_speech_prob": 0.006097058765590191}, {"id": 196, "seek": 87832, "start": 881.84, "end": 887.2800000000001, "text": " able to understand this full diagram. We start with a massive task pool from XLAN2. We randomly", "tokens": [50540, 1075, 281, 1223, 341, 1577, 10686, 13, 492, 722, 365, 257, 5994, 5633, 7005, 490, 1783, 36527, 17, 13, 492, 16979, 50812], "temperature": 0.0, "avg_logprob": -0.10202125402597281, "compression_ratio": 1.6511627906976745, "no_speech_prob": 0.006097058765590191}, {"id": 197, "seek": 87832, "start": 887.2800000000001, "end": 892.0, "text": " sample a bunch of those environments. We use PLR to check which ones have the highest fitness,", "tokens": [50812, 6889, 257, 3840, 295, 729, 12388, 13, 492, 764, 6999, 49, 281, 1520, 597, 2306, 362, 264, 6343, 15303, 11, 51048], "temperature": 0.0, "avg_logprob": -0.10202125402597281, "compression_ratio": 1.6511627906976745, "no_speech_prob": 0.006097058765590191}, {"id": 198, "seek": 87832, "start": 892.0, "end": 895.84, "text": " throw those into a training set that can be sampled from at any point during training. From", "tokens": [51048, 3507, 729, 666, 257, 3097, 992, 300, 393, 312, 3247, 15551, 490, 412, 604, 935, 1830, 3097, 13, 3358, 51240], "temperature": 0.0, "avg_logprob": -0.10202125402597281, "compression_ratio": 1.6511627906976745, "no_speech_prob": 0.006097058765590191}, {"id": 199, "seek": 87832, "start": 895.84, "end": 901.2, "text": " there, these get fed into the actual ADA agent, which uses in the beginning distillation updates,", "tokens": [51240, 456, 11, 613, 483, 4636, 666, 264, 3539, 39354, 9461, 11, 597, 4960, 294, 264, 2863, 42923, 399, 9205, 11, 51508], "temperature": 0.0, "avg_logprob": -0.10202125402597281, "compression_ratio": 1.6511627906976745, "no_speech_prob": 0.006097058765590191}, {"id": 200, "seek": 87832, "start": 901.2, "end": 906.24, "text": " and throughout the entire process, also RL updates to update this transformer based architecture.", "tokens": [51508, 293, 3710, 264, 2302, 1399, 11, 611, 497, 43, 9205, 281, 5623, 341, 31782, 2361, 9482, 13, 51760], "temperature": 0.0, "avg_logprob": -0.10202125402597281, "compression_ratio": 1.6511627906976745, "no_speech_prob": 0.006097058765590191}, {"id": 201, "seek": 90624, "start": 906.24, "end": 910.96, "text": " And because it has memory combined with this unique trial in episode format, we're able to", "tokens": [50364, 400, 570, 309, 575, 4675, 9354, 365, 341, 3845, 7308, 294, 3500, 7877, 11, 321, 434, 1075, 281, 50600], "temperature": 0.0, "avg_logprob": -0.08271247148513794, "compression_ratio": 1.8, "no_speech_prob": 0.007121008820831776}, {"id": 202, "seek": 90624, "start": 910.96, "end": 916.32, "text": " meta-learn how to adapt on the fly. And now that we understand, or at least I hope you all understand,", "tokens": [50600, 19616, 12, 306, 1083, 577, 281, 6231, 322, 264, 3603, 13, 400, 586, 300, 321, 1223, 11, 420, 412, 1935, 286, 1454, 291, 439, 1223, 11, 50868], "temperature": 0.0, "avg_logprob": -0.08271247148513794, "compression_ratio": 1.8, "no_speech_prob": 0.007121008820831776}, {"id": 203, "seek": 90624, "start": 916.32, "end": 921.52, "text": " that works, we can finally dive in to the results. So let's get started with these results that give", "tokens": [50868, 300, 1985, 11, 321, 393, 2721, 9192, 294, 281, 264, 3542, 13, 407, 718, 311, 483, 1409, 365, 613, 3542, 300, 976, 51128], "temperature": 0.0, "avg_logprob": -0.08271247148513794, "compression_ratio": 1.8, "no_speech_prob": 0.007121008820831776}, {"id": 204, "seek": 90624, "start": 921.52, "end": 926.08, "text": " us sort of an overview of how the fully trained agent is performing. The score you see here on", "tokens": [51128, 505, 1333, 295, 364, 12492, 295, 577, 264, 4498, 8895, 9461, 307, 10205, 13, 440, 6175, 291, 536, 510, 322, 51356], "temperature": 0.0, "avg_logprob": -0.08271247148513794, "compression_ratio": 1.8, "no_speech_prob": 0.007121008820831776}, {"id": 205, "seek": 90624, "start": 926.08, "end": 931.12, "text": " the Y axis is normalized based on a model that is fine-tuned on this test task, which means that a", "tokens": [51356, 264, 398, 10298, 307, 48704, 2361, 322, 257, 2316, 300, 307, 2489, 12, 83, 43703, 322, 341, 1500, 5633, 11, 597, 1355, 300, 257, 51608], "temperature": 0.0, "avg_logprob": -0.08271247148513794, "compression_ratio": 1.8, "no_speech_prob": 0.007121008820831776}, {"id": 206, "seek": 90624, "start": 931.12, "end": 935.84, "text": " score of around one is very good because it means that the model that is just trying to adapt on", "tokens": [51608, 6175, 295, 926, 472, 307, 588, 665, 570, 309, 1355, 300, 264, 2316, 300, 307, 445, 1382, 281, 6231, 322, 51844], "temperature": 0.0, "avg_logprob": -0.08271247148513794, "compression_ratio": 1.8, "no_speech_prob": 0.007121008820831776}, {"id": 207, "seek": 93584, "start": 935.84, "end": 940.5600000000001, "text": " the fly is just as good as something that has been fine-tuned on this task. And looking at this", "tokens": [50364, 264, 3603, 307, 445, 382, 665, 382, 746, 300, 575, 668, 2489, 12, 83, 43703, 322, 341, 5633, 13, 400, 1237, 412, 341, 50600], "temperature": 0.0, "avg_logprob": -0.06238724163600377, "compression_ratio": 1.715542521994135, "no_speech_prob": 0.0009697371278889477}, {"id": 208, "seek": 93584, "start": 940.5600000000001, "end": 945.2800000000001, "text": " figure, I think there's two main takeaways. And one is that ADA is pretty effectively able to make", "tokens": [50600, 2573, 11, 286, 519, 456, 311, 732, 2135, 45584, 13, 400, 472, 307, 300, 39354, 307, 1238, 8659, 1075, 281, 652, 50836], "temperature": 0.0, "avg_logprob": -0.06238724163600377, "compression_ratio": 1.715542521994135, "no_speech_prob": 0.0009697371278889477}, {"id": 209, "seek": 93584, "start": 945.2800000000001, "end": 951.6, "text": " use of its multiple trials. As we can see, there is a pretty large gap between having one and 13", "tokens": [50836, 764, 295, 1080, 3866, 12450, 13, 1018, 321, 393, 536, 11, 456, 307, 257, 1238, 2416, 7417, 1296, 1419, 472, 293, 3705, 51152], "temperature": 0.0, "avg_logprob": -0.06238724163600377, "compression_ratio": 1.715542521994135, "no_speech_prob": 0.0009697371278889477}, {"id": 210, "seek": 93584, "start": 951.6, "end": 956.24, "text": " trials. Though after eight or so trials, those benefits do start to wear off as we can see the", "tokens": [51152, 12450, 13, 10404, 934, 3180, 420, 370, 12450, 11, 729, 5311, 360, 722, 281, 3728, 766, 382, 321, 393, 536, 264, 51384], "temperature": 0.0, "avg_logprob": -0.06238724163600377, "compression_ratio": 1.715542521994135, "no_speech_prob": 0.0009697371278889477}, {"id": 211, "seek": 93584, "start": 956.24, "end": 960.64, "text": " eight and the 13 curves are pretty close to each other. Though this should be kind of what we were", "tokens": [51384, 3180, 293, 264, 3705, 19490, 366, 1238, 1998, 281, 1184, 661, 13, 10404, 341, 820, 312, 733, 295, 437, 321, 645, 51604], "temperature": 0.0, "avg_logprob": -0.06238724163600377, "compression_ratio": 1.715542521994135, "no_speech_prob": 0.0009697371278889477}, {"id": 212, "seek": 93584, "start": 960.64, "end": 965.2, "text": " expecting because ADA was only allowed up to six trials during training. So if it magically learned", "tokens": [51604, 9650, 570, 39354, 390, 787, 4350, 493, 281, 2309, 12450, 1830, 3097, 13, 407, 498, 309, 39763, 3264, 51832], "temperature": 0.0, "avg_logprob": -0.06238724163600377, "compression_ratio": 1.715542521994135, "no_speech_prob": 0.0009697371278889477}, {"id": 213, "seek": 96520, "start": 965.2, "end": 969.2, "text": " how to use more, well, that would be kind of crazy. And the second thing to take away, you know,", "tokens": [50364, 577, 281, 764, 544, 11, 731, 11, 300, 576, 312, 733, 295, 3219, 13, 400, 264, 1150, 551, 281, 747, 1314, 11, 291, 458, 11, 50564], "temperature": 0.0, "avg_logprob": -0.06282033294927879, "compression_ratio": 1.6575342465753424, "no_speech_prob": 0.003707075258716941}, {"id": 214, "seek": 96520, "start": 969.76, "end": 976.6400000000001, "text": " ADA is doing pretty good here. It's able to get greater than an 80% score on 72% of these tasks.", "tokens": [50592, 39354, 307, 884, 1238, 665, 510, 13, 467, 311, 1075, 281, 483, 5044, 813, 364, 4688, 4, 6175, 322, 18731, 4, 295, 613, 9608, 13, 50936], "temperature": 0.0, "avg_logprob": -0.06282033294927879, "compression_ratio": 1.6575342465753424, "no_speech_prob": 0.003707075258716941}, {"id": 215, "seek": 96520, "start": 976.6400000000001, "end": 981.76, "text": " And it's only a mere about 10% of tasks where ADA doesn't get any reward at all. So overall,", "tokens": [50936, 400, 309, 311, 787, 257, 8401, 466, 1266, 4, 295, 9608, 689, 39354, 1177, 380, 483, 604, 7782, 412, 439, 13, 407, 4787, 11, 51192], "temperature": 0.0, "avg_logprob": -0.06282033294927879, "compression_ratio": 1.6575342465753424, "no_speech_prob": 0.003707075258716941}, {"id": 216, "seek": 96520, "start": 981.76, "end": 986.4000000000001, "text": " pretty good, especially considering that ADA only has a limited amount of time to solve these tasks", "tokens": [51192, 1238, 665, 11, 2318, 8079, 300, 39354, 787, 575, 257, 5567, 2372, 295, 565, 281, 5039, 613, 9608, 51424], "temperature": 0.0, "avg_logprob": -0.06282033294927879, "compression_ratio": 1.6575342465753424, "no_speech_prob": 0.003707075258716941}, {"id": 217, "seek": 96520, "start": 986.4000000000001, "end": 991.2800000000001, "text": " that are pretty complicated and always going to be randomized. So the final model is doing pretty", "tokens": [51424, 300, 366, 1238, 6179, 293, 1009, 516, 281, 312, 38513, 13, 407, 264, 2572, 2316, 307, 884, 1238, 51668], "temperature": 0.0, "avg_logprob": -0.06282033294927879, "compression_ratio": 1.6575342465753424, "no_speech_prob": 0.003707075258716941}, {"id": 218, "seek": 99128, "start": 991.28, "end": 995.28, "text": " good. But how does this compare to humans? I said this is pretty good. These are pretty hard", "tokens": [50364, 665, 13, 583, 577, 775, 341, 6794, 281, 6255, 30, 286, 848, 341, 307, 1238, 665, 13, 1981, 366, 1238, 1152, 50564], "temperature": 0.0, "avg_logprob": -0.08206479416953193, "compression_ratio": 1.7378048780487805, "no_speech_prob": 0.01854539103806019}, {"id": 219, "seek": 99128, "start": 995.28, "end": 999.52, "text": " tasks. But maybe these are easier than I'm making them out to be. After all, maybe if we gave this", "tokens": [50564, 9608, 13, 583, 1310, 613, 366, 3571, 813, 286, 478, 1455, 552, 484, 281, 312, 13, 2381, 439, 11, 1310, 498, 321, 2729, 341, 50776], "temperature": 0.0, "avg_logprob": -0.08206479416953193, "compression_ratio": 1.7378048780487805, "no_speech_prob": 0.01854539103806019}, {"id": 220, "seek": 99128, "start": 999.52, "end": 1004.4, "text": " to humans, they would be able to do this much faster, you might think. Luckily for us, the", "tokens": [50776, 281, 6255, 11, 436, 576, 312, 1075, 281, 360, 341, 709, 4663, 11, 291, 1062, 519, 13, 19726, 337, 505, 11, 264, 51020], "temperature": 0.0, "avg_logprob": -0.08206479416953193, "compression_ratio": 1.7378048780487805, "no_speech_prob": 0.01854539103806019}, {"id": 221, "seek": 99128, "start": 1004.4, "end": 1008.9599999999999, "text": " authors actually tested this idea with a number of human trials. Here you can see the scores of", "tokens": [51020, 16552, 767, 8246, 341, 1558, 365, 257, 1230, 295, 1952, 12450, 13, 1692, 291, 393, 536, 264, 13444, 295, 51248], "temperature": 0.0, "avg_logprob": -0.08206479416953193, "compression_ratio": 1.7378048780487805, "no_speech_prob": 0.01854539103806019}, {"id": 222, "seek": 99128, "start": 1008.9599999999999, "end": 1014.56, "text": " humans and then also of the ADA agent as it gets more trials. And what you immediately notice is", "tokens": [51248, 6255, 293, 550, 611, 295, 264, 39354, 9461, 382, 309, 2170, 544, 12450, 13, 400, 437, 291, 4258, 3449, 307, 51528], "temperature": 0.0, "avg_logprob": -0.08206479416953193, "compression_ratio": 1.7378048780487805, "no_speech_prob": 0.01854539103806019}, {"id": 223, "seek": 99128, "start": 1014.56, "end": 1020.16, "text": " what is that? Look at that gap right there. I'm not going to lie. I expected humans to kind of", "tokens": [51528, 437, 307, 300, 30, 2053, 412, 300, 7417, 558, 456, 13, 286, 478, 406, 516, 281, 4544, 13, 286, 5176, 6255, 281, 733, 295, 51808], "temperature": 0.0, "avg_logprob": -0.08206479416953193, "compression_ratio": 1.7378048780487805, "no_speech_prob": 0.01854539103806019}, {"id": 224, "seek": 102016, "start": 1020.16, "end": 1024.0, "text": " crush this task. It seems like kind of difficult, but you would think that, you know, a human would", "tokens": [50364, 10321, 341, 5633, 13, 467, 2544, 411, 733, 295, 2252, 11, 457, 291, 576, 519, 300, 11, 291, 458, 11, 257, 1952, 576, 50556], "temperature": 0.0, "avg_logprob": -0.0644982201712472, "compression_ratio": 1.822085889570552, "no_speech_prob": 0.00555447768419981}, {"id": 225, "seek": 102016, "start": 1024.0, "end": 1028.6399999999999, "text": " be able to figure this out. So one thing I thought looking at this at first was, you know, maybe humans", "tokens": [50556, 312, 1075, 281, 2573, 341, 484, 13, 407, 472, 551, 286, 1194, 1237, 412, 341, 412, 700, 390, 11, 291, 458, 11, 1310, 6255, 50788], "temperature": 0.0, "avg_logprob": -0.0644982201712472, "compression_ratio": 1.822085889570552, "no_speech_prob": 0.00555447768419981}, {"id": 226, "seek": 102016, "start": 1028.6399999999999, "end": 1033.2, "text": " because they don't have any prior experience with this specific environment, it takes some time to", "tokens": [50788, 570, 436, 500, 380, 362, 604, 4059, 1752, 365, 341, 2685, 2823, 11, 309, 2516, 512, 565, 281, 51016], "temperature": 0.0, "avg_logprob": -0.0644982201712472, "compression_ratio": 1.822085889570552, "no_speech_prob": 0.00555447768419981}, {"id": 227, "seek": 102016, "start": 1033.2, "end": 1037.2, "text": " figure out the rules. There's all these rules. Some of them are hidden. There's different goals.", "tokens": [51016, 2573, 484, 264, 4474, 13, 821, 311, 439, 613, 4474, 13, 2188, 295, 552, 366, 7633, 13, 821, 311, 819, 5493, 13, 51216], "temperature": 0.0, "avg_logprob": -0.0644982201712472, "compression_ratio": 1.822085889570552, "no_speech_prob": 0.00555447768419981}, {"id": 228, "seek": 102016, "start": 1037.2, "end": 1041.76, "text": " Everything's randomized. But perhaps if a human, you know, maybe had some priming to this task,", "tokens": [51216, 5471, 311, 38513, 13, 583, 4317, 498, 257, 1952, 11, 291, 458, 11, 1310, 632, 512, 2886, 278, 281, 341, 5633, 11, 51444], "temperature": 0.0, "avg_logprob": -0.0644982201712472, "compression_ratio": 1.822085889570552, "no_speech_prob": 0.00555447768419981}, {"id": 229, "seek": 102016, "start": 1041.76, "end": 1045.68, "text": " then they would be able to do a lot better, maybe on par with ADA. Turns out they thought of that.", "tokens": [51444, 550, 436, 576, 312, 1075, 281, 360, 257, 688, 1101, 11, 1310, 322, 971, 365, 39354, 13, 29524, 484, 436, 1194, 295, 300, 13, 51640], "temperature": 0.0, "avg_logprob": -0.0644982201712472, "compression_ratio": 1.822085889570552, "no_speech_prob": 0.00555447768419981}, {"id": 230, "seek": 104568, "start": 1046.64, "end": 1051.6000000000001, "text": " The humans that participated in these trials, actually before they participated in these trials,", "tokens": [50412, 440, 6255, 300, 17978, 294, 613, 12450, 11, 767, 949, 436, 17978, 294, 613, 12450, 11, 50660], "temperature": 0.0, "avg_logprob": -0.06964068376380979, "compression_ratio": 1.862876254180602, "no_speech_prob": 0.001000481890514493}, {"id": 231, "seek": 104568, "start": 1051.6000000000001, "end": 1056.16, "text": " had to complete a bunch of test levels, not the ones that are shown in this graph,", "tokens": [50660, 632, 281, 3566, 257, 3840, 295, 1500, 4358, 11, 406, 264, 2306, 300, 366, 4898, 294, 341, 4295, 11, 50888], "temperature": 0.0, "avg_logprob": -0.06964068376380979, "compression_ratio": 1.862876254180602, "no_speech_prob": 0.001000481890514493}, {"id": 232, "seek": 104568, "start": 1056.16, "end": 1059.76, "text": " but a bunch of different test levels that would have familiarized them with the rules of how", "tokens": [50888, 457, 257, 3840, 295, 819, 1500, 4358, 300, 576, 362, 4963, 1602, 552, 365, 264, 4474, 295, 577, 51068], "temperature": 0.0, "avg_logprob": -0.06964068376380979, "compression_ratio": 1.862876254180602, "no_speech_prob": 0.001000481890514493}, {"id": 233, "seek": 104568, "start": 1059.76, "end": 1064.88, "text": " this works. So despite that, humans still kind of suck. Before we move on to this, I also want", "tokens": [51068, 341, 1985, 13, 407, 7228, 300, 11, 6255, 920, 733, 295, 9967, 13, 4546, 321, 1286, 322, 281, 341, 11, 286, 611, 528, 51324], "temperature": 0.0, "avg_logprob": -0.06964068376380979, "compression_ratio": 1.862876254180602, "no_speech_prob": 0.001000481890514493}, {"id": 234, "seek": 104568, "start": 1064.88, "end": 1069.3600000000001, "text": " to show you this. What we just saw was averaged over all these different tasks. But here we can", "tokens": [51324, 281, 855, 291, 341, 13, 708, 321, 445, 1866, 390, 18247, 2980, 670, 439, 613, 819, 9608, 13, 583, 510, 321, 393, 51548], "temperature": 0.0, "avg_logprob": -0.06964068376380979, "compression_ratio": 1.862876254180602, "no_speech_prob": 0.001000481890514493}, {"id": 235, "seek": 104568, "start": 1069.3600000000001, "end": 1073.76, "text": " see the individual tasks. And one thing to note is that there are some tasks where humans are", "tokens": [51548, 536, 264, 2609, 9608, 13, 400, 472, 551, 281, 3637, 307, 300, 456, 366, 512, 9608, 689, 6255, 366, 51768], "temperature": 0.0, "avg_logprob": -0.06964068376380979, "compression_ratio": 1.862876254180602, "no_speech_prob": 0.001000481890514493}, {"id": 236, "seek": 107376, "start": 1073.76, "end": 1079.04, "text": " actually able to perform better, or ADA really just can't do anything at all. However, in the", "tokens": [50364, 767, 1075, 281, 2042, 1101, 11, 420, 39354, 534, 445, 393, 380, 360, 1340, 412, 439, 13, 2908, 11, 294, 264, 50628], "temperature": 0.0, "avg_logprob": -0.07959644453866141, "compression_ratio": 1.7912772585669783, "no_speech_prob": 0.004468075465410948}, {"id": 237, "seek": 107376, "start": 1079.04, "end": 1083.36, "text": " vast majority of these tasks, as you saw reflected in the previous figure, ADA still does perform", "tokens": [50628, 8369, 6286, 295, 613, 9608, 11, 382, 291, 1866, 15502, 294, 264, 3894, 2573, 11, 39354, 920, 775, 2042, 50844], "temperature": 0.0, "avg_logprob": -0.07959644453866141, "compression_ratio": 1.7912772585669783, "no_speech_prob": 0.004468075465410948}, {"id": 238, "seek": 107376, "start": 1083.36, "end": 1087.28, "text": " better. So overall, there are some things that humans can do, but ADA can't do. And there are", "tokens": [50844, 1101, 13, 407, 4787, 11, 456, 366, 512, 721, 300, 6255, 393, 360, 11, 457, 39354, 393, 380, 360, 13, 400, 456, 366, 51040], "temperature": 0.0, "avg_logprob": -0.07959644453866141, "compression_ratio": 1.7912772585669783, "no_speech_prob": 0.004468075465410948}, {"id": 239, "seek": 107376, "start": 1087.28, "end": 1091.52, "text": " many things that ADA can do faster, but humans cannot do very well. But in the average, ADA is", "tokens": [51040, 867, 721, 300, 39354, 393, 360, 4663, 11, 457, 6255, 2644, 360, 588, 731, 13, 583, 294, 264, 4274, 11, 39354, 307, 51252], "temperature": 0.0, "avg_logprob": -0.07959644453866141, "compression_ratio": 1.7912772585669783, "no_speech_prob": 0.004468075465410948}, {"id": 240, "seek": 107376, "start": 1091.52, "end": 1095.6, "text": " much faster to learn than humans. The next thing we'll take a look at is multi agent learning,", "tokens": [51252, 709, 4663, 281, 1466, 813, 6255, 13, 440, 958, 551, 321, 603, 747, 257, 574, 412, 307, 4825, 9461, 2539, 11, 51456], "temperature": 0.0, "avg_logprob": -0.07959644453866141, "compression_ratio": 1.7912772585669783, "no_speech_prob": 0.004468075465410948}, {"id": 241, "seek": 107376, "start": 1095.6, "end": 1099.2, "text": " because apparently it wasn't enough for the authors just to do single agents, they need to do multi", "tokens": [51456, 570, 7970, 309, 2067, 380, 1547, 337, 264, 16552, 445, 281, 360, 2167, 12554, 11, 436, 643, 281, 360, 4825, 51636], "temperature": 0.0, "avg_logprob": -0.07959644453866141, "compression_ratio": 1.7912772585669783, "no_speech_prob": 0.004468075465410948}, {"id": 242, "seek": 109920, "start": 1099.2, "end": 1103.8400000000001, "text": " agent learning too. It's pretty crazy. So multi agent learning is unsurprisingly, when you have", "tokens": [50364, 9461, 2539, 886, 13, 467, 311, 1238, 3219, 13, 407, 4825, 9461, 2539, 307, 2693, 374, 34408, 11, 562, 291, 362, 50596], "temperature": 0.0, "avg_logprob": -0.0670879222728588, "compression_ratio": 1.9358108108108107, "no_speech_prob": 0.039633311331272125}, {"id": 243, "seek": 109920, "start": 1103.8400000000001, "end": 1107.68, "text": " multiple agents, but more interestingly, in these tasks, where there are multiple agents,", "tokens": [50596, 3866, 12554, 11, 457, 544, 25873, 11, 294, 613, 9608, 11, 689, 456, 366, 3866, 12554, 11, 50788], "temperature": 0.0, "avg_logprob": -0.0670879222728588, "compression_ratio": 1.9358108108108107, "no_speech_prob": 0.039633311331272125}, {"id": 244, "seek": 109920, "start": 1107.68, "end": 1111.76, "text": " these agents need to cooperate to get the maximum reward. For example, in this environment, you're", "tokens": [50788, 613, 12554, 643, 281, 26667, 281, 483, 264, 6674, 7782, 13, 1171, 1365, 11, 294, 341, 2823, 11, 291, 434, 50992], "temperature": 0.0, "avg_logprob": -0.0670879222728588, "compression_ratio": 1.9358108108108107, "no_speech_prob": 0.039633311331272125}, {"id": 245, "seek": 109920, "start": 1111.76, "end": 1116.4, "text": " seen right here, you have two agents one right here and one right here. But the goals for each of", "tokens": [50992, 1612, 558, 510, 11, 291, 362, 732, 12554, 472, 558, 510, 293, 472, 558, 510, 13, 583, 264, 5493, 337, 1184, 295, 51224], "temperature": 0.0, "avg_logprob": -0.0670879222728588, "compression_ratio": 1.9358108108108107, "no_speech_prob": 0.039633311331272125}, {"id": 246, "seek": 109920, "start": 1116.4, "end": 1120.56, "text": " them, I think are to hold these objects on the other side of the wall that they cannot pass. So", "tokens": [51224, 552, 11, 286, 519, 366, 281, 1797, 613, 6565, 322, 264, 661, 1252, 295, 264, 2929, 300, 436, 2644, 1320, 13, 407, 51432], "temperature": 0.0, "avg_logprob": -0.0670879222728588, "compression_ratio": 1.9358108108108107, "no_speech_prob": 0.039633311331272125}, {"id": 247, "seek": 109920, "start": 1120.56, "end": 1125.76, "text": " to maximize the total reward here, both of these agents need to throw their items to the other", "tokens": [51432, 281, 19874, 264, 3217, 7782, 510, 11, 1293, 295, 613, 12554, 643, 281, 3507, 641, 4754, 281, 264, 661, 51692], "temperature": 0.0, "avg_logprob": -0.0670879222728588, "compression_ratio": 1.9358108108108107, "no_speech_prob": 0.039633311331272125}, {"id": 248, "seek": 112576, "start": 1125.76, "end": 1130.24, "text": " side of the wall. So this is just one example of what a multi agent environment might look like.", "tokens": [50364, 1252, 295, 264, 2929, 13, 407, 341, 307, 445, 472, 1365, 295, 437, 257, 4825, 9461, 2823, 1062, 574, 411, 13, 50588], "temperature": 0.0, "avg_logprob": -0.06909026169195408, "compression_ratio": 1.9563953488372092, "no_speech_prob": 0.018544964492321014}, {"id": 249, "seek": 112576, "start": 1130.24, "end": 1133.12, "text": " At the figures up here, we can see what they're trying to measure. And what they're trying to", "tokens": [50588, 1711, 264, 9624, 493, 510, 11, 321, 393, 536, 437, 436, 434, 1382, 281, 3481, 13, 400, 437, 436, 434, 1382, 281, 50732], "temperature": 0.0, "avg_logprob": -0.06909026169195408, "compression_ratio": 1.9563953488372092, "no_speech_prob": 0.018544964492321014}, {"id": 250, "seek": 112576, "start": 1133.12, "end": 1138.0, "text": " measure for multi agent learning is is an ADA agent cooperating with another ADA agent. In other", "tokens": [50732, 3481, 337, 4825, 9461, 2539, 307, 307, 364, 39354, 9461, 13414, 990, 365, 1071, 39354, 9461, 13, 682, 661, 50976], "temperature": 0.0, "avg_logprob": -0.06909026169195408, "compression_ratio": 1.9563953488372092, "no_speech_prob": 0.018544964492321014}, {"id": 251, "seek": 112576, "start": 1138.0, "end": 1141.6, "text": " words, they have, you know, ADA, they can replicate it, they just have it work with itself. Is that", "tokens": [50976, 2283, 11, 436, 362, 11, 291, 458, 11, 39354, 11, 436, 393, 25356, 309, 11, 436, 445, 362, 309, 589, 365, 2564, 13, 1119, 300, 51156], "temperature": 0.0, "avg_logprob": -0.06909026169195408, "compression_ratio": 1.9563953488372092, "no_speech_prob": 0.018544964492321014}, {"id": 252, "seek": 112576, "start": 1141.6, "end": 1145.92, "text": " better than cooperating with a random agent? Because if it is, that would mean that ADA is", "tokens": [51156, 1101, 813, 13414, 990, 365, 257, 4974, 9461, 30, 1436, 498, 309, 307, 11, 300, 576, 914, 300, 39354, 307, 51372], "temperature": 0.0, "avg_logprob": -0.06909026169195408, "compression_ratio": 1.9563953488372092, "no_speech_prob": 0.018544964492321014}, {"id": 253, "seek": 112576, "start": 1145.92, "end": 1150.32, "text": " learning some sort of cooperation that's better than just random behavior. For all the tasks that", "tokens": [51372, 2539, 512, 1333, 295, 14968, 300, 311, 1101, 813, 445, 4974, 5223, 13, 1171, 439, 264, 9608, 300, 51592], "temperature": 0.0, "avg_logprob": -0.06909026169195408, "compression_ratio": 1.9563953488372092, "no_speech_prob": 0.018544964492321014}, {"id": 254, "seek": 112576, "start": 1150.32, "end": 1153.92, "text": " ADA is being given, if we look at the one where it's achieving the median score, we can see that", "tokens": [51592, 39354, 307, 885, 2212, 11, 498, 321, 574, 412, 264, 472, 689, 309, 311, 19626, 264, 26779, 6175, 11, 321, 393, 536, 300, 51772], "temperature": 0.0, "avg_logprob": -0.06909026169195408, "compression_ratio": 1.9563953488372092, "no_speech_prob": 0.018544964492321014}, {"id": 255, "seek": 115392, "start": 1154.0, "end": 1157.76, "text": " there's not actually a huge difference. But the reason there's not a huge difference is because", "tokens": [50368, 456, 311, 406, 767, 257, 2603, 2649, 13, 583, 264, 1778, 456, 311, 406, 257, 2603, 2649, 307, 570, 50556], "temperature": 0.0, "avg_logprob": -0.04197939726022574, "compression_ratio": 1.8571428571428572, "no_speech_prob": 0.003483317792415619}, {"id": 256, "seek": 115392, "start": 1157.76, "end": 1162.5600000000002, "text": " they're both already near one, they're already near optimal performance. If however, we instead", "tokens": [50556, 436, 434, 1293, 1217, 2651, 472, 11, 436, 434, 1217, 2651, 16252, 3389, 13, 759, 4461, 11, 321, 2602, 50796], "temperature": 0.0, "avg_logprob": -0.04197939726022574, "compression_ratio": 1.8571428571428572, "no_speech_prob": 0.003483317792415619}, {"id": 257, "seek": 115392, "start": 1162.5600000000002, "end": 1166.88, "text": " look at the 20th percentile, or in other words, tasks where ADA is not able to do quite as well", "tokens": [50796, 574, 412, 264, 945, 392, 3043, 794, 11, 420, 294, 661, 2283, 11, 9608, 689, 39354, 307, 406, 1075, 281, 360, 1596, 382, 731, 51012], "temperature": 0.0, "avg_logprob": -0.04197939726022574, "compression_ratio": 1.8571428571428572, "no_speech_prob": 0.003483317792415619}, {"id": 258, "seek": 115392, "start": 1166.88, "end": 1172.24, "text": " that are harder, we can see that cooperative self play, which is where ADA is playing with itself", "tokens": [51012, 300, 366, 6081, 11, 321, 393, 536, 300, 31772, 2698, 862, 11, 597, 307, 689, 39354, 307, 2433, 365, 2564, 51280], "temperature": 0.0, "avg_logprob": -0.04197939726022574, "compression_ratio": 1.8571428571428572, "no_speech_prob": 0.003483317792415619}, {"id": 259, "seek": 115392, "start": 1172.24, "end": 1177.04, "text": " instead of a random agent does decently outperform playing with a random agent. So this just shows", "tokens": [51280, 2602, 295, 257, 4974, 9461, 775, 979, 2276, 484, 26765, 2433, 365, 257, 4974, 9461, 13, 407, 341, 445, 3110, 51520], "temperature": 0.0, "avg_logprob": -0.04197939726022574, "compression_ratio": 1.8571428571428572, "no_speech_prob": 0.003483317792415619}, {"id": 260, "seek": 115392, "start": 1177.04, "end": 1181.76, "text": " us that ADA without any special multi agent objective can actually cooperate with other", "tokens": [51520, 505, 300, 39354, 1553, 604, 2121, 4825, 9461, 10024, 393, 767, 26667, 365, 661, 51756], "temperature": 0.0, "avg_logprob": -0.04197939726022574, "compression_ratio": 1.8571428571428572, "no_speech_prob": 0.003483317792415619}, {"id": 261, "seek": 118176, "start": 1181.76, "end": 1186.48, "text": " agents when those actions are in its favor. One reason I kind of like these results and I don't", "tokens": [50364, 12554, 562, 729, 5909, 366, 294, 1080, 2294, 13, 1485, 1778, 286, 733, 295, 411, 613, 3542, 293, 286, 500, 380, 50600], "temperature": 0.0, "avg_logprob": -0.08506113066709131, "compression_ratio": 1.771604938271605, "no_speech_prob": 0.019717833027243614}, {"id": 262, "seek": 118176, "start": 1186.48, "end": 1191.44, "text": " mean to be spin fire here, but when you go into sort of the multi agent universe of research", "tokens": [50600, 914, 281, 312, 6060, 2610, 510, 11, 457, 562, 291, 352, 666, 1333, 295, 264, 4825, 9461, 6445, 295, 2132, 50848], "temperature": 0.0, "avg_logprob": -0.08506113066709131, "compression_ratio": 1.771604938271605, "no_speech_prob": 0.019717833027243614}, {"id": 263, "seek": 118176, "start": 1191.44, "end": 1195.68, "text": " being done, there are lots of approaches where they treat multi agent as this sort of separate", "tokens": [50848, 885, 1096, 11, 456, 366, 3195, 295, 11587, 689, 436, 2387, 4825, 9461, 382, 341, 1333, 295, 4994, 51060], "temperature": 0.0, "avg_logprob": -0.08506113066709131, "compression_ratio": 1.771604938271605, "no_speech_prob": 0.019717833027243614}, {"id": 264, "seek": 118176, "start": 1195.68, "end": 1199.84, "text": " setting from the single agent RL problem. And while that can be useful, I think what this shows is", "tokens": [51060, 3287, 490, 264, 2167, 9461, 497, 43, 1154, 13, 400, 1339, 300, 393, 312, 4420, 11, 286, 519, 437, 341, 3110, 307, 51268], "temperature": 0.0, "avg_logprob": -0.08506113066709131, "compression_ratio": 1.771604938271605, "no_speech_prob": 0.019717833027243614}, {"id": 265, "seek": 118176, "start": 1199.84, "end": 1204.8799999999999, "text": " so long as it's beneficial to the single agent multi agent cooperation can arise as an emergent", "tokens": [51268, 370, 938, 382, 309, 311, 14072, 281, 264, 2167, 9461, 4825, 9461, 14968, 393, 20288, 382, 364, 4345, 6930, 51520], "temperature": 0.0, "avg_logprob": -0.08506113066709131, "compression_ratio": 1.771604938271605, "no_speech_prob": 0.019717833027243614}, {"id": 266, "seek": 118176, "start": 1204.8799999999999, "end": 1208.96, "text": " behavior in cases like these, at least there's actually no need to model them separately. Maybe", "tokens": [51520, 5223, 294, 3331, 411, 613, 11, 412, 1935, 456, 311, 767, 572, 643, 281, 2316, 552, 14759, 13, 2704, 51724], "temperature": 0.0, "avg_logprob": -0.08506113066709131, "compression_ratio": 1.771604938271605, "no_speech_prob": 0.019717833027243614}, {"id": 267, "seek": 120896, "start": 1208.96, "end": 1212.48, "text": " it would get you better performance or not. I'm not entirely sure, but it's just some new good", "tokens": [50364, 309, 576, 483, 291, 1101, 3389, 420, 406, 13, 286, 478, 406, 7696, 988, 11, 457, 309, 311, 445, 512, 777, 665, 50540], "temperature": 0.0, "avg_logprob": -0.0836648696508163, "compression_ratio": 1.657439446366782, "no_speech_prob": 0.007576858624815941}, {"id": 268, "seek": 120896, "start": 1212.48, "end": 1216.0, "text": " experiment showing that this kind of thing is also feasible. This next experiment we're taking", "tokens": [50540, 5120, 4099, 300, 341, 733, 295, 551, 307, 611, 26648, 13, 639, 958, 5120, 321, 434, 1940, 50716], "temperature": 0.0, "avg_logprob": -0.0836648696508163, "compression_ratio": 1.657439446366782, "no_speech_prob": 0.007576858624815941}, {"id": 269, "seek": 120896, "start": 1216.0, "end": 1220.56, "text": " a look at is a scaling experiment. I mean, come on, how can you publish a paper these days without", "tokens": [50716, 257, 574, 412, 307, 257, 21589, 5120, 13, 286, 914, 11, 808, 322, 11, 577, 393, 291, 11374, 257, 3035, 613, 1708, 1553, 50944], "temperature": 0.0, "avg_logprob": -0.0836648696508163, "compression_ratio": 1.657439446366782, "no_speech_prob": 0.007576858624815941}, {"id": 270, "seek": 120896, "start": 1220.56, "end": 1227.76, "text": " a scaling experiment? Am I right? I wonder if I should add in some like laughing noises here.", "tokens": [50944, 257, 21589, 5120, 30, 2012, 286, 558, 30, 286, 2441, 498, 286, 820, 909, 294, 512, 411, 5059, 14620, 510, 13, 51304], "temperature": 0.0, "avg_logprob": -0.0836648696508163, "compression_ratio": 1.657439446366782, "no_speech_prob": 0.007576858624815941}, {"id": 271, "seek": 120896, "start": 1233.28, "end": 1238.4, "text": " And unsurprisingly, as you might have guessed, the results here are bigger model do better. Now,", "tokens": [51580, 400, 2693, 374, 34408, 11, 382, 291, 1062, 362, 21852, 11, 264, 3542, 510, 366, 3801, 2316, 360, 1101, 13, 823, 11, 51836], "temperature": 0.0, "avg_logprob": -0.0836648696508163, "compression_ratio": 1.657439446366782, "no_speech_prob": 0.007576858624815941}, {"id": 272, "seek": 123840, "start": 1238.4, "end": 1242.0800000000002, "text": " I think we should, you know, maybe not take this for granted because it was shown", "tokens": [50364, 286, 519, 321, 820, 11, 291, 458, 11, 1310, 406, 747, 341, 337, 12344, 570, 309, 390, 4898, 50548], "temperature": 0.0, "avg_logprob": -0.0749089349836311, "compression_ratio": 1.7983651226158037, "no_speech_prob": 0.001244789338670671}, {"id": 273, "seek": 123840, "start": 1242.0800000000002, "end": 1245.76, "text": " without this whole distillation process that they're doing to get the bigger models working.", "tokens": [50548, 1553, 341, 1379, 42923, 399, 1399, 300, 436, 434, 884, 281, 483, 264, 3801, 5245, 1364, 13, 50732], "temperature": 0.0, "avg_logprob": -0.0749089349836311, "compression_ratio": 1.7983651226158037, "no_speech_prob": 0.001244789338670671}, {"id": 274, "seek": 123840, "start": 1245.76, "end": 1249.68, "text": " The larger models actually couldn't really learn it all. So these scaling curves are really possible", "tokens": [50732, 440, 4833, 5245, 767, 2809, 380, 534, 1466, 309, 439, 13, 407, 613, 21589, 19490, 366, 534, 1944, 50928], "temperature": 0.0, "avg_logprob": -0.0749089349836311, "compression_ratio": 1.7983651226158037, "no_speech_prob": 0.001244789338670671}, {"id": 275, "seek": 123840, "start": 1249.68, "end": 1255.1200000000001, "text": " because of that. The x axis ranges from 6 million to 265 million parameters, which I don't know,", "tokens": [50928, 570, 295, 300, 13, 440, 2031, 10298, 22526, 490, 1386, 2459, 281, 7551, 20, 2459, 9834, 11, 597, 286, 500, 380, 458, 11, 51200], "temperature": 0.0, "avg_logprob": -0.0749089349836311, "compression_ratio": 1.7983651226158037, "no_speech_prob": 0.001244789338670671}, {"id": 276, "seek": 123840, "start": 1255.1200000000001, "end": 1258.8000000000002, "text": " maybe if you only look at large language model stuff that may seem small, but that's huge for", "tokens": [51200, 1310, 498, 291, 787, 574, 412, 2416, 2856, 2316, 1507, 300, 815, 1643, 1359, 11, 457, 300, 311, 2603, 337, 51384], "temperature": 0.0, "avg_logprob": -0.0749089349836311, "compression_ratio": 1.7983651226158037, "no_speech_prob": 0.001244789338670671}, {"id": 277, "seek": 123840, "start": 1258.8000000000002, "end": 1262.64, "text": " reinforcement learning models. No one really does reinforcement learning models that big because", "tokens": [51384, 29280, 2539, 5245, 13, 883, 472, 534, 775, 29280, 2539, 5245, 300, 955, 570, 51576], "temperature": 0.0, "avg_logprob": -0.0749089349836311, "compression_ratio": 1.7983651226158037, "no_speech_prob": 0.001244789338670671}, {"id": 278, "seek": 123840, "start": 1262.64, "end": 1266.5600000000002, "text": " it's incredibly hard to train them as as you've seen. And then again, they have the score on the", "tokens": [51576, 309, 311, 6252, 1152, 281, 3847, 552, 382, 382, 291, 600, 1612, 13, 400, 550, 797, 11, 436, 362, 264, 6175, 322, 264, 51772], "temperature": 0.0, "avg_logprob": -0.0749089349836311, "compression_ratio": 1.7983651226158037, "no_speech_prob": 0.001244789338670671}, {"id": 279, "seek": 126656, "start": 1266.56, "end": 1270.96, "text": " y axis, but they're both log scaled. And in the paper, they say because these are log scaled and", "tokens": [50364, 288, 10298, 11, 457, 436, 434, 1293, 3565, 36039, 13, 400, 294, 264, 3035, 11, 436, 584, 570, 613, 366, 3565, 36039, 293, 50584], "temperature": 0.0, "avg_logprob": -0.08504303559562229, "compression_ratio": 1.7837837837837838, "no_speech_prob": 0.013221998699009418}, {"id": 280, "seek": 126656, "start": 1270.96, "end": 1276.32, "text": " these look approximately linear, this is a roughly power scaling law. Maybe maybe this is a bit of", "tokens": [50584, 613, 574, 10447, 8213, 11, 341, 307, 257, 9810, 1347, 21589, 2101, 13, 2704, 1310, 341, 307, 257, 857, 295, 50852], "temperature": 0.0, "avg_logprob": -0.08504303559562229, "compression_ratio": 1.7837837837837838, "no_speech_prob": 0.013221998699009418}, {"id": 281, "seek": 126656, "start": 1276.32, "end": 1280.1599999999999, "text": " a hot take. I'm sorry, I'm going to share a bit of a hot take. That's that I'm not sure if people", "tokens": [50852, 257, 2368, 747, 13, 286, 478, 2597, 11, 286, 478, 516, 281, 2073, 257, 857, 295, 257, 2368, 747, 13, 663, 311, 300, 286, 478, 406, 988, 498, 561, 51044], "temperature": 0.0, "avg_logprob": -0.08504303559562229, "compression_ratio": 1.7837837837837838, "no_speech_prob": 0.013221998699009418}, {"id": 282, "seek": 126656, "start": 1280.1599999999999, "end": 1284.32, "text": " are just like gung ho to say that everything's power scaling nowadays. But if there wasn't already", "tokens": [51044, 366, 445, 411, 290, 1063, 1106, 281, 584, 300, 1203, 311, 1347, 21589, 13434, 13, 583, 498, 456, 2067, 380, 1217, 51252], "temperature": 0.0, "avg_logprob": -0.08504303559562229, "compression_ratio": 1.7837837837837838, "no_speech_prob": 0.013221998699009418}, {"id": 283, "seek": 126656, "start": 1284.32, "end": 1289.28, "text": " this idea in everyone's heads that machine learning models scale via power law, I don't know, does this", "tokens": [51252, 341, 1558, 294, 1518, 311, 8050, 300, 3479, 2539, 5245, 4373, 5766, 1347, 2101, 11, 286, 500, 380, 458, 11, 775, 341, 51500], "temperature": 0.0, "avg_logprob": -0.08504303559562229, "compression_ratio": 1.7837837837837838, "no_speech_prob": 0.013221998699009418}, {"id": 284, "seek": 126656, "start": 1289.28, "end": 1294.24, "text": " does this look linear to other people? I guess here right at the top for the 13 number of trials,", "tokens": [51500, 775, 341, 574, 8213, 281, 661, 561, 30, 286, 2041, 510, 558, 412, 264, 1192, 337, 264, 3705, 1230, 295, 12450, 11, 51748], "temperature": 0.0, "avg_logprob": -0.08504303559562229, "compression_ratio": 1.7837837837837838, "no_speech_prob": 0.013221998699009418}, {"id": 285, "seek": 129424, "start": 1294.24, "end": 1298.08, "text": " it is tapering off because it's getting to a round maximum performance, right? But if we look", "tokens": [50364, 309, 307, 5119, 1794, 766, 570, 309, 311, 1242, 281, 257, 3098, 6674, 3389, 11, 558, 30, 583, 498, 321, 574, 50556], "temperature": 0.0, "avg_logprob": -0.11013778268474422, "compression_ratio": 1.7391304347826086, "no_speech_prob": 0.005554649513214827}, {"id": 286, "seek": 129424, "start": 1298.08, "end": 1301.28, "text": " on the right, where these are the scores for the 20th percentile tasks, like,", "tokens": [50556, 322, 264, 558, 11, 689, 613, 366, 264, 13444, 337, 264, 945, 392, 3043, 794, 9608, 11, 411, 11, 50716], "temperature": 0.0, "avg_logprob": -0.11013778268474422, "compression_ratio": 1.7391304347826086, "no_speech_prob": 0.005554649513214827}, {"id": 287, "seek": 129424, "start": 1304.0, "end": 1307.92, "text": " is that linear? It doesn't look like it to me. It looks, you know, a little bit more like that.", "tokens": [50852, 307, 300, 8213, 30, 467, 1177, 380, 574, 411, 309, 281, 385, 13, 467, 1542, 11, 291, 458, 11, 257, 707, 857, 544, 411, 300, 13, 51048], "temperature": 0.0, "avg_logprob": -0.11013778268474422, "compression_ratio": 1.7391304347826086, "no_speech_prob": 0.005554649513214827}, {"id": 288, "seek": 129424, "start": 1307.92, "end": 1312.24, "text": " Anyway, maybe I'm being a little picky here. But another thing to mention is they are showing", "tokens": [51048, 5684, 11, 1310, 286, 478, 885, 257, 707, 41099, 510, 13, 583, 1071, 551, 281, 2152, 307, 436, 366, 4099, 51264], "temperature": 0.0, "avg_logprob": -0.11013778268474422, "compression_ratio": 1.7391304347826086, "no_speech_prob": 0.005554649513214827}, {"id": 289, "seek": 129424, "start": 1312.24, "end": 1316.32, "text": " the median and the 20th percentile scores, which I have no problem with. That's perfectly fine.", "tokens": [51264, 264, 26779, 293, 264, 945, 392, 3043, 794, 13444, 11, 597, 286, 362, 572, 1154, 365, 13, 663, 311, 6239, 2489, 13, 51468], "temperature": 0.0, "avg_logprob": -0.11013778268474422, "compression_ratio": 1.7391304347826086, "no_speech_prob": 0.005554649513214827}, {"id": 290, "seek": 129424, "start": 1316.32, "end": 1320.88, "text": " But you know, if they showed the mean, how would that look? Would it also look like power law scaling?", "tokens": [51468, 583, 291, 458, 11, 498, 436, 4712, 264, 914, 11, 577, 576, 300, 574, 30, 6068, 309, 611, 574, 411, 1347, 2101, 21589, 30, 51696], "temperature": 0.0, "avg_logprob": -0.11013778268474422, "compression_ratio": 1.7391304347826086, "no_speech_prob": 0.005554649513214827}, {"id": 291, "seek": 132088, "start": 1320.88, "end": 1325.6000000000001, "text": " Would it look linear? I don't know. I don't know. I can't imagine thinking this is a power law if I", "tokens": [50364, 6068, 309, 574, 8213, 30, 286, 500, 380, 458, 13, 286, 500, 380, 458, 13, 286, 393, 380, 3811, 1953, 341, 307, 257, 1347, 2101, 498, 286, 50600], "temperature": 0.0, "avg_logprob": -0.0874437830250734, "compression_ratio": 1.81651376146789, "no_speech_prob": 0.05834142863750458}, {"id": 292, "seek": 132088, "start": 1325.6000000000001, "end": 1329.44, "text": " did this without already having that in my head. This isn't even like a flight. It's just me going", "tokens": [50600, 630, 341, 1553, 1217, 1419, 300, 294, 452, 1378, 13, 639, 1943, 380, 754, 411, 257, 7018, 13, 467, 311, 445, 385, 516, 50792], "temperature": 0.0, "avg_logprob": -0.0874437830250734, "compression_ratio": 1.81651376146789, "no_speech_prob": 0.05834142863750458}, {"id": 293, "seek": 132088, "start": 1329.44, "end": 1333.44, "text": " on a stupid rant. That's a tangent. So I'll get over this. I'll go to the next results. Now I'm", "tokens": [50792, 322, 257, 6631, 45332, 13, 663, 311, 257, 27747, 13, 407, 286, 603, 483, 670, 341, 13, 286, 603, 352, 281, 264, 958, 3542, 13, 823, 286, 478, 50992], "temperature": 0.0, "avg_logprob": -0.0874437830250734, "compression_ratio": 1.81651376146789, "no_speech_prob": 0.05834142863750458}, {"id": 294, "seek": 132088, "start": 1333.44, "end": 1337.8400000000001, "text": " just happy that it does scale. It's pretty great results. These next results are again about scaling,", "tokens": [50992, 445, 2055, 300, 309, 775, 4373, 13, 467, 311, 1238, 869, 3542, 13, 1981, 958, 3542, 366, 797, 466, 21589, 11, 51212], "temperature": 0.0, "avg_logprob": -0.0874437830250734, "compression_ratio": 1.81651376146789, "no_speech_prob": 0.05834142863750458}, {"id": 295, "seek": 132088, "start": 1337.8400000000001, "end": 1342.24, "text": " but this time they're scaling the memory length instead of the size of the model. I think pretty", "tokens": [51212, 457, 341, 565, 436, 434, 21589, 264, 4675, 4641, 2602, 295, 264, 2744, 295, 264, 2316, 13, 286, 519, 1238, 51432], "temperature": 0.0, "avg_logprob": -0.0874437830250734, "compression_ratio": 1.81651376146789, "no_speech_prob": 0.05834142863750458}, {"id": 296, "seek": 132088, "start": 1342.24, "end": 1347.3600000000001, "text": " unsurprisingly, as you get a larger memory length, you do get better results here. So that is great.", "tokens": [51432, 2693, 374, 34408, 11, 382, 291, 483, 257, 4833, 4675, 4641, 11, 291, 360, 483, 1101, 3542, 510, 13, 407, 300, 307, 869, 13, 51688], "temperature": 0.0, "avg_logprob": -0.0874437830250734, "compression_ratio": 1.81651376146789, "no_speech_prob": 0.05834142863750458}, {"id": 297, "seek": 134736, "start": 1347.36, "end": 1351.4399999999998, "text": " One thing that is interesting to point out here, maybe two things. One is that memory seems to be", "tokens": [50364, 1485, 551, 300, 307, 1880, 281, 935, 484, 510, 11, 1310, 732, 721, 13, 1485, 307, 300, 4675, 2544, 281, 312, 50568], "temperature": 0.0, "avg_logprob": -0.05094691779878405, "compression_ratio": 1.830188679245283, "no_speech_prob": 0.010651903226971626}, {"id": 298, "seek": 134736, "start": 1351.4399999999998, "end": 1356.3999999999999, "text": " much more important in cases where the agent is not doing as well. So again, these are the 20th", "tokens": [50568, 709, 544, 1021, 294, 3331, 689, 264, 9461, 307, 406, 884, 382, 731, 13, 407, 797, 11, 613, 366, 264, 945, 392, 50816], "temperature": 0.0, "avg_logprob": -0.05094691779878405, "compression_ratio": 1.830188679245283, "no_speech_prob": 0.010651903226971626}, {"id": 299, "seek": 134736, "start": 1356.3999999999999, "end": 1360.1599999999999, "text": " percentile tasks. In other words, it's the tasks that are harder for the agent. And perhaps the", "tokens": [50816, 3043, 794, 9608, 13, 682, 661, 2283, 11, 309, 311, 264, 9608, 300, 366, 6081, 337, 264, 9461, 13, 400, 4317, 264, 51004], "temperature": 0.0, "avg_logprob": -0.05094691779878405, "compression_ratio": 1.830188679245283, "no_speech_prob": 0.010651903226971626}, {"id": 300, "seek": 134736, "start": 1360.1599999999999, "end": 1364.9599999999998, "text": " reason memory is more helpful on these is because it can't do it like in one shot or in just a few", "tokens": [51004, 1778, 4675, 307, 544, 4961, 322, 613, 307, 570, 309, 393, 380, 360, 309, 411, 294, 472, 3347, 420, 294, 445, 257, 1326, 51244], "temperature": 0.0, "avg_logprob": -0.05094691779878405, "compression_ratio": 1.830188679245283, "no_speech_prob": 0.010651903226971626}, {"id": 301, "seek": 134736, "start": 1364.9599999999998, "end": 1369.12, "text": " trials. It needs more trials. Hence that longer memory comes more in handy. And the other thing", "tokens": [51244, 12450, 13, 467, 2203, 544, 12450, 13, 22229, 300, 2854, 4675, 1487, 544, 294, 13239, 13, 400, 264, 661, 551, 51452], "temperature": 0.0, "avg_logprob": -0.05094691779878405, "compression_ratio": 1.830188679245283, "no_speech_prob": 0.010651903226971626}, {"id": 302, "seek": 134736, "start": 1369.12, "end": 1373.12, "text": " that's interesting is that if we look at one case as an example here, say this case where we have", "tokens": [51452, 300, 311, 1880, 307, 300, 498, 321, 574, 412, 472, 1389, 382, 364, 1365, 510, 11, 584, 341, 1389, 689, 321, 362, 51652], "temperature": 0.0, "avg_logprob": -0.05094691779878405, "compression_ratio": 1.830188679245283, "no_speech_prob": 0.010651903226971626}, {"id": 303, "seek": 137312, "start": 1373.1999999999998, "end": 1380.3999999999999, "text": " five trials, there are 300 time steps in each trial. And five times 300 gives us 1,800 as a", "tokens": [50368, 1732, 12450, 11, 456, 366, 6641, 565, 4439, 294, 1184, 7308, 13, 400, 1732, 1413, 6641, 2709, 505, 502, 11, 14423, 382, 257, 50728], "temperature": 0.0, "avg_logprob": -0.06281685829162598, "compression_ratio": 1.75, "no_speech_prob": 0.02930941991508007}, {"id": 304, "seek": 137312, "start": 1380.3999999999999, "end": 1385.4399999999998, "text": " total number of time steps in all of these trials. However, if we look at this sort of greenish cyan", "tokens": [50728, 3217, 1230, 295, 565, 4439, 294, 439, 295, 613, 12450, 13, 2908, 11, 498, 321, 574, 412, 341, 1333, 295, 3092, 742, 47463, 50980], "temperature": 0.0, "avg_logprob": -0.06281685829162598, "compression_ratio": 1.75, "no_speech_prob": 0.02930941991508007}, {"id": 305, "seek": 137312, "start": 1385.4399999999998, "end": 1391.4399999999998, "text": " line, we will see that once it gets to 1,800, it still continues to go up a little bit. I mean", "tokens": [50980, 1622, 11, 321, 486, 536, 300, 1564, 309, 2170, 281, 502, 11, 14423, 11, 309, 920, 6515, 281, 352, 493, 257, 707, 857, 13, 286, 914, 51280], "temperature": 0.0, "avg_logprob": -0.06281685829162598, "compression_ratio": 1.75, "no_speech_prob": 0.02930941991508007}, {"id": 306, "seek": 137312, "start": 1391.4399999999998, "end": 1395.6799999999998, "text": " that even though the memory is getting longer than the total amount of steps it will actually see,", "tokens": [51280, 300, 754, 1673, 264, 4675, 307, 1242, 2854, 813, 264, 3217, 2372, 295, 4439, 309, 486, 767, 536, 11, 51492], "temperature": 0.0, "avg_logprob": -0.06281685829162598, "compression_ratio": 1.75, "no_speech_prob": 0.02930941991508007}, {"id": 307, "seek": 137312, "start": 1395.6799999999998, "end": 1399.84, "text": " it's still benefiting from that longer memory. And that's likely because even though it can keep", "tokens": [51492, 309, 311, 920, 47515, 490, 300, 2854, 4675, 13, 400, 300, 311, 3700, 570, 754, 1673, 309, 393, 1066, 51700], "temperature": 0.0, "avg_logprob": -0.06281685829162598, "compression_ratio": 1.75, "no_speech_prob": 0.02930941991508007}, {"id": 308, "seek": 139984, "start": 1399.84, "end": 1404.9599999999998, "text": " memory for 1,800 steps, it's still going to remember something that's much more recent and", "tokens": [50364, 4675, 337, 502, 11, 14423, 4439, 11, 309, 311, 920, 516, 281, 1604, 746, 300, 311, 709, 544, 5162, 293, 50620], "temperature": 0.0, "avg_logprob": -0.06428405111150225, "compression_ratio": 1.8403908794788273, "no_speech_prob": 0.03513973951339722}, {"id": 309, "seek": 139984, "start": 1404.9599999999998, "end": 1408.8799999999999, "text": " actually still happening like in the present. The agent's probably going to remember that much", "tokens": [50620, 767, 920, 2737, 411, 294, 264, 1974, 13, 440, 9461, 311, 1391, 516, 281, 1604, 300, 709, 50816], "temperature": 0.0, "avg_logprob": -0.06428405111150225, "compression_ratio": 1.8403908794788273, "no_speech_prob": 0.03513973951339722}, {"id": 310, "seek": 139984, "start": 1408.8799999999999, "end": 1413.28, "text": " better than something that happened 1,800 steps ago, even if it theoretically can. So even once", "tokens": [50816, 1101, 813, 746, 300, 2011, 502, 11, 14423, 4439, 2057, 11, 754, 498, 309, 29400, 393, 13, 407, 754, 1564, 51036], "temperature": 0.0, "avg_logprob": -0.06428405111150225, "compression_ratio": 1.8403908794788273, "no_speech_prob": 0.03513973951339722}, {"id": 311, "seek": 139984, "start": 1413.28, "end": 1416.8, "text": " you sort of hit that limit of the memory theoretically being able to contain everything", "tokens": [51036, 291, 1333, 295, 2045, 300, 4948, 295, 264, 4675, 29400, 885, 1075, 281, 5304, 1203, 51212], "temperature": 0.0, "avg_logprob": -0.06428405111150225, "compression_ratio": 1.8403908794788273, "no_speech_prob": 0.03513973951339722}, {"id": 312, "seek": 139984, "start": 1416.8, "end": 1420.8799999999999, "text": " that you need, expanding it still can help out a bit. And maybe one more thing I should point out,", "tokens": [51212, 300, 291, 643, 11, 14702, 309, 920, 393, 854, 484, 257, 857, 13, 400, 1310, 472, 544, 551, 286, 820, 935, 484, 11, 51416], "temperature": 0.0, "avg_logprob": -0.06428405111150225, "compression_ratio": 1.8403908794788273, "no_speech_prob": 0.03513973951339722}, {"id": 313, "seek": 139984, "start": 1420.8799999999999, "end": 1425.84, "text": " which is perhaps obvious, but important sort of as a sanity check is that the memory is the most", "tokens": [51416, 597, 307, 4317, 6322, 11, 457, 1021, 1333, 295, 382, 257, 47892, 1520, 307, 300, 264, 4675, 307, 264, 881, 51664], "temperature": 0.0, "avg_logprob": -0.06428405111150225, "compression_ratio": 1.8403908794788273, "no_speech_prob": 0.03513973951339722}, {"id": 314, "seek": 142584, "start": 1425.84, "end": 1430.1599999999999, "text": " helpful when going from one to two trials, which is exactly what we should expect, right? For one", "tokens": [50364, 4961, 562, 516, 490, 472, 281, 732, 12450, 11, 597, 307, 2293, 437, 321, 820, 2066, 11, 558, 30, 1171, 472, 50580], "temperature": 0.0, "avg_logprob": -0.07743251754576902, "compression_ratio": 1.8249336870026525, "no_speech_prob": 0.07158172875642776}, {"id": 315, "seek": 142584, "start": 1430.1599999999999, "end": 1434.56, "text": " trial, we just need enough memory to contain the current episode. But when we go up and we add more", "tokens": [50580, 7308, 11, 321, 445, 643, 1547, 4675, 281, 5304, 264, 2190, 3500, 13, 583, 562, 321, 352, 493, 293, 321, 909, 544, 50800], "temperature": 0.0, "avg_logprob": -0.07743251754576902, "compression_ratio": 1.8249336870026525, "no_speech_prob": 0.07158172875642776}, {"id": 316, "seek": 142584, "start": 1434.56, "end": 1438.8799999999999, "text": " trials, that memory suddenly becomes a lot more important, because not only are we just sort of", "tokens": [50800, 12450, 11, 300, 4675, 5800, 3643, 257, 688, 544, 1021, 11, 570, 406, 787, 366, 321, 445, 1333, 295, 51016], "temperature": 0.0, "avg_logprob": -0.07743251754576902, "compression_ratio": 1.8249336870026525, "no_speech_prob": 0.07158172875642776}, {"id": 317, "seek": 142584, "start": 1438.8799999999999, "end": 1443.4399999999998, "text": " remembering what our current trajectory is, we're remembering the fumbles we had in the past, and", "tokens": [51016, 20719, 437, 527, 2190, 21512, 307, 11, 321, 434, 20719, 264, 283, 12255, 321, 632, 294, 264, 1791, 11, 293, 51244], "temperature": 0.0, "avg_logprob": -0.07743251754576902, "compression_ratio": 1.8249336870026525, "no_speech_prob": 0.07158172875642776}, {"id": 318, "seek": 142584, "start": 1443.4399999999998, "end": 1446.9599999999998, "text": " we're learning how to improve those we're trying to adopt. Hence, you know, we see the biggest gap", "tokens": [51244, 321, 434, 2539, 577, 281, 3470, 729, 321, 434, 1382, 281, 6878, 13, 22229, 11, 291, 458, 11, 321, 536, 264, 3880, 7417, 51420], "temperature": 0.0, "avg_logprob": -0.07743251754576902, "compression_ratio": 1.8249336870026525, "no_speech_prob": 0.07158172875642776}, {"id": 319, "seek": 142584, "start": 1446.9599999999998, "end": 1450.8799999999999, "text": " here, which is sort of a sanity check that this is working how we would expect it to. And I swear", "tokens": [51420, 510, 11, 597, 307, 1333, 295, 257, 47892, 1520, 300, 341, 307, 1364, 577, 321, 576, 2066, 309, 281, 13, 400, 286, 11902, 51616], "temperature": 0.0, "avg_logprob": -0.07743251754576902, "compression_ratio": 1.8249336870026525, "no_speech_prob": 0.07158172875642776}, {"id": 320, "seek": 142584, "start": 1450.8799999999999, "end": 1454.6399999999999, "text": " that these are the last results that I'll show you that do some sort of scaling. Here we're looking", "tokens": [51616, 300, 613, 366, 264, 1036, 3542, 300, 286, 603, 855, 291, 300, 360, 512, 1333, 295, 21589, 13, 1692, 321, 434, 1237, 51804], "temperature": 0.0, "avg_logprob": -0.07743251754576902, "compression_ratio": 1.8249336870026525, "no_speech_prob": 0.07158172875642776}, {"id": 321, "seek": 145464, "start": 1454.72, "end": 1459.6000000000001, "text": " at scaling, I guess not really scaling, but just the number of tasks use when we use 200 million", "tokens": [50368, 412, 21589, 11, 286, 2041, 406, 534, 21589, 11, 457, 445, 264, 1230, 295, 9608, 764, 562, 321, 764, 2331, 2459, 50612], "temperature": 0.0, "avg_logprob": -0.08464566273475761, "compression_ratio": 1.881578947368421, "no_speech_prob": 0.01590518280863762}, {"id": 322, "seek": 145464, "start": 1459.6000000000001, "end": 1465.44, "text": " tasks versus 25 billion tasks, you know, just small difference, 200 million, 25 billion, a wee bit", "tokens": [50612, 9608, 5717, 3552, 5218, 9608, 11, 291, 458, 11, 445, 1359, 2649, 11, 2331, 2459, 11, 3552, 5218, 11, 257, 32753, 857, 50904], "temperature": 0.0, "avg_logprob": -0.08464566273475761, "compression_ratio": 1.881578947368421, "no_speech_prob": 0.01590518280863762}, {"id": 323, "seek": 145464, "start": 1465.44, "end": 1471.0400000000002, "text": " more. And as you get more tasks unsurprisingly, the performance increases, I think this isn't too", "tokens": [50904, 544, 13, 400, 382, 291, 483, 544, 9608, 2693, 374, 34408, 11, 264, 3389, 8637, 11, 286, 519, 341, 1943, 380, 886, 51184], "temperature": 0.0, "avg_logprob": -0.08464566273475761, "compression_ratio": 1.881578947368421, "no_speech_prob": 0.01590518280863762}, {"id": 324, "seek": 145464, "start": 1471.0400000000002, "end": 1476.72, "text": " much of a surprise. If you think of the excellent tasks as a distribution over tasks, we have 200", "tokens": [51184, 709, 295, 257, 6365, 13, 759, 291, 519, 295, 264, 7103, 9608, 382, 257, 7316, 670, 9608, 11, 321, 362, 2331, 51468], "temperature": 0.0, "avg_logprob": -0.08464566273475761, "compression_ratio": 1.881578947368421, "no_speech_prob": 0.01590518280863762}, {"id": 325, "seek": 145464, "start": 1476.72, "end": 1480.48, "text": " million tasks, you're of course sampling many different possibilities. But when you have 25", "tokens": [51468, 2459, 9608, 11, 291, 434, 295, 1164, 21179, 867, 819, 12178, 13, 583, 562, 291, 362, 3552, 51656], "temperature": 0.0, "avg_logprob": -0.08464566273475761, "compression_ratio": 1.881578947368421, "no_speech_prob": 0.01590518280863762}, {"id": 326, "seek": 145464, "start": 1480.48, "end": 1483.76, "text": " billion, you're sampling more possibilities, which means you're kind of filling out your", "tokens": [51656, 5218, 11, 291, 434, 21179, 544, 12178, 11, 597, 1355, 291, 434, 733, 295, 10623, 484, 428, 51820], "temperature": 0.0, "avg_logprob": -0.08464566273475761, "compression_ratio": 1.881578947368421, "no_speech_prob": 0.01590518280863762}, {"id": 327, "seek": 148376, "start": 1483.76, "end": 1487.44, "text": " training distribution a bit more, you're also getting more diversity in what you learned. So", "tokens": [50364, 3097, 7316, 257, 857, 544, 11, 291, 434, 611, 1242, 544, 8811, 294, 437, 291, 3264, 13, 407, 50548], "temperature": 0.0, "avg_logprob": -0.07861893777628891, "compression_ratio": 1.8349206349206348, "no_speech_prob": 0.001169478869996965}, {"id": 328, "seek": 148376, "start": 1487.44, "end": 1491.04, "text": " I think it makes sense that we should expect to see this perform better. And one thing that's kind", "tokens": [50548, 286, 519, 309, 1669, 2020, 300, 321, 820, 2066, 281, 536, 341, 2042, 1101, 13, 400, 472, 551, 300, 311, 733, 50728], "temperature": 0.0, "avg_logprob": -0.07861893777628891, "compression_ratio": 1.8349206349206348, "no_speech_prob": 0.001169478869996965}, {"id": 329, "seek": 148376, "start": 1491.04, "end": 1496.0, "text": " of cool is at least with two to three trials, we can actually have the smaller model, the 23", "tokens": [50728, 295, 1627, 307, 412, 1935, 365, 732, 281, 1045, 12450, 11, 321, 393, 767, 362, 264, 4356, 2316, 11, 264, 6673, 50976], "temperature": 0.0, "avg_logprob": -0.07861893777628891, "compression_ratio": 1.8349206349206348, "no_speech_prob": 0.001169478869996965}, {"id": 330, "seek": 148376, "start": 1496.0, "end": 1500.8799999999999, "text": " million parameter model in blue here outperform the 75 million parameter model by a bit just by", "tokens": [50976, 2459, 13075, 2316, 294, 3344, 510, 484, 26765, 264, 9562, 2459, 13075, 2316, 538, 257, 857, 445, 538, 51220], "temperature": 0.0, "avg_logprob": -0.07861893777628891, "compression_ratio": 1.8349206349206348, "no_speech_prob": 0.001169478869996965}, {"id": 331, "seek": 148376, "start": 1500.8799999999999, "end": 1504.8799999999999, "text": " adding more tasks showing that you know, scaling parameters is not the only thing that's important.", "tokens": [51220, 5127, 544, 9608, 4099, 300, 291, 458, 11, 21589, 9834, 307, 406, 264, 787, 551, 300, 311, 1021, 13, 51420], "temperature": 0.0, "avg_logprob": -0.07861893777628891, "compression_ratio": 1.8349206349206348, "no_speech_prob": 0.001169478869996965}, {"id": 332, "seek": 148376, "start": 1504.8799999999999, "end": 1509.76, "text": " Having a diverse wider set of tasks is also very important. And I think this kind of mirrors what", "tokens": [51420, 10222, 257, 9521, 11842, 992, 295, 9608, 307, 611, 588, 1021, 13, 400, 286, 519, 341, 733, 295, 24238, 437, 51664], "temperature": 0.0, "avg_logprob": -0.07861893777628891, "compression_ratio": 1.8349206349206348, "no_speech_prob": 0.001169478869996965}, {"id": 333, "seek": 150976, "start": 1509.76, "end": 1514.24, "text": " we've seen in NLP, where language models, you know, you need a bigger model to do better,", "tokens": [50364, 321, 600, 1612, 294, 426, 45196, 11, 689, 2856, 5245, 11, 291, 458, 11, 291, 643, 257, 3801, 2316, 281, 360, 1101, 11, 50588], "temperature": 0.0, "avg_logprob": -0.0764264300249625, "compression_ratio": 1.7584097859327217, "no_speech_prob": 0.01168657373636961}, {"id": 334, "seek": 150976, "start": 1514.24, "end": 1518.0, "text": " but you also need better data and that can help quite a bit. And now we finally wrap it back around", "tokens": [50588, 457, 291, 611, 643, 1101, 1412, 293, 300, 393, 854, 1596, 257, 857, 13, 400, 586, 321, 2721, 7019, 309, 646, 926, 50776], "temperature": 0.0, "avg_logprob": -0.0764264300249625, "compression_ratio": 1.7584097859327217, "no_speech_prob": 0.01168657373636961}, {"id": 335, "seek": 150976, "start": 1518.0, "end": 1522.16, "text": " these distillation results. I know I already showed you how a large model without distillation", "tokens": [50776, 613, 42923, 399, 3542, 13, 286, 458, 286, 1217, 4712, 291, 577, 257, 2416, 2316, 1553, 42923, 399, 50984], "temperature": 0.0, "avg_logprob": -0.0764264300249625, "compression_ratio": 1.7584097859327217, "no_speech_prob": 0.01168657373636961}, {"id": 336, "seek": 150976, "start": 1522.16, "end": 1526.8, "text": " feels miserably, but one interesting experiment they add on beneath this is what happens when they", "tokens": [50984, 3417, 17725, 1188, 11, 457, 472, 1880, 5120, 436, 909, 322, 17149, 341, 307, 437, 2314, 562, 436, 51216], "temperature": 0.0, "avg_logprob": -0.0764264300249625, "compression_ratio": 1.7584097859327217, "no_speech_prob": 0.01168657373636961}, {"id": 337, "seek": 150976, "start": 1526.8, "end": 1531.76, "text": " do distillation with two small models of the same size, you might expect that there would be no", "tokens": [51216, 360, 42923, 399, 365, 732, 1359, 5245, 295, 264, 912, 2744, 11, 291, 1062, 2066, 300, 456, 576, 312, 572, 51464], "temperature": 0.0, "avg_logprob": -0.0764264300249625, "compression_ratio": 1.7584097859327217, "no_speech_prob": 0.01168657373636961}, {"id": 338, "seek": 150976, "start": 1531.76, "end": 1536.24, "text": " benefit, or at least that's what I thought. But as it turns out, even when these two models are", "tokens": [51464, 5121, 11, 420, 412, 1935, 300, 311, 437, 286, 1194, 13, 583, 382, 309, 4523, 484, 11, 754, 562, 613, 732, 5245, 366, 51688], "temperature": 0.0, "avg_logprob": -0.0764264300249625, "compression_ratio": 1.7584097859327217, "no_speech_prob": 0.01168657373636961}, {"id": 339, "seek": 153624, "start": 1536.24, "end": 1540.8, "text": " the same size, the teacher and the student here, there is still some benefit. You can see that the", "tokens": [50364, 264, 912, 2744, 11, 264, 5027, 293, 264, 3107, 510, 11, 456, 307, 920, 512, 5121, 13, 509, 393, 536, 300, 264, 50592], "temperature": 0.0, "avg_logprob": -0.06513690267290388, "compression_ratio": 1.8389057750759878, "no_speech_prob": 0.05183672532439232}, {"id": 340, "seek": 153624, "start": 1540.8, "end": 1545.04, "text": " student ends up with a bit higher score than the teacher, even though they are trained for the same", "tokens": [50592, 3107, 5314, 493, 365, 257, 857, 2946, 6175, 813, 264, 5027, 11, 754, 1673, 436, 366, 8895, 337, 264, 912, 50804], "temperature": 0.0, "avg_logprob": -0.06513690267290388, "compression_ratio": 1.8389057750759878, "no_speech_prob": 0.05183672532439232}, {"id": 341, "seek": 153624, "start": 1545.04, "end": 1549.2, "text": " amount of training steps. The reason they mentioned the paper for why this might be happening is due", "tokens": [50804, 2372, 295, 3097, 4439, 13, 440, 1778, 436, 2835, 264, 3035, 337, 983, 341, 1062, 312, 2737, 307, 3462, 51012], "temperature": 0.0, "avg_logprob": -0.06513690267290388, "compression_ratio": 1.8389057750759878, "no_speech_prob": 0.05183672532439232}, {"id": 342, "seek": 153624, "start": 1549.2, "end": 1553.04, "text": " to what's called the primacy bias. Now they don't mention this name in the paper, but I'm pretty sure", "tokens": [51012, 281, 437, 311, 1219, 264, 2886, 2551, 12577, 13, 823, 436, 500, 380, 2152, 341, 1315, 294, 264, 3035, 11, 457, 286, 478, 1238, 988, 51204], "temperature": 0.0, "avg_logprob": -0.06513690267290388, "compression_ratio": 1.8389057750759878, "no_speech_prob": 0.05183672532439232}, {"id": 343, "seek": 153624, "start": 1553.04, "end": 1556.88, "text": " this is what they're referring to. There's a great paper all link in the description that talks about", "tokens": [51204, 341, 307, 437, 436, 434, 13761, 281, 13, 821, 311, 257, 869, 3035, 439, 2113, 294, 264, 3855, 300, 6686, 466, 51396], "temperature": 0.0, "avg_logprob": -0.06513690267290388, "compression_ratio": 1.8389057750759878, "no_speech_prob": 0.05183672532439232}, {"id": 344, "seek": 153624, "start": 1556.88, "end": 1562.0, "text": " this idea. And in the paper, the authors show that representations implicitly learned early on during", "tokens": [51396, 341, 1558, 13, 400, 294, 264, 3035, 11, 264, 16552, 855, 300, 33358, 26947, 356, 3264, 2440, 322, 1830, 51652], "temperature": 0.0, "avg_logprob": -0.06513690267290388, "compression_ratio": 1.8389057750759878, "no_speech_prob": 0.05183672532439232}, {"id": 345, "seek": 156200, "start": 1562.08, "end": 1565.92, "text": " reinforcement learning can hinder an agent's later performance. Essentially, the early", "tokens": [50368, 29280, 2539, 393, 276, 5669, 364, 9461, 311, 1780, 3389, 13, 23596, 11, 264, 2440, 50560], "temperature": 0.0, "avg_logprob": -0.07397109954083553, "compression_ratio": 1.7929936305732483, "no_speech_prob": 0.031139543280005455}, {"id": 346, "seek": 156200, "start": 1565.92, "end": 1570.16, "text": " representations can be bad, and then it's hard for the agent to unlearn those. So here are the", "tokens": [50560, 33358, 393, 312, 1578, 11, 293, 550, 309, 311, 1152, 337, 264, 9461, 281, 25272, 1083, 729, 13, 407, 510, 366, 264, 50772], "temperature": 0.0, "avg_logprob": -0.07397109954083553, "compression_ratio": 1.7929936305732483, "no_speech_prob": 0.031139543280005455}, {"id": 347, "seek": 156200, "start": 1570.16, "end": 1574.8, "text": " author's posture that perhaps the distillation process is helping to avoid these bad early", "tokens": [50772, 3793, 311, 18502, 300, 4317, 264, 42923, 399, 1399, 307, 4315, 281, 5042, 613, 1578, 2440, 51004], "temperature": 0.0, "avg_logprob": -0.07397109954083553, "compression_ratio": 1.7929936305732483, "no_speech_prob": 0.031139543280005455}, {"id": 348, "seek": 156200, "start": 1574.8, "end": 1579.44, "text": " representations. Honestly, I'm not entirely convinced though, because I think having an extra", "tokens": [51004, 33358, 13, 12348, 11, 286, 478, 406, 7696, 12561, 1673, 11, 570, 286, 519, 1419, 364, 2857, 51236], "temperature": 0.0, "avg_logprob": -0.07397109954083553, "compression_ratio": 1.7929936305732483, "no_speech_prob": 0.031139543280005455}, {"id": 349, "seek": 156200, "start": 1579.44, "end": 1583.76, "text": " distillation loss alone could be the reason for the better peak performance here. That being said,", "tokens": [51236, 42923, 399, 4470, 3312, 727, 312, 264, 1778, 337, 264, 1101, 10651, 3389, 510, 13, 663, 885, 848, 11, 51452], "temperature": 0.0, "avg_logprob": -0.07397109954083553, "compression_ratio": 1.7929936305732483, "no_speech_prob": 0.031139543280005455}, {"id": 350, "seek": 156200, "start": 1583.76, "end": 1588.48, "text": " I do think this would be an interesting avenue to explore further because it's certainly not 100%", "tokens": [51452, 286, 360, 519, 341, 576, 312, 364, 1880, 39230, 281, 6839, 3052, 570, 309, 311, 3297, 406, 2319, 4, 51688], "temperature": 0.0, "avg_logprob": -0.07397109954083553, "compression_ratio": 1.7929936305732483, "no_speech_prob": 0.031139543280005455}, {"id": 351, "seek": 158848, "start": 1588.48, "end": 1592.4, "text": " either way. I was actually taking some time to think about this and, you know, having to train", "tokens": [50364, 2139, 636, 13, 286, 390, 767, 1940, 512, 565, 281, 519, 466, 341, 293, 11, 291, 458, 11, 1419, 281, 3847, 50560], "temperature": 0.0, "avg_logprob": -0.06069500996516301, "compression_ratio": 1.864516129032258, "no_speech_prob": 0.023687291890382767}, {"id": 352, "seek": 158848, "start": 1592.4, "end": 1597.52, "text": " multiple models for this whole distillation process gets kind of messy and takes a lot of memory.", "tokens": [50560, 3866, 5245, 337, 341, 1379, 42923, 399, 1399, 2170, 733, 295, 16191, 293, 2516, 257, 688, 295, 4675, 13, 50816], "temperature": 0.0, "avg_logprob": -0.06069500996516301, "compression_ratio": 1.864516129032258, "no_speech_prob": 0.023687291890382767}, {"id": 353, "seek": 158848, "start": 1597.52, "end": 1601.68, "text": " It would be really nice if we could get this to work with just one model. And I was wondering if", "tokens": [50816, 467, 576, 312, 534, 1481, 498, 321, 727, 483, 341, 281, 589, 365, 445, 472, 2316, 13, 400, 286, 390, 6359, 498, 51024], "temperature": 0.0, "avg_logprob": -0.06069500996516301, "compression_ratio": 1.864516129032258, "no_speech_prob": 0.023687291890382767}, {"id": 354, "seek": 158848, "start": 1601.68, "end": 1605.68, "text": " that would be possible by training with a very high learning rate or maybe training multiple", "tokens": [51024, 300, 576, 312, 1944, 538, 3097, 365, 257, 588, 1090, 2539, 3314, 420, 1310, 3097, 3866, 51224], "temperature": 0.0, "avg_logprob": -0.06069500996516301, "compression_ratio": 1.864516129032258, "no_speech_prob": 0.023687291890382767}, {"id": 355, "seek": 158848, "start": 1605.68, "end": 1610.4, "text": " times in the beginning with these large models. One issue, of course, of training with a larger", "tokens": [51224, 1413, 294, 264, 2863, 365, 613, 2416, 5245, 13, 1485, 2734, 11, 295, 1164, 11, 295, 3097, 365, 257, 4833, 51460], "temperature": 0.0, "avg_logprob": -0.06069500996516301, "compression_ratio": 1.864516129032258, "no_speech_prob": 0.023687291890382767}, {"id": 356, "seek": 158848, "start": 1610.4, "end": 1614.32, "text": " learning rate is that some of the representations could end up bad or the model could start jumping", "tokens": [51460, 2539, 3314, 307, 300, 512, 295, 264, 33358, 727, 917, 493, 1578, 420, 264, 2316, 727, 722, 11233, 51656], "temperature": 0.0, "avg_logprob": -0.06069500996516301, "compression_ratio": 1.864516129032258, "no_speech_prob": 0.023687291890382767}, {"id": 357, "seek": 161432, "start": 1614.32, "end": 1618.96, "text": " into some rough optimization territory. But if we randomly reset layers of hidden units,", "tokens": [50364, 666, 512, 5903, 19618, 11360, 13, 583, 498, 321, 16979, 14322, 7914, 295, 7633, 6815, 11, 50596], "temperature": 0.0, "avg_logprob": -0.06278400421142578, "compression_ratio": 1.6567164179104477, "no_speech_prob": 0.03963621333241463}, {"id": 358, "seek": 161432, "start": 1618.96, "end": 1623.12, "text": " which is something they do in the primacy bias paper to unlearn bad representations,", "tokens": [50596, 597, 307, 746, 436, 360, 294, 264, 2886, 2551, 12577, 3035, 281, 25272, 1083, 1578, 33358, 11, 50804], "temperature": 0.0, "avg_logprob": -0.06278400421142578, "compression_ratio": 1.6567164179104477, "no_speech_prob": 0.03963621333241463}, {"id": 359, "seek": 161432, "start": 1623.12, "end": 1627.9199999999998, "text": " then maybe something like that would work. I really have no clue. This is a complete shot in", "tokens": [50804, 550, 1310, 746, 411, 300, 576, 589, 13, 286, 534, 362, 572, 13602, 13, 639, 307, 257, 3566, 3347, 294, 51044], "temperature": 0.0, "avg_logprob": -0.06278400421142578, "compression_ratio": 1.6567164179104477, "no_speech_prob": 0.03963621333241463}, {"id": 360, "seek": 161432, "start": 1627.9199999999998, "end": 1631.6, "text": " the dark and it's been quite the tangent. So I'll get back to the results now, but you know,", "tokens": [51044, 264, 2877, 293, 309, 311, 668, 1596, 264, 27747, 13, 407, 286, 603, 483, 646, 281, 264, 3542, 586, 11, 457, 291, 458, 11, 51228], "temperature": 0.0, "avg_logprob": -0.06278400421142578, "compression_ratio": 1.6567164179104477, "no_speech_prob": 0.03963621333241463}, {"id": 361, "seek": 161432, "start": 1631.6, "end": 1635.84, "text": " if anyone was looking for a research idea to try, there's an idea. We've covered what I think are", "tokens": [51228, 498, 2878, 390, 1237, 337, 257, 2132, 1558, 281, 853, 11, 456, 311, 364, 1558, 13, 492, 600, 5343, 437, 286, 519, 366, 51440], "temperature": 0.0, "avg_logprob": -0.06278400421142578, "compression_ratio": 1.6567164179104477, "no_speech_prob": 0.03963621333241463}, {"id": 362, "seek": 161432, "start": 1635.84, "end": 1640.48, "text": " the most interesting results from this paper. So with this, where do we now stand in the world of", "tokens": [51440, 264, 881, 1880, 3542, 490, 341, 3035, 13, 407, 365, 341, 11, 689, 360, 321, 586, 1463, 294, 264, 1002, 295, 51672], "temperature": 0.0, "avg_logprob": -0.06278400421142578, "compression_ratio": 1.6567164179104477, "no_speech_prob": 0.03963621333241463}, {"id": 363, "seek": 164048, "start": 1640.48, "end": 1645.1200000000001, "text": " RL foundation models? I'll start by saying, I think this is really great. Several papers I think", "tokens": [50364, 497, 43, 7030, 5245, 30, 286, 603, 722, 538, 1566, 11, 286, 519, 341, 307, 534, 869, 13, 22246, 10577, 286, 519, 50596], "temperature": 0.0, "avg_logprob": -0.10717580769513105, "compression_ratio": 1.7240356083086052, "no_speech_prob": 0.109685517847538}, {"id": 364, "seek": 164048, "start": 1645.1200000000001, "end": 1650.0, "text": " have been trying to get at this idea. For example, the Gatto paper from D-Mind and the VPT from", "tokens": [50596, 362, 668, 1382, 281, 483, 412, 341, 1558, 13, 1171, 1365, 11, 264, 460, 37491, 3035, 490, 413, 12, 44, 471, 293, 264, 35812, 51, 490, 50840], "temperature": 0.0, "avg_logprob": -0.10717580769513105, "compression_ratio": 1.7240356083086052, "no_speech_prob": 0.109685517847538}, {"id": 365, "seek": 164048, "start": 1650.0, "end": 1653.68, "text": " Minecraft paper from OpenAI, both getting at something similar. And I've actually covered", "tokens": [50840, 21029, 3035, 490, 7238, 48698, 11, 1293, 1242, 412, 746, 2531, 13, 400, 286, 600, 767, 5343, 51024], "temperature": 0.0, "avg_logprob": -0.10717580769513105, "compression_ratio": 1.7240356083086052, "no_speech_prob": 0.109685517847538}, {"id": 366, "seek": 164048, "start": 1653.68, "end": 1658.24, "text": " both on this channel if you are interested. But all of the other papers on this topic have shied away", "tokens": [51024, 1293, 322, 341, 2269, 498, 291, 366, 3102, 13, 583, 439, 295, 264, 661, 10577, 322, 341, 4829, 362, 402, 1091, 1314, 51252], "temperature": 0.0, "avg_logprob": -0.10717580769513105, "compression_ratio": 1.7240356083086052, "no_speech_prob": 0.109685517847538}, {"id": 367, "seek": 164048, "start": 1658.24, "end": 1663.04, "text": " from actually using RL from start to finish. For example, the Gatto paper, which used supervised", "tokens": [51252, 490, 767, 1228, 497, 43, 490, 722, 281, 2413, 13, 1171, 1365, 11, 264, 460, 37491, 3035, 11, 597, 1143, 46533, 51492], "temperature": 0.0, "avg_logprob": -0.10717580769513105, "compression_ratio": 1.7240356083086052, "no_speech_prob": 0.109685517847538}, {"id": 368, "seek": 164048, "start": 1663.04, "end": 1666.64, "text": " learning, I remember it kind of had a line in it. I'm going to be paraphrasing this because I don't", "tokens": [51492, 2539, 11, 286, 1604, 309, 733, 295, 632, 257, 1622, 294, 309, 13, 286, 478, 516, 281, 312, 36992, 1703, 3349, 341, 570, 286, 500, 380, 51672], "temperature": 0.0, "avg_logprob": -0.10717580769513105, "compression_ratio": 1.7240356083086052, "no_speech_prob": 0.109685517847538}, {"id": 369, "seek": 166664, "start": 1666.64, "end": 1670.72, "text": " remember it exactly, but it was something like, in theory, we could also do the same experiments", "tokens": [50364, 1604, 309, 2293, 11, 457, 309, 390, 746, 411, 11, 294, 5261, 11, 321, 727, 611, 360, 264, 912, 12050, 50568], "temperature": 0.0, "avg_logprob": -0.08880214257673784, "compression_ratio": 1.8993288590604027, "no_speech_prob": 0.09266789257526398}, {"id": 370, "seek": 166664, "start": 1670.72, "end": 1674.4, "text": " with reinforcement learning. And surely it would just work fine because, you know, it's just kind", "tokens": [50568, 365, 29280, 2539, 13, 400, 11468, 309, 576, 445, 589, 2489, 570, 11, 291, 458, 11, 309, 311, 445, 733, 50752], "temperature": 0.0, "avg_logprob": -0.08880214257673784, "compression_ratio": 1.8993288590604027, "no_speech_prob": 0.09266789257526398}, {"id": 371, "seek": 166664, "start": 1674.4, "end": 1680.24, "text": " of the same thing, but with RL instead of supervised learning. And no, hopefully I'm not", "tokens": [50752, 295, 264, 912, 551, 11, 457, 365, 497, 43, 2602, 295, 46533, 2539, 13, 400, 572, 11, 4696, 286, 478, 406, 51044], "temperature": 0.0, "avg_logprob": -0.08880214257673784, "compression_ratio": 1.8993288590604027, "no_speech_prob": 0.09266789257526398}, {"id": 372, "seek": 166664, "start": 1680.24, "end": 1684.88, "text": " butching what they said here. But looking at this paper now, I think it's obvious why doing this", "tokens": [51044, 457, 17354, 437, 436, 848, 510, 13, 583, 1237, 412, 341, 3035, 586, 11, 286, 519, 309, 311, 6322, 983, 884, 341, 51276], "temperature": 0.0, "avg_logprob": -0.08880214257673784, "compression_ratio": 1.8993288590604027, "no_speech_prob": 0.09266789257526398}, {"id": 373, "seek": 166664, "start": 1684.88, "end": 1688.96, "text": " with supervised learning and doing it with reinforcement learning is really not the same.", "tokens": [51276, 365, 46533, 2539, 293, 884, 309, 365, 29280, 2539, 307, 534, 406, 264, 912, 13, 51480], "temperature": 0.0, "avg_logprob": -0.08880214257673784, "compression_ratio": 1.8993288590604027, "no_speech_prob": 0.09266789257526398}, {"id": 374, "seek": 166664, "start": 1688.96, "end": 1693.68, "text": " Reinforcement learning, it's just a lot harder. The problem itself is a harder problem. Getting", "tokens": [51480, 42116, 9382, 2539, 11, 309, 311, 445, 257, 688, 6081, 13, 440, 1154, 2564, 307, 257, 6081, 1154, 13, 13674, 51716], "temperature": 0.0, "avg_logprob": -0.08880214257673784, "compression_ratio": 1.8993288590604027, "no_speech_prob": 0.09266789257526398}, {"id": 375, "seek": 169368, "start": 1693.68, "end": 1697.8400000000001, "text": " the model to scale was not straightforward at all. I mean, the whole distillation thing is not", "tokens": [50364, 264, 2316, 281, 4373, 390, 406, 15325, 412, 439, 13, 286, 914, 11, 264, 1379, 42923, 399, 551, 307, 406, 50572], "temperature": 0.0, "avg_logprob": -0.055155375145918485, "compression_ratio": 1.8324175824175823, "no_speech_prob": 0.025954538956284523}, {"id": 376, "seek": 169368, "start": 1697.8400000000001, "end": 1701.6000000000001, "text": " something I would have thought of at least. Not to mention it took billions of training steps,", "tokens": [50572, 746, 286, 576, 362, 1194, 295, 412, 1935, 13, 1726, 281, 2152, 309, 1890, 17375, 295, 3097, 4439, 11, 50760], "temperature": 0.0, "avg_logprob": -0.055155375145918485, "compression_ratio": 1.8324175824175823, "no_speech_prob": 0.025954538956284523}, {"id": 377, "seek": 169368, "start": 1701.6000000000001, "end": 1706.0, "text": " plus millions and millions of unique tasks to get this working. But I think the really great thing", "tokens": [50760, 1804, 6803, 293, 6803, 295, 3845, 9608, 281, 483, 341, 1364, 13, 583, 286, 519, 264, 534, 869, 551, 50980], "temperature": 0.0, "avg_logprob": -0.055155375145918485, "compression_ratio": 1.8324175824175823, "no_speech_prob": 0.025954538956284523}, {"id": 378, "seek": 169368, "start": 1706.0, "end": 1709.92, "text": " here is that now that we have a paper looking at this, not shying away from the hard questions,", "tokens": [50980, 510, 307, 300, 586, 300, 321, 362, 257, 3035, 1237, 412, 341, 11, 406, 402, 1840, 1314, 490, 264, 1152, 1651, 11, 51176], "temperature": 0.0, "avg_logprob": -0.055155375145918485, "compression_ratio": 1.8324175824175823, "no_speech_prob": 0.025954538956284523}, {"id": 379, "seek": 169368, "start": 1709.92, "end": 1713.8400000000001, "text": " is that we have a reference. And we can use this reference to start deciding the next important", "tokens": [51176, 307, 300, 321, 362, 257, 6408, 13, 400, 321, 393, 764, 341, 6408, 281, 722, 17990, 264, 958, 1021, 51372], "temperature": 0.0, "avg_logprob": -0.055155375145918485, "compression_ratio": 1.8324175824175823, "no_speech_prob": 0.025954538956284523}, {"id": 380, "seek": 169368, "start": 1713.8400000000001, "end": 1718.4, "text": " steps to make this feasible on more diverse sets of tasks. And the main one here definitely looks", "tokens": [51372, 4439, 281, 652, 341, 26648, 322, 544, 9521, 6352, 295, 9608, 13, 400, 264, 2135, 472, 510, 2138, 1542, 51600], "temperature": 0.0, "avg_logprob": -0.055155375145918485, "compression_ratio": 1.8324175824175823, "no_speech_prob": 0.025954538956284523}, {"id": 381, "seek": 169368, "start": 1718.4, "end": 1723.1200000000001, "text": " to be sample efficiency. Even if this is a foundation model requiring billions of steps,", "tokens": [51600, 281, 312, 6889, 10493, 13, 2754, 498, 341, 307, 257, 7030, 2316, 24165, 17375, 295, 4439, 11, 51836], "temperature": 0.0, "avg_logprob": -0.055155375145918485, "compression_ratio": 1.8324175824175823, "no_speech_prob": 0.025954538956284523}, {"id": 382, "seek": 172312, "start": 1723.12, "end": 1726.32, "text": " it's just not going to cut it for one, anyone that doesn't have, you know,", "tokens": [50364, 309, 311, 445, 406, 516, 281, 1723, 309, 337, 472, 11, 2878, 300, 1177, 380, 362, 11, 291, 458, 11, 50524], "temperature": 0.0, "avg_logprob": -0.10353165864944458, "compression_ratio": 1.6310679611650485, "no_speech_prob": 0.0016484307125210762}, {"id": 383, "seek": 172312, "start": 1726.32, "end": 1731.76, "text": " an army of TPUs at their command. Everyone other than Google and OpenAI. Oh, sorry,", "tokens": [50524, 364, 7267, 295, 314, 8115, 82, 412, 641, 5622, 13, 5198, 661, 813, 3329, 293, 7238, 48698, 13, 876, 11, 2597, 11, 50796], "temperature": 0.0, "avg_logprob": -0.10353165864944458, "compression_ratio": 1.6310679611650485, "no_speech_prob": 0.0016484307125210762}, {"id": 384, "seek": 172312, "start": 1731.76, "end": 1737.6799999999998, "text": " what was that? And two, any tasks where simulation is difficult or costly, like,", "tokens": [50796, 437, 390, 300, 30, 400, 732, 11, 604, 9608, 689, 16575, 307, 2252, 420, 28328, 11, 411, 11, 51092], "temperature": 0.0, "avg_logprob": -0.10353165864944458, "compression_ratio": 1.6310679611650485, "no_speech_prob": 0.0016484307125210762}, {"id": 385, "seek": 172312, "start": 1737.6799999999998, "end": 1742.7199999999998, "text": " you know, the real world, this kind of data requirement just is not going to be enough.", "tokens": [51092, 291, 458, 11, 264, 957, 1002, 11, 341, 733, 295, 1412, 11695, 445, 307, 406, 516, 281, 312, 1547, 13, 51344], "temperature": 0.0, "avg_logprob": -0.10353165864944458, "compression_ratio": 1.6310679611650485, "no_speech_prob": 0.0016484307125210762}, {"id": 386, "seek": 172312, "start": 1742.7199999999998, "end": 1746.8, "text": " Luckily, I think that there are lots of promising avenues here. As I mentioned earlier,", "tokens": [51344, 19726, 11, 286, 519, 300, 456, 366, 3195, 295, 20257, 43039, 510, 13, 1018, 286, 2835, 3071, 11, 51548], "temperature": 0.0, "avg_logprob": -0.10353165864944458, "compression_ratio": 1.6310679611650485, "no_speech_prob": 0.0016484307125210762}, {"id": 387, "seek": 172312, "start": 1746.8, "end": 1750.9599999999998, "text": " model based reinforcement learning methods like efficient zero and dreamer are, I think,", "tokens": [51548, 2316, 2361, 29280, 2539, 7150, 411, 7148, 4018, 293, 3055, 260, 366, 11, 286, 519, 11, 51756], "temperature": 0.0, "avg_logprob": -0.10353165864944458, "compression_ratio": 1.6310679611650485, "no_speech_prob": 0.0016484307125210762}, {"id": 388, "seek": 175096, "start": 1750.96, "end": 1755.2, "text": " on the order of magnitude astros there, I think they're about a hundred times more efficient", "tokens": [50364, 322, 264, 1668, 295, 15668, 5357, 2635, 456, 11, 286, 519, 436, 434, 466, 257, 3262, 1413, 544, 7148, 50576], "temperature": 0.0, "avg_logprob": -0.09517008759254633, "compression_ratio": 1.7685185185185186, "no_speech_prob": 0.009707997553050518}, {"id": 389, "seek": 175096, "start": 1755.2, "end": 1759.92, "text": " than something like musli. So that alone would already be a huge win if it turned out to be true", "tokens": [50576, 813, 746, 411, 1038, 2081, 13, 407, 300, 3312, 576, 1217, 312, 257, 2603, 1942, 498, 309, 3574, 484, 281, 312, 2074, 50812], "temperature": 0.0, "avg_logprob": -0.09517008759254633, "compression_ratio": 1.7685185185185186, "no_speech_prob": 0.009707997553050518}, {"id": 390, "seek": 175096, "start": 1759.92, "end": 1764.56, "text": " in this case. And then there are a lot of other really great methods for self supervised learning", "tokens": [50812, 294, 341, 1389, 13, 400, 550, 456, 366, 257, 688, 295, 661, 534, 869, 7150, 337, 2698, 46533, 2539, 51044], "temperature": 0.0, "avg_logprob": -0.09517008759254633, "compression_ratio": 1.7685185185185186, "no_speech_prob": 0.009707997553050518}, {"id": 391, "seek": 175096, "start": 1764.56, "end": 1768.96, "text": " in computer vision that I would imagine could also come in handy. Continuing learning could also", "tokens": [51044, 294, 3820, 5201, 300, 286, 576, 3811, 727, 611, 808, 294, 13239, 13, 47585, 2539, 727, 611, 51264], "temperature": 0.0, "avg_logprob": -0.09517008759254633, "compression_ratio": 1.7685185185185186, "no_speech_prob": 0.009707997553050518}, {"id": 392, "seek": 175096, "start": 1768.96, "end": 1773.1200000000001, "text": " play a pretty big role here, especially when it comes to foundation model. The whole idea of", "tokens": [51264, 862, 257, 1238, 955, 3090, 510, 11, 2318, 562, 309, 1487, 281, 7030, 2316, 13, 440, 1379, 1558, 295, 51472], "temperature": 0.0, "avg_logprob": -0.09517008759254633, "compression_ratio": 1.7685185185185186, "no_speech_prob": 0.009707997553050518}, {"id": 393, "seek": 175096, "start": 1773.1200000000001, "end": 1777.92, "text": " foundation models is that you train them once, and then you fine tune them for many other tasks", "tokens": [51472, 7030, 5245, 307, 300, 291, 3847, 552, 1564, 11, 293, 550, 291, 2489, 10864, 552, 337, 867, 661, 9608, 51712], "temperature": 0.0, "avg_logprob": -0.09517008759254633, "compression_ratio": 1.7685185185185186, "no_speech_prob": 0.009707997553050518}, {"id": 394, "seek": 177792, "start": 1777.92, "end": 1782.5600000000002, "text": " afterwards. But neural networks have a plasticity problem. The more you train them, the worse they", "tokens": [50364, 10543, 13, 583, 18161, 9590, 362, 257, 5900, 507, 1154, 13, 440, 544, 291, 3847, 552, 11, 264, 5324, 436, 50596], "temperature": 0.0, "avg_logprob": -0.09050102520706062, "compression_ratio": 1.6384839650145773, "no_speech_prob": 0.06558921933174133}, {"id": 395, "seek": 177792, "start": 1782.5600000000002, "end": 1786.4, "text": " get at learning new tasks, which is obviously not ideal. You can imagine that this is something", "tokens": [50596, 483, 412, 2539, 777, 9608, 11, 597, 307, 2745, 406, 7157, 13, 509, 393, 3811, 300, 341, 307, 746, 50788], "temperature": 0.0, "avg_logprob": -0.09050102520706062, "compression_ratio": 1.6384839650145773, "no_speech_prob": 0.06558921933174133}, {"id": 396, "seek": 177792, "start": 1786.4, "end": 1790.24, "text": " we definitely want to avoid here. Maybe check out my video on continual backprop, if that's", "tokens": [50788, 321, 2138, 528, 281, 5042, 510, 13, 2704, 1520, 484, 452, 960, 322, 1421, 901, 646, 79, 1513, 11, 498, 300, 311, 50980], "temperature": 0.0, "avg_logprob": -0.09050102520706062, "compression_ratio": 1.6384839650145773, "no_speech_prob": 0.06558921933174133}, {"id": 397, "seek": 177792, "start": 1790.24, "end": 1793.8400000000001, "text": " something you're interested in. But overall, this is really great work. Kudos to the team for", "tokens": [50980, 746, 291, 434, 3102, 294, 13, 583, 4787, 11, 341, 307, 534, 869, 589, 13, 591, 35063, 281, 264, 1469, 337, 51160], "temperature": 0.0, "avg_logprob": -0.09050102520706062, "compression_ratio": 1.6384839650145773, "no_speech_prob": 0.06558921933174133}, {"id": 398, "seek": 177792, "start": 1793.8400000000001, "end": 1797.52, "text": " getting this working. I'm incredibly excited to see where it goes. If you've enjoyed this,", "tokens": [51160, 1242, 341, 1364, 13, 286, 478, 6252, 2919, 281, 536, 689, 309, 1709, 13, 759, 291, 600, 4626, 341, 11, 51344], "temperature": 0.0, "avg_logprob": -0.09050102520706062, "compression_ratio": 1.6384839650145773, "no_speech_prob": 0.06558921933174133}, {"id": 399, "seek": 177792, "start": 1797.52, "end": 1801.28, "text": " consider subscribing for more dagos and more machine learning stuff. Thank you so much for", "tokens": [51344, 1949, 19981, 337, 544, 15460, 329, 293, 544, 3479, 2539, 1507, 13, 1044, 291, 370, 709, 337, 51532], "temperature": 0.0, "avg_logprob": -0.09050102520706062, "compression_ratio": 1.6384839650145773, "no_speech_prob": 0.06558921933174133}, {"id": 400, "seek": 180128, "start": 1801.28, "end": 1803.28, "text": " watching, and I hope to catch you next time.", "tokens": [50368, 1976, 11, 293, 286, 1454, 281, 3745, 291, 958, 565, 13, 50464], "temperature": 0.0, "avg_logprob": -0.3860699449266706, "compression_ratio": 0.88, "no_speech_prob": 0.3735968768596649}], "language": "en"}