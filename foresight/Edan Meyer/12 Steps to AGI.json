{"text": " The title does not mislead you. This is indeed a 12-step plan. 12 steps right here. You can count them 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, any with 12. That starts from essentially basic machine learning and ends up with the goal of basically the singularity, what they call intelligence amplification. And gosh, was this a fun read and interesting to think about. You might be wondering what random dude drank a bit too much one night and decided to write down their brilliant plan for this. But as it turns out, this is not written by some random dude. Rather, this is a new paper by Rich Sutton, Michael Bulling, and Patrick Polarski, who are all well-established researchers that head the Alberta branch of D-Mind and are also professors in the area. So yeah, I guess this is kind of unofficially a D-Mind paper. And not only is this just some plan, it is now the plan for research at the Alberta branch of D-Mind. Hence why it's called, you know, the Alberta plan. And it's just out there. It has some, I think, really interesting ideas and no one's been talking about it. So today, I hope to take you through the motivation for this and the plan itself. It is very interesting in the sense that it's building back up from the foundations of machine learning and reinforcement learning. And while it is very similar in most ways, it's also somewhat contrarian to many of the approaches, especially in reinforcement learning that are popular today. I love this because if you're a new graduate student or you can't find a research topic or you just want to get into research, well, here's 12 research topics handed to you on a silver platter. You really can't ask for much more. And it's also just, I personally think, interesting to know what some prominent researchers are thinking about and how they're going about approaching these problems that lots of us are very interested in. One last thing I'll mention before we dive into this is I cover lots of big ideas on this channel to keep you up to date with what's going on in the field, but also smaller ideas kind of like this kind of go under the radar to introduce you to some new interesting ideas. So if that's the type of thing that interests you, sort of subscribing to the channel, it means a lot and it really does help out. Anyway, let's dive into this. In this paper starts, we're going to come back the first two paragraphs in a second. I just want to start with the third paragraph because it really explains the core of what's going on here, what they're thinking. So starting from here, following the Alberta plan, we seek to understand and create long lived computational agents that interact with a vastly more complex world and come to predict and control their sensory input signals and blah, blah, blah. They are as scalable as possible. If you've seen Rich Sutton's better lesson, you know, maybe they're starting to look a little familiar and they have to adapt and change the complexity, which means they have to continually learn. Big focus on this here. And then another thing they mentioned is they must plan with a learned model of the world. Now I highlighted these specific things, except for the word follow, but I highlighted these specific things because these I think are things that are very core to this paper. So long lived, right, and continuing learning. These are things that some people in our work on, but are not super focused on that are really the sort of at the forefront of this paper. That means that agents are going to have to be able to work with certain challenges that are present in continuing learning. For example, if you try and take a neural network and have it continue to learn on a constantly changing task, many people don't realize this, but what actually happens is neural networks get worse and worse at performing or essentially learning new things, kind of like the, what do you say, an old dog can't learn new tricks. There's actually some concept of that in I think machine learning that lots of people are unfamiliar with. So things like that, those are issues that might be important in this situation. Anything vastly more complex world predicting and controlling sensory inputs. Some of these things are very like what you would assume is common, but you'll see that everything works under these assumptions in this paper. And we'll get into some of the more details of why I think some of these, you know, approaching these types of problems is interesting. The one thing that does stick out here a little bit is I think planning with a learning model of the world, because the other things are problems, right? Sort of having to be long lived and continuing to learn these, you know, having a vastly more complex world, these are statements about the problem. Whereas playing with a model is like a specific thing they do. So I guess they just think it's that important, which I would agree. But anyway, just some details about, you know, sort of the logistics of this plan before we maybe get into it. This is a five to 10 year plan, as they mentioned here. They also, you know, some of this is very like, you know, we think this will greatly affect the economy or society or individual lives. I don't think we need to go through all the, all the philosophical stuff. We can kind of get into the meat of this. So the purpose of giving this plan, which they write about here is, as they say, it's twofold. The main one is, or one of the two is that they want to sort of give their vision for AI. Well, lots of people have been working on these problems, like continue learning. They think it should really be at the forefront. So some of those things are a little bit different. So this is their vision, what problems they think should be focused on. And then also laying out, of course, a research plan for the purpose, you know, of doing research. This is not a to-do list. This is not like a things they completed. This is a research plan. They, they, as they very much stay here, gaps and uncertainties, which are to be expected, right? When you're tackling these big problems, when you have literally 12 steps, of course, it might not be enough. Maybe some things aren't going to be necessary. Things are going to change throughout this plan. There are some key points or some important things that they think sort of differentiates this Alberta plan from lots of the work going on at other places. So that's a little bit what they talk about in this next section. Ah, the other thing in this next section is this diagram that reminds me, I forgot to say what RL is. So this is going to be just a five second explanation. If you don't know what reinforcement learning is, just imagine you have some environment like the world, you have an agent, you know, it sees things from the environment, gets observations, takes actions, and it tries to maximize some reward. There's your five second explanation. Hopefully that keyed anyone in it wasn't okay. Anyway, what are these core tenants that they think differentiate the Alberta plan? So the first one, here it is, they say the first distinguishing feature is the emphasis on ordinary experience as described above. So an ordinary experience, as they'll mention here, this means that you can't do things like use specialized training sets. You can't use human, like assistance, you know, you can't put human data into this, you can't access the internal structure of the world, which lots of work and reinforcement learning does, right? Not everything, but lots of it does. And I should say, maybe clarify, human assistance here does not mean that an AI cannot interact with humans. It just means that once like the learning process has started, you can't have a human like tweaking like different parameters and like, you know, maybe training a bit like this and then a bit like this. It means that the learning process, you kind of have this agent, it starts out and then it has to do everything by itself. And why is this so important? And the idea here is that these types of methods that require human assistance, they usually don't scale, where do they say this, they typically do not scale with computational resources, and as such is not a research priority. Now, if you've read the bitter, the bitter lesson, that's basically the, you know, this is the one paragraph version of this. So what's the next thing that they put a focus on? The next thing is temporal uniformity. What does this mean? Essentially, it means that at all time steps, so at all times, in terms of the algorithms running, everything needs to be the same. You can't have, and this again, goes back to what I had mentioned earlier, you can't have a separate set of training data and testing data like you would have in supervised learning, or lots of people when they're evaluating reinforcement learning agents, like on the Atari 100k benchmark, I think it's pretty common to train with 100,000 steps of data, but then the actual testing is done on different data. So that would be a no, no here, right? And you might be like, oh, that's heresy, no training and testing, no separation. Well, yeah, but you have to remember, we're working in the continual learning setting, which means that you can just keep simulating your environment and getting new experiences. And well, if all those experiences are the same, well, then maybe your environment just isn't a very interesting environment at that point. So this is essentially talked about in the paper here, you know, they say no special training periods, if information is provided, then it has to be the same information on every time step. Or if it learns to plan, then it has to plan on every time step. If the agent constructs its own representations or subtasks, then the meta learned out are the meta algorithms for constructing them and operating them operate on every time step. So you might be wondering like, well, aren't on some time steps, maybe where things are more complicated, won't we maybe want to plan more like look further into the future? Yeah, maybe we'll want to do that. But the idea here is that should all be internal to the agent, as opposed for a human tweaking these parameters, right? You could have a meta learning algorithm that learns those types of things. And in general, I think this, why do they do this? I think they actually say it fairly well. Their focus on this is to leads us to interesting non stationary continuing environments, and in algorithms for continual learning and meta learning. So in one sense, I understand totally how this leads them. I have way too much yellow on this page, I'll start using something else. So algorithms for meta learning, this makes sense as I just mentioned, right? You need meta learning when you can't have a human tweaking these things, you should have an algorithm to that and meta learning is one way to do that. On the other hand, I don't see how this really leads to continuing environments or like continual learning, I would think it would be the other way around, right? You have continually like continual learning. And hence, you're going to want to focus on temporal uniformity, but maybe there's something I'm not seeing here. And another reason they mentioned this is that keeping everything temporarily uniform, it reduces the degrees of freedom and shrinks the agent design space, which to be fair, they think there's a decent point, really have plenty of things to look into. There's no reason to, you know, forcibly make that higher when we could simplify the space. Then if we go down a bit more, this is one thing we touched on earlier. And this is also from the bitter lesson. But the third distinguishing factor is it's cognizance of computational considerations and Moore's law. I do think lots of other, and this essentially, all this means, right, is compute computations getting stronger or more, we have more of it, it's more efficient. So we want to be able to take, make use of that. So we can't have anything that's not going to scale. And I think this has been getting better over the years, my personal opinion, it looks like lots of people are working in areas that do or on methods that do scale fairly well. Deep learning is one great example of this rate. And basically, everyone's using deep learning now. Oh, they even namedropped the bitter lesson. Okay, lovely. So then let's move on to the fourth. And I think this is the final one. The fourth distinguishing feature is the focus on the special case, or is it the special case in which the environment includes other intelligent agents. Now, what does this mean? It doesn't mean that there need to be other intelligent agents, but rather that intelligent agents. So like if I'm, you know, modeling a robot that's interacting with humans, you know, there's, there's nothing fancy that I do to model the fact that there's humans in the world, rather, they're just part of the environment. And I need to be cognizant, and I think they say this right here, you'd be cognizant that the environment may behave differently. And this might sound weird, but the environment is essentially, you can think of it as, you know, a living, breathing thing, or at least it could be in response to your own actions, right? So what you do might make the environment behave differently. And that's, you know, we shouldn't do anything special for these other agents. To be honest, I don't think they even need to put this here. Because I think this follows under the bullet point of using no human experience, right? The idea that we have other intelligent or like conscious agents, that is something that we're sort of embedding our human biases into this, right? Technically, we can't even prove other people are conscious. So I don't know, I don't think this is so weird, but I guess lots of places that do work with multi-agent settings, they do do this sort of thing where they're explicitly trying to model other agents. So that would be a no-no in this sort of setting, or at least that's, that's what they want to work with. So if we scroll down, we're almost to the research plan. I swear we're getting there. We're going to spend a good bit of time on that once we get there. But the last thing we need to touch on is the base agent. So I believe this is what they call the common model of the intelligent agent, as you're looking at right here. And they say that this is used in a few things, like areas like economics have similar ideas. But the idea is that we want to start from some base that we can maybe agree on, not that it's necessarily the best or something like that, but that this is a reasonable base that will inform the rest of the research. So what's essentially happening here is if we step through this, we start with an observation from the environment, pretty standard, then it goes into this perception. So perception, and this is going to be a recurrent thing, right? You also take in the last action, and you can see the output of perception feeds back into here. So this is recurrent. So this would be keeping what they like to call the agent state, which is like the agents, some people call this the belief state. It's maybe what the agent perceives and wants to remember about the world to do whatever it needs to do. It has reactive policies. So the policies are what transform these observations into actions. It has a value function. This is a normal value function in reinforcement learning, right? It essentially assigns value to certain states, which helps you do things like credit assignment, and it has a transition model, which allows you to do, as you can see, planning and planning means that you can more efficiently, you know, imagine scenarios in your head, not have to play through them in real life. And then that means that you can be a lot more sample efficient and potentially have other benefits too. One question, and this is a question I used to have, is why have these things? They are, they're one, two, three, four things. Why not other things, right? We could have, for example, a different portion here saying we want to model like other agents. I mean, we just said like this, this is not good, we shouldn't do this, but why not? Why are these other things okay to have? And the difference here, I don't think they mentioned the paper. But the difference, I believe they would say, is that these four things that they have here, they're not domain specific. These should essentially work in any domain, they're general, they're very general, whereas modeling other agents, well, you might not always need to do this in the way in which you might do this will probably change drastically, depending on what types of agents you're working with. So for that reason, these four things stay and other things don't quite make the cut. It has to do with the generality of these four components. Another thing you might mention is that the year, there are s's right here. So reactive policies, value functions, that is something we'll get into. These don't necessarily need to have one policy or one value function, we can work with multiple. And that is one of the steps of the plan that we'll get into. Speaking of the plan, we are finally about there. So here we go. A roadmap to an AI prototype. So here are 12 steps. I'm going to start off by just reading through them. And then they have bullet points for each of these that we can go through and talk about all of them and more depth. Some of them I'll go over somewhat quickly. Some of them I'll go over in a bit more depth because they have more written or I think they're more interesting. But anyway, starting off with item number one, we have continual supervised learning with given features, then two, supervised feature finding, three, continual generalized value function prediction learning. If you don't know what these words mean, don't worry, I'll go over all of it as we go through the individual points later. And four, continual active credit control, five, average reward, GVF learning, six, continuing control problems, seven, planning with average reward, eight, prototype AI one. Wow, incredible. One step model based around with continual function approximation, nine, search control and exploration, 10, the stomp progression, 11, oak and 12, intelligence amplification, or this is, I guess you could call this the singularity, though to be fair, I guess in making AIs make themselves better does not necessarily guarantee the singularity. So maybe that's a bit of a misnomer, but you know, it's a bit more catchy. It's more of a buzzword, I guess. So you might notice that looking over these points, Hey, this is just normal reinforcement learning. And to some extent, you wouldn't be entirely wrong. These are very common things that we see in reinforcement learning, like actor credit control. What's what's another thing model based around with continual function approximation, like what this is supposed to be a generally I that can do all these things. And that's what I meant when I was talking about at the beginning. These are very similar to what lots of people are doing today. But what you'll notice as we go through the points is that sort of the focus on the importance of what's important, that's what I think is a bit different, like the importance, for example, on continual RL. So we'll get into this, this, this, I should say, this isn't anything groundbreaking. It's more of just an interesting plan and an interesting way to go about this or to think about going about it this way. So let's start with our first big step, which is continual supervised learning with given features. So what let's start out with what this means. This essentially means given features. So maybe you have cart pull, and cart pull is a problem by the way, where you have this little cart, and it has a little pole on top of it. And the pole can go back and forth. And you need to move the cart back and forth to bounce the pole, very simple problem. So maybe for your features, you could have like the x, y values of this, and then the angular velocity of this cart, or something like that. So those would be given features. And to point out, you know, yes, we can already do things like this. But sort of the point I think of this plan is to really revisit things in the simplest setting. So they split the explicitly say this, we want to go back to the simplest setting, and essentially try and make everything as good as possible. Essentially try out everything that might have been overlooked, especially in these new settings in the setting of continual learning, and say, can we do better? So some things they mentioned that they might look into are how can we make things quicker? How can we train faster, be more robust, be more efficient, while also continuing again, over long periods of time, how can we do things like meta learn better representations? And that's be the most efficient. So these are all questions that that would be asked, although I guess meta learning better representations. I'm not sure if that's, I would think that would be in a different step. But, but anyway, what are some examples of some ways, you know, you could look into this, like, what are like, what can we really change when we have, you would think this is such a simple setting, there's not really much we can do, but there actually are some things we can look at. So one thing they mentioned is the global step size. So this is like your learning rate. Generally, you have a global learning rate. Now, this is somewhat, but certain optimizers, like Adam, that people have, some people have argued, I'm not sure, I don't know really the ins and outs of this, but they've argued that those sorts of optimizers, while they're really good for supervised learning, maybe aren't the best fit for reinforcement learning, because for reinforcement learning, maybe you want to have different learning rates, depending on how important certain features are for a certain task, or like what, you know, if you're trying to do different sorts of tasks, there's different things you could think of. But this is like one thing you could look at, right? Like, can you have a different step size for each single parameter? And how could you do that in a way that makes sense for continual reinforcement learning and not, you know, supervised learning, which is the context that most of these methods have been developed in. And of course, I think I'm not going to find it right now in this, this C of text, but they also mentioned that, you know, you don't want a human to be setting the learning rate. These are things that should be meta learned. Again, right? We don't want really too much human experience. We don't want that much fine tuning. We want the agent to be able to really do everything for itself. So these are all things that yes, people are looking into them. But the idea here, again, is go back to the simplest example, and get something, something working as best as possible. And then we'll start to scale up and make things more complicated. So some other things they mentioned, again, it's a bit too much here. Oh, here it is. So there's like normalizations of features, right? Like, of course, we know that you can normalize features. And that tends to be a good thing in machine learning. But when you're doing continual learning and reinforcement learning, does anything change? You know, these are all things that could be reconsidered in this slightly different context to maybe squeeze the most out of this that you can. So they go into this in a bit of depth. Honestly, I'm going to skip over this because I don't think it's particularly important. There's talking about examples of ways, you know, you can think about this, for example, like the meta learning per weight step parameter. That's something I just mentioned. I think they talk about this more in depth for this step, maybe because they already have some papers out about this. But anyway, let's get on to step two. So this is supervised feature finding. So before we had the features given to us, but now we actually want to find them, we want to generate new features, right? And features, right, these are just, we get some observation, we have our perception, remember from the base agent, now we want to use those to create new features to essentially use things that will serve representations that will help us do whatever tasks we're working on. Now, this is generally done via back propagation. And sorry, if you don't know what back prop is, but it's a bit too much to explain this video, there's plenty of good things. There's out there explaining it. And back prop, one issue actually with back prop is lots of people aren't aware of this. It doesn't work super well for continual learning. I mean, maybe not super well as in an overstatement, it does work. And it works fairly well, but it has an issue. It has an issue that the more you train, if you train on and on and on and on and you just keep going, it actually sort of does what they call sat and it gets saturated. I think they call like interridger saturation. Individual nodes get saturated, which means they stop essentially contributing. And over time, what you'll find is you'll see that maybe the performance of your agent, it starts off low, it goes up and it gets good and it kind of blows out. Well, if the task keeps changing, what you'll find is that the performance slowly starts to drop as the agent is unable to adapt because the network has essentially gotten saturated. Or at least, that's one explanation. I guess it's not fully explained yet. I actually explain this phenomenon as well with there's a method called continue backprop or CBP for short. And this is from Rich's lab, where they work on this problem. So this is one example of how supervised feature finding actually needs to be adjusted to the given scenario, which is continual learning. And this is one example of how although we have something that's really good already, we have back propagation, it might not be the best thing to use, at least not this exact instance we use of it right now, might not be the best thing for continual RL or something like that. One other thing that I think is really interesting, and I found this to be always very enticing, but sort of dangerous. I'm not dangerous and literally dangerous, but like dangerously enticing part of the work that like Rich's lab does is the way they think about modeling, like the typical modeling paradigm, their perspective is quite a bit different. So let me describe what I sort of think of as the typical paradigm. So maybe I'll do that on the left here. So I think the typical paradigm of, and this is not what all papers are about, but lots of them, one, you want to pick an architecture, right? You have some problems. So you want to pick the architecture you're going to use. This could be like a ResNet, right? You can use different ResNet layers. Maybe you want to use a transformer. Maybe you want to use some sort of self attention. There's lots of different architectures you could choose, right? You choose an architecture and then two, you choose an objective function. So this objective function is going to be like your loss. Like what are you trying to optimize here, especially in reinforcement learning, maybe this is usually going to be the reward or the value or whatever. But this could be a number of things. It could be the MSC with like an image. If you're trying to recreate an image or something like that. And then the third and last thing you do is you pick an optimizer. So 99% of the time, this is going to be like Adam or it's going to be RMS prop. But the thing is the way I find that within this paper, they talk about these sorts of things and reading other papers from the same people, they tend to think about these things a bit differently. Rather, they look at it from like a perspective of looking at individual neurons and the interaction between these neurons. And what do I mean by this? Because technically, like, you know, we're ending up using lots of the same things, but it is really just a difference in perspective. So for example, we can say, how can the utility be assigned to all the features that we're using? And then how could we make use of these utilities in the future, right? So this is like a very much we're looking at a neuron neuron basis, trying to find the utility, why might we want to do this sort of thing? Well, this could help us do things like evaluating existent features and discarding less promising features, so as to make room for new ones. Well, why don't we just keep expanding? Well, we have like a limited amount of computation here, right? Even though we are constantly getting more, this is where like the big world hypothesis, as they call it comes in, even though we have more and more compute, we also have just always a bigger world so that we'll never be able to match it. So we are going to have to forget things, right? So what do we forget? And that's kind of what this whole CBP thing I was just talking about. This does something very similar to this. But beyond just dropping features, there are also other things they talk about, you know, like initializing features. This is something you have to do. Normally, you do it when you create your own network, you initialize all your features, and then you just sort of train forever, right? But as it turns out, initialization, although it's kind of brushed to the side, it's actually much more important than you might think. Oftentimes, maybe not often, but occasionally, I'll implement a paper. And I've had this happen to me a couple of times, we'll implement it, and it won't work. And I'll go back and read the paper very carefully and realize, oh, they use this very specific form of initialization for their network, and they try it, and suddenly everything's working. This has happened to me, especially in papers that don't use backprop, but like use biologically inspired training. I found that this is actually more important than you might expect. Another thing to talk about is like, how should you adjust neurons or weights? One way to do this is obviously backprop. And I don't think they're, they're definitely not proposing we just get rid of backprop. I think they realize backprop scales very well. It works very well, but it might not be perfectly suited for a problem. So how can we look at this sort of more, I don't know if I want to call like an individual neuron approach, but something like that, more than neuron to neuron level, looking at them as features, features with utilities and features that we want to drop or get rid of. How can we implement like all these sorts of things? Like what, what would this look like in, in play? So maybe one example of this, right? And this is very similar to what CBP does is say one, we would do an initialization just as normal. We in it all our, our nodes, but this is one thing we could look into what are better ways to in it for these types of things. The second thing we might want to do is use, use backprop, right? We don't need to get rid of backprop. Backprop is strong, but as we use backprop, maybe then we evaluate the utility. And then four, we remove, or maybe I should just say we drop the nodes that are not super useful. And then the fifth thing we could do is we could reinit, but instead of just re-init normally, maybe we could have some sort of meta learning that figures out how to initialize nodes such that things will be learned faster, right? So there's, there's some sort of meta stuff you could do here. So this would be one example. And again, this is very similar. That's done what's done in CBP. I'll link my video to this in the description. Actually, if you're interested in checking out, but this is like one alternative, right? That still makes use of backprop, but might be better suited for something like continue learning or reinforcement learning. And could give you more control over how you go about developing these algorithms. As I just mentioned, I think this is a very exciting way to think about how to model things, thinking of individual features. It's almost less limiting the standard approach that I read out right here. But at the same time, it is dangerous because let's, let's not kid ourselves, back propagation works very well. It scales very well. So when you do want to do something different, you're going up against something that's already very well established and is known to work very well. So that's why I say it's kind of dangerous. It's, it's exciting, but hard to get working and not very explorative. It's a very underexplored area, I think. Okay, step number three, continue GVF prediction learning. This is where we start getting into non-IID data. So that's independent, individually distributed data. So when we're working with reinforcement learning or in the real world, we are often, we're going to be working with streams of data that come, you know, in, in a row. But one thing to note is that many machine learning methods, the theory is entirely based upon data that is IID. Or in other words, we are essentially breaking that when we move on to reinforcement learning most of the time. That's why people like to use, or one of the reasons people like to use things like replay buffers is because it helps you get this IID distribution, which can help you learn. Now, we have methods of somewhat counteracting this, but it is nevertheless still an issue. But one thing they also put in here is GVFs. I honestly don't know why they put this in here, but we can talk about it because they do. I just don't know why they put them together. So GVFs, this is generalized value function. And this is a fairly simple idea. If you're familiar with normal value functions, essentially a value function measures like how good a state is, like what the expected reward is. Whereas a generalized value function, well, it's essentially just a value function for something other than the reward, or it could be the reward, or any other feature, right? So you're predicting something. So they're just predictions or predictions about the world that are not reward based, not necessarily reward based. The last thing I'll mention about this step three is this is where perception comes in. And I think the reason they say this is where perception really starts to come in is because of the non IID data. That means if we have like sequential data, and we need to remember stuff that happened in the past, well, part of our perception needs to be remembering the important thing we need to know what's important, like if there's a key over there that I can't see anymore, I need to remember it's there so I can, you know, go back and get it if I see a locked door or something like that. The fourth thing we've been on now is continual actor critic control. And that is right up until this step. You might not have noticed there was no control involved. We're not actually interacting with the environment, but rather up to this point, we were just predicting what was happening. There's really not much more to say here. Now that we're bringing control in the mix, we need to figure out how to get that working with everything we've done so far. So then step five, average reward GVF learning. Now, average reward is I think something lots of people are not very familiar with. Understandably, so it's it's not very popular, I guess you could say. And if you've never thought about it, using something like a discount factor and for those that aren't familiar with reinforcement learning, usually you have this gamma parameter. And this is called the discount factor. And essentially it weighs how important current rewards are versus future rewards. And it's, if you think about it, it's kind of weird. The other thing that we could do, as you see right here is just have an average reward and say we want to maximize the average reward, which would essentially be in the same as saying we just want to maximize the reward total over everything. The thing is, when you do something like having gamma, you're really, I actually don't want to get into this too much. But if you've ever thought about it, it's kind of janky, right? And the argument for why to use average reward instead is not just the jankiness, but I don't want to get into it too much. So if you're interested, there is a whole section on this in the RL book that you can check out. So step six, and this is going to be kind of cut us off at the first half of this plan is the continuing control problems. Essentially, all this is going to say is we've essentially created up until this point, a, a model free RL agent. So essentially the point for this step is really, we just want to combine everything we've learned of the average reward stuff, the generating new feature stuff. And we want to try it out in a bunch of continuing environments, maybe make some more like new continuing environments, because most environments are episodic, like the open AI gym stuff they mentioned, it's mostly episodic, but you could convert it to continuing versions. And then we want to try the combination of everything we've worked for so far and see how we're stacking up. Because remember, this is not like a one shot plan, it will need to be adjusted as you go. So this would be a good point maybe to revise, see how things are doing, a workout, anything that's maybe not working as expected. So then when we're not going to jump just to the seventh step, this, this is kind of wraps up the first part of this. Once we have this done, we have a, I love how they say it, a more continual, true, it's a more continual model free learning method, which is good. We have like our first leg base. This is where I guess after this point, we can say we have something good now. Now it's time to start making it better. So what they focus on from here is primarily model based RL and things start to get a little bit more complicated. Ideas, I will say from the seventh step onward, tend to also be a little bit higher level without as many details, which I think is because, you know, they're later in the plan less certain about how these things will go because they're further out. So step seven is planning with average reward. So planning is very much done in RL today. And there are lots of great examples of this. For example, you can look at Mu zero. Mu zero is a great example of planning where the results are pretty incredible. There are some differences here, right? It will again be in the continual setting will want to do this with average reward. And I can say just right now, there are a lot of problems in planning that are still unsolved. And it's very like not clear how to do things like what's what's the best way of going about things. You also have like other alternatives to Mu zero, like dreamer dreamer is dreamer v2 also works very well, you know, what's what's the best way to go about planning in this setting? Who knows. So then step eight is prototype AI one. I love that. I got to love that prototype AI one one step model based RL with continual function approximation. So this is actually learning a model now, right? You can do planning if you have a pre given model, which I think is what they entailed for step seven. But now that we have model based RL, that means we're going to want to learn a model and then maybe do some sort of planning or something like that within that model. So a one step planning model, they actually give a few steps down here that we can talk about. So some things they want it to include. So one is a recursive update function or a perception process. So right, we had that and this essentially makes everything more efficient. Instead of having to like relearn the perception and the value function, the world model, the policy can have one shared perception function that learns some sort of useful representation and knows how to store memory of things that are important and that sort of thing. We also need a one step environment model. They say this presumably be an expectation model or sample model or something in between. I'm pretty sure it would not end up being an expectation model. This is I'm saying this because this is what I do in my own work and expectation models are kind of awful. They're really easy to learn, but they're they're not very good. So sample models are one thing they don't mention a distribution model here. But I think that's another possibility, although there's no reason to think a sample model couldn't also work. So and by the way, if you're unfamiliar with this, this is just the idea that we want to predict the next step, what's going to happen next, right? Because if we can predict what's going to happen next, then we can update based on our, you know, sort of visions in our head, like our it's kind of like humans. I think when you go to sleep, something like this happens, right? You get replayed memories and those can help you learn. So so that's kind of what happens with world modeling and planning. What else is happening? So feature finding as in step two, then importance feedback from the model. So this is now essentially tying steps to and this planning together, right? If we're doing plain, we should be able to use that plane to go back and change our actual features or how we're learning our features and improve them in some way. The other idea, sorry, not other idea, but the final step here is a ranking of features used for both feature finding and to determine which features are included in the environment model. Since step D is essentially ranking features for feature finding, determining what features are used in the environment model, because we might not need to use everything. If you've looked at like what's been going on with this recently, instead of actually trying to predict the future, lots of people are doing what's it called value equivalent models, where it doesn't actually matter what the model predicts, so long as it's getting the right value. So it might be predicting, you know, it might be leaving things out that are not important or stuff like that. And then an influence of model learning and planning on feature ranking. So I think they just kind of stated this. But anyway, I don't want to go maybe through these individual points. They're kind of weird. Maybe tie in all these together. What are they talking about, especially in these later few points? And the idea for this step eight is I think really that they want to bring a full integration circle between the policy value function and the model components, right? The idea that whenever you're doing any one of these things, when you're learning a value function, when you're learning how to plan, the planning should essentially help you make better features, which should then in turn help you make a better model. So all these different components, and there's too many, there would be too many errors to draw this out in the like common model we saw before. But the idea is that all these different parts should be affecting each other. And I do think that this is one interesting thing that is not like too out there at all, but it's one thing that is not fully done. Now in things like Mu zero, what they essentially do is they they have their input state, they pass this through like a representation function. So they get there, maybe I should call this the observation, this is like the state, then they do like the planning. So this is called like the dynamics model to get the next state. And then from here, you go up and you predict the policy and the value function. And then, and they do some Monte Carlo tree. So it gets very complicated, right? But you have this whole system, and it's trained from end to end. So when you actually train things go back like this, and you sort of update everything in one go. So this is one type of feedback, but it's far from the only type of feedback that could be used, right? There could be other types of feedback between these different components. Maybe again, these are all things that could potentially be meta learned to. And those other types of feedbacks might help these systems be more efficient, which would certainly always be great. We head down to step nine, we have search control, and exploration, just to sum this up this, there's not too much in this. But essentially, the idea here is that we want to make things more efficient and a bit better when we're doing search. So search would be done in something like planning, right? We're trying to look at the future, see what could happen. We want to see, like, you know, what, if we take this action, what are all the different things that could happen? Or also when we're updating over, like, you know, maybe we want to explore in a certain way that helps us learn what we're missing, like fill out the gaps in our knowledge. So different ways you could, you know, different things you could look into or like prioritize sweeping is one, that's where you, I guess, like look at certain states where you have the least knowledge of what will happen. But there's also a difference between, like, you know, instead of using Monte Carlo tree search, there's also different types of heuristic search. Now, I don't think they would just use heuristic search, because that seems like it uses a bit too much human experience. But perhaps the model could, could meta learn, I say meta learn too much, it could learn some heuristics for this, right? And that would be another way to go about planning. So that's what that is all about. And next is the stump progression. So stump, I believe, do they write it here? Yes, they do. So it stands for subtask, option, model, and planning. We've already talked about most of these, you can probably infer what a subtask is, right? It's the idea that you might want to have other tasks, other than the main task, the agent can, can work on. And of course, the agent would learn these tasks themselves, as opposed to like a human, a human doing these. So this is like one, one differentiation, one point they mentioned earlier, right? We don't want, and this is what a lot of people do now is if they want to learn like skills. So a skill is maybe, I don't know, you're going to drive a car and one skill is like turning the handle left or right. You have to move a lot of muscles in your arms. So if you have the steering wheel, how does, how do these normally look? I don't think this is close enough. If you want to move this left and right, you actually have to move a lot of muscles in your arm to get that to work, right? But for humans, it's very easy, because we've, we essentially know how to move our arms in certain ways. We have skills or options are the more general form of this. That's when option is in a subtask. If I'm learning how to live a good life, learning to drive is going to be one subtask. So that's important for me to master, right? So how can the agent pose its own subtasks and how can it learn options that solve those subtasks? And then how can we combine those? Or sorry, I'm getting a bit ahead of myself. Combining them is the next step. What they essentially propose, I'm not sure, I forget if they propose it here, but the idea is that they want to have GVFs. So they want to be predicting things or have certain features, right? So feature one, feature two, feature three, that tells things about the environment. And then they want to pose GVFs. So being able to predict these things and also control them. So say, how can we maximize feature one? How can we maximize feature two or feature three, and then learn options for each of those. So essentially what you're doing is you're learning how to control your environment in ways that are not just trying to maximize reward. And then maybe these options that you've learned could be reused later. They actually have a paper on this called, I think, Stomp or something like that. So if you're interested in that, I do encourage you to check that out. So then step 11 is oak. So oak is, it stands for another thing. I get, I'm forgetting, I think options, action knowledge or something like that. That might not be exactly right here. They actually introduce option keyboard. Now I'm surprised they mentioned this because the other steps don't really go in depth. Whereas the options keyboard is a very specific thing. There's a paper out about it. They link to it. I think it's a really interesting paper. And the idea is that let's say you have a set of options. So maybe option number one knows how to fish. And option number two knows how to use a computer. Now, I don't know why you would ever want to do these two things at, at the same time, but I know I hate fishing and I find it incredibly boring. So if I was going to fish and I had a computer, well, honestly, I'd probably enjoy the nature, but you know, you could do both at one time, right? There's no reason to say you can only do one fishing or using your computer. Choose you could do both at the same time. If you want to weather or not, it's a good idea. So and that's the idea of an option keyboard where you can essentially specify how much you want to do this option or that option. And instead of having to learn a billion different options to do all the different things you do, well, if you learn a good set of base options, now you can combine them and get massively more expressive options that are just combinations of others instead of having to learn those explicitly. And then we get to the last step, intelligence amplification on how far we've come. So intelligence amplification is I think what most people think of when they think of the singularity. It's essentially the idea that now we have our prototype AI number two, and it should be able to do things very well. They describe this in some interesting ways. So like there's an exo cerebellum, which is, you know, they talk about these things, but essentially the core of what they're getting at is really at the end here, where it says an intelligent application agent to perform policies and use planning to multiplicatively enhance the intelligence of another partner agent or part of a single agent, or I guess they could also change themselves, right? So we see these two versions being studied in both human agents and agent to agent interaction settings. So essentially the idea that you have this one agent right here, and then you have agent, agent two right here. And maybe agent two is as much bigger brain, much smarter, you know, great brain drawing for me. So what agent two can do is it can go to agent one and say, I'm gonna make you smarter. And then it makes agent one better because remember it's a machine, it can edit its code. And then agent one is like, oh, thanks for making me smarter. Now I'm going to go make you smarter. Or of course, maybe you could just have a single agent, reperforming this on itself, or maybe there's some risk there because it could mess itself up. Maybe that's why they talk about having two different agents. But anyway, I mean, this is the idea, right, of how you get better and better agents. At some point, you have an agent that is just better than humans at producing these sorts of, you know, AI agents. And that's when we can get this sort of multiplicative scaling. Now, I think if you're like me, you might be thinking, wait, wait, step 11 was talking about an options keeper. And in step 12, we're talking about the singularity. Yeah, it's a bit of a jump, right? It's a bit weird. But that's one things I'm going to be honest, I find interesting, but I'm uncertain about in this paper. The fact that everything they outline up to step 11, for the most part, is within very reasonable expectations. It's like what you would expect, but with a different focus. What I find interesting is that they think that we can go from step 11 to step 12. I'm not sure how much effort they think it will take, but essentially, that they think not much will be missing at that point. And it is interesting to think about. On one hand, I'm very inclined to say, no, of course, that's not going to be enough. We already have most of the things they talked about in these previous steps. But on the other hand, we don't actually have the things they mentioned in these previous steps. We have usually, for lots of these, we have specific instances, right? So for like planning, we have something like Mu zero, but we still have so much more planning to explore. There's so many different things we could try. And if everything below the planning level, like if if we have incredibly good ways to learn representations, and if we have incredibly more efficient ways to train your own networks for continuing learning problems, maybe things will go a lot better. It's really hard to say. And to be honest, I would cut the authors of this some slack. I don't think, you know, they don't think that they're just going to go through this plan and suddenly hit step 12 and everything's going to work out. I think they're probably, they probably realize, you know, they have to revise this. I think they even mentioned, yeah, this is provisional. Oh, not crossing this out. It's not not provisional. It is provisional. It's a draft and a working plan. It's going to be revised. But I do think it's interesting that someone like Rich Sutton, who's worked in the field for a long time, has had some really good ideas, thinks that this will be enough. And to be honest, I mean, I don't think I could come up with a better plan myself. And I'm not sure quite what's missing here. I guess what's missing are obviously the details that you have to fill out, right? Like meta learning, there's a million bazillion ways to do meta learning. How are they going to do it? What's the way to, I don't know, who knows? And that's, that's the thing, right? This is at the end of the day, it's a research plan. It talks about the things they want to focus on, not how to do them. And it is very vague in that sense. So overall, those are kind of my thoughts. I really like this. I think it's interesting to read. I think it's very familiar, well, at the same time being somewhat fairly different from what people, it's what, or rather, I should say, it's almost the same thing that people are working on, but with a different problem in mind, different sort of problem setting. And I think interesting differences could arise from that. I could certainly see people having a wide variety of reactions to this. Some people saying this is completely useless. It's not detailed enough at all. I could see other people saying, Oh, this is very interesting. I could see some people saying, Oh, this is, this is the next step in the future. I really don't know. I'm very curious. So let me know what you think in the comments. I'm, I think we'll have some very different opinions. I am excited to say, if you've enjoyed this, do consider subscribing to the channel, maybe check it out some of my other videos. I would really appreciate it. It really helps out. And hopefully you'll find some other interesting content. Anyway, thank you so much for joining me and I hope to catch you next time.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 3.52, "text": " The title does not mislead you. This is indeed a 12-step plan.", "tokens": [50364, 440, 4876, 775, 406, 3346, 306, 345, 291, 13, 639, 307, 6451, 257, 2272, 12, 16792, 1393, 13, 50540], "temperature": 0.0, "avg_logprob": -0.13030118750245778, "compression_ratio": 1.636094674556213, "no_speech_prob": 0.04019632190465927}, {"id": 1, "seek": 0, "start": 3.52, "end": 8.96, "text": " 12 steps right here. You can count them 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, any with 12.", "tokens": [50540, 2272, 4439, 558, 510, 13, 509, 393, 1207, 552, 502, 11, 568, 11, 805, 11, 1017, 11, 1025, 11, 1386, 11, 1614, 11, 1649, 11, 1722, 11, 1266, 11, 2975, 11, 604, 365, 2272, 13, 50812], "temperature": 0.0, "avg_logprob": -0.13030118750245778, "compression_ratio": 1.636094674556213, "no_speech_prob": 0.04019632190465927}, {"id": 2, "seek": 0, "start": 8.96, "end": 12.96, "text": " That starts from essentially basic machine learning and ends up with the goal of", "tokens": [50812, 663, 3719, 490, 4476, 3875, 3479, 2539, 293, 5314, 493, 365, 264, 3387, 295, 51012], "temperature": 0.0, "avg_logprob": -0.13030118750245778, "compression_ratio": 1.636094674556213, "no_speech_prob": 0.04019632190465927}, {"id": 3, "seek": 0, "start": 13.52, "end": 16.56, "text": " basically the singularity, what they call intelligence amplification.", "tokens": [51040, 1936, 264, 20010, 507, 11, 437, 436, 818, 7599, 9731, 3774, 13, 51192], "temperature": 0.0, "avg_logprob": -0.13030118750245778, "compression_ratio": 1.636094674556213, "no_speech_prob": 0.04019632190465927}, {"id": 4, "seek": 0, "start": 16.56, "end": 19.76, "text": " And gosh, was this a fun read and interesting to think about.", "tokens": [51192, 400, 6502, 11, 390, 341, 257, 1019, 1401, 293, 1880, 281, 519, 466, 13, 51352], "temperature": 0.0, "avg_logprob": -0.13030118750245778, "compression_ratio": 1.636094674556213, "no_speech_prob": 0.04019632190465927}, {"id": 5, "seek": 0, "start": 19.76, "end": 23.76, "text": " You might be wondering what random dude drank a bit too much one night and decided to write", "tokens": [51352, 509, 1062, 312, 6359, 437, 4974, 6449, 21011, 257, 857, 886, 709, 472, 1818, 293, 3047, 281, 2464, 51552], "temperature": 0.0, "avg_logprob": -0.13030118750245778, "compression_ratio": 1.636094674556213, "no_speech_prob": 0.04019632190465927}, {"id": 6, "seek": 0, "start": 23.76, "end": 28.16, "text": " down their brilliant plan for this. But as it turns out, this is not written by some random dude.", "tokens": [51552, 760, 641, 10248, 1393, 337, 341, 13, 583, 382, 309, 4523, 484, 11, 341, 307, 406, 3720, 538, 512, 4974, 6449, 13, 51772], "temperature": 0.0, "avg_logprob": -0.13030118750245778, "compression_ratio": 1.636094674556213, "no_speech_prob": 0.04019632190465927}, {"id": 7, "seek": 2816, "start": 28.16, "end": 32.56, "text": " Rather, this is a new paper by Rich Sutton, Michael Bulling, and Patrick Polarski,", "tokens": [50364, 16571, 11, 341, 307, 257, 777, 3035, 538, 6781, 40492, 1756, 11, 5116, 14131, 278, 11, 293, 13980, 3635, 685, 2984, 11, 50584], "temperature": 0.0, "avg_logprob": -0.09785216926728318, "compression_ratio": 1.7161290322580645, "no_speech_prob": 0.013221504166722298}, {"id": 8, "seek": 2816, "start": 32.56, "end": 36.64, "text": " who are all well-established researchers that head the Alberta branch of D-Mind and", "tokens": [50584, 567, 366, 439, 731, 12, 33542, 4173, 10309, 300, 1378, 264, 43279, 9819, 295, 413, 12, 44, 471, 293, 50788], "temperature": 0.0, "avg_logprob": -0.09785216926728318, "compression_ratio": 1.7161290322580645, "no_speech_prob": 0.013221504166722298}, {"id": 9, "seek": 2816, "start": 36.64, "end": 41.76, "text": " are also professors in the area. So yeah, I guess this is kind of unofficially a D-Mind paper.", "tokens": [50788, 366, 611, 15924, 294, 264, 1859, 13, 407, 1338, 11, 286, 2041, 341, 307, 733, 295, 8526, 3341, 2270, 257, 413, 12, 44, 471, 3035, 13, 51044], "temperature": 0.0, "avg_logprob": -0.09785216926728318, "compression_ratio": 1.7161290322580645, "no_speech_prob": 0.013221504166722298}, {"id": 10, "seek": 2816, "start": 41.76, "end": 47.44, "text": " And not only is this just some plan, it is now the plan for research at the Alberta branch of", "tokens": [51044, 400, 406, 787, 307, 341, 445, 512, 1393, 11, 309, 307, 586, 264, 1393, 337, 2132, 412, 264, 43279, 9819, 295, 51328], "temperature": 0.0, "avg_logprob": -0.09785216926728318, "compression_ratio": 1.7161290322580645, "no_speech_prob": 0.013221504166722298}, {"id": 11, "seek": 2816, "start": 47.44, "end": 52.400000000000006, "text": " D-Mind. Hence why it's called, you know, the Alberta plan. And it's just out there. It has some,", "tokens": [51328, 413, 12, 44, 471, 13, 22229, 983, 309, 311, 1219, 11, 291, 458, 11, 264, 43279, 1393, 13, 400, 309, 311, 445, 484, 456, 13, 467, 575, 512, 11, 51576], "temperature": 0.0, "avg_logprob": -0.09785216926728318, "compression_ratio": 1.7161290322580645, "no_speech_prob": 0.013221504166722298}, {"id": 12, "seek": 2816, "start": 52.400000000000006, "end": 56.32, "text": " I think, really interesting ideas and no one's been talking about it. So today,", "tokens": [51576, 286, 519, 11, 534, 1880, 3487, 293, 572, 472, 311, 668, 1417, 466, 309, 13, 407, 965, 11, 51772], "temperature": 0.0, "avg_logprob": -0.09785216926728318, "compression_ratio": 1.7161290322580645, "no_speech_prob": 0.013221504166722298}, {"id": 13, "seek": 5632, "start": 56.32, "end": 61.44, "text": " I hope to take you through the motivation for this and the plan itself. It is very interesting in", "tokens": [50364, 286, 1454, 281, 747, 291, 807, 264, 12335, 337, 341, 293, 264, 1393, 2564, 13, 467, 307, 588, 1880, 294, 50620], "temperature": 0.0, "avg_logprob": -0.0582103122364391, "compression_ratio": 1.734982332155477, "no_speech_prob": 0.04084130376577377}, {"id": 14, "seek": 5632, "start": 61.44, "end": 66.08, "text": " the sense that it's building back up from the foundations of machine learning and reinforcement", "tokens": [50620, 264, 2020, 300, 309, 311, 2390, 646, 493, 490, 264, 22467, 295, 3479, 2539, 293, 29280, 50852], "temperature": 0.0, "avg_logprob": -0.0582103122364391, "compression_ratio": 1.734982332155477, "no_speech_prob": 0.04084130376577377}, {"id": 15, "seek": 5632, "start": 66.08, "end": 71.76, "text": " learning. And while it is very similar in most ways, it's also somewhat contrarian to many of", "tokens": [50852, 2539, 13, 400, 1339, 309, 307, 588, 2531, 294, 881, 2098, 11, 309, 311, 611, 8344, 660, 5352, 952, 281, 867, 295, 51136], "temperature": 0.0, "avg_logprob": -0.0582103122364391, "compression_ratio": 1.734982332155477, "no_speech_prob": 0.04084130376577377}, {"id": 16, "seek": 5632, "start": 71.76, "end": 76.88, "text": " the approaches, especially in reinforcement learning that are popular today. I love this because if", "tokens": [51136, 264, 11587, 11, 2318, 294, 29280, 2539, 300, 366, 3743, 965, 13, 286, 959, 341, 570, 498, 51392], "temperature": 0.0, "avg_logprob": -0.0582103122364391, "compression_ratio": 1.734982332155477, "no_speech_prob": 0.04084130376577377}, {"id": 17, "seek": 5632, "start": 76.88, "end": 81.6, "text": " you're a new graduate student or you can't find a research topic or you just want to get into research,", "tokens": [51392, 291, 434, 257, 777, 8080, 3107, 420, 291, 393, 380, 915, 257, 2132, 4829, 420, 291, 445, 528, 281, 483, 666, 2132, 11, 51628], "temperature": 0.0, "avg_logprob": -0.0582103122364391, "compression_ratio": 1.734982332155477, "no_speech_prob": 0.04084130376577377}, {"id": 18, "seek": 8160, "start": 81.6, "end": 86.8, "text": " well, here's 12 research topics handed to you on a silver platter. You really can't ask for much", "tokens": [50364, 731, 11, 510, 311, 2272, 2132, 8378, 16013, 281, 291, 322, 257, 8753, 499, 1161, 13, 509, 534, 393, 380, 1029, 337, 709, 50624], "temperature": 0.0, "avg_logprob": -0.06155625921096245, "compression_ratio": 1.7299703264094954, "no_speech_prob": 0.014955800957977772}, {"id": 19, "seek": 8160, "start": 86.8, "end": 90.96, "text": " more. And it's also just, I personally think, interesting to know what some prominent researchers", "tokens": [50624, 544, 13, 400, 309, 311, 611, 445, 11, 286, 5665, 519, 11, 1880, 281, 458, 437, 512, 17034, 10309, 50832], "temperature": 0.0, "avg_logprob": -0.06155625921096245, "compression_ratio": 1.7299703264094954, "no_speech_prob": 0.014955800957977772}, {"id": 20, "seek": 8160, "start": 90.96, "end": 95.11999999999999, "text": " are thinking about and how they're going about approaching these problems that lots of us are", "tokens": [50832, 366, 1953, 466, 293, 577, 436, 434, 516, 466, 14908, 613, 2740, 300, 3195, 295, 505, 366, 51040], "temperature": 0.0, "avg_logprob": -0.06155625921096245, "compression_ratio": 1.7299703264094954, "no_speech_prob": 0.014955800957977772}, {"id": 21, "seek": 8160, "start": 95.11999999999999, "end": 99.6, "text": " very interested in. One last thing I'll mention before we dive into this is I cover lots of big", "tokens": [51040, 588, 3102, 294, 13, 1485, 1036, 551, 286, 603, 2152, 949, 321, 9192, 666, 341, 307, 286, 2060, 3195, 295, 955, 51264], "temperature": 0.0, "avg_logprob": -0.06155625921096245, "compression_ratio": 1.7299703264094954, "no_speech_prob": 0.014955800957977772}, {"id": 22, "seek": 8160, "start": 99.6, "end": 103.44, "text": " ideas on this channel to keep you up to date with what's going on in the field, but also smaller", "tokens": [51264, 3487, 322, 341, 2269, 281, 1066, 291, 493, 281, 4002, 365, 437, 311, 516, 322, 294, 264, 2519, 11, 457, 611, 4356, 51456], "temperature": 0.0, "avg_logprob": -0.06155625921096245, "compression_ratio": 1.7299703264094954, "no_speech_prob": 0.014955800957977772}, {"id": 23, "seek": 8160, "start": 103.44, "end": 108.32, "text": " ideas kind of like this kind of go under the radar to introduce you to some new interesting ideas. So", "tokens": [51456, 3487, 733, 295, 411, 341, 733, 295, 352, 833, 264, 16544, 281, 5366, 291, 281, 512, 777, 1880, 3487, 13, 407, 51700], "temperature": 0.0, "avg_logprob": -0.06155625921096245, "compression_ratio": 1.7299703264094954, "no_speech_prob": 0.014955800957977772}, {"id": 24, "seek": 10832, "start": 108.32, "end": 111.67999999999999, "text": " if that's the type of thing that interests you, sort of subscribing to the channel, it means a lot", "tokens": [50364, 498, 300, 311, 264, 2010, 295, 551, 300, 8847, 291, 11, 1333, 295, 19981, 281, 264, 2269, 11, 309, 1355, 257, 688, 50532], "temperature": 0.0, "avg_logprob": -0.08999651715271455, "compression_ratio": 1.7767584097859328, "no_speech_prob": 0.027582017704844475}, {"id": 25, "seek": 10832, "start": 111.67999999999999, "end": 115.6, "text": " and it really does help out. Anyway, let's dive into this. In this paper starts, we're going to", "tokens": [50532, 293, 309, 534, 775, 854, 484, 13, 5684, 11, 718, 311, 9192, 666, 341, 13, 682, 341, 3035, 3719, 11, 321, 434, 516, 281, 50728], "temperature": 0.0, "avg_logprob": -0.08999651715271455, "compression_ratio": 1.7767584097859328, "no_speech_prob": 0.027582017704844475}, {"id": 26, "seek": 10832, "start": 115.6, "end": 118.96, "text": " come back the first two paragraphs in a second. I just want to start with the third paragraph", "tokens": [50728, 808, 646, 264, 700, 732, 48910, 294, 257, 1150, 13, 286, 445, 528, 281, 722, 365, 264, 2636, 18865, 50896], "temperature": 0.0, "avg_logprob": -0.08999651715271455, "compression_ratio": 1.7767584097859328, "no_speech_prob": 0.027582017704844475}, {"id": 27, "seek": 10832, "start": 118.96, "end": 122.55999999999999, "text": " because it really explains the core of what's going on here, what they're thinking. So starting", "tokens": [50896, 570, 309, 534, 13948, 264, 4965, 295, 437, 311, 516, 322, 510, 11, 437, 436, 434, 1953, 13, 407, 2891, 51076], "temperature": 0.0, "avg_logprob": -0.08999651715271455, "compression_ratio": 1.7767584097859328, "no_speech_prob": 0.027582017704844475}, {"id": 28, "seek": 10832, "start": 122.55999999999999, "end": 128.07999999999998, "text": " from here, following the Alberta plan, we seek to understand and create long lived computational", "tokens": [51076, 490, 510, 11, 3480, 264, 43279, 1393, 11, 321, 8075, 281, 1223, 293, 1884, 938, 5152, 28270, 51352], "temperature": 0.0, "avg_logprob": -0.08999651715271455, "compression_ratio": 1.7767584097859328, "no_speech_prob": 0.027582017704844475}, {"id": 29, "seek": 10832, "start": 128.07999999999998, "end": 134.48, "text": " agents that interact with a vastly more complex world and come to predict and control their sensory", "tokens": [51352, 12554, 300, 4648, 365, 257, 41426, 544, 3997, 1002, 293, 808, 281, 6069, 293, 1969, 641, 27233, 51672], "temperature": 0.0, "avg_logprob": -0.08999651715271455, "compression_ratio": 1.7767584097859328, "no_speech_prob": 0.027582017704844475}, {"id": 30, "seek": 13448, "start": 134.48, "end": 139.92, "text": " input signals and blah, blah, blah. They are as scalable as possible. If you've seen Rich Sutton's", "tokens": [50364, 4846, 12354, 293, 12288, 11, 12288, 11, 12288, 13, 814, 366, 382, 38481, 382, 1944, 13, 759, 291, 600, 1612, 6781, 40492, 1756, 311, 50636], "temperature": 0.0, "avg_logprob": -0.09856289536205691, "compression_ratio": 1.7598784194528876, "no_speech_prob": 0.026756681501865387}, {"id": 31, "seek": 13448, "start": 139.92, "end": 143.2, "text": " better lesson, you know, maybe they're starting to look a little familiar and they have to adapt", "tokens": [50636, 1101, 6898, 11, 291, 458, 11, 1310, 436, 434, 2891, 281, 574, 257, 707, 4963, 293, 436, 362, 281, 6231, 50800], "temperature": 0.0, "avg_logprob": -0.09856289536205691, "compression_ratio": 1.7598784194528876, "no_speech_prob": 0.026756681501865387}, {"id": 32, "seek": 13448, "start": 143.2, "end": 147.92, "text": " and change the complexity, which means they have to continually learn. Big focus on this here. And", "tokens": [50800, 293, 1319, 264, 14024, 11, 597, 1355, 436, 362, 281, 22277, 1466, 13, 5429, 1879, 322, 341, 510, 13, 400, 51036], "temperature": 0.0, "avg_logprob": -0.09856289536205691, "compression_ratio": 1.7598784194528876, "no_speech_prob": 0.026756681501865387}, {"id": 33, "seek": 13448, "start": 147.92, "end": 151.76, "text": " then another thing they mentioned is they must plan with a learned model of the world. Now I", "tokens": [51036, 550, 1071, 551, 436, 2835, 307, 436, 1633, 1393, 365, 257, 3264, 2316, 295, 264, 1002, 13, 823, 286, 51228], "temperature": 0.0, "avg_logprob": -0.09856289536205691, "compression_ratio": 1.7598784194528876, "no_speech_prob": 0.026756681501865387}, {"id": 34, "seek": 13448, "start": 151.76, "end": 156.07999999999998, "text": " highlighted these specific things, except for the word follow, but I highlighted these specific", "tokens": [51228, 17173, 613, 2685, 721, 11, 3993, 337, 264, 1349, 1524, 11, 457, 286, 17173, 613, 2685, 51444], "temperature": 0.0, "avg_logprob": -0.09856289536205691, "compression_ratio": 1.7598784194528876, "no_speech_prob": 0.026756681501865387}, {"id": 35, "seek": 13448, "start": 156.07999999999998, "end": 161.04, "text": " things because these I think are things that are very core to this paper. So long lived, right,", "tokens": [51444, 721, 570, 613, 286, 519, 366, 721, 300, 366, 588, 4965, 281, 341, 3035, 13, 407, 938, 5152, 11, 558, 11, 51692], "temperature": 0.0, "avg_logprob": -0.09856289536205691, "compression_ratio": 1.7598784194528876, "no_speech_prob": 0.026756681501865387}, {"id": 36, "seek": 16104, "start": 161.12, "end": 165.92, "text": " and continuing learning. These are things that some people in our work on, but are not super", "tokens": [50368, 293, 9289, 2539, 13, 1981, 366, 721, 300, 512, 561, 294, 527, 589, 322, 11, 457, 366, 406, 1687, 50608], "temperature": 0.0, "avg_logprob": -0.11029689495380109, "compression_ratio": 1.8562300319488818, "no_speech_prob": 0.042080193758010864}, {"id": 37, "seek": 16104, "start": 165.92, "end": 171.04, "text": " focused on that are really the sort of at the forefront of this paper. That means that agents", "tokens": [50608, 5178, 322, 300, 366, 534, 264, 1333, 295, 412, 264, 27287, 295, 341, 3035, 13, 663, 1355, 300, 12554, 50864], "temperature": 0.0, "avg_logprob": -0.11029689495380109, "compression_ratio": 1.8562300319488818, "no_speech_prob": 0.042080193758010864}, {"id": 38, "seek": 16104, "start": 171.04, "end": 175.2, "text": " are going to have to be able to work with certain challenges that are present in continuing learning.", "tokens": [50864, 366, 516, 281, 362, 281, 312, 1075, 281, 589, 365, 1629, 4759, 300, 366, 1974, 294, 9289, 2539, 13, 51072], "temperature": 0.0, "avg_logprob": -0.11029689495380109, "compression_ratio": 1.8562300319488818, "no_speech_prob": 0.042080193758010864}, {"id": 39, "seek": 16104, "start": 175.2, "end": 180.23999999999998, "text": " For example, if you try and take a neural network and have it continue to learn on a constantly", "tokens": [51072, 1171, 1365, 11, 498, 291, 853, 293, 747, 257, 18161, 3209, 293, 362, 309, 2354, 281, 1466, 322, 257, 6460, 51324], "temperature": 0.0, "avg_logprob": -0.11029689495380109, "compression_ratio": 1.8562300319488818, "no_speech_prob": 0.042080193758010864}, {"id": 40, "seek": 16104, "start": 180.23999999999998, "end": 185.6, "text": " changing task, many people don't realize this, but what actually happens is neural networks get worse", "tokens": [51324, 4473, 5633, 11, 867, 561, 500, 380, 4325, 341, 11, 457, 437, 767, 2314, 307, 18161, 9590, 483, 5324, 51592], "temperature": 0.0, "avg_logprob": -0.11029689495380109, "compression_ratio": 1.8562300319488818, "no_speech_prob": 0.042080193758010864}, {"id": 41, "seek": 16104, "start": 185.6, "end": 189.6, "text": " and worse at performing or essentially learning new things, kind of like the, what do you say,", "tokens": [51592, 293, 5324, 412, 10205, 420, 4476, 2539, 777, 721, 11, 733, 295, 411, 264, 11, 437, 360, 291, 584, 11, 51792], "temperature": 0.0, "avg_logprob": -0.11029689495380109, "compression_ratio": 1.8562300319488818, "no_speech_prob": 0.042080193758010864}, {"id": 42, "seek": 18960, "start": 189.6, "end": 194.16, "text": " an old dog can't learn new tricks. There's actually some concept of that in I think machine", "tokens": [50364, 364, 1331, 3000, 393, 380, 1466, 777, 11733, 13, 821, 311, 767, 512, 3410, 295, 300, 294, 286, 519, 3479, 50592], "temperature": 0.0, "avg_logprob": -0.07429280090332031, "compression_ratio": 1.7886435331230284, "no_speech_prob": 0.0034832286182790995}, {"id": 43, "seek": 18960, "start": 194.16, "end": 198.48, "text": " learning that lots of people are unfamiliar with. So things like that, those are issues that might", "tokens": [50592, 2539, 300, 3195, 295, 561, 366, 29415, 365, 13, 407, 721, 411, 300, 11, 729, 366, 2663, 300, 1062, 50808], "temperature": 0.0, "avg_logprob": -0.07429280090332031, "compression_ratio": 1.7886435331230284, "no_speech_prob": 0.0034832286182790995}, {"id": 44, "seek": 18960, "start": 198.48, "end": 203.84, "text": " be important in this situation. Anything vastly more complex world predicting and controlling", "tokens": [50808, 312, 1021, 294, 341, 2590, 13, 11998, 41426, 544, 3997, 1002, 32884, 293, 14905, 51076], "temperature": 0.0, "avg_logprob": -0.07429280090332031, "compression_ratio": 1.7886435331230284, "no_speech_prob": 0.0034832286182790995}, {"id": 45, "seek": 18960, "start": 203.84, "end": 208.16, "text": " sensory inputs. Some of these things are very like what you would assume is common, but you'll", "tokens": [51076, 27233, 15743, 13, 2188, 295, 613, 721, 366, 588, 411, 437, 291, 576, 6552, 307, 2689, 11, 457, 291, 603, 51292], "temperature": 0.0, "avg_logprob": -0.07429280090332031, "compression_ratio": 1.7886435331230284, "no_speech_prob": 0.0034832286182790995}, {"id": 46, "seek": 18960, "start": 208.16, "end": 212.16, "text": " see that everything works under these assumptions in this paper. And we'll get into some of the", "tokens": [51292, 536, 300, 1203, 1985, 833, 613, 17695, 294, 341, 3035, 13, 400, 321, 603, 483, 666, 512, 295, 264, 51492], "temperature": 0.0, "avg_logprob": -0.07429280090332031, "compression_ratio": 1.7886435331230284, "no_speech_prob": 0.0034832286182790995}, {"id": 47, "seek": 18960, "start": 212.16, "end": 216.88, "text": " more details of why I think some of these, you know, approaching these types of problems is", "tokens": [51492, 544, 4365, 295, 983, 286, 519, 512, 295, 613, 11, 291, 458, 11, 14908, 613, 3467, 295, 2740, 307, 51728], "temperature": 0.0, "avg_logprob": -0.07429280090332031, "compression_ratio": 1.7886435331230284, "no_speech_prob": 0.0034832286182790995}, {"id": 48, "seek": 21688, "start": 216.88, "end": 221.2, "text": " interesting. The one thing that does stick out here a little bit is I think planning with", "tokens": [50364, 1880, 13, 440, 472, 551, 300, 775, 2897, 484, 510, 257, 707, 857, 307, 286, 519, 5038, 365, 50580], "temperature": 0.0, "avg_logprob": -0.11296023598200158, "compression_ratio": 1.7933884297520661, "no_speech_prob": 0.013635888695716858}, {"id": 49, "seek": 21688, "start": 221.2, "end": 224.96, "text": " a learning model of the world, because the other things are problems, right? Sort of having to", "tokens": [50580, 257, 2539, 2316, 295, 264, 1002, 11, 570, 264, 661, 721, 366, 2740, 11, 558, 30, 26149, 295, 1419, 281, 50768], "temperature": 0.0, "avg_logprob": -0.11296023598200158, "compression_ratio": 1.7933884297520661, "no_speech_prob": 0.013635888695716858}, {"id": 50, "seek": 21688, "start": 224.96, "end": 228.72, "text": " be long lived and continuing to learn these, you know, having a vastly more complex world,", "tokens": [50768, 312, 938, 5152, 293, 9289, 281, 1466, 613, 11, 291, 458, 11, 1419, 257, 41426, 544, 3997, 1002, 11, 50956], "temperature": 0.0, "avg_logprob": -0.11296023598200158, "compression_ratio": 1.7933884297520661, "no_speech_prob": 0.013635888695716858}, {"id": 51, "seek": 21688, "start": 228.72, "end": 232.72, "text": " these are statements about the problem. Whereas playing with a model is like a specific thing", "tokens": [50956, 613, 366, 12363, 466, 264, 1154, 13, 13813, 2433, 365, 257, 2316, 307, 411, 257, 2685, 551, 51156], "temperature": 0.0, "avg_logprob": -0.11296023598200158, "compression_ratio": 1.7933884297520661, "no_speech_prob": 0.013635888695716858}, {"id": 52, "seek": 21688, "start": 232.72, "end": 236.8, "text": " they do. So I guess they just think it's that important, which I would agree. But anyway,", "tokens": [51156, 436, 360, 13, 407, 286, 2041, 436, 445, 519, 309, 311, 300, 1021, 11, 597, 286, 576, 3986, 13, 583, 4033, 11, 51360], "temperature": 0.0, "avg_logprob": -0.11296023598200158, "compression_ratio": 1.7933884297520661, "no_speech_prob": 0.013635888695716858}, {"id": 53, "seek": 21688, "start": 236.8, "end": 241.04, "text": " just some details about, you know, sort of the logistics of this plan before we maybe get into", "tokens": [51360, 445, 512, 4365, 466, 11, 291, 458, 11, 1333, 295, 264, 27420, 295, 341, 1393, 949, 321, 1310, 483, 666, 51572], "temperature": 0.0, "avg_logprob": -0.11296023598200158, "compression_ratio": 1.7933884297520661, "no_speech_prob": 0.013635888695716858}, {"id": 54, "seek": 21688, "start": 241.04, "end": 245.76, "text": " it. This is a five to 10 year plan, as they mentioned here. They also, you know, some of this is", "tokens": [51572, 309, 13, 639, 307, 257, 1732, 281, 1266, 1064, 1393, 11, 382, 436, 2835, 510, 13, 814, 611, 11, 291, 458, 11, 512, 295, 341, 307, 51808], "temperature": 0.0, "avg_logprob": -0.11296023598200158, "compression_ratio": 1.7933884297520661, "no_speech_prob": 0.013635888695716858}, {"id": 55, "seek": 24576, "start": 245.76, "end": 249.92, "text": " very like, you know, we think this will greatly affect the economy or society or individual", "tokens": [50364, 588, 411, 11, 291, 458, 11, 321, 519, 341, 486, 14147, 3345, 264, 5010, 420, 4086, 420, 2609, 50572], "temperature": 0.0, "avg_logprob": -0.08278334870630381, "compression_ratio": 1.7256637168141593, "no_speech_prob": 0.0023965255822986364}, {"id": 56, "seek": 24576, "start": 249.92, "end": 254.95999999999998, "text": " lives. I don't think we need to go through all the, all the philosophical stuff. We can kind of get", "tokens": [50572, 2909, 13, 286, 500, 380, 519, 321, 643, 281, 352, 807, 439, 264, 11, 439, 264, 25066, 1507, 13, 492, 393, 733, 295, 483, 50824], "temperature": 0.0, "avg_logprob": -0.08278334870630381, "compression_ratio": 1.7256637168141593, "no_speech_prob": 0.0023965255822986364}, {"id": 57, "seek": 24576, "start": 254.95999999999998, "end": 259.84, "text": " into the meat of this. So the purpose of giving this plan, which they write about here is, as they", "tokens": [50824, 666, 264, 4615, 295, 341, 13, 407, 264, 4334, 295, 2902, 341, 1393, 11, 597, 436, 2464, 466, 510, 307, 11, 382, 436, 51068], "temperature": 0.0, "avg_logprob": -0.08278334870630381, "compression_ratio": 1.7256637168141593, "no_speech_prob": 0.0023965255822986364}, {"id": 58, "seek": 24576, "start": 259.84, "end": 264.32, "text": " say, it's twofold. The main one is, or one of the two is that they want to sort of give their vision", "tokens": [51068, 584, 11, 309, 311, 732, 18353, 13, 440, 2135, 472, 307, 11, 420, 472, 295, 264, 732, 307, 300, 436, 528, 281, 1333, 295, 976, 641, 5201, 51292], "temperature": 0.0, "avg_logprob": -0.08278334870630381, "compression_ratio": 1.7256637168141593, "no_speech_prob": 0.0023965255822986364}, {"id": 59, "seek": 24576, "start": 264.32, "end": 268.88, "text": " for AI. Well, lots of people have been working on these problems, like continue learning. They", "tokens": [51292, 337, 7318, 13, 1042, 11, 3195, 295, 561, 362, 668, 1364, 322, 613, 2740, 11, 411, 2354, 2539, 13, 814, 51520], "temperature": 0.0, "avg_logprob": -0.08278334870630381, "compression_ratio": 1.7256637168141593, "no_speech_prob": 0.0023965255822986364}, {"id": 60, "seek": 24576, "start": 268.88, "end": 272.08, "text": " think it should really be at the forefront. So some of those things are a little bit different. So", "tokens": [51520, 519, 309, 820, 534, 312, 412, 264, 27287, 13, 407, 512, 295, 729, 721, 366, 257, 707, 857, 819, 13, 407, 51680], "temperature": 0.0, "avg_logprob": -0.08278334870630381, "compression_ratio": 1.7256637168141593, "no_speech_prob": 0.0023965255822986364}, {"id": 61, "seek": 27208, "start": 272.08, "end": 276.08, "text": " this is their vision, what problems they think should be focused on. And then also laying out,", "tokens": [50364, 341, 307, 641, 5201, 11, 437, 2740, 436, 519, 820, 312, 5178, 322, 13, 400, 550, 611, 14903, 484, 11, 50564], "temperature": 0.0, "avg_logprob": -0.08986873096889919, "compression_ratio": 1.8081761006289307, "no_speech_prob": 0.0014103337889537215}, {"id": 62, "seek": 27208, "start": 276.08, "end": 281.91999999999996, "text": " of course, a research plan for the purpose, you know, of doing research. This is not a to-do list.", "tokens": [50564, 295, 1164, 11, 257, 2132, 1393, 337, 264, 4334, 11, 291, 458, 11, 295, 884, 2132, 13, 639, 307, 406, 257, 281, 12, 2595, 1329, 13, 50856], "temperature": 0.0, "avg_logprob": -0.08986873096889919, "compression_ratio": 1.8081761006289307, "no_speech_prob": 0.0014103337889537215}, {"id": 63, "seek": 27208, "start": 281.91999999999996, "end": 287.03999999999996, "text": " This is not like a things they completed. This is a research plan. They, they, as they very much", "tokens": [50856, 639, 307, 406, 411, 257, 721, 436, 7365, 13, 639, 307, 257, 2132, 1393, 13, 814, 11, 436, 11, 382, 436, 588, 709, 51112], "temperature": 0.0, "avg_logprob": -0.08986873096889919, "compression_ratio": 1.8081761006289307, "no_speech_prob": 0.0014103337889537215}, {"id": 64, "seek": 27208, "start": 287.03999999999996, "end": 291.2, "text": " stay here, gaps and uncertainties, which are to be expected, right? When you're tackling these", "tokens": [51112, 1754, 510, 11, 15031, 293, 11308, 6097, 11, 597, 366, 281, 312, 5176, 11, 558, 30, 1133, 291, 434, 34415, 613, 51320], "temperature": 0.0, "avg_logprob": -0.08986873096889919, "compression_ratio": 1.8081761006289307, "no_speech_prob": 0.0014103337889537215}, {"id": 65, "seek": 27208, "start": 291.2, "end": 296.56, "text": " big problems, when you have literally 12 steps, of course, it might not be enough. Maybe some", "tokens": [51320, 955, 2740, 11, 562, 291, 362, 3736, 2272, 4439, 11, 295, 1164, 11, 309, 1062, 406, 312, 1547, 13, 2704, 512, 51588], "temperature": 0.0, "avg_logprob": -0.08986873096889919, "compression_ratio": 1.8081761006289307, "no_speech_prob": 0.0014103337889537215}, {"id": 66, "seek": 27208, "start": 296.56, "end": 299.76, "text": " things aren't going to be necessary. Things are going to change throughout this plan. There are", "tokens": [51588, 721, 3212, 380, 516, 281, 312, 4818, 13, 9514, 366, 516, 281, 1319, 3710, 341, 1393, 13, 821, 366, 51748], "temperature": 0.0, "avg_logprob": -0.08986873096889919, "compression_ratio": 1.8081761006289307, "no_speech_prob": 0.0014103337889537215}, {"id": 67, "seek": 29976, "start": 299.84, "end": 305.03999999999996, "text": " some key points or some important things that they think sort of differentiates this Alberta plan", "tokens": [50368, 512, 2141, 2793, 420, 512, 1021, 721, 300, 436, 519, 1333, 295, 27372, 1024, 341, 43279, 1393, 50628], "temperature": 0.0, "avg_logprob": -0.0792732591982241, "compression_ratio": 1.842443729903537, "no_speech_prob": 0.004754995461553335}, {"id": 68, "seek": 29976, "start": 305.03999999999996, "end": 309.52, "text": " from lots of the work going on at other places. So that's a little bit what they talk about in", "tokens": [50628, 490, 3195, 295, 264, 589, 516, 322, 412, 661, 3190, 13, 407, 300, 311, 257, 707, 857, 437, 436, 751, 466, 294, 50852], "temperature": 0.0, "avg_logprob": -0.0792732591982241, "compression_ratio": 1.842443729903537, "no_speech_prob": 0.004754995461553335}, {"id": 69, "seek": 29976, "start": 309.52, "end": 314.0, "text": " this next section. Ah, the other thing in this next section is this diagram that reminds me,", "tokens": [50852, 341, 958, 3541, 13, 2438, 11, 264, 661, 551, 294, 341, 958, 3541, 307, 341, 10686, 300, 12025, 385, 11, 51076], "temperature": 0.0, "avg_logprob": -0.0792732591982241, "compression_ratio": 1.842443729903537, "no_speech_prob": 0.004754995461553335}, {"id": 70, "seek": 29976, "start": 314.0, "end": 319.12, "text": " I forgot to say what RL is. So this is going to be just a five second explanation. If you don't", "tokens": [51076, 286, 5298, 281, 584, 437, 497, 43, 307, 13, 407, 341, 307, 516, 281, 312, 445, 257, 1732, 1150, 10835, 13, 759, 291, 500, 380, 51332], "temperature": 0.0, "avg_logprob": -0.0792732591982241, "compression_ratio": 1.842443729903537, "no_speech_prob": 0.004754995461553335}, {"id": 71, "seek": 29976, "start": 319.12, "end": 323.36, "text": " know what reinforcement learning is, just imagine you have some environment like the world, you have", "tokens": [51332, 458, 437, 29280, 2539, 307, 11, 445, 3811, 291, 362, 512, 2823, 411, 264, 1002, 11, 291, 362, 51544], "temperature": 0.0, "avg_logprob": -0.0792732591982241, "compression_ratio": 1.842443729903537, "no_speech_prob": 0.004754995461553335}, {"id": 72, "seek": 29976, "start": 323.36, "end": 327.44, "text": " an agent, you know, it sees things from the environment, gets observations, takes actions,", "tokens": [51544, 364, 9461, 11, 291, 458, 11, 309, 8194, 721, 490, 264, 2823, 11, 2170, 18163, 11, 2516, 5909, 11, 51748], "temperature": 0.0, "avg_logprob": -0.0792732591982241, "compression_ratio": 1.842443729903537, "no_speech_prob": 0.004754995461553335}, {"id": 73, "seek": 32744, "start": 327.44, "end": 332.32, "text": " and it tries to maximize some reward. There's your five second explanation. Hopefully that keyed", "tokens": [50364, 293, 309, 9898, 281, 19874, 512, 7782, 13, 821, 311, 428, 1732, 1150, 10835, 13, 10429, 300, 2141, 292, 50608], "temperature": 0.0, "avg_logprob": -0.11475925445556641, "compression_ratio": 1.6794425087108014, "no_speech_prob": 0.001098697423003614}, {"id": 74, "seek": 32744, "start": 332.32, "end": 337.04, "text": " anyone in it wasn't okay. Anyway, what are these core tenants that they think differentiate the", "tokens": [50608, 2878, 294, 309, 2067, 380, 1392, 13, 5684, 11, 437, 366, 613, 4965, 31216, 300, 436, 519, 23203, 264, 50844], "temperature": 0.0, "avg_logprob": -0.11475925445556641, "compression_ratio": 1.6794425087108014, "no_speech_prob": 0.001098697423003614}, {"id": 75, "seek": 32744, "start": 337.04, "end": 342.56, "text": " Alberta plan? So the first one, here it is, they say the first distinguishing feature is the emphasis", "tokens": [50844, 43279, 1393, 30, 407, 264, 700, 472, 11, 510, 309, 307, 11, 436, 584, 264, 700, 11365, 3807, 4111, 307, 264, 16271, 51120], "temperature": 0.0, "avg_logprob": -0.11475925445556641, "compression_ratio": 1.6794425087108014, "no_speech_prob": 0.001098697423003614}, {"id": 76, "seek": 32744, "start": 342.56, "end": 349.12, "text": " on ordinary experience as described above. So an ordinary experience, as they'll mention here,", "tokens": [51120, 322, 10547, 1752, 382, 7619, 3673, 13, 407, 364, 10547, 1752, 11, 382, 436, 603, 2152, 510, 11, 51448], "temperature": 0.0, "avg_logprob": -0.11475925445556641, "compression_ratio": 1.6794425087108014, "no_speech_prob": 0.001098697423003614}, {"id": 77, "seek": 32744, "start": 349.12, "end": 353.84, "text": " this means that you can't do things like use specialized training sets. You can't use human,", "tokens": [51448, 341, 1355, 300, 291, 393, 380, 360, 721, 411, 764, 19813, 3097, 6352, 13, 509, 393, 380, 764, 1952, 11, 51684], "temperature": 0.0, "avg_logprob": -0.11475925445556641, "compression_ratio": 1.6794425087108014, "no_speech_prob": 0.001098697423003614}, {"id": 78, "seek": 35384, "start": 353.84, "end": 358.64, "text": " like assistance, you know, you can't put human data into this, you can't access the internal", "tokens": [50364, 411, 9683, 11, 291, 458, 11, 291, 393, 380, 829, 1952, 1412, 666, 341, 11, 291, 393, 380, 2105, 264, 6920, 50604], "temperature": 0.0, "avg_logprob": -0.09578939367223668, "compression_ratio": 1.9259259259259258, "no_speech_prob": 0.006692375522106886}, {"id": 79, "seek": 35384, "start": 358.64, "end": 362.32, "text": " structure of the world, which lots of work and reinforcement learning does, right? Not everything,", "tokens": [50604, 3877, 295, 264, 1002, 11, 597, 3195, 295, 589, 293, 29280, 2539, 775, 11, 558, 30, 1726, 1203, 11, 50788], "temperature": 0.0, "avg_logprob": -0.09578939367223668, "compression_ratio": 1.9259259259259258, "no_speech_prob": 0.006692375522106886}, {"id": 80, "seek": 35384, "start": 362.32, "end": 367.52, "text": " but lots of it does. And I should say, maybe clarify, human assistance here does not mean that", "tokens": [50788, 457, 3195, 295, 309, 775, 13, 400, 286, 820, 584, 11, 1310, 17594, 11, 1952, 9683, 510, 775, 406, 914, 300, 51048], "temperature": 0.0, "avg_logprob": -0.09578939367223668, "compression_ratio": 1.9259259259259258, "no_speech_prob": 0.006692375522106886}, {"id": 81, "seek": 35384, "start": 367.52, "end": 373.28, "text": " an AI cannot interact with humans. It just means that once like the learning process has started,", "tokens": [51048, 364, 7318, 2644, 4648, 365, 6255, 13, 467, 445, 1355, 300, 1564, 411, 264, 2539, 1399, 575, 1409, 11, 51336], "temperature": 0.0, "avg_logprob": -0.09578939367223668, "compression_ratio": 1.9259259259259258, "no_speech_prob": 0.006692375522106886}, {"id": 82, "seek": 35384, "start": 373.28, "end": 377.12, "text": " you can't have a human like tweaking like different parameters and like, you know, maybe", "tokens": [51336, 291, 393, 380, 362, 257, 1952, 411, 6986, 2456, 411, 819, 9834, 293, 411, 11, 291, 458, 11, 1310, 51528], "temperature": 0.0, "avg_logprob": -0.09578939367223668, "compression_ratio": 1.9259259259259258, "no_speech_prob": 0.006692375522106886}, {"id": 83, "seek": 35384, "start": 377.12, "end": 381.2, "text": " training a bit like this and then a bit like this. It means that the learning process, you kind of", "tokens": [51528, 3097, 257, 857, 411, 341, 293, 550, 257, 857, 411, 341, 13, 467, 1355, 300, 264, 2539, 1399, 11, 291, 733, 295, 51732], "temperature": 0.0, "avg_logprob": -0.09578939367223668, "compression_ratio": 1.9259259259259258, "no_speech_prob": 0.006692375522106886}, {"id": 84, "seek": 38120, "start": 381.2, "end": 386.88, "text": " have this agent, it starts out and then it has to do everything by itself. And why is this so", "tokens": [50364, 362, 341, 9461, 11, 309, 3719, 484, 293, 550, 309, 575, 281, 360, 1203, 538, 2564, 13, 400, 983, 307, 341, 370, 50648], "temperature": 0.0, "avg_logprob": -0.09271527013034685, "compression_ratio": 1.7425149700598803, "no_speech_prob": 0.005060028284788132}, {"id": 85, "seek": 38120, "start": 386.88, "end": 391.36, "text": " important? And the idea here is that these types of methods that require human assistance, they", "tokens": [50648, 1021, 30, 400, 264, 1558, 510, 307, 300, 613, 3467, 295, 7150, 300, 3651, 1952, 9683, 11, 436, 50872], "temperature": 0.0, "avg_logprob": -0.09271527013034685, "compression_ratio": 1.7425149700598803, "no_speech_prob": 0.005060028284788132}, {"id": 86, "seek": 38120, "start": 391.36, "end": 396.71999999999997, "text": " usually don't scale, where do they say this, they typically do not scale with computational resources,", "tokens": [50872, 2673, 500, 380, 4373, 11, 689, 360, 436, 584, 341, 11, 436, 5850, 360, 406, 4373, 365, 28270, 3593, 11, 51140], "temperature": 0.0, "avg_logprob": -0.09271527013034685, "compression_ratio": 1.7425149700598803, "no_speech_prob": 0.005060028284788132}, {"id": 87, "seek": 38120, "start": 396.71999999999997, "end": 401.68, "text": " and as such is not a research priority. Now, if you've read the bitter, the bitter lesson, that's", "tokens": [51140, 293, 382, 1270, 307, 406, 257, 2132, 9365, 13, 823, 11, 498, 291, 600, 1401, 264, 13871, 11, 264, 13871, 6898, 11, 300, 311, 51388], "temperature": 0.0, "avg_logprob": -0.09271527013034685, "compression_ratio": 1.7425149700598803, "no_speech_prob": 0.005060028284788132}, {"id": 88, "seek": 38120, "start": 401.68, "end": 406.08, "text": " basically the, you know, this is the one paragraph version of this. So what's the next thing that", "tokens": [51388, 1936, 264, 11, 291, 458, 11, 341, 307, 264, 472, 18865, 3037, 295, 341, 13, 407, 437, 311, 264, 958, 551, 300, 51608], "temperature": 0.0, "avg_logprob": -0.09271527013034685, "compression_ratio": 1.7425149700598803, "no_speech_prob": 0.005060028284788132}, {"id": 89, "seek": 38120, "start": 406.08, "end": 410.88, "text": " they put a focus on? The next thing is temporal uniformity. What does this mean? Essentially,", "tokens": [51608, 436, 829, 257, 1879, 322, 30, 440, 958, 551, 307, 30881, 9452, 507, 13, 708, 775, 341, 914, 30, 23596, 11, 51848], "temperature": 0.0, "avg_logprob": -0.09271527013034685, "compression_ratio": 1.7425149700598803, "no_speech_prob": 0.005060028284788132}, {"id": 90, "seek": 41088, "start": 410.96, "end": 415.44, "text": " it means that at all time steps, so at all times, in terms of the algorithms running,", "tokens": [50368, 309, 1355, 300, 412, 439, 565, 4439, 11, 370, 412, 439, 1413, 11, 294, 2115, 295, 264, 14642, 2614, 11, 50592], "temperature": 0.0, "avg_logprob": -0.09652282851082938, "compression_ratio": 1.7431192660550459, "no_speech_prob": 0.0055544208735227585}, {"id": 91, "seek": 41088, "start": 415.44, "end": 419.76, "text": " everything needs to be the same. You can't have, and this again, goes back to what I had mentioned", "tokens": [50592, 1203, 2203, 281, 312, 264, 912, 13, 509, 393, 380, 362, 11, 293, 341, 797, 11, 1709, 646, 281, 437, 286, 632, 2835, 50808], "temperature": 0.0, "avg_logprob": -0.09652282851082938, "compression_ratio": 1.7431192660550459, "no_speech_prob": 0.0055544208735227585}, {"id": 92, "seek": 41088, "start": 419.76, "end": 425.28, "text": " earlier, you can't have a separate set of training data and testing data like you would have in", "tokens": [50808, 3071, 11, 291, 393, 380, 362, 257, 4994, 992, 295, 3097, 1412, 293, 4997, 1412, 411, 291, 576, 362, 294, 51084], "temperature": 0.0, "avg_logprob": -0.09652282851082938, "compression_ratio": 1.7431192660550459, "no_speech_prob": 0.0055544208735227585}, {"id": 93, "seek": 41088, "start": 425.28, "end": 428.96, "text": " supervised learning, or lots of people when they're evaluating reinforcement learning agents,", "tokens": [51084, 46533, 2539, 11, 420, 3195, 295, 561, 562, 436, 434, 27479, 29280, 2539, 12554, 11, 51268], "temperature": 0.0, "avg_logprob": -0.09652282851082938, "compression_ratio": 1.7431192660550459, "no_speech_prob": 0.0055544208735227585}, {"id": 94, "seek": 41088, "start": 428.96, "end": 433.92, "text": " like on the Atari 100k benchmark, I think it's pretty common to train with 100,000 steps of data,", "tokens": [51268, 411, 322, 264, 41381, 2319, 74, 18927, 11, 286, 519, 309, 311, 1238, 2689, 281, 3847, 365, 2319, 11, 1360, 4439, 295, 1412, 11, 51516], "temperature": 0.0, "avg_logprob": -0.09652282851082938, "compression_ratio": 1.7431192660550459, "no_speech_prob": 0.0055544208735227585}, {"id": 95, "seek": 41088, "start": 433.92, "end": 437.68, "text": " but then the actual testing is done on different data. So that would be a no, no here, right? And", "tokens": [51516, 457, 550, 264, 3539, 4997, 307, 1096, 322, 819, 1412, 13, 407, 300, 576, 312, 257, 572, 11, 572, 510, 11, 558, 30, 400, 51704], "temperature": 0.0, "avg_logprob": -0.09652282851082938, "compression_ratio": 1.7431192660550459, "no_speech_prob": 0.0055544208735227585}, {"id": 96, "seek": 43768, "start": 437.68, "end": 442.24, "text": " you might be like, oh, that's heresy, no training and testing, no separation. Well, yeah, but you", "tokens": [50364, 291, 1062, 312, 411, 11, 1954, 11, 300, 311, 720, 17823, 11, 572, 3097, 293, 4997, 11, 572, 14634, 13, 1042, 11, 1338, 11, 457, 291, 50592], "temperature": 0.0, "avg_logprob": -0.06596084139240321, "compression_ratio": 1.8196202531645569, "no_speech_prob": 0.018545065075159073}, {"id": 97, "seek": 43768, "start": 442.24, "end": 446.08, "text": " have to remember, we're working in the continual learning setting, which means that you can just", "tokens": [50592, 362, 281, 1604, 11, 321, 434, 1364, 294, 264, 1421, 901, 2539, 3287, 11, 597, 1355, 300, 291, 393, 445, 50784], "temperature": 0.0, "avg_logprob": -0.06596084139240321, "compression_ratio": 1.8196202531645569, "no_speech_prob": 0.018545065075159073}, {"id": 98, "seek": 43768, "start": 446.08, "end": 450.64, "text": " keep simulating your environment and getting new experiences. And well, if all those experiences", "tokens": [50784, 1066, 1034, 12162, 428, 2823, 293, 1242, 777, 5235, 13, 400, 731, 11, 498, 439, 729, 5235, 51012], "temperature": 0.0, "avg_logprob": -0.06596084139240321, "compression_ratio": 1.8196202531645569, "no_speech_prob": 0.018545065075159073}, {"id": 99, "seek": 43768, "start": 450.64, "end": 455.2, "text": " are the same, well, then maybe your environment just isn't a very interesting environment at that", "tokens": [51012, 366, 264, 912, 11, 731, 11, 550, 1310, 428, 2823, 445, 1943, 380, 257, 588, 1880, 2823, 412, 300, 51240], "temperature": 0.0, "avg_logprob": -0.06596084139240321, "compression_ratio": 1.8196202531645569, "no_speech_prob": 0.018545065075159073}, {"id": 100, "seek": 43768, "start": 455.2, "end": 458.4, "text": " point. So this is essentially talked about in the paper here, you know, they say no special", "tokens": [51240, 935, 13, 407, 341, 307, 4476, 2825, 466, 294, 264, 3035, 510, 11, 291, 458, 11, 436, 584, 572, 2121, 51400], "temperature": 0.0, "avg_logprob": -0.06596084139240321, "compression_ratio": 1.8196202531645569, "no_speech_prob": 0.018545065075159073}, {"id": 101, "seek": 43768, "start": 458.4, "end": 464.8, "text": " training periods, if information is provided, then it has to be the same information on every", "tokens": [51400, 3097, 13804, 11, 498, 1589, 307, 5649, 11, 550, 309, 575, 281, 312, 264, 912, 1589, 322, 633, 51720], "temperature": 0.0, "avg_logprob": -0.06596084139240321, "compression_ratio": 1.8196202531645569, "no_speech_prob": 0.018545065075159073}, {"id": 102, "seek": 46480, "start": 464.88, "end": 470.56, "text": " time step. Or if it learns to plan, then it has to plan on every time step. If the agent constructs", "tokens": [50368, 565, 1823, 13, 1610, 498, 309, 27152, 281, 1393, 11, 550, 309, 575, 281, 1393, 322, 633, 565, 1823, 13, 759, 264, 9461, 7690, 82, 50652], "temperature": 0.0, "avg_logprob": -0.10673963228861491, "compression_ratio": 1.8421052631578947, "no_speech_prob": 0.04083990305662155}, {"id": 103, "seek": 46480, "start": 470.56, "end": 475.6, "text": " its own representations or subtasks, then the meta learned out are the meta algorithms for", "tokens": [50652, 1080, 1065, 33358, 420, 7257, 296, 1694, 11, 550, 264, 19616, 3264, 484, 366, 264, 19616, 14642, 337, 50904], "temperature": 0.0, "avg_logprob": -0.10673963228861491, "compression_ratio": 1.8421052631578947, "no_speech_prob": 0.04083990305662155}, {"id": 104, "seek": 46480, "start": 475.6, "end": 480.64, "text": " constructing them and operating them operate on every time step. So you might be wondering like,", "tokens": [50904, 39969, 552, 293, 7447, 552, 9651, 322, 633, 565, 1823, 13, 407, 291, 1062, 312, 6359, 411, 11, 51156], "temperature": 0.0, "avg_logprob": -0.10673963228861491, "compression_ratio": 1.8421052631578947, "no_speech_prob": 0.04083990305662155}, {"id": 105, "seek": 46480, "start": 480.64, "end": 485.36, "text": " well, aren't on some time steps, maybe where things are more complicated, won't we maybe want to plan", "tokens": [51156, 731, 11, 3212, 380, 322, 512, 565, 4439, 11, 1310, 689, 721, 366, 544, 6179, 11, 1582, 380, 321, 1310, 528, 281, 1393, 51392], "temperature": 0.0, "avg_logprob": -0.10673963228861491, "compression_ratio": 1.8421052631578947, "no_speech_prob": 0.04083990305662155}, {"id": 106, "seek": 46480, "start": 485.36, "end": 490.32, "text": " more like look further into the future? Yeah, maybe we'll want to do that. But the idea here is that", "tokens": [51392, 544, 411, 574, 3052, 666, 264, 2027, 30, 865, 11, 1310, 321, 603, 528, 281, 360, 300, 13, 583, 264, 1558, 510, 307, 300, 51640], "temperature": 0.0, "avg_logprob": -0.10673963228861491, "compression_ratio": 1.8421052631578947, "no_speech_prob": 0.04083990305662155}, {"id": 107, "seek": 49032, "start": 490.32, "end": 495.59999999999997, "text": " should all be internal to the agent, as opposed for a human tweaking these parameters, right?", "tokens": [50364, 820, 439, 312, 6920, 281, 264, 9461, 11, 382, 8851, 337, 257, 1952, 6986, 2456, 613, 9834, 11, 558, 30, 50628], "temperature": 0.0, "avg_logprob": -0.07913360955580226, "compression_ratio": 1.7350746268656716, "no_speech_prob": 0.03845968842506409}, {"id": 108, "seek": 49032, "start": 495.59999999999997, "end": 500.24, "text": " You could have a meta learning algorithm that learns those types of things. And in general,", "tokens": [50628, 509, 727, 362, 257, 19616, 2539, 9284, 300, 27152, 729, 3467, 295, 721, 13, 400, 294, 2674, 11, 50860], "temperature": 0.0, "avg_logprob": -0.07913360955580226, "compression_ratio": 1.7350746268656716, "no_speech_prob": 0.03845968842506409}, {"id": 109, "seek": 49032, "start": 500.24, "end": 505.36, "text": " I think this, why do they do this? I think they actually say it fairly well. Their focus on this", "tokens": [50860, 286, 519, 341, 11, 983, 360, 436, 360, 341, 30, 286, 519, 436, 767, 584, 309, 6457, 731, 13, 6710, 1879, 322, 341, 51116], "temperature": 0.0, "avg_logprob": -0.07913360955580226, "compression_ratio": 1.7350746268656716, "no_speech_prob": 0.03845968842506409}, {"id": 110, "seek": 49032, "start": 505.36, "end": 511.52, "text": " is to leads us to interesting non stationary continuing environments, and in algorithms", "tokens": [51116, 307, 281, 6689, 505, 281, 1880, 2107, 30452, 9289, 12388, 11, 293, 294, 14642, 51424], "temperature": 0.0, "avg_logprob": -0.07913360955580226, "compression_ratio": 1.7350746268656716, "no_speech_prob": 0.03845968842506409}, {"id": 111, "seek": 49032, "start": 511.52, "end": 516.4, "text": " for continual learning and meta learning. So in one sense, I understand totally how this leads", "tokens": [51424, 337, 1421, 901, 2539, 293, 19616, 2539, 13, 407, 294, 472, 2020, 11, 286, 1223, 3879, 577, 341, 6689, 51668], "temperature": 0.0, "avg_logprob": -0.07913360955580226, "compression_ratio": 1.7350746268656716, "no_speech_prob": 0.03845968842506409}, {"id": 112, "seek": 51640, "start": 516.48, "end": 519.68, "text": " them. I have way too much yellow on this page, I'll start using something else.", "tokens": [50368, 552, 13, 286, 362, 636, 886, 709, 5566, 322, 341, 3028, 11, 286, 603, 722, 1228, 746, 1646, 13, 50528], "temperature": 0.0, "avg_logprob": -0.1206449720594618, "compression_ratio": 1.8821548821548821, "no_speech_prob": 0.006487863603979349}, {"id": 113, "seek": 51640, "start": 520.56, "end": 524.9599999999999, "text": " So algorithms for meta learning, this makes sense as I just mentioned, right? You need meta", "tokens": [50572, 407, 14642, 337, 19616, 2539, 11, 341, 1669, 2020, 382, 286, 445, 2835, 11, 558, 30, 509, 643, 19616, 50792], "temperature": 0.0, "avg_logprob": -0.1206449720594618, "compression_ratio": 1.8821548821548821, "no_speech_prob": 0.006487863603979349}, {"id": 114, "seek": 51640, "start": 524.9599999999999, "end": 528.0799999999999, "text": " learning when you can't have a human tweaking these things, you should have an algorithm to", "tokens": [50792, 2539, 562, 291, 393, 380, 362, 257, 1952, 6986, 2456, 613, 721, 11, 291, 820, 362, 364, 9284, 281, 50948], "temperature": 0.0, "avg_logprob": -0.1206449720594618, "compression_ratio": 1.8821548821548821, "no_speech_prob": 0.006487863603979349}, {"id": 115, "seek": 51640, "start": 528.0799999999999, "end": 532.56, "text": " that and meta learning is one way to do that. On the other hand, I don't see how this really leads", "tokens": [50948, 300, 293, 19616, 2539, 307, 472, 636, 281, 360, 300, 13, 1282, 264, 661, 1011, 11, 286, 500, 380, 536, 577, 341, 534, 6689, 51172], "temperature": 0.0, "avg_logprob": -0.1206449720594618, "compression_ratio": 1.8821548821548821, "no_speech_prob": 0.006487863603979349}, {"id": 116, "seek": 51640, "start": 532.56, "end": 536.72, "text": " to continuing environments or like continual learning, I would think it would be the other way", "tokens": [51172, 281, 9289, 12388, 420, 411, 1421, 901, 2539, 11, 286, 576, 519, 309, 576, 312, 264, 661, 636, 51380], "temperature": 0.0, "avg_logprob": -0.1206449720594618, "compression_ratio": 1.8821548821548821, "no_speech_prob": 0.006487863603979349}, {"id": 117, "seek": 51640, "start": 536.72, "end": 543.12, "text": " around, right? You have continually like continual learning. And hence, you're going to want to focus", "tokens": [51380, 926, 11, 558, 30, 509, 362, 22277, 411, 1421, 901, 2539, 13, 400, 16678, 11, 291, 434, 516, 281, 528, 281, 1879, 51700], "temperature": 0.0, "avg_logprob": -0.1206449720594618, "compression_ratio": 1.8821548821548821, "no_speech_prob": 0.006487863603979349}, {"id": 118, "seek": 54312, "start": 543.12, "end": 546.72, "text": " on temporal uniformity, but maybe there's something I'm not seeing here. And another reason", "tokens": [50364, 322, 30881, 9452, 507, 11, 457, 1310, 456, 311, 746, 286, 478, 406, 2577, 510, 13, 400, 1071, 1778, 50544], "temperature": 0.0, "avg_logprob": -0.10365516773975679, "compression_ratio": 1.7341389728096677, "no_speech_prob": 0.02675660327076912}, {"id": 119, "seek": 54312, "start": 546.72, "end": 550.96, "text": " they mentioned this is that keeping everything temporarily uniform, it reduces the degrees of", "tokens": [50544, 436, 2835, 341, 307, 300, 5145, 1203, 23750, 9452, 11, 309, 18081, 264, 5310, 295, 50756], "temperature": 0.0, "avg_logprob": -0.10365516773975679, "compression_ratio": 1.7341389728096677, "no_speech_prob": 0.02675660327076912}, {"id": 120, "seek": 54312, "start": 550.96, "end": 555.52, "text": " freedom and shrinks the agent design space, which to be fair, they think there's a decent point,", "tokens": [50756, 5645, 293, 9884, 16431, 264, 9461, 1715, 1901, 11, 597, 281, 312, 3143, 11, 436, 519, 456, 311, 257, 8681, 935, 11, 50984], "temperature": 0.0, "avg_logprob": -0.10365516773975679, "compression_ratio": 1.7341389728096677, "no_speech_prob": 0.02675660327076912}, {"id": 121, "seek": 54312, "start": 555.52, "end": 561.84, "text": " really have plenty of things to look into. There's no reason to, you know, forcibly make that higher", "tokens": [50984, 534, 362, 7140, 295, 721, 281, 574, 666, 13, 821, 311, 572, 1778, 281, 11, 291, 458, 11, 337, 537, 25021, 652, 300, 2946, 51300], "temperature": 0.0, "avg_logprob": -0.10365516773975679, "compression_ratio": 1.7341389728096677, "no_speech_prob": 0.02675660327076912}, {"id": 122, "seek": 54312, "start": 561.84, "end": 565.84, "text": " when we could simplify the space. Then if we go down a bit more, this is one thing we touched on", "tokens": [51300, 562, 321, 727, 20460, 264, 1901, 13, 1396, 498, 321, 352, 760, 257, 857, 544, 11, 341, 307, 472, 551, 321, 9828, 322, 51500], "temperature": 0.0, "avg_logprob": -0.10365516773975679, "compression_ratio": 1.7341389728096677, "no_speech_prob": 0.02675660327076912}, {"id": 123, "seek": 54312, "start": 565.84, "end": 572.16, "text": " earlier. And this is also from the bitter lesson. But the third distinguishing factor is it's", "tokens": [51500, 3071, 13, 400, 341, 307, 611, 490, 264, 13871, 6898, 13, 583, 264, 2636, 11365, 3807, 5952, 307, 309, 311, 51816], "temperature": 0.0, "avg_logprob": -0.10365516773975679, "compression_ratio": 1.7341389728096677, "no_speech_prob": 0.02675660327076912}, {"id": 124, "seek": 57216, "start": 572.24, "end": 578.16, "text": " cognizance of computational considerations and Moore's law. I do think lots of other, and this", "tokens": [50368, 11786, 590, 719, 295, 28270, 24070, 293, 21644, 311, 2101, 13, 286, 360, 519, 3195, 295, 661, 11, 293, 341, 50664], "temperature": 0.0, "avg_logprob": -0.11517275817005347, "compression_ratio": 1.7584097859327217, "no_speech_prob": 0.00538463331758976}, {"id": 125, "seek": 57216, "start": 578.16, "end": 582.88, "text": " essentially, all this means, right, is compute computations getting stronger or more, we have", "tokens": [50664, 4476, 11, 439, 341, 1355, 11, 558, 11, 307, 14722, 2807, 763, 1242, 7249, 420, 544, 11, 321, 362, 50900], "temperature": 0.0, "avg_logprob": -0.11517275817005347, "compression_ratio": 1.7584097859327217, "no_speech_prob": 0.00538463331758976}, {"id": 126, "seek": 57216, "start": 582.88, "end": 587.92, "text": " more of it, it's more efficient. So we want to be able to take, make use of that. So we can't have", "tokens": [50900, 544, 295, 309, 11, 309, 311, 544, 7148, 13, 407, 321, 528, 281, 312, 1075, 281, 747, 11, 652, 764, 295, 300, 13, 407, 321, 393, 380, 362, 51152], "temperature": 0.0, "avg_logprob": -0.11517275817005347, "compression_ratio": 1.7584097859327217, "no_speech_prob": 0.00538463331758976}, {"id": 127, "seek": 57216, "start": 587.92, "end": 591.8399999999999, "text": " anything that's not going to scale. And I think this has been getting better over the years,", "tokens": [51152, 1340, 300, 311, 406, 516, 281, 4373, 13, 400, 286, 519, 341, 575, 668, 1242, 1101, 670, 264, 924, 11, 51348], "temperature": 0.0, "avg_logprob": -0.11517275817005347, "compression_ratio": 1.7584097859327217, "no_speech_prob": 0.00538463331758976}, {"id": 128, "seek": 57216, "start": 591.8399999999999, "end": 597.04, "text": " my personal opinion, it looks like lots of people are working in areas that do or on methods that", "tokens": [51348, 452, 2973, 4800, 11, 309, 1542, 411, 3195, 295, 561, 366, 1364, 294, 3179, 300, 360, 420, 322, 7150, 300, 51608], "temperature": 0.0, "avg_logprob": -0.11517275817005347, "compression_ratio": 1.7584097859327217, "no_speech_prob": 0.00538463331758976}, {"id": 129, "seek": 57216, "start": 597.04, "end": 601.8399999999999, "text": " do scale fairly well. Deep learning is one great example of this rate. And basically, everyone's", "tokens": [51608, 360, 4373, 6457, 731, 13, 14895, 2539, 307, 472, 869, 1365, 295, 341, 3314, 13, 400, 1936, 11, 1518, 311, 51848], "temperature": 0.0, "avg_logprob": -0.11517275817005347, "compression_ratio": 1.7584097859327217, "no_speech_prob": 0.00538463331758976}, {"id": 130, "seek": 60184, "start": 601.84, "end": 607.76, "text": " using deep learning now. Oh, they even namedropped the bitter lesson. Okay, lovely. So then let's", "tokens": [50364, 1228, 2452, 2539, 586, 13, 876, 11, 436, 754, 4926, 340, 3320, 264, 13871, 6898, 13, 1033, 11, 7496, 13, 407, 550, 718, 311, 50660], "temperature": 0.0, "avg_logprob": -0.09918737411499023, "compression_ratio": 1.8566775244299674, "no_speech_prob": 0.0021155935246497393}, {"id": 131, "seek": 60184, "start": 607.76, "end": 611.2, "text": " move on to the fourth. And I think this is the final one. The fourth distinguishing feature", "tokens": [50660, 1286, 322, 281, 264, 6409, 13, 400, 286, 519, 341, 307, 264, 2572, 472, 13, 440, 6409, 11365, 3807, 4111, 50832], "temperature": 0.0, "avg_logprob": -0.09918737411499023, "compression_ratio": 1.8566775244299674, "no_speech_prob": 0.0021155935246497393}, {"id": 132, "seek": 60184, "start": 611.2, "end": 616.8000000000001, "text": " is the focus on the special case, or is it the special case in which the environment includes", "tokens": [50832, 307, 264, 1879, 322, 264, 2121, 1389, 11, 420, 307, 309, 264, 2121, 1389, 294, 597, 264, 2823, 5974, 51112], "temperature": 0.0, "avg_logprob": -0.09918737411499023, "compression_ratio": 1.8566775244299674, "no_speech_prob": 0.0021155935246497393}, {"id": 133, "seek": 60184, "start": 616.8000000000001, "end": 622.48, "text": " other intelligent agents. Now, what does this mean? It doesn't mean that there need to be other", "tokens": [51112, 661, 13232, 12554, 13, 823, 11, 437, 775, 341, 914, 30, 467, 1177, 380, 914, 300, 456, 643, 281, 312, 661, 51396], "temperature": 0.0, "avg_logprob": -0.09918737411499023, "compression_ratio": 1.8566775244299674, "no_speech_prob": 0.0021155935246497393}, {"id": 134, "seek": 60184, "start": 622.48, "end": 627.2800000000001, "text": " intelligent agents, but rather that intelligent agents. So like if I'm, you know, modeling a", "tokens": [51396, 13232, 12554, 11, 457, 2831, 300, 13232, 12554, 13, 407, 411, 498, 286, 478, 11, 291, 458, 11, 15983, 257, 51636], "temperature": 0.0, "avg_logprob": -0.09918737411499023, "compression_ratio": 1.8566775244299674, "no_speech_prob": 0.0021155935246497393}, {"id": 135, "seek": 60184, "start": 627.2800000000001, "end": 631.36, "text": " robot that's interacting with humans, you know, there's, there's nothing fancy that I do to model", "tokens": [51636, 7881, 300, 311, 18017, 365, 6255, 11, 291, 458, 11, 456, 311, 11, 456, 311, 1825, 10247, 300, 286, 360, 281, 2316, 51840], "temperature": 0.0, "avg_logprob": -0.09918737411499023, "compression_ratio": 1.8566775244299674, "no_speech_prob": 0.0021155935246497393}, {"id": 136, "seek": 63136, "start": 631.44, "end": 635.04, "text": " the fact that there's humans in the world, rather, they're just part of the environment.", "tokens": [50368, 264, 1186, 300, 456, 311, 6255, 294, 264, 1002, 11, 2831, 11, 436, 434, 445, 644, 295, 264, 2823, 13, 50548], "temperature": 0.0, "avg_logprob": -0.08944019949509323, "compression_ratio": 1.9321533923303835, "no_speech_prob": 0.00609645014628768}, {"id": 137, "seek": 63136, "start": 635.04, "end": 639.12, "text": " And I need to be cognizant, and I think they say this right here, you'd be cognizant that the", "tokens": [50548, 400, 286, 643, 281, 312, 11786, 590, 394, 11, 293, 286, 519, 436, 584, 341, 558, 510, 11, 291, 1116, 312, 11786, 590, 394, 300, 264, 50752], "temperature": 0.0, "avg_logprob": -0.08944019949509323, "compression_ratio": 1.9321533923303835, "no_speech_prob": 0.00609645014628768}, {"id": 138, "seek": 63136, "start": 639.12, "end": 642.8000000000001, "text": " environment may behave differently. And this might sound weird, but the environment is essentially,", "tokens": [50752, 2823, 815, 15158, 7614, 13, 400, 341, 1062, 1626, 3657, 11, 457, 264, 2823, 307, 4476, 11, 50936], "temperature": 0.0, "avg_logprob": -0.08944019949509323, "compression_ratio": 1.9321533923303835, "no_speech_prob": 0.00609645014628768}, {"id": 139, "seek": 63136, "start": 642.8000000000001, "end": 647.2, "text": " you can think of it as, you know, a living, breathing thing, or at least it could be in", "tokens": [50936, 291, 393, 519, 295, 309, 382, 11, 291, 458, 11, 257, 2647, 11, 9570, 551, 11, 420, 412, 1935, 309, 727, 312, 294, 51156], "temperature": 0.0, "avg_logprob": -0.08944019949509323, "compression_ratio": 1.9321533923303835, "no_speech_prob": 0.00609645014628768}, {"id": 140, "seek": 63136, "start": 647.2, "end": 651.92, "text": " response to your own actions, right? So what you do might make the environment behave differently.", "tokens": [51156, 4134, 281, 428, 1065, 5909, 11, 558, 30, 407, 437, 291, 360, 1062, 652, 264, 2823, 15158, 7614, 13, 51392], "temperature": 0.0, "avg_logprob": -0.08944019949509323, "compression_ratio": 1.9321533923303835, "no_speech_prob": 0.00609645014628768}, {"id": 141, "seek": 63136, "start": 651.92, "end": 655.52, "text": " And that's, you know, we shouldn't do anything special for these other agents. To be honest,", "tokens": [51392, 400, 300, 311, 11, 291, 458, 11, 321, 4659, 380, 360, 1340, 2121, 337, 613, 661, 12554, 13, 1407, 312, 3245, 11, 51572], "temperature": 0.0, "avg_logprob": -0.08944019949509323, "compression_ratio": 1.9321533923303835, "no_speech_prob": 0.00609645014628768}, {"id": 142, "seek": 63136, "start": 655.52, "end": 658.32, "text": " I don't think they even need to put this here. Because I think this follows under the bullet", "tokens": [51572, 286, 500, 380, 519, 436, 754, 643, 281, 829, 341, 510, 13, 1436, 286, 519, 341, 10002, 833, 264, 11632, 51712], "temperature": 0.0, "avg_logprob": -0.08944019949509323, "compression_ratio": 1.9321533923303835, "no_speech_prob": 0.00609645014628768}, {"id": 143, "seek": 65832, "start": 658.32, "end": 663.6800000000001, "text": " point of using no human experience, right? The idea that we have other intelligent or like", "tokens": [50364, 935, 295, 1228, 572, 1952, 1752, 11, 558, 30, 440, 1558, 300, 321, 362, 661, 13232, 420, 411, 50632], "temperature": 0.0, "avg_logprob": -0.09413265846144984, "compression_ratio": 1.8415841584158417, "no_speech_prob": 0.025954514741897583}, {"id": 144, "seek": 65832, "start": 663.6800000000001, "end": 668.08, "text": " conscious agents, that is something that we're sort of embedding our human biases into this,", "tokens": [50632, 6648, 12554, 11, 300, 307, 746, 300, 321, 434, 1333, 295, 12240, 3584, 527, 1952, 32152, 666, 341, 11, 50852], "temperature": 0.0, "avg_logprob": -0.09413265846144984, "compression_ratio": 1.8415841584158417, "no_speech_prob": 0.025954514741897583}, {"id": 145, "seek": 65832, "start": 668.08, "end": 673.7600000000001, "text": " right? Technically, we can't even prove other people are conscious. So I don't know, I don't", "tokens": [50852, 558, 30, 42494, 11, 321, 393, 380, 754, 7081, 661, 561, 366, 6648, 13, 407, 286, 500, 380, 458, 11, 286, 500, 380, 51136], "temperature": 0.0, "avg_logprob": -0.09413265846144984, "compression_ratio": 1.8415841584158417, "no_speech_prob": 0.025954514741897583}, {"id": 146, "seek": 65832, "start": 673.7600000000001, "end": 678.5600000000001, "text": " think this is so weird, but I guess lots of places that do work with multi-agent settings, they do", "tokens": [51136, 519, 341, 307, 370, 3657, 11, 457, 286, 2041, 3195, 295, 3190, 300, 360, 589, 365, 4825, 12, 559, 317, 6257, 11, 436, 360, 51376], "temperature": 0.0, "avg_logprob": -0.09413265846144984, "compression_ratio": 1.8415841584158417, "no_speech_prob": 0.025954514741897583}, {"id": 147, "seek": 65832, "start": 678.5600000000001, "end": 683.2, "text": " do this sort of thing where they're explicitly trying to model other agents. So that would be a", "tokens": [51376, 360, 341, 1333, 295, 551, 689, 436, 434, 20803, 1382, 281, 2316, 661, 12554, 13, 407, 300, 576, 312, 257, 51608], "temperature": 0.0, "avg_logprob": -0.09413265846144984, "compression_ratio": 1.8415841584158417, "no_speech_prob": 0.025954514741897583}, {"id": 148, "seek": 65832, "start": 683.2, "end": 687.0400000000001, "text": " no-no in this sort of setting, or at least that's, that's what they want to work with.", "tokens": [51608, 572, 12, 1771, 294, 341, 1333, 295, 3287, 11, 420, 412, 1935, 300, 311, 11, 300, 311, 437, 436, 528, 281, 589, 365, 13, 51800], "temperature": 0.0, "avg_logprob": -0.09413265846144984, "compression_ratio": 1.8415841584158417, "no_speech_prob": 0.025954514741897583}, {"id": 149, "seek": 68704, "start": 687.5999999999999, "end": 691.8399999999999, "text": " So if we scroll down, we're almost to the research plan. I swear we're getting there.", "tokens": [50392, 407, 498, 321, 11369, 760, 11, 321, 434, 1920, 281, 264, 2132, 1393, 13, 286, 11902, 321, 434, 1242, 456, 13, 50604], "temperature": 0.0, "avg_logprob": -0.0778527226008422, "compression_ratio": 1.787781350482315, "no_speech_prob": 0.0005527631146833301}, {"id": 150, "seek": 68704, "start": 691.8399999999999, "end": 695.28, "text": " We're going to spend a good bit of time on that once we get there. But the last thing we need to", "tokens": [50604, 492, 434, 516, 281, 3496, 257, 665, 857, 295, 565, 322, 300, 1564, 321, 483, 456, 13, 583, 264, 1036, 551, 321, 643, 281, 50776], "temperature": 0.0, "avg_logprob": -0.0778527226008422, "compression_ratio": 1.787781350482315, "no_speech_prob": 0.0005527631146833301}, {"id": 151, "seek": 68704, "start": 695.28, "end": 700.56, "text": " touch on is the base agent. So I believe this is what they call the common model of the intelligent", "tokens": [50776, 2557, 322, 307, 264, 3096, 9461, 13, 407, 286, 1697, 341, 307, 437, 436, 818, 264, 2689, 2316, 295, 264, 13232, 51040], "temperature": 0.0, "avg_logprob": -0.0778527226008422, "compression_ratio": 1.787781350482315, "no_speech_prob": 0.0005527631146833301}, {"id": 152, "seek": 68704, "start": 700.56, "end": 704.7199999999999, "text": " agent, as you're looking at right here. And they say that this is used in a few things, like", "tokens": [51040, 9461, 11, 382, 291, 434, 1237, 412, 558, 510, 13, 400, 436, 584, 300, 341, 307, 1143, 294, 257, 1326, 721, 11, 411, 51248], "temperature": 0.0, "avg_logprob": -0.0778527226008422, "compression_ratio": 1.787781350482315, "no_speech_prob": 0.0005527631146833301}, {"id": 153, "seek": 68704, "start": 705.52, "end": 710.56, "text": " areas like economics have similar ideas. But the idea is that we want to start from some base", "tokens": [51288, 3179, 411, 14564, 362, 2531, 3487, 13, 583, 264, 1558, 307, 300, 321, 528, 281, 722, 490, 512, 3096, 51540], "temperature": 0.0, "avg_logprob": -0.0778527226008422, "compression_ratio": 1.787781350482315, "no_speech_prob": 0.0005527631146833301}, {"id": 154, "seek": 68704, "start": 710.56, "end": 714.4, "text": " that we can maybe agree on, not that it's necessarily the best or something like that,", "tokens": [51540, 300, 321, 393, 1310, 3986, 322, 11, 406, 300, 309, 311, 4725, 264, 1151, 420, 746, 411, 300, 11, 51732], "temperature": 0.0, "avg_logprob": -0.0778527226008422, "compression_ratio": 1.787781350482315, "no_speech_prob": 0.0005527631146833301}, {"id": 155, "seek": 71440, "start": 714.4, "end": 719.92, "text": " but that this is a reasonable base that will inform the rest of the research. So what's", "tokens": [50364, 457, 300, 341, 307, 257, 10585, 3096, 300, 486, 1356, 264, 1472, 295, 264, 2132, 13, 407, 437, 311, 50640], "temperature": 0.0, "avg_logprob": -0.07795598453148864, "compression_ratio": 1.8360655737704918, "no_speech_prob": 0.0007793385884724557}, {"id": 156, "seek": 71440, "start": 719.92, "end": 722.88, "text": " essentially happening here is if we step through this, we start with an observation from the", "tokens": [50640, 4476, 2737, 510, 307, 498, 321, 1823, 807, 341, 11, 321, 722, 365, 364, 14816, 490, 264, 50788], "temperature": 0.0, "avg_logprob": -0.07795598453148864, "compression_ratio": 1.8360655737704918, "no_speech_prob": 0.0007793385884724557}, {"id": 157, "seek": 71440, "start": 722.88, "end": 727.68, "text": " environment, pretty standard, then it goes into this perception. So perception, and this is going", "tokens": [50788, 2823, 11, 1238, 3832, 11, 550, 309, 1709, 666, 341, 12860, 13, 407, 12860, 11, 293, 341, 307, 516, 51028], "temperature": 0.0, "avg_logprob": -0.07795598453148864, "compression_ratio": 1.8360655737704918, "no_speech_prob": 0.0007793385884724557}, {"id": 158, "seek": 71440, "start": 727.68, "end": 731.92, "text": " to be a recurrent thing, right? You also take in the last action, and you can see the output", "tokens": [51028, 281, 312, 257, 18680, 1753, 551, 11, 558, 30, 509, 611, 747, 294, 264, 1036, 3069, 11, 293, 291, 393, 536, 264, 5598, 51240], "temperature": 0.0, "avg_logprob": -0.07795598453148864, "compression_ratio": 1.8360655737704918, "no_speech_prob": 0.0007793385884724557}, {"id": 159, "seek": 71440, "start": 731.92, "end": 736.16, "text": " of perception feeds back into here. So this is recurrent. So this would be keeping what they", "tokens": [51240, 295, 12860, 23712, 646, 666, 510, 13, 407, 341, 307, 18680, 1753, 13, 407, 341, 576, 312, 5145, 437, 436, 51452], "temperature": 0.0, "avg_logprob": -0.07795598453148864, "compression_ratio": 1.8360655737704918, "no_speech_prob": 0.0007793385884724557}, {"id": 160, "seek": 71440, "start": 736.16, "end": 740.48, "text": " like to call the agent state, which is like the agents, some people call this the belief state.", "tokens": [51452, 411, 281, 818, 264, 9461, 1785, 11, 597, 307, 411, 264, 12554, 11, 512, 561, 818, 341, 264, 7107, 1785, 13, 51668], "temperature": 0.0, "avg_logprob": -0.07795598453148864, "compression_ratio": 1.8360655737704918, "no_speech_prob": 0.0007793385884724557}, {"id": 161, "seek": 74048, "start": 740.48, "end": 745.36, "text": " It's maybe what the agent perceives and wants to remember about the world to do whatever it needs", "tokens": [50364, 467, 311, 1310, 437, 264, 9461, 9016, 1539, 293, 2738, 281, 1604, 466, 264, 1002, 281, 360, 2035, 309, 2203, 50608], "temperature": 0.0, "avg_logprob": -0.09398179421058069, "compression_ratio": 1.8170347003154574, "no_speech_prob": 0.0016484224470332265}, {"id": 162, "seek": 74048, "start": 745.36, "end": 752.0, "text": " to do. It has reactive policies. So the policies are what transform these observations into actions.", "tokens": [50608, 281, 360, 13, 467, 575, 28897, 7657, 13, 407, 264, 7657, 366, 437, 4088, 613, 18163, 666, 5909, 13, 50940], "temperature": 0.0, "avg_logprob": -0.09398179421058069, "compression_ratio": 1.8170347003154574, "no_speech_prob": 0.0016484224470332265}, {"id": 163, "seek": 74048, "start": 752.0, "end": 756.0, "text": " It has a value function. This is a normal value function in reinforcement learning, right?", "tokens": [50940, 467, 575, 257, 2158, 2445, 13, 639, 307, 257, 2710, 2158, 2445, 294, 29280, 2539, 11, 558, 30, 51140], "temperature": 0.0, "avg_logprob": -0.09398179421058069, "compression_ratio": 1.8170347003154574, "no_speech_prob": 0.0016484224470332265}, {"id": 164, "seek": 74048, "start": 756.8000000000001, "end": 760.72, "text": " It essentially assigns value to certain states, which helps you do things like credit assignment,", "tokens": [51180, 467, 4476, 6269, 82, 2158, 281, 1629, 4368, 11, 597, 3665, 291, 360, 721, 411, 5397, 15187, 11, 51376], "temperature": 0.0, "avg_logprob": -0.09398179421058069, "compression_ratio": 1.8170347003154574, "no_speech_prob": 0.0016484224470332265}, {"id": 165, "seek": 74048, "start": 760.72, "end": 765.36, "text": " and it has a transition model, which allows you to do, as you can see, planning and planning", "tokens": [51376, 293, 309, 575, 257, 6034, 2316, 11, 597, 4045, 291, 281, 360, 11, 382, 291, 393, 536, 11, 5038, 293, 5038, 51608], "temperature": 0.0, "avg_logprob": -0.09398179421058069, "compression_ratio": 1.8170347003154574, "no_speech_prob": 0.0016484224470332265}, {"id": 166, "seek": 74048, "start": 765.36, "end": 769.36, "text": " means that you can more efficiently, you know, imagine scenarios in your head, not have to play", "tokens": [51608, 1355, 300, 291, 393, 544, 19621, 11, 291, 458, 11, 3811, 15077, 294, 428, 1378, 11, 406, 362, 281, 862, 51808], "temperature": 0.0, "avg_logprob": -0.09398179421058069, "compression_ratio": 1.8170347003154574, "no_speech_prob": 0.0016484224470332265}, {"id": 167, "seek": 76936, "start": 769.36, "end": 773.92, "text": " through them in real life. And then that means that you can be a lot more sample efficient and", "tokens": [50364, 807, 552, 294, 957, 993, 13, 400, 550, 300, 1355, 300, 291, 393, 312, 257, 688, 544, 6889, 7148, 293, 50592], "temperature": 0.0, "avg_logprob": -0.09746384778559603, "compression_ratio": 1.8276923076923077, "no_speech_prob": 0.0019266611197963357}, {"id": 168, "seek": 76936, "start": 773.92, "end": 778.4, "text": " potentially have other benefits too. One question, and this is a question I used to have, is why have", "tokens": [50592, 7263, 362, 661, 5311, 886, 13, 1485, 1168, 11, 293, 341, 307, 257, 1168, 286, 1143, 281, 362, 11, 307, 983, 362, 50816], "temperature": 0.0, "avg_logprob": -0.09746384778559603, "compression_ratio": 1.8276923076923077, "no_speech_prob": 0.0019266611197963357}, {"id": 169, "seek": 76936, "start": 778.4, "end": 783.12, "text": " these things? They are, they're one, two, three, four things. Why not other things, right? We could", "tokens": [50816, 613, 721, 30, 814, 366, 11, 436, 434, 472, 11, 732, 11, 1045, 11, 1451, 721, 13, 1545, 406, 661, 721, 11, 558, 30, 492, 727, 51052], "temperature": 0.0, "avg_logprob": -0.09746384778559603, "compression_ratio": 1.8276923076923077, "no_speech_prob": 0.0019266611197963357}, {"id": 170, "seek": 76936, "start": 783.12, "end": 788.72, "text": " have, for example, a different portion here saying we want to model like other agents. I mean, we just", "tokens": [51052, 362, 11, 337, 1365, 11, 257, 819, 8044, 510, 1566, 321, 528, 281, 2316, 411, 661, 12554, 13, 286, 914, 11, 321, 445, 51332], "temperature": 0.0, "avg_logprob": -0.09746384778559603, "compression_ratio": 1.8276923076923077, "no_speech_prob": 0.0019266611197963357}, {"id": 171, "seek": 76936, "start": 788.72, "end": 792.5600000000001, "text": " said like this, this is not good, we shouldn't do this, but why not? Why are these other things", "tokens": [51332, 848, 411, 341, 11, 341, 307, 406, 665, 11, 321, 4659, 380, 360, 341, 11, 457, 983, 406, 30, 1545, 366, 613, 661, 721, 51524], "temperature": 0.0, "avg_logprob": -0.09746384778559603, "compression_ratio": 1.8276923076923077, "no_speech_prob": 0.0019266611197963357}, {"id": 172, "seek": 76936, "start": 792.5600000000001, "end": 797.44, "text": " okay to have? And the difference here, I don't think they mentioned the paper. But the difference,", "tokens": [51524, 1392, 281, 362, 30, 400, 264, 2649, 510, 11, 286, 500, 380, 519, 436, 2835, 264, 3035, 13, 583, 264, 2649, 11, 51768], "temperature": 0.0, "avg_logprob": -0.09746384778559603, "compression_ratio": 1.8276923076923077, "no_speech_prob": 0.0019266611197963357}, {"id": 173, "seek": 79744, "start": 797.44, "end": 802.24, "text": " I believe they would say, is that these four things that they have here, they're not domain", "tokens": [50364, 286, 1697, 436, 576, 584, 11, 307, 300, 613, 1451, 721, 300, 436, 362, 510, 11, 436, 434, 406, 9274, 50604], "temperature": 0.0, "avg_logprob": -0.0885399995026765, "compression_ratio": 1.923841059602649, "no_speech_prob": 0.0037071562837809324}, {"id": 174, "seek": 79744, "start": 802.24, "end": 806.32, "text": " specific. These should essentially work in any domain, they're general, they're very general,", "tokens": [50604, 2685, 13, 1981, 820, 4476, 589, 294, 604, 9274, 11, 436, 434, 2674, 11, 436, 434, 588, 2674, 11, 50808], "temperature": 0.0, "avg_logprob": -0.0885399995026765, "compression_ratio": 1.923841059602649, "no_speech_prob": 0.0037071562837809324}, {"id": 175, "seek": 79744, "start": 806.32, "end": 810.32, "text": " whereas modeling other agents, well, you might not always need to do this in the way in which you", "tokens": [50808, 9735, 15983, 661, 12554, 11, 731, 11, 291, 1062, 406, 1009, 643, 281, 360, 341, 294, 264, 636, 294, 597, 291, 51008], "temperature": 0.0, "avg_logprob": -0.0885399995026765, "compression_ratio": 1.923841059602649, "no_speech_prob": 0.0037071562837809324}, {"id": 176, "seek": 79744, "start": 810.32, "end": 814.32, "text": " might do this will probably change drastically, depending on what types of agents you're working", "tokens": [51008, 1062, 360, 341, 486, 1391, 1319, 29673, 11, 5413, 322, 437, 3467, 295, 12554, 291, 434, 1364, 51208], "temperature": 0.0, "avg_logprob": -0.0885399995026765, "compression_ratio": 1.923841059602649, "no_speech_prob": 0.0037071562837809324}, {"id": 177, "seek": 79744, "start": 814.32, "end": 818.72, "text": " with. So for that reason, these four things stay and other things don't quite make the cut. It has", "tokens": [51208, 365, 13, 407, 337, 300, 1778, 11, 613, 1451, 721, 1754, 293, 661, 721, 500, 380, 1596, 652, 264, 1723, 13, 467, 575, 51428], "temperature": 0.0, "avg_logprob": -0.0885399995026765, "compression_ratio": 1.923841059602649, "no_speech_prob": 0.0037071562837809324}, {"id": 178, "seek": 79744, "start": 818.72, "end": 823.6800000000001, "text": " to do with the generality of these four components. Another thing you might mention is that the year,", "tokens": [51428, 281, 360, 365, 264, 1337, 1860, 295, 613, 1451, 6677, 13, 3996, 551, 291, 1062, 2152, 307, 300, 264, 1064, 11, 51676], "temperature": 0.0, "avg_logprob": -0.0885399995026765, "compression_ratio": 1.923841059602649, "no_speech_prob": 0.0037071562837809324}, {"id": 179, "seek": 82368, "start": 823.76, "end": 828.8, "text": " there are s's right here. So reactive policies, value functions, that is something we'll get into.", "tokens": [50368, 456, 366, 262, 311, 558, 510, 13, 407, 28897, 7657, 11, 2158, 6828, 11, 300, 307, 746, 321, 603, 483, 666, 13, 50620], "temperature": 0.0, "avg_logprob": -0.10613253998429809, "compression_ratio": 1.8471337579617835, "no_speech_prob": 0.07368412613868713}, {"id": 180, "seek": 82368, "start": 828.8, "end": 832.9599999999999, "text": " These don't necessarily need to have one policy or one value function, we can work with multiple.", "tokens": [50620, 1981, 500, 380, 4725, 643, 281, 362, 472, 3897, 420, 472, 2158, 2445, 11, 321, 393, 589, 365, 3866, 13, 50828], "temperature": 0.0, "avg_logprob": -0.10613253998429809, "compression_ratio": 1.8471337579617835, "no_speech_prob": 0.07368412613868713}, {"id": 181, "seek": 82368, "start": 832.9599999999999, "end": 838.0, "text": " And that is one of the steps of the plan that we'll get into. Speaking of the plan, we are finally", "tokens": [50828, 400, 300, 307, 472, 295, 264, 4439, 295, 264, 1393, 300, 321, 603, 483, 666, 13, 13069, 295, 264, 1393, 11, 321, 366, 2721, 51080], "temperature": 0.0, "avg_logprob": -0.10613253998429809, "compression_ratio": 1.8471337579617835, "no_speech_prob": 0.07368412613868713}, {"id": 182, "seek": 82368, "start": 838.0, "end": 843.68, "text": " about there. So here we go. A roadmap to an AI prototype. So here are 12 steps. I'm going to", "tokens": [51080, 466, 456, 13, 407, 510, 321, 352, 13, 316, 35738, 281, 364, 7318, 19475, 13, 407, 510, 366, 2272, 4439, 13, 286, 478, 516, 281, 51364], "temperature": 0.0, "avg_logprob": -0.10613253998429809, "compression_ratio": 1.8471337579617835, "no_speech_prob": 0.07368412613868713}, {"id": 183, "seek": 82368, "start": 843.68, "end": 847.1999999999999, "text": " start off by just reading through them. And then they have bullet points for each of these that we", "tokens": [51364, 722, 766, 538, 445, 3760, 807, 552, 13, 400, 550, 436, 362, 11632, 2793, 337, 1184, 295, 613, 300, 321, 51540], "temperature": 0.0, "avg_logprob": -0.10613253998429809, "compression_ratio": 1.8471337579617835, "no_speech_prob": 0.07368412613868713}, {"id": 184, "seek": 82368, "start": 847.1999999999999, "end": 851.28, "text": " can go through and talk about all of them and more depth. Some of them I'll go over somewhat", "tokens": [51540, 393, 352, 807, 293, 751, 466, 439, 295, 552, 293, 544, 7161, 13, 2188, 295, 552, 286, 603, 352, 670, 8344, 51744], "temperature": 0.0, "avg_logprob": -0.10613253998429809, "compression_ratio": 1.8471337579617835, "no_speech_prob": 0.07368412613868713}, {"id": 185, "seek": 85128, "start": 851.36, "end": 855.12, "text": " quickly. Some of them I'll go over in a bit more depth because they have more written or I think", "tokens": [50368, 2661, 13, 2188, 295, 552, 286, 603, 352, 670, 294, 257, 857, 544, 7161, 570, 436, 362, 544, 3720, 420, 286, 519, 50556], "temperature": 0.0, "avg_logprob": -0.11790142765751592, "compression_ratio": 1.7341389728096677, "no_speech_prob": 0.007345221471041441}, {"id": 186, "seek": 85128, "start": 855.12, "end": 858.9599999999999, "text": " they're more interesting. But anyway, starting off with item number one, we have continual", "tokens": [50556, 436, 434, 544, 1880, 13, 583, 4033, 11, 2891, 766, 365, 3174, 1230, 472, 11, 321, 362, 1421, 901, 50748], "temperature": 0.0, "avg_logprob": -0.11790142765751592, "compression_ratio": 1.7341389728096677, "no_speech_prob": 0.007345221471041441}, {"id": 187, "seek": 85128, "start": 858.9599999999999, "end": 864.56, "text": " supervised learning with given features, then two, supervised feature finding, three, continual", "tokens": [50748, 46533, 2539, 365, 2212, 4122, 11, 550, 732, 11, 46533, 4111, 5006, 11, 1045, 11, 1421, 901, 51028], "temperature": 0.0, "avg_logprob": -0.11790142765751592, "compression_ratio": 1.7341389728096677, "no_speech_prob": 0.007345221471041441}, {"id": 188, "seek": 85128, "start": 864.56, "end": 868.16, "text": " generalized value function prediction learning. If you don't know what these words mean, don't", "tokens": [51028, 44498, 2158, 2445, 17630, 2539, 13, 759, 291, 500, 380, 458, 437, 613, 2283, 914, 11, 500, 380, 51208], "temperature": 0.0, "avg_logprob": -0.11790142765751592, "compression_ratio": 1.7341389728096677, "no_speech_prob": 0.007345221471041441}, {"id": 189, "seek": 85128, "start": 868.16, "end": 872.88, "text": " worry, I'll go over all of it as we go through the individual points later. And four, continual", "tokens": [51208, 3292, 11, 286, 603, 352, 670, 439, 295, 309, 382, 321, 352, 807, 264, 2609, 2793, 1780, 13, 400, 1451, 11, 1421, 901, 51444], "temperature": 0.0, "avg_logprob": -0.11790142765751592, "compression_ratio": 1.7341389728096677, "no_speech_prob": 0.007345221471041441}, {"id": 190, "seek": 85128, "start": 872.88, "end": 879.52, "text": " active credit control, five, average reward, GVF learning, six, continuing control problems, seven,", "tokens": [51444, 4967, 5397, 1969, 11, 1732, 11, 4274, 7782, 11, 460, 53, 37, 2539, 11, 2309, 11, 9289, 1969, 2740, 11, 3407, 11, 51776], "temperature": 0.0, "avg_logprob": -0.11790142765751592, "compression_ratio": 1.7341389728096677, "no_speech_prob": 0.007345221471041441}, {"id": 191, "seek": 87952, "start": 879.6, "end": 885.1999999999999, "text": " planning with average reward, eight, prototype AI one. Wow, incredible. One step model based", "tokens": [50368, 5038, 365, 4274, 7782, 11, 3180, 11, 19475, 7318, 472, 13, 3153, 11, 4651, 13, 1485, 1823, 2316, 2361, 50648], "temperature": 0.0, "avg_logprob": -0.15769623065816946, "compression_ratio": 1.6219931271477663, "no_speech_prob": 0.015904441475868225}, {"id": 192, "seek": 87952, "start": 885.1999999999999, "end": 890.96, "text": " around with continual function approximation, nine, search control and exploration, 10, the stomp", "tokens": [50648, 926, 365, 1421, 901, 2445, 28023, 11, 4949, 11, 3164, 1969, 293, 16197, 11, 1266, 11, 264, 342, 8586, 50936], "temperature": 0.0, "avg_logprob": -0.15769623065816946, "compression_ratio": 1.6219931271477663, "no_speech_prob": 0.015904441475868225}, {"id": 193, "seek": 87952, "start": 890.96, "end": 897.28, "text": " progression, 11, oak and 12, intelligence amplification, or this is, I guess you could call", "tokens": [50936, 18733, 11, 2975, 11, 31322, 293, 2272, 11, 7599, 9731, 3774, 11, 420, 341, 307, 11, 286, 2041, 291, 727, 818, 51252], "temperature": 0.0, "avg_logprob": -0.15769623065816946, "compression_ratio": 1.6219931271477663, "no_speech_prob": 0.015904441475868225}, {"id": 194, "seek": 87952, "start": 897.28, "end": 902.16, "text": " this the singularity, though to be fair, I guess in making AIs make themselves better does not", "tokens": [51252, 341, 264, 20010, 507, 11, 1673, 281, 312, 3143, 11, 286, 2041, 294, 1455, 316, 6802, 652, 2969, 1101, 775, 406, 51496], "temperature": 0.0, "avg_logprob": -0.15769623065816946, "compression_ratio": 1.6219931271477663, "no_speech_prob": 0.015904441475868225}, {"id": 195, "seek": 87952, "start": 902.16, "end": 906.4, "text": " necessarily guarantee the singularity. So maybe that's a bit of a misnomer, but you know, it's", "tokens": [51496, 4725, 10815, 264, 20010, 507, 13, 407, 1310, 300, 311, 257, 857, 295, 257, 3346, 77, 14301, 11, 457, 291, 458, 11, 309, 311, 51708], "temperature": 0.0, "avg_logprob": -0.15769623065816946, "compression_ratio": 1.6219931271477663, "no_speech_prob": 0.015904441475868225}, {"id": 196, "seek": 90640, "start": 906.4, "end": 912.4, "text": " a bit more catchy. It's more of a buzzword, I guess. So you might notice that looking over these", "tokens": [50364, 257, 857, 544, 47168, 13, 467, 311, 544, 295, 257, 13036, 7462, 11, 286, 2041, 13, 407, 291, 1062, 3449, 300, 1237, 670, 613, 50664], "temperature": 0.0, "avg_logprob": -0.11336633085294534, "compression_ratio": 1.736842105263158, "no_speech_prob": 0.0039452495984733105}, {"id": 197, "seek": 90640, "start": 912.4, "end": 918.0799999999999, "text": " points, Hey, this is just normal reinforcement learning. And to some extent, you wouldn't be", "tokens": [50664, 2793, 11, 1911, 11, 341, 307, 445, 2710, 29280, 2539, 13, 400, 281, 512, 8396, 11, 291, 2759, 380, 312, 50948], "temperature": 0.0, "avg_logprob": -0.11336633085294534, "compression_ratio": 1.736842105263158, "no_speech_prob": 0.0039452495984733105}, {"id": 198, "seek": 90640, "start": 918.0799999999999, "end": 922.72, "text": " entirely wrong. These are very common things that we see in reinforcement learning, like", "tokens": [50948, 7696, 2085, 13, 1981, 366, 588, 2689, 721, 300, 321, 536, 294, 29280, 2539, 11, 411, 51180], "temperature": 0.0, "avg_logprob": -0.11336633085294534, "compression_ratio": 1.736842105263158, "no_speech_prob": 0.0039452495984733105}, {"id": 199, "seek": 90640, "start": 922.72, "end": 927.92, "text": " actor credit control. What's what's another thing model based around with continual function", "tokens": [51180, 8747, 5397, 1969, 13, 708, 311, 437, 311, 1071, 551, 2316, 2361, 926, 365, 1421, 901, 2445, 51440], "temperature": 0.0, "avg_logprob": -0.11336633085294534, "compression_ratio": 1.736842105263158, "no_speech_prob": 0.0039452495984733105}, {"id": 200, "seek": 90640, "start": 927.92, "end": 932.3199999999999, "text": " approximation, like what this is supposed to be a generally I that can do all these things. And", "tokens": [51440, 28023, 11, 411, 437, 341, 307, 3442, 281, 312, 257, 5101, 286, 300, 393, 360, 439, 613, 721, 13, 400, 51660], "temperature": 0.0, "avg_logprob": -0.11336633085294534, "compression_ratio": 1.736842105263158, "no_speech_prob": 0.0039452495984733105}, {"id": 201, "seek": 90640, "start": 932.3199999999999, "end": 936.3199999999999, "text": " that's what I meant when I was talking about at the beginning. These are very similar to what", "tokens": [51660, 300, 311, 437, 286, 4140, 562, 286, 390, 1417, 466, 412, 264, 2863, 13, 1981, 366, 588, 2531, 281, 437, 51860], "temperature": 0.0, "avg_logprob": -0.11336633085294534, "compression_ratio": 1.736842105263158, "no_speech_prob": 0.0039452495984733105}, {"id": 202, "seek": 93632, "start": 936.32, "end": 940.1600000000001, "text": " lots of people are doing today. But what you'll notice as we go through the points is that sort of", "tokens": [50364, 3195, 295, 561, 366, 884, 965, 13, 583, 437, 291, 603, 3449, 382, 321, 352, 807, 264, 2793, 307, 300, 1333, 295, 50556], "temperature": 0.0, "avg_logprob": -0.09320800386626145, "compression_ratio": 1.8307210031347962, "no_speech_prob": 0.0007553486502729356}, {"id": 203, "seek": 93632, "start": 940.1600000000001, "end": 944.72, "text": " the focus on the importance of what's important, that's what I think is a bit different, like the", "tokens": [50556, 264, 1879, 322, 264, 7379, 295, 437, 311, 1021, 11, 300, 311, 437, 286, 519, 307, 257, 857, 819, 11, 411, 264, 50784], "temperature": 0.0, "avg_logprob": -0.09320800386626145, "compression_ratio": 1.8307210031347962, "no_speech_prob": 0.0007553486502729356}, {"id": 204, "seek": 93632, "start": 944.72, "end": 949.2, "text": " importance, for example, on continual RL. So we'll get into this, this, this, I should say, this", "tokens": [50784, 7379, 11, 337, 1365, 11, 322, 1421, 901, 497, 43, 13, 407, 321, 603, 483, 666, 341, 11, 341, 11, 341, 11, 286, 820, 584, 11, 341, 51008], "temperature": 0.0, "avg_logprob": -0.09320800386626145, "compression_ratio": 1.8307210031347962, "no_speech_prob": 0.0007553486502729356}, {"id": 205, "seek": 93632, "start": 949.2, "end": 954.1600000000001, "text": " isn't anything groundbreaking. It's more of just an interesting plan and an interesting way to go", "tokens": [51008, 1943, 380, 1340, 42491, 13, 467, 311, 544, 295, 445, 364, 1880, 1393, 293, 364, 1880, 636, 281, 352, 51256], "temperature": 0.0, "avg_logprob": -0.09320800386626145, "compression_ratio": 1.8307210031347962, "no_speech_prob": 0.0007553486502729356}, {"id": 206, "seek": 93632, "start": 954.1600000000001, "end": 959.12, "text": " about this or to think about going about it this way. So let's start with our first big step,", "tokens": [51256, 466, 341, 420, 281, 519, 466, 516, 466, 309, 341, 636, 13, 407, 718, 311, 722, 365, 527, 700, 955, 1823, 11, 51504], "temperature": 0.0, "avg_logprob": -0.09320800386626145, "compression_ratio": 1.8307210031347962, "no_speech_prob": 0.0007553486502729356}, {"id": 207, "seek": 93632, "start": 959.12, "end": 964.24, "text": " which is continual supervised learning with given features. So what let's start out with what this", "tokens": [51504, 597, 307, 1421, 901, 46533, 2539, 365, 2212, 4122, 13, 407, 437, 718, 311, 722, 484, 365, 437, 341, 51760], "temperature": 0.0, "avg_logprob": -0.09320800386626145, "compression_ratio": 1.8307210031347962, "no_speech_prob": 0.0007553486502729356}, {"id": 208, "seek": 96424, "start": 964.24, "end": 968.32, "text": " means. This essentially means given features. So maybe you have cart pull, and cart pull is a problem", "tokens": [50364, 1355, 13, 639, 4476, 1355, 2212, 4122, 13, 407, 1310, 291, 362, 5467, 2235, 11, 293, 5467, 2235, 307, 257, 1154, 50568], "temperature": 0.0, "avg_logprob": -0.10233830458281055, "compression_ratio": 1.976510067114094, "no_speech_prob": 0.019122473895549774}, {"id": 209, "seek": 96424, "start": 968.32, "end": 972.96, "text": " by the way, where you have this little cart, and it has a little pole on top of it. And the pole can", "tokens": [50568, 538, 264, 636, 11, 689, 291, 362, 341, 707, 5467, 11, 293, 309, 575, 257, 707, 13208, 322, 1192, 295, 309, 13, 400, 264, 13208, 393, 50800], "temperature": 0.0, "avg_logprob": -0.10233830458281055, "compression_ratio": 1.976510067114094, "no_speech_prob": 0.019122473895549774}, {"id": 210, "seek": 96424, "start": 972.96, "end": 977.28, "text": " go back and forth. And you need to move the cart back and forth to bounce the pole, very simple", "tokens": [50800, 352, 646, 293, 5220, 13, 400, 291, 643, 281, 1286, 264, 5467, 646, 293, 5220, 281, 15894, 264, 13208, 11, 588, 2199, 51016], "temperature": 0.0, "avg_logprob": -0.10233830458281055, "compression_ratio": 1.976510067114094, "no_speech_prob": 0.019122473895549774}, {"id": 211, "seek": 96424, "start": 977.28, "end": 981.6, "text": " problem. So maybe for your features, you could have like the x, y values of this, and then the", "tokens": [51016, 1154, 13, 407, 1310, 337, 428, 4122, 11, 291, 727, 362, 411, 264, 2031, 11, 288, 4190, 295, 341, 11, 293, 550, 264, 51232], "temperature": 0.0, "avg_logprob": -0.10233830458281055, "compression_ratio": 1.976510067114094, "no_speech_prob": 0.019122473895549774}, {"id": 212, "seek": 96424, "start": 981.6, "end": 987.2, "text": " angular velocity of this cart, or something like that. So those would be given features. And to", "tokens": [51232, 24413, 9269, 295, 341, 5467, 11, 420, 746, 411, 300, 13, 407, 729, 576, 312, 2212, 4122, 13, 400, 281, 51512], "temperature": 0.0, "avg_logprob": -0.10233830458281055, "compression_ratio": 1.976510067114094, "no_speech_prob": 0.019122473895549774}, {"id": 213, "seek": 96424, "start": 987.2, "end": 991.6800000000001, "text": " point out, you know, yes, we can already do things like this. But sort of the point I think of this", "tokens": [51512, 935, 484, 11, 291, 458, 11, 2086, 11, 321, 393, 1217, 360, 721, 411, 341, 13, 583, 1333, 295, 264, 935, 286, 519, 295, 341, 51736], "temperature": 0.0, "avg_logprob": -0.10233830458281055, "compression_ratio": 1.976510067114094, "no_speech_prob": 0.019122473895549774}, {"id": 214, "seek": 99168, "start": 991.76, "end": 997.1999999999999, "text": " plan is to really revisit things in the simplest setting. So they split the explicitly say this,", "tokens": [50368, 1393, 307, 281, 534, 32676, 721, 294, 264, 22811, 3287, 13, 407, 436, 7472, 264, 20803, 584, 341, 11, 50640], "temperature": 0.0, "avg_logprob": -0.11498124152421951, "compression_ratio": 1.9033333333333333, "no_speech_prob": 0.01450276467949152}, {"id": 215, "seek": 99168, "start": 997.1999999999999, "end": 1002.3199999999999, "text": " we want to go back to the simplest setting, and essentially try and make everything as good as", "tokens": [50640, 321, 528, 281, 352, 646, 281, 264, 22811, 3287, 11, 293, 4476, 853, 293, 652, 1203, 382, 665, 382, 50896], "temperature": 0.0, "avg_logprob": -0.11498124152421951, "compression_ratio": 1.9033333333333333, "no_speech_prob": 0.01450276467949152}, {"id": 216, "seek": 99168, "start": 1002.3199999999999, "end": 1006.64, "text": " possible. Essentially try out everything that might have been overlooked, especially in these new", "tokens": [50896, 1944, 13, 23596, 853, 484, 1203, 300, 1062, 362, 668, 32269, 11, 2318, 294, 613, 777, 51112], "temperature": 0.0, "avg_logprob": -0.11498124152421951, "compression_ratio": 1.9033333333333333, "no_speech_prob": 0.01450276467949152}, {"id": 217, "seek": 99168, "start": 1006.64, "end": 1011.4399999999999, "text": " settings in the setting of continual learning, and say, can we do better? So some things they", "tokens": [51112, 6257, 294, 264, 3287, 295, 1421, 901, 2539, 11, 293, 584, 11, 393, 321, 360, 1101, 30, 407, 512, 721, 436, 51352], "temperature": 0.0, "avg_logprob": -0.11498124152421951, "compression_ratio": 1.9033333333333333, "no_speech_prob": 0.01450276467949152}, {"id": 218, "seek": 99168, "start": 1011.4399999999999, "end": 1016.0, "text": " mentioned that they might look into are how can we make things quicker? How can we train faster,", "tokens": [51352, 2835, 300, 436, 1062, 574, 666, 366, 577, 393, 321, 652, 721, 16255, 30, 1012, 393, 321, 3847, 4663, 11, 51580], "temperature": 0.0, "avg_logprob": -0.11498124152421951, "compression_ratio": 1.9033333333333333, "no_speech_prob": 0.01450276467949152}, {"id": 219, "seek": 99168, "start": 1016.0, "end": 1020.56, "text": " be more robust, be more efficient, while also continuing again, over long periods of time,", "tokens": [51580, 312, 544, 13956, 11, 312, 544, 7148, 11, 1339, 611, 9289, 797, 11, 670, 938, 13804, 295, 565, 11, 51808], "temperature": 0.0, "avg_logprob": -0.11498124152421951, "compression_ratio": 1.9033333333333333, "no_speech_prob": 0.01450276467949152}, {"id": 220, "seek": 102056, "start": 1020.56, "end": 1026.6399999999999, "text": " how can we do things like meta learn better representations? And that's be the most efficient.", "tokens": [50364, 577, 393, 321, 360, 721, 411, 19616, 1466, 1101, 33358, 30, 400, 300, 311, 312, 264, 881, 7148, 13, 50668], "temperature": 0.0, "avg_logprob": -0.09279227404860976, "compression_ratio": 1.9269005847953216, "no_speech_prob": 0.0007321617449633777}, {"id": 221, "seek": 102056, "start": 1026.6399999999999, "end": 1030.0, "text": " So these are all questions that that would be asked, although I guess meta learning better", "tokens": [50668, 407, 613, 366, 439, 1651, 300, 300, 576, 312, 2351, 11, 4878, 286, 2041, 19616, 2539, 1101, 50836], "temperature": 0.0, "avg_logprob": -0.09279227404860976, "compression_ratio": 1.9269005847953216, "no_speech_prob": 0.0007321617449633777}, {"id": 222, "seek": 102056, "start": 1030.0, "end": 1034.72, "text": " representations. I'm not sure if that's, I would think that would be in a different step. But,", "tokens": [50836, 33358, 13, 286, 478, 406, 988, 498, 300, 311, 11, 286, 576, 519, 300, 576, 312, 294, 257, 819, 1823, 13, 583, 11, 51072], "temperature": 0.0, "avg_logprob": -0.09279227404860976, "compression_ratio": 1.9269005847953216, "no_speech_prob": 0.0007321617449633777}, {"id": 223, "seek": 102056, "start": 1034.72, "end": 1039.28, "text": " but anyway, what are some examples of some ways, you know, you could look into this, like, what are", "tokens": [51072, 457, 4033, 11, 437, 366, 512, 5110, 295, 512, 2098, 11, 291, 458, 11, 291, 727, 574, 666, 341, 11, 411, 11, 437, 366, 51300], "temperature": 0.0, "avg_logprob": -0.09279227404860976, "compression_ratio": 1.9269005847953216, "no_speech_prob": 0.0007321617449633777}, {"id": 224, "seek": 102056, "start": 1039.28, "end": 1042.6399999999999, "text": " like, what can we really change when we have, you would think this is such a simple setting,", "tokens": [51300, 411, 11, 437, 393, 321, 534, 1319, 562, 321, 362, 11, 291, 576, 519, 341, 307, 1270, 257, 2199, 3287, 11, 51468], "temperature": 0.0, "avg_logprob": -0.09279227404860976, "compression_ratio": 1.9269005847953216, "no_speech_prob": 0.0007321617449633777}, {"id": 225, "seek": 102056, "start": 1042.6399999999999, "end": 1045.9199999999998, "text": " there's not really much we can do, but there actually are some things we can look at. So one", "tokens": [51468, 456, 311, 406, 534, 709, 321, 393, 360, 11, 457, 456, 767, 366, 512, 721, 321, 393, 574, 412, 13, 407, 472, 51632], "temperature": 0.0, "avg_logprob": -0.09279227404860976, "compression_ratio": 1.9269005847953216, "no_speech_prob": 0.0007321617449633777}, {"id": 226, "seek": 102056, "start": 1045.9199999999998, "end": 1049.12, "text": " thing they mentioned is the global step size. So this is like your learning rate. Generally,", "tokens": [51632, 551, 436, 2835, 307, 264, 4338, 1823, 2744, 13, 407, 341, 307, 411, 428, 2539, 3314, 13, 21082, 11, 51792], "temperature": 0.0, "avg_logprob": -0.09279227404860976, "compression_ratio": 1.9269005847953216, "no_speech_prob": 0.0007321617449633777}, {"id": 227, "seek": 104912, "start": 1049.12, "end": 1052.9599999999998, "text": " you have a global learning rate. Now, this is somewhat, but certain optimizers, like Adam,", "tokens": [50364, 291, 362, 257, 4338, 2539, 3314, 13, 823, 11, 341, 307, 8344, 11, 457, 1629, 5028, 22525, 11, 411, 7938, 11, 50556], "temperature": 0.0, "avg_logprob": -0.07765880188384613, "compression_ratio": 1.990909090909091, "no_speech_prob": 0.012820090167224407}, {"id": 228, "seek": 104912, "start": 1052.9599999999998, "end": 1056.6399999999999, "text": " that people have, some people have argued, I'm not sure, I don't know really the ins and outs of", "tokens": [50556, 300, 561, 362, 11, 512, 561, 362, 20219, 11, 286, 478, 406, 988, 11, 286, 500, 380, 458, 534, 264, 1028, 293, 14758, 295, 50740], "temperature": 0.0, "avg_logprob": -0.07765880188384613, "compression_ratio": 1.990909090909091, "no_speech_prob": 0.012820090167224407}, {"id": 229, "seek": 104912, "start": 1056.6399999999999, "end": 1060.08, "text": " this, but they've argued that those sorts of optimizers, while they're really good for supervised", "tokens": [50740, 341, 11, 457, 436, 600, 20219, 300, 729, 7527, 295, 5028, 22525, 11, 1339, 436, 434, 534, 665, 337, 46533, 50912], "temperature": 0.0, "avg_logprob": -0.07765880188384613, "compression_ratio": 1.990909090909091, "no_speech_prob": 0.012820090167224407}, {"id": 230, "seek": 104912, "start": 1060.08, "end": 1064.0, "text": " learning, maybe aren't the best fit for reinforcement learning, because for reinforcement", "tokens": [50912, 2539, 11, 1310, 3212, 380, 264, 1151, 3318, 337, 29280, 2539, 11, 570, 337, 29280, 51108], "temperature": 0.0, "avg_logprob": -0.07765880188384613, "compression_ratio": 1.990909090909091, "no_speech_prob": 0.012820090167224407}, {"id": 231, "seek": 104912, "start": 1064.0, "end": 1067.9199999999998, "text": " learning, maybe you want to have different learning rates, depending on how important certain", "tokens": [51108, 2539, 11, 1310, 291, 528, 281, 362, 819, 2539, 6846, 11, 5413, 322, 577, 1021, 1629, 51304], "temperature": 0.0, "avg_logprob": -0.07765880188384613, "compression_ratio": 1.990909090909091, "no_speech_prob": 0.012820090167224407}, {"id": 232, "seek": 104912, "start": 1067.9199999999998, "end": 1073.12, "text": " features are for a certain task, or like what, you know, if you're trying to do different sorts", "tokens": [51304, 4122, 366, 337, 257, 1629, 5633, 11, 420, 411, 437, 11, 291, 458, 11, 498, 291, 434, 1382, 281, 360, 819, 7527, 51564], "temperature": 0.0, "avg_logprob": -0.07765880188384613, "compression_ratio": 1.990909090909091, "no_speech_prob": 0.012820090167224407}, {"id": 233, "seek": 104912, "start": 1073.12, "end": 1076.6399999999999, "text": " of tasks, there's different things you could think of. But this is like one thing you could", "tokens": [51564, 295, 9608, 11, 456, 311, 819, 721, 291, 727, 519, 295, 13, 583, 341, 307, 411, 472, 551, 291, 727, 51740], "temperature": 0.0, "avg_logprob": -0.07765880188384613, "compression_ratio": 1.990909090909091, "no_speech_prob": 0.012820090167224407}, {"id": 234, "seek": 107664, "start": 1076.64, "end": 1082.64, "text": " look at, right? Like, can you have a different step size for each single parameter? And how could", "tokens": [50364, 574, 412, 11, 558, 30, 1743, 11, 393, 291, 362, 257, 819, 1823, 2744, 337, 1184, 2167, 13075, 30, 400, 577, 727, 50664], "temperature": 0.0, "avg_logprob": -0.09227989117304485, "compression_ratio": 1.7417417417417418, "no_speech_prob": 0.004905202891677618}, {"id": 235, "seek": 107664, "start": 1082.64, "end": 1086.72, "text": " you do that in a way that makes sense for continual reinforcement learning and not, you know,", "tokens": [50664, 291, 360, 300, 294, 257, 636, 300, 1669, 2020, 337, 1421, 901, 29280, 2539, 293, 406, 11, 291, 458, 11, 50868], "temperature": 0.0, "avg_logprob": -0.09227989117304485, "compression_ratio": 1.7417417417417418, "no_speech_prob": 0.004905202891677618}, {"id": 236, "seek": 107664, "start": 1086.72, "end": 1091.0400000000002, "text": " supervised learning, which is the context that most of these methods have been developed in.", "tokens": [50868, 46533, 2539, 11, 597, 307, 264, 4319, 300, 881, 295, 613, 7150, 362, 668, 4743, 294, 13, 51084], "temperature": 0.0, "avg_logprob": -0.09227989117304485, "compression_ratio": 1.7417417417417418, "no_speech_prob": 0.004905202891677618}, {"id": 237, "seek": 107664, "start": 1091.0400000000002, "end": 1095.1200000000001, "text": " And of course, I think I'm not going to find it right now in this, this C of text, but they also", "tokens": [51084, 400, 295, 1164, 11, 286, 519, 286, 478, 406, 516, 281, 915, 309, 558, 586, 294, 341, 11, 341, 383, 295, 2487, 11, 457, 436, 611, 51288], "temperature": 0.0, "avg_logprob": -0.09227989117304485, "compression_ratio": 1.7417417417417418, "no_speech_prob": 0.004905202891677618}, {"id": 238, "seek": 107664, "start": 1095.1200000000001, "end": 1099.2, "text": " mentioned that, you know, you don't want a human to be setting the learning rate. These are things", "tokens": [51288, 2835, 300, 11, 291, 458, 11, 291, 500, 380, 528, 257, 1952, 281, 312, 3287, 264, 2539, 3314, 13, 1981, 366, 721, 51492], "temperature": 0.0, "avg_logprob": -0.09227989117304485, "compression_ratio": 1.7417417417417418, "no_speech_prob": 0.004905202891677618}, {"id": 239, "seek": 107664, "start": 1099.2, "end": 1104.4, "text": " that should be meta learned. Again, right? We don't want really too much human experience. We don't", "tokens": [51492, 300, 820, 312, 19616, 3264, 13, 3764, 11, 558, 30, 492, 500, 380, 528, 534, 886, 709, 1952, 1752, 13, 492, 500, 380, 51752], "temperature": 0.0, "avg_logprob": -0.09227989117304485, "compression_ratio": 1.7417417417417418, "no_speech_prob": 0.004905202891677618}, {"id": 240, "seek": 110440, "start": 1104.48, "end": 1108.64, "text": " want that much fine tuning. We want the agent to be able to really do everything for itself. So", "tokens": [50368, 528, 300, 709, 2489, 15164, 13, 492, 528, 264, 9461, 281, 312, 1075, 281, 534, 360, 1203, 337, 2564, 13, 407, 50576], "temperature": 0.0, "avg_logprob": -0.07277335190191501, "compression_ratio": 1.8369565217391304, "no_speech_prob": 0.008061329834163189}, {"id": 241, "seek": 110440, "start": 1108.64, "end": 1113.2800000000002, "text": " these are all things that yes, people are looking into them. But the idea here, again, is go back", "tokens": [50576, 613, 366, 439, 721, 300, 2086, 11, 561, 366, 1237, 666, 552, 13, 583, 264, 1558, 510, 11, 797, 11, 307, 352, 646, 50808], "temperature": 0.0, "avg_logprob": -0.07277335190191501, "compression_ratio": 1.8369565217391304, "no_speech_prob": 0.008061329834163189}, {"id": 242, "seek": 110440, "start": 1113.2800000000002, "end": 1118.0800000000002, "text": " to the simplest example, and get something, something working as best as possible. And then", "tokens": [50808, 281, 264, 22811, 1365, 11, 293, 483, 746, 11, 746, 1364, 382, 1151, 382, 1944, 13, 400, 550, 51048], "temperature": 0.0, "avg_logprob": -0.07277335190191501, "compression_ratio": 1.8369565217391304, "no_speech_prob": 0.008061329834163189}, {"id": 243, "seek": 110440, "start": 1118.0800000000002, "end": 1122.48, "text": " we'll start to scale up and make things more complicated. So some other things they mentioned,", "tokens": [51048, 321, 603, 722, 281, 4373, 493, 293, 652, 721, 544, 6179, 13, 407, 512, 661, 721, 436, 2835, 11, 51268], "temperature": 0.0, "avg_logprob": -0.07277335190191501, "compression_ratio": 1.8369565217391304, "no_speech_prob": 0.008061329834163189}, {"id": 244, "seek": 110440, "start": 1122.48, "end": 1126.5600000000002, "text": " again, it's a bit too much here. Oh, here it is. So there's like normalizations of features, right?", "tokens": [51268, 797, 11, 309, 311, 257, 857, 886, 709, 510, 13, 876, 11, 510, 309, 307, 13, 407, 456, 311, 411, 2710, 14455, 295, 4122, 11, 558, 30, 51472], "temperature": 0.0, "avg_logprob": -0.07277335190191501, "compression_ratio": 1.8369565217391304, "no_speech_prob": 0.008061329834163189}, {"id": 245, "seek": 110440, "start": 1126.5600000000002, "end": 1129.92, "text": " Like, of course, we know that you can normalize features. And that tends to be a good thing in", "tokens": [51472, 1743, 11, 295, 1164, 11, 321, 458, 300, 291, 393, 2710, 1125, 4122, 13, 400, 300, 12258, 281, 312, 257, 665, 551, 294, 51640], "temperature": 0.0, "avg_logprob": -0.07277335190191501, "compression_ratio": 1.8369565217391304, "no_speech_prob": 0.008061329834163189}, {"id": 246, "seek": 110440, "start": 1129.92, "end": 1133.76, "text": " machine learning. But when you're doing continual learning and reinforcement learning, does anything", "tokens": [51640, 3479, 2539, 13, 583, 562, 291, 434, 884, 1421, 901, 2539, 293, 29280, 2539, 11, 775, 1340, 51832], "temperature": 0.0, "avg_logprob": -0.07277335190191501, "compression_ratio": 1.8369565217391304, "no_speech_prob": 0.008061329834163189}, {"id": 247, "seek": 113376, "start": 1133.76, "end": 1138.24, "text": " change? You know, these are all things that could be reconsidered in this slightly different context", "tokens": [50364, 1319, 30, 509, 458, 11, 613, 366, 439, 721, 300, 727, 312, 40497, 292, 294, 341, 4748, 819, 4319, 50588], "temperature": 0.0, "avg_logprob": -0.07832030885538478, "compression_ratio": 1.76, "no_speech_prob": 0.004069898743182421}, {"id": 248, "seek": 113376, "start": 1138.24, "end": 1143.28, "text": " to maybe squeeze the most out of this that you can. So they go into this in a bit of depth.", "tokens": [50588, 281, 1310, 13578, 264, 881, 484, 295, 341, 300, 291, 393, 13, 407, 436, 352, 666, 341, 294, 257, 857, 295, 7161, 13, 50840], "temperature": 0.0, "avg_logprob": -0.07832030885538478, "compression_ratio": 1.76, "no_speech_prob": 0.004069898743182421}, {"id": 249, "seek": 113376, "start": 1143.28, "end": 1146.72, "text": " Honestly, I'm going to skip over this because I don't think it's particularly important.", "tokens": [50840, 12348, 11, 286, 478, 516, 281, 10023, 670, 341, 570, 286, 500, 380, 519, 309, 311, 4098, 1021, 13, 51012], "temperature": 0.0, "avg_logprob": -0.07832030885538478, "compression_ratio": 1.76, "no_speech_prob": 0.004069898743182421}, {"id": 250, "seek": 113376, "start": 1146.72, "end": 1150.24, "text": " There's talking about examples of ways, you know, you can think about this, for example,", "tokens": [51012, 821, 311, 1417, 466, 5110, 295, 2098, 11, 291, 458, 11, 291, 393, 519, 466, 341, 11, 337, 1365, 11, 51188], "temperature": 0.0, "avg_logprob": -0.07832030885538478, "compression_ratio": 1.76, "no_speech_prob": 0.004069898743182421}, {"id": 251, "seek": 113376, "start": 1150.24, "end": 1154.16, "text": " like the meta learning per weight step parameter. That's something I just mentioned. I think they", "tokens": [51188, 411, 264, 19616, 2539, 680, 3364, 1823, 13075, 13, 663, 311, 746, 286, 445, 2835, 13, 286, 519, 436, 51384], "temperature": 0.0, "avg_logprob": -0.07832030885538478, "compression_ratio": 1.76, "no_speech_prob": 0.004069898743182421}, {"id": 252, "seek": 113376, "start": 1154.16, "end": 1158.24, "text": " talk about this more in depth for this step, maybe because they already have some papers out about", "tokens": [51384, 751, 466, 341, 544, 294, 7161, 337, 341, 1823, 11, 1310, 570, 436, 1217, 362, 512, 10577, 484, 466, 51588], "temperature": 0.0, "avg_logprob": -0.07832030885538478, "compression_ratio": 1.76, "no_speech_prob": 0.004069898743182421}, {"id": 253, "seek": 113376, "start": 1158.24, "end": 1162.96, "text": " this. But anyway, let's get on to step two. So this is supervised feature finding. So before", "tokens": [51588, 341, 13, 583, 4033, 11, 718, 311, 483, 322, 281, 1823, 732, 13, 407, 341, 307, 46533, 4111, 5006, 13, 407, 949, 51824], "temperature": 0.0, "avg_logprob": -0.07832030885538478, "compression_ratio": 1.76, "no_speech_prob": 0.004069898743182421}, {"id": 254, "seek": 116296, "start": 1162.96, "end": 1167.68, "text": " we had the features given to us, but now we actually want to find them, we want to generate", "tokens": [50364, 321, 632, 264, 4122, 2212, 281, 505, 11, 457, 586, 321, 767, 528, 281, 915, 552, 11, 321, 528, 281, 8460, 50600], "temperature": 0.0, "avg_logprob": -0.12408889111855047, "compression_ratio": 1.8333333333333333, "no_speech_prob": 0.0026314782444387674}, {"id": 255, "seek": 116296, "start": 1167.68, "end": 1172.64, "text": " new features, right? And features, right, these are just, we get some observation, we have our", "tokens": [50600, 777, 4122, 11, 558, 30, 400, 4122, 11, 558, 11, 613, 366, 445, 11, 321, 483, 512, 14816, 11, 321, 362, 527, 50848], "temperature": 0.0, "avg_logprob": -0.12408889111855047, "compression_ratio": 1.8333333333333333, "no_speech_prob": 0.0026314782444387674}, {"id": 256, "seek": 116296, "start": 1172.64, "end": 1177.1200000000001, "text": " perception, remember from the base agent, now we want to use those to create new features to", "tokens": [50848, 12860, 11, 1604, 490, 264, 3096, 9461, 11, 586, 321, 528, 281, 764, 729, 281, 1884, 777, 4122, 281, 51072], "temperature": 0.0, "avg_logprob": -0.12408889111855047, "compression_ratio": 1.8333333333333333, "no_speech_prob": 0.0026314782444387674}, {"id": 257, "seek": 116296, "start": 1177.1200000000001, "end": 1182.16, "text": " essentially use things that will serve representations that will help us do whatever tasks we're", "tokens": [51072, 4476, 764, 721, 300, 486, 4596, 33358, 300, 486, 854, 505, 360, 2035, 9608, 321, 434, 51324], "temperature": 0.0, "avg_logprob": -0.12408889111855047, "compression_ratio": 1.8333333333333333, "no_speech_prob": 0.0026314782444387674}, {"id": 258, "seek": 116296, "start": 1182.16, "end": 1186.56, "text": " working on. Now, this is generally done via back propagation. And sorry, if you don't know what", "tokens": [51324, 1364, 322, 13, 823, 11, 341, 307, 5101, 1096, 5766, 646, 38377, 13, 400, 2597, 11, 498, 291, 500, 380, 458, 437, 51544], "temperature": 0.0, "avg_logprob": -0.12408889111855047, "compression_ratio": 1.8333333333333333, "no_speech_prob": 0.0026314782444387674}, {"id": 259, "seek": 116296, "start": 1186.56, "end": 1190.24, "text": " back prop is, but it's a bit too much to explain this video, there's plenty of good things. There's", "tokens": [51544, 646, 2365, 307, 11, 457, 309, 311, 257, 857, 886, 709, 281, 2903, 341, 960, 11, 456, 311, 7140, 295, 665, 721, 13, 821, 311, 51728], "temperature": 0.0, "avg_logprob": -0.12408889111855047, "compression_ratio": 1.8333333333333333, "no_speech_prob": 0.0026314782444387674}, {"id": 260, "seek": 119024, "start": 1190.24, "end": 1195.1200000000001, "text": " out there explaining it. And back prop, one issue actually with back prop is lots of people", "tokens": [50364, 484, 456, 13468, 309, 13, 400, 646, 2365, 11, 472, 2734, 767, 365, 646, 2365, 307, 3195, 295, 561, 50608], "temperature": 0.0, "avg_logprob": -0.138752526384059, "compression_ratio": 1.8326848249027237, "no_speech_prob": 0.013636201620101929}, {"id": 261, "seek": 119024, "start": 1195.1200000000001, "end": 1199.84, "text": " aren't aware of this. It doesn't work super well for continual learning. I mean, maybe not super", "tokens": [50608, 3212, 380, 3650, 295, 341, 13, 467, 1177, 380, 589, 1687, 731, 337, 1421, 901, 2539, 13, 286, 914, 11, 1310, 406, 1687, 50844], "temperature": 0.0, "avg_logprob": -0.138752526384059, "compression_ratio": 1.8326848249027237, "no_speech_prob": 0.013636201620101929}, {"id": 262, "seek": 119024, "start": 1199.84, "end": 1204.8, "text": " well as in an overstatement, it does work. And it works fairly well, but it has an issue. It has", "tokens": [50844, 731, 382, 294, 364, 670, 19435, 1712, 11, 309, 775, 589, 13, 400, 309, 1985, 6457, 731, 11, 457, 309, 575, 364, 2734, 13, 467, 575, 51092], "temperature": 0.0, "avg_logprob": -0.138752526384059, "compression_ratio": 1.8326848249027237, "no_speech_prob": 0.013636201620101929}, {"id": 263, "seek": 119024, "start": 1204.8, "end": 1210.0, "text": " an issue that the more you train, if you train on and on and on and on and you just keep going,", "tokens": [51092, 364, 2734, 300, 264, 544, 291, 3847, 11, 498, 291, 3847, 322, 293, 322, 293, 322, 293, 322, 293, 291, 445, 1066, 516, 11, 51352], "temperature": 0.0, "avg_logprob": -0.138752526384059, "compression_ratio": 1.8326848249027237, "no_speech_prob": 0.013636201620101929}, {"id": 264, "seek": 119024, "start": 1210.0, "end": 1214.32, "text": " it actually sort of does what they call sat and it gets saturated. I think they call like", "tokens": [51352, 309, 767, 1333, 295, 775, 437, 436, 818, 3227, 293, 309, 2170, 25408, 13, 286, 519, 436, 818, 411, 51568], "temperature": 0.0, "avg_logprob": -0.138752526384059, "compression_ratio": 1.8326848249027237, "no_speech_prob": 0.013636201620101929}, {"id": 265, "seek": 121432, "start": 1214.3999999999999, "end": 1220.96, "text": " interridger saturation. Individual nodes get saturated, which means they stop essentially", "tokens": [50368, 728, 8558, 1321, 27090, 13, 37292, 13891, 483, 25408, 11, 597, 1355, 436, 1590, 4476, 50696], "temperature": 0.0, "avg_logprob": -0.14813819527626038, "compression_ratio": 1.8143322475570032, "no_speech_prob": 0.07584827393293381}, {"id": 266, "seek": 121432, "start": 1220.96, "end": 1225.36, "text": " contributing. And over time, what you'll find is you'll see that maybe the performance of your", "tokens": [50696, 19270, 13, 400, 670, 565, 11, 437, 291, 603, 915, 307, 291, 603, 536, 300, 1310, 264, 3389, 295, 428, 50916], "temperature": 0.0, "avg_logprob": -0.14813819527626038, "compression_ratio": 1.8143322475570032, "no_speech_prob": 0.07584827393293381}, {"id": 267, "seek": 121432, "start": 1225.36, "end": 1229.2, "text": " agent, it starts off low, it goes up and it gets good and it kind of blows out. Well, if the task", "tokens": [50916, 9461, 11, 309, 3719, 766, 2295, 11, 309, 1709, 493, 293, 309, 2170, 665, 293, 309, 733, 295, 18458, 484, 13, 1042, 11, 498, 264, 5633, 51108], "temperature": 0.0, "avg_logprob": -0.14813819527626038, "compression_ratio": 1.8143322475570032, "no_speech_prob": 0.07584827393293381}, {"id": 268, "seek": 121432, "start": 1229.2, "end": 1233.28, "text": " keeps changing, what you'll find is that the performance slowly starts to drop as the agent", "tokens": [51108, 5965, 4473, 11, 437, 291, 603, 915, 307, 300, 264, 3389, 5692, 3719, 281, 3270, 382, 264, 9461, 51312], "temperature": 0.0, "avg_logprob": -0.14813819527626038, "compression_ratio": 1.8143322475570032, "no_speech_prob": 0.07584827393293381}, {"id": 269, "seek": 121432, "start": 1233.28, "end": 1238.08, "text": " is unable to adapt because the network has essentially gotten saturated. Or at least,", "tokens": [51312, 307, 11299, 281, 6231, 570, 264, 3209, 575, 4476, 5768, 25408, 13, 1610, 412, 1935, 11, 51552], "temperature": 0.0, "avg_logprob": -0.14813819527626038, "compression_ratio": 1.8143322475570032, "no_speech_prob": 0.07584827393293381}, {"id": 270, "seek": 121432, "start": 1238.08, "end": 1242.24, "text": " that's one explanation. I guess it's not fully explained yet. I actually explain this phenomenon", "tokens": [51552, 300, 311, 472, 10835, 13, 286, 2041, 309, 311, 406, 4498, 8825, 1939, 13, 286, 767, 2903, 341, 14029, 51760], "temperature": 0.0, "avg_logprob": -0.14813819527626038, "compression_ratio": 1.8143322475570032, "no_speech_prob": 0.07584827393293381}, {"id": 271, "seek": 124224, "start": 1242.24, "end": 1247.2, "text": " as well with there's a method called continue backprop or CBP for short. And this is from", "tokens": [50364, 382, 731, 365, 456, 311, 257, 3170, 1219, 2354, 646, 79, 1513, 420, 18745, 47, 337, 2099, 13, 400, 341, 307, 490, 50612], "temperature": 0.0, "avg_logprob": -0.10104269139906939, "compression_ratio": 1.8122977346278317, "no_speech_prob": 0.013221667148172855}, {"id": 272, "seek": 124224, "start": 1247.2, "end": 1251.76, "text": " Rich's lab, where they work on this problem. So this is one example of how supervised feature", "tokens": [50612, 6781, 311, 2715, 11, 689, 436, 589, 322, 341, 1154, 13, 407, 341, 307, 472, 1365, 295, 577, 46533, 4111, 50840], "temperature": 0.0, "avg_logprob": -0.10104269139906939, "compression_ratio": 1.8122977346278317, "no_speech_prob": 0.013221667148172855}, {"id": 273, "seek": 124224, "start": 1251.76, "end": 1256.96, "text": " finding actually needs to be adjusted to the given scenario, which is continual learning.", "tokens": [50840, 5006, 767, 2203, 281, 312, 19871, 281, 264, 2212, 9005, 11, 597, 307, 1421, 901, 2539, 13, 51100], "temperature": 0.0, "avg_logprob": -0.10104269139906939, "compression_ratio": 1.8122977346278317, "no_speech_prob": 0.013221667148172855}, {"id": 274, "seek": 124224, "start": 1256.96, "end": 1260.8, "text": " And this is one example of how although we have something that's really good already, we have", "tokens": [51100, 400, 341, 307, 472, 1365, 295, 577, 4878, 321, 362, 746, 300, 311, 534, 665, 1217, 11, 321, 362, 51292], "temperature": 0.0, "avg_logprob": -0.10104269139906939, "compression_ratio": 1.8122977346278317, "no_speech_prob": 0.013221667148172855}, {"id": 275, "seek": 124224, "start": 1260.8, "end": 1265.92, "text": " back propagation, it might not be the best thing to use, at least not this exact instance we use", "tokens": [51292, 646, 38377, 11, 309, 1062, 406, 312, 264, 1151, 551, 281, 764, 11, 412, 1935, 406, 341, 1900, 5197, 321, 764, 51548], "temperature": 0.0, "avg_logprob": -0.10104269139906939, "compression_ratio": 1.8122977346278317, "no_speech_prob": 0.013221667148172855}, {"id": 276, "seek": 124224, "start": 1265.92, "end": 1270.56, "text": " of it right now, might not be the best thing for continual RL or something like that. One other", "tokens": [51548, 295, 309, 558, 586, 11, 1062, 406, 312, 264, 1151, 551, 337, 1421, 901, 497, 43, 420, 746, 411, 300, 13, 1485, 661, 51780], "temperature": 0.0, "avg_logprob": -0.10104269139906939, "compression_ratio": 1.8122977346278317, "no_speech_prob": 0.013221667148172855}, {"id": 277, "seek": 127056, "start": 1270.56, "end": 1274.8799999999999, "text": " thing that I think is really interesting, and I found this to be always very enticing, but", "tokens": [50364, 551, 300, 286, 519, 307, 534, 1880, 11, 293, 286, 1352, 341, 281, 312, 1009, 588, 948, 5776, 11, 457, 50580], "temperature": 0.0, "avg_logprob": -0.07588176995935575, "compression_ratio": 1.9245901639344263, "no_speech_prob": 0.010326748713850975}, {"id": 278, "seek": 127056, "start": 1274.8799999999999, "end": 1280.08, "text": " sort of dangerous. I'm not dangerous and literally dangerous, but like dangerously enticing part of", "tokens": [50580, 1333, 295, 5795, 13, 286, 478, 406, 5795, 293, 3736, 5795, 11, 457, 411, 4330, 5098, 948, 5776, 644, 295, 50840], "temperature": 0.0, "avg_logprob": -0.07588176995935575, "compression_ratio": 1.9245901639344263, "no_speech_prob": 0.010326748713850975}, {"id": 279, "seek": 127056, "start": 1280.08, "end": 1285.52, "text": " the work that like Rich's lab does is the way they think about modeling, like the typical modeling", "tokens": [50840, 264, 589, 300, 411, 6781, 311, 2715, 775, 307, 264, 636, 436, 519, 466, 15983, 11, 411, 264, 7476, 15983, 51112], "temperature": 0.0, "avg_logprob": -0.07588176995935575, "compression_ratio": 1.9245901639344263, "no_speech_prob": 0.010326748713850975}, {"id": 280, "seek": 127056, "start": 1285.52, "end": 1290.32, "text": " paradigm, their perspective is quite a bit different. So let me describe what I sort of think of as", "tokens": [51112, 24709, 11, 641, 4585, 307, 1596, 257, 857, 819, 13, 407, 718, 385, 6786, 437, 286, 1333, 295, 519, 295, 382, 51352], "temperature": 0.0, "avg_logprob": -0.07588176995935575, "compression_ratio": 1.9245901639344263, "no_speech_prob": 0.010326748713850975}, {"id": 281, "seek": 127056, "start": 1290.32, "end": 1294.96, "text": " the typical paradigm. So maybe I'll do that on the left here. So I think the typical paradigm of,", "tokens": [51352, 264, 7476, 24709, 13, 407, 1310, 286, 603, 360, 300, 322, 264, 1411, 510, 13, 407, 286, 519, 264, 7476, 24709, 295, 11, 51584], "temperature": 0.0, "avg_logprob": -0.07588176995935575, "compression_ratio": 1.9245901639344263, "no_speech_prob": 0.010326748713850975}, {"id": 282, "seek": 127056, "start": 1294.96, "end": 1298.96, "text": " and this is not what all papers are about, but lots of them, one, you want to pick an architecture,", "tokens": [51584, 293, 341, 307, 406, 437, 439, 10577, 366, 466, 11, 457, 3195, 295, 552, 11, 472, 11, 291, 528, 281, 1888, 364, 9482, 11, 51784], "temperature": 0.0, "avg_logprob": -0.07588176995935575, "compression_ratio": 1.9245901639344263, "no_speech_prob": 0.010326748713850975}, {"id": 283, "seek": 129896, "start": 1299.04, "end": 1301.52, "text": " right? You have some problems. So you want to pick the architecture you're going to use. This", "tokens": [50368, 558, 30, 509, 362, 512, 2740, 13, 407, 291, 528, 281, 1888, 264, 9482, 291, 434, 516, 281, 764, 13, 639, 50492], "temperature": 0.0, "avg_logprob": -0.13461625651948772, "compression_ratio": 1.935672514619883, "no_speech_prob": 0.015421709977090359}, {"id": 284, "seek": 129896, "start": 1301.52, "end": 1305.52, "text": " could be like a ResNet, right? You can use different ResNet layers. Maybe you want to use a", "tokens": [50492, 727, 312, 411, 257, 5015, 31890, 11, 558, 30, 509, 393, 764, 819, 5015, 31890, 7914, 13, 2704, 291, 528, 281, 764, 257, 50692], "temperature": 0.0, "avg_logprob": -0.13461625651948772, "compression_ratio": 1.935672514619883, "no_speech_prob": 0.015421709977090359}, {"id": 285, "seek": 129896, "start": 1305.52, "end": 1310.4, "text": " transformer. Maybe you want to use some sort of self attention. There's lots of different", "tokens": [50692, 31782, 13, 2704, 291, 528, 281, 764, 512, 1333, 295, 2698, 3202, 13, 821, 311, 3195, 295, 819, 50936], "temperature": 0.0, "avg_logprob": -0.13461625651948772, "compression_ratio": 1.935672514619883, "no_speech_prob": 0.015421709977090359}, {"id": 286, "seek": 129896, "start": 1310.4, "end": 1314.88, "text": " architectures you could choose, right? You choose an architecture and then two, you choose an", "tokens": [50936, 6331, 1303, 291, 727, 2826, 11, 558, 30, 509, 2826, 364, 9482, 293, 550, 732, 11, 291, 2826, 364, 51160], "temperature": 0.0, "avg_logprob": -0.13461625651948772, "compression_ratio": 1.935672514619883, "no_speech_prob": 0.015421709977090359}, {"id": 287, "seek": 129896, "start": 1314.88, "end": 1318.88, "text": " objective function. So this objective function is going to be like your loss. Like what are you", "tokens": [51160, 10024, 2445, 13, 407, 341, 10024, 2445, 307, 516, 281, 312, 411, 428, 4470, 13, 1743, 437, 366, 291, 51360], "temperature": 0.0, "avg_logprob": -0.13461625651948772, "compression_ratio": 1.935672514619883, "no_speech_prob": 0.015421709977090359}, {"id": 288, "seek": 129896, "start": 1318.88, "end": 1322.96, "text": " trying to optimize here, especially in reinforcement learning, maybe this is usually going to be the", "tokens": [51360, 1382, 281, 19719, 510, 11, 2318, 294, 29280, 2539, 11, 1310, 341, 307, 2673, 516, 281, 312, 264, 51564], "temperature": 0.0, "avg_logprob": -0.13461625651948772, "compression_ratio": 1.935672514619883, "no_speech_prob": 0.015421709977090359}, {"id": 289, "seek": 129896, "start": 1322.96, "end": 1327.76, "text": " reward or the value or whatever. But this could be a number of things. It could be the MSC with", "tokens": [51564, 7782, 420, 264, 2158, 420, 2035, 13, 583, 341, 727, 312, 257, 1230, 295, 721, 13, 467, 727, 312, 264, 7395, 34, 365, 51804], "temperature": 0.0, "avg_logprob": -0.13461625651948772, "compression_ratio": 1.935672514619883, "no_speech_prob": 0.015421709977090359}, {"id": 290, "seek": 132776, "start": 1328.16, "end": 1331.28, "text": " like an image. If you're trying to recreate an image or something like that. And then the third", "tokens": [50384, 411, 364, 3256, 13, 759, 291, 434, 1382, 281, 25833, 364, 3256, 420, 746, 411, 300, 13, 400, 550, 264, 2636, 50540], "temperature": 0.0, "avg_logprob": -0.07314168903189645, "compression_ratio": 1.815625, "no_speech_prob": 0.003172541270032525}, {"id": 291, "seek": 132776, "start": 1331.28, "end": 1335.84, "text": " and last thing you do is you pick an optimizer. So 99% of the time, this is going to be like", "tokens": [50540, 293, 1036, 551, 291, 360, 307, 291, 1888, 364, 5028, 6545, 13, 407, 11803, 4, 295, 264, 565, 11, 341, 307, 516, 281, 312, 411, 50768], "temperature": 0.0, "avg_logprob": -0.07314168903189645, "compression_ratio": 1.815625, "no_speech_prob": 0.003172541270032525}, {"id": 292, "seek": 132776, "start": 1335.84, "end": 1341.52, "text": " Adam or it's going to be RMS prop. But the thing is the way I find that within this paper, they talk", "tokens": [50768, 7938, 420, 309, 311, 516, 281, 312, 497, 10288, 2365, 13, 583, 264, 551, 307, 264, 636, 286, 915, 300, 1951, 341, 3035, 11, 436, 751, 51052], "temperature": 0.0, "avg_logprob": -0.07314168903189645, "compression_ratio": 1.815625, "no_speech_prob": 0.003172541270032525}, {"id": 293, "seek": 132776, "start": 1341.52, "end": 1345.68, "text": " about these sorts of things and reading other papers from the same people, they tend to think", "tokens": [51052, 466, 613, 7527, 295, 721, 293, 3760, 661, 10577, 490, 264, 912, 561, 11, 436, 3928, 281, 519, 51260], "temperature": 0.0, "avg_logprob": -0.07314168903189645, "compression_ratio": 1.815625, "no_speech_prob": 0.003172541270032525}, {"id": 294, "seek": 132776, "start": 1345.68, "end": 1350.8799999999999, "text": " about these things a bit differently. Rather, they look at it from like a perspective of looking at", "tokens": [51260, 466, 613, 721, 257, 857, 7614, 13, 16571, 11, 436, 574, 412, 309, 490, 411, 257, 4585, 295, 1237, 412, 51520], "temperature": 0.0, "avg_logprob": -0.07314168903189645, "compression_ratio": 1.815625, "no_speech_prob": 0.003172541270032525}, {"id": 295, "seek": 132776, "start": 1350.8799999999999, "end": 1355.6, "text": " individual neurons and the interaction between these neurons. And what do I mean by this? Because", "tokens": [51520, 2609, 22027, 293, 264, 9285, 1296, 613, 22027, 13, 400, 437, 360, 286, 914, 538, 341, 30, 1436, 51756], "temperature": 0.0, "avg_logprob": -0.07314168903189645, "compression_ratio": 1.815625, "no_speech_prob": 0.003172541270032525}, {"id": 296, "seek": 135560, "start": 1355.6799999999998, "end": 1360.0, "text": " technically, like, you know, we're ending up using lots of the same things, but it is really just a", "tokens": [50368, 12120, 11, 411, 11, 291, 458, 11, 321, 434, 8121, 493, 1228, 3195, 295, 264, 912, 721, 11, 457, 309, 307, 534, 445, 257, 50584], "temperature": 0.0, "avg_logprob": -0.07865397354652141, "compression_ratio": 1.7914110429447854, "no_speech_prob": 0.009707726538181305}, {"id": 297, "seek": 135560, "start": 1360.0, "end": 1364.08, "text": " difference in perspective. So for example, we can say, how can the utility be assigned to all the", "tokens": [50584, 2649, 294, 4585, 13, 407, 337, 1365, 11, 321, 393, 584, 11, 577, 393, 264, 14877, 312, 13279, 281, 439, 264, 50788], "temperature": 0.0, "avg_logprob": -0.07865397354652141, "compression_ratio": 1.7914110429447854, "no_speech_prob": 0.009707726538181305}, {"id": 298, "seek": 135560, "start": 1364.08, "end": 1369.84, "text": " features that we're using? And then how could we make use of these utilities in the future, right?", "tokens": [50788, 4122, 300, 321, 434, 1228, 30, 400, 550, 577, 727, 321, 652, 764, 295, 613, 30482, 294, 264, 2027, 11, 558, 30, 51076], "temperature": 0.0, "avg_logprob": -0.07865397354652141, "compression_ratio": 1.7914110429447854, "no_speech_prob": 0.009707726538181305}, {"id": 299, "seek": 135560, "start": 1369.84, "end": 1373.6799999999998, "text": " So this is like a very much we're looking at a neuron neuron basis, trying to find the utility,", "tokens": [51076, 407, 341, 307, 411, 257, 588, 709, 321, 434, 1237, 412, 257, 34090, 34090, 5143, 11, 1382, 281, 915, 264, 14877, 11, 51268], "temperature": 0.0, "avg_logprob": -0.07865397354652141, "compression_ratio": 1.7914110429447854, "no_speech_prob": 0.009707726538181305}, {"id": 300, "seek": 135560, "start": 1373.6799999999998, "end": 1377.52, "text": " why might we want to do this sort of thing? Well, this could help us do things like evaluating", "tokens": [51268, 983, 1062, 321, 528, 281, 360, 341, 1333, 295, 551, 30, 1042, 11, 341, 727, 854, 505, 360, 721, 411, 27479, 51460], "temperature": 0.0, "avg_logprob": -0.07865397354652141, "compression_ratio": 1.7914110429447854, "no_speech_prob": 0.009707726538181305}, {"id": 301, "seek": 135560, "start": 1377.52, "end": 1382.3999999999999, "text": " existent features and discarding less promising features, so as to make room for new ones. Well,", "tokens": [51460, 2514, 317, 4122, 293, 31597, 278, 1570, 20257, 4122, 11, 370, 382, 281, 652, 1808, 337, 777, 2306, 13, 1042, 11, 51704], "temperature": 0.0, "avg_logprob": -0.07865397354652141, "compression_ratio": 1.7914110429447854, "no_speech_prob": 0.009707726538181305}, {"id": 302, "seek": 138240, "start": 1382.4, "end": 1386.48, "text": " why don't we just keep expanding? Well, we have like a limited amount of computation here, right?", "tokens": [50364, 983, 500, 380, 321, 445, 1066, 14702, 30, 1042, 11, 321, 362, 411, 257, 5567, 2372, 295, 24903, 510, 11, 558, 30, 50568], "temperature": 0.0, "avg_logprob": -0.10095916179396351, "compression_ratio": 1.8021680216802167, "no_speech_prob": 0.04741799086332321}, {"id": 303, "seek": 138240, "start": 1386.48, "end": 1390.88, "text": " Even though we are constantly getting more, this is where like the big world hypothesis, as they", "tokens": [50568, 2754, 1673, 321, 366, 6460, 1242, 544, 11, 341, 307, 689, 411, 264, 955, 1002, 17291, 11, 382, 436, 50788], "temperature": 0.0, "avg_logprob": -0.10095916179396351, "compression_ratio": 1.8021680216802167, "no_speech_prob": 0.04741799086332321}, {"id": 304, "seek": 138240, "start": 1390.88, "end": 1395.6000000000001, "text": " call it comes in, even though we have more and more compute, we also have just always a bigger", "tokens": [50788, 818, 309, 1487, 294, 11, 754, 1673, 321, 362, 544, 293, 544, 14722, 11, 321, 611, 362, 445, 1009, 257, 3801, 51024], "temperature": 0.0, "avg_logprob": -0.10095916179396351, "compression_ratio": 1.8021680216802167, "no_speech_prob": 0.04741799086332321}, {"id": 305, "seek": 138240, "start": 1395.6000000000001, "end": 1399.2, "text": " world so that we'll never be able to match it. So we are going to have to forget things, right?", "tokens": [51024, 1002, 370, 300, 321, 603, 1128, 312, 1075, 281, 2995, 309, 13, 407, 321, 366, 516, 281, 362, 281, 2870, 721, 11, 558, 30, 51204], "temperature": 0.0, "avg_logprob": -0.10095916179396351, "compression_ratio": 1.8021680216802167, "no_speech_prob": 0.04741799086332321}, {"id": 306, "seek": 138240, "start": 1399.2, "end": 1403.3600000000001, "text": " So what do we forget? And that's kind of what this whole CBP thing I was just talking about.", "tokens": [51204, 407, 437, 360, 321, 2870, 30, 400, 300, 311, 733, 295, 437, 341, 1379, 18745, 47, 551, 286, 390, 445, 1417, 466, 13, 51412], "temperature": 0.0, "avg_logprob": -0.10095916179396351, "compression_ratio": 1.8021680216802167, "no_speech_prob": 0.04741799086332321}, {"id": 307, "seek": 138240, "start": 1403.3600000000001, "end": 1407.68, "text": " This does something very similar to this. But beyond just dropping features, there are also", "tokens": [51412, 639, 775, 746, 588, 2531, 281, 341, 13, 583, 4399, 445, 13601, 4122, 11, 456, 366, 611, 51628], "temperature": 0.0, "avg_logprob": -0.10095916179396351, "compression_ratio": 1.8021680216802167, "no_speech_prob": 0.04741799086332321}, {"id": 308, "seek": 138240, "start": 1407.68, "end": 1411.3600000000001, "text": " other things they talk about, you know, like initializing features. This is something you have", "tokens": [51628, 661, 721, 436, 751, 466, 11, 291, 458, 11, 411, 5883, 3319, 4122, 13, 639, 307, 746, 291, 362, 51812], "temperature": 0.0, "avg_logprob": -0.10095916179396351, "compression_ratio": 1.8021680216802167, "no_speech_prob": 0.04741799086332321}, {"id": 309, "seek": 141136, "start": 1411.36, "end": 1414.56, "text": " to do. Normally, you do it when you create your own network, you initialize all your features,", "tokens": [50364, 281, 360, 13, 17424, 11, 291, 360, 309, 562, 291, 1884, 428, 1065, 3209, 11, 291, 5883, 1125, 439, 428, 4122, 11, 50524], "temperature": 0.0, "avg_logprob": -0.092601862820712, "compression_ratio": 1.814207650273224, "no_speech_prob": 0.036215782165527344}, {"id": 310, "seek": 141136, "start": 1414.56, "end": 1418.8, "text": " and then you just sort of train forever, right? But as it turns out, initialization, although", "tokens": [50524, 293, 550, 291, 445, 1333, 295, 3847, 5680, 11, 558, 30, 583, 382, 309, 4523, 484, 11, 5883, 2144, 11, 4878, 50736], "temperature": 0.0, "avg_logprob": -0.092601862820712, "compression_ratio": 1.814207650273224, "no_speech_prob": 0.036215782165527344}, {"id": 311, "seek": 141136, "start": 1418.8, "end": 1422.4799999999998, "text": " it's kind of brushed to the side, it's actually much more important than you might think. Oftentimes,", "tokens": [50736, 309, 311, 733, 295, 40694, 281, 264, 1252, 11, 309, 311, 767, 709, 544, 1021, 813, 291, 1062, 519, 13, 46636, 11, 50920], "temperature": 0.0, "avg_logprob": -0.092601862820712, "compression_ratio": 1.814207650273224, "no_speech_prob": 0.036215782165527344}, {"id": 312, "seek": 141136, "start": 1422.4799999999998, "end": 1426.4799999999998, "text": " maybe not often, but occasionally, I'll implement a paper. And I've had this happen to me a couple", "tokens": [50920, 1310, 406, 2049, 11, 457, 16895, 11, 286, 603, 4445, 257, 3035, 13, 400, 286, 600, 632, 341, 1051, 281, 385, 257, 1916, 51120], "temperature": 0.0, "avg_logprob": -0.092601862820712, "compression_ratio": 1.814207650273224, "no_speech_prob": 0.036215782165527344}, {"id": 313, "seek": 141136, "start": 1426.4799999999998, "end": 1430.1599999999999, "text": " of times, we'll implement it, and it won't work. And I'll go back and read the paper very carefully", "tokens": [51120, 295, 1413, 11, 321, 603, 4445, 309, 11, 293, 309, 1582, 380, 589, 13, 400, 286, 603, 352, 646, 293, 1401, 264, 3035, 588, 7500, 51304], "temperature": 0.0, "avg_logprob": -0.092601862820712, "compression_ratio": 1.814207650273224, "no_speech_prob": 0.036215782165527344}, {"id": 314, "seek": 141136, "start": 1430.1599999999999, "end": 1434.24, "text": " and realize, oh, they use this very specific form of initialization for their network,", "tokens": [51304, 293, 4325, 11, 1954, 11, 436, 764, 341, 588, 2685, 1254, 295, 5883, 2144, 337, 641, 3209, 11, 51508], "temperature": 0.0, "avg_logprob": -0.092601862820712, "compression_ratio": 1.814207650273224, "no_speech_prob": 0.036215782165527344}, {"id": 315, "seek": 141136, "start": 1434.24, "end": 1438.24, "text": " and they try it, and suddenly everything's working. This has happened to me, especially", "tokens": [51508, 293, 436, 853, 309, 11, 293, 5800, 1203, 311, 1364, 13, 639, 575, 2011, 281, 385, 11, 2318, 51708], "temperature": 0.0, "avg_logprob": -0.092601862820712, "compression_ratio": 1.814207650273224, "no_speech_prob": 0.036215782165527344}, {"id": 316, "seek": 143824, "start": 1438.24, "end": 1442.48, "text": " in papers that don't use backprop, but like use biologically inspired training. I found that", "tokens": [50364, 294, 10577, 300, 500, 380, 764, 646, 79, 1513, 11, 457, 411, 764, 3228, 17157, 7547, 3097, 13, 286, 1352, 300, 50576], "temperature": 0.0, "avg_logprob": -0.10482772300983298, "compression_ratio": 1.7530487804878048, "no_speech_prob": 0.030211124569177628}, {"id": 317, "seek": 143824, "start": 1442.48, "end": 1446.32, "text": " this is actually more important than you might expect. Another thing to talk about is like,", "tokens": [50576, 341, 307, 767, 544, 1021, 813, 291, 1062, 2066, 13, 3996, 551, 281, 751, 466, 307, 411, 11, 50768], "temperature": 0.0, "avg_logprob": -0.10482772300983298, "compression_ratio": 1.7530487804878048, "no_speech_prob": 0.030211124569177628}, {"id": 318, "seek": 143824, "start": 1446.32, "end": 1451.36, "text": " how should you adjust neurons or weights? One way to do this is obviously backprop. And I don't", "tokens": [50768, 577, 820, 291, 4369, 22027, 420, 17443, 30, 1485, 636, 281, 360, 341, 307, 2745, 646, 79, 1513, 13, 400, 286, 500, 380, 51020], "temperature": 0.0, "avg_logprob": -0.10482772300983298, "compression_ratio": 1.7530487804878048, "no_speech_prob": 0.030211124569177628}, {"id": 319, "seek": 143824, "start": 1451.36, "end": 1455.04, "text": " think they're, they're definitely not proposing we just get rid of backprop. I think they realize", "tokens": [51020, 519, 436, 434, 11, 436, 434, 2138, 406, 29939, 321, 445, 483, 3973, 295, 646, 79, 1513, 13, 286, 519, 436, 4325, 51204], "temperature": 0.0, "avg_logprob": -0.10482772300983298, "compression_ratio": 1.7530487804878048, "no_speech_prob": 0.030211124569177628}, {"id": 320, "seek": 143824, "start": 1455.04, "end": 1460.72, "text": " backprop scales very well. It works very well, but it might not be perfectly suited for a problem.", "tokens": [51204, 646, 79, 1513, 17408, 588, 731, 13, 467, 1985, 588, 731, 11, 457, 309, 1062, 406, 312, 6239, 24736, 337, 257, 1154, 13, 51488], "temperature": 0.0, "avg_logprob": -0.10482772300983298, "compression_ratio": 1.7530487804878048, "no_speech_prob": 0.030211124569177628}, {"id": 321, "seek": 143824, "start": 1460.72, "end": 1465.1200000000001, "text": " So how can we look at this sort of more, I don't know if I want to call like an individual neuron", "tokens": [51488, 407, 577, 393, 321, 574, 412, 341, 1333, 295, 544, 11, 286, 500, 380, 458, 498, 286, 528, 281, 818, 411, 364, 2609, 34090, 51708], "temperature": 0.0, "avg_logprob": -0.10482772300983298, "compression_ratio": 1.7530487804878048, "no_speech_prob": 0.030211124569177628}, {"id": 322, "seek": 146512, "start": 1465.12, "end": 1468.8, "text": " approach, but something like that, more than neuron to neuron level, looking at them as features,", "tokens": [50364, 3109, 11, 457, 746, 411, 300, 11, 544, 813, 34090, 281, 34090, 1496, 11, 1237, 412, 552, 382, 4122, 11, 50548], "temperature": 0.0, "avg_logprob": -0.1066464089058541, "compression_ratio": 1.7852760736196318, "no_speech_prob": 0.008577156811952591}, {"id": 323, "seek": 146512, "start": 1468.8, "end": 1473.76, "text": " features with utilities and features that we want to drop or get rid of. How can we implement like", "tokens": [50548, 4122, 365, 30482, 293, 4122, 300, 321, 528, 281, 3270, 420, 483, 3973, 295, 13, 1012, 393, 321, 4445, 411, 50796], "temperature": 0.0, "avg_logprob": -0.1066464089058541, "compression_ratio": 1.7852760736196318, "no_speech_prob": 0.008577156811952591}, {"id": 324, "seek": 146512, "start": 1473.76, "end": 1478.08, "text": " all these sorts of things? Like what, what would this look like in, in play? So maybe one example", "tokens": [50796, 439, 613, 7527, 295, 721, 30, 1743, 437, 11, 437, 576, 341, 574, 411, 294, 11, 294, 862, 30, 407, 1310, 472, 1365, 51012], "temperature": 0.0, "avg_logprob": -0.1066464089058541, "compression_ratio": 1.7852760736196318, "no_speech_prob": 0.008577156811952591}, {"id": 325, "seek": 146512, "start": 1478.08, "end": 1484.0, "text": " of this, right? And this is very similar to what CBP does is say one, we would do an initialization", "tokens": [51012, 295, 341, 11, 558, 30, 400, 341, 307, 588, 2531, 281, 437, 18745, 47, 775, 307, 584, 472, 11, 321, 576, 360, 364, 5883, 2144, 51308], "temperature": 0.0, "avg_logprob": -0.1066464089058541, "compression_ratio": 1.7852760736196318, "no_speech_prob": 0.008577156811952591}, {"id": 326, "seek": 146512, "start": 1484.0, "end": 1488.0, "text": " just as normal. We in it all our, our nodes, but this is one thing we could look into what are", "tokens": [51308, 445, 382, 2710, 13, 492, 294, 309, 439, 527, 11, 527, 13891, 11, 457, 341, 307, 472, 551, 321, 727, 574, 666, 437, 366, 51508], "temperature": 0.0, "avg_logprob": -0.1066464089058541, "compression_ratio": 1.7852760736196318, "no_speech_prob": 0.008577156811952591}, {"id": 327, "seek": 146512, "start": 1488.0, "end": 1491.76, "text": " better ways to in it for these types of things. The second thing we might want to do is use,", "tokens": [51508, 1101, 2098, 281, 294, 309, 337, 613, 3467, 295, 721, 13, 440, 1150, 551, 321, 1062, 528, 281, 360, 307, 764, 11, 51696], "temperature": 0.0, "avg_logprob": -0.1066464089058541, "compression_ratio": 1.7852760736196318, "no_speech_prob": 0.008577156811952591}, {"id": 328, "seek": 149176, "start": 1491.76, "end": 1495.52, "text": " use backprop, right? We don't need to get rid of backprop. Backprop is strong, but as we use", "tokens": [50364, 764, 646, 79, 1513, 11, 558, 30, 492, 500, 380, 643, 281, 483, 3973, 295, 646, 79, 1513, 13, 5833, 79, 1513, 307, 2068, 11, 457, 382, 321, 764, 50552], "temperature": 0.0, "avg_logprob": -0.10628754571573623, "compression_ratio": 1.8666666666666667, "no_speech_prob": 0.03209778666496277}, {"id": 329, "seek": 149176, "start": 1495.52, "end": 1500.8799999999999, "text": " backprop, maybe then we evaluate the utility. And then four, we remove, or maybe I should just say", "tokens": [50552, 646, 79, 1513, 11, 1310, 550, 321, 13059, 264, 14877, 13, 400, 550, 1451, 11, 321, 4159, 11, 420, 1310, 286, 820, 445, 584, 50820], "temperature": 0.0, "avg_logprob": -0.10628754571573623, "compression_ratio": 1.8666666666666667, "no_speech_prob": 0.03209778666496277}, {"id": 330, "seek": 149176, "start": 1500.8799999999999, "end": 1505.04, "text": " we drop the nodes that are not super useful. And then the fifth thing we could do is we could", "tokens": [50820, 321, 3270, 264, 13891, 300, 366, 406, 1687, 4420, 13, 400, 550, 264, 9266, 551, 321, 727, 360, 307, 321, 727, 51028], "temperature": 0.0, "avg_logprob": -0.10628754571573623, "compression_ratio": 1.8666666666666667, "no_speech_prob": 0.03209778666496277}, {"id": 331, "seek": 149176, "start": 1505.04, "end": 1510.0, "text": " reinit, but instead of just re-init normally, maybe we could have some sort of meta learning", "tokens": [51028, 6561, 270, 11, 457, 2602, 295, 445, 319, 12, 259, 270, 5646, 11, 1310, 321, 727, 362, 512, 1333, 295, 19616, 2539, 51276], "temperature": 0.0, "avg_logprob": -0.10628754571573623, "compression_ratio": 1.8666666666666667, "no_speech_prob": 0.03209778666496277}, {"id": 332, "seek": 149176, "start": 1510.0, "end": 1514.56, "text": " that figures out how to initialize nodes such that things will be learned faster, right?", "tokens": [51276, 300, 9624, 484, 577, 281, 5883, 1125, 13891, 1270, 300, 721, 486, 312, 3264, 4663, 11, 558, 30, 51504], "temperature": 0.0, "avg_logprob": -0.10628754571573623, "compression_ratio": 1.8666666666666667, "no_speech_prob": 0.03209778666496277}, {"id": 333, "seek": 149176, "start": 1514.56, "end": 1518.16, "text": " So there's, there's some sort of meta stuff you could do here. So this would be one example.", "tokens": [51504, 407, 456, 311, 11, 456, 311, 512, 1333, 295, 19616, 1507, 291, 727, 360, 510, 13, 407, 341, 576, 312, 472, 1365, 13, 51684], "temperature": 0.0, "avg_logprob": -0.10628754571573623, "compression_ratio": 1.8666666666666667, "no_speech_prob": 0.03209778666496277}, {"id": 334, "seek": 151816, "start": 1518.16, "end": 1523.1200000000001, "text": " And again, this is very similar. That's done what's done in CBP. I'll link my video to this", "tokens": [50364, 400, 797, 11, 341, 307, 588, 2531, 13, 663, 311, 1096, 437, 311, 1096, 294, 18745, 47, 13, 286, 603, 2113, 452, 960, 281, 341, 50612], "temperature": 0.0, "avg_logprob": -0.08895258139107974, "compression_ratio": 1.6918429003021147, "no_speech_prob": 0.010012995451688766}, {"id": 335, "seek": 151816, "start": 1523.1200000000001, "end": 1527.0400000000002, "text": " in the description. Actually, if you're interested in checking out, but this is like one alternative,", "tokens": [50612, 294, 264, 3855, 13, 5135, 11, 498, 291, 434, 3102, 294, 8568, 484, 11, 457, 341, 307, 411, 472, 8535, 11, 50808], "temperature": 0.0, "avg_logprob": -0.08895258139107974, "compression_ratio": 1.6918429003021147, "no_speech_prob": 0.010012995451688766}, {"id": 336, "seek": 151816, "start": 1527.0400000000002, "end": 1530.8000000000002, "text": " right? That still makes use of backprop, but might be better suited for something like", "tokens": [50808, 558, 30, 663, 920, 1669, 764, 295, 646, 79, 1513, 11, 457, 1062, 312, 1101, 24736, 337, 746, 411, 50996], "temperature": 0.0, "avg_logprob": -0.08895258139107974, "compression_ratio": 1.6918429003021147, "no_speech_prob": 0.010012995451688766}, {"id": 337, "seek": 151816, "start": 1530.8000000000002, "end": 1535.1200000000001, "text": " continue learning or reinforcement learning. And could give you more control over how you", "tokens": [50996, 2354, 2539, 420, 29280, 2539, 13, 400, 727, 976, 291, 544, 1969, 670, 577, 291, 51212], "temperature": 0.0, "avg_logprob": -0.08895258139107974, "compression_ratio": 1.6918429003021147, "no_speech_prob": 0.010012995451688766}, {"id": 338, "seek": 151816, "start": 1535.1200000000001, "end": 1540.3200000000002, "text": " go about developing these algorithms. As I just mentioned, I think this is a very exciting way", "tokens": [51212, 352, 466, 6416, 613, 14642, 13, 1018, 286, 445, 2835, 11, 286, 519, 341, 307, 257, 588, 4670, 636, 51472], "temperature": 0.0, "avg_logprob": -0.08895258139107974, "compression_ratio": 1.6918429003021147, "no_speech_prob": 0.010012995451688766}, {"id": 339, "seek": 151816, "start": 1540.3200000000002, "end": 1545.3600000000001, "text": " to think about how to model things, thinking of individual features. It's almost less limiting", "tokens": [51472, 281, 519, 466, 577, 281, 2316, 721, 11, 1953, 295, 2609, 4122, 13, 467, 311, 1920, 1570, 22083, 51724], "temperature": 0.0, "avg_logprob": -0.08895258139107974, "compression_ratio": 1.6918429003021147, "no_speech_prob": 0.010012995451688766}, {"id": 340, "seek": 154536, "start": 1545.36, "end": 1549.76, "text": " the standard approach that I read out right here. But at the same time, it is dangerous because let's,", "tokens": [50364, 264, 3832, 3109, 300, 286, 1401, 484, 558, 510, 13, 583, 412, 264, 912, 565, 11, 309, 307, 5795, 570, 718, 311, 11, 50584], "temperature": 0.0, "avg_logprob": -0.10046602755176778, "compression_ratio": 1.7253731343283583, "no_speech_prob": 0.01971832476556301}, {"id": 341, "seek": 154536, "start": 1549.76, "end": 1555.28, "text": " let's not kid ourselves, back propagation works very well. It scales very well. So when you do", "tokens": [50584, 718, 311, 406, 1636, 4175, 11, 646, 38377, 1985, 588, 731, 13, 467, 17408, 588, 731, 13, 407, 562, 291, 360, 50860], "temperature": 0.0, "avg_logprob": -0.10046602755176778, "compression_ratio": 1.7253731343283583, "no_speech_prob": 0.01971832476556301}, {"id": 342, "seek": 154536, "start": 1555.28, "end": 1558.6399999999999, "text": " want to do something different, you're going up against something that's already very well", "tokens": [50860, 528, 281, 360, 746, 819, 11, 291, 434, 516, 493, 1970, 746, 300, 311, 1217, 588, 731, 51028], "temperature": 0.0, "avg_logprob": -0.10046602755176778, "compression_ratio": 1.7253731343283583, "no_speech_prob": 0.01971832476556301}, {"id": 343, "seek": 154536, "start": 1558.6399999999999, "end": 1563.1999999999998, "text": " established and is known to work very well. So that's why I say it's kind of dangerous. It's,", "tokens": [51028, 7545, 293, 307, 2570, 281, 589, 588, 731, 13, 407, 300, 311, 983, 286, 584, 309, 311, 733, 295, 5795, 13, 467, 311, 11, 51256], "temperature": 0.0, "avg_logprob": -0.10046602755176778, "compression_ratio": 1.7253731343283583, "no_speech_prob": 0.01971832476556301}, {"id": 344, "seek": 154536, "start": 1563.1999999999998, "end": 1568.6399999999999, "text": " it's exciting, but hard to get working and not very explorative. It's a very underexplored area,", "tokens": [51256, 309, 311, 4670, 11, 457, 1152, 281, 483, 1364, 293, 406, 588, 24765, 1166, 13, 467, 311, 257, 588, 674, 323, 87, 564, 2769, 1859, 11, 51528], "temperature": 0.0, "avg_logprob": -0.10046602755176778, "compression_ratio": 1.7253731343283583, "no_speech_prob": 0.01971832476556301}, {"id": 345, "seek": 154536, "start": 1568.6399999999999, "end": 1573.76, "text": " I think. Okay, step number three, continue GVF prediction learning. This is where we start getting", "tokens": [51528, 286, 519, 13, 1033, 11, 1823, 1230, 1045, 11, 2354, 460, 53, 37, 17630, 2539, 13, 639, 307, 689, 321, 722, 1242, 51784], "temperature": 0.0, "avg_logprob": -0.10046602755176778, "compression_ratio": 1.7253731343283583, "no_speech_prob": 0.01971832476556301}, {"id": 346, "seek": 157376, "start": 1573.76, "end": 1578.8, "text": " into non-IID data. So that's independent, individually distributed data. So when we're", "tokens": [50364, 666, 2107, 12, 40, 2777, 1412, 13, 407, 300, 311, 6695, 11, 16652, 12631, 1412, 13, 407, 562, 321, 434, 50616], "temperature": 0.0, "avg_logprob": -0.08347013417412252, "compression_ratio": 1.859531772575251, "no_speech_prob": 0.011686199344694614}, {"id": 347, "seek": 157376, "start": 1578.8, "end": 1583.68, "text": " working with reinforcement learning or in the real world, we are often, we're going to be working", "tokens": [50616, 1364, 365, 29280, 2539, 420, 294, 264, 957, 1002, 11, 321, 366, 2049, 11, 321, 434, 516, 281, 312, 1364, 50860], "temperature": 0.0, "avg_logprob": -0.08347013417412252, "compression_ratio": 1.859531772575251, "no_speech_prob": 0.011686199344694614}, {"id": 348, "seek": 157376, "start": 1583.68, "end": 1588.32, "text": " with streams of data that come, you know, in, in a row. But one thing to note is that many", "tokens": [50860, 365, 15842, 295, 1412, 300, 808, 11, 291, 458, 11, 294, 11, 294, 257, 5386, 13, 583, 472, 551, 281, 3637, 307, 300, 867, 51092], "temperature": 0.0, "avg_logprob": -0.08347013417412252, "compression_ratio": 1.859531772575251, "no_speech_prob": 0.011686199344694614}, {"id": 349, "seek": 157376, "start": 1588.32, "end": 1595.52, "text": " machine learning methods, the theory is entirely based upon data that is IID. Or in other words,", "tokens": [51092, 3479, 2539, 7150, 11, 264, 5261, 307, 7696, 2361, 3564, 1412, 300, 307, 286, 2777, 13, 1610, 294, 661, 2283, 11, 51452], "temperature": 0.0, "avg_logprob": -0.08347013417412252, "compression_ratio": 1.859531772575251, "no_speech_prob": 0.011686199344694614}, {"id": 350, "seek": 157376, "start": 1595.52, "end": 1599.6, "text": " we are essentially breaking that when we move on to reinforcement learning most of the time.", "tokens": [51452, 321, 366, 4476, 7697, 300, 562, 321, 1286, 322, 281, 29280, 2539, 881, 295, 264, 565, 13, 51656], "temperature": 0.0, "avg_logprob": -0.08347013417412252, "compression_ratio": 1.859531772575251, "no_speech_prob": 0.011686199344694614}, {"id": 351, "seek": 157376, "start": 1599.6, "end": 1602.8799999999999, "text": " That's why people like to use, or one of the reasons people like to use things like replay", "tokens": [51656, 663, 311, 983, 561, 411, 281, 764, 11, 420, 472, 295, 264, 4112, 561, 411, 281, 764, 721, 411, 23836, 51820], "temperature": 0.0, "avg_logprob": -0.08347013417412252, "compression_ratio": 1.859531772575251, "no_speech_prob": 0.011686199344694614}, {"id": 352, "seek": 160288, "start": 1602.88, "end": 1607.8400000000001, "text": " buffers is because it helps you get this IID distribution, which can help you learn. Now,", "tokens": [50364, 9204, 433, 307, 570, 309, 3665, 291, 483, 341, 286, 2777, 7316, 11, 597, 393, 854, 291, 1466, 13, 823, 11, 50612], "temperature": 0.0, "avg_logprob": -0.07788147528966267, "compression_ratio": 1.7708978328173375, "no_speech_prob": 0.010651904158294201}, {"id": 353, "seek": 160288, "start": 1607.8400000000001, "end": 1612.24, "text": " we have methods of somewhat counteracting this, but it is nevertheless still an issue. But one", "tokens": [50612, 321, 362, 7150, 295, 8344, 5682, 41090, 341, 11, 457, 309, 307, 26924, 920, 364, 2734, 13, 583, 472, 50832], "temperature": 0.0, "avg_logprob": -0.07788147528966267, "compression_ratio": 1.7708978328173375, "no_speech_prob": 0.010651904158294201}, {"id": 354, "seek": 160288, "start": 1612.24, "end": 1616.64, "text": " thing they also put in here is GVFs. I honestly don't know why they put this in here, but we can", "tokens": [50832, 551, 436, 611, 829, 294, 510, 307, 460, 53, 37, 82, 13, 286, 6095, 500, 380, 458, 983, 436, 829, 341, 294, 510, 11, 457, 321, 393, 51052], "temperature": 0.0, "avg_logprob": -0.07788147528966267, "compression_ratio": 1.7708978328173375, "no_speech_prob": 0.010651904158294201}, {"id": 355, "seek": 160288, "start": 1616.64, "end": 1620.88, "text": " talk about it because they do. I just don't know why they put them together. So GVFs, this is", "tokens": [51052, 751, 466, 309, 570, 436, 360, 13, 286, 445, 500, 380, 458, 983, 436, 829, 552, 1214, 13, 407, 460, 53, 37, 82, 11, 341, 307, 51264], "temperature": 0.0, "avg_logprob": -0.07788147528966267, "compression_ratio": 1.7708978328173375, "no_speech_prob": 0.010651904158294201}, {"id": 356, "seek": 160288, "start": 1620.88, "end": 1625.2, "text": " generalized value function. And this is a fairly simple idea. If you're familiar with normal value", "tokens": [51264, 44498, 2158, 2445, 13, 400, 341, 307, 257, 6457, 2199, 1558, 13, 759, 291, 434, 4963, 365, 2710, 2158, 51480], "temperature": 0.0, "avg_logprob": -0.07788147528966267, "compression_ratio": 1.7708978328173375, "no_speech_prob": 0.010651904158294201}, {"id": 357, "seek": 160288, "start": 1625.2, "end": 1630.0, "text": " functions, essentially a value function measures like how good a state is, like what the expected", "tokens": [51480, 6828, 11, 4476, 257, 2158, 2445, 8000, 411, 577, 665, 257, 1785, 307, 11, 411, 437, 264, 5176, 51720], "temperature": 0.0, "avg_logprob": -0.07788147528966267, "compression_ratio": 1.7708978328173375, "no_speech_prob": 0.010651904158294201}, {"id": 358, "seek": 163000, "start": 1630.08, "end": 1634.4, "text": " reward is. Whereas a generalized value function, well, it's essentially just a value function", "tokens": [50368, 7782, 307, 13, 13813, 257, 44498, 2158, 2445, 11, 731, 11, 309, 311, 4476, 445, 257, 2158, 2445, 50584], "temperature": 0.0, "avg_logprob": -0.08263919150182443, "compression_ratio": 1.827922077922078, "no_speech_prob": 0.015904482454061508}, {"id": 359, "seek": 163000, "start": 1634.4, "end": 1638.72, "text": " for something other than the reward, or it could be the reward, or any other feature, right? So", "tokens": [50584, 337, 746, 661, 813, 264, 7782, 11, 420, 309, 727, 312, 264, 7782, 11, 420, 604, 661, 4111, 11, 558, 30, 407, 50800], "temperature": 0.0, "avg_logprob": -0.08263919150182443, "compression_ratio": 1.827922077922078, "no_speech_prob": 0.015904482454061508}, {"id": 360, "seek": 163000, "start": 1638.72, "end": 1643.28, "text": " you're predicting something. So they're just predictions or predictions about the world", "tokens": [50800, 291, 434, 32884, 746, 13, 407, 436, 434, 445, 21264, 420, 21264, 466, 264, 1002, 51028], "temperature": 0.0, "avg_logprob": -0.08263919150182443, "compression_ratio": 1.827922077922078, "no_speech_prob": 0.015904482454061508}, {"id": 361, "seek": 163000, "start": 1643.28, "end": 1648.16, "text": " that are not reward based, not necessarily reward based. The last thing I'll mention about this", "tokens": [51028, 300, 366, 406, 7782, 2361, 11, 406, 4725, 7782, 2361, 13, 440, 1036, 551, 286, 603, 2152, 466, 341, 51272], "temperature": 0.0, "avg_logprob": -0.08263919150182443, "compression_ratio": 1.827922077922078, "no_speech_prob": 0.015904482454061508}, {"id": 362, "seek": 163000, "start": 1648.16, "end": 1652.32, "text": " step three is this is where perception comes in. And I think the reason they say this is where", "tokens": [51272, 1823, 1045, 307, 341, 307, 689, 12860, 1487, 294, 13, 400, 286, 519, 264, 1778, 436, 584, 341, 307, 689, 51480], "temperature": 0.0, "avg_logprob": -0.08263919150182443, "compression_ratio": 1.827922077922078, "no_speech_prob": 0.015904482454061508}, {"id": 363, "seek": 163000, "start": 1652.32, "end": 1656.72, "text": " perception really starts to come in is because of the non IID data. That means if we have like", "tokens": [51480, 12860, 534, 3719, 281, 808, 294, 307, 570, 295, 264, 2107, 286, 2777, 1412, 13, 663, 1355, 498, 321, 362, 411, 51700], "temperature": 0.0, "avg_logprob": -0.08263919150182443, "compression_ratio": 1.827922077922078, "no_speech_prob": 0.015904482454061508}, {"id": 364, "seek": 165672, "start": 1656.72, "end": 1660.56, "text": " sequential data, and we need to remember stuff that happened in the past, well, part of our", "tokens": [50364, 42881, 1412, 11, 293, 321, 643, 281, 1604, 1507, 300, 2011, 294, 264, 1791, 11, 731, 11, 644, 295, 527, 50556], "temperature": 0.0, "avg_logprob": -0.08922085672054651, "compression_ratio": 1.8186813186813187, "no_speech_prob": 0.00460940832272172}, {"id": 365, "seek": 165672, "start": 1660.56, "end": 1664.0, "text": " perception needs to be remembering the important thing we need to know what's important, like if", "tokens": [50556, 12860, 2203, 281, 312, 20719, 264, 1021, 551, 321, 643, 281, 458, 437, 311, 1021, 11, 411, 498, 50728], "temperature": 0.0, "avg_logprob": -0.08922085672054651, "compression_ratio": 1.8186813186813187, "no_speech_prob": 0.00460940832272172}, {"id": 366, "seek": 165672, "start": 1664.0, "end": 1668.48, "text": " there's a key over there that I can't see anymore, I need to remember it's there so I can, you know,", "tokens": [50728, 456, 311, 257, 2141, 670, 456, 300, 286, 393, 380, 536, 3602, 11, 286, 643, 281, 1604, 309, 311, 456, 370, 286, 393, 11, 291, 458, 11, 50952], "temperature": 0.0, "avg_logprob": -0.08922085672054651, "compression_ratio": 1.8186813186813187, "no_speech_prob": 0.00460940832272172}, {"id": 367, "seek": 165672, "start": 1668.48, "end": 1672.08, "text": " go back and get it if I see a locked door or something like that. The fourth thing we've been", "tokens": [50952, 352, 646, 293, 483, 309, 498, 286, 536, 257, 9376, 2853, 420, 746, 411, 300, 13, 440, 6409, 551, 321, 600, 668, 51132], "temperature": 0.0, "avg_logprob": -0.08922085672054651, "compression_ratio": 1.8186813186813187, "no_speech_prob": 0.00460940832272172}, {"id": 368, "seek": 165672, "start": 1672.08, "end": 1677.52, "text": " on now is continual actor critic control. And that is right up until this step. You might not", "tokens": [51132, 322, 586, 307, 1421, 901, 8747, 7850, 1969, 13, 400, 300, 307, 558, 493, 1826, 341, 1823, 13, 509, 1062, 406, 51404], "temperature": 0.0, "avg_logprob": -0.08922085672054651, "compression_ratio": 1.8186813186813187, "no_speech_prob": 0.00460940832272172}, {"id": 369, "seek": 165672, "start": 1677.52, "end": 1681.52, "text": " have noticed there was no control involved. We're not actually interacting with the environment,", "tokens": [51404, 362, 5694, 456, 390, 572, 1969, 3288, 13, 492, 434, 406, 767, 18017, 365, 264, 2823, 11, 51604], "temperature": 0.0, "avg_logprob": -0.08922085672054651, "compression_ratio": 1.8186813186813187, "no_speech_prob": 0.00460940832272172}, {"id": 370, "seek": 165672, "start": 1681.52, "end": 1685.2, "text": " but rather up to this point, we were just predicting what was happening. There's really", "tokens": [51604, 457, 2831, 493, 281, 341, 935, 11, 321, 645, 445, 32884, 437, 390, 2737, 13, 821, 311, 534, 51788], "temperature": 0.0, "avg_logprob": -0.08922085672054651, "compression_ratio": 1.8186813186813187, "no_speech_prob": 0.00460940832272172}, {"id": 371, "seek": 168520, "start": 1685.2, "end": 1688.88, "text": " not much more to say here. Now that we're bringing control in the mix, we need to figure out how to", "tokens": [50364, 406, 709, 544, 281, 584, 510, 13, 823, 300, 321, 434, 5062, 1969, 294, 264, 2890, 11, 321, 643, 281, 2573, 484, 577, 281, 50548], "temperature": 0.0, "avg_logprob": -0.0939186414082845, "compression_ratio": 1.7537993920972645, "no_speech_prob": 0.006488049868494272}, {"id": 372, "seek": 168520, "start": 1688.88, "end": 1694.48, "text": " get that working with everything we've done so far. So then step five, average reward GVF learning.", "tokens": [50548, 483, 300, 1364, 365, 1203, 321, 600, 1096, 370, 1400, 13, 407, 550, 1823, 1732, 11, 4274, 7782, 460, 53, 37, 2539, 13, 50828], "temperature": 0.0, "avg_logprob": -0.0939186414082845, "compression_ratio": 1.7537993920972645, "no_speech_prob": 0.006488049868494272}, {"id": 373, "seek": 168520, "start": 1694.48, "end": 1699.1200000000001, "text": " Now, average reward is I think something lots of people are not very familiar with. Understandably,", "tokens": [50828, 823, 11, 4274, 7782, 307, 286, 519, 746, 3195, 295, 561, 366, 406, 588, 4963, 365, 13, 26093, 1188, 11, 51060], "temperature": 0.0, "avg_logprob": -0.0939186414082845, "compression_ratio": 1.7537993920972645, "no_speech_prob": 0.006488049868494272}, {"id": 374, "seek": 168520, "start": 1699.1200000000001, "end": 1705.92, "text": " so it's it's not very popular, I guess you could say. And if you've never thought about it,", "tokens": [51060, 370, 309, 311, 309, 311, 406, 588, 3743, 11, 286, 2041, 291, 727, 584, 13, 400, 498, 291, 600, 1128, 1194, 466, 309, 11, 51400], "temperature": 0.0, "avg_logprob": -0.0939186414082845, "compression_ratio": 1.7537993920972645, "no_speech_prob": 0.006488049868494272}, {"id": 375, "seek": 168520, "start": 1705.92, "end": 1709.52, "text": " using something like a discount factor and for those that aren't familiar with reinforcement", "tokens": [51400, 1228, 746, 411, 257, 11635, 5952, 293, 337, 729, 300, 3212, 380, 4963, 365, 29280, 51580], "temperature": 0.0, "avg_logprob": -0.0939186414082845, "compression_ratio": 1.7537993920972645, "no_speech_prob": 0.006488049868494272}, {"id": 376, "seek": 168520, "start": 1709.52, "end": 1714.32, "text": " learning, usually you have this gamma parameter. And this is called the discount factor. And", "tokens": [51580, 2539, 11, 2673, 291, 362, 341, 15546, 13075, 13, 400, 341, 307, 1219, 264, 11635, 5952, 13, 400, 51820], "temperature": 0.0, "avg_logprob": -0.0939186414082845, "compression_ratio": 1.7537993920972645, "no_speech_prob": 0.006488049868494272}, {"id": 377, "seek": 171432, "start": 1714.32, "end": 1719.6, "text": " essentially it weighs how important current rewards are versus future rewards. And it's,", "tokens": [50364, 4476, 309, 24911, 577, 1021, 2190, 17203, 366, 5717, 2027, 17203, 13, 400, 309, 311, 11, 50628], "temperature": 0.0, "avg_logprob": -0.08766344760326629, "compression_ratio": 1.8576051779935274, "no_speech_prob": 0.0011335349408909678}, {"id": 378, "seek": 171432, "start": 1719.6, "end": 1722.72, "text": " if you think about it, it's kind of weird. The other thing that we could do, as you see right", "tokens": [50628, 498, 291, 519, 466, 309, 11, 309, 311, 733, 295, 3657, 13, 440, 661, 551, 300, 321, 727, 360, 11, 382, 291, 536, 558, 50784], "temperature": 0.0, "avg_logprob": -0.08766344760326629, "compression_ratio": 1.8576051779935274, "no_speech_prob": 0.0011335349408909678}, {"id": 379, "seek": 171432, "start": 1722.72, "end": 1726.72, "text": " here is just have an average reward and say we want to maximize the average reward, which would", "tokens": [50784, 510, 307, 445, 362, 364, 4274, 7782, 293, 584, 321, 528, 281, 19874, 264, 4274, 7782, 11, 597, 576, 50984], "temperature": 0.0, "avg_logprob": -0.08766344760326629, "compression_ratio": 1.8576051779935274, "no_speech_prob": 0.0011335349408909678}, {"id": 380, "seek": 171432, "start": 1726.72, "end": 1731.4399999999998, "text": " essentially be in the same as saying we just want to maximize the reward total over everything.", "tokens": [50984, 4476, 312, 294, 264, 912, 382, 1566, 321, 445, 528, 281, 19874, 264, 7782, 3217, 670, 1203, 13, 51220], "temperature": 0.0, "avg_logprob": -0.08766344760326629, "compression_ratio": 1.8576051779935274, "no_speech_prob": 0.0011335349408909678}, {"id": 381, "seek": 171432, "start": 1731.4399999999998, "end": 1735.4399999999998, "text": " The thing is, when you do something like having gamma, you're really, I actually don't want to", "tokens": [51220, 440, 551, 307, 11, 562, 291, 360, 746, 411, 1419, 15546, 11, 291, 434, 534, 11, 286, 767, 500, 380, 528, 281, 51420], "temperature": 0.0, "avg_logprob": -0.08766344760326629, "compression_ratio": 1.8576051779935274, "no_speech_prob": 0.0011335349408909678}, {"id": 382, "seek": 171432, "start": 1735.4399999999998, "end": 1740.32, "text": " get into this too much. But if you've ever thought about it, it's kind of janky, right? And the argument", "tokens": [51420, 483, 666, 341, 886, 709, 13, 583, 498, 291, 600, 1562, 1194, 466, 309, 11, 309, 311, 733, 295, 361, 657, 88, 11, 558, 30, 400, 264, 6770, 51664], "temperature": 0.0, "avg_logprob": -0.08766344760326629, "compression_ratio": 1.8576051779935274, "no_speech_prob": 0.0011335349408909678}, {"id": 383, "seek": 174032, "start": 1740.32, "end": 1744.6399999999999, "text": " for why to use average reward instead is not just the jankiness, but I don't want to get into", "tokens": [50364, 337, 983, 281, 764, 4274, 7782, 2602, 307, 406, 445, 264, 361, 657, 1324, 11, 457, 286, 500, 380, 528, 281, 483, 666, 50580], "temperature": 0.0, "avg_logprob": -0.10159442474792053, "compression_ratio": 1.792332268370607, "no_speech_prob": 0.04335959628224373}, {"id": 384, "seek": 174032, "start": 1744.6399999999999, "end": 1748.0, "text": " it too much. So if you're interested, there is a whole section on this in the RL book that you", "tokens": [50580, 309, 886, 709, 13, 407, 498, 291, 434, 3102, 11, 456, 307, 257, 1379, 3541, 322, 341, 294, 264, 497, 43, 1446, 300, 291, 50748], "temperature": 0.0, "avg_logprob": -0.10159442474792053, "compression_ratio": 1.792332268370607, "no_speech_prob": 0.04335959628224373}, {"id": 385, "seek": 174032, "start": 1748.0, "end": 1753.52, "text": " can check out. So step six, and this is going to be kind of cut us off at the first half of this", "tokens": [50748, 393, 1520, 484, 13, 407, 1823, 2309, 11, 293, 341, 307, 516, 281, 312, 733, 295, 1723, 505, 766, 412, 264, 700, 1922, 295, 341, 51024], "temperature": 0.0, "avg_logprob": -0.10159442474792053, "compression_ratio": 1.792332268370607, "no_speech_prob": 0.04335959628224373}, {"id": 386, "seek": 174032, "start": 1753.52, "end": 1759.28, "text": " plan is the continuing control problems. Essentially, all this is going to say is we've", "tokens": [51024, 1393, 307, 264, 9289, 1969, 2740, 13, 23596, 11, 439, 341, 307, 516, 281, 584, 307, 321, 600, 51312], "temperature": 0.0, "avg_logprob": -0.10159442474792053, "compression_ratio": 1.792332268370607, "no_speech_prob": 0.04335959628224373}, {"id": 387, "seek": 174032, "start": 1759.28, "end": 1765.2, "text": " essentially created up until this point, a, a model free RL agent. So essentially the point", "tokens": [51312, 4476, 2942, 493, 1826, 341, 935, 11, 257, 11, 257, 2316, 1737, 497, 43, 9461, 13, 407, 4476, 264, 935, 51608], "temperature": 0.0, "avg_logprob": -0.10159442474792053, "compression_ratio": 1.792332268370607, "no_speech_prob": 0.04335959628224373}, {"id": 388, "seek": 174032, "start": 1765.2, "end": 1769.6, "text": " for this step is really, we just want to combine everything we've learned of the average reward", "tokens": [51608, 337, 341, 1823, 307, 534, 11, 321, 445, 528, 281, 10432, 1203, 321, 600, 3264, 295, 264, 4274, 7782, 51828], "temperature": 0.0, "avg_logprob": -0.10159442474792053, "compression_ratio": 1.792332268370607, "no_speech_prob": 0.04335959628224373}, {"id": 389, "seek": 176960, "start": 1769.6, "end": 1773.76, "text": " stuff, the generating new feature stuff. And we want to try it out in a bunch of continuing", "tokens": [50364, 1507, 11, 264, 17746, 777, 4111, 1507, 13, 400, 321, 528, 281, 853, 309, 484, 294, 257, 3840, 295, 9289, 50572], "temperature": 0.0, "avg_logprob": -0.10590292278089021, "compression_ratio": 1.8155339805825244, "no_speech_prob": 0.002323071239516139}, {"id": 390, "seek": 176960, "start": 1773.76, "end": 1778.32, "text": " environments, maybe make some more like new continuing environments, because most environments", "tokens": [50572, 12388, 11, 1310, 652, 512, 544, 411, 777, 9289, 12388, 11, 570, 881, 12388, 50800], "temperature": 0.0, "avg_logprob": -0.10590292278089021, "compression_ratio": 1.8155339805825244, "no_speech_prob": 0.002323071239516139}, {"id": 391, "seek": 176960, "start": 1778.32, "end": 1782.6399999999999, "text": " are episodic, like the open AI gym stuff they mentioned, it's mostly episodic, but you could", "tokens": [50800, 366, 39200, 299, 11, 411, 264, 1269, 7318, 9222, 1507, 436, 2835, 11, 309, 311, 5240, 39200, 299, 11, 457, 291, 727, 51016], "temperature": 0.0, "avg_logprob": -0.10590292278089021, "compression_ratio": 1.8155339805825244, "no_speech_prob": 0.002323071239516139}, {"id": 392, "seek": 176960, "start": 1782.6399999999999, "end": 1787.76, "text": " convert it to continuing versions. And then we want to try the combination of everything we've", "tokens": [51016, 7620, 309, 281, 9289, 9606, 13, 400, 550, 321, 528, 281, 853, 264, 6562, 295, 1203, 321, 600, 51272], "temperature": 0.0, "avg_logprob": -0.10590292278089021, "compression_ratio": 1.8155339805825244, "no_speech_prob": 0.002323071239516139}, {"id": 393, "seek": 176960, "start": 1787.76, "end": 1793.76, "text": " worked for so far and see how we're stacking up. Because remember, this is not like a one shot plan,", "tokens": [51272, 2732, 337, 370, 1400, 293, 536, 577, 321, 434, 41376, 493, 13, 1436, 1604, 11, 341, 307, 406, 411, 257, 472, 3347, 1393, 11, 51572], "temperature": 0.0, "avg_logprob": -0.10590292278089021, "compression_ratio": 1.8155339805825244, "no_speech_prob": 0.002323071239516139}, {"id": 394, "seek": 176960, "start": 1793.76, "end": 1798.24, "text": " it will need to be adjusted as you go. So this would be a good point maybe to revise,", "tokens": [51572, 309, 486, 643, 281, 312, 19871, 382, 291, 352, 13, 407, 341, 576, 312, 257, 665, 935, 1310, 281, 44252, 11, 51796], "temperature": 0.0, "avg_logprob": -0.10590292278089021, "compression_ratio": 1.8155339805825244, "no_speech_prob": 0.002323071239516139}, {"id": 395, "seek": 179824, "start": 1798.24, "end": 1802.64, "text": " see how things are doing, a workout, anything that's maybe not working as expected. So then", "tokens": [50364, 536, 577, 721, 366, 884, 11, 257, 12169, 11, 1340, 300, 311, 1310, 406, 1364, 382, 5176, 13, 407, 550, 50584], "temperature": 0.0, "avg_logprob": -0.12180279106493817, "compression_ratio": 1.763076923076923, "no_speech_prob": 0.0046092551201581955}, {"id": 396, "seek": 179824, "start": 1802.64, "end": 1806.24, "text": " when we're not going to jump just to the seventh step, this, this is kind of wraps up the first", "tokens": [50584, 562, 321, 434, 406, 516, 281, 3012, 445, 281, 264, 17875, 1823, 11, 341, 11, 341, 307, 733, 295, 25831, 493, 264, 700, 50764], "temperature": 0.0, "avg_logprob": -0.12180279106493817, "compression_ratio": 1.763076923076923, "no_speech_prob": 0.0046092551201581955}, {"id": 397, "seek": 179824, "start": 1806.24, "end": 1810.88, "text": " part of this. Once we have this done, we have a, I love how they say it, a more continual, true,", "tokens": [50764, 644, 295, 341, 13, 3443, 321, 362, 341, 1096, 11, 321, 362, 257, 11, 286, 959, 577, 436, 584, 309, 11, 257, 544, 1421, 901, 11, 2074, 11, 50996], "temperature": 0.0, "avg_logprob": -0.12180279106493817, "compression_ratio": 1.763076923076923, "no_speech_prob": 0.0046092551201581955}, {"id": 398, "seek": 179824, "start": 1810.88, "end": 1816.56, "text": " it's a more continual model free learning method, which is good. We have like our first leg base.", "tokens": [50996, 309, 311, 257, 544, 1421, 901, 2316, 1737, 2539, 3170, 11, 597, 307, 665, 13, 492, 362, 411, 527, 700, 1676, 3096, 13, 51280], "temperature": 0.0, "avg_logprob": -0.12180279106493817, "compression_ratio": 1.763076923076923, "no_speech_prob": 0.0046092551201581955}, {"id": 399, "seek": 179824, "start": 1816.56, "end": 1820.4, "text": " This is where I guess after this point, we can say we have something good now. Now it's time to", "tokens": [51280, 639, 307, 689, 286, 2041, 934, 341, 935, 11, 321, 393, 584, 321, 362, 746, 665, 586, 13, 823, 309, 311, 565, 281, 51472], "temperature": 0.0, "avg_logprob": -0.12180279106493817, "compression_ratio": 1.763076923076923, "no_speech_prob": 0.0046092551201581955}, {"id": 400, "seek": 179824, "start": 1820.4, "end": 1826.48, "text": " start making it better. So what they focus on from here is primarily model based RL and things", "tokens": [51472, 722, 1455, 309, 1101, 13, 407, 437, 436, 1879, 322, 490, 510, 307, 10029, 2316, 2361, 497, 43, 293, 721, 51776], "temperature": 0.0, "avg_logprob": -0.12180279106493817, "compression_ratio": 1.763076923076923, "no_speech_prob": 0.0046092551201581955}, {"id": 401, "seek": 182648, "start": 1826.48, "end": 1831.3600000000001, "text": " start to get a little bit more complicated. Ideas, I will say from the seventh step onward,", "tokens": [50364, 722, 281, 483, 257, 707, 857, 544, 6179, 13, 13090, 296, 11, 286, 486, 584, 490, 264, 17875, 1823, 322, 1007, 11, 50608], "temperature": 0.0, "avg_logprob": -0.09602095610903998, "compression_ratio": 1.7672955974842768, "no_speech_prob": 0.006097227800637484}, {"id": 402, "seek": 182648, "start": 1831.3600000000001, "end": 1835.6, "text": " tend to also be a little bit higher level without as many details, which I think is because, you", "tokens": [50608, 3928, 281, 611, 312, 257, 707, 857, 2946, 1496, 1553, 382, 867, 4365, 11, 597, 286, 519, 307, 570, 11, 291, 50820], "temperature": 0.0, "avg_logprob": -0.09602095610903998, "compression_ratio": 1.7672955974842768, "no_speech_prob": 0.006097227800637484}, {"id": 403, "seek": 182648, "start": 1835.6, "end": 1838.72, "text": " know, they're later in the plan less certain about how these things will go because they're", "tokens": [50820, 458, 11, 436, 434, 1780, 294, 264, 1393, 1570, 1629, 466, 577, 613, 721, 486, 352, 570, 436, 434, 50976], "temperature": 0.0, "avg_logprob": -0.09602095610903998, "compression_ratio": 1.7672955974842768, "no_speech_prob": 0.006097227800637484}, {"id": 404, "seek": 182648, "start": 1838.72, "end": 1845.04, "text": " further out. So step seven is planning with average reward. So planning is very much done", "tokens": [50976, 3052, 484, 13, 407, 1823, 3407, 307, 5038, 365, 4274, 7782, 13, 407, 5038, 307, 588, 709, 1096, 51292], "temperature": 0.0, "avg_logprob": -0.09602095610903998, "compression_ratio": 1.7672955974842768, "no_speech_prob": 0.006097227800637484}, {"id": 405, "seek": 182648, "start": 1845.04, "end": 1850.96, "text": " in RL today. And there are lots of great examples of this. For example, you can look at Mu zero.", "tokens": [51292, 294, 497, 43, 965, 13, 400, 456, 366, 3195, 295, 869, 5110, 295, 341, 13, 1171, 1365, 11, 291, 393, 574, 412, 15601, 4018, 13, 51588], "temperature": 0.0, "avg_logprob": -0.09602095610903998, "compression_ratio": 1.7672955974842768, "no_speech_prob": 0.006097227800637484}, {"id": 406, "seek": 182648, "start": 1850.96, "end": 1854.88, "text": " Mu zero is a great example of planning where the results are pretty incredible. There are some", "tokens": [51588, 15601, 4018, 307, 257, 869, 1365, 295, 5038, 689, 264, 3542, 366, 1238, 4651, 13, 821, 366, 512, 51784], "temperature": 0.0, "avg_logprob": -0.09602095610903998, "compression_ratio": 1.7672955974842768, "no_speech_prob": 0.006097227800637484}, {"id": 407, "seek": 185488, "start": 1854.88, "end": 1858.8000000000002, "text": " differences here, right? It will again be in the continual setting will want to do this with", "tokens": [50364, 7300, 510, 11, 558, 30, 467, 486, 797, 312, 294, 264, 1421, 901, 3287, 486, 528, 281, 360, 341, 365, 50560], "temperature": 0.0, "avg_logprob": -0.1378069052825103, "compression_ratio": 1.8264984227129337, "no_speech_prob": 0.013221804052591324}, {"id": 408, "seek": 185488, "start": 1858.8000000000002, "end": 1864.3200000000002, "text": " average reward. And I can say just right now, there are a lot of problems in planning that are", "tokens": [50560, 4274, 7782, 13, 400, 286, 393, 584, 445, 558, 586, 11, 456, 366, 257, 688, 295, 2740, 294, 5038, 300, 366, 50836], "temperature": 0.0, "avg_logprob": -0.1378069052825103, "compression_ratio": 1.8264984227129337, "no_speech_prob": 0.013221804052591324}, {"id": 409, "seek": 185488, "start": 1864.3200000000002, "end": 1868.64, "text": " still unsolved. And it's very like not clear how to do things like what's what's the best way of", "tokens": [50836, 920, 2693, 29110, 13, 400, 309, 311, 588, 411, 406, 1850, 577, 281, 360, 721, 411, 437, 311, 437, 311, 264, 1151, 636, 295, 51052], "temperature": 0.0, "avg_logprob": -0.1378069052825103, "compression_ratio": 1.8264984227129337, "no_speech_prob": 0.013221804052591324}, {"id": 410, "seek": 185488, "start": 1868.64, "end": 1873.7600000000002, "text": " going about things. You also have like other alternatives to Mu zero, like dreamer dreamer is", "tokens": [51052, 516, 466, 721, 13, 509, 611, 362, 411, 661, 20478, 281, 15601, 4018, 11, 411, 3055, 260, 3055, 260, 307, 51308], "temperature": 0.0, "avg_logprob": -0.1378069052825103, "compression_ratio": 1.8264984227129337, "no_speech_prob": 0.013221804052591324}, {"id": 411, "seek": 185488, "start": 1873.7600000000002, "end": 1877.44, "text": " dreamer v2 also works very well, you know, what's what's the best way to go about planning in this", "tokens": [51308, 3055, 260, 371, 17, 611, 1985, 588, 731, 11, 291, 458, 11, 437, 311, 437, 311, 264, 1151, 636, 281, 352, 466, 5038, 294, 341, 51492], "temperature": 0.0, "avg_logprob": -0.1378069052825103, "compression_ratio": 1.8264984227129337, "no_speech_prob": 0.013221804052591324}, {"id": 412, "seek": 185488, "start": 1877.44, "end": 1883.3600000000001, "text": " setting? Who knows. So then step eight is prototype AI one. I love that. I got to love that prototype", "tokens": [51492, 3287, 30, 2102, 3255, 13, 407, 550, 1823, 3180, 307, 19475, 7318, 472, 13, 286, 959, 300, 13, 286, 658, 281, 959, 300, 19475, 51788], "temperature": 0.0, "avg_logprob": -0.1378069052825103, "compression_ratio": 1.8264984227129337, "no_speech_prob": 0.013221804052591324}, {"id": 413, "seek": 188336, "start": 1883.4399999999998, "end": 1888.0, "text": " AI one one step model based RL with continual function approximation. So this is actually", "tokens": [50368, 7318, 472, 472, 1823, 2316, 2361, 497, 43, 365, 1421, 901, 2445, 28023, 13, 407, 341, 307, 767, 50596], "temperature": 0.0, "avg_logprob": -0.08244310879538246, "compression_ratio": 1.8493589743589745, "no_speech_prob": 0.0030751924496144056}, {"id": 414, "seek": 188336, "start": 1888.0, "end": 1893.36, "text": " learning a model now, right? You can do planning if you have a pre given model, which I think is", "tokens": [50596, 2539, 257, 2316, 586, 11, 558, 30, 509, 393, 360, 5038, 498, 291, 362, 257, 659, 2212, 2316, 11, 597, 286, 519, 307, 50864], "temperature": 0.0, "avg_logprob": -0.08244310879538246, "compression_ratio": 1.8493589743589745, "no_speech_prob": 0.0030751924496144056}, {"id": 415, "seek": 188336, "start": 1893.36, "end": 1897.52, "text": " what they entailed for step seven. But now that we have model based RL, that means we're going to", "tokens": [50864, 437, 436, 948, 24731, 337, 1823, 3407, 13, 583, 586, 300, 321, 362, 2316, 2361, 497, 43, 11, 300, 1355, 321, 434, 516, 281, 51072], "temperature": 0.0, "avg_logprob": -0.08244310879538246, "compression_ratio": 1.8493589743589745, "no_speech_prob": 0.0030751924496144056}, {"id": 416, "seek": 188336, "start": 1897.52, "end": 1902.4799999999998, "text": " want to learn a model and then maybe do some sort of planning or something like that within that", "tokens": [51072, 528, 281, 1466, 257, 2316, 293, 550, 1310, 360, 512, 1333, 295, 5038, 420, 746, 411, 300, 1951, 300, 51320], "temperature": 0.0, "avg_logprob": -0.08244310879538246, "compression_ratio": 1.8493589743589745, "no_speech_prob": 0.0030751924496144056}, {"id": 417, "seek": 188336, "start": 1902.4799999999998, "end": 1906.8799999999999, "text": " model. So a one step planning model, they actually give a few steps down here that we can talk about.", "tokens": [51320, 2316, 13, 407, 257, 472, 1823, 5038, 2316, 11, 436, 767, 976, 257, 1326, 4439, 760, 510, 300, 321, 393, 751, 466, 13, 51540], "temperature": 0.0, "avg_logprob": -0.08244310879538246, "compression_ratio": 1.8493589743589745, "no_speech_prob": 0.0030751924496144056}, {"id": 418, "seek": 188336, "start": 1906.8799999999999, "end": 1911.52, "text": " So some things they want it to include. So one is a recursive update function or a perception", "tokens": [51540, 407, 512, 721, 436, 528, 309, 281, 4090, 13, 407, 472, 307, 257, 20560, 488, 5623, 2445, 420, 257, 12860, 51772], "temperature": 0.0, "avg_logprob": -0.08244310879538246, "compression_ratio": 1.8493589743589745, "no_speech_prob": 0.0030751924496144056}, {"id": 419, "seek": 191152, "start": 1911.52, "end": 1916.32, "text": " process. So right, we had that and this essentially makes everything more efficient. Instead of having", "tokens": [50364, 1399, 13, 407, 558, 11, 321, 632, 300, 293, 341, 4476, 1669, 1203, 544, 7148, 13, 7156, 295, 1419, 50604], "temperature": 0.0, "avg_logprob": -0.09879522474985274, "compression_ratio": 1.8364779874213837, "no_speech_prob": 0.0017545339651405811}, {"id": 420, "seek": 191152, "start": 1916.32, "end": 1921.76, "text": " to like relearn the perception and the value function, the world model, the policy can have one", "tokens": [50604, 281, 411, 2951, 1083, 264, 12860, 293, 264, 2158, 2445, 11, 264, 1002, 2316, 11, 264, 3897, 393, 362, 472, 50876], "temperature": 0.0, "avg_logprob": -0.09879522474985274, "compression_ratio": 1.8364779874213837, "no_speech_prob": 0.0017545339651405811}, {"id": 421, "seek": 191152, "start": 1921.76, "end": 1926.96, "text": " shared perception function that learns some sort of useful representation and knows how to store", "tokens": [50876, 5507, 12860, 2445, 300, 27152, 512, 1333, 295, 4420, 10290, 293, 3255, 577, 281, 3531, 51136], "temperature": 0.0, "avg_logprob": -0.09879522474985274, "compression_ratio": 1.8364779874213837, "no_speech_prob": 0.0017545339651405811}, {"id": 422, "seek": 191152, "start": 1926.96, "end": 1931.92, "text": " memory of things that are important and that sort of thing. We also need a one step environment", "tokens": [51136, 4675, 295, 721, 300, 366, 1021, 293, 300, 1333, 295, 551, 13, 492, 611, 643, 257, 472, 1823, 2823, 51384], "temperature": 0.0, "avg_logprob": -0.09879522474985274, "compression_ratio": 1.8364779874213837, "no_speech_prob": 0.0017545339651405811}, {"id": 423, "seek": 191152, "start": 1931.92, "end": 1936.96, "text": " model. They say this presumably be an expectation model or sample model or something in between.", "tokens": [51384, 2316, 13, 814, 584, 341, 26742, 312, 364, 14334, 2316, 420, 6889, 2316, 420, 746, 294, 1296, 13, 51636], "temperature": 0.0, "avg_logprob": -0.09879522474985274, "compression_ratio": 1.8364779874213837, "no_speech_prob": 0.0017545339651405811}, {"id": 424, "seek": 191152, "start": 1936.96, "end": 1940.96, "text": " I'm pretty sure it would not end up being an expectation model. This is I'm saying this because", "tokens": [51636, 286, 478, 1238, 988, 309, 576, 406, 917, 493, 885, 364, 14334, 2316, 13, 639, 307, 286, 478, 1566, 341, 570, 51836], "temperature": 0.0, "avg_logprob": -0.09879522474985274, "compression_ratio": 1.8364779874213837, "no_speech_prob": 0.0017545339651405811}, {"id": 425, "seek": 194096, "start": 1940.96, "end": 1945.1200000000001, "text": " this is what I do in my own work and expectation models are kind of awful. They're really easy", "tokens": [50364, 341, 307, 437, 286, 360, 294, 452, 1065, 589, 293, 14334, 5245, 366, 733, 295, 11232, 13, 814, 434, 534, 1858, 50572], "temperature": 0.0, "avg_logprob": -0.08579582887537339, "compression_ratio": 1.8224043715846994, "no_speech_prob": 0.0013669498730450869}, {"id": 426, "seek": 194096, "start": 1945.1200000000001, "end": 1949.92, "text": " to learn, but they're they're not very good. So sample models are one thing they don't mention", "tokens": [50572, 281, 1466, 11, 457, 436, 434, 436, 434, 406, 588, 665, 13, 407, 6889, 5245, 366, 472, 551, 436, 500, 380, 2152, 50812], "temperature": 0.0, "avg_logprob": -0.08579582887537339, "compression_ratio": 1.8224043715846994, "no_speech_prob": 0.0013669498730450869}, {"id": 427, "seek": 194096, "start": 1949.92, "end": 1954.16, "text": " a distribution model here. But I think that's another possibility, although there's no reason", "tokens": [50812, 257, 7316, 2316, 510, 13, 583, 286, 519, 300, 311, 1071, 7959, 11, 4878, 456, 311, 572, 1778, 51024], "temperature": 0.0, "avg_logprob": -0.08579582887537339, "compression_ratio": 1.8224043715846994, "no_speech_prob": 0.0013669498730450869}, {"id": 428, "seek": 194096, "start": 1954.16, "end": 1957.68, "text": " to think a sample model couldn't also work. So and by the way, if you're unfamiliar with this,", "tokens": [51024, 281, 519, 257, 6889, 2316, 2809, 380, 611, 589, 13, 407, 293, 538, 264, 636, 11, 498, 291, 434, 29415, 365, 341, 11, 51200], "temperature": 0.0, "avg_logprob": -0.08579582887537339, "compression_ratio": 1.8224043715846994, "no_speech_prob": 0.0013669498730450869}, {"id": 429, "seek": 194096, "start": 1957.68, "end": 1961.68, "text": " this is just the idea that we want to predict the next step, what's going to happen next, right?", "tokens": [51200, 341, 307, 445, 264, 1558, 300, 321, 528, 281, 6069, 264, 958, 1823, 11, 437, 311, 516, 281, 1051, 958, 11, 558, 30, 51400], "temperature": 0.0, "avg_logprob": -0.08579582887537339, "compression_ratio": 1.8224043715846994, "no_speech_prob": 0.0013669498730450869}, {"id": 430, "seek": 194096, "start": 1961.68, "end": 1965.6000000000001, "text": " Because if we can predict what's going to happen next, then we can update based on our, you know,", "tokens": [51400, 1436, 498, 321, 393, 6069, 437, 311, 516, 281, 1051, 958, 11, 550, 321, 393, 5623, 2361, 322, 527, 11, 291, 458, 11, 51596], "temperature": 0.0, "avg_logprob": -0.08579582887537339, "compression_ratio": 1.8224043715846994, "no_speech_prob": 0.0013669498730450869}, {"id": 431, "seek": 194096, "start": 1965.6000000000001, "end": 1969.8400000000001, "text": " sort of visions in our head, like our it's kind of like humans. I think when you go to sleep,", "tokens": [51596, 1333, 295, 30746, 294, 527, 1378, 11, 411, 527, 309, 311, 733, 295, 411, 6255, 13, 286, 519, 562, 291, 352, 281, 2817, 11, 51808], "temperature": 0.0, "avg_logprob": -0.08579582887537339, "compression_ratio": 1.8224043715846994, "no_speech_prob": 0.0013669498730450869}, {"id": 432, "seek": 196984, "start": 1969.84, "end": 1974.24, "text": " something like this happens, right? You get replayed memories and those can help you learn. So", "tokens": [50364, 746, 411, 341, 2314, 11, 558, 30, 509, 483, 23836, 292, 8495, 293, 729, 393, 854, 291, 1466, 13, 407, 50584], "temperature": 0.0, "avg_logprob": -0.08251387185423914, "compression_ratio": 1.7901234567901234, "no_speech_prob": 0.002889414085075259}, {"id": 433, "seek": 196984, "start": 1974.24, "end": 1979.6799999999998, "text": " so that's kind of what happens with world modeling and planning. What else is happening? So feature", "tokens": [50584, 370, 300, 311, 733, 295, 437, 2314, 365, 1002, 15983, 293, 5038, 13, 708, 1646, 307, 2737, 30, 407, 4111, 50856], "temperature": 0.0, "avg_logprob": -0.08251387185423914, "compression_ratio": 1.7901234567901234, "no_speech_prob": 0.002889414085075259}, {"id": 434, "seek": 196984, "start": 1979.6799999999998, "end": 1984.6399999999999, "text": " finding as in step two, then importance feedback from the model. So this is now essentially tying", "tokens": [50856, 5006, 382, 294, 1823, 732, 11, 550, 7379, 5824, 490, 264, 2316, 13, 407, 341, 307, 586, 4476, 32405, 51104], "temperature": 0.0, "avg_logprob": -0.08251387185423914, "compression_ratio": 1.7901234567901234, "no_speech_prob": 0.002889414085075259}, {"id": 435, "seek": 196984, "start": 1984.6399999999999, "end": 1989.4399999999998, "text": " steps to and this planning together, right? If we're doing plain, we should be able to use that", "tokens": [51104, 4439, 281, 293, 341, 5038, 1214, 11, 558, 30, 759, 321, 434, 884, 11121, 11, 321, 820, 312, 1075, 281, 764, 300, 51344], "temperature": 0.0, "avg_logprob": -0.08251387185423914, "compression_ratio": 1.7901234567901234, "no_speech_prob": 0.002889414085075259}, {"id": 436, "seek": 196984, "start": 1989.4399999999998, "end": 1993.4399999999998, "text": " plane to go back and change our actual features or how we're learning our features and improve them", "tokens": [51344, 5720, 281, 352, 646, 293, 1319, 527, 3539, 4122, 420, 577, 321, 434, 2539, 527, 4122, 293, 3470, 552, 51544], "temperature": 0.0, "avg_logprob": -0.08251387185423914, "compression_ratio": 1.7901234567901234, "no_speech_prob": 0.002889414085075259}, {"id": 437, "seek": 196984, "start": 1993.4399999999998, "end": 1998.1599999999999, "text": " in some way. The other idea, sorry, not other idea, but the final step here is a ranking of", "tokens": [51544, 294, 512, 636, 13, 440, 661, 1558, 11, 2597, 11, 406, 661, 1558, 11, 457, 264, 2572, 1823, 510, 307, 257, 17833, 295, 51780], "temperature": 0.0, "avg_logprob": -0.08251387185423914, "compression_ratio": 1.7901234567901234, "no_speech_prob": 0.002889414085075259}, {"id": 438, "seek": 199816, "start": 1998.16, "end": 2002.5600000000002, "text": " features used for both feature finding and to determine which features are included in the", "tokens": [50364, 4122, 1143, 337, 1293, 4111, 5006, 293, 281, 6997, 597, 4122, 366, 5556, 294, 264, 50584], "temperature": 0.0, "avg_logprob": -0.09445135285254239, "compression_ratio": 1.9257142857142857, "no_speech_prob": 0.0018675177125260234}, {"id": 439, "seek": 199816, "start": 2002.5600000000002, "end": 2008.16, "text": " environment model. Since step D is essentially ranking features for feature finding, determining", "tokens": [50584, 2823, 2316, 13, 4162, 1823, 413, 307, 4476, 17833, 4122, 337, 4111, 5006, 11, 23751, 50864], "temperature": 0.0, "avg_logprob": -0.09445135285254239, "compression_ratio": 1.9257142857142857, "no_speech_prob": 0.0018675177125260234}, {"id": 440, "seek": 199816, "start": 2008.16, "end": 2012.0, "text": " what features are used in the environment model, because we might not need to use everything. If", "tokens": [50864, 437, 4122, 366, 1143, 294, 264, 2823, 2316, 11, 570, 321, 1062, 406, 643, 281, 764, 1203, 13, 759, 51056], "temperature": 0.0, "avg_logprob": -0.09445135285254239, "compression_ratio": 1.9257142857142857, "no_speech_prob": 0.0018675177125260234}, {"id": 441, "seek": 199816, "start": 2012.0, "end": 2015.8400000000001, "text": " you've looked at like what's been going on with this recently, instead of actually trying to predict", "tokens": [51056, 291, 600, 2956, 412, 411, 437, 311, 668, 516, 322, 365, 341, 3938, 11, 2602, 295, 767, 1382, 281, 6069, 51248], "temperature": 0.0, "avg_logprob": -0.09445135285254239, "compression_ratio": 1.9257142857142857, "no_speech_prob": 0.0018675177125260234}, {"id": 442, "seek": 199816, "start": 2015.8400000000001, "end": 2019.44, "text": " the future, lots of people are doing what's it called value equivalent models, where it doesn't", "tokens": [51248, 264, 2027, 11, 3195, 295, 561, 366, 884, 437, 311, 309, 1219, 2158, 10344, 5245, 11, 689, 309, 1177, 380, 51428], "temperature": 0.0, "avg_logprob": -0.09445135285254239, "compression_ratio": 1.9257142857142857, "no_speech_prob": 0.0018675177125260234}, {"id": 443, "seek": 199816, "start": 2019.44, "end": 2022.72, "text": " actually matter what the model predicts, so long as it's getting the right value. So it might be", "tokens": [51428, 767, 1871, 437, 264, 2316, 6069, 82, 11, 370, 938, 382, 309, 311, 1242, 264, 558, 2158, 13, 407, 309, 1062, 312, 51592], "temperature": 0.0, "avg_logprob": -0.09445135285254239, "compression_ratio": 1.9257142857142857, "no_speech_prob": 0.0018675177125260234}, {"id": 444, "seek": 199816, "start": 2022.72, "end": 2027.44, "text": " predicting, you know, it might be leaving things out that are not important or stuff like that.", "tokens": [51592, 32884, 11, 291, 458, 11, 309, 1062, 312, 5012, 721, 484, 300, 366, 406, 1021, 420, 1507, 411, 300, 13, 51828], "temperature": 0.0, "avg_logprob": -0.09445135285254239, "compression_ratio": 1.9257142857142857, "no_speech_prob": 0.0018675177125260234}, {"id": 445, "seek": 202744, "start": 2027.44, "end": 2031.76, "text": " And then an influence of model learning and planning on feature ranking. So I think they", "tokens": [50364, 400, 550, 364, 6503, 295, 2316, 2539, 293, 5038, 322, 4111, 17833, 13, 407, 286, 519, 436, 50580], "temperature": 0.0, "avg_logprob": -0.10637485707989176, "compression_ratio": 1.7692307692307692, "no_speech_prob": 0.0005192898097448051}, {"id": 446, "seek": 202744, "start": 2031.76, "end": 2036.72, "text": " just kind of stated this. But anyway, I don't want to go maybe through these individual points.", "tokens": [50580, 445, 733, 295, 11323, 341, 13, 583, 4033, 11, 286, 500, 380, 528, 281, 352, 1310, 807, 613, 2609, 2793, 13, 50828], "temperature": 0.0, "avg_logprob": -0.10637485707989176, "compression_ratio": 1.7692307692307692, "no_speech_prob": 0.0005192898097448051}, {"id": 447, "seek": 202744, "start": 2036.72, "end": 2040.72, "text": " They're kind of weird. Maybe tie in all these together. What are they talking about, especially", "tokens": [50828, 814, 434, 733, 295, 3657, 13, 2704, 7582, 294, 439, 613, 1214, 13, 708, 366, 436, 1417, 466, 11, 2318, 51028], "temperature": 0.0, "avg_logprob": -0.10637485707989176, "compression_ratio": 1.7692307692307692, "no_speech_prob": 0.0005192898097448051}, {"id": 448, "seek": 202744, "start": 2040.72, "end": 2045.28, "text": " in these later few points? And the idea for this step eight is I think really that they want to", "tokens": [51028, 294, 613, 1780, 1326, 2793, 30, 400, 264, 1558, 337, 341, 1823, 3180, 307, 286, 519, 534, 300, 436, 528, 281, 51256], "temperature": 0.0, "avg_logprob": -0.10637485707989176, "compression_ratio": 1.7692307692307692, "no_speech_prob": 0.0005192898097448051}, {"id": 449, "seek": 202744, "start": 2045.28, "end": 2052.16, "text": " bring a full integration circle between the policy value function and the model components, right?", "tokens": [51256, 1565, 257, 1577, 10980, 6329, 1296, 264, 3897, 2158, 2445, 293, 264, 2316, 6677, 11, 558, 30, 51600], "temperature": 0.0, "avg_logprob": -0.10637485707989176, "compression_ratio": 1.7692307692307692, "no_speech_prob": 0.0005192898097448051}, {"id": 450, "seek": 202744, "start": 2052.16, "end": 2056.08, "text": " The idea that whenever you're doing any one of these things, when you're learning a value function,", "tokens": [51600, 440, 1558, 300, 5699, 291, 434, 884, 604, 472, 295, 613, 721, 11, 562, 291, 434, 2539, 257, 2158, 2445, 11, 51796], "temperature": 0.0, "avg_logprob": -0.10637485707989176, "compression_ratio": 1.7692307692307692, "no_speech_prob": 0.0005192898097448051}, {"id": 451, "seek": 205608, "start": 2056.08, "end": 2060.72, "text": " when you're learning how to plan, the planning should essentially help you make better features,", "tokens": [50364, 562, 291, 434, 2539, 577, 281, 1393, 11, 264, 5038, 820, 4476, 854, 291, 652, 1101, 4122, 11, 50596], "temperature": 0.0, "avg_logprob": -0.08610250654011747, "compression_ratio": 1.9055374592833876, "no_speech_prob": 0.004754773806780577}, {"id": 452, "seek": 205608, "start": 2060.72, "end": 2064.96, "text": " which should then in turn help you make a better model. So all these different components, and", "tokens": [50596, 597, 820, 550, 294, 1261, 854, 291, 652, 257, 1101, 2316, 13, 407, 439, 613, 819, 6677, 11, 293, 50808], "temperature": 0.0, "avg_logprob": -0.08610250654011747, "compression_ratio": 1.9055374592833876, "no_speech_prob": 0.004754773806780577}, {"id": 453, "seek": 205608, "start": 2064.96, "end": 2069.04, "text": " there's too many, there would be too many errors to draw this out in the like common model we saw", "tokens": [50808, 456, 311, 886, 867, 11, 456, 576, 312, 886, 867, 13603, 281, 2642, 341, 484, 294, 264, 411, 2689, 2316, 321, 1866, 51012], "temperature": 0.0, "avg_logprob": -0.08610250654011747, "compression_ratio": 1.9055374592833876, "no_speech_prob": 0.004754773806780577}, {"id": 454, "seek": 205608, "start": 2069.04, "end": 2072.64, "text": " before. But the idea is that all these different parts should be affecting each other. And I do", "tokens": [51012, 949, 13, 583, 264, 1558, 307, 300, 439, 613, 819, 3166, 820, 312, 17476, 1184, 661, 13, 400, 286, 360, 51192], "temperature": 0.0, "avg_logprob": -0.08610250654011747, "compression_ratio": 1.9055374592833876, "no_speech_prob": 0.004754773806780577}, {"id": 455, "seek": 205608, "start": 2072.64, "end": 2077.84, "text": " think that this is one interesting thing that is not like too out there at all, but it's one thing", "tokens": [51192, 519, 300, 341, 307, 472, 1880, 551, 300, 307, 406, 411, 886, 484, 456, 412, 439, 11, 457, 309, 311, 472, 551, 51452], "temperature": 0.0, "avg_logprob": -0.08610250654011747, "compression_ratio": 1.9055374592833876, "no_speech_prob": 0.004754773806780577}, {"id": 456, "seek": 205608, "start": 2077.84, "end": 2083.36, "text": " that is not fully done. Now in things like Mu zero, what they essentially do is they they have their", "tokens": [51452, 300, 307, 406, 4498, 1096, 13, 823, 294, 721, 411, 15601, 4018, 11, 437, 436, 4476, 360, 307, 436, 436, 362, 641, 51728], "temperature": 0.0, "avg_logprob": -0.08610250654011747, "compression_ratio": 1.9055374592833876, "no_speech_prob": 0.004754773806780577}, {"id": 457, "seek": 208336, "start": 2083.36, "end": 2087.76, "text": " input state, they pass this through like a representation function. So they get there,", "tokens": [50364, 4846, 1785, 11, 436, 1320, 341, 807, 411, 257, 10290, 2445, 13, 407, 436, 483, 456, 11, 50584], "temperature": 0.0, "avg_logprob": -0.11617424828665597, "compression_ratio": 1.86084142394822, "no_speech_prob": 0.010986427776515484}, {"id": 458, "seek": 208336, "start": 2087.76, "end": 2092.1600000000003, "text": " maybe I should call this the observation, this is like the state, then they do like the planning.", "tokens": [50584, 1310, 286, 820, 818, 341, 264, 14816, 11, 341, 307, 411, 264, 1785, 11, 550, 436, 360, 411, 264, 5038, 13, 50804], "temperature": 0.0, "avg_logprob": -0.11617424828665597, "compression_ratio": 1.86084142394822, "no_speech_prob": 0.010986427776515484}, {"id": 459, "seek": 208336, "start": 2092.1600000000003, "end": 2096.6400000000003, "text": " So this is called like the dynamics model to get the next state. And then from here, you go up and", "tokens": [50804, 407, 341, 307, 1219, 411, 264, 15679, 2316, 281, 483, 264, 958, 1785, 13, 400, 550, 490, 510, 11, 291, 352, 493, 293, 51028], "temperature": 0.0, "avg_logprob": -0.11617424828665597, "compression_ratio": 1.86084142394822, "no_speech_prob": 0.010986427776515484}, {"id": 460, "seek": 208336, "start": 2096.6400000000003, "end": 2101.44, "text": " you predict the policy and the value function. And then, and they do some Monte Carlo tree. So it", "tokens": [51028, 291, 6069, 264, 3897, 293, 264, 2158, 2445, 13, 400, 550, 11, 293, 436, 360, 512, 38105, 45112, 4230, 13, 407, 309, 51268], "temperature": 0.0, "avg_logprob": -0.11617424828665597, "compression_ratio": 1.86084142394822, "no_speech_prob": 0.010986427776515484}, {"id": 461, "seek": 208336, "start": 2101.44, "end": 2106.08, "text": " gets very complicated, right? But you have this whole system, and it's trained from end to end.", "tokens": [51268, 2170, 588, 6179, 11, 558, 30, 583, 291, 362, 341, 1379, 1185, 11, 293, 309, 311, 8895, 490, 917, 281, 917, 13, 51500], "temperature": 0.0, "avg_logprob": -0.11617424828665597, "compression_ratio": 1.86084142394822, "no_speech_prob": 0.010986427776515484}, {"id": 462, "seek": 208336, "start": 2106.08, "end": 2112.48, "text": " So when you actually train things go back like this, and you sort of update everything in one go.", "tokens": [51500, 407, 562, 291, 767, 3847, 721, 352, 646, 411, 341, 11, 293, 291, 1333, 295, 5623, 1203, 294, 472, 352, 13, 51820], "temperature": 0.0, "avg_logprob": -0.11617424828665597, "compression_ratio": 1.86084142394822, "no_speech_prob": 0.010986427776515484}, {"id": 463, "seek": 211248, "start": 2112.48, "end": 2116.64, "text": " So this is one type of feedback, but it's far from the only type of feedback that could be used,", "tokens": [50364, 407, 341, 307, 472, 2010, 295, 5824, 11, 457, 309, 311, 1400, 490, 264, 787, 2010, 295, 5824, 300, 727, 312, 1143, 11, 50572], "temperature": 0.0, "avg_logprob": -0.09191373286356452, "compression_ratio": 1.8208469055374592, "no_speech_prob": 0.001501128077507019}, {"id": 464, "seek": 211248, "start": 2116.64, "end": 2120.4, "text": " right? There could be other types of feedback between these different components. Maybe again,", "tokens": [50572, 558, 30, 821, 727, 312, 661, 3467, 295, 5824, 1296, 613, 819, 6677, 13, 2704, 797, 11, 50760], "temperature": 0.0, "avg_logprob": -0.09191373286356452, "compression_ratio": 1.8208469055374592, "no_speech_prob": 0.001501128077507019}, {"id": 465, "seek": 211248, "start": 2120.4, "end": 2124.56, "text": " these are all things that could potentially be meta learned to. And those other types of", "tokens": [50760, 613, 366, 439, 721, 300, 727, 7263, 312, 19616, 3264, 281, 13, 400, 729, 661, 3467, 295, 50968], "temperature": 0.0, "avg_logprob": -0.09191373286356452, "compression_ratio": 1.8208469055374592, "no_speech_prob": 0.001501128077507019}, {"id": 466, "seek": 211248, "start": 2124.56, "end": 2128.72, "text": " feedbacks might help these systems be more efficient, which would certainly always be great.", "tokens": [50968, 5824, 82, 1062, 854, 613, 3652, 312, 544, 7148, 11, 597, 576, 3297, 1009, 312, 869, 13, 51176], "temperature": 0.0, "avg_logprob": -0.09191373286356452, "compression_ratio": 1.8208469055374592, "no_speech_prob": 0.001501128077507019}, {"id": 467, "seek": 211248, "start": 2128.72, "end": 2134.64, "text": " We head down to step nine, we have search control, and exploration, just to sum this up this,", "tokens": [51176, 492, 1378, 760, 281, 1823, 4949, 11, 321, 362, 3164, 1969, 11, 293, 16197, 11, 445, 281, 2408, 341, 493, 341, 11, 51472], "temperature": 0.0, "avg_logprob": -0.09191373286356452, "compression_ratio": 1.8208469055374592, "no_speech_prob": 0.001501128077507019}, {"id": 468, "seek": 211248, "start": 2134.64, "end": 2138.8, "text": " there's not too much in this. But essentially, the idea here is that we want to make things", "tokens": [51472, 456, 311, 406, 886, 709, 294, 341, 13, 583, 4476, 11, 264, 1558, 510, 307, 300, 321, 528, 281, 652, 721, 51680], "temperature": 0.0, "avg_logprob": -0.09191373286356452, "compression_ratio": 1.8208469055374592, "no_speech_prob": 0.001501128077507019}, {"id": 469, "seek": 213880, "start": 2138.8, "end": 2143.2000000000003, "text": " more efficient and a bit better when we're doing search. So search would be done in something", "tokens": [50364, 544, 7148, 293, 257, 857, 1101, 562, 321, 434, 884, 3164, 13, 407, 3164, 576, 312, 1096, 294, 746, 50584], "temperature": 0.0, "avg_logprob": -0.10057410487422237, "compression_ratio": 1.9292035398230087, "no_speech_prob": 0.021613506600260735}, {"id": 470, "seek": 213880, "start": 2143.2000000000003, "end": 2147.36, "text": " like planning, right? We're trying to look at the future, see what could happen. We want to see,", "tokens": [50584, 411, 5038, 11, 558, 30, 492, 434, 1382, 281, 574, 412, 264, 2027, 11, 536, 437, 727, 1051, 13, 492, 528, 281, 536, 11, 50792], "temperature": 0.0, "avg_logprob": -0.10057410487422237, "compression_ratio": 1.9292035398230087, "no_speech_prob": 0.021613506600260735}, {"id": 471, "seek": 213880, "start": 2147.36, "end": 2150.5600000000004, "text": " like, you know, what, if we take this action, what are all the different things that could happen?", "tokens": [50792, 411, 11, 291, 458, 11, 437, 11, 498, 321, 747, 341, 3069, 11, 437, 366, 439, 264, 819, 721, 300, 727, 1051, 30, 50952], "temperature": 0.0, "avg_logprob": -0.10057410487422237, "compression_ratio": 1.9292035398230087, "no_speech_prob": 0.021613506600260735}, {"id": 472, "seek": 213880, "start": 2151.28, "end": 2156.5600000000004, "text": " Or also when we're updating over, like, you know, maybe we want to explore in a certain way that", "tokens": [50988, 1610, 611, 562, 321, 434, 25113, 670, 11, 411, 11, 291, 458, 11, 1310, 321, 528, 281, 6839, 294, 257, 1629, 636, 300, 51252], "temperature": 0.0, "avg_logprob": -0.10057410487422237, "compression_ratio": 1.9292035398230087, "no_speech_prob": 0.021613506600260735}, {"id": 473, "seek": 213880, "start": 2156.5600000000004, "end": 2160.8, "text": " helps us learn what we're missing, like fill out the gaps in our knowledge. So different ways you", "tokens": [51252, 3665, 505, 1466, 437, 321, 434, 5361, 11, 411, 2836, 484, 264, 15031, 294, 527, 3601, 13, 407, 819, 2098, 291, 51464], "temperature": 0.0, "avg_logprob": -0.10057410487422237, "compression_ratio": 1.9292035398230087, "no_speech_prob": 0.021613506600260735}, {"id": 474, "seek": 213880, "start": 2160.8, "end": 2164.5600000000004, "text": " could, you know, different things you could look into or like prioritize sweeping is one,", "tokens": [51464, 727, 11, 291, 458, 11, 819, 721, 291, 727, 574, 666, 420, 411, 25164, 33285, 307, 472, 11, 51652], "temperature": 0.0, "avg_logprob": -0.10057410487422237, "compression_ratio": 1.9292035398230087, "no_speech_prob": 0.021613506600260735}, {"id": 475, "seek": 213880, "start": 2164.5600000000004, "end": 2167.92, "text": " that's where you, I guess, like look at certain states where you have the least", "tokens": [51652, 300, 311, 689, 291, 11, 286, 2041, 11, 411, 574, 412, 1629, 4368, 689, 291, 362, 264, 1935, 51820], "temperature": 0.0, "avg_logprob": -0.10057410487422237, "compression_ratio": 1.9292035398230087, "no_speech_prob": 0.021613506600260735}, {"id": 476, "seek": 216792, "start": 2167.92, "end": 2171.12, "text": " knowledge of what will happen. But there's also a difference between, like, you know,", "tokens": [50364, 3601, 295, 437, 486, 1051, 13, 583, 456, 311, 611, 257, 2649, 1296, 11, 411, 11, 291, 458, 11, 50524], "temperature": 0.0, "avg_logprob": -0.09552777653009119, "compression_ratio": 1.8044871794871795, "no_speech_prob": 0.0026315725408494473}, {"id": 477, "seek": 216792, "start": 2171.12, "end": 2174.8, "text": " instead of using Monte Carlo tree search, there's also different types of heuristic search. Now,", "tokens": [50524, 2602, 295, 1228, 38105, 45112, 4230, 3164, 11, 456, 311, 611, 819, 3467, 295, 415, 374, 3142, 3164, 13, 823, 11, 50708], "temperature": 0.0, "avg_logprob": -0.09552777653009119, "compression_ratio": 1.8044871794871795, "no_speech_prob": 0.0026315725408494473}, {"id": 478, "seek": 216792, "start": 2174.8, "end": 2179.36, "text": " I don't think they would just use heuristic search, because that seems like it uses a bit too much", "tokens": [50708, 286, 500, 380, 519, 436, 576, 445, 764, 415, 374, 3142, 3164, 11, 570, 300, 2544, 411, 309, 4960, 257, 857, 886, 709, 50936], "temperature": 0.0, "avg_logprob": -0.09552777653009119, "compression_ratio": 1.8044871794871795, "no_speech_prob": 0.0026315725408494473}, {"id": 479, "seek": 216792, "start": 2179.36, "end": 2183.84, "text": " human experience. But perhaps the model could, could meta learn, I say meta learn too much,", "tokens": [50936, 1952, 1752, 13, 583, 4317, 264, 2316, 727, 11, 727, 19616, 1466, 11, 286, 584, 19616, 1466, 886, 709, 11, 51160], "temperature": 0.0, "avg_logprob": -0.09552777653009119, "compression_ratio": 1.8044871794871795, "no_speech_prob": 0.0026315725408494473}, {"id": 480, "seek": 216792, "start": 2183.84, "end": 2187.84, "text": " it could learn some heuristics for this, right? And that would be another way to go about planning.", "tokens": [51160, 309, 727, 1466, 512, 415, 374, 6006, 337, 341, 11, 558, 30, 400, 300, 576, 312, 1071, 636, 281, 352, 466, 5038, 13, 51360], "temperature": 0.0, "avg_logprob": -0.09552777653009119, "compression_ratio": 1.8044871794871795, "no_speech_prob": 0.0026315725408494473}, {"id": 481, "seek": 216792, "start": 2187.84, "end": 2193.28, "text": " So that's what that is all about. And next is the stump progression. So stump, I believe,", "tokens": [51360, 407, 300, 311, 437, 300, 307, 439, 466, 13, 400, 958, 307, 264, 43164, 18733, 13, 407, 43164, 11, 286, 1697, 11, 51632], "temperature": 0.0, "avg_logprob": -0.09552777653009119, "compression_ratio": 1.8044871794871795, "no_speech_prob": 0.0026315725408494473}, {"id": 482, "seek": 219328, "start": 2193.36, "end": 2199.44, "text": " do they write it here? Yes, they do. So it stands for subtask, option, model, and planning. We've", "tokens": [50368, 360, 436, 2464, 309, 510, 30, 1079, 11, 436, 360, 13, 407, 309, 7382, 337, 7257, 3863, 11, 3614, 11, 2316, 11, 293, 5038, 13, 492, 600, 50672], "temperature": 0.0, "avg_logprob": -0.08488090833028157, "compression_ratio": 1.8018292682926829, "no_speech_prob": 0.08508886396884918}, {"id": 483, "seek": 219328, "start": 2199.44, "end": 2202.6400000000003, "text": " already talked about most of these, you can probably infer what a subtask is, right? It's the", "tokens": [50672, 1217, 2825, 466, 881, 295, 613, 11, 291, 393, 1391, 13596, 437, 257, 7257, 3863, 307, 11, 558, 30, 467, 311, 264, 50832], "temperature": 0.0, "avg_logprob": -0.08488090833028157, "compression_ratio": 1.8018292682926829, "no_speech_prob": 0.08508886396884918}, {"id": 484, "seek": 219328, "start": 2202.6400000000003, "end": 2207.6000000000004, "text": " idea that you might want to have other tasks, other than the main task, the agent can, can work on.", "tokens": [50832, 1558, 300, 291, 1062, 528, 281, 362, 661, 9608, 11, 661, 813, 264, 2135, 5633, 11, 264, 9461, 393, 11, 393, 589, 322, 13, 51080], "temperature": 0.0, "avg_logprob": -0.08488090833028157, "compression_ratio": 1.8018292682926829, "no_speech_prob": 0.08508886396884918}, {"id": 485, "seek": 219328, "start": 2207.6000000000004, "end": 2212.2400000000002, "text": " And of course, the agent would learn these tasks themselves, as opposed to like a human, a human", "tokens": [51080, 400, 295, 1164, 11, 264, 9461, 576, 1466, 613, 9608, 2969, 11, 382, 8851, 281, 411, 257, 1952, 11, 257, 1952, 51312], "temperature": 0.0, "avg_logprob": -0.08488090833028157, "compression_ratio": 1.8018292682926829, "no_speech_prob": 0.08508886396884918}, {"id": 486, "seek": 219328, "start": 2212.2400000000002, "end": 2216.7200000000003, "text": " doing these. So this is like one, one differentiation, one point they mentioned earlier, right? We don't", "tokens": [51312, 884, 613, 13, 407, 341, 307, 411, 472, 11, 472, 38902, 11, 472, 935, 436, 2835, 3071, 11, 558, 30, 492, 500, 380, 51536], "temperature": 0.0, "avg_logprob": -0.08488090833028157, "compression_ratio": 1.8018292682926829, "no_speech_prob": 0.08508886396884918}, {"id": 487, "seek": 219328, "start": 2216.7200000000003, "end": 2222.0, "text": " want, and this is what a lot of people do now is if they want to learn like skills. So a skill is", "tokens": [51536, 528, 11, 293, 341, 307, 437, 257, 688, 295, 561, 360, 586, 307, 498, 436, 528, 281, 1466, 411, 3942, 13, 407, 257, 5389, 307, 51800], "temperature": 0.0, "avg_logprob": -0.08488090833028157, "compression_ratio": 1.8018292682926829, "no_speech_prob": 0.08508886396884918}, {"id": 488, "seek": 222200, "start": 2222.56, "end": 2226.48, "text": " maybe, I don't know, you're going to drive a car and one skill is like turning the handle left", "tokens": [50392, 1310, 11, 286, 500, 380, 458, 11, 291, 434, 516, 281, 3332, 257, 1032, 293, 472, 5389, 307, 411, 6246, 264, 4813, 1411, 50588], "temperature": 0.0, "avg_logprob": -0.0871345660107284, "compression_ratio": 1.8838526912181304, "no_speech_prob": 0.0031725442968308926}, {"id": 489, "seek": 222200, "start": 2226.48, "end": 2229.68, "text": " or right. You have to move a lot of muscles in your arms. So if you have the steering wheel,", "tokens": [50588, 420, 558, 13, 509, 362, 281, 1286, 257, 688, 295, 9530, 294, 428, 5812, 13, 407, 498, 291, 362, 264, 14823, 5589, 11, 50748], "temperature": 0.0, "avg_logprob": -0.0871345660107284, "compression_ratio": 1.8838526912181304, "no_speech_prob": 0.0031725442968308926}, {"id": 490, "seek": 222200, "start": 2229.68, "end": 2234.24, "text": " how does, how do these normally look? I don't think this is close enough. If you want to move this", "tokens": [50748, 577, 775, 11, 577, 360, 613, 5646, 574, 30, 286, 500, 380, 519, 341, 307, 1998, 1547, 13, 759, 291, 528, 281, 1286, 341, 50976], "temperature": 0.0, "avg_logprob": -0.0871345660107284, "compression_ratio": 1.8838526912181304, "no_speech_prob": 0.0031725442968308926}, {"id": 491, "seek": 222200, "start": 2234.24, "end": 2238.0, "text": " left and right, you actually have to move a lot of muscles in your arm to get that to work, right?", "tokens": [50976, 1411, 293, 558, 11, 291, 767, 362, 281, 1286, 257, 688, 295, 9530, 294, 428, 3726, 281, 483, 300, 281, 589, 11, 558, 30, 51164], "temperature": 0.0, "avg_logprob": -0.0871345660107284, "compression_ratio": 1.8838526912181304, "no_speech_prob": 0.0031725442968308926}, {"id": 492, "seek": 222200, "start": 2238.0, "end": 2242.0, "text": " But for humans, it's very easy, because we've, we essentially know how to move our arms in", "tokens": [51164, 583, 337, 6255, 11, 309, 311, 588, 1858, 11, 570, 321, 600, 11, 321, 4476, 458, 577, 281, 1286, 527, 5812, 294, 51364], "temperature": 0.0, "avg_logprob": -0.0871345660107284, "compression_ratio": 1.8838526912181304, "no_speech_prob": 0.0031725442968308926}, {"id": 493, "seek": 222200, "start": 2242.0, "end": 2247.76, "text": " certain ways. We have skills or options are the more general form of this. That's when option is", "tokens": [51364, 1629, 2098, 13, 492, 362, 3942, 420, 3956, 366, 264, 544, 2674, 1254, 295, 341, 13, 663, 311, 562, 3614, 307, 51652], "temperature": 0.0, "avg_logprob": -0.0871345660107284, "compression_ratio": 1.8838526912181304, "no_speech_prob": 0.0031725442968308926}, {"id": 494, "seek": 222200, "start": 2247.76, "end": 2251.76, "text": " in a subtask. If I'm learning how to live a good life, learning to drive is going to be one", "tokens": [51652, 294, 257, 7257, 3863, 13, 759, 286, 478, 2539, 577, 281, 1621, 257, 665, 993, 11, 2539, 281, 3332, 307, 516, 281, 312, 472, 51852], "temperature": 0.0, "avg_logprob": -0.0871345660107284, "compression_ratio": 1.8838526912181304, "no_speech_prob": 0.0031725442968308926}, {"id": 495, "seek": 225176, "start": 2251.76, "end": 2257.1200000000003, "text": " subtask. So that's important for me to master, right? So how can the agent pose its own subtasks", "tokens": [50364, 7257, 3863, 13, 407, 300, 311, 1021, 337, 385, 281, 4505, 11, 558, 30, 407, 577, 393, 264, 9461, 10774, 1080, 1065, 7257, 296, 1694, 50632], "temperature": 0.0, "avg_logprob": -0.09998541710360738, "compression_ratio": 1.8599348534201954, "no_speech_prob": 0.0029808238614350557}, {"id": 496, "seek": 225176, "start": 2257.1200000000003, "end": 2261.5200000000004, "text": " and how can it learn options that solve those subtasks? And then how can we combine those?", "tokens": [50632, 293, 577, 393, 309, 1466, 3956, 300, 5039, 729, 7257, 296, 1694, 30, 400, 550, 577, 393, 321, 10432, 729, 30, 50852], "temperature": 0.0, "avg_logprob": -0.09998541710360738, "compression_ratio": 1.8599348534201954, "no_speech_prob": 0.0029808238614350557}, {"id": 497, "seek": 225176, "start": 2261.5200000000004, "end": 2264.88, "text": " Or sorry, I'm getting a bit ahead of myself. Combining them is the next step. What they", "tokens": [50852, 1610, 2597, 11, 286, 478, 1242, 257, 857, 2286, 295, 2059, 13, 25939, 1760, 552, 307, 264, 958, 1823, 13, 708, 436, 51020], "temperature": 0.0, "avg_logprob": -0.09998541710360738, "compression_ratio": 1.8599348534201954, "no_speech_prob": 0.0029808238614350557}, {"id": 498, "seek": 225176, "start": 2264.88, "end": 2268.8, "text": " essentially propose, I'm not sure, I forget if they propose it here, but the idea is that they", "tokens": [51020, 4476, 17421, 11, 286, 478, 406, 988, 11, 286, 2870, 498, 436, 17421, 309, 510, 11, 457, 264, 1558, 307, 300, 436, 51216], "temperature": 0.0, "avg_logprob": -0.09998541710360738, "compression_ratio": 1.8599348534201954, "no_speech_prob": 0.0029808238614350557}, {"id": 499, "seek": 225176, "start": 2268.8, "end": 2273.5200000000004, "text": " want to have GVFs. So they want to be predicting things or have certain features, right? So feature", "tokens": [51216, 528, 281, 362, 460, 53, 37, 82, 13, 407, 436, 528, 281, 312, 32884, 721, 420, 362, 1629, 4122, 11, 558, 30, 407, 4111, 51452], "temperature": 0.0, "avg_logprob": -0.09998541710360738, "compression_ratio": 1.8599348534201954, "no_speech_prob": 0.0029808238614350557}, {"id": 500, "seek": 225176, "start": 2273.5200000000004, "end": 2278.0800000000004, "text": " one, feature two, feature three, that tells things about the environment. And then they want to pose", "tokens": [51452, 472, 11, 4111, 732, 11, 4111, 1045, 11, 300, 5112, 721, 466, 264, 2823, 13, 400, 550, 436, 528, 281, 10774, 51680], "temperature": 0.0, "avg_logprob": -0.09998541710360738, "compression_ratio": 1.8599348534201954, "no_speech_prob": 0.0029808238614350557}, {"id": 501, "seek": 227808, "start": 2278.08, "end": 2283.2799999999997, "text": " GVFs. So being able to predict these things and also control them. So say, how can we maximize", "tokens": [50364, 460, 53, 37, 82, 13, 407, 885, 1075, 281, 6069, 613, 721, 293, 611, 1969, 552, 13, 407, 584, 11, 577, 393, 321, 19874, 50624], "temperature": 0.0, "avg_logprob": -0.07497554202731564, "compression_ratio": 1.8121019108280254, "no_speech_prob": 0.006289580371230841}, {"id": 502, "seek": 227808, "start": 2283.2799999999997, "end": 2287.2799999999997, "text": " feature one? How can we maximize feature two or feature three, and then learn options for each", "tokens": [50624, 4111, 472, 30, 1012, 393, 321, 19874, 4111, 732, 420, 4111, 1045, 11, 293, 550, 1466, 3956, 337, 1184, 50824], "temperature": 0.0, "avg_logprob": -0.07497554202731564, "compression_ratio": 1.8121019108280254, "no_speech_prob": 0.006289580371230841}, {"id": 503, "seek": 227808, "start": 2287.2799999999997, "end": 2291.92, "text": " of those. So essentially what you're doing is you're learning how to control your environment", "tokens": [50824, 295, 729, 13, 407, 4476, 437, 291, 434, 884, 307, 291, 434, 2539, 577, 281, 1969, 428, 2823, 51056], "temperature": 0.0, "avg_logprob": -0.07497554202731564, "compression_ratio": 1.8121019108280254, "no_speech_prob": 0.006289580371230841}, {"id": 504, "seek": 227808, "start": 2291.92, "end": 2296.0, "text": " in ways that are not just trying to maximize reward. And then maybe these options that you've", "tokens": [51056, 294, 2098, 300, 366, 406, 445, 1382, 281, 19874, 7782, 13, 400, 550, 1310, 613, 3956, 300, 291, 600, 51260], "temperature": 0.0, "avg_logprob": -0.07497554202731564, "compression_ratio": 1.8121019108280254, "no_speech_prob": 0.006289580371230841}, {"id": 505, "seek": 227808, "start": 2296.0, "end": 2300.64, "text": " learned could be reused later. They actually have a paper on this called, I think, Stomp or", "tokens": [51260, 3264, 727, 312, 319, 4717, 1780, 13, 814, 767, 362, 257, 3035, 322, 341, 1219, 11, 286, 519, 11, 745, 8586, 420, 51492], "temperature": 0.0, "avg_logprob": -0.07497554202731564, "compression_ratio": 1.8121019108280254, "no_speech_prob": 0.006289580371230841}, {"id": 506, "seek": 227808, "start": 2300.64, "end": 2305.52, "text": " something like that. So if you're interested in that, I do encourage you to check that out. So then", "tokens": [51492, 746, 411, 300, 13, 407, 498, 291, 434, 3102, 294, 300, 11, 286, 360, 5373, 291, 281, 1520, 300, 484, 13, 407, 550, 51736], "temperature": 0.0, "avg_logprob": -0.07497554202731564, "compression_ratio": 1.8121019108280254, "no_speech_prob": 0.006289580371230841}, {"id": 507, "seek": 230552, "start": 2305.52, "end": 2312.08, "text": " step 11 is oak. So oak is, it stands for another thing. I get, I'm forgetting, I think options,", "tokens": [50364, 1823, 2975, 307, 31322, 13, 407, 31322, 307, 11, 309, 7382, 337, 1071, 551, 13, 286, 483, 11, 286, 478, 25428, 11, 286, 519, 3956, 11, 50692], "temperature": 0.0, "avg_logprob": -0.1087535748378836, "compression_ratio": 1.780564263322884, "no_speech_prob": 0.0073455143719911575}, {"id": 508, "seek": 230552, "start": 2312.08, "end": 2316.08, "text": " action knowledge or something like that. That might not be exactly right here. They actually", "tokens": [50692, 3069, 3601, 420, 746, 411, 300, 13, 663, 1062, 406, 312, 2293, 558, 510, 13, 814, 767, 50892], "temperature": 0.0, "avg_logprob": -0.1087535748378836, "compression_ratio": 1.780564263322884, "no_speech_prob": 0.0073455143719911575}, {"id": 509, "seek": 230552, "start": 2316.08, "end": 2320.72, "text": " introduce option keyboard. Now I'm surprised they mentioned this because the other steps don't", "tokens": [50892, 5366, 3614, 10186, 13, 823, 286, 478, 6100, 436, 2835, 341, 570, 264, 661, 4439, 500, 380, 51124], "temperature": 0.0, "avg_logprob": -0.1087535748378836, "compression_ratio": 1.780564263322884, "no_speech_prob": 0.0073455143719911575}, {"id": 510, "seek": 230552, "start": 2320.72, "end": 2325.36, "text": " really go in depth. Whereas the options keyboard is a very specific thing. There's a paper out", "tokens": [51124, 534, 352, 294, 7161, 13, 13813, 264, 3956, 10186, 307, 257, 588, 2685, 551, 13, 821, 311, 257, 3035, 484, 51356], "temperature": 0.0, "avg_logprob": -0.1087535748378836, "compression_ratio": 1.780564263322884, "no_speech_prob": 0.0073455143719911575}, {"id": 511, "seek": 230552, "start": 2325.36, "end": 2329.28, "text": " about it. They link to it. I think it's a really interesting paper. And the idea is that let's", "tokens": [51356, 466, 309, 13, 814, 2113, 281, 309, 13, 286, 519, 309, 311, 257, 534, 1880, 3035, 13, 400, 264, 1558, 307, 300, 718, 311, 51552], "temperature": 0.0, "avg_logprob": -0.1087535748378836, "compression_ratio": 1.780564263322884, "no_speech_prob": 0.0073455143719911575}, {"id": 512, "seek": 230552, "start": 2329.28, "end": 2334.64, "text": " say you have a set of options. So maybe option number one knows how to fish. And option number", "tokens": [51552, 584, 291, 362, 257, 992, 295, 3956, 13, 407, 1310, 3614, 1230, 472, 3255, 577, 281, 3506, 13, 400, 3614, 1230, 51820], "temperature": 0.0, "avg_logprob": -0.1087535748378836, "compression_ratio": 1.780564263322884, "no_speech_prob": 0.0073455143719911575}, {"id": 513, "seek": 233464, "start": 2334.64, "end": 2339.8399999999997, "text": " two knows how to use a computer. Now, I don't know why you would ever want to do these two things", "tokens": [50364, 732, 3255, 577, 281, 764, 257, 3820, 13, 823, 11, 286, 500, 380, 458, 983, 291, 576, 1562, 528, 281, 360, 613, 732, 721, 50624], "temperature": 0.0, "avg_logprob": -0.09035950937578754, "compression_ratio": 1.7865853658536586, "no_speech_prob": 0.001700668130069971}, {"id": 514, "seek": 233464, "start": 2339.8399999999997, "end": 2344.96, "text": " at, at the same time, but I know I hate fishing and I find it incredibly boring. So if I was going", "tokens": [50624, 412, 11, 412, 264, 912, 565, 11, 457, 286, 458, 286, 4700, 10180, 293, 286, 915, 309, 6252, 9989, 13, 407, 498, 286, 390, 516, 50880], "temperature": 0.0, "avg_logprob": -0.09035950937578754, "compression_ratio": 1.7865853658536586, "no_speech_prob": 0.001700668130069971}, {"id": 515, "seek": 233464, "start": 2344.96, "end": 2349.3599999999997, "text": " to fish and I had a computer, well, honestly, I'd probably enjoy the nature, but you know, you could", "tokens": [50880, 281, 3506, 293, 286, 632, 257, 3820, 11, 731, 11, 6095, 11, 286, 1116, 1391, 2103, 264, 3687, 11, 457, 291, 458, 11, 291, 727, 51100], "temperature": 0.0, "avg_logprob": -0.09035950937578754, "compression_ratio": 1.7865853658536586, "no_speech_prob": 0.001700668130069971}, {"id": 516, "seek": 233464, "start": 2349.3599999999997, "end": 2354.24, "text": " do both at one time, right? There's no reason to say you can only do one fishing or using your", "tokens": [51100, 360, 1293, 412, 472, 565, 11, 558, 30, 821, 311, 572, 1778, 281, 584, 291, 393, 787, 360, 472, 10180, 420, 1228, 428, 51344], "temperature": 0.0, "avg_logprob": -0.09035950937578754, "compression_ratio": 1.7865853658536586, "no_speech_prob": 0.001700668130069971}, {"id": 517, "seek": 233464, "start": 2354.24, "end": 2359.04, "text": " computer. Choose you could do both at the same time. If you want to weather or not, it's a good", "tokens": [51344, 3820, 13, 21661, 291, 727, 360, 1293, 412, 264, 912, 565, 13, 759, 291, 528, 281, 5503, 420, 406, 11, 309, 311, 257, 665, 51584], "temperature": 0.0, "avg_logprob": -0.09035950937578754, "compression_ratio": 1.7865853658536586, "no_speech_prob": 0.001700668130069971}, {"id": 518, "seek": 233464, "start": 2359.04, "end": 2363.52, "text": " idea. So and that's the idea of an option keyboard where you can essentially specify how much you", "tokens": [51584, 1558, 13, 407, 293, 300, 311, 264, 1558, 295, 364, 3614, 10186, 689, 291, 393, 4476, 16500, 577, 709, 291, 51808], "temperature": 0.0, "avg_logprob": -0.09035950937578754, "compression_ratio": 1.7865853658536586, "no_speech_prob": 0.001700668130069971}, {"id": 519, "seek": 236352, "start": 2363.6, "end": 2367.7599999999998, "text": " want to do this option or that option. And instead of having to learn a billion different", "tokens": [50368, 528, 281, 360, 341, 3614, 420, 300, 3614, 13, 400, 2602, 295, 1419, 281, 1466, 257, 5218, 819, 50576], "temperature": 0.0, "avg_logprob": -0.08607476949691772, "compression_ratio": 1.9450171821305842, "no_speech_prob": 0.004754898138344288}, {"id": 520, "seek": 236352, "start": 2367.7599999999998, "end": 2372.32, "text": " options to do all the different things you do, well, if you learn a good set of base options,", "tokens": [50576, 3956, 281, 360, 439, 264, 819, 721, 291, 360, 11, 731, 11, 498, 291, 1466, 257, 665, 992, 295, 3096, 3956, 11, 50804], "temperature": 0.0, "avg_logprob": -0.08607476949691772, "compression_ratio": 1.9450171821305842, "no_speech_prob": 0.004754898138344288}, {"id": 521, "seek": 236352, "start": 2372.32, "end": 2377.04, "text": " now you can combine them and get massively more expressive options that are just combinations", "tokens": [50804, 586, 291, 393, 10432, 552, 293, 483, 29379, 544, 40189, 3956, 300, 366, 445, 21267, 51040], "temperature": 0.0, "avg_logprob": -0.08607476949691772, "compression_ratio": 1.9450171821305842, "no_speech_prob": 0.004754898138344288}, {"id": 522, "seek": 236352, "start": 2377.04, "end": 2381.28, "text": " of others instead of having to learn those explicitly. And then we get to the last step,", "tokens": [51040, 295, 2357, 2602, 295, 1419, 281, 1466, 729, 20803, 13, 400, 550, 321, 483, 281, 264, 1036, 1823, 11, 51252], "temperature": 0.0, "avg_logprob": -0.08607476949691772, "compression_ratio": 1.9450171821305842, "no_speech_prob": 0.004754898138344288}, {"id": 523, "seek": 236352, "start": 2381.28, "end": 2386.64, "text": " intelligence amplification on how far we've come. So intelligence amplification is I think what most", "tokens": [51252, 7599, 9731, 3774, 322, 577, 1400, 321, 600, 808, 13, 407, 7599, 9731, 3774, 307, 286, 519, 437, 881, 51520], "temperature": 0.0, "avg_logprob": -0.08607476949691772, "compression_ratio": 1.9450171821305842, "no_speech_prob": 0.004754898138344288}, {"id": 524, "seek": 236352, "start": 2386.64, "end": 2391.52, "text": " people think of when they think of the singularity. It's essentially the idea that now we have our", "tokens": [51520, 561, 519, 295, 562, 436, 519, 295, 264, 20010, 507, 13, 467, 311, 4476, 264, 1558, 300, 586, 321, 362, 527, 51764], "temperature": 0.0, "avg_logprob": -0.08607476949691772, "compression_ratio": 1.9450171821305842, "no_speech_prob": 0.004754898138344288}, {"id": 525, "seek": 239152, "start": 2391.6, "end": 2398.24, "text": " prototype AI number two, and it should be able to do things very well. They describe this in", "tokens": [50368, 19475, 7318, 1230, 732, 11, 293, 309, 820, 312, 1075, 281, 360, 721, 588, 731, 13, 814, 6786, 341, 294, 50700], "temperature": 0.0, "avg_logprob": -0.11387861899609836, "compression_ratio": 1.684981684981685, "no_speech_prob": 0.00232311082072556}, {"id": 526, "seek": 239152, "start": 2398.24, "end": 2402.72, "text": " some interesting ways. So like there's an exo cerebellum, which is, you know, they talk about", "tokens": [50700, 512, 1880, 2098, 13, 407, 411, 456, 311, 364, 454, 78, 11643, 7100, 449, 11, 597, 307, 11, 291, 458, 11, 436, 751, 466, 50924], "temperature": 0.0, "avg_logprob": -0.11387861899609836, "compression_ratio": 1.684981684981685, "no_speech_prob": 0.00232311082072556}, {"id": 527, "seek": 239152, "start": 2402.72, "end": 2405.7599999999998, "text": " these things, but essentially the core of what they're getting at is really at the end here,", "tokens": [50924, 613, 721, 11, 457, 4476, 264, 4965, 295, 437, 436, 434, 1242, 412, 307, 534, 412, 264, 917, 510, 11, 51076], "temperature": 0.0, "avg_logprob": -0.11387861899609836, "compression_ratio": 1.684981684981685, "no_speech_prob": 0.00232311082072556}, {"id": 528, "seek": 239152, "start": 2405.7599999999998, "end": 2410.16, "text": " where it says an intelligent application agent to perform policies and use planning to", "tokens": [51076, 689, 309, 1619, 364, 13232, 3861, 9461, 281, 2042, 7657, 293, 764, 5038, 281, 51296], "temperature": 0.0, "avg_logprob": -0.11387861899609836, "compression_ratio": 1.684981684981685, "no_speech_prob": 0.00232311082072556}, {"id": 529, "seek": 239152, "start": 2411.84, "end": 2417.36, "text": " multiplicatively enhance the intelligence of another partner agent or part of a single agent,", "tokens": [51380, 17596, 19020, 11985, 264, 7599, 295, 1071, 4975, 9461, 420, 644, 295, 257, 2167, 9461, 11, 51656], "temperature": 0.0, "avg_logprob": -0.11387861899609836, "compression_ratio": 1.684981684981685, "no_speech_prob": 0.00232311082072556}, {"id": 530, "seek": 241736, "start": 2417.44, "end": 2421.6800000000003, "text": " or I guess they could also change themselves, right? So we see these two versions being studied", "tokens": [50368, 420, 286, 2041, 436, 727, 611, 1319, 2969, 11, 558, 30, 407, 321, 536, 613, 732, 9606, 885, 9454, 50580], "temperature": 0.0, "avg_logprob": -0.1345433129204644, "compression_ratio": 1.8257575757575757, "no_speech_prob": 0.13657258450984955}, {"id": 531, "seek": 241736, "start": 2421.6800000000003, "end": 2427.04, "text": " in both human agents and agent to agent interaction settings. So essentially the idea that you have", "tokens": [50580, 294, 1293, 1952, 12554, 293, 9461, 281, 9461, 9285, 6257, 13, 407, 4476, 264, 1558, 300, 291, 362, 50848], "temperature": 0.0, "avg_logprob": -0.1345433129204644, "compression_ratio": 1.8257575757575757, "no_speech_prob": 0.13657258450984955}, {"id": 532, "seek": 241736, "start": 2427.04, "end": 2433.44, "text": " this one agent right here, and then you have agent, agent two right here. And maybe agent two is", "tokens": [50848, 341, 472, 9461, 558, 510, 11, 293, 550, 291, 362, 9461, 11, 9461, 732, 558, 510, 13, 400, 1310, 9461, 732, 307, 51168], "temperature": 0.0, "avg_logprob": -0.1345433129204644, "compression_ratio": 1.8257575757575757, "no_speech_prob": 0.13657258450984955}, {"id": 533, "seek": 241736, "start": 2433.44, "end": 2439.36, "text": " as much bigger brain, much smarter, you know, great brain drawing for me. So what agent two can do", "tokens": [51168, 382, 709, 3801, 3567, 11, 709, 20294, 11, 291, 458, 11, 869, 3567, 6316, 337, 385, 13, 407, 437, 9461, 732, 393, 360, 51464], "temperature": 0.0, "avg_logprob": -0.1345433129204644, "compression_ratio": 1.8257575757575757, "no_speech_prob": 0.13657258450984955}, {"id": 534, "seek": 241736, "start": 2439.36, "end": 2443.44, "text": " is it can go to agent one and say, I'm gonna make you smarter. And then it makes agent one", "tokens": [51464, 307, 309, 393, 352, 281, 9461, 472, 293, 584, 11, 286, 478, 799, 652, 291, 20294, 13, 400, 550, 309, 1669, 9461, 472, 51668], "temperature": 0.0, "avg_logprob": -0.1345433129204644, "compression_ratio": 1.8257575757575757, "no_speech_prob": 0.13657258450984955}, {"id": 535, "seek": 244344, "start": 2443.44, "end": 2447.84, "text": " better because remember it's a machine, it can edit its code. And then agent one is like, oh,", "tokens": [50364, 1101, 570, 1604, 309, 311, 257, 3479, 11, 309, 393, 8129, 1080, 3089, 13, 400, 550, 9461, 472, 307, 411, 11, 1954, 11, 50584], "temperature": 0.0, "avg_logprob": -0.10012708107630412, "compression_ratio": 1.7570093457943925, "no_speech_prob": 0.060080863535404205}, {"id": 536, "seek": 244344, "start": 2447.84, "end": 2452.32, "text": " thanks for making me smarter. Now I'm going to go make you smarter. Or of course, maybe you could", "tokens": [50584, 3231, 337, 1455, 385, 20294, 13, 823, 286, 478, 516, 281, 352, 652, 291, 20294, 13, 1610, 295, 1164, 11, 1310, 291, 727, 50808], "temperature": 0.0, "avg_logprob": -0.10012708107630412, "compression_ratio": 1.7570093457943925, "no_speech_prob": 0.060080863535404205}, {"id": 537, "seek": 244344, "start": 2452.32, "end": 2456.7200000000003, "text": " just have a single agent, reperforming this on itself, or maybe there's some risk there because", "tokens": [50808, 445, 362, 257, 2167, 9461, 11, 319, 26765, 278, 341, 322, 2564, 11, 420, 1310, 456, 311, 512, 3148, 456, 570, 51028], "temperature": 0.0, "avg_logprob": -0.10012708107630412, "compression_ratio": 1.7570093457943925, "no_speech_prob": 0.060080863535404205}, {"id": 538, "seek": 244344, "start": 2456.7200000000003, "end": 2461.04, "text": " it could mess itself up. Maybe that's why they talk about having two different agents. But anyway,", "tokens": [51028, 309, 727, 2082, 2564, 493, 13, 2704, 300, 311, 983, 436, 751, 466, 1419, 732, 819, 12554, 13, 583, 4033, 11, 51244], "temperature": 0.0, "avg_logprob": -0.10012708107630412, "compression_ratio": 1.7570093457943925, "no_speech_prob": 0.060080863535404205}, {"id": 539, "seek": 244344, "start": 2461.04, "end": 2465.28, "text": " I mean, this is the idea, right, of how you get better and better agents. At some point,", "tokens": [51244, 286, 914, 11, 341, 307, 264, 1558, 11, 558, 11, 295, 577, 291, 483, 1101, 293, 1101, 12554, 13, 1711, 512, 935, 11, 51456], "temperature": 0.0, "avg_logprob": -0.10012708107630412, "compression_ratio": 1.7570093457943925, "no_speech_prob": 0.060080863535404205}, {"id": 540, "seek": 244344, "start": 2465.28, "end": 2469.28, "text": " you have an agent that is just better than humans at producing these sorts of, you know,", "tokens": [51456, 291, 362, 364, 9461, 300, 307, 445, 1101, 813, 6255, 412, 10501, 613, 7527, 295, 11, 291, 458, 11, 51656], "temperature": 0.0, "avg_logprob": -0.10012708107630412, "compression_ratio": 1.7570093457943925, "no_speech_prob": 0.060080863535404205}, {"id": 541, "seek": 246928, "start": 2469.28, "end": 2475.6000000000004, "text": " AI agents. And that's when we can get this sort of multiplicative scaling. Now, I think if you're", "tokens": [50364, 7318, 12554, 13, 400, 300, 311, 562, 321, 393, 483, 341, 1333, 295, 17596, 1166, 21589, 13, 823, 11, 286, 519, 498, 291, 434, 50680], "temperature": 0.0, "avg_logprob": -0.13483373834452497, "compression_ratio": 1.6115702479338843, "no_speech_prob": 0.029308466240763664}, {"id": 542, "seek": 246928, "start": 2475.6000000000004, "end": 2480.32, "text": " like me, you might be thinking, wait, wait, step 11 was talking about an options keeper. And in", "tokens": [50680, 411, 385, 11, 291, 1062, 312, 1953, 11, 1699, 11, 1699, 11, 1823, 2975, 390, 1417, 466, 364, 3956, 38709, 13, 400, 294, 50916], "temperature": 0.0, "avg_logprob": -0.13483373834452497, "compression_ratio": 1.6115702479338843, "no_speech_prob": 0.029308466240763664}, {"id": 543, "seek": 246928, "start": 2480.32, "end": 2487.44, "text": " step 12, we're talking about the singularity. Yeah, it's a bit of a jump, right? It's a bit weird.", "tokens": [50916, 1823, 2272, 11, 321, 434, 1417, 466, 264, 20010, 507, 13, 865, 11, 309, 311, 257, 857, 295, 257, 3012, 11, 558, 30, 467, 311, 257, 857, 3657, 13, 51272], "temperature": 0.0, "avg_logprob": -0.13483373834452497, "compression_ratio": 1.6115702479338843, "no_speech_prob": 0.029308466240763664}, {"id": 544, "seek": 246928, "start": 2488.0, "end": 2493.28, "text": " But that's one things I'm going to be honest, I find interesting, but I'm uncertain about in this", "tokens": [51300, 583, 300, 311, 472, 721, 286, 478, 516, 281, 312, 3245, 11, 286, 915, 1880, 11, 457, 286, 478, 11308, 466, 294, 341, 51564], "temperature": 0.0, "avg_logprob": -0.13483373834452497, "compression_ratio": 1.6115702479338843, "no_speech_prob": 0.029308466240763664}, {"id": 545, "seek": 249328, "start": 2493.28, "end": 2499.52, "text": " paper. The fact that everything they outline up to step 11, for the most part, is within very", "tokens": [50364, 3035, 13, 440, 1186, 300, 1203, 436, 16387, 493, 281, 1823, 2975, 11, 337, 264, 881, 644, 11, 307, 1951, 588, 50676], "temperature": 0.0, "avg_logprob": -0.06693066053154055, "compression_ratio": 1.7240143369175627, "no_speech_prob": 0.08755652606487274}, {"id": 546, "seek": 249328, "start": 2499.52, "end": 2504.6400000000003, "text": " reasonable expectations. It's like what you would expect, but with a different focus. What I find", "tokens": [50676, 10585, 9843, 13, 467, 311, 411, 437, 291, 576, 2066, 11, 457, 365, 257, 819, 1879, 13, 708, 286, 915, 50932], "temperature": 0.0, "avg_logprob": -0.06693066053154055, "compression_ratio": 1.7240143369175627, "no_speech_prob": 0.08755652606487274}, {"id": 547, "seek": 249328, "start": 2504.6400000000003, "end": 2511.1200000000003, "text": " interesting is that they think that we can go from step 11 to step 12. I'm not sure how much", "tokens": [50932, 1880, 307, 300, 436, 519, 300, 321, 393, 352, 490, 1823, 2975, 281, 1823, 2272, 13, 286, 478, 406, 988, 577, 709, 51256], "temperature": 0.0, "avg_logprob": -0.06693066053154055, "compression_ratio": 1.7240143369175627, "no_speech_prob": 0.08755652606487274}, {"id": 548, "seek": 249328, "start": 2511.1200000000003, "end": 2515.84, "text": " effort they think it will take, but essentially, that they think not much will be missing at that", "tokens": [51256, 4630, 436, 519, 309, 486, 747, 11, 457, 4476, 11, 300, 436, 519, 406, 709, 486, 312, 5361, 412, 300, 51492], "temperature": 0.0, "avg_logprob": -0.06693066053154055, "compression_ratio": 1.7240143369175627, "no_speech_prob": 0.08755652606487274}, {"id": 549, "seek": 249328, "start": 2515.84, "end": 2521.1200000000003, "text": " point. And it is interesting to think about. On one hand, I'm very inclined to say, no, of course,", "tokens": [51492, 935, 13, 400, 309, 307, 1880, 281, 519, 466, 13, 1282, 472, 1011, 11, 286, 478, 588, 28173, 281, 584, 11, 572, 11, 295, 1164, 11, 51756], "temperature": 0.0, "avg_logprob": -0.06693066053154055, "compression_ratio": 1.7240143369175627, "no_speech_prob": 0.08755652606487274}, {"id": 550, "seek": 252112, "start": 2521.2, "end": 2524.08, "text": " that's not going to be enough. We already have most of the things they talked about in these", "tokens": [50368, 300, 311, 406, 516, 281, 312, 1547, 13, 492, 1217, 362, 881, 295, 264, 721, 436, 2825, 466, 294, 613, 50512], "temperature": 0.0, "avg_logprob": -0.07749689990327559, "compression_ratio": 1.8524590163934427, "no_speech_prob": 0.019717346876859665}, {"id": 551, "seek": 252112, "start": 2524.08, "end": 2528.24, "text": " previous steps. But on the other hand, we don't actually have the things they mentioned in these", "tokens": [50512, 3894, 4439, 13, 583, 322, 264, 661, 1011, 11, 321, 500, 380, 767, 362, 264, 721, 436, 2835, 294, 613, 50720], "temperature": 0.0, "avg_logprob": -0.07749689990327559, "compression_ratio": 1.8524590163934427, "no_speech_prob": 0.019717346876859665}, {"id": 552, "seek": 252112, "start": 2528.24, "end": 2533.8399999999997, "text": " previous steps. We have usually, for lots of these, we have specific instances, right? So for", "tokens": [50720, 3894, 4439, 13, 492, 362, 2673, 11, 337, 3195, 295, 613, 11, 321, 362, 2685, 14519, 11, 558, 30, 407, 337, 51000], "temperature": 0.0, "avg_logprob": -0.07749689990327559, "compression_ratio": 1.8524590163934427, "no_speech_prob": 0.019717346876859665}, {"id": 553, "seek": 252112, "start": 2533.8399999999997, "end": 2538.3199999999997, "text": " like planning, we have something like Mu zero, but we still have so much more planning to explore.", "tokens": [51000, 411, 5038, 11, 321, 362, 746, 411, 15601, 4018, 11, 457, 321, 920, 362, 370, 709, 544, 5038, 281, 6839, 13, 51224], "temperature": 0.0, "avg_logprob": -0.07749689990327559, "compression_ratio": 1.8524590163934427, "no_speech_prob": 0.019717346876859665}, {"id": 554, "seek": 252112, "start": 2538.3199999999997, "end": 2543.3599999999997, "text": " There's so many different things we could try. And if everything below the planning level, like if", "tokens": [51224, 821, 311, 370, 867, 819, 721, 321, 727, 853, 13, 400, 498, 1203, 2507, 264, 5038, 1496, 11, 411, 498, 51476], "temperature": 0.0, "avg_logprob": -0.07749689990327559, "compression_ratio": 1.8524590163934427, "no_speech_prob": 0.019717346876859665}, {"id": 555, "seek": 252112, "start": 2543.3599999999997, "end": 2547.68, "text": " if we have incredibly good ways to learn representations, and if we have incredibly", "tokens": [51476, 498, 321, 362, 6252, 665, 2098, 281, 1466, 33358, 11, 293, 498, 321, 362, 6252, 51692], "temperature": 0.0, "avg_logprob": -0.07749689990327559, "compression_ratio": 1.8524590163934427, "no_speech_prob": 0.019717346876859665}, {"id": 556, "seek": 254768, "start": 2547.68, "end": 2552.72, "text": " more efficient ways to train your own networks for continuing learning problems, maybe things will", "tokens": [50364, 544, 7148, 2098, 281, 3847, 428, 1065, 9590, 337, 9289, 2539, 2740, 11, 1310, 721, 486, 50616], "temperature": 0.0, "avg_logprob": -0.10315213138110017, "compression_ratio": 1.819935691318328, "no_speech_prob": 0.014062967151403427}, {"id": 557, "seek": 254768, "start": 2552.72, "end": 2558.64, "text": " go a lot better. It's really hard to say. And to be honest, I would cut the authors of this", "tokens": [50616, 352, 257, 688, 1101, 13, 467, 311, 534, 1152, 281, 584, 13, 400, 281, 312, 3245, 11, 286, 576, 1723, 264, 16552, 295, 341, 50912], "temperature": 0.0, "avg_logprob": -0.10315213138110017, "compression_ratio": 1.819935691318328, "no_speech_prob": 0.014062967151403427}, {"id": 558, "seek": 254768, "start": 2558.64, "end": 2561.7599999999998, "text": " some slack. I don't think, you know, they don't think that they're just going to go through this", "tokens": [50912, 512, 29767, 13, 286, 500, 380, 519, 11, 291, 458, 11, 436, 500, 380, 519, 300, 436, 434, 445, 516, 281, 352, 807, 341, 51068], "temperature": 0.0, "avg_logprob": -0.10315213138110017, "compression_ratio": 1.819935691318328, "no_speech_prob": 0.014062967151403427}, {"id": 559, "seek": 254768, "start": 2561.7599999999998, "end": 2565.6, "text": " plan and suddenly hit step 12 and everything's going to work out. I think they're probably,", "tokens": [51068, 1393, 293, 5800, 2045, 1823, 2272, 293, 1203, 311, 516, 281, 589, 484, 13, 286, 519, 436, 434, 1391, 11, 51260], "temperature": 0.0, "avg_logprob": -0.10315213138110017, "compression_ratio": 1.819935691318328, "no_speech_prob": 0.014062967151403427}, {"id": 560, "seek": 254768, "start": 2565.6, "end": 2568.64, "text": " they probably realize, you know, they have to revise this. I think they even mentioned, yeah,", "tokens": [51260, 436, 1391, 4325, 11, 291, 458, 11, 436, 362, 281, 44252, 341, 13, 286, 519, 436, 754, 2835, 11, 1338, 11, 51412], "temperature": 0.0, "avg_logprob": -0.10315213138110017, "compression_ratio": 1.819935691318328, "no_speech_prob": 0.014062967151403427}, {"id": 561, "seek": 254768, "start": 2568.64, "end": 2573.2, "text": " this is provisional. Oh, not crossing this out. It's not not provisional. It is provisional.", "tokens": [51412, 341, 307, 1439, 271, 1966, 13, 876, 11, 406, 14712, 341, 484, 13, 467, 311, 406, 406, 1439, 271, 1966, 13, 467, 307, 1439, 271, 1966, 13, 51640], "temperature": 0.0, "avg_logprob": -0.10315213138110017, "compression_ratio": 1.819935691318328, "no_speech_prob": 0.014062967151403427}, {"id": 562, "seek": 257320, "start": 2573.2799999999997, "end": 2578.3199999999997, "text": " It's a draft and a working plan. It's going to be revised. But I do think it's interesting that", "tokens": [50368, 467, 311, 257, 11206, 293, 257, 1364, 1393, 13, 467, 311, 516, 281, 312, 35228, 13, 583, 286, 360, 519, 309, 311, 1880, 300, 50620], "temperature": 0.0, "avg_logprob": -0.10684173337874873, "compression_ratio": 1.7164179104477613, "no_speech_prob": 0.0803481712937355}, {"id": 563, "seek": 257320, "start": 2578.3199999999997, "end": 2581.9199999999996, "text": " someone like Rich Sutton, who's worked in the field for a long time, has had some really good", "tokens": [50620, 1580, 411, 6781, 40492, 1756, 11, 567, 311, 2732, 294, 264, 2519, 337, 257, 938, 565, 11, 575, 632, 512, 534, 665, 50800], "temperature": 0.0, "avg_logprob": -0.10684173337874873, "compression_ratio": 1.7164179104477613, "no_speech_prob": 0.0803481712937355}, {"id": 564, "seek": 257320, "start": 2581.9199999999996, "end": 2587.2, "text": " ideas, thinks that this will be enough. And to be honest, I mean, I don't think I could come up", "tokens": [50800, 3487, 11, 7309, 300, 341, 486, 312, 1547, 13, 400, 281, 312, 3245, 11, 286, 914, 11, 286, 500, 380, 519, 286, 727, 808, 493, 51064], "temperature": 0.0, "avg_logprob": -0.10684173337874873, "compression_ratio": 1.7164179104477613, "no_speech_prob": 0.0803481712937355}, {"id": 565, "seek": 257320, "start": 2587.2, "end": 2591.2, "text": " with a better plan myself. And I'm not sure quite what's missing here. I guess what's missing are", "tokens": [51064, 365, 257, 1101, 1393, 2059, 13, 400, 286, 478, 406, 988, 1596, 437, 311, 5361, 510, 13, 286, 2041, 437, 311, 5361, 366, 51264], "temperature": 0.0, "avg_logprob": -0.10684173337874873, "compression_ratio": 1.7164179104477613, "no_speech_prob": 0.0803481712937355}, {"id": 566, "seek": 257320, "start": 2591.2, "end": 2595.8399999999997, "text": " obviously the details that you have to fill out, right? Like meta learning, there's a million", "tokens": [51264, 2745, 264, 4365, 300, 291, 362, 281, 2836, 484, 11, 558, 30, 1743, 19616, 2539, 11, 456, 311, 257, 2459, 51496], "temperature": 0.0, "avg_logprob": -0.10684173337874873, "compression_ratio": 1.7164179104477613, "no_speech_prob": 0.0803481712937355}, {"id": 567, "seek": 257320, "start": 2595.8399999999997, "end": 2599.2799999999997, "text": " bazillion ways to do meta learning. How are they going to do it? What's the way to, I don't know,", "tokens": [51496, 27147, 11836, 2098, 281, 360, 19616, 2539, 13, 1012, 366, 436, 516, 281, 360, 309, 30, 708, 311, 264, 636, 281, 11, 286, 500, 380, 458, 11, 51668], "temperature": 0.0, "avg_logprob": -0.10684173337874873, "compression_ratio": 1.7164179104477613, "no_speech_prob": 0.0803481712937355}, {"id": 568, "seek": 259928, "start": 2599.28, "end": 2604.0, "text": " who knows? And that's, that's the thing, right? This is at the end of the day, it's a research plan.", "tokens": [50364, 567, 3255, 30, 400, 300, 311, 11, 300, 311, 264, 551, 11, 558, 30, 639, 307, 412, 264, 917, 295, 264, 786, 11, 309, 311, 257, 2132, 1393, 13, 50600], "temperature": 0.0, "avg_logprob": -0.08812291900832932, "compression_ratio": 1.8761904761904762, "no_speech_prob": 0.014956247061491013}, {"id": 569, "seek": 259928, "start": 2604.0, "end": 2608.1600000000003, "text": " It talks about the things they want to focus on, not how to do them. And it is very vague in that", "tokens": [50600, 467, 6686, 466, 264, 721, 436, 528, 281, 1879, 322, 11, 406, 577, 281, 360, 552, 13, 400, 309, 307, 588, 24247, 294, 300, 50808], "temperature": 0.0, "avg_logprob": -0.08812291900832932, "compression_ratio": 1.8761904761904762, "no_speech_prob": 0.014956247061491013}, {"id": 570, "seek": 259928, "start": 2608.1600000000003, "end": 2612.8, "text": " sense. So overall, those are kind of my thoughts. I really like this. I think it's interesting to", "tokens": [50808, 2020, 13, 407, 4787, 11, 729, 366, 733, 295, 452, 4598, 13, 286, 534, 411, 341, 13, 286, 519, 309, 311, 1880, 281, 51040], "temperature": 0.0, "avg_logprob": -0.08812291900832932, "compression_ratio": 1.8761904761904762, "no_speech_prob": 0.014956247061491013}, {"id": 571, "seek": 259928, "start": 2612.8, "end": 2618.5600000000004, "text": " read. I think it's very familiar, well, at the same time being somewhat fairly different from what", "tokens": [51040, 1401, 13, 286, 519, 309, 311, 588, 4963, 11, 731, 11, 412, 264, 912, 565, 885, 8344, 6457, 819, 490, 437, 51328], "temperature": 0.0, "avg_logprob": -0.08812291900832932, "compression_ratio": 1.8761904761904762, "no_speech_prob": 0.014956247061491013}, {"id": 572, "seek": 259928, "start": 2618.5600000000004, "end": 2622.6400000000003, "text": " people, it's what, or rather, I should say, it's almost the same thing that people are working on,", "tokens": [51328, 561, 11, 309, 311, 437, 11, 420, 2831, 11, 286, 820, 584, 11, 309, 311, 1920, 264, 912, 551, 300, 561, 366, 1364, 322, 11, 51532], "temperature": 0.0, "avg_logprob": -0.08812291900832932, "compression_ratio": 1.8761904761904762, "no_speech_prob": 0.014956247061491013}, {"id": 573, "seek": 259928, "start": 2622.6400000000003, "end": 2627.44, "text": " but with a different problem in mind, different sort of problem setting. And I think interesting", "tokens": [51532, 457, 365, 257, 819, 1154, 294, 1575, 11, 819, 1333, 295, 1154, 3287, 13, 400, 286, 519, 1880, 51772], "temperature": 0.0, "avg_logprob": -0.08812291900832932, "compression_ratio": 1.8761904761904762, "no_speech_prob": 0.014956247061491013}, {"id": 574, "seek": 262744, "start": 2627.44, "end": 2632.08, "text": " differences could arise from that. I could certainly see people having a wide variety", "tokens": [50364, 7300, 727, 20288, 490, 300, 13, 286, 727, 3297, 536, 561, 1419, 257, 4874, 5673, 50596], "temperature": 0.0, "avg_logprob": -0.09532610353056367, "compression_ratio": 1.87012987012987, "no_speech_prob": 0.020330529659986496}, {"id": 575, "seek": 262744, "start": 2632.08, "end": 2637.68, "text": " of reactions to this. Some people saying this is completely useless. It's not detailed enough at all.", "tokens": [50596, 295, 12215, 281, 341, 13, 2188, 561, 1566, 341, 307, 2584, 14115, 13, 467, 311, 406, 9942, 1547, 412, 439, 13, 50876], "temperature": 0.0, "avg_logprob": -0.09532610353056367, "compression_ratio": 1.87012987012987, "no_speech_prob": 0.020330529659986496}, {"id": 576, "seek": 262744, "start": 2637.68, "end": 2641.52, "text": " I could see other people saying, Oh, this is very interesting. I could see some people saying, Oh,", "tokens": [50876, 286, 727, 536, 661, 561, 1566, 11, 876, 11, 341, 307, 588, 1880, 13, 286, 727, 536, 512, 561, 1566, 11, 876, 11, 51068], "temperature": 0.0, "avg_logprob": -0.09532610353056367, "compression_ratio": 1.87012987012987, "no_speech_prob": 0.020330529659986496}, {"id": 577, "seek": 262744, "start": 2641.52, "end": 2646.56, "text": " this is, this is the next step in the future. I really don't know. I'm very curious. So let me", "tokens": [51068, 341, 307, 11, 341, 307, 264, 958, 1823, 294, 264, 2027, 13, 286, 534, 500, 380, 458, 13, 286, 478, 588, 6369, 13, 407, 718, 385, 51320], "temperature": 0.0, "avg_logprob": -0.09532610353056367, "compression_ratio": 1.87012987012987, "no_speech_prob": 0.020330529659986496}, {"id": 578, "seek": 262744, "start": 2646.56, "end": 2652.0, "text": " know what you think in the comments. I'm, I think we'll have some very different opinions. I am", "tokens": [51320, 458, 437, 291, 519, 294, 264, 3053, 13, 286, 478, 11, 286, 519, 321, 603, 362, 512, 588, 819, 11819, 13, 286, 669, 51592], "temperature": 0.0, "avg_logprob": -0.09532610353056367, "compression_ratio": 1.87012987012987, "no_speech_prob": 0.020330529659986496}, {"id": 579, "seek": 262744, "start": 2652.0, "end": 2656.7200000000003, "text": " excited to say, if you've enjoyed this, do consider subscribing to the channel, maybe check it out", "tokens": [51592, 2919, 281, 584, 11, 498, 291, 600, 4626, 341, 11, 360, 1949, 19981, 281, 264, 2269, 11, 1310, 1520, 309, 484, 51828], "temperature": 0.0, "avg_logprob": -0.09532610353056367, "compression_ratio": 1.87012987012987, "no_speech_prob": 0.020330529659986496}, {"id": 580, "seek": 265672, "start": 2656.72, "end": 2660.3999999999996, "text": " some of my other videos. I would really appreciate it. It really helps out. And hopefully you'll", "tokens": [50364, 512, 295, 452, 661, 2145, 13, 286, 576, 534, 4449, 309, 13, 467, 534, 3665, 484, 13, 400, 4696, 291, 603, 50548], "temperature": 0.0, "avg_logprob": -0.15412537868206316, "compression_ratio": 1.4315068493150684, "no_speech_prob": 0.011156871914863586}, {"id": 581, "seek": 265672, "start": 2660.3999999999996, "end": 2665.04, "text": " find some other interesting content. Anyway, thank you so much for joining me and I hope to catch you", "tokens": [50548, 915, 512, 661, 1880, 2701, 13, 5684, 11, 1309, 291, 370, 709, 337, 5549, 385, 293, 286, 1454, 281, 3745, 291, 50780], "temperature": 0.0, "avg_logprob": -0.15412537868206316, "compression_ratio": 1.4315068493150684, "no_speech_prob": 0.011156871914863586}, {"id": 582, "seek": 265672, "start": 2665.04, "end": 2665.68, "text": " next time.", "tokens": [50780, 958, 565, 13, 50812], "temperature": 0.0, "avg_logprob": -0.15412537868206316, "compression_ratio": 1.4315068493150684, "no_speech_prob": 0.011156871914863586}], "language": "en"}