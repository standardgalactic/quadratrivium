start	end	text
0	4720	I think it's the dream of a lot of researchers, to have an agent where you can just give it a goal
4720	8960	and take a step back from your computer, go grab a coffee or, you know, if you're a little bit
8960	10800	more cultured like me, maybe some green tea.
14640	18560	Oh, that was not planned. Anyway, maybe even throw a YouTube video in there.
18560	23440	Then you come back after 10, 15 minutes and whabam. You have an agent that can do exactly
23440	28240	what you tasked it to do, because within those 10 to 15 minutes, it was efficiently exploring,
28320	32080	experimenting, and now it is honed in on a way to achieve the task you gave it.
32080	35840	And unfortunately, right now, that's just kind of a pipe dream. That's because
35840	40240	learning from scratch just takes way too long. It'd be kind of like getting a newborn to learn
40240	44480	how to play chess. I mean, how can you learn how to play chess when you don't even have
44480	48160	object permanence? Heck, first, you know, the baby would need to learn to move its arms.
48160	51920	Heck, babies don't even know that their arms are their arms. So the point being,
51920	56560	it just doesn't quite work. Or at least it didn't until very recently when DeepMind
56560	61120	published a paper on ADA, where in this paper, authors train an agent to be able to learn new
61120	66480	tasks most of the time faster than humans. And this is all taking place in a fairly complex
66480	71280	3D environment. For example, here you can see the agent running around, efficiently testing out
71280	75440	different ideas to see what will achieve its new goal. We'll dive deeper into this in a minute,
75440	79440	but first I'd like to address something. If you're familiar with work in this area,
79440	84080	you may be like, Eden, none of this is new. There's plenty of prior work on metal learning,
84160	87920	view shot learning, the same kind of stuff that's going on here. And you would be right.
87920	93280	ADA doesn't have any new groundbreaking ideas, but rather combines ideas that've worked pretty well
93280	97760	and puts them together in a way that allows this method to scale to larger models. I think that's
97760	102560	a pretty important contribution, primarily because ADA is a method for reinforcement learning. Or in
102560	106960	other words, it's trained an agent to maximize their reward as its goal. And if you know anything
106960	112160	about RL, you know, it can be pretty sample inefficient. And when you combine sample inefficiency
112160	116640	with scale, well, all hell breaks loose. As a little sneak peek of what's coming up, they actually
116640	122560	show this later down in this paper, where they prepare a 23 million and a 265 million parameter
122560	128160	model and train both on 22 billion frames of total interactions. And without any changes,
128160	133760	the bigger model actually performs significantly worse. In fact, it has a hard time learning at
133760	138160	all. As we'll see, the authors propose a way to combine ideas in the field to scale this up,
138160	142720	with the end goal being a framework for training an RL foundation model, because then instead of
142720	147200	always having to start training from scratch, you could have a baseline to significantly speed
147200	151760	up learning new tasks. You could think of this as the same way you can use GPT three to train
151760	156720	something like a sentiment analysis model, and just a few examples instead of well, millions.
156720	161520	Next, we're going to take a look at how ADA works. But before that, I want to thank clear ML for
161520	165360	making it possible for me to spend so much time on videos like this by being a sponsor.
165440	170640	Clear ML offers an end end platform for ML ops, where you can do everything from tracking experiments
170640	174800	to automating an entire machine learning pipeline through to deployment. Here's the code for one
174800	178640	of my current projects. And with just an extra several lines of code, I can integrate it with
178640	182800	clear mail. And with that, we can now pull up a dashboard and see the progress of my experiment
182800	186800	as it runs. Here, I'm just logging the loss because I wanted to get this done in a few seconds,
186800	190880	but there is a lot more you can do with this. But really, this is just the tip of the iceberg.
190880	194720	You can also create entire pipelines that pull in different versions of your data sets,
194720	199520	run hyper parameter sweeps, automatically set up new environments. And because of that last point,
199520	204000	clear mail has no problem with scaling. As I run many experiments in my own research,
204000	207760	I personally love some of the features that help me with that. For example, I love how clear
207760	212480	mail not only tracks code version, but also uncommitted get changes, which is very nice.
212480	216400	And also how through the dashboard, I can inject configs into my experiments,
216400	220880	which means I don't have to constantly be modifying my code to run a bunch of new experiments.
220880	224640	If that sounds like something you're interested in, you can try out their product for free by
224640	228880	following the link in the description below. And now let's get back into this paper and talk
228880	233360	about how 80 works right off the bat. The authors get pretty straight to the point by listing the
233360	238720	three key components that allow for adaptation to emerge. One is meta reinforcement learning
238720	244560	across a vast, smooth and diverse task distribution. Two is a policy parameter prize as a large scale
244560	250320	attention based memory architecture. And three is an effective automated curriculum that prioritizes
250320	254800	tasks at the frontier of the agent's capabilities. If we scroll down a bit, we'll see a high level
254800	259040	visualization of the whole training process. And we can use this as we talk about each of these
259040	264480	steps. Going back to our first point, we have meta learning over a diverse set of tasks, where in
264480	269120	our diagram here, we have that set of diverse tasks. And here we have the RL update that will
269120	273360	allow us to do meta learning, perhaps the most interesting thing about the use of meta RL here
273360	277520	is that the meta learning comes from how the environment tasks are structured and not the RL
277520	281920	algorithm itself. So let's take a look at the environment they use here and this key structuring
281920	286720	that I'm referring to the environment is called X land to, and as opposed to being a single environment,
286720	290720	it's built to be completely customizable so that you can make your own suite of environments.
290720	295680	Each different instance consists of one to two agents. Yes, they also do some multi agent learning
295680	300080	here, which we'll get into. Then there are some initial objects that start out in the world rules
300080	305280	for how those objects interact. For example, this rule right here specifies that when a black pyramid
305280	309200	touches a yellow sphere, it generates a black cube in their place. There are different types
309200	313520	of terrain. And then there is a goal for the agent to achieve, which in this case, the goal
313520	317840	is to hold the black cube for as long as possible. And then there's also a pretty strict time limit
317840	322080	for all of this. And if all of that wasn't hard enough already, several of these specifications
322080	326320	can actually be hidden from the agent itself. Sorry, I've cut it off a little bit here. For
326320	330720	each task instantiation, each of these components are randomized, which ends up giving a whopping
330720	335600	total of 10 to the 40 unique task combinations. That's, that's a lot of tasks. When the authors
335600	339840	said having diverse tasks was essential, they certainly were not skimping on this. But now
339840	344240	we can take a look at the part of the task setup that makes this into a meta RL formulation. And
344240	348880	that has to do with how they structure these trials and episodes. Typically in episodic
348880	353840	reinforcement learning, everything is reset after each episode. And the same process repeats episode
353840	358560	after episode. But here they use the RL squared algorithm for meta learning, which is a bit
358560	363120	different here for each task, a number of trials is determined, which is generally between one and
363120	367520	six while they're training their models. Each trial works kind of like an episode typically would
367520	371600	the same task is used. That means the same environment, the same goal, the same everything.
371600	376640	And every time a trial ends, everything is reset, except for one thing. And this is where it differs
376640	381680	from an episode. The one thing that is not reset that the agent has is a memory module that memory
381680	387200	module persists through trials. And it's only reset after all the trials are over at the end of an
387200	392480	episode. Well, this may seem like not a huge deal. This is actually essential for how this works,
392480	397040	because if the agent can remember previous trials, then it can use its previous exploration
397040	401200	to improve in future trials. And this works because the agent has memory. So if it tries
401200	405120	something in trial one, that doesn't work, and it still has that memory going into trial two,
405120	408960	well, it's probably not going to try the same thing in trial two. And that's the idea here.
408960	412560	What you're looking at right now is an example of this where the agent learns that if it picks up
412560	417200	this object, it disappears. So in the next trial, it learns from that and pushes the object instead.
417200	421280	Without making any changes to how the RL algorithm itself works, we've turned this
421280	425680	into a meta reinforcement learning problem. And speaking of the RL algorithm, which one do they
425680	430320	use here? Well, in theory, any solid reinforcement learning algorithm could work here. But for this,
430320	435440	they decide to use Mooseley or Mooseley or I don't know, hopefully I'm not saying it too wrong.
435440	440400	Mooseley is a model based reinforcement learning algorithm that's based off MPO, but adds a few
440400	444720	bells and whistles, including a value equivalent model component. Looking at the Mooseley paper,
444720	450000	we can see that it performs about the same as mu zero on Atari, but without the huge overhead
450000	454800	of having to do planning. I think one key thing to note here is that Mooseley's model does not try
454800	459760	to reconstruct observations or predict future observations, which is perfectly fine in and
459760	464560	of itself. But other model based methods that have done this, like efficient zero and dreamer,
464560	469280	tend to be significantly more sample efficient. As you'll see, the training in this paper happens
469280	473920	over billions of environment steps. So perhaps that change would be a simple way to make this
473920	478080	more sample efficient. But perhaps one thing that's even more important than sample efficiency in this
478080	483280	case is subscribing to the channel because without subscribing, happy doggo may be reconstructed into
483280	487600	mad doggo. And no one wants that. And if you want more machine learning content like this, you know,
487600	491600	consider subscribing. Anyway, I'll link the paper in the description if you want more details,
491600	495680	but it's not super important for understanding the takeaways of this paper. Awesome. With that,
495760	500240	the way we can check meta reinforcement learning off the list and move on to part two, which is
500240	505440	having a large scale attention based memory architecture, large architectures, attention.
506240	511120	I wonder what that could be. It's right here. How could I miss this? A transformer who could
511120	515360	have seen that one coming? No, but actually, it's not just transformers. In total, they try
515360	520080	three different architectures. The first of them is this transformer XL architecture, which is a
520080	524960	transformer that kind of has a sort of memory. The way it works is that each hidden unit in
524960	529680	the transformer also gets input from the layer below it at the previous time step, which means
529680	534240	that information from previous time steps are going into the future, which means that there is
534240	538080	kind of some sort of memory. If I want to be more specific, what this really does is that it
538080	542160	effectively extends the context length of the transformer. Though it's not written in this
542160	547360	list, they also try a vanilla RNN, a GRU to be specific, is just a very vanilla memory based
547360	551200	model where you have some sort of memory, and you have some gates that decide what gets remembered
551200	555600	and what gets forgotten at each step. Then they also try one more type of RNN that is augmented
555600	561200	with attention that attends over previous time step activations. So very similar to how a standard
561200	565040	RNN works, but just with that extra little bit of attention thrown in there. We can take a look
565040	569120	at how these methods compare to each other in this figure here, where the authors have graphed the
569120	574560	normalized score of these agents based on the final trials of episodes in the test tasks. Or in
574560	579200	Lehmann's terms, big number is good number. What we can see here is that both of the attention
579200	584240	based models significantly outperform the vanilla RNN, despite the fact that they use about the same
584240	588160	number of parameters. Clearly, the attention is doing something to help out quite a bit here. And
588160	592560	due to these results, most of the experiments we'll look at a bit later, use the transformer XL
592560	596320	architecture. So we've covered the architecture here, but there's actually one more portion of
596320	600400	the paper I should talk about in this architecture segment. And that is this distillation update.
600400	604320	If you remember what I showed you in the beginning about how this large model was not able to learn
604320	608560	from this setup, you may be wondering how they end up getting that to work. And the answer here is
608560	613280	kind of interesting. Nothing changes about the architecture, the RL training or any of that,
613280	617680	but rather they add two steps to the start of training to essentially kickstart the large
617680	622480	model's learning progress. And that step is where this idea of distillation comes into the picture.
622480	626800	First, they train a smaller model called the teacher model, using the whole reinforcement
626800	630800	learning training process we were just talking about. Once the smaller model has trained for
630800	635200	several billion steps, then they create the larger model. And when they create the larger model,
635200	638800	they don't just start training it with reinforcement learning from scratch, but rather
638800	642880	for the first four billion steps of training, they use an additional distillation loss. And what
642880	647200	this distillation loss does is essentially distills the teacher or the smaller model
647200	651840	into the big model. Or in other words, they try to have the larger model imitate the smaller
651840	655760	teacher model. The idea here is that because it's much easier to train a smaller model,
655760	659680	even though it might not be as good, you can start by training a small model. Then by having
659680	663680	the larger model imitate the small model, it gets a lot more signal that doesn't require
663680	668320	interacting with the environment some unreasonable amount of times. The idea is once they've done
668320	672640	this for the first four billion steps, the larger model will have already kickstarted its learning
672640	676800	process. So then it will be much easier to learn on its own and it won't just stagnate without being
676800	681280	able to learn anything at all. And now looking at this figure again, it should make a lot more sense
681280	686000	when we train the large model from scratch, it performs pretty horribly. But when we add a
686000	690640	distillation loss, it outperforms everything else. And this is the case for both tasks where the
690640	695680	agent is achieving a median level score, and tasks where the agent is only performing a 20th
695680	700160	percentile score. In other words, whether the agent is good or bad at this task, this approach
700160	704720	works either way. And that explains this distillation loss. Now it's actually kind of funny, because
704720	709280	most of the time when you see someone doing model distillation, it's being used to distill a large
709280	713200	model into a smaller version of that model, so that you can be more efficient at inference time.
713200	716800	But here they're actually doing the opposite in a successful attempt to train the large model
716800	720400	faster, which isn't something I thought about before, but it certainly is an interesting
720400	725520	use of model distillation. So now we know how to set up our tasks as a meta RL problem, we know
725520	729360	what algorithm we're going to use for training, and we know how to get that to work with a large
729360	733680	attention based model. But the point here is to build a foundation model that can explore and
733680	738000	solve tasks efficiently, especially in environments where reward is somewhat sparse. And this is
738000	743520	the case in XLAN 2. The issue is that if you throw a bunch of hard exploration tasks at a new agent,
743600	747920	it's just going to bang its head into the wall over and over and over. And it's not really going
747920	751920	to learn anything, because it's never going to get that first reward or enough reward in the
751920	756560	beginning that gives an incentive or enough signal to learn from our 10 to the 40 tasks,
756560	760800	we need to be able to choose ones that are not too hard, yet also not too easy for the agent at
760800	764960	any given time. And that is where step three comes into play, auto curriculum learning, which we
764960	769360	can see happening as these top steps right here. This brings us to the auto curriculum learning
769360	774080	section, unsurprisingly, where we want to build some sort of curriculum of tasks for the agent
774080	778320	based off its current skill level. And to do this, the authors try two different approaches.
778320	783120	Approach number one is no op filtering. The idea here is that you start by sampling a new task,
783120	788400	and you let both the ADA agent and the no op agent attempt the task. The no op agent takes no
788400	792960	actions, hence the name no op, so it can be used as a baseline for comparison. Given the outcome,
792960	797280	some heuristics are used to determine whether or not this task is at the right difficulty level.
797280	801920	One example of a heuristic they use is that the agent should be able to at least get some reward,
801920	806080	but it shouldn't be too good at the task either. And another criteria, for example,
806080	810400	is that the agent has to achieve a score that's sufficiently different from the no op agent.
810400	814480	And if all these defined heuristics are passed, then the task is used for training. The other
814480	819520	method used for generating curriculum they try is prioritized level replay or PLR for short.
819520	823280	Instead of essentially using a series of if statements like a no op filtering,
823280	827840	PLR runs ADA through candidate tasks and estimates the agent's regret. And regret is
827840	832640	essentially how much potential reward the agent failed to attain as estimated by its TD air.
832640	837280	A fitness score is calculated from the grid, higher regret being better because that means the
837280	841200	agent knows that it has more room to improve. And all the tasks with the highest fitness scores
841200	845600	are kept in a buffer that is sampled from during training. In ablations, we can see that both of
845600	850240	these methods, no op filtering and PLR were quite a bit better than uniform filtering,
850240	855680	which is just random sampling of tasks. However, overall, PLR does perform a little bit better
855680	860240	looking at how PLR and no op filtering changed the task difficulty over time.
860240	865200	In the examples they give here, we can see PLR starts out with less rules, more trials,
865200	869680	gives less dead end rules overall. And it also doesn't hide as many of the critical rules.
869680	873840	And because of these results and PLR's overall high performance, it's used as the curriculum
873840	878320	algorithm of choice for the experiments that we'll be looking at in a bit. So that was a lot,
878320	881840	but we've finally gotten through all the components of how it worked. So now we should be
881840	887280	able to understand this full diagram. We start with a massive task pool from XLAN2. We randomly
887280	892000	sample a bunch of those environments. We use PLR to check which ones have the highest fitness,
892000	895840	throw those into a training set that can be sampled from at any point during training. From
895840	901200	there, these get fed into the actual ADA agent, which uses in the beginning distillation updates,
901200	906240	and throughout the entire process, also RL updates to update this transformer based architecture.
906240	910960	And because it has memory combined with this unique trial in episode format, we're able to
910960	916320	meta-learn how to adapt on the fly. And now that we understand, or at least I hope you all understand,
916320	921520	that works, we can finally dive in to the results. So let's get started with these results that give
921520	926080	us sort of an overview of how the fully trained agent is performing. The score you see here on
926080	931120	the Y axis is normalized based on a model that is fine-tuned on this test task, which means that a
931120	935840	score of around one is very good because it means that the model that is just trying to adapt on
935840	940560	the fly is just as good as something that has been fine-tuned on this task. And looking at this
940560	945280	figure, I think there's two main takeaways. And one is that ADA is pretty effectively able to make
945280	951600	use of its multiple trials. As we can see, there is a pretty large gap between having one and 13
951600	956240	trials. Though after eight or so trials, those benefits do start to wear off as we can see the
956240	960640	eight and the 13 curves are pretty close to each other. Though this should be kind of what we were
960640	965200	expecting because ADA was only allowed up to six trials during training. So if it magically learned
965200	969200	how to use more, well, that would be kind of crazy. And the second thing to take away, you know,
969760	976640	ADA is doing pretty good here. It's able to get greater than an 80% score on 72% of these tasks.
976640	981760	And it's only a mere about 10% of tasks where ADA doesn't get any reward at all. So overall,
981760	986400	pretty good, especially considering that ADA only has a limited amount of time to solve these tasks
986400	991280	that are pretty complicated and always going to be randomized. So the final model is doing pretty
991280	995280	good. But how does this compare to humans? I said this is pretty good. These are pretty hard
995280	999520	tasks. But maybe these are easier than I'm making them out to be. After all, maybe if we gave this
999520	1004400	to humans, they would be able to do this much faster, you might think. Luckily for us, the
1004400	1008960	authors actually tested this idea with a number of human trials. Here you can see the scores of
1008960	1014560	humans and then also of the ADA agent as it gets more trials. And what you immediately notice is
1014560	1020160	what is that? Look at that gap right there. I'm not going to lie. I expected humans to kind of
1020160	1024000	crush this task. It seems like kind of difficult, but you would think that, you know, a human would
1024000	1028640	be able to figure this out. So one thing I thought looking at this at first was, you know, maybe humans
1028640	1033200	because they don't have any prior experience with this specific environment, it takes some time to
1033200	1037200	figure out the rules. There's all these rules. Some of them are hidden. There's different goals.
1037200	1041760	Everything's randomized. But perhaps if a human, you know, maybe had some priming to this task,
1041760	1045680	then they would be able to do a lot better, maybe on par with ADA. Turns out they thought of that.
1046640	1051600	The humans that participated in these trials, actually before they participated in these trials,
1051600	1056160	had to complete a bunch of test levels, not the ones that are shown in this graph,
1056160	1059760	but a bunch of different test levels that would have familiarized them with the rules of how
1059760	1064880	this works. So despite that, humans still kind of suck. Before we move on to this, I also want
1064880	1069360	to show you this. What we just saw was averaged over all these different tasks. But here we can
1069360	1073760	see the individual tasks. And one thing to note is that there are some tasks where humans are
1073760	1079040	actually able to perform better, or ADA really just can't do anything at all. However, in the
1079040	1083360	vast majority of these tasks, as you saw reflected in the previous figure, ADA still does perform
1083360	1087280	better. So overall, there are some things that humans can do, but ADA can't do. And there are
1087280	1091520	many things that ADA can do faster, but humans cannot do very well. But in the average, ADA is
1091520	1095600	much faster to learn than humans. The next thing we'll take a look at is multi agent learning,
1095600	1099200	because apparently it wasn't enough for the authors just to do single agents, they need to do multi
1099200	1103840	agent learning too. It's pretty crazy. So multi agent learning is unsurprisingly, when you have
1103840	1107680	multiple agents, but more interestingly, in these tasks, where there are multiple agents,
1107680	1111760	these agents need to cooperate to get the maximum reward. For example, in this environment, you're
1111760	1116400	seen right here, you have two agents one right here and one right here. But the goals for each of
1116400	1120560	them, I think are to hold these objects on the other side of the wall that they cannot pass. So
1120560	1125760	to maximize the total reward here, both of these agents need to throw their items to the other
1125760	1130240	side of the wall. So this is just one example of what a multi agent environment might look like.
1130240	1133120	At the figures up here, we can see what they're trying to measure. And what they're trying to
1133120	1138000	measure for multi agent learning is is an ADA agent cooperating with another ADA agent. In other
1138000	1141600	words, they have, you know, ADA, they can replicate it, they just have it work with itself. Is that
1141600	1145920	better than cooperating with a random agent? Because if it is, that would mean that ADA is
1145920	1150320	learning some sort of cooperation that's better than just random behavior. For all the tasks that
1150320	1153920	ADA is being given, if we look at the one where it's achieving the median score, we can see that
1154000	1157760	there's not actually a huge difference. But the reason there's not a huge difference is because
1157760	1162560	they're both already near one, they're already near optimal performance. If however, we instead
1162560	1166880	look at the 20th percentile, or in other words, tasks where ADA is not able to do quite as well
1166880	1172240	that are harder, we can see that cooperative self play, which is where ADA is playing with itself
1172240	1177040	instead of a random agent does decently outperform playing with a random agent. So this just shows
1177040	1181760	us that ADA without any special multi agent objective can actually cooperate with other
1181760	1186480	agents when those actions are in its favor. One reason I kind of like these results and I don't
1186480	1191440	mean to be spin fire here, but when you go into sort of the multi agent universe of research
1191440	1195680	being done, there are lots of approaches where they treat multi agent as this sort of separate
1195680	1199840	setting from the single agent RL problem. And while that can be useful, I think what this shows is
1199840	1204880	so long as it's beneficial to the single agent multi agent cooperation can arise as an emergent
1204880	1208960	behavior in cases like these, at least there's actually no need to model them separately. Maybe
1208960	1212480	it would get you better performance or not. I'm not entirely sure, but it's just some new good
1212480	1216000	experiment showing that this kind of thing is also feasible. This next experiment we're taking
1216000	1220560	a look at is a scaling experiment. I mean, come on, how can you publish a paper these days without
1220560	1227760	a scaling experiment? Am I right? I wonder if I should add in some like laughing noises here.
1233280	1238400	And unsurprisingly, as you might have guessed, the results here are bigger model do better. Now,
1238400	1242080	I think we should, you know, maybe not take this for granted because it was shown
1242080	1245760	without this whole distillation process that they're doing to get the bigger models working.
1245760	1249680	The larger models actually couldn't really learn it all. So these scaling curves are really possible
1249680	1255120	because of that. The x axis ranges from 6 million to 265 million parameters, which I don't know,
1255120	1258800	maybe if you only look at large language model stuff that may seem small, but that's huge for
1258800	1262640	reinforcement learning models. No one really does reinforcement learning models that big because
1262640	1266560	it's incredibly hard to train them as as you've seen. And then again, they have the score on the
1266560	1270960	y axis, but they're both log scaled. And in the paper, they say because these are log scaled and
1270960	1276320	these look approximately linear, this is a roughly power scaling law. Maybe maybe this is a bit of
1276320	1280160	a hot take. I'm sorry, I'm going to share a bit of a hot take. That's that I'm not sure if people
1280160	1284320	are just like gung ho to say that everything's power scaling nowadays. But if there wasn't already
1284320	1289280	this idea in everyone's heads that machine learning models scale via power law, I don't know, does this
1289280	1294240	does this look linear to other people? I guess here right at the top for the 13 number of trials,
1294240	1298080	it is tapering off because it's getting to a round maximum performance, right? But if we look
1298080	1301280	on the right, where these are the scores for the 20th percentile tasks, like,
1304000	1307920	is that linear? It doesn't look like it to me. It looks, you know, a little bit more like that.
1307920	1312240	Anyway, maybe I'm being a little picky here. But another thing to mention is they are showing
1312240	1316320	the median and the 20th percentile scores, which I have no problem with. That's perfectly fine.
1316320	1320880	But you know, if they showed the mean, how would that look? Would it also look like power law scaling?
1320880	1325600	Would it look linear? I don't know. I don't know. I can't imagine thinking this is a power law if I
1325600	1329440	did this without already having that in my head. This isn't even like a flight. It's just me going
1329440	1333440	on a stupid rant. That's a tangent. So I'll get over this. I'll go to the next results. Now I'm
1333440	1337840	just happy that it does scale. It's pretty great results. These next results are again about scaling,
1337840	1342240	but this time they're scaling the memory length instead of the size of the model. I think pretty
1342240	1347360	unsurprisingly, as you get a larger memory length, you do get better results here. So that is great.
1347360	1351440	One thing that is interesting to point out here, maybe two things. One is that memory seems to be
1351440	1356400	much more important in cases where the agent is not doing as well. So again, these are the 20th
1356400	1360160	percentile tasks. In other words, it's the tasks that are harder for the agent. And perhaps the
1360160	1364960	reason memory is more helpful on these is because it can't do it like in one shot or in just a few
1364960	1369120	trials. It needs more trials. Hence that longer memory comes more in handy. And the other thing
1369120	1373120	that's interesting is that if we look at one case as an example here, say this case where we have
1373200	1380400	five trials, there are 300 time steps in each trial. And five times 300 gives us 1,800 as a
1380400	1385440	total number of time steps in all of these trials. However, if we look at this sort of greenish cyan
1385440	1391440	line, we will see that once it gets to 1,800, it still continues to go up a little bit. I mean
1391440	1395680	that even though the memory is getting longer than the total amount of steps it will actually see,
1395680	1399840	it's still benefiting from that longer memory. And that's likely because even though it can keep
1399840	1404960	memory for 1,800 steps, it's still going to remember something that's much more recent and
1404960	1408880	actually still happening like in the present. The agent's probably going to remember that much
1408880	1413280	better than something that happened 1,800 steps ago, even if it theoretically can. So even once
1413280	1416800	you sort of hit that limit of the memory theoretically being able to contain everything
1416800	1420880	that you need, expanding it still can help out a bit. And maybe one more thing I should point out,
1420880	1425840	which is perhaps obvious, but important sort of as a sanity check is that the memory is the most
1425840	1430160	helpful when going from one to two trials, which is exactly what we should expect, right? For one
1430160	1434560	trial, we just need enough memory to contain the current episode. But when we go up and we add more
1434560	1438880	trials, that memory suddenly becomes a lot more important, because not only are we just sort of
1438880	1443440	remembering what our current trajectory is, we're remembering the fumbles we had in the past, and
1443440	1446960	we're learning how to improve those we're trying to adopt. Hence, you know, we see the biggest gap
1446960	1450880	here, which is sort of a sanity check that this is working how we would expect it to. And I swear
1450880	1454640	that these are the last results that I'll show you that do some sort of scaling. Here we're looking
1454720	1459600	at scaling, I guess not really scaling, but just the number of tasks use when we use 200 million
1459600	1465440	tasks versus 25 billion tasks, you know, just small difference, 200 million, 25 billion, a wee bit
1465440	1471040	more. And as you get more tasks unsurprisingly, the performance increases, I think this isn't too
1471040	1476720	much of a surprise. If you think of the excellent tasks as a distribution over tasks, we have 200
1476720	1480480	million tasks, you're of course sampling many different possibilities. But when you have 25
1480480	1483760	billion, you're sampling more possibilities, which means you're kind of filling out your
1483760	1487440	training distribution a bit more, you're also getting more diversity in what you learned. So
1487440	1491040	I think it makes sense that we should expect to see this perform better. And one thing that's kind
1491040	1496000	of cool is at least with two to three trials, we can actually have the smaller model, the 23
1496000	1500880	million parameter model in blue here outperform the 75 million parameter model by a bit just by
1500880	1504880	adding more tasks showing that you know, scaling parameters is not the only thing that's important.
1504880	1509760	Having a diverse wider set of tasks is also very important. And I think this kind of mirrors what
1509760	1514240	we've seen in NLP, where language models, you know, you need a bigger model to do better,
1514240	1518000	but you also need better data and that can help quite a bit. And now we finally wrap it back around
1518000	1522160	these distillation results. I know I already showed you how a large model without distillation
1522160	1526800	feels miserably, but one interesting experiment they add on beneath this is what happens when they
1526800	1531760	do distillation with two small models of the same size, you might expect that there would be no
1531760	1536240	benefit, or at least that's what I thought. But as it turns out, even when these two models are
1536240	1540800	the same size, the teacher and the student here, there is still some benefit. You can see that the
1540800	1545040	student ends up with a bit higher score than the teacher, even though they are trained for the same
1545040	1549200	amount of training steps. The reason they mentioned the paper for why this might be happening is due
1549200	1553040	to what's called the primacy bias. Now they don't mention this name in the paper, but I'm pretty sure
1553040	1556880	this is what they're referring to. There's a great paper all link in the description that talks about
1556880	1562000	this idea. And in the paper, the authors show that representations implicitly learned early on during
1562080	1565920	reinforcement learning can hinder an agent's later performance. Essentially, the early
1565920	1570160	representations can be bad, and then it's hard for the agent to unlearn those. So here are the
1570160	1574800	author's posture that perhaps the distillation process is helping to avoid these bad early
1574800	1579440	representations. Honestly, I'm not entirely convinced though, because I think having an extra
1579440	1583760	distillation loss alone could be the reason for the better peak performance here. That being said,
1583760	1588480	I do think this would be an interesting avenue to explore further because it's certainly not 100%
1588480	1592400	either way. I was actually taking some time to think about this and, you know, having to train
1592400	1597520	multiple models for this whole distillation process gets kind of messy and takes a lot of memory.
1597520	1601680	It would be really nice if we could get this to work with just one model. And I was wondering if
1601680	1605680	that would be possible by training with a very high learning rate or maybe training multiple
1605680	1610400	times in the beginning with these large models. One issue, of course, of training with a larger
1610400	1614320	learning rate is that some of the representations could end up bad or the model could start jumping
1614320	1618960	into some rough optimization territory. But if we randomly reset layers of hidden units,
1618960	1623120	which is something they do in the primacy bias paper to unlearn bad representations,
1623120	1627920	then maybe something like that would work. I really have no clue. This is a complete shot in
1627920	1631600	the dark and it's been quite the tangent. So I'll get back to the results now, but you know,
1631600	1635840	if anyone was looking for a research idea to try, there's an idea. We've covered what I think are
1635840	1640480	the most interesting results from this paper. So with this, where do we now stand in the world of
1640480	1645120	RL foundation models? I'll start by saying, I think this is really great. Several papers I think
1645120	1650000	have been trying to get at this idea. For example, the Gatto paper from D-Mind and the VPT from
1650000	1653680	Minecraft paper from OpenAI, both getting at something similar. And I've actually covered
1653680	1658240	both on this channel if you are interested. But all of the other papers on this topic have shied away
1658240	1663040	from actually using RL from start to finish. For example, the Gatto paper, which used supervised
1663040	1666640	learning, I remember it kind of had a line in it. I'm going to be paraphrasing this because I don't
1666640	1670720	remember it exactly, but it was something like, in theory, we could also do the same experiments
1670720	1674400	with reinforcement learning. And surely it would just work fine because, you know, it's just kind
1674400	1680240	of the same thing, but with RL instead of supervised learning. And no, hopefully I'm not
1680240	1684880	butching what they said here. But looking at this paper now, I think it's obvious why doing this
1684880	1688960	with supervised learning and doing it with reinforcement learning is really not the same.
1688960	1693680	Reinforcement learning, it's just a lot harder. The problem itself is a harder problem. Getting
1693680	1697840	the model to scale was not straightforward at all. I mean, the whole distillation thing is not
1697840	1701600	something I would have thought of at least. Not to mention it took billions of training steps,
1701600	1706000	plus millions and millions of unique tasks to get this working. But I think the really great thing
1706000	1709920	here is that now that we have a paper looking at this, not shying away from the hard questions,
1709920	1713840	is that we have a reference. And we can use this reference to start deciding the next important
1713840	1718400	steps to make this feasible on more diverse sets of tasks. And the main one here definitely looks
1718400	1723120	to be sample efficiency. Even if this is a foundation model requiring billions of steps,
1723120	1726320	it's just not going to cut it for one, anyone that doesn't have, you know,
1726320	1731760	an army of TPUs at their command. Everyone other than Google and OpenAI. Oh, sorry,
1731760	1737680	what was that? And two, any tasks where simulation is difficult or costly, like,
1737680	1742720	you know, the real world, this kind of data requirement just is not going to be enough.
1742720	1746800	Luckily, I think that there are lots of promising avenues here. As I mentioned earlier,
1746800	1750960	model based reinforcement learning methods like efficient zero and dreamer are, I think,
1750960	1755200	on the order of magnitude astros there, I think they're about a hundred times more efficient
1755200	1759920	than something like musli. So that alone would already be a huge win if it turned out to be true
1759920	1764560	in this case. And then there are a lot of other really great methods for self supervised learning
1764560	1768960	in computer vision that I would imagine could also come in handy. Continuing learning could also
1768960	1773120	play a pretty big role here, especially when it comes to foundation model. The whole idea of
1773120	1777920	foundation models is that you train them once, and then you fine tune them for many other tasks
1777920	1782560	afterwards. But neural networks have a plasticity problem. The more you train them, the worse they
1782560	1786400	get at learning new tasks, which is obviously not ideal. You can imagine that this is something
1786400	1790240	we definitely want to avoid here. Maybe check out my video on continual backprop, if that's
1790240	1793840	something you're interested in. But overall, this is really great work. Kudos to the team for
1793840	1797520	getting this working. I'm incredibly excited to see where it goes. If you've enjoyed this,
1797520	1801280	consider subscribing for more dagos and more machine learning stuff. Thank you so much for
1801280	1803280	watching, and I hope to catch you next time.
