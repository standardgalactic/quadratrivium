1
00:00:00,000 --> 00:00:03,520
The title does not mislead you. This is indeed a 12-step plan.

2
00:00:03,520 --> 00:00:08,960
12 steps right here. You can count them 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, any with 12.

3
00:00:08,960 --> 00:00:12,960
That starts from essentially basic machine learning and ends up with the goal of

4
00:00:13,520 --> 00:00:16,560
basically the singularity, what they call intelligence amplification.

5
00:00:16,560 --> 00:00:19,760
And gosh, was this a fun read and interesting to think about.

6
00:00:19,760 --> 00:00:23,760
You might be wondering what random dude drank a bit too much one night and decided to write

7
00:00:23,760 --> 00:00:28,160
down their brilliant plan for this. But as it turns out, this is not written by some random dude.

8
00:00:28,160 --> 00:00:32,560
Rather, this is a new paper by Rich Sutton, Michael Bulling, and Patrick Polarski,

9
00:00:32,560 --> 00:00:36,640
who are all well-established researchers that head the Alberta branch of D-Mind and

10
00:00:36,640 --> 00:00:41,760
are also professors in the area. So yeah, I guess this is kind of unofficially a D-Mind paper.

11
00:00:41,760 --> 00:00:47,440
And not only is this just some plan, it is now the plan for research at the Alberta branch of

12
00:00:47,440 --> 00:00:52,400
D-Mind. Hence why it's called, you know, the Alberta plan. And it's just out there. It has some,

13
00:00:52,400 --> 00:00:56,320
I think, really interesting ideas and no one's been talking about it. So today,

14
00:00:56,320 --> 00:01:01,440
I hope to take you through the motivation for this and the plan itself. It is very interesting in

15
00:01:01,440 --> 00:01:06,080
the sense that it's building back up from the foundations of machine learning and reinforcement

16
00:01:06,080 --> 00:01:11,760
learning. And while it is very similar in most ways, it's also somewhat contrarian to many of

17
00:01:11,760 --> 00:01:16,880
the approaches, especially in reinforcement learning that are popular today. I love this because if

18
00:01:16,880 --> 00:01:21,600
you're a new graduate student or you can't find a research topic or you just want to get into research,

19
00:01:21,600 --> 00:01:26,800
well, here's 12 research topics handed to you on a silver platter. You really can't ask for much

20
00:01:26,800 --> 00:01:30,960
more. And it's also just, I personally think, interesting to know what some prominent researchers

21
00:01:30,960 --> 00:01:35,120
are thinking about and how they're going about approaching these problems that lots of us are

22
00:01:35,120 --> 00:01:39,600
very interested in. One last thing I'll mention before we dive into this is I cover lots of big

23
00:01:39,600 --> 00:01:43,440
ideas on this channel to keep you up to date with what's going on in the field, but also smaller

24
00:01:43,440 --> 00:01:48,320
ideas kind of like this kind of go under the radar to introduce you to some new interesting ideas. So

25
00:01:48,320 --> 00:01:51,680
if that's the type of thing that interests you, sort of subscribing to the channel, it means a lot

26
00:01:51,680 --> 00:01:55,600
and it really does help out. Anyway, let's dive into this. In this paper starts, we're going to

27
00:01:55,600 --> 00:01:58,960
come back the first two paragraphs in a second. I just want to start with the third paragraph

28
00:01:58,960 --> 00:02:02,560
because it really explains the core of what's going on here, what they're thinking. So starting

29
00:02:02,560 --> 00:02:08,080
from here, following the Alberta plan, we seek to understand and create long lived computational

30
00:02:08,080 --> 00:02:14,480
agents that interact with a vastly more complex world and come to predict and control their sensory

31
00:02:14,480 --> 00:02:19,920
input signals and blah, blah, blah. They are as scalable as possible. If you've seen Rich Sutton's

32
00:02:19,920 --> 00:02:23,200
better lesson, you know, maybe they're starting to look a little familiar and they have to adapt

33
00:02:23,200 --> 00:02:27,920
and change the complexity, which means they have to continually learn. Big focus on this here. And

34
00:02:27,920 --> 00:02:31,760
then another thing they mentioned is they must plan with a learned model of the world. Now I

35
00:02:31,760 --> 00:02:36,080
highlighted these specific things, except for the word follow, but I highlighted these specific

36
00:02:36,080 --> 00:02:41,040
things because these I think are things that are very core to this paper. So long lived, right,

37
00:02:41,120 --> 00:02:45,920
and continuing learning. These are things that some people in our work on, but are not super

38
00:02:45,920 --> 00:02:51,040
focused on that are really the sort of at the forefront of this paper. That means that agents

39
00:02:51,040 --> 00:02:55,200
are going to have to be able to work with certain challenges that are present in continuing learning.

40
00:02:55,200 --> 00:03:00,240
For example, if you try and take a neural network and have it continue to learn on a constantly

41
00:03:00,240 --> 00:03:05,600
changing task, many people don't realize this, but what actually happens is neural networks get worse

42
00:03:05,600 --> 00:03:09,600
and worse at performing or essentially learning new things, kind of like the, what do you say,

43
00:03:09,600 --> 00:03:14,160
an old dog can't learn new tricks. There's actually some concept of that in I think machine

44
00:03:14,160 --> 00:03:18,480
learning that lots of people are unfamiliar with. So things like that, those are issues that might

45
00:03:18,480 --> 00:03:23,840
be important in this situation. Anything vastly more complex world predicting and controlling

46
00:03:23,840 --> 00:03:28,160
sensory inputs. Some of these things are very like what you would assume is common, but you'll

47
00:03:28,160 --> 00:03:32,160
see that everything works under these assumptions in this paper. And we'll get into some of the

48
00:03:32,160 --> 00:03:36,880
more details of why I think some of these, you know, approaching these types of problems is

49
00:03:36,880 --> 00:03:41,200
interesting. The one thing that does stick out here a little bit is I think planning with

50
00:03:41,200 --> 00:03:44,960
a learning model of the world, because the other things are problems, right? Sort of having to

51
00:03:44,960 --> 00:03:48,720
be long lived and continuing to learn these, you know, having a vastly more complex world,

52
00:03:48,720 --> 00:03:52,720
these are statements about the problem. Whereas playing with a model is like a specific thing

53
00:03:52,720 --> 00:03:56,800
they do. So I guess they just think it's that important, which I would agree. But anyway,

54
00:03:56,800 --> 00:04:01,040
just some details about, you know, sort of the logistics of this plan before we maybe get into

55
00:04:01,040 --> 00:04:05,760
it. This is a five to 10 year plan, as they mentioned here. They also, you know, some of this is

56
00:04:05,760 --> 00:04:09,920
very like, you know, we think this will greatly affect the economy or society or individual

57
00:04:09,920 --> 00:04:14,960
lives. I don't think we need to go through all the, all the philosophical stuff. We can kind of get

58
00:04:14,960 --> 00:04:19,840
into the meat of this. So the purpose of giving this plan, which they write about here is, as they

59
00:04:19,840 --> 00:04:24,320
say, it's twofold. The main one is, or one of the two is that they want to sort of give their vision

60
00:04:24,320 --> 00:04:28,880
for AI. Well, lots of people have been working on these problems, like continue learning. They

61
00:04:28,880 --> 00:04:32,080
think it should really be at the forefront. So some of those things are a little bit different. So

62
00:04:32,080 --> 00:04:36,080
this is their vision, what problems they think should be focused on. And then also laying out,

63
00:04:36,080 --> 00:04:41,920
of course, a research plan for the purpose, you know, of doing research. This is not a to-do list.

64
00:04:41,920 --> 00:04:47,040
This is not like a things they completed. This is a research plan. They, they, as they very much

65
00:04:47,040 --> 00:04:51,200
stay here, gaps and uncertainties, which are to be expected, right? When you're tackling these

66
00:04:51,200 --> 00:04:56,560
big problems, when you have literally 12 steps, of course, it might not be enough. Maybe some

67
00:04:56,560 --> 00:04:59,760
things aren't going to be necessary. Things are going to change throughout this plan. There are

68
00:04:59,840 --> 00:05:05,040
some key points or some important things that they think sort of differentiates this Alberta plan

69
00:05:05,040 --> 00:05:09,520
from lots of the work going on at other places. So that's a little bit what they talk about in

70
00:05:09,520 --> 00:05:14,000
this next section. Ah, the other thing in this next section is this diagram that reminds me,

71
00:05:14,000 --> 00:05:19,120
I forgot to say what RL is. So this is going to be just a five second explanation. If you don't

72
00:05:19,120 --> 00:05:23,360
know what reinforcement learning is, just imagine you have some environment like the world, you have

73
00:05:23,360 --> 00:05:27,440
an agent, you know, it sees things from the environment, gets observations, takes actions,

74
00:05:27,440 --> 00:05:32,320
and it tries to maximize some reward. There's your five second explanation. Hopefully that keyed

75
00:05:32,320 --> 00:05:37,040
anyone in it wasn't okay. Anyway, what are these core tenants that they think differentiate the

76
00:05:37,040 --> 00:05:42,560
Alberta plan? So the first one, here it is, they say the first distinguishing feature is the emphasis

77
00:05:42,560 --> 00:05:49,120
on ordinary experience as described above. So an ordinary experience, as they'll mention here,

78
00:05:49,120 --> 00:05:53,840
this means that you can't do things like use specialized training sets. You can't use human,

79
00:05:53,840 --> 00:05:58,640
like assistance, you know, you can't put human data into this, you can't access the internal

80
00:05:58,640 --> 00:06:02,320
structure of the world, which lots of work and reinforcement learning does, right? Not everything,

81
00:06:02,320 --> 00:06:07,520
but lots of it does. And I should say, maybe clarify, human assistance here does not mean that

82
00:06:07,520 --> 00:06:13,280
an AI cannot interact with humans. It just means that once like the learning process has started,

83
00:06:13,280 --> 00:06:17,120
you can't have a human like tweaking like different parameters and like, you know, maybe

84
00:06:17,120 --> 00:06:21,200
training a bit like this and then a bit like this. It means that the learning process, you kind of

85
00:06:21,200 --> 00:06:26,880
have this agent, it starts out and then it has to do everything by itself. And why is this so

86
00:06:26,880 --> 00:06:31,360
important? And the idea here is that these types of methods that require human assistance, they

87
00:06:31,360 --> 00:06:36,720
usually don't scale, where do they say this, they typically do not scale with computational resources,

88
00:06:36,720 --> 00:06:41,680
and as such is not a research priority. Now, if you've read the bitter, the bitter lesson, that's

89
00:06:41,680 --> 00:06:46,080
basically the, you know, this is the one paragraph version of this. So what's the next thing that

90
00:06:46,080 --> 00:06:50,880
they put a focus on? The next thing is temporal uniformity. What does this mean? Essentially,

91
00:06:50,960 --> 00:06:55,440
it means that at all time steps, so at all times, in terms of the algorithms running,

92
00:06:55,440 --> 00:06:59,760
everything needs to be the same. You can't have, and this again, goes back to what I had mentioned

93
00:06:59,760 --> 00:07:05,280
earlier, you can't have a separate set of training data and testing data like you would have in

94
00:07:05,280 --> 00:07:08,960
supervised learning, or lots of people when they're evaluating reinforcement learning agents,

95
00:07:08,960 --> 00:07:13,920
like on the Atari 100k benchmark, I think it's pretty common to train with 100,000 steps of data,

96
00:07:13,920 --> 00:07:17,680
but then the actual testing is done on different data. So that would be a no, no here, right? And

97
00:07:17,680 --> 00:07:22,240
you might be like, oh, that's heresy, no training and testing, no separation. Well, yeah, but you

98
00:07:22,240 --> 00:07:26,080
have to remember, we're working in the continual learning setting, which means that you can just

99
00:07:26,080 --> 00:07:30,640
keep simulating your environment and getting new experiences. And well, if all those experiences

100
00:07:30,640 --> 00:07:35,200
are the same, well, then maybe your environment just isn't a very interesting environment at that

101
00:07:35,200 --> 00:07:38,400
point. So this is essentially talked about in the paper here, you know, they say no special

102
00:07:38,400 --> 00:07:44,800
training periods, if information is provided, then it has to be the same information on every

103
00:07:44,880 --> 00:07:50,560
time step. Or if it learns to plan, then it has to plan on every time step. If the agent constructs

104
00:07:50,560 --> 00:07:55,600
its own representations or subtasks, then the meta learned out are the meta algorithms for

105
00:07:55,600 --> 00:08:00,640
constructing them and operating them operate on every time step. So you might be wondering like,

106
00:08:00,640 --> 00:08:05,360
well, aren't on some time steps, maybe where things are more complicated, won't we maybe want to plan

107
00:08:05,360 --> 00:08:10,320
more like look further into the future? Yeah, maybe we'll want to do that. But the idea here is that

108
00:08:10,320 --> 00:08:15,600
should all be internal to the agent, as opposed for a human tweaking these parameters, right?

109
00:08:15,600 --> 00:08:20,240
You could have a meta learning algorithm that learns those types of things. And in general,

110
00:08:20,240 --> 00:08:25,360
I think this, why do they do this? I think they actually say it fairly well. Their focus on this

111
00:08:25,360 --> 00:08:31,520
is to leads us to interesting non stationary continuing environments, and in algorithms

112
00:08:31,520 --> 00:08:36,400
for continual learning and meta learning. So in one sense, I understand totally how this leads

113
00:08:36,480 --> 00:08:39,680
them. I have way too much yellow on this page, I'll start using something else.

114
00:08:40,560 --> 00:08:44,960
So algorithms for meta learning, this makes sense as I just mentioned, right? You need meta

115
00:08:44,960 --> 00:08:48,080
learning when you can't have a human tweaking these things, you should have an algorithm to

116
00:08:48,080 --> 00:08:52,560
that and meta learning is one way to do that. On the other hand, I don't see how this really leads

117
00:08:52,560 --> 00:08:56,720
to continuing environments or like continual learning, I would think it would be the other way

118
00:08:56,720 --> 00:09:03,120
around, right? You have continually like continual learning. And hence, you're going to want to focus

119
00:09:03,120 --> 00:09:06,720
on temporal uniformity, but maybe there's something I'm not seeing here. And another reason

120
00:09:06,720 --> 00:09:10,960
they mentioned this is that keeping everything temporarily uniform, it reduces the degrees of

121
00:09:10,960 --> 00:09:15,520
freedom and shrinks the agent design space, which to be fair, they think there's a decent point,

122
00:09:15,520 --> 00:09:21,840
really have plenty of things to look into. There's no reason to, you know, forcibly make that higher

123
00:09:21,840 --> 00:09:25,840
when we could simplify the space. Then if we go down a bit more, this is one thing we touched on

124
00:09:25,840 --> 00:09:32,160
earlier. And this is also from the bitter lesson. But the third distinguishing factor is it's

125
00:09:32,240 --> 00:09:38,160
cognizance of computational considerations and Moore's law. I do think lots of other, and this

126
00:09:38,160 --> 00:09:42,880
essentially, all this means, right, is compute computations getting stronger or more, we have

127
00:09:42,880 --> 00:09:47,920
more of it, it's more efficient. So we want to be able to take, make use of that. So we can't have

128
00:09:47,920 --> 00:09:51,840
anything that's not going to scale. And I think this has been getting better over the years,

129
00:09:51,840 --> 00:09:57,040
my personal opinion, it looks like lots of people are working in areas that do or on methods that

130
00:09:57,040 --> 00:10:01,840
do scale fairly well. Deep learning is one great example of this rate. And basically, everyone's

131
00:10:01,840 --> 00:10:07,760
using deep learning now. Oh, they even namedropped the bitter lesson. Okay, lovely. So then let's

132
00:10:07,760 --> 00:10:11,200
move on to the fourth. And I think this is the final one. The fourth distinguishing feature

133
00:10:11,200 --> 00:10:16,800
is the focus on the special case, or is it the special case in which the environment includes

134
00:10:16,800 --> 00:10:22,480
other intelligent agents. Now, what does this mean? It doesn't mean that there need to be other

135
00:10:22,480 --> 00:10:27,280
intelligent agents, but rather that intelligent agents. So like if I'm, you know, modeling a

136
00:10:27,280 --> 00:10:31,360
robot that's interacting with humans, you know, there's, there's nothing fancy that I do to model

137
00:10:31,440 --> 00:10:35,040
the fact that there's humans in the world, rather, they're just part of the environment.

138
00:10:35,040 --> 00:10:39,120
And I need to be cognizant, and I think they say this right here, you'd be cognizant that the

139
00:10:39,120 --> 00:10:42,800
environment may behave differently. And this might sound weird, but the environment is essentially,

140
00:10:42,800 --> 00:10:47,200
you can think of it as, you know, a living, breathing thing, or at least it could be in

141
00:10:47,200 --> 00:10:51,920
response to your own actions, right? So what you do might make the environment behave differently.

142
00:10:51,920 --> 00:10:55,520
And that's, you know, we shouldn't do anything special for these other agents. To be honest,

143
00:10:55,520 --> 00:10:58,320
I don't think they even need to put this here. Because I think this follows under the bullet

144
00:10:58,320 --> 00:11:03,680
point of using no human experience, right? The idea that we have other intelligent or like

145
00:11:03,680 --> 00:11:08,080
conscious agents, that is something that we're sort of embedding our human biases into this,

146
00:11:08,080 --> 00:11:13,760
right? Technically, we can't even prove other people are conscious. So I don't know, I don't

147
00:11:13,760 --> 00:11:18,560
think this is so weird, but I guess lots of places that do work with multi-agent settings, they do

148
00:11:18,560 --> 00:11:23,200
do this sort of thing where they're explicitly trying to model other agents. So that would be a

149
00:11:23,200 --> 00:11:27,040
no-no in this sort of setting, or at least that's, that's what they want to work with.

150
00:11:27,600 --> 00:11:31,840
So if we scroll down, we're almost to the research plan. I swear we're getting there.

151
00:11:31,840 --> 00:11:35,280
We're going to spend a good bit of time on that once we get there. But the last thing we need to

152
00:11:35,280 --> 00:11:40,560
touch on is the base agent. So I believe this is what they call the common model of the intelligent

153
00:11:40,560 --> 00:11:44,720
agent, as you're looking at right here. And they say that this is used in a few things, like

154
00:11:45,520 --> 00:11:50,560
areas like economics have similar ideas. But the idea is that we want to start from some base

155
00:11:50,560 --> 00:11:54,400
that we can maybe agree on, not that it's necessarily the best or something like that,

156
00:11:54,400 --> 00:11:59,920
but that this is a reasonable base that will inform the rest of the research. So what's

157
00:11:59,920 --> 00:12:02,880
essentially happening here is if we step through this, we start with an observation from the

158
00:12:02,880 --> 00:12:07,680
environment, pretty standard, then it goes into this perception. So perception, and this is going

159
00:12:07,680 --> 00:12:11,920
to be a recurrent thing, right? You also take in the last action, and you can see the output

160
00:12:11,920 --> 00:12:16,160
of perception feeds back into here. So this is recurrent. So this would be keeping what they

161
00:12:16,160 --> 00:12:20,480
like to call the agent state, which is like the agents, some people call this the belief state.

162
00:12:20,480 --> 00:12:25,360
It's maybe what the agent perceives and wants to remember about the world to do whatever it needs

163
00:12:25,360 --> 00:12:32,000
to do. It has reactive policies. So the policies are what transform these observations into actions.

164
00:12:32,000 --> 00:12:36,000
It has a value function. This is a normal value function in reinforcement learning, right?

165
00:12:36,800 --> 00:12:40,720
It essentially assigns value to certain states, which helps you do things like credit assignment,

166
00:12:40,720 --> 00:12:45,360
and it has a transition model, which allows you to do, as you can see, planning and planning

167
00:12:45,360 --> 00:12:49,360
means that you can more efficiently, you know, imagine scenarios in your head, not have to play

168
00:12:49,360 --> 00:12:53,920
through them in real life. And then that means that you can be a lot more sample efficient and

169
00:12:53,920 --> 00:12:58,400
potentially have other benefits too. One question, and this is a question I used to have, is why have

170
00:12:58,400 --> 00:13:03,120
these things? They are, they're one, two, three, four things. Why not other things, right? We could

171
00:13:03,120 --> 00:13:08,720
have, for example, a different portion here saying we want to model like other agents. I mean, we just

172
00:13:08,720 --> 00:13:12,560
said like this, this is not good, we shouldn't do this, but why not? Why are these other things

173
00:13:12,560 --> 00:13:17,440
okay to have? And the difference here, I don't think they mentioned the paper. But the difference,

174
00:13:17,440 --> 00:13:22,240
I believe they would say, is that these four things that they have here, they're not domain

175
00:13:22,240 --> 00:13:26,320
specific. These should essentially work in any domain, they're general, they're very general,

176
00:13:26,320 --> 00:13:30,320
whereas modeling other agents, well, you might not always need to do this in the way in which you

177
00:13:30,320 --> 00:13:34,320
might do this will probably change drastically, depending on what types of agents you're working

178
00:13:34,320 --> 00:13:38,720
with. So for that reason, these four things stay and other things don't quite make the cut. It has

179
00:13:38,720 --> 00:13:43,680
to do with the generality of these four components. Another thing you might mention is that the year,

180
00:13:43,760 --> 00:13:48,800
there are s's right here. So reactive policies, value functions, that is something we'll get into.

181
00:13:48,800 --> 00:13:52,960
These don't necessarily need to have one policy or one value function, we can work with multiple.

182
00:13:52,960 --> 00:13:58,000
And that is one of the steps of the plan that we'll get into. Speaking of the plan, we are finally

183
00:13:58,000 --> 00:14:03,680
about there. So here we go. A roadmap to an AI prototype. So here are 12 steps. I'm going to

184
00:14:03,680 --> 00:14:07,200
start off by just reading through them. And then they have bullet points for each of these that we

185
00:14:07,200 --> 00:14:11,280
can go through and talk about all of them and more depth. Some of them I'll go over somewhat

186
00:14:11,360 --> 00:14:15,120
quickly. Some of them I'll go over in a bit more depth because they have more written or I think

187
00:14:15,120 --> 00:14:18,960
they're more interesting. But anyway, starting off with item number one, we have continual

188
00:14:18,960 --> 00:14:24,560
supervised learning with given features, then two, supervised feature finding, three, continual

189
00:14:24,560 --> 00:14:28,160
generalized value function prediction learning. If you don't know what these words mean, don't

190
00:14:28,160 --> 00:14:32,880
worry, I'll go over all of it as we go through the individual points later. And four, continual

191
00:14:32,880 --> 00:14:39,520
active credit control, five, average reward, GVF learning, six, continuing control problems, seven,

192
00:14:39,600 --> 00:14:45,200
planning with average reward, eight, prototype AI one. Wow, incredible. One step model based

193
00:14:45,200 --> 00:14:50,960
around with continual function approximation, nine, search control and exploration, 10, the stomp

194
00:14:50,960 --> 00:14:57,280
progression, 11, oak and 12, intelligence amplification, or this is, I guess you could call

195
00:14:57,280 --> 00:15:02,160
this the singularity, though to be fair, I guess in making AIs make themselves better does not

196
00:15:02,160 --> 00:15:06,400
necessarily guarantee the singularity. So maybe that's a bit of a misnomer, but you know, it's

197
00:15:06,400 --> 00:15:12,400
a bit more catchy. It's more of a buzzword, I guess. So you might notice that looking over these

198
00:15:12,400 --> 00:15:18,080
points, Hey, this is just normal reinforcement learning. And to some extent, you wouldn't be

199
00:15:18,080 --> 00:15:22,720
entirely wrong. These are very common things that we see in reinforcement learning, like

200
00:15:22,720 --> 00:15:27,920
actor credit control. What's what's another thing model based around with continual function

201
00:15:27,920 --> 00:15:32,320
approximation, like what this is supposed to be a generally I that can do all these things. And

202
00:15:32,320 --> 00:15:36,320
that's what I meant when I was talking about at the beginning. These are very similar to what

203
00:15:36,320 --> 00:15:40,160
lots of people are doing today. But what you'll notice as we go through the points is that sort of

204
00:15:40,160 --> 00:15:44,720
the focus on the importance of what's important, that's what I think is a bit different, like the

205
00:15:44,720 --> 00:15:49,200
importance, for example, on continual RL. So we'll get into this, this, this, I should say, this

206
00:15:49,200 --> 00:15:54,160
isn't anything groundbreaking. It's more of just an interesting plan and an interesting way to go

207
00:15:54,160 --> 00:15:59,120
about this or to think about going about it this way. So let's start with our first big step,

208
00:15:59,120 --> 00:16:04,240
which is continual supervised learning with given features. So what let's start out with what this

209
00:16:04,240 --> 00:16:08,320
means. This essentially means given features. So maybe you have cart pull, and cart pull is a problem

210
00:16:08,320 --> 00:16:12,960
by the way, where you have this little cart, and it has a little pole on top of it. And the pole can

211
00:16:12,960 --> 00:16:17,280
go back and forth. And you need to move the cart back and forth to bounce the pole, very simple

212
00:16:17,280 --> 00:16:21,600
problem. So maybe for your features, you could have like the x, y values of this, and then the

213
00:16:21,600 --> 00:16:27,200
angular velocity of this cart, or something like that. So those would be given features. And to

214
00:16:27,200 --> 00:16:31,680
point out, you know, yes, we can already do things like this. But sort of the point I think of this

215
00:16:31,760 --> 00:16:37,200
plan is to really revisit things in the simplest setting. So they split the explicitly say this,

216
00:16:37,200 --> 00:16:42,320
we want to go back to the simplest setting, and essentially try and make everything as good as

217
00:16:42,320 --> 00:16:46,640
possible. Essentially try out everything that might have been overlooked, especially in these new

218
00:16:46,640 --> 00:16:51,440
settings in the setting of continual learning, and say, can we do better? So some things they

219
00:16:51,440 --> 00:16:56,000
mentioned that they might look into are how can we make things quicker? How can we train faster,

220
00:16:56,000 --> 00:17:00,560
be more robust, be more efficient, while also continuing again, over long periods of time,

221
00:17:00,560 --> 00:17:06,640
how can we do things like meta learn better representations? And that's be the most efficient.

222
00:17:06,640 --> 00:17:10,000
So these are all questions that that would be asked, although I guess meta learning better

223
00:17:10,000 --> 00:17:14,720
representations. I'm not sure if that's, I would think that would be in a different step. But,

224
00:17:14,720 --> 00:17:19,280
but anyway, what are some examples of some ways, you know, you could look into this, like, what are

225
00:17:19,280 --> 00:17:22,640
like, what can we really change when we have, you would think this is such a simple setting,

226
00:17:22,640 --> 00:17:25,920
there's not really much we can do, but there actually are some things we can look at. So one

227
00:17:25,920 --> 00:17:29,120
thing they mentioned is the global step size. So this is like your learning rate. Generally,

228
00:17:29,120 --> 00:17:32,960
you have a global learning rate. Now, this is somewhat, but certain optimizers, like Adam,

229
00:17:32,960 --> 00:17:36,640
that people have, some people have argued, I'm not sure, I don't know really the ins and outs of

230
00:17:36,640 --> 00:17:40,080
this, but they've argued that those sorts of optimizers, while they're really good for supervised

231
00:17:40,080 --> 00:17:44,000
learning, maybe aren't the best fit for reinforcement learning, because for reinforcement

232
00:17:44,000 --> 00:17:47,920
learning, maybe you want to have different learning rates, depending on how important certain

233
00:17:47,920 --> 00:17:53,120
features are for a certain task, or like what, you know, if you're trying to do different sorts

234
00:17:53,120 --> 00:17:56,640
of tasks, there's different things you could think of. But this is like one thing you could

235
00:17:56,640 --> 00:18:02,640
look at, right? Like, can you have a different step size for each single parameter? And how could

236
00:18:02,640 --> 00:18:06,720
you do that in a way that makes sense for continual reinforcement learning and not, you know,

237
00:18:06,720 --> 00:18:11,040
supervised learning, which is the context that most of these methods have been developed in.

238
00:18:11,040 --> 00:18:15,120
And of course, I think I'm not going to find it right now in this, this C of text, but they also

239
00:18:15,120 --> 00:18:19,200
mentioned that, you know, you don't want a human to be setting the learning rate. These are things

240
00:18:19,200 --> 00:18:24,400
that should be meta learned. Again, right? We don't want really too much human experience. We don't

241
00:18:24,480 --> 00:18:28,640
want that much fine tuning. We want the agent to be able to really do everything for itself. So

242
00:18:28,640 --> 00:18:33,280
these are all things that yes, people are looking into them. But the idea here, again, is go back

243
00:18:33,280 --> 00:18:38,080
to the simplest example, and get something, something working as best as possible. And then

244
00:18:38,080 --> 00:18:42,480
we'll start to scale up and make things more complicated. So some other things they mentioned,

245
00:18:42,480 --> 00:18:46,560
again, it's a bit too much here. Oh, here it is. So there's like normalizations of features, right?

246
00:18:46,560 --> 00:18:49,920
Like, of course, we know that you can normalize features. And that tends to be a good thing in

247
00:18:49,920 --> 00:18:53,760
machine learning. But when you're doing continual learning and reinforcement learning, does anything

248
00:18:53,760 --> 00:18:58,240
change? You know, these are all things that could be reconsidered in this slightly different context

249
00:18:58,240 --> 00:19:03,280
to maybe squeeze the most out of this that you can. So they go into this in a bit of depth.

250
00:19:03,280 --> 00:19:06,720
Honestly, I'm going to skip over this because I don't think it's particularly important.

251
00:19:06,720 --> 00:19:10,240
There's talking about examples of ways, you know, you can think about this, for example,

252
00:19:10,240 --> 00:19:14,160
like the meta learning per weight step parameter. That's something I just mentioned. I think they

253
00:19:14,160 --> 00:19:18,240
talk about this more in depth for this step, maybe because they already have some papers out about

254
00:19:18,240 --> 00:19:22,960
this. But anyway, let's get on to step two. So this is supervised feature finding. So before

255
00:19:22,960 --> 00:19:27,680
we had the features given to us, but now we actually want to find them, we want to generate

256
00:19:27,680 --> 00:19:32,640
new features, right? And features, right, these are just, we get some observation, we have our

257
00:19:32,640 --> 00:19:37,120
perception, remember from the base agent, now we want to use those to create new features to

258
00:19:37,120 --> 00:19:42,160
essentially use things that will serve representations that will help us do whatever tasks we're

259
00:19:42,160 --> 00:19:46,560
working on. Now, this is generally done via back propagation. And sorry, if you don't know what

260
00:19:46,560 --> 00:19:50,240
back prop is, but it's a bit too much to explain this video, there's plenty of good things. There's

261
00:19:50,240 --> 00:19:55,120
out there explaining it. And back prop, one issue actually with back prop is lots of people

262
00:19:55,120 --> 00:19:59,840
aren't aware of this. It doesn't work super well for continual learning. I mean, maybe not super

263
00:19:59,840 --> 00:20:04,800
well as in an overstatement, it does work. And it works fairly well, but it has an issue. It has

264
00:20:04,800 --> 00:20:10,000
an issue that the more you train, if you train on and on and on and on and you just keep going,

265
00:20:10,000 --> 00:20:14,320
it actually sort of does what they call sat and it gets saturated. I think they call like

266
00:20:14,400 --> 00:20:20,960
interridger saturation. Individual nodes get saturated, which means they stop essentially

267
00:20:20,960 --> 00:20:25,360
contributing. And over time, what you'll find is you'll see that maybe the performance of your

268
00:20:25,360 --> 00:20:29,200
agent, it starts off low, it goes up and it gets good and it kind of blows out. Well, if the task

269
00:20:29,200 --> 00:20:33,280
keeps changing, what you'll find is that the performance slowly starts to drop as the agent

270
00:20:33,280 --> 00:20:38,080
is unable to adapt because the network has essentially gotten saturated. Or at least,

271
00:20:38,080 --> 00:20:42,240
that's one explanation. I guess it's not fully explained yet. I actually explain this phenomenon

272
00:20:42,240 --> 00:20:47,200
as well with there's a method called continue backprop or CBP for short. And this is from

273
00:20:47,200 --> 00:20:51,760
Rich's lab, where they work on this problem. So this is one example of how supervised feature

274
00:20:51,760 --> 00:20:56,960
finding actually needs to be adjusted to the given scenario, which is continual learning.

275
00:20:56,960 --> 00:21:00,800
And this is one example of how although we have something that's really good already, we have

276
00:21:00,800 --> 00:21:05,920
back propagation, it might not be the best thing to use, at least not this exact instance we use

277
00:21:05,920 --> 00:21:10,560
of it right now, might not be the best thing for continual RL or something like that. One other

278
00:21:10,560 --> 00:21:14,880
thing that I think is really interesting, and I found this to be always very enticing, but

279
00:21:14,880 --> 00:21:20,080
sort of dangerous. I'm not dangerous and literally dangerous, but like dangerously enticing part of

280
00:21:20,080 --> 00:21:25,520
the work that like Rich's lab does is the way they think about modeling, like the typical modeling

281
00:21:25,520 --> 00:21:30,320
paradigm, their perspective is quite a bit different. So let me describe what I sort of think of as

282
00:21:30,320 --> 00:21:34,960
the typical paradigm. So maybe I'll do that on the left here. So I think the typical paradigm of,

283
00:21:34,960 --> 00:21:38,960
and this is not what all papers are about, but lots of them, one, you want to pick an architecture,

284
00:21:39,040 --> 00:21:41,520
right? You have some problems. So you want to pick the architecture you're going to use. This

285
00:21:41,520 --> 00:21:45,520
could be like a ResNet, right? You can use different ResNet layers. Maybe you want to use a

286
00:21:45,520 --> 00:21:50,400
transformer. Maybe you want to use some sort of self attention. There's lots of different

287
00:21:50,400 --> 00:21:54,880
architectures you could choose, right? You choose an architecture and then two, you choose an

288
00:21:54,880 --> 00:21:58,880
objective function. So this objective function is going to be like your loss. Like what are you

289
00:21:58,880 --> 00:22:02,960
trying to optimize here, especially in reinforcement learning, maybe this is usually going to be the

290
00:22:02,960 --> 00:22:07,760
reward or the value or whatever. But this could be a number of things. It could be the MSC with

291
00:22:08,160 --> 00:22:11,280
like an image. If you're trying to recreate an image or something like that. And then the third

292
00:22:11,280 --> 00:22:15,840
and last thing you do is you pick an optimizer. So 99% of the time, this is going to be like

293
00:22:15,840 --> 00:22:21,520
Adam or it's going to be RMS prop. But the thing is the way I find that within this paper, they talk

294
00:22:21,520 --> 00:22:25,680
about these sorts of things and reading other papers from the same people, they tend to think

295
00:22:25,680 --> 00:22:30,880
about these things a bit differently. Rather, they look at it from like a perspective of looking at

296
00:22:30,880 --> 00:22:35,600
individual neurons and the interaction between these neurons. And what do I mean by this? Because

297
00:22:35,680 --> 00:22:40,000
technically, like, you know, we're ending up using lots of the same things, but it is really just a

298
00:22:40,000 --> 00:22:44,080
difference in perspective. So for example, we can say, how can the utility be assigned to all the

299
00:22:44,080 --> 00:22:49,840
features that we're using? And then how could we make use of these utilities in the future, right?

300
00:22:49,840 --> 00:22:53,680
So this is like a very much we're looking at a neuron neuron basis, trying to find the utility,

301
00:22:53,680 --> 00:22:57,520
why might we want to do this sort of thing? Well, this could help us do things like evaluating

302
00:22:57,520 --> 00:23:02,400
existent features and discarding less promising features, so as to make room for new ones. Well,

303
00:23:02,400 --> 00:23:06,480
why don't we just keep expanding? Well, we have like a limited amount of computation here, right?

304
00:23:06,480 --> 00:23:10,880
Even though we are constantly getting more, this is where like the big world hypothesis, as they

305
00:23:10,880 --> 00:23:15,600
call it comes in, even though we have more and more compute, we also have just always a bigger

306
00:23:15,600 --> 00:23:19,200
world so that we'll never be able to match it. So we are going to have to forget things, right?

307
00:23:19,200 --> 00:23:23,360
So what do we forget? And that's kind of what this whole CBP thing I was just talking about.

308
00:23:23,360 --> 00:23:27,680
This does something very similar to this. But beyond just dropping features, there are also

309
00:23:27,680 --> 00:23:31,360
other things they talk about, you know, like initializing features. This is something you have

310
00:23:31,360 --> 00:23:34,560
to do. Normally, you do it when you create your own network, you initialize all your features,

311
00:23:34,560 --> 00:23:38,800
and then you just sort of train forever, right? But as it turns out, initialization, although

312
00:23:38,800 --> 00:23:42,480
it's kind of brushed to the side, it's actually much more important than you might think. Oftentimes,

313
00:23:42,480 --> 00:23:46,480
maybe not often, but occasionally, I'll implement a paper. And I've had this happen to me a couple

314
00:23:46,480 --> 00:23:50,160
of times, we'll implement it, and it won't work. And I'll go back and read the paper very carefully

315
00:23:50,160 --> 00:23:54,240
and realize, oh, they use this very specific form of initialization for their network,

316
00:23:54,240 --> 00:23:58,240
and they try it, and suddenly everything's working. This has happened to me, especially

317
00:23:58,240 --> 00:24:02,480
in papers that don't use backprop, but like use biologically inspired training. I found that

318
00:24:02,480 --> 00:24:06,320
this is actually more important than you might expect. Another thing to talk about is like,

319
00:24:06,320 --> 00:24:11,360
how should you adjust neurons or weights? One way to do this is obviously backprop. And I don't

320
00:24:11,360 --> 00:24:15,040
think they're, they're definitely not proposing we just get rid of backprop. I think they realize

321
00:24:15,040 --> 00:24:20,720
backprop scales very well. It works very well, but it might not be perfectly suited for a problem.

322
00:24:20,720 --> 00:24:25,120
So how can we look at this sort of more, I don't know if I want to call like an individual neuron

323
00:24:25,120 --> 00:24:28,800
approach, but something like that, more than neuron to neuron level, looking at them as features,

324
00:24:28,800 --> 00:24:33,760
features with utilities and features that we want to drop or get rid of. How can we implement like

325
00:24:33,760 --> 00:24:38,080
all these sorts of things? Like what, what would this look like in, in play? So maybe one example

326
00:24:38,080 --> 00:24:44,000
of this, right? And this is very similar to what CBP does is say one, we would do an initialization

327
00:24:44,000 --> 00:24:48,000
just as normal. We in it all our, our nodes, but this is one thing we could look into what are

328
00:24:48,000 --> 00:24:51,760
better ways to in it for these types of things. The second thing we might want to do is use,

329
00:24:51,760 --> 00:24:55,520
use backprop, right? We don't need to get rid of backprop. Backprop is strong, but as we use

330
00:24:55,520 --> 00:25:00,880
backprop, maybe then we evaluate the utility. And then four, we remove, or maybe I should just say

331
00:25:00,880 --> 00:25:05,040
we drop the nodes that are not super useful. And then the fifth thing we could do is we could

332
00:25:05,040 --> 00:25:10,000
reinit, but instead of just re-init normally, maybe we could have some sort of meta learning

333
00:25:10,000 --> 00:25:14,560
that figures out how to initialize nodes such that things will be learned faster, right?

334
00:25:14,560 --> 00:25:18,160
So there's, there's some sort of meta stuff you could do here. So this would be one example.

335
00:25:18,160 --> 00:25:23,120
And again, this is very similar. That's done what's done in CBP. I'll link my video to this

336
00:25:23,120 --> 00:25:27,040
in the description. Actually, if you're interested in checking out, but this is like one alternative,

337
00:25:27,040 --> 00:25:30,800
right? That still makes use of backprop, but might be better suited for something like

338
00:25:30,800 --> 00:25:35,120
continue learning or reinforcement learning. And could give you more control over how you

339
00:25:35,120 --> 00:25:40,320
go about developing these algorithms. As I just mentioned, I think this is a very exciting way

340
00:25:40,320 --> 00:25:45,360
to think about how to model things, thinking of individual features. It's almost less limiting

341
00:25:45,360 --> 00:25:49,760
the standard approach that I read out right here. But at the same time, it is dangerous because let's,

342
00:25:49,760 --> 00:25:55,280
let's not kid ourselves, back propagation works very well. It scales very well. So when you do

343
00:25:55,280 --> 00:25:58,640
want to do something different, you're going up against something that's already very well

344
00:25:58,640 --> 00:26:03,200
established and is known to work very well. So that's why I say it's kind of dangerous. It's,

345
00:26:03,200 --> 00:26:08,640
it's exciting, but hard to get working and not very explorative. It's a very underexplored area,

346
00:26:08,640 --> 00:26:13,760
I think. Okay, step number three, continue GVF prediction learning. This is where we start getting

347
00:26:13,760 --> 00:26:18,800
into non-IID data. So that's independent, individually distributed data. So when we're

348
00:26:18,800 --> 00:26:23,680
working with reinforcement learning or in the real world, we are often, we're going to be working

349
00:26:23,680 --> 00:26:28,320
with streams of data that come, you know, in, in a row. But one thing to note is that many

350
00:26:28,320 --> 00:26:35,520
machine learning methods, the theory is entirely based upon data that is IID. Or in other words,

351
00:26:35,520 --> 00:26:39,600
we are essentially breaking that when we move on to reinforcement learning most of the time.

352
00:26:39,600 --> 00:26:42,880
That's why people like to use, or one of the reasons people like to use things like replay

353
00:26:42,880 --> 00:26:47,840
buffers is because it helps you get this IID distribution, which can help you learn. Now,

354
00:26:47,840 --> 00:26:52,240
we have methods of somewhat counteracting this, but it is nevertheless still an issue. But one

355
00:26:52,240 --> 00:26:56,640
thing they also put in here is GVFs. I honestly don't know why they put this in here, but we can

356
00:26:56,640 --> 00:27:00,880
talk about it because they do. I just don't know why they put them together. So GVFs, this is

357
00:27:00,880 --> 00:27:05,200
generalized value function. And this is a fairly simple idea. If you're familiar with normal value

358
00:27:05,200 --> 00:27:10,000
functions, essentially a value function measures like how good a state is, like what the expected

359
00:27:10,080 --> 00:27:14,400
reward is. Whereas a generalized value function, well, it's essentially just a value function

360
00:27:14,400 --> 00:27:18,720
for something other than the reward, or it could be the reward, or any other feature, right? So

361
00:27:18,720 --> 00:27:23,280
you're predicting something. So they're just predictions or predictions about the world

362
00:27:23,280 --> 00:27:28,160
that are not reward based, not necessarily reward based. The last thing I'll mention about this

363
00:27:28,160 --> 00:27:32,320
step three is this is where perception comes in. And I think the reason they say this is where

364
00:27:32,320 --> 00:27:36,720
perception really starts to come in is because of the non IID data. That means if we have like

365
00:27:36,720 --> 00:27:40,560
sequential data, and we need to remember stuff that happened in the past, well, part of our

366
00:27:40,560 --> 00:27:44,000
perception needs to be remembering the important thing we need to know what's important, like if

367
00:27:44,000 --> 00:27:48,480
there's a key over there that I can't see anymore, I need to remember it's there so I can, you know,

368
00:27:48,480 --> 00:27:52,080
go back and get it if I see a locked door or something like that. The fourth thing we've been

369
00:27:52,080 --> 00:27:57,520
on now is continual actor critic control. And that is right up until this step. You might not

370
00:27:57,520 --> 00:28:01,520
have noticed there was no control involved. We're not actually interacting with the environment,

371
00:28:01,520 --> 00:28:05,200
but rather up to this point, we were just predicting what was happening. There's really

372
00:28:05,200 --> 00:28:08,880
not much more to say here. Now that we're bringing control in the mix, we need to figure out how to

373
00:28:08,880 --> 00:28:14,480
get that working with everything we've done so far. So then step five, average reward GVF learning.

374
00:28:14,480 --> 00:28:19,120
Now, average reward is I think something lots of people are not very familiar with. Understandably,

375
00:28:19,120 --> 00:28:25,920
so it's it's not very popular, I guess you could say. And if you've never thought about it,

376
00:28:25,920 --> 00:28:29,520
using something like a discount factor and for those that aren't familiar with reinforcement

377
00:28:29,520 --> 00:28:34,320
learning, usually you have this gamma parameter. And this is called the discount factor. And

378
00:28:34,320 --> 00:28:39,600
essentially it weighs how important current rewards are versus future rewards. And it's,

379
00:28:39,600 --> 00:28:42,720
if you think about it, it's kind of weird. The other thing that we could do, as you see right

380
00:28:42,720 --> 00:28:46,720
here is just have an average reward and say we want to maximize the average reward, which would

381
00:28:46,720 --> 00:28:51,440
essentially be in the same as saying we just want to maximize the reward total over everything.

382
00:28:51,440 --> 00:28:55,440
The thing is, when you do something like having gamma, you're really, I actually don't want to

383
00:28:55,440 --> 00:29:00,320
get into this too much. But if you've ever thought about it, it's kind of janky, right? And the argument

384
00:29:00,320 --> 00:29:04,640
for why to use average reward instead is not just the jankiness, but I don't want to get into

385
00:29:04,640 --> 00:29:08,000
it too much. So if you're interested, there is a whole section on this in the RL book that you

386
00:29:08,000 --> 00:29:13,520
can check out. So step six, and this is going to be kind of cut us off at the first half of this

387
00:29:13,520 --> 00:29:19,280
plan is the continuing control problems. Essentially, all this is going to say is we've

388
00:29:19,280 --> 00:29:25,200
essentially created up until this point, a, a model free RL agent. So essentially the point

389
00:29:25,200 --> 00:29:29,600
for this step is really, we just want to combine everything we've learned of the average reward

390
00:29:29,600 --> 00:29:33,760
stuff, the generating new feature stuff. And we want to try it out in a bunch of continuing

391
00:29:33,760 --> 00:29:38,320
environments, maybe make some more like new continuing environments, because most environments

392
00:29:38,320 --> 00:29:42,640
are episodic, like the open AI gym stuff they mentioned, it's mostly episodic, but you could

393
00:29:42,640 --> 00:29:47,760
convert it to continuing versions. And then we want to try the combination of everything we've

394
00:29:47,760 --> 00:29:53,760
worked for so far and see how we're stacking up. Because remember, this is not like a one shot plan,

395
00:29:53,760 --> 00:29:58,240
it will need to be adjusted as you go. So this would be a good point maybe to revise,

396
00:29:58,240 --> 00:30:02,640
see how things are doing, a workout, anything that's maybe not working as expected. So then

397
00:30:02,640 --> 00:30:06,240
when we're not going to jump just to the seventh step, this, this is kind of wraps up the first

398
00:30:06,240 --> 00:30:10,880
part of this. Once we have this done, we have a, I love how they say it, a more continual, true,

399
00:30:10,880 --> 00:30:16,560
it's a more continual model free learning method, which is good. We have like our first leg base.

400
00:30:16,560 --> 00:30:20,400
This is where I guess after this point, we can say we have something good now. Now it's time to

401
00:30:20,400 --> 00:30:26,480
start making it better. So what they focus on from here is primarily model based RL and things

402
00:30:26,480 --> 00:30:31,360
start to get a little bit more complicated. Ideas, I will say from the seventh step onward,

403
00:30:31,360 --> 00:30:35,600
tend to also be a little bit higher level without as many details, which I think is because, you

404
00:30:35,600 --> 00:30:38,720
know, they're later in the plan less certain about how these things will go because they're

405
00:30:38,720 --> 00:30:45,040
further out. So step seven is planning with average reward. So planning is very much done

406
00:30:45,040 --> 00:30:50,960
in RL today. And there are lots of great examples of this. For example, you can look at Mu zero.

407
00:30:50,960 --> 00:30:54,880
Mu zero is a great example of planning where the results are pretty incredible. There are some

408
00:30:54,880 --> 00:30:58,800
differences here, right? It will again be in the continual setting will want to do this with

409
00:30:58,800 --> 00:31:04,320
average reward. And I can say just right now, there are a lot of problems in planning that are

410
00:31:04,320 --> 00:31:08,640
still unsolved. And it's very like not clear how to do things like what's what's the best way of

411
00:31:08,640 --> 00:31:13,760
going about things. You also have like other alternatives to Mu zero, like dreamer dreamer is

412
00:31:13,760 --> 00:31:17,440
dreamer v2 also works very well, you know, what's what's the best way to go about planning in this

413
00:31:17,440 --> 00:31:23,360
setting? Who knows. So then step eight is prototype AI one. I love that. I got to love that prototype

414
00:31:23,440 --> 00:31:28,000
AI one one step model based RL with continual function approximation. So this is actually

415
00:31:28,000 --> 00:31:33,360
learning a model now, right? You can do planning if you have a pre given model, which I think is

416
00:31:33,360 --> 00:31:37,520
what they entailed for step seven. But now that we have model based RL, that means we're going to

417
00:31:37,520 --> 00:31:42,480
want to learn a model and then maybe do some sort of planning or something like that within that

418
00:31:42,480 --> 00:31:46,880
model. So a one step planning model, they actually give a few steps down here that we can talk about.

419
00:31:46,880 --> 00:31:51,520
So some things they want it to include. So one is a recursive update function or a perception

420
00:31:51,520 --> 00:31:56,320
process. So right, we had that and this essentially makes everything more efficient. Instead of having

421
00:31:56,320 --> 00:32:01,760
to like relearn the perception and the value function, the world model, the policy can have one

422
00:32:01,760 --> 00:32:06,960
shared perception function that learns some sort of useful representation and knows how to store

423
00:32:06,960 --> 00:32:11,920
memory of things that are important and that sort of thing. We also need a one step environment

424
00:32:11,920 --> 00:32:16,960
model. They say this presumably be an expectation model or sample model or something in between.

425
00:32:16,960 --> 00:32:20,960
I'm pretty sure it would not end up being an expectation model. This is I'm saying this because

426
00:32:20,960 --> 00:32:25,120
this is what I do in my own work and expectation models are kind of awful. They're really easy

427
00:32:25,120 --> 00:32:29,920
to learn, but they're they're not very good. So sample models are one thing they don't mention

428
00:32:29,920 --> 00:32:34,160
a distribution model here. But I think that's another possibility, although there's no reason

429
00:32:34,160 --> 00:32:37,680
to think a sample model couldn't also work. So and by the way, if you're unfamiliar with this,

430
00:32:37,680 --> 00:32:41,680
this is just the idea that we want to predict the next step, what's going to happen next, right?

431
00:32:41,680 --> 00:32:45,600
Because if we can predict what's going to happen next, then we can update based on our, you know,

432
00:32:45,600 --> 00:32:49,840
sort of visions in our head, like our it's kind of like humans. I think when you go to sleep,

433
00:32:49,840 --> 00:32:54,240
something like this happens, right? You get replayed memories and those can help you learn. So

434
00:32:54,240 --> 00:32:59,680
so that's kind of what happens with world modeling and planning. What else is happening? So feature

435
00:32:59,680 --> 00:33:04,640
finding as in step two, then importance feedback from the model. So this is now essentially tying

436
00:33:04,640 --> 00:33:09,440
steps to and this planning together, right? If we're doing plain, we should be able to use that

437
00:33:09,440 --> 00:33:13,440
plane to go back and change our actual features or how we're learning our features and improve them

438
00:33:13,440 --> 00:33:18,160
in some way. The other idea, sorry, not other idea, but the final step here is a ranking of

439
00:33:18,160 --> 00:33:22,560
features used for both feature finding and to determine which features are included in the

440
00:33:22,560 --> 00:33:28,160
environment model. Since step D is essentially ranking features for feature finding, determining

441
00:33:28,160 --> 00:33:32,000
what features are used in the environment model, because we might not need to use everything. If

442
00:33:32,000 --> 00:33:35,840
you've looked at like what's been going on with this recently, instead of actually trying to predict

443
00:33:35,840 --> 00:33:39,440
the future, lots of people are doing what's it called value equivalent models, where it doesn't

444
00:33:39,440 --> 00:33:42,720
actually matter what the model predicts, so long as it's getting the right value. So it might be

445
00:33:42,720 --> 00:33:47,440
predicting, you know, it might be leaving things out that are not important or stuff like that.

446
00:33:47,440 --> 00:33:51,760
And then an influence of model learning and planning on feature ranking. So I think they

447
00:33:51,760 --> 00:33:56,720
just kind of stated this. But anyway, I don't want to go maybe through these individual points.

448
00:33:56,720 --> 00:34:00,720
They're kind of weird. Maybe tie in all these together. What are they talking about, especially

449
00:34:00,720 --> 00:34:05,280
in these later few points? And the idea for this step eight is I think really that they want to

450
00:34:05,280 --> 00:34:12,160
bring a full integration circle between the policy value function and the model components, right?

451
00:34:12,160 --> 00:34:16,080
The idea that whenever you're doing any one of these things, when you're learning a value function,

452
00:34:16,080 --> 00:34:20,720
when you're learning how to plan, the planning should essentially help you make better features,

453
00:34:20,720 --> 00:34:24,960
which should then in turn help you make a better model. So all these different components, and

454
00:34:24,960 --> 00:34:29,040
there's too many, there would be too many errors to draw this out in the like common model we saw

455
00:34:29,040 --> 00:34:32,640
before. But the idea is that all these different parts should be affecting each other. And I do

456
00:34:32,640 --> 00:34:37,840
think that this is one interesting thing that is not like too out there at all, but it's one thing

457
00:34:37,840 --> 00:34:43,360
that is not fully done. Now in things like Mu zero, what they essentially do is they they have their

458
00:34:43,360 --> 00:34:47,760
input state, they pass this through like a representation function. So they get there,

459
00:34:47,760 --> 00:34:52,160
maybe I should call this the observation, this is like the state, then they do like the planning.

460
00:34:52,160 --> 00:34:56,640
So this is called like the dynamics model to get the next state. And then from here, you go up and

461
00:34:56,640 --> 00:35:01,440
you predict the policy and the value function. And then, and they do some Monte Carlo tree. So it

462
00:35:01,440 --> 00:35:06,080
gets very complicated, right? But you have this whole system, and it's trained from end to end.

463
00:35:06,080 --> 00:35:12,480
So when you actually train things go back like this, and you sort of update everything in one go.

464
00:35:12,480 --> 00:35:16,640
So this is one type of feedback, but it's far from the only type of feedback that could be used,

465
00:35:16,640 --> 00:35:20,400
right? There could be other types of feedback between these different components. Maybe again,

466
00:35:20,400 --> 00:35:24,560
these are all things that could potentially be meta learned to. And those other types of

467
00:35:24,560 --> 00:35:28,720
feedbacks might help these systems be more efficient, which would certainly always be great.

468
00:35:28,720 --> 00:35:34,640
We head down to step nine, we have search control, and exploration, just to sum this up this,

469
00:35:34,640 --> 00:35:38,800
there's not too much in this. But essentially, the idea here is that we want to make things

470
00:35:38,800 --> 00:35:43,200
more efficient and a bit better when we're doing search. So search would be done in something

471
00:35:43,200 --> 00:35:47,360
like planning, right? We're trying to look at the future, see what could happen. We want to see,

472
00:35:47,360 --> 00:35:50,560
like, you know, what, if we take this action, what are all the different things that could happen?

473
00:35:51,280 --> 00:35:56,560
Or also when we're updating over, like, you know, maybe we want to explore in a certain way that

474
00:35:56,560 --> 00:36:00,800
helps us learn what we're missing, like fill out the gaps in our knowledge. So different ways you

475
00:36:00,800 --> 00:36:04,560
could, you know, different things you could look into or like prioritize sweeping is one,

476
00:36:04,560 --> 00:36:07,920
that's where you, I guess, like look at certain states where you have the least

477
00:36:07,920 --> 00:36:11,120
knowledge of what will happen. But there's also a difference between, like, you know,

478
00:36:11,120 --> 00:36:14,800
instead of using Monte Carlo tree search, there's also different types of heuristic search. Now,

479
00:36:14,800 --> 00:36:19,360
I don't think they would just use heuristic search, because that seems like it uses a bit too much

480
00:36:19,360 --> 00:36:23,840
human experience. But perhaps the model could, could meta learn, I say meta learn too much,

481
00:36:23,840 --> 00:36:27,840
it could learn some heuristics for this, right? And that would be another way to go about planning.

482
00:36:27,840 --> 00:36:33,280
So that's what that is all about. And next is the stump progression. So stump, I believe,

483
00:36:33,360 --> 00:36:39,440
do they write it here? Yes, they do. So it stands for subtask, option, model, and planning. We've

484
00:36:39,440 --> 00:36:42,640
already talked about most of these, you can probably infer what a subtask is, right? It's the

485
00:36:42,640 --> 00:36:47,600
idea that you might want to have other tasks, other than the main task, the agent can, can work on.

486
00:36:47,600 --> 00:36:52,240
And of course, the agent would learn these tasks themselves, as opposed to like a human, a human

487
00:36:52,240 --> 00:36:56,720
doing these. So this is like one, one differentiation, one point they mentioned earlier, right? We don't

488
00:36:56,720 --> 00:37:02,000
want, and this is what a lot of people do now is if they want to learn like skills. So a skill is

489
00:37:02,560 --> 00:37:06,480
maybe, I don't know, you're going to drive a car and one skill is like turning the handle left

490
00:37:06,480 --> 00:37:09,680
or right. You have to move a lot of muscles in your arms. So if you have the steering wheel,

491
00:37:09,680 --> 00:37:14,240
how does, how do these normally look? I don't think this is close enough. If you want to move this

492
00:37:14,240 --> 00:37:18,000
left and right, you actually have to move a lot of muscles in your arm to get that to work, right?

493
00:37:18,000 --> 00:37:22,000
But for humans, it's very easy, because we've, we essentially know how to move our arms in

494
00:37:22,000 --> 00:37:27,760
certain ways. We have skills or options are the more general form of this. That's when option is

495
00:37:27,760 --> 00:37:31,760
in a subtask. If I'm learning how to live a good life, learning to drive is going to be one

496
00:37:31,760 --> 00:37:37,120
subtask. So that's important for me to master, right? So how can the agent pose its own subtasks

497
00:37:37,120 --> 00:37:41,520
and how can it learn options that solve those subtasks? And then how can we combine those?

498
00:37:41,520 --> 00:37:44,880
Or sorry, I'm getting a bit ahead of myself. Combining them is the next step. What they

499
00:37:44,880 --> 00:37:48,800
essentially propose, I'm not sure, I forget if they propose it here, but the idea is that they

500
00:37:48,800 --> 00:37:53,520
want to have GVFs. So they want to be predicting things or have certain features, right? So feature

501
00:37:53,520 --> 00:37:58,080
one, feature two, feature three, that tells things about the environment. And then they want to pose

502
00:37:58,080 --> 00:38:03,280
GVFs. So being able to predict these things and also control them. So say, how can we maximize

503
00:38:03,280 --> 00:38:07,280
feature one? How can we maximize feature two or feature three, and then learn options for each

504
00:38:07,280 --> 00:38:11,920
of those. So essentially what you're doing is you're learning how to control your environment

505
00:38:11,920 --> 00:38:16,000
in ways that are not just trying to maximize reward. And then maybe these options that you've

506
00:38:16,000 --> 00:38:20,640
learned could be reused later. They actually have a paper on this called, I think, Stomp or

507
00:38:20,640 --> 00:38:25,520
something like that. So if you're interested in that, I do encourage you to check that out. So then

508
00:38:25,520 --> 00:38:32,080
step 11 is oak. So oak is, it stands for another thing. I get, I'm forgetting, I think options,

509
00:38:32,080 --> 00:38:36,080
action knowledge or something like that. That might not be exactly right here. They actually

510
00:38:36,080 --> 00:38:40,720
introduce option keyboard. Now I'm surprised they mentioned this because the other steps don't

511
00:38:40,720 --> 00:38:45,360
really go in depth. Whereas the options keyboard is a very specific thing. There's a paper out

512
00:38:45,360 --> 00:38:49,280
about it. They link to it. I think it's a really interesting paper. And the idea is that let's

513
00:38:49,280 --> 00:38:54,640
say you have a set of options. So maybe option number one knows how to fish. And option number

514
00:38:54,640 --> 00:38:59,840
two knows how to use a computer. Now, I don't know why you would ever want to do these two things

515
00:38:59,840 --> 00:39:04,960
at, at the same time, but I know I hate fishing and I find it incredibly boring. So if I was going

516
00:39:04,960 --> 00:39:09,360
to fish and I had a computer, well, honestly, I'd probably enjoy the nature, but you know, you could

517
00:39:09,360 --> 00:39:14,240
do both at one time, right? There's no reason to say you can only do one fishing or using your

518
00:39:14,240 --> 00:39:19,040
computer. Choose you could do both at the same time. If you want to weather or not, it's a good

519
00:39:19,040 --> 00:39:23,520
idea. So and that's the idea of an option keyboard where you can essentially specify how much you

520
00:39:23,600 --> 00:39:27,760
want to do this option or that option. And instead of having to learn a billion different

521
00:39:27,760 --> 00:39:32,320
options to do all the different things you do, well, if you learn a good set of base options,

522
00:39:32,320 --> 00:39:37,040
now you can combine them and get massively more expressive options that are just combinations

523
00:39:37,040 --> 00:39:41,280
of others instead of having to learn those explicitly. And then we get to the last step,

524
00:39:41,280 --> 00:39:46,640
intelligence amplification on how far we've come. So intelligence amplification is I think what most

525
00:39:46,640 --> 00:39:51,520
people think of when they think of the singularity. It's essentially the idea that now we have our

526
00:39:51,600 --> 00:39:58,240
prototype AI number two, and it should be able to do things very well. They describe this in

527
00:39:58,240 --> 00:40:02,720
some interesting ways. So like there's an exo cerebellum, which is, you know, they talk about

528
00:40:02,720 --> 00:40:05,760
these things, but essentially the core of what they're getting at is really at the end here,

529
00:40:05,760 --> 00:40:10,160
where it says an intelligent application agent to perform policies and use planning to

530
00:40:11,840 --> 00:40:17,360
multiplicatively enhance the intelligence of another partner agent or part of a single agent,

531
00:40:17,440 --> 00:40:21,680
or I guess they could also change themselves, right? So we see these two versions being studied

532
00:40:21,680 --> 00:40:27,040
in both human agents and agent to agent interaction settings. So essentially the idea that you have

533
00:40:27,040 --> 00:40:33,440
this one agent right here, and then you have agent, agent two right here. And maybe agent two is

534
00:40:33,440 --> 00:40:39,360
as much bigger brain, much smarter, you know, great brain drawing for me. So what agent two can do

535
00:40:39,360 --> 00:40:43,440
is it can go to agent one and say, I'm gonna make you smarter. And then it makes agent one

536
00:40:43,440 --> 00:40:47,840
better because remember it's a machine, it can edit its code. And then agent one is like, oh,

537
00:40:47,840 --> 00:40:52,320
thanks for making me smarter. Now I'm going to go make you smarter. Or of course, maybe you could

538
00:40:52,320 --> 00:40:56,720
just have a single agent, reperforming this on itself, or maybe there's some risk there because

539
00:40:56,720 --> 00:41:01,040
it could mess itself up. Maybe that's why they talk about having two different agents. But anyway,

540
00:41:01,040 --> 00:41:05,280
I mean, this is the idea, right, of how you get better and better agents. At some point,

541
00:41:05,280 --> 00:41:09,280
you have an agent that is just better than humans at producing these sorts of, you know,

542
00:41:09,280 --> 00:41:15,600
AI agents. And that's when we can get this sort of multiplicative scaling. Now, I think if you're

543
00:41:15,600 --> 00:41:20,320
like me, you might be thinking, wait, wait, step 11 was talking about an options keeper. And in

544
00:41:20,320 --> 00:41:27,440
step 12, we're talking about the singularity. Yeah, it's a bit of a jump, right? It's a bit weird.

545
00:41:28,000 --> 00:41:33,280
But that's one things I'm going to be honest, I find interesting, but I'm uncertain about in this

546
00:41:33,280 --> 00:41:39,520
paper. The fact that everything they outline up to step 11, for the most part, is within very

547
00:41:39,520 --> 00:41:44,640
reasonable expectations. It's like what you would expect, but with a different focus. What I find

548
00:41:44,640 --> 00:41:51,120
interesting is that they think that we can go from step 11 to step 12. I'm not sure how much

549
00:41:51,120 --> 00:41:55,840
effort they think it will take, but essentially, that they think not much will be missing at that

550
00:41:55,840 --> 00:42:01,120
point. And it is interesting to think about. On one hand, I'm very inclined to say, no, of course,

551
00:42:01,200 --> 00:42:04,080
that's not going to be enough. We already have most of the things they talked about in these

552
00:42:04,080 --> 00:42:08,240
previous steps. But on the other hand, we don't actually have the things they mentioned in these

553
00:42:08,240 --> 00:42:13,840
previous steps. We have usually, for lots of these, we have specific instances, right? So for

554
00:42:13,840 --> 00:42:18,320
like planning, we have something like Mu zero, but we still have so much more planning to explore.

555
00:42:18,320 --> 00:42:23,360
There's so many different things we could try. And if everything below the planning level, like if

556
00:42:23,360 --> 00:42:27,680
if we have incredibly good ways to learn representations, and if we have incredibly

557
00:42:27,680 --> 00:42:32,720
more efficient ways to train your own networks for continuing learning problems, maybe things will

558
00:42:32,720 --> 00:42:38,640
go a lot better. It's really hard to say. And to be honest, I would cut the authors of this

559
00:42:38,640 --> 00:42:41,760
some slack. I don't think, you know, they don't think that they're just going to go through this

560
00:42:41,760 --> 00:42:45,600
plan and suddenly hit step 12 and everything's going to work out. I think they're probably,

561
00:42:45,600 --> 00:42:48,640
they probably realize, you know, they have to revise this. I think they even mentioned, yeah,

562
00:42:48,640 --> 00:42:53,200
this is provisional. Oh, not crossing this out. It's not not provisional. It is provisional.

563
00:42:53,280 --> 00:42:58,320
It's a draft and a working plan. It's going to be revised. But I do think it's interesting that

564
00:42:58,320 --> 00:43:01,920
someone like Rich Sutton, who's worked in the field for a long time, has had some really good

565
00:43:01,920 --> 00:43:07,200
ideas, thinks that this will be enough. And to be honest, I mean, I don't think I could come up

566
00:43:07,200 --> 00:43:11,200
with a better plan myself. And I'm not sure quite what's missing here. I guess what's missing are

567
00:43:11,200 --> 00:43:15,840
obviously the details that you have to fill out, right? Like meta learning, there's a million

568
00:43:15,840 --> 00:43:19,280
bazillion ways to do meta learning. How are they going to do it? What's the way to, I don't know,

569
00:43:19,280 --> 00:43:24,000
who knows? And that's, that's the thing, right? This is at the end of the day, it's a research plan.

570
00:43:24,000 --> 00:43:28,160
It talks about the things they want to focus on, not how to do them. And it is very vague in that

571
00:43:28,160 --> 00:43:32,800
sense. So overall, those are kind of my thoughts. I really like this. I think it's interesting to

572
00:43:32,800 --> 00:43:38,560
read. I think it's very familiar, well, at the same time being somewhat fairly different from what

573
00:43:38,560 --> 00:43:42,640
people, it's what, or rather, I should say, it's almost the same thing that people are working on,

574
00:43:42,640 --> 00:43:47,440
but with a different problem in mind, different sort of problem setting. And I think interesting

575
00:43:47,440 --> 00:43:52,080
differences could arise from that. I could certainly see people having a wide variety

576
00:43:52,080 --> 00:43:57,680
of reactions to this. Some people saying this is completely useless. It's not detailed enough at all.

577
00:43:57,680 --> 00:44:01,520
I could see other people saying, Oh, this is very interesting. I could see some people saying, Oh,

578
00:44:01,520 --> 00:44:06,560
this is, this is the next step in the future. I really don't know. I'm very curious. So let me

579
00:44:06,560 --> 00:44:12,000
know what you think in the comments. I'm, I think we'll have some very different opinions. I am

580
00:44:12,000 --> 00:44:16,720
excited to say, if you've enjoyed this, do consider subscribing to the channel, maybe check it out

581
00:44:16,720 --> 00:44:20,400
some of my other videos. I would really appreciate it. It really helps out. And hopefully you'll

582
00:44:20,400 --> 00:44:25,040
find some other interesting content. Anyway, thank you so much for joining me and I hope to catch you

583
00:44:25,040 --> 00:44:25,680
next time.

