1
00:00:00,000 --> 00:00:01,000
in this episode.

2
00:00:01,000 --> 00:00:09,720
So there's a long history in 20th century philosophy of science of investigating the

3
00:00:09,720 --> 00:00:23,000
possibility of something like an automated science or a formalized science or a algorithmically

4
00:00:23,000 --> 00:00:34,640
implemented science, all aspects of science, including sort of uncovering theory.

5
00:00:34,640 --> 00:00:44,320
And this has been a rich tradition, but it's been strictly abstract about about hypothetical

6
00:00:44,320 --> 00:00:52,360
machines and hypothetical algorithms and hypothetical forms of automation of scientific labor rights.

7
00:00:52,360 --> 00:01:01,880
And all of a sudden sort of perhaps largely unanticipated by philosophers, we reach a

8
00:01:01,880 --> 00:01:11,160
point where boom, deep learning revolution, and suddenly there is a kind of massive shift

9
00:01:11,160 --> 00:01:23,720
towards taking aspects of scientific discovery and passing them off to a intelligent computational

10
00:01:23,720 --> 00:01:29,920
system.

11
00:01:29,920 --> 00:01:37,200
And I think there's been kind of a tendency to continue to view that within the lens of

12
00:01:37,200 --> 00:01:48,520
this automation of scientific discovery debates as it's existed without really attending to

13
00:01:48,520 --> 00:01:56,640
the details of how these technologies are in fact being implemented in scientific practice.

14
00:01:56,640 --> 00:02:04,280
And by the way, I think when, when what we are doing is collecting data from some system

15
00:02:04,280 --> 00:02:13,760
in the world and using some machine learning model to extract statistical patterns from

16
00:02:13,760 --> 00:02:17,720
that system in order to learn about that system, what we're doing is is effectively science.

17
00:02:17,720 --> 00:02:19,520
It follows the model of science.

18
00:02:19,520 --> 00:02:24,880
So in fact, most applications of machine learning ought to be considered a kind of science

19
00:02:24,880 --> 00:02:26,920
adjacent activity.

20
00:02:26,920 --> 00:02:31,160
And in my opinion, it held to the standards of good science.

21
00:02:31,480 --> 00:02:35,640
Hey, everyone, welcome to my conversation with the philosopher of science, Mel Andrews,

22
00:02:35,640 --> 00:02:40,240
who's doing work in machine learning and other adjacent fields such as the philosophy of

23
00:02:40,240 --> 00:02:44,360
mathematics, epistemology, and of course, ethics.

24
00:02:44,360 --> 00:02:48,960
It was a fantastic, cordial and riveting conversation for me.

25
00:02:48,960 --> 00:02:51,000
And I should first say it's good to be back.

26
00:02:51,000 --> 00:02:56,680
It's good to be back after my short holiday back doing podcasts, having these fascinating

27
00:02:56,680 --> 00:03:01,080
conversations with these superlative guests.

28
00:03:01,080 --> 00:03:05,200
And I couldn't think of a better person to start off with other than Mel.

29
00:03:05,200 --> 00:03:07,520
They is fascinating.

30
00:03:07,520 --> 00:03:14,440
They also has this way of kind of, you know, elucidating certain points that for a while

31
00:03:14,440 --> 00:03:16,880
I did struggle to understand.

32
00:03:16,880 --> 00:03:22,760
And of course, probably the best example would be this paper, the math is not the territory

33
00:03:22,760 --> 00:03:26,120
navigating the free energy principle.

34
00:03:26,120 --> 00:03:33,000
As someone who's deeply interested in the kind of philosophy of FEP, this paper really

35
00:03:33,000 --> 00:03:38,800
helped me understand what the ontology of the free energy principle is and what really

36
00:03:38,800 --> 00:03:45,080
is the proper way to view it as a kind of a mathematical, formalistic, conceptual framework

37
00:03:45,080 --> 00:03:50,080
that can be applied in many areas such as machine learning and of course cognitive science.

38
00:03:50,080 --> 00:03:54,880
But we did start the podcast discussing Mel's recent paper regarding the epistemic status

39
00:03:54,880 --> 00:04:01,260
of machine learning and they provocatively claimed that machine learning has a pseudo

40
00:04:01,260 --> 00:04:07,200
science problem which I vehemently agree with and even kind of brought up this resurgence

41
00:04:07,200 --> 00:04:12,600
of physiognomy kind of provocatively to make this point.

42
00:04:12,600 --> 00:04:17,840
It's a preprint, in fact, and we discussed a bit of that paper and kind of extrapolated

43
00:04:17,840 --> 00:04:25,240
some of those ideas into the broader socio-political discussion around AI ethics, which again is

44
00:04:25,240 --> 00:04:26,960
very interesting.

45
00:04:26,960 --> 00:04:33,720
And I do really appreciate that Mel is a very critical almost focodian analysis of these

46
00:04:33,720 --> 00:04:38,440
different disciplines that are getting a lot of attention these days such as AI and machine

47
00:04:38,440 --> 00:04:39,600
learning of course.

48
00:04:39,600 --> 00:04:46,960
I also mentioned to Mel that this credit feed is a place that I visit quite often online.

49
00:04:46,960 --> 00:04:52,160
I hate Twitter, I think Twitter is a hellscape, however there are a few accounts that I've

50
00:04:52,160 --> 00:04:59,240
bookmarked and I do frequently visit simply for entertainment but also to learn and Mel's

51
00:04:59,240 --> 00:05:03,600
Twitter feed is certainly one of those that are bookmarked.

52
00:05:03,600 --> 00:05:08,280
Having said that, before I get carried away talking about Twitter and all of that, a bit

53
00:05:08,280 --> 00:05:15,240
of a formal introduction to Mel Andrews is a philosopher of science working on the role

54
00:05:15,240 --> 00:05:20,080
of mathematical and computational methods in science and particular machine learning

55
00:05:20,080 --> 00:05:21,880
based methods.

56
00:05:21,880 --> 00:05:27,040
Mel is currently a predoctoral research associate at the department of machine learning at the

57
00:05:27,040 --> 00:05:31,680
Carnegie Mellon University and doing a PhD in philosophy of science at the University

58
00:05:31,680 --> 00:05:32,680
of Cincinnati.

59
00:05:32,680 --> 00:05:38,560
They are also a visiting scholar at the Australian National University and the University of

60
00:05:38,560 --> 00:05:40,040
Pittsburgh.

61
00:05:40,040 --> 00:05:43,480
Having said that, here's my conversation with Mel Andrews.

62
00:05:43,480 --> 00:05:47,000
So on housekeeping note I should mention that I'll leave links to everything we've discussed

63
00:05:47,000 --> 00:05:48,000
in the show notes.

64
00:05:48,000 --> 00:05:49,000
Okay, now to the podcast.

65
00:05:49,000 --> 00:05:50,000
Mel Andrews

66
00:05:50,000 --> 00:05:56,960
The sort of conclusion I reached eventually was, it just, I mean, in academia there's

67
00:05:56,960 --> 00:06:07,360
a certain portion of what you do that feels like running on a wheel, like it's not, it's

68
00:06:07,360 --> 00:06:11,000
a performance, it's not actually to accomplish anything but it just feels like such a high

69
00:06:11,040 --> 00:06:19,280
percentage of that in industry is just sort of, there's so many levels of removal.

70
00:06:19,280 --> 00:06:25,640
The person who's passing down the orders is so many levels removed from the people actually

71
00:06:25,640 --> 00:06:32,560
implementing solutions that you're almost not accomplishing anything.

72
00:06:32,560 --> 00:06:33,560
Mel Andrews

73
00:06:33,560 --> 00:06:39,200
Yeah, also like, I mean, look, it's, I think, Franz Kafka, he captured it best in his novel,

74
00:06:39,200 --> 00:06:43,360
you know, it's Kafka-esque in the sense that it seems like people are just doing things

75
00:06:43,360 --> 00:06:49,120
for the sake of doing things without really going anywhere and you're having all these,

76
00:06:49,120 --> 00:06:51,840
you know, wonky meetings.

77
00:06:51,840 --> 00:06:57,480
But to be fair though, like, I agree with you because, because I'm still outside of academia,

78
00:06:57,480 --> 00:07:02,720
I've kind of idealized academia, but after talking to a lot of people like yourself on

79
00:07:02,720 --> 00:07:07,920
the podcast, I'm getting a more realistic picture of what it is because at the end of

80
00:07:07,920 --> 00:07:13,800
the day, the university is still in our society, it's a part of our culture within them.

81
00:07:13,800 --> 00:07:14,800
Mel Andrews

82
00:07:14,800 --> 00:07:19,160
And it's beholden to a capitalist economic system.

83
00:07:19,160 --> 00:07:20,160
Mel Andrews

84
00:07:20,160 --> 00:07:21,160
Exactly, exactly.

85
00:07:21,160 --> 00:07:22,160
Yeah, yeah.

86
00:07:22,160 --> 00:07:27,360
And it kind of shows how much the, let's call it the capitalist tentacles reach into every

87
00:07:27,360 --> 00:07:29,880
corner of our social existence.

88
00:07:29,880 --> 00:07:35,320
I mean, although I even go a step further and I say, I mean, I'm a big fan of Jacques,

89
00:07:35,320 --> 00:07:41,480
it even changes our psyche, our subjective state in this world and, and like fundamentally

90
00:07:41,480 --> 00:07:47,640
who we are, as let's say beings in this world, you know, and then that of course affects academics

91
00:07:47,640 --> 00:07:52,360
and, and quote unquote, lay people and everyone really, but I see what you mean.

92
00:07:52,360 --> 00:07:53,360
Yeah, yeah.

93
00:07:53,360 --> 00:07:55,360
So I've got to ask now though.

94
00:07:55,360 --> 00:07:56,360
Mel Andrews

95
00:07:56,360 --> 00:07:57,360
Sorry.

96
00:07:57,360 --> 00:08:03,880
In terms of even like the AI ethics communities, you have like the AI risk, AI safety people,

97
00:08:04,880 --> 00:08:09,960
like existential risk, kind of the effective altruist oriented community.

98
00:08:09,960 --> 00:08:16,400
And you've got then, you know, academic, fairness, safety, regulation communities.

99
00:08:16,400 --> 00:08:20,280
And they're both sort of pointing at each other and accusing each other of like corporate

100
00:08:20,280 --> 00:08:21,280
capture.

101
00:08:21,280 --> 00:08:26,840
But of course, the irony is, is that these communities are both about as corporately

102
00:08:26,840 --> 00:08:28,840
captured as you can.

103
00:08:28,840 --> 00:08:29,840
Yeah.

104
00:08:29,840 --> 00:08:30,840
Yeah.

105
00:08:30,840 --> 00:08:35,440
And every entity really, you know, neoliberal society is corporate captured in some sense,

106
00:08:35,440 --> 00:08:39,800
you know, after the 70s and 80s, it fundamentally changed.

107
00:08:39,800 --> 00:08:42,480
I completely agree with you.

108
00:08:42,480 --> 00:08:44,480
No ethical work, no ethical consumption.

109
00:08:44,480 --> 00:08:49,880
I mean, there's only really a handful of, I'd say at this point, as in some sense I'm

110
00:08:49,880 --> 00:08:56,840
like, well, I am in your fight, but I'd say there's only a handful of researchers who

111
00:08:56,840 --> 00:08:59,960
I follow quite closely, you certainly being one of them.

112
00:08:59,960 --> 00:09:06,800
And then in S and a few others, because as in for me, there's this whole question about

113
00:09:06,800 --> 00:09:16,200
AI ethics and yeah, AI safety, I only find it to be useful when we kind of got it more

114
00:09:16,200 --> 00:09:21,920
from the, let's call it the ontological level of like, what is AI, what is machine learning

115
00:09:21,920 --> 00:09:28,480
as we have it now, and kind of starting at like that very fundamental level ontological

116
00:09:28,480 --> 00:09:32,960
level as you've done, you know, in some of your papers.

117
00:09:32,960 --> 00:09:38,760
Because for me, that gives a lot more hate use to stone because it's kind of cheap, but

118
00:09:38,760 --> 00:09:43,320
like realistic view of where we are with machine learning and AI and where we can go.

119
00:09:43,320 --> 00:09:54,240
Or even sticking to, you know, sticking to the realm of not what is conceivable.

120
00:09:54,240 --> 00:09:57,560
I mean, I think there's a lot of discourse happening at the level of what is conceivable,

121
00:09:57,560 --> 00:10:02,920
what sorts of technologies are conceivable, or what sorts of technologies are metaphysically

122
00:10:02,920 --> 00:10:04,320
possible or what haven't you.

123
00:10:04,320 --> 00:10:15,840
And it's like, well, let's think about what might come into being in the reality we occupy,

124
00:10:15,840 --> 00:10:23,200
because what comes into being in the reality we occupy is governed by market forces.

125
00:10:23,200 --> 00:10:31,440
And we should think about technologies that some person might conceivably be incentivized

126
00:10:31,440 --> 00:10:34,360
and capacitated to build.

127
00:10:34,360 --> 00:10:40,200
If there are technologies that no one would ever be incentivized and capacitated to build,

128
00:10:40,200 --> 00:10:45,160
realistically speaking, I don't think there's much point in debating what their capabilities

129
00:10:45,160 --> 00:10:49,040
are or what the danger is emerging from these technologies would be.

130
00:10:49,040 --> 00:10:53,280
And I think there's a lot of debate that's happening at the level without considering,

131
00:10:53,280 --> 00:10:56,360
you know, incentives.

132
00:10:57,000 --> 00:11:10,920
Truthfully, I think this is, again, the primary sort of access of opposition in the AI ethics

133
00:11:10,920 --> 00:11:17,560
communities or community is between the risk safety people and the sort of fairness responsible

134
00:11:17,560 --> 00:11:21,960
AI people, fact community, et cetera.

135
00:11:21,960 --> 00:11:30,720
And maybe the failing point of both of these communities is, in their scholarship, a total

136
00:11:30,720 --> 00:11:37,200
failure, at least in the major part of the work, to really, really consider incentive

137
00:11:37,200 --> 00:11:39,560
structures.

138
00:11:39,560 --> 00:11:45,080
So the interventions we're suggesting have to be able to work within incentive structures

139
00:11:45,120 --> 00:11:48,280
as they exist or they might conceivably exist.

140
00:11:48,280 --> 00:11:55,680
They have to consider how we might, if the point is to manipulate incentive structures,

141
00:11:55,680 --> 00:11:58,080
we have to have that conversation explicitly.

142
00:11:58,080 --> 00:12:05,400
How are we intending that the intervention we suggest would nudge incentive structures

143
00:12:05,400 --> 00:12:06,400
as they exist?

144
00:12:06,400 --> 00:12:13,360
And I think there's just sort of a failure to consider that in large part in a lot of

145
00:12:13,360 --> 00:12:20,360
the work falling under the age of AI ethics, writ as largely and abstractly as we possibly

146
00:12:20,360 --> 00:12:21,880
can, right?

147
00:12:21,880 --> 00:12:23,080
Undoubtedly, undoubtedly.

148
00:12:23,080 --> 00:12:24,080
Yeah.

149
00:12:24,080 --> 00:12:30,320
I mean, it sounds like a very straightforward thing, but, you know, AI doesn't sit in a silo.

150
00:12:30,320 --> 00:12:34,280
It's always within a, it sits within a psychosocial reality.

151
00:12:34,280 --> 00:12:36,320
I couldn't agree more.

152
00:12:36,320 --> 00:12:44,520
Which is deeply, deeply complex to the point that it, having to think through incentive

153
00:12:44,520 --> 00:12:50,160
structures feels like it makes any problem in this realm completely intractable.

154
00:12:50,160 --> 00:12:51,160
Yeah.

155
00:12:51,160 --> 00:12:52,160
So I get exploits.

156
00:12:52,160 --> 00:12:57,040
Although I want to add, which is where I again like what you're doing, and we'll certainly

157
00:12:57,040 --> 00:13:02,400
get to this in a bit, kind of trying to understand, which is why for me, I think only a philosopher

158
00:13:02,400 --> 00:13:09,080
of science and perhaps even someone with a bit of a background in sociology can kind

159
00:13:09,080 --> 00:13:17,360
of explore the kind of epistemic status of what is machine learning or what is AI in

160
00:13:17,360 --> 00:13:22,000
contemporary times and then maybe speculate and theorize on where it could go and develop

161
00:13:22,000 --> 00:13:23,000
in the future.

162
00:13:23,000 --> 00:13:26,800
Because I'm going to be honest, as more of an outsider, some of this sounds a lot like

163
00:13:26,800 --> 00:13:32,280
science fiction to me and I'm kind of like, are we really even discussing as to what

164
00:13:32,920 --> 00:13:38,080
the current models are, what are large language models, what do they really do without all

165
00:13:38,080 --> 00:13:44,080
of this speculation, which again, the speculation seems just like it's just a bunch of blokes

166
00:13:44,080 --> 00:13:51,200
having fun without considering all the factors that you mentioned, our psychosocial reality,

167
00:13:51,200 --> 00:13:58,400
ways, AI, incentive structure, kind of symbolic network that all comes together, which seems

168
00:13:58,400 --> 00:14:00,520
like an intractable problem, I agree.

169
00:14:00,520 --> 00:14:01,520
Yeah.

170
00:14:01,600 --> 00:14:10,200
But I mean, I think a lot of philosophy is self-gratifying, in a sense.

171
00:14:10,200 --> 00:14:11,200
Oh, for sure.

172
00:14:11,200 --> 00:14:13,120
I mean, I would say a lot of thinking in general.

173
00:14:13,120 --> 00:14:14,120
Yeah.

174
00:14:14,120 --> 00:14:15,120
Yeah.

175
00:14:15,120 --> 00:14:19,960
I'd say a big part of it is for that, because it's self-gratifying, not for any ethical.

176
00:14:19,960 --> 00:14:20,960
Yeah.

177
00:14:20,960 --> 00:14:29,160
I mean, I think I have a very sort of pragmatic embodied view of thinking and it's sort of

178
00:14:29,200 --> 00:14:36,720
a very, I think that goes into sort of pragmatic ethical or political stance as well, where

179
00:14:36,720 --> 00:14:41,320
I think, I mean, I think we think in order to affect action in the world.

180
00:14:41,320 --> 00:14:47,000
Thinking, I don't think thought takes place.

181
00:14:47,000 --> 00:14:53,480
I don't think we can say cognition has really taken place unless we see the hallmarks of

182
00:14:53,480 --> 00:14:56,840
it's in behavior.

183
00:14:56,840 --> 00:15:04,920
So I mean, I actually think I'm a sort of weird behaviorist in the sense that I think

184
00:15:04,920 --> 00:15:13,200
that consciousness and sentience and agency and cognition and a lot of sort of what's

185
00:15:13,200 --> 00:15:23,840
been considered unobservable mental characteristics are actually readable from behavioral dynamics.

186
00:15:23,840 --> 00:15:34,200
And I think that we should not be thinking of philosophy as an eye will pursue.

187
00:15:34,200 --> 00:15:40,920
It impacts the way we go about the world or it ought to be approached as though it's

188
00:15:40,920 --> 00:15:49,320
going to have immediate impact on how we interact with the phenomena it treats as its subject

189
00:15:49,320 --> 00:15:50,320
matter.

190
00:15:50,320 --> 00:15:51,320
Yeah.

191
00:15:52,320 --> 00:15:57,280
I'm just saying I'm fully a pragmatist with respect.

192
00:15:57,280 --> 00:15:58,280
Yeah.

193
00:15:58,280 --> 00:15:59,280
I get you.

194
00:15:59,280 --> 00:16:01,440
But also, I mean, not only as a pragmatist, but I would even say, you know, I've been

195
00:16:01,440 --> 00:16:07,520
deeply interested in psychoanalysis and I've spoken to heaps of psychoanalysts and one misunderstanding

196
00:16:07,520 --> 00:16:12,120
I think a lot of people have coming from like the Carl Jung type is like the idea of depth

197
00:16:12,120 --> 00:16:18,960
psychology where they think the truth of a person's desires and motives are somewhere

198
00:16:18,960 --> 00:16:22,120
deep within and you need like a psychoanalyst to find it.

199
00:16:22,120 --> 00:16:27,440
But the flip is the Freudian more Lacanian idea is no, no, no, that the truth lies in

200
00:16:27,440 --> 00:16:28,640
your actions.

201
00:16:28,640 --> 00:16:35,080
The truth is never like deep within rather it's it's it's very it's very conspicuous.

202
00:16:35,080 --> 00:16:36,080
It's out there.

203
00:16:36,080 --> 00:16:40,800
Hey, you know, and you can see it in in how people act and how people engage in their

204
00:16:40,800 --> 00:16:41,800
social world.

205
00:16:41,800 --> 00:16:44,680
So I can say nothing is really hidden with that.

206
00:16:44,680 --> 00:16:45,680
Yeah.

207
00:16:45,680 --> 00:16:48,200
That's the ultimately come to realize nothing is really hidden.

208
00:16:48,560 --> 00:16:55,880
So you wrote, you've been working on a few papers, kind of working on the epistemic status

209
00:16:55,880 --> 00:17:01,440
of machine learning, which I've thoroughly enjoyed reading and learned a lot.

210
00:17:01,440 --> 00:17:06,080
One of them was this paper I read, this was I read this a while back, which is the machine

211
00:17:06,080 --> 00:17:08,880
learning and the theory free ideal.

212
00:17:08,880 --> 00:17:11,760
I'll leave a link to that in the in the show notes.

213
00:17:11,760 --> 00:17:16,360
But also the recent preprint you shared with me, which I thought was a very good provocative

214
00:17:16,360 --> 00:17:23,600
piece, in fact, titled ghosts in the machine learning, the reanimation of pseudoscience

215
00:17:23,600 --> 00:17:26,240
and its ethical repercussions.

216
00:17:26,240 --> 00:17:32,080
So to perhaps to orient the listeners, if you could kind of give us an introduction as to

217
00:17:32,080 --> 00:17:37,720
yeah, what is what is this work you're doing, researching the epistemic status of machine

218
00:17:37,720 --> 00:17:44,720
learning, apropos philosophy of science, and then perhaps, you know, you know, as a as

219
00:17:44,760 --> 00:17:54,040
carrying on from our ethics conversation or chat, how that how those two ideas are connected,

220
00:17:54,040 --> 00:17:58,400
you know, the epistemic status of machine learning, and then what's its relationship

221
00:17:58,400 --> 00:17:59,960
to ethics.

222
00:18:01,680 --> 00:18:02,680
Yeah.

223
00:18:02,680 --> 00:18:11,800
So there's a long history in 20th century philosophy of science of investigating the

224
00:18:11,800 --> 00:18:24,960
possibility of something like an automated science, or a formalized science, or a algorithmically

225
00:18:24,960 --> 00:18:31,680
implemented science, all aspects of science, including sort of uncovering theory.

226
00:18:32,680 --> 00:18:46,360
And this has been a rich tradition, but it's been strictly abstract about about hypothetical

227
00:18:46,360 --> 00:18:54,360
machines and hypothetical algorithms and hypothetical forms of automation of scientific labor rights.

228
00:18:54,360 --> 00:19:03,920
And all of a sudden, sort of, perhaps largely unanticipated by philosophers, we reach a

229
00:19:03,920 --> 00:19:07,960
point where boom, deep learning revolution.

230
00:19:07,960 --> 00:19:19,240
And suddenly, there is a kind of massive shift towards taking aspects of scientific discovery

231
00:19:19,280 --> 00:19:26,720
and passing them off to a intelligent computational system, right.

232
00:19:31,800 --> 00:19:39,840
And I think there's been kind of a tendency to continue to view that within the lens of of this

233
00:19:40,120 --> 00:19:50,720
automation of scientific discovery debate as it's existed, without really attending to the

234
00:19:50,720 --> 00:19:58,440
details of how these technologies are, in fact, being implemented in scientific practice.

235
00:19:58,440 --> 00:20:06,280
And by the way, I think I think when when what we are doing is collecting data from some system

236
00:20:06,280 --> 00:20:15,920
in the world, and using some machine learning model to extract statistical patterns from that

237
00:20:15,920 --> 00:20:20,200
system in order to learn about that system, what we're doing is effectively science, it follows

238
00:20:20,200 --> 00:20:26,360
the model of science. So in fact, most applications of machine learning ought to be considered a

239
00:20:26,360 --> 00:20:33,280
kind of science adjacent activity. And in my opinion, held to the standards of good science.

240
00:20:33,920 --> 00:20:42,280
Now, I think that there's a lot of good work being done with machine learning and science.

241
00:20:42,280 --> 00:20:49,320
There's a lot of really epistendically careful work. And I think there's also, like with most

242
00:20:49,320 --> 00:21:02,720
things, a glut of garbage, pick a genre of anything. I don't care if it's films or psychological

243
00:21:02,720 --> 00:21:12,880
studies, or books on the history of hip hop, or like whatever it is, or perhaps even people for

244
00:21:12,880 --> 00:21:20,160
that matter. Yeah, yeah, most most of it's crap. Yeah. And there's a very small percentage of it

245
00:21:20,160 --> 00:21:29,120
that's quite good. Machine learning is no different. There's a lot of good work being done, and 1000

246
00:21:29,120 --> 00:21:37,680
fold more bad work being done. The issue is that it's being adopted so rapidly, these methods are

247
00:21:37,680 --> 00:21:43,560
being adopted so rapidly, in so many contexts, across society, most people don't understand how

248
00:21:43,560 --> 00:21:52,840
the technology works. There's a lot of AI hype, as long as there has been AI, there has been AI

249
00:21:52,920 --> 00:22:01,080
hype. And what hype is, I think it should be made explicit that what hype is, is it's a targeted

250
00:22:01,080 --> 00:22:07,280
disinformation campaign. Yeah, perhaps you could elaborate on what you mean by that.

251
00:22:07,280 --> 00:22:20,040
Yeah, so AI hype refers to people, when Sam Altman says, we'll have AGI by X, or when Sam

252
00:22:20,040 --> 00:22:28,680
Altman says, well, GPT three could take a 20 minute activity and reduce it to five minute

253
00:22:28,680 --> 00:22:36,840
activity, but GPT next will reduce a week long activity to five minutes or something like that.

254
00:22:36,840 --> 00:22:38,360
You know, that's AI hype.

255
00:22:39,960 --> 00:22:46,760
Now, I saw I saw a recent statement by, I believe, anyway, I want to mention any names that the next

256
00:22:46,760 --> 00:22:54,800
GPT model will will be able to replace PhD researchers and we won't need any PhD researchers

257
00:22:54,800 --> 00:22:58,200
anymore. So yeah, I'm in shock.

258
00:22:58,200 --> 00:23:05,840
That's AI hype. Scientists and science popularizers saying the theorist will be replaced, the

259
00:23:05,840 --> 00:23:13,960
physicist will be replaced, the doctor will be replaced, you know, in medical context saying,

260
00:23:13,960 --> 00:23:17,640
oh, we won't need, you know, we won't need secretaries or we won't need nurses anymore.

261
00:23:21,080 --> 00:23:28,120
Any of that drama, but it also includes the doomerism about AI, like soon AI will surpass

262
00:23:28,120 --> 00:23:32,680
human intelligence and it will, you know, hurt us somehow.

263
00:23:32,680 --> 00:23:37,480
That's the part which I, for me, at least feels a lot like science fiction, if I'm being honest,

264
00:23:37,480 --> 00:23:42,200
the doomer. Yeah. Yeah. There's something even it's interesting to even science fiction.

265
00:23:42,280 --> 00:23:48,120
I mean, yeah. And I have nothing, nothing against I love science fiction, but it's got his own

266
00:23:48,120 --> 00:23:51,720
status in our dialogue, right? I mean, science fiction, it's called fiction.

267
00:23:51,720 --> 00:23:58,360
But saying that's, that's some particular tech breakthrough is going to replace

268
00:24:01,080 --> 00:24:10,760
cardiologists even is absolutely, will it, you know, have some impact on the role of

269
00:24:10,760 --> 00:24:19,160
cardiologists in specifically how they approach analyzing the results of imaging,

270
00:24:20,680 --> 00:24:26,200
right? Totally. Is it going to replace cardiologists? No, that's science fiction, right?

271
00:24:26,200 --> 00:24:31,320
And it should, given all technology affects how we do our jobs, how we live our lives,

272
00:24:31,320 --> 00:24:35,880
and that's completely fine. It probably should affect how they do their job.

273
00:24:36,680 --> 00:24:41,640
If it works, it should. That's, that's true. It doesn't work if it can, but if it works,

274
00:24:41,640 --> 00:24:45,240
it should have an impact. It's just that there's, there's,

275
00:24:48,680 --> 00:24:54,520
in anything that's, you know, marketable, there's an incentive to misrepresent

276
00:24:55,480 --> 00:25:04,440
what it does. This is true of, you can pull out a magazine from 1952 and how it's

277
00:25:05,080 --> 00:25:15,080
advertises some cooking implement to stay at home moms. And it misrepresent, you know, it's like,

278
00:25:15,080 --> 00:25:19,160
this is a life changing, it's going to lie to you about what the thing does, right? But

279
00:25:21,640 --> 00:25:27,320
with AI specifically, you know, I think, I think there's just to some extent,

280
00:25:27,880 --> 00:25:35,480
even the, the average housewife in the 1950s was like, yeah, probably, probably not all of this

281
00:25:35,480 --> 00:25:43,160
about the oven is actually the ground truth, you know, whereas with AI, people seem to be willing

282
00:25:43,160 --> 00:25:50,520
to believe really radically untrue things, you know, just things that are actually

283
00:25:51,480 --> 00:25:56,520
to anyone in the know, blatant lies. But there's a culture of

284
00:25:58,520 --> 00:26:10,520
really leaning into highly fabulous lies, propagating them to no end. And no one seems to be doing

285
00:26:10,520 --> 00:26:17,320
fact checking. And the worst part to me is, is, I would think that's in, so I'm, I'm coming from

286
00:26:18,040 --> 00:26:25,320
this position of history and philosophy of science, where it's like, we've studied hundreds and

287
00:26:25,320 --> 00:26:31,720
hundreds of years of science and technology, and how people misrepresent what it does and what it

288
00:26:31,720 --> 00:26:42,120
actually does, right? So we would, I would hope that we would have a critical lens on this. And

289
00:26:42,120 --> 00:26:45,800
it strikes me that there's a lot of philosophy that's simply

290
00:26:47,560 --> 00:26:52,920
parroting or, or lending kind of philosophical justification to these hype narratives that,

291
00:26:54,360 --> 00:26:59,960
you know, deep learning will radically change the face of particle physics or something like

292
00:26:59,960 --> 00:27:06,360
that. It's like, really, really though. Okay, perhaps just, well, this is, this is a good,

293
00:27:06,360 --> 00:27:11,880
good place to go to. So one, one term, which I really liked in the paper, which you use with

294
00:27:11,880 --> 00:27:17,480
this idea of global, the theory free ideal, I really liked that, that, that term. Because I

295
00:27:17,480 --> 00:27:23,400
think for me that captures what apropos philosophy of science or let's, let's say apropos the,

296
00:27:23,400 --> 00:27:31,800
the scientific method, what the hope or the dream with ML models are. So if you could melt, just,

297
00:27:31,800 --> 00:27:38,200
just to like, again, to flesh this out a bit more as, as to where we are right now with the

298
00:27:38,200 --> 00:27:45,080
current paradigm of machine learning, what is the epistemic status of machine learning?

299
00:27:45,880 --> 00:27:52,920
And then could you then probably connect that to what, what, what, what is this theory free ideal?

300
00:27:52,920 --> 00:27:57,800
And then as you point out in your papers, what are the mistakes that people make with, with,

301
00:27:57,880 --> 00:28:03,480
to, you know, by having this ideal of science being theory free when done with a,

302
00:28:04,200 --> 00:28:10,920
let's say an ML model, for instance. Yeah. Yeah. So I think machine learning models are statistical

303
00:28:10,920 --> 00:28:17,880
models that are computationally instantiated. There's, there's nothing that would in principle

304
00:28:19,560 --> 00:28:24,360
make the epistemic status of these technologies any different from

305
00:28:25,080 --> 00:28:33,320
any kind of other statistical model. Now, you're pushing yourself into really high

306
00:28:33,320 --> 00:28:43,480
dimensional spaces in which data is transposed. And so there's intrinsically,

307
00:28:45,320 --> 00:28:52,760
the, the dimensionality of the patterns you're finding is much higher than what you're doing

308
00:28:52,760 --> 00:29:03,960
with classical statistics. But I guess I doubt that when people are doing multiple regression,

309
00:29:04,840 --> 00:29:14,120
they are kind of holding all the dimensions in their head in the way that they would have to be

310
00:29:14,120 --> 00:29:20,600
in order for the contrast that's typically drawn between deep learning and classical statistics

311
00:29:20,680 --> 00:29:27,000
to make sense. Like there's, there's meant to be a kind of opacity, a kind of intrinsic deep

312
00:29:28,360 --> 00:29:33,640
unknowability of the kinds of patterns that these statistical methods that is deep learning methods

313
00:29:33,640 --> 00:29:39,320
are finding relative to classical statistical methods. And I just don't see that distinction being

314
00:29:41,960 --> 00:29:47,480
substantive and absolute in the way that it's proposed to be. I think these are at their heart

315
00:29:47,480 --> 00:29:52,200
statistical methods like other statistical methods. And if there's a difference,

316
00:29:53,960 --> 00:30:01,000
it's a sociological difference. Okay, I think I've followed you all the way except the last bit,

317
00:30:01,000 --> 00:30:06,520
the sociological, but if you could probably touch that up. Yeah, so it's, it's the corporatization

318
00:30:06,520 --> 00:30:12,280
of these technologies. I see. Okay. It's the hype narrative. So even in a research context,

319
00:30:12,360 --> 00:30:14,440
there's because, because machine learning, I mean,

320
00:30:19,000 --> 00:30:26,440
because machine learning is this place where stats met up with AI and AI is something that's

321
00:30:26,440 --> 00:30:32,280
always, AI refers to a lot of different research traditions that have had historically very little

322
00:30:32,280 --> 00:30:36,200
to do with each other in terms of their, their substance in terms of their subject matter in

323
00:30:36,200 --> 00:30:45,800
terms of the methods, they mostly have to do with where funding is being targeted and the kinds of

324
00:30:46,760 --> 00:30:51,400
narratives spun around these research methods. So there's very little substantive that holds

325
00:30:51,400 --> 00:30:55,560
everything that's historically been called AI to going back to cybernetics, going back to McCarthy,

326
00:30:55,560 --> 00:31:03,080
going back to, you know, all the tips and, you know, like going back all through the history of

327
00:31:03,080 --> 00:31:09,640
things being called artificial intelligence. There's very little that connects all of these, but

328
00:31:11,960 --> 00:31:16,280
who they're targeting for funding and the kinds of narratives they're using in

329
00:31:16,280 --> 00:31:24,440
convincing the public and funding bodies of what they're doing. So there are methods and statistics

330
00:31:24,440 --> 00:31:32,920
that have been, you know, approaching something like machine learning, going, going back 80s, 90s,

331
00:31:32,920 --> 00:31:42,920
whatever, right? But where that meets up with the AI narrative, you get this hype and disinformation

332
00:31:42,920 --> 00:31:58,760
and overselling of competence, right? And so there's, there's an attitude and a meta narrative

333
00:31:58,760 --> 00:32:06,440
surrounding machine learning that is, I think, more what sets it apart from classical statistics

334
00:32:06,440 --> 00:32:15,880
than anything else. This is, this is a spicy take, you know, kind of with a grain of salt. But

335
00:32:15,880 --> 00:32:24,040
this is my, my challenge is really like, tell me what is so radically epistemically different

336
00:32:24,040 --> 00:32:30,680
about these technologies that they should be placed in some category that's discrete from

337
00:32:30,680 --> 00:32:36,840
classical statistics. I have not seen it, you know. And you point this out. Yeah. And also,

338
00:32:36,840 --> 00:32:44,280
I guess it's this meta narrative that, that drives its impetus behind this theory free ideal

339
00:32:44,920 --> 00:32:50,920
when it comes to science. Yeah. And, and there's been so philosophers, natural philosophers,

340
00:32:50,920 --> 00:32:58,360
scientists have debated what theory is and what its proper role in science is since the, the

341
00:32:58,360 --> 00:33:03,480
incipients of what we call modern science, right, since Bacon and Newton and Galilea, you know,

342
00:33:03,480 --> 00:33:13,000
modern science, right? Even going back to Bacon, there's, there's, there's a kind of push for a

343
00:33:13,000 --> 00:33:22,120
kind of radical empiricism that pushes away as much as possible the role of theory about just,

344
00:33:22,120 --> 00:33:26,600
just tabulating as much data as possible and sifting through it for patterns and, and not

345
00:33:26,680 --> 00:33:30,280
bringing our kind of conceptual infrastructure to bear on it. But then since Hume,

346
00:33:32,120 --> 00:33:37,080
since Hume brought his sort of problem of induction to the table in epistemology,

347
00:33:39,000 --> 00:33:45,080
there's this widespread recognition that, well, all, all knowledge of the natural world

348
00:33:45,080 --> 00:33:49,640
is knowledge by induction. You do not get deductive certainty about empirical

349
00:33:49,640 --> 00:33:57,960
matters. Yeah. So the typical example is the sunrising just because it rose

350
00:33:57,960 --> 00:34:03,800
yesterday. We can't say necessarily it'll every day of our lives, the sun has risen in the morning.

351
00:34:03,800 --> 00:34:09,400
It's risen. Yeah. Yeah. And yet give me a deductive proof, give me logical certainty

352
00:34:09,400 --> 00:34:13,720
that it will rise tomorrow. There is none. There's only the, the

353
00:34:14,120 --> 00:34:22,280
you know, the remit of our experience to tell us that it will rise tomorrow. There's no

354
00:34:23,400 --> 00:34:25,960
logical necessity that it will rise again tomorrow.

355
00:34:28,520 --> 00:34:35,480
So knowledge of nature, scientific knowledge is, and all our kind of day to day practical

356
00:34:35,480 --> 00:34:39,640
knowledge, like I can eat this bread and it won't poison me because the bread I ate yesterday that

357
00:34:39,720 --> 00:34:45,080
I got from the same baker didn't poison me. You know, this is inductive knowledge, which means

358
00:34:45,080 --> 00:34:53,800
we don't get deductive certainty. And it also means that to get that kind of knowledge, you need to

359
00:34:55,080 --> 00:35:01,960
start off with rich conceptual infrastructure, which I'm calling theory. I think, I think when,

360
00:35:01,960 --> 00:35:05,240
so I think there are lots of ways that philosophers have traditionally

361
00:35:05,240 --> 00:35:10,440
cashed out what we mean by theory, I think that when we talk about theory free science, we mean

362
00:35:12,520 --> 00:35:19,400
a powerful influence at the beginning of inquiry, at the beginning of the investigatory

363
00:35:19,400 --> 00:35:25,720
procedure of our prior conceptual resources, our prior conceptual

364
00:35:26,920 --> 00:35:32,360
acquaintance with the target phenomena, right? We do not get, we do not get inductive

365
00:35:32,360 --> 00:35:38,280
inference off the ground without bringing to bear prior theory or conceptual.

366
00:35:39,720 --> 00:35:43,160
The philosopher and historian of science, John Norton calls it

367
00:35:43,880 --> 00:35:49,880
bringing to bear material facts, right? But there are lots of ways of putting it, but

368
00:35:51,880 --> 00:35:58,040
you need theory to get empirical knowledge off the ground. And so in this sense,

369
00:35:58,760 --> 00:36:05,320
you cannot have theory free knowledge of natural systems. And I think that

370
00:36:08,200 --> 00:36:12,120
you can look back for hundreds of years, and there's always this dialogue between, no, we should get rid

371
00:36:12,120 --> 00:36:19,800
of as much, as much as we can push away the influence of prior conceptualization, we should do that,

372
00:36:19,800 --> 00:36:25,800
and that's scientific objectivity. This is, I think, one notion of scientific objectivity.

373
00:36:26,200 --> 00:36:33,640
Right? That has been kind of implicitly in the background of a lot of discourse in philosophy,

374
00:36:33,640 --> 00:36:39,160
natural philosophy science for hundreds of years. And then another stream that says,

375
00:36:40,680 --> 00:36:45,800
well, you can't actually have knowledge of nature without bringing conceptual resources to bear.

376
00:36:45,800 --> 00:36:51,960
So it's about documenting them. It's about recognizing them. It's about, to some extent,

377
00:36:51,960 --> 00:36:57,240
those assumptions and saying which of those assumptions are, in fact, substantiated by

378
00:36:57,240 --> 00:37:03,400
what we've then been able to observe and deduce from what we've measured or observed in nature,

379
00:37:03,400 --> 00:37:09,880
right? And what is, in fact, just arbitrary or unknown, right?

380
00:37:12,280 --> 00:37:18,120
And I think since the rise of domain generic statistical methods in the 20th century,

381
00:37:18,200 --> 00:37:22,840
in particular, stats really gets off the ground after the axiomatization of probability theory

382
00:37:22,840 --> 00:37:31,960
with Komagorov. Statistical reasoning, probabilistic reasoning is, to the extent that's so widespread

383
00:37:31,960 --> 00:37:36,840
in science, it's relatively new. It's really a kind of 20th century, like statistical reasoning

384
00:37:36,840 --> 00:37:41,960
is kind of a 20th century thing. I mean, it sort of got off the ground with gambling and stuff in

385
00:37:41,960 --> 00:37:52,040
the 17th century. But as a scientific method, it's new. And really, I think since the mid-20th

386
00:37:52,040 --> 00:38:03,800
century, you get a lot of this, what I call a theory free ideal. And it's in the kind of

387
00:38:07,080 --> 00:38:11,320
fundamental, I don't know if I believe in this destination, but in the more fundamental sciences

388
00:38:11,960 --> 00:38:18,680
who know how to theorize because they've been doing it for hundreds of years and know how to

389
00:38:19,240 --> 00:38:23,640
mathematically represent their phenomena, because they've been doing it for hundreds of years,

390
00:38:25,560 --> 00:38:34,120
you get less of this. But in the younger sciences, like the quantitative social sciences, like

391
00:38:34,120 --> 00:38:48,840
social psychology, like population genetics, what have you, economics, there's this belief that

392
00:38:51,160 --> 00:38:57,720
the more theory free, the more data driven the methods are, the more objective they are and

393
00:38:57,720 --> 00:39:03,640
the more sciencey. Yeah, so when you mean the more fundamental, you mean like physics, for instance,

394
00:39:03,640 --> 00:39:11,320
right? Yeah, areas of, areas of, but not all areas of physics, right? The areas of physics

395
00:39:11,320 --> 00:39:16,040
that are really established. Yeah, although, although, you know, this beautifully connects

396
00:39:16,040 --> 00:39:21,160
with your sociological point, because I'm sure you're aware of the whole Bohr-Einstein debate and

397
00:39:21,160 --> 00:39:25,000
that, you know, shut up and calculate. There's like a lot of, in the history of 20th century

398
00:39:25,000 --> 00:39:31,000
physics, is his idea that even within physics, you shouldn't ask, but given that physics is the,

399
00:39:31,080 --> 00:39:36,200
it's a foundational discipline, the ontology of the physical world, you know, there was a time,

400
00:39:36,200 --> 00:39:43,400
especially because of, you know, World War II and the nukes and all that, just don't ask the,

401
00:39:44,440 --> 00:39:48,600
don't ask the ontological questions, just shut up and calculate, just that there's a

402
00:39:48,600 --> 00:39:55,480
ruthless pragmatism, ruthless, just create. And so physics kind of sometimes, and again, I again

403
00:39:55,480 --> 00:40:01,160
say this as a bit of an outsider, it becomes a bit more like engineering or a foundational

404
00:40:01,800 --> 00:40:05,320
discipline where you're asking, well, what exists, what is reality?

405
00:40:05,320 --> 00:40:07,160
That's sort of the Los Alamos attitude, right?

406
00:40:07,160 --> 00:40:09,080
As in the idea that physics is engineering?

407
00:40:10,680 --> 00:40:12,760
Does that happen and calculate attitudes?

408
00:40:13,560 --> 00:40:18,680
Yeah, yeah, I mean, I know more from the Copenhagen school, I mean, I followed that, I think Tim

409
00:40:18,680 --> 00:40:25,080
Modellin, he speaks quite well about that. And just, it's like a, you know, interesting

410
00:40:25,080 --> 00:40:30,600
peculiarity in the history of physics, especially because if you look at the big figures, like,

411
00:40:30,600 --> 00:40:37,880
like the Einstein's, or like the Maxwell's, they were deeply interested in these philosophical

412
00:40:37,880 --> 00:40:43,000
questions, it's like, what exists, you know, it's not Einstein was a philosopher, he certainly was,

413
00:40:43,000 --> 00:40:48,200
he certainly was, you know, without a doubt. Yeah. I mean, probably after Newton is probably the

414
00:40:48,200 --> 00:40:54,360
quintessential natural philosopher, I couldn't agree more. And yeah, so I mean, just on this note,

415
00:40:55,240 --> 00:41:01,080
I couldn't read the paper too carefully, but I do love the, perhaps it's worth mentioning,

416
00:41:01,080 --> 00:41:06,840
because it's rather provocative and I like that a bit, where you say, yeah, machine learning has

417
00:41:06,840 --> 00:41:12,760
a pseudoscience problem. And then you bring up the, well, you and the other other writers, Andrew

418
00:41:12,760 --> 00:41:21,880
and Bieber, is it? Yeah, bring up. Yeah, yeah, they bring up the idea of how there's a resurgence

419
00:41:21,880 --> 00:41:29,800
of physiognomy in kind of these ML communities. So just, if you could just humor me with that for

420
00:41:29,800 --> 00:41:35,480
a bit, just kind of what all that, that's about why you claim, you know, machine learning has a

421
00:41:35,480 --> 00:41:40,600
pseudoscience problem, which I think you already kind of did outline quite, quite in detail. But

422
00:41:40,600 --> 00:41:46,840
then this little, little example you use on, on physiognomy. Yeah, well, part of it is this sort,

423
00:41:46,840 --> 00:41:53,000
this sort of runaway idea that we can do science without theory that picks up steam in the mid

424
00:41:53,000 --> 00:41:58,920
century. And then with the rise of machine learning techniques, and these being adopted

425
00:41:58,920 --> 00:42:04,520
widely, it just becomes, it becomes, it gets bundled into this hype narrative about how these

426
00:42:04,600 --> 00:42:12,840
technologies work. And then everyone sort of believes that these technologies are capable of

427
00:42:14,360 --> 00:42:24,120
extracting true knowledge of some natural system in virtue of having achieved high

428
00:42:24,120 --> 00:42:37,720
classifier accuracy on some natural data set, right? And what's actually happening is, is researchers

429
00:42:37,720 --> 00:42:44,040
are interpreting that pattern as having discovered precisely whatever their intuitive idea of what

430
00:42:44,040 --> 00:42:48,120
they were going to discover was beforehand. Because if they're not explicitly doing the

431
00:42:48,120 --> 00:42:53,640
theory, if they're not, if they're not explicitly theorizing, then they're implicitly theorizing,

432
00:42:53,640 --> 00:42:58,760
which means that they're effectively trying to con you into believing whatever their intuitions

433
00:42:58,760 --> 00:43:05,480
were at the start of the research procedure, without effectively having furnished evidence of

434
00:43:05,480 --> 00:43:12,600
that, besides having told you that they train some model, and there's some pattern in the data that

435
00:43:12,600 --> 00:43:21,240
satisfied some criteria, right, for success. But I think because of all these hype narratives

436
00:43:21,240 --> 00:43:30,920
surrounding machine learning, and again, not for substantive epistemic differences in how these

437
00:43:30,920 --> 00:43:38,520
statistical methods work, but rather for sociological reasons, you get a lot of bad, bad,

438
00:43:39,240 --> 00:43:50,760
bad science. So if I want to do a quantitative social science study, and I'm in a sociology

439
00:43:50,760 --> 00:44:00,760
department, or an economics department, my advisors won't let me do that until I've

440
00:44:00,760 --> 00:44:07,320
read up on the hundred years long history of scientists in my field having studied that

441
00:44:07,320 --> 00:44:13,160
exact problem, right. Machine learners on the other hand, machine learners don't even read the

442
00:44:13,160 --> 00:44:20,760
history of their own work. Machine, like, it is not typical for someone in machine learning to have

443
00:44:22,280 --> 00:44:30,200
even a five years deep understanding of the history of their own field, right,

444
00:44:31,400 --> 00:44:35,960
which is there are of course exceptions, but the general rule is people in machine learning do not

445
00:44:35,960 --> 00:44:42,040
read. Yeah. And so by you're talking, are you talking about more on the scientific side, or do

446
00:44:42,040 --> 00:44:48,600
you just mean general, you know, commercial ML engineering? Practitioners, but also in academics.

447
00:44:48,600 --> 00:44:58,360
Academics, okay. That's unfortunate. And so when you go to apply ML, you know, everything that's

448
00:44:58,440 --> 00:45:02,600
submitted to Art Tripoli, or, or NeurIPS, or what have you.

449
00:45:05,720 --> 00:45:14,680
ACM, you get just this glut of work of people applying the methods of ML, especially DL, to

450
00:45:15,560 --> 00:45:21,400
some problem that scientists have spent maybe hundreds of years working on. And there's no

451
00:45:21,400 --> 00:45:27,400
acknowledgement that what they're tackling, what they're attempting to tackle is a scientific problem

452
00:45:27,400 --> 00:45:36,360
that some very specific field of, you know, molecular biomechanicists or whatever the field is,

453
00:45:36,360 --> 00:45:42,920
right, have been working on for hundreds of years. And then there's this attitude that

454
00:45:43,640 --> 00:45:48,920
while deep learning will solve the problem, and I don't have to pay my dues and read about the

455
00:45:48,920 --> 00:45:55,800
methods in this field. And then the reviewing practices at, well, you know, part of it's like

456
00:45:55,800 --> 00:46:01,800
we got rid of, we got rid of traditional peer review and machine learning, which is like, was

457
00:46:01,800 --> 00:46:09,800
traditional peer review hopelessly broken? Yes. Did we introduce new problems by getting rid of it

458
00:46:09,800 --> 00:46:16,120
wholesale? Also, yes. Right. And so then you've got, you know, so there aren't standard journals in

459
00:46:16,120 --> 00:46:26,600
machine learning the way they are in biomechanics or biochemistry or socioeconomics or whatever,

460
00:46:26,600 --> 00:46:39,240
right? You are submitting to the big name machine learning conferences, but peer review there is

461
00:46:39,560 --> 00:46:51,160
I mean, I have to say it's pretty radically incumbent. I don't know that I don't think anyone

462
00:46:52,280 --> 00:47:05,640
who reviews for or submits to machine learning venues would try to fight me on that. I mean,

463
00:47:05,640 --> 00:47:14,360
I think the consensus is that the peer review process for these venues is wildly inadequate.

464
00:47:14,360 --> 00:47:19,320
Inadequate, yeah. So and it's because it's because when you apply machine learning,

465
00:47:20,840 --> 00:47:26,520
right, you're applying it to some domain, you're applying it to some domain where there is a vast

466
00:47:26,520 --> 00:47:33,080
history of people trying to solve some problem. And when you submit your little deep learning thing

467
00:47:33,080 --> 00:47:39,320
to IEEE and you're trying to tackle some problem in social science, they're not asking social

468
00:47:39,320 --> 00:47:46,920
scientists to review that. God, no. Right. They're asking other people who trained a, you know,

469
00:47:47,960 --> 00:47:55,560
transformer to with data of that shape, right? But they're not asking people who have the

470
00:47:55,560 --> 00:48:03,480
disciplinary knowledge to review the methods for what actually matters to doing science, right?

471
00:48:06,200 --> 00:48:12,680
Yeah. Yeah. And when do you think this changed? Well, was this, is this imminent to the

472
00:48:13,800 --> 00:48:19,640
practice itself? Or when do you think this change took place where the peer reviewing

473
00:48:19,640 --> 00:48:24,360
method became a bit lax or inadequate as you pointed out?

474
00:48:26,680 --> 00:48:26,920
Well,

475
00:48:33,960 --> 00:48:37,560
it was just sort of an organic thing, right? You have

476
00:48:40,040 --> 00:48:46,520
on the one hand, I mean, it was never the case that it was never the case that there were

477
00:48:47,160 --> 00:48:51,960
people applying machine learning, as far as I know, where it was never the case that people were

478
00:48:51,960 --> 00:48:56,680
applying machine learning to some problem in biology, and then submitting that to a biojournal.

479
00:48:56,680 --> 00:49:01,240
That's not the, I mean, occasionally that happens, right? But that's not standard practice. And I

480
00:49:01,240 --> 00:49:10,040
don't think it ever was. It was also not the case that there were kind of standard journals for,

481
00:49:10,040 --> 00:49:14,600
there were standard journals for stats, right? But there were never kind of standardized deep

482
00:49:14,840 --> 00:49:22,520
learning journals, right? There were, there were computing or stats

483
00:49:25,000 --> 00:49:33,320
conferences that got kind of evolved into ML specific conferences or new ML specific conferences

484
00:49:33,320 --> 00:49:39,720
emerged. And like, you know, a lot of the main ones are actually, they didn't start out as ML

485
00:49:39,880 --> 00:49:46,920
conferences, and they evolved to be ML conferences. But also you have at the same time the emergence

486
00:49:46,920 --> 00:49:58,360
of pre-printing servers. And so you get, you get the emergence of a new kind of, like machine

487
00:49:58,360 --> 00:50:04,440
learning has been machine learning, the methods we call machine learning have been around since

488
00:50:05,400 --> 00:50:11,800
like the 80s, right? You could trace it back earlier to kind of proto machine learning methods.

489
00:50:13,000 --> 00:50:18,200
Those go back much, I mean, again, like, it was out of World War II, it was out of the research

490
00:50:18,200 --> 00:50:24,920
at Los Alamos, that you got like MCMC sampling, right? Like Mark O'Chain Monte Carlo sampling,

491
00:50:24,920 --> 00:50:33,000
like Metropolis Hastings sampling. I didn't know that. Yeah. Yeah, it goes back to like the early

492
00:50:33,080 --> 00:50:39,080
50s, late 40s, early 50s. So even before that's kind of machine learning, right? Yeah.

493
00:50:42,040 --> 00:50:47,320
But, but, you know, machine learning as it's the early 2000s that machine learning kind of

494
00:50:48,120 --> 00:50:51,880
goes like, hey, we're a scientific field, or hey, we're an engineering discipline, like we're a

495
00:50:51,880 --> 00:50:58,360
discipline now, right? And it's at the same time that you're getting pre-printing servers as the

496
00:50:58,360 --> 00:51:02,600
sort of way that that stuff is disseminated. So there's, there's, effectively, there's no

497
00:51:03,560 --> 00:51:08,920
incentive to start journals, and there's incentive against starting journals, I would say. This is

498
00:51:08,920 --> 00:51:13,800
my, I'm making this up on the spot, but that's kind of how I would reconstruct that history.

499
00:51:13,800 --> 00:51:18,360
No, that makes sense. Yeah. I mean, it's partially contingent. It's just historically how things have

500
00:51:18,360 --> 00:51:21,880
been. Yeah. And also, there's this widespread recognition that there's something deeply broken

501
00:51:21,880 --> 00:51:28,280
about traditional peer review, which is true. Yeah. Which virtually, I believe all every academic

502
00:51:28,360 --> 00:51:33,000
I've spoken to has said that. So unequivocally, I think it's just a general consensus. Yeah. Yeah.

503
00:51:33,000 --> 00:51:37,800
Excellent. Excellent. Now, that's, that's, that's great, Mel. I want to be cognizant of the time,

504
00:51:37,800 --> 00:51:42,760
which is, which is why I want to get to this. And I'm sorry if this sounds like I'm flattering you,

505
00:51:42,760 --> 00:51:48,840
but your paper on the free energy principle, it is, I've probably read it like three or four

506
00:51:48,840 --> 00:51:55,080
times. And I think I can probably parrot out certain parts of it verbatim, because I've read it so

507
00:51:55,080 --> 00:51:59,480
many times, especially because I hate it. It's actually, it's a really good paper. It's a fantastic

508
00:51:59,480 --> 00:52:03,480
paper. It's a fantastic. And so now I'm trying to like, I'm like, okay, what, you know, I'm trying

509
00:52:03,480 --> 00:52:09,480
to write a subsequent paper. And it's like, it's not that good. I mean, because like it's, it's

510
00:52:09,480 --> 00:52:14,440
probably your most cited paper, right? I mean, I found you through this. In fact, like, I didn't

511
00:52:14,440 --> 00:52:20,040
even know who you are until I came across this work. Yeah. So just for the listener, it's called

512
00:52:20,120 --> 00:52:25,800
the math is not the territory navigating the free energy principle. Yeah, I mean, it's just,

513
00:52:25,800 --> 00:52:30,680
it's philosophically interesting. It's got so much into like the history of science. And then,

514
00:52:31,400 --> 00:52:36,760
you know, like, what is formalization? You speak about the structure like, yeah, he keeps a lot

515
00:52:36,760 --> 00:52:41,960
to discuss here. Although, although before we get to the, let's say the nitty-gritty, and one thing

516
00:52:41,960 --> 00:52:46,680
I want to mention is, so I've been trying to get through this, this book on active inference. And

517
00:52:46,680 --> 00:52:53,960
I've got to say, because when I'm reading this, every page, I'm kind of reading it in a way through

518
00:52:53,960 --> 00:52:59,720
the kind of the lens of what you put in, in me through this paper, you know, it's like, I've

519
00:52:59,720 --> 00:53:07,000
got a bias now, I've got the, I've got the, the math is not the territory bias. Because it really

520
00:53:07,000 --> 00:53:12,120
helped me understand. Yeah, it really did help me understand the ontology of what really is the

521
00:53:12,200 --> 00:53:17,000
free energy principle, because it's got so much interest. So many people talk about it. And,

522
00:53:17,000 --> 00:53:23,400
you know, Carl Friston, he's fantastic. I've learned so much, but he sometimes isn't the

523
00:53:23,400 --> 00:53:29,080
best elucidator, you know, like he, when it comes to, you know, he's not a philosopher. Correct. I

524
00:53:29,080 --> 00:53:36,520
think that could be the reason Einstein was a philosopher. Carl Friston is like Isaac Newton,

525
00:53:36,520 --> 00:53:44,920
in that he's kind of like, I don't care what's like, he'll ascent to any metaphysics,

526
00:53:44,920 --> 00:53:49,400
Newton would ascent to any Newton was like, I'm not doing metaphysics. I'm associating

527
00:53:51,560 --> 00:53:58,440
relationships I see in data. Right. But don't tell me about the physical seat of gravity. I'm

528
00:53:58,440 --> 00:54:02,120
not talking about that. Yeah. Oh, yeah. I mean, that certainly wasn't wasn't a dig at

529
00:54:03,080 --> 00:54:07,400
Professor Friston, because he he himself says, he says, yeah, I'm not a philosopher, I'm a

530
00:54:07,400 --> 00:54:12,520
scientist. And I don't really, when he doesn't even really take a philosophical position or

531
00:54:12,520 --> 00:54:20,280
metaphysical position pertaining to the FEP. But but having said that, Mel, so, oh, sorry,

532
00:54:20,280 --> 00:54:27,160
be that as it may, I mean, what, what, why, yeah, why do you think there's such deep philosophical

533
00:54:27,160 --> 00:54:33,400
interest in the free energy principle of every any philosopher who works in the philosophy of

534
00:54:33,400 --> 00:54:39,560
biology or the cognitive scientist or ML, but it's such deep philosophical interest. So, yeah,

535
00:54:40,440 --> 00:54:45,800
why do you think the reason for that is, I think there are a lot of reasons one of them is, okay,

536
00:54:45,800 --> 00:54:53,080
so I think, I think of all math, all applied mathematics, or scientific math or models,

537
00:54:53,080 --> 00:54:57,880
as a kind of thinking tool, math is a thinking tool for science, right. But

538
00:54:59,720 --> 00:55:05,400
the free energy principles are thinking tool in a different way in that it's not actually meant to

539
00:55:06,200 --> 00:55:13,640
be placed in contact with empirical data, right, it's just about enabling us to conceptualize

540
00:55:13,640 --> 00:55:21,800
of target phenomena and new ways. And it happens to be new ways that are actually really

541
00:55:24,040 --> 00:55:33,480
a philosophically generative and novel to suck to agree. Not exactly novel, but overlooked. I mean,

542
00:55:33,480 --> 00:55:41,320
you get some of like, some of what's being said about life. It's if you look hard enough, it's

543
00:55:41,320 --> 00:55:50,280
really in Schrodinger's, Schrodinger's what is life. You look at that text, there's a lot of the

544
00:55:50,360 --> 00:55:55,560
ideas that are being brought out in the FEP in that text originally.

545
00:55:57,880 --> 00:56:03,560
Yeah, he brings up new ideas go back, but largely ignore, because they're very much

546
00:56:04,680 --> 00:56:11,080
opposed to the kind of neo Darwinian canon that we have now in biology, where we're really looking

547
00:56:11,080 --> 00:56:20,520
at population level analysis. And you're not looking at physical exigencies or structural

548
00:56:20,520 --> 00:56:26,840
exigencies of biology at the physical systems involved in biology and what kinds of necessities

549
00:56:26,840 --> 00:56:35,800
need to be there for life to exist. And then connecting that up to a view of cognition as

550
00:56:35,800 --> 00:56:41,720
fundamentally oriented towards action and interaction with the environment too. I mean,

551
00:56:41,720 --> 00:56:51,720
that's also there. So there's a lot that's philosophically rich that is associated with the FEP

552
00:56:51,720 --> 00:56:57,880
in how the FEP is discussed. It's not necessitated by the FEP, the FEP is just math, but it's math

553
00:56:57,880 --> 00:57:04,200
that allows us to conceptualize of things that we're taking away. It's also cool math, it's fun

554
00:57:04,200 --> 00:57:08,840
math. Part of it is there's a lot of flourished that math that doesn't need to be there. That's

555
00:57:08,840 --> 00:57:18,440
just it just if you like math, it's it's cool. And you can keep pushing it new cooler. It's

556
00:57:18,440 --> 00:57:26,280
kind of it's it's like this magpied. It's like a bunch of shiny math taken from 18 difference

557
00:57:26,840 --> 00:57:32,520
distinct. You know, you've got you've got some of its machine learning math.

558
00:57:33,480 --> 00:57:39,000
Like some of it just is it just is elbow, right, to minimize free energy, the quantity known as

559
00:57:39,000 --> 00:57:44,040
free energy as a kind of information theoretic construct to minimize free energy is is to just

560
00:57:44,920 --> 00:57:49,080
optimize the evidence lower bound, which is elbow, which is machine learning technique, right?

561
00:57:49,640 --> 00:57:56,280
Um, it's also Fokker Planck, or the master equation, or the Kolmogorov forward equation,

562
00:57:56,280 --> 00:58:05,480
which is, you know, a principle of stat mech. It's also, I connected up to Max Ent,

563
00:58:05,480 --> 00:58:10,680
maximum entropy principle, which is a sort of, James,

564
00:58:11,320 --> 00:58:22,680
put forward this idea that we can view. Basically, the core principles of thermodynamics,

565
00:58:22,680 --> 00:58:31,400
expressed statistically, can also be reoriented as a kind of

566
00:58:32,280 --> 00:58:45,800
epistemic principle for like keeping your priors flat, basically, except for when the evidence.

567
00:58:46,920 --> 00:58:52,520
Does that make sense? No, I think I got the latter bit of it regarding the epistemic principles.

568
00:58:52,600 --> 00:58:58,920
Yeah. Yeah. So, James in the, is this 50s? Max Ent, James.

569
00:59:01,240 --> 00:59:02,920
When's the first Max Ent paper?

570
00:59:05,240 --> 00:59:10,520
50s, right? Yeah, it's a 50. It's 57. It's 57.

571
00:59:13,560 --> 00:59:17,240
Yeah. So, the idea is that. Yeah.

572
00:59:17,960 --> 00:59:22,120
Yeah. 57. That's right. Yeah. Yeah. BT James. Two papers.

573
00:59:23,960 --> 00:59:30,200
So, it's like a closed thermodynamic system. The principles of thermodynamics tell us that

574
00:59:30,200 --> 00:59:37,640
a closed thermodynamic system will max out its entropy, right? Like, that's,

575
00:59:40,600 --> 00:59:43,960
this is what thermodynamics tells us. At a certain point, yeah, of course. Right.

576
00:59:44,680 --> 00:59:50,760
Ultimately, in the limit. You can give this

577
00:59:52,920 --> 01:00:02,600
a gloss as a principle for best inference. Like, it's already probabilistically formulated,

578
01:00:02,600 --> 01:00:16,520
but then view that as a rule for governing the probability distribution over some belief.

579
01:00:18,200 --> 01:00:26,840
And it's, again, Max Ent, because your priors need to be as flat as,

580
01:00:26,840 --> 01:00:32,120
as the probability distribution is as flat as it can be, given the evidence, right?

581
01:00:33,240 --> 01:00:37,480
So, there's.

582
01:00:40,440 --> 01:00:46,600
And you're saying the FAP does bring this into its, it's theorizing too, like.

583
01:00:47,320 --> 01:00:52,600
But they did. I don't think they did. I think I pointed it out, and then they started to,

584
01:00:54,280 --> 01:00:58,680
so there's, their recent work is actually explicitly incorporating the James stuff.

585
01:00:58,760 --> 01:01:03,800
Okay. Fascinating. Yeah. I think it was actually because I was like, so James, right?

586
01:01:05,480 --> 01:01:09,000
That's fascinating. I didn't know that, because I mean, I mostly view it through,

587
01:01:09,000 --> 01:01:13,560
you know, predictive coding and Bayesian reasoning. I mean, really, that's.

588
01:01:13,560 --> 01:01:20,200
I actually doubted that lineage. I doubted that lineage, but I thought that they've

589
01:01:20,200 --> 01:01:25,800
made that connection explicitly beforehand. But the people who are now explicitly doing the

590
01:01:25,800 --> 01:01:31,640
Max Ent incorporating with FEP are attributing it to me. So I'm like, you know, it's if he wants.

591
01:01:32,440 --> 01:01:37,720
Okay. Yeah. Are there any, because like, I mean, I'd love to read up on this. Are there any papers?

592
01:01:37,720 --> 01:01:40,520
Because I mean, I've only just started on the book. This is the.

593
01:01:40,520 --> 01:01:43,800
I think the best. Okay. So, so the, Tom Parr is fucking excellent, but.

594
01:01:44,360 --> 01:01:48,040
And all of those people who wrote that book are excellent, but I think.

595
01:01:48,040 --> 01:01:49,720
They're very good writers too. Yeah.

596
01:01:50,680 --> 01:01:51,960
I think the most competent.

597
01:01:54,520 --> 01:01:59,960
Mathematician who's working within the free energy framework is Dalton.

598
01:02:01,960 --> 01:02:05,400
Okay. I got a, who I spell that.

599
01:02:06,360 --> 01:02:08,440
Do you know how to spell Tamil names? Yeah.

600
01:02:09,880 --> 01:02:14,280
The first name is very Scottish, which is a little well. Okay. There we go. Yeah.

601
01:02:15,160 --> 01:02:24,040
Um, so actually they, they took it from, so they took the Max Ent. I, so I proposed like, hey,

602
01:02:24,040 --> 01:02:30,040
you know, well, what the FEP is kind of is you're, you're, you're taking principles of statistical

603
01:02:30,040 --> 01:02:37,000
mechanics and you're epistemic sizing them. You're giving them an inferential reading.

604
01:02:37,640 --> 01:02:41,080
You're taking laws, laws of stats. Yeah. Yeah.

605
01:02:41,080 --> 01:02:46,360
And you're reading them as principles, as principles for.

606
01:02:48,920 --> 01:02:55,000
Cognition. Yeah. Yeah. I mean, I mean, I, I, I, I, I'd say for me initially when I came

607
01:02:55,000 --> 01:02:58,200
across FEP, that's what I found quite interesting where, you know,

608
01:02:58,200 --> 01:03:02,680
Carthus and his background is even in physics and like you take from statistical mechanics,

609
01:03:02,680 --> 01:03:07,480
which is in physics. I think, I think he did a bachelor's in physics in like the 60s.

610
01:03:07,480 --> 01:03:11,160
Yeah. I shouldn't say background. You're right. I think he studied physics. He certainly is a

611
01:03:11,160 --> 01:03:17,400
neuroscientist, but, but like, I think he does like the physics he's getting is largely outdated.

612
01:03:18,920 --> 01:03:22,840
Well, I think he's just drawing from a lot of areas of, of physics.

613
01:03:25,400 --> 01:03:31,320
And really it's like, he's in clinical neuro, right? He's, he's spent a career in clinical

614
01:03:31,400 --> 01:03:36,920
neuroscience. Well, I mean, generally what interested me was that the fact that you take these

615
01:03:37,480 --> 01:03:41,320
theories and principles from physics, like, you know, all from statistical mechanics or,

616
01:03:41,320 --> 01:03:47,960
as you point out, like, you know, max and, and then apply them to move to like cognition,

617
01:03:47,960 --> 01:03:54,200
reasoning, Bayesian inference and the likes. I just, I don't know. I find that fascinating.

618
01:03:54,200 --> 01:03:59,640
And I don't know. I just feel like I, again, as a neophyte, just, it just excites me to see where

619
01:04:00,360 --> 01:04:06,120
the, the developments that goes on the FEP and the, you know, the concomitant, more engineering

620
01:04:06,120 --> 01:04:14,280
work that comes along with it. Having, having said that, Mel, one thing is this, in your paper,

621
01:04:14,280 --> 01:04:19,160
you say the math is not the territory. And I think here's where we get to the, really, the,

622
01:04:19,720 --> 01:04:23,640
the crux of the argument, the philosophy of this paper. And I'm just going to read out a bit of

623
01:04:23,640 --> 01:04:28,680
an excerpt. I think it's valuable for the, for the listener in case they haven't read it already,

624
01:04:28,680 --> 01:04:34,520
which I, I, if you're interested in the FEP or ML, for that matter, I highly recommend reading

625
01:04:34,520 --> 01:04:39,000
this. So you just, you claim here, I think this is from the abstract. I'm not sure. Anyway, I've

626
01:04:39,000 --> 01:04:44,600
got this excerpt here. Conceptual verification is a common ailment of scientific modeling.

627
01:04:45,160 --> 01:04:52,520
It is particularly likely to occur in cases in which models have somewhat convoluted histories.

628
01:04:53,160 --> 01:04:55,960
Rification involves, and here's the important bit, in fact,

629
01:04:56,600 --> 01:05:03,240
verification involves mistaking an aspect of a model, its structure, its construal, or the union

630
01:05:03,240 --> 01:05:11,720
of both, for an aspect of the empirical of the natural world, mistaking the math for the territory,

631
01:05:11,720 --> 01:05:19,480
so to speak. So yeah, could you please elaborate on that little statement and then, you know,

632
01:05:19,480 --> 01:05:28,200
kind of connect that to the FEP? Yeah, so I think, I think there's a tendency to

633
01:05:32,120 --> 01:05:39,800
conflate scientific realism with realism about the conceptual tools that we use in science.

634
01:05:39,800 --> 01:05:45,560
So I'm, I'm a full-blooded scientific realist. I think, albeit a pragmatic realist.

635
01:05:46,520 --> 01:05:51,080
I'm somewhat of a content. I think I'm more of a real, I think I'm more of a realist-realist

636
01:05:51,080 --> 01:05:54,680
than most people who call themselves pragmatic realists. I think people who call themselves

637
01:05:54,680 --> 01:05:57,320
pragmatic realists, I'm like, you're not really a realist. I'm actually a realist,

638
01:05:57,320 --> 01:06:03,240
just pragmatically so. No, I mean, I would say science without a doubt, it does give us

639
01:06:03,240 --> 01:06:13,560
truths about reality. I mean, I, in the content sense, let's say. Yeah, whatever the truth knowledge,

640
01:06:13,560 --> 01:06:19,480
whatever the epistemic goods of science are, I want to say it's truth sub-pragmatic or knowledge

641
01:06:19,480 --> 01:06:27,160
sub-pragmatic, right? I think, I think there's, it's not absolute, all-encompassing, omniscient

642
01:06:28,840 --> 01:06:36,840
truth about nature. It's, it's pragmatic truth. It has its limits and it's oriented towards us

643
01:06:36,840 --> 01:06:41,560
as the kinds of beings that we are having to occupy the world and navigate it the way that we do,

644
01:06:41,560 --> 01:06:46,440
right? But I think it is truth. I mean, if anything is truth or knowledge, that's, that's

645
01:06:46,440 --> 01:06:52,440
what that is. Exactly, with all those caveats, it gives us truths, let's say. Yeah, yeah, yeah.

646
01:06:52,440 --> 01:07:01,400
But then we introduce all this conceptual machinery into science to be able to do science,

647
01:07:02,440 --> 01:07:10,440
like the idea of a Hamiltonian or the idea of a gravitational field or the idea of a fermion

648
01:07:10,440 --> 01:07:19,000
or the idea of, you know, a force function. And that includes all the, you know, the

649
01:07:19,000 --> 01:07:22,600
Fokker-Planck equation that includes all the sort of mathematical infrastructure too.

650
01:07:22,600 --> 01:07:36,760
These are all the conceptual tools of science. And I think that there's a lot of slippage that's,

651
01:07:37,560 --> 01:07:44,920
that's like, well, science delivers truth. Therefore, all the conceptual tools we introduce

652
01:07:44,920 --> 01:07:50,840
in the process of doing science and getting to that truth are also true and real, in some sense.

653
01:07:50,840 --> 01:07:59,000
And I'm like, no, that requires a lot of careful work at the end of the day. At the final kind of

654
01:07:59,000 --> 01:08:04,920
interpretive stage of a scientific procedure, you ask yourself, do we think strings are real?

655
01:08:04,920 --> 01:08:11,800
Do we think that quantum fields are real? Do we think that Hamiltonians are real? Or was this

656
01:08:11,800 --> 01:08:15,720
just sort of a calculational device? And I think there's a lot of, in between, between it's real

657
01:08:15,720 --> 01:08:21,640
and it's a calculational device. But I don't think at the end of the day that's a distinction that

658
01:08:21,640 --> 01:08:34,200
can be fully upheld. But there's a tendency, and I think it's true, even among careful philosophers

659
01:08:34,200 --> 01:08:48,200
to attribute realism or truth to, to try to make the conceptual tools of science truth out

660
01:08:48,200 --> 01:08:50,840
when they ought not to be read as truth out.

661
01:08:50,840 --> 01:08:54,840
Yeah, when it's a thing itself. I mean, it depends, right? For instance,

662
01:08:54,840 --> 01:08:59,800
I've spoken to a few mathematicians on this podcast, one person who I, as I came that comes to

663
01:09:00,280 --> 01:09:07,160
shoulder with Hamkins, he's a Platonist. So for him, of course, yes, FEP, if it's a mathematical

664
01:09:07,160 --> 01:09:13,000
structure, it could really exist in like a platonic sense. But in your paper, you're talking

665
01:09:13,640 --> 01:09:19,720
more in regards to our physical world, what science studies, you know, you're not talking

666
01:09:19,720 --> 01:09:24,200
about like a platonic realm. I don't know what your views are on Platonism.

667
01:09:24,200 --> 01:09:33,240
So with math, I think there's a lot of accounts of how math works in science that are, whether

668
01:09:33,240 --> 01:09:39,320
explicitly or not, deeply committed to Platonism, or something akin to it. And

669
01:09:42,360 --> 01:09:50,600
for me, I'm like, if your account of how natural science works, depends on assuming

670
01:09:51,400 --> 01:09:57,160
the most kind of industrial strength metaphysics possible. It's not a good account of natural

671
01:09:57,160 --> 01:10:03,080
science. Like I'm an, I believe in naturalism. I'm a naturalist. And a lot of naturalists,

672
01:10:03,720 --> 01:10:08,520
a lot of proclaimed naturalists happen to also be Platonists. And I'm like, that's not real

673
01:10:08,520 --> 01:10:14,760
naturalism, right? You need an account of science that assumes as little in the way of metaphysics

674
01:10:14,760 --> 01:10:21,160
as possible. And is cognizant of what metaphysics it does assume to the extent that you can't get

675
01:10:21,160 --> 01:10:29,240
away from that. Yeah, that's why you mentioned all the, all the presuppositions that your

676
01:10:29,240 --> 01:10:37,560
realism is based on, right? Right? Yeah. For me, it's like the word, the word dog or the word

677
01:10:37,720 --> 01:10:46,600
mitochondrion, right? Do we think the word dog exists? It's like,

678
01:10:49,560 --> 01:10:57,240
I think dogs exist. Yeah, yeah. I think that there's a class of natural entities that is well

679
01:10:57,240 --> 01:11:03,320
carved out by our best science. And we refer to it via this concept that has this label attached

680
01:11:03,320 --> 01:11:09,480
called dog. Yeah. But I don't think, I think the word exists as a cultural mental artifact.

681
01:11:09,480 --> 01:11:13,800
But I don't think it exists mine independently. And I feel the same about math and science. It's

682
01:11:13,800 --> 01:11:19,720
like. Yeah, sort of interrupt, but I think, I think I agree with you. I mean, I'm, I'm still,

683
01:11:19,720 --> 01:11:24,200
I hate to use this like cheap like agnosticism, but I'm kind of agnostic as to mathematical

684
01:11:24,200 --> 01:11:29,320
Platonism. Although, because I kind of view it in the way I think John Peer Jay said that,

685
01:11:29,880 --> 01:11:34,600
and if you think about it all of mathematics, it's just, it's semiotics and linguistics.

686
01:11:35,800 --> 01:11:41,320
The thing though is it kind of does, it has to blow our minds as to how consistently it's worked.

687
01:11:41,320 --> 01:11:48,760
I mean, at a pure pragmatic level, it is rather bizarre that there is this internal consistency

688
01:11:48,760 --> 01:11:54,440
in mathematics. So I'm still unreasonable efficacy. Exactly. Yeah. Yeah. I mean, you can't

689
01:11:55,000 --> 01:12:00,440
necessarily say that therefore that can be true in some fundamental metaphysical sense,

690
01:12:00,440 --> 01:12:09,880
but it does make one wonder. Yeah. So back to the idea, you know, in the, in the paper,

691
01:12:09,880 --> 01:12:16,280
I kind of want to also discuss in this paper, also you do. Oh, I think I'm, I made a mistake

692
01:12:16,280 --> 01:12:22,520
there. I'm confusing papers. Do you, in this, in the math, it's not the territory. Do you discuss

693
01:12:23,480 --> 01:12:31,240
the, the natural selection, like you kind of contrast or juxtapose the FEP with natural selection,

694
01:12:31,240 --> 01:12:34,760
or could it be the other paper you wrote, which was more like an introduction to

695
01:12:35,800 --> 01:12:39,720
the paper, the free-range principle and accessible introduction to its derivations,

696
01:12:39,720 --> 01:12:47,240
implications and applications? I don't know. Good question. Yeah. Yeah. Because regardless,

697
01:12:47,240 --> 01:12:53,240
what I, what I also liked was, and this probably connects to my previous, my initial question,

698
01:12:53,240 --> 01:13:00,200
why a lot of philosophical interest in the FEP, it's just, yeah. So like the FEP, is it like,

699
01:13:00,200 --> 01:13:05,800
is it like, is it like a meta narrative or like a meta theory, or it's called a super theory,

700
01:13:05,800 --> 01:13:14,600
not a meta theory, a super theory, something like natural selection. Yeah, or like almost an intuition

701
01:13:14,600 --> 01:13:19,160
pump? Yeah, I think I prefer that. Because you can't really, it doesn't really fall into the

702
01:13:19,160 --> 01:13:23,800
Hopperian falsifiability and, you know, in that rigorous sense, right? I should say maybe in it,

703
01:13:23,800 --> 01:13:31,480
that, that was sort of a, I'm sort of adapting the Dennett concept. That's, that's not exactly,

704
01:13:32,280 --> 01:13:38,040
that's not how he uses it. But, but one could say it's an intuition pump in a stretch of that

705
01:13:38,040 --> 01:13:48,680
terminology. I see. In that. It's, it's a kind of conceptual framework

706
01:13:50,760 --> 01:13:59,800
that enables us to think about things very differently in science. But it's not actually,

707
01:13:59,800 --> 01:14:08,920
it's not a method. It's not a formal model. It's not a theory or a hypothesis or a, you know,

708
01:14:08,920 --> 01:14:12,200
there are all these sorts of concepts, there are all these conceptual tools in science and it's,

709
01:14:13,080 --> 01:14:18,680
it's not any of the kind of lower order conceptual tools of science in the same way that natural

710
01:14:18,680 --> 01:14:24,680
selection is not, I don't view it as a theory. Natural selection isn't a theory or a hypothesis

711
01:14:24,680 --> 01:14:33,400
or a model or a, I mean, it's really sort of a meta conceptual framework, I would say.

712
01:14:33,400 --> 01:14:37,480
Yeah. Yeah. Yeah. And I think in that sense, it's very far removed from the actual

713
01:14:39,080 --> 01:14:44,920
day-to-day work of science. Of scientists. Yeah. Yeah. Experimental work, for instance. Yeah.

714
01:14:44,920 --> 01:14:52,200
And I think in, you know, part 1.5, you essentially say, I propose here that we reserve the free

715
01:14:52,200 --> 01:15:00,360
energy principle to denote only the maths of which the FEP is composed of models utilizing this

716
01:15:00,360 --> 01:15:06,760
formalism to study natural systems, bear also an interpretation, lending a means of interpreting

717
01:15:06,760 --> 01:15:13,800
the maths as about the systems in nature. So, I mean, that idea of, yeah, it's kind of a formalistic

718
01:15:13,800 --> 01:15:22,040
conceptual structure. I like thinking of it that way. Excellent. So, okay, let's see.

719
01:15:22,200 --> 01:15:26,040
In terms of time, yeah, we've got a few more. Is it okay if you go for about 10 more minutes?

720
01:15:26,040 --> 01:15:29,320
Yeah, yeah, totally. All right. All right. Fantastic. Thanks. Thank you, Mel.

721
01:15:29,320 --> 01:15:33,160
I kind of... I'm happy to chat again at some point if that's...

722
01:15:33,880 --> 01:15:39,240
Yes. Yes. I mean, in fact... I just have to run at like, you know, I have to run in 15 minutes, but...

723
01:15:39,240 --> 01:15:43,560
Yeah. Just 10 more minutes off your time, only because one thing I really try to do with this

724
01:15:43,560 --> 01:15:47,560
podcast is, I don't want to say a bridge, but I try to bring into dialogue

725
01:15:48,280 --> 01:15:54,440
a so-called continental philosophy and more of the analytic philosophy that kind of you work through.

726
01:15:54,440 --> 01:16:01,960
I should say, I studied with Dennett in my undergraduate and it was forbidden to me to touch

727
01:16:01,960 --> 01:16:08,360
continental philosophy. Yeah. You see, this is, you know, kiss and point. And I don't like that,

728
01:16:08,360 --> 01:16:11,960
to be honest. I don't even like creating this demarcation, really. It's just philosophy. It's

729
01:16:12,200 --> 01:16:15,160
thinking of a philosopher as a philosopher as far as I'm concerned.

730
01:16:15,160 --> 01:16:23,720
See, he knew my mind and he knew what reading Foucault would have done to me and it was nothing good.

731
01:16:24,520 --> 01:16:29,880
Yeah. There you go. You know, you start talking about the epistemes and all of that. Yeah. Yeah.

732
01:16:29,880 --> 01:16:36,840
But what's fascinating is like, look at how much of, you know, fruitful, productive output you've had,

733
01:16:37,640 --> 01:16:45,880
if I may, because you do take in the sociological aspects that Foucault, perhaps above all,

734
01:16:45,880 --> 01:16:53,640
point out very carefully when it pertains to our knowledge and epistemology. But yeah,

735
01:16:53,640 --> 01:17:00,600
on that note, let me just ask a very, very broad general question, Amel. As a philosopher of AI,

736
01:17:00,680 --> 01:17:07,240
as a philosopher of science, what insights do you think us in the, let's say, Anglo-American,

737
01:17:07,240 --> 01:17:13,080
you know, I'm in Australia. Are you Canadian or are you in the U.S.? U.S. Right. So yeah,

738
01:17:13,080 --> 01:17:20,680
the Anglo-American kind of. Yeah, there you go. The more analytic, you know, oriented philosophy,

739
01:17:21,560 --> 01:17:28,520
which, when I spoke to this philosopher called Simon Critchley, he said it's more for just a

740
01:17:28,600 --> 01:17:35,800
historical phenomena. It's got, it has nothing to do with really with ideas per se. It was just,

741
01:17:36,440 --> 01:17:43,800
you know, how things changed with the world, with World War II and how that, you know,

742
01:17:43,800 --> 01:17:48,920
certain analytic philosophers, it's in political, it was more of a political phenomena, more than a

743
01:17:48,920 --> 01:17:56,200
philosophical phenomena in history. But, you know, be that as it may, yeah, as a, as a philosopher of AI

744
01:17:56,200 --> 01:18:03,240
and a philosopher of ML, what insights do you think we can gain from kind of discourse within

745
01:18:04,120 --> 01:18:07,880
continental philosophy with thinkers all the way from thinkers like Heidegger

746
01:18:08,600 --> 01:18:14,200
or to people like Zizek and to, you know, sociologists like Foucault?

747
01:18:14,920 --> 01:18:25,000
Um, I don't delve into that work very much, but I know people who do and they,

748
01:18:26,920 --> 01:18:34,120
I think, make very interesting work of it. Elmer Feiden is doing, is more,

749
01:18:35,320 --> 01:18:40,840
he's a philosophy of AI, is more, is more, is more continentally influenced.

750
01:18:41,640 --> 01:18:46,120
I haven't heard of that person. Fantastic. Okay, thanks for that.

751
01:18:48,520 --> 01:18:52,040
Um, he will know other people.

752
01:18:54,600 --> 01:19:01,880
Yeah, yeah. There's also some, I think, I mean, I think there's a way in which sort of STS scholarship

753
01:19:01,880 --> 01:19:12,920
on technology, AI, ML is also more continentally leaning.

754
01:19:14,600 --> 01:19:20,120
Well, this is a book that I've just started on called The Critical Theory and AI,

755
01:19:21,000 --> 01:19:30,440
which I'm hoping to speak to, it's by Simon, Simon Lindegren. I just started on the books. I

756
01:19:30,440 --> 01:19:36,200
can't really say much on it. It was just published last year. Yeah, we just put out this, this,

757
01:19:38,520 --> 01:19:45,800
the pseudoscience paper. And a lot of the reviewers were suggesting some sort of like

758
01:19:45,800 --> 01:19:51,400
critical theory. Oh, you mean the one that goes in the machine learning paper? Yeah, our reviewers

759
01:19:51,400 --> 01:19:56,840
were suggesting some of that stuff. Fascinating. Yeah, yeah. I mean, yeah, which is like for me and,

760
01:19:56,840 --> 01:20:01,880
you know, leaving aside the names, like just generally for you, as you pointed out, you know,

761
01:20:01,880 --> 01:20:08,040
Dennett said, you know, no touching, Corneal Philosophers, but just, I'm curious, well,

762
01:20:08,040 --> 01:20:15,560
why did you rebel against, I'm joking, of course, but your master's commands and kind of get interested

763
01:20:15,560 --> 01:20:20,440
in, because I've seen through your Twitter feed, for instance, that you do, you do retweet stuff by

764
01:20:21,080 --> 01:20:28,280
some Corneal Philosopher, by Zizek, or you do, you aren't totally a verse to it or a post to it.

765
01:20:29,240 --> 01:20:38,920
Yeah, I mean, I think the divide is, sorry. I think, again, the divide is pretty superficial,

766
01:20:39,880 --> 01:20:44,680
as you said. And I think,

767
01:20:48,600 --> 01:20:53,480
as I said, like with everything, most of everything is, this is again,

768
01:20:59,160 --> 01:21:07,160
a Dennettism, but 99% of everything is crap was his life. And I think that's true of both

769
01:21:07,160 --> 01:21:12,840
analytic and continental philosophy. There's a dominant tradition. There are aspects of, like,

770
01:21:12,840 --> 01:21:19,640
if we think of analytic philosophy and continental philosophy as methods, I think there are really

771
01:21:21,800 --> 01:21:27,000
problematic features of both of those methodologies as methodologies for philosophy.

772
01:21:27,000 --> 01:21:32,520
I think there are really sort of stultifying features of both of those methodologies. And

773
01:21:33,080 --> 01:21:41,400
there's brilliant work. There's brilliant ideas coming out of both traditions. But in a way,

774
01:21:42,520 --> 01:21:48,280
it's working against some of the kind of primary tendencies of those traditions, in a sense.

775
01:21:51,480 --> 01:21:57,080
And so in my work, I'm just trying to say something

776
01:21:57,160 --> 01:22:05,800
interesting that matters for the reality we live in. And I'm trying as much as possible not to be

777
01:22:08,680 --> 01:22:17,240
operating within a particular conception of what good thought is. I mean, I think

778
01:22:18,120 --> 01:22:20,920
I think thinking according to some

779
01:22:23,080 --> 01:22:31,320
conception of what is the right way to think is not a right way to think. I think it

780
01:22:33,640 --> 01:22:38,040
keeps us short of discovering truths.

781
01:22:39,000 --> 01:22:43,160
So I try to not

782
01:22:47,240 --> 01:22:54,440
adhere as much as possible to principles of particular genres of philosophy,

783
01:22:54,440 --> 01:22:59,720
in so much as I can do that and still get published enough in philosophy venues to get a job.

784
01:23:00,840 --> 01:23:05,240
That's a whole other conversation. Just on a side note, Mel, are you familiar with

785
01:23:06,200 --> 01:23:14,200
the thinker, Bianchel Hahn? No. Yeah, so he wrote this book, Psychopolitics. Where is it?

786
01:23:15,320 --> 01:23:21,960
He's quite big in Germany. He's a German philosopher. In fact, no, he's originally from

787
01:23:21,960 --> 01:23:29,800
South Korea, but now he works in Germany. And I was, when I read your paper, in fact, when I read

788
01:23:30,760 --> 01:23:40,440
the theory free ideal paper, and then also the recent one, the pseudoscience one,

789
01:23:40,440 --> 01:23:47,480
he has one chapter in this book, Psychopolitics, on big data. And he wrote this in like the early

790
01:23:47,480 --> 01:23:55,080
2000s. A lot of this, I'd love to, in fact, as you said, you'll be open to another conversation,

791
01:23:55,080 --> 01:24:01,640
I'd love to kind of discuss. It's just like 15 pages, that chapter, that just his ideas on it,

792
01:24:01,640 --> 01:24:06,760
because I think you'd be the perfect person to comment on it with your kind of analytic background

793
01:24:08,680 --> 01:24:14,600
with the work you've done, because it really did remind me of his ideas when I read that,

794
01:24:14,600 --> 01:24:21,480
when I read your work, in any case. But yeah, again, I want to be cognizant of your time. So

795
01:24:21,480 --> 01:24:27,800
just one last question, Mel. What would you say, currently, I just like asking this,

796
01:24:27,800 --> 01:24:34,440
because it kind of excites me as to what kind of work I should do. In this space of philosophy of

797
01:24:34,440 --> 01:24:42,440
ML and philosophy of AI, apart from what we've discussed so far, what other avenues for research

798
01:24:42,440 --> 01:24:47,720
do you think people should look into and take into consideration and start thinking about?

799
01:24:48,520 --> 01:24:55,240
Oh, God, there are a million. I think there are a million avenues that are

800
01:24:56,360 --> 01:25:03,240
pursued or could be pursued that I think are kind of fruitless. But there are a million avenues that

801
01:25:03,240 --> 01:25:14,200
are really fascinating. So I mean, I think the way machine learning technologies are being adopted

802
01:25:14,200 --> 01:25:25,960
and changing labor and changing the way these technologies are changing our relationship to

803
01:25:29,000 --> 01:25:35,800
the sort of resource consumption to the means of production, to the output of labor, to the

804
01:25:35,800 --> 01:25:42,840
output of intellectual labor, or epistemic labor, one might say, which are, I think,

805
01:25:42,840 --> 01:25:56,920
much more subtle than we've given credit to so far. There's a problem of opacity or

806
01:25:56,920 --> 01:26:02,200
explainability or what have you. I think it's clear that the sort of research avenues that

807
01:26:02,200 --> 01:26:12,040
we've been on with that are deeply flawed, and the kind of technologies we've invented to tackle

808
01:26:12,040 --> 01:26:19,880
those problems in so much as they are problems are deeply flawed. But figuring that out is usually

809
01:26:19,880 --> 01:26:33,640
important. How these technologies are bringing about harms in the real world is a tremendous issue.

810
01:26:33,640 --> 01:26:43,240
And tackling that in a way that is deeply cognizant of incentive structures.

811
01:26:48,680 --> 01:26:58,440
And investing in interventions that those in the positions of power to create and deploy these

812
01:26:58,520 --> 01:27:06,440
technologies would actually be incentivized to adopt and working on that incentivization.

813
01:27:08,600 --> 01:27:13,880
So this requires a vastly interdisciplinary perspective. This requires having whatever

814
01:27:13,880 --> 01:27:22,520
the domain of application is, if that's a hospital, if that is in biological science,

815
01:27:22,520 --> 01:27:29,880
if that is in building cell phone applications, if that is it, whatever the application is,

816
01:27:29,880 --> 01:27:36,440
having deep domain knowledge of that application, having knowledge of the regulatory structures

817
01:27:36,440 --> 01:27:43,240
or lack thereof, having knowledge of the incentives of those who are creating these

818
01:27:43,240 --> 01:27:48,120
technologies and deploying them, and having knowledge of the knowledge level of those

819
01:27:48,120 --> 01:27:51,000
who are creating these technologies and deploying them, et cetera, et cetera, et cetera.

820
01:27:51,720 --> 01:27:59,160
And so I think there's, it's like now is the time when we need to stop messing around and start

821
01:27:59,160 --> 01:28:04,840
doing really, really good, really dialed in interdisciplinary work and supporting that,

822
01:28:04,840 --> 01:28:08,520
because you cannot effectively

823
01:28:12,360 --> 01:28:20,040
prevent or remediate the ill usage of these technologies from a single disciplinary background.

824
01:28:20,040 --> 01:28:25,640
You need teams of people with expertise in a lot of different domains who know how to talk to each

825
01:28:25,640 --> 01:28:31,560
other. Yeah, oh, that's for sure. Yeah. Yeah. So I think, sorry, I lied. That was the penultimate

826
01:28:31,560 --> 01:28:36,760
question. This is the last question. And then I'll let you go, I promise, because you spoke about

827
01:28:36,760 --> 01:28:42,200
kind of social harms. One other question I wanted to ask from you was, we spoke about

828
01:28:42,840 --> 01:28:51,480
science fiction and kind of the dooms kind of mentality. But would you compare, would you

829
01:28:51,480 --> 01:28:57,880
like, would you say the current paradigm of AI, the current technologies we have, that it can be

830
01:28:57,880 --> 01:29:03,560
compared to something like the nuclear bomb in the nukes in the 20th century, or do you think

831
01:29:03,560 --> 01:29:09,000
that's an exaggeration or I'm just being dramatic? Like we aren't there yet. It's not that grave.

832
01:29:11,080 --> 01:29:20,120
Well, the impact of nuclear bombs, the impact of nuclear technology

833
01:29:20,360 --> 01:29:27,720
was in one part the reality of this groundbreaking technology that made

834
01:29:30,680 --> 01:29:37,800
killing and destruction possible at only levels. On another part, it was a psychological phenomenon.

835
01:29:38,760 --> 01:29:39,240
It was

836
01:29:42,120 --> 01:29:53,000
almost more radically a psychological shift, right? I think the thing with machine learning

837
01:29:53,000 --> 01:29:59,880
technologies and any possible future development of these technologies, in fact, not just what we

838
01:29:59,960 --> 01:30:08,920
have right now in front of us, is almost that level of psychological reaction to what these

839
01:30:08,920 --> 01:30:21,640
technologies mean. But the disruptive force is not owing to a really radical scientific or technological

840
01:30:21,640 --> 01:30:28,440
breakthrough. It's more the psychological and social time.

841
01:30:33,800 --> 01:30:40,200
I mean, it has the potential to really radically disrupt labor markets, and it already is. But

842
01:30:40,200 --> 01:30:46,520
that was something that was already happening. Workers were already being pushed out. I mean,

843
01:30:46,600 --> 01:30:50,440
that was happening since the Industrial Revolution. Yeah, and that's capitalism. Yeah,

844
01:30:50,440 --> 01:30:56,520
that's just capitalism. Yeah. It's just that now there's an even more, it's perpetually a more

845
01:30:56,520 --> 01:31:04,280
and more urgent need to shift away from a mode of production in which if you don't work, you die.

846
01:31:07,160 --> 01:31:13,560
The world in which if you do not labor, you starve to death, is not one that works in which

847
01:31:13,560 --> 01:31:20,040
most people are being automated out of their jobs. Yeah, regardless of the technology. Yeah.

848
01:31:23,480 --> 01:31:28,120
On that grim note, thank you very much for your time, Mel. I've thoroughly enjoyed,

849
01:31:28,840 --> 01:31:33,880
I've learned a lot from you, but also just your Twitter feed is fantastic. I visited,

850
01:31:33,880 --> 01:31:39,560
I frequent it often. I hate Twitter, I think it's a terrible place, but I go to like yours,

851
01:31:39,560 --> 01:31:44,280
and I've got like five people bookmarked. I just visited their feeds. But yeah, thank you.

852
01:31:44,280 --> 01:31:47,880
Thank you very much for your time, Mel. And I hope we can chat again soon.

853
01:31:47,880 --> 01:31:51,160
Yeah, thanks, Lucas. Yeah, I'm happy to follow up. And send me the thing, the

854
01:31:53,800 --> 01:32:00,440
Korean-German author. Yes, the one by Byung-Shul Han. Yeah, I'll send you, I've got a PDF,

855
01:32:00,440 --> 01:32:05,000
I'll send you the PDF and all that, because I'd love to kind of, you know, follow up on that and

856
01:32:05,000 --> 01:32:10,280
see what ideas you have. But yeah, thank you, Mel. Yeah, thanks.

