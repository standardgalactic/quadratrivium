start	end	text
0	5680	I want to discuss, uh, explanatory coherence. Uh, you've done a lot of work on that. And I'm
5680	11280	wondering if you could kind of give a brief introduction to what, uh, explanatory coherence
11280	18160	is to a lay audience. What at the same time, uh, why you think, uh, it's, uh, what, what are the,
18160	21200	what are the implications in fields such as AI and psychology?
22640	26720	Oh, that's a big question, but it's a really interesting one. So let's go back to Darwin.
26720	30320	So I mentioned one of my favorite books of all time is the origin of species.
30320	34560	And so what was he doing in that? Uh, well, in my view, what he was trying to do is to
34560	38320	give a coherent explanation of all sorts of things that he observed. So he went on this
38320	43440	voyage around the world and he collected all sorts of other kinds of biological information.
43440	48720	And he gradually seemed to him that it seemed in fact that the species had evolved. I mean,
48720	54720	now everybody knows that kids get that probably in grade four, but it was a, um, a very controversial
54720	60160	idea. Some people had maintained it, but it went up against religious doctrines. And so he
60160	67600	gradually started to amass more and more evidence that species had evolved. But then from reading a
67600	72080	crazy economist named Malthus, he suddenly got the idea of how they evolved. And that's how he
72080	76800	came up with the idea of, of, of the natural selection. So now we had not only a bunch of
76800	81600	observations and the idea that evolution probably had occurred that would explain it,
81600	87520	but an idea of how, how evolution had occurred. That is that natural selection was the mechanism
87520	94960	behind evolution. So what he did in that book was an incredibly beautiful argument for his view,
94960	100160	as opposed to the view that was dominant at the time, which was divine creation. So what he was
100160	106880	trying to show is that his view was a better explanation, but then divine creation, because
106880	113520	it was more coherent with the evidence. So this is an account that philosophers call inference to
113520	119200	the best explanation. You can argue that something's the best explanation because it's more coherent
119200	124000	with the evidence, but now you have to say what coherence is. So I had these early ideas coming
124000	132080	out of my philosophy of science background, but then in 1987, I got one of the best ideas I've
132080	137520	ever had, which was how to turn coherence from a sort of vague philosophical idea into a precise
137520	144000	computational one. That is, how can you compute coherence? So I've been working on neural networks
144000	151040	in collaboration with my colleague Keith Holyoke. And he came up with an idea that you could use
151040	156800	neural networks to explain analogy. These neural networks, of course, are now absolutely central
156800	161360	to artificial intelligence. It's, it's really taken off. That's a whole fascinating topic in
161360	166480	itself. But he figured out a way of doing that. And by that time, I'd done my master's in computer
166480	171440	science, so I was a pretty good programmer. And so I programmed up a program to use neural networks
171440	177360	to do analogies. So that, that was nice. And then I thought, what else could apply to, and then I
177360	181280	thought back to the problem that was part of my doctoral dissertation, which was inference to the
181280	186800	best explanation. How do you pick up the best theory? And so then I realized that that kind of coherence
186800	192320	can be understood using the same kind of neural network technique that Keith and I had done for,
192320	198720	for analogy. So it was a really powerful method, both computationally, but also psychologically,
198720	202480	because there have now since then been lots of psychological experiments that back up this
202480	207440	idea of coherence. So I think of the mind, the brain as essentially a coherence engine.
207440	211280	There's some people who think that it's primarily a predictive engine, but I don't think that's
211280	215760	true. I think it's primarily a coherence engine. We're trying to make sense of things, whether
215760	220880	we're making sense of the past, which is what explanations do, or making sense of the future,
220880	227520	we're making coherent predictions, or we're trying to identify things. Is that a rabbit or a squirrel?
227520	232080	Those are, are different kinds of thought. Everything we do can be understood as having
232080	236960	coherence behind it. But coherence now isn't just a sort of vague metaphor that it was for
236960	241920	philosophers. It's not just a matter of consistency. It's rather of taking a whole bunch of different
241920	248000	things and putting them into a good package. But what's a good package? Well, here there's an idea
248000	254320	that came out of the neural network world called constraint satisfaction. So we're trying to satisfy
254320	259200	a bunch of constraints. What constraints did Darwin face? Well, he was trying to explain as much as
259200	265200	possible about what he'd seen in the biological world. That's the positive constraints, but he
265200	269440	also had a negative constraint. And so he had to show that he could do that better than the theory
269440	273920	that was the computer at the top competitor at the time, which is divine creation. So that's a
273920	278800	negative constraint. So what you're doing in all of these things, whether it's decision making or
278800	284400	pattern recognition, or even emotion, you're putting together different sorts of constraints to
284400	290240	evaluate what's the most coherent view. So that's how I came to see coherence, not just as a vague
290240	296080	philosophical idea, but as a quite precise computational one that can be used to explain
296640	302240	the mechanisms that underlie a vast amount of human thinking. So that's why I think coherence
302240	310000	is really a fundamental idea to psychology and cognitive science and to these philosophical
310000	315440	projects as well. Yeah, excellent, excellent. I'm glad he brought up predictive coding or
315440	321040	predictive processing, because I'll be talking to a cognitive scientist next month, in fact,
321040	327760	based here in Melbourne. And I want to ask her about your theory of explanatory coherence, because
327760	334000	I believe you do have certain critiques of predictive processing, but also in your book,
334000	340160	you are critical of, in fact, no, I think you wrote an article on this, a paper on this, pardon me,
340160	346400	you also critical of functionalism, kind of what Hilary Putnam and the likes put forward. So from
346400	353600	your kind of theory of mind, what would you say are your critiques of one predictive processing,
353600	358560	and then functionalism? Those are two different views, I don't think they have anything to do with
358560	363520	each other. I'm just curious, I quote them independently, what would be your critiques?
363520	368000	Okay, let's do one at a time. Predictive processing definitely is an influential view right now,
368000	373520	but I think it's just not right. It says that the mind is the brain is a predictive engine,
374240	378880	as if everything is prediction. But the mind doesn't just do prediction, it does at least
378880	384400	five other things that are just as important. It does explanation, which involves explaining the
384400	388800	past, that's not prediction, that's the past prediction is about the future, or even pattern
388800	394880	recognition. I mentioned, I see an animal in my backyard, what is it? Is that a squirrel or a rabbit?
394880	398400	Well, that's pattern recognition, that's not necessarily a prediction, I want to know what it
398400	405760	is. We also want to do, and this is really important, evaluation. Is this good or bad for me?
405760	410240	Is this a threat to me? Or is this something I can eat? How do we do evaluation? Well, in humans,
410240	414720	that comes from emotion. The predictive processing approach has said nothing interesting to say about
414720	420560	emotion at all, but that's absolutely fundamental to many areas of human thinking. I've got a whole
420560	428320	theory of emotion, of which coherence is part of it, but it's only part of it. So you have to
428320	434560	have evaluation going on. Communication, we sometimes predict in order to communicate with
434560	439840	other people, but there's lots of other things going on where we want to be able to get our ideas
439840	445280	across to others. So that's just at least five things that are part of the human mind other
445280	451440	than predictive processing. So that's my first critique. My second critique is the way that
451440	455440	people in that world think that predictive processing work doesn't correspond to how the
455440	460880	brain works very well. They're Bayesians. They say that the brain uses probability theory in
460880	467760	accord with Bayes' theorem to predict the next thing. Well, this is crazy computationally.
467760	473680	Bayesian processing is well known in artificial intelligence to be extremely inefficient. You
473680	479280	can prove that it's computationally intractable. You can show that it causes all sorts of problems.
479280	483760	Bayesians have to jump through all sorts of hoops to try to deal with anything larger than that.
483840	487920	I've done a little bit of Bayesian modeling because I wanted to, I drew a couple of papers
487920	492880	where I compared a Bayesian model of legal reasoning to my explanatory coherence one.
492880	498160	But the Bayesian models are crazy because you have to generate all sorts of conditional probabilities
498160	502240	that nobody has an idea what they are. So if you actually do Bayesian modeling seriously,
502240	505920	you'll find first of all, you don't know any of the probabilities. You don't know any of the
505920	510400	conditional probabilities. You don't have the computational or neural resources to actually
510400	515840	commute the probabilities. So the way that predictive processing with its too narrow view
515840	520240	of how the brain works fills it out is by making brains Bayesian when they're not.
520240	526640	A really good contrast here is with the new generative AI models, which are actually incredibly
526640	533680	good at predicting. Have you used chat GPT or any of the others? They're astonishing. They're
533680	538480	astonishing at how good they are at predicting the next word to say. And they end up producing
538480	543120	really coherent stuff. And they get things really bad, badly wrong sometimes, but often
543120	548080	they're really good, but they don't use Bayesian predictions. It's they've got all different kinds
548080	553920	of algorithms that they use tension mechanism and sorts of things. So they realize that that the
553920	559120	Bayesian approach is not going to work for them. So those are my two major criticisms of the
559120	564160	predictive processing. The brain is a multi fast, it's a coherence engine doing six things
564560	570320	as well as prediction. And it's not doing it using Bayesian probability calculations.
571360	573280	Okay, so is that good enough for predictive processing?
574880	578320	No, I think that's perfect. I want to ask you about the free energy principle, but probably we'll
578320	584720	get to the functionalism and then maybe come back to the principle. Yeah, okay. So functionalism
584720	589440	is a view in the philosophy of mind. It's actually a really bad term because functionalism is a term
589440	593280	that operates in about six different fields or six different meetings. So we need to pin it down a
593280	599120	bit. Let's call it computational functionalism. Because it came in the 60s when computers started
599120	604080	to become aware and Hillary Putnam knew about the advances in computing. When computers were
604080	608560	really primitive, then I've got a watch now that was better than all the computers, way better than
608560	615600	anything that came along for decades. But but still, people were starting to think that with
615600	621680	computers and the possibility of artificial intelligence, we've got this abstract way of
621680	628160	thinking of thinking as a kind of computation. Now, one thing that's really true or seems to be
628160	633840	true about computation is that it doesn't really matter what you run it on. So here I'm using a
633840	639680	Macintosh. I don't know what kind of computer you've got. It could be a PC or running a different
639680	644080	kind of hardware altogether. It doesn't matter. We can all run the same software. And so the
644080	650880	analogy that Putnam hit on was mind is software rather than hardware. You can take the same software
650880	654320	and run it on a bunch of people or hardware. All that matters is it has the appropriate
654320	658960	computational functions. That's where the word functionalism comes from. So if you have inputs
658960	665760	and outputs, and you have the functions in between, you want to be able to make thinking work. And so
665760	672240	forget about the hardware. Forget about the brain, for example. The psychologist had been studying
672240	678560	the brain at that point for, I guess, 60 or 70 years seriously. The functionalist in the 60s said,
678560	683680	let's forget about the brain. It's all computation. It's just like AI. Anything that runs in the mind
683680	689200	can run on a computer. Okay. And actually in the 1960s and 70s, that was a pretty reasonable idea.
689200	693680	And this is why a lot of philosophers consider themselves functionalists. It became the dominant
693680	699360	view in the philosophy of mind. So I think that was a pretty good idea in the 60s and 70s because
699360	706000	AI had become a real field. Computers had become at least rudimentarily powerful. And so not a
706000	712480	bad idea then. But things changed in the 80s. In the 80s, a bunch of things changed. First of all,
713440	717760	brain scanning came along. It had been really hard to study the brain before because you had to do
717760	722000	things like poke electrodes into brains that had been exposed. And so it was really hard to study
722000	727440	the brain. But in the 80s, brain scans came along. First of all, I forget what they were called,
727440	731920	and then eventually fMRI. But suddenly you could actually study the brain in a much more detailed
731920	736880	way. And then you could start to test some of the claims that had been made. So when people started
736880	742560	doing fMRI studies, they thought, oh, we're going to be able to show that the mind really is module,
742560	747360	that is modular, that is different parts of the brain are doing very specific things. And so we
747360	751360	should be able to find that this part of the brain does high level thinking, this part of the brain
751360	756560	does emotion, and that part of the brain does vision, and we could localize it. But once people had
756560	761360	this new tool, they started to realize, hey, it's not like that at all. Lots of what goes on the
761360	767120	brain involves interactions of lots of different areas. So suddenly the brain became much more
767120	771600	interesting. It didn't look like just some other kind of hardware you might run thoughts on. It
771600	777120	looked like you could study on its own. So there were these empirical findings coming out of the
777120	781360	new tools available for studying the brain that suggested that, well, maybe the structure of the
781360	788560	brain really does matter. So that was an incredibly important empirical basis for starting to question
788560	793920	functionalism. But there was also a really interesting theoretical basis coming out of
793920	798720	the ideas about neural networks. So the ideas of neural networks have been around really back
798720	804160	since the 50s, but it didn't work very well. And people like Marvin Minsky had argued that, no,
804160	808000	those these ideas about neural networks are not going to work very well. They're just,
808000	813280	they're just, they're just simply not theoretically strong enough. But in the 1980s,
814000	820000	people greatly expanded the possibilities of what neural networks could do. They invented a new
820000	825040	algorithm called back propagation that does learning. A whole movement got started called
825040	830320	connectionism, which said that knowledge isn't a matter of the words you've got or the symbols,
830320	834560	which is what artificial intelligence, but it's rather it's the connections, it's the neural
834560	839440	connections. So suddenly, people were modeling their computer models, because these are being done
839440	844320	with computerized neural networks, they were modeling on ideas about the brain, how you can
844320	849680	have different neurons working in parallel with simple connections with them, and nevertheless
849680	857040	doing that. So in the 1980s, suddenly, I functionalism was in trouble. Not many people noticed because
857040	861680	they weren't tracking what was happening in neuroscience and in neural network theory,
861680	867440	but it wasn't. And by the 1990s, I think it really had completely turned around. I think by the 1990s,
867440	871840	functionalism was no longer plausible. You needed to take the brain seriously if you wanted to
871840	877280	understand. And the whole field of cognitive psychology changed. It went from being completely
877280	883040	abstract and computational to doing almost everything it did in relation to what happened
883040	886560	in the brain. So cognitive psychology is now completely connected with neuroscience in the
886560	891680	field of cognitive neuroscience. Other areas of psychology, developmental social also became
891680	896880	intensely tied in with the brain. So the idea that the hardware doesn't matter, which was what
896880	902160	was behind Putnam's functionalism, just by the 90s, didn't seem plausible at all. So that's why I
902160	906080	think functionalism is a defunct view in the philosophy of mind, even though there are people
906080	910560	who seem to assume that it's right. Sometimes it goes under other names. There's another name that
910560	918960	people use, it's called substrate independence. The idea is the substrate is the physical,
919840	924160	and so that doesn't matter. And there are people who use that because it suits some of their views,
924240	929280	such as the idea that we're all living in a simulation, which I think is a really dumb view.
929280	934320	But in order to believe that, you have to believe that substrate independence is true,
934320	937360	which is another word for functionalism, which says the hardware doesn't matter,
937920	943120	because the idea of where a simulation is some computer in the future is basically
943120	947920	simulating our thoughts now. Well, that assumes that a computer can simulate all our thoughts,
947920	953840	which assumes functionalism or some straight independence, independence, which I think is
953840	959120	wrong. And I've actually just published a paper in Philosophy of Science two years ago that gives
959120	963280	a whole bunch of arguments based on energy about why it's wrong. But there are other reasons as
963280	970480	well for thinking that functionalism or substrate independence is wrong. Okay, that's enough.
971440	975200	That's really fun. But go ahead. I didn't mean to interrupt you there, but please.
975200	979680	I just wanted to summarize. So it was a great idea in the Philosophy of Mind
979680	984080	that no longer should be taken very seriously, given what we know about brains and energy.
985520	992720	So even look at the way AI is going right now. So the generative AI models, the large language
992720	997760	models are incredible, but they're really energy pigs. It takes vast amounts of energy to train
997760	1004080	these things and answer questions. Our brains are astonishing. Our brains work on basically 40
1004160	1009520	watts, like a small light bulb, very small amounts of energy, very efficient, and yet we're still
1009520	1016000	smarter than any computer with all these resources. So there's a full field called neuromorphic AI,
1016000	1020400	which is trying to make computers more like the brain to get these advantages of energy and
1020400	1027520	efficiency and working in real time. So I think these are really interesting research areas that
1027520	1033680	show that functionalism just wasn't it is no longer a plausible view in the Philosophy of Mind.
1034560	1038560	Now that's an astute point, Professor, because I was, well, two points on there. Firstly, I always
1038560	1044240	found functionalists to be good old Cartesian's where they had the mind, the mind matter. They
1044240	1048880	think mind is independent to matter, which for me never made any sense, given we have physical
1048880	1055920	embodied beings. And secondly, you are 100% right that I was listened to a talk by Scott
1055920	1061520	Aronson, the American computer scientist, and he is now a researcher at Open AI. And Open AI is
1061520	1068480	heavily investing in quantum computing and even in nuclear energy, because they've understood
1068480	1075280	that if they are to grow their LLMs, they need infinite amounts of energy, because the compute
1075280	1082880	power for LLMs are so, they're so high compared to like our puny little brains, which is a fascinating
1082880	1084880	conversation.
