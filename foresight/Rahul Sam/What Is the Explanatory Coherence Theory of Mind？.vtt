WEBVTT

00:00.000 --> 00:05.680
I want to discuss, uh, explanatory coherence. Uh, you've done a lot of work on that. And I'm

00:05.680 --> 00:11.280
wondering if you could kind of give a brief introduction to what, uh, explanatory coherence

00:11.280 --> 00:18.160
is to a lay audience. What at the same time, uh, why you think, uh, it's, uh, what, what are the,

00:18.160 --> 00:21.200
what are the implications in fields such as AI and psychology?

00:22.640 --> 00:26.720
Oh, that's a big question, but it's a really interesting one. So let's go back to Darwin.

00:26.720 --> 00:30.320
So I mentioned one of my favorite books of all time is the origin of species.

00:30.320 --> 00:34.560
And so what was he doing in that? Uh, well, in my view, what he was trying to do is to

00:34.560 --> 00:38.320
give a coherent explanation of all sorts of things that he observed. So he went on this

00:38.320 --> 00:43.440
voyage around the world and he collected all sorts of other kinds of biological information.

00:43.440 --> 00:48.720
And he gradually seemed to him that it seemed in fact that the species had evolved. I mean,

00:48.720 --> 00:54.720
now everybody knows that kids get that probably in grade four, but it was a, um, a very controversial

00:54.720 --> 01:00.160
idea. Some people had maintained it, but it went up against religious doctrines. And so he

01:00.160 --> 01:07.600
gradually started to amass more and more evidence that species had evolved. But then from reading a

01:07.600 --> 01:12.080
crazy economist named Malthus, he suddenly got the idea of how they evolved. And that's how he

01:12.080 --> 01:16.800
came up with the idea of, of, of the natural selection. So now we had not only a bunch of

01:16.800 --> 01:21.600
observations and the idea that evolution probably had occurred that would explain it,

01:21.600 --> 01:27.520
but an idea of how, how evolution had occurred. That is that natural selection was the mechanism

01:27.520 --> 01:34.960
behind evolution. So what he did in that book was an incredibly beautiful argument for his view,

01:34.960 --> 01:40.160
as opposed to the view that was dominant at the time, which was divine creation. So what he was

01:40.160 --> 01:46.880
trying to show is that his view was a better explanation, but then divine creation, because

01:46.880 --> 01:53.520
it was more coherent with the evidence. So this is an account that philosophers call inference to

01:53.520 --> 01:59.200
the best explanation. You can argue that something's the best explanation because it's more coherent

01:59.200 --> 02:04.000
with the evidence, but now you have to say what coherence is. So I had these early ideas coming

02:04.000 --> 02:12.080
out of my philosophy of science background, but then in 1987, I got one of the best ideas I've

02:12.080 --> 02:17.520
ever had, which was how to turn coherence from a sort of vague philosophical idea into a precise

02:17.520 --> 02:24.000
computational one. That is, how can you compute coherence? So I've been working on neural networks

02:24.000 --> 02:31.040
in collaboration with my colleague Keith Holyoke. And he came up with an idea that you could use

02:31.040 --> 02:36.800
neural networks to explain analogy. These neural networks, of course, are now absolutely central

02:36.800 --> 02:41.360
to artificial intelligence. It's, it's really taken off. That's a whole fascinating topic in

02:41.360 --> 02:46.480
itself. But he figured out a way of doing that. And by that time, I'd done my master's in computer

02:46.480 --> 02:51.440
science, so I was a pretty good programmer. And so I programmed up a program to use neural networks

02:51.440 --> 02:57.360
to do analogies. So that, that was nice. And then I thought, what else could apply to, and then I

02:57.360 --> 03:01.280
thought back to the problem that was part of my doctoral dissertation, which was inference to the

03:01.280 --> 03:06.800
best explanation. How do you pick up the best theory? And so then I realized that that kind of coherence

03:06.800 --> 03:12.320
can be understood using the same kind of neural network technique that Keith and I had done for,

03:12.320 --> 03:18.720
for analogy. So it was a really powerful method, both computationally, but also psychologically,

03:18.720 --> 03:22.480
because there have now since then been lots of psychological experiments that back up this

03:22.480 --> 03:27.440
idea of coherence. So I think of the mind, the brain as essentially a coherence engine.

03:27.440 --> 03:31.280
There's some people who think that it's primarily a predictive engine, but I don't think that's

03:31.280 --> 03:35.760
true. I think it's primarily a coherence engine. We're trying to make sense of things, whether

03:35.760 --> 03:40.880
we're making sense of the past, which is what explanations do, or making sense of the future,

03:40.880 --> 03:47.520
we're making coherent predictions, or we're trying to identify things. Is that a rabbit or a squirrel?

03:47.520 --> 03:52.080
Those are, are different kinds of thought. Everything we do can be understood as having

03:52.080 --> 03:56.960
coherence behind it. But coherence now isn't just a sort of vague metaphor that it was for

03:56.960 --> 04:01.920
philosophers. It's not just a matter of consistency. It's rather of taking a whole bunch of different

04:01.920 --> 04:08.000
things and putting them into a good package. But what's a good package? Well, here there's an idea

04:08.000 --> 04:14.320
that came out of the neural network world called constraint satisfaction. So we're trying to satisfy

04:14.320 --> 04:19.200
a bunch of constraints. What constraints did Darwin face? Well, he was trying to explain as much as

04:19.200 --> 04:25.200
possible about what he'd seen in the biological world. That's the positive constraints, but he

04:25.200 --> 04:29.440
also had a negative constraint. And so he had to show that he could do that better than the theory

04:29.440 --> 04:33.920
that was the computer at the top competitor at the time, which is divine creation. So that's a

04:33.920 --> 04:38.800
negative constraint. So what you're doing in all of these things, whether it's decision making or

04:38.800 --> 04:44.400
pattern recognition, or even emotion, you're putting together different sorts of constraints to

04:44.400 --> 04:50.240
evaluate what's the most coherent view. So that's how I came to see coherence, not just as a vague

04:50.240 --> 04:56.080
philosophical idea, but as a quite precise computational one that can be used to explain

04:56.640 --> 05:02.240
the mechanisms that underlie a vast amount of human thinking. So that's why I think coherence

05:02.240 --> 05:10.000
is really a fundamental idea to psychology and cognitive science and to these philosophical

05:10.000 --> 05:15.440
projects as well. Yeah, excellent, excellent. I'm glad he brought up predictive coding or

05:15.440 --> 05:21.040
predictive processing, because I'll be talking to a cognitive scientist next month, in fact,

05:21.040 --> 05:27.760
based here in Melbourne. And I want to ask her about your theory of explanatory coherence, because

05:27.760 --> 05:34.000
I believe you do have certain critiques of predictive processing, but also in your book,

05:34.000 --> 05:40.160
you are critical of, in fact, no, I think you wrote an article on this, a paper on this, pardon me,

05:40.160 --> 05:46.400
you also critical of functionalism, kind of what Hilary Putnam and the likes put forward. So from

05:46.400 --> 05:53.600
your kind of theory of mind, what would you say are your critiques of one predictive processing,

05:53.600 --> 05:58.560
and then functionalism? Those are two different views, I don't think they have anything to do with

05:58.560 --> 06:03.520
each other. I'm just curious, I quote them independently, what would be your critiques?

06:03.520 --> 06:08.000
Okay, let's do one at a time. Predictive processing definitely is an influential view right now,

06:08.000 --> 06:13.520
but I think it's just not right. It says that the mind is the brain is a predictive engine,

06:14.240 --> 06:18.880
as if everything is prediction. But the mind doesn't just do prediction, it does at least

06:18.880 --> 06:24.400
five other things that are just as important. It does explanation, which involves explaining the

06:24.400 --> 06:28.800
past, that's not prediction, that's the past prediction is about the future, or even pattern

06:28.800 --> 06:34.880
recognition. I mentioned, I see an animal in my backyard, what is it? Is that a squirrel or a rabbit?

06:34.880 --> 06:38.400
Well, that's pattern recognition, that's not necessarily a prediction, I want to know what it

06:38.400 --> 06:45.760
is. We also want to do, and this is really important, evaluation. Is this good or bad for me?

06:45.760 --> 06:50.240
Is this a threat to me? Or is this something I can eat? How do we do evaluation? Well, in humans,

06:50.240 --> 06:54.720
that comes from emotion. The predictive processing approach has said nothing interesting to say about

06:54.720 --> 07:00.560
emotion at all, but that's absolutely fundamental to many areas of human thinking. I've got a whole

07:00.560 --> 07:08.320
theory of emotion, of which coherence is part of it, but it's only part of it. So you have to

07:08.320 --> 07:14.560
have evaluation going on. Communication, we sometimes predict in order to communicate with

07:14.560 --> 07:19.840
other people, but there's lots of other things going on where we want to be able to get our ideas

07:19.840 --> 07:25.280
across to others. So that's just at least five things that are part of the human mind other

07:25.280 --> 07:31.440
than predictive processing. So that's my first critique. My second critique is the way that

07:31.440 --> 07:35.440
people in that world think that predictive processing work doesn't correspond to how the

07:35.440 --> 07:40.880
brain works very well. They're Bayesians. They say that the brain uses probability theory in

07:40.880 --> 07:47.760
accord with Bayes' theorem to predict the next thing. Well, this is crazy computationally.

07:47.760 --> 07:53.680
Bayesian processing is well known in artificial intelligence to be extremely inefficient. You

07:53.680 --> 07:59.280
can prove that it's computationally intractable. You can show that it causes all sorts of problems.

07:59.280 --> 08:03.760
Bayesians have to jump through all sorts of hoops to try to deal with anything larger than that.

08:03.840 --> 08:07.920
I've done a little bit of Bayesian modeling because I wanted to, I drew a couple of papers

08:07.920 --> 08:12.880
where I compared a Bayesian model of legal reasoning to my explanatory coherence one.

08:12.880 --> 08:18.160
But the Bayesian models are crazy because you have to generate all sorts of conditional probabilities

08:18.160 --> 08:22.240
that nobody has an idea what they are. So if you actually do Bayesian modeling seriously,

08:22.240 --> 08:25.920
you'll find first of all, you don't know any of the probabilities. You don't know any of the

08:25.920 --> 08:30.400
conditional probabilities. You don't have the computational or neural resources to actually

08:30.400 --> 08:35.840
commute the probabilities. So the way that predictive processing with its too narrow view

08:35.840 --> 08:40.240
of how the brain works fills it out is by making brains Bayesian when they're not.

08:40.240 --> 08:46.640
A really good contrast here is with the new generative AI models, which are actually incredibly

08:46.640 --> 08:53.680
good at predicting. Have you used chat GPT or any of the others? They're astonishing. They're

08:53.680 --> 08:58.480
astonishing at how good they are at predicting the next word to say. And they end up producing

08:58.480 --> 09:03.120
really coherent stuff. And they get things really bad, badly wrong sometimes, but often

09:03.120 --> 09:08.080
they're really good, but they don't use Bayesian predictions. It's they've got all different kinds

09:08.080 --> 09:13.920
of algorithms that they use tension mechanism and sorts of things. So they realize that that the

09:13.920 --> 09:19.120
Bayesian approach is not going to work for them. So those are my two major criticisms of the

09:19.120 --> 09:24.160
predictive processing. The brain is a multi fast, it's a coherence engine doing six things

09:24.560 --> 09:30.320
as well as prediction. And it's not doing it using Bayesian probability calculations.

09:31.360 --> 09:33.280
Okay, so is that good enough for predictive processing?

09:34.880 --> 09:38.320
No, I think that's perfect. I want to ask you about the free energy principle, but probably we'll

09:38.320 --> 09:44.720
get to the functionalism and then maybe come back to the principle. Yeah, okay. So functionalism

09:44.720 --> 09:49.440
is a view in the philosophy of mind. It's actually a really bad term because functionalism is a term

09:49.440 --> 09:53.280
that operates in about six different fields or six different meetings. So we need to pin it down a

09:53.280 --> 09:59.120
bit. Let's call it computational functionalism. Because it came in the 60s when computers started

09:59.120 --> 10:04.080
to become aware and Hillary Putnam knew about the advances in computing. When computers were

10:04.080 --> 10:08.560
really primitive, then I've got a watch now that was better than all the computers, way better than

10:08.560 --> 10:15.600
anything that came along for decades. But but still, people were starting to think that with

10:15.600 --> 10:21.680
computers and the possibility of artificial intelligence, we've got this abstract way of

10:21.680 --> 10:28.160
thinking of thinking as a kind of computation. Now, one thing that's really true or seems to be

10:28.160 --> 10:33.840
true about computation is that it doesn't really matter what you run it on. So here I'm using a

10:33.840 --> 10:39.680
Macintosh. I don't know what kind of computer you've got. It could be a PC or running a different

10:39.680 --> 10:44.080
kind of hardware altogether. It doesn't matter. We can all run the same software. And so the

10:44.080 --> 10:50.880
analogy that Putnam hit on was mind is software rather than hardware. You can take the same software

10:50.880 --> 10:54.320
and run it on a bunch of people or hardware. All that matters is it has the appropriate

10:54.320 --> 10:58.960
computational functions. That's where the word functionalism comes from. So if you have inputs

10:58.960 --> 11:05.760
and outputs, and you have the functions in between, you want to be able to make thinking work. And so

11:05.760 --> 11:12.240
forget about the hardware. Forget about the brain, for example. The psychologist had been studying

11:12.240 --> 11:18.560
the brain at that point for, I guess, 60 or 70 years seriously. The functionalist in the 60s said,

11:18.560 --> 11:23.680
let's forget about the brain. It's all computation. It's just like AI. Anything that runs in the mind

11:23.680 --> 11:29.200
can run on a computer. Okay. And actually in the 1960s and 70s, that was a pretty reasonable idea.

11:29.200 --> 11:33.680
And this is why a lot of philosophers consider themselves functionalists. It became the dominant

11:33.680 --> 11:39.360
view in the philosophy of mind. So I think that was a pretty good idea in the 60s and 70s because

11:39.360 --> 11:46.000
AI had become a real field. Computers had become at least rudimentarily powerful. And so not a

11:46.000 --> 11:52.480
bad idea then. But things changed in the 80s. In the 80s, a bunch of things changed. First of all,

11:53.440 --> 11:57.760
brain scanning came along. It had been really hard to study the brain before because you had to do

11:57.760 --> 12:02.000
things like poke electrodes into brains that had been exposed. And so it was really hard to study

12:02.000 --> 12:07.440
the brain. But in the 80s, brain scans came along. First of all, I forget what they were called,

12:07.440 --> 12:11.920
and then eventually fMRI. But suddenly you could actually study the brain in a much more detailed

12:11.920 --> 12:16.880
way. And then you could start to test some of the claims that had been made. So when people started

12:16.880 --> 12:22.560
doing fMRI studies, they thought, oh, we're going to be able to show that the mind really is module,

12:22.560 --> 12:27.360
that is modular, that is different parts of the brain are doing very specific things. And so we

12:27.360 --> 12:31.360
should be able to find that this part of the brain does high level thinking, this part of the brain

12:31.360 --> 12:36.560
does emotion, and that part of the brain does vision, and we could localize it. But once people had

12:36.560 --> 12:41.360
this new tool, they started to realize, hey, it's not like that at all. Lots of what goes on the

12:41.360 --> 12:47.120
brain involves interactions of lots of different areas. So suddenly the brain became much more

12:47.120 --> 12:51.600
interesting. It didn't look like just some other kind of hardware you might run thoughts on. It

12:51.600 --> 12:57.120
looked like you could study on its own. So there were these empirical findings coming out of the

12:57.120 --> 13:01.360
new tools available for studying the brain that suggested that, well, maybe the structure of the

13:01.360 --> 13:08.560
brain really does matter. So that was an incredibly important empirical basis for starting to question

13:08.560 --> 13:13.920
functionalism. But there was also a really interesting theoretical basis coming out of

13:13.920 --> 13:18.720
the ideas about neural networks. So the ideas of neural networks have been around really back

13:18.720 --> 13:24.160
since the 50s, but it didn't work very well. And people like Marvin Minsky had argued that, no,

13:24.160 --> 13:28.000
those these ideas about neural networks are not going to work very well. They're just,

13:28.000 --> 13:33.280
they're just, they're just simply not theoretically strong enough. But in the 1980s,

13:34.000 --> 13:40.000
people greatly expanded the possibilities of what neural networks could do. They invented a new

13:40.000 --> 13:45.040
algorithm called back propagation that does learning. A whole movement got started called

13:45.040 --> 13:50.320
connectionism, which said that knowledge isn't a matter of the words you've got or the symbols,

13:50.320 --> 13:54.560
which is what artificial intelligence, but it's rather it's the connections, it's the neural

13:54.560 --> 13:59.440
connections. So suddenly, people were modeling their computer models, because these are being done

13:59.440 --> 14:04.320
with computerized neural networks, they were modeling on ideas about the brain, how you can

14:04.320 --> 14:09.680
have different neurons working in parallel with simple connections with them, and nevertheless

14:09.680 --> 14:17.040
doing that. So in the 1980s, suddenly, I functionalism was in trouble. Not many people noticed because

14:17.040 --> 14:21.680
they weren't tracking what was happening in neuroscience and in neural network theory,

14:21.680 --> 14:27.440
but it wasn't. And by the 1990s, I think it really had completely turned around. I think by the 1990s,

14:27.440 --> 14:31.840
functionalism was no longer plausible. You needed to take the brain seriously if you wanted to

14:31.840 --> 14:37.280
understand. And the whole field of cognitive psychology changed. It went from being completely

14:37.280 --> 14:43.040
abstract and computational to doing almost everything it did in relation to what happened

14:43.040 --> 14:46.560
in the brain. So cognitive psychology is now completely connected with neuroscience in the

14:46.560 --> 14:51.680
field of cognitive neuroscience. Other areas of psychology, developmental social also became

14:51.680 --> 14:56.880
intensely tied in with the brain. So the idea that the hardware doesn't matter, which was what

14:56.880 --> 15:02.160
was behind Putnam's functionalism, just by the 90s, didn't seem plausible at all. So that's why I

15:02.160 --> 15:06.080
think functionalism is a defunct view in the philosophy of mind, even though there are people

15:06.080 --> 15:10.560
who seem to assume that it's right. Sometimes it goes under other names. There's another name that

15:10.560 --> 15:18.960
people use, it's called substrate independence. The idea is the substrate is the physical,

15:19.840 --> 15:24.160
and so that doesn't matter. And there are people who use that because it suits some of their views,

15:24.240 --> 15:29.280
such as the idea that we're all living in a simulation, which I think is a really dumb view.

15:29.280 --> 15:34.320
But in order to believe that, you have to believe that substrate independence is true,

15:34.320 --> 15:37.360
which is another word for functionalism, which says the hardware doesn't matter,

15:37.920 --> 15:43.120
because the idea of where a simulation is some computer in the future is basically

15:43.120 --> 15:47.920
simulating our thoughts now. Well, that assumes that a computer can simulate all our thoughts,

15:47.920 --> 15:53.840
which assumes functionalism or some straight independence, independence, which I think is

15:53.840 --> 15:59.120
wrong. And I've actually just published a paper in Philosophy of Science two years ago that gives

15:59.120 --> 16:03.280
a whole bunch of arguments based on energy about why it's wrong. But there are other reasons as

16:03.280 --> 16:10.480
well for thinking that functionalism or substrate independence is wrong. Okay, that's enough.

16:11.440 --> 16:15.200
That's really fun. But go ahead. I didn't mean to interrupt you there, but please.

16:15.200 --> 16:19.680
I just wanted to summarize. So it was a great idea in the Philosophy of Mind

16:19.680 --> 16:24.080
that no longer should be taken very seriously, given what we know about brains and energy.

16:25.520 --> 16:32.720
So even look at the way AI is going right now. So the generative AI models, the large language

16:32.720 --> 16:37.760
models are incredible, but they're really energy pigs. It takes vast amounts of energy to train

16:37.760 --> 16:44.080
these things and answer questions. Our brains are astonishing. Our brains work on basically 40

16:44.160 --> 16:49.520
watts, like a small light bulb, very small amounts of energy, very efficient, and yet we're still

16:49.520 --> 16:56.000
smarter than any computer with all these resources. So there's a full field called neuromorphic AI,

16:56.000 --> 17:00.400
which is trying to make computers more like the brain to get these advantages of energy and

17:00.400 --> 17:07.520
efficiency and working in real time. So I think these are really interesting research areas that

17:07.520 --> 17:13.680
show that functionalism just wasn't it is no longer a plausible view in the Philosophy of Mind.

17:14.560 --> 17:18.560
Now that's an astute point, Professor, because I was, well, two points on there. Firstly, I always

17:18.560 --> 17:24.240
found functionalists to be good old Cartesian's where they had the mind, the mind matter. They

17:24.240 --> 17:28.880
think mind is independent to matter, which for me never made any sense, given we have physical

17:28.880 --> 17:35.920
embodied beings. And secondly, you are 100% right that I was listened to a talk by Scott

17:35.920 --> 17:41.520
Aronson, the American computer scientist, and he is now a researcher at Open AI. And Open AI is

17:41.520 --> 17:48.480
heavily investing in quantum computing and even in nuclear energy, because they've understood

17:48.480 --> 17:55.280
that if they are to grow their LLMs, they need infinite amounts of energy, because the compute

17:55.280 --> 18:02.880
power for LLMs are so, they're so high compared to like our puny little brains, which is a fascinating

18:02.880 --> 18:04.880
conversation.

