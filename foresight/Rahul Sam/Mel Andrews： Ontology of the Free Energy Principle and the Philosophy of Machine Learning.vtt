WEBVTT

00:00.000 --> 00:01.000
in this episode.

00:01.000 --> 00:09.720
So there's a long history in 20th century philosophy of science of investigating the

00:09.720 --> 00:23.000
possibility of something like an automated science or a formalized science or a algorithmically

00:23.000 --> 00:34.640
implemented science, all aspects of science, including sort of uncovering theory.

00:34.640 --> 00:44.320
And this has been a rich tradition, but it's been strictly abstract about about hypothetical

00:44.320 --> 00:52.360
machines and hypothetical algorithms and hypothetical forms of automation of scientific labor rights.

00:52.360 --> 01:01.880
And all of a sudden sort of perhaps largely unanticipated by philosophers, we reach a

01:01.880 --> 01:11.160
point where boom, deep learning revolution, and suddenly there is a kind of massive shift

01:11.160 --> 01:23.720
towards taking aspects of scientific discovery and passing them off to a intelligent computational

01:23.720 --> 01:29.920
system.

01:29.920 --> 01:37.200
And I think there's been kind of a tendency to continue to view that within the lens of

01:37.200 --> 01:48.520
this automation of scientific discovery debates as it's existed without really attending to

01:48.520 --> 01:56.640
the details of how these technologies are in fact being implemented in scientific practice.

01:56.640 --> 02:04.280
And by the way, I think when, when what we are doing is collecting data from some system

02:04.280 --> 02:13.760
in the world and using some machine learning model to extract statistical patterns from

02:13.760 --> 02:17.720
that system in order to learn about that system, what we're doing is is effectively science.

02:17.720 --> 02:19.520
It follows the model of science.

02:19.520 --> 02:24.880
So in fact, most applications of machine learning ought to be considered a kind of science

02:24.880 --> 02:26.920
adjacent activity.

02:26.920 --> 02:31.160
And in my opinion, it held to the standards of good science.

02:31.480 --> 02:35.640
Hey, everyone, welcome to my conversation with the philosopher of science, Mel Andrews,

02:35.640 --> 02:40.240
who's doing work in machine learning and other adjacent fields such as the philosophy of

02:40.240 --> 02:44.360
mathematics, epistemology, and of course, ethics.

02:44.360 --> 02:48.960
It was a fantastic, cordial and riveting conversation for me.

02:48.960 --> 02:51.000
And I should first say it's good to be back.

02:51.000 --> 02:56.680
It's good to be back after my short holiday back doing podcasts, having these fascinating

02:56.680 --> 03:01.080
conversations with these superlative guests.

03:01.080 --> 03:05.200
And I couldn't think of a better person to start off with other than Mel.

03:05.200 --> 03:07.520
They is fascinating.

03:07.520 --> 03:14.440
They also has this way of kind of, you know, elucidating certain points that for a while

03:14.440 --> 03:16.880
I did struggle to understand.

03:16.880 --> 03:22.760
And of course, probably the best example would be this paper, the math is not the territory

03:22.760 --> 03:26.120
navigating the free energy principle.

03:26.120 --> 03:33.000
As someone who's deeply interested in the kind of philosophy of FEP, this paper really

03:33.000 --> 03:38.800
helped me understand what the ontology of the free energy principle is and what really

03:38.800 --> 03:45.080
is the proper way to view it as a kind of a mathematical, formalistic, conceptual framework

03:45.080 --> 03:50.080
that can be applied in many areas such as machine learning and of course cognitive science.

03:50.080 --> 03:54.880
But we did start the podcast discussing Mel's recent paper regarding the epistemic status

03:54.880 --> 04:01.260
of machine learning and they provocatively claimed that machine learning has a pseudo

04:01.260 --> 04:07.200
science problem which I vehemently agree with and even kind of brought up this resurgence

04:07.200 --> 04:12.600
of physiognomy kind of provocatively to make this point.

04:12.600 --> 04:17.840
It's a preprint, in fact, and we discussed a bit of that paper and kind of extrapolated

04:17.840 --> 04:25.240
some of those ideas into the broader socio-political discussion around AI ethics, which again is

04:25.240 --> 04:26.960
very interesting.

04:26.960 --> 04:33.720
And I do really appreciate that Mel is a very critical almost focodian analysis of these

04:33.720 --> 04:38.440
different disciplines that are getting a lot of attention these days such as AI and machine

04:38.440 --> 04:39.600
learning of course.

04:39.600 --> 04:46.960
I also mentioned to Mel that this credit feed is a place that I visit quite often online.

04:46.960 --> 04:52.160
I hate Twitter, I think Twitter is a hellscape, however there are a few accounts that I've

04:52.160 --> 04:59.240
bookmarked and I do frequently visit simply for entertainment but also to learn and Mel's

04:59.240 --> 05:03.600
Twitter feed is certainly one of those that are bookmarked.

05:03.600 --> 05:08.280
Having said that, before I get carried away talking about Twitter and all of that, a bit

05:08.280 --> 05:15.240
of a formal introduction to Mel Andrews is a philosopher of science working on the role

05:15.240 --> 05:20.080
of mathematical and computational methods in science and particular machine learning

05:20.080 --> 05:21.880
based methods.

05:21.880 --> 05:27.040
Mel is currently a predoctoral research associate at the department of machine learning at the

05:27.040 --> 05:31.680
Carnegie Mellon University and doing a PhD in philosophy of science at the University

05:31.680 --> 05:32.680
of Cincinnati.

05:32.680 --> 05:38.560
They are also a visiting scholar at the Australian National University and the University of

05:38.560 --> 05:40.040
Pittsburgh.

05:40.040 --> 05:43.480
Having said that, here's my conversation with Mel Andrews.

05:43.480 --> 05:47.000
So on housekeeping note I should mention that I'll leave links to everything we've discussed

05:47.000 --> 05:48.000
in the show notes.

05:48.000 --> 05:49.000
Okay, now to the podcast.

05:49.000 --> 05:50.000
Mel Andrews

05:50.000 --> 05:56.960
The sort of conclusion I reached eventually was, it just, I mean, in academia there's

05:56.960 --> 06:07.360
a certain portion of what you do that feels like running on a wheel, like it's not, it's

06:07.360 --> 06:11.000
a performance, it's not actually to accomplish anything but it just feels like such a high

06:11.040 --> 06:19.280
percentage of that in industry is just sort of, there's so many levels of removal.

06:19.280 --> 06:25.640
The person who's passing down the orders is so many levels removed from the people actually

06:25.640 --> 06:32.560
implementing solutions that you're almost not accomplishing anything.

06:32.560 --> 06:33.560
Mel Andrews

06:33.560 --> 06:39.200
Yeah, also like, I mean, look, it's, I think, Franz Kafka, he captured it best in his novel,

06:39.200 --> 06:43.360
you know, it's Kafka-esque in the sense that it seems like people are just doing things

06:43.360 --> 06:49.120
for the sake of doing things without really going anywhere and you're having all these,

06:49.120 --> 06:51.840
you know, wonky meetings.

06:51.840 --> 06:57.480
But to be fair though, like, I agree with you because, because I'm still outside of academia,

06:57.480 --> 07:02.720
I've kind of idealized academia, but after talking to a lot of people like yourself on

07:02.720 --> 07:07.920
the podcast, I'm getting a more realistic picture of what it is because at the end of

07:07.920 --> 07:13.800
the day, the university is still in our society, it's a part of our culture within them.

07:13.800 --> 07:14.800
Mel Andrews

07:14.800 --> 07:19.160
And it's beholden to a capitalist economic system.

07:19.160 --> 07:20.160
Mel Andrews

07:20.160 --> 07:21.160
Exactly, exactly.

07:21.160 --> 07:22.160
Yeah, yeah.

07:22.160 --> 07:27.360
And it kind of shows how much the, let's call it the capitalist tentacles reach into every

07:27.360 --> 07:29.880
corner of our social existence.

07:29.880 --> 07:35.320
I mean, although I even go a step further and I say, I mean, I'm a big fan of Jacques,

07:35.320 --> 07:41.480
it even changes our psyche, our subjective state in this world and, and like fundamentally

07:41.480 --> 07:47.640
who we are, as let's say beings in this world, you know, and then that of course affects academics

07:47.640 --> 07:52.360
and, and quote unquote, lay people and everyone really, but I see what you mean.

07:52.360 --> 07:53.360
Yeah, yeah.

07:53.360 --> 07:55.360
So I've got to ask now though.

07:55.360 --> 07:56.360
Mel Andrews

07:56.360 --> 07:57.360
Sorry.

07:57.360 --> 08:03.880
In terms of even like the AI ethics communities, you have like the AI risk, AI safety people,

08:04.880 --> 08:09.960
like existential risk, kind of the effective altruist oriented community.

08:09.960 --> 08:16.400
And you've got then, you know, academic, fairness, safety, regulation communities.

08:16.400 --> 08:20.280
And they're both sort of pointing at each other and accusing each other of like corporate

08:20.280 --> 08:21.280
capture.

08:21.280 --> 08:26.840
But of course, the irony is, is that these communities are both about as corporately

08:26.840 --> 08:28.840
captured as you can.

08:28.840 --> 08:29.840
Yeah.

08:29.840 --> 08:30.840
Yeah.

08:30.840 --> 08:35.440
And every entity really, you know, neoliberal society is corporate captured in some sense,

08:35.440 --> 08:39.800
you know, after the 70s and 80s, it fundamentally changed.

08:39.800 --> 08:42.480
I completely agree with you.

08:42.480 --> 08:44.480
No ethical work, no ethical consumption.

08:44.480 --> 08:49.880
I mean, there's only really a handful of, I'd say at this point, as in some sense I'm

08:49.880 --> 08:56.840
like, well, I am in your fight, but I'd say there's only a handful of researchers who

08:56.840 --> 08:59.960
I follow quite closely, you certainly being one of them.

08:59.960 --> 09:06.800
And then in S and a few others, because as in for me, there's this whole question about

09:06.800 --> 09:16.200
AI ethics and yeah, AI safety, I only find it to be useful when we kind of got it more

09:16.200 --> 09:21.920
from the, let's call it the ontological level of like, what is AI, what is machine learning

09:21.920 --> 09:28.480
as we have it now, and kind of starting at like that very fundamental level ontological

09:28.480 --> 09:32.960
level as you've done, you know, in some of your papers.

09:32.960 --> 09:38.760
Because for me, that gives a lot more hate use to stone because it's kind of cheap, but

09:38.760 --> 09:43.320
like realistic view of where we are with machine learning and AI and where we can go.

09:43.320 --> 09:54.240
Or even sticking to, you know, sticking to the realm of not what is conceivable.

09:54.240 --> 09:57.560
I mean, I think there's a lot of discourse happening at the level of what is conceivable,

09:57.560 --> 10:02.920
what sorts of technologies are conceivable, or what sorts of technologies are metaphysically

10:02.920 --> 10:04.320
possible or what haven't you.

10:04.320 --> 10:15.840
And it's like, well, let's think about what might come into being in the reality we occupy,

10:15.840 --> 10:23.200
because what comes into being in the reality we occupy is governed by market forces.

10:23.200 --> 10:31.440
And we should think about technologies that some person might conceivably be incentivized

10:31.440 --> 10:34.360
and capacitated to build.

10:34.360 --> 10:40.200
If there are technologies that no one would ever be incentivized and capacitated to build,

10:40.200 --> 10:45.160
realistically speaking, I don't think there's much point in debating what their capabilities

10:45.160 --> 10:49.040
are or what the danger is emerging from these technologies would be.

10:49.040 --> 10:53.280
And I think there's a lot of debate that's happening at the level without considering,

10:53.280 --> 10:56.360
you know, incentives.

10:57.000 --> 11:10.920
Truthfully, I think this is, again, the primary sort of access of opposition in the AI ethics

11:10.920 --> 11:17.560
communities or community is between the risk safety people and the sort of fairness responsible

11:17.560 --> 11:21.960
AI people, fact community, et cetera.

11:21.960 --> 11:30.720
And maybe the failing point of both of these communities is, in their scholarship, a total

11:30.720 --> 11:37.200
failure, at least in the major part of the work, to really, really consider incentive

11:37.200 --> 11:39.560
structures.

11:39.560 --> 11:45.080
So the interventions we're suggesting have to be able to work within incentive structures

11:45.120 --> 11:48.280
as they exist or they might conceivably exist.

11:48.280 --> 11:55.680
They have to consider how we might, if the point is to manipulate incentive structures,

11:55.680 --> 11:58.080
we have to have that conversation explicitly.

11:58.080 --> 12:05.400
How are we intending that the intervention we suggest would nudge incentive structures

12:05.400 --> 12:06.400
as they exist?

12:06.400 --> 12:13.360
And I think there's just sort of a failure to consider that in large part in a lot of

12:13.360 --> 12:20.360
the work falling under the age of AI ethics, writ as largely and abstractly as we possibly

12:20.360 --> 12:21.880
can, right?

12:21.880 --> 12:23.080
Undoubtedly, undoubtedly.

12:23.080 --> 12:24.080
Yeah.

12:24.080 --> 12:30.320
I mean, it sounds like a very straightforward thing, but, you know, AI doesn't sit in a silo.

12:30.320 --> 12:34.280
It's always within a, it sits within a psychosocial reality.

12:34.280 --> 12:36.320
I couldn't agree more.

12:36.320 --> 12:44.520
Which is deeply, deeply complex to the point that it, having to think through incentive

12:44.520 --> 12:50.160
structures feels like it makes any problem in this realm completely intractable.

12:50.160 --> 12:51.160
Yeah.

12:51.160 --> 12:52.160
So I get exploits.

12:52.160 --> 12:57.040
Although I want to add, which is where I again like what you're doing, and we'll certainly

12:57.040 --> 13:02.400
get to this in a bit, kind of trying to understand, which is why for me, I think only a philosopher

13:02.400 --> 13:09.080
of science and perhaps even someone with a bit of a background in sociology can kind

13:09.080 --> 13:17.360
of explore the kind of epistemic status of what is machine learning or what is AI in

13:17.360 --> 13:22.000
contemporary times and then maybe speculate and theorize on where it could go and develop

13:22.000 --> 13:23.000
in the future.

13:23.000 --> 13:26.800
Because I'm going to be honest, as more of an outsider, some of this sounds a lot like

13:26.800 --> 13:32.280
science fiction to me and I'm kind of like, are we really even discussing as to what

13:32.920 --> 13:38.080
the current models are, what are large language models, what do they really do without all

13:38.080 --> 13:44.080
of this speculation, which again, the speculation seems just like it's just a bunch of blokes

13:44.080 --> 13:51.200
having fun without considering all the factors that you mentioned, our psychosocial reality,

13:51.200 --> 13:58.400
ways, AI, incentive structure, kind of symbolic network that all comes together, which seems

13:58.400 --> 14:00.520
like an intractable problem, I agree.

14:00.520 --> 14:01.520
Yeah.

14:01.600 --> 14:10.200
But I mean, I think a lot of philosophy is self-gratifying, in a sense.

14:10.200 --> 14:11.200
Oh, for sure.

14:11.200 --> 14:13.120
I mean, I would say a lot of thinking in general.

14:13.120 --> 14:14.120
Yeah.

14:14.120 --> 14:15.120
Yeah.

14:15.120 --> 14:19.960
I'd say a big part of it is for that, because it's self-gratifying, not for any ethical.

14:19.960 --> 14:20.960
Yeah.

14:20.960 --> 14:29.160
I mean, I think I have a very sort of pragmatic embodied view of thinking and it's sort of

14:29.200 --> 14:36.720
a very, I think that goes into sort of pragmatic ethical or political stance as well, where

14:36.720 --> 14:41.320
I think, I mean, I think we think in order to affect action in the world.

14:41.320 --> 14:47.000
Thinking, I don't think thought takes place.

14:47.000 --> 14:53.480
I don't think we can say cognition has really taken place unless we see the hallmarks of

14:53.480 --> 14:56.840
it's in behavior.

14:56.840 --> 15:04.920
So I mean, I actually think I'm a sort of weird behaviorist in the sense that I think

15:04.920 --> 15:13.200
that consciousness and sentience and agency and cognition and a lot of sort of what's

15:13.200 --> 15:23.840
been considered unobservable mental characteristics are actually readable from behavioral dynamics.

15:23.840 --> 15:34.200
And I think that we should not be thinking of philosophy as an eye will pursue.

15:34.200 --> 15:40.920
It impacts the way we go about the world or it ought to be approached as though it's

15:40.920 --> 15:49.320
going to have immediate impact on how we interact with the phenomena it treats as its subject

15:49.320 --> 15:50.320
matter.

15:50.320 --> 15:51.320
Yeah.

15:52.320 --> 15:57.280
I'm just saying I'm fully a pragmatist with respect.

15:57.280 --> 15:58.280
Yeah.

15:58.280 --> 15:59.280
I get you.

15:59.280 --> 16:01.440
But also, I mean, not only as a pragmatist, but I would even say, you know, I've been

16:01.440 --> 16:07.520
deeply interested in psychoanalysis and I've spoken to heaps of psychoanalysts and one misunderstanding

16:07.520 --> 16:12.120
I think a lot of people have coming from like the Carl Jung type is like the idea of depth

16:12.120 --> 16:18.960
psychology where they think the truth of a person's desires and motives are somewhere

16:18.960 --> 16:22.120
deep within and you need like a psychoanalyst to find it.

16:22.120 --> 16:27.440
But the flip is the Freudian more Lacanian idea is no, no, no, that the truth lies in

16:27.440 --> 16:28.640
your actions.

16:28.640 --> 16:35.080
The truth is never like deep within rather it's it's it's very it's very conspicuous.

16:35.080 --> 16:36.080
It's out there.

16:36.080 --> 16:40.800
Hey, you know, and you can see it in in how people act and how people engage in their

16:40.800 --> 16:41.800
social world.

16:41.800 --> 16:44.680
So I can say nothing is really hidden with that.

16:44.680 --> 16:45.680
Yeah.

16:45.680 --> 16:48.200
That's the ultimately come to realize nothing is really hidden.

16:48.560 --> 16:55.880
So you wrote, you've been working on a few papers, kind of working on the epistemic status

16:55.880 --> 17:01.440
of machine learning, which I've thoroughly enjoyed reading and learned a lot.

17:01.440 --> 17:06.080
One of them was this paper I read, this was I read this a while back, which is the machine

17:06.080 --> 17:08.880
learning and the theory free ideal.

17:08.880 --> 17:11.760
I'll leave a link to that in the in the show notes.

17:11.760 --> 17:16.360
But also the recent preprint you shared with me, which I thought was a very good provocative

17:16.360 --> 17:23.600
piece, in fact, titled ghosts in the machine learning, the reanimation of pseudoscience

17:23.600 --> 17:26.240
and its ethical repercussions.

17:26.240 --> 17:32.080
So to perhaps to orient the listeners, if you could kind of give us an introduction as to

17:32.080 --> 17:37.720
yeah, what is what is this work you're doing, researching the epistemic status of machine

17:37.720 --> 17:44.720
learning, apropos philosophy of science, and then perhaps, you know, you know, as a as

17:44.760 --> 17:54.040
carrying on from our ethics conversation or chat, how that how those two ideas are connected,

17:54.040 --> 17:58.400
you know, the epistemic status of machine learning, and then what's its relationship

17:58.400 --> 17:59.960
to ethics.

18:01.680 --> 18:02.680
Yeah.

18:02.680 --> 18:11.800
So there's a long history in 20th century philosophy of science of investigating the

18:11.800 --> 18:24.960
possibility of something like an automated science, or a formalized science, or a algorithmically

18:24.960 --> 18:31.680
implemented science, all aspects of science, including sort of uncovering theory.

18:32.680 --> 18:46.360
And this has been a rich tradition, but it's been strictly abstract about about hypothetical

18:46.360 --> 18:54.360
machines and hypothetical algorithms and hypothetical forms of automation of scientific labor rights.

18:54.360 --> 19:03.920
And all of a sudden, sort of, perhaps largely unanticipated by philosophers, we reach a

19:03.920 --> 19:07.960
point where boom, deep learning revolution.

19:07.960 --> 19:19.240
And suddenly, there is a kind of massive shift towards taking aspects of scientific discovery

19:19.280 --> 19:26.720
and passing them off to a intelligent computational system, right.

19:31.800 --> 19:39.840
And I think there's been kind of a tendency to continue to view that within the lens of of this

19:40.120 --> 19:50.720
automation of scientific discovery debate as it's existed, without really attending to the

19:50.720 --> 19:58.440
details of how these technologies are, in fact, being implemented in scientific practice.

19:58.440 --> 20:06.280
And by the way, I think I think when when what we are doing is collecting data from some system

20:06.280 --> 20:15.920
in the world, and using some machine learning model to extract statistical patterns from that

20:15.920 --> 20:20.200
system in order to learn about that system, what we're doing is effectively science, it follows

20:20.200 --> 20:26.360
the model of science. So in fact, most applications of machine learning ought to be considered a

20:26.360 --> 20:33.280
kind of science adjacent activity. And in my opinion, held to the standards of good science.

20:33.920 --> 20:42.280
Now, I think that there's a lot of good work being done with machine learning and science.

20:42.280 --> 20:49.320
There's a lot of really epistendically careful work. And I think there's also, like with most

20:49.320 --> 21:02.720
things, a glut of garbage, pick a genre of anything. I don't care if it's films or psychological

21:02.720 --> 21:12.880
studies, or books on the history of hip hop, or like whatever it is, or perhaps even people for

21:12.880 --> 21:20.160
that matter. Yeah, yeah, most most of it's crap. Yeah. And there's a very small percentage of it

21:20.160 --> 21:29.120
that's quite good. Machine learning is no different. There's a lot of good work being done, and 1000

21:29.120 --> 21:37.680
fold more bad work being done. The issue is that it's being adopted so rapidly, these methods are

21:37.680 --> 21:43.560
being adopted so rapidly, in so many contexts, across society, most people don't understand how

21:43.560 --> 21:52.840
the technology works. There's a lot of AI hype, as long as there has been AI, there has been AI

21:52.920 --> 22:01.080
hype. And what hype is, I think it should be made explicit that what hype is, is it's a targeted

22:01.080 --> 22:07.280
disinformation campaign. Yeah, perhaps you could elaborate on what you mean by that.

22:07.280 --> 22:20.040
Yeah, so AI hype refers to people, when Sam Altman says, we'll have AGI by X, or when Sam

22:20.040 --> 22:28.680
Altman says, well, GPT three could take a 20 minute activity and reduce it to five minute

22:28.680 --> 22:36.840
activity, but GPT next will reduce a week long activity to five minutes or something like that.

22:36.840 --> 22:38.360
You know, that's AI hype.

22:39.960 --> 22:46.760
Now, I saw I saw a recent statement by, I believe, anyway, I want to mention any names that the next

22:46.760 --> 22:54.800
GPT model will will be able to replace PhD researchers and we won't need any PhD researchers

22:54.800 --> 22:58.200
anymore. So yeah, I'm in shock.

22:58.200 --> 23:05.840
That's AI hype. Scientists and science popularizers saying the theorist will be replaced, the

23:05.840 --> 23:13.960
physicist will be replaced, the doctor will be replaced, you know, in medical context saying,

23:13.960 --> 23:17.640
oh, we won't need, you know, we won't need secretaries or we won't need nurses anymore.

23:21.080 --> 23:28.120
Any of that drama, but it also includes the doomerism about AI, like soon AI will surpass

23:28.120 --> 23:32.680
human intelligence and it will, you know, hurt us somehow.

23:32.680 --> 23:37.480
That's the part which I, for me, at least feels a lot like science fiction, if I'm being honest,

23:37.480 --> 23:42.200
the doomer. Yeah. Yeah. There's something even it's interesting to even science fiction.

23:42.280 --> 23:48.120
I mean, yeah. And I have nothing, nothing against I love science fiction, but it's got his own

23:48.120 --> 23:51.720
status in our dialogue, right? I mean, science fiction, it's called fiction.

23:51.720 --> 23:58.360
But saying that's, that's some particular tech breakthrough is going to replace

24:01.080 --> 24:10.760
cardiologists even is absolutely, will it, you know, have some impact on the role of

24:10.760 --> 24:19.160
cardiologists in specifically how they approach analyzing the results of imaging,

24:20.680 --> 24:26.200
right? Totally. Is it going to replace cardiologists? No, that's science fiction, right?

24:26.200 --> 24:31.320
And it should, given all technology affects how we do our jobs, how we live our lives,

24:31.320 --> 24:35.880
and that's completely fine. It probably should affect how they do their job.

24:36.680 --> 24:41.640
If it works, it should. That's, that's true. It doesn't work if it can, but if it works,

24:41.640 --> 24:45.240
it should have an impact. It's just that there's, there's,

24:48.680 --> 24:54.520
in anything that's, you know, marketable, there's an incentive to misrepresent

24:55.480 --> 25:04.440
what it does. This is true of, you can pull out a magazine from 1952 and how it's

25:05.080 --> 25:15.080
advertises some cooking implement to stay at home moms. And it misrepresent, you know, it's like,

25:15.080 --> 25:19.160
this is a life changing, it's going to lie to you about what the thing does, right? But

25:21.640 --> 25:27.320
with AI specifically, you know, I think, I think there's just to some extent,

25:27.880 --> 25:35.480
even the, the average housewife in the 1950s was like, yeah, probably, probably not all of this

25:35.480 --> 25:43.160
about the oven is actually the ground truth, you know, whereas with AI, people seem to be willing

25:43.160 --> 25:50.520
to believe really radically untrue things, you know, just things that are actually

25:51.480 --> 25:56.520
to anyone in the know, blatant lies. But there's a culture of

25:58.520 --> 26:10.520
really leaning into highly fabulous lies, propagating them to no end. And no one seems to be doing

26:10.520 --> 26:17.320
fact checking. And the worst part to me is, is, I would think that's in, so I'm, I'm coming from

26:18.040 --> 26:25.320
this position of history and philosophy of science, where it's like, we've studied hundreds and

26:25.320 --> 26:31.720
hundreds of years of science and technology, and how people misrepresent what it does and what it

26:31.720 --> 26:42.120
actually does, right? So we would, I would hope that we would have a critical lens on this. And

26:42.120 --> 26:45.800
it strikes me that there's a lot of philosophy that's simply

26:47.560 --> 26:52.920
parroting or, or lending kind of philosophical justification to these hype narratives that,

26:54.360 --> 26:59.960
you know, deep learning will radically change the face of particle physics or something like

26:59.960 --> 27:06.360
that. It's like, really, really though. Okay, perhaps just, well, this is, this is a good,

27:06.360 --> 27:11.880
good place to go to. So one, one term, which I really liked in the paper, which you use with

27:11.880 --> 27:17.480
this idea of global, the theory free ideal, I really liked that, that, that term. Because I

27:17.480 --> 27:23.400
think for me that captures what apropos philosophy of science or let's, let's say apropos the,

27:23.400 --> 27:31.800
the scientific method, what the hope or the dream with ML models are. So if you could melt, just,

27:31.800 --> 27:38.200
just to like, again, to flesh this out a bit more as, as to where we are right now with the

27:38.200 --> 27:45.080
current paradigm of machine learning, what is the epistemic status of machine learning?

27:45.880 --> 27:52.920
And then could you then probably connect that to what, what, what, what is this theory free ideal?

27:52.920 --> 27:57.800
And then as you point out in your papers, what are the mistakes that people make with, with,

27:57.880 --> 28:03.480
to, you know, by having this ideal of science being theory free when done with a,

28:04.200 --> 28:10.920
let's say an ML model, for instance. Yeah. Yeah. So I think machine learning models are statistical

28:10.920 --> 28:17.880
models that are computationally instantiated. There's, there's nothing that would in principle

28:19.560 --> 28:24.360
make the epistemic status of these technologies any different from

28:25.080 --> 28:33.320
any kind of other statistical model. Now, you're pushing yourself into really high

28:33.320 --> 28:43.480
dimensional spaces in which data is transposed. And so there's intrinsically,

28:45.320 --> 28:52.760
the, the dimensionality of the patterns you're finding is much higher than what you're doing

28:52.760 --> 29:03.960
with classical statistics. But I guess I doubt that when people are doing multiple regression,

29:04.840 --> 29:14.120
they are kind of holding all the dimensions in their head in the way that they would have to be

29:14.120 --> 29:20.600
in order for the contrast that's typically drawn between deep learning and classical statistics

29:20.680 --> 29:27.000
to make sense. Like there's, there's meant to be a kind of opacity, a kind of intrinsic deep

29:28.360 --> 29:33.640
unknowability of the kinds of patterns that these statistical methods that is deep learning methods

29:33.640 --> 29:39.320
are finding relative to classical statistical methods. And I just don't see that distinction being

29:41.960 --> 29:47.480
substantive and absolute in the way that it's proposed to be. I think these are at their heart

29:47.480 --> 29:52.200
statistical methods like other statistical methods. And if there's a difference,

29:53.960 --> 30:01.000
it's a sociological difference. Okay, I think I've followed you all the way except the last bit,

30:01.000 --> 30:06.520
the sociological, but if you could probably touch that up. Yeah, so it's, it's the corporatization

30:06.520 --> 30:12.280
of these technologies. I see. Okay. It's the hype narrative. So even in a research context,

30:12.360 --> 30:14.440
there's because, because machine learning, I mean,

30:19.000 --> 30:26.440
because machine learning is this place where stats met up with AI and AI is something that's

30:26.440 --> 30:32.280
always, AI refers to a lot of different research traditions that have had historically very little

30:32.280 --> 30:36.200
to do with each other in terms of their, their substance in terms of their subject matter in

30:36.200 --> 30:45.800
terms of the methods, they mostly have to do with where funding is being targeted and the kinds of

30:46.760 --> 30:51.400
narratives spun around these research methods. So there's very little substantive that holds

30:51.400 --> 30:55.560
everything that's historically been called AI to going back to cybernetics, going back to McCarthy,

30:55.560 --> 31:03.080
going back to, you know, all the tips and, you know, like going back all through the history of

31:03.080 --> 31:09.640
things being called artificial intelligence. There's very little that connects all of these, but

31:11.960 --> 31:16.280
who they're targeting for funding and the kinds of narratives they're using in

31:16.280 --> 31:24.440
convincing the public and funding bodies of what they're doing. So there are methods and statistics

31:24.440 --> 31:32.920
that have been, you know, approaching something like machine learning, going, going back 80s, 90s,

31:32.920 --> 31:42.920
whatever, right? But where that meets up with the AI narrative, you get this hype and disinformation

31:42.920 --> 31:58.760
and overselling of competence, right? And so there's, there's an attitude and a meta narrative

31:58.760 --> 32:06.440
surrounding machine learning that is, I think, more what sets it apart from classical statistics

32:06.440 --> 32:15.880
than anything else. This is, this is a spicy take, you know, kind of with a grain of salt. But

32:15.880 --> 32:24.040
this is my, my challenge is really like, tell me what is so radically epistemically different

32:24.040 --> 32:30.680
about these technologies that they should be placed in some category that's discrete from

32:30.680 --> 32:36.840
classical statistics. I have not seen it, you know. And you point this out. Yeah. And also,

32:36.840 --> 32:44.280
I guess it's this meta narrative that, that drives its impetus behind this theory free ideal

32:44.920 --> 32:50.920
when it comes to science. Yeah. And, and there's been so philosophers, natural philosophers,

32:50.920 --> 32:58.360
scientists have debated what theory is and what its proper role in science is since the, the

32:58.360 --> 33:03.480
incipients of what we call modern science, right, since Bacon and Newton and Galilea, you know,

33:03.480 --> 33:13.000
modern science, right? Even going back to Bacon, there's, there's, there's a kind of push for a

33:13.000 --> 33:22.120
kind of radical empiricism that pushes away as much as possible the role of theory about just,

33:22.120 --> 33:26.600
just tabulating as much data as possible and sifting through it for patterns and, and not

33:26.680 --> 33:30.280
bringing our kind of conceptual infrastructure to bear on it. But then since Hume,

33:32.120 --> 33:37.080
since Hume brought his sort of problem of induction to the table in epistemology,

33:39.000 --> 33:45.080
there's this widespread recognition that, well, all, all knowledge of the natural world

33:45.080 --> 33:49.640
is knowledge by induction. You do not get deductive certainty about empirical

33:49.640 --> 33:57.960
matters. Yeah. So the typical example is the sunrising just because it rose

33:57.960 --> 34:03.800
yesterday. We can't say necessarily it'll every day of our lives, the sun has risen in the morning.

34:03.800 --> 34:09.400
It's risen. Yeah. Yeah. And yet give me a deductive proof, give me logical certainty

34:09.400 --> 34:13.720
that it will rise tomorrow. There is none. There's only the, the

34:14.120 --> 34:22.280
you know, the remit of our experience to tell us that it will rise tomorrow. There's no

34:23.400 --> 34:25.960
logical necessity that it will rise again tomorrow.

34:28.520 --> 34:35.480
So knowledge of nature, scientific knowledge is, and all our kind of day to day practical

34:35.480 --> 34:39.640
knowledge, like I can eat this bread and it won't poison me because the bread I ate yesterday that

34:39.720 --> 34:45.080
I got from the same baker didn't poison me. You know, this is inductive knowledge, which means

34:45.080 --> 34:53.800
we don't get deductive certainty. And it also means that to get that kind of knowledge, you need to

34:55.080 --> 35:01.960
start off with rich conceptual infrastructure, which I'm calling theory. I think, I think when,

35:01.960 --> 35:05.240
so I think there are lots of ways that philosophers have traditionally

35:05.240 --> 35:10.440
cashed out what we mean by theory, I think that when we talk about theory free science, we mean

35:12.520 --> 35:19.400
a powerful influence at the beginning of inquiry, at the beginning of the investigatory

35:19.400 --> 35:25.720
procedure of our prior conceptual resources, our prior conceptual

35:26.920 --> 35:32.360
acquaintance with the target phenomena, right? We do not get, we do not get inductive

35:32.360 --> 35:38.280
inference off the ground without bringing to bear prior theory or conceptual.

35:39.720 --> 35:43.160
The philosopher and historian of science, John Norton calls it

35:43.880 --> 35:49.880
bringing to bear material facts, right? But there are lots of ways of putting it, but

35:51.880 --> 35:58.040
you need theory to get empirical knowledge off the ground. And so in this sense,

35:58.760 --> 36:05.320
you cannot have theory free knowledge of natural systems. And I think that

36:08.200 --> 36:12.120
you can look back for hundreds of years, and there's always this dialogue between, no, we should get rid

36:12.120 --> 36:19.800
of as much, as much as we can push away the influence of prior conceptualization, we should do that,

36:19.800 --> 36:25.800
and that's scientific objectivity. This is, I think, one notion of scientific objectivity.

36:26.200 --> 36:33.640
Right? That has been kind of implicitly in the background of a lot of discourse in philosophy,

36:33.640 --> 36:39.160
natural philosophy science for hundreds of years. And then another stream that says,

36:40.680 --> 36:45.800
well, you can't actually have knowledge of nature without bringing conceptual resources to bear.

36:45.800 --> 36:51.960
So it's about documenting them. It's about recognizing them. It's about, to some extent,

36:51.960 --> 36:57.240
those assumptions and saying which of those assumptions are, in fact, substantiated by

36:57.240 --> 37:03.400
what we've then been able to observe and deduce from what we've measured or observed in nature,

37:03.400 --> 37:09.880
right? And what is, in fact, just arbitrary or unknown, right?

37:12.280 --> 37:18.120
And I think since the rise of domain generic statistical methods in the 20th century,

37:18.200 --> 37:22.840
in particular, stats really gets off the ground after the axiomatization of probability theory

37:22.840 --> 37:31.960
with Komagorov. Statistical reasoning, probabilistic reasoning is, to the extent that's so widespread

37:31.960 --> 37:36.840
in science, it's relatively new. It's really a kind of 20th century, like statistical reasoning

37:36.840 --> 37:41.960
is kind of a 20th century thing. I mean, it sort of got off the ground with gambling and stuff in

37:41.960 --> 37:52.040
the 17th century. But as a scientific method, it's new. And really, I think since the mid-20th

37:52.040 --> 38:03.800
century, you get a lot of this, what I call a theory free ideal. And it's in the kind of

38:07.080 --> 38:11.320
fundamental, I don't know if I believe in this destination, but in the more fundamental sciences

38:11.960 --> 38:18.680
who know how to theorize because they've been doing it for hundreds of years and know how to

38:19.240 --> 38:23.640
mathematically represent their phenomena, because they've been doing it for hundreds of years,

38:25.560 --> 38:34.120
you get less of this. But in the younger sciences, like the quantitative social sciences, like

38:34.120 --> 38:48.840
social psychology, like population genetics, what have you, economics, there's this belief that

38:51.160 --> 38:57.720
the more theory free, the more data driven the methods are, the more objective they are and

38:57.720 --> 39:03.640
the more sciencey. Yeah, so when you mean the more fundamental, you mean like physics, for instance,

39:03.640 --> 39:11.320
right? Yeah, areas of, areas of, but not all areas of physics, right? The areas of physics

39:11.320 --> 39:16.040
that are really established. Yeah, although, although, you know, this beautifully connects

39:16.040 --> 39:21.160
with your sociological point, because I'm sure you're aware of the whole Bohr-Einstein debate and

39:21.160 --> 39:25.000
that, you know, shut up and calculate. There's like a lot of, in the history of 20th century

39:25.000 --> 39:31.000
physics, is his idea that even within physics, you shouldn't ask, but given that physics is the,

39:31.080 --> 39:36.200
it's a foundational discipline, the ontology of the physical world, you know, there was a time,

39:36.200 --> 39:43.400
especially because of, you know, World War II and the nukes and all that, just don't ask the,

39:44.440 --> 39:48.600
don't ask the ontological questions, just shut up and calculate, just that there's a

39:48.600 --> 39:55.480
ruthless pragmatism, ruthless, just create. And so physics kind of sometimes, and again, I again

39:55.480 --> 40:01.160
say this as a bit of an outsider, it becomes a bit more like engineering or a foundational

40:01.800 --> 40:05.320
discipline where you're asking, well, what exists, what is reality?

40:05.320 --> 40:07.160
That's sort of the Los Alamos attitude, right?

40:07.160 --> 40:09.080
As in the idea that physics is engineering?

40:10.680 --> 40:12.760
Does that happen and calculate attitudes?

40:13.560 --> 40:18.680
Yeah, yeah, I mean, I know more from the Copenhagen school, I mean, I followed that, I think Tim

40:18.680 --> 40:25.080
Modellin, he speaks quite well about that. And just, it's like a, you know, interesting

40:25.080 --> 40:30.600
peculiarity in the history of physics, especially because if you look at the big figures, like,

40:30.600 --> 40:37.880
like the Einstein's, or like the Maxwell's, they were deeply interested in these philosophical

40:37.880 --> 40:43.000
questions, it's like, what exists, you know, it's not Einstein was a philosopher, he certainly was,

40:43.000 --> 40:48.200
he certainly was, you know, without a doubt. Yeah. I mean, probably after Newton is probably the

40:48.200 --> 40:54.360
quintessential natural philosopher, I couldn't agree more. And yeah, so I mean, just on this note,

40:55.240 --> 41:01.080
I couldn't read the paper too carefully, but I do love the, perhaps it's worth mentioning,

41:01.080 --> 41:06.840
because it's rather provocative and I like that a bit, where you say, yeah, machine learning has

41:06.840 --> 41:12.760
a pseudoscience problem. And then you bring up the, well, you and the other other writers, Andrew

41:12.760 --> 41:21.880
and Bieber, is it? Yeah, bring up. Yeah, yeah, they bring up the idea of how there's a resurgence

41:21.880 --> 41:29.800
of physiognomy in kind of these ML communities. So just, if you could just humor me with that for

41:29.800 --> 41:35.480
a bit, just kind of what all that, that's about why you claim, you know, machine learning has a

41:35.480 --> 41:40.600
pseudoscience problem, which I think you already kind of did outline quite, quite in detail. But

41:40.600 --> 41:46.840
then this little, little example you use on, on physiognomy. Yeah, well, part of it is this sort,

41:46.840 --> 41:53.000
this sort of runaway idea that we can do science without theory that picks up steam in the mid

41:53.000 --> 41:58.920
century. And then with the rise of machine learning techniques, and these being adopted

41:58.920 --> 42:04.520
widely, it just becomes, it becomes, it gets bundled into this hype narrative about how these

42:04.600 --> 42:12.840
technologies work. And then everyone sort of believes that these technologies are capable of

42:14.360 --> 42:24.120
extracting true knowledge of some natural system in virtue of having achieved high

42:24.120 --> 42:37.720
classifier accuracy on some natural data set, right? And what's actually happening is, is researchers

42:37.720 --> 42:44.040
are interpreting that pattern as having discovered precisely whatever their intuitive idea of what

42:44.040 --> 42:48.120
they were going to discover was beforehand. Because if they're not explicitly doing the

42:48.120 --> 42:53.640
theory, if they're not, if they're not explicitly theorizing, then they're implicitly theorizing,

42:53.640 --> 42:58.760
which means that they're effectively trying to con you into believing whatever their intuitions

42:58.760 --> 43:05.480
were at the start of the research procedure, without effectively having furnished evidence of

43:05.480 --> 43:12.600
that, besides having told you that they train some model, and there's some pattern in the data that

43:12.600 --> 43:21.240
satisfied some criteria, right, for success. But I think because of all these hype narratives

43:21.240 --> 43:30.920
surrounding machine learning, and again, not for substantive epistemic differences in how these

43:30.920 --> 43:38.520
statistical methods work, but rather for sociological reasons, you get a lot of bad, bad,

43:39.240 --> 43:50.760
bad science. So if I want to do a quantitative social science study, and I'm in a sociology

43:50.760 --> 44:00.760
department, or an economics department, my advisors won't let me do that until I've

44:00.760 --> 44:07.320
read up on the hundred years long history of scientists in my field having studied that

44:07.320 --> 44:13.160
exact problem, right. Machine learners on the other hand, machine learners don't even read the

44:13.160 --> 44:20.760
history of their own work. Machine, like, it is not typical for someone in machine learning to have

44:22.280 --> 44:30.200
even a five years deep understanding of the history of their own field, right,

44:31.400 --> 44:35.960
which is there are of course exceptions, but the general rule is people in machine learning do not

44:35.960 --> 44:42.040
read. Yeah. And so by you're talking, are you talking about more on the scientific side, or do

44:42.040 --> 44:48.600
you just mean general, you know, commercial ML engineering? Practitioners, but also in academics.

44:48.600 --> 44:58.360
Academics, okay. That's unfortunate. And so when you go to apply ML, you know, everything that's

44:58.440 --> 45:02.600
submitted to Art Tripoli, or, or NeurIPS, or what have you.

45:05.720 --> 45:14.680
ACM, you get just this glut of work of people applying the methods of ML, especially DL, to

45:15.560 --> 45:21.400
some problem that scientists have spent maybe hundreds of years working on. And there's no

45:21.400 --> 45:27.400
acknowledgement that what they're tackling, what they're attempting to tackle is a scientific problem

45:27.400 --> 45:36.360
that some very specific field of, you know, molecular biomechanicists or whatever the field is,

45:36.360 --> 45:42.920
right, have been working on for hundreds of years. And then there's this attitude that

45:43.640 --> 45:48.920
while deep learning will solve the problem, and I don't have to pay my dues and read about the

45:48.920 --> 45:55.800
methods in this field. And then the reviewing practices at, well, you know, part of it's like

45:55.800 --> 46:01.800
we got rid of, we got rid of traditional peer review and machine learning, which is like, was

46:01.800 --> 46:09.800
traditional peer review hopelessly broken? Yes. Did we introduce new problems by getting rid of it

46:09.800 --> 46:16.120
wholesale? Also, yes. Right. And so then you've got, you know, so there aren't standard journals in

46:16.120 --> 46:26.600
machine learning the way they are in biomechanics or biochemistry or socioeconomics or whatever,

46:26.600 --> 46:39.240
right? You are submitting to the big name machine learning conferences, but peer review there is

46:39.560 --> 46:51.160
I mean, I have to say it's pretty radically incumbent. I don't know that I don't think anyone

46:52.280 --> 47:05.640
who reviews for or submits to machine learning venues would try to fight me on that. I mean,

47:05.640 --> 47:14.360
I think the consensus is that the peer review process for these venues is wildly inadequate.

47:14.360 --> 47:19.320
Inadequate, yeah. So and it's because it's because when you apply machine learning,

47:20.840 --> 47:26.520
right, you're applying it to some domain, you're applying it to some domain where there is a vast

47:26.520 --> 47:33.080
history of people trying to solve some problem. And when you submit your little deep learning thing

47:33.080 --> 47:39.320
to IEEE and you're trying to tackle some problem in social science, they're not asking social

47:39.320 --> 47:46.920
scientists to review that. God, no. Right. They're asking other people who trained a, you know,

47:47.960 --> 47:55.560
transformer to with data of that shape, right? But they're not asking people who have the

47:55.560 --> 48:03.480
disciplinary knowledge to review the methods for what actually matters to doing science, right?

48:06.200 --> 48:12.680
Yeah. Yeah. And when do you think this changed? Well, was this, is this imminent to the

48:13.800 --> 48:19.640
practice itself? Or when do you think this change took place where the peer reviewing

48:19.640 --> 48:24.360
method became a bit lax or inadequate as you pointed out?

48:26.680 --> 48:26.920
Well,

48:33.960 --> 48:37.560
it was just sort of an organic thing, right? You have

48:40.040 --> 48:46.520
on the one hand, I mean, it was never the case that it was never the case that there were

48:47.160 --> 48:51.960
people applying machine learning, as far as I know, where it was never the case that people were

48:51.960 --> 48:56.680
applying machine learning to some problem in biology, and then submitting that to a biojournal.

48:56.680 --> 49:01.240
That's not the, I mean, occasionally that happens, right? But that's not standard practice. And I

49:01.240 --> 49:10.040
don't think it ever was. It was also not the case that there were kind of standard journals for,

49:10.040 --> 49:14.600
there were standard journals for stats, right? But there were never kind of standardized deep

49:14.840 --> 49:22.520
learning journals, right? There were, there were computing or stats

49:25.000 --> 49:33.320
conferences that got kind of evolved into ML specific conferences or new ML specific conferences

49:33.320 --> 49:39.720
emerged. And like, you know, a lot of the main ones are actually, they didn't start out as ML

49:39.880 --> 49:46.920
conferences, and they evolved to be ML conferences. But also you have at the same time the emergence

49:46.920 --> 49:58.360
of pre-printing servers. And so you get, you get the emergence of a new kind of, like machine

49:58.360 --> 50:04.440
learning has been machine learning, the methods we call machine learning have been around since

50:05.400 --> 50:11.800
like the 80s, right? You could trace it back earlier to kind of proto machine learning methods.

50:13.000 --> 50:18.200
Those go back much, I mean, again, like, it was out of World War II, it was out of the research

50:18.200 --> 50:24.920
at Los Alamos, that you got like MCMC sampling, right? Like Mark O'Chain Monte Carlo sampling,

50:24.920 --> 50:33.000
like Metropolis Hastings sampling. I didn't know that. Yeah. Yeah, it goes back to like the early

50:33.080 --> 50:39.080
50s, late 40s, early 50s. So even before that's kind of machine learning, right? Yeah.

50:42.040 --> 50:47.320
But, but, you know, machine learning as it's the early 2000s that machine learning kind of

50:48.120 --> 50:51.880
goes like, hey, we're a scientific field, or hey, we're an engineering discipline, like we're a

50:51.880 --> 50:58.360
discipline now, right? And it's at the same time that you're getting pre-printing servers as the

50:58.360 --> 51:02.600
sort of way that that stuff is disseminated. So there's, there's, effectively, there's no

51:03.560 --> 51:08.920
incentive to start journals, and there's incentive against starting journals, I would say. This is

51:08.920 --> 51:13.800
my, I'm making this up on the spot, but that's kind of how I would reconstruct that history.

51:13.800 --> 51:18.360
No, that makes sense. Yeah. I mean, it's partially contingent. It's just historically how things have

51:18.360 --> 51:21.880
been. Yeah. And also, there's this widespread recognition that there's something deeply broken

51:21.880 --> 51:28.280
about traditional peer review, which is true. Yeah. Which virtually, I believe all every academic

51:28.360 --> 51:33.000
I've spoken to has said that. So unequivocally, I think it's just a general consensus. Yeah. Yeah.

51:33.000 --> 51:37.800
Excellent. Excellent. Now, that's, that's, that's great, Mel. I want to be cognizant of the time,

51:37.800 --> 51:42.760
which is, which is why I want to get to this. And I'm sorry if this sounds like I'm flattering you,

51:42.760 --> 51:48.840
but your paper on the free energy principle, it is, I've probably read it like three or four

51:48.840 --> 51:55.080
times. And I think I can probably parrot out certain parts of it verbatim, because I've read it so

51:55.080 --> 51:59.480
many times, especially because I hate it. It's actually, it's a really good paper. It's a fantastic

51:59.480 --> 52:03.480
paper. It's a fantastic. And so now I'm trying to like, I'm like, okay, what, you know, I'm trying

52:03.480 --> 52:09.480
to write a subsequent paper. And it's like, it's not that good. I mean, because like it's, it's

52:09.480 --> 52:14.440
probably your most cited paper, right? I mean, I found you through this. In fact, like, I didn't

52:14.440 --> 52:20.040
even know who you are until I came across this work. Yeah. So just for the listener, it's called

52:20.120 --> 52:25.800
the math is not the territory navigating the free energy principle. Yeah, I mean, it's just,

52:25.800 --> 52:30.680
it's philosophically interesting. It's got so much into like the history of science. And then,

52:31.400 --> 52:36.760
you know, like, what is formalization? You speak about the structure like, yeah, he keeps a lot

52:36.760 --> 52:41.960
to discuss here. Although, although before we get to the, let's say the nitty-gritty, and one thing

52:41.960 --> 52:46.680
I want to mention is, so I've been trying to get through this, this book on active inference. And

52:46.680 --> 52:53.960
I've got to say, because when I'm reading this, every page, I'm kind of reading it in a way through

52:53.960 --> 52:59.720
the kind of the lens of what you put in, in me through this paper, you know, it's like, I've

52:59.720 --> 53:07.000
got a bias now, I've got the, I've got the, the math is not the territory bias. Because it really

53:07.000 --> 53:12.120
helped me understand. Yeah, it really did help me understand the ontology of what really is the

53:12.200 --> 53:17.000
free energy principle, because it's got so much interest. So many people talk about it. And,

53:17.000 --> 53:23.400
you know, Carl Friston, he's fantastic. I've learned so much, but he sometimes isn't the

53:23.400 --> 53:29.080
best elucidator, you know, like he, when it comes to, you know, he's not a philosopher. Correct. I

53:29.080 --> 53:36.520
think that could be the reason Einstein was a philosopher. Carl Friston is like Isaac Newton,

53:36.520 --> 53:44.920
in that he's kind of like, I don't care what's like, he'll ascent to any metaphysics,

53:44.920 --> 53:49.400
Newton would ascent to any Newton was like, I'm not doing metaphysics. I'm associating

53:51.560 --> 53:58.440
relationships I see in data. Right. But don't tell me about the physical seat of gravity. I'm

53:58.440 --> 54:02.120
not talking about that. Yeah. Oh, yeah. I mean, that certainly wasn't wasn't a dig at

54:03.080 --> 54:07.400
Professor Friston, because he he himself says, he says, yeah, I'm not a philosopher, I'm a

54:07.400 --> 54:12.520
scientist. And I don't really, when he doesn't even really take a philosophical position or

54:12.520 --> 54:20.280
metaphysical position pertaining to the FEP. But but having said that, Mel, so, oh, sorry,

54:20.280 --> 54:27.160
be that as it may, I mean, what, what, why, yeah, why do you think there's such deep philosophical

54:27.160 --> 54:33.400
interest in the free energy principle of every any philosopher who works in the philosophy of

54:33.400 --> 54:39.560
biology or the cognitive scientist or ML, but it's such deep philosophical interest. So, yeah,

54:40.440 --> 54:45.800
why do you think the reason for that is, I think there are a lot of reasons one of them is, okay,

54:45.800 --> 54:53.080
so I think, I think of all math, all applied mathematics, or scientific math or models,

54:53.080 --> 54:57.880
as a kind of thinking tool, math is a thinking tool for science, right. But

54:59.720 --> 55:05.400
the free energy principles are thinking tool in a different way in that it's not actually meant to

55:06.200 --> 55:13.640
be placed in contact with empirical data, right, it's just about enabling us to conceptualize

55:13.640 --> 55:21.800
of target phenomena and new ways. And it happens to be new ways that are actually really

55:24.040 --> 55:33.480
a philosophically generative and novel to suck to agree. Not exactly novel, but overlooked. I mean,

55:33.480 --> 55:41.320
you get some of like, some of what's being said about life. It's if you look hard enough, it's

55:41.320 --> 55:50.280
really in Schrodinger's, Schrodinger's what is life. You look at that text, there's a lot of the

55:50.360 --> 55:55.560
ideas that are being brought out in the FEP in that text originally.

55:57.880 --> 56:03.560
Yeah, he brings up new ideas go back, but largely ignore, because they're very much

56:04.680 --> 56:11.080
opposed to the kind of neo Darwinian canon that we have now in biology, where we're really looking

56:11.080 --> 56:20.520
at population level analysis. And you're not looking at physical exigencies or structural

56:20.520 --> 56:26.840
exigencies of biology at the physical systems involved in biology and what kinds of necessities

56:26.840 --> 56:35.800
need to be there for life to exist. And then connecting that up to a view of cognition as

56:35.800 --> 56:41.720
fundamentally oriented towards action and interaction with the environment too. I mean,

56:41.720 --> 56:51.720
that's also there. So there's a lot that's philosophically rich that is associated with the FEP

56:51.720 --> 56:57.880
in how the FEP is discussed. It's not necessitated by the FEP, the FEP is just math, but it's math

56:57.880 --> 57:04.200
that allows us to conceptualize of things that we're taking away. It's also cool math, it's fun

57:04.200 --> 57:08.840
math. Part of it is there's a lot of flourished that math that doesn't need to be there. That's

57:08.840 --> 57:18.440
just it just if you like math, it's it's cool. And you can keep pushing it new cooler. It's

57:18.440 --> 57:26.280
kind of it's it's like this magpied. It's like a bunch of shiny math taken from 18 difference

57:26.840 --> 57:32.520
distinct. You know, you've got you've got some of its machine learning math.

57:33.480 --> 57:39.000
Like some of it just is it just is elbow, right, to minimize free energy, the quantity known as

57:39.000 --> 57:44.040
free energy as a kind of information theoretic construct to minimize free energy is is to just

57:44.920 --> 57:49.080
optimize the evidence lower bound, which is elbow, which is machine learning technique, right?

57:49.640 --> 57:56.280
Um, it's also Fokker Planck, or the master equation, or the Kolmogorov forward equation,

57:56.280 --> 58:05.480
which is, you know, a principle of stat mech. It's also, I connected up to Max Ent,

58:05.480 --> 58:10.680
maximum entropy principle, which is a sort of, James,

58:11.320 --> 58:22.680
put forward this idea that we can view. Basically, the core principles of thermodynamics,

58:22.680 --> 58:31.400
expressed statistically, can also be reoriented as a kind of

58:32.280 --> 58:45.800
epistemic principle for like keeping your priors flat, basically, except for when the evidence.

58:46.920 --> 58:52.520
Does that make sense? No, I think I got the latter bit of it regarding the epistemic principles.

58:52.600 --> 58:58.920
Yeah. Yeah. So, James in the, is this 50s? Max Ent, James.

59:01.240 --> 59:02.920
When's the first Max Ent paper?

59:05.240 --> 59:10.520
50s, right? Yeah, it's a 50. It's 57. It's 57.

59:13.560 --> 59:17.240
Yeah. So, the idea is that. Yeah.

59:17.960 --> 59:22.120
Yeah. 57. That's right. Yeah. Yeah. BT James. Two papers.

59:23.960 --> 59:30.200
So, it's like a closed thermodynamic system. The principles of thermodynamics tell us that

59:30.200 --> 59:37.640
a closed thermodynamic system will max out its entropy, right? Like, that's,

59:40.600 --> 59:43.960
this is what thermodynamics tells us. At a certain point, yeah, of course. Right.

59:44.680 --> 59:50.760
Ultimately, in the limit. You can give this

59:52.920 --> 01:00:02.600
a gloss as a principle for best inference. Like, it's already probabilistically formulated,

01:00:02.600 --> 01:00:16.520
but then view that as a rule for governing the probability distribution over some belief.

01:00:18.200 --> 01:00:26.840
And it's, again, Max Ent, because your priors need to be as flat as,

01:00:26.840 --> 01:00:32.120
as the probability distribution is as flat as it can be, given the evidence, right?

01:00:33.240 --> 01:00:37.480
So, there's.

01:00:40.440 --> 01:00:46.600
And you're saying the FAP does bring this into its, it's theorizing too, like.

01:00:47.320 --> 01:00:52.600
But they did. I don't think they did. I think I pointed it out, and then they started to,

01:00:54.280 --> 01:00:58.680
so there's, their recent work is actually explicitly incorporating the James stuff.

01:00:58.760 --> 01:01:03.800
Okay. Fascinating. Yeah. I think it was actually because I was like, so James, right?

01:01:05.480 --> 01:01:09.000
That's fascinating. I didn't know that, because I mean, I mostly view it through,

01:01:09.000 --> 01:01:13.560
you know, predictive coding and Bayesian reasoning. I mean, really, that's.

01:01:13.560 --> 01:01:20.200
I actually doubted that lineage. I doubted that lineage, but I thought that they've

01:01:20.200 --> 01:01:25.800
made that connection explicitly beforehand. But the people who are now explicitly doing the

01:01:25.800 --> 01:01:31.640
Max Ent incorporating with FEP are attributing it to me. So I'm like, you know, it's if he wants.

01:01:32.440 --> 01:01:37.720
Okay. Yeah. Are there any, because like, I mean, I'd love to read up on this. Are there any papers?

01:01:37.720 --> 01:01:40.520
Because I mean, I've only just started on the book. This is the.

01:01:40.520 --> 01:01:43.800
I think the best. Okay. So, so the, Tom Parr is fucking excellent, but.

01:01:44.360 --> 01:01:48.040
And all of those people who wrote that book are excellent, but I think.

01:01:48.040 --> 01:01:49.720
They're very good writers too. Yeah.

01:01:50.680 --> 01:01:51.960
I think the most competent.

01:01:54.520 --> 01:01:59.960
Mathematician who's working within the free energy framework is Dalton.

01:02:01.960 --> 01:02:05.400
Okay. I got a, who I spell that.

01:02:06.360 --> 01:02:08.440
Do you know how to spell Tamil names? Yeah.

01:02:09.880 --> 01:02:14.280
The first name is very Scottish, which is a little well. Okay. There we go. Yeah.

01:02:15.160 --> 01:02:24.040
Um, so actually they, they took it from, so they took the Max Ent. I, so I proposed like, hey,

01:02:24.040 --> 01:02:30.040
you know, well, what the FEP is kind of is you're, you're, you're taking principles of statistical

01:02:30.040 --> 01:02:37.000
mechanics and you're epistemic sizing them. You're giving them an inferential reading.

01:02:37.640 --> 01:02:41.080
You're taking laws, laws of stats. Yeah. Yeah.

01:02:41.080 --> 01:02:46.360
And you're reading them as principles, as principles for.

01:02:48.920 --> 01:02:55.000
Cognition. Yeah. Yeah. I mean, I mean, I, I, I, I, I'd say for me initially when I came

01:02:55.000 --> 01:02:58.200
across FEP, that's what I found quite interesting where, you know,

01:02:58.200 --> 01:03:02.680
Carthus and his background is even in physics and like you take from statistical mechanics,

01:03:02.680 --> 01:03:07.480
which is in physics. I think, I think he did a bachelor's in physics in like the 60s.

01:03:07.480 --> 01:03:11.160
Yeah. I shouldn't say background. You're right. I think he studied physics. He certainly is a

01:03:11.160 --> 01:03:17.400
neuroscientist, but, but like, I think he does like the physics he's getting is largely outdated.

01:03:18.920 --> 01:03:22.840
Well, I think he's just drawing from a lot of areas of, of physics.

01:03:25.400 --> 01:03:31.320
And really it's like, he's in clinical neuro, right? He's, he's spent a career in clinical

01:03:31.400 --> 01:03:36.920
neuroscience. Well, I mean, generally what interested me was that the fact that you take these

01:03:37.480 --> 01:03:41.320
theories and principles from physics, like, you know, all from statistical mechanics or,

01:03:41.320 --> 01:03:47.960
as you point out, like, you know, max and, and then apply them to move to like cognition,

01:03:47.960 --> 01:03:54.200
reasoning, Bayesian inference and the likes. I just, I don't know. I find that fascinating.

01:03:54.200 --> 01:03:59.640
And I don't know. I just feel like I, again, as a neophyte, just, it just excites me to see where

01:04:00.360 --> 01:04:06.120
the, the developments that goes on the FEP and the, you know, the concomitant, more engineering

01:04:06.120 --> 01:04:14.280
work that comes along with it. Having, having said that, Mel, one thing is this, in your paper,

01:04:14.280 --> 01:04:19.160
you say the math is not the territory. And I think here's where we get to the, really, the,

01:04:19.720 --> 01:04:23.640
the crux of the argument, the philosophy of this paper. And I'm just going to read out a bit of

01:04:23.640 --> 01:04:28.680
an excerpt. I think it's valuable for the, for the listener in case they haven't read it already,

01:04:28.680 --> 01:04:34.520
which I, I, if you're interested in the FEP or ML, for that matter, I highly recommend reading

01:04:34.520 --> 01:04:39.000
this. So you just, you claim here, I think this is from the abstract. I'm not sure. Anyway, I've

01:04:39.000 --> 01:04:44.600
got this excerpt here. Conceptual verification is a common ailment of scientific modeling.

01:04:45.160 --> 01:04:52.520
It is particularly likely to occur in cases in which models have somewhat convoluted histories.

01:04:53.160 --> 01:04:55.960
Rification involves, and here's the important bit, in fact,

01:04:56.600 --> 01:05:03.240
verification involves mistaking an aspect of a model, its structure, its construal, or the union

01:05:03.240 --> 01:05:11.720
of both, for an aspect of the empirical of the natural world, mistaking the math for the territory,

01:05:11.720 --> 01:05:19.480
so to speak. So yeah, could you please elaborate on that little statement and then, you know,

01:05:19.480 --> 01:05:28.200
kind of connect that to the FEP? Yeah, so I think, I think there's a tendency to

01:05:32.120 --> 01:05:39.800
conflate scientific realism with realism about the conceptual tools that we use in science.

01:05:39.800 --> 01:05:45.560
So I'm, I'm a full-blooded scientific realist. I think, albeit a pragmatic realist.

01:05:46.520 --> 01:05:51.080
I'm somewhat of a content. I think I'm more of a real, I think I'm more of a realist-realist

01:05:51.080 --> 01:05:54.680
than most people who call themselves pragmatic realists. I think people who call themselves

01:05:54.680 --> 01:05:57.320
pragmatic realists, I'm like, you're not really a realist. I'm actually a realist,

01:05:57.320 --> 01:06:03.240
just pragmatically so. No, I mean, I would say science without a doubt, it does give us

01:06:03.240 --> 01:06:13.560
truths about reality. I mean, I, in the content sense, let's say. Yeah, whatever the truth knowledge,

01:06:13.560 --> 01:06:19.480
whatever the epistemic goods of science are, I want to say it's truth sub-pragmatic or knowledge

01:06:19.480 --> 01:06:27.160
sub-pragmatic, right? I think, I think there's, it's not absolute, all-encompassing, omniscient

01:06:28.840 --> 01:06:36.840
truth about nature. It's, it's pragmatic truth. It has its limits and it's oriented towards us

01:06:36.840 --> 01:06:41.560
as the kinds of beings that we are having to occupy the world and navigate it the way that we do,

01:06:41.560 --> 01:06:46.440
right? But I think it is truth. I mean, if anything is truth or knowledge, that's, that's

01:06:46.440 --> 01:06:52.440
what that is. Exactly, with all those caveats, it gives us truths, let's say. Yeah, yeah, yeah.

01:06:52.440 --> 01:07:01.400
But then we introduce all this conceptual machinery into science to be able to do science,

01:07:02.440 --> 01:07:10.440
like the idea of a Hamiltonian or the idea of a gravitational field or the idea of a fermion

01:07:10.440 --> 01:07:19.000
or the idea of, you know, a force function. And that includes all the, you know, the

01:07:19.000 --> 01:07:22.600
Fokker-Planck equation that includes all the sort of mathematical infrastructure too.

01:07:22.600 --> 01:07:36.760
These are all the conceptual tools of science. And I think that there's a lot of slippage that's,

01:07:37.560 --> 01:07:44.920
that's like, well, science delivers truth. Therefore, all the conceptual tools we introduce

01:07:44.920 --> 01:07:50.840
in the process of doing science and getting to that truth are also true and real, in some sense.

01:07:50.840 --> 01:07:59.000
And I'm like, no, that requires a lot of careful work at the end of the day. At the final kind of

01:07:59.000 --> 01:08:04.920
interpretive stage of a scientific procedure, you ask yourself, do we think strings are real?

01:08:04.920 --> 01:08:11.800
Do we think that quantum fields are real? Do we think that Hamiltonians are real? Or was this

01:08:11.800 --> 01:08:15.720
just sort of a calculational device? And I think there's a lot of, in between, between it's real

01:08:15.720 --> 01:08:21.640
and it's a calculational device. But I don't think at the end of the day that's a distinction that

01:08:21.640 --> 01:08:34.200
can be fully upheld. But there's a tendency, and I think it's true, even among careful philosophers

01:08:34.200 --> 01:08:48.200
to attribute realism or truth to, to try to make the conceptual tools of science truth out

01:08:48.200 --> 01:08:50.840
when they ought not to be read as truth out.

01:08:50.840 --> 01:08:54.840
Yeah, when it's a thing itself. I mean, it depends, right? For instance,

01:08:54.840 --> 01:08:59.800
I've spoken to a few mathematicians on this podcast, one person who I, as I came that comes to

01:09:00.280 --> 01:09:07.160
shoulder with Hamkins, he's a Platonist. So for him, of course, yes, FEP, if it's a mathematical

01:09:07.160 --> 01:09:13.000
structure, it could really exist in like a platonic sense. But in your paper, you're talking

01:09:13.640 --> 01:09:19.720
more in regards to our physical world, what science studies, you know, you're not talking

01:09:19.720 --> 01:09:24.200
about like a platonic realm. I don't know what your views are on Platonism.

01:09:24.200 --> 01:09:33.240
So with math, I think there's a lot of accounts of how math works in science that are, whether

01:09:33.240 --> 01:09:39.320
explicitly or not, deeply committed to Platonism, or something akin to it. And

01:09:42.360 --> 01:09:50.600
for me, I'm like, if your account of how natural science works, depends on assuming

01:09:51.400 --> 01:09:57.160
the most kind of industrial strength metaphysics possible. It's not a good account of natural

01:09:57.160 --> 01:10:03.080
science. Like I'm an, I believe in naturalism. I'm a naturalist. And a lot of naturalists,

01:10:03.720 --> 01:10:08.520
a lot of proclaimed naturalists happen to also be Platonists. And I'm like, that's not real

01:10:08.520 --> 01:10:14.760
naturalism, right? You need an account of science that assumes as little in the way of metaphysics

01:10:14.760 --> 01:10:21.160
as possible. And is cognizant of what metaphysics it does assume to the extent that you can't get

01:10:21.160 --> 01:10:29.240
away from that. Yeah, that's why you mentioned all the, all the presuppositions that your

01:10:29.240 --> 01:10:37.560
realism is based on, right? Right? Yeah. For me, it's like the word, the word dog or the word

01:10:37.720 --> 01:10:46.600
mitochondrion, right? Do we think the word dog exists? It's like,

01:10:49.560 --> 01:10:57.240
I think dogs exist. Yeah, yeah. I think that there's a class of natural entities that is well

01:10:57.240 --> 01:11:03.320
carved out by our best science. And we refer to it via this concept that has this label attached

01:11:03.320 --> 01:11:09.480
called dog. Yeah. But I don't think, I think the word exists as a cultural mental artifact.

01:11:09.480 --> 01:11:13.800
But I don't think it exists mine independently. And I feel the same about math and science. It's

01:11:13.800 --> 01:11:19.720
like. Yeah, sort of interrupt, but I think, I think I agree with you. I mean, I'm, I'm still,

01:11:19.720 --> 01:11:24.200
I hate to use this like cheap like agnosticism, but I'm kind of agnostic as to mathematical

01:11:24.200 --> 01:11:29.320
Platonism. Although, because I kind of view it in the way I think John Peer Jay said that,

01:11:29.880 --> 01:11:34.600
and if you think about it all of mathematics, it's just, it's semiotics and linguistics.

01:11:35.800 --> 01:11:41.320
The thing though is it kind of does, it has to blow our minds as to how consistently it's worked.

01:11:41.320 --> 01:11:48.760
I mean, at a pure pragmatic level, it is rather bizarre that there is this internal consistency

01:11:48.760 --> 01:11:54.440
in mathematics. So I'm still unreasonable efficacy. Exactly. Yeah. Yeah. I mean, you can't

01:11:55.000 --> 01:12:00.440
necessarily say that therefore that can be true in some fundamental metaphysical sense,

01:12:00.440 --> 01:12:09.880
but it does make one wonder. Yeah. So back to the idea, you know, in the, in the paper,

01:12:09.880 --> 01:12:16.280
I kind of want to also discuss in this paper, also you do. Oh, I think I'm, I made a mistake

01:12:16.280 --> 01:12:22.520
there. I'm confusing papers. Do you, in this, in the math, it's not the territory. Do you discuss

01:12:23.480 --> 01:12:31.240
the, the natural selection, like you kind of contrast or juxtapose the FEP with natural selection,

01:12:31.240 --> 01:12:34.760
or could it be the other paper you wrote, which was more like an introduction to

01:12:35.800 --> 01:12:39.720
the paper, the free-range principle and accessible introduction to its derivations,

01:12:39.720 --> 01:12:47.240
implications and applications? I don't know. Good question. Yeah. Yeah. Because regardless,

01:12:47.240 --> 01:12:53.240
what I, what I also liked was, and this probably connects to my previous, my initial question,

01:12:53.240 --> 01:13:00.200
why a lot of philosophical interest in the FEP, it's just, yeah. So like the FEP, is it like,

01:13:00.200 --> 01:13:05.800
is it like, is it like a meta narrative or like a meta theory, or it's called a super theory,

01:13:05.800 --> 01:13:14.600
not a meta theory, a super theory, something like natural selection. Yeah, or like almost an intuition

01:13:14.600 --> 01:13:19.160
pump? Yeah, I think I prefer that. Because you can't really, it doesn't really fall into the

01:13:19.160 --> 01:13:23.800
Hopperian falsifiability and, you know, in that rigorous sense, right? I should say maybe in it,

01:13:23.800 --> 01:13:31.480
that, that was sort of a, I'm sort of adapting the Dennett concept. That's, that's not exactly,

01:13:32.280 --> 01:13:38.040
that's not how he uses it. But, but one could say it's an intuition pump in a stretch of that

01:13:38.040 --> 01:13:48.680
terminology. I see. In that. It's, it's a kind of conceptual framework

01:13:50.760 --> 01:13:59.800
that enables us to think about things very differently in science. But it's not actually,

01:13:59.800 --> 01:14:08.920
it's not a method. It's not a formal model. It's not a theory or a hypothesis or a, you know,

01:14:08.920 --> 01:14:12.200
there are all these sorts of concepts, there are all these conceptual tools in science and it's,

01:14:13.080 --> 01:14:18.680
it's not any of the kind of lower order conceptual tools of science in the same way that natural

01:14:18.680 --> 01:14:24.680
selection is not, I don't view it as a theory. Natural selection isn't a theory or a hypothesis

01:14:24.680 --> 01:14:33.400
or a model or a, I mean, it's really sort of a meta conceptual framework, I would say.

01:14:33.400 --> 01:14:37.480
Yeah. Yeah. Yeah. And I think in that sense, it's very far removed from the actual

01:14:39.080 --> 01:14:44.920
day-to-day work of science. Of scientists. Yeah. Yeah. Experimental work, for instance. Yeah.

01:14:44.920 --> 01:14:52.200
And I think in, you know, part 1.5, you essentially say, I propose here that we reserve the free

01:14:52.200 --> 01:15:00.360
energy principle to denote only the maths of which the FEP is composed of models utilizing this

01:15:00.360 --> 01:15:06.760
formalism to study natural systems, bear also an interpretation, lending a means of interpreting

01:15:06.760 --> 01:15:13.800
the maths as about the systems in nature. So, I mean, that idea of, yeah, it's kind of a formalistic

01:15:13.800 --> 01:15:22.040
conceptual structure. I like thinking of it that way. Excellent. So, okay, let's see.

01:15:22.200 --> 01:15:26.040
In terms of time, yeah, we've got a few more. Is it okay if you go for about 10 more minutes?

01:15:26.040 --> 01:15:29.320
Yeah, yeah, totally. All right. All right. Fantastic. Thanks. Thank you, Mel.

01:15:29.320 --> 01:15:33.160
I kind of... I'm happy to chat again at some point if that's...

01:15:33.880 --> 01:15:39.240
Yes. Yes. I mean, in fact... I just have to run at like, you know, I have to run in 15 minutes, but...

01:15:39.240 --> 01:15:43.560
Yeah. Just 10 more minutes off your time, only because one thing I really try to do with this

01:15:43.560 --> 01:15:47.560
podcast is, I don't want to say a bridge, but I try to bring into dialogue

01:15:48.280 --> 01:15:54.440
a so-called continental philosophy and more of the analytic philosophy that kind of you work through.

01:15:54.440 --> 01:16:01.960
I should say, I studied with Dennett in my undergraduate and it was forbidden to me to touch

01:16:01.960 --> 01:16:08.360
continental philosophy. Yeah. You see, this is, you know, kiss and point. And I don't like that,

01:16:08.360 --> 01:16:11.960
to be honest. I don't even like creating this demarcation, really. It's just philosophy. It's

01:16:12.200 --> 01:16:15.160
thinking of a philosopher as a philosopher as far as I'm concerned.

01:16:15.160 --> 01:16:23.720
See, he knew my mind and he knew what reading Foucault would have done to me and it was nothing good.

01:16:24.520 --> 01:16:29.880
Yeah. There you go. You know, you start talking about the epistemes and all of that. Yeah. Yeah.

01:16:29.880 --> 01:16:36.840
But what's fascinating is like, look at how much of, you know, fruitful, productive output you've had,

01:16:37.640 --> 01:16:45.880
if I may, because you do take in the sociological aspects that Foucault, perhaps above all,

01:16:45.880 --> 01:16:53.640
point out very carefully when it pertains to our knowledge and epistemology. But yeah,

01:16:53.640 --> 01:17:00.600
on that note, let me just ask a very, very broad general question, Amel. As a philosopher of AI,

01:17:00.680 --> 01:17:07.240
as a philosopher of science, what insights do you think us in the, let's say, Anglo-American,

01:17:07.240 --> 01:17:13.080
you know, I'm in Australia. Are you Canadian or are you in the U.S.? U.S. Right. So yeah,

01:17:13.080 --> 01:17:20.680
the Anglo-American kind of. Yeah, there you go. The more analytic, you know, oriented philosophy,

01:17:21.560 --> 01:17:28.520
which, when I spoke to this philosopher called Simon Critchley, he said it's more for just a

01:17:28.600 --> 01:17:35.800
historical phenomena. It's got, it has nothing to do with really with ideas per se. It was just,

01:17:36.440 --> 01:17:43.800
you know, how things changed with the world, with World War II and how that, you know,

01:17:43.800 --> 01:17:48.920
certain analytic philosophers, it's in political, it was more of a political phenomena, more than a

01:17:48.920 --> 01:17:56.200
philosophical phenomena in history. But, you know, be that as it may, yeah, as a, as a philosopher of AI

01:17:56.200 --> 01:18:03.240
and a philosopher of ML, what insights do you think we can gain from kind of discourse within

01:18:04.120 --> 01:18:07.880
continental philosophy with thinkers all the way from thinkers like Heidegger

01:18:08.600 --> 01:18:14.200
or to people like Zizek and to, you know, sociologists like Foucault?

01:18:14.920 --> 01:18:25.000
Um, I don't delve into that work very much, but I know people who do and they,

01:18:26.920 --> 01:18:34.120
I think, make very interesting work of it. Elmer Feiden is doing, is more,

01:18:35.320 --> 01:18:40.840
he's a philosophy of AI, is more, is more, is more continentally influenced.

01:18:41.640 --> 01:18:46.120
I haven't heard of that person. Fantastic. Okay, thanks for that.

01:18:48.520 --> 01:18:52.040
Um, he will know other people.

01:18:54.600 --> 01:19:01.880
Yeah, yeah. There's also some, I think, I mean, I think there's a way in which sort of STS scholarship

01:19:01.880 --> 01:19:12.920
on technology, AI, ML is also more continentally leaning.

01:19:14.600 --> 01:19:20.120
Well, this is a book that I've just started on called The Critical Theory and AI,

01:19:21.000 --> 01:19:30.440
which I'm hoping to speak to, it's by Simon, Simon Lindegren. I just started on the books. I

01:19:30.440 --> 01:19:36.200
can't really say much on it. It was just published last year. Yeah, we just put out this, this,

01:19:38.520 --> 01:19:45.800
the pseudoscience paper. And a lot of the reviewers were suggesting some sort of like

01:19:45.800 --> 01:19:51.400
critical theory. Oh, you mean the one that goes in the machine learning paper? Yeah, our reviewers

01:19:51.400 --> 01:19:56.840
were suggesting some of that stuff. Fascinating. Yeah, yeah. I mean, yeah, which is like for me and,

01:19:56.840 --> 01:20:01.880
you know, leaving aside the names, like just generally for you, as you pointed out, you know,

01:20:01.880 --> 01:20:08.040
Dennett said, you know, no touching, Corneal Philosophers, but just, I'm curious, well,

01:20:08.040 --> 01:20:15.560
why did you rebel against, I'm joking, of course, but your master's commands and kind of get interested

01:20:15.560 --> 01:20:20.440
in, because I've seen through your Twitter feed, for instance, that you do, you do retweet stuff by

01:20:21.080 --> 01:20:28.280
some Corneal Philosopher, by Zizek, or you do, you aren't totally a verse to it or a post to it.

01:20:29.240 --> 01:20:38.920
Yeah, I mean, I think the divide is, sorry. I think, again, the divide is pretty superficial,

01:20:39.880 --> 01:20:44.680
as you said. And I think,

01:20:48.600 --> 01:20:53.480
as I said, like with everything, most of everything is, this is again,

01:20:59.160 --> 01:21:07.160
a Dennettism, but 99% of everything is crap was his life. And I think that's true of both

01:21:07.160 --> 01:21:12.840
analytic and continental philosophy. There's a dominant tradition. There are aspects of, like,

01:21:12.840 --> 01:21:19.640
if we think of analytic philosophy and continental philosophy as methods, I think there are really

01:21:21.800 --> 01:21:27.000
problematic features of both of those methodologies as methodologies for philosophy.

01:21:27.000 --> 01:21:32.520
I think there are really sort of stultifying features of both of those methodologies. And

01:21:33.080 --> 01:21:41.400
there's brilliant work. There's brilliant ideas coming out of both traditions. But in a way,

01:21:42.520 --> 01:21:48.280
it's working against some of the kind of primary tendencies of those traditions, in a sense.

01:21:51.480 --> 01:21:57.080
And so in my work, I'm just trying to say something

01:21:57.160 --> 01:22:05.800
interesting that matters for the reality we live in. And I'm trying as much as possible not to be

01:22:08.680 --> 01:22:17.240
operating within a particular conception of what good thought is. I mean, I think

01:22:18.120 --> 01:22:20.920
I think thinking according to some

01:22:23.080 --> 01:22:31.320
conception of what is the right way to think is not a right way to think. I think it

01:22:33.640 --> 01:22:38.040
keeps us short of discovering truths.

01:22:39.000 --> 01:22:43.160
So I try to not

01:22:47.240 --> 01:22:54.440
adhere as much as possible to principles of particular genres of philosophy,

01:22:54.440 --> 01:22:59.720
in so much as I can do that and still get published enough in philosophy venues to get a job.

01:23:00.840 --> 01:23:05.240
That's a whole other conversation. Just on a side note, Mel, are you familiar with

01:23:06.200 --> 01:23:14.200
the thinker, Bianchel Hahn? No. Yeah, so he wrote this book, Psychopolitics. Where is it?

01:23:15.320 --> 01:23:21.960
He's quite big in Germany. He's a German philosopher. In fact, no, he's originally from

01:23:21.960 --> 01:23:29.800
South Korea, but now he works in Germany. And I was, when I read your paper, in fact, when I read

01:23:30.760 --> 01:23:40.440
the theory free ideal paper, and then also the recent one, the pseudoscience one,

01:23:40.440 --> 01:23:47.480
he has one chapter in this book, Psychopolitics, on big data. And he wrote this in like the early

01:23:47.480 --> 01:23:55.080
2000s. A lot of this, I'd love to, in fact, as you said, you'll be open to another conversation,

01:23:55.080 --> 01:24:01.640
I'd love to kind of discuss. It's just like 15 pages, that chapter, that just his ideas on it,

01:24:01.640 --> 01:24:06.760
because I think you'd be the perfect person to comment on it with your kind of analytic background

01:24:08.680 --> 01:24:14.600
with the work you've done, because it really did remind me of his ideas when I read that,

01:24:14.600 --> 01:24:21.480
when I read your work, in any case. But yeah, again, I want to be cognizant of your time. So

01:24:21.480 --> 01:24:27.800
just one last question, Mel. What would you say, currently, I just like asking this,

01:24:27.800 --> 01:24:34.440
because it kind of excites me as to what kind of work I should do. In this space of philosophy of

01:24:34.440 --> 01:24:42.440
ML and philosophy of AI, apart from what we've discussed so far, what other avenues for research

01:24:42.440 --> 01:24:47.720
do you think people should look into and take into consideration and start thinking about?

01:24:48.520 --> 01:24:55.240
Oh, God, there are a million. I think there are a million avenues that are

01:24:56.360 --> 01:25:03.240
pursued or could be pursued that I think are kind of fruitless. But there are a million avenues that

01:25:03.240 --> 01:25:14.200
are really fascinating. So I mean, I think the way machine learning technologies are being adopted

01:25:14.200 --> 01:25:25.960
and changing labor and changing the way these technologies are changing our relationship to

01:25:29.000 --> 01:25:35.800
the sort of resource consumption to the means of production, to the output of labor, to the

01:25:35.800 --> 01:25:42.840
output of intellectual labor, or epistemic labor, one might say, which are, I think,

01:25:42.840 --> 01:25:56.920
much more subtle than we've given credit to so far. There's a problem of opacity or

01:25:56.920 --> 01:26:02.200
explainability or what have you. I think it's clear that the sort of research avenues that

01:26:02.200 --> 01:26:12.040
we've been on with that are deeply flawed, and the kind of technologies we've invented to tackle

01:26:12.040 --> 01:26:19.880
those problems in so much as they are problems are deeply flawed. But figuring that out is usually

01:26:19.880 --> 01:26:33.640
important. How these technologies are bringing about harms in the real world is a tremendous issue.

01:26:33.640 --> 01:26:43.240
And tackling that in a way that is deeply cognizant of incentive structures.

01:26:48.680 --> 01:26:58.440
And investing in interventions that those in the positions of power to create and deploy these

01:26:58.520 --> 01:27:06.440
technologies would actually be incentivized to adopt and working on that incentivization.

01:27:08.600 --> 01:27:13.880
So this requires a vastly interdisciplinary perspective. This requires having whatever

01:27:13.880 --> 01:27:22.520
the domain of application is, if that's a hospital, if that is in biological science,

01:27:22.520 --> 01:27:29.880
if that is in building cell phone applications, if that is it, whatever the application is,

01:27:29.880 --> 01:27:36.440
having deep domain knowledge of that application, having knowledge of the regulatory structures

01:27:36.440 --> 01:27:43.240
or lack thereof, having knowledge of the incentives of those who are creating these

01:27:43.240 --> 01:27:48.120
technologies and deploying them, and having knowledge of the knowledge level of those

01:27:48.120 --> 01:27:51.000
who are creating these technologies and deploying them, et cetera, et cetera, et cetera.

01:27:51.720 --> 01:27:59.160
And so I think there's, it's like now is the time when we need to stop messing around and start

01:27:59.160 --> 01:28:04.840
doing really, really good, really dialed in interdisciplinary work and supporting that,

01:28:04.840 --> 01:28:08.520
because you cannot effectively

01:28:12.360 --> 01:28:20.040
prevent or remediate the ill usage of these technologies from a single disciplinary background.

01:28:20.040 --> 01:28:25.640
You need teams of people with expertise in a lot of different domains who know how to talk to each

01:28:25.640 --> 01:28:31.560
other. Yeah, oh, that's for sure. Yeah. Yeah. So I think, sorry, I lied. That was the penultimate

01:28:31.560 --> 01:28:36.760
question. This is the last question. And then I'll let you go, I promise, because you spoke about

01:28:36.760 --> 01:28:42.200
kind of social harms. One other question I wanted to ask from you was, we spoke about

01:28:42.840 --> 01:28:51.480
science fiction and kind of the dooms kind of mentality. But would you compare, would you

01:28:51.480 --> 01:28:57.880
like, would you say the current paradigm of AI, the current technologies we have, that it can be

01:28:57.880 --> 01:29:03.560
compared to something like the nuclear bomb in the nukes in the 20th century, or do you think

01:29:03.560 --> 01:29:09.000
that's an exaggeration or I'm just being dramatic? Like we aren't there yet. It's not that grave.

01:29:11.080 --> 01:29:20.120
Well, the impact of nuclear bombs, the impact of nuclear technology

01:29:20.360 --> 01:29:27.720
was in one part the reality of this groundbreaking technology that made

01:29:30.680 --> 01:29:37.800
killing and destruction possible at only levels. On another part, it was a psychological phenomenon.

01:29:38.760 --> 01:29:39.240
It was

01:29:42.120 --> 01:29:53.000
almost more radically a psychological shift, right? I think the thing with machine learning

01:29:53.000 --> 01:29:59.880
technologies and any possible future development of these technologies, in fact, not just what we

01:29:59.960 --> 01:30:08.920
have right now in front of us, is almost that level of psychological reaction to what these

01:30:08.920 --> 01:30:21.640
technologies mean. But the disruptive force is not owing to a really radical scientific or technological

01:30:21.640 --> 01:30:28.440
breakthrough. It's more the psychological and social time.

01:30:33.800 --> 01:30:40.200
I mean, it has the potential to really radically disrupt labor markets, and it already is. But

01:30:40.200 --> 01:30:46.520
that was something that was already happening. Workers were already being pushed out. I mean,

01:30:46.600 --> 01:30:50.440
that was happening since the Industrial Revolution. Yeah, and that's capitalism. Yeah,

01:30:50.440 --> 01:30:56.520
that's just capitalism. Yeah. It's just that now there's an even more, it's perpetually a more

01:30:56.520 --> 01:31:04.280
and more urgent need to shift away from a mode of production in which if you don't work, you die.

01:31:07.160 --> 01:31:13.560
The world in which if you do not labor, you starve to death, is not one that works in which

01:31:13.560 --> 01:31:20.040
most people are being automated out of their jobs. Yeah, regardless of the technology. Yeah.

01:31:23.480 --> 01:31:28.120
On that grim note, thank you very much for your time, Mel. I've thoroughly enjoyed,

01:31:28.840 --> 01:31:33.880
I've learned a lot from you, but also just your Twitter feed is fantastic. I visited,

01:31:33.880 --> 01:31:39.560
I frequent it often. I hate Twitter, I think it's a terrible place, but I go to like yours,

01:31:39.560 --> 01:31:44.280
and I've got like five people bookmarked. I just visited their feeds. But yeah, thank you.

01:31:44.280 --> 01:31:47.880
Thank you very much for your time, Mel. And I hope we can chat again soon.

01:31:47.880 --> 01:31:51.160
Yeah, thanks, Lucas. Yeah, I'm happy to follow up. And send me the thing, the

01:31:53.800 --> 01:32:00.440
Korean-German author. Yes, the one by Byung-Shul Han. Yeah, I'll send you, I've got a PDF,

01:32:00.440 --> 01:32:05.000
I'll send you the PDF and all that, because I'd love to kind of, you know, follow up on that and

01:32:05.000 --> 01:32:10.280
see what ideas you have. But yeah, thank you, Mel. Yeah, thanks.

