1
00:00:00,000 --> 00:00:05,680
I want to discuss, uh, explanatory coherence. Uh, you've done a lot of work on that. And I'm

2
00:00:05,680 --> 00:00:11,280
wondering if you could kind of give a brief introduction to what, uh, explanatory coherence

3
00:00:11,280 --> 00:00:18,160
is to a lay audience. What at the same time, uh, why you think, uh, it's, uh, what, what are the,

4
00:00:18,160 --> 00:00:21,200
what are the implications in fields such as AI and psychology?

5
00:00:22,640 --> 00:00:26,720
Oh, that's a big question, but it's a really interesting one. So let's go back to Darwin.

6
00:00:26,720 --> 00:00:30,320
So I mentioned one of my favorite books of all time is the origin of species.

7
00:00:30,320 --> 00:00:34,560
And so what was he doing in that? Uh, well, in my view, what he was trying to do is to

8
00:00:34,560 --> 00:00:38,320
give a coherent explanation of all sorts of things that he observed. So he went on this

9
00:00:38,320 --> 00:00:43,440
voyage around the world and he collected all sorts of other kinds of biological information.

10
00:00:43,440 --> 00:00:48,720
And he gradually seemed to him that it seemed in fact that the species had evolved. I mean,

11
00:00:48,720 --> 00:00:54,720
now everybody knows that kids get that probably in grade four, but it was a, um, a very controversial

12
00:00:54,720 --> 00:01:00,160
idea. Some people had maintained it, but it went up against religious doctrines. And so he

13
00:01:00,160 --> 00:01:07,600
gradually started to amass more and more evidence that species had evolved. But then from reading a

14
00:01:07,600 --> 00:01:12,080
crazy economist named Malthus, he suddenly got the idea of how they evolved. And that's how he

15
00:01:12,080 --> 00:01:16,800
came up with the idea of, of, of the natural selection. So now we had not only a bunch of

16
00:01:16,800 --> 00:01:21,600
observations and the idea that evolution probably had occurred that would explain it,

17
00:01:21,600 --> 00:01:27,520
but an idea of how, how evolution had occurred. That is that natural selection was the mechanism

18
00:01:27,520 --> 00:01:34,960
behind evolution. So what he did in that book was an incredibly beautiful argument for his view,

19
00:01:34,960 --> 00:01:40,160
as opposed to the view that was dominant at the time, which was divine creation. So what he was

20
00:01:40,160 --> 00:01:46,880
trying to show is that his view was a better explanation, but then divine creation, because

21
00:01:46,880 --> 00:01:53,520
it was more coherent with the evidence. So this is an account that philosophers call inference to

22
00:01:53,520 --> 00:01:59,200
the best explanation. You can argue that something's the best explanation because it's more coherent

23
00:01:59,200 --> 00:02:04,000
with the evidence, but now you have to say what coherence is. So I had these early ideas coming

24
00:02:04,000 --> 00:02:12,080
out of my philosophy of science background, but then in 1987, I got one of the best ideas I've

25
00:02:12,080 --> 00:02:17,520
ever had, which was how to turn coherence from a sort of vague philosophical idea into a precise

26
00:02:17,520 --> 00:02:24,000
computational one. That is, how can you compute coherence? So I've been working on neural networks

27
00:02:24,000 --> 00:02:31,040
in collaboration with my colleague Keith Holyoke. And he came up with an idea that you could use

28
00:02:31,040 --> 00:02:36,800
neural networks to explain analogy. These neural networks, of course, are now absolutely central

29
00:02:36,800 --> 00:02:41,360
to artificial intelligence. It's, it's really taken off. That's a whole fascinating topic in

30
00:02:41,360 --> 00:02:46,480
itself. But he figured out a way of doing that. And by that time, I'd done my master's in computer

31
00:02:46,480 --> 00:02:51,440
science, so I was a pretty good programmer. And so I programmed up a program to use neural networks

32
00:02:51,440 --> 00:02:57,360
to do analogies. So that, that was nice. And then I thought, what else could apply to, and then I

33
00:02:57,360 --> 00:03:01,280
thought back to the problem that was part of my doctoral dissertation, which was inference to the

34
00:03:01,280 --> 00:03:06,800
best explanation. How do you pick up the best theory? And so then I realized that that kind of coherence

35
00:03:06,800 --> 00:03:12,320
can be understood using the same kind of neural network technique that Keith and I had done for,

36
00:03:12,320 --> 00:03:18,720
for analogy. So it was a really powerful method, both computationally, but also psychologically,

37
00:03:18,720 --> 00:03:22,480
because there have now since then been lots of psychological experiments that back up this

38
00:03:22,480 --> 00:03:27,440
idea of coherence. So I think of the mind, the brain as essentially a coherence engine.

39
00:03:27,440 --> 00:03:31,280
There's some people who think that it's primarily a predictive engine, but I don't think that's

40
00:03:31,280 --> 00:03:35,760
true. I think it's primarily a coherence engine. We're trying to make sense of things, whether

41
00:03:35,760 --> 00:03:40,880
we're making sense of the past, which is what explanations do, or making sense of the future,

42
00:03:40,880 --> 00:03:47,520
we're making coherent predictions, or we're trying to identify things. Is that a rabbit or a squirrel?

43
00:03:47,520 --> 00:03:52,080
Those are, are different kinds of thought. Everything we do can be understood as having

44
00:03:52,080 --> 00:03:56,960
coherence behind it. But coherence now isn't just a sort of vague metaphor that it was for

45
00:03:56,960 --> 00:04:01,920
philosophers. It's not just a matter of consistency. It's rather of taking a whole bunch of different

46
00:04:01,920 --> 00:04:08,000
things and putting them into a good package. But what's a good package? Well, here there's an idea

47
00:04:08,000 --> 00:04:14,320
that came out of the neural network world called constraint satisfaction. So we're trying to satisfy

48
00:04:14,320 --> 00:04:19,200
a bunch of constraints. What constraints did Darwin face? Well, he was trying to explain as much as

49
00:04:19,200 --> 00:04:25,200
possible about what he'd seen in the biological world. That's the positive constraints, but he

50
00:04:25,200 --> 00:04:29,440
also had a negative constraint. And so he had to show that he could do that better than the theory

51
00:04:29,440 --> 00:04:33,920
that was the computer at the top competitor at the time, which is divine creation. So that's a

52
00:04:33,920 --> 00:04:38,800
negative constraint. So what you're doing in all of these things, whether it's decision making or

53
00:04:38,800 --> 00:04:44,400
pattern recognition, or even emotion, you're putting together different sorts of constraints to

54
00:04:44,400 --> 00:04:50,240
evaluate what's the most coherent view. So that's how I came to see coherence, not just as a vague

55
00:04:50,240 --> 00:04:56,080
philosophical idea, but as a quite precise computational one that can be used to explain

56
00:04:56,640 --> 00:05:02,240
the mechanisms that underlie a vast amount of human thinking. So that's why I think coherence

57
00:05:02,240 --> 00:05:10,000
is really a fundamental idea to psychology and cognitive science and to these philosophical

58
00:05:10,000 --> 00:05:15,440
projects as well. Yeah, excellent, excellent. I'm glad he brought up predictive coding or

59
00:05:15,440 --> 00:05:21,040
predictive processing, because I'll be talking to a cognitive scientist next month, in fact,

60
00:05:21,040 --> 00:05:27,760
based here in Melbourne. And I want to ask her about your theory of explanatory coherence, because

61
00:05:27,760 --> 00:05:34,000
I believe you do have certain critiques of predictive processing, but also in your book,

62
00:05:34,000 --> 00:05:40,160
you are critical of, in fact, no, I think you wrote an article on this, a paper on this, pardon me,

63
00:05:40,160 --> 00:05:46,400
you also critical of functionalism, kind of what Hilary Putnam and the likes put forward. So from

64
00:05:46,400 --> 00:05:53,600
your kind of theory of mind, what would you say are your critiques of one predictive processing,

65
00:05:53,600 --> 00:05:58,560
and then functionalism? Those are two different views, I don't think they have anything to do with

66
00:05:58,560 --> 00:06:03,520
each other. I'm just curious, I quote them independently, what would be your critiques?

67
00:06:03,520 --> 00:06:08,000
Okay, let's do one at a time. Predictive processing definitely is an influential view right now,

68
00:06:08,000 --> 00:06:13,520
but I think it's just not right. It says that the mind is the brain is a predictive engine,

69
00:06:14,240 --> 00:06:18,880
as if everything is prediction. But the mind doesn't just do prediction, it does at least

70
00:06:18,880 --> 00:06:24,400
five other things that are just as important. It does explanation, which involves explaining the

71
00:06:24,400 --> 00:06:28,800
past, that's not prediction, that's the past prediction is about the future, or even pattern

72
00:06:28,800 --> 00:06:34,880
recognition. I mentioned, I see an animal in my backyard, what is it? Is that a squirrel or a rabbit?

73
00:06:34,880 --> 00:06:38,400
Well, that's pattern recognition, that's not necessarily a prediction, I want to know what it

74
00:06:38,400 --> 00:06:45,760
is. We also want to do, and this is really important, evaluation. Is this good or bad for me?

75
00:06:45,760 --> 00:06:50,240
Is this a threat to me? Or is this something I can eat? How do we do evaluation? Well, in humans,

76
00:06:50,240 --> 00:06:54,720
that comes from emotion. The predictive processing approach has said nothing interesting to say about

77
00:06:54,720 --> 00:07:00,560
emotion at all, but that's absolutely fundamental to many areas of human thinking. I've got a whole

78
00:07:00,560 --> 00:07:08,320
theory of emotion, of which coherence is part of it, but it's only part of it. So you have to

79
00:07:08,320 --> 00:07:14,560
have evaluation going on. Communication, we sometimes predict in order to communicate with

80
00:07:14,560 --> 00:07:19,840
other people, but there's lots of other things going on where we want to be able to get our ideas

81
00:07:19,840 --> 00:07:25,280
across to others. So that's just at least five things that are part of the human mind other

82
00:07:25,280 --> 00:07:31,440
than predictive processing. So that's my first critique. My second critique is the way that

83
00:07:31,440 --> 00:07:35,440
people in that world think that predictive processing work doesn't correspond to how the

84
00:07:35,440 --> 00:07:40,880
brain works very well. They're Bayesians. They say that the brain uses probability theory in

85
00:07:40,880 --> 00:07:47,760
accord with Bayes' theorem to predict the next thing. Well, this is crazy computationally.

86
00:07:47,760 --> 00:07:53,680
Bayesian processing is well known in artificial intelligence to be extremely inefficient. You

87
00:07:53,680 --> 00:07:59,280
can prove that it's computationally intractable. You can show that it causes all sorts of problems.

88
00:07:59,280 --> 00:08:03,760
Bayesians have to jump through all sorts of hoops to try to deal with anything larger than that.

89
00:08:03,840 --> 00:08:07,920
I've done a little bit of Bayesian modeling because I wanted to, I drew a couple of papers

90
00:08:07,920 --> 00:08:12,880
where I compared a Bayesian model of legal reasoning to my explanatory coherence one.

91
00:08:12,880 --> 00:08:18,160
But the Bayesian models are crazy because you have to generate all sorts of conditional probabilities

92
00:08:18,160 --> 00:08:22,240
that nobody has an idea what they are. So if you actually do Bayesian modeling seriously,

93
00:08:22,240 --> 00:08:25,920
you'll find first of all, you don't know any of the probabilities. You don't know any of the

94
00:08:25,920 --> 00:08:30,400
conditional probabilities. You don't have the computational or neural resources to actually

95
00:08:30,400 --> 00:08:35,840
commute the probabilities. So the way that predictive processing with its too narrow view

96
00:08:35,840 --> 00:08:40,240
of how the brain works fills it out is by making brains Bayesian when they're not.

97
00:08:40,240 --> 00:08:46,640
A really good contrast here is with the new generative AI models, which are actually incredibly

98
00:08:46,640 --> 00:08:53,680
good at predicting. Have you used chat GPT or any of the others? They're astonishing. They're

99
00:08:53,680 --> 00:08:58,480
astonishing at how good they are at predicting the next word to say. And they end up producing

100
00:08:58,480 --> 00:09:03,120
really coherent stuff. And they get things really bad, badly wrong sometimes, but often

101
00:09:03,120 --> 00:09:08,080
they're really good, but they don't use Bayesian predictions. It's they've got all different kinds

102
00:09:08,080 --> 00:09:13,920
of algorithms that they use tension mechanism and sorts of things. So they realize that that the

103
00:09:13,920 --> 00:09:19,120
Bayesian approach is not going to work for them. So those are my two major criticisms of the

104
00:09:19,120 --> 00:09:24,160
predictive processing. The brain is a multi fast, it's a coherence engine doing six things

105
00:09:24,560 --> 00:09:30,320
as well as prediction. And it's not doing it using Bayesian probability calculations.

106
00:09:31,360 --> 00:09:33,280
Okay, so is that good enough for predictive processing?

107
00:09:34,880 --> 00:09:38,320
No, I think that's perfect. I want to ask you about the free energy principle, but probably we'll

108
00:09:38,320 --> 00:09:44,720
get to the functionalism and then maybe come back to the principle. Yeah, okay. So functionalism

109
00:09:44,720 --> 00:09:49,440
is a view in the philosophy of mind. It's actually a really bad term because functionalism is a term

110
00:09:49,440 --> 00:09:53,280
that operates in about six different fields or six different meetings. So we need to pin it down a

111
00:09:53,280 --> 00:09:59,120
bit. Let's call it computational functionalism. Because it came in the 60s when computers started

112
00:09:59,120 --> 00:10:04,080
to become aware and Hillary Putnam knew about the advances in computing. When computers were

113
00:10:04,080 --> 00:10:08,560
really primitive, then I've got a watch now that was better than all the computers, way better than

114
00:10:08,560 --> 00:10:15,600
anything that came along for decades. But but still, people were starting to think that with

115
00:10:15,600 --> 00:10:21,680
computers and the possibility of artificial intelligence, we've got this abstract way of

116
00:10:21,680 --> 00:10:28,160
thinking of thinking as a kind of computation. Now, one thing that's really true or seems to be

117
00:10:28,160 --> 00:10:33,840
true about computation is that it doesn't really matter what you run it on. So here I'm using a

118
00:10:33,840 --> 00:10:39,680
Macintosh. I don't know what kind of computer you've got. It could be a PC or running a different

119
00:10:39,680 --> 00:10:44,080
kind of hardware altogether. It doesn't matter. We can all run the same software. And so the

120
00:10:44,080 --> 00:10:50,880
analogy that Putnam hit on was mind is software rather than hardware. You can take the same software

121
00:10:50,880 --> 00:10:54,320
and run it on a bunch of people or hardware. All that matters is it has the appropriate

122
00:10:54,320 --> 00:10:58,960
computational functions. That's where the word functionalism comes from. So if you have inputs

123
00:10:58,960 --> 00:11:05,760
and outputs, and you have the functions in between, you want to be able to make thinking work. And so

124
00:11:05,760 --> 00:11:12,240
forget about the hardware. Forget about the brain, for example. The psychologist had been studying

125
00:11:12,240 --> 00:11:18,560
the brain at that point for, I guess, 60 or 70 years seriously. The functionalist in the 60s said,

126
00:11:18,560 --> 00:11:23,680
let's forget about the brain. It's all computation. It's just like AI. Anything that runs in the mind

127
00:11:23,680 --> 00:11:29,200
can run on a computer. Okay. And actually in the 1960s and 70s, that was a pretty reasonable idea.

128
00:11:29,200 --> 00:11:33,680
And this is why a lot of philosophers consider themselves functionalists. It became the dominant

129
00:11:33,680 --> 00:11:39,360
view in the philosophy of mind. So I think that was a pretty good idea in the 60s and 70s because

130
00:11:39,360 --> 00:11:46,000
AI had become a real field. Computers had become at least rudimentarily powerful. And so not a

131
00:11:46,000 --> 00:11:52,480
bad idea then. But things changed in the 80s. In the 80s, a bunch of things changed. First of all,

132
00:11:53,440 --> 00:11:57,760
brain scanning came along. It had been really hard to study the brain before because you had to do

133
00:11:57,760 --> 00:12:02,000
things like poke electrodes into brains that had been exposed. And so it was really hard to study

134
00:12:02,000 --> 00:12:07,440
the brain. But in the 80s, brain scans came along. First of all, I forget what they were called,

135
00:12:07,440 --> 00:12:11,920
and then eventually fMRI. But suddenly you could actually study the brain in a much more detailed

136
00:12:11,920 --> 00:12:16,880
way. And then you could start to test some of the claims that had been made. So when people started

137
00:12:16,880 --> 00:12:22,560
doing fMRI studies, they thought, oh, we're going to be able to show that the mind really is module,

138
00:12:22,560 --> 00:12:27,360
that is modular, that is different parts of the brain are doing very specific things. And so we

139
00:12:27,360 --> 00:12:31,360
should be able to find that this part of the brain does high level thinking, this part of the brain

140
00:12:31,360 --> 00:12:36,560
does emotion, and that part of the brain does vision, and we could localize it. But once people had

141
00:12:36,560 --> 00:12:41,360
this new tool, they started to realize, hey, it's not like that at all. Lots of what goes on the

142
00:12:41,360 --> 00:12:47,120
brain involves interactions of lots of different areas. So suddenly the brain became much more

143
00:12:47,120 --> 00:12:51,600
interesting. It didn't look like just some other kind of hardware you might run thoughts on. It

144
00:12:51,600 --> 00:12:57,120
looked like you could study on its own. So there were these empirical findings coming out of the

145
00:12:57,120 --> 00:13:01,360
new tools available for studying the brain that suggested that, well, maybe the structure of the

146
00:13:01,360 --> 00:13:08,560
brain really does matter. So that was an incredibly important empirical basis for starting to question

147
00:13:08,560 --> 00:13:13,920
functionalism. But there was also a really interesting theoretical basis coming out of

148
00:13:13,920 --> 00:13:18,720
the ideas about neural networks. So the ideas of neural networks have been around really back

149
00:13:18,720 --> 00:13:24,160
since the 50s, but it didn't work very well. And people like Marvin Minsky had argued that, no,

150
00:13:24,160 --> 00:13:28,000
those these ideas about neural networks are not going to work very well. They're just,

151
00:13:28,000 --> 00:13:33,280
they're just, they're just simply not theoretically strong enough. But in the 1980s,

152
00:13:34,000 --> 00:13:40,000
people greatly expanded the possibilities of what neural networks could do. They invented a new

153
00:13:40,000 --> 00:13:45,040
algorithm called back propagation that does learning. A whole movement got started called

154
00:13:45,040 --> 00:13:50,320
connectionism, which said that knowledge isn't a matter of the words you've got or the symbols,

155
00:13:50,320 --> 00:13:54,560
which is what artificial intelligence, but it's rather it's the connections, it's the neural

156
00:13:54,560 --> 00:13:59,440
connections. So suddenly, people were modeling their computer models, because these are being done

157
00:13:59,440 --> 00:14:04,320
with computerized neural networks, they were modeling on ideas about the brain, how you can

158
00:14:04,320 --> 00:14:09,680
have different neurons working in parallel with simple connections with them, and nevertheless

159
00:14:09,680 --> 00:14:17,040
doing that. So in the 1980s, suddenly, I functionalism was in trouble. Not many people noticed because

160
00:14:17,040 --> 00:14:21,680
they weren't tracking what was happening in neuroscience and in neural network theory,

161
00:14:21,680 --> 00:14:27,440
but it wasn't. And by the 1990s, I think it really had completely turned around. I think by the 1990s,

162
00:14:27,440 --> 00:14:31,840
functionalism was no longer plausible. You needed to take the brain seriously if you wanted to

163
00:14:31,840 --> 00:14:37,280
understand. And the whole field of cognitive psychology changed. It went from being completely

164
00:14:37,280 --> 00:14:43,040
abstract and computational to doing almost everything it did in relation to what happened

165
00:14:43,040 --> 00:14:46,560
in the brain. So cognitive psychology is now completely connected with neuroscience in the

166
00:14:46,560 --> 00:14:51,680
field of cognitive neuroscience. Other areas of psychology, developmental social also became

167
00:14:51,680 --> 00:14:56,880
intensely tied in with the brain. So the idea that the hardware doesn't matter, which was what

168
00:14:56,880 --> 00:15:02,160
was behind Putnam's functionalism, just by the 90s, didn't seem plausible at all. So that's why I

169
00:15:02,160 --> 00:15:06,080
think functionalism is a defunct view in the philosophy of mind, even though there are people

170
00:15:06,080 --> 00:15:10,560
who seem to assume that it's right. Sometimes it goes under other names. There's another name that

171
00:15:10,560 --> 00:15:18,960
people use, it's called substrate independence. The idea is the substrate is the physical,

172
00:15:19,840 --> 00:15:24,160
and so that doesn't matter. And there are people who use that because it suits some of their views,

173
00:15:24,240 --> 00:15:29,280
such as the idea that we're all living in a simulation, which I think is a really dumb view.

174
00:15:29,280 --> 00:15:34,320
But in order to believe that, you have to believe that substrate independence is true,

175
00:15:34,320 --> 00:15:37,360
which is another word for functionalism, which says the hardware doesn't matter,

176
00:15:37,920 --> 00:15:43,120
because the idea of where a simulation is some computer in the future is basically

177
00:15:43,120 --> 00:15:47,920
simulating our thoughts now. Well, that assumes that a computer can simulate all our thoughts,

178
00:15:47,920 --> 00:15:53,840
which assumes functionalism or some straight independence, independence, which I think is

179
00:15:53,840 --> 00:15:59,120
wrong. And I've actually just published a paper in Philosophy of Science two years ago that gives

180
00:15:59,120 --> 00:16:03,280
a whole bunch of arguments based on energy about why it's wrong. But there are other reasons as

181
00:16:03,280 --> 00:16:10,480
well for thinking that functionalism or substrate independence is wrong. Okay, that's enough.

182
00:16:11,440 --> 00:16:15,200
That's really fun. But go ahead. I didn't mean to interrupt you there, but please.

183
00:16:15,200 --> 00:16:19,680
I just wanted to summarize. So it was a great idea in the Philosophy of Mind

184
00:16:19,680 --> 00:16:24,080
that no longer should be taken very seriously, given what we know about brains and energy.

185
00:16:25,520 --> 00:16:32,720
So even look at the way AI is going right now. So the generative AI models, the large language

186
00:16:32,720 --> 00:16:37,760
models are incredible, but they're really energy pigs. It takes vast amounts of energy to train

187
00:16:37,760 --> 00:16:44,080
these things and answer questions. Our brains are astonishing. Our brains work on basically 40

188
00:16:44,160 --> 00:16:49,520
watts, like a small light bulb, very small amounts of energy, very efficient, and yet we're still

189
00:16:49,520 --> 00:16:56,000
smarter than any computer with all these resources. So there's a full field called neuromorphic AI,

190
00:16:56,000 --> 00:17:00,400
which is trying to make computers more like the brain to get these advantages of energy and

191
00:17:00,400 --> 00:17:07,520
efficiency and working in real time. So I think these are really interesting research areas that

192
00:17:07,520 --> 00:17:13,680
show that functionalism just wasn't it is no longer a plausible view in the Philosophy of Mind.

193
00:17:14,560 --> 00:17:18,560
Now that's an astute point, Professor, because I was, well, two points on there. Firstly, I always

194
00:17:18,560 --> 00:17:24,240
found functionalists to be good old Cartesian's where they had the mind, the mind matter. They

195
00:17:24,240 --> 00:17:28,880
think mind is independent to matter, which for me never made any sense, given we have physical

196
00:17:28,880 --> 00:17:35,920
embodied beings. And secondly, you are 100% right that I was listened to a talk by Scott

197
00:17:35,920 --> 00:17:41,520
Aronson, the American computer scientist, and he is now a researcher at Open AI. And Open AI is

198
00:17:41,520 --> 00:17:48,480
heavily investing in quantum computing and even in nuclear energy, because they've understood

199
00:17:48,480 --> 00:17:55,280
that if they are to grow their LLMs, they need infinite amounts of energy, because the compute

200
00:17:55,280 --> 00:18:02,880
power for LLMs are so, they're so high compared to like our puny little brains, which is a fascinating

201
00:18:02,880 --> 00:18:04,880
conversation.

