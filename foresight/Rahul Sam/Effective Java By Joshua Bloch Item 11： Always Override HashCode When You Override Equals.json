{"text": " Hey everyone, welcome to yet another episode of my video series on Joshua Bloch's effective Java and today we're going to cover item number 11, always override hash code when you override equals. But of course, before I get to the item as I usually do in the series, I want to go through this proviso and make a few points. Starting off, this is not a tutorial. I'm not a teacher. Nor am I certainly an expert in Java or object oriented programming in general. So please don't take what I say as gospel truth, always double check, do your own research. This is simply me sharing my learning experience. That's purely it. So don't treat this video series like a tutorial. I may spew or give out false information, unfortunately, though I will try not to. Think of it more as the blind, leading the blind, that's an analogy that I've been using for a while now. I am as blind or perhaps even more blind than you are. So this is merely a exploratory disquisition. I'm just figuring this thing out and hopefully we can learn together. In fact, I've already been corrected in a few of my other videos and I've been trying to rectify those errors. Therefore, again, not to repeat the same point, I will inevitably make mistakes. So please do point them out more for the benefit of the videos and myself. But yes, of course, for the benefit of myself too, selfish speaking. And given this is a, in fact, programming video series, all the code that I use that I've been using for all the previous items that you can see here, it's all available on my GitHub repo. I'll leave a link to the code for today's item, item number 11 to my repo, but also to Joshua Block's sort of official repo that he has for this book. Saying that without further ado, let's get started. So yeah, as I said, item number 11, which is always override hash code when you override equals and the item starts off like this. Joshua Block states, you must override hash code in every class that overrides equals. If you fail to do so, your class will violate the general contract for hash code, which will prevent it from functioning properly in collections such as hash map and hash set. But before we continue on with the item, let's get some key phrases or key terms out of the way, get some definitions out of the way. So what is hash code? And who better to ask than chat GPT? And oh, I forgot to mention, this item is a part of chapter three methods common to all objects. So back to it. What is hash code? Chat GPT states, and I think it's accurate here because I did double check with a few other resources or definitions online. In computing, a hash code also called a hash value, check sum or simply a hash is a fixed array, sorry, is a fixed size string of characters that is generated by a one way mathematical function called a hash function. From an input of any size, often call the message. The resulting hash code is typically a hexadecimal number that is a unique representation of the input. The main purpose of a hash code is to take an input and produce a fixed size string of characters that can be used to identify or verify the input. Hash codes are commonly used in data structures such as hash tables, OS, just look at stated hash maps and hash sets. And in various algorithms, such as message, sorry, such as message authentication codes, digital signatures and check sums. Oh, this is an important point regarding hash functions. Hash functions are designed to be one way and deterministic, meaning that the same input will always produce, let me zoom in a bit actually, meaning that the same input will always produce the same output, but it is computationally infeasible to determine the original input from the output. And then more in the context of Java, I got this definition from educative.io, when the hash code, which is the hash code method, is called on two separate objects which are equal according to the equals method. So by the way, we discussed the equals method in the previous items. If you want to understand, get a, in fact, a really deep understanding of the equals method, I suggest or recommend you watch all four parts for item number 10 because it was such a big item. I had to break it down into four different videos, but in that, I dealt deeply into the equals method. In that case, getting back to this, which is called on two separate objects, it returns the same hash code value. However, if it is called on two unequal objects, it will not necessarily return different integer values. So that's the definition in the context of Java. So to get started, here's a really simple, straightforward class as to how the hash code is used, example, courtesy of educative.io, of course. So there's a class called hash, declared two strings, and keep in mind that these strings are immutable. And if equals b, this function will indicate that object a is equal to object b. And it's important to keep in mind, if there is logical equality between two objects, if the hash code wasn't overridden, then unfortunately, we would get two different hash codes for these two objects that have logical equality. And that, in fact, is a anti-pattern or a violation of the hash code contract. And that's what we're trying to demonstrate here. So also, if it's not equal, here, it'll say that it's not equal. So if I run this, you'll see that a is equal to b, and c is not equal to d. So we have the equal variables. And then we have, let me zoom in a bit. We have the equal variables and the unequal variables. And you can clearly see, in the equal variables, because the string class follows the hash code contract, the hash code is the same. And for unequal variables, there are different hash codes. So that's good. That means the string class has overridden the hash code method. Speaking of the hash code contract, what does that mean? What is the hash code contract? As we saw from the demo, if two objects have logical equality, if there's an invocation of the hash code method, it should always consistently return the same value, whatever it may be. Although a caveat is it can differ depending on the application state, but that too should be considered when designing the hash code method. Or to put in a bit more clear way, let's say, using chargeGPT. For example, if you have an object that has a unique ID that is generated when the object is created, the hash code method could use this ID as a part of its calculation. Because the ID is unique to each instance of the object, the hash code method would return a unique value for each object. However, if the application is run again, the ID may not be the same. And thus, the hash code method would return a different value. This is the example that chargeGPT gave me when I asked to give an example, let's say, in the real world of how the hash code would differ depending on the application state. So that kind of makes sense because it's kind of depending on this external resource, which is the unique ID in this case. And of course, this seems like an obvious point, but it has to be stated. If they don't have logical equality, then it would probably give different hash codes when the hash code method is indicated. However, that also does not mean or there's no requirement that it should produce distinct results. Despite two objects not having logical equality, it's possible for it to return the same hash code. Though the vice versa, the opposite of that would be if they do have logical equality, then most certainly the hash code should be the same. Or put more succinctly, logical inequality doesn't necessarily mean objects will have different hash codes. However, as I've said here, if hash code, the implication of the method hash code, does return distinct integers for objects, this will improve performance in hash based collections as this will reduce collisions. Because there could be collisions if while they don't have logical equality, it still returns the same hash code. So if we can design a good quality hash code method, that will even try and prevent this, where we know for sure that two logically unequal objects will always have distinct hash codes, it will reduce these collisions. And the collection, and so whatever the collection that's using a hash based collection, won't have to look for logical equality prior to putting an object in a location or a hash bucket to be more specific. Now what I mean by this latter point, putting it into a location in a hash based data structure, it'll make more sense as we go through the demos. But the idea is, in a hash map or hash table or whatever, hash based data structure, when you give it objects, it'll look for, it will do the calculation and get the hash code, and then it'll put it in a specific hash bucket depending on the hash code, but also it'll look for logical equality. It'll look for logical equality because if there is logical equality, then without doing any more calculations, it could straight away put it in the specific hash bucket. The thing though is, I believe, and I think it works this way, it'll also create a kind of a link list if these objects have the same hash bucket. So it'll compare actually on second thought, I may be talking out of my ass here. So I'm just going to shut up and keep going and not try and make that point with such certainty. The point being, generally, considering both logical equality and inequality, it's perhaps a good idea to reduce collisions. Just take that as a general rule of thumb, let's say. Sorry, man. I've been thinking out loud here, and I don't know. Sometimes when you think out loud, you clearly do say bullshit, and I apologize if I did say something false there. Okay, so to clean up my mess and to not keep digging myself into this hole, I'm just going to go read off the book as that, in fact, is a good source of truth. On the point that I was trying to articulate here, here's what Joshua Block states. The key provision that is violated when you fail to override hash code is the second one. Equal objects must have equal hash codes. That's a really important point. Two distinct instances may be logically equal, according to a class's equals method, but two objects hash code method, they're just two objects with nothing much in common. And of course, therefore, objects hash code method returns to seemingly random numbers instead of two equal numbers as required by the hash code contract, which we discussed just here. So the idea of logical equality, giving the same hash code for hash code method invocation can be simply demoed with this phone number class that Joshua Block, in fact, has used. And this class was also used in item 10, I believe. We have a class called phone number that represents a phone number with an area code prefix and a line number. So if we go to the main method in this class, I've created an instance of, well, I've got a hash map here, sorry. And this hash map has key values of phone number and then a string, which is a phone number. If I put a new phone number object with the name Jenny into this hash map, what I'd expect is, as you can see here, I'm creating a new phone number object and I'm putting it in. Now, this hash map, because it's a hash based data structure, it's going to use the hash code of this object where to find the location in the map to put this object, the key and the value. So there's going to be the key with the hash code and the value is going to be Jenny. And it'll do that based on the hash code of this object, as I said, like 10 times already. Then in the next line, I'm trying to get that same value, Jenny, from the hash map. So what I'm doing is I'm saying m.get, so that's the hash map and I'm saying, hey, here's the key, give me that value. But unfortunately, because we haven't overwritten the hash code method, when this code is run, it returns a null. Because what happens is in the get, it'll create a different hash code, a new hash code, and it'll look for it in the hash map and it can't find it, so it'll return null. Despite both of these objects, this one here and this one here clearly having logical equality because they've got the same area code, prefix and line number. Now, simply the way to fix it is to override the hash code, which we've done here. I'll go through all of this stuff a bit later, much more punctiliously. But at the moment, just assume it has been overwritten to reflect logical equality. And now what will happen is the same bit of code that I'm going to run, it'll return Jenny. There you go. Which is good, because that's what we want. Because that means the hash base collection identified that we're in fact looking for this the same object. Or as Joshua block states, the phone number classes failure to override hash code courses the two equal instances to have unequal hash codes, which is why initially before the overwritten in the retrieval, we got a null in violation of the hash code contract. Therefore, the get method is likely. So I'll leave it in the code because it makes more sense than that. That's the get method. The therefore the get method is likely to look for the phone number in a different hash bracket from the one in which it was stored by the put method. Even if the two instances happen to hash to the same bucket, the get method will almost certainly return null because hash map has an optimization that caches the hash code associated with each entry and doesn't bother checking for object equality if the hash codes don't match. This kind of relates to the previous point that I tried to terribly articulate using bloody length list and all that. But the idea is that the idea is that if they do have hash codes that don't match, it's because the value is cash, it'll just return the cash value in this case a null. Now fixing the problem would be overriding the hash code method. The example I showed here is in fact, this is a good way of overriding it. But the worst way, despite it being legal, would be to simply return the same value returning 42 here. One should never do this because in this case for every single value, it's going to return the same hash code and that is that is hell. Or instead of invoking biblical references to put it the way Josh of Lockwood, it's legal because it ensures that equal objects have the same hash code. It's atrocious because it ensures that every object in fact has the same hash code and that shouldn't be the case. Therefore, every object hashes to the same bucket and hash tables degenerate to link lists. This was the point I was trying to make before and I kind of confused the two. And then he said it's programs that should run in linear time instead of running quadratic time. So just to get a bit of that computer science out of the way, I wouldn't say out of the way, it's in fact quite interesting. This is quite important to understand when it comes to designing algorithms, the difference between linear time and quadratic time. In linear time, the number of steps required is directly related one to one. So big O of n. So it's a number of elements, let's say in a data structure. So if it's in an array, it's the number of elements in that array. If the array is of size five, it would be big O of five here. In quadratic time, the number of steps is taken to accomplish a task is n squared because each item has to be compared with with each other. Now, because I'm partially retarded, I had to tell Judge GPT to explain this to me like I was Michael Scott from the office, because in many ways I embody that archetype. Yes, Michael Scott is an archetype in fact. And I ask from, sorry about that, a bit of background noise, let the plane pass. It's passing. And it's passed. Excellent. So I told Judge GPT, explain linear time versus quadratic time in computing like Michael Scott from the office. And here's what Judge GPT told me. And in fact, this is really good. This is really good explanation. Sure. Let's imagine you're trying to find a specific paperclip, haha, office reference, because you know, it's a paper company, find a specific paperclip in a huge pile of paperclips. If you use a linear time algorithm, it's like you're checking each paperclip one by one in a line, which is why it's called linear, I guess, until you find the one you're looking for. It takes a little longer with more paperclips, but the time it takes to search for the paperclips grow at a steady pace, just like a straight line. So it's, would it be called exponential? No, it's correlated to the amount of paperclips or simply put big O of N. But if you use quadratic time algorithm or a quadratic time algorithm, it's like you're checking each paperclip against every other paperclip in the pile over and over again, as the number of paperclips grows. The amount of time it takes to search for one, you're looking for grows at an exponential rate. So sorry, just to keep in mind, quadratic time is exponential, like a big square. So linear time is like searching through a line of paperclips, while quadratic time is like searching through a pile of paperclips. The former is faster and more efficient as the number of paperclips grows. So it's because of that reason, a good hash function, leaving aside the legality, so to speak, should produce distinct hash codes for unequal instances to prevent this, this mess of linear time and quadratic quadratic time when putting into a hash based data structure. Or as put in the book, this is exactly what is meant by the third part of the hash code contract. Ideally, a hash function should distribute any reasonable collection of unequal instances uniformly across all int values. And I kind of wanted to a more elaborated definition of this. So of course, I once again asked Judge CPT, and it said, for example, if the hash function returns integers, it should distribute the hash values evenly across the possible integer value. So if the hash function is applied to 10 unequal objects, the hash values produced for those objects should be spread across all the possible integer values, not just a small range of values. This helps to ensure efficient hash based data structures, such as hash tables, where hash collision should be kept to a minimum. Now, all of this explained here, this is, I think it kind of seems not self evident, but after this explanation, it seems to make sense. It's about uniformity. It should be spread across uniformly, depending on the int number of values you give it. But how do we achieve this? Because that's the ideal, and it can be a bit tricky at first glance. But fortunately, Joshua Block has stated there is a recipe for a high quality hash function. So to understand this hash function, firstly, let's go through the theory step by step, and then I'll jump into the demo and it'll obviously make a lot more sense when you see the actual code. Step number one is to declare an int variable called result and simply assign the value of the first significant field into this result variable. And keep in mind, as Joshua Block has stated here, recall from item 10 that a significant field is a field that affects equals comparisons. So whatever it may be, assign that to this variable value. Obviously, I screwed that up. I reread that a couple of times, and I made a blunder, I made a huge error there. It's you don't assign the value of the first significant field, you assign the hash code value of the first significant field. That is a bad mistake, because that can truly scrub the whole recipe. So keep in mind, you whatever the first significant field is, you calculate the hash code and you assign that to the result here. And then step number two, for every other remaining significant field F in your object, do the following compute int the int value hash code C. So all the other fields, if the field is a primitive type, so that could be an int, a char, whatever, use a box primitive and then use that and sort of like what I described here. So there's a primitive field 42 here, primitive int, use the box in and use value off and then use the box primitive hash code method to calculate the hash code value because the primitive type won't have a way to calculate the hash code method. So obviously, each primitive type in Java will have a corresponding box primitive, like here where int has integer. So the second part of that is that it's important to consider how the equals method compares field values. What I mean by that is, if the equals method recursively invokes equals on the object reference fields, then the hash code method should recursively invoke hash code on those fields. So in an object, whatever the fields that the equals method invokes equals to the hash code method should do the same. I will put this way, what I realized when I was going through this item, or more specifically, this recipe for the hash function is that a lot of the things that the equals method does, the hash code method should do too. A lot of the patterns that it follows, that the equals method does or follows, the hash code method should follow too. So the second part of that is, if the equals method requires a more complex comparison for the object reference field, the hash code method should compute a canonical representation for the field and invoke hash code on that representation. So what does this mean? We discussed canonical representation in item 10. As the name suggests, it's a canonical value. So if there's a certain field value in the object that's pro to change, that is rather dynamic, for the sake of comparison, or in this case, for the sake of calculating the hash code, we could assign a sort of static variable value that we consider to be the canonical representation of that field. And that can be used for all the computation and all the calculations of the equals method calculations, but also in the hash code method. That makes it gives our methods a bit more structure for object fields that have rather dynamic and volatile, and I don't mean volatile in the Java sense, I mean volatile just conceptually speaking, significant fields in an object. And the third part is, if the value of the field is null, obviously, the hash code method should use a constant value such as zero to represent the hash code for the field. That seems pretty self-explanatory because given that this result variable that we're using is an integer is of type int, we need something to correspond to a null where zero probably would be the apt value to use. And if the field is an array, with all fields of the array being significant, use arrays or hash code. So we can use this method from the arrays class in the library. Or as Joshua Booker said it here, if the field is an array, compute a hash code for each significant element by applying these rules recursively and combine the values per step to be. So per step to be is the next step. We'll get to that too. If the array has no significant elements, use a constant, preferably not zero. That makes sense because zero would normally be used for something like a null. We don't want to have that kind of conflict or confusion. And then step two be, of course, is to, all the values that we used here, that we computed here, sorry, combine them to get the hash code where we have the result value multiplied by 31 plus C. And what's C? It is the computed hash code value. And then you return result in the hash code function. This will all make sense once we look at the demo. In this demo, I've got a class called person. And this person class has some significant fields, their first name, type string, last name, type string, and address. And the address is a class that I've defined in this file, in fact. And the address essentially has a street, city, state, and zip code. So the addresses of address, as you can see clearly. And then we've got the age, which is of primitive type int. And also we have a string array, which I've called language is spoken. So the language this person speaks. And then we've got the constructor and the equals method also I've overridden. I'm not going to go through that because it's kind of on a scope. And I kind of went through this already in the previous item. And then we've overridden the hash code. So this hash code theoretically should be a high quality hash function or a hash code method, because I've followed the recipe delineated in just a blog's book or in effective Java. Initially, the result as we saw in step one has been arbitrary number 17 has been picked as a constant. This will reduce collisions. Keep in mind, don't pick a number like zero, start off with something like this. And then we've used the significant fields to calculate the result using the hash codes and then adding that to the result value. And again, multiplying by 31, this again is to give it give it more uniqueness or more more distinctiveness and preventing collisions. And then in the address, we've done it a bit differently where we're in fact looking for the hash value of the address object. And if it has been cached, or pardon me, if it's null, then we get economical address value. And if it is null, then we're going to return zero. So we're going to want to make sure that the address is not null and that this person has an object. And so we get canonical, the canonical address, which by the way, this method is defined in the address class. So back to this. And then we are also given that the age is of type int, it's a primitive type, we're using the box primitives. Sorry, yeah, box primitives and using the value often getting the hash code. And then for languages spoken, because the string array, we're using arrays or hash code, again, as stated in the hash function recipe, and then simply we return that result. So here's an example, the client using that class, we use Alha Camus, my one of my favorite essentialist authors, highly recommend you read him, start off probably with the stranger. And then if you're more philosophically inclined, perhaps the myth of Sisyphus, I love the stranger so much that I, in fact, I'm trying to learn French, just because of Camus. It's a beautiful book. So the languages Camus speaks, I found out, in fact, he speaks Arabic too. So it's English, French and Arabic. And then they're giving his address, I just randomly found this online, I'm not even sure if this is accurate, but this is probably where he lived. And unfortunately, he only lived to an age of 46 years because he died in a car crash, which is quite a tragedy, given I would have loved to read more of his work if he did live up to old age. And then when you run this function, it calculates the hash code for the Albert Camus, Albert Camus object. And then Joshua Book states, when you are finished writing the hash code method, ask yourself whether equal instances have equal hash codes, and many states do use unit tests and whatnot to figure that out. Now I haven't written unit tests for this, but clearly the way this client has used it, this can be converted to unit tests and used in a variety of ways. And then obviously, if equal instances have unequal hash codes, figure out why and fix the damn problem. He doesn't say damn, I just put it in there because why not? He's a nice guy. So that makes a question, or perhaps it doesn't, but at least it begs the question for me, what to exclude from the hash code computation? That's the typo that's on hash code, hash code computation. And he states you could in fact exclude so-called derived fields. And let's understand what they are before we get to the demo. Those are values that can be computed from other field values already in the hash code computation. So we did go through this idea of derived fields before in the, in item 10, I believe using a pentagon or some mathematical structure. But let's take a look at another demo. I think it was a polygon we used in item 10. And I didn't want to use that example here because, you know, firstly, my math is shit. I'm quite embarrassed about that. Therefore, I found it a bit hard to explain using the polygon class example. That's something I should work towards. And in fact, teach myself some mathematics, some basic mathematics at least. But this is way more straightforward. So what's the derived field? We have a full name. And obviously a full name can be derived off the first name and the last name. And in the hash code, we don't have to, when calculating the hash code, we don't have to use the full name in the hash code calculation, given that it's already derived from first name and last name. That's it. That's what a derived field is. And that could be excluded in a hash code method. Okay, so this next part, the order of the fields. And the point is the quality of the hash code method is contingent on the order of the fields if a given class has similar fields. Now, to be totally upfront, I found this part quite difficult to understand. So I'm going to try my best to try and explain this. I think I got it, but I think I'm going to struggle a bit to articulate it because it's not, intuitively, it's hard to get it on first glance. It requires a bit of thinking. So the reason for this is because of 2B. That is this calculation we make here. It's this is the reason that makes the ordering contingent for the hash code method. So Joshua book states, for example, if the multiplication were emitted from a string hash function, all anagrams would have identical hash codes. The value 31 was chosen because it is an odd prime. If it were even and the multiplication overflowed, information would be lost because modification by two is equivalent to shifting. Another way to think about that is that this multiplication here, 31 times i, that can be replaced by this the shifting of the of the sum or the left shift operator in Java. As far as Java is concerned, that and sorry, that and that are mathematically equivalent. Now we'll get to the definition a bit before a bit later, sorry. But firstly, let's take a look at some demos. So in this example, and this is a bad example where the order hasn't been considered. If you run this, there's a possibility that these two hash codes, so for object A and object B, which are two anagrams, that the hash code could be the same, because all they're doing in the hash code method, if you can see here is we're simply returning the hash code of word of the string value. So we're just using the hash code method in the class string. We aren't really considering anything else apart from that. In fact, I don't even know why they'll be written as second place here. It's like this is a superfluous or unnecessary method. And even though I ran this a couple of times, and this is why I said it depends on the application state, it is possible theoretically for this to return the same hash code despite these two being different. And then here, we are in fact multiplying by 31, and we are even having this arbitrary value initially. The chance of these hash code values being the same is a lot more different in comparison to the previous example that I showed in the other class. Both classes are called anagrams, it's a bit confusing, but they're in different directories, or yeah, the packages are different. So that's because we modified that a bit in hash code. This is a bad example. I only put this in here because it kind of goes along with what Joshua Block had stated in the book. I in fact thought using a separate class called person would be better to understand this. The reason I use anagram is because that's the example that he's used, but frankly, is equally so. I didn't really get it much. So maybe we'll try to understand it with the person class. I put that other part in there just to stick with what's in the book. And I thought that'll help in some way. Given he said if the multiplication were emitted from a string hash function, all anagrams would have identical hash codes. Ah, I now seek my confusion. He did say if the multiplication were emitted from a string hash function. So the reason we're getting different ones here is because in fact in the string hash function, it wasn't emitted. If it wasn't, it was kind of like this, just simply returning it. It could have been the same. But in the string hash function, if you look at the hash function, there is some multiplication being done. There's some work being done here. I'm not going to take a look at it. It's a bit too complicated for me, but it's not just simply taking that value in and passing the hash code. So anagrams themselves would have different hash codes, which is great. Apologies about that. I should have gone through that a bit more carefully beforehand, but it kind of makes sense now what he's trying to say here. And then the part I was trying to explain before about these two statements being mathematically equivalent. Joshua Bock states, a nice property of 31 is that the multiplication can be replaced by a shift and a subtraction for better performance on some architectures, because as I said, these two are equal, mathematically speaking. And modern VMs do this sort of optimization automatically. I perhaps did a terrible job at explaining that, because I too am trying to understand this idea of the shifting operator and all that. So I asked Judge GPT, and I think this is much more clearer than what I could ever say. Here's what Judge GPT states regarding this. The double listed operator is the left shift operator in Java, which shifts the bits of an integer to the left by a specified number of positions, effectively multiplying by 2 to the power n, where n is the number of positions shifted. That's why it's shifting. The minus operator is the subtraction operator, which subtracts the second operand from the first. So the expression i double less than 5 minus i shifts the bits of i to the left by 5 positions, effectively multiplying it by 2 to the power 5 or 32, and then subtracts i from the result. Since 32 minus 1 equals 31, the expression is equivalent to 31 times i. That's what it's this bit that gives this equality that Joshua Bloch speaks of. By using the optimization, the hash function implementation can take advantage of the efficient left shift operation on some architectures potentially leading to improved performance, and also Judge GPT repeats what he stated. Modern virtual machines are designed to automatically perform this kind of optimization, so the hash function implementation can remain unchanged and still benefit from the performance improvement. So a simple example of this would be if I ran this code, j would be equal to k. It'll print true because both these are equal. They both will be 310. And then let's take a look at another demo. I think we already kind of took a look at this. Sorry, by the way, not related to this is separate because Joshua Bloch states, say all that, let's apply this previous recipe to the phone number class. Now I already did apply it and show the demo in my own way, but it's important to take a look at what Joshua Bloch has done too. Here's the demo. As we saw, it returns the hash code of the initial value area code that he set, says he said to take the first significant field and assign the hash code value to result, and then do the calculation accordingly. And that returns the result. And on this method, he states, because this method returns the result of a simple deterministic computation, whose only inputs are the three significant fields in a phone number instance, that is area code, prefix and line number, is clear that that equal phone number instances have equal hash codes. It is simple, is reasonably fast, and does a reasonable job of dispersing unequal numbers into different hash buckets. And of course, a bit of a caveat here, as we saw in item 10, when it comes to equals comparison, even in the hash codes, he stated, if you have a bona fide need for hash functions, less likely to produce collisions, see Guava's co Java library, which is Google's co libraries for Java, and the hashing there does it much better than you and I ever could. And he's continued by giving a much more simpler one line hash function, which you'll see here. Comment out the other one. So these are all hash functions. There's different ways of implementing them. He's given three separate examples. The third one will go through soon. This is a one line hash function using the objects dot hash. The caveat here, despite it being a one line hash function, is that it should only be used if performance isn't critical, because it does return an array. And every time it's invoked, it returns an array and also involves auto boxing if we do pass a primitive type. And that takes us to an interesting part in these hash code implementations, which which is threat safe lazy initialization. So lazy initialization can be used if you believe your class is immutable. And if it considers or not considers it involves the invocation of the hash code involves recalculating the hash code every every time it's requested. And he says if you believe that most objects of this type will be used as hash keys, then you should calculate the hash code when the instance is created. Otherwise, you might choose to lazily initialize the hash code the first time hash code is invoked. Some care is required to ensure that the class remains threat safe in the presence of a lazily initialized field. Now we shall look into that because creating it to be threat safe is in fact an important not even an acillary point here, but it's in fact very much related to this idea of lazy initialization. But before we look at the threat safety example, let's firstly look at the phone number example. And the phone number class doesn't require this kind of threat safety. He's even stated that in the book. And what is lazy initialization? It's quite simple, really. We have a private in hash code method, and we check if the result is zero. If there's no cash result, we directly return that result. So that means every time the hash code method is invoked, it doesn't have to go through this bit and do the computation slash calculation. If it's cached, it could just be it could just be returned. And that's what lazy initialization is. And this is automatically initialized to zero initially. So as I've stated here in the comment, apt for immutable classes with expensive hash code calculation. Okay, now let's get to the threat safety bit, which is in fact, despite it being a bit complicated at first, I found most interesting in this item. I kind of enjoyed it, especially because I kind of do enjoy that part of Java, the whole multi threading bit in shelf. So here's an example of lazy initialization with threat safety. So we have this class called canna. And canna holds it's a it's an this class is atomic as we're using the atomic integer method. So atomic means and competing atomic means the either the change happens, or it doesn't happen. I always understood atomic operations in the context of a database. So a good example is something like a bank transaction. You either want the money to go go through, or you don't want it to go through at all. It's very binary in that sense, no pun intended. And that's what an atomic computation is. There's no there's no murky territory. There's no half of the computation happening. So we're using the count of type atomic integer. And then we have we have another hash code value here, a field value of also atomic integer. We have the constructors whatnot, we're setting the count getting the count forget about all that, not forget about all that ignore all that for this context. And then we have the hash code method. And the hash code method. Remember, all of this is still using the atomic integer. And here, though the hash code is computed lazily, it's still the same thing that we saw in the phone number class, where we're checking if the value is there checking if the value is equal to zero, only if it's equal to zero, where we're setting the value and and passing it, we're setting the value and then returning it. If not, if it's not equal to zero, that means the value has been cached. So we just return that value straight away. So that's the lazy initialization bit. But now we get to the thread safety bit, the multi threading bit. So I've started two threads here, t1, t2. And they're both they both got two counters. So firstly, I've got an instance of counter called count counter, of course, of the constructs, I've initialized that with 10. And then I've created two threads that sets the count that that changes the count concurrently. So two threads are started. So here, since we've used t1 dot join and t2 dot join, because of the use of this join method, only the final state of the counter, that's this one here, this object will be used, in this case, for get count, but for anything else, because the main thread waits for both child threads to finish, finish execution. So there are two threads spun up and there's a main thread, of course, that'll wait until both threads finish execution by using join. And since the atomic integer has been used, the value will either be 15 or 20. It's atomic, because the type for the hash code is atomic. And 15 and 20, of course, comes from this set count that we used here. So depending on which thread finishes first, it'll always be the final state will be reflected. So if I maybe write it a couple of times, this is how it came. Final count and so the final count is, so the final count is 20. And the hash code also comes as 20. But I think that's because the value is cached here. So if I run that a couple of times, let's see if at one point, it's always the second thread that finishes last. So it's passing that value. But let's say I change this, I don't know, I multiply this by 21, for some reason, and I run it again. In this case, the hash code value will change. And then if I run it again, now it's going to keep returning that because that value is cached. And the point being, because we're using type atomic integer, these operations will be atomic. And this is what Joshua Block means by creating lazy initialization to work with thread safety. So in this case, really, this bit doesn't matter too much. What matters is when we declare the hash code value, we use a type of atomic integer that ensures it's thread safe. So a few caveats here, the hash code initialization field value should not be the hash code of a commonly created instance. So that initial value we set for the hash code calculation, this is not a good example. If we go back to the phone number class, this initial value we set, it shouldn't be the hash code of a value that's commonly created, pardon me, not a commonly created value, a commonly created instance or some kind of object that's used quite a lot. So that initialization value, because that would obviously defeat the purpose of caching as it'll constantly be changing. And the other thing, and this is really important, like what Don Knuth stated, oh, it wasn't Don Knuth, wasn't it? It's apocryphally attributed to Don Knuth, but the root of optimization, sorry, I'd push it back. The root of all evil is premature optimization. So always choose accuracy over optimization. And the point is poor quality hash functions that are inaccurate will degrade hash tables to the point of being unusable. It's kind of connected to that previous point where we just set one constant value that legally makes sense, but would make a terrible hash function. So Joshua Bock states, do not be tempted to exclude significant fields from the hash code computation to improve performance. So always pick accuracy of optimization, especially with modern CPUs these days. Like why would you? Well, why would you? It makes more sense to focus on accuracy because there's pretty much infinite compute in the modern world. Or as he's put it here, in particular, the hash function may be confronted with a large collection of instances that differ mainly in regions that you've chosen to ignore. If this happens, the hash function will map all these instances to a few hash codes and programs that run in linear time. Sorry, I'd push it back. If this happens, the hash function will map all these instances to a few hash codes and programs that should run in linear time will instead run in quadratic time. So if we take this class example, this is an example of a poor implementation of the hash code. Last name in this person class, did I say class example? If we take this person class example, last name of this class is a significant field, obviously. But in the hash code, we've ignored it. Now, what that would do is to put it more clearly, I use strategy PD, because I struggled to articulate that. If we create a large collection of person objects, so from this class, that differ mainly in their last name field, so this field that we've ignored. The hash function implemented in the person class will be of poor quality and the hash based data structure, so like a hash map, that use it will experience many collisions. This can lead to poor performance and even course programs that should run in linear time to run in quadratic time as the book suggests. So I'll propose ignoring significant fields like what we've seen here. One example, just like a real world implementation is prior to Java 2, the string hash function in fact used at most 16 evenly spread characters to calculate the hash code. However, unfortunately, any string with more than 16 characters, as he said he has things such as URLs, would give a low quality hash function or a poor hash function essentially, as the name suggests. And that would be bad, that's the point I'm trying to make here. So keep the hash function flexible and open for future change, which is kind of, isn't that one pattern in the solid principles in object oriented programming. So make sure that's flexible and not static in like the string hash function prior to Java 2. And then the other important point is don't provide a detailed specification for the value returned by hash code, so clients can't reasonably depend on it. This gives you the flexibility to change it. Because if you do have a detailed specification, then people might rely on the hashing algorithm when clients use your class and also make it hard to do this open for future change bit, because the hash function isn't flexible. And so an example here is in fact the string class where if you can see here, people will rely on this formula for calculating the hash function. And that makes this hash code method less flexible. So a good example would be to keep the hashing algorithm hidden, like what we've seen here where all you see here is return objects dot hash name and age, we don't know what the hashing algorithm is in the object dot hash. So that's what he means by not not being explicit or specifying how the the hash code is calculated. Because as he states, if you leave the details unspecified, and a flow is found in the hash function, or a better hash function is discovered, you can change it in subsequent in a subsequent release. And then to end, he states, in summary, you must override hash code every time you override equals, or your program will not run correctly. Your hash code method must obey the general contract specified an object, that's the object class, and must do a reasonably, and must do a reasonable job assigning unequal hash codes to unequal instances. That was also a bit of a long item. And I did feel like I can't mess up a few parts in that item. Got feeling. I don't know what I'm probably see when I'm editing the video. If I did, I apologize. Please double check everything I've stated here. And if I made some significant blunders, I'll try and correct them in some way, either in the description or in the video itself. Nonetheless, I'm going to try my best. Because that's all one could do. Appreciate it. I shall see you in the next one. Item number 12, which I've already started on. I don't know why I'm doing this if you can see the book. Okay, there's the evidence that I've started on item number 12, which states always override to string. So in that one, cheers.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 6.96, "text": " Hey everyone, welcome to yet another episode of my video series on Joshua Bloch's effective Java", "tokens": [50364, 1911, 1518, 11, 2928, 281, 1939, 1071, 3500, 295, 452, 960, 2638, 322, 24005, 9865, 339, 311, 4942, 10745, 50712], "temperature": 0.0, "avg_logprob": -0.1539080540339152, "compression_ratio": 1.5384615384615385, "no_speech_prob": 0.100426085293293}, {"id": 1, "seek": 0, "start": 6.96, "end": 14.64, "text": " and today we're going to cover item number 11, always override hash code when you override equals.", "tokens": [50712, 293, 965, 321, 434, 516, 281, 2060, 3174, 1230, 2975, 11, 1009, 42321, 22019, 3089, 562, 291, 42321, 6915, 13, 51096], "temperature": 0.0, "avg_logprob": -0.1539080540339152, "compression_ratio": 1.5384615384615385, "no_speech_prob": 0.100426085293293}, {"id": 2, "seek": 0, "start": 15.280000000000001, "end": 20.32, "text": " But of course, before I get to the item as I usually do in the series, I want to go through", "tokens": [51128, 583, 295, 1164, 11, 949, 286, 483, 281, 264, 3174, 382, 286, 2673, 360, 294, 264, 2638, 11, 286, 528, 281, 352, 807, 51380], "temperature": 0.0, "avg_logprob": -0.1539080540339152, "compression_ratio": 1.5384615384615385, "no_speech_prob": 0.100426085293293}, {"id": 3, "seek": 0, "start": 20.32, "end": 26.48, "text": " this proviso and make a few points. Starting off, this is not a tutorial. I'm not a teacher.", "tokens": [51380, 341, 1439, 19227, 293, 652, 257, 1326, 2793, 13, 16217, 766, 11, 341, 307, 406, 257, 7073, 13, 286, 478, 406, 257, 5027, 13, 51688], "temperature": 0.0, "avg_logprob": -0.1539080540339152, "compression_ratio": 1.5384615384615385, "no_speech_prob": 0.100426085293293}, {"id": 4, "seek": 2648, "start": 27.04, "end": 34.56, "text": " Nor am I certainly an expert in Java or object oriented programming in general. So please don't", "tokens": [50392, 6966, 669, 286, 3297, 364, 5844, 294, 10745, 420, 2657, 21841, 9410, 294, 2674, 13, 407, 1767, 500, 380, 50768], "temperature": 0.0, "avg_logprob": -0.12734844373620074, "compression_ratio": 1.4595959595959596, "no_speech_prob": 0.12405222654342651}, {"id": 5, "seek": 2648, "start": 34.56, "end": 43.04, "text": " take what I say as gospel truth, always double check, do your own research. This is simply me", "tokens": [50768, 747, 437, 286, 584, 382, 14943, 3494, 11, 1009, 3834, 1520, 11, 360, 428, 1065, 2132, 13, 639, 307, 2935, 385, 51192], "temperature": 0.0, "avg_logprob": -0.12734844373620074, "compression_ratio": 1.4595959595959596, "no_speech_prob": 0.12405222654342651}, {"id": 6, "seek": 2648, "start": 43.04, "end": 49.519999999999996, "text": " sharing my learning experience. That's purely it. So don't treat this video series like a tutorial.", "tokens": [51192, 5414, 452, 2539, 1752, 13, 663, 311, 17491, 309, 13, 407, 500, 380, 2387, 341, 960, 2638, 411, 257, 7073, 13, 51516], "temperature": 0.0, "avg_logprob": -0.12734844373620074, "compression_ratio": 1.4595959595959596, "no_speech_prob": 0.12405222654342651}, {"id": 7, "seek": 4952, "start": 49.52, "end": 56.720000000000006, "text": " I may spew or give out false information, unfortunately, though I will try not to.", "tokens": [50364, 286, 815, 768, 86, 420, 976, 484, 7908, 1589, 11, 7015, 11, 1673, 286, 486, 853, 406, 281, 13, 50724], "temperature": 0.0, "avg_logprob": -0.15251896778742471, "compression_ratio": 1.5527426160337552, "no_speech_prob": 0.18676424026489258}, {"id": 8, "seek": 4952, "start": 57.760000000000005, "end": 64.24000000000001, "text": " Think of it more as the blind, leading the blind, that's an analogy that I've been using for a while", "tokens": [50776, 6557, 295, 309, 544, 382, 264, 6865, 11, 5775, 264, 6865, 11, 300, 311, 364, 21663, 300, 286, 600, 668, 1228, 337, 257, 1339, 51100], "temperature": 0.0, "avg_logprob": -0.15251896778742471, "compression_ratio": 1.5527426160337552, "no_speech_prob": 0.18676424026489258}, {"id": 9, "seek": 4952, "start": 64.24000000000001, "end": 72.56, "text": " now. I am as blind or perhaps even more blind than you are. So this is merely a exploratory", "tokens": [51100, 586, 13, 286, 669, 382, 6865, 420, 4317, 754, 544, 6865, 813, 291, 366, 13, 407, 341, 307, 17003, 257, 24765, 4745, 51516], "temperature": 0.0, "avg_logprob": -0.15251896778742471, "compression_ratio": 1.5527426160337552, "no_speech_prob": 0.18676424026489258}, {"id": 10, "seek": 4952, "start": 72.56, "end": 77.68, "text": " disquisition. I'm just figuring this thing out and hopefully we can learn together. In fact,", "tokens": [51516, 717, 15398, 849, 13, 286, 478, 445, 15213, 341, 551, 484, 293, 4696, 321, 393, 1466, 1214, 13, 682, 1186, 11, 51772], "temperature": 0.0, "avg_logprob": -0.15251896778742471, "compression_ratio": 1.5527426160337552, "no_speech_prob": 0.18676424026489258}, {"id": 11, "seek": 7768, "start": 77.68, "end": 82.56, "text": " I've already been corrected in a few of my other videos and I've been trying to rectify those errors.", "tokens": [50364, 286, 600, 1217, 668, 31687, 294, 257, 1326, 295, 452, 661, 2145, 293, 286, 600, 668, 1382, 281, 11048, 2505, 729, 13603, 13, 50608], "temperature": 0.0, "avg_logprob": -0.10843995746813323, "compression_ratio": 1.6118143459915613, "no_speech_prob": 0.016905102878808975}, {"id": 12, "seek": 7768, "start": 83.68, "end": 91.52000000000001, "text": " Therefore, again, not to repeat the same point, I will inevitably make mistakes. So please do", "tokens": [50664, 7504, 11, 797, 11, 406, 281, 7149, 264, 912, 935, 11, 286, 486, 28171, 652, 8038, 13, 407, 1767, 360, 51056], "temperature": 0.0, "avg_logprob": -0.10843995746813323, "compression_ratio": 1.6118143459915613, "no_speech_prob": 0.016905102878808975}, {"id": 13, "seek": 7768, "start": 91.52000000000001, "end": 96.56, "text": " point them out more for the benefit of the videos and myself. But yes, of course, for the benefit", "tokens": [51056, 935, 552, 484, 544, 337, 264, 5121, 295, 264, 2145, 293, 2059, 13, 583, 2086, 11, 295, 1164, 11, 337, 264, 5121, 51308], "temperature": 0.0, "avg_logprob": -0.10843995746813323, "compression_ratio": 1.6118143459915613, "no_speech_prob": 0.016905102878808975}, {"id": 14, "seek": 7768, "start": 96.56, "end": 104.64000000000001, "text": " of myself too, selfish speaking. And given this is a, in fact, programming video series,", "tokens": [51308, 295, 2059, 886, 11, 19074, 4124, 13, 400, 2212, 341, 307, 257, 11, 294, 1186, 11, 9410, 960, 2638, 11, 51712], "temperature": 0.0, "avg_logprob": -0.10843995746813323, "compression_ratio": 1.6118143459915613, "no_speech_prob": 0.016905102878808975}, {"id": 15, "seek": 10464, "start": 104.64, "end": 109.92, "text": " all the code that I use that I've been using for all the previous items that you can see here,", "tokens": [50364, 439, 264, 3089, 300, 286, 764, 300, 286, 600, 668, 1228, 337, 439, 264, 3894, 4754, 300, 291, 393, 536, 510, 11, 50628], "temperature": 0.0, "avg_logprob": -0.11841761589050293, "compression_ratio": 1.606060606060606, "no_speech_prob": 0.284327894449234}, {"id": 16, "seek": 10464, "start": 109.92, "end": 117.04, "text": " it's all available on my GitHub repo. I'll leave a link to the code for today's item, item number 11", "tokens": [50628, 309, 311, 439, 2435, 322, 452, 23331, 49040, 13, 286, 603, 1856, 257, 2113, 281, 264, 3089, 337, 965, 311, 3174, 11, 3174, 1230, 2975, 50984], "temperature": 0.0, "avg_logprob": -0.11841761589050293, "compression_ratio": 1.606060606060606, "no_speech_prob": 0.284327894449234}, {"id": 17, "seek": 10464, "start": 117.04, "end": 125.28, "text": " to my repo, but also to Joshua Block's sort of official repo that he has for this book.", "tokens": [50984, 281, 452, 49040, 11, 457, 611, 281, 24005, 17500, 311, 1333, 295, 4783, 49040, 300, 415, 575, 337, 341, 1446, 13, 51396], "temperature": 0.0, "avg_logprob": -0.11841761589050293, "compression_ratio": 1.606060606060606, "no_speech_prob": 0.284327894449234}, {"id": 18, "seek": 10464, "start": 125.28, "end": 129.36, "text": " Saying that without further ado, let's get started. So yeah, as I said, item number 11,", "tokens": [51396, 34087, 300, 1553, 3052, 22450, 11, 718, 311, 483, 1409, 13, 407, 1338, 11, 382, 286, 848, 11, 3174, 1230, 2975, 11, 51600], "temperature": 0.0, "avg_logprob": -0.11841761589050293, "compression_ratio": 1.606060606060606, "no_speech_prob": 0.284327894449234}, {"id": 19, "seek": 12936, "start": 130.08, "end": 137.28, "text": " which is always override hash code when you override equals and the item starts off like this.", "tokens": [50400, 597, 307, 1009, 42321, 22019, 3089, 562, 291, 42321, 6915, 293, 264, 3174, 3719, 766, 411, 341, 13, 50760], "temperature": 0.0, "avg_logprob": -0.08695487749008905, "compression_ratio": 1.7175925925925926, "no_speech_prob": 0.15199190378189087}, {"id": 20, "seek": 12936, "start": 139.20000000000002, "end": 145.28, "text": " Joshua Block states, you must override hash code in every class that overrides equals.", "tokens": [50856, 24005, 17500, 4368, 11, 291, 1633, 42321, 22019, 3089, 294, 633, 1508, 300, 670, 81, 1875, 6915, 13, 51160], "temperature": 0.0, "avg_logprob": -0.08695487749008905, "compression_ratio": 1.7175925925925926, "no_speech_prob": 0.15199190378189087}, {"id": 21, "seek": 12936, "start": 145.28, "end": 151.92000000000002, "text": " If you fail to do so, your class will violate the general contract for hash code, which will", "tokens": [51160, 759, 291, 3061, 281, 360, 370, 11, 428, 1508, 486, 37478, 264, 2674, 4364, 337, 22019, 3089, 11, 597, 486, 51492], "temperature": 0.0, "avg_logprob": -0.08695487749008905, "compression_ratio": 1.7175925925925926, "no_speech_prob": 0.15199190378189087}, {"id": 22, "seek": 12936, "start": 151.92000000000002, "end": 158.24, "text": " prevent it from functioning properly in collections such as hash map and hash set. But before we", "tokens": [51492, 4871, 309, 490, 18483, 6108, 294, 16641, 1270, 382, 22019, 4471, 293, 22019, 992, 13, 583, 949, 321, 51808], "temperature": 0.0, "avg_logprob": -0.08695487749008905, "compression_ratio": 1.7175925925925926, "no_speech_prob": 0.15199190378189087}, {"id": 23, "seek": 15824, "start": 158.24, "end": 163.92000000000002, "text": " continue on with the item, let's get some key phrases or key terms out of the way, get some", "tokens": [50364, 2354, 322, 365, 264, 3174, 11, 718, 311, 483, 512, 2141, 20312, 420, 2141, 2115, 484, 295, 264, 636, 11, 483, 512, 50648], "temperature": 0.0, "avg_logprob": -0.12495046558946667, "compression_ratio": 1.6343612334801763, "no_speech_prob": 0.005729679483920336}, {"id": 24, "seek": 15824, "start": 163.92000000000002, "end": 170.64000000000001, "text": " definitions out of the way. So what is hash code? And who better to ask than chat GPT? And oh,", "tokens": [50648, 21988, 484, 295, 264, 636, 13, 407, 437, 307, 22019, 3089, 30, 400, 567, 1101, 281, 1029, 813, 5081, 26039, 51, 30, 400, 1954, 11, 50984], "temperature": 0.0, "avg_logprob": -0.12495046558946667, "compression_ratio": 1.6343612334801763, "no_speech_prob": 0.005729679483920336}, {"id": 25, "seek": 15824, "start": 170.64000000000001, "end": 177.04000000000002, "text": " I forgot to mention, this item is a part of chapter three methods common to all objects.", "tokens": [50984, 286, 5298, 281, 2152, 11, 341, 3174, 307, 257, 644, 295, 7187, 1045, 7150, 2689, 281, 439, 6565, 13, 51304], "temperature": 0.0, "avg_logprob": -0.12495046558946667, "compression_ratio": 1.6343612334801763, "no_speech_prob": 0.005729679483920336}, {"id": 26, "seek": 15824, "start": 177.04000000000002, "end": 184.16000000000003, "text": " So back to it. What is hash code? Chat GPT states, and I think it's accurate here because I did", "tokens": [51304, 407, 646, 281, 309, 13, 708, 307, 22019, 3089, 30, 27503, 26039, 51, 4368, 11, 293, 286, 519, 309, 311, 8559, 510, 570, 286, 630, 51660], "temperature": 0.0, "avg_logprob": -0.12495046558946667, "compression_ratio": 1.6343612334801763, "no_speech_prob": 0.005729679483920336}, {"id": 27, "seek": 18416, "start": 184.16, "end": 191.84, "text": " double check with a few other resources or definitions online. In computing, a hash code", "tokens": [50364, 3834, 1520, 365, 257, 1326, 661, 3593, 420, 21988, 2950, 13, 682, 15866, 11, 257, 22019, 3089, 50748], "temperature": 0.0, "avg_logprob": -0.14521526923546424, "compression_ratio": 1.6, "no_speech_prob": 0.020020177587866783}, {"id": 28, "seek": 18416, "start": 191.84, "end": 200.64, "text": " also called a hash value, check sum or simply a hash is a fixed array, sorry, is a fixed size", "tokens": [50748, 611, 1219, 257, 22019, 2158, 11, 1520, 2408, 420, 2935, 257, 22019, 307, 257, 6806, 10225, 11, 2597, 11, 307, 257, 6806, 2744, 51188], "temperature": 0.0, "avg_logprob": -0.14521526923546424, "compression_ratio": 1.6, "no_speech_prob": 0.020020177587866783}, {"id": 29, "seek": 18416, "start": 200.64, "end": 207.35999999999999, "text": " string of characters that is generated by a one way mathematical function called a hash function.", "tokens": [51188, 6798, 295, 4342, 300, 307, 10833, 538, 257, 472, 636, 18894, 2445, 1219, 257, 22019, 2445, 13, 51524], "temperature": 0.0, "avg_logprob": -0.14521526923546424, "compression_ratio": 1.6, "no_speech_prob": 0.020020177587866783}, {"id": 30, "seek": 20736, "start": 208.08, "end": 215.12, "text": " From an input of any size, often call the message. The resulting hash code is typically", "tokens": [50400, 3358, 364, 4846, 295, 604, 2744, 11, 2049, 818, 264, 3636, 13, 440, 16505, 22019, 3089, 307, 5850, 50752], "temperature": 0.0, "avg_logprob": -0.08398541088761954, "compression_ratio": 1.6711711711711712, "no_speech_prob": 0.025175122544169426}, {"id": 31, "seek": 20736, "start": 215.68, "end": 222.16000000000003, "text": " a hexadecimal number that is a unique representation of the input. The main purpose of a hash code", "tokens": [50780, 257, 23291, 762, 66, 10650, 1230, 300, 307, 257, 3845, 10290, 295, 264, 4846, 13, 440, 2135, 4334, 295, 257, 22019, 3089, 51104], "temperature": 0.0, "avg_logprob": -0.08398541088761954, "compression_ratio": 1.6711711711711712, "no_speech_prob": 0.025175122544169426}, {"id": 32, "seek": 20736, "start": 222.16000000000003, "end": 228.64000000000001, "text": " is to take an input and produce a fixed size string of characters that can be used to identify", "tokens": [51104, 307, 281, 747, 364, 4846, 293, 5258, 257, 6806, 2744, 6798, 295, 4342, 300, 393, 312, 1143, 281, 5876, 51428], "temperature": 0.0, "avg_logprob": -0.08398541088761954, "compression_ratio": 1.6711711711711712, "no_speech_prob": 0.025175122544169426}, {"id": 33, "seek": 20736, "start": 228.64000000000001, "end": 234.88000000000002, "text": " or verify the input. Hash codes are commonly used in data structures such as hash tables,", "tokens": [51428, 420, 16888, 264, 4846, 13, 30775, 14211, 366, 12719, 1143, 294, 1412, 9227, 1270, 382, 22019, 8020, 11, 51740], "temperature": 0.0, "avg_logprob": -0.08398541088761954, "compression_ratio": 1.6711711711711712, "no_speech_prob": 0.025175122544169426}, {"id": 34, "seek": 23488, "start": 234.96, "end": 241.68, "text": " OS, just look at stated hash maps and hash sets. And in various algorithms, such as message,", "tokens": [50368, 12731, 11, 445, 574, 412, 11323, 22019, 11317, 293, 22019, 6352, 13, 400, 294, 3683, 14642, 11, 1270, 382, 3636, 11, 50704], "temperature": 0.0, "avg_logprob": -0.18394860660328585, "compression_ratio": 1.5948275862068966, "no_speech_prob": 0.002182560972869396}, {"id": 35, "seek": 23488, "start": 242.64, "end": 248.56, "text": " sorry, such as message authentication codes, digital signatures and check sums. Oh,", "tokens": [50752, 2597, 11, 1270, 382, 3636, 26643, 14211, 11, 4562, 32322, 293, 1520, 34499, 13, 876, 11, 51048], "temperature": 0.0, "avg_logprob": -0.18394860660328585, "compression_ratio": 1.5948275862068966, "no_speech_prob": 0.002182560972869396}, {"id": 36, "seek": 23488, "start": 248.56, "end": 254.4, "text": " this is an important point regarding hash functions. Hash functions are designed to be one way and", "tokens": [51048, 341, 307, 364, 1021, 935, 8595, 22019, 6828, 13, 30775, 6828, 366, 4761, 281, 312, 472, 636, 293, 51340], "temperature": 0.0, "avg_logprob": -0.18394860660328585, "compression_ratio": 1.5948275862068966, "no_speech_prob": 0.002182560972869396}, {"id": 37, "seek": 23488, "start": 254.4, "end": 261.12, "text": " deterministic, meaning that the same input will always produce, let me zoom in a bit actually,", "tokens": [51340, 15957, 3142, 11, 3620, 300, 264, 912, 4846, 486, 1009, 5258, 11, 718, 385, 8863, 294, 257, 857, 767, 11, 51676], "temperature": 0.0, "avg_logprob": -0.18394860660328585, "compression_ratio": 1.5948275862068966, "no_speech_prob": 0.002182560972869396}, {"id": 38, "seek": 26112, "start": 262.08, "end": 267.28000000000003, "text": " meaning that the same input will always produce the same output, but it is computationally", "tokens": [50412, 3620, 300, 264, 912, 4846, 486, 1009, 5258, 264, 912, 5598, 11, 457, 309, 307, 24903, 379, 50672], "temperature": 0.0, "avg_logprob": -0.12749248172925867, "compression_ratio": 1.7362637362637363, "no_speech_prob": 0.003272819332778454}, {"id": 39, "seek": 26112, "start": 267.28000000000003, "end": 273.12, "text": " infeasible to determine the original input from the output. And then more in the context of Java,", "tokens": [50672, 1536, 68, 296, 964, 281, 6997, 264, 3380, 4846, 490, 264, 5598, 13, 400, 550, 544, 294, 264, 4319, 295, 10745, 11, 50964], "temperature": 0.0, "avg_logprob": -0.12749248172925867, "compression_ratio": 1.7362637362637363, "no_speech_prob": 0.003272819332778454}, {"id": 40, "seek": 26112, "start": 273.12, "end": 278.4, "text": " I got this definition from educative.io, when the hash code, which is the hash code method,", "tokens": [50964, 286, 658, 341, 7123, 490, 2400, 1166, 13, 1004, 11, 562, 264, 22019, 3089, 11, 597, 307, 264, 22019, 3089, 3170, 11, 51228], "temperature": 0.0, "avg_logprob": -0.12749248172925867, "compression_ratio": 1.7362637362637363, "no_speech_prob": 0.003272819332778454}, {"id": 41, "seek": 26112, "start": 278.4, "end": 284.72, "text": " is called on two separate objects which are equal according to the equals method. So by the way,", "tokens": [51228, 307, 1219, 322, 732, 4994, 6565, 597, 366, 2681, 4650, 281, 264, 6915, 3170, 13, 407, 538, 264, 636, 11, 51544], "temperature": 0.0, "avg_logprob": -0.12749248172925867, "compression_ratio": 1.7362637362637363, "no_speech_prob": 0.003272819332778454}, {"id": 42, "seek": 26112, "start": 284.72, "end": 290.08, "text": " we discussed the equals method in the previous items. If you want to understand, get a, in fact,", "tokens": [51544, 321, 7152, 264, 6915, 3170, 294, 264, 3894, 4754, 13, 759, 291, 528, 281, 1223, 11, 483, 257, 11, 294, 1186, 11, 51812], "temperature": 0.0, "avg_logprob": -0.12749248172925867, "compression_ratio": 1.7362637362637363, "no_speech_prob": 0.003272819332778454}, {"id": 43, "seek": 29008, "start": 290.08, "end": 294.32, "text": " a really deep understanding of the equals method, I suggest or recommend you watch", "tokens": [50364, 257, 534, 2452, 3701, 295, 264, 6915, 3170, 11, 286, 3402, 420, 2748, 291, 1159, 50576], "temperature": 0.0, "avg_logprob": -0.11594652599758572, "compression_ratio": 1.6120689655172413, "no_speech_prob": 0.006692042574286461}, {"id": 44, "seek": 29008, "start": 295.2, "end": 301.44, "text": " all four parts for item number 10 because it was such a big item. I had to break it down into four", "tokens": [50620, 439, 1451, 3166, 337, 3174, 1230, 1266, 570, 309, 390, 1270, 257, 955, 3174, 13, 286, 632, 281, 1821, 309, 760, 666, 1451, 50932], "temperature": 0.0, "avg_logprob": -0.11594652599758572, "compression_ratio": 1.6120689655172413, "no_speech_prob": 0.006692042574286461}, {"id": 45, "seek": 29008, "start": 301.44, "end": 307.59999999999997, "text": " different videos, but in that, I dealt deeply into the equals method. In that case, getting back to", "tokens": [50932, 819, 2145, 11, 457, 294, 300, 11, 286, 15991, 8760, 666, 264, 6915, 3170, 13, 682, 300, 1389, 11, 1242, 646, 281, 51240], "temperature": 0.0, "avg_logprob": -0.11594652599758572, "compression_ratio": 1.6120689655172413, "no_speech_prob": 0.006692042574286461}, {"id": 46, "seek": 29008, "start": 307.59999999999997, "end": 315.2, "text": " this, which is called on two separate objects, it returns the same hash code value. However,", "tokens": [51240, 341, 11, 597, 307, 1219, 322, 732, 4994, 6565, 11, 309, 11247, 264, 912, 22019, 3089, 2158, 13, 2908, 11, 51620], "temperature": 0.0, "avg_logprob": -0.11594652599758572, "compression_ratio": 1.6120689655172413, "no_speech_prob": 0.006692042574286461}, {"id": 47, "seek": 31520, "start": 315.28, "end": 320.8, "text": " if it is called on two unequal objects, it will not necessarily return different integer values.", "tokens": [50368, 498, 309, 307, 1219, 322, 732, 2251, 22345, 6565, 11, 309, 486, 406, 4725, 2736, 819, 24922, 4190, 13, 50644], "temperature": 0.0, "avg_logprob": -0.10891760549237652, "compression_ratio": 1.6266094420600858, "no_speech_prob": 0.00970750767737627}, {"id": 48, "seek": 31520, "start": 320.8, "end": 326.8, "text": " So that's the definition in the context of Java. So to get started, here's a really simple,", "tokens": [50644, 407, 300, 311, 264, 7123, 294, 264, 4319, 295, 10745, 13, 407, 281, 483, 1409, 11, 510, 311, 257, 534, 2199, 11, 50944], "temperature": 0.0, "avg_logprob": -0.10891760549237652, "compression_ratio": 1.6266094420600858, "no_speech_prob": 0.00970750767737627}, {"id": 49, "seek": 31520, "start": 326.8, "end": 333.84, "text": " straightforward class as to how the hash code is used, example, courtesy of educative.io, of course.", "tokens": [50944, 15325, 1508, 382, 281, 577, 264, 22019, 3089, 307, 1143, 11, 1365, 11, 41704, 295, 2400, 1166, 13, 1004, 11, 295, 1164, 13, 51296], "temperature": 0.0, "avg_logprob": -0.10891760549237652, "compression_ratio": 1.6266094420600858, "no_speech_prob": 0.00970750767737627}, {"id": 50, "seek": 31520, "start": 334.64, "end": 341.28, "text": " So there's a class called hash, declared two strings, and keep in mind that these strings", "tokens": [51336, 407, 456, 311, 257, 1508, 1219, 22019, 11, 15489, 732, 13985, 11, 293, 1066, 294, 1575, 300, 613, 13985, 51668], "temperature": 0.0, "avg_logprob": -0.10891760549237652, "compression_ratio": 1.6266094420600858, "no_speech_prob": 0.00970750767737627}, {"id": 51, "seek": 34128, "start": 341.28, "end": 349.67999999999995, "text": " are immutable. And if equals b, this function will indicate that object a is equal to object b.", "tokens": [50364, 366, 3397, 32148, 13, 400, 498, 6915, 272, 11, 341, 2445, 486, 13330, 300, 2657, 257, 307, 2681, 281, 2657, 272, 13, 50784], "temperature": 0.0, "avg_logprob": -0.11478587943063655, "compression_ratio": 1.5879120879120878, "no_speech_prob": 0.03461102768778801}, {"id": 52, "seek": 34128, "start": 350.64, "end": 357.11999999999995, "text": " And it's important to keep in mind, if there is logical equality between two objects, if the hash", "tokens": [50832, 400, 309, 311, 1021, 281, 1066, 294, 1575, 11, 498, 456, 307, 14978, 14949, 1296, 732, 6565, 11, 498, 264, 22019, 51156], "temperature": 0.0, "avg_logprob": -0.11478587943063655, "compression_ratio": 1.5879120879120878, "no_speech_prob": 0.03461102768778801}, {"id": 53, "seek": 34128, "start": 357.11999999999995, "end": 365.28, "text": " code wasn't overridden, then unfortunately, we would get two different hash codes for these two", "tokens": [51156, 3089, 2067, 380, 670, 81, 6171, 11, 550, 7015, 11, 321, 576, 483, 732, 819, 22019, 14211, 337, 613, 732, 51564], "temperature": 0.0, "avg_logprob": -0.11478587943063655, "compression_ratio": 1.5879120879120878, "no_speech_prob": 0.03461102768778801}, {"id": 54, "seek": 36528, "start": 365.28, "end": 370.96, "text": " objects that have logical equality. And that, in fact, is a anti-pattern or a violation of the", "tokens": [50364, 6565, 300, 362, 14978, 14949, 13, 400, 300, 11, 294, 1186, 11, 307, 257, 6061, 12, 79, 1161, 77, 420, 257, 22840, 295, 264, 50648], "temperature": 0.0, "avg_logprob": -0.10753155606133598, "compression_ratio": 1.7058823529411764, "no_speech_prob": 0.0325753279030323}, {"id": 55, "seek": 36528, "start": 370.96, "end": 376.47999999999996, "text": " hash code contract. And that's what we're trying to demonstrate here. So also, if it's not equal,", "tokens": [50648, 22019, 3089, 4364, 13, 400, 300, 311, 437, 321, 434, 1382, 281, 11698, 510, 13, 407, 611, 11, 498, 309, 311, 406, 2681, 11, 50924], "temperature": 0.0, "avg_logprob": -0.10753155606133598, "compression_ratio": 1.7058823529411764, "no_speech_prob": 0.0325753279030323}, {"id": 56, "seek": 36528, "start": 376.47999999999996, "end": 382.79999999999995, "text": " here, it'll say that it's not equal. So if I run this, you'll see that a is equal to b, and c is", "tokens": [50924, 510, 11, 309, 603, 584, 300, 309, 311, 406, 2681, 13, 407, 498, 286, 1190, 341, 11, 291, 603, 536, 300, 257, 307, 2681, 281, 272, 11, 293, 269, 307, 51240], "temperature": 0.0, "avg_logprob": -0.10753155606133598, "compression_ratio": 1.7058823529411764, "no_speech_prob": 0.0325753279030323}, {"id": 57, "seek": 36528, "start": 382.79999999999995, "end": 388.55999999999995, "text": " not equal to d. So we have the equal variables. And then we have, let me zoom in a bit.", "tokens": [51240, 406, 2681, 281, 274, 13, 407, 321, 362, 264, 2681, 9102, 13, 400, 550, 321, 362, 11, 718, 385, 8863, 294, 257, 857, 13, 51528], "temperature": 0.0, "avg_logprob": -0.10753155606133598, "compression_ratio": 1.7058823529411764, "no_speech_prob": 0.0325753279030323}, {"id": 58, "seek": 38856, "start": 389.28000000000003, "end": 395.28000000000003, "text": " We have the equal variables and the unequal variables. And you can clearly see, in the equal", "tokens": [50400, 492, 362, 264, 2681, 9102, 293, 264, 2251, 22345, 9102, 13, 400, 291, 393, 4448, 536, 11, 294, 264, 2681, 50700], "temperature": 0.0, "avg_logprob": -0.1366642661716627, "compression_ratio": 2.0382978723404257, "no_speech_prob": 0.0031235425267368555}, {"id": 59, "seek": 38856, "start": 395.28000000000003, "end": 402.56, "text": " variables, because the string class follows the hash code contract, the hash code is the same.", "tokens": [50700, 9102, 11, 570, 264, 6798, 1508, 10002, 264, 22019, 3089, 4364, 11, 264, 22019, 3089, 307, 264, 912, 13, 51064], "temperature": 0.0, "avg_logprob": -0.1366642661716627, "compression_ratio": 2.0382978723404257, "no_speech_prob": 0.0031235425267368555}, {"id": 60, "seek": 38856, "start": 402.56, "end": 406.8, "text": " And for unequal variables, there are different hash codes. So that's good. That means the string", "tokens": [51064, 400, 337, 2251, 22345, 9102, 11, 456, 366, 819, 22019, 14211, 13, 407, 300, 311, 665, 13, 663, 1355, 264, 6798, 51276], "temperature": 0.0, "avg_logprob": -0.1366642661716627, "compression_ratio": 2.0382978723404257, "no_speech_prob": 0.0031235425267368555}, {"id": 61, "seek": 38856, "start": 406.8, "end": 412.0, "text": " class has overridden the hash code method. Speaking of the hash code contract, what does that mean?", "tokens": [51276, 1508, 575, 670, 81, 6171, 264, 22019, 3089, 3170, 13, 13069, 295, 264, 22019, 3089, 4364, 11, 437, 775, 300, 914, 30, 51536], "temperature": 0.0, "avg_logprob": -0.1366642661716627, "compression_ratio": 2.0382978723404257, "no_speech_prob": 0.0031235425267368555}, {"id": 62, "seek": 38856, "start": 412.0, "end": 417.2, "text": " What is the hash code contract? As we saw from the demo, if two objects have logical equality,", "tokens": [51536, 708, 307, 264, 22019, 3089, 4364, 30, 1018, 321, 1866, 490, 264, 10723, 11, 498, 732, 6565, 362, 14978, 14949, 11, 51796], "temperature": 0.0, "avg_logprob": -0.1366642661716627, "compression_ratio": 2.0382978723404257, "no_speech_prob": 0.0031235425267368555}, {"id": 63, "seek": 41720, "start": 417.92, "end": 422.4, "text": " if there's an invocation of the hash code method, it should always consistently return the same", "tokens": [50400, 498, 456, 311, 364, 1048, 27943, 295, 264, 22019, 3089, 3170, 11, 309, 820, 1009, 14961, 2736, 264, 912, 50624], "temperature": 0.0, "avg_logprob": -0.2195231789036801, "compression_ratio": 1.5761316872427984, "no_speech_prob": 0.005640887655317783}, {"id": 64, "seek": 41720, "start": 422.4, "end": 428.24, "text": " value, whatever it may be. Although a caveat is it can differ depending on the application state,", "tokens": [50624, 2158, 11, 2035, 309, 815, 312, 13, 5780, 257, 43012, 307, 309, 393, 743, 5413, 322, 264, 3861, 1785, 11, 50916], "temperature": 0.0, "avg_logprob": -0.2195231789036801, "compression_ratio": 1.5761316872427984, "no_speech_prob": 0.005640887655317783}, {"id": 65, "seek": 41720, "start": 428.24, "end": 433.84, "text": " but that too should be considered when designing the hash code method. Or to put in a bit more", "tokens": [50916, 457, 300, 886, 820, 312, 4888, 562, 14685, 264, 22019, 3089, 3170, 13, 1610, 281, 829, 294, 257, 857, 544, 51196], "temperature": 0.0, "avg_logprob": -0.2195231789036801, "compression_ratio": 1.5761316872427984, "no_speech_prob": 0.005640887655317783}, {"id": 66, "seek": 41720, "start": 433.84, "end": 441.52, "text": " clear way, let's say, using chargeGPT. For example, if you have an object that has a unique ID", "tokens": [51196, 1850, 636, 11, 718, 311, 584, 11, 1228, 4602, 38, 47, 51, 13, 1171, 1365, 11, 498, 291, 362, 364, 2657, 300, 575, 257, 3845, 7348, 51580], "temperature": 0.0, "avg_logprob": -0.2195231789036801, "compression_ratio": 1.5761316872427984, "no_speech_prob": 0.005640887655317783}, {"id": 67, "seek": 44152, "start": 441.76, "end": 449.2, "text": " that is generated when the object is created, the hash code method could use this ID as a part", "tokens": [50376, 300, 307, 10833, 562, 264, 2657, 307, 2942, 11, 264, 22019, 3089, 3170, 727, 764, 341, 7348, 382, 257, 644, 50748], "temperature": 0.0, "avg_logprob": -0.08317407865202829, "compression_ratio": 1.8407960199004976, "no_speech_prob": 0.0059104845859110355}, {"id": 68, "seek": 44152, "start": 449.2, "end": 455.03999999999996, "text": " of its calculation. Because the ID is unique to each instance of the object, the hash code method", "tokens": [50748, 295, 1080, 17108, 13, 1436, 264, 7348, 307, 3845, 281, 1184, 5197, 295, 264, 2657, 11, 264, 22019, 3089, 3170, 51040], "temperature": 0.0, "avg_logprob": -0.08317407865202829, "compression_ratio": 1.8407960199004976, "no_speech_prob": 0.0059104845859110355}, {"id": 69, "seek": 44152, "start": 455.03999999999996, "end": 460.56, "text": " would return a unique value for each object. However, if the application is run again,", "tokens": [51040, 576, 2736, 257, 3845, 2158, 337, 1184, 2657, 13, 2908, 11, 498, 264, 3861, 307, 1190, 797, 11, 51316], "temperature": 0.0, "avg_logprob": -0.08317407865202829, "compression_ratio": 1.8407960199004976, "no_speech_prob": 0.0059104845859110355}, {"id": 70, "seek": 44152, "start": 460.56, "end": 465.91999999999996, "text": " the ID may not be the same. And thus, the hash code method would return a different value.", "tokens": [51316, 264, 7348, 815, 406, 312, 264, 912, 13, 400, 8807, 11, 264, 22019, 3089, 3170, 576, 2736, 257, 819, 2158, 13, 51584], "temperature": 0.0, "avg_logprob": -0.08317407865202829, "compression_ratio": 1.8407960199004976, "no_speech_prob": 0.0059104845859110355}, {"id": 71, "seek": 46592, "start": 466.88, "end": 472.72, "text": " This is the example that chargeGPT gave me when I asked to give an example, let's say, in the real", "tokens": [50412, 639, 307, 264, 1365, 300, 4602, 38, 47, 51, 2729, 385, 562, 286, 2351, 281, 976, 364, 1365, 11, 718, 311, 584, 11, 294, 264, 957, 50704], "temperature": 0.0, "avg_logprob": -0.09091915747131964, "compression_ratio": 1.6049382716049383, "no_speech_prob": 0.015422947704792023}, {"id": 72, "seek": 46592, "start": 472.72, "end": 478.96000000000004, "text": " world of how the hash code would differ depending on the application state. So that kind of makes", "tokens": [50704, 1002, 295, 577, 264, 22019, 3089, 576, 743, 5413, 322, 264, 3861, 1785, 13, 407, 300, 733, 295, 1669, 51016], "temperature": 0.0, "avg_logprob": -0.09091915747131964, "compression_ratio": 1.6049382716049383, "no_speech_prob": 0.015422947704792023}, {"id": 73, "seek": 46592, "start": 478.96000000000004, "end": 485.52000000000004, "text": " sense because it's kind of depending on this external resource, which is the unique ID in this", "tokens": [51016, 2020, 570, 309, 311, 733, 295, 5413, 322, 341, 8320, 7684, 11, 597, 307, 264, 3845, 7348, 294, 341, 51344], "temperature": 0.0, "avg_logprob": -0.09091915747131964, "compression_ratio": 1.6049382716049383, "no_speech_prob": 0.015422947704792023}, {"id": 74, "seek": 46592, "start": 485.52000000000004, "end": 491.36, "text": " case. And of course, this seems like an obvious point, but it has to be stated. If they don't have", "tokens": [51344, 1389, 13, 400, 295, 1164, 11, 341, 2544, 411, 364, 6322, 935, 11, 457, 309, 575, 281, 312, 11323, 13, 759, 436, 500, 380, 362, 51636], "temperature": 0.0, "avg_logprob": -0.09091915747131964, "compression_ratio": 1.6049382716049383, "no_speech_prob": 0.015422947704792023}, {"id": 75, "seek": 49136, "start": 491.44, "end": 499.04, "text": " logical equality, then it would probably give different hash codes when the hash code method", "tokens": [50368, 14978, 14949, 11, 550, 309, 576, 1391, 976, 819, 22019, 14211, 562, 264, 22019, 3089, 3170, 50748], "temperature": 0.0, "avg_logprob": -0.10214720453534808, "compression_ratio": 1.6872246696035242, "no_speech_prob": 0.06095549464225769}, {"id": 76, "seek": 49136, "start": 499.04, "end": 504.8, "text": " is indicated. However, that also does not mean or there's no requirement that it should produce", "tokens": [50748, 307, 16176, 13, 2908, 11, 300, 611, 775, 406, 914, 420, 456, 311, 572, 11695, 300, 309, 820, 5258, 51036], "temperature": 0.0, "avg_logprob": -0.10214720453534808, "compression_ratio": 1.6872246696035242, "no_speech_prob": 0.06095549464225769}, {"id": 77, "seek": 49136, "start": 504.8, "end": 513.44, "text": " distinct results. Despite two objects not having logical equality, it's possible for it to return", "tokens": [51036, 10644, 3542, 13, 11334, 732, 6565, 406, 1419, 14978, 14949, 11, 309, 311, 1944, 337, 309, 281, 2736, 51468], "temperature": 0.0, "avg_logprob": -0.10214720453534808, "compression_ratio": 1.6872246696035242, "no_speech_prob": 0.06095549464225769}, {"id": 78, "seek": 49136, "start": 513.44, "end": 518.96, "text": " the same hash code. Though the vice versa, the opposite of that would be if they do have logical", "tokens": [51468, 264, 912, 22019, 3089, 13, 10404, 264, 11964, 25650, 11, 264, 6182, 295, 300, 576, 312, 498, 436, 360, 362, 14978, 51744], "temperature": 0.0, "avg_logprob": -0.10214720453534808, "compression_ratio": 1.6872246696035242, "no_speech_prob": 0.06095549464225769}, {"id": 79, "seek": 51896, "start": 518.96, "end": 522.88, "text": " equality, then most certainly the hash code should be the same. Or put more succinctly,", "tokens": [50364, 14949, 11, 550, 881, 3297, 264, 22019, 3089, 820, 312, 264, 912, 13, 1610, 829, 544, 21578, 5460, 356, 11, 50560], "temperature": 0.0, "avg_logprob": -0.11338506093839319, "compression_ratio": 1.6801801801801801, "no_speech_prob": 0.02262704260647297}, {"id": 80, "seek": 51896, "start": 522.88, "end": 528.64, "text": " logical inequality doesn't necessarily mean objects will have different hash codes. However,", "tokens": [50560, 14978, 16970, 1177, 380, 4725, 914, 6565, 486, 362, 819, 22019, 14211, 13, 2908, 11, 50848], "temperature": 0.0, "avg_logprob": -0.11338506093839319, "compression_ratio": 1.6801801801801801, "no_speech_prob": 0.02262704260647297}, {"id": 81, "seek": 51896, "start": 528.64, "end": 535.36, "text": " as I've said here, if hash code, the implication of the method hash code, does return distinct", "tokens": [50848, 382, 286, 600, 848, 510, 11, 498, 22019, 3089, 11, 264, 37814, 295, 264, 3170, 22019, 3089, 11, 775, 2736, 10644, 51184], "temperature": 0.0, "avg_logprob": -0.11338506093839319, "compression_ratio": 1.6801801801801801, "no_speech_prob": 0.02262704260647297}, {"id": 82, "seek": 51896, "start": 535.36, "end": 541.84, "text": " integers for objects, this will improve performance in hash based collections as this will reduce", "tokens": [51184, 41674, 337, 6565, 11, 341, 486, 3470, 3389, 294, 22019, 2361, 16641, 382, 341, 486, 5407, 51508], "temperature": 0.0, "avg_logprob": -0.11338506093839319, "compression_ratio": 1.6801801801801801, "no_speech_prob": 0.02262704260647297}, {"id": 83, "seek": 54184, "start": 541.84, "end": 548.72, "text": " collisions. Because there could be collisions if while they don't have logical equality,", "tokens": [50364, 46537, 13, 1436, 456, 727, 312, 46537, 498, 1339, 436, 500, 380, 362, 14978, 14949, 11, 50708], "temperature": 0.0, "avg_logprob": -0.13467937860733423, "compression_ratio": 1.6699029126213591, "no_speech_prob": 0.03963271528482437}, {"id": 84, "seek": 54184, "start": 549.2800000000001, "end": 555.52, "text": " it still returns the same hash code. So if we can design a good quality hash code method,", "tokens": [50736, 309, 920, 11247, 264, 912, 22019, 3089, 13, 407, 498, 321, 393, 1715, 257, 665, 3125, 22019, 3089, 3170, 11, 51048], "temperature": 0.0, "avg_logprob": -0.13467937860733423, "compression_ratio": 1.6699029126213591, "no_speech_prob": 0.03963271528482437}, {"id": 85, "seek": 54184, "start": 555.52, "end": 562.88, "text": " that will even try and prevent this, where we know for sure that two logically", "tokens": [51048, 300, 486, 754, 853, 293, 4871, 341, 11, 689, 321, 458, 337, 988, 300, 732, 38887, 51416], "temperature": 0.0, "avg_logprob": -0.13467937860733423, "compression_ratio": 1.6699029126213591, "no_speech_prob": 0.03963271528482437}, {"id": 86, "seek": 54184, "start": 562.88, "end": 567.76, "text": " unequal objects will always have distinct hash codes, it will reduce these collisions.", "tokens": [51416, 2251, 22345, 6565, 486, 1009, 362, 10644, 22019, 14211, 11, 309, 486, 5407, 613, 46537, 13, 51660], "temperature": 0.0, "avg_logprob": -0.13467937860733423, "compression_ratio": 1.6699029126213591, "no_speech_prob": 0.03963271528482437}, {"id": 87, "seek": 56776, "start": 567.76, "end": 573.68, "text": " And the collection, and so whatever the collection that's using a hash based collection,", "tokens": [50364, 400, 264, 5765, 11, 293, 370, 2035, 264, 5765, 300, 311, 1228, 257, 22019, 2361, 5765, 11, 50660], "temperature": 0.0, "avg_logprob": -0.1054892495413807, "compression_ratio": 1.8523206751054853, "no_speech_prob": 0.005301551427692175}, {"id": 88, "seek": 56776, "start": 573.68, "end": 578.3199999999999, "text": " won't have to look for logical equality prior to putting an object in a location", "tokens": [50660, 1582, 380, 362, 281, 574, 337, 14978, 14949, 4059, 281, 3372, 364, 2657, 294, 257, 4914, 50892], "temperature": 0.0, "avg_logprob": -0.1054892495413807, "compression_ratio": 1.8523206751054853, "no_speech_prob": 0.005301551427692175}, {"id": 89, "seek": 56776, "start": 578.3199999999999, "end": 582.3199999999999, "text": " or a hash bucket to be more specific. Now what I mean by this latter point,", "tokens": [50892, 420, 257, 22019, 13058, 281, 312, 544, 2685, 13, 823, 437, 286, 914, 538, 341, 18481, 935, 11, 51092], "temperature": 0.0, "avg_logprob": -0.1054892495413807, "compression_ratio": 1.8523206751054853, "no_speech_prob": 0.005301551427692175}, {"id": 90, "seek": 56776, "start": 582.3199999999999, "end": 587.2, "text": " putting it into a location in a hash based data structure, it'll make more sense as we go through", "tokens": [51092, 3372, 309, 666, 257, 4914, 294, 257, 22019, 2361, 1412, 3877, 11, 309, 603, 652, 544, 2020, 382, 321, 352, 807, 51336], "temperature": 0.0, "avg_logprob": -0.1054892495413807, "compression_ratio": 1.8523206751054853, "no_speech_prob": 0.005301551427692175}, {"id": 91, "seek": 56776, "start": 587.2, "end": 596.3199999999999, "text": " the demos. But the idea is, in a hash map or hash table or whatever, hash based data structure,", "tokens": [51336, 264, 33788, 13, 583, 264, 1558, 307, 11, 294, 257, 22019, 4471, 420, 22019, 3199, 420, 2035, 11, 22019, 2361, 1412, 3877, 11, 51792], "temperature": 0.0, "avg_logprob": -0.1054892495413807, "compression_ratio": 1.8523206751054853, "no_speech_prob": 0.005301551427692175}, {"id": 92, "seek": 59632, "start": 596.32, "end": 602.8000000000001, "text": " when you give it objects, it'll look for, it will do the calculation and get the hash code,", "tokens": [50364, 562, 291, 976, 309, 6565, 11, 309, 603, 574, 337, 11, 309, 486, 360, 264, 17108, 293, 483, 264, 22019, 3089, 11, 50688], "temperature": 0.0, "avg_logprob": -0.12643550060413503, "compression_ratio": 1.9344978165938864, "no_speech_prob": 0.002672398230060935}, {"id": 93, "seek": 59632, "start": 602.8000000000001, "end": 606.6400000000001, "text": " and then it'll put it in a specific hash bucket depending on the hash code, but also", "tokens": [50688, 293, 550, 309, 603, 829, 309, 294, 257, 2685, 22019, 13058, 5413, 322, 264, 22019, 3089, 11, 457, 611, 50880], "temperature": 0.0, "avg_logprob": -0.12643550060413503, "compression_ratio": 1.9344978165938864, "no_speech_prob": 0.002672398230060935}, {"id": 94, "seek": 59632, "start": 607.7600000000001, "end": 613.5200000000001, "text": " it'll look for logical equality. It'll look for logical equality because if there is logical", "tokens": [50936, 309, 603, 574, 337, 14978, 14949, 13, 467, 603, 574, 337, 14978, 14949, 570, 498, 456, 307, 14978, 51224], "temperature": 0.0, "avg_logprob": -0.12643550060413503, "compression_ratio": 1.9344978165938864, "no_speech_prob": 0.002672398230060935}, {"id": 95, "seek": 59632, "start": 613.5200000000001, "end": 617.5200000000001, "text": " equality, then without doing any more calculations, it could straight away put it in the specific", "tokens": [51224, 14949, 11, 550, 1553, 884, 604, 544, 20448, 11, 309, 727, 2997, 1314, 829, 309, 294, 264, 2685, 51424], "temperature": 0.0, "avg_logprob": -0.12643550060413503, "compression_ratio": 1.9344978165938864, "no_speech_prob": 0.002672398230060935}, {"id": 96, "seek": 59632, "start": 618.5600000000001, "end": 624.8000000000001, "text": " hash bucket. The thing though is, I believe, and I think it works this way,", "tokens": [51476, 22019, 13058, 13, 440, 551, 1673, 307, 11, 286, 1697, 11, 293, 286, 519, 309, 1985, 341, 636, 11, 51788], "temperature": 0.0, "avg_logprob": -0.12643550060413503, "compression_ratio": 1.9344978165938864, "no_speech_prob": 0.002672398230060935}, {"id": 97, "seek": 62480, "start": 625.5999999999999, "end": 635.1999999999999, "text": " it'll also create a kind of a link list if these objects have the same hash bucket. So it'll compare", "tokens": [50404, 309, 603, 611, 1884, 257, 733, 295, 257, 2113, 1329, 498, 613, 6565, 362, 264, 912, 22019, 13058, 13, 407, 309, 603, 6794, 50884], "temperature": 0.0, "avg_logprob": -0.1378560463587443, "compression_ratio": 1.5, "no_speech_prob": 0.005468140356242657}, {"id": 98, "seek": 62480, "start": 638.64, "end": 643.8399999999999, "text": " actually on second thought, I may be talking out of my ass here. So I'm just going to shut up", "tokens": [51056, 767, 322, 1150, 1194, 11, 286, 815, 312, 1417, 484, 295, 452, 1256, 510, 13, 407, 286, 478, 445, 516, 281, 5309, 493, 51316], "temperature": 0.0, "avg_logprob": -0.1378560463587443, "compression_ratio": 1.5, "no_speech_prob": 0.005468140356242657}, {"id": 99, "seek": 62480, "start": 643.8399999999999, "end": 650.0, "text": " and keep going and not try and make that point with such certainty. The point being,", "tokens": [51316, 293, 1066, 516, 293, 406, 853, 293, 652, 300, 935, 365, 1270, 27022, 13, 440, 935, 885, 11, 51624], "temperature": 0.0, "avg_logprob": -0.1378560463587443, "compression_ratio": 1.5, "no_speech_prob": 0.005468140356242657}, {"id": 100, "seek": 65000, "start": 650.64, "end": 658.08, "text": " generally, considering both logical equality and inequality, it's perhaps a good idea to reduce", "tokens": [50396, 5101, 11, 8079, 1293, 14978, 14949, 293, 16970, 11, 309, 311, 4317, 257, 665, 1558, 281, 5407, 50768], "temperature": 0.0, "avg_logprob": -0.11723806460698445, "compression_ratio": 1.594142259414226, "no_speech_prob": 0.028867943212389946}, {"id": 101, "seek": 65000, "start": 658.08, "end": 666.24, "text": " collisions. Just take that as a general rule of thumb, let's say. Sorry, man. I've been thinking", "tokens": [50768, 46537, 13, 1449, 747, 300, 382, 257, 2674, 4978, 295, 9298, 11, 718, 311, 584, 13, 4919, 11, 587, 13, 286, 600, 668, 1953, 51176], "temperature": 0.0, "avg_logprob": -0.11723806460698445, "compression_ratio": 1.594142259414226, "no_speech_prob": 0.028867943212389946}, {"id": 102, "seek": 65000, "start": 666.24, "end": 671.84, "text": " out loud here, and I don't know. Sometimes when you think out loud, you clearly do say bullshit,", "tokens": [51176, 484, 6588, 510, 11, 293, 286, 500, 380, 458, 13, 4803, 562, 291, 519, 484, 6588, 11, 291, 4448, 360, 584, 22676, 11, 51456], "temperature": 0.0, "avg_logprob": -0.11723806460698445, "compression_ratio": 1.594142259414226, "no_speech_prob": 0.028867943212389946}, {"id": 103, "seek": 65000, "start": 671.84, "end": 676.4, "text": " and I apologize if I did say something false there. Okay, so to clean up my mess and to not", "tokens": [51456, 293, 286, 12328, 498, 286, 630, 584, 746, 7908, 456, 13, 1033, 11, 370, 281, 2541, 493, 452, 2082, 293, 281, 406, 51684], "temperature": 0.0, "avg_logprob": -0.11723806460698445, "compression_ratio": 1.594142259414226, "no_speech_prob": 0.028867943212389946}, {"id": 104, "seek": 67640, "start": 676.4, "end": 683.4399999999999, "text": " keep digging myself into this hole, I'm just going to go read off the book as that, in fact, is a", "tokens": [50364, 1066, 17343, 2059, 666, 341, 5458, 11, 286, 478, 445, 516, 281, 352, 1401, 766, 264, 1446, 382, 300, 11, 294, 1186, 11, 307, 257, 50716], "temperature": 0.0, "avg_logprob": -0.11122390057178254, "compression_ratio": 1.54251012145749, "no_speech_prob": 0.0025908711832016706}, {"id": 105, "seek": 67640, "start": 683.4399999999999, "end": 688.56, "text": " good source of truth. On the point that I was trying to articulate here, here's what Joshua", "tokens": [50716, 665, 4009, 295, 3494, 13, 1282, 264, 935, 300, 286, 390, 1382, 281, 30305, 510, 11, 510, 311, 437, 24005, 50972], "temperature": 0.0, "avg_logprob": -0.11122390057178254, "compression_ratio": 1.54251012145749, "no_speech_prob": 0.0025908711832016706}, {"id": 106, "seek": 67640, "start": 688.56, "end": 696.0, "text": " Block states. The key provision that is violated when you fail to override hash code is the second", "tokens": [50972, 17500, 4368, 13, 440, 2141, 17225, 300, 307, 33239, 562, 291, 3061, 281, 42321, 22019, 3089, 307, 264, 1150, 51344], "temperature": 0.0, "avg_logprob": -0.11122390057178254, "compression_ratio": 1.54251012145749, "no_speech_prob": 0.0025908711832016706}, {"id": 107, "seek": 67640, "start": 696.0, "end": 703.04, "text": " one. Equal objects must have equal hash codes. That's a really important point. Two distinct", "tokens": [51344, 472, 13, 15624, 304, 6565, 1633, 362, 2681, 22019, 14211, 13, 663, 311, 257, 534, 1021, 935, 13, 4453, 10644, 51696], "temperature": 0.0, "avg_logprob": -0.11122390057178254, "compression_ratio": 1.54251012145749, "no_speech_prob": 0.0025908711832016706}, {"id": 108, "seek": 70304, "start": 703.04, "end": 709.04, "text": " instances may be logically equal, according to a class's equals method, but two objects", "tokens": [50364, 14519, 815, 312, 38887, 2681, 11, 4650, 281, 257, 1508, 311, 6915, 3170, 11, 457, 732, 6565, 50664], "temperature": 0.0, "avg_logprob": -0.094930499792099, "compression_ratio": 1.7014218009478672, "no_speech_prob": 0.014502808451652527}, {"id": 109, "seek": 70304, "start": 709.04, "end": 715.4399999999999, "text": " hash code method, they're just two objects with nothing much in common. And of course,", "tokens": [50664, 22019, 3089, 3170, 11, 436, 434, 445, 732, 6565, 365, 1825, 709, 294, 2689, 13, 400, 295, 1164, 11, 50984], "temperature": 0.0, "avg_logprob": -0.094930499792099, "compression_ratio": 1.7014218009478672, "no_speech_prob": 0.014502808451652527}, {"id": 110, "seek": 70304, "start": 715.4399999999999, "end": 721.92, "text": " therefore, objects hash code method returns to seemingly random numbers instead of two equal", "tokens": [50984, 4412, 11, 6565, 22019, 3089, 3170, 11247, 281, 18709, 4974, 3547, 2602, 295, 732, 2681, 51308], "temperature": 0.0, "avg_logprob": -0.094930499792099, "compression_ratio": 1.7014218009478672, "no_speech_prob": 0.014502808451652527}, {"id": 111, "seek": 70304, "start": 721.92, "end": 728.0799999999999, "text": " numbers as required by the hash code contract, which we discussed just here. So the idea of", "tokens": [51308, 3547, 382, 4739, 538, 264, 22019, 3089, 4364, 11, 597, 321, 7152, 445, 510, 13, 407, 264, 1558, 295, 51616], "temperature": 0.0, "avg_logprob": -0.094930499792099, "compression_ratio": 1.7014218009478672, "no_speech_prob": 0.014502808451652527}, {"id": 112, "seek": 72808, "start": 728.08, "end": 735.6800000000001, "text": " logical equality, giving the same hash code for hash code method invocation can be simply", "tokens": [50364, 14978, 14949, 11, 2902, 264, 912, 22019, 3089, 337, 22019, 3089, 3170, 1048, 27943, 393, 312, 2935, 50744], "temperature": 0.0, "avg_logprob": -0.09154073484651334, "compression_ratio": 1.6986301369863013, "no_speech_prob": 0.00460942042991519}, {"id": 113, "seek": 72808, "start": 735.6800000000001, "end": 740.72, "text": " demoed with this phone number class that Joshua Block, in fact, has used. And this class was also", "tokens": [50744, 10723, 292, 365, 341, 2593, 1230, 1508, 300, 24005, 17500, 11, 294, 1186, 11, 575, 1143, 13, 400, 341, 1508, 390, 611, 50996], "temperature": 0.0, "avg_logprob": -0.09154073484651334, "compression_ratio": 1.6986301369863013, "no_speech_prob": 0.00460942042991519}, {"id": 114, "seek": 72808, "start": 740.72, "end": 746.88, "text": " used in item 10, I believe. We have a class called phone number that represents a phone number with an", "tokens": [50996, 1143, 294, 3174, 1266, 11, 286, 1697, 13, 492, 362, 257, 1508, 1219, 2593, 1230, 300, 8855, 257, 2593, 1230, 365, 364, 51304], "temperature": 0.0, "avg_logprob": -0.09154073484651334, "compression_ratio": 1.6986301369863013, "no_speech_prob": 0.00460942042991519}, {"id": 115, "seek": 72808, "start": 746.88, "end": 754.4000000000001, "text": " area code prefix and a line number. So if we go to the main method in this class,", "tokens": [51304, 1859, 3089, 46969, 293, 257, 1622, 1230, 13, 407, 498, 321, 352, 281, 264, 2135, 3170, 294, 341, 1508, 11, 51680], "temperature": 0.0, "avg_logprob": -0.09154073484651334, "compression_ratio": 1.6986301369863013, "no_speech_prob": 0.00460942042991519}, {"id": 116, "seek": 75440, "start": 754.8, "end": 764.0799999999999, "text": " I've created an instance of, well, I've got a hash map here, sorry. And this hash map has", "tokens": [50384, 286, 600, 2942, 364, 5197, 295, 11, 731, 11, 286, 600, 658, 257, 22019, 4471, 510, 11, 2597, 13, 400, 341, 22019, 4471, 575, 50848], "temperature": 0.0, "avg_logprob": -0.16505880479688767, "compression_ratio": 1.6046511627906976, "no_speech_prob": 0.011506051756441593}, {"id": 117, "seek": 75440, "start": 765.28, "end": 772.16, "text": " key values of phone number and then a string, which is a phone number. If I put a new phone number", "tokens": [50908, 2141, 4190, 295, 2593, 1230, 293, 550, 257, 6798, 11, 597, 307, 257, 2593, 1230, 13, 759, 286, 829, 257, 777, 2593, 1230, 51252], "temperature": 0.0, "avg_logprob": -0.16505880479688767, "compression_ratio": 1.6046511627906976, "no_speech_prob": 0.011506051756441593}, {"id": 118, "seek": 75440, "start": 772.16, "end": 780.3199999999999, "text": " object with the name Jenny into this hash map, what I'd expect is, as you can see here,", "tokens": [51252, 2657, 365, 264, 1315, 20580, 666, 341, 22019, 4471, 11, 437, 286, 1116, 2066, 307, 11, 382, 291, 393, 536, 510, 11, 51660], "temperature": 0.0, "avg_logprob": -0.16505880479688767, "compression_ratio": 1.6046511627906976, "no_speech_prob": 0.011506051756441593}, {"id": 119, "seek": 78032, "start": 780.32, "end": 787.12, "text": " I'm creating a new phone number object and I'm putting it in. Now, this hash map, because it's a", "tokens": [50364, 286, 478, 4084, 257, 777, 2593, 1230, 2657, 293, 286, 478, 3372, 309, 294, 13, 823, 11, 341, 22019, 4471, 11, 570, 309, 311, 257, 50704], "temperature": 0.0, "avg_logprob": -0.10506610114975731, "compression_ratio": 1.8349514563106797, "no_speech_prob": 0.01150710228830576}, {"id": 120, "seek": 78032, "start": 787.12, "end": 793.84, "text": " hash based data structure, it's going to use the hash code of this object where to find the", "tokens": [50704, 22019, 2361, 1412, 3877, 11, 309, 311, 516, 281, 764, 264, 22019, 3089, 295, 341, 2657, 689, 281, 915, 264, 51040], "temperature": 0.0, "avg_logprob": -0.10506610114975731, "compression_ratio": 1.8349514563106797, "no_speech_prob": 0.01150710228830576}, {"id": 121, "seek": 78032, "start": 793.84, "end": 799.6, "text": " location in the map to put this object, the key and the value. So there's going to be the key", "tokens": [51040, 4914, 294, 264, 4471, 281, 829, 341, 2657, 11, 264, 2141, 293, 264, 2158, 13, 407, 456, 311, 516, 281, 312, 264, 2141, 51328], "temperature": 0.0, "avg_logprob": -0.10506610114975731, "compression_ratio": 1.8349514563106797, "no_speech_prob": 0.01150710228830576}, {"id": 122, "seek": 78032, "start": 799.6, "end": 807.36, "text": " with the hash code and the value is going to be Jenny. And it'll do that based on the hash code", "tokens": [51328, 365, 264, 22019, 3089, 293, 264, 2158, 307, 516, 281, 312, 20580, 13, 400, 309, 603, 360, 300, 2361, 322, 264, 22019, 3089, 51716], "temperature": 0.0, "avg_logprob": -0.10506610114975731, "compression_ratio": 1.8349514563106797, "no_speech_prob": 0.01150710228830576}, {"id": 123, "seek": 80736, "start": 807.36, "end": 814.96, "text": " of this object, as I said, like 10 times already. Then in the next line, I'm trying to get that", "tokens": [50364, 295, 341, 2657, 11, 382, 286, 848, 11, 411, 1266, 1413, 1217, 13, 1396, 294, 264, 958, 1622, 11, 286, 478, 1382, 281, 483, 300, 50744], "temperature": 0.0, "avg_logprob": -0.09587062488902699, "compression_ratio": 1.6317991631799162, "no_speech_prob": 0.004538135137408972}, {"id": 124, "seek": 80736, "start": 814.96, "end": 822.48, "text": " same value, Jenny, from the hash map. So what I'm doing is I'm saying m.get, so that's the hash map", "tokens": [50744, 912, 2158, 11, 20580, 11, 490, 264, 22019, 4471, 13, 407, 437, 286, 478, 884, 307, 286, 478, 1566, 275, 13, 847, 11, 370, 300, 311, 264, 22019, 4471, 51120], "temperature": 0.0, "avg_logprob": -0.09587062488902699, "compression_ratio": 1.6317991631799162, "no_speech_prob": 0.004538135137408972}, {"id": 125, "seek": 80736, "start": 822.48, "end": 828.48, "text": " and I'm saying, hey, here's the key, give me that value. But unfortunately, because we haven't", "tokens": [51120, 293, 286, 478, 1566, 11, 4177, 11, 510, 311, 264, 2141, 11, 976, 385, 300, 2158, 13, 583, 7015, 11, 570, 321, 2378, 380, 51420], "temperature": 0.0, "avg_logprob": -0.09587062488902699, "compression_ratio": 1.6317991631799162, "no_speech_prob": 0.004538135137408972}, {"id": 126, "seek": 80736, "start": 828.48, "end": 835.44, "text": " overwritten the hash code method, when this code is run, it returns a null. Because what happens is", "tokens": [51420, 670, 26859, 264, 22019, 3089, 3170, 11, 562, 341, 3089, 307, 1190, 11, 309, 11247, 257, 18184, 13, 1436, 437, 2314, 307, 51768], "temperature": 0.0, "avg_logprob": -0.09587062488902699, "compression_ratio": 1.6317991631799162, "no_speech_prob": 0.004538135137408972}, {"id": 127, "seek": 83544, "start": 835.44, "end": 841.2, "text": " in the get, it'll create a different hash code, a new hash code, and it'll look for it in the", "tokens": [50364, 294, 264, 483, 11, 309, 603, 1884, 257, 819, 22019, 3089, 11, 257, 777, 22019, 3089, 11, 293, 309, 603, 574, 337, 309, 294, 264, 50652], "temperature": 0.0, "avg_logprob": -0.09492849836162492, "compression_ratio": 1.7017543859649122, "no_speech_prob": 0.006097167730331421}, {"id": 128, "seek": 83544, "start": 841.2, "end": 849.7600000000001, "text": " hash map and it can't find it, so it'll return null. Despite both of these objects, this one here", "tokens": [50652, 22019, 4471, 293, 309, 393, 380, 915, 309, 11, 370, 309, 603, 2736, 18184, 13, 11334, 1293, 295, 613, 6565, 11, 341, 472, 510, 51080], "temperature": 0.0, "avg_logprob": -0.09492849836162492, "compression_ratio": 1.7017543859649122, "no_speech_prob": 0.006097167730331421}, {"id": 129, "seek": 83544, "start": 849.7600000000001, "end": 855.0400000000001, "text": " and this one here clearly having logical equality because they've got the same area code, prefix", "tokens": [51080, 293, 341, 472, 510, 4448, 1419, 14978, 14949, 570, 436, 600, 658, 264, 912, 1859, 3089, 11, 46969, 51344], "temperature": 0.0, "avg_logprob": -0.09492849836162492, "compression_ratio": 1.7017543859649122, "no_speech_prob": 0.006097167730331421}, {"id": 130, "seek": 83544, "start": 855.0400000000001, "end": 861.36, "text": " and line number. Now, simply the way to fix it is to override the hash code, which we've done here.", "tokens": [51344, 293, 1622, 1230, 13, 823, 11, 2935, 264, 636, 281, 3191, 309, 307, 281, 42321, 264, 22019, 3089, 11, 597, 321, 600, 1096, 510, 13, 51660], "temperature": 0.0, "avg_logprob": -0.09492849836162492, "compression_ratio": 1.7017543859649122, "no_speech_prob": 0.006097167730331421}, {"id": 131, "seek": 86136, "start": 861.36, "end": 866.48, "text": " I'll go through all of this stuff a bit later, much more punctiliously. But at the moment, just", "tokens": [50364, 286, 603, 352, 807, 439, 295, 341, 1507, 257, 857, 1780, 11, 709, 544, 27006, 388, 8994, 13, 583, 412, 264, 1623, 11, 445, 50620], "temperature": 0.0, "avg_logprob": -0.12136629072286315, "compression_ratio": 1.5901639344262295, "no_speech_prob": 0.03731701150536537}, {"id": 132, "seek": 86136, "start": 866.48, "end": 872.32, "text": " assume it has been overwritten to reflect logical equality. And now what will happen is the same", "tokens": [50620, 6552, 309, 575, 668, 670, 26859, 281, 5031, 14978, 14949, 13, 400, 586, 437, 486, 1051, 307, 264, 912, 50912], "temperature": 0.0, "avg_logprob": -0.12136629072286315, "compression_ratio": 1.5901639344262295, "no_speech_prob": 0.03731701150536537}, {"id": 133, "seek": 86136, "start": 872.32, "end": 878.64, "text": " bit of code that I'm going to run, it'll return Jenny. There you go. Which is good, because that's", "tokens": [50912, 857, 295, 3089, 300, 286, 478, 516, 281, 1190, 11, 309, 603, 2736, 20580, 13, 821, 291, 352, 13, 3013, 307, 665, 11, 570, 300, 311, 51228], "temperature": 0.0, "avg_logprob": -0.12136629072286315, "compression_ratio": 1.5901639344262295, "no_speech_prob": 0.03731701150536537}, {"id": 134, "seek": 86136, "start": 878.64, "end": 884.8000000000001, "text": " what we want. Because that means the hash base collection identified that we're in fact looking", "tokens": [51228, 437, 321, 528, 13, 1436, 300, 1355, 264, 22019, 3096, 5765, 9234, 300, 321, 434, 294, 1186, 1237, 51536], "temperature": 0.0, "avg_logprob": -0.12136629072286315, "compression_ratio": 1.5901639344262295, "no_speech_prob": 0.03731701150536537}, {"id": 135, "seek": 86136, "start": 884.8000000000001, "end": 890.24, "text": " for this the same object. Or as Joshua block states, the phone number classes failure to override", "tokens": [51536, 337, 341, 264, 912, 2657, 13, 1610, 382, 24005, 3461, 4368, 11, 264, 2593, 1230, 5359, 7763, 281, 42321, 51808], "temperature": 0.0, "avg_logprob": -0.12136629072286315, "compression_ratio": 1.5901639344262295, "no_speech_prob": 0.03731701150536537}, {"id": 136, "seek": 89024, "start": 890.24, "end": 896.72, "text": " hash code courses the two equal instances to have unequal hash codes, which is why initially", "tokens": [50364, 22019, 3089, 7712, 264, 732, 2681, 14519, 281, 362, 2251, 22345, 22019, 14211, 11, 597, 307, 983, 9105, 50688], "temperature": 0.0, "avg_logprob": -0.0991032634462629, "compression_ratio": 1.8274509803921568, "no_speech_prob": 0.003707104828208685}, {"id": 137, "seek": 89024, "start": 896.72, "end": 902.48, "text": " before the overwritten in the retrieval, we got a null in violation of the hash code contract.", "tokens": [50688, 949, 264, 670, 26859, 294, 264, 19817, 3337, 11, 321, 658, 257, 18184, 294, 22840, 295, 264, 22019, 3089, 4364, 13, 50976], "temperature": 0.0, "avg_logprob": -0.0991032634462629, "compression_ratio": 1.8274509803921568, "no_speech_prob": 0.003707104828208685}, {"id": 138, "seek": 89024, "start": 903.52, "end": 908.64, "text": " Therefore, the get method is likely. So I'll leave it in the code because it makes more sense", "tokens": [51028, 7504, 11, 264, 483, 3170, 307, 3700, 13, 407, 286, 603, 1856, 309, 294, 264, 3089, 570, 309, 1669, 544, 2020, 51284], "temperature": 0.0, "avg_logprob": -0.0991032634462629, "compression_ratio": 1.8274509803921568, "no_speech_prob": 0.003707104828208685}, {"id": 139, "seek": 89024, "start": 908.64, "end": 914.16, "text": " than that. That's the get method. The therefore the get method is likely to look for the phone", "tokens": [51284, 813, 300, 13, 663, 311, 264, 483, 3170, 13, 440, 4412, 264, 483, 3170, 307, 3700, 281, 574, 337, 264, 2593, 51560], "temperature": 0.0, "avg_logprob": -0.0991032634462629, "compression_ratio": 1.8274509803921568, "no_speech_prob": 0.003707104828208685}, {"id": 140, "seek": 89024, "start": 914.16, "end": 918.8, "text": " number in a different hash bracket from the one in which it was stored by the put method.", "tokens": [51560, 1230, 294, 257, 819, 22019, 16904, 490, 264, 472, 294, 597, 309, 390, 12187, 538, 264, 829, 3170, 13, 51792], "temperature": 0.0, "avg_logprob": -0.0991032634462629, "compression_ratio": 1.8274509803921568, "no_speech_prob": 0.003707104828208685}, {"id": 141, "seek": 91880, "start": 919.52, "end": 925.12, "text": " Even if the two instances happen to hash to the same bucket, the get method will almost", "tokens": [50400, 2754, 498, 264, 732, 14519, 1051, 281, 22019, 281, 264, 912, 13058, 11, 264, 483, 3170, 486, 1920, 50680], "temperature": 0.0, "avg_logprob": -0.09806579198592748, "compression_ratio": 1.5991189427312775, "no_speech_prob": 0.0025112763978540897}, {"id": 142, "seek": 91880, "start": 925.12, "end": 931.76, "text": " certainly return null because hash map has an optimization that caches the hash code associated", "tokens": [50680, 3297, 2736, 18184, 570, 22019, 4471, 575, 364, 19618, 300, 269, 13272, 264, 22019, 3089, 6615, 51012], "temperature": 0.0, "avg_logprob": -0.09806579198592748, "compression_ratio": 1.5991189427312775, "no_speech_prob": 0.0025112763978540897}, {"id": 143, "seek": 91880, "start": 931.76, "end": 938.4799999999999, "text": " with each entry and doesn't bother checking for object equality if the hash codes don't match.", "tokens": [51012, 365, 1184, 8729, 293, 1177, 380, 8677, 8568, 337, 2657, 14949, 498, 264, 22019, 14211, 500, 380, 2995, 13, 51348], "temperature": 0.0, "avg_logprob": -0.09806579198592748, "compression_ratio": 1.5991189427312775, "no_speech_prob": 0.0025112763978540897}, {"id": 144, "seek": 91880, "start": 938.4799999999999, "end": 942.88, "text": " This kind of relates to the previous point that I tried to terribly articulate using", "tokens": [51348, 639, 733, 295, 16155, 281, 264, 3894, 935, 300, 286, 3031, 281, 22903, 30305, 1228, 51568], "temperature": 0.0, "avg_logprob": -0.09806579198592748, "compression_ratio": 1.5991189427312775, "no_speech_prob": 0.0025112763978540897}, {"id": 145, "seek": 94288, "start": 943.4399999999999, "end": 948.24, "text": " bloody length list and all that. But the idea is that the idea is that if they do have", "tokens": [50392, 18938, 4641, 1329, 293, 439, 300, 13, 583, 264, 1558, 307, 300, 264, 1558, 307, 300, 498, 436, 360, 362, 50632], "temperature": 0.0, "avg_logprob": -0.13605875804506498, "compression_ratio": 1.783132530120482, "no_speech_prob": 0.020331334322690964}, {"id": 146, "seek": 94288, "start": 949.68, "end": 954.24, "text": " hash codes that don't match, it's because the value is cash, it'll just return the cash value", "tokens": [50704, 22019, 14211, 300, 500, 380, 2995, 11, 309, 311, 570, 264, 2158, 307, 6388, 11, 309, 603, 445, 2736, 264, 6388, 2158, 50932], "temperature": 0.0, "avg_logprob": -0.13605875804506498, "compression_ratio": 1.783132530120482, "no_speech_prob": 0.020331334322690964}, {"id": 147, "seek": 94288, "start": 954.24, "end": 958.4, "text": " in this case a null. Now fixing the problem would be overriding the hash code method. The", "tokens": [50932, 294, 341, 1389, 257, 18184, 13, 823, 19442, 264, 1154, 576, 312, 670, 81, 2819, 264, 22019, 3089, 3170, 13, 440, 51140], "temperature": 0.0, "avg_logprob": -0.13605875804506498, "compression_ratio": 1.783132530120482, "no_speech_prob": 0.020331334322690964}, {"id": 148, "seek": 94288, "start": 958.4, "end": 963.36, "text": " example I showed here is in fact, this is a good way of overriding it. But the worst way,", "tokens": [51140, 1365, 286, 4712, 510, 307, 294, 1186, 11, 341, 307, 257, 665, 636, 295, 670, 81, 2819, 309, 13, 583, 264, 5855, 636, 11, 51388], "temperature": 0.0, "avg_logprob": -0.13605875804506498, "compression_ratio": 1.783132530120482, "no_speech_prob": 0.020331334322690964}, {"id": 149, "seek": 94288, "start": 963.36, "end": 968.96, "text": " despite it being legal, would be to simply return the same value returning 42 here.", "tokens": [51388, 7228, 309, 885, 5089, 11, 576, 312, 281, 2935, 2736, 264, 912, 2158, 12678, 14034, 510, 13, 51668], "temperature": 0.0, "avg_logprob": -0.13605875804506498, "compression_ratio": 1.783132530120482, "no_speech_prob": 0.020331334322690964}, {"id": 150, "seek": 96896, "start": 969.44, "end": 973.76, "text": " One should never do this because in this case for every single value, it's going to return", "tokens": [50388, 1485, 820, 1128, 360, 341, 570, 294, 341, 1389, 337, 633, 2167, 2158, 11, 309, 311, 516, 281, 2736, 50604], "temperature": 0.0, "avg_logprob": -0.14276390075683593, "compression_ratio": 1.7922705314009661, "no_speech_prob": 0.03358610346913338}, {"id": 151, "seek": 96896, "start": 974.4000000000001, "end": 979.76, "text": " the same hash code and that is that is hell. Or instead of invoking biblical references to", "tokens": [50636, 264, 912, 22019, 3089, 293, 300, 307, 300, 307, 4921, 13, 1610, 2602, 295, 1048, 5953, 26083, 15400, 281, 50904], "temperature": 0.0, "avg_logprob": -0.14276390075683593, "compression_ratio": 1.7922705314009661, "no_speech_prob": 0.03358610346913338}, {"id": 152, "seek": 96896, "start": 979.76, "end": 986.4000000000001, "text": " put it the way Josh of Lockwood, it's legal because it ensures that equal objects have the", "tokens": [50904, 829, 309, 264, 636, 9785, 295, 16736, 6092, 11, 309, 311, 5089, 570, 309, 28111, 300, 2681, 6565, 362, 264, 51236], "temperature": 0.0, "avg_logprob": -0.14276390075683593, "compression_ratio": 1.7922705314009661, "no_speech_prob": 0.03358610346913338}, {"id": 153, "seek": 96896, "start": 986.4000000000001, "end": 993.76, "text": " same hash code. It's atrocious because it ensures that every object in fact has the same hash code", "tokens": [51236, 912, 22019, 3089, 13, 467, 311, 412, 340, 4139, 570, 309, 28111, 300, 633, 2657, 294, 1186, 575, 264, 912, 22019, 3089, 51604], "temperature": 0.0, "avg_logprob": -0.14276390075683593, "compression_ratio": 1.7922705314009661, "no_speech_prob": 0.03358610346913338}, {"id": 154, "seek": 99376, "start": 994.3199999999999, "end": 999.28, "text": " and that shouldn't be the case. Therefore, every object hashes to the same bucket", "tokens": [50392, 293, 300, 4659, 380, 312, 264, 1389, 13, 7504, 11, 633, 2657, 575, 8076, 281, 264, 912, 13058, 50640], "temperature": 0.0, "avg_logprob": -0.12498352357319423, "compression_ratio": 1.7213740458015268, "no_speech_prob": 0.042716190218925476}, {"id": 155, "seek": 99376, "start": 999.28, "end": 1004.08, "text": " and hash tables degenerate to link lists. This was the point I was trying to make before", "tokens": [50640, 293, 22019, 8020, 40520, 473, 281, 2113, 14511, 13, 639, 390, 264, 935, 286, 390, 1382, 281, 652, 949, 50880], "temperature": 0.0, "avg_logprob": -0.12498352357319423, "compression_ratio": 1.7213740458015268, "no_speech_prob": 0.042716190218925476}, {"id": 156, "seek": 99376, "start": 1004.72, "end": 1011.68, "text": " and I kind of confused the two. And then he said it's programs that should run in linear time", "tokens": [50912, 293, 286, 733, 295, 9019, 264, 732, 13, 400, 550, 415, 848, 309, 311, 4268, 300, 820, 1190, 294, 8213, 565, 51260], "temperature": 0.0, "avg_logprob": -0.12498352357319423, "compression_ratio": 1.7213740458015268, "no_speech_prob": 0.042716190218925476}, {"id": 157, "seek": 99376, "start": 1011.68, "end": 1016.96, "text": " instead of running quadratic time. So just to get a bit of that computer science out of the way,", "tokens": [51260, 2602, 295, 2614, 37262, 565, 13, 407, 445, 281, 483, 257, 857, 295, 300, 3820, 3497, 484, 295, 264, 636, 11, 51524], "temperature": 0.0, "avg_logprob": -0.12498352357319423, "compression_ratio": 1.7213740458015268, "no_speech_prob": 0.042716190218925476}, {"id": 158, "seek": 99376, "start": 1016.96, "end": 1021.4399999999999, "text": " I wouldn't say out of the way, it's in fact quite interesting. This is quite important to", "tokens": [51524, 286, 2759, 380, 584, 484, 295, 264, 636, 11, 309, 311, 294, 1186, 1596, 1880, 13, 639, 307, 1596, 1021, 281, 51748], "temperature": 0.0, "avg_logprob": -0.12498352357319423, "compression_ratio": 1.7213740458015268, "no_speech_prob": 0.042716190218925476}, {"id": 159, "seek": 102144, "start": 1021.44, "end": 1026.0, "text": " understand when it comes to designing algorithms, the difference between linear time and quadratic", "tokens": [50364, 1223, 562, 309, 1487, 281, 14685, 14642, 11, 264, 2649, 1296, 8213, 565, 293, 37262, 50592], "temperature": 0.0, "avg_logprob": -0.1250192960103353, "compression_ratio": 1.837837837837838, "no_speech_prob": 0.00970722921192646}, {"id": 160, "seek": 102144, "start": 1026.0, "end": 1033.2, "text": " time. In linear time, the number of steps required is directly related one to one. So big O of n.", "tokens": [50592, 565, 13, 682, 8213, 565, 11, 264, 1230, 295, 4439, 4739, 307, 3838, 4077, 472, 281, 472, 13, 407, 955, 422, 295, 297, 13, 50952], "temperature": 0.0, "avg_logprob": -0.1250192960103353, "compression_ratio": 1.837837837837838, "no_speech_prob": 0.00970722921192646}, {"id": 161, "seek": 102144, "start": 1033.2, "end": 1038.0, "text": " So it's a number of elements, let's say in a data structure. So if it's in an array,", "tokens": [50952, 407, 309, 311, 257, 1230, 295, 4959, 11, 718, 311, 584, 294, 257, 1412, 3877, 13, 407, 498, 309, 311, 294, 364, 10225, 11, 51192], "temperature": 0.0, "avg_logprob": -0.1250192960103353, "compression_ratio": 1.837837837837838, "no_speech_prob": 0.00970722921192646}, {"id": 162, "seek": 102144, "start": 1038.0, "end": 1043.3600000000001, "text": " it's the number of elements in that array. If the array is of size five, it would be big O of five", "tokens": [51192, 309, 311, 264, 1230, 295, 4959, 294, 300, 10225, 13, 759, 264, 10225, 307, 295, 2744, 1732, 11, 309, 576, 312, 955, 422, 295, 1732, 51460], "temperature": 0.0, "avg_logprob": -0.1250192960103353, "compression_ratio": 1.837837837837838, "no_speech_prob": 0.00970722921192646}, {"id": 163, "seek": 102144, "start": 1043.3600000000001, "end": 1050.64, "text": " here. In quadratic time, the number of steps is taken to accomplish a task is n squared because", "tokens": [51460, 510, 13, 682, 37262, 565, 11, 264, 1230, 295, 4439, 307, 2726, 281, 9021, 257, 5633, 307, 297, 8889, 570, 51824], "temperature": 0.0, "avg_logprob": -0.1250192960103353, "compression_ratio": 1.837837837837838, "no_speech_prob": 0.00970722921192646}, {"id": 164, "seek": 105064, "start": 1050.64, "end": 1057.1200000000001, "text": " each item has to be compared with with each other. Now, because I'm partially retarded,", "tokens": [50364, 1184, 3174, 575, 281, 312, 5347, 365, 365, 1184, 661, 13, 823, 11, 570, 286, 478, 18886, 1533, 22803, 11, 50688], "temperature": 0.0, "avg_logprob": -0.14948201860700336, "compression_ratio": 1.497175141242938, "no_speech_prob": 0.013017123565077782}, {"id": 165, "seek": 105064, "start": 1058.4, "end": 1065.92, "text": " I had to tell Judge GPT to explain this to me like I was Michael Scott from the office,", "tokens": [50752, 286, 632, 281, 980, 19476, 26039, 51, 281, 2903, 341, 281, 385, 411, 286, 390, 5116, 6659, 490, 264, 3398, 11, 51128], "temperature": 0.0, "avg_logprob": -0.14948201860700336, "compression_ratio": 1.497175141242938, "no_speech_prob": 0.013017123565077782}, {"id": 166, "seek": 105064, "start": 1065.92, "end": 1070.48, "text": " because in many ways I embody that archetype. Yes, Michael Scott is an archetype in fact.", "tokens": [51128, 570, 294, 867, 2098, 286, 42575, 300, 41852, 494, 13, 1079, 11, 5116, 6659, 307, 364, 41852, 494, 294, 1186, 13, 51356], "temperature": 0.0, "avg_logprob": -0.14948201860700336, "compression_ratio": 1.497175141242938, "no_speech_prob": 0.013017123565077782}, {"id": 167, "seek": 107048, "start": 1071.44, "end": 1080.24, "text": " And I ask from, sorry about that, a bit of background noise, let the plane pass. It's passing.", "tokens": [50412, 400, 286, 1029, 490, 11, 2597, 466, 300, 11, 257, 857, 295, 3678, 5658, 11, 718, 264, 5720, 1320, 13, 467, 311, 8437, 13, 50852], "temperature": 0.0, "avg_logprob": -0.17838942209879557, "compression_ratio": 1.4387755102040816, "no_speech_prob": 0.10969172418117523}, {"id": 168, "seek": 107048, "start": 1081.76, "end": 1089.3600000000001, "text": " And it's passed. Excellent. So I told Judge GPT, explain linear time versus quadratic time in", "tokens": [50928, 400, 309, 311, 4678, 13, 16723, 13, 407, 286, 1907, 19476, 26039, 51, 11, 2903, 8213, 565, 5717, 37262, 565, 294, 51308], "temperature": 0.0, "avg_logprob": -0.17838942209879557, "compression_ratio": 1.4387755102040816, "no_speech_prob": 0.10969172418117523}, {"id": 169, "seek": 107048, "start": 1089.3600000000001, "end": 1095.28, "text": " computing like Michael Scott from the office. And here's what Judge GPT told me. And in fact,", "tokens": [51308, 15866, 411, 5116, 6659, 490, 264, 3398, 13, 400, 510, 311, 437, 19476, 26039, 51, 1907, 385, 13, 400, 294, 1186, 11, 51604], "temperature": 0.0, "avg_logprob": -0.17838942209879557, "compression_ratio": 1.4387755102040816, "no_speech_prob": 0.10969172418117523}, {"id": 170, "seek": 109528, "start": 1095.36, "end": 1100.8, "text": " this is really good. This is really good explanation. Sure. Let's imagine you're trying to find a", "tokens": [50368, 341, 307, 534, 665, 13, 639, 307, 534, 665, 10835, 13, 4894, 13, 961, 311, 3811, 291, 434, 1382, 281, 915, 257, 50640], "temperature": 0.0, "avg_logprob": -0.13621697976039007, "compression_ratio": 1.7412280701754386, "no_speech_prob": 0.0636996254324913}, {"id": 171, "seek": 109528, "start": 1100.8, "end": 1106.8, "text": " specific paperclip, haha, office reference, because you know, it's a paper company, find a specific", "tokens": [50640, 2685, 3035, 21614, 11, 17236, 11, 3398, 6408, 11, 570, 291, 458, 11, 309, 311, 257, 3035, 2237, 11, 915, 257, 2685, 50940], "temperature": 0.0, "avg_logprob": -0.13621697976039007, "compression_ratio": 1.7412280701754386, "no_speech_prob": 0.0636996254324913}, {"id": 172, "seek": 109528, "start": 1106.8, "end": 1114.16, "text": " paperclip in a huge pile of paperclips. If you use a linear time algorithm, it's like you're checking", "tokens": [50940, 3035, 21614, 294, 257, 2603, 14375, 295, 3035, 3474, 2600, 13, 759, 291, 764, 257, 8213, 565, 9284, 11, 309, 311, 411, 291, 434, 8568, 51308], "temperature": 0.0, "avg_logprob": -0.13621697976039007, "compression_ratio": 1.7412280701754386, "no_speech_prob": 0.0636996254324913}, {"id": 173, "seek": 109528, "start": 1114.16, "end": 1121.44, "text": " each paperclip one by one in a line, which is why it's called linear, I guess, until you find the", "tokens": [51308, 1184, 3035, 21614, 472, 538, 472, 294, 257, 1622, 11, 597, 307, 983, 309, 311, 1219, 8213, 11, 286, 2041, 11, 1826, 291, 915, 264, 51672], "temperature": 0.0, "avg_logprob": -0.13621697976039007, "compression_ratio": 1.7412280701754386, "no_speech_prob": 0.0636996254324913}, {"id": 174, "seek": 112144, "start": 1121.52, "end": 1127.1200000000001, "text": " one you're looking for. It takes a little longer with more paperclips, but the time it takes to", "tokens": [50368, 472, 291, 434, 1237, 337, 13, 467, 2516, 257, 707, 2854, 365, 544, 3035, 3474, 2600, 11, 457, 264, 565, 309, 2516, 281, 50648], "temperature": 0.0, "avg_logprob": -0.12031994443951231, "compression_ratio": 1.6622222222222223, "no_speech_prob": 0.04145525395870209}, {"id": 175, "seek": 112144, "start": 1127.1200000000001, "end": 1132.72, "text": " search for the paperclips grow at a steady pace, just like a straight line. So it's,", "tokens": [50648, 3164, 337, 264, 3035, 3474, 2600, 1852, 412, 257, 13211, 11638, 11, 445, 411, 257, 2997, 1622, 13, 407, 309, 311, 11, 50928], "temperature": 0.0, "avg_logprob": -0.12031994443951231, "compression_ratio": 1.6622222222222223, "no_speech_prob": 0.04145525395870209}, {"id": 176, "seek": 112144, "start": 1133.92, "end": 1140.24, "text": " would it be called exponential? No, it's correlated to the amount of paperclips or simply put big", "tokens": [50988, 576, 309, 312, 1219, 21510, 30, 883, 11, 309, 311, 38574, 281, 264, 2372, 295, 3035, 3474, 2600, 420, 2935, 829, 955, 51304], "temperature": 0.0, "avg_logprob": -0.12031994443951231, "compression_ratio": 1.6622222222222223, "no_speech_prob": 0.04145525395870209}, {"id": 177, "seek": 112144, "start": 1140.24, "end": 1148.48, "text": " O of N. But if you use quadratic time algorithm or a quadratic time algorithm, it's like you're", "tokens": [51304, 422, 295, 426, 13, 583, 498, 291, 764, 37262, 565, 9284, 420, 257, 37262, 565, 9284, 11, 309, 311, 411, 291, 434, 51716], "temperature": 0.0, "avg_logprob": -0.12031994443951231, "compression_ratio": 1.6622222222222223, "no_speech_prob": 0.04145525395870209}, {"id": 178, "seek": 114848, "start": 1148.48, "end": 1155.44, "text": " checking each paperclip against every other paperclip in the pile over and over again,", "tokens": [50364, 8568, 1184, 3035, 21614, 1970, 633, 661, 3035, 21614, 294, 264, 14375, 670, 293, 670, 797, 11, 50712], "temperature": 0.0, "avg_logprob": -0.08271618576737137, "compression_ratio": 1.9404255319148935, "no_speech_prob": 0.0031725517474114895}, {"id": 179, "seek": 114848, "start": 1155.44, "end": 1160.48, "text": " as the number of paperclips grows. The amount of time it takes to search for one,", "tokens": [50712, 382, 264, 1230, 295, 3035, 3474, 2600, 13156, 13, 440, 2372, 295, 565, 309, 2516, 281, 3164, 337, 472, 11, 50964], "temperature": 0.0, "avg_logprob": -0.08271618576737137, "compression_ratio": 1.9404255319148935, "no_speech_prob": 0.0031725517474114895}, {"id": 180, "seek": 114848, "start": 1160.48, "end": 1165.1200000000001, "text": " you're looking for grows at an exponential rate. So sorry, just to keep in mind, quadratic time", "tokens": [50964, 291, 434, 1237, 337, 13156, 412, 364, 21510, 3314, 13, 407, 2597, 11, 445, 281, 1066, 294, 1575, 11, 37262, 565, 51196], "temperature": 0.0, "avg_logprob": -0.08271618576737137, "compression_ratio": 1.9404255319148935, "no_speech_prob": 0.0031725517474114895}, {"id": 181, "seek": 114848, "start": 1165.1200000000001, "end": 1170.88, "text": " is exponential, like a big square. So linear time is like searching through a line of paperclips,", "tokens": [51196, 307, 21510, 11, 411, 257, 955, 3732, 13, 407, 8213, 565, 307, 411, 10808, 807, 257, 1622, 295, 3035, 3474, 2600, 11, 51484], "temperature": 0.0, "avg_logprob": -0.08271618576737137, "compression_ratio": 1.9404255319148935, "no_speech_prob": 0.0031725517474114895}, {"id": 182, "seek": 114848, "start": 1170.88, "end": 1176.72, "text": " while quadratic time is like searching through a pile of paperclips. The former is faster and", "tokens": [51484, 1339, 37262, 565, 307, 411, 10808, 807, 257, 14375, 295, 3035, 3474, 2600, 13, 440, 5819, 307, 4663, 293, 51776], "temperature": 0.0, "avg_logprob": -0.08271618576737137, "compression_ratio": 1.9404255319148935, "no_speech_prob": 0.0031725517474114895}, {"id": 183, "seek": 117672, "start": 1176.72, "end": 1182.8, "text": " more efficient as the number of paperclips grows. So it's because of that reason, a good hash function,", "tokens": [50364, 544, 7148, 382, 264, 1230, 295, 3035, 3474, 2600, 13156, 13, 407, 309, 311, 570, 295, 300, 1778, 11, 257, 665, 22019, 2445, 11, 50668], "temperature": 0.0, "avg_logprob": -0.09052894426428754, "compression_ratio": 1.598360655737705, "no_speech_prob": 0.0018968879012390971}, {"id": 184, "seek": 117672, "start": 1182.8, "end": 1189.04, "text": " leaving aside the legality, so to speak, should produce distinct hash codes for unequal instances", "tokens": [50668, 5012, 7359, 264, 1676, 1860, 11, 370, 281, 1710, 11, 820, 5258, 10644, 22019, 14211, 337, 2251, 22345, 14519, 50980], "temperature": 0.0, "avg_logprob": -0.09052894426428754, "compression_ratio": 1.598360655737705, "no_speech_prob": 0.0018968879012390971}, {"id": 185, "seek": 117672, "start": 1189.04, "end": 1195.2, "text": " to prevent this, this mess of linear time and quadratic quadratic time when putting into a", "tokens": [50980, 281, 4871, 341, 11, 341, 2082, 295, 8213, 565, 293, 37262, 37262, 565, 562, 3372, 666, 257, 51288], "temperature": 0.0, "avg_logprob": -0.09052894426428754, "compression_ratio": 1.598360655737705, "no_speech_prob": 0.0018968879012390971}, {"id": 186, "seek": 117672, "start": 1195.2, "end": 1201.3600000000001, "text": " hash based data structure. Or as put in the book, this is exactly what is meant by the third part", "tokens": [51288, 22019, 2361, 1412, 3877, 13, 1610, 382, 829, 294, 264, 1446, 11, 341, 307, 2293, 437, 307, 4140, 538, 264, 2636, 644, 51596], "temperature": 0.0, "avg_logprob": -0.09052894426428754, "compression_ratio": 1.598360655737705, "no_speech_prob": 0.0018968879012390971}, {"id": 187, "seek": 120136, "start": 1201.36, "end": 1208.0, "text": " of the hash code contract. Ideally, a hash function should distribute any reasonable collection", "tokens": [50364, 295, 264, 22019, 3089, 4364, 13, 40817, 11, 257, 22019, 2445, 820, 20594, 604, 10585, 5765, 50696], "temperature": 0.0, "avg_logprob": -0.14093945073146447, "compression_ratio": 1.7568627450980392, "no_speech_prob": 0.06369832903146744}, {"id": 188, "seek": 120136, "start": 1208.0, "end": 1212.8, "text": " of unequal instances uniformly across all int values. And I kind of wanted to", "tokens": [50696, 295, 2251, 22345, 14519, 48806, 2108, 439, 560, 4190, 13, 400, 286, 733, 295, 1415, 281, 50936], "temperature": 0.0, "avg_logprob": -0.14093945073146447, "compression_ratio": 1.7568627450980392, "no_speech_prob": 0.06369832903146744}, {"id": 189, "seek": 120136, "start": 1213.52, "end": 1218.56, "text": " a more elaborated definition of this. So of course, I once again asked Judge CPT,", "tokens": [50972, 257, 544, 16298, 770, 7123, 295, 341, 13, 407, 295, 1164, 11, 286, 1564, 797, 2351, 19476, 22431, 51, 11, 51224], "temperature": 0.0, "avg_logprob": -0.14093945073146447, "compression_ratio": 1.7568627450980392, "no_speech_prob": 0.06369832903146744}, {"id": 190, "seek": 120136, "start": 1219.52, "end": 1224.7199999999998, "text": " and it said, for example, if the hash function returns integers, it should distribute the hash", "tokens": [51272, 293, 309, 848, 11, 337, 1365, 11, 498, 264, 22019, 2445, 11247, 41674, 11, 309, 820, 20594, 264, 22019, 51532], "temperature": 0.0, "avg_logprob": -0.14093945073146447, "compression_ratio": 1.7568627450980392, "no_speech_prob": 0.06369832903146744}, {"id": 191, "seek": 120136, "start": 1224.7199999999998, "end": 1230.24, "text": " values evenly across the possible integer value. So if the hash function is applied to 10 unequal", "tokens": [51532, 4190, 17658, 2108, 264, 1944, 24922, 2158, 13, 407, 498, 264, 22019, 2445, 307, 6456, 281, 1266, 2251, 22345, 51808], "temperature": 0.0, "avg_logprob": -0.14093945073146447, "compression_ratio": 1.7568627450980392, "no_speech_prob": 0.06369832903146744}, {"id": 192, "seek": 123024, "start": 1230.24, "end": 1237.52, "text": " objects, the hash values produced for those objects should be spread across all the possible", "tokens": [50364, 6565, 11, 264, 22019, 4190, 7126, 337, 729, 6565, 820, 312, 3974, 2108, 439, 264, 1944, 50728], "temperature": 0.0, "avg_logprob": -0.08955396607864735, "compression_ratio": 1.631578947368421, "no_speech_prob": 0.003649784717708826}, {"id": 193, "seek": 123024, "start": 1237.52, "end": 1245.68, "text": " integer values, not just a small range of values. This helps to ensure efficient hash based data", "tokens": [50728, 24922, 4190, 11, 406, 445, 257, 1359, 3613, 295, 4190, 13, 639, 3665, 281, 5586, 7148, 22019, 2361, 1412, 51136], "temperature": 0.0, "avg_logprob": -0.08955396607864735, "compression_ratio": 1.631578947368421, "no_speech_prob": 0.003649784717708826}, {"id": 194, "seek": 123024, "start": 1245.68, "end": 1251.92, "text": " structures, such as hash tables, where hash collision should be kept to a minimum. Now,", "tokens": [51136, 9227, 11, 1270, 382, 22019, 8020, 11, 689, 22019, 24644, 820, 312, 4305, 281, 257, 7285, 13, 823, 11, 51448], "temperature": 0.0, "avg_logprob": -0.08955396607864735, "compression_ratio": 1.631578947368421, "no_speech_prob": 0.003649784717708826}, {"id": 195, "seek": 123024, "start": 1251.92, "end": 1257.2, "text": " all of this explained here, this is, I think it kind of seems not self evident, but after this", "tokens": [51448, 439, 295, 341, 8825, 510, 11, 341, 307, 11, 286, 519, 309, 733, 295, 2544, 406, 2698, 16371, 11, 457, 934, 341, 51712], "temperature": 0.0, "avg_logprob": -0.08955396607864735, "compression_ratio": 1.631578947368421, "no_speech_prob": 0.003649784717708826}, {"id": 196, "seek": 125720, "start": 1257.2, "end": 1263.3600000000001, "text": " explanation, it seems to make sense. It's about uniformity. It should be spread across uniformly,", "tokens": [50364, 10835, 11, 309, 2544, 281, 652, 2020, 13, 467, 311, 466, 9452, 507, 13, 467, 820, 312, 3974, 2108, 48806, 11, 50672], "temperature": 0.0, "avg_logprob": -0.12547831699765963, "compression_ratio": 1.6433566433566433, "no_speech_prob": 0.009858683682978153}, {"id": 197, "seek": 125720, "start": 1263.3600000000001, "end": 1268.64, "text": " depending on the int number of values you give it. But how do we achieve this? Because that's", "tokens": [50672, 5413, 322, 264, 560, 1230, 295, 4190, 291, 976, 309, 13, 583, 577, 360, 321, 4584, 341, 30, 1436, 300, 311, 50936], "temperature": 0.0, "avg_logprob": -0.12547831699765963, "compression_ratio": 1.6433566433566433, "no_speech_prob": 0.009858683682978153}, {"id": 198, "seek": 125720, "start": 1268.64, "end": 1273.8400000000001, "text": " the ideal, and it can be a bit tricky at first glance. But fortunately, Joshua Block has stated", "tokens": [50936, 264, 7157, 11, 293, 309, 393, 312, 257, 857, 12414, 412, 700, 21094, 13, 583, 25511, 11, 24005, 17500, 575, 11323, 51196], "temperature": 0.0, "avg_logprob": -0.12547831699765963, "compression_ratio": 1.6433566433566433, "no_speech_prob": 0.009858683682978153}, {"id": 199, "seek": 125720, "start": 1275.04, "end": 1280.24, "text": " there is a recipe for a high quality hash function. So to understand this hash function,", "tokens": [51256, 456, 307, 257, 6782, 337, 257, 1090, 3125, 22019, 2445, 13, 407, 281, 1223, 341, 22019, 2445, 11, 51516], "temperature": 0.0, "avg_logprob": -0.12547831699765963, "compression_ratio": 1.6433566433566433, "no_speech_prob": 0.009858683682978153}, {"id": 200, "seek": 125720, "start": 1280.24, "end": 1287.1200000000001, "text": " firstly, let's go through the theory step by step, and then I'll jump into the demo and it'll", "tokens": [51516, 27376, 11, 718, 311, 352, 807, 264, 5261, 1823, 538, 1823, 11, 293, 550, 286, 603, 3012, 666, 264, 10723, 293, 309, 603, 51860], "temperature": 0.0, "avg_logprob": -0.12547831699765963, "compression_ratio": 1.6433566433566433, "no_speech_prob": 0.009858683682978153}, {"id": 201, "seek": 128712, "start": 1287.1999999999998, "end": 1291.76, "text": " obviously make a lot more sense when you see the actual code. Step number one is to declare an", "tokens": [50368, 2745, 652, 257, 688, 544, 2020, 562, 291, 536, 264, 3539, 3089, 13, 5470, 1230, 472, 307, 281, 19710, 364, 50596], "temperature": 0.0, "avg_logprob": -0.1083650364595301, "compression_ratio": 1.61864406779661, "no_speech_prob": 0.004399106837809086}, {"id": 202, "seek": 128712, "start": 1291.76, "end": 1299.04, "text": " int variable called result and simply assign the value of the first significant field into this", "tokens": [50596, 560, 7006, 1219, 1874, 293, 2935, 6269, 264, 2158, 295, 264, 700, 4776, 2519, 666, 341, 50960], "temperature": 0.0, "avg_logprob": -0.1083650364595301, "compression_ratio": 1.61864406779661, "no_speech_prob": 0.004399106837809086}, {"id": 203, "seek": 128712, "start": 1299.04, "end": 1304.9599999999998, "text": " result variable. And keep in mind, as Joshua Block has stated here, recall from item 10 that a", "tokens": [50960, 1874, 7006, 13, 400, 1066, 294, 1575, 11, 382, 24005, 17500, 575, 11323, 510, 11, 9901, 490, 3174, 1266, 300, 257, 51256], "temperature": 0.0, "avg_logprob": -0.1083650364595301, "compression_ratio": 1.61864406779661, "no_speech_prob": 0.004399106837809086}, {"id": 204, "seek": 128712, "start": 1304.9599999999998, "end": 1310.4799999999998, "text": " significant field is a field that affects equals comparisons. So whatever it may be, assign that", "tokens": [51256, 4776, 2519, 307, 257, 2519, 300, 11807, 6915, 33157, 13, 407, 2035, 309, 815, 312, 11, 6269, 300, 51532], "temperature": 0.0, "avg_logprob": -0.1083650364595301, "compression_ratio": 1.61864406779661, "no_speech_prob": 0.004399106837809086}, {"id": 205, "seek": 131048, "start": 1310.48, "end": 1320.96, "text": " to this variable value. Obviously, I screwed that up. I reread that a couple of times, and I", "tokens": [50364, 281, 341, 7006, 2158, 13, 7580, 11, 286, 20331, 300, 493, 13, 286, 46453, 345, 300, 257, 1916, 295, 1413, 11, 293, 286, 50888], "temperature": 0.0, "avg_logprob": -0.10687183380126954, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.003945197910070419}, {"id": 206, "seek": 131048, "start": 1320.96, "end": 1327.04, "text": " made a blunder, I made a huge error there. It's you don't assign the value of the first significant", "tokens": [50888, 1027, 257, 888, 6617, 11, 286, 1027, 257, 2603, 6713, 456, 13, 467, 311, 291, 500, 380, 6269, 264, 2158, 295, 264, 700, 4776, 51192], "temperature": 0.0, "avg_logprob": -0.10687183380126954, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.003945197910070419}, {"id": 207, "seek": 131048, "start": 1327.04, "end": 1337.52, "text": " field, you assign the hash code value of the first significant field. That is a bad mistake,", "tokens": [51192, 2519, 11, 291, 6269, 264, 22019, 3089, 2158, 295, 264, 700, 4776, 2519, 13, 663, 307, 257, 1578, 6146, 11, 51716], "temperature": 0.0, "avg_logprob": -0.10687183380126954, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.003945197910070419}, {"id": 208, "seek": 133752, "start": 1337.52, "end": 1342.8, "text": " because that can truly scrub the whole recipe. So keep in mind, you whatever the first significant", "tokens": [50364, 570, 300, 393, 4908, 24163, 264, 1379, 6782, 13, 407, 1066, 294, 1575, 11, 291, 2035, 264, 700, 4776, 50628], "temperature": 0.0, "avg_logprob": -0.13029886602045415, "compression_ratio": 1.682608695652174, "no_speech_prob": 0.04534440115094185}, {"id": 209, "seek": 133752, "start": 1342.8, "end": 1348.72, "text": " field is, you calculate the hash code and you assign that to the result here. And then step", "tokens": [50628, 2519, 307, 11, 291, 8873, 264, 22019, 3089, 293, 291, 6269, 300, 281, 264, 1874, 510, 13, 400, 550, 1823, 50924], "temperature": 0.0, "avg_logprob": -0.13029886602045415, "compression_ratio": 1.682608695652174, "no_speech_prob": 0.04534440115094185}, {"id": 210, "seek": 133752, "start": 1348.72, "end": 1356.24, "text": " number two, for every other remaining significant field F in your object, do the following compute", "tokens": [50924, 1230, 732, 11, 337, 633, 661, 8877, 4776, 2519, 479, 294, 428, 2657, 11, 360, 264, 3480, 14722, 51300], "temperature": 0.0, "avg_logprob": -0.13029886602045415, "compression_ratio": 1.682608695652174, "no_speech_prob": 0.04534440115094185}, {"id": 211, "seek": 133752, "start": 1356.24, "end": 1362.48, "text": " int the int value hash code C. So all the other fields, if the field is a primitive type, so that", "tokens": [51300, 560, 264, 560, 2158, 22019, 3089, 383, 13, 407, 439, 264, 661, 7909, 11, 498, 264, 2519, 307, 257, 28540, 2010, 11, 370, 300, 51612], "temperature": 0.0, "avg_logprob": -0.13029886602045415, "compression_ratio": 1.682608695652174, "no_speech_prob": 0.04534440115094185}, {"id": 212, "seek": 136248, "start": 1362.48, "end": 1369.76, "text": " could be an int, a char, whatever, use a box primitive and then use that and sort of like", "tokens": [50364, 727, 312, 364, 560, 11, 257, 1290, 11, 2035, 11, 764, 257, 2424, 28540, 293, 550, 764, 300, 293, 1333, 295, 411, 50728], "temperature": 0.0, "avg_logprob": -0.12262901826338335, "compression_ratio": 1.9871794871794872, "no_speech_prob": 0.02930847369134426}, {"id": 213, "seek": 136248, "start": 1369.76, "end": 1375.04, "text": " what I described here. So there's a primitive field 42 here, primitive int, use the box in", "tokens": [50728, 437, 286, 7619, 510, 13, 407, 456, 311, 257, 28540, 2519, 14034, 510, 11, 28540, 560, 11, 764, 264, 2424, 294, 50992], "temperature": 0.0, "avg_logprob": -0.12262901826338335, "compression_ratio": 1.9871794871794872, "no_speech_prob": 0.02930847369134426}, {"id": 214, "seek": 136248, "start": 1375.04, "end": 1380.88, "text": " and use value off and then use the box primitive hash code method to calculate the hash code value", "tokens": [50992, 293, 764, 2158, 766, 293, 550, 764, 264, 2424, 28540, 22019, 3089, 3170, 281, 8873, 264, 22019, 3089, 2158, 51284], "temperature": 0.0, "avg_logprob": -0.12262901826338335, "compression_ratio": 1.9871794871794872, "no_speech_prob": 0.02930847369134426}, {"id": 215, "seek": 136248, "start": 1380.88, "end": 1386.48, "text": " because the primitive type won't have a way to calculate the hash code method. So obviously,", "tokens": [51284, 570, 264, 28540, 2010, 1582, 380, 362, 257, 636, 281, 8873, 264, 22019, 3089, 3170, 13, 407, 2745, 11, 51564], "temperature": 0.0, "avg_logprob": -0.12262901826338335, "compression_ratio": 1.9871794871794872, "no_speech_prob": 0.02930847369134426}, {"id": 216, "seek": 136248, "start": 1386.48, "end": 1392.16, "text": " each primitive type in Java will have a corresponding box primitive, like here where int has", "tokens": [51564, 1184, 28540, 2010, 294, 10745, 486, 362, 257, 11760, 2424, 28540, 11, 411, 510, 689, 560, 575, 51848], "temperature": 0.0, "avg_logprob": -0.12262901826338335, "compression_ratio": 1.9871794871794872, "no_speech_prob": 0.02930847369134426}, {"id": 217, "seek": 139216, "start": 1392.16, "end": 1398.3200000000002, "text": " integer. So the second part of that is that it's important to consider how the equals method compares", "tokens": [50364, 24922, 13, 407, 264, 1150, 644, 295, 300, 307, 300, 309, 311, 1021, 281, 1949, 577, 264, 6915, 3170, 38334, 50672], "temperature": 0.0, "avg_logprob": -0.06805189539877217, "compression_ratio": 1.9597989949748744, "no_speech_prob": 0.004538040608167648}, {"id": 218, "seek": 139216, "start": 1398.3200000000002, "end": 1404.4, "text": " field values. What I mean by that is, if the equals method recursively invokes equals on the", "tokens": [50672, 2519, 4190, 13, 708, 286, 914, 538, 300, 307, 11, 498, 264, 6915, 3170, 20560, 3413, 1048, 8606, 6915, 322, 264, 50976], "temperature": 0.0, "avg_logprob": -0.06805189539877217, "compression_ratio": 1.9597989949748744, "no_speech_prob": 0.004538040608167648}, {"id": 219, "seek": 139216, "start": 1404.4, "end": 1410.88, "text": " object reference fields, then the hash code method should recursively invoke hash code on those fields.", "tokens": [50976, 2657, 6408, 7909, 11, 550, 264, 22019, 3089, 3170, 820, 20560, 3413, 41117, 22019, 3089, 322, 729, 7909, 13, 51300], "temperature": 0.0, "avg_logprob": -0.06805189539877217, "compression_ratio": 1.9597989949748744, "no_speech_prob": 0.004538040608167648}, {"id": 220, "seek": 139216, "start": 1410.88, "end": 1420.24, "text": " So in an object, whatever the fields that the equals method invokes equals to the hash code", "tokens": [51300, 407, 294, 364, 2657, 11, 2035, 264, 7909, 300, 264, 6915, 3170, 1048, 8606, 6915, 281, 264, 22019, 3089, 51768], "temperature": 0.0, "avg_logprob": -0.06805189539877217, "compression_ratio": 1.9597989949748744, "no_speech_prob": 0.004538040608167648}, {"id": 221, "seek": 142024, "start": 1420.24, "end": 1425.36, "text": " method should do the same. I will put this way, what I realized when I was going through this", "tokens": [50364, 3170, 820, 360, 264, 912, 13, 286, 486, 829, 341, 636, 11, 437, 286, 5334, 562, 286, 390, 516, 807, 341, 50620], "temperature": 0.0, "avg_logprob": -0.16202279572845787, "compression_ratio": 1.9396984924623115, "no_speech_prob": 0.015904288738965988}, {"id": 222, "seek": 142024, "start": 1426.48, "end": 1433.44, "text": " item, or more specifically, this recipe for the hash function is that a lot of the things that the", "tokens": [50676, 3174, 11, 420, 544, 4682, 11, 341, 6782, 337, 264, 22019, 2445, 307, 300, 257, 688, 295, 264, 721, 300, 264, 51024], "temperature": 0.0, "avg_logprob": -0.16202279572845787, "compression_ratio": 1.9396984924623115, "no_speech_prob": 0.015904288738965988}, {"id": 223, "seek": 142024, "start": 1433.44, "end": 1439.2, "text": " equals method does, the hash code method should do too. A lot of the patterns that it follows,", "tokens": [51024, 6915, 3170, 775, 11, 264, 22019, 3089, 3170, 820, 360, 886, 13, 316, 688, 295, 264, 8294, 300, 309, 10002, 11, 51312], "temperature": 0.0, "avg_logprob": -0.16202279572845787, "compression_ratio": 1.9396984924623115, "no_speech_prob": 0.015904288738965988}, {"id": 224, "seek": 142024, "start": 1440.16, "end": 1445.44, "text": " that the equals method does or follows, the hash code method should follow too. So the second part", "tokens": [51360, 300, 264, 6915, 3170, 775, 420, 10002, 11, 264, 22019, 3089, 3170, 820, 1524, 886, 13, 407, 264, 1150, 644, 51624], "temperature": 0.0, "avg_logprob": -0.16202279572845787, "compression_ratio": 1.9396984924623115, "no_speech_prob": 0.015904288738965988}, {"id": 225, "seek": 144544, "start": 1445.44, "end": 1451.2, "text": " of that is, if the equals method requires a more complex comparison for the object reference field,", "tokens": [50364, 295, 300, 307, 11, 498, 264, 6915, 3170, 7029, 257, 544, 3997, 9660, 337, 264, 2657, 6408, 2519, 11, 50652], "temperature": 0.0, "avg_logprob": -0.08279872508276076, "compression_ratio": 1.794392523364486, "no_speech_prob": 0.01941622793674469}, {"id": 226, "seek": 144544, "start": 1451.2, "end": 1457.76, "text": " the hash code method should compute a canonical representation for the field and invoke hash", "tokens": [50652, 264, 22019, 3089, 3170, 820, 14722, 257, 46491, 10290, 337, 264, 2519, 293, 41117, 22019, 50980], "temperature": 0.0, "avg_logprob": -0.08279872508276076, "compression_ratio": 1.794392523364486, "no_speech_prob": 0.01941622793674469}, {"id": 227, "seek": 144544, "start": 1457.76, "end": 1463.52, "text": " code on that representation. So what does this mean? We discussed canonical representation in item 10.", "tokens": [50980, 3089, 322, 300, 10290, 13, 407, 437, 775, 341, 914, 30, 492, 7152, 46491, 10290, 294, 3174, 1266, 13, 51268], "temperature": 0.0, "avg_logprob": -0.08279872508276076, "compression_ratio": 1.794392523364486, "no_speech_prob": 0.01941622793674469}, {"id": 228, "seek": 144544, "start": 1464.48, "end": 1471.76, "text": " As the name suggests, it's a canonical value. So if there's a certain field value in the", "tokens": [51316, 1018, 264, 1315, 13409, 11, 309, 311, 257, 46491, 2158, 13, 407, 498, 456, 311, 257, 1629, 2519, 2158, 294, 264, 51680], "temperature": 0.0, "avg_logprob": -0.08279872508276076, "compression_ratio": 1.794392523364486, "no_speech_prob": 0.01941622793674469}, {"id": 229, "seek": 147176, "start": 1471.76, "end": 1479.52, "text": " object that's pro to change, that is rather dynamic, for the sake of comparison, or in this", "tokens": [50364, 2657, 300, 311, 447, 281, 1319, 11, 300, 307, 2831, 8546, 11, 337, 264, 9717, 295, 9660, 11, 420, 294, 341, 50752], "temperature": 0.0, "avg_logprob": -0.11064602672189906, "compression_ratio": 1.6363636363636365, "no_speech_prob": 0.0028444777708500624}, {"id": 230, "seek": 147176, "start": 1479.52, "end": 1489.84, "text": " case, for the sake of calculating the hash code, we could assign a sort of static variable value", "tokens": [50752, 1389, 11, 337, 264, 9717, 295, 28258, 264, 22019, 3089, 11, 321, 727, 6269, 257, 1333, 295, 13437, 7006, 2158, 51268], "temperature": 0.0, "avg_logprob": -0.11064602672189906, "compression_ratio": 1.6363636363636365, "no_speech_prob": 0.0028444777708500624}, {"id": 231, "seek": 147176, "start": 1489.84, "end": 1497.12, "text": " that we consider to be the canonical representation of that field. And that can be used for all the", "tokens": [51268, 300, 321, 1949, 281, 312, 264, 46491, 10290, 295, 300, 2519, 13, 400, 300, 393, 312, 1143, 337, 439, 264, 51632], "temperature": 0.0, "avg_logprob": -0.11064602672189906, "compression_ratio": 1.6363636363636365, "no_speech_prob": 0.0028444777708500624}, {"id": 232, "seek": 149712, "start": 1497.1999999999998, "end": 1501.6, "text": " computation and all the calculations of the equals method calculations, but also in the", "tokens": [50368, 24903, 293, 439, 264, 20448, 295, 264, 6915, 3170, 20448, 11, 457, 611, 294, 264, 50588], "temperature": 0.0, "avg_logprob": -0.10454342478797549, "compression_ratio": 1.6711111111111112, "no_speech_prob": 0.0034829899668693542}, {"id": 233, "seek": 149712, "start": 1501.6, "end": 1512.4799999999998, "text": " hash code method. That makes it gives our methods a bit more structure for object fields that have", "tokens": [50588, 22019, 3089, 3170, 13, 663, 1669, 309, 2709, 527, 7150, 257, 857, 544, 3877, 337, 2657, 7909, 300, 362, 51132], "temperature": 0.0, "avg_logprob": -0.10454342478797549, "compression_ratio": 1.6711111111111112, "no_speech_prob": 0.0034829899668693542}, {"id": 234, "seek": 149712, "start": 1512.4799999999998, "end": 1517.76, "text": " rather dynamic and volatile, and I don't mean volatile in the Java sense, I mean volatile just", "tokens": [51132, 2831, 8546, 293, 34377, 11, 293, 286, 500, 380, 914, 34377, 294, 264, 10745, 2020, 11, 286, 914, 34377, 445, 51396], "temperature": 0.0, "avg_logprob": -0.10454342478797549, "compression_ratio": 1.6711111111111112, "no_speech_prob": 0.0034829899668693542}, {"id": 235, "seek": 149712, "start": 1517.76, "end": 1524.3999999999999, "text": " conceptually speaking, significant fields in an object. And the third part is, if the value of", "tokens": [51396, 3410, 671, 4124, 11, 4776, 7909, 294, 364, 2657, 13, 400, 264, 2636, 644, 307, 11, 498, 264, 2158, 295, 51728], "temperature": 0.0, "avg_logprob": -0.10454342478797549, "compression_ratio": 1.6711111111111112, "no_speech_prob": 0.0034829899668693542}, {"id": 236, "seek": 152440, "start": 1524.4, "end": 1529.2800000000002, "text": " the field is null, obviously, the hash code method should use a constant value such as zero", "tokens": [50364, 264, 2519, 307, 18184, 11, 2745, 11, 264, 22019, 3089, 3170, 820, 764, 257, 5754, 2158, 1270, 382, 4018, 50608], "temperature": 0.0, "avg_logprob": -0.10709580277974627, "compression_ratio": 1.7472118959107807, "no_speech_prob": 0.0040069520473480225}, {"id": 237, "seek": 152440, "start": 1529.2800000000002, "end": 1534.5600000000002, "text": " to represent the hash code for the field. That seems pretty self-explanatory because given that", "tokens": [50608, 281, 2906, 264, 22019, 3089, 337, 264, 2519, 13, 663, 2544, 1238, 2698, 12, 3121, 16554, 4745, 570, 2212, 300, 50872], "temperature": 0.0, "avg_logprob": -0.10709580277974627, "compression_ratio": 1.7472118959107807, "no_speech_prob": 0.0040069520473480225}, {"id": 238, "seek": 152440, "start": 1534.5600000000002, "end": 1540.88, "text": " this result variable that we're using is an integer is of type int, we need something to", "tokens": [50872, 341, 1874, 7006, 300, 321, 434, 1228, 307, 364, 24922, 307, 295, 2010, 560, 11, 321, 643, 746, 281, 51188], "temperature": 0.0, "avg_logprob": -0.10709580277974627, "compression_ratio": 1.7472118959107807, "no_speech_prob": 0.0040069520473480225}, {"id": 239, "seek": 152440, "start": 1540.88, "end": 1547.92, "text": " correspond to a null where zero probably would be the apt value to use. And if the field is an array,", "tokens": [51188, 6805, 281, 257, 18184, 689, 4018, 1391, 576, 312, 264, 29427, 2158, 281, 764, 13, 400, 498, 264, 2519, 307, 364, 10225, 11, 51540], "temperature": 0.0, "avg_logprob": -0.10709580277974627, "compression_ratio": 1.7472118959107807, "no_speech_prob": 0.0040069520473480225}, {"id": 240, "seek": 152440, "start": 1547.92, "end": 1552.4, "text": " with all fields of the array being significant, use arrays or hash code. So we can use this", "tokens": [51540, 365, 439, 7909, 295, 264, 10225, 885, 4776, 11, 764, 41011, 420, 22019, 3089, 13, 407, 321, 393, 764, 341, 51764], "temperature": 0.0, "avg_logprob": -0.10709580277974627, "compression_ratio": 1.7472118959107807, "no_speech_prob": 0.0040069520473480225}, {"id": 241, "seek": 155240, "start": 1553.0400000000002, "end": 1559.44, "text": " method from the arrays class in the library. Or as Joshua Booker said it here, if the field is an", "tokens": [50396, 3170, 490, 264, 41011, 1508, 294, 264, 6405, 13, 1610, 382, 24005, 9476, 260, 848, 309, 510, 11, 498, 264, 2519, 307, 364, 50716], "temperature": 0.0, "avg_logprob": -0.11566990072076971, "compression_ratio": 1.6255707762557077, "no_speech_prob": 0.005554459057748318}, {"id": 242, "seek": 155240, "start": 1559.44, "end": 1566.24, "text": " array, compute a hash code for each significant element by applying these rules recursively", "tokens": [50716, 10225, 11, 14722, 257, 22019, 3089, 337, 1184, 4776, 4478, 538, 9275, 613, 4474, 20560, 3413, 51056], "temperature": 0.0, "avg_logprob": -0.11566990072076971, "compression_ratio": 1.6255707762557077, "no_speech_prob": 0.005554459057748318}, {"id": 243, "seek": 155240, "start": 1566.24, "end": 1572.48, "text": " and combine the values per step to be. So per step to be is the next step.", "tokens": [51056, 293, 10432, 264, 4190, 680, 1823, 281, 312, 13, 407, 680, 1823, 281, 312, 307, 264, 958, 1823, 13, 51368], "temperature": 0.0, "avg_logprob": -0.11566990072076971, "compression_ratio": 1.6255707762557077, "no_speech_prob": 0.005554459057748318}, {"id": 244, "seek": 155240, "start": 1573.8400000000001, "end": 1579.52, "text": " We'll get to that too. If the array has no significant elements, use a constant, preferably", "tokens": [51436, 492, 603, 483, 281, 300, 886, 13, 759, 264, 10225, 575, 572, 4776, 4959, 11, 764, 257, 5754, 11, 45916, 51720], "temperature": 0.0, "avg_logprob": -0.11566990072076971, "compression_ratio": 1.6255707762557077, "no_speech_prob": 0.005554459057748318}, {"id": 245, "seek": 157952, "start": 1579.52, "end": 1584.16, "text": " not zero. That makes sense because zero would normally be used for something like a null.", "tokens": [50364, 406, 4018, 13, 663, 1669, 2020, 570, 4018, 576, 5646, 312, 1143, 337, 746, 411, 257, 18184, 13, 50596], "temperature": 0.0, "avg_logprob": -0.09399674302440579, "compression_ratio": 1.7279411764705883, "no_speech_prob": 0.016402103006839752}, {"id": 246, "seek": 157952, "start": 1584.16, "end": 1587.92, "text": " We don't want to have that kind of conflict or confusion. And then step two be, of course, is", "tokens": [50596, 492, 500, 380, 528, 281, 362, 300, 733, 295, 6596, 420, 15075, 13, 400, 550, 1823, 732, 312, 11, 295, 1164, 11, 307, 50784], "temperature": 0.0, "avg_logprob": -0.09399674302440579, "compression_ratio": 1.7279411764705883, "no_speech_prob": 0.016402103006839752}, {"id": 247, "seek": 157952, "start": 1588.48, "end": 1594.56, "text": " to, all the values that we used here, that we computed here, sorry, combine them to get the", "tokens": [50812, 281, 11, 439, 264, 4190, 300, 321, 1143, 510, 11, 300, 321, 40610, 510, 11, 2597, 11, 10432, 552, 281, 483, 264, 51116], "temperature": 0.0, "avg_logprob": -0.09399674302440579, "compression_ratio": 1.7279411764705883, "no_speech_prob": 0.016402103006839752}, {"id": 248, "seek": 157952, "start": 1594.56, "end": 1603.6, "text": " hash code where we have the result value multiplied by 31 plus C. And what's C? It is the computed", "tokens": [51116, 22019, 3089, 689, 321, 362, 264, 1874, 2158, 17207, 538, 10353, 1804, 383, 13, 400, 437, 311, 383, 30, 467, 307, 264, 40610, 51568], "temperature": 0.0, "avg_logprob": -0.09399674302440579, "compression_ratio": 1.7279411764705883, "no_speech_prob": 0.016402103006839752}, {"id": 249, "seek": 157952, "start": 1603.6, "end": 1608.32, "text": " hash code value. And then you return result in the hash code function. This will all make sense", "tokens": [51568, 22019, 3089, 2158, 13, 400, 550, 291, 2736, 1874, 294, 264, 22019, 3089, 2445, 13, 639, 486, 439, 652, 2020, 51804], "temperature": 0.0, "avg_logprob": -0.09399674302440579, "compression_ratio": 1.7279411764705883, "no_speech_prob": 0.016402103006839752}, {"id": 250, "seek": 160832, "start": 1608.32, "end": 1614.1599999999999, "text": " once we look at the demo. In this demo, I've got a class called person. And this person class", "tokens": [50364, 1564, 321, 574, 412, 264, 10723, 13, 682, 341, 10723, 11, 286, 600, 658, 257, 1508, 1219, 954, 13, 400, 341, 954, 1508, 50656], "temperature": 0.0, "avg_logprob": -0.10286512278547191, "compression_ratio": 1.8095238095238095, "no_speech_prob": 0.013635504059493542}, {"id": 251, "seek": 160832, "start": 1614.1599999999999, "end": 1621.9199999999998, "text": " has some significant fields, their first name, type string, last name, type string, and address.", "tokens": [50656, 575, 512, 4776, 7909, 11, 641, 700, 1315, 11, 2010, 6798, 11, 1036, 1315, 11, 2010, 6798, 11, 293, 2985, 13, 51044], "temperature": 0.0, "avg_logprob": -0.10286512278547191, "compression_ratio": 1.8095238095238095, "no_speech_prob": 0.013635504059493542}, {"id": 252, "seek": 160832, "start": 1621.9199999999998, "end": 1628.6399999999999, "text": " And the address is a class that I've defined in this file, in fact. And the address essentially", "tokens": [51044, 400, 264, 2985, 307, 257, 1508, 300, 286, 600, 7642, 294, 341, 3991, 11, 294, 1186, 13, 400, 264, 2985, 4476, 51380], "temperature": 0.0, "avg_logprob": -0.10286512278547191, "compression_ratio": 1.8095238095238095, "no_speech_prob": 0.013635504059493542}, {"id": 253, "seek": 160832, "start": 1628.6399999999999, "end": 1635.84, "text": " has a street, city, state, and zip code. So the addresses of address, as you can see clearly.", "tokens": [51380, 575, 257, 4838, 11, 2307, 11, 1785, 11, 293, 20730, 3089, 13, 407, 264, 16862, 295, 2985, 11, 382, 291, 393, 536, 4448, 13, 51740], "temperature": 0.0, "avg_logprob": -0.10286512278547191, "compression_ratio": 1.8095238095238095, "no_speech_prob": 0.013635504059493542}, {"id": 254, "seek": 163584, "start": 1635.84, "end": 1643.76, "text": " And then we've got the age, which is of primitive type int. And also we have a string array,", "tokens": [50364, 400, 550, 321, 600, 658, 264, 3205, 11, 597, 307, 295, 28540, 2010, 560, 13, 400, 611, 321, 362, 257, 6798, 10225, 11, 50760], "temperature": 0.0, "avg_logprob": -0.14785751700401306, "compression_ratio": 1.6726457399103138, "no_speech_prob": 0.010488069616258144}, {"id": 255, "seek": 163584, "start": 1645.4399999999998, "end": 1650.72, "text": " which I've called language is spoken. So the language this person speaks. And then we've", "tokens": [50844, 597, 286, 600, 1219, 2856, 307, 10759, 13, 407, 264, 2856, 341, 954, 10789, 13, 400, 550, 321, 600, 51108], "temperature": 0.0, "avg_logprob": -0.14785751700401306, "compression_ratio": 1.6726457399103138, "no_speech_prob": 0.010488069616258144}, {"id": 256, "seek": 163584, "start": 1650.72, "end": 1655.76, "text": " got the constructor and the equals method also I've overridden. I'm not going to go through that", "tokens": [51108, 658, 264, 47479, 293, 264, 6915, 3170, 611, 286, 600, 670, 81, 6171, 13, 286, 478, 406, 516, 281, 352, 807, 300, 51360], "temperature": 0.0, "avg_logprob": -0.14785751700401306, "compression_ratio": 1.6726457399103138, "no_speech_prob": 0.010488069616258144}, {"id": 257, "seek": 163584, "start": 1655.76, "end": 1659.52, "text": " because it's kind of on a scope. And I kind of went through this already in the previous item.", "tokens": [51360, 570, 309, 311, 733, 295, 322, 257, 11923, 13, 400, 286, 733, 295, 1437, 807, 341, 1217, 294, 264, 3894, 3174, 13, 51548], "temperature": 0.0, "avg_logprob": -0.14785751700401306, "compression_ratio": 1.6726457399103138, "no_speech_prob": 0.010488069616258144}, {"id": 258, "seek": 165952, "start": 1660.08, "end": 1668.24, "text": " And then we've overridden the hash code. So this hash code theoretically should be a high quality", "tokens": [50392, 400, 550, 321, 600, 670, 81, 6171, 264, 22019, 3089, 13, 407, 341, 22019, 3089, 29400, 820, 312, 257, 1090, 3125, 50800], "temperature": 0.0, "avg_logprob": -0.18584012985229492, "compression_ratio": 1.5381165919282511, "no_speech_prob": 0.03846275806427002}, {"id": 259, "seek": 165952, "start": 1669.04, "end": 1672.32, "text": " hash function or a hash code method, because I've followed the recipe", "tokens": [50840, 22019, 2445, 420, 257, 22019, 3089, 3170, 11, 570, 286, 600, 6263, 264, 6782, 51004], "temperature": 0.0, "avg_logprob": -0.18584012985229492, "compression_ratio": 1.5381165919282511, "no_speech_prob": 0.03846275806427002}, {"id": 260, "seek": 165952, "start": 1672.96, "end": 1681.52, "text": " delineated in just a blog's book or in effective Java. Initially, the result as we saw in step one", "tokens": [51036, 1103, 533, 770, 294, 445, 257, 6968, 311, 1446, 420, 294, 4942, 10745, 13, 29446, 11, 264, 1874, 382, 321, 1866, 294, 1823, 472, 51464], "temperature": 0.0, "avg_logprob": -0.18584012985229492, "compression_ratio": 1.5381165919282511, "no_speech_prob": 0.03846275806427002}, {"id": 261, "seek": 165952, "start": 1681.52, "end": 1686.24, "text": " has been arbitrary number 17 has been picked as a constant. This will reduce", "tokens": [51464, 575, 668, 23211, 1230, 3282, 575, 668, 6183, 382, 257, 5754, 13, 639, 486, 5407, 51700], "temperature": 0.0, "avg_logprob": -0.18584012985229492, "compression_ratio": 1.5381165919282511, "no_speech_prob": 0.03846275806427002}, {"id": 262, "seek": 168624, "start": 1687.04, "end": 1692.72, "text": " collisions. Keep in mind, don't pick a number like zero, start off with something like this.", "tokens": [50404, 46537, 13, 5527, 294, 1575, 11, 500, 380, 1888, 257, 1230, 411, 4018, 11, 722, 766, 365, 746, 411, 341, 13, 50688], "temperature": 0.0, "avg_logprob": -0.1391221089149589, "compression_ratio": 1.5303867403314917, "no_speech_prob": 0.014501931145787239}, {"id": 263, "seek": 168624, "start": 1692.72, "end": 1701.28, "text": " And then we've used the significant fields to calculate the result using the hash codes and", "tokens": [50688, 400, 550, 321, 600, 1143, 264, 4776, 7909, 281, 8873, 264, 1874, 1228, 264, 22019, 14211, 293, 51116], "temperature": 0.0, "avg_logprob": -0.1391221089149589, "compression_ratio": 1.5303867403314917, "no_speech_prob": 0.014501931145787239}, {"id": 264, "seek": 168624, "start": 1701.28, "end": 1706.8, "text": " then adding that to the result value. And again, multiplying by 31, this again is to give it", "tokens": [51116, 550, 5127, 300, 281, 264, 1874, 2158, 13, 400, 797, 11, 30955, 538, 10353, 11, 341, 797, 307, 281, 976, 309, 51392], "temperature": 0.0, "avg_logprob": -0.1391221089149589, "compression_ratio": 1.5303867403314917, "no_speech_prob": 0.014501931145787239}, {"id": 265, "seek": 170680, "start": 1707.44, "end": 1713.04, "text": " give it more uniqueness or more more distinctiveness and preventing collisions.", "tokens": [50396, 976, 309, 544, 48294, 420, 544, 544, 10644, 8477, 293, 19965, 46537, 13, 50676], "temperature": 0.0, "avg_logprob": -0.137874796149436, "compression_ratio": 1.7121951219512195, "no_speech_prob": 0.022283628582954407}, {"id": 266, "seek": 170680, "start": 1714.1599999999999, "end": 1720.72, "text": " And then in the address, we've done it a bit differently where we're in fact looking for", "tokens": [50732, 400, 550, 294, 264, 2985, 11, 321, 600, 1096, 309, 257, 857, 7614, 689, 321, 434, 294, 1186, 1237, 337, 51060], "temperature": 0.0, "avg_logprob": -0.137874796149436, "compression_ratio": 1.7121951219512195, "no_speech_prob": 0.022283628582954407}, {"id": 267, "seek": 170680, "start": 1720.72, "end": 1727.6, "text": " the hash value of the address object. And if it has been cached, or pardon me, if it's null,", "tokens": [51060, 264, 22019, 2158, 295, 264, 2985, 2657, 13, 400, 498, 309, 575, 668, 269, 15095, 11, 420, 22440, 385, 11, 498, 309, 311, 18184, 11, 51404], "temperature": 0.0, "avg_logprob": -0.137874796149436, "compression_ratio": 1.7121951219512195, "no_speech_prob": 0.022283628582954407}, {"id": 268, "seek": 170680, "start": 1728.1599999999999, "end": 1734.48, "text": " then we get economical address value. And if it is null, then we're going to return zero.", "tokens": [51432, 550, 321, 483, 42473, 2985, 2158, 13, 400, 498, 309, 307, 18184, 11, 550, 321, 434, 516, 281, 2736, 4018, 13, 51748], "temperature": 0.0, "avg_logprob": -0.137874796149436, "compression_ratio": 1.7121951219512195, "no_speech_prob": 0.022283628582954407}, {"id": 269, "seek": 173448, "start": 1735.1200000000001, "end": 1739.68, "text": " So we're going to want to make sure that the address is not null and that this person has", "tokens": [50396, 407, 321, 434, 516, 281, 528, 281, 652, 988, 300, 264, 2985, 307, 406, 18184, 293, 300, 341, 954, 575, 50624], "temperature": 0.0, "avg_logprob": -0.10831885867648655, "compression_ratio": 1.7616822429906542, "no_speech_prob": 0.0018385463627055287}, {"id": 270, "seek": 173448, "start": 1739.68, "end": 1747.44, "text": " an object. And so we get canonical, the canonical address, which by the way, this method is defined", "tokens": [50624, 364, 2657, 13, 400, 370, 321, 483, 46491, 11, 264, 46491, 2985, 11, 597, 538, 264, 636, 11, 341, 3170, 307, 7642, 51012], "temperature": 0.0, "avg_logprob": -0.10831885867648655, "compression_ratio": 1.7616822429906542, "no_speech_prob": 0.0018385463627055287}, {"id": 271, "seek": 173448, "start": 1747.44, "end": 1755.76, "text": " in the address class. So back to this. And then we are also given that the age is of type int,", "tokens": [51012, 294, 264, 2985, 1508, 13, 407, 646, 281, 341, 13, 400, 550, 321, 366, 611, 2212, 300, 264, 3205, 307, 295, 2010, 560, 11, 51428], "temperature": 0.0, "avg_logprob": -0.10831885867648655, "compression_ratio": 1.7616822429906542, "no_speech_prob": 0.0018385463627055287}, {"id": 272, "seek": 173448, "start": 1755.76, "end": 1762.56, "text": " it's a primitive type, we're using the box primitives. Sorry, yeah, box primitives and using", "tokens": [51428, 309, 311, 257, 28540, 2010, 11, 321, 434, 1228, 264, 2424, 2886, 38970, 13, 4919, 11, 1338, 11, 2424, 2886, 38970, 293, 1228, 51768], "temperature": 0.0, "avg_logprob": -0.10831885867648655, "compression_ratio": 1.7616822429906542, "no_speech_prob": 0.0018385463627055287}, {"id": 273, "seek": 176256, "start": 1762.56, "end": 1768.08, "text": " the value often getting the hash code. And then for languages spoken, because the string array,", "tokens": [50364, 264, 2158, 2049, 1242, 264, 22019, 3089, 13, 400, 550, 337, 8650, 10759, 11, 570, 264, 6798, 10225, 11, 50640], "temperature": 0.0, "avg_logprob": -0.2421492887346932, "compression_ratio": 1.6126126126126126, "no_speech_prob": 0.028856154531240463}, {"id": 274, "seek": 176256, "start": 1768.08, "end": 1775.36, "text": " we're using arrays or hash code, again, as stated in the hash function recipe,", "tokens": [50640, 321, 434, 1228, 41011, 420, 22019, 3089, 11, 797, 11, 382, 11323, 294, 264, 22019, 2445, 6782, 11, 51004], "temperature": 0.0, "avg_logprob": -0.2421492887346932, "compression_ratio": 1.6126126126126126, "no_speech_prob": 0.028856154531240463}, {"id": 275, "seek": 176256, "start": 1775.36, "end": 1781.76, "text": " and then simply we return that result. So here's an example, the client using that class,", "tokens": [51004, 293, 550, 2935, 321, 2736, 300, 1874, 13, 407, 510, 311, 364, 1365, 11, 264, 6423, 1228, 300, 1508, 11, 51324], "temperature": 0.0, "avg_logprob": -0.2421492887346932, "compression_ratio": 1.6126126126126126, "no_speech_prob": 0.028856154531240463}, {"id": 276, "seek": 176256, "start": 1782.3999999999999, "end": 1788.24, "text": " we use Alha Camus, my one of my favorite essentialist authors, highly recommend you read him,", "tokens": [51356, 321, 764, 967, 1641, 6886, 301, 11, 452, 472, 295, 452, 2954, 7115, 468, 16552, 11, 5405, 2748, 291, 1401, 796, 11, 51648], "temperature": 0.0, "avg_logprob": -0.2421492887346932, "compression_ratio": 1.6126126126126126, "no_speech_prob": 0.028856154531240463}, {"id": 277, "seek": 178824, "start": 1788.24, "end": 1792.4, "text": " start off probably with the stranger. And then if you're more philosophically inclined,", "tokens": [50364, 722, 766, 1391, 365, 264, 18834, 13, 400, 550, 498, 291, 434, 544, 14529, 984, 28173, 11, 50572], "temperature": 0.0, "avg_logprob": -0.13525707995305297, "compression_ratio": 1.6534296028880866, "no_speech_prob": 0.09798725694417953}, {"id": 278, "seek": 178824, "start": 1792.4, "end": 1800.16, "text": " perhaps the myth of Sisyphus, I love the stranger so much that I, in fact, I'm trying to learn French,", "tokens": [50572, 4317, 264, 9474, 295, 318, 14169, 950, 301, 11, 286, 959, 264, 18834, 370, 709, 300, 286, 11, 294, 1186, 11, 286, 478, 1382, 281, 1466, 5522, 11, 50960], "temperature": 0.0, "avg_logprob": -0.13525707995305297, "compression_ratio": 1.6534296028880866, "no_speech_prob": 0.09798725694417953}, {"id": 279, "seek": 178824, "start": 1800.16, "end": 1806.24, "text": " just because of Camus. It's a beautiful book. So the languages Camus speaks, I found out,", "tokens": [50960, 445, 570, 295, 6886, 301, 13, 467, 311, 257, 2238, 1446, 13, 407, 264, 8650, 6886, 301, 10789, 11, 286, 1352, 484, 11, 51264], "temperature": 0.0, "avg_logprob": -0.13525707995305297, "compression_ratio": 1.6534296028880866, "no_speech_prob": 0.09798725694417953}, {"id": 280, "seek": 178824, "start": 1806.24, "end": 1811.1200000000001, "text": " in fact, he speaks Arabic too. So it's English, French and Arabic. And then", "tokens": [51264, 294, 1186, 11, 415, 10789, 19938, 886, 13, 407, 309, 311, 3669, 11, 5522, 293, 19938, 13, 400, 550, 51508], "temperature": 0.0, "avg_logprob": -0.13525707995305297, "compression_ratio": 1.6534296028880866, "no_speech_prob": 0.09798725694417953}, {"id": 281, "seek": 178824, "start": 1812.16, "end": 1816.08, "text": " they're giving his address, I just randomly found this online, I'm not even sure if this is accurate,", "tokens": [51560, 436, 434, 2902, 702, 2985, 11, 286, 445, 16979, 1352, 341, 2950, 11, 286, 478, 406, 754, 988, 498, 341, 307, 8559, 11, 51756], "temperature": 0.0, "avg_logprob": -0.13525707995305297, "compression_ratio": 1.6534296028880866, "no_speech_prob": 0.09798725694417953}, {"id": 282, "seek": 181608, "start": 1816.08, "end": 1821.84, "text": " but this is probably where he lived. And unfortunately, he only lived to an age of 46 years", "tokens": [50364, 457, 341, 307, 1391, 689, 415, 5152, 13, 400, 7015, 11, 415, 787, 5152, 281, 364, 3205, 295, 17835, 924, 50652], "temperature": 0.0, "avg_logprob": -0.1716216232465661, "compression_ratio": 1.59009009009009, "no_speech_prob": 0.015902457758784294}, {"id": 283, "seek": 181608, "start": 1821.84, "end": 1826.8799999999999, "text": " because he died in a car crash, which is quite a tragedy, given I would have loved to", "tokens": [50652, 570, 415, 4539, 294, 257, 1032, 8252, 11, 597, 307, 1596, 257, 18563, 11, 2212, 286, 576, 362, 4333, 281, 50904], "temperature": 0.0, "avg_logprob": -0.1716216232465661, "compression_ratio": 1.59009009009009, "no_speech_prob": 0.015902457758784294}, {"id": 284, "seek": 181608, "start": 1828.08, "end": 1833.52, "text": " read more of his work if he did live up to old age. And then when you run this function,", "tokens": [50964, 1401, 544, 295, 702, 589, 498, 415, 630, 1621, 493, 281, 1331, 3205, 13, 400, 550, 562, 291, 1190, 341, 2445, 11, 51236], "temperature": 0.0, "avg_logprob": -0.1716216232465661, "compression_ratio": 1.59009009009009, "no_speech_prob": 0.015902457758784294}, {"id": 285, "seek": 181608, "start": 1833.52, "end": 1839.52, "text": " it calculates the hash code for the Albert Camus, Albert Camus object. And then Joshua", "tokens": [51236, 309, 4322, 1024, 264, 22019, 3089, 337, 264, 20812, 6886, 301, 11, 20812, 6886, 301, 2657, 13, 400, 550, 24005, 51536], "temperature": 0.0, "avg_logprob": -0.1716216232465661, "compression_ratio": 1.59009009009009, "no_speech_prob": 0.015902457758784294}, {"id": 286, "seek": 183952, "start": 1839.52, "end": 1845.52, "text": " Book states, when you are finished writing the hash code method, ask yourself whether equal", "tokens": [50364, 9476, 4368, 11, 562, 291, 366, 4335, 3579, 264, 22019, 3089, 3170, 11, 1029, 1803, 1968, 2681, 50664], "temperature": 0.0, "avg_logprob": -0.11966787685047496, "compression_ratio": 1.6696428571428572, "no_speech_prob": 0.31393277645111084}, {"id": 287, "seek": 183952, "start": 1845.52, "end": 1853.68, "text": " instances have equal hash codes, and many states do use unit tests and whatnot to figure that out.", "tokens": [50664, 14519, 362, 2681, 22019, 14211, 11, 293, 867, 4368, 360, 764, 4985, 6921, 293, 25882, 281, 2573, 300, 484, 13, 51072], "temperature": 0.0, "avg_logprob": -0.11966787685047496, "compression_ratio": 1.6696428571428572, "no_speech_prob": 0.31393277645111084}, {"id": 288, "seek": 183952, "start": 1853.68, "end": 1858.72, "text": " Now I haven't written unit tests for this, but clearly the way this client has used it, this", "tokens": [51072, 823, 286, 2378, 380, 3720, 4985, 6921, 337, 341, 11, 457, 4448, 264, 636, 341, 6423, 575, 1143, 309, 11, 341, 51324], "temperature": 0.0, "avg_logprob": -0.11966787685047496, "compression_ratio": 1.6696428571428572, "no_speech_prob": 0.31393277645111084}, {"id": 289, "seek": 183952, "start": 1858.72, "end": 1864.6399999999999, "text": " can be converted to unit tests and used in a variety of ways. And then obviously, if equal", "tokens": [51324, 393, 312, 16424, 281, 4985, 6921, 293, 1143, 294, 257, 5673, 295, 2098, 13, 400, 550, 2745, 11, 498, 2681, 51620], "temperature": 0.0, "avg_logprob": -0.11966787685047496, "compression_ratio": 1.6696428571428572, "no_speech_prob": 0.31393277645111084}, {"id": 290, "seek": 186464, "start": 1864.64, "end": 1871.1200000000001, "text": " instances have unequal hash codes, figure out why and fix the damn problem. He doesn't say damn,", "tokens": [50364, 14519, 362, 2251, 22345, 22019, 14211, 11, 2573, 484, 983, 293, 3191, 264, 8151, 1154, 13, 634, 1177, 380, 584, 8151, 11, 50688], "temperature": 0.0, "avg_logprob": -0.11887183240664903, "compression_ratio": 1.599056603773585, "no_speech_prob": 0.012623501941561699}, {"id": 291, "seek": 186464, "start": 1871.1200000000001, "end": 1874.72, "text": " I just put it in there because why not? He's a nice guy.", "tokens": [50688, 286, 445, 829, 309, 294, 456, 570, 983, 406, 30, 634, 311, 257, 1481, 2146, 13, 50868], "temperature": 0.0, "avg_logprob": -0.11887183240664903, "compression_ratio": 1.599056603773585, "no_speech_prob": 0.012623501941561699}, {"id": 292, "seek": 186464, "start": 1877.8400000000001, "end": 1881.6000000000001, "text": " So that makes a question, or perhaps it doesn't, but at least it begs the question for me,", "tokens": [51024, 407, 300, 1669, 257, 1168, 11, 420, 4317, 309, 1177, 380, 11, 457, 412, 1935, 309, 4612, 82, 264, 1168, 337, 385, 11, 51212], "temperature": 0.0, "avg_logprob": -0.11887183240664903, "compression_ratio": 1.599056603773585, "no_speech_prob": 0.012623501941561699}, {"id": 293, "seek": 186464, "start": 1882.16, "end": 1888.0800000000002, "text": " what to exclude from the hash code computation? That's the typo that's on hash code, hash code", "tokens": [51240, 437, 281, 33536, 490, 264, 22019, 3089, 24903, 30, 663, 311, 264, 2125, 78, 300, 311, 322, 22019, 3089, 11, 22019, 3089, 51536], "temperature": 0.0, "avg_logprob": -0.11887183240664903, "compression_ratio": 1.599056603773585, "no_speech_prob": 0.012623501941561699}, {"id": 294, "seek": 188808, "start": 1888.1599999999999, "end": 1895.6799999999998, "text": " computation. And he states you could in fact exclude so-called derived fields. And let's", "tokens": [50368, 24903, 13, 400, 415, 4368, 291, 727, 294, 1186, 33536, 370, 12, 11880, 18949, 7909, 13, 400, 718, 311, 50744], "temperature": 0.0, "avg_logprob": -0.11215880898868337, "compression_ratio": 1.6396396396396395, "no_speech_prob": 0.03513231500983238}, {"id": 295, "seek": 188808, "start": 1895.6799999999998, "end": 1901.36, "text": " understand what they are before we get to the demo. Those are values that can be computed from", "tokens": [50744, 1223, 437, 436, 366, 949, 321, 483, 281, 264, 10723, 13, 3950, 366, 4190, 300, 393, 312, 40610, 490, 51028], "temperature": 0.0, "avg_logprob": -0.11215880898868337, "compression_ratio": 1.6396396396396395, "no_speech_prob": 0.03513231500983238}, {"id": 296, "seek": 188808, "start": 1901.36, "end": 1906.32, "text": " other field values already in the hash code computation. So we did go through this idea", "tokens": [51028, 661, 2519, 4190, 1217, 294, 264, 22019, 3089, 24903, 13, 407, 321, 630, 352, 807, 341, 1558, 51276], "temperature": 0.0, "avg_logprob": -0.11215880898868337, "compression_ratio": 1.6396396396396395, "no_speech_prob": 0.03513231500983238}, {"id": 297, "seek": 188808, "start": 1906.32, "end": 1916.24, "text": " of derived fields before in the, in item 10, I believe using a pentagon or some mathematical", "tokens": [51276, 295, 18949, 7909, 949, 294, 264, 11, 294, 3174, 1266, 11, 286, 1697, 1228, 257, 16834, 6709, 420, 512, 18894, 51772], "temperature": 0.0, "avg_logprob": -0.11215880898868337, "compression_ratio": 1.6396396396396395, "no_speech_prob": 0.03513231500983238}, {"id": 298, "seek": 191624, "start": 1916.24, "end": 1922.72, "text": " structure. But let's take a look at another demo. I think it was a polygon we used in item 10. And", "tokens": [50364, 3877, 13, 583, 718, 311, 747, 257, 574, 412, 1071, 10723, 13, 286, 519, 309, 390, 257, 48242, 321, 1143, 294, 3174, 1266, 13, 400, 50688], "temperature": 0.0, "avg_logprob": -0.08865097013570494, "compression_ratio": 1.606060606060606, "no_speech_prob": 0.011157347820699215}, {"id": 299, "seek": 191624, "start": 1922.72, "end": 1928.0, "text": " I didn't want to use that example here because, you know, firstly, my math is shit. I'm quite", "tokens": [50688, 286, 994, 380, 528, 281, 764, 300, 1365, 510, 570, 11, 291, 458, 11, 27376, 11, 452, 5221, 307, 4611, 13, 286, 478, 1596, 50952], "temperature": 0.0, "avg_logprob": -0.08865097013570494, "compression_ratio": 1.606060606060606, "no_speech_prob": 0.011157347820699215}, {"id": 300, "seek": 191624, "start": 1928.0, "end": 1933.92, "text": " embarrassed about that. Therefore, I found it a bit hard to explain using the polygon class example.", "tokens": [50952, 16843, 466, 300, 13, 7504, 11, 286, 1352, 309, 257, 857, 1152, 281, 2903, 1228, 264, 48242, 1508, 1365, 13, 51248], "temperature": 0.0, "avg_logprob": -0.08865097013570494, "compression_ratio": 1.606060606060606, "no_speech_prob": 0.011157347820699215}, {"id": 301, "seek": 191624, "start": 1934.72, "end": 1938.48, "text": " That's something I should work towards. And in fact, teach myself some mathematics,", "tokens": [51288, 663, 311, 746, 286, 820, 589, 3030, 13, 400, 294, 1186, 11, 2924, 2059, 512, 18666, 11, 51476], "temperature": 0.0, "avg_logprob": -0.08865097013570494, "compression_ratio": 1.606060606060606, "no_speech_prob": 0.011157347820699215}, {"id": 302, "seek": 191624, "start": 1938.48, "end": 1944.64, "text": " some basic mathematics at least. But this is way more straightforward. So what's the derived field?", "tokens": [51476, 512, 3875, 18666, 412, 1935, 13, 583, 341, 307, 636, 544, 15325, 13, 407, 437, 311, 264, 18949, 2519, 30, 51784], "temperature": 0.0, "avg_logprob": -0.08865097013570494, "compression_ratio": 1.606060606060606, "no_speech_prob": 0.011157347820699215}, {"id": 303, "seek": 194464, "start": 1944.64, "end": 1949.76, "text": " We have a full name. And obviously a full name can be derived off the first name and the last name.", "tokens": [50364, 492, 362, 257, 1577, 1315, 13, 400, 2745, 257, 1577, 1315, 393, 312, 18949, 766, 264, 700, 1315, 293, 264, 1036, 1315, 13, 50620], "temperature": 0.0, "avg_logprob": -0.08246161890964882, "compression_ratio": 2.0523560209424083, "no_speech_prob": 0.004133729264140129}, {"id": 304, "seek": 194464, "start": 1950.96, "end": 1959.1200000000001, "text": " And in the hash code, we don't have to, when calculating the hash code, we don't have to use", "tokens": [50680, 400, 294, 264, 22019, 3089, 11, 321, 500, 380, 362, 281, 11, 562, 28258, 264, 22019, 3089, 11, 321, 500, 380, 362, 281, 764, 51088], "temperature": 0.0, "avg_logprob": -0.08246161890964882, "compression_ratio": 2.0523560209424083, "no_speech_prob": 0.004133729264140129}, {"id": 305, "seek": 194464, "start": 1960.5600000000002, "end": 1968.5600000000002, "text": " the full name in the hash code calculation, given that it's already derived from first name and last", "tokens": [51160, 264, 1577, 1315, 294, 264, 22019, 3089, 17108, 11, 2212, 300, 309, 311, 1217, 18949, 490, 700, 1315, 293, 1036, 51560], "temperature": 0.0, "avg_logprob": -0.08246161890964882, "compression_ratio": 2.0523560209424083, "no_speech_prob": 0.004133729264140129}, {"id": 306, "seek": 194464, "start": 1968.5600000000002, "end": 1973.92, "text": " name. That's it. That's what a derived field is. And that could be excluded in a hash code method.", "tokens": [51560, 1315, 13, 663, 311, 309, 13, 663, 311, 437, 257, 18949, 2519, 307, 13, 400, 300, 727, 312, 29486, 294, 257, 22019, 3089, 3170, 13, 51828], "temperature": 0.0, "avg_logprob": -0.08246161890964882, "compression_ratio": 2.0523560209424083, "no_speech_prob": 0.004133729264140129}, {"id": 307, "seek": 197392, "start": 1973.92, "end": 1979.76, "text": " Okay, so this next part, the order of the fields. And the point is the quality of the hash code", "tokens": [50364, 1033, 11, 370, 341, 958, 644, 11, 264, 1668, 295, 264, 7909, 13, 400, 264, 935, 307, 264, 3125, 295, 264, 22019, 3089, 50656], "temperature": 0.0, "avg_logprob": -0.09303681373596191, "compression_ratio": 1.6885964912280702, "no_speech_prob": 0.0009253376047126949}, {"id": 308, "seek": 197392, "start": 1979.76, "end": 1986.88, "text": " method is contingent on the order of the fields if a given class has similar fields. Now, to be", "tokens": [50656, 3170, 307, 27820, 317, 322, 264, 1668, 295, 264, 7909, 498, 257, 2212, 1508, 575, 2531, 7909, 13, 823, 11, 281, 312, 51012], "temperature": 0.0, "avg_logprob": -0.09303681373596191, "compression_ratio": 1.6885964912280702, "no_speech_prob": 0.0009253376047126949}, {"id": 309, "seek": 197392, "start": 1986.88, "end": 1992.48, "text": " totally upfront, I found this part quite difficult to understand. So I'm going to try my best to", "tokens": [51012, 3879, 30264, 11, 286, 1352, 341, 644, 1596, 2252, 281, 1223, 13, 407, 286, 478, 516, 281, 853, 452, 1151, 281, 51292], "temperature": 0.0, "avg_logprob": -0.09303681373596191, "compression_ratio": 1.6885964912280702, "no_speech_prob": 0.0009253376047126949}, {"id": 310, "seek": 197392, "start": 1992.48, "end": 1997.52, "text": " try and explain this. I think I got it, but I think I'm going to struggle a bit to articulate it", "tokens": [51292, 853, 293, 2903, 341, 13, 286, 519, 286, 658, 309, 11, 457, 286, 519, 286, 478, 516, 281, 7799, 257, 857, 281, 30305, 309, 51544], "temperature": 0.0, "avg_logprob": -0.09303681373596191, "compression_ratio": 1.6885964912280702, "no_speech_prob": 0.0009253376047126949}, {"id": 311, "seek": 199752, "start": 1997.52, "end": 2004.08, "text": " because it's not, intuitively, it's hard to get it on first glance. It requires a bit of thinking.", "tokens": [50364, 570, 309, 311, 406, 11, 46506, 11, 309, 311, 1152, 281, 483, 309, 322, 700, 21094, 13, 467, 7029, 257, 857, 295, 1953, 13, 50692], "temperature": 0.0, "avg_logprob": -0.15399445714177312, "compression_ratio": 1.5543478260869565, "no_speech_prob": 0.007937688380479813}, {"id": 312, "seek": 199752, "start": 2004.08, "end": 2011.36, "text": " So the reason for this is because of 2B. That is this calculation we make here. It's this is the", "tokens": [50692, 407, 264, 1778, 337, 341, 307, 570, 295, 568, 33, 13, 663, 307, 341, 17108, 321, 652, 510, 13, 467, 311, 341, 307, 264, 51056], "temperature": 0.0, "avg_logprob": -0.15399445714177312, "compression_ratio": 1.5543478260869565, "no_speech_prob": 0.007937688380479813}, {"id": 313, "seek": 199752, "start": 2011.36, "end": 2020.8, "text": " reason that makes the ordering contingent for the hash code method. So Joshua book states,", "tokens": [51056, 1778, 300, 1669, 264, 21739, 27820, 317, 337, 264, 22019, 3089, 3170, 13, 407, 24005, 1446, 4368, 11, 51528], "temperature": 0.0, "avg_logprob": -0.15399445714177312, "compression_ratio": 1.5543478260869565, "no_speech_prob": 0.007937688380479813}, {"id": 314, "seek": 202080, "start": 2020.8, "end": 2025.36, "text": " for example, if the multiplication were emitted from a string hash function,", "tokens": [50364, 337, 1365, 11, 498, 264, 27290, 645, 44897, 490, 257, 6798, 22019, 2445, 11, 50592], "temperature": 0.0, "avg_logprob": -0.12614628120704932, "compression_ratio": 1.6460176991150441, "no_speech_prob": 0.007121147122234106}, {"id": 315, "seek": 202080, "start": 2026.3999999999999, "end": 2034.56, "text": " all anagrams would have identical hash codes. The value 31 was chosen because it is an odd prime.", "tokens": [50644, 439, 364, 3914, 82, 576, 362, 14800, 22019, 14211, 13, 440, 2158, 10353, 390, 8614, 570, 309, 307, 364, 7401, 5835, 13, 51052], "temperature": 0.0, "avg_logprob": -0.12614628120704932, "compression_ratio": 1.6460176991150441, "no_speech_prob": 0.007121147122234106}, {"id": 316, "seek": 202080, "start": 2035.28, "end": 2040.6399999999999, "text": " If it were even and the multiplication overflowed, information would be lost because modification", "tokens": [51088, 759, 309, 645, 754, 293, 264, 27290, 37772, 292, 11, 1589, 576, 312, 2731, 570, 26747, 51356], "temperature": 0.0, "avg_logprob": -0.12614628120704932, "compression_ratio": 1.6460176991150441, "no_speech_prob": 0.007121147122234106}, {"id": 317, "seek": 202080, "start": 2040.6399999999999, "end": 2047.9199999999998, "text": " by two is equivalent to shifting. Another way to think about that is that this multiplication here,", "tokens": [51356, 538, 732, 307, 10344, 281, 17573, 13, 3996, 636, 281, 519, 466, 300, 307, 300, 341, 27290, 510, 11, 51720], "temperature": 0.0, "avg_logprob": -0.12614628120704932, "compression_ratio": 1.6460176991150441, "no_speech_prob": 0.007121147122234106}, {"id": 318, "seek": 204792, "start": 2047.92, "end": 2056.56, "text": " 31 times i, that can be replaced by this the shifting of the of the sum or the left shift", "tokens": [50364, 10353, 1413, 741, 11, 300, 393, 312, 10772, 538, 341, 264, 17573, 295, 264, 295, 264, 2408, 420, 264, 1411, 5513, 50796], "temperature": 0.0, "avg_logprob": -0.16046510423932756, "compression_ratio": 1.5363128491620113, "no_speech_prob": 0.00490480475127697}, {"id": 319, "seek": 204792, "start": 2056.56, "end": 2066.56, "text": " operator in Java. As far as Java is concerned, that and sorry, that and that are mathematically", "tokens": [50796, 12973, 294, 10745, 13, 1018, 1400, 382, 10745, 307, 5922, 11, 300, 293, 2597, 11, 300, 293, 300, 366, 44003, 51296], "temperature": 0.0, "avg_logprob": -0.16046510423932756, "compression_ratio": 1.5363128491620113, "no_speech_prob": 0.00490480475127697}, {"id": 320, "seek": 204792, "start": 2066.56, "end": 2071.92, "text": " equivalent. Now we'll get to the definition a bit before a bit later, sorry. But firstly,", "tokens": [51296, 10344, 13, 823, 321, 603, 483, 281, 264, 7123, 257, 857, 949, 257, 857, 1780, 11, 2597, 13, 583, 27376, 11, 51564], "temperature": 0.0, "avg_logprob": -0.16046510423932756, "compression_ratio": 1.5363128491620113, "no_speech_prob": 0.00490480475127697}, {"id": 321, "seek": 207192, "start": 2071.92, "end": 2078.32, "text": " let's take a look at some demos. So in this example, and this is a bad example where the", "tokens": [50364, 718, 311, 747, 257, 574, 412, 512, 33788, 13, 407, 294, 341, 1365, 11, 293, 341, 307, 257, 1578, 1365, 689, 264, 50684], "temperature": 0.0, "avg_logprob": -0.11159705132553258, "compression_ratio": 1.635135135135135, "no_speech_prob": 0.016400465741753578}, {"id": 322, "seek": 207192, "start": 2078.32, "end": 2087.52, "text": " order hasn't been considered. If you run this, there's a possibility that these two hash codes,", "tokens": [50684, 1668, 6132, 380, 668, 4888, 13, 759, 291, 1190, 341, 11, 456, 311, 257, 7959, 300, 613, 732, 22019, 14211, 11, 51144], "temperature": 0.0, "avg_logprob": -0.11159705132553258, "compression_ratio": 1.635135135135135, "no_speech_prob": 0.016400465741753578}, {"id": 323, "seek": 207192, "start": 2087.52, "end": 2093.6, "text": " so for object A and object B, which are two anagrams, that the hash code could be the same,", "tokens": [51144, 370, 337, 2657, 316, 293, 2657, 363, 11, 597, 366, 732, 364, 3914, 82, 11, 300, 264, 22019, 3089, 727, 312, 264, 912, 11, 51448], "temperature": 0.0, "avg_logprob": -0.11159705132553258, "compression_ratio": 1.635135135135135, "no_speech_prob": 0.016400465741753578}, {"id": 324, "seek": 207192, "start": 2093.6, "end": 2097.76, "text": " because all they're doing in the hash code method, if you can see here is we're simply", "tokens": [51448, 570, 439, 436, 434, 884, 294, 264, 22019, 3089, 3170, 11, 498, 291, 393, 536, 510, 307, 321, 434, 2935, 51656], "temperature": 0.0, "avg_logprob": -0.11159705132553258, "compression_ratio": 1.635135135135135, "no_speech_prob": 0.016400465741753578}, {"id": 325, "seek": 209776, "start": 2098.7200000000003, "end": 2103.6800000000003, "text": " returning the hash code of word of the string value. So we're just using the", "tokens": [50412, 12678, 264, 22019, 3089, 295, 1349, 295, 264, 6798, 2158, 13, 407, 321, 434, 445, 1228, 264, 50660], "temperature": 0.0, "avg_logprob": -0.16552321116129556, "compression_ratio": 1.6631205673758864, "no_speech_prob": 0.0453389510512352}, {"id": 326, "seek": 209776, "start": 2103.6800000000003, "end": 2108.5600000000004, "text": " hash code method in the class string. We aren't really considering anything else apart from that.", "tokens": [50660, 22019, 3089, 3170, 294, 264, 1508, 6798, 13, 492, 3212, 380, 534, 8079, 1340, 1646, 4936, 490, 300, 13, 50904], "temperature": 0.0, "avg_logprob": -0.16552321116129556, "compression_ratio": 1.6631205673758864, "no_speech_prob": 0.0453389510512352}, {"id": 327, "seek": 209776, "start": 2108.5600000000004, "end": 2115.6000000000004, "text": " In fact, I don't even know why they'll be written as second place here. It's like this is a superfluous", "tokens": [50904, 682, 1186, 11, 286, 500, 380, 754, 458, 983, 436, 603, 312, 3720, 382, 1150, 1081, 510, 13, 467, 311, 411, 341, 307, 257, 1687, 49253, 563, 51256], "temperature": 0.0, "avg_logprob": -0.16552321116129556, "compression_ratio": 1.6631205673758864, "no_speech_prob": 0.0453389510512352}, {"id": 328, "seek": 209776, "start": 2115.6000000000004, "end": 2121.84, "text": " or unnecessary method. And even though I ran this a couple of times, and this is why I said it", "tokens": [51256, 420, 19350, 3170, 13, 400, 754, 1673, 286, 5872, 341, 257, 1916, 295, 1413, 11, 293, 341, 307, 983, 286, 848, 309, 51568], "temperature": 0.0, "avg_logprob": -0.16552321116129556, "compression_ratio": 1.6631205673758864, "no_speech_prob": 0.0453389510512352}, {"id": 329, "seek": 209776, "start": 2121.84, "end": 2126.8, "text": " depends on the application state, it is possible theoretically for this to return the same hash", "tokens": [51568, 5946, 322, 264, 3861, 1785, 11, 309, 307, 1944, 29400, 337, 341, 281, 2736, 264, 912, 22019, 51816], "temperature": 0.0, "avg_logprob": -0.16552321116129556, "compression_ratio": 1.6631205673758864, "no_speech_prob": 0.0453389510512352}, {"id": 330, "seek": 212680, "start": 2126.88, "end": 2133.44, "text": " code despite these two being different. And then here, we are in fact multiplying by 31,", "tokens": [50368, 3089, 7228, 613, 732, 885, 819, 13, 400, 550, 510, 11, 321, 366, 294, 1186, 30955, 538, 10353, 11, 50696], "temperature": 0.0, "avg_logprob": -0.15990377153669086, "compression_ratio": 1.6591760299625469, "no_speech_prob": 0.006096827797591686}, {"id": 331, "seek": 212680, "start": 2133.44, "end": 2140.2400000000002, "text": " and we are even having this arbitrary value initially. The chance of these hash code values", "tokens": [50696, 293, 321, 366, 754, 1419, 341, 23211, 2158, 9105, 13, 440, 2931, 295, 613, 22019, 3089, 4190, 51036], "temperature": 0.0, "avg_logprob": -0.15990377153669086, "compression_ratio": 1.6591760299625469, "no_speech_prob": 0.006096827797591686}, {"id": 332, "seek": 212680, "start": 2140.2400000000002, "end": 2145.6000000000004, "text": " being the same is a lot more different in comparison to the previous example that I showed", "tokens": [51036, 885, 264, 912, 307, 257, 688, 544, 819, 294, 9660, 281, 264, 3894, 1365, 300, 286, 4712, 51304], "temperature": 0.0, "avg_logprob": -0.15990377153669086, "compression_ratio": 1.6591760299625469, "no_speech_prob": 0.006096827797591686}, {"id": 333, "seek": 212680, "start": 2146.7200000000003, "end": 2149.92, "text": " in the other class. Both classes are called anagrams, it's a bit confusing,", "tokens": [51360, 294, 264, 661, 1508, 13, 6767, 5359, 366, 1219, 364, 3914, 82, 11, 309, 311, 257, 857, 13181, 11, 51520], "temperature": 0.0, "avg_logprob": -0.15990377153669086, "compression_ratio": 1.6591760299625469, "no_speech_prob": 0.006096827797591686}, {"id": 334, "seek": 212680, "start": 2150.88, "end": 2155.84, "text": " but they're in different directories, or yeah, the packages are different. So that's because we", "tokens": [51568, 457, 436, 434, 294, 819, 5391, 530, 11, 420, 1338, 11, 264, 17401, 366, 819, 13, 407, 300, 311, 570, 321, 51816], "temperature": 0.0, "avg_logprob": -0.15990377153669086, "compression_ratio": 1.6591760299625469, "no_speech_prob": 0.006096827797591686}, {"id": 335, "seek": 215584, "start": 2156.0, "end": 2162.2400000000002, "text": " modified that a bit in hash code. This is a bad example. I only put this in here because it kind", "tokens": [50372, 15873, 300, 257, 857, 294, 22019, 3089, 13, 639, 307, 257, 1578, 1365, 13, 286, 787, 829, 341, 294, 510, 570, 309, 733, 50684], "temperature": 0.0, "avg_logprob": -0.14252297217104615, "compression_ratio": 1.6808510638297873, "no_speech_prob": 0.018261170014739037}, {"id": 336, "seek": 215584, "start": 2162.2400000000002, "end": 2168.0, "text": " of goes along with what Joshua Block had stated in the book. I in fact thought using a separate", "tokens": [50684, 295, 1709, 2051, 365, 437, 24005, 17500, 632, 11323, 294, 264, 1446, 13, 286, 294, 1186, 1194, 1228, 257, 4994, 50972], "temperature": 0.0, "avg_logprob": -0.14252297217104615, "compression_ratio": 1.6808510638297873, "no_speech_prob": 0.018261170014739037}, {"id": 337, "seek": 215584, "start": 2168.0, "end": 2173.04, "text": " class called person would be better to understand this. The reason I use anagram is because that's", "tokens": [50972, 1508, 1219, 954, 576, 312, 1101, 281, 1223, 341, 13, 440, 1778, 286, 764, 364, 3914, 307, 570, 300, 311, 51224], "temperature": 0.0, "avg_logprob": -0.14252297217104615, "compression_ratio": 1.6808510638297873, "no_speech_prob": 0.018261170014739037}, {"id": 338, "seek": 215584, "start": 2173.04, "end": 2178.8, "text": " the example that he's used, but frankly, is equally so. I didn't really get it much. So", "tokens": [51224, 264, 1365, 300, 415, 311, 1143, 11, 457, 11939, 11, 307, 12309, 370, 13, 286, 994, 380, 534, 483, 309, 709, 13, 407, 51512], "temperature": 0.0, "avg_logprob": -0.14252297217104615, "compression_ratio": 1.6808510638297873, "no_speech_prob": 0.018261170014739037}, {"id": 339, "seek": 215584, "start": 2178.8, "end": 2184.0, "text": " maybe we'll try to understand it with the person class. I put that other part in there just to", "tokens": [51512, 1310, 321, 603, 853, 281, 1223, 309, 365, 264, 954, 1508, 13, 286, 829, 300, 661, 644, 294, 456, 445, 281, 51772], "temperature": 0.0, "avg_logprob": -0.14252297217104615, "compression_ratio": 1.6808510638297873, "no_speech_prob": 0.018261170014739037}, {"id": 340, "seek": 218400, "start": 2184.08, "end": 2191.76, "text": " stick with what's in the book. And I thought that'll help in some way. Given he said if the", "tokens": [50368, 2897, 365, 437, 311, 294, 264, 1446, 13, 400, 286, 1194, 300, 603, 854, 294, 512, 636, 13, 18600, 415, 848, 498, 264, 50752], "temperature": 0.0, "avg_logprob": -0.07878351211547852, "compression_ratio": 1.7302325581395348, "no_speech_prob": 0.006487773731350899}, {"id": 341, "seek": 218400, "start": 2191.76, "end": 2197.68, "text": " multiplication were emitted from a string hash function, all anagrams would have identical", "tokens": [50752, 27290, 645, 44897, 490, 257, 6798, 22019, 2445, 11, 439, 364, 3914, 82, 576, 362, 14800, 51048], "temperature": 0.0, "avg_logprob": -0.07878351211547852, "compression_ratio": 1.7302325581395348, "no_speech_prob": 0.006487773731350899}, {"id": 342, "seek": 218400, "start": 2197.68, "end": 2205.44, "text": " hash codes. Ah, I now seek my confusion. He did say if the multiplication were emitted from a", "tokens": [51048, 22019, 14211, 13, 2438, 11, 286, 586, 8075, 452, 15075, 13, 634, 630, 584, 498, 264, 27290, 645, 44897, 490, 257, 51436], "temperature": 0.0, "avg_logprob": -0.07878351211547852, "compression_ratio": 1.7302325581395348, "no_speech_prob": 0.006487773731350899}, {"id": 343, "seek": 218400, "start": 2205.44, "end": 2209.92, "text": " string hash function. So the reason we're getting different ones here is because in fact in the", "tokens": [51436, 6798, 22019, 2445, 13, 407, 264, 1778, 321, 434, 1242, 819, 2306, 510, 307, 570, 294, 1186, 294, 264, 51660], "temperature": 0.0, "avg_logprob": -0.07878351211547852, "compression_ratio": 1.7302325581395348, "no_speech_prob": 0.006487773731350899}, {"id": 344, "seek": 220992, "start": 2210.0, "end": 2216.32, "text": " string hash function, it wasn't emitted. If it wasn't, it was kind of like this,", "tokens": [50368, 6798, 22019, 2445, 11, 309, 2067, 380, 44897, 13, 759, 309, 2067, 380, 11, 309, 390, 733, 295, 411, 341, 11, 50684], "temperature": 0.0, "avg_logprob": -0.08264835434730607, "compression_ratio": 1.7980769230769231, "no_speech_prob": 0.01941675879061222}, {"id": 345, "seek": 220992, "start": 2216.32, "end": 2221.84, "text": " just simply returning it. It could have been the same. But in the string hash function, if you look", "tokens": [50684, 445, 2935, 12678, 309, 13, 467, 727, 362, 668, 264, 912, 13, 583, 294, 264, 6798, 22019, 2445, 11, 498, 291, 574, 50960], "temperature": 0.0, "avg_logprob": -0.08264835434730607, "compression_ratio": 1.7980769230769231, "no_speech_prob": 0.01941675879061222}, {"id": 346, "seek": 220992, "start": 2221.84, "end": 2230.64, "text": " at the hash function, there is some multiplication being done. There's some work being done here.", "tokens": [50960, 412, 264, 22019, 2445, 11, 456, 307, 512, 27290, 885, 1096, 13, 821, 311, 512, 589, 885, 1096, 510, 13, 51400], "temperature": 0.0, "avg_logprob": -0.08264835434730607, "compression_ratio": 1.7980769230769231, "no_speech_prob": 0.01941675879061222}, {"id": 347, "seek": 220992, "start": 2230.64, "end": 2235.6, "text": " I'm not going to take a look at it. It's a bit too complicated for me, but it's not just simply", "tokens": [51400, 286, 478, 406, 516, 281, 747, 257, 574, 412, 309, 13, 467, 311, 257, 857, 886, 6179, 337, 385, 11, 457, 309, 311, 406, 445, 2935, 51648], "temperature": 0.0, "avg_logprob": -0.08264835434730607, "compression_ratio": 1.7980769230769231, "no_speech_prob": 0.01941675879061222}, {"id": 348, "seek": 223560, "start": 2236.56, "end": 2241.2799999999997, "text": " taking that value in and passing the hash code. So anagrams themselves would have different", "tokens": [50412, 1940, 300, 2158, 294, 293, 8437, 264, 22019, 3089, 13, 407, 364, 3914, 82, 2969, 576, 362, 819, 50648], "temperature": 0.0, "avg_logprob": -0.10542247772216796, "compression_ratio": 1.6360294117647058, "no_speech_prob": 0.0041986131109297276}, {"id": 349, "seek": 223560, "start": 2241.2799999999997, "end": 2246.7999999999997, "text": " hash codes, which is great. Apologies about that. I should have gone through that a bit more carefully", "tokens": [50648, 22019, 14211, 11, 597, 307, 869, 13, 8723, 6204, 466, 300, 13, 286, 820, 362, 2780, 807, 300, 257, 857, 544, 7500, 50924], "temperature": 0.0, "avg_logprob": -0.10542247772216796, "compression_ratio": 1.6360294117647058, "no_speech_prob": 0.0041986131109297276}, {"id": 350, "seek": 223560, "start": 2246.7999999999997, "end": 2250.08, "text": " beforehand, but it kind of makes sense now what he's trying to say here.", "tokens": [50924, 22893, 11, 457, 309, 733, 295, 1669, 2020, 586, 437, 415, 311, 1382, 281, 584, 510, 13, 51088], "temperature": 0.0, "avg_logprob": -0.10542247772216796, "compression_ratio": 1.6360294117647058, "no_speech_prob": 0.0041986131109297276}, {"id": 351, "seek": 223560, "start": 2252.08, "end": 2256.48, "text": " And then the part I was trying to explain before about these two statements being", "tokens": [51188, 400, 550, 264, 644, 286, 390, 1382, 281, 2903, 949, 466, 613, 732, 12363, 885, 51408], "temperature": 0.0, "avg_logprob": -0.10542247772216796, "compression_ratio": 1.6360294117647058, "no_speech_prob": 0.0041986131109297276}, {"id": 352, "seek": 223560, "start": 2256.48, "end": 2263.52, "text": " mathematically equivalent. Joshua Bock states, a nice property of 31 is that the multiplication", "tokens": [51408, 44003, 10344, 13, 24005, 47672, 4368, 11, 257, 1481, 4707, 295, 10353, 307, 300, 264, 27290, 51760], "temperature": 0.0, "avg_logprob": -0.10542247772216796, "compression_ratio": 1.6360294117647058, "no_speech_prob": 0.0041986131109297276}, {"id": 353, "seek": 226352, "start": 2263.52, "end": 2270.0, "text": " can be replaced by a shift and a subtraction for better performance on some architectures,", "tokens": [50364, 393, 312, 10772, 538, 257, 5513, 293, 257, 16390, 313, 337, 1101, 3389, 322, 512, 6331, 1303, 11, 50688], "temperature": 0.0, "avg_logprob": -0.11734660620828277, "compression_ratio": 1.5869565217391304, "no_speech_prob": 0.003172504249960184}, {"id": 354, "seek": 226352, "start": 2270.0, "end": 2276.96, "text": " because as I said, these two are equal, mathematically speaking. And modern VMs do", "tokens": [50688, 570, 382, 286, 848, 11, 613, 732, 366, 2681, 11, 44003, 4124, 13, 400, 4363, 18038, 82, 360, 51036], "temperature": 0.0, "avg_logprob": -0.11734660620828277, "compression_ratio": 1.5869565217391304, "no_speech_prob": 0.003172504249960184}, {"id": 355, "seek": 226352, "start": 2276.96, "end": 2281.36, "text": " this sort of optimization automatically. I perhaps did a terrible job at explaining that,", "tokens": [51036, 341, 1333, 295, 19618, 6772, 13, 286, 4317, 630, 257, 6237, 1691, 412, 13468, 300, 11, 51256], "temperature": 0.0, "avg_logprob": -0.11734660620828277, "compression_ratio": 1.5869565217391304, "no_speech_prob": 0.003172504249960184}, {"id": 356, "seek": 226352, "start": 2281.36, "end": 2286.64, "text": " because I too am trying to understand this idea of the shifting operator and all that.", "tokens": [51256, 570, 286, 886, 669, 1382, 281, 1223, 341, 1558, 295, 264, 17573, 12973, 293, 439, 300, 13, 51520], "temperature": 0.0, "avg_logprob": -0.11734660620828277, "compression_ratio": 1.5869565217391304, "no_speech_prob": 0.003172504249960184}, {"id": 357, "seek": 226352, "start": 2287.6, "end": 2292.4, "text": " So I asked Judge GPT, and I think this is much more clearer than what I could ever say.", "tokens": [51568, 407, 286, 2351, 19476, 26039, 51, 11, 293, 286, 519, 341, 307, 709, 544, 26131, 813, 437, 286, 727, 1562, 584, 13, 51808], "temperature": 0.0, "avg_logprob": -0.11734660620828277, "compression_ratio": 1.5869565217391304, "no_speech_prob": 0.003172504249960184}, {"id": 358, "seek": 229240, "start": 2293.2000000000003, "end": 2300.64, "text": " Here's what Judge GPT states regarding this. The double listed operator is the left shift", "tokens": [50404, 1692, 311, 437, 19476, 26039, 51, 4368, 8595, 341, 13, 440, 3834, 10052, 12973, 307, 264, 1411, 5513, 50776], "temperature": 0.0, "avg_logprob": -0.09080266952514648, "compression_ratio": 1.7641509433962264, "no_speech_prob": 0.005729563068598509}, {"id": 359, "seek": 229240, "start": 2300.64, "end": 2307.36, "text": " operator in Java, which shifts the bits of an integer to the left by a specified number of", "tokens": [50776, 12973, 294, 10745, 11, 597, 19201, 264, 9239, 295, 364, 24922, 281, 264, 1411, 538, 257, 22206, 1230, 295, 51112], "temperature": 0.0, "avg_logprob": -0.09080266952514648, "compression_ratio": 1.7641509433962264, "no_speech_prob": 0.005729563068598509}, {"id": 360, "seek": 229240, "start": 2307.36, "end": 2313.6, "text": " positions, effectively multiplying by 2 to the power n, where n is the number of positions shifted.", "tokens": [51112, 8432, 11, 8659, 30955, 538, 568, 281, 264, 1347, 297, 11, 689, 297, 307, 264, 1230, 295, 8432, 18892, 13, 51424], "temperature": 0.0, "avg_logprob": -0.09080266952514648, "compression_ratio": 1.7641509433962264, "no_speech_prob": 0.005729563068598509}, {"id": 361, "seek": 229240, "start": 2314.8, "end": 2320.48, "text": " That's why it's shifting. The minus operator is the subtraction operator, which subtracts the", "tokens": [51484, 663, 311, 983, 309, 311, 17573, 13, 440, 3175, 12973, 307, 264, 16390, 313, 12973, 11, 597, 16390, 82, 264, 51768], "temperature": 0.0, "avg_logprob": -0.09080266952514648, "compression_ratio": 1.7641509433962264, "no_speech_prob": 0.005729563068598509}, {"id": 362, "seek": 232048, "start": 2320.48, "end": 2329.2, "text": " second operand from the first. So the expression i double less than 5 minus i shifts the bits of i", "tokens": [50364, 1150, 2208, 474, 490, 264, 700, 13, 407, 264, 6114, 741, 3834, 1570, 813, 1025, 3175, 741, 19201, 264, 9239, 295, 741, 50800], "temperature": 0.0, "avg_logprob": -0.10337105350217958, "compression_ratio": 1.5714285714285714, "no_speech_prob": 0.0018101525492966175}, {"id": 363, "seek": 232048, "start": 2329.2, "end": 2335.2, "text": " to the left by 5 positions, effectively multiplying it by 2 to the power 5 or 32,", "tokens": [50800, 281, 264, 1411, 538, 1025, 8432, 11, 8659, 30955, 309, 538, 568, 281, 264, 1347, 1025, 420, 8858, 11, 51100], "temperature": 0.0, "avg_logprob": -0.10337105350217958, "compression_ratio": 1.5714285714285714, "no_speech_prob": 0.0018101525492966175}, {"id": 364, "seek": 232048, "start": 2335.2, "end": 2343.44, "text": " and then subtracts i from the result. Since 32 minus 1 equals 31, the expression is equivalent", "tokens": [51100, 293, 550, 16390, 82, 741, 490, 264, 1874, 13, 4162, 8858, 3175, 502, 6915, 10353, 11, 264, 6114, 307, 10344, 51512], "temperature": 0.0, "avg_logprob": -0.10337105350217958, "compression_ratio": 1.5714285714285714, "no_speech_prob": 0.0018101525492966175}, {"id": 365, "seek": 234344, "start": 2343.44, "end": 2352.56, "text": " to 31 times i. That's what it's this bit that gives this equality that Joshua Bloch speaks of.", "tokens": [50364, 281, 10353, 1413, 741, 13, 663, 311, 437, 309, 311, 341, 857, 300, 2709, 341, 14949, 300, 24005, 9865, 339, 10789, 295, 13, 50820], "temperature": 0.0, "avg_logprob": -0.09318120662982647, "compression_ratio": 1.5182186234817814, "no_speech_prob": 0.009857703931629658}, {"id": 366, "seek": 234344, "start": 2353.52, "end": 2358.4, "text": " By using the optimization, the hash function implementation can take advantage of the", "tokens": [50868, 3146, 1228, 264, 19618, 11, 264, 22019, 2445, 11420, 393, 747, 5002, 295, 264, 51112], "temperature": 0.0, "avg_logprob": -0.09318120662982647, "compression_ratio": 1.5182186234817814, "no_speech_prob": 0.009857703931629658}, {"id": 367, "seek": 234344, "start": 2358.4, "end": 2364.16, "text": " efficient left shift operation on some architectures potentially leading to improved performance,", "tokens": [51112, 7148, 1411, 5513, 6916, 322, 512, 6331, 1303, 7263, 5775, 281, 9689, 3389, 11, 51400], "temperature": 0.0, "avg_logprob": -0.09318120662982647, "compression_ratio": 1.5182186234817814, "no_speech_prob": 0.009857703931629658}, {"id": 368, "seek": 234344, "start": 2364.16, "end": 2369.36, "text": " and also Judge GPT repeats what he stated. Modern virtual machines are designed to automatically", "tokens": [51400, 293, 611, 19476, 26039, 51, 35038, 437, 415, 11323, 13, 19814, 6374, 8379, 366, 4761, 281, 6772, 51660], "temperature": 0.0, "avg_logprob": -0.09318120662982647, "compression_ratio": 1.5182186234817814, "no_speech_prob": 0.009857703931629658}, {"id": 369, "seek": 236936, "start": 2369.36, "end": 2374.4, "text": " perform this kind of optimization, so the hash function implementation can remain unchanged", "tokens": [50364, 2042, 341, 733, 295, 19618, 11, 370, 264, 22019, 2445, 11420, 393, 6222, 44553, 50616], "temperature": 0.0, "avg_logprob": -0.1293345341640236, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.028002828359603882}, {"id": 370, "seek": 236936, "start": 2374.4, "end": 2380.6400000000003, "text": " and still benefit from the performance improvement. So a simple example of this would be if I ran this", "tokens": [50616, 293, 920, 5121, 490, 264, 3389, 10444, 13, 407, 257, 2199, 1365, 295, 341, 576, 312, 498, 286, 5872, 341, 50928], "temperature": 0.0, "avg_logprob": -0.1293345341640236, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.028002828359603882}, {"id": 371, "seek": 236936, "start": 2380.6400000000003, "end": 2388.6400000000003, "text": " code, j would be equal to k. It'll print true because both these are equal. They both will be 310.", "tokens": [50928, 3089, 11, 361, 576, 312, 2681, 281, 350, 13, 467, 603, 4482, 2074, 570, 1293, 613, 366, 2681, 13, 814, 1293, 486, 312, 805, 3279, 13, 51328], "temperature": 0.0, "avg_logprob": -0.1293345341640236, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.028002828359603882}, {"id": 372, "seek": 236936, "start": 2389.44, "end": 2393.92, "text": " And then let's take a look at another demo. I think we already kind of took a look at this.", "tokens": [51368, 400, 550, 718, 311, 747, 257, 574, 412, 1071, 10723, 13, 286, 519, 321, 1217, 733, 295, 1890, 257, 574, 412, 341, 13, 51592], "temperature": 0.0, "avg_logprob": -0.1293345341640236, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.028002828359603882}, {"id": 373, "seek": 236936, "start": 2393.92, "end": 2397.92, "text": " Sorry, by the way, not related to this is separate because Joshua Bloch states,", "tokens": [51592, 4919, 11, 538, 264, 636, 11, 406, 4077, 281, 341, 307, 4994, 570, 24005, 9865, 339, 4368, 11, 51792], "temperature": 0.0, "avg_logprob": -0.1293345341640236, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.028002828359603882}, {"id": 374, "seek": 239792, "start": 2398.64, "end": 2403.76, "text": " say all that, let's apply this previous recipe to the phone number class. Now I already did apply it", "tokens": [50400, 584, 439, 300, 11, 718, 311, 3079, 341, 3894, 6782, 281, 264, 2593, 1230, 1508, 13, 823, 286, 1217, 630, 3079, 309, 50656], "temperature": 0.0, "avg_logprob": -0.11654415130615234, "compression_ratio": 1.638655462184874, "no_speech_prob": 0.005729727912694216}, {"id": 375, "seek": 239792, "start": 2403.76, "end": 2410.48, "text": " and show the demo in my own way, but it's important to take a look at what Joshua Bloch has done too.", "tokens": [50656, 293, 855, 264, 10723, 294, 452, 1065, 636, 11, 457, 309, 311, 1021, 281, 747, 257, 574, 412, 437, 24005, 9865, 339, 575, 1096, 886, 13, 50992], "temperature": 0.0, "avg_logprob": -0.11654415130615234, "compression_ratio": 1.638655462184874, "no_speech_prob": 0.005729727912694216}, {"id": 376, "seek": 239792, "start": 2410.48, "end": 2418.96, "text": " Here's the demo. As we saw, it returns the hash code of the initial value area code that he set,", "tokens": [50992, 1692, 311, 264, 10723, 13, 1018, 321, 1866, 11, 309, 11247, 264, 22019, 3089, 295, 264, 5883, 2158, 1859, 3089, 300, 415, 992, 11, 51416], "temperature": 0.0, "avg_logprob": -0.11654415130615234, "compression_ratio": 1.638655462184874, "no_speech_prob": 0.005729727912694216}, {"id": 377, "seek": 239792, "start": 2418.96, "end": 2423.84, "text": " says he said to take the first significant field and assign the hash code value to result,", "tokens": [51416, 1619, 415, 848, 281, 747, 264, 700, 4776, 2519, 293, 6269, 264, 22019, 3089, 2158, 281, 1874, 11, 51660], "temperature": 0.0, "avg_logprob": -0.11654415130615234, "compression_ratio": 1.638655462184874, "no_speech_prob": 0.005729727912694216}, {"id": 378, "seek": 242384, "start": 2423.84, "end": 2428.8, "text": " and then do the calculation accordingly. And that returns the result. And on this method,", "tokens": [50364, 293, 550, 360, 264, 17108, 19717, 13, 400, 300, 11247, 264, 1874, 13, 400, 322, 341, 3170, 11, 50612], "temperature": 0.0, "avg_logprob": -0.11165285699161483, "compression_ratio": 1.7922705314009661, "no_speech_prob": 0.011507106013596058}, {"id": 379, "seek": 242384, "start": 2428.8, "end": 2436.0, "text": " he states, because this method returns the result of a simple deterministic computation,", "tokens": [50612, 415, 4368, 11, 570, 341, 3170, 11247, 264, 1874, 295, 257, 2199, 15957, 3142, 24903, 11, 50972], "temperature": 0.0, "avg_logprob": -0.11165285699161483, "compression_ratio": 1.7922705314009661, "no_speech_prob": 0.011507106013596058}, {"id": 380, "seek": 242384, "start": 2436.56, "end": 2443.28, "text": " whose only inputs are the three significant fields in a phone number instance, that is area code,", "tokens": [51000, 6104, 787, 15743, 366, 264, 1045, 4776, 7909, 294, 257, 2593, 1230, 5197, 11, 300, 307, 1859, 3089, 11, 51336], "temperature": 0.0, "avg_logprob": -0.11165285699161483, "compression_ratio": 1.7922705314009661, "no_speech_prob": 0.011507106013596058}, {"id": 381, "seek": 242384, "start": 2443.28, "end": 2451.92, "text": " prefix and line number, is clear that that equal phone number instances have equal hash codes.", "tokens": [51336, 46969, 293, 1622, 1230, 11, 307, 1850, 300, 300, 2681, 2593, 1230, 14519, 362, 2681, 22019, 14211, 13, 51768], "temperature": 0.0, "avg_logprob": -0.11165285699161483, "compression_ratio": 1.7922705314009661, "no_speech_prob": 0.011507106013596058}, {"id": 382, "seek": 245192, "start": 2452.48, "end": 2458.8, "text": " It is simple, is reasonably fast, and does a reasonable job of dispersing unequal numbers", "tokens": [50392, 467, 307, 2199, 11, 307, 23551, 2370, 11, 293, 775, 257, 10585, 1691, 295, 24631, 278, 2251, 22345, 3547, 50708], "temperature": 0.0, "avg_logprob": -0.14207761664139598, "compression_ratio": 1.5635593220338984, "no_speech_prob": 0.006096986588090658}, {"id": 383, "seek": 245192, "start": 2458.8, "end": 2465.28, "text": " into different hash buckets. And of course, a bit of a caveat here, as we saw in item 10,", "tokens": [50708, 666, 819, 22019, 32191, 13, 400, 295, 1164, 11, 257, 857, 295, 257, 43012, 510, 11, 382, 321, 1866, 294, 3174, 1266, 11, 51032], "temperature": 0.0, "avg_logprob": -0.14207761664139598, "compression_ratio": 1.5635593220338984, "no_speech_prob": 0.006096986588090658}, {"id": 384, "seek": 245192, "start": 2465.28, "end": 2472.56, "text": " when it comes to equals comparison, even in the hash codes, he stated, if you have a bona fide", "tokens": [51032, 562, 309, 1487, 281, 6915, 9660, 11, 754, 294, 264, 22019, 14211, 11, 415, 11323, 11, 498, 291, 362, 257, 49012, 283, 482, 51396], "temperature": 0.0, "avg_logprob": -0.14207761664139598, "compression_ratio": 1.5635593220338984, "no_speech_prob": 0.006096986588090658}, {"id": 385, "seek": 245192, "start": 2472.56, "end": 2480.96, "text": " need for hash functions, less likely to produce collisions, see Guava's co Java library, which", "tokens": [51396, 643, 337, 22019, 6828, 11, 1570, 3700, 281, 5258, 46537, 11, 536, 2694, 4061, 311, 598, 10745, 6405, 11, 597, 51816], "temperature": 0.0, "avg_logprob": -0.14207761664139598, "compression_ratio": 1.5635593220338984, "no_speech_prob": 0.006096986588090658}, {"id": 386, "seek": 248096, "start": 2480.96, "end": 2486.8, "text": " is Google's co libraries for Java, and the hashing there does it much better than you and I ever", "tokens": [50364, 307, 3329, 311, 598, 15148, 337, 10745, 11, 293, 264, 575, 571, 456, 775, 309, 709, 1101, 813, 291, 293, 286, 1562, 50656], "temperature": 0.0, "avg_logprob": -0.11487107527883429, "compression_ratio": 1.7075812274368232, "no_speech_prob": 0.0010816676076501608}, {"id": 387, "seek": 248096, "start": 2486.8, "end": 2492.48, "text": " could. And he's continued by giving a much more simpler one line hash function, which you'll see", "tokens": [50656, 727, 13, 400, 415, 311, 7014, 538, 2902, 257, 709, 544, 18587, 472, 1622, 22019, 2445, 11, 597, 291, 603, 536, 50940], "temperature": 0.0, "avg_logprob": -0.11487107527883429, "compression_ratio": 1.7075812274368232, "no_speech_prob": 0.0010816676076501608}, {"id": 388, "seek": 248096, "start": 2492.48, "end": 2499.36, "text": " here. Comment out the other one. So these are all hash functions. There's different ways of", "tokens": [50940, 510, 13, 16328, 484, 264, 661, 472, 13, 407, 613, 366, 439, 22019, 6828, 13, 821, 311, 819, 2098, 295, 51284], "temperature": 0.0, "avg_logprob": -0.11487107527883429, "compression_ratio": 1.7075812274368232, "no_speech_prob": 0.0010816676076501608}, {"id": 389, "seek": 248096, "start": 2499.36, "end": 2503.12, "text": " implementing them. He's given three separate examples. The third one will go through soon.", "tokens": [51284, 18114, 552, 13, 634, 311, 2212, 1045, 4994, 5110, 13, 440, 2636, 472, 486, 352, 807, 2321, 13, 51472], "temperature": 0.0, "avg_logprob": -0.11487107527883429, "compression_ratio": 1.7075812274368232, "no_speech_prob": 0.0010816676076501608}, {"id": 390, "seek": 248096, "start": 2503.76, "end": 2509.84, "text": " This is a one line hash function using the objects dot hash. The caveat here, despite it being a", "tokens": [51504, 639, 307, 257, 472, 1622, 22019, 2445, 1228, 264, 6565, 5893, 22019, 13, 440, 43012, 510, 11, 7228, 309, 885, 257, 51808], "temperature": 0.0, "avg_logprob": -0.11487107527883429, "compression_ratio": 1.7075812274368232, "no_speech_prob": 0.0010816676076501608}, {"id": 391, "seek": 250984, "start": 2509.84, "end": 2516.8, "text": " one line hash function, is that it should only be used if performance isn't critical, because it", "tokens": [50364, 472, 1622, 22019, 2445, 11, 307, 300, 309, 820, 787, 312, 1143, 498, 3389, 1943, 380, 4924, 11, 570, 309, 50712], "temperature": 0.0, "avg_logprob": -0.1280954725602094, "compression_ratio": 1.508108108108108, "no_speech_prob": 0.0039452072232961655}, {"id": 392, "seek": 250984, "start": 2516.8, "end": 2524.08, "text": " does return an array. And every time it's invoked, it returns an array and also involves auto boxing", "tokens": [50712, 775, 2736, 364, 10225, 13, 400, 633, 565, 309, 311, 1048, 9511, 11, 309, 11247, 364, 10225, 293, 611, 11626, 8399, 24424, 51076], "temperature": 0.0, "avg_logprob": -0.1280954725602094, "compression_ratio": 1.508108108108108, "no_speech_prob": 0.0039452072232961655}, {"id": 393, "seek": 250984, "start": 2524.08, "end": 2529.92, "text": " if we do pass a primitive type. And that takes us to an interesting part in these", "tokens": [51076, 498, 321, 360, 1320, 257, 28540, 2010, 13, 400, 300, 2516, 505, 281, 364, 1880, 644, 294, 613, 51368], "temperature": 0.0, "avg_logprob": -0.1280954725602094, "compression_ratio": 1.508108108108108, "no_speech_prob": 0.0039452072232961655}, {"id": 394, "seek": 252992, "start": 2530.56, "end": 2540.2400000000002, "text": " hash code implementations, which which is threat safe lazy initialization. So lazy initialization", "tokens": [50396, 22019, 3089, 4445, 763, 11, 597, 597, 307, 4734, 3273, 14847, 5883, 2144, 13, 407, 14847, 5883, 2144, 50880], "temperature": 0.0, "avg_logprob": -0.1520396564783675, "compression_ratio": 1.8246445497630333, "no_speech_prob": 0.07054795324802399}, {"id": 395, "seek": 252992, "start": 2540.2400000000002, "end": 2547.2000000000003, "text": " can be used if you believe your class is immutable. And if it considers or not considers it involves", "tokens": [50880, 393, 312, 1143, 498, 291, 1697, 428, 1508, 307, 3397, 32148, 13, 400, 498, 309, 33095, 420, 406, 33095, 309, 11626, 51228], "temperature": 0.0, "avg_logprob": -0.1520396564783675, "compression_ratio": 1.8246445497630333, "no_speech_prob": 0.07054795324802399}, {"id": 396, "seek": 252992, "start": 2547.76, "end": 2552.2400000000002, "text": " the invocation of the hash code involves recalculating the hash code every every time it's", "tokens": [51256, 264, 1048, 27943, 295, 264, 22019, 3089, 11626, 850, 304, 2444, 990, 264, 22019, 3089, 633, 633, 565, 309, 311, 51480], "temperature": 0.0, "avg_logprob": -0.1520396564783675, "compression_ratio": 1.8246445497630333, "no_speech_prob": 0.07054795324802399}, {"id": 397, "seek": 252992, "start": 2552.2400000000002, "end": 2559.28, "text": " requested. And he says if you believe that most objects of this type will be used as hash keys,", "tokens": [51480, 16436, 13, 400, 415, 1619, 498, 291, 1697, 300, 881, 6565, 295, 341, 2010, 486, 312, 1143, 382, 22019, 9317, 11, 51832], "temperature": 0.0, "avg_logprob": -0.1520396564783675, "compression_ratio": 1.8246445497630333, "no_speech_prob": 0.07054795324802399}, {"id": 398, "seek": 255992, "start": 2560.08, "end": 2567.12, "text": " then you should calculate the hash code when the instance is created. Otherwise, you might", "tokens": [50372, 550, 291, 820, 8873, 264, 22019, 3089, 562, 264, 5197, 307, 2942, 13, 10328, 11, 291, 1062, 50724], "temperature": 0.0, "avg_logprob": -0.09550245698676052, "compression_ratio": 1.748768472906404, "no_speech_prob": 0.001956833293661475}, {"id": 399, "seek": 255992, "start": 2567.12, "end": 2572.32, "text": " choose to lazily initialize the hash code the first time hash code is invoked.", "tokens": [50724, 2826, 281, 19320, 953, 5883, 1125, 264, 22019, 3089, 264, 700, 565, 22019, 3089, 307, 1048, 9511, 13, 50984], "temperature": 0.0, "avg_logprob": -0.09550245698676052, "compression_ratio": 1.748768472906404, "no_speech_prob": 0.001956833293661475}, {"id": 400, "seek": 255992, "start": 2573.44, "end": 2578.64, "text": " Some care is required to ensure that the class remains threat safe in the presence of a", "tokens": [51040, 2188, 1127, 307, 4739, 281, 5586, 300, 264, 1508, 7023, 4734, 3273, 294, 264, 6814, 295, 257, 51300], "temperature": 0.0, "avg_logprob": -0.09550245698676052, "compression_ratio": 1.748768472906404, "no_speech_prob": 0.001956833293661475}, {"id": 401, "seek": 255992, "start": 2578.64, "end": 2584.2400000000002, "text": " lazily initialized field. Now we shall look into that because creating it to be threat safe is in", "tokens": [51300, 19320, 953, 5883, 1602, 2519, 13, 823, 321, 4393, 574, 666, 300, 570, 4084, 309, 281, 312, 4734, 3273, 307, 294, 51580], "temperature": 0.0, "avg_logprob": -0.09550245698676052, "compression_ratio": 1.748768472906404, "no_speech_prob": 0.001956833293661475}, {"id": 402, "seek": 258424, "start": 2584.3199999999997, "end": 2591.2799999999997, "text": " fact an important not even an acillary point here, but it's in fact very much related to this idea of", "tokens": [50368, 1186, 364, 1021, 406, 754, 364, 696, 46367, 935, 510, 11, 457, 309, 311, 294, 1186, 588, 709, 4077, 281, 341, 1558, 295, 50716], "temperature": 0.0, "avg_logprob": -0.1299863012213456, "compression_ratio": 1.7017543859649122, "no_speech_prob": 0.0033764599356800318}, {"id": 403, "seek": 258424, "start": 2591.2799999999997, "end": 2596.72, "text": " lazy initialization. But before we look at the threat safety example, let's firstly look at the", "tokens": [50716, 14847, 5883, 2144, 13, 583, 949, 321, 574, 412, 264, 4734, 4514, 1365, 11, 718, 311, 27376, 574, 412, 264, 50988], "temperature": 0.0, "avg_logprob": -0.1299863012213456, "compression_ratio": 1.7017543859649122, "no_speech_prob": 0.0033764599356800318}, {"id": 404, "seek": 258424, "start": 2596.72, "end": 2601.68, "text": " phone number example. And the phone number class doesn't require this kind of threat safety. He's", "tokens": [50988, 2593, 1230, 1365, 13, 400, 264, 2593, 1230, 1508, 1177, 380, 3651, 341, 733, 295, 4734, 4514, 13, 634, 311, 51236], "temperature": 0.0, "avg_logprob": -0.1299863012213456, "compression_ratio": 1.7017543859649122, "no_speech_prob": 0.0033764599356800318}, {"id": 405, "seek": 258424, "start": 2601.68, "end": 2607.6, "text": " even stated that in the book. And what is lazy initialization? It's quite simple, really. We", "tokens": [51236, 754, 11323, 300, 294, 264, 1446, 13, 400, 437, 307, 14847, 5883, 2144, 30, 467, 311, 1596, 2199, 11, 534, 13, 492, 51532], "temperature": 0.0, "avg_logprob": -0.1299863012213456, "compression_ratio": 1.7017543859649122, "no_speech_prob": 0.0033764599356800318}, {"id": 406, "seek": 260760, "start": 2607.6, "end": 2616.24, "text": " have a private in hash code method, and we check if the result is zero. If there's no", "tokens": [50364, 362, 257, 4551, 294, 22019, 3089, 3170, 11, 293, 321, 1520, 498, 264, 1874, 307, 4018, 13, 759, 456, 311, 572, 50796], "temperature": 0.0, "avg_logprob": -0.0879634420077006, "compression_ratio": 1.7104072398190044, "no_speech_prob": 0.008061651140451431}, {"id": 407, "seek": 260760, "start": 2616.24, "end": 2623.68, "text": " cash result, we directly return that result. So that means every time the hash code method is", "tokens": [50796, 6388, 1874, 11, 321, 3838, 2736, 300, 1874, 13, 407, 300, 1355, 633, 565, 264, 22019, 3089, 3170, 307, 51168], "temperature": 0.0, "avg_logprob": -0.0879634420077006, "compression_ratio": 1.7104072398190044, "no_speech_prob": 0.008061651140451431}, {"id": 408, "seek": 260760, "start": 2623.68, "end": 2629.2, "text": " invoked, it doesn't have to go through this bit and do the computation slash calculation. If it's", "tokens": [51168, 1048, 9511, 11, 309, 1177, 380, 362, 281, 352, 807, 341, 857, 293, 360, 264, 24903, 17330, 17108, 13, 759, 309, 311, 51444], "temperature": 0.0, "avg_logprob": -0.0879634420077006, "compression_ratio": 1.7104072398190044, "no_speech_prob": 0.008061651140451431}, {"id": 409, "seek": 260760, "start": 2629.2, "end": 2635.36, "text": " cached, it could just be it could just be returned. And that's what lazy initialization is. And this", "tokens": [51444, 269, 15095, 11, 309, 727, 445, 312, 309, 727, 445, 312, 8752, 13, 400, 300, 311, 437, 14847, 5883, 2144, 307, 13, 400, 341, 51752], "temperature": 0.0, "avg_logprob": -0.0879634420077006, "compression_ratio": 1.7104072398190044, "no_speech_prob": 0.008061651140451431}, {"id": 410, "seek": 263536, "start": 2635.36, "end": 2642.32, "text": " is automatically initialized to zero initially. So as I've stated here in the comment, apt for", "tokens": [50364, 307, 6772, 5883, 1602, 281, 4018, 9105, 13, 407, 382, 286, 600, 11323, 510, 294, 264, 2871, 11, 29427, 337, 50712], "temperature": 0.0, "avg_logprob": -0.1153336933680943, "compression_ratio": 1.5643153526970954, "no_speech_prob": 0.0013249998446553946}, {"id": 411, "seek": 263536, "start": 2642.32, "end": 2648.6400000000003, "text": " immutable classes with expensive hash code calculation. Okay, now let's get to the threat", "tokens": [50712, 3397, 32148, 5359, 365, 5124, 22019, 3089, 17108, 13, 1033, 11, 586, 718, 311, 483, 281, 264, 4734, 51028], "temperature": 0.0, "avg_logprob": -0.1153336933680943, "compression_ratio": 1.5643153526970954, "no_speech_prob": 0.0013249998446553946}, {"id": 412, "seek": 263536, "start": 2648.6400000000003, "end": 2654.48, "text": " safety bit, which is in fact, despite it being a bit complicated at first, I found most interesting", "tokens": [51028, 4514, 857, 11, 597, 307, 294, 1186, 11, 7228, 309, 885, 257, 857, 6179, 412, 700, 11, 286, 1352, 881, 1880, 51320], "temperature": 0.0, "avg_logprob": -0.1153336933680943, "compression_ratio": 1.5643153526970954, "no_speech_prob": 0.0013249998446553946}, {"id": 413, "seek": 263536, "start": 2655.2000000000003, "end": 2660.6400000000003, "text": " in this item. I kind of enjoyed it, especially because I kind of do enjoy that part of Java,", "tokens": [51356, 294, 341, 3174, 13, 286, 733, 295, 4626, 309, 11, 2318, 570, 286, 733, 295, 360, 2103, 300, 644, 295, 10745, 11, 51628], "temperature": 0.0, "avg_logprob": -0.1153336933680943, "compression_ratio": 1.5643153526970954, "no_speech_prob": 0.0013249998446553946}, {"id": 414, "seek": 266064, "start": 2660.64, "end": 2668.0, "text": " the whole multi threading bit in shelf. So here's an example of lazy initialization", "tokens": [50364, 264, 1379, 4825, 7207, 278, 857, 294, 15222, 13, 407, 510, 311, 364, 1365, 295, 14847, 5883, 2144, 50732], "temperature": 0.0, "avg_logprob": -0.21341638786848202, "compression_ratio": 1.6698564593301435, "no_speech_prob": 0.004069854039698839}, {"id": 415, "seek": 266064, "start": 2669.12, "end": 2675.68, "text": " with threat safety. So we have this class called canna. And canna holds it's a it's an", "tokens": [50788, 365, 4734, 4514, 13, 407, 321, 362, 341, 1508, 1219, 393, 629, 13, 400, 393, 629, 9190, 309, 311, 257, 309, 311, 364, 51116], "temperature": 0.0, "avg_logprob": -0.21341638786848202, "compression_ratio": 1.6698564593301435, "no_speech_prob": 0.004069854039698839}, {"id": 416, "seek": 266064, "start": 2676.3199999999997, "end": 2682.4, "text": " this class is atomic as we're using the atomic integer method. So atomic means and competing", "tokens": [51148, 341, 1508, 307, 22275, 382, 321, 434, 1228, 264, 22275, 24922, 3170, 13, 407, 22275, 1355, 293, 15439, 51452], "temperature": 0.0, "avg_logprob": -0.21341638786848202, "compression_ratio": 1.6698564593301435, "no_speech_prob": 0.004069854039698839}, {"id": 417, "seek": 266064, "start": 2682.4, "end": 2688.8799999999997, "text": " atomic means the either the change happens, or it doesn't happen. I always understood", "tokens": [51452, 22275, 1355, 264, 2139, 264, 1319, 2314, 11, 420, 309, 1177, 380, 1051, 13, 286, 1009, 7320, 51776], "temperature": 0.0, "avg_logprob": -0.21341638786848202, "compression_ratio": 1.6698564593301435, "no_speech_prob": 0.004069854039698839}, {"id": 418, "seek": 268888, "start": 2688.88, "end": 2694.2400000000002, "text": " atomic operations in the context of a database. So a good example is something like a bank", "tokens": [50364, 22275, 7705, 294, 264, 4319, 295, 257, 8149, 13, 407, 257, 665, 1365, 307, 746, 411, 257, 3765, 50632], "temperature": 0.0, "avg_logprob": -0.12149052717247788, "compression_ratio": 1.6428571428571428, "no_speech_prob": 0.001284194178879261}, {"id": 419, "seek": 268888, "start": 2694.2400000000002, "end": 2699.44, "text": " transaction. You either want the money to go go through, or you don't want it to go through at all.", "tokens": [50632, 14425, 13, 509, 2139, 528, 264, 1460, 281, 352, 352, 807, 11, 420, 291, 500, 380, 528, 309, 281, 352, 807, 412, 439, 13, 50892], "temperature": 0.0, "avg_logprob": -0.12149052717247788, "compression_ratio": 1.6428571428571428, "no_speech_prob": 0.001284194178879261}, {"id": 420, "seek": 268888, "start": 2699.44, "end": 2706.6400000000003, "text": " It's very binary in that sense, no pun intended. And that's what an atomic computation is. There's no", "tokens": [50892, 467, 311, 588, 17434, 294, 300, 2020, 11, 572, 4468, 10226, 13, 400, 300, 311, 437, 364, 22275, 24903, 307, 13, 821, 311, 572, 51252], "temperature": 0.0, "avg_logprob": -0.12149052717247788, "compression_ratio": 1.6428571428571428, "no_speech_prob": 0.001284194178879261}, {"id": 421, "seek": 268888, "start": 2706.6400000000003, "end": 2713.44, "text": " there's no murky territory. There's no half of the computation happening. So we're using the count", "tokens": [51252, 456, 311, 572, 5257, 4133, 11360, 13, 821, 311, 572, 1922, 295, 264, 24903, 2737, 13, 407, 321, 434, 1228, 264, 1207, 51592], "temperature": 0.0, "avg_logprob": -0.12149052717247788, "compression_ratio": 1.6428571428571428, "no_speech_prob": 0.001284194178879261}, {"id": 422, "seek": 271344, "start": 2713.44, "end": 2717.6, "text": " of type atomic integer. And then we have we have another hash code value here,", "tokens": [50364, 295, 2010, 22275, 24922, 13, 400, 550, 321, 362, 321, 362, 1071, 22019, 3089, 2158, 510, 11, 50572], "temperature": 0.0, "avg_logprob": -0.16483679782138783, "compression_ratio": 1.9734042553191489, "no_speech_prob": 0.014062623493373394}, {"id": 423, "seek": 271344, "start": 2718.8, "end": 2724.64, "text": " a field value of also atomic integer. We have the constructors whatnot, we're setting the count", "tokens": [50632, 257, 2519, 2158, 295, 611, 22275, 24922, 13, 492, 362, 264, 7690, 830, 25882, 11, 321, 434, 3287, 264, 1207, 50924], "temperature": 0.0, "avg_logprob": -0.16483679782138783, "compression_ratio": 1.9734042553191489, "no_speech_prob": 0.014062623493373394}, {"id": 424, "seek": 271344, "start": 2724.64, "end": 2728.8, "text": " getting the count forget about all that, not forget about all that ignore all that for this context.", "tokens": [50924, 1242, 264, 1207, 2870, 466, 439, 300, 11, 406, 2870, 466, 439, 300, 11200, 439, 300, 337, 341, 4319, 13, 51132], "temperature": 0.0, "avg_logprob": -0.16483679782138783, "compression_ratio": 1.9734042553191489, "no_speech_prob": 0.014062623493373394}, {"id": 425, "seek": 271344, "start": 2730.4, "end": 2736.4, "text": " And then we have the hash code method. And the hash code method. Remember, all of this is still", "tokens": [51212, 400, 550, 321, 362, 264, 22019, 3089, 3170, 13, 400, 264, 22019, 3089, 3170, 13, 5459, 11, 439, 295, 341, 307, 920, 51512], "temperature": 0.0, "avg_logprob": -0.16483679782138783, "compression_ratio": 1.9734042553191489, "no_speech_prob": 0.014062623493373394}, {"id": 426, "seek": 273640, "start": 2736.4, "end": 2744.64, "text": " using the atomic integer. And here, though the hash code is computed lazily, it's still the same", "tokens": [50364, 1228, 264, 22275, 24922, 13, 400, 510, 11, 1673, 264, 22019, 3089, 307, 40610, 19320, 953, 11, 309, 311, 920, 264, 912, 50776], "temperature": 0.0, "avg_logprob": -0.10468757853788488, "compression_ratio": 1.9554455445544554, "no_speech_prob": 0.08033937215805054}, {"id": 427, "seek": 273640, "start": 2744.64, "end": 2749.84, "text": " thing that we saw in the phone number class, where we're checking if the value is there checking if", "tokens": [50776, 551, 300, 321, 1866, 294, 264, 2593, 1230, 1508, 11, 689, 321, 434, 8568, 498, 264, 2158, 307, 456, 8568, 498, 51036], "temperature": 0.0, "avg_logprob": -0.10468757853788488, "compression_ratio": 1.9554455445544554, "no_speech_prob": 0.08033937215805054}, {"id": 428, "seek": 273640, "start": 2749.84, "end": 2757.44, "text": " the value is equal to zero, only if it's equal to zero, where we're setting the value and and passing", "tokens": [51036, 264, 2158, 307, 2681, 281, 4018, 11, 787, 498, 309, 311, 2681, 281, 4018, 11, 689, 321, 434, 3287, 264, 2158, 293, 293, 8437, 51416], "temperature": 0.0, "avg_logprob": -0.10468757853788488, "compression_ratio": 1.9554455445544554, "no_speech_prob": 0.08033937215805054}, {"id": 429, "seek": 273640, "start": 2757.44, "end": 2763.6800000000003, "text": " it, we're setting the value and then returning it. If not, if it's not equal to zero, that means", "tokens": [51416, 309, 11, 321, 434, 3287, 264, 2158, 293, 550, 12678, 309, 13, 759, 406, 11, 498, 309, 311, 406, 2681, 281, 4018, 11, 300, 1355, 51728], "temperature": 0.0, "avg_logprob": -0.10468757853788488, "compression_ratio": 1.9554455445544554, "no_speech_prob": 0.08033937215805054}, {"id": 430, "seek": 276368, "start": 2763.68, "end": 2768.3199999999997, "text": " the value has been cached. So we just return that value straight away. So that's the lazy", "tokens": [50364, 264, 2158, 575, 668, 269, 15095, 13, 407, 321, 445, 2736, 300, 2158, 2997, 1314, 13, 407, 300, 311, 264, 14847, 50596], "temperature": 0.0, "avg_logprob": -0.19632991609119232, "compression_ratio": 1.6767241379310345, "no_speech_prob": 0.004069819580763578}, {"id": 431, "seek": 276368, "start": 2768.3199999999997, "end": 2775.2799999999997, "text": " initialization bit. But now we get to the thread safety bit, the multi threading bit. So I've started", "tokens": [50596, 5883, 2144, 857, 13, 583, 586, 321, 483, 281, 264, 7207, 4514, 857, 11, 264, 4825, 7207, 278, 857, 13, 407, 286, 600, 1409, 50944], "temperature": 0.0, "avg_logprob": -0.19632991609119232, "compression_ratio": 1.6767241379310345, "no_speech_prob": 0.004069819580763578}, {"id": 432, "seek": 276368, "start": 2775.2799999999997, "end": 2782.48, "text": " two threads here, t1, t2. And they're both they both got two counters. So firstly, I've got an", "tokens": [50944, 732, 19314, 510, 11, 256, 16, 11, 256, 17, 13, 400, 436, 434, 1293, 436, 1293, 658, 732, 39338, 13, 407, 27376, 11, 286, 600, 658, 364, 51304], "temperature": 0.0, "avg_logprob": -0.19632991609119232, "compression_ratio": 1.6767241379310345, "no_speech_prob": 0.004069819580763578}, {"id": 433, "seek": 276368, "start": 2782.48, "end": 2788.7999999999997, "text": " instance of counter called count counter, of course, of the constructs, I've initialized that with 10.", "tokens": [51304, 5197, 295, 5682, 1219, 1207, 5682, 11, 295, 1164, 11, 295, 264, 7690, 82, 11, 286, 600, 5883, 1602, 300, 365, 1266, 13, 51620], "temperature": 0.0, "avg_logprob": -0.19632991609119232, "compression_ratio": 1.6767241379310345, "no_speech_prob": 0.004069819580763578}, {"id": 434, "seek": 278880, "start": 2789.76, "end": 2797.6000000000004, "text": " And then I've created two threads that sets the count that that changes the count concurrently.", "tokens": [50412, 400, 550, 286, 600, 2942, 732, 19314, 300, 6352, 264, 1207, 300, 300, 2962, 264, 1207, 37702, 356, 13, 50804], "temperature": 0.0, "avg_logprob": -0.17058089344771868, "compression_ratio": 1.7799043062200957, "no_speech_prob": 0.007815257646143436}, {"id": 435, "seek": 278880, "start": 2798.1600000000003, "end": 2804.6400000000003, "text": " So two threads are started. So here, since we've used t1 dot join and t2 dot join,", "tokens": [50832, 407, 732, 19314, 366, 1409, 13, 407, 510, 11, 1670, 321, 600, 1143, 256, 16, 5893, 3917, 293, 256, 17, 5893, 3917, 11, 51156], "temperature": 0.0, "avg_logprob": -0.17058089344771868, "compression_ratio": 1.7799043062200957, "no_speech_prob": 0.007815257646143436}, {"id": 436, "seek": 278880, "start": 2804.6400000000003, "end": 2810.0800000000004, "text": " because of the use of this join method, only the final state of the counter, that's this one here,", "tokens": [51156, 570, 295, 264, 764, 295, 341, 3917, 3170, 11, 787, 264, 2572, 1785, 295, 264, 5682, 11, 300, 311, 341, 472, 510, 11, 51428], "temperature": 0.0, "avg_logprob": -0.17058089344771868, "compression_ratio": 1.7799043062200957, "no_speech_prob": 0.007815257646143436}, {"id": 437, "seek": 278880, "start": 2810.0800000000004, "end": 2815.04, "text": " this object will be used, in this case, for get count, but for anything else, because the main", "tokens": [51428, 341, 2657, 486, 312, 1143, 11, 294, 341, 1389, 11, 337, 483, 1207, 11, 457, 337, 1340, 1646, 11, 570, 264, 2135, 51676], "temperature": 0.0, "avg_logprob": -0.17058089344771868, "compression_ratio": 1.7799043062200957, "no_speech_prob": 0.007815257646143436}, {"id": 438, "seek": 281504, "start": 2815.12, "end": 2822.8, "text": " thread waits for both child threads to finish, finish execution. So there are two threads spun", "tokens": [50368, 7207, 40597, 337, 1293, 1440, 19314, 281, 2413, 11, 2413, 15058, 13, 407, 456, 366, 732, 19314, 37038, 50752], "temperature": 0.0, "avg_logprob": -0.10971432736045436, "compression_ratio": 1.6872246696035242, "no_speech_prob": 0.0045381407253444195}, {"id": 439, "seek": 281504, "start": 2822.8, "end": 2828.64, "text": " up and there's a main thread, of course, that'll wait until both threads finish execution by using", "tokens": [50752, 493, 293, 456, 311, 257, 2135, 7207, 11, 295, 1164, 11, 300, 603, 1699, 1826, 1293, 19314, 2413, 15058, 538, 1228, 51044], "temperature": 0.0, "avg_logprob": -0.10971432736045436, "compression_ratio": 1.6872246696035242, "no_speech_prob": 0.0045381407253444195}, {"id": 440, "seek": 281504, "start": 2828.64, "end": 2835.44, "text": " join. And since the atomic integer has been used, the value will either be 15 or 20. It's atomic,", "tokens": [51044, 3917, 13, 400, 1670, 264, 22275, 24922, 575, 668, 1143, 11, 264, 2158, 486, 2139, 312, 2119, 420, 945, 13, 467, 311, 22275, 11, 51384], "temperature": 0.0, "avg_logprob": -0.10971432736045436, "compression_ratio": 1.6872246696035242, "no_speech_prob": 0.0045381407253444195}, {"id": 441, "seek": 281504, "start": 2836.08, "end": 2842.8, "text": " because the type for the hash code is atomic. And 15 and 20, of course, comes from this set", "tokens": [51416, 570, 264, 2010, 337, 264, 22019, 3089, 307, 22275, 13, 400, 2119, 293, 945, 11, 295, 1164, 11, 1487, 490, 341, 992, 51752], "temperature": 0.0, "avg_logprob": -0.10971432736045436, "compression_ratio": 1.6872246696035242, "no_speech_prob": 0.0045381407253444195}, {"id": 442, "seek": 284280, "start": 2842.8, "end": 2849.36, "text": " count that we used here. So depending on which thread finishes first, it'll always be the final", "tokens": [50364, 1207, 300, 321, 1143, 510, 13, 407, 5413, 322, 597, 7207, 23615, 700, 11, 309, 603, 1009, 312, 264, 2572, 50692], "temperature": 0.0, "avg_logprob": -0.12207903052276035, "compression_ratio": 1.7312775330396475, "no_speech_prob": 0.010986116714775562}, {"id": 443, "seek": 284280, "start": 2849.36, "end": 2854.96, "text": " state will be reflected. So if I maybe write it a couple of times, this is how it came. Final count", "tokens": [50692, 1785, 486, 312, 15502, 13, 407, 498, 286, 1310, 2464, 309, 257, 1916, 295, 1413, 11, 341, 307, 577, 309, 1361, 13, 13443, 1207, 50972], "temperature": 0.0, "avg_logprob": -0.12207903052276035, "compression_ratio": 1.7312775330396475, "no_speech_prob": 0.010986116714775562}, {"id": 444, "seek": 284280, "start": 2854.96, "end": 2864.1600000000003, "text": " and so the final count is, so the final count is 20. And the hash code also comes as 20. But I think", "tokens": [50972, 293, 370, 264, 2572, 1207, 307, 11, 370, 264, 2572, 1207, 307, 945, 13, 400, 264, 22019, 3089, 611, 1487, 382, 945, 13, 583, 286, 519, 51432], "temperature": 0.0, "avg_logprob": -0.12207903052276035, "compression_ratio": 1.7312775330396475, "no_speech_prob": 0.010986116714775562}, {"id": 445, "seek": 284280, "start": 2864.1600000000003, "end": 2872.2400000000002, "text": " that's because the value is cached here. So if I run that a couple of times, let's see if at one", "tokens": [51432, 300, 311, 570, 264, 2158, 307, 269, 15095, 510, 13, 407, 498, 286, 1190, 300, 257, 1916, 295, 1413, 11, 718, 311, 536, 498, 412, 472, 51836], "temperature": 0.0, "avg_logprob": -0.12207903052276035, "compression_ratio": 1.7312775330396475, "no_speech_prob": 0.010986116714775562}, {"id": 446, "seek": 287224, "start": 2872.3199999999997, "end": 2879.04, "text": " point, it's always the second thread that finishes last. So it's passing that value. But", "tokens": [50368, 935, 11, 309, 311, 1009, 264, 1150, 7207, 300, 23615, 1036, 13, 407, 309, 311, 8437, 300, 2158, 13, 583, 50704], "temperature": 0.0, "avg_logprob": -0.13395443529185683, "compression_ratio": 1.6863636363636363, "no_speech_prob": 0.015423072502017021}, {"id": 447, "seek": 287224, "start": 2879.7599999999998, "end": 2887.52, "text": " let's say I change this, I don't know, I multiply this by 21, for some reason, and I run it again.", "tokens": [50740, 718, 311, 584, 286, 1319, 341, 11, 286, 500, 380, 458, 11, 286, 12972, 341, 538, 5080, 11, 337, 512, 1778, 11, 293, 286, 1190, 309, 797, 13, 51128], "temperature": 0.0, "avg_logprob": -0.13395443529185683, "compression_ratio": 1.6863636363636363, "no_speech_prob": 0.015423072502017021}, {"id": 448, "seek": 287224, "start": 2887.52, "end": 2891.8399999999997, "text": " In this case, the hash code value will change. And then if I run it again, now it's going to keep", "tokens": [51128, 682, 341, 1389, 11, 264, 22019, 3089, 2158, 486, 1319, 13, 400, 550, 498, 286, 1190, 309, 797, 11, 586, 309, 311, 516, 281, 1066, 51344], "temperature": 0.0, "avg_logprob": -0.13395443529185683, "compression_ratio": 1.6863636363636363, "no_speech_prob": 0.015423072502017021}, {"id": 449, "seek": 287224, "start": 2891.8399999999997, "end": 2896.64, "text": " returning that because that value is cached. And the point being, because we're using", "tokens": [51344, 12678, 300, 570, 300, 2158, 307, 269, 15095, 13, 400, 264, 935, 885, 11, 570, 321, 434, 1228, 51584], "temperature": 0.0, "avg_logprob": -0.13395443529185683, "compression_ratio": 1.6863636363636363, "no_speech_prob": 0.015423072502017021}, {"id": 450, "seek": 289664, "start": 2897.52, "end": 2901.6, "text": " type atomic integer, these operations will be atomic. And this is what", "tokens": [50408, 2010, 22275, 24922, 11, 613, 7705, 486, 312, 22275, 13, 400, 341, 307, 437, 50612], "temperature": 0.0, "avg_logprob": -0.10754929967673428, "compression_ratio": 1.5787037037037037, "no_speech_prob": 0.012819977477192879}, {"id": 451, "seek": 289664, "start": 2902.24, "end": 2908.7999999999997, "text": " Joshua Block means by creating lazy initialization to work with thread safety. So", "tokens": [50644, 24005, 17500, 1355, 538, 4084, 14847, 5883, 2144, 281, 589, 365, 7207, 4514, 13, 407, 50972], "temperature": 0.0, "avg_logprob": -0.10754929967673428, "compression_ratio": 1.5787037037037037, "no_speech_prob": 0.012819977477192879}, {"id": 452, "seek": 289664, "start": 2908.7999999999997, "end": 2913.52, "text": " in this case, really, this bit doesn't matter too much. What matters is when we declare the", "tokens": [50972, 294, 341, 1389, 11, 534, 11, 341, 857, 1177, 380, 1871, 886, 709, 13, 708, 7001, 307, 562, 321, 19710, 264, 51208], "temperature": 0.0, "avg_logprob": -0.10754929967673428, "compression_ratio": 1.5787037037037037, "no_speech_prob": 0.012819977477192879}, {"id": 453, "seek": 289664, "start": 2913.52, "end": 2922.7999999999997, "text": " hash code value, we use a type of atomic integer that ensures it's thread safe. So a few caveats", "tokens": [51208, 22019, 3089, 2158, 11, 321, 764, 257, 2010, 295, 22275, 24922, 300, 28111, 309, 311, 7207, 3273, 13, 407, 257, 1326, 11730, 1720, 51672], "temperature": 0.0, "avg_logprob": -0.10754929967673428, "compression_ratio": 1.5787037037037037, "no_speech_prob": 0.012819977477192879}, {"id": 454, "seek": 292280, "start": 2922.8, "end": 2929.44, "text": " here, the hash code initialization field value should not be the hash code of a commonly created", "tokens": [50364, 510, 11, 264, 22019, 3089, 5883, 2144, 2519, 2158, 820, 406, 312, 264, 22019, 3089, 295, 257, 12719, 2942, 50696], "temperature": 0.0, "avg_logprob": -0.06849253177642822, "compression_ratio": 1.9895287958115184, "no_speech_prob": 0.06185366213321686}, {"id": 455, "seek": 292280, "start": 2929.44, "end": 2936.96, "text": " instance. So that initial value we set for the hash code calculation, this is not a good example.", "tokens": [50696, 5197, 13, 407, 300, 5883, 2158, 321, 992, 337, 264, 22019, 3089, 17108, 11, 341, 307, 406, 257, 665, 1365, 13, 51072], "temperature": 0.0, "avg_logprob": -0.06849253177642822, "compression_ratio": 1.9895287958115184, "no_speech_prob": 0.06185366213321686}, {"id": 456, "seek": 292280, "start": 2936.96, "end": 2943.28, "text": " If we go back to the phone number class, this initial value we set, it shouldn't be the hash", "tokens": [51072, 759, 321, 352, 646, 281, 264, 2593, 1230, 1508, 11, 341, 5883, 2158, 321, 992, 11, 309, 4659, 380, 312, 264, 22019, 51388], "temperature": 0.0, "avg_logprob": -0.06849253177642822, "compression_ratio": 1.9895287958115184, "no_speech_prob": 0.06185366213321686}, {"id": 457, "seek": 292280, "start": 2943.28, "end": 2948.6400000000003, "text": " code of a value that's commonly created, pardon me, not a commonly created value, a commonly", "tokens": [51388, 3089, 295, 257, 2158, 300, 311, 12719, 2942, 11, 22440, 385, 11, 406, 257, 12719, 2942, 2158, 11, 257, 12719, 51656], "temperature": 0.0, "avg_logprob": -0.06849253177642822, "compression_ratio": 1.9895287958115184, "no_speech_prob": 0.06185366213321686}, {"id": 458, "seek": 294864, "start": 2948.64, "end": 2953.44, "text": " created instance or some kind of object that's used quite a lot. So that initialization value,", "tokens": [50364, 2942, 5197, 420, 512, 733, 295, 2657, 300, 311, 1143, 1596, 257, 688, 13, 407, 300, 5883, 2144, 2158, 11, 50604], "temperature": 0.0, "avg_logprob": -0.13415187167138168, "compression_ratio": 1.5847457627118644, "no_speech_prob": 0.02441672421991825}, {"id": 459, "seek": 294864, "start": 2953.44, "end": 2958.3199999999997, "text": " because that would obviously defeat the purpose of caching as it'll constantly be changing.", "tokens": [50604, 570, 300, 576, 2745, 11785, 264, 4334, 295, 269, 2834, 382, 309, 603, 6460, 312, 4473, 13, 50848], "temperature": 0.0, "avg_logprob": -0.13415187167138168, "compression_ratio": 1.5847457627118644, "no_speech_prob": 0.02441672421991825}, {"id": 460, "seek": 294864, "start": 2958.8799999999997, "end": 2964.16, "text": " And the other thing, and this is really important, like what Don Knuth stated, oh, it wasn't Don", "tokens": [50876, 400, 264, 661, 551, 11, 293, 341, 307, 534, 1021, 11, 411, 437, 1468, 10519, 2910, 11323, 11, 1954, 11, 309, 2067, 380, 1468, 51140], "temperature": 0.0, "avg_logprob": -0.13415187167138168, "compression_ratio": 1.5847457627118644, "no_speech_prob": 0.02441672421991825}, {"id": 461, "seek": 294864, "start": 2964.16, "end": 2969.6, "text": " Knuth, wasn't it? It's apocryphally attributed to Don Knuth, but the root of optimization,", "tokens": [51140, 10519, 2910, 11, 2067, 380, 309, 30, 467, 311, 1882, 905, 627, 950, 379, 30976, 281, 1468, 10519, 2910, 11, 457, 264, 5593, 295, 19618, 11, 51412], "temperature": 0.0, "avg_logprob": -0.13415187167138168, "compression_ratio": 1.5847457627118644, "no_speech_prob": 0.02441672421991825}, {"id": 462, "seek": 296960, "start": 2970.48, "end": 2978.96, "text": " sorry, I'd push it back. The root of all evil is premature optimization. So always choose", "tokens": [50408, 2597, 11, 286, 1116, 2944, 309, 646, 13, 440, 5593, 295, 439, 6724, 307, 34877, 19618, 13, 407, 1009, 2826, 50832], "temperature": 0.0, "avg_logprob": -0.15880067664456654, "compression_ratio": 1.5938864628820961, "no_speech_prob": 0.007460154592990875}, {"id": 463, "seek": 296960, "start": 2978.96, "end": 2987.04, "text": " accuracy over optimization. And the point is poor quality hash functions that are inaccurate", "tokens": [50832, 14170, 670, 19618, 13, 400, 264, 935, 307, 4716, 3125, 22019, 6828, 300, 366, 46443, 51236], "temperature": 0.0, "avg_logprob": -0.15880067664456654, "compression_ratio": 1.5938864628820961, "no_speech_prob": 0.007460154592990875}, {"id": 464, "seek": 296960, "start": 2987.04, "end": 2991.44, "text": " will degrade hash tables to the point of being unusable. It's kind of connected to that previous", "tokens": [51236, 486, 368, 8692, 22019, 8020, 281, 264, 935, 295, 885, 10054, 712, 13, 467, 311, 733, 295, 4582, 281, 300, 3894, 51456], "temperature": 0.0, "avg_logprob": -0.15880067664456654, "compression_ratio": 1.5938864628820961, "no_speech_prob": 0.007460154592990875}, {"id": 465, "seek": 296960, "start": 2992.16, "end": 2996.88, "text": " point where we just set one constant value that legally makes sense, but would make a", "tokens": [51492, 935, 689, 321, 445, 992, 472, 5754, 2158, 300, 21106, 1669, 2020, 11, 457, 576, 652, 257, 51728], "temperature": 0.0, "avg_logprob": -0.15880067664456654, "compression_ratio": 1.5938864628820961, "no_speech_prob": 0.007460154592990875}, {"id": 466, "seek": 299688, "start": 2996.88, "end": 3004.48, "text": " terrible hash function. So Joshua Bock states, do not be tempted to exclude significant fields", "tokens": [50364, 6237, 22019, 2445, 13, 407, 24005, 47672, 4368, 11, 360, 406, 312, 29941, 281, 33536, 4776, 7909, 50744], "temperature": 0.0, "avg_logprob": -0.15540097399455746, "compression_ratio": 1.5365853658536586, "no_speech_prob": 0.010165638290345669}, {"id": 467, "seek": 299688, "start": 3004.48, "end": 3011.36, "text": " from the hash code computation to improve performance. So always pick accuracy of optimization,", "tokens": [50744, 490, 264, 22019, 3089, 24903, 281, 3470, 3389, 13, 407, 1009, 1888, 14170, 295, 19618, 11, 51088], "temperature": 0.0, "avg_logprob": -0.15540097399455746, "compression_ratio": 1.5365853658536586, "no_speech_prob": 0.010165638290345669}, {"id": 468, "seek": 299688, "start": 3011.36, "end": 3018.0, "text": " especially with modern CPUs these days. Like why would you? Well, why would you? It makes more", "tokens": [51088, 2318, 365, 4363, 13199, 82, 613, 1708, 13, 1743, 983, 576, 291, 30, 1042, 11, 983, 576, 291, 30, 467, 1669, 544, 51420], "temperature": 0.0, "avg_logprob": -0.15540097399455746, "compression_ratio": 1.5365853658536586, "no_speech_prob": 0.010165638290345669}, {"id": 469, "seek": 299688, "start": 3018.0, "end": 3024.32, "text": " sense to focus on accuracy because there's pretty much infinite compute in the modern world.", "tokens": [51420, 2020, 281, 1879, 322, 14170, 570, 456, 311, 1238, 709, 13785, 14722, 294, 264, 4363, 1002, 13, 51736], "temperature": 0.0, "avg_logprob": -0.15540097399455746, "compression_ratio": 1.5365853658536586, "no_speech_prob": 0.010165638290345669}, {"id": 470, "seek": 302432, "start": 3024.88, "end": 3030.1600000000003, "text": " Or as he's put it here, in particular, the hash function may be confronted with a", "tokens": [50392, 1610, 382, 415, 311, 829, 309, 510, 11, 294, 1729, 11, 264, 22019, 2445, 815, 312, 31257, 365, 257, 50656], "temperature": 0.0, "avg_logprob": -0.0865453008621458, "compression_ratio": 1.5780346820809248, "no_speech_prob": 0.0031724635045975447}, {"id": 471, "seek": 302432, "start": 3030.88, "end": 3037.28, "text": " large collection of instances that differ mainly in regions that you've chosen to ignore. If this", "tokens": [50692, 2416, 5765, 295, 14519, 300, 743, 8704, 294, 10682, 300, 291, 600, 8614, 281, 11200, 13, 759, 341, 51012], "temperature": 0.0, "avg_logprob": -0.0865453008621458, "compression_ratio": 1.5780346820809248, "no_speech_prob": 0.0031724635045975447}, {"id": 472, "seek": 302432, "start": 3037.28, "end": 3043.6800000000003, "text": " happens, the hash function will map all these instances to a few hash codes and programs that", "tokens": [51012, 2314, 11, 264, 22019, 2445, 486, 4471, 439, 613, 14519, 281, 257, 1326, 22019, 14211, 293, 4268, 300, 51332], "temperature": 0.0, "avg_logprob": -0.0865453008621458, "compression_ratio": 1.5780346820809248, "no_speech_prob": 0.0031724635045975447}, {"id": 473, "seek": 304368, "start": 3043.9199999999996, "end": 3055.6, "text": " run in linear time. Sorry, I'd push it back. If this happens, the hash function will map all", "tokens": [50376, 1190, 294, 8213, 565, 13, 4919, 11, 286, 1116, 2944, 309, 646, 13, 759, 341, 2314, 11, 264, 22019, 2445, 486, 4471, 439, 50960], "temperature": 0.0, "avg_logprob": -0.10362837280052295, "compression_ratio": 1.615819209039548, "no_speech_prob": 0.012053129263222218}, {"id": 474, "seek": 304368, "start": 3055.6, "end": 3063.12, "text": " these instances to a few hash codes and programs that should run in linear time will instead run", "tokens": [50960, 613, 14519, 281, 257, 1326, 22019, 14211, 293, 4268, 300, 820, 1190, 294, 8213, 565, 486, 2602, 1190, 51336], "temperature": 0.0, "avg_logprob": -0.10362837280052295, "compression_ratio": 1.615819209039548, "no_speech_prob": 0.012053129263222218}, {"id": 475, "seek": 304368, "start": 3063.12, "end": 3069.9199999999996, "text": " in quadratic time. So if we take this class example, this is an example of a poor implementation", "tokens": [51336, 294, 37262, 565, 13, 407, 498, 321, 747, 341, 1508, 1365, 11, 341, 307, 364, 1365, 295, 257, 4716, 11420, 51676], "temperature": 0.0, "avg_logprob": -0.10362837280052295, "compression_ratio": 1.615819209039548, "no_speech_prob": 0.012053129263222218}, {"id": 476, "seek": 306992, "start": 3069.92, "end": 3077.36, "text": " of the hash code. Last name in this person class, did I say class example? If we take this person", "tokens": [50364, 295, 264, 22019, 3089, 13, 5264, 1315, 294, 341, 954, 1508, 11, 630, 286, 584, 1508, 1365, 30, 759, 321, 747, 341, 954, 50736], "temperature": 0.0, "avg_logprob": -0.17053352518284576, "compression_ratio": 1.6244541484716157, "no_speech_prob": 0.0583304762840271}, {"id": 477, "seek": 306992, "start": 3077.36, "end": 3082.7200000000003, "text": " class example, last name of this class is a significant field, obviously. But in the hash", "tokens": [50736, 1508, 1365, 11, 1036, 1315, 295, 341, 1508, 307, 257, 4776, 2519, 11, 2745, 13, 583, 294, 264, 22019, 51004], "temperature": 0.0, "avg_logprob": -0.17053352518284576, "compression_ratio": 1.6244541484716157, "no_speech_prob": 0.0583304762840271}, {"id": 478, "seek": 306992, "start": 3082.7200000000003, "end": 3089.6800000000003, "text": " code, we've ignored it. Now, what that would do is to put it more clearly, I use strategy PD,", "tokens": [51004, 3089, 11, 321, 600, 19735, 309, 13, 823, 11, 437, 300, 576, 360, 307, 281, 829, 309, 544, 4448, 11, 286, 764, 5206, 10464, 11, 51352], "temperature": 0.0, "avg_logprob": -0.17053352518284576, "compression_ratio": 1.6244541484716157, "no_speech_prob": 0.0583304762840271}, {"id": 479, "seek": 306992, "start": 3089.6800000000003, "end": 3095.6800000000003, "text": " because I struggled to articulate that. If we create a large collection of person objects,", "tokens": [51352, 570, 286, 19023, 281, 30305, 300, 13, 759, 321, 1884, 257, 2416, 5765, 295, 954, 6565, 11, 51652], "temperature": 0.0, "avg_logprob": -0.17053352518284576, "compression_ratio": 1.6244541484716157, "no_speech_prob": 0.0583304762840271}, {"id": 480, "seek": 309568, "start": 3095.68, "end": 3102.08, "text": " so from this class, that differ mainly in their last name field, so this field that we've ignored.", "tokens": [50364, 370, 490, 341, 1508, 11, 300, 743, 8704, 294, 641, 1036, 1315, 2519, 11, 370, 341, 2519, 300, 321, 600, 19735, 13, 50684], "temperature": 0.0, "avg_logprob": -0.11133508904035701, "compression_ratio": 1.6608695652173913, "no_speech_prob": 0.0016228347085416317}, {"id": 481, "seek": 309568, "start": 3104.3199999999997, "end": 3109.8399999999997, "text": " The hash function implemented in the person class will be of poor quality and the hash", "tokens": [50796, 440, 22019, 2445, 12270, 294, 264, 954, 1508, 486, 312, 295, 4716, 3125, 293, 264, 22019, 51072], "temperature": 0.0, "avg_logprob": -0.11133508904035701, "compression_ratio": 1.6608695652173913, "no_speech_prob": 0.0016228347085416317}, {"id": 482, "seek": 309568, "start": 3109.8399999999997, "end": 3116.0, "text": " based data structure, so like a hash map, that use it will experience many collisions. This can", "tokens": [51072, 2361, 1412, 3877, 11, 370, 411, 257, 22019, 4471, 11, 300, 764, 309, 486, 1752, 867, 46537, 13, 639, 393, 51380], "temperature": 0.0, "avg_logprob": -0.11133508904035701, "compression_ratio": 1.6608695652173913, "no_speech_prob": 0.0016228347085416317}, {"id": 483, "seek": 309568, "start": 3116.0, "end": 3121.04, "text": " lead to poor performance and even course programs that should run in linear time to run in quadratic", "tokens": [51380, 1477, 281, 4716, 3389, 293, 754, 1164, 4268, 300, 820, 1190, 294, 8213, 565, 281, 1190, 294, 37262, 51632], "temperature": 0.0, "avg_logprob": -0.11133508904035701, "compression_ratio": 1.6608695652173913, "no_speech_prob": 0.0016228347085416317}, {"id": 484, "seek": 312104, "start": 3121.04, "end": 3126.72, "text": " time as the book suggests. So I'll propose ignoring significant fields like what we've seen here.", "tokens": [50364, 565, 382, 264, 1446, 13409, 13, 407, 286, 603, 17421, 26258, 4776, 7909, 411, 437, 321, 600, 1612, 510, 13, 50648], "temperature": 0.0, "avg_logprob": -0.23222997818870106, "compression_ratio": 1.5217391304347827, "no_speech_prob": 0.014725836925208569}, {"id": 485, "seek": 312104, "start": 3127.2799999999997, "end": 3134.56, "text": " One example, just like a real world implementation is prior to Java 2, the string hash function", "tokens": [50676, 1485, 1365, 11, 445, 411, 257, 957, 1002, 11420, 307, 4059, 281, 10745, 568, 11, 264, 6798, 22019, 2445, 51040], "temperature": 0.0, "avg_logprob": -0.23222997818870106, "compression_ratio": 1.5217391304347827, "no_speech_prob": 0.014725836925208569}, {"id": 486, "seek": 312104, "start": 3134.56, "end": 3143.36, "text": " in fact used at most 16 evenly spread characters to calculate the hash code. However, unfortunately,", "tokens": [51040, 294, 1186, 1143, 412, 881, 3165, 17658, 3974, 4342, 281, 8873, 264, 22019, 3089, 13, 2908, 11, 7015, 11, 51480], "temperature": 0.0, "avg_logprob": -0.23222997818870106, "compression_ratio": 1.5217391304347827, "no_speech_prob": 0.014725836925208569}, {"id": 487, "seek": 312104, "start": 3144.0, "end": 3150.8, "text": " any string with more than 16 characters, as he said he has things such as URLs, would give", "tokens": [51512, 604, 6798, 365, 544, 813, 3165, 4342, 11, 382, 415, 848, 415, 575, 721, 1270, 382, 43267, 11, 576, 976, 51852], "temperature": 0.0, "avg_logprob": -0.23222997818870106, "compression_ratio": 1.5217391304347827, "no_speech_prob": 0.014725836925208569}, {"id": 488, "seek": 315104, "start": 3151.04, "end": 3157.92, "text": " a low quality hash function or a poor hash function essentially, as the name suggests.", "tokens": [50364, 257, 2295, 3125, 22019, 2445, 420, 257, 4716, 22019, 2445, 4476, 11, 382, 264, 1315, 13409, 13, 50708], "temperature": 0.0, "avg_logprob": -0.19889088798971738, "compression_ratio": 1.6574074074074074, "no_speech_prob": 0.0017006335547193885}, {"id": 489, "seek": 315104, "start": 3158.88, "end": 3164.88, "text": " And that would be bad, that's the point I'm trying to make here. So keep the hash function", "tokens": [50756, 400, 300, 576, 312, 1578, 11, 300, 311, 264, 935, 286, 478, 1382, 281, 652, 510, 13, 407, 1066, 264, 22019, 2445, 51056], "temperature": 0.0, "avg_logprob": -0.19889088798971738, "compression_ratio": 1.6574074074074074, "no_speech_prob": 0.0017006335547193885}, {"id": 490, "seek": 315104, "start": 3164.88, "end": 3169.84, "text": " flexible and open for future change, which is kind of, isn't that one pattern in the", "tokens": [51056, 11358, 293, 1269, 337, 2027, 1319, 11, 597, 307, 733, 295, 11, 1943, 380, 300, 472, 5102, 294, 264, 51304], "temperature": 0.0, "avg_logprob": -0.19889088798971738, "compression_ratio": 1.6574074074074074, "no_speech_prob": 0.0017006335547193885}, {"id": 491, "seek": 315104, "start": 3169.84, "end": 3176.48, "text": " solid principles in object oriented programming. So make sure that's flexible and not static in", "tokens": [51304, 5100, 9156, 294, 2657, 21841, 9410, 13, 407, 652, 988, 300, 311, 11358, 293, 406, 13437, 294, 51636], "temperature": 0.0, "avg_logprob": -0.19889088798971738, "compression_ratio": 1.6574074074074074, "no_speech_prob": 0.0017006335547193885}, {"id": 492, "seek": 317648, "start": 3176.48, "end": 3183.2, "text": " like the string hash function prior to Java 2. And then the other important point is don't provide", "tokens": [50364, 411, 264, 6798, 22019, 2445, 4059, 281, 10745, 568, 13, 400, 550, 264, 661, 1021, 935, 307, 500, 380, 2893, 50700], "temperature": 0.0, "avg_logprob": -0.08978427141562276, "compression_ratio": 1.6348547717842323, "no_speech_prob": 0.010327182710170746}, {"id": 493, "seek": 317648, "start": 3183.2, "end": 3189.52, "text": " a detailed specification for the value returned by hash code, so clients can't reasonably depend on", "tokens": [50700, 257, 9942, 31256, 337, 264, 2158, 8752, 538, 22019, 3089, 11, 370, 6982, 393, 380, 23551, 5672, 322, 51016], "temperature": 0.0, "avg_logprob": -0.08978427141562276, "compression_ratio": 1.6348547717842323, "no_speech_prob": 0.010327182710170746}, {"id": 494, "seek": 317648, "start": 3189.52, "end": 3196.0, "text": " it. This gives you the flexibility to change it. Because if you do have a detailed specification,", "tokens": [51016, 309, 13, 639, 2709, 291, 264, 12635, 281, 1319, 309, 13, 1436, 498, 291, 360, 362, 257, 9942, 31256, 11, 51340], "temperature": 0.0, "avg_logprob": -0.08978427141562276, "compression_ratio": 1.6348547717842323, "no_speech_prob": 0.010327182710170746}, {"id": 495, "seek": 317648, "start": 3196.8, "end": 3204.96, "text": " then people might rely on the hashing algorithm when clients use your class and also make it hard", "tokens": [51380, 550, 561, 1062, 10687, 322, 264, 575, 571, 9284, 562, 6982, 764, 428, 1508, 293, 611, 652, 309, 1152, 51788], "temperature": 0.0, "avg_logprob": -0.08978427141562276, "compression_ratio": 1.6348547717842323, "no_speech_prob": 0.010327182710170746}, {"id": 496, "seek": 320496, "start": 3204.96, "end": 3210.16, "text": " to do this open for future change bit, because the hash function isn't flexible. And so an example", "tokens": [50364, 281, 360, 341, 1269, 337, 2027, 1319, 857, 11, 570, 264, 22019, 2445, 1943, 380, 11358, 13, 400, 370, 364, 1365, 50624], "temperature": 0.0, "avg_logprob": -0.12686635707986765, "compression_ratio": 1.8389513108614233, "no_speech_prob": 0.001810030429624021}, {"id": 497, "seek": 320496, "start": 3210.16, "end": 3217.12, "text": " here is in fact the string class where if you can see here, people will rely on this formula for", "tokens": [50624, 510, 307, 294, 1186, 264, 6798, 1508, 689, 498, 291, 393, 536, 510, 11, 561, 486, 10687, 322, 341, 8513, 337, 50972], "temperature": 0.0, "avg_logprob": -0.12686635707986765, "compression_ratio": 1.8389513108614233, "no_speech_prob": 0.001810030429624021}, {"id": 498, "seek": 320496, "start": 3217.12, "end": 3223.12, "text": " calculating the hash function. And that makes this hash code method less flexible. So a good example", "tokens": [50972, 28258, 264, 22019, 2445, 13, 400, 300, 1669, 341, 22019, 3089, 3170, 1570, 11358, 13, 407, 257, 665, 1365, 51272], "temperature": 0.0, "avg_logprob": -0.12686635707986765, "compression_ratio": 1.8389513108614233, "no_speech_prob": 0.001810030429624021}, {"id": 499, "seek": 320496, "start": 3223.12, "end": 3228.48, "text": " would be to keep the hashing algorithm hidden, like what we've seen here where all you see here is", "tokens": [51272, 576, 312, 281, 1066, 264, 575, 571, 9284, 7633, 11, 411, 437, 321, 600, 1612, 510, 689, 439, 291, 536, 510, 307, 51540], "temperature": 0.0, "avg_logprob": -0.12686635707986765, "compression_ratio": 1.8389513108614233, "no_speech_prob": 0.001810030429624021}, {"id": 500, "seek": 320496, "start": 3228.48, "end": 3233.76, "text": " return objects dot hash name and age, we don't know what the hashing algorithm is in the object", "tokens": [51540, 2736, 6565, 5893, 22019, 1315, 293, 3205, 11, 321, 500, 380, 458, 437, 264, 575, 571, 9284, 307, 294, 264, 2657, 51804], "temperature": 0.0, "avg_logprob": -0.12686635707986765, "compression_ratio": 1.8389513108614233, "no_speech_prob": 0.001810030429624021}, {"id": 501, "seek": 323376, "start": 3233.76, "end": 3242.7200000000003, "text": " dot hash. So that's what he means by not not being explicit or specifying how the the hash", "tokens": [50364, 5893, 22019, 13, 407, 300, 311, 437, 415, 1355, 538, 406, 406, 885, 13691, 420, 1608, 5489, 577, 264, 264, 22019, 50812], "temperature": 0.0, "avg_logprob": -0.11024864432737999, "compression_ratio": 1.7212389380530972, "no_speech_prob": 0.008061607368290424}, {"id": 502, "seek": 323376, "start": 3242.7200000000003, "end": 3249.6800000000003, "text": " code is calculated. Because as he states, if you leave the details unspecified, and a flow is found", "tokens": [50812, 3089, 307, 15598, 13, 1436, 382, 415, 4368, 11, 498, 291, 1856, 264, 4365, 2693, 494, 66, 2587, 11, 293, 257, 3095, 307, 1352, 51160], "temperature": 0.0, "avg_logprob": -0.11024864432737999, "compression_ratio": 1.7212389380530972, "no_speech_prob": 0.008061607368290424}, {"id": 503, "seek": 323376, "start": 3249.6800000000003, "end": 3255.6800000000003, "text": " in the hash function, or a better hash function is discovered, you can change it in subsequent in a", "tokens": [51160, 294, 264, 22019, 2445, 11, 420, 257, 1101, 22019, 2445, 307, 6941, 11, 291, 393, 1319, 309, 294, 19962, 294, 257, 51460], "temperature": 0.0, "avg_logprob": -0.11024864432737999, "compression_ratio": 1.7212389380530972, "no_speech_prob": 0.008061607368290424}, {"id": 504, "seek": 323376, "start": 3255.6800000000003, "end": 3262.96, "text": " subsequent release. And then to end, he states, in summary, you must override hash code every time", "tokens": [51460, 19962, 4374, 13, 400, 550, 281, 917, 11, 415, 4368, 11, 294, 12691, 11, 291, 1633, 42321, 22019, 3089, 633, 565, 51824], "temperature": 0.0, "avg_logprob": -0.11024864432737999, "compression_ratio": 1.7212389380530972, "no_speech_prob": 0.008061607368290424}, {"id": 505, "seek": 326296, "start": 3263.04, "end": 3270.2400000000002, "text": " you override equals, or your program will not run correctly. Your hash code method must obey", "tokens": [50368, 291, 42321, 6915, 11, 420, 428, 1461, 486, 406, 1190, 8944, 13, 2260, 22019, 3089, 3170, 1633, 19297, 50728], "temperature": 0.0, "avg_logprob": -0.12391944365067915, "compression_ratio": 1.6227272727272728, "no_speech_prob": 0.016401052474975586}, {"id": 506, "seek": 326296, "start": 3270.2400000000002, "end": 3276.48, "text": " the general contract specified an object, that's the object class, and must do a reasonably,", "tokens": [50728, 264, 2674, 4364, 22206, 364, 2657, 11, 300, 311, 264, 2657, 1508, 11, 293, 1633, 360, 257, 23551, 11, 51040], "temperature": 0.0, "avg_logprob": -0.12391944365067915, "compression_ratio": 1.6227272727272728, "no_speech_prob": 0.016401052474975586}, {"id": 507, "seek": 326296, "start": 3277.52, "end": 3282.8, "text": " and must do a reasonable job assigning unequal hash codes to unequal instances.", "tokens": [51092, 293, 1633, 360, 257, 10585, 1691, 49602, 2251, 22345, 22019, 14211, 281, 2251, 22345, 14519, 13, 51356], "temperature": 0.0, "avg_logprob": -0.12391944365067915, "compression_ratio": 1.6227272727272728, "no_speech_prob": 0.016401052474975586}, {"id": 508, "seek": 326296, "start": 3284.16, "end": 3290.32, "text": " That was also a bit of a long item. And I did feel like I can't mess up a few parts in that", "tokens": [51424, 663, 390, 611, 257, 857, 295, 257, 938, 3174, 13, 400, 286, 630, 841, 411, 286, 393, 380, 2082, 493, 257, 1326, 3166, 294, 300, 51732], "temperature": 0.0, "avg_logprob": -0.12391944365067915, "compression_ratio": 1.6227272727272728, "no_speech_prob": 0.016401052474975586}, {"id": 509, "seek": 329032, "start": 3290.32, "end": 3296.7200000000003, "text": " item. Got feeling. I don't know what I'm probably see when I'm editing the video. If I did, I", "tokens": [50364, 3174, 13, 5803, 2633, 13, 286, 500, 380, 458, 437, 286, 478, 1391, 536, 562, 286, 478, 10000, 264, 960, 13, 759, 286, 630, 11, 286, 50684], "temperature": 0.0, "avg_logprob": -0.13549830181763903, "compression_ratio": 1.5403225806451613, "no_speech_prob": 0.05031711608171463}, {"id": 510, "seek": 329032, "start": 3296.7200000000003, "end": 3302.48, "text": " apologize. Please double check everything I've stated here. And if I made some significant", "tokens": [50684, 12328, 13, 2555, 3834, 1520, 1203, 286, 600, 11323, 510, 13, 400, 498, 286, 1027, 512, 4776, 50972], "temperature": 0.0, "avg_logprob": -0.13549830181763903, "compression_ratio": 1.5403225806451613, "no_speech_prob": 0.05031711608171463}, {"id": 511, "seek": 329032, "start": 3302.48, "end": 3307.6000000000004, "text": " blunders, I'll try and correct them in some way, either in the description or in the video itself.", "tokens": [50972, 888, 997, 433, 11, 286, 603, 853, 293, 3006, 552, 294, 512, 636, 11, 2139, 294, 264, 3855, 420, 294, 264, 960, 2564, 13, 51228], "temperature": 0.0, "avg_logprob": -0.13549830181763903, "compression_ratio": 1.5403225806451613, "no_speech_prob": 0.05031711608171463}, {"id": 512, "seek": 329032, "start": 3308.96, "end": 3316.32, "text": " Nonetheless, I'm going to try my best. Because that's all one could do. Appreciate it. I shall see", "tokens": [51296, 45437, 11, 286, 478, 516, 281, 853, 452, 1151, 13, 1436, 300, 311, 439, 472, 727, 360, 13, 37601, 309, 13, 286, 4393, 536, 51664], "temperature": 0.0, "avg_logprob": -0.13549830181763903, "compression_ratio": 1.5403225806451613, "no_speech_prob": 0.05031711608171463}, {"id": 513, "seek": 331632, "start": 3316.32, "end": 3320.6400000000003, "text": " you in the next one. Item number 12, which I've already started on. I don't know why I'm doing", "tokens": [50364, 291, 294, 264, 958, 472, 13, 31066, 1230, 2272, 11, 597, 286, 600, 1217, 1409, 322, 13, 286, 500, 380, 458, 983, 286, 478, 884, 50580], "temperature": 0.0, "avg_logprob": -0.1822101620660312, "compression_ratio": 1.5272727272727273, "no_speech_prob": 0.007934770546853542}, {"id": 514, "seek": 331632, "start": 3320.6400000000003, "end": 3325.28, "text": " this if you can see the book. Okay, there's the evidence that I've started on item number 12,", "tokens": [50580, 341, 498, 291, 393, 536, 264, 1446, 13, 1033, 11, 456, 311, 264, 4467, 300, 286, 600, 1409, 322, 3174, 1230, 2272, 11, 50812], "temperature": 0.0, "avg_logprob": -0.1822101620660312, "compression_ratio": 1.5272727272727273, "no_speech_prob": 0.007934770546853542}, {"id": 515, "seek": 331632, "start": 3325.28, "end": 3332.2400000000002, "text": " which states always override to string. So in that one, cheers.", "tokens": [50812, 597, 4368, 1009, 42321, 281, 6798, 13, 407, 294, 300, 472, 11, 15301, 13, 51160], "temperature": 0.0, "avg_logprob": -0.1822101620660312, "compression_ratio": 1.5272727272727273, "no_speech_prob": 0.007934770546853542}], "language": "en"}