1
00:00:00,000 --> 00:00:04,000
These are the graphics from Unreal Engine 5.

2
00:00:04,000 --> 00:00:07,000
They're quite good.

3
00:00:16,000 --> 00:00:19,000
The engine is simulating physics so well,

4
00:00:19,000 --> 00:00:22,000
sometimes it can be hard to tell the difference from reality.

5
00:00:24,000 --> 00:00:28,000
And to accomplish this, something that is absolutely essential

6
00:00:28,000 --> 00:00:32,000
is numerical linear algebra, or NLA,

7
00:00:32,000 --> 00:00:35,000
which is a study of linear algebra applied with computers,

8
00:00:35,000 --> 00:00:40,000
and is better labeled applied linear algebra because that's really all it is.

9
00:00:40,000 --> 00:00:44,000
Now without it, there are no good computer graphics.

10
00:00:44,000 --> 00:00:48,000
Mapping 3D objects to the 2D view screen, that's gone.

11
00:00:48,000 --> 00:00:51,000
Rotating those 3D objects, gone.

12
00:00:51,000 --> 00:00:57,000
Rasterization, where triangle vertices are used to determine pixel color intensities, also gone.

13
00:00:58,000 --> 00:01:04,000
But seriously, without NLA, a lack of video games would be the least of our problems.

14
00:01:04,000 --> 00:01:08,000
We also wouldn't have weather forecasting, many types of data compression,

15
00:01:08,000 --> 00:01:10,000
finite element methods for stress testing,

16
00:01:10,000 --> 00:01:13,000
compressed sensing for MRI, fluid dynamic simulations,

17
00:01:13,000 --> 00:01:17,000
recommendation systems, search, and of course deep neural networks.

18
00:01:19,000 --> 00:01:23,000
An argument from one of the leaders of the field, the late Gene Gulub,

19
00:01:23,000 --> 00:01:28,000
was to point out that of the top 10 most important algorithms of the 20th century,

20
00:01:28,000 --> 00:01:31,000
six of them related to numerical linear algebra.

21
00:01:31,000 --> 00:01:33,000
And that was almost 20 years ago.

22
00:01:35,000 --> 00:01:38,000
Now I decided to make this video when I stumbled upon this paper,

23
00:01:38,000 --> 00:01:41,000
Randomized Numerical Linear Algebra.

24
00:01:41,000 --> 00:01:44,000
In it I found some remarkable claims.

25
00:01:44,000 --> 00:01:51,000
They show how some well-placed randomness can speed up some NLA algorithms by an order of magnitude.

26
00:01:51,000 --> 00:01:56,000
In other words, we're looking at a potential breakthrough in scientific computing.

27
00:01:56,000 --> 00:01:59,000
And randomization for speed seems like a really bizarre idea.

28
00:01:59,000 --> 00:02:03,000
Why would an algorithm that wants to compute something exactly be super sped up

29
00:02:03,000 --> 00:02:05,000
by doing something akin to coin flipping?

30
00:02:05,000 --> 00:02:09,000
Now to fully appreciate this, we'll need some background.

31
00:02:09,000 --> 00:02:16,000
But first, this episode is brought to you by True Theta, the Data Science Consultancy.

32
00:02:16,000 --> 00:02:19,000
More about us at the end of the episode.

33
00:02:20,000 --> 00:02:24,000
The first thing we need to ask is, what is linear algebra?

34
00:02:24,000 --> 00:02:29,000
Simply put, it's the mathematics of vectors and matrices.

35
00:02:29,000 --> 00:02:31,000
We'll start with a vector.

36
00:02:31,000 --> 00:02:36,000
A vector is just a list of numbers and can be imagined geometrically as an arrow.

37
00:02:36,000 --> 00:02:39,000
Now, this is just a wimpy 2D vector.

38
00:02:39,000 --> 00:02:45,000
The powerful stuff comes from using n-dimensional vectors, which can represent complicated things.

39
00:02:45,000 --> 00:02:52,000
Everything from images to words to audio to a person's credit profile.

40
00:02:52,000 --> 00:02:57,000
Basically, most things you can think of can be represented as a vector.

41
00:02:57,000 --> 00:03:00,000
Now we consider a matrix.

42
00:03:00,000 --> 00:03:03,000
On the surface, a matrix is just a grid of numbers.

43
00:03:03,000 --> 00:03:07,000
Or you can consider it a list of vectors.

44
00:03:07,000 --> 00:03:11,000
As a list of vectors, you can imagine it as a bunch of arrows.

45
00:03:11,000 --> 00:03:16,000
When a matrix represents data, they're commonly represented as points.

46
00:03:16,000 --> 00:03:20,000
But in linear algebra, matrices represent something else.

47
00:03:20,000 --> 00:03:22,000
They relate to functions.

48
00:03:22,000 --> 00:03:26,000
In our case, a function is something that receives an input vector x

49
00:03:26,000 --> 00:03:31,000
and gives back a vector y, possibly of a different dimension.

50
00:03:31,000 --> 00:03:35,000
And it needs to do that for a range of x values.

51
00:03:35,000 --> 00:03:41,000
If we were to increase the dimensions, the function would be connecting exponentially huge regions.

52
00:03:41,000 --> 00:03:45,000
In this sense, functions are very information-rich objects,

53
00:03:45,000 --> 00:03:49,000
since they represent an enormous set of connections.

54
00:03:49,000 --> 00:03:53,000
But handling all that information explicitly is often impossible.

55
00:03:53,000 --> 00:03:57,000
So, we make a fantastically liberating assumption.

56
00:03:57,000 --> 00:03:59,000
We assume the function is linear,

57
00:03:59,000 --> 00:04:05,000
which means it's like a line, or a plane, or something similarly flat in higher dimensions.

58
00:04:05,000 --> 00:04:11,000
More rigorously, linearity comes from some algebraic properties, but we don't need to get into all that.

59
00:04:11,000 --> 00:04:14,000
What's important is why linearity is useful.

60
00:04:14,000 --> 00:04:19,000
Consider two cases, where the inputs and outputs are both one-dimensional.

61
00:04:19,000 --> 00:04:26,000
In both cases, suppose we'd like to guess the function, after being shown two input-output pairs.

62
00:04:26,000 --> 00:04:30,000
Now, on the left, we don't make any assumptions about the function.

63
00:04:30,000 --> 00:04:32,000
It can be any function.

64
00:04:32,000 --> 00:04:35,000
On the right, we assume the function is linear.

65
00:04:35,000 --> 00:04:38,000
With this, can we determine the functions?

66
00:04:38,000 --> 00:04:42,000
Well, when we assume linearity, yes, it's easy.

67
00:04:42,000 --> 00:04:46,000
There's only one linear function that goes through these two points.

68
00:04:46,000 --> 00:04:51,000
But without the assumption, there are infinite functions that pass through those two points.

69
00:04:51,000 --> 00:04:53,000
And we have no way to pick one.

70
00:04:53,000 --> 00:04:56,000
This is totally essential.

71
00:04:56,000 --> 00:05:03,000
Linearity allows us to determine these information-rich objects' functions with very little.

72
00:05:03,000 --> 00:05:09,000
Basically, to know a linear function in one region is to know it in all regions.

73
00:05:09,000 --> 00:05:16,000
This fact is so useful, much of applied mathematics involves making the linearity assumption wherever we can get away with it.

74
00:05:16,000 --> 00:05:21,000
Doing so allows us to extrapolate in space, or time, or something else.

75
00:05:21,000 --> 00:05:28,000
And crucially, it makes the exceptionally powerful theorems of linear algebra available to the problem at hand.

76
00:05:28,000 --> 00:05:35,000
If we assume linearity, we essentially get an instruction manual for computing useful things.

77
00:05:35,000 --> 00:05:38,000
So how does a linear function relate to a matrix?

78
00:05:38,000 --> 00:05:47,000
Simply, if f of x is a linear function, then f of x equals a times x for some matrix A.

79
00:05:47,000 --> 00:05:55,000
Saying A times x is to do matrix multiplication, which is a simple but tedious calculation of sums and products.

80
00:05:55,000 --> 00:05:57,000
Please don't think about the details.

81
00:05:57,000 --> 00:06:02,000
Okay, now we can ask, what is numerical linear algebra?

82
00:06:02,000 --> 00:06:08,000
Well, that's the study of applying linear algebra fast and efficiently with computers.

83
00:06:08,000 --> 00:06:11,000
This brings two fundamental challenges.

84
00:06:11,000 --> 00:06:14,000
The first is the computer's finite precision.

85
00:06:14,000 --> 00:06:18,000
The second is that fast algorithms are machine dependent.

86
00:06:18,000 --> 00:06:21,000
Let's start with the computer's finite precision.

87
00:06:21,000 --> 00:06:27,000
If a point is selected at random, it almost surely can't be represented exactly in the machine.

88
00:06:27,000 --> 00:06:31,000
The machine will have to truncate it to an approximation, creating a small error.

89
00:06:31,000 --> 00:06:36,000
This fact, as harmless as it seems, hardly complicates everything.

90
00:06:36,000 --> 00:06:41,000
One effect is, to a computer, addition isn't always associative.

91
00:06:41,000 --> 00:06:49,000
Sometimes, adding numbers A and B before adding to C doesn't give the same answer as summing B and C first.

92
00:06:49,000 --> 00:06:54,000
And when an algorithm fails to anticipate issues like this, it's prone to incorrect answers.

93
00:06:54,000 --> 00:06:57,000
We say such algorithms are numerically unstable.

94
00:06:57,000 --> 00:07:03,000
Now, we consider the second challenge, that fast algorithms are machine dependent.

95
00:07:03,000 --> 00:07:07,000
Let's say we want to multiply two matrices A and B.

96
00:07:07,000 --> 00:07:19,000
By how matrix multiplication is defined, the resulting matrix is the sum product between every row of A with every column of B.

97
00:07:19,000 --> 00:07:22,000
That's how it's defined, but how is it computed?

98
00:07:22,000 --> 00:07:29,000
Well, it helps to imagine it like this, where we are looking to fill up the cells of this matrix C.

99
00:07:29,000 --> 00:07:34,000
Each cell corresponds to a combination of a row of A with a column of B.

100
00:07:34,000 --> 00:07:37,000
From here, there are several ways we could order the computation.

101
00:07:37,000 --> 00:07:43,000
One way is, we could finish the computation of each cell of C before moving on to the next cell.

102
00:07:43,000 --> 00:07:48,000
That's probably how you do it after you heard my definition.

103
00:07:48,000 --> 00:07:50,000
But there's another way.

104
00:07:50,000 --> 00:07:57,000
We could proceed like this, finishing the use of each column of A and row of B,

105
00:07:57,000 --> 00:08:03,000
which is swapped from how I define matrix multiplication, but gives an equivalent answer.

106
00:08:03,000 --> 00:08:06,000
And there are other ways of ordering operations.

107
00:08:06,000 --> 00:08:11,000
The problem is, ordering makes a difference for the algorithm's speed.

108
00:08:11,000 --> 00:08:15,000
They bring different amounts of data movement and differing data accessing patterns.

109
00:08:15,000 --> 00:08:22,000
And this speed depends, at the least, on A and B's dimensions, how they are physically stored in memory,

110
00:08:22,000 --> 00:08:29,000
and how much can be stored in a processor's registries, which are very small but provide extremely fast access.

111
00:08:29,000 --> 00:08:32,000
And this is just one flavor of the algorithm's machine dependence.

112
00:08:32,000 --> 00:08:38,000
Everything from the memory layout to the processor to the operating system to the capacity for parallelism

113
00:08:38,000 --> 00:08:40,000
determines the algorithm's speed.

114
00:08:40,000 --> 00:08:48,000
And these things must be anticipated in all their diversity across users and time to design the best NLA algorithm.

115
00:08:48,000 --> 00:08:53,000
As you can imagine, this gets very complicated very quickly.

116
00:08:53,000 --> 00:08:59,000
To appreciate what's offered by randomized NLA, we'll also need some history.

117
00:08:59,000 --> 00:09:03,000
John von Neumann is often named when discussing the origin of the field.

118
00:09:03,000 --> 00:09:11,000
Because in 1947, he and his co-author Herman Goldsten published one of the earliest uses of computers for applying linear algebra.

119
00:09:11,000 --> 00:09:21,000
But this isn't a relevant origin for modern NLA software, since the programming paradigms of the 40s and 50s were just so different, very little of it persists today.

120
00:09:21,000 --> 00:09:27,000
The code was terribly verbose, machine dependent, hard to share, and just an absolute headache to write.

121
00:09:27,000 --> 00:09:37,000
So in 1957, IBM created the Fortran language, designed to ease programming for scientific computing and provide some machine independence.

122
00:09:37,000 --> 00:09:48,000
And in the 1960s, James Hardy Wilkinson and his colleagues collected and published papers on how to apply linear algebra with computers, but not with the Fortran language.

123
00:09:48,000 --> 00:09:54,000
Most of their papers discussed how a specific algorithm could be applied to a category of matrix.

124
00:09:54,000 --> 00:10:02,000
This work earned Wilkinson a Turing Award, and along with Fortran, it created the environment from which modern NLA software was born.

125
00:10:02,000 --> 00:10:11,000
That happened in 1979, when BLOZ came out from the Jet Propulsion Laboratory in California, the basic linear algebra sub-program for Fortran.

126
00:10:11,000 --> 00:10:18,000
It provided a small set of vector operations that were fast and tested across a variety of machines.

127
00:10:18,000 --> 00:10:23,000
Most importantly, they could implement NLA algorithms, like those proposed by Wilkinson.

128
00:10:23,000 --> 00:10:33,000
And so in 1979, LINPAC came out, which was built on top of BLOZ, and had been developed over the previous decade, primarily for supercomputers.

129
00:10:33,000 --> 00:10:40,000
It wasn't the first NLA package, but it was a major step forward in speed, reliability, and distribution across the scientific community.

130
00:10:40,000 --> 00:10:44,000
But at the same time, computer architectures were evolving.

131
00:10:44,000 --> 00:10:53,000
And so BLOZ 2 was released in 1980 form to perform matrix vector operations, which took advantage of the vector processor CPUs of the time.

132
00:10:53,000 --> 00:10:59,000
But architectures changed again, to ones with shared memory cache-based parallel processing.

133
00:10:59,000 --> 00:11:05,000
And so BLOZ 3 was released in 1990, which performed fast matrix-matrix operations.

134
00:11:05,000 --> 00:11:11,000
And this is a perpetual story. Architectures changed, so BLOZ 1, 2, and 3 need to be updated.

135
00:11:11,000 --> 00:11:14,000
Architectures changed again, and so we need more software updates.

136
00:11:14,000 --> 00:11:20,000
But now, to fully leverage BLOZ 1, 2, and 3, a new NLA package was needed.

137
00:11:20,000 --> 00:11:27,000
In 1992, LAPAC was released, after having been proposed five years earlier.

138
00:11:27,000 --> 00:11:33,000
The authors included some modern NLA heroes, like Jack Dungara and James Demo.

139
00:11:33,000 --> 00:11:43,000
Both have been involved ever since, architecting, writing, standardizing, testing, optimizing, and communicating the software to produce what it is today.

140
00:11:43,000 --> 00:11:52,000
And that's quite a feat. Today in the 2020s, an absolutely enormous amount of linear algebra gets applied with LAPAC or BLOZ.

141
00:11:52,000 --> 00:12:02,000
If you're doing any linear algebra with MATLAB, Python, or C++ or any other language you can name, it's very likely you're using this software or very close derivative of it.

142
00:12:02,000 --> 00:12:07,000
And if you aren't, you're probably doing something wrong.

143
00:12:07,000 --> 00:12:12,000
Okay, and now I need to confess this short history is a major oversimplification.

144
00:12:12,000 --> 00:12:16,000
LAPAC exists in an ecosystem dedicated to scientific computing.

145
00:12:16,000 --> 00:12:22,000
And so my cute linear story fails to represent the entangled, messy truth of the matter.

146
00:12:22,000 --> 00:12:27,000
To mitigate this, I'll give a very quick tour of the current software landscape.

147
00:12:28,000 --> 00:12:39,000
LAPAC doesn't work with distributed memory parallel processing, so we have ScalaPAC relying on PBLOZ, which performs many of the same operations as BLOZ, but executed across a large network of heterogeneous machines.

148
00:12:39,000 --> 00:12:47,000
LAPAC isn't designed for sparse matrices, so there have been efforts to capture the gains of sparsity, but none have been received quite like LAPAC.

149
00:12:47,000 --> 00:12:55,000
LAPAC also isn't designed for GPUs and modern multicore architectures, so we have MAGMA, which enables use of NVIDIA and AMD's fancy GPU hardware.

150
00:12:55,000 --> 00:12:57,000
Speaking of NVIDIA, we also have Kubloss.

151
00:12:57,000 --> 00:13:05,000
See, BLOZ comes with a bunch of knobs that need to be optimized for particular hardware, and so Kubloss has those knobs set for NVIDIA GPUs, among other things.

152
00:13:05,000 --> 00:13:08,000
Also, Apple has Accelerate to do that same tuning for their hardware.

153
00:13:08,000 --> 00:13:14,000
And since tuning knobs for hardware isn't easy, we also have Atlas, which automates some of this tuning.

154
00:13:14,000 --> 00:13:19,000
More recently, there's GPT Tune, which uses Bayesian optimization and Gaussian processes.

155
00:13:19,000 --> 00:13:22,000
If you don't know what those are, I wonder where you could learn about them.

156
00:13:22,000 --> 00:13:26,000
Oh, wait, true theta.io! That must be a great place for data science help.

157
00:13:26,000 --> 00:13:32,000
Moving on, people also want to perform large batches of small and similar NLA operations, so we have batch BLOZ.

158
00:13:32,000 --> 00:13:35,000
I'd like to say that covers it, but it doesn't.

159
00:13:35,000 --> 00:13:41,000
There's a lot of software out there doing NLA, and this list right here is just the free open source stuff.

160
00:13:43,000 --> 00:13:47,000
And now we can finally discuss the paper on randomized numerical linear algebra.

161
00:13:47,000 --> 00:13:51,000
Again, I'm making this video because of some remarkable claims it made.

162
00:13:51,000 --> 00:13:58,000
And it took them especially seriously because it's authored by some of the original developers of BLOZ and LA-PAC, like Jack and James.

163
00:13:58,000 --> 00:14:04,000
In fact, I spoke with the first author Riley Murray to make sure I understood exactly what's going on here.

164
00:14:04,000 --> 00:14:09,000
Now, when reading this, I knew LA-PAC and BLOZ were virtually impossible to dethrone.

165
00:14:09,000 --> 00:14:16,000
Also, from a distance, my impression was the field of NLA had matured, and there probably weren't huge gains to be had.

166
00:14:16,000 --> 00:14:18,000
And I'm not alone in that impression.

167
00:14:18,000 --> 00:14:23,000
To understand this, we need to talk about how we describe an algorithm's efficiency.

168
00:14:23,000 --> 00:14:26,000
Say we're given a matrix A, and it's an N by N matrix.

169
00:14:26,000 --> 00:14:30,000
Let's say we like to multiply it by another matrix B of the same size.

170
00:14:30,000 --> 00:14:36,000
Now, if we were to do this the standard way, that would be what is called an order N cubed algorithm.

171
00:14:36,000 --> 00:14:43,000
That means, as the side length N grows, the number of operations grows like N cubed, roughly.

172
00:14:44,000 --> 00:14:51,000
For example, say multiplying 10 by 10 matrices takes some fraction C of one second.

173
00:14:51,000 --> 00:15:00,000
If we multiply 100 by 100 matrices, then we'd have to wait an amount of time close to that same fraction C, but now of a thousand seconds.

174
00:15:00,000 --> 00:15:07,000
So, we increase the side length by a factor of 10, and the time increased by a factor of a thousand.

175
00:15:07,000 --> 00:15:13,000
Fast hardware can bring down C, whatever it is, but it won't change its painful growth.

176
00:15:13,000 --> 00:15:17,000
However, in 1969, something remarkable did.

177
00:15:17,000 --> 00:15:26,000
Volcker Strassen surprised everyone with an algorithm that does multiplication in a way that grows like N to the 2.8-ish.

178
00:15:26,000 --> 00:15:33,000
This was extremely surprising, since matrix multiplication, the standard way, involves three for loops.

179
00:15:33,000 --> 00:15:39,000
So, the exponent of three seems totally unavoidable, and yet, a lower exponent was possible,

180
00:15:39,000 --> 00:15:43,000
and so the result kicked off research to get that 2.8 exponent down.

181
00:15:43,000 --> 00:15:48,000
It has since leveled out, and it's leveling out where I formed my impression.

182
00:15:48,000 --> 00:15:56,000
In general, for all important matrix algorithms, pushing these exponents significantly further down seems effectively impossible.

183
00:15:56,000 --> 00:16:01,000
So, as I was reading this, I believed gains in speed would not come from fundamentally new algorithms,

184
00:16:01,000 --> 00:16:04,000
but just better scaled-up hardware.

185
00:16:04,000 --> 00:16:07,000
That's expensive, but it seems like it's the only option.

186
00:16:07,000 --> 00:16:11,000
But that understanding changed when I read these two paragraphs.

187
00:16:11,000 --> 00:16:13,000
Here's what it's saying.

188
00:16:13,000 --> 00:16:17,000
A problem you see absolutely everywhere is the problem of least squares.

189
00:16:17,000 --> 00:16:19,000
I'll explain what it is.

190
00:16:19,000 --> 00:16:26,000
We're given a matrix A, which has M rows and N columns, and we'll assume that N is much less than M,

191
00:16:26,000 --> 00:16:28,000
which is actually pretty typical in practice.

192
00:16:28,000 --> 00:16:31,000
We're also given a vector B, which has dimension M.

193
00:16:31,000 --> 00:16:35,000
So, A and B are matrix and a vector that are given to us for this problem.

194
00:16:35,000 --> 00:16:38,000
Now, the goal is going to be to find a vector X.

195
00:16:38,000 --> 00:16:45,000
First, we form AX, which, as mentioned, is a linear function defined by multiplication with A.

196
00:16:45,000 --> 00:16:48,000
Then we consider its distance from B.

197
00:16:48,000 --> 00:16:51,000
That's what this notation means.

198
00:16:51,000 --> 00:16:54,000
Now, our goal is to minimize this distance.

199
00:16:54,000 --> 00:17:01,000
So, in one sentence, our goal is to find X such that AX is as close as possible to the vector B.

200
00:17:01,000 --> 00:17:07,000
Okay, now the best NLA algorithm to solve this involves order MN squared operations.

201
00:17:07,000 --> 00:17:13,000
So, we have an exponent of 2, but at least it's on N and not the much larger M.

202
00:17:13,000 --> 00:17:19,000
Now, what Rand NLA says is, if you're willing to accept a small and controllable error in your answer,

203
00:17:19,000 --> 00:17:24,000
which we'll call epsilon, then randomized algorithms can actually solve this

204
00:17:24,000 --> 00:17:29,000
in order MN log 1 over epsilon plus N cubed operations.

205
00:17:29,000 --> 00:17:33,000
It may not sound like much, but this is huge.

206
00:17:33,000 --> 00:17:37,000
In heavy-duty applications, both M and N can be big.

207
00:17:37,000 --> 00:17:42,000
N might be in the thousands, and M could be in the millions or billions.

208
00:17:42,000 --> 00:17:47,000
Now, if we want a strong approximation, like one within a tenth or a hundredth of a percent,

209
00:17:47,000 --> 00:17:50,000
then this term is going to be in the single digits.

210
00:17:50,000 --> 00:17:55,000
And since M is the big problematic number, then this term likely won't matter much.

211
00:17:55,000 --> 00:17:58,000
So, let's look at the ratio of the dominant terms.

212
00:17:58,000 --> 00:18:03,000
This will give a sense of how many times faster the randomized algorithm is than the classic one.

213
00:18:03,000 --> 00:18:09,000
Things cancel, and now we're looking at a speed-up factor of N over log 1 over epsilon.

214
00:18:09,000 --> 00:18:13,000
If N is in the thousands, and this is in the single digits,

215
00:18:13,000 --> 00:18:18,000
we're looking at a speed-up factor of around a thousand X. That's absurd.

216
00:18:18,000 --> 00:18:23,000
Now, due to some omitted details, we don't actually get a thousand X speed-ups in practice.

217
00:18:23,000 --> 00:18:27,000
However, we do get twenty X, and that's still huge.

218
00:18:27,000 --> 00:18:30,000
If we did the pure parallelization and hardware approach,

219
00:18:30,000 --> 00:18:34,000
recreating that gain might take twenty X the energy or twenty X the cost.

220
00:18:34,000 --> 00:18:38,000
Okay, but such bold claims raise some questions.

221
00:18:38,000 --> 00:18:41,000
First, what are these algorithms doing?

222
00:18:41,000 --> 00:18:43,000
Well, I'll get more into that later.

223
00:18:43,000 --> 00:18:47,000
But to describe it briefly, with high probability,

224
00:18:47,000 --> 00:18:54,000
a random summary of the data massively shrinks the problem while preserving virtually all of the relevant information.

225
00:18:54,000 --> 00:18:56,000
And that raises another question.

226
00:18:56,000 --> 00:19:04,000
A long-standing goal of classic NLA is to compute the most exact answer possible as fast as possible.

227
00:19:04,000 --> 00:19:10,000
That is, get the answer as precisely as the machine will allow and then optimize for speed.

228
00:19:10,000 --> 00:19:17,000
But in randomized NLA, the goal is to compute a close enough answer as fast as possible with high probability.

229
00:19:17,000 --> 00:19:20,000
And this allows for much faster algorithms.

230
00:19:20,000 --> 00:19:24,000
The question is, why are we allowing this new standard?

231
00:19:24,000 --> 00:19:28,000
Well, one motivation is the recent trend in machine learning.

232
00:19:28,000 --> 00:19:31,000
Machine learning accepts that the data is noisy.

233
00:19:31,000 --> 00:19:33,000
The data is just an approximation of the truth.

234
00:19:33,000 --> 00:19:37,000
And so computing things exactly is unnecessary.

235
00:19:37,000 --> 00:19:40,000
The exact answer would change with the change in the meaningless noise.

236
00:19:40,000 --> 00:19:43,000
So an approximate answer is, life be just as good.

237
00:19:43,000 --> 00:19:47,000
And it might even be an exact answer with a different sample of data.

238
00:19:47,000 --> 00:19:50,000
And so this looks a lot more reasonable.

239
00:19:50,000 --> 00:19:57,000
Also, when you look at some algorithms, we find that close enough can sometimes be made extremely close.

240
00:19:57,000 --> 00:20:02,000
And high probability can sometimes be made so high, it's not even worth mentioning.

241
00:20:02,000 --> 00:20:09,000
In general, there's a trade-off between speed and accuracy, and how favorable that trade-off is depends on the algorithm.

242
00:20:09,000 --> 00:20:12,000
Okay, next question.

243
00:20:12,000 --> 00:20:15,000
This argument is just a heuristic illustration.

244
00:20:15,000 --> 00:20:19,000
It's not pointing to an implemented algorithm with measurable performance.

245
00:20:19,000 --> 00:20:22,000
So what is the actual performance?

246
00:20:22,000 --> 00:20:27,000
Well, there are many papers that demonstrate significant concrete improvements.

247
00:20:27,000 --> 00:20:31,000
One striking demonstration I saw came from this paper,

248
00:20:31,000 --> 00:20:35,000
which is actually co-authored by Stephen Brunton and Nathan Kutz,

249
00:20:35,000 --> 00:20:38,000
two researchers who are active educators on YouTube.

250
00:20:38,000 --> 00:20:39,000
You may have seen them.

251
00:20:39,000 --> 00:20:46,000
One of their goals is to improve the SPD algorithm for low-rank matrices in the programming language R.

252
00:20:46,000 --> 00:20:52,000
SPD involves taking a matrix A and decomposing it into a product of three matrices.

253
00:20:52,000 --> 00:20:56,000
And without getting into the details, these matrices have some nice properties

254
00:20:56,000 --> 00:21:00,000
that help us do things like dimensionality reduction or solving least squares problems.

255
00:21:00,000 --> 00:21:05,000
Now, their randomized SPD algorithm gets this performance.

256
00:21:05,000 --> 00:21:10,000
Each plot is for a different size of A, specified as rows by columns.

257
00:21:10,000 --> 00:21:16,000
The y-axis tells us how many times faster an algorithm is than the plain SPD algorithm.

258
00:21:16,000 --> 00:21:21,000
So the plain SPD algorithm itself, its speed is always 1.

259
00:21:21,000 --> 00:21:29,000
That means the randomized SPD algorithm is between 40 and over 100 times faster, depending on the size of A.

260
00:21:29,000 --> 00:21:34,000
And the errors from randomization are comparable to those of the non-randomized routines.

261
00:21:34,000 --> 00:21:39,000
So we're looking at massive real gains, but we need to make some comments.

262
00:21:39,000 --> 00:21:43,000
First, they're only considering low-rank matrices.

263
00:21:43,000 --> 00:21:46,000
Randomized algorithms are especially useful for those.

264
00:21:46,000 --> 00:21:52,000
Second, their algorithm is entirely implemented in R, a high-level language.

265
00:21:52,000 --> 00:21:56,000
In contrast, one of the other benchmarking algorithms, R-Spectra,

266
00:21:56,000 --> 00:22:03,000
provides speed by granting R-axis to Spectra, a C++ library optimized for eigenvalue problems.

267
00:22:03,000 --> 00:22:09,000
So maybe randomization is best coded at the lower level, like that of C++.

268
00:22:09,000 --> 00:22:12,000
So let's check out an algorithm that does that.

269
00:22:12,000 --> 00:22:17,000
This one, Cholesky QR with randomization and pivoting for tall matrices.

270
00:22:17,000 --> 00:22:25,000
This one does an impressive job of wrangling the interplay of hardware, software, and randomization to produce a dominant algorithm.

271
00:22:25,000 --> 00:22:30,000
In fact, it strikes such a favorable balance of speed and accuracy,

272
00:22:30,000 --> 00:22:37,000
the authors claim that the algorithm design question is effectively solved for this class of matrix and problem.

273
00:22:37,000 --> 00:22:39,000
That's quite a claim.

274
00:22:39,000 --> 00:22:41,000
So what's the problem they're solving?

275
00:22:41,000 --> 00:22:47,000
Well, once again, we're given a matrix A, which in this case is assumed to be very tall.

276
00:22:47,000 --> 00:22:52,000
The goal is to decompose it into matrices with certain properties.

277
00:22:52,000 --> 00:22:56,000
Again, explaining the decomposition would take us really far afield.

278
00:22:56,000 --> 00:22:58,000
But here's one important detail.

279
00:22:58,000 --> 00:23:02,000
By including this matrix P, called the permutation matrix,

280
00:23:02,000 --> 00:23:08,000
we're asking the algorithm to order its operations in a special way to improve numerical stability

281
00:23:08,000 --> 00:23:13,000
and, for lack of a better explanation, provide more information on the decomposed matrix.

282
00:23:13,000 --> 00:23:18,000
Including P is significantly more work, but virtually guarantees we'll get the right answer.

283
00:23:18,000 --> 00:23:21,000
That is, it's much more numerically stable.

284
00:23:21,000 --> 00:23:26,000
Doing this decomposition is called QR decomposition with column pivoting.

285
00:23:26,000 --> 00:23:31,000
The QR matrices enable several useful things, like solving least squares problems.

286
00:23:31,000 --> 00:23:35,000
Now to explain their algorithm's performance, here are two blank plots.

287
00:23:35,000 --> 00:23:41,000
On the left, we'll see performance on matrices with about 32,000 rows.

288
00:23:41,000 --> 00:23:44,000
On the right, about 130,000 rows.

289
00:23:44,000 --> 00:23:51,000
Along the horizontal axis, the number of columns varies from about 500 to about 8,000.

290
00:23:51,000 --> 00:23:57,000
So we're looking at A matrices of different shapes, but they're all either tall or really tall matrices.

291
00:23:57,000 --> 00:24:01,000
Now the vertical axis is billions of floating point operations per second,

292
00:24:01,000 --> 00:24:04,000
which is a bit of a weird thing to measure.

293
00:24:04,000 --> 00:24:10,000
But the G-flops are those of a benchmark algorithm run on A, and so it's fixed across algorithms.

294
00:24:10,000 --> 00:24:15,000
In other words, just interpret the vertical axis as relative speed, like in the previous paper.

295
00:24:15,000 --> 00:24:21,000
Okay, now, this is LA-PAC's algorithm for QR with column pivoting.

296
00:24:21,000 --> 00:24:27,000
Remember, LA-PAC is the very well-optimized industry standard, but maybe not for long,

297
00:24:27,000 --> 00:24:32,000
because this is their algorithm, pronounced secret.

298
00:24:32,000 --> 00:24:37,000
As you can see, we're looking at 10 to 20x speed-ups, and if you're wondering what's happening here,

299
00:24:37,000 --> 00:24:44,000
the authors point out that the matrices no longer fit in the cache, so moving more data around becomes necessary.

300
00:24:44,000 --> 00:24:51,000
As an aside, moving data or data communication is a major source of algorithmic slowness.

301
00:24:51,000 --> 00:24:55,000
Now in the paper, the plot shows alternative fast algorithms for a comparison.

302
00:24:55,000 --> 00:25:00,000
They're acknowledging other approaches people might be familiar with to further benchmark their algorithm.

303
00:25:00,000 --> 00:25:08,000
However, in my view, the authors are being a bit modest, because these other fast algorithms aren't doing the full job.

304
00:25:08,000 --> 00:25:10,000
It's not apples to apples.

305
00:25:10,000 --> 00:25:18,000
This one and this one don't perform column pivoting, so the decomposition gives us less information about the matrix.

306
00:25:18,000 --> 00:25:26,000
If we ignore that fact and try to use the decomposition just like it had done pivoting, we get inaccurate or even flat-out wrong answers.

307
00:25:27,000 --> 00:25:36,000
This one and this one are numerically stable, but the former only applies to full-rank matrices, so not all tall, skinny matrices.

308
00:25:36,000 --> 00:25:41,000
And the latter only delivers an implicit representation of the matrices we want.

309
00:25:41,000 --> 00:25:48,000
This means, in many practical cases, we'd need to do extra work, slowing the algorithm down considerably.

310
00:25:48,000 --> 00:25:53,000
All things considered, this plot understates how much better secret really is.

311
00:25:53,000 --> 00:26:01,000
And if you're thinking, okay, but this can't actually replace the LA-PAC algorithm, because randomized algorithms come with some error, right?

312
00:26:01,000 --> 00:26:08,000
Well, in this case, the error can be made so small, it's essentially just as good as the LA-PAC routine.

313
00:26:08,000 --> 00:26:11,000
And this highlights the bizarre magic of randomization.

314
00:26:11,000 --> 00:26:16,000
You add a well-placed pinch of it, and you're able to get essentially the same answer, but many times faster.

315
00:26:16,000 --> 00:26:18,000
It's a trick almost without baggage.

316
00:26:18,000 --> 00:26:23,000
It's not an optimization that only works for a fixed set of machines or a niche class of matrix.

317
00:26:23,000 --> 00:26:25,000
It works essentially across the board.

318
00:26:25,000 --> 00:26:28,000
It makes you wonder, what are these algorithms doing?

319
00:26:28,000 --> 00:26:32,000
Well, RAND-NLA algorithms come in several flavors.

320
00:26:32,000 --> 00:26:35,000
I'll go with one that's especially simple to present.

321
00:26:35,000 --> 00:26:37,000
Let's bring back the least squares problem.

322
00:26:37,000 --> 00:26:44,000
Again, we're looking for the vector x such that ax's distance from b is as small as possible.

323
00:26:44,000 --> 00:26:48,000
We'll call the x vector that achieves this minimum x star.

324
00:26:48,000 --> 00:26:50,000
Again, we'll assume that a is a tall matrix.

325
00:26:50,000 --> 00:26:54,000
One randomized approach is called sketch and solve.

326
00:26:54,000 --> 00:26:57,000
We start by sampling a random matrix s.

327
00:26:57,000 --> 00:27:00,000
How we do that doesn't matter right now.

328
00:27:00,000 --> 00:27:07,000
What's important is that we'll be solving the least squares problem, but we'll replace a with s a and b with s b.

329
00:27:07,000 --> 00:27:13,000
And s will be designed such that s a has many fewer rows than the tall matrix a.

330
00:27:13,000 --> 00:27:16,000
And in the same way, s b is much smaller than b.

331
00:27:16,000 --> 00:27:21,000
Essentially, multiplying by s produces a compressed problem that's much faster to solve.

332
00:27:21,000 --> 00:27:24,000
Doing this is to form a small sketch of the problem.

333
00:27:24,000 --> 00:27:26,000
Here's the remarkable thing.

334
00:27:26,000 --> 00:27:31,000
If we solve this new least squares problem, giving us a vector we'll call x tilde,

335
00:27:31,000 --> 00:27:40,000
then the distance it achieves in the original problem is about the same as the best achievable distance in that original problem with high probability.

336
00:27:40,000 --> 00:27:49,000
In other words, it's very likely that solving this much smaller problem will give an answer that's nearly just as good as what would get solving the original big problem.

337
00:27:49,000 --> 00:27:51,000
And that's great news.

338
00:27:51,000 --> 00:27:53,000
A much smaller problem is much faster to solve.

339
00:27:53,000 --> 00:27:56,000
And that's where we get these order of magnitude speed ups.

340
00:27:56,000 --> 00:28:01,000
And what makes this practically useful is that we can control how good the approximation is.

341
00:28:01,000 --> 00:28:05,000
And the probability that that level of approximation is achieved.

342
00:28:05,000 --> 00:28:13,000
We can say, I want a 99.99% chance that the distance is within 1% of the best achievable distance.

343
00:28:13,000 --> 00:28:18,000
Declaring that will tell us how many rows s needs, depending on how s is randomly sampled.

344
00:28:18,000 --> 00:28:24,000
In fact, developing theorems to make statements like these, that's where a lot of the hard work of the field is.

345
00:28:24,000 --> 00:28:27,000
Further, how s is sampled is a question all its own.

346
00:28:27,000 --> 00:28:32,000
Every value could just be a sample from a normal distribution, but there are fancier techniques,

347
00:28:32,000 --> 00:28:36,000
like ones that allow you to perform the essay multiplication extremely quickly.

348
00:28:36,000 --> 00:28:41,000
Because, in the presentation that I just gave you, doing that multiplication would actually be a dominant cost.

349
00:28:41,000 --> 00:28:46,000
Okay, but this explanation doesn't answer the why does it work question, really.

350
00:28:46,000 --> 00:28:49,000
I'm just showing that it relies on an approximation, which I'm asking you to accept.

351
00:28:49,000 --> 00:28:52,000
So, let's go a little further.

352
00:28:52,000 --> 00:28:54,000
Let's say we're given the following least squares problem.

353
00:28:54,000 --> 00:28:59,000
A is equal to some tall matrix with two columns, and B is equal to some long vector.

354
00:28:59,000 --> 00:29:06,000
Now, if you know something about least squares, you know finding x is going to involve, among other things, the covariance matrix of A.

355
00:29:06,000 --> 00:29:09,000
That's a component of the solution I'd like to focus on.

356
00:29:09,000 --> 00:29:11,000
So, let's ignore B for now.

357
00:29:11,000 --> 00:29:15,000
Now, if you don't know what the covariance matrix of A means, that's fine.

358
00:29:15,000 --> 00:29:18,000
With just two columns, it's a very easy thing to visualize.

359
00:29:18,000 --> 00:29:22,000
What we'll do is plot each row of A as a point.

360
00:29:22,000 --> 00:29:26,000
Now, the covariance matrix just tells you this elliptical shape.

361
00:29:26,000 --> 00:29:30,000
And it's this shape that partially determines the minimizing x.

362
00:29:30,000 --> 00:29:37,000
Next, let's consider SA, where S is properly scaled random Gaussians with D rows.

363
00:29:37,000 --> 00:29:43,000
If D is equal to 8, that means SA can be plotted as 8 summary data points.

364
00:29:43,000 --> 00:29:54,000
Again, we compute the covariance matrix, and again, this shape partially determines the minimizing x, this time in the shrunken, sketched version of the problem.

365
00:29:54,000 --> 00:29:57,000
Now, here's what RAN and LA exploits.

366
00:29:57,000 --> 00:30:04,000
As D increases, the covariances of SA and A become more and more similar.

367
00:30:04,000 --> 00:30:12,000
So, between A and SA, the covariance structures, things that determine the minimizing x's, are very similar.

368
00:30:12,000 --> 00:30:20,000
In other words, as far as this covariance piece of the answer goes, A and SA give us nearly the same thing.

369
00:30:20,000 --> 00:30:24,000
And D doesn't need to be that large to get a good approximation.

370
00:30:24,000 --> 00:30:31,000
So, we can solve a much smaller SA least squares problem and get virtually the same result.

371
00:30:31,000 --> 00:30:33,000
Okay, but what about B?

372
00:30:33,000 --> 00:30:38,000
Yeah, the minimizing x is also determined by the covariance between A and B.

373
00:30:38,000 --> 00:30:44,000
So, let's consider an augmented matrix, which is A concatenated with B as a new column.

374
00:30:44,000 --> 00:30:49,000
The covariance of this new matrix now includes everything that determines the minimizing x.

375
00:30:49,000 --> 00:30:52,000
Looking at this, this is just another matrix.

376
00:30:52,000 --> 00:30:56,000
So, each covariance matrix will be similar to that of its sketched version.

377
00:30:56,000 --> 00:31:02,000
In other words, everything that drives the minimizing x is similar across the original and sketched problem.

378
00:31:02,000 --> 00:31:07,000
That's why the sketched version gives us approximately the same answer.

379
00:31:07,000 --> 00:31:09,000
Now, I need to confess something.

380
00:31:09,000 --> 00:31:14,000
This approximation is actually pretty weak and isn't really what's powering the randomized algorithms.

381
00:31:14,000 --> 00:31:18,000
But it captures the essence and can be animated, so I went with it.

382
00:31:18,000 --> 00:31:25,000
However, Riley pointed out that what's actually happening involves much stronger approximations regarding relative differences.

383
00:31:25,000 --> 00:31:32,000
Now, since defining relative things for matrices involves some head bending, I backed away.

384
00:31:32,000 --> 00:31:37,000
So, this gives a sense of the mathematical properties I play, but it's not the full story.

385
00:31:37,000 --> 00:31:41,000
For a bigger picture, Riley gave me the following analogy.

386
00:31:41,000 --> 00:31:47,000
The strategy of LA-PAC is to cast NLA algorithms into the use of efficient BLOZ functions.

387
00:31:47,000 --> 00:31:51,000
The highly optimized General Matrix Multiply or GEM function.

388
00:31:51,000 --> 00:31:56,000
The more an algorithm can be written as repeated use of this operation, the faster it'll get.

389
00:31:56,000 --> 00:31:58,000
That's what LA-PAC did.

390
00:31:58,000 --> 00:32:02,000
But there's a limit to how much algorithms can be recast into GEM.

391
00:32:02,000 --> 00:32:07,000
But randomization provides a new basket of functions that can be applied in a similar way to GEM.

392
00:32:07,000 --> 00:32:13,000
If least squares is suddenly super efficient, we'll try to reframe everything we can as solving repeated least squares problems.

393
00:32:13,000 --> 00:32:16,000
It's a huge space for creativity and big gains.

394
00:32:16,000 --> 00:32:24,000
And that's why in the Rand-NLA paper, they're talking about a new software, Randblast and Rand-LA-PAC,

395
00:32:24,000 --> 00:32:28,000
which would serve as a new pillar for NLA, the randomized approach.

396
00:32:28,000 --> 00:32:35,000
If they pull it off, and it's really saying something, we'd be in for a widespread upgrade in scientific computing.

397
00:32:35,000 --> 00:32:42,000
All those technologies stand to be improved from gaming to weather forecasting to artificial intelligence.

398
00:32:43,000 --> 00:32:49,000
To be comprehensive, I should mention at least two other approaches to speed.

399
00:32:49,000 --> 00:32:52,000
The first is communication avoiding algorithms.

400
00:32:52,000 --> 00:32:56,000
Algorithms which anticipate the hardware to minimize the amount of data movement.

401
00:32:56,000 --> 00:33:00,000
Since moving data takes so much time, these provide big speed ups as well.

402
00:33:00,000 --> 00:33:03,000
Second, there are hardware accelerators.

403
00:33:03,000 --> 00:33:08,000
Specialized hardware designed to do very specific operations extremely fast.

404
00:33:08,000 --> 00:33:12,000
Now, both of these are totally effective paths to speed,

405
00:33:12,000 --> 00:33:16,000
and in fact, can be combined with randomization to produce even larger gains.

406
00:33:16,000 --> 00:33:20,000
That said, these approaches bring some inflexibility.

407
00:33:20,000 --> 00:33:25,000
Communication avoiding algorithms need to be designed especially carefully to the hardware,

408
00:33:25,000 --> 00:33:29,000
and accelerators only do fixed, highly specialized operations.

409
00:33:29,000 --> 00:33:32,000
You can't change what an accelerator does after it's built.

410
00:33:33,000 --> 00:33:39,000
In comparison, randomized algorithms are exceptional because they are entirely an idea of mathematics.

411
00:33:39,000 --> 00:33:43,000
Randomness isn't upgraded hardware or an algorithm designed for specific hardware,

412
00:33:43,000 --> 00:33:46,000
yet it gives you speed and scalability as though it were.

413
00:33:46,000 --> 00:33:51,000
It does this by allowing simple algorithms, ones that otherwise struggle with scalability,

414
00:33:51,000 --> 00:33:53,000
to be applied to huge data.

415
00:33:53,000 --> 00:33:57,000
And it's this quality of simple but powerful that I believe is necessary

416
00:33:57,000 --> 00:34:01,000
for producing a significant and widespread upgrade in scientific computing.

417
00:34:01,000 --> 00:34:06,000
Since this is evolving, I'm going to keep track of updates as I hear about them

418
00:34:06,000 --> 00:34:10,000
and to the best of my ability on a post on truthata.io.

419
00:34:10,000 --> 00:34:17,000
There I'll also answer some other questions, like why isn't this Monte Carlo or who cares about least squares?

420
00:34:17,000 --> 00:34:21,000
In general, I inevitably learn more about a topic after I publish a video on it,

421
00:34:21,000 --> 00:34:26,000
so this post can evolve as I hear from you, others, or just learn more about it myself.

422
00:34:26,000 --> 00:34:30,000
And wow, what an incredible segue into talking about true theta.

423
00:34:30,000 --> 00:34:33,000
Truthata is my data science consultancy.

424
00:34:33,000 --> 00:34:36,000
We have experienced building machine learning systems for pricing,

425
00:34:36,000 --> 00:34:39,000
credit risk modeling, causal inference, and forecasting.

426
00:34:39,000 --> 00:34:42,000
If you're at a company looking for this type of work, we should talk.

427
00:34:42,000 --> 00:34:46,000
You can send an email to increase at truthata.io to get in touch.

428
00:34:46,000 --> 00:34:48,000
Alright, that's it.

429
00:34:48,000 --> 00:34:52,000
If you'd like to learn more about randomized NLA, I have my sources in the description.

430
00:34:52,000 --> 00:34:57,000
Also, I'd like to make a special thank you to Riley Murray for our discussions on this topic.

431
00:34:57,000 --> 00:35:01,000
And I'd like to thank everyone else who provided useful commentary.

432
00:35:01,000 --> 00:35:05,000
And finally, thank you for watching, and until next time.

