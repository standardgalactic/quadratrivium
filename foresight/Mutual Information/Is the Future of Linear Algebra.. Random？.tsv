start	end	text
0	4000	These are the graphics from Unreal Engine 5.
4000	7000	They're quite good.
16000	19000	The engine is simulating physics so well,
19000	22000	sometimes it can be hard to tell the difference from reality.
24000	28000	And to accomplish this, something that is absolutely essential
28000	32000	is numerical linear algebra, or NLA,
32000	35000	which is a study of linear algebra applied with computers,
35000	40000	and is better labeled applied linear algebra because that's really all it is.
40000	44000	Now without it, there are no good computer graphics.
44000	48000	Mapping 3D objects to the 2D view screen, that's gone.
48000	51000	Rotating those 3D objects, gone.
51000	57000	Rasterization, where triangle vertices are used to determine pixel color intensities, also gone.
58000	64000	But seriously, without NLA, a lack of video games would be the least of our problems.
64000	68000	We also wouldn't have weather forecasting, many types of data compression,
68000	70000	finite element methods for stress testing,
70000	73000	compressed sensing for MRI, fluid dynamic simulations,
73000	77000	recommendation systems, search, and of course deep neural networks.
79000	83000	An argument from one of the leaders of the field, the late Gene Gulub,
83000	88000	was to point out that of the top 10 most important algorithms of the 20th century,
88000	91000	six of them related to numerical linear algebra.
91000	93000	And that was almost 20 years ago.
95000	98000	Now I decided to make this video when I stumbled upon this paper,
98000	101000	Randomized Numerical Linear Algebra.
101000	104000	In it I found some remarkable claims.
104000	111000	They show how some well-placed randomness can speed up some NLA algorithms by an order of magnitude.
111000	116000	In other words, we're looking at a potential breakthrough in scientific computing.
116000	119000	And randomization for speed seems like a really bizarre idea.
119000	123000	Why would an algorithm that wants to compute something exactly be super sped up
123000	125000	by doing something akin to coin flipping?
125000	129000	Now to fully appreciate this, we'll need some background.
129000	136000	But first, this episode is brought to you by True Theta, the Data Science Consultancy.
136000	139000	More about us at the end of the episode.
140000	144000	The first thing we need to ask is, what is linear algebra?
144000	149000	Simply put, it's the mathematics of vectors and matrices.
149000	151000	We'll start with a vector.
151000	156000	A vector is just a list of numbers and can be imagined geometrically as an arrow.
156000	159000	Now, this is just a wimpy 2D vector.
159000	165000	The powerful stuff comes from using n-dimensional vectors, which can represent complicated things.
165000	172000	Everything from images to words to audio to a person's credit profile.
172000	177000	Basically, most things you can think of can be represented as a vector.
177000	180000	Now we consider a matrix.
180000	183000	On the surface, a matrix is just a grid of numbers.
183000	187000	Or you can consider it a list of vectors.
187000	191000	As a list of vectors, you can imagine it as a bunch of arrows.
191000	196000	When a matrix represents data, they're commonly represented as points.
196000	200000	But in linear algebra, matrices represent something else.
200000	202000	They relate to functions.
202000	206000	In our case, a function is something that receives an input vector x
206000	211000	and gives back a vector y, possibly of a different dimension.
211000	215000	And it needs to do that for a range of x values.
215000	221000	If we were to increase the dimensions, the function would be connecting exponentially huge regions.
221000	225000	In this sense, functions are very information-rich objects,
225000	229000	since they represent an enormous set of connections.
229000	233000	But handling all that information explicitly is often impossible.
233000	237000	So, we make a fantastically liberating assumption.
237000	239000	We assume the function is linear,
239000	245000	which means it's like a line, or a plane, or something similarly flat in higher dimensions.
245000	251000	More rigorously, linearity comes from some algebraic properties, but we don't need to get into all that.
251000	254000	What's important is why linearity is useful.
254000	259000	Consider two cases, where the inputs and outputs are both one-dimensional.
259000	266000	In both cases, suppose we'd like to guess the function, after being shown two input-output pairs.
266000	270000	Now, on the left, we don't make any assumptions about the function.
270000	272000	It can be any function.
272000	275000	On the right, we assume the function is linear.
275000	278000	With this, can we determine the functions?
278000	282000	Well, when we assume linearity, yes, it's easy.
282000	286000	There's only one linear function that goes through these two points.
286000	291000	But without the assumption, there are infinite functions that pass through those two points.
291000	293000	And we have no way to pick one.
293000	296000	This is totally essential.
296000	303000	Linearity allows us to determine these information-rich objects' functions with very little.
303000	309000	Basically, to know a linear function in one region is to know it in all regions.
309000	316000	This fact is so useful, much of applied mathematics involves making the linearity assumption wherever we can get away with it.
316000	321000	Doing so allows us to extrapolate in space, or time, or something else.
321000	328000	And crucially, it makes the exceptionally powerful theorems of linear algebra available to the problem at hand.
328000	335000	If we assume linearity, we essentially get an instruction manual for computing useful things.
335000	338000	So how does a linear function relate to a matrix?
338000	347000	Simply, if f of x is a linear function, then f of x equals a times x for some matrix A.
347000	355000	Saying A times x is to do matrix multiplication, which is a simple but tedious calculation of sums and products.
355000	357000	Please don't think about the details.
357000	362000	Okay, now we can ask, what is numerical linear algebra?
362000	368000	Well, that's the study of applying linear algebra fast and efficiently with computers.
368000	371000	This brings two fundamental challenges.
371000	374000	The first is the computer's finite precision.
374000	378000	The second is that fast algorithms are machine dependent.
378000	381000	Let's start with the computer's finite precision.
381000	387000	If a point is selected at random, it almost surely can't be represented exactly in the machine.
387000	391000	The machine will have to truncate it to an approximation, creating a small error.
391000	396000	This fact, as harmless as it seems, hardly complicates everything.
396000	401000	One effect is, to a computer, addition isn't always associative.
401000	409000	Sometimes, adding numbers A and B before adding to C doesn't give the same answer as summing B and C first.
409000	414000	And when an algorithm fails to anticipate issues like this, it's prone to incorrect answers.
414000	417000	We say such algorithms are numerically unstable.
417000	423000	Now, we consider the second challenge, that fast algorithms are machine dependent.
423000	427000	Let's say we want to multiply two matrices A and B.
427000	439000	By how matrix multiplication is defined, the resulting matrix is the sum product between every row of A with every column of B.
439000	442000	That's how it's defined, but how is it computed?
442000	449000	Well, it helps to imagine it like this, where we are looking to fill up the cells of this matrix C.
449000	454000	Each cell corresponds to a combination of a row of A with a column of B.
454000	457000	From here, there are several ways we could order the computation.
457000	463000	One way is, we could finish the computation of each cell of C before moving on to the next cell.
463000	468000	That's probably how you do it after you heard my definition.
468000	470000	But there's another way.
470000	477000	We could proceed like this, finishing the use of each column of A and row of B,
477000	483000	which is swapped from how I define matrix multiplication, but gives an equivalent answer.
483000	486000	And there are other ways of ordering operations.
486000	491000	The problem is, ordering makes a difference for the algorithm's speed.
491000	495000	They bring different amounts of data movement and differing data accessing patterns.
495000	502000	And this speed depends, at the least, on A and B's dimensions, how they are physically stored in memory,
502000	509000	and how much can be stored in a processor's registries, which are very small but provide extremely fast access.
509000	512000	And this is just one flavor of the algorithm's machine dependence.
512000	518000	Everything from the memory layout to the processor to the operating system to the capacity for parallelism
518000	520000	determines the algorithm's speed.
520000	528000	And these things must be anticipated in all their diversity across users and time to design the best NLA algorithm.
528000	533000	As you can imagine, this gets very complicated very quickly.
533000	539000	To appreciate what's offered by randomized NLA, we'll also need some history.
539000	543000	John von Neumann is often named when discussing the origin of the field.
543000	551000	Because in 1947, he and his co-author Herman Goldsten published one of the earliest uses of computers for applying linear algebra.
551000	561000	But this isn't a relevant origin for modern NLA software, since the programming paradigms of the 40s and 50s were just so different, very little of it persists today.
561000	567000	The code was terribly verbose, machine dependent, hard to share, and just an absolute headache to write.
567000	577000	So in 1957, IBM created the Fortran language, designed to ease programming for scientific computing and provide some machine independence.
577000	588000	And in the 1960s, James Hardy Wilkinson and his colleagues collected and published papers on how to apply linear algebra with computers, but not with the Fortran language.
588000	594000	Most of their papers discussed how a specific algorithm could be applied to a category of matrix.
594000	602000	This work earned Wilkinson a Turing Award, and along with Fortran, it created the environment from which modern NLA software was born.
602000	611000	That happened in 1979, when BLOZ came out from the Jet Propulsion Laboratory in California, the basic linear algebra sub-program for Fortran.
611000	618000	It provided a small set of vector operations that were fast and tested across a variety of machines.
618000	623000	Most importantly, they could implement NLA algorithms, like those proposed by Wilkinson.
623000	633000	And so in 1979, LINPAC came out, which was built on top of BLOZ, and had been developed over the previous decade, primarily for supercomputers.
633000	640000	It wasn't the first NLA package, but it was a major step forward in speed, reliability, and distribution across the scientific community.
640000	644000	But at the same time, computer architectures were evolving.
644000	653000	And so BLOZ 2 was released in 1980 form to perform matrix vector operations, which took advantage of the vector processor CPUs of the time.
653000	659000	But architectures changed again, to ones with shared memory cache-based parallel processing.
659000	665000	And so BLOZ 3 was released in 1990, which performed fast matrix-matrix operations.
665000	671000	And this is a perpetual story. Architectures changed, so BLOZ 1, 2, and 3 need to be updated.
671000	674000	Architectures changed again, and so we need more software updates.
674000	680000	But now, to fully leverage BLOZ 1, 2, and 3, a new NLA package was needed.
680000	687000	In 1992, LAPAC was released, after having been proposed five years earlier.
687000	693000	The authors included some modern NLA heroes, like Jack Dungara and James Demo.
693000	703000	Both have been involved ever since, architecting, writing, standardizing, testing, optimizing, and communicating the software to produce what it is today.
703000	712000	And that's quite a feat. Today in the 2020s, an absolutely enormous amount of linear algebra gets applied with LAPAC or BLOZ.
712000	722000	If you're doing any linear algebra with MATLAB, Python, or C++ or any other language you can name, it's very likely you're using this software or very close derivative of it.
722000	727000	And if you aren't, you're probably doing something wrong.
727000	732000	Okay, and now I need to confess this short history is a major oversimplification.
732000	736000	LAPAC exists in an ecosystem dedicated to scientific computing.
736000	742000	And so my cute linear story fails to represent the entangled, messy truth of the matter.
742000	747000	To mitigate this, I'll give a very quick tour of the current software landscape.
748000	759000	LAPAC doesn't work with distributed memory parallel processing, so we have ScalaPAC relying on PBLOZ, which performs many of the same operations as BLOZ, but executed across a large network of heterogeneous machines.
759000	767000	LAPAC isn't designed for sparse matrices, so there have been efforts to capture the gains of sparsity, but none have been received quite like LAPAC.
767000	775000	LAPAC also isn't designed for GPUs and modern multicore architectures, so we have MAGMA, which enables use of NVIDIA and AMD's fancy GPU hardware.
775000	777000	Speaking of NVIDIA, we also have Kubloss.
777000	785000	See, BLOZ comes with a bunch of knobs that need to be optimized for particular hardware, and so Kubloss has those knobs set for NVIDIA GPUs, among other things.
785000	788000	Also, Apple has Accelerate to do that same tuning for their hardware.
788000	794000	And since tuning knobs for hardware isn't easy, we also have Atlas, which automates some of this tuning.
794000	799000	More recently, there's GPT Tune, which uses Bayesian optimization and Gaussian processes.
799000	802000	If you don't know what those are, I wonder where you could learn about them.
802000	806000	Oh, wait, true theta.io! That must be a great place for data science help.
806000	812000	Moving on, people also want to perform large batches of small and similar NLA operations, so we have batch BLOZ.
812000	815000	I'd like to say that covers it, but it doesn't.
815000	821000	There's a lot of software out there doing NLA, and this list right here is just the free open source stuff.
823000	827000	And now we can finally discuss the paper on randomized numerical linear algebra.
827000	831000	Again, I'm making this video because of some remarkable claims it made.
831000	838000	And it took them especially seriously because it's authored by some of the original developers of BLOZ and LA-PAC, like Jack and James.
838000	844000	In fact, I spoke with the first author Riley Murray to make sure I understood exactly what's going on here.
844000	849000	Now, when reading this, I knew LA-PAC and BLOZ were virtually impossible to dethrone.
849000	856000	Also, from a distance, my impression was the field of NLA had matured, and there probably weren't huge gains to be had.
856000	858000	And I'm not alone in that impression.
858000	863000	To understand this, we need to talk about how we describe an algorithm's efficiency.
863000	866000	Say we're given a matrix A, and it's an N by N matrix.
866000	870000	Let's say we like to multiply it by another matrix B of the same size.
870000	876000	Now, if we were to do this the standard way, that would be what is called an order N cubed algorithm.
876000	883000	That means, as the side length N grows, the number of operations grows like N cubed, roughly.
884000	891000	For example, say multiplying 10 by 10 matrices takes some fraction C of one second.
891000	900000	If we multiply 100 by 100 matrices, then we'd have to wait an amount of time close to that same fraction C, but now of a thousand seconds.
900000	907000	So, we increase the side length by a factor of 10, and the time increased by a factor of a thousand.
907000	913000	Fast hardware can bring down C, whatever it is, but it won't change its painful growth.
913000	917000	However, in 1969, something remarkable did.
917000	926000	Volcker Strassen surprised everyone with an algorithm that does multiplication in a way that grows like N to the 2.8-ish.
926000	933000	This was extremely surprising, since matrix multiplication, the standard way, involves three for loops.
933000	939000	So, the exponent of three seems totally unavoidable, and yet, a lower exponent was possible,
939000	943000	and so the result kicked off research to get that 2.8 exponent down.
943000	948000	It has since leveled out, and it's leveling out where I formed my impression.
948000	956000	In general, for all important matrix algorithms, pushing these exponents significantly further down seems effectively impossible.
956000	961000	So, as I was reading this, I believed gains in speed would not come from fundamentally new algorithms,
961000	964000	but just better scaled-up hardware.
964000	967000	That's expensive, but it seems like it's the only option.
967000	971000	But that understanding changed when I read these two paragraphs.
971000	973000	Here's what it's saying.
973000	977000	A problem you see absolutely everywhere is the problem of least squares.
977000	979000	I'll explain what it is.
979000	986000	We're given a matrix A, which has M rows and N columns, and we'll assume that N is much less than M,
986000	988000	which is actually pretty typical in practice.
988000	991000	We're also given a vector B, which has dimension M.
991000	995000	So, A and B are matrix and a vector that are given to us for this problem.
995000	998000	Now, the goal is going to be to find a vector X.
998000	1005000	First, we form AX, which, as mentioned, is a linear function defined by multiplication with A.
1005000	1008000	Then we consider its distance from B.
1008000	1011000	That's what this notation means.
1011000	1014000	Now, our goal is to minimize this distance.
1014000	1021000	So, in one sentence, our goal is to find X such that AX is as close as possible to the vector B.
1021000	1027000	Okay, now the best NLA algorithm to solve this involves order MN squared operations.
1027000	1033000	So, we have an exponent of 2, but at least it's on N and not the much larger M.
1033000	1039000	Now, what Rand NLA says is, if you're willing to accept a small and controllable error in your answer,
1039000	1044000	which we'll call epsilon, then randomized algorithms can actually solve this
1044000	1049000	in order MN log 1 over epsilon plus N cubed operations.
1049000	1053000	It may not sound like much, but this is huge.
1053000	1057000	In heavy-duty applications, both M and N can be big.
1057000	1062000	N might be in the thousands, and M could be in the millions or billions.
1062000	1067000	Now, if we want a strong approximation, like one within a tenth or a hundredth of a percent,
1067000	1070000	then this term is going to be in the single digits.
1070000	1075000	And since M is the big problematic number, then this term likely won't matter much.
1075000	1078000	So, let's look at the ratio of the dominant terms.
1078000	1083000	This will give a sense of how many times faster the randomized algorithm is than the classic one.
1083000	1089000	Things cancel, and now we're looking at a speed-up factor of N over log 1 over epsilon.
1089000	1093000	If N is in the thousands, and this is in the single digits,
1093000	1098000	we're looking at a speed-up factor of around a thousand X. That's absurd.
1098000	1103000	Now, due to some omitted details, we don't actually get a thousand X speed-ups in practice.
1103000	1107000	However, we do get twenty X, and that's still huge.
1107000	1110000	If we did the pure parallelization and hardware approach,
1110000	1114000	recreating that gain might take twenty X the energy or twenty X the cost.
1114000	1118000	Okay, but such bold claims raise some questions.
1118000	1121000	First, what are these algorithms doing?
1121000	1123000	Well, I'll get more into that later.
1123000	1127000	But to describe it briefly, with high probability,
1127000	1134000	a random summary of the data massively shrinks the problem while preserving virtually all of the relevant information.
1134000	1136000	And that raises another question.
1136000	1144000	A long-standing goal of classic NLA is to compute the most exact answer possible as fast as possible.
1144000	1150000	That is, get the answer as precisely as the machine will allow and then optimize for speed.
1150000	1157000	But in randomized NLA, the goal is to compute a close enough answer as fast as possible with high probability.
1157000	1160000	And this allows for much faster algorithms.
1160000	1164000	The question is, why are we allowing this new standard?
1164000	1168000	Well, one motivation is the recent trend in machine learning.
1168000	1171000	Machine learning accepts that the data is noisy.
1171000	1173000	The data is just an approximation of the truth.
1173000	1177000	And so computing things exactly is unnecessary.
1177000	1180000	The exact answer would change with the change in the meaningless noise.
1180000	1183000	So an approximate answer is, life be just as good.
1183000	1187000	And it might even be an exact answer with a different sample of data.
1187000	1190000	And so this looks a lot more reasonable.
1190000	1197000	Also, when you look at some algorithms, we find that close enough can sometimes be made extremely close.
1197000	1202000	And high probability can sometimes be made so high, it's not even worth mentioning.
1202000	1209000	In general, there's a trade-off between speed and accuracy, and how favorable that trade-off is depends on the algorithm.
1209000	1212000	Okay, next question.
1212000	1215000	This argument is just a heuristic illustration.
1215000	1219000	It's not pointing to an implemented algorithm with measurable performance.
1219000	1222000	So what is the actual performance?
1222000	1227000	Well, there are many papers that demonstrate significant concrete improvements.
1227000	1231000	One striking demonstration I saw came from this paper,
1231000	1235000	which is actually co-authored by Stephen Brunton and Nathan Kutz,
1235000	1238000	two researchers who are active educators on YouTube.
1238000	1239000	You may have seen them.
1239000	1246000	One of their goals is to improve the SPD algorithm for low-rank matrices in the programming language R.
1246000	1252000	SPD involves taking a matrix A and decomposing it into a product of three matrices.
1252000	1256000	And without getting into the details, these matrices have some nice properties
1256000	1260000	that help us do things like dimensionality reduction or solving least squares problems.
1260000	1265000	Now, their randomized SPD algorithm gets this performance.
1265000	1270000	Each plot is for a different size of A, specified as rows by columns.
1270000	1276000	The y-axis tells us how many times faster an algorithm is than the plain SPD algorithm.
1276000	1281000	So the plain SPD algorithm itself, its speed is always 1.
1281000	1289000	That means the randomized SPD algorithm is between 40 and over 100 times faster, depending on the size of A.
1289000	1294000	And the errors from randomization are comparable to those of the non-randomized routines.
1294000	1299000	So we're looking at massive real gains, but we need to make some comments.
1299000	1303000	First, they're only considering low-rank matrices.
1303000	1306000	Randomized algorithms are especially useful for those.
1306000	1312000	Second, their algorithm is entirely implemented in R, a high-level language.
1312000	1316000	In contrast, one of the other benchmarking algorithms, R-Spectra,
1316000	1323000	provides speed by granting R-axis to Spectra, a C++ library optimized for eigenvalue problems.
1323000	1329000	So maybe randomization is best coded at the lower level, like that of C++.
1329000	1332000	So let's check out an algorithm that does that.
1332000	1337000	This one, Cholesky QR with randomization and pivoting for tall matrices.
1337000	1345000	This one does an impressive job of wrangling the interplay of hardware, software, and randomization to produce a dominant algorithm.
1345000	1350000	In fact, it strikes such a favorable balance of speed and accuracy,
1350000	1357000	the authors claim that the algorithm design question is effectively solved for this class of matrix and problem.
1357000	1359000	That's quite a claim.
1359000	1361000	So what's the problem they're solving?
1361000	1367000	Well, once again, we're given a matrix A, which in this case is assumed to be very tall.
1367000	1372000	The goal is to decompose it into matrices with certain properties.
1372000	1376000	Again, explaining the decomposition would take us really far afield.
1376000	1378000	But here's one important detail.
1378000	1382000	By including this matrix P, called the permutation matrix,
1382000	1388000	we're asking the algorithm to order its operations in a special way to improve numerical stability
1388000	1393000	and, for lack of a better explanation, provide more information on the decomposed matrix.
1393000	1398000	Including P is significantly more work, but virtually guarantees we'll get the right answer.
1398000	1401000	That is, it's much more numerically stable.
1401000	1406000	Doing this decomposition is called QR decomposition with column pivoting.
1406000	1411000	The QR matrices enable several useful things, like solving least squares problems.
1411000	1415000	Now to explain their algorithm's performance, here are two blank plots.
1415000	1421000	On the left, we'll see performance on matrices with about 32,000 rows.
1421000	1424000	On the right, about 130,000 rows.
1424000	1431000	Along the horizontal axis, the number of columns varies from about 500 to about 8,000.
1431000	1437000	So we're looking at A matrices of different shapes, but they're all either tall or really tall matrices.
1437000	1441000	Now the vertical axis is billions of floating point operations per second,
1441000	1444000	which is a bit of a weird thing to measure.
1444000	1450000	But the G-flops are those of a benchmark algorithm run on A, and so it's fixed across algorithms.
1450000	1455000	In other words, just interpret the vertical axis as relative speed, like in the previous paper.
1455000	1461000	Okay, now, this is LA-PAC's algorithm for QR with column pivoting.
1461000	1467000	Remember, LA-PAC is the very well-optimized industry standard, but maybe not for long,
1467000	1472000	because this is their algorithm, pronounced secret.
1472000	1477000	As you can see, we're looking at 10 to 20x speed-ups, and if you're wondering what's happening here,
1477000	1484000	the authors point out that the matrices no longer fit in the cache, so moving more data around becomes necessary.
1484000	1491000	As an aside, moving data or data communication is a major source of algorithmic slowness.
1491000	1495000	Now in the paper, the plot shows alternative fast algorithms for a comparison.
1495000	1500000	They're acknowledging other approaches people might be familiar with to further benchmark their algorithm.
1500000	1508000	However, in my view, the authors are being a bit modest, because these other fast algorithms aren't doing the full job.
1508000	1510000	It's not apples to apples.
1510000	1518000	This one and this one don't perform column pivoting, so the decomposition gives us less information about the matrix.
1518000	1526000	If we ignore that fact and try to use the decomposition just like it had done pivoting, we get inaccurate or even flat-out wrong answers.
1527000	1536000	This one and this one are numerically stable, but the former only applies to full-rank matrices, so not all tall, skinny matrices.
1536000	1541000	And the latter only delivers an implicit representation of the matrices we want.
1541000	1548000	This means, in many practical cases, we'd need to do extra work, slowing the algorithm down considerably.
1548000	1553000	All things considered, this plot understates how much better secret really is.
1553000	1561000	And if you're thinking, okay, but this can't actually replace the LA-PAC algorithm, because randomized algorithms come with some error, right?
1561000	1568000	Well, in this case, the error can be made so small, it's essentially just as good as the LA-PAC routine.
1568000	1571000	And this highlights the bizarre magic of randomization.
1571000	1576000	You add a well-placed pinch of it, and you're able to get essentially the same answer, but many times faster.
1576000	1578000	It's a trick almost without baggage.
1578000	1583000	It's not an optimization that only works for a fixed set of machines or a niche class of matrix.
1583000	1585000	It works essentially across the board.
1585000	1588000	It makes you wonder, what are these algorithms doing?
1588000	1592000	Well, RAND-NLA algorithms come in several flavors.
1592000	1595000	I'll go with one that's especially simple to present.
1595000	1597000	Let's bring back the least squares problem.
1597000	1604000	Again, we're looking for the vector x such that ax's distance from b is as small as possible.
1604000	1608000	We'll call the x vector that achieves this minimum x star.
1608000	1610000	Again, we'll assume that a is a tall matrix.
1610000	1614000	One randomized approach is called sketch and solve.
1614000	1617000	We start by sampling a random matrix s.
1617000	1620000	How we do that doesn't matter right now.
1620000	1627000	What's important is that we'll be solving the least squares problem, but we'll replace a with s a and b with s b.
1627000	1633000	And s will be designed such that s a has many fewer rows than the tall matrix a.
1633000	1636000	And in the same way, s b is much smaller than b.
1636000	1641000	Essentially, multiplying by s produces a compressed problem that's much faster to solve.
1641000	1644000	Doing this is to form a small sketch of the problem.
1644000	1646000	Here's the remarkable thing.
1646000	1651000	If we solve this new least squares problem, giving us a vector we'll call x tilde,
1651000	1660000	then the distance it achieves in the original problem is about the same as the best achievable distance in that original problem with high probability.
1660000	1669000	In other words, it's very likely that solving this much smaller problem will give an answer that's nearly just as good as what would get solving the original big problem.
1669000	1671000	And that's great news.
1671000	1673000	A much smaller problem is much faster to solve.
1673000	1676000	And that's where we get these order of magnitude speed ups.
1676000	1681000	And what makes this practically useful is that we can control how good the approximation is.
1681000	1685000	And the probability that that level of approximation is achieved.
1685000	1693000	We can say, I want a 99.99% chance that the distance is within 1% of the best achievable distance.
1693000	1698000	Declaring that will tell us how many rows s needs, depending on how s is randomly sampled.
1698000	1704000	In fact, developing theorems to make statements like these, that's where a lot of the hard work of the field is.
1704000	1707000	Further, how s is sampled is a question all its own.
1707000	1712000	Every value could just be a sample from a normal distribution, but there are fancier techniques,
1712000	1716000	like ones that allow you to perform the essay multiplication extremely quickly.
1716000	1721000	Because, in the presentation that I just gave you, doing that multiplication would actually be a dominant cost.
1721000	1726000	Okay, but this explanation doesn't answer the why does it work question, really.
1726000	1729000	I'm just showing that it relies on an approximation, which I'm asking you to accept.
1729000	1732000	So, let's go a little further.
1732000	1734000	Let's say we're given the following least squares problem.
1734000	1739000	A is equal to some tall matrix with two columns, and B is equal to some long vector.
1739000	1746000	Now, if you know something about least squares, you know finding x is going to involve, among other things, the covariance matrix of A.
1746000	1749000	That's a component of the solution I'd like to focus on.
1749000	1751000	So, let's ignore B for now.
1751000	1755000	Now, if you don't know what the covariance matrix of A means, that's fine.
1755000	1758000	With just two columns, it's a very easy thing to visualize.
1758000	1762000	What we'll do is plot each row of A as a point.
1762000	1766000	Now, the covariance matrix just tells you this elliptical shape.
1766000	1770000	And it's this shape that partially determines the minimizing x.
1770000	1777000	Next, let's consider SA, where S is properly scaled random Gaussians with D rows.
1777000	1783000	If D is equal to 8, that means SA can be plotted as 8 summary data points.
1783000	1794000	Again, we compute the covariance matrix, and again, this shape partially determines the minimizing x, this time in the shrunken, sketched version of the problem.
1794000	1797000	Now, here's what RAN and LA exploits.
1797000	1804000	As D increases, the covariances of SA and A become more and more similar.
1804000	1812000	So, between A and SA, the covariance structures, things that determine the minimizing x's, are very similar.
1812000	1820000	In other words, as far as this covariance piece of the answer goes, A and SA give us nearly the same thing.
1820000	1824000	And D doesn't need to be that large to get a good approximation.
1824000	1831000	So, we can solve a much smaller SA least squares problem and get virtually the same result.
1831000	1833000	Okay, but what about B?
1833000	1838000	Yeah, the minimizing x is also determined by the covariance between A and B.
1838000	1844000	So, let's consider an augmented matrix, which is A concatenated with B as a new column.
1844000	1849000	The covariance of this new matrix now includes everything that determines the minimizing x.
1849000	1852000	Looking at this, this is just another matrix.
1852000	1856000	So, each covariance matrix will be similar to that of its sketched version.
1856000	1862000	In other words, everything that drives the minimizing x is similar across the original and sketched problem.
1862000	1867000	That's why the sketched version gives us approximately the same answer.
1867000	1869000	Now, I need to confess something.
1869000	1874000	This approximation is actually pretty weak and isn't really what's powering the randomized algorithms.
1874000	1878000	But it captures the essence and can be animated, so I went with it.
1878000	1885000	However, Riley pointed out that what's actually happening involves much stronger approximations regarding relative differences.
1885000	1892000	Now, since defining relative things for matrices involves some head bending, I backed away.
1892000	1897000	So, this gives a sense of the mathematical properties I play, but it's not the full story.
1897000	1901000	For a bigger picture, Riley gave me the following analogy.
1901000	1907000	The strategy of LA-PAC is to cast NLA algorithms into the use of efficient BLOZ functions.
1907000	1911000	The highly optimized General Matrix Multiply or GEM function.
1911000	1916000	The more an algorithm can be written as repeated use of this operation, the faster it'll get.
1916000	1918000	That's what LA-PAC did.
1918000	1922000	But there's a limit to how much algorithms can be recast into GEM.
1922000	1927000	But randomization provides a new basket of functions that can be applied in a similar way to GEM.
1927000	1933000	If least squares is suddenly super efficient, we'll try to reframe everything we can as solving repeated least squares problems.
1933000	1936000	It's a huge space for creativity and big gains.
1936000	1944000	And that's why in the Rand-NLA paper, they're talking about a new software, Randblast and Rand-LA-PAC,
1944000	1948000	which would serve as a new pillar for NLA, the randomized approach.
1948000	1955000	If they pull it off, and it's really saying something, we'd be in for a widespread upgrade in scientific computing.
1955000	1962000	All those technologies stand to be improved from gaming to weather forecasting to artificial intelligence.
1963000	1969000	To be comprehensive, I should mention at least two other approaches to speed.
1969000	1972000	The first is communication avoiding algorithms.
1972000	1976000	Algorithms which anticipate the hardware to minimize the amount of data movement.
1976000	1980000	Since moving data takes so much time, these provide big speed ups as well.
1980000	1983000	Second, there are hardware accelerators.
1983000	1988000	Specialized hardware designed to do very specific operations extremely fast.
1988000	1992000	Now, both of these are totally effective paths to speed,
1992000	1996000	and in fact, can be combined with randomization to produce even larger gains.
1996000	2000000	That said, these approaches bring some inflexibility.
2000000	2005000	Communication avoiding algorithms need to be designed especially carefully to the hardware,
2005000	2009000	and accelerators only do fixed, highly specialized operations.
2009000	2012000	You can't change what an accelerator does after it's built.
2013000	2019000	In comparison, randomized algorithms are exceptional because they are entirely an idea of mathematics.
2019000	2023000	Randomness isn't upgraded hardware or an algorithm designed for specific hardware,
2023000	2026000	yet it gives you speed and scalability as though it were.
2026000	2031000	It does this by allowing simple algorithms, ones that otherwise struggle with scalability,
2031000	2033000	to be applied to huge data.
2033000	2037000	And it's this quality of simple but powerful that I believe is necessary
2037000	2041000	for producing a significant and widespread upgrade in scientific computing.
2041000	2046000	Since this is evolving, I'm going to keep track of updates as I hear about them
2046000	2050000	and to the best of my ability on a post on truthata.io.
2050000	2057000	There I'll also answer some other questions, like why isn't this Monte Carlo or who cares about least squares?
2057000	2061000	In general, I inevitably learn more about a topic after I publish a video on it,
2061000	2066000	so this post can evolve as I hear from you, others, or just learn more about it myself.
2066000	2070000	And wow, what an incredible segue into talking about true theta.
2070000	2073000	Truthata is my data science consultancy.
2073000	2076000	We have experienced building machine learning systems for pricing,
2076000	2079000	credit risk modeling, causal inference, and forecasting.
2079000	2082000	If you're at a company looking for this type of work, we should talk.
2082000	2086000	You can send an email to increase at truthata.io to get in touch.
2086000	2088000	Alright, that's it.
2088000	2092000	If you'd like to learn more about randomized NLA, I have my sources in the description.
2092000	2097000	Also, I'd like to make a special thank you to Riley Murray for our discussions on this topic.
2097000	2101000	And I'd like to thank everyone else who provided useful commentary.
2101000	2105000	And finally, thank you for watching, and until next time.
