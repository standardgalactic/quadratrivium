WEBVTT

00:00.000 --> 00:04.000
These are the graphics from Unreal Engine 5.

00:04.000 --> 00:07.000
They're quite good.

00:16.000 --> 00:19.000
The engine is simulating physics so well,

00:19.000 --> 00:22.000
sometimes it can be hard to tell the difference from reality.

00:24.000 --> 00:28.000
And to accomplish this, something that is absolutely essential

00:28.000 --> 00:32.000
is numerical linear algebra, or NLA,

00:32.000 --> 00:35.000
which is a study of linear algebra applied with computers,

00:35.000 --> 00:40.000
and is better labeled applied linear algebra because that's really all it is.

00:40.000 --> 00:44.000
Now without it, there are no good computer graphics.

00:44.000 --> 00:48.000
Mapping 3D objects to the 2D view screen, that's gone.

00:48.000 --> 00:51.000
Rotating those 3D objects, gone.

00:51.000 --> 00:57.000
Rasterization, where triangle vertices are used to determine pixel color intensities, also gone.

00:58.000 --> 01:04.000
But seriously, without NLA, a lack of video games would be the least of our problems.

01:04.000 --> 01:08.000
We also wouldn't have weather forecasting, many types of data compression,

01:08.000 --> 01:10.000
finite element methods for stress testing,

01:10.000 --> 01:13.000
compressed sensing for MRI, fluid dynamic simulations,

01:13.000 --> 01:17.000
recommendation systems, search, and of course deep neural networks.

01:19.000 --> 01:23.000
An argument from one of the leaders of the field, the late Gene Gulub,

01:23.000 --> 01:28.000
was to point out that of the top 10 most important algorithms of the 20th century,

01:28.000 --> 01:31.000
six of them related to numerical linear algebra.

01:31.000 --> 01:33.000
And that was almost 20 years ago.

01:35.000 --> 01:38.000
Now I decided to make this video when I stumbled upon this paper,

01:38.000 --> 01:41.000
Randomized Numerical Linear Algebra.

01:41.000 --> 01:44.000
In it I found some remarkable claims.

01:44.000 --> 01:51.000
They show how some well-placed randomness can speed up some NLA algorithms by an order of magnitude.

01:51.000 --> 01:56.000
In other words, we're looking at a potential breakthrough in scientific computing.

01:56.000 --> 01:59.000
And randomization for speed seems like a really bizarre idea.

01:59.000 --> 02:03.000
Why would an algorithm that wants to compute something exactly be super sped up

02:03.000 --> 02:05.000
by doing something akin to coin flipping?

02:05.000 --> 02:09.000
Now to fully appreciate this, we'll need some background.

02:09.000 --> 02:16.000
But first, this episode is brought to you by True Theta, the Data Science Consultancy.

02:16.000 --> 02:19.000
More about us at the end of the episode.

02:20.000 --> 02:24.000
The first thing we need to ask is, what is linear algebra?

02:24.000 --> 02:29.000
Simply put, it's the mathematics of vectors and matrices.

02:29.000 --> 02:31.000
We'll start with a vector.

02:31.000 --> 02:36.000
A vector is just a list of numbers and can be imagined geometrically as an arrow.

02:36.000 --> 02:39.000
Now, this is just a wimpy 2D vector.

02:39.000 --> 02:45.000
The powerful stuff comes from using n-dimensional vectors, which can represent complicated things.

02:45.000 --> 02:52.000
Everything from images to words to audio to a person's credit profile.

02:52.000 --> 02:57.000
Basically, most things you can think of can be represented as a vector.

02:57.000 --> 03:00.000
Now we consider a matrix.

03:00.000 --> 03:03.000
On the surface, a matrix is just a grid of numbers.

03:03.000 --> 03:07.000
Or you can consider it a list of vectors.

03:07.000 --> 03:11.000
As a list of vectors, you can imagine it as a bunch of arrows.

03:11.000 --> 03:16.000
When a matrix represents data, they're commonly represented as points.

03:16.000 --> 03:20.000
But in linear algebra, matrices represent something else.

03:20.000 --> 03:22.000
They relate to functions.

03:22.000 --> 03:26.000
In our case, a function is something that receives an input vector x

03:26.000 --> 03:31.000
and gives back a vector y, possibly of a different dimension.

03:31.000 --> 03:35.000
And it needs to do that for a range of x values.

03:35.000 --> 03:41.000
If we were to increase the dimensions, the function would be connecting exponentially huge regions.

03:41.000 --> 03:45.000
In this sense, functions are very information-rich objects,

03:45.000 --> 03:49.000
since they represent an enormous set of connections.

03:49.000 --> 03:53.000
But handling all that information explicitly is often impossible.

03:53.000 --> 03:57.000
So, we make a fantastically liberating assumption.

03:57.000 --> 03:59.000
We assume the function is linear,

03:59.000 --> 04:05.000
which means it's like a line, or a plane, or something similarly flat in higher dimensions.

04:05.000 --> 04:11.000
More rigorously, linearity comes from some algebraic properties, but we don't need to get into all that.

04:11.000 --> 04:14.000
What's important is why linearity is useful.

04:14.000 --> 04:19.000
Consider two cases, where the inputs and outputs are both one-dimensional.

04:19.000 --> 04:26.000
In both cases, suppose we'd like to guess the function, after being shown two input-output pairs.

04:26.000 --> 04:30.000
Now, on the left, we don't make any assumptions about the function.

04:30.000 --> 04:32.000
It can be any function.

04:32.000 --> 04:35.000
On the right, we assume the function is linear.

04:35.000 --> 04:38.000
With this, can we determine the functions?

04:38.000 --> 04:42.000
Well, when we assume linearity, yes, it's easy.

04:42.000 --> 04:46.000
There's only one linear function that goes through these two points.

04:46.000 --> 04:51.000
But without the assumption, there are infinite functions that pass through those two points.

04:51.000 --> 04:53.000
And we have no way to pick one.

04:53.000 --> 04:56.000
This is totally essential.

04:56.000 --> 05:03.000
Linearity allows us to determine these information-rich objects' functions with very little.

05:03.000 --> 05:09.000
Basically, to know a linear function in one region is to know it in all regions.

05:09.000 --> 05:16.000
This fact is so useful, much of applied mathematics involves making the linearity assumption wherever we can get away with it.

05:16.000 --> 05:21.000
Doing so allows us to extrapolate in space, or time, or something else.

05:21.000 --> 05:28.000
And crucially, it makes the exceptionally powerful theorems of linear algebra available to the problem at hand.

05:28.000 --> 05:35.000
If we assume linearity, we essentially get an instruction manual for computing useful things.

05:35.000 --> 05:38.000
So how does a linear function relate to a matrix?

05:38.000 --> 05:47.000
Simply, if f of x is a linear function, then f of x equals a times x for some matrix A.

05:47.000 --> 05:55.000
Saying A times x is to do matrix multiplication, which is a simple but tedious calculation of sums and products.

05:55.000 --> 05:57.000
Please don't think about the details.

05:57.000 --> 06:02.000
Okay, now we can ask, what is numerical linear algebra?

06:02.000 --> 06:08.000
Well, that's the study of applying linear algebra fast and efficiently with computers.

06:08.000 --> 06:11.000
This brings two fundamental challenges.

06:11.000 --> 06:14.000
The first is the computer's finite precision.

06:14.000 --> 06:18.000
The second is that fast algorithms are machine dependent.

06:18.000 --> 06:21.000
Let's start with the computer's finite precision.

06:21.000 --> 06:27.000
If a point is selected at random, it almost surely can't be represented exactly in the machine.

06:27.000 --> 06:31.000
The machine will have to truncate it to an approximation, creating a small error.

06:31.000 --> 06:36.000
This fact, as harmless as it seems, hardly complicates everything.

06:36.000 --> 06:41.000
One effect is, to a computer, addition isn't always associative.

06:41.000 --> 06:49.000
Sometimes, adding numbers A and B before adding to C doesn't give the same answer as summing B and C first.

06:49.000 --> 06:54.000
And when an algorithm fails to anticipate issues like this, it's prone to incorrect answers.

06:54.000 --> 06:57.000
We say such algorithms are numerically unstable.

06:57.000 --> 07:03.000
Now, we consider the second challenge, that fast algorithms are machine dependent.

07:03.000 --> 07:07.000
Let's say we want to multiply two matrices A and B.

07:07.000 --> 07:19.000
By how matrix multiplication is defined, the resulting matrix is the sum product between every row of A with every column of B.

07:19.000 --> 07:22.000
That's how it's defined, but how is it computed?

07:22.000 --> 07:29.000
Well, it helps to imagine it like this, where we are looking to fill up the cells of this matrix C.

07:29.000 --> 07:34.000
Each cell corresponds to a combination of a row of A with a column of B.

07:34.000 --> 07:37.000
From here, there are several ways we could order the computation.

07:37.000 --> 07:43.000
One way is, we could finish the computation of each cell of C before moving on to the next cell.

07:43.000 --> 07:48.000
That's probably how you do it after you heard my definition.

07:48.000 --> 07:50.000
But there's another way.

07:50.000 --> 07:57.000
We could proceed like this, finishing the use of each column of A and row of B,

07:57.000 --> 08:03.000
which is swapped from how I define matrix multiplication, but gives an equivalent answer.

08:03.000 --> 08:06.000
And there are other ways of ordering operations.

08:06.000 --> 08:11.000
The problem is, ordering makes a difference for the algorithm's speed.

08:11.000 --> 08:15.000
They bring different amounts of data movement and differing data accessing patterns.

08:15.000 --> 08:22.000
And this speed depends, at the least, on A and B's dimensions, how they are physically stored in memory,

08:22.000 --> 08:29.000
and how much can be stored in a processor's registries, which are very small but provide extremely fast access.

08:29.000 --> 08:32.000
And this is just one flavor of the algorithm's machine dependence.

08:32.000 --> 08:38.000
Everything from the memory layout to the processor to the operating system to the capacity for parallelism

08:38.000 --> 08:40.000
determines the algorithm's speed.

08:40.000 --> 08:48.000
And these things must be anticipated in all their diversity across users and time to design the best NLA algorithm.

08:48.000 --> 08:53.000
As you can imagine, this gets very complicated very quickly.

08:53.000 --> 08:59.000
To appreciate what's offered by randomized NLA, we'll also need some history.

08:59.000 --> 09:03.000
John von Neumann is often named when discussing the origin of the field.

09:03.000 --> 09:11.000
Because in 1947, he and his co-author Herman Goldsten published one of the earliest uses of computers for applying linear algebra.

09:11.000 --> 09:21.000
But this isn't a relevant origin for modern NLA software, since the programming paradigms of the 40s and 50s were just so different, very little of it persists today.

09:21.000 --> 09:27.000
The code was terribly verbose, machine dependent, hard to share, and just an absolute headache to write.

09:27.000 --> 09:37.000
So in 1957, IBM created the Fortran language, designed to ease programming for scientific computing and provide some machine independence.

09:37.000 --> 09:48.000
And in the 1960s, James Hardy Wilkinson and his colleagues collected and published papers on how to apply linear algebra with computers, but not with the Fortran language.

09:48.000 --> 09:54.000
Most of their papers discussed how a specific algorithm could be applied to a category of matrix.

09:54.000 --> 10:02.000
This work earned Wilkinson a Turing Award, and along with Fortran, it created the environment from which modern NLA software was born.

10:02.000 --> 10:11.000
That happened in 1979, when BLOZ came out from the Jet Propulsion Laboratory in California, the basic linear algebra sub-program for Fortran.

10:11.000 --> 10:18.000
It provided a small set of vector operations that were fast and tested across a variety of machines.

10:18.000 --> 10:23.000
Most importantly, they could implement NLA algorithms, like those proposed by Wilkinson.

10:23.000 --> 10:33.000
And so in 1979, LINPAC came out, which was built on top of BLOZ, and had been developed over the previous decade, primarily for supercomputers.

10:33.000 --> 10:40.000
It wasn't the first NLA package, but it was a major step forward in speed, reliability, and distribution across the scientific community.

10:40.000 --> 10:44.000
But at the same time, computer architectures were evolving.

10:44.000 --> 10:53.000
And so BLOZ 2 was released in 1980 form to perform matrix vector operations, which took advantage of the vector processor CPUs of the time.

10:53.000 --> 10:59.000
But architectures changed again, to ones with shared memory cache-based parallel processing.

10:59.000 --> 11:05.000
And so BLOZ 3 was released in 1990, which performed fast matrix-matrix operations.

11:05.000 --> 11:11.000
And this is a perpetual story. Architectures changed, so BLOZ 1, 2, and 3 need to be updated.

11:11.000 --> 11:14.000
Architectures changed again, and so we need more software updates.

11:14.000 --> 11:20.000
But now, to fully leverage BLOZ 1, 2, and 3, a new NLA package was needed.

11:20.000 --> 11:27.000
In 1992, LAPAC was released, after having been proposed five years earlier.

11:27.000 --> 11:33.000
The authors included some modern NLA heroes, like Jack Dungara and James Demo.

11:33.000 --> 11:43.000
Both have been involved ever since, architecting, writing, standardizing, testing, optimizing, and communicating the software to produce what it is today.

11:43.000 --> 11:52.000
And that's quite a feat. Today in the 2020s, an absolutely enormous amount of linear algebra gets applied with LAPAC or BLOZ.

11:52.000 --> 12:02.000
If you're doing any linear algebra with MATLAB, Python, or C++ or any other language you can name, it's very likely you're using this software or very close derivative of it.

12:02.000 --> 12:07.000
And if you aren't, you're probably doing something wrong.

12:07.000 --> 12:12.000
Okay, and now I need to confess this short history is a major oversimplification.

12:12.000 --> 12:16.000
LAPAC exists in an ecosystem dedicated to scientific computing.

12:16.000 --> 12:22.000
And so my cute linear story fails to represent the entangled, messy truth of the matter.

12:22.000 --> 12:27.000
To mitigate this, I'll give a very quick tour of the current software landscape.

12:28.000 --> 12:39.000
LAPAC doesn't work with distributed memory parallel processing, so we have ScalaPAC relying on PBLOZ, which performs many of the same operations as BLOZ, but executed across a large network of heterogeneous machines.

12:39.000 --> 12:47.000
LAPAC isn't designed for sparse matrices, so there have been efforts to capture the gains of sparsity, but none have been received quite like LAPAC.

12:47.000 --> 12:55.000
LAPAC also isn't designed for GPUs and modern multicore architectures, so we have MAGMA, which enables use of NVIDIA and AMD's fancy GPU hardware.

12:55.000 --> 12:57.000
Speaking of NVIDIA, we also have Kubloss.

12:57.000 --> 13:05.000
See, BLOZ comes with a bunch of knobs that need to be optimized for particular hardware, and so Kubloss has those knobs set for NVIDIA GPUs, among other things.

13:05.000 --> 13:08.000
Also, Apple has Accelerate to do that same tuning for their hardware.

13:08.000 --> 13:14.000
And since tuning knobs for hardware isn't easy, we also have Atlas, which automates some of this tuning.

13:14.000 --> 13:19.000
More recently, there's GPT Tune, which uses Bayesian optimization and Gaussian processes.

13:19.000 --> 13:22.000
If you don't know what those are, I wonder where you could learn about them.

13:22.000 --> 13:26.000
Oh, wait, true theta.io! That must be a great place for data science help.

13:26.000 --> 13:32.000
Moving on, people also want to perform large batches of small and similar NLA operations, so we have batch BLOZ.

13:32.000 --> 13:35.000
I'd like to say that covers it, but it doesn't.

13:35.000 --> 13:41.000
There's a lot of software out there doing NLA, and this list right here is just the free open source stuff.

13:43.000 --> 13:47.000
And now we can finally discuss the paper on randomized numerical linear algebra.

13:47.000 --> 13:51.000
Again, I'm making this video because of some remarkable claims it made.

13:51.000 --> 13:58.000
And it took them especially seriously because it's authored by some of the original developers of BLOZ and LA-PAC, like Jack and James.

13:58.000 --> 14:04.000
In fact, I spoke with the first author Riley Murray to make sure I understood exactly what's going on here.

14:04.000 --> 14:09.000
Now, when reading this, I knew LA-PAC and BLOZ were virtually impossible to dethrone.

14:09.000 --> 14:16.000
Also, from a distance, my impression was the field of NLA had matured, and there probably weren't huge gains to be had.

14:16.000 --> 14:18.000
And I'm not alone in that impression.

14:18.000 --> 14:23.000
To understand this, we need to talk about how we describe an algorithm's efficiency.

14:23.000 --> 14:26.000
Say we're given a matrix A, and it's an N by N matrix.

14:26.000 --> 14:30.000
Let's say we like to multiply it by another matrix B of the same size.

14:30.000 --> 14:36.000
Now, if we were to do this the standard way, that would be what is called an order N cubed algorithm.

14:36.000 --> 14:43.000
That means, as the side length N grows, the number of operations grows like N cubed, roughly.

14:44.000 --> 14:51.000
For example, say multiplying 10 by 10 matrices takes some fraction C of one second.

14:51.000 --> 15:00.000
If we multiply 100 by 100 matrices, then we'd have to wait an amount of time close to that same fraction C, but now of a thousand seconds.

15:00.000 --> 15:07.000
So, we increase the side length by a factor of 10, and the time increased by a factor of a thousand.

15:07.000 --> 15:13.000
Fast hardware can bring down C, whatever it is, but it won't change its painful growth.

15:13.000 --> 15:17.000
However, in 1969, something remarkable did.

15:17.000 --> 15:26.000
Volcker Strassen surprised everyone with an algorithm that does multiplication in a way that grows like N to the 2.8-ish.

15:26.000 --> 15:33.000
This was extremely surprising, since matrix multiplication, the standard way, involves three for loops.

15:33.000 --> 15:39.000
So, the exponent of three seems totally unavoidable, and yet, a lower exponent was possible,

15:39.000 --> 15:43.000
and so the result kicked off research to get that 2.8 exponent down.

15:43.000 --> 15:48.000
It has since leveled out, and it's leveling out where I formed my impression.

15:48.000 --> 15:56.000
In general, for all important matrix algorithms, pushing these exponents significantly further down seems effectively impossible.

15:56.000 --> 16:01.000
So, as I was reading this, I believed gains in speed would not come from fundamentally new algorithms,

16:01.000 --> 16:04.000
but just better scaled-up hardware.

16:04.000 --> 16:07.000
That's expensive, but it seems like it's the only option.

16:07.000 --> 16:11.000
But that understanding changed when I read these two paragraphs.

16:11.000 --> 16:13.000
Here's what it's saying.

16:13.000 --> 16:17.000
A problem you see absolutely everywhere is the problem of least squares.

16:17.000 --> 16:19.000
I'll explain what it is.

16:19.000 --> 16:26.000
We're given a matrix A, which has M rows and N columns, and we'll assume that N is much less than M,

16:26.000 --> 16:28.000
which is actually pretty typical in practice.

16:28.000 --> 16:31.000
We're also given a vector B, which has dimension M.

16:31.000 --> 16:35.000
So, A and B are matrix and a vector that are given to us for this problem.

16:35.000 --> 16:38.000
Now, the goal is going to be to find a vector X.

16:38.000 --> 16:45.000
First, we form AX, which, as mentioned, is a linear function defined by multiplication with A.

16:45.000 --> 16:48.000
Then we consider its distance from B.

16:48.000 --> 16:51.000
That's what this notation means.

16:51.000 --> 16:54.000
Now, our goal is to minimize this distance.

16:54.000 --> 17:01.000
So, in one sentence, our goal is to find X such that AX is as close as possible to the vector B.

17:01.000 --> 17:07.000
Okay, now the best NLA algorithm to solve this involves order MN squared operations.

17:07.000 --> 17:13.000
So, we have an exponent of 2, but at least it's on N and not the much larger M.

17:13.000 --> 17:19.000
Now, what Rand NLA says is, if you're willing to accept a small and controllable error in your answer,

17:19.000 --> 17:24.000
which we'll call epsilon, then randomized algorithms can actually solve this

17:24.000 --> 17:29.000
in order MN log 1 over epsilon plus N cubed operations.

17:29.000 --> 17:33.000
It may not sound like much, but this is huge.

17:33.000 --> 17:37.000
In heavy-duty applications, both M and N can be big.

17:37.000 --> 17:42.000
N might be in the thousands, and M could be in the millions or billions.

17:42.000 --> 17:47.000
Now, if we want a strong approximation, like one within a tenth or a hundredth of a percent,

17:47.000 --> 17:50.000
then this term is going to be in the single digits.

17:50.000 --> 17:55.000
And since M is the big problematic number, then this term likely won't matter much.

17:55.000 --> 17:58.000
So, let's look at the ratio of the dominant terms.

17:58.000 --> 18:03.000
This will give a sense of how many times faster the randomized algorithm is than the classic one.

18:03.000 --> 18:09.000
Things cancel, and now we're looking at a speed-up factor of N over log 1 over epsilon.

18:09.000 --> 18:13.000
If N is in the thousands, and this is in the single digits,

18:13.000 --> 18:18.000
we're looking at a speed-up factor of around a thousand X. That's absurd.

18:18.000 --> 18:23.000
Now, due to some omitted details, we don't actually get a thousand X speed-ups in practice.

18:23.000 --> 18:27.000
However, we do get twenty X, and that's still huge.

18:27.000 --> 18:30.000
If we did the pure parallelization and hardware approach,

18:30.000 --> 18:34.000
recreating that gain might take twenty X the energy or twenty X the cost.

18:34.000 --> 18:38.000
Okay, but such bold claims raise some questions.

18:38.000 --> 18:41.000
First, what are these algorithms doing?

18:41.000 --> 18:43.000
Well, I'll get more into that later.

18:43.000 --> 18:47.000
But to describe it briefly, with high probability,

18:47.000 --> 18:54.000
a random summary of the data massively shrinks the problem while preserving virtually all of the relevant information.

18:54.000 --> 18:56.000
And that raises another question.

18:56.000 --> 19:04.000
A long-standing goal of classic NLA is to compute the most exact answer possible as fast as possible.

19:04.000 --> 19:10.000
That is, get the answer as precisely as the machine will allow and then optimize for speed.

19:10.000 --> 19:17.000
But in randomized NLA, the goal is to compute a close enough answer as fast as possible with high probability.

19:17.000 --> 19:20.000
And this allows for much faster algorithms.

19:20.000 --> 19:24.000
The question is, why are we allowing this new standard?

19:24.000 --> 19:28.000
Well, one motivation is the recent trend in machine learning.

19:28.000 --> 19:31.000
Machine learning accepts that the data is noisy.

19:31.000 --> 19:33.000
The data is just an approximation of the truth.

19:33.000 --> 19:37.000
And so computing things exactly is unnecessary.

19:37.000 --> 19:40.000
The exact answer would change with the change in the meaningless noise.

19:40.000 --> 19:43.000
So an approximate answer is, life be just as good.

19:43.000 --> 19:47.000
And it might even be an exact answer with a different sample of data.

19:47.000 --> 19:50.000
And so this looks a lot more reasonable.

19:50.000 --> 19:57.000
Also, when you look at some algorithms, we find that close enough can sometimes be made extremely close.

19:57.000 --> 20:02.000
And high probability can sometimes be made so high, it's not even worth mentioning.

20:02.000 --> 20:09.000
In general, there's a trade-off between speed and accuracy, and how favorable that trade-off is depends on the algorithm.

20:09.000 --> 20:12.000
Okay, next question.

20:12.000 --> 20:15.000
This argument is just a heuristic illustration.

20:15.000 --> 20:19.000
It's not pointing to an implemented algorithm with measurable performance.

20:19.000 --> 20:22.000
So what is the actual performance?

20:22.000 --> 20:27.000
Well, there are many papers that demonstrate significant concrete improvements.

20:27.000 --> 20:31.000
One striking demonstration I saw came from this paper,

20:31.000 --> 20:35.000
which is actually co-authored by Stephen Brunton and Nathan Kutz,

20:35.000 --> 20:38.000
two researchers who are active educators on YouTube.

20:38.000 --> 20:39.000
You may have seen them.

20:39.000 --> 20:46.000
One of their goals is to improve the SPD algorithm for low-rank matrices in the programming language R.

20:46.000 --> 20:52.000
SPD involves taking a matrix A and decomposing it into a product of three matrices.

20:52.000 --> 20:56.000
And without getting into the details, these matrices have some nice properties

20:56.000 --> 21:00.000
that help us do things like dimensionality reduction or solving least squares problems.

21:00.000 --> 21:05.000
Now, their randomized SPD algorithm gets this performance.

21:05.000 --> 21:10.000
Each plot is for a different size of A, specified as rows by columns.

21:10.000 --> 21:16.000
The y-axis tells us how many times faster an algorithm is than the plain SPD algorithm.

21:16.000 --> 21:21.000
So the plain SPD algorithm itself, its speed is always 1.

21:21.000 --> 21:29.000
That means the randomized SPD algorithm is between 40 and over 100 times faster, depending on the size of A.

21:29.000 --> 21:34.000
And the errors from randomization are comparable to those of the non-randomized routines.

21:34.000 --> 21:39.000
So we're looking at massive real gains, but we need to make some comments.

21:39.000 --> 21:43.000
First, they're only considering low-rank matrices.

21:43.000 --> 21:46.000
Randomized algorithms are especially useful for those.

21:46.000 --> 21:52.000
Second, their algorithm is entirely implemented in R, a high-level language.

21:52.000 --> 21:56.000
In contrast, one of the other benchmarking algorithms, R-Spectra,

21:56.000 --> 22:03.000
provides speed by granting R-axis to Spectra, a C++ library optimized for eigenvalue problems.

22:03.000 --> 22:09.000
So maybe randomization is best coded at the lower level, like that of C++.

22:09.000 --> 22:12.000
So let's check out an algorithm that does that.

22:12.000 --> 22:17.000
This one, Cholesky QR with randomization and pivoting for tall matrices.

22:17.000 --> 22:25.000
This one does an impressive job of wrangling the interplay of hardware, software, and randomization to produce a dominant algorithm.

22:25.000 --> 22:30.000
In fact, it strikes such a favorable balance of speed and accuracy,

22:30.000 --> 22:37.000
the authors claim that the algorithm design question is effectively solved for this class of matrix and problem.

22:37.000 --> 22:39.000
That's quite a claim.

22:39.000 --> 22:41.000
So what's the problem they're solving?

22:41.000 --> 22:47.000
Well, once again, we're given a matrix A, which in this case is assumed to be very tall.

22:47.000 --> 22:52.000
The goal is to decompose it into matrices with certain properties.

22:52.000 --> 22:56.000
Again, explaining the decomposition would take us really far afield.

22:56.000 --> 22:58.000
But here's one important detail.

22:58.000 --> 23:02.000
By including this matrix P, called the permutation matrix,

23:02.000 --> 23:08.000
we're asking the algorithm to order its operations in a special way to improve numerical stability

23:08.000 --> 23:13.000
and, for lack of a better explanation, provide more information on the decomposed matrix.

23:13.000 --> 23:18.000
Including P is significantly more work, but virtually guarantees we'll get the right answer.

23:18.000 --> 23:21.000
That is, it's much more numerically stable.

23:21.000 --> 23:26.000
Doing this decomposition is called QR decomposition with column pivoting.

23:26.000 --> 23:31.000
The QR matrices enable several useful things, like solving least squares problems.

23:31.000 --> 23:35.000
Now to explain their algorithm's performance, here are two blank plots.

23:35.000 --> 23:41.000
On the left, we'll see performance on matrices with about 32,000 rows.

23:41.000 --> 23:44.000
On the right, about 130,000 rows.

23:44.000 --> 23:51.000
Along the horizontal axis, the number of columns varies from about 500 to about 8,000.

23:51.000 --> 23:57.000
So we're looking at A matrices of different shapes, but they're all either tall or really tall matrices.

23:57.000 --> 24:01.000
Now the vertical axis is billions of floating point operations per second,

24:01.000 --> 24:04.000
which is a bit of a weird thing to measure.

24:04.000 --> 24:10.000
But the G-flops are those of a benchmark algorithm run on A, and so it's fixed across algorithms.

24:10.000 --> 24:15.000
In other words, just interpret the vertical axis as relative speed, like in the previous paper.

24:15.000 --> 24:21.000
Okay, now, this is LA-PAC's algorithm for QR with column pivoting.

24:21.000 --> 24:27.000
Remember, LA-PAC is the very well-optimized industry standard, but maybe not for long,

24:27.000 --> 24:32.000
because this is their algorithm, pronounced secret.

24:32.000 --> 24:37.000
As you can see, we're looking at 10 to 20x speed-ups, and if you're wondering what's happening here,

24:37.000 --> 24:44.000
the authors point out that the matrices no longer fit in the cache, so moving more data around becomes necessary.

24:44.000 --> 24:51.000
As an aside, moving data or data communication is a major source of algorithmic slowness.

24:51.000 --> 24:55.000
Now in the paper, the plot shows alternative fast algorithms for a comparison.

24:55.000 --> 25:00.000
They're acknowledging other approaches people might be familiar with to further benchmark their algorithm.

25:00.000 --> 25:08.000
However, in my view, the authors are being a bit modest, because these other fast algorithms aren't doing the full job.

25:08.000 --> 25:10.000
It's not apples to apples.

25:10.000 --> 25:18.000
This one and this one don't perform column pivoting, so the decomposition gives us less information about the matrix.

25:18.000 --> 25:26.000
If we ignore that fact and try to use the decomposition just like it had done pivoting, we get inaccurate or even flat-out wrong answers.

25:27.000 --> 25:36.000
This one and this one are numerically stable, but the former only applies to full-rank matrices, so not all tall, skinny matrices.

25:36.000 --> 25:41.000
And the latter only delivers an implicit representation of the matrices we want.

25:41.000 --> 25:48.000
This means, in many practical cases, we'd need to do extra work, slowing the algorithm down considerably.

25:48.000 --> 25:53.000
All things considered, this plot understates how much better secret really is.

25:53.000 --> 26:01.000
And if you're thinking, okay, but this can't actually replace the LA-PAC algorithm, because randomized algorithms come with some error, right?

26:01.000 --> 26:08.000
Well, in this case, the error can be made so small, it's essentially just as good as the LA-PAC routine.

26:08.000 --> 26:11.000
And this highlights the bizarre magic of randomization.

26:11.000 --> 26:16.000
You add a well-placed pinch of it, and you're able to get essentially the same answer, but many times faster.

26:16.000 --> 26:18.000
It's a trick almost without baggage.

26:18.000 --> 26:23.000
It's not an optimization that only works for a fixed set of machines or a niche class of matrix.

26:23.000 --> 26:25.000
It works essentially across the board.

26:25.000 --> 26:28.000
It makes you wonder, what are these algorithms doing?

26:28.000 --> 26:32.000
Well, RAND-NLA algorithms come in several flavors.

26:32.000 --> 26:35.000
I'll go with one that's especially simple to present.

26:35.000 --> 26:37.000
Let's bring back the least squares problem.

26:37.000 --> 26:44.000
Again, we're looking for the vector x such that ax's distance from b is as small as possible.

26:44.000 --> 26:48.000
We'll call the x vector that achieves this minimum x star.

26:48.000 --> 26:50.000
Again, we'll assume that a is a tall matrix.

26:50.000 --> 26:54.000
One randomized approach is called sketch and solve.

26:54.000 --> 26:57.000
We start by sampling a random matrix s.

26:57.000 --> 27:00.000
How we do that doesn't matter right now.

27:00.000 --> 27:07.000
What's important is that we'll be solving the least squares problem, but we'll replace a with s a and b with s b.

27:07.000 --> 27:13.000
And s will be designed such that s a has many fewer rows than the tall matrix a.

27:13.000 --> 27:16.000
And in the same way, s b is much smaller than b.

27:16.000 --> 27:21.000
Essentially, multiplying by s produces a compressed problem that's much faster to solve.

27:21.000 --> 27:24.000
Doing this is to form a small sketch of the problem.

27:24.000 --> 27:26.000
Here's the remarkable thing.

27:26.000 --> 27:31.000
If we solve this new least squares problem, giving us a vector we'll call x tilde,

27:31.000 --> 27:40.000
then the distance it achieves in the original problem is about the same as the best achievable distance in that original problem with high probability.

27:40.000 --> 27:49.000
In other words, it's very likely that solving this much smaller problem will give an answer that's nearly just as good as what would get solving the original big problem.

27:49.000 --> 27:51.000
And that's great news.

27:51.000 --> 27:53.000
A much smaller problem is much faster to solve.

27:53.000 --> 27:56.000
And that's where we get these order of magnitude speed ups.

27:56.000 --> 28:01.000
And what makes this practically useful is that we can control how good the approximation is.

28:01.000 --> 28:05.000
And the probability that that level of approximation is achieved.

28:05.000 --> 28:13.000
We can say, I want a 99.99% chance that the distance is within 1% of the best achievable distance.

28:13.000 --> 28:18.000
Declaring that will tell us how many rows s needs, depending on how s is randomly sampled.

28:18.000 --> 28:24.000
In fact, developing theorems to make statements like these, that's where a lot of the hard work of the field is.

28:24.000 --> 28:27.000
Further, how s is sampled is a question all its own.

28:27.000 --> 28:32.000
Every value could just be a sample from a normal distribution, but there are fancier techniques,

28:32.000 --> 28:36.000
like ones that allow you to perform the essay multiplication extremely quickly.

28:36.000 --> 28:41.000
Because, in the presentation that I just gave you, doing that multiplication would actually be a dominant cost.

28:41.000 --> 28:46.000
Okay, but this explanation doesn't answer the why does it work question, really.

28:46.000 --> 28:49.000
I'm just showing that it relies on an approximation, which I'm asking you to accept.

28:49.000 --> 28:52.000
So, let's go a little further.

28:52.000 --> 28:54.000
Let's say we're given the following least squares problem.

28:54.000 --> 28:59.000
A is equal to some tall matrix with two columns, and B is equal to some long vector.

28:59.000 --> 29:06.000
Now, if you know something about least squares, you know finding x is going to involve, among other things, the covariance matrix of A.

29:06.000 --> 29:09.000
That's a component of the solution I'd like to focus on.

29:09.000 --> 29:11.000
So, let's ignore B for now.

29:11.000 --> 29:15.000
Now, if you don't know what the covariance matrix of A means, that's fine.

29:15.000 --> 29:18.000
With just two columns, it's a very easy thing to visualize.

29:18.000 --> 29:22.000
What we'll do is plot each row of A as a point.

29:22.000 --> 29:26.000
Now, the covariance matrix just tells you this elliptical shape.

29:26.000 --> 29:30.000
And it's this shape that partially determines the minimizing x.

29:30.000 --> 29:37.000
Next, let's consider SA, where S is properly scaled random Gaussians with D rows.

29:37.000 --> 29:43.000
If D is equal to 8, that means SA can be plotted as 8 summary data points.

29:43.000 --> 29:54.000
Again, we compute the covariance matrix, and again, this shape partially determines the minimizing x, this time in the shrunken, sketched version of the problem.

29:54.000 --> 29:57.000
Now, here's what RAN and LA exploits.

29:57.000 --> 30:04.000
As D increases, the covariances of SA and A become more and more similar.

30:04.000 --> 30:12.000
So, between A and SA, the covariance structures, things that determine the minimizing x's, are very similar.

30:12.000 --> 30:20.000
In other words, as far as this covariance piece of the answer goes, A and SA give us nearly the same thing.

30:20.000 --> 30:24.000
And D doesn't need to be that large to get a good approximation.

30:24.000 --> 30:31.000
So, we can solve a much smaller SA least squares problem and get virtually the same result.

30:31.000 --> 30:33.000
Okay, but what about B?

30:33.000 --> 30:38.000
Yeah, the minimizing x is also determined by the covariance between A and B.

30:38.000 --> 30:44.000
So, let's consider an augmented matrix, which is A concatenated with B as a new column.

30:44.000 --> 30:49.000
The covariance of this new matrix now includes everything that determines the minimizing x.

30:49.000 --> 30:52.000
Looking at this, this is just another matrix.

30:52.000 --> 30:56.000
So, each covariance matrix will be similar to that of its sketched version.

30:56.000 --> 31:02.000
In other words, everything that drives the minimizing x is similar across the original and sketched problem.

31:02.000 --> 31:07.000
That's why the sketched version gives us approximately the same answer.

31:07.000 --> 31:09.000
Now, I need to confess something.

31:09.000 --> 31:14.000
This approximation is actually pretty weak and isn't really what's powering the randomized algorithms.

31:14.000 --> 31:18.000
But it captures the essence and can be animated, so I went with it.

31:18.000 --> 31:25.000
However, Riley pointed out that what's actually happening involves much stronger approximations regarding relative differences.

31:25.000 --> 31:32.000
Now, since defining relative things for matrices involves some head bending, I backed away.

31:32.000 --> 31:37.000
So, this gives a sense of the mathematical properties I play, but it's not the full story.

31:37.000 --> 31:41.000
For a bigger picture, Riley gave me the following analogy.

31:41.000 --> 31:47.000
The strategy of LA-PAC is to cast NLA algorithms into the use of efficient BLOZ functions.

31:47.000 --> 31:51.000
The highly optimized General Matrix Multiply or GEM function.

31:51.000 --> 31:56.000
The more an algorithm can be written as repeated use of this operation, the faster it'll get.

31:56.000 --> 31:58.000
That's what LA-PAC did.

31:58.000 --> 32:02.000
But there's a limit to how much algorithms can be recast into GEM.

32:02.000 --> 32:07.000
But randomization provides a new basket of functions that can be applied in a similar way to GEM.

32:07.000 --> 32:13.000
If least squares is suddenly super efficient, we'll try to reframe everything we can as solving repeated least squares problems.

32:13.000 --> 32:16.000
It's a huge space for creativity and big gains.

32:16.000 --> 32:24.000
And that's why in the Rand-NLA paper, they're talking about a new software, Randblast and Rand-LA-PAC,

32:24.000 --> 32:28.000
which would serve as a new pillar for NLA, the randomized approach.

32:28.000 --> 32:35.000
If they pull it off, and it's really saying something, we'd be in for a widespread upgrade in scientific computing.

32:35.000 --> 32:42.000
All those technologies stand to be improved from gaming to weather forecasting to artificial intelligence.

32:43.000 --> 32:49.000
To be comprehensive, I should mention at least two other approaches to speed.

32:49.000 --> 32:52.000
The first is communication avoiding algorithms.

32:52.000 --> 32:56.000
Algorithms which anticipate the hardware to minimize the amount of data movement.

32:56.000 --> 33:00.000
Since moving data takes so much time, these provide big speed ups as well.

33:00.000 --> 33:03.000
Second, there are hardware accelerators.

33:03.000 --> 33:08.000
Specialized hardware designed to do very specific operations extremely fast.

33:08.000 --> 33:12.000
Now, both of these are totally effective paths to speed,

33:12.000 --> 33:16.000
and in fact, can be combined with randomization to produce even larger gains.

33:16.000 --> 33:20.000
That said, these approaches bring some inflexibility.

33:20.000 --> 33:25.000
Communication avoiding algorithms need to be designed especially carefully to the hardware,

33:25.000 --> 33:29.000
and accelerators only do fixed, highly specialized operations.

33:29.000 --> 33:32.000
You can't change what an accelerator does after it's built.

33:33.000 --> 33:39.000
In comparison, randomized algorithms are exceptional because they are entirely an idea of mathematics.

33:39.000 --> 33:43.000
Randomness isn't upgraded hardware or an algorithm designed for specific hardware,

33:43.000 --> 33:46.000
yet it gives you speed and scalability as though it were.

33:46.000 --> 33:51.000
It does this by allowing simple algorithms, ones that otherwise struggle with scalability,

33:51.000 --> 33:53.000
to be applied to huge data.

33:53.000 --> 33:57.000
And it's this quality of simple but powerful that I believe is necessary

33:57.000 --> 34:01.000
for producing a significant and widespread upgrade in scientific computing.

34:01.000 --> 34:06.000
Since this is evolving, I'm going to keep track of updates as I hear about them

34:06.000 --> 34:10.000
and to the best of my ability on a post on truthata.io.

34:10.000 --> 34:17.000
There I'll also answer some other questions, like why isn't this Monte Carlo or who cares about least squares?

34:17.000 --> 34:21.000
In general, I inevitably learn more about a topic after I publish a video on it,

34:21.000 --> 34:26.000
so this post can evolve as I hear from you, others, or just learn more about it myself.

34:26.000 --> 34:30.000
And wow, what an incredible segue into talking about true theta.

34:30.000 --> 34:33.000
Truthata is my data science consultancy.

34:33.000 --> 34:36.000
We have experienced building machine learning systems for pricing,

34:36.000 --> 34:39.000
credit risk modeling, causal inference, and forecasting.

34:39.000 --> 34:42.000
If you're at a company looking for this type of work, we should talk.

34:42.000 --> 34:46.000
You can send an email to increase at truthata.io to get in touch.

34:46.000 --> 34:48.000
Alright, that's it.

34:48.000 --> 34:52.000
If you'd like to learn more about randomized NLA, I have my sources in the description.

34:52.000 --> 34:57.000
Also, I'd like to make a special thank you to Riley Murray for our discussions on this topic.

34:57.000 --> 35:01.000
And I'd like to thank everyone else who provided useful commentary.

35:01.000 --> 35:05.000
And finally, thank you for watching, and until next time.

