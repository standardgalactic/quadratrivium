Hi, thank you. Thank you so much for having me. I'm very excited to not tell you how AI
is going to save us all. This talk is called The Expanding Dark Forest in Generative AI.
It's going to be about writing on the web, trust, and human relationships, so like small
fish. And also AI, unfortunately, I'm sorry. There always has to be one AI talk at every
conference at this point, but at least you get me and not someone telling you how it's
going to take all your jobs. So I only give a footnote with this talk where I say, well,
this talk is up to date as of about a week ago, as of about Monday. So anything that's
happened since Monday, I can't take accountability for. It's probably all out of date by now,
but this is the state of the AI industry. So first some context. This is me. I look
like this on the internet. My name is Maggie. I am a designer at an AI research lab called
ELICIT. We do use language models to help scientists and researchers do literature review,
which is a long, boring task, creating many thousands of PDFs, and this is something that
language models are actually quite good at helping with. I also am very online. I'm
very on Twitter, X, whatever you want to call it. I write a lot online, and this has led
to lots of really positive relationships, a lot of really good career success, and this
will become relevant later as to why I care so much about people in the future being able
to do that, to be able to write online and connect with others by writing online.
I'm also a cultural anthropologist. I originally trained in this for my undergraduate degree
before becoming a designer because you can't get paid much to be a cultural anthropologist,
but a lot of theory I learned there plays back into my thoughts on design and development
and how we build things on the web. If you want to take notes on things, you can also
just scan this QR code, and this whole talk is transcribed with slides and everything
on this link, so don't worry about taking pictures of slides you like or trying to remember
what I said. You can reference it later. I might have to update it a bit because the
talk evolves, but one version that's mostly the same is on that QR code.
Here's what I'm going to talk about. First, we're going to talk about the dark forest
theory of the web. What is that? Next, I'm going to talk about the state of generative
AI as of a week ago. I'm third going to present some problems, and then we can kind of talk
about whether they actually are problems. We can question whether we think they're legitimate
or not. Lastly, I'm going to talk about possible futures, so how to deal with all these hypothetical
problems that I present. First, to explain the dark forest theory of the web, I'm first
going to have to explain the dark forest theory of the universe. This is a theory that tries
to explain why we haven't found intelligent life in the universe. Here we are in the universe,
the pale blue dot, and as far as we know, we are the only intelligent life around. We've
been beaming out messages for like 60 years now with like SETI, trying to find other intelligent
life, trying to see if there are aliens or some other kind of being that could respond
to us. We haven't heard anything back. The big question here is why. Dark forest theory
says it's because the universe is like a dark forest at night. It's a place that seems
quiet and lifeless because if you make noise, the predators come eat you. If you draw attention
to yourself, you're going to be attacked and destroyed. It stands to reason that all the
other intelligent civilizations that may or may not exist have either died or learned
to shut up. And we don't know which one we are yet. So the web version of this builds
off this concept. It's a theory that was proposed by Nancy Strickler, who's a really good thinker
and writer back in 2019. And Nancy wrote this article describing what it feels like to be
in public spaces on the web around this time. And Nancy pointed out these two main vibes,
let's say. And the first is that being on the web often feels like a very lifeless, automated
place that's devoid of humans. It's got all this ads and click bait and predatory behaviors
and none of it feels like real humans are trying to connect with us on the web. It just
feels commercial. The second vibe, actually, here, so here we are on the web, right? And
we are naively writing a bunch of very sincere, authentic accounts of our lives and thoughts
and experiences and trying to make connections to the other intelligent humans. We're sending
out messages, trying to beam out life. But then what we hear in response is content that
seems very inauthentic and human. It sounds like a bunch of robots and automations doing
marketing automations and growth hacking, kind of pumping out generic click bait. So we've
seen all this stuff, right? This is like low-quality listicles, right? Productivity rubbish, growth
hacking advice, you know, banal motivational quotes, dramatic click bait. Like, a lot of
this may as well be automated, even if a human didn't make it in some sense, right? They're
rarely trying to communicate some sincere original of thought to other humans. They're
trying to get you to click, right, and rack up some views. And this flood of really low-quality
content has made us retreat away from the public spaces of the web. It's very costly for us
to spend time and energy wading through all this craft. So the second vibe of the dark web is
that there's a lot of unnecessarily antagonistic behavior at a large scale. So when we are putting
out all these signals, right, trying to authentically connect to other humans, we could become a
target, right? We risk the Twitter mob coming to eat us is what can happen. So there's a term on
Twitter called getting main character. I don't know if people have heard of this. And I don't
know if people remember, this is one year ago, Garden Lady. This was a very famous tweet, if
everyone saw this one, where this really lovely woman got on Twitter one morning and she said,
my husband and I wake up every morning and we bring our coffee out to the garden and we sit and
talk for hours and it never gets old and we never run out of things to talk about and I love him
so much. And everyone's like, that's such a nice tweet. That's so wonderful. And then this went
viral. They got picked up and the Twitter reply started rolling in. So someone said, that's cool.
I wake up every morning and fight my wavy traffic for an hour in Miami to get to work. That must be
nice. Someone else, I wake up at 6am, right? This is an unattainable goal for most people. Not
that she said it was a goal, but interpret it as you will. Another one, again, complaining about
the morning routine and then goes, it must be nice being a trust fund baby with not a care in the
world. So I thought this tech talk summed it up nicely. I don't care if something good happened
to you. It should have happened to me instead. So this seems like a dumb example, but it was a
really good moment on Twitter that shows kind of the energy flows that happen on these really
large-scale social media platforms, right? Someone can publish something that's very kind and nice
and it gets interpreted the wrong way. People take it in bad faith. They take it out of context.
They try to amplify it to unintended audiences. We will take every opportunity to misinterpret
things and be ungenerous to each other. And this is how we get cancelling some pylons and we've
all seen so many examples of this happening in what seems like a very unfair way. John Ronson
wrote an entire book about this called You've Been Publicly Shamed. The catalog is quite a
few of these. It's a little out of date now, but it's a lot of kind of classic original examples
of people getting cancels. And then the real material consequences they fix, right? They lose
jobs. They lose friends. They are alienated from their community. It is not just internet drama.
They really do have to face repercussions for these pylons. And so this makes the web a very
sincerely dangerous place to sincerely publish your thoughts, to publish honest things on.
And this makes it hard to find people, right? It's very difficult to find people who are being
sincere, who are seeking coherence and who are trying to build collective knowledge in public.
I know this is not what everyone wants to do with the web, right? Like some people just want to
dance on TikTok and that's completely fine. We have to let them do that. But I'm interested in
at least some of the web enabling this kind of productive discourse and having spaces of community
building and I'm hoping some people here feel the same, right? Rather than it being like this
threatening inhuman place where you can't actually say what you think. So how do we cope with this,
right? We're all wandering around this dark forest of like Facebook and LinkedIn and Twitter
and we realize we need to go somewhere safer. So what we end up doing is we retreat primarily to
what's being called the cozy web. So this was a term coined by Venkatesh Rao in direct response
to the dark web theory. And Venkat pointed out that we've all started going underground. We move
into semi-private spaces like newsletters or personal websites where you're less at risk of
attack. You're not on these big platforms. You're on your own separate domain or you're on your own
separate newsletter. So this gives us some safety. We can at least decide a little bit who reads it
and understand our audience. But we often retreat even further into gate kept spaces like slacks,
WhatsApps, Discord, Signal groups, right? This is where we end up spending the most of our time
and having real human relationships where we can express our ideas safely, right? So things
that we say we know will be taken in good faith in these smaller groups. We can engage in real
discussions. But there's some problems here, right? Like none of this is indexable or searchable.
It's very hard to include people who aren't already in the group. And it hides collective
knowledge in these private databases that are even hard for the users themselves to access.
And also like good luck finding anything on Discord. Like you'll never be able to use the
search functionality in these apps. So my current theory, sadly, is that the dark forest is about
to expand because we now have this thing called generative AI. So I'm sure everyone has mostly
heard of this, but what I'm talking about specifically here is machine learning models
and neural networks that can create content that before this point in history only humans could
make, right? This is text, images, audio and video that mimics human creations in a very
compelling and believable way. Here are kind of some of the major foundational models that you
might have heard of for different media types, right? We have GPT4 and clod for text, mid-journey
and stable diffusion for images. There's now video ones like runway ML. And you might have
heard some of the news that a lot of these models are now becoming multimodal so they can do text
to image, image to text, audio to text. Like you can kind of go anywhere you want with media here.
And this is, of course, chat GPT. I'm sure we've all seen a thousand screenshots of this at this
point. We understand what it is. But to recap, it's right, we know it can generate huge volumes
of high-quality text in seconds. The outputs are indistinguishable from human-made text when we
try to get people to guess what's chat GPT and what's human. They often can't. It's trained on a
huge volume of text scraped primarily from the English-speaking web. And this all sounds very
simple, right? But it leads to many kind of complex and potentially useful behaviors, but it's
very emergent. We don't really understand what's possible because of this capability yet. We can
also now, of course, generate images, right? This is mid-journey, which usually makes pretty
beautiful impressive stuff. But so we found that these, like, generative AI models are now very
easy to use and very widely accessible, right? They don't require technical skills. They're
incredibly cheap. And they're increasingly becoming a feature in existing software you already have
access to, like Adobe or Photoshop or Notion. They're just becoming pervasive.
But the product category that I'm most nervous about, not just, like, Notion generating a plan
for you, is what's being called content generators, mostly for content marketers. So I'm going to pick
on one product, but there are many. This one's called Blaze. And it creates articles and social
media content for you, right? In half the time, who wouldn't want that? Who doesn't want more
content on the internet? And so I want to show you how this works. So you decide what kind of
content you want to make, right? You can say a blog post or a newsletter or a bunch of Twitter posts.
And I'm going to say I want to write a blog post. And you type in what you want to write about,
and your target audience, and SEO keywords. So I've decided I want to write about why
plant-based meat is morally wrong, which I don't believe, but that sounds like a good clickbait
to me. Like, someone's going to be like, yeah, I want to find out why that's bad.
You know, maybe I'm a company that has some financial interest in plant-based meats going
badly. So I'm going to go ahead and have this model write a little article for me. It lets
me pick a title, which is a nice customization. And then it chans out 700 words, right? And this
is now ready for me to hit publish, right? Or at least gives me some base to work off.
And if I'm blobbing against plant-based meats, I can just generate 100 of these, right? And,
like, optimize them for Google SEO and publish them all at once. And, like, hard days out of
Cassie Dunn. Right? The quality and truthfulness of what's written in here is very questionable.
We'll get to problems with that later. But the point is, this is super easy to do at scale
very cheaply. And it essentially murders Google such, right? Like, this just does away with SEO
optimized content. Because anyone can publish this immediately. It gets even better at the end.
Like, it prompts me to generate more content. So it's like, oh, you have this blog post. Why not
generate LinkedIn posts and tweets and YouTube scripts and everything. We're not just getting
crappy Google articles. This is across every publishing platform. Right? So there's tons of
things to do this. There's, like, AI-linked post-generators, generate your next tweet,
right? YouTube content and autopilot, just thousands of these tools are pouring out.
So most of the examples I showed actually have a very simple architecture, right? You have a
single input, like, write me an article on plant-based meat. And you feed it into this big,
black mystery box of a language model, right? And we don't really understand totally what
happens inside, but it gives you an output, right? Rates you an essay. But you can't really tweak
what happened in the middle. You can edit the output, but you can't kind of pull the knobs
on the actual language model itself. Which isn't very sophisticated. We don't have a lot of control
or transparency in what's happening. But the industry has realized this is a problem, and we
started building architectures that are much more flexible and powerful. So we now have a language
model architecture where we take that same black box of the language model, but we give it access
to external tools, right? We say, okay, now we're going to tell it it can search the web through
an API. We give it access to a calculator. We give it access to a code REPL and APIs. It's now
getting a lot more capable, right? It can now look up, can do maths, you know? It can look up
information that things might not be right. It can double check its answers. Also language models
are usually quite forgetful. You might have found this, chat GPT after a long string. We'll forget
what you said earlier on. We can now hook them up to long-term memory databases and have them
reference things like many weeks or months in the past, which makes them a lot more capable.
And we've also found that they perform much better if you give them these cognitive prompts.
Like you tell it to do something, but then you say, you know, think about your answer, critique it,
and then answer me again. And that actually improves the quality of the answer quite a lot.
That's often called chain of thought prompting, self critique. It can observe what it knows and
plan the next step. And it's getting these more cognitive capacities by adding on these kind of
extra techniques. So this is being called the agent architecture, right? You tell the language
model to act like an agent. It ends up as being like the centralized brain and you give it, you
can say you can use any of these tools and then it composes which tools it wants to use to achieve
your goals. So it ends up being a chain like this where you give it your goal, it'll like observe,
it'll plan, it'll call a different tool, it'll observe, it'll plan. And we can actually do really
complex, kind of scarily impressive things when we level up to these more sophisticated architectures.
And actually on Monday, OpenAI did this big dev day talk, I don't know if people saw this,
and they announced a new API called the assistance API that makes all that stuff that I showed that
used to require quite a lot of Python code and kind of insider knowledge, and they're just making
it super easy for everyone to now do this architecture, where you're able to kind of
run any function, call any API, all hooked up to their really powerful models.
So we're about to enter this phase where this very capable agent architecture is becoming
pervasive and widespread and might be the foundation of a lot of new tools being built.
So we're sort of on the precipice of really unnerving moment, let's say.
Because agent architectures, I think, means we're about to enter a stage of sharing the web with
non-human agents, right? These agents are very different to what we've currently noticed bots
in the past, like a completely different architecture and set of capabilities.
They're going to have a lot more data on how realistic humans behave,
and they're rapidly going to get more and more capable as time goes on.
And soon, probably already now, we're not going to be able to tell difference between
these agents and real humans. If anyone else spends a lot of time on Twitter slash X,
you'll already have noticed there's a lot of accounts you stumble across that have a weird
vibe to them, and you definitely realize this is just chat GPT hooked up to a Twitter account,
but otherwise is trying to look real, but like every tweet is very optimized and comes back in
like a second. It's just replying to things, you know, three seconds later. So it's happening.
And sharing the web, I want to say with agents, I don't want to jump to saying this is like
inherently bad. I think they could have lots of good use cases, right? We could have
automated moderators in communities. We could have search assistants, but I think it's mostly
that it's going to get complicated, and this is going to be a huge product and cultural problem
we're going to need to think about carefully and deal with. So we should get into why is this
a problem for the web, right? I'm only going to focus on how this will affect human relationships
and information quality on the web. Anything else, like how we might all end up unemployed or dead
soon is like well beyond my pay grade, so I'm just limiting the space to just like how do we
make meaningful human connections and find a good quality content on the web.
Because, yeah, the cost of creating and publishing content just dropped to almost zero at this point,
right? Like humans are quite expensive and slow at making content, right? We need time to research
and think, and we like clumsily string words together, and then we want to take breaks,
and we want to be able to nap and eat and sleep, and then we demand people pay us like
extraordinary rates, right, to do this research. And generative models don't need time off,
and they don't get bored, and they cost like a couple fractions of a cent to write a few thousand
words. So given the dynamics here, it's very likely that models are going to become the main
generators of content online. So I think we're about to drown in a sea of informational garbage,
right? I think we're just going to be absolutely swamped in masses of mediocre content. Like every
marketer and SEO strategist and optimizer bro is just going to have a field day here, you know,
just filling the whole internet with all of their keyword stuff, optimized crap. And this explosion
of noise is going to make it really difficult to find both good quality people, real people,
and good quality content, and hear any signal through the noise. And we can tell this is
happening because scammers and scammers are currently quite lazy, and we're kind of in the
baby phases of this. So there's a phrase that you might have seen chat GPT reply with. It sometimes
says, as an AI language model, I do not have political beliefs, or as an AI language model,
I cannot answer that question. And this phrase, if you search and direct quotes for it around the
web, shows up everywhere, just like Amazon, Google, Yelp reviews, tweets, LinkedIn posts,
it's full of this phrase, because people can't be bothered to like control F and like delete the
one phrase that gives them away. All right. So I did a quick search for this on LinkedIn, it got
16,000 hits, and they're like really boring attempts to like write engaging content, but they
all begin with the phrase as an AI language model. Look in the first sentence. And these are real
people too. I did look at their profiles. They genuinely have jobs, and they're trying to optimize
their presence or something. But yeah, this is starting to happen. The motivation for doing
this rate isn't hard to understand. So let's like think of the hypothetical scenario. So this is
Nigel. He's written a book about why nepotism is great, right? And he wants to be a book fluencer.
He's like, it's his first book, he's self-published on Amazon, he wants to like, you know, become a
big book guy. So he spends up an agent, right? Not unlike an actual publishing agent. He might have
hired in the past. And he says, hey, like, help me promote my book, you know? And so the agent
thinks for a while, and it goes off, and it strategizes, and it generates a steady stream
of tweets, right, based about on the content of the book, like real insights from the book,
and it starts tweeting those out from Nigel's account, and he's given it access, you know?
And it goes and it does the same thing for LinkedIn and Facebook, you know, pretty easy.
And then it writes and schedules a newsletter to go out over the course of six months so that
his followers will always kind of get updates on new things he's researching. It sets up a
medium account, it reposts those as articles, right? Makes a set of addictive TikTok videos
based on that content, generates a bunch of podcast episodes, use Nigel's voice. We can
totally do that now. It's pretty easy. And then it finds a bunch of other people who like are
talking about nepotism and starts replying to them on LinkedIn and Twitter and making friends,
and maybe they're actually agents interacting with it, and like, it's a whole bunch of just
agents interacting with agents. And none of this is different to what Nigel could do on his own.
So we don't know that content moderation or spam filters are actually going to pick this
stuff up, because maybe it's tweeting it slow enough that a human could have done it,
and it really is in Nigel's voice. It's used his writing to write this content.
We don't necessarily have automated ways to filter any of this out.
And the thing is, without an agent, 99% of Nigel-type people wouldn't have gone to all
this effort. They don't have the time and energy to have made all this content. But with the agent,
suddenly, we have people like Nigel, but times 99 of them, able to create this amount of content
all the time. And this is how we kind of get the flood of just tons of content more than we can
really cope with. So the scale and the quality of the content is actually what's different here.
Strangely enough, it might have written better stuff than Nigel ever would. We might have way
better quality content about nepotism all over the internet. But you can imagine how this would
play out at another 100x scale, right, with political lobbying groups who have very vested
interests in certain ideas or beliefs or truths getting out into the world. Specific agendas,
large companies that want you to believe certain things about their product or certain things
about scientific claims. They all have access to these assistants and agents, too.
So I do have some good news. Like, this has all been a bit dark, a bit of a downer.
The good news is that this might not be a problem. Maybe this is all just fine, right?
This is only a problem if we want to use the web for very particular purposes,
such as facilitating genuine human relationships or pursuing collective sense-making and knowledge
building or grounding our knowledge of the world in reality. So we don't care about any of these
things. This is all fine. We're going to have amazing content on TikTok. We're going to be
very entertained. The thing is, I'm quite keen on a lot of these outcomes. I write on the web a
lot. I've had overwhelmingly positive experiences writing on the web and meeting people for doing
that. I have this whole thing called digital gardening I bang on about, about making everyone
publish their unfinished notes to the web and improve them over time and use that as a way
to meet people interested in what you're interested in. But, yeah, the goal of that stuff is to make
the web a space where that's possible for collective understanding and knowledge building.
And I'm really worried that generative agents like meaningfully threaten this in the very near
term, like the six to 12 month kind of time range. I can't even think beyond that.
So when I talk to people about my worries, I talk to a lot of people in the AI safety and
research world. They kind of go, why does it matter? My AI agent is going to make much better
content than you ever would. Why do you care that an agent made it and not a human? I'm like,
okay, let's engage with that question properly. I'm sympathetic to that point.
So here's the reasons that generated content is a little bit different, right?
The first is its connection to reality. The second is the social context they live within.
And the third is its potential for human relationships. And I'm going to go into
these in detail. So first, generated content, you probably have heard of this, is different
because it has a different relationship to reality than we do, right? We are embodied humans,
right? And we are sharing a physical reality. And we have all this rich embodied information,
like we all understand we're in this kind of beautiful theater and we're in Brighton and we
have a lot of physical embodied context about what we know about each other. And often what we're
doing on the web is we're reading other people's accounts of this reality and we compare it against
our own and we're like, do I agree with that? Is that really true? This is like the cycle of all
of art and science and literature, you know, reading and comparing and writing your own version of
things. And what we've done now is we've fed that huge trove of information into a neural network
or a large language model. And it's created a sort of representation of that text, right? It's
created a model of the things that we've already known about the world and have published to the
web. And the thing is that model can now generate text that's predictably similar to what it was
before, but it's totally unhinged from the physical reality that it once came from, right? It has
some big connection. It did come from there. There's like a chain here, but it fully like cannot
access that reality, right? Even if we put in robots, right, with like arms and eyes and ears,
it can't sense the world the way we sense the world until we make like a fully synthetic like
mimicry human that like is hooked up to a language model. But I think like 10 years away, I don't know,
I think we have some time. Right, it can't validate its claims. It's the big thing.
We politely call this hallucination, right? This is when language models say things that aren't
true about the world. We say it's hallucinating, right? Like it's some side kind of very smart
person on some like mild drugs who's confused about like who they are or where they are,
but they're saying very intelligent things. You're sort of holding them lightly, you know?
Language models are also different because they have a very different social context, right?
They have a very strange relationship to our social world. So hopefully you know this, but
everything you and I say is situated in a social context, right? We understand what we share in
common. And if you met someone who spoke a different language from a different culture,
you would not assume they thought the same things about the world that you would if you met someone
from your own neighborhood, right? If one of us met someone from like the Kenzie in England,
we would have very different understandings of like hygiene and science and like how the world
works. We would know some things in common. We technically speak the same language,
but we would know that we didn't have a shared social context in the same way.
But a language model is not a person and it does not have a fixed reality, right? They know nothing
about the cultural context of who they're talking to and they take on different characters depending
on what you tell them to do. You can say, you know, pretend to be a professor, pretend to be an
athlete, pretend to be a young child and it will take on that character. So it doesn't even have
a fixed place it's talking to you from in the way that a human does. But they do represent a very
particular way of seeing the world because we trained them primarily on text on the web that
was generated by a majority English-speaking, like 95% of the training data is English-speaking,
a primarily English-speaking westernized population, people who have mostly written a
lot on Reddit and lived between about 1900 and 2023, which like in the grand scheme of history
and geography is a very narrow slice of humanity, right? Of all possible cultures we've had in the
past, all possible cultures we could have in the future and all possible languages. This is just
such a small representation of reality and yet we're now making it the source of truth, right?
The Oracle. You go to chat, GBT to ask everything. So it's taking this already dominant way of
seeing the world and reinforcing that dominance, which is problematic and is like a whole different
talk that I don't even think I'm qualified to do but someone should. And we hope that this will
improve over time but it's really hard to do without lots of data and most cultures don't have the
vast kind of written record that an English-speaking westernized online population does. So lastly,
generated content lacks the potential for human relationships that human-made content does,
right? If you write something online and I read it and I find it compelling, I can DM you on Twitter
or I can find you on Blue Sky or I could find you somehow, ideally hopefully not on LinkedIn,
but somehow and message you and be like, I love this. This was such good ideas. Like I want to
write a piece in response to you and like we start having a little dialogue that I've had so many
relationships blossom this way. But if you have a language model, it's not going to be able to do
that. So this is a still from the film, Her, right? This has become kind of a cultural touch
point of like parasocial relationships with AI. Hopefully people have seen it but if not,
so Joaquin Phoenix, our lovely main character, he has this great relationship with his personal AI,
he talks to her in his ear, it falls in love with her. But then the AI of course grows bored of him
because he's a very kind of basic human and leaves and he's destroyed. And like some people were
supposed to get this like film is supposed to be a warning, right? And some people took it as a
suggestion. So there's a company called replica who make AI companions for you that you can
make friends with, possibly date and fall in love with. There's a lot of suggestions of sort of
lonely young men engaging with this and their marketing copy. And I mean, maybe I do need to
explicitly point this out. An AI replica or any other kind of like generative agent person cannot
fulfill all our human needs, right? They cannot give you a hug, they cannot come to your birthday
party, they cannot kind of engage with you in a meaningful, full human way. And so any kind of
language model agent on the internet has no capacity for that back and forth relationship.
Even if it faked it, it's very unclear that it would actually satisfy what we need when we have
an actual friend that we can go out to coffee with. So that all sounds quite bad again. Like
deep breaths, the whole talk is really just digging you in a ditch, I'm sorry. But I'm now
going to talk about possible futures. And again, these futures I think are not mutually exclusive,
I think they all might unfold in different ways over the next five to 10 years. And like I can't
speculate beyond that, God knows where we are. But yeah, I'm hoping they all kind of happen
in parallel. So the first is I think we're about to spend a lot of time thinking about how we
pass the reverse Turing test. So how do we prove we're human on a web filled with agents? So the
original Turing test, you have a human talk to a computer and another human through like a wall
so they can't see the typing messages of each other. And then the original test, the computer had
to prove that it was the human. It had to prove it was competent. And on the new web, we are now the
ones under scrutiny. We have to prove we're real. So we're going to end up like we will assume
everyone is an agent until proven otherwise. So I kind of wrote this post where I was
thinking about some short term tactics. Like we could use funny terminology. We could all try to
become teenagers who like have this insider jargon that the language models don't know about, but
they'll pick up on it pretty quick, you know, and then you'll have to abandon it, get a new jargon.
We could write in non-dominant languages. If you speak something like Catalan or Welsh,
you're probably in a pretty good position, you know, you'll be able to write in a way that's
more native than a language model ever could. And ideally, we just do higher quality writing,
we do more research, we do more critical thinking. We really reference events and people that could
we could only know about from the real world, from being embodied humans in space. I don't know how
long those kind of defenses will last, but that's in the short term something we can at least pull
on. This next one, I apologize for the phrase, but it too perfectly, it catches the point. I'm
not going to explain it. You can Google that later. But the point is that the content from models
might end up becoming our source of truth, and that how we know things simply was like,
well, a language model once said it, you know, and then it's forever captured in our circular flow
of information. So right in this current model, right, the training data is at least based on
real world experiences. It's kind of going in a single loop. But we're now going to use that
generated text to train new models. And so we enter this loop where like there's this very
tenuous link to the real world that was once a long time ago, the source of this data. This is
already starting to happen. AI researchers are worried we're kind of going to run out of data to
train models on within five years. And so there's a lot of talk of how do we generate information
that we can feed the models, feed back into the models. This one was a really funny example of
this happening. I don't know if people saw this. So someone noticed that if you ask Google if you
can melt an egg, it's like smart AI summary said yes. And they were like, why would it say that?
They weren't investigating. And it turned out that fact was pulled from Quora that was generated
from chat GPT. And because Quora is considered a reputable website with good SEO standing,
it got pulled up into the Google like smart answer. And it's not hard to imagine how all
kinds of hallucinated answers are going to become part of this loop if all these major websites are
using chat GPT or their own agent to generate answers and then they just feed it to each other.
It's like at one point, can you melt an egg? This phenomenon is very worrying for the scientific
community and they have good reason to be. We're already seeing a lot of evidence that scientific
researchers are using language models to help them write papers. So again, this is a paper
that was published in a genuine fairly legitimate journal, Environmental Science and Pollution
Research. And it included the phrase regenerate response at the end of a paragraph, which is the
button above chat GPT's input box. There's another one in August that was published on
fossil fuel allocation that included the phrase as an AI language model. Now, there's a lot of
debate in the community where they were like, well, this doesn't mean the science in these papers is
totally false, right? They could have done real research on real experiments and they were just
trying to get this paper out and at the end they went, you know what, chat GPT summarizes paragraph
for me. That totally could have happened. But the problem is we don't know. There's no protocol for
the transparency of how you say what you didn't, didn't use chat GPT or whatever kind of model for.
So now people are trying to make it more legitimate. Some people are listing chat GPT as an author on
papers. And there's a real risk that people with much worse intentions will kind of take this and
run with it and just make scientific paper mills. There's a lot of companies that have a lot of
vested interests in publishing science that agree with the thing that they would like to be true.
Usually you find this out when you get to the funding source section of the paper where you're
like, oh, yeah, funded by the people who make this drug. Interesting. But you can tell that if
they're going to be able to use generative models to kind of pump out lots of research papers, maybe
based on dubious science, it becomes very hard for us to tell what is actually real, what is vetted.
I mean, it just takes the whole, the replication crisis to a whole new level. So this one seems
the most obvious, right? One possible future is we will just retreat further into the cozy web,
right? The dark frost will grow larger and we will just go, okay, I'm only interacting with
Discord and WhatsApp, right? LinkedIn is dead, Twitter is dead. We've had to abandon it. We have
to maybe make new privatized gate kept spaces, which I think has a lot of downsides, but this
just might be the best way to deal with it. I think authors are going to increasingly put
content behind blocks and paywalls. I think this is already happening with things like
sub-stack and medium, where you're constantly having to log in or prove that you are part of
this community to access the content. You understand why authors do this, right? Because
actually you're having your content scraped and then fed into generative models puts you in a
disadvantage. Like your ideas could be taken out of context, maybe it's taken something that you
wanted to actually charge for and given out for free. Someone could train a model on your work
and have it start writing in your voice. There's a lot of ways this can go really badly for someone
whose full-time job is being a researcher or a content creator. We'll also see more websites
that have a large amount of content blocking scraping for language models, or one way to do
it is to just charge a huge amount for your API. Reddit did this recently. We're going to put the
price so high that no company in their right mind would really pay it, or it would just cost them
so much. Twitter kind of did the same. It's hard to tell if this was actually strategic or on purpose,
but raising the price to whatever it was, like 42,000 per month, means that very few people
can access Twitter's really high-quality content in an age where content to train models is the
new goals. It's kind of leading us to a place where the web is not open by default. You can't
just query any API for any content you want. Everything is locked down and gatecapped and
kind of cordoned off. Next, I think we're going to have what I'm calling the Meet Space Premium.
We are in the Meet Space Premium. It's when we begin to prefer and preference offline first
interactions. So we will start to doubt all people online. And the only way to confirm someone's
humanity is to meet them in person, right, to go for coffee or beer. And once you do that,
you can kind of set up a little trust network, right? You can say, oh, I've already met Sarah
over there, and she's a real human, and you've already met Tom, and he's a real human, and we can
kind of like coordinate our networks to vet who's real on the web. And then when you read their
writing, you kind of know it's from an actual person, or you'd hope so. You may be of some
trust network of people who aren't writing generated stuff under their own name.
I think this has knock-on effects. Like, people might move back to cities or
higher population and places. In-person events are going to be preferable. I think there's
obvious disadvantages to this, right? The web was this huge democratization thing to enable
people who are maybe disabled or have young children or who are caregivers who can't get
out of the house for a whole bunch of reasons aren't going to have the same access to the trust
network that someone can who could physically show up in space a lot.
Yeah. So, a natural follow-on from this also. So, a lot of people have been like,
well, why don't we just put it on the blockchain, you know? Why don't we just get a third party
to verify our humanity with a cryptographic key, and then you can sign all your published
content with it, and it'll link back to your identity, and this is how we'll have decentralized
trust networks. And I'm like, okay, I don't know the details of this. This sounds weird.
So, there's a project called Worldcoin that, funnily enough, is also funded by OpenAI's
leader, Sam Altman. He partially helped found it, which shows he kind of knows the problem he's
helped contribute to. So, this scary orb scans your eyeball to confirm your identity,
and then it creates a unique human credential for you to use online to sign all your stuff with.
It's really not taking off as a project, but it's around. And people are still trying to do this.
There's a whole community thinking this is the future that I don't have sophisticated thoughts
on yet, and I'm still like, oh, cryptocurrency. But maybe this is some way to get around it.
I'm also expecting any day that Elon's going to announce the purple check,
where you pay $30 a month, and you don't actually have to. You just take a box that's like,
I'm human, and then you get this little check and solve it. So, those are all a bit negative.
I do think there is some hope in this future. We can certainly fight fire with fire.
So, I think it's reasonable to assume that we're all going to have a set of personal language
models to kind of help defend us and serve our needs on the web. They can filter information,
they can manage information. And I expect these to be baked into browsers, or maybe even the
operating system, right? And they're going to do things like identify generated content,
they're going to debunk claims, they're going to flag misinformation, they're going to go
help hunt down real scientific sources for you, maybe vet scientific papers, curate and suggest
things to you. So, I think this actually could work in both directions. It's not just all like
the bad actors get this power. We also get a lot of capacity and capability from these models.
We might find it absurd that anyone would browse like the raw web without one of these
kind of in tow. It's the same way you wouldn't like go onto the dark web, like you know what's
there, but you know you don't want to see it. It might be kind of like that. Okay, so I'm almost
done wrapping this up. But the question I want to leave everyone with is which of these possible
futures would you like to make happen, right? Generative AI is not necessarily a destructive
force, you know, as with all technology, it depends how you wield it. Oh, I went back to the
wrong slide, sorry. There we go. The way that we choose to deploy this in the world is really
what matters, right? The product decisions we make as individuals and companies, if you are
working in the space or you're trying to get into it. Because obviously if you are working on a tool
that like turns out tons of human-like content from marketing and influence purposes, like
you can stop, like that's really like we don't need that. You can just stop doing that. But what
should you be building instead? It's maybe a more helpful question. So I tried to come up with a few
principles for building products language models that are probably going to evolve over time, but
this is like a first pass. And the first is to protect human agency. The second is to treat models
as reasoning engines and not sources of truth. And lastly that we should be augmenting our
cognitive abilities and not replacing them. So protecting human agency, this is like usually
at the moment you have a human prompter and it hands something off to an autonomous agent
and the agent goes and does stuff, right? This is like the open AI assistance model or any of
the architectures I showed before. And this is like the path to self-destruction. This is what
most AI safety researchers are very afraid of is that the locus of agency sits within the agent.
But the ideal form of this is that the locus of agent stays within the human
and it has a collaborative agent on hand and there's this very short continuous feedback loop
that is constantly going between them. Where the human is the one checking, should I do that? Do
I want that? Is that true? Like they're able to actually fact check things and then the agent is
much more of a helper. Short feedback loops, close supervision, limited power, it's slower but it's
safer. That ties into the second principle that we should treat models as tiny reasoning engines
and not sources of truth. So one way to use these models is to like ask it for every answer and
ask it every question and trust what it says. Another one is you can train them to do specific
things like just summarize this text, just extract data from this paper, just find contradictions in
this statement and then you can bring your own data which could be legitimate scientific papers,
it could be your own notes, it could be Wikipedia and then you use these models to just do these
very small scoped things where you can observe every single output and check that it's actually
legitimate and you're not handing off this big complex task to this big black box model.
And lastly, we should augment our cognitive abilities and not replace them, right? Language models
are very good at things that humans are not good at like searching and discovering things in large
datasets, role playing as identities and characters, they're actually really good at doing that,
rapidly organizing data, turning fuzzy inputs into structured outputs, there's a lot that they're
good at that we're bad at and we should use them for those things. There's tons that like we're
good at they're not that we're still trying to like make them do like checking claims against
physical reality, long-term memory, having embodied knowledge, understanding social context,
having emotional intelligence, I think combining the two of these so that we're doing things models
can't do and they're doing things we're not very good at actually leverages the best of both worlds.
Because a lot of AI researchers in the moment, they use this metaphor of aliens, this is from
the 1970s alien film or frightening, it just it makes me think this is like not the most appealing
collaborative partner this metaphor, this like big scary unknown consciousness that like might
kill you, but there's another metaphor that I like more that Kate Darling is a robotics researcher
at MIT and she wrote this book called The New Breed, arguing we should think about robots as
animals, we have a long cultural legal history with animals and working collaboratively with them,
oxen, dogs, pigs, right in this very like mutually beneficial relationship most of the time and this
is actually a pretty good metaphor to expand to AI where we have to kind of treat them a little bit
like some form of intelligent species but one that we are in community with and are part of our
systems and are not this like big scary alien who might come kill us all is like usually what it
gets talked about as. So yeah there's this big push for this philosophical approach, some people
call it cyber-organism, there's a very long article there was a written on less wrong which is not
my favorite website but it's a good article that kind of goes in depth into this if you do want
to read more about it. So that's all I have, I want to thank you so much for listening, again
slide the notes on this QR code if you like missed anything, I'm on Twitter X at mappleton still
until that really does fall apart and you can DM me there, you can message me again I love meeting
people through writing on the web if you have blogs that like relate to these kind of topics
send them to me but yeah thank you so much for listening, I appreciate it.
