1
00:00:00,000 --> 00:00:16,480
Hi, thank you. Thank you so much for having me. I'm very excited to not tell you how AI

2
00:00:16,480 --> 00:00:22,320
is going to save us all. This talk is called The Expanding Dark Forest in Generative AI.

3
00:00:22,320 --> 00:00:27,360
It's going to be about writing on the web, trust, and human relationships, so like small

4
00:00:27,360 --> 00:00:33,520
fish. And also AI, unfortunately, I'm sorry. There always has to be one AI talk at every

5
00:00:33,520 --> 00:00:36,760
conference at this point, but at least you get me and not someone telling you how it's

6
00:00:36,760 --> 00:00:41,920
going to take all your jobs. So I only give a footnote with this talk where I say, well,

7
00:00:41,920 --> 00:00:45,560
this talk is up to date as of about a week ago, as of about Monday. So anything that's

8
00:00:45,560 --> 00:00:50,160
happened since Monday, I can't take accountability for. It's probably all out of date by now,

9
00:00:50,160 --> 00:00:55,680
but this is the state of the AI industry. So first some context. This is me. I look

10
00:00:55,760 --> 00:01:00,960
like this on the internet. My name is Maggie. I am a designer at an AI research lab called

11
00:01:00,960 --> 00:01:06,840
ELICIT. We do use language models to help scientists and researchers do literature review,

12
00:01:06,840 --> 00:01:11,080
which is a long, boring task, creating many thousands of PDFs, and this is something that

13
00:01:11,080 --> 00:01:16,560
language models are actually quite good at helping with. I also am very online. I'm

14
00:01:16,560 --> 00:01:22,120
very on Twitter, X, whatever you want to call it. I write a lot online, and this has led

15
00:01:22,160 --> 00:01:26,520
to lots of really positive relationships, a lot of really good career success, and this

16
00:01:26,520 --> 00:01:30,520
will become relevant later as to why I care so much about people in the future being able

17
00:01:30,520 --> 00:01:36,680
to do that, to be able to write online and connect with others by writing online.

18
00:01:36,680 --> 00:01:40,880
I'm also a cultural anthropologist. I originally trained in this for my undergraduate degree

19
00:01:40,880 --> 00:01:45,360
before becoming a designer because you can't get paid much to be a cultural anthropologist,

20
00:01:45,360 --> 00:01:50,160
but a lot of theory I learned there plays back into my thoughts on design and development

21
00:01:50,360 --> 00:01:56,960
and how we build things on the web. If you want to take notes on things, you can also

22
00:01:56,960 --> 00:02:01,040
just scan this QR code, and this whole talk is transcribed with slides and everything

23
00:02:01,040 --> 00:02:05,520
on this link, so don't worry about taking pictures of slides you like or trying to remember

24
00:02:05,520 --> 00:02:08,800
what I said. You can reference it later. I might have to update it a bit because the

25
00:02:08,800 --> 00:02:14,680
talk evolves, but one version that's mostly the same is on that QR code.

26
00:02:14,680 --> 00:02:19,440
Here's what I'm going to talk about. First, we're going to talk about the dark forest

27
00:02:19,480 --> 00:02:24,520
theory of the web. What is that? Next, I'm going to talk about the state of generative

28
00:02:24,520 --> 00:02:31,640
AI as of a week ago. I'm third going to present some problems, and then we can kind of talk

29
00:02:31,640 --> 00:02:34,320
about whether they actually are problems. We can question whether we think they're legitimate

30
00:02:34,320 --> 00:02:39,760
or not. Lastly, I'm going to talk about possible futures, so how to deal with all these hypothetical

31
00:02:39,760 --> 00:02:46,480
problems that I present. First, to explain the dark forest theory of the web, I'm first

32
00:02:46,520 --> 00:02:52,360
going to have to explain the dark forest theory of the universe. This is a theory that tries

33
00:02:52,360 --> 00:02:58,040
to explain why we haven't found intelligent life in the universe. Here we are in the universe,

34
00:02:58,040 --> 00:03:03,520
the pale blue dot, and as far as we know, we are the only intelligent life around. We've

35
00:03:03,520 --> 00:03:08,720
been beaming out messages for like 60 years now with like SETI, trying to find other intelligent

36
00:03:08,720 --> 00:03:12,560
life, trying to see if there are aliens or some other kind of being that could respond

37
00:03:12,600 --> 00:03:19,600
to us. We haven't heard anything back. The big question here is why. Dark forest theory

38
00:03:19,840 --> 00:03:25,320
says it's because the universe is like a dark forest at night. It's a place that seems

39
00:03:25,320 --> 00:03:32,320
quiet and lifeless because if you make noise, the predators come eat you. If you draw attention

40
00:03:32,320 --> 00:03:37,040
to yourself, you're going to be attacked and destroyed. It stands to reason that all the

41
00:03:37,040 --> 00:03:41,800
other intelligent civilizations that may or may not exist have either died or learned

42
00:03:41,840 --> 00:03:48,520
to shut up. And we don't know which one we are yet. So the web version of this builds

43
00:03:48,520 --> 00:03:53,800
off this concept. It's a theory that was proposed by Nancy Strickler, who's a really good thinker

44
00:03:53,800 --> 00:03:59,280
and writer back in 2019. And Nancy wrote this article describing what it feels like to be

45
00:03:59,280 --> 00:04:04,760
in public spaces on the web around this time. And Nancy pointed out these two main vibes,

46
00:04:04,760 --> 00:04:10,400
let's say. And the first is that being on the web often feels like a very lifeless, automated

47
00:04:10,480 --> 00:04:16,320
place that's devoid of humans. It's got all this ads and click bait and predatory behaviors

48
00:04:16,320 --> 00:04:19,640
and none of it feels like real humans are trying to connect with us on the web. It just

49
00:04:19,640 --> 00:04:25,520
feels commercial. The second vibe, actually, here, so here we are on the web, right? And

50
00:04:25,520 --> 00:04:31,320
we are naively writing a bunch of very sincere, authentic accounts of our lives and thoughts

51
00:04:31,320 --> 00:04:36,040
and experiences and trying to make connections to the other intelligent humans. We're sending

52
00:04:36,080 --> 00:04:41,560
out messages, trying to beam out life. But then what we hear in response is content that

53
00:04:41,560 --> 00:04:47,400
seems very inauthentic and human. It sounds like a bunch of robots and automations doing

54
00:04:47,400 --> 00:04:52,520
marketing automations and growth hacking, kind of pumping out generic click bait. So we've

55
00:04:52,520 --> 00:04:57,800
seen all this stuff, right? This is like low-quality listicles, right? Productivity rubbish, growth

56
00:04:57,800 --> 00:05:03,560
hacking advice, you know, banal motivational quotes, dramatic click bait. Like, a lot of

57
00:05:03,600 --> 00:05:08,080
this may as well be automated, even if a human didn't make it in some sense, right? They're

58
00:05:08,080 --> 00:05:12,840
rarely trying to communicate some sincere original of thought to other humans. They're

59
00:05:12,840 --> 00:05:17,680
trying to get you to click, right, and rack up some views. And this flood of really low-quality

60
00:05:17,680 --> 00:05:23,160
content has made us retreat away from the public spaces of the web. It's very costly for us

61
00:05:23,160 --> 00:05:29,200
to spend time and energy wading through all this craft. So the second vibe of the dark web is

62
00:05:29,280 --> 00:05:36,000
that there's a lot of unnecessarily antagonistic behavior at a large scale. So when we are putting

63
00:05:36,000 --> 00:05:40,800
out all these signals, right, trying to authentically connect to other humans, we could become a

64
00:05:40,800 --> 00:05:47,840
target, right? We risk the Twitter mob coming to eat us is what can happen. So there's a term on

65
00:05:47,840 --> 00:05:52,000
Twitter called getting main character. I don't know if people have heard of this. And I don't

66
00:05:52,000 --> 00:05:56,240
know if people remember, this is one year ago, Garden Lady. This was a very famous tweet, if

67
00:05:56,320 --> 00:06:01,360
everyone saw this one, where this really lovely woman got on Twitter one morning and she said,

68
00:06:01,360 --> 00:06:05,520
my husband and I wake up every morning and we bring our coffee out to the garden and we sit and

69
00:06:05,520 --> 00:06:09,280
talk for hours and it never gets old and we never run out of things to talk about and I love him

70
00:06:09,280 --> 00:06:14,320
so much. And everyone's like, that's such a nice tweet. That's so wonderful. And then this went

71
00:06:14,320 --> 00:06:20,160
viral. They got picked up and the Twitter reply started rolling in. So someone said, that's cool.

72
00:06:20,160 --> 00:06:24,320
I wake up every morning and fight my wavy traffic for an hour in Miami to get to work. That must be

73
00:06:24,320 --> 00:06:30,320
nice. Someone else, I wake up at 6am, right? This is an unattainable goal for most people. Not

74
00:06:30,320 --> 00:06:35,680
that she said it was a goal, but interpret it as you will. Another one, again, complaining about

75
00:06:35,680 --> 00:06:39,440
the morning routine and then goes, it must be nice being a trust fund baby with not a care in the

76
00:06:39,440 --> 00:06:46,000
world. So I thought this tech talk summed it up nicely. I don't care if something good happened

77
00:06:46,000 --> 00:06:52,080
to you. It should have happened to me instead. So this seems like a dumb example, but it was a

78
00:06:52,080 --> 00:06:56,320
really good moment on Twitter that shows kind of the energy flows that happen on these really

79
00:06:56,320 --> 00:07:01,360
large-scale social media platforms, right? Someone can publish something that's very kind and nice

80
00:07:01,360 --> 00:07:06,160
and it gets interpreted the wrong way. People take it in bad faith. They take it out of context.

81
00:07:06,160 --> 00:07:10,880
They try to amplify it to unintended audiences. We will take every opportunity to misinterpret

82
00:07:10,880 --> 00:07:15,440
things and be ungenerous to each other. And this is how we get cancelling some pylons and we've

83
00:07:15,440 --> 00:07:21,600
all seen so many examples of this happening in what seems like a very unfair way. John Ronson

84
00:07:21,600 --> 00:07:24,960
wrote an entire book about this called You've Been Publicly Shamed. The catalog is quite a

85
00:07:24,960 --> 00:07:29,520
few of these. It's a little out of date now, but it's a lot of kind of classic original examples

86
00:07:29,520 --> 00:07:34,400
of people getting cancels. And then the real material consequences they fix, right? They lose

87
00:07:34,400 --> 00:07:39,200
jobs. They lose friends. They are alienated from their community. It is not just internet drama.

88
00:07:39,200 --> 00:07:44,960
They really do have to face repercussions for these pylons. And so this makes the web a very

89
00:07:44,960 --> 00:07:49,200
sincerely dangerous place to sincerely publish your thoughts, to publish honest things on.

90
00:07:51,600 --> 00:07:55,520
And this makes it hard to find people, right? It's very difficult to find people who are being

91
00:07:55,520 --> 00:08:00,400
sincere, who are seeking coherence and who are trying to build collective knowledge in public.

92
00:08:01,360 --> 00:08:05,520
I know this is not what everyone wants to do with the web, right? Like some people just want to

93
00:08:05,520 --> 00:08:10,240
dance on TikTok and that's completely fine. We have to let them do that. But I'm interested in

94
00:08:10,240 --> 00:08:16,000
at least some of the web enabling this kind of productive discourse and having spaces of community

95
00:08:16,000 --> 00:08:20,800
building and I'm hoping some people here feel the same, right? Rather than it being like this

96
00:08:20,800 --> 00:08:26,000
threatening inhuman place where you can't actually say what you think. So how do we cope with this,

97
00:08:26,000 --> 00:08:31,440
right? We're all wandering around this dark forest of like Facebook and LinkedIn and Twitter

98
00:08:31,440 --> 00:08:36,640
and we realize we need to go somewhere safer. So what we end up doing is we retreat primarily to

99
00:08:36,640 --> 00:08:43,120
what's being called the cozy web. So this was a term coined by Venkatesh Rao in direct response

100
00:08:43,120 --> 00:08:49,040
to the dark web theory. And Venkat pointed out that we've all started going underground. We move

101
00:08:49,040 --> 00:08:54,240
into semi-private spaces like newsletters or personal websites where you're less at risk of

102
00:08:54,240 --> 00:08:59,520
attack. You're not on these big platforms. You're on your own separate domain or you're on your own

103
00:08:59,520 --> 00:09:04,720
separate newsletter. So this gives us some safety. We can at least decide a little bit who reads it

104
00:09:04,720 --> 00:09:10,800
and understand our audience. But we often retreat even further into gate kept spaces like slacks,

105
00:09:10,800 --> 00:09:16,080
WhatsApps, Discord, Signal groups, right? This is where we end up spending the most of our time

106
00:09:16,080 --> 00:09:21,280
and having real human relationships where we can express our ideas safely, right? So things

107
00:09:21,280 --> 00:09:26,080
that we say we know will be taken in good faith in these smaller groups. We can engage in real

108
00:09:26,080 --> 00:09:31,200
discussions. But there's some problems here, right? Like none of this is indexable or searchable.

109
00:09:31,200 --> 00:09:37,040
It's very hard to include people who aren't already in the group. And it hides collective

110
00:09:37,040 --> 00:09:40,720
knowledge in these private databases that are even hard for the users themselves to access.

111
00:09:41,440 --> 00:09:45,120
And also like good luck finding anything on Discord. Like you'll never be able to use the

112
00:09:45,120 --> 00:09:52,080
search functionality in these apps. So my current theory, sadly, is that the dark forest is about

113
00:09:52,080 --> 00:09:59,280
to expand because we now have this thing called generative AI. So I'm sure everyone has mostly

114
00:09:59,280 --> 00:10:03,120
heard of this, but what I'm talking about specifically here is machine learning models

115
00:10:03,120 --> 00:10:08,080
and neural networks that can create content that before this point in history only humans could

116
00:10:08,080 --> 00:10:14,320
make, right? This is text, images, audio and video that mimics human creations in a very

117
00:10:14,320 --> 00:10:20,000
compelling and believable way. Here are kind of some of the major foundational models that you

118
00:10:20,000 --> 00:10:25,360
might have heard of for different media types, right? We have GPT4 and clod for text, mid-journey

119
00:10:25,360 --> 00:10:30,080
and stable diffusion for images. There's now video ones like runway ML. And you might have

120
00:10:30,080 --> 00:10:34,560
heard some of the news that a lot of these models are now becoming multimodal so they can do text

121
00:10:34,560 --> 00:10:39,600
to image, image to text, audio to text. Like you can kind of go anywhere you want with media here.

122
00:10:40,560 --> 00:10:45,360
And this is, of course, chat GPT. I'm sure we've all seen a thousand screenshots of this at this

123
00:10:45,360 --> 00:10:51,040
point. We understand what it is. But to recap, it's right, we know it can generate huge volumes

124
00:10:51,040 --> 00:10:56,480
of high-quality text in seconds. The outputs are indistinguishable from human-made text when we

125
00:10:56,480 --> 00:11:01,680
try to get people to guess what's chat GPT and what's human. They often can't. It's trained on a

126
00:11:01,680 --> 00:11:07,200
huge volume of text scraped primarily from the English-speaking web. And this all sounds very

127
00:11:07,200 --> 00:11:11,680
simple, right? But it leads to many kind of complex and potentially useful behaviors, but it's

128
00:11:11,680 --> 00:11:17,280
very emergent. We don't really understand what's possible because of this capability yet. We can

129
00:11:17,280 --> 00:11:21,360
also now, of course, generate images, right? This is mid-journey, which usually makes pretty

130
00:11:21,360 --> 00:11:27,440
beautiful impressive stuff. But so we found that these, like, generative AI models are now very

131
00:11:27,440 --> 00:11:31,120
easy to use and very widely accessible, right? They don't require technical skills. They're

132
00:11:31,120 --> 00:11:35,520
incredibly cheap. And they're increasingly becoming a feature in existing software you already have

133
00:11:35,520 --> 00:11:38,960
access to, like Adobe or Photoshop or Notion. They're just becoming pervasive.

134
00:11:41,200 --> 00:11:45,360
But the product category that I'm most nervous about, not just, like, Notion generating a plan

135
00:11:45,360 --> 00:11:49,680
for you, is what's being called content generators, mostly for content marketers. So I'm going to pick

136
00:11:49,680 --> 00:11:54,720
on one product, but there are many. This one's called Blaze. And it creates articles and social

137
00:11:54,720 --> 00:11:58,160
media content for you, right? In half the time, who wouldn't want that? Who doesn't want more

138
00:11:58,160 --> 00:12:03,520
content on the internet? And so I want to show you how this works. So you decide what kind of

139
00:12:03,520 --> 00:12:08,640
content you want to make, right? You can say a blog post or a newsletter or a bunch of Twitter posts.

140
00:12:08,640 --> 00:12:12,960
And I'm going to say I want to write a blog post. And you type in what you want to write about,

141
00:12:12,960 --> 00:12:17,040
and your target audience, and SEO keywords. So I've decided I want to write about why

142
00:12:17,600 --> 00:12:20,960
plant-based meat is morally wrong, which I don't believe, but that sounds like a good clickbait

143
00:12:20,960 --> 00:12:23,440
to me. Like, someone's going to be like, yeah, I want to find out why that's bad.

144
00:12:24,480 --> 00:12:28,560
You know, maybe I'm a company that has some financial interest in plant-based meats going

145
00:12:28,560 --> 00:12:33,840
badly. So I'm going to go ahead and have this model write a little article for me. It lets

146
00:12:33,840 --> 00:12:38,640
me pick a title, which is a nice customization. And then it chans out 700 words, right? And this

147
00:12:38,640 --> 00:12:41,840
is now ready for me to hit publish, right? Or at least gives me some base to work off.

148
00:12:42,640 --> 00:12:46,080
And if I'm blobbing against plant-based meats, I can just generate 100 of these, right? And,

149
00:12:46,080 --> 00:12:50,320
like, optimize them for Google SEO and publish them all at once. And, like, hard days out of

150
00:12:50,320 --> 00:12:54,800
Cassie Dunn. Right? The quality and truthfulness of what's written in here is very questionable.

151
00:12:54,800 --> 00:12:59,920
We'll get to problems with that later. But the point is, this is super easy to do at scale

152
00:12:59,920 --> 00:13:04,800
very cheaply. And it essentially murders Google such, right? Like, this just does away with SEO

153
00:13:04,800 --> 00:13:10,880
optimized content. Because anyone can publish this immediately. It gets even better at the end.

154
00:13:10,880 --> 00:13:14,800
Like, it prompts me to generate more content. So it's like, oh, you have this blog post. Why not

155
00:13:14,800 --> 00:13:18,800
generate LinkedIn posts and tweets and YouTube scripts and everything. We're not just getting

156
00:13:18,800 --> 00:13:25,440
crappy Google articles. This is across every publishing platform. Right? So there's tons of

157
00:13:25,440 --> 00:13:28,960
things to do this. There's, like, AI-linked post-generators, generate your next tweet,

158
00:13:28,960 --> 00:13:32,240
right? YouTube content and autopilot, just thousands of these tools are pouring out.

159
00:13:34,080 --> 00:13:38,240
So most of the examples I showed actually have a very simple architecture, right? You have a

160
00:13:38,240 --> 00:13:42,880
single input, like, write me an article on plant-based meat. And you feed it into this big,

161
00:13:42,880 --> 00:13:46,960
black mystery box of a language model, right? And we don't really understand totally what

162
00:13:47,040 --> 00:13:51,120
happens inside, but it gives you an output, right? Rates you an essay. But you can't really tweak

163
00:13:51,120 --> 00:13:55,280
what happened in the middle. You can edit the output, but you can't kind of pull the knobs

164
00:13:55,280 --> 00:14:00,000
on the actual language model itself. Which isn't very sophisticated. We don't have a lot of control

165
00:14:00,000 --> 00:14:04,960
or transparency in what's happening. But the industry has realized this is a problem, and we

166
00:14:04,960 --> 00:14:11,040
started building architectures that are much more flexible and powerful. So we now have a language

167
00:14:11,040 --> 00:14:15,440
model architecture where we take that same black box of the language model, but we give it access

168
00:14:15,440 --> 00:14:18,800
to external tools, right? We say, okay, now we're going to tell it it can search the web through

169
00:14:18,800 --> 00:14:24,720
an API. We give it access to a calculator. We give it access to a code REPL and APIs. It's now

170
00:14:24,720 --> 00:14:30,160
getting a lot more capable, right? It can now look up, can do maths, you know? It can look up

171
00:14:30,160 --> 00:14:35,440
information that things might not be right. It can double check its answers. Also language models

172
00:14:35,440 --> 00:14:38,800
are usually quite forgetful. You might have found this, chat GPT after a long string. We'll forget

173
00:14:38,800 --> 00:14:42,880
what you said earlier on. We can now hook them up to long-term memory databases and have them

174
00:14:42,880 --> 00:14:46,560
reference things like many weeks or months in the past, which makes them a lot more capable.

175
00:14:47,680 --> 00:14:52,160
And we've also found that they perform much better if you give them these cognitive prompts.

176
00:14:52,160 --> 00:14:56,000
Like you tell it to do something, but then you say, you know, think about your answer, critique it,

177
00:14:56,000 --> 00:14:59,440
and then answer me again. And that actually improves the quality of the answer quite a lot.

178
00:15:00,080 --> 00:15:04,960
That's often called chain of thought prompting, self critique. It can observe what it knows and

179
00:15:04,960 --> 00:15:10,240
plan the next step. And it's getting these more cognitive capacities by adding on these kind of

180
00:15:11,120 --> 00:15:16,240
extra techniques. So this is being called the agent architecture, right? You tell the language

181
00:15:16,240 --> 00:15:21,680
model to act like an agent. It ends up as being like the centralized brain and you give it, you

182
00:15:21,680 --> 00:15:27,200
can say you can use any of these tools and then it composes which tools it wants to use to achieve

183
00:15:27,200 --> 00:15:31,520
your goals. So it ends up being a chain like this where you give it your goal, it'll like observe,

184
00:15:31,520 --> 00:15:36,720
it'll plan, it'll call a different tool, it'll observe, it'll plan. And we can actually do really

185
00:15:36,800 --> 00:15:42,240
complex, kind of scarily impressive things when we level up to these more sophisticated architectures.

186
00:15:43,360 --> 00:15:49,680
And actually on Monday, OpenAI did this big dev day talk, I don't know if people saw this,

187
00:15:49,680 --> 00:15:54,480
and they announced a new API called the assistance API that makes all that stuff that I showed that

188
00:15:54,480 --> 00:15:59,040
used to require quite a lot of Python code and kind of insider knowledge, and they're just making

189
00:15:59,040 --> 00:16:03,040
it super easy for everyone to now do this architecture, where you're able to kind of

190
00:16:03,120 --> 00:16:06,960
run any function, call any API, all hooked up to their really powerful models.

191
00:16:07,760 --> 00:16:11,920
So we're about to enter this phase where this very capable agent architecture is becoming

192
00:16:11,920 --> 00:16:16,080
pervasive and widespread and might be the foundation of a lot of new tools being built.

193
00:16:16,080 --> 00:16:19,680
So we're sort of on the precipice of really unnerving moment, let's say.

194
00:16:21,120 --> 00:16:26,160
Because agent architectures, I think, means we're about to enter a stage of sharing the web with

195
00:16:26,160 --> 00:16:31,520
non-human agents, right? These agents are very different to what we've currently noticed bots

196
00:16:31,520 --> 00:16:34,640
in the past, like a completely different architecture and set of capabilities.

197
00:16:35,520 --> 00:16:39,040
They're going to have a lot more data on how realistic humans behave,

198
00:16:39,040 --> 00:16:41,680
and they're rapidly going to get more and more capable as time goes on.

199
00:16:42,480 --> 00:16:46,880
And soon, probably already now, we're not going to be able to tell difference between

200
00:16:46,880 --> 00:16:52,080
these agents and real humans. If anyone else spends a lot of time on Twitter slash X,

201
00:16:52,080 --> 00:16:55,920
you'll already have noticed there's a lot of accounts you stumble across that have a weird

202
00:16:55,920 --> 00:17:00,400
vibe to them, and you definitely realize this is just chat GPT hooked up to a Twitter account,

203
00:17:00,400 --> 00:17:05,440
but otherwise is trying to look real, but like every tweet is very optimized and comes back in

204
00:17:05,440 --> 00:17:09,920
like a second. It's just replying to things, you know, three seconds later. So it's happening.

205
00:17:11,120 --> 00:17:14,400
And sharing the web, I want to say with agents, I don't want to jump to saying this is like

206
00:17:14,400 --> 00:17:18,320
inherently bad. I think they could have lots of good use cases, right? We could have

207
00:17:18,320 --> 00:17:23,120
automated moderators in communities. We could have search assistants, but I think it's mostly

208
00:17:23,120 --> 00:17:26,960
that it's going to get complicated, and this is going to be a huge product and cultural problem

209
00:17:26,960 --> 00:17:32,480
we're going to need to think about carefully and deal with. So we should get into why is this

210
00:17:32,480 --> 00:17:37,840
a problem for the web, right? I'm only going to focus on how this will affect human relationships

211
00:17:37,840 --> 00:17:43,360
and information quality on the web. Anything else, like how we might all end up unemployed or dead

212
00:17:43,360 --> 00:17:48,320
soon is like well beyond my pay grade, so I'm just limiting the space to just like how do we

213
00:17:48,880 --> 00:17:52,640
make meaningful human connections and find a good quality content on the web.

214
00:17:53,600 --> 00:17:59,440
Because, yeah, the cost of creating and publishing content just dropped to almost zero at this point,

215
00:18:00,080 --> 00:18:05,760
right? Like humans are quite expensive and slow at making content, right? We need time to research

216
00:18:05,760 --> 00:18:10,320
and think, and we like clumsily string words together, and then we want to take breaks,

217
00:18:10,320 --> 00:18:15,200
and we want to be able to nap and eat and sleep, and then we demand people pay us like

218
00:18:15,200 --> 00:18:20,880
extraordinary rates, right, to do this research. And generative models don't need time off,

219
00:18:20,880 --> 00:18:24,240
and they don't get bored, and they cost like a couple fractions of a cent to write a few thousand

220
00:18:24,240 --> 00:18:30,480
words. So given the dynamics here, it's very likely that models are going to become the main

221
00:18:30,480 --> 00:18:37,440
generators of content online. So I think we're about to drown in a sea of informational garbage,

222
00:18:37,440 --> 00:18:42,320
right? I think we're just going to be absolutely swamped in masses of mediocre content. Like every

223
00:18:42,320 --> 00:18:46,240
marketer and SEO strategist and optimizer bro is just going to have a field day here, you know,

224
00:18:46,240 --> 00:18:51,760
just filling the whole internet with all of their keyword stuff, optimized crap. And this explosion

225
00:18:51,760 --> 00:18:56,320
of noise is going to make it really difficult to find both good quality people, real people,

226
00:18:56,320 --> 00:19:01,680
and good quality content, and hear any signal through the noise. And we can tell this is

227
00:19:01,680 --> 00:19:05,520
happening because scammers and scammers are currently quite lazy, and we're kind of in the

228
00:19:05,520 --> 00:19:10,880
baby phases of this. So there's a phrase that you might have seen chat GPT reply with. It sometimes

229
00:19:10,880 --> 00:19:15,040
says, as an AI language model, I do not have political beliefs, or as an AI language model,

230
00:19:15,120 --> 00:19:19,600
I cannot answer that question. And this phrase, if you search and direct quotes for it around the

231
00:19:19,600 --> 00:19:25,360
web, shows up everywhere, just like Amazon, Google, Yelp reviews, tweets, LinkedIn posts,

232
00:19:25,360 --> 00:19:29,200
it's full of this phrase, because people can't be bothered to like control F and like delete the

233
00:19:29,200 --> 00:19:34,720
one phrase that gives them away. All right. So I did a quick search for this on LinkedIn, it got

234
00:19:34,720 --> 00:19:40,720
16,000 hits, and they're like really boring attempts to like write engaging content, but they

235
00:19:40,720 --> 00:19:45,440
all begin with the phrase as an AI language model. Look in the first sentence. And these are real

236
00:19:45,440 --> 00:19:48,880
people too. I did look at their profiles. They genuinely have jobs, and they're trying to optimize

237
00:19:48,880 --> 00:19:55,680
their presence or something. But yeah, this is starting to happen. The motivation for doing

238
00:19:55,680 --> 00:19:59,840
this rate isn't hard to understand. So let's like think of the hypothetical scenario. So this is

239
00:19:59,840 --> 00:20:05,360
Nigel. He's written a book about why nepotism is great, right? And he wants to be a book fluencer.

240
00:20:05,360 --> 00:20:09,280
He's like, it's his first book, he's self-published on Amazon, he wants to like, you know, become a

241
00:20:09,280 --> 00:20:14,080
big book guy. So he spends up an agent, right? Not unlike an actual publishing agent. He might have

242
00:20:14,080 --> 00:20:20,400
hired in the past. And he says, hey, like, help me promote my book, you know? And so the agent

243
00:20:20,400 --> 00:20:25,520
thinks for a while, and it goes off, and it strategizes, and it generates a steady stream

244
00:20:25,520 --> 00:20:28,880
of tweets, right, based about on the content of the book, like real insights from the book,

245
00:20:28,880 --> 00:20:32,000
and it starts tweeting those out from Nigel's account, and he's given it access, you know?

246
00:20:33,120 --> 00:20:36,560
And it goes and it does the same thing for LinkedIn and Facebook, you know, pretty easy.

247
00:20:37,520 --> 00:20:41,680
And then it writes and schedules a newsletter to go out over the course of six months so that

248
00:20:41,680 --> 00:20:45,920
his followers will always kind of get updates on new things he's researching. It sets up a

249
00:20:45,920 --> 00:20:50,480
medium account, it reposts those as articles, right? Makes a set of addictive TikTok videos

250
00:20:50,480 --> 00:20:55,120
based on that content, generates a bunch of podcast episodes, use Nigel's voice. We can

251
00:20:55,120 --> 00:21:00,480
totally do that now. It's pretty easy. And then it finds a bunch of other people who like are

252
00:21:00,480 --> 00:21:04,720
talking about nepotism and starts replying to them on LinkedIn and Twitter and making friends,

253
00:21:04,720 --> 00:21:08,320
and maybe they're actually agents interacting with it, and like, it's a whole bunch of just

254
00:21:09,440 --> 00:21:14,240
agents interacting with agents. And none of this is different to what Nigel could do on his own.

255
00:21:14,240 --> 00:21:20,240
So we don't know that content moderation or spam filters are actually going to pick this

256
00:21:20,240 --> 00:21:24,960
stuff up, because maybe it's tweeting it slow enough that a human could have done it,

257
00:21:24,960 --> 00:21:28,080
and it really is in Nigel's voice. It's used his writing to write this content.

258
00:21:28,640 --> 00:21:31,840
We don't necessarily have automated ways to filter any of this out.

259
00:21:32,640 --> 00:21:37,840
And the thing is, without an agent, 99% of Nigel-type people wouldn't have gone to all

260
00:21:37,840 --> 00:21:42,160
this effort. They don't have the time and energy to have made all this content. But with the agent,

261
00:21:42,720 --> 00:21:49,040
suddenly, we have people like Nigel, but times 99 of them, able to create this amount of content

262
00:21:49,040 --> 00:21:53,120
all the time. And this is how we kind of get the flood of just tons of content more than we can

263
00:21:53,120 --> 00:21:59,120
really cope with. So the scale and the quality of the content is actually what's different here.

264
00:21:59,360 --> 00:22:03,120
Strangely enough, it might have written better stuff than Nigel ever would. We might have way

265
00:22:03,120 --> 00:22:08,240
better quality content about nepotism all over the internet. But you can imagine how this would

266
00:22:08,240 --> 00:22:14,080
play out at another 100x scale, right, with political lobbying groups who have very vested

267
00:22:14,080 --> 00:22:19,840
interests in certain ideas or beliefs or truths getting out into the world. Specific agendas,

268
00:22:20,560 --> 00:22:24,640
large companies that want you to believe certain things about their product or certain things

269
00:22:24,640 --> 00:22:29,280
about scientific claims. They all have access to these assistants and agents, too.

270
00:22:30,720 --> 00:22:33,600
So I do have some good news. Like, this has all been a bit dark, a bit of a downer.

271
00:22:34,560 --> 00:22:39,200
The good news is that this might not be a problem. Maybe this is all just fine, right?

272
00:22:39,920 --> 00:22:43,840
This is only a problem if we want to use the web for very particular purposes,

273
00:22:44,640 --> 00:22:51,120
such as facilitating genuine human relationships or pursuing collective sense-making and knowledge

274
00:22:51,120 --> 00:22:58,160
building or grounding our knowledge of the world in reality. So we don't care about any of these

275
00:22:58,160 --> 00:23:02,560
things. This is all fine. We're going to have amazing content on TikTok. We're going to be

276
00:23:02,560 --> 00:23:08,720
very entertained. The thing is, I'm quite keen on a lot of these outcomes. I write on the web a

277
00:23:08,720 --> 00:23:13,360
lot. I've had overwhelmingly positive experiences writing on the web and meeting people for doing

278
00:23:13,360 --> 00:23:18,800
that. I have this whole thing called digital gardening I bang on about, about making everyone

279
00:23:18,800 --> 00:23:23,440
publish their unfinished notes to the web and improve them over time and use that as a way

280
00:23:23,440 --> 00:23:28,880
to meet people interested in what you're interested in. But, yeah, the goal of that stuff is to make

281
00:23:29,680 --> 00:23:33,120
the web a space where that's possible for collective understanding and knowledge building.

282
00:23:33,760 --> 00:23:37,520
And I'm really worried that generative agents like meaningfully threaten this in the very near

283
00:23:37,520 --> 00:23:41,120
term, like the six to 12 month kind of time range. I can't even think beyond that.

284
00:23:43,120 --> 00:23:46,560
So when I talk to people about my worries, I talk to a lot of people in the AI safety and

285
00:23:46,640 --> 00:23:51,680
research world. They kind of go, why does it matter? My AI agent is going to make much better

286
00:23:51,680 --> 00:23:55,840
content than you ever would. Why do you care that an agent made it and not a human? I'm like,

287
00:23:55,840 --> 00:23:59,760
okay, let's engage with that question properly. I'm sympathetic to that point.

288
00:24:01,680 --> 00:24:04,160
So here's the reasons that generated content is a little bit different, right?

289
00:24:04,800 --> 00:24:09,360
The first is its connection to reality. The second is the social context they live within.

290
00:24:10,080 --> 00:24:13,840
And the third is its potential for human relationships. And I'm going to go into

291
00:24:13,840 --> 00:24:19,680
these in detail. So first, generated content, you probably have heard of this, is different

292
00:24:19,680 --> 00:24:24,320
because it has a different relationship to reality than we do, right? We are embodied humans,

293
00:24:24,320 --> 00:24:28,480
right? And we are sharing a physical reality. And we have all this rich embodied information,

294
00:24:28,480 --> 00:24:31,760
like we all understand we're in this kind of beautiful theater and we're in Brighton and we

295
00:24:31,760 --> 00:24:37,680
have a lot of physical embodied context about what we know about each other. And often what we're

296
00:24:37,680 --> 00:24:41,200
doing on the web is we're reading other people's accounts of this reality and we compare it against

297
00:24:41,200 --> 00:24:45,200
our own and we're like, do I agree with that? Is that really true? This is like the cycle of all

298
00:24:45,200 --> 00:24:49,600
of art and science and literature, you know, reading and comparing and writing your own version of

299
00:24:49,600 --> 00:24:56,160
things. And what we've done now is we've fed that huge trove of information into a neural network

300
00:24:56,160 --> 00:25:00,880
or a large language model. And it's created a sort of representation of that text, right? It's

301
00:25:00,880 --> 00:25:04,560
created a model of the things that we've already known about the world and have published to the

302
00:25:04,560 --> 00:25:09,360
web. And the thing is that model can now generate text that's predictably similar to what it was

303
00:25:09,360 --> 00:25:15,040
before, but it's totally unhinged from the physical reality that it once came from, right? It has

304
00:25:15,040 --> 00:25:19,440
some big connection. It did come from there. There's like a chain here, but it fully like cannot

305
00:25:19,440 --> 00:25:24,400
access that reality, right? Even if we put in robots, right, with like arms and eyes and ears,

306
00:25:24,400 --> 00:25:29,680
it can't sense the world the way we sense the world until we make like a fully synthetic like

307
00:25:29,680 --> 00:25:33,760
mimicry human that like is hooked up to a language model. But I think like 10 years away, I don't know,

308
00:25:33,760 --> 00:25:38,240
I think we have some time. Right, it can't validate its claims. It's the big thing.

309
00:25:40,000 --> 00:25:43,440
We politely call this hallucination, right? This is when language models say things that aren't

310
00:25:43,440 --> 00:25:47,920
true about the world. We say it's hallucinating, right? Like it's some side kind of very smart

311
00:25:47,920 --> 00:25:51,760
person on some like mild drugs who's confused about like who they are or where they are,

312
00:25:51,760 --> 00:25:55,360
but they're saying very intelligent things. You're sort of holding them lightly, you know?

313
00:25:57,200 --> 00:26:01,120
Language models are also different because they have a very different social context, right?

314
00:26:01,120 --> 00:26:05,360
They have a very strange relationship to our social world. So hopefully you know this, but

315
00:26:05,360 --> 00:26:10,400
everything you and I say is situated in a social context, right? We understand what we share in

316
00:26:10,400 --> 00:26:13,920
common. And if you met someone who spoke a different language from a different culture,

317
00:26:13,920 --> 00:26:17,680
you would not assume they thought the same things about the world that you would if you met someone

318
00:26:17,680 --> 00:26:22,880
from your own neighborhood, right? If one of us met someone from like the Kenzie in England,

319
00:26:22,880 --> 00:26:27,360
we would have very different understandings of like hygiene and science and like how the world

320
00:26:27,360 --> 00:26:30,800
works. We would know some things in common. We technically speak the same language,

321
00:26:30,800 --> 00:26:34,240
but we would know that we didn't have a shared social context in the same way.

322
00:26:36,080 --> 00:26:41,280
But a language model is not a person and it does not have a fixed reality, right? They know nothing

323
00:26:41,280 --> 00:26:45,760
about the cultural context of who they're talking to and they take on different characters depending

324
00:26:45,760 --> 00:26:50,320
on what you tell them to do. You can say, you know, pretend to be a professor, pretend to be an

325
00:26:50,320 --> 00:26:54,880
athlete, pretend to be a young child and it will take on that character. So it doesn't even have

326
00:26:54,880 --> 00:27:01,040
a fixed place it's talking to you from in the way that a human does. But they do represent a very

327
00:27:01,040 --> 00:27:06,880
particular way of seeing the world because we trained them primarily on text on the web that

328
00:27:06,880 --> 00:27:13,520
was generated by a majority English-speaking, like 95% of the training data is English-speaking,

329
00:27:14,560 --> 00:27:19,200
a primarily English-speaking westernized population, people who have mostly written a

330
00:27:19,200 --> 00:27:25,360
lot on Reddit and lived between about 1900 and 2023, which like in the grand scheme of history

331
00:27:25,360 --> 00:27:31,680
and geography is a very narrow slice of humanity, right? Of all possible cultures we've had in the

332
00:27:31,680 --> 00:27:36,800
past, all possible cultures we could have in the future and all possible languages. This is just

333
00:27:36,800 --> 00:27:42,000
such a small representation of reality and yet we're now making it the source of truth, right?

334
00:27:42,000 --> 00:27:48,480
The Oracle. You go to chat, GBT to ask everything. So it's taking this already dominant way of

335
00:27:48,480 --> 00:27:52,320
seeing the world and reinforcing that dominance, which is problematic and is like a whole different

336
00:27:52,320 --> 00:27:57,600
talk that I don't even think I'm qualified to do but someone should. And we hope that this will

337
00:27:57,600 --> 00:28:02,400
improve over time but it's really hard to do without lots of data and most cultures don't have the

338
00:28:02,400 --> 00:28:09,520
vast kind of written record that an English-speaking westernized online population does. So lastly,

339
00:28:10,320 --> 00:28:14,560
generated content lacks the potential for human relationships that human-made content does,

340
00:28:14,560 --> 00:28:20,400
right? If you write something online and I read it and I find it compelling, I can DM you on Twitter

341
00:28:20,400 --> 00:28:24,240
or I can find you on Blue Sky or I could find you somehow, ideally hopefully not on LinkedIn,

342
00:28:24,240 --> 00:28:28,160
but somehow and message you and be like, I love this. This was such good ideas. Like I want to

343
00:28:28,160 --> 00:28:32,560
write a piece in response to you and like we start having a little dialogue that I've had so many

344
00:28:32,560 --> 00:28:38,080
relationships blossom this way. But if you have a language model, it's not going to be able to do

345
00:28:38,080 --> 00:28:43,040
that. So this is a still from the film, Her, right? This has become kind of a cultural touch

346
00:28:43,040 --> 00:28:49,040
point of like parasocial relationships with AI. Hopefully people have seen it but if not,

347
00:28:49,040 --> 00:28:53,920
so Joaquin Phoenix, our lovely main character, he has this great relationship with his personal AI,

348
00:28:53,920 --> 00:28:59,760
he talks to her in his ear, it falls in love with her. But then the AI of course grows bored of him

349
00:28:59,760 --> 00:29:06,000
because he's a very kind of basic human and leaves and he's destroyed. And like some people were

350
00:29:06,000 --> 00:29:09,600
supposed to get this like film is supposed to be a warning, right? And some people took it as a

351
00:29:09,600 --> 00:29:18,720
suggestion. So there's a company called replica who make AI companions for you that you can

352
00:29:20,000 --> 00:29:24,400
make friends with, possibly date and fall in love with. There's a lot of suggestions of sort of

353
00:29:24,400 --> 00:29:30,160
lonely young men engaging with this and their marketing copy. And I mean, maybe I do need to

354
00:29:30,160 --> 00:29:36,480
explicitly point this out. An AI replica or any other kind of like generative agent person cannot

355
00:29:36,480 --> 00:29:40,800
fulfill all our human needs, right? They cannot give you a hug, they cannot come to your birthday

356
00:29:40,800 --> 00:29:47,440
party, they cannot kind of engage with you in a meaningful, full human way. And so any kind of

357
00:29:47,440 --> 00:29:51,680
language model agent on the internet has no capacity for that back and forth relationship.

358
00:29:51,680 --> 00:29:55,840
Even if it faked it, it's very unclear that it would actually satisfy what we need when we have

359
00:29:55,840 --> 00:30:00,640
an actual friend that we can go out to coffee with. So that all sounds quite bad again. Like

360
00:30:00,640 --> 00:30:05,760
deep breaths, the whole talk is really just digging you in a ditch, I'm sorry. But I'm now

361
00:30:05,760 --> 00:30:11,920
going to talk about possible futures. And again, these futures I think are not mutually exclusive,

362
00:30:11,920 --> 00:30:15,760
I think they all might unfold in different ways over the next five to 10 years. And like I can't

363
00:30:15,760 --> 00:30:20,160
speculate beyond that, God knows where we are. But yeah, I'm hoping they all kind of happen

364
00:30:20,160 --> 00:30:25,440
in parallel. So the first is I think we're about to spend a lot of time thinking about how we

365
00:30:25,440 --> 00:30:31,600
pass the reverse Turing test. So how do we prove we're human on a web filled with agents? So the

366
00:30:31,600 --> 00:30:36,480
original Turing test, you have a human talk to a computer and another human through like a wall

367
00:30:36,480 --> 00:30:41,200
so they can't see the typing messages of each other. And then the original test, the computer had

368
00:30:41,200 --> 00:30:46,880
to prove that it was the human. It had to prove it was competent. And on the new web, we are now the

369
00:30:46,880 --> 00:30:54,000
ones under scrutiny. We have to prove we're real. So we're going to end up like we will assume

370
00:30:54,000 --> 00:30:58,480
everyone is an agent until proven otherwise. So I kind of wrote this post where I was

371
00:30:59,200 --> 00:31:03,040
thinking about some short term tactics. Like we could use funny terminology. We could all try to

372
00:31:03,040 --> 00:31:07,120
become teenagers who like have this insider jargon that the language models don't know about, but

373
00:31:07,120 --> 00:31:10,320
they'll pick up on it pretty quick, you know, and then you'll have to abandon it, get a new jargon.

374
00:31:11,120 --> 00:31:14,800
We could write in non-dominant languages. If you speak something like Catalan or Welsh,

375
00:31:14,800 --> 00:31:18,400
you're probably in a pretty good position, you know, you'll be able to write in a way that's

376
00:31:18,400 --> 00:31:23,520
more native than a language model ever could. And ideally, we just do higher quality writing,

377
00:31:23,520 --> 00:31:29,120
we do more research, we do more critical thinking. We really reference events and people that could

378
00:31:29,120 --> 00:31:33,760
we could only know about from the real world, from being embodied humans in space. I don't know how

379
00:31:33,760 --> 00:31:38,000
long those kind of defenses will last, but that's in the short term something we can at least pull

380
00:31:38,000 --> 00:31:43,840
on. This next one, I apologize for the phrase, but it too perfectly, it catches the point. I'm

381
00:31:43,840 --> 00:31:50,560
not going to explain it. You can Google that later. But the point is that the content from models

382
00:31:50,560 --> 00:31:54,880
might end up becoming our source of truth, and that how we know things simply was like,

383
00:31:54,880 --> 00:31:59,280
well, a language model once said it, you know, and then it's forever captured in our circular flow

384
00:31:59,280 --> 00:32:02,960
of information. So right in this current model, right, the training data is at least based on

385
00:32:02,960 --> 00:32:07,760
real world experiences. It's kind of going in a single loop. But we're now going to use that

386
00:32:07,760 --> 00:32:14,080
generated text to train new models. And so we enter this loop where like there's this very

387
00:32:14,080 --> 00:32:18,960
tenuous link to the real world that was once a long time ago, the source of this data. This is

388
00:32:18,960 --> 00:32:23,200
already starting to happen. AI researchers are worried we're kind of going to run out of data to

389
00:32:23,200 --> 00:32:27,600
train models on within five years. And so there's a lot of talk of how do we generate information

390
00:32:27,600 --> 00:32:33,440
that we can feed the models, feed back into the models. This one was a really funny example of

391
00:32:33,440 --> 00:32:37,680
this happening. I don't know if people saw this. So someone noticed that if you ask Google if you

392
00:32:37,680 --> 00:32:42,240
can melt an egg, it's like smart AI summary said yes. And they were like, why would it say that?

393
00:32:42,240 --> 00:32:46,560
They weren't investigating. And it turned out that fact was pulled from Quora that was generated

394
00:32:46,560 --> 00:32:52,320
from chat GPT. And because Quora is considered a reputable website with good SEO standing,

395
00:32:52,320 --> 00:32:56,960
it got pulled up into the Google like smart answer. And it's not hard to imagine how all

396
00:32:56,960 --> 00:33:01,360
kinds of hallucinated answers are going to become part of this loop if all these major websites are

397
00:33:01,360 --> 00:33:06,400
using chat GPT or their own agent to generate answers and then they just feed it to each other.

398
00:33:06,400 --> 00:33:13,200
It's like at one point, can you melt an egg? This phenomenon is very worrying for the scientific

399
00:33:13,280 --> 00:33:17,520
community and they have good reason to be. We're already seeing a lot of evidence that scientific

400
00:33:17,520 --> 00:33:24,720
researchers are using language models to help them write papers. So again, this is a paper

401
00:33:24,720 --> 00:33:29,840
that was published in a genuine fairly legitimate journal, Environmental Science and Pollution

402
00:33:29,840 --> 00:33:35,280
Research. And it included the phrase regenerate response at the end of a paragraph, which is the

403
00:33:35,280 --> 00:33:41,840
button above chat GPT's input box. There's another one in August that was published on

404
00:33:41,920 --> 00:33:48,400
fossil fuel allocation that included the phrase as an AI language model. Now, there's a lot of

405
00:33:48,400 --> 00:33:51,600
debate in the community where they were like, well, this doesn't mean the science in these papers is

406
00:33:51,600 --> 00:33:55,200
totally false, right? They could have done real research on real experiments and they were just

407
00:33:55,200 --> 00:33:59,120
trying to get this paper out and at the end they went, you know what, chat GPT summarizes paragraph

408
00:33:59,120 --> 00:34:03,840
for me. That totally could have happened. But the problem is we don't know. There's no protocol for

409
00:34:03,840 --> 00:34:08,720
the transparency of how you say what you didn't, didn't use chat GPT or whatever kind of model for.

410
00:34:09,120 --> 00:34:14,960
So now people are trying to make it more legitimate. Some people are listing chat GPT as an author on

411
00:34:14,960 --> 00:34:20,880
papers. And there's a real risk that people with much worse intentions will kind of take this and

412
00:34:20,880 --> 00:34:25,440
run with it and just make scientific paper mills. There's a lot of companies that have a lot of

413
00:34:25,440 --> 00:34:30,640
vested interests in publishing science that agree with the thing that they would like to be true.

414
00:34:30,640 --> 00:34:34,400
Usually you find this out when you get to the funding source section of the paper where you're

415
00:34:34,400 --> 00:34:39,360
like, oh, yeah, funded by the people who make this drug. Interesting. But you can tell that if

416
00:34:39,360 --> 00:34:43,280
they're going to be able to use generative models to kind of pump out lots of research papers, maybe

417
00:34:43,280 --> 00:34:48,640
based on dubious science, it becomes very hard for us to tell what is actually real, what is vetted.

418
00:34:48,640 --> 00:34:54,320
I mean, it just takes the whole, the replication crisis to a whole new level. So this one seems

419
00:34:54,320 --> 00:34:58,880
the most obvious, right? One possible future is we will just retreat further into the cozy web,

420
00:34:58,880 --> 00:35:04,080
right? The dark frost will grow larger and we will just go, okay, I'm only interacting with

421
00:35:04,080 --> 00:35:09,120
Discord and WhatsApp, right? LinkedIn is dead, Twitter is dead. We've had to abandon it. We have

422
00:35:09,120 --> 00:35:13,760
to maybe make new privatized gate kept spaces, which I think has a lot of downsides, but this

423
00:35:13,760 --> 00:35:18,560
just might be the best way to deal with it. I think authors are going to increasingly put

424
00:35:18,560 --> 00:35:22,000
content behind blocks and paywalls. I think this is already happening with things like

425
00:35:22,000 --> 00:35:27,680
sub-stack and medium, where you're constantly having to log in or prove that you are part of

426
00:35:27,680 --> 00:35:33,200
this community to access the content. You understand why authors do this, right? Because

427
00:35:33,200 --> 00:35:37,680
actually you're having your content scraped and then fed into generative models puts you in a

428
00:35:37,680 --> 00:35:43,440
disadvantage. Like your ideas could be taken out of context, maybe it's taken something that you

429
00:35:43,440 --> 00:35:48,240
wanted to actually charge for and given out for free. Someone could train a model on your work

430
00:35:48,240 --> 00:35:52,000
and have it start writing in your voice. There's a lot of ways this can go really badly for someone

431
00:35:52,000 --> 00:35:58,720
whose full-time job is being a researcher or a content creator. We'll also see more websites

432
00:35:58,800 --> 00:36:04,160
that have a large amount of content blocking scraping for language models, or one way to do

433
00:36:04,160 --> 00:36:09,360
it is to just charge a huge amount for your API. Reddit did this recently. We're going to put the

434
00:36:09,360 --> 00:36:13,360
price so high that no company in their right mind would really pay it, or it would just cost them

435
00:36:13,360 --> 00:36:19,360
so much. Twitter kind of did the same. It's hard to tell if this was actually strategic or on purpose,

436
00:36:19,360 --> 00:36:25,280
but raising the price to whatever it was, like 42,000 per month, means that very few people

437
00:36:25,280 --> 00:36:31,200
can access Twitter's really high-quality content in an age where content to train models is the

438
00:36:31,200 --> 00:36:36,240
new goals. It's kind of leading us to a place where the web is not open by default. You can't

439
00:36:36,240 --> 00:36:40,880
just query any API for any content you want. Everything is locked down and gatecapped and

440
00:36:40,880 --> 00:36:45,920
kind of cordoned off. Next, I think we're going to have what I'm calling the Meet Space Premium.

441
00:36:47,840 --> 00:36:52,320
We are in the Meet Space Premium. It's when we begin to prefer and preference offline first

442
00:36:52,320 --> 00:36:58,320
interactions. So we will start to doubt all people online. And the only way to confirm someone's

443
00:36:58,320 --> 00:37:02,160
humanity is to meet them in person, right, to go for coffee or beer. And once you do that,

444
00:37:02,160 --> 00:37:06,240
you can kind of set up a little trust network, right? You can say, oh, I've already met Sarah

445
00:37:06,240 --> 00:37:09,920
over there, and she's a real human, and you've already met Tom, and he's a real human, and we can

446
00:37:09,920 --> 00:37:14,480
kind of like coordinate our networks to vet who's real on the web. And then when you read their

447
00:37:14,480 --> 00:37:18,880
writing, you kind of know it's from an actual person, or you'd hope so. You may be of some

448
00:37:18,880 --> 00:37:21,920
trust network of people who aren't writing generated stuff under their own name.

449
00:37:23,040 --> 00:37:26,160
I think this has knock-on effects. Like, people might move back to cities or

450
00:37:26,160 --> 00:37:30,960
higher population and places. In-person events are going to be preferable. I think there's

451
00:37:30,960 --> 00:37:35,040
obvious disadvantages to this, right? The web was this huge democratization thing to enable

452
00:37:35,040 --> 00:37:39,120
people who are maybe disabled or have young children or who are caregivers who can't get

453
00:37:39,120 --> 00:37:42,960
out of the house for a whole bunch of reasons aren't going to have the same access to the trust

454
00:37:42,960 --> 00:37:46,400
network that someone can who could physically show up in space a lot.

455
00:37:49,200 --> 00:37:53,040
Yeah. So, a natural follow-on from this also. So, a lot of people have been like,

456
00:37:53,040 --> 00:37:57,680
well, why don't we just put it on the blockchain, you know? Why don't we just get a third party

457
00:37:57,680 --> 00:38:02,400
to verify our humanity with a cryptographic key, and then you can sign all your published

458
00:38:02,400 --> 00:38:06,480
content with it, and it'll link back to your identity, and this is how we'll have decentralized

459
00:38:06,480 --> 00:38:10,640
trust networks. And I'm like, okay, I don't know the details of this. This sounds weird.

460
00:38:10,640 --> 00:38:16,240
So, there's a project called Worldcoin that, funnily enough, is also funded by OpenAI's

461
00:38:16,320 --> 00:38:20,560
leader, Sam Altman. He partially helped found it, which shows he kind of knows the problem he's

462
00:38:20,560 --> 00:38:25,920
helped contribute to. So, this scary orb scans your eyeball to confirm your identity,

463
00:38:25,920 --> 00:38:30,560
and then it creates a unique human credential for you to use online to sign all your stuff with.

464
00:38:30,560 --> 00:38:34,080
It's really not taking off as a project, but it's around. And people are still trying to do this.

465
00:38:34,080 --> 00:38:39,040
There's a whole community thinking this is the future that I don't have sophisticated thoughts

466
00:38:39,040 --> 00:38:44,000
on yet, and I'm still like, oh, cryptocurrency. But maybe this is some way to get around it.

467
00:38:44,960 --> 00:38:48,560
I'm also expecting any day that Elon's going to announce the purple check,

468
00:38:48,560 --> 00:38:52,160
where you pay $30 a month, and you don't actually have to. You just take a box that's like,

469
00:38:52,160 --> 00:38:57,920
I'm human, and then you get this little check and solve it. So, those are all a bit negative.

470
00:38:57,920 --> 00:39:02,400
I do think there is some hope in this future. We can certainly fight fire with fire.

471
00:39:02,400 --> 00:39:06,320
So, I think it's reasonable to assume that we're all going to have a set of personal language

472
00:39:06,320 --> 00:39:11,280
models to kind of help defend us and serve our needs on the web. They can filter information,

473
00:39:11,280 --> 00:39:16,080
they can manage information. And I expect these to be baked into browsers, or maybe even the

474
00:39:16,080 --> 00:39:20,080
operating system, right? And they're going to do things like identify generated content,

475
00:39:20,080 --> 00:39:24,000
they're going to debunk claims, they're going to flag misinformation, they're going to go

476
00:39:24,000 --> 00:39:29,040
help hunt down real scientific sources for you, maybe vet scientific papers, curate and suggest

477
00:39:29,040 --> 00:39:32,800
things to you. So, I think this actually could work in both directions. It's not just all like

478
00:39:32,800 --> 00:39:37,360
the bad actors get this power. We also get a lot of capacity and capability from these models.

479
00:39:38,320 --> 00:39:42,800
We might find it absurd that anyone would browse like the raw web without one of these

480
00:39:42,800 --> 00:39:46,160
kind of in tow. It's the same way you wouldn't like go onto the dark web, like you know what's

481
00:39:46,160 --> 00:39:51,520
there, but you know you don't want to see it. It might be kind of like that. Okay, so I'm almost

482
00:39:51,520 --> 00:39:55,840
done wrapping this up. But the question I want to leave everyone with is which of these possible

483
00:39:55,840 --> 00:40:01,200
futures would you like to make happen, right? Generative AI is not necessarily a destructive

484
00:40:01,200 --> 00:40:05,040
force, you know, as with all technology, it depends how you wield it. Oh, I went back to the

485
00:40:05,040 --> 00:40:12,080
wrong slide, sorry. There we go. The way that we choose to deploy this in the world is really

486
00:40:12,080 --> 00:40:15,520
what matters, right? The product decisions we make as individuals and companies, if you are

487
00:40:15,520 --> 00:40:20,880
working in the space or you're trying to get into it. Because obviously if you are working on a tool

488
00:40:20,880 --> 00:40:27,840
that like turns out tons of human-like content from marketing and influence purposes, like

489
00:40:27,840 --> 00:40:33,040
you can stop, like that's really like we don't need that. You can just stop doing that. But what

490
00:40:33,040 --> 00:40:37,200
should you be building instead? It's maybe a more helpful question. So I tried to come up with a few

491
00:40:37,200 --> 00:40:41,200
principles for building products language models that are probably going to evolve over time, but

492
00:40:41,200 --> 00:40:47,440
this is like a first pass. And the first is to protect human agency. The second is to treat models

493
00:40:47,440 --> 00:40:52,800
as reasoning engines and not sources of truth. And lastly that we should be augmenting our

494
00:40:52,800 --> 00:40:58,960
cognitive abilities and not replacing them. So protecting human agency, this is like usually

495
00:40:59,040 --> 00:41:03,120
at the moment you have a human prompter and it hands something off to an autonomous agent

496
00:41:03,120 --> 00:41:07,360
and the agent goes and does stuff, right? This is like the open AI assistance model or any of

497
00:41:07,360 --> 00:41:10,800
the architectures I showed before. And this is like the path to self-destruction. This is what

498
00:41:10,800 --> 00:41:15,120
most AI safety researchers are very afraid of is that the locus of agency sits within the agent.

499
00:41:16,400 --> 00:41:20,240
But the ideal form of this is that the locus of agent stays within the human

500
00:41:20,240 --> 00:41:24,080
and it has a collaborative agent on hand and there's this very short continuous feedback loop

501
00:41:24,080 --> 00:41:28,320
that is constantly going between them. Where the human is the one checking, should I do that? Do

502
00:41:28,320 --> 00:41:33,840
I want that? Is that true? Like they're able to actually fact check things and then the agent is

503
00:41:33,840 --> 00:41:40,960
much more of a helper. Short feedback loops, close supervision, limited power, it's slower but it's

504
00:41:40,960 --> 00:41:47,360
safer. That ties into the second principle that we should treat models as tiny reasoning engines

505
00:41:47,360 --> 00:41:52,160
and not sources of truth. So one way to use these models is to like ask it for every answer and

506
00:41:52,160 --> 00:41:57,520
ask it every question and trust what it says. Another one is you can train them to do specific

507
00:41:57,520 --> 00:42:03,360
things like just summarize this text, just extract data from this paper, just find contradictions in

508
00:42:03,360 --> 00:42:08,400
this statement and then you can bring your own data which could be legitimate scientific papers,

509
00:42:08,400 --> 00:42:12,480
it could be your own notes, it could be Wikipedia and then you use these models to just do these

510
00:42:12,480 --> 00:42:16,880
very small scoped things where you can observe every single output and check that it's actually

511
00:42:16,880 --> 00:42:21,360
legitimate and you're not handing off this big complex task to this big black box model.

512
00:42:22,240 --> 00:42:28,480
And lastly, we should augment our cognitive abilities and not replace them, right? Language models

513
00:42:28,480 --> 00:42:33,280
are very good at things that humans are not good at like searching and discovering things in large

514
00:42:33,280 --> 00:42:37,840
datasets, role playing as identities and characters, they're actually really good at doing that,

515
00:42:38,400 --> 00:42:43,040
rapidly organizing data, turning fuzzy inputs into structured outputs, there's a lot that they're

516
00:42:43,040 --> 00:42:47,040
good at that we're bad at and we should use them for those things. There's tons that like we're

517
00:42:47,040 --> 00:42:50,960
good at they're not that we're still trying to like make them do like checking claims against

518
00:42:50,960 --> 00:42:56,480
physical reality, long-term memory, having embodied knowledge, understanding social context,

519
00:42:56,480 --> 00:43:01,440
having emotional intelligence, I think combining the two of these so that we're doing things models

520
00:43:01,440 --> 00:43:06,320
can't do and they're doing things we're not very good at actually leverages the best of both worlds.

521
00:43:07,360 --> 00:43:11,280
Because a lot of AI researchers in the moment, they use this metaphor of aliens, this is from

522
00:43:11,280 --> 00:43:17,760
the 1970s alien film or frightening, it just it makes me think this is like not the most appealing

523
00:43:17,760 --> 00:43:22,960
collaborative partner this metaphor, this like big scary unknown consciousness that like might

524
00:43:22,960 --> 00:43:28,880
kill you, but there's another metaphor that I like more that Kate Darling is a robotics researcher

525
00:43:28,880 --> 00:43:33,360
at MIT and she wrote this book called The New Breed, arguing we should think about robots as

526
00:43:33,360 --> 00:43:38,480
animals, we have a long cultural legal history with animals and working collaboratively with them,

527
00:43:38,480 --> 00:43:44,240
oxen, dogs, pigs, right in this very like mutually beneficial relationship most of the time and this

528
00:43:44,240 --> 00:43:48,480
is actually a pretty good metaphor to expand to AI where we have to kind of treat them a little bit

529
00:43:48,480 --> 00:43:54,240
like some form of intelligent species but one that we are in community with and are part of our

530
00:43:54,240 --> 00:43:59,600
systems and are not this like big scary alien who might come kill us all is like usually what it

531
00:43:59,600 --> 00:44:06,720
gets talked about as. So yeah there's this big push for this philosophical approach, some people

532
00:44:06,720 --> 00:44:10,720
call it cyber-organism, there's a very long article there was a written on less wrong which is not

533
00:44:10,720 --> 00:44:15,440
my favorite website but it's a good article that kind of goes in depth into this if you do want

534
00:44:15,440 --> 00:44:21,680
to read more about it. So that's all I have, I want to thank you so much for listening, again

535
00:44:21,680 --> 00:44:28,640
slide the notes on this QR code if you like missed anything, I'm on Twitter X at mappleton still

536
00:44:28,640 --> 00:44:33,920
until that really does fall apart and you can DM me there, you can message me again I love meeting

537
00:44:33,920 --> 00:44:38,880
people through writing on the web if you have blogs that like relate to these kind of topics

538
00:44:38,880 --> 00:44:50,880
send them to me but yeah thank you so much for listening, I appreciate it.

