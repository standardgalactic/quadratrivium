start	end	text
0	16480	Hi, thank you. Thank you so much for having me. I'm very excited to not tell you how AI
16480	22320	is going to save us all. This talk is called The Expanding Dark Forest in Generative AI.
22320	27360	It's going to be about writing on the web, trust, and human relationships, so like small
27360	33520	fish. And also AI, unfortunately, I'm sorry. There always has to be one AI talk at every
33520	36760	conference at this point, but at least you get me and not someone telling you how it's
36760	41920	going to take all your jobs. So I only give a footnote with this talk where I say, well,
41920	45560	this talk is up to date as of about a week ago, as of about Monday. So anything that's
45560	50160	happened since Monday, I can't take accountability for. It's probably all out of date by now,
50160	55680	but this is the state of the AI industry. So first some context. This is me. I look
55760	60960	like this on the internet. My name is Maggie. I am a designer at an AI research lab called
60960	66840	ELICIT. We do use language models to help scientists and researchers do literature review,
66840	71080	which is a long, boring task, creating many thousands of PDFs, and this is something that
71080	76560	language models are actually quite good at helping with. I also am very online. I'm
76560	82120	very on Twitter, X, whatever you want to call it. I write a lot online, and this has led
82160	86520	to lots of really positive relationships, a lot of really good career success, and this
86520	90520	will become relevant later as to why I care so much about people in the future being able
90520	96680	to do that, to be able to write online and connect with others by writing online.
96680	100880	I'm also a cultural anthropologist. I originally trained in this for my undergraduate degree
100880	105360	before becoming a designer because you can't get paid much to be a cultural anthropologist,
105360	110160	but a lot of theory I learned there plays back into my thoughts on design and development
110360	116960	and how we build things on the web. If you want to take notes on things, you can also
116960	121040	just scan this QR code, and this whole talk is transcribed with slides and everything
121040	125520	on this link, so don't worry about taking pictures of slides you like or trying to remember
125520	128800	what I said. You can reference it later. I might have to update it a bit because the
128800	134680	talk evolves, but one version that's mostly the same is on that QR code.
134680	139440	Here's what I'm going to talk about. First, we're going to talk about the dark forest
139480	144520	theory of the web. What is that? Next, I'm going to talk about the state of generative
144520	151640	AI as of a week ago. I'm third going to present some problems, and then we can kind of talk
151640	154320	about whether they actually are problems. We can question whether we think they're legitimate
154320	159760	or not. Lastly, I'm going to talk about possible futures, so how to deal with all these hypothetical
159760	166480	problems that I present. First, to explain the dark forest theory of the web, I'm first
166520	172360	going to have to explain the dark forest theory of the universe. This is a theory that tries
172360	178040	to explain why we haven't found intelligent life in the universe. Here we are in the universe,
178040	183520	the pale blue dot, and as far as we know, we are the only intelligent life around. We've
183520	188720	been beaming out messages for like 60 years now with like SETI, trying to find other intelligent
188720	192560	life, trying to see if there are aliens or some other kind of being that could respond
192600	199600	to us. We haven't heard anything back. The big question here is why. Dark forest theory
199840	205320	says it's because the universe is like a dark forest at night. It's a place that seems
205320	212320	quiet and lifeless because if you make noise, the predators come eat you. If you draw attention
212320	217040	to yourself, you're going to be attacked and destroyed. It stands to reason that all the
217040	221800	other intelligent civilizations that may or may not exist have either died or learned
221840	228520	to shut up. And we don't know which one we are yet. So the web version of this builds
228520	233800	off this concept. It's a theory that was proposed by Nancy Strickler, who's a really good thinker
233800	239280	and writer back in 2019. And Nancy wrote this article describing what it feels like to be
239280	244760	in public spaces on the web around this time. And Nancy pointed out these two main vibes,
244760	250400	let's say. And the first is that being on the web often feels like a very lifeless, automated
250480	256320	place that's devoid of humans. It's got all this ads and click bait and predatory behaviors
256320	259640	and none of it feels like real humans are trying to connect with us on the web. It just
259640	265520	feels commercial. The second vibe, actually, here, so here we are on the web, right? And
265520	271320	we are naively writing a bunch of very sincere, authentic accounts of our lives and thoughts
271320	276040	and experiences and trying to make connections to the other intelligent humans. We're sending
276080	281560	out messages, trying to beam out life. But then what we hear in response is content that
281560	287400	seems very inauthentic and human. It sounds like a bunch of robots and automations doing
287400	292520	marketing automations and growth hacking, kind of pumping out generic click bait. So we've
292520	297800	seen all this stuff, right? This is like low-quality listicles, right? Productivity rubbish, growth
297800	303560	hacking advice, you know, banal motivational quotes, dramatic click bait. Like, a lot of
303600	308080	this may as well be automated, even if a human didn't make it in some sense, right? They're
308080	312840	rarely trying to communicate some sincere original of thought to other humans. They're
312840	317680	trying to get you to click, right, and rack up some views. And this flood of really low-quality
317680	323160	content has made us retreat away from the public spaces of the web. It's very costly for us
323160	329200	to spend time and energy wading through all this craft. So the second vibe of the dark web is
329280	336000	that there's a lot of unnecessarily antagonistic behavior at a large scale. So when we are putting
336000	340800	out all these signals, right, trying to authentically connect to other humans, we could become a
340800	347840	target, right? We risk the Twitter mob coming to eat us is what can happen. So there's a term on
347840	352000	Twitter called getting main character. I don't know if people have heard of this. And I don't
352000	356240	know if people remember, this is one year ago, Garden Lady. This was a very famous tweet, if
356320	361360	everyone saw this one, where this really lovely woman got on Twitter one morning and she said,
361360	365520	my husband and I wake up every morning and we bring our coffee out to the garden and we sit and
365520	369280	talk for hours and it never gets old and we never run out of things to talk about and I love him
369280	374320	so much. And everyone's like, that's such a nice tweet. That's so wonderful. And then this went
374320	380160	viral. They got picked up and the Twitter reply started rolling in. So someone said, that's cool.
380160	384320	I wake up every morning and fight my wavy traffic for an hour in Miami to get to work. That must be
384320	390320	nice. Someone else, I wake up at 6am, right? This is an unattainable goal for most people. Not
390320	395680	that she said it was a goal, but interpret it as you will. Another one, again, complaining about
395680	399440	the morning routine and then goes, it must be nice being a trust fund baby with not a care in the
399440	406000	world. So I thought this tech talk summed it up nicely. I don't care if something good happened
406000	412080	to you. It should have happened to me instead. So this seems like a dumb example, but it was a
412080	416320	really good moment on Twitter that shows kind of the energy flows that happen on these really
416320	421360	large-scale social media platforms, right? Someone can publish something that's very kind and nice
421360	426160	and it gets interpreted the wrong way. People take it in bad faith. They take it out of context.
426160	430880	They try to amplify it to unintended audiences. We will take every opportunity to misinterpret
430880	435440	things and be ungenerous to each other. And this is how we get cancelling some pylons and we've
435440	441600	all seen so many examples of this happening in what seems like a very unfair way. John Ronson
441600	444960	wrote an entire book about this called You've Been Publicly Shamed. The catalog is quite a
444960	449520	few of these. It's a little out of date now, but it's a lot of kind of classic original examples
449520	454400	of people getting cancels. And then the real material consequences they fix, right? They lose
454400	459200	jobs. They lose friends. They are alienated from their community. It is not just internet drama.
459200	464960	They really do have to face repercussions for these pylons. And so this makes the web a very
464960	469200	sincerely dangerous place to sincerely publish your thoughts, to publish honest things on.
471600	475520	And this makes it hard to find people, right? It's very difficult to find people who are being
475520	480400	sincere, who are seeking coherence and who are trying to build collective knowledge in public.
481360	485520	I know this is not what everyone wants to do with the web, right? Like some people just want to
485520	490240	dance on TikTok and that's completely fine. We have to let them do that. But I'm interested in
490240	496000	at least some of the web enabling this kind of productive discourse and having spaces of community
496000	500800	building and I'm hoping some people here feel the same, right? Rather than it being like this
500800	506000	threatening inhuman place where you can't actually say what you think. So how do we cope with this,
506000	511440	right? We're all wandering around this dark forest of like Facebook and LinkedIn and Twitter
511440	516640	and we realize we need to go somewhere safer. So what we end up doing is we retreat primarily to
516640	523120	what's being called the cozy web. So this was a term coined by Venkatesh Rao in direct response
523120	529040	to the dark web theory. And Venkat pointed out that we've all started going underground. We move
529040	534240	into semi-private spaces like newsletters or personal websites where you're less at risk of
534240	539520	attack. You're not on these big platforms. You're on your own separate domain or you're on your own
539520	544720	separate newsletter. So this gives us some safety. We can at least decide a little bit who reads it
544720	550800	and understand our audience. But we often retreat even further into gate kept spaces like slacks,
550800	556080	WhatsApps, Discord, Signal groups, right? This is where we end up spending the most of our time
556080	561280	and having real human relationships where we can express our ideas safely, right? So things
561280	566080	that we say we know will be taken in good faith in these smaller groups. We can engage in real
566080	571200	discussions. But there's some problems here, right? Like none of this is indexable or searchable.
571200	577040	It's very hard to include people who aren't already in the group. And it hides collective
577040	580720	knowledge in these private databases that are even hard for the users themselves to access.
581440	585120	And also like good luck finding anything on Discord. Like you'll never be able to use the
585120	592080	search functionality in these apps. So my current theory, sadly, is that the dark forest is about
592080	599280	to expand because we now have this thing called generative AI. So I'm sure everyone has mostly
599280	603120	heard of this, but what I'm talking about specifically here is machine learning models
603120	608080	and neural networks that can create content that before this point in history only humans could
608080	614320	make, right? This is text, images, audio and video that mimics human creations in a very
614320	620000	compelling and believable way. Here are kind of some of the major foundational models that you
620000	625360	might have heard of for different media types, right? We have GPT4 and clod for text, mid-journey
625360	630080	and stable diffusion for images. There's now video ones like runway ML. And you might have
630080	634560	heard some of the news that a lot of these models are now becoming multimodal so they can do text
634560	639600	to image, image to text, audio to text. Like you can kind of go anywhere you want with media here.
640560	645360	And this is, of course, chat GPT. I'm sure we've all seen a thousand screenshots of this at this
645360	651040	point. We understand what it is. But to recap, it's right, we know it can generate huge volumes
651040	656480	of high-quality text in seconds. The outputs are indistinguishable from human-made text when we
656480	661680	try to get people to guess what's chat GPT and what's human. They often can't. It's trained on a
661680	667200	huge volume of text scraped primarily from the English-speaking web. And this all sounds very
667200	671680	simple, right? But it leads to many kind of complex and potentially useful behaviors, but it's
671680	677280	very emergent. We don't really understand what's possible because of this capability yet. We can
677280	681360	also now, of course, generate images, right? This is mid-journey, which usually makes pretty
681360	687440	beautiful impressive stuff. But so we found that these, like, generative AI models are now very
687440	691120	easy to use and very widely accessible, right? They don't require technical skills. They're
691120	695520	incredibly cheap. And they're increasingly becoming a feature in existing software you already have
695520	698960	access to, like Adobe or Photoshop or Notion. They're just becoming pervasive.
701200	705360	But the product category that I'm most nervous about, not just, like, Notion generating a plan
705360	709680	for you, is what's being called content generators, mostly for content marketers. So I'm going to pick
709680	714720	on one product, but there are many. This one's called Blaze. And it creates articles and social
714720	718160	media content for you, right? In half the time, who wouldn't want that? Who doesn't want more
718160	723520	content on the internet? And so I want to show you how this works. So you decide what kind of
723520	728640	content you want to make, right? You can say a blog post or a newsletter or a bunch of Twitter posts.
728640	732960	And I'm going to say I want to write a blog post. And you type in what you want to write about,
732960	737040	and your target audience, and SEO keywords. So I've decided I want to write about why
737600	740960	plant-based meat is morally wrong, which I don't believe, but that sounds like a good clickbait
740960	743440	to me. Like, someone's going to be like, yeah, I want to find out why that's bad.
744480	748560	You know, maybe I'm a company that has some financial interest in plant-based meats going
748560	753840	badly. So I'm going to go ahead and have this model write a little article for me. It lets
753840	758640	me pick a title, which is a nice customization. And then it chans out 700 words, right? And this
758640	761840	is now ready for me to hit publish, right? Or at least gives me some base to work off.
762640	766080	And if I'm blobbing against plant-based meats, I can just generate 100 of these, right? And,
766080	770320	like, optimize them for Google SEO and publish them all at once. And, like, hard days out of
770320	774800	Cassie Dunn. Right? The quality and truthfulness of what's written in here is very questionable.
774800	779920	We'll get to problems with that later. But the point is, this is super easy to do at scale
779920	784800	very cheaply. And it essentially murders Google such, right? Like, this just does away with SEO
784800	790880	optimized content. Because anyone can publish this immediately. It gets even better at the end.
790880	794800	Like, it prompts me to generate more content. So it's like, oh, you have this blog post. Why not
794800	798800	generate LinkedIn posts and tweets and YouTube scripts and everything. We're not just getting
798800	805440	crappy Google articles. This is across every publishing platform. Right? So there's tons of
805440	808960	things to do this. There's, like, AI-linked post-generators, generate your next tweet,
808960	812240	right? YouTube content and autopilot, just thousands of these tools are pouring out.
814080	818240	So most of the examples I showed actually have a very simple architecture, right? You have a
818240	822880	single input, like, write me an article on plant-based meat. And you feed it into this big,
822880	826960	black mystery box of a language model, right? And we don't really understand totally what
827040	831120	happens inside, but it gives you an output, right? Rates you an essay. But you can't really tweak
831120	835280	what happened in the middle. You can edit the output, but you can't kind of pull the knobs
835280	840000	on the actual language model itself. Which isn't very sophisticated. We don't have a lot of control
840000	844960	or transparency in what's happening. But the industry has realized this is a problem, and we
844960	851040	started building architectures that are much more flexible and powerful. So we now have a language
851040	855440	model architecture where we take that same black box of the language model, but we give it access
855440	858800	to external tools, right? We say, okay, now we're going to tell it it can search the web through
858800	864720	an API. We give it access to a calculator. We give it access to a code REPL and APIs. It's now
864720	870160	getting a lot more capable, right? It can now look up, can do maths, you know? It can look up
870160	875440	information that things might not be right. It can double check its answers. Also language models
875440	878800	are usually quite forgetful. You might have found this, chat GPT after a long string. We'll forget
878800	882880	what you said earlier on. We can now hook them up to long-term memory databases and have them
882880	886560	reference things like many weeks or months in the past, which makes them a lot more capable.
887680	892160	And we've also found that they perform much better if you give them these cognitive prompts.
892160	896000	Like you tell it to do something, but then you say, you know, think about your answer, critique it,
896000	899440	and then answer me again. And that actually improves the quality of the answer quite a lot.
900080	904960	That's often called chain of thought prompting, self critique. It can observe what it knows and
904960	910240	plan the next step. And it's getting these more cognitive capacities by adding on these kind of
911120	916240	extra techniques. So this is being called the agent architecture, right? You tell the language
916240	921680	model to act like an agent. It ends up as being like the centralized brain and you give it, you
921680	927200	can say you can use any of these tools and then it composes which tools it wants to use to achieve
927200	931520	your goals. So it ends up being a chain like this where you give it your goal, it'll like observe,
931520	936720	it'll plan, it'll call a different tool, it'll observe, it'll plan. And we can actually do really
936800	942240	complex, kind of scarily impressive things when we level up to these more sophisticated architectures.
943360	949680	And actually on Monday, OpenAI did this big dev day talk, I don't know if people saw this,
949680	954480	and they announced a new API called the assistance API that makes all that stuff that I showed that
954480	959040	used to require quite a lot of Python code and kind of insider knowledge, and they're just making
959040	963040	it super easy for everyone to now do this architecture, where you're able to kind of
963120	966960	run any function, call any API, all hooked up to their really powerful models.
967760	971920	So we're about to enter this phase where this very capable agent architecture is becoming
971920	976080	pervasive and widespread and might be the foundation of a lot of new tools being built.
976080	979680	So we're sort of on the precipice of really unnerving moment, let's say.
981120	986160	Because agent architectures, I think, means we're about to enter a stage of sharing the web with
986160	991520	non-human agents, right? These agents are very different to what we've currently noticed bots
991520	994640	in the past, like a completely different architecture and set of capabilities.
995520	999040	They're going to have a lot more data on how realistic humans behave,
999040	1001680	and they're rapidly going to get more and more capable as time goes on.
1002480	1006880	And soon, probably already now, we're not going to be able to tell difference between
1006880	1012080	these agents and real humans. If anyone else spends a lot of time on Twitter slash X,
1012080	1015920	you'll already have noticed there's a lot of accounts you stumble across that have a weird
1015920	1020400	vibe to them, and you definitely realize this is just chat GPT hooked up to a Twitter account,
1020400	1025440	but otherwise is trying to look real, but like every tweet is very optimized and comes back in
1025440	1029920	like a second. It's just replying to things, you know, three seconds later. So it's happening.
1031120	1034400	And sharing the web, I want to say with agents, I don't want to jump to saying this is like
1034400	1038320	inherently bad. I think they could have lots of good use cases, right? We could have
1038320	1043120	automated moderators in communities. We could have search assistants, but I think it's mostly
1043120	1046960	that it's going to get complicated, and this is going to be a huge product and cultural problem
1046960	1052480	we're going to need to think about carefully and deal with. So we should get into why is this
1052480	1057840	a problem for the web, right? I'm only going to focus on how this will affect human relationships
1057840	1063360	and information quality on the web. Anything else, like how we might all end up unemployed or dead
1063360	1068320	soon is like well beyond my pay grade, so I'm just limiting the space to just like how do we
1068880	1072640	make meaningful human connections and find a good quality content on the web.
1073600	1079440	Because, yeah, the cost of creating and publishing content just dropped to almost zero at this point,
1080080	1085760	right? Like humans are quite expensive and slow at making content, right? We need time to research
1085760	1090320	and think, and we like clumsily string words together, and then we want to take breaks,
1090320	1095200	and we want to be able to nap and eat and sleep, and then we demand people pay us like
1095200	1100880	extraordinary rates, right, to do this research. And generative models don't need time off,
1100880	1104240	and they don't get bored, and they cost like a couple fractions of a cent to write a few thousand
1104240	1110480	words. So given the dynamics here, it's very likely that models are going to become the main
1110480	1117440	generators of content online. So I think we're about to drown in a sea of informational garbage,
1117440	1122320	right? I think we're just going to be absolutely swamped in masses of mediocre content. Like every
1122320	1126240	marketer and SEO strategist and optimizer bro is just going to have a field day here, you know,
1126240	1131760	just filling the whole internet with all of their keyword stuff, optimized crap. And this explosion
1131760	1136320	of noise is going to make it really difficult to find both good quality people, real people,
1136320	1141680	and good quality content, and hear any signal through the noise. And we can tell this is
1141680	1145520	happening because scammers and scammers are currently quite lazy, and we're kind of in the
1145520	1150880	baby phases of this. So there's a phrase that you might have seen chat GPT reply with. It sometimes
1150880	1155040	says, as an AI language model, I do not have political beliefs, or as an AI language model,
1155120	1159600	I cannot answer that question. And this phrase, if you search and direct quotes for it around the
1159600	1165360	web, shows up everywhere, just like Amazon, Google, Yelp reviews, tweets, LinkedIn posts,
1165360	1169200	it's full of this phrase, because people can't be bothered to like control F and like delete the
1169200	1174720	one phrase that gives them away. All right. So I did a quick search for this on LinkedIn, it got
1174720	1180720	16,000 hits, and they're like really boring attempts to like write engaging content, but they
1180720	1185440	all begin with the phrase as an AI language model. Look in the first sentence. And these are real
1185440	1188880	people too. I did look at their profiles. They genuinely have jobs, and they're trying to optimize
1188880	1195680	their presence or something. But yeah, this is starting to happen. The motivation for doing
1195680	1199840	this rate isn't hard to understand. So let's like think of the hypothetical scenario. So this is
1199840	1205360	Nigel. He's written a book about why nepotism is great, right? And he wants to be a book fluencer.
1205360	1209280	He's like, it's his first book, he's self-published on Amazon, he wants to like, you know, become a
1209280	1214080	big book guy. So he spends up an agent, right? Not unlike an actual publishing agent. He might have
1214080	1220400	hired in the past. And he says, hey, like, help me promote my book, you know? And so the agent
1220400	1225520	thinks for a while, and it goes off, and it strategizes, and it generates a steady stream
1225520	1228880	of tweets, right, based about on the content of the book, like real insights from the book,
1228880	1232000	and it starts tweeting those out from Nigel's account, and he's given it access, you know?
1233120	1236560	And it goes and it does the same thing for LinkedIn and Facebook, you know, pretty easy.
1237520	1241680	And then it writes and schedules a newsletter to go out over the course of six months so that
1241680	1245920	his followers will always kind of get updates on new things he's researching. It sets up a
1245920	1250480	medium account, it reposts those as articles, right? Makes a set of addictive TikTok videos
1250480	1255120	based on that content, generates a bunch of podcast episodes, use Nigel's voice. We can
1255120	1260480	totally do that now. It's pretty easy. And then it finds a bunch of other people who like are
1260480	1264720	talking about nepotism and starts replying to them on LinkedIn and Twitter and making friends,
1264720	1268320	and maybe they're actually agents interacting with it, and like, it's a whole bunch of just
1269440	1274240	agents interacting with agents. And none of this is different to what Nigel could do on his own.
1274240	1280240	So we don't know that content moderation or spam filters are actually going to pick this
1280240	1284960	stuff up, because maybe it's tweeting it slow enough that a human could have done it,
1284960	1288080	and it really is in Nigel's voice. It's used his writing to write this content.
1288640	1291840	We don't necessarily have automated ways to filter any of this out.
1292640	1297840	And the thing is, without an agent, 99% of Nigel-type people wouldn't have gone to all
1297840	1302160	this effort. They don't have the time and energy to have made all this content. But with the agent,
1302720	1309040	suddenly, we have people like Nigel, but times 99 of them, able to create this amount of content
1309040	1313120	all the time. And this is how we kind of get the flood of just tons of content more than we can
1313120	1319120	really cope with. So the scale and the quality of the content is actually what's different here.
1319360	1323120	Strangely enough, it might have written better stuff than Nigel ever would. We might have way
1323120	1328240	better quality content about nepotism all over the internet. But you can imagine how this would
1328240	1334080	play out at another 100x scale, right, with political lobbying groups who have very vested
1334080	1339840	interests in certain ideas or beliefs or truths getting out into the world. Specific agendas,
1340560	1344640	large companies that want you to believe certain things about their product or certain things
1344640	1349280	about scientific claims. They all have access to these assistants and agents, too.
1350720	1353600	So I do have some good news. Like, this has all been a bit dark, a bit of a downer.
1354560	1359200	The good news is that this might not be a problem. Maybe this is all just fine, right?
1359920	1363840	This is only a problem if we want to use the web for very particular purposes,
1364640	1371120	such as facilitating genuine human relationships or pursuing collective sense-making and knowledge
1371120	1378160	building or grounding our knowledge of the world in reality. So we don't care about any of these
1378160	1382560	things. This is all fine. We're going to have amazing content on TikTok. We're going to be
1382560	1388720	very entertained. The thing is, I'm quite keen on a lot of these outcomes. I write on the web a
1388720	1393360	lot. I've had overwhelmingly positive experiences writing on the web and meeting people for doing
1393360	1398800	that. I have this whole thing called digital gardening I bang on about, about making everyone
1398800	1403440	publish their unfinished notes to the web and improve them over time and use that as a way
1403440	1408880	to meet people interested in what you're interested in. But, yeah, the goal of that stuff is to make
1409680	1413120	the web a space where that's possible for collective understanding and knowledge building.
1413760	1417520	And I'm really worried that generative agents like meaningfully threaten this in the very near
1417520	1421120	term, like the six to 12 month kind of time range. I can't even think beyond that.
1423120	1426560	So when I talk to people about my worries, I talk to a lot of people in the AI safety and
1426640	1431680	research world. They kind of go, why does it matter? My AI agent is going to make much better
1431680	1435840	content than you ever would. Why do you care that an agent made it and not a human? I'm like,
1435840	1439760	okay, let's engage with that question properly. I'm sympathetic to that point.
1441680	1444160	So here's the reasons that generated content is a little bit different, right?
1444800	1449360	The first is its connection to reality. The second is the social context they live within.
1450080	1453840	And the third is its potential for human relationships. And I'm going to go into
1453840	1459680	these in detail. So first, generated content, you probably have heard of this, is different
1459680	1464320	because it has a different relationship to reality than we do, right? We are embodied humans,
1464320	1468480	right? And we are sharing a physical reality. And we have all this rich embodied information,
1468480	1471760	like we all understand we're in this kind of beautiful theater and we're in Brighton and we
1471760	1477680	have a lot of physical embodied context about what we know about each other. And often what we're
1477680	1481200	doing on the web is we're reading other people's accounts of this reality and we compare it against
1481200	1485200	our own and we're like, do I agree with that? Is that really true? This is like the cycle of all
1485200	1489600	of art and science and literature, you know, reading and comparing and writing your own version of
1489600	1496160	things. And what we've done now is we've fed that huge trove of information into a neural network
1496160	1500880	or a large language model. And it's created a sort of representation of that text, right? It's
1500880	1504560	created a model of the things that we've already known about the world and have published to the
1504560	1509360	web. And the thing is that model can now generate text that's predictably similar to what it was
1509360	1515040	before, but it's totally unhinged from the physical reality that it once came from, right? It has
1515040	1519440	some big connection. It did come from there. There's like a chain here, but it fully like cannot
1519440	1524400	access that reality, right? Even if we put in robots, right, with like arms and eyes and ears,
1524400	1529680	it can't sense the world the way we sense the world until we make like a fully synthetic like
1529680	1533760	mimicry human that like is hooked up to a language model. But I think like 10 years away, I don't know,
1533760	1538240	I think we have some time. Right, it can't validate its claims. It's the big thing.
1540000	1543440	We politely call this hallucination, right? This is when language models say things that aren't
1543440	1547920	true about the world. We say it's hallucinating, right? Like it's some side kind of very smart
1547920	1551760	person on some like mild drugs who's confused about like who they are or where they are,
1551760	1555360	but they're saying very intelligent things. You're sort of holding them lightly, you know?
1557200	1561120	Language models are also different because they have a very different social context, right?
1561120	1565360	They have a very strange relationship to our social world. So hopefully you know this, but
1565360	1570400	everything you and I say is situated in a social context, right? We understand what we share in
1570400	1573920	common. And if you met someone who spoke a different language from a different culture,
1573920	1577680	you would not assume they thought the same things about the world that you would if you met someone
1577680	1582880	from your own neighborhood, right? If one of us met someone from like the Kenzie in England,
1582880	1587360	we would have very different understandings of like hygiene and science and like how the world
1587360	1590800	works. We would know some things in common. We technically speak the same language,
1590800	1594240	but we would know that we didn't have a shared social context in the same way.
1596080	1601280	But a language model is not a person and it does not have a fixed reality, right? They know nothing
1601280	1605760	about the cultural context of who they're talking to and they take on different characters depending
1605760	1610320	on what you tell them to do. You can say, you know, pretend to be a professor, pretend to be an
1610320	1614880	athlete, pretend to be a young child and it will take on that character. So it doesn't even have
1614880	1621040	a fixed place it's talking to you from in the way that a human does. But they do represent a very
1621040	1626880	particular way of seeing the world because we trained them primarily on text on the web that
1626880	1633520	was generated by a majority English-speaking, like 95% of the training data is English-speaking,
1634560	1639200	a primarily English-speaking westernized population, people who have mostly written a
1639200	1645360	lot on Reddit and lived between about 1900 and 2023, which like in the grand scheme of history
1645360	1651680	and geography is a very narrow slice of humanity, right? Of all possible cultures we've had in the
1651680	1656800	past, all possible cultures we could have in the future and all possible languages. This is just
1656800	1662000	such a small representation of reality and yet we're now making it the source of truth, right?
1662000	1668480	The Oracle. You go to chat, GBT to ask everything. So it's taking this already dominant way of
1668480	1672320	seeing the world and reinforcing that dominance, which is problematic and is like a whole different
1672320	1677600	talk that I don't even think I'm qualified to do but someone should. And we hope that this will
1677600	1682400	improve over time but it's really hard to do without lots of data and most cultures don't have the
1682400	1689520	vast kind of written record that an English-speaking westernized online population does. So lastly,
1690320	1694560	generated content lacks the potential for human relationships that human-made content does,
1694560	1700400	right? If you write something online and I read it and I find it compelling, I can DM you on Twitter
1700400	1704240	or I can find you on Blue Sky or I could find you somehow, ideally hopefully not on LinkedIn,
1704240	1708160	but somehow and message you and be like, I love this. This was such good ideas. Like I want to
1708160	1712560	write a piece in response to you and like we start having a little dialogue that I've had so many
1712560	1718080	relationships blossom this way. But if you have a language model, it's not going to be able to do
1718080	1723040	that. So this is a still from the film, Her, right? This has become kind of a cultural touch
1723040	1729040	point of like parasocial relationships with AI. Hopefully people have seen it but if not,
1729040	1733920	so Joaquin Phoenix, our lovely main character, he has this great relationship with his personal AI,
1733920	1739760	he talks to her in his ear, it falls in love with her. But then the AI of course grows bored of him
1739760	1746000	because he's a very kind of basic human and leaves and he's destroyed. And like some people were
1746000	1749600	supposed to get this like film is supposed to be a warning, right? And some people took it as a
1749600	1758720	suggestion. So there's a company called replica who make AI companions for you that you can
1760000	1764400	make friends with, possibly date and fall in love with. There's a lot of suggestions of sort of
1764400	1770160	lonely young men engaging with this and their marketing copy. And I mean, maybe I do need to
1770160	1776480	explicitly point this out. An AI replica or any other kind of like generative agent person cannot
1776480	1780800	fulfill all our human needs, right? They cannot give you a hug, they cannot come to your birthday
1780800	1787440	party, they cannot kind of engage with you in a meaningful, full human way. And so any kind of
1787440	1791680	language model agent on the internet has no capacity for that back and forth relationship.
1791680	1795840	Even if it faked it, it's very unclear that it would actually satisfy what we need when we have
1795840	1800640	an actual friend that we can go out to coffee with. So that all sounds quite bad again. Like
1800640	1805760	deep breaths, the whole talk is really just digging you in a ditch, I'm sorry. But I'm now
1805760	1811920	going to talk about possible futures. And again, these futures I think are not mutually exclusive,
1811920	1815760	I think they all might unfold in different ways over the next five to 10 years. And like I can't
1815760	1820160	speculate beyond that, God knows where we are. But yeah, I'm hoping they all kind of happen
1820160	1825440	in parallel. So the first is I think we're about to spend a lot of time thinking about how we
1825440	1831600	pass the reverse Turing test. So how do we prove we're human on a web filled with agents? So the
1831600	1836480	original Turing test, you have a human talk to a computer and another human through like a wall
1836480	1841200	so they can't see the typing messages of each other. And then the original test, the computer had
1841200	1846880	to prove that it was the human. It had to prove it was competent. And on the new web, we are now the
1846880	1854000	ones under scrutiny. We have to prove we're real. So we're going to end up like we will assume
1854000	1858480	everyone is an agent until proven otherwise. So I kind of wrote this post where I was
1859200	1863040	thinking about some short term tactics. Like we could use funny terminology. We could all try to
1863040	1867120	become teenagers who like have this insider jargon that the language models don't know about, but
1867120	1870320	they'll pick up on it pretty quick, you know, and then you'll have to abandon it, get a new jargon.
1871120	1874800	We could write in non-dominant languages. If you speak something like Catalan or Welsh,
1874800	1878400	you're probably in a pretty good position, you know, you'll be able to write in a way that's
1878400	1883520	more native than a language model ever could. And ideally, we just do higher quality writing,
1883520	1889120	we do more research, we do more critical thinking. We really reference events and people that could
1889120	1893760	we could only know about from the real world, from being embodied humans in space. I don't know how
1893760	1898000	long those kind of defenses will last, but that's in the short term something we can at least pull
1898000	1903840	on. This next one, I apologize for the phrase, but it too perfectly, it catches the point. I'm
1903840	1910560	not going to explain it. You can Google that later. But the point is that the content from models
1910560	1914880	might end up becoming our source of truth, and that how we know things simply was like,
1914880	1919280	well, a language model once said it, you know, and then it's forever captured in our circular flow
1919280	1922960	of information. So right in this current model, right, the training data is at least based on
1922960	1927760	real world experiences. It's kind of going in a single loop. But we're now going to use that
1927760	1934080	generated text to train new models. And so we enter this loop where like there's this very
1934080	1938960	tenuous link to the real world that was once a long time ago, the source of this data. This is
1938960	1943200	already starting to happen. AI researchers are worried we're kind of going to run out of data to
1943200	1947600	train models on within five years. And so there's a lot of talk of how do we generate information
1947600	1953440	that we can feed the models, feed back into the models. This one was a really funny example of
1953440	1957680	this happening. I don't know if people saw this. So someone noticed that if you ask Google if you
1957680	1962240	can melt an egg, it's like smart AI summary said yes. And they were like, why would it say that?
1962240	1966560	They weren't investigating. And it turned out that fact was pulled from Quora that was generated
1966560	1972320	from chat GPT. And because Quora is considered a reputable website with good SEO standing,
1972320	1976960	it got pulled up into the Google like smart answer. And it's not hard to imagine how all
1976960	1981360	kinds of hallucinated answers are going to become part of this loop if all these major websites are
1981360	1986400	using chat GPT or their own agent to generate answers and then they just feed it to each other.
1986400	1993200	It's like at one point, can you melt an egg? This phenomenon is very worrying for the scientific
1993280	1997520	community and they have good reason to be. We're already seeing a lot of evidence that scientific
1997520	2004720	researchers are using language models to help them write papers. So again, this is a paper
2004720	2009840	that was published in a genuine fairly legitimate journal, Environmental Science and Pollution
2009840	2015280	Research. And it included the phrase regenerate response at the end of a paragraph, which is the
2015280	2021840	button above chat GPT's input box. There's another one in August that was published on
2021920	2028400	fossil fuel allocation that included the phrase as an AI language model. Now, there's a lot of
2028400	2031600	debate in the community where they were like, well, this doesn't mean the science in these papers is
2031600	2035200	totally false, right? They could have done real research on real experiments and they were just
2035200	2039120	trying to get this paper out and at the end they went, you know what, chat GPT summarizes paragraph
2039120	2043840	for me. That totally could have happened. But the problem is we don't know. There's no protocol for
2043840	2048720	the transparency of how you say what you didn't, didn't use chat GPT or whatever kind of model for.
2049120	2054960	So now people are trying to make it more legitimate. Some people are listing chat GPT as an author on
2054960	2060880	papers. And there's a real risk that people with much worse intentions will kind of take this and
2060880	2065440	run with it and just make scientific paper mills. There's a lot of companies that have a lot of
2065440	2070640	vested interests in publishing science that agree with the thing that they would like to be true.
2070640	2074400	Usually you find this out when you get to the funding source section of the paper where you're
2074400	2079360	like, oh, yeah, funded by the people who make this drug. Interesting. But you can tell that if
2079360	2083280	they're going to be able to use generative models to kind of pump out lots of research papers, maybe
2083280	2088640	based on dubious science, it becomes very hard for us to tell what is actually real, what is vetted.
2088640	2094320	I mean, it just takes the whole, the replication crisis to a whole new level. So this one seems
2094320	2098880	the most obvious, right? One possible future is we will just retreat further into the cozy web,
2098880	2104080	right? The dark frost will grow larger and we will just go, okay, I'm only interacting with
2104080	2109120	Discord and WhatsApp, right? LinkedIn is dead, Twitter is dead. We've had to abandon it. We have
2109120	2113760	to maybe make new privatized gate kept spaces, which I think has a lot of downsides, but this
2113760	2118560	just might be the best way to deal with it. I think authors are going to increasingly put
2118560	2122000	content behind blocks and paywalls. I think this is already happening with things like
2122000	2127680	sub-stack and medium, where you're constantly having to log in or prove that you are part of
2127680	2133200	this community to access the content. You understand why authors do this, right? Because
2133200	2137680	actually you're having your content scraped and then fed into generative models puts you in a
2137680	2143440	disadvantage. Like your ideas could be taken out of context, maybe it's taken something that you
2143440	2148240	wanted to actually charge for and given out for free. Someone could train a model on your work
2148240	2152000	and have it start writing in your voice. There's a lot of ways this can go really badly for someone
2152000	2158720	whose full-time job is being a researcher or a content creator. We'll also see more websites
2158800	2164160	that have a large amount of content blocking scraping for language models, or one way to do
2164160	2169360	it is to just charge a huge amount for your API. Reddit did this recently. We're going to put the
2169360	2173360	price so high that no company in their right mind would really pay it, or it would just cost them
2173360	2179360	so much. Twitter kind of did the same. It's hard to tell if this was actually strategic or on purpose,
2179360	2185280	but raising the price to whatever it was, like 42,000 per month, means that very few people
2185280	2191200	can access Twitter's really high-quality content in an age where content to train models is the
2191200	2196240	new goals. It's kind of leading us to a place where the web is not open by default. You can't
2196240	2200880	just query any API for any content you want. Everything is locked down and gatecapped and
2200880	2205920	kind of cordoned off. Next, I think we're going to have what I'm calling the Meet Space Premium.
2207840	2212320	We are in the Meet Space Premium. It's when we begin to prefer and preference offline first
2212320	2218320	interactions. So we will start to doubt all people online. And the only way to confirm someone's
2218320	2222160	humanity is to meet them in person, right, to go for coffee or beer. And once you do that,
2222160	2226240	you can kind of set up a little trust network, right? You can say, oh, I've already met Sarah
2226240	2229920	over there, and she's a real human, and you've already met Tom, and he's a real human, and we can
2229920	2234480	kind of like coordinate our networks to vet who's real on the web. And then when you read their
2234480	2238880	writing, you kind of know it's from an actual person, or you'd hope so. You may be of some
2238880	2241920	trust network of people who aren't writing generated stuff under their own name.
2243040	2246160	I think this has knock-on effects. Like, people might move back to cities or
2246160	2250960	higher population and places. In-person events are going to be preferable. I think there's
2250960	2255040	obvious disadvantages to this, right? The web was this huge democratization thing to enable
2255040	2259120	people who are maybe disabled or have young children or who are caregivers who can't get
2259120	2262960	out of the house for a whole bunch of reasons aren't going to have the same access to the trust
2262960	2266400	network that someone can who could physically show up in space a lot.
2269200	2273040	Yeah. So, a natural follow-on from this also. So, a lot of people have been like,
2273040	2277680	well, why don't we just put it on the blockchain, you know? Why don't we just get a third party
2277680	2282400	to verify our humanity with a cryptographic key, and then you can sign all your published
2282400	2286480	content with it, and it'll link back to your identity, and this is how we'll have decentralized
2286480	2290640	trust networks. And I'm like, okay, I don't know the details of this. This sounds weird.
2290640	2296240	So, there's a project called Worldcoin that, funnily enough, is also funded by OpenAI's
2296320	2300560	leader, Sam Altman. He partially helped found it, which shows he kind of knows the problem he's
2300560	2305920	helped contribute to. So, this scary orb scans your eyeball to confirm your identity,
2305920	2310560	and then it creates a unique human credential for you to use online to sign all your stuff with.
2310560	2314080	It's really not taking off as a project, but it's around. And people are still trying to do this.
2314080	2319040	There's a whole community thinking this is the future that I don't have sophisticated thoughts
2319040	2324000	on yet, and I'm still like, oh, cryptocurrency. But maybe this is some way to get around it.
2324960	2328560	I'm also expecting any day that Elon's going to announce the purple check,
2328560	2332160	where you pay $30 a month, and you don't actually have to. You just take a box that's like,
2332160	2337920	I'm human, and then you get this little check and solve it. So, those are all a bit negative.
2337920	2342400	I do think there is some hope in this future. We can certainly fight fire with fire.
2342400	2346320	So, I think it's reasonable to assume that we're all going to have a set of personal language
2346320	2351280	models to kind of help defend us and serve our needs on the web. They can filter information,
2351280	2356080	they can manage information. And I expect these to be baked into browsers, or maybe even the
2356080	2360080	operating system, right? And they're going to do things like identify generated content,
2360080	2364000	they're going to debunk claims, they're going to flag misinformation, they're going to go
2364000	2369040	help hunt down real scientific sources for you, maybe vet scientific papers, curate and suggest
2369040	2372800	things to you. So, I think this actually could work in both directions. It's not just all like
2372800	2377360	the bad actors get this power. We also get a lot of capacity and capability from these models.
2378320	2382800	We might find it absurd that anyone would browse like the raw web without one of these
2382800	2386160	kind of in tow. It's the same way you wouldn't like go onto the dark web, like you know what's
2386160	2391520	there, but you know you don't want to see it. It might be kind of like that. Okay, so I'm almost
2391520	2395840	done wrapping this up. But the question I want to leave everyone with is which of these possible
2395840	2401200	futures would you like to make happen, right? Generative AI is not necessarily a destructive
2401200	2405040	force, you know, as with all technology, it depends how you wield it. Oh, I went back to the
2405040	2412080	wrong slide, sorry. There we go. The way that we choose to deploy this in the world is really
2412080	2415520	what matters, right? The product decisions we make as individuals and companies, if you are
2415520	2420880	working in the space or you're trying to get into it. Because obviously if you are working on a tool
2420880	2427840	that like turns out tons of human-like content from marketing and influence purposes, like
2427840	2433040	you can stop, like that's really like we don't need that. You can just stop doing that. But what
2433040	2437200	should you be building instead? It's maybe a more helpful question. So I tried to come up with a few
2437200	2441200	principles for building products language models that are probably going to evolve over time, but
2441200	2447440	this is like a first pass. And the first is to protect human agency. The second is to treat models
2447440	2452800	as reasoning engines and not sources of truth. And lastly that we should be augmenting our
2452800	2458960	cognitive abilities and not replacing them. So protecting human agency, this is like usually
2459040	2463120	at the moment you have a human prompter and it hands something off to an autonomous agent
2463120	2467360	and the agent goes and does stuff, right? This is like the open AI assistance model or any of
2467360	2470800	the architectures I showed before. And this is like the path to self-destruction. This is what
2470800	2475120	most AI safety researchers are very afraid of is that the locus of agency sits within the agent.
2476400	2480240	But the ideal form of this is that the locus of agent stays within the human
2480240	2484080	and it has a collaborative agent on hand and there's this very short continuous feedback loop
2484080	2488320	that is constantly going between them. Where the human is the one checking, should I do that? Do
2488320	2493840	I want that? Is that true? Like they're able to actually fact check things and then the agent is
2493840	2500960	much more of a helper. Short feedback loops, close supervision, limited power, it's slower but it's
2500960	2507360	safer. That ties into the second principle that we should treat models as tiny reasoning engines
2507360	2512160	and not sources of truth. So one way to use these models is to like ask it for every answer and
2512160	2517520	ask it every question and trust what it says. Another one is you can train them to do specific
2517520	2523360	things like just summarize this text, just extract data from this paper, just find contradictions in
2523360	2528400	this statement and then you can bring your own data which could be legitimate scientific papers,
2528400	2532480	it could be your own notes, it could be Wikipedia and then you use these models to just do these
2532480	2536880	very small scoped things where you can observe every single output and check that it's actually
2536880	2541360	legitimate and you're not handing off this big complex task to this big black box model.
2542240	2548480	And lastly, we should augment our cognitive abilities and not replace them, right? Language models
2548480	2553280	are very good at things that humans are not good at like searching and discovering things in large
2553280	2557840	datasets, role playing as identities and characters, they're actually really good at doing that,
2558400	2563040	rapidly organizing data, turning fuzzy inputs into structured outputs, there's a lot that they're
2563040	2567040	good at that we're bad at and we should use them for those things. There's tons that like we're
2567040	2570960	good at they're not that we're still trying to like make them do like checking claims against
2570960	2576480	physical reality, long-term memory, having embodied knowledge, understanding social context,
2576480	2581440	having emotional intelligence, I think combining the two of these so that we're doing things models
2581440	2586320	can't do and they're doing things we're not very good at actually leverages the best of both worlds.
2587360	2591280	Because a lot of AI researchers in the moment, they use this metaphor of aliens, this is from
2591280	2597760	the 1970s alien film or frightening, it just it makes me think this is like not the most appealing
2597760	2602960	collaborative partner this metaphor, this like big scary unknown consciousness that like might
2602960	2608880	kill you, but there's another metaphor that I like more that Kate Darling is a robotics researcher
2608880	2613360	at MIT and she wrote this book called The New Breed, arguing we should think about robots as
2613360	2618480	animals, we have a long cultural legal history with animals and working collaboratively with them,
2618480	2624240	oxen, dogs, pigs, right in this very like mutually beneficial relationship most of the time and this
2624240	2628480	is actually a pretty good metaphor to expand to AI where we have to kind of treat them a little bit
2628480	2634240	like some form of intelligent species but one that we are in community with and are part of our
2634240	2639600	systems and are not this like big scary alien who might come kill us all is like usually what it
2639600	2646720	gets talked about as. So yeah there's this big push for this philosophical approach, some people
2646720	2650720	call it cyber-organism, there's a very long article there was a written on less wrong which is not
2650720	2655440	my favorite website but it's a good article that kind of goes in depth into this if you do want
2655440	2661680	to read more about it. So that's all I have, I want to thank you so much for listening, again
2661680	2668640	slide the notes on this QR code if you like missed anything, I'm on Twitter X at mappleton still
2668640	2673920	until that really does fall apart and you can DM me there, you can message me again I love meeting
2673920	2678880	people through writing on the web if you have blogs that like relate to these kind of topics
2678880	2690880	send them to me but yeah thank you so much for listening, I appreciate it.
