WEBVTT

00:00.000 --> 00:07.000
In this episode, I'm joined by Mark Andreessen to discuss accelerationism, AI, technology,

00:07.000 --> 00:10.400
the future, energy and more.

00:10.400 --> 00:14.560
I'd like to say a big thank you to all my paying patrons and subscribers for making

00:14.560 --> 00:16.440
all of this work possible.

00:16.440 --> 00:20.560
And if you'd like to support the podcast as it runs off patronage alone, then please

00:20.560 --> 00:22.520
find links in the description below.

00:22.520 --> 00:24.920
Otherwise, please enjoy.

00:24.920 --> 00:29.280
So, Mark Andreessen, thanks very much for joining us on Hermitix podcast.

00:29.280 --> 00:32.160
Hey James, thanks for having me.

00:32.160 --> 00:41.040
We are going to be discussing accelerationism, AI, technology, the future, technology is

00:41.040 --> 00:43.040
probably the key one here, I think.

00:43.040 --> 00:48.440
But I want to basically begin with probably something that on the usual podcast you go

00:48.440 --> 00:52.000
on, you probably aren't asked, like a lot of people will know who you are, but in the

00:52.000 --> 00:54.800
sphere that I'm working, people might not.

00:54.800 --> 01:00.240
So just tell us a little bit about yourself and what it is you do before we get started

01:00.240 --> 01:01.240
here.

01:01.240 --> 01:02.240
Yeah.

01:02.240 --> 01:04.800
So I'm probably the polar opposite from your usual guest.

01:04.800 --> 01:05.800
Exactly.

01:05.800 --> 01:11.040
So I'm bringing diversity to your production.

01:11.040 --> 01:13.480
So my background, I'm an engineer.

01:13.480 --> 01:19.840
So I'm a computer programmer, computer science, computer engineer by background.

01:19.840 --> 01:23.360
I was training kind of the old school computer science where they kind of teach you every

01:23.360 --> 01:27.720
layer of the system, including hardware and software.

01:27.720 --> 01:32.320
And then I was a programmer and then an entrepreneur in the 90s.

01:32.320 --> 01:36.200
And probably my main kind of claim to fame is I was sort of president of the creation

01:36.200 --> 01:40.280
of what today you'd consider the internet, sort of the modern consumer internet that

01:40.280 --> 01:42.360
people use.

01:42.360 --> 01:46.840
And so my work first at the University of Illinois and then later at a company I co-founded

01:46.840 --> 01:53.040
called Netscape, sort of popularized the idea of ordinary people being online.

01:53.400 --> 01:57.360
And then helped to build what today you experience as the modern web browser and kind of the

01:57.360 --> 02:00.120
modern internet experience.

02:00.120 --> 02:07.200
And then I was involved kind of through a broad range of Silicon Valley waves over the

02:07.200 --> 02:12.040
course of the next 20 years in the 90s and 2000s, including cloud computing where I started

02:12.040 --> 02:15.640
a company in and social networking I started a company in.

02:15.640 --> 02:20.880
And then in 2009, I started with my long-time business partner, I started a venture capital

02:20.880 --> 02:25.960
firm and our firm, which is called Injuries and Horowitz is now kind of one of the firms

02:25.960 --> 02:31.680
at the center of funding, all of the new generations of technology startups.

02:31.680 --> 02:37.640
And maybe the main thing I kind of underlined there is just technology, quote unquote technology,

02:37.640 --> 02:43.160
high tech, computer technology in particular, kind of used to be, it's always been kind

02:43.160 --> 02:47.400
of interesting and important in the economy for the last 50 years or something.

02:47.400 --> 02:52.520
In the last 15 years, I think a lot of people kind of feel that like technology has really

02:52.520 --> 02:58.120
spread out and it has become integral to many more aspects of life.

02:58.120 --> 03:02.640
And so my firm today finds itself very involved in the application of technology to everything

03:02.640 --> 03:11.240
from education, housing, energy, national defense, national security, as well as kind

03:11.240 --> 03:16.840
of every possible artificial intelligence robotics, kind of every different dimension

03:17.240 --> 03:19.080
of how you might touch technology in your life.

03:21.000 --> 03:25.880
And you picked up on something that will come into the conversation in a couple of questions

03:25.880 --> 03:30.920
time, but this notion of you is basically completely opposite to the majority of guests,

03:30.920 --> 03:36.880
not in a bad way, but often it's a lot of philosophy and which theory and not practice.

03:36.880 --> 03:41.440
And also this notion of technology in relation to either pessimism or optimism.

03:41.440 --> 03:46.880
And this is super, super key, I think, for the ongoing atmosphere of really the West,

03:46.880 --> 03:48.200
of where we're going to end up.

03:48.200 --> 03:52.720
But before we get to these questions, I mean, this is a question I'm slowly phasing out,

03:52.920 --> 03:56.320
but I think it will work for the sake of our conversation, because we're talking more

03:56.320 --> 03:57.560
broadly around themes.

03:58.720 --> 04:01.960
I know you've listened to the podcast before, so it is the Hermitix question.

04:02.240 --> 04:06.120
You can place three thinkers living or dead into a room and listen in on the conversation.

04:06.120 --> 04:06.760
Who do you pick?

04:07.760 --> 04:13.520
Yeah, I think that maybe I'll give you two versions of the answer and then maybe I can combine them.

04:13.520 --> 04:15.640
So there's kind of a timeless answer.

04:15.640 --> 04:22.200
And the timeless answer would be something like Plato or Socrates, Socrates and then Nietzsche.

04:22.200 --> 04:30.160
And then maybe I'd throw in one of your favorite people, Nick Land, I think would be interesting.

04:30.160 --> 04:36.160
The somewhat more applied version of that would be something a lot, and this is sort of maybe

04:36.240 --> 04:39.040
a little bit more topical these days with this movie Oppenheimer that just came out.

04:39.040 --> 04:45.960
But it's like John von Neumann, who was one of the co-inventors of both the atomic bomb

04:45.960 --> 04:50.800
and the computer, Alan Turing, who became famous a few years ago with another movie,

04:52.080 --> 04:57.080
The Imitation Game, and then let's throw in Oppenheimer there also, because those three

04:57.080 --> 05:00.560
guys were sort of present at the creation of what we would consider to be the modern

05:00.560 --> 05:06.120
technological world, including literally those guys were at the center, especially

05:06.160 --> 05:10.160
von Neumann and Turing were at the center of both World War II, the atomic bomb,

05:11.440 --> 05:16.640
the sort of information warfare, the whole kind of decryption kind of phenomenon,

05:16.640 --> 05:21.600
which really a lot of people think one, World War II, along ultimately with the A-bomb,

05:21.600 --> 05:26.720
and then also right precisely at that time with those people, the birth of the computer

05:26.720 --> 05:30.800
and everything that followed. So is that more of a practical room for you or

05:31.680 --> 05:36.080
in terms of like a vision going forward into the future? Or is there something else going

05:36.160 --> 05:44.240
on there between those sort of six figures? Those guys were very, it's almost impossible to

05:45.200 --> 05:51.680
overstate how smart and visionary and far seeing they were like, there's actually the

05:51.680 --> 05:55.440
von Neumann biography came out recently called The Man from the Future, and in anything like

05:55.440 --> 05:58.400
von Neumann is a more interesting character than Oppenheimer in a lot of ways, because he

05:59.040 --> 06:03.680
touched a lot more of these fields. And of the people who knew them that von Neumann was always

06:03.680 --> 06:06.560
considered that he was the smartest of what were called the Martians at that time, right,

06:06.560 --> 06:11.520
which were the sort of group of super geniuses that originated in Hungary in that era.

06:12.560 --> 06:18.720
And so, you know, they were very, very conceptual thinkers. I'll just give you one

06:18.720 --> 06:23.040
example of how conceptual they were, how profoundly smart they were. So they basically

06:23.040 --> 06:26.720
birthed the idea of artificial intelligence right in the middle of the heat of World War II.

06:26.720 --> 06:30.480
Like the minute they created the computer, like they created the computer, right? They created

06:30.480 --> 06:33.600
like the electronic computer, as we know it today, in the heat of World War II. And then

06:33.600 --> 06:37.040
they immediately said, aha, this means we can build electronic brains. And then they immediately

06:37.040 --> 06:41.840
began theorizing and developing designs for artificial intelligence. And in fact, the core

06:41.840 --> 06:45.760
algorithm of artificial intelligence is this idea of neural networks, right, which is this idea of

06:45.760 --> 06:50.080
a computer architecture that sort of is mirrors in some ways the sort of mechanical operation of

06:50.080 --> 06:55.200
the human brain. You know, that was literally an idea from that era in the early 1940s. There was a

06:55.200 --> 07:01.600
paper, two other guys who were in this world wrote a paper in 1943, outlining the theory of

07:01.600 --> 07:06.960
neural networks. And that literally is the same technology. That is the core idea behind

07:06.960 --> 07:13.200
like what you see when you use JGPT today, 80 years later. And so there was a very, very deep

07:13.200 --> 07:18.480
level of intellectual and philosophical, you know, I don't know what it is, like they tapped

07:18.480 --> 07:22.000
into or discovered or developed a very deep well that we're still drawing out of today.

07:22.960 --> 07:26.480
I was gonna, yeah, I was gonna ask that immediately, but you covered it. I mean,

07:26.480 --> 07:31.760
is there any significant changes between AI then and AI now? Or is it really just a matter of

07:31.760 --> 07:37.040
practicality? Like we've got the, we've got more resources and more ability to create it.

07:38.000 --> 07:41.600
Yeah, we're at this fairly shocking moment. So for people who haven't been following this,

07:41.600 --> 07:44.960
basically, it's this, it's one of these amazing things where it's like, there's like this 80

07:44.960 --> 07:49.360
year overnight success that all of a sudden is paying off. And so that it's, you know, it's,

07:49.440 --> 07:53.440
there were, you know, there were 80 years of scholars and researchers and projects

07:53.440 --> 07:57.120
and attempts to build electronic brains. And like every step of the way people thought that

07:57.120 --> 08:01.600
they were super close, you know, there was this famous seminar on the campus of I think it was

08:01.600 --> 08:05.680
Dartmouth University in like 1956, where they got this grant to spend 10 weeks together, they'd

08:05.680 --> 08:08.800
get all the AI scientists together in 1956, because they thought that after that they'd have,

08:08.800 --> 08:14.880
they'd have AI, you know, and turn out they didn't. And so, so it's what, but like it's

08:14.880 --> 08:19.520
starting to work, right? And so when you use ChatGPT today, or you use on the artistic side,

08:19.520 --> 08:23.520
you simply admit journey or stable diffusion, like you're seeing the payoff from that.

08:25.040 --> 08:28.720
I think the way to think about it is it's the deep thinking that took place up front.

08:29.840 --> 08:34.320
It's, and then, you know, just obviously tremendous amount of scientific and technological

08:34.320 --> 08:37.360
thinking and development, you know, and elaboration that took place since then.

08:38.160 --> 08:42.240
But then there's two other kind of key things that are making AI work today that are kind of,

08:42.240 --> 08:45.120
and there's sort of, again, there's sort of a combination of sort of incremental, but also

08:45.120 --> 08:52.240
step function breakthroughs along the way. So one is data. And so just like it turns out a big part

08:52.240 --> 08:57.680
of getting a neural network to work is feeding in enough data. And so, you know, and the analogy

08:57.680 --> 09:01.280
is irresistible, right? It's like, you know, if you want to, if you're trying to educate a student,

09:01.280 --> 09:06.080
right, you want to feed them, you know, and feed them a lot of material in the human world also.

09:06.080 --> 09:10.240
And so it just turns out there's this thing with neural networks and data where, as they say,

09:10.400 --> 09:15.520
you know, quantity has a quality all its own. And you really needed actually the internet to

09:15.520 --> 09:19.120
get to the scale of data. You needed internet scale data, you know, you needed the web to

09:19.120 --> 09:22.720
generate enough text data, you needed like, you know, Google images and YouTube to generate enough

09:22.720 --> 09:28.720
video and imagery to be able to train. So we're kind of getting a payoff from the internet itself,

09:28.720 --> 09:32.640
you know, combined with neural networks. And then the third is the advances in semiconductors.

09:33.920 --> 09:36.960
And, you know, and this is, you know, sort of the famous Moore's Law. But, you know,

09:37.840 --> 09:41.840
this phenomenon that, you know, that kind of we refer to as, you know, quote unquote teaching

09:41.840 --> 09:47.440
sand to think. And so kind of this idea, right, that you can literally convert, you know,

09:48.560 --> 09:54.240
silicon, you know, sand, rocks into, you know, into chips, and then ultimately into brains

09:55.040 --> 09:59.120
is kind of this amazing thing. And actually, as I don't know if you follow this stuff, but as

09:59.120 --> 10:03.600
we're recording right now, there's this like amazing phenomenon happening in the world of

10:03.600 --> 10:09.520
semiconductors and physics right now, which is there's this, we may be, we may be, we may be

10:09.520 --> 10:12.560
right now, we may have just discovered the first room temperature superconductor.

10:13.120 --> 10:18.160
I've been seeing this, but I'm not smart enough. Can you give me a brief overview of why this is

10:18.160 --> 10:21.520
so important? I mean, I'm guessing is this a resource input issue?

10:22.320 --> 10:26.160
So basically, every time you build a circuit today, right, every time you build any kind of

10:26.160 --> 10:31.760
circuit, a wire, a chip, you know, anything like that, an engine, a motor, you know, you have

10:31.760 --> 10:36.080
basically this process. And by the way, this actually relates to the philosophy of acceleration,

10:36.080 --> 10:40.560
as we'll talk about, but you have this sort of thermodynamic process where you're taking in

10:40.560 --> 10:45.360
energy on the one side, right, and then you have a system, right, like a, you know, an electrical

10:45.360 --> 10:49.680
transmission line or a computer chip or something, you have a system that's basically using that

10:49.680 --> 10:55.840
energy to accomplish something. And then that system is inefficient and that system is dumping heat

10:55.840 --> 11:00.880
out the other end. And, you know, and this is why when you use your computer, you know, if you

11:00.880 --> 11:03.760
got, you know, an older laptop computer, you know, the fan turns on at a certain point,

11:04.720 --> 11:08.080
if you have a newer laptop computer, it just starts to get hot, you know, you probably notice

11:08.080 --> 11:12.000
your phone starts to get hot, you know, let, you know, batteries every once in a while do what

11:12.000 --> 11:15.760
they call the cook off, they, you know, they lithium ion batteries will explode, right, like

11:15.760 --> 11:20.000
they're, you're dumping, there's always some, there's, there's always a byproduct of heat,

11:20.000 --> 11:23.440
and therefore, you know, sort of increased entropy kind of coming out the other side of any sort of

11:23.440 --> 11:28.000
electrical or mechanical system. And that's just because with, you know, kind of running energy

11:28.000 --> 11:33.440
through wires of any kind, you just have a level of inefficiency. By the way, the human body does

11:33.440 --> 11:37.120
the same thing, right, like, you know, we take in, you know, energy, and then we, you know, we're

11:37.120 --> 11:39.840
sitting here, you know, we don't feel it, but we're sitting here humming along at, you know,

11:39.840 --> 11:44.240
whatever 98.6 degrees Fahrenheit, you know, significantly higher than room temperature,

11:44.240 --> 11:48.560
because, you know, we're generating our actual biochemical process of life, right,

11:48.560 --> 11:53.360
bioelectrical is generating heat and dumping it out. Anyway, so the idea of the superconductor is

11:53.360 --> 11:57.680
basically think about it in the abstract as a wire that basically transmits information without,

11:57.680 --> 12:01.040
you know, with basically perfect fidelity, you know, perfect conservation of energy

12:01.040 --> 12:06.160
without dumping any heat into the environment. And it turns out that if you could do that,

12:06.160 --> 12:09.840
if you do that at room temperature, then all of a sudden you can have like, you know, basically,

12:09.840 --> 12:14.720
like, you know, incredibly more efficient, you know, kinds of batteries, electrical transmission,

12:14.720 --> 12:19.680
motors, you know, computer chips. And so you can start to think about, for example,

12:20.400 --> 12:24.800
just, you know, an example people talk about is if you, if you, if you cover the Sahara Desert

12:24.800 --> 12:28.960
and solar panels, you know, you could power, you know, basically the entire planet's, you know,

12:28.960 --> 12:32.160
power, you know, energy needs today. The problem is there's no way to transmit that,

12:33.360 --> 12:36.000
you know, transfer that power from the Sahara to the rest of the world

12:36.640 --> 12:40.160
with existing transmission line technology with superconducting transmission lines,

12:40.160 --> 12:45.440
all of a sudden you could, you know, quantum computers, you know, today they exist,

12:45.440 --> 12:49.280
but they're sharply limited because they have to be operated at these, you know, super cool

12:49.280 --> 12:53.520
temperatures, you know, in these very carefully constructed labs, you know, with superconductors

12:53.520 --> 12:58.720
in theory, you have desktop quantum computers, you know, you have levitating trains, you've got,

12:58.720 --> 13:03.840
you know, you just, you have a very broad cross section, you know, you have handheld MRIs,

13:04.560 --> 13:07.440
right, like every doctor, every nurse, you know, has an MRI and they can just, you know,

13:07.440 --> 13:12.560
take a scan wherever they need to, you know, on the fly, you know, and like, like, like the Star Trek,

13:12.560 --> 13:18.240
you know, the, the tricorder, you know, kind of thing. And so anyway, it's fascinating. So,

13:18.240 --> 13:22.080
so, so sitting here today, there's, there's, there's the reports of this, of this breakthrough.

13:22.080 --> 13:27.600
And there are the sort of almost, these almost UFO style videos of, of, of this material levitating,

13:27.600 --> 13:31.680
where it's not supposed to be levitating as a consequence of this breakthrough. And there are

13:31.680 --> 13:35.840
betting markets on scientific progress, and the betting markets, as of this morning, have the odds

13:35.840 --> 13:39.840
of this being a real breakthrough at exactly 50-50. And so, we...

13:39.840 --> 13:40.880
Not the worst odds.

13:42.080 --> 13:45.360
No, but it's, it's funny. If you think about it, it's funny because it's, it's, it's the,

13:45.360 --> 13:48.480
our entire world right now, from a physics standpoint, it's like Schrodinger's cat,

13:48.480 --> 13:52.800
like we live in a, we live, we live sitting here today in a superposition of two worlds,

13:52.800 --> 13:55.600
one in which we now have room temperatures, some conductors, and one of which we don't.

13:57.440 --> 14:01.040
People are, you know, these are radically different potential futures for, for humanity,

14:01.040 --> 14:05.200
right? And so, if it turns out it's true, you know, it's an amazing stuff, function,

14:05.200 --> 14:08.000
breakthrough. If not, it'll, you know, it'll, it'll set us back and we'll, you know, people

14:08.000 --> 14:12.240
will go back to trying to work on it, figure it out. But, you know, but, but between the time

14:12.240 --> 14:16.400
we're recording, between the time we release, we may even find out whether the cat, the, the

14:16.400 --> 14:21.360
superconducting cat, the box is alive or dead. That alive or dead state, I mean, these, these

14:21.360 --> 14:26.160
two separate futures is really something that I, I see, you know, when I was reading your blog,

14:26.160 --> 14:31.520
when I was looking at, uh, effective, effective acceler, accelerationism and accelerationism

14:31.520 --> 14:36.560
we'll get to, but these two futures, I think is the big question that I want to ask you, which is,

14:36.560 --> 14:41.760
because, because you've lived through this time, which is going through the, the optimism of the

14:41.760 --> 14:45.520
90s, especially, you know, you mentioned Nick Lander, the star, I mean, you see that in philosophy,

14:45.600 --> 14:51.280
you see that in technology, see that in the history, this huge, um, so let's call it a cyberpunk

14:51.280 --> 14:56.240
optimism regarding our technological future. And I would say now, I don't know, you know,

14:56.240 --> 15:01.440
whether or not you agree with me, please let me know. We have entered into what land himself

15:01.440 --> 15:10.640
called a slump from the 2000s, like late 2000s, you know, early 2000s onwards. And there seems to be

15:10.640 --> 15:16.320
within the, within the air, a sort of cynicism, a sort of pessimism that we've just ended up in this,

15:16.320 --> 15:22.080
like, place of stagnance. And do you see, I mean, if you agree with me in terms of those two,

15:22.080 --> 15:26.480
two possibilities, do you, I mean, I think I would be right in saying you're an optimist.

15:26.480 --> 15:33.680
Do you see us now re-entering into that, a new phase of optimism regarding technology and regarding

15:33.680 --> 15:38.400
the future? Well, so there's, there's, there's several layers to this question. I would be happy

15:38.480 --> 15:42.880
to kind of go through them. Then we can spend as much time in this as you want. But the, the, the,

15:42.880 --> 15:46.160
the core layer we're talking about, and I totally, by the way, totally acknowledge and, and I think

15:46.160 --> 15:51.920
this is a great topic. And, you know, the, your observations are very real. The core thing that

15:51.920 --> 15:55.760
I would go to, to start with is not kind of the social, political, you know, kind of, you know,

15:55.760 --> 15:59.280
philosophical dimension. The core thing I would go to, to start with is the technological dimension.

16:00.800 --> 16:04.960
In other words, at the substantive level, like, what is the actual rate of technological change

16:04.960 --> 16:10.240
in our world? And, and you'll know, you'll note, I don't know, you'll note that on the, on the social

16:10.240 --> 16:13.920
dimension, we seem to whip back and forth between, oh my God, there's too much change,

16:13.920 --> 16:17.920
and is he stabilizing everything? And then we whip right around to, oh my God, there's not enough

16:17.920 --> 16:22.720
change. And we're stagnant, right? And that's horrible. So, so there's kind of dystopian versions,

16:22.720 --> 16:26.320
you know, there's kind of dystopian mindsets in the air, kind of in, in, in both directions.

16:27.920 --> 16:32.000
So, so anyway, so I would start with kind of the technological kind of substantive layer to it.

16:32.880 --> 16:36.320
And there, you know, the observation, and this is not an original observation on my part,

16:36.320 --> 16:39.760
you know, Peter Thiel and Tyler Cohen, in particular, have gone through this in a lot

16:39.760 --> 16:45.360
of detail in their work. But, you know, basically, like, if you look at the long arc of technological

16:45.360 --> 16:49.200
development over the course of, you know, basic, you know, which, which effectively started with

16:49.200 --> 16:52.560
the Enlightenment, right? So you sort of, practically speaking, you're sort of starting

16:52.560 --> 16:57.840
around 1700 and projecting forward to today. It's about 300 years worth of what we would

16:57.840 --> 17:02.880
consider kind of systematic technological development. You know, it's basically, if you

17:02.880 --> 17:07.280
look at kind of that long arc, and then if you basically measure the pace of technological

17:07.280 --> 17:12.080
development and applause by saying you actually can measure the pace of technological development

17:12.080 --> 17:18.080
in the economy with a metric that economists call productivity growth. And so, and basically,

17:18.080 --> 17:22.240
the way that that works is, you know, economic productivity is defined basically as output

17:22.240 --> 17:26.480
per unit of input, right? And you can, you know, whatever your inputs are, could be energy, right?

17:26.480 --> 17:31.520
It could be, you know, raw materials, you know, whatever you want. And then, you know, output is

17:31.520 --> 17:35.280
in, you know, actual, you know, actual output, you know, more cars, more chips, more this,

17:35.280 --> 17:40.400
more that, more clothes, more food, more houses. And so, basically, what economists will tell you

17:40.400 --> 17:44.720
is the rate of productivity growth in the economy, which they measure annually, basically, is the

17:44.720 --> 17:49.920
rate of technological change in the system, right? And so, if technology is paying off, right, if the

17:49.920 --> 17:54.640
advances are real, then your economy is able to generate more output with the same inputs.

17:55.200 --> 17:59.680
If your technological development is stagnant, then that's not the case. And it's an aggregate

17:59.680 --> 18:04.640
measure, but it's a good measure overall. If you look at those statistics, basically, what you find

18:04.640 --> 18:09.600
is we had very, we think more recently in the last century, we had very rapid productivity growth in

18:09.600 --> 18:15.840
the West, basically, for the first half of the 20th century. So, from the basically, you know,

18:15.840 --> 18:21.680
what was called the Second Industrial Revolution, which started around 1880, 1890, through to basically

18:21.680 --> 18:26.400
the mid-60s, we had actually a very rapid rate of technological development. And by the way,

18:26.400 --> 18:31.840
in that era, right, we got, you know, the car, the interstate highway system, the power grid,

18:31.840 --> 18:37.360
telegraph, telephone, radio, television, you know, we got computers, we got, you know,

18:37.360 --> 18:41.760
we got like all, you know, all we got, you know, atomic, we got, you know, both atomic weapons

18:41.760 --> 18:46.560
and also nuclear power technology, right? And so, there was this tremendous kind of technological

18:46.560 --> 18:51.920
surge that took place, you know, in that sort of scholar 1880 to 1960, 1965 kind of period.

18:51.920 --> 18:55.280
The productivity growth ran, you know, through that era, two to 4% a year,

18:56.320 --> 19:00.000
which, which, and the aggregate is very fast, you know, for the economy overall, like that's,

19:00.000 --> 19:06.960
that's a very fast pace of change. Basically, since the mid-60s, early 70s, the rate of productivity

19:06.960 --> 19:12.560
growth basically took a sharp deceleration. And so, in the, in the, basically, the 50 years, 52

19:12.560 --> 19:17.680
years now that I've been alive, you know, it's, it's been a step lower, it's been 1 or 2% a year,

19:17.680 --> 19:22.160
it's been kind of persistently too low relative to what it should be. And, and, you know, I think

19:22.160 --> 19:26.720
there's a bunch of possible explanations for that. But I think the most obvious one is that

19:27.600 --> 19:32.960
basically the, the world of technology bifurcated in the 70s and 80s into two domains, one domain is

19:32.960 --> 19:36.720
the domain of bits, you know, the domain of computers and the internet, where there has been,

19:36.720 --> 19:39.920
you know, obviously very rapid technological development, you know, you know, potentially,

19:39.920 --> 19:44.800
you know, now culminating in AI. But then there's also the world of atoms. And, you know, the,

19:44.800 --> 19:49.200
the diagnosis at least that I would apply is we, we, we essentially outlawed technological

19:49.200 --> 19:53.520
development and innovation in the, in the realm of atoms, you know, basically since the 1970s.

19:54.480 --> 19:57.760
There are many examples of how we've done this. And, you know, you can look at things like housing

19:57.760 --> 20:01.520
policy, and you can kind of see it quite clearly, but also very specifically, you can see it in

20:01.520 --> 20:07.280
energy, which is, you know, we discovered nuclear power, right? We discovered a source of, you know,

20:07.280 --> 20:11.840
a limited, you know, zero emissions energy that, you know, compared to every other form of energy

20:11.840 --> 20:16.320
is like ultra safe, you know, nuclear energy is like, by far the safest form of energy that we

20:16.320 --> 20:21.760
know of. And, you know, in the 1970s, we essentially made it illegal, you know, just like totally

20:21.760 --> 20:26.640
banned it. And we talked more about that, but like that, that was like a draconian thing that,

20:26.640 --> 20:31.600
that, you know, has consequences through to, to the world we live in today. And so, so we live in

20:31.600 --> 20:35.600
this, or any, you mentioned cyberpunk, and this is, this is actually kind of the cyberpunk ethos

20:35.600 --> 20:39.200
that I think actually reflects something real, which is, you know, if you're in the virtual world,

20:39.200 --> 20:43.520
it's like, wow, right? It's like, you know, it's amazing. Like everything is like spectacular.

20:43.520 --> 20:46.800
And, and yeah, look, even like a podcast like yours, like, right, would have been, you know,

20:46.800 --> 20:52.320
inconceivable 30 years ago, right? And so like information, transmission, communication, coordination,

20:52.960 --> 20:57.440
you know, all these things are have taken huge leaps forward. But then the minute we, you know,

20:57.440 --> 21:01.360
the minute you get into a car, or the minute you plug something into the wall, right, or the minute

21:01.360 --> 21:06.880
you eat food, right, you're still living in the 1950s. And so I think we live in a schizophrenic

21:06.880 --> 21:13.200
world with respect to that question. Why then, so you write about this in your blog post on AI,

21:13.200 --> 21:19.120
which we'll get to, but you draw in Prometheus, right, this, this consistent historical cycle of

21:19.120 --> 21:22.160
when there is a new technology, it's going to destroy us, everything's going to end,

21:22.160 --> 21:26.480
it's the worst thing ever, we need to be careful of it, you know, the TV is going to burn your

21:26.480 --> 21:31.120
eyeballs out of your sockets, the vacuum cleaner is going to, I don't know, like explode or whatever,

21:31.120 --> 21:37.200
but every time there is a, like a cyclic change of a new technological innovation, it's this

21:37.200 --> 21:40.960
Promethean thing of where we're pretty terrified of it and we want it to go away. And then eventually

21:40.960 --> 21:44.960
we're like, Oh, actually, no, that's pretty helpful. But there seems to be, as you said,

21:44.960 --> 21:49.600
there's something that happened in the 1970s, where we just pushed away the atomic world in

21:49.600 --> 21:54.480
favor of the bits, you know, which makes sense. But why, I mean, there's probably a lot of

21:54.480 --> 22:00.000
governmental reasons for this as well. But why were we so, it seems like a fear, really,

22:00.000 --> 22:07.200
the way you talk about it, like why were we so in a way scared to then develop the atomic world

22:07.200 --> 22:13.120
in the way we had the bit world? Yeah, so I go start even deeper, I think, which is there's

22:13.120 --> 22:18.720
a deep fear in the human psyche, and I think probably in the human animal of new knowledge,

22:18.720 --> 22:23.280
like it's even a level like technology is an expression of knowledge, right, like the Greeks

22:23.280 --> 22:27.280
right have this term, Techni, which is sort of this, you know, which is where the word technology

22:27.280 --> 22:30.000
comes from. But I think the underlying meaning is more like general knowledge.

22:31.120 --> 22:35.440
You know, the Christian, you know, the key to the Christian, you know, kind of theology,

22:35.440 --> 22:39.600
right, is the, you know, what is, you know, what was the original sin, right, it was eating the

22:39.600 --> 22:44.480
apple from the Tree of Knowledge, right, it was, it was mankind, right, mankind learning that,

22:44.480 --> 22:49.040
which he was not supposed to learn. And so, you know, the Greeks had the Prometheus myth,

22:49.040 --> 22:54.000
the Christians have the snake in the Garden of Eden and the Tree of Knowledge, like there's

22:54.000 --> 22:58.800
something very, very deep, like there's, there's an asymmetry, I think, wire deeply in the human

22:58.800 --> 23:04.480
brain, right, which is, you know, sort of, you know, fear versus hope, which, which from an

23:04.480 --> 23:07.600
evolutionary standpoint, like would make a lot of sense, right, which is like, okay, if you're

23:07.600 --> 23:11.680
living in, let's say prehistoric times, you know, in the sort of long evolutionary landscape that

23:11.680 --> 23:18.000
we lived in, you know, is new information likely to be good or bad, probably over the sweep of,

23:18.000 --> 23:21.360
you know, the billions of years of evolution that we went through, most new information was bad,

23:21.360 --> 23:26.000
right, most new information was the predators coming over the hill to kill you. And so, I think

23:26.000 --> 23:31.680
there's something like deeply resonant about the idea that new is bad, that, you know, and by the

23:31.680 --> 23:36.000
way, look, in the, in the West, like, we probably, you know, we actually, I think, from a historical

23:36.000 --> 23:39.600
and maybe comparative standpoint, like we're actually quite enamored by new things as compared

23:39.600 --> 23:43.680
to a lot of traditional societies. And so, if anything, we've overcome some of our national

23:43.680 --> 23:48.480
instincts on this, but that, that, that impulse is still deep. And then if you go up one level to

23:48.480 --> 23:55.760
kind of the social level, you know, I'm quite bought into an explanation on this that was provided,

23:55.760 --> 24:00.640
there's a, there was a philosopher of science, historian of science named Elting Morrison

24:00.640 --> 24:05.600
at MIT in the, in the first half of the 20th century, who talked about this. And he said,

24:05.600 --> 24:08.720
look, you need to think about basically technology intersects with social systems.

24:09.680 --> 24:13.600
When a new technology intersects with a social system, basically what it does is it threatens

24:13.600 --> 24:19.360
to upend the social order, right. And so, at any given moment in time, you have a social order,

24:19.360 --> 24:24.400
right, with status hierarchies, right, and people who are in charge of things. And basically what

24:24.400 --> 24:28.560
he says is the social order of any time is basically, you know, in sort of Western sort of

24:28.560 --> 24:32.400
modern sort of enlightenment, Western civilization, the social order is a function of the technologies

24:32.400 --> 24:36.320
that led up to it, right. And so you have a certain way of organizing the military, you have a certain

24:36.320 --> 24:39.520
way of organizing, you know, industrial society, you have a certain way of organizing, you know,

24:39.520 --> 24:44.880
political affairs. And they are the consequence of the technologies up to that point. And then

24:44.880 --> 24:49.200
you introduce a new technology, and the new technology basically threatens to upend that

24:49.200 --> 24:53.440
status hierarchy. And the people who are in power all of a sudden aren't, and there are new people

24:53.440 --> 24:57.280
in power. And of course, you know, what is the thing that people will fight the hardest to maintain,

24:57.280 --> 25:01.520
you know, as, you know, as their status in the hierarchy. And then he goes through example

25:01.520 --> 25:05.840
after example of this throughout history, including this incredible example of the development of the

25:05.840 --> 25:11.760
first naval gun that adjusted for the role of a battleship and battle, which increased the firing

25:11.760 --> 25:18.000
accuracy of naval guns by like 10x. It was one of the great decisive breakthroughs in modern weaponry.

25:19.120 --> 25:23.760
And it still took both the US and the UK British navies 25 years to adopt it.

25:25.040 --> 25:31.280
Because the entire command status hierarchy of how naval combat vessels were run and how

25:31.280 --> 25:35.600
gunnery systems worked and how tactics and strategy worked for naval battles, like had to be upended

25:35.600 --> 25:40.240
with the invention of this new gun. Anyway, and so like he would basically say, you know,

25:40.240 --> 25:45.120
essentially, duh, you know, you roll out this new technology, it, you know, it causes people who

25:45.120 --> 25:49.680
used to have power and no longer have power, puts new people in power, you know, in modern terms,

25:49.680 --> 25:53.280
you know, the language that we would use to describe this as gatekeepers, right? Like so,

25:53.280 --> 25:58.640
you know, why is the traditional journalism press so, you know, it just absolutely furious about

25:58.640 --> 26:03.040
the internet, right? And it's because like the internet gives right regular people the opportunity

26:03.040 --> 26:06.640
to basically be on a, on at least a peer relationship, if not, you know, in the case of

26:06.640 --> 26:11.760
somebody like Joe Rogan, a superior relationship, right? And then it's an upending of the status

26:11.760 --> 26:16.640
hierarchy. And kind of, you know, the same thing, you know, through, basically, like one of the

26:16.640 --> 26:20.320
ways to interpret the story of our time from a social standpoint is all of the gatekeepers who

26:20.320 --> 26:25.360
were strong in the 60s and 70s are basically being torn down. Another obvious example, political

26:25.360 --> 26:30.640
parties, right? Why are so many Western political parties in a state of some combination of freak

26:30.640 --> 26:34.960
out and meltdown right now, right? Well, it's because in an era of radio and television,

26:34.960 --> 26:38.320
they were able to broadcast a top down message, and they were able to tell voters basically

26:38.320 --> 26:42.400
what to think in the, in the new model voters are deciding what they think based on what they

26:42.400 --> 26:46.080
read online. And then they're reflecting that back up and finding their politicians wanting,

26:46.080 --> 26:50.480
right? And so therefore, like the re-rise of populism and, you know, sort of the blowing out of,

26:50.480 --> 26:54.240
you know, sort of both left-wing and right-wing ideologies, right? The sort of, you know,

26:54.240 --> 26:58.160
the center is not holding. And so anyway, that would be another example in Morris's framework.

26:59.120 --> 27:03.280
And then I'll just close on this. Morrison has this fast. He says there's this as a consequence

27:03.280 --> 27:07.840
to the fact that technology changes social hierarchies. He says there's a predictable

27:07.840 --> 27:12.560
three stage process to the reaction to any new technology by the status quo, but basically

27:12.560 --> 27:18.560
the people in power at that time. He says, step one is ignore. And so just like pretend it doesn't

27:18.560 --> 27:22.880
exist. Which by the way, is actually a pretty good strategy because like most technologies don't

27:22.880 --> 27:26.080
upend social orders, like most new technologies don't work at the time that they're first

27:26.080 --> 27:31.120
presented. So maybe ignore is actually a rational strategy. Step two is what he calls rational

27:31.120 --> 27:35.200
counter argument. And so that's where you get like the laundry list of all the things that are

27:35.200 --> 27:39.520
wrong with the new technology, right? And then he says step three is when the name calling begins.

27:41.840 --> 27:46.240
This, I mean, I watched a couple of your other interviews recently. And this relates to,

27:46.240 --> 27:51.280
I know you've been talking about Nietzsche's master in slavery morality recently. And this

27:51.360 --> 27:55.680
seems to tie to that in this notion of Nietzsche and, you know, he does a typical

27:55.680 --> 28:00.160
philosophical thing of taking a French word and drawing it out. But Rosentum on, right? Instead

28:00.160 --> 28:06.800
of, you know, just having a look at nuclear power and seeing where it would go and allowing that

28:06.800 --> 28:13.680
power to unfold within society, you try invert the morals. So you say, well, actually, the good thing

28:13.680 --> 28:18.400
to do is because these people don't have the will to power, because they don't have the ability

28:18.400 --> 28:24.000
or the engineering skills, I guess in your own case, to like, you know, to utilize the thing,

28:24.000 --> 28:29.120
they invert the morals and say, well, actually, the good thing is to do the inverse is to not have

28:29.120 --> 28:34.800
it like this is bad. And now that then immediately puts them in the in the good camp. But it seems

28:34.800 --> 28:40.640
like, to be honest, it really feels especially with AI and also now with nuclear power, now that,

28:40.640 --> 28:44.480
you know, especially in Germany, certain things have been tried. And now it's like, okay, this was

28:44.480 --> 28:49.760
a really bad mistake in terms of energy, like the cat's out of the bag. And we like, there's now

28:49.760 --> 28:54.800
this force of having to move, you were then talking about the second to second and third stages there.

28:54.800 --> 28:59.680
It's almost like, look, with AI, especially the cat's out of the bag, like, we have to move, there's

28:59.680 --> 29:04.400
no, there's no choice of like, ignoring or reacting against it. Now you have to deal with it or you

29:04.400 --> 29:08.240
don't. Yeah, so let's let's spend a little one more moment on nuclear power and then and then go

29:08.240 --> 29:12.720
to AI. So nuclear power is so interesting, because nuclear power is the tell. Like, I always look for

29:12.720 --> 29:15.360
like the little signals that people don't really mean what they say, or that they don't, they're

29:15.360 --> 29:19.040
not really like they're, you know, they're, they're, they're, they're sort of, you know, moral system

29:19.040 --> 29:23.280
doesn't quite line up properly. And so nuclear power is this like amazing, it's this amazing thing,

29:23.280 --> 29:26.640
it's like, literally, it's like, okay, you build this thing, it generates power, it basically,

29:26.640 --> 29:31.360
it generates a small amount of nuclear waste, it generates steam, but it generates zero emissions,

29:31.360 --> 29:37.120
right, zero carbon, right. And so you have this basically, it's amazing phenomenon where you have

29:37.120 --> 29:41.360
this, and let's just take them completely as space value, I'm not gonna, this is not me questioning,

29:41.360 --> 29:44.080
I'm not going to question carbon emissions or global war, I'm just gonna, I'm gonna assume

29:44.080 --> 29:47.520
that everything the environmentalists say about carbon emissions, climate, you know, change all

29:47.520 --> 29:51.200
our stuff. Let's assume that that's all totally real. Like, let's just, let's just grant them all

29:51.200 --> 29:56.720
that. It's like, okay, well, like, okay, so how could, how can you solve the sort of climate

29:56.720 --> 30:00.160
crisis, the carbon emissions crisis, it's like, well, you have the silver bullet technology,

30:00.160 --> 30:05.040
you could roll out in the form of nuclear fission today. You could generate a limited power. Richard

30:05.040 --> 30:09.760
Nixon, by the way, the, you know, the heavily, heavily condemned Richard Nixon in 1972,

30:10.480 --> 30:13.440
you know, proposed something at the time he called project independence.

30:14.400 --> 30:17.920
Project independence was going to be the United States building 1000 new civilian nuclear power

30:17.920 --> 30:23.200
plants by the year 1980, and cutting the entire US energy grid, including the transportation system,

30:23.200 --> 30:28.720
cars, everything, home heating, everything over to nuclear power by 1980, going zero emission

30:28.720 --> 30:32.320
in the US economy. And by the way, right, geopolitically removing us from the Middle East,

30:33.040 --> 30:38.160
right, right. So no, right, no, Iraq, FK, all that stuff, like just completely unnecessary,

30:38.160 --> 30:43.920
right. And, you know, you'll note that like project independence did not, did not happen,

30:44.640 --> 30:48.000
right, like we don't, we don't live in that world today. And so it's like, okay, you've got this

30:48.000 --> 30:53.040
like crisis, you've got this like silver bowl solution to for it, and you very deliberately

30:53.040 --> 30:58.160
have chosen to not adopt that solution. And it's like, and there's this actually very interesting

30:58.160 --> 31:01.600
split in the environmental movement today. And it's really kind of, you know, I think kind of

31:01.600 --> 31:06.160
bizarre. And it's like a 99 to one split, you asked like 99% of environmental activists about

31:06.160 --> 31:10.720
nuclear power, they just just sort of categorically dismiss it was, of course, that's not an option.

31:10.720 --> 31:15.520
You do have this kind of radical fringe with people like Stuart Brand, who are like, basically

31:15.520 --> 31:18.960
now pointing out that it is, it is a silver bullet answer, but most of them are saying, no,

31:18.960 --> 31:22.720
it's not an answer. And it's like, okay, well, why are they doing that? It's like, well, like,

31:22.720 --> 31:25.920
what is it that they're saying that they want to do? And what they're saying they want to do

31:25.920 --> 31:29.920
is what they call, you know, degrowth, right. And so they want to decarbonize the economy,

31:29.920 --> 31:34.000
they want to deenergize the economy, they want to degrow the economy. And then, you know, when

31:34.000 --> 31:38.000
you get down to it, and you ask them a very, you know, specific question about the implications of

31:38.000 --> 31:41.760
this, you know, basically what you find is the general model is they want to reduce the human

31:41.760 --> 31:45.520
population on the planet to about 500 million people. You know, it's kind of the answer that

31:45.520 --> 31:50.320
they ultimately come down to. And so ultimately, the, you know, the big agenda is to is to reduce

31:50.320 --> 31:54.720
the human, you know, basically the human herd, you know, quite sharply. And, you know, they kind

31:54.720 --> 31:57.600
of dance around this a little bit, but when they when they really get down to it, this is what they

31:57.600 --> 32:00.720
talk about. And of course, you know, Paul Ehrlich, you know, is kind of one of the kind of famous

32:00.720 --> 32:04.400
icons of this, he's been talking about this for decades. I think it was Jane Goodall, who used

32:04.400 --> 32:10.480
the 500, you know, million, you know, number recently in public. And so, and so, and so then

32:10.480 --> 32:15.280
you got this kind of very interesting, you know, technological philosophical moral question, which

32:15.280 --> 32:19.600
is like, well, what, what is the goal here, right, is the goal to like solve climate change, or is

32:19.600 --> 32:24.640
the goal to like depopulate the planet, right. And to the extent that like free unlimited power,

32:25.200 --> 32:28.720
right, would interfere with, you know, to the extent that that's a problem, the problem it

32:28.720 --> 32:33.760
would be as if the actual agenda is to depopulate the planet. And like, I would like this to not

32:33.760 --> 32:37.840
be the case. Like, I think, you know, again, take taking everything else that they say at face value,

32:37.840 --> 32:40.800
you'd like to solve carbon emissions and climate change and everything else. But like,

32:41.440 --> 32:45.040
you know, like, I think you, you know, you might also say you want a planet in which there are

32:45.040 --> 32:49.040
not only 8 billion people, but maybe, you know, maybe people are good, right. Maybe you're actually

32:49.040 --> 32:53.440
should have 20 billion or 50 billion people. And we have the technology to do that. And we're

32:53.440 --> 32:59.440
choosing not to do it. So, so, so, so this is the thing, like this gets into these very deep

32:59.440 --> 33:04.000
questions, right, to your point of like, okay, very deep questions about morality. And like,

33:04.000 --> 33:08.800
how did we maneuver or, you know, like per nature, like how did we reverse ourselves into a situation

33:08.800 --> 33:13.360
where we're actually arguing against human life. And of course, and this is we'll get to it, but

33:13.360 --> 33:16.560
this, this of course is then, you know, a big part of the origin of the idea of effective

33:16.560 --> 33:21.840
accelerationism, which is basically new, like let's go sharply in the other direction. Oh, and

33:21.840 --> 33:27.280
then yeah, so AI, yeah, AI is playing out much the same way as already playing out the same way.

33:27.280 --> 33:31.360
And here you've got this like just incredible phenomenon happening where we, we, you know,

33:31.360 --> 33:34.720
it looks like we have a key breakthrough to basically increase the level of intelligence,

33:34.720 --> 33:39.040
you know, basically all throughout society and around the world, you know, through, you know,

33:39.040 --> 33:43.200
basically for the first time, you know, directly applying your general intelligence to the world.

33:44.720 --> 33:49.360
And, you know, there is this like incredibly basically aggressive movement that is actually

33:49.440 --> 33:55.520
having tangible impact today in the halls of power in Washington DC and in the EU and other places,

33:56.080 --> 33:59.120
you know, that is seeking to stop and reverse it, you know, as aggressively as they possibly can.

33:59.840 --> 34:04.320
And so we're kind of, we're going through, we're going through, I would say, a suddenly accelerated

34:04.320 --> 34:08.320
and very sharp and aggressive version of exactly what happened with nuclear power happening with

34:08.320 --> 34:13.520
AI right now. I mean, this is the thing that can, can, well, there's two questions because

34:13.520 --> 34:19.120
on your blog, you, it's really refreshing to see you, you're pretty to the point when you say,

34:19.120 --> 34:25.440
look, AI is code, it's code written by people, by human beings on computers developed by human

34:25.440 --> 34:31.040
beings, you know, like we're in control, you're not of this, I think there was, you know, Musk

34:31.040 --> 34:35.920
signed a big thing where like, you know, 1000 people signed this thing to say like, we need to hold

34:35.920 --> 34:41.280
this the whole Rocco's Basilisk AI is going to be terminated to come in and blowing us up with

34:41.280 --> 34:45.280
robots, etc. So it's going to kill us all. You're very much like, no, this is code, this is just

34:45.280 --> 34:51.040
an intelligence for us to use. Now that's one question, you know, I guess, why isn't AI going

34:51.040 --> 34:54.480
to kill us all? And I know you've spoken about that a lot. So that answer can be brief. But

34:54.480 --> 35:01.200
secondly, this whole idea of trying to reverse it, to me, it seems inherent within AI as a thing

35:01.200 --> 35:06.800
that it wants, you know, it's the cats out the back, you can't like once it's here, you, you,

35:06.880 --> 35:13.040
outside of really draconian measures, you can't because how do you how do you hold an

35:13.040 --> 35:18.560
intelligence which is growing, right? Well, except, you know, they did stall at nuclear power,

35:18.560 --> 35:24.400
right? So, right, like so they did, like it worked. So why did project independence not happen?

35:24.400 --> 35:28.240
Why do we not have like, you know, unlimited nuclear power today? You know, the reason is

35:28.240 --> 35:31.920
because it was it was blocked by the by the political system, right? And so, so, you know,

35:31.920 --> 35:35.040
Richard Nixon, who I mentioned, you know, proposed this, he also created the Environmental

35:35.040 --> 35:39.280
Protection Agency and the Nuclear Regulatory Commission. You know, the nuclear, it's actually

35:39.280 --> 35:44.800
that this actually been a big week. The first new nuclear power plant design, the first newly

35:44.800 --> 35:49.360
designed nuclear power plant, in the last 50 years, just went online in Georgia, you know,

35:50.400 --> 35:54.480
$20 billion over budget and you know, it's got it's a story of its own, but at least we got one

35:54.480 --> 35:58.800
online. It's the first new nuclear power plant design ever authorized by the Nuclear Regulatory

35:58.800 --> 36:04.720
Commission, says Nixon created that commission, right? And so, so, so we put in place a regulatory

36:04.720 --> 36:09.360
regime around nuclear power in the 1970s that, you know, all but made it impossible. By the way,

36:09.360 --> 36:12.080
you alluded to the Germany thing earlier, I'll just touch on that for a second. So,

36:12.720 --> 36:15.920
you know, that, you know, you've, I'm sure you've heard of the idea of the precautionary principle,

36:15.920 --> 36:20.160
right? Right, which is this, this idea that basically scientists and technologists have a

36:20.160 --> 36:24.640
moral obligation to think through all the possible negative consequences of a new technology before

36:24.640 --> 36:28.240
it's rolled out. The precautionary principle, right, the precautionary principle, and we could

36:28.240 --> 36:32.160
talk about that, including whether scientists and technologists are actually qualified to do that.

36:33.120 --> 36:38.960
But, you know, this was also a central theme of Oppenheimer, but the precautionary principle

36:38.960 --> 36:43.120
was invented by the German Greens in the 1970s, and it was prevented specifically to stop nuclear

36:43.120 --> 36:49.360
power. And, you know, it is just amazing, we're sitting here in 2023, and there's this, you know,

36:49.360 --> 36:53.680
where we effectively, we in the West are effectively at this, you know, at war with Russia,

36:54.960 --> 36:59.280
right? And, you know, it's a proxy war right now that, you know, hopefully doesn't turn into a real

36:59.280 --> 37:04.160
war, but who knows, you know, the proxy wars have a, you know, have a disconcerting, you know,

37:04.160 --> 37:10.240
pattern of spilling over into becoming real wars. And, you know, a lot of this is, it's a tale of

37:10.240 --> 37:16.880
energy. And, you know, basically the Russian economy, you know, is like 70% energy exports,

37:16.880 --> 37:22.080
right, oil and gas exports. The major buyer of that energy historically has been Europe and

37:22.080 --> 37:27.760
specifically Germany. You know, Europe and Germany specifically essentially have funded the Russian

37:27.760 --> 37:32.320
state, the Putin state, you know, and that funding is what basically built and sustains

37:32.320 --> 37:37.840
their military engine, which is what they've used to invade Ukraine, right? And so it's this like,

37:38.560 --> 37:42.720
like, there's this counterfactual, right, where the German Greens did not do what they did in

37:42.720 --> 37:47.120
the 1970s, nuclear power was not blocked, you know, Germany and France and the rest of Europe

37:47.120 --> 37:51.120
today is like fully energy independent running on nuclear power, you know, the Russia state,

37:51.120 --> 37:55.360
it would be greatly weakened because the value of their exports would be, you know, enormously

37:55.360 --> 38:00.480
diminished. And they would not have the wherewithal to invade other countries or to threaten

38:00.480 --> 38:06.640
Europe. And so like, these decisions have like real consequences. And, you know, these people,

38:07.680 --> 38:12.560
use the pejorative sense, like they are so confident that they can step into these, you know,

38:12.560 --> 38:15.520
debates, you know, kind of questions around, you know, new technologies and how they should be

38:15.520 --> 38:19.040
applied and what the consequences are, they can step in and they can use the political

38:19.040 --> 38:23.280
machine to basically throw sand in the gears and stop these things from happening. So, so like,

38:23.280 --> 38:27.440
AI, this is what's happening right now. So like, you know, in the, in the sort of, you know,

38:27.440 --> 38:31.120
theoretical position where AI is kind of this, you know, potentially runaway thing, then, right,

38:31.120 --> 38:35.360
maybe it can be constrained, like, in the real world, it very much can be constrained. And

38:35.360 --> 38:39.520
the reason it can be constrained in the real world is because it uses physical resources,

38:39.520 --> 38:45.760
right? It has a physical, it has a physical layer to it. And that layer is energy usage.

38:46.640 --> 38:52.160
And that layer is chips. And that layer is, you know, telecom bandwidth. And that layer is data

38:52.160 --> 38:58.080
centers, physical data centers, right? And so, and that layer is like, you know, by the way,

38:58.080 --> 39:01.760
that layer also includes the actual technologists, like working in the field, and their ability to

39:01.760 --> 39:06.800
actually do what they do. And there are, you know, a very large number of sort of control points and

39:06.800 --> 39:11.840
pressure points that, you know, the state can put on those layers to prevent them from being used

39:11.840 --> 39:17.600
for whatever it wants to prevent. And, you know, and look, the EU is on the verge, the EU has this

39:17.600 --> 39:21.600
like anti AI bill that it looks like is going to pass that is like extremely draconian and may

39:21.600 --> 39:26.720
result in Europe not even having an AI industry and may result in, you know, American AI companies

39:26.720 --> 39:31.040
not even operating in Europe. And then in the US, we have a very kind of similar push happening is,

39:31.040 --> 39:37.120
you know, the sort of ant, what I would describe as the anti AI zealots are, you know, they are,

39:37.120 --> 39:41.360
they are in the White House today, right, arguing that, you know, this is bad, it should be stopped.

39:42.480 --> 39:46.640
And it's like, you know, it's, it's, it's amazing because it's like, how many times are we going

39:46.640 --> 39:50.720
to like run through this loop? How many times are we going to like repeat history here? How,

39:50.720 --> 39:54.560
how many times are we going to be kind of self defeating like this? And like apparently the,

39:54.560 --> 39:57.040
the impulse to be self defeating, we have not worked it out of our system.

39:58.720 --> 40:02.960
You don't want to be self defeating them. I mean, let's move into this peculiar four letters,

40:02.960 --> 40:07.680
which is found at the moment at the end of your Twitter name and the end floating around Twitter,

40:07.680 --> 40:14.640
mostly e slash act or effective accelerationism. And this like, this is just beautiful to me.

40:14.640 --> 40:18.480
It's like the, the acceleration is Renaissance. I've been set talking about it in that way. I

40:18.480 --> 40:22.640
don't want to gatekeep it too much, but you know, I wrote my master's thesis on accelerationism,

40:22.640 --> 40:26.480
like I love it. I love talking about it. You don't want any of this holding back. You don't

40:26.480 --> 40:30.800
want to hold anything back. You want to accelerate. So firstly, I mean, there's two questions there.

40:31.440 --> 40:36.320
What is it for you to accelerate and what is effective accelerationism?

40:37.600 --> 40:41.360
Yeah. So let me, let me just say where that, where it came from, I'll reverse the second one

40:41.360 --> 40:45.840
first and then go to the broader topic. So, so, so it's a, it's a combination. There's, there's,

40:45.840 --> 40:49.040
there's, you know, kind of two, two words there, effective and accelerationism. So the, you know,

40:49.040 --> 40:52.960
the acceleration, accelerationism part of it is obviously building on what you've talked about

40:52.960 --> 40:56.960
and what Nick Landon and others have talked about for a long time. And of course, as you,

40:56.960 --> 41:00.000
as you've talked about, there's, there's all these different versions of accelerationism. And so this

41:00.000 --> 41:03.440
is, this is, you know, proposing one that, you know, it's this, this one is like the closest to

41:03.440 --> 41:06.560
what you would call right, right accelerationism, although, you know, maybe without some of the

41:06.560 --> 41:12.080
political overtones. And so there is that component. There's also the effective part of it. And the

41:12.160 --> 41:16.400
effective part of it, it's sort of a half humorous reference, obviously, to effective altruism.

41:18.800 --> 41:21.120
And it's a little bit tongue in cheek, because it's like, of course, if you're going to have a

41:21.120 --> 41:25.120
philosophy, of course, you would like it to be effective. But, you know, but, but also look

41:25.120 --> 41:31.840
like EAC is like very much like EAC's enemy, right, the oppositional force that the thing that EAC was

41:31.840 --> 41:37.680
sort of formed to fight is actually, you know, specifically effective altruism. Right. And so

41:38.400 --> 41:43.200
it's also like, you also sort of, you know, use that term to the term effective to kind of kind

41:43.200 --> 41:48.880
of make that point, like this is in that world. And this is opposed to that. And, and, and the

41:48.880 --> 41:54.320
reason why like this is happening now, like the reason why the concept of effective accelerationism,

41:54.320 --> 41:57.280
you know, has kind of come into being. And by the way, that, you know, the people, this is not

41:57.280 --> 42:02.080
originally my formulation, this is, this is, there's, you know, kind of ultra smart Twitter

42:02.080 --> 42:09.680
characters, who I think are still mostly operating under assumed names. But there's Beth Jesus,

42:09.680 --> 42:14.880
and Bayes Lord are the two, the two, two of them that I know. And they're, you know, these are

42:14.880 --> 42:20.000
like top and Silicon Valley, you know, engineers, scientists, technologists. But, you know, at least

42:20.000 --> 42:25.360
for now, they're operating kind of under undercover pseudonym. So the reason this is happening now

42:25.360 --> 42:29.840
is because of what I, what I was describing earlier with AI, which is you have this, you have this

42:29.840 --> 42:33.680
other movement, you have this movement of what's sort of called sometimes it's used different

42:33.680 --> 42:40.800
terms AI risk, AI safety, AI alignment. Sometimes you'll hear the term X risk. You know, sometimes,

42:40.800 --> 42:44.800
and then this is sort of directly attached. This is all part of the, you know, EA world,

42:44.800 --> 42:50.240
the effective altruism world. And then, you know, the central characters of this other world are,

42:50.240 --> 42:55.040
you know, Nick Bostrom, Elisir Yadkowski, you know, the open philanthropy organization.

42:55.040 --> 42:59.600
And, you know, a bunch of these, a bunch of these kind of, you know, the sort of the AI,

42:59.600 --> 43:05.360
what we call the AI doomers running around, like the AI doomer movement is basically a

43:05.360 --> 43:10.640
part and parcel with the effective altruism movement. And, you know, AI existential risk has

43:10.640 --> 43:14.560
always been kind of the boogeyman of effective altruism, kind of going back, you know, over the

43:14.560 --> 43:20.320
20 year development of EA. And so anyway, that that EA movement is the movement, by the way,

43:20.320 --> 43:24.720
with lavish funding by like EA billionaires, which is which is part of the problem, by the way,

43:24.720 --> 43:31.280
who made all their money in tech, which is also amazing. But, you know, so you've got this funding

43:31.280 --> 43:36.160
complex, you've got this EA movement, you've got this attached AI risk safety movement,

43:36.160 --> 43:42.080
and now you've got like active lobbying, you know, sort of anti AI PR campaign. And so anyway,

43:42.080 --> 43:46.720
so effective effective acceleration is intended to be the polar opposite of that, it's intended to

43:46.720 --> 43:52.560
be the, you know, to head boldly and firmly and strongly and confidently into the into the future.

43:53.520 --> 43:58.320
You know, it's like, why, you know, why, why this form of positive accelerationism, you know,

43:58.320 --> 44:02.800
it's, there's a couple different layers of it. The founders of the, of the concept of the act

44:02.800 --> 44:06.000
have a thermodynamic, you know, kind of thing, which, which we could talk about, but it's kind

44:06.000 --> 44:10.560
of one layer down from our operate. The layer operate is more at the level of engineering. And

44:10.560 --> 44:14.480
when I think about it, I think in terms of essentially fundamentally of material conditions.

44:14.480 --> 44:21.680
So human flourishing, quality of life, standard of living of human beings on earth. And back to

44:21.680 --> 44:26.320
that concept of productivity growth, you know, the application of technology, to be able to

44:26.320 --> 44:30.640
cause the economy to be more productive and therefore cause more material wealth, higher

44:30.640 --> 44:34.720
levels of material welfare, you know, for people all over the world, by the way, also with reduced

44:34.720 --> 44:39.520
inputs, right. And so not just greater levels of development and greater levels of advance,

44:39.520 --> 44:44.400
but also greater levels of efficiency. And the nature of technology as a lever on the physical

44:44.400 --> 44:47.680
world is you can have your cake and eat it too, you can get higher levels of output with lower

44:47.680 --> 44:50.400
levels of input. And the result of that is a much higher standard of living. So,

44:50.960 --> 44:56.400
so I kind of adopt my, my philosophical grounding is sort of, you know, I don't know if I call it

44:56.400 --> 45:01.440
like a positive materialism or something, you know, which is like, I think the thing that we,

45:01.440 --> 45:04.560
the thing that the technology industry does best is improve material quality of life.

45:05.840 --> 45:09.440
I think that we should accelerate as hard into that as we possibly can. I think the quote,

45:09.440 --> 45:16.720
unquote risks around that are greatly exaggerated, if not, if not false. And, and, you know, I think

45:16.720 --> 45:20.480
the forces against basically technological progress, you know, they're like the environmental

45:20.480 --> 45:24.320
movement I described, you know, they're fundamentally sort of at some deep level,

45:24.320 --> 45:28.800
they're sort of anti-human, you know, they want fewer people and they want a lower quality

45:28.800 --> 45:31.120
living on earth. And like, I just, I very much disagree with both of those.

45:32.160 --> 45:37.280
And what is this at the thermodynamic level? Is this, is this the, you know, we are ultimate

45:37.280 --> 45:43.440
enemy is entropy? So there's, there's a thermodynamic part gets complicated. And this is

45:43.440 --> 45:46.640
not my, my field. So there's, there's other people that you should probably have on to talk

45:46.640 --> 45:51.600
about this, but the effect of accelerationism version of the thermodynamic thing is, is based

45:51.600 --> 45:57.040
on the work of this physicist named Jeremy England, who is this very interesting character

45:57.600 --> 46:05.040
actually, he's actually trained by one of my partners. And is now basically, he's an MIT,

46:05.040 --> 46:09.760
you know, physicist, you know, biologist, and by the way, and also by the way, interesting guy,

46:09.760 --> 46:12.800
I don't know him, but a very interesting guy from the distance, he's also a trained rabbi.

46:14.080 --> 46:18.640
And so he's an interesting cat. And so he basically has this theory that basically,

46:19.200 --> 46:24.080
basically it's, it's sort of life is the direct result life, life, like the phenomenon of life

46:24.080 --> 46:29.440
itself is a direct consequence of thermodynamics. And, you know, the way he describes it is

46:29.440 --> 46:35.200
basically, basically, if you take, you know, basically the universe with a level of energy

46:35.200 --> 46:40.080
that's washing around and raw materials, and you sort of apply kind of natural selection at a very

46:40.080 --> 46:45.120
deep level, you know, you know, even at the level of just like the formation of materials,

46:45.120 --> 46:49.600
like on a planet or something, you basically have this thing where basically a matter wants

46:49.600 --> 46:55.040
to organize itself into states where it's able to absorb energy and achieve higher levels of

46:55.040 --> 47:00.560
structure. And so you have absorption of energy, you have achievement of higher levels of structure,

47:00.560 --> 47:04.480
in the case of organic life, that's, you know, starts with our basic RNA, and then kind of works

47:04.480 --> 47:09.200
this way up to, you know, full living systems. And then on the other side of that, as we talked

47:09.200 --> 47:13.520
about before, on the other side of that is your, the result of that is your sort of your dumping

47:13.520 --> 47:18.720
heat, which is to say entropy, you know, kind of out into the broader system. And so it's almost

47:18.720 --> 47:24.000
like saying the second law of thermodynamics has an upside, right, which is basically, yes, entropy

47:24.000 --> 47:29.040
in the universe is increasing over time, but a lot of that increases the result of structures

47:29.040 --> 47:34.640
forming that are basically absorbing energy and then exporting entropy. And one form of that

47:34.640 --> 47:39.760
structure is actually life. And this, and this is actually a thermodynamic, you know,

47:39.760 --> 47:44.080
biomechanical, bioelectrical kind of explanation of actually how organic life works. Like this is

47:44.080 --> 47:48.480
what we are, we are machines for gathering energy, you know, forming increasingly, you know, complicated

47:48.480 --> 47:53.600
biological machines, replicating those machines, right. And of course, you know, he talks about

47:53.600 --> 47:57.280
like, you know, natural selection, like it's not surprising that natural selection is so oriented

47:57.280 --> 48:00.800
around replication, right, because replication is the easiest way to generate more structure.

48:01.440 --> 48:05.920
Right. Like replication is the way that a system that is basically in business to

48:05.920 --> 48:09.200
generate structure, it's the way that it can most efficiently generate more structure.

48:10.480 --> 48:16.400
And so anyway, basically, the universe wants us to basically be alive. The universe wants us to

48:16.400 --> 48:22.240
become more sophisticated. You know, the universe wants us to replicate. You know, the universe

48:22.240 --> 48:26.160
feeds us and, you know, an essentially a limited amount of energy and raw materials with which to

48:26.160 --> 48:32.080
do that. You know, yes, we dump entropy out the other side, but we get structure and life,

48:32.080 --> 48:34.400
you know, to basically, to basically compensate for that.

48:35.360 --> 48:40.160
The universe is a, the universe is pronatalist and kind of Nietzsche in there as well.

48:40.960 --> 48:45.600
Yeah, exactly. 100%. Yeah. So anyway, so that's, that's the, that's the thermodynamic,

48:45.600 --> 48:50.240
that's the thermodynamic underpins of effective accelerationism. The people who have encountered

48:50.240 --> 48:54.880
effective acceleration, effective accelerationism, some of that get very, some of them get very

48:54.880 --> 48:58.720
deeply into that. And there's a very deep kind of well there to, to draw from this guy, Jeremy

48:58.720 --> 49:02.080
England has a book out. Actually, you'll appreciate this. This guy, Jeremy England has a book out

49:02.080 --> 49:07.120
and the title of the book is something like every life is on fire. And it's actually funny,

49:07.120 --> 49:11.040
because it's like, if you read Heraclitus, you're like, Oh my God, you know, he saw it.

49:12.480 --> 49:17.040
Right. Like, it's like, there's something very, very deep going on here with this sort of

49:17.040 --> 49:21.920
intersection of energy life. But so he's got this book out, which apparently is quite good.

49:22.880 --> 49:26.640
And so some people in effective acceleration kind of, kind of go deep. There's a tongue-in-cheek

49:26.640 --> 49:30.800
reference to the so-called thermodynamic God, right, which is not, you know, which is not a

49:30.800 --> 49:34.800
literal, you know, religious, in the literal religious sense, like a, you know, sort of

49:34.800 --> 49:38.640
a conscious God or a sentient God, but more of this, this idea that the universe is, is, is,

49:38.640 --> 49:42.960
is sort of designed to express itself in the forms, you know, basically in higher and higher

49:42.960 --> 49:47.360
forms of life. Yeah, to your point, like there's an obviously direct niche in connection.

49:47.680 --> 49:52.800
And, you know, so maybe, maybe he saw a lot of this too. You know, and obviously he, you know,

49:52.800 --> 49:56.480
he was obviously writing and thinking at the same time Darwin was figuring a lot of this out on the,

49:56.480 --> 50:00.960
on the natural selection evolution side. Yeah, so there's that. But, but having said that,

50:00.960 --> 50:06.160
like, like I said, my take on it is more, you know, I find that stuff fascinating. I'm more

50:06.160 --> 50:09.760
naturally inclined as an engineer, more naturally inclined towards the material side.

50:10.640 --> 50:14.640
And so I just more naturally think in terms of the, the social systems and the technological

50:14.640 --> 50:19.360
development and the impact on, on, on, on material quality of life. And so I think you

50:19.360 --> 50:23.280
can also just take it at that level and not, not, not have to get all the way down into thermodynamics

50:23.280 --> 50:27.120
if you don't want to. I mean, there's an odd, I mean, yeah, drawing it down to this level of

50:27.120 --> 50:30.640
engineering, well, not down to, but just to this level of engineering, there's this odd

50:30.640 --> 50:34.720
learned helplessness. And I mean, just to take the two examples we've given so far. So, and,

50:34.720 --> 50:39.680
you know, they work quite well actually nuclear energy on the atomic side and AI on the bit side

50:39.760 --> 50:45.600
of things, virtual, I guess, virtual reality and reality. You posted this really interesting

50:45.600 --> 50:50.400
essay on your, on your blog about availability cascades, which is about basically, in short,

50:50.400 --> 50:58.240
if I'm getting this right, this idea of why are so many people interested in this thing or this

50:58.240 --> 51:04.080
view of whatever the, the opinion or the idea is that's floating around. And it seems on both of

51:04.080 --> 51:09.680
those, both nuclear energy and AI, we have that same opinion, which is like, mimetically infected

51:09.680 --> 51:13.920
culture of a sort of learned helplessness, like, oh, no, you know, we've already spoken about this

51:13.920 --> 51:17.600
a bit, but like, oh, no, we need to get rid of this, we can't deal with this. But it seems,

51:17.600 --> 51:21.680
do you think on the engineering side of things, and I guess it overlaps also into the social in

51:21.680 --> 51:28.720
terms of how you engineer and how you promote these ideas socially as, as tools, as things that

51:28.800 --> 51:35.520
people use is an attempt to like, invert that availability cascade and like, try to like,

51:37.040 --> 51:43.360
begin some mimesis on the side of like, it's okay to want a better quality of living, it's okay to

51:43.360 --> 51:49.200
want to grow, it's okay to want energy, like, you don't have to be almost like submissive to,

51:49.200 --> 51:54.800
to whatever this strange, self-defeating learned helplessness is that we have in terms of

51:54.880 --> 52:00.880
technology and our like, our like weird allegiance to just this, this stagnant comfort that we've

52:00.880 --> 52:04.960
had for too long. Yeah, that's, that's, that's right. That's exactly right. And like I said,

52:04.960 --> 52:08.160
like we said, like we talked about earlier, like, there's, there's this, I think there's a natural

52:08.160 --> 52:13.040
human impulse deeply wired into like the limbic system or something, which is basically, right,

52:13.040 --> 52:18.160
fear over hope, right? You know, like, what's most likely to come over the ridge, right,

52:18.160 --> 52:21.520
a sabre to tiger to eat you or like something warm and cuddly that wants to be your friend,

52:21.520 --> 52:26.080
right? I guess a quack or something like that, right? So, right, it's, it's probably the tiger,

52:26.080 --> 52:29.680
right? And you know, there's, there's a sort of, you know, false positive, false negative,

52:29.680 --> 52:32.960
right, two ways of making mistakes. And you definitely, from an evolutionary standpoint,

52:32.960 --> 52:37.040
want to err in the direction of being, you know, more, you know, more, you want to overestimate

52:37.040 --> 52:43.600
the rate of Cybertooth Tigers, right, to, to, to survive. So, so, so that, that impulses deep.

52:43.600 --> 52:46.720
Yeah. But then, you know, what we have is, you know, we have, we have sentience, we have the,

52:46.720 --> 52:50.240
you know, the, we're not just limbic systems anymore, we have a, we have the ability to control

52:50.240 --> 52:54.080
environment, the ability to build tools, we're not afraid to save the two tigers anymore.

52:55.200 --> 52:59.920
And so, yeah, we have the ability to shape our world. You know, we develop rationality and

52:59.920 --> 53:03.520
the enlightenment and science and technology and markets and everything else to be able to control

53:03.520 --> 53:08.240
the world, you know, to our benefit. And so, we, you know, we don't, we don't have to live

53:08.240 --> 53:13.760
cowering in fear anymore, you know, as much as, or as much as that might be like grimly satisfying,

53:13.760 --> 53:17.680
like we don't actually have to do that. And there's actually a, you know, very, there are many,

53:17.680 --> 53:20.880
many good reasons over the last 300 years to believe that, you know, there's, there's a much

53:20.880 --> 53:25.680
better way to live. Yeah, but look, somebody has to say, you know, somebody has to actually say that.

53:26.560 --> 53:32.560
And then look, I think the other part is, I think there's a big divide. I think there's a big divide

53:32.560 --> 53:37.280
between, I'll pull up my, my Burnham on this a little bit is, is a big divide on this stuff

53:37.280 --> 53:42.400
between what you describe as the elites and the masses that has turned out to be pretty interesting.

53:42.400 --> 53:47.200
So the, the, I would say this problem, this problem of fear of technology,

53:48.000 --> 53:52.400
or hatred of technology or desire to stop technology, I think it's primarily a phenomenon of the

53:52.400 --> 53:58.640
elites. I actually don't think it's particularly shared by the masses. And it just seems like

53:58.640 --> 54:03.360
I just take AI as an obvious example. One of the amazing things about AI is it's like freely

54:03.360 --> 54:07.840
available for use by everybody in the world right now today, fully state of the art, like the best

54:07.840 --> 54:14.800
AI in the world is on, you know, websites from open AI and Google and Microsoft. And you can go

54:14.800 --> 54:19.200
on there and you can use it for free today. And people, you know, hundreds, hundred, already

54:19.200 --> 54:22.000
a hundred, 200 million people, something like that around the world are already doing this,

54:22.000 --> 54:26.560
right? And if you talk to anybody who's, you know, if you talk to any teacher, right, you know,

54:26.560 --> 54:29.520
you know, they'll already tell you they've got students using chat GPD to write essays and so

54:29.520 --> 54:35.440
forth. Right. And so you've got this amazing thing where, you know, like the internet before it

54:35.440 --> 54:40.080
and like the personal computer before it and like the smartphone before it, AI is, it's like

54:40.080 --> 54:44.880
immediately democratized, right? Like it's immediately available in its full state of the art

54:44.880 --> 54:50.560
version. Like there's no more advanced version of like GPT that I can buy for a million dollars than

54:50.560 --> 54:56.400
you can get for free or by paying 20 bucks for the upgraded version on the open AI website.

54:56.400 --> 55:02.400
Like the state of the art stuff is fully available for free. And so you have people all over the

55:02.400 --> 55:05.440
world. And this is one of my, this would be a source of optimism that the AI doomers are going

55:05.440 --> 55:09.040
to lose almost by definition, right? Is you have people all over the world who are just already

55:09.040 --> 55:12.400
using this and they're getting, you know, great value out of it in their daily lives. They love

55:12.400 --> 55:16.000
it. They're having a huge amount of fun with it. You know, it's great. They're good, you know,

55:16.000 --> 55:19.200
making new art and they're, you know, doing all kinds of, you know, asking all kinds of things

55:19.200 --> 55:22.400
and it's helping them in their jobs and in school and everything else and they love it. So,

55:22.960 --> 55:27.600
so I think there's this thing where like, I actually think that what we, what we're actually

55:27.680 --> 55:31.200
talking about from a social standpoint is basically essentially a corrupt elite,

55:32.240 --> 55:36.640
a corrupt oligarchic elite that basically has been in a position of gatekeeping power,

55:36.640 --> 55:41.600
you know, for, you know, basically in its modern form for 60 years. And every new technology

55:41.600 --> 55:46.400
development comes along as a threat to that. And back to the Morrison thing like that,

55:46.400 --> 55:50.720
that's why they hate and fear new technology. You know, they would very much like to control it.

55:50.720 --> 55:54.000
You know, it's like social media, like they're all just like completely furious about social

55:54.000 --> 55:57.280
media, but like, you know, 3 billion people use social media every day and they love it.

55:58.320 --> 56:02.320
And so it's only the elites that are constantly kind of raging against it. The problem is,

56:02.320 --> 56:06.240
the elites are actually in charge of right from a, from a formal, you know, government,

56:06.240 --> 56:10.560
like they actually have the ability to write laws. By the way, you also see this in polls.

56:11.280 --> 56:16.560
If you pull on, like there's two, two, two very interesting kind of phenomena,

56:16.560 --> 56:20.720
phenomena kind of unfolding if you do these broad based polls on trust in institutions.

56:21.200 --> 56:25.760
And there's organizations Gallup in particular, and then there's another organization called Edelman

56:25.760 --> 56:29.600
that does these polls every year of basically essentially, they pull regular people and the

56:29.600 --> 56:33.200
question is like, which institutions do you trust? And that the institutions here includes

56:33.200 --> 56:37.200
everything from the military to, you know, religion to schools to government to, you know,

56:37.200 --> 56:42.480
journalism to, you know, big companies, big tech, and so forth, small business. And basically,

56:42.480 --> 56:48.880
the two big themes you see in those polls is one is ordinary people trust in institutions,

56:48.880 --> 56:54.000
trust in any sort of centralized gatekeeping function is been in basically secular decline

56:54.000 --> 56:57.680
basically since the 1970s, corresponding to the period we've been, we've been talking about.

56:58.720 --> 57:02.400
And so generally, people as a whole have kind of had it with the gatekeepers,

57:03.680 --> 57:06.880
which is very interesting. And by the way, that phenomenon, actually, the beginning of that

57:06.880 --> 57:12.480
predates the internet and social media. And so that traces back to the early 70s. And so I think

57:12.480 --> 57:17.360
that which I think is not an accident. It was just like, it's where the current regime basically

57:17.360 --> 57:22.800
essentially took control. And then the other thing that's so striking is that, you know, although

57:22.800 --> 57:27.200
you can you can sit and read the news all day long, and where they just like hate on tech

57:27.200 --> 57:32.160
companies all day long, if you do the poll of, you know, basically businesses by category,

57:32.160 --> 57:37.520
tech polls by far at the top. And so again, ordinary people are just like, wow, my iPhone's

57:37.520 --> 57:42.960
pretty cool. I kind of like it. And the strategy PT thing seems really nifty. And so I do think

57:42.960 --> 57:48.240
there's this weird, like I do think there is this this aspect of this where like, it's a cliche to

57:48.240 --> 57:50.560
say the elites are out of touch, of course, the elites are out of touch, the elites are always

57:50.560 --> 57:54.400
out of touch. But like, it seems like the elites are particularly out of touch right now, including

57:54.400 --> 57:59.920
on this issue. And another way kind of through this not whole is, you know, they may just simply

57:59.920 --> 58:05.440
discredit themselves. Like the EU is a great example, the EU may pass this anti AI law, and the

58:05.440 --> 58:10.720
population of Europe might just be like, what's the hell? Right. And so that that's that would be

58:10.800 --> 58:16.160
another white pill against what otherwise looks like a deep kind of drive in our society for

58:16.160 --> 58:21.360
stagnation. It would also be really strange to try and find a way to like define AI in that sense,

58:21.360 --> 58:26.320
because it's not like we haven't been, we haven't been using it in a minor form before all this

58:26.320 --> 58:30.080
for a while, right? So I don't know how they'd go about defining that in that way.

58:31.040 --> 58:36.800
Yes. So, yes. So do you ban linear algebra? Right? Do you ban linear algebra? And it's

58:36.800 --> 58:39.440
actually really funny, because I don't know if you know this, there actually is a push under

58:39.440 --> 58:42.880
way to quote unquote ban algebra. And it's literally in California, there's a big push

58:42.880 --> 58:47.440
underway to drive it out of the schools in California. So there's a big push first to

58:47.440 --> 58:50.320
start with a push to drive calculus out of the schools in California, and now it's extended

58:50.320 --> 58:54.800
to drive algebra out. And of course, this is being done under the so called rubric of equity,

58:54.800 --> 58:59.040
right? Because it turns out, you know, test scores for advanced math, you know, vary by,

58:59.600 --> 59:05.200
you know, vary by group. And so, you know, there's this weird thing where like in California,

59:05.200 --> 59:09.360
we're trying to push algebra out of the school. In Washington, we're trying to push algebra like

59:09.360 --> 59:14.000
out of tech. Like the whole thing is, and this is where I get like really, you know, this is

59:14.000 --> 59:17.200
where I start to get emotional, because it's like really like we spent, you know, 500 years

59:17.200 --> 59:20.320
climbing our way out of, you know, primitivism and getting to the point where we have like

59:20.320 --> 59:25.120
advanced science and math, and we're literally going to try to ban it. I was involved, I was

59:25.120 --> 59:28.800
involved, if you remember this, there was actually a similar push like this, there was a push in

59:28.800 --> 59:37.600
the 1990s to ban cryptography, right, to ban the idea of codes, right, ciphers, right. And as you

59:37.600 --> 59:42.720
probably know, like codes and ciphers are just math, like all they are is math, right. And there was

59:42.720 --> 59:46.240
a move in the 1990s where people who thought that cryptography, obviously, you know, there's all

59:46.240 --> 59:50.400
these anti-cryptography arguments, because like bad guys can use it to hide and so forth. And so,

59:50.400 --> 59:53.520
there was this like concerted effort by the US government and other Western governments to

59:53.520 --> 59:57.280
ban cryptography in the 90s. And it took us years to fight and defeat that. And I was like, okay,

59:57.280 --> 01:00:01.360
that was so stupid, that will certainly never happen again. And like we're literally back at

01:00:01.360 --> 01:00:07.920
trying to ban math again. Well, that does lead me just to the final question here,

01:00:09.040 --> 01:00:12.880
which is to do with the future. I mean, whether or not it's your optimistic,

01:00:12.880 --> 01:00:16.480
pessimistic in relation to, you know, I guess it would draw in on what we've just been talking

01:00:16.480 --> 01:00:20.800
about there. How do you envision the short term future, which I've put down here like 10 to 50

01:00:20.800 --> 01:00:29.680
years, and then how do you, what do you foresee for the year 3000 AD? Oh boy. So, I should start

01:00:29.680 --> 01:00:35.200
by saying I'm not a utopian. So, you know, we talked a little bit early about kind of these

01:00:35.200 --> 01:00:39.360
impulses, the kind of dry people to these kind of extreme points of view. Like the way I think

01:00:39.360 --> 01:00:45.120
about it is like there's a natural drive. A lot of people have what Thomas Soll called the unconstrained

01:00:45.120 --> 01:00:48.640
vision, so that they've got these kind of very broad based kind of visions. And you know, those

01:00:48.640 --> 01:00:53.760
visions kind of then split into like a utopian vision. And that might be, you know, in for AI,

01:00:53.760 --> 01:00:57.600
that might be something like the singularity, right? Or in the 1990s, these were called the

01:00:57.600 --> 01:01:02.960
Extropians, right, which is sort of this idea of kind of a material utopia as a consequence of like

01:01:02.960 --> 01:01:08.000
AI and let's say nanotechnology on the one hand. And that's where that, by the way, the idea of

01:01:08.000 --> 01:01:11.520
the singularity came from, right, which is Ray Kurzweil and Bernard Vinge, they were like at some

01:01:11.520 --> 01:01:17.200
point you get this kind of, you know, point of no return, which is like a utopian point of no return.

01:01:17.200 --> 01:01:22.560
But then of course, the flip side of every utopia is, you know, apocalypse. And so then that's where

01:01:22.560 --> 01:01:26.880
the sort of AI, you know, sort of the Singulitarians 20 years ago have become that, you know,

01:01:26.880 --> 01:01:30.800
a lot of them have become the AI doomers of today. And they, you know, they have sort of this sort

01:01:30.800 --> 01:01:34.400
of, you know, they have the same utopian impulse, they've just flipped the bit and made it negative.

01:01:35.120 --> 01:01:40.480
So I should say like, I'm not one of those. I'm probably more of a materialist and a little bit

01:01:40.480 --> 01:01:44.800
more of a, like I said, an engineer where, you know, things, for example, have constraints in

01:01:44.800 --> 01:01:50.720
the real world. So I don't think we tend to get the extreme, quite the extreme outcomes, but I do

01:01:50.720 --> 01:01:54.400
think we get, you know, we get change, like we get, we get change, we get change in the margin,

01:01:54.400 --> 01:01:57.520
and then, you know, change in the margin that compounds over time can become, you know, quite,

01:01:57.520 --> 01:02:04.320
quite striking. So look over 10 to 50 years, you know, look sitting here today, like, if we want it,

01:02:05.600 --> 01:02:10.160
you know, you can imagine the next 50 years to be characterized by, you know, the rise of AI,

01:02:10.160 --> 01:02:15.120
looks like we kind of figured that out now. You know, this superconductor thing, if it's real,

01:02:15.120 --> 01:02:19.200
that's a, you know, turning point moment. And by the way, if it's not real, it may be, you know,

01:02:19.200 --> 01:02:22.320
that this result points us in the direction of something that becomes real in the next few years.

01:02:23.040 --> 01:02:28.720
And so you can imagine some combination of AI, superconductors, you know, biotech,

01:02:28.720 --> 01:02:32.960
you know, you know, all these new techniques for, you know, biooptimization, gene editing,

01:02:34.320 --> 01:02:38.800
you know, and then, you know, nuclear, you know, if we get our act together on nuclear fission,

01:02:38.800 --> 01:02:41.760
by the way, there's a lot of really smart people working on nuclear fusion right now.

01:02:43.040 --> 01:02:46.480
You know, fusion is, you know, would be an even bigger, you know, kind of opportunity

01:02:46.480 --> 01:02:51.280
for unlimited clean energy. You know, now, you know, my cynical, the cynic in me would say

01:02:51.280 --> 01:02:55.200
if fission is illegal, then they're certainly going to make fusion illegal. But, you know, that,

01:02:55.200 --> 01:02:58.720
you know, that's a choice. We all get to decide whether we want to live in a world where fusion

01:02:58.720 --> 01:03:03.840
is illegal. So, you know, we get nuclear fusion. And so sitting here 50 years from now, you know,

01:03:03.840 --> 01:03:09.040
we basically are like, wow, you know, we are like, we have, you know, we are all much smarter

01:03:09.040 --> 01:03:11.760
than we were because we have these smart machines working with us and everything.

01:03:12.640 --> 01:03:16.880
You know, we have solved whatever environmental problems we thought we had, you know, with,

01:03:16.880 --> 01:03:20.320
we have abundant energy in an increasingly clean environment.

01:03:21.200 --> 01:03:26.160
You know, we're curing diseases at a rapid pace. And, you know, new babies are born that are immune

01:03:26.160 --> 01:03:31.040
to disease. And so, you know, you know, not quite a material utopia, but like, you know,

01:03:31.040 --> 01:03:35.200
a significant, you know, meaningful step function upgrades in human quality of life.

01:03:35.200 --> 01:03:38.960
Like, I think that's all very, over a 50-year period, for sure, like that. And that's all very

01:03:38.960 --> 01:03:44.000
possible. Over a 3,000, over whatever the year 3,000, over a 1,000-year period. I mean, look,

01:03:44.560 --> 01:03:48.560
you do get into these questions, you know, you do, if you're going to talk about 1,000 years,

01:03:48.560 --> 01:03:52.240
like you do get into these questions of like, you know, for example, a merger of man and machine,

01:03:52.240 --> 01:03:56.160
right? So you do have to, over that time frame, you have to start thinking about things like,

01:03:56.160 --> 01:04:00.640
you know, the neural link, you know, like where neural link takes you. And, you know, you know,

01:04:00.640 --> 01:04:03.920
over that period of time, you know, you'll definitely have like, you know, neural augmentation.

01:04:03.920 --> 01:04:08.880
So, you know, do you have shifting definitions of humanity? You know, where is the transhumanist

01:04:08.880 --> 01:04:13.280
movement actually taking us? You know, becomes a very interesting question over that time frame.

01:04:14.000 --> 01:04:17.600
Obviously, you have lots of questions over that time frame of space, you know, exploration,

01:04:17.600 --> 01:04:21.280
getting to other planets, you know, other life, you know, either other life in the universe or

01:04:21.280 --> 01:04:24.560
not other life in the universe. So kind of the spread of the, you know, the spread of our civilization

01:04:24.560 --> 01:04:30.160
more broadly. You know, so there you truly get into science fiction scenarios. You know, then,

01:04:30.800 --> 01:04:35.280
yeah, that's it. I'm always fun to talk about. I will admit, I am much more focused on the next

01:04:35.280 --> 01:04:40.400
50 years. Yeah, I mean, is there anything you'd like to add into the conversation

01:04:41.120 --> 01:04:43.280
that you feel, you know, is key that we haven't touched on?

01:04:44.080 --> 01:04:47.760
Yeah, no, I think that's a good, I think that was a good, covered a lot of ground.

01:04:47.760 --> 01:04:53.440
Yeah. So for effective accelerationism, just if you Google, there's a number of good already

01:04:53.440 --> 01:04:57.920
websites and substacks talking about that. A lot of the conversations happening on Twitter.

01:04:59.120 --> 01:05:05.120
So, and I already dropped the names of the of the EAT guys. So Beth Jesus and Bayes-Laurie,

01:05:05.120 --> 01:05:11.760
definitely follow those guys. You know, I've not met Nick Land, but I would definitely give a

01:05:11.760 --> 01:05:14.560
shout out and say, for anybody who hasn't encountered his work, they should definitely read

01:05:14.560 --> 01:05:21.040
up on it. He is, I think pretty clearly like the philosopher of our time and not even because,

01:05:21.040 --> 01:05:24.480
you know, whether I agree or disagree with him on everything he said. And of course,

01:05:24.480 --> 01:05:29.360
he's changed his views on a lot of things over time, but just the framework that he operates,

01:05:29.360 --> 01:05:35.120
like his willingness to actually go deep and actually think through the consequences of the

01:05:35.120 --> 01:05:39.440
kinds of technologies that I deal with every day, you know, are just, I think, way beyond most other

01:05:39.440 --> 01:05:44.720
people in his field. And so it's, it's, and I know he took kind of a long road to get here,

01:05:44.720 --> 01:05:49.360
so it's fun to see, you know, it's fascinating to read that. Oh, I'll point to one other thing.

01:05:49.360 --> 01:05:52.640
So I already mentioned the Jeremy England book, and I'll point to one other book that people

01:05:52.640 --> 01:05:57.360
might find interesting. So, so a lot of Lance work and a lot of accelerationism, right, is based on

01:05:57.360 --> 01:06:02.400
these, these, the ideas mob this field called cybernetics, which is kind of this, this, it's

01:06:02.400 --> 01:06:05.840
cybernetics is interesting because it's kind of this lost field of engineering.

01:06:06.960 --> 01:06:14.800
It was super hot in the, as an engineering field from the 1940s to the 1960s. And it basically was

01:06:14.800 --> 01:06:18.400
sort of the original computer science. And then it was sort of, it was also sort of the original

01:06:18.400 --> 01:06:22.560
artificial intelligence. A lot of the AI people of that era kind of call themselves cybernetics

01:06:22.560 --> 01:06:27.920
or cybernetics. But it really is an engineering field that kind of went away or got a lot more

01:06:27.920 --> 01:06:34.560
sedate after the 60s. And, but, but, but as I mentioned, like a lot of the ideas around AI and,

01:06:34.560 --> 01:06:37.840
you know, machine world of machines and thermodynamics, a lot of those ideas were being

01:06:37.840 --> 01:06:42.640
explored as far back as the 30s and 40s. So the cybernetics people of that era thought a lot

01:06:42.640 --> 01:06:46.240
about a lot of these questions. Anyway, there's this great book is a lot of original source

01:06:46.240 --> 01:06:50.960
material on this. And the, you know, the key character of that movement was Norbert Wiener,

01:06:50.960 --> 01:06:54.320
and there's a bunch of books by him and about him. But there's also a great book came out recently

01:06:54.320 --> 01:06:58.960
called Rise of the Machines by an author named Thomas Red. And it sort of reconstructs the

01:06:58.960 --> 01:07:03.680
archaeology of cybernetics and sort of makes clear how relevant those ideas are today.

01:07:04.720 --> 01:07:08.960
And so if you read that in conjunction with, with, with, with Nick Land's work, I think you'll find

01:07:08.960 --> 01:07:14.480
it pretty interesting. I'll be sure to put the link for your Twitter and your blog in the description

01:07:14.480 --> 01:07:18.800
below as well. But yeah, I think that's a good place to finish up. Mark Andreessen, thanks very

01:07:18.800 --> 01:07:24.400
much. Good, James, a pleasure. Thank you.

