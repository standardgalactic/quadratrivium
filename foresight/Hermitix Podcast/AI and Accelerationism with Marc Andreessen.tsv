start	end	text
0	7000	In this episode, I'm joined by Mark Andreessen to discuss accelerationism, AI, technology,
7000	10400	the future, energy and more.
10400	14560	I'd like to say a big thank you to all my paying patrons and subscribers for making
14560	16440	all of this work possible.
16440	20560	And if you'd like to support the podcast as it runs off patronage alone, then please
20560	22520	find links in the description below.
22520	24920	Otherwise, please enjoy.
24920	29280	So, Mark Andreessen, thanks very much for joining us on Hermitix podcast.
29280	32160	Hey James, thanks for having me.
32160	41040	We are going to be discussing accelerationism, AI, technology, the future, technology is
41040	43040	probably the key one here, I think.
43040	48440	But I want to basically begin with probably something that on the usual podcast you go
48440	52000	on, you probably aren't asked, like a lot of people will know who you are, but in the
52000	54800	sphere that I'm working, people might not.
54800	60240	So just tell us a little bit about yourself and what it is you do before we get started
60240	61240	here.
61240	62240	Yeah.
62240	64800	So I'm probably the polar opposite from your usual guest.
64800	65800	Exactly.
65800	71040	So I'm bringing diversity to your production.
71040	73480	So my background, I'm an engineer.
73480	79840	So I'm a computer programmer, computer science, computer engineer by background.
79840	83360	I was training kind of the old school computer science where they kind of teach you every
83360	87720	layer of the system, including hardware and software.
87720	92320	And then I was a programmer and then an entrepreneur in the 90s.
92320	96200	And probably my main kind of claim to fame is I was sort of president of the creation
96200	100280	of what today you'd consider the internet, sort of the modern consumer internet that
100280	102360	people use.
102360	106840	And so my work first at the University of Illinois and then later at a company I co-founded
106840	113040	called Netscape, sort of popularized the idea of ordinary people being online.
113400	117360	And then helped to build what today you experience as the modern web browser and kind of the
117360	120120	modern internet experience.
120120	127200	And then I was involved kind of through a broad range of Silicon Valley waves over the
127200	132040	course of the next 20 years in the 90s and 2000s, including cloud computing where I started
132040	135640	a company in and social networking I started a company in.
135640	140880	And then in 2009, I started with my long-time business partner, I started a venture capital
140880	145960	firm and our firm, which is called Injuries and Horowitz is now kind of one of the firms
145960	151680	at the center of funding, all of the new generations of technology startups.
151680	157640	And maybe the main thing I kind of underlined there is just technology, quote unquote technology,
157640	163160	high tech, computer technology in particular, kind of used to be, it's always been kind
163160	167400	of interesting and important in the economy for the last 50 years or something.
167400	172520	In the last 15 years, I think a lot of people kind of feel that like technology has really
172520	178120	spread out and it has become integral to many more aspects of life.
178120	182640	And so my firm today finds itself very involved in the application of technology to everything
182640	191240	from education, housing, energy, national defense, national security, as well as kind
191240	196840	of every possible artificial intelligence robotics, kind of every different dimension
197240	199080	of how you might touch technology in your life.
201000	205880	And you picked up on something that will come into the conversation in a couple of questions
205880	210920	time, but this notion of you is basically completely opposite to the majority of guests,
210920	216880	not in a bad way, but often it's a lot of philosophy and which theory and not practice.
216880	221440	And also this notion of technology in relation to either pessimism or optimism.
221440	226880	And this is super, super key, I think, for the ongoing atmosphere of really the West,
226880	228200	of where we're going to end up.
228200	232720	But before we get to these questions, I mean, this is a question I'm slowly phasing out,
232920	236320	but I think it will work for the sake of our conversation, because we're talking more
236320	237560	broadly around themes.
238720	241960	I know you've listened to the podcast before, so it is the Hermitix question.
242240	246120	You can place three thinkers living or dead into a room and listen in on the conversation.
246120	246760	Who do you pick?
247760	253520	Yeah, I think that maybe I'll give you two versions of the answer and then maybe I can combine them.
253520	255640	So there's kind of a timeless answer.
255640	262200	And the timeless answer would be something like Plato or Socrates, Socrates and then Nietzsche.
262200	270160	And then maybe I'd throw in one of your favorite people, Nick Land, I think would be interesting.
270160	276160	The somewhat more applied version of that would be something a lot, and this is sort of maybe
276240	279040	a little bit more topical these days with this movie Oppenheimer that just came out.
279040	285960	But it's like John von Neumann, who was one of the co-inventors of both the atomic bomb
285960	290800	and the computer, Alan Turing, who became famous a few years ago with another movie,
292080	297080	The Imitation Game, and then let's throw in Oppenheimer there also, because those three
297080	300560	guys were sort of present at the creation of what we would consider to be the modern
300560	306120	technological world, including literally those guys were at the center, especially
306160	310160	von Neumann and Turing were at the center of both World War II, the atomic bomb,
311440	316640	the sort of information warfare, the whole kind of decryption kind of phenomenon,
316640	321600	which really a lot of people think one, World War II, along ultimately with the A-bomb,
321600	326720	and then also right precisely at that time with those people, the birth of the computer
326720	330800	and everything that followed. So is that more of a practical room for you or
331680	336080	in terms of like a vision going forward into the future? Or is there something else going
336160	344240	on there between those sort of six figures? Those guys were very, it's almost impossible to
345200	351680	overstate how smart and visionary and far seeing they were like, there's actually the
351680	355440	von Neumann biography came out recently called The Man from the Future, and in anything like
355440	358400	von Neumann is a more interesting character than Oppenheimer in a lot of ways, because he
359040	363680	touched a lot more of these fields. And of the people who knew them that von Neumann was always
363680	366560	considered that he was the smartest of what were called the Martians at that time, right,
366560	371520	which were the sort of group of super geniuses that originated in Hungary in that era.
372560	378720	And so, you know, they were very, very conceptual thinkers. I'll just give you one
378720	383040	example of how conceptual they were, how profoundly smart they were. So they basically
383040	386720	birthed the idea of artificial intelligence right in the middle of the heat of World War II.
386720	390480	Like the minute they created the computer, like they created the computer, right? They created
390480	393600	like the electronic computer, as we know it today, in the heat of World War II. And then
393600	397040	they immediately said, aha, this means we can build electronic brains. And then they immediately
397040	401840	began theorizing and developing designs for artificial intelligence. And in fact, the core
401840	405760	algorithm of artificial intelligence is this idea of neural networks, right, which is this idea of
405760	410080	a computer architecture that sort of is mirrors in some ways the sort of mechanical operation of
410080	415200	the human brain. You know, that was literally an idea from that era in the early 1940s. There was a
415200	421600	paper, two other guys who were in this world wrote a paper in 1943, outlining the theory of
421600	426960	neural networks. And that literally is the same technology. That is the core idea behind
426960	433200	like what you see when you use JGPT today, 80 years later. And so there was a very, very deep
433200	438480	level of intellectual and philosophical, you know, I don't know what it is, like they tapped
438480	442000	into or discovered or developed a very deep well that we're still drawing out of today.
442960	446480	I was gonna, yeah, I was gonna ask that immediately, but you covered it. I mean,
446480	451760	is there any significant changes between AI then and AI now? Or is it really just a matter of
451760	457040	practicality? Like we've got the, we've got more resources and more ability to create it.
458000	461600	Yeah, we're at this fairly shocking moment. So for people who haven't been following this,
461600	464960	basically, it's this, it's one of these amazing things where it's like, there's like this 80
464960	469360	year overnight success that all of a sudden is paying off. And so that it's, you know, it's,
469440	473440	there were, you know, there were 80 years of scholars and researchers and projects
473440	477120	and attempts to build electronic brains. And like every step of the way people thought that
477120	481600	they were super close, you know, there was this famous seminar on the campus of I think it was
481600	485680	Dartmouth University in like 1956, where they got this grant to spend 10 weeks together, they'd
485680	488800	get all the AI scientists together in 1956, because they thought that after that they'd have,
488800	494880	they'd have AI, you know, and turn out they didn't. And so, so it's what, but like it's
494880	499520	starting to work, right? And so when you use ChatGPT today, or you use on the artistic side,
499520	503520	you simply admit journey or stable diffusion, like you're seeing the payoff from that.
505040	508720	I think the way to think about it is it's the deep thinking that took place up front.
509840	514320	It's, and then, you know, just obviously tremendous amount of scientific and technological
514320	517360	thinking and development, you know, and elaboration that took place since then.
518160	522240	But then there's two other kind of key things that are making AI work today that are kind of,
522240	525120	and there's sort of, again, there's sort of a combination of sort of incremental, but also
525120	532240	step function breakthroughs along the way. So one is data. And so just like it turns out a big part
532240	537680	of getting a neural network to work is feeding in enough data. And so, you know, and the analogy
537680	541280	is irresistible, right? It's like, you know, if you want to, if you're trying to educate a student,
541280	546080	right, you want to feed them, you know, and feed them a lot of material in the human world also.
546080	550240	And so it just turns out there's this thing with neural networks and data where, as they say,
550400	555520	you know, quantity has a quality all its own. And you really needed actually the internet to
555520	559120	get to the scale of data. You needed internet scale data, you know, you needed the web to
559120	562720	generate enough text data, you needed like, you know, Google images and YouTube to generate enough
562720	568720	video and imagery to be able to train. So we're kind of getting a payoff from the internet itself,
568720	572640	you know, combined with neural networks. And then the third is the advances in semiconductors.
573920	576960	And, you know, and this is, you know, sort of the famous Moore's Law. But, you know,
577840	581840	this phenomenon that, you know, that kind of we refer to as, you know, quote unquote teaching
581840	587440	sand to think. And so kind of this idea, right, that you can literally convert, you know,
588560	594240	silicon, you know, sand, rocks into, you know, into chips, and then ultimately into brains
595040	599120	is kind of this amazing thing. And actually, as I don't know if you follow this stuff, but as
599120	603600	we're recording right now, there's this like amazing phenomenon happening in the world of
603600	609520	semiconductors and physics right now, which is there's this, we may be, we may be, we may be
609520	612560	right now, we may have just discovered the first room temperature superconductor.
613120	618160	I've been seeing this, but I'm not smart enough. Can you give me a brief overview of why this is
618160	621520	so important? I mean, I'm guessing is this a resource input issue?
622320	626160	So basically, every time you build a circuit today, right, every time you build any kind of
626160	631760	circuit, a wire, a chip, you know, anything like that, an engine, a motor, you know, you have
631760	636080	basically this process. And by the way, this actually relates to the philosophy of acceleration,
636080	640560	as we'll talk about, but you have this sort of thermodynamic process where you're taking in
640560	645360	energy on the one side, right, and then you have a system, right, like a, you know, an electrical
645360	649680	transmission line or a computer chip or something, you have a system that's basically using that
649680	655840	energy to accomplish something. And then that system is inefficient and that system is dumping heat
655840	660880	out the other end. And, you know, and this is why when you use your computer, you know, if you
660880	663760	got, you know, an older laptop computer, you know, the fan turns on at a certain point,
664720	668080	if you have a newer laptop computer, it just starts to get hot, you know, you probably notice
668080	672000	your phone starts to get hot, you know, let, you know, batteries every once in a while do what
672000	675760	they call the cook off, they, you know, they lithium ion batteries will explode, right, like
675760	680000	they're, you're dumping, there's always some, there's, there's always a byproduct of heat,
680000	683440	and therefore, you know, sort of increased entropy kind of coming out the other side of any sort of
683440	688000	electrical or mechanical system. And that's just because with, you know, kind of running energy
688000	693440	through wires of any kind, you just have a level of inefficiency. By the way, the human body does
693440	697120	the same thing, right, like, you know, we take in, you know, energy, and then we, you know, we're
697120	699840	sitting here, you know, we don't feel it, but we're sitting here humming along at, you know,
699840	704240	whatever 98.6 degrees Fahrenheit, you know, significantly higher than room temperature,
704240	708560	because, you know, we're generating our actual biochemical process of life, right,
708560	713360	bioelectrical is generating heat and dumping it out. Anyway, so the idea of the superconductor is
713360	717680	basically think about it in the abstract as a wire that basically transmits information without,
717680	721040	you know, with basically perfect fidelity, you know, perfect conservation of energy
721040	726160	without dumping any heat into the environment. And it turns out that if you could do that,
726160	729840	if you do that at room temperature, then all of a sudden you can have like, you know, basically,
729840	734720	like, you know, incredibly more efficient, you know, kinds of batteries, electrical transmission,
734720	739680	motors, you know, computer chips. And so you can start to think about, for example,
740400	744800	just, you know, an example people talk about is if you, if you, if you cover the Sahara Desert
744800	748960	and solar panels, you know, you could power, you know, basically the entire planet's, you know,
748960	752160	power, you know, energy needs today. The problem is there's no way to transmit that,
753360	756000	you know, transfer that power from the Sahara to the rest of the world
756640	760160	with existing transmission line technology with superconducting transmission lines,
760160	765440	all of a sudden you could, you know, quantum computers, you know, today they exist,
765440	769280	but they're sharply limited because they have to be operated at these, you know, super cool
769280	773520	temperatures, you know, in these very carefully constructed labs, you know, with superconductors
773520	778720	in theory, you have desktop quantum computers, you know, you have levitating trains, you've got,
778720	783840	you know, you just, you have a very broad cross section, you know, you have handheld MRIs,
784560	787440	right, like every doctor, every nurse, you know, has an MRI and they can just, you know,
787440	792560	take a scan wherever they need to, you know, on the fly, you know, and like, like, like the Star Trek,
792560	798240	you know, the, the tricorder, you know, kind of thing. And so anyway, it's fascinating. So,
798240	802080	so, so sitting here today, there's, there's, there's the reports of this, of this breakthrough.
802080	807600	And there are the sort of almost, these almost UFO style videos of, of, of this material levitating,
807600	811680	where it's not supposed to be levitating as a consequence of this breakthrough. And there are
811680	815840	betting markets on scientific progress, and the betting markets, as of this morning, have the odds
815840	819840	of this being a real breakthrough at exactly 50-50. And so, we...
819840	820880	Not the worst odds.
822080	825360	No, but it's, it's funny. If you think about it, it's funny because it's, it's, it's the,
825360	828480	our entire world right now, from a physics standpoint, it's like Schrodinger's cat,
828480	832800	like we live in a, we live, we live sitting here today in a superposition of two worlds,
832800	835600	one in which we now have room temperatures, some conductors, and one of which we don't.
837440	841040	People are, you know, these are radically different potential futures for, for humanity,
841040	845200	right? And so, if it turns out it's true, you know, it's an amazing stuff, function,
845200	848000	breakthrough. If not, it'll, you know, it'll, it'll set us back and we'll, you know, people
848000	852240	will go back to trying to work on it, figure it out. But, you know, but, but between the time
852240	856400	we're recording, between the time we release, we may even find out whether the cat, the, the
856400	861360	superconducting cat, the box is alive or dead. That alive or dead state, I mean, these, these
861360	866160	two separate futures is really something that I, I see, you know, when I was reading your blog,
866160	871520	when I was looking at, uh, effective, effective acceler, accelerationism and accelerationism
871520	876560	we'll get to, but these two futures, I think is the big question that I want to ask you, which is,
876560	881760	because, because you've lived through this time, which is going through the, the optimism of the
881760	885520	90s, especially, you know, you mentioned Nick Lander, the star, I mean, you see that in philosophy,
885600	891280	you see that in technology, see that in the history, this huge, um, so let's call it a cyberpunk
891280	896240	optimism regarding our technological future. And I would say now, I don't know, you know,
896240	901440	whether or not you agree with me, please let me know. We have entered into what land himself
901440	910640	called a slump from the 2000s, like late 2000s, you know, early 2000s onwards. And there seems to be
910640	916320	within the, within the air, a sort of cynicism, a sort of pessimism that we've just ended up in this,
916320	922080	like, place of stagnance. And do you see, I mean, if you agree with me in terms of those two,
922080	926480	two possibilities, do you, I mean, I think I would be right in saying you're an optimist.
926480	933680	Do you see us now re-entering into that, a new phase of optimism regarding technology and regarding
933680	938400	the future? Well, so there's, there's, there's several layers to this question. I would be happy
938480	942880	to kind of go through them. Then we can spend as much time in this as you want. But the, the, the,
942880	946160	the core layer we're talking about, and I totally, by the way, totally acknowledge and, and I think
946160	951920	this is a great topic. And, you know, the, your observations are very real. The core thing that
951920	955760	I would go to, to start with is not kind of the social, political, you know, kind of, you know,
955760	959280	philosophical dimension. The core thing I would go to, to start with is the technological dimension.
960800	964960	In other words, at the substantive level, like, what is the actual rate of technological change
964960	970240	in our world? And, and you'll know, you'll note, I don't know, you'll note that on the, on the social
970240	973920	dimension, we seem to whip back and forth between, oh my God, there's too much change,
973920	977920	and is he stabilizing everything? And then we whip right around to, oh my God, there's not enough
977920	982720	change. And we're stagnant, right? And that's horrible. So, so there's kind of dystopian versions,
982720	986320	you know, there's kind of dystopian mindsets in the air, kind of in, in, in both directions.
987920	992000	So, so anyway, so I would start with kind of the technological kind of substantive layer to it.
992880	996320	And there, you know, the observation, and this is not an original observation on my part,
996320	999760	you know, Peter Thiel and Tyler Cohen, in particular, have gone through this in a lot
999760	1005360	of detail in their work. But, you know, basically, like, if you look at the long arc of technological
1005360	1009200	development over the course of, you know, basic, you know, which, which effectively started with
1009200	1012560	the Enlightenment, right? So you sort of, practically speaking, you're sort of starting
1012560	1017840	around 1700 and projecting forward to today. It's about 300 years worth of what we would
1017840	1022880	consider kind of systematic technological development. You know, it's basically, if you
1022880	1027280	look at kind of that long arc, and then if you basically measure the pace of technological
1027280	1032080	development and applause by saying you actually can measure the pace of technological development
1032080	1038080	in the economy with a metric that economists call productivity growth. And so, and basically,
1038080	1042240	the way that that works is, you know, economic productivity is defined basically as output
1042240	1046480	per unit of input, right? And you can, you know, whatever your inputs are, could be energy, right?
1046480	1051520	It could be, you know, raw materials, you know, whatever you want. And then, you know, output is
1051520	1055280	in, you know, actual, you know, actual output, you know, more cars, more chips, more this,
1055280	1060400	more that, more clothes, more food, more houses. And so, basically, what economists will tell you
1060400	1064720	is the rate of productivity growth in the economy, which they measure annually, basically, is the
1064720	1069920	rate of technological change in the system, right? And so, if technology is paying off, right, if the
1069920	1074640	advances are real, then your economy is able to generate more output with the same inputs.
1075200	1079680	If your technological development is stagnant, then that's not the case. And it's an aggregate
1079680	1084640	measure, but it's a good measure overall. If you look at those statistics, basically, what you find
1084640	1089600	is we had very, we think more recently in the last century, we had very rapid productivity growth in
1089600	1095840	the West, basically, for the first half of the 20th century. So, from the basically, you know,
1095840	1101680	what was called the Second Industrial Revolution, which started around 1880, 1890, through to basically
1101680	1106400	the mid-60s, we had actually a very rapid rate of technological development. And by the way,
1106400	1111840	in that era, right, we got, you know, the car, the interstate highway system, the power grid,
1111840	1117360	telegraph, telephone, radio, television, you know, we got computers, we got, you know,
1117360	1121760	we got like all, you know, all we got, you know, atomic, we got, you know, both atomic weapons
1121760	1126560	and also nuclear power technology, right? And so, there was this tremendous kind of technological
1126560	1131920	surge that took place, you know, in that sort of scholar 1880 to 1960, 1965 kind of period.
1131920	1135280	The productivity growth ran, you know, through that era, two to 4% a year,
1136320	1140000	which, which, and the aggregate is very fast, you know, for the economy overall, like that's,
1140000	1146960	that's a very fast pace of change. Basically, since the mid-60s, early 70s, the rate of productivity
1146960	1152560	growth basically took a sharp deceleration. And so, in the, in the, basically, the 50 years, 52
1152560	1157680	years now that I've been alive, you know, it's, it's been a step lower, it's been 1 or 2% a year,
1157680	1162160	it's been kind of persistently too low relative to what it should be. And, and, you know, I think
1162160	1166720	there's a bunch of possible explanations for that. But I think the most obvious one is that
1167600	1172960	basically the, the world of technology bifurcated in the 70s and 80s into two domains, one domain is
1172960	1176720	the domain of bits, you know, the domain of computers and the internet, where there has been,
1176720	1179920	you know, obviously very rapid technological development, you know, you know, potentially,
1179920	1184800	you know, now culminating in AI. But then there's also the world of atoms. And, you know, the,
1184800	1189200	the diagnosis at least that I would apply is we, we, we essentially outlawed technological
1189200	1193520	development and innovation in the, in the realm of atoms, you know, basically since the 1970s.
1194480	1197760	There are many examples of how we've done this. And, you know, you can look at things like housing
1197760	1201520	policy, and you can kind of see it quite clearly, but also very specifically, you can see it in
1201520	1207280	energy, which is, you know, we discovered nuclear power, right? We discovered a source of, you know,
1207280	1211840	a limited, you know, zero emissions energy that, you know, compared to every other form of energy
1211840	1216320	is like ultra safe, you know, nuclear energy is like, by far the safest form of energy that we
1216320	1221760	know of. And, you know, in the 1970s, we essentially made it illegal, you know, just like totally
1221760	1226640	banned it. And we talked more about that, but like that, that was like a draconian thing that,
1226640	1231600	that, you know, has consequences through to, to the world we live in today. And so, so we live in
1231600	1235600	this, or any, you mentioned cyberpunk, and this is, this is actually kind of the cyberpunk ethos
1235600	1239200	that I think actually reflects something real, which is, you know, if you're in the virtual world,
1239200	1243520	it's like, wow, right? It's like, you know, it's amazing. Like everything is like spectacular.
1243520	1246800	And, and yeah, look, even like a podcast like yours, like, right, would have been, you know,
1246800	1252320	inconceivable 30 years ago, right? And so like information, transmission, communication, coordination,
1252960	1257440	you know, all these things are have taken huge leaps forward. But then the minute we, you know,
1257440	1261360	the minute you get into a car, or the minute you plug something into the wall, right, or the minute
1261360	1266880	you eat food, right, you're still living in the 1950s. And so I think we live in a schizophrenic
1266880	1273200	world with respect to that question. Why then, so you write about this in your blog post on AI,
1273200	1279120	which we'll get to, but you draw in Prometheus, right, this, this consistent historical cycle of
1279120	1282160	when there is a new technology, it's going to destroy us, everything's going to end,
1282160	1286480	it's the worst thing ever, we need to be careful of it, you know, the TV is going to burn your
1286480	1291120	eyeballs out of your sockets, the vacuum cleaner is going to, I don't know, like explode or whatever,
1291120	1297200	but every time there is a, like a cyclic change of a new technological innovation, it's this
1297200	1300960	Promethean thing of where we're pretty terrified of it and we want it to go away. And then eventually
1300960	1304960	we're like, Oh, actually, no, that's pretty helpful. But there seems to be, as you said,
1304960	1309600	there's something that happened in the 1970s, where we just pushed away the atomic world in
1309600	1314480	favor of the bits, you know, which makes sense. But why, I mean, there's probably a lot of
1314480	1320000	governmental reasons for this as well. But why were we so, it seems like a fear, really,
1320000	1327200	the way you talk about it, like why were we so in a way scared to then develop the atomic world
1327200	1333120	in the way we had the bit world? Yeah, so I go start even deeper, I think, which is there's
1333120	1338720	a deep fear in the human psyche, and I think probably in the human animal of new knowledge,
1338720	1343280	like it's even a level like technology is an expression of knowledge, right, like the Greeks
1343280	1347280	right have this term, Techni, which is sort of this, you know, which is where the word technology
1347280	1350000	comes from. But I think the underlying meaning is more like general knowledge.
1351120	1355440	You know, the Christian, you know, the key to the Christian, you know, kind of theology,
1355440	1359600	right, is the, you know, what is, you know, what was the original sin, right, it was eating the
1359600	1364480	apple from the Tree of Knowledge, right, it was, it was mankind, right, mankind learning that,
1364480	1369040	which he was not supposed to learn. And so, you know, the Greeks had the Prometheus myth,
1369040	1374000	the Christians have the snake in the Garden of Eden and the Tree of Knowledge, like there's
1374000	1378800	something very, very deep, like there's, there's an asymmetry, I think, wire deeply in the human
1378800	1384480	brain, right, which is, you know, sort of, you know, fear versus hope, which, which from an
1384480	1387600	evolutionary standpoint, like would make a lot of sense, right, which is like, okay, if you're
1387600	1391680	living in, let's say prehistoric times, you know, in the sort of long evolutionary landscape that
1391680	1398000	we lived in, you know, is new information likely to be good or bad, probably over the sweep of,
1398000	1401360	you know, the billions of years of evolution that we went through, most new information was bad,
1401360	1406000	right, most new information was the predators coming over the hill to kill you. And so, I think
1406000	1411680	there's something like deeply resonant about the idea that new is bad, that, you know, and by the
1411680	1416000	way, look, in the, in the West, like, we probably, you know, we actually, I think, from a historical
1416000	1419600	and maybe comparative standpoint, like we're actually quite enamored by new things as compared
1419600	1423680	to a lot of traditional societies. And so, if anything, we've overcome some of our national
1423680	1428480	instincts on this, but that, that, that impulse is still deep. And then if you go up one level to
1428480	1435760	kind of the social level, you know, I'm quite bought into an explanation on this that was provided,
1435760	1440640	there's a, there was a philosopher of science, historian of science named Elting Morrison
1440640	1445600	at MIT in the, in the first half of the 20th century, who talked about this. And he said,
1445600	1448720	look, you need to think about basically technology intersects with social systems.
1449680	1453600	When a new technology intersects with a social system, basically what it does is it threatens
1453600	1459360	to upend the social order, right. And so, at any given moment in time, you have a social order,
1459360	1464400	right, with status hierarchies, right, and people who are in charge of things. And basically what
1464400	1468560	he says is the social order of any time is basically, you know, in sort of Western sort of
1468560	1472400	modern sort of enlightenment, Western civilization, the social order is a function of the technologies
1472400	1476320	that led up to it, right. And so you have a certain way of organizing the military, you have a certain
1476320	1479520	way of organizing, you know, industrial society, you have a certain way of organizing, you know,
1479520	1484880	political affairs. And they are the consequence of the technologies up to that point. And then
1484880	1489200	you introduce a new technology, and the new technology basically threatens to upend that
1489200	1493440	status hierarchy. And the people who are in power all of a sudden aren't, and there are new people
1493440	1497280	in power. And of course, you know, what is the thing that people will fight the hardest to maintain,
1497280	1501520	you know, as, you know, as their status in the hierarchy. And then he goes through example
1501520	1505840	after example of this throughout history, including this incredible example of the development of the
1505840	1511760	first naval gun that adjusted for the role of a battleship and battle, which increased the firing
1511760	1518000	accuracy of naval guns by like 10x. It was one of the great decisive breakthroughs in modern weaponry.
1519120	1523760	And it still took both the US and the UK British navies 25 years to adopt it.
1525040	1531280	Because the entire command status hierarchy of how naval combat vessels were run and how
1531280	1535600	gunnery systems worked and how tactics and strategy worked for naval battles, like had to be upended
1535600	1540240	with the invention of this new gun. Anyway, and so like he would basically say, you know,
1540240	1545120	essentially, duh, you know, you roll out this new technology, it, you know, it causes people who
1545120	1549680	used to have power and no longer have power, puts new people in power, you know, in modern terms,
1549680	1553280	you know, the language that we would use to describe this as gatekeepers, right? Like so,
1553280	1558640	you know, why is the traditional journalism press so, you know, it just absolutely furious about
1558640	1563040	the internet, right? And it's because like the internet gives right regular people the opportunity
1563040	1566640	to basically be on a, on at least a peer relationship, if not, you know, in the case of
1566640	1571760	somebody like Joe Rogan, a superior relationship, right? And then it's an upending of the status
1571760	1576640	hierarchy. And kind of, you know, the same thing, you know, through, basically, like one of the
1576640	1580320	ways to interpret the story of our time from a social standpoint is all of the gatekeepers who
1580320	1585360	were strong in the 60s and 70s are basically being torn down. Another obvious example, political
1585360	1590640	parties, right? Why are so many Western political parties in a state of some combination of freak
1590640	1594960	out and meltdown right now, right? Well, it's because in an era of radio and television,
1594960	1598320	they were able to broadcast a top down message, and they were able to tell voters basically
1598320	1602400	what to think in the, in the new model voters are deciding what they think based on what they
1602400	1606080	read online. And then they're reflecting that back up and finding their politicians wanting,
1606080	1610480	right? And so therefore, like the re-rise of populism and, you know, sort of the blowing out of,
1610480	1614240	you know, sort of both left-wing and right-wing ideologies, right? The sort of, you know,
1614240	1618160	the center is not holding. And so anyway, that would be another example in Morris's framework.
1619120	1623280	And then I'll just close on this. Morrison has this fast. He says there's this as a consequence
1623280	1627840	to the fact that technology changes social hierarchies. He says there's a predictable
1627840	1632560	three stage process to the reaction to any new technology by the status quo, but basically
1632560	1638560	the people in power at that time. He says, step one is ignore. And so just like pretend it doesn't
1638560	1642880	exist. Which by the way, is actually a pretty good strategy because like most technologies don't
1642880	1646080	upend social orders, like most new technologies don't work at the time that they're first
1646080	1651120	presented. So maybe ignore is actually a rational strategy. Step two is what he calls rational
1651120	1655200	counter argument. And so that's where you get like the laundry list of all the things that are
1655200	1659520	wrong with the new technology, right? And then he says step three is when the name calling begins.
1661840	1666240	This, I mean, I watched a couple of your other interviews recently. And this relates to,
1666240	1671280	I know you've been talking about Nietzsche's master in slavery morality recently. And this
1671360	1675680	seems to tie to that in this notion of Nietzsche and, you know, he does a typical
1675680	1680160	philosophical thing of taking a French word and drawing it out. But Rosentum on, right? Instead
1680160	1686800	of, you know, just having a look at nuclear power and seeing where it would go and allowing that
1686800	1693680	power to unfold within society, you try invert the morals. So you say, well, actually, the good thing
1693680	1698400	to do is because these people don't have the will to power, because they don't have the ability
1698400	1704000	or the engineering skills, I guess in your own case, to like, you know, to utilize the thing,
1704000	1709120	they invert the morals and say, well, actually, the good thing is to do the inverse is to not have
1709120	1714800	it like this is bad. And now that then immediately puts them in the in the good camp. But it seems
1714800	1720640	like, to be honest, it really feels especially with AI and also now with nuclear power, now that,
1720640	1724480	you know, especially in Germany, certain things have been tried. And now it's like, okay, this was
1724480	1729760	a really bad mistake in terms of energy, like the cat's out of the bag. And we like, there's now
1729760	1734800	this force of having to move, you were then talking about the second to second and third stages there.
1734800	1739680	It's almost like, look, with AI, especially the cat's out of the bag, like, we have to move, there's
1739680	1744400	no, there's no choice of like, ignoring or reacting against it. Now you have to deal with it or you
1744400	1748240	don't. Yeah, so let's let's spend a little one more moment on nuclear power and then and then go
1748240	1752720	to AI. So nuclear power is so interesting, because nuclear power is the tell. Like, I always look for
1752720	1755360	like the little signals that people don't really mean what they say, or that they don't, they're
1755360	1759040	not really like they're, you know, they're, they're, they're, they're sort of, you know, moral system
1759040	1763280	doesn't quite line up properly. And so nuclear power is this like amazing, it's this amazing thing,
1763280	1766640	it's like, literally, it's like, okay, you build this thing, it generates power, it basically,
1766640	1771360	it generates a small amount of nuclear waste, it generates steam, but it generates zero emissions,
1771360	1777120	right, zero carbon, right. And so you have this basically, it's amazing phenomenon where you have
1777120	1781360	this, and let's just take them completely as space value, I'm not gonna, this is not me questioning,
1781360	1784080	I'm not going to question carbon emissions or global war, I'm just gonna, I'm gonna assume
1784080	1787520	that everything the environmentalists say about carbon emissions, climate, you know, change all
1787520	1791200	our stuff. Let's assume that that's all totally real. Like, let's just, let's just grant them all
1791200	1796720	that. It's like, okay, well, like, okay, so how could, how can you solve the sort of climate
1796720	1800160	crisis, the carbon emissions crisis, it's like, well, you have the silver bullet technology,
1800160	1805040	you could roll out in the form of nuclear fission today. You could generate a limited power. Richard
1805040	1809760	Nixon, by the way, the, you know, the heavily, heavily condemned Richard Nixon in 1972,
1810480	1813440	you know, proposed something at the time he called project independence.
1814400	1817920	Project independence was going to be the United States building 1000 new civilian nuclear power
1817920	1823200	plants by the year 1980, and cutting the entire US energy grid, including the transportation system,
1823200	1828720	cars, everything, home heating, everything over to nuclear power by 1980, going zero emission
1828720	1832320	in the US economy. And by the way, right, geopolitically removing us from the Middle East,
1833040	1838160	right, right. So no, right, no, Iraq, FK, all that stuff, like just completely unnecessary,
1838160	1843920	right. And, you know, you'll note that like project independence did not, did not happen,
1844640	1848000	right, like we don't, we don't live in that world today. And so it's like, okay, you've got this
1848000	1853040	like crisis, you've got this like silver bowl solution to for it, and you very deliberately
1853040	1858160	have chosen to not adopt that solution. And it's like, and there's this actually very interesting
1858160	1861600	split in the environmental movement today. And it's really kind of, you know, I think kind of
1861600	1866160	bizarre. And it's like a 99 to one split, you asked like 99% of environmental activists about
1866160	1870720	nuclear power, they just just sort of categorically dismiss it was, of course, that's not an option.
1870720	1875520	You do have this kind of radical fringe with people like Stuart Brand, who are like, basically
1875520	1878960	now pointing out that it is, it is a silver bullet answer, but most of them are saying, no,
1878960	1882720	it's not an answer. And it's like, okay, well, why are they doing that? It's like, well, like,
1882720	1885920	what is it that they're saying that they want to do? And what they're saying they want to do
1885920	1889920	is what they call, you know, degrowth, right. And so they want to decarbonize the economy,
1889920	1894000	they want to deenergize the economy, they want to degrow the economy. And then, you know, when
1894000	1898000	you get down to it, and you ask them a very, you know, specific question about the implications of
1898000	1901760	this, you know, basically what you find is the general model is they want to reduce the human
1901760	1905520	population on the planet to about 500 million people. You know, it's kind of the answer that
1905520	1910320	they ultimately come down to. And so ultimately, the, you know, the big agenda is to is to reduce
1910320	1914720	the human, you know, basically the human herd, you know, quite sharply. And, you know, they kind
1914720	1917600	of dance around this a little bit, but when they when they really get down to it, this is what they
1917600	1920720	talk about. And of course, you know, Paul Ehrlich, you know, is kind of one of the kind of famous
1920720	1924400	icons of this, he's been talking about this for decades. I think it was Jane Goodall, who used
1924400	1930480	the 500, you know, million, you know, number recently in public. And so, and so, and so then
1930480	1935280	you got this kind of very interesting, you know, technological philosophical moral question, which
1935280	1939600	is like, well, what, what is the goal here, right, is the goal to like solve climate change, or is
1939600	1944640	the goal to like depopulate the planet, right. And to the extent that like free unlimited power,
1945200	1948720	right, would interfere with, you know, to the extent that that's a problem, the problem it
1948720	1953760	would be as if the actual agenda is to depopulate the planet. And like, I would like this to not
1953760	1957840	be the case. Like, I think, you know, again, take taking everything else that they say at face value,
1957840	1960800	you'd like to solve carbon emissions and climate change and everything else. But like,
1961440	1965040	you know, like, I think you, you know, you might also say you want a planet in which there are
1965040	1969040	not only 8 billion people, but maybe, you know, maybe people are good, right. Maybe you're actually
1969040	1973440	should have 20 billion or 50 billion people. And we have the technology to do that. And we're
1973440	1979440	choosing not to do it. So, so, so, so this is the thing, like this gets into these very deep
1979440	1984000	questions, right, to your point of like, okay, very deep questions about morality. And like,
1984000	1988800	how did we maneuver or, you know, like per nature, like how did we reverse ourselves into a situation
1988800	1993360	where we're actually arguing against human life. And of course, and this is we'll get to it, but
1993360	1996560	this, this of course is then, you know, a big part of the origin of the idea of effective
1996560	2001840	accelerationism, which is basically new, like let's go sharply in the other direction. Oh, and
2001840	2007280	then yeah, so AI, yeah, AI is playing out much the same way as already playing out the same way.
2007280	2011360	And here you've got this like just incredible phenomenon happening where we, we, you know,
2011360	2014720	it looks like we have a key breakthrough to basically increase the level of intelligence,
2014720	2019040	you know, basically all throughout society and around the world, you know, through, you know,
2019040	2023200	basically for the first time, you know, directly applying your general intelligence to the world.
2024720	2029360	And, you know, there is this like incredibly basically aggressive movement that is actually
2029440	2035520	having tangible impact today in the halls of power in Washington DC and in the EU and other places,
2036080	2039120	you know, that is seeking to stop and reverse it, you know, as aggressively as they possibly can.
2039840	2044320	And so we're kind of, we're going through, we're going through, I would say, a suddenly accelerated
2044320	2048320	and very sharp and aggressive version of exactly what happened with nuclear power happening with
2048320	2053520	AI right now. I mean, this is the thing that can, can, well, there's two questions because
2053520	2059120	on your blog, you, it's really refreshing to see you, you're pretty to the point when you say,
2059120	2065440	look, AI is code, it's code written by people, by human beings on computers developed by human
2065440	2071040	beings, you know, like we're in control, you're not of this, I think there was, you know, Musk
2071040	2075920	signed a big thing where like, you know, 1000 people signed this thing to say like, we need to hold
2075920	2081280	this the whole Rocco's Basilisk AI is going to be terminated to come in and blowing us up with
2081280	2085280	robots, etc. So it's going to kill us all. You're very much like, no, this is code, this is just
2085280	2091040	an intelligence for us to use. Now that's one question, you know, I guess, why isn't AI going
2091040	2094480	to kill us all? And I know you've spoken about that a lot. So that answer can be brief. But
2094480	2101200	secondly, this whole idea of trying to reverse it, to me, it seems inherent within AI as a thing
2101200	2106800	that it wants, you know, it's the cats out the back, you can't like once it's here, you, you,
2106880	2113040	outside of really draconian measures, you can't because how do you how do you hold an
2113040	2118560	intelligence which is growing, right? Well, except, you know, they did stall at nuclear power,
2118560	2124400	right? So, right, like so they did, like it worked. So why did project independence not happen?
2124400	2128240	Why do we not have like, you know, unlimited nuclear power today? You know, the reason is
2128240	2131920	because it was it was blocked by the by the political system, right? And so, so, you know,
2131920	2135040	Richard Nixon, who I mentioned, you know, proposed this, he also created the Environmental
2135040	2139280	Protection Agency and the Nuclear Regulatory Commission. You know, the nuclear, it's actually
2139280	2144800	that this actually been a big week. The first new nuclear power plant design, the first newly
2144800	2149360	designed nuclear power plant, in the last 50 years, just went online in Georgia, you know,
2150400	2154480	$20 billion over budget and you know, it's got it's a story of its own, but at least we got one
2154480	2158800	online. It's the first new nuclear power plant design ever authorized by the Nuclear Regulatory
2158800	2164720	Commission, says Nixon created that commission, right? And so, so, so we put in place a regulatory
2164720	2169360	regime around nuclear power in the 1970s that, you know, all but made it impossible. By the way,
2169360	2172080	you alluded to the Germany thing earlier, I'll just touch on that for a second. So,
2172720	2175920	you know, that, you know, you've, I'm sure you've heard of the idea of the precautionary principle,
2175920	2180160	right? Right, which is this, this idea that basically scientists and technologists have a
2180160	2184640	moral obligation to think through all the possible negative consequences of a new technology before
2184640	2188240	it's rolled out. The precautionary principle, right, the precautionary principle, and we could
2188240	2192160	talk about that, including whether scientists and technologists are actually qualified to do that.
2193120	2198960	But, you know, this was also a central theme of Oppenheimer, but the precautionary principle
2198960	2203120	was invented by the German Greens in the 1970s, and it was prevented specifically to stop nuclear
2203120	2209360	power. And, you know, it is just amazing, we're sitting here in 2023, and there's this, you know,
2209360	2213680	where we effectively, we in the West are effectively at this, you know, at war with Russia,
2214960	2219280	right? And, you know, it's a proxy war right now that, you know, hopefully doesn't turn into a real
2219280	2224160	war, but who knows, you know, the proxy wars have a, you know, have a disconcerting, you know,
2224160	2230240	pattern of spilling over into becoming real wars. And, you know, a lot of this is, it's a tale of
2230240	2236880	energy. And, you know, basically the Russian economy, you know, is like 70% energy exports,
2236880	2242080	right, oil and gas exports. The major buyer of that energy historically has been Europe and
2242080	2247760	specifically Germany. You know, Europe and Germany specifically essentially have funded the Russian
2247760	2252320	state, the Putin state, you know, and that funding is what basically built and sustains
2252320	2257840	their military engine, which is what they've used to invade Ukraine, right? And so it's this like,
2258560	2262720	like, there's this counterfactual, right, where the German Greens did not do what they did in
2262720	2267120	the 1970s, nuclear power was not blocked, you know, Germany and France and the rest of Europe
2267120	2271120	today is like fully energy independent running on nuclear power, you know, the Russia state,
2271120	2275360	it would be greatly weakened because the value of their exports would be, you know, enormously
2275360	2280480	diminished. And they would not have the wherewithal to invade other countries or to threaten
2280480	2286640	Europe. And so like, these decisions have like real consequences. And, you know, these people,
2287680	2292560	use the pejorative sense, like they are so confident that they can step into these, you know,
2292560	2295520	debates, you know, kind of questions around, you know, new technologies and how they should be
2295520	2299040	applied and what the consequences are, they can step in and they can use the political
2299040	2303280	machine to basically throw sand in the gears and stop these things from happening. So, so like,
2303280	2307440	AI, this is what's happening right now. So like, you know, in the, in the sort of, you know,
2307440	2311120	theoretical position where AI is kind of this, you know, potentially runaway thing, then, right,
2311120	2315360	maybe it can be constrained, like, in the real world, it very much can be constrained. And
2315360	2319520	the reason it can be constrained in the real world is because it uses physical resources,
2319520	2325760	right? It has a physical, it has a physical layer to it. And that layer is energy usage.
2326640	2332160	And that layer is chips. And that layer is, you know, telecom bandwidth. And that layer is data
2332160	2338080	centers, physical data centers, right? And so, and that layer is like, you know, by the way,
2338080	2341760	that layer also includes the actual technologists, like working in the field, and their ability to
2341760	2346800	actually do what they do. And there are, you know, a very large number of sort of control points and
2346800	2351840	pressure points that, you know, the state can put on those layers to prevent them from being used
2351840	2357600	for whatever it wants to prevent. And, you know, and look, the EU is on the verge, the EU has this
2357600	2361600	like anti AI bill that it looks like is going to pass that is like extremely draconian and may
2361600	2366720	result in Europe not even having an AI industry and may result in, you know, American AI companies
2366720	2371040	not even operating in Europe. And then in the US, we have a very kind of similar push happening is,
2371040	2377120	you know, the sort of ant, what I would describe as the anti AI zealots are, you know, they are,
2377120	2381360	they are in the White House today, right, arguing that, you know, this is bad, it should be stopped.
2382480	2386640	And it's like, you know, it's, it's, it's amazing because it's like, how many times are we going
2386640	2390720	to like run through this loop? How many times are we going to like repeat history here? How,
2390720	2394560	how many times are we going to be kind of self defeating like this? And like apparently the,
2394560	2397040	the impulse to be self defeating, we have not worked it out of our system.
2398720	2402960	You don't want to be self defeating them. I mean, let's move into this peculiar four letters,
2402960	2407680	which is found at the moment at the end of your Twitter name and the end floating around Twitter,
2407680	2414640	mostly e slash act or effective accelerationism. And this like, this is just beautiful to me.
2414640	2418480	It's like the, the acceleration is Renaissance. I've been set talking about it in that way. I
2418480	2422640	don't want to gatekeep it too much, but you know, I wrote my master's thesis on accelerationism,
2422640	2426480	like I love it. I love talking about it. You don't want any of this holding back. You don't
2426480	2430800	want to hold anything back. You want to accelerate. So firstly, I mean, there's two questions there.
2431440	2436320	What is it for you to accelerate and what is effective accelerationism?
2437600	2441360	Yeah. So let me, let me just say where that, where it came from, I'll reverse the second one
2441360	2445840	first and then go to the broader topic. So, so, so it's a, it's a combination. There's, there's,
2445840	2449040	there's, you know, kind of two, two words there, effective and accelerationism. So the, you know,
2449040	2452960	the acceleration, accelerationism part of it is obviously building on what you've talked about
2452960	2456960	and what Nick Landon and others have talked about for a long time. And of course, as you,
2456960	2460000	as you've talked about, there's, there's all these different versions of accelerationism. And so this
2460000	2463440	is, this is, you know, proposing one that, you know, it's this, this one is like the closest to
2463440	2466560	what you would call right, right accelerationism, although, you know, maybe without some of the
2466560	2472080	political overtones. And so there is that component. There's also the effective part of it. And the
2472160	2476400	effective part of it, it's sort of a half humorous reference, obviously, to effective altruism.
2478800	2481120	And it's a little bit tongue in cheek, because it's like, of course, if you're going to have a
2481120	2485120	philosophy, of course, you would like it to be effective. But, you know, but, but also look
2485120	2491840	like EAC is like very much like EAC's enemy, right, the oppositional force that the thing that EAC was
2491840	2497680	sort of formed to fight is actually, you know, specifically effective altruism. Right. And so
2498400	2503200	it's also like, you also sort of, you know, use that term to the term effective to kind of kind
2503200	2508880	of make that point, like this is in that world. And this is opposed to that. And, and, and the
2508880	2514320	reason why like this is happening now, like the reason why the concept of effective accelerationism,
2514320	2517280	you know, has kind of come into being. And by the way, that, you know, the people, this is not
2517280	2522080	originally my formulation, this is, this is, there's, you know, kind of ultra smart Twitter
2522080	2529680	characters, who I think are still mostly operating under assumed names. But there's Beth Jesus,
2529680	2534880	and Bayes Lord are the two, the two, two of them that I know. And they're, you know, these are
2534880	2540000	like top and Silicon Valley, you know, engineers, scientists, technologists. But, you know, at least
2540000	2545360	for now, they're operating kind of under undercover pseudonym. So the reason this is happening now
2545360	2549840	is because of what I, what I was describing earlier with AI, which is you have this, you have this
2549840	2553680	other movement, you have this movement of what's sort of called sometimes it's used different
2553680	2560800	terms AI risk, AI safety, AI alignment. Sometimes you'll hear the term X risk. You know, sometimes,
2560800	2564800	and then this is sort of directly attached. This is all part of the, you know, EA world,
2564800	2570240	the effective altruism world. And then, you know, the central characters of this other world are,
2570240	2575040	you know, Nick Bostrom, Elisir Yadkowski, you know, the open philanthropy organization.
2575040	2579600	And, you know, a bunch of these, a bunch of these kind of, you know, the sort of the AI,
2579600	2585360	what we call the AI doomers running around, like the AI doomer movement is basically a
2585360	2590640	part and parcel with the effective altruism movement. And, you know, AI existential risk has
2590640	2594560	always been kind of the boogeyman of effective altruism, kind of going back, you know, over the
2594560	2600320	20 year development of EA. And so anyway, that that EA movement is the movement, by the way,
2600320	2604720	with lavish funding by like EA billionaires, which is which is part of the problem, by the way,
2604720	2611280	who made all their money in tech, which is also amazing. But, you know, so you've got this funding
2611280	2616160	complex, you've got this EA movement, you've got this attached AI risk safety movement,
2616160	2622080	and now you've got like active lobbying, you know, sort of anti AI PR campaign. And so anyway,
2622080	2626720	so effective effective acceleration is intended to be the polar opposite of that, it's intended to
2626720	2632560	be the, you know, to head boldly and firmly and strongly and confidently into the into the future.
2633520	2638320	You know, it's like, why, you know, why, why this form of positive accelerationism, you know,
2638320	2642800	it's, there's a couple different layers of it. The founders of the, of the concept of the act
2642800	2646000	have a thermodynamic, you know, kind of thing, which, which we could talk about, but it's kind
2646000	2650560	of one layer down from our operate. The layer operate is more at the level of engineering. And
2650560	2654480	when I think about it, I think in terms of essentially fundamentally of material conditions.
2654480	2661680	So human flourishing, quality of life, standard of living of human beings on earth. And back to
2661680	2666320	that concept of productivity growth, you know, the application of technology, to be able to
2666320	2670640	cause the economy to be more productive and therefore cause more material wealth, higher
2670640	2674720	levels of material welfare, you know, for people all over the world, by the way, also with reduced
2674720	2679520	inputs, right. And so not just greater levels of development and greater levels of advance,
2679520	2684400	but also greater levels of efficiency. And the nature of technology as a lever on the physical
2684400	2687680	world is you can have your cake and eat it too, you can get higher levels of output with lower
2687680	2690400	levels of input. And the result of that is a much higher standard of living. So,
2690960	2696400	so I kind of adopt my, my philosophical grounding is sort of, you know, I don't know if I call it
2696400	2701440	like a positive materialism or something, you know, which is like, I think the thing that we,
2701440	2704560	the thing that the technology industry does best is improve material quality of life.
2705840	2709440	I think that we should accelerate as hard into that as we possibly can. I think the quote,
2709440	2716720	unquote risks around that are greatly exaggerated, if not, if not false. And, and, you know, I think
2716720	2720480	the forces against basically technological progress, you know, they're like the environmental
2720480	2724320	movement I described, you know, they're fundamentally sort of at some deep level,
2724320	2728800	they're sort of anti-human, you know, they want fewer people and they want a lower quality
2728800	2731120	living on earth. And like, I just, I very much disagree with both of those.
2732160	2737280	And what is this at the thermodynamic level? Is this, is this the, you know, we are ultimate
2737280	2743440	enemy is entropy? So there's, there's a thermodynamic part gets complicated. And this is
2743440	2746640	not my, my field. So there's, there's other people that you should probably have on to talk
2746640	2751600	about this, but the effect of accelerationism version of the thermodynamic thing is, is based
2751600	2757040	on the work of this physicist named Jeremy England, who is this very interesting character
2757600	2765040	actually, he's actually trained by one of my partners. And is now basically, he's an MIT,
2765040	2769760	you know, physicist, you know, biologist, and by the way, and also by the way, interesting guy,
2769760	2772800	I don't know him, but a very interesting guy from the distance, he's also a trained rabbi.
2774080	2778640	And so he's an interesting cat. And so he basically has this theory that basically,
2779200	2784080	basically it's, it's sort of life is the direct result life, life, like the phenomenon of life
2784080	2789440	itself is a direct consequence of thermodynamics. And, you know, the way he describes it is
2789440	2795200	basically, basically, if you take, you know, basically the universe with a level of energy
2795200	2800080	that's washing around and raw materials, and you sort of apply kind of natural selection at a very
2800080	2805120	deep level, you know, you know, even at the level of just like the formation of materials,
2805120	2809600	like on a planet or something, you basically have this thing where basically a matter wants
2809600	2815040	to organize itself into states where it's able to absorb energy and achieve higher levels of
2815040	2820560	structure. And so you have absorption of energy, you have achievement of higher levels of structure,
2820560	2824480	in the case of organic life, that's, you know, starts with our basic RNA, and then kind of works
2824480	2829200	this way up to, you know, full living systems. And then on the other side of that, as we talked
2829200	2833520	about before, on the other side of that is your, the result of that is your sort of your dumping
2833520	2838720	heat, which is to say entropy, you know, kind of out into the broader system. And so it's almost
2838720	2844000	like saying the second law of thermodynamics has an upside, right, which is basically, yes, entropy
2844000	2849040	in the universe is increasing over time, but a lot of that increases the result of structures
2849040	2854640	forming that are basically absorbing energy and then exporting entropy. And one form of that
2854640	2859760	structure is actually life. And this, and this is actually a thermodynamic, you know,
2859760	2864080	biomechanical, bioelectrical kind of explanation of actually how organic life works. Like this is
2864080	2868480	what we are, we are machines for gathering energy, you know, forming increasingly, you know, complicated
2868480	2873600	biological machines, replicating those machines, right. And of course, you know, he talks about
2873600	2877280	like, you know, natural selection, like it's not surprising that natural selection is so oriented
2877280	2880800	around replication, right, because replication is the easiest way to generate more structure.
2881440	2885920	Right. Like replication is the way that a system that is basically in business to
2885920	2889200	generate structure, it's the way that it can most efficiently generate more structure.
2890480	2896400	And so anyway, basically, the universe wants us to basically be alive. The universe wants us to
2896400	2902240	become more sophisticated. You know, the universe wants us to replicate. You know, the universe
2902240	2906160	feeds us and, you know, an essentially a limited amount of energy and raw materials with which to
2906160	2912080	do that. You know, yes, we dump entropy out the other side, but we get structure and life,
2912080	2914400	you know, to basically, to basically compensate for that.
2915360	2920160	The universe is a, the universe is pronatalist and kind of Nietzsche in there as well.
2920960	2925600	Yeah, exactly. 100%. Yeah. So anyway, so that's, that's the, that's the thermodynamic,
2925600	2930240	that's the thermodynamic underpins of effective accelerationism. The people who have encountered
2930240	2934880	effective acceleration, effective accelerationism, some of that get very, some of them get very
2934880	2938720	deeply into that. And there's a very deep kind of well there to, to draw from this guy, Jeremy
2938720	2942080	England has a book out. Actually, you'll appreciate this. This guy, Jeremy England has a book out
2942080	2947120	and the title of the book is something like every life is on fire. And it's actually funny,
2947120	2951040	because it's like, if you read Heraclitus, you're like, Oh my God, you know, he saw it.
2952480	2957040	Right. Like, it's like, there's something very, very deep going on here with this sort of
2957040	2961920	intersection of energy life. But so he's got this book out, which apparently is quite good.
2962880	2966640	And so some people in effective acceleration kind of, kind of go deep. There's a tongue-in-cheek
2966640	2970800	reference to the so-called thermodynamic God, right, which is not, you know, which is not a
2970800	2974800	literal, you know, religious, in the literal religious sense, like a, you know, sort of
2974800	2978640	a conscious God or a sentient God, but more of this, this idea that the universe is, is, is,
2978640	2982960	is sort of designed to express itself in the forms, you know, basically in higher and higher
2982960	2987360	forms of life. Yeah, to your point, like there's an obviously direct niche in connection.
2987680	2992800	And, you know, so maybe, maybe he saw a lot of this too. You know, and obviously he, you know,
2992800	2996480	he was obviously writing and thinking at the same time Darwin was figuring a lot of this out on the,
2996480	3000960	on the natural selection evolution side. Yeah, so there's that. But, but having said that,
3000960	3006160	like, like I said, my take on it is more, you know, I find that stuff fascinating. I'm more
3006160	3009760	naturally inclined as an engineer, more naturally inclined towards the material side.
3010640	3014640	And so I just more naturally think in terms of the, the social systems and the technological
3014640	3019360	development and the impact on, on, on, on material quality of life. And so I think you
3019360	3023280	can also just take it at that level and not, not, not have to get all the way down into thermodynamics
3023280	3027120	if you don't want to. I mean, there's an odd, I mean, yeah, drawing it down to this level of
3027120	3030640	engineering, well, not down to, but just to this level of engineering, there's this odd
3030640	3034720	learned helplessness. And I mean, just to take the two examples we've given so far. So, and,
3034720	3039680	you know, they work quite well actually nuclear energy on the atomic side and AI on the bit side
3039760	3045600	of things, virtual, I guess, virtual reality and reality. You posted this really interesting
3045600	3050400	essay on your, on your blog about availability cascades, which is about basically, in short,
3050400	3058240	if I'm getting this right, this idea of why are so many people interested in this thing or this
3058240	3064080	view of whatever the, the opinion or the idea is that's floating around. And it seems on both of
3064080	3069680	those, both nuclear energy and AI, we have that same opinion, which is like, mimetically infected
3069680	3073920	culture of a sort of learned helplessness, like, oh, no, you know, we've already spoken about this
3073920	3077600	a bit, but like, oh, no, we need to get rid of this, we can't deal with this. But it seems,
3077600	3081680	do you think on the engineering side of things, and I guess it overlaps also into the social in
3081680	3088720	terms of how you engineer and how you promote these ideas socially as, as tools, as things that
3088800	3095520	people use is an attempt to like, invert that availability cascade and like, try to like,
3097040	3103360	begin some mimesis on the side of like, it's okay to want a better quality of living, it's okay to
3103360	3109200	want to grow, it's okay to want energy, like, you don't have to be almost like submissive to,
3109200	3114800	to whatever this strange, self-defeating learned helplessness is that we have in terms of
3114880	3120880	technology and our like, our like weird allegiance to just this, this stagnant comfort that we've
3120880	3124960	had for too long. Yeah, that's, that's, that's right. That's exactly right. And like I said,
3124960	3128160	like we said, like we talked about earlier, like, there's, there's this, I think there's a natural
3128160	3133040	human impulse deeply wired into like the limbic system or something, which is basically, right,
3133040	3138160	fear over hope, right? You know, like, what's most likely to come over the ridge, right,
3138160	3141520	a sabre to tiger to eat you or like something warm and cuddly that wants to be your friend,
3141520	3146080	right? I guess a quack or something like that, right? So, right, it's, it's probably the tiger,
3146080	3149680	right? And you know, there's, there's a sort of, you know, false positive, false negative,
3149680	3152960	right, two ways of making mistakes. And you definitely, from an evolutionary standpoint,
3152960	3157040	want to err in the direction of being, you know, more, you know, more, you want to overestimate
3157040	3163600	the rate of Cybertooth Tigers, right, to, to, to survive. So, so, so that, that impulses deep.
3163600	3166720	Yeah. But then, you know, what we have is, you know, we have, we have sentience, we have the,
3166720	3170240	you know, the, we're not just limbic systems anymore, we have a, we have the ability to control
3170240	3174080	environment, the ability to build tools, we're not afraid to save the two tigers anymore.
3175200	3179920	And so, yeah, we have the ability to shape our world. You know, we develop rationality and
3179920	3183520	the enlightenment and science and technology and markets and everything else to be able to control
3183520	3188240	the world, you know, to our benefit. And so, we, you know, we don't, we don't have to live
3188240	3193760	cowering in fear anymore, you know, as much as, or as much as that might be like grimly satisfying,
3193760	3197680	like we don't actually have to do that. And there's actually a, you know, very, there are many,
3197680	3200880	many good reasons over the last 300 years to believe that, you know, there's, there's a much
3200880	3205680	better way to live. Yeah, but look, somebody has to say, you know, somebody has to actually say that.
3206560	3212560	And then look, I think the other part is, I think there's a big divide. I think there's a big divide
3212560	3217280	between, I'll pull up my, my Burnham on this a little bit is, is a big divide on this stuff
3217280	3222400	between what you describe as the elites and the masses that has turned out to be pretty interesting.
3222400	3227200	So the, the, I would say this problem, this problem of fear of technology,
3228000	3232400	or hatred of technology or desire to stop technology, I think it's primarily a phenomenon of the
3232400	3238640	elites. I actually don't think it's particularly shared by the masses. And it just seems like
3238640	3243360	I just take AI as an obvious example. One of the amazing things about AI is it's like freely
3243360	3247840	available for use by everybody in the world right now today, fully state of the art, like the best
3247840	3254800	AI in the world is on, you know, websites from open AI and Google and Microsoft. And you can go
3254800	3259200	on there and you can use it for free today. And people, you know, hundreds, hundred, already
3259200	3262000	a hundred, 200 million people, something like that around the world are already doing this,
3262000	3266560	right? And if you talk to anybody who's, you know, if you talk to any teacher, right, you know,
3266560	3269520	you know, they'll already tell you they've got students using chat GPD to write essays and so
3269520	3275440	forth. Right. And so you've got this amazing thing where, you know, like the internet before it
3275440	3280080	and like the personal computer before it and like the smartphone before it, AI is, it's like
3280080	3284880	immediately democratized, right? Like it's immediately available in its full state of the art
3284880	3290560	version. Like there's no more advanced version of like GPT that I can buy for a million dollars than
3290560	3296400	you can get for free or by paying 20 bucks for the upgraded version on the open AI website.
3296400	3302400	Like the state of the art stuff is fully available for free. And so you have people all over the
3302400	3305440	world. And this is one of my, this would be a source of optimism that the AI doomers are going
3305440	3309040	to lose almost by definition, right? Is you have people all over the world who are just already
3309040	3312400	using this and they're getting, you know, great value out of it in their daily lives. They love
3312400	3316000	it. They're having a huge amount of fun with it. You know, it's great. They're good, you know,
3316000	3319200	making new art and they're, you know, doing all kinds of, you know, asking all kinds of things
3319200	3322400	and it's helping them in their jobs and in school and everything else and they love it. So,
3322960	3327600	so I think there's this thing where like, I actually think that what we, what we're actually
3327680	3331200	talking about from a social standpoint is basically essentially a corrupt elite,
3332240	3336640	a corrupt oligarchic elite that basically has been in a position of gatekeeping power,
3336640	3341600	you know, for, you know, basically in its modern form for 60 years. And every new technology
3341600	3346400	development comes along as a threat to that. And back to the Morrison thing like that,
3346400	3350720	that's why they hate and fear new technology. You know, they would very much like to control it.
3350720	3354000	You know, it's like social media, like they're all just like completely furious about social
3354000	3357280	media, but like, you know, 3 billion people use social media every day and they love it.
3358320	3362320	And so it's only the elites that are constantly kind of raging against it. The problem is,
3362320	3366240	the elites are actually in charge of right from a, from a formal, you know, government,
3366240	3370560	like they actually have the ability to write laws. By the way, you also see this in polls.
3371280	3376560	If you pull on, like there's two, two, two very interesting kind of phenomena,
3376560	3380720	phenomena kind of unfolding if you do these broad based polls on trust in institutions.
3381200	3385760	And there's organizations Gallup in particular, and then there's another organization called Edelman
3385760	3389600	that does these polls every year of basically essentially, they pull regular people and the
3389600	3393200	question is like, which institutions do you trust? And that the institutions here includes
3393200	3397200	everything from the military to, you know, religion to schools to government to, you know,
3397200	3402480	journalism to, you know, big companies, big tech, and so forth, small business. And basically,
3402480	3408880	the two big themes you see in those polls is one is ordinary people trust in institutions,
3408880	3414000	trust in any sort of centralized gatekeeping function is been in basically secular decline
3414000	3417680	basically since the 1970s, corresponding to the period we've been, we've been talking about.
3418720	3422400	And so generally, people as a whole have kind of had it with the gatekeepers,
3423680	3426880	which is very interesting. And by the way, that phenomenon, actually, the beginning of that
3426880	3432480	predates the internet and social media. And so that traces back to the early 70s. And so I think
3432480	3437360	that which I think is not an accident. It was just like, it's where the current regime basically
3437360	3442800	essentially took control. And then the other thing that's so striking is that, you know, although
3442800	3447200	you can you can sit and read the news all day long, and where they just like hate on tech
3447200	3452160	companies all day long, if you do the poll of, you know, basically businesses by category,
3452160	3457520	tech polls by far at the top. And so again, ordinary people are just like, wow, my iPhone's
3457520	3462960	pretty cool. I kind of like it. And the strategy PT thing seems really nifty. And so I do think
3462960	3468240	there's this weird, like I do think there is this this aspect of this where like, it's a cliche to
3468240	3470560	say the elites are out of touch, of course, the elites are out of touch, the elites are always
3470560	3474400	out of touch. But like, it seems like the elites are particularly out of touch right now, including
3474400	3479920	on this issue. And another way kind of through this not whole is, you know, they may just simply
3479920	3485440	discredit themselves. Like the EU is a great example, the EU may pass this anti AI law, and the
3485440	3490720	population of Europe might just be like, what's the hell? Right. And so that that's that would be
3490800	3496160	another white pill against what otherwise looks like a deep kind of drive in our society for
3496160	3501360	stagnation. It would also be really strange to try and find a way to like define AI in that sense,
3501360	3506320	because it's not like we haven't been, we haven't been using it in a minor form before all this
3506320	3510080	for a while, right? So I don't know how they'd go about defining that in that way.
3511040	3516800	Yes. So, yes. So do you ban linear algebra? Right? Do you ban linear algebra? And it's
3516800	3519440	actually really funny, because I don't know if you know this, there actually is a push under
3519440	3522880	way to quote unquote ban algebra. And it's literally in California, there's a big push
3522880	3527440	underway to drive it out of the schools in California. So there's a big push first to
3527440	3530320	start with a push to drive calculus out of the schools in California, and now it's extended
3530320	3534800	to drive algebra out. And of course, this is being done under the so called rubric of equity,
3534800	3539040	right? Because it turns out, you know, test scores for advanced math, you know, vary by,
3539600	3545200	you know, vary by group. And so, you know, there's this weird thing where like in California,
3545200	3549360	we're trying to push algebra out of the school. In Washington, we're trying to push algebra like
3549360	3554000	out of tech. Like the whole thing is, and this is where I get like really, you know, this is
3554000	3557200	where I start to get emotional, because it's like really like we spent, you know, 500 years
3557200	3560320	climbing our way out of, you know, primitivism and getting to the point where we have like
3560320	3565120	advanced science and math, and we're literally going to try to ban it. I was involved, I was
3565120	3568800	involved, if you remember this, there was actually a similar push like this, there was a push in
3568800	3577600	the 1990s to ban cryptography, right, to ban the idea of codes, right, ciphers, right. And as you
3577600	3582720	probably know, like codes and ciphers are just math, like all they are is math, right. And there was
3582720	3586240	a move in the 1990s where people who thought that cryptography, obviously, you know, there's all
3586240	3590400	these anti-cryptography arguments, because like bad guys can use it to hide and so forth. And so,
3590400	3593520	there was this like concerted effort by the US government and other Western governments to
3593520	3597280	ban cryptography in the 90s. And it took us years to fight and defeat that. And I was like, okay,
3597280	3601360	that was so stupid, that will certainly never happen again. And like we're literally back at
3601360	3607920	trying to ban math again. Well, that does lead me just to the final question here,
3609040	3612880	which is to do with the future. I mean, whether or not it's your optimistic,
3612880	3616480	pessimistic in relation to, you know, I guess it would draw in on what we've just been talking
3616480	3620800	about there. How do you envision the short term future, which I've put down here like 10 to 50
3620800	3629680	years, and then how do you, what do you foresee for the year 3000 AD? Oh boy. So, I should start
3629680	3635200	by saying I'm not a utopian. So, you know, we talked a little bit early about kind of these
3635200	3639360	impulses, the kind of dry people to these kind of extreme points of view. Like the way I think
3639360	3645120	about it is like there's a natural drive. A lot of people have what Thomas Soll called the unconstrained
3645120	3648640	vision, so that they've got these kind of very broad based kind of visions. And you know, those
3648640	3653760	visions kind of then split into like a utopian vision. And that might be, you know, in for AI,
3653760	3657600	that might be something like the singularity, right? Or in the 1990s, these were called the
3657600	3662960	Extropians, right, which is sort of this idea of kind of a material utopia as a consequence of like
3662960	3668000	AI and let's say nanotechnology on the one hand. And that's where that, by the way, the idea of
3668000	3671520	the singularity came from, right, which is Ray Kurzweil and Bernard Vinge, they were like at some
3671520	3677200	point you get this kind of, you know, point of no return, which is like a utopian point of no return.
3677200	3682560	But then of course, the flip side of every utopia is, you know, apocalypse. And so then that's where
3682560	3686880	the sort of AI, you know, sort of the Singulitarians 20 years ago have become that, you know,
3686880	3690800	a lot of them have become the AI doomers of today. And they, you know, they have sort of this sort
3690800	3694400	of, you know, they have the same utopian impulse, they've just flipped the bit and made it negative.
3695120	3700480	So I should say like, I'm not one of those. I'm probably more of a materialist and a little bit
3700480	3704800	more of a, like I said, an engineer where, you know, things, for example, have constraints in
3704800	3710720	the real world. So I don't think we tend to get the extreme, quite the extreme outcomes, but I do
3710720	3714400	think we get, you know, we get change, like we get, we get change, we get change in the margin,
3714400	3717520	and then, you know, change in the margin that compounds over time can become, you know, quite,
3717520	3724320	quite striking. So look over 10 to 50 years, you know, look sitting here today, like, if we want it,
3725600	3730160	you know, you can imagine the next 50 years to be characterized by, you know, the rise of AI,
3730160	3735120	looks like we kind of figured that out now. You know, this superconductor thing, if it's real,
3735120	3739200	that's a, you know, turning point moment. And by the way, if it's not real, it may be, you know,
3739200	3742320	that this result points us in the direction of something that becomes real in the next few years.
3743040	3748720	And so you can imagine some combination of AI, superconductors, you know, biotech,
3748720	3752960	you know, you know, all these new techniques for, you know, biooptimization, gene editing,
3754320	3758800	you know, and then, you know, nuclear, you know, if we get our act together on nuclear fission,
3758800	3761760	by the way, there's a lot of really smart people working on nuclear fusion right now.
3763040	3766480	You know, fusion is, you know, would be an even bigger, you know, kind of opportunity
3766480	3771280	for unlimited clean energy. You know, now, you know, my cynical, the cynic in me would say
3771280	3775200	if fission is illegal, then they're certainly going to make fusion illegal. But, you know, that,
3775200	3778720	you know, that's a choice. We all get to decide whether we want to live in a world where fusion
3778720	3783840	is illegal. So, you know, we get nuclear fusion. And so sitting here 50 years from now, you know,
3783840	3789040	we basically are like, wow, you know, we are like, we have, you know, we are all much smarter
3789040	3791760	than we were because we have these smart machines working with us and everything.
3792640	3796880	You know, we have solved whatever environmental problems we thought we had, you know, with,
3796880	3800320	we have abundant energy in an increasingly clean environment.
3801200	3806160	You know, we're curing diseases at a rapid pace. And, you know, new babies are born that are immune
3806160	3811040	to disease. And so, you know, you know, not quite a material utopia, but like, you know,
3811040	3815200	a significant, you know, meaningful step function upgrades in human quality of life.
3815200	3818960	Like, I think that's all very, over a 50-year period, for sure, like that. And that's all very
3818960	3824000	possible. Over a 3,000, over whatever the year 3,000, over a 1,000-year period. I mean, look,
3824560	3828560	you do get into these questions, you know, you do, if you're going to talk about 1,000 years,
3828560	3832240	like you do get into these questions of like, you know, for example, a merger of man and machine,
3832240	3836160	right? So you do have to, over that time frame, you have to start thinking about things like,
3836160	3840640	you know, the neural link, you know, like where neural link takes you. And, you know, you know,
3840640	3843920	over that period of time, you know, you'll definitely have like, you know, neural augmentation.
3843920	3848880	So, you know, do you have shifting definitions of humanity? You know, where is the transhumanist
3848880	3853280	movement actually taking us? You know, becomes a very interesting question over that time frame.
3854000	3857600	Obviously, you have lots of questions over that time frame of space, you know, exploration,
3857600	3861280	getting to other planets, you know, other life, you know, either other life in the universe or
3861280	3864560	not other life in the universe. So kind of the spread of the, you know, the spread of our civilization
3864560	3870160	more broadly. You know, so there you truly get into science fiction scenarios. You know, then,
3870800	3875280	yeah, that's it. I'm always fun to talk about. I will admit, I am much more focused on the next
3875280	3880400	50 years. Yeah, I mean, is there anything you'd like to add into the conversation
3881120	3883280	that you feel, you know, is key that we haven't touched on?
3884080	3887760	Yeah, no, I think that's a good, I think that was a good, covered a lot of ground.
3887760	3893440	Yeah. So for effective accelerationism, just if you Google, there's a number of good already
3893440	3897920	websites and substacks talking about that. A lot of the conversations happening on Twitter.
3899120	3905120	So, and I already dropped the names of the of the EAT guys. So Beth Jesus and Bayes-Laurie,
3905120	3911760	definitely follow those guys. You know, I've not met Nick Land, but I would definitely give a
3911760	3914560	shout out and say, for anybody who hasn't encountered his work, they should definitely read
3914560	3921040	up on it. He is, I think pretty clearly like the philosopher of our time and not even because,
3921040	3924480	you know, whether I agree or disagree with him on everything he said. And of course,
3924480	3929360	he's changed his views on a lot of things over time, but just the framework that he operates,
3929360	3935120	like his willingness to actually go deep and actually think through the consequences of the
3935120	3939440	kinds of technologies that I deal with every day, you know, are just, I think, way beyond most other
3939440	3944720	people in his field. And so it's, it's, and I know he took kind of a long road to get here,
3944720	3949360	so it's fun to see, you know, it's fascinating to read that. Oh, I'll point to one other thing.
3949360	3952640	So I already mentioned the Jeremy England book, and I'll point to one other book that people
3952640	3957360	might find interesting. So, so a lot of Lance work and a lot of accelerationism, right, is based on
3957360	3962400	these, these, the ideas mob this field called cybernetics, which is kind of this, this, it's
3962400	3965840	cybernetics is interesting because it's kind of this lost field of engineering.
3966960	3974800	It was super hot in the, as an engineering field from the 1940s to the 1960s. And it basically was
3974800	3978400	sort of the original computer science. And then it was sort of, it was also sort of the original
3978400	3982560	artificial intelligence. A lot of the AI people of that era kind of call themselves cybernetics
3982560	3987920	or cybernetics. But it really is an engineering field that kind of went away or got a lot more
3987920	3994560	sedate after the 60s. And, but, but, but as I mentioned, like a lot of the ideas around AI and,
3994560	3997840	you know, machine world of machines and thermodynamics, a lot of those ideas were being
3997840	4002640	explored as far back as the 30s and 40s. So the cybernetics people of that era thought a lot
4002640	4006240	about a lot of these questions. Anyway, there's this great book is a lot of original source
4006240	4010960	material on this. And the, you know, the key character of that movement was Norbert Wiener,
4010960	4014320	and there's a bunch of books by him and about him. But there's also a great book came out recently
4014320	4018960	called Rise of the Machines by an author named Thomas Red. And it sort of reconstructs the
4018960	4023680	archaeology of cybernetics and sort of makes clear how relevant those ideas are today.
4024720	4028960	And so if you read that in conjunction with, with, with, with Nick Land's work, I think you'll find
4028960	4034480	it pretty interesting. I'll be sure to put the link for your Twitter and your blog in the description
4034480	4038800	below as well. But yeah, I think that's a good place to finish up. Mark Andreessen, thanks very
4038800	4044400	much. Good, James, a pleasure. Thank you.
