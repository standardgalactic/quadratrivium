1
00:00:00,000 --> 00:00:07,000
In this episode, I'm joined by Mark Andreessen to discuss accelerationism, AI, technology,

2
00:00:07,000 --> 00:00:10,400
the future, energy and more.

3
00:00:10,400 --> 00:00:14,560
I'd like to say a big thank you to all my paying patrons and subscribers for making

4
00:00:14,560 --> 00:00:16,440
all of this work possible.

5
00:00:16,440 --> 00:00:20,560
And if you'd like to support the podcast as it runs off patronage alone, then please

6
00:00:20,560 --> 00:00:22,520
find links in the description below.

7
00:00:22,520 --> 00:00:24,920
Otherwise, please enjoy.

8
00:00:24,920 --> 00:00:29,280
So, Mark Andreessen, thanks very much for joining us on Hermitix podcast.

9
00:00:29,280 --> 00:00:32,160
Hey James, thanks for having me.

10
00:00:32,160 --> 00:00:41,040
We are going to be discussing accelerationism, AI, technology, the future, technology is

11
00:00:41,040 --> 00:00:43,040
probably the key one here, I think.

12
00:00:43,040 --> 00:00:48,440
But I want to basically begin with probably something that on the usual podcast you go

13
00:00:48,440 --> 00:00:52,000
on, you probably aren't asked, like a lot of people will know who you are, but in the

14
00:00:52,000 --> 00:00:54,800
sphere that I'm working, people might not.

15
00:00:54,800 --> 00:01:00,240
So just tell us a little bit about yourself and what it is you do before we get started

16
00:01:00,240 --> 00:01:01,240
here.

17
00:01:01,240 --> 00:01:02,240
Yeah.

18
00:01:02,240 --> 00:01:04,800
So I'm probably the polar opposite from your usual guest.

19
00:01:04,800 --> 00:01:05,800
Exactly.

20
00:01:05,800 --> 00:01:11,040
So I'm bringing diversity to your production.

21
00:01:11,040 --> 00:01:13,480
So my background, I'm an engineer.

22
00:01:13,480 --> 00:01:19,840
So I'm a computer programmer, computer science, computer engineer by background.

23
00:01:19,840 --> 00:01:23,360
I was training kind of the old school computer science where they kind of teach you every

24
00:01:23,360 --> 00:01:27,720
layer of the system, including hardware and software.

25
00:01:27,720 --> 00:01:32,320
And then I was a programmer and then an entrepreneur in the 90s.

26
00:01:32,320 --> 00:01:36,200
And probably my main kind of claim to fame is I was sort of president of the creation

27
00:01:36,200 --> 00:01:40,280
of what today you'd consider the internet, sort of the modern consumer internet that

28
00:01:40,280 --> 00:01:42,360
people use.

29
00:01:42,360 --> 00:01:46,840
And so my work first at the University of Illinois and then later at a company I co-founded

30
00:01:46,840 --> 00:01:53,040
called Netscape, sort of popularized the idea of ordinary people being online.

31
00:01:53,400 --> 00:01:57,360
And then helped to build what today you experience as the modern web browser and kind of the

32
00:01:57,360 --> 00:02:00,120
modern internet experience.

33
00:02:00,120 --> 00:02:07,200
And then I was involved kind of through a broad range of Silicon Valley waves over the

34
00:02:07,200 --> 00:02:12,040
course of the next 20 years in the 90s and 2000s, including cloud computing where I started

35
00:02:12,040 --> 00:02:15,640
a company in and social networking I started a company in.

36
00:02:15,640 --> 00:02:20,880
And then in 2009, I started with my long-time business partner, I started a venture capital

37
00:02:20,880 --> 00:02:25,960
firm and our firm, which is called Injuries and Horowitz is now kind of one of the firms

38
00:02:25,960 --> 00:02:31,680
at the center of funding, all of the new generations of technology startups.

39
00:02:31,680 --> 00:02:37,640
And maybe the main thing I kind of underlined there is just technology, quote unquote technology,

40
00:02:37,640 --> 00:02:43,160
high tech, computer technology in particular, kind of used to be, it's always been kind

41
00:02:43,160 --> 00:02:47,400
of interesting and important in the economy for the last 50 years or something.

42
00:02:47,400 --> 00:02:52,520
In the last 15 years, I think a lot of people kind of feel that like technology has really

43
00:02:52,520 --> 00:02:58,120
spread out and it has become integral to many more aspects of life.

44
00:02:58,120 --> 00:03:02,640
And so my firm today finds itself very involved in the application of technology to everything

45
00:03:02,640 --> 00:03:11,240
from education, housing, energy, national defense, national security, as well as kind

46
00:03:11,240 --> 00:03:16,840
of every possible artificial intelligence robotics, kind of every different dimension

47
00:03:17,240 --> 00:03:19,080
of how you might touch technology in your life.

48
00:03:21,000 --> 00:03:25,880
And you picked up on something that will come into the conversation in a couple of questions

49
00:03:25,880 --> 00:03:30,920
time, but this notion of you is basically completely opposite to the majority of guests,

50
00:03:30,920 --> 00:03:36,880
not in a bad way, but often it's a lot of philosophy and which theory and not practice.

51
00:03:36,880 --> 00:03:41,440
And also this notion of technology in relation to either pessimism or optimism.

52
00:03:41,440 --> 00:03:46,880
And this is super, super key, I think, for the ongoing atmosphere of really the West,

53
00:03:46,880 --> 00:03:48,200
of where we're going to end up.

54
00:03:48,200 --> 00:03:52,720
But before we get to these questions, I mean, this is a question I'm slowly phasing out,

55
00:03:52,920 --> 00:03:56,320
but I think it will work for the sake of our conversation, because we're talking more

56
00:03:56,320 --> 00:03:57,560
broadly around themes.

57
00:03:58,720 --> 00:04:01,960
I know you've listened to the podcast before, so it is the Hermitix question.

58
00:04:02,240 --> 00:04:06,120
You can place three thinkers living or dead into a room and listen in on the conversation.

59
00:04:06,120 --> 00:04:06,760
Who do you pick?

60
00:04:07,760 --> 00:04:13,520
Yeah, I think that maybe I'll give you two versions of the answer and then maybe I can combine them.

61
00:04:13,520 --> 00:04:15,640
So there's kind of a timeless answer.

62
00:04:15,640 --> 00:04:22,200
And the timeless answer would be something like Plato or Socrates, Socrates and then Nietzsche.

63
00:04:22,200 --> 00:04:30,160
And then maybe I'd throw in one of your favorite people, Nick Land, I think would be interesting.

64
00:04:30,160 --> 00:04:36,160
The somewhat more applied version of that would be something a lot, and this is sort of maybe

65
00:04:36,240 --> 00:04:39,040
a little bit more topical these days with this movie Oppenheimer that just came out.

66
00:04:39,040 --> 00:04:45,960
But it's like John von Neumann, who was one of the co-inventors of both the atomic bomb

67
00:04:45,960 --> 00:04:50,800
and the computer, Alan Turing, who became famous a few years ago with another movie,

68
00:04:52,080 --> 00:04:57,080
The Imitation Game, and then let's throw in Oppenheimer there also, because those three

69
00:04:57,080 --> 00:05:00,560
guys were sort of present at the creation of what we would consider to be the modern

70
00:05:00,560 --> 00:05:06,120
technological world, including literally those guys were at the center, especially

71
00:05:06,160 --> 00:05:10,160
von Neumann and Turing were at the center of both World War II, the atomic bomb,

72
00:05:11,440 --> 00:05:16,640
the sort of information warfare, the whole kind of decryption kind of phenomenon,

73
00:05:16,640 --> 00:05:21,600
which really a lot of people think one, World War II, along ultimately with the A-bomb,

74
00:05:21,600 --> 00:05:26,720
and then also right precisely at that time with those people, the birth of the computer

75
00:05:26,720 --> 00:05:30,800
and everything that followed. So is that more of a practical room for you or

76
00:05:31,680 --> 00:05:36,080
in terms of like a vision going forward into the future? Or is there something else going

77
00:05:36,160 --> 00:05:44,240
on there between those sort of six figures? Those guys were very, it's almost impossible to

78
00:05:45,200 --> 00:05:51,680
overstate how smart and visionary and far seeing they were like, there's actually the

79
00:05:51,680 --> 00:05:55,440
von Neumann biography came out recently called The Man from the Future, and in anything like

80
00:05:55,440 --> 00:05:58,400
von Neumann is a more interesting character than Oppenheimer in a lot of ways, because he

81
00:05:59,040 --> 00:06:03,680
touched a lot more of these fields. And of the people who knew them that von Neumann was always

82
00:06:03,680 --> 00:06:06,560
considered that he was the smartest of what were called the Martians at that time, right,

83
00:06:06,560 --> 00:06:11,520
which were the sort of group of super geniuses that originated in Hungary in that era.

84
00:06:12,560 --> 00:06:18,720
And so, you know, they were very, very conceptual thinkers. I'll just give you one

85
00:06:18,720 --> 00:06:23,040
example of how conceptual they were, how profoundly smart they were. So they basically

86
00:06:23,040 --> 00:06:26,720
birthed the idea of artificial intelligence right in the middle of the heat of World War II.

87
00:06:26,720 --> 00:06:30,480
Like the minute they created the computer, like they created the computer, right? They created

88
00:06:30,480 --> 00:06:33,600
like the electronic computer, as we know it today, in the heat of World War II. And then

89
00:06:33,600 --> 00:06:37,040
they immediately said, aha, this means we can build electronic brains. And then they immediately

90
00:06:37,040 --> 00:06:41,840
began theorizing and developing designs for artificial intelligence. And in fact, the core

91
00:06:41,840 --> 00:06:45,760
algorithm of artificial intelligence is this idea of neural networks, right, which is this idea of

92
00:06:45,760 --> 00:06:50,080
a computer architecture that sort of is mirrors in some ways the sort of mechanical operation of

93
00:06:50,080 --> 00:06:55,200
the human brain. You know, that was literally an idea from that era in the early 1940s. There was a

94
00:06:55,200 --> 00:07:01,600
paper, two other guys who were in this world wrote a paper in 1943, outlining the theory of

95
00:07:01,600 --> 00:07:06,960
neural networks. And that literally is the same technology. That is the core idea behind

96
00:07:06,960 --> 00:07:13,200
like what you see when you use JGPT today, 80 years later. And so there was a very, very deep

97
00:07:13,200 --> 00:07:18,480
level of intellectual and philosophical, you know, I don't know what it is, like they tapped

98
00:07:18,480 --> 00:07:22,000
into or discovered or developed a very deep well that we're still drawing out of today.

99
00:07:22,960 --> 00:07:26,480
I was gonna, yeah, I was gonna ask that immediately, but you covered it. I mean,

100
00:07:26,480 --> 00:07:31,760
is there any significant changes between AI then and AI now? Or is it really just a matter of

101
00:07:31,760 --> 00:07:37,040
practicality? Like we've got the, we've got more resources and more ability to create it.

102
00:07:38,000 --> 00:07:41,600
Yeah, we're at this fairly shocking moment. So for people who haven't been following this,

103
00:07:41,600 --> 00:07:44,960
basically, it's this, it's one of these amazing things where it's like, there's like this 80

104
00:07:44,960 --> 00:07:49,360
year overnight success that all of a sudden is paying off. And so that it's, you know, it's,

105
00:07:49,440 --> 00:07:53,440
there were, you know, there were 80 years of scholars and researchers and projects

106
00:07:53,440 --> 00:07:57,120
and attempts to build electronic brains. And like every step of the way people thought that

107
00:07:57,120 --> 00:08:01,600
they were super close, you know, there was this famous seminar on the campus of I think it was

108
00:08:01,600 --> 00:08:05,680
Dartmouth University in like 1956, where they got this grant to spend 10 weeks together, they'd

109
00:08:05,680 --> 00:08:08,800
get all the AI scientists together in 1956, because they thought that after that they'd have,

110
00:08:08,800 --> 00:08:14,880
they'd have AI, you know, and turn out they didn't. And so, so it's what, but like it's

111
00:08:14,880 --> 00:08:19,520
starting to work, right? And so when you use ChatGPT today, or you use on the artistic side,

112
00:08:19,520 --> 00:08:23,520
you simply admit journey or stable diffusion, like you're seeing the payoff from that.

113
00:08:25,040 --> 00:08:28,720
I think the way to think about it is it's the deep thinking that took place up front.

114
00:08:29,840 --> 00:08:34,320
It's, and then, you know, just obviously tremendous amount of scientific and technological

115
00:08:34,320 --> 00:08:37,360
thinking and development, you know, and elaboration that took place since then.

116
00:08:38,160 --> 00:08:42,240
But then there's two other kind of key things that are making AI work today that are kind of,

117
00:08:42,240 --> 00:08:45,120
and there's sort of, again, there's sort of a combination of sort of incremental, but also

118
00:08:45,120 --> 00:08:52,240
step function breakthroughs along the way. So one is data. And so just like it turns out a big part

119
00:08:52,240 --> 00:08:57,680
of getting a neural network to work is feeding in enough data. And so, you know, and the analogy

120
00:08:57,680 --> 00:09:01,280
is irresistible, right? It's like, you know, if you want to, if you're trying to educate a student,

121
00:09:01,280 --> 00:09:06,080
right, you want to feed them, you know, and feed them a lot of material in the human world also.

122
00:09:06,080 --> 00:09:10,240
And so it just turns out there's this thing with neural networks and data where, as they say,

123
00:09:10,400 --> 00:09:15,520
you know, quantity has a quality all its own. And you really needed actually the internet to

124
00:09:15,520 --> 00:09:19,120
get to the scale of data. You needed internet scale data, you know, you needed the web to

125
00:09:19,120 --> 00:09:22,720
generate enough text data, you needed like, you know, Google images and YouTube to generate enough

126
00:09:22,720 --> 00:09:28,720
video and imagery to be able to train. So we're kind of getting a payoff from the internet itself,

127
00:09:28,720 --> 00:09:32,640
you know, combined with neural networks. And then the third is the advances in semiconductors.

128
00:09:33,920 --> 00:09:36,960
And, you know, and this is, you know, sort of the famous Moore's Law. But, you know,

129
00:09:37,840 --> 00:09:41,840
this phenomenon that, you know, that kind of we refer to as, you know, quote unquote teaching

130
00:09:41,840 --> 00:09:47,440
sand to think. And so kind of this idea, right, that you can literally convert, you know,

131
00:09:48,560 --> 00:09:54,240
silicon, you know, sand, rocks into, you know, into chips, and then ultimately into brains

132
00:09:55,040 --> 00:09:59,120
is kind of this amazing thing. And actually, as I don't know if you follow this stuff, but as

133
00:09:59,120 --> 00:10:03,600
we're recording right now, there's this like amazing phenomenon happening in the world of

134
00:10:03,600 --> 00:10:09,520
semiconductors and physics right now, which is there's this, we may be, we may be, we may be

135
00:10:09,520 --> 00:10:12,560
right now, we may have just discovered the first room temperature superconductor.

136
00:10:13,120 --> 00:10:18,160
I've been seeing this, but I'm not smart enough. Can you give me a brief overview of why this is

137
00:10:18,160 --> 00:10:21,520
so important? I mean, I'm guessing is this a resource input issue?

138
00:10:22,320 --> 00:10:26,160
So basically, every time you build a circuit today, right, every time you build any kind of

139
00:10:26,160 --> 00:10:31,760
circuit, a wire, a chip, you know, anything like that, an engine, a motor, you know, you have

140
00:10:31,760 --> 00:10:36,080
basically this process. And by the way, this actually relates to the philosophy of acceleration,

141
00:10:36,080 --> 00:10:40,560
as we'll talk about, but you have this sort of thermodynamic process where you're taking in

142
00:10:40,560 --> 00:10:45,360
energy on the one side, right, and then you have a system, right, like a, you know, an electrical

143
00:10:45,360 --> 00:10:49,680
transmission line or a computer chip or something, you have a system that's basically using that

144
00:10:49,680 --> 00:10:55,840
energy to accomplish something. And then that system is inefficient and that system is dumping heat

145
00:10:55,840 --> 00:11:00,880
out the other end. And, you know, and this is why when you use your computer, you know, if you

146
00:11:00,880 --> 00:11:03,760
got, you know, an older laptop computer, you know, the fan turns on at a certain point,

147
00:11:04,720 --> 00:11:08,080
if you have a newer laptop computer, it just starts to get hot, you know, you probably notice

148
00:11:08,080 --> 00:11:12,000
your phone starts to get hot, you know, let, you know, batteries every once in a while do what

149
00:11:12,000 --> 00:11:15,760
they call the cook off, they, you know, they lithium ion batteries will explode, right, like

150
00:11:15,760 --> 00:11:20,000
they're, you're dumping, there's always some, there's, there's always a byproduct of heat,

151
00:11:20,000 --> 00:11:23,440
and therefore, you know, sort of increased entropy kind of coming out the other side of any sort of

152
00:11:23,440 --> 00:11:28,000
electrical or mechanical system. And that's just because with, you know, kind of running energy

153
00:11:28,000 --> 00:11:33,440
through wires of any kind, you just have a level of inefficiency. By the way, the human body does

154
00:11:33,440 --> 00:11:37,120
the same thing, right, like, you know, we take in, you know, energy, and then we, you know, we're

155
00:11:37,120 --> 00:11:39,840
sitting here, you know, we don't feel it, but we're sitting here humming along at, you know,

156
00:11:39,840 --> 00:11:44,240
whatever 98.6 degrees Fahrenheit, you know, significantly higher than room temperature,

157
00:11:44,240 --> 00:11:48,560
because, you know, we're generating our actual biochemical process of life, right,

158
00:11:48,560 --> 00:11:53,360
bioelectrical is generating heat and dumping it out. Anyway, so the idea of the superconductor is

159
00:11:53,360 --> 00:11:57,680
basically think about it in the abstract as a wire that basically transmits information without,

160
00:11:57,680 --> 00:12:01,040
you know, with basically perfect fidelity, you know, perfect conservation of energy

161
00:12:01,040 --> 00:12:06,160
without dumping any heat into the environment. And it turns out that if you could do that,

162
00:12:06,160 --> 00:12:09,840
if you do that at room temperature, then all of a sudden you can have like, you know, basically,

163
00:12:09,840 --> 00:12:14,720
like, you know, incredibly more efficient, you know, kinds of batteries, electrical transmission,

164
00:12:14,720 --> 00:12:19,680
motors, you know, computer chips. And so you can start to think about, for example,

165
00:12:20,400 --> 00:12:24,800
just, you know, an example people talk about is if you, if you, if you cover the Sahara Desert

166
00:12:24,800 --> 00:12:28,960
and solar panels, you know, you could power, you know, basically the entire planet's, you know,

167
00:12:28,960 --> 00:12:32,160
power, you know, energy needs today. The problem is there's no way to transmit that,

168
00:12:33,360 --> 00:12:36,000
you know, transfer that power from the Sahara to the rest of the world

169
00:12:36,640 --> 00:12:40,160
with existing transmission line technology with superconducting transmission lines,

170
00:12:40,160 --> 00:12:45,440
all of a sudden you could, you know, quantum computers, you know, today they exist,

171
00:12:45,440 --> 00:12:49,280
but they're sharply limited because they have to be operated at these, you know, super cool

172
00:12:49,280 --> 00:12:53,520
temperatures, you know, in these very carefully constructed labs, you know, with superconductors

173
00:12:53,520 --> 00:12:58,720
in theory, you have desktop quantum computers, you know, you have levitating trains, you've got,

174
00:12:58,720 --> 00:13:03,840
you know, you just, you have a very broad cross section, you know, you have handheld MRIs,

175
00:13:04,560 --> 00:13:07,440
right, like every doctor, every nurse, you know, has an MRI and they can just, you know,

176
00:13:07,440 --> 00:13:12,560
take a scan wherever they need to, you know, on the fly, you know, and like, like, like the Star Trek,

177
00:13:12,560 --> 00:13:18,240
you know, the, the tricorder, you know, kind of thing. And so anyway, it's fascinating. So,

178
00:13:18,240 --> 00:13:22,080
so, so sitting here today, there's, there's, there's the reports of this, of this breakthrough.

179
00:13:22,080 --> 00:13:27,600
And there are the sort of almost, these almost UFO style videos of, of, of this material levitating,

180
00:13:27,600 --> 00:13:31,680
where it's not supposed to be levitating as a consequence of this breakthrough. And there are

181
00:13:31,680 --> 00:13:35,840
betting markets on scientific progress, and the betting markets, as of this morning, have the odds

182
00:13:35,840 --> 00:13:39,840
of this being a real breakthrough at exactly 50-50. And so, we...

183
00:13:39,840 --> 00:13:40,880
Not the worst odds.

184
00:13:42,080 --> 00:13:45,360
No, but it's, it's funny. If you think about it, it's funny because it's, it's, it's the,

185
00:13:45,360 --> 00:13:48,480
our entire world right now, from a physics standpoint, it's like Schrodinger's cat,

186
00:13:48,480 --> 00:13:52,800
like we live in a, we live, we live sitting here today in a superposition of two worlds,

187
00:13:52,800 --> 00:13:55,600
one in which we now have room temperatures, some conductors, and one of which we don't.

188
00:13:57,440 --> 00:14:01,040
People are, you know, these are radically different potential futures for, for humanity,

189
00:14:01,040 --> 00:14:05,200
right? And so, if it turns out it's true, you know, it's an amazing stuff, function,

190
00:14:05,200 --> 00:14:08,000
breakthrough. If not, it'll, you know, it'll, it'll set us back and we'll, you know, people

191
00:14:08,000 --> 00:14:12,240
will go back to trying to work on it, figure it out. But, you know, but, but between the time

192
00:14:12,240 --> 00:14:16,400
we're recording, between the time we release, we may even find out whether the cat, the, the

193
00:14:16,400 --> 00:14:21,360
superconducting cat, the box is alive or dead. That alive or dead state, I mean, these, these

194
00:14:21,360 --> 00:14:26,160
two separate futures is really something that I, I see, you know, when I was reading your blog,

195
00:14:26,160 --> 00:14:31,520
when I was looking at, uh, effective, effective acceler, accelerationism and accelerationism

196
00:14:31,520 --> 00:14:36,560
we'll get to, but these two futures, I think is the big question that I want to ask you, which is,

197
00:14:36,560 --> 00:14:41,760
because, because you've lived through this time, which is going through the, the optimism of the

198
00:14:41,760 --> 00:14:45,520
90s, especially, you know, you mentioned Nick Lander, the star, I mean, you see that in philosophy,

199
00:14:45,600 --> 00:14:51,280
you see that in technology, see that in the history, this huge, um, so let's call it a cyberpunk

200
00:14:51,280 --> 00:14:56,240
optimism regarding our technological future. And I would say now, I don't know, you know,

201
00:14:56,240 --> 00:15:01,440
whether or not you agree with me, please let me know. We have entered into what land himself

202
00:15:01,440 --> 00:15:10,640
called a slump from the 2000s, like late 2000s, you know, early 2000s onwards. And there seems to be

203
00:15:10,640 --> 00:15:16,320
within the, within the air, a sort of cynicism, a sort of pessimism that we've just ended up in this,

204
00:15:16,320 --> 00:15:22,080
like, place of stagnance. And do you see, I mean, if you agree with me in terms of those two,

205
00:15:22,080 --> 00:15:26,480
two possibilities, do you, I mean, I think I would be right in saying you're an optimist.

206
00:15:26,480 --> 00:15:33,680
Do you see us now re-entering into that, a new phase of optimism regarding technology and regarding

207
00:15:33,680 --> 00:15:38,400
the future? Well, so there's, there's, there's several layers to this question. I would be happy

208
00:15:38,480 --> 00:15:42,880
to kind of go through them. Then we can spend as much time in this as you want. But the, the, the,

209
00:15:42,880 --> 00:15:46,160
the core layer we're talking about, and I totally, by the way, totally acknowledge and, and I think

210
00:15:46,160 --> 00:15:51,920
this is a great topic. And, you know, the, your observations are very real. The core thing that

211
00:15:51,920 --> 00:15:55,760
I would go to, to start with is not kind of the social, political, you know, kind of, you know,

212
00:15:55,760 --> 00:15:59,280
philosophical dimension. The core thing I would go to, to start with is the technological dimension.

213
00:16:00,800 --> 00:16:04,960
In other words, at the substantive level, like, what is the actual rate of technological change

214
00:16:04,960 --> 00:16:10,240
in our world? And, and you'll know, you'll note, I don't know, you'll note that on the, on the social

215
00:16:10,240 --> 00:16:13,920
dimension, we seem to whip back and forth between, oh my God, there's too much change,

216
00:16:13,920 --> 00:16:17,920
and is he stabilizing everything? And then we whip right around to, oh my God, there's not enough

217
00:16:17,920 --> 00:16:22,720
change. And we're stagnant, right? And that's horrible. So, so there's kind of dystopian versions,

218
00:16:22,720 --> 00:16:26,320
you know, there's kind of dystopian mindsets in the air, kind of in, in, in both directions.

219
00:16:27,920 --> 00:16:32,000
So, so anyway, so I would start with kind of the technological kind of substantive layer to it.

220
00:16:32,880 --> 00:16:36,320
And there, you know, the observation, and this is not an original observation on my part,

221
00:16:36,320 --> 00:16:39,760
you know, Peter Thiel and Tyler Cohen, in particular, have gone through this in a lot

222
00:16:39,760 --> 00:16:45,360
of detail in their work. But, you know, basically, like, if you look at the long arc of technological

223
00:16:45,360 --> 00:16:49,200
development over the course of, you know, basic, you know, which, which effectively started with

224
00:16:49,200 --> 00:16:52,560
the Enlightenment, right? So you sort of, practically speaking, you're sort of starting

225
00:16:52,560 --> 00:16:57,840
around 1700 and projecting forward to today. It's about 300 years worth of what we would

226
00:16:57,840 --> 00:17:02,880
consider kind of systematic technological development. You know, it's basically, if you

227
00:17:02,880 --> 00:17:07,280
look at kind of that long arc, and then if you basically measure the pace of technological

228
00:17:07,280 --> 00:17:12,080
development and applause by saying you actually can measure the pace of technological development

229
00:17:12,080 --> 00:17:18,080
in the economy with a metric that economists call productivity growth. And so, and basically,

230
00:17:18,080 --> 00:17:22,240
the way that that works is, you know, economic productivity is defined basically as output

231
00:17:22,240 --> 00:17:26,480
per unit of input, right? And you can, you know, whatever your inputs are, could be energy, right?

232
00:17:26,480 --> 00:17:31,520
It could be, you know, raw materials, you know, whatever you want. And then, you know, output is

233
00:17:31,520 --> 00:17:35,280
in, you know, actual, you know, actual output, you know, more cars, more chips, more this,

234
00:17:35,280 --> 00:17:40,400
more that, more clothes, more food, more houses. And so, basically, what economists will tell you

235
00:17:40,400 --> 00:17:44,720
is the rate of productivity growth in the economy, which they measure annually, basically, is the

236
00:17:44,720 --> 00:17:49,920
rate of technological change in the system, right? And so, if technology is paying off, right, if the

237
00:17:49,920 --> 00:17:54,640
advances are real, then your economy is able to generate more output with the same inputs.

238
00:17:55,200 --> 00:17:59,680
If your technological development is stagnant, then that's not the case. And it's an aggregate

239
00:17:59,680 --> 00:18:04,640
measure, but it's a good measure overall. If you look at those statistics, basically, what you find

240
00:18:04,640 --> 00:18:09,600
is we had very, we think more recently in the last century, we had very rapid productivity growth in

241
00:18:09,600 --> 00:18:15,840
the West, basically, for the first half of the 20th century. So, from the basically, you know,

242
00:18:15,840 --> 00:18:21,680
what was called the Second Industrial Revolution, which started around 1880, 1890, through to basically

243
00:18:21,680 --> 00:18:26,400
the mid-60s, we had actually a very rapid rate of technological development. And by the way,

244
00:18:26,400 --> 00:18:31,840
in that era, right, we got, you know, the car, the interstate highway system, the power grid,

245
00:18:31,840 --> 00:18:37,360
telegraph, telephone, radio, television, you know, we got computers, we got, you know,

246
00:18:37,360 --> 00:18:41,760
we got like all, you know, all we got, you know, atomic, we got, you know, both atomic weapons

247
00:18:41,760 --> 00:18:46,560
and also nuclear power technology, right? And so, there was this tremendous kind of technological

248
00:18:46,560 --> 00:18:51,920
surge that took place, you know, in that sort of scholar 1880 to 1960, 1965 kind of period.

249
00:18:51,920 --> 00:18:55,280
The productivity growth ran, you know, through that era, two to 4% a year,

250
00:18:56,320 --> 00:19:00,000
which, which, and the aggregate is very fast, you know, for the economy overall, like that's,

251
00:19:00,000 --> 00:19:06,960
that's a very fast pace of change. Basically, since the mid-60s, early 70s, the rate of productivity

252
00:19:06,960 --> 00:19:12,560
growth basically took a sharp deceleration. And so, in the, in the, basically, the 50 years, 52

253
00:19:12,560 --> 00:19:17,680
years now that I've been alive, you know, it's, it's been a step lower, it's been 1 or 2% a year,

254
00:19:17,680 --> 00:19:22,160
it's been kind of persistently too low relative to what it should be. And, and, you know, I think

255
00:19:22,160 --> 00:19:26,720
there's a bunch of possible explanations for that. But I think the most obvious one is that

256
00:19:27,600 --> 00:19:32,960
basically the, the world of technology bifurcated in the 70s and 80s into two domains, one domain is

257
00:19:32,960 --> 00:19:36,720
the domain of bits, you know, the domain of computers and the internet, where there has been,

258
00:19:36,720 --> 00:19:39,920
you know, obviously very rapid technological development, you know, you know, potentially,

259
00:19:39,920 --> 00:19:44,800
you know, now culminating in AI. But then there's also the world of atoms. And, you know, the,

260
00:19:44,800 --> 00:19:49,200
the diagnosis at least that I would apply is we, we, we essentially outlawed technological

261
00:19:49,200 --> 00:19:53,520
development and innovation in the, in the realm of atoms, you know, basically since the 1970s.

262
00:19:54,480 --> 00:19:57,760
There are many examples of how we've done this. And, you know, you can look at things like housing

263
00:19:57,760 --> 00:20:01,520
policy, and you can kind of see it quite clearly, but also very specifically, you can see it in

264
00:20:01,520 --> 00:20:07,280
energy, which is, you know, we discovered nuclear power, right? We discovered a source of, you know,

265
00:20:07,280 --> 00:20:11,840
a limited, you know, zero emissions energy that, you know, compared to every other form of energy

266
00:20:11,840 --> 00:20:16,320
is like ultra safe, you know, nuclear energy is like, by far the safest form of energy that we

267
00:20:16,320 --> 00:20:21,760
know of. And, you know, in the 1970s, we essentially made it illegal, you know, just like totally

268
00:20:21,760 --> 00:20:26,640
banned it. And we talked more about that, but like that, that was like a draconian thing that,

269
00:20:26,640 --> 00:20:31,600
that, you know, has consequences through to, to the world we live in today. And so, so we live in

270
00:20:31,600 --> 00:20:35,600
this, or any, you mentioned cyberpunk, and this is, this is actually kind of the cyberpunk ethos

271
00:20:35,600 --> 00:20:39,200
that I think actually reflects something real, which is, you know, if you're in the virtual world,

272
00:20:39,200 --> 00:20:43,520
it's like, wow, right? It's like, you know, it's amazing. Like everything is like spectacular.

273
00:20:43,520 --> 00:20:46,800
And, and yeah, look, even like a podcast like yours, like, right, would have been, you know,

274
00:20:46,800 --> 00:20:52,320
inconceivable 30 years ago, right? And so like information, transmission, communication, coordination,

275
00:20:52,960 --> 00:20:57,440
you know, all these things are have taken huge leaps forward. But then the minute we, you know,

276
00:20:57,440 --> 00:21:01,360
the minute you get into a car, or the minute you plug something into the wall, right, or the minute

277
00:21:01,360 --> 00:21:06,880
you eat food, right, you're still living in the 1950s. And so I think we live in a schizophrenic

278
00:21:06,880 --> 00:21:13,200
world with respect to that question. Why then, so you write about this in your blog post on AI,

279
00:21:13,200 --> 00:21:19,120
which we'll get to, but you draw in Prometheus, right, this, this consistent historical cycle of

280
00:21:19,120 --> 00:21:22,160
when there is a new technology, it's going to destroy us, everything's going to end,

281
00:21:22,160 --> 00:21:26,480
it's the worst thing ever, we need to be careful of it, you know, the TV is going to burn your

282
00:21:26,480 --> 00:21:31,120
eyeballs out of your sockets, the vacuum cleaner is going to, I don't know, like explode or whatever,

283
00:21:31,120 --> 00:21:37,200
but every time there is a, like a cyclic change of a new technological innovation, it's this

284
00:21:37,200 --> 00:21:40,960
Promethean thing of where we're pretty terrified of it and we want it to go away. And then eventually

285
00:21:40,960 --> 00:21:44,960
we're like, Oh, actually, no, that's pretty helpful. But there seems to be, as you said,

286
00:21:44,960 --> 00:21:49,600
there's something that happened in the 1970s, where we just pushed away the atomic world in

287
00:21:49,600 --> 00:21:54,480
favor of the bits, you know, which makes sense. But why, I mean, there's probably a lot of

288
00:21:54,480 --> 00:22:00,000
governmental reasons for this as well. But why were we so, it seems like a fear, really,

289
00:22:00,000 --> 00:22:07,200
the way you talk about it, like why were we so in a way scared to then develop the atomic world

290
00:22:07,200 --> 00:22:13,120
in the way we had the bit world? Yeah, so I go start even deeper, I think, which is there's

291
00:22:13,120 --> 00:22:18,720
a deep fear in the human psyche, and I think probably in the human animal of new knowledge,

292
00:22:18,720 --> 00:22:23,280
like it's even a level like technology is an expression of knowledge, right, like the Greeks

293
00:22:23,280 --> 00:22:27,280
right have this term, Techni, which is sort of this, you know, which is where the word technology

294
00:22:27,280 --> 00:22:30,000
comes from. But I think the underlying meaning is more like general knowledge.

295
00:22:31,120 --> 00:22:35,440
You know, the Christian, you know, the key to the Christian, you know, kind of theology,

296
00:22:35,440 --> 00:22:39,600
right, is the, you know, what is, you know, what was the original sin, right, it was eating the

297
00:22:39,600 --> 00:22:44,480
apple from the Tree of Knowledge, right, it was, it was mankind, right, mankind learning that,

298
00:22:44,480 --> 00:22:49,040
which he was not supposed to learn. And so, you know, the Greeks had the Prometheus myth,

299
00:22:49,040 --> 00:22:54,000
the Christians have the snake in the Garden of Eden and the Tree of Knowledge, like there's

300
00:22:54,000 --> 00:22:58,800
something very, very deep, like there's, there's an asymmetry, I think, wire deeply in the human

301
00:22:58,800 --> 00:23:04,480
brain, right, which is, you know, sort of, you know, fear versus hope, which, which from an

302
00:23:04,480 --> 00:23:07,600
evolutionary standpoint, like would make a lot of sense, right, which is like, okay, if you're

303
00:23:07,600 --> 00:23:11,680
living in, let's say prehistoric times, you know, in the sort of long evolutionary landscape that

304
00:23:11,680 --> 00:23:18,000
we lived in, you know, is new information likely to be good or bad, probably over the sweep of,

305
00:23:18,000 --> 00:23:21,360
you know, the billions of years of evolution that we went through, most new information was bad,

306
00:23:21,360 --> 00:23:26,000
right, most new information was the predators coming over the hill to kill you. And so, I think

307
00:23:26,000 --> 00:23:31,680
there's something like deeply resonant about the idea that new is bad, that, you know, and by the

308
00:23:31,680 --> 00:23:36,000
way, look, in the, in the West, like, we probably, you know, we actually, I think, from a historical

309
00:23:36,000 --> 00:23:39,600
and maybe comparative standpoint, like we're actually quite enamored by new things as compared

310
00:23:39,600 --> 00:23:43,680
to a lot of traditional societies. And so, if anything, we've overcome some of our national

311
00:23:43,680 --> 00:23:48,480
instincts on this, but that, that, that impulse is still deep. And then if you go up one level to

312
00:23:48,480 --> 00:23:55,760
kind of the social level, you know, I'm quite bought into an explanation on this that was provided,

313
00:23:55,760 --> 00:24:00,640
there's a, there was a philosopher of science, historian of science named Elting Morrison

314
00:24:00,640 --> 00:24:05,600
at MIT in the, in the first half of the 20th century, who talked about this. And he said,

315
00:24:05,600 --> 00:24:08,720
look, you need to think about basically technology intersects with social systems.

316
00:24:09,680 --> 00:24:13,600
When a new technology intersects with a social system, basically what it does is it threatens

317
00:24:13,600 --> 00:24:19,360
to upend the social order, right. And so, at any given moment in time, you have a social order,

318
00:24:19,360 --> 00:24:24,400
right, with status hierarchies, right, and people who are in charge of things. And basically what

319
00:24:24,400 --> 00:24:28,560
he says is the social order of any time is basically, you know, in sort of Western sort of

320
00:24:28,560 --> 00:24:32,400
modern sort of enlightenment, Western civilization, the social order is a function of the technologies

321
00:24:32,400 --> 00:24:36,320
that led up to it, right. And so you have a certain way of organizing the military, you have a certain

322
00:24:36,320 --> 00:24:39,520
way of organizing, you know, industrial society, you have a certain way of organizing, you know,

323
00:24:39,520 --> 00:24:44,880
political affairs. And they are the consequence of the technologies up to that point. And then

324
00:24:44,880 --> 00:24:49,200
you introduce a new technology, and the new technology basically threatens to upend that

325
00:24:49,200 --> 00:24:53,440
status hierarchy. And the people who are in power all of a sudden aren't, and there are new people

326
00:24:53,440 --> 00:24:57,280
in power. And of course, you know, what is the thing that people will fight the hardest to maintain,

327
00:24:57,280 --> 00:25:01,520
you know, as, you know, as their status in the hierarchy. And then he goes through example

328
00:25:01,520 --> 00:25:05,840
after example of this throughout history, including this incredible example of the development of the

329
00:25:05,840 --> 00:25:11,760
first naval gun that adjusted for the role of a battleship and battle, which increased the firing

330
00:25:11,760 --> 00:25:18,000
accuracy of naval guns by like 10x. It was one of the great decisive breakthroughs in modern weaponry.

331
00:25:19,120 --> 00:25:23,760
And it still took both the US and the UK British navies 25 years to adopt it.

332
00:25:25,040 --> 00:25:31,280
Because the entire command status hierarchy of how naval combat vessels were run and how

333
00:25:31,280 --> 00:25:35,600
gunnery systems worked and how tactics and strategy worked for naval battles, like had to be upended

334
00:25:35,600 --> 00:25:40,240
with the invention of this new gun. Anyway, and so like he would basically say, you know,

335
00:25:40,240 --> 00:25:45,120
essentially, duh, you know, you roll out this new technology, it, you know, it causes people who

336
00:25:45,120 --> 00:25:49,680
used to have power and no longer have power, puts new people in power, you know, in modern terms,

337
00:25:49,680 --> 00:25:53,280
you know, the language that we would use to describe this as gatekeepers, right? Like so,

338
00:25:53,280 --> 00:25:58,640
you know, why is the traditional journalism press so, you know, it just absolutely furious about

339
00:25:58,640 --> 00:26:03,040
the internet, right? And it's because like the internet gives right regular people the opportunity

340
00:26:03,040 --> 00:26:06,640
to basically be on a, on at least a peer relationship, if not, you know, in the case of

341
00:26:06,640 --> 00:26:11,760
somebody like Joe Rogan, a superior relationship, right? And then it's an upending of the status

342
00:26:11,760 --> 00:26:16,640
hierarchy. And kind of, you know, the same thing, you know, through, basically, like one of the

343
00:26:16,640 --> 00:26:20,320
ways to interpret the story of our time from a social standpoint is all of the gatekeepers who

344
00:26:20,320 --> 00:26:25,360
were strong in the 60s and 70s are basically being torn down. Another obvious example, political

345
00:26:25,360 --> 00:26:30,640
parties, right? Why are so many Western political parties in a state of some combination of freak

346
00:26:30,640 --> 00:26:34,960
out and meltdown right now, right? Well, it's because in an era of radio and television,

347
00:26:34,960 --> 00:26:38,320
they were able to broadcast a top down message, and they were able to tell voters basically

348
00:26:38,320 --> 00:26:42,400
what to think in the, in the new model voters are deciding what they think based on what they

349
00:26:42,400 --> 00:26:46,080
read online. And then they're reflecting that back up and finding their politicians wanting,

350
00:26:46,080 --> 00:26:50,480
right? And so therefore, like the re-rise of populism and, you know, sort of the blowing out of,

351
00:26:50,480 --> 00:26:54,240
you know, sort of both left-wing and right-wing ideologies, right? The sort of, you know,

352
00:26:54,240 --> 00:26:58,160
the center is not holding. And so anyway, that would be another example in Morris's framework.

353
00:26:59,120 --> 00:27:03,280
And then I'll just close on this. Morrison has this fast. He says there's this as a consequence

354
00:27:03,280 --> 00:27:07,840
to the fact that technology changes social hierarchies. He says there's a predictable

355
00:27:07,840 --> 00:27:12,560
three stage process to the reaction to any new technology by the status quo, but basically

356
00:27:12,560 --> 00:27:18,560
the people in power at that time. He says, step one is ignore. And so just like pretend it doesn't

357
00:27:18,560 --> 00:27:22,880
exist. Which by the way, is actually a pretty good strategy because like most technologies don't

358
00:27:22,880 --> 00:27:26,080
upend social orders, like most new technologies don't work at the time that they're first

359
00:27:26,080 --> 00:27:31,120
presented. So maybe ignore is actually a rational strategy. Step two is what he calls rational

360
00:27:31,120 --> 00:27:35,200
counter argument. And so that's where you get like the laundry list of all the things that are

361
00:27:35,200 --> 00:27:39,520
wrong with the new technology, right? And then he says step three is when the name calling begins.

362
00:27:41,840 --> 00:27:46,240
This, I mean, I watched a couple of your other interviews recently. And this relates to,

363
00:27:46,240 --> 00:27:51,280
I know you've been talking about Nietzsche's master in slavery morality recently. And this

364
00:27:51,360 --> 00:27:55,680
seems to tie to that in this notion of Nietzsche and, you know, he does a typical

365
00:27:55,680 --> 00:28:00,160
philosophical thing of taking a French word and drawing it out. But Rosentum on, right? Instead

366
00:28:00,160 --> 00:28:06,800
of, you know, just having a look at nuclear power and seeing where it would go and allowing that

367
00:28:06,800 --> 00:28:13,680
power to unfold within society, you try invert the morals. So you say, well, actually, the good thing

368
00:28:13,680 --> 00:28:18,400
to do is because these people don't have the will to power, because they don't have the ability

369
00:28:18,400 --> 00:28:24,000
or the engineering skills, I guess in your own case, to like, you know, to utilize the thing,

370
00:28:24,000 --> 00:28:29,120
they invert the morals and say, well, actually, the good thing is to do the inverse is to not have

371
00:28:29,120 --> 00:28:34,800
it like this is bad. And now that then immediately puts them in the in the good camp. But it seems

372
00:28:34,800 --> 00:28:40,640
like, to be honest, it really feels especially with AI and also now with nuclear power, now that,

373
00:28:40,640 --> 00:28:44,480
you know, especially in Germany, certain things have been tried. And now it's like, okay, this was

374
00:28:44,480 --> 00:28:49,760
a really bad mistake in terms of energy, like the cat's out of the bag. And we like, there's now

375
00:28:49,760 --> 00:28:54,800
this force of having to move, you were then talking about the second to second and third stages there.

376
00:28:54,800 --> 00:28:59,680
It's almost like, look, with AI, especially the cat's out of the bag, like, we have to move, there's

377
00:28:59,680 --> 00:29:04,400
no, there's no choice of like, ignoring or reacting against it. Now you have to deal with it or you

378
00:29:04,400 --> 00:29:08,240
don't. Yeah, so let's let's spend a little one more moment on nuclear power and then and then go

379
00:29:08,240 --> 00:29:12,720
to AI. So nuclear power is so interesting, because nuclear power is the tell. Like, I always look for

380
00:29:12,720 --> 00:29:15,360
like the little signals that people don't really mean what they say, or that they don't, they're

381
00:29:15,360 --> 00:29:19,040
not really like they're, you know, they're, they're, they're, they're sort of, you know, moral system

382
00:29:19,040 --> 00:29:23,280
doesn't quite line up properly. And so nuclear power is this like amazing, it's this amazing thing,

383
00:29:23,280 --> 00:29:26,640
it's like, literally, it's like, okay, you build this thing, it generates power, it basically,

384
00:29:26,640 --> 00:29:31,360
it generates a small amount of nuclear waste, it generates steam, but it generates zero emissions,

385
00:29:31,360 --> 00:29:37,120
right, zero carbon, right. And so you have this basically, it's amazing phenomenon where you have

386
00:29:37,120 --> 00:29:41,360
this, and let's just take them completely as space value, I'm not gonna, this is not me questioning,

387
00:29:41,360 --> 00:29:44,080
I'm not going to question carbon emissions or global war, I'm just gonna, I'm gonna assume

388
00:29:44,080 --> 00:29:47,520
that everything the environmentalists say about carbon emissions, climate, you know, change all

389
00:29:47,520 --> 00:29:51,200
our stuff. Let's assume that that's all totally real. Like, let's just, let's just grant them all

390
00:29:51,200 --> 00:29:56,720
that. It's like, okay, well, like, okay, so how could, how can you solve the sort of climate

391
00:29:56,720 --> 00:30:00,160
crisis, the carbon emissions crisis, it's like, well, you have the silver bullet technology,

392
00:30:00,160 --> 00:30:05,040
you could roll out in the form of nuclear fission today. You could generate a limited power. Richard

393
00:30:05,040 --> 00:30:09,760
Nixon, by the way, the, you know, the heavily, heavily condemned Richard Nixon in 1972,

394
00:30:10,480 --> 00:30:13,440
you know, proposed something at the time he called project independence.

395
00:30:14,400 --> 00:30:17,920
Project independence was going to be the United States building 1000 new civilian nuclear power

396
00:30:17,920 --> 00:30:23,200
plants by the year 1980, and cutting the entire US energy grid, including the transportation system,

397
00:30:23,200 --> 00:30:28,720
cars, everything, home heating, everything over to nuclear power by 1980, going zero emission

398
00:30:28,720 --> 00:30:32,320
in the US economy. And by the way, right, geopolitically removing us from the Middle East,

399
00:30:33,040 --> 00:30:38,160
right, right. So no, right, no, Iraq, FK, all that stuff, like just completely unnecessary,

400
00:30:38,160 --> 00:30:43,920
right. And, you know, you'll note that like project independence did not, did not happen,

401
00:30:44,640 --> 00:30:48,000
right, like we don't, we don't live in that world today. And so it's like, okay, you've got this

402
00:30:48,000 --> 00:30:53,040
like crisis, you've got this like silver bowl solution to for it, and you very deliberately

403
00:30:53,040 --> 00:30:58,160
have chosen to not adopt that solution. And it's like, and there's this actually very interesting

404
00:30:58,160 --> 00:31:01,600
split in the environmental movement today. And it's really kind of, you know, I think kind of

405
00:31:01,600 --> 00:31:06,160
bizarre. And it's like a 99 to one split, you asked like 99% of environmental activists about

406
00:31:06,160 --> 00:31:10,720
nuclear power, they just just sort of categorically dismiss it was, of course, that's not an option.

407
00:31:10,720 --> 00:31:15,520
You do have this kind of radical fringe with people like Stuart Brand, who are like, basically

408
00:31:15,520 --> 00:31:18,960
now pointing out that it is, it is a silver bullet answer, but most of them are saying, no,

409
00:31:18,960 --> 00:31:22,720
it's not an answer. And it's like, okay, well, why are they doing that? It's like, well, like,

410
00:31:22,720 --> 00:31:25,920
what is it that they're saying that they want to do? And what they're saying they want to do

411
00:31:25,920 --> 00:31:29,920
is what they call, you know, degrowth, right. And so they want to decarbonize the economy,

412
00:31:29,920 --> 00:31:34,000
they want to deenergize the economy, they want to degrow the economy. And then, you know, when

413
00:31:34,000 --> 00:31:38,000
you get down to it, and you ask them a very, you know, specific question about the implications of

414
00:31:38,000 --> 00:31:41,760
this, you know, basically what you find is the general model is they want to reduce the human

415
00:31:41,760 --> 00:31:45,520
population on the planet to about 500 million people. You know, it's kind of the answer that

416
00:31:45,520 --> 00:31:50,320
they ultimately come down to. And so ultimately, the, you know, the big agenda is to is to reduce

417
00:31:50,320 --> 00:31:54,720
the human, you know, basically the human herd, you know, quite sharply. And, you know, they kind

418
00:31:54,720 --> 00:31:57,600
of dance around this a little bit, but when they when they really get down to it, this is what they

419
00:31:57,600 --> 00:32:00,720
talk about. And of course, you know, Paul Ehrlich, you know, is kind of one of the kind of famous

420
00:32:00,720 --> 00:32:04,400
icons of this, he's been talking about this for decades. I think it was Jane Goodall, who used

421
00:32:04,400 --> 00:32:10,480
the 500, you know, million, you know, number recently in public. And so, and so, and so then

422
00:32:10,480 --> 00:32:15,280
you got this kind of very interesting, you know, technological philosophical moral question, which

423
00:32:15,280 --> 00:32:19,600
is like, well, what, what is the goal here, right, is the goal to like solve climate change, or is

424
00:32:19,600 --> 00:32:24,640
the goal to like depopulate the planet, right. And to the extent that like free unlimited power,

425
00:32:25,200 --> 00:32:28,720
right, would interfere with, you know, to the extent that that's a problem, the problem it

426
00:32:28,720 --> 00:32:33,760
would be as if the actual agenda is to depopulate the planet. And like, I would like this to not

427
00:32:33,760 --> 00:32:37,840
be the case. Like, I think, you know, again, take taking everything else that they say at face value,

428
00:32:37,840 --> 00:32:40,800
you'd like to solve carbon emissions and climate change and everything else. But like,

429
00:32:41,440 --> 00:32:45,040
you know, like, I think you, you know, you might also say you want a planet in which there are

430
00:32:45,040 --> 00:32:49,040
not only 8 billion people, but maybe, you know, maybe people are good, right. Maybe you're actually

431
00:32:49,040 --> 00:32:53,440
should have 20 billion or 50 billion people. And we have the technology to do that. And we're

432
00:32:53,440 --> 00:32:59,440
choosing not to do it. So, so, so, so this is the thing, like this gets into these very deep

433
00:32:59,440 --> 00:33:04,000
questions, right, to your point of like, okay, very deep questions about morality. And like,

434
00:33:04,000 --> 00:33:08,800
how did we maneuver or, you know, like per nature, like how did we reverse ourselves into a situation

435
00:33:08,800 --> 00:33:13,360
where we're actually arguing against human life. And of course, and this is we'll get to it, but

436
00:33:13,360 --> 00:33:16,560
this, this of course is then, you know, a big part of the origin of the idea of effective

437
00:33:16,560 --> 00:33:21,840
accelerationism, which is basically new, like let's go sharply in the other direction. Oh, and

438
00:33:21,840 --> 00:33:27,280
then yeah, so AI, yeah, AI is playing out much the same way as already playing out the same way.

439
00:33:27,280 --> 00:33:31,360
And here you've got this like just incredible phenomenon happening where we, we, you know,

440
00:33:31,360 --> 00:33:34,720
it looks like we have a key breakthrough to basically increase the level of intelligence,

441
00:33:34,720 --> 00:33:39,040
you know, basically all throughout society and around the world, you know, through, you know,

442
00:33:39,040 --> 00:33:43,200
basically for the first time, you know, directly applying your general intelligence to the world.

443
00:33:44,720 --> 00:33:49,360
And, you know, there is this like incredibly basically aggressive movement that is actually

444
00:33:49,440 --> 00:33:55,520
having tangible impact today in the halls of power in Washington DC and in the EU and other places,

445
00:33:56,080 --> 00:33:59,120
you know, that is seeking to stop and reverse it, you know, as aggressively as they possibly can.

446
00:33:59,840 --> 00:34:04,320
And so we're kind of, we're going through, we're going through, I would say, a suddenly accelerated

447
00:34:04,320 --> 00:34:08,320
and very sharp and aggressive version of exactly what happened with nuclear power happening with

448
00:34:08,320 --> 00:34:13,520
AI right now. I mean, this is the thing that can, can, well, there's two questions because

449
00:34:13,520 --> 00:34:19,120
on your blog, you, it's really refreshing to see you, you're pretty to the point when you say,

450
00:34:19,120 --> 00:34:25,440
look, AI is code, it's code written by people, by human beings on computers developed by human

451
00:34:25,440 --> 00:34:31,040
beings, you know, like we're in control, you're not of this, I think there was, you know, Musk

452
00:34:31,040 --> 00:34:35,920
signed a big thing where like, you know, 1000 people signed this thing to say like, we need to hold

453
00:34:35,920 --> 00:34:41,280
this the whole Rocco's Basilisk AI is going to be terminated to come in and blowing us up with

454
00:34:41,280 --> 00:34:45,280
robots, etc. So it's going to kill us all. You're very much like, no, this is code, this is just

455
00:34:45,280 --> 00:34:51,040
an intelligence for us to use. Now that's one question, you know, I guess, why isn't AI going

456
00:34:51,040 --> 00:34:54,480
to kill us all? And I know you've spoken about that a lot. So that answer can be brief. But

457
00:34:54,480 --> 00:35:01,200
secondly, this whole idea of trying to reverse it, to me, it seems inherent within AI as a thing

458
00:35:01,200 --> 00:35:06,800
that it wants, you know, it's the cats out the back, you can't like once it's here, you, you,

459
00:35:06,880 --> 00:35:13,040
outside of really draconian measures, you can't because how do you how do you hold an

460
00:35:13,040 --> 00:35:18,560
intelligence which is growing, right? Well, except, you know, they did stall at nuclear power,

461
00:35:18,560 --> 00:35:24,400
right? So, right, like so they did, like it worked. So why did project independence not happen?

462
00:35:24,400 --> 00:35:28,240
Why do we not have like, you know, unlimited nuclear power today? You know, the reason is

463
00:35:28,240 --> 00:35:31,920
because it was it was blocked by the by the political system, right? And so, so, you know,

464
00:35:31,920 --> 00:35:35,040
Richard Nixon, who I mentioned, you know, proposed this, he also created the Environmental

465
00:35:35,040 --> 00:35:39,280
Protection Agency and the Nuclear Regulatory Commission. You know, the nuclear, it's actually

466
00:35:39,280 --> 00:35:44,800
that this actually been a big week. The first new nuclear power plant design, the first newly

467
00:35:44,800 --> 00:35:49,360
designed nuclear power plant, in the last 50 years, just went online in Georgia, you know,

468
00:35:50,400 --> 00:35:54,480
$20 billion over budget and you know, it's got it's a story of its own, but at least we got one

469
00:35:54,480 --> 00:35:58,800
online. It's the first new nuclear power plant design ever authorized by the Nuclear Regulatory

470
00:35:58,800 --> 00:36:04,720
Commission, says Nixon created that commission, right? And so, so, so we put in place a regulatory

471
00:36:04,720 --> 00:36:09,360
regime around nuclear power in the 1970s that, you know, all but made it impossible. By the way,

472
00:36:09,360 --> 00:36:12,080
you alluded to the Germany thing earlier, I'll just touch on that for a second. So,

473
00:36:12,720 --> 00:36:15,920
you know, that, you know, you've, I'm sure you've heard of the idea of the precautionary principle,

474
00:36:15,920 --> 00:36:20,160
right? Right, which is this, this idea that basically scientists and technologists have a

475
00:36:20,160 --> 00:36:24,640
moral obligation to think through all the possible negative consequences of a new technology before

476
00:36:24,640 --> 00:36:28,240
it's rolled out. The precautionary principle, right, the precautionary principle, and we could

477
00:36:28,240 --> 00:36:32,160
talk about that, including whether scientists and technologists are actually qualified to do that.

478
00:36:33,120 --> 00:36:38,960
But, you know, this was also a central theme of Oppenheimer, but the precautionary principle

479
00:36:38,960 --> 00:36:43,120
was invented by the German Greens in the 1970s, and it was prevented specifically to stop nuclear

480
00:36:43,120 --> 00:36:49,360
power. And, you know, it is just amazing, we're sitting here in 2023, and there's this, you know,

481
00:36:49,360 --> 00:36:53,680
where we effectively, we in the West are effectively at this, you know, at war with Russia,

482
00:36:54,960 --> 00:36:59,280
right? And, you know, it's a proxy war right now that, you know, hopefully doesn't turn into a real

483
00:36:59,280 --> 00:37:04,160
war, but who knows, you know, the proxy wars have a, you know, have a disconcerting, you know,

484
00:37:04,160 --> 00:37:10,240
pattern of spilling over into becoming real wars. And, you know, a lot of this is, it's a tale of

485
00:37:10,240 --> 00:37:16,880
energy. And, you know, basically the Russian economy, you know, is like 70% energy exports,

486
00:37:16,880 --> 00:37:22,080
right, oil and gas exports. The major buyer of that energy historically has been Europe and

487
00:37:22,080 --> 00:37:27,760
specifically Germany. You know, Europe and Germany specifically essentially have funded the Russian

488
00:37:27,760 --> 00:37:32,320
state, the Putin state, you know, and that funding is what basically built and sustains

489
00:37:32,320 --> 00:37:37,840
their military engine, which is what they've used to invade Ukraine, right? And so it's this like,

490
00:37:38,560 --> 00:37:42,720
like, there's this counterfactual, right, where the German Greens did not do what they did in

491
00:37:42,720 --> 00:37:47,120
the 1970s, nuclear power was not blocked, you know, Germany and France and the rest of Europe

492
00:37:47,120 --> 00:37:51,120
today is like fully energy independent running on nuclear power, you know, the Russia state,

493
00:37:51,120 --> 00:37:55,360
it would be greatly weakened because the value of their exports would be, you know, enormously

494
00:37:55,360 --> 00:38:00,480
diminished. And they would not have the wherewithal to invade other countries or to threaten

495
00:38:00,480 --> 00:38:06,640
Europe. And so like, these decisions have like real consequences. And, you know, these people,

496
00:38:07,680 --> 00:38:12,560
use the pejorative sense, like they are so confident that they can step into these, you know,

497
00:38:12,560 --> 00:38:15,520
debates, you know, kind of questions around, you know, new technologies and how they should be

498
00:38:15,520 --> 00:38:19,040
applied and what the consequences are, they can step in and they can use the political

499
00:38:19,040 --> 00:38:23,280
machine to basically throw sand in the gears and stop these things from happening. So, so like,

500
00:38:23,280 --> 00:38:27,440
AI, this is what's happening right now. So like, you know, in the, in the sort of, you know,

501
00:38:27,440 --> 00:38:31,120
theoretical position where AI is kind of this, you know, potentially runaway thing, then, right,

502
00:38:31,120 --> 00:38:35,360
maybe it can be constrained, like, in the real world, it very much can be constrained. And

503
00:38:35,360 --> 00:38:39,520
the reason it can be constrained in the real world is because it uses physical resources,

504
00:38:39,520 --> 00:38:45,760
right? It has a physical, it has a physical layer to it. And that layer is energy usage.

505
00:38:46,640 --> 00:38:52,160
And that layer is chips. And that layer is, you know, telecom bandwidth. And that layer is data

506
00:38:52,160 --> 00:38:58,080
centers, physical data centers, right? And so, and that layer is like, you know, by the way,

507
00:38:58,080 --> 00:39:01,760
that layer also includes the actual technologists, like working in the field, and their ability to

508
00:39:01,760 --> 00:39:06,800
actually do what they do. And there are, you know, a very large number of sort of control points and

509
00:39:06,800 --> 00:39:11,840
pressure points that, you know, the state can put on those layers to prevent them from being used

510
00:39:11,840 --> 00:39:17,600
for whatever it wants to prevent. And, you know, and look, the EU is on the verge, the EU has this

511
00:39:17,600 --> 00:39:21,600
like anti AI bill that it looks like is going to pass that is like extremely draconian and may

512
00:39:21,600 --> 00:39:26,720
result in Europe not even having an AI industry and may result in, you know, American AI companies

513
00:39:26,720 --> 00:39:31,040
not even operating in Europe. And then in the US, we have a very kind of similar push happening is,

514
00:39:31,040 --> 00:39:37,120
you know, the sort of ant, what I would describe as the anti AI zealots are, you know, they are,

515
00:39:37,120 --> 00:39:41,360
they are in the White House today, right, arguing that, you know, this is bad, it should be stopped.

516
00:39:42,480 --> 00:39:46,640
And it's like, you know, it's, it's, it's amazing because it's like, how many times are we going

517
00:39:46,640 --> 00:39:50,720
to like run through this loop? How many times are we going to like repeat history here? How,

518
00:39:50,720 --> 00:39:54,560
how many times are we going to be kind of self defeating like this? And like apparently the,

519
00:39:54,560 --> 00:39:57,040
the impulse to be self defeating, we have not worked it out of our system.

520
00:39:58,720 --> 00:40:02,960
You don't want to be self defeating them. I mean, let's move into this peculiar four letters,

521
00:40:02,960 --> 00:40:07,680
which is found at the moment at the end of your Twitter name and the end floating around Twitter,

522
00:40:07,680 --> 00:40:14,640
mostly e slash act or effective accelerationism. And this like, this is just beautiful to me.

523
00:40:14,640 --> 00:40:18,480
It's like the, the acceleration is Renaissance. I've been set talking about it in that way. I

524
00:40:18,480 --> 00:40:22,640
don't want to gatekeep it too much, but you know, I wrote my master's thesis on accelerationism,

525
00:40:22,640 --> 00:40:26,480
like I love it. I love talking about it. You don't want any of this holding back. You don't

526
00:40:26,480 --> 00:40:30,800
want to hold anything back. You want to accelerate. So firstly, I mean, there's two questions there.

527
00:40:31,440 --> 00:40:36,320
What is it for you to accelerate and what is effective accelerationism?

528
00:40:37,600 --> 00:40:41,360
Yeah. So let me, let me just say where that, where it came from, I'll reverse the second one

529
00:40:41,360 --> 00:40:45,840
first and then go to the broader topic. So, so, so it's a, it's a combination. There's, there's,

530
00:40:45,840 --> 00:40:49,040
there's, you know, kind of two, two words there, effective and accelerationism. So the, you know,

531
00:40:49,040 --> 00:40:52,960
the acceleration, accelerationism part of it is obviously building on what you've talked about

532
00:40:52,960 --> 00:40:56,960
and what Nick Landon and others have talked about for a long time. And of course, as you,

533
00:40:56,960 --> 00:41:00,000
as you've talked about, there's, there's all these different versions of accelerationism. And so this

534
00:41:00,000 --> 00:41:03,440
is, this is, you know, proposing one that, you know, it's this, this one is like the closest to

535
00:41:03,440 --> 00:41:06,560
what you would call right, right accelerationism, although, you know, maybe without some of the

536
00:41:06,560 --> 00:41:12,080
political overtones. And so there is that component. There's also the effective part of it. And the

537
00:41:12,160 --> 00:41:16,400
effective part of it, it's sort of a half humorous reference, obviously, to effective altruism.

538
00:41:18,800 --> 00:41:21,120
And it's a little bit tongue in cheek, because it's like, of course, if you're going to have a

539
00:41:21,120 --> 00:41:25,120
philosophy, of course, you would like it to be effective. But, you know, but, but also look

540
00:41:25,120 --> 00:41:31,840
like EAC is like very much like EAC's enemy, right, the oppositional force that the thing that EAC was

541
00:41:31,840 --> 00:41:37,680
sort of formed to fight is actually, you know, specifically effective altruism. Right. And so

542
00:41:38,400 --> 00:41:43,200
it's also like, you also sort of, you know, use that term to the term effective to kind of kind

543
00:41:43,200 --> 00:41:48,880
of make that point, like this is in that world. And this is opposed to that. And, and, and the

544
00:41:48,880 --> 00:41:54,320
reason why like this is happening now, like the reason why the concept of effective accelerationism,

545
00:41:54,320 --> 00:41:57,280
you know, has kind of come into being. And by the way, that, you know, the people, this is not

546
00:41:57,280 --> 00:42:02,080
originally my formulation, this is, this is, there's, you know, kind of ultra smart Twitter

547
00:42:02,080 --> 00:42:09,680
characters, who I think are still mostly operating under assumed names. But there's Beth Jesus,

548
00:42:09,680 --> 00:42:14,880
and Bayes Lord are the two, the two, two of them that I know. And they're, you know, these are

549
00:42:14,880 --> 00:42:20,000
like top and Silicon Valley, you know, engineers, scientists, technologists. But, you know, at least

550
00:42:20,000 --> 00:42:25,360
for now, they're operating kind of under undercover pseudonym. So the reason this is happening now

551
00:42:25,360 --> 00:42:29,840
is because of what I, what I was describing earlier with AI, which is you have this, you have this

552
00:42:29,840 --> 00:42:33,680
other movement, you have this movement of what's sort of called sometimes it's used different

553
00:42:33,680 --> 00:42:40,800
terms AI risk, AI safety, AI alignment. Sometimes you'll hear the term X risk. You know, sometimes,

554
00:42:40,800 --> 00:42:44,800
and then this is sort of directly attached. This is all part of the, you know, EA world,

555
00:42:44,800 --> 00:42:50,240
the effective altruism world. And then, you know, the central characters of this other world are,

556
00:42:50,240 --> 00:42:55,040
you know, Nick Bostrom, Elisir Yadkowski, you know, the open philanthropy organization.

557
00:42:55,040 --> 00:42:59,600
And, you know, a bunch of these, a bunch of these kind of, you know, the sort of the AI,

558
00:42:59,600 --> 00:43:05,360
what we call the AI doomers running around, like the AI doomer movement is basically a

559
00:43:05,360 --> 00:43:10,640
part and parcel with the effective altruism movement. And, you know, AI existential risk has

560
00:43:10,640 --> 00:43:14,560
always been kind of the boogeyman of effective altruism, kind of going back, you know, over the

561
00:43:14,560 --> 00:43:20,320
20 year development of EA. And so anyway, that that EA movement is the movement, by the way,

562
00:43:20,320 --> 00:43:24,720
with lavish funding by like EA billionaires, which is which is part of the problem, by the way,

563
00:43:24,720 --> 00:43:31,280
who made all their money in tech, which is also amazing. But, you know, so you've got this funding

564
00:43:31,280 --> 00:43:36,160
complex, you've got this EA movement, you've got this attached AI risk safety movement,

565
00:43:36,160 --> 00:43:42,080
and now you've got like active lobbying, you know, sort of anti AI PR campaign. And so anyway,

566
00:43:42,080 --> 00:43:46,720
so effective effective acceleration is intended to be the polar opposite of that, it's intended to

567
00:43:46,720 --> 00:43:52,560
be the, you know, to head boldly and firmly and strongly and confidently into the into the future.

568
00:43:53,520 --> 00:43:58,320
You know, it's like, why, you know, why, why this form of positive accelerationism, you know,

569
00:43:58,320 --> 00:44:02,800
it's, there's a couple different layers of it. The founders of the, of the concept of the act

570
00:44:02,800 --> 00:44:06,000
have a thermodynamic, you know, kind of thing, which, which we could talk about, but it's kind

571
00:44:06,000 --> 00:44:10,560
of one layer down from our operate. The layer operate is more at the level of engineering. And

572
00:44:10,560 --> 00:44:14,480
when I think about it, I think in terms of essentially fundamentally of material conditions.

573
00:44:14,480 --> 00:44:21,680
So human flourishing, quality of life, standard of living of human beings on earth. And back to

574
00:44:21,680 --> 00:44:26,320
that concept of productivity growth, you know, the application of technology, to be able to

575
00:44:26,320 --> 00:44:30,640
cause the economy to be more productive and therefore cause more material wealth, higher

576
00:44:30,640 --> 00:44:34,720
levels of material welfare, you know, for people all over the world, by the way, also with reduced

577
00:44:34,720 --> 00:44:39,520
inputs, right. And so not just greater levels of development and greater levels of advance,

578
00:44:39,520 --> 00:44:44,400
but also greater levels of efficiency. And the nature of technology as a lever on the physical

579
00:44:44,400 --> 00:44:47,680
world is you can have your cake and eat it too, you can get higher levels of output with lower

580
00:44:47,680 --> 00:44:50,400
levels of input. And the result of that is a much higher standard of living. So,

581
00:44:50,960 --> 00:44:56,400
so I kind of adopt my, my philosophical grounding is sort of, you know, I don't know if I call it

582
00:44:56,400 --> 00:45:01,440
like a positive materialism or something, you know, which is like, I think the thing that we,

583
00:45:01,440 --> 00:45:04,560
the thing that the technology industry does best is improve material quality of life.

584
00:45:05,840 --> 00:45:09,440
I think that we should accelerate as hard into that as we possibly can. I think the quote,

585
00:45:09,440 --> 00:45:16,720
unquote risks around that are greatly exaggerated, if not, if not false. And, and, you know, I think

586
00:45:16,720 --> 00:45:20,480
the forces against basically technological progress, you know, they're like the environmental

587
00:45:20,480 --> 00:45:24,320
movement I described, you know, they're fundamentally sort of at some deep level,

588
00:45:24,320 --> 00:45:28,800
they're sort of anti-human, you know, they want fewer people and they want a lower quality

589
00:45:28,800 --> 00:45:31,120
living on earth. And like, I just, I very much disagree with both of those.

590
00:45:32,160 --> 00:45:37,280
And what is this at the thermodynamic level? Is this, is this the, you know, we are ultimate

591
00:45:37,280 --> 00:45:43,440
enemy is entropy? So there's, there's a thermodynamic part gets complicated. And this is

592
00:45:43,440 --> 00:45:46,640
not my, my field. So there's, there's other people that you should probably have on to talk

593
00:45:46,640 --> 00:45:51,600
about this, but the effect of accelerationism version of the thermodynamic thing is, is based

594
00:45:51,600 --> 00:45:57,040
on the work of this physicist named Jeremy England, who is this very interesting character

595
00:45:57,600 --> 00:46:05,040
actually, he's actually trained by one of my partners. And is now basically, he's an MIT,

596
00:46:05,040 --> 00:46:09,760
you know, physicist, you know, biologist, and by the way, and also by the way, interesting guy,

597
00:46:09,760 --> 00:46:12,800
I don't know him, but a very interesting guy from the distance, he's also a trained rabbi.

598
00:46:14,080 --> 00:46:18,640
And so he's an interesting cat. And so he basically has this theory that basically,

599
00:46:19,200 --> 00:46:24,080
basically it's, it's sort of life is the direct result life, life, like the phenomenon of life

600
00:46:24,080 --> 00:46:29,440
itself is a direct consequence of thermodynamics. And, you know, the way he describes it is

601
00:46:29,440 --> 00:46:35,200
basically, basically, if you take, you know, basically the universe with a level of energy

602
00:46:35,200 --> 00:46:40,080
that's washing around and raw materials, and you sort of apply kind of natural selection at a very

603
00:46:40,080 --> 00:46:45,120
deep level, you know, you know, even at the level of just like the formation of materials,

604
00:46:45,120 --> 00:46:49,600
like on a planet or something, you basically have this thing where basically a matter wants

605
00:46:49,600 --> 00:46:55,040
to organize itself into states where it's able to absorb energy and achieve higher levels of

606
00:46:55,040 --> 00:47:00,560
structure. And so you have absorption of energy, you have achievement of higher levels of structure,

607
00:47:00,560 --> 00:47:04,480
in the case of organic life, that's, you know, starts with our basic RNA, and then kind of works

608
00:47:04,480 --> 00:47:09,200
this way up to, you know, full living systems. And then on the other side of that, as we talked

609
00:47:09,200 --> 00:47:13,520
about before, on the other side of that is your, the result of that is your sort of your dumping

610
00:47:13,520 --> 00:47:18,720
heat, which is to say entropy, you know, kind of out into the broader system. And so it's almost

611
00:47:18,720 --> 00:47:24,000
like saying the second law of thermodynamics has an upside, right, which is basically, yes, entropy

612
00:47:24,000 --> 00:47:29,040
in the universe is increasing over time, but a lot of that increases the result of structures

613
00:47:29,040 --> 00:47:34,640
forming that are basically absorbing energy and then exporting entropy. And one form of that

614
00:47:34,640 --> 00:47:39,760
structure is actually life. And this, and this is actually a thermodynamic, you know,

615
00:47:39,760 --> 00:47:44,080
biomechanical, bioelectrical kind of explanation of actually how organic life works. Like this is

616
00:47:44,080 --> 00:47:48,480
what we are, we are machines for gathering energy, you know, forming increasingly, you know, complicated

617
00:47:48,480 --> 00:47:53,600
biological machines, replicating those machines, right. And of course, you know, he talks about

618
00:47:53,600 --> 00:47:57,280
like, you know, natural selection, like it's not surprising that natural selection is so oriented

619
00:47:57,280 --> 00:48:00,800
around replication, right, because replication is the easiest way to generate more structure.

620
00:48:01,440 --> 00:48:05,920
Right. Like replication is the way that a system that is basically in business to

621
00:48:05,920 --> 00:48:09,200
generate structure, it's the way that it can most efficiently generate more structure.

622
00:48:10,480 --> 00:48:16,400
And so anyway, basically, the universe wants us to basically be alive. The universe wants us to

623
00:48:16,400 --> 00:48:22,240
become more sophisticated. You know, the universe wants us to replicate. You know, the universe

624
00:48:22,240 --> 00:48:26,160
feeds us and, you know, an essentially a limited amount of energy and raw materials with which to

625
00:48:26,160 --> 00:48:32,080
do that. You know, yes, we dump entropy out the other side, but we get structure and life,

626
00:48:32,080 --> 00:48:34,400
you know, to basically, to basically compensate for that.

627
00:48:35,360 --> 00:48:40,160
The universe is a, the universe is pronatalist and kind of Nietzsche in there as well.

628
00:48:40,960 --> 00:48:45,600
Yeah, exactly. 100%. Yeah. So anyway, so that's, that's the, that's the thermodynamic,

629
00:48:45,600 --> 00:48:50,240
that's the thermodynamic underpins of effective accelerationism. The people who have encountered

630
00:48:50,240 --> 00:48:54,880
effective acceleration, effective accelerationism, some of that get very, some of them get very

631
00:48:54,880 --> 00:48:58,720
deeply into that. And there's a very deep kind of well there to, to draw from this guy, Jeremy

632
00:48:58,720 --> 00:49:02,080
England has a book out. Actually, you'll appreciate this. This guy, Jeremy England has a book out

633
00:49:02,080 --> 00:49:07,120
and the title of the book is something like every life is on fire. And it's actually funny,

634
00:49:07,120 --> 00:49:11,040
because it's like, if you read Heraclitus, you're like, Oh my God, you know, he saw it.

635
00:49:12,480 --> 00:49:17,040
Right. Like, it's like, there's something very, very deep going on here with this sort of

636
00:49:17,040 --> 00:49:21,920
intersection of energy life. But so he's got this book out, which apparently is quite good.

637
00:49:22,880 --> 00:49:26,640
And so some people in effective acceleration kind of, kind of go deep. There's a tongue-in-cheek

638
00:49:26,640 --> 00:49:30,800
reference to the so-called thermodynamic God, right, which is not, you know, which is not a

639
00:49:30,800 --> 00:49:34,800
literal, you know, religious, in the literal religious sense, like a, you know, sort of

640
00:49:34,800 --> 00:49:38,640
a conscious God or a sentient God, but more of this, this idea that the universe is, is, is,

641
00:49:38,640 --> 00:49:42,960
is sort of designed to express itself in the forms, you know, basically in higher and higher

642
00:49:42,960 --> 00:49:47,360
forms of life. Yeah, to your point, like there's an obviously direct niche in connection.

643
00:49:47,680 --> 00:49:52,800
And, you know, so maybe, maybe he saw a lot of this too. You know, and obviously he, you know,

644
00:49:52,800 --> 00:49:56,480
he was obviously writing and thinking at the same time Darwin was figuring a lot of this out on the,

645
00:49:56,480 --> 00:50:00,960
on the natural selection evolution side. Yeah, so there's that. But, but having said that,

646
00:50:00,960 --> 00:50:06,160
like, like I said, my take on it is more, you know, I find that stuff fascinating. I'm more

647
00:50:06,160 --> 00:50:09,760
naturally inclined as an engineer, more naturally inclined towards the material side.

648
00:50:10,640 --> 00:50:14,640
And so I just more naturally think in terms of the, the social systems and the technological

649
00:50:14,640 --> 00:50:19,360
development and the impact on, on, on, on material quality of life. And so I think you

650
00:50:19,360 --> 00:50:23,280
can also just take it at that level and not, not, not have to get all the way down into thermodynamics

651
00:50:23,280 --> 00:50:27,120
if you don't want to. I mean, there's an odd, I mean, yeah, drawing it down to this level of

652
00:50:27,120 --> 00:50:30,640
engineering, well, not down to, but just to this level of engineering, there's this odd

653
00:50:30,640 --> 00:50:34,720
learned helplessness. And I mean, just to take the two examples we've given so far. So, and,

654
00:50:34,720 --> 00:50:39,680
you know, they work quite well actually nuclear energy on the atomic side and AI on the bit side

655
00:50:39,760 --> 00:50:45,600
of things, virtual, I guess, virtual reality and reality. You posted this really interesting

656
00:50:45,600 --> 00:50:50,400
essay on your, on your blog about availability cascades, which is about basically, in short,

657
00:50:50,400 --> 00:50:58,240
if I'm getting this right, this idea of why are so many people interested in this thing or this

658
00:50:58,240 --> 00:51:04,080
view of whatever the, the opinion or the idea is that's floating around. And it seems on both of

659
00:51:04,080 --> 00:51:09,680
those, both nuclear energy and AI, we have that same opinion, which is like, mimetically infected

660
00:51:09,680 --> 00:51:13,920
culture of a sort of learned helplessness, like, oh, no, you know, we've already spoken about this

661
00:51:13,920 --> 00:51:17,600
a bit, but like, oh, no, we need to get rid of this, we can't deal with this. But it seems,

662
00:51:17,600 --> 00:51:21,680
do you think on the engineering side of things, and I guess it overlaps also into the social in

663
00:51:21,680 --> 00:51:28,720
terms of how you engineer and how you promote these ideas socially as, as tools, as things that

664
00:51:28,800 --> 00:51:35,520
people use is an attempt to like, invert that availability cascade and like, try to like,

665
00:51:37,040 --> 00:51:43,360
begin some mimesis on the side of like, it's okay to want a better quality of living, it's okay to

666
00:51:43,360 --> 00:51:49,200
want to grow, it's okay to want energy, like, you don't have to be almost like submissive to,

667
00:51:49,200 --> 00:51:54,800
to whatever this strange, self-defeating learned helplessness is that we have in terms of

668
00:51:54,880 --> 00:52:00,880
technology and our like, our like weird allegiance to just this, this stagnant comfort that we've

669
00:52:00,880 --> 00:52:04,960
had for too long. Yeah, that's, that's, that's right. That's exactly right. And like I said,

670
00:52:04,960 --> 00:52:08,160
like we said, like we talked about earlier, like, there's, there's this, I think there's a natural

671
00:52:08,160 --> 00:52:13,040
human impulse deeply wired into like the limbic system or something, which is basically, right,

672
00:52:13,040 --> 00:52:18,160
fear over hope, right? You know, like, what's most likely to come over the ridge, right,

673
00:52:18,160 --> 00:52:21,520
a sabre to tiger to eat you or like something warm and cuddly that wants to be your friend,

674
00:52:21,520 --> 00:52:26,080
right? I guess a quack or something like that, right? So, right, it's, it's probably the tiger,

675
00:52:26,080 --> 00:52:29,680
right? And you know, there's, there's a sort of, you know, false positive, false negative,

676
00:52:29,680 --> 00:52:32,960
right, two ways of making mistakes. And you definitely, from an evolutionary standpoint,

677
00:52:32,960 --> 00:52:37,040
want to err in the direction of being, you know, more, you know, more, you want to overestimate

678
00:52:37,040 --> 00:52:43,600
the rate of Cybertooth Tigers, right, to, to, to survive. So, so, so that, that impulses deep.

679
00:52:43,600 --> 00:52:46,720
Yeah. But then, you know, what we have is, you know, we have, we have sentience, we have the,

680
00:52:46,720 --> 00:52:50,240
you know, the, we're not just limbic systems anymore, we have a, we have the ability to control

681
00:52:50,240 --> 00:52:54,080
environment, the ability to build tools, we're not afraid to save the two tigers anymore.

682
00:52:55,200 --> 00:52:59,920
And so, yeah, we have the ability to shape our world. You know, we develop rationality and

683
00:52:59,920 --> 00:53:03,520
the enlightenment and science and technology and markets and everything else to be able to control

684
00:53:03,520 --> 00:53:08,240
the world, you know, to our benefit. And so, we, you know, we don't, we don't have to live

685
00:53:08,240 --> 00:53:13,760
cowering in fear anymore, you know, as much as, or as much as that might be like grimly satisfying,

686
00:53:13,760 --> 00:53:17,680
like we don't actually have to do that. And there's actually a, you know, very, there are many,

687
00:53:17,680 --> 00:53:20,880
many good reasons over the last 300 years to believe that, you know, there's, there's a much

688
00:53:20,880 --> 00:53:25,680
better way to live. Yeah, but look, somebody has to say, you know, somebody has to actually say that.

689
00:53:26,560 --> 00:53:32,560
And then look, I think the other part is, I think there's a big divide. I think there's a big divide

690
00:53:32,560 --> 00:53:37,280
between, I'll pull up my, my Burnham on this a little bit is, is a big divide on this stuff

691
00:53:37,280 --> 00:53:42,400
between what you describe as the elites and the masses that has turned out to be pretty interesting.

692
00:53:42,400 --> 00:53:47,200
So the, the, I would say this problem, this problem of fear of technology,

693
00:53:48,000 --> 00:53:52,400
or hatred of technology or desire to stop technology, I think it's primarily a phenomenon of the

694
00:53:52,400 --> 00:53:58,640
elites. I actually don't think it's particularly shared by the masses. And it just seems like

695
00:53:58,640 --> 00:54:03,360
I just take AI as an obvious example. One of the amazing things about AI is it's like freely

696
00:54:03,360 --> 00:54:07,840
available for use by everybody in the world right now today, fully state of the art, like the best

697
00:54:07,840 --> 00:54:14,800
AI in the world is on, you know, websites from open AI and Google and Microsoft. And you can go

698
00:54:14,800 --> 00:54:19,200
on there and you can use it for free today. And people, you know, hundreds, hundred, already

699
00:54:19,200 --> 00:54:22,000
a hundred, 200 million people, something like that around the world are already doing this,

700
00:54:22,000 --> 00:54:26,560
right? And if you talk to anybody who's, you know, if you talk to any teacher, right, you know,

701
00:54:26,560 --> 00:54:29,520
you know, they'll already tell you they've got students using chat GPD to write essays and so

702
00:54:29,520 --> 00:54:35,440
forth. Right. And so you've got this amazing thing where, you know, like the internet before it

703
00:54:35,440 --> 00:54:40,080
and like the personal computer before it and like the smartphone before it, AI is, it's like

704
00:54:40,080 --> 00:54:44,880
immediately democratized, right? Like it's immediately available in its full state of the art

705
00:54:44,880 --> 00:54:50,560
version. Like there's no more advanced version of like GPT that I can buy for a million dollars than

706
00:54:50,560 --> 00:54:56,400
you can get for free or by paying 20 bucks for the upgraded version on the open AI website.

707
00:54:56,400 --> 00:55:02,400
Like the state of the art stuff is fully available for free. And so you have people all over the

708
00:55:02,400 --> 00:55:05,440
world. And this is one of my, this would be a source of optimism that the AI doomers are going

709
00:55:05,440 --> 00:55:09,040
to lose almost by definition, right? Is you have people all over the world who are just already

710
00:55:09,040 --> 00:55:12,400
using this and they're getting, you know, great value out of it in their daily lives. They love

711
00:55:12,400 --> 00:55:16,000
it. They're having a huge amount of fun with it. You know, it's great. They're good, you know,

712
00:55:16,000 --> 00:55:19,200
making new art and they're, you know, doing all kinds of, you know, asking all kinds of things

713
00:55:19,200 --> 00:55:22,400
and it's helping them in their jobs and in school and everything else and they love it. So,

714
00:55:22,960 --> 00:55:27,600
so I think there's this thing where like, I actually think that what we, what we're actually

715
00:55:27,680 --> 00:55:31,200
talking about from a social standpoint is basically essentially a corrupt elite,

716
00:55:32,240 --> 00:55:36,640
a corrupt oligarchic elite that basically has been in a position of gatekeeping power,

717
00:55:36,640 --> 00:55:41,600
you know, for, you know, basically in its modern form for 60 years. And every new technology

718
00:55:41,600 --> 00:55:46,400
development comes along as a threat to that. And back to the Morrison thing like that,

719
00:55:46,400 --> 00:55:50,720
that's why they hate and fear new technology. You know, they would very much like to control it.

720
00:55:50,720 --> 00:55:54,000
You know, it's like social media, like they're all just like completely furious about social

721
00:55:54,000 --> 00:55:57,280
media, but like, you know, 3 billion people use social media every day and they love it.

722
00:55:58,320 --> 00:56:02,320
And so it's only the elites that are constantly kind of raging against it. The problem is,

723
00:56:02,320 --> 00:56:06,240
the elites are actually in charge of right from a, from a formal, you know, government,

724
00:56:06,240 --> 00:56:10,560
like they actually have the ability to write laws. By the way, you also see this in polls.

725
00:56:11,280 --> 00:56:16,560
If you pull on, like there's two, two, two very interesting kind of phenomena,

726
00:56:16,560 --> 00:56:20,720
phenomena kind of unfolding if you do these broad based polls on trust in institutions.

727
00:56:21,200 --> 00:56:25,760
And there's organizations Gallup in particular, and then there's another organization called Edelman

728
00:56:25,760 --> 00:56:29,600
that does these polls every year of basically essentially, they pull regular people and the

729
00:56:29,600 --> 00:56:33,200
question is like, which institutions do you trust? And that the institutions here includes

730
00:56:33,200 --> 00:56:37,200
everything from the military to, you know, religion to schools to government to, you know,

731
00:56:37,200 --> 00:56:42,480
journalism to, you know, big companies, big tech, and so forth, small business. And basically,

732
00:56:42,480 --> 00:56:48,880
the two big themes you see in those polls is one is ordinary people trust in institutions,

733
00:56:48,880 --> 00:56:54,000
trust in any sort of centralized gatekeeping function is been in basically secular decline

734
00:56:54,000 --> 00:56:57,680
basically since the 1970s, corresponding to the period we've been, we've been talking about.

735
00:56:58,720 --> 00:57:02,400
And so generally, people as a whole have kind of had it with the gatekeepers,

736
00:57:03,680 --> 00:57:06,880
which is very interesting. And by the way, that phenomenon, actually, the beginning of that

737
00:57:06,880 --> 00:57:12,480
predates the internet and social media. And so that traces back to the early 70s. And so I think

738
00:57:12,480 --> 00:57:17,360
that which I think is not an accident. It was just like, it's where the current regime basically

739
00:57:17,360 --> 00:57:22,800
essentially took control. And then the other thing that's so striking is that, you know, although

740
00:57:22,800 --> 00:57:27,200
you can you can sit and read the news all day long, and where they just like hate on tech

741
00:57:27,200 --> 00:57:32,160
companies all day long, if you do the poll of, you know, basically businesses by category,

742
00:57:32,160 --> 00:57:37,520
tech polls by far at the top. And so again, ordinary people are just like, wow, my iPhone's

743
00:57:37,520 --> 00:57:42,960
pretty cool. I kind of like it. And the strategy PT thing seems really nifty. And so I do think

744
00:57:42,960 --> 00:57:48,240
there's this weird, like I do think there is this this aspect of this where like, it's a cliche to

745
00:57:48,240 --> 00:57:50,560
say the elites are out of touch, of course, the elites are out of touch, the elites are always

746
00:57:50,560 --> 00:57:54,400
out of touch. But like, it seems like the elites are particularly out of touch right now, including

747
00:57:54,400 --> 00:57:59,920
on this issue. And another way kind of through this not whole is, you know, they may just simply

748
00:57:59,920 --> 00:58:05,440
discredit themselves. Like the EU is a great example, the EU may pass this anti AI law, and the

749
00:58:05,440 --> 00:58:10,720
population of Europe might just be like, what's the hell? Right. And so that that's that would be

750
00:58:10,800 --> 00:58:16,160
another white pill against what otherwise looks like a deep kind of drive in our society for

751
00:58:16,160 --> 00:58:21,360
stagnation. It would also be really strange to try and find a way to like define AI in that sense,

752
00:58:21,360 --> 00:58:26,320
because it's not like we haven't been, we haven't been using it in a minor form before all this

753
00:58:26,320 --> 00:58:30,080
for a while, right? So I don't know how they'd go about defining that in that way.

754
00:58:31,040 --> 00:58:36,800
Yes. So, yes. So do you ban linear algebra? Right? Do you ban linear algebra? And it's

755
00:58:36,800 --> 00:58:39,440
actually really funny, because I don't know if you know this, there actually is a push under

756
00:58:39,440 --> 00:58:42,880
way to quote unquote ban algebra. And it's literally in California, there's a big push

757
00:58:42,880 --> 00:58:47,440
underway to drive it out of the schools in California. So there's a big push first to

758
00:58:47,440 --> 00:58:50,320
start with a push to drive calculus out of the schools in California, and now it's extended

759
00:58:50,320 --> 00:58:54,800
to drive algebra out. And of course, this is being done under the so called rubric of equity,

760
00:58:54,800 --> 00:58:59,040
right? Because it turns out, you know, test scores for advanced math, you know, vary by,

761
00:58:59,600 --> 00:59:05,200
you know, vary by group. And so, you know, there's this weird thing where like in California,

762
00:59:05,200 --> 00:59:09,360
we're trying to push algebra out of the school. In Washington, we're trying to push algebra like

763
00:59:09,360 --> 00:59:14,000
out of tech. Like the whole thing is, and this is where I get like really, you know, this is

764
00:59:14,000 --> 00:59:17,200
where I start to get emotional, because it's like really like we spent, you know, 500 years

765
00:59:17,200 --> 00:59:20,320
climbing our way out of, you know, primitivism and getting to the point where we have like

766
00:59:20,320 --> 00:59:25,120
advanced science and math, and we're literally going to try to ban it. I was involved, I was

767
00:59:25,120 --> 00:59:28,800
involved, if you remember this, there was actually a similar push like this, there was a push in

768
00:59:28,800 --> 00:59:37,600
the 1990s to ban cryptography, right, to ban the idea of codes, right, ciphers, right. And as you

769
00:59:37,600 --> 00:59:42,720
probably know, like codes and ciphers are just math, like all they are is math, right. And there was

770
00:59:42,720 --> 00:59:46,240
a move in the 1990s where people who thought that cryptography, obviously, you know, there's all

771
00:59:46,240 --> 00:59:50,400
these anti-cryptography arguments, because like bad guys can use it to hide and so forth. And so,

772
00:59:50,400 --> 00:59:53,520
there was this like concerted effort by the US government and other Western governments to

773
00:59:53,520 --> 00:59:57,280
ban cryptography in the 90s. And it took us years to fight and defeat that. And I was like, okay,

774
00:59:57,280 --> 01:00:01,360
that was so stupid, that will certainly never happen again. And like we're literally back at

775
01:00:01,360 --> 01:00:07,920
trying to ban math again. Well, that does lead me just to the final question here,

776
01:00:09,040 --> 01:00:12,880
which is to do with the future. I mean, whether or not it's your optimistic,

777
01:00:12,880 --> 01:00:16,480
pessimistic in relation to, you know, I guess it would draw in on what we've just been talking

778
01:00:16,480 --> 01:00:20,800
about there. How do you envision the short term future, which I've put down here like 10 to 50

779
01:00:20,800 --> 01:00:29,680
years, and then how do you, what do you foresee for the year 3000 AD? Oh boy. So, I should start

780
01:00:29,680 --> 01:00:35,200
by saying I'm not a utopian. So, you know, we talked a little bit early about kind of these

781
01:00:35,200 --> 01:00:39,360
impulses, the kind of dry people to these kind of extreme points of view. Like the way I think

782
01:00:39,360 --> 01:00:45,120
about it is like there's a natural drive. A lot of people have what Thomas Soll called the unconstrained

783
01:00:45,120 --> 01:00:48,640
vision, so that they've got these kind of very broad based kind of visions. And you know, those

784
01:00:48,640 --> 01:00:53,760
visions kind of then split into like a utopian vision. And that might be, you know, in for AI,

785
01:00:53,760 --> 01:00:57,600
that might be something like the singularity, right? Or in the 1990s, these were called the

786
01:00:57,600 --> 01:01:02,960
Extropians, right, which is sort of this idea of kind of a material utopia as a consequence of like

787
01:01:02,960 --> 01:01:08,000
AI and let's say nanotechnology on the one hand. And that's where that, by the way, the idea of

788
01:01:08,000 --> 01:01:11,520
the singularity came from, right, which is Ray Kurzweil and Bernard Vinge, they were like at some

789
01:01:11,520 --> 01:01:17,200
point you get this kind of, you know, point of no return, which is like a utopian point of no return.

790
01:01:17,200 --> 01:01:22,560
But then of course, the flip side of every utopia is, you know, apocalypse. And so then that's where

791
01:01:22,560 --> 01:01:26,880
the sort of AI, you know, sort of the Singulitarians 20 years ago have become that, you know,

792
01:01:26,880 --> 01:01:30,800
a lot of them have become the AI doomers of today. And they, you know, they have sort of this sort

793
01:01:30,800 --> 01:01:34,400
of, you know, they have the same utopian impulse, they've just flipped the bit and made it negative.

794
01:01:35,120 --> 01:01:40,480
So I should say like, I'm not one of those. I'm probably more of a materialist and a little bit

795
01:01:40,480 --> 01:01:44,800
more of a, like I said, an engineer where, you know, things, for example, have constraints in

796
01:01:44,800 --> 01:01:50,720
the real world. So I don't think we tend to get the extreme, quite the extreme outcomes, but I do

797
01:01:50,720 --> 01:01:54,400
think we get, you know, we get change, like we get, we get change, we get change in the margin,

798
01:01:54,400 --> 01:01:57,520
and then, you know, change in the margin that compounds over time can become, you know, quite,

799
01:01:57,520 --> 01:02:04,320
quite striking. So look over 10 to 50 years, you know, look sitting here today, like, if we want it,

800
01:02:05,600 --> 01:02:10,160
you know, you can imagine the next 50 years to be characterized by, you know, the rise of AI,

801
01:02:10,160 --> 01:02:15,120
looks like we kind of figured that out now. You know, this superconductor thing, if it's real,

802
01:02:15,120 --> 01:02:19,200
that's a, you know, turning point moment. And by the way, if it's not real, it may be, you know,

803
01:02:19,200 --> 01:02:22,320
that this result points us in the direction of something that becomes real in the next few years.

804
01:02:23,040 --> 01:02:28,720
And so you can imagine some combination of AI, superconductors, you know, biotech,

805
01:02:28,720 --> 01:02:32,960
you know, you know, all these new techniques for, you know, biooptimization, gene editing,

806
01:02:34,320 --> 01:02:38,800
you know, and then, you know, nuclear, you know, if we get our act together on nuclear fission,

807
01:02:38,800 --> 01:02:41,760
by the way, there's a lot of really smart people working on nuclear fusion right now.

808
01:02:43,040 --> 01:02:46,480
You know, fusion is, you know, would be an even bigger, you know, kind of opportunity

809
01:02:46,480 --> 01:02:51,280
for unlimited clean energy. You know, now, you know, my cynical, the cynic in me would say

810
01:02:51,280 --> 01:02:55,200
if fission is illegal, then they're certainly going to make fusion illegal. But, you know, that,

811
01:02:55,200 --> 01:02:58,720
you know, that's a choice. We all get to decide whether we want to live in a world where fusion

812
01:02:58,720 --> 01:03:03,840
is illegal. So, you know, we get nuclear fusion. And so sitting here 50 years from now, you know,

813
01:03:03,840 --> 01:03:09,040
we basically are like, wow, you know, we are like, we have, you know, we are all much smarter

814
01:03:09,040 --> 01:03:11,760
than we were because we have these smart machines working with us and everything.

815
01:03:12,640 --> 01:03:16,880
You know, we have solved whatever environmental problems we thought we had, you know, with,

816
01:03:16,880 --> 01:03:20,320
we have abundant energy in an increasingly clean environment.

817
01:03:21,200 --> 01:03:26,160
You know, we're curing diseases at a rapid pace. And, you know, new babies are born that are immune

818
01:03:26,160 --> 01:03:31,040
to disease. And so, you know, you know, not quite a material utopia, but like, you know,

819
01:03:31,040 --> 01:03:35,200
a significant, you know, meaningful step function upgrades in human quality of life.

820
01:03:35,200 --> 01:03:38,960
Like, I think that's all very, over a 50-year period, for sure, like that. And that's all very

821
01:03:38,960 --> 01:03:44,000
possible. Over a 3,000, over whatever the year 3,000, over a 1,000-year period. I mean, look,

822
01:03:44,560 --> 01:03:48,560
you do get into these questions, you know, you do, if you're going to talk about 1,000 years,

823
01:03:48,560 --> 01:03:52,240
like you do get into these questions of like, you know, for example, a merger of man and machine,

824
01:03:52,240 --> 01:03:56,160
right? So you do have to, over that time frame, you have to start thinking about things like,

825
01:03:56,160 --> 01:04:00,640
you know, the neural link, you know, like where neural link takes you. And, you know, you know,

826
01:04:00,640 --> 01:04:03,920
over that period of time, you know, you'll definitely have like, you know, neural augmentation.

827
01:04:03,920 --> 01:04:08,880
So, you know, do you have shifting definitions of humanity? You know, where is the transhumanist

828
01:04:08,880 --> 01:04:13,280
movement actually taking us? You know, becomes a very interesting question over that time frame.

829
01:04:14,000 --> 01:04:17,600
Obviously, you have lots of questions over that time frame of space, you know, exploration,

830
01:04:17,600 --> 01:04:21,280
getting to other planets, you know, other life, you know, either other life in the universe or

831
01:04:21,280 --> 01:04:24,560
not other life in the universe. So kind of the spread of the, you know, the spread of our civilization

832
01:04:24,560 --> 01:04:30,160
more broadly. You know, so there you truly get into science fiction scenarios. You know, then,

833
01:04:30,800 --> 01:04:35,280
yeah, that's it. I'm always fun to talk about. I will admit, I am much more focused on the next

834
01:04:35,280 --> 01:04:40,400
50 years. Yeah, I mean, is there anything you'd like to add into the conversation

835
01:04:41,120 --> 01:04:43,280
that you feel, you know, is key that we haven't touched on?

836
01:04:44,080 --> 01:04:47,760
Yeah, no, I think that's a good, I think that was a good, covered a lot of ground.

837
01:04:47,760 --> 01:04:53,440
Yeah. So for effective accelerationism, just if you Google, there's a number of good already

838
01:04:53,440 --> 01:04:57,920
websites and substacks talking about that. A lot of the conversations happening on Twitter.

839
01:04:59,120 --> 01:05:05,120
So, and I already dropped the names of the of the EAT guys. So Beth Jesus and Bayes-Laurie,

840
01:05:05,120 --> 01:05:11,760
definitely follow those guys. You know, I've not met Nick Land, but I would definitely give a

841
01:05:11,760 --> 01:05:14,560
shout out and say, for anybody who hasn't encountered his work, they should definitely read

842
01:05:14,560 --> 01:05:21,040
up on it. He is, I think pretty clearly like the philosopher of our time and not even because,

843
01:05:21,040 --> 01:05:24,480
you know, whether I agree or disagree with him on everything he said. And of course,

844
01:05:24,480 --> 01:05:29,360
he's changed his views on a lot of things over time, but just the framework that he operates,

845
01:05:29,360 --> 01:05:35,120
like his willingness to actually go deep and actually think through the consequences of the

846
01:05:35,120 --> 01:05:39,440
kinds of technologies that I deal with every day, you know, are just, I think, way beyond most other

847
01:05:39,440 --> 01:05:44,720
people in his field. And so it's, it's, and I know he took kind of a long road to get here,

848
01:05:44,720 --> 01:05:49,360
so it's fun to see, you know, it's fascinating to read that. Oh, I'll point to one other thing.

849
01:05:49,360 --> 01:05:52,640
So I already mentioned the Jeremy England book, and I'll point to one other book that people

850
01:05:52,640 --> 01:05:57,360
might find interesting. So, so a lot of Lance work and a lot of accelerationism, right, is based on

851
01:05:57,360 --> 01:06:02,400
these, these, the ideas mob this field called cybernetics, which is kind of this, this, it's

852
01:06:02,400 --> 01:06:05,840
cybernetics is interesting because it's kind of this lost field of engineering.

853
01:06:06,960 --> 01:06:14,800
It was super hot in the, as an engineering field from the 1940s to the 1960s. And it basically was

854
01:06:14,800 --> 01:06:18,400
sort of the original computer science. And then it was sort of, it was also sort of the original

855
01:06:18,400 --> 01:06:22,560
artificial intelligence. A lot of the AI people of that era kind of call themselves cybernetics

856
01:06:22,560 --> 01:06:27,920
or cybernetics. But it really is an engineering field that kind of went away or got a lot more

857
01:06:27,920 --> 01:06:34,560
sedate after the 60s. And, but, but, but as I mentioned, like a lot of the ideas around AI and,

858
01:06:34,560 --> 01:06:37,840
you know, machine world of machines and thermodynamics, a lot of those ideas were being

859
01:06:37,840 --> 01:06:42,640
explored as far back as the 30s and 40s. So the cybernetics people of that era thought a lot

860
01:06:42,640 --> 01:06:46,240
about a lot of these questions. Anyway, there's this great book is a lot of original source

861
01:06:46,240 --> 01:06:50,960
material on this. And the, you know, the key character of that movement was Norbert Wiener,

862
01:06:50,960 --> 01:06:54,320
and there's a bunch of books by him and about him. But there's also a great book came out recently

863
01:06:54,320 --> 01:06:58,960
called Rise of the Machines by an author named Thomas Red. And it sort of reconstructs the

864
01:06:58,960 --> 01:07:03,680
archaeology of cybernetics and sort of makes clear how relevant those ideas are today.

865
01:07:04,720 --> 01:07:08,960
And so if you read that in conjunction with, with, with, with Nick Land's work, I think you'll find

866
01:07:08,960 --> 01:07:14,480
it pretty interesting. I'll be sure to put the link for your Twitter and your blog in the description

867
01:07:14,480 --> 01:07:18,800
below as well. But yeah, I think that's a good place to finish up. Mark Andreessen, thanks very

868
01:07:18,800 --> 01:07:24,400
much. Good, James, a pleasure. Thank you.

