start	end	text
0	1480	Mike, thanks for coming on.
1480	2320	Yeah, good to see you again.
2320	3280	Thanks for having me.
3280	4200	Yeah, such a pleasure.
4200	5320	Absolutely.
5320	8120	I can't believe we're on round three of our conversations
8120	8960	here right now.
8960	10160	Time flies by.
10160	13000	And for people in the audience who haven't yet
13000	14640	caught our first two rounds, those
14640	17280	will be linked below in the description.
17280	19680	And around one, we covered the computational boundary
19680	22760	of the self paper, the cognitive light cone diagram
22760	24840	that folks will be familiar with, I'm sure.
24840	27960	And then around two, we covered observer-dependent computing,
28000	31640	your paper with Joshua Bungard on polycomputing,
31640	35160	and also the technological approach to mind everywhere.
35160	37640	Today, we're going to cover a couple of papers
37640	41120	that were recently published just in the last couple of months.
41120	43200	Your paper on bioelectric networks
43200	46280	being the cognitive glue for organisms,
46280	49280	and then Darwin's engential materials.
49280	51080	That'll be the second paper we cover.
51080	54360	And then finally, biology, Buddhism AI,
54360	56440	your paper with collaborators that I think
56440	60880	will be the third act of our conversations today.
60880	63280	So I'd love to get us kicked off first.
63280	65160	And those papers will be linked below in the description
65160	67880	as well for folks who want to dive into those.
67880	69880	They can be pretty technical, but I
69880	73840	recommend going through them and discovering them
73840	74360	for yourselves.
74360	76440	There's a lot of great diagrams in there as well
76440	78840	to help people kind of grok these concepts too.
78840	83560	So if we get started off on the bioelectric networks,
83560	86480	the cognitive glue enabling evolutionary scaling
86480	89520	from physiology to mind, your paper,
89520	92240	could you provide us with a brief high level summary
92240	94160	of the paper and what's covered here?
94160	94640	Yeah.
94640	97200	Well, I guess the first thing to do
97200	99400	is to talk about what cognitive glue is
99400	101640	and why such a thing is needed.
101640	108120	And I use that term just to kind of draw attention
108120	109000	to the following thing.
109000	115520	We often people think about collective intelligences
115520	120840	as flocks of birds and colonies or termites or bees
120840	122160	or something like that.
122160	124520	And they contrast that sharply with themselves.
124520	126000	They say, well, I'm not a colony.
126000	131040	I'm a unified individual with my own thoughts and goals
131040	132040	and all of that.
132040	135600	But actually, if you sort of look inside, what you find
135600	137240	is that, no, actually, you are a colony.
137240	140680	Like each of us is a collection of cells, neurons
140680	142280	and a whole bunch of other stuff.
142280	146160	And I want to emphasize this idea
146160	149560	that that is not to be taken for granted.
149560	151080	You cannot take for granted.
151080	154560	It's actually kind of a miracle, not in the sort of religious
154560	156600	sense, but in the scientific sense of something really
156600	160080	profound that needs understanding and explanation.
160080	163600	It's kind of a miracle that a collection of individual cells
163600	165320	with their own agendas and their own ability
165360	168800	to pursue various goals and physiological space
168800	171280	and gene expression space and so on,
171280	173560	that there is a way to arrange those things that
173560	178800	gives rise to this emergent new self that operates
178800	182040	in a different problem space and, in fact, will end up
182040	183600	often will end up denying the fact
183600	186720	that it is made up of parts with their own agendas.
186720	188160	I mean, that's kind of wild.
188160	189080	That's interesting.
189080	189640	Right?
189640	194560	And has its own goals where some of these goals are often
194600	196640	at odds with the goals of the parts.
196640	199360	And we often do things that aren't particularly good
199360	201960	for certain cells in the body and so on, right?
201960	203920	Or even certain organs.
203920	206680	So that's it right there.
206680	210320	That it's very clear that there must be a mechanism
210320	217160	for doing that, for transitioning from just a pile of cells
217160	220880	to something with its own self.
220880	224120	And we can sort of define, we can do a little bit of definition
224120	225080	there too, if you want.
225080	227440	But so that's the goal, right?
227440	231000	So now, even though people don't think about that very much,
231000	233560	like that, the reality is that, of course,
233560	235840	the whole field of neuroscience is predicated on the fact
235840	238800	that we know what the cognitive glue is
238800	240400	for behavior in the brain.
240400	242840	It's electrical signaling.
242840	247320	And let's say electrochemical signaling in the brain.
247320	249560	So that's the idea.
249560	253880	And my point in this paper and in some previous papers
253880	258000	is that there's a reason why electrical signaling is so
258000	259400	good at this in the brain.
259400	262600	It's because it's had lots of practice evolutionarily
262600	265520	where this all comes from is by serving as cognitive glue
265520	268560	for a morphogenetic agent, which is the thing that
268560	272000	arises when a bunch of individual cells in an embryonic
272000	275320	blastoderm suddenly start to cooperate
275320	276720	toward a very specific goal.
276720	279360	They're all going on a journey in this anatomical space
279360	282200	of possible configurations, of possible shapes
282200	283160	that anything can be.
283360	285360	They're all committed to helping each other
285360	287360	get to one particular region of that space that
287360	290440	corresponds to the target morphology of that species.
292560	295880	And so, yeah, so that's what this paper is about.
295880	298240	It's about how bioelectricity serves
298240	301480	as that kind of cognitive glue and then evolution kind of
301480	304360	pivoted it to do the same thing in three-dimensional space
304360	306240	for the control of behavior.
306240	306840	Awesome, yeah.
306840	307840	Thank you for that summary.
307840	310240	And I'd love to hear, I mean, one of the second questions
310240	313720	I had here was, how did you land on the term cognitive glue?
313720	316560	Like, were there any other terms of phrases
316560	320320	that you were kind of deciding between?
320320	321760	For that one, I mean, I don't know,
321760	324640	I use a lot of kind of these kind of terms
324640	325800	that I just sort of come up with.
325800	329400	But for that one, I'm not sure if there were any competitors.
329400	332640	I mean, you can think about some sort of binding policies.
332640	337680	You can call it some sort of an self-emergence mechanism.
337680	339280	I mean, you can come up with it.
339280	341360	But I just thought it was simpler to point out
341360	345280	that it literally is a kind of cognitive glue,
345280	348480	because without it, think about what
348480	350840	happens during general anesthesia, right?
350840	356600	So you walk into the doctor, and there you are.
356600	358920	And you have all kinds of thoughts and hopes about what
358920	361120	happens, and you say, boy, I hope the surgery goes well.
361120	362720	I've got a big thing.
362720	364120	I've got to do a month through now.
364120	368040	And then the gas comes in, and one of the things that happens
368080	372520	is the gap junctions between your cells get inhibited.
372520	374440	So now you're gone.
374440	376520	For the next however many hours, you're not there.
376520	377760	Your cells are still there.
377760	379360	All the pieces are still there.
379360	380240	Nobody's damaged.
380240	380880	Nobody's dead.
380880	384840	The cells are all functional, but you're gone.
384840	387960	And one reason you're gone is because that cognitive glue
387960	389760	has been temporarily dissolved.
389760	392080	It's the there really needs to be something
392080	394320	that binds all this together.
394320	396600	And glue, it's kind of silly, because it's not
396600	397880	that physical kind of thing.
397880	403880	But this idea that everybody has to be kept together
403880	407080	and the way that glue does in a particular space.
407080	409440	It's not just physical proximity.
409440	410960	The cells stay close to each other.
410960	414320	It's a kind of informational proximity.
414320	417720	And actually, people like Giulio Tononi and others
417720	419560	actually study this from an information study,
419560	421760	that integration from an informational perspective.
421760	424000	But that's really what's needed is a mechanism
424000	428600	to hold together the cognitive system that
428600	430280	is then going to make claims about itself
430280	434640	being a separate agent from the parts that make it up.
434640	436680	Yeah, it's interesting.
436680	439240	One very specific question I had in the paper.
439240	442080	And it may have been in there, but maybe I just potentially
442080	442580	missed it.
442580	444720	But there's a quote in there.
444720	447440	If you don't mind me just reading it off here.
447440	450160	It's developmental bioelectricity as a precursor
450160	452880	of brain-like processes, which reveals not only
453160	456200	evolutionary pivots between two different problem spaces,
456200	458560	but also shows a path to solving the problem of collective
458560	461560	intelligence across scales of organization.
461560	464000	What are the two different problem spaces?
464000	466000	And perhaps it's just an example being made there.
466000	469880	But I think there's a mention here of evolutionary pivots.
469880	473680	Can you give the audience an example of what you mean by that?
473680	474080	Sure.
474080	476680	Well, so first, let's name some problem spaces here.
476680	481000	Just kind of roughly in order from the beginning of life.
481040	484840	So you've got the space of metabolism.
484840	486040	That's a problem space because you
486040	487040	have to figure out how to keep.
487040	488640	If you're going to persist in the real world,
488640	492000	you have to figure out how to operate metabolism.
492000	495600	And then there'll be some sort of physiological problem
495600	497440	space where, aside from metabolism,
497440	498840	you have other physiological features
498840	499960	that you're trying to outkeep.
499960	502160	And then at some point, you get a transcriptional space
502160	504800	where there's actually genes now that can be expressed.
504800	507760	And so that space, a very high-dimensional space
507760	509680	of the correct gene expression for whatever
509720	511480	you need to be doing at the moment.
511480	513960	And then when you get to multicellularity,
513960	516240	there's an anatomical morphospace.
516240	518400	How many eyes are you supposed to have?
518400	519520	Are you supposed to have eyes?
519520	521640	What's the where's the head go?
521640	523240	Where's the tail go?
523240	523960	Those kind of things.
523960	526960	So that's the space of that's an anatomical space.
526960	529560	And then after that, you get to behavioral space
529560	532480	because eventually you develop muscles and nerves.
532480	535680	And now you can actually move around in three-dimensional space.
535680	538560	And so now you can do what's classically known as behavior.
538560	539880	All these other things are behavior, too.
539880	542000	They're just behavior in weird spaces
542000	543720	that we don't normally think about.
543720	547600	And then maybe if you're some sort of human
547600	548960	or something else,
548960	551680	you might also operate in linguistic space.
551680	554120	So we actually have a project now looking at navigation
554120	555760	of linguistic space as exactly
555760	557880	that kind of navigational process.
557880	558720	Oh, interesting.
558720	559560	Yeah, yeah.
559560	561080	And maintaining this idea.
561080	563280	So there's one central concept to all of this,
563280	564360	which is navigation.
564360	566480	This idea that the space is rich,
566480	567440	it has structure.
567440	571720	You, as an agent, have various kinds of preferences
571720	573560	about which parts of that space are better for you
573560	574600	and which parts are worse.
574600	576160	And therefore you need to navigate it.
576160	578480	Now, you have different degrees of competency
578480	579320	of navigating it.
579320	582240	You might navigate it the way that a bowling ball
582240	584040	or a dandelion seed might,
584040	586600	which is you have very little internal control,
586600	588400	if any of what happens.
588400	590400	Or you might be some sort of something in between.
590400	593840	You know, there's various seeds with little cork screws
593840	595400	and things that kind of help them
595440	597040	do various simple things.
597040	599680	You might be some kind of a, you know,
599680	601080	a simple homeostatic agent,
601080	603080	like a little thermostat that's actually better
603080	604320	than these other things.
604320	606000	Or you might actually be a learning system
606000	610680	that can have anticipation and associative learning
610680	611520	and things like that.
611520	613720	Or you might be really complex and you might have planning
613720	617040	and you might be able to really think forward pretty well.
617040	619240	And then you might be really complex
619240	621200	and have a gigantic cognitive light cone
621200	624600	where you can sort of imagine lots of complex things
624640	628040	far into the future way outside of your current scenario.
628040	630600	So anywhere along that continuum,
630600	633040	there's some, you might have some competencies, right?
633040	635720	And you navigate that, those spaces.
635720	637280	And so what I mean by evolutionary pivots
637280	639240	is simply that once you're good
639240	641280	at navigating one kind of space,
642120	645600	it's relatively, I think, this is all, you know,
645600	647800	this is all kind of the framework that I work on.
648680	650840	So lots of things we don't know yet,
650840	653280	but I think it's relatively easy for evolution
653280	655200	to switch spaces on you.
655200	657440	Because if you're good at navigating
657440	659120	a particular kind of space,
659120	661920	we can swap some sensors and some effectors
661920	665520	and you can use all the competencies you have
665520	666720	to navigate some other space.
666720	668360	So just for example,
669800	670840	hybrids, right?
670840	672320	Hybrids are when you take a brain
672320	675080	and you put it in a robotic body such as a vehicle.
675080	679560	And so in that case, the brain instead of leg muscles
679560	681760	might be connected to some wheels
681760	684320	and instead of eyes, it might have some photo arrays.
684320	687360	And if the brain is good at doing these things,
687360	688720	and so people have made these things
688720	690640	and they have various behaviors.
690640	693800	And it sounds kind of crazy and wild,
693800	696960	except that that is the normal scenario.
696960	700080	You see, your brain doesn't actually interface with reality.
700080	701640	Your brain interfaces with your eyes
701640	703240	and your muscles and various things.
704320	706440	And all of this is highly plastic.
706440	711440	When we've made tadpoles where the eye is on its tail
711480	713120	instead of in the head,
713120	715680	those animals can see perfectly well immediately.
715680	718160	They don't require new evolutionary adaptation,
718160	719680	periods of evolutionary adaptation,
719680	721520	they could just see in this new configuration.
721520	724200	And that's because, and the same reason why
724200	726800	sensory augmentation and sensory substitution.
726800	729160	So this has been known since at least the 70s,
729160	731440	probably before that, that you can do all kinds of,
731440	733720	you can give humans all kinds of weird sensors
733720	736360	and effectors and they very quickly become part of them.
736360	739560	So they become, for example,
739600	741680	Rocky Rita in the 70s used to do this thing
741680	743400	where it's like, you know that toy,
743400	744240	I don't know what it's called,
744240	745520	but it's like the square thing
745520	747120	with a bunch of metal nails
747120	749840	and you put your hand on it and it kind of makes the imprint.
749840	752240	So you can imagine an inverse of that
752240	754760	where the little pegs go and move in and out.
754760	756600	So you take that thing and you put each peg,
756600	758400	you connect each peg to a pixel on a camera
758400	759720	that you wear on top of your head.
759720	761560	And then you take that thing
761560	763800	and you put it up against your belly
763800	766160	so that it's poking you based on the pixel
766160	767800	that nail is poking you or not, right?
767800	771920	And so he did experiments with people who were blind
771920	773720	who learned to navigate that way
773720	777320	because you can remap the information
777320	779360	that you're getting through your skin
779360	780960	because the plasticity is incredible.
780960	783120	Look at the rubber hand illusion.
783120	785320	You can see these videos online
785320	788200	where you've had, as a tetrapod,
788200	790280	you've had, your brain has known
790280	793200	how many hands you have for how many millions of years now.
793200	796240	And within seven minutes of this visual input
796240	797960	of somebody stroking this rubber hand,
797960	799520	you've now decided that you have three hands.
799520	801600	You're perfectly willing to abandon that prior.
801600	803720	And when somebody hammers the rubber hand,
803720	805080	people jump up and scream.
805080	807880	And so the plasticity is incredible.
807880	809760	And that's why when people get prosthetics
809760	812720	where the wrist goes 360 degrees around,
812720	814160	when they reach for a cup,
814160	817440	they'll rotate the way that your normal wrist never rotates
817440	819840	because they get used to it, that's your body now.
819840	824640	And so all of these things, this kind of plasticity.
824640	827280	So that's why I think evolution fundamentally
827280	829200	makes these kinds of systems
829200	833240	that they kind of figure out what they are on the flies
833240	836400	or like Josh Bongard's robots from what, 2006 or so,
836400	837880	when he made these robots
837880	841080	that didn't have a predetermined model of what they were
841080	843400	and where the effectors were and where the sensors were.
843400	846920	And they sort of flopped around like babies,
846920	848360	they flopped around until eventually
848360	850360	they figured out how to move around
850360	852160	because they built a model of themselves
852160	854160	and what their structure was.
854160	858520	And that has the awesome side effect
858520	860080	that if you rip off one of the legs,
860080	862840	it'll just go through a similar process
862840	865960	and remap to the new and then move in a different way,
865960	867440	given what you have now.
867440	870520	This is exactly what we see in biology
870520	874400	where with a very wide range of various
876840	880680	accommodations to novel circumstances can be had
880680	883720	because evolution makes these problem-solving machines
883720	886800	that are very good at defining
886800	888960	and redefining themselves on the fly.
888960	890000	So that's what I mean, right?
890000	892680	These tricks that work well in one world,
892680	894520	swap some stuff around, swap a time scale,
894520	896120	swap the sensors, swap the effectors
896120	898320	and now you're walking around in some other kind of space.
898320	900040	I have a feeling that that's what's happening.
900040	901720	That's my guess.
901720	902560	That's wild.
902560	905640	I'd never heard about this, the 2006 Bongard.
905640	906760	Oh yeah, look it up, it's great.
906760	910960	I think it's very, it's really foundational, yeah.
910960	913200	I have to do some digging on that for sure.
914280	915120	Thank you for mentioning.
915120	916840	And there's, yeah, so many great,
916840	919480	wow, so much awesome stuff I wanna unpack there.
921440	925520	One of which, well, a few things that I think keep coming up,
926480	928880	sometimes like a phrase or an idea
928880	931280	that from our discussions
931280	933280	or from listening to you on other podcasts,
933280	936280	you know, you say all intelligence is collective intelligence.
936280	937280	So I think it's really great.
937280	940760	I think the emphasis, a lot of people need to hear that.
940760	943720	The other thing, and I think you just touched on it a bit,
943720	947480	is that, or maybe in our previous discussions you did too,
947480	949120	we, a lot of people think of the brain
949120	951480	as having this like exalted status
951480	953440	of that the brain can do all these things
953440	955480	that other parts of the body can't do.
956560	958560	But what's, and it's in this paper as well
958560	960120	and other papers too,
960120	963280	but sometimes there's memory stored outside the brain,
963280	966320	you know, in some of the research that you've done.
966320	969920	Can you give us a sense of what does the brain do
969920	972920	that's actually different than the body cells?
972920	976920	So, yeah, so what does it have
976920	980640	that say other parts of the body can't do?
980640	982560	Yeah, yeah.
982560	986120	So as much as I try to lean on the commonalities
986120	988320	between brain and other tissues,
988320	989600	I mean, it's pretty obvious.
989600	990920	There's a reason why we have brains.
990920	992280	Brains give us extra features
992280	994160	that we wouldn't have otherwise.
994160	997560	And so just to, you know, just, I mean,
997560	999880	the most obvious one is speed.
999880	1003120	So neural bioelectricity is way faster
1003120	1004520	than developmental bioelectricity.
1004520	1006640	And I think that was part of an arms race
1006640	1008520	at the beginning when things started moving around
1008520	1011400	and trying to eat each other and avoid being eaten.
1011400	1012960	Speed became of the essence.
1012960	1014720	You know, I mean, developmental biology,
1014720	1017080	I mean, yes, you want to complete embryogenesis
1017080	1017920	as quickly as you can,
1017920	1019680	but it's not under the same speed constraint,
1019680	1021200	I think as actual behavior, you know,
1021200	1022720	three-dimensional behavior.
1022720	1024760	So there's speed in this idea.
1024760	1027080	Then there are the point connections.
1027080	1029160	So neurons can be incredibly long.
1029160	1031160	And so if you want to make a directed connection
1031160	1032360	from here over there,
1033360	1035520	without that neural architecture
1035520	1037600	of having an axon that reaches all the way down,
1037600	1039960	I mean, they can be meters long in some animals.
1040960	1045040	Without that, the basic bioelectric system is cumbersome
1045040	1047200	because it basically thinks spread as waves
1047200	1048800	or, you know, they propagate
1048800	1050120	through the gap junctional milieu.
1050120	1052320	It's not the same as, right?
1052320	1054960	So those are kind of the architectural things.
1054960	1058440	I mean, a lot of the components are conserved.
1058840	1062280	So ion channels, neurotransmitters, electrical synapses,
1062280	1063640	all that stuff is conserved,
1063640	1065440	but it's used in a different way,
1065440	1068880	both for speed and for direct connections, you know?
1068880	1071320	And then there's just, there are the things
1071320	1073840	that we know that are associated with brains
1073840	1075240	that we haven't found anywhere else.
1075240	1079200	So for just as a sort of high-end example is language, right?
1079200	1081920	So I've seen no credible claims
1081920	1084000	that the other organs in your body
1084000	1086560	are using this kind of syntactic language structures
1086560	1087840	that brains use.
1087840	1089120	Not saying it's impossible.
1089120	1091280	Who knows, we might find some kind of syntax,
1091280	1094320	but there isn't any evidence for that yet that I know of.
1094320	1097160	So that I think, you know,
1097160	1098920	but other basic stuff is the same,
1098920	1102720	you know, perceptual control and predictive coding
1102720	1105040	and all kinds of that happens in all cells.
1106080	1107440	I wanna come back real quick,
1107440	1108960	because I do think this is important
1108960	1111560	to something else that you said a minute ago
1111560	1113040	about the collective intelligence
1113040	1114480	and the fact that people need to hear that.
1114480	1117560	So I wanna just talk for a moment
1117560	1120120	about what it is that I think they need to hear about it.
1120120	1124600	Because some people, and I keep, I'm always fishing,
1124600	1126440	let's see if this helps at all,
1126440	1128560	but I'm always fishing for a better way to make this point.
1128560	1130560	And I don't know how effective this is.
1132520	1137520	Some people get really kind of destabilized
1137600	1138840	by these sorts of ideas,
1138840	1143840	because what they hear is, I'm not real, I'm an illusion.
1144120	1145400	And there are lots of scientists
1145400	1147200	that are pushing this narrative, right?
1147200	1149600	So we are to blame for this.
1149600	1151920	I'm not surprised that people have these ideas.
1151920	1154840	But, you know, this idea that, well, we don't exist.
1154840	1157160	You know, you're a big pile of cells
1157160	1159960	and you're not really here and it's all an illusion.
1159960	1162080	And that's a really destabilizing idea.
1162080	1163800	It's destabilizing on a personal level,
1163800	1165280	it contributes to the loss of meaning,
1165280	1168160	it contributes to societal issues.
1169120	1173680	And so I wanna just say what I think the lesson here is.
1174680	1177280	When you see science like this,
1177280	1182280	the lesson isn't that you are somehow devalued
1182720	1184640	from what you thought you were,
1184640	1188920	and that the meaning of your life is reduced,
1188920	1190640	and that your primary experience
1190640	1193200	of being a coherent being with choice,
1193200	1196200	with the responsibility of deciding what do you do next,
1196200	1199120	that these things are now out the windows somehow.
1199120	1201360	I think that's the wrong conclusion to draw from any of this.
1201360	1204160	The conclusion isn't that the majesty
1204160	1207280	of the integrated mind is reduced.
1207280	1209920	The right conclusion is, well, two things.
1209920	1213840	One is that actually we just learned an amazing thing
1213840	1215840	that matter can do.
1215840	1217000	We didn't know that before.
1217000	1220040	We thought dumb matter was just kind of dumb.
1221840	1223040	And what we're learning now,
1223040	1226520	we're not learning something that changes
1226520	1227360	how we view ourselves,
1227360	1229640	we're learning something that changes how we view matter.
1229640	1233200	And this is something that I think Ian McGilchrist says as well
1233200	1238040	that we've underestimated certain kinds of matter.
1239880	1241680	This idea that, no, actually,
1241680	1245200	and this is, I know I've used this example before,
1245200	1248040	but there's a scene in a lot of science fiction,
1248040	1250640	most recently I saw it in Ex Machina, right,
1250640	1254400	where the guy starts cutting his arm
1254400	1257880	because now he's worried that he's an android, right?
1257880	1260320	And I mean, yeah, a lot of people feel that way,
1260320	1264320	but if you cut your arm or you go get a CT scan
1264320	1265480	and they say, whoa, you know what?
1265480	1267280	You're full of cogs and gears.
1267280	1269120	The conclusion from that isn't,
1269120	1271440	oh man, well, I guess I'm not as real as I thought
1271440	1272280	and I'm not, you know,
1272280	1274040	I guess I don't get to use my free will now.
1274040	1275840	And that's not the answer.
1275840	1279440	The conclusion should be amazing.
1279440	1284360	Cogs and gears can give me my spiritual meaning like amazing.
1284360	1286760	I've just learned something about cogs and gears, great.
1286760	1289240	So that's, you know, I just want to be clear
1289240	1293520	that I think this kind of analysis of what it is
1293520	1295160	that we are, how we get here, you know,
1295160	1299400	the self-construction of the self from cells
1299400	1301200	during embryogenesis and all that,
1301200	1304600	that doesn't, it doesn't have any negative implications
1304600	1307200	for what you are and what you can do, just the opposite.
1307200	1310920	It sort of raises the remarkable magic
1310920	1315360	of these unbinding principles that you say, wild,
1315360	1319320	you can actually create a being that I know I am.
1319320	1320680	I mean, that's the part that I think Descartes
1320680	1322640	had had exactly right.
1324640	1327800	Apparently you can get there through this particular method.
1327800	1329560	And there was, I forget what it is,
1329560	1330800	but there was also another,
1330800	1333200	an old science fiction story where, you know,
1333200	1335560	these aliens kind of land on earth
1335560	1339160	and they find out that humans are basically made of meat.
1339160	1341560	And they say, that is the most disgusting,
1341560	1343400	like you're telling me a pile of meat
1343400	1345280	can have these exalted thoughts that we,
1345280	1347400	in our silicon, you know, implementations,
1347400	1348920	how like that's, there's no way.
1348920	1352200	There's no way a pile of carbonaceous agoo
1352200	1355000	is gonna have these kind of, you know, mathematical truths
1355000	1357200	that we perceive with our, with our silicon minds.
1357200	1362200	And right, and it's all completely arbitrary.
1362800	1367880	Somebody who doesn't want to find gears under their skin
1367880	1369680	and cogs and things like that.
1369680	1370520	Why not?
1370520	1372600	Because they've bought into the fact that proteins
1372600	1377600	and RNA does it, is that anymore?
1377960	1380520	You know, why are you any happier with that?
1380520	1383360	None of that, none of that is intrinsically
1383360	1384600	any better than anything else.
1384600	1387080	So I think it's very important not to get,
1387080	1389800	not to take the wrong message from all this kind of stuff
1389800	1393840	and to somehow dissolve your fundamental worth
1393840	1395280	just because we've seen some of the parts
1395280	1396240	that are under the hood.
1396240	1398200	We knew that we're gonna be parts under the hood.
1398200	1400280	In fact, I'll go one step further
1400360	1401280	and for the people who, right?
1401280	1405280	Because one thing that people sometimes say
1405280	1407280	at this point is, you're right,
1407280	1410080	you've just done a deconstruction of all materialism
1410080	1412320	and none of it matters unless we're a soul.
1412320	1414280	Yeah, so some people feel that way
1414280	1417480	that basically, right, the proteins,
1417480	1419880	just like the gears all know good.
1419880	1422400	So the thing with that is without even getting
1422400	1424480	into the kind of the factual nature
1424480	1427560	or just think purely logically, fine, fine,
1427560	1428400	maybe you're a soul.
1428400	1429560	What's the mechanism of the soul?
1429600	1430560	There's gonna be one.
1430560	1431480	It's gonna do something
1431480	1433680	and it's gonna have some kind of features.
1433680	1436080	If it doesn't have any, if it has no parts,
1436080	1439080	then it can't change, then you can't learn,
1439080	1441200	you can't make decisions, you can't improve,
1441200	1442040	you can't do anything.
1442040	1443680	It's gonna have some kind of structure.
1443680	1445040	Maybe it's, I'll give it to you
1445040	1447640	that maybe it's a completely different,
1447640	1450080	something material science has never seen before.
1450080	1453840	Great, but it's still gonna have some kind of descriptive
1453840	1458560	laws that govern how it acts in whatever space it lives in.
1458560	1459880	And then we're gonna be back to the same.
1459880	1461240	Somebody's gonna say,
1461240	1463840	but it's got rules that govern its behavior.
1463840	1465360	That's not what I mean,
1465360	1469800	but that's not enough to give me my magical feeling of stuff.
1469800	1472040	So we gotta get over this.
1472040	1476640	The non-material kind of way forward
1476640	1478200	doesn't help at all.
1478200	1480320	And we have to get over the fact
1480320	1482360	that finding mechanisms under this somehow
1482360	1485080	robs the larger scale of its meaning.
1485080	1488120	That's just, sorry, that's a long diatribe,
1488120	1490000	but that's what I wanted to say.
1490000	1491320	No, no, that's great.
1491320	1493080	That's spot on too.
1493080	1497560	It's funny, because I had on Bobby Azarian,
1497560	1500640	who's a cosmologist and wrote a great book
1500640	1501720	called Romance of Reality.
1501720	1503640	And he refers to this quite a bit,
1503640	1508200	more in the sort of the deterministic lack of free will,
1508200	1510280	but the similar kind of vein
1510280	1512560	that a lot of people feel like
1512560	1515520	as soon as you start to explain this stuff,
1515520	1519040	it's like you're explaining away the soul or the mystery
1519040	1521000	or what makes it interesting, right?
1521000	1523280	But at least it seems like to me
1523280	1525360	that it's just like a never-ending process.
1525360	1526760	Like there's always something to learn
1526760	1529000	and continue to be curious about.
1529000	1534000	And it's funny, yeah, if you update your priors
1534320	1538360	and you're not stuck to, if you're open-minded enough
1538360	1541640	to all the different possibilities that are out there
1541640	1543240	and you get this new information,
1543240	1545800	yeah, you just keep, like you said, if you're a robot,
1545800	1549160	you just say, okay, well, that's interesting.
1549160	1550000	Yeah, that's interesting.
1550000	1551400	And you just update and you move on
1551400	1554320	and you've changed how you view the world.
1554320	1555160	Yeah, that's fine.
1555160	1557160	I'm the world's most amazing robot.
1557160	1558600	I thought I was a meaty robot.
1558600	1561000	Now I'm a metallic robot.
1561000	1562480	Great, who cares?
1562480	1564400	My list of things I was gonna do,
1564400	1566440	the amazing things I was gonna do,
1566440	1569240	still nothing's canceled, still right there.
1569240	1571520	I'm still gonna go do those things.
1572000	1573440	So that's what I wish,
1573440	1577200	I don't know how convincing any of this is to anybody,
1577200	1579760	but that's one thing that I really wish
1579760	1581400	people wouldn't internalize.
1581400	1585960	There's so much, both personal and social loss of,
1587360	1589560	loss of meaning, I think is the best way,
1589560	1592680	as Varvaki says, around all of this kind of stuff.
1592680	1595120	And that's the last thing I wanna contribute to.
1597360	1600360	Yeah, I mean, that's a whole fascinating route
1600360	1601200	that I could go down to,
1601200	1604320	because I feel like, let's just say this,
1604320	1608880	like if that new information would lead you down
1608880	1611280	this kind of nihilistic path,
1611280	1612120	and I kind of feel like
1612120	1614280	you were probably gonna go down that path anyway.
1614280	1615520	Yeah, 100%.
1615520	1616920	Yeah, it's sort of,
1616920	1619960	you're using it as a reason to get stuck
1619960	1622440	in a really a vicious cycle potentially,
1622440	1625960	but yeah, that's really interesting.
1625960	1627440	But actually, I think this,
1627440	1629040	where I was gonna go to next anyway,
1629040	1631960	and it's in this paper and the next paper as well,
1631960	1636600	you had this concept called the play the hand your dealt.
1638160	1640400	And you, I mean, you're using this very,
1642000	1643800	how should I say it, materially,
1643800	1646160	you're talking about the cellular collectives
1646160	1649160	that can carry out different steps
1649160	1651760	with its sort of second order functions.
1651760	1656880	It's not, they're far more, let's say malleable,
1656880	1660880	and far more open-ended kind of strategies
1660880	1662560	for accomplishing their goals.
1662560	1665280	Can you talk to us about that and what that,
1665280	1668280	I think you, the acronym is PhD,
1668280	1670000	so play the hand your dealt.
1670000	1674240	Yeah, well, let's just start with a specific example.
1675120	1680120	So you take a salamander egg
1680680	1683480	and there's a certain tricks you can do
1683480	1686360	to increase the number of the chromosome count
1686360	1689680	within the number of copies of the genome
1689680	1690880	that are in there.
1690880	1693120	And so when you do that,
1693120	1696200	so let's say we increase instead of two N,
1696200	1698920	you can make four N, five N, six N and so on.
1698920	1700560	So as you do that, the cells,
1700560	1703200	the embryonic cells get bigger and bigger.
1703200	1705800	The salamander stays the same size.
1705800	1709520	If you take a cross section through a kidney tubule,
1709520	1711440	which normally is made of, I don't know if you can picture
1711440	1715520	this, but like six to eight cells that work together
1715520	1718280	to form this kind of like long tube,
1718280	1719400	they get bigger and bigger,
1719400	1721480	but the animal stays the same size
1721480	1722840	and the tubule stays the same size.
1722840	1724760	So what this means is that fewer and fewer
1724760	1728040	of these larger cells participate in each tubule, right?
1728960	1732000	And so far, so we have two amazing things so far.
1732000	1735520	Number one is you've got the wrong number of chromosomes,
1735520	1738040	fine, you're still a good salamander, amazing.
1738040	1741520	Number two, your cells are the wrong size, no problem.
1741520	1744360	We manage the cell number to make up
1744360	1746120	for this different in cell size.
1746120	1748080	You're still a good salamander.
1748080	1751920	Number three, if you make the cells absolutely gigantic,
1751920	1754000	and these are I think six or eight N,
1754000	1757320	I don't remember exactly, what happens is,
1757320	1759960	there's not room for even two cells to be there.
1759960	1763360	So what happens is one single cell wraps around itself
1763360	1764840	and leaves a hole in the middle
1764840	1766960	to give you that same tubule, okay?
1766960	1768520	Aside from the whole issue.
1768520	1772640	So what it is is it's using different molecular mechanisms.
1772640	1774880	In the first case, it was cell-to-cell communication.
1774880	1777120	In the next case, it's cytoskeletal bending.
1778200	1780640	You're using different molecular mechanisms
1780640	1783400	in the service of an anatomical large-scale goal.
1783400	1785840	That's a kind of top-down causation, super interesting,
1785840	1788360	but for our purpose, more importantly,
1788360	1790480	look at your job as a salamander.
1790480	1791760	You come into this world,
1791760	1795120	you can't tell how many chromosomes you're gonna have.
1795120	1796960	You don't know what your cell size is.
1796960	1798520	You don't know how many cells you're gonna have
1798520	1799400	because people have done that too.
1799400	1801440	You can take away cells, you can add cells.
1801440	1802400	You don't know any of that stuff.
1802400	1804000	You need to be able to, your goal
1804000	1806080	is to be able to make a good salamander
1806080	1807480	no matter what you start with.
1807480	1810000	So that's within limits, obviously.
1810000	1812760	I mean, all of these things are not infinitely stretchable.
1812760	1814480	So that's one example.
1814480	1817000	Here's another example, plenaria, flatworms.
1817000	1819080	The species that we work with
1819080	1821880	reproduces by tearing themselves in half and regenerating.
1821880	1823400	That's how they reproduce.
1823400	1826600	That means that they have somatic inheritance.
1826600	1829520	Any mutation that doesn't kill the stem cell
1829520	1831920	is gonna be proliferated into the body
1831920	1834440	and expanded in the body in the next generation.
1834440	1836640	So if you look at these worms, they're mix-a-ploid.
1836640	1838560	Every cell has a different number of chromosomes.
1838560	1840080	The genome is a total mess
1840080	1841960	because they just accumulate this stuff.
1841960	1844560	400 million years they've been accumulating all this junk.
1844560	1848880	And yet that's the species with perfect regeneration,
1848880	1852560	high cancer resistance, no aging in the asexual form.
1855200	1856840	Incredible, right?
1856840	1861400	And so the same story there is what you have to do
1861400	1863160	is you have to have an algorithm
1863160	1865360	that builds a correct plenarian
1866440	1868880	despite errors in the hardware.
1868880	1870200	And that's what I think.
1870200	1873760	So there's my play the hand-in-the-dilts concept
1873760	1876760	is that coming into this world as a new creature,
1876760	1879760	there's precious few things you can depend on for most.
1879760	1882080	I mean, there are hardwired things like C. elegans,
1882080	1884800	nematodes, and maybe some other species.
1884800	1887520	But I think, and there's all spectrum.
1887520	1890440	So I would sort of imagine that something like nematodes
1890480	1892480	where every nematode, every C. elegans
1892480	1893960	has exactly the same number of cells
1893960	1895200	and they all have the same lineage.
1895200	1897880	So that's a very cookie cutter organism and that's here.
1897880	1900600	Plenaria are super plastic and they're out there.
1900600	1902000	And salamanders are somewhere here
1902000	1903040	and humans are somewhere,
1903040	1904160	mammals are somewhere here.
1904160	1907800	We all have different degrees of that competency.
1909200	1910720	But that's the idea.
1910720	1912280	Most of us don't come into the world
1912280	1915200	being able to expect very specific things
1915200	1917560	and then just crashing and burning
1917560	1919280	when those assumptions aren't met.
1920200	1922560	This is why we can make tadpoles with eyes on their backs
1922560	1927880	that can see, and this is why slippers goat
1927880	1933880	when that two-legged, right, the goat that didn't have any form.
1933880	1936120	Shippers, goat, yeah, something like that.
1936120	1939280	Yeah, learn to walk upright.
1939280	1942480	And they found that a lot of the changes
1942480	1947000	that come along with bipedal locomotion were already made
1947000	1952000	because of the incredible plasticity of the organism.
1953200	1956400	And it's why skin cells taken off of frog embryos
1956400	1958440	becomes xenobots and why,
1958440	1960560	and instead of just like collapsing and dying
1960560	1964160	and all of these things, that's play the hand you're dealt.
1964160	1967240	That's the idea that we have to...
1967240	1968720	There may have been life at one point
1968720	1972440	that was much more cookie cutter, but none of that survived.
1972440	1974520	Nowadays, if you're gonna survive,
1974520	1978320	nowadays, you're the kind of life that is able to do this.
1978320	1980240	Right, and can you tell us about...
1981680	1982960	One thing I was thinking about too,
1982960	1984120	with the play the hand you're dealt,
1984120	1986960	sort of like whatever materials you have there,
1986960	1988480	that's what you're gonna use,
1988480	1990720	is, and with bioelectricity, of course,
1990720	1993960	like the electromagnetic spectrum,
1993960	1997680	that's sort of what's available to these cells, right?
1997680	1999760	And I'm thinking, and perhaps this is just
1999760	2001520	because it's at the limits of our science,
2001520	2004200	have you found either in your own research
2004200	2007120	or in research of your contemporaries,
2007120	2011600	like evidence of organisms making use of quantum fields?
2011600	2013960	Like are all the different fields that are available,
2013960	2015440	you know what, I mean, electricity,
2015440	2019400	I understand why bioelectricity is so great.
2019400	2021080	I mean, we use a lot of the same,
2022040	2025520	the ion, like the ion channels, like logic gates,
2025520	2027800	we use that stuff in our technology,
2027800	2029720	you know, similar kind of processes.
2029720	2032160	But anything in the quantum realm that you've seen?
2033160	2036840	So we don't study that specifically,
2036840	2038920	so I don't have any data about it.
2038920	2041480	There are certainly people that study quantum biology,
2041480	2045440	there's some great people who, you know,
2045440	2047440	Claree Ciello and my former postdoc,
2047440	2050560	Nerocia Morgan, they're into this kind of stuff.
2050560	2052760	And as our other people,
2052760	2054440	and I'm sure there are interesting things
2054440	2056400	that we don't particularly work on it.
2057400	2060080	My gut feeling is that,
2061720	2062920	not being an expert in this,
2062920	2065800	but just for whatever it's worth, my gut feeling,
2065800	2068520	it's gonna be the sort of thing that if it exists,
2068520	2071480	it's not gonna be some weird special adaptation where,
2071480	2073880	you know, all look birds are using, you know,
2073880	2076840	quantum spin to navigate the magnetic field
2076840	2078000	and that's it, it's not gonna be like that.
2078000	2080040	I think, I mean, that may also be,
2080040	2081400	but I don't mean that.
2081400	2085360	I think what we're gonna find is that it's everywhere,
2085360	2088640	like it's a basic fundamental, if it's there,
2088640	2089680	if quantum biology is there,
2089680	2091400	which I suspect it probably is,
2091400	2094280	it's gonna be used for everything.
2094280	2095320	We're gonna find out all the things
2095320	2098520	that we take for granted now that are just,
2098520	2099520	you know, we kind of assume
2099520	2100880	there's a classical explanation for it,
2100880	2102120	but we don't know what it is.
2102120	2104480	I have a feeling we're gonna find out a lot of this stuff
2104480	2107480	is at bottom, exploiting some of those properties.
2107480	2110160	That's just, that's totally a guess on my part.
2110160	2113960	I don't have any data to, you know, support any of that.
2113960	2115800	But that's my guess.
2115800	2117720	Yeah, yeah, that's interesting.
2117720	2121840	I would see, the very little I know about the quantum,
2121840	2124000	like quantum mechanics is that I would assume
2124000	2126160	these structures, even cellular structures,
2126160	2129440	they're tiny to us, but they're too large,
2130360	2131520	like, you know, it breaks down.
2131520	2133860	The quantum effects are just,
2133860	2135240	what happened to a way smaller lens,
2135240	2137040	but like you just pointed out,
2137040	2139360	like I think that there's some research
2139360	2140240	I've done a little bit of digging,
2140240	2143240	but I have to do more about quantum biology.
2143960	2147920	Yeah, look at, also look at the work of Chris Fields
2147920	2150040	and folks that he works with,
2150040	2154200	because there are aspects of what's important about that field
2154200	2156400	that has nothing to do with being small.
2156400	2159000	So there are really important aspects of, you know,
2159000	2160880	in terms of what's an observer and reference,
2160880	2162160	for observer reference frames,
2162160	2165720	and these kinds of things that apply across scales.
2165720	2168920	So Chris has some beautiful papers about that.
2168920	2172360	So it's not just for tiny particles and things like that.
2173240	2174440	That's good to know too.
2174440	2177040	And before we move into Darwin's and Gentryl materials,
2177040	2178760	I think it's still applicable here.
2178760	2181200	Can you tell us about, I think there was one podcast you did
2181200	2185280	where you talked about trophic memory in deer
2185280	2186440	and antler structures,
2186440	2187960	and actually the scientist that was studying it
2187960	2189680	actually sent them to you,
2189680	2191120	and you have a bunch of these.
2191120	2193680	Can you just tell us a little anecdote?
2193680	2196880	Yeah, well, okay, so first, what is trophic memory?
2196880	2201400	So there's certain species of deer that every year
2202040	2204160	they shed their antlers and then they grow them back.
2204160	2206640	Antlers are real bone, they're not like horns.
2206640	2209320	They're real bone with velvet and innovation and all that.
2209320	2213000	So there was this team, last named Bubenek,
2213000	2216680	and it was two folks, a father and son,
2216680	2218320	and they live in Canada.
2218320	2220120	And they did experiments for,
2220120	2221680	I'm gonna say well over 30 years,
2221680	2224320	maybe 40 years together, something like that,
2224320	2226920	where what they found is that if you see,
2226920	2228720	you got this deer and you take a knife
2228720	2233000	and you etch a little cut into somewhere
2233000	2235320	on this branch structure.
2235320	2238480	And that year, it kind of heals with a little callus,
2238480	2240040	the bone heals, and that's that.
2240040	2242200	And then the whole thing drops off.
2242200	2245840	And then next year, when they regrow their antlers,
2245840	2249040	it grows with an ectopic branch point at the location
2249040	2251640	where the damage was last year.
2251640	2255760	Now, I read this, and so they got these papers
2255760	2258560	from 60s and 70s and beyond.
2258560	2260520	It's an amazing data set.
2260520	2262200	No one's ever gonna get a data set like this.
2262200	2264800	I mean, who's got a herd of deer
2264800	2266360	that they can watch for 40 years?
2266360	2268560	It's a, in modern biology careers,
2268560	2273560	that's not exactly conducive to getting a good associate,
2274320	2276480	this is in a professor position.
2276480	2279720	But they've got this incredible data set.
2279720	2281120	And when I first read about this,
2281120	2285320	I thought it was amazing because if you try to think
2285320	2290320	about the current, what passes for an explanation
2291360	2293360	for a biological phenomenon nowadays?
2293360	2296400	So if you find this, you look at a typical cell paper,
2296400	2298960	figure seven is gonna be a molecular pathway, right?
2298960	2300760	So there's some arrow diagrams on this thing bind,
2300760	2301960	so that thing, and then they go over here
2301960	2303640	and they bind to this other thing.
2303640	2306320	Just try to come up with a model like that
2306320	2307760	for what's going on here.
2307760	2310800	So there's a large scale structure.
2310800	2314240	It gets a damage input at a particular location.
2314280	2315840	The whole thing falls off.
2315840	2320000	The cells at the scalp, which are tens of centimeters away,
2320000	2322480	remember the three-dimensional position
2322480	2324720	where the damage was last year.
2324720	2326600	And then when the bone cells start growing,
2326600	2330240	they revise the genetically encoded rules to say,
2330240	2331920	oh, by the way, when you get to this point,
2331920	2333800	make an extra thing to the right.
2333800	2337600	Like what possible explanation
2337600	2340800	using conventional conceptual tools that we have?
2340800	2341880	Could you come up with it?
2341880	2343240	It just completely fails.
2343240	2345400	And so this is one of the things I tell my students
2345400	2348320	is that your developmental biology textbook
2348320	2350680	is full of things that are readily explained
2350680	2353280	by the conceptual tools that we have now.
2353280	2355720	That's what's in the textbook, other success cases.
2355720	2360160	So what's fun is to look for the blank space in between.
2360160	2361840	What are all the things that are not in there, right?
2361840	2362680	And they're not in there
2362680	2364040	because we don't have a clue as to how it works.
2364040	2366040	So this is the kind of, I mean, I love that stuff.
2366040	2368560	So I'm always thinking about these kinds of things.
2368560	2370120	So anyway, so at one point,
2370120	2373320	so we wrote Daniel Lobo and I wrote a paper about this
2373320	2376360	and explaining like what the implications of that are
2376360	2378840	for the inverse problem,
2378840	2381200	which plagues regenerative medicine and so on.
2381200	2385080	And Bubenek emailed me and he said,
2385080	2386840	he said, you're one of the few people
2386840	2388960	that sites this stuff nowadays.
2388960	2393080	I need to like basically clean out the house, the garage.
2393080	2394440	Would you like these antlers?
2394440	2396760	And I was like, to hell yes, of course I would.
2396760	2399320	And that's such a unique, I mean, it's amazing work.
2399360	2402320	It's such a unique dataset.
2402320	2405840	And so we received in my lab 13, I think it was 13,
2405840	2408240	13 large boxes of these antlers.
2408240	2413240	And I sent them all to the veteran,
2415240	2416280	a Tufts Veterinary School.
2416280	2420600	They have a CAT scan machine where they CT scan horses
2420600	2421440	and things like that.
2421440	2423400	And so they CT scanned all of these things.
2423400	2426400	So I have somewhere, I've still got the antlers
2426400	2427640	during boxes in the closet.
2427640	2429080	I have a couple of them on the wall actually
2429080	2430320	in the lab as you walk into the lab.
2430320	2431400	There's a couple on the wall.
2431400	2434880	But yeah, but I still got these boxes.
2434880	2437720	And it was incredible because every set of antlers
2437720	2441240	is labeled with the name of the deer and the year.
2441240	2444720	So it'll be like, Lenny, 1987, Lenny, 1988.
2444720	2446440	And because you have to do these longitudinal,
2446440	2448040	like you have to know which deer it was.
2448040	2450000	And he kept meticulous records.
2450000	2455640	And so we have all of these antlers.
2455640	2460640	And it's a very unfortunate model system
2460840	2462200	because if you want to do experiments,
2462200	2464320	I mean, who's going to have deer and wait years
2464320	2465720	for an outcome like it's crazy.
2465720	2467880	But the closest thing to that actually
2467880	2469560	is our two-headed plenarium
2469560	2471800	because it's a very similar kind of thing.
2471800	2474920	It's a physiological stimulus
2474920	2476520	that gets somehow catalyzed
2476520	2480360	into multi-generational repair processes.
2480360	2482440	Because once you've make them two-headed,
2482440	2483400	you can keep cutting them.
2483400	2485560	They stay two-headed forever.
2485920	2488640	And luckily, plenarium are much more tractable than deer.
2488640	2492000	So I suspect those things are highly related,
2492000	2494920	but that's my deer antler story.
2494920	2497360	That's awesome. That's so cool.
2497360	2500520	That must be what a fixture to have in the lab, too.
2500520	2502040	It's right in the front as you walk in.
2502040	2503360	Great story, too.
2503360	2505280	Awesome. Thanks for sharing that.
2505280	2506680	So I'd love to turn towards,
2506680	2507800	and there's going to be a lot of overlap
2507800	2510200	between this paper and what we just discussed.
2510200	2513640	And I'm sure we'll talk more about bi-electricity as well.
2513640	2517400	But the second paper, Darwin's Ingencial Materials,
2517400	2520160	Evolutionary Implications of Multiscale Competency
2520160	2521920	and Developmental Biology.
2521920	2523840	And both of these papers, the first two,
2523840	2526080	they really just dropped in the last few months, right?
2526080	2527400	April 2023.
2527400	2530440	So these are hot off the presses for folks.
2530440	2533480	I don't really heard you talk about these in other podcasts.
2533480	2537000	So it's great that we're getting a chance today to do that.
2537960	2539240	And before we jump right into the paper,
2539240	2540240	is this an illusion at all
2540240	2543120	to the His Dark Materials book series?
2543120	2544680	Like the title?
2544680	2546800	No, not really.
2546800	2547640	No.
2547640	2548480	Okay, just wondering.
2548480	2549920	I mean, that's a little bit of a flyer,
2549920	2551600	because it's Darwin's Materials.
2551600	2552880	I was like, is there sort of a...
2552880	2555160	Yeah, it does sound like that, doesn't it?
2555160	2557360	No, not really.
2557360	2559720	Okay, cool. Yeah, just an aside.
2559720	2561880	But would you mind, I mean, just like you did
2561880	2562720	for the first paper,
2562720	2564360	could you provide like a simple overview
2564360	2567040	and then I have a bunch of questions to ask you about?
2567040	2567880	Yeah.
2568760	2572320	Let's start from this idea
2573200	2576360	and I tried to formalize this in my tame paper
2576360	2578280	from a few years ago.
2578280	2580920	This idea of a spectrum of,
2580920	2582000	you can call it many things,
2582000	2583560	it's a spectrum of persuadability.
2583560	2585840	That's what I call it in the paper, a spectrum of agency.
2585840	2588640	Just the idea that you can put any system
2588640	2591000	on this continuous spectrum
2591000	2595560	that tries to capture how much autonomy the thing has
2595560	2597160	from an engineering perspective.
2597160	2598840	How much can I expect?
2598840	2602280	How much problem-solving chops does this thing have?
2603000	2604520	When I'm not around to force it.
2604520	2609520	So, and you can go from Legos to thermostats,
2612680	2615240	to animals that learn, to humans,
2615240	2617480	and then everything in between is there, right?
2617480	2620640	Okay, so now we can ask the following question.
2620640	2621880	As an engineer,
2621880	2624120	and so this is another paper I wrote with Jamie Davies
2624120	2627080	a few years ago, maybe a year ago,
2627080	2630000	talking about engineering with agential materials,
2630000	2632680	because engineering is very different
2632680	2635840	based on where along that spectrum your parts are.
2635840	2639280	If you are engineering with Legos, everything is on you.
2639280	2641880	The only thing the Lego's gonna do is keep its shape.
2641880	2642960	That's all it knows how to do.
2642960	2644120	So everything else is on you.
2644120	2646160	Everything that you need to happen,
2646160	2647920	you have to somehow make sure it happens.
2647920	2651560	And this is how molecular synthetic biology works.
2651560	2654040	You're gonna put in circuits that do specific things.
2654040	2656840	It's on you to implement every part
2656840	2658600	of the functionality that you want.
2660560	2664680	If you, and so humans have been engineering with bricks
2664680	2668080	and wood and metal for thousands of years,
2668080	2671280	and use a certain set of techniques to do that.
2671280	2672440	What are those techniques?
2672440	2676360	Well, they don't involve psychological tools.
2676360	2679120	They involve very kind of low level,
2679120	2681520	put everything where it goes and glue it down
2681520	2683040	and attach this thing to that thing.
2683040	2684840	That's your toolbox.
2684840	2688840	Well, if you're building autonomous vehicles
2688840	2691960	or self-guided missiles or houses or whatever,
2691960	2693280	you've got some other stuff,
2693280	2695080	which are, for example, thermostats.
2695080	2697560	So if you've got a thermostat, it's interesting
2697560	2700120	because you don't even necessarily need to know
2700120	2701280	how the whole thing works.
2701280	2703920	What you need to know is where is the set point
2703920	2705280	and how do I change it, right?
2705280	2706640	And what are the inputs and what are the outputs?
2706640	2707640	Where does the thermometer go?
2707640	2710360	Where does the connection to the heating,
2710360	2712040	to the cooling and heating go?
2712040	2714000	And what you know is that it's gonna do certain things
2714000	2715680	when you're out there to micromanage it.
2715680	2717720	Do you have to leave rules for what to do
2717720	2718880	at every single temperature level?
2718880	2719720	You don't.
2719720	2720680	It's gonna do that on its own.
2720680	2722000	So you've got some other tools
2722000	2723280	and these are the tools of cybernetics.
2723280	2725160	So for more complex systems,
2725160	2728440	you'll have control theory and all these kinds of tools
2728440	2732120	to deal with something that has a simple level of agency.
2732120	2735600	It's got very basic, kind of primitive goals
2735600	2739240	that it tries to set, and if you don't understand
2739240	2740880	anything about cybernetics,
2740880	2744480	you are not going to get the most out of these components.
2744480	2746840	If you don't understand what homeostasis is,
2746840	2748400	what goal-directed loops are,
2748400	2749560	you're not really gonna be very good
2749560	2751800	at using those in your engineering.
2751800	2752880	Okay.
2752880	2754600	And then you move forward and you say,
2754600	2759600	okay, I'm a proprietor of a circus,
2759960	2761200	of a rat circus.
2761200	2764080	And I want these rats to do little tricks, right?
2764080	2767400	I want them to, and in the Jamie's paper,
2767400	2770680	I think we talked about building a tower out of dogs.
2770680	2774280	I could try the traditional route
2774280	2776400	of stacking them on top of each other,
2776440	2777360	but that isn't gonna work.
2777360	2778960	They're gonna crawl off.
2778960	2780560	They're not gonna just stay where I put them.
2780560	2782200	So that doesn't work at all.
2782200	2784000	I have to use a completely different set of tricks
2784000	2786040	that people who work with wood and metal don't have to use.
2786040	2787720	I have to train my material.
2787720	2789640	I have to, now on the one hand,
2789640	2790800	it's a bit of a pain in the butt
2790800	2793160	because your material has its own agenda
2793160	2795280	and you have to have tools to manage it.
2795280	2796280	But here's the beauty of it.
2796280	2799640	Once you've trained them to keep a little tower,
2799640	2801680	if you knock it over, guess what it does?
2801680	2803040	They get right back up on their own.
2803040	2804880	You don't have to be there to rebuild it.
2804880	2805800	Isn't that amazing?
2806720	2808240	So you've gained something very interesting
2808240	2810800	by switching the bag of tools you bring to the problem.
2810800	2812120	You've gained something interesting
2812120	2813200	and you were only able to do that
2813200	2815520	because you recognized the agency of the material.
2815520	2816920	You wouldn't do that, right?
2816920	2819320	You wouldn't do that if you didn't know,
2819320	2820920	if you thought that these things are dumb,
2820920	2823080	like bricks and Legos.
2823080	2824680	So it's very important.
2824680	2827800	And then you see this all the way up.
2827800	2831240	If you're a hacker, you might be hacking the computer,
2831240	2833920	but you might be doing what they call social engineering.
2833920	2835920	You might find out that it's much easier
2835920	2838080	to just trick somebody giving you their password
2838080	2842000	than to spend all day brute forcing the hashtag or whatever.
2842000	2847000	So there's this has to be this impedance match
2847040	2848800	between the tools you bring to the problem.
2848800	2852400	And the successful engineer is one that recognizes
2852400	2856280	what's the right level of agency in my material.
2856280	2859440	And so that leads to the question for regenerative medicine.
2859440	2862840	We have cells and tissues and you ask,
2862840	2865040	so what are the tools that I'm gonna use there?
2865040	2867040	And the assumption up until now,
2867040	2869760	the assumption has been, well, they're like the bricks.
2869760	2871920	You have to micromanage all of it.
2871920	2876920	Yeah, that's been the standard assumption of the paradigm.
2877160	2879840	And but it's very much an open question.
2879840	2881800	Are they more like the Legos
2881800	2883040	or are they more like the rats
2883040	2884840	or are they more like something in between?
2884840	2886920	Or are they more like the thermostat or where are they?
2886920	2889600	And regenerative medicine is gonna be cracked
2889600	2892100	by the people who pick the right level.
2892100	2895220	It's not gonna be cracked by assuming the wrong level
2895220	2898020	nor down, so it's not gonna happen
2898020	2902300	if you assume these things are low agency machines.
2902300	2904780	It's also not going to happen if you assume
2904780	2908540	that they're magical, inexplicable things
2908540	2912480	that aren't gonna obey any kind of rational rules.
2912480	2914460	That's not gonna work either somewhere in the middle,
2914460	2916020	which is what of course what my lab tries to do
2916020	2918780	is to pick the right set of tools from cybernetics,
2918780	2921980	from behavioral science to take advantage.
2922360	2925500	So having said all of that, here's the thing with the paper.
2925500	2927460	So what I just took you through
2927460	2931540	is how human engineers view the spectrum
2931540	2933980	of agential materials, right?
2933980	2936100	And so now it comes time to, so we wrote,
2936100	2937980	so Jamie and I wrote that, and then I said, okay,
2937980	2940020	so now what does evolution do?
2940020	2941580	Evolution is also an engineer.
2941580	2946580	So the question is, does evolution assume
2947140	2949540	and work at the, assume that these things are like Legos
2949540	2950980	and work at the lowest level,
2950980	2952780	which means search the really difficult
2952780	2957780	and really kind of rugged space of molecular properties
2958900	2961780	or would evolution take advantage
2961780	2963820	of the competency of the material?
2963820	2965660	Because the thing about evolution is evolution
2965660	2968620	doesn't work with Legos, evolution works with cells
2968620	2971700	and cells and tissues used to be independent organisms.
2971700	2973580	They don't come as blank slates that are dumb
2973580	2975220	and have to be micromanaged.
2975220	2977580	They come with behavioral competencies,
2977620	2981100	with preferences, with various kinds of agendas.
2981100	2985060	And so what that paper is, is an exploration of,
2985060	2987460	if we take that seriously, the fact that evolution,
2987460	2989220	of course, evolution is very opportunistic,
2989220	2992020	it makes use of everything it can, we know that.
2992020	2995900	What can we conclude if we take seriously the idea
2995900	2997660	that evolution will not have missed the fact
2997660	2999140	that it's dealing with a very powerful
2999140	3000420	agential material that these cells
3000420	3001740	already know how to do things?
3001740	3003780	And so what I do in that paper is run down
3003780	3005500	all of the implications.
3005500	3006820	What does it mean for evolution
3006820	3009180	that it isn't working with Legos, it's working?
3009180	3012620	Of course, people have used evolutionary computation,
3012620	3014620	like genetic algorithms and things like that
3014620	3016380	with materials that really are dumb.
3016380	3020060	So the typical evolutionary algorithm
3020060	3025060	is done over passive data and it shows you improvements
3025060	3026420	but the material is very passive
3026420	3028740	and here I'm saying biological evolution isn't like that.
3028740	3029620	So what does that mean?
3029620	3031140	What will that do to evolution?
3031140	3032540	And that's kind of the flip side, right?
3032540	3036620	A lot of people study how intelligence,
3036620	3038700	how evolution gives rise to intelligence
3038700	3041460	of different types, I sort of reverse that
3041460	3043980	and of course both are happening simultaneously
3043980	3045420	but I looked at the other side of things
3045420	3047620	which is how does intelligence impact
3047620	3050740	the actual evolution, the intelligence of the substrate?
3051940	3054220	Sure, that's interesting, still thought provoking.
3054220	3058540	The, can I tell you something,
3058540	3060620	an impulse I had while reading the paper
3060620	3062420	even though it's not stated explicitly
3062420	3064660	and even something you just said.
3064660	3067900	Would you, how do you view even the evolutionary process?
3067900	3070460	I mean, it almost sounds like evolution is an agent
3070460	3072020	in and of itself.
3072020	3075660	Do you view it that way or how do you look at it?
3075660	3078020	Yeah, this is an interesting point.
3078020	3083020	That's a paper that is on my list to write.
3083540	3085220	It'll probably be next year at this point
3085220	3088580	but this idea of evolution itself as an agent.
3088580	3090740	I wanna be very careful here
3090740	3095740	because a lot of people still have this kind of ancient view
3096340	3098220	that there are two kinds of things in the world.
3098220	3100580	There are dumb material things,
3100580	3105580	like the machines that the quote unquote machines and so on.
3106620	3108500	And then there are the mindful things
3108500	3111860	like humans and angels and God and whatever else, right?
3111860	3115500	And so when I say, actually I do think
3115500	3119620	that there's a lens on evolution
3119620	3123460	which does see it as an agent.
3123460	3126540	What I'm not saying, okay, so super clear,
3126540	3131540	not saying that evolution has a high level purpose
3132020	3134340	the way that a human level purpose
3134340	3137500	or a beyond human level purpose, not saying any of that.
3137500	3140660	What I am saying is that there's a very rich spectrum
3140660	3142820	of agency from very low.
3142820	3146300	I'm not sure there's a zero, but certainly from very low
3146300	3148580	all the way to human and beyond.
3148580	3151180	And I don't think we can blindly assume
3151180	3154260	that the level of agency for the evolutionary process
3154260	3156380	is down at the low end.
3156380	3158380	It might be non-zero
3158380	3161900	and it might be important to understand what it is
3161900	3163740	but just again, really clear,
3163740	3166940	not saying that there's any human
3166940	3168900	or above level intelligence out there
3168900	3172740	picking where the lineages go.
3174060	3178380	I think Carl first and probably said this
3178380	3182980	well before me, this idea that you can use that framework
3184740	3187340	to you can, so imagine,
3187340	3188580	so it's kind of two ways to think about this.
3188580	3192900	One way to think about this is that imagine a lineage,
3192900	3195220	I don't know, 15 million years of alligators or something
3195220	3198180	just imagine some kind of lineage.
3199180	3203300	And you can imagine that whole thing as an agent.
3203300	3205060	It's a very long lived agent
3205060	3208100	but we're just bad at noticing agency
3208100	3209500	at different time scales.
3209500	3211500	It's a spatially a huge agent
3211500	3213220	but we're also too fixated on agents
3213220	3216620	that are roughly the size of us, medium sized objects.
3216620	3217700	So if you forget that
3217700	3219700	and assume that agents can be whatever,
3220740	3222020	what is happening there?
3222020	3223860	What's happening there is that continuously
3223860	3226260	generates hypotheses about the environment.
3226260	3229380	Those hypotheses are cashed out as offspring
3229380	3230740	with different features.
3230740	3232620	Some of those hypotheses are proven false.
3232620	3234980	Some of those hypotheses are supported.
3234980	3237660	Those supported hypotheses go on
3237660	3241020	and shape the cognitive system of the collective
3241020	3243980	to form new hypotheses that might be even better
3243980	3246860	more correctly described, you know, reality.
3246860	3251740	And what's interesting is that much like kind of
3251740	3253700	consistent with what I said before,
3253700	3257300	what these hypotheses are are not flat
3257300	3260180	kind of first order statements about the world
3260180	3263820	meaning a hardwired solution.
3263820	3265860	And you know, it is what it is.
3265860	3268180	They're actually, these hypotheses
3268180	3271740	are actually problem solving strategies.
3271740	3272980	They're heuristics.
3272980	3277980	They're like what comes out of evolution is not,
3278820	3281180	here's how you be a salamander and that's it.
3281180	3284900	It's a set of policies that cellular collectives
3284900	3287460	can operate depending on what's going on.
3287460	3288900	They're context sensitive.
3288900	3293020	So these, it's like instead of generating guesses
3293020	3297420	about the world, what you're generating are policies.
3297420	3300420	You're generating navigational heuristics, right?
3300420	3302500	And so that's what that agent is doing.
3302500	3303340	That's pretty good.
3303340	3305540	That's not super low agency to be able to do that.
3305540	3306380	That's something.
3306380	3308100	That doesn't mean you have self-reflective,
3308100	3311060	you know, I'd love to evolve some humans
3311060	3313580	because that would be just like, that's not what I mean.
3313580	3316380	But it isn't, you know, it's being able to generate
3316380	3320340	hypotheses and get them falsified and have this like,
3320340	3322780	so Carl, you know, Carl Friston has some great thoughts
3322780	3326740	on this, Richard Watson has some great thoughts on this.
3326740	3330580	But yeah, I mean, there's another way maybe to think
3330580	3332300	about it, which is that the whole evolutionary,
3332300	3333180	but never mind the lineage,
3333180	3335820	but the whole evolutionary process itself,
3335820	3336980	and that gets hard.
3336980	3338500	I don't have too much to say about it right now,
3338500	3339620	but I'm working on it.
3339620	3343740	This notion of what actually can be an agent,
3343740	3345740	you know, processes as agents.
3345780	3347660	That's a whole other kind of kettle of fish.
3347660	3349620	So I think, you know, well, next year,
3349620	3351740	maybe if I get anywhere with it,
3351740	3353340	maybe we talk about it next year.
3353340	3354700	Yeah, yeah, that'd be interesting.
3354700	3359700	I'm gonna definitely be on the lookout for that paper.
3360700	3362620	And I think, yeah, you touched on so much, yeah,
3362620	3364980	so much already about what's in this paper,
3364980	3368140	the difference between first order and second order,
3369380	3373060	say goals or say a certain amount of flexibility
3373060	3378060	that is built in to the first initial levels there.
3381020	3385460	What else, what do you think is, I guess,
3385460	3387860	if you had to say for people to take one thing away
3387860	3389940	from this paper, say a lay audience,
3389940	3393940	what do you think would be something that,
3395220	3396420	you know, this is hard.
3396420	3398260	I wanna get to a little bit of this,
3398260	3400180	I love your work, it's fantastic.
3400260	3402900	But what I want to perhaps push
3402900	3404980	for some of the people I interview a little bit more on
3404980	3407460	is like, okay, this is amazing stuff.
3407460	3410380	How does someone, you know, an individual person,
3410380	3412100	is there stuff that we can take away here
3412100	3415540	that are there like truths that, you know,
3415540	3417420	maybe as part of our day-to-day lives
3417420	3422340	that we can sort of integrate ideas like from this
3422340	3424340	or anything from your own life, perhaps,
3424340	3427580	examples of things that this is illuminated for yourself.
3428260	3430580	Well, let's put it this way, you know,
3430580	3434020	I don't know if anything from this paper
3434020	3437380	will, you know, revolutionize day-to-day,
3437380	3440420	like mundane life, but a lot of people,
3440420	3443700	but I will, let's try to pull in that direction.
3443700	3447380	A lot of people think about evolution
3447380	3450620	and one of the things that always bugs them,
3450620	3455060	and especially, you know, a lot of engineers too,
3455060	3460060	is this standard story of, well, we make random changes
3461100	3463340	and then we pick the good ones, right?
3463340	3466140	I don't know about you, but when I first learned about this,
3466140	3469140	I had been building, you know, electronics
3469140	3471980	and things like that as a kid for some years,
3471980	3473980	and then I learned this theory, you know,
3473980	3476500	it sounded laughable, it sounded like,
3476500	3479460	you're telling me that I'm gonna make random changes
3479460	3482580	in this thing, I huff and puff for many hours
3482580	3484460	and then the person who made these transistors
3484460	3486540	and everything else will put in even more work than that,
3486540	3488940	like we're all busting our butts on this stuff
3488940	3490700	and you're gonna make random changes
3490700	3492700	and you think eventually things will get better,
3492700	3494980	like that's, you know, if you've ever built anything
3494980	3497100	or written code, that sounds crazy,
3497100	3499940	and then okay, you know, so you learn some things
3499940	3504340	about modularity and evolvability and some things that,
3504340	3509060	but a lot of people are still left with this idea
3509060	3512100	that, okay, in theory it might work,
3512100	3514980	but we kind of know that genetic algorithms,
3514980	3516140	you know, they did great for a while,
3516140	3518100	but they sort of peter out, there's some limit,
3518100	3521100	you know, there's kind of limitations to what they can do,
3521100	3523980	and in particular, but part of what makes it,
3523980	3528220	what makes it a little challenging is what Andreas Wagner,
3528220	3531620	which I think his work is amazing and important,
3531620	3534420	what he calls the problem of the arrival of the fittest,
3534420	3539420	which is that if the best solution is hiding somewhere
3539540	3542820	in your population, I suppose I'll give you that,
3542820	3545100	I'll give it to you that eventually we'll sort of find it
3545100	3548300	and let it expand, but who guaranteed it was there
3548300	3549140	in the first place?
3549140	3550740	How do you know that the right solution
3550740	3551820	is ever going to be there,
3551820	3553220	depending on what your problems be?
3553220	3557580	So anyway, so a lot of people are still left with this,
3557580	3560460	you know, this dissatisfaction about how there's just,
3560460	3562340	how do we know that there's been enough time
3562340	3564740	for this kind of process to give us the amazing things
3564740	3566260	we see in the biological world?
3567300	3568580	And I'm not talking about that, I mean,
3568620	3571020	so there are some people that will never buy the story
3571020	3573180	because they are fundamentally like committed
3573180	3574780	to another kind of story, I'm not talking about them,
3574780	3579260	I'm talking about people with a scientific kind of worldview
3579260	3581380	that want to understand in a naturalistic way
3581380	3586260	what's going on, but the standard story doesn't seem
3586260	3588260	like it's the whole story, right?
3588260	3590460	And there's a lot of very smart people
3590460	3592780	who are sort of thinking along those lines.
3592780	3596060	So what I would point out, my kind of contribution
3596100	3599420	to that is this, part of what makes,
3599420	3602460	part of what makes that process so magical
3602460	3604260	is not just the process itself,
3604260	3607580	it's the fact that you're working on a material that's smart,
3607580	3609660	you're working with an agential material,
3609660	3611780	that's part of where the power comes from.
3611780	3615060	If the whole thing seems tough to you,
3615060	3617900	add to your sort of mental picture,
3617900	3620740	the fact that it's not really searching
3620740	3622940	the incredibly difficult and large space
3622940	3624980	of all the possible things that could happen,
3625020	3626700	what it's searching is the space
3626700	3629340	of behavior-shaping signals
3629340	3632660	by which cells tell other cells what to do.
3632660	3635940	And that's a much easier space to search, right?
3635940	3640340	If you're dealing with, if you run that rat circus,
3640340	3644060	you could try to come up with a kind of
3645980	3648580	an optogenetic strategy to control every neuron
3648580	3651500	to get the rats to sort of do whatever they're gonna do,
3651500	3652660	that's really hard.
3652660	3653940	We'll be here till the sun burns out
3653940	3656900	before we can micromanage it.
3656900	3658020	But you don't have to do that
3658020	3659500	because you can train the rats.
3659500	3661820	And that's a much easier, I mean, it's still a bit of search
3661820	3664060	because you still have to figure out, well, what's the reward,
3664060	3666180	what's the punishment, what are they capable of,
3666180	3668100	do they do place conditioning,
3668100	3670100	do they do associative learning, what do they...
3670100	3671900	So there's still some searching involved,
3671900	3676300	but it's a much easier problem than going bottom up.
3676300	3678820	And so this is what I want people to take away from this
3678820	3681540	is that part of what makes evolution so magical
3681540	3684980	is that it's working with an agential material
3684980	3686700	that has tons of competencies.
3686700	3691700	It's evolution is playing with hardware
3691700	3693740	that is so far in capability.
3693740	3696820	And I don't mean the fact that it goes down to the nano level
3696820	3699340	and I don't mean that it's, in fact,
3699340	3701620	it's way noisier and more brittle and whatever
3701620	3703860	than all the things we try to build.
3703860	3706420	Despite all that, it is so much more powerful
3706420	3707820	than anything we've ever made
3707820	3710940	because it is not a single specific thing.
3710940	3715940	It's a learning machine, so to speak.
3715940	3720460	And that puts evolution on steroids.
3720460	3724660	That's what I think is a huge motivating factor.
3724660	3726380	And I think until we understand that,
3726380	3728580	we are not going to have, I mean, that's to me,
3728580	3730740	that's if I can dare to say this,
3730740	3731980	the thing that I think is missing
3731980	3734460	from the standard evolutionary synthesis is this.
3734460	3737300	It's the appreciation of the intelligence of the substrate.
3737300	3738900	It treats the standard story,
3738900	3741460	it treats the substrate as a bunch of dumb Lego blocks
3741460	3742620	and everything changes.
3742620	3745380	When the material has agendas, everything changes
3745380	3747700	and evolution changes massively.
3747700	3750660	So, I don't know if that counts as everyday life
3750660	3753980	for people, I suppose that some people think
3753980	3755540	about that stuff every day, so.
3755540	3756700	No, no, it's good.
3756700	3759300	I find it useful because it's a frame,
3760460	3765060	it's a perspective on how you look at the problem.
3765060	3766700	I mean, that's something we've talked about in round two.
3766700	3770820	I think like polycomputing and the role of the absurd,
3770820	3772700	the same thing can be computed.
3772700	3775580	But depending on how you look at it, you actually get,
3775580	3777900	you actually are extracting that different value,
3777900	3780580	different utility from the same exact thing.
3780580	3783500	So, yeah, that's wonderful.
3783500	3785380	And actually, I think this will,
3786900	3788300	just looking at the time a little bit here,
3788300	3789980	I do wanna cover a little bit
3789980	3793140	about the biology, Buddhism and AI paper.
3793980	3796980	I think it was Kair as a driver of...
3796980	3797980	I forget exactly.
3797980	3799820	Yeah, Kair is a driver of intelligence,
3799820	3802740	but she worked on with a few collaborators,
3802740	3806100	Dr. Witkowski, Salmanova and Dwayne.
3806100	3811100	And this, I think actually does bleed into this fairly well
3811980	3814900	because in the Ingencial Point Materials paper,
3814900	3817300	you mentioned the idea of like beginner mind.
3817300	3821500	And you have a quote from Suzuki about the,
3821500	3823220	in the beginner's mind, there's many possibilities
3823220	3825380	and the expert's mind, there are a few.
3825380	3829900	So you kind of wanna have a frame of like always being,
3829900	3831660	having a frame of always being a beginner
3831660	3833880	and being like open and curious to things all the time
3833880	3838160	is a lot better than, this is a straw manning it,
3838160	3839860	but it's a lot better than being an expert
3839860	3842740	and being like, I know everything, I've mastered,
3842740	3843940	there's nothing else left to go, right?
3843940	3848260	There's always potential for more learning and more mastery.
3848260	3852380	So for the Biology, Buddhism and AI paper,
3852380	3854540	would you mind providing us again,
3854540	3856620	just a brief overview of the overall paper
3856620	3860180	and also how you got interested in this topic
3860180	3863740	to begin with and how you got into this group
3863740	3866820	of folks who are studying this.
3866820	3870460	Yeah, well, how I got interested in it,
3870460	3872220	I've been interested in these kinds of things
3872220	3874700	for a really long time, both from the perspective
3875700	3878260	of kind of Eastern thought about the philosophy of mind
3878260	3881460	and things like that and more broadly questions
3881460	3886460	of concern and compassion and things like that.
3887180	3890260	How we met up, to tell you the truth,
3890260	3893300	I don't remember who reached out first,
3893300	3896020	it might have been Thomas Doctor that emailed me,
3896020	3898180	I mean, Olaf and I have known each other,
3898180	3902540	he's a great contributor to the artificial life community
3902540	3905980	and so I've known him and his work for a long time,
3905980	3909580	Bill and Eliza and Thomas I met afterwards.
3910500	3912740	Yeah, I don't recall who made the first step,
3912740	3915100	but anyway, we've been talking about this stuff
3915100	3916180	and thinking about it for a long time
3916180	3917340	and there's actually a second paper
3917340	3920700	that just got accepted or we just returned it,
3920700	3921900	I don't remember exactly where it is,
3921900	3923380	but there's a second paper following up
3923380	3924380	on all of this stuff.
3925420	3928620	You can find the preprint is on the website,
3928620	3933620	but this idea of specifically this idea of care
3934780	3936580	and what do agents care about?
3936580	3940820	And for me, it has lots of important implications
3940820	3943660	because I try to understand collective intelligences
3943660	3945740	and so if you're gonna have a collective intelligence,
3945740	3948220	what is it going to care about?
3948220	3950100	We don't have a good science of that.
3950100	3955100	And then there's the notion of embedding care
3956060	3957420	in artifacts that we make.
3957420	3960780	So robotics, AIs, what are they gonna care about?
3960780	3965220	It's kind of a funny story when my kid was,
3965220	3968540	my youngest was, I wanna say he was three or four
3968540	3971740	and he said, we used to build stuff together all the time
3971740	3974260	and we did all kinds of engineering things
3974260	3978060	and one day he said to me, let's make a cat.
3978060	3979580	And I said, well, like a robotic,
3979580	3980700	you wanna make a robotic cat.
3980700	3982620	And I said, well, let's make a list
3982620	3984020	of what are the design specs here?
3984020	3985740	Like what does this thing need to do?
3985740	3988660	And he says, well, it needs to move around.
3988660	3991260	I'm like, yeah, maybe like that, that may be doable
3991260	3993260	and it needs to make meowing noises
3993260	3994780	and it's pretty much doable.
3994780	3996900	And it needs to do something else that it needed to do.
3996900	3998540	And I said, yeah, probably we could do that.
3998540	4000900	And then he says, and it needs to care.
4000900	4002380	And I said, what do you mean?
4002380	4004180	You mean it needs to like walk over to you
4004180	4006580	and let you pet it?
4006580	4009220	He goes, no, no, not act as if it cared.
4009220	4010940	I wanted to actually care.
4010940	4011900	And I was like, all right, well, that's it.
4011900	4013180	You've just broke the whole project
4013180	4015420	because we don't have a clue
4015420	4017940	as to how that is going to happen.
4017940	4019940	And that is, you know, that,
4019940	4024940	figuring out how that ties into the whole,
4026340	4028500	you know, the cognitive light cone story
4028500	4029340	that I've been telling
4029340	4031620	and the kind of the spectrum of precipitability.
4031620	4033820	It's like intelligence is one thing,
4033820	4037100	problem solving is one thing.
4037100	4040660	But where does the care come from, right?
4040660	4041980	And what do we mean by that?
4042340	4044780	And a lot of people say things like,
4044780	4046540	I care about stuff.
4046540	4048420	That's just the machine.
4048420	4049940	They're usually pointing at some AI thing
4049940	4051580	or some robot or something.
4051580	4052540	And they say, well, that's just the machine.
4052540	4054060	Machines can't care.
4054060	4056740	And I'm like, well, you used to be a single cell.
4056740	4060500	And do paramecia care?
4060500	4062020	Because now you got a problem.
4062020	4065260	If you say that the paramecia care,
4065260	4067620	then well, guess what's inside of paramecia?
4067620	4069540	A bunch of molecular cogs and wheels.
4069540	4073500	So, you know, you gotta,
4073500	4076380	so maybe machines and certainly molecular biologists
4076380	4078340	see single cells as a kind of machine.
4078340	4083180	And that analogy has done pretty well for us.
4083180	4085140	On the other hand, if you say, no, no,
4085140	4086380	the paramecium doesn't care.
4086380	4088100	It's just a bunch of chemical reactions.
4088100	4089780	I mean, we can sort of see what's going on in there.
4089780	4091980	It's a bunch of chemistry that doesn't care.
4091980	4092820	I care.
4092820	4094260	Like, well, you used to be a single cell.
4094260	4095460	So why don't you tell me where that,
4095460	4096660	when the care got beamed,
4096660	4099540	like what stage of embryogenesis
4099540	4100780	does the care get beamed down, right?
4100780	4101940	That's a problem too.
4101940	4104460	So you got this real issue with people
4104460	4105980	who think in binary categories,
4105980	4107500	they get trapped in this pseudo problem
4107500	4109380	that I think I'm unsolvable.
4110380	4114660	So anyway, so we were really interested.
4114660	4116500	And then of course, from their perspective,
4116500	4119780	they're interested in the questions of,
4119780	4123900	there's a Buddhist story about care and compassion
4123900	4124740	and those kinds of things.
4124740	4127380	So I was interested to see how compatible those things are.
4127380	4130260	Can we use some tools from that thought?
4130260	4132100	I mean, there's a whole other thing,
4132100	4137100	which is this notion of the impermanence of the self.
4137180	4138700	And it goes, I mean, there are obviously
4138700	4141540	all different kinds of opinions on this
4141540	4144900	all the way from there is no such thing.
4144900	4147900	It's a total illusion, right?
4147900	4149380	That's one set of views.
4149380	4151180	And then on the other is the kind of like
4151180	4154220	the sort of demand on the street version,
4154220	4157460	which is, well, I've got this permanent to self
4157460	4160340	and it's this like thing and then it exists.
4160340	4162180	And then what I'm interested in
4162180	4164260	is kind of the space in between,
4164260	4165820	which I think is more accurate.
4165820	4168460	So, and by the way, I'm no Buddhist scholar.
4168460	4173180	I don't pretend to know who thinks what in that area.
4173180	4176700	I'm just, I'm trying to keep up with Thomas
4176700	4178940	and Elizabeth and so on.
4178940	4182540	But there's an intermediate version,
4182540	4185380	which is that it's not that you don't exist.
4185380	4188180	You do in the same sense as everything else exists,
4188180	4190180	which is as a useful metaphor.
4190180	4194060	And in fact, you are the most useful metaphor for all,
4194060	4196940	of all, because you might do away with,
4196940	4199180	you might somehow do away with metaphors
4199180	4204180	of talking about, I don't know what social structures are,
4205620	4207300	what grocery stores are.
4207300	4211100	Maybe you don't, maybe you now see,
4211100	4214060	like Eddington said, that a table is mostly empty space.
4214060	4216020	And so maybe you've internalized physics enough
4216020	4219100	to know that even the table isn't a great metaphor
4219100	4221900	that it's mostly fields and whatnot.
4221900	4223740	Like all of that is fine,
4223740	4228740	but there is this metaphor of a self that can do things
4232940	4234980	because it's on you to do stuff or not do it.
4234980	4236460	You have to make those decisions.
4236460	4239580	That's a pretty useful metaphor.
4240180	4241700	So I don't think it's not real.
4241700	4243300	I think it is real in the same sense
4243300	4244260	that everything else is real,
4244260	4246420	which is a useful construction.
4247780	4250300	Also, I think what's useful about it
4250300	4253780	is that it is continuously self-constructed.
4253780	4256220	And here's what I mean by that.
4256220	4259000	At any given moment, so right now,
4259000	4262100	you don't really have access to your past.
4262100	4264220	The only thing you have access to are the engrams
4264220	4266580	left in your brain and body by your past experiences.
4266580	4267780	That's all you have access to.
4267780	4270980	And from that, you reconstruct the story of your past.
4270980	4272900	So including the school you went to
4272900	4273740	and various other things,
4273740	4276320	you're building that right now at every moment.
4276320	4280980	It's a little bit like anterior grade amnesia patients
4280980	4283100	who can't form new memories.
4283100	4285340	And so they use these scratch pads,
4285340	4286940	at least some of them where you write down,
4286940	4288500	the first thing you write down is that
4288500	4290060	I have anterior grade amnesia
4290060	4291700	and then some stuff that happened.
4291700	4292740	And then at the end, that says,
4292740	4295300	and don't forget, write this note again tomorrow.
4296140	4299140	And that's your, like most of us have the same thing.
4299140	4300540	It's just internalized,
4300540	4302580	but now they're using the Stigmergically,
4302580	4305140	this outside tool because there's some problem.
4305140	4308740	So we are all really in that state.
4308740	4311780	It's just we're using an internal scratch pad.
4311780	4313060	Right now, you don't have any access
4313060	4314820	to what happened years ago.
4314820	4317420	You just have the memories and they're actively rebuilt.
4317420	4319860	And we all know our ability to rebuild accurately
4319860	4321900	is crap basically, right?
4321900	4326220	And these things morph and change and whatnot.
4326220	4328300	So the story of ourselves changes all the time.
4328300	4333300	I actually, I did a, I gave a talk the other day
4333740	4337460	at this UCLA symposium and I talked about Plenaria
4337460	4341100	and this idea, in Plenaria, if you teach them something
4341100	4342860	and then you chop off their heads
4342860	4344100	and the tail sits there
4344100	4345820	and then eventually they regrow a new head
4345820	4347620	and they regenerate their memories, right?
4347620	4350060	And so that means, okay, the memory is stored somewhere
4350060	4351100	to who knows where it is,
4351100	4354140	but the interesting part of that is the memories
4354140	4358380	are actually imprinted onto the new brain as it develops.
4358380	4360700	And so this new being, this new Plenarian
4360700	4363260	has to rebuild itself along with its memories.
4363260	4365300	I mean, I don't know how rich a Plenarian's memories really are
4365300	4366740	but whatever they are,
4366740	4371100	it has to like completely rebuild itself from these memories.
4371100	4374180	And that sounds all crazy and weird
4374180	4376980	and it sounds like the cases of like,
4377820	4382020	what's the, you know, Blade Runner and everything
4382020	4385020	when you find out that, oh crap, my memories aren't really
4385020	4388140	my memories, I mean, as far as this Plenarian brain is concerned
4388140	4390300	it just got downloaded a bunch of false memories.
4390300	4392380	That brain was not part of any of the things
4392380	4394580	that it remembers, they wasn't there, didn't exist.
4394580	4397140	So it's kind of a bunch of false memories.
4397140	4399740	And so, and so it was like, wow, these Plenarians are crazy
4399740	4401620	and this thing with the androids is nuts.
4401620	4406460	And my point was, no, no, no, this is what we are 24 seven.
4406460	4408540	This is completely normal because all of us
4408540	4411380	are reconstructing ourselves at every moment.
4411380	4412700	And I don't know how wide the moment is,
4412700	4415580	but I'm sure that neuroscientists will tell us
4415580	4419580	you are reconstructing yourself from these past memories.
4419580	4423060	And so I think that's a deep kind of philosophical thing
4423060	4428060	because, you know, your self isn't some permanent
4428900	4431460	monadic structure that just kind of exists.
4431460	4433940	It's an active construction, it's a process,
4433940	4437580	it's a constant information processing.
4437580	4440820	Autopoiesis, you know, of the mind doesn't stop
4440820	4443940	during embryogenesis, it kind of keeps going, it has to.
4443940	4448940	And it has these interesting implications.
4449540	4452980	If somebody, it will go going back to, you know,
4452980	4457980	I guess I'm now in the business of trying to normalize
4458300	4459940	a lot of things that people get freaked out about,
4459940	4461740	but you know, imagine, right?
4461820	4464460	So if somebody finds out that, oh my God,
4464460	4466900	like all of these memories that I have now,
4466900	4468540	that wasn't me, that were downloaded,
4468540	4471620	my body was just, you know, was it Boltzmann
4471620	4474100	or was it Humor, somebody had this puzzle,
4474100	4476660	like what happens if you're not,
4476660	4478980	if all your memories were just like,
4478980	4481140	you were created 10 seconds ago,
4481140	4484420	including your memories, right, or something like that.
4484420	4485660	If you think about it hard enough,
4485660	4488900	given that that's normally our situation anyway,
4488900	4490860	my answer is, who cares?
4490860	4493660	Great, like move on, you've got them now, go for it.
4493660	4496140	Now you've got some great memories, like roll with it,
4496140	4498380	because what else is it gonna be?
4498380	4500700	Of course you were just constructed with your memories,
4500700	4501700	what else could it be?
4501700	4503620	You are constantly constructing yourself
4503620	4505260	from the n-grams in your head,
4505260	4508300	you don't have access to what actually happened before.
4508300	4510540	You are, I just don't even, you know,
4510540	4512700	that view seems weird to people,
4512700	4515620	and I can't even verbalize what the alternative would be,
4515620	4519220	I just don't even understand what an alternative story
4519220	4520500	could possibly be.
4521380	4522860	So from that perspective,
4524500	4528700	you know, in fact, you can go further with this,
4528700	4530340	the body that I have now,
4530340	4533580	given the turnover of cells and molecules in your body,
4533580	4536180	was this body actually around 30 years ago
4536180	4538340	to do the things that I remember doing?
4538340	4540060	It actually wasn't, we know it wasn't,
4540060	4541860	even though I've not been part of some weird memory
4541860	4544100	replacement experiment, you know, and I'm not an Android,
4544100	4546060	this body wasn't there, I know it wasn't,
4546060	4547340	we know it wasn't there.
4547340	4549780	And yet I have these memories, so am I complaining?
4549780	4550700	What would you complain about?
4550700	4552820	If somebody told you that, you know,
4552820	4554100	yeah, your body was just, you know,
4554100	4555660	you were killed in a record,
4555660	4557020	or maybe your body never existed,
4557020	4557860	but we just made you,
4557860	4559900	and like here are some great memories of a past life,
4559900	4562460	like bring it on, fantastic, you know,
4562460	4563500	I hope they're good ones.
4563500	4565420	And because I don't know what the,
4565420	4567460	I don't even know what the alternative would be.
4567460	4570660	So I think, for that reason,
4570660	4573220	I think all of these things are really,
4573220	4574460	really hopeful and positive.
4574460	4576500	You know, they just tell us that,
4576500	4578660	we shouldn't be afraid of these technologies,
4579340	4583820	this is the amazing thing about being a self
4583820	4584940	in this universe,
4584940	4587820	is you get to constantly construct yourself,
4587820	4589700	and by the way, guide what happens in the future,
4589700	4592900	what you do now determines the experiences
4592900	4594940	you're going to have, the reactions you're going to have,
4594940	4597940	which of course, you know, those kinds of disciplines
4597940	4602940	and traditions are all about that,
4602940	4605700	about consistent practice to train yourself to be better,
4605700	4608580	to have, you know, to improve your cognitive,
4608580	4610460	you know, apparatus and so on.
4610460	4613060	And the commitment, this is the last thing,
4613060	4616540	the bodhisattva vow, which is huge,
4616540	4619020	it's this commitment, it's a meta goal,
4619020	4622220	it's the commitment to enlarge your cognitive apparatus
4622220	4623420	to enable bigger goals,
4623420	4625020	to enable you to pursue bigger goals
4625020	4626940	with more compassion facing outwards.
4626940	4629660	That I think is critical because I think,
4629660	4631860	once you are a system with the ability
4631860	4634780	to make that commitment,
4634780	4637860	it's sort of like, it's an exponential rise after that,
4637860	4639820	right, it's like discovering the scientific process.
4639820	4641820	Before that, it was all sort of screwing around,
4641820	4642860	you know, trial and error,
4642860	4643860	but as soon as you figured out
4643860	4645740	that there's a systematic thing that you,
4645740	4648980	I am going to literally work to get better at this
4648980	4650300	and to be able to have more,
4650300	4653220	increase my cognitive light cone of compassion,
4653220	4656180	you can now sort of exponentially go up
4656180	4657420	because you understand what you're doing,
4657420	4660860	it's not just, you know, just a random walk.
4660860	4664180	Yeah, now I love what you've said before too
4664180	4666620	about how something to this degree
4666620	4668940	that you can't control your next thought,
4668940	4671740	but you can't control your thoughts 10 years from now
4671740	4673100	by what you do today, right,
4673100	4674180	and what you do every day,
4674180	4677020	you can like influence the future.
4678860	4680660	One thing I really wanted to ask you about,
4680660	4682260	and it's not in this paper,
4682260	4685260	but I imagine other folks have worked on this,
4686140	4690620	the idea of what it's like to perceive,
4690620	4693460	let's say the self in a broader way.
4693460	4696180	And like you mentioned with the Bodhisattva vow,
4696180	4697460	and I think this is actually really nice
4697460	4699820	because it brings us full circle with our conversation
4699820	4700900	from our very first one,
4700900	4702980	which was around the cognitive light cones.
4702980	4705260	And in this paper, you do have a diagram
4705260	4708420	where you've like overlaid what the,
4709340	4712540	what someone with a wider sense of self,
4712540	4714780	say a one to help out the community
4714780	4717700	or something that's also one that takes the vow
4717700	4721180	to not achieve enlightenment until everyone else does, right?
4721180	4725580	It's like this leave no living being behind kind of idea, right?
4727100	4729660	But do we have any research or even discussions
4729660	4732820	with say monks, people who engage
4732820	4736740	in say like deep Buddhist meditative practices?
4736740	4739860	Do they perceive themselves as like a part of a whole?
4739860	4744260	Like is their actual sensory experience different
4744260	4747940	than say a very enlightened person like myself
4747940	4749780	or like most folks?
4749780	4750860	Like does that make any sense?
4750860	4753820	Like how they perceive themselves in the world?
4753820	4756100	Yeah, I think that's a great question.
4756100	4759180	I don't think I've got the expertise to answer that question.
4759180	4761220	I think you could talk to,
4761220	4763740	you could have Thomas Doctor on, for example,
4763740	4768340	and he's a very kind of experienced scholar in that area.
4768340	4772380	And I think he would, you'd have a good time.
4772380	4773860	You could explain all that stuff.
4774060	4776140	Yeah, yeah, I think I'll have to.
4776140	4780060	And let's see.
4780060	4780780	And that's great.
4780780	4783580	So you gave me so much to chew on and so much for the audience too,
4783580	4785980	I'm sure this idea that who you are today
4785980	4787340	is not who you were 30 years ago,
4787340	4790780	just in a very literal materialistic sense.
4790780	4792860	And that there's like just this illusion
4792860	4796860	of a consistent, continuous kind of experience.
4796860	4799980	It's a really, it's a wonderful way to put it.
4799980	4801980	And if you wouldn't mind, I mean, I do,
4802100	4805820	I'll probably overlay the cognitive light cone
4805820	4810540	with the Bodhisattva, the Bodhisattva vowel one.
4810540	4812220	How has this been, because this came out,
4812220	4813780	this paper came out about a year ago,
4813780	4814700	how has it been received?
4814700	4816740	Have you heard any response from the paper?
4821980	4824460	A little bit, a little bit.
4826660	4827820	Yeah, I don't know.
4827820	4832100	I don't track responses super, super carefully,
4832100	4833380	so I'm not sure.
4833380	4836100	Haven't heard a ton, but I'm gonna guess,
4836100	4839460	I'm gonna guess Thomas heard more on it.
4839460	4842140	But I think that, I've certainly had interesting people
4842140	4843380	reach out to me to talk about it,
4843380	4846340	which is pretty much one of the things
4846340	4848500	I hope for in writing these things.
4848500	4850300	So I've had lots of cool discussions.
4850300	4851500	I don't know more broadly.
4851500	4853900	I mean, I have no idea how, in any of these papers,
4853900	4857100	by the way, I have no clue if anybody reads them
4857100	4859420	or who reads them or what happens after that.
4859420	4861300	I don't know, yeah, yeah, well, I appreciate that.
4861300	4863900	And certainly people contact me,
4863900	4868180	but it's very hard to know how these things
4868180	4869980	are actually spreading or not spreading
4869980	4871620	through the community.
4871620	4873260	Gotcha, and I'm gonna let you go in a moment.
4873260	4874980	But before I do one last thing,
4874980	4876780	you're a big mid-journey fan.
4878500	4881380	You oftentimes have, seems like you're also a fan
4881380	4882580	of surrealist arts.
4882580	4886300	I've seen stuff from prompts that are,
4886380	4889500	like, do this in the style of the Codex Seraphoninus
4889500	4892860	or in the style of the Chironomous Bosch.
4892860	4895420	Do you know, like, what is it that draws you towards,
4895420	4896900	like, those kind of art styles?
4900060	4901900	I don't know.
4901900	4905460	I have had zero art training.
4905460	4906340	I kind of know what I like,
4906340	4907740	but I don't know anything about art.
4907740	4910780	I can't draw on myself at all, like nothing.
4910780	4915500	And yeah, and I mean, all I know is that
4916420	4919260	I really like, I'm really interested
4919260	4920860	in the space of the possible
4920860	4925540	and much, much more so than the actual.
4925540	4930540	And everything I look at art-wise
4930540	4935540	is either photography of nature or photography thereof.
4935660	4938180	I like nature, but other than that,
4939740	4941820	yeah, I'm really into imagining
4941820	4945220	what could be the latent space of possibilities.
4945220	4948260	And I think, you know, mid-journey and systems like that
4948260	4950620	are pretty cool in that respect.
4950620	4954460	They let you explore this wacky latent space
4954460	4958340	that it has of images of all different kinds.
4958340	4960740	Yeah, that's just kind of generally,
4962060	4966820	I think very much forward in terms of, like, what now,
4966820	4968900	what next, what could we do next?
4968900	4971860	That's, you know, maybe that explains a lot
4971860	4976060	of my kind of, you know, not getting worked up
4976060	4978900	about whether the past is real or not or what.
4978900	4980700	It's mostly the forward-looking stuff.
4980700	4982260	I mean, like whatever in the past, but like now,
4982260	4983260	what do we do now?
4983260	4985380	So that's more what I'm interested in.
4985380	4990060	And I like this kind of art that lets you,
4990060	4992460	yeah, imagine things that could be,
4992460	4995100	what are the possibilities moving forward?
4995100	4998260	Yeah, I share same love for that style as well,
4998260	4999500	those styles.
4999580	5002500	And I just want to say thank you so much, Mike.
5002500	5004660	These three conversations have been wonderful.
5004660	5006700	I appreciate your time, your energy.
5006700	5009300	And where should people find out more about you?
5009300	5010820	I'll link in the description.
5010820	5011660	Yeah.
5011660	5015060	Well, everything is, there's an official website,
5015060	5018980	the academic website is www.drmike11.org.
5018980	5021100	So one word, drmike11.org.
5021100	5024340	And that's got all the links to the papers, the software,
5024340	5026940	the presentations, everything else.
5027260	5031660	I've got a science Twitter presence at drmike11.
5031660	5033300	And I think in a couple of months,
5033300	5034700	there will be a WordPress site.
5034700	5037100	So I've been working on a site kind of,
5037100	5039460	yeah, I was sort of thinking sub-stack WordPress.
5039460	5041620	I decided WordPress.
5041620	5043180	And so there's going to be a site
5043180	5045900	that's kind of less the official academic stuff
5045900	5047820	and all kinds of writing that I want to do
5047820	5049380	that doesn't really, you know,
5049380	5051580	kind of tired of asking the question of,
5051580	5054500	you know, you write this thing and it's not really,
5054540	5057060	it isn't a primary paper and it isn't really a review
5057060	5058340	and it's kind of a perspective,
5058340	5059500	but it's really interdisciplinary.
5059500	5060900	It's way too long for a journal.
5060900	5063660	And I'm sort of just tired of this issue
5063660	5065540	of finding a home for it, you know,
5065540	5067100	what they say, like, where are we going to put?
5067100	5068420	Like, okay, forget it.
5068420	5070060	We'll just, it'll go there
5070060	5071940	and then anybody who's interested can read it.
5071940	5072900	And that'll be that.
5072900	5075700	So, yeah, so there'll be, that doesn't exist yet,
5075700	5078980	but in the next couple of months, it should be up.
5078980	5079900	Awesome, great.
5079900	5081500	Well, we will watch that space.
5081500	5082940	And I'll, when it comes live,
5082940	5085100	I'll update all the other videos so that people can get there.
5085100	5085940	That's great, I appreciate it.
5085940	5086780	Thank you, thank you.
5086780	5087620	Yeah, absolutely.
5087620	5088460	Thank you, Mike.
5088460	5089300	Thanks very much.
5089300	5090140	Yeah, thanks for having me on.
5090140	5090980	It was great, thanks.
5090980	5091820	It was awesome.
5091820	5092660	Thank you so much.
5092660	5093500	All right, all right.
