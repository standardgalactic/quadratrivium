start	end	text
0	9760	This week's Moment of Zen is a feed drop for one of Turpentine's biggest shows, The
9760	13280	Cognitive Revolution, hosted by Nathan LeBenz.
13280	17520	Biology and Nathan discussed the evolution, challenges, and role of AI in different realms
17520	21200	like politics, environmentalism, medicine, and more.
21200	24640	There's a lot of new content here about where biology thinks AI is going and interesting
24640	26800	parallels with religion.
26800	30680	Nathan brings up some AI safety concerns in the course of the discussion.
30680	34920	If you like what you hear, check out our other Moment of Zen episodes with Boloji and check
34920	37200	out The Cognitive Revolution.
37200	39200	Please enjoy.
39200	42720	Boloji Srinivasan, welcome to The Cognitive Revolution.
42720	43720	All right.
43720	44720	I feel welcome.
44720	49020	Well, we've got a ton to talk about, you know, obviously you bring a lot of different
49020	53920	perspectives to everything that you think about and work on.
53920	58720	And today I want to just try to muster all those different perspectives onto this, you
58720	62920	know, what I see is really the defining question of our time, which is like, what's up with
62920	66400	AI and, you know, how's it going to turn out?
66400	70200	I thought maybe for starters, I would love to just get your baseline kind of table setting
70200	77920	on how much more AI progress do you expect us to see over the next few years, like how
77920	83600	powerful our AI system is going to become in, again, kind of a relatively short timeline.
83600	87040	And then maybe if you want to take, you know, a bigger stab at it, you could answer that
87040	90240	same question for a longer timeline, like the rest of our lives or whatever.
90240	91240	Sure.
91240	92240	Let me give an abstract answer.
92240	94360	Then let me give a technical answer.
94360	100120	You know, if you look at evolution, we've seen something as complex as flight evolved
100120	104520	independently in birds, bats, and bees.
104520	113320	And even intelligence, we've seen fairly high intelligence in dolphins, in whales, in octopuses,
114280	116760	you know, octopus in particular can do like tool manipulation.
116760	119760	They've got things that are a lot like hands, you know, with tentacles.
119760	127520	And so that indicates that it is plausible that you could have multiple pathways to intelligence,
127520	131440	whether, you know, we have carbon based intelligence, or we could have silicon based intelligence
131440	134440	that just has a totally different form or the fundamental thing is an electromagnetic
134440	139080	wave and data storage as opposed to, you know, DNA and so on, right?
139080	143160	So that's like a plausibility argument in terms of evolution is being so resourceful
143160	148200	that it's invented really complicated things in different ways, okay?
148200	152280	Then in terms of the technical point, I think as of like right now, I should probably date
152280	156640	it as like December 11, 2023, because this field moves so fast, right?
156640	161440	My view is, and maybe you'll have a different view, is that the breakthroughs that are really
161440	167840	needed for something that's like true artificial intelligence that is human independent, right?
167840	171880	Maybe the next step after the Turing test, I've got an article that, you know, we're
171920	176520	writing called the Turing thresholds, which tries to generalize the Turing test to like
176520	178920	the cartage of scale, you know, have you got energy thresholds?
178920	181480	Like what are useful scales beyond that?
181480	188040	And right now, I think that what we call AI is absolutely amazing for environments that
188040	191560	are not time varying or rule varying.
191560	196520	And what I mean by that is, so you kind of have, let's say two large schools of AI, and
196520	200200	obviously there's overlap in terms of the personnel and so on, but there's like the
200200	204000	deep mind school, which has gotten less press recently, but got more press, you know, a
204000	207560	few years ago, and that is game playing, right?
207560	211880	It is, you know, superhuman playing of go without go.
212160	216360	It is, you know, all the video game stuff they've done where they learn at the pixel
216360	219520	level and they don't, they just teach the very basic rules and it figures it out from
219520	223400	there. And it's also, you know, the protein folding stuff and what have you, right?
224240	228000	But in general, I think they're known for reinforcement learning and those kinds of
228040	230360	approaches. I mean, they're good at a lot of things, but that's what I think he finds
230360	234560	known for. Of course, they put out this new model recently, the Gemini model.
234560	237440	So I'm not saying that they're not good at everything, but that's just kind of what
237440	238840	they're maybe most known for.
238840	244000	And then you have the open AI chat, GBT school of generative AI, and it includes stable
244000	248520	diffusion and just as a pioneer, even if, you know, they're not, I don't know how much
248520	252480	they're used right now, but basically, you know, you have the diffusion models for images
252480	256560	and you have large language models and now you have the multimodals that integrate them.
256680	262400	And so the difference, I think with these is the reinforcement learning approaches are
262400	267680	based on an assumption of static rules, like the rules of chess, the rules that go, the
267680	270840	rules of a video game are not changing with time, they're discoverable, they're like the
270840	277760	laws of physics. And similarly, like the body of language where you're learning it, English
277760	282920	is not rapidly time varying. That is to say, the rules of grammar that are implicit aren't
282960	287040	changing, the meanings of words aren't changing very rapidly, you can argue they're changing
287040	292000	over the span of decades or centuries, but not extremely rapidly, right? So therefore,
292000	297040	when you generate a new result, training data from five years ago for English is actually
297040	301840	still fairly valuable, and the same input roughly gives the same output. Now, of course,
301840	306920	there are facts that change with time, like who is the the ruler of England, right, the
306920	309920	Queen of England is passed away now, it's the King of England, right, which is facts
309920	313920	that change with time. But I think more fundamentally is when there's rules that change with time,
313920	319400	you know, you have, for example, changes in law and countries, right? But most interestingly,
319400	324080	perhaps changes in markets, because the same input does not give the same output in a market.
324080	328200	If you try that, then what will happen is there's adversarial behavior on the other side. And
328200	331360	once people see it enough times, they'll see your strategy, and they're going to trade
331360	335560	against you on that, right? And I can get to other technical examples on that, but I think,
335560	339320	and probably people in the space are aware of this, but I think that is a true frontier
339360	345240	is dealing with time varying rule varying systems, as opposed to systems where the implicit rules
345240	349600	are static. Let me pause there. Yeah, I think that makes sense. I think the, you know, in the
349600	356600	very practical, you know, just trying to get as V calls it mundane utility from AI, that is often
356600	361640	kind of cashed out to AI is good at tasks, but it's not good at whole jobs. You know, it can
361640	366240	handle these kind of small things where you can define, you know, what good looks like and tell
366280	373040	it exactly what to do. But in the sort of broader context of, you know, handling things that come
373040	380320	up as they come up, it's definitely not there yet. And I agree that there's likely to be some
380320	386480	synthesis, you know, which is kind of the subject of all the Q star rumors. Recently, I would say
386480	393760	is kind of the, the prospect that there could be already, you know, within the labs, a beginning
393800	399280	of a synthesis between the, I kind of think of it as like harder edged reinforcement learning
399280	405360	systems, you know, that are like small, efficient and deadly versus the like language model systems
405360	411360	that are like kind of slow and soft and, you know, but have a sense of our values, which is really
411360	416160	a remarkable accomplishment that that they're able to have even, you know, an approximation of our
416160	424960	values that seems like reasonably good. So yeah, I think I agree with that framing. But I guess I
424960	432720	would, you know, still wonder like, how far do you think this goes in the near term? Because I have
432720	436160	a lot of uncertainty about that. And I think the field has a lot of uncertainty. You hear people
436160	440080	say, Well, you know, it's never going to get smarter than its training data, you know, it'll
440080	444000	kind of level out where humans are. But we certainly don't see that in the reinforcement
444000	450960	learning side, right? Like once it usually don't take too long at human level of these games,
450960	454720	and then it like blows past human level. Interestingly, you do still see some adversarial
454720	460560	vulnerability, like there's a great paper from the team at FAR AI, and I'm planning to have
461280	464800	Adam Gleave, the head of that organization on soon to talk about that and other things,
464800	472240	where they found like a basically a hack where a really simple, but unexpected attack on the
472240	477600	superhuman go player can defeat it. So you do have these like very interesting vulnerabilities
477600	482960	or kind of lack of adversarial robustness, still kind of wondering like, where do you think that
482960	487920	leaves us in say a three to five years time? Obviously, huge uncertainty on that. It's really
487920	493840	hard to predict something like this. Just to your point, generative AI is generic AI, right?
493840	498880	It's like generically smart, but doesn't have specific intelligence or creativity or facts.
498880	502720	And as you're saying, just like we have, you know, adversarial images
502720	508800	back in full programs that are trained on a certain set of data, and they just give some
508800	513120	weird, you know, pattern that looks like a giraffe, but the algorithm thinks it's a dog,
513680	517360	you can do the same thing for game playing, and you can have out of sample input that can beat,
518480	522000	you know, these very sophisticated reinforcement learners.
523360	528240	And an interesting question is whether that is a fundamental thing, or whether it is a
529760	534000	work aroundable thing. And you'd think it was work aroundable, you know,
534880	539600	because there's probably some robustification because these pictures look like giraffes,
539600	545120	you know, and yet they're being recognized as dogs. See, there's, you would think that
545760	551360	the right proximity metric would group it with giraffes, you know, but maybe there's some,
552000	556400	I don't know, maybe there's some result there. My intuition would be we can probably
557280	561760	robustify these systems so that they are less vulnerable to adversarial input.
563280	566320	But if we can't, then that leads us in a totally different direction,
566320	569280	where these systems are fragile in a fundamental way.
570160	575680	So that's one big branch point is how fragile these systems are, because if they're fragile in
575680	581360	a certain way, then it's almost like you can always kill them, which is kind of good, right,
581360	587280	in a sense, that there's that, you know, almost like the, you know, the 50 IQ, 100 IQ, 150 IQ
587280	593280	thing, like the, the meme. Yeah, the meme, right. So the 50 IQ guys like these machines will never
593280	598560	be as creative as humans or whatever. 100 IQ is look at all the things they can do. The 150 IQ
598560	603680	is like, well, there's some like equivalent equivalent result, you know, that's like some
603680	608000	impossibility proof that shows that we the dimensional space of a giraffe is too high,
608000	611600	and we can't actually learn what a true giraffe, I don't think that's true.
612240	616400	But maybe it's true from the perspective of how these learners are working,
616400	620080	because my understanding is people have been trying, and I mean, I'm not the cutting edge of
620080	624080	this. So, you know, maybe something, but my understanding is we haven't yet been able to
624080	629040	robustify these models against adversarial input. Am I wrong about that?
629680	630720	Yeah, that's definitely right.
631280	634080	We'll continue our interview in a moment after a word from our sponsors.
634400	662480	There's no single architecture, as far as I know, that is demonstrably robust. And on the contrary,
662480	666720	you know, even with language models, there's a, we did a whole episode on the universal jailbreak,
667280	671920	where, especially if you have access to the weights, not to change the weights, but just to
671920	677600	kind of probe around in the weights, then you have a really hard time, you know, guaranteeing
677600	684480	any sort of robustness. The conjecture is, see, for humans, you can't like, mirror their brain
684480	690160	and analyze it. Okay, but we have enough humans that we've got things like optical illusions,
690240	695680	stuff like that, that works on enough humans, and our brains aren't changing enough, right?
696240	702720	A conjecture is, if you had, as you said, open weights, open weights mean safety, because if you
702720	707920	have open weights, you can always reverse engineer adversarial input, and then you can always break
707920	714080	the system. Conjecture. Yeah, there's I, that's again, with Adam from far AI, I'm really interested
714080	719520	to get into that, because they are starting to study, as I understand it, kind of proto scaling
719520	726640	laws for adversarial robustness. And I think a huge question there is, what are the kind of
727680	732800	frontiers of possibility there? Like, do you need, you know, how do the orders of magnitude work,
732800	738480	right? Do you need another 10x as much adversarial training to half the rate of your
739520	744560	adversarial failures? And if so, you know, can we generate that many, it may always sort of
744640	751360	be fleeting. So far AI, and they are, they're working on cutting edge of adversarial input.
751920	757680	Yeah, they're the group that did the attack on the Alpha Go model, and found that like, you know,
757680	761120	and what was really interesting about that, I mean, multiple things, right? First, that they could
761120	765680	beat a super human Go player at all. But second, that the technique that they used would not work
765680	771040	at all if playing a quality human. Or is, you know, it's a strategy that is trivial to beat if
771040	776080	you're a quality human Go player. But the Alpha Go is just totally blind to it.
776080	779440	You know, that's why I say the conjecture is, if you have the model,
780800	786000	then you can generate the adversarial input. And then so if that is true, and that itself is
786000	793360	an important conjecture about AI safety, right? Because if open weights are inherently something
793360	798000	where you can generate adversarial input from that and break or crash or defeat the AI,
798240	805440	then that AI is not omnipotent, right? You have some power words, you can speak to it, almost
805440	812560	like magical words, that'll just make it, power down, so to speak, right? It's like those movies
812560	817680	where the monsters can't see you if you stand really still, or if you, you don't make a noise or
817680	821920	something like that, right? They're very powerful on Dimension X, but they're very weak on Dimension
821920	826560	1. A kind of an obvious point, but you know, I'm not sure how important it's going to be in the
826560	832240	future. Your next question was on like, you know, humanoid robots and so on. And before we get to
832240	838960	that, maybe obviously, but all of these models are trained on things that we can easily record,
838960	848880	which are sights and sounds, right? But touch and taste and smell, we don't have amazing datasets
848880	854800	on those. Well, I mean, there's some haptic stuff, right? There's, there's probably some,
854800	859920	you know, some work on taste and smell and so on. But those, there's five senses, right? I wonder if
859920	866080	there's something like that where you might be like, okay, how are you going to out smell, you know,
866080	870000	a robot or something like that? Well, dogs actually have a very powerful sense of smell,
870000	874240	and that's being very important for them, you know? And it may turn out that there's,
874240	877440	maybe it's just that we just haven't collected the data, and it could become a much better
877440	881200	smeller or whatever, or, you know, taster than anything else. I wouldn't be surprised. It could
881200	886800	be a much better wine taster because you can do molecular diagnostics. But it's just kind of,
886800	890640	I just use that as an analogy to say there's areas of the human experience that we haven't
890640	895760	yet quantified. And maybe it's just the opera term is yet, okay? But there's areas of the
895760	900640	human experience we haven't yet quantified, which are also an area that AIs at least are not yet
900640	909360	capable at. Yeah, I guess maybe my expectation boils down to, I think the really powerful systems
909360	916000	are probably likely to mix architectures in some sort of ensemble, you know, when you think about
916000	920960	just the structure of the brain, it's not, I mean, there certainly are aspects of it that are repeated,
920960	926320	right? You look at the frontal cortex, and it's like, there is kind of this, you know, unit that
926320	930800	gets repeated over and over again, in a sense, that's kind of analogous to say the transformer block
930800	935520	that just gets, you know, stacked layer on layer. But it is striking in a transformer that it's
935520	940960	basically the same exact mechanism at every layer that's doing kind of all the different kinds of
940960	946080	processing. And so whatever weaknesses that structure has, and you know, with the transformer
946080	950560	and the attention mechanism, there's like some pretty profound ones like finite context window,
951120	955840	you know, you kind of need, I would think a different sort of architecture with a little bit
955840	963040	of a different strength and weakness profile to complement that in such a way that, you know,
963040	967600	kind of more similar to like a biological system where you kind of have this like dynamic feedback
967600	971360	where, you know, if we have obviously, you know, thinking fast and slow and all sorts of different
971360	978320	modules in the brain, and they kind of cross regulate each other and don't let any one system,
978320	983200	you know, go totally, you know, down the wrong path on its own, right, without something kind of
983200	987760	coming back and trying to override that. It seems to me like that's a big part of what is missing
987760	994880	from the current crop of AIs in terms of their robustness. And I don't know how long that takes
994880	1001600	to show up. But we are starting to see some, you know, possibly, you know, I think people are maybe
1001600	1004800	thinking about this a little bit the wrong way. They're just in the last couple of weeks, there's
1004800	1011280	been a number of papers that are really looking at the state space model, kind of alternative,
1011280	1015600	it's being framed as an alternative to the transformer. But when I see that, I'm much more
1015600	1021040	like, it's probably a compliment to the transformer or, you know, these two things probably get
1021040	1024960	integrated in some form, because to the degree that they do have very different strengths and
1024960	1029440	weaknesses, ultimately, you're going to want the best of both in a robust system, certainly if you're
1029440	1033120	trying to make an agent, certainly if you're trying to make, you know, a humanoid robot that can go
1033120	1038640	around your house and like do useful work, but also be robust enough that it doesn't, you get tricked
1038640	1043040	into attacking your kid or your dog or, you know, whatever, you're going to want to have more checks
1043040	1048880	and balances than just kind of a single stack of, you know, the same block over and over again.
1048880	1054640	Well, so I know Boston Dynamics with their legged robots is all control theory and it's not
1054640	1059200	classical ML development. It's really interesting to see how they've accomplished it. And they do
1059200	1064720	have essentially a state space model where they have a big position vector that's got all the
1064720	1068880	coordinates of all the joints and then a bunch of matrix algebra to figure out how this thing is
1068880	1073280	moving and all the feedback control and so on there. And it's more complicated than that, but
1073280	1078080	that's, you know, I think the V1 of it. Sorry, it was there. I wasn't following this though.
1078080	1082960	Are you saying that there's papers that are integrating that with the kind of gender to
1082960	1086560	eye transformer model? You know, what's like, what's a good citation for me to look at?
1086560	1092960	Yeah, starting to, we did an episode, for example, with one of the technology leads at Skydio,
1092960	1099840	the, you know, the US is champion drone maker. And they have kind of a similar thing where
1100640	1108880	they have built over, you know, a decade, right, a fully explicit multiple orders of,
1108880	1115360	you know, spanning multiple orders of magnitude control stack. And now over the top of that,
1115360	1119600	they're starting to layer this kind of, you know, it's not exactly generative AI in their case,
1119600	1124480	because they're not like generating content, but it's kind of the high level, you know,
1124480	1130080	can I give the thing verbal instructions, have it go out and kind of understand, okay, like,
1130080	1135840	this is a bridge, I'm supposed to kind of, you know, survey the bridge and translate those high
1135840	1143840	level instructions to a plan, and then use the lower level explicit code that is fully deterministic,
1143840	1148480	and, you know, runs on control theory and all that kind of stuff to actually execute the plan at
1148480	1152960	a low level. But also, you know, at times like surface errors up to the top and say like, hey,
1152960	1156320	we've got a problem, you know, whatever, I'm not able to do it, you know, can you now,
1157200	1162880	at the higher level, the semantic layer, adjust the plan. That stuff is starting to happen in
1162880	1167200	multiple domains, I would say. Yeah. And so I think that makes sense is basically it's like,
1167200	1171520	gender AI is almost the front end. And then you have almost like an assembly, like,
1172080	1178080	you give instructions to Figma, and the objects there are their shapes and their images. And so
1178080	1184000	there's not it's not text, you give instructions to a drone, and the objects are like GPS coordinates
1184000	1191280	and paths and so on. And so you are generating structures that are in a different domain, or
1191280	1196800	it's like in VR, you're generating 3d structures again, as opposed to text. And then that compute
1196800	1200800	engine takes those three structures and does something with them in a much more rules based
1200800	1205680	way. So you have like a statistical user friendly front end with a generative AI, and then you have
1205680	1211760	a more deterministic, or usually totally deterministic, almost like assembly language
1211760	1214960	backend that actually takes that and does that's what you're saying, right? Yeah, pretty much.
1214960	1219120	And I would say there's another analogy to just, again, our biological experience where it's like,
1219680	1225360	I'm, you know, sort of in a semi conscious level, right, I kind of think about what I want to do.
1225360	1230640	But the low level movements of the hand, you know, are both like not conscious. And also,
1230640	1236000	you know, if I do encounter some pain, or you know, hit some, you know, hot item or whatever,
1236000	1241600	like, there's a quick reaction that's sort of mediated by a lower level control system.
1241600	1245440	And then that fires back up to the brain and is like, Hey, you know, we need a new plan here.
1245440	1252640	So that is only starting to come into focus, I think with, you know, because obviously these,
1252640	1258400	I mean, it's amazing, as you said, it's all moving so fast. What is always striking to me,
1258400	1262960	I just, and I kind of like recite timelines to myself almost as like a mantra, right? Like,
1262960	1268480	the first instruction following AI that hit the public was just January 2022. That was
1268480	1273280	OpenAI's Text of NGOO2 was the first one where you could say like, do X and it would do X,
1273280	1276640	as opposed to having, you know, an elaborate prompt engineering type of setup.
1277440	1281520	GPT-4, you know, just a little over a year ago, finished training, not even a year that it's
1281520	1288400	been in the public. And, you know, it has been amazing to see how quickly this kind of technology
1288400	1292080	is being integrated into those systems, but it's definitely still very much a work in progress.
1292640	1299760	Yeah, I mean, the tricky part is, like the training data and so on, like a large existing
1299760	1307360	scale company like a Figma or DJI that has millions or billions of user sessions will have
1307360	1314240	a much easier time training and they have a unique data set. And then everybody else will
1314880	1319680	not be able to do that. So there is actually almost like, I mean, a return on scale where
1319680	1323680	the massive data set, if you've got a massive clean data set and a unique domain that lots of
1323680	1329920	people are using, then you can crush it. And if you don't, I suppose, I mean, there's lots of
1329920	1334000	people who work on zero shot stuff and sort of sort of, but it still strikes me that there'll
1334080	1340400	probably be an advantage to see those sessions. I find it hard to believe that you could generate
1340400	1347280	a really good drone command language without lots of drone flight paths, but you can see.
1347280	1351440	And where it doesn't exist, people are, obviously, need deep pockets for this, but the likes of
1351440	1357280	Google are starting to just grind out the generation of that, right? They've got their kind of
1357840	1363440	test kitchen, which is a literal physical kitchen at Google, where the robots go around and do
1363440	1369360	tasks. And when they get stuck, my understanding of their kind of critical path, as I understand,
1369360	1376400	they understand it, is robots going to get stuck. We'll have a human operator remotely
1377360	1384160	operate the robot to show what to do. And then that data becomes the bridge from what the robot
1384160	1389600	can't do to what it's supposed to learn to do next time. And they're going to need a lot of that,
1389600	1394640	for sure. But they increasingly have, I don't know exactly how many robots they have now, but
1394640	1400480	last I talked to someone there, it was like, into the dozens. And presumably, they're continuing
1400480	1407600	to scale that. I think they just view that they can probably brute force it to the point where
1407600	1412080	it's good enough to put out into the world. And then very much like a Waymo or a cruise or whatever,
1412080	1416160	they probably still have remote operators, even when the robot is in your home,
1416960	1419280	you know, when it encounters something that it doesn't know what to do about,
1420000	1424480	raise that alarm, get the human supervision to help it over the hump, and then, you know,
1424480	1427600	obviously, that's where you really get the scale that you're talking about.
1428160	1433200	This raises a couple of questions I wanted to ask that are conceptual. So, you know, obviously,
1433200	1439120	there's huge questions around like, again, highest level, how is all this going to play out?
1439120	1444960	One big debate is, to what degree does AI favor the incumbents? To what degree, you know,
1444960	1448720	does it enable startups? Obviously, it's both. But, you know, if you're interested in your
1448720	1453120	perspective on that, also really interested in your perspective on like, offense versus defense,
1453120	1457440	that's something that a lot of people now and in the future, right, that seems like it probably
1457440	1462320	really matters a lot, whether it's a more offense enabling or defense enabling technology. So,
1462320	1467440	I love your take on those two dimensions. Hey, we'll continue our interview in a moment after
1467440	1472480	a word from our sponsors. If you're a startup founder or executive running a growing business,
1472480	1476400	you know that as you scale, your systems break down and the cracks start to show.
1476960	1482160	If this resonates with you, there are three numbers you need to know. 36,000, 25, and 1.
1482720	1487440	36,000. That's the number of businesses which have upgraded to NetSuite by Oracle. NetSuite is the
1487440	1491600	number one cloud financial system, streamline accounting, financial management, inventory,
1491600	1497760	HR, and more. 25. NetSuite turns 25 this year. That's 25 years of helping businesses do more
1497760	1503360	with less, close their books in days, not weeks, and drive down costs. One, because your business
1503360	1508080	is one of a kind, so you get a customized solution for all your KPIs in one efficient system with
1508080	1513120	one source of truth. Manage risk, get reliable forecasts, and improve margins, everything you
1513120	1519040	need all in one place. Right now, download NetSuite's popular KPI checklist designed to give you
1519040	1525200	consistently excellent performance, absolutely free at netsuite.com slash zen. That's netsuite.com
1526080	1529840	to get your own KPI checklist, netsuite.com slash zen.
1530800	1534720	So, like offense or defense in the sense of disenabled disruptors or incumbents?
1535360	1540480	Both in business and in potentially outright conflict. I'd be interested to hear your analysis
1540480	1546480	on both. All right. A lot of views on this. Obviously, if you've got a competent existing
1546480	1552560	tech CEO who's still in their prime, like Amjad of Replet, or
1553360	1560640	you know, Dillon Field of Figma, or, you know, those are two who have thought of who are very
1560640	1566320	good and, you know, will be on top of it. Amjad is very early on integrating AI into Replet,
1566320	1569680	and it's basically built that into an AI-first company, which is really impressive.
1570400	1578000	Those are folks who cleanly made a pivot. It's as big or bigger than, comparable to, I would say,
1578000	1584000	the pivot from desktop to mobile that broke a bunch of companies in the late 2000s and early
1584000	1590080	2010s. Like Facebook in 2012 had no mobile revenue, roughly, at the time of their IPO,
1590080	1595200	and then they had to like redo the whole thing. And it's hard to turn a company 90 degrees when
1595200	1599840	something new like that hits, you know? Those that are run by kind of tech CEOs in their prime
1600880	1606160	will adapt and will AI-ify their existing services. And the question is, obviously,
1606160	1609920	there's new things that are coming out, like pika and character.ai. There's some like really
1609920	1616480	good stuff that's out there. The question is, you know, will the disruption be allowed to happen
1616480	1622800	in the US regulatory environment? And so my view is actually that, you know, so this is from like
1622800	1627520	the network state book, right? I talk about, you know, people talk about a multipolar world or
1627520	1632880	unipolar world. The political axis is actually really important in my view for thinking about
1632880	1638240	whether AI will be allowed to disrupt, okay? Because we'll get to this probably later,
1638240	1644000	but the 640K of compute is enough for everyone executive order, you know, 640K of memory,
1644000	1648080	the apocryphal, he didn't bill gates and actually say it, but that quote kind of
1648080	1652560	gives a certain mindset about computing. That should be enough for everybody. So the 10 to the 26
1652560	1657280	of compute should be enough for everyone bill. I actually think it's very bad. And I think it's
1657280	1663680	just the beginning of their attempts to build like a software FDA, okay, to decelerate, control,
1663680	1669440	regulate, red tape, the entire space, just like how, you know, the threat of nuclear terrorism
1669440	1676400	got turned into the TSA. The threat of, you know, terminators and AGI gets turned into a million
1676400	1682160	rules on whether you can set up servers and this last free sector of the economy is strangled or at
1682160	1689360	least controlled within the territory controlled by Washington DC. Now, why does this relate
1689360	1694880	to the political? Well, obviously this, you know, you can just spend your entire life just tracking
1694880	1699840	AI papers and that's moving like at the speed of light like this, right? What's also happening
1699840	1704720	as you can kind of see in your peripheral vision is there's political developments that are happening
1704720	1708240	at the speed of light much faster than they've happened in our lifespans. Like there's more,
1708240	1712880	you just notice more wars, more serious online conflicts like, you know, there's a sovereign
1712880	1717920	debt crisis, all of those things that can show graph after graph of things looking like their own
1717920	1722800	types of singularities, you know, like military debts are way up, you know, the long piece that
1722800	1726800	Stephen Pinker showed it's looking like a you that suddenly way up after Ukraine and some of
1726800	1731360	these other wars are happening, unfortunately, right? Interest payments rush way up to the side.
1731360	1737840	What's my point? Point is, I think that the world is going to become from the Pax Americana world
1738240	1744000	of just like basically one superpower, hyperpower that we grew up in from 91 to 2021 roughly,
1744640	1751360	that we're going to get a specifically tripolar world, not unipolar, not bipolar, not multipolar,
1751360	1757600	but tripolar. And those three poles, I kind of think of as NYT, CCP, BTC, or you could think of
1757600	1761920	them as, and those are just certain labels that are associated with them, but they're roughly
1762640	1768640	US tech, the US environment, China tech and China environment, and global tech and the global
1768640	1774560	environment. And why do I identify BTC and crypto and so on with global tech? Because that's a tech
1774560	1780240	that decentralized out of the US. And right now people think of crypto as finance, but it's also
1780240	1788560	financiers. Okay, and in this next run up, it is, I think quite likely about depending on how you
1788560	1794240	count, between a third to a half of the world's billionaires will be crypto. Okay, around, you
1794240	1798080	know, I calculated this a while back around Bitcoin at a few hundred thousands, around a third to a
1798080	1803120	half of the world's billionaires are crypto. That's the unlocked pool of capital. And those are the
1803120	1810640	people who do not bow to DC or Beijing. And they might by the way be Indians or Israelis or every
1810640	1814640	other demographic in the world, or they could be American libertarians, or they could be Chinese
1814640	1819280	liberals like Jack Ma were pushed out of Beijing sphere. Okay, or the next Jack Ma, you know,
1819280	1825440	Jack Ma himself may not be able to do too much. Okay, that group of people who are, let's say,
1825440	1830720	the dissident technologists who are not going to just kneel to anything that comes out of
1830720	1837520	Washington DC or Beijing, that is the that's decentralized AI. That's crypto. That's decentralized
1837520	1841440	social media. So you can think of it as, you know, where we talked about in the recent pirate
1841440	1846880	wires podcast, freedom to speak with decentralized censorship resistant social media, freedom to
1846880	1853840	transact with cryptocurrency, freedom to compute with open source AI, and no compute limits. Okay,
1854480	1859920	that's a freedom movement. And that's like the same spirit as a pirate bay, the same spirit
1859920	1865920	as BitTorrent, the same spirit as Bitcoin, the same spirit as peer to peer and into an encryption.
1865920	1871280	That's a very different spirit than having Kamala Harris regulate a superintelligence
1871280	1878560	or signing it over to Xi Jinping thought. And the reason I say this is, I think that that group
1878560	1884400	of people, of which I think Indians and Israelis will be a very prominent, maybe a plurality, right,
1884400	1888160	just because the sheer quantity of Indians are like the third sort of big group that's kind of
1888160	1892960	coming up. And they're relatively underpriced, you know, China is, I don't say it's price
1892960	1898400	to perfection. But it's something that people when I say priced, I mean, people were dismissive
1898480	1904400	of China even up until 2019. And then it was after 2020, if you look at people started to take China
1904400	1910080	seriously. And I mean, that is the West Coast tech people knew that China actually had a plus tech
1910080	1913760	companies and was a very strong competitor. But the East Coast still thought of them as a third
1913760	1919200	world country until after COVID, when now, you know, the East Coast was sort of threatened by them
1919200	1924800	politically. And it wasn't just blue collars, but blue America that was threatened by China.
1925440	1929920	And so that's why the reaction to China went from Oh, who cares, just taking some manufacturing
1929920	1934800	jobs to this is an empire that can contend with us for control of the world. That's why the hostility
1934800	1937760	is ramped up in my view. There's a lot of other dimensions to it. But that's a big part of it.
1938960	1943120	So India is also kind of there, but it's like the third. And India is not going to play for number
1943120	1948960	one or number two. But India and Israel, if you look at like tech founders, depending on how you
1948960	1954000	count, especially if you include diasporas, it's on the order of 30 to 50% of tech founders, right.
1954000	1958320	And it's obviously some, you know, very good tech CEOs and, you know, Satya and Sundar and investors
1958320	1964240	and whatnot. Those are folks Indians do not want about to DC or to Beijing, neither do Israelis
1964240	1968720	for all kinds of reasons, even if Israel has to, you know, take some direction from the US now,
1968720	1973360	they're bristling at it, right. And then a bunch of other countries don't. So the question is,
1973920	1979600	who breaks away? And now we get to your point on the reason I had to say that is that that's
1979600	1986160	preface, the political environment is a tripolar thing of US tech and US regulated Chinese tech
1986160	1991680	and China regulated and global tech that's free. Okay, of course, there's, even though I identify
1991680	1997040	those three polls, there's of course boundary regions. EAC is actually on the boundary of US
1997040	2001760	tech and decentralized tech, you know, and I'm sure there'll be some Chinese thing that comes out
2001760	2006080	that is also on the boundary there. For example, Binance is on the boundary of Chinese tech
2006080	2010320	and global and decentralized tech, if that makes any sense, right? There's probably others Apple is
2010320	2013920	actually on the boundary of US tech and Chinese tech, because they make all of their stuff in
2013920	2018960	China, right? So these are not totally disjoint groups, but there's boundary areas, but you can
2018960	2024640	think about why is this third group so important in my view? Both the Chinese group and the
2024640	2030320	decentralized group will be very strong competition for the American group for totally different reasons.
2031280	2036720	China has things like WeChat, these super apps. I mean, obviously not likely, but like
2037440	2042800	WeChat is a super app, but they also have, for example, their digital yuan, right? They have
2042800	2047920	the largest cleanest data sets in the world that are constantly updated in real time that they can
2047920	2055920	mandate their entire population opt into. And most of the Chinese language speaking people are under
2055920	2061680	their ambit, right? So that doesn't include Taiwan, doesn't include Singapore, doesn't include,
2062480	2066320	you know, some of the Chinese yaspera, but basically anything that's happening in Chinese
2066320	2072400	for 99% of it, 95, whatever the ratio is, they can see it and they can coerce it and they can control
2072400	2079840	it. So they can tell all of their people, okay, here's five bucks in, you know, digital yuan,
2079840	2085360	do this micro task, okay? All of these digital blue collar jobs, both China and India, I think,
2085360	2088720	can do quite a lot with that and they'll come back to it. So they can make their people do
2088720	2093120	immense amounts of training data, clean up lots of data sets. Once it's clear that you have to
2093120	2098000	build this and do this, they can just kind of execute on that. And they can also deploy. I mean,
2098000	2102720	in many ways, the US is still very strong in digital technology, but in the physical world,
2102720	2107840	it's terrible because of all the regulations, because of the nimbyism and so on. It's not like
2107840	2112720	that in China. So anything which kind of works in the US at a physical level, like the Boston
2112800	2116720	Dynamics stuff, they're already cloning it in China and they can scale it out in the physical
2116720	2121680	world. You already have drones, little sidewalk drone things that come to your hotel room and
2121680	2126160	drop things off. That's already like very common in China. In many ways, it's already ahead if you
2126160	2131040	go to the Chinese cities. So the Chinese version of AI is ultra centralized, more centralized,
2131040	2136000	more monitoring, less privacy and so on than the American version. And therefore they will have
2136000	2141040	potentially better data sets, at least for the Chinese population. And so we chat AI, I don't
2141040	2144880	even know what it's going to be, but it'll be probably really good. It'll also be really dangerous
2144880	2151520	in other words. Then the decentralized sphere has power for a different reason, because the
2151520	2160080	decentralized sphere can train on full Hollywood movies. It can train on all books, all mp3s and
2160080	2166560	just say, screw all this copyright stuff, like what Psyhub and Libgen are doing, because all the
2166560	2174080	copyright, first of all, it's like Disney lobbying politicians to put another 60 or 70 or 90, I
2174080	2177920	don't even know what it is, some crazy amount on copyright, so you can keep milking this stuff and
2177920	2181440	it doesn't go into public domain number one. And second, you know how Hollywood was built in the
2181440	2186320	first place? It was all patent copyright and IP violation. Essentially Edison had all the patents,
2186320	2192480	he's in New Jersey-ish, okay, that East Coast area. And Neil Gabler has this great book called
2192480	2198080	An Empire of Their Own, where he talks about how immigrant populations, you know, the Jewish
2198080	2203280	community in particular, and also others, went to Southern California in part, so they could just
2203280	2207120	make movies that Edison coming and suing them for all the patents and so on and so forth.
2207120	2210880	And they made enough money that they could fight those battles in court, and that's how they built
2210880	2216960	Hollywood, okay? So, you know, one of my big theses is history is running in reverse, and I can get
2216960	2221520	to why, but it's like 1950s and mirror moment, you go more decentralized backwards and forwards
2221520	2227360	in time is like, you have these huge centralized states like the US and USSR and China, you know,
2227360	2232480	all these things exist, and their fist relaxes as you go forwards and backwards in time. For example,
2232480	2237680	backwards in time, the Western frontier closed, and forwards in time, the Eastern frontier opens.
2237680	2241040	Backwards in time, you have the robber barons, forwards in time, you have the tech billionaires.
2241040	2244720	Backwards in time, you have Spanish flu, forwards in time, you have COVID-19. And I've got dozens
2244720	2250160	of examples of this in the book. The point is that if you go backwards in time, the ability to
2250160	2254720	enforce patents and copyrights and so on starts dropping off, right? You have much more of a
2254720	2260320	Grand Theft Auto environment. And you go forwards in time, and that's happening again. So, India
2260320	2267360	in particular, for many years, basically just didn't obey Western patent protections and all these
2267360	2272480	stupid rules basically, you know, it's a combination of artificial scarcity on the patent side and
2272480	2276960	artificial regulation on the FDI side. That's a big part of what jacks up drug costs, where these
2276960	2282000	things cost, you know, only cents to manufacturing, they sell them for so much money. All the delays,
2282000	2286000	of course, that are imposed on the process, the only way they can pay for the manufacturers to
2286000	2289920	take it out of your hide. What India did is they just said, we're not going to obey any of that stuff.
2290560	2296080	So, they have a whole massive generic drugs and biotech industry that arose because they built
2296080	2299600	all the skills for that. That's why they could do their own vaccine during COVID. And they're one
2299600	2305120	of the biggest biotech industries in the world because they said screw Western restrictive
2305200	2310400	IPs and other stuff. So, I was actually talking with the founder of Flipkart, that's India's largest
2310400	2315840	exit. And we were talking about this a few months ago. And what we want is for India and other
2315840	2321040	countries like it, do something similar, not just generic drugs, but generic AI, meaning
2322080	2328720	just let people train on Hollywood movies, let them train on full songs, let them train on every
2328720	2335680	book, let them train on anything. And you know what, sue them in India, right? And have the servers
2335680	2341520	in India and let people also train models in India, because that's something that can build up a
2341520	2346320	domestic industry with skills that the rest of the world, you know, people will want the model
2346320	2350000	output, they'll want to use the software service there, and they'll be fighting in court on the
2350000	2355920	back. And this is similar to how all of the record companies fought Napster and Kazaa and so on,
2355920	2358720	but they couldn't take down Spotify. Do you know that story? Do you remember that?
2358720	2364160	Basically, because Spotify was legitimately, you know, a European company and that a combination of
2364160	2369680	execution and, you know, negotiation, they couldn't take them down. They did take down Napster,
2369680	2374640	they took down Limewire, they took down Groove Shark, and Kazaa had Estonians, I don't know
2374640	2377840	exactly how it was incorporated, but it was probably two US proximal, and that's why they
2377840	2382320	were able to get them. But Spotify was far enough away that they couldn't just sue them and they
2382320	2387600	actually genuinely had European traction. That's why the RA had to negotiate. So being far away
2388400	2394240	from San Francisco may also be an advantage in AI, because it means you're far away from the blue
2394240	2398160	city in the blue state in the Union. This relates to another really important point.
2399040	2402800	When you actually think about deploying AI, there's those jobs you can disrupt that are not
2402800	2409520	regulated jobs, like, you know, obviously, programmers are not, thank God, you don't need
2409600	2414080	a license to be a programmer, but programmers adopt this kind of stuff naturally, right? So
2414080	2418880	get up, co-pilot, replete, we just boom, use it, and now it's amplified intelligence, okay?
2419680	2424240	But a lot of other jobs, there's some that are unionized and then some that are licensed, right?
2424240	2428240	So Hollywood screenwriters are complaining, right? Journalists are complaining,
2428240	2433840	artists are complaining. This is a good chunk of blue America. If you add in licensed jobs,
2433840	2440080	like lawyers and doctors and bureaucrats, right? You know, especially lawyers and doctors are very
2440080	2445280	politically powerful, MDs and JDs. They have strong lobbying organizations, AMA and, you know,
2445280	2452160	ABA and so on. Basically, AI is part of the economic apocalypse for blue America,
2453200	2459920	okay? It just attacks these overpriced jobs. They say overpriced relative to
2460720	2464720	what an Indian could do with an Android phone, what a South American could do with an Android phone,
2464720	2467920	what someone in the Middle East or the Midwest could do with an Android phone.
2468640	2475600	Now, those folks have, you know, been armed with generative AI. They can do way more.
2476240	2480000	They're ready to work, they're ready to work for much less money, and they're a massive threat
2480800	2486400	to blue America. Blue America is now feeling like the blue collars of 10 or 20 years ago, where
2487360	2491520	the blue collars had their jobs, you know, going to China and other places, right? And they were mad
2491520	2495680	about that. Factories got shut down and so on. That's about to happen to blue America, already
2495680	2502480	happening, okay? And so that's going to mean a political backlash by blue America of protectionism,
2502480	2508800	again, already happening. And the AI safety stuff, that's a whole separate thing, but it's going to
2508800	2513600	be used. I'm going to use a phrase, and I hope you won't be offended by this. Have you heard
2513600	2519200	the phrase, useful idiots, like by Lenin or whatever, okay? It basically means like, okay,
2519200	2524240	those guys, you know, they're useful idiots for communism and so on. So there's, let me put it
2524240	2531920	like naive people who think that the US government is interested in AI safety, are trying to give a
2531920	2535760	lot of power to the US government. And the reason is they haven't actually thought through from
2535760	2538800	first principles, what is the most powerful action in the world, a convective. They're trying to get
2538800	2542640	power to the US government to regulate AI safety. But the government doesn't care about safety of
2542640	2549920	anything. They literally funded the COVID virus in Wuhan, credibly alleged, right? There's at
2549920	2555120	least it is a reasonable hypothesis based on a lot of the data. Matt Ridley wrote a whole book on
2555120	2559040	this. There's a lot of data that indicates a lot of scientists believe it. I'm actually like a
2559040	2563760	bioinformatics genomics guy. If you look at the sequences, there is a gap and a jump where it looks
2563760	2568320	like this thing could have been engineered or partially engineered or evolved. There's Peter
2568400	2573360	Dazak. There's Zeng Lishi. There's actually a lot of evidence here. So the US government and the
2573360	2578400	Chinese government are responsible for an existential risk by studying it, they created it.
2579280	2584560	They're responsible for risking nuclear war with Russia over this piece of land in eastern Ukraine,
2584560	2591040	which probably is going to get wound down. So they don't care about your safety at all.
2592480	2596160	These are immediate things where we can show and there's nobody who's punished for this,
2596160	2602720	nobody who's fired for this, literally rolling the dice on millions, hundreds of millions of
2602720	2609200	people's lives has not been punished. In fact, it's not even talked about. We're past the pandemic
2609200	2615760	and these institutions can't be punished. So they don't care about AI safety. What they care about
2615760	2622000	is AI control. And so the people in tech who are like, well, the government will guarantee AI
2622000	2626720	safety. That's actually what we're going to actually get is something on the current path,
2626720	2632080	like what happened with nuclear technology, where you got nuclear weapons, but not nuclear power,
2632080	2635520	or at least not to the scale that we could have had it, right? We could have had much cheaper
2635520	2640320	energy for everything. Instead, we got the militarization and the regulation and the
2640320	2646400	deceleration worst of all worlds where you can blow people up, but you can't build nuclear power
2646400	2651920	plants. And like even getting into nuclear technology, forget about nuclear power plants.
2651920	2655520	We don't have nuclear submarines. We don't have nuclear planes, all that kind of stuff. I don't
2655520	2658640	have nuclear planes are possible, but I do know nuclear submarines are possible. You can do a lot
2658640	2663200	more cruise ships, a lot more stuff like that. You could probably have nuclear trains. You have
2663200	2667600	to look at exactly how big those are. I don't know exactly how big those engines are and what
2667600	2671760	the spies, but I wouldn't be surprised if you could. We don't have that. Why don't we have that?
2671760	2676000	Because we had the wrong fear-driven regulation in the early 70s.
2676800	2683520	Putting it all together, I think that the current AI safety stuff is similar to nuclear safety stuff
2684560	2688960	that the US government has a terrible track record on safety in general. It doesn't care about it.
2688960	2695280	It funded the COVID virus, incredibly alleged. It definitely risked nuclear war with Russia recently.
2695280	2699280	Hot war with Russia was the red line we were not supposed to cross, and we're now like way
2699280	2704720	into that. It doesn't care about AI safety, it doesn't care about your safety. It's also not
2704720	2710480	even good at regulating. What it cares about is control. We are going to have potentially a bad
2710480	2717440	outcome where Silicon Valley and San Francisco is the Xerox Park of AI. Maybe that's too strong,
2717440	2723520	okay? Basically, it develops it, and there's a lot of things it can't do because it lobbied for
2723520	2728160	this regulation that is going to come back and choke it. Then there are other two spheres
2728240	2733040	we'll push ahead because it's not about the technology, it's also about the political layer.
2733040	2737280	You know the Steve Jobs saying, actually Alan Kay by way of Steve Jobs, if you're
2737920	2742800	really serious about software, you need your own hardware, right? So if you're really serious
2742800	2749200	about technology, you need your own sovereignty. Because what the AI people haven't thought about
2749200	2756800	is there's a platform beneath you, which is not just compute, it is regulate. It's a law, okay?
2756800	2760480	And it's a law doesn't allow you to compute so much for all of your stuff above that.
2761280	2766080	And I know you're saying, oh, it's only a 10 to 26 compute ban and so on and so forth.
2766080	2773440	Have you seen the first IRS tax form? It's always, always super simple. It's only the super, super,
2773440	2777760	super rich who's we're going to get in at first doesn't matter to you. So that's called
2778400	2782400	boiling the frog slowly. There's a million, you know, slippery slope, slippery slope isn't a fallacy.
2782400	2787120	It's literally how things work, right? You know, Apple, one of the reasons they, you know,
2787120	2792080	they talk about not setting a precedent. Zuck starts off is a very hard line on setting precedents
2792080	2797040	because he understands the long-term equivalent of setting a precedent, right? The precedent setting
2797040	2801840	is that they're setting up a software FDA and they're going to and DC is so energized on this
2801840	2806320	because they know how much social media disrupted them. That's why they're on the attack on crypto
2806320	2810640	and AI. That's why they're on the attack on self-driving cars. They want to freeze the current
2810640	2816160	social order and amber domestically and globally. So they think they can sanction China and stop it
2816160	2821120	from developing chips. They think they can impose regulations on the US and stop it from developing
2821120	2827120	AI, but they can't. And also, by the way, they're totally schizophrenic on this, where when they're
2827120	2831360	talking about China, they're like, we're going to stop their chips to make sure America is a global
2831360	2836240	leader. This is Gina Raimondo who's English. And then domestically, they're like, we're going to
2836240	2841040	regulate you so you stop accelerating AI. We're not about AI acceleration. EAC is weird over there.
2841040	2845840	Okay. So think about how schizophrenic that is. Okay, you're going to be far ahead of China.
2845840	2850720	We're also going to be make sure to control the US. So they want to try and slow what they actually
2850720	2855760	want is to freeze the current system and amber, try to go back to pre 2007 before all these tech
2855760	2860720	guys disrupted everything. But that's not what's going to happen. So, but they're going to try to
2860720	2865760	do it. And so everybody who's still loyal to the DC sphere, which includes an enormous chunk
2865760	2872720	of AI people. And because they're all in a lot of them in San Francisco, right? And the political
2872720	2881200	chaos of the last few years was not sufficient for them to relocate yet. Not all of them. I mean,
2881200	2886320	Elon is in Texas. And that it may turn out that grok, for example, and what they're doing there,
2886320	2890400	because he's a very legit, I mean, you know, he's Elon. So he's capable of doing a lot. He's very
2890400	2894800	early on opening AI, he understands, you know, the right, it may turn out that grok
2895600	2901200	becomes red AI, or the community around that, you know, an opening eye and deep mind or still blue
2901200	2904320	AI. And we have Chinese and we're going to have decentralized AI. Okay, let me pause there. I
2904320	2910560	know there's a big download. Well, I for starters, I would say, broadly, I have a pretty similar
2912080	2916640	intellectual, you know, tendency as you, I would broadly describe myself as a techno
2916720	2925040	optimist, libertarian, just about every issue. And I think your analysis of the dynamics is
2925040	2928640	super interesting. And I think it, you know, a lot of it sounds pretty plausible, although I'll
2928640	2933520	kind of float a couple of things that I think maybe bucking the trend. But I think it's maybe
2933520	2939600	useful to kind of try to separate this into scenarios. Because all the analysis that you're
2939600	2947120	describing here seem, if I understand it correctly, it seems to have the implicit assumption
2947840	2955360	that the AI itself is not going to get super powerful or hard to control. It's like, if we
2955360	2961040	assume that it's kind of a normal technology, then you're off to the races on this analysis. And
2961040	2966080	then we can get into the fine points. But I do want to take at least one moment and say,
2966800	2972800	how confident are you on that? Because if it's a totally different kind of technology from other
2972800	2979120	technologies that we've seen, if it's more, you raise the gain of function research example,
2979120	2986400	if it's that sort of technology that has these sort of non-local possible impacts or
2987600	2994000	self-reinforcing kind of dynamics, which need not be like an Eliezer style snap of the fingers fume,
2994000	3000400	but even over, say, a decade, let's imagine that over the next 10 years that AI's kind of
3001120	3004480	multiple architectures develop and they sort of get integrated and we have something that
3004480	3009920	kind of looks like robust, silicon-based intelligence, maybe not totally robust, but as
3009920	3016720	robust or more robust than us and running faster and the kind of thing that can do lots of full
3016720	3023440	jobs or maybe even be tech CEOs, then it kind of feels like a lot of this analysis probably
3023440	3030800	doesn't hold, because we're just in a totally different regime that is just extremely hard
3030800	3036640	to predict. And I guess I wonder, first of all, do you agree with that? There seems to be a big
3036640	3042720	fork in the road there that's like, just how fast and how powerful do these AI's become super
3042720	3046560	powerful or do they not? And if they don't, then yeah, I think we're much more into real
3046560	3051200	politic type of analysis, but I'm not at all confident in that. To me, it feels like there's
3051200	3058080	a very real chance that AI of 10 years from now is, and by the way, this is what the leaders
3058080	3064160	are saying, right? I mean, open AI is saying this, Anthropic is saying this, Demis and Shane
3064160	3070480	Legge are certainly saying things like this. It seems like they expect that we will have AI's
3070480	3076880	that are more powerful than any individual human and that that becomes the bigger question
3077760	3084880	than anything else. So do you agree with that kind of division of scenarios? First of all,
3084880	3088640	and then maybe you could kind of say like how likely you think each one is. And obviously,
3088640	3093120	that one where it takes off is like super hard to analyze. And I also definitely think it is
3093120	3097200	worth analyzing this scenario where it doesn't take off. But I just wanted to flag that it seems
3097200	3103200	like there's a, you know, there's a big, if you talk to the AI safety people, any world in which
3103280	3110560	it's like, you know, we're suing Indian AI firms in Indian court over like IP is like a normal world
3110560	3113360	in their mind, right? And that's not the kind of world that they're most worried about.
3113920	3119520	I think that there have been some plausible sounding things that have been said. But I want
3119520	3125360	to just kind of talk about a few technical counter arguments, mathematical or physical,
3125360	3131120	that constrain what is possible. Okay. And actually, Martin Casado and Vijay and I are
3131200	3136000	working on a long thing on this where, you know, Vijay did folding at home, he's a physicist,
3136000	3140800	Martin, sold in the Syrah for, you know, a billion dollars and knows a lot about how a
3140800	3145600	Stuxnet like thing could work at the systems level. And I've thought about it from other angles and,
3145600	3152080	you know, and some of the math stuff that I'll get to. So for example, one thing, and I'm going to
3152080	3155840	give a bunch of different technical arguments, and then let's kind of combine them. Okay.
3156640	3160720	One thing that's being talked about is, if you have a super intelligence, it can
3160720	3165840	double it right for a million years, and then it can make one move and it's going to outthink you
3165840	3170640	all the time and so on and so forth. Okay. Well, if you're familiar with the math of chaos,
3171280	3177440	or the math of turbulence, there are limits to even very simple systems that you can set up,
3177440	3184480	where they can become very unpredictable quite quickly. Okay. And so you can, if you want to,
3184560	3191360	engineer a system where you have very rapid diversions of predictability, so that, I don't know,
3191360	3195200	it's like the heat depth of the universe before you can predict out in timestamps.
3196160	3201280	Do you understand what I'm saying? Right? This is sort of akin to like a wolf from like simple,
3201280	3206480	even simple rules can generate patterns such that you can't know them without literally computing
3206480	3212320	them. Yeah, exactly. Right. So at least right now with chaos and turbulence, you can get things
3212320	3220240	that are extremely provably difficult to forecast without actually doing it. Okay. You know, I can
3220240	3223840	make that argument quantitative, but that's just something to look at, right? It's almost like a
3223840	3228160	delta epsilon challenge from calculus, like, okay, how hard do you want me to make this to predict?
3228160	3232560	Okay, I can set up a problem that is, that is like that, right? It's basically extreme sensitivity
3232560	3238240	to initial conditions lead to extreme divergence in outcomes. So you could design systems to be
3238240	3242960	chaotic, that might be AI immune, because they can't be forecasted that well, you have to kind of
3242960	3246880	react to them in real time. The ultimate version of this is not even a chaotic system, it's a
3246880	3252960	cryptographic system, where I've got a whole slide deck on this, how AI makes everything fake,
3252960	3259920	easy to fake, crypto makes it hard to fake again. Right? Because crypto in the broader sense of
3259920	3265680	cryptography, but also in the narrow sense, I think crypto is to cryptography as the internet
3265680	3270160	is to computer science. It's like the primary place where all this stuff is applied, but obviously
3270160	3274800	it's not the equivalent. Okay. And AI can fake an image, but it can't fake a digital signature,
3274800	3280080	unless it can break certain math, you know, and so sort of like a, you know, solve factors,
3280080	3283440	each problem or something like that. So cryptography is another mathematical thing that
3283440	3290960	constrains AI, similar to chaos and turbulence, it constrains how much an AI can infer things.
3290960	3295760	You can't statistically infer it. Okay, you need to actually have the private key to solve that
3295760	3301680	equation. So that is another math. So I'm going to rules of math, right? Math is very powerful
3301680	3306560	because you can make proofs that will work no matter what devices we come up with. Okay,
3306560	3310640	you start to put an AI in a cage, it can't predict beyond a certain amount because of chaos and
3310640	3316960	turbulence math, it cannot solve certain equations unless it has a private key is because of what
3316960	3322240	we know about cryptography math. Okay, again, if somebody proves P equals NP, some of this stuff
3322240	3326560	breaks down, but this is when the bounds of our mathematical knowledge right now, physics wise,
3327440	3337360	physical friction exists, a lot of physical friction exists. And a huge amount of the writing on AI
3337360	3341760	assumes by guys like a laser who I like, I don't, I don't dislike it, you know, but
3342000	3348160	it is extremely, it's there's two things that really stick out to me about it. First is extremely
3348160	3353200	theoretical and not empirical. And second, extremely Abrahamic rather than Dharmic or
3353200	3361040	signing. Okay, white theoretical and not empirical. It's not trivial to turn something from the
3361040	3368080	computer into a real world thing. Okay, one of the biggest gaps in all of this thinking is what
3368800	3374720	are the sensors and actuators? Okay, because like if you actually build, you know, I've built
3375520	3379840	industrial robot systems that you know, 10 years ago, I, you know, a genome sequencing lab with
3379840	3388480	robots, that's hard. That's physical friction. Okay, and a lot of the AI scenarios seem to basically
3388480	3394560	say, Oh, it's going to be a self programming Stuxnet that's going to escape and live off the land
3394560	3400400	and hypnotize people into doing things. Okay, now, each of those is actually really,
3400400	3406720	really difficult steps. First is self programming Stuxnet, like, this would have to be a computer
3406720	3413360	virus that can live on any device, despite the fact that Apple or Google can push a software
3413360	3420000	update to a billion devices, right, a few executives coordinating almost certainly can I mean, the
3420000	3426720	off switch exists, right? Like, this is actually like the core thing, lots of AI safety guys get
3426720	3431520	themselves into the mindset that the off switch doesn't exist. But guess what, there's almost
3431520	3437520	nothing living that we haven't been able to kill. Right, like, can we kill it? This thing exists.
3437520	3442080	And this is getting back to living off land. A even if you had like something that could solve
3442080	3447600	some other technical problems that I'll get to it exists as an electromagnetic wave kind of thing
3447600	3452720	on on a certain, you know, on chips and so on and so forth. It's taking it out in the environment
3452720	3457600	is like putting a really smart human into outer space. Right, your body just explodes and you die.
3458160	3463040	Doesn't matter how smart you are, that that strength on this axis, but you're weak on this
3463040	3468160	axis. And, you know, so strength on the x axis, not strength on the y or the z axis in AI outside,
3468160	3474000	you know, pour water on it. You know, this is why I mean the 50 IQ, 150 IQ thing, you know,
3474000	3478320	the 150 IQ way of saying it is it's strong on this x and weak on this x and the 50 IQ way is
3478320	3485200	pour water on it, disconnect it, you know, turn the power off. Okay, right. Like, it'll, it'll be
3485200	3490080	very difficult to build a system where you literally cannot turn it off. The closest thing we have
3490080	3496880	to that is actually not stuck snap. It's Bitcoin. And Bitcoin only exists because millions of humans
3497520	3502960	keep it going. So you, you need, so that gets the second point living off the land
3503600	3509760	for an AI to live off land, meaning without human cooperation. Okay, that's the next
3509760	3515520	Turing threshold in AI to live without human cooperation. It would need to be able to control
3515520	3523360	robots sufficient to dig or out of the ground, set up data centers and generators and connect them
3523360	3528640	and defend that against human attack, literally a terminator scenario. Okay, that's a big leap
3528640	3532320	in terms. I mean, is it completely impossible? I can't say it's completely impossible,
3532320	3537120	but it's not happening tomorrow. No matter what your AI timelines are, you would need to have
3537120	3544320	like a billion or hundreds of millions of internet connected autonomous robots that this
3544320	3551040	Stuxnet AI could hijack that were sufficient to carve or out of the earth and, you know, set up
3551040	3556640	data centers and make the AI duplicate. We're not there. That's a huge amount of physical friction.
3556640	3560880	That's AI operating without a human to make itself propagate, right? A human doesn't need
3560880	3567040	the cooperation of a lizard to self replicate. For an AI to replicate right now, it would need
3567040	3572880	the cooperation of a human in some sense, because otherwise those humans can kill it because there's
3572880	3576960	not that many different pieces of, you know, operating systems around the world. I'm just
3576960	3580480	talking about the practical constraints of our current world, right? You know, actually existing
3580480	3585600	reality, not AI safety guys, you know, you know, reality where all these things don't exist. There's
3585600	3590160	just a few operating systems, just a few countries. If everybody is going with torches and search
3590160	3597040	lights through the internet, it's very hard for a virus to continue. Okay. So A, on the practical
3597040	3602320	difficulties that there's the technical stuff with, you know, with the chaos and turbulence and
3602320	3607680	with cryptography itself or AI can't predict and it can't solve certain equations. B, on the physical
3607680	3613840	difficulties, it probably, I mean, like to be a Stuxnet, Microsoft and Google and so on could kill
3613840	3618480	it. The off switch exists. Can it live off the land? No, it cannot because it doesn't have, you
3618480	3627440	know, drones to mine or and stuff out of the ground. And can it like exist without humans? Can it be
3627440	3631760	this hypnotizing thing? Okay. So the hypnotizing thing, by the way, this is one of the things that's
3631840	3637840	the most hilarious self fulfilling prophecy in my view. Okay. And no offense anybody listening to
3637840	3644000	this podcast, but I think the absolutely dumbest kind of tweet that I've seen on AI is, I typed this
3644000	3650560	in and oh my God, it told me this. Like, I asked it how to make sarin gas and it told me X or whatever.
3650560	3657040	Right. That's just a search engine. Okay. What, what basically a lot of these people are doing is
3657040	3662800	they're saying, what if there were people out there that were so impressionable that they would
3662800	3668160	type things into an AI and, and follow it as if they were hearing voices. And that's actually
3668160	3673600	not the, the, the model or whatever that's doing it. That's like this AI cult that has evolved
3673600	3677840	around the world, like a Aum Shinrikyo, you know, that, that hears voices and does like the sarin
3677840	3684480	gas. The point is an AI can't just like hypnotize people. Those people have to like participate
3684480	3689360	in it. They're typing things into the machine or whatever. Okay. Now you might say, all right,
3689360	3694400	let's project out a few years. In a few years, what you have is, you have an AI that is not
3694400	3700320	just text, but it appears as Jesus. What would, what would AI Jesus do? What would AI Lee Kwan
3700320	3705200	you do? What would AI George Washington do? So it appears as 3d. Okay. So it's generating that.
3705760	3712960	It speaks in your language and in a voice. It knows the history of your whole culture. Okay.
3712960	3717600	That would be very convincing. Absolutely be very convincing. But it still can't exist without
3717600	3723120	human programmers who are like the priests tending this AI God, whether it's AI Jesus or AI Lee
3723120	3726800	Kwan you or something like that. The thing about the hypnotization thing that I really want to
3726800	3731280	poke on that, are you familiar with the concept of the principal Asian problem? Basically in every,
3731280	3738960	every time you've got like a CEO and a, and a, a worker, or you have a LP and a VC, or you have,
3739920	3745680	you know, an employer and a contractor, every edge there, there are four possibilities in a
3745680	3754560	two by two matrix. Win, win, win, lose, lose, win, lose, lose. Okay. And so for example, win, win is,
3754560	3758720	you know, when, when somebody joins a tech startup, the, the CEO makes a lot of money and so does a
3758720	3764560	worker. Okay. That's win, win, lose, lose is they both lose money. Win, lose is the CEO makes money
3764560	3770000	and the employee doesn't lose. Win is the company fails, but the employee got paid a very high salary.
3770000	3774640	So what equity does is it aligns people. That's where the top console alignment comes from.
3774640	3779040	It aligns people to the upper left corner of win, win. That's when you have one, one CEO and one
3779040	3785200	employee. When you have one CEO and two employees, you don't have two squared outcomes. You have two
3785200	3790560	cubed outcomes because you have win, win, win, win, win, lose, win, lose, lose, etc. Right.
3790640	3795120	Because all three people can be win or lose. Because CEO can be winner, lose. Employee can
3795120	3799280	be winner, lose. Employee number two can be winner, lose. If you have N people, rather than three
3799280	3803040	people, you have two to the N possible outcomes and you have essentially a two by two by two by two
3803040	3807440	by two by N hypercube of possibilities. Okay. It's all literally just two dimensions on each axis.
3808400	3812800	There's tons of possible defecting kinds of things that happen there. So that's why in a large company,
3812800	3817920	there's lose, win coalitions that happen where M people gang up on the other K people and they win
3817920	3821520	what the other people lose. That's how politics happens. When you've got a startup that's driven
3821520	3825840	by equity and the biggest payoff, people don't have to try to think, okay, well, I make more money
3825840	3829440	by politics, we'll make more money by the win, win, win, win, win column because the exit makes
3829440	3833440	everybody make the most money. That's actually how the opening AI people were able to coordinate
3833440	3837920	around. We want an $80 billion company. The economics helped find the sell that was actually
3837920	3841280	the most beneficial to all of them helped them coordinate. Okay. So you search that hypercube.
3841280	3848560	Okay. That's a point of equity is lining. Still, despite all of this, that that's one of our best
3848560	3853360	mechanisms for coordinating large numbers of people in the principal agent problem. Despite all
3853360	3860400	of this, the possibility exists for any of these people to win while the others lose, right with
3860400	3865520	me so far. And I'll explain why this is important. What that means is those 1000 employees of the
3865520	3871760	CEO are their own agents with their own payoff functions that are not perfectly aligned with
3871760	3876640	the CEO's payoff function. As such, there are scenarios under which they will defect and do
3876640	3886640	other things. Okay. The only way they become like actual limbs, see my hand is not an agent of its
3886640	3892160	own. It lives or dies with me. Therefore it does exactly what I'm saying at this time. I tell it
3892160	3897040	to go up, it goes up, tell it to go down, it goes down, sideways, sideways, right. An employee is
3897040	3902400	not like that. They will do this and this and sideways, sideways up to a certain point. And if
3902400	3906400	you, if you have them do something that's extremely against their interests, they will not do your
3906400	3911520	action. Do you understand my point? Okay. That is the difference between an AI hypnotizing humans
3912160	3917200	versus an AI controlling drones. AI controlling drones is like your hands. They're actually
3917200	3921680	pieces of your body. There's no defecting. There's no loose wind. They have no mind of their own.
3921680	3924880	They're literally taking instructions. Okay. They have no payoff function. They will
3924880	3930320	kill themselves for the hoard. Right. An AI hypnotizing humans has a thousand principal
3930320	3935600	Asian problems for every thousand humans. And it has to incentivize them to continue and
3935600	3940720	has to generate huge payoffs. It's like an AI CEO. That's really hard to do. Right. The history
3940720	3945440	of evolution shows us how hard it is to coordinate multicellular organisms. You have to make them
3945440	3950320	all live or die as one. Then you get something along these lines. Like an ant colony can coordinate
3950320	3954400	like that because if the queen doesn't reproduce all the ants, it doesn't matter what they're
3954400	3959920	having sort of genetic material. Okay. We are not currently set up for those humans to not be able
3959920	3965440	to reproduce unless the AI reproduces. Do I think we eventually get to a configuration like that?
3965440	3971840	Maybe. Where you have an AI brain is at the center of civilization and it's coordinating all the
3971840	3977280	people around it. And every civilization that makes it is capable of crowdfunding and operating
3977280	3982160	its own AI. That gets me to my other critique of the AI safety guys. I mentioned that the first
3982160	3986480	critique is very theoretical rather than empirical. The second critique is their Abrahamic rather than
3987120	3992800	Dharmic or Sinic. Okay. And you know, our background culture influences things in ways we don't even
3992800	3998560	think about. So much of the paperclip thinking is like a vengeful God will turn you into pillars
3998560	4003440	of salt, except it's a vengeful, you know, AI God will turn you into paperclips. Okay.
4004080	4009440	The polytheistic model of many gods as opposed to one God is we're all going to have our own AI
4009440	4014720	gods and there'll be war of the gods like Zeus and Hera and so on. That's the closest western
4014720	4018800	version, you know, the paganism that predated, you know, Abrahamic religions, but that's still
4018800	4023200	there in India. That's still how Indians think. That's why India is sort of people got so woke
4023200	4027440	that they don't even make large scale cultural generalizations anymore. But it's true that India
4027440	4034640	is just culturally more amenable to decentralization to, you know, multiple gods rather than one
4034640	4040240	God in one state. Okay. And then the Chinese model is yet the opposite, like they have like,
4040240	4043840	I mean, of course they have their tech entrepreneurs and so on, but they're, if India is more
4043840	4047520	decentralized, China is more centralized, they have like one government and one leader for the
4047520	4053360	entire civilization. Okay. And, and that the biggest thing that China has done over the last 20 or
4053360	4058640	30 years is they've taken various, you know, U.S. things and they've made sure that they have their
4058640	4063040	own Chinese version where they have root. So they take U.S. social media and they made sure they had
4063040	4067920	root over Sina Weibo. Okay. They make sure they have their own Chinese version of electric cars,
4067920	4073280	the most Chinese version. So the private keys in the sense are with G. So that means that they also
4073920	4078960	at a minimum, you combine these two things, you're at a minimum going to get polytheistic AI
4078960	4084160	of the U.S. and Chinese varieties. And then you add the Indian version on it and you're going to get
4084160	4088640	quite a few of these different AIs around there. And then you have War of the Gods where maybe
4088640	4094480	they are good at coordinating humans who, who, you know, take instructions from them,
4094480	4099360	but they can't live without the humans. And humans are giving input to them. That's a series of
4099360	4103360	things I could probably make that clearer if I just laid it out in bullets in an essay, but just
4103360	4110080	to recap it, A, technical reasons like chaos, turbulence, cryptography, why AI is limited
4110080	4116320	in its ability to predict timeframes and to solve equations, B, practical limits. And AI cannot
4116320	4122960	easily be a Stuxnet because Microsoft and Google and Apple can install software on a billion devices
4122960	4128320	and just kill it, right? Like basically guys with torches come. All right. It can't easily live off
4128320	4132560	the land without humans because they would need hundreds of millions of autonomous robots out there
4132560	4138000	to control, to mine the ore and, and set the data centers. It can't just hypnotize humans
4138000	4141920	like it can control drones because of the principal agent problem and the degree of human
4141920	4147440	defection. To make those humans do that, you'd have to have such massive alignment between the AI
4147440	4151680	and humans that the humans all know they'll die if the AI dies and vice versa. We're not there.
4151680	4155280	Maybe we'll be there in like, I don't know, n number of years, but not for a while. That's a
4155280	4161280	total change in like how states are organized. Okay. Finally, let me just talk about the physics
4161280	4167120	a little bit more. There's a lot of stuff which is talked about at a very sci-fi book level of
4167120	4171920	it'll just invent nanomedicine and nanotech and kill us all and so on and so forth. Now look,
4171920	4175600	I like Robert Freitas, obviously Richard Feynman's a genius and so on and so forth,
4176160	4182320	but nanotech somehow hasn't been invented yet. Okay. Meaning that, you know, there's a lot of
4182320	4188800	chemists that have worked in this area. Okay. And a lot of what nanotech is like rebranded chemistry
4188800	4194720	because those are the molecular machines, you know, for example, DNA polymerase or ribosome,
4194720	4198400	those are molecular machines that we can get to work at that scale, the evolved ones.
4199120	4202640	To my knowledge, and I may be wrong about this, I haven't looked at it very, very recently,
4203280	4207840	we haven't actually been able to make artificial, you know, replicators of the stuff that they're
4207840	4212960	talking about, which means it's possible that there's some practical difficulty that intervened
4212960	4217920	between Feynman and Freitas and so on's calculations, right? Just a sheer fact that those
4217920	4222320	books have came out decades ago and no progress has been made indicates that maybe there's a road
4222320	4226640	block that wasn't contemplated, right? So you can't just click your fingers and say, boom,
4226640	4230880	nanomus, and it's sort of like clicking your fingers and saying, boom, time travel, right?
4230880	4235360	Nanomus and exists. That was a good poke that I had a while ago in a conversation like this,
4235360	4239520	where the AI guy, AI safety guy on their side was like, well, time travel, that's too implausible.
4239520	4244400	I'm like, yeah, but you're waiting on the nanotech thing you're thinking is like here,
4244400	4249280	and you're making so many assumptions there that I want to actually see some more work there. I
4249280	4254000	want to actually see that nanotech is actually more possible than you think it is. As for, oh,
4254000	4258560	we just need to mix things in a beaker and make a, you know, virus and so on. You know what is
4258560	4263920	really, really good at defending against novel viruses, like the human immune, that's something
4263920	4270240	that's within envelope, right? Like you have evolved to not die and to fight off viruses. Is it possible
4270240	4276320	that maybe you could make some super virus? I mean, maybe, but again, like humans are really good
4276320	4280240	and the immune system is really good at that kind of thing. That is what we're set up to do,
4280240	4285040	right? To adapt to that billions of years of evolution being set up. Physical constraints
4285840	4290800	are not really contemplated when people talk about these super powerful mathematical constraints,
4290800	4294560	practical constraints are not contemplated. And I could give more, but I think that was a lot
4294640	4299760	right there. Let me pause here. Yeah. Let me try to steal man a few things. And then
4301040	4303920	I do think, you know, it's before too long, I want to kind of get back to the
4305600	4309520	somewhat less, you know, radically transformative scenarios and ask a few follow up questions on
4309520	4314080	that too. But I think for starters, I would say the, the sort of Eleazar, you know, he's updated
4314080	4317680	his thinking over time as well. And I would say probably doesn't get quite enough credit for it
4317680	4322640	because he's definitely on record, you know, repeatedly saying, yeah, I was kind of expecting
4322720	4328400	more something from like the deep mind school to pop out and be, you know, wildly overpowered
4328960	4334800	very quickly. And on the contrary, it seems like we're in more of a slow takeoff type of scenario
4334800	4339680	where, you know, we've got these, again, like super high surface area kind of suck up all the
4339680	4343920	knowledge, gradually get better at everything. Some surprises in there, you know, certainly some
4343920	4349200	emergent properties, if you will accept that term, you know, surprise surprises to the developers
4349200	4354880	of nothing else, right, that are definitely things we don't fully understand. But it does seem to be a,
4354880	4360880	you know, more gradual turning up of capability versus some like, you know, super sudden surprise.
4361920	4368320	But okay, so then what is the alternative? I'm going to try to kind of give you the what I,
4368320	4377040	what I think of as the most consensus strongest scenario where humans lose track of the future
4377680	4382080	and or lose control of the future, maybe starting by kind of losing track of the president and then
4382080	4386080	having that kind of, you know, give way to losing control of the future. And I think within that,
4386080	4391920	by the way, the, I'm not really one who cares that much about like, whether AIs say something
4391920	4397440	offensive today, I'm not easily offended and like, whatever. That's not, that's not world ending. I
4397440	4401520	understand your point. That's not like, who cares, whatever, that's within scope, that's within envelope.
4401520	4405920	Within within this bigger kind of, you know, what is the real, you know, most likely
4407520	4413120	path to like AI disaster, as understood, I think by the smartest people today, I think that is
4413120	4418880	still a useful leading indicator, because it's like, okay, the developers, you know, whether you
4418880	4421840	agree with their politics, whether you agree with their, whether you think their commercial
4421840	4427920	reasons are their sincere reasons or not, they have made it a goal to get the AI to not say
4427920	4431600	certain things, right? They don't want it to be offensive. The most naive, you know, kind of
4431600	4435120	down the fairway interpretation of that is like, hey, they want to sell it to corporate customers.
4435120	4439280	They know that their corporate customers don't want, you know, to have their AI saying offensive
4439280	4445440	things. So they don't want to say offensive things. And yet they can't really control it. It's like
4445440	4451920	still pretty easy to break. So I view that as just kind of a leading indicator of, okay, we've seen
4451920	4459680	GPT two, three and four over the last four years. And that's, you know, a big delta in capability.
4460240	4466560	How much control have we seen developed in that time? And does it seem to be keeping pace?
4467120	4472640	And my answer would be on the face of it, it seems like the answer is no, you know, we, we don't
4472640	4479360	have the ability to really dial in the behavior such that we can say, okay, you're going to,
4479360	4484480	you know, you can expect, you can trust that these AIs will like not do, you know, aviancy.
4485200	4488720	On the contrary, it's like, if you're a little clever, you know, you can get them to do it.
4488720	4490240	You can break out of the sandbox on it.
4490800	4494080	Yeah. And it's not even like, I mean, we've talked about, you know, things where you have
4494080	4497600	access to the weights and you're doing like counter optimizations, but you don't even need that,
4497600	4502080	you know, the kind of stuff I do in like my red teaming in public is literally just like
4502800	4507200	feed the AI a couple of words, put a couple of words in its mouth, you know, and it will kind
4507200	4511920	of carry on from there. So with that in mind is just a leading indicator. You know, I don't know
4511920	4516240	how powerful the most powerful AI systems get over the next few years, but it seems very plausible
4516240	4521920	to me that it might be as powerful as like an Elon Musk type figure, you know, somebody who's
4521920	4527680	like really good at thinking from first principles, really smart, you know, really dynamic across a
4527680	4534320	wide range of different contexts. And, you know, he's not powerful enough to like in and of himself
4534320	4540160	take over the world, but he is kind of becoming transformative. Now imagine that you have that
4540160	4546480	kind of system, and it's trivial to replicate it. So, you know, if you have like one Elon Musk,
4546480	4551520	all of a sudden you can have arbitrary, you know, functionally arbitrary numbers of Elon Musk power
4551520	4556880	things that are clones of each other. Maybe I can pause you there. So that's my polytheistic AI
4556880	4562160	scenario. But here's the thing that is this is background, but I want to push it to foreground.
4563120	4568080	You still have a human typing in things into that thing. The human is doing the jailbreak, right?
4568080	4572880	What we're talking about is not artificial intelligence in the sense of something separate
4572880	4578960	from a human, but amplified intelligence. Amplified intelligence, I very much believe in. The reason
4578960	4584320	is amplified intelligence. So here's something that people may not know about humans. There's this
4584480	4592800	great book, Cooking Made Us Human. Okay, tool use has shifted your biology in the following way.
4592800	4597920	For example, I know I'll map it to the present day. This book by Richard Rang and Cooking Made Us
4597920	4604560	Human, where the fact that we started cooking and using fire meant that we could do metabolism
4604560	4612240	outside the body, which meant it freed up energy for more brain development. Okay,
4612240	4616080	similarly, developing clothes meant that we didn't have to evolve as much
4616080	4621360	fur, again, more energy for brain development. Evolving tools meant we didn't have as much
4621360	4626720	fangs and claws and muscles, again, more energy for brain development, right? So encephalization
4626720	4632960	quotient rose as tool use meant that we didn't have to do as much natively and we could push
4632960	4638960	more to the machines. In a very real sense, we have been a man machine symbiosis since the
4638960	4646160	invention of fire and the stone axe and clothes, right? You do not exist as a human being on your
4646160	4651840	own like the entire Ted Kaczynski concept of like living in nature by itself. Humans are social
4651840	4658400	organisms that are adapted to working with other humans and using tools and you have for and we
4658400	4663520	have been for millennia. Okay, this goes back, not just human history, but like hundreds of
4663520	4669760	thousands years before 100 gatherers are using tools. Okay, so what that means is man machine
4669760	4675920	symbiosis is not some new thing. It's actually the old thing that broke us away from other
4675920	4681120	primate lineages that weren't using tools. Okay, this is the fundamental difference between what I
4681120	4687280	call Uncle Ted and Uncle Fred. Uncle Ted is Ted Kaczynski. It's a unabomber. It's a doomer. It's
4687280	4692400	a decelerator, the de grother who thinks we need to go back to Gaia and Eden and become monkeys
4692400	4698960	and live in the jungle like, like, you know, Ted Kaczynski, right? The unabomber cell. Uncle Fred
4698960	4705280	is Friedrich Nietzsche, right? Nietzsche and we must get the stars and become ubermen and so
4705280	4709920	on and so forth. This I think is going to become, and I actually tweeted about this years ago before
4709920	4716400	current AI debates, that, you know, between anarcho primitivism, de growth, deceleration,
4716400	4724640	okay, on the one hand, and transhumanism and acceleration and human 2.0 and human self-improvement
4724640	4729680	and make it just the stars, on the other hand, this is the future political axis, the current one.
4730320	4735760	And roughly speaking, you can cut, it's not really left and right because you'll have both
4735760	4740800	left status and right conservatives go over here. You know, left states will say it's against
4740800	4744480	the state and the right states will say, the right conservatives say it's against God,
4744480	4748400	okay, and you'll have left libertarians and right libertarians over here,
4748400	4752640	where left libertarians say it's my body and, you know, the right libertarians say it's my,
4752640	4758080	you know, my money, right? And so that is a re-architecting of the political axis where,
4758080	4761360	you know, Uncle Ted and Uncle Fred, which is kind of a clever way of putting it, okay?
4762640	4767840	And the problem with the Uncle Ted guys in my view is, as I said, yeah, if they go and want to live
4767840	4773200	in the, you know, the woods, fine, go get them. But once you start having even like a thousand,
4773200	4778320	forget a thousand, a hundred people doing that, your trees will very quickly get exfoliated,
4778320	4782800	you know, the leaves are going to get all picked off of them. Humans are not set up to just literally
4782800	4786960	live in the jungle right now. You've had hundreds of thousands of years of evolution that have
4786960	4792320	driven you in the direction of tool use, social organisms, farming, etc., etc. The man machine
4792320	4797840	symbiosis is not today, it's yesterday and the day before and 10,000 years ago and 100,000 years ago.
4798400	4802160	And how do we know we've got man machine symbiosis? Can you live without
4803440	4807360	even if you're not living, even if you're not using the stove, somebody's using the stove
4807360	4812960	to make you food, right? Can you live without the tractors that are digging up the grains? Can you
4812960	4818400	live without indoor heating? Can you live without your clothes? Frankly, can you do your work without
4818400	4824320	your phone, without your computer? No, you can't. You are already a man machine symbiosis. Once we
4824320	4829840	accept that, then the question is, what's the next step? And right now, we're in the middle of that
4829840	4835920	next step, which is AI is amplified intelligence. So what you're talking about is not that the AI
4835920	4843200	is Elon Musk, it is that the AI human fusion means there's another 20 Elon Musk's or whatever the
4843200	4850880	number is, okay? And that's good. That's fine. That's within envelope. That's just a bunch of smarter
4850880	4856560	humans on the planet. That is amplified intelligence. That is more like, you know, I mentioned the tool
4856560	4860800	thing. Okay, the other analogy would be like a dog. You know, a dog is man's best friend.
4861840	4868960	Right? So that AI does not live without humans can turn it off. They have to power it. They have
4868960	4873760	to give it substance, right? Eventually, that might become like a ceremonial thing, like this is our
4873760	4879520	God that we pray to, right? Because it's wiser and smarter than us and it appears in an image.
4879520	4884240	But the priests maintain it. You know, just like you go to a Hindu temple or something like that,
4884320	4888000	and the priests will pour out the ghee, you know, for the fires and so on and so forth. And then
4888000	4892960	everybody comes in and prays, okay? The priests believe in the whole thing, but they also maintain
4892960	4897440	the back of the house. They do the system administration for the temple. Same, you know,
4897440	4903360	in a Christian church, right? The, the, you know, like, it's not like it appears out of nowhere.
4903360	4910240	Somebody, you know, went and assembled this cathedral, right? They saw the back of the house,
4910240	4914000	the fact that it was just woods and rocks and so on that came together. Then when people come
4914000	4918480	there, it feels like a spiritual experience. You see what I'm saying? Okay. So the equivalent of that,
4918480	4924160	the priests or the, you know, the people maintaining temples, cathedrals, mosques, whatever, is
4926080	4931760	engineers who are maintaining these future AIs, which appear to you as Jesus. They appear to you,
4931760	4936160	maybe even a hologram. Okay, you come there, you ask it for guidance as an oracle. You've also got
4936160	4942560	the personal version on your phone. You ask it for guidance. But guess what? You're still a human
4943120	4948080	AI symbiosis until and unless the AI actually has the terminator scenario where it's got
4948080	4952400	lots of robots that can live on its own. I'm not saying that's physically impossible. I did give
4952400	4957200	some constraints on it earlier, but for a while we're not going to be there. So that alone means
4957200	4960960	it's not fume because we don't have lots of drones running around. The AI has to be with
4960960	4967120	the human. It's a human AI symbiosis. It's not AI Elon Musk. It is human AI fusion that becomes
4967120	4970800	Elon Musk. And frankly, that's not that different from what Elon Musk himself is. Elon Musk would
4970800	4974720	not be Elon Musk without the internet. Without the internet, you can't tweet and reach 150 million
4974720	4981040	people. The internet itself made Elon what he is. And so this is the next version of that.
4981040	4984400	Maybe there's now 30 Elon's because the AI makes the next 30 Elon's.
4984400	4991120	Yeah, I mean, again, I think I'm largely with you with just this one very important nagging worry
4991120	4995680	that's like, what if this time is different because what if these systems are getting
4996560	5003520	so powerful so quickly that we don't really have time for that techno human fusion
5003520	5007280	to really work out? And I'll just give you kind of a couple of data points on that.
5008640	5012880	You said it's still somebody putting something into the AI. Well, sort of, right? I mean,
5012880	5018320	already we have these proto agents and the super simple scaffolding of an agent is just
5018880	5025840	run it in a loop, give it a goal, and have it kind of pursue some like plan, act, get feedback,
5025840	5032000	and loop type of structure, right? It doesn't seem to take a lot. Now, they're not smart enough yet
5032000	5038000	to accomplish big things in the world. But it seems like the language model to agent
5038880	5045520	switch is less one right now that is gated by the structure or the architecture and more one
5045520	5050160	that's just gated by the fact that like the language models when framed as agents just aren't
5050160	5054480	that successful at like doing practical things and getting over hump. So they tend to get stuck.
5055440	5058800	But it doesn't seem that hard to imagine that like, you know, if you had something that is sort
5058800	5065040	of that next level that you put it into a loop, you say, okay, you're Elon Musk, LLM, and your
5065040	5071600	job is to like make, you know, us, whatever us exactly is a, you know, multi planetary species.
5071600	5077360	And then you just kind of keep updating your status, keep updating your plans, keep trying
5077360	5083280	stuff, keep getting feedback. And, you know, like what really limits that.
5084160	5090400	There may be like a really good program. But the whole AI kills everyone thing is so it's like,
5091040	5098960	where's the actuator? Okay, I hit enter. What kills me? Right? Is it a hypnotized human who's
5098960	5104480	being hypnotized by an AI that he's typed into? And he's radicalized himself by typing into a
5104480	5108800	computer? Okay, that's not that different from a lot of other things that have happened in the past.
5108800	5114000	Right. So who is actually striking me? Right? Who's striking the human? It's another human within
5114000	5118960	acts that he's been radicalized by an AI. Okay, he's not actually that's not even the right term.
5118960	5124640	We're giving agency to the AI when it's not really an agent. It is a human who's self radicalized by
5124640	5131120	typing into a computer screen and has hit another human. That's one scenario. The other scenario is
5131120	5135840	it's literally a Skynet drone that's hitting you. Those are the only two. How else is it going to be
5135840	5141440	physical? Right? How does it? The actuation step is a part that is skipped over and it's a non trivial
5141440	5146160	step. Well, I think it could be lots of things, right? I mean, if it's not one of those two,
5146160	5152320	if it's not human or drone hitting you, what is it? Just habitat degradation, right? I mean,
5152320	5156640	how do we kill most of the other species that we drive to extinction? We don't go out and hunt them
5156640	5161920	down with axes one by one. We just change the environment more broadly to the point where
5161920	5166720	it's not suitable for them anymore and they don't have enough space and they kind of die out, right?
5166720	5171760	So we did hunt down some of the megafauna literally one by one with spears and stuff,
5171760	5178720	but most of the recent loss of species is just like we're out there just extracting resources for
5178720	5184240	our own purposes. And in the course of doing that, you know, whatever bird or whatever, you know,
5184240	5189360	thing just kind of loses its place and then it's no more. And I don't think that's like totally
5189360	5195280	implausible. Wait, so that is though, I think within normal world, right? What does that mean?
5195280	5202400	That means that some people, some amplified intelligence, and maybe we might call it HAI,
5202400	5209440	okay, human plus AI combination, right? Some HAIs outcompete others economically and they lose
5209440	5214080	their jobs. Is that what you're talking about? I think also the humans potentially become
5214080	5219520	unnecessary in a lot of the configurations, like just a recent paper from DeepMind.
5219520	5225280	It's your marginal product workers. Or negative. Yeah, I mean, so the last, you know, DeepMind has
5225280	5232320	been on Google, Google DeepMind has been on a tear of increasingly impressive medical AIs.
5232320	5238160	Their most recent one takes a bunch of difficult case studies from the literature. I mean, case
5238160	5244080	studies, you know, this is like rare diseases, hard to diagnose stuff, and asks an AI to do the
5244080	5250560	differential diagnosis, compares that to human and compares it to human plus AI. And they phrase
5250560	5256240	their results like in a very understated way. But the headline is the AI blows away the human
5256240	5262480	plus AI. The human makes the AI worse. So here's the thing. I'll say something provocative, maybe.
5262480	5268800	Okay, like I have in a very fine. I do think that the ABCs of economic apocalypse for blue America
5268800	5274480	are AI, Bitcoin and China, where AI takes away their, a lot of the revenue streams, the licensures
5274480	5279920	that have made medical and legal costs and other things so high. Bitcoin takes away the power of
5279920	5286160	money and China takes away their military power. So I've received total meltdown for blue America
5287280	5292640	in the years and, you know, maybe decade to come already kind of happening. But that's different
5292640	5298160	than being at the end of the world, right? Like blue America had a really great time for a long time
5298160	5303920	and they've got these licensure locks. But because of that, they've hyperinflated the cost of medicine.
5304720	5310160	It's like how much, so what you're talking about is, wow, we have infinite free medicine.
5310720	5314320	Man, Dr. Billing events are going to get ahead. That's the point.
5315040	5319360	Yeah. And to be clear, I'm really with you on that too. Like I want to see one of the things,
5319360	5324240	when people say like, what is good about AI, you know, why should we, why should we pursue this?
5324960	5331360	This, my standard answer is high quality medical advice for everyone at pennies,
5331440	5336000	you know, per visit, right? It is orders of magnitude cheaper. We're already starting to
5336000	5340080	see that in some ways it's better. People prefer it, you know, that AI is more patient,
5340080	5345360	it has better bedside manner. I wouldn't say, you know, if I was giving my, you know, my own
5345360	5350560	family advice today, I would say use both a human doctor and an AI, but definitely use the AI as
5350560	5355200	part of your mix. Absolutely. That's right. That's right. But you're prompting it still, right?
5355200	5360480	The smarter you are, the smarter the AI is, you notice this immediately with your vocabulary,
5360480	5364800	right? The more sophisticated your vocabulary, the finer the distinctions you can have,
5364800	5369600	the better your own ability to spot errors. You can generate a basic program with it, right?
5369600	5373280	But really amplified intelligence is I think a much better way of thinking about it,
5373280	5378560	because whatever your IQ is, it surges it upward by a factor of three or whatever the number.
5378560	5383360	And maybe the amplifier increases with your intelligence, but that, that internal intelligence
5383360	5387280	difference still exists. It's just like what a computer is, a computer is an amplifier for
5387280	5392320	intelligence. If you're smart, you can hit enter and programs can go to like, like thinking about
5392320	5398720	the Minecraft guy, right? Or Satoshi, one person built a billion or an associated trillion dollar
5398720	5404880	thing, you know, obviously other people continued Bitcoin and so on and so forth, right? So what I
5404880	5411280	feel though is this is what I mean by going from nuclear terrorism to the TSA. Okay, we went from
5411280	5416800	AI will kill everyone. And I'm like, what's the actuator to, okay, it'll gradually to greater
5416800	5419680	environment. What does that mean? Okay, some people lose their jobs, but then we're back in normal
5419680	5423600	worlds. Well, hold on, let me paint a little bit more complete picture because I don't think we're
5423600	5431040	quite there yet. So I think the differential diagnosis, recent paper, that's just a data point
5431040	5435520	where it's kind of like chess. This, you know, this came long before, right? There was a period
5435520	5439520	where humans are the best chess players, then there was a period where the best were the hybrid
5439520	5445120	human AI systems. And now as far as I understand it, we're in a regime where the human can't really
5445120	5450640	help the AI anymore. And so the AIs are, you know, the best chess players are just pure AIs.
5450640	5455440	We're not there in medicine, but we're starting to see examples where, hey, in a pretty defined study
5456000	5461040	differential diagnosis, the AI is beating, not just beating the humans, but also beating the AI
5461040	5467680	human hybrid or the human with access to AI. So, okay, that's not it, right? There's a paper
5467680	5477200	recently called Eureka out of NVIDIA. This is Jim Fan's lab where they use GPT-4 to write the reward
5477200	5484160	functions to train a robot. So you want to train a robot to like twirl a pencil in fingers. Hard,
5484160	5488880	you know, hard for me to do. Robots, you definitely can't do it. How do you train that? Well, you
5488880	5493040	need a reward function, the reward function. Basically, while you're in the early process
5493040	5498080	of learning and failing all the time, the reward function gives you encouragement when you're on
5498080	5502400	the right track, right? So there are people who, you know, have developed this skill and you might
5502400	5506720	do something like, well, if the pencil has angular momentum, you know, then that seems like you're
5506720	5510320	on maybe sort of the right track. So give that, you know, a reward, even though at the beginning
5510320	5516000	you're just failing all the time. Turns out GPT-4 is way better than humans at this, right? So it's
5516000	5521760	better at training robots. So all of that is awesome. And it's great. And, but here is, here's the thing,
5521760	5527120	is there's a huge difference between AI is going to kill everybody and turn everybody into paper
5527120	5534880	clips, okay, versus some humans with some AI are going to make a lot more money. And some people
5534880	5539120	are going to lose their jobs. Yeah, I'm not scared of that. I'm not scared of that scenario. I mean,
5539120	5544160	it could be disruptive. It could be disruptive, but it's not existential under itself.
5545120	5549280	Big deal. Okay. So that's why I went, right. There's the, the, the beta, to me, it comes,
5549280	5554560	if I, if I ask just one question is what is the actuator, right? You know, sensors and actuators,
5554560	5560240	right? What is the thing that's actually going to plunge a knife or a bullet into you and kill
5560240	5569200	you? It is either a human who has hypnotized themselves by typing into a computer, like
5569200	5574720	basically an AI terrorist, you know, which is kind of where some of the EAs are going in my view,
5574800	5582640	or it is like an autonomous drone that is controlled in a starcraft or terminator like way.
5583280	5588560	We are not there yet in terms of having enough humanoid or autonomous drones that are intranet
5588560	5593840	connected and programmable. That won't be there for some time. Okay. So that alone means fast take
5593840	5600480	off is, and what I think by the time we get there, you will have a cryptographic control over them.
5600480	5605120	That's a crucial thing. Cryptography fragments the whole space in a very fundamental way.
5605680	5610480	If you don't have the private keys, you do not have control over it. So long as that piece of
5610480	5615280	hardware, the cryptographic controller, you've nailed the equations on that. And frankly, you can
5615280	5621040	use AI to attack that as well to make sure the code is perfect, right? Remember you talked about
5621040	5628320	attack and defense? AI is attack crypto's defense, right? Because one of the things that crypto is
5628320	5634080	done, do you know the PKI problem is public key infrastructure? I'll say no, on behalf of the
5634080	5638480	audience. This is good. We should do more of these actually. I feel it's a good, you know, fusion of
5638480	5644800	things or whatever, right? But the public key infrastructure problem, the public key infrastructure
5644800	5651040	problem is something that was sort of lots of cryptography papers and computer science papers
5651040	5657200	in the 90s and 2000s assumed that this could exist and essentially meant if you could assume that
5657200	5664640	everybody on the internet had a public key that was public and a private key that was kept both
5664640	5669360	secure and available at all times, then there's all kinds of amazing things you can do with
5669360	5676160	privacy preserving, messaging and authentication and so on. The problem is that for many years,
5676160	5681120	what cryptographers try to do is they try to nag people into keeping their private keys secure
5681120	5686080	and available. And the issue is it's trivial to keep it secure and unavailable where you write
5686080	5690480	it down, you put it into a lockbox and you lose the lockbox. It's trivial to keep it available
5690480	5696400	and not secure, okay, where you put it on your public website and it's available all the time,
5696400	5702480	you never lose it, but it's not secure because anybody can see it. When you actually ask,
5702480	5707840	what does it mean to keep something secure and available? That's actually a very high cost.
5707840	5713280	It's precious space because it's based on your wallet, right? Your wallet is on your person
5713280	5719360	at all times, so it's available, but it's not available to everybody else, so it's secure.
5719360	5724800	So you actually have to touch it constantly, yes, right? So it turns out that the crypto wallet
5725600	5731680	by adding a literal incentive to keep your private keys secure and available because if they're not
5731680	5736320	available, you've lost your money. If they're not secure, you've lost your money, okay? To have both
5736320	5741520	of them, that was what solved the PKI problem. Now we have hundreds of millions of people with
5741520	5746400	public private key pairs where the private keys are secure and available. That means all kinds
5746400	5751280	of cryptographic schemes, zero knowledge stuff, there's this amazing universe of things that is
5751280	5755360	happening now. Zero knowledge in particular has made cryptography much more programmable.
5755360	5760320	There's a whole topic which is, if you want something that's kind of, you know, like AI was
5760320	5764320	creeping for a while and people, specialists were paying attention to it and then just burst
5764320	5768640	out on the scene, zero knowledge is kind of like that for cryptography. Thanks to the,
5768640	5770960	you know, you've probably heard of zero knowledge before.
5770960	5780080	Yeah, we did one episode with Daniel Kong on the use of zero knowledge proofs to basically
5780960	5785680	to prove without revealing like the weights that you actually ran the model you said you were
5785680	5788000	going to run and things like that, I think are super interesting.
5788640	5794960	Exactly, right? So what kinds of stuff, why is that useful in the AI space? Well, first is you
5794960	5799280	can use it, for example, for training on medical records while keeping them both private,
5799280	5804000	but also getting the data you want. For example, let's say you've got a collection of
5805040	5814000	genomes, okay, and you want to ask, okay, how many G's were in this data set, how many C's,
5814000	5817520	how many A's, how many T's, okay, like you just say, like, that's a very simple downstairs.
5817520	5823840	What's the ACG T content of this, you know, the sequence data set, you could get those numbers,
5823840	5827040	you could prove they were correct without giving any information about the individual sequences,
5827040	5831120	right, or more specifically, you do it at one locus and you say, how many G's and how many C's
5831120	5837920	are at this particular locus and you get the SNP distribution, okay. So it's useful for what you
5837920	5842160	just said, which is like showing that you ran a particular model without giving anything else away,
5842160	5847680	it's useful for certain kinds of data analysis. There's a lot of overhead on compute on this
5847680	5850960	right now, so it's not something that you do trivially, okay, but it'll probably come down with
5850960	5859920	time. But what is perhaps most interestingly useful for it is in the context of AI is coming up with
5859920	5865840	things in AI can't fake. So what we talked about earlier, right, like an AI can come up with all
5865840	5872880	kinds of plausible sounding images, but if it wasn't cryptographically signed by the sender,
5874080	5880160	then, you know, it, and it should be signed by sender and put on chain. And then at least you
5880160	5887600	know that this person or this entity with this private key asserted that this object existed at
5887600	5891680	this time in a way that'd be extremely expensive to falsify because it's either on the Bitcoin
5891680	5895760	blockchain or another blockchain, it's very expensive to rewind, okay. This starts to be
5895760	5901040	a bunch of facts that an AI can't fake. You know, so going back to the kind of big picture
5901920	5906240	loss of control story, I was just kind of trying to build up a few of these data points that like,
5906240	5911280	hey, look at this differential diagnosis, we already see like humans are not really adding value
5911280	5916880	to AI's anymore. That's kind of striking. And like similarly with training robot hands, GPT-4 is
5916880	5923840	outperforming human experts. And by the way, all of the sort of latent spaces are like totally
5923840	5928240	bridgeable, right? I mean, one of the most striking observations of the last couple of years of study
5928240	5934480	is that AI's can talk to each other in high dimensional space, which we don't really have a
5934480	5938720	way of understanding natively, right? It takes a lot of work for us to decode.
5939440	5943600	This is like the language thing? We're starting to see AI's kind of develop,
5943600	5951600	not obviously totally on their own as of now, but there is becoming an increasingly reliable
5951600	5958480	go-to set of techniques if you want to bridge different modalities with like a pretty small
5958480	5962880	parameter adapter. That's interesting. Actually, what's a good paper on that? I actually hadn't
5962880	5966880	seen that. The Blip family of models out of Salesforce research is really interesting,
5966880	5971520	and I've used that in production at Salesforce. Really? Yeah, Salesforce research. They have a
5971520	5979680	crack team that has open sourced a ton of stuff in the language model, computer vision, joint space.
5980560	5986640	And you see this all over the place now, but basically what they did in a paper called Blip
5986640	5991120	2, and they've had like five of these with a bunch of different techniques. But in Blip 2,
5991840	5996480	they took a pre-trained language model and then a pre-trained computer vision model,
5997040	6001760	and they were able to train just a very small model that kind of connects the two. So you could
6001760	6009040	take an image, put it into the image space, then have their little bridge that over to language
6009040	6013040	space, and that everything else, the two big models are frozen. So they were able to do this on
6013040	6018640	just like a couple days worth of GPU time, which I do think goes to show how it is going to be
6018640	6023280	very difficult to contain proliferation. Which is good. In my view, that's really good.
6023280	6028000	As long as it doesn't get out of control, I'm probably with you on that too. But by bridging
6028000	6033280	this vision space into the language space, then the language model would be able to converse with you
6033280	6038960	about the image, even though the language model was never trained on images, but you just had this
6038960	6044080	connector that kind of bridges those modalities. It's just, it's like another layer of the network
6044080	6050160	that just bridges two networks almost. Yeah, it bridges the spaces. It bridges the conceptual
6050160	6055040	spaces between something that has only understood images and something that has only understood
6055040	6058960	language, but now you can kind of bring those together. As I think about it, it's not that
6058960	6064880	surprising because that's what, for example, text image models are basically that. They're
6064880	6070400	bridging two spaces in a sense, right? But I'll check this paper out. So on the one hand,
6070480	6074160	it's not that surprising. On the other hand, I should see how they implemented it or whatever,
6074160	6079200	so blip two. Okay. Yeah, I think the most striking thing about that is just how small it is. Like,
6079200	6085840	you took these two off the shelf models that were trained independently for other purposes,
6086400	6093200	and you're able to bridge them with a relatively small connector. And that seems to be kind of,
6093200	6097920	you know, happening all over the place. I would also look at the Flamingo architecture,
6097920	6104160	which is like a year and a half ago now out of DeepMind. That was one for me where I was like,
6104160	6108720	oh my, and it's also a language to vision, where they keep the language model frozen.
6108720	6114000	And then they kind of, in my mind, it's like, I can see the person in their garage like tinkering
6114000	6117440	with their soldering iron, you know, because it's just like, wow, you took this whole language
6117440	6122080	thing that was frozen, and you kind of injected some, you know, vision stuff here, and you added
6122080	6126560	a couple layers, and you kind of Frankenstein it, and it works. And it's like, wow, that's not really,
6127200	6132240	it wasn't like super principled, you know, it was just kind of hack a few things together and,
6132240	6135280	you know, try training it. And I don't want to diminish what they did, because I'm sure there
6135280	6141600	were, you know, more insights to it than that. But it seems like we are kind of seeing a reliable
6141600	6148000	pattern of the key point here being model to model communication through high dimensional
6148000	6156240	space, which is not mediated by human language, is I think one of the reasons that I would expect,
6156240	6160480	and by the way, there's lots of papers too on like, you know, language models are human level,
6160480	6164560	or even superhuman prompt engineers, you know, they're they're self prompting, like,
6164560	6168880	techniques are getting pretty good. So if I'm imagining the big picture of like,
6169920	6173440	and we can, you know, get back to like, okay, well, how do we use any techniques crypto or
6173440	6178800	otherwise to keep this under control? And then I would say this is kind of the newer school of
6178800	6184800	the big picture AI safety worry. Obviously, there's a lot of flavors. But if you were to,
6184800	6188480	you know, go look at like a Jay Acatra, for example, I think a really good writer on this.
6189760	6194000	Her worldview is less that we're going to have this fume and more that over a period of time,
6194000	6197440	and it may not be a long period of time, maybe it's like a generation, maybe it's 10 years,
6197440	6202000	maybe it's 100 years. But obviously, those are all small in the sort of, you know, grand scheme of
6202000	6213520	the future. We have, in all likelihood, the development of AI centric schemes of production,
6213520	6217280	where you've got kind of your high level executive function is like your language model,
6217280	6221920	you've got all these like lower level models, they're all bridgeable, all the spaces are bridgeable
6221920	6226720	in high dimensional form, where they're not really mediated by language, unless we enforce
6226720	6232160	that. I mean, we could say, you know, it must always be mediated by language so we can read the
6232160	6238400	logs. But there's a text to that, right? Because going through language is like highly compressed,
6239120	6243600	compared to the high dimensional space to space. All right, so let me see if I can
6243600	6247680	steal man or articulate your case, you're saying, AI's are going to get good enough,
6247680	6250960	they're going to be able to communicate with each other good enough, and they'll be able to do enough
6250960	6255520	tasks that more and more humans will be rendered economically marginal and unnecessary.
6255520	6258880	I'm not saying I think that will happen, I'm just saying I think there's a good enough chance
6258880	6263200	that that will happen, but it's worth taking really seriously. I actually think that will happen,
6263200	6267440	something along those lines, or in the sense of at least massive economic disruption,
6267440	6273440	definitely. Okay, but I'll give an answer to that, which is both, you know, maybe fun and not fun.
6273440	6278800	Have you seen the graph of the percentage of America that was involved in farming?
6279520	6281520	Yeah, I tweeted a version of that once.
6282160	6286240	I did. Okay, great. Good. So you're familiar with this, and you're familiar with what I mean by
6286240	6292640	the implication of it, where basically Americans used to identify themselves as farmers, right?
6292640	6301360	And manufacturing rose as agriculture collapsed, right? And here is the graph on that. But from
6301360	6308240	like 40% in the year 1900 to like a total collapse of agriculture, and then also more recently a
6308240	6313600	collapse of manufacturing into bureaucracy, paperwork, legal work, what is up into the right
6313600	6321120	since then is, you know, the lawyers, what is up into the right? What is replacing that?
6321840	6327520	Starting in around the 1970s, we used to be adding energy production and energy production
6327520	6332720	flatlined once people got angry about nuclear power. So this is a future that could have been,
6332720	6338720	we could be on Mars by now, but we got flatlined, right? What did go up into the right? So construction
6338720	6343680	costs, this is the bad scenario where the miracle energy got destroyed because regulations,
6344560	6349200	the cost was flat. And then when vertical, when regulations were imposed, all the progress was
6349200	6355360	stopped by decels and degrowthers. And then Alara was implemented, which said nuclear energy
6355360	6360720	has to be as low risk as reasonably necessary, as reasonably achievable. And that meant that you
6360720	6364640	just keep adding quote safety to it until it's as same as cost as everything else, which means you
6364640	6369440	destroyed the value of it, right? But you know what was up into the right? What replaced those
6369440	6373760	agriculture and manufacturing jobs? Look at this, you see this graph? For the audio only, we will
6373760	6378320	put this on YouTube. So if you want to see the graph do the YouTube version of this, for the audio
6378320	6383040	only group, it's an exponential curve in the number of lawyers in the United States from,
6383040	6387760	looks like maybe two thirds of a million to 13 million over the last 140 years.
6387760	6393600	Yeah. And in 1880, it was like, like sub 100,000 or something like that, right? And then it's just
6393600	6399040	like, especially that 1970 point, that's when it went totally vertical. Okay. And it's probably
6399040	6404160	even more since. So, you know, if you add paperwork jobs, bureaucratic jobs, you know,
6404160	6409040	every lawyer is like, you know, net, sorry, lawyers, but you're basically negative value add,
6409040	6414000	right? Because it should, the fact that you have a lawyer means that you couldn't just self serve a
6414000	6418880	form, right? Basic government is platform is where you can just self serve and you fill it out.
6418880	6423280	And you don't have to have somebody like code something for you custom, you know, lawyers
6423280	6428640	that's doing custom code is because the legal code is so complicated. So, you know, the whole
6428640	6432000	Shakespeare thing, like first thing we do, let's, you know, kill all the lawyers. First thing we
6432000	6437840	do, let's automate all the lawyers, right? Only something that's the hammer blow of AI
6437840	6443360	can break the backbone and it will. That's it's going to break the backbone of blue America,
6443360	6447440	right? It's going to cause that's why the political layer and the sovereignty layer
6447440	6453120	is not what AI people think about. But it's like crucial for thinking about AI, because what tribes
6453120	6460080	does AI benefit? And again, we got away from why does AI kill everybody? Well, it's going to need
6460080	6463680	actuators. Who's going to stab you? Who's going to shoot you? It's got to be a human hypnotized
6463680	6468560	by AI or a drone that AI controls. A human hypnotized by AI is actually a conventional
6468560	6472320	threat. It looks like a terrorist cell. We know how to deal with that, right? It's just like radicalized
6472320	6476960	humans that worship some AI that stab you. It's like the pause AI people are one step, I think,
6476960	6481360	away from that. All right. But that's just like on Shin Riko. That's like allocated. That's like
6481360	6486960	basically terrorists who think that the AI is telling them what to do. Fine. If it's not a human
6486960	6492000	that's stabbing you, it is a drone. And that's like a very different future where
6492880	6496720	like five or 10 or 15 years up, maybe we have enough internet connected drones out there,
6496720	6501680	but even then they'll have private keys. So there's going to be fragmentation of address space.
6501680	6507360	Not all drones be controlled by everybody in my view. Okay. That's what AI safety is. AI safety is
6507360	6512720	can you turn it off? Can you kill it? Can you stop it from controlling drones? That's what AI
6512720	6518320	safety is. Can you also open the model weights so you can generate adversarial inputs? Can you
6518320	6521680	open the model weights and proliferate it? You're saying, oh, proliferation is bad. I'm saying
6521680	6526880	proliferation is good because if everybody has one, then nobody has an advantage on it.
6527440	6532800	Right. Not relatively speaking. Okay. I have very few super confident positions. So I wouldn't
6532800	6540880	necessarily say I think that proliferation is bad. I'd say so far it's good. It has and even
6540960	6547440	most of the AI safety people, I would say if I could speak on the behalf of the AI safety
6548320	6555920	consensus, I would say most people would say even that the Llama 2 release has proven
6555920	6560160	good for AI safety for the reasons that you're saying. But they opposed it.
6560160	6565280	Well, some didn't, some didn't. I would say the main posture that I see AI safety people taking
6565280	6571200	is that we're getting really close to or we might be getting really close.
6572160	6576240	Certainly if we just naively extrapolate out recent progress, it would seem that we're getting
6576240	6583920	really close to systems that are sufficiently powerful that it's very hard to predict what
6583920	6592400	happens if they proliferate. Llama 2, not there. And so, yes, it has enabled a lot of interpretability
6592400	6596720	work. It has enabled things like representation engineering, which there is a lot of good
6596720	6602160	stuff that has come from it. The big thing that I want to kind of establish is you agree with me
6602160	6608800	on the actuation point or not. The thing is this thing, like Llama 2 proliferates and so
6608800	6614000	businesses are disrupted and people, maybe they paid a lot of money for their MD degree and they
6614000	6618640	can't make us a bunch of money. That's within the realm of what I call conventional warfare.
6618720	6622240	You know what I mean? That's like we're still in normal world as we were talking about, okay?
6622960	6629200	Unconventional warfare is, you know, Skynet arises and kills everybody, okay? And that is what is
6629200	6634640	being sold over here. And when you think about the actuators, we don't have the drones out there,
6634640	6639600	we don't have the humanoid robots at control, and hypnotized humans are a very tiny subset of humans,
6639600	6644400	probably. And even if they aren't, that just looks like a religion or a cult or a terrorist
6644400	6648880	cell, and we know how to deal with that as well. The super intelligent AI with, you know,
6648880	6654320	lots of robots at control in a starcraft form, I would agree, is something that humans haven't
6654320	6659520	faced yet. But by the time we get that many robots out there, you won't be able to control all of
6659520	6664480	them at once because of the private key things I mentioned. So that's why I'm like, okay,
6664480	6668080	everything else we're talking about is in normal world. That is the single biggest thing
6668080	6673520	that I wanted to get, like economic disruption, people losing jobs, proliferation so that the
6673520	6678480	balance of power is redistributed, all that is fine. The reason I say this is people keep trying
6678480	6682880	to link AI to existential risk. A great example is one of the things you actually had in here,
6682880	6686720	this is similar to the AI policy and so on. It's a totally reasonable question, but then I'm going
6686720	6690480	to, in my view, deconstruct the question. What would you think about putting the limit on the
6690480	6693600	right to compute or their capabilities and AI system might demonstrate that we make you think
6693600	6697360	open access no longer wise? The most common near term answer here to be seems to be related to
6697360	6702000	risk of pandemic via novel pathogen engineering. So guess what? You know who the novel pathogen
6702080	6708240	engineers are? The US and Chinese governments, right? They did it, or probably did it, credibly
6708240	6712400	did it, credibly mean accused of doing it. They haven't been punished for COVID-19. In fact,
6712400	6716480	they covered up their culpability and pointed everywhere other than themselves. They used it
6716480	6722480	to gain more power in both the US and China with both lockdown in China and in the US and all kinds
6722480	6728240	of COVID era, trillions of dollars was printed and spent and so on and so forth. They did everything
6728240	6733120	other than actually solve the problem. That was actually getting the vaccines in the private
6733120	6737600	sector and they studied the existential risk only to generate it and they're even paid to
6737600	6742560	generate pandemic prevention and failed. So this would be the ultimate Fox guarding the henhouse.
6745120	6748400	The two organizations responsible for killing millions of people with novel pathogen are going
6748400	6755200	to prevent people from doing this by restricting compute. No, you know what it is actually. What's
6755200	6761360	happening here is one of the concepts I have in the network state is this idea of God, state,
6761360	6765600	and network. Meaning, what do you think is the most powerful force in the world? Is it Almighty
6765600	6775920	God? Is it the US government? Or is it encryption? Or eventually maybe an AGI? What's happening here
6775920	6782240	is a lot of people are implicitly, without realizing it, even if they are secular atheists,
6782240	6788080	they're treating GOV as GOD. They treat the US government as God as the final mover.
6788800	6792960	No, I appreciate your little, I take inspiration from you actually in terms of
6792960	6799200	trying to come up with these little quips that are memorable. So I was just smiling at that
6799200	6805760	because I think you do a great job of that and I try to encourage, I have less success
6805760	6810640	coining terms than you have, but certainly try to follow your example on that front.
6810720	6814960	It's like a helpful, if you can compress it down, it's like more memorable. So that's what I try
6814960	6820240	to do, right? So exactly, a lot of these people who are secular, think of themselves as atheists,
6820240	6825760	have just replaced GOD with GOV. They worship the US government as God. And there's two versions
6825760	6830880	of this. You know how like God has both the male and female version, right? The female version is
6830880	6835680	the Democrat God within the USA that has infinite money and can take care of everybody and care
6835680	6840080	for everybody. And the Republican God is the US military that can blow up anybody and it's the
6840080	6847840	biggest and strongest and most powerful America F. Yeah. Okay. And everybody who thinks of the US
6847840	6854800	government as being able to stop something is praying to a dead God. Okay, when you say this,
6854800	6859200	you actually get an interesting reaction from AI safety people where you've actually hit their
6859200	6866080	true solar plexus. All right, the true solar plexus is not that they believe in AI, it's that
6866080	6872560	they believe in the US government. That's a true solar plexus because they are appealing to their
6872560	6878080	praying to this dead God that can't even clean the poop off the streets in San Francisco, right?
6878080	6883920	That is losing wars or fighting them to sell me. It says lost all these wars around the world
6883920	6888560	that spent trillions of dollars has been through financial crisis, Coronavirus, Iraq war, you
6888560	6893200	know, total meltdown politically. Okay, that is interest that is now has interest payments more
6893200	6899520	than the defense budget. That is, you know, that spent $100 billion on the California train without
6899520	6904720	laying a single track. It's like that, you know, that Morgan Freeman thing for you know, the clip
6904720	6910000	from Batman, who is like, So this man has a billionaire, blah, blah, blah, this and that,
6910000	6914960	and your plan is to threaten him, right? And so you're going to create this super intelligence
6914960	6921280	and have Kamala Harris regulate it. Come on, man, so to speak, right? Like these people are praying
6921280	6930000	to a blind, deaf and dumb God that was powerful in 1945, right? That's why, by the way, all the
6930000	6936240	popular movies, what are they? It's Barbie, it's Oppenheimer, right? It's, it's top gun. They're
6936240	6943040	all throwbacks the 80s or the 50s when the USA was really big and strong. And the future is a
6943040	6948080	black mirror. Yeah, I think that's tragic. One of the projects that I do like, and you might appreciate
6948080	6954800	this, I don't know if you've seen it, is the From the Future of Life Institute, a project called
6955600	6961840	Imagine a World, I think is the name of it. And they basically challenged, you know, their
6962560	6970960	audience and the public to come up with positive visions of a future, you know, where technology
6970960	6976720	changes a lot. And obviously, I pretty central to a lot of those stories. And, you know, one of the
6976720	6982880	challenges that people go through and how do we get there and whatever, but a purposeful effort
6982880	6992080	to imagine positive futures super under provided. And I really liked the investment that they made
6992080	6996720	in that. You know, one of the things I've got in the Never See It book is there's certain megatrends
6996720	7002880	that are happening, right? And megatrends, I mean, it's possible for like one miraculous human maybe
7002880	7008000	to reverse them, okay? Because I think both the impersonal force of history theory and the
7008000	7014080	great man theory of history have some truth to them. But the megatrends are the decline of Washington,
7014080	7020480	DC, the rise of the internet, the rise of India, the rise of China. That is like my worldview. And
7020480	7025680	I can give a thousand graphs and charts and so on for that. But that's basically the last 30 years.
7026320	7030240	And maybe the next X, right? I'm not saying there can't be trend reversal. Of course, it can be
7030240	7033680	trend reversal, as I just mentioned, some hammer blow could hit it, but that's what's happening.
7034320	7038800	And so because of that, the people who are optimistic about the future are aligned with either the
7038800	7044960	internet, India or China. And the people who are not optimistic about the future are blue Americans
7044960	7051120	or left out red Americans, okay, or Westerners in general, who are not tech tech people. Okay,
7051120	7056160	if they're not tech people, they're not up into the right, basically, because the internet's if you
7056160	7060640	I mean, one of the things is we have a misnomer, as I was saying earlier, calling it the United
7060640	7064800	States, because the dis United States, it's, it's like talking about, you know, talking about
7064800	7068080	America is like talking about Korea, there's North Korea and South Korea, they're totally different
7068080	7074400	populations. And, you know, communism and capitalism are totally different systems. And the thing that
7074400	7079120	is good for one is bad for another and vice versa. And so like America doesn't exist, there's only
7079120	7082960	just like there's no Korea, there's only North Korea and South Korea, there's no America, there's
7082960	7089920	blue America and red America and also gray America, tech America. And blue America is harmed, or they
7089920	7094480	think they're harmed, or they've gotten themselves into a spot where they're harmed by every technological
7094480	7100240	development, which is why they hate it so much, right? AI versus journalist jobs, crypto takes away
7100240	7104800	banking jobs, you know, everything, you know, self driving cars, they just take away regulator
7104800	7109600	control, right? Anything that reduces their power, they hate, and they're just trying to freeze an
7109600	7115120	amber with regulations. Red America got crushed a long time ago by offshoring to China and so on,
7115120	7120320	they're, they're making, you know, inroads ally with tech America or gray America. Tech America
7120320	7124320	is like the one piece of America that's actually still functional and globally competitive. And
7124320	7129280	people always do this fallacy of aggregation, where they talk about the USA. And it's really
7129280	7133440	this component that's up into the right, and the others that are down into the right, red,
7133440	7138160	best flat, like red, but they're like down, right? Like, red is like, okay, functional, blue is down.
7138160	7144000	The point is, tech America, I think we're going to find is not even truly or
7146080	7151760	how American is tech America, because it's like 50% immigrants, right? And like a lot of children
7151760	7157840	immigrants, and most of their customers are overseas, and their users are overseas. And
7157840	7164480	their vantage point is global, right? And they're basically not, I know we're in this
7164480	7169600	ultra nationalist kick right now. And I know that there's going to be, there's a degree of a fork
7169600	7177760	here, where you fork technology into Silicon Valley and the internet. Okay, where Silicon
7177760	7182000	Valley is American, and they'll be making like American military equipment and so on and so
7182000	7187520	forth, and they're signaling USA, which is fine. Okay. And then the internet is international,
7187520	7194080	global capitalism. And the difference is Silicon Valley, or let's say US tech, let me less, you
7194080	7199200	know, US tech says ban tick tock, build military equipment, etc. It's really identifying itself
7199200	7204800	as American. And it's thinking of being anti China. Okay, but there's US and China are only 20% of
7204800	7210720	the world, 80% of the world is neither American nor Chinese. So the internet is for everybody else
7210720	7217040	who wants actual global rule of law, right? When as a US decays as a rule space order, and people
7217040	7222000	don't want to be under China, people want to be under something like blockchains, where you've got
7222000	7227360	like property rights, contract law, cross borders that are enforced by an impartial authority. Okay,
7227360	7231440	that's also the kind of laws that can bind AI's like AI's across borders, if you want to make
7231440	7235600	sure they're going to do something, cryptography can bind an AI in such a way that it can't fake
7235600	7241120	it. It can't an AI can't mint more Bitcoin, you know, my here's my last question for you. AI discourse
7241120	7247040	right now does seem to be polarizing into camps. Obviously, a big way that you think about the world
7247040	7251840	is by trying to figure out, you know, what are the different camps? How do they relate to each other
7252000	7260880	so on and so forth? I have the view that AI is so weird. And so unlike other things that we've
7260880	7265360	encountered in the past, including just like, unlike humans, right, I always say AI, alien
7265360	7271680	intelligence, that I feel like it's really important to to borrow a phrase from Paul Graham,
7271680	7280240	keep our identities small, and try to have a scout mindset to really just take things on their
7280240	7284240	own terms, right? And not necessarily put them through a prism of like, who's team am I on? Or,
7284240	7290480	you know, is this benefit my team or hurt the other team or whatever. But you know, just try to be as
7290480	7297200	kind of directly engaged with the things themselves as we can without mediating it through all these
7297200	7303680	lenses. You know, I think about you mentioned like, the gain of function, right? And I don't know
7303680	7307840	for sure what happened, but it certainly does seem like there's a very significant chance
7307920	7313120	that it was a lab leak. Certainly there's a long history of lab leaks. But it would be like, you
7313120	7319840	know, it would seem to me a failure to say, okay, well, what's the what's the opposite of just having
7319840	7323840	like a couple of government labs, like, everybody gets their own gain of function lab, right? Like,
7323840	7327680	if we could, and this is kind of what we're doing with AI, we're like, let's compress this power down
7327680	7332400	to as small as we can. Let's make a kit that can run in everybody's home. Would we want to
7333120	7338880	send out these like gain of function, you know, wet lab research kits to like every home in the
7338880	7343600	world and be like, hope you find something interesting, you know, like, let us know if you
7343600	7348560	find any new pathogens or hey, maybe you'll find life saving drugs, like whatever, we'll see what
7348560	7353520	you find, you know, all eight billion of you. That to me seems like it would be definitely a
7353520	7359920	big misstep. And that's the kind of thing that I see coming out of ideologically
7360880	7365120	motivated reasoning, or like, you know, tribal reasoning. And so I guess,
7365120	7372080	I wonder how you think about the role that tribalism and ideology is playing and should
7372080	7378160	or shouldn't play as we try to understand AI. Okay, so first is, you're absolutely right,
7378160	7385440	that just because a is bad does not mean that B is good, right? So a could be a bad option,
7385440	7390800	B could be a bad option, C could be a bad option. There might be, you have to go down to option G
7390800	7394000	before you find a good option, or there might be three good options and seven bad options,
7394000	7400880	for example, right? So to map that here, in my view, an extremely bad option is to ask the US
7400880	7407040	and Chinese governments to do something. Anything the US government does at the federal level,
7407040	7412560	at the state level in blue states at the city level has been a failure. And the way that here's
7412720	7416640	here's a metaway of thinking about it, you invest in companies, right? So as an investor,
7416640	7421520	here's a really important thing. You might have 10 people who come to you with the same words in
7421520	7426880	their pitch. They're all, for example, building social networks. But one of them is Facebook,
7426880	7431440	and the others are Friendster and whatever. Okay, and no offense to Friendster, you know,
7431440	7436400	that those guys were like, you know, pioneers in their own way. But they just got outmatched by
7436400	7441920	Facebook. So point is that the words were the same on each of these packages, but the execution was
7442000	7449840	completely different. So could I imagine a highly competent government that could execute and that
7449840	7455680	actually did, you know, like, you know, make the right balance of things and so on? I can't say
7455680	7463760	it's impossible, but I can't say that it wouldn't be this government. Okay, and so you are talking
7463760	7468640	about the words and I'm talking about the substance. The words are, we will protect you from AI,
7468640	7472880	right? In my view, the substances, they aren't protecting you from anything, right? You're basically
7472880	7477280	giving money and power to a completely incompetent and in fact, malicious organization, which is
7477280	7482960	Washington DC, which is the US government that has basically over the last 30 years, gone from a
7482960	7487600	hyperpower that wins everywhere without fighting to a declining power that fights everywhere without
7487600	7493840	winning. Okay, like just literally burn trillions of dollars doing this, take maybe the greatest
7493840	7498480	decline in fortunes in 30 years and maybe human history, not even the Roman Empire went down this
7498560	7505440	fast on this many power dimensions this quickly, right? So giving that guy, let's trust him,
7505440	7509760	that's just people running an old script in their heads that they inherited. They are not
7509760	7515440	thinking about it from first principles that this state is a failure. Okay, and like how much
7515440	7518800	of a failure it is, you have to look at sovereign debt crisis, look at graphs that other people
7518800	7525360	aren't looking at, but like, you know, the domain of what Blue America can regulate is already
7525440	7530640	collapsing because it can't regulate Russia anymore. It can't regulate China anymore.
7530640	7535120	It's less able to regulate India. It's less able even to regulate Florida and Texas.
7535120	7538960	States are breaking away from it domestically. So this gets to your other point. Why is the
7538960	7545200	tribal lens not something that we can put in the back? We have to put in the absolute front
7545200	7552160	because the world is retribalizing. Like basically your tribe determines what law you're bound by.
7552160	7556960	If you think you can pass some policy that binds the whole world, well, there have to be guys with
7556960	7562000	guns who enforce that policy. And if I have guys with guns on their side that say we're not enforcing
7562000	7566000	their policy, then you have no policy. You've only bound your own people. Does that make sense,
7566000	7573200	right? And so Blue America will probably succeed in choking the life out of AI within Blue America.
7573200	7578320	But Blue America controls less and less of the world. So it'll have more power or a fewer people.
7579280	7585760	I can go into why this is, but essentially a financial Berlin Wall is arising. There's a lot
7585760	7591520	of taxation and regulation and effectively financial repression, de facto confiscation,
7591520	7595440	that will have to happen for the level of debt service that the US has been taking on.
7596560	7601520	There's one graph just to make the point. And if you want to dig into this, you can.
7602400	7607440	But the reason this impacts things is when you're talking about AI safety, you're talking about
7607440	7612480	AI regulation, you're talking about the US government, right? And you have to ask, what does
7612480	7618640	that actually mean? And it's like, in my view, it's like asking the Soviet Union in 1989 to
7618640	7623520	regulate the internet, right? That's going to outlive, you know, the country. US interest
7623520	7627760	payment on federal debt versus defense spending. The white line is defense spending. Look at the
7627760	7632880	red line. That's just gone absolutely vertical. That's interest. And it's going to go more vertical
7632880	7638400	next year because all of this debt is getting refinanced at much higher interest rates. That's
7638400	7644000	why look at this, you want AI timelines, right? The question for me is DC's timeline.
7644560	7652000	What is DC's time left to live? Okay, this is the kind of thing that kills empires. And you either
7652000	7657520	have this just go to the absolute moon, or they cut rates and they print a lot. And either way,
7658080	7663440	you know, the fundamental assumption underpinning all the AI safety, all the AI regulation work
7663440	7668640	is that they have a functional golem in Washington DC, where if they convince it to do something,
7668640	7673120	it has enough power to control enough of the world. When that assumption is broken,
7674400	7679760	then a lot of assumptions are broken, right? And so in my view, you have to,
7681040	7686960	you must think about a polytheistic AI world, because other tribes are already into this,
7687040	7691120	they're already funding their own, right? The proliferation is already happening.
7691840	7696640	And they're not going to bow to blue tribe. So that's why I think the tribal lens is not
7696640	7701840	secondary. It's not some, you know, totally separate thing. It is an absolutely primary
7701840	7706080	way in which to look at this. And in a sense, it's almost like a, you know, in a well done
7706800	7714960	movie, all the plot lines come together at the end. Okay. And all the disruptions that are happening,
7714960	7720640	the China disruption, the rise of India, the rise of the internet, the rise of crypto,
7720640	7724960	the rise of AI, and the decline of DC, and the internal political conflict,
7725520	7729920	and, you know, various other theaters like what's happening in Europe and, you know, and Middle
7729920	7735840	East, all of those come together into a crescendo of, ah, there's a lot of those graphs are all
7735840	7740400	having the same time. And it's not something you can analyze by just, I think, looking at one of
7740400	7744880	these curves on itself. I think that's a great note to wrap on. I am always lamenting the fact
7745120	7752240	that so many people are thinking about this AI moment in just fundamentally too small
7752240	7758560	of terms. But I don't think you're one that will easily be accused of that. So with
7759600	7764480	an invitation to come back and continue in the not too distant future, for now, I will say
7764480	7768320	Balaji Srinivasan, thank you for being part of the Cognitive Revolution.
7769360	7771040	Thank you, Nathan. Good to be here.
