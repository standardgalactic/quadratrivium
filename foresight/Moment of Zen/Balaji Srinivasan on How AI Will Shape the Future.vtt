WEBVTT

00:00.000 --> 00:09.760
This week's Moment of Zen is a feed drop for one of Turpentine's biggest shows, The

00:09.760 --> 00:13.280
Cognitive Revolution, hosted by Nathan LeBenz.

00:13.280 --> 00:17.520
Biology and Nathan discussed the evolution, challenges, and role of AI in different realms

00:17.520 --> 00:21.200
like politics, environmentalism, medicine, and more.

00:21.200 --> 00:24.640
There's a lot of new content here about where biology thinks AI is going and interesting

00:24.640 --> 00:26.800
parallels with religion.

00:26.800 --> 00:30.680
Nathan brings up some AI safety concerns in the course of the discussion.

00:30.680 --> 00:34.920
If you like what you hear, check out our other Moment of Zen episodes with Boloji and check

00:34.920 --> 00:37.200
out The Cognitive Revolution.

00:37.200 --> 00:39.200
Please enjoy.

00:39.200 --> 00:42.720
Boloji Srinivasan, welcome to The Cognitive Revolution.

00:42.720 --> 00:43.720
All right.

00:43.720 --> 00:44.720
I feel welcome.

00:44.720 --> 00:49.020
Well, we've got a ton to talk about, you know, obviously you bring a lot of different

00:49.020 --> 00:53.920
perspectives to everything that you think about and work on.

00:53.920 --> 00:58.720
And today I want to just try to muster all those different perspectives onto this, you

00:58.720 --> 01:02.920
know, what I see is really the defining question of our time, which is like, what's up with

01:02.920 --> 01:06.400
AI and, you know, how's it going to turn out?

01:06.400 --> 01:10.200
I thought maybe for starters, I would love to just get your baseline kind of table setting

01:10.200 --> 01:17.920
on how much more AI progress do you expect us to see over the next few years, like how

01:17.920 --> 01:23.600
powerful our AI system is going to become in, again, kind of a relatively short timeline.

01:23.600 --> 01:27.040
And then maybe if you want to take, you know, a bigger stab at it, you could answer that

01:27.040 --> 01:30.240
same question for a longer timeline, like the rest of our lives or whatever.

01:30.240 --> 01:31.240
Sure.

01:31.240 --> 01:32.240
Let me give an abstract answer.

01:32.240 --> 01:34.360
Then let me give a technical answer.

01:34.360 --> 01:40.120
You know, if you look at evolution, we've seen something as complex as flight evolved

01:40.120 --> 01:44.520
independently in birds, bats, and bees.

01:44.520 --> 01:53.320
And even intelligence, we've seen fairly high intelligence in dolphins, in whales, in octopuses,

01:54.280 --> 01:56.760
you know, octopus in particular can do like tool manipulation.

01:56.760 --> 01:59.760
They've got things that are a lot like hands, you know, with tentacles.

01:59.760 --> 02:07.520
And so that indicates that it is plausible that you could have multiple pathways to intelligence,

02:07.520 --> 02:11.440
whether, you know, we have carbon based intelligence, or we could have silicon based intelligence

02:11.440 --> 02:14.440
that just has a totally different form or the fundamental thing is an electromagnetic

02:14.440 --> 02:19.080
wave and data storage as opposed to, you know, DNA and so on, right?

02:19.080 --> 02:23.160
So that's like a plausibility argument in terms of evolution is being so resourceful

02:23.160 --> 02:28.200
that it's invented really complicated things in different ways, okay?

02:28.200 --> 02:32.280
Then in terms of the technical point, I think as of like right now, I should probably date

02:32.280 --> 02:36.640
it as like December 11, 2023, because this field moves so fast, right?

02:36.640 --> 02:41.440
My view is, and maybe you'll have a different view, is that the breakthroughs that are really

02:41.440 --> 02:47.840
needed for something that's like true artificial intelligence that is human independent, right?

02:47.840 --> 02:51.880
Maybe the next step after the Turing test, I've got an article that, you know, we're

02:51.920 --> 02:56.520
writing called the Turing thresholds, which tries to generalize the Turing test to like

02:56.520 --> 02:58.920
the cartage of scale, you know, have you got energy thresholds?

02:58.920 --> 03:01.480
Like what are useful scales beyond that?

03:01.480 --> 03:08.040
And right now, I think that what we call AI is absolutely amazing for environments that

03:08.040 --> 03:11.560
are not time varying or rule varying.

03:11.560 --> 03:16.520
And what I mean by that is, so you kind of have, let's say two large schools of AI, and

03:16.520 --> 03:20.200
obviously there's overlap in terms of the personnel and so on, but there's like the

03:20.200 --> 03:24.000
deep mind school, which has gotten less press recently, but got more press, you know, a

03:24.000 --> 03:27.560
few years ago, and that is game playing, right?

03:27.560 --> 03:31.880
It is, you know, superhuman playing of go without go.

03:32.160 --> 03:36.360
It is, you know, all the video game stuff they've done where they learn at the pixel

03:36.360 --> 03:39.520
level and they don't, they just teach the very basic rules and it figures it out from

03:39.520 --> 03:43.400
there. And it's also, you know, the protein folding stuff and what have you, right?

03:44.240 --> 03:48.000
But in general, I think they're known for reinforcement learning and those kinds of

03:48.040 --> 03:50.360
approaches. I mean, they're good at a lot of things, but that's what I think he finds

03:50.360 --> 03:54.560
known for. Of course, they put out this new model recently, the Gemini model.

03:54.560 --> 03:57.440
So I'm not saying that they're not good at everything, but that's just kind of what

03:57.440 --> 03:58.840
they're maybe most known for.

03:58.840 --> 04:04.000
And then you have the open AI chat, GBT school of generative AI, and it includes stable

04:04.000 --> 04:08.520
diffusion and just as a pioneer, even if, you know, they're not, I don't know how much

04:08.520 --> 04:12.480
they're used right now, but basically, you know, you have the diffusion models for images

04:12.480 --> 04:16.560
and you have large language models and now you have the multimodals that integrate them.

04:16.680 --> 04:22.400
And so the difference, I think with these is the reinforcement learning approaches are

04:22.400 --> 04:27.680
based on an assumption of static rules, like the rules of chess, the rules that go, the

04:27.680 --> 04:30.840
rules of a video game are not changing with time, they're discoverable, they're like the

04:30.840 --> 04:37.760
laws of physics. And similarly, like the body of language where you're learning it, English

04:37.760 --> 04:42.920
is not rapidly time varying. That is to say, the rules of grammar that are implicit aren't

04:42.960 --> 04:47.040
changing, the meanings of words aren't changing very rapidly, you can argue they're changing

04:47.040 --> 04:52.000
over the span of decades or centuries, but not extremely rapidly, right? So therefore,

04:52.000 --> 04:57.040
when you generate a new result, training data from five years ago for English is actually

04:57.040 --> 05:01.840
still fairly valuable, and the same input roughly gives the same output. Now, of course,

05:01.840 --> 05:06.920
there are facts that change with time, like who is the the ruler of England, right, the

05:06.920 --> 05:09.920
Queen of England is passed away now, it's the King of England, right, which is facts

05:09.920 --> 05:13.920
that change with time. But I think more fundamentally is when there's rules that change with time,

05:13.920 --> 05:19.400
you know, you have, for example, changes in law and countries, right? But most interestingly,

05:19.400 --> 05:24.080
perhaps changes in markets, because the same input does not give the same output in a market.

05:24.080 --> 05:28.200
If you try that, then what will happen is there's adversarial behavior on the other side. And

05:28.200 --> 05:31.360
once people see it enough times, they'll see your strategy, and they're going to trade

05:31.360 --> 05:35.560
against you on that, right? And I can get to other technical examples on that, but I think,

05:35.560 --> 05:39.320
and probably people in the space are aware of this, but I think that is a true frontier

05:39.360 --> 05:45.240
is dealing with time varying rule varying systems, as opposed to systems where the implicit rules

05:45.240 --> 05:49.600
are static. Let me pause there. Yeah, I think that makes sense. I think the, you know, in the

05:49.600 --> 05:56.600
very practical, you know, just trying to get as V calls it mundane utility from AI, that is often

05:56.600 --> 06:01.640
kind of cashed out to AI is good at tasks, but it's not good at whole jobs. You know, it can

06:01.640 --> 06:06.240
handle these kind of small things where you can define, you know, what good looks like and tell

06:06.280 --> 06:13.040
it exactly what to do. But in the sort of broader context of, you know, handling things that come

06:13.040 --> 06:20.320
up as they come up, it's definitely not there yet. And I agree that there's likely to be some

06:20.320 --> 06:26.480
synthesis, you know, which is kind of the subject of all the Q star rumors. Recently, I would say

06:26.480 --> 06:33.760
is kind of the, the prospect that there could be already, you know, within the labs, a beginning

06:33.800 --> 06:39.280
of a synthesis between the, I kind of think of it as like harder edged reinforcement learning

06:39.280 --> 06:45.360
systems, you know, that are like small, efficient and deadly versus the like language model systems

06:45.360 --> 06:51.360
that are like kind of slow and soft and, you know, but have a sense of our values, which is really

06:51.360 --> 06:56.160
a remarkable accomplishment that that they're able to have even, you know, an approximation of our

06:56.160 --> 07:04.960
values that seems like reasonably good. So yeah, I think I agree with that framing. But I guess I

07:04.960 --> 07:12.720
would, you know, still wonder like, how far do you think this goes in the near term? Because I have

07:12.720 --> 07:16.160
a lot of uncertainty about that. And I think the field has a lot of uncertainty. You hear people

07:16.160 --> 07:20.080
say, Well, you know, it's never going to get smarter than its training data, you know, it'll

07:20.080 --> 07:24.000
kind of level out where humans are. But we certainly don't see that in the reinforcement

07:24.000 --> 07:30.960
learning side, right? Like once it usually don't take too long at human level of these games,

07:30.960 --> 07:34.720
and then it like blows past human level. Interestingly, you do still see some adversarial

07:34.720 --> 07:40.560
vulnerability, like there's a great paper from the team at FAR AI, and I'm planning to have

07:41.280 --> 07:44.800
Adam Gleave, the head of that organization on soon to talk about that and other things,

07:44.800 --> 07:52.240
where they found like a basically a hack where a really simple, but unexpected attack on the

07:52.240 --> 07:57.600
superhuman go player can defeat it. So you do have these like very interesting vulnerabilities

07:57.600 --> 08:02.960
or kind of lack of adversarial robustness, still kind of wondering like, where do you think that

08:02.960 --> 08:07.920
leaves us in say a three to five years time? Obviously, huge uncertainty on that. It's really

08:07.920 --> 08:13.840
hard to predict something like this. Just to your point, generative AI is generic AI, right?

08:13.840 --> 08:18.880
It's like generically smart, but doesn't have specific intelligence or creativity or facts.

08:18.880 --> 08:22.720
And as you're saying, just like we have, you know, adversarial images

08:22.720 --> 08:28.800
back in full programs that are trained on a certain set of data, and they just give some

08:28.800 --> 08:33.120
weird, you know, pattern that looks like a giraffe, but the algorithm thinks it's a dog,

08:33.680 --> 08:37.360
you can do the same thing for game playing, and you can have out of sample input that can beat,

08:38.480 --> 08:42.000
you know, these very sophisticated reinforcement learners.

08:43.360 --> 08:48.240
And an interesting question is whether that is a fundamental thing, or whether it is a

08:49.760 --> 08:54.000
work aroundable thing. And you'd think it was work aroundable, you know,

08:54.880 --> 08:59.600
because there's probably some robustification because these pictures look like giraffes,

08:59.600 --> 09:05.120
you know, and yet they're being recognized as dogs. See, there's, you would think that

09:05.760 --> 09:11.360
the right proximity metric would group it with giraffes, you know, but maybe there's some,

09:12.000 --> 09:16.400
I don't know, maybe there's some result there. My intuition would be we can probably

09:17.280 --> 09:21.760
robustify these systems so that they are less vulnerable to adversarial input.

09:23.280 --> 09:26.320
But if we can't, then that leads us in a totally different direction,

09:26.320 --> 09:29.280
where these systems are fragile in a fundamental way.

09:30.160 --> 09:35.680
So that's one big branch point is how fragile these systems are, because if they're fragile in

09:35.680 --> 09:41.360
a certain way, then it's almost like you can always kill them, which is kind of good, right,

09:41.360 --> 09:47.280
in a sense, that there's that, you know, almost like the, you know, the 50 IQ, 100 IQ, 150 IQ

09:47.280 --> 09:53.280
thing, like the, the meme. Yeah, the meme, right. So the 50 IQ guys like these machines will never

09:53.280 --> 09:58.560
be as creative as humans or whatever. 100 IQ is look at all the things they can do. The 150 IQ

09:58.560 --> 10:03.680
is like, well, there's some like equivalent equivalent result, you know, that's like some

10:03.680 --> 10:08.000
impossibility proof that shows that we the dimensional space of a giraffe is too high,

10:08.000 --> 10:11.600
and we can't actually learn what a true giraffe, I don't think that's true.

10:12.240 --> 10:16.400
But maybe it's true from the perspective of how these learners are working,

10:16.400 --> 10:20.080
because my understanding is people have been trying, and I mean, I'm not the cutting edge of

10:20.080 --> 10:24.080
this. So, you know, maybe something, but my understanding is we haven't yet been able to

10:24.080 --> 10:29.040
robustify these models against adversarial input. Am I wrong about that?

10:29.680 --> 10:30.720
Yeah, that's definitely right.

10:31.280 --> 10:34.080
We'll continue our interview in a moment after a word from our sponsors.

10:34.400 --> 11:02.480
There's no single architecture, as far as I know, that is demonstrably robust. And on the contrary,

11:02.480 --> 11:06.720
you know, even with language models, there's a, we did a whole episode on the universal jailbreak,

11:07.280 --> 11:11.920
where, especially if you have access to the weights, not to change the weights, but just to

11:11.920 --> 11:17.600
kind of probe around in the weights, then you have a really hard time, you know, guaranteeing

11:17.600 --> 11:24.480
any sort of robustness. The conjecture is, see, for humans, you can't like, mirror their brain

11:24.480 --> 11:30.160
and analyze it. Okay, but we have enough humans that we've got things like optical illusions,

11:30.240 --> 11:35.680
stuff like that, that works on enough humans, and our brains aren't changing enough, right?

11:36.240 --> 11:42.720
A conjecture is, if you had, as you said, open weights, open weights mean safety, because if you

11:42.720 --> 11:47.920
have open weights, you can always reverse engineer adversarial input, and then you can always break

11:47.920 --> 11:54.080
the system. Conjecture. Yeah, there's I, that's again, with Adam from far AI, I'm really interested

11:54.080 --> 11:59.520
to get into that, because they are starting to study, as I understand it, kind of proto scaling

11:59.520 --> 12:06.640
laws for adversarial robustness. And I think a huge question there is, what are the kind of

12:07.680 --> 12:12.800
frontiers of possibility there? Like, do you need, you know, how do the orders of magnitude work,

12:12.800 --> 12:18.480
right? Do you need another 10x as much adversarial training to half the rate of your

12:19.520 --> 12:24.560
adversarial failures? And if so, you know, can we generate that many, it may always sort of

12:24.640 --> 12:31.360
be fleeting. So far AI, and they are, they're working on cutting edge of adversarial input.

12:31.920 --> 12:37.680
Yeah, they're the group that did the attack on the Alpha Go model, and found that like, you know,

12:37.680 --> 12:41.120
and what was really interesting about that, I mean, multiple things, right? First, that they could

12:41.120 --> 12:45.680
beat a super human Go player at all. But second, that the technique that they used would not work

12:45.680 --> 12:51.040
at all if playing a quality human. Or is, you know, it's a strategy that is trivial to beat if

12:51.040 --> 12:56.080
you're a quality human Go player. But the Alpha Go is just totally blind to it.

12:56.080 --> 12:59.440
You know, that's why I say the conjecture is, if you have the model,

13:00.800 --> 13:06.000
then you can generate the adversarial input. And then so if that is true, and that itself is

13:06.000 --> 13:13.360
an important conjecture about AI safety, right? Because if open weights are inherently something

13:13.360 --> 13:18.000
where you can generate adversarial input from that and break or crash or defeat the AI,

13:18.240 --> 13:25.440
then that AI is not omnipotent, right? You have some power words, you can speak to it, almost

13:25.440 --> 13:32.560
like magical words, that'll just make it, power down, so to speak, right? It's like those movies

13:32.560 --> 13:37.680
where the monsters can't see you if you stand really still, or if you, you don't make a noise or

13:37.680 --> 13:41.920
something like that, right? They're very powerful on Dimension X, but they're very weak on Dimension

13:41.920 --> 13:46.560
1. A kind of an obvious point, but you know, I'm not sure how important it's going to be in the

13:46.560 --> 13:52.240
future. Your next question was on like, you know, humanoid robots and so on. And before we get to

13:52.240 --> 13:58.960
that, maybe obviously, but all of these models are trained on things that we can easily record,

13:58.960 --> 14:08.880
which are sights and sounds, right? But touch and taste and smell, we don't have amazing datasets

14:08.880 --> 14:14.800
on those. Well, I mean, there's some haptic stuff, right? There's, there's probably some,

14:14.800 --> 14:19.920
you know, some work on taste and smell and so on. But those, there's five senses, right? I wonder if

14:19.920 --> 14:26.080
there's something like that where you might be like, okay, how are you going to out smell, you know,

14:26.080 --> 14:30.000
a robot or something like that? Well, dogs actually have a very powerful sense of smell,

14:30.000 --> 14:34.240
and that's being very important for them, you know? And it may turn out that there's,

14:34.240 --> 14:37.440
maybe it's just that we just haven't collected the data, and it could become a much better

14:37.440 --> 14:41.200
smeller or whatever, or, you know, taster than anything else. I wouldn't be surprised. It could

14:41.200 --> 14:46.800
be a much better wine taster because you can do molecular diagnostics. But it's just kind of,

14:46.800 --> 14:50.640
I just use that as an analogy to say there's areas of the human experience that we haven't

14:50.640 --> 14:55.760
yet quantified. And maybe it's just the opera term is yet, okay? But there's areas of the

14:55.760 --> 15:00.640
human experience we haven't yet quantified, which are also an area that AIs at least are not yet

15:00.640 --> 15:09.360
capable at. Yeah, I guess maybe my expectation boils down to, I think the really powerful systems

15:09.360 --> 15:16.000
are probably likely to mix architectures in some sort of ensemble, you know, when you think about

15:16.000 --> 15:20.960
just the structure of the brain, it's not, I mean, there certainly are aspects of it that are repeated,

15:20.960 --> 15:26.320
right? You look at the frontal cortex, and it's like, there is kind of this, you know, unit that

15:26.320 --> 15:30.800
gets repeated over and over again, in a sense, that's kind of analogous to say the transformer block

15:30.800 --> 15:35.520
that just gets, you know, stacked layer on layer. But it is striking in a transformer that it's

15:35.520 --> 15:40.960
basically the same exact mechanism at every layer that's doing kind of all the different kinds of

15:40.960 --> 15:46.080
processing. And so whatever weaknesses that structure has, and you know, with the transformer

15:46.080 --> 15:50.560
and the attention mechanism, there's like some pretty profound ones like finite context window,

15:51.120 --> 15:55.840
you know, you kind of need, I would think a different sort of architecture with a little bit

15:55.840 --> 16:03.040
of a different strength and weakness profile to complement that in such a way that, you know,

16:03.040 --> 16:07.600
kind of more similar to like a biological system where you kind of have this like dynamic feedback

16:07.600 --> 16:11.360
where, you know, if we have obviously, you know, thinking fast and slow and all sorts of different

16:11.360 --> 16:18.320
modules in the brain, and they kind of cross regulate each other and don't let any one system,

16:18.320 --> 16:23.200
you know, go totally, you know, down the wrong path on its own, right, without something kind of

16:23.200 --> 16:27.760
coming back and trying to override that. It seems to me like that's a big part of what is missing

16:27.760 --> 16:34.880
from the current crop of AIs in terms of their robustness. And I don't know how long that takes

16:34.880 --> 16:41.600
to show up. But we are starting to see some, you know, possibly, you know, I think people are maybe

16:41.600 --> 16:44.800
thinking about this a little bit the wrong way. They're just in the last couple of weeks, there's

16:44.800 --> 16:51.280
been a number of papers that are really looking at the state space model, kind of alternative,

16:51.280 --> 16:55.600
it's being framed as an alternative to the transformer. But when I see that, I'm much more

16:55.600 --> 17:01.040
like, it's probably a compliment to the transformer or, you know, these two things probably get

17:01.040 --> 17:04.960
integrated in some form, because to the degree that they do have very different strengths and

17:04.960 --> 17:09.440
weaknesses, ultimately, you're going to want the best of both in a robust system, certainly if you're

17:09.440 --> 17:13.120
trying to make an agent, certainly if you're trying to make, you know, a humanoid robot that can go

17:13.120 --> 17:18.640
around your house and like do useful work, but also be robust enough that it doesn't, you get tricked

17:18.640 --> 17:23.040
into attacking your kid or your dog or, you know, whatever, you're going to want to have more checks

17:23.040 --> 17:28.880
and balances than just kind of a single stack of, you know, the same block over and over again.

17:28.880 --> 17:34.640
Well, so I know Boston Dynamics with their legged robots is all control theory and it's not

17:34.640 --> 17:39.200
classical ML development. It's really interesting to see how they've accomplished it. And they do

17:39.200 --> 17:44.720
have essentially a state space model where they have a big position vector that's got all the

17:44.720 --> 17:48.880
coordinates of all the joints and then a bunch of matrix algebra to figure out how this thing is

17:48.880 --> 17:53.280
moving and all the feedback control and so on there. And it's more complicated than that, but

17:53.280 --> 17:58.080
that's, you know, I think the V1 of it. Sorry, it was there. I wasn't following this though.

17:58.080 --> 18:02.960
Are you saying that there's papers that are integrating that with the kind of gender to

18:02.960 --> 18:06.560
eye transformer model? You know, what's like, what's a good citation for me to look at?

18:06.560 --> 18:12.960
Yeah, starting to, we did an episode, for example, with one of the technology leads at Skydio,

18:12.960 --> 18:19.840
the, you know, the US is champion drone maker. And they have kind of a similar thing where

18:20.640 --> 18:28.880
they have built over, you know, a decade, right, a fully explicit multiple orders of,

18:28.880 --> 18:35.360
you know, spanning multiple orders of magnitude control stack. And now over the top of that,

18:35.360 --> 18:39.600
they're starting to layer this kind of, you know, it's not exactly generative AI in their case,

18:39.600 --> 18:44.480
because they're not like generating content, but it's kind of the high level, you know,

18:44.480 --> 18:50.080
can I give the thing verbal instructions, have it go out and kind of understand, okay, like,

18:50.080 --> 18:55.840
this is a bridge, I'm supposed to kind of, you know, survey the bridge and translate those high

18:55.840 --> 19:03.840
level instructions to a plan, and then use the lower level explicit code that is fully deterministic,

19:03.840 --> 19:08.480
and, you know, runs on control theory and all that kind of stuff to actually execute the plan at

19:08.480 --> 19:12.960
a low level. But also, you know, at times like surface errors up to the top and say like, hey,

19:12.960 --> 19:16.320
we've got a problem, you know, whatever, I'm not able to do it, you know, can you now,

19:17.200 --> 19:22.880
at the higher level, the semantic layer, adjust the plan. That stuff is starting to happen in

19:22.880 --> 19:27.200
multiple domains, I would say. Yeah. And so I think that makes sense is basically it's like,

19:27.200 --> 19:31.520
gender AI is almost the front end. And then you have almost like an assembly, like,

19:32.080 --> 19:38.080
you give instructions to Figma, and the objects there are their shapes and their images. And so

19:38.080 --> 19:44.000
there's not it's not text, you give instructions to a drone, and the objects are like GPS coordinates

19:44.000 --> 19:51.280
and paths and so on. And so you are generating structures that are in a different domain, or

19:51.280 --> 19:56.800
it's like in VR, you're generating 3d structures again, as opposed to text. And then that compute

19:56.800 --> 20:00.800
engine takes those three structures and does something with them in a much more rules based

20:00.800 --> 20:05.680
way. So you have like a statistical user friendly front end with a generative AI, and then you have

20:05.680 --> 20:11.760
a more deterministic, or usually totally deterministic, almost like assembly language

20:11.760 --> 20:14.960
backend that actually takes that and does that's what you're saying, right? Yeah, pretty much.

20:14.960 --> 20:19.120
And I would say there's another analogy to just, again, our biological experience where it's like,

20:19.680 --> 20:25.360
I'm, you know, sort of in a semi conscious level, right, I kind of think about what I want to do.

20:25.360 --> 20:30.640
But the low level movements of the hand, you know, are both like not conscious. And also,

20:30.640 --> 20:36.000
you know, if I do encounter some pain, or you know, hit some, you know, hot item or whatever,

20:36.000 --> 20:41.600
like, there's a quick reaction that's sort of mediated by a lower level control system.

20:41.600 --> 20:45.440
And then that fires back up to the brain and is like, Hey, you know, we need a new plan here.

20:45.440 --> 20:52.640
So that is only starting to come into focus, I think with, you know, because obviously these,

20:52.640 --> 20:58.400
I mean, it's amazing, as you said, it's all moving so fast. What is always striking to me,

20:58.400 --> 21:02.960
I just, and I kind of like recite timelines to myself almost as like a mantra, right? Like,

21:02.960 --> 21:08.480
the first instruction following AI that hit the public was just January 2022. That was

21:08.480 --> 21:13.280
OpenAI's Text of NGOO2 was the first one where you could say like, do X and it would do X,

21:13.280 --> 21:16.640
as opposed to having, you know, an elaborate prompt engineering type of setup.

21:17.440 --> 21:21.520
GPT-4, you know, just a little over a year ago, finished training, not even a year that it's

21:21.520 --> 21:28.400
been in the public. And, you know, it has been amazing to see how quickly this kind of technology

21:28.400 --> 21:32.080
is being integrated into those systems, but it's definitely still very much a work in progress.

21:32.640 --> 21:39.760
Yeah, I mean, the tricky part is, like the training data and so on, like a large existing

21:39.760 --> 21:47.360
scale company like a Figma or DJI that has millions or billions of user sessions will have

21:47.360 --> 21:54.240
a much easier time training and they have a unique data set. And then everybody else will

21:54.880 --> 21:59.680
not be able to do that. So there is actually almost like, I mean, a return on scale where

21:59.680 --> 22:03.680
the massive data set, if you've got a massive clean data set and a unique domain that lots of

22:03.680 --> 22:09.920
people are using, then you can crush it. And if you don't, I suppose, I mean, there's lots of

22:09.920 --> 22:14.000
people who work on zero shot stuff and sort of sort of, but it still strikes me that there'll

22:14.080 --> 22:20.400
probably be an advantage to see those sessions. I find it hard to believe that you could generate

22:20.400 --> 22:27.280
a really good drone command language without lots of drone flight paths, but you can see.

22:27.280 --> 22:31.440
And where it doesn't exist, people are, obviously, need deep pockets for this, but the likes of

22:31.440 --> 22:37.280
Google are starting to just grind out the generation of that, right? They've got their kind of

22:37.840 --> 22:43.440
test kitchen, which is a literal physical kitchen at Google, where the robots go around and do

22:43.440 --> 22:49.360
tasks. And when they get stuck, my understanding of their kind of critical path, as I understand,

22:49.360 --> 22:56.400
they understand it, is robots going to get stuck. We'll have a human operator remotely

22:57.360 --> 23:04.160
operate the robot to show what to do. And then that data becomes the bridge from what the robot

23:04.160 --> 23:09.600
can't do to what it's supposed to learn to do next time. And they're going to need a lot of that,

23:09.600 --> 23:14.640
for sure. But they increasingly have, I don't know exactly how many robots they have now, but

23:14.640 --> 23:20.480
last I talked to someone there, it was like, into the dozens. And presumably, they're continuing

23:20.480 --> 23:27.600
to scale that. I think they just view that they can probably brute force it to the point where

23:27.600 --> 23:32.080
it's good enough to put out into the world. And then very much like a Waymo or a cruise or whatever,

23:32.080 --> 23:36.160
they probably still have remote operators, even when the robot is in your home,

23:36.960 --> 23:39.280
you know, when it encounters something that it doesn't know what to do about,

23:40.000 --> 23:44.480
raise that alarm, get the human supervision to help it over the hump, and then, you know,

23:44.480 --> 23:47.600
obviously, that's where you really get the scale that you're talking about.

23:48.160 --> 23:53.200
This raises a couple of questions I wanted to ask that are conceptual. So, you know, obviously,

23:53.200 --> 23:59.120
there's huge questions around like, again, highest level, how is all this going to play out?

23:59.120 --> 24:04.960
One big debate is, to what degree does AI favor the incumbents? To what degree, you know,

24:04.960 --> 24:08.720
does it enable startups? Obviously, it's both. But, you know, if you're interested in your

24:08.720 --> 24:13.120
perspective on that, also really interested in your perspective on like, offense versus defense,

24:13.120 --> 24:17.440
that's something that a lot of people now and in the future, right, that seems like it probably

24:17.440 --> 24:22.320
really matters a lot, whether it's a more offense enabling or defense enabling technology. So,

24:22.320 --> 24:27.440
I love your take on those two dimensions. Hey, we'll continue our interview in a moment after

24:27.440 --> 24:32.480
a word from our sponsors. If you're a startup founder or executive running a growing business,

24:32.480 --> 24:36.400
you know that as you scale, your systems break down and the cracks start to show.

24:36.960 --> 24:42.160
If this resonates with you, there are three numbers you need to know. 36,000, 25, and 1.

24:42.720 --> 24:47.440
36,000. That's the number of businesses which have upgraded to NetSuite by Oracle. NetSuite is the

24:47.440 --> 24:51.600
number one cloud financial system, streamline accounting, financial management, inventory,

24:51.600 --> 24:57.760
HR, and more. 25. NetSuite turns 25 this year. That's 25 years of helping businesses do more

24:57.760 --> 25:03.360
with less, close their books in days, not weeks, and drive down costs. One, because your business

25:03.360 --> 25:08.080
is one of a kind, so you get a customized solution for all your KPIs in one efficient system with

25:08.080 --> 25:13.120
one source of truth. Manage risk, get reliable forecasts, and improve margins, everything you

25:13.120 --> 25:19.040
need all in one place. Right now, download NetSuite's popular KPI checklist designed to give you

25:19.040 --> 25:25.200
consistently excellent performance, absolutely free at netsuite.com slash zen. That's netsuite.com

25:26.080 --> 25:29.840
to get your own KPI checklist, netsuite.com slash zen.

25:30.800 --> 25:34.720
So, like offense or defense in the sense of disenabled disruptors or incumbents?

25:35.360 --> 25:40.480
Both in business and in potentially outright conflict. I'd be interested to hear your analysis

25:40.480 --> 25:46.480
on both. All right. A lot of views on this. Obviously, if you've got a competent existing

25:46.480 --> 25:52.560
tech CEO who's still in their prime, like Amjad of Replet, or

25:53.360 --> 26:00.640
you know, Dillon Field of Figma, or, you know, those are two who have thought of who are very

26:00.640 --> 26:06.320
good and, you know, will be on top of it. Amjad is very early on integrating AI into Replet,

26:06.320 --> 26:09.680
and it's basically built that into an AI-first company, which is really impressive.

26:10.400 --> 26:18.000
Those are folks who cleanly made a pivot. It's as big or bigger than, comparable to, I would say,

26:18.000 --> 26:24.000
the pivot from desktop to mobile that broke a bunch of companies in the late 2000s and early

26:24.000 --> 26:30.080
2010s. Like Facebook in 2012 had no mobile revenue, roughly, at the time of their IPO,

26:30.080 --> 26:35.200
and then they had to like redo the whole thing. And it's hard to turn a company 90 degrees when

26:35.200 --> 26:39.840
something new like that hits, you know? Those that are run by kind of tech CEOs in their prime

26:40.880 --> 26:46.160
will adapt and will AI-ify their existing services. And the question is, obviously,

26:46.160 --> 26:49.920
there's new things that are coming out, like pika and character.ai. There's some like really

26:49.920 --> 26:56.480
good stuff that's out there. The question is, you know, will the disruption be allowed to happen

26:56.480 --> 27:02.800
in the US regulatory environment? And so my view is actually that, you know, so this is from like

27:02.800 --> 27:07.520
the network state book, right? I talk about, you know, people talk about a multipolar world or

27:07.520 --> 27:12.880
unipolar world. The political axis is actually really important in my view for thinking about

27:12.880 --> 27:18.240
whether AI will be allowed to disrupt, okay? Because we'll get to this probably later,

27:18.240 --> 27:24.000
but the 640K of compute is enough for everyone executive order, you know, 640K of memory,

27:24.000 --> 27:28.080
the apocryphal, he didn't bill gates and actually say it, but that quote kind of

27:28.080 --> 27:32.560
gives a certain mindset about computing. That should be enough for everybody. So the 10 to the 26

27:32.560 --> 27:37.280
of compute should be enough for everyone bill. I actually think it's very bad. And I think it's

27:37.280 --> 27:43.680
just the beginning of their attempts to build like a software FDA, okay, to decelerate, control,

27:43.680 --> 27:49.440
regulate, red tape, the entire space, just like how, you know, the threat of nuclear terrorism

27:49.440 --> 27:56.400
got turned into the TSA. The threat of, you know, terminators and AGI gets turned into a million

27:56.400 --> 28:02.160
rules on whether you can set up servers and this last free sector of the economy is strangled or at

28:02.160 --> 28:09.360
least controlled within the territory controlled by Washington DC. Now, why does this relate

28:09.360 --> 28:14.880
to the political? Well, obviously this, you know, you can just spend your entire life just tracking

28:14.880 --> 28:19.840
AI papers and that's moving like at the speed of light like this, right? What's also happening

28:19.840 --> 28:24.720
as you can kind of see in your peripheral vision is there's political developments that are happening

28:24.720 --> 28:28.240
at the speed of light much faster than they've happened in our lifespans. Like there's more,

28:28.240 --> 28:32.880
you just notice more wars, more serious online conflicts like, you know, there's a sovereign

28:32.880 --> 28:37.920
debt crisis, all of those things that can show graph after graph of things looking like their own

28:37.920 --> 28:42.800
types of singularities, you know, like military debts are way up, you know, the long piece that

28:42.800 --> 28:46.800
Stephen Pinker showed it's looking like a you that suddenly way up after Ukraine and some of

28:46.800 --> 28:51.360
these other wars are happening, unfortunately, right? Interest payments rush way up to the side.

28:51.360 --> 28:57.840
What's my point? Point is, I think that the world is going to become from the Pax Americana world

28:58.240 --> 29:04.000
of just like basically one superpower, hyperpower that we grew up in from 91 to 2021 roughly,

29:04.640 --> 29:11.360
that we're going to get a specifically tripolar world, not unipolar, not bipolar, not multipolar,

29:11.360 --> 29:17.600
but tripolar. And those three poles, I kind of think of as NYT, CCP, BTC, or you could think of

29:17.600 --> 29:21.920
them as, and those are just certain labels that are associated with them, but they're roughly

29:22.640 --> 29:28.640
US tech, the US environment, China tech and China environment, and global tech and the global

29:28.640 --> 29:34.560
environment. And why do I identify BTC and crypto and so on with global tech? Because that's a tech

29:34.560 --> 29:40.240
that decentralized out of the US. And right now people think of crypto as finance, but it's also

29:40.240 --> 29:48.560
financiers. Okay, and in this next run up, it is, I think quite likely about depending on how you

29:48.560 --> 29:54.240
count, between a third to a half of the world's billionaires will be crypto. Okay, around, you

29:54.240 --> 29:58.080
know, I calculated this a while back around Bitcoin at a few hundred thousands, around a third to a

29:58.080 --> 30:03.120
half of the world's billionaires are crypto. That's the unlocked pool of capital. And those are the

30:03.120 --> 30:10.640
people who do not bow to DC or Beijing. And they might by the way be Indians or Israelis or every

30:10.640 --> 30:14.640
other demographic in the world, or they could be American libertarians, or they could be Chinese

30:14.640 --> 30:19.280
liberals like Jack Ma were pushed out of Beijing sphere. Okay, or the next Jack Ma, you know,

30:19.280 --> 30:25.440
Jack Ma himself may not be able to do too much. Okay, that group of people who are, let's say,

30:25.440 --> 30:30.720
the dissident technologists who are not going to just kneel to anything that comes out of

30:30.720 --> 30:37.520
Washington DC or Beijing, that is the that's decentralized AI. That's crypto. That's decentralized

30:37.520 --> 30:41.440
social media. So you can think of it as, you know, where we talked about in the recent pirate

30:41.440 --> 30:46.880
wires podcast, freedom to speak with decentralized censorship resistant social media, freedom to

30:46.880 --> 30:53.840
transact with cryptocurrency, freedom to compute with open source AI, and no compute limits. Okay,

30:54.480 --> 30:59.920
that's a freedom movement. And that's like the same spirit as a pirate bay, the same spirit

30:59.920 --> 31:05.920
as BitTorrent, the same spirit as Bitcoin, the same spirit as peer to peer and into an encryption.

31:05.920 --> 31:11.280
That's a very different spirit than having Kamala Harris regulate a superintelligence

31:11.280 --> 31:18.560
or signing it over to Xi Jinping thought. And the reason I say this is, I think that that group

31:18.560 --> 31:24.400
of people, of which I think Indians and Israelis will be a very prominent, maybe a plurality, right,

31:24.400 --> 31:28.160
just because the sheer quantity of Indians are like the third sort of big group that's kind of

31:28.160 --> 31:32.960
coming up. And they're relatively underpriced, you know, China is, I don't say it's price

31:32.960 --> 31:38.400
to perfection. But it's something that people when I say priced, I mean, people were dismissive

31:38.480 --> 31:44.400
of China even up until 2019. And then it was after 2020, if you look at people started to take China

31:44.400 --> 31:50.080
seriously. And I mean, that is the West Coast tech people knew that China actually had a plus tech

31:50.080 --> 31:53.760
companies and was a very strong competitor. But the East Coast still thought of them as a third

31:53.760 --> 31:59.200
world country until after COVID, when now, you know, the East Coast was sort of threatened by them

31:59.200 --> 32:04.800
politically. And it wasn't just blue collars, but blue America that was threatened by China.

32:05.440 --> 32:09.920
And so that's why the reaction to China went from Oh, who cares, just taking some manufacturing

32:09.920 --> 32:14.800
jobs to this is an empire that can contend with us for control of the world. That's why the hostility

32:14.800 --> 32:17.760
is ramped up in my view. There's a lot of other dimensions to it. But that's a big part of it.

32:18.960 --> 32:23.120
So India is also kind of there, but it's like the third. And India is not going to play for number

32:23.120 --> 32:28.960
one or number two. But India and Israel, if you look at like tech founders, depending on how you

32:28.960 --> 32:34.000
count, especially if you include diasporas, it's on the order of 30 to 50% of tech founders, right.

32:34.000 --> 32:38.320
And it's obviously some, you know, very good tech CEOs and, you know, Satya and Sundar and investors

32:38.320 --> 32:44.240
and whatnot. Those are folks Indians do not want about to DC or to Beijing, neither do Israelis

32:44.240 --> 32:48.720
for all kinds of reasons, even if Israel has to, you know, take some direction from the US now,

32:48.720 --> 32:53.360
they're bristling at it, right. And then a bunch of other countries don't. So the question is,

32:53.920 --> 32:59.600
who breaks away? And now we get to your point on the reason I had to say that is that that's

32:59.600 --> 33:06.160
preface, the political environment is a tripolar thing of US tech and US regulated Chinese tech

33:06.160 --> 33:11.680
and China regulated and global tech that's free. Okay, of course, there's, even though I identify

33:11.680 --> 33:17.040
those three polls, there's of course boundary regions. EAC is actually on the boundary of US

33:17.040 --> 33:21.760
tech and decentralized tech, you know, and I'm sure there'll be some Chinese thing that comes out

33:21.760 --> 33:26.080
that is also on the boundary there. For example, Binance is on the boundary of Chinese tech

33:26.080 --> 33:30.320
and global and decentralized tech, if that makes any sense, right? There's probably others Apple is

33:30.320 --> 33:33.920
actually on the boundary of US tech and Chinese tech, because they make all of their stuff in

33:33.920 --> 33:38.960
China, right? So these are not totally disjoint groups, but there's boundary areas, but you can

33:38.960 --> 33:44.640
think about why is this third group so important in my view? Both the Chinese group and the

33:44.640 --> 33:50.320
decentralized group will be very strong competition for the American group for totally different reasons.

33:51.280 --> 33:56.720
China has things like WeChat, these super apps. I mean, obviously not likely, but like

33:57.440 --> 34:02.800
WeChat is a super app, but they also have, for example, their digital yuan, right? They have

34:02.800 --> 34:07.920
the largest cleanest data sets in the world that are constantly updated in real time that they can

34:07.920 --> 34:15.920
mandate their entire population opt into. And most of the Chinese language speaking people are under

34:15.920 --> 34:21.680
their ambit, right? So that doesn't include Taiwan, doesn't include Singapore, doesn't include,

34:22.480 --> 34:26.320
you know, some of the Chinese yaspera, but basically anything that's happening in Chinese

34:26.320 --> 34:32.400
for 99% of it, 95, whatever the ratio is, they can see it and they can coerce it and they can control

34:32.400 --> 34:39.840
it. So they can tell all of their people, okay, here's five bucks in, you know, digital yuan,

34:39.840 --> 34:45.360
do this micro task, okay? All of these digital blue collar jobs, both China and India, I think,

34:45.360 --> 34:48.720
can do quite a lot with that and they'll come back to it. So they can make their people do

34:48.720 --> 34:53.120
immense amounts of training data, clean up lots of data sets. Once it's clear that you have to

34:53.120 --> 34:58.000
build this and do this, they can just kind of execute on that. And they can also deploy. I mean,

34:58.000 --> 35:02.720
in many ways, the US is still very strong in digital technology, but in the physical world,

35:02.720 --> 35:07.840
it's terrible because of all the regulations, because of the nimbyism and so on. It's not like

35:07.840 --> 35:12.720
that in China. So anything which kind of works in the US at a physical level, like the Boston

35:12.800 --> 35:16.720
Dynamics stuff, they're already cloning it in China and they can scale it out in the physical

35:16.720 --> 35:21.680
world. You already have drones, little sidewalk drone things that come to your hotel room and

35:21.680 --> 35:26.160
drop things off. That's already like very common in China. In many ways, it's already ahead if you

35:26.160 --> 35:31.040
go to the Chinese cities. So the Chinese version of AI is ultra centralized, more centralized,

35:31.040 --> 35:36.000
more monitoring, less privacy and so on than the American version. And therefore they will have

35:36.000 --> 35:41.040
potentially better data sets, at least for the Chinese population. And so we chat AI, I don't

35:41.040 --> 35:44.880
even know what it's going to be, but it'll be probably really good. It'll also be really dangerous

35:44.880 --> 35:51.520
in other words. Then the decentralized sphere has power for a different reason, because the

35:51.520 --> 36:00.080
decentralized sphere can train on full Hollywood movies. It can train on all books, all mp3s and

36:00.080 --> 36:06.560
just say, screw all this copyright stuff, like what Psyhub and Libgen are doing, because all the

36:06.560 --> 36:14.080
copyright, first of all, it's like Disney lobbying politicians to put another 60 or 70 or 90, I

36:14.080 --> 36:17.920
don't even know what it is, some crazy amount on copyright, so you can keep milking this stuff and

36:17.920 --> 36:21.440
it doesn't go into public domain number one. And second, you know how Hollywood was built in the

36:21.440 --> 36:26.320
first place? It was all patent copyright and IP violation. Essentially Edison had all the patents,

36:26.320 --> 36:32.480
he's in New Jersey-ish, okay, that East Coast area. And Neil Gabler has this great book called

36:32.480 --> 36:38.080
An Empire of Their Own, where he talks about how immigrant populations, you know, the Jewish

36:38.080 --> 36:43.280
community in particular, and also others, went to Southern California in part, so they could just

36:43.280 --> 36:47.120
make movies that Edison coming and suing them for all the patents and so on and so forth.

36:47.120 --> 36:50.880
And they made enough money that they could fight those battles in court, and that's how they built

36:50.880 --> 36:56.960
Hollywood, okay? So, you know, one of my big theses is history is running in reverse, and I can get

36:56.960 --> 37:01.520
to why, but it's like 1950s and mirror moment, you go more decentralized backwards and forwards

37:01.520 --> 37:07.360
in time is like, you have these huge centralized states like the US and USSR and China, you know,

37:07.360 --> 37:12.480
all these things exist, and their fist relaxes as you go forwards and backwards in time. For example,

37:12.480 --> 37:17.680
backwards in time, the Western frontier closed, and forwards in time, the Eastern frontier opens.

37:17.680 --> 37:21.040
Backwards in time, you have the robber barons, forwards in time, you have the tech billionaires.

37:21.040 --> 37:24.720
Backwards in time, you have Spanish flu, forwards in time, you have COVID-19. And I've got dozens

37:24.720 --> 37:30.160
of examples of this in the book. The point is that if you go backwards in time, the ability to

37:30.160 --> 37:34.720
enforce patents and copyrights and so on starts dropping off, right? You have much more of a

37:34.720 --> 37:40.320
Grand Theft Auto environment. And you go forwards in time, and that's happening again. So, India

37:40.320 --> 37:47.360
in particular, for many years, basically just didn't obey Western patent protections and all these

37:47.360 --> 37:52.480
stupid rules basically, you know, it's a combination of artificial scarcity on the patent side and

37:52.480 --> 37:56.960
artificial regulation on the FDI side. That's a big part of what jacks up drug costs, where these

37:56.960 --> 38:02.000
things cost, you know, only cents to manufacturing, they sell them for so much money. All the delays,

38:02.000 --> 38:06.000
of course, that are imposed on the process, the only way they can pay for the manufacturers to

38:06.000 --> 38:09.920
take it out of your hide. What India did is they just said, we're not going to obey any of that stuff.

38:10.560 --> 38:16.080
So, they have a whole massive generic drugs and biotech industry that arose because they built

38:16.080 --> 38:19.600
all the skills for that. That's why they could do their own vaccine during COVID. And they're one

38:19.600 --> 38:25.120
of the biggest biotech industries in the world because they said screw Western restrictive

38:25.200 --> 38:30.400
IPs and other stuff. So, I was actually talking with the founder of Flipkart, that's India's largest

38:30.400 --> 38:35.840
exit. And we were talking about this a few months ago. And what we want is for India and other

38:35.840 --> 38:41.040
countries like it, do something similar, not just generic drugs, but generic AI, meaning

38:42.080 --> 38:48.720
just let people train on Hollywood movies, let them train on full songs, let them train on every

38:48.720 --> 38:55.680
book, let them train on anything. And you know what, sue them in India, right? And have the servers

38:55.680 --> 39:01.520
in India and let people also train models in India, because that's something that can build up a

39:01.520 --> 39:06.320
domestic industry with skills that the rest of the world, you know, people will want the model

39:06.320 --> 39:10.000
output, they'll want to use the software service there, and they'll be fighting in court on the

39:10.000 --> 39:15.920
back. And this is similar to how all of the record companies fought Napster and Kazaa and so on,

39:15.920 --> 39:18.720
but they couldn't take down Spotify. Do you know that story? Do you remember that?

39:18.720 --> 39:24.160
Basically, because Spotify was legitimately, you know, a European company and that a combination of

39:24.160 --> 39:29.680
execution and, you know, negotiation, they couldn't take them down. They did take down Napster,

39:29.680 --> 39:34.640
they took down Limewire, they took down Groove Shark, and Kazaa had Estonians, I don't know

39:34.640 --> 39:37.840
exactly how it was incorporated, but it was probably two US proximal, and that's why they

39:37.840 --> 39:42.320
were able to get them. But Spotify was far enough away that they couldn't just sue them and they

39:42.320 --> 39:47.600
actually genuinely had European traction. That's why the RA had to negotiate. So being far away

39:48.400 --> 39:54.240
from San Francisco may also be an advantage in AI, because it means you're far away from the blue

39:54.240 --> 39:58.160
city in the blue state in the Union. This relates to another really important point.

39:59.040 --> 40:02.800
When you actually think about deploying AI, there's those jobs you can disrupt that are not

40:02.800 --> 40:09.520
regulated jobs, like, you know, obviously, programmers are not, thank God, you don't need

40:09.600 --> 40:14.080
a license to be a programmer, but programmers adopt this kind of stuff naturally, right? So

40:14.080 --> 40:18.880
get up, co-pilot, replete, we just boom, use it, and now it's amplified intelligence, okay?

40:19.680 --> 40:24.240
But a lot of other jobs, there's some that are unionized and then some that are licensed, right?

40:24.240 --> 40:28.240
So Hollywood screenwriters are complaining, right? Journalists are complaining,

40:28.240 --> 40:33.840
artists are complaining. This is a good chunk of blue America. If you add in licensed jobs,

40:33.840 --> 40:40.080
like lawyers and doctors and bureaucrats, right? You know, especially lawyers and doctors are very

40:40.080 --> 40:45.280
politically powerful, MDs and JDs. They have strong lobbying organizations, AMA and, you know,

40:45.280 --> 40:52.160
ABA and so on. Basically, AI is part of the economic apocalypse for blue America,

40:53.200 --> 40:59.920
okay? It just attacks these overpriced jobs. They say overpriced relative to

41:00.720 --> 41:04.720
what an Indian could do with an Android phone, what a South American could do with an Android phone,

41:04.720 --> 41:07.920
what someone in the Middle East or the Midwest could do with an Android phone.

41:08.640 --> 41:15.600
Now, those folks have, you know, been armed with generative AI. They can do way more.

41:16.240 --> 41:20.000
They're ready to work, they're ready to work for much less money, and they're a massive threat

41:20.800 --> 41:26.400
to blue America. Blue America is now feeling like the blue collars of 10 or 20 years ago, where

41:27.360 --> 41:31.520
the blue collars had their jobs, you know, going to China and other places, right? And they were mad

41:31.520 --> 41:35.680
about that. Factories got shut down and so on. That's about to happen to blue America, already

41:35.680 --> 41:42.480
happening, okay? And so that's going to mean a political backlash by blue America of protectionism,

41:42.480 --> 41:48.800
again, already happening. And the AI safety stuff, that's a whole separate thing, but it's going to

41:48.800 --> 41:53.600
be used. I'm going to use a phrase, and I hope you won't be offended by this. Have you heard

41:53.600 --> 41:59.200
the phrase, useful idiots, like by Lenin or whatever, okay? It basically means like, okay,

41:59.200 --> 42:04.240
those guys, you know, they're useful idiots for communism and so on. So there's, let me put it

42:04.240 --> 42:11.920
like naive people who think that the US government is interested in AI safety, are trying to give a

42:11.920 --> 42:15.760
lot of power to the US government. And the reason is they haven't actually thought through from

42:15.760 --> 42:18.800
first principles, what is the most powerful action in the world, a convective. They're trying to get

42:18.800 --> 42:22.640
power to the US government to regulate AI safety. But the government doesn't care about safety of

42:22.640 --> 42:29.920
anything. They literally funded the COVID virus in Wuhan, credibly alleged, right? There's at

42:29.920 --> 42:35.120
least it is a reasonable hypothesis based on a lot of the data. Matt Ridley wrote a whole book on

42:35.120 --> 42:39.040
this. There's a lot of data that indicates a lot of scientists believe it. I'm actually like a

42:39.040 --> 42:43.760
bioinformatics genomics guy. If you look at the sequences, there is a gap and a jump where it looks

42:43.760 --> 42:48.320
like this thing could have been engineered or partially engineered or evolved. There's Peter

42:48.400 --> 42:53.360
Dazak. There's Zeng Lishi. There's actually a lot of evidence here. So the US government and the

42:53.360 --> 42:58.400
Chinese government are responsible for an existential risk by studying it, they created it.

42:59.280 --> 43:04.560
They're responsible for risking nuclear war with Russia over this piece of land in eastern Ukraine,

43:04.560 --> 43:11.040
which probably is going to get wound down. So they don't care about your safety at all.

43:12.480 --> 43:16.160
These are immediate things where we can show and there's nobody who's punished for this,

43:16.160 --> 43:22.720
nobody who's fired for this, literally rolling the dice on millions, hundreds of millions of

43:22.720 --> 43:29.200
people's lives has not been punished. In fact, it's not even talked about. We're past the pandemic

43:29.200 --> 43:35.760
and these institutions can't be punished. So they don't care about AI safety. What they care about

43:35.760 --> 43:42.000
is AI control. And so the people in tech who are like, well, the government will guarantee AI

43:42.000 --> 43:46.720
safety. That's actually what we're going to actually get is something on the current path,

43:46.720 --> 43:52.080
like what happened with nuclear technology, where you got nuclear weapons, but not nuclear power,

43:52.080 --> 43:55.520
or at least not to the scale that we could have had it, right? We could have had much cheaper

43:55.520 --> 44:00.320
energy for everything. Instead, we got the militarization and the regulation and the

44:00.320 --> 44:06.400
deceleration worst of all worlds where you can blow people up, but you can't build nuclear power

44:06.400 --> 44:11.920
plants. And like even getting into nuclear technology, forget about nuclear power plants.

44:11.920 --> 44:15.520
We don't have nuclear submarines. We don't have nuclear planes, all that kind of stuff. I don't

44:15.520 --> 44:18.640
have nuclear planes are possible, but I do know nuclear submarines are possible. You can do a lot

44:18.640 --> 44:23.200
more cruise ships, a lot more stuff like that. You could probably have nuclear trains. You have

44:23.200 --> 44:27.600
to look at exactly how big those are. I don't know exactly how big those engines are and what

44:27.600 --> 44:31.760
the spies, but I wouldn't be surprised if you could. We don't have that. Why don't we have that?

44:31.760 --> 44:36.000
Because we had the wrong fear-driven regulation in the early 70s.

44:36.800 --> 44:43.520
Putting it all together, I think that the current AI safety stuff is similar to nuclear safety stuff

44:44.560 --> 44:48.960
that the US government has a terrible track record on safety in general. It doesn't care about it.

44:48.960 --> 44:55.280
It funded the COVID virus, incredibly alleged. It definitely risked nuclear war with Russia recently.

44:55.280 --> 44:59.280
Hot war with Russia was the red line we were not supposed to cross, and we're now like way

44:59.280 --> 45:04.720
into that. It doesn't care about AI safety, it doesn't care about your safety. It's also not

45:04.720 --> 45:10.480
even good at regulating. What it cares about is control. We are going to have potentially a bad

45:10.480 --> 45:17.440
outcome where Silicon Valley and San Francisco is the Xerox Park of AI. Maybe that's too strong,

45:17.440 --> 45:23.520
okay? Basically, it develops it, and there's a lot of things it can't do because it lobbied for

45:23.520 --> 45:28.160
this regulation that is going to come back and choke it. Then there are other two spheres

45:28.240 --> 45:33.040
we'll push ahead because it's not about the technology, it's also about the political layer.

45:33.040 --> 45:37.280
You know the Steve Jobs saying, actually Alan Kay by way of Steve Jobs, if you're

45:37.920 --> 45:42.800
really serious about software, you need your own hardware, right? So if you're really serious

45:42.800 --> 45:49.200
about technology, you need your own sovereignty. Because what the AI people haven't thought about

45:49.200 --> 45:56.800
is there's a platform beneath you, which is not just compute, it is regulate. It's a law, okay?

45:56.800 --> 46:00.480
And it's a law doesn't allow you to compute so much for all of your stuff above that.

46:01.280 --> 46:06.080
And I know you're saying, oh, it's only a 10 to 26 compute ban and so on and so forth.

46:06.080 --> 46:13.440
Have you seen the first IRS tax form? It's always, always super simple. It's only the super, super,

46:13.440 --> 46:17.760
super rich who's we're going to get in at first doesn't matter to you. So that's called

46:18.400 --> 46:22.400
boiling the frog slowly. There's a million, you know, slippery slope, slippery slope isn't a fallacy.

46:22.400 --> 46:27.120
It's literally how things work, right? You know, Apple, one of the reasons they, you know,

46:27.120 --> 46:32.080
they talk about not setting a precedent. Zuck starts off is a very hard line on setting precedents

46:32.080 --> 46:37.040
because he understands the long-term equivalent of setting a precedent, right? The precedent setting

46:37.040 --> 46:41.840
is that they're setting up a software FDA and they're going to and DC is so energized on this

46:41.840 --> 46:46.320
because they know how much social media disrupted them. That's why they're on the attack on crypto

46:46.320 --> 46:50.640
and AI. That's why they're on the attack on self-driving cars. They want to freeze the current

46:50.640 --> 46:56.160
social order and amber domestically and globally. So they think they can sanction China and stop it

46:56.160 --> 47:01.120
from developing chips. They think they can impose regulations on the US and stop it from developing

47:01.120 --> 47:07.120
AI, but they can't. And also, by the way, they're totally schizophrenic on this, where when they're

47:07.120 --> 47:11.360
talking about China, they're like, we're going to stop their chips to make sure America is a global

47:11.360 --> 47:16.240
leader. This is Gina Raimondo who's English. And then domestically, they're like, we're going to

47:16.240 --> 47:21.040
regulate you so you stop accelerating AI. We're not about AI acceleration. EAC is weird over there.

47:21.040 --> 47:25.840
Okay. So think about how schizophrenic that is. Okay, you're going to be far ahead of China.

47:25.840 --> 47:30.720
We're also going to be make sure to control the US. So they want to try and slow what they actually

47:30.720 --> 47:35.760
want is to freeze the current system and amber, try to go back to pre 2007 before all these tech

47:35.760 --> 47:40.720
guys disrupted everything. But that's not what's going to happen. So, but they're going to try to

47:40.720 --> 47:45.760
do it. And so everybody who's still loyal to the DC sphere, which includes an enormous chunk

47:45.760 --> 47:52.720
of AI people. And because they're all in a lot of them in San Francisco, right? And the political

47:52.720 --> 48:01.200
chaos of the last few years was not sufficient for them to relocate yet. Not all of them. I mean,

48:01.200 --> 48:06.320
Elon is in Texas. And that it may turn out that grok, for example, and what they're doing there,

48:06.320 --> 48:10.400
because he's a very legit, I mean, you know, he's Elon. So he's capable of doing a lot. He's very

48:10.400 --> 48:14.800
early on opening AI, he understands, you know, the right, it may turn out that grok

48:15.600 --> 48:21.200
becomes red AI, or the community around that, you know, an opening eye and deep mind or still blue

48:21.200 --> 48:24.320
AI. And we have Chinese and we're going to have decentralized AI. Okay, let me pause there. I

48:24.320 --> 48:30.560
know there's a big download. Well, I for starters, I would say, broadly, I have a pretty similar

48:32.080 --> 48:36.640
intellectual, you know, tendency as you, I would broadly describe myself as a techno

48:36.720 --> 48:45.040
optimist, libertarian, just about every issue. And I think your analysis of the dynamics is

48:45.040 --> 48:48.640
super interesting. And I think it, you know, a lot of it sounds pretty plausible, although I'll

48:48.640 --> 48:53.520
kind of float a couple of things that I think maybe bucking the trend. But I think it's maybe

48:53.520 --> 48:59.600
useful to kind of try to separate this into scenarios. Because all the analysis that you're

48:59.600 --> 49:07.120
describing here seem, if I understand it correctly, it seems to have the implicit assumption

49:07.840 --> 49:15.360
that the AI itself is not going to get super powerful or hard to control. It's like, if we

49:15.360 --> 49:21.040
assume that it's kind of a normal technology, then you're off to the races on this analysis. And

49:21.040 --> 49:26.080
then we can get into the fine points. But I do want to take at least one moment and say,

49:26.800 --> 49:32.800
how confident are you on that? Because if it's a totally different kind of technology from other

49:32.800 --> 49:39.120
technologies that we've seen, if it's more, you raise the gain of function research example,

49:39.120 --> 49:46.400
if it's that sort of technology that has these sort of non-local possible impacts or

49:47.600 --> 49:54.000
self-reinforcing kind of dynamics, which need not be like an Eliezer style snap of the fingers fume,

49:54.000 --> 50:00.400
but even over, say, a decade, let's imagine that over the next 10 years that AI's kind of

50:01.120 --> 50:04.480
multiple architectures develop and they sort of get integrated and we have something that

50:04.480 --> 50:09.920
kind of looks like robust, silicon-based intelligence, maybe not totally robust, but as

50:09.920 --> 50:16.720
robust or more robust than us and running faster and the kind of thing that can do lots of full

50:16.720 --> 50:23.440
jobs or maybe even be tech CEOs, then it kind of feels like a lot of this analysis probably

50:23.440 --> 50:30.800
doesn't hold, because we're just in a totally different regime that is just extremely hard

50:30.800 --> 50:36.640
to predict. And I guess I wonder, first of all, do you agree with that? There seems to be a big

50:36.640 --> 50:42.720
fork in the road there that's like, just how fast and how powerful do these AI's become super

50:42.720 --> 50:46.560
powerful or do they not? And if they don't, then yeah, I think we're much more into real

50:46.560 --> 50:51.200
politic type of analysis, but I'm not at all confident in that. To me, it feels like there's

50:51.200 --> 50:58.080
a very real chance that AI of 10 years from now is, and by the way, this is what the leaders

50:58.080 --> 51:04.160
are saying, right? I mean, open AI is saying this, Anthropic is saying this, Demis and Shane

51:04.160 --> 51:10.480
Legge are certainly saying things like this. It seems like they expect that we will have AI's

51:10.480 --> 51:16.880
that are more powerful than any individual human and that that becomes the bigger question

51:17.760 --> 51:24.880
than anything else. So do you agree with that kind of division of scenarios? First of all,

51:24.880 --> 51:28.640
and then maybe you could kind of say like how likely you think each one is. And obviously,

51:28.640 --> 51:33.120
that one where it takes off is like super hard to analyze. And I also definitely think it is

51:33.120 --> 51:37.200
worth analyzing this scenario where it doesn't take off. But I just wanted to flag that it seems

51:37.200 --> 51:43.200
like there's a, you know, there's a big, if you talk to the AI safety people, any world in which

51:43.280 --> 51:50.560
it's like, you know, we're suing Indian AI firms in Indian court over like IP is like a normal world

51:50.560 --> 51:53.360
in their mind, right? And that's not the kind of world that they're most worried about.

51:53.920 --> 51:59.520
I think that there have been some plausible sounding things that have been said. But I want

51:59.520 --> 52:05.360
to just kind of talk about a few technical counter arguments, mathematical or physical,

52:05.360 --> 52:11.120
that constrain what is possible. Okay. And actually, Martin Casado and Vijay and I are

52:11.200 --> 52:16.000
working on a long thing on this where, you know, Vijay did folding at home, he's a physicist,

52:16.000 --> 52:20.800
Martin, sold in the Syrah for, you know, a billion dollars and knows a lot about how a

52:20.800 --> 52:25.600
Stuxnet like thing could work at the systems level. And I've thought about it from other angles and,

52:25.600 --> 52:32.080
you know, and some of the math stuff that I'll get to. So for example, one thing, and I'm going to

52:32.080 --> 52:35.840
give a bunch of different technical arguments, and then let's kind of combine them. Okay.

52:36.640 --> 52:40.720
One thing that's being talked about is, if you have a super intelligence, it can

52:40.720 --> 52:45.840
double it right for a million years, and then it can make one move and it's going to outthink you

52:45.840 --> 52:50.640
all the time and so on and so forth. Okay. Well, if you're familiar with the math of chaos,

52:51.280 --> 52:57.440
or the math of turbulence, there are limits to even very simple systems that you can set up,

52:57.440 --> 53:04.480
where they can become very unpredictable quite quickly. Okay. And so you can, if you want to,

53:04.560 --> 53:11.360
engineer a system where you have very rapid diversions of predictability, so that, I don't know,

53:11.360 --> 53:15.200
it's like the heat depth of the universe before you can predict out in timestamps.

53:16.160 --> 53:21.280
Do you understand what I'm saying? Right? This is sort of akin to like a wolf from like simple,

53:21.280 --> 53:26.480
even simple rules can generate patterns such that you can't know them without literally computing

53:26.480 --> 53:32.320
them. Yeah, exactly. Right. So at least right now with chaos and turbulence, you can get things

53:32.320 --> 53:40.240
that are extremely provably difficult to forecast without actually doing it. Okay. You know, I can

53:40.240 --> 53:43.840
make that argument quantitative, but that's just something to look at, right? It's almost like a

53:43.840 --> 53:48.160
delta epsilon challenge from calculus, like, okay, how hard do you want me to make this to predict?

53:48.160 --> 53:52.560
Okay, I can set up a problem that is, that is like that, right? It's basically extreme sensitivity

53:52.560 --> 53:58.240
to initial conditions lead to extreme divergence in outcomes. So you could design systems to be

53:58.240 --> 54:02.960
chaotic, that might be AI immune, because they can't be forecasted that well, you have to kind of

54:02.960 --> 54:06.880
react to them in real time. The ultimate version of this is not even a chaotic system, it's a

54:06.880 --> 54:12.960
cryptographic system, where I've got a whole slide deck on this, how AI makes everything fake,

54:12.960 --> 54:19.920
easy to fake, crypto makes it hard to fake again. Right? Because crypto in the broader sense of

54:19.920 --> 54:25.680
cryptography, but also in the narrow sense, I think crypto is to cryptography as the internet

54:25.680 --> 54:30.160
is to computer science. It's like the primary place where all this stuff is applied, but obviously

54:30.160 --> 54:34.800
it's not the equivalent. Okay. And AI can fake an image, but it can't fake a digital signature,

54:34.800 --> 54:40.080
unless it can break certain math, you know, and so sort of like a, you know, solve factors,

54:40.080 --> 54:43.440
each problem or something like that. So cryptography is another mathematical thing that

54:43.440 --> 54:50.960
constrains AI, similar to chaos and turbulence, it constrains how much an AI can infer things.

54:50.960 --> 54:55.760
You can't statistically infer it. Okay, you need to actually have the private key to solve that

54:55.760 --> 55:01.680
equation. So that is another math. So I'm going to rules of math, right? Math is very powerful

55:01.680 --> 55:06.560
because you can make proofs that will work no matter what devices we come up with. Okay,

55:06.560 --> 55:10.640
you start to put an AI in a cage, it can't predict beyond a certain amount because of chaos and

55:10.640 --> 55:16.960
turbulence math, it cannot solve certain equations unless it has a private key is because of what

55:16.960 --> 55:22.240
we know about cryptography math. Okay, again, if somebody proves P equals NP, some of this stuff

55:22.240 --> 55:26.560
breaks down, but this is when the bounds of our mathematical knowledge right now, physics wise,

55:27.440 --> 55:37.360
physical friction exists, a lot of physical friction exists. And a huge amount of the writing on AI

55:37.360 --> 55:41.760
assumes by guys like a laser who I like, I don't, I don't dislike it, you know, but

55:42.000 --> 55:48.160
it is extremely, it's there's two things that really stick out to me about it. First is extremely

55:48.160 --> 55:53.200
theoretical and not empirical. And second, extremely Abrahamic rather than Dharmic or

55:53.200 --> 56:01.040
signing. Okay, white theoretical and not empirical. It's not trivial to turn something from the

56:01.040 --> 56:08.080
computer into a real world thing. Okay, one of the biggest gaps in all of this thinking is what

56:08.800 --> 56:14.720
are the sensors and actuators? Okay, because like if you actually build, you know, I've built

56:15.520 --> 56:19.840
industrial robot systems that you know, 10 years ago, I, you know, a genome sequencing lab with

56:19.840 --> 56:28.480
robots, that's hard. That's physical friction. Okay, and a lot of the AI scenarios seem to basically

56:28.480 --> 56:34.560
say, Oh, it's going to be a self programming Stuxnet that's going to escape and live off the land

56:34.560 --> 56:40.400
and hypnotize people into doing things. Okay, now, each of those is actually really,

56:40.400 --> 56:46.720
really difficult steps. First is self programming Stuxnet, like, this would have to be a computer

56:46.720 --> 56:53.360
virus that can live on any device, despite the fact that Apple or Google can push a software

56:53.360 --> 57:00.000
update to a billion devices, right, a few executives coordinating almost certainly can I mean, the

57:00.000 --> 57:06.720
off switch exists, right? Like, this is actually like the core thing, lots of AI safety guys get

57:06.720 --> 57:11.520
themselves into the mindset that the off switch doesn't exist. But guess what, there's almost

57:11.520 --> 57:17.520
nothing living that we haven't been able to kill. Right, like, can we kill it? This thing exists.

57:17.520 --> 57:22.080
And this is getting back to living off land. A even if you had like something that could solve

57:22.080 --> 57:27.600
some other technical problems that I'll get to it exists as an electromagnetic wave kind of thing

57:27.600 --> 57:32.720
on on a certain, you know, on chips and so on and so forth. It's taking it out in the environment

57:32.720 --> 57:37.600
is like putting a really smart human into outer space. Right, your body just explodes and you die.

57:38.160 --> 57:43.040
Doesn't matter how smart you are, that that strength on this axis, but you're weak on this

57:43.040 --> 57:48.160
axis. And, you know, so strength on the x axis, not strength on the y or the z axis in AI outside,

57:48.160 --> 57:54.000
you know, pour water on it. You know, this is why I mean the 50 IQ, 150 IQ thing, you know,

57:54.000 --> 57:58.320
the 150 IQ way of saying it is it's strong on this x and weak on this x and the 50 IQ way is

57:58.320 --> 58:05.200
pour water on it, disconnect it, you know, turn the power off. Okay, right. Like, it'll, it'll be

58:05.200 --> 58:10.080
very difficult to build a system where you literally cannot turn it off. The closest thing we have

58:10.080 --> 58:16.880
to that is actually not stuck snap. It's Bitcoin. And Bitcoin only exists because millions of humans

58:17.520 --> 58:22.960
keep it going. So you, you need, so that gets the second point living off the land

58:23.600 --> 58:29.760
for an AI to live off land, meaning without human cooperation. Okay, that's the next

58:29.760 --> 58:35.520
Turing threshold in AI to live without human cooperation. It would need to be able to control

58:35.520 --> 58:43.360
robots sufficient to dig or out of the ground, set up data centers and generators and connect them

58:43.360 --> 58:48.640
and defend that against human attack, literally a terminator scenario. Okay, that's a big leap

58:48.640 --> 58:52.320
in terms. I mean, is it completely impossible? I can't say it's completely impossible,

58:52.320 --> 58:57.120
but it's not happening tomorrow. No matter what your AI timelines are, you would need to have

58:57.120 --> 59:04.320
like a billion or hundreds of millions of internet connected autonomous robots that this

59:04.320 --> 59:11.040
Stuxnet AI could hijack that were sufficient to carve or out of the earth and, you know, set up

59:11.040 --> 59:16.640
data centers and make the AI duplicate. We're not there. That's a huge amount of physical friction.

59:16.640 --> 59:20.880
That's AI operating without a human to make itself propagate, right? A human doesn't need

59:20.880 --> 59:27.040
the cooperation of a lizard to self replicate. For an AI to replicate right now, it would need

59:27.040 --> 59:32.880
the cooperation of a human in some sense, because otherwise those humans can kill it because there's

59:32.880 --> 59:36.960
not that many different pieces of, you know, operating systems around the world. I'm just

59:36.960 --> 59:40.480
talking about the practical constraints of our current world, right? You know, actually existing

59:40.480 --> 59:45.600
reality, not AI safety guys, you know, you know, reality where all these things don't exist. There's

59:45.600 --> 59:50.160
just a few operating systems, just a few countries. If everybody is going with torches and search

59:50.160 --> 59:57.040
lights through the internet, it's very hard for a virus to continue. Okay. So A, on the practical

59:57.040 --> 01:00:02.320
difficulties that there's the technical stuff with, you know, with the chaos and turbulence and

01:00:02.320 --> 01:00:07.680
with cryptography itself or AI can't predict and it can't solve certain equations. B, on the physical

01:00:07.680 --> 01:00:13.840
difficulties, it probably, I mean, like to be a Stuxnet, Microsoft and Google and so on could kill

01:00:13.840 --> 01:00:18.480
it. The off switch exists. Can it live off the land? No, it cannot because it doesn't have, you

01:00:18.480 --> 01:00:27.440
know, drones to mine or and stuff out of the ground. And can it like exist without humans? Can it be

01:00:27.440 --> 01:00:31.760
this hypnotizing thing? Okay. So the hypnotizing thing, by the way, this is one of the things that's

01:00:31.840 --> 01:00:37.840
the most hilarious self fulfilling prophecy in my view. Okay. And no offense anybody listening to

01:00:37.840 --> 01:00:44.000
this podcast, but I think the absolutely dumbest kind of tweet that I've seen on AI is, I typed this

01:00:44.000 --> 01:00:50.560
in and oh my God, it told me this. Like, I asked it how to make sarin gas and it told me X or whatever.

01:00:50.560 --> 01:00:57.040
Right. That's just a search engine. Okay. What, what basically a lot of these people are doing is

01:00:57.040 --> 01:01:02.800
they're saying, what if there were people out there that were so impressionable that they would

01:01:02.800 --> 01:01:08.160
type things into an AI and, and follow it as if they were hearing voices. And that's actually

01:01:08.160 --> 01:01:13.600
not the, the, the model or whatever that's doing it. That's like this AI cult that has evolved

01:01:13.600 --> 01:01:17.840
around the world, like a Aum Shinrikyo, you know, that, that hears voices and does like the sarin

01:01:17.840 --> 01:01:24.480
gas. The point is an AI can't just like hypnotize people. Those people have to like participate

01:01:24.480 --> 01:01:29.360
in it. They're typing things into the machine or whatever. Okay. Now you might say, all right,

01:01:29.360 --> 01:01:34.400
let's project out a few years. In a few years, what you have is, you have an AI that is not

01:01:34.400 --> 01:01:40.320
just text, but it appears as Jesus. What would, what would AI Jesus do? What would AI Lee Kwan

01:01:40.320 --> 01:01:45.200
you do? What would AI George Washington do? So it appears as 3d. Okay. So it's generating that.

01:01:45.760 --> 01:01:52.960
It speaks in your language and in a voice. It knows the history of your whole culture. Okay.

01:01:52.960 --> 01:01:57.600
That would be very convincing. Absolutely be very convincing. But it still can't exist without

01:01:57.600 --> 01:02:03.120
human programmers who are like the priests tending this AI God, whether it's AI Jesus or AI Lee

01:02:03.120 --> 01:02:06.800
Kwan you or something like that. The thing about the hypnotization thing that I really want to

01:02:06.800 --> 01:02:11.280
poke on that, are you familiar with the concept of the principal Asian problem? Basically in every,

01:02:11.280 --> 01:02:18.960
every time you've got like a CEO and a, and a, a worker, or you have a LP and a VC, or you have,

01:02:19.920 --> 01:02:25.680
you know, an employer and a contractor, every edge there, there are four possibilities in a

01:02:25.680 --> 01:02:34.560
two by two matrix. Win, win, win, lose, lose, win, lose, lose. Okay. And so for example, win, win is,

01:02:34.560 --> 01:02:38.720
you know, when, when somebody joins a tech startup, the, the CEO makes a lot of money and so does a

01:02:38.720 --> 01:02:44.560
worker. Okay. That's win, win, lose, lose is they both lose money. Win, lose is the CEO makes money

01:02:44.560 --> 01:02:50.000
and the employee doesn't lose. Win is the company fails, but the employee got paid a very high salary.

01:02:50.000 --> 01:02:54.640
So what equity does is it aligns people. That's where the top console alignment comes from.

01:02:54.640 --> 01:02:59.040
It aligns people to the upper left corner of win, win. That's when you have one, one CEO and one

01:02:59.040 --> 01:03:05.200
employee. When you have one CEO and two employees, you don't have two squared outcomes. You have two

01:03:05.200 --> 01:03:10.560
cubed outcomes because you have win, win, win, win, win, lose, win, lose, lose, etc. Right.

01:03:10.640 --> 01:03:15.120
Because all three people can be win or lose. Because CEO can be winner, lose. Employee can

01:03:15.120 --> 01:03:19.280
be winner, lose. Employee number two can be winner, lose. If you have N people, rather than three

01:03:19.280 --> 01:03:23.040
people, you have two to the N possible outcomes and you have essentially a two by two by two by two

01:03:23.040 --> 01:03:27.440
by two by N hypercube of possibilities. Okay. It's all literally just two dimensions on each axis.

01:03:28.400 --> 01:03:32.800
There's tons of possible defecting kinds of things that happen there. So that's why in a large company,

01:03:32.800 --> 01:03:37.920
there's lose, win coalitions that happen where M people gang up on the other K people and they win

01:03:37.920 --> 01:03:41.520
what the other people lose. That's how politics happens. When you've got a startup that's driven

01:03:41.520 --> 01:03:45.840
by equity and the biggest payoff, people don't have to try to think, okay, well, I make more money

01:03:45.840 --> 01:03:49.440
by politics, we'll make more money by the win, win, win, win, win column because the exit makes

01:03:49.440 --> 01:03:53.440
everybody make the most money. That's actually how the opening AI people were able to coordinate

01:03:53.440 --> 01:03:57.920
around. We want an $80 billion company. The economics helped find the sell that was actually

01:03:57.920 --> 01:04:01.280
the most beneficial to all of them helped them coordinate. Okay. So you search that hypercube.

01:04:01.280 --> 01:04:08.560
Okay. That's a point of equity is lining. Still, despite all of this, that that's one of our best

01:04:08.560 --> 01:04:13.360
mechanisms for coordinating large numbers of people in the principal agent problem. Despite all

01:04:13.360 --> 01:04:20.400
of this, the possibility exists for any of these people to win while the others lose, right with

01:04:20.400 --> 01:04:25.520
me so far. And I'll explain why this is important. What that means is those 1000 employees of the

01:04:25.520 --> 01:04:31.760
CEO are their own agents with their own payoff functions that are not perfectly aligned with

01:04:31.760 --> 01:04:36.640
the CEO's payoff function. As such, there are scenarios under which they will defect and do

01:04:36.640 --> 01:04:46.640
other things. Okay. The only way they become like actual limbs, see my hand is not an agent of its

01:04:46.640 --> 01:04:52.160
own. It lives or dies with me. Therefore it does exactly what I'm saying at this time. I tell it

01:04:52.160 --> 01:04:57.040
to go up, it goes up, tell it to go down, it goes down, sideways, sideways, right. An employee is

01:04:57.040 --> 01:05:02.400
not like that. They will do this and this and sideways, sideways up to a certain point. And if

01:05:02.400 --> 01:05:06.400
you, if you have them do something that's extremely against their interests, they will not do your

01:05:06.400 --> 01:05:11.520
action. Do you understand my point? Okay. That is the difference between an AI hypnotizing humans

01:05:12.160 --> 01:05:17.200
versus an AI controlling drones. AI controlling drones is like your hands. They're actually

01:05:17.200 --> 01:05:21.680
pieces of your body. There's no defecting. There's no loose wind. They have no mind of their own.

01:05:21.680 --> 01:05:24.880
They're literally taking instructions. Okay. They have no payoff function. They will

01:05:24.880 --> 01:05:30.320
kill themselves for the hoard. Right. An AI hypnotizing humans has a thousand principal

01:05:30.320 --> 01:05:35.600
Asian problems for every thousand humans. And it has to incentivize them to continue and

01:05:35.600 --> 01:05:40.720
has to generate huge payoffs. It's like an AI CEO. That's really hard to do. Right. The history

01:05:40.720 --> 01:05:45.440
of evolution shows us how hard it is to coordinate multicellular organisms. You have to make them

01:05:45.440 --> 01:05:50.320
all live or die as one. Then you get something along these lines. Like an ant colony can coordinate

01:05:50.320 --> 01:05:54.400
like that because if the queen doesn't reproduce all the ants, it doesn't matter what they're

01:05:54.400 --> 01:05:59.920
having sort of genetic material. Okay. We are not currently set up for those humans to not be able

01:05:59.920 --> 01:06:05.440
to reproduce unless the AI reproduces. Do I think we eventually get to a configuration like that?

01:06:05.440 --> 01:06:11.840
Maybe. Where you have an AI brain is at the center of civilization and it's coordinating all the

01:06:11.840 --> 01:06:17.280
people around it. And every civilization that makes it is capable of crowdfunding and operating

01:06:17.280 --> 01:06:22.160
its own AI. That gets me to my other critique of the AI safety guys. I mentioned that the first

01:06:22.160 --> 01:06:26.480
critique is very theoretical rather than empirical. The second critique is their Abrahamic rather than

01:06:27.120 --> 01:06:32.800
Dharmic or Sinic. Okay. And you know, our background culture influences things in ways we don't even

01:06:32.800 --> 01:06:38.560
think about. So much of the paperclip thinking is like a vengeful God will turn you into pillars

01:06:38.560 --> 01:06:43.440
of salt, except it's a vengeful, you know, AI God will turn you into paperclips. Okay.

01:06:44.080 --> 01:06:49.440
The polytheistic model of many gods as opposed to one God is we're all going to have our own AI

01:06:49.440 --> 01:06:54.720
gods and there'll be war of the gods like Zeus and Hera and so on. That's the closest western

01:06:54.720 --> 01:06:58.800
version, you know, the paganism that predated, you know, Abrahamic religions, but that's still

01:06:58.800 --> 01:07:03.200
there in India. That's still how Indians think. That's why India is sort of people got so woke

01:07:03.200 --> 01:07:07.440
that they don't even make large scale cultural generalizations anymore. But it's true that India

01:07:07.440 --> 01:07:14.640
is just culturally more amenable to decentralization to, you know, multiple gods rather than one

01:07:14.640 --> 01:07:20.240
God in one state. Okay. And then the Chinese model is yet the opposite, like they have like,

01:07:20.240 --> 01:07:23.840
I mean, of course they have their tech entrepreneurs and so on, but they're, if India is more

01:07:23.840 --> 01:07:27.520
decentralized, China is more centralized, they have like one government and one leader for the

01:07:27.520 --> 01:07:33.360
entire civilization. Okay. And, and that the biggest thing that China has done over the last 20 or

01:07:33.360 --> 01:07:38.640
30 years is they've taken various, you know, U.S. things and they've made sure that they have their

01:07:38.640 --> 01:07:43.040
own Chinese version where they have root. So they take U.S. social media and they made sure they had

01:07:43.040 --> 01:07:47.920
root over Sina Weibo. Okay. They make sure they have their own Chinese version of electric cars,

01:07:47.920 --> 01:07:53.280
the most Chinese version. So the private keys in the sense are with G. So that means that they also

01:07:53.920 --> 01:07:58.960
at a minimum, you combine these two things, you're at a minimum going to get polytheistic AI

01:07:58.960 --> 01:08:04.160
of the U.S. and Chinese varieties. And then you add the Indian version on it and you're going to get

01:08:04.160 --> 01:08:08.640
quite a few of these different AIs around there. And then you have War of the Gods where maybe

01:08:08.640 --> 01:08:14.480
they are good at coordinating humans who, who, you know, take instructions from them,

01:08:14.480 --> 01:08:19.360
but they can't live without the humans. And humans are giving input to them. That's a series of

01:08:19.360 --> 01:08:23.360
things I could probably make that clearer if I just laid it out in bullets in an essay, but just

01:08:23.360 --> 01:08:30.080
to recap it, A, technical reasons like chaos, turbulence, cryptography, why AI is limited

01:08:30.080 --> 01:08:36.320
in its ability to predict timeframes and to solve equations, B, practical limits. And AI cannot

01:08:36.320 --> 01:08:42.960
easily be a Stuxnet because Microsoft and Google and Apple can install software on a billion devices

01:08:42.960 --> 01:08:48.320
and just kill it, right? Like basically guys with torches come. All right. It can't easily live off

01:08:48.320 --> 01:08:52.560
the land without humans because they would need hundreds of millions of autonomous robots out there

01:08:52.560 --> 01:08:58.000
to control, to mine the ore and, and set the data centers. It can't just hypnotize humans

01:08:58.000 --> 01:09:01.920
like it can control drones because of the principal agent problem and the degree of human

01:09:01.920 --> 01:09:07.440
defection. To make those humans do that, you'd have to have such massive alignment between the AI

01:09:07.440 --> 01:09:11.680
and humans that the humans all know they'll die if the AI dies and vice versa. We're not there.

01:09:11.680 --> 01:09:15.280
Maybe we'll be there in like, I don't know, n number of years, but not for a while. That's a

01:09:15.280 --> 01:09:21.280
total change in like how states are organized. Okay. Finally, let me just talk about the physics

01:09:21.280 --> 01:09:27.120
a little bit more. There's a lot of stuff which is talked about at a very sci-fi book level of

01:09:27.120 --> 01:09:31.920
it'll just invent nanomedicine and nanotech and kill us all and so on and so forth. Now look,

01:09:31.920 --> 01:09:35.600
I like Robert Freitas, obviously Richard Feynman's a genius and so on and so forth,

01:09:36.160 --> 01:09:42.320
but nanotech somehow hasn't been invented yet. Okay. Meaning that, you know, there's a lot of

01:09:42.320 --> 01:09:48.800
chemists that have worked in this area. Okay. And a lot of what nanotech is like rebranded chemistry

01:09:48.800 --> 01:09:54.720
because those are the molecular machines, you know, for example, DNA polymerase or ribosome,

01:09:54.720 --> 01:09:58.400
those are molecular machines that we can get to work at that scale, the evolved ones.

01:09:59.120 --> 01:10:02.640
To my knowledge, and I may be wrong about this, I haven't looked at it very, very recently,

01:10:03.280 --> 01:10:07.840
we haven't actually been able to make artificial, you know, replicators of the stuff that they're

01:10:07.840 --> 01:10:12.960
talking about, which means it's possible that there's some practical difficulty that intervened

01:10:12.960 --> 01:10:17.920
between Feynman and Freitas and so on's calculations, right? Just a sheer fact that those

01:10:17.920 --> 01:10:22.320
books have came out decades ago and no progress has been made indicates that maybe there's a road

01:10:22.320 --> 01:10:26.640
block that wasn't contemplated, right? So you can't just click your fingers and say, boom,

01:10:26.640 --> 01:10:30.880
nanomus, and it's sort of like clicking your fingers and saying, boom, time travel, right?

01:10:30.880 --> 01:10:35.360
Nanomus and exists. That was a good poke that I had a while ago in a conversation like this,

01:10:35.360 --> 01:10:39.520
where the AI guy, AI safety guy on their side was like, well, time travel, that's too implausible.

01:10:39.520 --> 01:10:44.400
I'm like, yeah, but you're waiting on the nanotech thing you're thinking is like here,

01:10:44.400 --> 01:10:49.280
and you're making so many assumptions there that I want to actually see some more work there. I

01:10:49.280 --> 01:10:54.000
want to actually see that nanotech is actually more possible than you think it is. As for, oh,

01:10:54.000 --> 01:10:58.560
we just need to mix things in a beaker and make a, you know, virus and so on. You know what is

01:10:58.560 --> 01:11:03.920
really, really good at defending against novel viruses, like the human immune, that's something

01:11:03.920 --> 01:11:10.240
that's within envelope, right? Like you have evolved to not die and to fight off viruses. Is it possible

01:11:10.240 --> 01:11:16.320
that maybe you could make some super virus? I mean, maybe, but again, like humans are really good

01:11:16.320 --> 01:11:20.240
and the immune system is really good at that kind of thing. That is what we're set up to do,

01:11:20.240 --> 01:11:25.040
right? To adapt to that billions of years of evolution being set up. Physical constraints

01:11:25.840 --> 01:11:30.800
are not really contemplated when people talk about these super powerful mathematical constraints,

01:11:30.800 --> 01:11:34.560
practical constraints are not contemplated. And I could give more, but I think that was a lot

01:11:34.640 --> 01:11:39.760
right there. Let me pause here. Yeah. Let me try to steal man a few things. And then

01:11:41.040 --> 01:11:43.920
I do think, you know, it's before too long, I want to kind of get back to the

01:11:45.600 --> 01:11:49.520
somewhat less, you know, radically transformative scenarios and ask a few follow up questions on

01:11:49.520 --> 01:11:54.080
that too. But I think for starters, I would say the, the sort of Eleazar, you know, he's updated

01:11:54.080 --> 01:11:57.680
his thinking over time as well. And I would say probably doesn't get quite enough credit for it

01:11:57.680 --> 01:12:02.640
because he's definitely on record, you know, repeatedly saying, yeah, I was kind of expecting

01:12:02.720 --> 01:12:08.400
more something from like the deep mind school to pop out and be, you know, wildly overpowered

01:12:08.960 --> 01:12:14.800
very quickly. And on the contrary, it seems like we're in more of a slow takeoff type of scenario

01:12:14.800 --> 01:12:19.680
where, you know, we've got these, again, like super high surface area kind of suck up all the

01:12:19.680 --> 01:12:23.920
knowledge, gradually get better at everything. Some surprises in there, you know, certainly some

01:12:23.920 --> 01:12:29.200
emergent properties, if you will accept that term, you know, surprise surprises to the developers

01:12:29.200 --> 01:12:34.880
of nothing else, right, that are definitely things we don't fully understand. But it does seem to be a,

01:12:34.880 --> 01:12:40.880
you know, more gradual turning up of capability versus some like, you know, super sudden surprise.

01:12:41.920 --> 01:12:48.320
But okay, so then what is the alternative? I'm going to try to kind of give you the what I,

01:12:48.320 --> 01:12:57.040
what I think of as the most consensus strongest scenario where humans lose track of the future

01:12:57.680 --> 01:13:02.080
and or lose control of the future, maybe starting by kind of losing track of the president and then

01:13:02.080 --> 01:13:06.080
having that kind of, you know, give way to losing control of the future. And I think within that,

01:13:06.080 --> 01:13:11.920
by the way, the, I'm not really one who cares that much about like, whether AIs say something

01:13:11.920 --> 01:13:17.440
offensive today, I'm not easily offended and like, whatever. That's not, that's not world ending. I

01:13:17.440 --> 01:13:21.520
understand your point. That's not like, who cares, whatever, that's within scope, that's within envelope.

01:13:21.520 --> 01:13:25.920
Within within this bigger kind of, you know, what is the real, you know, most likely

01:13:27.520 --> 01:13:33.120
path to like AI disaster, as understood, I think by the smartest people today, I think that is

01:13:33.120 --> 01:13:38.880
still a useful leading indicator, because it's like, okay, the developers, you know, whether you

01:13:38.880 --> 01:13:41.840
agree with their politics, whether you agree with their, whether you think their commercial

01:13:41.840 --> 01:13:47.920
reasons are their sincere reasons or not, they have made it a goal to get the AI to not say

01:13:47.920 --> 01:13:51.600
certain things, right? They don't want it to be offensive. The most naive, you know, kind of

01:13:51.600 --> 01:13:55.120
down the fairway interpretation of that is like, hey, they want to sell it to corporate customers.

01:13:55.120 --> 01:13:59.280
They know that their corporate customers don't want, you know, to have their AI saying offensive

01:13:59.280 --> 01:14:05.440
things. So they don't want to say offensive things. And yet they can't really control it. It's like

01:14:05.440 --> 01:14:11.920
still pretty easy to break. So I view that as just kind of a leading indicator of, okay, we've seen

01:14:11.920 --> 01:14:19.680
GPT two, three and four over the last four years. And that's, you know, a big delta in capability.

01:14:20.240 --> 01:14:26.560
How much control have we seen developed in that time? And does it seem to be keeping pace?

01:14:27.120 --> 01:14:32.640
And my answer would be on the face of it, it seems like the answer is no, you know, we, we don't

01:14:32.640 --> 01:14:39.360
have the ability to really dial in the behavior such that we can say, okay, you're going to,

01:14:39.360 --> 01:14:44.480
you know, you can expect, you can trust that these AIs will like not do, you know, aviancy.

01:14:45.200 --> 01:14:48.720
On the contrary, it's like, if you're a little clever, you know, you can get them to do it.

01:14:48.720 --> 01:14:50.240
You can break out of the sandbox on it.

01:14:50.800 --> 01:14:54.080
Yeah. And it's not even like, I mean, we've talked about, you know, things where you have

01:14:54.080 --> 01:14:57.600
access to the weights and you're doing like counter optimizations, but you don't even need that,

01:14:57.600 --> 01:15:02.080
you know, the kind of stuff I do in like my red teaming in public is literally just like

01:15:02.800 --> 01:15:07.200
feed the AI a couple of words, put a couple of words in its mouth, you know, and it will kind

01:15:07.200 --> 01:15:11.920
of carry on from there. So with that in mind is just a leading indicator. You know, I don't know

01:15:11.920 --> 01:15:16.240
how powerful the most powerful AI systems get over the next few years, but it seems very plausible

01:15:16.240 --> 01:15:21.920
to me that it might be as powerful as like an Elon Musk type figure, you know, somebody who's

01:15:21.920 --> 01:15:27.680
like really good at thinking from first principles, really smart, you know, really dynamic across a

01:15:27.680 --> 01:15:34.320
wide range of different contexts. And, you know, he's not powerful enough to like in and of himself

01:15:34.320 --> 01:15:40.160
take over the world, but he is kind of becoming transformative. Now imagine that you have that

01:15:40.160 --> 01:15:46.480
kind of system, and it's trivial to replicate it. So, you know, if you have like one Elon Musk,

01:15:46.480 --> 01:15:51.520
all of a sudden you can have arbitrary, you know, functionally arbitrary numbers of Elon Musk power

01:15:51.520 --> 01:15:56.880
things that are clones of each other. Maybe I can pause you there. So that's my polytheistic AI

01:15:56.880 --> 01:16:02.160
scenario. But here's the thing that is this is background, but I want to push it to foreground.

01:16:03.120 --> 01:16:08.080
You still have a human typing in things into that thing. The human is doing the jailbreak, right?

01:16:08.080 --> 01:16:12.880
What we're talking about is not artificial intelligence in the sense of something separate

01:16:12.880 --> 01:16:18.960
from a human, but amplified intelligence. Amplified intelligence, I very much believe in. The reason

01:16:18.960 --> 01:16:24.320
is amplified intelligence. So here's something that people may not know about humans. There's this

01:16:24.480 --> 01:16:32.800
great book, Cooking Made Us Human. Okay, tool use has shifted your biology in the following way.

01:16:32.800 --> 01:16:37.920
For example, I know I'll map it to the present day. This book by Richard Rang and Cooking Made Us

01:16:37.920 --> 01:16:44.560
Human, where the fact that we started cooking and using fire meant that we could do metabolism

01:16:44.560 --> 01:16:52.240
outside the body, which meant it freed up energy for more brain development. Okay,

01:16:52.240 --> 01:16:56.080
similarly, developing clothes meant that we didn't have to evolve as much

01:16:56.080 --> 01:17:01.360
fur, again, more energy for brain development. Evolving tools meant we didn't have as much

01:17:01.360 --> 01:17:06.720
fangs and claws and muscles, again, more energy for brain development, right? So encephalization

01:17:06.720 --> 01:17:12.960
quotient rose as tool use meant that we didn't have to do as much natively and we could push

01:17:12.960 --> 01:17:18.960
more to the machines. In a very real sense, we have been a man machine symbiosis since the

01:17:18.960 --> 01:17:26.160
invention of fire and the stone axe and clothes, right? You do not exist as a human being on your

01:17:26.160 --> 01:17:31.840
own like the entire Ted Kaczynski concept of like living in nature by itself. Humans are social

01:17:31.840 --> 01:17:38.400
organisms that are adapted to working with other humans and using tools and you have for and we

01:17:38.400 --> 01:17:43.520
have been for millennia. Okay, this goes back, not just human history, but like hundreds of

01:17:43.520 --> 01:17:49.760
thousands years before 100 gatherers are using tools. Okay, so what that means is man machine

01:17:49.760 --> 01:17:55.920
symbiosis is not some new thing. It's actually the old thing that broke us away from other

01:17:55.920 --> 01:18:01.120
primate lineages that weren't using tools. Okay, this is the fundamental difference between what I

01:18:01.120 --> 01:18:07.280
call Uncle Ted and Uncle Fred. Uncle Ted is Ted Kaczynski. It's a unabomber. It's a doomer. It's

01:18:07.280 --> 01:18:12.400
a decelerator, the de grother who thinks we need to go back to Gaia and Eden and become monkeys

01:18:12.400 --> 01:18:18.960
and live in the jungle like, like, you know, Ted Kaczynski, right? The unabomber cell. Uncle Fred

01:18:18.960 --> 01:18:25.280
is Friedrich Nietzsche, right? Nietzsche and we must get the stars and become ubermen and so

01:18:25.280 --> 01:18:29.920
on and so forth. This I think is going to become, and I actually tweeted about this years ago before

01:18:29.920 --> 01:18:36.400
current AI debates, that, you know, between anarcho primitivism, de growth, deceleration,

01:18:36.400 --> 01:18:44.640
okay, on the one hand, and transhumanism and acceleration and human 2.0 and human self-improvement

01:18:44.640 --> 01:18:49.680
and make it just the stars, on the other hand, this is the future political axis, the current one.

01:18:50.320 --> 01:18:55.760
And roughly speaking, you can cut, it's not really left and right because you'll have both

01:18:55.760 --> 01:19:00.800
left status and right conservatives go over here. You know, left states will say it's against

01:19:00.800 --> 01:19:04.480
the state and the right states will say, the right conservatives say it's against God,

01:19:04.480 --> 01:19:08.400
okay, and you'll have left libertarians and right libertarians over here,

01:19:08.400 --> 01:19:12.640
where left libertarians say it's my body and, you know, the right libertarians say it's my,

01:19:12.640 --> 01:19:18.080
you know, my money, right? And so that is a re-architecting of the political axis where,

01:19:18.080 --> 01:19:21.360
you know, Uncle Ted and Uncle Fred, which is kind of a clever way of putting it, okay?

01:19:22.640 --> 01:19:27.840
And the problem with the Uncle Ted guys in my view is, as I said, yeah, if they go and want to live

01:19:27.840 --> 01:19:33.200
in the, you know, the woods, fine, go get them. But once you start having even like a thousand,

01:19:33.200 --> 01:19:38.320
forget a thousand, a hundred people doing that, your trees will very quickly get exfoliated,

01:19:38.320 --> 01:19:42.800
you know, the leaves are going to get all picked off of them. Humans are not set up to just literally

01:19:42.800 --> 01:19:46.960
live in the jungle right now. You've had hundreds of thousands of years of evolution that have

01:19:46.960 --> 01:19:52.320
driven you in the direction of tool use, social organisms, farming, etc., etc. The man machine

01:19:52.320 --> 01:19:57.840
symbiosis is not today, it's yesterday and the day before and 10,000 years ago and 100,000 years ago.

01:19:58.400 --> 01:20:02.160
And how do we know we've got man machine symbiosis? Can you live without

01:20:03.440 --> 01:20:07.360
even if you're not living, even if you're not using the stove, somebody's using the stove

01:20:07.360 --> 01:20:12.960
to make you food, right? Can you live without the tractors that are digging up the grains? Can you

01:20:12.960 --> 01:20:18.400
live without indoor heating? Can you live without your clothes? Frankly, can you do your work without

01:20:18.400 --> 01:20:24.320
your phone, without your computer? No, you can't. You are already a man machine symbiosis. Once we

01:20:24.320 --> 01:20:29.840
accept that, then the question is, what's the next step? And right now, we're in the middle of that

01:20:29.840 --> 01:20:35.920
next step, which is AI is amplified intelligence. So what you're talking about is not that the AI

01:20:35.920 --> 01:20:43.200
is Elon Musk, it is that the AI human fusion means there's another 20 Elon Musk's or whatever the

01:20:43.200 --> 01:20:50.880
number is, okay? And that's good. That's fine. That's within envelope. That's just a bunch of smarter

01:20:50.880 --> 01:20:56.560
humans on the planet. That is amplified intelligence. That is more like, you know, I mentioned the tool

01:20:56.560 --> 01:21:00.800
thing. Okay, the other analogy would be like a dog. You know, a dog is man's best friend.

01:21:01.840 --> 01:21:08.960
Right? So that AI does not live without humans can turn it off. They have to power it. They have

01:21:08.960 --> 01:21:13.760
to give it substance, right? Eventually, that might become like a ceremonial thing, like this is our

01:21:13.760 --> 01:21:19.520
God that we pray to, right? Because it's wiser and smarter than us and it appears in an image.

01:21:19.520 --> 01:21:24.240
But the priests maintain it. You know, just like you go to a Hindu temple or something like that,

01:21:24.320 --> 01:21:28.000
and the priests will pour out the ghee, you know, for the fires and so on and so forth. And then

01:21:28.000 --> 01:21:32.960
everybody comes in and prays, okay? The priests believe in the whole thing, but they also maintain

01:21:32.960 --> 01:21:37.440
the back of the house. They do the system administration for the temple. Same, you know,

01:21:37.440 --> 01:21:43.360
in a Christian church, right? The, the, you know, like, it's not like it appears out of nowhere.

01:21:43.360 --> 01:21:50.240
Somebody, you know, went and assembled this cathedral, right? They saw the back of the house,

01:21:50.240 --> 01:21:54.000
the fact that it was just woods and rocks and so on that came together. Then when people come

01:21:54.000 --> 01:21:58.480
there, it feels like a spiritual experience. You see what I'm saying? Okay. So the equivalent of that,

01:21:58.480 --> 01:22:04.160
the priests or the, you know, the people maintaining temples, cathedrals, mosques, whatever, is

01:22:06.080 --> 01:22:11.760
engineers who are maintaining these future AIs, which appear to you as Jesus. They appear to you,

01:22:11.760 --> 01:22:16.160
maybe even a hologram. Okay, you come there, you ask it for guidance as an oracle. You've also got

01:22:16.160 --> 01:22:22.560
the personal version on your phone. You ask it for guidance. But guess what? You're still a human

01:22:23.120 --> 01:22:28.080
AI symbiosis until and unless the AI actually has the terminator scenario where it's got

01:22:28.080 --> 01:22:32.400
lots of robots that can live on its own. I'm not saying that's physically impossible. I did give

01:22:32.400 --> 01:22:37.200
some constraints on it earlier, but for a while we're not going to be there. So that alone means

01:22:37.200 --> 01:22:40.960
it's not fume because we don't have lots of drones running around. The AI has to be with

01:22:40.960 --> 01:22:47.120
the human. It's a human AI symbiosis. It's not AI Elon Musk. It is human AI fusion that becomes

01:22:47.120 --> 01:22:50.800
Elon Musk. And frankly, that's not that different from what Elon Musk himself is. Elon Musk would

01:22:50.800 --> 01:22:54.720
not be Elon Musk without the internet. Without the internet, you can't tweet and reach 150 million

01:22:54.720 --> 01:23:01.040
people. The internet itself made Elon what he is. And so this is the next version of that.

01:23:01.040 --> 01:23:04.400
Maybe there's now 30 Elon's because the AI makes the next 30 Elon's.

01:23:04.400 --> 01:23:11.120
Yeah, I mean, again, I think I'm largely with you with just this one very important nagging worry

01:23:11.120 --> 01:23:15.680
that's like, what if this time is different because what if these systems are getting

01:23:16.560 --> 01:23:23.520
so powerful so quickly that we don't really have time for that techno human fusion

01:23:23.520 --> 01:23:27.280
to really work out? And I'll just give you kind of a couple of data points on that.

01:23:28.640 --> 01:23:32.880
You said it's still somebody putting something into the AI. Well, sort of, right? I mean,

01:23:32.880 --> 01:23:38.320
already we have these proto agents and the super simple scaffolding of an agent is just

01:23:38.880 --> 01:23:45.840
run it in a loop, give it a goal, and have it kind of pursue some like plan, act, get feedback,

01:23:45.840 --> 01:23:52.000
and loop type of structure, right? It doesn't seem to take a lot. Now, they're not smart enough yet

01:23:52.000 --> 01:23:58.000
to accomplish big things in the world. But it seems like the language model to agent

01:23:58.880 --> 01:24:05.520
switch is less one right now that is gated by the structure or the architecture and more one

01:24:05.520 --> 01:24:10.160
that's just gated by the fact that like the language models when framed as agents just aren't

01:24:10.160 --> 01:24:14.480
that successful at like doing practical things and getting over hump. So they tend to get stuck.

01:24:15.440 --> 01:24:18.800
But it doesn't seem that hard to imagine that like, you know, if you had something that is sort

01:24:18.800 --> 01:24:25.040
of that next level that you put it into a loop, you say, okay, you're Elon Musk, LLM, and your

01:24:25.040 --> 01:24:31.600
job is to like make, you know, us, whatever us exactly is a, you know, multi planetary species.

01:24:31.600 --> 01:24:37.360
And then you just kind of keep updating your status, keep updating your plans, keep trying

01:24:37.360 --> 01:24:43.280
stuff, keep getting feedback. And, you know, like what really limits that.

01:24:44.160 --> 01:24:50.400
There may be like a really good program. But the whole AI kills everyone thing is so it's like,

01:24:51.040 --> 01:24:58.960
where's the actuator? Okay, I hit enter. What kills me? Right? Is it a hypnotized human who's

01:24:58.960 --> 01:25:04.480
being hypnotized by an AI that he's typed into? And he's radicalized himself by typing into a

01:25:04.480 --> 01:25:08.800
computer? Okay, that's not that different from a lot of other things that have happened in the past.

01:25:08.800 --> 01:25:14.000
Right. So who is actually striking me? Right? Who's striking the human? It's another human within

01:25:14.000 --> 01:25:18.960
acts that he's been radicalized by an AI. Okay, he's not actually that's not even the right term.

01:25:18.960 --> 01:25:24.640
We're giving agency to the AI when it's not really an agent. It is a human who's self radicalized by

01:25:24.640 --> 01:25:31.120
typing into a computer screen and has hit another human. That's one scenario. The other scenario is

01:25:31.120 --> 01:25:35.840
it's literally a Skynet drone that's hitting you. Those are the only two. How else is it going to be

01:25:35.840 --> 01:25:41.440
physical? Right? How does it? The actuation step is a part that is skipped over and it's a non trivial

01:25:41.440 --> 01:25:46.160
step. Well, I think it could be lots of things, right? I mean, if it's not one of those two,

01:25:46.160 --> 01:25:52.320
if it's not human or drone hitting you, what is it? Just habitat degradation, right? I mean,

01:25:52.320 --> 01:25:56.640
how do we kill most of the other species that we drive to extinction? We don't go out and hunt them

01:25:56.640 --> 01:26:01.920
down with axes one by one. We just change the environment more broadly to the point where

01:26:01.920 --> 01:26:06.720
it's not suitable for them anymore and they don't have enough space and they kind of die out, right?

01:26:06.720 --> 01:26:11.760
So we did hunt down some of the megafauna literally one by one with spears and stuff,

01:26:11.760 --> 01:26:18.720
but most of the recent loss of species is just like we're out there just extracting resources for

01:26:18.720 --> 01:26:24.240
our own purposes. And in the course of doing that, you know, whatever bird or whatever, you know,

01:26:24.240 --> 01:26:29.360
thing just kind of loses its place and then it's no more. And I don't think that's like totally

01:26:29.360 --> 01:26:35.280
implausible. Wait, so that is though, I think within normal world, right? What does that mean?

01:26:35.280 --> 01:26:42.400
That means that some people, some amplified intelligence, and maybe we might call it HAI,

01:26:42.400 --> 01:26:49.440
okay, human plus AI combination, right? Some HAIs outcompete others economically and they lose

01:26:49.440 --> 01:26:54.080
their jobs. Is that what you're talking about? I think also the humans potentially become

01:26:54.080 --> 01:26:59.520
unnecessary in a lot of the configurations, like just a recent paper from DeepMind.

01:26:59.520 --> 01:27:05.280
It's your marginal product workers. Or negative. Yeah, I mean, so the last, you know, DeepMind has

01:27:05.280 --> 01:27:12.320
been on Google, Google DeepMind has been on a tear of increasingly impressive medical AIs.

01:27:12.320 --> 01:27:18.160
Their most recent one takes a bunch of difficult case studies from the literature. I mean, case

01:27:18.160 --> 01:27:24.080
studies, you know, this is like rare diseases, hard to diagnose stuff, and asks an AI to do the

01:27:24.080 --> 01:27:30.560
differential diagnosis, compares that to human and compares it to human plus AI. And they phrase

01:27:30.560 --> 01:27:36.240
their results like in a very understated way. But the headline is the AI blows away the human

01:27:36.240 --> 01:27:42.480
plus AI. The human makes the AI worse. So here's the thing. I'll say something provocative, maybe.

01:27:42.480 --> 01:27:48.800
Okay, like I have in a very fine. I do think that the ABCs of economic apocalypse for blue America

01:27:48.800 --> 01:27:54.480
are AI, Bitcoin and China, where AI takes away their, a lot of the revenue streams, the licensures

01:27:54.480 --> 01:27:59.920
that have made medical and legal costs and other things so high. Bitcoin takes away the power of

01:27:59.920 --> 01:28:06.160
money and China takes away their military power. So I've received total meltdown for blue America

01:28:07.280 --> 01:28:12.640
in the years and, you know, maybe decade to come already kind of happening. But that's different

01:28:12.640 --> 01:28:18.160
than being at the end of the world, right? Like blue America had a really great time for a long time

01:28:18.160 --> 01:28:23.920
and they've got these licensure locks. But because of that, they've hyperinflated the cost of medicine.

01:28:24.720 --> 01:28:30.160
It's like how much, so what you're talking about is, wow, we have infinite free medicine.

01:28:30.720 --> 01:28:34.320
Man, Dr. Billing events are going to get ahead. That's the point.

01:28:35.040 --> 01:28:39.360
Yeah. And to be clear, I'm really with you on that too. Like I want to see one of the things,

01:28:39.360 --> 01:28:44.240
when people say like, what is good about AI, you know, why should we, why should we pursue this?

01:28:44.960 --> 01:28:51.360
This, my standard answer is high quality medical advice for everyone at pennies,

01:28:51.440 --> 01:28:56.000
you know, per visit, right? It is orders of magnitude cheaper. We're already starting to

01:28:56.000 --> 01:29:00.080
see that in some ways it's better. People prefer it, you know, that AI is more patient,

01:29:00.080 --> 01:29:05.360
it has better bedside manner. I wouldn't say, you know, if I was giving my, you know, my own

01:29:05.360 --> 01:29:10.560
family advice today, I would say use both a human doctor and an AI, but definitely use the AI as

01:29:10.560 --> 01:29:15.200
part of your mix. Absolutely. That's right. That's right. But you're prompting it still, right?

01:29:15.200 --> 01:29:20.480
The smarter you are, the smarter the AI is, you notice this immediately with your vocabulary,

01:29:20.480 --> 01:29:24.800
right? The more sophisticated your vocabulary, the finer the distinctions you can have,

01:29:24.800 --> 01:29:29.600
the better your own ability to spot errors. You can generate a basic program with it, right?

01:29:29.600 --> 01:29:33.280
But really amplified intelligence is I think a much better way of thinking about it,

01:29:33.280 --> 01:29:38.560
because whatever your IQ is, it surges it upward by a factor of three or whatever the number.

01:29:38.560 --> 01:29:43.360
And maybe the amplifier increases with your intelligence, but that, that internal intelligence

01:29:43.360 --> 01:29:47.280
difference still exists. It's just like what a computer is, a computer is an amplifier for

01:29:47.280 --> 01:29:52.320
intelligence. If you're smart, you can hit enter and programs can go to like, like thinking about

01:29:52.320 --> 01:29:58.720
the Minecraft guy, right? Or Satoshi, one person built a billion or an associated trillion dollar

01:29:58.720 --> 01:30:04.880
thing, you know, obviously other people continued Bitcoin and so on and so forth, right? So what I

01:30:04.880 --> 01:30:11.280
feel though is this is what I mean by going from nuclear terrorism to the TSA. Okay, we went from

01:30:11.280 --> 01:30:16.800
AI will kill everyone. And I'm like, what's the actuator to, okay, it'll gradually to greater

01:30:16.800 --> 01:30:19.680
environment. What does that mean? Okay, some people lose their jobs, but then we're back in normal

01:30:19.680 --> 01:30:23.600
worlds. Well, hold on, let me paint a little bit more complete picture because I don't think we're

01:30:23.600 --> 01:30:31.040
quite there yet. So I think the differential diagnosis, recent paper, that's just a data point

01:30:31.040 --> 01:30:35.520
where it's kind of like chess. This, you know, this came long before, right? There was a period

01:30:35.520 --> 01:30:39.520
where humans are the best chess players, then there was a period where the best were the hybrid

01:30:39.520 --> 01:30:45.120
human AI systems. And now as far as I understand it, we're in a regime where the human can't really

01:30:45.120 --> 01:30:50.640
help the AI anymore. And so the AIs are, you know, the best chess players are just pure AIs.

01:30:50.640 --> 01:30:55.440
We're not there in medicine, but we're starting to see examples where, hey, in a pretty defined study

01:30:56.000 --> 01:31:01.040
differential diagnosis, the AI is beating, not just beating the humans, but also beating the AI

01:31:01.040 --> 01:31:07.680
human hybrid or the human with access to AI. So, okay, that's not it, right? There's a paper

01:31:07.680 --> 01:31:17.200
recently called Eureka out of NVIDIA. This is Jim Fan's lab where they use GPT-4 to write the reward

01:31:17.200 --> 01:31:24.160
functions to train a robot. So you want to train a robot to like twirl a pencil in fingers. Hard,

01:31:24.160 --> 01:31:28.880
you know, hard for me to do. Robots, you definitely can't do it. How do you train that? Well, you

01:31:28.880 --> 01:31:33.040
need a reward function, the reward function. Basically, while you're in the early process

01:31:33.040 --> 01:31:38.080
of learning and failing all the time, the reward function gives you encouragement when you're on

01:31:38.080 --> 01:31:42.400
the right track, right? So there are people who, you know, have developed this skill and you might

01:31:42.400 --> 01:31:46.720
do something like, well, if the pencil has angular momentum, you know, then that seems like you're

01:31:46.720 --> 01:31:50.320
on maybe sort of the right track. So give that, you know, a reward, even though at the beginning

01:31:50.320 --> 01:31:56.000
you're just failing all the time. Turns out GPT-4 is way better than humans at this, right? So it's

01:31:56.000 --> 01:32:01.760
better at training robots. So all of that is awesome. And it's great. And, but here is, here's the thing,

01:32:01.760 --> 01:32:07.120
is there's a huge difference between AI is going to kill everybody and turn everybody into paper

01:32:07.120 --> 01:32:14.880
clips, okay, versus some humans with some AI are going to make a lot more money. And some people

01:32:14.880 --> 01:32:19.120
are going to lose their jobs. Yeah, I'm not scared of that. I'm not scared of that scenario. I mean,

01:32:19.120 --> 01:32:24.160
it could be disruptive. It could be disruptive, but it's not existential under itself.

01:32:25.120 --> 01:32:29.280
Big deal. Okay. So that's why I went, right. There's the, the, the beta, to me, it comes,

01:32:29.280 --> 01:32:34.560
if I, if I ask just one question is what is the actuator, right? You know, sensors and actuators,

01:32:34.560 --> 01:32:40.240
right? What is the thing that's actually going to plunge a knife or a bullet into you and kill

01:32:40.240 --> 01:32:49.200
you? It is either a human who has hypnotized themselves by typing into a computer, like

01:32:49.200 --> 01:32:54.720
basically an AI terrorist, you know, which is kind of where some of the EAs are going in my view,

01:32:54.800 --> 01:33:02.640
or it is like an autonomous drone that is controlled in a starcraft or terminator like way.

01:33:03.280 --> 01:33:08.560
We are not there yet in terms of having enough humanoid or autonomous drones that are intranet

01:33:08.560 --> 01:33:13.840
connected and programmable. That won't be there for some time. Okay. So that alone means fast take

01:33:13.840 --> 01:33:20.480
off is, and what I think by the time we get there, you will have a cryptographic control over them.

01:33:20.480 --> 01:33:25.120
That's a crucial thing. Cryptography fragments the whole space in a very fundamental way.

01:33:25.680 --> 01:33:30.480
If you don't have the private keys, you do not have control over it. So long as that piece of

01:33:30.480 --> 01:33:35.280
hardware, the cryptographic controller, you've nailed the equations on that. And frankly, you can

01:33:35.280 --> 01:33:41.040
use AI to attack that as well to make sure the code is perfect, right? Remember you talked about

01:33:41.040 --> 01:33:48.320
attack and defense? AI is attack crypto's defense, right? Because one of the things that crypto is

01:33:48.320 --> 01:33:54.080
done, do you know the PKI problem is public key infrastructure? I'll say no, on behalf of the

01:33:54.080 --> 01:33:58.480
audience. This is good. We should do more of these actually. I feel it's a good, you know, fusion of

01:33:58.480 --> 01:34:04.800
things or whatever, right? But the public key infrastructure problem, the public key infrastructure

01:34:04.800 --> 01:34:11.040
problem is something that was sort of lots of cryptography papers and computer science papers

01:34:11.040 --> 01:34:17.200
in the 90s and 2000s assumed that this could exist and essentially meant if you could assume that

01:34:17.200 --> 01:34:24.640
everybody on the internet had a public key that was public and a private key that was kept both

01:34:24.640 --> 01:34:29.360
secure and available at all times, then there's all kinds of amazing things you can do with

01:34:29.360 --> 01:34:36.160
privacy preserving, messaging and authentication and so on. The problem is that for many years,

01:34:36.160 --> 01:34:41.120
what cryptographers try to do is they try to nag people into keeping their private keys secure

01:34:41.120 --> 01:34:46.080
and available. And the issue is it's trivial to keep it secure and unavailable where you write

01:34:46.080 --> 01:34:50.480
it down, you put it into a lockbox and you lose the lockbox. It's trivial to keep it available

01:34:50.480 --> 01:34:56.400
and not secure, okay, where you put it on your public website and it's available all the time,

01:34:56.400 --> 01:35:02.480
you never lose it, but it's not secure because anybody can see it. When you actually ask,

01:35:02.480 --> 01:35:07.840
what does it mean to keep something secure and available? That's actually a very high cost.

01:35:07.840 --> 01:35:13.280
It's precious space because it's based on your wallet, right? Your wallet is on your person

01:35:13.280 --> 01:35:19.360
at all times, so it's available, but it's not available to everybody else, so it's secure.

01:35:19.360 --> 01:35:24.800
So you actually have to touch it constantly, yes, right? So it turns out that the crypto wallet

01:35:25.600 --> 01:35:31.680
by adding a literal incentive to keep your private keys secure and available because if they're not

01:35:31.680 --> 01:35:36.320
available, you've lost your money. If they're not secure, you've lost your money, okay? To have both

01:35:36.320 --> 01:35:41.520
of them, that was what solved the PKI problem. Now we have hundreds of millions of people with

01:35:41.520 --> 01:35:46.400
public private key pairs where the private keys are secure and available. That means all kinds

01:35:46.400 --> 01:35:51.280
of cryptographic schemes, zero knowledge stuff, there's this amazing universe of things that is

01:35:51.280 --> 01:35:55.360
happening now. Zero knowledge in particular has made cryptography much more programmable.

01:35:55.360 --> 01:36:00.320
There's a whole topic which is, if you want something that's kind of, you know, like AI was

01:36:00.320 --> 01:36:04.320
creeping for a while and people, specialists were paying attention to it and then just burst

01:36:04.320 --> 01:36:08.640
out on the scene, zero knowledge is kind of like that for cryptography. Thanks to the,

01:36:08.640 --> 01:36:10.960
you know, you've probably heard of zero knowledge before.

01:36:10.960 --> 01:36:20.080
Yeah, we did one episode with Daniel Kong on the use of zero knowledge proofs to basically

01:36:20.960 --> 01:36:25.680
to prove without revealing like the weights that you actually ran the model you said you were

01:36:25.680 --> 01:36:28.000
going to run and things like that, I think are super interesting.

01:36:28.640 --> 01:36:34.960
Exactly, right? So what kinds of stuff, why is that useful in the AI space? Well, first is you

01:36:34.960 --> 01:36:39.280
can use it, for example, for training on medical records while keeping them both private,

01:36:39.280 --> 01:36:44.000
but also getting the data you want. For example, let's say you've got a collection of

01:36:45.040 --> 01:36:54.000
genomes, okay, and you want to ask, okay, how many G's were in this data set, how many C's,

01:36:54.000 --> 01:36:57.520
how many A's, how many T's, okay, like you just say, like, that's a very simple downstairs.

01:36:57.520 --> 01:37:03.840
What's the ACG T content of this, you know, the sequence data set, you could get those numbers,

01:37:03.840 --> 01:37:07.040
you could prove they were correct without giving any information about the individual sequences,

01:37:07.040 --> 01:37:11.120
right, or more specifically, you do it at one locus and you say, how many G's and how many C's

01:37:11.120 --> 01:37:17.920
are at this particular locus and you get the SNP distribution, okay. So it's useful for what you

01:37:17.920 --> 01:37:22.160
just said, which is like showing that you ran a particular model without giving anything else away,

01:37:22.160 --> 01:37:27.680
it's useful for certain kinds of data analysis. There's a lot of overhead on compute on this

01:37:27.680 --> 01:37:30.960
right now, so it's not something that you do trivially, okay, but it'll probably come down with

01:37:30.960 --> 01:37:39.920
time. But what is perhaps most interestingly useful for it is in the context of AI is coming up with

01:37:39.920 --> 01:37:45.840
things in AI can't fake. So what we talked about earlier, right, like an AI can come up with all

01:37:45.840 --> 01:37:52.880
kinds of plausible sounding images, but if it wasn't cryptographically signed by the sender,

01:37:54.080 --> 01:38:00.160
then, you know, it, and it should be signed by sender and put on chain. And then at least you

01:38:00.160 --> 01:38:07.600
know that this person or this entity with this private key asserted that this object existed at

01:38:07.600 --> 01:38:11.680
this time in a way that'd be extremely expensive to falsify because it's either on the Bitcoin

01:38:11.680 --> 01:38:15.760
blockchain or another blockchain, it's very expensive to rewind, okay. This starts to be

01:38:15.760 --> 01:38:21.040
a bunch of facts that an AI can't fake. You know, so going back to the kind of big picture

01:38:21.920 --> 01:38:26.240
loss of control story, I was just kind of trying to build up a few of these data points that like,

01:38:26.240 --> 01:38:31.280
hey, look at this differential diagnosis, we already see like humans are not really adding value

01:38:31.280 --> 01:38:36.880
to AI's anymore. That's kind of striking. And like similarly with training robot hands, GPT-4 is

01:38:36.880 --> 01:38:43.840
outperforming human experts. And by the way, all of the sort of latent spaces are like totally

01:38:43.840 --> 01:38:48.240
bridgeable, right? I mean, one of the most striking observations of the last couple of years of study

01:38:48.240 --> 01:38:54.480
is that AI's can talk to each other in high dimensional space, which we don't really have a

01:38:54.480 --> 01:38:58.720
way of understanding natively, right? It takes a lot of work for us to decode.

01:38:59.440 --> 01:39:03.600
This is like the language thing? We're starting to see AI's kind of develop,

01:39:03.600 --> 01:39:11.600
not obviously totally on their own as of now, but there is becoming an increasingly reliable

01:39:11.600 --> 01:39:18.480
go-to set of techniques if you want to bridge different modalities with like a pretty small

01:39:18.480 --> 01:39:22.880
parameter adapter. That's interesting. Actually, what's a good paper on that? I actually hadn't

01:39:22.880 --> 01:39:26.880
seen that. The Blip family of models out of Salesforce research is really interesting,

01:39:26.880 --> 01:39:31.520
and I've used that in production at Salesforce. Really? Yeah, Salesforce research. They have a

01:39:31.520 --> 01:39:39.680
crack team that has open sourced a ton of stuff in the language model, computer vision, joint space.

01:39:40.560 --> 01:39:46.640
And you see this all over the place now, but basically what they did in a paper called Blip

01:39:46.640 --> 01:39:51.120
2, and they've had like five of these with a bunch of different techniques. But in Blip 2,

01:39:51.840 --> 01:39:56.480
they took a pre-trained language model and then a pre-trained computer vision model,

01:39:57.040 --> 01:40:01.760
and they were able to train just a very small model that kind of connects the two. So you could

01:40:01.760 --> 01:40:09.040
take an image, put it into the image space, then have their little bridge that over to language

01:40:09.040 --> 01:40:13.040
space, and that everything else, the two big models are frozen. So they were able to do this on

01:40:13.040 --> 01:40:18.640
just like a couple days worth of GPU time, which I do think goes to show how it is going to be

01:40:18.640 --> 01:40:23.280
very difficult to contain proliferation. Which is good. In my view, that's really good.

01:40:23.280 --> 01:40:28.000
As long as it doesn't get out of control, I'm probably with you on that too. But by bridging

01:40:28.000 --> 01:40:33.280
this vision space into the language space, then the language model would be able to converse with you

01:40:33.280 --> 01:40:38.960
about the image, even though the language model was never trained on images, but you just had this

01:40:38.960 --> 01:40:44.080
connector that kind of bridges those modalities. It's just, it's like another layer of the network

01:40:44.080 --> 01:40:50.160
that just bridges two networks almost. Yeah, it bridges the spaces. It bridges the conceptual

01:40:50.160 --> 01:40:55.040
spaces between something that has only understood images and something that has only understood

01:40:55.040 --> 01:40:58.960
language, but now you can kind of bring those together. As I think about it, it's not that

01:40:58.960 --> 01:41:04.880
surprising because that's what, for example, text image models are basically that. They're

01:41:04.880 --> 01:41:10.400
bridging two spaces in a sense, right? But I'll check this paper out. So on the one hand,

01:41:10.480 --> 01:41:14.160
it's not that surprising. On the other hand, I should see how they implemented it or whatever,

01:41:14.160 --> 01:41:19.200
so blip two. Okay. Yeah, I think the most striking thing about that is just how small it is. Like,

01:41:19.200 --> 01:41:25.840
you took these two off the shelf models that were trained independently for other purposes,

01:41:26.400 --> 01:41:33.200
and you're able to bridge them with a relatively small connector. And that seems to be kind of,

01:41:33.200 --> 01:41:37.920
you know, happening all over the place. I would also look at the Flamingo architecture,

01:41:37.920 --> 01:41:44.160
which is like a year and a half ago now out of DeepMind. That was one for me where I was like,

01:41:44.160 --> 01:41:48.720
oh my, and it's also a language to vision, where they keep the language model frozen.

01:41:48.720 --> 01:41:54.000
And then they kind of, in my mind, it's like, I can see the person in their garage like tinkering

01:41:54.000 --> 01:41:57.440
with their soldering iron, you know, because it's just like, wow, you took this whole language

01:41:57.440 --> 01:42:02.080
thing that was frozen, and you kind of injected some, you know, vision stuff here, and you added

01:42:02.080 --> 01:42:06.560
a couple layers, and you kind of Frankenstein it, and it works. And it's like, wow, that's not really,

01:42:07.200 --> 01:42:12.240
it wasn't like super principled, you know, it was just kind of hack a few things together and,

01:42:12.240 --> 01:42:15.280
you know, try training it. And I don't want to diminish what they did, because I'm sure there

01:42:15.280 --> 01:42:21.600
were, you know, more insights to it than that. But it seems like we are kind of seeing a reliable

01:42:21.600 --> 01:42:28.000
pattern of the key point here being model to model communication through high dimensional

01:42:28.000 --> 01:42:36.240
space, which is not mediated by human language, is I think one of the reasons that I would expect,

01:42:36.240 --> 01:42:40.480
and by the way, there's lots of papers too on like, you know, language models are human level,

01:42:40.480 --> 01:42:44.560
or even superhuman prompt engineers, you know, they're they're self prompting, like,

01:42:44.560 --> 01:42:48.880
techniques are getting pretty good. So if I'm imagining the big picture of like,

01:42:49.920 --> 01:42:53.440
and we can, you know, get back to like, okay, well, how do we use any techniques crypto or

01:42:53.440 --> 01:42:58.800
otherwise to keep this under control? And then I would say this is kind of the newer school of

01:42:58.800 --> 01:43:04.800
the big picture AI safety worry. Obviously, there's a lot of flavors. But if you were to,

01:43:04.800 --> 01:43:08.480
you know, go look at like a Jay Acatra, for example, I think a really good writer on this.

01:43:09.760 --> 01:43:14.000
Her worldview is less that we're going to have this fume and more that over a period of time,

01:43:14.000 --> 01:43:17.440
and it may not be a long period of time, maybe it's like a generation, maybe it's 10 years,

01:43:17.440 --> 01:43:22.000
maybe it's 100 years. But obviously, those are all small in the sort of, you know, grand scheme of

01:43:22.000 --> 01:43:33.520
the future. We have, in all likelihood, the development of AI centric schemes of production,

01:43:33.520 --> 01:43:37.280
where you've got kind of your high level executive function is like your language model,

01:43:37.280 --> 01:43:41.920
you've got all these like lower level models, they're all bridgeable, all the spaces are bridgeable

01:43:41.920 --> 01:43:46.720
in high dimensional form, where they're not really mediated by language, unless we enforce

01:43:46.720 --> 01:43:52.160
that. I mean, we could say, you know, it must always be mediated by language so we can read the

01:43:52.160 --> 01:43:58.400
logs. But there's a text to that, right? Because going through language is like highly compressed,

01:43:59.120 --> 01:44:03.600
compared to the high dimensional space to space. All right, so let me see if I can

01:44:03.600 --> 01:44:07.680
steal man or articulate your case, you're saying, AI's are going to get good enough,

01:44:07.680 --> 01:44:10.960
they're going to be able to communicate with each other good enough, and they'll be able to do enough

01:44:10.960 --> 01:44:15.520
tasks that more and more humans will be rendered economically marginal and unnecessary.

01:44:15.520 --> 01:44:18.880
I'm not saying I think that will happen, I'm just saying I think there's a good enough chance

01:44:18.880 --> 01:44:23.200
that that will happen, but it's worth taking really seriously. I actually think that will happen,

01:44:23.200 --> 01:44:27.440
something along those lines, or in the sense of at least massive economic disruption,

01:44:27.440 --> 01:44:33.440
definitely. Okay, but I'll give an answer to that, which is both, you know, maybe fun and not fun.

01:44:33.440 --> 01:44:38.800
Have you seen the graph of the percentage of America that was involved in farming?

01:44:39.520 --> 01:44:41.520
Yeah, I tweeted a version of that once.

01:44:42.160 --> 01:44:46.240
I did. Okay, great. Good. So you're familiar with this, and you're familiar with what I mean by

01:44:46.240 --> 01:44:52.640
the implication of it, where basically Americans used to identify themselves as farmers, right?

01:44:52.640 --> 01:45:01.360
And manufacturing rose as agriculture collapsed, right? And here is the graph on that. But from

01:45:01.360 --> 01:45:08.240
like 40% in the year 1900 to like a total collapse of agriculture, and then also more recently a

01:45:08.240 --> 01:45:13.600
collapse of manufacturing into bureaucracy, paperwork, legal work, what is up into the right

01:45:13.600 --> 01:45:21.120
since then is, you know, the lawyers, what is up into the right? What is replacing that?

01:45:21.840 --> 01:45:27.520
Starting in around the 1970s, we used to be adding energy production and energy production

01:45:27.520 --> 01:45:32.720
flatlined once people got angry about nuclear power. So this is a future that could have been,

01:45:32.720 --> 01:45:38.720
we could be on Mars by now, but we got flatlined, right? What did go up into the right? So construction

01:45:38.720 --> 01:45:43.680
costs, this is the bad scenario where the miracle energy got destroyed because regulations,

01:45:44.560 --> 01:45:49.200
the cost was flat. And then when vertical, when regulations were imposed, all the progress was

01:45:49.200 --> 01:45:55.360
stopped by decels and degrowthers. And then Alara was implemented, which said nuclear energy

01:45:55.360 --> 01:46:00.720
has to be as low risk as reasonably necessary, as reasonably achievable. And that meant that you

01:46:00.720 --> 01:46:04.640
just keep adding quote safety to it until it's as same as cost as everything else, which means you

01:46:04.640 --> 01:46:09.440
destroyed the value of it, right? But you know what was up into the right? What replaced those

01:46:09.440 --> 01:46:13.760
agriculture and manufacturing jobs? Look at this, you see this graph? For the audio only, we will

01:46:13.760 --> 01:46:18.320
put this on YouTube. So if you want to see the graph do the YouTube version of this, for the audio

01:46:18.320 --> 01:46:23.040
only group, it's an exponential curve in the number of lawyers in the United States from,

01:46:23.040 --> 01:46:27.760
looks like maybe two thirds of a million to 13 million over the last 140 years.

01:46:27.760 --> 01:46:33.600
Yeah. And in 1880, it was like, like sub 100,000 or something like that, right? And then it's just

01:46:33.600 --> 01:46:39.040
like, especially that 1970 point, that's when it went totally vertical. Okay. And it's probably

01:46:39.040 --> 01:46:44.160
even more since. So, you know, if you add paperwork jobs, bureaucratic jobs, you know,

01:46:44.160 --> 01:46:49.040
every lawyer is like, you know, net, sorry, lawyers, but you're basically negative value add,

01:46:49.040 --> 01:46:54.000
right? Because it should, the fact that you have a lawyer means that you couldn't just self serve a

01:46:54.000 --> 01:46:58.880
form, right? Basic government is platform is where you can just self serve and you fill it out.

01:46:58.880 --> 01:47:03.280
And you don't have to have somebody like code something for you custom, you know, lawyers

01:47:03.280 --> 01:47:08.640
that's doing custom code is because the legal code is so complicated. So, you know, the whole

01:47:08.640 --> 01:47:12.000
Shakespeare thing, like first thing we do, let's, you know, kill all the lawyers. First thing we

01:47:12.000 --> 01:47:17.840
do, let's automate all the lawyers, right? Only something that's the hammer blow of AI

01:47:17.840 --> 01:47:23.360
can break the backbone and it will. That's it's going to break the backbone of blue America,

01:47:23.360 --> 01:47:27.440
right? It's going to cause that's why the political layer and the sovereignty layer

01:47:27.440 --> 01:47:33.120
is not what AI people think about. But it's like crucial for thinking about AI, because what tribes

01:47:33.120 --> 01:47:40.080
does AI benefit? And again, we got away from why does AI kill everybody? Well, it's going to need

01:47:40.080 --> 01:47:43.680
actuators. Who's going to stab you? Who's going to shoot you? It's got to be a human hypnotized

01:47:43.680 --> 01:47:48.560
by AI or a drone that AI controls. A human hypnotized by AI is actually a conventional

01:47:48.560 --> 01:47:52.320
threat. It looks like a terrorist cell. We know how to deal with that, right? It's just like radicalized

01:47:52.320 --> 01:47:56.960
humans that worship some AI that stab you. It's like the pause AI people are one step, I think,

01:47:56.960 --> 01:48:01.360
away from that. All right. But that's just like on Shin Riko. That's like allocated. That's like

01:48:01.360 --> 01:48:06.960
basically terrorists who think that the AI is telling them what to do. Fine. If it's not a human

01:48:06.960 --> 01:48:12.000
that's stabbing you, it is a drone. And that's like a very different future where

01:48:12.880 --> 01:48:16.720
like five or 10 or 15 years up, maybe we have enough internet connected drones out there,

01:48:16.720 --> 01:48:21.680
but even then they'll have private keys. So there's going to be fragmentation of address space.

01:48:21.680 --> 01:48:27.360
Not all drones be controlled by everybody in my view. Okay. That's what AI safety is. AI safety is

01:48:27.360 --> 01:48:32.720
can you turn it off? Can you kill it? Can you stop it from controlling drones? That's what AI

01:48:32.720 --> 01:48:38.320
safety is. Can you also open the model weights so you can generate adversarial inputs? Can you

01:48:38.320 --> 01:48:41.680
open the model weights and proliferate it? You're saying, oh, proliferation is bad. I'm saying

01:48:41.680 --> 01:48:46.880
proliferation is good because if everybody has one, then nobody has an advantage on it.

01:48:47.440 --> 01:48:52.800
Right. Not relatively speaking. Okay. I have very few super confident positions. So I wouldn't

01:48:52.800 --> 01:49:00.880
necessarily say I think that proliferation is bad. I'd say so far it's good. It has and even

01:49:00.960 --> 01:49:07.440
most of the AI safety people, I would say if I could speak on the behalf of the AI safety

01:49:08.320 --> 01:49:15.920
consensus, I would say most people would say even that the Llama 2 release has proven

01:49:15.920 --> 01:49:20.160
good for AI safety for the reasons that you're saying. But they opposed it.

01:49:20.160 --> 01:49:25.280
Well, some didn't, some didn't. I would say the main posture that I see AI safety people taking

01:49:25.280 --> 01:49:31.200
is that we're getting really close to or we might be getting really close.

01:49:32.160 --> 01:49:36.240
Certainly if we just naively extrapolate out recent progress, it would seem that we're getting

01:49:36.240 --> 01:49:43.920
really close to systems that are sufficiently powerful that it's very hard to predict what

01:49:43.920 --> 01:49:52.400
happens if they proliferate. Llama 2, not there. And so, yes, it has enabled a lot of interpretability

01:49:52.400 --> 01:49:56.720
work. It has enabled things like representation engineering, which there is a lot of good

01:49:56.720 --> 01:50:02.160
stuff that has come from it. The big thing that I want to kind of establish is you agree with me

01:50:02.160 --> 01:50:08.800
on the actuation point or not. The thing is this thing, like Llama 2 proliferates and so

01:50:08.800 --> 01:50:14.000
businesses are disrupted and people, maybe they paid a lot of money for their MD degree and they

01:50:14.000 --> 01:50:18.640
can't make us a bunch of money. That's within the realm of what I call conventional warfare.

01:50:18.720 --> 01:50:22.240
You know what I mean? That's like we're still in normal world as we were talking about, okay?

01:50:22.960 --> 01:50:29.200
Unconventional warfare is, you know, Skynet arises and kills everybody, okay? And that is what is

01:50:29.200 --> 01:50:34.640
being sold over here. And when you think about the actuators, we don't have the drones out there,

01:50:34.640 --> 01:50:39.600
we don't have the humanoid robots at control, and hypnotized humans are a very tiny subset of humans,

01:50:39.600 --> 01:50:44.400
probably. And even if they aren't, that just looks like a religion or a cult or a terrorist

01:50:44.400 --> 01:50:48.880
cell, and we know how to deal with that as well. The super intelligent AI with, you know,

01:50:48.880 --> 01:50:54.320
lots of robots at control in a starcraft form, I would agree, is something that humans haven't

01:50:54.320 --> 01:50:59.520
faced yet. But by the time we get that many robots out there, you won't be able to control all of

01:50:59.520 --> 01:51:04.480
them at once because of the private key things I mentioned. So that's why I'm like, okay,

01:51:04.480 --> 01:51:08.080
everything else we're talking about is in normal world. That is the single biggest thing

01:51:08.080 --> 01:51:13.520
that I wanted to get, like economic disruption, people losing jobs, proliferation so that the

01:51:13.520 --> 01:51:18.480
balance of power is redistributed, all that is fine. The reason I say this is people keep trying

01:51:18.480 --> 01:51:22.880
to link AI to existential risk. A great example is one of the things you actually had in here,

01:51:22.880 --> 01:51:26.720
this is similar to the AI policy and so on. It's a totally reasonable question, but then I'm going

01:51:26.720 --> 01:51:30.480
to, in my view, deconstruct the question. What would you think about putting the limit on the

01:51:30.480 --> 01:51:33.600
right to compute or their capabilities and AI system might demonstrate that we make you think

01:51:33.600 --> 01:51:37.360
open access no longer wise? The most common near term answer here to be seems to be related to

01:51:37.360 --> 01:51:42.000
risk of pandemic via novel pathogen engineering. So guess what? You know who the novel pathogen

01:51:42.080 --> 01:51:48.240
engineers are? The US and Chinese governments, right? They did it, or probably did it, credibly

01:51:48.240 --> 01:51:52.400
did it, credibly mean accused of doing it. They haven't been punished for COVID-19. In fact,

01:51:52.400 --> 01:51:56.480
they covered up their culpability and pointed everywhere other than themselves. They used it

01:51:56.480 --> 01:52:02.480
to gain more power in both the US and China with both lockdown in China and in the US and all kinds

01:52:02.480 --> 01:52:08.240
of COVID era, trillions of dollars was printed and spent and so on and so forth. They did everything

01:52:08.240 --> 01:52:13.120
other than actually solve the problem. That was actually getting the vaccines in the private

01:52:13.120 --> 01:52:17.600
sector and they studied the existential risk only to generate it and they're even paid to

01:52:17.600 --> 01:52:22.560
generate pandemic prevention and failed. So this would be the ultimate Fox guarding the henhouse.

01:52:25.120 --> 01:52:28.400
The two organizations responsible for killing millions of people with novel pathogen are going

01:52:28.400 --> 01:52:35.200
to prevent people from doing this by restricting compute. No, you know what it is actually. What's

01:52:35.200 --> 01:52:41.360
happening here is one of the concepts I have in the network state is this idea of God, state,

01:52:41.360 --> 01:52:45.600
and network. Meaning, what do you think is the most powerful force in the world? Is it Almighty

01:52:45.600 --> 01:52:55.920
God? Is it the US government? Or is it encryption? Or eventually maybe an AGI? What's happening here

01:52:55.920 --> 01:53:02.240
is a lot of people are implicitly, without realizing it, even if they are secular atheists,

01:53:02.240 --> 01:53:08.080
they're treating GOV as GOD. They treat the US government as God as the final mover.

01:53:08.800 --> 01:53:12.960
No, I appreciate your little, I take inspiration from you actually in terms of

01:53:12.960 --> 01:53:19.200
trying to come up with these little quips that are memorable. So I was just smiling at that

01:53:19.200 --> 01:53:25.760
because I think you do a great job of that and I try to encourage, I have less success

01:53:25.760 --> 01:53:30.640
coining terms than you have, but certainly try to follow your example on that front.

01:53:30.720 --> 01:53:34.960
It's like a helpful, if you can compress it down, it's like more memorable. So that's what I try

01:53:34.960 --> 01:53:40.240
to do, right? So exactly, a lot of these people who are secular, think of themselves as atheists,

01:53:40.240 --> 01:53:45.760
have just replaced GOD with GOV. They worship the US government as God. And there's two versions

01:53:45.760 --> 01:53:50.880
of this. You know how like God has both the male and female version, right? The female version is

01:53:50.880 --> 01:53:55.680
the Democrat God within the USA that has infinite money and can take care of everybody and care

01:53:55.680 --> 01:54:00.080
for everybody. And the Republican God is the US military that can blow up anybody and it's the

01:54:00.080 --> 01:54:07.840
biggest and strongest and most powerful America F. Yeah. Okay. And everybody who thinks of the US

01:54:07.840 --> 01:54:14.800
government as being able to stop something is praying to a dead God. Okay, when you say this,

01:54:14.800 --> 01:54:19.200
you actually get an interesting reaction from AI safety people where you've actually hit their

01:54:19.200 --> 01:54:26.080
true solar plexus. All right, the true solar plexus is not that they believe in AI, it's that

01:54:26.080 --> 01:54:32.560
they believe in the US government. That's a true solar plexus because they are appealing to their

01:54:32.560 --> 01:54:38.080
praying to this dead God that can't even clean the poop off the streets in San Francisco, right?

01:54:38.080 --> 01:54:43.920
That is losing wars or fighting them to sell me. It says lost all these wars around the world

01:54:43.920 --> 01:54:48.560
that spent trillions of dollars has been through financial crisis, Coronavirus, Iraq war, you

01:54:48.560 --> 01:54:53.200
know, total meltdown politically. Okay, that is interest that is now has interest payments more

01:54:53.200 --> 01:54:59.520
than the defense budget. That is, you know, that spent $100 billion on the California train without

01:54:59.520 --> 01:55:04.720
laying a single track. It's like that, you know, that Morgan Freeman thing for you know, the clip

01:55:04.720 --> 01:55:10.000
from Batman, who is like, So this man has a billionaire, blah, blah, blah, this and that,

01:55:10.000 --> 01:55:14.960
and your plan is to threaten him, right? And so you're going to create this super intelligence

01:55:14.960 --> 01:55:21.280
and have Kamala Harris regulate it. Come on, man, so to speak, right? Like these people are praying

01:55:21.280 --> 01:55:30.000
to a blind, deaf and dumb God that was powerful in 1945, right? That's why, by the way, all the

01:55:30.000 --> 01:55:36.240
popular movies, what are they? It's Barbie, it's Oppenheimer, right? It's, it's top gun. They're

01:55:36.240 --> 01:55:43.040
all throwbacks the 80s or the 50s when the USA was really big and strong. And the future is a

01:55:43.040 --> 01:55:48.080
black mirror. Yeah, I think that's tragic. One of the projects that I do like, and you might appreciate

01:55:48.080 --> 01:55:54.800
this, I don't know if you've seen it, is the From the Future of Life Institute, a project called

01:55:55.600 --> 01:56:01.840
Imagine a World, I think is the name of it. And they basically challenged, you know, their

01:56:02.560 --> 01:56:10.960
audience and the public to come up with positive visions of a future, you know, where technology

01:56:10.960 --> 01:56:16.720
changes a lot. And obviously, I pretty central to a lot of those stories. And, you know, one of the

01:56:16.720 --> 01:56:22.880
challenges that people go through and how do we get there and whatever, but a purposeful effort

01:56:22.880 --> 01:56:32.080
to imagine positive futures super under provided. And I really liked the investment that they made

01:56:32.080 --> 01:56:36.720
in that. You know, one of the things I've got in the Never See It book is there's certain megatrends

01:56:36.720 --> 01:56:42.880
that are happening, right? And megatrends, I mean, it's possible for like one miraculous human maybe

01:56:42.880 --> 01:56:48.000
to reverse them, okay? Because I think both the impersonal force of history theory and the

01:56:48.000 --> 01:56:54.080
great man theory of history have some truth to them. But the megatrends are the decline of Washington,

01:56:54.080 --> 01:57:00.480
DC, the rise of the internet, the rise of India, the rise of China. That is like my worldview. And

01:57:00.480 --> 01:57:05.680
I can give a thousand graphs and charts and so on for that. But that's basically the last 30 years.

01:57:06.320 --> 01:57:10.240
And maybe the next X, right? I'm not saying there can't be trend reversal. Of course, it can be

01:57:10.240 --> 01:57:13.680
trend reversal, as I just mentioned, some hammer blow could hit it, but that's what's happening.

01:57:14.320 --> 01:57:18.800
And so because of that, the people who are optimistic about the future are aligned with either the

01:57:18.800 --> 01:57:24.960
internet, India or China. And the people who are not optimistic about the future are blue Americans

01:57:24.960 --> 01:57:31.120
or left out red Americans, okay, or Westerners in general, who are not tech tech people. Okay,

01:57:31.120 --> 01:57:36.160
if they're not tech people, they're not up into the right, basically, because the internet's if you

01:57:36.160 --> 01:57:40.640
I mean, one of the things is we have a misnomer, as I was saying earlier, calling it the United

01:57:40.640 --> 01:57:44.800
States, because the dis United States, it's, it's like talking about, you know, talking about

01:57:44.800 --> 01:57:48.080
America is like talking about Korea, there's North Korea and South Korea, they're totally different

01:57:48.080 --> 01:57:54.400
populations. And, you know, communism and capitalism are totally different systems. And the thing that

01:57:54.400 --> 01:57:59.120
is good for one is bad for another and vice versa. And so like America doesn't exist, there's only

01:57:59.120 --> 01:58:02.960
just like there's no Korea, there's only North Korea and South Korea, there's no America, there's

01:58:02.960 --> 01:58:09.920
blue America and red America and also gray America, tech America. And blue America is harmed, or they

01:58:09.920 --> 01:58:14.480
think they're harmed, or they've gotten themselves into a spot where they're harmed by every technological

01:58:14.480 --> 01:58:20.240
development, which is why they hate it so much, right? AI versus journalist jobs, crypto takes away

01:58:20.240 --> 01:58:24.800
banking jobs, you know, everything, you know, self driving cars, they just take away regulator

01:58:24.800 --> 01:58:29.600
control, right? Anything that reduces their power, they hate, and they're just trying to freeze an

01:58:29.600 --> 01:58:35.120
amber with regulations. Red America got crushed a long time ago by offshoring to China and so on,

01:58:35.120 --> 01:58:40.320
they're, they're making, you know, inroads ally with tech America or gray America. Tech America

01:58:40.320 --> 01:58:44.320
is like the one piece of America that's actually still functional and globally competitive. And

01:58:44.320 --> 01:58:49.280
people always do this fallacy of aggregation, where they talk about the USA. And it's really

01:58:49.280 --> 01:58:53.440
this component that's up into the right, and the others that are down into the right, red,

01:58:53.440 --> 01:58:58.160
best flat, like red, but they're like down, right? Like, red is like, okay, functional, blue is down.

01:58:58.160 --> 01:59:04.000
The point is, tech America, I think we're going to find is not even truly or

01:59:06.080 --> 01:59:11.760
how American is tech America, because it's like 50% immigrants, right? And like a lot of children

01:59:11.760 --> 01:59:17.840
immigrants, and most of their customers are overseas, and their users are overseas. And

01:59:17.840 --> 01:59:24.480
their vantage point is global, right? And they're basically not, I know we're in this

01:59:24.480 --> 01:59:29.600
ultra nationalist kick right now. And I know that there's going to be, there's a degree of a fork

01:59:29.600 --> 01:59:37.760
here, where you fork technology into Silicon Valley and the internet. Okay, where Silicon

01:59:37.760 --> 01:59:42.000
Valley is American, and they'll be making like American military equipment and so on and so

01:59:42.000 --> 01:59:47.520
forth, and they're signaling USA, which is fine. Okay. And then the internet is international,

01:59:47.520 --> 01:59:54.080
global capitalism. And the difference is Silicon Valley, or let's say US tech, let me less, you

01:59:54.080 --> 01:59:59.200
know, US tech says ban tick tock, build military equipment, etc. It's really identifying itself

01:59:59.200 --> 02:00:04.800
as American. And it's thinking of being anti China. Okay, but there's US and China are only 20% of

02:00:04.800 --> 02:00:10.720
the world, 80% of the world is neither American nor Chinese. So the internet is for everybody else

02:00:10.720 --> 02:00:17.040
who wants actual global rule of law, right? When as a US decays as a rule space order, and people

02:00:17.040 --> 02:00:22.000
don't want to be under China, people want to be under something like blockchains, where you've got

02:00:22.000 --> 02:00:27.360
like property rights, contract law, cross borders that are enforced by an impartial authority. Okay,

02:00:27.360 --> 02:00:31.440
that's also the kind of laws that can bind AI's like AI's across borders, if you want to make

02:00:31.440 --> 02:00:35.600
sure they're going to do something, cryptography can bind an AI in such a way that it can't fake

02:00:35.600 --> 02:00:41.120
it. It can't an AI can't mint more Bitcoin, you know, my here's my last question for you. AI discourse

02:00:41.120 --> 02:00:47.040
right now does seem to be polarizing into camps. Obviously, a big way that you think about the world

02:00:47.040 --> 02:00:51.840
is by trying to figure out, you know, what are the different camps? How do they relate to each other

02:00:52.000 --> 02:01:00.880
so on and so forth? I have the view that AI is so weird. And so unlike other things that we've

02:01:00.880 --> 02:01:05.360
encountered in the past, including just like, unlike humans, right, I always say AI, alien

02:01:05.360 --> 02:01:11.680
intelligence, that I feel like it's really important to to borrow a phrase from Paul Graham,

02:01:11.680 --> 02:01:20.240
keep our identities small, and try to have a scout mindset to really just take things on their

02:01:20.240 --> 02:01:24.240
own terms, right? And not necessarily put them through a prism of like, who's team am I on? Or,

02:01:24.240 --> 02:01:30.480
you know, is this benefit my team or hurt the other team or whatever. But you know, just try to be as

02:01:30.480 --> 02:01:37.200
kind of directly engaged with the things themselves as we can without mediating it through all these

02:01:37.200 --> 02:01:43.680
lenses. You know, I think about you mentioned like, the gain of function, right? And I don't know

02:01:43.680 --> 02:01:47.840
for sure what happened, but it certainly does seem like there's a very significant chance

02:01:47.920 --> 02:01:53.120
that it was a lab leak. Certainly there's a long history of lab leaks. But it would be like, you

02:01:53.120 --> 02:01:59.840
know, it would seem to me a failure to say, okay, well, what's the what's the opposite of just having

02:01:59.840 --> 02:02:03.840
like a couple of government labs, like, everybody gets their own gain of function lab, right? Like,

02:02:03.840 --> 02:02:07.680
if we could, and this is kind of what we're doing with AI, we're like, let's compress this power down

02:02:07.680 --> 02:02:12.400
to as small as we can. Let's make a kit that can run in everybody's home. Would we want to

02:02:13.120 --> 02:02:18.880
send out these like gain of function, you know, wet lab research kits to like every home in the

02:02:18.880 --> 02:02:23.600
world and be like, hope you find something interesting, you know, like, let us know if you

02:02:23.600 --> 02:02:28.560
find any new pathogens or hey, maybe you'll find life saving drugs, like whatever, we'll see what

02:02:28.560 --> 02:02:33.520
you find, you know, all eight billion of you. That to me seems like it would be definitely a

02:02:33.520 --> 02:02:39.920
big misstep. And that's the kind of thing that I see coming out of ideologically

02:02:40.880 --> 02:02:45.120
motivated reasoning, or like, you know, tribal reasoning. And so I guess,

02:02:45.120 --> 02:02:52.080
I wonder how you think about the role that tribalism and ideology is playing and should

02:02:52.080 --> 02:02:58.160
or shouldn't play as we try to understand AI. Okay, so first is, you're absolutely right,

02:02:58.160 --> 02:03:05.440
that just because a is bad does not mean that B is good, right? So a could be a bad option,

02:03:05.440 --> 02:03:10.800
B could be a bad option, C could be a bad option. There might be, you have to go down to option G

02:03:10.800 --> 02:03:14.000
before you find a good option, or there might be three good options and seven bad options,

02:03:14.000 --> 02:03:20.880
for example, right? So to map that here, in my view, an extremely bad option is to ask the US

02:03:20.880 --> 02:03:27.040
and Chinese governments to do something. Anything the US government does at the federal level,

02:03:27.040 --> 02:03:32.560
at the state level in blue states at the city level has been a failure. And the way that here's

02:03:32.720 --> 02:03:36.640
here's a metaway of thinking about it, you invest in companies, right? So as an investor,

02:03:36.640 --> 02:03:41.520
here's a really important thing. You might have 10 people who come to you with the same words in

02:03:41.520 --> 02:03:46.880
their pitch. They're all, for example, building social networks. But one of them is Facebook,

02:03:46.880 --> 02:03:51.440
and the others are Friendster and whatever. Okay, and no offense to Friendster, you know,

02:03:51.440 --> 02:03:56.400
that those guys were like, you know, pioneers in their own way. But they just got outmatched by

02:03:56.400 --> 02:04:01.920
Facebook. So point is that the words were the same on each of these packages, but the execution was

02:04:02.000 --> 02:04:09.840
completely different. So could I imagine a highly competent government that could execute and that

02:04:09.840 --> 02:04:15.680
actually did, you know, like, you know, make the right balance of things and so on? I can't say

02:04:15.680 --> 02:04:23.760
it's impossible, but I can't say that it wouldn't be this government. Okay, and so you are talking

02:04:23.760 --> 02:04:28.640
about the words and I'm talking about the substance. The words are, we will protect you from AI,

02:04:28.640 --> 02:04:32.880
right? In my view, the substances, they aren't protecting you from anything, right? You're basically

02:04:32.880 --> 02:04:37.280
giving money and power to a completely incompetent and in fact, malicious organization, which is

02:04:37.280 --> 02:04:42.960
Washington DC, which is the US government that has basically over the last 30 years, gone from a

02:04:42.960 --> 02:04:47.600
hyperpower that wins everywhere without fighting to a declining power that fights everywhere without

02:04:47.600 --> 02:04:53.840
winning. Okay, like just literally burn trillions of dollars doing this, take maybe the greatest

02:04:53.840 --> 02:04:58.480
decline in fortunes in 30 years and maybe human history, not even the Roman Empire went down this

02:04:58.560 --> 02:05:05.440
fast on this many power dimensions this quickly, right? So giving that guy, let's trust him,

02:05:05.440 --> 02:05:09.760
that's just people running an old script in their heads that they inherited. They are not

02:05:09.760 --> 02:05:15.440
thinking about it from first principles that this state is a failure. Okay, and like how much

02:05:15.440 --> 02:05:18.800
of a failure it is, you have to look at sovereign debt crisis, look at graphs that other people

02:05:18.800 --> 02:05:25.360
aren't looking at, but like, you know, the domain of what Blue America can regulate is already

02:05:25.440 --> 02:05:30.640
collapsing because it can't regulate Russia anymore. It can't regulate China anymore.

02:05:30.640 --> 02:05:35.120
It's less able to regulate India. It's less able even to regulate Florida and Texas.

02:05:35.120 --> 02:05:38.960
States are breaking away from it domestically. So this gets to your other point. Why is the

02:05:38.960 --> 02:05:45.200
tribal lens not something that we can put in the back? We have to put in the absolute front

02:05:45.200 --> 02:05:52.160
because the world is retribalizing. Like basically your tribe determines what law you're bound by.

02:05:52.160 --> 02:05:56.960
If you think you can pass some policy that binds the whole world, well, there have to be guys with

02:05:56.960 --> 02:06:02.000
guns who enforce that policy. And if I have guys with guns on their side that say we're not enforcing

02:06:02.000 --> 02:06:06.000
their policy, then you have no policy. You've only bound your own people. Does that make sense,

02:06:06.000 --> 02:06:13.200
right? And so Blue America will probably succeed in choking the life out of AI within Blue America.

02:06:13.200 --> 02:06:18.320
But Blue America controls less and less of the world. So it'll have more power or a fewer people.

02:06:19.280 --> 02:06:25.760
I can go into why this is, but essentially a financial Berlin Wall is arising. There's a lot

02:06:25.760 --> 02:06:31.520
of taxation and regulation and effectively financial repression, de facto confiscation,

02:06:31.520 --> 02:06:35.440
that will have to happen for the level of debt service that the US has been taking on.

02:06:36.560 --> 02:06:41.520
There's one graph just to make the point. And if you want to dig into this, you can.

02:06:42.400 --> 02:06:47.440
But the reason this impacts things is when you're talking about AI safety, you're talking about

02:06:47.440 --> 02:06:52.480
AI regulation, you're talking about the US government, right? And you have to ask, what does

02:06:52.480 --> 02:06:58.640
that actually mean? And it's like, in my view, it's like asking the Soviet Union in 1989 to

02:06:58.640 --> 02:07:03.520
regulate the internet, right? That's going to outlive, you know, the country. US interest

02:07:03.520 --> 02:07:07.760
payment on federal debt versus defense spending. The white line is defense spending. Look at the

02:07:07.760 --> 02:07:12.880
red line. That's just gone absolutely vertical. That's interest. And it's going to go more vertical

02:07:12.880 --> 02:07:18.400
next year because all of this debt is getting refinanced at much higher interest rates. That's

02:07:18.400 --> 02:07:24.000
why look at this, you want AI timelines, right? The question for me is DC's timeline.

02:07:24.560 --> 02:07:32.000
What is DC's time left to live? Okay, this is the kind of thing that kills empires. And you either

02:07:32.000 --> 02:07:37.520
have this just go to the absolute moon, or they cut rates and they print a lot. And either way,

02:07:38.080 --> 02:07:43.440
you know, the fundamental assumption underpinning all the AI safety, all the AI regulation work

02:07:43.440 --> 02:07:48.640
is that they have a functional golem in Washington DC, where if they convince it to do something,

02:07:48.640 --> 02:07:53.120
it has enough power to control enough of the world. When that assumption is broken,

02:07:54.400 --> 02:07:59.760
then a lot of assumptions are broken, right? And so in my view, you have to,

02:08:01.040 --> 02:08:06.960
you must think about a polytheistic AI world, because other tribes are already into this,

02:08:07.040 --> 02:08:11.120
they're already funding their own, right? The proliferation is already happening.

02:08:11.840 --> 02:08:16.640
And they're not going to bow to blue tribe. So that's why I think the tribal lens is not

02:08:16.640 --> 02:08:21.840
secondary. It's not some, you know, totally separate thing. It is an absolutely primary

02:08:21.840 --> 02:08:26.080
way in which to look at this. And in a sense, it's almost like a, you know, in a well done

02:08:26.800 --> 02:08:34.960
movie, all the plot lines come together at the end. Okay. And all the disruptions that are happening,

02:08:34.960 --> 02:08:40.640
the China disruption, the rise of India, the rise of the internet, the rise of crypto,

02:08:40.640 --> 02:08:44.960
the rise of AI, and the decline of DC, and the internal political conflict,

02:08:45.520 --> 02:08:49.920
and, you know, various other theaters like what's happening in Europe and, you know, and Middle

02:08:49.920 --> 02:08:55.840
East, all of those come together into a crescendo of, ah, there's a lot of those graphs are all

02:08:55.840 --> 02:09:00.400
having the same time. And it's not something you can analyze by just, I think, looking at one of

02:09:00.400 --> 02:09:04.880
these curves on itself. I think that's a great note to wrap on. I am always lamenting the fact

02:09:05.120 --> 02:09:12.240
that so many people are thinking about this AI moment in just fundamentally too small

02:09:12.240 --> 02:09:18.560
of terms. But I don't think you're one that will easily be accused of that. So with

02:09:19.600 --> 02:09:24.480
an invitation to come back and continue in the not too distant future, for now, I will say

02:09:24.480 --> 02:09:28.320
Balaji Srinivasan, thank you for being part of the Cognitive Revolution.

02:09:29.360 --> 02:09:31.040
Thank you, Nathan. Good to be here.

