start	end	text
0	10280	Hey everyone, today on Moment of Zen, we chat with Beth Jayzos, the co-founder of the EAC
10280	14880	movement, or Effective Accelerationism, who was doxxed on Friday night by Forbes reporters.
14880	17120	We'll link to some articles about it below.
17120	21320	Up ahead, we get the inside story from Beth, whose real name is Guillaume Verdome.
21320	25680	We also discuss anonymity, the media, and the role of journalism in our society.
25680	30320	Also joining the conversation is Baselord, Beth's EAC co-founder, who remains anonymous,
30320	33720	and Nathan LeBenz, who hosts the Cognitive Revolution podcast.
33720	35720	Quick disclaimer, and then we'll get to the show.
35720	40400	I'm an investor in XTropic, the stealth AI hardware startup that Guillaume founded.
40400	41960	Here's Moment of Zen with Beth.
41960	42960	Yo, yo.
42960	47400	What do we got going on here, Steve Jobs?
47400	52800	Well, you know, I got to switch up the look, keep the commenters chirping.
52800	53800	Yeah.
53800	55800	A lot of people hop in.
55800	62880	So, Somali Pirate to, you know, confident design partner, I don't know, like what's
62880	64880	going on here.
64880	66200	Yeah, exactly.
66200	67200	Just testing it out.
67200	70040	Did you get a better webcam, too?
70040	71040	No.
71040	73040	Oh, okay.
73040	76480	But, but I'm going to.
76480	78640	Is the Binance topic like a five minute topic?
78640	81160	I mean, I can give an update on it if you want.
81520	82520	Yeah.
82520	83520	Well, I'll start that.
83520	84520	You're giving an update on the Binance topic.
84520	90920	Well, I mean, it's a little bit old news at this point, but we had a major settlement
90920	99600	that Binance, the CEO of Binance CZ, who, he, you know, kind of kicked off the SBF issue
99600	101760	and is big of a name in crypto.
101760	107400	I mean, it's CZ, SBF, and Brian Armstrong were the three big names during the last cycle,
107400	111800	and SBF has now been convicted.
111800	117680	And CZ, there were always kind of rumors that there was like a big DOJ investigation.
117680	120760	Then there was an SEC suit that happened this year.
120760	127560	But in terms of the real criminal stuff, that had always been swirling and a lot of allegations
127560	130120	around sanctions.
130120	136000	And remember, sanctions are a U.S. tool because the world is run on dollars.
136000	142560	So the U.S. can basically say, hey, you can't send dollars or anything crypto to these specific
142560	149120	countries or individuals, and the allegations were that Binance was facilitating that.
149120	154040	Now, I actually don't even know the details specifically whether or not they did, to what
154040	164480	extent, but what came out was, and the settlement was CZ is stepping back as CEO.
164840	173800	And I think the record fines for sanctions have been as a result of violating sanctions
173800	174800	with Iran.
174800	179520	I want to say it was like BNP Paribas, which is a French bank, paid something like $8 billion,
179520	182960	or maybe it was HSBC, I forget.
182960	187760	And then I think another big set of fines, HSBC might have been the drug cartels, but
187760	193120	basically it's like Iran, which is a geopolitical issue, and then the drug cartels, if you do
193120	197760	money laundering, you pay major fines and there's criminal liability.
197760	200680	So the settlement was he steps back.
200680	202760	Binance can still exist, which I'm actually surprised.
202760	207080	I think he's paying something on the order of like $50 million fine.
207080	213520	And the thing I didn't quite get in the first day of the news, but then it turned out later
213520	217360	kind of seemed to bubble up is he's still in the U.S., I think, and actually may have
217360	222440	to do some jail time, which is like a pretty big deal.
222440	223960	Because he's voluntarily given it up.
223960	231160	So you know, if he's willing to step back, pay $4 billion fine, $50 million personally,
231160	235840	and potentially do jail time in the U.S., that means they were going to throw a huge
235840	239240	amount of the whole book at him.
239240	244800	But what's interesting is the SEC is not done with their case, which I think is a civil
244800	245800	case.
245800	250200	So that will still be another big hit on Binance if they have to settle it.
251200	252920	Obviously, you don't have the founder anymore.
252920	258640	If he has to do jail time, it can't even be, you know, indirectly influencing.
258640	262000	And then I would imagine the European Union and every other jurisdiction that's kind
262000	265880	of within the global U.S. sphere is going to say, well, we don't want to look like we
265880	267480	were not tough on these guys.
267480	270320	And so these guys are going to be paying a lot of money.
270320	275480	So I'm curious to see if they end up existing, you know, five years from now.
275480	277600	I mean, five years in crypto is a long time.
277840	283480	But basically Binance popped up during, you know, call it 2017 era when there was another
283480	288320	exchange called PoloniEx, which was actually based in the U.S., but they were only serving
288320	295160	international customers in theory, that they knocked PoloniEx off the top spot and never
295160	296160	kind of looked back.
296160	299480	And SBF was gunning to try to be the next Binance.
299480	305120	So what's left in the wake of this is, you know, Coinbase, which I think got a lot of
305160	311840	ridicule from, you know, the mid-wit, Finn Twitter, like Bukko Capital and all these,
311840	316960	like, kind of a non-accounts that are too afraid to put their name next to an account.
316960	322240	So they like to make fun of, you know, entrepreneurs on the sidelines when obviously everything
322240	327520	corrected last year, they were all like Coinbase is not, you know, like a real business.
327520	332560	I mean, Bukko Capital was calling Coinbase a cancer as of last week because crypto prices
332560	333560	are pumping.
333560	338360	But Coinbase actually is the only one that's been following the law and didn't have a crazy
338360	339360	risk program.
339360	345200	So FTX disappears, Binance disappears, Gemini basically was doing a bunch of shady stuff
345200	347680	and blew up as a result of that.
347680	349440	What was the other one?
349440	350840	BlockFi, right?
350840	351960	Like, there's a pretty famous meme.
351960	356320	I like pump, but pump kind of was like pumping them and it's like to the moon and like this.
356320	359560	And obviously that was fraudulent.
359560	362120	And so it turns out like Coinbase is the only one that was legit.
362120	366320	And yes, like that's been the MO of the company the whole time.
366320	369480	And so finally with the government cleaning up the space, it's like now Coinbase is sitting,
369480	372720	you know, Coinbase, Bitcoin just crossed 40,000.
372720	378360	And Coinbase is basically the only exchange with like meaningful fiat rails and customer
378360	379360	base.
379360	382000	I mean, there are overseas exchanges that will take place in Binance.
382000	385720	But it's a huge vindication, I think, for Brian Armstrong, who's taken a lot of arrows
385720	388400	for a lot of different things.
388400	390040	I'm also happy about the share price personally.
391040	392360	Yeah, totally.
392360	394360	You're hopefully incentivized.
394360	396480	Yeah, let's be clear.
396480	398640	I'm extremely biased here.
398640	402240	But what I would say is like, if you want me to give you like the counterpoint, I do
402240	410320	think Binance pushed the industry forward, Modulo put the illegal stuff off the side.
410320	416400	They never had a major hack, which is usually how a lot of these exchanges go down.
416440	420960	And to the degree that I understand it as a result of the DOJ, it wasn't misappropriating
420960	422560	customer funds, right?
422560	426880	So now the SEC may come and say something different, but big difference between SPF,
426880	428640	which was actual fraud, right?
428640	432000	Like you was taking customer funds, which you're not supposed to touch, and was using
432000	436960	it to, you know, pay Tom Brady and donate to political campaigns.
436960	441360	So I think Binance was an honest actor as it relates to customers.
441360	445120	I think they just seems like they were a little fast and loose with the, you know, the international
445120	447760	sanctions regime.
447760	448760	Perfect timing.
448760	454680	Fast and loose with the international sanctions regime, and Antonio hops on to the call.
454680	455680	Who sanctions?
455680	456680	What regime?
456680	458440	Yeah, that's actually a good one.
458440	463920	We know a few people who would like to argue that US sanctions are not valid, but the reality
463920	467880	is if you live in a dollar denominated world, you got to pay attention to those sanctions
467880	470480	because you can go to jail if you violate them.
470480	473880	We were talking about Binance.
473880	476400	Where is the man of the hour?
476400	478760	He's having trouble getting in.
478760	481680	I've never seen this issue before.
481680	487640	Maybe he's using some like futuristic quantum web browser that doesn't have support for
487640	490680	lowly riverside web RTC.
490680	495000	Or Emily Baker White put some weird malware that she used to like sample his voice to
495000	496000	out him.
496000	497000	And it's still like on the machine.
497000	498000	Did they add him on our podcast?
498000	499000	Yeah.
499000	504480	I find that this wasn't actually MBS in collaboration with Forbes, right?
504480	508240	Hey, Beth, how are you?
508240	511520	You guys know the reference, right?
511520	513920	No, what?
513920	514920	MBS?
514920	520800	It's still not confirmed, and I think it's probably fake, but it's also maybe real.
520800	528240	Bezos' marriage got broken up as a result of MBS sending spyware in WhatsApp to Jeff.
528240	533280	And the text messages in question were, hi, Jeff, how are you?
533280	534680	This is MBS.
534680	540480	Well, it was probably good for him and bad for American cities everywhere, but yeah,
540480	541480	exactly.
541480	542480	Exactly.
542480	543480	He got divorced.
543480	544480	I don't know.
544480	547920	Lauren Sanchez, she's pretty happy, right?
547920	555000	And she's now on the front of the yacht that they just finished, because Chris Sailing
555000	557840	Antonio is such a blue collar thing, right?
558440	559440	He's not sailing.
559440	560440	He's motoring.
560440	561440	Motoring is totally different.
561440	562940	Doesn't he have a sailboat?
562940	564440	Isn't it a big sailing yacht?
564440	566080	I don't know.
566080	569520	The deck photos I've seen are clearly a large motor yacht type thing.
569520	571680	Someone, we should put it in the YouTube comments.
571680	577760	The pictures of Jeff Bezos' boat, my understanding is that there are sails, but it could be.
577760	580360	All right, we got it.
580360	585440	You guys have the most interesting guests on the podcast the one time I can't make it.
585440	586440	Oh, don't worry.
586840	587840	Oh, Dominic Cummings.
587840	588840	Yeah.
591840	595240	He sounded super pumped to like Rijig or San Francisco, I have to say, which was interesting.
595240	596240	Yeah.
596240	597240	I don't know.
597240	600240	I think saving SF is kind of cope.
600240	601240	You think it's unsavable?
601240	603240	I mean, are you running, Eric?
603240	604240	Sir.
604240	606240	Hey, everyone.
606240	607240	Welcome.
607240	610240	Welcome to FACE Twitter.
610240	611240	FACE Doxxed World.
611240	615240	Well, hello, world.
615240	619140	I see the Eigenfunction on your quantum computer has clapped into the state in which you can
619140	624840	join the Riverside and some other worlds you were in fact still struggling, but Schrodinger's
624840	625840	Riverside episode.
625840	626840	Yeah.
626840	627840	Yeah.
627840	628840	Yeah.
628840	632040	We need to upgrade the quantum computers to use this wonderful app.
632040	639040	Speaking of which, what is that menacing blinking box behind you over your right shoulder?
639040	640640	That's just, don't worry about that.
640640	643640	Don't worry about that.
644640	645640	Yeah.
645640	649640	No, I'm in EAC HQ or Extropic HQ.
649640	655740	I've got our banner in the back and of course I've got some Dr. Pepper and some ketones
655740	657940	back there.
657940	658940	Very memetic.
658940	662440	But I guess we're recording right now, right?
662440	663440	We're recording.
663440	664440	We're watching it.
664440	665440	Okay.
665440	666440	Wonderful.
666440	667440	Eric doesn't read you your Miranda rights.
667440	668440	You just go right into it.
668440	669440	It's like a cold.
669440	670440	Okay.
670440	671440	Got it.
671440	672440	Let's go.
672440	676640	Well, super happy to be here.
676640	678240	It definitely feels weird.
678240	680840	It definitely feels very weird.
680840	685600	It's been kind of a compartmentalized part of my life, you know, felt like a video game
685600	687400	on my phone more or less, right?
687400	692640	Like having an alternative Twitter and having a very professional life day to day, you would
692640	699240	never guess that I'm back from just like day to day interactions, frankly.
699240	703080	And now the two have kind of bled into one another and they are one.
703080	706440	And so I'm still pricing that in mentally, frankly.
706440	713240	So it's been an interesting couple of days, obviously, a lot of inbound, more DMs I can
713240	715740	ever count.
715740	720240	And yeah, I mean, it's been a very surprising reaction.
720240	722760	Well, I guess not that surprising.
722760	727600	I guess like, you know, tech Twitter kind of, kind of hates the establishment media
727600	733640	at this point and, you know, came to my defense, which, which felt, felt quite good.
733640	741400	But you know, I'm happy to go into any part of it, you know, how it went down, you know,
741400	747600	the why I started Beth, anonymity, and anything, the startup, let me know how you want to
747600	748600	structure this.
748600	749600	This.
749600	750600	Yeah.
750640	754480	With the sorted details of how they figured it out and what voice they sampled and how
754480	757680	that whole sorted tail played out.
757680	758680	Yeah.
758680	765000	I mean, there was a couple instances where there's reporters that, you know, think they
765000	771480	have my docs and I'm just like, you know, you can't, you can't dox people, you know,
771480	773800	I do not want you to do this.
773800	778160	And usually that kind of kills the story to some extent, but you know, it's kind of been,
778160	781480	you know, I've gone, I've been in the SF social scene in person.
781480	785560	I know a bunch of investors, for example, they, some of them know some of them have correlated
785560	786560	the two.
786560	790160	Um, but obviously I, I, I spent a bunch of time in Twitter spaces.
790160	795600	I've had, I have some online lectures from my days in grad school and quantum computing.
795600	799160	And really that's basically what they've, what they've correlated.
799160	806160	Uh, they, they, they use some, I don't know, I call it CIA technology as a joke, but, uh,
806160	812280	you know, they use some government grade technology to identify my voice, uh, you know, just because
812280	816040	I have a 50 K follower account, uh, that speaks truth to power.
816040	821000	I guess, I guess it tells you that, you know, we're doing something right and that some
821000	827120	people in the establishment, you know, want to have leverage over the leader of the grassroots
827120	834760	movement, uh, that, you know, is for freedom and, and, and against top down control, which
834880	835880	is very scary.
835880	836880	Cause that's what they like.
836880	837880	Right.
837880	842720	So, Hey, we'll continue our interview in a moment after a word from our sponsors.
842720	843720	Real quick.
843720	847680	What's the easiest choice you can make taking the window instead of the middle seat outsourcing
847680	850280	business tasks that you absolutely hate.
850280	853920	What about selling with Shopify?
853920	858680	Shopify is the global commerce platform that helps you sell at every stage of your business.
858680	863400	Shopify powers 10% of all e-commerce in the U S and Shopify is the global force behind
863440	868760	all birds, Rothy's and Brooklyn and millions of other entrepreneurs of every size across
868760	871280	175 countries.
871280	874640	Whether you're selling security systems or marketing memory modules, Shopify helps you
874640	880000	sell everywhere from their all in one e-commerce platform to their in person POS system, wherever
880000	883040	and whatever you're selling, Shopify's got you covered.
883040	885560	I've used it in the past at the companies I've founded.
885560	890480	And when we launch merch here at Turpentine, Shopify will be our go to Shopify helps turn
890520	895480	browsers into buyers with the internet's best converting checkout up to 36% better compared
895480	897520	to other leading commerce platforms.
897520	901960	And Shopify helps you sell more with less effort thanks to Shopify magic, your AI powered
901960	907760	all star with Shopify magic whip up captivating content that converts from blog posts to product
907760	914080	descriptions, generate instant FAQ answers, pick the perfect email, send time, plus Shopify
914080	920000	magic is free for every Shopify seller businesses that grow, grow with Shopify, sign up for
920040	925800	a $1 per month trial period at Shopify.com slash moment of Zen, go to Shopify.com slash
925800	930440	moment of Zen now to grow your business no matter what stage you're in Shopify.com slash
930440	931040	moment of Zen.
933480	936520	You spoke truth to power and now they're going to try to speak power to truth, so to speak.
936720	941280	But I'm curious, someone must have, they must have been tipped off because they didn't do an
941280	942320	all against all voice sample.
942320	945400	They, they started after this pair wise interaction, right?
945400	947040	Because there was nothing online that would have suggested it.
947240	949080	And then they confirmed it via that, I imagine.
949800	955400	Yeah, so see, they must have like asked around, you know, I mean, it's kind of like a people
955400	957440	know other people's docs is right.
957440	962480	Like there's kind of Twitter parties in person and people go by their alternative name.
962480	966480	So they know your face and eventually they correlate things, but there's kind of a, yeah,
966480	969760	there's an unwritten rule of like, you don't, you don't like talk to reporters.
969760	972800	You don't share people's docs with other consent.
973160	977880	I'm sure some people, you know, broke that rule, but you know, I don't know what drove
977880	979200	them to really break the story.
979200	982240	Now, I think there were some latent variables there.
983560	989160	But yeah, essentially, I think it was on Thursday, I get a text from some of my investors.
989680	992040	They identified some of my investors.
992200	995200	So this podcast is going to drop after we announced the round.
995200	997800	So I'll just mention investors.
998280	1004280	But yes, so some of our big investors get a text like, Hey, I think this, you know, over
1004280	1009040	this first reporter, I think it was Conrad has started correlating your identity with
1009040	1013520	Beth, they didn't correlate all the company because we had a, you know, I had a company
1013520	1018280	change the name, now we're extrapik, they hadn't correlated everything perfectly, but
1018280	1020840	then the censor fuse the cross reporters internally.
1021080	1025160	So they didn't have enough to ship it on Thursday, but then on Friday, a different
1025160	1030360	reporter, Emily joined force, the filings, the track name changes.
1031360	1040240	They went to my personal Facebook to, you know, to identify a photo I shared like on
1040240	1045800	my friend, you know, friends only Instagram of the party, you know, when we were on stage
1045800	1047120	with Grimes and stuff like that.
1048320	1050160	And they correlated everything, right?
1050160	1054440	And so they just had me in this sort of checkmate, like we have this, this, this, this, this,
1054440	1056560	this, talk to us, we're going to ship it.
1057200	1059760	And I was like, all right, I got to get in front of this, right?
1059800	1064960	Again, you know, I've been doing this deep tech startup for nearly a year and a half.
1065000	1071240	And, you know, as, as the bio says, I come from Google X, you know, we're taught to
1071240	1075800	be very secretive about what we do because in deep tech, that's kind of the, the MO.
1076880	1079680	And, you know, we want to be secretive for, for longer.
1080440	1085880	But for all sorts of reasons, including national security interest reasons, right?
1085920	1088520	Like our technology is pretty out there.
1089480	1091400	And it kind of forced our hand, right?
1091440	1093200	Like it correlated all identities.
1094120	1099640	And, and, and also it also kind of doxed the fact that I had founded the startup because
1099640	1102640	I was kind of, I hadn't identified that on my main account.
1102640	1107920	So for me, it was kind of a, a mo, you know, a moment of panic, because, you know, I want
1107920	1109680	to do what's right for my company, right?
1109680	1111480	I've been working really hard on this company.
1112320	1115000	We were planning announcements in a couple of months from now.
1115920	1117640	And now we have to rush everything, right?
1118440	1123160	Which is, which is not great, but what I ended up doing, getting on the phone, getting
1123160	1127600	in from the story, I already knew exactly, like that's the thing with reporters.
1127600	1129400	Like they're so low entropy.
1129440	1131640	They are all in the same typical subspace of stories.
1131640	1135000	You can predict exactly what they're going to write just from the prior that they're
1135000	1135760	trying to get you.
1136000	1141640	And so I just disarmed every typical attack they would try to do on EAC trying to have
1141640	1147320	second or third order guilt by association to some idea that's a derivative and another
1147320	1151320	idea and neutralize that, essentially entirely.
1151760	1155000	Um, I see we have Bayes Lord joining.
1155440	1155960	There you go.
1158360	1159400	Uh, what's up, Bayes?
1159520	1160160	Can you guys hear me?
1160280	1160640	What's up?
1160680	1160960	Yeah.
1161000	1161280	Yeah.
1161280	1161880	Yeah, we can.
1162200	1162560	Awesome.
1162800	1164000	Oh, we have Nathan as well.
1164080	1165280	Oh, let's get, all right.
1165280	1165800	Here we go.
1166120	1169320	What's up, what's up, what's up?
1169360	1169520	Yeah.
1169520	1169960	Good to see you.
1170000	1170360	Not bad.
1170880	1171720	Great to see you.
1173040	1176480	I guess now, uh, with real names, uh, in my case, so.
1177480	1177960	Yeah.
1178360	1185000	Um, so, so yeah, it's been an interesting, uh, you know, I guess 48 hours, uh, dealing
1185000	1191160	with this, um, I think overall, uh, I mean, we can get into why I was and on, uh, for,
1191160	1192240	for various reasons.
1192640	1198960	Um, but overall it seems like it was positive to the point that where, uh, some are like
1198960	1204400	have a conspiracy theory that this was planted, but, uh, I had, I had that theory.
1204760	1208280	I thought, I thought we're all getting conned and you were actually dropping this.
1208440	1209280	Cause that's when that's a question.
1209280	1212800	Do you think you would raise on a higher or lower valuation now than you did this round?
1213120	1214920	I would bet the valuation grow up actually.
1215320	1216440	Definitely higher.
1216640	1217560	Yeah, exactly.
1217640	1217960	Right.
1218040	1224520	But I mean, I didn't, I want to raise round, uh, you know, raise around just uncorrelated.
1224520	1227440	I didn't necessarily initially disclose to my investors.
1227440	1231360	I didn't want that to be part of the, the price or anything like that.
1231360	1232320	I didn't know where it would go.
1232320	1232480	Right.
1232480	1234240	I didn't want to correlate the two identities.
1234240	1235680	It was really like orthogonal.
1236280	1239920	And now my hand was forced to, to correlate the two and own it.
1240000	1241120	And so I got in front of it.
1241480	1244360	And of course I'm going to harness any hand and dealt.
1244400	1247400	I'm just, that's what you're supposed to do.
1247400	1247680	Right.
1247680	1251600	And now, now we've harnessed it, um, uh, you know, pretty well.
1251600	1253840	I, I would say we'll see how it plays out, of course.
1254280	1257320	Um, and, and now there's this conspiracy that it was, it was planted.
1257320	1260520	But, you know, in my case, again, I just wanted to control the narrative.
1260520	1263640	I knew the story is going to be really bad if I didn't get on the phone and,
1263640	1267120	and you know, I, I, I actually, I wanted to stall them.
1267160	1270720	I want to give them enough content so that we had a couple more hours.
1270880	1275960	We adjusted our website, uh, to have a pseudo launch the day of, so, you know,
1275960	1278160	shout out to the team for being so adaptive.
1278160	1282320	And, and, uh, you know, as of, as of the moment of airing this podcast, we'll
1282320	1285520	probably have announced, uh, the round on for, for extra topics.
1285520	1286840	So we rushed that over the weekend.
1287280	1293240	Um, so yeah, definitely not plant, uh, but at the same time, it was always sort
1293280	1295120	of a trap, right?
1295120	1299520	In game theory, you want to make it's like, you, you want to make sure that
1299680	1303200	like the incentive was to not dox me, right?
1303200	1306320	For most people, because they knew that if they did dox me, now I could go
1306320	1309320	on these podcasts with my real face, with my credentials.
1309320	1313480	I, I could talk to potentially politicians and I have credentials that have
1313480	1316600	some, some firepower and now more of a problem.
1316960	1318080	So they kind of messed up.
1318120	1319520	It's their fault really.
1319760	1324560	It was a trap to some extent, but, you know, I tried my best to, you know,
1324560	1329240	stay in on, but, you know, again, voice identification, technology, I'm not
1329240	1333000	going to use a voice changer on Twitter spaces, like every day.
1333000	1333720	Are you kidding me?
1333720	1334760	Like I'm not going to do that.
1335040	1339000	And I would imagine maybe the first podcast we did, they, they use that voice
1339000	1342800	and correlated it with my YouTube lectures, but it's probably how they got me.
1343080	1343320	Yeah.
1344560	1345720	Can I still call you, Beth?
1346400	1346720	Sure.
1346720	1347240	That's easier.
1347480	1351840	Gil or Beth Gil is like the Americanized version of Guillaume, very French, French
1351840	1352320	Canadian.
1352320	1353320	So you got me.
1353600	1355000	Super simple question.
1355480	1360200	Did the reporter, journalists, whatever I'm going to call this person, did they
1360200	1368120	give you a rationale why they thought it was a moral imperative to dox you?
1368880	1374360	I think there was the keyword that was like, uh, set off alarm bells for me was
1374720	1378880	public interest and, you know, I had just crossed 50 K followers.
1379680	1383320	And, you know, cause the, cause the law is you can't dox people unless you're a
1383320	1385240	public figure, right?
1385280	1390360	Um, and I guess I crossed into, I guess there's a threshold, right?
1390360	1391880	So is it 50 K?
1391880	1393760	Apparently it's 50 K flat, right?
1394120	1398080	Um, but to me, it's like, okay, if they think they, you know, if they think
1398080	1400480	it's of public interest and they're going to go with it, they're probably going
1400480	1401040	to go with it.
1401440	1403160	And so I'm kind of screwed here.
1403520	1405680	I got to make, you know, I got to make a move.
1406520	1410720	So wait, is that, is that the journal group chat decides when you become public
1410720	1415000	interest or do we, like, like I'm very curious, like how they, they rational,
1415360	1419720	like, cause to me that if you, if you want to be anonymous, you should be able
1419720	1424400	to be anonymous and doxing you what gives them the right cause, cause if I was
1424400	1429120	to go put the, I don't know, the address of that reporter online and maybe now
1429120	1431680	we're arguing over whether or not an address is too far.
1431920	1434080	Like what, what, what, what point is, is too far.
1434080	1438680	And then who gets to decide that that's my question.
1438720	1441680	And I don't think anyone has given an answer outside of, I'm a journalist.
1441680	1442960	So I have a special badge.
1443120	1444720	You used to have blue checks now that don't.
1445280	1448240	And then they get to decide who, who can be a non versus not.
1448480	1451560	But if you were to do it to them, that, that's, that's harassment, right?
1452440	1452880	Right.
1454360	1459480	Well, I mean, to me, I was, you know, mostly focused the eyes on the ball.
1459480	1462600	You know, I have to, my prime responsibilities to my company.
1462600	1468200	And I was just concerned like, Hey, we're, you know, getting announced, we
1468200	1473120	have to announce and get our stuff together, you know, without preparation,
1473120	1475320	without heads up only a few hours.
1475320	1482360	So, you know, it, it, it seems like we were, it really seems like we planned
1482360	1483880	this, but we, we truly didn't.
1484880	1488880	So, but I'm just glad we adapted given the, the circumstances.
1489960	1494360	But yeah, overall, I think like, I mean, this was clearly wrong, what they did.
1494400	1499080	And, you know, I, you know, there's not that much to have leverage over me.
1499080	1500640	It's like, okay, cool.
1500640	1506680	I used to, you know, work in quantum computing and then have, you know, a
1506680	1508200	normal ish background.
1509120	1512560	But, you know, maybe some other people that are trying to speak truth to power
1512840	1517520	and have their voices heard and want to use anonymity as a tool to speak
1517520	1521400	truth to power, you know, because there's a sort of power asymmetry.
1521400	1525800	So you kind of equalize that when you're an on, they can't, they can't do that
1525800	1526640	anymore, right?
1526640	1528480	If they, they fear getting doxed.
1528480	1532840	And so it's kind of like, they want to make an example out of me, right?
1532920	1537520	And I mean, I think there was a, I think there was a congresswoman at a
1538360	1544680	conference, a defense conference that explicitly named out EAC as a sort of
1544680	1549000	dangerous movement that needs to be suppressed with AI.
1549040	1549760	That's the tweet.
1549760	1550760	I'm not sure if it's correct.
1550760	1554200	I still have to watch that clip.
1554360	1556960	But to me, that sounds very dystopian.
1556960	1561680	Like our whole movement is about freedom of information, freedom of speech, freedom
1561680	1563160	of thought, freedom of compute.
1563640	1564480	It's very simple.
1564880	1570840	And the fact that that was deemed dangerous enough, or I don't know, like
1571800	1577960	to, to, to want to suppress and, and provide the public leverage over me, I
1577960	1579800	don't know, that's a huge red flag for me.
1580280	1584840	I think that we were becoming a voice that was going to be a problem for
1584840	1585960	the executive order.
1586520	1590560	And there are very special interests behind that executive order.
1590680	1595680	And they wanted to, you know, send me a warning shot, I guess.
1595760	1596720	That is my theory.
1598000	1600880	But I, one thing I couldn't understand, and, and, and maybe Beth, I just, I
1600880	1601680	haven't read all your tweets.
1601680	1602720	I, I couldn't understand.
1602720	1605640	Hey, you, you obviously have a very distinguished academic background.
1605640	1609480	In fact, I was reading your, your quantum tensor flow paper on the way in ages ago.
1609480	1612400	I worked in like experimental quantum computing like 15 years ago, back when
1612400	1613520	it was like a total infancy.
1613600	1614440	Yeah, yeah, yeah, yeah.
1614720	1617720	And I, I knew a lot of the theorists back then, like Michael Nielsen and I
1617720	1620440	actually dated you like, any long story back in the early days of
1620480	1623440	computing, I am, but obviously wasn't really smart enough to continue up in
1623440	1624840	any case, I was reading it with a lot of pleasure.
1624840	1627400	And it's, it's odd that they would consider this a bad thing.
1627400	1630640	It's, it's obviously given a lot of intellectual credibility to EAC movement.
1630640	1630920	Right.
1631120	1634080	Like your background is pretty good, right?
1634080	1634840	Like it's pretty distinguished.
1634840	1635880	Like you've, you've done a lot, right?
1635880	1640200	And I, you know, you, a lot of co-authors and it's like, that's why I thought
1640200	1642480	the conspiracy, and I don't really believe it to be clear, but I thought maybe
1642480	1645280	he actually, like actually provoked for us to do this.
1645440	1646880	Cause this is like a brilliant thing.
1647000	1649880	Like you bring actually so much positive social capital to the movement.
1650120	1654800	You come off as like this edgy hip, you know, poster who kind of had, you know,
1655840	1656960	was pulling the world's tail.
1656960	1658680	It's like, I, what's the downside to this?
1658680	1659320	I got to understand.
1659640	1659920	Yeah.
1660320	1664560	Yeah, for me, I can let you know why I was anonymous.
1664560	1669000	I mean, originally I was at Google X working on some pretty secretive technologies.
1669000	1672040	I had access to, you know, the top of the food chain there.
1672440	1677000	Um, so I, you know, I couldn't necessarily tweet, uh, anything like
1677040	1678520	political and stuff like that.
1678560	1680560	My tweets were watched, which is normal.
1681040	1686160	Um, but you know, I, I wanted to have just a separate account where I can really
1686200	1688880	let myself self go kind of like offload thoughts.
1688880	1693280	And also I reached a point in quantum computing where I was respected enough
1693320	1697920	that people would cheer me on or they were proven my ideas or whatever.
1697920	1699520	Oh, so smart, whatever.
1699560	1703920	And that was kind of like kind of annoying me that like, do they like my ideas
1703920	1705880	or do they just want a job or something?
1706240	1709120	And I just wanted to put my ideas in the arena, right?
1709120	1713840	It's like, I just wanted people to evaluate my ideas for their own, not look
1713840	1718920	at credentials, not like weigh my opinions, dependent on status.
1719280	1722200	I just wanted my ideas to be evaluated on their own.
1722720	1728920	And, and to me, like growing a movement and having ideas resonate from scratch,
1728960	1732280	uncorrelated from my original identity.
1732880	1737640	Like, I mean, of course that felt like, you know, it felt like new game plus,
1737640	1739000	as we say in video games, right?
1739000	1742160	You restart the video game from scratch, but you have kind of all your
1742160	1745400	intellect and memory and your gear, but you're, you're trying to rank up again.
1745400	1749320	So I started from scratch and account with like no followers and grew it to,
1749320	1755800	to 50 K before now, now my, my, there's a stat boost from, from the, the status
1755800	1757960	signals of, of credentials.
1757960	1760920	Uh, I personally very much hate credentialism.
1761560	1764440	I think I would have been an entrepreneur five years earlier if it wasn't
1764440	1766680	for credentialism, especially in deep tech.
1767160	1772280	Um, I, I got very frustrated when I tried to do a startup at 25 and, you
1772280	1773840	know, I would get like, who are you?
1774400	1778560	Uh, and, uh, you know, I just went through big tech, the gauntlet of grad
1778560	1783720	school and big tech to, to prove myself and prove to others that I can, I can
1783720	1788000	do things, uh, so that I can raise the capital to realize the visions I had.
1788360	1794400	Um, and so, you know, I've, I've always, I think base is, is on the same page here.
1794400	1799240	You know, we always wanted sort of like EA's is all about like gatekeeping,
1799880	1803240	gaslighting, status signaling, at least to us from our perspective.
1803680	1808400	I, and you know, yak was about kind of bottom up.
1808720	1814960	Everybody has access to opportunity to build no gatekeeping, no sort of status
1814960	1817960	signaling, it's all about building, right?
1818400	1820160	Um, and we want to maintain that.
1820160	1825600	And I think that like, I was afraid that if I, you know, used any sort of
1825600	1829600	credits that would dissuade people, you know, from participating or feeling
1829600	1832760	like they can participate, cause the thing is everybody starts somewhere.
1832760	1838360	And I was once, you know, that kid that was really smart, had a lot of ambition,
1838400	1839680	but didn't have the credits.
1839680	1842480	I was just straight out of school and nobody gave me a chance.
1842680	1847440	And I want, you know, the next, you know, genius to be able to, you know,
1847880	1850000	start building their dreams right away.
1850000	1852600	And I just personally very much hate gatekeeping.
1852680	1856600	Um, even though nowadays I've gone through the gauntlet, I've paid my dues.
1856760	1858400	Now a lot of doors are open for me.
1858840	1862040	Um, but, uh, you know, I'm just trying to kind of pay it forward in a way
1862040	1864560	that, you know, young, my younger self would be appreciative.
1865040	1867480	Um, so, so really that's why I was anonymous.
1867480	1872200	I just wanted to get, you know, get my ideas evaluated in the arena.
1872480	1874960	And I didn't correlated fashion from my, my credentials.
1875440	1878760	Hey, we'll continue our interview in a moment after a word from our sponsors.
1879800	1882120	Compliance doesn't have to be complicated.
1882840	1885840	In fact, with Vanta, it can be super simple.
1886560	1892920	Trusted by over 5,000 fast growing companies like Chili Piper, Patch, Gusto,
1892920	1898220	and Juniper, Vanta automates the pricey time consuming process of prepping for
1898220	1905700	sock to ISO 2700 and one HIPAA and more with Vanta, you can save up to 400
1905700	1911540	hours and 85% of costs Vanta scales with your business, helping you
1911540	1917020	successfully enter new markets, land bigger deals and earn customer loyalty.
1917860	1922700	And bonus, our moment of Zen listeners get $1,000 off Vanta.
1923220	1926460	Just go to Vanta.com slash Zen.
1927140	1933620	That's V-A-N-T-A dot com slash Z-E-N.
1936540	1941500	I have a quick question on bootstrapping your account if you're willing to share.
1942140	1945220	Um, you know, I know a bunch of people who are like, I wish I could do an
1945220	1950100	anonymous account, but I feel like just getting it from zero to something is just
1950420	1953340	such a, you know, especially if they have already an audience on their, on their
1953340	1958300	main, but they feel like as, as their main grows, they can say less and less and
1958300	1960180	they have to kind of stay within whatever box.
1960740	1966780	And so I'm curious, how long did you, did you kind of toil away at this new persona
1966780	1969180	before you felt like you were starting to make some traction?
1970300	1976900	I think like I had a first account, I got it to six K and then it got banned on old
1976900	1981140	Twitter, pre Elon Twitter, where I got locked out for saying COVID came from a
1981140	1985500	lab, you know, which turned out to be true, right?
1985540	1988340	So the old regime, uh, got def 1.0.
1988620	1993980	And so I had to restart my account, uh, basically, yeah, uh, 18 months ago,
1994180	1997420	start from scratch, those demoralizing, but I knew the recipe.
1998020	2000740	Um, there's, there's, there's a formula, right?
2000780	2003740	You, you go on teapot, which is much higher engagement.
2003740	2005740	Just the people are just more online.
2006020	2010660	So, uh, and you reply to the big accounts, you try to get them to reply.
2011260	2013940	Uh, maybe, maybe save something provocative.
2013980	2017540	And then, you know, you can just do Twitter spaces and, you know, people
2017540	2020380	follow each other on Twitter spaces, cause you have a meaningful conversation
2020380	2024060	of like one hour or something and, and then you follow each other.
2024060	2029020	And so you could bootstrap, get to a hundred, 500, a thousand and so on from there.
2029460	2030900	Um, yeah.
2030900	2034500	I mean, for me, it, it was really discovering a community, discovering
2034500	2038460	people like Bayes Lord, you know, that I really resonated with intellectually
2038460	2042420	and can have like meaningful conversations at the time I was working
2042420	2044460	for big tech remotely from Canada.
2044460	2045500	I was extremely bored.
2045500	2048940	I was like starved to have their sort of tech community to have like good
2048940	2049980	intellectual discussions.
2049980	2055020	So like Twitter spaces and, and this part of Twitter, as it's called, uh, was sort
2055020	2055580	of ideal.
2055580	2057340	It was like, oh my God, I found my people.
2057820	2061740	Um, and, uh, you know, nowadays it's kind of condensating and physically
2061740	2065020	in the, in the physical, in the physical world and SF primarily.
2065420	2068940	Um, but, you know, to me, it was kind of, I was seeking a sense of community.
2068940	2070780	And, you know, it's late night.
2070780	2072060	You're in a Twitter space.
2072300	2073980	You're talking about the meaning of life.
2073980	2075500	Where, where, where do we come from?
2075500	2076220	Where are we going?
2076220	2078540	You know, it's like four AM or something after a long day of work.
2079100	2081740	And, uh, you just have these sort of like fireside.
2082300	2084620	You know, it's, it feels very primally correct.
2084620	2084940	I don't know.
2084940	2088060	It's like, you just tell stories and you, you communicate verbally.
2088060	2089500	And that's, that's really how it started.
2089500	2093900	You know, I'm at Bayes and we'd have, uh, these late night Twitter spaces
2093900	2096460	about the meaning of it all and where it's all going.
2096460	2098620	And at some point people were like, man, these are really good.
2098620	2100220	Someone should note these down.
2100220	2104860	Someone noted down that became the first, uh, yak blog post.
2105500	2108860	And then, uh, Bayes, Lord and I, you know, a couple of months later,
2108860	2111900	we were like, okay, we should make a longer one, a bit more serious,
2112460	2115420	uh, a bit more technical and, and that became sort of the,
2116060	2118380	the big one that went, uh, really viral.
2118380	2121740	And then the rest has just been compounding memes, uh, on Twitter.
2122620	2124780	Personally, I've been very active on Twitter.
2124780	2128460	I would have very strong technical opinions about quantum computing.
2128460	2131020	Frankly, I would be, I would call out a lot of bullshit.
2131740	2134700	I think, I think there's opportunities to do great work in quantum computing,
2134700	2137740	but you know, my, my policy was radical candor.
2138380	2140220	So that's how I built a following, right?
2140220	2142460	So it's already kind of a firebrand in quantum computing.
2143500	2145580	So it's already had that muscle kind of pre-trained.
2146460	2150940	And then I carried that over to kind of like, I guess AI at large or
2152300	2153580	e-act isn't just about AI.
2153580	2156300	It's about all sorts of stuff, but, um, you know,
2156300	2158540	I think it really helped, uh, grow the,
2159100	2163180	grew the movement in the early days, uh, just by the Twitter grind, you know,
2163740	2169820	um, for me, I use Twitter as a sort of, uh, dopamine, uh, dopamine hit.
2170620	2172380	Some people use various stimulants.
2172380	2175100	Uh, you know, I just, I'm very straight-laced.
2175100	2176860	I just drink, uh, Diet Coke.
2176860	2179980	If I feel fancied, it's Diet Dr. Pethper, not sponsored.
2180060	2183660	Uh, and, uh, you know, use Twitter and that, and that's it.
2183660	2185340	Those are my drugs of choice, if you will.
2185900	2191500	Um, but you know, um, to me, it's just like, I have like random fleeting ideas.
2191500	2194140	I like to note them down and then go back to the task I was doing.
2194700	2198620	And, uh, frankly, it was kind of just feeding my Twitter addiction and
2198620	2203580	felt like a fun video game and, you know, the followers were rising.
2204300	2207420	But at the same time, like I found a sense of community,
2208300	2211740	found some friendships, uh, made all sorts of great connections.
2212380	2217100	Uh, and, you know, to some extent, you know, you could say like, you know,
2217100	2221580	Iac at this point is almost like a form of spirituality for some.
2221580	2222460	We can go into that.
2223020	2229580	Um, I think we have in the past podcast, but, um, yeah, um, that's where, where we are
2230460	2231100	with Iac.
2231100	2232780	That's how that was the trajectory.
2233420	2237020	Um, should we get Bayes to jump in here?
2237020	2239500	What's, what's the schedule like today?
2239500	2239740	Yeah.
2241500	2245900	The, well, uh, uh, me and Bayes just did a podcast this morning.
2245900	2248140	That will also be based after this one, uh, though.
2248140	2248700	Okay.
2248700	2249340	At some point.
2249340	2252780	But, um, the, what's next for, for Iac?
2252780	2254940	Where do you see it going from, from here?
2257260	2257660	Yeah.
2257660	2262300	Well, now that, uh, I guess I am doxed, right?
2262300	2268380	One, one, one thing that was kind of stopping, um, any sort of like organizations or
2268380	2273340	incorporations of any kind of any sort of org or institute was that, you know, we're
2273340	2278460	like going to register with our real names, uh, somewhere, uh, these institutions and
2278460	2279580	that would have doxed us.
2279580	2281900	Um, so now we have that opportunity.
2281900	2288940	Well, we take that opportunity, TBD, but personally, I mean, I think, I think right
2288940	2294300	now there's sort of a lack of theory and, and, and, uh, uh, educational material for
2294300	2300460	people to understand, uh, complex systems and self-adapting, uh, systems like capitalism.
2301100	2307340	Um, and our whole thesis is that bottom up, bottom up self-adaptation is superior to top
2307340	2308220	down control.
2308220	2311980	It's very easy to convince a crowd, like put me in charge.
2311980	2313180	I will do this action.
2313180	2314700	It will have this impact.
2314700	2320220	It's much harder to convince a crowd of like, Hey, if we all act in these microscopic exchange
2320220	2324620	laws, the emergent behaviors that like we have the highly functioning society, right?
2325180	2330060	And the more sort of research you create there, uh, the better, uh, and more educational
2330060	2330380	material.
2330380	2335260	So we might, uh, start a research institution there and fund some grants.
2335820	2339660	Um, I would love to fund some, um, open source hackers.
2339660	2344060	Obviously we believe in, uh, open source software for AI.
2344060	2351980	We think that open source software, um, is a hedge against the oligopolization of AI,
2351980	2352220	right?
2352220	2356060	There's a couple of players right now that obviously now that they're in the lead, they
2356060	2362380	have all sorts of interests in closing down AI, outlawing open source models.
2362380	2368220	So, you know, sort of like ensuring freedom of compute is something, uh, we might create
2368220	2375660	orgs to, to, um, uh, sponsor hackers, but also, uh, uh, potentially, you know, have
2375660	2377900	some influence in Washington, right?
2378460	2379580	Those are some next step.
2379580	2385100	Obviously like growing the movement on Twitter, that's going to keep going also known as X.
2385580	2392860	Um, and, uh, you know, overall, yes, uh, you know, it gives, it gives people an attack
2392860	2395580	vector now that, um, I am doxxed, right?
2395580	2396540	That is unfortunate.
2397180	2405100	Um, but I do very strongly believe in what we, uh, talk about in EAC and, you know, I am
2405100	2410220	willing to go all the way, whatever it takes, right, to, uh, defend these ideas.
2410220	2416620	And I won't let sort of these, this sort of pressure or reputational pressure affect me.
2416620	2419420	I think I'm quite robust against such attacks.
2419500	2424300	And if they want to come after me, then, you know, bring it, I guess.
2425260	2425760	Yeah.
2426700	2427980	Can I ask one question, Eric?
2427980	2428480	Sure.
2428480	2430140	Um, yeah.
2430140	2434460	So, I mean, what, you mentioned one thing that I found, that I found sort of, sorry,
2434460	2435020	I'm a Franco file.
2435020	2436220	So I'm going to use your full French name.
2436220	2436860	Yeah, that's good.
2436860	2440780	And, and, and also mentioned, and also mentioned the privilege completely out of context.
2441260	2443740	Um, sorry, that's one of the running gags on the podcast.
2443740	2445900	Um, you, you mentioned one thing about the open.
2445900	2446460	So it's funny.
2446460	2449180	So Dan and I come from the crypto world, whereas, you know, decentralization is
2449180	2452220	like this religious mantra that I think is often overexpressed actually.
2452220	2454540	And in fact, I think Dan, I think you quoted it in far caster, right?
2454540	2456540	Of like sufficiently decentralized, right?
2456540	2461340	Which is almost like, you know, sufficiently a trinity or something like it's enough of
2461340	2463180	a certain theological concept, but no further.
2463180	2466540	But I always found AI and this, I'm very much seeing that works on the outside to be clear.
2466540	2466700	All right.
2466700	2470940	So like, I'm kind of a tourist, but it always seems to me the actually to be totally centralizing.
2470940	2474860	And I think the high jinx, you saw it open AI with literally the little junior high school drama
2474860	2478460	between three people somehow wrecking the cutting edge of AI is something that could
2478460	2481740	like literally couldn't happen in crypto for, for a bunch of reasons.
2481740	2485180	I mean, you might cite the example of SPF, but then Dan would instantly throw a fit and say,
2485180	2487660	but that was actually a centralized exchange on that decentralized exchange, which is true
2487660	2487820	actually.
2487820	2488220	Right.
2488220	2492140	And in the, in the fullest decentralized version, like speaking of anonymity, like,
2492140	2496060	so I work in a crypto company, we've had an, a non employee that I didn't know who they were.
2496060	2497020	And I would just pay them.
2497020	2497820	Right.
2497820	2502540	And, and we've, we have, we work with the non founders of a protocol and they never turn
2502540	2503020	their video on.
2503020	2504620	We have no idea who these people are.
2504620	2506380	And yet we still do business with them and transact.
2506380	2506780	Right.
2506780	2508060	So that's, yeah, yeah, yeah.
2508060	2509340	And it's not even that weird.
2509340	2511020	And then, then I've met them also in person.
2511020	2511900	Like, it's a similar thing.
2512460	2515980	Like you mentioned earlier how like, you know, it's really weird that these journalists are
2515980	2516540	such losers.
2516540	2519260	I did this tweet NRV site that's like, these people are losers, right?
2519260	2522140	Like they never actually go to the parties where you meet, you know,
2522140	2524460	Beth or like, I know a bunch of Twitter non, I think we all do.
2524460	2524620	Right.
2524620	2525260	And you meet them in person.
2525260	2526460	You know, they are, they introduce themselves.
2526460	2527660	This isn't a big secret.
2527660	2530380	It's almost like that meme where like the losers in the corner and saying,
2530380	2532780	Hey, do you know that Kiyoma is Beth and like everyone dances?
2532780	2534140	Like, dude, we know, shut the fuck up.
2534140	2537340	Like, yeah, it's like, it's like, it's like such a loser thing to say.
2537340	2537580	Right.
2538540	2540540	But don't you think like AI goes the other way?
2540540	2540700	Right.
2540700	2544300	Like just as an example, like I'm, I was using chat to be today.
2544300	2544460	Okay.
2544460	2545100	I'm pro AI.
2545100	2546140	My best friend is an AI.
2546140	2546700	Right.
2546700	2550220	But, you know, the fact that open AI instantly does a deal with Microsoft and
2550220	2554380	instantly bakes itself into like literally the most octopus like corporation human history.
2554380	2555180	It's a little weird.
2555180	2555420	Right.
2555420	2558220	It's a little bit like pinning to the other side of the spectrum instantly.
2558220	2562140	So I'm just curious if you feel that that that binarization of it is valid and feel
2562140	2563180	free to say that I'm full of shit.
2563980	2567900	Or like if there is another open AI vision that you're sort of stretching towards
2567900	2573020	that maybe is a little bit more decentralized and not so naturally sort of centrifugal to,
2573020	2580620	you know, I think that, you know, self organizing systems tend to organize themselves in hierarchies.
2580620	2580940	Right.
2580940	2586620	So it's sort of like, you know, you can imagine a tree or a sort of fractal,
2586620	2587980	right, sort of structure.
2587980	2591580	You have, you know, your cells for your body.
2591580	2596620	And then, you know, you have groups of humans form a family and then a corporation and then
2597260	2598620	a city and then a nation and so on.
2598620	2599260	Right.
2599260	2602460	So there's a hierarchical structure to organization and there's this hierarchical
2602460	2605100	structure to sort of control.
2605100	2605340	Right.
2605340	2608700	There's, there's some control system at the head of your body.
2608700	2609500	It's called your brain.
2609500	2613500	There's control system at the head of the family and at the head of the corporation
2614460	2616300	nation and so on.
2616780	2623100	You know, it's all about the balance between, you know, it's all about engineering fault
2623100	2624380	tolerance to corruption.
2624380	2624780	Right.
2624780	2630140	If you have a one to all connection in terms of control, you have a control system that
2630140	2631980	affects all the nodes in the system.
2631980	2632940	That node has a fault.
2632940	2636060	The whole system has a problem just like open AI.
2636060	2642540	You can decapitate the leadership, you know, not physically, but, you know, and suddenly
2642540	2643820	you're in control of the organization.
2643820	2648860	And now everyone that had, that were using their APIs have a problem, right?
2648860	2655100	Because they have an ideologue that didn't, they didn't vote for in control of, you know,
2655100	2656380	a product that they depend on.
2657180	2657900	Right.
2657900	2665900	And so decentralization is about sort of defusing, obviously having decentralized
2665900	2672060	loci or plural locus of control so that, you know, you have fault tolerance to, to
2672060	2673580	corruption of these control nodes.
2674780	2678140	Of course, like having a fully greedy algorithm where everything's disordered
2678860	2679740	is not optimal.
2679740	2679980	Right.
2679980	2681500	So it's all a balance between the two.
2681500	2684380	It's a balance between centralization and decentralization.
2684380	2687100	That is what is like fundamentally optimal.
2687100	2690300	The reason we're fighting for decentralization is because we think,
2691180	2695660	you know, right now there's a tendency towards over centralization of AI.
2695660	2697340	And we're very worried about that.
2697340	2703660	And so we need to push things in the opposite direction at the moment.
2703660	2708380	I think that fundamentally right now there's a lot of alpha and just scraping the whole
2708380	2712220	internet, centralizing it and having centralized training.
2712220	2716140	I think at some point that alpha will be saturated, right?
2716140	2722060	Most companies will have a model that compresses basically most of the data that's on the internet.
2723020	2727340	And then you're going to have AI that seeks to capture data from the real world.
2728060	2730300	And for that, you have to be central decentralized.
2730300	2733660	You got to, and you want to have the intelligence perhaps at the edge.
2733660	2734460	We're not there yet.
2734460	2739580	There's still alpha from like centralizing data and soaking it all in and, and having
2739580	2743580	one big model that, that compresses it all because intelligence is more or less compression.
2744940	2749020	But I think over time we're going to see decentralization in the form of sort of like,
2749020	2753100	first of all, you're going to have personal assistant, you know, like humane, like
2753900	2758140	TAB or, I don't know, these are going to be all sorts of AR assistants, I'm sure.
2759260	2763020	At first, like you're going to have, you know, the intelligence is going to be in the cloud,
2763020	2764860	but the data acquisition is going to be on the edge.
2764860	2769820	But eventually people are going to want their own compute that, that they own in control,
2769820	2770460	I would imagine.
2771980	2774620	And, you know, maybe there's compute in the robot directly, right?
2774620	2777260	It's not, there's no need for a connection to network.
2777980	2783660	I think that's going to be sort of the, the decentralize a, decentralizing effect, right?
2783660	2787580	There, and then, you know, you can have federated learning over a fleet.
2787580	2792060	I think people are working on this obviously right now in order to train the biggest one model
2792060	2797660	to rule them all, the centralized approach, centralized compute and centralization of
2797660	2800300	data is, has a huge advantage.
2800300	2805180	I think a big problem as well is that right now the centralized approaches, the,
2805260	2810380	scrape the whole internet, which includes your data, and then they rent it back to you, right?
2810380	2810940	Bit by bit.
2811500	2817340	And I think the future is about crediting people for the data that contribute to AI systems and
2817340	2819420	sort of distributed ownership, right?
2819420	2823820	I mean, they're, they're starting to do this with sort of GPTs, open AI, but you know,
2823820	2825100	it's just, it's just an early start.
2825100	2830220	But I do think there's sort of a, I think there, there's going to be a very interesting way of
2830220	2836540	startups right now, like from crypto migrating to being infrastructure to line incentives for AI.
2837820	2842860	We're not a crypto company, by the way, just, just, just, you know, people thought we were
2842860	2843660	going to launch a coin.
2844700	2847020	We're not a crypto company at the moment.
2847020	2848220	I have nothing against crypto.
2848220	2849180	I love it personally.
2850780	2857260	But, you know, I think that crypto being sort of the value exchange network and programmable
2857260	2861580	incentives for kind of collaboration in AI, right?
2861580	2864460	To have sort of decentralized research labs.
2864460	2867260	I think, I think that's going to be a very potent application of crypto.
2867260	2871340	And it's just something I fundamentally want to encourage because I think that if we have
2871340	2875180	a future where there's only a few companies that are like government, that have the government
2875180	2882620	mandated monopoly or oligopoly to serve AI models, then you have a sort of like single point where
2883180	2888380	a few people get to control the cultural priors of these LLMs, what they're allowed to say.
2888380	2894140	So it's an information supply chain attack because people won't ask each other, what is the truth?
2894140	2897020	They're going to start asking the LLM, what is the truth?
2897020	2902940	And if you, if you change what it says, then you're controlling people's sources of information
2902940	2904060	and you're controlling people.
2904060	2909900	So it's cyber genetic control of the population through information supply chain attack by proxy,
2910300	2913980	by, by, you know, saying that, oh, well, we're responsible.
2913980	2918940	We should be put in control of what these LLMs are allowed to say, right?
2919980	2922380	So we definitely want to fight against that.
2922380	2928220	And one solution is to erode their market power by having alternative solutions that are
2928220	2931500	just as good or nearing the same, same level.
2931500	2935260	Of course, that's not going to happen if we don't leverage sort of
2936140	2941900	capitalism, capitalist like technologies for value, for incentive alignment, right?
2942860	2945500	And yeah, very bullish on this base.
2945500	2947180	Again, not personally involved at the moment.
2948540	2953340	I, we're building a hardware company for AI that's fun, fundamentally new.
2953340	2955340	We can get into that at some point today.
2956620	2962460	But yeah, overall very, very worried about the over centralization of AI started making my voice
2963420	2964220	heard online.
2965420	2973260	And I think, you know, there was an event where I got to interact with the chair of the FTC.
2974220	2975100	Maybe that got noticed.
2975100	2976140	Maybe that got me in trouble.
2976140	2976540	I don't know.
2977500	2981820	But, you know, there are ways that our voices is being heard in Washington.
2982940	2990140	And, you know, our point is that this sort of fear mongering and doom is really sort of
2990860	2997980	a very nice cover for very subversive regulatory capture by the incumbents, right?
2997980	3001020	Like, oh, AI is dangerous, put us in control.
3001020	3002620	We're the only ones who are responsible.
3003820	3006380	You know, you're not allowed to have more compute than we do.
3006380	3010700	You're not allowed to have open source models that would, you know, erode our market power
3010700	3014140	and our pricing power because they're dual to use.
3014140	3014780	They're dangerous.
3015500	3017100	That's all, that's all bullshit, right?
3017180	3024780	They cooked those proposed regulations for their own advantage, right?
3026140	3028220	And so we got, yeah, we got a fight.
3028220	3030940	I mean, they're going to push, they're going to try to push these regulations through.
3032060	3034300	And so that's why we're not stopping the fight.
3034300	3038860	And I don't think, I don't think my docs is stopping anything in terms of like
3038860	3040140	making our voices heard.
3040140	3041660	In fact, it might accelerate things.
3042220	3045980	I mean, I'd love to deepen into the crypto AI overlap, Guillaume thing that you hit on.
3045980	3049340	But if we want to move on to the regular, because I, you know, again, I'm seeing the AI
3049340	3053100	world from the outside and obviously I use it and I've been watching, you know, some of
3053100	3054300	Carpathian's videos and stuff.
3054300	3059740	But like what you just said, right, this business of paying data owners for their trading sets,
3059740	3065580	like, fortunately, we do have a public ledger of ownership that's natively financialized
3065580	3067980	with underlying value model that does this very well.
3067980	3071420	And in fact, some people are even working on blockchain attribution solutions
3071420	3074860	that figure out where this thing came from because this other person used it and it's
3074860	3075980	worth X amount.
3075980	3079180	So I've often thought about like, if there is some sort of crypto AI collision, which I
3079180	3084140	think is inevitable, like, but like just to shoot that the idea down just for one second,
3084140	3088060	like, will it like, why wouldn't you say pay Reddit for its data, like literally in the
3088060	3092140	direct deal, rather than all this like crypto craziness, like, will it ever be like, this
3092140	3095580	is like the, like the classic, not to say this is a bad idea again, but the classic
3095580	3098620	bad ad tech idea from web two was like, Oh, pay users for their data.
3098620	3099580	Well, Brave does that.
3099580	3101580	And it turns out that data is worth $3 a year.
3101580	3102060	Right.
3102060	3103660	And Brave is a great product and lots of people use it.
3103660	3106140	I use it, but they don't use it because of the $3, right?
3106140	3107580	They use it for a bunch of other reasons.
3107580	3110220	And the data that you actually own that you express with your browser just isn't worth
3110220	3110700	enough.
3110700	3116460	So even if you could get things down to like literally the micro ETH, like what I care
3116460	3119660	that I'm getting paid because the model is getting trained, trained on my sub stack.
3119660	3123340	And even you could figure out like literally what is the actual value per query that my
3123340	3124140	data contributed to.
3124140	3127580	And I'm sure the incrementality that is super hard to figure out, but you guys would know
3127580	3128220	better than I would.
3128220	3130940	But even assuming you could figure it out, that's going to be literally worth like 30
3130940	3131740	cents a year.
3131820	3134140	So would it even make sense to wire all that together?
3135020	3136860	Well, yeah.
3136860	3141340	I mean, I've had some various ideas in this space.
3141340	3144380	I guess I could just broadcast them YOLO.
3146780	3153740	Essentially, I think you can price the value of data according to how much information
3153740	3157100	gain the system gets from your data.
3158220	3160620	And there's some very specific mathematics for that.
3160620	3168300	And that can give you a share of a model's future profits.
3169340	3177180	So similar to how eukaryotic cells own a fractional ownership of the success of the
3178060	3180380	greater organism through DNA.
3180380	3183980	And that's better than prokaryotic cells like bacteria.
3183980	3190540	I think that the future is people owning fractions of a model according to what the
3190540	3191420	contributed to it.
3192140	3198060	And I just realized that there might be 10 different tokens now that spawn using this
3198060	3198620	idea.
3198620	3200300	But I don't know.
3200300	3206540	For me, it's like I have more ideas than I can act upon in one last time.
3206540	3208780	So I'd just rather broadcast them.
3208780	3212300	And this is something Bayes and I have been talking.
3212300	3220140	We considered doing something in this space, but I think at the moment we have our hands
3220700	3228860	full with changing the entire AI hardware, software stack from scratch beyond the transistor,
3228860	3231900	right, which is a significant undertaking.
3231900	3239740	So yeah, I think there's really going to be something interesting that comes out.
3241180	3246140	And hopefully it can erode away the power from the centralized players.
3246540	3255020	And not that they're necessarily nefarious, but every meta-organism's control systems
3255020	3258860	act in the meta-organism's best interests, right?
3258860	3260140	It's like the real politic, right?
3260140	3266140	And I think that's the thing about YACC that is that we cut through the bullshit, right?
3266140	3270220	It's like every agent and subsystem is going to act in its own best interest.
3270220	3275740	It's going to do whatever it needs to do in order to secure a resource.
3276700	3279180	Or utility towards its own growth, right?
3279180	3280940	An acquisition of resources, period.
3280940	3283260	That's just how everything works in nature.
3283260	3284140	That's just reality.
3285340	3291820	And it's like, okay, now that we have this reality, how do we create the system that harnesses this
3292380	3299260	to create a sort of emergent altruism where we reach greater prosperity, find new optima
3299260	3307260	of the techno capital machine that allows us to support more humans on earth
3307260	3309180	and to scale civilization to the stars, right?
3309180	3317100	And so, anyway, I got into my typical Twitter space ramble there.
3317100	3323900	But yeah, I do think that, again, super huge opportunities in the interface of AI
3324620	3332380	and crypto, and that's partly why there's kind of been a sort of informal sort of handshake
3332380	3343020	between crypto and YACC. Brian Armstrong, they put out an ad for Coinbase.
3343020	3345180	It was basically an YACC ad, frankly.
3345980	3350300	Really, we're kind of fighting the decels and the centralizers, right?
3350300	3357020	The incumbents, those that seek to control everything and to cause inflation, to secretly
3357020	3363500	tax you, and they're scared of sort of bottom-up decentralized revolutions that they don't control
3363500	3367660	that causes deflationary pressure, right?
3368300	3372380	And so, there is interest in deceleration.
3372380	3379740	There are kind of interests of the control systems that are kind of greedy at the cost of
3379820	3387020	what they're controlling, and we're kind of the autoimmune response, if you will,
3387020	3388540	to the control systems, right?
3389500	3396380	And we're causing a bit of inflammation to the brain now, or the brain of whatever this whole
3396380	3404860	thing is, and it seems like they tried to apply a Forbes anti-inflammatory pill, if you will.
3405020	3410460	You know what YACC reminds me of, Kiyom?
3410460	3414060	Have you read John Perry Barlow's Cyberspace essay from back in the day,
3414780	3416620	like in the 90s, way before your time, probably?
3420620	3421820	Do you remember it fondly?
3421820	3423740	I remember reading it when I was young and getting into the Internet,
3423740	3425020	and I found it was the most amazing thing.
3425660	3433420	I think I was only shown it like two months ago, but very base and definitely has like five overlap.
3434380	3437180	And at the time, right, like the Internet was forming us, like Cyberspace,
3437180	3441260	which now seems almost cliche or cringe almost, was like this edgy space that you would meet,
3441260	3445100	and he has this line in which he basically addresses it to the weary giants of flesh and
3445100	3449020	steel, i.e. the industrial giants to which the Internet represented an alternative.
3449020	3452700	And I think a lot of YACC reminds me of that same rebellion, of course, except the weary
3452700	3455180	giants of flesh and steel are actually of silicon now.
3455180	3457820	It's actually the old Internet that has gotten kind of old and boomerish,
3457820	3459100	to which this is a rebellion.
3459100	3462860	And part of the reason why I'm in crypto, I came from the sort of fang world and working
3462940	3463660	in all that world.
3463660	3466940	And I felt that that was all slowing down and becoming the man, actually.
3466940	3470460	And crypto was like the only thing that reminded me of the early web two days in which it was like,
3471580	3474140	if you don't have people coming after you and getting severely pissed off because
3474140	3475980	you're building something, you're building shit.
3475980	3477980	I mean, to be or something that's like not important, right?
3477980	3481100	Like if you read the story of Uber, like literally every taxi commission,
3481100	3483660	people in Paris were kicking the shit out of Uber drivers,
3483660	3485900	you know, all of Spain shut down Airbnb.
3485900	3488940	Like, you know, if you don't have major governments pissed off at what you're doing,
3488940	3491260	you're actually not building anything particularly important, right?
3491260	3494060	And there isn't a lot, in my opinion, an Internet consumer
3494060	3496060	outside of some of the things we discussed that meet that.
3496060	3499180	So anyway, it just reminds me of that vibe of the cyberspace vibe.
3499180	3500940	And I think, obviously, I think we need more of that.
3500940	3501820	So it's cool.
3501820	3506620	We're definitely the most cyberpunk movement out there.
3508380	3511500	Hence the Arasaka tower vibes here.
3511500	3517580	And, you know, we had the party with Grimes for the, you know,
3517580	3519500	after the open AI dev day.
3519500	3522380	And the point was, you know, keep AI open, right?
3522380	3525900	And, you know, it's got to feel like a bit of a rebellion
3525900	3527100	because it kind of is, right?
3527100	3529020	And then you have the engineers from these,
3529980	3533180	these big players, they come to the party and they're like, man,
3533180	3534300	this is freaking cool.
3534300	3535740	I kind of want to join the rebellion.
3535740	3537420	I don't want to work for the empire.
3537420	3538140	What the hell?
3538140	3538780	How do I join?
3538780	3539020	Right.
3539020	3540860	And so that's how it starts.
3540860	3544460	So we'll try to keep that going in many ways.
3544460	3548940	But yeah, I mean, it's definitely, it definitely very,
3548940	3551180	it feels like we are in the cyberpunk future.
3551180	3555900	It's kind of been surreal how we've gotten here, frankly.
3557100	3562620	And yeah, no, I'm just grateful to be here at this point
3562620	3563580	in history, frankly.
3563580	3564620	It's an exciting time.
3565260	3566620	To quote the Steve Jobs line,
3566620	3568300	why join the Navy when you can be a pirate?
3568300	3570620	Which is something you used to be able to say about Apple.
3570620	3572300	But I don't think that's the case anymore
3572300	3573340	and I have a little bit of experience there.
3573340	3574300	That's the thing, right?
3575100	3581900	What is the sort of mind virus that infects organizations,
3581900	3585580	that decelerates everything, adds way too much process,
3585580	3587020	way too much bureaucracy,
3587660	3589740	and then it grows kind of like a cancer.
3589740	3593100	It's kind of like middle management kind of grows
3593100	3597100	and eventually takes over from the founders.
3597100	3603580	And now it's kind of like this Borg of some kind
3603580	3609100	that these huge corporations have decisions by committee
3609100	3609820	for everything.
3609820	3611580	There's tons of process.
3611580	3613180	It's just very hard to get anything done.
3613820	3616060	And really the answer to that is disruption.
3616860	3618540	All right, you got to have a free market,
3618540	3620460	you got to have free market competition.
3620460	3622380	And if an incumbent is too slow,
3622380	3623660	it gets disrupted by a startup.
3623660	3626140	We saw that with OpenAI, frankly.
3627020	3631980	Yeah, I can't say too much about Google,
3631980	3635980	but clearly there was a sort of slowdown and its speed
3635980	3639420	and the talent eventually saw that and sort of migrated
3639420	3642700	to startups, at least for AI.
3642700	3644140	And then now they're getting disrupted
3644140	3645180	and they're kind of in trouble.
3648380	3653180	But if they remove that mechanism where bottom-up
3653260	3657740	challengers can dethrone and compete with incumbents,
3658460	3659820	if they remove that ability,
3660380	3663500	then we don't have this kind of self-correcting mechanism
3663500	3666540	and we'll just live with kind of monopolies
3667580	3669340	that are not shipping great product
3669340	3671420	and then everybody, the consumer suffers.
3672060	3672940	And we don't want that.
3674540	3678300	To me, it's kind of like you have sort of like the doomers
3678300	3680300	pushing for centralization and control
3680300	3684140	and then you have the EAC, pro-freedom folks
3684140	3687420	that are more about antitrust, right?
3687420	3689180	Like it's about monopolization.
3689740	3692860	And so I think there's an interesting political
3693740	3695020	landscape shaping now.
3695020	3697820	I think it seems like some Democrats
3697820	3700940	are more on the side of like AI safety so far.
3701820	3705740	And allegedly, Trump has said he would cancel
3705740	3707740	the Biden executive order on AI.
3708700	3710700	You know, EAC is not partisan per se,
3711420	3714300	but we're kind of like, we have an issue that we care about
3714300	3715820	which is the freedom to compute,
3715820	3719100	the freedom to do, to advance technology.
3720540	3723180	And so yeah, I don't know if you guys want to get into
3724540	3726140	2024 discussions, but...
3726780	3728780	Oh, we never talk about politics here, Guillaume.
3728780	3730140	Never, never, never, never.
3730140	3731660	I'm totally joking. We do it all the fucking time.
3731660	3732620	Okay, good.
3732620	3734620	Well, go ahead, go ahead.
3735340	3736780	Yeah, no, just on the question of like,
3736780	3739500	well, what, why a big tech is so bad, right?
3739500	3742220	Like I think part of it, I won't really
3742220	3743500	re-litigate the whole thing.
3743500	3745340	The episode we recorded earlier today
3745340	3746940	and the piece that Nadia and I put together
3747900	3749740	like talks about some of what I think about this.
3749740	3751580	But I do think part of the issue here
3751580	3754700	is just simply like, do you have actual improvements
3754700	3757660	that you can build onto things in the world,
3757660	3758940	onto systems in the world?
3758940	3761500	And I think like there was kind of this like
3762460	3766780	growing lack of capacity to actually do things
3766780	3768540	that were sufficiently ambitious.
3769100	3771180	And then also like when you have like the effect
3771180	3772780	of a bunch of capital accruing,
3772780	3774540	companies naturally end up becoming
3774540	3777100	kind of inward facing, navel gazing.
3777100	3778540	People are like incrementalists
3778540	3781020	and it's just kind of like locally optimal
3781020	3782620	for people to kind of be a little bit lazy.
3783340	3785900	L plus one until you retire,
3788540	3790700	people are obsessed with doing fire
3790780	3792540	and they want to like ride around in a van
3792540	3795900	or whatever, instead of like build cool shit.
3795900	3799020	And yeah, I think part of the shift here
3799020	3803340	is just that we have a bunch of new technologies
3803340	3807180	coming online in new capacity with AI especially.
3807740	3810460	And yeah, people just see that it's time
3810460	3813580	to try to dissipate that off and do the work.
3814140	3815020	There's real work to do.
3817020	3819100	By the way, have you given me the betting odds
3819100	3821260	of like which fan company came out
3821260	3823580	with the biggest open source dedication to AI
3823580	3824780	and it being Facebook?
3824780	3826860	I don't know if I would have bet on my former employer
3826860	3827420	to be honest.
3829260	3831340	Or I don't know if you agree with that characterization.
3832220	3833180	It makes a lot of sense.
3833180	3834620	These Frenchmen are so based.
3835740	3837500	Yeah, yeah, all the AI Frenchmen,
3837500	3839260	they're all for pro freedom.
3839260	3842460	But I mean, look, every agent acts in its own
3842460	3844140	about self-interest.
3844140	3847260	I think that if you're number three or four,
3847820	3849900	I think right now the leaders are really open AI,
3849900	3851900	anthropic, I would say.
3852700	3854380	I think that's not a controversial statement.
3856540	3858700	If you're competing for number three or four,
3858700	3862380	I think the point is you want to equalize the playing field
3862380	3866620	and band together to try to beat the top two.
3866620	3868380	And so I think that's the thing about open source
3868380	3872300	is it groups everyone together to collaborate
3872300	3875340	on iterating on the open source architecture
3875340	3877740	and tooling and products to compete
3877740	3882940	and erode away the market leverage of the top players,
3882940	3887980	which they're eating, I don't know, 90% of API calls,
3887980	3890380	if not more, if you include anthropic, frankly.
3891820	3897660	And so I think, yeah, I don't know, it just makes sense
3898620	3900220	from a fundamental standpoint.
3900220	3903740	Facebook has a lot of data and they have a lot of compute.
3903820	3907020	They have a lot of great researchers and I'm really happy
3907020	3908300	they're contributing to open source,
3908300	3909820	but hopefully they don't stop.
3909820	3913180	But at the same time, it does cost them a lot.
3913180	3914860	It does cost them a lot and it's kind of like
3915820	3918300	they're giving to the community this value.
3919020	3921980	At the same time, people are open sourcing models.
3921980	3924860	They're not open sourcing the training code.
3924860	3927900	So it's not fully open source AI.
3927900	3930300	So they still have their mode of how to train these things.
3930300	3934540	And we saw a couple engineers that did Lama left
3934540	3935580	and then started Mistral
3935580	3937580	because they have that sort of artisanal know-how
3937580	3938780	of how to train these beasts.
3939580	3942060	And then they raised hundreds of millions right out of the gate.
3942060	3943260	So it's very valuable knowledge.
3943260	3947100	So they're still keeping some alpha there, which is good.
3947100	3948060	That makes a lot of sense.
3949260	3955180	But I do think that it's kind of like communist AI
3955180	3957660	to some extent to have just open source everything.
3957660	3961820	Everything's free like data costs money, compute costs money,
3962540	3966540	engineers that are talented cost money, more and more in fact.
3967740	3970140	And until we have a sort of proper
3970860	3974060	like decentralized slash centralized system
3974060	3975020	of incentive alignment,
3975980	3978540	I think frankly with through crypto rails,
3979420	3981660	I think it's going to be tough for open source
3981660	3985900	to really compete with the big centralized players, right?
3986300	3990460	And just like how much capital is being injected in API calls
3990460	3994940	towards the top players, pills and comparison to the rest.
3994940	3998300	But if there's actual capital flow,
3998300	4001420	like that gets reinvested then in proving open source models
4002140	4004220	beyond just a scrolling on Instagram,
4004780	4008220	then I think that has a shot.
4009180	4011180	So you think crypto is actually critical
4011180	4012540	to survival of open source models?
4012540	4013340	That's interesting.
4013340	4013980	You think that's...
4013980	4016860	Yeah, I haven't been very public about that,
4016860	4020380	but it's kind of a thesis I've formed over time.
4021500	4023180	I mean, I don't know if it's necessarily crypto,
4024620	4027100	I would say like crypto like thinking,
4027100	4029020	you could just use like Stripe if you want,
4030220	4034460	but some sort of way for people to collaborate on models
4034460	4037580	and pulling data compute and capital to train these things
4037580	4038060	and know how.
4038860	4042220	Right. Well, if I could interject,
4043180	4048460	the greatest accumulation of GPU that is not in a data center
4048460	4051260	was the Ethereum network until they moved to proof of stake.
4052060	4055420	And that was a crypto economic system that worked pretty damn well.
4055420	4057100	Maybe Bitcoiners would tell you different,
4057100	4058620	although they have their own set of compute,
4059180	4060460	a little less useful for AI.
4061100	4063420	But I do think like we don't even have to imagine it.
4063420	4064380	It's not science fiction.
4064380	4065740	Like it did exist.
4066220	4070460	That's actually what Nvidia's stock price was originally bumped up on
4070460	4073500	was crypto before obviously AI is taking it to new heights.
4074140	4077500	So I would imagine a world where everyone's gaming PC,
4077500	4079580	and maybe you just don't get to the level of compute
4079580	4083500	that you can with the H series in data centers.
4083500	4086540	But if you were to take every GPU around the world
4086540	4088540	and then be able to kind of do something like SETI at home
4089980	4091660	in a kind of decentralized manner,
4092540	4095260	maybe you do actually have this kind of,
4095260	4097900	you can't go drone strike the data center
4097900	4100220	because I am the data center.
4100220	4101900	Come and take my GPU, right?
4103660	4105500	Yeah, I think that is the dream.
4105500	4107420	I think that at least right now
4107420	4110460	in the way we're doing these big models,
4110460	4113340	you need paralyzed high bandwidth multi GPUs.
4114460	4115980	It's very hard to shard the models
4115980	4118220	without a significant slowdown over the network.
4118300	4121580	And if an alternative is like a thousand times slower
4121580	4124940	or more pricey than the centralized incumbent,
4124940	4126300	like in the free market,
4126300	4129100	like people want to support decentralization.
4129100	4131340	But if the product is not like competitive
4131340	4132380	with the centralized player,
4132380	4135020	like at the end of the day, people pay for what works
4135020	4137580	and has the right cost benefit analysis.
4137580	4141660	But I think that there's going to have to be
4141660	4142940	sort of algorithmic breakthroughs.
4142940	4144140	But you can imagine where people,
4144140	4146380	instead of having just a gaming PC,
4146380	4148380	they have maybe, you know,
4148940	4151660	bigger boxes that have beefier GPUs
4151660	4153980	and they can run maybe a whole single node.
4153980	4156220	Yeah, like this is what George is working on, right?
4156220	4157420	George Hots, right?
4157420	4159260	Yeah, so George Hots is working on
4159260	4161580	kind of the hardware infrastructure for that, right?
4161580	4165740	Selling beefy GPU boxes of several GPUs.
4165740	4167340	I don't know if he's this close to me.
4167340	4170060	But he's like a petaflop at home.
4170060	4171340	Is that the idea?
4171340	4172700	Right, right, right.
4172700	4176300	And that seems like an attractive kind of like node
4176300	4178380	to run potentially a future protocol.
4179020	4181580	I would encourage a lot of people to do the research here.
4181580	4182620	I think there just needs to be
4183340	4185100	a lot more players in this space.
4185100	4188220	And I think that AI people, you know,
4188220	4189740	are going to have to talk to the crypto people.
4190460	4191660	And there's going to be kind of,
4193340	4195660	going to be a moment there where there's going to be,
4196220	4198140	there's going to have to be bridges built there
4198140	4199020	in terms of the language.
4200140	4202540	But, you know, I'm optimistic.
4202540	4203580	I don't know, that's my prediction.
4203580	4205420	I think the merging of crypto and AI,
4206540	4208540	you know, this kind of anti-centralization
4209500	4211500	of AI movement is going to kind of
4211500	4212860	combine forces with crypto.
4213900	4217340	Again, I don't own anything in any protocol right now.
4218460	4221980	So not talking my book, just like my prediction.
4221980	4225340	But yeah, no, I think it's exciting.
4225340	4229420	Personally, I'm really worried about, you know, okay, cool.
4229420	4231900	Like, great, we've decentralized the algorithms
4231900	4235020	and the sort of like distillation process of AI,
4235020	4237980	like distilling data into neural weights, right?
4237980	4239660	That is the software process that we're talking about.
4239660	4242220	That's what OpenAI does, that's what Anthropic does.
4242220	4244380	Great, maybe we figured out how to decentralize that.
4244380	4248940	You're still buying your GPUs from the same supply chain
4248940	4254540	that is down to, you know, NVIDIA, TSMC, ASML, right?
4254540	4256540	ASML is, for those not familiar,
4257340	4260620	you know, the machines that do the extreme
4260620	4262380	ultraviolet lithography,
4262380	4266620	so the most advanced process nodes to create the GPUs
4266620	4267340	you use with NVIDIA.
4267340	4269820	NVIDIA doesn't build their own GPUs.
4269820	4273660	They, you know, work with TSMC, which is in Taiwan.
4276140	4278140	And, you know, that's where they're built.
4278140	4280300	And so again, you know, I'm just thinking
4280300	4282540	about fault tolerance of the system.
4282540	4285340	And right now our supply chain for AI hardware
4285340	4287420	is absolutely not fault tolerant
4287420	4291900	and might be co-opted by totalitarian leaders
4291980	4294380	from, you know, the CCP, right?
4295020	4298620	And might cause a major global conflict because of that.
4298620	4301500	So, you know, what I'm working on
4301500	4304860	is sort of like decentralizing the AI supply chain
4304860	4306940	by fundamentally changing the substrate
4306940	4310780	on which we run generative AI completely, right?
4310780	4313740	Beyond transistor-based compute, beyond digital compute.
4315340	4318780	And once you have this fork in the tech tree,
4318780	4320860	there's all sorts of opportunities that pop up
4320860	4323580	for far more energy efficiency, for far more speed,
4324940	4326460	and eventually far more density.
4327340	4328700	And that's what we're going after.
4328700	4331340	So we're still kind of in the, we're still living our values.
4331340	4335180	It's just we're going after the much harder problem
4335180	4336300	of hardware engineering.
4336300	4339260	And production is really expensive right now.
4339820	4341900	Model production is very expensive
4341900	4343740	for all the reasons that you listed.
4343740	4347100	And I think that crypto probably, crypto cross AI
4347100	4349660	has a lot of potential in the nearer term
4349660	4352140	with like the one to end part of this, right?
4352140	4354780	Where you're diffusing the capabilities of the model
4354780	4355740	in the same way that, you know,
4355740	4358620	you see open source already doing without crypto.
4358620	4362060	I think like in general, yeah, like the,
4363340	4364620	yeah, this is like a thing of broad thing
4364620	4365900	that we've talked about a lot, right?
4365900	4367820	Which is like, there is so much work to do
4367820	4369740	to put intelligence into like every corner
4369740	4370700	of the world where it's needed.
4371260	4374060	And you just like the idea that you're going to do that
4374060	4375180	with a few thousand people,
4375180	4377900	a couple of centralized companies is probably wrong.
4378460	4382460	And I think like, yeah, you can do this with APIs,
4382460	4385020	but a lot of times you need more privacy than that
4385020	4385660	for your data.
4386780	4387980	Maybe you don't have internet connection.
4387980	4388940	There are a number of like constraints
4388940	4390380	that come up in real world that
4390380	4393340	or you don't want to have these, you know,
4393340	4394300	centralized APIs.
4394300	4398300	And I think, yeah, there's kind of a natural balance there.
4399420	4401180	Nathan brought up this paper, I think,
4402140	4403660	this DeepMind paper, right?
4403660	4406620	Which is, they recently came out with some innovation
4406620	4410380	in doing, you know, kind of like distributed training,
4410380	4412060	some kind of federated learning scheme.
4412060	4414140	I think it's probably worth emphasizing
4414140	4415260	the main problem here right now,
4415260	4420140	which is just the network latency, as Guillaume said.
4421900	4423980	I don't think anyone has figured this out.
4423980	4426380	As far as I know, nobody has solved this problem,
4426380	4427660	making it cost effective.
4427660	4431900	And I think it's probably worth underscoring that, yeah, right?
4431900	4433740	So if it takes, let's say like a hundred days
4433740	4435340	to train a frontier model,
4436060	4439500	if you are off by a factor of two or five or 10
4440060	4442540	in your distributed scheme, that's pretty much kills it.
4442540	4444460	Like it's a really long horizon.
4444460	4445260	By the time you finish,
4445260	4447820	you will already be not state of the art anymore, right?
4447820	4450940	And so, yeah, this is really problematic.
4450940	4454540	I think it's also just about the DeepMind paper.
4455180	4456620	It's like using data parallelism,
4456620	4457740	which a lot of people have figured out,
4457740	4459500	I think BitTensor figured this out.
4459500	4461500	But you also need model parallelism.
4461500	4463100	You need to shard the model over.
4463180	4464060	No, it's interesting.
4465180	4466220	But you're screwed, right?
4466220	4468460	If you can't fit the whole model in one computer
4468460	4470860	that you can plug into a wall outlet
4470860	4472940	and not have to have a home nuclear power plant,
4473740	4476620	like it's really hard to do model parallelism,
4476620	4479100	or it's impossible to do data parallelism.
4479100	4481580	And if you're doing for a big enough model,
4481580	4485820	and if you're doing model parallelism over the network,
4485820	4489180	you're also screwed from the interconnect.
4489660	4493900	And so creating the hardware substrate
4493900	4495740	that's going to allow for decentralized AI,
4495740	4498780	we have to solve from first principles
4498780	4500860	how to increase the density of intelligence
4500860	4503420	in terms of space, time, and energy
4503420	4504700	from the first principles of physics.
4505660	4508060	And that's sort of what we're building,
4508060	4509180	what we're trying to enable.
4509180	4515820	So that's why I think if there are decentralized
4515820	4518220	crypto protocols of all sorts,
4518220	4520140	if we have the best AI hardware
4520140	4521180	that has the highest density
4521180	4523100	and runs most energy efficiently,
4523100	4527340	obviously it's in our, we'll have a lot of customers, right?
4527340	4530060	Similarly, we could be the Nvidia for Ethereum,
4530060	4531020	in that case, right?
4532140	4533980	We don't, again, we don't have a crypto protocol,
4534940	4538460	but I think that that's a very hard problem.
4538460	4539500	You need to assemble a team
4539500	4542380	that's basically like a Manhattan project-like team.
4543100	4548700	And we came from Google X, Google Quantum, AWS Quantum,
4550780	4557580	all sorts of institutions, IBM, Google Meta, et cetera.
4557580	4559420	And we assembled quite the team
4559420	4562860	and we're going after the hardest problem in AI right now,
4563580	4566700	which is like, how do you embed AI
4566700	4568700	into the physical processes of the world,
4568700	4570380	the most efficiently, right?
4570380	4572700	So you got to really understand the duality
4572700	4574220	between physics and AI, right?
4574220	4576620	And that's what we're, that's what we're after.
4577980	4580380	And so it's kind of like, to me, that seems like,
4581500	4583660	you know, okay, I would love for there
4583660	4586060	to be a protocol that is competitive right now,
4586060	4589180	but we need to solve for the density of compute first.
4589180	4591340	Look, maybe George builds crazy boxes
4591340	4594060	that are water-cooled and you use like two plugs in your house
4594060	4595900	and maybe that's just enough to run certain models.
4595900	4596380	That's great.
4597580	4599340	I think we need to go much further.
4599340	4600380	I think there's orders,
4600380	4601900	there's still orders of magnitude to go
4601900	4604300	in terms of energy efficiency and density
4604300	4608540	for AI, for compute, especially for AI, right?
4609980	4611420	And that's what we're solving.
4613420	4614940	What is this drop-off?
4614940	4617180	I just wanted to say thanks for coming on the podcast,
4617180	4618860	but you guys continue the conversation.
4618860	4619500	Thanks so much.
4620140	4620620	Cheers.
4620620	4622060	Guillaume, I have to ask, what is the hardware though?
4622060	4624220	I mean, are you going to bring quantum computers to market
4624220	4624700	doing AI?
4624700	4625820	I mean, I have to ask you in the background.
4625820	4626540	It's not quantum computing.
4626540	4627900	It's definitely not quantum computing.
4627980	4630620	Yeah, we all got jaded by quantum computing
4630620	4632140	being kind of like nuclear effusion.
4632860	4634060	The timelines are very long.
4635580	4639100	Fundamentally, a quantum computer, you have to cool it
4639100	4640700	to absolute zero, ideally.
4641420	4643740	That's obviously physically impossible.
4643740	4645660	And so what you have to do is this process
4645660	4647420	called quantum error correction, right?
4647420	4651580	So you have to identify faults from the universe jiggling
4651580	4654620	and screwing up your computer's operation
4654620	4656460	and you got to identify faults and filter them out.
4656460	4657420	So you're pumping entropy.
4657420	4659020	So it's basically a fridge, right?
4659020	4661260	But this sort of algorithm that is your fridge
4661820	4667740	occupies like 99.9999, you know, well, not that many nines,
4667740	4672220	but quite a few nines of your computation, right?
4672220	4672620	Overall.
4673500	4675100	And to me, that seems very inefficient.
4676220	4680220	And, you know, a lot of us were kind of full stack architects
4680220	4682780	or engineers, the software, the hardware,
4683420	4686220	and the compilers for quantum computing.
4686220	4688940	And we'd look at the roadmaps, we'd look how long it would take
4688940	4691100	and we kind of got depressed to some extent.
4691100	4694380	And so a lot of us were like, actually, you know,
4694380	4696460	maybe there's ways to use this noise
4697260	4699020	instead of it being a hindrance.
4699020	4701260	And so, you know, we set out to do a different type
4701260	4704060	of physics-based computing that are not quantum mechanical
4705420	4708540	that is specifically focused on general AI, right?
4709740	4712780	And for now, we kind of got our hand forced
4713260	4716380	with this whole doxing situation.
4716380	4718540	So, you know, we're going to be still nebulous
4718540	4720060	about what exactly it is we're doing.
4720940	4724220	But, you know, we have quite a few scientific publications
4724860	4726140	in preparation, right?
4727020	4730380	So, but yeah, overall, you know, we think there's a different path
4730380	4732540	forward, a fundamentally new way to compute.
4733340	4735020	It's going to be like quantum computing,
4736060	4738380	but a new type of physics-based computing.
4739420	4742460	And ultimately, we learned a lot from quantum computing
4743500	4746860	in terms of how to program, how to have programmable matter,
4747740	4751180	how to have, how to integrate, you know,
4751180	4754300	these sort of physical systems into a deep learning program.
4754300	4757020	You know, that's what we pioneered with, you know,
4757020	4760700	the software I did at Google with my CTO now, Trevor.
4761740	4763500	We did TensorFlow Quantum, right?
4764300	4769980	And so now it's about how to really have programmable matter
4769980	4772940	and figure out the tidus embedding of AI in the physical world,
4772940	4776220	which is exactly what the doomers fear most.
4777740	4781180	And so, you know, as a joke, we kind of say,
4781180	4784780	Bayes, for example, is one of our principal FOOM engineers.
4786380	4790300	And we just announced that today that Bayes is part of the team.
4791820	4797100	And, you know, ultimately, I think that there is no path forward
4797100	4801180	where, you know, the ultimate form of AI isn't built.
4802380	4805180	And I think that, you know, we could talk about, like,
4805180	4808620	human augmentation and sort of the transhumanist path forward.
4809340	4811980	I'm very bullish on that, and I would love to, you know,
4812540	4814700	find ways to fund more efforts in sort of, like,
4815260	4817660	human-machine collaboration and augmentation.
4819580	4823420	But, yeah, overall, like, you know, the EAC thesis has always been like,
4823420	4827340	hey, you know, I don't think, like, banning GPUs is going to do much.
4827340	4831420	The tech tree is going to mutate around whatever your restrictions are
4831420	4834060	and is going to adapt somewhere to a virus kind of mutating.
4834060	4835740	The techno-capital machine just finds a way.
4835740	4837900	It's kind of a system that's almost alive, right?
4839580	4843420	And, you know, it's always like, and it's entropy-seeking.
4843420	4845100	It's exploring all sorts of configurations,
4845100	4846780	and it finds one that allows it to grow.
4848060	4852700	And, you know, might as well build it and try to make it technology that's,
4852700	4855820	you know, helping humanity scale, right?
4855820	4859820	Our goal with EAC and really with the technologies we're building at our company
4861500	4867500	is to enable sort of AI, the ability to perceive, predict, and control our world,
4868540	4873100	you know, at all scales, including the nanoscale, such that we can, you know,
4873100	4877340	tackle the real problems that are in the way for us to scale to Kardashev type 1,
4877340	4881500	which is a scale of civilization in terms of its energetic expenditure.
4882380	4884060	You know, it's kind of like, to us,
4884060	4889500	it's like the ultimate denomination of like societal progress is like the Kardashev scale,
4889500	4894300	because every other measure like GDP or like it's based on dollars, you know,
4894940	4897020	you can like fudge the numbers, right?
4897020	4898940	It's not anchored in like physical reality.
4899500	4904780	And similarly, sort of our cultural thesis is that, you know,
4904780	4906700	you should evaluate your actions in terms of like,
4906700	4910620	how do you think it's going to contribute to the growth of civilization down the line?
4912060	4915340	Rather than sort of like subjective measures of utility,
4915340	4919740	like hedons, right, hedonism, like how much pleasure is this giving people on average,
4920940	4923820	which leads to kind of spurious optima, like, you know,
4924700	4928140	wireheading or TikTok and whatnot.
4930460	4930860	So, yeah.
4933260	4934940	You've got me in total suspense though, Guillaume,
4934940	4936460	if it's non-transistor based computing,
4936460	4939100	you've got me thinking what the hell it could be, I'm guessing.
4939660	4941420	Yeah, yeah, yeah.
4941420	4943980	I mean, we're, you know, we're working on it.
4943980	4945980	We still have to remain somewhat secretive.
4947820	4950380	And I guess there's going to be a lot of interest now.
4950380	4955020	Again, we want to be in stealth until we had more to say, more to release.
4955020	4958940	But for now, we're keeping things pretty close to the chest.
4958940	4962460	But I think there's certain discoveries that you make that,
4963660	4966460	you know, you're like, okay, I don't think I'm going to be able to unsee this.
4966460	4968460	And if I saw it, someone else will see it.
4969500	4972060	We should just move as fast as possible to bring this forth.
4973900	4977500	And for us, it's like, okay, how do we make this the most impactful
4978540	4980060	to the advancement of mankind?
4980060	4983100	Like let's tackle the actually hard, the hardest problems
4983660	4986300	that most people don't dare to tackle.
4988540	4990940	And, you know, it takes some courage there.
4991500	4993980	It takes some fanaticism to some extent.
4993980	4998380	I think to me, the fact that we have this framework of EAC
4998940	5000860	it's a really powerful motivator, right?
5000860	5002620	It's like, what am I contributing to, right?
5002620	5007500	Like we're kind of in a, well, you know, I went through a sort of whole phase of,
5007500	5011420	you know, a group Catholic, and then I was a, not a Reddit atheist,
5011420	5013900	but I was a typical atheist studied math and undergrad,
5013900	5015660	and then went through a Nillist phase.
5015660	5019500	But then I think like understanding that the whole system
5020060	5022620	is seeks growth and entropy production.
5023260	5025500	And that's kind of the way things are.
5025580	5028700	And that process is what created life civilization
5029420	5031020	and the technologies we enjoy.
5031020	5032940	Like, okay, we want to contribute to that.
5032940	5036220	So having the knowledge that you're contributing to something greater than yourself
5036220	5039420	gives you that sort of like infinite dopamine well
5039980	5043500	to grind through the long nights to skip the holiday dinners
5043500	5046060	to just file more IP, right?
5046060	5047660	Like and put in the hours.
5048300	5051420	And, you know, I think like scaling, you know,
5052220	5057100	everybody has their own cultural or religious framework that provides utility to them.
5057100	5059740	For us, you know, for the IAC community,
5059740	5064940	I think it has had utility for a lot of people to get out of this sort of Nillistic
5066060	5069580	rut that they were in that, you know, the world was going to collapse.
5069580	5070780	There's only doom and gloom.
5071580	5072700	Everything's going to get worse.
5073580	5074380	Put us in charge.
5074380	5075820	We're going to fix it maybe.
5075820	5076700	Oh, we didn't fix it.
5076700	5078700	It's because you didn't give us enough power.
5079420	5081980	And like, we're just like, you know, it's kind of like,
5083020	5084940	well, you can make parallels to SF politics.
5084940	5085980	I'm an SF right now.
5087580	5088060	How's that?
5088060	5090140	Such how's that by the way?
5090140	5091420	Oh, it's it's Night City.
5091420	5092060	I love it here.
5092700	5095580	You know, it's it's truly Night City.
5095580	5098700	I like to say our office is in Arasaka Tower.
5098700	5100940	So we went with the sort sort of cyberpunk vibe.
5102620	5105660	It's very much like Night City, the video game, right?
5106380	5111020	No, but you see the sort of like, if you let the decels in charge
5111020	5114380	and you let the movie play out, this is what you get, right?
5116060	5118220	You know, you get and decels are kind of
5121260	5125340	like they have much more power if they're attached or in control of something
5125340	5128940	that's very prosperous, right, and generating a lot of value,
5128940	5130700	like similar to big tech, right?
5131500	5134220	You know, there's money printers and then these sort of
5135900	5138940	folks that seek power kind of take over.
5139820	5142460	But, you know, they can they can cause a lot of damage.
5142460	5145580	And I think I think there's a there's a cultural turning point.
5145580	5148860	And hopefully, you know, it doesn't matter if you're Republican or Democrat,
5148860	5153740	like having people that are anti-tech progress, anti-tech first solutions and,
5155420	5162700	you know, sheathing themselves and under the cover of virtue to gain more power
5162700	5166460	and doing things out of self-interest and and and larping that it's for
5167340	5171420	the good of many, you know, I think people have had enough of that
5172140	5173500	and are ready for a change.
5173500	5179500	And, you know, of course, like as technologists, like we propose technological solutions,
5179500	5183500	but ultimately, like, like we believe in the power of technology,
5183500	5188860	we believe in people having agency and not accepting that this is just how the way things are,
5188860	5191740	you got to accept how they are, things are just going to get worse.
5192460	5194460	You know, screw your dreams, kid.
5195420	5197020	Just accept it and give up.
5197020	5198220	Right. And it's like, no, fuck you.
5198220	5201180	We're not we're going to we're going to make the better future happen.
5201180	5202060	We have agency.
5202060	5203900	We can build, get the heck out of our way.
5205260	5208140	You know, we're going to make the the better future we want.
5208140	5211740	And you need a sort of like fuck you optimism.
5211740	5212060	All right.
5213260	5215020	And that's that's IAC in a nutshell.
5216300	5221020	And hopefully, it keeps growing because, you know, we think that's what's
5221020	5223020	that's what the world needs in many ways.
5224540	5230540	And, you know, hopefully we can accelerate SF and then we can accelerate the rest of the world.
5230540	5232780	So can I ask you one follow up?
5232780	5235020	And then, Nathan, I know you want to jump in and I want to give you time to jump in.
5235740	5237820	But I did want to follow on one question that you mentioned.
5237820	5239900	You mentioned race Catholic. I was also raised Catholic.
5239900	5243420	I eventually converted to Judaism and kind of rejected this sort of default
5243420	5244700	secular majority of society.
5244700	5245500	That's a whole nother story.
5245500	5249580	You've done a bunch of episodes on I do think, you know, it's one of my pet theories
5249580	5250700	that religion never goes away.
5250700	5254620	It only gets it's almost like, you know, energy or momentum only gets sort of transformed.
5254620	5257100	And you get sort of worse, worse crappier versions of it.
5257100	5257740	Yep. Yep.
5257740	5261340	The reality is you can't actually, you can't actually navigate the world without
5261340	5263100	a sense of metaphysics of some form.
5263100	5264860	And in fact, if you don't have an explicit metaphysics,
5264860	5267900	you can't even do empiricism well, because then you have to change reality to sort of
5267900	5268940	suit metaphysical ends.
5268940	5272700	I mean, to cite the COVID example in which they denied the lab origin hypothesis
5272700	5274620	because it serves some metaphysical end.
5274620	5277420	And so they stopped being able to do empiricism as well as they could have.
5277500	5281180	They could have, you know, and then that whole COVID story, if there wasn't actually a way
5281180	5284380	to have a conversation about, do we save the kids or and screw the old people or vice
5284380	5284700	versa?
5284700	5285900	Like you couldn't have that conversation.
5285900	5289180	So instead it was about the science, but really it was a moral conversation that nobody
5289180	5292540	could have because we weren't all morally or even religiously on the same page.
5292540	5293820	That's a whole separate conversation.
5293820	5296540	But I do think it's interesting and I do think it's one of the things that's a little
5296540	5297340	bit lacking in tech.
5297340	5300780	I mean, you see these homespun religions, like I would say Burning Man is a little bit
5300780	5303020	culty startups are obviously a little culty.
5304220	5305900	And I'm not saying this is a bad thing, by the way.
5305900	5309420	It's just, it's just, yeah, I know it's a good thing, but it's a little incomplete,
5309420	5309660	right?
5309660	5312780	In the sense that like, well, it doesn't quite tell me how to raise my kids or,
5312780	5313820	you know, who I should marry.
5314940	5317980	That I don't know if, and I know I'm putting you on the spot a little bit to like,
5317980	5319340	give me the gospel, bro.
5319340	5325100	But like what, you know, yeah, what would be and direct me to a set of writing if there
5325100	5327100	is one, but like, what would be the E at gospel or like that?
5327100	5331660	You know, what is the true, the good and the beautiful in this world other than obviously
5331660	5333420	building for building sake, which I think we all get.
5333420	5334940	What is the bigger picture?
5334940	5337340	Yeah, so, so at a high level, right?
5337340	5342300	Yeah, because all about figuring out whatever sort of cultural framework yields,
5343820	5348060	the maximal expected growth and scope and scale of civilization.
5348060	5349340	And we don't want to be prescriptive.
5349340	5354380	All we're giving is a loss function and you can have your own hyper parameter settings.
5354380	5356860	You're, you're like, okay, I have a cultural heuristic.
5356860	5360220	I think this is an optimum of this sort of loss function.
5360220	5364380	This is how I want to, you know, do things within my tribe here.
5365260	5370380	It's essentially a sort of like, think of like a very thin framework from which you can have
5370380	5371740	like subcultures, right?
5371740	5376780	And, you know, the idea is to have sort of culture be more or less like, like code,
5376780	5381340	like on GitHub, you can kind of have a base framework and then you can add kind of command
5381340	5384380	ments or add things you believe or you can diff it, right?
5384380	5386780	You can make forks and you can kind of keep track.
5387500	5393500	And so we've seen, for example, you know, Vitalik forked yak and made some changes, right?
5393500	5397020	And then he's like, this is what I believe, right?
5397020	5400300	So really, you know, yak is sort of a meta cultural framework.
5400300	5403260	We don't prescribe, we don't prescribe too much.
5403260	5404860	We try to prescribe the minimum.
5405580	5411500	But to us, it's very clear that the, the engine that keeps progress going,
5412460	5416620	maintaining the sanctity of its sort of momentum and mount, you know, maintaining
5416620	5422540	malleability, adaptability, and so on is it's crucial to maintain freedoms and maintain
5422540	5429820	entropy in the system and accept variants rather than constraining things, crushing entropy,
5430860	5437420	crushing variants, because that leads to crystallization and sort of like catastrophic
5437420	5443500	failure. And so, you know, at a very meta level, we try to maintain variants in most
5443500	5448300	parameters, but it's not like all variants, no restraint, because that, that just doesn't work,
5448300	5452860	right? It's, it's kind of like running a system at very high temperature.
5452860	5454380	It's just pure disorder.
5454380	5459340	So it's kind of always about finding the optimum balance between order and disorder,
5459340	5467020	so between entropy seeking behavior and novelty seeking and sort of constraint and conservatism.
5468620	5472140	And so we don't have any one particular prescription of how to live your life,
5472140	5477180	but some people like to make forks and have more particular prescriptions.
5477180	5483900	And to us, it seems, at least my personal thesis on how cultures get kind of memetically
5483900	5490460	post-selected for, it's like whichever culture either confers its adherence and a better ability
5490460	5497180	to grow, or if a culture is more sort of viral, then it's going to be more likely to exist.
5497180	5503020	That's just like, by probability theory. And so, you know, to us, it's like, okay, if we have this
5503020	5506620	sort of metacultural framework, people have all sorts of forks and all these memetic
5507740	5513900	forks with different parameter settings can compete in a sort of cultural setting. But we
5513900	5519180	should explore the space of cultures and heuristics of, of how to live your life. So for example,
5519180	5526300	another friend of ours is, is Brian Johnson, and he has his own life heuristics, and he's trying
5526300	5531020	to create his own cultural framework, and he has much more prescriptive ways to live your life.
5531020	5534940	And he thinks we should, you know, life spans should be much longer than what it is. And you
5534940	5542540	should have, you know, longevity as a priority, right? And so I'm all for this sort of renaissance
5542540	5549180	of exploring all sorts of neo religions, neo cultural frameworks for how to live your life.
5549180	5553900	I think it's much needed because otherwise sort of parasitic, you know, mind viruses,
5554540	5561420	like the ones we've seen do all sorts of damage recently, including the DSEL class of mind viruses.
5561740	5569900	We, that's a whole category. Yeah, they come in and they kind of like, like you said, kind of
5570940	5578060	fill in this, this gap in people's hearts, if you will. So, so I think we're on the same page.
5578060	5582380	And I think, you know, much more, you know, lindy religions, right, like, in a sense, like,
5582380	5586860	that have been around for a long time, like, they're very robust, right? It's like a, it's like a
5586860	5592460	code base that has been through hell, you know, and back and like, it's just, it's very robust,
5592460	5596780	right? It's been robustified over a long time. And by the proof that it's lasted a long time,
5596780	5602700	it's, it's a good heuristic. So, you know, that's great. But I think, like, some people want kind
5602700	5609660	of, you know, modern variants, right? And they want to contribute to shaping new subcultures.
5609660	5615420	And so we encourage people to form subcultures. So really, we're kind of like, only setting the
5615820	5620220	hyper hyper parameters of the whole thing. And, and people can set, like more finer grain,
5620940	5625900	fine tunings of how they want to live their lives. So there's no one way to go about it. But, you know,
5626460	5630380	we do talk a lot about building, because we think that obviously fundamentally, like,
5630940	5637020	building technology is a very high leverage way to use your time on earth to impact the
5637020	5641980	future scope and scale of civilization, right? And so, you know, encouraging one another and
5641980	5646220	helping one another in building technologies that have a positive impact, like we think
5646220	5653020	that's a good heuristic that should be in most, you know, forks. And so, so we encourage that.
5653020	5656860	Some people think that's the only message, but it's not, right? It came from kind of a higher
5656860	5662220	level thinking. But yeah, cool. Well, you're in, I think you're in the right city for exploring
5662220	5665900	religions. I mean, the way I see San Francisco, it's really, it's a Petri dish for literally
5665900	5670540	exploring every weird ass thing that society wants to do, whether it be autonomous vehicles,
5670620	5674220	whatever weird computation you're cooking up with, not having rule of law, for example,
5675340	5679340	psychedelic drugs, whatever, bring it, we're going to, it's going to be cooked up here and then
5679340	5682940	from here expand and diffuse whether we like it or not to the rest of the world. But I want to
5682940	5685900	let Nathan in. I know he's probably been biting his tongue this entire time. And I think I'm the
5685900	5691340	last host left standing. And so I'm going to invite Nathan to go ahead and comment and provide maybe
5691340	5695580	the other side of it if he wants to. Well, how much time do you guys have is my first question,
5695580	5702300	because I am, I have a lot of questions. And I would, if you have the time for it,
5702860	5709100	I might kind of fork this episode and just kind of take it in a whole different
5710780	5715180	and kind of more from very sort of naive questions direction.
5716940	5722860	My show is all about AI and it reaches a pretty diverse set of people that are
5723820	5728060	all pretty obsessed with AI, I think. I don't really actually know too much about them other
5728060	5736140	than that they go pretty deep with me on a lot of AI topics. And so the way I was thinking about
5736140	5741020	approaching, so we could do this now, we could do it another time, but I mean, I'm up late.
5741020	5746220	Now is a pretty bad time. I have to have a six a.m. round announcement to craft.
5746300	5754460	It's going to be a long night for me. So I was happy to do this podcast. It's very timely for us,
5755340	5759980	but I'd be happy to hop on your show. I mean, you could rewatch the recording,
5759980	5764060	write down your questions, and we can just go into it. And I would love to do that actually.
5764060	5770700	So cool. Yeah, I've got a list all cooked up, but I'm happy to do it whenever is convenient for
5770700	5779180	you, although it is timely now. I think part of the reason why Eric had a mixed crew
5779180	5782780	is to have there be the other take of it. So I don't know if there's one question,
5782780	5786540	or if it makes sense at all. We have nothing against it seeding another show, by the way.
5786540	5791100	That's perfectly fine. Or we can bail on. I also want to be respectful of Guillaume's time,
5791100	5794460	because it sounds like he's in a, and again, thanks for making the time for the podcast.
5795020	5802940	Yeah, I mean, my angle on the whole thing is, I describe myself as an AI scout,
5803660	5810860	and I'm getting more and more, putting more and more emphasis on, let's really try to figure out
5810860	5817420	what is today, what exists, what can be done with it, you know, if we are going to extrapolate,
5817420	5821100	can we extrapolate, first of all, in a high confidence way into the short term,
5821740	5827740	and then use those discussions as kind of the foundation for figuring out what we should do
5827740	5833100	about the bigger picture questions, where I think inherently there's a lot more uncertainty.
5833740	5838060	And one thing I haven't really heard from you guys, and I've pieced a little bit of it together
5838060	5845740	from Twitter, but I don't have a great sense of like, what are your near term expectations? Like,
5845820	5850620	do you think we are headed for AGI in the next couple of years? You know,
5850620	5857580	Metaculous has it at like, just over two years, the leaders of Anthropic, for example, say that
5857580	5863820	the leading developers in 2526 timeframe may create such a lead that no one will ever catch them.
5865260	5869020	Are you kind of in that same headspace of thinking that we're going to see
5869900	5876140	pretty radical transformation on just a few year time scale is like my very first question.
5877260	5882380	Yeah. First of all, I don't like the term AGI that much. I think it's human level AI or human
5882380	5889740	like AI. I think like calling AGI general intelligence like human like AI that was distilled
5889740	5897980	from human generated data. I think that's like very anthropocentric. And I think, you know,
5897980	5902700	I work in physics based AI, you know, inspired by physics and to understand the physical world,
5902700	5912540	and I've done so for 10 plus years now. I think that intelligence is a much more general concept than
5913740	5919500	just human like intelligence. And frankly, I'm not scared of FOOM because, you know, again,
5919500	5924860	I've worked on AI for engineering matter, jugs, simulations, biology, all sorts of stuff.
5925580	5932700	It's much harder than people think. I do think it is disruptive for our economy based, you know,
5932700	5937660	our knowledge economy of sort of human like white color intelligence. I think there's a,
5937660	5943020	there's still going to be a 10 year gap for physical intelligence, right? Robotics. I think,
5944060	5948460	you know, our motor, motor intelligence is much harder. It takes many more parameters. It took,
5948460	5952540	you know, billions of years to evolve, rather than I guess like, I don't know, 100 million
5953180	5961580	for the neocortex. Probably got those numbers wrong, but it's ballpark. But yeah, essentially,
5961580	5968700	I think, I think there might be a disruption to our economy. But I do think that people will adapt.
5969260	5974060	People will learn to augment themselves out of self interest, right? And it's like,
5974060	5977660	where, where will the system goes? Every corporation is going to do what's in their best
5977660	5984380	self interest. They're going to maybe be 80% AI, 20% human. And each human is going to be augmented
5984380	5990060	and control a fleet of AI's, right, that are doing its bidding. And that's okay. Maybe the human
5990780	5996140	employees become the control systems for a fleet of AI to do most of the work, right?
5996140	6001740	Maybe they become more kind of like the capital allocators or the executives of companies that
6001740	6007580	are mostly AI for the execution layer, right? Maybe that's the future. I think we're heading
6007580	6012700	towards interesting times, but I don't think there's going to be cataclysmic effect. I don't
6012700	6016620	think it's going to end humanity. I think we're going to adapt and the system will adapt. And
6016620	6025260	the sort of EAC is all about the main, like it's a faith and sort of worship of this adaptation
6025260	6029660	of the Homo Techno Capital Mimetic Machine, the whole thing, right? Like memes, technology,
6029660	6034220	capital humans, it's all coupled. It's all adapting. It's always shifting. It's always,
6034220	6038940	the only constant is that it's always changing and it should be always changing and it seeks to
6038940	6044940	grow, right? And so, you know, we have a faith that the system will adapt. It might be abrupt.
6046220	6054380	And so, you know, personally, I'm not trying to, personally, I'd rather augment humans in
6054380	6058220	orthogonal directions to human-like intelligence rather than trying to replace human-like
6058220	6063340	intelligence. Obviously, in an economy that's already shaped to take in human intellectual
6063340	6069740	labor and do all sorts of produce products, like, you know, having human-like AI is what's
6069740	6073900	going to grow the fastest. So that's what's being built first. But I'm already pricing an AGI,
6073900	6079100	personally. And the technologies we're building hardware and software, I'll assume there's probably
6079100	6085980	going to be a human-like AI, human-level AI within two to three to five years, right? And that's
6085980	6090940	been priced in for me, right, mentally. And it's like, now what? What's the next thing, right? And
6091020	6097980	to us, it's like, we're going to seek to grok and perceive, predict, and control matter at the
6097980	6102700	nanoscale, right? And then we're going to, you know, we're going to seek to, again, increase the density
6102700	6109500	of intelligence in terms of the substrate that's what we're working on. And so, yeah, I think,
6110060	6114060	yeah, like, I think it's coming. I don't think it's going to be cataclysmic, but I think that
6114620	6120860	people should start preparing and start integrating their business processes,
6120860	6126860	integrating their personal life and their personal workflow with AI, right? There's going to be the
6126860	6133660	class of sort of the tech forward people that embrace AI and sort of do well, right? They
6133660	6139580	integrate with it and those that refuse to use it that don't do so well, right? But the important
6139580	6146140	thing is that if people get to own a piece of the system in which they're contributing,
6146860	6152380	at least they own a piece of the future as it grows. Whereas if they, there's only centralized
6152380	6157740	players from which you, you, you have to pay rent constantly. And maybe they'll give you a
6157740	6162540	sprinkle of UBI at the end. That seems pretty dystopian to me. So that's the future we're trying
6162540	6168060	to prevent. I don't think the disruption can be stopped at this point. It's, it's coming. And
6168380	6172140	the only question is like, who's going to own the future? I think it's a good place to put it.
6172140	6177580	Questions than that, actually. But I'm going to have to hop to, but I think you have needs to go.
6177580	6181980	Can I ask you one last, it literally is a 30 second thing, by the way. One of my employees,
6181980	6186300	he was digging up some of your old videos. He, he saw that you had a four hundred and five
6186300	6192220	pound bench PR. Is that actually true? Yeah. He was very impressed by that. Yeah. Yeah. I mean,
6192220	6198060	I was, I played, I played college ball, college football. I have a friend who's a doctor in the
6198060	6204460	NFL, played with a great friend. And yeah, I mean, I just kept lifting after football. And,
6205340	6209180	you know, to me, it's just been like, I was a mathematician and I was a power lifter. And to
6209180	6215900	me, it's just like engineering signals in order to have neural adaptation. And it's all one in
6215900	6221900	the same. And so, yeah, I mean, you know, the best Jesus character, it's all about mind and body.
6221980	6229260	And, you know, I, I do like to cultivate strength and, you know, push, push myself to the, to the
6229260	6234700	limits to some extent. And yeah, that is a, I'll just post the video if you want. I'll tweet it.
6235980	6238780	Well, he found it. I have no idea how he found it on Vimeo, but I'm impressed because, you know,
6238780	6243420	a lot of this transhumanism business is very agnostic and kind of denying the physical, but
6243420	6246780	you're actually embracing it. And yeah, that's also other conversation. Thanks for making time.
6246780	6250460	I'm going to have to hop off. I assume everything will upload when we all hop off. I'm not sure
6250860	6253660	Erica's number disappeared before. So I literally have no idea what's going to happen.
6253660	6257900	All right, to be continued, guys. And yeah, definitely to be continued.
6257900	6260620	And one of these days, I want to talk crypto and AI with you when you're over the whole
6260620	6263660	hump of fundraising and all that stuff. Cause I've been thinking about this whole thing for a
6263660	6270460	long time and I'm very, yeah, I'm impressed. Okay, cool. Awesome. All right. See you guys. Cheers.
