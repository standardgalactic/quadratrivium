1
00:00:00,000 --> 00:00:09,760
This week's Moment of Zen is a feed drop for one of Turpentine's biggest shows, The

2
00:00:09,760 --> 00:00:13,280
Cognitive Revolution, hosted by Nathan LeBenz.

3
00:00:13,280 --> 00:00:17,520
Biology and Nathan discussed the evolution, challenges, and role of AI in different realms

4
00:00:17,520 --> 00:00:21,200
like politics, environmentalism, medicine, and more.

5
00:00:21,200 --> 00:00:24,640
There's a lot of new content here about where biology thinks AI is going and interesting

6
00:00:24,640 --> 00:00:26,800
parallels with religion.

7
00:00:26,800 --> 00:00:30,680
Nathan brings up some AI safety concerns in the course of the discussion.

8
00:00:30,680 --> 00:00:34,920
If you like what you hear, check out our other Moment of Zen episodes with Boloji and check

9
00:00:34,920 --> 00:00:37,200
out The Cognitive Revolution.

10
00:00:37,200 --> 00:00:39,200
Please enjoy.

11
00:00:39,200 --> 00:00:42,720
Boloji Srinivasan, welcome to The Cognitive Revolution.

12
00:00:42,720 --> 00:00:43,720
All right.

13
00:00:43,720 --> 00:00:44,720
I feel welcome.

14
00:00:44,720 --> 00:00:49,020
Well, we've got a ton to talk about, you know, obviously you bring a lot of different

15
00:00:49,020 --> 00:00:53,920
perspectives to everything that you think about and work on.

16
00:00:53,920 --> 00:00:58,720
And today I want to just try to muster all those different perspectives onto this, you

17
00:00:58,720 --> 00:01:02,920
know, what I see is really the defining question of our time, which is like, what's up with

18
00:01:02,920 --> 00:01:06,400
AI and, you know, how's it going to turn out?

19
00:01:06,400 --> 00:01:10,200
I thought maybe for starters, I would love to just get your baseline kind of table setting

20
00:01:10,200 --> 00:01:17,920
on how much more AI progress do you expect us to see over the next few years, like how

21
00:01:17,920 --> 00:01:23,600
powerful our AI system is going to become in, again, kind of a relatively short timeline.

22
00:01:23,600 --> 00:01:27,040
And then maybe if you want to take, you know, a bigger stab at it, you could answer that

23
00:01:27,040 --> 00:01:30,240
same question for a longer timeline, like the rest of our lives or whatever.

24
00:01:30,240 --> 00:01:31,240
Sure.

25
00:01:31,240 --> 00:01:32,240
Let me give an abstract answer.

26
00:01:32,240 --> 00:01:34,360
Then let me give a technical answer.

27
00:01:34,360 --> 00:01:40,120
You know, if you look at evolution, we've seen something as complex as flight evolved

28
00:01:40,120 --> 00:01:44,520
independently in birds, bats, and bees.

29
00:01:44,520 --> 00:01:53,320
And even intelligence, we've seen fairly high intelligence in dolphins, in whales, in octopuses,

30
00:01:54,280 --> 00:01:56,760
you know, octopus in particular can do like tool manipulation.

31
00:01:56,760 --> 00:01:59,760
They've got things that are a lot like hands, you know, with tentacles.

32
00:01:59,760 --> 00:02:07,520
And so that indicates that it is plausible that you could have multiple pathways to intelligence,

33
00:02:07,520 --> 00:02:11,440
whether, you know, we have carbon based intelligence, or we could have silicon based intelligence

34
00:02:11,440 --> 00:02:14,440
that just has a totally different form or the fundamental thing is an electromagnetic

35
00:02:14,440 --> 00:02:19,080
wave and data storage as opposed to, you know, DNA and so on, right?

36
00:02:19,080 --> 00:02:23,160
So that's like a plausibility argument in terms of evolution is being so resourceful

37
00:02:23,160 --> 00:02:28,200
that it's invented really complicated things in different ways, okay?

38
00:02:28,200 --> 00:02:32,280
Then in terms of the technical point, I think as of like right now, I should probably date

39
00:02:32,280 --> 00:02:36,640
it as like December 11, 2023, because this field moves so fast, right?

40
00:02:36,640 --> 00:02:41,440
My view is, and maybe you'll have a different view, is that the breakthroughs that are really

41
00:02:41,440 --> 00:02:47,840
needed for something that's like true artificial intelligence that is human independent, right?

42
00:02:47,840 --> 00:02:51,880
Maybe the next step after the Turing test, I've got an article that, you know, we're

43
00:02:51,920 --> 00:02:56,520
writing called the Turing thresholds, which tries to generalize the Turing test to like

44
00:02:56,520 --> 00:02:58,920
the cartage of scale, you know, have you got energy thresholds?

45
00:02:58,920 --> 00:03:01,480
Like what are useful scales beyond that?

46
00:03:01,480 --> 00:03:08,040
And right now, I think that what we call AI is absolutely amazing for environments that

47
00:03:08,040 --> 00:03:11,560
are not time varying or rule varying.

48
00:03:11,560 --> 00:03:16,520
And what I mean by that is, so you kind of have, let's say two large schools of AI, and

49
00:03:16,520 --> 00:03:20,200
obviously there's overlap in terms of the personnel and so on, but there's like the

50
00:03:20,200 --> 00:03:24,000
deep mind school, which has gotten less press recently, but got more press, you know, a

51
00:03:24,000 --> 00:03:27,560
few years ago, and that is game playing, right?

52
00:03:27,560 --> 00:03:31,880
It is, you know, superhuman playing of go without go.

53
00:03:32,160 --> 00:03:36,360
It is, you know, all the video game stuff they've done where they learn at the pixel

54
00:03:36,360 --> 00:03:39,520
level and they don't, they just teach the very basic rules and it figures it out from

55
00:03:39,520 --> 00:03:43,400
there. And it's also, you know, the protein folding stuff and what have you, right?

56
00:03:44,240 --> 00:03:48,000
But in general, I think they're known for reinforcement learning and those kinds of

57
00:03:48,040 --> 00:03:50,360
approaches. I mean, they're good at a lot of things, but that's what I think he finds

58
00:03:50,360 --> 00:03:54,560
known for. Of course, they put out this new model recently, the Gemini model.

59
00:03:54,560 --> 00:03:57,440
So I'm not saying that they're not good at everything, but that's just kind of what

60
00:03:57,440 --> 00:03:58,840
they're maybe most known for.

61
00:03:58,840 --> 00:04:04,000
And then you have the open AI chat, GBT school of generative AI, and it includes stable

62
00:04:04,000 --> 00:04:08,520
diffusion and just as a pioneer, even if, you know, they're not, I don't know how much

63
00:04:08,520 --> 00:04:12,480
they're used right now, but basically, you know, you have the diffusion models for images

64
00:04:12,480 --> 00:04:16,560
and you have large language models and now you have the multimodals that integrate them.

65
00:04:16,680 --> 00:04:22,400
And so the difference, I think with these is the reinforcement learning approaches are

66
00:04:22,400 --> 00:04:27,680
based on an assumption of static rules, like the rules of chess, the rules that go, the

67
00:04:27,680 --> 00:04:30,840
rules of a video game are not changing with time, they're discoverable, they're like the

68
00:04:30,840 --> 00:04:37,760
laws of physics. And similarly, like the body of language where you're learning it, English

69
00:04:37,760 --> 00:04:42,920
is not rapidly time varying. That is to say, the rules of grammar that are implicit aren't

70
00:04:42,960 --> 00:04:47,040
changing, the meanings of words aren't changing very rapidly, you can argue they're changing

71
00:04:47,040 --> 00:04:52,000
over the span of decades or centuries, but not extremely rapidly, right? So therefore,

72
00:04:52,000 --> 00:04:57,040
when you generate a new result, training data from five years ago for English is actually

73
00:04:57,040 --> 00:05:01,840
still fairly valuable, and the same input roughly gives the same output. Now, of course,

74
00:05:01,840 --> 00:05:06,920
there are facts that change with time, like who is the the ruler of England, right, the

75
00:05:06,920 --> 00:05:09,920
Queen of England is passed away now, it's the King of England, right, which is facts

76
00:05:09,920 --> 00:05:13,920
that change with time. But I think more fundamentally is when there's rules that change with time,

77
00:05:13,920 --> 00:05:19,400
you know, you have, for example, changes in law and countries, right? But most interestingly,

78
00:05:19,400 --> 00:05:24,080
perhaps changes in markets, because the same input does not give the same output in a market.

79
00:05:24,080 --> 00:05:28,200
If you try that, then what will happen is there's adversarial behavior on the other side. And

80
00:05:28,200 --> 00:05:31,360
once people see it enough times, they'll see your strategy, and they're going to trade

81
00:05:31,360 --> 00:05:35,560
against you on that, right? And I can get to other technical examples on that, but I think,

82
00:05:35,560 --> 00:05:39,320
and probably people in the space are aware of this, but I think that is a true frontier

83
00:05:39,360 --> 00:05:45,240
is dealing with time varying rule varying systems, as opposed to systems where the implicit rules

84
00:05:45,240 --> 00:05:49,600
are static. Let me pause there. Yeah, I think that makes sense. I think the, you know, in the

85
00:05:49,600 --> 00:05:56,600
very practical, you know, just trying to get as V calls it mundane utility from AI, that is often

86
00:05:56,600 --> 00:06:01,640
kind of cashed out to AI is good at tasks, but it's not good at whole jobs. You know, it can

87
00:06:01,640 --> 00:06:06,240
handle these kind of small things where you can define, you know, what good looks like and tell

88
00:06:06,280 --> 00:06:13,040
it exactly what to do. But in the sort of broader context of, you know, handling things that come

89
00:06:13,040 --> 00:06:20,320
up as they come up, it's definitely not there yet. And I agree that there's likely to be some

90
00:06:20,320 --> 00:06:26,480
synthesis, you know, which is kind of the subject of all the Q star rumors. Recently, I would say

91
00:06:26,480 --> 00:06:33,760
is kind of the, the prospect that there could be already, you know, within the labs, a beginning

92
00:06:33,800 --> 00:06:39,280
of a synthesis between the, I kind of think of it as like harder edged reinforcement learning

93
00:06:39,280 --> 00:06:45,360
systems, you know, that are like small, efficient and deadly versus the like language model systems

94
00:06:45,360 --> 00:06:51,360
that are like kind of slow and soft and, you know, but have a sense of our values, which is really

95
00:06:51,360 --> 00:06:56,160
a remarkable accomplishment that that they're able to have even, you know, an approximation of our

96
00:06:56,160 --> 00:07:04,960
values that seems like reasonably good. So yeah, I think I agree with that framing. But I guess I

97
00:07:04,960 --> 00:07:12,720
would, you know, still wonder like, how far do you think this goes in the near term? Because I have

98
00:07:12,720 --> 00:07:16,160
a lot of uncertainty about that. And I think the field has a lot of uncertainty. You hear people

99
00:07:16,160 --> 00:07:20,080
say, Well, you know, it's never going to get smarter than its training data, you know, it'll

100
00:07:20,080 --> 00:07:24,000
kind of level out where humans are. But we certainly don't see that in the reinforcement

101
00:07:24,000 --> 00:07:30,960
learning side, right? Like once it usually don't take too long at human level of these games,

102
00:07:30,960 --> 00:07:34,720
and then it like blows past human level. Interestingly, you do still see some adversarial

103
00:07:34,720 --> 00:07:40,560
vulnerability, like there's a great paper from the team at FAR AI, and I'm planning to have

104
00:07:41,280 --> 00:07:44,800
Adam Gleave, the head of that organization on soon to talk about that and other things,

105
00:07:44,800 --> 00:07:52,240
where they found like a basically a hack where a really simple, but unexpected attack on the

106
00:07:52,240 --> 00:07:57,600
superhuman go player can defeat it. So you do have these like very interesting vulnerabilities

107
00:07:57,600 --> 00:08:02,960
or kind of lack of adversarial robustness, still kind of wondering like, where do you think that

108
00:08:02,960 --> 00:08:07,920
leaves us in say a three to five years time? Obviously, huge uncertainty on that. It's really

109
00:08:07,920 --> 00:08:13,840
hard to predict something like this. Just to your point, generative AI is generic AI, right?

110
00:08:13,840 --> 00:08:18,880
It's like generically smart, but doesn't have specific intelligence or creativity or facts.

111
00:08:18,880 --> 00:08:22,720
And as you're saying, just like we have, you know, adversarial images

112
00:08:22,720 --> 00:08:28,800
back in full programs that are trained on a certain set of data, and they just give some

113
00:08:28,800 --> 00:08:33,120
weird, you know, pattern that looks like a giraffe, but the algorithm thinks it's a dog,

114
00:08:33,680 --> 00:08:37,360
you can do the same thing for game playing, and you can have out of sample input that can beat,

115
00:08:38,480 --> 00:08:42,000
you know, these very sophisticated reinforcement learners.

116
00:08:43,360 --> 00:08:48,240
And an interesting question is whether that is a fundamental thing, or whether it is a

117
00:08:49,760 --> 00:08:54,000
work aroundable thing. And you'd think it was work aroundable, you know,

118
00:08:54,880 --> 00:08:59,600
because there's probably some robustification because these pictures look like giraffes,

119
00:08:59,600 --> 00:09:05,120
you know, and yet they're being recognized as dogs. See, there's, you would think that

120
00:09:05,760 --> 00:09:11,360
the right proximity metric would group it with giraffes, you know, but maybe there's some,

121
00:09:12,000 --> 00:09:16,400
I don't know, maybe there's some result there. My intuition would be we can probably

122
00:09:17,280 --> 00:09:21,760
robustify these systems so that they are less vulnerable to adversarial input.

123
00:09:23,280 --> 00:09:26,320
But if we can't, then that leads us in a totally different direction,

124
00:09:26,320 --> 00:09:29,280
where these systems are fragile in a fundamental way.

125
00:09:30,160 --> 00:09:35,680
So that's one big branch point is how fragile these systems are, because if they're fragile in

126
00:09:35,680 --> 00:09:41,360
a certain way, then it's almost like you can always kill them, which is kind of good, right,

127
00:09:41,360 --> 00:09:47,280
in a sense, that there's that, you know, almost like the, you know, the 50 IQ, 100 IQ, 150 IQ

128
00:09:47,280 --> 00:09:53,280
thing, like the, the meme. Yeah, the meme, right. So the 50 IQ guys like these machines will never

129
00:09:53,280 --> 00:09:58,560
be as creative as humans or whatever. 100 IQ is look at all the things they can do. The 150 IQ

130
00:09:58,560 --> 00:10:03,680
is like, well, there's some like equivalent equivalent result, you know, that's like some

131
00:10:03,680 --> 00:10:08,000
impossibility proof that shows that we the dimensional space of a giraffe is too high,

132
00:10:08,000 --> 00:10:11,600
and we can't actually learn what a true giraffe, I don't think that's true.

133
00:10:12,240 --> 00:10:16,400
But maybe it's true from the perspective of how these learners are working,

134
00:10:16,400 --> 00:10:20,080
because my understanding is people have been trying, and I mean, I'm not the cutting edge of

135
00:10:20,080 --> 00:10:24,080
this. So, you know, maybe something, but my understanding is we haven't yet been able to

136
00:10:24,080 --> 00:10:29,040
robustify these models against adversarial input. Am I wrong about that?

137
00:10:29,680 --> 00:10:30,720
Yeah, that's definitely right.

138
00:10:31,280 --> 00:10:34,080
We'll continue our interview in a moment after a word from our sponsors.

139
00:10:34,400 --> 00:11:02,480
There's no single architecture, as far as I know, that is demonstrably robust. And on the contrary,

140
00:11:02,480 --> 00:11:06,720
you know, even with language models, there's a, we did a whole episode on the universal jailbreak,

141
00:11:07,280 --> 00:11:11,920
where, especially if you have access to the weights, not to change the weights, but just to

142
00:11:11,920 --> 00:11:17,600
kind of probe around in the weights, then you have a really hard time, you know, guaranteeing

143
00:11:17,600 --> 00:11:24,480
any sort of robustness. The conjecture is, see, for humans, you can't like, mirror their brain

144
00:11:24,480 --> 00:11:30,160
and analyze it. Okay, but we have enough humans that we've got things like optical illusions,

145
00:11:30,240 --> 00:11:35,680
stuff like that, that works on enough humans, and our brains aren't changing enough, right?

146
00:11:36,240 --> 00:11:42,720
A conjecture is, if you had, as you said, open weights, open weights mean safety, because if you

147
00:11:42,720 --> 00:11:47,920
have open weights, you can always reverse engineer adversarial input, and then you can always break

148
00:11:47,920 --> 00:11:54,080
the system. Conjecture. Yeah, there's I, that's again, with Adam from far AI, I'm really interested

149
00:11:54,080 --> 00:11:59,520
to get into that, because they are starting to study, as I understand it, kind of proto scaling

150
00:11:59,520 --> 00:12:06,640
laws for adversarial robustness. And I think a huge question there is, what are the kind of

151
00:12:07,680 --> 00:12:12,800
frontiers of possibility there? Like, do you need, you know, how do the orders of magnitude work,

152
00:12:12,800 --> 00:12:18,480
right? Do you need another 10x as much adversarial training to half the rate of your

153
00:12:19,520 --> 00:12:24,560
adversarial failures? And if so, you know, can we generate that many, it may always sort of

154
00:12:24,640 --> 00:12:31,360
be fleeting. So far AI, and they are, they're working on cutting edge of adversarial input.

155
00:12:31,920 --> 00:12:37,680
Yeah, they're the group that did the attack on the Alpha Go model, and found that like, you know,

156
00:12:37,680 --> 00:12:41,120
and what was really interesting about that, I mean, multiple things, right? First, that they could

157
00:12:41,120 --> 00:12:45,680
beat a super human Go player at all. But second, that the technique that they used would not work

158
00:12:45,680 --> 00:12:51,040
at all if playing a quality human. Or is, you know, it's a strategy that is trivial to beat if

159
00:12:51,040 --> 00:12:56,080
you're a quality human Go player. But the Alpha Go is just totally blind to it.

160
00:12:56,080 --> 00:12:59,440
You know, that's why I say the conjecture is, if you have the model,

161
00:13:00,800 --> 00:13:06,000
then you can generate the adversarial input. And then so if that is true, and that itself is

162
00:13:06,000 --> 00:13:13,360
an important conjecture about AI safety, right? Because if open weights are inherently something

163
00:13:13,360 --> 00:13:18,000
where you can generate adversarial input from that and break or crash or defeat the AI,

164
00:13:18,240 --> 00:13:25,440
then that AI is not omnipotent, right? You have some power words, you can speak to it, almost

165
00:13:25,440 --> 00:13:32,560
like magical words, that'll just make it, power down, so to speak, right? It's like those movies

166
00:13:32,560 --> 00:13:37,680
where the monsters can't see you if you stand really still, or if you, you don't make a noise or

167
00:13:37,680 --> 00:13:41,920
something like that, right? They're very powerful on Dimension X, but they're very weak on Dimension

168
00:13:41,920 --> 00:13:46,560
1. A kind of an obvious point, but you know, I'm not sure how important it's going to be in the

169
00:13:46,560 --> 00:13:52,240
future. Your next question was on like, you know, humanoid robots and so on. And before we get to

170
00:13:52,240 --> 00:13:58,960
that, maybe obviously, but all of these models are trained on things that we can easily record,

171
00:13:58,960 --> 00:14:08,880
which are sights and sounds, right? But touch and taste and smell, we don't have amazing datasets

172
00:14:08,880 --> 00:14:14,800
on those. Well, I mean, there's some haptic stuff, right? There's, there's probably some,

173
00:14:14,800 --> 00:14:19,920
you know, some work on taste and smell and so on. But those, there's five senses, right? I wonder if

174
00:14:19,920 --> 00:14:26,080
there's something like that where you might be like, okay, how are you going to out smell, you know,

175
00:14:26,080 --> 00:14:30,000
a robot or something like that? Well, dogs actually have a very powerful sense of smell,

176
00:14:30,000 --> 00:14:34,240
and that's being very important for them, you know? And it may turn out that there's,

177
00:14:34,240 --> 00:14:37,440
maybe it's just that we just haven't collected the data, and it could become a much better

178
00:14:37,440 --> 00:14:41,200
smeller or whatever, or, you know, taster than anything else. I wouldn't be surprised. It could

179
00:14:41,200 --> 00:14:46,800
be a much better wine taster because you can do molecular diagnostics. But it's just kind of,

180
00:14:46,800 --> 00:14:50,640
I just use that as an analogy to say there's areas of the human experience that we haven't

181
00:14:50,640 --> 00:14:55,760
yet quantified. And maybe it's just the opera term is yet, okay? But there's areas of the

182
00:14:55,760 --> 00:15:00,640
human experience we haven't yet quantified, which are also an area that AIs at least are not yet

183
00:15:00,640 --> 00:15:09,360
capable at. Yeah, I guess maybe my expectation boils down to, I think the really powerful systems

184
00:15:09,360 --> 00:15:16,000
are probably likely to mix architectures in some sort of ensemble, you know, when you think about

185
00:15:16,000 --> 00:15:20,960
just the structure of the brain, it's not, I mean, there certainly are aspects of it that are repeated,

186
00:15:20,960 --> 00:15:26,320
right? You look at the frontal cortex, and it's like, there is kind of this, you know, unit that

187
00:15:26,320 --> 00:15:30,800
gets repeated over and over again, in a sense, that's kind of analogous to say the transformer block

188
00:15:30,800 --> 00:15:35,520
that just gets, you know, stacked layer on layer. But it is striking in a transformer that it's

189
00:15:35,520 --> 00:15:40,960
basically the same exact mechanism at every layer that's doing kind of all the different kinds of

190
00:15:40,960 --> 00:15:46,080
processing. And so whatever weaknesses that structure has, and you know, with the transformer

191
00:15:46,080 --> 00:15:50,560
and the attention mechanism, there's like some pretty profound ones like finite context window,

192
00:15:51,120 --> 00:15:55,840
you know, you kind of need, I would think a different sort of architecture with a little bit

193
00:15:55,840 --> 00:16:03,040
of a different strength and weakness profile to complement that in such a way that, you know,

194
00:16:03,040 --> 00:16:07,600
kind of more similar to like a biological system where you kind of have this like dynamic feedback

195
00:16:07,600 --> 00:16:11,360
where, you know, if we have obviously, you know, thinking fast and slow and all sorts of different

196
00:16:11,360 --> 00:16:18,320
modules in the brain, and they kind of cross regulate each other and don't let any one system,

197
00:16:18,320 --> 00:16:23,200
you know, go totally, you know, down the wrong path on its own, right, without something kind of

198
00:16:23,200 --> 00:16:27,760
coming back and trying to override that. It seems to me like that's a big part of what is missing

199
00:16:27,760 --> 00:16:34,880
from the current crop of AIs in terms of their robustness. And I don't know how long that takes

200
00:16:34,880 --> 00:16:41,600
to show up. But we are starting to see some, you know, possibly, you know, I think people are maybe

201
00:16:41,600 --> 00:16:44,800
thinking about this a little bit the wrong way. They're just in the last couple of weeks, there's

202
00:16:44,800 --> 00:16:51,280
been a number of papers that are really looking at the state space model, kind of alternative,

203
00:16:51,280 --> 00:16:55,600
it's being framed as an alternative to the transformer. But when I see that, I'm much more

204
00:16:55,600 --> 00:17:01,040
like, it's probably a compliment to the transformer or, you know, these two things probably get

205
00:17:01,040 --> 00:17:04,960
integrated in some form, because to the degree that they do have very different strengths and

206
00:17:04,960 --> 00:17:09,440
weaknesses, ultimately, you're going to want the best of both in a robust system, certainly if you're

207
00:17:09,440 --> 00:17:13,120
trying to make an agent, certainly if you're trying to make, you know, a humanoid robot that can go

208
00:17:13,120 --> 00:17:18,640
around your house and like do useful work, but also be robust enough that it doesn't, you get tricked

209
00:17:18,640 --> 00:17:23,040
into attacking your kid or your dog or, you know, whatever, you're going to want to have more checks

210
00:17:23,040 --> 00:17:28,880
and balances than just kind of a single stack of, you know, the same block over and over again.

211
00:17:28,880 --> 00:17:34,640
Well, so I know Boston Dynamics with their legged robots is all control theory and it's not

212
00:17:34,640 --> 00:17:39,200
classical ML development. It's really interesting to see how they've accomplished it. And they do

213
00:17:39,200 --> 00:17:44,720
have essentially a state space model where they have a big position vector that's got all the

214
00:17:44,720 --> 00:17:48,880
coordinates of all the joints and then a bunch of matrix algebra to figure out how this thing is

215
00:17:48,880 --> 00:17:53,280
moving and all the feedback control and so on there. And it's more complicated than that, but

216
00:17:53,280 --> 00:17:58,080
that's, you know, I think the V1 of it. Sorry, it was there. I wasn't following this though.

217
00:17:58,080 --> 00:18:02,960
Are you saying that there's papers that are integrating that with the kind of gender to

218
00:18:02,960 --> 00:18:06,560
eye transformer model? You know, what's like, what's a good citation for me to look at?

219
00:18:06,560 --> 00:18:12,960
Yeah, starting to, we did an episode, for example, with one of the technology leads at Skydio,

220
00:18:12,960 --> 00:18:19,840
the, you know, the US is champion drone maker. And they have kind of a similar thing where

221
00:18:20,640 --> 00:18:28,880
they have built over, you know, a decade, right, a fully explicit multiple orders of,

222
00:18:28,880 --> 00:18:35,360
you know, spanning multiple orders of magnitude control stack. And now over the top of that,

223
00:18:35,360 --> 00:18:39,600
they're starting to layer this kind of, you know, it's not exactly generative AI in their case,

224
00:18:39,600 --> 00:18:44,480
because they're not like generating content, but it's kind of the high level, you know,

225
00:18:44,480 --> 00:18:50,080
can I give the thing verbal instructions, have it go out and kind of understand, okay, like,

226
00:18:50,080 --> 00:18:55,840
this is a bridge, I'm supposed to kind of, you know, survey the bridge and translate those high

227
00:18:55,840 --> 00:19:03,840
level instructions to a plan, and then use the lower level explicit code that is fully deterministic,

228
00:19:03,840 --> 00:19:08,480
and, you know, runs on control theory and all that kind of stuff to actually execute the plan at

229
00:19:08,480 --> 00:19:12,960
a low level. But also, you know, at times like surface errors up to the top and say like, hey,

230
00:19:12,960 --> 00:19:16,320
we've got a problem, you know, whatever, I'm not able to do it, you know, can you now,

231
00:19:17,200 --> 00:19:22,880
at the higher level, the semantic layer, adjust the plan. That stuff is starting to happen in

232
00:19:22,880 --> 00:19:27,200
multiple domains, I would say. Yeah. And so I think that makes sense is basically it's like,

233
00:19:27,200 --> 00:19:31,520
gender AI is almost the front end. And then you have almost like an assembly, like,

234
00:19:32,080 --> 00:19:38,080
you give instructions to Figma, and the objects there are their shapes and their images. And so

235
00:19:38,080 --> 00:19:44,000
there's not it's not text, you give instructions to a drone, and the objects are like GPS coordinates

236
00:19:44,000 --> 00:19:51,280
and paths and so on. And so you are generating structures that are in a different domain, or

237
00:19:51,280 --> 00:19:56,800
it's like in VR, you're generating 3d structures again, as opposed to text. And then that compute

238
00:19:56,800 --> 00:20:00,800
engine takes those three structures and does something with them in a much more rules based

239
00:20:00,800 --> 00:20:05,680
way. So you have like a statistical user friendly front end with a generative AI, and then you have

240
00:20:05,680 --> 00:20:11,760
a more deterministic, or usually totally deterministic, almost like assembly language

241
00:20:11,760 --> 00:20:14,960
backend that actually takes that and does that's what you're saying, right? Yeah, pretty much.

242
00:20:14,960 --> 00:20:19,120
And I would say there's another analogy to just, again, our biological experience where it's like,

243
00:20:19,680 --> 00:20:25,360
I'm, you know, sort of in a semi conscious level, right, I kind of think about what I want to do.

244
00:20:25,360 --> 00:20:30,640
But the low level movements of the hand, you know, are both like not conscious. And also,

245
00:20:30,640 --> 00:20:36,000
you know, if I do encounter some pain, or you know, hit some, you know, hot item or whatever,

246
00:20:36,000 --> 00:20:41,600
like, there's a quick reaction that's sort of mediated by a lower level control system.

247
00:20:41,600 --> 00:20:45,440
And then that fires back up to the brain and is like, Hey, you know, we need a new plan here.

248
00:20:45,440 --> 00:20:52,640
So that is only starting to come into focus, I think with, you know, because obviously these,

249
00:20:52,640 --> 00:20:58,400
I mean, it's amazing, as you said, it's all moving so fast. What is always striking to me,

250
00:20:58,400 --> 00:21:02,960
I just, and I kind of like recite timelines to myself almost as like a mantra, right? Like,

251
00:21:02,960 --> 00:21:08,480
the first instruction following AI that hit the public was just January 2022. That was

252
00:21:08,480 --> 00:21:13,280
OpenAI's Text of NGOO2 was the first one where you could say like, do X and it would do X,

253
00:21:13,280 --> 00:21:16,640
as opposed to having, you know, an elaborate prompt engineering type of setup.

254
00:21:17,440 --> 00:21:21,520
GPT-4, you know, just a little over a year ago, finished training, not even a year that it's

255
00:21:21,520 --> 00:21:28,400
been in the public. And, you know, it has been amazing to see how quickly this kind of technology

256
00:21:28,400 --> 00:21:32,080
is being integrated into those systems, but it's definitely still very much a work in progress.

257
00:21:32,640 --> 00:21:39,760
Yeah, I mean, the tricky part is, like the training data and so on, like a large existing

258
00:21:39,760 --> 00:21:47,360
scale company like a Figma or DJI that has millions or billions of user sessions will have

259
00:21:47,360 --> 00:21:54,240
a much easier time training and they have a unique data set. And then everybody else will

260
00:21:54,880 --> 00:21:59,680
not be able to do that. So there is actually almost like, I mean, a return on scale where

261
00:21:59,680 --> 00:22:03,680
the massive data set, if you've got a massive clean data set and a unique domain that lots of

262
00:22:03,680 --> 00:22:09,920
people are using, then you can crush it. And if you don't, I suppose, I mean, there's lots of

263
00:22:09,920 --> 00:22:14,000
people who work on zero shot stuff and sort of sort of, but it still strikes me that there'll

264
00:22:14,080 --> 00:22:20,400
probably be an advantage to see those sessions. I find it hard to believe that you could generate

265
00:22:20,400 --> 00:22:27,280
a really good drone command language without lots of drone flight paths, but you can see.

266
00:22:27,280 --> 00:22:31,440
And where it doesn't exist, people are, obviously, need deep pockets for this, but the likes of

267
00:22:31,440 --> 00:22:37,280
Google are starting to just grind out the generation of that, right? They've got their kind of

268
00:22:37,840 --> 00:22:43,440
test kitchen, which is a literal physical kitchen at Google, where the robots go around and do

269
00:22:43,440 --> 00:22:49,360
tasks. And when they get stuck, my understanding of their kind of critical path, as I understand,

270
00:22:49,360 --> 00:22:56,400
they understand it, is robots going to get stuck. We'll have a human operator remotely

271
00:22:57,360 --> 00:23:04,160
operate the robot to show what to do. And then that data becomes the bridge from what the robot

272
00:23:04,160 --> 00:23:09,600
can't do to what it's supposed to learn to do next time. And they're going to need a lot of that,

273
00:23:09,600 --> 00:23:14,640
for sure. But they increasingly have, I don't know exactly how many robots they have now, but

274
00:23:14,640 --> 00:23:20,480
last I talked to someone there, it was like, into the dozens. And presumably, they're continuing

275
00:23:20,480 --> 00:23:27,600
to scale that. I think they just view that they can probably brute force it to the point where

276
00:23:27,600 --> 00:23:32,080
it's good enough to put out into the world. And then very much like a Waymo or a cruise or whatever,

277
00:23:32,080 --> 00:23:36,160
they probably still have remote operators, even when the robot is in your home,

278
00:23:36,960 --> 00:23:39,280
you know, when it encounters something that it doesn't know what to do about,

279
00:23:40,000 --> 00:23:44,480
raise that alarm, get the human supervision to help it over the hump, and then, you know,

280
00:23:44,480 --> 00:23:47,600
obviously, that's where you really get the scale that you're talking about.

281
00:23:48,160 --> 00:23:53,200
This raises a couple of questions I wanted to ask that are conceptual. So, you know, obviously,

282
00:23:53,200 --> 00:23:59,120
there's huge questions around like, again, highest level, how is all this going to play out?

283
00:23:59,120 --> 00:24:04,960
One big debate is, to what degree does AI favor the incumbents? To what degree, you know,

284
00:24:04,960 --> 00:24:08,720
does it enable startups? Obviously, it's both. But, you know, if you're interested in your

285
00:24:08,720 --> 00:24:13,120
perspective on that, also really interested in your perspective on like, offense versus defense,

286
00:24:13,120 --> 00:24:17,440
that's something that a lot of people now and in the future, right, that seems like it probably

287
00:24:17,440 --> 00:24:22,320
really matters a lot, whether it's a more offense enabling or defense enabling technology. So,

288
00:24:22,320 --> 00:24:27,440
I love your take on those two dimensions. Hey, we'll continue our interview in a moment after

289
00:24:27,440 --> 00:24:32,480
a word from our sponsors. If you're a startup founder or executive running a growing business,

290
00:24:32,480 --> 00:24:36,400
you know that as you scale, your systems break down and the cracks start to show.

291
00:24:36,960 --> 00:24:42,160
If this resonates with you, there are three numbers you need to know. 36,000, 25, and 1.

292
00:24:42,720 --> 00:24:47,440
36,000. That's the number of businesses which have upgraded to NetSuite by Oracle. NetSuite is the

293
00:24:47,440 --> 00:24:51,600
number one cloud financial system, streamline accounting, financial management, inventory,

294
00:24:51,600 --> 00:24:57,760
HR, and more. 25. NetSuite turns 25 this year. That's 25 years of helping businesses do more

295
00:24:57,760 --> 00:25:03,360
with less, close their books in days, not weeks, and drive down costs. One, because your business

296
00:25:03,360 --> 00:25:08,080
is one of a kind, so you get a customized solution for all your KPIs in one efficient system with

297
00:25:08,080 --> 00:25:13,120
one source of truth. Manage risk, get reliable forecasts, and improve margins, everything you

298
00:25:13,120 --> 00:25:19,040
need all in one place. Right now, download NetSuite's popular KPI checklist designed to give you

299
00:25:19,040 --> 00:25:25,200
consistently excellent performance, absolutely free at netsuite.com slash zen. That's netsuite.com

300
00:25:26,080 --> 00:25:29,840
to get your own KPI checklist, netsuite.com slash zen.

301
00:25:30,800 --> 00:25:34,720
So, like offense or defense in the sense of disenabled disruptors or incumbents?

302
00:25:35,360 --> 00:25:40,480
Both in business and in potentially outright conflict. I'd be interested to hear your analysis

303
00:25:40,480 --> 00:25:46,480
on both. All right. A lot of views on this. Obviously, if you've got a competent existing

304
00:25:46,480 --> 00:25:52,560
tech CEO who's still in their prime, like Amjad of Replet, or

305
00:25:53,360 --> 00:26:00,640
you know, Dillon Field of Figma, or, you know, those are two who have thought of who are very

306
00:26:00,640 --> 00:26:06,320
good and, you know, will be on top of it. Amjad is very early on integrating AI into Replet,

307
00:26:06,320 --> 00:26:09,680
and it's basically built that into an AI-first company, which is really impressive.

308
00:26:10,400 --> 00:26:18,000
Those are folks who cleanly made a pivot. It's as big or bigger than, comparable to, I would say,

309
00:26:18,000 --> 00:26:24,000
the pivot from desktop to mobile that broke a bunch of companies in the late 2000s and early

310
00:26:24,000 --> 00:26:30,080
2010s. Like Facebook in 2012 had no mobile revenue, roughly, at the time of their IPO,

311
00:26:30,080 --> 00:26:35,200
and then they had to like redo the whole thing. And it's hard to turn a company 90 degrees when

312
00:26:35,200 --> 00:26:39,840
something new like that hits, you know? Those that are run by kind of tech CEOs in their prime

313
00:26:40,880 --> 00:26:46,160
will adapt and will AI-ify their existing services. And the question is, obviously,

314
00:26:46,160 --> 00:26:49,920
there's new things that are coming out, like pika and character.ai. There's some like really

315
00:26:49,920 --> 00:26:56,480
good stuff that's out there. The question is, you know, will the disruption be allowed to happen

316
00:26:56,480 --> 00:27:02,800
in the US regulatory environment? And so my view is actually that, you know, so this is from like

317
00:27:02,800 --> 00:27:07,520
the network state book, right? I talk about, you know, people talk about a multipolar world or

318
00:27:07,520 --> 00:27:12,880
unipolar world. The political axis is actually really important in my view for thinking about

319
00:27:12,880 --> 00:27:18,240
whether AI will be allowed to disrupt, okay? Because we'll get to this probably later,

320
00:27:18,240 --> 00:27:24,000
but the 640K of compute is enough for everyone executive order, you know, 640K of memory,

321
00:27:24,000 --> 00:27:28,080
the apocryphal, he didn't bill gates and actually say it, but that quote kind of

322
00:27:28,080 --> 00:27:32,560
gives a certain mindset about computing. That should be enough for everybody. So the 10 to the 26

323
00:27:32,560 --> 00:27:37,280
of compute should be enough for everyone bill. I actually think it's very bad. And I think it's

324
00:27:37,280 --> 00:27:43,680
just the beginning of their attempts to build like a software FDA, okay, to decelerate, control,

325
00:27:43,680 --> 00:27:49,440
regulate, red tape, the entire space, just like how, you know, the threat of nuclear terrorism

326
00:27:49,440 --> 00:27:56,400
got turned into the TSA. The threat of, you know, terminators and AGI gets turned into a million

327
00:27:56,400 --> 00:28:02,160
rules on whether you can set up servers and this last free sector of the economy is strangled or at

328
00:28:02,160 --> 00:28:09,360
least controlled within the territory controlled by Washington DC. Now, why does this relate

329
00:28:09,360 --> 00:28:14,880
to the political? Well, obviously this, you know, you can just spend your entire life just tracking

330
00:28:14,880 --> 00:28:19,840
AI papers and that's moving like at the speed of light like this, right? What's also happening

331
00:28:19,840 --> 00:28:24,720
as you can kind of see in your peripheral vision is there's political developments that are happening

332
00:28:24,720 --> 00:28:28,240
at the speed of light much faster than they've happened in our lifespans. Like there's more,

333
00:28:28,240 --> 00:28:32,880
you just notice more wars, more serious online conflicts like, you know, there's a sovereign

334
00:28:32,880 --> 00:28:37,920
debt crisis, all of those things that can show graph after graph of things looking like their own

335
00:28:37,920 --> 00:28:42,800
types of singularities, you know, like military debts are way up, you know, the long piece that

336
00:28:42,800 --> 00:28:46,800
Stephen Pinker showed it's looking like a you that suddenly way up after Ukraine and some of

337
00:28:46,800 --> 00:28:51,360
these other wars are happening, unfortunately, right? Interest payments rush way up to the side.

338
00:28:51,360 --> 00:28:57,840
What's my point? Point is, I think that the world is going to become from the Pax Americana world

339
00:28:58,240 --> 00:29:04,000
of just like basically one superpower, hyperpower that we grew up in from 91 to 2021 roughly,

340
00:29:04,640 --> 00:29:11,360
that we're going to get a specifically tripolar world, not unipolar, not bipolar, not multipolar,

341
00:29:11,360 --> 00:29:17,600
but tripolar. And those three poles, I kind of think of as NYT, CCP, BTC, or you could think of

342
00:29:17,600 --> 00:29:21,920
them as, and those are just certain labels that are associated with them, but they're roughly

343
00:29:22,640 --> 00:29:28,640
US tech, the US environment, China tech and China environment, and global tech and the global

344
00:29:28,640 --> 00:29:34,560
environment. And why do I identify BTC and crypto and so on with global tech? Because that's a tech

345
00:29:34,560 --> 00:29:40,240
that decentralized out of the US. And right now people think of crypto as finance, but it's also

346
00:29:40,240 --> 00:29:48,560
financiers. Okay, and in this next run up, it is, I think quite likely about depending on how you

347
00:29:48,560 --> 00:29:54,240
count, between a third to a half of the world's billionaires will be crypto. Okay, around, you

348
00:29:54,240 --> 00:29:58,080
know, I calculated this a while back around Bitcoin at a few hundred thousands, around a third to a

349
00:29:58,080 --> 00:30:03,120
half of the world's billionaires are crypto. That's the unlocked pool of capital. And those are the

350
00:30:03,120 --> 00:30:10,640
people who do not bow to DC or Beijing. And they might by the way be Indians or Israelis or every

351
00:30:10,640 --> 00:30:14,640
other demographic in the world, or they could be American libertarians, or they could be Chinese

352
00:30:14,640 --> 00:30:19,280
liberals like Jack Ma were pushed out of Beijing sphere. Okay, or the next Jack Ma, you know,

353
00:30:19,280 --> 00:30:25,440
Jack Ma himself may not be able to do too much. Okay, that group of people who are, let's say,

354
00:30:25,440 --> 00:30:30,720
the dissident technologists who are not going to just kneel to anything that comes out of

355
00:30:30,720 --> 00:30:37,520
Washington DC or Beijing, that is the that's decentralized AI. That's crypto. That's decentralized

356
00:30:37,520 --> 00:30:41,440
social media. So you can think of it as, you know, where we talked about in the recent pirate

357
00:30:41,440 --> 00:30:46,880
wires podcast, freedom to speak with decentralized censorship resistant social media, freedom to

358
00:30:46,880 --> 00:30:53,840
transact with cryptocurrency, freedom to compute with open source AI, and no compute limits. Okay,

359
00:30:54,480 --> 00:30:59,920
that's a freedom movement. And that's like the same spirit as a pirate bay, the same spirit

360
00:30:59,920 --> 00:31:05,920
as BitTorrent, the same spirit as Bitcoin, the same spirit as peer to peer and into an encryption.

361
00:31:05,920 --> 00:31:11,280
That's a very different spirit than having Kamala Harris regulate a superintelligence

362
00:31:11,280 --> 00:31:18,560
or signing it over to Xi Jinping thought. And the reason I say this is, I think that that group

363
00:31:18,560 --> 00:31:24,400
of people, of which I think Indians and Israelis will be a very prominent, maybe a plurality, right,

364
00:31:24,400 --> 00:31:28,160
just because the sheer quantity of Indians are like the third sort of big group that's kind of

365
00:31:28,160 --> 00:31:32,960
coming up. And they're relatively underpriced, you know, China is, I don't say it's price

366
00:31:32,960 --> 00:31:38,400
to perfection. But it's something that people when I say priced, I mean, people were dismissive

367
00:31:38,480 --> 00:31:44,400
of China even up until 2019. And then it was after 2020, if you look at people started to take China

368
00:31:44,400 --> 00:31:50,080
seriously. And I mean, that is the West Coast tech people knew that China actually had a plus tech

369
00:31:50,080 --> 00:31:53,760
companies and was a very strong competitor. But the East Coast still thought of them as a third

370
00:31:53,760 --> 00:31:59,200
world country until after COVID, when now, you know, the East Coast was sort of threatened by them

371
00:31:59,200 --> 00:32:04,800
politically. And it wasn't just blue collars, but blue America that was threatened by China.

372
00:32:05,440 --> 00:32:09,920
And so that's why the reaction to China went from Oh, who cares, just taking some manufacturing

373
00:32:09,920 --> 00:32:14,800
jobs to this is an empire that can contend with us for control of the world. That's why the hostility

374
00:32:14,800 --> 00:32:17,760
is ramped up in my view. There's a lot of other dimensions to it. But that's a big part of it.

375
00:32:18,960 --> 00:32:23,120
So India is also kind of there, but it's like the third. And India is not going to play for number

376
00:32:23,120 --> 00:32:28,960
one or number two. But India and Israel, if you look at like tech founders, depending on how you

377
00:32:28,960 --> 00:32:34,000
count, especially if you include diasporas, it's on the order of 30 to 50% of tech founders, right.

378
00:32:34,000 --> 00:32:38,320
And it's obviously some, you know, very good tech CEOs and, you know, Satya and Sundar and investors

379
00:32:38,320 --> 00:32:44,240
and whatnot. Those are folks Indians do not want about to DC or to Beijing, neither do Israelis

380
00:32:44,240 --> 00:32:48,720
for all kinds of reasons, even if Israel has to, you know, take some direction from the US now,

381
00:32:48,720 --> 00:32:53,360
they're bristling at it, right. And then a bunch of other countries don't. So the question is,

382
00:32:53,920 --> 00:32:59,600
who breaks away? And now we get to your point on the reason I had to say that is that that's

383
00:32:59,600 --> 00:33:06,160
preface, the political environment is a tripolar thing of US tech and US regulated Chinese tech

384
00:33:06,160 --> 00:33:11,680
and China regulated and global tech that's free. Okay, of course, there's, even though I identify

385
00:33:11,680 --> 00:33:17,040
those three polls, there's of course boundary regions. EAC is actually on the boundary of US

386
00:33:17,040 --> 00:33:21,760
tech and decentralized tech, you know, and I'm sure there'll be some Chinese thing that comes out

387
00:33:21,760 --> 00:33:26,080
that is also on the boundary there. For example, Binance is on the boundary of Chinese tech

388
00:33:26,080 --> 00:33:30,320
and global and decentralized tech, if that makes any sense, right? There's probably others Apple is

389
00:33:30,320 --> 00:33:33,920
actually on the boundary of US tech and Chinese tech, because they make all of their stuff in

390
00:33:33,920 --> 00:33:38,960
China, right? So these are not totally disjoint groups, but there's boundary areas, but you can

391
00:33:38,960 --> 00:33:44,640
think about why is this third group so important in my view? Both the Chinese group and the

392
00:33:44,640 --> 00:33:50,320
decentralized group will be very strong competition for the American group for totally different reasons.

393
00:33:51,280 --> 00:33:56,720
China has things like WeChat, these super apps. I mean, obviously not likely, but like

394
00:33:57,440 --> 00:34:02,800
WeChat is a super app, but they also have, for example, their digital yuan, right? They have

395
00:34:02,800 --> 00:34:07,920
the largest cleanest data sets in the world that are constantly updated in real time that they can

396
00:34:07,920 --> 00:34:15,920
mandate their entire population opt into. And most of the Chinese language speaking people are under

397
00:34:15,920 --> 00:34:21,680
their ambit, right? So that doesn't include Taiwan, doesn't include Singapore, doesn't include,

398
00:34:22,480 --> 00:34:26,320
you know, some of the Chinese yaspera, but basically anything that's happening in Chinese

399
00:34:26,320 --> 00:34:32,400
for 99% of it, 95, whatever the ratio is, they can see it and they can coerce it and they can control

400
00:34:32,400 --> 00:34:39,840
it. So they can tell all of their people, okay, here's five bucks in, you know, digital yuan,

401
00:34:39,840 --> 00:34:45,360
do this micro task, okay? All of these digital blue collar jobs, both China and India, I think,

402
00:34:45,360 --> 00:34:48,720
can do quite a lot with that and they'll come back to it. So they can make their people do

403
00:34:48,720 --> 00:34:53,120
immense amounts of training data, clean up lots of data sets. Once it's clear that you have to

404
00:34:53,120 --> 00:34:58,000
build this and do this, they can just kind of execute on that. And they can also deploy. I mean,

405
00:34:58,000 --> 00:35:02,720
in many ways, the US is still very strong in digital technology, but in the physical world,

406
00:35:02,720 --> 00:35:07,840
it's terrible because of all the regulations, because of the nimbyism and so on. It's not like

407
00:35:07,840 --> 00:35:12,720
that in China. So anything which kind of works in the US at a physical level, like the Boston

408
00:35:12,800 --> 00:35:16,720
Dynamics stuff, they're already cloning it in China and they can scale it out in the physical

409
00:35:16,720 --> 00:35:21,680
world. You already have drones, little sidewalk drone things that come to your hotel room and

410
00:35:21,680 --> 00:35:26,160
drop things off. That's already like very common in China. In many ways, it's already ahead if you

411
00:35:26,160 --> 00:35:31,040
go to the Chinese cities. So the Chinese version of AI is ultra centralized, more centralized,

412
00:35:31,040 --> 00:35:36,000
more monitoring, less privacy and so on than the American version. And therefore they will have

413
00:35:36,000 --> 00:35:41,040
potentially better data sets, at least for the Chinese population. And so we chat AI, I don't

414
00:35:41,040 --> 00:35:44,880
even know what it's going to be, but it'll be probably really good. It'll also be really dangerous

415
00:35:44,880 --> 00:35:51,520
in other words. Then the decentralized sphere has power for a different reason, because the

416
00:35:51,520 --> 00:36:00,080
decentralized sphere can train on full Hollywood movies. It can train on all books, all mp3s and

417
00:36:00,080 --> 00:36:06,560
just say, screw all this copyright stuff, like what Psyhub and Libgen are doing, because all the

418
00:36:06,560 --> 00:36:14,080
copyright, first of all, it's like Disney lobbying politicians to put another 60 or 70 or 90, I

419
00:36:14,080 --> 00:36:17,920
don't even know what it is, some crazy amount on copyright, so you can keep milking this stuff and

420
00:36:17,920 --> 00:36:21,440
it doesn't go into public domain number one. And second, you know how Hollywood was built in the

421
00:36:21,440 --> 00:36:26,320
first place? It was all patent copyright and IP violation. Essentially Edison had all the patents,

422
00:36:26,320 --> 00:36:32,480
he's in New Jersey-ish, okay, that East Coast area. And Neil Gabler has this great book called

423
00:36:32,480 --> 00:36:38,080
An Empire of Their Own, where he talks about how immigrant populations, you know, the Jewish

424
00:36:38,080 --> 00:36:43,280
community in particular, and also others, went to Southern California in part, so they could just

425
00:36:43,280 --> 00:36:47,120
make movies that Edison coming and suing them for all the patents and so on and so forth.

426
00:36:47,120 --> 00:36:50,880
And they made enough money that they could fight those battles in court, and that's how they built

427
00:36:50,880 --> 00:36:56,960
Hollywood, okay? So, you know, one of my big theses is history is running in reverse, and I can get

428
00:36:56,960 --> 00:37:01,520
to why, but it's like 1950s and mirror moment, you go more decentralized backwards and forwards

429
00:37:01,520 --> 00:37:07,360
in time is like, you have these huge centralized states like the US and USSR and China, you know,

430
00:37:07,360 --> 00:37:12,480
all these things exist, and their fist relaxes as you go forwards and backwards in time. For example,

431
00:37:12,480 --> 00:37:17,680
backwards in time, the Western frontier closed, and forwards in time, the Eastern frontier opens.

432
00:37:17,680 --> 00:37:21,040
Backwards in time, you have the robber barons, forwards in time, you have the tech billionaires.

433
00:37:21,040 --> 00:37:24,720
Backwards in time, you have Spanish flu, forwards in time, you have COVID-19. And I've got dozens

434
00:37:24,720 --> 00:37:30,160
of examples of this in the book. The point is that if you go backwards in time, the ability to

435
00:37:30,160 --> 00:37:34,720
enforce patents and copyrights and so on starts dropping off, right? You have much more of a

436
00:37:34,720 --> 00:37:40,320
Grand Theft Auto environment. And you go forwards in time, and that's happening again. So, India

437
00:37:40,320 --> 00:37:47,360
in particular, for many years, basically just didn't obey Western patent protections and all these

438
00:37:47,360 --> 00:37:52,480
stupid rules basically, you know, it's a combination of artificial scarcity on the patent side and

439
00:37:52,480 --> 00:37:56,960
artificial regulation on the FDI side. That's a big part of what jacks up drug costs, where these

440
00:37:56,960 --> 00:38:02,000
things cost, you know, only cents to manufacturing, they sell them for so much money. All the delays,

441
00:38:02,000 --> 00:38:06,000
of course, that are imposed on the process, the only way they can pay for the manufacturers to

442
00:38:06,000 --> 00:38:09,920
take it out of your hide. What India did is they just said, we're not going to obey any of that stuff.

443
00:38:10,560 --> 00:38:16,080
So, they have a whole massive generic drugs and biotech industry that arose because they built

444
00:38:16,080 --> 00:38:19,600
all the skills for that. That's why they could do their own vaccine during COVID. And they're one

445
00:38:19,600 --> 00:38:25,120
of the biggest biotech industries in the world because they said screw Western restrictive

446
00:38:25,200 --> 00:38:30,400
IPs and other stuff. So, I was actually talking with the founder of Flipkart, that's India's largest

447
00:38:30,400 --> 00:38:35,840
exit. And we were talking about this a few months ago. And what we want is for India and other

448
00:38:35,840 --> 00:38:41,040
countries like it, do something similar, not just generic drugs, but generic AI, meaning

449
00:38:42,080 --> 00:38:48,720
just let people train on Hollywood movies, let them train on full songs, let them train on every

450
00:38:48,720 --> 00:38:55,680
book, let them train on anything. And you know what, sue them in India, right? And have the servers

451
00:38:55,680 --> 00:39:01,520
in India and let people also train models in India, because that's something that can build up a

452
00:39:01,520 --> 00:39:06,320
domestic industry with skills that the rest of the world, you know, people will want the model

453
00:39:06,320 --> 00:39:10,000
output, they'll want to use the software service there, and they'll be fighting in court on the

454
00:39:10,000 --> 00:39:15,920
back. And this is similar to how all of the record companies fought Napster and Kazaa and so on,

455
00:39:15,920 --> 00:39:18,720
but they couldn't take down Spotify. Do you know that story? Do you remember that?

456
00:39:18,720 --> 00:39:24,160
Basically, because Spotify was legitimately, you know, a European company and that a combination of

457
00:39:24,160 --> 00:39:29,680
execution and, you know, negotiation, they couldn't take them down. They did take down Napster,

458
00:39:29,680 --> 00:39:34,640
they took down Limewire, they took down Groove Shark, and Kazaa had Estonians, I don't know

459
00:39:34,640 --> 00:39:37,840
exactly how it was incorporated, but it was probably two US proximal, and that's why they

460
00:39:37,840 --> 00:39:42,320
were able to get them. But Spotify was far enough away that they couldn't just sue them and they

461
00:39:42,320 --> 00:39:47,600
actually genuinely had European traction. That's why the RA had to negotiate. So being far away

462
00:39:48,400 --> 00:39:54,240
from San Francisco may also be an advantage in AI, because it means you're far away from the blue

463
00:39:54,240 --> 00:39:58,160
city in the blue state in the Union. This relates to another really important point.

464
00:39:59,040 --> 00:40:02,800
When you actually think about deploying AI, there's those jobs you can disrupt that are not

465
00:40:02,800 --> 00:40:09,520
regulated jobs, like, you know, obviously, programmers are not, thank God, you don't need

466
00:40:09,600 --> 00:40:14,080
a license to be a programmer, but programmers adopt this kind of stuff naturally, right? So

467
00:40:14,080 --> 00:40:18,880
get up, co-pilot, replete, we just boom, use it, and now it's amplified intelligence, okay?

468
00:40:19,680 --> 00:40:24,240
But a lot of other jobs, there's some that are unionized and then some that are licensed, right?

469
00:40:24,240 --> 00:40:28,240
So Hollywood screenwriters are complaining, right? Journalists are complaining,

470
00:40:28,240 --> 00:40:33,840
artists are complaining. This is a good chunk of blue America. If you add in licensed jobs,

471
00:40:33,840 --> 00:40:40,080
like lawyers and doctors and bureaucrats, right? You know, especially lawyers and doctors are very

472
00:40:40,080 --> 00:40:45,280
politically powerful, MDs and JDs. They have strong lobbying organizations, AMA and, you know,

473
00:40:45,280 --> 00:40:52,160
ABA and so on. Basically, AI is part of the economic apocalypse for blue America,

474
00:40:53,200 --> 00:40:59,920
okay? It just attacks these overpriced jobs. They say overpriced relative to

475
00:41:00,720 --> 00:41:04,720
what an Indian could do with an Android phone, what a South American could do with an Android phone,

476
00:41:04,720 --> 00:41:07,920
what someone in the Middle East or the Midwest could do with an Android phone.

477
00:41:08,640 --> 00:41:15,600
Now, those folks have, you know, been armed with generative AI. They can do way more.

478
00:41:16,240 --> 00:41:20,000
They're ready to work, they're ready to work for much less money, and they're a massive threat

479
00:41:20,800 --> 00:41:26,400
to blue America. Blue America is now feeling like the blue collars of 10 or 20 years ago, where

480
00:41:27,360 --> 00:41:31,520
the blue collars had their jobs, you know, going to China and other places, right? And they were mad

481
00:41:31,520 --> 00:41:35,680
about that. Factories got shut down and so on. That's about to happen to blue America, already

482
00:41:35,680 --> 00:41:42,480
happening, okay? And so that's going to mean a political backlash by blue America of protectionism,

483
00:41:42,480 --> 00:41:48,800
again, already happening. And the AI safety stuff, that's a whole separate thing, but it's going to

484
00:41:48,800 --> 00:41:53,600
be used. I'm going to use a phrase, and I hope you won't be offended by this. Have you heard

485
00:41:53,600 --> 00:41:59,200
the phrase, useful idiots, like by Lenin or whatever, okay? It basically means like, okay,

486
00:41:59,200 --> 00:42:04,240
those guys, you know, they're useful idiots for communism and so on. So there's, let me put it

487
00:42:04,240 --> 00:42:11,920
like naive people who think that the US government is interested in AI safety, are trying to give a

488
00:42:11,920 --> 00:42:15,760
lot of power to the US government. And the reason is they haven't actually thought through from

489
00:42:15,760 --> 00:42:18,800
first principles, what is the most powerful action in the world, a convective. They're trying to get

490
00:42:18,800 --> 00:42:22,640
power to the US government to regulate AI safety. But the government doesn't care about safety of

491
00:42:22,640 --> 00:42:29,920
anything. They literally funded the COVID virus in Wuhan, credibly alleged, right? There's at

492
00:42:29,920 --> 00:42:35,120
least it is a reasonable hypothesis based on a lot of the data. Matt Ridley wrote a whole book on

493
00:42:35,120 --> 00:42:39,040
this. There's a lot of data that indicates a lot of scientists believe it. I'm actually like a

494
00:42:39,040 --> 00:42:43,760
bioinformatics genomics guy. If you look at the sequences, there is a gap and a jump where it looks

495
00:42:43,760 --> 00:42:48,320
like this thing could have been engineered or partially engineered or evolved. There's Peter

496
00:42:48,400 --> 00:42:53,360
Dazak. There's Zeng Lishi. There's actually a lot of evidence here. So the US government and the

497
00:42:53,360 --> 00:42:58,400
Chinese government are responsible for an existential risk by studying it, they created it.

498
00:42:59,280 --> 00:43:04,560
They're responsible for risking nuclear war with Russia over this piece of land in eastern Ukraine,

499
00:43:04,560 --> 00:43:11,040
which probably is going to get wound down. So they don't care about your safety at all.

500
00:43:12,480 --> 00:43:16,160
These are immediate things where we can show and there's nobody who's punished for this,

501
00:43:16,160 --> 00:43:22,720
nobody who's fired for this, literally rolling the dice on millions, hundreds of millions of

502
00:43:22,720 --> 00:43:29,200
people's lives has not been punished. In fact, it's not even talked about. We're past the pandemic

503
00:43:29,200 --> 00:43:35,760
and these institutions can't be punished. So they don't care about AI safety. What they care about

504
00:43:35,760 --> 00:43:42,000
is AI control. And so the people in tech who are like, well, the government will guarantee AI

505
00:43:42,000 --> 00:43:46,720
safety. That's actually what we're going to actually get is something on the current path,

506
00:43:46,720 --> 00:43:52,080
like what happened with nuclear technology, where you got nuclear weapons, but not nuclear power,

507
00:43:52,080 --> 00:43:55,520
or at least not to the scale that we could have had it, right? We could have had much cheaper

508
00:43:55,520 --> 00:44:00,320
energy for everything. Instead, we got the militarization and the regulation and the

509
00:44:00,320 --> 00:44:06,400
deceleration worst of all worlds where you can blow people up, but you can't build nuclear power

510
00:44:06,400 --> 00:44:11,920
plants. And like even getting into nuclear technology, forget about nuclear power plants.

511
00:44:11,920 --> 00:44:15,520
We don't have nuclear submarines. We don't have nuclear planes, all that kind of stuff. I don't

512
00:44:15,520 --> 00:44:18,640
have nuclear planes are possible, but I do know nuclear submarines are possible. You can do a lot

513
00:44:18,640 --> 00:44:23,200
more cruise ships, a lot more stuff like that. You could probably have nuclear trains. You have

514
00:44:23,200 --> 00:44:27,600
to look at exactly how big those are. I don't know exactly how big those engines are and what

515
00:44:27,600 --> 00:44:31,760
the spies, but I wouldn't be surprised if you could. We don't have that. Why don't we have that?

516
00:44:31,760 --> 00:44:36,000
Because we had the wrong fear-driven regulation in the early 70s.

517
00:44:36,800 --> 00:44:43,520
Putting it all together, I think that the current AI safety stuff is similar to nuclear safety stuff

518
00:44:44,560 --> 00:44:48,960
that the US government has a terrible track record on safety in general. It doesn't care about it.

519
00:44:48,960 --> 00:44:55,280
It funded the COVID virus, incredibly alleged. It definitely risked nuclear war with Russia recently.

520
00:44:55,280 --> 00:44:59,280
Hot war with Russia was the red line we were not supposed to cross, and we're now like way

521
00:44:59,280 --> 00:45:04,720
into that. It doesn't care about AI safety, it doesn't care about your safety. It's also not

522
00:45:04,720 --> 00:45:10,480
even good at regulating. What it cares about is control. We are going to have potentially a bad

523
00:45:10,480 --> 00:45:17,440
outcome where Silicon Valley and San Francisco is the Xerox Park of AI. Maybe that's too strong,

524
00:45:17,440 --> 00:45:23,520
okay? Basically, it develops it, and there's a lot of things it can't do because it lobbied for

525
00:45:23,520 --> 00:45:28,160
this regulation that is going to come back and choke it. Then there are other two spheres

526
00:45:28,240 --> 00:45:33,040
we'll push ahead because it's not about the technology, it's also about the political layer.

527
00:45:33,040 --> 00:45:37,280
You know the Steve Jobs saying, actually Alan Kay by way of Steve Jobs, if you're

528
00:45:37,920 --> 00:45:42,800
really serious about software, you need your own hardware, right? So if you're really serious

529
00:45:42,800 --> 00:45:49,200
about technology, you need your own sovereignty. Because what the AI people haven't thought about

530
00:45:49,200 --> 00:45:56,800
is there's a platform beneath you, which is not just compute, it is regulate. It's a law, okay?

531
00:45:56,800 --> 00:46:00,480
And it's a law doesn't allow you to compute so much for all of your stuff above that.

532
00:46:01,280 --> 00:46:06,080
And I know you're saying, oh, it's only a 10 to 26 compute ban and so on and so forth.

533
00:46:06,080 --> 00:46:13,440
Have you seen the first IRS tax form? It's always, always super simple. It's only the super, super,

534
00:46:13,440 --> 00:46:17,760
super rich who's we're going to get in at first doesn't matter to you. So that's called

535
00:46:18,400 --> 00:46:22,400
boiling the frog slowly. There's a million, you know, slippery slope, slippery slope isn't a fallacy.

536
00:46:22,400 --> 00:46:27,120
It's literally how things work, right? You know, Apple, one of the reasons they, you know,

537
00:46:27,120 --> 00:46:32,080
they talk about not setting a precedent. Zuck starts off is a very hard line on setting precedents

538
00:46:32,080 --> 00:46:37,040
because he understands the long-term equivalent of setting a precedent, right? The precedent setting

539
00:46:37,040 --> 00:46:41,840
is that they're setting up a software FDA and they're going to and DC is so energized on this

540
00:46:41,840 --> 00:46:46,320
because they know how much social media disrupted them. That's why they're on the attack on crypto

541
00:46:46,320 --> 00:46:50,640
and AI. That's why they're on the attack on self-driving cars. They want to freeze the current

542
00:46:50,640 --> 00:46:56,160
social order and amber domestically and globally. So they think they can sanction China and stop it

543
00:46:56,160 --> 00:47:01,120
from developing chips. They think they can impose regulations on the US and stop it from developing

544
00:47:01,120 --> 00:47:07,120
AI, but they can't. And also, by the way, they're totally schizophrenic on this, where when they're

545
00:47:07,120 --> 00:47:11,360
talking about China, they're like, we're going to stop their chips to make sure America is a global

546
00:47:11,360 --> 00:47:16,240
leader. This is Gina Raimondo who's English. And then domestically, they're like, we're going to

547
00:47:16,240 --> 00:47:21,040
regulate you so you stop accelerating AI. We're not about AI acceleration. EAC is weird over there.

548
00:47:21,040 --> 00:47:25,840
Okay. So think about how schizophrenic that is. Okay, you're going to be far ahead of China.

549
00:47:25,840 --> 00:47:30,720
We're also going to be make sure to control the US. So they want to try and slow what they actually

550
00:47:30,720 --> 00:47:35,760
want is to freeze the current system and amber, try to go back to pre 2007 before all these tech

551
00:47:35,760 --> 00:47:40,720
guys disrupted everything. But that's not what's going to happen. So, but they're going to try to

552
00:47:40,720 --> 00:47:45,760
do it. And so everybody who's still loyal to the DC sphere, which includes an enormous chunk

553
00:47:45,760 --> 00:47:52,720
of AI people. And because they're all in a lot of them in San Francisco, right? And the political

554
00:47:52,720 --> 00:48:01,200
chaos of the last few years was not sufficient for them to relocate yet. Not all of them. I mean,

555
00:48:01,200 --> 00:48:06,320
Elon is in Texas. And that it may turn out that grok, for example, and what they're doing there,

556
00:48:06,320 --> 00:48:10,400
because he's a very legit, I mean, you know, he's Elon. So he's capable of doing a lot. He's very

557
00:48:10,400 --> 00:48:14,800
early on opening AI, he understands, you know, the right, it may turn out that grok

558
00:48:15,600 --> 00:48:21,200
becomes red AI, or the community around that, you know, an opening eye and deep mind or still blue

559
00:48:21,200 --> 00:48:24,320
AI. And we have Chinese and we're going to have decentralized AI. Okay, let me pause there. I

560
00:48:24,320 --> 00:48:30,560
know there's a big download. Well, I for starters, I would say, broadly, I have a pretty similar

561
00:48:32,080 --> 00:48:36,640
intellectual, you know, tendency as you, I would broadly describe myself as a techno

562
00:48:36,720 --> 00:48:45,040
optimist, libertarian, just about every issue. And I think your analysis of the dynamics is

563
00:48:45,040 --> 00:48:48,640
super interesting. And I think it, you know, a lot of it sounds pretty plausible, although I'll

564
00:48:48,640 --> 00:48:53,520
kind of float a couple of things that I think maybe bucking the trend. But I think it's maybe

565
00:48:53,520 --> 00:48:59,600
useful to kind of try to separate this into scenarios. Because all the analysis that you're

566
00:48:59,600 --> 00:49:07,120
describing here seem, if I understand it correctly, it seems to have the implicit assumption

567
00:49:07,840 --> 00:49:15,360
that the AI itself is not going to get super powerful or hard to control. It's like, if we

568
00:49:15,360 --> 00:49:21,040
assume that it's kind of a normal technology, then you're off to the races on this analysis. And

569
00:49:21,040 --> 00:49:26,080
then we can get into the fine points. But I do want to take at least one moment and say,

570
00:49:26,800 --> 00:49:32,800
how confident are you on that? Because if it's a totally different kind of technology from other

571
00:49:32,800 --> 00:49:39,120
technologies that we've seen, if it's more, you raise the gain of function research example,

572
00:49:39,120 --> 00:49:46,400
if it's that sort of technology that has these sort of non-local possible impacts or

573
00:49:47,600 --> 00:49:54,000
self-reinforcing kind of dynamics, which need not be like an Eliezer style snap of the fingers fume,

574
00:49:54,000 --> 00:50:00,400
but even over, say, a decade, let's imagine that over the next 10 years that AI's kind of

575
00:50:01,120 --> 00:50:04,480
multiple architectures develop and they sort of get integrated and we have something that

576
00:50:04,480 --> 00:50:09,920
kind of looks like robust, silicon-based intelligence, maybe not totally robust, but as

577
00:50:09,920 --> 00:50:16,720
robust or more robust than us and running faster and the kind of thing that can do lots of full

578
00:50:16,720 --> 00:50:23,440
jobs or maybe even be tech CEOs, then it kind of feels like a lot of this analysis probably

579
00:50:23,440 --> 00:50:30,800
doesn't hold, because we're just in a totally different regime that is just extremely hard

580
00:50:30,800 --> 00:50:36,640
to predict. And I guess I wonder, first of all, do you agree with that? There seems to be a big

581
00:50:36,640 --> 00:50:42,720
fork in the road there that's like, just how fast and how powerful do these AI's become super

582
00:50:42,720 --> 00:50:46,560
powerful or do they not? And if they don't, then yeah, I think we're much more into real

583
00:50:46,560 --> 00:50:51,200
politic type of analysis, but I'm not at all confident in that. To me, it feels like there's

584
00:50:51,200 --> 00:50:58,080
a very real chance that AI of 10 years from now is, and by the way, this is what the leaders

585
00:50:58,080 --> 00:51:04,160
are saying, right? I mean, open AI is saying this, Anthropic is saying this, Demis and Shane

586
00:51:04,160 --> 00:51:10,480
Legge are certainly saying things like this. It seems like they expect that we will have AI's

587
00:51:10,480 --> 00:51:16,880
that are more powerful than any individual human and that that becomes the bigger question

588
00:51:17,760 --> 00:51:24,880
than anything else. So do you agree with that kind of division of scenarios? First of all,

589
00:51:24,880 --> 00:51:28,640
and then maybe you could kind of say like how likely you think each one is. And obviously,

590
00:51:28,640 --> 00:51:33,120
that one where it takes off is like super hard to analyze. And I also definitely think it is

591
00:51:33,120 --> 00:51:37,200
worth analyzing this scenario where it doesn't take off. But I just wanted to flag that it seems

592
00:51:37,200 --> 00:51:43,200
like there's a, you know, there's a big, if you talk to the AI safety people, any world in which

593
00:51:43,280 --> 00:51:50,560
it's like, you know, we're suing Indian AI firms in Indian court over like IP is like a normal world

594
00:51:50,560 --> 00:51:53,360
in their mind, right? And that's not the kind of world that they're most worried about.

595
00:51:53,920 --> 00:51:59,520
I think that there have been some plausible sounding things that have been said. But I want

596
00:51:59,520 --> 00:52:05,360
to just kind of talk about a few technical counter arguments, mathematical or physical,

597
00:52:05,360 --> 00:52:11,120
that constrain what is possible. Okay. And actually, Martin Casado and Vijay and I are

598
00:52:11,200 --> 00:52:16,000
working on a long thing on this where, you know, Vijay did folding at home, he's a physicist,

599
00:52:16,000 --> 00:52:20,800
Martin, sold in the Syrah for, you know, a billion dollars and knows a lot about how a

600
00:52:20,800 --> 00:52:25,600
Stuxnet like thing could work at the systems level. And I've thought about it from other angles and,

601
00:52:25,600 --> 00:52:32,080
you know, and some of the math stuff that I'll get to. So for example, one thing, and I'm going to

602
00:52:32,080 --> 00:52:35,840
give a bunch of different technical arguments, and then let's kind of combine them. Okay.

603
00:52:36,640 --> 00:52:40,720
One thing that's being talked about is, if you have a super intelligence, it can

604
00:52:40,720 --> 00:52:45,840
double it right for a million years, and then it can make one move and it's going to outthink you

605
00:52:45,840 --> 00:52:50,640
all the time and so on and so forth. Okay. Well, if you're familiar with the math of chaos,

606
00:52:51,280 --> 00:52:57,440
or the math of turbulence, there are limits to even very simple systems that you can set up,

607
00:52:57,440 --> 00:53:04,480
where they can become very unpredictable quite quickly. Okay. And so you can, if you want to,

608
00:53:04,560 --> 00:53:11,360
engineer a system where you have very rapid diversions of predictability, so that, I don't know,

609
00:53:11,360 --> 00:53:15,200
it's like the heat depth of the universe before you can predict out in timestamps.

610
00:53:16,160 --> 00:53:21,280
Do you understand what I'm saying? Right? This is sort of akin to like a wolf from like simple,

611
00:53:21,280 --> 00:53:26,480
even simple rules can generate patterns such that you can't know them without literally computing

612
00:53:26,480 --> 00:53:32,320
them. Yeah, exactly. Right. So at least right now with chaos and turbulence, you can get things

613
00:53:32,320 --> 00:53:40,240
that are extremely provably difficult to forecast without actually doing it. Okay. You know, I can

614
00:53:40,240 --> 00:53:43,840
make that argument quantitative, but that's just something to look at, right? It's almost like a

615
00:53:43,840 --> 00:53:48,160
delta epsilon challenge from calculus, like, okay, how hard do you want me to make this to predict?

616
00:53:48,160 --> 00:53:52,560
Okay, I can set up a problem that is, that is like that, right? It's basically extreme sensitivity

617
00:53:52,560 --> 00:53:58,240
to initial conditions lead to extreme divergence in outcomes. So you could design systems to be

618
00:53:58,240 --> 00:54:02,960
chaotic, that might be AI immune, because they can't be forecasted that well, you have to kind of

619
00:54:02,960 --> 00:54:06,880
react to them in real time. The ultimate version of this is not even a chaotic system, it's a

620
00:54:06,880 --> 00:54:12,960
cryptographic system, where I've got a whole slide deck on this, how AI makes everything fake,

621
00:54:12,960 --> 00:54:19,920
easy to fake, crypto makes it hard to fake again. Right? Because crypto in the broader sense of

622
00:54:19,920 --> 00:54:25,680
cryptography, but also in the narrow sense, I think crypto is to cryptography as the internet

623
00:54:25,680 --> 00:54:30,160
is to computer science. It's like the primary place where all this stuff is applied, but obviously

624
00:54:30,160 --> 00:54:34,800
it's not the equivalent. Okay. And AI can fake an image, but it can't fake a digital signature,

625
00:54:34,800 --> 00:54:40,080
unless it can break certain math, you know, and so sort of like a, you know, solve factors,

626
00:54:40,080 --> 00:54:43,440
each problem or something like that. So cryptography is another mathematical thing that

627
00:54:43,440 --> 00:54:50,960
constrains AI, similar to chaos and turbulence, it constrains how much an AI can infer things.

628
00:54:50,960 --> 00:54:55,760
You can't statistically infer it. Okay, you need to actually have the private key to solve that

629
00:54:55,760 --> 00:55:01,680
equation. So that is another math. So I'm going to rules of math, right? Math is very powerful

630
00:55:01,680 --> 00:55:06,560
because you can make proofs that will work no matter what devices we come up with. Okay,

631
00:55:06,560 --> 00:55:10,640
you start to put an AI in a cage, it can't predict beyond a certain amount because of chaos and

632
00:55:10,640 --> 00:55:16,960
turbulence math, it cannot solve certain equations unless it has a private key is because of what

633
00:55:16,960 --> 00:55:22,240
we know about cryptography math. Okay, again, if somebody proves P equals NP, some of this stuff

634
00:55:22,240 --> 00:55:26,560
breaks down, but this is when the bounds of our mathematical knowledge right now, physics wise,

635
00:55:27,440 --> 00:55:37,360
physical friction exists, a lot of physical friction exists. And a huge amount of the writing on AI

636
00:55:37,360 --> 00:55:41,760
assumes by guys like a laser who I like, I don't, I don't dislike it, you know, but

637
00:55:42,000 --> 00:55:48,160
it is extremely, it's there's two things that really stick out to me about it. First is extremely

638
00:55:48,160 --> 00:55:53,200
theoretical and not empirical. And second, extremely Abrahamic rather than Dharmic or

639
00:55:53,200 --> 00:56:01,040
signing. Okay, white theoretical and not empirical. It's not trivial to turn something from the

640
00:56:01,040 --> 00:56:08,080
computer into a real world thing. Okay, one of the biggest gaps in all of this thinking is what

641
00:56:08,800 --> 00:56:14,720
are the sensors and actuators? Okay, because like if you actually build, you know, I've built

642
00:56:15,520 --> 00:56:19,840
industrial robot systems that you know, 10 years ago, I, you know, a genome sequencing lab with

643
00:56:19,840 --> 00:56:28,480
robots, that's hard. That's physical friction. Okay, and a lot of the AI scenarios seem to basically

644
00:56:28,480 --> 00:56:34,560
say, Oh, it's going to be a self programming Stuxnet that's going to escape and live off the land

645
00:56:34,560 --> 00:56:40,400
and hypnotize people into doing things. Okay, now, each of those is actually really,

646
00:56:40,400 --> 00:56:46,720
really difficult steps. First is self programming Stuxnet, like, this would have to be a computer

647
00:56:46,720 --> 00:56:53,360
virus that can live on any device, despite the fact that Apple or Google can push a software

648
00:56:53,360 --> 00:57:00,000
update to a billion devices, right, a few executives coordinating almost certainly can I mean, the

649
00:57:00,000 --> 00:57:06,720
off switch exists, right? Like, this is actually like the core thing, lots of AI safety guys get

650
00:57:06,720 --> 00:57:11,520
themselves into the mindset that the off switch doesn't exist. But guess what, there's almost

651
00:57:11,520 --> 00:57:17,520
nothing living that we haven't been able to kill. Right, like, can we kill it? This thing exists.

652
00:57:17,520 --> 00:57:22,080
And this is getting back to living off land. A even if you had like something that could solve

653
00:57:22,080 --> 00:57:27,600
some other technical problems that I'll get to it exists as an electromagnetic wave kind of thing

654
00:57:27,600 --> 00:57:32,720
on on a certain, you know, on chips and so on and so forth. It's taking it out in the environment

655
00:57:32,720 --> 00:57:37,600
is like putting a really smart human into outer space. Right, your body just explodes and you die.

656
00:57:38,160 --> 00:57:43,040
Doesn't matter how smart you are, that that strength on this axis, but you're weak on this

657
00:57:43,040 --> 00:57:48,160
axis. And, you know, so strength on the x axis, not strength on the y or the z axis in AI outside,

658
00:57:48,160 --> 00:57:54,000
you know, pour water on it. You know, this is why I mean the 50 IQ, 150 IQ thing, you know,

659
00:57:54,000 --> 00:57:58,320
the 150 IQ way of saying it is it's strong on this x and weak on this x and the 50 IQ way is

660
00:57:58,320 --> 00:58:05,200
pour water on it, disconnect it, you know, turn the power off. Okay, right. Like, it'll, it'll be

661
00:58:05,200 --> 00:58:10,080
very difficult to build a system where you literally cannot turn it off. The closest thing we have

662
00:58:10,080 --> 00:58:16,880
to that is actually not stuck snap. It's Bitcoin. And Bitcoin only exists because millions of humans

663
00:58:17,520 --> 00:58:22,960
keep it going. So you, you need, so that gets the second point living off the land

664
00:58:23,600 --> 00:58:29,760
for an AI to live off land, meaning without human cooperation. Okay, that's the next

665
00:58:29,760 --> 00:58:35,520
Turing threshold in AI to live without human cooperation. It would need to be able to control

666
00:58:35,520 --> 00:58:43,360
robots sufficient to dig or out of the ground, set up data centers and generators and connect them

667
00:58:43,360 --> 00:58:48,640
and defend that against human attack, literally a terminator scenario. Okay, that's a big leap

668
00:58:48,640 --> 00:58:52,320
in terms. I mean, is it completely impossible? I can't say it's completely impossible,

669
00:58:52,320 --> 00:58:57,120
but it's not happening tomorrow. No matter what your AI timelines are, you would need to have

670
00:58:57,120 --> 00:59:04,320
like a billion or hundreds of millions of internet connected autonomous robots that this

671
00:59:04,320 --> 00:59:11,040
Stuxnet AI could hijack that were sufficient to carve or out of the earth and, you know, set up

672
00:59:11,040 --> 00:59:16,640
data centers and make the AI duplicate. We're not there. That's a huge amount of physical friction.

673
00:59:16,640 --> 00:59:20,880
That's AI operating without a human to make itself propagate, right? A human doesn't need

674
00:59:20,880 --> 00:59:27,040
the cooperation of a lizard to self replicate. For an AI to replicate right now, it would need

675
00:59:27,040 --> 00:59:32,880
the cooperation of a human in some sense, because otherwise those humans can kill it because there's

676
00:59:32,880 --> 00:59:36,960
not that many different pieces of, you know, operating systems around the world. I'm just

677
00:59:36,960 --> 00:59:40,480
talking about the practical constraints of our current world, right? You know, actually existing

678
00:59:40,480 --> 00:59:45,600
reality, not AI safety guys, you know, you know, reality where all these things don't exist. There's

679
00:59:45,600 --> 00:59:50,160
just a few operating systems, just a few countries. If everybody is going with torches and search

680
00:59:50,160 --> 00:59:57,040
lights through the internet, it's very hard for a virus to continue. Okay. So A, on the practical

681
00:59:57,040 --> 01:00:02,320
difficulties that there's the technical stuff with, you know, with the chaos and turbulence and

682
01:00:02,320 --> 01:00:07,680
with cryptography itself or AI can't predict and it can't solve certain equations. B, on the physical

683
01:00:07,680 --> 01:00:13,840
difficulties, it probably, I mean, like to be a Stuxnet, Microsoft and Google and so on could kill

684
01:00:13,840 --> 01:00:18,480
it. The off switch exists. Can it live off the land? No, it cannot because it doesn't have, you

685
01:00:18,480 --> 01:00:27,440
know, drones to mine or and stuff out of the ground. And can it like exist without humans? Can it be

686
01:00:27,440 --> 01:00:31,760
this hypnotizing thing? Okay. So the hypnotizing thing, by the way, this is one of the things that's

687
01:00:31,840 --> 01:00:37,840
the most hilarious self fulfilling prophecy in my view. Okay. And no offense anybody listening to

688
01:00:37,840 --> 01:00:44,000
this podcast, but I think the absolutely dumbest kind of tweet that I've seen on AI is, I typed this

689
01:00:44,000 --> 01:00:50,560
in and oh my God, it told me this. Like, I asked it how to make sarin gas and it told me X or whatever.

690
01:00:50,560 --> 01:00:57,040
Right. That's just a search engine. Okay. What, what basically a lot of these people are doing is

691
01:00:57,040 --> 01:01:02,800
they're saying, what if there were people out there that were so impressionable that they would

692
01:01:02,800 --> 01:01:08,160
type things into an AI and, and follow it as if they were hearing voices. And that's actually

693
01:01:08,160 --> 01:01:13,600
not the, the, the model or whatever that's doing it. That's like this AI cult that has evolved

694
01:01:13,600 --> 01:01:17,840
around the world, like a Aum Shinrikyo, you know, that, that hears voices and does like the sarin

695
01:01:17,840 --> 01:01:24,480
gas. The point is an AI can't just like hypnotize people. Those people have to like participate

696
01:01:24,480 --> 01:01:29,360
in it. They're typing things into the machine or whatever. Okay. Now you might say, all right,

697
01:01:29,360 --> 01:01:34,400
let's project out a few years. In a few years, what you have is, you have an AI that is not

698
01:01:34,400 --> 01:01:40,320
just text, but it appears as Jesus. What would, what would AI Jesus do? What would AI Lee Kwan

699
01:01:40,320 --> 01:01:45,200
you do? What would AI George Washington do? So it appears as 3d. Okay. So it's generating that.

700
01:01:45,760 --> 01:01:52,960
It speaks in your language and in a voice. It knows the history of your whole culture. Okay.

701
01:01:52,960 --> 01:01:57,600
That would be very convincing. Absolutely be very convincing. But it still can't exist without

702
01:01:57,600 --> 01:02:03,120
human programmers who are like the priests tending this AI God, whether it's AI Jesus or AI Lee

703
01:02:03,120 --> 01:02:06,800
Kwan you or something like that. The thing about the hypnotization thing that I really want to

704
01:02:06,800 --> 01:02:11,280
poke on that, are you familiar with the concept of the principal Asian problem? Basically in every,

705
01:02:11,280 --> 01:02:18,960
every time you've got like a CEO and a, and a, a worker, or you have a LP and a VC, or you have,

706
01:02:19,920 --> 01:02:25,680
you know, an employer and a contractor, every edge there, there are four possibilities in a

707
01:02:25,680 --> 01:02:34,560
two by two matrix. Win, win, win, lose, lose, win, lose, lose. Okay. And so for example, win, win is,

708
01:02:34,560 --> 01:02:38,720
you know, when, when somebody joins a tech startup, the, the CEO makes a lot of money and so does a

709
01:02:38,720 --> 01:02:44,560
worker. Okay. That's win, win, lose, lose is they both lose money. Win, lose is the CEO makes money

710
01:02:44,560 --> 01:02:50,000
and the employee doesn't lose. Win is the company fails, but the employee got paid a very high salary.

711
01:02:50,000 --> 01:02:54,640
So what equity does is it aligns people. That's where the top console alignment comes from.

712
01:02:54,640 --> 01:02:59,040
It aligns people to the upper left corner of win, win. That's when you have one, one CEO and one

713
01:02:59,040 --> 01:03:05,200
employee. When you have one CEO and two employees, you don't have two squared outcomes. You have two

714
01:03:05,200 --> 01:03:10,560
cubed outcomes because you have win, win, win, win, win, lose, win, lose, lose, etc. Right.

715
01:03:10,640 --> 01:03:15,120
Because all three people can be win or lose. Because CEO can be winner, lose. Employee can

716
01:03:15,120 --> 01:03:19,280
be winner, lose. Employee number two can be winner, lose. If you have N people, rather than three

717
01:03:19,280 --> 01:03:23,040
people, you have two to the N possible outcomes and you have essentially a two by two by two by two

718
01:03:23,040 --> 01:03:27,440
by two by N hypercube of possibilities. Okay. It's all literally just two dimensions on each axis.

719
01:03:28,400 --> 01:03:32,800
There's tons of possible defecting kinds of things that happen there. So that's why in a large company,

720
01:03:32,800 --> 01:03:37,920
there's lose, win coalitions that happen where M people gang up on the other K people and they win

721
01:03:37,920 --> 01:03:41,520
what the other people lose. That's how politics happens. When you've got a startup that's driven

722
01:03:41,520 --> 01:03:45,840
by equity and the biggest payoff, people don't have to try to think, okay, well, I make more money

723
01:03:45,840 --> 01:03:49,440
by politics, we'll make more money by the win, win, win, win, win column because the exit makes

724
01:03:49,440 --> 01:03:53,440
everybody make the most money. That's actually how the opening AI people were able to coordinate

725
01:03:53,440 --> 01:03:57,920
around. We want an $80 billion company. The economics helped find the sell that was actually

726
01:03:57,920 --> 01:04:01,280
the most beneficial to all of them helped them coordinate. Okay. So you search that hypercube.

727
01:04:01,280 --> 01:04:08,560
Okay. That's a point of equity is lining. Still, despite all of this, that that's one of our best

728
01:04:08,560 --> 01:04:13,360
mechanisms for coordinating large numbers of people in the principal agent problem. Despite all

729
01:04:13,360 --> 01:04:20,400
of this, the possibility exists for any of these people to win while the others lose, right with

730
01:04:20,400 --> 01:04:25,520
me so far. And I'll explain why this is important. What that means is those 1000 employees of the

731
01:04:25,520 --> 01:04:31,760
CEO are their own agents with their own payoff functions that are not perfectly aligned with

732
01:04:31,760 --> 01:04:36,640
the CEO's payoff function. As such, there are scenarios under which they will defect and do

733
01:04:36,640 --> 01:04:46,640
other things. Okay. The only way they become like actual limbs, see my hand is not an agent of its

734
01:04:46,640 --> 01:04:52,160
own. It lives or dies with me. Therefore it does exactly what I'm saying at this time. I tell it

735
01:04:52,160 --> 01:04:57,040
to go up, it goes up, tell it to go down, it goes down, sideways, sideways, right. An employee is

736
01:04:57,040 --> 01:05:02,400
not like that. They will do this and this and sideways, sideways up to a certain point. And if

737
01:05:02,400 --> 01:05:06,400
you, if you have them do something that's extremely against their interests, they will not do your

738
01:05:06,400 --> 01:05:11,520
action. Do you understand my point? Okay. That is the difference between an AI hypnotizing humans

739
01:05:12,160 --> 01:05:17,200
versus an AI controlling drones. AI controlling drones is like your hands. They're actually

740
01:05:17,200 --> 01:05:21,680
pieces of your body. There's no defecting. There's no loose wind. They have no mind of their own.

741
01:05:21,680 --> 01:05:24,880
They're literally taking instructions. Okay. They have no payoff function. They will

742
01:05:24,880 --> 01:05:30,320
kill themselves for the hoard. Right. An AI hypnotizing humans has a thousand principal

743
01:05:30,320 --> 01:05:35,600
Asian problems for every thousand humans. And it has to incentivize them to continue and

744
01:05:35,600 --> 01:05:40,720
has to generate huge payoffs. It's like an AI CEO. That's really hard to do. Right. The history

745
01:05:40,720 --> 01:05:45,440
of evolution shows us how hard it is to coordinate multicellular organisms. You have to make them

746
01:05:45,440 --> 01:05:50,320
all live or die as one. Then you get something along these lines. Like an ant colony can coordinate

747
01:05:50,320 --> 01:05:54,400
like that because if the queen doesn't reproduce all the ants, it doesn't matter what they're

748
01:05:54,400 --> 01:05:59,920
having sort of genetic material. Okay. We are not currently set up for those humans to not be able

749
01:05:59,920 --> 01:06:05,440
to reproduce unless the AI reproduces. Do I think we eventually get to a configuration like that?

750
01:06:05,440 --> 01:06:11,840
Maybe. Where you have an AI brain is at the center of civilization and it's coordinating all the

751
01:06:11,840 --> 01:06:17,280
people around it. And every civilization that makes it is capable of crowdfunding and operating

752
01:06:17,280 --> 01:06:22,160
its own AI. That gets me to my other critique of the AI safety guys. I mentioned that the first

753
01:06:22,160 --> 01:06:26,480
critique is very theoretical rather than empirical. The second critique is their Abrahamic rather than

754
01:06:27,120 --> 01:06:32,800
Dharmic or Sinic. Okay. And you know, our background culture influences things in ways we don't even

755
01:06:32,800 --> 01:06:38,560
think about. So much of the paperclip thinking is like a vengeful God will turn you into pillars

756
01:06:38,560 --> 01:06:43,440
of salt, except it's a vengeful, you know, AI God will turn you into paperclips. Okay.

757
01:06:44,080 --> 01:06:49,440
The polytheistic model of many gods as opposed to one God is we're all going to have our own AI

758
01:06:49,440 --> 01:06:54,720
gods and there'll be war of the gods like Zeus and Hera and so on. That's the closest western

759
01:06:54,720 --> 01:06:58,800
version, you know, the paganism that predated, you know, Abrahamic religions, but that's still

760
01:06:58,800 --> 01:07:03,200
there in India. That's still how Indians think. That's why India is sort of people got so woke

761
01:07:03,200 --> 01:07:07,440
that they don't even make large scale cultural generalizations anymore. But it's true that India

762
01:07:07,440 --> 01:07:14,640
is just culturally more amenable to decentralization to, you know, multiple gods rather than one

763
01:07:14,640 --> 01:07:20,240
God in one state. Okay. And then the Chinese model is yet the opposite, like they have like,

764
01:07:20,240 --> 01:07:23,840
I mean, of course they have their tech entrepreneurs and so on, but they're, if India is more

765
01:07:23,840 --> 01:07:27,520
decentralized, China is more centralized, they have like one government and one leader for the

766
01:07:27,520 --> 01:07:33,360
entire civilization. Okay. And, and that the biggest thing that China has done over the last 20 or

767
01:07:33,360 --> 01:07:38,640
30 years is they've taken various, you know, U.S. things and they've made sure that they have their

768
01:07:38,640 --> 01:07:43,040
own Chinese version where they have root. So they take U.S. social media and they made sure they had

769
01:07:43,040 --> 01:07:47,920
root over Sina Weibo. Okay. They make sure they have their own Chinese version of electric cars,

770
01:07:47,920 --> 01:07:53,280
the most Chinese version. So the private keys in the sense are with G. So that means that they also

771
01:07:53,920 --> 01:07:58,960
at a minimum, you combine these two things, you're at a minimum going to get polytheistic AI

772
01:07:58,960 --> 01:08:04,160
of the U.S. and Chinese varieties. And then you add the Indian version on it and you're going to get

773
01:08:04,160 --> 01:08:08,640
quite a few of these different AIs around there. And then you have War of the Gods where maybe

774
01:08:08,640 --> 01:08:14,480
they are good at coordinating humans who, who, you know, take instructions from them,

775
01:08:14,480 --> 01:08:19,360
but they can't live without the humans. And humans are giving input to them. That's a series of

776
01:08:19,360 --> 01:08:23,360
things I could probably make that clearer if I just laid it out in bullets in an essay, but just

777
01:08:23,360 --> 01:08:30,080
to recap it, A, technical reasons like chaos, turbulence, cryptography, why AI is limited

778
01:08:30,080 --> 01:08:36,320
in its ability to predict timeframes and to solve equations, B, practical limits. And AI cannot

779
01:08:36,320 --> 01:08:42,960
easily be a Stuxnet because Microsoft and Google and Apple can install software on a billion devices

780
01:08:42,960 --> 01:08:48,320
and just kill it, right? Like basically guys with torches come. All right. It can't easily live off

781
01:08:48,320 --> 01:08:52,560
the land without humans because they would need hundreds of millions of autonomous robots out there

782
01:08:52,560 --> 01:08:58,000
to control, to mine the ore and, and set the data centers. It can't just hypnotize humans

783
01:08:58,000 --> 01:09:01,920
like it can control drones because of the principal agent problem and the degree of human

784
01:09:01,920 --> 01:09:07,440
defection. To make those humans do that, you'd have to have such massive alignment between the AI

785
01:09:07,440 --> 01:09:11,680
and humans that the humans all know they'll die if the AI dies and vice versa. We're not there.

786
01:09:11,680 --> 01:09:15,280
Maybe we'll be there in like, I don't know, n number of years, but not for a while. That's a

787
01:09:15,280 --> 01:09:21,280
total change in like how states are organized. Okay. Finally, let me just talk about the physics

788
01:09:21,280 --> 01:09:27,120
a little bit more. There's a lot of stuff which is talked about at a very sci-fi book level of

789
01:09:27,120 --> 01:09:31,920
it'll just invent nanomedicine and nanotech and kill us all and so on and so forth. Now look,

790
01:09:31,920 --> 01:09:35,600
I like Robert Freitas, obviously Richard Feynman's a genius and so on and so forth,

791
01:09:36,160 --> 01:09:42,320
but nanotech somehow hasn't been invented yet. Okay. Meaning that, you know, there's a lot of

792
01:09:42,320 --> 01:09:48,800
chemists that have worked in this area. Okay. And a lot of what nanotech is like rebranded chemistry

793
01:09:48,800 --> 01:09:54,720
because those are the molecular machines, you know, for example, DNA polymerase or ribosome,

794
01:09:54,720 --> 01:09:58,400
those are molecular machines that we can get to work at that scale, the evolved ones.

795
01:09:59,120 --> 01:10:02,640
To my knowledge, and I may be wrong about this, I haven't looked at it very, very recently,

796
01:10:03,280 --> 01:10:07,840
we haven't actually been able to make artificial, you know, replicators of the stuff that they're

797
01:10:07,840 --> 01:10:12,960
talking about, which means it's possible that there's some practical difficulty that intervened

798
01:10:12,960 --> 01:10:17,920
between Feynman and Freitas and so on's calculations, right? Just a sheer fact that those

799
01:10:17,920 --> 01:10:22,320
books have came out decades ago and no progress has been made indicates that maybe there's a road

800
01:10:22,320 --> 01:10:26,640
block that wasn't contemplated, right? So you can't just click your fingers and say, boom,

801
01:10:26,640 --> 01:10:30,880
nanomus, and it's sort of like clicking your fingers and saying, boom, time travel, right?

802
01:10:30,880 --> 01:10:35,360
Nanomus and exists. That was a good poke that I had a while ago in a conversation like this,

803
01:10:35,360 --> 01:10:39,520
where the AI guy, AI safety guy on their side was like, well, time travel, that's too implausible.

804
01:10:39,520 --> 01:10:44,400
I'm like, yeah, but you're waiting on the nanotech thing you're thinking is like here,

805
01:10:44,400 --> 01:10:49,280
and you're making so many assumptions there that I want to actually see some more work there. I

806
01:10:49,280 --> 01:10:54,000
want to actually see that nanotech is actually more possible than you think it is. As for, oh,

807
01:10:54,000 --> 01:10:58,560
we just need to mix things in a beaker and make a, you know, virus and so on. You know what is

808
01:10:58,560 --> 01:11:03,920
really, really good at defending against novel viruses, like the human immune, that's something

809
01:11:03,920 --> 01:11:10,240
that's within envelope, right? Like you have evolved to not die and to fight off viruses. Is it possible

810
01:11:10,240 --> 01:11:16,320
that maybe you could make some super virus? I mean, maybe, but again, like humans are really good

811
01:11:16,320 --> 01:11:20,240
and the immune system is really good at that kind of thing. That is what we're set up to do,

812
01:11:20,240 --> 01:11:25,040
right? To adapt to that billions of years of evolution being set up. Physical constraints

813
01:11:25,840 --> 01:11:30,800
are not really contemplated when people talk about these super powerful mathematical constraints,

814
01:11:30,800 --> 01:11:34,560
practical constraints are not contemplated. And I could give more, but I think that was a lot

815
01:11:34,640 --> 01:11:39,760
right there. Let me pause here. Yeah. Let me try to steal man a few things. And then

816
01:11:41,040 --> 01:11:43,920
I do think, you know, it's before too long, I want to kind of get back to the

817
01:11:45,600 --> 01:11:49,520
somewhat less, you know, radically transformative scenarios and ask a few follow up questions on

818
01:11:49,520 --> 01:11:54,080
that too. But I think for starters, I would say the, the sort of Eleazar, you know, he's updated

819
01:11:54,080 --> 01:11:57,680
his thinking over time as well. And I would say probably doesn't get quite enough credit for it

820
01:11:57,680 --> 01:12:02,640
because he's definitely on record, you know, repeatedly saying, yeah, I was kind of expecting

821
01:12:02,720 --> 01:12:08,400
more something from like the deep mind school to pop out and be, you know, wildly overpowered

822
01:12:08,960 --> 01:12:14,800
very quickly. And on the contrary, it seems like we're in more of a slow takeoff type of scenario

823
01:12:14,800 --> 01:12:19,680
where, you know, we've got these, again, like super high surface area kind of suck up all the

824
01:12:19,680 --> 01:12:23,920
knowledge, gradually get better at everything. Some surprises in there, you know, certainly some

825
01:12:23,920 --> 01:12:29,200
emergent properties, if you will accept that term, you know, surprise surprises to the developers

826
01:12:29,200 --> 01:12:34,880
of nothing else, right, that are definitely things we don't fully understand. But it does seem to be a,

827
01:12:34,880 --> 01:12:40,880
you know, more gradual turning up of capability versus some like, you know, super sudden surprise.

828
01:12:41,920 --> 01:12:48,320
But okay, so then what is the alternative? I'm going to try to kind of give you the what I,

829
01:12:48,320 --> 01:12:57,040
what I think of as the most consensus strongest scenario where humans lose track of the future

830
01:12:57,680 --> 01:13:02,080
and or lose control of the future, maybe starting by kind of losing track of the president and then

831
01:13:02,080 --> 01:13:06,080
having that kind of, you know, give way to losing control of the future. And I think within that,

832
01:13:06,080 --> 01:13:11,920
by the way, the, I'm not really one who cares that much about like, whether AIs say something

833
01:13:11,920 --> 01:13:17,440
offensive today, I'm not easily offended and like, whatever. That's not, that's not world ending. I

834
01:13:17,440 --> 01:13:21,520
understand your point. That's not like, who cares, whatever, that's within scope, that's within envelope.

835
01:13:21,520 --> 01:13:25,920
Within within this bigger kind of, you know, what is the real, you know, most likely

836
01:13:27,520 --> 01:13:33,120
path to like AI disaster, as understood, I think by the smartest people today, I think that is

837
01:13:33,120 --> 01:13:38,880
still a useful leading indicator, because it's like, okay, the developers, you know, whether you

838
01:13:38,880 --> 01:13:41,840
agree with their politics, whether you agree with their, whether you think their commercial

839
01:13:41,840 --> 01:13:47,920
reasons are their sincere reasons or not, they have made it a goal to get the AI to not say

840
01:13:47,920 --> 01:13:51,600
certain things, right? They don't want it to be offensive. The most naive, you know, kind of

841
01:13:51,600 --> 01:13:55,120
down the fairway interpretation of that is like, hey, they want to sell it to corporate customers.

842
01:13:55,120 --> 01:13:59,280
They know that their corporate customers don't want, you know, to have their AI saying offensive

843
01:13:59,280 --> 01:14:05,440
things. So they don't want to say offensive things. And yet they can't really control it. It's like

844
01:14:05,440 --> 01:14:11,920
still pretty easy to break. So I view that as just kind of a leading indicator of, okay, we've seen

845
01:14:11,920 --> 01:14:19,680
GPT two, three and four over the last four years. And that's, you know, a big delta in capability.

846
01:14:20,240 --> 01:14:26,560
How much control have we seen developed in that time? And does it seem to be keeping pace?

847
01:14:27,120 --> 01:14:32,640
And my answer would be on the face of it, it seems like the answer is no, you know, we, we don't

848
01:14:32,640 --> 01:14:39,360
have the ability to really dial in the behavior such that we can say, okay, you're going to,

849
01:14:39,360 --> 01:14:44,480
you know, you can expect, you can trust that these AIs will like not do, you know, aviancy.

850
01:14:45,200 --> 01:14:48,720
On the contrary, it's like, if you're a little clever, you know, you can get them to do it.

851
01:14:48,720 --> 01:14:50,240
You can break out of the sandbox on it.

852
01:14:50,800 --> 01:14:54,080
Yeah. And it's not even like, I mean, we've talked about, you know, things where you have

853
01:14:54,080 --> 01:14:57,600
access to the weights and you're doing like counter optimizations, but you don't even need that,

854
01:14:57,600 --> 01:15:02,080
you know, the kind of stuff I do in like my red teaming in public is literally just like

855
01:15:02,800 --> 01:15:07,200
feed the AI a couple of words, put a couple of words in its mouth, you know, and it will kind

856
01:15:07,200 --> 01:15:11,920
of carry on from there. So with that in mind is just a leading indicator. You know, I don't know

857
01:15:11,920 --> 01:15:16,240
how powerful the most powerful AI systems get over the next few years, but it seems very plausible

858
01:15:16,240 --> 01:15:21,920
to me that it might be as powerful as like an Elon Musk type figure, you know, somebody who's

859
01:15:21,920 --> 01:15:27,680
like really good at thinking from first principles, really smart, you know, really dynamic across a

860
01:15:27,680 --> 01:15:34,320
wide range of different contexts. And, you know, he's not powerful enough to like in and of himself

861
01:15:34,320 --> 01:15:40,160
take over the world, but he is kind of becoming transformative. Now imagine that you have that

862
01:15:40,160 --> 01:15:46,480
kind of system, and it's trivial to replicate it. So, you know, if you have like one Elon Musk,

863
01:15:46,480 --> 01:15:51,520
all of a sudden you can have arbitrary, you know, functionally arbitrary numbers of Elon Musk power

864
01:15:51,520 --> 01:15:56,880
things that are clones of each other. Maybe I can pause you there. So that's my polytheistic AI

865
01:15:56,880 --> 01:16:02,160
scenario. But here's the thing that is this is background, but I want to push it to foreground.

866
01:16:03,120 --> 01:16:08,080
You still have a human typing in things into that thing. The human is doing the jailbreak, right?

867
01:16:08,080 --> 01:16:12,880
What we're talking about is not artificial intelligence in the sense of something separate

868
01:16:12,880 --> 01:16:18,960
from a human, but amplified intelligence. Amplified intelligence, I very much believe in. The reason

869
01:16:18,960 --> 01:16:24,320
is amplified intelligence. So here's something that people may not know about humans. There's this

870
01:16:24,480 --> 01:16:32,800
great book, Cooking Made Us Human. Okay, tool use has shifted your biology in the following way.

871
01:16:32,800 --> 01:16:37,920
For example, I know I'll map it to the present day. This book by Richard Rang and Cooking Made Us

872
01:16:37,920 --> 01:16:44,560
Human, where the fact that we started cooking and using fire meant that we could do metabolism

873
01:16:44,560 --> 01:16:52,240
outside the body, which meant it freed up energy for more brain development. Okay,

874
01:16:52,240 --> 01:16:56,080
similarly, developing clothes meant that we didn't have to evolve as much

875
01:16:56,080 --> 01:17:01,360
fur, again, more energy for brain development. Evolving tools meant we didn't have as much

876
01:17:01,360 --> 01:17:06,720
fangs and claws and muscles, again, more energy for brain development, right? So encephalization

877
01:17:06,720 --> 01:17:12,960
quotient rose as tool use meant that we didn't have to do as much natively and we could push

878
01:17:12,960 --> 01:17:18,960
more to the machines. In a very real sense, we have been a man machine symbiosis since the

879
01:17:18,960 --> 01:17:26,160
invention of fire and the stone axe and clothes, right? You do not exist as a human being on your

880
01:17:26,160 --> 01:17:31,840
own like the entire Ted Kaczynski concept of like living in nature by itself. Humans are social

881
01:17:31,840 --> 01:17:38,400
organisms that are adapted to working with other humans and using tools and you have for and we

882
01:17:38,400 --> 01:17:43,520
have been for millennia. Okay, this goes back, not just human history, but like hundreds of

883
01:17:43,520 --> 01:17:49,760
thousands years before 100 gatherers are using tools. Okay, so what that means is man machine

884
01:17:49,760 --> 01:17:55,920
symbiosis is not some new thing. It's actually the old thing that broke us away from other

885
01:17:55,920 --> 01:18:01,120
primate lineages that weren't using tools. Okay, this is the fundamental difference between what I

886
01:18:01,120 --> 01:18:07,280
call Uncle Ted and Uncle Fred. Uncle Ted is Ted Kaczynski. It's a unabomber. It's a doomer. It's

887
01:18:07,280 --> 01:18:12,400
a decelerator, the de grother who thinks we need to go back to Gaia and Eden and become monkeys

888
01:18:12,400 --> 01:18:18,960
and live in the jungle like, like, you know, Ted Kaczynski, right? The unabomber cell. Uncle Fred

889
01:18:18,960 --> 01:18:25,280
is Friedrich Nietzsche, right? Nietzsche and we must get the stars and become ubermen and so

890
01:18:25,280 --> 01:18:29,920
on and so forth. This I think is going to become, and I actually tweeted about this years ago before

891
01:18:29,920 --> 01:18:36,400
current AI debates, that, you know, between anarcho primitivism, de growth, deceleration,

892
01:18:36,400 --> 01:18:44,640
okay, on the one hand, and transhumanism and acceleration and human 2.0 and human self-improvement

893
01:18:44,640 --> 01:18:49,680
and make it just the stars, on the other hand, this is the future political axis, the current one.

894
01:18:50,320 --> 01:18:55,760
And roughly speaking, you can cut, it's not really left and right because you'll have both

895
01:18:55,760 --> 01:19:00,800
left status and right conservatives go over here. You know, left states will say it's against

896
01:19:00,800 --> 01:19:04,480
the state and the right states will say, the right conservatives say it's against God,

897
01:19:04,480 --> 01:19:08,400
okay, and you'll have left libertarians and right libertarians over here,

898
01:19:08,400 --> 01:19:12,640
where left libertarians say it's my body and, you know, the right libertarians say it's my,

899
01:19:12,640 --> 01:19:18,080
you know, my money, right? And so that is a re-architecting of the political axis where,

900
01:19:18,080 --> 01:19:21,360
you know, Uncle Ted and Uncle Fred, which is kind of a clever way of putting it, okay?

901
01:19:22,640 --> 01:19:27,840
And the problem with the Uncle Ted guys in my view is, as I said, yeah, if they go and want to live

902
01:19:27,840 --> 01:19:33,200
in the, you know, the woods, fine, go get them. But once you start having even like a thousand,

903
01:19:33,200 --> 01:19:38,320
forget a thousand, a hundred people doing that, your trees will very quickly get exfoliated,

904
01:19:38,320 --> 01:19:42,800
you know, the leaves are going to get all picked off of them. Humans are not set up to just literally

905
01:19:42,800 --> 01:19:46,960
live in the jungle right now. You've had hundreds of thousands of years of evolution that have

906
01:19:46,960 --> 01:19:52,320
driven you in the direction of tool use, social organisms, farming, etc., etc. The man machine

907
01:19:52,320 --> 01:19:57,840
symbiosis is not today, it's yesterday and the day before and 10,000 years ago and 100,000 years ago.

908
01:19:58,400 --> 01:20:02,160
And how do we know we've got man machine symbiosis? Can you live without

909
01:20:03,440 --> 01:20:07,360
even if you're not living, even if you're not using the stove, somebody's using the stove

910
01:20:07,360 --> 01:20:12,960
to make you food, right? Can you live without the tractors that are digging up the grains? Can you

911
01:20:12,960 --> 01:20:18,400
live without indoor heating? Can you live without your clothes? Frankly, can you do your work without

912
01:20:18,400 --> 01:20:24,320
your phone, without your computer? No, you can't. You are already a man machine symbiosis. Once we

913
01:20:24,320 --> 01:20:29,840
accept that, then the question is, what's the next step? And right now, we're in the middle of that

914
01:20:29,840 --> 01:20:35,920
next step, which is AI is amplified intelligence. So what you're talking about is not that the AI

915
01:20:35,920 --> 01:20:43,200
is Elon Musk, it is that the AI human fusion means there's another 20 Elon Musk's or whatever the

916
01:20:43,200 --> 01:20:50,880
number is, okay? And that's good. That's fine. That's within envelope. That's just a bunch of smarter

917
01:20:50,880 --> 01:20:56,560
humans on the planet. That is amplified intelligence. That is more like, you know, I mentioned the tool

918
01:20:56,560 --> 01:21:00,800
thing. Okay, the other analogy would be like a dog. You know, a dog is man's best friend.

919
01:21:01,840 --> 01:21:08,960
Right? So that AI does not live without humans can turn it off. They have to power it. They have

920
01:21:08,960 --> 01:21:13,760
to give it substance, right? Eventually, that might become like a ceremonial thing, like this is our

921
01:21:13,760 --> 01:21:19,520
God that we pray to, right? Because it's wiser and smarter than us and it appears in an image.

922
01:21:19,520 --> 01:21:24,240
But the priests maintain it. You know, just like you go to a Hindu temple or something like that,

923
01:21:24,320 --> 01:21:28,000
and the priests will pour out the ghee, you know, for the fires and so on and so forth. And then

924
01:21:28,000 --> 01:21:32,960
everybody comes in and prays, okay? The priests believe in the whole thing, but they also maintain

925
01:21:32,960 --> 01:21:37,440
the back of the house. They do the system administration for the temple. Same, you know,

926
01:21:37,440 --> 01:21:43,360
in a Christian church, right? The, the, you know, like, it's not like it appears out of nowhere.

927
01:21:43,360 --> 01:21:50,240
Somebody, you know, went and assembled this cathedral, right? They saw the back of the house,

928
01:21:50,240 --> 01:21:54,000
the fact that it was just woods and rocks and so on that came together. Then when people come

929
01:21:54,000 --> 01:21:58,480
there, it feels like a spiritual experience. You see what I'm saying? Okay. So the equivalent of that,

930
01:21:58,480 --> 01:22:04,160
the priests or the, you know, the people maintaining temples, cathedrals, mosques, whatever, is

931
01:22:06,080 --> 01:22:11,760
engineers who are maintaining these future AIs, which appear to you as Jesus. They appear to you,

932
01:22:11,760 --> 01:22:16,160
maybe even a hologram. Okay, you come there, you ask it for guidance as an oracle. You've also got

933
01:22:16,160 --> 01:22:22,560
the personal version on your phone. You ask it for guidance. But guess what? You're still a human

934
01:22:23,120 --> 01:22:28,080
AI symbiosis until and unless the AI actually has the terminator scenario where it's got

935
01:22:28,080 --> 01:22:32,400
lots of robots that can live on its own. I'm not saying that's physically impossible. I did give

936
01:22:32,400 --> 01:22:37,200
some constraints on it earlier, but for a while we're not going to be there. So that alone means

937
01:22:37,200 --> 01:22:40,960
it's not fume because we don't have lots of drones running around. The AI has to be with

938
01:22:40,960 --> 01:22:47,120
the human. It's a human AI symbiosis. It's not AI Elon Musk. It is human AI fusion that becomes

939
01:22:47,120 --> 01:22:50,800
Elon Musk. And frankly, that's not that different from what Elon Musk himself is. Elon Musk would

940
01:22:50,800 --> 01:22:54,720
not be Elon Musk without the internet. Without the internet, you can't tweet and reach 150 million

941
01:22:54,720 --> 01:23:01,040
people. The internet itself made Elon what he is. And so this is the next version of that.

942
01:23:01,040 --> 01:23:04,400
Maybe there's now 30 Elon's because the AI makes the next 30 Elon's.

943
01:23:04,400 --> 01:23:11,120
Yeah, I mean, again, I think I'm largely with you with just this one very important nagging worry

944
01:23:11,120 --> 01:23:15,680
that's like, what if this time is different because what if these systems are getting

945
01:23:16,560 --> 01:23:23,520
so powerful so quickly that we don't really have time for that techno human fusion

946
01:23:23,520 --> 01:23:27,280
to really work out? And I'll just give you kind of a couple of data points on that.

947
01:23:28,640 --> 01:23:32,880
You said it's still somebody putting something into the AI. Well, sort of, right? I mean,

948
01:23:32,880 --> 01:23:38,320
already we have these proto agents and the super simple scaffolding of an agent is just

949
01:23:38,880 --> 01:23:45,840
run it in a loop, give it a goal, and have it kind of pursue some like plan, act, get feedback,

950
01:23:45,840 --> 01:23:52,000
and loop type of structure, right? It doesn't seem to take a lot. Now, they're not smart enough yet

951
01:23:52,000 --> 01:23:58,000
to accomplish big things in the world. But it seems like the language model to agent

952
01:23:58,880 --> 01:24:05,520
switch is less one right now that is gated by the structure or the architecture and more one

953
01:24:05,520 --> 01:24:10,160
that's just gated by the fact that like the language models when framed as agents just aren't

954
01:24:10,160 --> 01:24:14,480
that successful at like doing practical things and getting over hump. So they tend to get stuck.

955
01:24:15,440 --> 01:24:18,800
But it doesn't seem that hard to imagine that like, you know, if you had something that is sort

956
01:24:18,800 --> 01:24:25,040
of that next level that you put it into a loop, you say, okay, you're Elon Musk, LLM, and your

957
01:24:25,040 --> 01:24:31,600
job is to like make, you know, us, whatever us exactly is a, you know, multi planetary species.

958
01:24:31,600 --> 01:24:37,360
And then you just kind of keep updating your status, keep updating your plans, keep trying

959
01:24:37,360 --> 01:24:43,280
stuff, keep getting feedback. And, you know, like what really limits that.

960
01:24:44,160 --> 01:24:50,400
There may be like a really good program. But the whole AI kills everyone thing is so it's like,

961
01:24:51,040 --> 01:24:58,960
where's the actuator? Okay, I hit enter. What kills me? Right? Is it a hypnotized human who's

962
01:24:58,960 --> 01:25:04,480
being hypnotized by an AI that he's typed into? And he's radicalized himself by typing into a

963
01:25:04,480 --> 01:25:08,800
computer? Okay, that's not that different from a lot of other things that have happened in the past.

964
01:25:08,800 --> 01:25:14,000
Right. So who is actually striking me? Right? Who's striking the human? It's another human within

965
01:25:14,000 --> 01:25:18,960
acts that he's been radicalized by an AI. Okay, he's not actually that's not even the right term.

966
01:25:18,960 --> 01:25:24,640
We're giving agency to the AI when it's not really an agent. It is a human who's self radicalized by

967
01:25:24,640 --> 01:25:31,120
typing into a computer screen and has hit another human. That's one scenario. The other scenario is

968
01:25:31,120 --> 01:25:35,840
it's literally a Skynet drone that's hitting you. Those are the only two. How else is it going to be

969
01:25:35,840 --> 01:25:41,440
physical? Right? How does it? The actuation step is a part that is skipped over and it's a non trivial

970
01:25:41,440 --> 01:25:46,160
step. Well, I think it could be lots of things, right? I mean, if it's not one of those two,

971
01:25:46,160 --> 01:25:52,320
if it's not human or drone hitting you, what is it? Just habitat degradation, right? I mean,

972
01:25:52,320 --> 01:25:56,640
how do we kill most of the other species that we drive to extinction? We don't go out and hunt them

973
01:25:56,640 --> 01:26:01,920
down with axes one by one. We just change the environment more broadly to the point where

974
01:26:01,920 --> 01:26:06,720
it's not suitable for them anymore and they don't have enough space and they kind of die out, right?

975
01:26:06,720 --> 01:26:11,760
So we did hunt down some of the megafauna literally one by one with spears and stuff,

976
01:26:11,760 --> 01:26:18,720
but most of the recent loss of species is just like we're out there just extracting resources for

977
01:26:18,720 --> 01:26:24,240
our own purposes. And in the course of doing that, you know, whatever bird or whatever, you know,

978
01:26:24,240 --> 01:26:29,360
thing just kind of loses its place and then it's no more. And I don't think that's like totally

979
01:26:29,360 --> 01:26:35,280
implausible. Wait, so that is though, I think within normal world, right? What does that mean?

980
01:26:35,280 --> 01:26:42,400
That means that some people, some amplified intelligence, and maybe we might call it HAI,

981
01:26:42,400 --> 01:26:49,440
okay, human plus AI combination, right? Some HAIs outcompete others economically and they lose

982
01:26:49,440 --> 01:26:54,080
their jobs. Is that what you're talking about? I think also the humans potentially become

983
01:26:54,080 --> 01:26:59,520
unnecessary in a lot of the configurations, like just a recent paper from DeepMind.

984
01:26:59,520 --> 01:27:05,280
It's your marginal product workers. Or negative. Yeah, I mean, so the last, you know, DeepMind has

985
01:27:05,280 --> 01:27:12,320
been on Google, Google DeepMind has been on a tear of increasingly impressive medical AIs.

986
01:27:12,320 --> 01:27:18,160
Their most recent one takes a bunch of difficult case studies from the literature. I mean, case

987
01:27:18,160 --> 01:27:24,080
studies, you know, this is like rare diseases, hard to diagnose stuff, and asks an AI to do the

988
01:27:24,080 --> 01:27:30,560
differential diagnosis, compares that to human and compares it to human plus AI. And they phrase

989
01:27:30,560 --> 01:27:36,240
their results like in a very understated way. But the headline is the AI blows away the human

990
01:27:36,240 --> 01:27:42,480
plus AI. The human makes the AI worse. So here's the thing. I'll say something provocative, maybe.

991
01:27:42,480 --> 01:27:48,800
Okay, like I have in a very fine. I do think that the ABCs of economic apocalypse for blue America

992
01:27:48,800 --> 01:27:54,480
are AI, Bitcoin and China, where AI takes away their, a lot of the revenue streams, the licensures

993
01:27:54,480 --> 01:27:59,920
that have made medical and legal costs and other things so high. Bitcoin takes away the power of

994
01:27:59,920 --> 01:28:06,160
money and China takes away their military power. So I've received total meltdown for blue America

995
01:28:07,280 --> 01:28:12,640
in the years and, you know, maybe decade to come already kind of happening. But that's different

996
01:28:12,640 --> 01:28:18,160
than being at the end of the world, right? Like blue America had a really great time for a long time

997
01:28:18,160 --> 01:28:23,920
and they've got these licensure locks. But because of that, they've hyperinflated the cost of medicine.

998
01:28:24,720 --> 01:28:30,160
It's like how much, so what you're talking about is, wow, we have infinite free medicine.

999
01:28:30,720 --> 01:28:34,320
Man, Dr. Billing events are going to get ahead. That's the point.

1000
01:28:35,040 --> 01:28:39,360
Yeah. And to be clear, I'm really with you on that too. Like I want to see one of the things,

1001
01:28:39,360 --> 01:28:44,240
when people say like, what is good about AI, you know, why should we, why should we pursue this?

1002
01:28:44,960 --> 01:28:51,360
This, my standard answer is high quality medical advice for everyone at pennies,

1003
01:28:51,440 --> 01:28:56,000
you know, per visit, right? It is orders of magnitude cheaper. We're already starting to

1004
01:28:56,000 --> 01:29:00,080
see that in some ways it's better. People prefer it, you know, that AI is more patient,

1005
01:29:00,080 --> 01:29:05,360
it has better bedside manner. I wouldn't say, you know, if I was giving my, you know, my own

1006
01:29:05,360 --> 01:29:10,560
family advice today, I would say use both a human doctor and an AI, but definitely use the AI as

1007
01:29:10,560 --> 01:29:15,200
part of your mix. Absolutely. That's right. That's right. But you're prompting it still, right?

1008
01:29:15,200 --> 01:29:20,480
The smarter you are, the smarter the AI is, you notice this immediately with your vocabulary,

1009
01:29:20,480 --> 01:29:24,800
right? The more sophisticated your vocabulary, the finer the distinctions you can have,

1010
01:29:24,800 --> 01:29:29,600
the better your own ability to spot errors. You can generate a basic program with it, right?

1011
01:29:29,600 --> 01:29:33,280
But really amplified intelligence is I think a much better way of thinking about it,

1012
01:29:33,280 --> 01:29:38,560
because whatever your IQ is, it surges it upward by a factor of three or whatever the number.

1013
01:29:38,560 --> 01:29:43,360
And maybe the amplifier increases with your intelligence, but that, that internal intelligence

1014
01:29:43,360 --> 01:29:47,280
difference still exists. It's just like what a computer is, a computer is an amplifier for

1015
01:29:47,280 --> 01:29:52,320
intelligence. If you're smart, you can hit enter and programs can go to like, like thinking about

1016
01:29:52,320 --> 01:29:58,720
the Minecraft guy, right? Or Satoshi, one person built a billion or an associated trillion dollar

1017
01:29:58,720 --> 01:30:04,880
thing, you know, obviously other people continued Bitcoin and so on and so forth, right? So what I

1018
01:30:04,880 --> 01:30:11,280
feel though is this is what I mean by going from nuclear terrorism to the TSA. Okay, we went from

1019
01:30:11,280 --> 01:30:16,800
AI will kill everyone. And I'm like, what's the actuator to, okay, it'll gradually to greater

1020
01:30:16,800 --> 01:30:19,680
environment. What does that mean? Okay, some people lose their jobs, but then we're back in normal

1021
01:30:19,680 --> 01:30:23,600
worlds. Well, hold on, let me paint a little bit more complete picture because I don't think we're

1022
01:30:23,600 --> 01:30:31,040
quite there yet. So I think the differential diagnosis, recent paper, that's just a data point

1023
01:30:31,040 --> 01:30:35,520
where it's kind of like chess. This, you know, this came long before, right? There was a period

1024
01:30:35,520 --> 01:30:39,520
where humans are the best chess players, then there was a period where the best were the hybrid

1025
01:30:39,520 --> 01:30:45,120
human AI systems. And now as far as I understand it, we're in a regime where the human can't really

1026
01:30:45,120 --> 01:30:50,640
help the AI anymore. And so the AIs are, you know, the best chess players are just pure AIs.

1027
01:30:50,640 --> 01:30:55,440
We're not there in medicine, but we're starting to see examples where, hey, in a pretty defined study

1028
01:30:56,000 --> 01:31:01,040
differential diagnosis, the AI is beating, not just beating the humans, but also beating the AI

1029
01:31:01,040 --> 01:31:07,680
human hybrid or the human with access to AI. So, okay, that's not it, right? There's a paper

1030
01:31:07,680 --> 01:31:17,200
recently called Eureka out of NVIDIA. This is Jim Fan's lab where they use GPT-4 to write the reward

1031
01:31:17,200 --> 01:31:24,160
functions to train a robot. So you want to train a robot to like twirl a pencil in fingers. Hard,

1032
01:31:24,160 --> 01:31:28,880
you know, hard for me to do. Robots, you definitely can't do it. How do you train that? Well, you

1033
01:31:28,880 --> 01:31:33,040
need a reward function, the reward function. Basically, while you're in the early process

1034
01:31:33,040 --> 01:31:38,080
of learning and failing all the time, the reward function gives you encouragement when you're on

1035
01:31:38,080 --> 01:31:42,400
the right track, right? So there are people who, you know, have developed this skill and you might

1036
01:31:42,400 --> 01:31:46,720
do something like, well, if the pencil has angular momentum, you know, then that seems like you're

1037
01:31:46,720 --> 01:31:50,320
on maybe sort of the right track. So give that, you know, a reward, even though at the beginning

1038
01:31:50,320 --> 01:31:56,000
you're just failing all the time. Turns out GPT-4 is way better than humans at this, right? So it's

1039
01:31:56,000 --> 01:32:01,760
better at training robots. So all of that is awesome. And it's great. And, but here is, here's the thing,

1040
01:32:01,760 --> 01:32:07,120
is there's a huge difference between AI is going to kill everybody and turn everybody into paper

1041
01:32:07,120 --> 01:32:14,880
clips, okay, versus some humans with some AI are going to make a lot more money. And some people

1042
01:32:14,880 --> 01:32:19,120
are going to lose their jobs. Yeah, I'm not scared of that. I'm not scared of that scenario. I mean,

1043
01:32:19,120 --> 01:32:24,160
it could be disruptive. It could be disruptive, but it's not existential under itself.

1044
01:32:25,120 --> 01:32:29,280
Big deal. Okay. So that's why I went, right. There's the, the, the beta, to me, it comes,

1045
01:32:29,280 --> 01:32:34,560
if I, if I ask just one question is what is the actuator, right? You know, sensors and actuators,

1046
01:32:34,560 --> 01:32:40,240
right? What is the thing that's actually going to plunge a knife or a bullet into you and kill

1047
01:32:40,240 --> 01:32:49,200
you? It is either a human who has hypnotized themselves by typing into a computer, like

1048
01:32:49,200 --> 01:32:54,720
basically an AI terrorist, you know, which is kind of where some of the EAs are going in my view,

1049
01:32:54,800 --> 01:33:02,640
or it is like an autonomous drone that is controlled in a starcraft or terminator like way.

1050
01:33:03,280 --> 01:33:08,560
We are not there yet in terms of having enough humanoid or autonomous drones that are intranet

1051
01:33:08,560 --> 01:33:13,840
connected and programmable. That won't be there for some time. Okay. So that alone means fast take

1052
01:33:13,840 --> 01:33:20,480
off is, and what I think by the time we get there, you will have a cryptographic control over them.

1053
01:33:20,480 --> 01:33:25,120
That's a crucial thing. Cryptography fragments the whole space in a very fundamental way.

1054
01:33:25,680 --> 01:33:30,480
If you don't have the private keys, you do not have control over it. So long as that piece of

1055
01:33:30,480 --> 01:33:35,280
hardware, the cryptographic controller, you've nailed the equations on that. And frankly, you can

1056
01:33:35,280 --> 01:33:41,040
use AI to attack that as well to make sure the code is perfect, right? Remember you talked about

1057
01:33:41,040 --> 01:33:48,320
attack and defense? AI is attack crypto's defense, right? Because one of the things that crypto is

1058
01:33:48,320 --> 01:33:54,080
done, do you know the PKI problem is public key infrastructure? I'll say no, on behalf of the

1059
01:33:54,080 --> 01:33:58,480
audience. This is good. We should do more of these actually. I feel it's a good, you know, fusion of

1060
01:33:58,480 --> 01:34:04,800
things or whatever, right? But the public key infrastructure problem, the public key infrastructure

1061
01:34:04,800 --> 01:34:11,040
problem is something that was sort of lots of cryptography papers and computer science papers

1062
01:34:11,040 --> 01:34:17,200
in the 90s and 2000s assumed that this could exist and essentially meant if you could assume that

1063
01:34:17,200 --> 01:34:24,640
everybody on the internet had a public key that was public and a private key that was kept both

1064
01:34:24,640 --> 01:34:29,360
secure and available at all times, then there's all kinds of amazing things you can do with

1065
01:34:29,360 --> 01:34:36,160
privacy preserving, messaging and authentication and so on. The problem is that for many years,

1066
01:34:36,160 --> 01:34:41,120
what cryptographers try to do is they try to nag people into keeping their private keys secure

1067
01:34:41,120 --> 01:34:46,080
and available. And the issue is it's trivial to keep it secure and unavailable where you write

1068
01:34:46,080 --> 01:34:50,480
it down, you put it into a lockbox and you lose the lockbox. It's trivial to keep it available

1069
01:34:50,480 --> 01:34:56,400
and not secure, okay, where you put it on your public website and it's available all the time,

1070
01:34:56,400 --> 01:35:02,480
you never lose it, but it's not secure because anybody can see it. When you actually ask,

1071
01:35:02,480 --> 01:35:07,840
what does it mean to keep something secure and available? That's actually a very high cost.

1072
01:35:07,840 --> 01:35:13,280
It's precious space because it's based on your wallet, right? Your wallet is on your person

1073
01:35:13,280 --> 01:35:19,360
at all times, so it's available, but it's not available to everybody else, so it's secure.

1074
01:35:19,360 --> 01:35:24,800
So you actually have to touch it constantly, yes, right? So it turns out that the crypto wallet

1075
01:35:25,600 --> 01:35:31,680
by adding a literal incentive to keep your private keys secure and available because if they're not

1076
01:35:31,680 --> 01:35:36,320
available, you've lost your money. If they're not secure, you've lost your money, okay? To have both

1077
01:35:36,320 --> 01:35:41,520
of them, that was what solved the PKI problem. Now we have hundreds of millions of people with

1078
01:35:41,520 --> 01:35:46,400
public private key pairs where the private keys are secure and available. That means all kinds

1079
01:35:46,400 --> 01:35:51,280
of cryptographic schemes, zero knowledge stuff, there's this amazing universe of things that is

1080
01:35:51,280 --> 01:35:55,360
happening now. Zero knowledge in particular has made cryptography much more programmable.

1081
01:35:55,360 --> 01:36:00,320
There's a whole topic which is, if you want something that's kind of, you know, like AI was

1082
01:36:00,320 --> 01:36:04,320
creeping for a while and people, specialists were paying attention to it and then just burst

1083
01:36:04,320 --> 01:36:08,640
out on the scene, zero knowledge is kind of like that for cryptography. Thanks to the,

1084
01:36:08,640 --> 01:36:10,960
you know, you've probably heard of zero knowledge before.

1085
01:36:10,960 --> 01:36:20,080
Yeah, we did one episode with Daniel Kong on the use of zero knowledge proofs to basically

1086
01:36:20,960 --> 01:36:25,680
to prove without revealing like the weights that you actually ran the model you said you were

1087
01:36:25,680 --> 01:36:28,000
going to run and things like that, I think are super interesting.

1088
01:36:28,640 --> 01:36:34,960
Exactly, right? So what kinds of stuff, why is that useful in the AI space? Well, first is you

1089
01:36:34,960 --> 01:36:39,280
can use it, for example, for training on medical records while keeping them both private,

1090
01:36:39,280 --> 01:36:44,000
but also getting the data you want. For example, let's say you've got a collection of

1091
01:36:45,040 --> 01:36:54,000
genomes, okay, and you want to ask, okay, how many G's were in this data set, how many C's,

1092
01:36:54,000 --> 01:36:57,520
how many A's, how many T's, okay, like you just say, like, that's a very simple downstairs.

1093
01:36:57,520 --> 01:37:03,840
What's the ACG T content of this, you know, the sequence data set, you could get those numbers,

1094
01:37:03,840 --> 01:37:07,040
you could prove they were correct without giving any information about the individual sequences,

1095
01:37:07,040 --> 01:37:11,120
right, or more specifically, you do it at one locus and you say, how many G's and how many C's

1096
01:37:11,120 --> 01:37:17,920
are at this particular locus and you get the SNP distribution, okay. So it's useful for what you

1097
01:37:17,920 --> 01:37:22,160
just said, which is like showing that you ran a particular model without giving anything else away,

1098
01:37:22,160 --> 01:37:27,680
it's useful for certain kinds of data analysis. There's a lot of overhead on compute on this

1099
01:37:27,680 --> 01:37:30,960
right now, so it's not something that you do trivially, okay, but it'll probably come down with

1100
01:37:30,960 --> 01:37:39,920
time. But what is perhaps most interestingly useful for it is in the context of AI is coming up with

1101
01:37:39,920 --> 01:37:45,840
things in AI can't fake. So what we talked about earlier, right, like an AI can come up with all

1102
01:37:45,840 --> 01:37:52,880
kinds of plausible sounding images, but if it wasn't cryptographically signed by the sender,

1103
01:37:54,080 --> 01:38:00,160
then, you know, it, and it should be signed by sender and put on chain. And then at least you

1104
01:38:00,160 --> 01:38:07,600
know that this person or this entity with this private key asserted that this object existed at

1105
01:38:07,600 --> 01:38:11,680
this time in a way that'd be extremely expensive to falsify because it's either on the Bitcoin

1106
01:38:11,680 --> 01:38:15,760
blockchain or another blockchain, it's very expensive to rewind, okay. This starts to be

1107
01:38:15,760 --> 01:38:21,040
a bunch of facts that an AI can't fake. You know, so going back to the kind of big picture

1108
01:38:21,920 --> 01:38:26,240
loss of control story, I was just kind of trying to build up a few of these data points that like,

1109
01:38:26,240 --> 01:38:31,280
hey, look at this differential diagnosis, we already see like humans are not really adding value

1110
01:38:31,280 --> 01:38:36,880
to AI's anymore. That's kind of striking. And like similarly with training robot hands, GPT-4 is

1111
01:38:36,880 --> 01:38:43,840
outperforming human experts. And by the way, all of the sort of latent spaces are like totally

1112
01:38:43,840 --> 01:38:48,240
bridgeable, right? I mean, one of the most striking observations of the last couple of years of study

1113
01:38:48,240 --> 01:38:54,480
is that AI's can talk to each other in high dimensional space, which we don't really have a

1114
01:38:54,480 --> 01:38:58,720
way of understanding natively, right? It takes a lot of work for us to decode.

1115
01:38:59,440 --> 01:39:03,600
This is like the language thing? We're starting to see AI's kind of develop,

1116
01:39:03,600 --> 01:39:11,600
not obviously totally on their own as of now, but there is becoming an increasingly reliable

1117
01:39:11,600 --> 01:39:18,480
go-to set of techniques if you want to bridge different modalities with like a pretty small

1118
01:39:18,480 --> 01:39:22,880
parameter adapter. That's interesting. Actually, what's a good paper on that? I actually hadn't

1119
01:39:22,880 --> 01:39:26,880
seen that. The Blip family of models out of Salesforce research is really interesting,

1120
01:39:26,880 --> 01:39:31,520
and I've used that in production at Salesforce. Really? Yeah, Salesforce research. They have a

1121
01:39:31,520 --> 01:39:39,680
crack team that has open sourced a ton of stuff in the language model, computer vision, joint space.

1122
01:39:40,560 --> 01:39:46,640
And you see this all over the place now, but basically what they did in a paper called Blip

1123
01:39:46,640 --> 01:39:51,120
2, and they've had like five of these with a bunch of different techniques. But in Blip 2,

1124
01:39:51,840 --> 01:39:56,480
they took a pre-trained language model and then a pre-trained computer vision model,

1125
01:39:57,040 --> 01:40:01,760
and they were able to train just a very small model that kind of connects the two. So you could

1126
01:40:01,760 --> 01:40:09,040
take an image, put it into the image space, then have their little bridge that over to language

1127
01:40:09,040 --> 01:40:13,040
space, and that everything else, the two big models are frozen. So they were able to do this on

1128
01:40:13,040 --> 01:40:18,640
just like a couple days worth of GPU time, which I do think goes to show how it is going to be

1129
01:40:18,640 --> 01:40:23,280
very difficult to contain proliferation. Which is good. In my view, that's really good.

1130
01:40:23,280 --> 01:40:28,000
As long as it doesn't get out of control, I'm probably with you on that too. But by bridging

1131
01:40:28,000 --> 01:40:33,280
this vision space into the language space, then the language model would be able to converse with you

1132
01:40:33,280 --> 01:40:38,960
about the image, even though the language model was never trained on images, but you just had this

1133
01:40:38,960 --> 01:40:44,080
connector that kind of bridges those modalities. It's just, it's like another layer of the network

1134
01:40:44,080 --> 01:40:50,160
that just bridges two networks almost. Yeah, it bridges the spaces. It bridges the conceptual

1135
01:40:50,160 --> 01:40:55,040
spaces between something that has only understood images and something that has only understood

1136
01:40:55,040 --> 01:40:58,960
language, but now you can kind of bring those together. As I think about it, it's not that

1137
01:40:58,960 --> 01:41:04,880
surprising because that's what, for example, text image models are basically that. They're

1138
01:41:04,880 --> 01:41:10,400
bridging two spaces in a sense, right? But I'll check this paper out. So on the one hand,

1139
01:41:10,480 --> 01:41:14,160
it's not that surprising. On the other hand, I should see how they implemented it or whatever,

1140
01:41:14,160 --> 01:41:19,200
so blip two. Okay. Yeah, I think the most striking thing about that is just how small it is. Like,

1141
01:41:19,200 --> 01:41:25,840
you took these two off the shelf models that were trained independently for other purposes,

1142
01:41:26,400 --> 01:41:33,200
and you're able to bridge them with a relatively small connector. And that seems to be kind of,

1143
01:41:33,200 --> 01:41:37,920
you know, happening all over the place. I would also look at the Flamingo architecture,

1144
01:41:37,920 --> 01:41:44,160
which is like a year and a half ago now out of DeepMind. That was one for me where I was like,

1145
01:41:44,160 --> 01:41:48,720
oh my, and it's also a language to vision, where they keep the language model frozen.

1146
01:41:48,720 --> 01:41:54,000
And then they kind of, in my mind, it's like, I can see the person in their garage like tinkering

1147
01:41:54,000 --> 01:41:57,440
with their soldering iron, you know, because it's just like, wow, you took this whole language

1148
01:41:57,440 --> 01:42:02,080
thing that was frozen, and you kind of injected some, you know, vision stuff here, and you added

1149
01:42:02,080 --> 01:42:06,560
a couple layers, and you kind of Frankenstein it, and it works. And it's like, wow, that's not really,

1150
01:42:07,200 --> 01:42:12,240
it wasn't like super principled, you know, it was just kind of hack a few things together and,

1151
01:42:12,240 --> 01:42:15,280
you know, try training it. And I don't want to diminish what they did, because I'm sure there

1152
01:42:15,280 --> 01:42:21,600
were, you know, more insights to it than that. But it seems like we are kind of seeing a reliable

1153
01:42:21,600 --> 01:42:28,000
pattern of the key point here being model to model communication through high dimensional

1154
01:42:28,000 --> 01:42:36,240
space, which is not mediated by human language, is I think one of the reasons that I would expect,

1155
01:42:36,240 --> 01:42:40,480
and by the way, there's lots of papers too on like, you know, language models are human level,

1156
01:42:40,480 --> 01:42:44,560
or even superhuman prompt engineers, you know, they're they're self prompting, like,

1157
01:42:44,560 --> 01:42:48,880
techniques are getting pretty good. So if I'm imagining the big picture of like,

1158
01:42:49,920 --> 01:42:53,440
and we can, you know, get back to like, okay, well, how do we use any techniques crypto or

1159
01:42:53,440 --> 01:42:58,800
otherwise to keep this under control? And then I would say this is kind of the newer school of

1160
01:42:58,800 --> 01:43:04,800
the big picture AI safety worry. Obviously, there's a lot of flavors. But if you were to,

1161
01:43:04,800 --> 01:43:08,480
you know, go look at like a Jay Acatra, for example, I think a really good writer on this.

1162
01:43:09,760 --> 01:43:14,000
Her worldview is less that we're going to have this fume and more that over a period of time,

1163
01:43:14,000 --> 01:43:17,440
and it may not be a long period of time, maybe it's like a generation, maybe it's 10 years,

1164
01:43:17,440 --> 01:43:22,000
maybe it's 100 years. But obviously, those are all small in the sort of, you know, grand scheme of

1165
01:43:22,000 --> 01:43:33,520
the future. We have, in all likelihood, the development of AI centric schemes of production,

1166
01:43:33,520 --> 01:43:37,280
where you've got kind of your high level executive function is like your language model,

1167
01:43:37,280 --> 01:43:41,920
you've got all these like lower level models, they're all bridgeable, all the spaces are bridgeable

1168
01:43:41,920 --> 01:43:46,720
in high dimensional form, where they're not really mediated by language, unless we enforce

1169
01:43:46,720 --> 01:43:52,160
that. I mean, we could say, you know, it must always be mediated by language so we can read the

1170
01:43:52,160 --> 01:43:58,400
logs. But there's a text to that, right? Because going through language is like highly compressed,

1171
01:43:59,120 --> 01:44:03,600
compared to the high dimensional space to space. All right, so let me see if I can

1172
01:44:03,600 --> 01:44:07,680
steal man or articulate your case, you're saying, AI's are going to get good enough,

1173
01:44:07,680 --> 01:44:10,960
they're going to be able to communicate with each other good enough, and they'll be able to do enough

1174
01:44:10,960 --> 01:44:15,520
tasks that more and more humans will be rendered economically marginal and unnecessary.

1175
01:44:15,520 --> 01:44:18,880
I'm not saying I think that will happen, I'm just saying I think there's a good enough chance

1176
01:44:18,880 --> 01:44:23,200
that that will happen, but it's worth taking really seriously. I actually think that will happen,

1177
01:44:23,200 --> 01:44:27,440
something along those lines, or in the sense of at least massive economic disruption,

1178
01:44:27,440 --> 01:44:33,440
definitely. Okay, but I'll give an answer to that, which is both, you know, maybe fun and not fun.

1179
01:44:33,440 --> 01:44:38,800
Have you seen the graph of the percentage of America that was involved in farming?

1180
01:44:39,520 --> 01:44:41,520
Yeah, I tweeted a version of that once.

1181
01:44:42,160 --> 01:44:46,240
I did. Okay, great. Good. So you're familiar with this, and you're familiar with what I mean by

1182
01:44:46,240 --> 01:44:52,640
the implication of it, where basically Americans used to identify themselves as farmers, right?

1183
01:44:52,640 --> 01:45:01,360
And manufacturing rose as agriculture collapsed, right? And here is the graph on that. But from

1184
01:45:01,360 --> 01:45:08,240
like 40% in the year 1900 to like a total collapse of agriculture, and then also more recently a

1185
01:45:08,240 --> 01:45:13,600
collapse of manufacturing into bureaucracy, paperwork, legal work, what is up into the right

1186
01:45:13,600 --> 01:45:21,120
since then is, you know, the lawyers, what is up into the right? What is replacing that?

1187
01:45:21,840 --> 01:45:27,520
Starting in around the 1970s, we used to be adding energy production and energy production

1188
01:45:27,520 --> 01:45:32,720
flatlined once people got angry about nuclear power. So this is a future that could have been,

1189
01:45:32,720 --> 01:45:38,720
we could be on Mars by now, but we got flatlined, right? What did go up into the right? So construction

1190
01:45:38,720 --> 01:45:43,680
costs, this is the bad scenario where the miracle energy got destroyed because regulations,

1191
01:45:44,560 --> 01:45:49,200
the cost was flat. And then when vertical, when regulations were imposed, all the progress was

1192
01:45:49,200 --> 01:45:55,360
stopped by decels and degrowthers. And then Alara was implemented, which said nuclear energy

1193
01:45:55,360 --> 01:46:00,720
has to be as low risk as reasonably necessary, as reasonably achievable. And that meant that you

1194
01:46:00,720 --> 01:46:04,640
just keep adding quote safety to it until it's as same as cost as everything else, which means you

1195
01:46:04,640 --> 01:46:09,440
destroyed the value of it, right? But you know what was up into the right? What replaced those

1196
01:46:09,440 --> 01:46:13,760
agriculture and manufacturing jobs? Look at this, you see this graph? For the audio only, we will

1197
01:46:13,760 --> 01:46:18,320
put this on YouTube. So if you want to see the graph do the YouTube version of this, for the audio

1198
01:46:18,320 --> 01:46:23,040
only group, it's an exponential curve in the number of lawyers in the United States from,

1199
01:46:23,040 --> 01:46:27,760
looks like maybe two thirds of a million to 13 million over the last 140 years.

1200
01:46:27,760 --> 01:46:33,600
Yeah. And in 1880, it was like, like sub 100,000 or something like that, right? And then it's just

1201
01:46:33,600 --> 01:46:39,040
like, especially that 1970 point, that's when it went totally vertical. Okay. And it's probably

1202
01:46:39,040 --> 01:46:44,160
even more since. So, you know, if you add paperwork jobs, bureaucratic jobs, you know,

1203
01:46:44,160 --> 01:46:49,040
every lawyer is like, you know, net, sorry, lawyers, but you're basically negative value add,

1204
01:46:49,040 --> 01:46:54,000
right? Because it should, the fact that you have a lawyer means that you couldn't just self serve a

1205
01:46:54,000 --> 01:46:58,880
form, right? Basic government is platform is where you can just self serve and you fill it out.

1206
01:46:58,880 --> 01:47:03,280
And you don't have to have somebody like code something for you custom, you know, lawyers

1207
01:47:03,280 --> 01:47:08,640
that's doing custom code is because the legal code is so complicated. So, you know, the whole

1208
01:47:08,640 --> 01:47:12,000
Shakespeare thing, like first thing we do, let's, you know, kill all the lawyers. First thing we

1209
01:47:12,000 --> 01:47:17,840
do, let's automate all the lawyers, right? Only something that's the hammer blow of AI

1210
01:47:17,840 --> 01:47:23,360
can break the backbone and it will. That's it's going to break the backbone of blue America,

1211
01:47:23,360 --> 01:47:27,440
right? It's going to cause that's why the political layer and the sovereignty layer

1212
01:47:27,440 --> 01:47:33,120
is not what AI people think about. But it's like crucial for thinking about AI, because what tribes

1213
01:47:33,120 --> 01:47:40,080
does AI benefit? And again, we got away from why does AI kill everybody? Well, it's going to need

1214
01:47:40,080 --> 01:47:43,680
actuators. Who's going to stab you? Who's going to shoot you? It's got to be a human hypnotized

1215
01:47:43,680 --> 01:47:48,560
by AI or a drone that AI controls. A human hypnotized by AI is actually a conventional

1216
01:47:48,560 --> 01:47:52,320
threat. It looks like a terrorist cell. We know how to deal with that, right? It's just like radicalized

1217
01:47:52,320 --> 01:47:56,960
humans that worship some AI that stab you. It's like the pause AI people are one step, I think,

1218
01:47:56,960 --> 01:48:01,360
away from that. All right. But that's just like on Shin Riko. That's like allocated. That's like

1219
01:48:01,360 --> 01:48:06,960
basically terrorists who think that the AI is telling them what to do. Fine. If it's not a human

1220
01:48:06,960 --> 01:48:12,000
that's stabbing you, it is a drone. And that's like a very different future where

1221
01:48:12,880 --> 01:48:16,720
like five or 10 or 15 years up, maybe we have enough internet connected drones out there,

1222
01:48:16,720 --> 01:48:21,680
but even then they'll have private keys. So there's going to be fragmentation of address space.

1223
01:48:21,680 --> 01:48:27,360
Not all drones be controlled by everybody in my view. Okay. That's what AI safety is. AI safety is

1224
01:48:27,360 --> 01:48:32,720
can you turn it off? Can you kill it? Can you stop it from controlling drones? That's what AI

1225
01:48:32,720 --> 01:48:38,320
safety is. Can you also open the model weights so you can generate adversarial inputs? Can you

1226
01:48:38,320 --> 01:48:41,680
open the model weights and proliferate it? You're saying, oh, proliferation is bad. I'm saying

1227
01:48:41,680 --> 01:48:46,880
proliferation is good because if everybody has one, then nobody has an advantage on it.

1228
01:48:47,440 --> 01:48:52,800
Right. Not relatively speaking. Okay. I have very few super confident positions. So I wouldn't

1229
01:48:52,800 --> 01:49:00,880
necessarily say I think that proliferation is bad. I'd say so far it's good. It has and even

1230
01:49:00,960 --> 01:49:07,440
most of the AI safety people, I would say if I could speak on the behalf of the AI safety

1231
01:49:08,320 --> 01:49:15,920
consensus, I would say most people would say even that the Llama 2 release has proven

1232
01:49:15,920 --> 01:49:20,160
good for AI safety for the reasons that you're saying. But they opposed it.

1233
01:49:20,160 --> 01:49:25,280
Well, some didn't, some didn't. I would say the main posture that I see AI safety people taking

1234
01:49:25,280 --> 01:49:31,200
is that we're getting really close to or we might be getting really close.

1235
01:49:32,160 --> 01:49:36,240
Certainly if we just naively extrapolate out recent progress, it would seem that we're getting

1236
01:49:36,240 --> 01:49:43,920
really close to systems that are sufficiently powerful that it's very hard to predict what

1237
01:49:43,920 --> 01:49:52,400
happens if they proliferate. Llama 2, not there. And so, yes, it has enabled a lot of interpretability

1238
01:49:52,400 --> 01:49:56,720
work. It has enabled things like representation engineering, which there is a lot of good

1239
01:49:56,720 --> 01:50:02,160
stuff that has come from it. The big thing that I want to kind of establish is you agree with me

1240
01:50:02,160 --> 01:50:08,800
on the actuation point or not. The thing is this thing, like Llama 2 proliferates and so

1241
01:50:08,800 --> 01:50:14,000
businesses are disrupted and people, maybe they paid a lot of money for their MD degree and they

1242
01:50:14,000 --> 01:50:18,640
can't make us a bunch of money. That's within the realm of what I call conventional warfare.

1243
01:50:18,720 --> 01:50:22,240
You know what I mean? That's like we're still in normal world as we were talking about, okay?

1244
01:50:22,960 --> 01:50:29,200
Unconventional warfare is, you know, Skynet arises and kills everybody, okay? And that is what is

1245
01:50:29,200 --> 01:50:34,640
being sold over here. And when you think about the actuators, we don't have the drones out there,

1246
01:50:34,640 --> 01:50:39,600
we don't have the humanoid robots at control, and hypnotized humans are a very tiny subset of humans,

1247
01:50:39,600 --> 01:50:44,400
probably. And even if they aren't, that just looks like a religion or a cult or a terrorist

1248
01:50:44,400 --> 01:50:48,880
cell, and we know how to deal with that as well. The super intelligent AI with, you know,

1249
01:50:48,880 --> 01:50:54,320
lots of robots at control in a starcraft form, I would agree, is something that humans haven't

1250
01:50:54,320 --> 01:50:59,520
faced yet. But by the time we get that many robots out there, you won't be able to control all of

1251
01:50:59,520 --> 01:51:04,480
them at once because of the private key things I mentioned. So that's why I'm like, okay,

1252
01:51:04,480 --> 01:51:08,080
everything else we're talking about is in normal world. That is the single biggest thing

1253
01:51:08,080 --> 01:51:13,520
that I wanted to get, like economic disruption, people losing jobs, proliferation so that the

1254
01:51:13,520 --> 01:51:18,480
balance of power is redistributed, all that is fine. The reason I say this is people keep trying

1255
01:51:18,480 --> 01:51:22,880
to link AI to existential risk. A great example is one of the things you actually had in here,

1256
01:51:22,880 --> 01:51:26,720
this is similar to the AI policy and so on. It's a totally reasonable question, but then I'm going

1257
01:51:26,720 --> 01:51:30,480
to, in my view, deconstruct the question. What would you think about putting the limit on the

1258
01:51:30,480 --> 01:51:33,600
right to compute or their capabilities and AI system might demonstrate that we make you think

1259
01:51:33,600 --> 01:51:37,360
open access no longer wise? The most common near term answer here to be seems to be related to

1260
01:51:37,360 --> 01:51:42,000
risk of pandemic via novel pathogen engineering. So guess what? You know who the novel pathogen

1261
01:51:42,080 --> 01:51:48,240
engineers are? The US and Chinese governments, right? They did it, or probably did it, credibly

1262
01:51:48,240 --> 01:51:52,400
did it, credibly mean accused of doing it. They haven't been punished for COVID-19. In fact,

1263
01:51:52,400 --> 01:51:56,480
they covered up their culpability and pointed everywhere other than themselves. They used it

1264
01:51:56,480 --> 01:52:02,480
to gain more power in both the US and China with both lockdown in China and in the US and all kinds

1265
01:52:02,480 --> 01:52:08,240
of COVID era, trillions of dollars was printed and spent and so on and so forth. They did everything

1266
01:52:08,240 --> 01:52:13,120
other than actually solve the problem. That was actually getting the vaccines in the private

1267
01:52:13,120 --> 01:52:17,600
sector and they studied the existential risk only to generate it and they're even paid to

1268
01:52:17,600 --> 01:52:22,560
generate pandemic prevention and failed. So this would be the ultimate Fox guarding the henhouse.

1269
01:52:25,120 --> 01:52:28,400
The two organizations responsible for killing millions of people with novel pathogen are going

1270
01:52:28,400 --> 01:52:35,200
to prevent people from doing this by restricting compute. No, you know what it is actually. What's

1271
01:52:35,200 --> 01:52:41,360
happening here is one of the concepts I have in the network state is this idea of God, state,

1272
01:52:41,360 --> 01:52:45,600
and network. Meaning, what do you think is the most powerful force in the world? Is it Almighty

1273
01:52:45,600 --> 01:52:55,920
God? Is it the US government? Or is it encryption? Or eventually maybe an AGI? What's happening here

1274
01:52:55,920 --> 01:53:02,240
is a lot of people are implicitly, without realizing it, even if they are secular atheists,

1275
01:53:02,240 --> 01:53:08,080
they're treating GOV as GOD. They treat the US government as God as the final mover.

1276
01:53:08,800 --> 01:53:12,960
No, I appreciate your little, I take inspiration from you actually in terms of

1277
01:53:12,960 --> 01:53:19,200
trying to come up with these little quips that are memorable. So I was just smiling at that

1278
01:53:19,200 --> 01:53:25,760
because I think you do a great job of that and I try to encourage, I have less success

1279
01:53:25,760 --> 01:53:30,640
coining terms than you have, but certainly try to follow your example on that front.

1280
01:53:30,720 --> 01:53:34,960
It's like a helpful, if you can compress it down, it's like more memorable. So that's what I try

1281
01:53:34,960 --> 01:53:40,240
to do, right? So exactly, a lot of these people who are secular, think of themselves as atheists,

1282
01:53:40,240 --> 01:53:45,760
have just replaced GOD with GOV. They worship the US government as God. And there's two versions

1283
01:53:45,760 --> 01:53:50,880
of this. You know how like God has both the male and female version, right? The female version is

1284
01:53:50,880 --> 01:53:55,680
the Democrat God within the USA that has infinite money and can take care of everybody and care

1285
01:53:55,680 --> 01:54:00,080
for everybody. And the Republican God is the US military that can blow up anybody and it's the

1286
01:54:00,080 --> 01:54:07,840
biggest and strongest and most powerful America F. Yeah. Okay. And everybody who thinks of the US

1287
01:54:07,840 --> 01:54:14,800
government as being able to stop something is praying to a dead God. Okay, when you say this,

1288
01:54:14,800 --> 01:54:19,200
you actually get an interesting reaction from AI safety people where you've actually hit their

1289
01:54:19,200 --> 01:54:26,080
true solar plexus. All right, the true solar plexus is not that they believe in AI, it's that

1290
01:54:26,080 --> 01:54:32,560
they believe in the US government. That's a true solar plexus because they are appealing to their

1291
01:54:32,560 --> 01:54:38,080
praying to this dead God that can't even clean the poop off the streets in San Francisco, right?

1292
01:54:38,080 --> 01:54:43,920
That is losing wars or fighting them to sell me. It says lost all these wars around the world

1293
01:54:43,920 --> 01:54:48,560
that spent trillions of dollars has been through financial crisis, Coronavirus, Iraq war, you

1294
01:54:48,560 --> 01:54:53,200
know, total meltdown politically. Okay, that is interest that is now has interest payments more

1295
01:54:53,200 --> 01:54:59,520
than the defense budget. That is, you know, that spent $100 billion on the California train without

1296
01:54:59,520 --> 01:55:04,720
laying a single track. It's like that, you know, that Morgan Freeman thing for you know, the clip

1297
01:55:04,720 --> 01:55:10,000
from Batman, who is like, So this man has a billionaire, blah, blah, blah, this and that,

1298
01:55:10,000 --> 01:55:14,960
and your plan is to threaten him, right? And so you're going to create this super intelligence

1299
01:55:14,960 --> 01:55:21,280
and have Kamala Harris regulate it. Come on, man, so to speak, right? Like these people are praying

1300
01:55:21,280 --> 01:55:30,000
to a blind, deaf and dumb God that was powerful in 1945, right? That's why, by the way, all the

1301
01:55:30,000 --> 01:55:36,240
popular movies, what are they? It's Barbie, it's Oppenheimer, right? It's, it's top gun. They're

1302
01:55:36,240 --> 01:55:43,040
all throwbacks the 80s or the 50s when the USA was really big and strong. And the future is a

1303
01:55:43,040 --> 01:55:48,080
black mirror. Yeah, I think that's tragic. One of the projects that I do like, and you might appreciate

1304
01:55:48,080 --> 01:55:54,800
this, I don't know if you've seen it, is the From the Future of Life Institute, a project called

1305
01:55:55,600 --> 01:56:01,840
Imagine a World, I think is the name of it. And they basically challenged, you know, their

1306
01:56:02,560 --> 01:56:10,960
audience and the public to come up with positive visions of a future, you know, where technology

1307
01:56:10,960 --> 01:56:16,720
changes a lot. And obviously, I pretty central to a lot of those stories. And, you know, one of the

1308
01:56:16,720 --> 01:56:22,880
challenges that people go through and how do we get there and whatever, but a purposeful effort

1309
01:56:22,880 --> 01:56:32,080
to imagine positive futures super under provided. And I really liked the investment that they made

1310
01:56:32,080 --> 01:56:36,720
in that. You know, one of the things I've got in the Never See It book is there's certain megatrends

1311
01:56:36,720 --> 01:56:42,880
that are happening, right? And megatrends, I mean, it's possible for like one miraculous human maybe

1312
01:56:42,880 --> 01:56:48,000
to reverse them, okay? Because I think both the impersonal force of history theory and the

1313
01:56:48,000 --> 01:56:54,080
great man theory of history have some truth to them. But the megatrends are the decline of Washington,

1314
01:56:54,080 --> 01:57:00,480
DC, the rise of the internet, the rise of India, the rise of China. That is like my worldview. And

1315
01:57:00,480 --> 01:57:05,680
I can give a thousand graphs and charts and so on for that. But that's basically the last 30 years.

1316
01:57:06,320 --> 01:57:10,240
And maybe the next X, right? I'm not saying there can't be trend reversal. Of course, it can be

1317
01:57:10,240 --> 01:57:13,680
trend reversal, as I just mentioned, some hammer blow could hit it, but that's what's happening.

1318
01:57:14,320 --> 01:57:18,800
And so because of that, the people who are optimistic about the future are aligned with either the

1319
01:57:18,800 --> 01:57:24,960
internet, India or China. And the people who are not optimistic about the future are blue Americans

1320
01:57:24,960 --> 01:57:31,120
or left out red Americans, okay, or Westerners in general, who are not tech tech people. Okay,

1321
01:57:31,120 --> 01:57:36,160
if they're not tech people, they're not up into the right, basically, because the internet's if you

1322
01:57:36,160 --> 01:57:40,640
I mean, one of the things is we have a misnomer, as I was saying earlier, calling it the United

1323
01:57:40,640 --> 01:57:44,800
States, because the dis United States, it's, it's like talking about, you know, talking about

1324
01:57:44,800 --> 01:57:48,080
America is like talking about Korea, there's North Korea and South Korea, they're totally different

1325
01:57:48,080 --> 01:57:54,400
populations. And, you know, communism and capitalism are totally different systems. And the thing that

1326
01:57:54,400 --> 01:57:59,120
is good for one is bad for another and vice versa. And so like America doesn't exist, there's only

1327
01:57:59,120 --> 01:58:02,960
just like there's no Korea, there's only North Korea and South Korea, there's no America, there's

1328
01:58:02,960 --> 01:58:09,920
blue America and red America and also gray America, tech America. And blue America is harmed, or they

1329
01:58:09,920 --> 01:58:14,480
think they're harmed, or they've gotten themselves into a spot where they're harmed by every technological

1330
01:58:14,480 --> 01:58:20,240
development, which is why they hate it so much, right? AI versus journalist jobs, crypto takes away

1331
01:58:20,240 --> 01:58:24,800
banking jobs, you know, everything, you know, self driving cars, they just take away regulator

1332
01:58:24,800 --> 01:58:29,600
control, right? Anything that reduces their power, they hate, and they're just trying to freeze an

1333
01:58:29,600 --> 01:58:35,120
amber with regulations. Red America got crushed a long time ago by offshoring to China and so on,

1334
01:58:35,120 --> 01:58:40,320
they're, they're making, you know, inroads ally with tech America or gray America. Tech America

1335
01:58:40,320 --> 01:58:44,320
is like the one piece of America that's actually still functional and globally competitive. And

1336
01:58:44,320 --> 01:58:49,280
people always do this fallacy of aggregation, where they talk about the USA. And it's really

1337
01:58:49,280 --> 01:58:53,440
this component that's up into the right, and the others that are down into the right, red,

1338
01:58:53,440 --> 01:58:58,160
best flat, like red, but they're like down, right? Like, red is like, okay, functional, blue is down.

1339
01:58:58,160 --> 01:59:04,000
The point is, tech America, I think we're going to find is not even truly or

1340
01:59:06,080 --> 01:59:11,760
how American is tech America, because it's like 50% immigrants, right? And like a lot of children

1341
01:59:11,760 --> 01:59:17,840
immigrants, and most of their customers are overseas, and their users are overseas. And

1342
01:59:17,840 --> 01:59:24,480
their vantage point is global, right? And they're basically not, I know we're in this

1343
01:59:24,480 --> 01:59:29,600
ultra nationalist kick right now. And I know that there's going to be, there's a degree of a fork

1344
01:59:29,600 --> 01:59:37,760
here, where you fork technology into Silicon Valley and the internet. Okay, where Silicon

1345
01:59:37,760 --> 01:59:42,000
Valley is American, and they'll be making like American military equipment and so on and so

1346
01:59:42,000 --> 01:59:47,520
forth, and they're signaling USA, which is fine. Okay. And then the internet is international,

1347
01:59:47,520 --> 01:59:54,080
global capitalism. And the difference is Silicon Valley, or let's say US tech, let me less, you

1348
01:59:54,080 --> 01:59:59,200
know, US tech says ban tick tock, build military equipment, etc. It's really identifying itself

1349
01:59:59,200 --> 02:00:04,800
as American. And it's thinking of being anti China. Okay, but there's US and China are only 20% of

1350
02:00:04,800 --> 02:00:10,720
the world, 80% of the world is neither American nor Chinese. So the internet is for everybody else

1351
02:00:10,720 --> 02:00:17,040
who wants actual global rule of law, right? When as a US decays as a rule space order, and people

1352
02:00:17,040 --> 02:00:22,000
don't want to be under China, people want to be under something like blockchains, where you've got

1353
02:00:22,000 --> 02:00:27,360
like property rights, contract law, cross borders that are enforced by an impartial authority. Okay,

1354
02:00:27,360 --> 02:00:31,440
that's also the kind of laws that can bind AI's like AI's across borders, if you want to make

1355
02:00:31,440 --> 02:00:35,600
sure they're going to do something, cryptography can bind an AI in such a way that it can't fake

1356
02:00:35,600 --> 02:00:41,120
it. It can't an AI can't mint more Bitcoin, you know, my here's my last question for you. AI discourse

1357
02:00:41,120 --> 02:00:47,040
right now does seem to be polarizing into camps. Obviously, a big way that you think about the world

1358
02:00:47,040 --> 02:00:51,840
is by trying to figure out, you know, what are the different camps? How do they relate to each other

1359
02:00:52,000 --> 02:01:00,880
so on and so forth? I have the view that AI is so weird. And so unlike other things that we've

1360
02:01:00,880 --> 02:01:05,360
encountered in the past, including just like, unlike humans, right, I always say AI, alien

1361
02:01:05,360 --> 02:01:11,680
intelligence, that I feel like it's really important to to borrow a phrase from Paul Graham,

1362
02:01:11,680 --> 02:01:20,240
keep our identities small, and try to have a scout mindset to really just take things on their

1363
02:01:20,240 --> 02:01:24,240
own terms, right? And not necessarily put them through a prism of like, who's team am I on? Or,

1364
02:01:24,240 --> 02:01:30,480
you know, is this benefit my team or hurt the other team or whatever. But you know, just try to be as

1365
02:01:30,480 --> 02:01:37,200
kind of directly engaged with the things themselves as we can without mediating it through all these

1366
02:01:37,200 --> 02:01:43,680
lenses. You know, I think about you mentioned like, the gain of function, right? And I don't know

1367
02:01:43,680 --> 02:01:47,840
for sure what happened, but it certainly does seem like there's a very significant chance

1368
02:01:47,920 --> 02:01:53,120
that it was a lab leak. Certainly there's a long history of lab leaks. But it would be like, you

1369
02:01:53,120 --> 02:01:59,840
know, it would seem to me a failure to say, okay, well, what's the what's the opposite of just having

1370
02:01:59,840 --> 02:02:03,840
like a couple of government labs, like, everybody gets their own gain of function lab, right? Like,

1371
02:02:03,840 --> 02:02:07,680
if we could, and this is kind of what we're doing with AI, we're like, let's compress this power down

1372
02:02:07,680 --> 02:02:12,400
to as small as we can. Let's make a kit that can run in everybody's home. Would we want to

1373
02:02:13,120 --> 02:02:18,880
send out these like gain of function, you know, wet lab research kits to like every home in the

1374
02:02:18,880 --> 02:02:23,600
world and be like, hope you find something interesting, you know, like, let us know if you

1375
02:02:23,600 --> 02:02:28,560
find any new pathogens or hey, maybe you'll find life saving drugs, like whatever, we'll see what

1376
02:02:28,560 --> 02:02:33,520
you find, you know, all eight billion of you. That to me seems like it would be definitely a

1377
02:02:33,520 --> 02:02:39,920
big misstep. And that's the kind of thing that I see coming out of ideologically

1378
02:02:40,880 --> 02:02:45,120
motivated reasoning, or like, you know, tribal reasoning. And so I guess,

1379
02:02:45,120 --> 02:02:52,080
I wonder how you think about the role that tribalism and ideology is playing and should

1380
02:02:52,080 --> 02:02:58,160
or shouldn't play as we try to understand AI. Okay, so first is, you're absolutely right,

1381
02:02:58,160 --> 02:03:05,440
that just because a is bad does not mean that B is good, right? So a could be a bad option,

1382
02:03:05,440 --> 02:03:10,800
B could be a bad option, C could be a bad option. There might be, you have to go down to option G

1383
02:03:10,800 --> 02:03:14,000
before you find a good option, or there might be three good options and seven bad options,

1384
02:03:14,000 --> 02:03:20,880
for example, right? So to map that here, in my view, an extremely bad option is to ask the US

1385
02:03:20,880 --> 02:03:27,040
and Chinese governments to do something. Anything the US government does at the federal level,

1386
02:03:27,040 --> 02:03:32,560
at the state level in blue states at the city level has been a failure. And the way that here's

1387
02:03:32,720 --> 02:03:36,640
here's a metaway of thinking about it, you invest in companies, right? So as an investor,

1388
02:03:36,640 --> 02:03:41,520
here's a really important thing. You might have 10 people who come to you with the same words in

1389
02:03:41,520 --> 02:03:46,880
their pitch. They're all, for example, building social networks. But one of them is Facebook,

1390
02:03:46,880 --> 02:03:51,440
and the others are Friendster and whatever. Okay, and no offense to Friendster, you know,

1391
02:03:51,440 --> 02:03:56,400
that those guys were like, you know, pioneers in their own way. But they just got outmatched by

1392
02:03:56,400 --> 02:04:01,920
Facebook. So point is that the words were the same on each of these packages, but the execution was

1393
02:04:02,000 --> 02:04:09,840
completely different. So could I imagine a highly competent government that could execute and that

1394
02:04:09,840 --> 02:04:15,680
actually did, you know, like, you know, make the right balance of things and so on? I can't say

1395
02:04:15,680 --> 02:04:23,760
it's impossible, but I can't say that it wouldn't be this government. Okay, and so you are talking

1396
02:04:23,760 --> 02:04:28,640
about the words and I'm talking about the substance. The words are, we will protect you from AI,

1397
02:04:28,640 --> 02:04:32,880
right? In my view, the substances, they aren't protecting you from anything, right? You're basically

1398
02:04:32,880 --> 02:04:37,280
giving money and power to a completely incompetent and in fact, malicious organization, which is

1399
02:04:37,280 --> 02:04:42,960
Washington DC, which is the US government that has basically over the last 30 years, gone from a

1400
02:04:42,960 --> 02:04:47,600
hyperpower that wins everywhere without fighting to a declining power that fights everywhere without

1401
02:04:47,600 --> 02:04:53,840
winning. Okay, like just literally burn trillions of dollars doing this, take maybe the greatest

1402
02:04:53,840 --> 02:04:58,480
decline in fortunes in 30 years and maybe human history, not even the Roman Empire went down this

1403
02:04:58,560 --> 02:05:05,440
fast on this many power dimensions this quickly, right? So giving that guy, let's trust him,

1404
02:05:05,440 --> 02:05:09,760
that's just people running an old script in their heads that they inherited. They are not

1405
02:05:09,760 --> 02:05:15,440
thinking about it from first principles that this state is a failure. Okay, and like how much

1406
02:05:15,440 --> 02:05:18,800
of a failure it is, you have to look at sovereign debt crisis, look at graphs that other people

1407
02:05:18,800 --> 02:05:25,360
aren't looking at, but like, you know, the domain of what Blue America can regulate is already

1408
02:05:25,440 --> 02:05:30,640
collapsing because it can't regulate Russia anymore. It can't regulate China anymore.

1409
02:05:30,640 --> 02:05:35,120
It's less able to regulate India. It's less able even to regulate Florida and Texas.

1410
02:05:35,120 --> 02:05:38,960
States are breaking away from it domestically. So this gets to your other point. Why is the

1411
02:05:38,960 --> 02:05:45,200
tribal lens not something that we can put in the back? We have to put in the absolute front

1412
02:05:45,200 --> 02:05:52,160
because the world is retribalizing. Like basically your tribe determines what law you're bound by.

1413
02:05:52,160 --> 02:05:56,960
If you think you can pass some policy that binds the whole world, well, there have to be guys with

1414
02:05:56,960 --> 02:06:02,000
guns who enforce that policy. And if I have guys with guns on their side that say we're not enforcing

1415
02:06:02,000 --> 02:06:06,000
their policy, then you have no policy. You've only bound your own people. Does that make sense,

1416
02:06:06,000 --> 02:06:13,200
right? And so Blue America will probably succeed in choking the life out of AI within Blue America.

1417
02:06:13,200 --> 02:06:18,320
But Blue America controls less and less of the world. So it'll have more power or a fewer people.

1418
02:06:19,280 --> 02:06:25,760
I can go into why this is, but essentially a financial Berlin Wall is arising. There's a lot

1419
02:06:25,760 --> 02:06:31,520
of taxation and regulation and effectively financial repression, de facto confiscation,

1420
02:06:31,520 --> 02:06:35,440
that will have to happen for the level of debt service that the US has been taking on.

1421
02:06:36,560 --> 02:06:41,520
There's one graph just to make the point. And if you want to dig into this, you can.

1422
02:06:42,400 --> 02:06:47,440
But the reason this impacts things is when you're talking about AI safety, you're talking about

1423
02:06:47,440 --> 02:06:52,480
AI regulation, you're talking about the US government, right? And you have to ask, what does

1424
02:06:52,480 --> 02:06:58,640
that actually mean? And it's like, in my view, it's like asking the Soviet Union in 1989 to

1425
02:06:58,640 --> 02:07:03,520
regulate the internet, right? That's going to outlive, you know, the country. US interest

1426
02:07:03,520 --> 02:07:07,760
payment on federal debt versus defense spending. The white line is defense spending. Look at the

1427
02:07:07,760 --> 02:07:12,880
red line. That's just gone absolutely vertical. That's interest. And it's going to go more vertical

1428
02:07:12,880 --> 02:07:18,400
next year because all of this debt is getting refinanced at much higher interest rates. That's

1429
02:07:18,400 --> 02:07:24,000
why look at this, you want AI timelines, right? The question for me is DC's timeline.

1430
02:07:24,560 --> 02:07:32,000
What is DC's time left to live? Okay, this is the kind of thing that kills empires. And you either

1431
02:07:32,000 --> 02:07:37,520
have this just go to the absolute moon, or they cut rates and they print a lot. And either way,

1432
02:07:38,080 --> 02:07:43,440
you know, the fundamental assumption underpinning all the AI safety, all the AI regulation work

1433
02:07:43,440 --> 02:07:48,640
is that they have a functional golem in Washington DC, where if they convince it to do something,

1434
02:07:48,640 --> 02:07:53,120
it has enough power to control enough of the world. When that assumption is broken,

1435
02:07:54,400 --> 02:07:59,760
then a lot of assumptions are broken, right? And so in my view, you have to,

1436
02:08:01,040 --> 02:08:06,960
you must think about a polytheistic AI world, because other tribes are already into this,

1437
02:08:07,040 --> 02:08:11,120
they're already funding their own, right? The proliferation is already happening.

1438
02:08:11,840 --> 02:08:16,640
And they're not going to bow to blue tribe. So that's why I think the tribal lens is not

1439
02:08:16,640 --> 02:08:21,840
secondary. It's not some, you know, totally separate thing. It is an absolutely primary

1440
02:08:21,840 --> 02:08:26,080
way in which to look at this. And in a sense, it's almost like a, you know, in a well done

1441
02:08:26,800 --> 02:08:34,960
movie, all the plot lines come together at the end. Okay. And all the disruptions that are happening,

1442
02:08:34,960 --> 02:08:40,640
the China disruption, the rise of India, the rise of the internet, the rise of crypto,

1443
02:08:40,640 --> 02:08:44,960
the rise of AI, and the decline of DC, and the internal political conflict,

1444
02:08:45,520 --> 02:08:49,920
and, you know, various other theaters like what's happening in Europe and, you know, and Middle

1445
02:08:49,920 --> 02:08:55,840
East, all of those come together into a crescendo of, ah, there's a lot of those graphs are all

1446
02:08:55,840 --> 02:09:00,400
having the same time. And it's not something you can analyze by just, I think, looking at one of

1447
02:09:00,400 --> 02:09:04,880
these curves on itself. I think that's a great note to wrap on. I am always lamenting the fact

1448
02:09:05,120 --> 02:09:12,240
that so many people are thinking about this AI moment in just fundamentally too small

1449
02:09:12,240 --> 02:09:18,560
of terms. But I don't think you're one that will easily be accused of that. So with

1450
02:09:19,600 --> 02:09:24,480
an invitation to come back and continue in the not too distant future, for now, I will say

1451
02:09:24,480 --> 02:09:28,320
Balaji Srinivasan, thank you for being part of the Cognitive Revolution.

1452
02:09:29,360 --> 02:09:31,040
Thank you, Nathan. Good to be here.

