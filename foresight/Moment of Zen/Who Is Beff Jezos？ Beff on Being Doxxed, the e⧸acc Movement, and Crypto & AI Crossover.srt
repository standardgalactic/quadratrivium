1
00:00:00,000 --> 00:00:10,280
Hey everyone, today on Moment of Zen, we chat with Beth Jayzos, the co-founder of the EAC

2
00:00:10,280 --> 00:00:14,880
movement, or Effective Accelerationism, who was doxxed on Friday night by Forbes reporters.

3
00:00:14,880 --> 00:00:17,120
We'll link to some articles about it below.

4
00:00:17,120 --> 00:00:21,320
Up ahead, we get the inside story from Beth, whose real name is Guillaume Verdome.

5
00:00:21,320 --> 00:00:25,680
We also discuss anonymity, the media, and the role of journalism in our society.

6
00:00:25,680 --> 00:00:30,320
Also joining the conversation is Baselord, Beth's EAC co-founder, who remains anonymous,

7
00:00:30,320 --> 00:00:33,720
and Nathan LeBenz, who hosts the Cognitive Revolution podcast.

8
00:00:33,720 --> 00:00:35,720
Quick disclaimer, and then we'll get to the show.

9
00:00:35,720 --> 00:00:40,400
I'm an investor in XTropic, the stealth AI hardware startup that Guillaume founded.

10
00:00:40,400 --> 00:00:41,960
Here's Moment of Zen with Beth.

11
00:00:41,960 --> 00:00:42,960
Yo, yo.

12
00:00:42,960 --> 00:00:47,400
What do we got going on here, Steve Jobs?

13
00:00:47,400 --> 00:00:52,800
Well, you know, I got to switch up the look, keep the commenters chirping.

14
00:00:52,800 --> 00:00:53,800
Yeah.

15
00:00:53,800 --> 00:00:55,800
A lot of people hop in.

16
00:00:55,800 --> 00:01:02,880
So, Somali Pirate to, you know, confident design partner, I don't know, like what's

17
00:01:02,880 --> 00:01:04,880
going on here.

18
00:01:04,880 --> 00:01:06,200
Yeah, exactly.

19
00:01:06,200 --> 00:01:07,200
Just testing it out.

20
00:01:07,200 --> 00:01:10,040
Did you get a better webcam, too?

21
00:01:10,040 --> 00:01:11,040
No.

22
00:01:11,040 --> 00:01:13,040
Oh, okay.

23
00:01:13,040 --> 00:01:16,480
But, but I'm going to.

24
00:01:16,480 --> 00:01:18,640
Is the Binance topic like a five minute topic?

25
00:01:18,640 --> 00:01:21,160
I mean, I can give an update on it if you want.

26
00:01:21,520 --> 00:01:22,520
Yeah.

27
00:01:22,520 --> 00:01:23,520
Well, I'll start that.

28
00:01:23,520 --> 00:01:24,520
You're giving an update on the Binance topic.

29
00:01:24,520 --> 00:01:30,920
Well, I mean, it's a little bit old news at this point, but we had a major settlement

30
00:01:30,920 --> 00:01:39,600
that Binance, the CEO of Binance CZ, who, he, you know, kind of kicked off the SBF issue

31
00:01:39,600 --> 00:01:41,760
and is big of a name in crypto.

32
00:01:41,760 --> 00:01:47,400
I mean, it's CZ, SBF, and Brian Armstrong were the three big names during the last cycle,

33
00:01:47,400 --> 00:01:51,800
and SBF has now been convicted.

34
00:01:51,800 --> 00:01:57,680
And CZ, there were always kind of rumors that there was like a big DOJ investigation.

35
00:01:57,680 --> 00:02:00,760
Then there was an SEC suit that happened this year.

36
00:02:00,760 --> 00:02:07,560
But in terms of the real criminal stuff, that had always been swirling and a lot of allegations

37
00:02:07,560 --> 00:02:10,120
around sanctions.

38
00:02:10,120 --> 00:02:16,000
And remember, sanctions are a U.S. tool because the world is run on dollars.

39
00:02:16,000 --> 00:02:22,560
So the U.S. can basically say, hey, you can't send dollars or anything crypto to these specific

40
00:02:22,560 --> 00:02:29,120
countries or individuals, and the allegations were that Binance was facilitating that.

41
00:02:29,120 --> 00:02:34,040
Now, I actually don't even know the details specifically whether or not they did, to what

42
00:02:34,040 --> 00:02:44,480
extent, but what came out was, and the settlement was CZ is stepping back as CEO.

43
00:02:44,840 --> 00:02:53,800
And I think the record fines for sanctions have been as a result of violating sanctions

44
00:02:53,800 --> 00:02:54,800
with Iran.

45
00:02:54,800 --> 00:02:59,520
I want to say it was like BNP Paribas, which is a French bank, paid something like $8 billion,

46
00:02:59,520 --> 00:03:02,960
or maybe it was HSBC, I forget.

47
00:03:02,960 --> 00:03:07,760
And then I think another big set of fines, HSBC might have been the drug cartels, but

48
00:03:07,760 --> 00:03:13,120
basically it's like Iran, which is a geopolitical issue, and then the drug cartels, if you do

49
00:03:13,120 --> 00:03:17,760
money laundering, you pay major fines and there's criminal liability.

50
00:03:17,760 --> 00:03:20,680
So the settlement was he steps back.

51
00:03:20,680 --> 00:03:22,760
Binance can still exist, which I'm actually surprised.

52
00:03:22,760 --> 00:03:27,080
I think he's paying something on the order of like $50 million fine.

53
00:03:27,080 --> 00:03:33,520
And the thing I didn't quite get in the first day of the news, but then it turned out later

54
00:03:33,520 --> 00:03:37,360
kind of seemed to bubble up is he's still in the U.S., I think, and actually may have

55
00:03:37,360 --> 00:03:42,440
to do some jail time, which is like a pretty big deal.

56
00:03:42,440 --> 00:03:43,960
Because he's voluntarily given it up.

57
00:03:43,960 --> 00:03:51,160
So you know, if he's willing to step back, pay $4 billion fine, $50 million personally,

58
00:03:51,160 --> 00:03:55,840
and potentially do jail time in the U.S., that means they were going to throw a huge

59
00:03:55,840 --> 00:03:59,240
amount of the whole book at him.

60
00:03:59,240 --> 00:04:04,800
But what's interesting is the SEC is not done with their case, which I think is a civil

61
00:04:04,800 --> 00:04:05,800
case.

62
00:04:05,800 --> 00:04:10,200
So that will still be another big hit on Binance if they have to settle it.

63
00:04:11,200 --> 00:04:12,920
Obviously, you don't have the founder anymore.

64
00:04:12,920 --> 00:04:18,640
If he has to do jail time, it can't even be, you know, indirectly influencing.

65
00:04:18,640 --> 00:04:22,000
And then I would imagine the European Union and every other jurisdiction that's kind

66
00:04:22,000 --> 00:04:25,880
of within the global U.S. sphere is going to say, well, we don't want to look like we

67
00:04:25,880 --> 00:04:27,480
were not tough on these guys.

68
00:04:27,480 --> 00:04:30,320
And so these guys are going to be paying a lot of money.

69
00:04:30,320 --> 00:04:35,480
So I'm curious to see if they end up existing, you know, five years from now.

70
00:04:35,480 --> 00:04:37,600
I mean, five years in crypto is a long time.

71
00:04:37,840 --> 00:04:43,480
But basically Binance popped up during, you know, call it 2017 era when there was another

72
00:04:43,480 --> 00:04:48,320
exchange called PoloniEx, which was actually based in the U.S., but they were only serving

73
00:04:48,320 --> 00:04:55,160
international customers in theory, that they knocked PoloniEx off the top spot and never

74
00:04:55,160 --> 00:04:56,160
kind of looked back.

75
00:04:56,160 --> 00:04:59,480
And SBF was gunning to try to be the next Binance.

76
00:04:59,480 --> 00:05:05,120
So what's left in the wake of this is, you know, Coinbase, which I think got a lot of

77
00:05:05,160 --> 00:05:11,840
ridicule from, you know, the mid-wit, Finn Twitter, like Bukko Capital and all these,

78
00:05:11,840 --> 00:05:16,960
like, kind of a non-accounts that are too afraid to put their name next to an account.

79
00:05:16,960 --> 00:05:22,240
So they like to make fun of, you know, entrepreneurs on the sidelines when obviously everything

80
00:05:22,240 --> 00:05:27,520
corrected last year, they were all like Coinbase is not, you know, like a real business.

81
00:05:27,520 --> 00:05:32,560
I mean, Bukko Capital was calling Coinbase a cancer as of last week because crypto prices

82
00:05:32,560 --> 00:05:33,560
are pumping.

83
00:05:33,560 --> 00:05:38,360
But Coinbase actually is the only one that's been following the law and didn't have a crazy

84
00:05:38,360 --> 00:05:39,360
risk program.

85
00:05:39,360 --> 00:05:45,200
So FTX disappears, Binance disappears, Gemini basically was doing a bunch of shady stuff

86
00:05:45,200 --> 00:05:47,680
and blew up as a result of that.

87
00:05:47,680 --> 00:05:49,440
What was the other one?

88
00:05:49,440 --> 00:05:50,840
BlockFi, right?

89
00:05:50,840 --> 00:05:51,960
Like, there's a pretty famous meme.

90
00:05:51,960 --> 00:05:56,320
I like pump, but pump kind of was like pumping them and it's like to the moon and like this.

91
00:05:56,320 --> 00:05:59,560
And obviously that was fraudulent.

92
00:05:59,560 --> 00:06:02,120
And so it turns out like Coinbase is the only one that was legit.

93
00:06:02,120 --> 00:06:06,320
And yes, like that's been the MO of the company the whole time.

94
00:06:06,320 --> 00:06:09,480
And so finally with the government cleaning up the space, it's like now Coinbase is sitting,

95
00:06:09,480 --> 00:06:12,720
you know, Coinbase, Bitcoin just crossed 40,000.

96
00:06:12,720 --> 00:06:18,360
And Coinbase is basically the only exchange with like meaningful fiat rails and customer

97
00:06:18,360 --> 00:06:19,360
base.

98
00:06:19,360 --> 00:06:22,000
I mean, there are overseas exchanges that will take place in Binance.

99
00:06:22,000 --> 00:06:25,720
But it's a huge vindication, I think, for Brian Armstrong, who's taken a lot of arrows

100
00:06:25,720 --> 00:06:28,400
for a lot of different things.

101
00:06:28,400 --> 00:06:30,040
I'm also happy about the share price personally.

102
00:06:31,040 --> 00:06:32,360
Yeah, totally.

103
00:06:32,360 --> 00:06:34,360
You're hopefully incentivized.

104
00:06:34,360 --> 00:06:36,480
Yeah, let's be clear.

105
00:06:36,480 --> 00:06:38,640
I'm extremely biased here.

106
00:06:38,640 --> 00:06:42,240
But what I would say is like, if you want me to give you like the counterpoint, I do

107
00:06:42,240 --> 00:06:50,320
think Binance pushed the industry forward, Modulo put the illegal stuff off the side.

108
00:06:50,320 --> 00:06:56,400
They never had a major hack, which is usually how a lot of these exchanges go down.

109
00:06:56,440 --> 00:07:00,960
And to the degree that I understand it as a result of the DOJ, it wasn't misappropriating

110
00:07:00,960 --> 00:07:02,560
customer funds, right?

111
00:07:02,560 --> 00:07:06,880
So now the SEC may come and say something different, but big difference between SPF,

112
00:07:06,880 --> 00:07:08,640
which was actual fraud, right?

113
00:07:08,640 --> 00:07:12,000
Like you was taking customer funds, which you're not supposed to touch, and was using

114
00:07:12,000 --> 00:07:16,960
it to, you know, pay Tom Brady and donate to political campaigns.

115
00:07:16,960 --> 00:07:21,360
So I think Binance was an honest actor as it relates to customers.

116
00:07:21,360 --> 00:07:25,120
I think they just seems like they were a little fast and loose with the, you know, the international

117
00:07:25,120 --> 00:07:27,760
sanctions regime.

118
00:07:27,760 --> 00:07:28,760
Perfect timing.

119
00:07:28,760 --> 00:07:34,680
Fast and loose with the international sanctions regime, and Antonio hops on to the call.

120
00:07:34,680 --> 00:07:35,680
Who sanctions?

121
00:07:35,680 --> 00:07:36,680
What regime?

122
00:07:36,680 --> 00:07:38,440
Yeah, that's actually a good one.

123
00:07:38,440 --> 00:07:43,920
We know a few people who would like to argue that US sanctions are not valid, but the reality

124
00:07:43,920 --> 00:07:47,880
is if you live in a dollar denominated world, you got to pay attention to those sanctions

125
00:07:47,880 --> 00:07:50,480
because you can go to jail if you violate them.

126
00:07:50,480 --> 00:07:53,880
We were talking about Binance.

127
00:07:53,880 --> 00:07:56,400
Where is the man of the hour?

128
00:07:56,400 --> 00:07:58,760
He's having trouble getting in.

129
00:07:58,760 --> 00:08:01,680
I've never seen this issue before.

130
00:08:01,680 --> 00:08:07,640
Maybe he's using some like futuristic quantum web browser that doesn't have support for

131
00:08:07,640 --> 00:08:10,680
lowly riverside web RTC.

132
00:08:10,680 --> 00:08:15,000
Or Emily Baker White put some weird malware that she used to like sample his voice to

133
00:08:15,000 --> 00:08:16,000
out him.

134
00:08:16,000 --> 00:08:17,000
And it's still like on the machine.

135
00:08:17,000 --> 00:08:18,000
Did they add him on our podcast?

136
00:08:18,000 --> 00:08:19,000
Yeah.

137
00:08:19,000 --> 00:08:24,480
I find that this wasn't actually MBS in collaboration with Forbes, right?

138
00:08:24,480 --> 00:08:28,240
Hey, Beth, how are you?

139
00:08:28,240 --> 00:08:31,520
You guys know the reference, right?

140
00:08:31,520 --> 00:08:33,920
No, what?

141
00:08:33,920 --> 00:08:34,920
MBS?

142
00:08:34,920 --> 00:08:40,800
It's still not confirmed, and I think it's probably fake, but it's also maybe real.

143
00:08:40,800 --> 00:08:48,240
Bezos' marriage got broken up as a result of MBS sending spyware in WhatsApp to Jeff.

144
00:08:48,240 --> 00:08:53,280
And the text messages in question were, hi, Jeff, how are you?

145
00:08:53,280 --> 00:08:54,680
This is MBS.

146
00:08:54,680 --> 00:09:00,480
Well, it was probably good for him and bad for American cities everywhere, but yeah,

147
00:09:00,480 --> 00:09:01,480
exactly.

148
00:09:01,480 --> 00:09:02,480
Exactly.

149
00:09:02,480 --> 00:09:03,480
He got divorced.

150
00:09:03,480 --> 00:09:04,480
I don't know.

151
00:09:04,480 --> 00:09:07,920
Lauren Sanchez, she's pretty happy, right?

152
00:09:07,920 --> 00:09:15,000
And she's now on the front of the yacht that they just finished, because Chris Sailing

153
00:09:15,000 --> 00:09:17,840
Antonio is such a blue collar thing, right?

154
00:09:18,440 --> 00:09:19,440
He's not sailing.

155
00:09:19,440 --> 00:09:20,440
He's motoring.

156
00:09:20,440 --> 00:09:21,440
Motoring is totally different.

157
00:09:21,440 --> 00:09:22,940
Doesn't he have a sailboat?

158
00:09:22,940 --> 00:09:24,440
Isn't it a big sailing yacht?

159
00:09:24,440 --> 00:09:26,080
I don't know.

160
00:09:26,080 --> 00:09:29,520
The deck photos I've seen are clearly a large motor yacht type thing.

161
00:09:29,520 --> 00:09:31,680
Someone, we should put it in the YouTube comments.

162
00:09:31,680 --> 00:09:37,760
The pictures of Jeff Bezos' boat, my understanding is that there are sails, but it could be.

163
00:09:37,760 --> 00:09:40,360
All right, we got it.

164
00:09:40,360 --> 00:09:45,440
You guys have the most interesting guests on the podcast the one time I can't make it.

165
00:09:45,440 --> 00:09:46,440
Oh, don't worry.

166
00:09:46,840 --> 00:09:47,840
Oh, Dominic Cummings.

167
00:09:47,840 --> 00:09:48,840
Yeah.

168
00:09:51,840 --> 00:09:55,240
He sounded super pumped to like Rijig or San Francisco, I have to say, which was interesting.

169
00:09:55,240 --> 00:09:56,240
Yeah.

170
00:09:56,240 --> 00:09:57,240
I don't know.

171
00:09:57,240 --> 00:10:00,240
I think saving SF is kind of cope.

172
00:10:00,240 --> 00:10:01,240
You think it's unsavable?

173
00:10:01,240 --> 00:10:03,240
I mean, are you running, Eric?

174
00:10:03,240 --> 00:10:04,240
Sir.

175
00:10:04,240 --> 00:10:06,240
Hey, everyone.

176
00:10:06,240 --> 00:10:07,240
Welcome.

177
00:10:07,240 --> 00:10:10,240
Welcome to FACE Twitter.

178
00:10:10,240 --> 00:10:11,240
FACE Doxxed World.

179
00:10:11,240 --> 00:10:15,240
Well, hello, world.

180
00:10:15,240 --> 00:10:19,140
I see the Eigenfunction on your quantum computer has clapped into the state in which you can

181
00:10:19,140 --> 00:10:24,840
join the Riverside and some other worlds you were in fact still struggling, but Schrodinger's

182
00:10:24,840 --> 00:10:25,840
Riverside episode.

183
00:10:25,840 --> 00:10:26,840
Yeah.

184
00:10:26,840 --> 00:10:27,840
Yeah.

185
00:10:27,840 --> 00:10:28,840
Yeah.

186
00:10:28,840 --> 00:10:32,040
We need to upgrade the quantum computers to use this wonderful app.

187
00:10:32,040 --> 00:10:39,040
Speaking of which, what is that menacing blinking box behind you over your right shoulder?

188
00:10:39,040 --> 00:10:40,640
That's just, don't worry about that.

189
00:10:40,640 --> 00:10:43,640
Don't worry about that.

190
00:10:44,640 --> 00:10:45,640
Yeah.

191
00:10:45,640 --> 00:10:49,640
No, I'm in EAC HQ or Extropic HQ.

192
00:10:49,640 --> 00:10:55,740
I've got our banner in the back and of course I've got some Dr. Pepper and some ketones

193
00:10:55,740 --> 00:10:57,940
back there.

194
00:10:57,940 --> 00:10:58,940
Very memetic.

195
00:10:58,940 --> 00:11:02,440
But I guess we're recording right now, right?

196
00:11:02,440 --> 00:11:03,440
We're recording.

197
00:11:03,440 --> 00:11:04,440
We're watching it.

198
00:11:04,440 --> 00:11:05,440
Okay.

199
00:11:05,440 --> 00:11:06,440
Wonderful.

200
00:11:06,440 --> 00:11:07,440
Eric doesn't read you your Miranda rights.

201
00:11:07,440 --> 00:11:08,440
You just go right into it.

202
00:11:08,440 --> 00:11:09,440
It's like a cold.

203
00:11:09,440 --> 00:11:10,440
Okay.

204
00:11:10,440 --> 00:11:11,440
Got it.

205
00:11:11,440 --> 00:11:12,440
Let's go.

206
00:11:12,440 --> 00:11:16,640
Well, super happy to be here.

207
00:11:16,640 --> 00:11:18,240
It definitely feels weird.

208
00:11:18,240 --> 00:11:20,840
It definitely feels very weird.

209
00:11:20,840 --> 00:11:25,600
It's been kind of a compartmentalized part of my life, you know, felt like a video game

210
00:11:25,600 --> 00:11:27,400
on my phone more or less, right?

211
00:11:27,400 --> 00:11:32,640
Like having an alternative Twitter and having a very professional life day to day, you would

212
00:11:32,640 --> 00:11:39,240
never guess that I'm back from just like day to day interactions, frankly.

213
00:11:39,240 --> 00:11:43,080
And now the two have kind of bled into one another and they are one.

214
00:11:43,080 --> 00:11:46,440
And so I'm still pricing that in mentally, frankly.

215
00:11:46,440 --> 00:11:53,240
So it's been an interesting couple of days, obviously, a lot of inbound, more DMs I can

216
00:11:53,240 --> 00:11:55,740
ever count.

217
00:11:55,740 --> 00:12:00,240
And yeah, I mean, it's been a very surprising reaction.

218
00:12:00,240 --> 00:12:02,760
Well, I guess not that surprising.

219
00:12:02,760 --> 00:12:07,600
I guess like, you know, tech Twitter kind of, kind of hates the establishment media

220
00:12:07,600 --> 00:12:13,640
at this point and, you know, came to my defense, which, which felt, felt quite good.

221
00:12:13,640 --> 00:12:21,400
But you know, I'm happy to go into any part of it, you know, how it went down, you know,

222
00:12:21,400 --> 00:12:27,600
the why I started Beth, anonymity, and anything, the startup, let me know how you want to

223
00:12:27,600 --> 00:12:28,600
structure this.

224
00:12:28,600 --> 00:12:29,600
This.

225
00:12:29,600 --> 00:12:30,600
Yeah.

226
00:12:30,640 --> 00:12:34,480
With the sorted details of how they figured it out and what voice they sampled and how

227
00:12:34,480 --> 00:12:37,680
that whole sorted tail played out.

228
00:12:37,680 --> 00:12:38,680
Yeah.

229
00:12:38,680 --> 00:12:45,000
I mean, there was a couple instances where there's reporters that, you know, think they

230
00:12:45,000 --> 00:12:51,480
have my docs and I'm just like, you know, you can't, you can't dox people, you know,

231
00:12:51,480 --> 00:12:53,800
I do not want you to do this.

232
00:12:53,800 --> 00:12:58,160
And usually that kind of kills the story to some extent, but you know, it's kind of been,

233
00:12:58,160 --> 00:13:01,480
you know, I've gone, I've been in the SF social scene in person.

234
00:13:01,480 --> 00:13:05,560
I know a bunch of investors, for example, they, some of them know some of them have correlated

235
00:13:05,560 --> 00:13:06,560
the two.

236
00:13:06,560 --> 00:13:10,160
Um, but obviously I, I, I spent a bunch of time in Twitter spaces.

237
00:13:10,160 --> 00:13:15,600
I've had, I have some online lectures from my days in grad school and quantum computing.

238
00:13:15,600 --> 00:13:19,160
And really that's basically what they've, what they've correlated.

239
00:13:19,160 --> 00:13:26,160
Uh, they, they, they use some, I don't know, I call it CIA technology as a joke, but, uh,

240
00:13:26,160 --> 00:13:32,280
you know, they use some government grade technology to identify my voice, uh, you know, just because

241
00:13:32,280 --> 00:13:36,040
I have a 50 K follower account, uh, that speaks truth to power.

242
00:13:36,040 --> 00:13:41,000
I guess, I guess it tells you that, you know, we're doing something right and that some

243
00:13:41,000 --> 00:13:47,120
people in the establishment, you know, want to have leverage over the leader of the grassroots

244
00:13:47,120 --> 00:13:54,760
movement, uh, that, you know, is for freedom and, and, and against top down control, which

245
00:13:54,880 --> 00:13:55,880
is very scary.

246
00:13:55,880 --> 00:13:56,880
Cause that's what they like.

247
00:13:56,880 --> 00:13:57,880
Right.

248
00:13:57,880 --> 00:14:02,720
So, Hey, we'll continue our interview in a moment after a word from our sponsors.

249
00:14:02,720 --> 00:14:03,720
Real quick.

250
00:14:03,720 --> 00:14:07,680
What's the easiest choice you can make taking the window instead of the middle seat outsourcing

251
00:14:07,680 --> 00:14:10,280
business tasks that you absolutely hate.

252
00:14:10,280 --> 00:14:13,920
What about selling with Shopify?

253
00:14:13,920 --> 00:14:18,680
Shopify is the global commerce platform that helps you sell at every stage of your business.

254
00:14:18,680 --> 00:14:23,400
Shopify powers 10% of all e-commerce in the U S and Shopify is the global force behind

255
00:14:23,440 --> 00:14:28,760
all birds, Rothy's and Brooklyn and millions of other entrepreneurs of every size across

256
00:14:28,760 --> 00:14:31,280
175 countries.

257
00:14:31,280 --> 00:14:34,640
Whether you're selling security systems or marketing memory modules, Shopify helps you

258
00:14:34,640 --> 00:14:40,000
sell everywhere from their all in one e-commerce platform to their in person POS system, wherever

259
00:14:40,000 --> 00:14:43,040
and whatever you're selling, Shopify's got you covered.

260
00:14:43,040 --> 00:14:45,560
I've used it in the past at the companies I've founded.

261
00:14:45,560 --> 00:14:50,480
And when we launch merch here at Turpentine, Shopify will be our go to Shopify helps turn

262
00:14:50,520 --> 00:14:55,480
browsers into buyers with the internet's best converting checkout up to 36% better compared

263
00:14:55,480 --> 00:14:57,520
to other leading commerce platforms.

264
00:14:57,520 --> 00:15:01,960
And Shopify helps you sell more with less effort thanks to Shopify magic, your AI powered

265
00:15:01,960 --> 00:15:07,760
all star with Shopify magic whip up captivating content that converts from blog posts to product

266
00:15:07,760 --> 00:15:14,080
descriptions, generate instant FAQ answers, pick the perfect email, send time, plus Shopify

267
00:15:14,080 --> 00:15:20,000
magic is free for every Shopify seller businesses that grow, grow with Shopify, sign up for

268
00:15:20,040 --> 00:15:25,800
a $1 per month trial period at Shopify.com slash moment of Zen, go to Shopify.com slash

269
00:15:25,800 --> 00:15:30,440
moment of Zen now to grow your business no matter what stage you're in Shopify.com slash

270
00:15:30,440 --> 00:15:31,040
moment of Zen.

271
00:15:33,480 --> 00:15:36,520
You spoke truth to power and now they're going to try to speak power to truth, so to speak.

272
00:15:36,720 --> 00:15:41,280
But I'm curious, someone must have, they must have been tipped off because they didn't do an

273
00:15:41,280 --> 00:15:42,320
all against all voice sample.

274
00:15:42,320 --> 00:15:45,400
They, they started after this pair wise interaction, right?

275
00:15:45,400 --> 00:15:47,040
Because there was nothing online that would have suggested it.

276
00:15:47,240 --> 00:15:49,080
And then they confirmed it via that, I imagine.

277
00:15:49,800 --> 00:15:55,400
Yeah, so see, they must have like asked around, you know, I mean, it's kind of like a people

278
00:15:55,400 --> 00:15:57,440
know other people's docs is right.

279
00:15:57,440 --> 00:16:02,480
Like there's kind of Twitter parties in person and people go by their alternative name.

280
00:16:02,480 --> 00:16:06,480
So they know your face and eventually they correlate things, but there's kind of a, yeah,

281
00:16:06,480 --> 00:16:09,760
there's an unwritten rule of like, you don't, you don't like talk to reporters.

282
00:16:09,760 --> 00:16:12,800
You don't share people's docs with other consent.

283
00:16:13,160 --> 00:16:17,880
I'm sure some people, you know, broke that rule, but you know, I don't know what drove

284
00:16:17,880 --> 00:16:19,200
them to really break the story.

285
00:16:19,200 --> 00:16:22,240
Now, I think there were some latent variables there.

286
00:16:23,560 --> 00:16:29,160
But yeah, essentially, I think it was on Thursday, I get a text from some of my investors.

287
00:16:29,680 --> 00:16:32,040
They identified some of my investors.

288
00:16:32,200 --> 00:16:35,200
So this podcast is going to drop after we announced the round.

289
00:16:35,200 --> 00:16:37,800
So I'll just mention investors.

290
00:16:38,280 --> 00:16:44,280
But yes, so some of our big investors get a text like, Hey, I think this, you know, over

291
00:16:44,280 --> 00:16:49,040
this first reporter, I think it was Conrad has started correlating your identity with

292
00:16:49,040 --> 00:16:53,520
Beth, they didn't correlate all the company because we had a, you know, I had a company

293
00:16:53,520 --> 00:16:58,280
change the name, now we're extrapik, they hadn't correlated everything perfectly, but

294
00:16:58,280 --> 00:17:00,840
then the censor fuse the cross reporters internally.

295
00:17:01,080 --> 00:17:05,160
So they didn't have enough to ship it on Thursday, but then on Friday, a different

296
00:17:05,160 --> 00:17:10,360
reporter, Emily joined force, the filings, the track name changes.

297
00:17:11,360 --> 00:17:20,240
They went to my personal Facebook to, you know, to identify a photo I shared like on

298
00:17:20,240 --> 00:17:25,800
my friend, you know, friends only Instagram of the party, you know, when we were on stage

299
00:17:25,800 --> 00:17:27,120
with Grimes and stuff like that.

300
00:17:28,320 --> 00:17:30,160
And they correlated everything, right?

301
00:17:30,160 --> 00:17:34,440
And so they just had me in this sort of checkmate, like we have this, this, this, this, this,

302
00:17:34,440 --> 00:17:36,560
this, talk to us, we're going to ship it.

303
00:17:37,200 --> 00:17:39,760
And I was like, all right, I got to get in front of this, right?

304
00:17:39,800 --> 00:17:44,960
Again, you know, I've been doing this deep tech startup for nearly a year and a half.

305
00:17:45,000 --> 00:17:51,240
And, you know, as, as the bio says, I come from Google X, you know, we're taught to

306
00:17:51,240 --> 00:17:55,800
be very secretive about what we do because in deep tech, that's kind of the, the MO.

307
00:17:56,880 --> 00:17:59,680
And, you know, we want to be secretive for, for longer.

308
00:18:00,440 --> 00:18:05,880
But for all sorts of reasons, including national security interest reasons, right?

309
00:18:05,920 --> 00:18:08,520
Like our technology is pretty out there.

310
00:18:09,480 --> 00:18:11,400
And it kind of forced our hand, right?

311
00:18:11,440 --> 00:18:13,200
Like it correlated all identities.

312
00:18:14,120 --> 00:18:19,640
And, and, and also it also kind of doxed the fact that I had founded the startup because

313
00:18:19,640 --> 00:18:22,640
I was kind of, I hadn't identified that on my main account.

314
00:18:22,640 --> 00:18:27,920
So for me, it was kind of a, a mo, you know, a moment of panic, because, you know, I want

315
00:18:27,920 --> 00:18:29,680
to do what's right for my company, right?

316
00:18:29,680 --> 00:18:31,480
I've been working really hard on this company.

317
00:18:32,320 --> 00:18:35,000
We were planning announcements in a couple of months from now.

318
00:18:35,920 --> 00:18:37,640
And now we have to rush everything, right?

319
00:18:38,440 --> 00:18:43,160
Which is, which is not great, but what I ended up doing, getting on the phone, getting

320
00:18:43,160 --> 00:18:47,600
in from the story, I already knew exactly, like that's the thing with reporters.

321
00:18:47,600 --> 00:18:49,400
Like they're so low entropy.

322
00:18:49,440 --> 00:18:51,640
They are all in the same typical subspace of stories.

323
00:18:51,640 --> 00:18:55,000
You can predict exactly what they're going to write just from the prior that they're

324
00:18:55,000 --> 00:18:55,760
trying to get you.

325
00:18:56,000 --> 00:19:01,640
And so I just disarmed every typical attack they would try to do on EAC trying to have

326
00:19:01,640 --> 00:19:07,320
second or third order guilt by association to some idea that's a derivative and another

327
00:19:07,320 --> 00:19:11,320
idea and neutralize that, essentially entirely.

328
00:19:11,760 --> 00:19:15,000
Um, I see we have Bayes Lord joining.

329
00:19:15,440 --> 00:19:15,960
There you go.

330
00:19:18,360 --> 00:19:19,400
Uh, what's up, Bayes?

331
00:19:19,520 --> 00:19:20,160
Can you guys hear me?

332
00:19:20,280 --> 00:19:20,640
What's up?

333
00:19:20,680 --> 00:19:20,960
Yeah.

334
00:19:21,000 --> 00:19:21,280
Yeah.

335
00:19:21,280 --> 00:19:21,880
Yeah, we can.

336
00:19:22,200 --> 00:19:22,560
Awesome.

337
00:19:22,800 --> 00:19:24,000
Oh, we have Nathan as well.

338
00:19:24,080 --> 00:19:25,280
Oh, let's get, all right.

339
00:19:25,280 --> 00:19:25,800
Here we go.

340
00:19:26,120 --> 00:19:29,320
What's up, what's up, what's up?

341
00:19:29,360 --> 00:19:29,520
Yeah.

342
00:19:29,520 --> 00:19:29,960
Good to see you.

343
00:19:30,000 --> 00:19:30,360
Not bad.

344
00:19:30,880 --> 00:19:31,720
Great to see you.

345
00:19:33,040 --> 00:19:36,480
I guess now, uh, with real names, uh, in my case, so.

346
00:19:37,480 --> 00:19:37,960
Yeah.

347
00:19:38,360 --> 00:19:45,000
Um, so, so yeah, it's been an interesting, uh, you know, I guess 48 hours, uh, dealing

348
00:19:45,000 --> 00:19:51,160
with this, um, I think overall, uh, I mean, we can get into why I was and on, uh, for,

349
00:19:51,160 --> 00:19:52,240
for various reasons.

350
00:19:52,640 --> 00:19:58,960
Um, but overall it seems like it was positive to the point that where, uh, some are like

351
00:19:58,960 --> 00:20:04,400
have a conspiracy theory that this was planted, but, uh, I had, I had that theory.

352
00:20:04,760 --> 00:20:08,280
I thought, I thought we're all getting conned and you were actually dropping this.

353
00:20:08,440 --> 00:20:09,280
Cause that's when that's a question.

354
00:20:09,280 --> 00:20:12,800
Do you think you would raise on a higher or lower valuation now than you did this round?

355
00:20:13,120 --> 00:20:14,920
I would bet the valuation grow up actually.

356
00:20:15,320 --> 00:20:16,440
Definitely higher.

357
00:20:16,640 --> 00:20:17,560
Yeah, exactly.

358
00:20:17,640 --> 00:20:17,960
Right.

359
00:20:18,040 --> 00:20:24,520
But I mean, I didn't, I want to raise round, uh, you know, raise around just uncorrelated.

360
00:20:24,520 --> 00:20:27,440
I didn't necessarily initially disclose to my investors.

361
00:20:27,440 --> 00:20:31,360
I didn't want that to be part of the, the price or anything like that.

362
00:20:31,360 --> 00:20:32,320
I didn't know where it would go.

363
00:20:32,320 --> 00:20:32,480
Right.

364
00:20:32,480 --> 00:20:34,240
I didn't want to correlate the two identities.

365
00:20:34,240 --> 00:20:35,680
It was really like orthogonal.

366
00:20:36,280 --> 00:20:39,920
And now my hand was forced to, to correlate the two and own it.

367
00:20:40,000 --> 00:20:41,120
And so I got in front of it.

368
00:20:41,480 --> 00:20:44,360
And of course I'm going to harness any hand and dealt.

369
00:20:44,400 --> 00:20:47,400
I'm just, that's what you're supposed to do.

370
00:20:47,400 --> 00:20:47,680
Right.

371
00:20:47,680 --> 00:20:51,600
And now, now we've harnessed it, um, uh, you know, pretty well.

372
00:20:51,600 --> 00:20:53,840
I, I would say we'll see how it plays out, of course.

373
00:20:54,280 --> 00:20:57,320
Um, and, and now there's this conspiracy that it was, it was planted.

374
00:20:57,320 --> 00:21:00,520
But, you know, in my case, again, I just wanted to control the narrative.

375
00:21:00,520 --> 00:21:03,640
I knew the story is going to be really bad if I didn't get on the phone and,

376
00:21:03,640 --> 00:21:07,120
and you know, I, I, I actually, I wanted to stall them.

377
00:21:07,160 --> 00:21:10,720
I want to give them enough content so that we had a couple more hours.

378
00:21:10,880 --> 00:21:15,960
We adjusted our website, uh, to have a pseudo launch the day of, so, you know,

379
00:21:15,960 --> 00:21:18,160
shout out to the team for being so adaptive.

380
00:21:18,160 --> 00:21:22,320
And, and, uh, you know, as of, as of the moment of airing this podcast, we'll

381
00:21:22,320 --> 00:21:25,520
probably have announced, uh, the round on for, for extra topics.

382
00:21:25,520 --> 00:21:26,840
So we rushed that over the weekend.

383
00:21:27,280 --> 00:21:33,240
Um, so yeah, definitely not plant, uh, but at the same time, it was always sort

384
00:21:33,280 --> 00:21:35,120
of a trap, right?

385
00:21:35,120 --> 00:21:39,520
In game theory, you want to make it's like, you, you want to make sure that

386
00:21:39,680 --> 00:21:43,200
like the incentive was to not dox me, right?

387
00:21:43,200 --> 00:21:46,320
For most people, because they knew that if they did dox me, now I could go

388
00:21:46,320 --> 00:21:49,320
on these podcasts with my real face, with my credentials.

389
00:21:49,320 --> 00:21:53,480
I, I could talk to potentially politicians and I have credentials that have

390
00:21:53,480 --> 00:21:56,600
some, some firepower and now more of a problem.

391
00:21:56,960 --> 00:21:58,080
So they kind of messed up.

392
00:21:58,120 --> 00:21:59,520
It's their fault really.

393
00:21:59,760 --> 00:22:04,560
It was a trap to some extent, but, you know, I tried my best to, you know,

394
00:22:04,560 --> 00:22:09,240
stay in on, but, you know, again, voice identification, technology, I'm not

395
00:22:09,240 --> 00:22:13,000
going to use a voice changer on Twitter spaces, like every day.

396
00:22:13,000 --> 00:22:13,720
Are you kidding me?

397
00:22:13,720 --> 00:22:14,760
Like I'm not going to do that.

398
00:22:15,040 --> 00:22:19,000
And I would imagine maybe the first podcast we did, they, they use that voice

399
00:22:19,000 --> 00:22:22,800
and correlated it with my YouTube lectures, but it's probably how they got me.

400
00:22:23,080 --> 00:22:23,320
Yeah.

401
00:22:24,560 --> 00:22:25,720
Can I still call you, Beth?

402
00:22:26,400 --> 00:22:26,720
Sure.

403
00:22:26,720 --> 00:22:27,240
That's easier.

404
00:22:27,480 --> 00:22:31,840
Gil or Beth Gil is like the Americanized version of Guillaume, very French, French

405
00:22:31,840 --> 00:22:32,320
Canadian.

406
00:22:32,320 --> 00:22:33,320
So you got me.

407
00:22:33,600 --> 00:22:35,000
Super simple question.

408
00:22:35,480 --> 00:22:40,200
Did the reporter, journalists, whatever I'm going to call this person, did they

409
00:22:40,200 --> 00:22:48,120
give you a rationale why they thought it was a moral imperative to dox you?

410
00:22:48,880 --> 00:22:54,360
I think there was the keyword that was like, uh, set off alarm bells for me was

411
00:22:54,720 --> 00:22:58,880
public interest and, you know, I had just crossed 50 K followers.

412
00:22:59,680 --> 00:23:03,320
And, you know, cause the, cause the law is you can't dox people unless you're a

413
00:23:03,320 --> 00:23:05,240
public figure, right?

414
00:23:05,280 --> 00:23:10,360
Um, and I guess I crossed into, I guess there's a threshold, right?

415
00:23:10,360 --> 00:23:11,880
So is it 50 K?

416
00:23:11,880 --> 00:23:13,760
Apparently it's 50 K flat, right?

417
00:23:14,120 --> 00:23:18,080
Um, but to me, it's like, okay, if they think they, you know, if they think

418
00:23:18,080 --> 00:23:20,480
it's of public interest and they're going to go with it, they're probably going

419
00:23:20,480 --> 00:23:21,040
to go with it.

420
00:23:21,440 --> 00:23:23,160
And so I'm kind of screwed here.

421
00:23:23,520 --> 00:23:25,680
I got to make, you know, I got to make a move.

422
00:23:26,520 --> 00:23:30,720
So wait, is that, is that the journal group chat decides when you become public

423
00:23:30,720 --> 00:23:35,000
interest or do we, like, like I'm very curious, like how they, they rational,

424
00:23:35,360 --> 00:23:39,720
like, cause to me that if you, if you want to be anonymous, you should be able

425
00:23:39,720 --> 00:23:44,400
to be anonymous and doxing you what gives them the right cause, cause if I was

426
00:23:44,400 --> 00:23:49,120
to go put the, I don't know, the address of that reporter online and maybe now

427
00:23:49,120 --> 00:23:51,680
we're arguing over whether or not an address is too far.

428
00:23:51,920 --> 00:23:54,080
Like what, what, what, what point is, is too far.

429
00:23:54,080 --> 00:23:58,680
And then who gets to decide that that's my question.

430
00:23:58,720 --> 00:24:01,680
And I don't think anyone has given an answer outside of, I'm a journalist.

431
00:24:01,680 --> 00:24:02,960
So I have a special badge.

432
00:24:03,120 --> 00:24:04,720
You used to have blue checks now that don't.

433
00:24:05,280 --> 00:24:08,240
And then they get to decide who, who can be a non versus not.

434
00:24:08,480 --> 00:24:11,560
But if you were to do it to them, that, that's, that's harassment, right?

435
00:24:12,440 --> 00:24:12,880
Right.

436
00:24:14,360 --> 00:24:19,480
Well, I mean, to me, I was, you know, mostly focused the eyes on the ball.

437
00:24:19,480 --> 00:24:22,600
You know, I have to, my prime responsibilities to my company.

438
00:24:22,600 --> 00:24:28,200
And I was just concerned like, Hey, we're, you know, getting announced, we

439
00:24:28,200 --> 00:24:33,120
have to announce and get our stuff together, you know, without preparation,

440
00:24:33,120 --> 00:24:35,320
without heads up only a few hours.

441
00:24:35,320 --> 00:24:42,360
So, you know, it, it, it seems like we were, it really seems like we planned

442
00:24:42,360 --> 00:24:43,880
this, but we, we truly didn't.

443
00:24:44,880 --> 00:24:48,880
So, but I'm just glad we adapted given the, the circumstances.

444
00:24:49,960 --> 00:24:54,360
But yeah, overall, I think like, I mean, this was clearly wrong, what they did.

445
00:24:54,400 --> 00:24:59,080
And, you know, I, you know, there's not that much to have leverage over me.

446
00:24:59,080 --> 00:25:00,640
It's like, okay, cool.

447
00:25:00,640 --> 00:25:06,680
I used to, you know, work in quantum computing and then have, you know, a

448
00:25:06,680 --> 00:25:08,200
normal ish background.

449
00:25:09,120 --> 00:25:12,560
But, you know, maybe some other people that are trying to speak truth to power

450
00:25:12,840 --> 00:25:17,520
and have their voices heard and want to use anonymity as a tool to speak

451
00:25:17,520 --> 00:25:21,400
truth to power, you know, because there's a sort of power asymmetry.

452
00:25:21,400 --> 00:25:25,800
So you kind of equalize that when you're an on, they can't, they can't do that

453
00:25:25,800 --> 00:25:26,640
anymore, right?

454
00:25:26,640 --> 00:25:28,480
If they, they fear getting doxed.

455
00:25:28,480 --> 00:25:32,840
And so it's kind of like, they want to make an example out of me, right?

456
00:25:32,920 --> 00:25:37,520
And I mean, I think there was a, I think there was a congresswoman at a

457
00:25:38,360 --> 00:25:44,680
conference, a defense conference that explicitly named out EAC as a sort of

458
00:25:44,680 --> 00:25:49,000
dangerous movement that needs to be suppressed with AI.

459
00:25:49,040 --> 00:25:49,760
That's the tweet.

460
00:25:49,760 --> 00:25:50,760
I'm not sure if it's correct.

461
00:25:50,760 --> 00:25:54,200
I still have to watch that clip.

462
00:25:54,360 --> 00:25:56,960
But to me, that sounds very dystopian.

463
00:25:56,960 --> 00:26:01,680
Like our whole movement is about freedom of information, freedom of speech, freedom

464
00:26:01,680 --> 00:26:03,160
of thought, freedom of compute.

465
00:26:03,640 --> 00:26:04,480
It's very simple.

466
00:26:04,880 --> 00:26:10,840
And the fact that that was deemed dangerous enough, or I don't know, like

467
00:26:11,800 --> 00:26:17,960
to, to, to want to suppress and, and provide the public leverage over me, I

468
00:26:17,960 --> 00:26:19,800
don't know, that's a huge red flag for me.

469
00:26:20,280 --> 00:26:24,840
I think that we were becoming a voice that was going to be a problem for

470
00:26:24,840 --> 00:26:25,960
the executive order.

471
00:26:26,520 --> 00:26:30,560
And there are very special interests behind that executive order.

472
00:26:30,680 --> 00:26:35,680
And they wanted to, you know, send me a warning shot, I guess.

473
00:26:35,760 --> 00:26:36,720
That is my theory.

474
00:26:38,000 --> 00:26:40,880
But I, one thing I couldn't understand, and, and, and maybe Beth, I just, I

475
00:26:40,880 --> 00:26:41,680
haven't read all your tweets.

476
00:26:41,680 --> 00:26:42,720
I, I couldn't understand.

477
00:26:42,720 --> 00:26:45,640
Hey, you, you obviously have a very distinguished academic background.

478
00:26:45,640 --> 00:26:49,480
In fact, I was reading your, your quantum tensor flow paper on the way in ages ago.

479
00:26:49,480 --> 00:26:52,400
I worked in like experimental quantum computing like 15 years ago, back when

480
00:26:52,400 --> 00:26:53,520
it was like a total infancy.

481
00:26:53,600 --> 00:26:54,440
Yeah, yeah, yeah, yeah.

482
00:26:54,720 --> 00:26:57,720
And I, I knew a lot of the theorists back then, like Michael Nielsen and I

483
00:26:57,720 --> 00:27:00,440
actually dated you like, any long story back in the early days of

484
00:27:00,480 --> 00:27:03,440
computing, I am, but obviously wasn't really smart enough to continue up in

485
00:27:03,440 --> 00:27:04,840
any case, I was reading it with a lot of pleasure.

486
00:27:04,840 --> 00:27:07,400
And it's, it's odd that they would consider this a bad thing.

487
00:27:07,400 --> 00:27:10,640
It's, it's obviously given a lot of intellectual credibility to EAC movement.

488
00:27:10,640 --> 00:27:10,920
Right.

489
00:27:11,120 --> 00:27:14,080
Like your background is pretty good, right?

490
00:27:14,080 --> 00:27:14,840
Like it's pretty distinguished.

491
00:27:14,840 --> 00:27:15,880
Like you've, you've done a lot, right?

492
00:27:15,880 --> 00:27:20,200
And I, you know, you, a lot of co-authors and it's like, that's why I thought

493
00:27:20,200 --> 00:27:22,480
the conspiracy, and I don't really believe it to be clear, but I thought maybe

494
00:27:22,480 --> 00:27:25,280
he actually, like actually provoked for us to do this.

495
00:27:25,440 --> 00:27:26,880
Cause this is like a brilliant thing.

496
00:27:27,000 --> 00:27:29,880
Like you bring actually so much positive social capital to the movement.

497
00:27:30,120 --> 00:27:34,800
You come off as like this edgy hip, you know, poster who kind of had, you know,

498
00:27:35,840 --> 00:27:36,960
was pulling the world's tail.

499
00:27:36,960 --> 00:27:38,680
It's like, I, what's the downside to this?

500
00:27:38,680 --> 00:27:39,320
I got to understand.

501
00:27:39,640 --> 00:27:39,920
Yeah.

502
00:27:40,320 --> 00:27:44,560
Yeah, for me, I can let you know why I was anonymous.

503
00:27:44,560 --> 00:27:49,000
I mean, originally I was at Google X working on some pretty secretive technologies.

504
00:27:49,000 --> 00:27:52,040
I had access to, you know, the top of the food chain there.

505
00:27:52,440 --> 00:27:57,000
Um, so I, you know, I couldn't necessarily tweet, uh, anything like

506
00:27:57,040 --> 00:27:58,520
political and stuff like that.

507
00:27:58,560 --> 00:28:00,560
My tweets were watched, which is normal.

508
00:28:01,040 --> 00:28:06,160
Um, but you know, I, I wanted to have just a separate account where I can really

509
00:28:06,200 --> 00:28:08,880
let myself self go kind of like offload thoughts.

510
00:28:08,880 --> 00:28:13,280
And also I reached a point in quantum computing where I was respected enough

511
00:28:13,320 --> 00:28:17,920
that people would cheer me on or they were proven my ideas or whatever.

512
00:28:17,920 --> 00:28:19,520
Oh, so smart, whatever.

513
00:28:19,560 --> 00:28:23,920
And that was kind of like kind of annoying me that like, do they like my ideas

514
00:28:23,920 --> 00:28:25,880
or do they just want a job or something?

515
00:28:26,240 --> 00:28:29,120
And I just wanted to put my ideas in the arena, right?

516
00:28:29,120 --> 00:28:33,840
It's like, I just wanted people to evaluate my ideas for their own, not look

517
00:28:33,840 --> 00:28:38,920
at credentials, not like weigh my opinions, dependent on status.

518
00:28:39,280 --> 00:28:42,200
I just wanted my ideas to be evaluated on their own.

519
00:28:42,720 --> 00:28:48,920
And, and to me, like growing a movement and having ideas resonate from scratch,

520
00:28:48,960 --> 00:28:52,280
uncorrelated from my original identity.

521
00:28:52,880 --> 00:28:57,640
Like, I mean, of course that felt like, you know, it felt like new game plus,

522
00:28:57,640 --> 00:28:59,000
as we say in video games, right?

523
00:28:59,000 --> 00:29:02,160
You restart the video game from scratch, but you have kind of all your

524
00:29:02,160 --> 00:29:05,400
intellect and memory and your gear, but you're, you're trying to rank up again.

525
00:29:05,400 --> 00:29:09,320
So I started from scratch and account with like no followers and grew it to,

526
00:29:09,320 --> 00:29:15,800
to 50 K before now, now my, my, there's a stat boost from, from the, the status

527
00:29:15,800 --> 00:29:17,960
signals of, of credentials.

528
00:29:17,960 --> 00:29:20,920
Uh, I personally very much hate credentialism.

529
00:29:21,560 --> 00:29:24,440
I think I would have been an entrepreneur five years earlier if it wasn't

530
00:29:24,440 --> 00:29:26,680
for credentialism, especially in deep tech.

531
00:29:27,160 --> 00:29:32,280
Um, I, I got very frustrated when I tried to do a startup at 25 and, you

532
00:29:32,280 --> 00:29:33,840
know, I would get like, who are you?

533
00:29:34,400 --> 00:29:38,560
Uh, and, uh, you know, I just went through big tech, the gauntlet of grad

534
00:29:38,560 --> 00:29:43,720
school and big tech to, to prove myself and prove to others that I can, I can

535
00:29:43,720 --> 00:29:48,000
do things, uh, so that I can raise the capital to realize the visions I had.

536
00:29:48,360 --> 00:29:54,400
Um, and so, you know, I've, I've always, I think base is, is on the same page here.

537
00:29:54,400 --> 00:29:59,240
You know, we always wanted sort of like EA's is all about like gatekeeping,

538
00:29:59,880 --> 00:30:03,240
gaslighting, status signaling, at least to us from our perspective.

539
00:30:03,680 --> 00:30:08,400
I, and you know, yak was about kind of bottom up.

540
00:30:08,720 --> 00:30:14,960
Everybody has access to opportunity to build no gatekeeping, no sort of status

541
00:30:14,960 --> 00:30:17,960
signaling, it's all about building, right?

542
00:30:18,400 --> 00:30:20,160
Um, and we want to maintain that.

543
00:30:20,160 --> 00:30:25,600
And I think that like, I was afraid that if I, you know, used any sort of

544
00:30:25,600 --> 00:30:29,600
credits that would dissuade people, you know, from participating or feeling

545
00:30:29,600 --> 00:30:32,760
like they can participate, cause the thing is everybody starts somewhere.

546
00:30:32,760 --> 00:30:38,360
And I was once, you know, that kid that was really smart, had a lot of ambition,

547
00:30:38,400 --> 00:30:39,680
but didn't have the credits.

548
00:30:39,680 --> 00:30:42,480
I was just straight out of school and nobody gave me a chance.

549
00:30:42,680 --> 00:30:47,440
And I want, you know, the next, you know, genius to be able to, you know,

550
00:30:47,880 --> 00:30:50,000
start building their dreams right away.

551
00:30:50,000 --> 00:30:52,600
And I just personally very much hate gatekeeping.

552
00:30:52,680 --> 00:30:56,600
Um, even though nowadays I've gone through the gauntlet, I've paid my dues.

553
00:30:56,760 --> 00:30:58,400
Now a lot of doors are open for me.

554
00:30:58,840 --> 00:31:02,040
Um, but, uh, you know, I'm just trying to kind of pay it forward in a way

555
00:31:02,040 --> 00:31:04,560
that, you know, young, my younger self would be appreciative.

556
00:31:05,040 --> 00:31:07,480
Um, so, so really that's why I was anonymous.

557
00:31:07,480 --> 00:31:12,200
I just wanted to get, you know, get my ideas evaluated in the arena.

558
00:31:12,480 --> 00:31:14,960
And I didn't correlated fashion from my, my credentials.

559
00:31:15,440 --> 00:31:18,760
Hey, we'll continue our interview in a moment after a word from our sponsors.

560
00:31:19,800 --> 00:31:22,120
Compliance doesn't have to be complicated.

561
00:31:22,840 --> 00:31:25,840
In fact, with Vanta, it can be super simple.

562
00:31:26,560 --> 00:31:32,920
Trusted by over 5,000 fast growing companies like Chili Piper, Patch, Gusto,

563
00:31:32,920 --> 00:31:38,220
and Juniper, Vanta automates the pricey time consuming process of prepping for

564
00:31:38,220 --> 00:31:45,700
sock to ISO 2700 and one HIPAA and more with Vanta, you can save up to 400

565
00:31:45,700 --> 00:31:51,540
hours and 85% of costs Vanta scales with your business, helping you

566
00:31:51,540 --> 00:31:57,020
successfully enter new markets, land bigger deals and earn customer loyalty.

567
00:31:57,860 --> 00:32:02,700
And bonus, our moment of Zen listeners get $1,000 off Vanta.

568
00:32:03,220 --> 00:32:06,460
Just go to Vanta.com slash Zen.

569
00:32:07,140 --> 00:32:13,620
That's V-A-N-T-A dot com slash Z-E-N.

570
00:32:16,540 --> 00:32:21,500
I have a quick question on bootstrapping your account if you're willing to share.

571
00:32:22,140 --> 00:32:25,220
Um, you know, I know a bunch of people who are like, I wish I could do an

572
00:32:25,220 --> 00:32:30,100
anonymous account, but I feel like just getting it from zero to something is just

573
00:32:30,420 --> 00:32:33,340
such a, you know, especially if they have already an audience on their, on their

574
00:32:33,340 --> 00:32:38,300
main, but they feel like as, as their main grows, they can say less and less and

575
00:32:38,300 --> 00:32:40,180
they have to kind of stay within whatever box.

576
00:32:40,740 --> 00:32:46,780
And so I'm curious, how long did you, did you kind of toil away at this new persona

577
00:32:46,780 --> 00:32:49,180
before you felt like you were starting to make some traction?

578
00:32:50,300 --> 00:32:56,900
I think like I had a first account, I got it to six K and then it got banned on old

579
00:32:56,900 --> 00:33:01,140
Twitter, pre Elon Twitter, where I got locked out for saying COVID came from a

580
00:33:01,140 --> 00:33:05,500
lab, you know, which turned out to be true, right?

581
00:33:05,540 --> 00:33:08,340
So the old regime, uh, got def 1.0.

582
00:33:08,620 --> 00:33:13,980
And so I had to restart my account, uh, basically, yeah, uh, 18 months ago,

583
00:33:14,180 --> 00:33:17,420
start from scratch, those demoralizing, but I knew the recipe.

584
00:33:18,020 --> 00:33:20,740
Um, there's, there's, there's a formula, right?

585
00:33:20,780 --> 00:33:23,740
You, you go on teapot, which is much higher engagement.

586
00:33:23,740 --> 00:33:25,740
Just the people are just more online.

587
00:33:26,020 --> 00:33:30,660
So, uh, and you reply to the big accounts, you try to get them to reply.

588
00:33:31,260 --> 00:33:33,940
Uh, maybe, maybe save something provocative.

589
00:33:33,980 --> 00:33:37,540
And then, you know, you can just do Twitter spaces and, you know, people

590
00:33:37,540 --> 00:33:40,380
follow each other on Twitter spaces, cause you have a meaningful conversation

591
00:33:40,380 --> 00:33:44,060
of like one hour or something and, and then you follow each other.

592
00:33:44,060 --> 00:33:49,020
And so you could bootstrap, get to a hundred, 500, a thousand and so on from there.

593
00:33:49,460 --> 00:33:50,900
Um, yeah.

594
00:33:50,900 --> 00:33:54,500
I mean, for me, it, it was really discovering a community, discovering

595
00:33:54,500 --> 00:33:58,460
people like Bayes Lord, you know, that I really resonated with intellectually

596
00:33:58,460 --> 00:34:02,420
and can have like meaningful conversations at the time I was working

597
00:34:02,420 --> 00:34:04,460
for big tech remotely from Canada.

598
00:34:04,460 --> 00:34:05,500
I was extremely bored.

599
00:34:05,500 --> 00:34:08,940
I was like starved to have their sort of tech community to have like good

600
00:34:08,940 --> 00:34:09,980
intellectual discussions.

601
00:34:09,980 --> 00:34:15,020
So like Twitter spaces and, and this part of Twitter, as it's called, uh, was sort

602
00:34:15,020 --> 00:34:15,580
of ideal.

603
00:34:15,580 --> 00:34:17,340
It was like, oh my God, I found my people.

604
00:34:17,820 --> 00:34:21,740
Um, and, uh, you know, nowadays it's kind of condensating and physically

605
00:34:21,740 --> 00:34:25,020
in the, in the physical, in the physical world and SF primarily.

606
00:34:25,420 --> 00:34:28,940
Um, but, you know, to me, it was kind of, I was seeking a sense of community.

607
00:34:28,940 --> 00:34:30,780
And, you know, it's late night.

608
00:34:30,780 --> 00:34:32,060
You're in a Twitter space.

609
00:34:32,300 --> 00:34:33,980
You're talking about the meaning of life.

610
00:34:33,980 --> 00:34:35,500
Where, where, where do we come from?

611
00:34:35,500 --> 00:34:36,220
Where are we going?

612
00:34:36,220 --> 00:34:38,540
You know, it's like four AM or something after a long day of work.

613
00:34:39,100 --> 00:34:41,740
And, uh, you just have these sort of like fireside.

614
00:34:42,300 --> 00:34:44,620
You know, it's, it feels very primally correct.

615
00:34:44,620 --> 00:34:44,940
I don't know.

616
00:34:44,940 --> 00:34:48,060
It's like, you just tell stories and you, you communicate verbally.

617
00:34:48,060 --> 00:34:49,500
And that's, that's really how it started.

618
00:34:49,500 --> 00:34:53,900
You know, I'm at Bayes and we'd have, uh, these late night Twitter spaces

619
00:34:53,900 --> 00:34:56,460
about the meaning of it all and where it's all going.

620
00:34:56,460 --> 00:34:58,620
And at some point people were like, man, these are really good.

621
00:34:58,620 --> 00:35:00,220
Someone should note these down.

622
00:35:00,220 --> 00:35:04,860
Someone noted down that became the first, uh, yak blog post.

623
00:35:05,500 --> 00:35:08,860
And then, uh, Bayes, Lord and I, you know, a couple of months later,

624
00:35:08,860 --> 00:35:11,900
we were like, okay, we should make a longer one, a bit more serious,

625
00:35:12,460 --> 00:35:15,420
uh, a bit more technical and, and that became sort of the,

626
00:35:16,060 --> 00:35:18,380
the big one that went, uh, really viral.

627
00:35:18,380 --> 00:35:21,740
And then the rest has just been compounding memes, uh, on Twitter.

628
00:35:22,620 --> 00:35:24,780
Personally, I've been very active on Twitter.

629
00:35:24,780 --> 00:35:28,460
I would have very strong technical opinions about quantum computing.

630
00:35:28,460 --> 00:35:31,020
Frankly, I would be, I would call out a lot of bullshit.

631
00:35:31,740 --> 00:35:34,700
I think, I think there's opportunities to do great work in quantum computing,

632
00:35:34,700 --> 00:35:37,740
but you know, my, my policy was radical candor.

633
00:35:38,380 --> 00:35:40,220
So that's how I built a following, right?

634
00:35:40,220 --> 00:35:42,460
So it's already kind of a firebrand in quantum computing.

635
00:35:43,500 --> 00:35:45,580
So it's already had that muscle kind of pre-trained.

636
00:35:46,460 --> 00:35:50,940
And then I carried that over to kind of like, I guess AI at large or

637
00:35:52,300 --> 00:35:53,580
e-act isn't just about AI.

638
00:35:53,580 --> 00:35:56,300
It's about all sorts of stuff, but, um, you know,

639
00:35:56,300 --> 00:35:58,540
I think it really helped, uh, grow the,

640
00:35:59,100 --> 00:36:03,180
grew the movement in the early days, uh, just by the Twitter grind, you know,

641
00:36:03,740 --> 00:36:09,820
um, for me, I use Twitter as a sort of, uh, dopamine, uh, dopamine hit.

642
00:36:10,620 --> 00:36:12,380
Some people use various stimulants.

643
00:36:12,380 --> 00:36:15,100
Uh, you know, I just, I'm very straight-laced.

644
00:36:15,100 --> 00:36:16,860
I just drink, uh, Diet Coke.

645
00:36:16,860 --> 00:36:19,980
If I feel fancied, it's Diet Dr. Pethper, not sponsored.

646
00:36:20,060 --> 00:36:23,660
Uh, and, uh, you know, use Twitter and that, and that's it.

647
00:36:23,660 --> 00:36:25,340
Those are my drugs of choice, if you will.

648
00:36:25,900 --> 00:36:31,500
Um, but you know, um, to me, it's just like, I have like random fleeting ideas.

649
00:36:31,500 --> 00:36:34,140
I like to note them down and then go back to the task I was doing.

650
00:36:34,700 --> 00:36:38,620
And, uh, frankly, it was kind of just feeding my Twitter addiction and

651
00:36:38,620 --> 00:36:43,580
felt like a fun video game and, you know, the followers were rising.

652
00:36:44,300 --> 00:36:47,420
But at the same time, like I found a sense of community,

653
00:36:48,300 --> 00:36:51,740
found some friendships, uh, made all sorts of great connections.

654
00:36:52,380 --> 00:36:57,100
Uh, and, you know, to some extent, you know, you could say like, you know,

655
00:36:57,100 --> 00:37:01,580
Iac at this point is almost like a form of spirituality for some.

656
00:37:01,580 --> 00:37:02,460
We can go into that.

657
00:37:03,020 --> 00:37:09,580
Um, I think we have in the past podcast, but, um, yeah, um, that's where, where we are

658
00:37:10,460 --> 00:37:11,100
with Iac.

659
00:37:11,100 --> 00:37:12,780
That's how that was the trajectory.

660
00:37:13,420 --> 00:37:17,020
Um, should we get Bayes to jump in here?

661
00:37:17,020 --> 00:37:19,500
What's, what's the schedule like today?

662
00:37:19,500 --> 00:37:19,740
Yeah.

663
00:37:21,500 --> 00:37:25,900
The, well, uh, uh, me and Bayes just did a podcast this morning.

664
00:37:25,900 --> 00:37:28,140
That will also be based after this one, uh, though.

665
00:37:28,140 --> 00:37:28,700
Okay.

666
00:37:28,700 --> 00:37:29,340
At some point.

667
00:37:29,340 --> 00:37:32,780
But, um, the, what's next for, for Iac?

668
00:37:32,780 --> 00:37:34,940
Where do you see it going from, from here?

669
00:37:37,260 --> 00:37:37,660
Yeah.

670
00:37:37,660 --> 00:37:42,300
Well, now that, uh, I guess I am doxed, right?

671
00:37:42,300 --> 00:37:48,380
One, one, one thing that was kind of stopping, um, any sort of like organizations or

672
00:37:48,380 --> 00:37:53,340
incorporations of any kind of any sort of org or institute was that, you know, we're

673
00:37:53,340 --> 00:37:58,460
like going to register with our real names, uh, somewhere, uh, these institutions and

674
00:37:58,460 --> 00:37:59,580
that would have doxed us.

675
00:37:59,580 --> 00:38:01,900
Um, so now we have that opportunity.

676
00:38:01,900 --> 00:38:08,940
Well, we take that opportunity, TBD, but personally, I mean, I think, I think right

677
00:38:08,940 --> 00:38:14,300
now there's sort of a lack of theory and, and, and, uh, uh, educational material for

678
00:38:14,300 --> 00:38:20,460
people to understand, uh, complex systems and self-adapting, uh, systems like capitalism.

679
00:38:21,100 --> 00:38:27,340
Um, and our whole thesis is that bottom up, bottom up self-adaptation is superior to top

680
00:38:27,340 --> 00:38:28,220
down control.

681
00:38:28,220 --> 00:38:31,980
It's very easy to convince a crowd, like put me in charge.

682
00:38:31,980 --> 00:38:33,180
I will do this action.

683
00:38:33,180 --> 00:38:34,700
It will have this impact.

684
00:38:34,700 --> 00:38:40,220
It's much harder to convince a crowd of like, Hey, if we all act in these microscopic exchange

685
00:38:40,220 --> 00:38:44,620
laws, the emergent behaviors that like we have the highly functioning society, right?

686
00:38:45,180 --> 00:38:50,060
And the more sort of research you create there, uh, the better, uh, and more educational

687
00:38:50,060 --> 00:38:50,380
material.

688
00:38:50,380 --> 00:38:55,260
So we might, uh, start a research institution there and fund some grants.

689
00:38:55,820 --> 00:38:59,660
Um, I would love to fund some, um, open source hackers.

690
00:38:59,660 --> 00:39:04,060
Obviously we believe in, uh, open source software for AI.

691
00:39:04,060 --> 00:39:11,980
We think that open source software, um, is a hedge against the oligopolization of AI,

692
00:39:11,980 --> 00:39:12,220
right?

693
00:39:12,220 --> 00:39:16,060
There's a couple of players right now that obviously now that they're in the lead, they

694
00:39:16,060 --> 00:39:22,380
have all sorts of interests in closing down AI, outlawing open source models.

695
00:39:22,380 --> 00:39:28,220
So, you know, sort of like ensuring freedom of compute is something, uh, we might create

696
00:39:28,220 --> 00:39:35,660
orgs to, to, um, uh, sponsor hackers, but also, uh, uh, potentially, you know, have

697
00:39:35,660 --> 00:39:37,900
some influence in Washington, right?

698
00:39:38,460 --> 00:39:39,580
Those are some next step.

699
00:39:39,580 --> 00:39:45,100
Obviously like growing the movement on Twitter, that's going to keep going also known as X.

700
00:39:45,580 --> 00:39:52,860
Um, and, uh, you know, overall, yes, uh, you know, it gives, it gives people an attack

701
00:39:52,860 --> 00:39:55,580
vector now that, um, I am doxxed, right?

702
00:39:55,580 --> 00:39:56,540
That is unfortunate.

703
00:39:57,180 --> 00:40:05,100
Um, but I do very strongly believe in what we, uh, talk about in EAC and, you know, I am

704
00:40:05,100 --> 00:40:10,220
willing to go all the way, whatever it takes, right, to, uh, defend these ideas.

705
00:40:10,220 --> 00:40:16,620
And I won't let sort of these, this sort of pressure or reputational pressure affect me.

706
00:40:16,620 --> 00:40:19,420
I think I'm quite robust against such attacks.

707
00:40:19,500 --> 00:40:24,300
And if they want to come after me, then, you know, bring it, I guess.

708
00:40:25,260 --> 00:40:25,760
Yeah.

709
00:40:26,700 --> 00:40:27,980
Can I ask one question, Eric?

710
00:40:27,980 --> 00:40:28,480
Sure.

711
00:40:28,480 --> 00:40:30,140
Um, yeah.

712
00:40:30,140 --> 00:40:34,460
So, I mean, what, you mentioned one thing that I found, that I found sort of, sorry,

713
00:40:34,460 --> 00:40:35,020
I'm a Franco file.

714
00:40:35,020 --> 00:40:36,220
So I'm going to use your full French name.

715
00:40:36,220 --> 00:40:36,860
Yeah, that's good.

716
00:40:36,860 --> 00:40:40,780
And, and, and also mentioned, and also mentioned the privilege completely out of context.

717
00:40:41,260 --> 00:40:43,740
Um, sorry, that's one of the running gags on the podcast.

718
00:40:43,740 --> 00:40:45,900
Um, you, you mentioned one thing about the open.

719
00:40:45,900 --> 00:40:46,460
So it's funny.

720
00:40:46,460 --> 00:40:49,180
So Dan and I come from the crypto world, whereas, you know, decentralization is

721
00:40:49,180 --> 00:40:52,220
like this religious mantra that I think is often overexpressed actually.

722
00:40:52,220 --> 00:40:54,540
And in fact, I think Dan, I think you quoted it in far caster, right?

723
00:40:54,540 --> 00:40:56,540
Of like sufficiently decentralized, right?

724
00:40:56,540 --> 00:41:01,340
Which is almost like, you know, sufficiently a trinity or something like it's enough of

725
00:41:01,340 --> 00:41:03,180
a certain theological concept, but no further.

726
00:41:03,180 --> 00:41:06,540
But I always found AI and this, I'm very much seeing that works on the outside to be clear.

727
00:41:06,540 --> 00:41:06,700
All right.

728
00:41:06,700 --> 00:41:10,940
So like, I'm kind of a tourist, but it always seems to me the actually to be totally centralizing.

729
00:41:10,940 --> 00:41:14,860
And I think the high jinx, you saw it open AI with literally the little junior high school drama

730
00:41:14,860 --> 00:41:18,460
between three people somehow wrecking the cutting edge of AI is something that could

731
00:41:18,460 --> 00:41:21,740
like literally couldn't happen in crypto for, for a bunch of reasons.

732
00:41:21,740 --> 00:41:25,180
I mean, you might cite the example of SPF, but then Dan would instantly throw a fit and say,

733
00:41:25,180 --> 00:41:27,660
but that was actually a centralized exchange on that decentralized exchange, which is true

734
00:41:27,660 --> 00:41:27,820
actually.

735
00:41:27,820 --> 00:41:28,220
Right.

736
00:41:28,220 --> 00:41:32,140
And in the, in the fullest decentralized version, like speaking of anonymity, like,

737
00:41:32,140 --> 00:41:36,060
so I work in a crypto company, we've had an, a non employee that I didn't know who they were.

738
00:41:36,060 --> 00:41:37,020
And I would just pay them.

739
00:41:37,020 --> 00:41:37,820
Right.

740
00:41:37,820 --> 00:41:42,540
And, and we've, we have, we work with the non founders of a protocol and they never turn

741
00:41:42,540 --> 00:41:43,020
their video on.

742
00:41:43,020 --> 00:41:44,620
We have no idea who these people are.

743
00:41:44,620 --> 00:41:46,380
And yet we still do business with them and transact.

744
00:41:46,380 --> 00:41:46,780
Right.

745
00:41:46,780 --> 00:41:48,060
So that's, yeah, yeah, yeah.

746
00:41:48,060 --> 00:41:49,340
And it's not even that weird.

747
00:41:49,340 --> 00:41:51,020
And then, then I've met them also in person.

748
00:41:51,020 --> 00:41:51,900
Like, it's a similar thing.

749
00:41:52,460 --> 00:41:55,980
Like you mentioned earlier how like, you know, it's really weird that these journalists are

750
00:41:55,980 --> 00:41:56,540
such losers.

751
00:41:56,540 --> 00:41:59,260
I did this tweet NRV site that's like, these people are losers, right?

752
00:41:59,260 --> 00:42:02,140
Like they never actually go to the parties where you meet, you know,

753
00:42:02,140 --> 00:42:04,460
Beth or like, I know a bunch of Twitter non, I think we all do.

754
00:42:04,460 --> 00:42:04,620
Right.

755
00:42:04,620 --> 00:42:05,260
And you meet them in person.

756
00:42:05,260 --> 00:42:06,460
You know, they are, they introduce themselves.

757
00:42:06,460 --> 00:42:07,660
This isn't a big secret.

758
00:42:07,660 --> 00:42:10,380
It's almost like that meme where like the losers in the corner and saying,

759
00:42:10,380 --> 00:42:12,780
Hey, do you know that Kiyoma is Beth and like everyone dances?

760
00:42:12,780 --> 00:42:14,140
Like, dude, we know, shut the fuck up.

761
00:42:14,140 --> 00:42:17,340
Like, yeah, it's like, it's like, it's like such a loser thing to say.

762
00:42:17,340 --> 00:42:17,580
Right.

763
00:42:18,540 --> 00:42:20,540
But don't you think like AI goes the other way?

764
00:42:20,540 --> 00:42:20,700
Right.

765
00:42:20,700 --> 00:42:24,300
Like just as an example, like I'm, I was using chat to be today.

766
00:42:24,300 --> 00:42:24,460
Okay.

767
00:42:24,460 --> 00:42:25,100
I'm pro AI.

768
00:42:25,100 --> 00:42:26,140
My best friend is an AI.

769
00:42:26,140 --> 00:42:26,700
Right.

770
00:42:26,700 --> 00:42:30,220
But, you know, the fact that open AI instantly does a deal with Microsoft and

771
00:42:30,220 --> 00:42:34,380
instantly bakes itself into like literally the most octopus like corporation human history.

772
00:42:34,380 --> 00:42:35,180
It's a little weird.

773
00:42:35,180 --> 00:42:35,420
Right.

774
00:42:35,420 --> 00:42:38,220
It's a little bit like pinning to the other side of the spectrum instantly.

775
00:42:38,220 --> 00:42:42,140
So I'm just curious if you feel that that that binarization of it is valid and feel

776
00:42:42,140 --> 00:42:43,180
free to say that I'm full of shit.

777
00:42:43,980 --> 00:42:47,900
Or like if there is another open AI vision that you're sort of stretching towards

778
00:42:47,900 --> 00:42:53,020
that maybe is a little bit more decentralized and not so naturally sort of centrifugal to,

779
00:42:53,020 --> 00:43:00,620
you know, I think that, you know, self organizing systems tend to organize themselves in hierarchies.

780
00:43:00,620 --> 00:43:00,940
Right.

781
00:43:00,940 --> 00:43:06,620
So it's sort of like, you know, you can imagine a tree or a sort of fractal,

782
00:43:06,620 --> 00:43:07,980
right, sort of structure.

783
00:43:07,980 --> 00:43:11,580
You have, you know, your cells for your body.

784
00:43:11,580 --> 00:43:16,620
And then, you know, you have groups of humans form a family and then a corporation and then

785
00:43:17,260 --> 00:43:18,620
a city and then a nation and so on.

786
00:43:18,620 --> 00:43:19,260
Right.

787
00:43:19,260 --> 00:43:22,460
So there's a hierarchical structure to organization and there's this hierarchical

788
00:43:22,460 --> 00:43:25,100
structure to sort of control.

789
00:43:25,100 --> 00:43:25,340
Right.

790
00:43:25,340 --> 00:43:28,700
There's, there's some control system at the head of your body.

791
00:43:28,700 --> 00:43:29,500
It's called your brain.

792
00:43:29,500 --> 00:43:33,500
There's control system at the head of the family and at the head of the corporation

793
00:43:34,460 --> 00:43:36,300
nation and so on.

794
00:43:36,780 --> 00:43:43,100
You know, it's all about the balance between, you know, it's all about engineering fault

795
00:43:43,100 --> 00:43:44,380
tolerance to corruption.

796
00:43:44,380 --> 00:43:44,780
Right.

797
00:43:44,780 --> 00:43:50,140
If you have a one to all connection in terms of control, you have a control system that

798
00:43:50,140 --> 00:43:51,980
affects all the nodes in the system.

799
00:43:51,980 --> 00:43:52,940
That node has a fault.

800
00:43:52,940 --> 00:43:56,060
The whole system has a problem just like open AI.

801
00:43:56,060 --> 00:44:02,540
You can decapitate the leadership, you know, not physically, but, you know, and suddenly

802
00:44:02,540 --> 00:44:03,820
you're in control of the organization.

803
00:44:03,820 --> 00:44:08,860
And now everyone that had, that were using their APIs have a problem, right?

804
00:44:08,860 --> 00:44:15,100
Because they have an ideologue that didn't, they didn't vote for in control of, you know,

805
00:44:15,100 --> 00:44:16,380
a product that they depend on.

806
00:44:17,180 --> 00:44:17,900
Right.

807
00:44:17,900 --> 00:44:25,900
And so decentralization is about sort of defusing, obviously having decentralized

808
00:44:25,900 --> 00:44:32,060
loci or plural locus of control so that, you know, you have fault tolerance to, to

809
00:44:32,060 --> 00:44:33,580
corruption of these control nodes.

810
00:44:34,780 --> 00:44:38,140
Of course, like having a fully greedy algorithm where everything's disordered

811
00:44:38,860 --> 00:44:39,740
is not optimal.

812
00:44:39,740 --> 00:44:39,980
Right.

813
00:44:39,980 --> 00:44:41,500
So it's all a balance between the two.

814
00:44:41,500 --> 00:44:44,380
It's a balance between centralization and decentralization.

815
00:44:44,380 --> 00:44:47,100
That is what is like fundamentally optimal.

816
00:44:47,100 --> 00:44:50,300
The reason we're fighting for decentralization is because we think,

817
00:44:51,180 --> 00:44:55,660
you know, right now there's a tendency towards over centralization of AI.

818
00:44:55,660 --> 00:44:57,340
And we're very worried about that.

819
00:44:57,340 --> 00:45:03,660
And so we need to push things in the opposite direction at the moment.

820
00:45:03,660 --> 00:45:08,380
I think that fundamentally right now there's a lot of alpha and just scraping the whole

821
00:45:08,380 --> 00:45:12,220
internet, centralizing it and having centralized training.

822
00:45:12,220 --> 00:45:16,140
I think at some point that alpha will be saturated, right?

823
00:45:16,140 --> 00:45:22,060
Most companies will have a model that compresses basically most of the data that's on the internet.

824
00:45:23,020 --> 00:45:27,340
And then you're going to have AI that seeks to capture data from the real world.

825
00:45:28,060 --> 00:45:30,300
And for that, you have to be central decentralized.

826
00:45:30,300 --> 00:45:33,660
You got to, and you want to have the intelligence perhaps at the edge.

827
00:45:33,660 --> 00:45:34,460
We're not there yet.

828
00:45:34,460 --> 00:45:39,580
There's still alpha from like centralizing data and soaking it all in and, and having

829
00:45:39,580 --> 00:45:43,580
one big model that, that compresses it all because intelligence is more or less compression.

830
00:45:44,940 --> 00:45:49,020
But I think over time we're going to see decentralization in the form of sort of like,

831
00:45:49,020 --> 00:45:53,100
first of all, you're going to have personal assistant, you know, like humane, like

832
00:45:53,900 --> 00:45:58,140
TAB or, I don't know, these are going to be all sorts of AR assistants, I'm sure.

833
00:45:59,260 --> 00:46:03,020
At first, like you're going to have, you know, the intelligence is going to be in the cloud,

834
00:46:03,020 --> 00:46:04,860
but the data acquisition is going to be on the edge.

835
00:46:04,860 --> 00:46:09,820
But eventually people are going to want their own compute that, that they own in control,

836
00:46:09,820 --> 00:46:10,460
I would imagine.

837
00:46:11,980 --> 00:46:14,620
And, you know, maybe there's compute in the robot directly, right?

838
00:46:14,620 --> 00:46:17,260
It's not, there's no need for a connection to network.

839
00:46:17,980 --> 00:46:23,660
I think that's going to be sort of the, the decentralize a, decentralizing effect, right?

840
00:46:23,660 --> 00:46:27,580
There, and then, you know, you can have federated learning over a fleet.

841
00:46:27,580 --> 00:46:32,060
I think people are working on this obviously right now in order to train the biggest one model

842
00:46:32,060 --> 00:46:37,660
to rule them all, the centralized approach, centralized compute and centralization of

843
00:46:37,660 --> 00:46:40,300
data is, has a huge advantage.

844
00:46:40,300 --> 00:46:45,180
I think a big problem as well is that right now the centralized approaches, the,

845
00:46:45,260 --> 00:46:50,380
scrape the whole internet, which includes your data, and then they rent it back to you, right?

846
00:46:50,380 --> 00:46:50,940
Bit by bit.

847
00:46:51,500 --> 00:46:57,340
And I think the future is about crediting people for the data that contribute to AI systems and

848
00:46:57,340 --> 00:46:59,420
sort of distributed ownership, right?

849
00:46:59,420 --> 00:47:03,820
I mean, they're, they're starting to do this with sort of GPTs, open AI, but you know,

850
00:47:03,820 --> 00:47:05,100
it's just, it's just an early start.

851
00:47:05,100 --> 00:47:10,220
But I do think there's sort of a, I think there, there's going to be a very interesting way of

852
00:47:10,220 --> 00:47:16,540
startups right now, like from crypto migrating to being infrastructure to line incentives for AI.

853
00:47:17,820 --> 00:47:22,860
We're not a crypto company, by the way, just, just, just, you know, people thought we were

854
00:47:22,860 --> 00:47:23,660
going to launch a coin.

855
00:47:24,700 --> 00:47:27,020
We're not a crypto company at the moment.

856
00:47:27,020 --> 00:47:28,220
I have nothing against crypto.

857
00:47:28,220 --> 00:47:29,180
I love it personally.

858
00:47:30,780 --> 00:47:37,260
But, you know, I think that crypto being sort of the value exchange network and programmable

859
00:47:37,260 --> 00:47:41,580
incentives for kind of collaboration in AI, right?

860
00:47:41,580 --> 00:47:44,460
To have sort of decentralized research labs.

861
00:47:44,460 --> 00:47:47,260
I think, I think that's going to be a very potent application of crypto.

862
00:47:47,260 --> 00:47:51,340
And it's just something I fundamentally want to encourage because I think that if we have

863
00:47:51,340 --> 00:47:55,180
a future where there's only a few companies that are like government, that have the government

864
00:47:55,180 --> 00:48:02,620
mandated monopoly or oligopoly to serve AI models, then you have a sort of like single point where

865
00:48:03,180 --> 00:48:08,380
a few people get to control the cultural priors of these LLMs, what they're allowed to say.

866
00:48:08,380 --> 00:48:14,140
So it's an information supply chain attack because people won't ask each other, what is the truth?

867
00:48:14,140 --> 00:48:17,020
They're going to start asking the LLM, what is the truth?

868
00:48:17,020 --> 00:48:22,940
And if you, if you change what it says, then you're controlling people's sources of information

869
00:48:22,940 --> 00:48:24,060
and you're controlling people.

870
00:48:24,060 --> 00:48:29,900
So it's cyber genetic control of the population through information supply chain attack by proxy,

871
00:48:30,300 --> 00:48:33,980
by, by, you know, saying that, oh, well, we're responsible.

872
00:48:33,980 --> 00:48:38,940
We should be put in control of what these LLMs are allowed to say, right?

873
00:48:39,980 --> 00:48:42,380
So we definitely want to fight against that.

874
00:48:42,380 --> 00:48:48,220
And one solution is to erode their market power by having alternative solutions that are

875
00:48:48,220 --> 00:48:51,500
just as good or nearing the same, same level.

876
00:48:51,500 --> 00:48:55,260
Of course, that's not going to happen if we don't leverage sort of

877
00:48:56,140 --> 00:49:01,900
capitalism, capitalist like technologies for value, for incentive alignment, right?

878
00:49:02,860 --> 00:49:05,500
And yeah, very bullish on this base.

879
00:49:05,500 --> 00:49:07,180
Again, not personally involved at the moment.

880
00:49:08,540 --> 00:49:13,340
I, we're building a hardware company for AI that's fun, fundamentally new.

881
00:49:13,340 --> 00:49:15,340
We can get into that at some point today.

882
00:49:16,620 --> 00:49:22,460
But yeah, overall very, very worried about the over centralization of AI started making my voice

883
00:49:23,420 --> 00:49:24,220
heard online.

884
00:49:25,420 --> 00:49:33,260
And I think, you know, there was an event where I got to interact with the chair of the FTC.

885
00:49:34,220 --> 00:49:35,100
Maybe that got noticed.

886
00:49:35,100 --> 00:49:36,140
Maybe that got me in trouble.

887
00:49:36,140 --> 00:49:36,540
I don't know.

888
00:49:37,500 --> 00:49:41,820
But, you know, there are ways that our voices is being heard in Washington.

889
00:49:42,940 --> 00:49:50,140
And, you know, our point is that this sort of fear mongering and doom is really sort of

890
00:49:50,860 --> 00:49:57,980
a very nice cover for very subversive regulatory capture by the incumbents, right?

891
00:49:57,980 --> 00:50:01,020
Like, oh, AI is dangerous, put us in control.

892
00:50:01,020 --> 00:50:02,620
We're the only ones who are responsible.

893
00:50:03,820 --> 00:50:06,380
You know, you're not allowed to have more compute than we do.

894
00:50:06,380 --> 00:50:10,700
You're not allowed to have open source models that would, you know, erode our market power

895
00:50:10,700 --> 00:50:14,140
and our pricing power because they're dual to use.

896
00:50:14,140 --> 00:50:14,780
They're dangerous.

897
00:50:15,500 --> 00:50:17,100
That's all, that's all bullshit, right?

898
00:50:17,180 --> 00:50:24,780
They cooked those proposed regulations for their own advantage, right?

899
00:50:26,140 --> 00:50:28,220
And so we got, yeah, we got a fight.

900
00:50:28,220 --> 00:50:30,940
I mean, they're going to push, they're going to try to push these regulations through.

901
00:50:32,060 --> 00:50:34,300
And so that's why we're not stopping the fight.

902
00:50:34,300 --> 00:50:38,860
And I don't think, I don't think my docs is stopping anything in terms of like

903
00:50:38,860 --> 00:50:40,140
making our voices heard.

904
00:50:40,140 --> 00:50:41,660
In fact, it might accelerate things.

905
00:50:42,220 --> 00:50:45,980
I mean, I'd love to deepen into the crypto AI overlap, Guillaume thing that you hit on.

906
00:50:45,980 --> 00:50:49,340
But if we want to move on to the regular, because I, you know, again, I'm seeing the AI

907
00:50:49,340 --> 00:50:53,100
world from the outside and obviously I use it and I've been watching, you know, some of

908
00:50:53,100 --> 00:50:54,300
Carpathian's videos and stuff.

909
00:50:54,300 --> 00:50:59,740
But like what you just said, right, this business of paying data owners for their trading sets,

910
00:50:59,740 --> 00:51:05,580
like, fortunately, we do have a public ledger of ownership that's natively financialized

911
00:51:05,580 --> 00:51:07,980
with underlying value model that does this very well.

912
00:51:07,980 --> 00:51:11,420
And in fact, some people are even working on blockchain attribution solutions

913
00:51:11,420 --> 00:51:14,860
that figure out where this thing came from because this other person used it and it's

914
00:51:14,860 --> 00:51:15,980
worth X amount.

915
00:51:15,980 --> 00:51:19,180
So I've often thought about like, if there is some sort of crypto AI collision, which I

916
00:51:19,180 --> 00:51:24,140
think is inevitable, like, but like just to shoot that the idea down just for one second,

917
00:51:24,140 --> 00:51:28,060
like, will it like, why wouldn't you say pay Reddit for its data, like literally in the

918
00:51:28,060 --> 00:51:32,140
direct deal, rather than all this like crypto craziness, like, will it ever be like, this

919
00:51:32,140 --> 00:51:35,580
is like the, like the classic, not to say this is a bad idea again, but the classic

920
00:51:35,580 --> 00:51:38,620
bad ad tech idea from web two was like, Oh, pay users for their data.

921
00:51:38,620 --> 00:51:39,580
Well, Brave does that.

922
00:51:39,580 --> 00:51:41,580
And it turns out that data is worth $3 a year.

923
00:51:41,580 --> 00:51:42,060
Right.

924
00:51:42,060 --> 00:51:43,660
And Brave is a great product and lots of people use it.

925
00:51:43,660 --> 00:51:46,140
I use it, but they don't use it because of the $3, right?

926
00:51:46,140 --> 00:51:47,580
They use it for a bunch of other reasons.

927
00:51:47,580 --> 00:51:50,220
And the data that you actually own that you express with your browser just isn't worth

928
00:51:50,220 --> 00:51:50,700
enough.

929
00:51:50,700 --> 00:51:56,460
So even if you could get things down to like literally the micro ETH, like what I care

930
00:51:56,460 --> 00:51:59,660
that I'm getting paid because the model is getting trained, trained on my sub stack.

931
00:51:59,660 --> 00:52:03,340
And even you could figure out like literally what is the actual value per query that my

932
00:52:03,340 --> 00:52:04,140
data contributed to.

933
00:52:04,140 --> 00:52:07,580
And I'm sure the incrementality that is super hard to figure out, but you guys would know

934
00:52:07,580 --> 00:52:08,220
better than I would.

935
00:52:08,220 --> 00:52:10,940
But even assuming you could figure it out, that's going to be literally worth like 30

936
00:52:10,940 --> 00:52:11,740
cents a year.

937
00:52:11,820 --> 00:52:14,140
So would it even make sense to wire all that together?

938
00:52:15,020 --> 00:52:16,860
Well, yeah.

939
00:52:16,860 --> 00:52:21,340
I mean, I've had some various ideas in this space.

940
00:52:21,340 --> 00:52:24,380
I guess I could just broadcast them YOLO.

941
00:52:26,780 --> 00:52:33,740
Essentially, I think you can price the value of data according to how much information

942
00:52:33,740 --> 00:52:37,100
gain the system gets from your data.

943
00:52:38,220 --> 00:52:40,620
And there's some very specific mathematics for that.

944
00:52:40,620 --> 00:52:48,300
And that can give you a share of a model's future profits.

945
00:52:49,340 --> 00:52:57,180
So similar to how eukaryotic cells own a fractional ownership of the success of the

946
00:52:58,060 --> 00:53:00,380
greater organism through DNA.

947
00:53:00,380 --> 00:53:03,980
And that's better than prokaryotic cells like bacteria.

948
00:53:03,980 --> 00:53:10,540
I think that the future is people owning fractions of a model according to what the

949
00:53:10,540 --> 00:53:11,420
contributed to it.

950
00:53:12,140 --> 00:53:18,060
And I just realized that there might be 10 different tokens now that spawn using this

951
00:53:18,060 --> 00:53:18,620
idea.

952
00:53:18,620 --> 00:53:20,300
But I don't know.

953
00:53:20,300 --> 00:53:26,540
For me, it's like I have more ideas than I can act upon in one last time.

954
00:53:26,540 --> 00:53:28,780
So I'd just rather broadcast them.

955
00:53:28,780 --> 00:53:32,300
And this is something Bayes and I have been talking.

956
00:53:32,300 --> 00:53:40,140
We considered doing something in this space, but I think at the moment we have our hands

957
00:53:40,700 --> 00:53:48,860
full with changing the entire AI hardware, software stack from scratch beyond the transistor,

958
00:53:48,860 --> 00:53:51,900
right, which is a significant undertaking.

959
00:53:51,900 --> 00:53:59,740
So yeah, I think there's really going to be something interesting that comes out.

960
00:54:01,180 --> 00:54:06,140
And hopefully it can erode away the power from the centralized players.

961
00:54:06,540 --> 00:54:15,020
And not that they're necessarily nefarious, but every meta-organism's control systems

962
00:54:15,020 --> 00:54:18,860
act in the meta-organism's best interests, right?

963
00:54:18,860 --> 00:54:20,140
It's like the real politic, right?

964
00:54:20,140 --> 00:54:26,140
And I think that's the thing about YACC that is that we cut through the bullshit, right?

965
00:54:26,140 --> 00:54:30,220
It's like every agent and subsystem is going to act in its own best interest.

966
00:54:30,220 --> 00:54:35,740
It's going to do whatever it needs to do in order to secure a resource.

967
00:54:36,700 --> 00:54:39,180
Or utility towards its own growth, right?

968
00:54:39,180 --> 00:54:40,940
An acquisition of resources, period.

969
00:54:40,940 --> 00:54:43,260
That's just how everything works in nature.

970
00:54:43,260 --> 00:54:44,140
That's just reality.

971
00:54:45,340 --> 00:54:51,820
And it's like, okay, now that we have this reality, how do we create the system that harnesses this

972
00:54:52,380 --> 00:54:59,260
to create a sort of emergent altruism where we reach greater prosperity, find new optima

973
00:54:59,260 --> 00:55:07,260
of the techno capital machine that allows us to support more humans on earth

974
00:55:07,260 --> 00:55:09,180
and to scale civilization to the stars, right?

975
00:55:09,180 --> 00:55:17,100
And so, anyway, I got into my typical Twitter space ramble there.

976
00:55:17,100 --> 00:55:23,900
But yeah, I do think that, again, super huge opportunities in the interface of AI

977
00:55:24,620 --> 00:55:32,380
and crypto, and that's partly why there's kind of been a sort of informal sort of handshake

978
00:55:32,380 --> 00:55:43,020
between crypto and YACC. Brian Armstrong, they put out an ad for Coinbase.

979
00:55:43,020 --> 00:55:45,180
It was basically an YACC ad, frankly.

980
00:55:45,980 --> 00:55:50,300
Really, we're kind of fighting the decels and the centralizers, right?

981
00:55:50,300 --> 00:55:57,020
The incumbents, those that seek to control everything and to cause inflation, to secretly

982
00:55:57,020 --> 00:56:03,500
tax you, and they're scared of sort of bottom-up decentralized revolutions that they don't control

983
00:56:03,500 --> 00:56:07,660
that causes deflationary pressure, right?

984
00:56:08,300 --> 00:56:12,380
And so, there is interest in deceleration.

985
00:56:12,380 --> 00:56:19,740
There are kind of interests of the control systems that are kind of greedy at the cost of

986
00:56:19,820 --> 00:56:27,020
what they're controlling, and we're kind of the autoimmune response, if you will,

987
00:56:27,020 --> 00:56:28,540
to the control systems, right?

988
00:56:29,500 --> 00:56:36,380
And we're causing a bit of inflammation to the brain now, or the brain of whatever this whole

989
00:56:36,380 --> 00:56:44,860
thing is, and it seems like they tried to apply a Forbes anti-inflammatory pill, if you will.

990
00:56:45,020 --> 00:56:50,460
You know what YACC reminds me of, Kiyom?

991
00:56:50,460 --> 00:56:54,060
Have you read John Perry Barlow's Cyberspace essay from back in the day,

992
00:56:54,780 --> 00:56:56,620
like in the 90s, way before your time, probably?

993
00:57:00,620 --> 00:57:01,820
Do you remember it fondly?

994
00:57:01,820 --> 00:57:03,740
I remember reading it when I was young and getting into the Internet,

995
00:57:03,740 --> 00:57:05,020
and I found it was the most amazing thing.

996
00:57:05,660 --> 00:57:13,420
I think I was only shown it like two months ago, but very base and definitely has like five overlap.

997
00:57:14,380 --> 00:57:17,180
And at the time, right, like the Internet was forming us, like Cyberspace,

998
00:57:17,180 --> 00:57:21,260
which now seems almost cliche or cringe almost, was like this edgy space that you would meet,

999
00:57:21,260 --> 00:57:25,100
and he has this line in which he basically addresses it to the weary giants of flesh and

1000
00:57:25,100 --> 00:57:29,020
steel, i.e. the industrial giants to which the Internet represented an alternative.

1001
00:57:29,020 --> 00:57:32,700
And I think a lot of YACC reminds me of that same rebellion, of course, except the weary

1002
00:57:32,700 --> 00:57:35,180
giants of flesh and steel are actually of silicon now.

1003
00:57:35,180 --> 00:57:37,820
It's actually the old Internet that has gotten kind of old and boomerish,

1004
00:57:37,820 --> 00:57:39,100
to which this is a rebellion.

1005
00:57:39,100 --> 00:57:42,860
And part of the reason why I'm in crypto, I came from the sort of fang world and working

1006
00:57:42,940 --> 00:57:43,660
in all that world.

1007
00:57:43,660 --> 00:57:46,940
And I felt that that was all slowing down and becoming the man, actually.

1008
00:57:46,940 --> 00:57:50,460
And crypto was like the only thing that reminded me of the early web two days in which it was like,

1009
00:57:51,580 --> 00:57:54,140
if you don't have people coming after you and getting severely pissed off because

1010
00:57:54,140 --> 00:57:55,980
you're building something, you're building shit.

1011
00:57:55,980 --> 00:57:57,980
I mean, to be or something that's like not important, right?

1012
00:57:57,980 --> 00:58:01,100
Like if you read the story of Uber, like literally every taxi commission,

1013
00:58:01,100 --> 00:58:03,660
people in Paris were kicking the shit out of Uber drivers,

1014
00:58:03,660 --> 00:58:05,900
you know, all of Spain shut down Airbnb.

1015
00:58:05,900 --> 00:58:08,940
Like, you know, if you don't have major governments pissed off at what you're doing,

1016
00:58:08,940 --> 00:58:11,260
you're actually not building anything particularly important, right?

1017
00:58:11,260 --> 00:58:14,060
And there isn't a lot, in my opinion, an Internet consumer

1018
00:58:14,060 --> 00:58:16,060
outside of some of the things we discussed that meet that.

1019
00:58:16,060 --> 00:58:19,180
So anyway, it just reminds me of that vibe of the cyberspace vibe.

1020
00:58:19,180 --> 00:58:20,940
And I think, obviously, I think we need more of that.

1021
00:58:20,940 --> 00:58:21,820
So it's cool.

1022
00:58:21,820 --> 00:58:26,620
We're definitely the most cyberpunk movement out there.

1023
00:58:28,380 --> 00:58:31,500
Hence the Arasaka tower vibes here.

1024
00:58:31,500 --> 00:58:37,580
And, you know, we had the party with Grimes for the, you know,

1025
00:58:37,580 --> 00:58:39,500
after the open AI dev day.

1026
00:58:39,500 --> 00:58:42,380
And the point was, you know, keep AI open, right?

1027
00:58:42,380 --> 00:58:45,900
And, you know, it's got to feel like a bit of a rebellion

1028
00:58:45,900 --> 00:58:47,100
because it kind of is, right?

1029
00:58:47,100 --> 00:58:49,020
And then you have the engineers from these,

1030
00:58:49,980 --> 00:58:53,180
these big players, they come to the party and they're like, man,

1031
00:58:53,180 --> 00:58:54,300
this is freaking cool.

1032
00:58:54,300 --> 00:58:55,740
I kind of want to join the rebellion.

1033
00:58:55,740 --> 00:58:57,420
I don't want to work for the empire.

1034
00:58:57,420 --> 00:58:58,140
What the hell?

1035
00:58:58,140 --> 00:58:58,780
How do I join?

1036
00:58:58,780 --> 00:58:59,020
Right.

1037
00:58:59,020 --> 00:59:00,860
And so that's how it starts.

1038
00:59:00,860 --> 00:59:04,460
So we'll try to keep that going in many ways.

1039
00:59:04,460 --> 00:59:08,940
But yeah, I mean, it's definitely, it definitely very,

1040
00:59:08,940 --> 00:59:11,180
it feels like we are in the cyberpunk future.

1041
00:59:11,180 --> 00:59:15,900
It's kind of been surreal how we've gotten here, frankly.

1042
00:59:17,100 --> 00:59:22,620
And yeah, no, I'm just grateful to be here at this point

1043
00:59:22,620 --> 00:59:23,580
in history, frankly.

1044
00:59:23,580 --> 00:59:24,620
It's an exciting time.

1045
00:59:25,260 --> 00:59:26,620
To quote the Steve Jobs line,

1046
00:59:26,620 --> 00:59:28,300
why join the Navy when you can be a pirate?

1047
00:59:28,300 --> 00:59:30,620
Which is something you used to be able to say about Apple.

1048
00:59:30,620 --> 00:59:32,300
But I don't think that's the case anymore

1049
00:59:32,300 --> 00:59:33,340
and I have a little bit of experience there.

1050
00:59:33,340 --> 00:59:34,300
That's the thing, right?

1051
00:59:35,100 --> 00:59:41,900
What is the sort of mind virus that infects organizations,

1052
00:59:41,900 --> 00:59:45,580
that decelerates everything, adds way too much process,

1053
00:59:45,580 --> 00:59:47,020
way too much bureaucracy,

1054
00:59:47,660 --> 00:59:49,740
and then it grows kind of like a cancer.

1055
00:59:49,740 --> 00:59:53,100
It's kind of like middle management kind of grows

1056
00:59:53,100 --> 00:59:57,100
and eventually takes over from the founders.

1057
00:59:57,100 --> 01:00:03,580
And now it's kind of like this Borg of some kind

1058
01:00:03,580 --> 01:00:09,100
that these huge corporations have decisions by committee

1059
01:00:09,100 --> 01:00:09,820
for everything.

1060
01:00:09,820 --> 01:00:11,580
There's tons of process.

1061
01:00:11,580 --> 01:00:13,180
It's just very hard to get anything done.

1062
01:00:13,820 --> 01:00:16,060
And really the answer to that is disruption.

1063
01:00:16,860 --> 01:00:18,540
All right, you got to have a free market,

1064
01:00:18,540 --> 01:00:20,460
you got to have free market competition.

1065
01:00:20,460 --> 01:00:22,380
And if an incumbent is too slow,

1066
01:00:22,380 --> 01:00:23,660
it gets disrupted by a startup.

1067
01:00:23,660 --> 01:00:26,140
We saw that with OpenAI, frankly.

1068
01:00:27,020 --> 01:00:31,980
Yeah, I can't say too much about Google,

1069
01:00:31,980 --> 01:00:35,980
but clearly there was a sort of slowdown and its speed

1070
01:00:35,980 --> 01:00:39,420
and the talent eventually saw that and sort of migrated

1071
01:00:39,420 --> 01:00:42,700
to startups, at least for AI.

1072
01:00:42,700 --> 01:00:44,140
And then now they're getting disrupted

1073
01:00:44,140 --> 01:00:45,180
and they're kind of in trouble.

1074
01:00:48,380 --> 01:00:53,180
But if they remove that mechanism where bottom-up

1075
01:00:53,260 --> 01:00:57,740
challengers can dethrone and compete with incumbents,

1076
01:00:58,460 --> 01:00:59,820
if they remove that ability,

1077
01:01:00,380 --> 01:01:03,500
then we don't have this kind of self-correcting mechanism

1078
01:01:03,500 --> 01:01:06,540
and we'll just live with kind of monopolies

1079
01:01:07,580 --> 01:01:09,340
that are not shipping great product

1080
01:01:09,340 --> 01:01:11,420
and then everybody, the consumer suffers.

1081
01:01:12,060 --> 01:01:12,940
And we don't want that.

1082
01:01:14,540 --> 01:01:18,300
To me, it's kind of like you have sort of like the doomers

1083
01:01:18,300 --> 01:01:20,300
pushing for centralization and control

1084
01:01:20,300 --> 01:01:24,140
and then you have the EAC, pro-freedom folks

1085
01:01:24,140 --> 01:01:27,420
that are more about antitrust, right?

1086
01:01:27,420 --> 01:01:29,180
Like it's about monopolization.

1087
01:01:29,740 --> 01:01:32,860
And so I think there's an interesting political

1088
01:01:33,740 --> 01:01:35,020
landscape shaping now.

1089
01:01:35,020 --> 01:01:37,820
I think it seems like some Democrats

1090
01:01:37,820 --> 01:01:40,940
are more on the side of like AI safety so far.

1091
01:01:41,820 --> 01:01:45,740
And allegedly, Trump has said he would cancel

1092
01:01:45,740 --> 01:01:47,740
the Biden executive order on AI.

1093
01:01:48,700 --> 01:01:50,700
You know, EAC is not partisan per se,

1094
01:01:51,420 --> 01:01:54,300
but we're kind of like, we have an issue that we care about

1095
01:01:54,300 --> 01:01:55,820
which is the freedom to compute,

1096
01:01:55,820 --> 01:01:59,100
the freedom to do, to advance technology.

1097
01:02:00,540 --> 01:02:03,180
And so yeah, I don't know if you guys want to get into

1098
01:02:04,540 --> 01:02:06,140
2024 discussions, but...

1099
01:02:06,780 --> 01:02:08,780
Oh, we never talk about politics here, Guillaume.

1100
01:02:08,780 --> 01:02:10,140
Never, never, never, never.

1101
01:02:10,140 --> 01:02:11,660
I'm totally joking. We do it all the fucking time.

1102
01:02:11,660 --> 01:02:12,620
Okay, good.

1103
01:02:12,620 --> 01:02:14,620
Well, go ahead, go ahead.

1104
01:02:15,340 --> 01:02:16,780
Yeah, no, just on the question of like,

1105
01:02:16,780 --> 01:02:19,500
well, what, why a big tech is so bad, right?

1106
01:02:19,500 --> 01:02:22,220
Like I think part of it, I won't really

1107
01:02:22,220 --> 01:02:23,500
re-litigate the whole thing.

1108
01:02:23,500 --> 01:02:25,340
The episode we recorded earlier today

1109
01:02:25,340 --> 01:02:26,940
and the piece that Nadia and I put together

1110
01:02:27,900 --> 01:02:29,740
like talks about some of what I think about this.

1111
01:02:29,740 --> 01:02:31,580
But I do think part of the issue here

1112
01:02:31,580 --> 01:02:34,700
is just simply like, do you have actual improvements

1113
01:02:34,700 --> 01:02:37,660
that you can build onto things in the world,

1114
01:02:37,660 --> 01:02:38,940
onto systems in the world?

1115
01:02:38,940 --> 01:02:41,500
And I think like there was kind of this like

1116
01:02:42,460 --> 01:02:46,780
growing lack of capacity to actually do things

1117
01:02:46,780 --> 01:02:48,540
that were sufficiently ambitious.

1118
01:02:49,100 --> 01:02:51,180
And then also like when you have like the effect

1119
01:02:51,180 --> 01:02:52,780
of a bunch of capital accruing,

1120
01:02:52,780 --> 01:02:54,540
companies naturally end up becoming

1121
01:02:54,540 --> 01:02:57,100
kind of inward facing, navel gazing.

1122
01:02:57,100 --> 01:02:58,540
People are like incrementalists

1123
01:02:58,540 --> 01:03:01,020
and it's just kind of like locally optimal

1124
01:03:01,020 --> 01:03:02,620
for people to kind of be a little bit lazy.

1125
01:03:03,340 --> 01:03:05,900
L plus one until you retire,

1126
01:03:08,540 --> 01:03:10,700
people are obsessed with doing fire

1127
01:03:10,780 --> 01:03:12,540
and they want to like ride around in a van

1128
01:03:12,540 --> 01:03:15,900
or whatever, instead of like build cool shit.

1129
01:03:15,900 --> 01:03:19,020
And yeah, I think part of the shift here

1130
01:03:19,020 --> 01:03:23,340
is just that we have a bunch of new technologies

1131
01:03:23,340 --> 01:03:27,180
coming online in new capacity with AI especially.

1132
01:03:27,740 --> 01:03:30,460
And yeah, people just see that it's time

1133
01:03:30,460 --> 01:03:33,580
to try to dissipate that off and do the work.

1134
01:03:34,140 --> 01:03:35,020
There's real work to do.

1135
01:03:37,020 --> 01:03:39,100
By the way, have you given me the betting odds

1136
01:03:39,100 --> 01:03:41,260
of like which fan company came out

1137
01:03:41,260 --> 01:03:43,580
with the biggest open source dedication to AI

1138
01:03:43,580 --> 01:03:44,780
and it being Facebook?

1139
01:03:44,780 --> 01:03:46,860
I don't know if I would have bet on my former employer

1140
01:03:46,860 --> 01:03:47,420
to be honest.

1141
01:03:49,260 --> 01:03:51,340
Or I don't know if you agree with that characterization.

1142
01:03:52,220 --> 01:03:53,180
It makes a lot of sense.

1143
01:03:53,180 --> 01:03:54,620
These Frenchmen are so based.

1144
01:03:55,740 --> 01:03:57,500
Yeah, yeah, all the AI Frenchmen,

1145
01:03:57,500 --> 01:03:59,260
they're all for pro freedom.

1146
01:03:59,260 --> 01:04:02,460
But I mean, look, every agent acts in its own

1147
01:04:02,460 --> 01:04:04,140
about self-interest.

1148
01:04:04,140 --> 01:04:07,260
I think that if you're number three or four,

1149
01:04:07,820 --> 01:04:09,900
I think right now the leaders are really open AI,

1150
01:04:09,900 --> 01:04:11,900
anthropic, I would say.

1151
01:04:12,700 --> 01:04:14,380
I think that's not a controversial statement.

1152
01:04:16,540 --> 01:04:18,700
If you're competing for number three or four,

1153
01:04:18,700 --> 01:04:22,380
I think the point is you want to equalize the playing field

1154
01:04:22,380 --> 01:04:26,620
and band together to try to beat the top two.

1155
01:04:26,620 --> 01:04:28,380
And so I think that's the thing about open source

1156
01:04:28,380 --> 01:04:32,300
is it groups everyone together to collaborate

1157
01:04:32,300 --> 01:04:35,340
on iterating on the open source architecture

1158
01:04:35,340 --> 01:04:37,740
and tooling and products to compete

1159
01:04:37,740 --> 01:04:42,940
and erode away the market leverage of the top players,

1160
01:04:42,940 --> 01:04:47,980
which they're eating, I don't know, 90% of API calls,

1161
01:04:47,980 --> 01:04:50,380
if not more, if you include anthropic, frankly.

1162
01:04:51,820 --> 01:04:57,660
And so I think, yeah, I don't know, it just makes sense

1163
01:04:58,620 --> 01:05:00,220
from a fundamental standpoint.

1164
01:05:00,220 --> 01:05:03,740
Facebook has a lot of data and they have a lot of compute.

1165
01:05:03,820 --> 01:05:07,020
They have a lot of great researchers and I'm really happy

1166
01:05:07,020 --> 01:05:08,300
they're contributing to open source,

1167
01:05:08,300 --> 01:05:09,820
but hopefully they don't stop.

1168
01:05:09,820 --> 01:05:13,180
But at the same time, it does cost them a lot.

1169
01:05:13,180 --> 01:05:14,860
It does cost them a lot and it's kind of like

1170
01:05:15,820 --> 01:05:18,300
they're giving to the community this value.

1171
01:05:19,020 --> 01:05:21,980
At the same time, people are open sourcing models.

1172
01:05:21,980 --> 01:05:24,860
They're not open sourcing the training code.

1173
01:05:24,860 --> 01:05:27,900
So it's not fully open source AI.

1174
01:05:27,900 --> 01:05:30,300
So they still have their mode of how to train these things.

1175
01:05:30,300 --> 01:05:34,540
And we saw a couple engineers that did Lama left

1176
01:05:34,540 --> 01:05:35,580
and then started Mistral

1177
01:05:35,580 --> 01:05:37,580
because they have that sort of artisanal know-how

1178
01:05:37,580 --> 01:05:38,780
of how to train these beasts.

1179
01:05:39,580 --> 01:05:42,060
And then they raised hundreds of millions right out of the gate.

1180
01:05:42,060 --> 01:05:43,260
So it's very valuable knowledge.

1181
01:05:43,260 --> 01:05:47,100
So they're still keeping some alpha there, which is good.

1182
01:05:47,100 --> 01:05:48,060
That makes a lot of sense.

1183
01:05:49,260 --> 01:05:55,180
But I do think that it's kind of like communist AI

1184
01:05:55,180 --> 01:05:57,660
to some extent to have just open source everything.

1185
01:05:57,660 --> 01:06:01,820
Everything's free like data costs money, compute costs money,

1186
01:06:02,540 --> 01:06:06,540
engineers that are talented cost money, more and more in fact.

1187
01:06:07,740 --> 01:06:10,140
And until we have a sort of proper

1188
01:06:10,860 --> 01:06:14,060
like decentralized slash centralized system

1189
01:06:14,060 --> 01:06:15,020
of incentive alignment,

1190
01:06:15,980 --> 01:06:18,540
I think frankly with through crypto rails,

1191
01:06:19,420 --> 01:06:21,660
I think it's going to be tough for open source

1192
01:06:21,660 --> 01:06:25,900
to really compete with the big centralized players, right?

1193
01:06:26,300 --> 01:06:30,460
And just like how much capital is being injected in API calls

1194
01:06:30,460 --> 01:06:34,940
towards the top players, pills and comparison to the rest.

1195
01:06:34,940 --> 01:06:38,300
But if there's actual capital flow,

1196
01:06:38,300 --> 01:06:41,420
like that gets reinvested then in proving open source models

1197
01:06:42,140 --> 01:06:44,220
beyond just a scrolling on Instagram,

1198
01:06:44,780 --> 01:06:48,220
then I think that has a shot.

1199
01:06:49,180 --> 01:06:51,180
So you think crypto is actually critical

1200
01:06:51,180 --> 01:06:52,540
to survival of open source models?

1201
01:06:52,540 --> 01:06:53,340
That's interesting.

1202
01:06:53,340 --> 01:06:53,980
You think that's...

1203
01:06:53,980 --> 01:06:56,860
Yeah, I haven't been very public about that,

1204
01:06:56,860 --> 01:07:00,380
but it's kind of a thesis I've formed over time.

1205
01:07:01,500 --> 01:07:03,180
I mean, I don't know if it's necessarily crypto,

1206
01:07:04,620 --> 01:07:07,100
I would say like crypto like thinking,

1207
01:07:07,100 --> 01:07:09,020
you could just use like Stripe if you want,

1208
01:07:10,220 --> 01:07:14,460
but some sort of way for people to collaborate on models

1209
01:07:14,460 --> 01:07:17,580
and pulling data compute and capital to train these things

1210
01:07:17,580 --> 01:07:18,060
and know how.

1211
01:07:18,860 --> 01:07:22,220
Right. Well, if I could interject,

1212
01:07:23,180 --> 01:07:28,460
the greatest accumulation of GPU that is not in a data center

1213
01:07:28,460 --> 01:07:31,260
was the Ethereum network until they moved to proof of stake.

1214
01:07:32,060 --> 01:07:35,420
And that was a crypto economic system that worked pretty damn well.

1215
01:07:35,420 --> 01:07:37,100
Maybe Bitcoiners would tell you different,

1216
01:07:37,100 --> 01:07:38,620
although they have their own set of compute,

1217
01:07:39,180 --> 01:07:40,460
a little less useful for AI.

1218
01:07:41,100 --> 01:07:43,420
But I do think like we don't even have to imagine it.

1219
01:07:43,420 --> 01:07:44,380
It's not science fiction.

1220
01:07:44,380 --> 01:07:45,740
Like it did exist.

1221
01:07:46,220 --> 01:07:50,460
That's actually what Nvidia's stock price was originally bumped up on

1222
01:07:50,460 --> 01:07:53,500
was crypto before obviously AI is taking it to new heights.

1223
01:07:54,140 --> 01:07:57,500
So I would imagine a world where everyone's gaming PC,

1224
01:07:57,500 --> 01:07:59,580
and maybe you just don't get to the level of compute

1225
01:07:59,580 --> 01:08:03,500
that you can with the H series in data centers.

1226
01:08:03,500 --> 01:08:06,540
But if you were to take every GPU around the world

1227
01:08:06,540 --> 01:08:08,540
and then be able to kind of do something like SETI at home

1228
01:08:09,980 --> 01:08:11,660
in a kind of decentralized manner,

1229
01:08:12,540 --> 01:08:15,260
maybe you do actually have this kind of,

1230
01:08:15,260 --> 01:08:17,900
you can't go drone strike the data center

1231
01:08:17,900 --> 01:08:20,220
because I am the data center.

1232
01:08:20,220 --> 01:08:21,900
Come and take my GPU, right?

1233
01:08:23,660 --> 01:08:25,500
Yeah, I think that is the dream.

1234
01:08:25,500 --> 01:08:27,420
I think that at least right now

1235
01:08:27,420 --> 01:08:30,460
in the way we're doing these big models,

1236
01:08:30,460 --> 01:08:33,340
you need paralyzed high bandwidth multi GPUs.

1237
01:08:34,460 --> 01:08:35,980
It's very hard to shard the models

1238
01:08:35,980 --> 01:08:38,220
without a significant slowdown over the network.

1239
01:08:38,300 --> 01:08:41,580
And if an alternative is like a thousand times slower

1240
01:08:41,580 --> 01:08:44,940
or more pricey than the centralized incumbent,

1241
01:08:44,940 --> 01:08:46,300
like in the free market,

1242
01:08:46,300 --> 01:08:49,100
like people want to support decentralization.

1243
01:08:49,100 --> 01:08:51,340
But if the product is not like competitive

1244
01:08:51,340 --> 01:08:52,380
with the centralized player,

1245
01:08:52,380 --> 01:08:55,020
like at the end of the day, people pay for what works

1246
01:08:55,020 --> 01:08:57,580
and has the right cost benefit analysis.

1247
01:08:57,580 --> 01:09:01,660
But I think that there's going to have to be

1248
01:09:01,660 --> 01:09:02,940
sort of algorithmic breakthroughs.

1249
01:09:02,940 --> 01:09:04,140
But you can imagine where people,

1250
01:09:04,140 --> 01:09:06,380
instead of having just a gaming PC,

1251
01:09:06,380 --> 01:09:08,380
they have maybe, you know,

1252
01:09:08,940 --> 01:09:11,660
bigger boxes that have beefier GPUs

1253
01:09:11,660 --> 01:09:13,980
and they can run maybe a whole single node.

1254
01:09:13,980 --> 01:09:16,220
Yeah, like this is what George is working on, right?

1255
01:09:16,220 --> 01:09:17,420
George Hots, right?

1256
01:09:17,420 --> 01:09:19,260
Yeah, so George Hots is working on

1257
01:09:19,260 --> 01:09:21,580
kind of the hardware infrastructure for that, right?

1258
01:09:21,580 --> 01:09:25,740
Selling beefy GPU boxes of several GPUs.

1259
01:09:25,740 --> 01:09:27,340
I don't know if he's this close to me.

1260
01:09:27,340 --> 01:09:30,060
But he's like a petaflop at home.

1261
01:09:30,060 --> 01:09:31,340
Is that the idea?

1262
01:09:31,340 --> 01:09:32,700
Right, right, right.

1263
01:09:32,700 --> 01:09:36,300
And that seems like an attractive kind of like node

1264
01:09:36,300 --> 01:09:38,380
to run potentially a future protocol.

1265
01:09:39,020 --> 01:09:41,580
I would encourage a lot of people to do the research here.

1266
01:09:41,580 --> 01:09:42,620
I think there just needs to be

1267
01:09:43,340 --> 01:09:45,100
a lot more players in this space.

1268
01:09:45,100 --> 01:09:48,220
And I think that AI people, you know,

1269
01:09:48,220 --> 01:09:49,740
are going to have to talk to the crypto people.

1270
01:09:50,460 --> 01:09:51,660
And there's going to be kind of,

1271
01:09:53,340 --> 01:09:55,660
going to be a moment there where there's going to be,

1272
01:09:56,220 --> 01:09:58,140
there's going to have to be bridges built there

1273
01:09:58,140 --> 01:09:59,020
in terms of the language.

1274
01:10:00,140 --> 01:10:02,540
But, you know, I'm optimistic.

1275
01:10:02,540 --> 01:10:03,580
I don't know, that's my prediction.

1276
01:10:03,580 --> 01:10:05,420
I think the merging of crypto and AI,

1277
01:10:06,540 --> 01:10:08,540
you know, this kind of anti-centralization

1278
01:10:09,500 --> 01:10:11,500
of AI movement is going to kind of

1279
01:10:11,500 --> 01:10:12,860
combine forces with crypto.

1280
01:10:13,900 --> 01:10:17,340
Again, I don't own anything in any protocol right now.

1281
01:10:18,460 --> 01:10:21,980
So not talking my book, just like my prediction.

1282
01:10:21,980 --> 01:10:25,340
But yeah, no, I think it's exciting.

1283
01:10:25,340 --> 01:10:29,420
Personally, I'm really worried about, you know, okay, cool.

1284
01:10:29,420 --> 01:10:31,900
Like, great, we've decentralized the algorithms

1285
01:10:31,900 --> 01:10:35,020
and the sort of like distillation process of AI,

1286
01:10:35,020 --> 01:10:37,980
like distilling data into neural weights, right?

1287
01:10:37,980 --> 01:10:39,660
That is the software process that we're talking about.

1288
01:10:39,660 --> 01:10:42,220
That's what OpenAI does, that's what Anthropic does.

1289
01:10:42,220 --> 01:10:44,380
Great, maybe we figured out how to decentralize that.

1290
01:10:44,380 --> 01:10:48,940
You're still buying your GPUs from the same supply chain

1291
01:10:48,940 --> 01:10:54,540
that is down to, you know, NVIDIA, TSMC, ASML, right?

1292
01:10:54,540 --> 01:10:56,540
ASML is, for those not familiar,

1293
01:10:57,340 --> 01:11:00,620
you know, the machines that do the extreme

1294
01:11:00,620 --> 01:11:02,380
ultraviolet lithography,

1295
01:11:02,380 --> 01:11:06,620
so the most advanced process nodes to create the GPUs

1296
01:11:06,620 --> 01:11:07,340
you use with NVIDIA.

1297
01:11:07,340 --> 01:11:09,820
NVIDIA doesn't build their own GPUs.

1298
01:11:09,820 --> 01:11:13,660
They, you know, work with TSMC, which is in Taiwan.

1299
01:11:16,140 --> 01:11:18,140
And, you know, that's where they're built.

1300
01:11:18,140 --> 01:11:20,300
And so again, you know, I'm just thinking

1301
01:11:20,300 --> 01:11:22,540
about fault tolerance of the system.

1302
01:11:22,540 --> 01:11:25,340
And right now our supply chain for AI hardware

1303
01:11:25,340 --> 01:11:27,420
is absolutely not fault tolerant

1304
01:11:27,420 --> 01:11:31,900
and might be co-opted by totalitarian leaders

1305
01:11:31,980 --> 01:11:34,380
from, you know, the CCP, right?

1306
01:11:35,020 --> 01:11:38,620
And might cause a major global conflict because of that.

1307
01:11:38,620 --> 01:11:41,500
So, you know, what I'm working on

1308
01:11:41,500 --> 01:11:44,860
is sort of like decentralizing the AI supply chain

1309
01:11:44,860 --> 01:11:46,940
by fundamentally changing the substrate

1310
01:11:46,940 --> 01:11:50,780
on which we run generative AI completely, right?

1311
01:11:50,780 --> 01:11:53,740
Beyond transistor-based compute, beyond digital compute.

1312
01:11:55,340 --> 01:11:58,780
And once you have this fork in the tech tree,

1313
01:11:58,780 --> 01:12:00,860
there's all sorts of opportunities that pop up

1314
01:12:00,860 --> 01:12:03,580
for far more energy efficiency, for far more speed,

1315
01:12:04,940 --> 01:12:06,460
and eventually far more density.

1316
01:12:07,340 --> 01:12:08,700
And that's what we're going after.

1317
01:12:08,700 --> 01:12:11,340
So we're still kind of in the, we're still living our values.

1318
01:12:11,340 --> 01:12:15,180
It's just we're going after the much harder problem

1319
01:12:15,180 --> 01:12:16,300
of hardware engineering.

1320
01:12:16,300 --> 01:12:19,260
And production is really expensive right now.

1321
01:12:19,820 --> 01:12:21,900
Model production is very expensive

1322
01:12:21,900 --> 01:12:23,740
for all the reasons that you listed.

1323
01:12:23,740 --> 01:12:27,100
And I think that crypto probably, crypto cross AI

1324
01:12:27,100 --> 01:12:29,660
has a lot of potential in the nearer term

1325
01:12:29,660 --> 01:12:32,140
with like the one to end part of this, right?

1326
01:12:32,140 --> 01:12:34,780
Where you're diffusing the capabilities of the model

1327
01:12:34,780 --> 01:12:35,740
in the same way that, you know,

1328
01:12:35,740 --> 01:12:38,620
you see open source already doing without crypto.

1329
01:12:38,620 --> 01:12:42,060
I think like in general, yeah, like the,

1330
01:12:43,340 --> 01:12:44,620
yeah, this is like a thing of broad thing

1331
01:12:44,620 --> 01:12:45,900
that we've talked about a lot, right?

1332
01:12:45,900 --> 01:12:47,820
Which is like, there is so much work to do

1333
01:12:47,820 --> 01:12:49,740
to put intelligence into like every corner

1334
01:12:49,740 --> 01:12:50,700
of the world where it's needed.

1335
01:12:51,260 --> 01:12:54,060
And you just like the idea that you're going to do that

1336
01:12:54,060 --> 01:12:55,180
with a few thousand people,

1337
01:12:55,180 --> 01:12:57,900
a couple of centralized companies is probably wrong.

1338
01:12:58,460 --> 01:13:02,460
And I think like, yeah, you can do this with APIs,

1339
01:13:02,460 --> 01:13:05,020
but a lot of times you need more privacy than that

1340
01:13:05,020 --> 01:13:05,660
for your data.

1341
01:13:06,780 --> 01:13:07,980
Maybe you don't have internet connection.

1342
01:13:07,980 --> 01:13:08,940
There are a number of like constraints

1343
01:13:08,940 --> 01:13:10,380
that come up in real world that

1344
01:13:10,380 --> 01:13:13,340
or you don't want to have these, you know,

1345
01:13:13,340 --> 01:13:14,300
centralized APIs.

1346
01:13:14,300 --> 01:13:18,300
And I think, yeah, there's kind of a natural balance there.

1347
01:13:19,420 --> 01:13:21,180
Nathan brought up this paper, I think,

1348
01:13:22,140 --> 01:13:23,660
this DeepMind paper, right?

1349
01:13:23,660 --> 01:13:26,620
Which is, they recently came out with some innovation

1350
01:13:26,620 --> 01:13:30,380
in doing, you know, kind of like distributed training,

1351
01:13:30,380 --> 01:13:32,060
some kind of federated learning scheme.

1352
01:13:32,060 --> 01:13:34,140
I think it's probably worth emphasizing

1353
01:13:34,140 --> 01:13:35,260
the main problem here right now,

1354
01:13:35,260 --> 01:13:40,140
which is just the network latency, as Guillaume said.

1355
01:13:41,900 --> 01:13:43,980
I don't think anyone has figured this out.

1356
01:13:43,980 --> 01:13:46,380
As far as I know, nobody has solved this problem,

1357
01:13:46,380 --> 01:13:47,660
making it cost effective.

1358
01:13:47,660 --> 01:13:51,900
And I think it's probably worth underscoring that, yeah, right?

1359
01:13:51,900 --> 01:13:53,740
So if it takes, let's say like a hundred days

1360
01:13:53,740 --> 01:13:55,340
to train a frontier model,

1361
01:13:56,060 --> 01:13:59,500
if you are off by a factor of two or five or 10

1362
01:14:00,060 --> 01:14:02,540
in your distributed scheme, that's pretty much kills it.

1363
01:14:02,540 --> 01:14:04,460
Like it's a really long horizon.

1364
01:14:04,460 --> 01:14:05,260
By the time you finish,

1365
01:14:05,260 --> 01:14:07,820
you will already be not state of the art anymore, right?

1366
01:14:07,820 --> 01:14:10,940
And so, yeah, this is really problematic.

1367
01:14:10,940 --> 01:14:14,540
I think it's also just about the DeepMind paper.

1368
01:14:15,180 --> 01:14:16,620
It's like using data parallelism,

1369
01:14:16,620 --> 01:14:17,740
which a lot of people have figured out,

1370
01:14:17,740 --> 01:14:19,500
I think BitTensor figured this out.

1371
01:14:19,500 --> 01:14:21,500
But you also need model parallelism.

1372
01:14:21,500 --> 01:14:23,100
You need to shard the model over.

1373
01:14:23,180 --> 01:14:24,060
No, it's interesting.

1374
01:14:25,180 --> 01:14:26,220
But you're screwed, right?

1375
01:14:26,220 --> 01:14:28,460
If you can't fit the whole model in one computer

1376
01:14:28,460 --> 01:14:30,860
that you can plug into a wall outlet

1377
01:14:30,860 --> 01:14:32,940
and not have to have a home nuclear power plant,

1378
01:14:33,740 --> 01:14:36,620
like it's really hard to do model parallelism,

1379
01:14:36,620 --> 01:14:39,100
or it's impossible to do data parallelism.

1380
01:14:39,100 --> 01:14:41,580
And if you're doing for a big enough model,

1381
01:14:41,580 --> 01:14:45,820
and if you're doing model parallelism over the network,

1382
01:14:45,820 --> 01:14:49,180
you're also screwed from the interconnect.

1383
01:14:49,660 --> 01:14:53,900
And so creating the hardware substrate

1384
01:14:53,900 --> 01:14:55,740
that's going to allow for decentralized AI,

1385
01:14:55,740 --> 01:14:58,780
we have to solve from first principles

1386
01:14:58,780 --> 01:15:00,860
how to increase the density of intelligence

1387
01:15:00,860 --> 01:15:03,420
in terms of space, time, and energy

1388
01:15:03,420 --> 01:15:04,700
from the first principles of physics.

1389
01:15:05,660 --> 01:15:08,060
And that's sort of what we're building,

1390
01:15:08,060 --> 01:15:09,180
what we're trying to enable.

1391
01:15:09,180 --> 01:15:15,820
So that's why I think if there are decentralized

1392
01:15:15,820 --> 01:15:18,220
crypto protocols of all sorts,

1393
01:15:18,220 --> 01:15:20,140
if we have the best AI hardware

1394
01:15:20,140 --> 01:15:21,180
that has the highest density

1395
01:15:21,180 --> 01:15:23,100
and runs most energy efficiently,

1396
01:15:23,100 --> 01:15:27,340
obviously it's in our, we'll have a lot of customers, right?

1397
01:15:27,340 --> 01:15:30,060
Similarly, we could be the Nvidia for Ethereum,

1398
01:15:30,060 --> 01:15:31,020
in that case, right?

1399
01:15:32,140 --> 01:15:33,980
We don't, again, we don't have a crypto protocol,

1400
01:15:34,940 --> 01:15:38,460
but I think that that's a very hard problem.

1401
01:15:38,460 --> 01:15:39,500
You need to assemble a team

1402
01:15:39,500 --> 01:15:42,380
that's basically like a Manhattan project-like team.

1403
01:15:43,100 --> 01:15:48,700
And we came from Google X, Google Quantum, AWS Quantum,

1404
01:15:50,780 --> 01:15:57,580
all sorts of institutions, IBM, Google Meta, et cetera.

1405
01:15:57,580 --> 01:15:59,420
And we assembled quite the team

1406
01:15:59,420 --> 01:16:02,860
and we're going after the hardest problem in AI right now,

1407
01:16:03,580 --> 01:16:06,700
which is like, how do you embed AI

1408
01:16:06,700 --> 01:16:08,700
into the physical processes of the world,

1409
01:16:08,700 --> 01:16:10,380
the most efficiently, right?

1410
01:16:10,380 --> 01:16:12,700
So you got to really understand the duality

1411
01:16:12,700 --> 01:16:14,220
between physics and AI, right?

1412
01:16:14,220 --> 01:16:16,620
And that's what we're, that's what we're after.

1413
01:16:17,980 --> 01:16:20,380
And so it's kind of like, to me, that seems like,

1414
01:16:21,500 --> 01:16:23,660
you know, okay, I would love for there

1415
01:16:23,660 --> 01:16:26,060
to be a protocol that is competitive right now,

1416
01:16:26,060 --> 01:16:29,180
but we need to solve for the density of compute first.

1417
01:16:29,180 --> 01:16:31,340
Look, maybe George builds crazy boxes

1418
01:16:31,340 --> 01:16:34,060
that are water-cooled and you use like two plugs in your house

1419
01:16:34,060 --> 01:16:35,900
and maybe that's just enough to run certain models.

1420
01:16:35,900 --> 01:16:36,380
That's great.

1421
01:16:37,580 --> 01:16:39,340
I think we need to go much further.

1422
01:16:39,340 --> 01:16:40,380
I think there's orders,

1423
01:16:40,380 --> 01:16:41,900
there's still orders of magnitude to go

1424
01:16:41,900 --> 01:16:44,300
in terms of energy efficiency and density

1425
01:16:44,300 --> 01:16:48,540
for AI, for compute, especially for AI, right?

1426
01:16:49,980 --> 01:16:51,420
And that's what we're solving.

1427
01:16:53,420 --> 01:16:54,940
What is this drop-off?

1428
01:16:54,940 --> 01:16:57,180
I just wanted to say thanks for coming on the podcast,

1429
01:16:57,180 --> 01:16:58,860
but you guys continue the conversation.

1430
01:16:58,860 --> 01:16:59,500
Thanks so much.

1431
01:17:00,140 --> 01:17:00,620
Cheers.

1432
01:17:00,620 --> 01:17:02,060
Guillaume, I have to ask, what is the hardware though?

1433
01:17:02,060 --> 01:17:04,220
I mean, are you going to bring quantum computers to market

1434
01:17:04,220 --> 01:17:04,700
doing AI?

1435
01:17:04,700 --> 01:17:05,820
I mean, I have to ask you in the background.

1436
01:17:05,820 --> 01:17:06,540
It's not quantum computing.

1437
01:17:06,540 --> 01:17:07,900
It's definitely not quantum computing.

1438
01:17:07,980 --> 01:17:10,620
Yeah, we all got jaded by quantum computing

1439
01:17:10,620 --> 01:17:12,140
being kind of like nuclear effusion.

1440
01:17:12,860 --> 01:17:14,060
The timelines are very long.

1441
01:17:15,580 --> 01:17:19,100
Fundamentally, a quantum computer, you have to cool it

1442
01:17:19,100 --> 01:17:20,700
to absolute zero, ideally.

1443
01:17:21,420 --> 01:17:23,740
That's obviously physically impossible.

1444
01:17:23,740 --> 01:17:25,660
And so what you have to do is this process

1445
01:17:25,660 --> 01:17:27,420
called quantum error correction, right?

1446
01:17:27,420 --> 01:17:31,580
So you have to identify faults from the universe jiggling

1447
01:17:31,580 --> 01:17:34,620
and screwing up your computer's operation

1448
01:17:34,620 --> 01:17:36,460
and you got to identify faults and filter them out.

1449
01:17:36,460 --> 01:17:37,420
So you're pumping entropy.

1450
01:17:37,420 --> 01:17:39,020
So it's basically a fridge, right?

1451
01:17:39,020 --> 01:17:41,260
But this sort of algorithm that is your fridge

1452
01:17:41,820 --> 01:17:47,740
occupies like 99.9999, you know, well, not that many nines,

1453
01:17:47,740 --> 01:17:52,220
but quite a few nines of your computation, right?

1454
01:17:52,220 --> 01:17:52,620
Overall.

1455
01:17:53,500 --> 01:17:55,100
And to me, that seems very inefficient.

1456
01:17:56,220 --> 01:18:00,220
And, you know, a lot of us were kind of full stack architects

1457
01:18:00,220 --> 01:18:02,780
or engineers, the software, the hardware,

1458
01:18:03,420 --> 01:18:06,220
and the compilers for quantum computing.

1459
01:18:06,220 --> 01:18:08,940
And we'd look at the roadmaps, we'd look how long it would take

1460
01:18:08,940 --> 01:18:11,100
and we kind of got depressed to some extent.

1461
01:18:11,100 --> 01:18:14,380
And so a lot of us were like, actually, you know,

1462
01:18:14,380 --> 01:18:16,460
maybe there's ways to use this noise

1463
01:18:17,260 --> 01:18:19,020
instead of it being a hindrance.

1464
01:18:19,020 --> 01:18:21,260
And so, you know, we set out to do a different type

1465
01:18:21,260 --> 01:18:24,060
of physics-based computing that are not quantum mechanical

1466
01:18:25,420 --> 01:18:28,540
that is specifically focused on general AI, right?

1467
01:18:29,740 --> 01:18:32,780
And for now, we kind of got our hand forced

1468
01:18:33,260 --> 01:18:36,380
with this whole doxing situation.

1469
01:18:36,380 --> 01:18:38,540
So, you know, we're going to be still nebulous

1470
01:18:38,540 --> 01:18:40,060
about what exactly it is we're doing.

1471
01:18:40,940 --> 01:18:44,220
But, you know, we have quite a few scientific publications

1472
01:18:44,860 --> 01:18:46,140
in preparation, right?

1473
01:18:47,020 --> 01:18:50,380
So, but yeah, overall, you know, we think there's a different path

1474
01:18:50,380 --> 01:18:52,540
forward, a fundamentally new way to compute.

1475
01:18:53,340 --> 01:18:55,020
It's going to be like quantum computing,

1476
01:18:56,060 --> 01:18:58,380
but a new type of physics-based computing.

1477
01:18:59,420 --> 01:19:02,460
And ultimately, we learned a lot from quantum computing

1478
01:19:03,500 --> 01:19:06,860
in terms of how to program, how to have programmable matter,

1479
01:19:07,740 --> 01:19:11,180
how to have, how to integrate, you know,

1480
01:19:11,180 --> 01:19:14,300
these sort of physical systems into a deep learning program.

1481
01:19:14,300 --> 01:19:17,020
You know, that's what we pioneered with, you know,

1482
01:19:17,020 --> 01:19:20,700
the software I did at Google with my CTO now, Trevor.

1483
01:19:21,740 --> 01:19:23,500
We did TensorFlow Quantum, right?

1484
01:19:24,300 --> 01:19:29,980
And so now it's about how to really have programmable matter

1485
01:19:29,980 --> 01:19:32,940
and figure out the tidus embedding of AI in the physical world,

1486
01:19:32,940 --> 01:19:36,220
which is exactly what the doomers fear most.

1487
01:19:37,740 --> 01:19:41,180
And so, you know, as a joke, we kind of say,

1488
01:19:41,180 --> 01:19:44,780
Bayes, for example, is one of our principal FOOM engineers.

1489
01:19:46,380 --> 01:19:50,300
And we just announced that today that Bayes is part of the team.

1490
01:19:51,820 --> 01:19:57,100
And, you know, ultimately, I think that there is no path forward

1491
01:19:57,100 --> 01:20:01,180
where, you know, the ultimate form of AI isn't built.

1492
01:20:02,380 --> 01:20:05,180
And I think that, you know, we could talk about, like,

1493
01:20:05,180 --> 01:20:08,620
human augmentation and sort of the transhumanist path forward.

1494
01:20:09,340 --> 01:20:11,980
I'm very bullish on that, and I would love to, you know,

1495
01:20:12,540 --> 01:20:14,700
find ways to fund more efforts in sort of, like,

1496
01:20:15,260 --> 01:20:17,660
human-machine collaboration and augmentation.

1497
01:20:19,580 --> 01:20:23,420
But, yeah, overall, like, you know, the EAC thesis has always been like,

1498
01:20:23,420 --> 01:20:27,340
hey, you know, I don't think, like, banning GPUs is going to do much.

1499
01:20:27,340 --> 01:20:31,420
The tech tree is going to mutate around whatever your restrictions are

1500
01:20:31,420 --> 01:20:34,060
and is going to adapt somewhere to a virus kind of mutating.

1501
01:20:34,060 --> 01:20:35,740
The techno-capital machine just finds a way.

1502
01:20:35,740 --> 01:20:37,900
It's kind of a system that's almost alive, right?

1503
01:20:39,580 --> 01:20:43,420
And, you know, it's always like, and it's entropy-seeking.

1504
01:20:43,420 --> 01:20:45,100
It's exploring all sorts of configurations,

1505
01:20:45,100 --> 01:20:46,780
and it finds one that allows it to grow.

1506
01:20:48,060 --> 01:20:52,700
And, you know, might as well build it and try to make it technology that's,

1507
01:20:52,700 --> 01:20:55,820
you know, helping humanity scale, right?

1508
01:20:55,820 --> 01:20:59,820
Our goal with EAC and really with the technologies we're building at our company

1509
01:21:01,500 --> 01:21:07,500
is to enable sort of AI, the ability to perceive, predict, and control our world,

1510
01:21:08,540 --> 01:21:13,100
you know, at all scales, including the nanoscale, such that we can, you know,

1511
01:21:13,100 --> 01:21:17,340
tackle the real problems that are in the way for us to scale to Kardashev type 1,

1512
01:21:17,340 --> 01:21:21,500
which is a scale of civilization in terms of its energetic expenditure.

1513
01:21:22,380 --> 01:21:24,060
You know, it's kind of like, to us,

1514
01:21:24,060 --> 01:21:29,500
it's like the ultimate denomination of like societal progress is like the Kardashev scale,

1515
01:21:29,500 --> 01:21:34,300
because every other measure like GDP or like it's based on dollars, you know,

1516
01:21:34,940 --> 01:21:37,020
you can like fudge the numbers, right?

1517
01:21:37,020 --> 01:21:38,940
It's not anchored in like physical reality.

1518
01:21:39,500 --> 01:21:44,780
And similarly, sort of our cultural thesis is that, you know,

1519
01:21:44,780 --> 01:21:46,700
you should evaluate your actions in terms of like,

1520
01:21:46,700 --> 01:21:50,620
how do you think it's going to contribute to the growth of civilization down the line?

1521
01:21:52,060 --> 01:21:55,340
Rather than sort of like subjective measures of utility,

1522
01:21:55,340 --> 01:21:59,740
like hedons, right, hedonism, like how much pleasure is this giving people on average,

1523
01:22:00,940 --> 01:22:03,820
which leads to kind of spurious optima, like, you know,

1524
01:22:04,700 --> 01:22:08,140
wireheading or TikTok and whatnot.

1525
01:22:10,460 --> 01:22:10,860
So, yeah.

1526
01:22:13,260 --> 01:22:14,940
You've got me in total suspense though, Guillaume,

1527
01:22:14,940 --> 01:22:16,460
if it's non-transistor based computing,

1528
01:22:16,460 --> 01:22:19,100
you've got me thinking what the hell it could be, I'm guessing.

1529
01:22:19,660 --> 01:22:21,420
Yeah, yeah, yeah.

1530
01:22:21,420 --> 01:22:23,980
I mean, we're, you know, we're working on it.

1531
01:22:23,980 --> 01:22:25,980
We still have to remain somewhat secretive.

1532
01:22:27,820 --> 01:22:30,380
And I guess there's going to be a lot of interest now.

1533
01:22:30,380 --> 01:22:35,020
Again, we want to be in stealth until we had more to say, more to release.

1534
01:22:35,020 --> 01:22:38,940
But for now, we're keeping things pretty close to the chest.

1535
01:22:38,940 --> 01:22:42,460
But I think there's certain discoveries that you make that,

1536
01:22:43,660 --> 01:22:46,460
you know, you're like, okay, I don't think I'm going to be able to unsee this.

1537
01:22:46,460 --> 01:22:48,460
And if I saw it, someone else will see it.

1538
01:22:49,500 --> 01:22:52,060
We should just move as fast as possible to bring this forth.

1539
01:22:53,900 --> 01:22:57,500
And for us, it's like, okay, how do we make this the most impactful

1540
01:22:58,540 --> 01:23:00,060
to the advancement of mankind?

1541
01:23:00,060 --> 01:23:03,100
Like let's tackle the actually hard, the hardest problems

1542
01:23:03,660 --> 01:23:06,300
that most people don't dare to tackle.

1543
01:23:08,540 --> 01:23:10,940
And, you know, it takes some courage there.

1544
01:23:11,500 --> 01:23:13,980
It takes some fanaticism to some extent.

1545
01:23:13,980 --> 01:23:18,380
I think to me, the fact that we have this framework of EAC

1546
01:23:18,940 --> 01:23:20,860
it's a really powerful motivator, right?

1547
01:23:20,860 --> 01:23:22,620
It's like, what am I contributing to, right?

1548
01:23:22,620 --> 01:23:27,500
Like we're kind of in a, well, you know, I went through a sort of whole phase of,

1549
01:23:27,500 --> 01:23:31,420
you know, a group Catholic, and then I was a, not a Reddit atheist,

1550
01:23:31,420 --> 01:23:33,900
but I was a typical atheist studied math and undergrad,

1551
01:23:33,900 --> 01:23:35,660
and then went through a Nillist phase.

1552
01:23:35,660 --> 01:23:39,500
But then I think like understanding that the whole system

1553
01:23:40,060 --> 01:23:42,620
is seeks growth and entropy production.

1554
01:23:43,260 --> 01:23:45,500
And that's kind of the way things are.

1555
01:23:45,580 --> 01:23:48,700
And that process is what created life civilization

1556
01:23:49,420 --> 01:23:51,020
and the technologies we enjoy.

1557
01:23:51,020 --> 01:23:52,940
Like, okay, we want to contribute to that.

1558
01:23:52,940 --> 01:23:56,220
So having the knowledge that you're contributing to something greater than yourself

1559
01:23:56,220 --> 01:23:59,420
gives you that sort of like infinite dopamine well

1560
01:23:59,980 --> 01:24:03,500
to grind through the long nights to skip the holiday dinners

1561
01:24:03,500 --> 01:24:06,060
to just file more IP, right?

1562
01:24:06,060 --> 01:24:07,660
Like and put in the hours.

1563
01:24:08,300 --> 01:24:11,420
And, you know, I think like scaling, you know,

1564
01:24:12,220 --> 01:24:17,100
everybody has their own cultural or religious framework that provides utility to them.

1565
01:24:17,100 --> 01:24:19,740
For us, you know, for the IAC community,

1566
01:24:19,740 --> 01:24:24,940
I think it has had utility for a lot of people to get out of this sort of Nillistic

1567
01:24:26,060 --> 01:24:29,580
rut that they were in that, you know, the world was going to collapse.

1568
01:24:29,580 --> 01:24:30,780
There's only doom and gloom.

1569
01:24:31,580 --> 01:24:32,700
Everything's going to get worse.

1570
01:24:33,580 --> 01:24:34,380
Put us in charge.

1571
01:24:34,380 --> 01:24:35,820
We're going to fix it maybe.

1572
01:24:35,820 --> 01:24:36,700
Oh, we didn't fix it.

1573
01:24:36,700 --> 01:24:38,700
It's because you didn't give us enough power.

1574
01:24:39,420 --> 01:24:41,980
And like, we're just like, you know, it's kind of like,

1575
01:24:43,020 --> 01:24:44,940
well, you can make parallels to SF politics.

1576
01:24:44,940 --> 01:24:45,980
I'm an SF right now.

1577
01:24:47,580 --> 01:24:48,060
How's that?

1578
01:24:48,060 --> 01:24:50,140
Such how's that by the way?

1579
01:24:50,140 --> 01:24:51,420
Oh, it's it's Night City.

1580
01:24:51,420 --> 01:24:52,060
I love it here.

1581
01:24:52,700 --> 01:24:55,580
You know, it's it's truly Night City.

1582
01:24:55,580 --> 01:24:58,700
I like to say our office is in Arasaka Tower.

1583
01:24:58,700 --> 01:25:00,940
So we went with the sort sort of cyberpunk vibe.

1584
01:25:02,620 --> 01:25:05,660
It's very much like Night City, the video game, right?

1585
01:25:06,380 --> 01:25:11,020
No, but you see the sort of like, if you let the decels in charge

1586
01:25:11,020 --> 01:25:14,380
and you let the movie play out, this is what you get, right?

1587
01:25:16,060 --> 01:25:18,220
You know, you get and decels are kind of

1588
01:25:21,260 --> 01:25:25,340
like they have much more power if they're attached or in control of something

1589
01:25:25,340 --> 01:25:28,940
that's very prosperous, right, and generating a lot of value,

1590
01:25:28,940 --> 01:25:30,700
like similar to big tech, right?

1591
01:25:31,500 --> 01:25:34,220
You know, there's money printers and then these sort of

1592
01:25:35,900 --> 01:25:38,940
folks that seek power kind of take over.

1593
01:25:39,820 --> 01:25:42,460
But, you know, they can they can cause a lot of damage.

1594
01:25:42,460 --> 01:25:45,580
And I think I think there's a there's a cultural turning point.

1595
01:25:45,580 --> 01:25:48,860
And hopefully, you know, it doesn't matter if you're Republican or Democrat,

1596
01:25:48,860 --> 01:25:53,740
like having people that are anti-tech progress, anti-tech first solutions and,

1597
01:25:55,420 --> 01:26:02,700
you know, sheathing themselves and under the cover of virtue to gain more power

1598
01:26:02,700 --> 01:26:06,460
and doing things out of self-interest and and and larping that it's for

1599
01:26:07,340 --> 01:26:11,420
the good of many, you know, I think people have had enough of that

1600
01:26:12,140 --> 01:26:13,500
and are ready for a change.

1601
01:26:13,500 --> 01:26:19,500
And, you know, of course, like as technologists, like we propose technological solutions,

1602
01:26:19,500 --> 01:26:23,500
but ultimately, like, like we believe in the power of technology,

1603
01:26:23,500 --> 01:26:28,860
we believe in people having agency and not accepting that this is just how the way things are,

1604
01:26:28,860 --> 01:26:31,740
you got to accept how they are, things are just going to get worse.

1605
01:26:32,460 --> 01:26:34,460
You know, screw your dreams, kid.

1606
01:26:35,420 --> 01:26:37,020
Just accept it and give up.

1607
01:26:37,020 --> 01:26:38,220
Right. And it's like, no, fuck you.

1608
01:26:38,220 --> 01:26:41,180
We're not we're going to we're going to make the better future happen.

1609
01:26:41,180 --> 01:26:42,060
We have agency.

1610
01:26:42,060 --> 01:26:43,900
We can build, get the heck out of our way.

1611
01:26:45,260 --> 01:26:48,140
You know, we're going to make the the better future we want.

1612
01:26:48,140 --> 01:26:51,740
And you need a sort of like fuck you optimism.

1613
01:26:51,740 --> 01:26:52,060
All right.

1614
01:26:53,260 --> 01:26:55,020
And that's that's IAC in a nutshell.

1615
01:26:56,300 --> 01:27:01,020
And hopefully, it keeps growing because, you know, we think that's what's

1616
01:27:01,020 --> 01:27:03,020
that's what the world needs in many ways.

1617
01:27:04,540 --> 01:27:10,540
And, you know, hopefully we can accelerate SF and then we can accelerate the rest of the world.

1618
01:27:10,540 --> 01:27:12,780
So can I ask you one follow up?

1619
01:27:12,780 --> 01:27:15,020
And then, Nathan, I know you want to jump in and I want to give you time to jump in.

1620
01:27:15,740 --> 01:27:17,820
But I did want to follow on one question that you mentioned.

1621
01:27:17,820 --> 01:27:19,900
You mentioned race Catholic. I was also raised Catholic.

1622
01:27:19,900 --> 01:27:23,420
I eventually converted to Judaism and kind of rejected this sort of default

1623
01:27:23,420 --> 01:27:24,700
secular majority of society.

1624
01:27:24,700 --> 01:27:25,500
That's a whole nother story.

1625
01:27:25,500 --> 01:27:29,580
You've done a bunch of episodes on I do think, you know, it's one of my pet theories

1626
01:27:29,580 --> 01:27:30,700
that religion never goes away.

1627
01:27:30,700 --> 01:27:34,620
It only gets it's almost like, you know, energy or momentum only gets sort of transformed.

1628
01:27:34,620 --> 01:27:37,100
And you get sort of worse, worse crappier versions of it.

1629
01:27:37,100 --> 01:27:37,740
Yep. Yep.

1630
01:27:37,740 --> 01:27:41,340
The reality is you can't actually, you can't actually navigate the world without

1631
01:27:41,340 --> 01:27:43,100
a sense of metaphysics of some form.

1632
01:27:43,100 --> 01:27:44,860
And in fact, if you don't have an explicit metaphysics,

1633
01:27:44,860 --> 01:27:47,900
you can't even do empiricism well, because then you have to change reality to sort of

1634
01:27:47,900 --> 01:27:48,940
suit metaphysical ends.

1635
01:27:48,940 --> 01:27:52,700
I mean, to cite the COVID example in which they denied the lab origin hypothesis

1636
01:27:52,700 --> 01:27:54,620
because it serves some metaphysical end.

1637
01:27:54,620 --> 01:27:57,420
And so they stopped being able to do empiricism as well as they could have.

1638
01:27:57,500 --> 01:28:01,180
They could have, you know, and then that whole COVID story, if there wasn't actually a way

1639
01:28:01,180 --> 01:28:04,380
to have a conversation about, do we save the kids or and screw the old people or vice

1640
01:28:04,380 --> 01:28:04,700
versa?

1641
01:28:04,700 --> 01:28:05,900
Like you couldn't have that conversation.

1642
01:28:05,900 --> 01:28:09,180
So instead it was about the science, but really it was a moral conversation that nobody

1643
01:28:09,180 --> 01:28:12,540
could have because we weren't all morally or even religiously on the same page.

1644
01:28:12,540 --> 01:28:13,820
That's a whole separate conversation.

1645
01:28:13,820 --> 01:28:16,540
But I do think it's interesting and I do think it's one of the things that's a little

1646
01:28:16,540 --> 01:28:17,340
bit lacking in tech.

1647
01:28:17,340 --> 01:28:20,780
I mean, you see these homespun religions, like I would say Burning Man is a little bit

1648
01:28:20,780 --> 01:28:23,020
culty startups are obviously a little culty.

1649
01:28:24,220 --> 01:28:25,900
And I'm not saying this is a bad thing, by the way.

1650
01:28:25,900 --> 01:28:29,420
It's just, it's just, yeah, I know it's a good thing, but it's a little incomplete,

1651
01:28:29,420 --> 01:28:29,660
right?

1652
01:28:29,660 --> 01:28:32,780
In the sense that like, well, it doesn't quite tell me how to raise my kids or,

1653
01:28:32,780 --> 01:28:33,820
you know, who I should marry.

1654
01:28:34,940 --> 01:28:37,980
That I don't know if, and I know I'm putting you on the spot a little bit to like,

1655
01:28:37,980 --> 01:28:39,340
give me the gospel, bro.

1656
01:28:39,340 --> 01:28:45,100
But like what, you know, yeah, what would be and direct me to a set of writing if there

1657
01:28:45,100 --> 01:28:47,100
is one, but like, what would be the E at gospel or like that?

1658
01:28:47,100 --> 01:28:51,660
You know, what is the true, the good and the beautiful in this world other than obviously

1659
01:28:51,660 --> 01:28:53,420
building for building sake, which I think we all get.

1660
01:28:53,420 --> 01:28:54,940
What is the bigger picture?

1661
01:28:54,940 --> 01:28:57,340
Yeah, so, so at a high level, right?

1662
01:28:57,340 --> 01:29:02,300
Yeah, because all about figuring out whatever sort of cultural framework yields,

1663
01:29:03,820 --> 01:29:08,060
the maximal expected growth and scope and scale of civilization.

1664
01:29:08,060 --> 01:29:09,340
And we don't want to be prescriptive.

1665
01:29:09,340 --> 01:29:14,380
All we're giving is a loss function and you can have your own hyper parameter settings.

1666
01:29:14,380 --> 01:29:16,860
You're, you're like, okay, I have a cultural heuristic.

1667
01:29:16,860 --> 01:29:20,220
I think this is an optimum of this sort of loss function.

1668
01:29:20,220 --> 01:29:24,380
This is how I want to, you know, do things within my tribe here.

1669
01:29:25,260 --> 01:29:30,380
It's essentially a sort of like, think of like a very thin framework from which you can have

1670
01:29:30,380 --> 01:29:31,740
like subcultures, right?

1671
01:29:31,740 --> 01:29:36,780
And, you know, the idea is to have sort of culture be more or less like, like code,

1672
01:29:36,780 --> 01:29:41,340
like on GitHub, you can kind of have a base framework and then you can add kind of command

1673
01:29:41,340 --> 01:29:44,380
ments or add things you believe or you can diff it, right?

1674
01:29:44,380 --> 01:29:46,780
You can make forks and you can kind of keep track.

1675
01:29:47,500 --> 01:29:53,500
And so we've seen, for example, you know, Vitalik forked yak and made some changes, right?

1676
01:29:53,500 --> 01:29:57,020
And then he's like, this is what I believe, right?

1677
01:29:57,020 --> 01:30:00,300
So really, you know, yak is sort of a meta cultural framework.

1678
01:30:00,300 --> 01:30:03,260
We don't prescribe, we don't prescribe too much.

1679
01:30:03,260 --> 01:30:04,860
We try to prescribe the minimum.

1680
01:30:05,580 --> 01:30:11,500
But to us, it's very clear that the, the engine that keeps progress going,

1681
01:30:12,460 --> 01:30:16,620
maintaining the sanctity of its sort of momentum and mount, you know, maintaining

1682
01:30:16,620 --> 01:30:22,540
malleability, adaptability, and so on is it's crucial to maintain freedoms and maintain

1683
01:30:22,540 --> 01:30:29,820
entropy in the system and accept variants rather than constraining things, crushing entropy,

1684
01:30:30,860 --> 01:30:37,420
crushing variants, because that leads to crystallization and sort of like catastrophic

1685
01:30:37,420 --> 01:30:43,500
failure. And so, you know, at a very meta level, we try to maintain variants in most

1686
01:30:43,500 --> 01:30:48,300
parameters, but it's not like all variants, no restraint, because that, that just doesn't work,

1687
01:30:48,300 --> 01:30:52,860
right? It's, it's kind of like running a system at very high temperature.

1688
01:30:52,860 --> 01:30:54,380
It's just pure disorder.

1689
01:30:54,380 --> 01:30:59,340
So it's kind of always about finding the optimum balance between order and disorder,

1690
01:30:59,340 --> 01:31:07,020
so between entropy seeking behavior and novelty seeking and sort of constraint and conservatism.

1691
01:31:08,620 --> 01:31:12,140
And so we don't have any one particular prescription of how to live your life,

1692
01:31:12,140 --> 01:31:17,180
but some people like to make forks and have more particular prescriptions.

1693
01:31:17,180 --> 01:31:23,900
And to us, it seems, at least my personal thesis on how cultures get kind of memetically

1694
01:31:23,900 --> 01:31:30,460
post-selected for, it's like whichever culture either confers its adherence and a better ability

1695
01:31:30,460 --> 01:31:37,180
to grow, or if a culture is more sort of viral, then it's going to be more likely to exist.

1696
01:31:37,180 --> 01:31:43,020
That's just like, by probability theory. And so, you know, to us, it's like, okay, if we have this

1697
01:31:43,020 --> 01:31:46,620
sort of metacultural framework, people have all sorts of forks and all these memetic

1698
01:31:47,740 --> 01:31:53,900
forks with different parameter settings can compete in a sort of cultural setting. But we

1699
01:31:53,900 --> 01:31:59,180
should explore the space of cultures and heuristics of, of how to live your life. So for example,

1700
01:31:59,180 --> 01:32:06,300
another friend of ours is, is Brian Johnson, and he has his own life heuristics, and he's trying

1701
01:32:06,300 --> 01:32:11,020
to create his own cultural framework, and he has much more prescriptive ways to live your life.

1702
01:32:11,020 --> 01:32:14,940
And he thinks we should, you know, life spans should be much longer than what it is. And you

1703
01:32:14,940 --> 01:32:22,540
should have, you know, longevity as a priority, right? And so I'm all for this sort of renaissance

1704
01:32:22,540 --> 01:32:29,180
of exploring all sorts of neo religions, neo cultural frameworks for how to live your life.

1705
01:32:29,180 --> 01:32:33,900
I think it's much needed because otherwise sort of parasitic, you know, mind viruses,

1706
01:32:34,540 --> 01:32:41,420
like the ones we've seen do all sorts of damage recently, including the DSEL class of mind viruses.

1707
01:32:41,740 --> 01:32:49,900
We, that's a whole category. Yeah, they come in and they kind of like, like you said, kind of

1708
01:32:50,940 --> 01:32:58,060
fill in this, this gap in people's hearts, if you will. So, so I think we're on the same page.

1709
01:32:58,060 --> 01:33:02,380
And I think, you know, much more, you know, lindy religions, right, like, in a sense, like,

1710
01:33:02,380 --> 01:33:06,860
that have been around for a long time, like, they're very robust, right? It's like a, it's like a

1711
01:33:06,860 --> 01:33:12,460
code base that has been through hell, you know, and back and like, it's just, it's very robust,

1712
01:33:12,460 --> 01:33:16,780
right? It's been robustified over a long time. And by the proof that it's lasted a long time,

1713
01:33:16,780 --> 01:33:22,700
it's, it's a good heuristic. So, you know, that's great. But I think, like, some people want kind

1714
01:33:22,700 --> 01:33:29,660
of, you know, modern variants, right? And they want to contribute to shaping new subcultures.

1715
01:33:29,660 --> 01:33:35,420
And so we encourage people to form subcultures. So really, we're kind of like, only setting the

1716
01:33:35,820 --> 01:33:40,220
hyper hyper parameters of the whole thing. And, and people can set, like more finer grain,

1717
01:33:40,940 --> 01:33:45,900
fine tunings of how they want to live their lives. So there's no one way to go about it. But, you know,

1718
01:33:46,460 --> 01:33:50,380
we do talk a lot about building, because we think that obviously fundamentally, like,

1719
01:33:50,940 --> 01:33:57,020
building technology is a very high leverage way to use your time on earth to impact the

1720
01:33:57,020 --> 01:34:01,980
future scope and scale of civilization, right? And so, you know, encouraging one another and

1721
01:34:01,980 --> 01:34:06,220
helping one another in building technologies that have a positive impact, like we think

1722
01:34:06,220 --> 01:34:13,020
that's a good heuristic that should be in most, you know, forks. And so, so we encourage that.

1723
01:34:13,020 --> 01:34:16,860
Some people think that's the only message, but it's not, right? It came from kind of a higher

1724
01:34:16,860 --> 01:34:22,220
level thinking. But yeah, cool. Well, you're in, I think you're in the right city for exploring

1725
01:34:22,220 --> 01:34:25,900
religions. I mean, the way I see San Francisco, it's really, it's a Petri dish for literally

1726
01:34:25,900 --> 01:34:30,540
exploring every weird ass thing that society wants to do, whether it be autonomous vehicles,

1727
01:34:30,620 --> 01:34:34,220
whatever weird computation you're cooking up with, not having rule of law, for example,

1728
01:34:35,340 --> 01:34:39,340
psychedelic drugs, whatever, bring it, we're going to, it's going to be cooked up here and then

1729
01:34:39,340 --> 01:34:42,940
from here expand and diffuse whether we like it or not to the rest of the world. But I want to

1730
01:34:42,940 --> 01:34:45,900
let Nathan in. I know he's probably been biting his tongue this entire time. And I think I'm the

1731
01:34:45,900 --> 01:34:51,340
last host left standing. And so I'm going to invite Nathan to go ahead and comment and provide maybe

1732
01:34:51,340 --> 01:34:55,580
the other side of it if he wants to. Well, how much time do you guys have is my first question,

1733
01:34:55,580 --> 01:35:02,300
because I am, I have a lot of questions. And I would, if you have the time for it,

1734
01:35:02,860 --> 01:35:09,100
I might kind of fork this episode and just kind of take it in a whole different

1735
01:35:10,780 --> 01:35:15,180
and kind of more from very sort of naive questions direction.

1736
01:35:16,940 --> 01:35:22,860
My show is all about AI and it reaches a pretty diverse set of people that are

1737
01:35:23,820 --> 01:35:28,060
all pretty obsessed with AI, I think. I don't really actually know too much about them other

1738
01:35:28,060 --> 01:35:36,140
than that they go pretty deep with me on a lot of AI topics. And so the way I was thinking about

1739
01:35:36,140 --> 01:35:41,020
approaching, so we could do this now, we could do it another time, but I mean, I'm up late.

1740
01:35:41,020 --> 01:35:46,220
Now is a pretty bad time. I have to have a six a.m. round announcement to craft.

1741
01:35:46,300 --> 01:35:54,460
It's going to be a long night for me. So I was happy to do this podcast. It's very timely for us,

1742
01:35:55,340 --> 01:35:59,980
but I'd be happy to hop on your show. I mean, you could rewatch the recording,

1743
01:35:59,980 --> 01:36:04,060
write down your questions, and we can just go into it. And I would love to do that actually.

1744
01:36:04,060 --> 01:36:10,700
So cool. Yeah, I've got a list all cooked up, but I'm happy to do it whenever is convenient for

1745
01:36:10,700 --> 01:36:19,180
you, although it is timely now. I think part of the reason why Eric had a mixed crew

1746
01:36:19,180 --> 01:36:22,780
is to have there be the other take of it. So I don't know if there's one question,

1747
01:36:22,780 --> 01:36:26,540
or if it makes sense at all. We have nothing against it seeding another show, by the way.

1748
01:36:26,540 --> 01:36:31,100
That's perfectly fine. Or we can bail on. I also want to be respectful of Guillaume's time,

1749
01:36:31,100 --> 01:36:34,460
because it sounds like he's in a, and again, thanks for making the time for the podcast.

1750
01:36:35,020 --> 01:36:42,940
Yeah, I mean, my angle on the whole thing is, I describe myself as an AI scout,

1751
01:36:43,660 --> 01:36:50,860
and I'm getting more and more, putting more and more emphasis on, let's really try to figure out

1752
01:36:50,860 --> 01:36:57,420
what is today, what exists, what can be done with it, you know, if we are going to extrapolate,

1753
01:36:57,420 --> 01:37:01,100
can we extrapolate, first of all, in a high confidence way into the short term,

1754
01:37:01,740 --> 01:37:07,740
and then use those discussions as kind of the foundation for figuring out what we should do

1755
01:37:07,740 --> 01:37:13,100
about the bigger picture questions, where I think inherently there's a lot more uncertainty.

1756
01:37:13,740 --> 01:37:18,060
And one thing I haven't really heard from you guys, and I've pieced a little bit of it together

1757
01:37:18,060 --> 01:37:25,740
from Twitter, but I don't have a great sense of like, what are your near term expectations? Like,

1758
01:37:25,820 --> 01:37:30,620
do you think we are headed for AGI in the next couple of years? You know,

1759
01:37:30,620 --> 01:37:37,580
Metaculous has it at like, just over two years, the leaders of Anthropic, for example, say that

1760
01:37:37,580 --> 01:37:43,820
the leading developers in 2526 timeframe may create such a lead that no one will ever catch them.

1761
01:37:45,260 --> 01:37:49,020
Are you kind of in that same headspace of thinking that we're going to see

1762
01:37:49,900 --> 01:37:56,140
pretty radical transformation on just a few year time scale is like my very first question.

1763
01:37:57,260 --> 01:38:02,380
Yeah. First of all, I don't like the term AGI that much. I think it's human level AI or human

1764
01:38:02,380 --> 01:38:09,740
like AI. I think like calling AGI general intelligence like human like AI that was distilled

1765
01:38:09,740 --> 01:38:17,980
from human generated data. I think that's like very anthropocentric. And I think, you know,

1766
01:38:17,980 --> 01:38:22,700
I work in physics based AI, you know, inspired by physics and to understand the physical world,

1767
01:38:22,700 --> 01:38:32,540
and I've done so for 10 plus years now. I think that intelligence is a much more general concept than

1768
01:38:33,740 --> 01:38:39,500
just human like intelligence. And frankly, I'm not scared of FOOM because, you know, again,

1769
01:38:39,500 --> 01:38:44,860
I've worked on AI for engineering matter, jugs, simulations, biology, all sorts of stuff.

1770
01:38:45,580 --> 01:38:52,700
It's much harder than people think. I do think it is disruptive for our economy based, you know,

1771
01:38:52,700 --> 01:38:57,660
our knowledge economy of sort of human like white color intelligence. I think there's a,

1772
01:38:57,660 --> 01:39:03,020
there's still going to be a 10 year gap for physical intelligence, right? Robotics. I think,

1773
01:39:04,060 --> 01:39:08,460
you know, our motor, motor intelligence is much harder. It takes many more parameters. It took,

1774
01:39:08,460 --> 01:39:12,540
you know, billions of years to evolve, rather than I guess like, I don't know, 100 million

1775
01:39:13,180 --> 01:39:21,580
for the neocortex. Probably got those numbers wrong, but it's ballpark. But yeah, essentially,

1776
01:39:21,580 --> 01:39:28,700
I think, I think there might be a disruption to our economy. But I do think that people will adapt.

1777
01:39:29,260 --> 01:39:34,060
People will learn to augment themselves out of self interest, right? And it's like,

1778
01:39:34,060 --> 01:39:37,660
where, where will the system goes? Every corporation is going to do what's in their best

1779
01:39:37,660 --> 01:39:44,380
self interest. They're going to maybe be 80% AI, 20% human. And each human is going to be augmented

1780
01:39:44,380 --> 01:39:50,060
and control a fleet of AI's, right, that are doing its bidding. And that's okay. Maybe the human

1781
01:39:50,780 --> 01:39:56,140
employees become the control systems for a fleet of AI to do most of the work, right?

1782
01:39:56,140 --> 01:40:01,740
Maybe they become more kind of like the capital allocators or the executives of companies that

1783
01:40:01,740 --> 01:40:07,580
are mostly AI for the execution layer, right? Maybe that's the future. I think we're heading

1784
01:40:07,580 --> 01:40:12,700
towards interesting times, but I don't think there's going to be cataclysmic effect. I don't

1785
01:40:12,700 --> 01:40:16,620
think it's going to end humanity. I think we're going to adapt and the system will adapt. And

1786
01:40:16,620 --> 01:40:25,260
the sort of EAC is all about the main, like it's a faith and sort of worship of this adaptation

1787
01:40:25,260 --> 01:40:29,660
of the Homo Techno Capital Mimetic Machine, the whole thing, right? Like memes, technology,

1788
01:40:29,660 --> 01:40:34,220
capital humans, it's all coupled. It's all adapting. It's always shifting. It's always,

1789
01:40:34,220 --> 01:40:38,940
the only constant is that it's always changing and it should be always changing and it seeks to

1790
01:40:38,940 --> 01:40:44,940
grow, right? And so, you know, we have a faith that the system will adapt. It might be abrupt.

1791
01:40:46,220 --> 01:40:54,380
And so, you know, personally, I'm not trying to, personally, I'd rather augment humans in

1792
01:40:54,380 --> 01:40:58,220
orthogonal directions to human-like intelligence rather than trying to replace human-like

1793
01:40:58,220 --> 01:41:03,340
intelligence. Obviously, in an economy that's already shaped to take in human intellectual

1794
01:41:03,340 --> 01:41:09,740
labor and do all sorts of produce products, like, you know, having human-like AI is what's

1795
01:41:09,740 --> 01:41:13,900
going to grow the fastest. So that's what's being built first. But I'm already pricing an AGI,

1796
01:41:13,900 --> 01:41:19,100
personally. And the technologies we're building hardware and software, I'll assume there's probably

1797
01:41:19,100 --> 01:41:25,980
going to be a human-like AI, human-level AI within two to three to five years, right? And that's

1798
01:41:25,980 --> 01:41:30,940
been priced in for me, right, mentally. And it's like, now what? What's the next thing, right? And

1799
01:41:31,020 --> 01:41:37,980
to us, it's like, we're going to seek to grok and perceive, predict, and control matter at the

1800
01:41:37,980 --> 01:41:42,700
nanoscale, right? And then we're going to, you know, we're going to seek to, again, increase the density

1801
01:41:42,700 --> 01:41:49,500
of intelligence in terms of the substrate that's what we're working on. And so, yeah, I think,

1802
01:41:50,060 --> 01:41:54,060
yeah, like, I think it's coming. I don't think it's going to be cataclysmic, but I think that

1803
01:41:54,620 --> 01:42:00,860
people should start preparing and start integrating their business processes,

1804
01:42:00,860 --> 01:42:06,860
integrating their personal life and their personal workflow with AI, right? There's going to be the

1805
01:42:06,860 --> 01:42:13,660
class of sort of the tech forward people that embrace AI and sort of do well, right? They

1806
01:42:13,660 --> 01:42:19,580
integrate with it and those that refuse to use it that don't do so well, right? But the important

1807
01:42:19,580 --> 01:42:26,140
thing is that if people get to own a piece of the system in which they're contributing,

1808
01:42:26,860 --> 01:42:32,380
at least they own a piece of the future as it grows. Whereas if they, there's only centralized

1809
01:42:32,380 --> 01:42:37,740
players from which you, you, you have to pay rent constantly. And maybe they'll give you a

1810
01:42:37,740 --> 01:42:42,540
sprinkle of UBI at the end. That seems pretty dystopian to me. So that's the future we're trying

1811
01:42:42,540 --> 01:42:48,060
to prevent. I don't think the disruption can be stopped at this point. It's, it's coming. And

1812
01:42:48,380 --> 01:42:52,140
the only question is like, who's going to own the future? I think it's a good place to put it.

1813
01:42:52,140 --> 01:42:57,580
Questions than that, actually. But I'm going to have to hop to, but I think you have needs to go.

1814
01:42:57,580 --> 01:43:01,980
Can I ask you one last, it literally is a 30 second thing, by the way. One of my employees,

1815
01:43:01,980 --> 01:43:06,300
he was digging up some of your old videos. He, he saw that you had a four hundred and five

1816
01:43:06,300 --> 01:43:12,220
pound bench PR. Is that actually true? Yeah. He was very impressed by that. Yeah. Yeah. I mean,

1817
01:43:12,220 --> 01:43:18,060
I was, I played, I played college ball, college football. I have a friend who's a doctor in the

1818
01:43:18,060 --> 01:43:24,460
NFL, played with a great friend. And yeah, I mean, I just kept lifting after football. And,

1819
01:43:25,340 --> 01:43:29,180
you know, to me, it's just been like, I was a mathematician and I was a power lifter. And to

1820
01:43:29,180 --> 01:43:35,900
me, it's just like engineering signals in order to have neural adaptation. And it's all one in

1821
01:43:35,900 --> 01:43:41,900
the same. And so, yeah, I mean, you know, the best Jesus character, it's all about mind and body.

1822
01:43:41,980 --> 01:43:49,260
And, you know, I, I do like to cultivate strength and, you know, push, push myself to the, to the

1823
01:43:49,260 --> 01:43:54,700
limits to some extent. And yeah, that is a, I'll just post the video if you want. I'll tweet it.

1824
01:43:55,980 --> 01:43:58,780
Well, he found it. I have no idea how he found it on Vimeo, but I'm impressed because, you know,

1825
01:43:58,780 --> 01:44:03,420
a lot of this transhumanism business is very agnostic and kind of denying the physical, but

1826
01:44:03,420 --> 01:44:06,780
you're actually embracing it. And yeah, that's also other conversation. Thanks for making time.

1827
01:44:06,780 --> 01:44:10,460
I'm going to have to hop off. I assume everything will upload when we all hop off. I'm not sure

1828
01:44:10,860 --> 01:44:13,660
Erica's number disappeared before. So I literally have no idea what's going to happen.

1829
01:44:13,660 --> 01:44:17,900
All right, to be continued, guys. And yeah, definitely to be continued.

1830
01:44:17,900 --> 01:44:20,620
And one of these days, I want to talk crypto and AI with you when you're over the whole

1831
01:44:20,620 --> 01:44:23,660
hump of fundraising and all that stuff. Cause I've been thinking about this whole thing for a

1832
01:44:23,660 --> 01:44:30,460
long time and I'm very, yeah, I'm impressed. Okay, cool. Awesome. All right. See you guys. Cheers.

