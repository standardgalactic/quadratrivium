Thank you, everybody. Thank you, Joel. It's true, I'm kind of the, I have the most gray
hair here. I've been around for a long time and I want to kind of talk about the long
view a bit. Actually, so this is the challenge. First, let me say, though, it's really exciting
just to talk to all you young people learning about the field, learning about, wanting to
get into AI and reinforcement learning. And it's just like, it's a special opportunity.
I really appreciate it. And so when I do this, I have to think, what can I try to communicate
to you? Because, you know, we have really a brief amount of time and the field is immense.
I've written a whole book on the subject. And so what do we, what can I try to do? Now,
my topic normally today is temporal difference learning. And certainly I'm going to talk
about that. But I also want to, you know, just speak to you as young people entering
the field and just tell you what, the way I think about it from a sort of a longer view.
And well, so I think the first thing to be said along those lines is that this is a really
special time. And we can look forward now to, to doing an amazing thing, which is maybe
understanding the way the mind works, the way intelligence works. This is, this is like
a monumental event, not just, you know, this century, you know, for thousands of years,
maybe in the history of the, of the earth, when intelligent beings, animals, things that
can replicate themselves, finally come to understand the way they work well enough to
by design create intelligence. And so the big thing that's going to happen is that we're
going to come to understand how the mind works, how intelligence can work, how it does work.
And just the fact of our understanding it is going to change the world. It's going to
change, obviously, there'll be lots of applications, but just, it'll change ourselves, our view
of ourselves, what we do, what we play with, what we work at, everything. It's a big event.
It's a big event. And we have to, we should keep that in mind. And as I say, you know,
I'm not saying it's going to happen, you know, tomorrow. It's not, but, you know, 10 years
it could happen, 30 years it could happen. And then the great sweep of history, that's
a small amount of time. It's going to happen within your lives with high probability. So,
so the first thing we have to ask is why is this happening? And I think it's really comes
down to like Moore's law. Okay, so I took a slide from Kurzweil. This is his standard
slide of the increasing computation per dollar. And it's just reaching and a point where
it starts to compete with the computation that we can do with our own brains, with natural
systems. Okay, that's happening now. And it's just going to continue increasing dramatically.
I'm sure you've seen these kind of slides. Here we have time, years along the bottom.
And on the other, we have computations per second per dollar. Okay, and you see it's
a log scale. So, of course, a straight line would be exponential increase. And Kurzweil
argues that it's actually slightly super exponential. But the point is, the computations become
available to us has become enormous. And that changes things. It's the rise of deep learning.
And all that is purely because of GPUs and computation getting cheaper. And that will
only continue and it will become more extreme as a vastly increased computation. So, that
alters everything. And it really is profound. And I'm just going to try to slide to say
what I conclude from this. As I think it's an answer to like a question for AI that's
had for 60, 70 years. Okay, so the answer, I'm not going to, I'm just going to do a couple
slides like this. But I guess I should, one last bit of preface is that my talk today is
going to be like half, you know, these big things. Okay, and like I'm starting to do
now. And half like really small things. We go back to the foundations, real stuff that's
basic that we need to understand. Okay, I think the details, the big pictures is important.
But the details also really matter. And like we heard a lot of specific algorithms early
today, exactly how they work is really important. The details matter, but the big picture matters.
Sometimes we lose sight of the big picture. Sometimes it becomes so obvious that we overlook
it. I think the computation is as long that lines. And its implication, its implication
in a phrase is that methods that scale with computation are the future of AI. Okay, and
that's, that's by scale computation I mean is we get more computer power, those methods
become more powerful. And this has, has not always been true. But certainly it's true
of learning and search methods. These are what we call general purpose methods. And
it's the answer to this, one of the oldest questions in AI for 60 years old. Do we want
weak methods? That's what they call them in the old days. They call general purpose methods
weak because they just, they just use the data or use computation they can only, they're,
they're general purpose. They're general, they're weak. Okay, that's what they call
them in the old days. The strong ones were the ways that would lose human knowledge and
human expertise and human insight to make their system so much better. Okay, now probably
you guys are thinking that's crazy. The general purpose ones. Are you thinking that with me?
General purpose? How many of you are going to go human in sight? Oh, some of all? Okay,
yeah. That's, that's, that's nice compromise each sort of position. And so all today I'm
going to try, or all in this big part of the talk, I'm going to try to talk about, I'm
going to present strong views. Okay, really maybe you should do compromises and nuances,
but it's good to talk about strong views because they give you a working hypothesis. They give
you a point of view. And you know, you, you can say this is a strong point, but you don't
have to believe it. You have to say, well, that's a strong point of view that I should
be thinking about. And then maybe you have several of them. Anyway, so I'm going to present
the strong point of view that this, all this question has been answered and it's been answered
in favor of the weak methods. And yeah, and now I don't want you to, I could talk about
this all day, but I'll refrain from it. I'll just note the next thing to say is that you
may, you're thinking you're good. You're thinking you're into deep learning or reinforcement
learning. So you're on the right side of history, but I'm not sure that's right. Okay?
Because we take things like any kind of supervised learning, even model free reinforcement learning,
the thing I love more than anything, right? It's only weekly scalable. If I had, you know,
a million times its computation, I'm still limited in my model free reinforcement learning
by how fast I can gather data. And, you know, I'm only learning a value function, maybe
a policy. What's the big deal? It's a tiny object and some map from states to what to
do. Okay? And that's not a big thing. I don't care how many features you have. It's, and
super, certainly for supervised learning is only weekly scalable because it requires
people to provide data sets. Okay? And they become the bottleneck. If you have, you know,
get people to label things on the web, things scale, but only weekly, only as fast as you
can gather the data. And eventually, eventually that becomes a bottleneck. You really want
to be able to learn from raw data. You want to be scalable. Okay? So if these things that
we love are not scalable, what is scalable? What would be fully scalable? Well, my answer
is simple. It's what I call prediction learning. Prediction learning means learning to predict
what will happen. Okay? And so it's the unsupervised learning because we have targets. We're
supervised learning. We just wait and we see what does happen. And that's our target.
Okay? And so you don't need to have human labeling or you have a target, but you don't
need a human to provide it. Just get it from the data. So it's unsupervised learning.
But anyway, it's definitely the scalable model free learning. And maybe it's the scalable
model free learning. And prediction learning is at the heart of all of our control methods
where you learn value functions. I think maybe, and that's sort of the argument I wanted,
the big argument about temporal difference learning is that it's the scalable model free
method. And of course, I haven't given you even a step towards that yet. I'm just saying
that the idea of predicting, predicting, of having learning that predicts is the key problem.
It's what we should be thinking about. And you may say, well, deep learning, supervised
learning is all about prediction learning, but that's not true. It's about predicting
what the other, what the label will be, but it's not about predicting over time. It's
not about predicting where you have to wait and see what happens. And that makes all the
difference. Okay. So with all these words, I want to ground this a little bit and remind
us what the data, what the data looks like. So I put in the slide of, of this is what
real life is like. Real life is a temporal stream. We have things like playing soccer
and we have to make actions at every moment. We're maybe a hyena being chased by a lion
and trying to predict whether it's going to live or die. We're maybe a baseball player
and we're, our eyes are watching this tiny little ball flash by us really fast. I have
to swing it just the right moment to hit the ball. Or maybe you're talking to someone,
you're trying to predict what they will do. So this is the data streams that AI should
be learning to deal with. And so we should always keep this in mind. When I say learning
to predict, well, I think, I think that hyena trying to predict, you know, fear, fear is
your, is your prediction of are you going to die? Okay. So he's trying to predict it.
Several times it looks good and bad. And at the end here, it's not looking so good. Okay.
So with those prefaces, let me start talking about temporal difference learning. Temporal
difference learning. Temporal difference learning, it's a method for learning to predict. It's
why we use reinforcement learning to predict future award value functions. It's basically
the center of the core of many methods that you know about Q learning and SARSA, TD Lambda,
deep Q networks, TD Gammon, the world champion, backgammon player using deep reinforcement
learning from 15, no, 25 years ago. My God, 25 years ago, deep reinforcement learning.
1992. But not all reinforcement learning methods. So AlphaGo, it happens not to use
TD learning, uses other kinds of reinforcement learning. The helicopter thing that you might
have heard, that doesn't use it. And then there's sort of pure, like Peter's talk was
interesting because he talked about policy based methods and some of those don't use
temporal difference learning, but eventually he would get to it and put it in to make things
better. So it's sort of, yeah. So it's sort of optional. It feels like it's optional.
Many people will say it's optional, but I do want to argue that you do want to use it,
that really you should always be using it. It's ubiquitous. And okay, it seems to me
how the brain works, what else to say? Oh, now it can be used to predict anything. It's
for general method for prediction learning, not just for rewards. Okay? So don't be fooled
by, in reinforcement, when you read all about TD, it's all about predicting value functions
and reward. But we can use it for anything. And since my talk is just about TD, I want
to be sure to think about this general use. Question?
Not saying that it's key to AlphaGo, but don't they use TD learning to train value functions
that they use to evaluate the states that have the research?
Yeah, I've forgotten exactly the details. The initial learning system was learned from
actual outcomes, actual games, and they would go all the way to the final outcome. They
would make a prediction, look at a position, make a prediction of how am I going to win
or lose, and they would go ahead and see who actually won the game at the end. And they
would use who actually won instead of a temporal difference learning. So that's what we want
to work for. Do they wait and see who actually won? Do they see the outcome or the return?
Or do they update a guess from a guess? Okay? Let me make that clear. So that's my next
slide. As a TD learning, what is TD learning? Basically, it's learning a prediction from
another later prediction. Okay? So if AlphaGo is looking at a position, making a prediction,
and then zooming all the way to the end of the game, it's who won, and then it's not
TD. It's not learning that prediction from a prediction. On the other hand, if you make
a prediction from a position, and then you make one move, and you see your next prediction,
and you use that next prediction to form a target, then you're doing TD learning. Okay?
So the quick word, quick phrase is we are learning a guess from a guess. Okay? Sounds
a bit dangerous, doesn't it? What would constrain it? What would tie it down? Okay? But that
is the idea. We want to learn an estimate from an estimate, and we have to talk about
whether this is good or not. Okay? The TD error, the TD error is the difference between
two predictions, two temporally successive predictions. Right? So if you're playing your
game, you say, I think I'm winning, then you take another move, you know, and now I think
I'm losing, you try to learn from that. You don't know who actually is going to win yet,
but you try to learn from the temporal difference in your predictions. Okay? Now, after that,
it's the same as what you're all used to. You just have an error, and you send it back
proper through or whatever you're doing, and so it's just really, where does the error
come from? Or where does the target come from? Does the target come from the end of the game,
or does the target come from the next prediction? Okay? So here's the example of TD Gammon,
originally 1992, and he'd had a chest, a back end position, and he would send that into a
neural network, which would filter through actually just a single, well, he had many
versions, the standard version was a single hidden layer, and he'd end up with this probability
of winning, and the error then was the probability of winning in one position minus the probability
of winning in the next position. So we look at the change in the estimated probability
of winning, and that was used as the error that would be back-populated through it, and
this would learn just sticking in the corner, playing against itself, learning from itself,
from its own trial and error, and it came out to be competitive with the world's best
players, really the best in the world. Okay, so that's familiar. Now, what we, I'm trying
to get to this question, this question is, do you need to use TD learning? Because this
is all this is, is to motivate. Motivation is the most important. Do I need to use TD
learning, or can I get away with it? Because you go to the field now, maybe even in reinforcement
learning, you'll find a good fraction of the people don't believe in TD learning, and they
think they can get away without it. Okay, and so it's a real question we should all be asking.
Do we need it? I want you to understand, I want you, even as people learning about the
field to be able to engage with this question, and know the basic facts pertinent to whether
we should, we need to use TD learning. Okay, so I will skip over, over Atari, and you've
seen that already. So TD learning, when do you, when do we need it? Okay, well it's only
relevant on what I, on multi-step prediction problems. That's the first thing. Only when
the thing predicted is multiple steps in the future. So obviously if you're predicting
the outcome of a game, that's multiple steps in the future, but if you're predicting a
label or, or in AlphaGo where the data was, the initial data at least was, here's a guess,
and then I'll just see who won the game. And if you just use who won the game, then it's
essentially a one-step prediction. Okay, okay, so now I want to say that it's really broadly
applicable. Like I said, it's only applicable when you have multi-step predictions, but
really everything you want to do is going to be multi-step prediction. If you want to
predict, oh I have some examples, multi-step prediction. If you want to predict the outcome
of a game, if you want to predict what a stock market index will be, well really you could
just predict what it's going to be, but you'll get more data on every day after that, and
you'll be able to make new predictions. So you'll make a mold, you'll make repeated predictions
about a long-term outcome. If you want to predict who will be the next president, you
can predict that, you know, every, every day as a new event happens. If you want to predict
who the U.S. will go to war against next, so all these are long-term predictions. They
don't, they don't jump to the end. Now even, even if you want to predict a sensory observation,
if you want to predict just the very next sensory observation, that would not be multi-step
prediction. But, but, but if you were to predict, you know, even 10 steps ahead, or a discounted
measure of the future, those are long-term or multi-step predictions. And yeah, we think
about, go back to the real world, the, the hyena and the, and the lions, or the, the
conversation, or, or messy playing soccer. He's got to make long-term predictions. It's
not, you know, what's the next thing? It's, am I going to make this goal? Will I get around
that fellow? Where's the wall going to be in a few milliseconds from now? All these things
are multi-step predictions. Now, can we, can we treat it? Can we just use our one-step methods?
Like, you know, sure things happen bit by bit, but ignore that. Just wait and see what happens.
You know, like, see who won the game and use our one-step methods. And, or can you learn a
one-step model and then compose your model? Okay? And the answer is that we really can't do these
things. And I want to try to give some sense of that today. I'm really going to talk mainly about
this first question. Can we think of the multi-step case as one big step? Or do we have to deal with
it bit by bit? And, but I want to do one slide on the second bit. The second bit is, can we learn
one-step predictions like a model and then iterate them to get a multi-step prediction when you need
to? And I just, I just want to say that I think it's a trap. Okay? And I don't know if I could
really properly explain this to you, but I think it's a trap. I think it's enough to model the world
to make like a low-level simulation of the world. To make like a, think, treat the world as a mark
of decision process and model the transition probabilities. Or treat the world as a, as a, as
engineering model where we just have to learn the velocities and the accelerate, the effect of the,
on the accelerations or actions and then integrate this low-level differential equation. This is,
this is all a trap. These short-term models and then iteration is, it feels good because we know,
we know that if it can be done perfectly on the one step, then it can be done perfectly
for however far we want to look into the future. But there's two problems. First of all, we can't
do it perfectly. And when we do it imperfectly with it, because we're always going to have an
approximation. Then when we try to iterate them, we, we get a propagation of errors and a compounding
of our errors. And we get a useless long-term prediction. And secondly, of course, it's exponentially
complex because as we look ahead, each step, there'll be many possibilities. The world is stochastic
and also our actions. Maybe we have, we have different choices we might want to look at for
our actions. So it quickly becomes computationally intractable. And it will always be computationally
intractable to try to look ahead many small steps into the future. Like to try to, try to
iterate your model of physics to get to, you know, how much fun will I have going to Montreal
and taking the summer school? Okay? It's crazy. And it just doesn't, there's no future in that,
that way of thinking. It's a trap and lots of people are, in my opinion, are falling into it.
Okay. But let's go back to the other side. The other side, remember, I'm just going backwards
here. We have these two things, two ways to get away from TD if we don't want to get away. We don't
like TD. Can we learn to model and iterate it? That's the second one. Or the first one. Can we
think of it as a one-step thing and just use, do the one-step thing? Okay, so the one-step thing,
I'll do it, I'm going to do it in, I'm about to transition to my low-level part of the talk,
but I don't want to try to answer it here. Just at the high level. And then maybe we can even
take questions after this, this high-level part of the talk. Can't we just use our familiar one-step
supervised learning method and, and reinforce something? These are known as Monte Carlo methods.
I mean, you just roll it out or whatever, see what happens and use that what happens as a target.
Okay, so this has costs. Number one cost is, is you have to, you're making, you're making this
prediction, then you're rolling it out to the end. And really, you're going to make a prediction at
every moment in time. So you've got to remember all whole mess of predictions. As you, as you go
out to the end to see the outcome, or really, if you have returns, you get the outcomes of
different, different steps, different outcomes, you have to relate them back to the, to the earlier
situations. It's horribly complex. It's nasty. Okay. It's, first of all, you need to remember all
the things you did. Think about yourself. Maybe you're the lion there. You're trying to make a
good prediction. Okay. And what do you have? You have all the stuff swirling around you. You have
the hyenas running away. You have a glimpse of him. You're, you're feeling of your feet. You have all
the stuff that you can sense. And, and then you want to relate that to how well it's going. How,
how good you should feel about this chase. Okay. And so what is it sensible to think
or just be much better if you can do it now. If right now, when all the stuff is in your mind
and in your sensors, learn if it's going well or going poorly and learn now, as opposed to
wait, wait five seconds later when you have had, you know, a whole number, a large number of frames
of different, different sensations and different patterns of sensation, you've forgotten, you
know, if you had to wait five seconds till the end of it, it's too late. You can't remember all
that. Whatever you remembered will be a tiny shadow of the real vivid representation you had at the
time it happened. Okay. And of course, the computation is, is, is, is poorly distributed. You can't
learn now. So what are you doing now? Later, you'll find an outcome. You'll have to do all the
learning then. Just a poor temporal distribution. And you can avoid these problems with special
methods. And that's what really what TD is about. Specialized method for the multi-step case.
And another reason that you don't want to wait is that sometimes you never know the target.
Like, I don't know, let's say you're playing your chess game and there's a fire alarm and you never
finish your game. So you never see a final outcome. But if you're TD, you know, maybe you thought you
were winning and then, you know, it was going really poorly. And you, so you did something bad.
You can learn without waiting till you're checkmated. Maybe the fire alarm just like one move away from
the checkmate. So technically the game never ended. And, but you can learn a lot from your experience.
So obviously we could try to ignore all these things. Think of them as nuisances. But I think of
them as clues. These are hints from nature about how we should proceed. Okay. Okay. So now,
now I'm going to get down to it more technically. But I hope you're starting to see the view I'm
trying to present. We really need to learn from new predictions so that we can do it as we go along.
And I think it's really ubiquitous in all the different kinds of learning we're looking at in AI.
Okay. So I'm going to use notations a little bit different than what we've heard earlier today.
That's what I call my new notation. I use it in the second edition of the
Reinforcement Learning Textbook. The big thing is that we're trying to use the
statisticians convention that random variables are capital letters and instances are lowercase
letters. So all of the things that happen that make up life are capital letters, right? Because
they're the random events that have actually happened. They're not possibilities. They're
whatever happened. So S0 is the first state. A0 is the first action taken in that state.
R1 is the first reward that depends on that state and taking that action in that state.
And then as the same time as we get the reward, we get the new state. So I like to give them
the same temporal index. R1 and S1, they occur together. They're jointly determined in fact.
Okay. And then life goes on and on. And that's the data. That's all we have in terms of data. We
have trajectories and maybe a single trajectory. And we're interested in classically in the return.
And the return is a sum of rewards. And I'm not going to use capital R for the return
because capital R is actually the actual rewards, the sequence of rewards. And so I'm going to
define the return. I need a new letter. I'm calling it G, capital G, because it's a random event.
Whatever sum of rewards after time t actually was, we're going to call that G of t. G of t is the
return. And as we note here, if I can use this thing, this dot just means that this is a definition.
It's not a statement of something that's true because it follows from other things that I've
said. It's a definition. So G of t, the return is the sum of the future rewards with the discount
rate is the most common way of dealing with it. And if that, of course, can be written
now we're going to use equality because it's not a definition, that equals the first reward
plus gamma times the sum of all the later rewards. We're just taking one factor of gamma out of all
of these guys. So this is no, no gamma. This one is one gamma and this guy is two gammas. We just
take the one out. And then the rest of this is the sum of future rewards from one time step later.
So it's a bit like what we started with. It's just like the same thing. It's G of t plus one.
This is just a definite. This is a true equality here. Any return be written as the first reward
plus gamma times the next return. And that is going to be the basis for our temporal difference
learning because we're going to use this as a target. We're going to use the next reward
plus gamma times the next return essentially as a target. I guess that's going to be explained
right next. We look at a state value function. Since we're using capital letters for the random
variables, I can't use capital V for the true value function. It's got to be lowercase.
It's a function of the policy. So V pi of s or s is any particular state. It's an instance,
any state, lowercase s. Its value is the expectation of the random variable, the return,
if we started in stat state s. So what we can expect the return could be under pi,
pi is the policy. So it's some way of picking the actions. Of course the value of state depends
on what you do. If you dance at the top of the Grand Canyon, it might be bad. But if you
can sedately walk up to the railings, it's good. So policies, values depend on policies.
Okay, so then since this is the return, we can just use the above equation. The return can be
written as the first reward plus gamma times the rest of the return. And then since we're
taking the expectation of this, this is the expected next reward and expected value of the
next state. Okay, so that naturally leads to the notion of an error. We can use, we can compare
the estimate, estimated value of a state at some time to the reward and the estimated value of the
next state. Okay, so that's going to be our TD error. This is what we're going to use to replace
the normal conventional error. Now this V is a random variable. This is our estimate.
And our estimate will depend on what happens. And so that is random. The estimated values are a
random value, or a random function. And so it's capital. And that's our TD error. You got it? The
TD error? Is that clear? Good. Okay, so now let's talk about our methods. I want to contrast
supervised learning. And remember I said supervised learning is called Monte Carlo in this context.
So what exactly is that? That's we take our estimated value function for the state that we run into,
V of s of t. Okay, we're going to update it based upon some experience. Okay, here's the experience.
Here we are at s of t. And this is the tree of the things that might happen. Like we might pick
either of these two actions. And if we did pick this action, either of these two states might arise.
So yeah, the black dots are actions, the open dots are states. So basically, this is the tree of
all the features that might happen. And in this case, we're imagining that there are terminal
states, we're basically adding things up till we reach a terminal state. So here is a particular
trajectory that might happen. Let's say it did happen. From that state, we would this, this,
this, this, and then we terminated. Okay, so we now, once we've terminated, we know what g is.
We know what the return is. And we can do this update rule. We can compare our estimate
for the state at time t up here to the actual return. And we make that that error. And then we
increment, I should, this is the step size alpha. So there's a number like 0.1. So we
increment towards, towards this target. Okay, that's, that's a standard, it'd be a Monte Carlo
learning rule, a supervised learning rule. And that's the, the competition for the TD method.
The simplest TD method looks instead like this. We only look ahead one step. We're at s of t. We
look ahead at one. We see the reward that happens and the next state that happens. And based on
those, we form this TD error, which is again comparing, comparing, we're updating the, the
estimate of this for the state at time t. So we're going to make an error between what we were guessing
for that state and this new target, the reward plus gamma times the estimate value of the next state.
Okay, now you've probably also heard about dynamic programming. You can think, you can put dynamic
programming in the same figure, the same kind of figure. And if you were, the dynamic program
version looks like this. Because it's not considering a single line through the possible
tree. It's considering all possibilities. It's considering both actions and both possible
next states. So this is where you need the model of the world. Because although you, you know,
you're probably picking each, each action, the probability that the world will give you
possible next states will be known only to the world. But in dynamic programming, you assume
you know all that. So in dynamic programming, the equation is that the value, the estimated value
for a state is moved towards the expectation of the first reward and the expectation of gamma
times the value of the next state. Okay, so there's this expectation. And that's what makes it dynamic
programming. Because you see me know all the probabilities. You can figure out that expectation.
It doesn't give you the answer because your value will still be, you're still learning a guess
from a guess. You're learning your new estimate, still from your old estimate. But that's dynamic
programming. So, so really, we can say the following. What's special about team methods is they boot
strap and sample. So bootstrapping is this idea that your target involves a guess, an existing
prediction. Okay, so Monte Carlo, Monte Carlo, the whole point is that it doesn't bootstrap. It's
just looking all the way to the end and seeing what the return is. There's no, there's no estimates
playing a role in the return. Dynamic programming also bootstraps. Dynamic programming says look
ahead one step and look at the expected value of the next state and back it up. So you're only,
you're using your estimates. And your estimates gradually get better. TD, of course, also is
using your estimate. Yeah, it's like Monte Carlo and TD are learning methods. I guess that's my
next point. The learning methods, Monte Carlo and TD, they sample. They sample what happens because
you don't know how the world works. And dynamic programming does, they're not sampled. This uses
expectation. It assumes you know what will happen, what could happen. So those are the two basic
dimensions. Whether you're sampling and therefore learning and whether you are bootstrapping. You're
using your, your, your bootstrapping, your estimate from other estimates. You're learning guesses
from guesses. And so TD prediction, basically I'm just saying this is, this is the update you saw
before, erupting. The Monte Carlo is here and the TD is there. So just the contrast is that one,
the target is the actual return and the other is the target is this sort of one step estimate of
what the return will be. Okay. Now let's think, let's do an example. So here I am. I'm coming home
after working a hard day at the office and I'm trying to guess how long it will take me to get
home. Okay. So I'm, I'm leaving my office. It's Friday at six o'clock. I have some other features
and I make a guess of how long it will take. So I will, I'm going to guess it'll take 30 minutes
to get home. Okay. So, and that's, that's my prediction of my total time because I haven't
gone, you know, I'm just starting now. So my elapsed time is zero. Now as I come out of the,
I come out of my building, go to the parking lot and I see it's raining. Okay. And it's raining,
you know, it's going to take me longer because everyone drives slower in the rain. So I think,
well, first of all, it's already, I've already spent five minutes just getting down from my
office into the parking lot and, and I also think it's going to take me longer. I think it's going
to take me 35 minutes from now for a total of 40 minutes. Okay. So I, what I want you to see,
the first thing you want to see is that my guess about how long it's going to take me and my guess
about the total time you go home, it's constantly changing. As I get more information, I, I revise
my estimates. Okay. So to carry the example through, I get, I get, I, I start getting my car, I drive
on the highway. Turns out it didn't take, didn't take so long as I thought. I've spent 20 minutes
total now. And I think it'll only take me 15 more to go home. It wasn't so bad within the rain. And
so that's 35 minutes total. Now this, this car, this column is my total estimate as it goes up
and down. And then I get stuck behind a truck on a secondary road. And so I think it's going to
take me longer. And then I reached my home street. And I think it'll take me 43 minutes. And it does
take me 43 minutes. Okay. So that's a possible thing that might happen. A possible trajectory.
And what I want you to ask is what you might learn from that. Okay. So if you're doing a Monte
Carlo methods, you just say, well, it took me 43 minutes to get home. That's the answer. So all
my estimates, my first initial estimate of 30 minutes, that's going to be moved towards 43
minutes. That's the error. In fact, all of these will be moved up towards 43 minutes. Because
whatever guess I made at each point in time, it should be moved towards what actually happened
or whatever, whatever is remaining in the future at that point. Okay. Now, if you're using a TD
method, if you're using your, your guests can learn a guess from a guest, then something very
different happens. So even some of the signs change. So your first prediction will move up
because you start out 30 and then after you found out it's raining, so you'll move up.
But this one, for example, will move down. And the actual, all the errors are different.
All the errors are different. And the long, long, long rounded law will wash out. But for all
actual learning is a law, is very different. Okay. Now, I also want you to think about the
computational consequences. Okay. If you're doing TD, then, you know, when you're here
and you go to the next stage, you get an error and you can update right away. You can say, well,
why did I make that prediction? What are my features there? How, how shall I change those?
What are the contents of my deep network that led me to make that prediction? I need to change those.
And, and that's true at each step. And you, so when you go from here to here to here,
you can update this guy. And then you can forget about him, but you're never going to update him
again. Whereas in Monte Carlo, you have to remember why you made each one of these predictions
until you get to the end. Then you have to go back and say, well, okay, why did I make that one?
And then, and then adjust its weights, know it with knowledge of its feature vector and the
contents of your network and, and so on. Yeah. And it's terrible. It's distribution because you
keep, all this time you're doing nothing. You're driving home, but you can't do any learning.
Okay. You can only wait till the end. You know the answer. And then you can go back
to all the earlier things and learn them. So the distribution of computation is poor.
The memory is poor. It's, it's, it's just kind of inconvenient. It's much more convenient if you
can do it as you go along. And you think about it. You're in your car. You're trying to drive home.
You get stuck behind a truck. Do you say, you say, you say, this is bad. You know, I say,
it's going to take me longer than I thought. I was too optimistic before. You don't say, well, you know,
maybe this truck will disappear. And you don't say, hold the whole judgment. You could hold judgment
until you get home. But, you know, my feeling is I'm learning as I go along and I'm responding to
what I see. And we actually do learn as we go along. Okay. So I think I've said these things in TD
with Monte Carlo. You can be fully incremental learning as you go along. You can learn before
you know the final outcome. This means you need less memory and less peak computation. You don't
have to do it all at the end. You can even learn if you don't, if you never find out how long it
takes you to actually go home. You know, maybe you're, get a phone call and you're, you're called
away for something important and you never find out. But you can learn without knowing the final,
the final outcome. Now, when you do the math, both of these methods will converge. And but,
so the only question is which is faster. Okay. This is the only question, but it's a big question.
Okay. So I don't know. Let's just do a simple experiment and find, find out. Okay. So here's a
trivial experiment, famous, famous, I don't know. I did this a long time ago. It's just a random walk
and it's meant to test the idea of which one of these is better. So we're going to,
we're going to have five states and we're just going to have an estimate for each state
of what the outcome will be. Okay. This random walk, it takes 50 step right and left. And you
start in the middle and you go back and forth, back and forth, back and forth until you end at
one side. Okay. If you end at this side, you get a zero. And you do get zeros all along the way
for your reward. But if you know what you get a non-zero is if you end on the right side,
you get a, you get a reward of one. Okay. So are you with me? What's the correct prediction?
So the correct prediction, there's, there's no discounting here. So we're just trying to predict
the sum of the rewards up until the end. What's the correct prediction for the start state C?
You're in C. What's the correct prediction for the expected value of your return?
Gamma squared. Gamma squared. Gamma is one.
So the expected, expected return. So if you, if you end, if you go blah, blah, blah, blah,
and you end on this side, the return is one. If you end on the other side,
the return has to be zero. When you start in the middle, what do we expect the return to be?
You know, by symmetry it's going to be like 0.5. Okay. And
state B, I don't know, it's going to be less than 0.5.
And state A, still less. Anyone want to guess what they're, what they are?
The true values of all the states? C is definitely a half. What do you think B is?
Guess, just guess. A third. Yeah. A third is a third. And, and the next one is a sixth.
Yeah. These just go by sixths. 1, 6, 2, 6, 3, 6, 4, 6, 5, 6. And those are plotted here.
This line is supposed to be the true values. So state A truly has a true value that's 1,
6. And state B is a true value that's 1 third. This has a true value of 1 half.
And so forth. And these other lines are the estimated values from, from applying TD to it.
Okay. So TD, you do have to care about the initial conditions because it's making a guess
from a guess, right? So your, your guess is, you know, affect things. They either pollute things or,
or brilliantly provide good guesses and value. Okay. They, they, so the initial guess is 0.
So at time, at episode 0, all of the estimated values, excuse me, all the estimated values are
half. Because since there could be zeros and ones for possible returns, it seemed reasonable to start
the estimated values all at a half. What doesn't happen to be right for the middle state, but it's,
it's quite a bit wrong for the other states. And then we're going to do episodes and, and,
and we're going to learn on every time step. And we're going to update the states according to
the TD rule. And after one episode, we have this, this darker line. This is the episode number. After
one episode, we have these values for our estimated values of the five states, right? So what do you
know happened on the first episode? You ended on this side because, well, what's going to have,
what is this TD rule going to do? What is, what is the TD error going to do? Let's say that we
start in the middle and we go either way, we move around. Well, what's, what's the TD error going
to be? It's going to be the reward. The reward beginning is going to be zero. And then it's
gamma's ones. And we forget about gamma. And then we just basically the change in the value.
Okay. And if you went from, say, this state to this state, what's the change in value?
Zero. Because they're all, their estimated values are all a half. And so we went from one state
from an estimate of a half to another state with an estimate of a half. So as we go all this
bouncing all around, nothing is going to happen really until we run into one of the ends. Well,
we ran, we could run into this end or we could run into that end. If we run into
this end, we'll go from a state that was a half to, oh, the terminal state. The terminal state
always by definition has a value of zero. Okay. So, so over here, if you, if you did this
transition, you get a reward of one, a reward of one, the starting state, the starting state here
would be a half and you get the thing that has zero. Okay. So it'll be one minus a half. It'll
be positive a half. Anyway, the, the estimate of state E would go up and that didn't happen
because here's the estimate of state E, it's still at a half. Instead, what happened is we ended
on this side and we went from here that had an estimated value of a half to this thing which
has an estimated value zero, the terminal state. And so we went from a half to zero and our, our
TD error is minus a half. So we moved down from a half towards zero and you can actually see how
far we moved. We actually moved by from a half to 0.45 and so our step size alpha was 10, was one
tenth. Okay. We can understand this algorithm. It's very simple. And, and then as you get more
episodes, you get closer and closer to the true value after 10 episodes. You get the blue line
after 100 episodes, you get close to the true values. You never get exactly to the true values
because there's always randomness in the individual episode and, and alpha is non-zero. It's a tenth
and so you keep bumping around, bubbling around the true values. So that's, that's an example. Now,
let's compare now Monte Carlo versus TD on this problem. Okay. And we have to draw whole learning
curves now and we have to worry about what's the value of the step size. Okay. So what I'm showing
you in these, this is a learning curve, meaning the x-axis is time or episode number and the y-axis
is some measure of error. It's actually the root mean square error averaged over the five states
and many, many iterations of the whole experiment. I guess 100 iterations of the whole experiment.
Okay. So as I said, they're going down and everything's getting better over time, but
they, things will not go to zero because, well, we have the step size, one tenth and,
or whatever the step size is, it's, it's always going to be there and we're always going to have
some residual error. I don't know which one should we look at first. Maybe Monte Carlo,
it's simpler. We just, Monte Carlo, we just wait until you know what the final return is and then
you do your update for all the, all the states that were visited. So if you take a nice slow
learning rate, one hundredth, we just gradually move down towards zero error and it's actually,
this, this alpha equals one hundredth. They're very slow. We'll actually get the closest and the
long, long, long run to zero error, but it's very slow. So you might want to go faster. If we take
alpha as one fiftieth, we go down faster, but we're not, we're going to start to
bubble and what we try, try going as fast as .04 and we do go, the initial part is fastest,
but now we're definitely bubbling and you can't really do better. There's no step size which
will do better than the ones that are shown here. If you try to go faster, you're going to,
you know, you may be a little bit faster at the beginning, but you're going to level out at a
higher level. Okay. And for TD, we see a similar pattern, but all the numbers are lower. Okay. So
here's our, the one, the slowest one I'm showing here is .05 and it goes,
it's slowest, but it gets lower and then other ones are faster and of course they, they bubble
more and they, they don't get as low in the long run. Okay. Now, if we have the long, someone may
ask you, you may, some of you may be wondering, what's going on with that TD stuff? Because
it seems like they go down and they start to come up again. Is anybody wondering that?
Anybody wondering that? Yeah. It's, it's weird, isn't it? And it's real. It's not a bug in my
program. That's the first thing to be sure of. Yeah. It's, it has to do with the fact that actually
starting out the estimates at a half is, is not, is not stupid. It's actually a, a reasonable guess.
If you started all the estimates out at something really bad, then you wouldn't see that bounce.
Like all the bounces, we go down and then we seem to bounce and we come up a little bit higher.
And that bounce is really interesting. It has to do with the fact that we do have some, some
effect of the initial estimates in TD and whereas we don't really, at least not as much
for Monte Carlo. Okay. So, so this is just a random walk. And I've sort of, I've been
systematic about the random walk. And I don't know, the big picture is that TD is faster. Okay.
There's a bounce, okay, whatever. But it's still much faster. But this is just one problem.
This is just a random walk. Okay. Maybe it's something special about the random walk. Or maybe if I
did it on, you know, Atari games, I would get a more fundamental result. Now, I like to do simple
things. Question. Oh, no, they, they, they all converge even with a, with a non, well, with a non
zero, if you don't reduce the step size, then you don't expect anything to converge. Right.
They would converge in the mean. Okay. And all of them will converge to a mean
that depends on the step size and higher step sizes would be higher and lower step size would be
lower convergence point. Yeah. The convergence properties are roughly the same in both cases.
I want, I want to, I want to ask now, can I say anything about the general case,
not for the random walk? Can I say general case? Actually, no, no, I'm going to do,
I'm going to do that in a minute. But first I'm going to do the random walk again under this,
this setting kind of called batch updating. Okay. Batch updating means we, we take some
training set like 100 episodes or 10 episodes or whatever, and we present it over and over again
until it does converge. So even for a finite step size, we will get complete convergence if we
repeatedly present the same training set. Okay. Because there's no randomness in random samples,
you're just spending the same data over and over again, and you will converge, the two methods,
TD and Monte Carlo converge to two different things. And this is for constant alpha. These
diagrams converge to different things. As long as your step size is small enough, it won't depend on,
on your step size. Yeah. All step sizes, as long as they're small enough that you don't diverge,
will converge to the same thing. Okay. And they converge to different things. The two
algorithms converge to different things. So we can ask on this problem, which one converges to
a better thing if we present the data over and over again to the algorithm. Okay. And here's the
results on the random walk again. We have, we have different numbers of different sizes
are our training sets. We're increasing that along here. But for each case, say, say with 50,
a training set of 50 episodes, we present those 50 over and over and over again until we converge,
and we measure the, the asymptotic error that is independent now of the step size. So I,
I can now eliminate this, the effect of the step size, just going to measure, you know,
which algorithm is giving me a better result on this problem. Okay. And TD is faster
and by this measure, I mean, we're doing a lot more computation, we have to go to convergence,
we have to repeatedly present things, none of which I like, but it's getting us
insight into what the real difference between the two algorithms. So it's like TD is moving
towards a better place, even on a single example, that's suggested by the initial results. And if
you, if you go over and over again, you can get that. Okay. Now, this again is all random walk,
and you have to ask if this is, happens on all problems. So one approach would be to do all
problems. And that's obviously not satisfactory. So, so what can you do instead? You can try to
prove a theorem. Okay. And you can, and you can also try to get insight. I guess I'm going to try
to get insight first, and then we'll do the, the formal result. So let's try to get insight
into us as people. I want you to, you, you to be the predictor. Imagine you were having some
experience. So imagine you were experiencing a training set of these eight, eight episodes. These
are all very short episodes, right? So most of them are episodes like B zero means I'm in state B,
and then I get a reward of zero, and the end of the episode, that's the end of the episode.
Okay. Or I see state B, I get a reward of one, and that's the end of the episode. The only non-trivial
episode is this first one, where I'm in state A, I get a zero, and then I go to state B, and for B,
I get a reward of zero, and that's the end of the episode. Okay. So that's the data you see,
just these eight episodes. And I want you to tell me what, what prediction would you make?
Okay. The first question is, what prediction would you make for state B? If you found yourself
in state B, what would you guess for the expected return ahead of you from state B?
Say again?
Three quarters. I agree, because what would you, how do we do that? We said, well, I was in state
B all eight times, and six of them ended up with a one, and two of them ended up with a zero,
so you're going to guess three quarters. Okay. Okay. That was an easy one.
What about state A? State A, it's really much more uncertain. We've only been in state A once.
And what are you going to ask for state A? Just take a moment and think about it.
What are you going to guess the return if you find yourself again in state A?
What is the estimated value? What would you estimate the value of state A as?
Okay. Now, I'm going to say right away that this is a question there's multiple good answers to.
Okay. So I'd like someone to raise their hand and give me one, give me one answer and why it's
a good answer. Okay. How about you? You always, if you always go from A to B, and the only time
you've seen A, we went from A to B, and B has value 75%, three quarters, then A should also.
That sort of makes sense. What's another good answer?
Zero, because every time that A occurs, the reward is zero.
Yeah. I don't know if everyone heard that. You said you've seen A once. Every time you saw it,
the return was zero. So, you know, why not predict zero? Okay. Now, those two answers,
those two answers are the two, the Monte Carlo's answers and TD's answers.
Okay. So, we could say zero. That's what Monte Carlo would say. Monte Carlo just looks at what
happened. I was in A once and the outcome was zero. So, I should predict zero. Monte Carlo.
Now, the other one, the other one is what TD predicts. It's also what you would predict and
this is the gentleman explained what was going on in his head. He was saying, well, I'd seen
A go to B and all that sort of stuff. He was building in his head this model. He was saying,
I'd seen, the only time I'd seen A, it went to state B. By the way, the reward was zero on that
transition. So, let's guess that that happens every time. And then in B, B I saw A times and
six out of the eight went one way to a one and then stopped and two out of the, I saw it eight
times. Six out of the eight went to a one and two out of the eight went to a zero. So, I'm building
this in my head. Okay. This is like, and this has a name. This is called the maximum likelihood
model of the MDP. It just means what you'd get by counting. Say, how often do you go from here,
turn those into probabilities. Okay. This is the maximum likelihood model of the underlying
Markov process. And then if you take this model and you solve it, if this is the true world,
then the true value is three quarters. Okay. And so, this is the general phenomenon of what TD does.
If you present a training set over and over again to it, it gets the answer that you would get if
you collected all the data, made a maximum likelihood model of the world, and then solved
that model with dynamic programming or with any method. The true solution if that model was the
reality. Okay. And so, that's, well, that's, that's why TD is, is, can be faster, can be better.
Because it's using this Markov property that, saying, oh, and I've gotten a B. I know B is a
Markov state. And whereas, Monte Carlo just says, I don't care about what happened in between.
I ended up with a, getting a zero. Okay. So, to summarize that, the prediction that best
matches the training data is the Monte Carlo estimate. Best matches the training data. Remember
the training data? Okay. And if you saw, you saw A once, and it ended up with a zero. So,
you want to match the training data, the right prediction is the value of A is zero.
That is the prediction that will best match the data. Okay. Now, of course, I want to tell you,
we don't want to match the data. We don't want to, we don't want to minimize the mean square error
on the training set. Weird, huh? I mean, it seems like we should want to minimize the mean
square error on the training set. And that's why I've gone through at some length this example
with you guys. So, I want you to have some intuition of why we don't want to minimize the
mean square error on the training set. So, what can I offer you if you, if I can't offer you
minimizing the mean square error on the training set, it's going to be minimizing the mean square
error on future experience. Because we don't really care about the training set, past experience.
We care about the future. And so, we think if we believe we have real states here,
we would think that the estimate, the value of A is three quarters will actually be a better
match to future data. Okay. If we get a new experience with state A, it's probably going to be
end in a one, three quarters chance of being in a one. Okay. So, it's interesting. Now, we have
to really distinguish between minimizing error on the training set, minimizing error on the future.
These are different things. And TD can be faster because it can take advantage of the state
property and match future experience better. Now, even as I said that, you may be able to get
immediately a sense of possible limitation of TD methods. As I said, they're going to take
advantage of the state property that I know when I get to B. It doesn't matter how I got to B.
But in real life, you don't normally have complete state knowledge. You have incomplete state
knowledge. If any time you're using function approximation, here we're just using discrete
states. And any time you're using function approximation, you're going to have imperfect
knowledge, imperfect state information. And so, in the end, it's going to be a mix.
It's going to be a question which is going to win in practice.
But in the end, it's going to be TD that wins in practice. I'm thinking. Okay. In the end, Ed.
Okay. Okay. So, yeah.
Good. Good time for questions.
So, usually, when you introduce the problem of originally being of your, you use B to predict
the return, right? But in this example, B is predicting the reward of each state.
No. V is still predicting the sum of the rewards from the state to the end.
And so, you remember the example was A is followed by zero, is followed by B is followed by another
zero, is followed by termination. So, the right, we're still trying to predict the cumulative
reward until the end. Okay. Thank you for clarifying that question. Yes. Thank you for
for this possible disadvantage that you are so thinking that the Markov model said it can be
a drawback. I'm coming back to the initial comparison of this initial idea that to be scalable,
to really have this computing to be non-lockout. Yes? Yes. You said that. I was thinking that,
I mean, if you have an explicit representation of the states. Yes. Then the memory overhead of
mc is linear. Because to have this state there, you can put just a number of things. And remember
which, who they are. This is linear. And of course, the computation is not localized. That's okay.
But then if you have an implicit representation, you point to when you say that you're having a
function approximation. Then of course, in this case, td is not going to be subliminal.
Yes, depending on the size of the representation of the function. But then
they obey this function, this compact representation can be expensive. So,
there's something there about the space and the locality. It's hard to have both of them.
So, let's think that through. So, let's assume that instead of having a table look,
this is all table look up. But instead of that, we have a complicated neural network.
And so then when we get a new error, we have to back propagate through the network. We have to
do somewhat expensive computation. But it's not, should we consider that expensive? I'm going to
say no. Because even the back propagation of one error, back propagation through the network,
that complexity is the same, it's the same order as a forward pass to the network. So,
we already spent, we had to make a forward pass in order to get the prediction. And so,
there's an equivalent complexity to do the update. So, even though it's a bunch of weights,
it's, we should consider that cheap. Okay? Okay. It's linear in the size of the network. Good.
Good. Any other questions?
Good. Okay. So,
so, I've just done one step methods, tabular methods, model free methods. All these qualifiers
can be generalized. But even here in the simplest case, one step method meaning we're looking from
one step to the next step rather than one step to five steps ahead like in AC3.
But there, we can see the basic ideas. And it's tabular, tabular is easy to think about,
but it all, all the ideas really do generalize to the, to the network case. Complicated function
approximator. We've seen the basic things is that we're going to bootstrap and sample,
combine aspect of dynamic programming, Monte Carlo. The TD methods are computationally congenial,
just a little bit of work on each step. And you don't have to wait until the end and then do a
whole bunch of work. And if the world is truly mark-off, then TD methods are faster. That's what
we see. And it has to do with the past data versus the future data. Now, before I go into
a somewhat new thing, I'd like to also try to summarize where we are in terms of pictorially.
What today we've talked about contrasting TD method, a one-step TD method, which is like,
this is what I, this is a little picture is what I used to summarize the method. That
thing at the top, this thing updating it. And this says I go ahead one action and one next
state and then I use my estimate here to improve this guy. This is like a picture of the algorithm.
And the same kind of picture for Monte Carlo is you want to estimate, improve the estimate of
this guy's value. You go ahead one state, one action state, action state, all the way to the end,
and you see the final outcome. And then you back all that up. And this is like a dimension.
You can occupy intermediate methods. You can do two, two-step methods, three-step methods,
four-step methods, five-step methods. This is like an infinite step method where you go all the way
to the end of the episode. And then there's the parameter lambda you might have heard about in
TD lambda, the eligibility trace parameter. It's really a bootstrapping parameter. It determines
it's not the number of steps, but it's analogous to the number of steps.
And so this is really a dimension and we can occupy any point along this dimension.
Okay? And that's, now there's a second dimension which is are we going to use planning? Okay? Are we
going to use knowledge of the model of the world? Okay? Dynamic programming. Dynamic programming
is this corner. It means we're still going to do one-step methods. We're only, in dynamic
point, you only have one step. And use your estimates at one step and look ahead into the future.
Okay? And so that's moving along the top here. It says keep, keep these short
backups, one-step backups, but instead of doing a sample, do all the possibilities.
And that's dynamic programming. And then there's a fourth corner where the analogous of Monte Carlo,
but with planning, is like exhaustive search. We consider all the possibilities all the way to
the end. And so we can get these four corners as plastic methods and then we can occupy the
area in between them. And that's kind of a big space of reinforcement learning methods,
although it's, it's certainly not the whole space.
Okay. Now, where should we go next? I don't have time to present all that I wanted to say,
but let me just sort of, we've done a good group here. I can almost sort of, sort of wrap up
and talk about the future from here. But let me just tell you some of the things that we,
if we had more time, we might talk about. Okay? First, we would talk about estimating,
instead of state values, we'd talk about estimating action values, because as you know,
really for control, you want to do the action values. And it's not that different.
You would just estimate q pi instead of v pi. And it's going to be lowercase because that's,
this is the true value function, q pi. And you would estimate it with, with an action value
estimate, since it's just tabular, I can say a big q of s a for the actual state and action
counter. And then this update, this update is essentially the, the TD update. It's just done
state action pairs rather than on states, right? This is still a TD error. This is my written
estimate. This is the estimate for the next state. This is the SARSA algorithm. That's,
that's quite straightforward. And so the SARSA algorithm ends up being that rule, that update
done over and over again. And, and some examples. Q learning, Q learning is, is, is almost the same.
The rule is, you know, we're doing, updating a state, a state action value. The new one is the
old one plus a step size times a TD error. But the TD error is a little bit different. The TD error,
we, you know, we're comparing our original estimate to next reward plus something of the next state.
But it's the max of our possible actions at the next state. That's Q learning.
I like to draw this picture. This is its picture. The picture says, you know, I'm updating a state
action, all the possible estimated action values, take the max of them. That's this, this part of
the rule, right? Max or all possible a. And, and then I back up the max to improve the estimate of
this guy at the top. That's Q learning. It's a TD algorithm with that particular target. And
this is a nice example. Cliff walk is a nice example comparing SARSA and Q learning. SARSA
is an on policy method. Q learning is an off policy method. And we see that actually here,
the Y axis or the, you know, the Y axis is reward per episode. SARSA will actually work out better.
I don't have time to explain this example. I wouldn't, I didn't want to do something yet.
You guys didn't really understand. Better to skip over it. Okay. Here's one sort of new algorithm.
Expected SARSA. Expected SARSA, if we look at the picture, right? In Q learning, you take
all these possible things you might do and you take the best of them. Take the max. The arc means
max. And if you have, if you don't have an arc, then it means expectation. Okay. So,
an expected SARSA, you don't take the best of the things you might do. You take the,
the expectation based on how you would actually do them according to your policy. So, here we are.
We're summing over all the things we might do. How likely are we to do it under our policy?
Which we know. We know our policy. We know how likely we are. Like maybe it's epsilon greedy.
And we take the expectation. The action value times our likelihood of doing that action. And we
back up that. Okay. And that's, that's arguably an improved version of SARSA. And it can also be
made an off policy version of SARSA. And there's some other novelties. You can, you can do an
off policy version of expected SARSA. And I've used the word off policy a couple of times without
explaining it. I'm sorry about that. But off policy means that you're learning about a policy
that's different than the one you're following. Okay. And on policy means you're learning about
the same policy as the one you're following. The same one that's generating the data. So,
the way to remember it is that on policy is almost one policy. And in on policy methods,
there is only one policy. It's a policy you're doing. It's a policy you're learning about.
But very often these want to be different. Like you want to do something that is more exploratory.
And you might want to learn the optimal policy. Okay. So, if you're going to learn the optimal
policy, but you're going to actually get your data in an exploratory way, which is not going to be
optimal, then you have two policies. Okay. And then you're in the realm of off policy learning. Q
learning does this, but off policy learning is, is theoretically more difficult and more challenging
for our methods. Okay. That's, that's off policy. Okay. So, so I basically just extended these
things to control. Okay. Now, we've seen some, some methods that can do the on policy case
and the off policy case. We didn't talk about double Q learning. Okay. So, I've talked a lot about
about, do we want to use Monte Carlo or do we want to use TD? Okay. And it's, there's a sense in
which we don't have to choose because if you use an algorithm like, if you use TD, you can
parametrically vary lambda or vary the, the height of your backups to get, to give you any intermediate
point between one step TD and Monte Carlo. You can get both. And a key heat way of doing this is
with the parameter lambda, the bootstrapping parameter, which I didn't really talk about,
but it is a way to vary parametrically between TD and Monte Carlo. So if lambda equals zero,
which is the left side of all these graphs, that's pure TD, pure bootstrapping. Okay. And if
lambda is one, that means you're not doing any bootstrapping. That's Monte Carlo. Okay. Okay.
So now all these graphs have lambda across the bottom. So it's basically like this to pure,
to no bootstrapping. And they all have a measure of performance on the top where, in all cases,
lower is better. Okay. So it's like, it's like a mountain car and you want to have
few steps to get to the top of the hill. Okay. So, and what you see, looking at this, is that,
you know, performance depends on lambda. This is the, this is the random walk. And it's actually
not best at, at lambda equals zero. Pure TD is not the best. You can do better if you do some
amount of TD, intermediate between pure TD and Monte Carlo. But if you go all the way to lambda
equals one, then things get really bad. That's like the worst case in general. And that's the
pattern. Lambda equals zero is Monte Carlo. And Monte Carlo has really high variance and has to
be ruled out. It's not very happy if you are committed to Monte Carlo. You can do TD and say,
oh, I can pick any step in between. That's what you want to have this facility of doing some
bootstrapping. And that's sort of some evidence for that, even though this is old data, I think,
you know, like Peter would agree that, that Monte Carlo is, is, is not really an efficient strategy
to do it in a pure way. Okay. Now another, I want to give one slide also for taking questions.
On, on the linear case, a case with a real function approximation, I'm not going to go to
nonlinear networks, but I want to go to something which you talk so much theoretically about,
which is the linear case. So suppose we're doing linear function approximation. Linear
function approximation means that our estimate, estimated value is formed as an inner product
between a parameter, a, a weight vector and a feature vector. Okay. So feature vectors are
phi, phi for feature, phi of t is, is, is our feature vector for the state at time t.
And the parameter vector is theta. And that might think about as the weights of your,
of your network of, this is a linear network. Okay. So we take the inner product, and so
this transpose thing means the inner product. So theta inner product with phi is our estimated
value of state t. It's the, it's the estimated value of the state at time t because this is the
feature vector for the state at time t. Okay. So this is our estimate, this is the estimated value
of time t. This is our estimated value at time t plus 1. So this really is a TD error.
And this is a TD rule. The TD rule is that the parameters are the old parameters plus step size
times our TD error. And this, the gradient in, in the general, in general, nonlinear cases would
be a gradient of, of the, of the prediction with respect to the parameters. In the linear case,
it's just the feature vector phi, phi, excuse me. Okay. So that rule should be fairly familiar to
you now. It's just a TD rule using a stochastic gradient descent. It's, lots can be said about
that. But that's, that's the standard t linear TD t zero. And if you look at this, you can,
of course, write it like this. You can take the phi, the phi and carry it inside. Here it's there,
and you carry it inside here with a little some transposy stuff. You can write it like that.
And this is a vector. If you take the expectation, we're going to take the expectation. Okay. So
in expectation, the, the new feature vector is the old one plus a step size. And this, this thing,
this thing, in expectation, what is it? Okay. Well, I'm just going to make some names for it. This
thing is a vector. And this thing is a matrix times theta. Okay. So b is what I'm going to call that
vector. So that vector b is just the expected value of this thing. It's, it is a well defined
vector. You don't know it, but it's, it's, it's there. And this thing is a matrix because it's an
outer product of feature vector with a change in the feature vectors. So the expectation of that
matrix is what I'm going to call a. So that means let's me write a whole expectation like this.
And I'm interested in the, in what happens at the fixed point. Where will this settle
at an expectation? Where will it converge? Where will the expected update be zero? Well, the expected
update is basically this part. So I want to know when is that zero? Well, that's going to be zero
when b equals a theta. Or when b minus a theta is zero. Okay. And that, that theta, that's a
special theta for which this is true. So b minus a theta equals zero. I'm going to call that theta
td because it's the fixed point that td converges to, the linear td converges to. Okay. And then
you can just compute it. So b is, is, is a times theta. And so you have to take the inverse of a
and you get the td fixed point is the a inverse b. And which, which by the way is, is, is, is a key
to another algorithm. Lee squares algorithm says estimate a directly, even though it's a matrix.
Take its inverse and also estimate b directly and then multiply them together to get, that's how the
least squares td works. Okay. But, but this way of computing what, what, what the algorithm converges
to. And then you can say something theoretical about it. This is your, our guarantee that we get,
that the mean square value error, measure how often the values are, is bounded by an expansion
times the, the mean square value error of the best theta. So this means that we don't find the best
theta. Okay. But we do get an expansion of it. Anyways, that's what the theory would look like
if we had more time to talk about it. I just wanted to mention quickly some, some of the
frontiers, some things that people are working on now. So off policy prediction is a big area
people are working on, trying to generalize to the off policy case. Also, we'd like to talk about
non, have some theory for the case of nonlinear function approximation. There's just a little
bit of that. There's also very little convergence theory for control methods period. And I think
Chaba, maybe we'll talk about that tomorrow. And we also like to say things beyond convergence.
We'd like to like to, you know, how well can you do in a finite amount of time? How fast do you
converge? Now, when you combine TDE with, with deep learning, a lot of different issues come up.
And I think there's just a lot of uncertainty. Do we really need a replay buffer? So that one of the,
the folk theorems is that, oh, especially you have instability and correlation, quote,
correlation. So we need this thing called the replay buffer. But I think it's really,
there's lots of questions about what happens when we combine TDE with deep learning.
And finally, the idea of predicting things other than reward. Remember, I started with that. We
might want to, this is TDE is a general prediction method, multi step prediction methods. We want
to use it to predict other things. And in particular, we want to learn it to use, learn a model of
the world. So in conclusion, I guess what I want to say is something like this. The TDE learning
is a uniquely important kind of learning. Anyway, maybe it's ubiquitous. We're always going to be
using it. And I think this may be true. It's a hypothesis. So anyway, it's learning to predict,
which is perhaps the only scalable kind of learning. It's, it's, it's a kind of learning that's
specialized for general multi step prediction, which may be the key to perception, modeling the
world, the meaning of our knowledge. It's key ideas to take advantage of the state property,
which can make it fast and efficient, but can also make it asymptotically biased. And it's
other key claim to fame is that it's computationally cheap, congenial. And we're only beginning to use
it to explore different ways of using it for things other than reward. Thank you very much.
