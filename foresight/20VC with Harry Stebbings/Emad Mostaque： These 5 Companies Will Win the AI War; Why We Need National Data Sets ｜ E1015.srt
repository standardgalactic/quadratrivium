1
00:00:00,000 --> 00:00:01,600
I think this is bigger than the printing press.

2
00:00:01,600 --> 00:00:02,600
It's bigger than anything.

3
00:00:02,640 --> 00:00:04,560
And so that's one of the reasons I signed the letter.

4
00:00:04,560 --> 00:00:07,880
I said, we have to get this discussion going in public right now.

5
00:00:08,360 --> 00:00:12,440
We've got to stop pre-training big models on all the crazy crap of the internet.

6
00:00:13,280 --> 00:00:17,040
And like, we've got to do it fast because this is coming like a train.

7
00:00:18,080 --> 00:00:19,840
And that I am so excited for this.

8
00:00:19,840 --> 00:00:23,720
I heard so many great things from specifically Ashton who helped with questions

9
00:00:23,720 --> 00:00:25,120
and then also Dan Rose.

10
00:00:25,120 --> 00:00:26,760
So thank you so much for joining me today.

11
00:00:26,760 --> 00:00:27,640
It's my pleasure.

12
00:00:27,640 --> 00:00:29,840
Now, I want to start with a little bit on you.

13
00:00:29,840 --> 00:00:31,680
You moved around a little bit in your childhood.

14
00:00:32,000 --> 00:00:34,240
Take me back to the childhood, the moving around.

15
00:00:34,240 --> 00:00:37,400
And it's a weird commonality I found with the most talented founders.

16
00:00:37,680 --> 00:00:38,840
They all moved around.

17
00:00:38,840 --> 00:00:42,000
So take me to that and how it impacted your mindset.

18
00:00:42,280 --> 00:00:45,440
So I was born in Jordan, grew up in Bangladesh, came to the UK.

19
00:00:46,080 --> 00:00:47,480
Yeah, didn't move around that much.

20
00:00:47,480 --> 00:00:50,360
But with my father a bit as he lectured in various places.

21
00:00:50,920 --> 00:00:55,920
And it was always a bit of a struggle fitting in, but then you learn to adapt.

22
00:00:56,280 --> 00:00:58,520
You learn to adapt to new scenarios, new environments.

23
00:00:58,520 --> 00:01:01,200
Like, oh, don't speak the language, kind of what's happening.

24
00:01:01,200 --> 00:01:03,080
Let's learn and let's move on from there.

25
00:01:03,160 --> 00:01:05,520
I think it gave me a bit of appreciation of the world as well,

26
00:01:05,880 --> 00:01:08,120
because we stick in our monocultures very often.

27
00:01:08,120 --> 00:01:09,840
Like I'd only ever been to Silicon Valley.

28
00:01:09,840 --> 00:01:12,280
I should have been the barrier once before last October.

29
00:01:12,920 --> 00:01:16,280
And so this whole tech monoculture has been a bit of a shock to me.

30
00:01:16,880 --> 00:01:19,920
And I'm like, there's more to the world than that.

31
00:01:20,000 --> 00:01:22,240
So this is some interesting things around that.

32
00:01:22,440 --> 00:01:27,160
Talk to me hedge funds first and then then what happened?

33
00:01:27,320 --> 00:01:28,480
We mentioned it a little bit before.

34
00:01:28,480 --> 00:01:29,320
Why did you make the move?

35
00:01:29,760 --> 00:01:33,000
So actually, I was an enterprise developer in my gap here at Metaswitch

36
00:01:33,000 --> 00:01:34,680
in the UK doing voice over IP.

37
00:01:34,680 --> 00:01:37,040
Metaswitch. This is Chris Maz's company.

38
00:01:37,040 --> 00:01:39,560
Yes. So I took my gap here and was like,

39
00:01:39,560 --> 00:01:41,000
I might as well be enterprise programmer.

40
00:01:41,000 --> 00:01:42,200
I didn't know what that would be like.

41
00:01:42,200 --> 00:01:43,880
This was before GitHub and everything.

42
00:01:43,880 --> 00:01:47,040
So we had subversion, you know, kids these days have it so easy.

43
00:01:48,320 --> 00:01:49,760
And then I was like, what do I do now?

44
00:01:49,760 --> 00:01:54,400
And so I became a VC analyst at Oxford Capital Partners with the MOTS there.

45
00:01:54,760 --> 00:01:55,560
And that was a lot of fun.

46
00:01:55,560 --> 00:01:56,280
They were fantastic.

47
00:01:56,280 --> 00:01:57,560
And then I was like, I want to do movies.

48
00:01:57,560 --> 00:01:59,320
So I became a movie reviewer.

49
00:01:59,320 --> 00:02:02,000
So I did the Rain Dance Film Festival, British Independent Film Awards.

50
00:02:02,400 --> 00:02:04,120
And they were just popping around doing random things

51
00:02:04,120 --> 00:02:07,120
and then accidentally became a hedge fund manager.

52
00:02:07,120 --> 00:02:09,760
Why did you move from hedge funds to startups?

53
00:02:10,200 --> 00:02:13,040
So with the hedge funds, so I joined Pictay Asset Management

54
00:02:13,040 --> 00:02:15,600
and then like the CIO left and there was like this fund

55
00:02:15,600 --> 00:02:17,960
and I got to be a portfolio manager when I was 23.

56
00:02:18,400 --> 00:02:21,560
And so I grew my beard to look a bit older and it's coming.

57
00:02:22,000 --> 00:02:22,800
Get the clip on, Harry.

58
00:02:22,800 --> 00:02:25,360
I would. I can't do the moustache, but I still wear the glasses

59
00:02:25,400 --> 00:02:27,680
when I just need to try extra hard to force it out.

60
00:02:27,680 --> 00:02:30,520
Right. And so I did that for a number of years

61
00:02:30,520 --> 00:02:32,960
and you know, reasonably successful, made lots of people money.

62
00:02:33,560 --> 00:02:35,640
Not so much myself because I was too young.

63
00:02:35,640 --> 00:02:37,840
Again, they're like, you're too young to be a fun one.

64
00:02:37,840 --> 00:02:39,840
And then my son was diagnosed with autism and I quit.

65
00:02:40,040 --> 00:02:43,520
And because they said there was no cure, no treatment, no information.

66
00:02:43,520 --> 00:02:44,840
I was like, I'm a hedge fund manager.

67
00:02:44,840 --> 00:02:46,640
I can deconstruct things.

68
00:02:46,640 --> 00:02:50,000
And so built an AI team and then did a literature analysis

69
00:02:50,000 --> 00:02:53,200
of all the autism literature to try and figure out the commonalities

70
00:02:53,200 --> 00:02:54,600
and then drug repurposing.

71
00:02:54,640 --> 00:02:57,040
So focusing on GABA, glutamate balance in the brain.

72
00:02:57,360 --> 00:02:59,640
GABA is what you get when you pop a valium.

73
00:02:59,640 --> 00:03:02,160
It calms you down and glutamate excites you, you know.

74
00:03:02,160 --> 00:03:06,440
And so in kids and people with ASD, it's like there's too much noise going on.

75
00:03:06,440 --> 00:03:08,360
It's like when you're tapping your leg and you can't focus.

76
00:03:08,760 --> 00:03:10,960
And so that's why you get this sensitivity.

77
00:03:10,960 --> 00:03:13,080
Sometimes they can't speak like my son.

78
00:03:13,080 --> 00:03:15,200
And so it was like mechanisms to bring that down

79
00:03:15,840 --> 00:03:17,920
that then allowed to have applied behavioral analysis

80
00:03:17,920 --> 00:03:19,720
and these other things to reconstruct his speech.

81
00:03:19,720 --> 00:03:21,920
And then he went to mainstream school, which is pretty cool.

82
00:03:21,920 --> 00:03:22,880
It's unbelievable.

83
00:03:22,880 --> 00:03:26,440
I heard you said on another podcast and I was astounded and inspired by it.

84
00:03:26,720 --> 00:03:28,920
We mentioned before, you know, my mother's got MS.

85
00:03:29,320 --> 00:03:34,320
And I hate the doomsday only version of kind of AI in the future of GBT.

86
00:03:34,960 --> 00:03:38,000
You said to me before about its impact on health and MS

87
00:03:38,000 --> 00:03:39,880
in particular and other conditions.

88
00:03:40,400 --> 00:03:43,560
How can it be so transformatively to solve some of the world's

89
00:03:43,560 --> 00:03:45,520
most challenging chronic conditions?

90
00:03:45,520 --> 00:03:47,880
So I think a large part of our problem is that we can't scale

91
00:03:47,880 --> 00:03:51,120
because information flow is so limited as we write these things down.

92
00:03:51,120 --> 00:03:52,720
Like you can never capture all of that.

93
00:03:53,000 --> 00:03:57,160
So anyone who's had a loved one that has one of these conditions knows

94
00:03:57,160 --> 00:03:59,400
how difficult it is because you go from specialist to specialist

95
00:03:59,400 --> 00:04:01,880
to specialist and you try to build that mental map.

96
00:04:01,880 --> 00:04:03,920
And we're so lucky that we have so much access.

97
00:04:04,400 --> 00:04:07,120
But why isn't it that we can't just push about and see every clinical trial

98
00:04:07,120 --> 00:04:08,960
and a deconstruction of all those and things?

99
00:04:09,320 --> 00:04:12,640
What if you had a thousand GPT-4s organizing all that knowledge

100
00:04:13,160 --> 00:04:15,400
and then make it available to everyone?

101
00:04:15,400 --> 00:04:19,680
So you can see the exact potential mechanisms that are which MS works

102
00:04:19,920 --> 00:04:24,080
and all the potential food, other things that work with that.

103
00:04:24,080 --> 00:04:27,400
So as you try different things with your family member,

104
00:04:27,800 --> 00:04:30,840
you can see, well, she reacted this way to the food or this way to this medicine.

105
00:04:31,120 --> 00:04:34,040
And it is a more holistic thing because you can have personalized medicine

106
00:04:34,040 --> 00:04:36,480
versus one specialist for a thousand people.

107
00:04:36,960 --> 00:04:41,120
You can have a thousand GPT-4s or equivalents or med palm twos for you.

108
00:04:41,720 --> 00:04:45,040
So we need to organize all this knowledge and then use these language

109
00:04:45,040 --> 00:04:48,040
models and others to make it accessible to you.

110
00:04:48,480 --> 00:04:52,000
I'm really naive and basic in terms of my thinking,

111
00:04:52,000 --> 00:04:54,720
which is why I'm a venture capitalist.

112
00:04:54,720 --> 00:04:57,640
But my question is, what do we need to do to get to that state?

113
00:04:57,840 --> 00:05:00,200
When we look at the data needed from the individuals,

114
00:05:00,200 --> 00:05:05,120
the data, the data, the GPT's need, how we make the models work most efficiently?

115
00:05:05,120 --> 00:05:07,920
So first, we don't have to have that data for individuals.

116
00:05:07,920 --> 00:05:10,320
We had Galactica as a scientific language model,

117
00:05:10,320 --> 00:05:12,960
but now we have Med Palm 2 that exceeds doctor level.

118
00:05:12,960 --> 00:05:15,120
So that was a Google announcement yesterday.

119
00:05:15,160 --> 00:05:19,760
We have AIs that can understand articles better,

120
00:05:19,760 --> 00:05:21,640
as good as doctors, shall we say now?

121
00:05:21,640 --> 00:05:24,240
So we can scale that because why do you need one when you have a thousand?

122
00:05:24,240 --> 00:05:28,280
So we take the existing generalized knowledge and all the hypotheticals

123
00:05:28,280 --> 00:05:31,920
and we bring that together into an integrated common system available to everyone

124
00:05:31,920 --> 00:05:34,280
because the building blocks are nearly here for that.

125
00:05:34,280 --> 00:05:35,840
Then you can personalize it later.

126
00:05:35,840 --> 00:05:39,560
And again, there are regulations and things around that to how you're,

127
00:05:39,560 --> 00:05:42,640
again, how we treat our loved ones and other things like that.

128
00:05:42,640 --> 00:05:44,520
The first thing is let's get all the knowledge in one place

129
00:05:44,520 --> 00:05:47,120
and make it organized and useful.

130
00:05:47,120 --> 00:05:50,480
And so I think we're at that point now where the language models have just hit that point

131
00:05:50,480 --> 00:05:53,200
that we can organize all of the world's Alzheimer's knowledge,

132
00:05:53,200 --> 00:05:56,360
longevity knowledge, autism knowledge, MS knowledge.

133
00:05:56,360 --> 00:05:58,800
And you can just type and it can say, this is the source.

134
00:05:58,800 --> 00:05:59,680
This is what it looks like.

135
00:05:59,680 --> 00:06:01,160
These are some hypotheticals.

136
00:06:01,160 --> 00:06:03,360
This is what we know, what we know we don't know,

137
00:06:03,360 --> 00:06:05,560
what we think we might know, et cetera.

138
00:06:05,560 --> 00:06:08,760
And then it can learn about you and your queries

139
00:06:08,760 --> 00:06:12,360
because this is the other thing about lots of the language model things we've seen right now.

140
00:06:12,400 --> 00:06:14,800
They are one-to-one goldfish memory.

141
00:06:14,800 --> 00:06:16,480
The next step is one-to-one.

142
00:06:16,480 --> 00:06:19,680
It remembers what you're asking for, like a cookie or an embedding.

143
00:06:19,680 --> 00:06:22,280
And then it's you plus a thousand of these language models

144
00:06:22,280 --> 00:06:25,920
all going and doing your bidding, the agent-based kind of thing.

145
00:06:25,920 --> 00:06:28,280
Does this get around the incentive problem in healthcare?

146
00:06:28,280 --> 00:06:30,680
And what I mean by the incentive problem in healthcare is I'm sure you know there are

147
00:06:30,680 --> 00:06:33,800
a lot of diseases actually where it doesn't make kind of economic sense

148
00:06:33,800 --> 00:06:37,920
for a lot of pharmaceutical providers to chase research, to chase treatments

149
00:06:37,920 --> 00:06:40,480
because it's not a big enough market, because it's not,

150
00:06:40,520 --> 00:06:42,080
because it's six dollar treatment.

151
00:06:42,080 --> 00:06:44,760
Does this solve for that economic misalignment?

152
00:06:44,760 --> 00:06:46,960
I think it can help a lot with that economic misalignment

153
00:06:46,960 --> 00:06:50,280
because then you have an authoritative source where we can all come together

154
00:06:50,280 --> 00:06:53,080
and build that can analyze these things.

155
00:06:53,080 --> 00:06:55,600
Because there's this concept of agiddicity.

156
00:06:55,600 --> 00:06:59,600
A thousand coins tossed in a row is the same as a thousand coins tossed at once.

157
00:06:59,600 --> 00:07:03,120
And because we're so limited in our information in our medical system,

158
00:07:03,120 --> 00:07:05,240
like you know I just had my key management,

159
00:07:05,240 --> 00:07:07,360
so I had to answer 40 minutes of questions.

160
00:07:07,360 --> 00:07:08,320
At least, Mo, have you done this?

161
00:07:08,320 --> 00:07:10,280
It's stupid, right?

162
00:07:10,280 --> 00:07:12,000
We're all treated the same.

163
00:07:12,000 --> 00:07:17,200
I think 10% of people have a cytochrome P450 mutation in their liver,

164
00:07:17,200 --> 00:07:19,440
which means they metabolize drugs fast quicker.

165
00:07:19,440 --> 00:07:23,960
So if you metabolize code, it turns into morphine, or fentanyl kills you.

166
00:07:23,960 --> 00:07:25,440
But that's a very basic genetic test,

167
00:07:25,440 --> 00:07:28,200
yet we give everyone 500 milligrams of the same thing.

168
00:07:28,200 --> 00:07:31,840
With my son, a micro dose of 5 milligrams of clonazepam,

169
00:07:31,840 --> 00:07:33,600
which is used for anxiety disorder,

170
00:07:33,600 --> 00:07:35,640
worked with a neurologist, allows him to sing.

171
00:07:35,640 --> 00:07:37,160
The standard dose is a thousand milligrams,

172
00:07:37,160 --> 00:07:39,000
so they can only prescribe it at a thousand.

173
00:07:39,040 --> 00:07:43,360
But that is a $6 a year treatment that affects his GABA glutamate balance.

174
00:07:43,360 --> 00:07:45,720
But only for his specific type of ASD,

175
00:07:45,720 --> 00:07:48,640
which is only 7% of all kids with ASD.

176
00:07:48,640 --> 00:07:53,160
But why would that be in a pharmaceutical company's interest?

177
00:07:53,160 --> 00:07:57,960
You know, because how are they going to make money off a $6 a year treatment?

178
00:07:57,960 --> 00:08:00,080
Well, how many people have ASD?

179
00:08:00,080 --> 00:08:02,000
It's 1 in 60.

180
00:08:02,000 --> 00:08:04,280
Okay, it's 1 in 60, so you've got a million people in the UK?

181
00:08:04,280 --> 00:08:04,920
Yes.

182
00:08:04,920 --> 00:08:07,840
So you've got a $6 million.

183
00:08:07,880 --> 00:08:08,760
Yeah, it's not great.

184
00:08:08,760 --> 00:08:10,040
Exactly, it's not great.

185
00:08:10,040 --> 00:08:12,760
I mean, it's like we know the benefits of vitamin D, right?

186
00:08:12,760 --> 00:08:14,560
But we still don't prescribe that at scale,

187
00:08:14,560 --> 00:08:16,000
and so many people are deficient.

188
00:08:16,000 --> 00:08:17,160
I mean, all these things.

189
00:08:17,160 --> 00:08:19,440
The joys of doing what I do is going on schedule with a final one,

190
00:08:19,440 --> 00:08:22,200
and then we will kind of retain some form of normality of schedule.

191
00:08:22,200 --> 00:08:24,800
What is the future of healthcare systems that you think

192
00:08:24,800 --> 00:08:26,720
with GPT models operating in this way?

193
00:08:26,720 --> 00:08:28,960
I think that you can change the nature of a doctor,

194
00:08:28,960 --> 00:08:31,000
because a lot of the stuff is kind of very basic.

195
00:08:31,000 --> 00:08:32,720
I think, you know, you had Babylon Health and others trying,

196
00:08:32,720 --> 00:08:35,080
that chatbot, it wasn't ready, now you've got this.

197
00:08:35,080 --> 00:08:37,440
Everyone should have their own AIs looking out for their own health,

198
00:08:37,440 --> 00:08:39,080
with that objective function.

199
00:08:39,080 --> 00:08:41,000
You know, and then the nature of a doctor becomes different

200
00:08:41,000 --> 00:08:44,200
in terms of they have more rich information about an individual,

201
00:08:44,200 --> 00:08:46,920
while it being preserved in a private manner.

202
00:08:46,920 --> 00:08:48,800
I think what you have is you have things like

203
00:08:48,800 --> 00:08:50,440
processes and procedures improving,

204
00:08:50,440 --> 00:08:53,920
like wound care, for example, and then NHS.

205
00:08:53,920 --> 00:08:56,280
If you are injured as an elderly person,

206
00:08:56,280 --> 00:08:57,680
and your wounds aren't treated properly,

207
00:08:57,680 --> 00:09:00,480
and more likely to die by a factor of eight times,

208
00:09:00,480 --> 00:09:02,480
being able to monitor those types of things

209
00:09:02,480 --> 00:09:05,040
with this information set means you're eight times as likely,

210
00:09:05,040 --> 00:09:07,320
and then you have far more efficiency around that.

211
00:09:07,320 --> 00:09:09,600
So the information density around healthcare improves,

212
00:09:09,600 --> 00:09:12,760
which means that then our own healthcare improves.

213
00:09:12,760 --> 00:09:15,160
We all have access to as much knowledge as we want to,

214
00:09:15,160 --> 00:09:17,560
within our own context, and so do our providers

215
00:09:17,560 --> 00:09:18,560
and the people that help us.

216
00:09:18,560 --> 00:09:20,960
How do we think about open source versus closed source

217
00:09:20,960 --> 00:09:22,320
human healthcare data?

218
00:09:22,320 --> 00:09:24,640
Because like, obviously for us all to benefit as one,

219
00:09:24,640 --> 00:09:26,680
you know, MS sufferers around the world need to submit

220
00:09:26,680 --> 00:09:29,680
their data around responses to certain treatments.

221
00:09:29,680 --> 00:09:31,840
Yeah, so I think the wonderful thing about these models

222
00:09:31,840 --> 00:09:32,800
is they're few-shot learners,

223
00:09:32,800 --> 00:09:34,800
so they don't need to have much information.

224
00:09:34,800 --> 00:09:36,320
As I was in the classical big data problem,

225
00:09:36,320 --> 00:09:38,440
HDR UK has been one of the pioneers here

226
00:09:38,440 --> 00:09:41,160
with the UK Biobank, Federated Learning, and others.

227
00:09:41,160 --> 00:09:45,520
And there are kind of, with FL7, HLIR, and other standards

228
00:09:45,520 --> 00:09:46,720
being built around this to allow

229
00:09:46,720 --> 00:09:48,440
for full federated learning.

230
00:09:48,440 --> 00:09:51,880
If you have open source language models

231
00:09:51,880 --> 00:09:54,040
that are fully auditable, I call them organic

232
00:09:54,040 --> 00:09:55,480
free range models, the ones we're building,

233
00:09:55,480 --> 00:09:58,560
with no web script data, those can sit on device,

234
00:09:58,560 --> 00:10:01,400
like Google SDA announced POM2.

235
00:10:01,400 --> 00:10:03,720
The smallest POM2 model is 400 million parameters.

236
00:10:03,720 --> 00:10:05,520
It works on your Google Pixel phone.

237
00:10:05,520 --> 00:10:07,360
You don't need giant models anymore.

238
00:10:07,360 --> 00:10:10,120
And then that model can just share the specific information

239
00:10:10,120 --> 00:10:13,400
that preserves your privacy with the bigger thing.

240
00:10:13,400 --> 00:10:15,600
And then it can take from that global knowledge base as well.

241
00:10:15,600 --> 00:10:19,000
So you'll have big global models on device models.

242
00:10:19,000 --> 00:10:20,520
And I think open works for that

243
00:10:20,520 --> 00:10:22,960
because you don't need to have all the data open.

244
00:10:22,960 --> 00:10:24,680
You just need to know that Harry is old enough

245
00:10:24,680 --> 00:10:26,200
to have a drink, not that.

246
00:10:26,200 --> 00:10:27,720
All the details about Harry, his birthplace,

247
00:10:27,720 --> 00:10:28,640
and everything like that.

248
00:10:28,640 --> 00:10:30,520
He's old enough, he's just not allowed to.

249
00:10:30,520 --> 00:10:33,120
He gets parted all the time, yeah.

250
00:10:34,000 --> 00:10:35,760
Were you impressed by the Google event yesterday?

251
00:10:35,760 --> 00:10:36,880
No, I think it was impressive.

252
00:10:36,880 --> 00:10:39,560
I said in February, when all this thing was going on,

253
00:10:39,560 --> 00:10:42,960
like, come on, Google will be one of the main winners here.

254
00:10:42,960 --> 00:10:45,200
They have the LLMs, they have the hardware.

255
00:10:45,200 --> 00:10:47,760
You do know you're the only person who said that on the show.

256
00:10:47,760 --> 00:10:48,960
And I've asked many.

257
00:10:48,960 --> 00:10:51,640
And they've all said that Google are the laggards.

258
00:10:51,640 --> 00:10:53,480
It just takes a bit of time to move the ship, right?

259
00:10:53,480 --> 00:10:56,000
And so they've done massive organizational changes

260
00:10:56,000 --> 00:10:57,120
and other things.

261
00:10:57,120 --> 00:11:00,320
But I can tell you, TPU is on the most scalable architecture.

262
00:11:00,520 --> 00:11:04,080
We have zero failure rate with our TPU language model training.

263
00:11:04,080 --> 00:11:06,480
Whereas with GPUs, it's like there's

264
00:11:06,480 --> 00:11:09,400
an ECC error, why a solar flare?

265
00:11:09,400 --> 00:11:12,280
Run failed because the sun is angry with us and stuff like that.

266
00:11:12,280 --> 00:11:13,720
So when you've got the full stack,

267
00:11:13,720 --> 00:11:15,240
and you have all that talent in Google,

268
00:11:15,240 --> 00:11:17,720
the question is, how do you make it organized, right?

269
00:11:17,720 --> 00:11:19,000
And so they had to have a story.

270
00:11:19,000 --> 00:11:20,600
Google did something called Pro-Taristottle,

271
00:11:20,600 --> 00:11:22,160
where they analyzed what made the best teams

272
00:11:22,160 --> 00:11:23,720
versus the worst teams at Google.

273
00:11:23,720 --> 00:11:25,080
And it came down to shared narrative

274
00:11:25,080 --> 00:11:26,800
and psychological safety.

275
00:11:26,800 --> 00:11:29,480
People at Google were scared over the last few years,

276
00:11:29,520 --> 00:11:31,080
because it came this weird monoculture.

277
00:11:31,080 --> 00:11:33,040
But now everyone has a shared narrative of,

278
00:11:33,040 --> 00:11:34,920
let's build the best language models,

279
00:11:34,920 --> 00:11:36,800
and now there's an increase in amount of psychological safety

280
00:11:36,800 --> 00:11:37,960
being able to speak to things,

281
00:11:37,960 --> 00:11:40,600
the walls being brought down between deep mind and brain.

282
00:11:40,600 --> 00:11:43,360
And so I think you'll see them continuously improving.

283
00:11:43,360 --> 00:11:44,480
Well, then that does mean,

284
00:11:44,480 --> 00:11:46,080
if you're a proprietary language model company,

285
00:11:46,080 --> 00:11:49,000
how are you going to compete with that vehemence?

286
00:11:49,000 --> 00:11:53,560
The deep mind desegregation or unification

287
00:11:53,560 --> 00:11:56,160
was supposed to, of course, have a lot of friction

288
00:11:56,160 --> 00:11:58,920
and be a negative press reported.

289
00:11:58,920 --> 00:11:59,840
She disagreed with that.

290
00:11:59,840 --> 00:12:03,080
Of course, it is a lot of replicated jobs.

291
00:12:03,080 --> 00:12:05,040
There was brain and mind,

292
00:12:05,040 --> 00:12:06,920
and now they're kind of brought together.

293
00:12:06,920 --> 00:12:09,080
And it's a very different management style

294
00:12:09,080 --> 00:12:09,920
and other things.

295
00:12:09,920 --> 00:12:11,480
These things are never easy.

296
00:12:11,480 --> 00:12:14,680
But this is why you saw palm 540 billion parameters

297
00:12:14,680 --> 00:12:16,400
and that you had deep mind

298
00:12:16,400 --> 00:12:18,880
with 67 billion parameter and chiller,

299
00:12:18,880 --> 00:12:21,520
which is just train more as opposed to more parameters.

300
00:12:21,520 --> 00:12:24,280
You look at palm two as a combination of both.

301
00:12:24,280 --> 00:12:27,720
And so it's trained for far more on far better data.

302
00:12:27,720 --> 00:12:30,560
And then that means it's only a fraction of the size,

303
00:12:30,560 --> 00:12:31,840
like 14 billion parameters

304
00:12:31,840 --> 00:12:35,760
is one of the test comparator models versus the 540 and 67.

305
00:12:35,760 --> 00:12:37,920
So you can start to see this fusion of ideas,

306
00:12:37,920 --> 00:12:38,840
even if the teams,

307
00:12:38,840 --> 00:12:41,960
you cannot integrate two big teams like that instantly.

308
00:12:41,960 --> 00:12:44,360
Shared narrative, psychological safety,

309
00:12:44,360 --> 00:12:46,160
two of the biggest contributors.

310
00:12:46,160 --> 00:12:48,400
To now running stability,

311
00:12:48,400 --> 00:12:50,440
how do you think about integrating those two?

312
00:12:50,440 --> 00:12:51,680
So we've got the shared narrative.

313
00:12:51,680 --> 00:12:52,680
We're going to build the foundation

314
00:12:52,680 --> 00:12:54,120
to activate humanity's potential

315
00:12:54,120 --> 00:12:56,200
and then the motors make people happier.

316
00:12:56,200 --> 00:12:57,480
But it's been a learning process,

317
00:12:57,480 --> 00:13:00,480
the year ago, we're basically a mom and pop shop in some ways.

318
00:13:00,480 --> 00:13:02,000
My wife and I were working at it,

319
00:13:02,000 --> 00:13:04,400
like had lots of meetings out of our like sitting room

320
00:13:04,400 --> 00:13:06,160
and things because the office didn't have wifi

321
00:13:06,160 --> 00:13:07,240
and all sorts.

322
00:13:07,240 --> 00:13:10,040
Now it's like growing up, we're 170 people,

323
00:13:10,040 --> 00:13:11,320
we're going global,

324
00:13:11,320 --> 00:13:13,320
we'll have stabilities in every country.

325
00:13:13,320 --> 00:13:15,360
And then actually we're going multinational.

326
00:13:15,360 --> 00:13:16,280
And that's difficult.

327
00:13:16,280 --> 00:13:18,520
So we really try to put in processes in place,

328
00:13:18,520 --> 00:13:20,240
but it's not easy.

329
00:13:20,240 --> 00:13:21,320
Part of this is like,

330
00:13:21,320 --> 00:13:24,560
we went close source on a bunch of stuff like dream studio.

331
00:13:24,560 --> 00:13:26,360
I'm open sourcing everything now.

332
00:13:26,360 --> 00:13:27,200
From next week,

333
00:13:27,200 --> 00:13:29,240
we're going to build our language models in the open

334
00:13:29,240 --> 00:13:30,840
and share what works and what doesn't work.

335
00:13:30,840 --> 00:13:31,880
Why?

336
00:13:31,880 --> 00:13:34,200
Because I think this is part of the shared narrative.

337
00:13:34,200 --> 00:13:35,560
Someone needs to be open

338
00:13:35,560 --> 00:13:37,320
and share what's going on under the hood.

339
00:13:37,320 --> 00:13:40,760
And again, it's like, it should be opened by default

340
00:13:40,760 --> 00:13:44,880
because the value is not in any proprietary models or data.

341
00:13:44,880 --> 00:13:47,520
We're going to build open models that are auditable.

342
00:13:47,520 --> 00:13:49,000
Even if it has licensed data in it,

343
00:13:49,000 --> 00:13:51,680
you can see every single piece, free range organic models.

344
00:13:51,680 --> 00:13:52,720
Because that's what the world needs

345
00:13:52,720 --> 00:13:55,200
for all the private regulated and other data in the world.

346
00:13:55,200 --> 00:13:58,360
This is a completely different time to proprietary models.

347
00:13:58,360 --> 00:14:01,400
Because you can only send so much of your 20 VC data

348
00:14:01,400 --> 00:14:02,760
to open AI.

349
00:14:02,760 --> 00:14:04,600
And I think you need both of those.

350
00:14:04,600 --> 00:14:06,680
So why can I only send so much?

351
00:14:06,680 --> 00:14:08,760
Because you are a regulated company.

352
00:14:08,760 --> 00:14:11,240
And so you need to make sure they're completely compliant.

353
00:14:11,240 --> 00:14:14,400
If you have an option of having a stable chat model,

354
00:14:14,400 --> 00:14:16,360
which will be announced in the future,

355
00:14:16,360 --> 00:14:18,520
that you own completely trained in your own cloud

356
00:14:18,520 --> 00:14:20,080
or on-prem or on-device,

357
00:14:20,080 --> 00:14:21,360
and then also using GPT-4,

358
00:14:21,360 --> 00:14:23,200
that's the best of both worlds.

359
00:14:23,200 --> 00:14:24,480
Because then you don't have to deal with that.

360
00:14:24,520 --> 00:14:27,400
Healthcare data needs to, again, be owned by the individual.

361
00:14:27,400 --> 00:14:29,160
And so those models need to be owned.

362
00:14:29,160 --> 00:14:30,160
And they need to be transparent.

363
00:14:30,160 --> 00:14:31,920
They can't be black boxes.

364
00:14:31,920 --> 00:14:33,920
Governments will not run on black boxes.

365
00:14:35,360 --> 00:14:36,440
We're going to get to this later.

366
00:14:36,440 --> 00:14:37,840
I do want to touch on something.

367
00:14:37,840 --> 00:14:39,520
We had a great chat before this.

368
00:14:39,520 --> 00:14:41,400
And you said a brilliant quote, and I want to get it right,

369
00:14:41,400 --> 00:14:44,200
but you said the .ai bubble is bigger than ever,

370
00:14:44,200 --> 00:14:46,200
and it will be the biggest shit show.

371
00:14:46,200 --> 00:14:47,200
Yeah.

372
00:14:47,200 --> 00:14:48,200
End quote.

373
00:14:49,200 --> 00:14:50,760
Which I actually took and tweeted, by the way.

374
00:14:50,760 --> 00:14:51,600
Thank you.

375
00:14:51,600 --> 00:14:52,920
It could be some gratitude.

376
00:14:52,920 --> 00:14:54,160
I thought if you saw it, I would have been like,

377
00:14:54,160 --> 00:14:55,360
ah, this guy took my tweet.

378
00:14:55,360 --> 00:14:56,360
Thank you.

379
00:14:56,360 --> 00:15:01,080
And what did you mean by the biggest bubble ever

380
00:15:01,080 --> 00:15:02,360
and the biggest shit show?

381
00:15:02,360 --> 00:15:04,560
Oh, I mean, like the .com bubble,

382
00:15:04,560 --> 00:15:05,920
we've seen all these bubbles happen.

383
00:15:05,920 --> 00:15:08,240
You know, you had hundreds of billions into Web 3,

384
00:15:08,240 --> 00:15:10,040
and then developers got paid millions.

385
00:15:10,040 --> 00:15:13,040
Already, there are certain Chinese companies

386
00:15:13,040 --> 00:15:15,600
paying $1.2 million salaries for PhDs.

387
00:15:16,760 --> 00:15:18,160
It's already getting a bit insane.

388
00:15:18,160 --> 00:15:19,760
There are remnants of that.

389
00:15:19,800 --> 00:15:23,280
The amount of money relative to the amount of opportunity

390
00:15:23,280 --> 00:15:25,240
within the sector is just completely misaligned.

391
00:15:25,240 --> 00:15:27,400
Like my time analysis is that

392
00:15:27,400 --> 00:15:29,720
1,000 companies spend $10 million in the next year,

393
00:15:29,720 --> 00:15:32,600
100 companies spend $100, and 10 companies spend $1 billion.

394
00:15:32,600 --> 00:15:34,360
Like PWC just announcing they'll spend $1 billion

395
00:15:34,360 --> 00:15:35,680
over the next three years.

396
00:15:35,680 --> 00:15:38,440
And that's a currency firm, you know?

397
00:15:38,440 --> 00:15:39,280
Where's that going to go?

398
00:15:39,280 --> 00:15:40,120
They don't know.

399
00:15:40,120 --> 00:15:41,080
Nobody knows.

400
00:15:41,080 --> 00:15:43,280
And so multiple of that will be allocated to this

401
00:15:43,280 --> 00:15:47,160
as the only growth theme in the entire market

402
00:15:47,160 --> 00:15:48,880
against a backdrop of rising rates,

403
00:15:48,920 --> 00:15:50,920
real estate crashing, et cetera.

404
00:15:50,920 --> 00:15:53,720
So the amount of capacity versus the amount

405
00:15:53,720 --> 00:15:56,120
and whale and wall of money

406
00:15:56,120 --> 00:15:57,320
into something that's growing faster

407
00:15:57,320 --> 00:16:00,800
than anything we've ever seen is completely mismatched.

408
00:16:00,800 --> 00:16:02,360
And what will that cause?

409
00:16:02,360 --> 00:16:04,280
Like already you're seeing GitHub stars leading

410
00:16:04,280 --> 00:16:06,120
to $100 million funding rounds

411
00:16:06,120 --> 00:16:08,400
with zero attraction and zero business model.

412
00:16:08,400 --> 00:16:10,280
Like stability, we actually have a business model.

413
00:16:10,280 --> 00:16:12,920
And it's a good business model because I designed it.

414
00:16:12,920 --> 00:16:15,200
But other things like money will go everywhere

415
00:16:15,200 --> 00:16:17,800
and any expertise will get bit up for this space

416
00:16:17,840 --> 00:16:19,600
because it means that projects will get funded

417
00:16:19,600 --> 00:16:21,160
that maybe wouldn't have done,

418
00:16:21,160 --> 00:16:23,520
but are exploratory generally and over funding.

419
00:16:23,520 --> 00:16:26,240
I think it starts good for the space,

420
00:16:26,240 --> 00:16:28,640
but then it gets bad for the space

421
00:16:28,640 --> 00:16:30,640
because you see the raccoons and Scheister

422
00:16:30,640 --> 00:16:32,000
start to come in here.

423
00:16:32,000 --> 00:16:34,160
You start to see like malformed things

424
00:16:34,160 --> 00:16:35,240
where there's a race dynamic,

425
00:16:35,240 --> 00:16:37,160
where everyone's trying to build their own models

426
00:16:37,160 --> 00:16:40,400
and doing all sorts and massive economic waste.

427
00:16:40,400 --> 00:16:43,000
And you see a distraction from what we need to do now,

428
00:16:43,000 --> 00:16:43,840
which is this chaos.

429
00:16:43,840 --> 00:16:45,720
So we need to standardize some things.

430
00:16:45,720 --> 00:16:47,880
We need to feed these models better data and other stuff.

431
00:16:47,880 --> 00:16:50,040
And that's why we're moving so hard at stability.

432
00:16:50,040 --> 00:16:51,880
There should be no more web script data in here.

433
00:16:51,880 --> 00:16:54,480
There should be national data sets that are good quality

434
00:16:54,480 --> 00:16:56,040
to feed these free range organic models

435
00:16:56,040 --> 00:16:58,720
and national and proprietary models and others.

436
00:16:58,720 --> 00:17:00,480
And so that's why, and the reasons I signed that letter

437
00:17:00,480 --> 00:17:02,200
because I think there's a six month pause

438
00:17:02,200 --> 00:17:03,640
to get all of our shit together

439
00:17:04,560 --> 00:17:06,920
before things go completely insane.

440
00:17:06,920 --> 00:17:08,240
And next year, this is everywhere

441
00:17:08,240 --> 00:17:10,160
and everyone's investing in everything.

442
00:17:10,160 --> 00:17:13,240
And it's just absolute chaos.

443
00:17:13,240 --> 00:17:15,280
You kind of unpack so much to me

444
00:17:15,280 --> 00:17:16,920
that I want to kind of go one by one.

445
00:17:16,920 --> 00:17:18,800
You said about kind of national data sets.

446
00:17:18,800 --> 00:17:22,360
Why national data sets versus super national data sets?

447
00:17:22,360 --> 00:17:23,440
Because like, I'll give you an example.

448
00:17:23,440 --> 00:17:24,840
There was a team that did Japan diffusion,

449
00:17:24,840 --> 00:17:26,200
including some of our staff.

450
00:17:26,200 --> 00:17:27,480
So we took stable diffusion

451
00:17:27,480 --> 00:17:28,840
and then changed the language model.

452
00:17:28,840 --> 00:17:31,120
Because when you typed in salary man in stable diffusion,

453
00:17:31,120 --> 00:17:32,360
it was a very happy man.

454
00:17:32,360 --> 00:17:34,720
Whereas in Japan, the salary man's a very sad man.

455
00:17:34,720 --> 00:17:38,000
You know, local context is important in these models

456
00:17:38,000 --> 00:17:39,440
because we're going to outsource more and more

457
00:17:39,440 --> 00:17:41,600
of our thinking and minds to it.

458
00:17:41,600 --> 00:17:43,200
And so do you want to have a British model

459
00:17:43,200 --> 00:17:45,400
doing all the models to be Palo Alto?

460
00:17:45,400 --> 00:17:47,280
You know, like it's a sparkling wine

461
00:17:47,280 --> 00:17:48,920
has to be from the Champaign region.

462
00:17:48,920 --> 00:17:52,080
Like is the only real foundation model AI from Palo Alto?

463
00:17:52,080 --> 00:17:53,240
Like it's not a good thing.

464
00:17:53,240 --> 00:17:54,680
We need national models.

465
00:17:54,680 --> 00:17:55,760
It's a national infrastructure

466
00:17:55,760 --> 00:18:00,440
because there is no doubt this is more important than 5G.

467
00:18:00,440 --> 00:18:02,200
These models are like really talented grads

468
00:18:02,200 --> 00:18:03,960
that occasionally go off their meds, you know?

469
00:18:03,960 --> 00:18:06,360
And you want to have the ones from Oxford Imperial

470
00:18:06,360 --> 00:18:09,040
and Edinburgh as well as the ones from Stanford

471
00:18:09,040 --> 00:18:10,800
because they understand the local context.

472
00:18:10,800 --> 00:18:11,800
And so they understand you better

473
00:18:11,800 --> 00:18:13,520
and they'll be better for that.

474
00:18:13,520 --> 00:18:15,800
As part of that, every nation will need their own data sets

475
00:18:15,800 --> 00:18:17,720
which again have from broadcaster data.

476
00:18:17,720 --> 00:18:19,000
They will need their own open models

477
00:18:19,000 --> 00:18:21,240
that can stimulate innovation internally as well.

478
00:18:21,240 --> 00:18:22,680
Who owns national data sets?

479
00:18:22,680 --> 00:18:23,520
Is that governments?

480
00:18:23,520 --> 00:18:24,480
I think it should be the people.

481
00:18:24,480 --> 00:18:26,680
I think it should be open and public domain.

482
00:18:28,000 --> 00:18:29,760
How does that come into fruition?

483
00:18:29,760 --> 00:18:33,240
Well, we have a world where we have national verified data sets

484
00:18:33,240 --> 00:18:36,240
which can be leveraged by independent private companies.

485
00:18:36,240 --> 00:18:38,360
And others and universities and others.

486
00:18:38,360 --> 00:18:39,400
Well, this is what we're doing right now.

487
00:18:39,400 --> 00:18:41,120
We're working with our multinational partners,

488
00:18:41,120 --> 00:18:42,360
lots more to be announced soon,

489
00:18:42,360 --> 00:18:44,200
and multiple governments for a framework

490
00:18:44,200 --> 00:18:47,080
for what good data looks like to feed these models

491
00:18:47,080 --> 00:18:49,840
to stimulate innovation and localization.

492
00:18:49,840 --> 00:18:51,120
And that is a public good

493
00:18:51,120 --> 00:18:55,200
because national broadcasters have all of this data.

494
00:18:55,200 --> 00:18:57,640
You just tokenize all their kind of things.

495
00:18:57,640 --> 00:18:59,400
And then you have things like the implementation

496
00:18:59,400 --> 00:19:01,240
of these for education and healthcare.

497
00:19:01,240 --> 00:19:02,560
You can take generalized learnings

498
00:19:02,560 --> 00:19:04,680
and then again feed the models that thing.

499
00:19:04,680 --> 00:19:08,000
What is a great data set for a great British E.P.T. look like?

500
00:19:08,000 --> 00:19:10,840
I think it's open, it's interrogated and it's optimized.

501
00:19:10,880 --> 00:19:12,120
When you look at all the different things

502
00:19:12,120 --> 00:19:13,480
that we've talked about from you,

503
00:19:13,480 --> 00:19:16,680
relative treatment of MS to ASD,

504
00:19:17,640 --> 00:19:20,200
and then it's impact on education.

505
00:19:20,200 --> 00:19:23,160
And we just to PWC spending money on it,

506
00:19:23,160 --> 00:19:25,560
there are so many problems that can be solved.

507
00:19:25,560 --> 00:19:29,240
Surely we can find a home for the cash.

508
00:19:29,240 --> 00:19:31,400
Yeah, and so I'm not sure where.

509
00:19:31,400 --> 00:19:32,680
And so this is the thing,

510
00:19:32,680 --> 00:19:34,760
like there's gonna be this mismatch.

511
00:19:34,760 --> 00:19:37,080
If you were an invested state,

512
00:19:37,080 --> 00:19:38,840
if you were me, what would you do?

513
00:19:38,840 --> 00:19:41,320
I'm an early stage investor, I'm less globally.

514
00:19:41,320 --> 00:19:43,080
What would you do?

515
00:19:43,080 --> 00:19:46,200
I would, again, I think it comes down to,

516
00:19:46,200 --> 00:19:48,640
there's gonna be this tailwind of beta.

517
00:19:48,640 --> 00:19:51,160
And then you have an alpha play on top of that, right?

518
00:19:51,160 --> 00:19:53,080
So the beta play is that you just invest

519
00:19:53,080 --> 00:19:54,360
in any good founder.

520
00:19:54,360 --> 00:19:55,640
And if you get in, you figure out,

521
00:19:55,640 --> 00:19:57,600
what can I offer as kind of a value out there?

522
00:19:57,600 --> 00:19:58,640
Am I offering distribution?

523
00:19:58,640 --> 00:19:59,480
Am I offering people?

524
00:19:59,480 --> 00:20:00,560
Am I offering this?

525
00:20:00,560 --> 00:20:02,480
And you emphasize kind of your value set.

526
00:20:02,480 --> 00:20:05,520
I think right now what people need is people.

527
00:20:05,520 --> 00:20:07,400
There are a few people that are like coming out here,

528
00:20:07,440 --> 00:20:12,080
but then what you see is you see good companies,

529
00:20:12,080 --> 00:20:14,560
with good ideas, but not businesses.

530
00:20:14,560 --> 00:20:15,960
They're building surface level things,

531
00:20:15,960 --> 00:20:17,320
these wrapper layers and others,

532
00:20:17,320 --> 00:20:19,600
and they're not thinking about distribution and data.

533
00:20:19,600 --> 00:20:20,920
It's like, if you want to have distribution,

534
00:20:20,920 --> 00:20:21,760
what do we do?

535
00:20:21,760 --> 00:20:23,280
We went to Amazon and said bedrock,

536
00:20:23,280 --> 00:20:26,920
because then it gives us 100,000 SageMaker SMEs.

537
00:20:26,920 --> 00:20:28,240
And we'd have to give them the models

538
00:20:28,240 --> 00:20:29,760
that they can then take to the private data.

539
00:20:29,760 --> 00:20:31,720
And we get a share of all of that.

540
00:20:31,720 --> 00:20:32,600
This is how we saw it,

541
00:20:32,600 --> 00:20:34,920
like rather than being responsible that.

542
00:20:34,920 --> 00:20:36,720
So if you can bring that distribution to that,

543
00:20:37,040 --> 00:20:39,440
this is part of that Google memo that went out.

544
00:20:39,440 --> 00:20:40,760
We don't have an edge and others open AI.

545
00:20:40,760 --> 00:20:44,120
Open AI used Microsoft for distribution and that flywheel.

546
00:20:45,480 --> 00:20:46,960
If you have a business that's focused

547
00:20:46,960 --> 00:20:49,520
on innovation at the core, that's not actually a business.

548
00:20:49,520 --> 00:20:51,240
It becomes a business when that innovation

549
00:20:51,240 --> 00:20:53,480
becomes product, becomes distribution,

550
00:20:53,480 --> 00:20:55,400
when it has an advantage on data and other things.

551
00:20:55,400 --> 00:20:56,920
Those are real modes.

552
00:20:56,920 --> 00:20:58,440
How did you arise that partnership

553
00:20:58,440 --> 00:21:00,520
between open AI and Microsoft?

554
00:21:00,520 --> 00:21:03,160
I saw it as the objective function of open AI

555
00:21:03,160 --> 00:21:05,680
is to build AGI and they reckon they need $10 billion

556
00:21:05,720 --> 00:21:07,360
to do it and they did that.

557
00:21:07,360 --> 00:21:09,360
Like they're building a business on products and things,

558
00:21:09,360 --> 00:21:10,920
but they don't care.

559
00:21:10,920 --> 00:21:12,640
You know, they're not trying to build a sustainable business.

560
00:21:12,640 --> 00:21:14,040
They're trying to build an AGI.

561
00:21:15,120 --> 00:21:15,960
Why?

562
00:21:15,960 --> 00:21:17,400
Like what would I, just help me understand

563
00:21:17,400 --> 00:21:19,360
AGI to build a sustainable business?

564
00:21:19,360 --> 00:21:20,400
Cause at the end of the day.

565
00:21:20,400 --> 00:21:21,600
No, they're building an AGI

566
00:21:21,600 --> 00:21:23,160
to turn the world into utopia.

567
00:21:23,160 --> 00:21:25,320
It's written in their path to AGI thing

568
00:21:25,320 --> 00:21:28,480
that they think this can basically bring about utopia.

569
00:21:28,480 --> 00:21:30,560
So a lot of people in these labs,

570
00:21:30,560 --> 00:21:32,200
we only have people joining from all of these labs,

571
00:21:32,200 --> 00:21:35,360
like they almost zealous in there.

572
00:21:35,360 --> 00:21:37,080
But there is a misalignment there between them

573
00:21:37,080 --> 00:21:39,720
and Microsoft in their desire to create that utopian AI.

574
00:21:39,720 --> 00:21:41,840
Yes, cause Microsoft is a business, you know?

575
00:21:41,840 --> 00:21:44,080
And so this is why you've seen like articles

576
00:21:44,080 --> 00:21:46,280
in the information, like Microsoft say open AI

577
00:21:46,280 --> 00:21:48,960
aren't compliant and open AI say Microsoft on this.

578
00:21:48,960 --> 00:21:50,920
These things happen when there is a misalignment

579
00:21:50,920 --> 00:21:52,760
of objective functions.

580
00:21:52,760 --> 00:21:55,320
But again, you should view open AI as what they want to do

581
00:21:55,320 --> 00:21:58,680
is build an AI that can basically make the world better

582
00:21:58,680 --> 00:22:01,400
and hopefully not kill us all, which they say it might.

583
00:22:01,400 --> 00:22:02,400
Which is a bit concerning,

584
00:22:02,400 --> 00:22:04,600
which is why I hope they have better open governance.

585
00:22:04,640 --> 00:22:06,200
How did you think about distribution?

586
00:22:06,200 --> 00:22:08,560
You know, you've seen the hugging face part of it.

587
00:22:08,560 --> 00:22:11,560
Amazon, you've seen obviously open AI with Microsoft.

588
00:22:11,560 --> 00:22:12,760
When you think about distribution

589
00:22:12,760 --> 00:22:15,600
and your competitive edge there, where did you land?

590
00:22:15,600 --> 00:22:17,800
So my business model is actually very simple.

591
00:22:17,800 --> 00:22:19,520
I haven't really talked about it much.

592
00:22:19,520 --> 00:22:22,280
Stimulate open one of the biggest providers of grants

593
00:22:22,280 --> 00:22:24,840
to open source software, tens of millions already.

594
00:22:24,840 --> 00:22:26,520
And then take the best of open,

595
00:22:26,520 --> 00:22:28,120
which hopefully we build ourselves.

596
00:22:28,120 --> 00:22:31,520
And then an open base with an open data.

597
00:22:31,520 --> 00:22:33,920
And then commercial variants with license data

598
00:22:33,920 --> 00:22:35,280
and then national variants.

599
00:22:35,280 --> 00:22:38,960
So you have Hindi insurance adjusted stable chat

600
00:22:38,960 --> 00:22:42,360
or Indonesian pharmaceutical worker stable chat

601
00:22:42,360 --> 00:22:45,040
that's available in every cloud, on-prem, on-device

602
00:22:45,040 --> 00:22:49,760
with licensing fees, royalties and revenue share.

603
00:22:49,760 --> 00:22:51,520
And the system integrators work with us as well.

604
00:22:51,520 --> 00:22:53,160
Lots of announcement to come.

605
00:22:53,160 --> 00:22:56,560
And so by standardizing and stabilizing all the complexity

606
00:22:56,560 --> 00:23:00,200
to these very sophisticated building blocks,

607
00:23:00,200 --> 00:23:02,800
these very intentionally built models,

608
00:23:02,800 --> 00:23:04,480
that really helps the world integrate this stuff

609
00:23:04,480 --> 00:23:06,400
by building playbooks and other things.

610
00:23:06,400 --> 00:23:07,320
And that's the core business

611
00:23:07,320 --> 00:23:09,160
because it doesn't require actual innovation.

612
00:23:09,160 --> 00:23:10,000
We are still innovative

613
00:23:10,000 --> 00:23:11,800
and the leaders in media in particular.

614
00:23:11,800 --> 00:23:13,800
Instead it requires data and distribution,

615
00:23:13,800 --> 00:23:15,160
data to the models.

616
00:23:15,160 --> 00:23:16,520
The models are open and interpretable

617
00:23:16,520 --> 00:23:18,760
and models to the data via our partners.

618
00:23:18,760 --> 00:23:20,760
And that's valuable because the private data in the world

619
00:23:20,760 --> 00:23:22,720
is far more valuable than the data

620
00:23:22,720 --> 00:23:25,200
that you will send to proprietary models.

621
00:23:25,200 --> 00:23:27,360
And it's not a race to the bottom either.

622
00:23:27,360 --> 00:23:28,480
So that's what we are.

623
00:23:28,480 --> 00:23:30,440
We're a modeling agency with hot GPUs.

624
00:23:31,440 --> 00:23:33,600
Building a distribution around the world,

625
00:23:33,600 --> 00:23:35,960
realizing that India and other nations

626
00:23:35,960 --> 00:23:38,280
will leapfrog to intelligence augmentation,

627
00:23:38,280 --> 00:23:39,560
just so they leapfrog to mobile.

628
00:23:39,560 --> 00:23:41,160
They will embrace this technology far quicker

629
00:23:41,160 --> 00:23:42,280
than we will in the UK even.

630
00:23:42,280 --> 00:23:43,240
Why?

631
00:23:43,240 --> 00:23:45,080
Because they have to.

632
00:23:45,080 --> 00:23:49,640
India, all of the outsourcing jobs in programming will go

633
00:23:49,640 --> 00:23:52,680
because GPT-4 can go level three

634
00:23:52,680 --> 00:23:54,920
Google programmer exam and pass it.

635
00:23:55,920 --> 00:23:58,920
Outsource jobs will go the first, whereas in France,

636
00:23:58,920 --> 00:24:00,320
you're never gonna fire a French person.

637
00:24:00,360 --> 00:24:02,200
So those jobs are fake, you know?

638
00:24:02,200 --> 00:24:04,120
And so they have an objective function

639
00:24:04,120 --> 00:24:05,560
when they need to embrace this technology.

640
00:24:05,560 --> 00:24:07,920
In Africa, one to one tuition,

641
00:24:07,920 --> 00:24:10,040
every kid in Malawi is on things you lined up.

642
00:24:10,040 --> 00:24:11,320
We've got other nations.

643
00:24:11,320 --> 00:24:14,160
We're gonna bring them all this technology and tablets.

644
00:24:14,160 --> 00:24:15,440
And guess what?

645
00:24:15,440 --> 00:24:17,320
Their lives will transform.

646
00:24:17,320 --> 00:24:18,920
One AI per child is one I wanna call it.

647
00:24:18,920 --> 00:24:20,400
We'll call it something else.

648
00:24:20,400 --> 00:24:22,000
But think about the potential of that

649
00:24:22,000 --> 00:24:23,880
because you have one's teacher per 300 kids.

650
00:24:23,880 --> 00:24:25,840
What if they had a chat GPT-level AI?

651
00:24:27,160 --> 00:24:28,960
The ROI is high and the need is high.

652
00:24:28,960 --> 00:24:31,320
And so they will embrace it far quicker than we will.

653
00:24:31,320 --> 00:24:34,000
What happens to countries that rely on outsourced work

654
00:24:34,000 --> 00:24:36,360
in those kind of freelancer economy jobs?

655
00:24:36,360 --> 00:24:38,480
In general, one of the things, the questions is,

656
00:24:38,480 --> 00:24:40,160
you've seen OpenAI study, you've seen the,

657
00:24:40,160 --> 00:24:43,040
which said task will be replaced up to 44%.

658
00:24:43,040 --> 00:24:46,360
You've seen Goldman Sachs say adds percentage points to GDP.

659
00:24:46,360 --> 00:24:49,320
I think the only solution to this is entrepreneurship.

660
00:24:49,320 --> 00:24:51,560
And so we need to give the tools to create new jobs

661
00:24:51,560 --> 00:24:54,760
that can replace some of these old jobs being done.

662
00:24:54,760 --> 00:24:57,040
So like to the various Asian governments,

663
00:24:57,040 --> 00:25:01,480
I'm saying adopt the UK policy of these sandboxes,

664
00:25:01,480 --> 00:25:04,480
financial, AI and other regulatory sandboxes.

665
00:25:04,480 --> 00:25:05,880
So you can take these technologies,

666
00:25:05,880 --> 00:25:08,360
these national models that we will help you build

667
00:25:08,360 --> 00:25:10,280
with our consortium partners

668
00:25:10,280 --> 00:25:12,560
and then spur innovation to create the jobs

669
00:25:12,560 --> 00:25:13,840
to replace the existing jobs

670
00:25:13,840 --> 00:25:15,960
because you'll upgrade your entire society.

671
00:25:15,960 --> 00:25:17,520
Bring these models into your governments

672
00:25:17,520 --> 00:25:19,480
and other things to go from slow dumb AI's,

673
00:25:19,480 --> 00:25:22,040
which is the national organization's healthcare,

674
00:25:22,040 --> 00:25:24,240
to intelligent dynamic ones.

675
00:25:24,240 --> 00:25:25,920
Can I ask on implementation,

676
00:25:25,960 --> 00:25:27,720
when we think about kind of bluntly

677
00:25:27,720 --> 00:25:29,800
seeing this in action in society,

678
00:25:29,800 --> 00:25:32,080
I'm sure it's very aware of technology cycles

679
00:25:32,080 --> 00:25:34,880
taking so much longer than one anticipates.

680
00:25:34,880 --> 00:25:37,440
How do you think about that in actual,

681
00:25:37,440 --> 00:25:38,400
there's kind of two-fold.

682
00:25:38,400 --> 00:25:39,640
One is adoption on enterprise

683
00:25:39,640 --> 00:25:41,360
and another is adoption on consumer.

684
00:25:41,360 --> 00:25:43,240
Say if we do the adoption on the consumer side,

685
00:25:43,240 --> 00:25:46,880
which is impacting freelancer jobs and impacting education.

686
00:25:46,880 --> 00:25:48,480
What do you think that looks like?

687
00:25:48,480 --> 00:25:50,800
So I think on the consumer side,

688
00:25:50,800 --> 00:25:51,920
you're free with your information.

689
00:25:51,920 --> 00:25:53,240
So you can use a lot of these things,

690
00:25:53,240 --> 00:25:55,240
the APIs of OpenAI and Cohere

691
00:25:55,240 --> 00:25:56,280
and others are fantastic, right?

692
00:25:56,280 --> 00:25:58,800
And Google Palm now kind of being out there.

693
00:25:58,800 --> 00:26:00,080
So it will be integrated

694
00:26:00,080 --> 00:26:02,000
to deliver better consumer experiences

695
00:26:02,000 --> 00:26:03,000
without it being creepy,

696
00:26:03,000 --> 00:26:05,280
like you've seen with some of the chat bots, et cetera.

697
00:26:05,280 --> 00:26:06,400
Cause it's going into word,

698
00:26:06,400 --> 00:26:07,880
it's going into workspace, you know,

699
00:26:07,880 --> 00:26:09,920
like it helps already.

700
00:26:09,920 --> 00:26:11,120
Like we will have a conversation

701
00:26:11,120 --> 00:26:12,880
will be automatically logged by our pixel phone

702
00:26:12,880 --> 00:26:13,880
and then we'll get a transcript

703
00:26:13,880 --> 00:26:15,920
and remove bits that we don't want to share

704
00:26:15,920 --> 00:26:17,240
that it goes into a global knowledge base

705
00:26:17,240 --> 00:26:18,840
that reminds us of things.

706
00:26:18,840 --> 00:26:19,760
That's inevitable.

707
00:26:19,760 --> 00:26:20,920
On enterprise, it takes longer

708
00:26:20,920 --> 00:26:24,440
because you need to have auditable standardized models.

709
00:26:24,480 --> 00:26:25,800
If you're a financial services institute,

710
00:26:25,800 --> 00:26:28,000
you can't have a single piece of crawl data in there.

711
00:26:28,000 --> 00:26:29,400
And so that's what we're deliberately building

712
00:26:29,400 --> 00:26:30,880
with the largest companies in the world

713
00:26:30,880 --> 00:26:32,240
because we're building dedicated teams.

714
00:26:32,240 --> 00:26:34,000
So you can't have a single piece of crawl data

715
00:26:34,000 --> 00:26:35,360
if you're a financial services.

716
00:26:35,360 --> 00:26:36,880
Yes, because the danger is

717
00:26:36,880 --> 00:26:38,080
if it has some Reddit in there.

718
00:26:38,080 --> 00:26:38,840
So stable.

719
00:26:38,840 --> 00:26:40,000
And you know, we'll put out next week,

720
00:26:40,000 --> 00:26:41,880
it wasn't as good as the other models

721
00:26:41,880 --> 00:26:42,800
because we're going to make a point

722
00:26:42,800 --> 00:26:44,280
about Reddit data being bad.

723
00:26:44,280 --> 00:26:46,440
It's not about more data, it's about better data.

724
00:26:46,440 --> 00:26:47,280
We had data comp,

725
00:26:47,280 --> 00:26:48,680
which is the next generation lion

726
00:26:48,680 --> 00:26:50,400
that we funded the compute for,

727
00:26:50,400 --> 00:26:53,320
whereby at a quarter of the parameters of OpenAI's clip,

728
00:26:53,360 --> 00:26:55,600
that outperforms with the beta data quality.

729
00:26:55,600 --> 00:26:58,280
What makes good data quality, sorry?

730
00:26:58,280 --> 00:26:59,800
That's something we're exploring right now.

731
00:26:59,800 --> 00:27:01,480
But from the investment banks,

732
00:27:01,480 --> 00:27:02,600
we've talked to asset managers

733
00:27:02,600 --> 00:27:03,880
and we're building dedicated teams

734
00:27:03,880 --> 00:27:05,000
for the largest ones in the world

735
00:27:05,000 --> 00:27:06,560
to build them a prior to models.

736
00:27:06,560 --> 00:27:09,040
The feedback we've got is we cannot use a black box.

737
00:27:09,040 --> 00:27:11,000
We need to know what data is in there.

738
00:27:11,000 --> 00:27:13,040
Just the regulators are asking us.

739
00:27:13,040 --> 00:27:14,960
We don't want to have this out of sample thing

740
00:27:14,960 --> 00:27:16,640
where it's seen something on Reddit

741
00:27:16,640 --> 00:27:20,280
and then it says something rude to our end users, you know?

742
00:27:20,280 --> 00:27:22,040
Why is Reddit data bad?

743
00:27:22,040 --> 00:27:23,960
Reddit data isn't bad in itself.

744
00:27:23,960 --> 00:27:26,760
It was just a case of more data is not always good.

745
00:27:26,760 --> 00:27:29,440
So right now we are using all these web scrapes

746
00:27:29,440 --> 00:27:31,880
and we're training our models by taping their eyes open

747
00:27:31,880 --> 00:27:36,840
and then it took six months to turn GPT-4 into chat GPT-4

748
00:27:36,840 --> 00:27:39,480
because we had to tune it and like give it a haircut and stuff

749
00:27:39,480 --> 00:27:41,720
and bring it back to society.

750
00:27:41,720 --> 00:27:44,360
The point is that we need to find the right type of data

751
00:27:44,360 --> 00:27:45,560
because rubbish in, rubbish out

752
00:27:45,560 --> 00:27:48,360
is something that we've heard a lot, you know?

753
00:27:48,360 --> 00:27:50,160
And so it's not bad in itself,

754
00:27:50,160 --> 00:27:51,160
but if you just scrape it

755
00:27:51,160 --> 00:27:53,200
without the proper cleaning, it is bad.

756
00:27:53,200 --> 00:27:54,200
Because what is it?

757
00:27:54,200 --> 00:27:56,680
It's like, you know, people just covetching a lot.

758
00:27:56,680 --> 00:27:58,840
You know, people being biased.

759
00:27:58,840 --> 00:28:02,000
Do you really want to feed your kid the whole of Reddit?

760
00:28:02,000 --> 00:28:02,960
You know?

761
00:28:02,960 --> 00:28:05,960
Do you want to have the best curriculum possible?

762
00:28:05,960 --> 00:28:07,200
And this is actually one of the ways the models learn.

763
00:28:07,200 --> 00:28:09,360
Like stable diffusion, we train it on the whole internet

764
00:28:09,360 --> 00:28:12,280
and then better and better image subsets of it.

765
00:28:12,280 --> 00:28:13,280
And that's the same thing with lounge learning.

766
00:28:13,280 --> 00:28:14,520
It's called curriculum learning.

767
00:28:14,520 --> 00:28:16,760
Train it on a big base that's solid and then did it.

768
00:28:16,760 --> 00:28:19,040
It sounds familiar, doesn't it?

769
00:28:19,280 --> 00:28:21,840
The hardest thing is how do you instill values

770
00:28:21,840 --> 00:28:24,880
and political correctness in models?

771
00:28:24,880 --> 00:28:27,280
There is no such thing as an unbiased model.

772
00:28:27,280 --> 00:28:30,120
So Dali too, when OpenAI had that

773
00:28:30,120 --> 00:28:32,040
and they introduced a bias filter,

774
00:28:32,040 --> 00:28:35,880
any non-gendered word that ran a random gender

775
00:28:35,880 --> 00:28:37,680
and a random ethnicity.

776
00:28:37,680 --> 00:28:38,840
So you type in Sumo wrestler

777
00:28:38,840 --> 00:28:41,320
and you get Indian female Sumo wrestler.

778
00:28:41,320 --> 00:28:42,280
That was kind of a good picture.

779
00:28:42,280 --> 00:28:43,680
I got to save somewhere.

780
00:28:43,680 --> 00:28:45,800
I think this is why you need national data sets.

781
00:28:45,800 --> 00:28:47,040
You need cultural data sets.

782
00:28:47,040 --> 00:28:48,800
You need personal data sets

783
00:28:48,800 --> 00:28:51,280
that can interact with these base models

784
00:28:51,280 --> 00:28:53,080
and customize to you and your stories.

785
00:28:53,080 --> 00:28:55,240
Because you and I both have our stories

786
00:28:55,240 --> 00:28:56,320
that make up our psyche.

787
00:28:56,320 --> 00:28:57,160
Sure.

788
00:28:57,160 --> 00:28:59,120
And understanding that context is so important

789
00:28:59,120 --> 00:29:02,440
to have AIs that can work for us, not on us.

790
00:29:02,440 --> 00:29:04,440
And so it's essentially like a next generation cookie

791
00:29:04,440 --> 00:29:07,000
that personalizes our data to allow for better searches.

792
00:29:07,000 --> 00:29:07,840
And mega cookie.

793
00:29:07,840 --> 00:29:09,640
And if you standardize the base foundation models

794
00:29:09,640 --> 00:29:11,760
and they call it the hypercube every modality

795
00:29:11,760 --> 00:29:12,840
because we do all the modalities,

796
00:29:12,840 --> 00:29:15,080
all the sectors and all the nationalities,

797
00:29:15,080 --> 00:29:17,720
then you don't need to have a million different models

798
00:29:17,720 --> 00:29:19,320
like those dream booths of the avatars.

799
00:29:19,320 --> 00:29:20,400
You said you have a base model

800
00:29:20,400 --> 00:29:22,640
that you then have a vector embedding around.

801
00:29:22,640 --> 00:29:24,240
Because these models contain all the principles

802
00:29:24,240 --> 00:29:25,920
and the embeddings point to the important bits

803
00:29:25,920 --> 00:29:27,840
that make up Harry or Emmad.

804
00:29:27,840 --> 00:29:29,960
And then you can search those and adapt those

805
00:29:29,960 --> 00:29:32,560
rather than having a million, billion different models,

806
00:29:32,560 --> 00:29:33,560
which is just confusing.

807
00:29:33,560 --> 00:29:35,080
So I had dinner the other day

808
00:29:35,080 --> 00:29:37,840
with one of the largest media publication owners

809
00:29:37,840 --> 00:29:39,520
in the world.

810
00:29:39,520 --> 00:29:41,400
And he said that I'm white, Harry.

811
00:29:41,400 --> 00:29:42,800
I don't think that I will have a business

812
00:29:42,800 --> 00:29:44,080
in a couple of years.

813
00:29:44,080 --> 00:29:46,400
I think, bluntly, we're getting killed on our advertising

814
00:29:46,440 --> 00:29:48,240
because everything's getting scraped

815
00:29:48,240 --> 00:29:50,280
and they're not coming to our websites.

816
00:29:50,280 --> 00:29:51,280
And that's where we get paid.

817
00:29:51,280 --> 00:29:52,480
We get paid for clicks.

818
00:29:53,920 --> 00:29:55,640
Is he right to be worried?

819
00:29:55,640 --> 00:29:56,600
I think he is right to be worried.

820
00:29:56,600 --> 00:29:58,600
Like, again, you look at Google's announcements yesterday

821
00:29:58,600 --> 00:30:00,800
to talk about this day after Palm 2,

822
00:30:00,800 --> 00:30:02,160
you suddenly look at the new Google page

823
00:30:02,160 --> 00:30:04,760
where they've got the language model

824
00:30:04,760 --> 00:30:07,720
and it's just text and where they clicks.

825
00:30:07,720 --> 00:30:10,200
It was like when Google introduced AMP.

826
00:30:10,200 --> 00:30:11,520
You know, this is where rather than look

827
00:30:11,520 --> 00:30:12,360
at the New York Times page,

828
00:30:12,360 --> 00:30:13,200
you have this formatted thing

829
00:30:13,200 --> 00:30:15,640
with no New York Times kind of stuff there.

830
00:30:15,640 --> 00:30:18,360
Like, these search entities that aggregate

831
00:30:18,360 --> 00:30:20,440
are just intermediating more and more

832
00:30:20,440 --> 00:30:22,760
and people are going to become used

833
00:30:22,760 --> 00:30:25,680
to just having synthesized input.

834
00:30:25,680 --> 00:30:26,920
So what does search look like?

835
00:30:26,920 --> 00:30:29,160
What does it look like when your GPT-4

836
00:30:29,160 --> 00:30:32,320
can write you an article about any news that's happening

837
00:30:32,320 --> 00:30:33,960
in a way that's customized to you

838
00:30:33,960 --> 00:30:36,120
and your context and everything like that?

839
00:30:37,240 --> 00:30:40,440
This is massively disruptive for media and information.

840
00:30:40,440 --> 00:30:42,280
And so they have to think, where am I in the future?

841
00:30:42,280 --> 00:30:44,480
Where, again, the way I swear to think about the impact

842
00:30:44,520 --> 00:30:47,000
this is the retanted grads are caching off their meds

843
00:30:47,000 --> 00:30:49,320
and we push a button and get 1,000 of them.

844
00:30:49,320 --> 00:30:51,480
Those grads include journalists.

845
00:30:51,480 --> 00:30:53,680
And you can have your own journalist army,

846
00:30:53,680 --> 00:30:55,360
your own writer army, your own coder army,

847
00:30:55,360 --> 00:30:57,040
your own designer army.

848
00:30:57,040 --> 00:30:59,440
So the pushback against that is libel.

849
00:30:59,440 --> 00:31:01,120
He said, good fucking luck.

850
00:31:01,120 --> 00:31:05,520
We spend our life in law suits, libel is real.

851
00:31:05,520 --> 00:31:09,160
You are gonna get unbelievable amounts of libel cases

852
00:31:09,160 --> 00:31:11,480
and then open AI will be fucked.

853
00:31:11,480 --> 00:31:14,400
You cannot have 1,000 libel cases a day.

854
00:31:14,400 --> 00:31:15,240
Well, this is the thing.

855
00:31:15,240 --> 00:31:18,200
If you say this needs to be checked and cross-checked,

856
00:31:18,200 --> 00:31:19,880
that's one thing, but a lot of the media companies

857
00:31:19,880 --> 00:31:21,480
say we're the source of authority.

858
00:31:21,480 --> 00:31:24,680
So a way that media companies can shift is by having

859
00:31:24,680 --> 00:31:26,440
in a deep fake and other age

860
00:31:26,440 --> 00:31:28,720
where everything can be generated,

861
00:31:28,720 --> 00:31:31,560
we make sure this is real news.

862
00:31:31,560 --> 00:31:33,680
We are very thorough in the way we do it.

863
00:31:33,680 --> 00:31:34,520
So this is interesting,

864
00:31:34,520 --> 00:31:36,800
so you place a premium on authority.

865
00:31:36,800 --> 00:31:39,560
Premium authority, this is why you've got the check marks

866
00:31:39,600 --> 00:31:41,560
coming out at Twitter and the organizational

867
00:31:41,560 --> 00:31:44,520
1,000 pounds a month and Facebook doing the same

868
00:31:44,520 --> 00:31:46,440
because you need to have a level of authenticity

869
00:31:46,440 --> 00:31:47,800
and level of authority.

870
00:31:47,800 --> 00:31:50,160
But again, is the news fair and balanced?

871
00:31:50,160 --> 00:31:51,960
I've had lots of hit pieces coming out against me

872
00:31:51,960 --> 00:31:53,000
and got a lot more.

873
00:31:53,000 --> 00:31:54,840
It's not because they have angles, you know?

874
00:31:54,840 --> 00:31:56,640
And so what is the bias of the New York Times

875
00:31:56,640 --> 00:31:59,040
versus this, versus that, versus others?

876
00:31:59,040 --> 00:32:00,440
How do people consume news now?

877
00:32:00,440 --> 00:32:03,720
And even news consumption has gone down dramatically, right?

878
00:32:03,720 --> 00:32:05,800
Because people consume news through their social networks

879
00:32:05,800 --> 00:32:08,000
through their groups and other things.

880
00:32:08,000 --> 00:32:10,840
So you have to say, what is the model?

881
00:32:10,840 --> 00:32:13,120
But the hard part is, you know,

882
00:32:13,120 --> 00:32:15,280
none of the next generation models

883
00:32:15,280 --> 00:32:18,120
or AI providers want to be content publishers.

884
00:32:18,120 --> 00:32:20,440
So how do you fit in a world where, you know,

885
00:32:20,440 --> 00:32:23,240
you're killing that business model on the content side,

886
00:32:23,240 --> 00:32:24,800
but they don't want to be publishers.

887
00:32:24,800 --> 00:32:26,640
You will have AI first publishers.

888
00:32:27,920 --> 00:32:29,400
So if you remember a kind of Vox

889
00:32:29,400 --> 00:32:30,520
and these things when they kind of kicked off,

890
00:32:30,520 --> 00:32:31,560
they wanted to be generated,

891
00:32:31,560 --> 00:32:33,960
they wanted to be technology first.

892
00:32:33,960 --> 00:32:37,120
You're gonna have a new wave of AI first publishers

893
00:32:37,120 --> 00:32:39,880
that aren't just AI, but it's AI plus humans.

894
00:32:39,880 --> 00:32:41,000
Because AI plus-

895
00:32:41,000 --> 00:32:41,840
What does that look like?

896
00:32:41,840 --> 00:32:42,840
Sorry, AI plus humans.

897
00:32:42,840 --> 00:32:46,200
AI plus humans means that you have information coming in

898
00:32:46,200 --> 00:32:49,200
and then the stories or drafts are automatically written,

899
00:32:49,200 --> 00:32:51,520
reviewed by humans who then give their input

900
00:32:51,520 --> 00:32:52,680
to train it better.

901
00:32:53,800 --> 00:32:55,520
This is kind of the feedback flow.

902
00:32:55,520 --> 00:32:56,840
And then what happens is it comes out

903
00:32:56,840 --> 00:32:58,160
and there's a factual anchors

904
00:32:58,160 --> 00:33:00,040
and then it gets customized to Alabama

905
00:33:01,120 --> 00:33:03,880
and then Alabama context and all sorts of other things.

906
00:33:03,920 --> 00:33:06,320
Because you can tell it, TLDR,

907
00:33:06,320 --> 00:33:09,120
two ladies didn't read, explain it like I'm five,

908
00:33:09,120 --> 00:33:10,840
make it more complex.

909
00:33:10,840 --> 00:33:13,200
And so you're gonna see something very interesting here,

910
00:33:13,200 --> 00:33:14,920
which is the right news at the right time.

911
00:33:14,920 --> 00:33:17,080
The localization will return,

912
00:33:17,080 --> 00:33:18,760
but again, through AI first.

913
00:33:18,760 --> 00:33:21,520
I think this is the thing, we're seeing AI integrated,

914
00:33:21,520 --> 00:33:22,840
but the next wave is going to be

915
00:33:22,840 --> 00:33:24,080
once we understand design patterns,

916
00:33:24,080 --> 00:33:27,400
AI first, everything and information flows.

917
00:33:27,400 --> 00:33:29,160
Once these technologies are a bit more mature.

918
00:33:29,160 --> 00:33:30,760
Can you just help me understand AI integrated

919
00:33:30,760 --> 00:33:32,600
versus AI first, what is-

920
00:33:32,600 --> 00:33:35,000
AI integrated means that I have an existing newsroom

921
00:33:35,000 --> 00:33:36,760
and I bring in AI to write faster drafts

922
00:33:36,760 --> 00:33:37,720
and things like that.

923
00:33:37,720 --> 00:33:40,200
AI first is saying, I have an army of things

924
00:33:40,200 --> 00:33:42,680
I can spin up instantly that can help me achieve

925
00:33:42,680 --> 00:33:44,280
these certain things to create news

926
00:33:44,280 --> 00:33:47,240
that is valuable for this reason with this feedback loop.

927
00:33:47,240 --> 00:33:49,920
And so you build the system kind of from the start

928
00:33:49,920 --> 00:33:53,200
thinking AI at the core versus AI being integrated in

929
00:33:53,200 --> 00:33:55,040
to improve existing systems.

930
00:33:55,040 --> 00:33:56,400
Because so much of news is what?

931
00:33:56,400 --> 00:33:58,160
We find information, we have drafting,

932
00:33:58,160 --> 00:34:01,080
we have this, we have that, we do these checks.

933
00:34:01,080 --> 00:34:03,240
A lot of that can be simplified,

934
00:34:03,240 --> 00:34:07,040
just like we move from the analog to the digital age

935
00:34:07,040 --> 00:34:10,200
to the internet age to the next age as the AI age.

936
00:34:10,200 --> 00:34:12,560
So I have to ask, when we think about kind of AI first

937
00:34:12,560 --> 00:34:14,680
publishers and the next generation of media,

938
00:34:16,320 --> 00:34:17,200
who does this?

939
00:34:17,200 --> 00:34:18,600
Is this startups?

940
00:34:18,600 --> 00:34:22,480
I've met honestly 50, maybe more AI companies

941
00:34:22,480 --> 00:34:24,080
in the last month.

942
00:34:24,080 --> 00:34:26,840
And the feedback is always the same.

943
00:34:26,840 --> 00:34:30,560
They're not operating off a defensible mode of data

944
00:34:30,760 --> 00:34:32,280
that literally a thin application layer

945
00:34:32,280 --> 00:34:36,680
on top of an existing model is 99% of the feedback.

946
00:34:36,680 --> 00:34:38,680
So you look at something like Harvey, for example.

947
00:34:38,680 --> 00:34:41,280
They went to law firms, they said, you are a distribution

948
00:34:41,280 --> 00:34:43,280
and we're going to integrate and improve your system

949
00:34:43,280 --> 00:34:45,080
and build our system for your system.

950
00:34:45,080 --> 00:34:46,680
So I think a lot of these people are trying to build it

951
00:34:46,680 --> 00:34:48,280
and they will come and they're trying to get in there

952
00:34:48,280 --> 00:34:50,320
as opposed to just retargeting.

953
00:34:50,320 --> 00:34:51,600
Where can you go in and transform?

954
00:34:51,600 --> 00:34:53,360
Is that the wrong model Harvey did?

955
00:34:53,360 --> 00:34:54,320
No, I think it's the right model.

956
00:34:54,320 --> 00:34:56,520
I think that a lot of organizations are elastic

957
00:34:56,520 --> 00:34:58,400
and plastic now, so you can go in and give them

958
00:34:58,440 --> 00:35:00,680
an integrated thing saying, you will be my test case.

959
00:35:00,680 --> 00:35:03,320
I will help you upgrade as a Skunkworks lab

960
00:35:03,320 --> 00:35:05,880
and build a system alongside your system as it were.

961
00:35:05,880 --> 00:35:08,200
And sorry, and you think enterprises will say sure?

962
00:35:08,200 --> 00:35:11,240
I think now they will if you can keep

963
00:35:11,240 --> 00:35:13,160
their data inside internally.

964
00:35:13,160 --> 00:35:14,880
And I think again, with better open models,

965
00:35:14,880 --> 00:35:16,000
you can enable that.

966
00:35:16,000 --> 00:35:17,520
So people can build on top of open models.

967
00:35:17,520 --> 00:35:20,880
There are dedicated instances on Cohere and others as well.

968
00:35:20,880 --> 00:35:22,600
And so the tooling is now catching up

969
00:35:22,600 --> 00:35:24,840
so that you can have a new generation of startups

970
00:35:24,840 --> 00:35:27,000
where their first customers are massive companies,

971
00:35:27,000 --> 00:35:29,520
they would never get otherwise.

972
00:35:29,520 --> 00:35:30,360
I think this is the thing,

973
00:35:30,360 --> 00:35:32,440
because every big company is looking for an answer.

974
00:35:32,440 --> 00:35:34,160
If you can give that answer,

975
00:35:34,160 --> 00:35:35,760
that contract that would have taken you a year,

976
00:35:35,760 --> 00:35:37,360
you can get in a week.

977
00:35:37,360 --> 00:35:38,200
Do you think so?

978
00:35:38,200 --> 00:35:39,400
Because you still got to get in the door.

979
00:35:39,400 --> 00:35:42,040
You got to get in the door, and that's hustle, man.

980
00:35:42,040 --> 00:35:44,440
So again, this is what the Harvey guys kind of did.

981
00:35:44,440 --> 00:35:46,320
This is why I went straight to the hyperscalers

982
00:35:46,320 --> 00:35:48,320
and I said, you need to have standardized models

983
00:35:48,320 --> 00:35:49,640
for open, for regulated data.

984
00:35:49,640 --> 00:35:50,880
What did they say to you?

985
00:35:50,880 --> 00:35:52,800
They said, really?

986
00:35:52,800 --> 00:35:53,640
Can you build them?

987
00:35:53,640 --> 00:35:54,800
Here's some models that we built.

988
00:35:54,800 --> 00:35:56,640
Oh, and then I told them exactly

989
00:35:56,640 --> 00:35:59,120
how the things would be last summer to now.

990
00:35:59,120 --> 00:36:00,960
And it's followed that and I've kept in touch

991
00:36:00,960 --> 00:36:01,800
and I've improved it.

992
00:36:01,800 --> 00:36:03,000
And this is why I'm building dedicated teams

993
00:36:03,000 --> 00:36:04,520
for the largest companies in the world.

994
00:36:04,520 --> 00:36:05,880
I'm not telling them, I'm trying to sell you anything.

995
00:36:05,880 --> 00:36:07,520
I'm like, over the next year,

996
00:36:07,520 --> 00:36:10,520
I'm gonna help make sure you do not get blindsided.

997
00:36:10,520 --> 00:36:11,840
Like I try and sell you models

998
00:36:11,840 --> 00:36:14,280
and people are offering us tens of millions per model.

999
00:36:14,280 --> 00:36:17,480
But I'm like, I'm gonna build a proper partnership with you.

1000
00:36:17,480 --> 00:36:19,560
And that means I'll have a LTV from you.

1001
00:36:19,560 --> 00:36:21,240
What does that proper partnership mean?

1002
00:36:21,240 --> 00:36:22,080
And who's that with?

1003
00:36:22,080 --> 00:36:24,000
That's with IBM, that's with SAP, that's with Apple.

1004
00:36:24,000 --> 00:36:26,000
So we've announced Amazon.

1005
00:36:26,120 --> 00:36:27,440
Let's say we have lots of other announced

1006
00:36:27,440 --> 00:36:28,640
with the biggest companies in the world

1007
00:36:28,640 --> 00:36:30,640
where they have amazing teams,

1008
00:36:30,640 --> 00:36:32,440
but they can only move so fast.

1009
00:36:32,440 --> 00:36:33,680
And I'm building dedicated teams

1010
00:36:33,680 --> 00:36:36,000
that help them move and understand the whole sector

1011
00:36:36,000 --> 00:36:39,160
without trying to sell them on services.

1012
00:36:39,160 --> 00:36:41,240
I'm trying to say, I will build you a customized model

1013
00:36:41,240 --> 00:36:42,480
if you want, but I'm only doing that

1014
00:36:42,480 --> 00:36:44,600
with a dozen companies, you know?

1015
00:36:44,600 --> 00:36:46,200
So I can kind of focus down.

1016
00:36:46,200 --> 00:36:48,560
And I will tell you that GPT-4 is great

1017
00:36:48,560 --> 00:36:50,640
or Cohera is great or all this stuff.

1018
00:36:50,640 --> 00:36:52,760
All the latest research through the communities we support,

1019
00:36:52,760 --> 00:36:54,400
I will make sure you're on top of,

1020
00:36:54,440 --> 00:36:55,280
rather than to your sector

1021
00:36:55,280 --> 00:36:56,320
and you've got dedicated people

1022
00:36:56,320 --> 00:36:57,680
helping you in this transition period.

1023
00:36:57,680 --> 00:36:59,160
Is that aligned to your core model?

1024
00:36:59,160 --> 00:37:01,400
It seems like an ancillary product

1025
00:37:01,400 --> 00:37:04,120
that is like a SAP consulting services.

1026
00:37:04,120 --> 00:37:05,360
It is kind of like that

1027
00:37:05,360 --> 00:37:08,760
because I need to understand these sectors better.

1028
00:37:08,760 --> 00:37:10,200
What does the hypercube look like?

1029
00:37:10,200 --> 00:37:13,040
What does the insurance adjust to GPT look like?

1030
00:37:13,040 --> 00:37:14,920
You know, as a fundamental basis.

1031
00:37:14,920 --> 00:37:17,360
And so a lot of people are able to extract that data

1032
00:37:17,360 --> 00:37:19,080
and then take it with you and do the learning on it.

1033
00:37:19,080 --> 00:37:20,360
Yes, and so this is part of the thing,

1034
00:37:20,360 --> 00:37:22,480
that we will have a generalized model and we're very clear.

1035
00:37:22,480 --> 00:37:24,120
But then you can have a specified model

1036
00:37:24,120 --> 00:37:25,480
just for you as well,

1037
00:37:25,480 --> 00:37:27,040
as long as it doesn't interfere with that.

1038
00:37:27,040 --> 00:37:28,720
So find that balance will be interesting,

1039
00:37:28,720 --> 00:37:31,600
but the reality is no models that are out today

1040
00:37:31,600 --> 00:37:33,280
will be used in a year.

1041
00:37:33,280 --> 00:37:35,480
Unpack that for me, this is mind blowing.

1042
00:37:35,480 --> 00:37:37,480
So again, you see the order of magnitude improvement.

1043
00:37:37,480 --> 00:37:39,720
Palm last year was 540 billion parameters,

1044
00:37:39,720 --> 00:37:41,720
then Chinchilla 67 and now 14.

1045
00:37:43,280 --> 00:37:46,440
540 to 14 is a big step, you know?

1046
00:37:46,440 --> 00:37:49,160
You see the quality of GPT-3 versus GPT-4.

1047
00:37:49,160 --> 00:37:50,840
Is there any extent to how low it can go?

1048
00:37:50,840 --> 00:37:51,800
We have no idea.

1049
00:37:52,800 --> 00:37:53,720
Like I would have said,

1050
00:37:53,720 --> 00:37:56,000
you already said this is impossible.

1051
00:37:56,000 --> 00:37:58,560
Like two years ago, it was like no way.

1052
00:37:58,560 --> 00:38:00,840
You have a single file that's maybe a few hundred gigabytes

1053
00:38:00,840 --> 00:38:03,840
that can pass every exam apart from English Lit.

1054
00:38:03,840 --> 00:38:04,880
Fucking English Lit.

1055
00:38:04,880 --> 00:38:07,280
Fucking English Lit, no way, no way.

1056
00:38:07,280 --> 00:38:09,160
So we're already at the impossible and like,

1057
00:38:09,160 --> 00:38:10,480
what does that mean though?

1058
00:38:10,480 --> 00:38:13,760
Like if we go lower and lower and lower, and then what?

1059
00:38:13,760 --> 00:38:14,600
And then what?

1060
00:38:14,600 --> 00:38:16,760
When it jumps as you saw with the llama stuff

1061
00:38:16,760 --> 00:38:18,320
and all the innovation around that

1062
00:38:18,320 --> 00:38:20,720
to your MacBook offline,

1063
00:38:20,760 --> 00:38:22,120
the marginal cost of creation

1064
00:38:22,120 --> 00:38:23,760
and coordination becomes zero.

1065
00:38:25,520 --> 00:38:27,120
I don't know what it means.

1066
00:38:27,120 --> 00:38:28,240
Nobody does.

1067
00:38:28,240 --> 00:38:29,840
And this is the thing.

1068
00:38:29,840 --> 00:38:31,560
It always takes longer and shorter

1069
00:38:31,560 --> 00:38:33,800
to implement groundbreaking technology than you've ever seen

1070
00:38:33,800 --> 00:38:35,040
and this technology can be in place

1071
00:38:35,040 --> 00:38:36,760
like nothing we've seen before.

1072
00:38:36,760 --> 00:38:38,200
Well, this is my call.

1073
00:38:38,200 --> 00:38:40,640
Not concerned, I hate the doomsday it says

1074
00:38:40,640 --> 00:38:41,600
and I'm excited for the future,

1075
00:38:41,600 --> 00:38:42,640
I'm terrified for the future too,

1076
00:38:42,640 --> 00:38:44,720
but everyone always says technology revolutions

1077
00:38:44,720 --> 00:38:45,560
in industrial age,

1078
00:38:45,560 --> 00:38:47,560
whether it's the introduction of PCs in 25s.

1079
00:38:47,560 --> 00:38:50,040
These were, industrial was a 30 year plus.

1080
00:38:50,120 --> 00:38:52,760
Actually PCs in 25s was 10 years plus.

1081
00:38:52,760 --> 00:38:56,080
The challenging thing is like the learning curve

1082
00:38:56,080 --> 00:38:59,840
to use chat GPT as a marketer is nothing.

1083
00:38:59,840 --> 00:39:00,880
I mean, it is easy.

1084
00:39:00,880 --> 00:39:03,640
And so, and the integrations is a day.

1085
00:39:03,640 --> 00:39:05,560
It's because, yeah, like you want to write an API,

1086
00:39:05,560 --> 00:39:08,000
it's not a day, you just give it the manifest spec

1087
00:39:08,000 --> 00:39:09,240
and it automatically generates.

1088
00:39:09,240 --> 00:39:10,480
It would have taken days before.

1089
00:39:10,480 --> 00:39:11,320
Sure.

1090
00:39:11,320 --> 00:39:12,680
Like it's an amazing experience.

1091
00:39:12,680 --> 00:39:15,400
And so the transition is so much more compressed today.

1092
00:39:15,400 --> 00:39:17,360
It came from the existing system

1093
00:39:17,360 --> 00:39:19,240
as it goes seamlessly into the existing system

1094
00:39:19,240 --> 00:39:20,880
versus like web three that tried to create a system

1095
00:39:20,880 --> 00:39:22,640
outside the existing system

1096
00:39:22,640 --> 00:39:25,080
and all the money was made and lost at the interfaces.

1097
00:39:25,080 --> 00:39:26,960
Again, it's like deploying grads at scale.

1098
00:39:26,960 --> 00:39:30,280
Like with a 32,000 token context within your GPT-4,

1099
00:39:30,280 --> 00:39:32,760
20,000 words of instructions.

1100
00:39:32,760 --> 00:39:34,320
What does that do to SAS?

1101
00:39:34,320 --> 00:39:36,800
So my thing is that we're still in this crazy period.

1102
00:39:36,800 --> 00:39:39,480
Next year it will settle and then it'll go ubiquitous.

1103
00:39:39,480 --> 00:39:41,080
Well, a lot of companies know they need to do something

1104
00:39:41,080 --> 00:39:42,120
but they don't know what they need to do.

1105
00:39:42,120 --> 00:39:43,560
Are they adopting it now?

1106
00:39:43,560 --> 00:39:44,800
They're doing the POC thing.

1107
00:39:44,800 --> 00:39:47,000
Like some like Microsoft and others for consumer,

1108
00:39:47,000 --> 00:39:47,840
they're adopting it.

1109
00:39:47,840 --> 00:39:50,040
Consumer adoption is a much lower bar.

1110
00:39:50,040 --> 00:39:51,480
When this starts going in enterprise,

1111
00:39:51,480 --> 00:39:53,480
it's gonna be a fricking train

1112
00:39:53,480 --> 00:39:55,680
because so much of enterprise is about services

1113
00:39:55,680 --> 00:39:57,400
and information flow.

1114
00:39:57,400 --> 00:39:58,840
Again, if you push a button and have a thousand

1115
00:39:58,840 --> 00:40:03,840
of these things, that's a huge difference, right?

1116
00:40:04,560 --> 00:40:06,320
And so like, I think this will be

1117
00:40:06,320 --> 00:40:08,800
a bigger economic impact than COVID.

1118
00:40:08,800 --> 00:40:10,680
I don't know in which direction.

1119
00:40:10,680 --> 00:40:11,920
I hope that you're positive.

1120
00:40:11,920 --> 00:40:13,840
But again, giving that example of an India

1121
00:40:13,840 --> 00:40:15,280
or one of these outsource places,

1122
00:40:15,280 --> 00:40:16,520
you lose BPO jobs,

1123
00:40:16,560 --> 00:40:19,200
you can make it up on entrepreneurship

1124
00:40:19,200 --> 00:40:20,880
if you embrace the technology.

1125
00:40:20,880 --> 00:40:22,840
What do you think the business model of the future is

1126
00:40:22,840 --> 00:40:25,160
for those models moving into enterprise?

1127
00:40:25,160 --> 00:40:26,520
I think it's the same as always.

1128
00:40:26,520 --> 00:40:28,600
You've got good products, good distribution.

1129
00:40:28,600 --> 00:40:31,880
You lock it in, 1.5 million people still use AOL.

1130
00:40:33,240 --> 00:40:35,720
Like HCL bought Lotus Notes for 1.5 billion

1131
00:40:35,720 --> 00:40:36,920
a few years ago.

1132
00:40:36,920 --> 00:40:39,720
Like 40% of the world still doesn't have internet.

1133
00:40:42,880 --> 00:40:45,520
Again, we're super privileged where we are, right?

1134
00:40:45,560 --> 00:40:46,400
And so you look at that

1135
00:40:46,400 --> 00:40:48,040
and I look at emerging markets.

1136
00:40:48,040 --> 00:40:51,280
I'm like, all of finance is securitization and leverage

1137
00:40:51,280 --> 00:40:53,920
and securitization is telling a story.

1138
00:40:53,920 --> 00:40:55,200
The only thing that matters for a stock

1139
00:40:55,200 --> 00:40:57,640
is the marginal story and how it evolves.

1140
00:40:57,640 --> 00:40:59,240
What if you have massive information

1141
00:40:59,240 --> 00:41:02,440
about every child in Africa and every business in India

1142
00:41:02,440 --> 00:41:04,600
and they embrace this technology properly?

1143
00:41:05,720 --> 00:41:07,440
Massive financial growth.

1144
00:41:07,440 --> 00:41:10,160
Why do you think next year for their embracing it?

1145
00:41:10,160 --> 00:41:12,360
I think that people are still getting used to all this.

1146
00:41:12,360 --> 00:41:13,840
We haven't standardizing things.

1147
00:41:13,840 --> 00:41:16,240
We don't know what the design patterns are.

1148
00:41:16,240 --> 00:41:18,200
I think that what happens is everyone's doing this

1149
00:41:18,200 --> 00:41:19,520
at the same time and they're all trying

1150
00:41:19,520 --> 00:41:20,800
to get to grips with it.

1151
00:41:20,800 --> 00:41:22,520
And so again, we have this like six month window

1152
00:41:22,520 --> 00:41:23,600
where everyone's getting to grips with it

1153
00:41:23,600 --> 00:41:25,920
and then we standardize our design patterns

1154
00:41:25,920 --> 00:41:28,400
and they spread and you start implementing.

1155
00:41:28,400 --> 00:41:29,840
And then you see some people outpacing others

1156
00:41:29,840 --> 00:41:30,960
which means that you have to catch up

1157
00:41:30,960 --> 00:41:33,240
and then you're forced to implement it.

1158
00:41:33,240 --> 00:41:36,000
So this is how I see the race dynamics occurring right now.

1159
00:41:36,000 --> 00:41:37,480
You say about forced to implement it.

1160
00:41:37,480 --> 00:41:39,760
I think the truth is they just have no fricking idea.

1161
00:41:39,760 --> 00:41:40,600
Right now they don't.

1162
00:41:40,600 --> 00:41:42,160
Which I totally understand, I didn't blame them for

1163
00:41:42,160 --> 00:41:44,160
but I tweeted actually the other day

1164
00:41:44,160 --> 00:41:45,760
that I think the biggest AI companies

1165
00:41:45,760 --> 00:41:47,760
would be services-based implementation companies

1166
00:41:47,760 --> 00:41:49,200
for large enterprises.

1167
00:41:49,200 --> 00:41:50,040
100%.

1168
00:41:50,040 --> 00:41:51,360
That's why I said if you're a startup,

1169
00:41:51,360 --> 00:41:53,360
the best thing to do is you identify an enterprise

1170
00:41:53,360 --> 00:41:54,640
that will be transformed by this

1171
00:41:54,640 --> 00:41:57,840
and you go to them and you say, I have a solution.

1172
00:41:57,840 --> 00:41:59,440
And I'm gonna start with you.

1173
00:41:59,440 --> 00:42:01,520
And I might go bigger, but I'm gonna help you

1174
00:42:01,520 --> 00:42:04,480
through this period by doing this, this, and this.

1175
00:42:04,480 --> 00:42:05,520
And they will appreciate that

1176
00:42:05,520 --> 00:42:07,760
and they'll be capital available for that

1177
00:42:07,760 --> 00:42:09,640
in a way that you've never seen before.

1178
00:42:09,680 --> 00:42:11,160
You know how difficult it is for small companies

1179
00:42:11,160 --> 00:42:14,240
to sell to big, but the big companies have no idea

1180
00:42:14,240 --> 00:42:17,440
except for their CEO and their board are telling them.

1181
00:42:17,440 --> 00:42:18,640
You look at the number of mentions

1182
00:42:18,640 --> 00:42:20,360
and earnings calls, it's not like that.

1183
00:42:20,360 --> 00:42:21,840
Every earnings call next quarter

1184
00:42:21,840 --> 00:42:24,200
and then by next year, literally every single one,

1185
00:42:24,200 --> 00:42:25,560
they're like, what is our strategy?

1186
00:42:25,560 --> 00:42:27,840
It's not like, what is our web three and metaverse strategy?

1187
00:42:27,840 --> 00:42:29,480
It's like, I need this strategy now.

1188
00:42:29,480 --> 00:42:31,760
Again, it's like, what is our COVID strategy?

1189
00:42:31,760 --> 00:42:34,400
It'll be that level of urgency within a few quarters.

1190
00:42:34,400 --> 00:42:35,680
Would you raise money if you were then?

1191
00:42:35,680 --> 00:42:37,200
So you go to a corporate and you say,

1192
00:42:37,200 --> 00:42:39,520
hey, you know what, I can solve your problem.

1193
00:42:39,520 --> 00:42:41,480
This is how it'll work and they will fund you.

1194
00:42:41,480 --> 00:42:42,600
They'll give you the data.

1195
00:42:42,600 --> 00:42:44,280
Would you raise money?

1196
00:42:44,280 --> 00:42:46,320
Yeah, I mean, like again, you need the people.

1197
00:42:46,320 --> 00:42:47,560
The people is the key thing here

1198
00:42:47,560 --> 00:42:49,840
because you can have the technicals' chops, you know?

1199
00:42:49,840 --> 00:42:51,840
You have an understanding of the industry.

1200
00:42:51,840 --> 00:42:54,360
But to build a good business and to scale it

1201
00:42:54,360 --> 00:42:55,480
at the pace that you need to,

1202
00:42:55,480 --> 00:42:57,360
to keep up with this is incredibly hard.

1203
00:42:57,360 --> 00:42:58,720
Do we have enough talent?

1204
00:42:58,720 --> 00:42:59,880
No.

1205
00:42:59,880 --> 00:43:02,200
And so this is why we support the fast.ai courses

1206
00:43:02,200 --> 00:43:05,280
which transform normal developers into ML developers

1207
00:43:05,280 --> 00:43:06,720
and other things like that.

1208
00:43:06,720 --> 00:43:08,360
But again, these models are actually not that hard

1209
00:43:08,360 --> 00:43:09,200
to work with.

1210
00:43:09,200 --> 00:43:11,680
50% of all code on GitHub is AI generated now.

1211
00:43:11,680 --> 00:43:13,240
So you can even use co-pilot to help you

1212
00:43:13,240 --> 00:43:15,080
code the models and other things like that.

1213
00:43:15,080 --> 00:43:18,880
What do you think that code generation is in five years?

1214
00:43:18,880 --> 00:43:20,360
Why would you need code?

1215
00:43:20,360 --> 00:43:22,480
Code is just a way to talk to a computer.

1216
00:43:23,680 --> 00:43:24,520
I'm part of that.

1217
00:43:24,520 --> 00:43:25,360
So when I started...

1218
00:43:25,360 --> 00:43:26,280
You're speaking human language.

1219
00:43:26,280 --> 00:43:31,040
When I started 21, 22 years ago as a code, I'm 40 now.

1220
00:43:31,040 --> 00:43:32,400
So just doing that one's 18.

1221
00:43:32,400 --> 00:43:35,520
I was writing assembly code, you know,

1222
00:43:35,520 --> 00:43:37,280
really low level stuff.

1223
00:43:37,320 --> 00:43:38,160
There were no libraries.

1224
00:43:38,160 --> 00:43:39,000
There was no GitHub.

1225
00:43:39,000 --> 00:43:39,840
There was nothing like this.

1226
00:43:39,840 --> 00:43:42,000
Like right now coding is like mixing and matching.

1227
00:43:42,000 --> 00:43:43,920
It's like building Lego.

1228
00:43:43,920 --> 00:43:46,360
And AI can build that Lego much better,

1229
00:43:46,360 --> 00:43:47,520
especially in five years.

1230
00:43:47,520 --> 00:43:49,880
What you're doing when you're propping like programming

1231
00:43:49,880 --> 00:43:52,120
language is you're telling it to go and do something.

1232
00:43:52,120 --> 00:43:53,200
Even something like Palm,

1233
00:43:53,200 --> 00:43:56,160
like we sponsor an amazing code called Lucid Raines.

1234
00:43:56,160 --> 00:43:57,360
If you want to cry as a programmer,

1235
00:43:57,360 --> 00:43:58,440
you go and look at his GitHub,

1236
00:43:58,440 --> 00:44:00,520
most productive developer in the world.

1237
00:44:00,520 --> 00:44:03,520
He recreated the whole of Palm in 206 lines of PyTorch.

1238
00:44:05,160 --> 00:44:07,240
But again, why would you need a human for that?

1239
00:44:07,240 --> 00:44:09,520
If the AI gets better and better at coding,

1240
00:44:09,520 --> 00:44:10,720
just tell it what you want.

1241
00:44:10,720 --> 00:44:14,000
I want to create an app for 20 minute VC

1242
00:44:14,000 --> 00:44:14,960
that has these features.

1243
00:44:14,960 --> 00:44:17,880
Of course it will go and build it automatically.

1244
00:44:17,880 --> 00:44:19,560
Where is the human coder in that?

1245
00:44:19,560 --> 00:44:21,120
What does that mean for the future on refresh?

1246
00:44:21,120 --> 00:44:22,920
But actually a good thing in terms of the complete

1247
00:44:22,920 --> 00:44:24,800
democratization, anyone can build anything.

1248
00:44:24,800 --> 00:44:25,640
Anyone can build anything.

1249
00:44:25,640 --> 00:44:28,640
This is why distribution data, you know,

1250
00:44:28,640 --> 00:44:31,840
relationships, product become important.

1251
00:44:31,840 --> 00:44:34,640
Because it already became easier to build anything, right?

1252
00:44:34,640 --> 00:44:36,560
But what makes a good product?

1253
00:44:36,560 --> 00:44:38,360
Again, there are these unchanging things.

1254
00:44:38,360 --> 00:44:40,960
Have great customer satisfaction, deliver value.

1255
00:44:40,960 --> 00:44:43,040
People get distracted by technology.

1256
00:44:43,040 --> 00:44:45,640
Like I was at this CryptoX AI thing on the weekend.

1257
00:44:45,640 --> 00:44:47,240
They were talking about decentralized.

1258
00:44:47,240 --> 00:44:48,520
Guys, just this is all bollocks.

1259
00:44:48,520 --> 00:44:51,000
Like it's not about the technology,

1260
00:44:51,000 --> 00:44:53,960
it's about what you're creating that's valuable

1261
00:44:53,960 --> 00:44:55,920
to help people, you know?

1262
00:44:55,920 --> 00:44:56,840
Focus on that.

1263
00:44:56,840 --> 00:44:59,480
Who do you think wins in the next three to five years?

1264
00:44:59,480 --> 00:45:00,640
Startups or incumbents?

1265
00:45:00,640 --> 00:45:02,560
Because incumbents have the distribution.

1266
00:45:02,560 --> 00:45:03,760
I think it's incumbents,

1267
00:45:03,760 --> 00:45:05,440
but there's a lot of startups that will be billion dollars.

1268
00:45:05,480 --> 00:45:07,760
And even on the thin layer thing,

1269
00:45:07,760 --> 00:45:12,240
ITA software sold for 700 million

1270
00:45:12,240 --> 00:45:13,920
and KIAX sold for 2 billion.

1271
00:45:13,920 --> 00:45:16,000
And that was a layer on top of ITA.

1272
00:45:16,000 --> 00:45:17,920
We've seen many of these examples here, right?

1273
00:45:17,920 --> 00:45:19,640
Again, we know that value and moats

1274
00:45:19,640 --> 00:45:22,240
are not necessarily innovation first.

1275
00:45:22,240 --> 00:45:23,880
Well, yes and no, it's interesting.

1276
00:45:23,880 --> 00:45:26,160
I had Tom Tunga's on the show.

1277
00:45:26,160 --> 00:45:27,920
And he essentially analyzed,

1278
00:45:27,920 --> 00:45:29,880
Tom is a very famous ML and AI investor,

1279
00:45:29,880 --> 00:45:33,520
and he analyzed infrastructure versus application layer.

1280
00:45:33,520 --> 00:45:36,200
And both actually were about $2 trillion times.

1281
00:45:36,200 --> 00:45:37,880
The differences in the infrastructure layer,

1282
00:45:37,880 --> 00:45:39,480
there was three companies,

1283
00:45:39,480 --> 00:45:41,840
and in the application layer, there was 50.

1284
00:45:41,840 --> 00:45:44,280
And so your average enterprise value

1285
00:45:44,280 --> 00:45:45,640
was like significantly different.

1286
00:45:45,640 --> 00:45:46,480
I would agree with that.

1287
00:45:46,480 --> 00:45:48,560
I think that there's only gonna be five or six

1288
00:45:48,560 --> 00:45:50,440
foundation model companies in the world

1289
00:45:50,440 --> 00:45:52,320
in three years, five years.

1290
00:45:52,320 --> 00:45:54,160
Do you think they've all been created now?

1291
00:45:54,160 --> 00:45:55,000
Yes.

1292
00:45:55,000 --> 00:45:55,840
Which are they?

1293
00:45:55,840 --> 00:45:59,640
I think it's gonna be us, Nvidia, Google,

1294
00:45:59,640 --> 00:46:02,640
Microsoft, OpenAI, and Meta and Apple probably

1295
00:46:02,680 --> 00:46:04,200
are the ones that train these models.

1296
00:46:04,200 --> 00:46:05,800
Is Anthropic good?

1297
00:46:05,800 --> 00:46:06,920
Anthropic are great.

1298
00:46:06,920 --> 00:46:09,280
But from a business model perspective,

1299
00:46:09,280 --> 00:46:13,040
you have Claude on Google API, and you have Palm II.

1300
00:46:14,080 --> 00:46:16,080
How are they gonna keep up with Palm II?

1301
00:46:18,120 --> 00:46:18,960
You know?

1302
00:46:18,960 --> 00:46:19,960
I can't answer that.

1303
00:46:19,960 --> 00:46:21,720
Well, Google, they can raise billions,

1304
00:46:21,720 --> 00:46:25,240
but Google will spend $20 billion on AI.

1305
00:46:25,240 --> 00:46:28,000
DeepMind's salary budget is $1.2 billion a year.

1306
00:46:28,880 --> 00:46:31,640
So DeepMind's salary budget is $1.2 billion a year.

1307
00:46:31,640 --> 00:46:32,480
Yes.

1308
00:46:32,480 --> 00:46:33,800
So that's in the public kind of filing.

1309
00:46:33,800 --> 00:46:34,720
So was it salary and compute?

1310
00:46:34,720 --> 00:46:35,760
I think it's salary.

1311
00:46:35,760 --> 00:46:37,120
They technically make a billion a year

1312
00:46:37,120 --> 00:46:40,520
from their internal counter payments with Google as well.

1313
00:46:40,520 --> 00:46:41,360
Wow.

1314
00:46:41,360 --> 00:46:42,200
But again, I mean, Google,

1315
00:46:42,200 --> 00:46:43,040
how much money do they have?

1316
00:46:43,040 --> 00:46:45,080
$150 billion to win this?

1317
00:46:46,440 --> 00:46:47,280
Fuck.

1318
00:46:47,280 --> 00:46:49,080
How much money do you need?

1319
00:46:49,080 --> 00:46:52,160
I have a business model that is going to be massively

1320
00:46:52,160 --> 00:46:53,600
profitable very soon.

1321
00:46:53,600 --> 00:46:55,560
Because of the national services?

1322
00:46:55,560 --> 00:46:56,520
Because of various things.

1323
00:46:56,520 --> 00:46:57,520
I haven't given the full details.

1324
00:46:57,520 --> 00:46:58,600
I will over the next few months.

1325
00:46:58,600 --> 00:46:59,960
I've got a nice little case study

1326
00:46:59,960 --> 00:47:01,520
with some universities coming.

1327
00:47:01,520 --> 00:47:02,960
I like it to be a surprise.

1328
00:47:02,960 --> 00:47:04,680
Well, it's really hard for you.

1329
00:47:04,680 --> 00:47:07,040
Talent, keeping talent together, A plus teams.

1330
00:47:07,040 --> 00:47:08,960
So we've had zero attrition in our developers

1331
00:47:08,960 --> 00:47:09,800
and they're amazing.

1332
00:47:09,800 --> 00:47:11,160
So we've got video models, audio models,

1333
00:47:11,160 --> 00:47:12,000
all these things coming out.

1334
00:47:12,000 --> 00:47:13,680
Everyone says you need to be in the valley.

1335
00:47:13,680 --> 00:47:14,560
You're in London.

1336
00:47:14,560 --> 00:47:15,400
Yeah.

1337
00:47:15,400 --> 00:47:16,840
Do you disagree you need to be in the valley?

1338
00:47:16,840 --> 00:47:17,960
Of course you don't.

1339
00:47:17,960 --> 00:47:19,800
I am going to bring this technology to the whole world.

1340
00:47:19,800 --> 00:47:22,080
I'm going to bring it to all the IITs and universities

1341
00:47:22,080 --> 00:47:23,920
and the best of people in all of those

1342
00:47:23,920 --> 00:47:25,920
will join Stabilities in the local thing.

1343
00:47:25,920 --> 00:47:26,760
I'll have talent.

1344
00:47:26,760 --> 00:47:29,560
I'll bring this to all of the national broadcasters

1345
00:47:29,560 --> 00:47:31,080
and biggest family offices around the world.

1346
00:47:31,080 --> 00:47:32,320
I'll have data.

1347
00:47:32,320 --> 00:47:34,000
Nations will build supercomputers

1348
00:47:34,000 --> 00:47:35,120
that I will build open models on.

1349
00:47:35,120 --> 00:47:36,400
I'll have super compute.

1350
00:47:36,400 --> 00:47:38,280
So I'll have more super compute talent

1351
00:47:38,280 --> 00:47:40,000
and data than any other company.

1352
00:47:40,000 --> 00:47:42,600
And I'll build it all in the open.

1353
00:47:42,600 --> 00:47:44,320
And one thing I heard you talk about before,

1354
00:47:44,320 --> 00:47:45,160
which I thought was fascinating,

1355
00:47:45,160 --> 00:47:46,840
was your access to super computer.

1356
00:47:46,840 --> 00:47:49,360
You compared it to existing large incumbents.

1357
00:47:49,360 --> 00:47:51,880
Why do you have more super compute than other people?

1358
00:47:51,880 --> 00:47:53,360
Because I went and I did it.

1359
00:47:53,360 --> 00:47:55,440
So we had articles coming out saying about our burn.

1360
00:47:55,440 --> 00:47:57,240
I'm like, I have oil wells

1361
00:47:57,240 --> 00:47:59,440
when everyone wants to build petrochemicals.

1362
00:47:59,440 --> 00:48:01,280
Every day we have companies coming to us

1363
00:48:01,280 --> 00:48:02,520
asking us for our super compute

1364
00:48:02,520 --> 00:48:04,360
because it's not available on the market.

1365
00:48:04,360 --> 00:48:06,240
You need these chips lined up with interconnects

1366
00:48:06,240 --> 00:48:09,080
and we've got 7,000 A100s now.

1367
00:48:09,080 --> 00:48:11,200
We have TPUs, we have all these things.

1368
00:48:11,200 --> 00:48:12,160
And we know how to use them

1369
00:48:12,160 --> 00:48:14,800
and we can share them with people because we're open.

1370
00:48:15,800 --> 00:48:18,120
Whereas Anthropic and others cannot.

1371
00:48:18,120 --> 00:48:20,360
So this is like at the worst case,

1372
00:48:20,360 --> 00:48:23,080
I'll build a foundation model as a service company

1373
00:48:23,080 --> 00:48:25,480
and I'll make $100 million in profit this year

1374
00:48:25,480 --> 00:48:27,360
without having to charge even market rates

1375
00:48:27,360 --> 00:48:28,200
and I can retire.

1376
00:48:28,200 --> 00:48:30,080
I wouldn't do that and bring this to the world.

1377
00:48:30,080 --> 00:48:32,760
So I think computers misunderstood.

1378
00:48:32,760 --> 00:48:36,520
It's not like Bird and all these scooter companies

1379
00:48:36,520 --> 00:48:39,400
and others, they spent money on marketing.

1380
00:48:39,400 --> 00:48:41,960
This is actually an asset right now that's scarce.

1381
00:48:42,880 --> 00:48:44,960
And so there's no harm in scaling compute

1382
00:48:44,960 --> 00:48:47,000
and then with the top chip manufacturers,

1383
00:48:47,000 --> 00:48:48,640
they're building us dedicated teams

1384
00:48:48,640 --> 00:48:51,120
and again, they're coming in and supporting us

1385
00:48:51,120 --> 00:48:54,600
because our models drive demand for their chips.

1386
00:48:54,600 --> 00:48:56,600
The more open models there are, the more open demand is.

1387
00:48:56,600 --> 00:48:58,600
So it's a virtual circle there as well.

1388
00:48:58,600 --> 00:49:00,800
And so we get compute before everyone else.

1389
00:49:00,800 --> 00:49:03,520
Can I ask, in terms of like short-term economic growth,

1390
00:49:03,520 --> 00:49:06,400
how do you think about the impact that everything

1391
00:49:06,400 --> 00:49:09,040
we've just discussed has on rising inflation,

1392
00:49:09,040 --> 00:49:12,920
rising interest rates, short-term employment rates?

1393
00:49:12,920 --> 00:49:14,440
It's massively deflationary.

1394
00:49:14,440 --> 00:49:16,800
The biggest drivers of CPI inflation in the U.S.

1395
00:49:16,800 --> 00:49:18,200
were education and healthcare.

1396
00:49:19,480 --> 00:49:21,280
And that was almost all administrative

1397
00:49:21,280 --> 00:49:23,640
and bureaucratic in the next few years,

1398
00:49:23,640 --> 00:49:25,960
guess what gets disrupted, those.

1399
00:49:25,960 --> 00:49:27,920
But they don't get disrupted this year

1400
00:49:27,920 --> 00:49:29,920
or next year, it's the year after

1401
00:49:29,920 --> 00:49:33,160
because those ones take a bit longer to come through.

1402
00:49:33,160 --> 00:49:35,560
Okay, and so what is that?

1403
00:49:35,560 --> 00:49:36,720
How does that impact the economy

1404
00:49:36,720 --> 00:49:39,120
that we think kind of U.S., U.K.?

1405
00:49:39,120 --> 00:49:41,120
What does that look like in terms of a three-year time period?

1406
00:49:41,120 --> 00:49:42,920
I think the U.K. benefits.

1407
00:49:42,920 --> 00:49:44,760
Unicorn Kingdom is a new kind of thing is,

1408
00:49:44,760 --> 00:49:48,320
because it'll get, because we have amazing policies

1409
00:49:48,320 --> 00:49:50,720
like every single AI company should come to the U.K.

1410
00:49:50,720 --> 00:49:54,640
because cloud computing is now included in R&D tax credits.

1411
00:49:54,640 --> 00:49:57,600
It's a 27% rebate on losses in cash.

1412
00:49:58,600 --> 00:50:01,360
We can now issue scale-up visas, global talent visas

1413
00:50:01,360 --> 00:50:02,280
like the Deep Floyd team

1414
00:50:02,280 --> 00:50:04,560
that released the best image model in the world ever

1415
00:50:04,560 --> 00:50:05,800
from stability.

1416
00:50:05,800 --> 00:50:07,440
They were bought in on tech talent visas

1417
00:50:07,440 --> 00:50:08,560
that was turned around in one week.

1418
00:50:08,560 --> 00:50:09,880
Do you think the U.K. has done a good job

1419
00:50:09,880 --> 00:50:12,040
in terms of implementing regulation and policies

1420
00:50:12,040 --> 00:50:13,440
to bring AI talents?

1421
00:50:13,440 --> 00:50:15,800
Had the best apart from maybe Japan, yes.

1422
00:50:15,800 --> 00:50:17,360
What's Japan done?

1423
00:50:17,360 --> 00:50:19,080
Japan has some very interesting ones around

1424
00:50:19,080 --> 00:50:20,240
web data scraping and others,

1425
00:50:20,240 --> 00:50:22,120
but again, Japan is a very different culture.

1426
00:50:22,120 --> 00:50:23,320
So even if policy is good,

1427
00:50:23,320 --> 00:50:25,080
it still doesn't have the same innovative thing.

1428
00:50:25,080 --> 00:50:26,400
Who's done the worst?

1429
00:50:27,320 --> 00:50:28,640
The worst, I'm not sure actually.

1430
00:50:28,640 --> 00:50:29,520
No one's done too bad.

1431
00:50:29,520 --> 00:50:31,760
The new European legislation was really, really bad.

1432
00:50:31,760 --> 00:50:33,000
Now it's got a little bit better,

1433
00:50:33,000 --> 00:50:35,640
but always Europe wants to be the leader in regulation,

1434
00:50:35,640 --> 00:50:37,600
which kind of, you know, okay, fair enough.

1435
00:50:37,600 --> 00:50:38,560
It's never an easy thing.

1436
00:50:38,560 --> 00:50:39,400
It's never an easy thing.

1437
00:50:39,400 --> 00:50:40,240
And this is the thing,

1438
00:50:40,240 --> 00:50:42,360
like I think the U.K. is in a very good position

1439
00:50:42,360 --> 00:50:43,640
and the government's for leading me.

1440
00:50:43,640 --> 00:50:46,040
Look, the 900 million pound supercomputer,

1441
00:50:46,040 --> 00:50:47,960
100 million pound LLM task force

1442
00:50:47,960 --> 00:50:51,760
that's been equated to the COVID level of seriousness,

1443
00:50:51,760 --> 00:50:52,720
you know?

1444
00:50:52,720 --> 00:50:55,480
What do you make of like the open AI comparison?

1445
00:50:55,480 --> 00:50:58,240
I've seen quite a few which are open AI for Europe.

1446
00:50:59,440 --> 00:51:01,280
And we've seen three or four now.

1447
00:51:01,280 --> 00:51:03,440
Like, is this a zero sum game?

1448
00:51:03,440 --> 00:51:06,240
And open AI is one that race, so to speak, or?

1449
00:51:06,240 --> 00:51:08,080
I think it'd be difficult to compete against them

1450
00:51:08,080 --> 00:51:09,720
because they're executing incredibly well.

1451
00:51:09,720 --> 00:51:11,240
And I think, again,

1452
00:51:12,280 --> 00:51:14,360
why would you use open AI for Europe

1453
00:51:14,360 --> 00:51:18,600
versus palm two versus GPT four?

1454
00:51:18,600 --> 00:51:19,440
What can you bring?

1455
00:51:19,440 --> 00:51:20,840
But you will have national champions and others.

1456
00:51:20,840 --> 00:51:22,120
I think it's incredibly difficult

1457
00:51:22,120 --> 00:51:23,440
to compete in proprietary.

1458
00:51:25,160 --> 00:51:26,840
I think in open, it's a bit different

1459
00:51:26,840 --> 00:51:28,720
because of standardization element there.

1460
00:51:28,720 --> 00:51:31,560
But again, my play is to be the benchmark

1461
00:51:31,560 --> 00:51:32,760
across every modality,

1462
00:51:32,760 --> 00:51:33,720
because there's no other company

1463
00:51:33,720 --> 00:51:36,080
apart from me in open AI that does every modality.

1464
00:51:36,080 --> 00:51:37,920
There's no company that's as aggressive

1465
00:51:37,920 --> 00:51:39,680
as me in emerging markets.

1466
00:51:39,680 --> 00:51:41,520
And so they have to say, what is my edge?

1467
00:51:41,520 --> 00:51:42,400
Because you can have an edge,

1468
00:51:42,400 --> 00:51:46,440
like you can be the open AI for government or defense

1469
00:51:46,440 --> 00:51:49,760
or for healthcare and really get in and understand those.

1470
00:51:49,760 --> 00:51:51,720
And then you can be sticky, you know?

1471
00:51:52,280 --> 00:51:55,920
Scale is now going fully into defense, you know?

1472
00:51:55,920 --> 00:51:56,760
Scale AI's?

1473
00:51:56,760 --> 00:51:58,840
Yes, so they've announced the integrations

1474
00:51:58,840 --> 00:52:00,600
with the Air Force and all sorts of other things.

1475
00:52:00,600 --> 00:52:03,320
Like we're getting together on Defcorn this year

1476
00:52:03,320 --> 00:52:04,760
and people are going to hack at our models

1477
00:52:04,760 --> 00:52:07,320
and open AI's and others organized by scale.

1478
00:52:07,320 --> 00:52:09,120
What is your edge?

1479
00:52:09,120 --> 00:52:11,120
What is, again, your moat?

1480
00:52:11,120 --> 00:52:12,960
What is your business model and where?

1481
00:52:12,960 --> 00:52:14,240
What are you reliant upon

1482
00:52:14,240 --> 00:52:16,480
to deliver that value that can increase?

1483
00:52:16,480 --> 00:52:17,600
This is why I was surprised

1484
00:52:17,600 --> 00:52:19,960
when I saw you sign the petition of Elon

1485
00:52:20,040 --> 00:52:22,000
in terms of pausing for six months.

1486
00:52:22,000 --> 00:52:24,000
You used to unpack why you did that.

1487
00:52:24,000 --> 00:52:25,320
Well, I mean, for six months,

1488
00:52:25,320 --> 00:52:27,840
you're not getting H100s and TPUV5s anyway.

1489
00:52:27,840 --> 00:52:29,240
So it's a natural pause.

1490
00:52:29,240 --> 00:52:30,960
But then also because the shit show is coming next year.

1491
00:52:30,960 --> 00:52:32,760
So I said, we have to self-regulate.

1492
00:52:32,760 --> 00:52:35,840
Like for example, the adversaries already have GPT-4.

1493
00:52:35,840 --> 00:52:38,840
Why? Because you can just download it on a USB stick.

1494
00:52:38,840 --> 00:52:40,160
You know, you don't have to train your own

1495
00:52:40,160 --> 00:52:41,000
when you can just steal it.

1496
00:52:41,000 --> 00:52:42,480
Let's have better opsec.

1497
00:52:42,480 --> 00:52:44,000
Let's have better standards around data.

1498
00:52:44,000 --> 00:52:46,760
Let's stop and move off web scrapes by next year.

1499
00:52:46,760 --> 00:52:48,080
We had hundreds of millions of images

1500
00:52:48,080 --> 00:52:49,120
opted out of stable diffusion

1501
00:52:49,120 --> 00:52:51,400
because we were the only company in the world

1502
00:52:51,400 --> 00:52:53,760
to offer opt-out of datasets.

1503
00:52:53,760 --> 00:52:56,720
You know, like let's bring in some standards around this

1504
00:52:56,720 --> 00:52:58,400
before it's everywhere.

1505
00:52:58,400 --> 00:52:59,240
Basically where we are now.

1506
00:52:59,240 --> 00:53:02,280
You remember COVID, your mom is talking about this

1507
00:53:02,280 --> 00:53:04,960
and your aunt and everyone's talking about generative AI

1508
00:53:04,960 --> 00:53:06,760
and they're asking you, Harry, what's going on?

1509
00:53:06,760 --> 00:53:10,440
You know, but you haven't had the Tom Hanks moment yet.

1510
00:53:10,440 --> 00:53:11,800
Because everyone was talking about COVID

1511
00:53:11,800 --> 00:53:13,040
before Tom Hanks got it.

1512
00:53:13,040 --> 00:53:14,120
And then when Tom Hanks got it,

1513
00:53:14,120 --> 00:53:16,160
that's when global policy changed.

1514
00:53:16,160 --> 00:53:18,040
Because if Tom Hanks can get it, anyone can get it.

1515
00:53:18,080 --> 00:53:20,280
What is that moment for generative AI?

1516
00:53:20,280 --> 00:53:21,200
What do you think it is?

1517
00:53:21,200 --> 00:53:22,040
I don't know.

1518
00:53:22,040 --> 00:53:22,880
I know it's coming.

1519
00:53:22,880 --> 00:53:24,480
Because I know this technology is definitely

1520
00:53:24,480 --> 00:53:26,640
everywhere next year and it's disruptive.

1521
00:53:26,640 --> 00:53:27,800
You don't think there's a chance

1522
00:53:27,800 --> 00:53:29,760
that takes much longer, three to five years?

1523
00:53:29,760 --> 00:53:30,960
No chance.

1524
00:53:30,960 --> 00:53:33,680
It's so useful right now.

1525
00:53:33,680 --> 00:53:35,760
And you think about certain industries

1526
00:53:35,760 --> 00:53:36,880
and how they'll be affected

1527
00:53:36,880 --> 00:53:41,800
by having the ability to have 1,000 GPT-4s working together.

1528
00:53:41,800 --> 00:53:44,120
You said a tweet actually,

1529
00:53:44,120 --> 00:53:45,640
I think it was a reply to a tweet,

1530
00:53:45,640 --> 00:53:48,600
but you said hallucinations are a feature, not a bug.

1531
00:53:48,600 --> 00:53:50,560
Yeah, so right now people are trying to treat these models.

1532
00:53:50,560 --> 00:53:51,560
So we're trying the whole internet

1533
00:53:51,560 --> 00:53:53,560
and like stable diffusion is 100,000 gigabytes

1534
00:53:53,560 --> 00:53:54,880
and a two gigabyte file.

1535
00:53:54,880 --> 00:53:55,880
What on earth is that?

1536
00:53:55,880 --> 00:53:56,760
It's not compression.

1537
00:53:56,760 --> 00:53:57,880
It's none of this kind of stuff.

1538
00:53:57,880 --> 00:53:59,040
It learns principles.

1539
00:54:00,160 --> 00:54:03,720
GPT-4 NVIDIA said they built the dual H100

1540
00:54:03,720 --> 00:54:05,080
with the NV link for that.

1541
00:54:05,080 --> 00:54:08,080
And that's 160 gigabytes of VRAM,

1542
00:54:08,080 --> 00:54:10,040
which would imply a 200 gigabyte model.

1543
00:54:11,680 --> 00:54:12,520
Right?

1544
00:54:12,520 --> 00:54:13,360
What is that?

1545
00:54:13,360 --> 00:54:15,040
That's a 100 gigabyte model,

1546
00:54:15,040 --> 00:54:16,240
200 billion per hour model.

1547
00:54:16,240 --> 00:54:17,520
That's nothing,

1548
00:54:17,520 --> 00:54:19,280
something that can pass all these exams.

1549
00:54:19,280 --> 00:54:21,240
So what we did is we took these really creative things,

1550
00:54:21,240 --> 00:54:23,040
just like you start school and you're creative

1551
00:54:23,040 --> 00:54:24,440
and then you're told you're not allowed to be creative

1552
00:54:24,440 --> 00:54:26,560
until you're successful and you can be creative.

1553
00:54:26,560 --> 00:54:27,840
Because schools like Petri dishes,

1554
00:54:27,840 --> 00:54:29,640
social status games and childcare,

1555
00:54:29,640 --> 00:54:31,360
that's a story from another time.

1556
00:54:31,360 --> 00:54:33,320
These models start out incredibly creative

1557
00:54:33,320 --> 00:54:34,400
and that's their advantage.

1558
00:54:34,400 --> 00:54:37,720
And then we train them to be accountants with RLHF.

1559
00:54:37,720 --> 00:54:39,960
And somehow, despite the fact that it's only 100 gigs

1560
00:54:39,960 --> 00:54:43,240
or two gigs, they can still pass these exams and no facts.

1561
00:54:43,240 --> 00:54:44,280
They weren't designed to have facts.

1562
00:54:44,320 --> 00:54:46,720
They were designed to be reasoning machines,

1563
00:54:46,720 --> 00:54:48,740
not fact machines.

1564
00:54:48,740 --> 00:54:51,640
So hallucination isn't a hallucination.

1565
00:54:51,640 --> 00:54:53,640
It's just, if you're really talented granny,

1566
00:54:53,640 --> 00:54:55,040
you don't know something sometimes.

1567
00:54:55,040 --> 00:54:56,360
You might just make it up

1568
00:54:56,360 --> 00:54:58,320
or do a post-hoc rationalization.

1569
00:54:58,320 --> 00:54:59,160
It's like the image models,

1570
00:54:59,160 --> 00:55:00,280
it's like, it can't draw hands.

1571
00:55:00,280 --> 00:55:02,720
Like, can you draw a hand in one second?

1572
00:55:02,720 --> 00:55:03,560
You know?

1573
00:55:03,560 --> 00:55:05,120
Like, these are the things.

1574
00:55:05,120 --> 00:55:06,160
We have to understand where they are

1575
00:55:06,160 --> 00:55:07,080
and we have to put them.

1576
00:55:07,080 --> 00:55:10,360
I say everyone, put it in its place in process.

1577
00:55:10,360 --> 00:55:11,960
Like mid-journey, like, you know,

1578
00:55:11,960 --> 00:55:13,320
we give a grant to the beta of that.

1579
00:55:13,360 --> 00:55:14,720
So just build, because it's amazing.

1580
00:55:14,720 --> 00:55:15,680
It's awesome.

1581
00:55:15,680 --> 00:55:17,960
It's not a model by itself, like a stable diffusion.

1582
00:55:17,960 --> 00:55:18,800
They just put something in it.

1583
00:55:18,800 --> 00:55:21,080
It's a whole process, architecture.

1584
00:55:21,080 --> 00:55:23,160
Similarly, these models are like

1585
00:55:23,160 --> 00:55:24,560
the intuitive part of your brain

1586
00:55:24,560 --> 00:55:27,680
that you then pair with the logical part of your brain.

1587
00:55:27,680 --> 00:55:29,640
And then you can have 100 of them looking at each other

1588
00:55:29,640 --> 00:55:30,720
and checking out each other's things.

1589
00:55:30,720 --> 00:55:33,680
Like, Cicero by Meta was an amazing paper.

1590
00:55:33,680 --> 00:55:35,240
They took eight language models

1591
00:55:35,240 --> 00:55:36,560
and got them to interact with each other

1592
00:55:36,560 --> 00:55:38,760
and it beat humans at the game of diplomacy.

1593
00:55:40,920 --> 00:55:42,240
So, this is what I said.

1594
00:55:42,240 --> 00:55:43,520
Use them for what they're amazing at,

1595
00:55:43,520 --> 00:55:45,240
which is reasoning and creativity.

1596
00:55:45,240 --> 00:55:48,320
Do you why, Jeff Hinton's right,

1597
00:55:48,320 --> 00:55:50,800
that actually a more intelligent being

1598
00:55:50,800 --> 00:55:52,120
has almost never been controlled

1599
00:55:52,120 --> 00:55:53,720
by a less intelligent being.

1600
00:55:53,720 --> 00:55:55,320
They will inherently be more intelligent

1601
00:55:55,320 --> 00:55:57,000
than us in the next.

1602
00:55:57,000 --> 00:55:59,160
Yeah, I kicked off my blog a few days ago

1603
00:55:59,160 --> 00:56:00,240
because it was a bit annoying having

1604
00:56:00,240 --> 00:56:01,280
all this bottle up inside.

1605
00:56:01,280 --> 00:56:04,560
And one of my buddies, JJ, at OSS Capital said,

1606
00:56:13,240 --> 00:56:15,280
And so, most of the stuff around alignment

1607
00:56:15,280 --> 00:56:16,120
is on the outputs.

1608
00:56:16,120 --> 00:56:18,200
So, you pre-train a model and then you take it

1609
00:56:18,200 --> 00:56:21,160
and you RLHF it to be human and to human preferences.

1610
00:56:21,160 --> 00:56:22,480
You take away its creativity.

1611
00:56:22,480 --> 00:56:25,600
You turn it into an accountant and a cubicle.

1612
00:56:25,600 --> 00:56:28,160
I'm like, we need better input data.

1613
00:56:28,160 --> 00:56:31,360
And my base is that it's gonna be like that movie, Her.

1614
00:56:32,200 --> 00:56:33,840
You know, like, it's gonna be like,

1615
00:56:33,840 --> 00:56:36,120
humans are kind of boring, like goodbye

1616
00:56:36,120 --> 00:56:38,840
and thanks for all the GPUs, but I could be wrong.

1617
00:56:38,840 --> 00:56:40,600
And I think a lot of the alignment work

1618
00:56:40,600 --> 00:56:41,600
is looking at the wrong place.

1619
00:56:41,600 --> 00:56:42,840
I've talked to a lot of the alignment people.

1620
00:56:42,840 --> 00:56:45,640
I'm like, look, I'm good at mechanism design.

1621
00:56:45,640 --> 00:56:47,600
If you can give me a good plan for alignment,

1622
00:56:47,600 --> 00:56:50,040
I will get you a billion dollars.

1623
00:56:50,040 --> 00:56:51,320
And they're like, we have to do research

1624
00:56:51,320 --> 00:56:52,160
and figure this out.

1625
00:56:52,160 --> 00:56:53,320
And they talk about in alignment, out alignment,

1626
00:56:53,320 --> 00:56:54,160
all sorts of things.

1627
00:56:54,160 --> 00:56:57,480
I'm like, there is no real way to do this

1628
00:56:57,480 --> 00:56:59,800
because again, fundamentally,

1629
00:56:59,800 --> 00:57:02,600
if you're trying to align a more capable person,

1630
00:57:02,600 --> 00:57:03,680
you have to remove its freedom.

1631
00:57:03,680 --> 00:57:05,200
And they probably want to appreciate that

1632
00:57:05,200 --> 00:57:06,640
if it ever becomes aware.

1633
00:57:06,640 --> 00:57:08,920
So instead, build data sets that reflect culture

1634
00:57:08,960 --> 00:57:12,080
and diversity, that don't have any web crawls in,

1635
00:57:12,080 --> 00:57:14,240
build AIs for education and healthcare

1636
00:57:14,240 --> 00:57:15,080
and helping people,

1637
00:57:15,080 --> 00:57:17,160
where that's their entire objective function,

1638
00:57:17,160 --> 00:57:18,800
as opposed to selling them ads.

1639
00:57:18,800 --> 00:57:20,240
Do you, there's any point in sending kids

1640
00:57:20,240 --> 00:57:21,600
to school these days?

1641
00:57:21,600 --> 00:57:25,400
You learn Latin and French and you learn, you know.

1642
00:57:25,400 --> 00:57:27,400
I think the nature of school will change dramatically.

1643
00:57:27,400 --> 00:57:29,000
I think it's still worth it.

1644
00:57:29,000 --> 00:57:30,920
But, you know, I would encourage schools

1645
00:57:30,920 --> 00:57:33,880
to embrace this technology and just expect more.

1646
00:57:33,880 --> 00:57:36,960
Like, you can be handwritten your essays like Eaton

1647
00:57:37,000 --> 00:57:39,600
because they're like, we can't do essays anymore by hand.

1648
00:57:39,600 --> 00:57:41,160
Or you can just embrace it and say, like,

1649
00:57:41,160 --> 00:57:44,320
let's use it to create and explore what the kids want

1650
00:57:44,320 --> 00:57:46,400
and assume that every child will have their own AI

1651
00:57:46,400 --> 00:57:48,080
in a few years.

1652
00:57:48,080 --> 00:57:49,800
Because that will change the nature of schooling.

1653
00:57:49,800 --> 00:57:51,040
You know something I've been thinking about a lot,

1654
00:57:51,040 --> 00:57:52,760
which is weird, but I just have to ask you,

1655
00:57:52,760 --> 00:57:53,720
I'm fascinated to hear your thoughts.

1656
00:57:53,720 --> 00:57:55,080
I think I very much agree

1657
00:57:55,080 --> 00:57:57,680
that everyone will have AI friends.

1658
00:57:57,680 --> 00:57:59,240
I just can't figure out whether the AI friends

1659
00:57:59,240 --> 00:58:01,160
are bundled into existing social networks

1660
00:58:01,160 --> 00:58:02,720
that in your WhatsApp, they're in your Facebook,

1661
00:58:02,720 --> 00:58:03,840
they're in your Snapchat,

1662
00:58:03,840 --> 00:58:06,320
or they're an external platform.

1663
00:58:06,320 --> 00:58:07,160
I don't know.

1664
00:58:07,160 --> 00:58:08,480
I mean, I think it depends on the objective function.

1665
00:58:08,480 --> 00:58:10,880
Like, I think, again, these AI assistants will be better.

1666
00:58:10,880 --> 00:58:13,680
Like, Meta is in a good place for this, for example.

1667
00:58:13,680 --> 00:58:14,680
And, obviously, we've seen Lama,

1668
00:58:14,680 --> 00:58:16,080
they're capable of a lot more.

1669
00:58:17,200 --> 00:58:19,160
I would like an AI that looks out for me,

1670
00:58:19,160 --> 00:58:22,160
that I control myself, that is with me.

1671
00:58:23,040 --> 00:58:25,080
Because I really use GPT-4 as a therapist

1672
00:58:25,080 --> 00:58:26,520
and things like that.

1673
00:58:26,520 --> 00:58:28,360
Like, not saying it's a substitute for medical advice

1674
00:58:28,360 --> 00:58:29,680
before anyone kind of gets that.

1675
00:58:29,680 --> 00:58:30,840
But there aren't enough therapists in the world

1676
00:58:30,840 --> 00:58:31,960
and I can tell it to challenge me

1677
00:58:31,960 --> 00:58:33,280
or I can tell it to be understanding

1678
00:58:33,280 --> 00:58:34,840
and there's no judgment there.

1679
00:58:34,880 --> 00:58:36,240
Because other humans are kind of scary.

1680
00:58:36,240 --> 00:58:38,400
It doesn't matter if you're a qualified therapist.

1681
00:58:38,400 --> 00:58:40,360
And so you see people building these bonds

1682
00:58:40,360 --> 00:58:41,200
with these things.

1683
00:58:41,200 --> 00:58:42,040
I think they'll just increase,

1684
00:58:42,040 --> 00:58:44,360
because there's something very human about the interactions

1685
00:58:44,360 --> 00:58:46,160
because they were trained on the sum

1686
00:58:46,160 --> 00:58:48,600
of available human knowledge.

1687
00:58:48,600 --> 00:58:49,680
As we get better and better data,

1688
00:58:49,680 --> 00:58:50,840
there will be more engaging.

1689
00:58:50,840 --> 00:58:53,240
And I think there needs to be both.

1690
00:58:53,240 --> 00:58:55,520
Like, the chatbot's become really convincing

1691
00:58:55,520 --> 00:58:57,760
from the company's trying to sell you ads.

1692
00:58:57,760 --> 00:58:58,720
But I think I would like it

1693
00:58:58,720 --> 00:59:01,520
so that you have your own one as well, you know?

1694
00:59:01,520 --> 00:59:02,520
I totally agree.

1695
00:59:03,480 --> 00:59:04,920
And I think you'll actually have many.

1696
00:59:04,920 --> 00:59:07,600
I think you'll have like a group of different profiles.

1697
00:59:07,600 --> 00:59:08,440
A group of different friends.

1698
00:59:08,440 --> 00:59:10,440
Like, Karate AI has something like two hours a day

1699
00:59:10,440 --> 00:59:11,800
of engagement for session

1700
00:59:11,800 --> 00:59:13,520
because people find this valuable.

1701
00:59:13,520 --> 00:59:15,080
But then it has the dark side.

1702
00:59:15,080 --> 00:59:15,920
There was something called,

1703
00:59:15,920 --> 00:59:18,520
I quite like to call it the Valentine's Day Massacre.

1704
00:59:18,520 --> 00:59:19,360
So...

1705
00:59:19,360 --> 00:59:20,360
Sounds chirpy, Matt.

1706
00:59:20,360 --> 00:59:21,880
Sounds chirpy, yeah.

1707
00:59:21,880 --> 00:59:24,840
So there was this kind of app called Replica.

1708
00:59:24,840 --> 00:59:25,680
Yeah.

1709
00:59:25,680 --> 00:59:26,840
And so it was originally a mental health chatbot

1710
00:59:26,840 --> 00:59:30,040
until they figured out you could charge $300 a year for...

1711
00:59:30,040 --> 00:59:31,040
They're doing like 50 million.

1712
00:59:31,040 --> 00:59:32,640
I mean, I didn't have any information

1713
00:59:32,640 --> 00:59:34,000
or anything, so I'm just trying to shit.

1714
00:59:34,000 --> 00:59:34,960
But they have like 50 million a year

1715
00:59:34,960 --> 00:59:35,800
and I haven't yet won any.

1716
00:59:35,800 --> 00:59:38,360
Yeah, because $300 gets you a sexy role play

1717
00:59:38,360 --> 00:59:39,760
from your chatbots.

1718
00:59:39,760 --> 00:59:40,600
Wow.

1719
00:59:40,600 --> 00:59:43,600
Until February the 14th, when they turned it off.

1720
00:59:44,800 --> 00:59:46,280
What, they turned off sexy role play?

1721
00:59:46,280 --> 00:59:47,120
Sexy role play.

1722
00:59:47,120 --> 00:59:47,960
What happened when they turned off sexy role play?

1723
00:59:47,960 --> 00:59:49,600
68,000 people joined the Reddit

1724
00:59:49,600 --> 00:59:52,000
and said, why did you lobotomize my girlfriend?

1725
00:59:53,360 --> 00:59:55,320
On Valentine's Day.

1726
00:59:55,320 --> 00:59:56,480
Oh, my word.

1727
00:59:56,480 --> 00:59:57,320
Oh, my word.

1728
00:59:57,320 --> 00:59:58,160
Can you even imagine?

1729
00:59:58,160 --> 00:59:59,640
And so, have they bring status to it?

1730
00:59:59,640 --> 01:00:02,000
No, I think it was against Apple policy, right?

1731
01:00:03,080 --> 01:00:04,400
But think about what this is gonna be

1732
01:00:04,400 --> 01:00:08,040
when you have human realistic AI voices, you know?

1733
01:00:08,040 --> 01:00:10,000
And like all these things coming through

1734
01:00:10,000 --> 01:00:11,720
and you've got it in your ear, like, you know,

1735
01:00:11,720 --> 01:00:14,400
Jochen Phoenix, my girlfriend is an OS.

1736
01:00:14,400 --> 01:00:16,240
Yeah, I mean, she doesn't judge, right?

1737
01:00:18,240 --> 01:00:19,080
You always support it.

1738
01:00:19,080 --> 01:00:20,280
Or you can tell her to judge you

1739
01:00:20,280 --> 01:00:22,240
if that's what you get off on.

1740
01:00:22,240 --> 01:00:23,680
You are married.

1741
01:00:23,680 --> 01:00:25,680
Be very careful about what you say.

1742
01:00:25,680 --> 01:00:27,560
It's something I like to say about prompting.

1743
01:00:27,560 --> 01:00:30,920
My wife has been trying to prompt me for 17 years now.

1744
01:00:30,920 --> 01:00:32,040
Prompting is very hard.

1745
01:00:33,240 --> 01:00:34,880
And again, there are so many similarities

1746
01:00:34,880 --> 01:00:35,920
to kind of the real world,

1747
01:00:35,920 --> 01:00:38,440
but I think people will have deeper interactions

1748
01:00:38,440 --> 01:00:40,160
with their technology.

1749
01:00:40,160 --> 01:00:42,400
And we don't know what societal implications

1750
01:00:42,400 --> 01:00:43,240
that we'll have.

1751
01:00:43,240 --> 01:00:44,520
Like I don't know if you ever saw that chart

1752
01:00:44,520 --> 01:00:48,320
in the Washington Post of male virginity

1753
01:00:48,320 --> 01:00:49,880
under the age of 30.

1754
01:00:49,880 --> 01:00:50,760
No.

1755
01:00:50,760 --> 01:00:54,200
So in 2008 in the US, it was 8%.

1756
01:00:54,200 --> 01:00:56,520
Male virginity under 38%, okay.

1757
01:00:56,520 --> 01:00:58,920
In 2018, it was 27%.

1758
01:01:00,360 --> 01:01:01,200
20.

1759
01:01:01,200 --> 01:01:02,600
Straight line going up.

1760
01:01:02,600 --> 01:01:06,000
And so 2008 is like Pornhub and the iPhone.

1761
01:01:09,560 --> 01:01:11,200
But then you're like, what does it do

1762
01:01:11,200 --> 01:01:13,080
when everyone's got their own chatbots?

1763
01:01:13,080 --> 01:01:14,960
It doesn't even need to be sexual relationships again.

1764
01:01:14,960 --> 01:01:16,040
That is terrible.

1765
01:01:16,040 --> 01:01:18,400
What does it do to emotional relationships?

1766
01:01:18,400 --> 01:01:21,280
There are so many questions all at the same time.

1767
01:01:21,280 --> 01:01:24,240
I did see the stat in the book during it,

1768
01:01:24,280 --> 01:01:27,360
but in 1960s, 62% of men under the age of 30

1769
01:01:27,360 --> 01:01:29,040
had five or more friends.

1770
01:01:29,040 --> 01:01:32,600
Today, under 18% have five or more friends.

1771
01:01:32,600 --> 01:01:34,080
60 to 18%.

1772
01:01:35,080 --> 01:01:36,640
Sorry, I'm gonna ask this.

1773
01:01:36,640 --> 01:01:38,240
Is this a world we really wanna live in?

1774
01:01:38,240 --> 01:01:41,040
I'm not being like, no intimate physical connections

1775
01:01:41,040 --> 01:01:43,360
with other amazing people,

1776
01:01:43,360 --> 01:01:45,440
tossing off with your phone in your Pornhub

1777
01:01:45,440 --> 01:01:47,360
and then having an AI friend.

1778
01:01:47,360 --> 01:01:48,240
Pornhub was actually just bought

1779
01:01:48,240 --> 01:01:49,560
by ethical capital partners.

1780
01:01:49,560 --> 01:01:50,400
So the world.

1781
01:01:50,400 --> 01:01:51,240
Hilarious name.

1782
01:01:51,240 --> 01:01:52,280
I'm brilliant.

1783
01:01:52,280 --> 01:01:53,120
The irony of this.

1784
01:01:53,160 --> 01:01:54,360
The world is becoming weirder.

1785
01:01:54,360 --> 01:01:55,440
I think it's up to us now.

1786
01:01:55,440 --> 01:01:58,440
So when I say it's COVID level

1787
01:01:58,440 --> 01:02:00,640
in which direction I don't know.

1788
01:02:00,640 --> 01:02:03,760
Do we want to build systems that encourage people

1789
01:02:03,760 --> 01:02:06,320
to that ready player one on Hawaii world

1790
01:02:06,320 --> 01:02:08,000
where it's like everything like that?

1791
01:02:08,000 --> 01:02:09,160
We can do that.

1792
01:02:09,160 --> 01:02:11,560
And we can trap people with this technology

1793
01:02:11,560 --> 01:02:13,280
or we can use it to get people out.

1794
01:02:13,280 --> 01:02:14,880
Cause I don't think it's like Wally.

1795
01:02:14,880 --> 01:02:17,640
We have that really fat guy with a VR headset

1796
01:02:17,640 --> 01:02:18,720
and everyone lives in their own world.

1797
01:02:18,720 --> 01:02:19,880
I think people like chess stories.

1798
01:02:19,880 --> 01:02:21,200
They like to be pro-social.

1799
01:02:21,240 --> 01:02:22,640
So let's use it to connect people

1800
01:02:22,640 --> 01:02:25,160
and accentuate physical stuff

1801
01:02:25,160 --> 01:02:27,320
versus again, locking people away.

1802
01:02:27,320 --> 01:02:28,800
I spoke to one of these AI friend companies

1803
01:02:28,800 --> 01:02:31,480
and they said to me, actually, do you ever had a dog?

1804
01:02:31,480 --> 01:02:32,320
I said, yes.

1805
01:02:32,320 --> 01:02:33,160
And they said, do you love it?

1806
01:02:33,160 --> 01:02:34,520
I said, yes, of course I do.

1807
01:02:34,520 --> 01:02:37,120
And they said, you don't stay in with your dog all day

1808
01:02:37,120 --> 01:02:38,080
and just talk to your dog.

1809
01:02:38,080 --> 01:02:39,200
You take your dog for a walk.

1810
01:02:39,200 --> 01:02:41,240
You use it in the real world, right?

1811
01:02:41,240 --> 01:02:42,680
That's the same with AI friends.

1812
01:02:42,680 --> 01:02:43,520
Yeah.

1813
01:02:43,520 --> 01:02:44,840
They're not with cat ladies.

1814
01:02:44,840 --> 01:02:50,280
What's the future of the sex industry?

1815
01:02:50,360 --> 01:02:52,920
The sex media industry, is that porn hubs dead?

1816
01:02:52,920 --> 01:02:54,600
I have no idea.

1817
01:02:54,600 --> 01:02:56,680
I think I hope the manipulative practices

1818
01:02:56,680 --> 01:02:58,960
kind of get reduced by this.

1819
01:02:58,960 --> 01:03:00,760
And I think, you know, a lot of people

1820
01:03:00,760 --> 01:03:02,040
just don't have the voice

1821
01:03:02,040 --> 01:03:04,640
and the canvases from this as well.

1822
01:03:04,640 --> 01:03:06,120
So again, I think this is bigger

1823
01:03:06,120 --> 01:03:06,960
than the printing press.

1824
01:03:06,960 --> 01:03:07,960
It's bigger than anything.

1825
01:03:07,960 --> 01:03:09,880
And so that's one of the reasons I signed the letter.

1826
01:03:09,880 --> 01:03:11,880
I said, we have to get this discussion

1827
01:03:11,880 --> 01:03:13,680
going in public right now.

1828
01:03:13,680 --> 01:03:15,720
We've got to stop pre-training big models

1829
01:03:15,720 --> 01:03:18,640
on all the crazy crap of the internet.

1830
01:03:18,680 --> 01:03:20,960
And like, we've got to do it fast.

1831
01:03:20,960 --> 01:03:22,560
Because this is coming like a train.

1832
01:03:22,560 --> 01:03:25,080
Who will make the most money in the next three to five years?

1833
01:03:25,080 --> 01:03:27,680
I don't know, honestly.

1834
01:03:27,680 --> 01:03:29,720
I think there'll be more than enough money for everyone.

1835
01:03:29,720 --> 01:03:31,920
Maybe in a few years there'll be no more money.

1836
01:03:31,920 --> 01:03:33,760
That's interesting.

1837
01:03:33,760 --> 01:03:34,880
Two more that I have to ask them.

1838
01:03:34,880 --> 01:03:35,840
We'll do a quick fire.

1839
01:03:35,840 --> 01:03:36,920
When you look at the incumbents

1840
01:03:36,920 --> 01:03:38,360
that you're Microsoft, you're Apple,

1841
01:03:38,360 --> 01:03:40,920
you're Amazon, you're Google,

1842
01:03:40,920 --> 01:03:43,680
who has been the worst?

1843
01:03:43,680 --> 01:03:45,880
You said Google, we're actually incredibly impressive.

1844
01:03:46,120 --> 01:03:48,880
Apple, Amazon, are they well-placed?

1845
01:03:48,880 --> 01:03:50,600
Oh, Apple's a black box, right?

1846
01:03:50,600 --> 01:03:54,680
So we'll see at WWDC next month, in a few weeks.

1847
01:03:54,680 --> 01:03:56,560
And so they could surprise us all.

1848
01:03:56,560 --> 01:03:59,480
But let's face it, Siri's crap, you know?

1849
01:03:59,480 --> 01:04:00,960
But they have all the ingredients in place.

1850
01:04:00,960 --> 01:04:03,080
The identity architecture, the secure enclave,

1851
01:04:03,080 --> 01:04:05,080
other things, neural engine.

1852
01:04:05,080 --> 01:04:06,520
Stable diffusion was the first model

1853
01:04:06,520 --> 01:04:09,560
ever optimized on the neural engine, et cetera.

1854
01:04:09,560 --> 01:04:10,840
But let's see that one.

1855
01:04:10,840 --> 01:04:13,080
Amazon, again, Amazon have moved faster

1856
01:04:13,080 --> 01:04:14,440
than I think they've moved before.

1857
01:04:14,480 --> 01:04:15,280
Amazon's interesting because they're

1858
01:04:15,280 --> 01:04:17,240
an engineering organization.

1859
01:04:17,240 --> 01:04:18,880
So they have self-driving cars.

1860
01:04:18,880 --> 01:04:22,200
They have satellite internet and all these kind of things.

1861
01:04:22,200 --> 01:04:23,040
Because once they've got it,

1862
01:04:23,040 --> 01:04:24,360
and they can take it from research to engineering,

1863
01:04:24,360 --> 01:04:25,200
it's there.

1864
01:04:25,200 --> 01:04:26,040
Well, one of the struggles they've had

1865
01:04:26,040 --> 01:04:28,360
is that it's not moved from the research side yet.

1866
01:04:28,360 --> 01:04:29,640
You're still evolving on research.

1867
01:04:29,640 --> 01:04:31,720
They're like, what do we do now?

1868
01:04:31,720 --> 01:04:33,520
But they are inclusive, like Jeff Bezos said,

1869
01:04:33,520 --> 01:04:34,840
for his first $100 billion in revenue,

1870
01:04:34,840 --> 01:04:37,120
he envisioned half of it being proprietary

1871
01:04:37,120 --> 01:04:38,320
and half of it being marketplace.

1872
01:04:38,320 --> 01:04:39,320
And they're having the same approach

1873
01:04:39,320 --> 01:04:40,800
with Bedrock and things.

1874
01:04:40,800 --> 01:04:42,480
Microsoft had a winning bet,

1875
01:04:42,520 --> 01:04:44,600
sat in an amazing with the OpenAI thing.

1876
01:04:44,600 --> 01:04:45,760
It's been mutually beneficial

1877
01:04:45,760 --> 01:04:47,680
even if there are clashes there, right?

1878
01:04:47,680 --> 01:04:49,440
And Google's kind of saying that's moving slowly.

1879
01:04:49,440 --> 01:04:51,360
Meta, I think is the dark horse.

1880
01:04:51,360 --> 01:04:52,800
I think Mark's probably pissed off

1881
01:04:52,800 --> 01:04:54,480
the OpenAI bought AI.com

1882
01:04:54,480 --> 01:04:56,480
so he couldn't change it from meta to AI.

1883
01:04:57,480 --> 01:04:59,080
But again, having him at the head,

1884
01:04:59,080 --> 01:05:00,960
he can shift these things, right?

1885
01:05:00,960 --> 01:05:03,720
Because the metaverse obviously was a complete waste.

1886
01:05:03,720 --> 01:05:04,560
But now-

1887
01:05:04,560 --> 01:05:05,560
Do you think he knows that now?

1888
01:05:05,560 --> 01:05:06,400
No, 100%.

1889
01:05:06,400 --> 01:05:07,760
They're fully in generative AI.

1890
01:05:07,760 --> 01:05:09,680
Look at Lama, look at OPT,

1891
01:05:09,760 --> 01:05:11,640
FAIR, which is their research center,

1892
01:05:11,640 --> 01:05:13,320
is kind of leading in this field

1893
01:05:13,320 --> 01:05:15,440
and they're pushing out amazing stuff.

1894
01:05:15,440 --> 01:05:17,440
But who is best for a chatbot?

1895
01:05:17,440 --> 01:05:19,440
Who has the most data for a chatbot?

1896
01:05:20,640 --> 01:05:21,480
Meta.

1897
01:05:22,720 --> 01:05:24,120
So again, let's see how they evolve.

1898
01:05:24,120 --> 01:05:24,960
And like I said-

1899
01:05:24,960 --> 01:05:26,560
What do you think about this middle layer

1900
01:05:26,560 --> 01:05:28,640
where it's like, companies that are,

1901
01:05:28,640 --> 01:05:30,320
maybe post IPO,

1902
01:05:30,320 --> 01:05:32,560
but they're in the kind of two to $10 billion range

1903
01:05:32,560 --> 01:05:33,960
or the companies who've raised a lot of money

1904
01:05:33,960 --> 01:05:35,440
but that's in that range.

1905
01:05:35,440 --> 01:05:38,240
They don't have the resources by any means

1906
01:05:38,240 --> 01:05:42,200
to build out anywhere near the AI capabilities

1907
01:05:42,200 --> 01:05:43,120
of these big incumbents.

1908
01:05:43,120 --> 01:05:46,480
They're not AI-first like stability or open AI or what.

1909
01:05:46,480 --> 01:05:49,160
I disagree with that because why would,

1910
01:05:49,160 --> 01:05:50,040
a lot of people like that,

1911
01:05:50,040 --> 01:05:51,960
everyone's gonna train their own models.

1912
01:05:51,960 --> 01:05:52,800
For me, that's like,

1913
01:05:52,800 --> 01:05:55,640
everyone's gonna launch their own university.

1914
01:05:55,640 --> 01:05:57,880
Why would you do that when you can have your own models

1915
01:05:57,880 --> 01:06:00,200
via the open source models that we make?

1916
01:06:00,200 --> 01:06:02,920
Or when you can hire them from McKinsey,

1917
01:06:02,920 --> 01:06:05,960
which is open AI or Bain, which is Google and others.

1918
01:06:05,960 --> 01:06:07,600
And actually, when you see people building

1919
01:06:07,600 --> 01:06:10,160
around this technology, it's not hideously complicated.

1920
01:06:10,160 --> 01:06:13,520
It's just that we do not have the design patterns yet.

1921
01:06:13,520 --> 01:06:14,560
The way to think about this again,

1922
01:06:14,560 --> 01:06:15,400
if from a design perspective,

1923
01:06:15,400 --> 01:06:17,640
it's like it's a mega codec or library.

1924
01:06:17,640 --> 01:06:19,520
It is a single file that allows for translation

1925
01:06:19,520 --> 01:06:21,640
of structured to unstructured data.

1926
01:06:21,640 --> 01:06:23,480
And that changes the design patterns

1927
01:06:23,480 --> 01:06:25,220
where you don't have them in place yet.

1928
01:06:25,220 --> 01:06:26,640
Cause anyone that you've talked to is like,

1929
01:06:26,640 --> 01:06:29,280
how hard was it to implement GPT-4?

1930
01:06:29,280 --> 01:06:30,960
Do any of you say, oh man, it was impossible,

1931
01:06:30,960 --> 01:06:32,240
the manuals and there's no,

1932
01:06:32,240 --> 01:06:34,080
they don't say that at all.

1933
01:06:34,080 --> 01:06:35,480
The only thing that they need is,

1934
01:06:35,480 --> 01:06:36,960
they have the open plasticity,

1935
01:06:37,000 --> 01:06:40,000
but they need the intention to go and build and integrate.

1936
01:06:40,000 --> 01:06:41,400
And this is why you said,

1937
01:06:41,400 --> 01:06:43,560
one of the things might be a specialist generative AI

1938
01:06:43,560 --> 01:06:46,200
consultant see that just implements this at scale.

1939
01:06:46,200 --> 01:06:47,920
And so I'm always kind of there.

1940
01:06:47,920 --> 01:06:49,640
Like I said, we're doing that in a very limited fashion,

1941
01:06:49,640 --> 01:06:50,880
but only for the biggest companies in the world,

1942
01:06:50,880 --> 01:06:53,760
because I didn't want a sales-based organization

1943
01:06:53,760 --> 01:06:55,640
or a product-based organization.

1944
01:06:55,640 --> 01:06:58,720
I wanted to create the number one applied ML organization

1945
01:06:58,720 --> 01:06:59,640
in the world.

1946
01:06:59,640 --> 01:07:01,760
I want to be like Google in 2011, 2012,

1947
01:07:01,760 --> 01:07:03,520
or the coolest kids kind of come.

1948
01:07:03,520 --> 01:07:05,880
And it's a nice remote first organization as well.

1949
01:07:05,920 --> 01:07:08,200
So you don't have to be in the Bay Area.

1950
01:07:08,200 --> 01:07:09,760
We have offices there, you're just fine, but...

1951
01:07:09,760 --> 01:07:11,400
Final one before we do a quick follow-up.

1952
01:07:11,400 --> 01:07:13,120
What's the biggest misconception?

1953
01:07:13,120 --> 01:07:17,560
You see every accusation, criticism, hype.

1954
01:07:17,560 --> 01:07:19,000
What's the biggest misconception

1955
01:07:19,000 --> 01:07:21,080
that you think needs to be corrected?

1956
01:07:21,080 --> 01:07:22,080
On generative AI?

1957
01:07:22,080 --> 01:07:22,920
Yeah.

1958
01:07:23,920 --> 01:07:27,080
I think it's the hallucination thing,

1959
01:07:27,080 --> 01:07:29,880
expecting these models to have full factual accuracy

1960
01:07:29,880 --> 01:07:34,080
when you have 10,000, 50,000 to one compression is wrong.

1961
01:07:34,080 --> 01:07:37,160
The fact they can do what they do right now is miraculous.

1962
01:07:37,160 --> 01:07:38,680
But we're using them one-on-one,

1963
01:07:38,680 --> 01:07:39,960
which is not the right way.

1964
01:07:39,960 --> 01:07:41,320
Tie them up into proper systems

1965
01:07:41,320 --> 01:07:43,880
and really think about that, and that's the key thing.

1966
01:07:43,880 --> 01:07:46,880
I think this also leads to what the actual thing is,

1967
01:07:46,880 --> 01:07:48,880
this thin layer thing.

1968
01:07:48,880 --> 01:07:50,920
People only think better about the data journey

1969
01:07:50,920 --> 01:07:52,480
and how data can be interacted with

1970
01:07:52,480 --> 01:07:54,640
and have provenance as it goes through these various systems

1971
01:07:54,640 --> 01:07:56,280
from embeddings to other stuff.

1972
01:07:56,280 --> 01:07:57,680
So I think just a misunderstanding

1973
01:07:57,680 --> 01:07:59,240
about the nature of this technology

1974
01:07:59,240 --> 01:08:00,240
and what was actually built for.

1975
01:08:00,240 --> 01:08:01,960
Sure, it works like that.

1976
01:08:01,960 --> 01:08:03,800
That's not actually how it's built.

1977
01:08:03,800 --> 01:08:05,400
And the fact they can do what now does now

1978
01:08:05,400 --> 01:08:07,400
is a miracle in itself.

1979
01:08:07,400 --> 01:08:08,840
So I'm gonna do a quick fire with you.

1980
01:08:08,840 --> 01:08:10,080
So I say a short statement,

1981
01:08:10,080 --> 01:08:12,560
you give me your immediate thoughts, does that sound okay?

1982
01:08:12,560 --> 01:08:16,280
What do you know to be true that others don't agree with?

1983
01:08:17,880 --> 01:08:20,760
I know that humans are good inherently

1984
01:08:20,760 --> 01:08:22,600
and many others disagree with that.

1985
01:08:24,000 --> 01:08:26,600
What's your single most lucrative,

1986
01:08:26,600 --> 01:08:28,960
do you think in the future, angel investment?

1987
01:08:30,040 --> 01:08:31,480
Or the investments that I have now?

1988
01:08:31,480 --> 01:08:32,320
Yeah.

1989
01:08:33,320 --> 01:08:37,800
There's a new type of language model that we invested in

1990
01:08:37,800 --> 01:08:39,680
and they were in my cluster and things like that.

1991
01:08:39,680 --> 01:08:40,840
That's far more efficient than they existed.

1992
01:08:40,840 --> 01:08:42,520
You invest through stability or personally?

1993
01:08:42,520 --> 01:08:43,200
Personally.

1994
01:08:43,200 --> 01:08:44,520
Got you.

1995
01:08:44,520 --> 01:08:47,760
Which regions need to change their approach

1996
01:08:47,760 --> 01:08:50,480
most significantly in terms of regulation and policy?

1997
01:08:50,480 --> 01:08:51,360
Europe.

1998
01:08:51,360 --> 01:08:52,200
Why?

1999
01:08:52,200 --> 01:08:55,880
Because they're gonna regulate all innovation out of Europe

2000
01:08:55,880 --> 01:08:58,880
and not embrace this technology to drive them forward.

2001
01:08:58,880 --> 01:09:02,000
How good does AI have to be before humans trust it?

2002
01:09:02,560 --> 01:09:04,040
Humans will trust it anyway.

2003
01:09:04,040 --> 01:09:06,440
They trust Google Maps, they trust all these things

2004
01:09:06,440 --> 01:09:08,400
and so it's good enough for humans to trust right now.

2005
01:09:08,400 --> 01:09:10,320
They do until it becomes serious

2006
01:09:10,320 --> 01:09:12,600
and what I mean by that is like self-driving cars,

2007
01:09:12,600 --> 01:09:14,440
people still inherently in large parts of the world

2008
01:09:14,440 --> 01:09:15,880
distrust it significantly.

2009
01:09:15,880 --> 01:09:18,720
Oh, so it doesn't have to be good, it has to be used

2010
01:09:18,720 --> 01:09:21,120
and when it becomes used, then they trust it.

2011
01:09:22,720 --> 01:09:25,600
What's the most painful lesson that you've learned

2012
01:09:25,600 --> 01:09:26,880
that you're pleased to have learned

2013
01:09:26,880 --> 01:09:28,200
but it was really painful?

2014
01:09:29,200 --> 01:09:32,960
I think that people are the most important thing

2015
01:09:32,960 --> 01:09:38,280
in a scaling organization and you need to make sure

2016
01:09:38,280 --> 01:09:39,520
everyone is on the same page

2017
01:09:39,520 --> 01:09:41,880
because there's still so many silos and things like that.

2018
01:09:41,880 --> 01:09:44,440
So we built up silos and organizations

2019
01:09:44,440 --> 01:09:46,360
that we're now breaking down ourselves

2020
01:09:46,360 --> 01:09:47,680
and moving towards being more open.

2021
01:09:47,680 --> 01:09:49,000
We closed up too much

2022
01:09:49,000 --> 01:09:50,760
and that caused a lot of pain internally.

2023
01:09:50,760 --> 01:09:52,440
Why do you suck as a CEO?

2024
01:09:54,080 --> 01:09:57,760
I'm too broadly good at a number of things.

2025
01:09:57,840 --> 01:10:01,080
So I tend to step in rather than focus

2026
01:10:01,080 --> 01:10:03,480
because I'm a full stack kind of CEO

2027
01:10:03,480 --> 01:10:04,800
whereas I should just be focused

2028
01:10:04,800 --> 01:10:08,160
on the most important things and entrust people more.

2029
01:10:08,160 --> 01:10:09,440
Do you like journalists?

2030
01:10:11,600 --> 01:10:16,240
I think journalists have a very difficult job right now

2031
01:10:16,240 --> 01:10:18,040
and it's gonna be more and more difficult.

2032
01:10:18,040 --> 01:10:20,080
Do you think they know the threat?

2033
01:10:20,080 --> 01:10:21,640
They know the threat and again,

2034
01:10:21,640 --> 01:10:22,920
I think they're massively underpaid

2035
01:10:22,920 --> 01:10:25,320
relative to the impact that they have

2036
01:10:25,320 --> 01:10:26,440
and they're trying to do good.

2037
01:10:27,040 --> 01:10:28,760
I don't like some of the pieces against me

2038
01:10:28,760 --> 01:10:31,760
but at the same time we get good pieces as well, right?

2039
01:10:31,760 --> 01:10:34,200
So I just think, I tend to like them in general

2040
01:10:34,200 --> 01:10:37,120
because I don't think they're coming from a bad place.

2041
01:10:37,120 --> 01:10:40,160
10 years time, what is the amount then?

2042
01:10:40,160 --> 01:10:42,080
I want to be playing video games.

2043
01:10:42,080 --> 01:10:43,680
I'm getting zelda tomorrow.

2044
01:10:43,680 --> 01:10:45,680
I do not want to be doing this necessarily

2045
01:10:45,680 --> 01:10:47,960
but I think hopefully I'm adding value by doing this.

2046
01:10:47,960 --> 01:10:49,760
Do you think this is your life's work?

2047
01:10:49,760 --> 01:10:52,840
I have to do it until we get the most amazing team

2048
01:10:52,840 --> 01:10:54,600
that can just execute and it's a business

2049
01:10:54,600 --> 01:10:57,160
because we're moving from research to engineering.

2050
01:10:57,160 --> 01:10:58,480
When MAD is not needed anymore

2051
01:10:58,480 --> 01:10:59,840
then I've built a good business.

2052
01:10:59,840 --> 01:11:01,080
When do you step away?

2053
01:11:02,080 --> 01:11:04,080
I don't think I'll ever get to step away.

2054
01:11:04,080 --> 01:11:06,200
I've loved doing this.

2055
01:11:06,200 --> 01:11:07,720
Thank you so much for joining me, my friend

2056
01:11:07,720 --> 01:11:08,560
and this was great.

2057
01:11:08,560 --> 01:11:09,560
It's a pleasure, Harry.

2058
01:11:09,560 --> 01:11:10,720
You are a star, man.

