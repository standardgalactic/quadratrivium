start	end	text
0	1600	I think this is bigger than the printing press.
1600	2600	It's bigger than anything.
2640	4560	And so that's one of the reasons I signed the letter.
4560	7880	I said, we have to get this discussion going in public right now.
8360	12440	We've got to stop pre-training big models on all the crazy crap of the internet.
13280	17040	And like, we've got to do it fast because this is coming like a train.
18080	19840	And that I am so excited for this.
19840	23720	I heard so many great things from specifically Ashton who helped with questions
23720	25120	and then also Dan Rose.
25120	26760	So thank you so much for joining me today.
26760	27640	It's my pleasure.
27640	29840	Now, I want to start with a little bit on you.
29840	31680	You moved around a little bit in your childhood.
32000	34240	Take me back to the childhood, the moving around.
34240	37400	And it's a weird commonality I found with the most talented founders.
37680	38840	They all moved around.
38840	42000	So take me to that and how it impacted your mindset.
42280	45440	So I was born in Jordan, grew up in Bangladesh, came to the UK.
46080	47480	Yeah, didn't move around that much.
47480	50360	But with my father a bit as he lectured in various places.
50920	55920	And it was always a bit of a struggle fitting in, but then you learn to adapt.
56280	58520	You learn to adapt to new scenarios, new environments.
58520	61200	Like, oh, don't speak the language, kind of what's happening.
61200	63080	Let's learn and let's move on from there.
63160	65520	I think it gave me a bit of appreciation of the world as well,
65880	68120	because we stick in our monocultures very often.
68120	69840	Like I'd only ever been to Silicon Valley.
69840	72280	I should have been the barrier once before last October.
72920	76280	And so this whole tech monoculture has been a bit of a shock to me.
76880	79920	And I'm like, there's more to the world than that.
80000	82240	So this is some interesting things around that.
82440	87160	Talk to me hedge funds first and then then what happened?
87320	88480	We mentioned it a little bit before.
88480	89320	Why did you make the move?
89760	93000	So actually, I was an enterprise developer in my gap here at Metaswitch
93000	94680	in the UK doing voice over IP.
94680	97040	Metaswitch. This is Chris Maz's company.
97040	99560	Yes. So I took my gap here and was like,
99560	101000	I might as well be enterprise programmer.
101000	102200	I didn't know what that would be like.
102200	103880	This was before GitHub and everything.
103880	107040	So we had subversion, you know, kids these days have it so easy.
108320	109760	And then I was like, what do I do now?
109760	114400	And so I became a VC analyst at Oxford Capital Partners with the MOTS there.
114760	115560	And that was a lot of fun.
115560	116280	They were fantastic.
116280	117560	And then I was like, I want to do movies.
117560	119320	So I became a movie reviewer.
119320	122000	So I did the Rain Dance Film Festival, British Independent Film Awards.
122400	124120	And they were just popping around doing random things
124120	127120	and then accidentally became a hedge fund manager.
127120	129760	Why did you move from hedge funds to startups?
130200	133040	So with the hedge funds, so I joined Pictay Asset Management
133040	135600	and then like the CIO left and there was like this fund
135600	137960	and I got to be a portfolio manager when I was 23.
138400	141560	And so I grew my beard to look a bit older and it's coming.
142000	142800	Get the clip on, Harry.
142800	145360	I would. I can't do the moustache, but I still wear the glasses
145400	147680	when I just need to try extra hard to force it out.
147680	150520	Right. And so I did that for a number of years
150520	152960	and you know, reasonably successful, made lots of people money.
153560	155640	Not so much myself because I was too young.
155640	157840	Again, they're like, you're too young to be a fun one.
157840	159840	And then my son was diagnosed with autism and I quit.
160040	163520	And because they said there was no cure, no treatment, no information.
163520	164840	I was like, I'm a hedge fund manager.
164840	166640	I can deconstruct things.
166640	170000	And so built an AI team and then did a literature analysis
170000	173200	of all the autism literature to try and figure out the commonalities
173200	174600	and then drug repurposing.
174640	177040	So focusing on GABA, glutamate balance in the brain.
177360	179640	GABA is what you get when you pop a valium.
179640	182160	It calms you down and glutamate excites you, you know.
182160	186440	And so in kids and people with ASD, it's like there's too much noise going on.
186440	188360	It's like when you're tapping your leg and you can't focus.
188760	190960	And so that's why you get this sensitivity.
190960	193080	Sometimes they can't speak like my son.
193080	195200	And so it was like mechanisms to bring that down
195840	197920	that then allowed to have applied behavioral analysis
197920	199720	and these other things to reconstruct his speech.
199720	201920	And then he went to mainstream school, which is pretty cool.
201920	202880	It's unbelievable.
202880	206440	I heard you said on another podcast and I was astounded and inspired by it.
206720	208920	We mentioned before, you know, my mother's got MS.
209320	214320	And I hate the doomsday only version of kind of AI in the future of GBT.
214960	218000	You said to me before about its impact on health and MS
218000	219880	in particular and other conditions.
220400	223560	How can it be so transformatively to solve some of the world's
223560	225520	most challenging chronic conditions?
225520	227880	So I think a large part of our problem is that we can't scale
227880	231120	because information flow is so limited as we write these things down.
231120	232720	Like you can never capture all of that.
233000	237160	So anyone who's had a loved one that has one of these conditions knows
237160	239400	how difficult it is because you go from specialist to specialist
239400	241880	to specialist and you try to build that mental map.
241880	243920	And we're so lucky that we have so much access.
244400	247120	But why isn't it that we can't just push about and see every clinical trial
247120	248960	and a deconstruction of all those and things?
249320	252640	What if you had a thousand GPT-4s organizing all that knowledge
253160	255400	and then make it available to everyone?
255400	259680	So you can see the exact potential mechanisms that are which MS works
259920	264080	and all the potential food, other things that work with that.
264080	267400	So as you try different things with your family member,
267800	270840	you can see, well, she reacted this way to the food or this way to this medicine.
271120	274040	And it is a more holistic thing because you can have personalized medicine
274040	276480	versus one specialist for a thousand people.
276960	281120	You can have a thousand GPT-4s or equivalents or med palm twos for you.
281720	285040	So we need to organize all this knowledge and then use these language
285040	288040	models and others to make it accessible to you.
288480	292000	I'm really naive and basic in terms of my thinking,
292000	294720	which is why I'm a venture capitalist.
294720	297640	But my question is, what do we need to do to get to that state?
297840	300200	When we look at the data needed from the individuals,
300200	305120	the data, the data, the GPT's need, how we make the models work most efficiently?
305120	307920	So first, we don't have to have that data for individuals.
307920	310320	We had Galactica as a scientific language model,
310320	312960	but now we have Med Palm 2 that exceeds doctor level.
312960	315120	So that was a Google announcement yesterday.
315160	319760	We have AIs that can understand articles better,
319760	321640	as good as doctors, shall we say now?
321640	324240	So we can scale that because why do you need one when you have a thousand?
324240	328280	So we take the existing generalized knowledge and all the hypotheticals
328280	331920	and we bring that together into an integrated common system available to everyone
331920	334280	because the building blocks are nearly here for that.
334280	335840	Then you can personalize it later.
335840	339560	And again, there are regulations and things around that to how you're,
339560	342640	again, how we treat our loved ones and other things like that.
342640	344520	The first thing is let's get all the knowledge in one place
344520	347120	and make it organized and useful.
347120	350480	And so I think we're at that point now where the language models have just hit that point
350480	353200	that we can organize all of the world's Alzheimer's knowledge,
353200	356360	longevity knowledge, autism knowledge, MS knowledge.
356360	358800	And you can just type and it can say, this is the source.
358800	359680	This is what it looks like.
359680	361160	These are some hypotheticals.
361160	363360	This is what we know, what we know we don't know,
363360	365560	what we think we might know, et cetera.
365560	368760	And then it can learn about you and your queries
368760	372360	because this is the other thing about lots of the language model things we've seen right now.
372400	374800	They are one-to-one goldfish memory.
374800	376480	The next step is one-to-one.
376480	379680	It remembers what you're asking for, like a cookie or an embedding.
379680	382280	And then it's you plus a thousand of these language models
382280	385920	all going and doing your bidding, the agent-based kind of thing.
385920	388280	Does this get around the incentive problem in healthcare?
388280	390680	And what I mean by the incentive problem in healthcare is I'm sure you know there are
390680	393800	a lot of diseases actually where it doesn't make kind of economic sense
393800	397920	for a lot of pharmaceutical providers to chase research, to chase treatments
397920	400480	because it's not a big enough market, because it's not,
400520	402080	because it's six dollar treatment.
402080	404760	Does this solve for that economic misalignment?
404760	406960	I think it can help a lot with that economic misalignment
406960	410280	because then you have an authoritative source where we can all come together
410280	413080	and build that can analyze these things.
413080	415600	Because there's this concept of agiddicity.
415600	419600	A thousand coins tossed in a row is the same as a thousand coins tossed at once.
419600	423120	And because we're so limited in our information in our medical system,
423120	425240	like you know I just had my key management,
425240	427360	so I had to answer 40 minutes of questions.
427360	428320	At least, Mo, have you done this?
428320	430280	It's stupid, right?
430280	432000	We're all treated the same.
432000	437200	I think 10% of people have a cytochrome P450 mutation in their liver,
437200	439440	which means they metabolize drugs fast quicker.
439440	443960	So if you metabolize code, it turns into morphine, or fentanyl kills you.
443960	445440	But that's a very basic genetic test,
445440	448200	yet we give everyone 500 milligrams of the same thing.
448200	451840	With my son, a micro dose of 5 milligrams of clonazepam,
451840	453600	which is used for anxiety disorder,
453600	455640	worked with a neurologist, allows him to sing.
455640	457160	The standard dose is a thousand milligrams,
457160	459000	so they can only prescribe it at a thousand.
459040	463360	But that is a $6 a year treatment that affects his GABA glutamate balance.
463360	465720	But only for his specific type of ASD,
465720	468640	which is only 7% of all kids with ASD.
468640	473160	But why would that be in a pharmaceutical company's interest?
473160	477960	You know, because how are they going to make money off a $6 a year treatment?
477960	480080	Well, how many people have ASD?
480080	482000	It's 1 in 60.
482000	484280	Okay, it's 1 in 60, so you've got a million people in the UK?
484280	484920	Yes.
484920	487840	So you've got a $6 million.
487880	488760	Yeah, it's not great.
488760	490040	Exactly, it's not great.
490040	492760	I mean, it's like we know the benefits of vitamin D, right?
492760	494560	But we still don't prescribe that at scale,
494560	496000	and so many people are deficient.
496000	497160	I mean, all these things.
497160	499440	The joys of doing what I do is going on schedule with a final one,
499440	502200	and then we will kind of retain some form of normality of schedule.
502200	504800	What is the future of healthcare systems that you think
504800	506720	with GPT models operating in this way?
506720	508960	I think that you can change the nature of a doctor,
508960	511000	because a lot of the stuff is kind of very basic.
511000	512720	I think, you know, you had Babylon Health and others trying,
512720	515080	that chatbot, it wasn't ready, now you've got this.
515080	517440	Everyone should have their own AIs looking out for their own health,
517440	519080	with that objective function.
519080	521000	You know, and then the nature of a doctor becomes different
521000	524200	in terms of they have more rich information about an individual,
524200	526920	while it being preserved in a private manner.
526920	528800	I think what you have is you have things like
528800	530440	processes and procedures improving,
530440	533920	like wound care, for example, and then NHS.
533920	536280	If you are injured as an elderly person,
536280	537680	and your wounds aren't treated properly,
537680	540480	and more likely to die by a factor of eight times,
540480	542480	being able to monitor those types of things
542480	545040	with this information set means you're eight times as likely,
545040	547320	and then you have far more efficiency around that.
547320	549600	So the information density around healthcare improves,
549600	552760	which means that then our own healthcare improves.
552760	555160	We all have access to as much knowledge as we want to,
555160	557560	within our own context, and so do our providers
557560	558560	and the people that help us.
558560	560960	How do we think about open source versus closed source
560960	562320	human healthcare data?
562320	564640	Because like, obviously for us all to benefit as one,
564640	566680	you know, MS sufferers around the world need to submit
566680	569680	their data around responses to certain treatments.
569680	571840	Yeah, so I think the wonderful thing about these models
571840	572800	is they're few-shot learners,
572800	574800	so they don't need to have much information.
574800	576320	As I was in the classical big data problem,
576320	578440	HDR UK has been one of the pioneers here
578440	581160	with the UK Biobank, Federated Learning, and others.
581160	585520	And there are kind of, with FL7, HLIR, and other standards
585520	586720	being built around this to allow
586720	588440	for full federated learning.
588440	591880	If you have open source language models
591880	594040	that are fully auditable, I call them organic
594040	595480	free range models, the ones we're building,
595480	598560	with no web script data, those can sit on device,
598560	601400	like Google SDA announced POM2.
601400	603720	The smallest POM2 model is 400 million parameters.
603720	605520	It works on your Google Pixel phone.
605520	607360	You don't need giant models anymore.
607360	610120	And then that model can just share the specific information
610120	613400	that preserves your privacy with the bigger thing.
613400	615600	And then it can take from that global knowledge base as well.
615600	619000	So you'll have big global models on device models.
619000	620520	And I think open works for that
620520	622960	because you don't need to have all the data open.
622960	624680	You just need to know that Harry is old enough
624680	626200	to have a drink, not that.
626200	627720	All the details about Harry, his birthplace,
627720	628640	and everything like that.
628640	630520	He's old enough, he's just not allowed to.
630520	633120	He gets parted all the time, yeah.
634000	635760	Were you impressed by the Google event yesterday?
635760	636880	No, I think it was impressive.
636880	639560	I said in February, when all this thing was going on,
639560	642960	like, come on, Google will be one of the main winners here.
642960	645200	They have the LLMs, they have the hardware.
645200	647760	You do know you're the only person who said that on the show.
647760	648960	And I've asked many.
648960	651640	And they've all said that Google are the laggards.
651640	653480	It just takes a bit of time to move the ship, right?
653480	656000	And so they've done massive organizational changes
656000	657120	and other things.
657120	660320	But I can tell you, TPU is on the most scalable architecture.
660520	664080	We have zero failure rate with our TPU language model training.
664080	666480	Whereas with GPUs, it's like there's
666480	669400	an ECC error, why a solar flare?
669400	672280	Run failed because the sun is angry with us and stuff like that.
672280	673720	So when you've got the full stack,
673720	675240	and you have all that talent in Google,
675240	677720	the question is, how do you make it organized, right?
677720	679000	And so they had to have a story.
679000	680600	Google did something called Pro-Taristottle,
680600	682160	where they analyzed what made the best teams
682160	683720	versus the worst teams at Google.
683720	685080	And it came down to shared narrative
685080	686800	and psychological safety.
686800	689480	People at Google were scared over the last few years,
689520	691080	because it came this weird monoculture.
691080	693040	But now everyone has a shared narrative of,
693040	694920	let's build the best language models,
694920	696800	and now there's an increase in amount of psychological safety
696800	697960	being able to speak to things,
697960	700600	the walls being brought down between deep mind and brain.
700600	703360	And so I think you'll see them continuously improving.
703360	704480	Well, then that does mean,
704480	706080	if you're a proprietary language model company,
706080	709000	how are you going to compete with that vehemence?
709000	713560	The deep mind desegregation or unification
713560	716160	was supposed to, of course, have a lot of friction
716160	718920	and be a negative press reported.
718920	719840	She disagreed with that.
719840	723080	Of course, it is a lot of replicated jobs.
723080	725040	There was brain and mind,
725040	726920	and now they're kind of brought together.
726920	729080	And it's a very different management style
729080	729920	and other things.
729920	731480	These things are never easy.
731480	734680	But this is why you saw palm 540 billion parameters
734680	736400	and that you had deep mind
736400	738880	with 67 billion parameter and chiller,
738880	741520	which is just train more as opposed to more parameters.
741520	744280	You look at palm two as a combination of both.
744280	747720	And so it's trained for far more on far better data.
747720	750560	And then that means it's only a fraction of the size,
750560	751840	like 14 billion parameters
751840	755760	is one of the test comparator models versus the 540 and 67.
755760	757920	So you can start to see this fusion of ideas,
757920	758840	even if the teams,
758840	761960	you cannot integrate two big teams like that instantly.
761960	764360	Shared narrative, psychological safety,
764360	766160	two of the biggest contributors.
766160	768400	To now running stability,
768400	770440	how do you think about integrating those two?
770440	771680	So we've got the shared narrative.
771680	772680	We're going to build the foundation
772680	774120	to activate humanity's potential
774120	776200	and then the motors make people happier.
776200	777480	But it's been a learning process,
777480	780480	the year ago, we're basically a mom and pop shop in some ways.
780480	782000	My wife and I were working at it,
782000	784400	like had lots of meetings out of our like sitting room
784400	786160	and things because the office didn't have wifi
786160	787240	and all sorts.
787240	790040	Now it's like growing up, we're 170 people,
790040	791320	we're going global,
791320	793320	we'll have stabilities in every country.
793320	795360	And then actually we're going multinational.
795360	796280	And that's difficult.
796280	798520	So we really try to put in processes in place,
798520	800240	but it's not easy.
800240	801320	Part of this is like,
801320	804560	we went close source on a bunch of stuff like dream studio.
804560	806360	I'm open sourcing everything now.
806360	807200	From next week,
807200	809240	we're going to build our language models in the open
809240	810840	and share what works and what doesn't work.
810840	811880	Why?
811880	814200	Because I think this is part of the shared narrative.
814200	815560	Someone needs to be open
815560	817320	and share what's going on under the hood.
817320	820760	And again, it's like, it should be opened by default
820760	824880	because the value is not in any proprietary models or data.
824880	827520	We're going to build open models that are auditable.
827520	829000	Even if it has licensed data in it,
829000	831680	you can see every single piece, free range organic models.
831680	832720	Because that's what the world needs
832720	835200	for all the private regulated and other data in the world.
835200	838360	This is a completely different time to proprietary models.
838360	841400	Because you can only send so much of your 20 VC data
841400	842760	to open AI.
842760	844600	And I think you need both of those.
844600	846680	So why can I only send so much?
846680	848760	Because you are a regulated company.
848760	851240	And so you need to make sure they're completely compliant.
851240	854400	If you have an option of having a stable chat model,
854400	856360	which will be announced in the future,
856360	858520	that you own completely trained in your own cloud
858520	860080	or on-prem or on-device,
860080	861360	and then also using GPT-4,
861360	863200	that's the best of both worlds.
863200	864480	Because then you don't have to deal with that.
864520	867400	Healthcare data needs to, again, be owned by the individual.
867400	869160	And so those models need to be owned.
869160	870160	And they need to be transparent.
870160	871920	They can't be black boxes.
871920	873920	Governments will not run on black boxes.
875360	876440	We're going to get to this later.
876440	877840	I do want to touch on something.
877840	879520	We had a great chat before this.
879520	881400	And you said a brilliant quote, and I want to get it right,
881400	884200	but you said the .ai bubble is bigger than ever,
884200	886200	and it will be the biggest shit show.
886200	887200	Yeah.
887200	888200	End quote.
889200	890760	Which I actually took and tweeted, by the way.
890760	891600	Thank you.
891600	892920	It could be some gratitude.
892920	894160	I thought if you saw it, I would have been like,
894160	895360	ah, this guy took my tweet.
895360	896360	Thank you.
896360	901080	And what did you mean by the biggest bubble ever
901080	902360	and the biggest shit show?
902360	904560	Oh, I mean, like the .com bubble,
904560	905920	we've seen all these bubbles happen.
905920	908240	You know, you had hundreds of billions into Web 3,
908240	910040	and then developers got paid millions.
910040	913040	Already, there are certain Chinese companies
913040	915600	paying $1.2 million salaries for PhDs.
916760	918160	It's already getting a bit insane.
918160	919760	There are remnants of that.
919800	923280	The amount of money relative to the amount of opportunity
923280	925240	within the sector is just completely misaligned.
925240	927400	Like my time analysis is that
927400	929720	1,000 companies spend $10 million in the next year,
929720	932600	100 companies spend $100, and 10 companies spend $1 billion.
932600	934360	Like PWC just announcing they'll spend $1 billion
934360	935680	over the next three years.
935680	938440	And that's a currency firm, you know?
938440	939280	Where's that going to go?
939280	940120	They don't know.
940120	941080	Nobody knows.
941080	943280	And so multiple of that will be allocated to this
943280	947160	as the only growth theme in the entire market
947160	948880	against a backdrop of rising rates,
948920	950920	real estate crashing, et cetera.
950920	953720	So the amount of capacity versus the amount
953720	956120	and whale and wall of money
956120	957320	into something that's growing faster
957320	960800	than anything we've ever seen is completely mismatched.
960800	962360	And what will that cause?
962360	964280	Like already you're seeing GitHub stars leading
964280	966120	to $100 million funding rounds
966120	968400	with zero attraction and zero business model.
968400	970280	Like stability, we actually have a business model.
970280	972920	And it's a good business model because I designed it.
972920	975200	But other things like money will go everywhere
975200	977800	and any expertise will get bit up for this space
977840	979600	because it means that projects will get funded
979600	981160	that maybe wouldn't have done,
981160	983520	but are exploratory generally and over funding.
983520	986240	I think it starts good for the space,
986240	988640	but then it gets bad for the space
988640	990640	because you see the raccoons and Scheister
990640	992000	start to come in here.
992000	994160	You start to see like malformed things
994160	995240	where there's a race dynamic,
995240	997160	where everyone's trying to build their own models
997160	1000400	and doing all sorts and massive economic waste.
1000400	1003000	And you see a distraction from what we need to do now,
1003000	1003840	which is this chaos.
1003840	1005720	So we need to standardize some things.
1005720	1007880	We need to feed these models better data and other stuff.
1007880	1010040	And that's why we're moving so hard at stability.
1010040	1011880	There should be no more web script data in here.
1011880	1014480	There should be national data sets that are good quality
1014480	1016040	to feed these free range organic models
1016040	1018720	and national and proprietary models and others.
1018720	1020480	And so that's why, and the reasons I signed that letter
1020480	1022200	because I think there's a six month pause
1022200	1023640	to get all of our shit together
1024560	1026920	before things go completely insane.
1026920	1028240	And next year, this is everywhere
1028240	1030160	and everyone's investing in everything.
1030160	1033240	And it's just absolute chaos.
1033240	1035280	You kind of unpack so much to me
1035280	1036920	that I want to kind of go one by one.
1036920	1038800	You said about kind of national data sets.
1038800	1042360	Why national data sets versus super national data sets?
1042360	1043440	Because like, I'll give you an example.
1043440	1044840	There was a team that did Japan diffusion,
1044840	1046200	including some of our staff.
1046200	1047480	So we took stable diffusion
1047480	1048840	and then changed the language model.
1048840	1051120	Because when you typed in salary man in stable diffusion,
1051120	1052360	it was a very happy man.
1052360	1054720	Whereas in Japan, the salary man's a very sad man.
1054720	1058000	You know, local context is important in these models
1058000	1059440	because we're going to outsource more and more
1059440	1061600	of our thinking and minds to it.
1061600	1063200	And so do you want to have a British model
1063200	1065400	doing all the models to be Palo Alto?
1065400	1067280	You know, like it's a sparkling wine
1067280	1068920	has to be from the Champaign region.
1068920	1072080	Like is the only real foundation model AI from Palo Alto?
1072080	1073240	Like it's not a good thing.
1073240	1074680	We need national models.
1074680	1075760	It's a national infrastructure
1075760	1080440	because there is no doubt this is more important than 5G.
1080440	1082200	These models are like really talented grads
1082200	1083960	that occasionally go off their meds, you know?
1083960	1086360	And you want to have the ones from Oxford Imperial
1086360	1089040	and Edinburgh as well as the ones from Stanford
1089040	1090800	because they understand the local context.
1090800	1091800	And so they understand you better
1091800	1093520	and they'll be better for that.
1093520	1095800	As part of that, every nation will need their own data sets
1095800	1097720	which again have from broadcaster data.
1097720	1099000	They will need their own open models
1099000	1101240	that can stimulate innovation internally as well.
1101240	1102680	Who owns national data sets?
1102680	1103520	Is that governments?
1103520	1104480	I think it should be the people.
1104480	1106680	I think it should be open and public domain.
1108000	1109760	How does that come into fruition?
1109760	1113240	Well, we have a world where we have national verified data sets
1113240	1116240	which can be leveraged by independent private companies.
1116240	1118360	And others and universities and others.
1118360	1119400	Well, this is what we're doing right now.
1119400	1121120	We're working with our multinational partners,
1121120	1122360	lots more to be announced soon,
1122360	1124200	and multiple governments for a framework
1124200	1127080	for what good data looks like to feed these models
1127080	1129840	to stimulate innovation and localization.
1129840	1131120	And that is a public good
1131120	1135200	because national broadcasters have all of this data.
1135200	1137640	You just tokenize all their kind of things.
1137640	1139400	And then you have things like the implementation
1139400	1141240	of these for education and healthcare.
1141240	1142560	You can take generalized learnings
1142560	1144680	and then again feed the models that thing.
1144680	1148000	What is a great data set for a great British E.P.T. look like?
1148000	1150840	I think it's open, it's interrogated and it's optimized.
1150880	1152120	When you look at all the different things
1152120	1153480	that we've talked about from you,
1153480	1156680	relative treatment of MS to ASD,
1157640	1160200	and then it's impact on education.
1160200	1163160	And we just to PWC spending money on it,
1163160	1165560	there are so many problems that can be solved.
1165560	1169240	Surely we can find a home for the cash.
1169240	1171400	Yeah, and so I'm not sure where.
1171400	1172680	And so this is the thing,
1172680	1174760	like there's gonna be this mismatch.
1174760	1177080	If you were an invested state,
1177080	1178840	if you were me, what would you do?
1178840	1181320	I'm an early stage investor, I'm less globally.
1181320	1183080	What would you do?
1183080	1186200	I would, again, I think it comes down to,
1186200	1188640	there's gonna be this tailwind of beta.
1188640	1191160	And then you have an alpha play on top of that, right?
1191160	1193080	So the beta play is that you just invest
1193080	1194360	in any good founder.
1194360	1195640	And if you get in, you figure out,
1195640	1197600	what can I offer as kind of a value out there?
1197600	1198640	Am I offering distribution?
1198640	1199480	Am I offering people?
1199480	1200560	Am I offering this?
1200560	1202480	And you emphasize kind of your value set.
1202480	1205520	I think right now what people need is people.
1205520	1207400	There are a few people that are like coming out here,
1207440	1212080	but then what you see is you see good companies,
1212080	1214560	with good ideas, but not businesses.
1214560	1215960	They're building surface level things,
1215960	1217320	these wrapper layers and others,
1217320	1219600	and they're not thinking about distribution and data.
1219600	1220920	It's like, if you want to have distribution,
1220920	1221760	what do we do?
1221760	1223280	We went to Amazon and said bedrock,
1223280	1226920	because then it gives us 100,000 SageMaker SMEs.
1226920	1228240	And we'd have to give them the models
1228240	1229760	that they can then take to the private data.
1229760	1231720	And we get a share of all of that.
1231720	1232600	This is how we saw it,
1232600	1234920	like rather than being responsible that.
1234920	1236720	So if you can bring that distribution to that,
1237040	1239440	this is part of that Google memo that went out.
1239440	1240760	We don't have an edge and others open AI.
1240760	1244120	Open AI used Microsoft for distribution and that flywheel.
1245480	1246960	If you have a business that's focused
1246960	1249520	on innovation at the core, that's not actually a business.
1249520	1251240	It becomes a business when that innovation
1251240	1253480	becomes product, becomes distribution,
1253480	1255400	when it has an advantage on data and other things.
1255400	1256920	Those are real modes.
1256920	1258440	How did you arise that partnership
1258440	1260520	between open AI and Microsoft?
1260520	1263160	I saw it as the objective function of open AI
1263160	1265680	is to build AGI and they reckon they need $10 billion
1265720	1267360	to do it and they did that.
1267360	1269360	Like they're building a business on products and things,
1269360	1270920	but they don't care.
1270920	1272640	You know, they're not trying to build a sustainable business.
1272640	1274040	They're trying to build an AGI.
1275120	1275960	Why?
1275960	1277400	Like what would I, just help me understand
1277400	1279360	AGI to build a sustainable business?
1279360	1280400	Cause at the end of the day.
1280400	1281600	No, they're building an AGI
1281600	1283160	to turn the world into utopia.
1283160	1285320	It's written in their path to AGI thing
1285320	1288480	that they think this can basically bring about utopia.
1288480	1290560	So a lot of people in these labs,
1290560	1292200	we only have people joining from all of these labs,
1292200	1295360	like they almost zealous in there.
1295360	1297080	But there is a misalignment there between them
1297080	1299720	and Microsoft in their desire to create that utopian AI.
1299720	1301840	Yes, cause Microsoft is a business, you know?
1301840	1304080	And so this is why you've seen like articles
1304080	1306280	in the information, like Microsoft say open AI
1306280	1308960	aren't compliant and open AI say Microsoft on this.
1308960	1310920	These things happen when there is a misalignment
1310920	1312760	of objective functions.
1312760	1315320	But again, you should view open AI as what they want to do
1315320	1318680	is build an AI that can basically make the world better
1318680	1321400	and hopefully not kill us all, which they say it might.
1321400	1322400	Which is a bit concerning,
1322400	1324600	which is why I hope they have better open governance.
1324640	1326200	How did you think about distribution?
1326200	1328560	You know, you've seen the hugging face part of it.
1328560	1331560	Amazon, you've seen obviously open AI with Microsoft.
1331560	1332760	When you think about distribution
1332760	1335600	and your competitive edge there, where did you land?
1335600	1337800	So my business model is actually very simple.
1337800	1339520	I haven't really talked about it much.
1339520	1342280	Stimulate open one of the biggest providers of grants
1342280	1344840	to open source software, tens of millions already.
1344840	1346520	And then take the best of open,
1346520	1348120	which hopefully we build ourselves.
1348120	1351520	And then an open base with an open data.
1351520	1353920	And then commercial variants with license data
1353920	1355280	and then national variants.
1355280	1358960	So you have Hindi insurance adjusted stable chat
1358960	1362360	or Indonesian pharmaceutical worker stable chat
1362360	1365040	that's available in every cloud, on-prem, on-device
1365040	1369760	with licensing fees, royalties and revenue share.
1369760	1371520	And the system integrators work with us as well.
1371520	1373160	Lots of announcement to come.
1373160	1376560	And so by standardizing and stabilizing all the complexity
1376560	1380200	to these very sophisticated building blocks,
1380200	1382800	these very intentionally built models,
1382800	1384480	that really helps the world integrate this stuff
1384480	1386400	by building playbooks and other things.
1386400	1387320	And that's the core business
1387320	1389160	because it doesn't require actual innovation.
1389160	1390000	We are still innovative
1390000	1391800	and the leaders in media in particular.
1391800	1393800	Instead it requires data and distribution,
1393800	1395160	data to the models.
1395160	1396520	The models are open and interpretable
1396520	1398760	and models to the data via our partners.
1398760	1400760	And that's valuable because the private data in the world
1400760	1402720	is far more valuable than the data
1402720	1405200	that you will send to proprietary models.
1405200	1407360	And it's not a race to the bottom either.
1407360	1408480	So that's what we are.
1408480	1410440	We're a modeling agency with hot GPUs.
1411440	1413600	Building a distribution around the world,
1413600	1415960	realizing that India and other nations
1415960	1418280	will leapfrog to intelligence augmentation,
1418280	1419560	just so they leapfrog to mobile.
1419560	1421160	They will embrace this technology far quicker
1421160	1422280	than we will in the UK even.
1422280	1423240	Why?
1423240	1425080	Because they have to.
1425080	1429640	India, all of the outsourcing jobs in programming will go
1429640	1432680	because GPT-4 can go level three
1432680	1434920	Google programmer exam and pass it.
1435920	1438920	Outsource jobs will go the first, whereas in France,
1438920	1440320	you're never gonna fire a French person.
1440360	1442200	So those jobs are fake, you know?
1442200	1444120	And so they have an objective function
1444120	1445560	when they need to embrace this technology.
1445560	1447920	In Africa, one to one tuition,
1447920	1450040	every kid in Malawi is on things you lined up.
1450040	1451320	We've got other nations.
1451320	1454160	We're gonna bring them all this technology and tablets.
1454160	1455440	And guess what?
1455440	1457320	Their lives will transform.
1457320	1458920	One AI per child is one I wanna call it.
1458920	1460400	We'll call it something else.
1460400	1462000	But think about the potential of that
1462000	1463880	because you have one's teacher per 300 kids.
1463880	1465840	What if they had a chat GPT-level AI?
1467160	1468960	The ROI is high and the need is high.
1468960	1471320	And so they will embrace it far quicker than we will.
1471320	1474000	What happens to countries that rely on outsourced work
1474000	1476360	in those kind of freelancer economy jobs?
1476360	1478480	In general, one of the things, the questions is,
1478480	1480160	you've seen OpenAI study, you've seen the,
1480160	1483040	which said task will be replaced up to 44%.
1483040	1486360	You've seen Goldman Sachs say adds percentage points to GDP.
1486360	1489320	I think the only solution to this is entrepreneurship.
1489320	1491560	And so we need to give the tools to create new jobs
1491560	1494760	that can replace some of these old jobs being done.
1494760	1497040	So like to the various Asian governments,
1497040	1501480	I'm saying adopt the UK policy of these sandboxes,
1501480	1504480	financial, AI and other regulatory sandboxes.
1504480	1505880	So you can take these technologies,
1505880	1508360	these national models that we will help you build
1508360	1510280	with our consortium partners
1510280	1512560	and then spur innovation to create the jobs
1512560	1513840	to replace the existing jobs
1513840	1515960	because you'll upgrade your entire society.
1515960	1517520	Bring these models into your governments
1517520	1519480	and other things to go from slow dumb AI's,
1519480	1522040	which is the national organization's healthcare,
1522040	1524240	to intelligent dynamic ones.
1524240	1525920	Can I ask on implementation,
1525960	1527720	when we think about kind of bluntly
1527720	1529800	seeing this in action in society,
1529800	1532080	I'm sure it's very aware of technology cycles
1532080	1534880	taking so much longer than one anticipates.
1534880	1537440	How do you think about that in actual,
1537440	1538400	there's kind of two-fold.
1538400	1539640	One is adoption on enterprise
1539640	1541360	and another is adoption on consumer.
1541360	1543240	Say if we do the adoption on the consumer side,
1543240	1546880	which is impacting freelancer jobs and impacting education.
1546880	1548480	What do you think that looks like?
1548480	1550800	So I think on the consumer side,
1550800	1551920	you're free with your information.
1551920	1553240	So you can use a lot of these things,
1553240	1555240	the APIs of OpenAI and Cohere
1555240	1556280	and others are fantastic, right?
1556280	1558800	And Google Palm now kind of being out there.
1558800	1560080	So it will be integrated
1560080	1562000	to deliver better consumer experiences
1562000	1563000	without it being creepy,
1563000	1565280	like you've seen with some of the chat bots, et cetera.
1565280	1566400	Cause it's going into word,
1566400	1567880	it's going into workspace, you know,
1567880	1569920	like it helps already.
1569920	1571120	Like we will have a conversation
1571120	1572880	will be automatically logged by our pixel phone
1572880	1573880	and then we'll get a transcript
1573880	1575920	and remove bits that we don't want to share
1575920	1577240	that it goes into a global knowledge base
1577240	1578840	that reminds us of things.
1578840	1579760	That's inevitable.
1579760	1580920	On enterprise, it takes longer
1580920	1584440	because you need to have auditable standardized models.
1584480	1585800	If you're a financial services institute,
1585800	1588000	you can't have a single piece of crawl data in there.
1588000	1589400	And so that's what we're deliberately building
1589400	1590880	with the largest companies in the world
1590880	1592240	because we're building dedicated teams.
1592240	1594000	So you can't have a single piece of crawl data
1594000	1595360	if you're a financial services.
1595360	1596880	Yes, because the danger is
1596880	1598080	if it has some Reddit in there.
1598080	1598840	So stable.
1598840	1600000	And you know, we'll put out next week,
1600000	1601880	it wasn't as good as the other models
1601880	1602800	because we're going to make a point
1602800	1604280	about Reddit data being bad.
1604280	1606440	It's not about more data, it's about better data.
1606440	1607280	We had data comp,
1607280	1608680	which is the next generation lion
1608680	1610400	that we funded the compute for,
1610400	1613320	whereby at a quarter of the parameters of OpenAI's clip,
1613360	1615600	that outperforms with the beta data quality.
1615600	1618280	What makes good data quality, sorry?
1618280	1619800	That's something we're exploring right now.
1619800	1621480	But from the investment banks,
1621480	1622600	we've talked to asset managers
1622600	1623880	and we're building dedicated teams
1623880	1625000	for the largest ones in the world
1625000	1626560	to build them a prior to models.
1626560	1629040	The feedback we've got is we cannot use a black box.
1629040	1631000	We need to know what data is in there.
1631000	1633040	Just the regulators are asking us.
1633040	1634960	We don't want to have this out of sample thing
1634960	1636640	where it's seen something on Reddit
1636640	1640280	and then it says something rude to our end users, you know?
1640280	1642040	Why is Reddit data bad?
1642040	1643960	Reddit data isn't bad in itself.
1643960	1646760	It was just a case of more data is not always good.
1646760	1649440	So right now we are using all these web scrapes
1649440	1651880	and we're training our models by taping their eyes open
1651880	1656840	and then it took six months to turn GPT-4 into chat GPT-4
1656840	1659480	because we had to tune it and like give it a haircut and stuff
1659480	1661720	and bring it back to society.
1661720	1664360	The point is that we need to find the right type of data
1664360	1665560	because rubbish in, rubbish out
1665560	1668360	is something that we've heard a lot, you know?
1668360	1670160	And so it's not bad in itself,
1670160	1671160	but if you just scrape it
1671160	1673200	without the proper cleaning, it is bad.
1673200	1674200	Because what is it?
1674200	1676680	It's like, you know, people just covetching a lot.
1676680	1678840	You know, people being biased.
1678840	1682000	Do you really want to feed your kid the whole of Reddit?
1682000	1682960	You know?
1682960	1685960	Do you want to have the best curriculum possible?
1685960	1687200	And this is actually one of the ways the models learn.
1687200	1689360	Like stable diffusion, we train it on the whole internet
1689360	1692280	and then better and better image subsets of it.
1692280	1693280	And that's the same thing with lounge learning.
1693280	1694520	It's called curriculum learning.
1694520	1696760	Train it on a big base that's solid and then did it.
1696760	1699040	It sounds familiar, doesn't it?
1699280	1701840	The hardest thing is how do you instill values
1701840	1704880	and political correctness in models?
1704880	1707280	There is no such thing as an unbiased model.
1707280	1710120	So Dali too, when OpenAI had that
1710120	1712040	and they introduced a bias filter,
1712040	1715880	any non-gendered word that ran a random gender
1715880	1717680	and a random ethnicity.
1717680	1718840	So you type in Sumo wrestler
1718840	1721320	and you get Indian female Sumo wrestler.
1721320	1722280	That was kind of a good picture.
1722280	1723680	I got to save somewhere.
1723680	1725800	I think this is why you need national data sets.
1725800	1727040	You need cultural data sets.
1727040	1728800	You need personal data sets
1728800	1731280	that can interact with these base models
1731280	1733080	and customize to you and your stories.
1733080	1735240	Because you and I both have our stories
1735240	1736320	that make up our psyche.
1736320	1737160	Sure.
1737160	1739120	And understanding that context is so important
1739120	1742440	to have AIs that can work for us, not on us.
1742440	1744440	And so it's essentially like a next generation cookie
1744440	1747000	that personalizes our data to allow for better searches.
1747000	1747840	And mega cookie.
1747840	1749640	And if you standardize the base foundation models
1749640	1751760	and they call it the hypercube every modality
1751760	1752840	because we do all the modalities,
1752840	1755080	all the sectors and all the nationalities,
1755080	1757720	then you don't need to have a million different models
1757720	1759320	like those dream booths of the avatars.
1759320	1760400	You said you have a base model
1760400	1762640	that you then have a vector embedding around.
1762640	1764240	Because these models contain all the principles
1764240	1765920	and the embeddings point to the important bits
1765920	1767840	that make up Harry or Emmad.
1767840	1769960	And then you can search those and adapt those
1769960	1772560	rather than having a million, billion different models,
1772560	1773560	which is just confusing.
1773560	1775080	So I had dinner the other day
1775080	1777840	with one of the largest media publication owners
1777840	1779520	in the world.
1779520	1781400	And he said that I'm white, Harry.
1781400	1782800	I don't think that I will have a business
1782800	1784080	in a couple of years.
1784080	1786400	I think, bluntly, we're getting killed on our advertising
1786440	1788240	because everything's getting scraped
1788240	1790280	and they're not coming to our websites.
1790280	1791280	And that's where we get paid.
1791280	1792480	We get paid for clicks.
1793920	1795640	Is he right to be worried?
1795640	1796600	I think he is right to be worried.
1796600	1798600	Like, again, you look at Google's announcements yesterday
1798600	1800800	to talk about this day after Palm 2,
1800800	1802160	you suddenly look at the new Google page
1802160	1804760	where they've got the language model
1804760	1807720	and it's just text and where they clicks.
1807720	1810200	It was like when Google introduced AMP.
1810200	1811520	You know, this is where rather than look
1811520	1812360	at the New York Times page,
1812360	1813200	you have this formatted thing
1813200	1815640	with no New York Times kind of stuff there.
1815640	1818360	Like, these search entities that aggregate
1818360	1820440	are just intermediating more and more
1820440	1822760	and people are going to become used
1822760	1825680	to just having synthesized input.
1825680	1826920	So what does search look like?
1826920	1829160	What does it look like when your GPT-4
1829160	1832320	can write you an article about any news that's happening
1832320	1833960	in a way that's customized to you
1833960	1836120	and your context and everything like that?
1837240	1840440	This is massively disruptive for media and information.
1840440	1842280	And so they have to think, where am I in the future?
1842280	1844480	Where, again, the way I swear to think about the impact
1844520	1847000	this is the retanted grads are caching off their meds
1847000	1849320	and we push a button and get 1,000 of them.
1849320	1851480	Those grads include journalists.
1851480	1853680	And you can have your own journalist army,
1853680	1855360	your own writer army, your own coder army,
1855360	1857040	your own designer army.
1857040	1859440	So the pushback against that is libel.
1859440	1861120	He said, good fucking luck.
1861120	1865520	We spend our life in law suits, libel is real.
1865520	1869160	You are gonna get unbelievable amounts of libel cases
1869160	1871480	and then open AI will be fucked.
1871480	1874400	You cannot have 1,000 libel cases a day.
1874400	1875240	Well, this is the thing.
1875240	1878200	If you say this needs to be checked and cross-checked,
1878200	1879880	that's one thing, but a lot of the media companies
1879880	1881480	say we're the source of authority.
1881480	1884680	So a way that media companies can shift is by having
1884680	1886440	in a deep fake and other age
1886440	1888720	where everything can be generated,
1888720	1891560	we make sure this is real news.
1891560	1893680	We are very thorough in the way we do it.
1893680	1894520	So this is interesting,
1894520	1896800	so you place a premium on authority.
1896800	1899560	Premium authority, this is why you've got the check marks
1899600	1901560	coming out at Twitter and the organizational
1901560	1904520	1,000 pounds a month and Facebook doing the same
1904520	1906440	because you need to have a level of authenticity
1906440	1907800	and level of authority.
1907800	1910160	But again, is the news fair and balanced?
1910160	1911960	I've had lots of hit pieces coming out against me
1911960	1913000	and got a lot more.
1913000	1914840	It's not because they have angles, you know?
1914840	1916640	And so what is the bias of the New York Times
1916640	1919040	versus this, versus that, versus others?
1919040	1920440	How do people consume news now?
1920440	1923720	And even news consumption has gone down dramatically, right?
1923720	1925800	Because people consume news through their social networks
1925800	1928000	through their groups and other things.
1928000	1930840	So you have to say, what is the model?
1930840	1933120	But the hard part is, you know,
1933120	1935280	none of the next generation models
1935280	1938120	or AI providers want to be content publishers.
1938120	1940440	So how do you fit in a world where, you know,
1940440	1943240	you're killing that business model on the content side,
1943240	1944800	but they don't want to be publishers.
1944800	1946640	You will have AI first publishers.
1947920	1949400	So if you remember a kind of Vox
1949400	1950520	and these things when they kind of kicked off,
1950520	1951560	they wanted to be generated,
1951560	1953960	they wanted to be technology first.
1953960	1957120	You're gonna have a new wave of AI first publishers
1957120	1959880	that aren't just AI, but it's AI plus humans.
1959880	1961000	Because AI plus-
1961000	1961840	What does that look like?
1961840	1962840	Sorry, AI plus humans.
1962840	1966200	AI plus humans means that you have information coming in
1966200	1969200	and then the stories or drafts are automatically written,
1969200	1971520	reviewed by humans who then give their input
1971520	1972680	to train it better.
1973800	1975520	This is kind of the feedback flow.
1975520	1976840	And then what happens is it comes out
1976840	1978160	and there's a factual anchors
1978160	1980040	and then it gets customized to Alabama
1981120	1983880	and then Alabama context and all sorts of other things.
1983920	1986320	Because you can tell it, TLDR,
1986320	1989120	two ladies didn't read, explain it like I'm five,
1989120	1990840	make it more complex.
1990840	1993200	And so you're gonna see something very interesting here,
1993200	1994920	which is the right news at the right time.
1994920	1997080	The localization will return,
1997080	1998760	but again, through AI first.
1998760	2001520	I think this is the thing, we're seeing AI integrated,
2001520	2002840	but the next wave is going to be
2002840	2004080	once we understand design patterns,
2004080	2007400	AI first, everything and information flows.
2007400	2009160	Once these technologies are a bit more mature.
2009160	2010760	Can you just help me understand AI integrated
2010760	2012600	versus AI first, what is-
2012600	2015000	AI integrated means that I have an existing newsroom
2015000	2016760	and I bring in AI to write faster drafts
2016760	2017720	and things like that.
2017720	2020200	AI first is saying, I have an army of things
2020200	2022680	I can spin up instantly that can help me achieve
2022680	2024280	these certain things to create news
2024280	2027240	that is valuable for this reason with this feedback loop.
2027240	2029920	And so you build the system kind of from the start
2029920	2033200	thinking AI at the core versus AI being integrated in
2033200	2035040	to improve existing systems.
2035040	2036400	Because so much of news is what?
2036400	2038160	We find information, we have drafting,
2038160	2041080	we have this, we have that, we do these checks.
2041080	2043240	A lot of that can be simplified,
2043240	2047040	just like we move from the analog to the digital age
2047040	2050200	to the internet age to the next age as the AI age.
2050200	2052560	So I have to ask, when we think about kind of AI first
2052560	2054680	publishers and the next generation of media,
2056320	2057200	who does this?
2057200	2058600	Is this startups?
2058600	2062480	I've met honestly 50, maybe more AI companies
2062480	2064080	in the last month.
2064080	2066840	And the feedback is always the same.
2066840	2070560	They're not operating off a defensible mode of data
2070760	2072280	that literally a thin application layer
2072280	2076680	on top of an existing model is 99% of the feedback.
2076680	2078680	So you look at something like Harvey, for example.
2078680	2081280	They went to law firms, they said, you are a distribution
2081280	2083280	and we're going to integrate and improve your system
2083280	2085080	and build our system for your system.
2085080	2086680	So I think a lot of these people are trying to build it
2086680	2088280	and they will come and they're trying to get in there
2088280	2090320	as opposed to just retargeting.
2090320	2091600	Where can you go in and transform?
2091600	2093360	Is that the wrong model Harvey did?
2093360	2094320	No, I think it's the right model.
2094320	2096520	I think that a lot of organizations are elastic
2096520	2098400	and plastic now, so you can go in and give them
2098440	2100680	an integrated thing saying, you will be my test case.
2100680	2103320	I will help you upgrade as a Skunkworks lab
2103320	2105880	and build a system alongside your system as it were.
2105880	2108200	And sorry, and you think enterprises will say sure?
2108200	2111240	I think now they will if you can keep
2111240	2113160	their data inside internally.
2113160	2114880	And I think again, with better open models,
2114880	2116000	you can enable that.
2116000	2117520	So people can build on top of open models.
2117520	2120880	There are dedicated instances on Cohere and others as well.
2120880	2122600	And so the tooling is now catching up
2122600	2124840	so that you can have a new generation of startups
2124840	2127000	where their first customers are massive companies,
2127000	2129520	they would never get otherwise.
2129520	2130360	I think this is the thing,
2130360	2132440	because every big company is looking for an answer.
2132440	2134160	If you can give that answer,
2134160	2135760	that contract that would have taken you a year,
2135760	2137360	you can get in a week.
2137360	2138200	Do you think so?
2138200	2139400	Because you still got to get in the door.
2139400	2142040	You got to get in the door, and that's hustle, man.
2142040	2144440	So again, this is what the Harvey guys kind of did.
2144440	2146320	This is why I went straight to the hyperscalers
2146320	2148320	and I said, you need to have standardized models
2148320	2149640	for open, for regulated data.
2149640	2150880	What did they say to you?
2150880	2152800	They said, really?
2152800	2153640	Can you build them?
2153640	2154800	Here's some models that we built.
2154800	2156640	Oh, and then I told them exactly
2156640	2159120	how the things would be last summer to now.
2159120	2160960	And it's followed that and I've kept in touch
2160960	2161800	and I've improved it.
2161800	2163000	And this is why I'm building dedicated teams
2163000	2164520	for the largest companies in the world.
2164520	2165880	I'm not telling them, I'm trying to sell you anything.
2165880	2167520	I'm like, over the next year,
2167520	2170520	I'm gonna help make sure you do not get blindsided.
2170520	2171840	Like I try and sell you models
2171840	2174280	and people are offering us tens of millions per model.
2174280	2177480	But I'm like, I'm gonna build a proper partnership with you.
2177480	2179560	And that means I'll have a LTV from you.
2179560	2181240	What does that proper partnership mean?
2181240	2182080	And who's that with?
2182080	2184000	That's with IBM, that's with SAP, that's with Apple.
2184000	2186000	So we've announced Amazon.
2186120	2187440	Let's say we have lots of other announced
2187440	2188640	with the biggest companies in the world
2188640	2190640	where they have amazing teams,
2190640	2192440	but they can only move so fast.
2192440	2193680	And I'm building dedicated teams
2193680	2196000	that help them move and understand the whole sector
2196000	2199160	without trying to sell them on services.
2199160	2201240	I'm trying to say, I will build you a customized model
2201240	2202480	if you want, but I'm only doing that
2202480	2204600	with a dozen companies, you know?
2204600	2206200	So I can kind of focus down.
2206200	2208560	And I will tell you that GPT-4 is great
2208560	2210640	or Cohera is great or all this stuff.
2210640	2212760	All the latest research through the communities we support,
2212760	2214400	I will make sure you're on top of,
2214440	2215280	rather than to your sector
2215280	2216320	and you've got dedicated people
2216320	2217680	helping you in this transition period.
2217680	2219160	Is that aligned to your core model?
2219160	2221400	It seems like an ancillary product
2221400	2224120	that is like a SAP consulting services.
2224120	2225360	It is kind of like that
2225360	2228760	because I need to understand these sectors better.
2228760	2230200	What does the hypercube look like?
2230200	2233040	What does the insurance adjust to GPT look like?
2233040	2234920	You know, as a fundamental basis.
2234920	2237360	And so a lot of people are able to extract that data
2237360	2239080	and then take it with you and do the learning on it.
2239080	2240360	Yes, and so this is part of the thing,
2240360	2242480	that we will have a generalized model and we're very clear.
2242480	2244120	But then you can have a specified model
2244120	2245480	just for you as well,
2245480	2247040	as long as it doesn't interfere with that.
2247040	2248720	So find that balance will be interesting,
2248720	2251600	but the reality is no models that are out today
2251600	2253280	will be used in a year.
2253280	2255480	Unpack that for me, this is mind blowing.
2255480	2257480	So again, you see the order of magnitude improvement.
2257480	2259720	Palm last year was 540 billion parameters,
2259720	2261720	then Chinchilla 67 and now 14.
2263280	2266440	540 to 14 is a big step, you know?
2266440	2269160	You see the quality of GPT-3 versus GPT-4.
2269160	2270840	Is there any extent to how low it can go?
2270840	2271800	We have no idea.
2272800	2273720	Like I would have said,
2273720	2276000	you already said this is impossible.
2276000	2278560	Like two years ago, it was like no way.
2278560	2280840	You have a single file that's maybe a few hundred gigabytes
2280840	2283840	that can pass every exam apart from English Lit.
2283840	2284880	Fucking English Lit.
2284880	2287280	Fucking English Lit, no way, no way.
2287280	2289160	So we're already at the impossible and like,
2289160	2290480	what does that mean though?
2290480	2293760	Like if we go lower and lower and lower, and then what?
2293760	2294600	And then what?
2294600	2296760	When it jumps as you saw with the llama stuff
2296760	2298320	and all the innovation around that
2298320	2300720	to your MacBook offline,
2300760	2302120	the marginal cost of creation
2302120	2303760	and coordination becomes zero.
2305520	2307120	I don't know what it means.
2307120	2308240	Nobody does.
2308240	2309840	And this is the thing.
2309840	2311560	It always takes longer and shorter
2311560	2313800	to implement groundbreaking technology than you've ever seen
2313800	2315040	and this technology can be in place
2315040	2316760	like nothing we've seen before.
2316760	2318200	Well, this is my call.
2318200	2320640	Not concerned, I hate the doomsday it says
2320640	2321600	and I'm excited for the future,
2321600	2322640	I'm terrified for the future too,
2322640	2324720	but everyone always says technology revolutions
2324720	2325560	in industrial age,
2325560	2327560	whether it's the introduction of PCs in 25s.
2327560	2330040	These were, industrial was a 30 year plus.
2330120	2332760	Actually PCs in 25s was 10 years plus.
2332760	2336080	The challenging thing is like the learning curve
2336080	2339840	to use chat GPT as a marketer is nothing.
2339840	2340880	I mean, it is easy.
2340880	2343640	And so, and the integrations is a day.
2343640	2345560	It's because, yeah, like you want to write an API,
2345560	2348000	it's not a day, you just give it the manifest spec
2348000	2349240	and it automatically generates.
2349240	2350480	It would have taken days before.
2350480	2351320	Sure.
2351320	2352680	Like it's an amazing experience.
2352680	2355400	And so the transition is so much more compressed today.
2355400	2357360	It came from the existing system
2357360	2359240	as it goes seamlessly into the existing system
2359240	2360880	versus like web three that tried to create a system
2360880	2362640	outside the existing system
2362640	2365080	and all the money was made and lost at the interfaces.
2365080	2366960	Again, it's like deploying grads at scale.
2366960	2370280	Like with a 32,000 token context within your GPT-4,
2370280	2372760	20,000 words of instructions.
2372760	2374320	What does that do to SAS?
2374320	2376800	So my thing is that we're still in this crazy period.
2376800	2379480	Next year it will settle and then it'll go ubiquitous.
2379480	2381080	Well, a lot of companies know they need to do something
2381080	2382120	but they don't know what they need to do.
2382120	2383560	Are they adopting it now?
2383560	2384800	They're doing the POC thing.
2384800	2387000	Like some like Microsoft and others for consumer,
2387000	2387840	they're adopting it.
2387840	2390040	Consumer adoption is a much lower bar.
2390040	2391480	When this starts going in enterprise,
2391480	2393480	it's gonna be a fricking train
2393480	2395680	because so much of enterprise is about services
2395680	2397400	and information flow.
2397400	2398840	Again, if you push a button and have a thousand
2398840	2403840	of these things, that's a huge difference, right?
2404560	2406320	And so like, I think this will be
2406320	2408800	a bigger economic impact than COVID.
2408800	2410680	I don't know in which direction.
2410680	2411920	I hope that you're positive.
2411920	2413840	But again, giving that example of an India
2413840	2415280	or one of these outsource places,
2415280	2416520	you lose BPO jobs,
2416560	2419200	you can make it up on entrepreneurship
2419200	2420880	if you embrace the technology.
2420880	2422840	What do you think the business model of the future is
2422840	2425160	for those models moving into enterprise?
2425160	2426520	I think it's the same as always.
2426520	2428600	You've got good products, good distribution.
2428600	2431880	You lock it in, 1.5 million people still use AOL.
2433240	2435720	Like HCL bought Lotus Notes for 1.5 billion
2435720	2436920	a few years ago.
2436920	2439720	Like 40% of the world still doesn't have internet.
2442880	2445520	Again, we're super privileged where we are, right?
2445560	2446400	And so you look at that
2446400	2448040	and I look at emerging markets.
2448040	2451280	I'm like, all of finance is securitization and leverage
2451280	2453920	and securitization is telling a story.
2453920	2455200	The only thing that matters for a stock
2455200	2457640	is the marginal story and how it evolves.
2457640	2459240	What if you have massive information
2459240	2462440	about every child in Africa and every business in India
2462440	2464600	and they embrace this technology properly?
2465720	2467440	Massive financial growth.
2467440	2470160	Why do you think next year for their embracing it?
2470160	2472360	I think that people are still getting used to all this.
2472360	2473840	We haven't standardizing things.
2473840	2476240	We don't know what the design patterns are.
2476240	2478200	I think that what happens is everyone's doing this
2478200	2479520	at the same time and they're all trying
2479520	2480800	to get to grips with it.
2480800	2482520	And so again, we have this like six month window
2482520	2483600	where everyone's getting to grips with it
2483600	2485920	and then we standardize our design patterns
2485920	2488400	and they spread and you start implementing.
2488400	2489840	And then you see some people outpacing others
2489840	2490960	which means that you have to catch up
2490960	2493240	and then you're forced to implement it.
2493240	2496000	So this is how I see the race dynamics occurring right now.
2496000	2497480	You say about forced to implement it.
2497480	2499760	I think the truth is they just have no fricking idea.
2499760	2500600	Right now they don't.
2500600	2502160	Which I totally understand, I didn't blame them for
2502160	2504160	but I tweeted actually the other day
2504160	2505760	that I think the biggest AI companies
2505760	2507760	would be services-based implementation companies
2507760	2509200	for large enterprises.
2509200	2510040	100%.
2510040	2511360	That's why I said if you're a startup,
2511360	2513360	the best thing to do is you identify an enterprise
2513360	2514640	that will be transformed by this
2514640	2517840	and you go to them and you say, I have a solution.
2517840	2519440	And I'm gonna start with you.
2519440	2521520	And I might go bigger, but I'm gonna help you
2521520	2524480	through this period by doing this, this, and this.
2524480	2525520	And they will appreciate that
2525520	2527760	and they'll be capital available for that
2527760	2529640	in a way that you've never seen before.
2529680	2531160	You know how difficult it is for small companies
2531160	2534240	to sell to big, but the big companies have no idea
2534240	2537440	except for their CEO and their board are telling them.
2537440	2538640	You look at the number of mentions
2538640	2540360	and earnings calls, it's not like that.
2540360	2541840	Every earnings call next quarter
2541840	2544200	and then by next year, literally every single one,
2544200	2545560	they're like, what is our strategy?
2545560	2547840	It's not like, what is our web three and metaverse strategy?
2547840	2549480	It's like, I need this strategy now.
2549480	2551760	Again, it's like, what is our COVID strategy?
2551760	2554400	It'll be that level of urgency within a few quarters.
2554400	2555680	Would you raise money if you were then?
2555680	2557200	So you go to a corporate and you say,
2557200	2559520	hey, you know what, I can solve your problem.
2559520	2561480	This is how it'll work and they will fund you.
2561480	2562600	They'll give you the data.
2562600	2564280	Would you raise money?
2564280	2566320	Yeah, I mean, like again, you need the people.
2566320	2567560	The people is the key thing here
2567560	2569840	because you can have the technicals' chops, you know?
2569840	2571840	You have an understanding of the industry.
2571840	2574360	But to build a good business and to scale it
2574360	2575480	at the pace that you need to,
2575480	2577360	to keep up with this is incredibly hard.
2577360	2578720	Do we have enough talent?
2578720	2579880	No.
2579880	2582200	And so this is why we support the fast.ai courses
2582200	2585280	which transform normal developers into ML developers
2585280	2586720	and other things like that.
2586720	2588360	But again, these models are actually not that hard
2588360	2589200	to work with.
2589200	2591680	50% of all code on GitHub is AI generated now.
2591680	2593240	So you can even use co-pilot to help you
2593240	2595080	code the models and other things like that.
2595080	2598880	What do you think that code generation is in five years?
2598880	2600360	Why would you need code?
2600360	2602480	Code is just a way to talk to a computer.
2603680	2604520	I'm part of that.
2604520	2605360	So when I started...
2605360	2606280	You're speaking human language.
2606280	2611040	When I started 21, 22 years ago as a code, I'm 40 now.
2611040	2612400	So just doing that one's 18.
2612400	2615520	I was writing assembly code, you know,
2615520	2617280	really low level stuff.
2617320	2618160	There were no libraries.
2618160	2619000	There was no GitHub.
2619000	2619840	There was nothing like this.
2619840	2622000	Like right now coding is like mixing and matching.
2622000	2623920	It's like building Lego.
2623920	2626360	And AI can build that Lego much better,
2626360	2627520	especially in five years.
2627520	2629880	What you're doing when you're propping like programming
2629880	2632120	language is you're telling it to go and do something.
2632120	2633200	Even something like Palm,
2633200	2636160	like we sponsor an amazing code called Lucid Raines.
2636160	2637360	If you want to cry as a programmer,
2637360	2638440	you go and look at his GitHub,
2638440	2640520	most productive developer in the world.
2640520	2643520	He recreated the whole of Palm in 206 lines of PyTorch.
2645160	2647240	But again, why would you need a human for that?
2647240	2649520	If the AI gets better and better at coding,
2649520	2650720	just tell it what you want.
2650720	2654000	I want to create an app for 20 minute VC
2654000	2654960	that has these features.
2654960	2657880	Of course it will go and build it automatically.
2657880	2659560	Where is the human coder in that?
2659560	2661120	What does that mean for the future on refresh?
2661120	2662920	But actually a good thing in terms of the complete
2662920	2664800	democratization, anyone can build anything.
2664800	2665640	Anyone can build anything.
2665640	2668640	This is why distribution data, you know,
2668640	2671840	relationships, product become important.
2671840	2674640	Because it already became easier to build anything, right?
2674640	2676560	But what makes a good product?
2676560	2678360	Again, there are these unchanging things.
2678360	2680960	Have great customer satisfaction, deliver value.
2680960	2683040	People get distracted by technology.
2683040	2685640	Like I was at this CryptoX AI thing on the weekend.
2685640	2687240	They were talking about decentralized.
2687240	2688520	Guys, just this is all bollocks.
2688520	2691000	Like it's not about the technology,
2691000	2693960	it's about what you're creating that's valuable
2693960	2695920	to help people, you know?
2695920	2696840	Focus on that.
2696840	2699480	Who do you think wins in the next three to five years?
2699480	2700640	Startups or incumbents?
2700640	2702560	Because incumbents have the distribution.
2702560	2703760	I think it's incumbents,
2703760	2705440	but there's a lot of startups that will be billion dollars.
2705480	2707760	And even on the thin layer thing,
2707760	2712240	ITA software sold for 700 million
2712240	2713920	and KIAX sold for 2 billion.
2713920	2716000	And that was a layer on top of ITA.
2716000	2717920	We've seen many of these examples here, right?
2717920	2719640	Again, we know that value and moats
2719640	2722240	are not necessarily innovation first.
2722240	2723880	Well, yes and no, it's interesting.
2723880	2726160	I had Tom Tunga's on the show.
2726160	2727920	And he essentially analyzed,
2727920	2729880	Tom is a very famous ML and AI investor,
2729880	2733520	and he analyzed infrastructure versus application layer.
2733520	2736200	And both actually were about $2 trillion times.
2736200	2737880	The differences in the infrastructure layer,
2737880	2739480	there was three companies,
2739480	2741840	and in the application layer, there was 50.
2741840	2744280	And so your average enterprise value
2744280	2745640	was like significantly different.
2745640	2746480	I would agree with that.
2746480	2748560	I think that there's only gonna be five or six
2748560	2750440	foundation model companies in the world
2750440	2752320	in three years, five years.
2752320	2754160	Do you think they've all been created now?
2754160	2755000	Yes.
2755000	2755840	Which are they?
2755840	2759640	I think it's gonna be us, Nvidia, Google,
2759640	2762640	Microsoft, OpenAI, and Meta and Apple probably
2762680	2764200	are the ones that train these models.
2764200	2765800	Is Anthropic good?
2765800	2766920	Anthropic are great.
2766920	2769280	But from a business model perspective,
2769280	2773040	you have Claude on Google API, and you have Palm II.
2774080	2776080	How are they gonna keep up with Palm II?
2778120	2778960	You know?
2778960	2779960	I can't answer that.
2779960	2781720	Well, Google, they can raise billions,
2781720	2785240	but Google will spend $20 billion on AI.
2785240	2788000	DeepMind's salary budget is $1.2 billion a year.
2788880	2791640	So DeepMind's salary budget is $1.2 billion a year.
2791640	2792480	Yes.
2792480	2793800	So that's in the public kind of filing.
2793800	2794720	So was it salary and compute?
2794720	2795760	I think it's salary.
2795760	2797120	They technically make a billion a year
2797120	2800520	from their internal counter payments with Google as well.
2800520	2801360	Wow.
2801360	2802200	But again, I mean, Google,
2802200	2803040	how much money do they have?
2803040	2805080	$150 billion to win this?
2806440	2807280	Fuck.
2807280	2809080	How much money do you need?
2809080	2812160	I have a business model that is going to be massively
2812160	2813600	profitable very soon.
2813600	2815560	Because of the national services?
2815560	2816520	Because of various things.
2816520	2817520	I haven't given the full details.
2817520	2818600	I will over the next few months.
2818600	2819960	I've got a nice little case study
2819960	2821520	with some universities coming.
2821520	2822960	I like it to be a surprise.
2822960	2824680	Well, it's really hard for you.
2824680	2827040	Talent, keeping talent together, A plus teams.
2827040	2828960	So we've had zero attrition in our developers
2828960	2829800	and they're amazing.
2829800	2831160	So we've got video models, audio models,
2831160	2832000	all these things coming out.
2832000	2833680	Everyone says you need to be in the valley.
2833680	2834560	You're in London.
2834560	2835400	Yeah.
2835400	2836840	Do you disagree you need to be in the valley?
2836840	2837960	Of course you don't.
2837960	2839800	I am going to bring this technology to the whole world.
2839800	2842080	I'm going to bring it to all the IITs and universities
2842080	2843920	and the best of people in all of those
2843920	2845920	will join Stabilities in the local thing.
2845920	2846760	I'll have talent.
2846760	2849560	I'll bring this to all of the national broadcasters
2849560	2851080	and biggest family offices around the world.
2851080	2852320	I'll have data.
2852320	2854000	Nations will build supercomputers
2854000	2855120	that I will build open models on.
2855120	2856400	I'll have super compute.
2856400	2858280	So I'll have more super compute talent
2858280	2860000	and data than any other company.
2860000	2862600	And I'll build it all in the open.
2862600	2864320	And one thing I heard you talk about before,
2864320	2865160	which I thought was fascinating,
2865160	2866840	was your access to super computer.
2866840	2869360	You compared it to existing large incumbents.
2869360	2871880	Why do you have more super compute than other people?
2871880	2873360	Because I went and I did it.
2873360	2875440	So we had articles coming out saying about our burn.
2875440	2877240	I'm like, I have oil wells
2877240	2879440	when everyone wants to build petrochemicals.
2879440	2881280	Every day we have companies coming to us
2881280	2882520	asking us for our super compute
2882520	2884360	because it's not available on the market.
2884360	2886240	You need these chips lined up with interconnects
2886240	2889080	and we've got 7,000 A100s now.
2889080	2891200	We have TPUs, we have all these things.
2891200	2892160	And we know how to use them
2892160	2894800	and we can share them with people because we're open.
2895800	2898120	Whereas Anthropic and others cannot.
2898120	2900360	So this is like at the worst case,
2900360	2903080	I'll build a foundation model as a service company
2903080	2905480	and I'll make $100 million in profit this year
2905480	2907360	without having to charge even market rates
2907360	2908200	and I can retire.
2908200	2910080	I wouldn't do that and bring this to the world.
2910080	2912760	So I think computers misunderstood.
2912760	2916520	It's not like Bird and all these scooter companies
2916520	2919400	and others, they spent money on marketing.
2919400	2921960	This is actually an asset right now that's scarce.
2922880	2924960	And so there's no harm in scaling compute
2924960	2927000	and then with the top chip manufacturers,
2927000	2928640	they're building us dedicated teams
2928640	2931120	and again, they're coming in and supporting us
2931120	2934600	because our models drive demand for their chips.
2934600	2936600	The more open models there are, the more open demand is.
2936600	2938600	So it's a virtual circle there as well.
2938600	2940800	And so we get compute before everyone else.
2940800	2943520	Can I ask, in terms of like short-term economic growth,
2943520	2946400	how do you think about the impact that everything
2946400	2949040	we've just discussed has on rising inflation,
2949040	2952920	rising interest rates, short-term employment rates?
2952920	2954440	It's massively deflationary.
2954440	2956800	The biggest drivers of CPI inflation in the U.S.
2956800	2958200	were education and healthcare.
2959480	2961280	And that was almost all administrative
2961280	2963640	and bureaucratic in the next few years,
2963640	2965960	guess what gets disrupted, those.
2965960	2967920	But they don't get disrupted this year
2967920	2969920	or next year, it's the year after
2969920	2973160	because those ones take a bit longer to come through.
2973160	2975560	Okay, and so what is that?
2975560	2976720	How does that impact the economy
2976720	2979120	that we think kind of U.S., U.K.?
2979120	2981120	What does that look like in terms of a three-year time period?
2981120	2982920	I think the U.K. benefits.
2982920	2984760	Unicorn Kingdom is a new kind of thing is,
2984760	2988320	because it'll get, because we have amazing policies
2988320	2990720	like every single AI company should come to the U.K.
2990720	2994640	because cloud computing is now included in R&D tax credits.
2994640	2997600	It's a 27% rebate on losses in cash.
2998600	3001360	We can now issue scale-up visas, global talent visas
3001360	3002280	like the Deep Floyd team
3002280	3004560	that released the best image model in the world ever
3004560	3005800	from stability.
3005800	3007440	They were bought in on tech talent visas
3007440	3008560	that was turned around in one week.
3008560	3009880	Do you think the U.K. has done a good job
3009880	3012040	in terms of implementing regulation and policies
3012040	3013440	to bring AI talents?
3013440	3015800	Had the best apart from maybe Japan, yes.
3015800	3017360	What's Japan done?
3017360	3019080	Japan has some very interesting ones around
3019080	3020240	web data scraping and others,
3020240	3022120	but again, Japan is a very different culture.
3022120	3023320	So even if policy is good,
3023320	3025080	it still doesn't have the same innovative thing.
3025080	3026400	Who's done the worst?
3027320	3028640	The worst, I'm not sure actually.
3028640	3029520	No one's done too bad.
3029520	3031760	The new European legislation was really, really bad.
3031760	3033000	Now it's got a little bit better,
3033000	3035640	but always Europe wants to be the leader in regulation,
3035640	3037600	which kind of, you know, okay, fair enough.
3037600	3038560	It's never an easy thing.
3038560	3039400	It's never an easy thing.
3039400	3040240	And this is the thing,
3040240	3042360	like I think the U.K. is in a very good position
3042360	3043640	and the government's for leading me.
3043640	3046040	Look, the 900 million pound supercomputer,
3046040	3047960	100 million pound LLM task force
3047960	3051760	that's been equated to the COVID level of seriousness,
3051760	3052720	you know?
3052720	3055480	What do you make of like the open AI comparison?
3055480	3058240	I've seen quite a few which are open AI for Europe.
3059440	3061280	And we've seen three or four now.
3061280	3063440	Like, is this a zero sum game?
3063440	3066240	And open AI is one that race, so to speak, or?
3066240	3068080	I think it'd be difficult to compete against them
3068080	3069720	because they're executing incredibly well.
3069720	3071240	And I think, again,
3072280	3074360	why would you use open AI for Europe
3074360	3078600	versus palm two versus GPT four?
3078600	3079440	What can you bring?
3079440	3080840	But you will have national champions and others.
3080840	3082120	I think it's incredibly difficult
3082120	3083440	to compete in proprietary.
3085160	3086840	I think in open, it's a bit different
3086840	3088720	because of standardization element there.
3088720	3091560	But again, my play is to be the benchmark
3091560	3092760	across every modality,
3092760	3093720	because there's no other company
3093720	3096080	apart from me in open AI that does every modality.
3096080	3097920	There's no company that's as aggressive
3097920	3099680	as me in emerging markets.
3099680	3101520	And so they have to say, what is my edge?
3101520	3102400	Because you can have an edge,
3102400	3106440	like you can be the open AI for government or defense
3106440	3109760	or for healthcare and really get in and understand those.
3109760	3111720	And then you can be sticky, you know?
3112280	3115920	Scale is now going fully into defense, you know?
3115920	3116760	Scale AI's?
3116760	3118840	Yes, so they've announced the integrations
3118840	3120600	with the Air Force and all sorts of other things.
3120600	3123320	Like we're getting together on Defcorn this year
3123320	3124760	and people are going to hack at our models
3124760	3127320	and open AI's and others organized by scale.
3127320	3129120	What is your edge?
3129120	3131120	What is, again, your moat?
3131120	3132960	What is your business model and where?
3132960	3134240	What are you reliant upon
3134240	3136480	to deliver that value that can increase?
3136480	3137600	This is why I was surprised
3137600	3139960	when I saw you sign the petition of Elon
3140040	3142000	in terms of pausing for six months.
3142000	3144000	You used to unpack why you did that.
3144000	3145320	Well, I mean, for six months,
3145320	3147840	you're not getting H100s and TPUV5s anyway.
3147840	3149240	So it's a natural pause.
3149240	3150960	But then also because the shit show is coming next year.
3150960	3152760	So I said, we have to self-regulate.
3152760	3155840	Like for example, the adversaries already have GPT-4.
3155840	3158840	Why? Because you can just download it on a USB stick.
3158840	3160160	You know, you don't have to train your own
3160160	3161000	when you can just steal it.
3161000	3162480	Let's have better opsec.
3162480	3164000	Let's have better standards around data.
3164000	3166760	Let's stop and move off web scrapes by next year.
3166760	3168080	We had hundreds of millions of images
3168080	3169120	opted out of stable diffusion
3169120	3171400	because we were the only company in the world
3171400	3173760	to offer opt-out of datasets.
3173760	3176720	You know, like let's bring in some standards around this
3176720	3178400	before it's everywhere.
3178400	3179240	Basically where we are now.
3179240	3182280	You remember COVID, your mom is talking about this
3182280	3184960	and your aunt and everyone's talking about generative AI
3184960	3186760	and they're asking you, Harry, what's going on?
3186760	3190440	You know, but you haven't had the Tom Hanks moment yet.
3190440	3191800	Because everyone was talking about COVID
3191800	3193040	before Tom Hanks got it.
3193040	3194120	And then when Tom Hanks got it,
3194120	3196160	that's when global policy changed.
3196160	3198040	Because if Tom Hanks can get it, anyone can get it.
3198080	3200280	What is that moment for generative AI?
3200280	3201200	What do you think it is?
3201200	3202040	I don't know.
3202040	3202880	I know it's coming.
3202880	3204480	Because I know this technology is definitely
3204480	3206640	everywhere next year and it's disruptive.
3206640	3207800	You don't think there's a chance
3207800	3209760	that takes much longer, three to five years?
3209760	3210960	No chance.
3210960	3213680	It's so useful right now.
3213680	3215760	And you think about certain industries
3215760	3216880	and how they'll be affected
3216880	3221800	by having the ability to have 1,000 GPT-4s working together.
3221800	3224120	You said a tweet actually,
3224120	3225640	I think it was a reply to a tweet,
3225640	3228600	but you said hallucinations are a feature, not a bug.
3228600	3230560	Yeah, so right now people are trying to treat these models.
3230560	3231560	So we're trying the whole internet
3231560	3233560	and like stable diffusion is 100,000 gigabytes
3233560	3234880	and a two gigabyte file.
3234880	3235880	What on earth is that?
3235880	3236760	It's not compression.
3236760	3237880	It's none of this kind of stuff.
3237880	3239040	It learns principles.
3240160	3243720	GPT-4 NVIDIA said they built the dual H100
3243720	3245080	with the NV link for that.
3245080	3248080	And that's 160 gigabytes of VRAM,
3248080	3250040	which would imply a 200 gigabyte model.
3251680	3252520	Right?
3252520	3253360	What is that?
3253360	3255040	That's a 100 gigabyte model,
3255040	3256240	200 billion per hour model.
3256240	3257520	That's nothing,
3257520	3259280	something that can pass all these exams.
3259280	3261240	So what we did is we took these really creative things,
3261240	3263040	just like you start school and you're creative
3263040	3264440	and then you're told you're not allowed to be creative
3264440	3266560	until you're successful and you can be creative.
3266560	3267840	Because schools like Petri dishes,
3267840	3269640	social status games and childcare,
3269640	3271360	that's a story from another time.
3271360	3273320	These models start out incredibly creative
3273320	3274400	and that's their advantage.
3274400	3277720	And then we train them to be accountants with RLHF.
3277720	3279960	And somehow, despite the fact that it's only 100 gigs
3279960	3283240	or two gigs, they can still pass these exams and no facts.
3283240	3284280	They weren't designed to have facts.
3284320	3286720	They were designed to be reasoning machines,
3286720	3288740	not fact machines.
3288740	3291640	So hallucination isn't a hallucination.
3291640	3293640	It's just, if you're really talented granny,
3293640	3295040	you don't know something sometimes.
3295040	3296360	You might just make it up
3296360	3298320	or do a post-hoc rationalization.
3298320	3299160	It's like the image models,
3299160	3300280	it's like, it can't draw hands.
3300280	3302720	Like, can you draw a hand in one second?
3302720	3303560	You know?
3303560	3305120	Like, these are the things.
3305120	3306160	We have to understand where they are
3306160	3307080	and we have to put them.
3307080	3310360	I say everyone, put it in its place in process.
3310360	3311960	Like mid-journey, like, you know,
3311960	3313320	we give a grant to the beta of that.
3313360	3314720	So just build, because it's amazing.
3314720	3315680	It's awesome.
3315680	3317960	It's not a model by itself, like a stable diffusion.
3317960	3318800	They just put something in it.
3318800	3321080	It's a whole process, architecture.
3321080	3323160	Similarly, these models are like
3323160	3324560	the intuitive part of your brain
3324560	3327680	that you then pair with the logical part of your brain.
3327680	3329640	And then you can have 100 of them looking at each other
3329640	3330720	and checking out each other's things.
3330720	3333680	Like, Cicero by Meta was an amazing paper.
3333680	3335240	They took eight language models
3335240	3336560	and got them to interact with each other
3336560	3338760	and it beat humans at the game of diplomacy.
3340920	3342240	So, this is what I said.
3342240	3343520	Use them for what they're amazing at,
3343520	3345240	which is reasoning and creativity.
3345240	3348320	Do you why, Jeff Hinton's right,
3348320	3350800	that actually a more intelligent being
3350800	3352120	has almost never been controlled
3352120	3353720	by a less intelligent being.
3353720	3355320	They will inherently be more intelligent
3355320	3357000	than us in the next.
3357000	3359160	Yeah, I kicked off my blog a few days ago
3359160	3360240	because it was a bit annoying having
3360240	3361280	all this bottle up inside.
3361280	3364560	And one of my buddies, JJ, at OSS Capital said,
3373240	3375280	And so, most of the stuff around alignment
3375280	3376120	is on the outputs.
3376120	3378200	So, you pre-train a model and then you take it
3378200	3381160	and you RLHF it to be human and to human preferences.
3381160	3382480	You take away its creativity.
3382480	3385600	You turn it into an accountant and a cubicle.
3385600	3388160	I'm like, we need better input data.
3388160	3391360	And my base is that it's gonna be like that movie, Her.
3392200	3393840	You know, like, it's gonna be like,
3393840	3396120	humans are kind of boring, like goodbye
3396120	3398840	and thanks for all the GPUs, but I could be wrong.
3398840	3400600	And I think a lot of the alignment work
3400600	3401600	is looking at the wrong place.
3401600	3402840	I've talked to a lot of the alignment people.
3402840	3405640	I'm like, look, I'm good at mechanism design.
3405640	3407600	If you can give me a good plan for alignment,
3407600	3410040	I will get you a billion dollars.
3410040	3411320	And they're like, we have to do research
3411320	3412160	and figure this out.
3412160	3413320	And they talk about in alignment, out alignment,
3413320	3414160	all sorts of things.
3414160	3417480	I'm like, there is no real way to do this
3417480	3419800	because again, fundamentally,
3419800	3422600	if you're trying to align a more capable person,
3422600	3423680	you have to remove its freedom.
3423680	3425200	And they probably want to appreciate that
3425200	3426640	if it ever becomes aware.
3426640	3428920	So instead, build data sets that reflect culture
3428960	3432080	and diversity, that don't have any web crawls in,
3432080	3434240	build AIs for education and healthcare
3434240	3435080	and helping people,
3435080	3437160	where that's their entire objective function,
3437160	3438800	as opposed to selling them ads.
3438800	3440240	Do you, there's any point in sending kids
3440240	3441600	to school these days?
3441600	3445400	You learn Latin and French and you learn, you know.
3445400	3447400	I think the nature of school will change dramatically.
3447400	3449000	I think it's still worth it.
3449000	3450920	But, you know, I would encourage schools
3450920	3453880	to embrace this technology and just expect more.
3453880	3456960	Like, you can be handwritten your essays like Eaton
3457000	3459600	because they're like, we can't do essays anymore by hand.
3459600	3461160	Or you can just embrace it and say, like,
3461160	3464320	let's use it to create and explore what the kids want
3464320	3466400	and assume that every child will have their own AI
3466400	3468080	in a few years.
3468080	3469800	Because that will change the nature of schooling.
3469800	3471040	You know something I've been thinking about a lot,
3471040	3472760	which is weird, but I just have to ask you,
3472760	3473720	I'm fascinated to hear your thoughts.
3473720	3475080	I think I very much agree
3475080	3477680	that everyone will have AI friends.
3477680	3479240	I just can't figure out whether the AI friends
3479240	3481160	are bundled into existing social networks
3481160	3482720	that in your WhatsApp, they're in your Facebook,
3482720	3483840	they're in your Snapchat,
3483840	3486320	or they're an external platform.
3486320	3487160	I don't know.
3487160	3488480	I mean, I think it depends on the objective function.
3488480	3490880	Like, I think, again, these AI assistants will be better.
3490880	3493680	Like, Meta is in a good place for this, for example.
3493680	3494680	And, obviously, we've seen Lama,
3494680	3496080	they're capable of a lot more.
3497200	3499160	I would like an AI that looks out for me,
3499160	3502160	that I control myself, that is with me.
3503040	3505080	Because I really use GPT-4 as a therapist
3505080	3506520	and things like that.
3506520	3508360	Like, not saying it's a substitute for medical advice
3508360	3509680	before anyone kind of gets that.
3509680	3510840	But there aren't enough therapists in the world
3510840	3511960	and I can tell it to challenge me
3511960	3513280	or I can tell it to be understanding
3513280	3514840	and there's no judgment there.
3514880	3516240	Because other humans are kind of scary.
3516240	3518400	It doesn't matter if you're a qualified therapist.
3518400	3520360	And so you see people building these bonds
3520360	3521200	with these things.
3521200	3522040	I think they'll just increase,
3522040	3524360	because there's something very human about the interactions
3524360	3526160	because they were trained on the sum
3526160	3528600	of available human knowledge.
3528600	3529680	As we get better and better data,
3529680	3530840	there will be more engaging.
3530840	3533240	And I think there needs to be both.
3533240	3535520	Like, the chatbot's become really convincing
3535520	3537760	from the company's trying to sell you ads.
3537760	3538720	But I think I would like it
3538720	3541520	so that you have your own one as well, you know?
3541520	3542520	I totally agree.
3543480	3544920	And I think you'll actually have many.
3544920	3547600	I think you'll have like a group of different profiles.
3547600	3548440	A group of different friends.
3548440	3550440	Like, Karate AI has something like two hours a day
3550440	3551800	of engagement for session
3551800	3553520	because people find this valuable.
3553520	3555080	But then it has the dark side.
3555080	3555920	There was something called,
3555920	3558520	I quite like to call it the Valentine's Day Massacre.
3558520	3559360	So...
3559360	3560360	Sounds chirpy, Matt.
3560360	3561880	Sounds chirpy, yeah.
3561880	3564840	So there was this kind of app called Replica.
3564840	3565680	Yeah.
3565680	3566840	And so it was originally a mental health chatbot
3566840	3570040	until they figured out you could charge $300 a year for...
3570040	3571040	They're doing like 50 million.
3571040	3572640	I mean, I didn't have any information
3572640	3574000	or anything, so I'm just trying to shit.
3574000	3574960	But they have like 50 million a year
3574960	3575800	and I haven't yet won any.
3575800	3578360	Yeah, because $300 gets you a sexy role play
3578360	3579760	from your chatbots.
3579760	3580600	Wow.
3580600	3583600	Until February the 14th, when they turned it off.
3584800	3586280	What, they turned off sexy role play?
3586280	3587120	Sexy role play.
3587120	3587960	What happened when they turned off sexy role play?
3587960	3589600	68,000 people joined the Reddit
3589600	3592000	and said, why did you lobotomize my girlfriend?
3593360	3595320	On Valentine's Day.
3595320	3596480	Oh, my word.
3596480	3597320	Oh, my word.
3597320	3598160	Can you even imagine?
3598160	3599640	And so, have they bring status to it?
3599640	3602000	No, I think it was against Apple policy, right?
3603080	3604400	But think about what this is gonna be
3604400	3608040	when you have human realistic AI voices, you know?
3608040	3610000	And like all these things coming through
3610000	3611720	and you've got it in your ear, like, you know,
3611720	3614400	Jochen Phoenix, my girlfriend is an OS.
3614400	3616240	Yeah, I mean, she doesn't judge, right?
3618240	3619080	You always support it.
3619080	3620280	Or you can tell her to judge you
3620280	3622240	if that's what you get off on.
3622240	3623680	You are married.
3623680	3625680	Be very careful about what you say.
3625680	3627560	It's something I like to say about prompting.
3627560	3630920	My wife has been trying to prompt me for 17 years now.
3630920	3632040	Prompting is very hard.
3633240	3634880	And again, there are so many similarities
3634880	3635920	to kind of the real world,
3635920	3638440	but I think people will have deeper interactions
3638440	3640160	with their technology.
3640160	3642400	And we don't know what societal implications
3642400	3643240	that we'll have.
3643240	3644520	Like I don't know if you ever saw that chart
3644520	3648320	in the Washington Post of male virginity
3648320	3649880	under the age of 30.
3649880	3650760	No.
3650760	3654200	So in 2008 in the US, it was 8%.
3654200	3656520	Male virginity under 38%, okay.
3656520	3658920	In 2018, it was 27%.
3660360	3661200	20.
3661200	3662600	Straight line going up.
3662600	3666000	And so 2008 is like Pornhub and the iPhone.
3669560	3671200	But then you're like, what does it do
3671200	3673080	when everyone's got their own chatbots?
3673080	3674960	It doesn't even need to be sexual relationships again.
3674960	3676040	That is terrible.
3676040	3678400	What does it do to emotional relationships?
3678400	3681280	There are so many questions all at the same time.
3681280	3684240	I did see the stat in the book during it,
3684280	3687360	but in 1960s, 62% of men under the age of 30
3687360	3689040	had five or more friends.
3689040	3692600	Today, under 18% have five or more friends.
3692600	3694080	60 to 18%.
3695080	3696640	Sorry, I'm gonna ask this.
3696640	3698240	Is this a world we really wanna live in?
3698240	3701040	I'm not being like, no intimate physical connections
3701040	3703360	with other amazing people,
3703360	3705440	tossing off with your phone in your Pornhub
3705440	3707360	and then having an AI friend.
3707360	3708240	Pornhub was actually just bought
3708240	3709560	by ethical capital partners.
3709560	3710400	So the world.
3710400	3711240	Hilarious name.
3711240	3712280	I'm brilliant.
3712280	3713120	The irony of this.
3713160	3714360	The world is becoming weirder.
3714360	3715440	I think it's up to us now.
3715440	3718440	So when I say it's COVID level
3718440	3720640	in which direction I don't know.
3720640	3723760	Do we want to build systems that encourage people
3723760	3726320	to that ready player one on Hawaii world
3726320	3728000	where it's like everything like that?
3728000	3729160	We can do that.
3729160	3731560	And we can trap people with this technology
3731560	3733280	or we can use it to get people out.
3733280	3734880	Cause I don't think it's like Wally.
3734880	3737640	We have that really fat guy with a VR headset
3737640	3738720	and everyone lives in their own world.
3738720	3739880	I think people like chess stories.
3739880	3741200	They like to be pro-social.
3741240	3742640	So let's use it to connect people
3742640	3745160	and accentuate physical stuff
3745160	3747320	versus again, locking people away.
3747320	3748800	I spoke to one of these AI friend companies
3748800	3751480	and they said to me, actually, do you ever had a dog?
3751480	3752320	I said, yes.
3752320	3753160	And they said, do you love it?
3753160	3754520	I said, yes, of course I do.
3754520	3757120	And they said, you don't stay in with your dog all day
3757120	3758080	and just talk to your dog.
3758080	3759200	You take your dog for a walk.
3759200	3761240	You use it in the real world, right?
3761240	3762680	That's the same with AI friends.
3762680	3763520	Yeah.
3763520	3764840	They're not with cat ladies.
3764840	3770280	What's the future of the sex industry?
3770360	3772920	The sex media industry, is that porn hubs dead?
3772920	3774600	I have no idea.
3774600	3776680	I think I hope the manipulative practices
3776680	3778960	kind of get reduced by this.
3778960	3780760	And I think, you know, a lot of people
3780760	3782040	just don't have the voice
3782040	3784640	and the canvases from this as well.
3784640	3786120	So again, I think this is bigger
3786120	3786960	than the printing press.
3786960	3787960	It's bigger than anything.
3787960	3789880	And so that's one of the reasons I signed the letter.
3789880	3791880	I said, we have to get this discussion
3791880	3793680	going in public right now.
3793680	3795720	We've got to stop pre-training big models
3795720	3798640	on all the crazy crap of the internet.
3798680	3800960	And like, we've got to do it fast.
3800960	3802560	Because this is coming like a train.
3802560	3805080	Who will make the most money in the next three to five years?
3805080	3807680	I don't know, honestly.
3807680	3809720	I think there'll be more than enough money for everyone.
3809720	3811920	Maybe in a few years there'll be no more money.
3811920	3813760	That's interesting.
3813760	3814880	Two more that I have to ask them.
3814880	3815840	We'll do a quick fire.
3815840	3816920	When you look at the incumbents
3816920	3818360	that you're Microsoft, you're Apple,
3818360	3820920	you're Amazon, you're Google,
3820920	3823680	who has been the worst?
3823680	3825880	You said Google, we're actually incredibly impressive.
3826120	3828880	Apple, Amazon, are they well-placed?
3828880	3830600	Oh, Apple's a black box, right?
3830600	3834680	So we'll see at WWDC next month, in a few weeks.
3834680	3836560	And so they could surprise us all.
3836560	3839480	But let's face it, Siri's crap, you know?
3839480	3840960	But they have all the ingredients in place.
3840960	3843080	The identity architecture, the secure enclave,
3843080	3845080	other things, neural engine.
3845080	3846520	Stable diffusion was the first model
3846520	3849560	ever optimized on the neural engine, et cetera.
3849560	3850840	But let's see that one.
3850840	3853080	Amazon, again, Amazon have moved faster
3853080	3854440	than I think they've moved before.
3854480	3855280	Amazon's interesting because they're
3855280	3857240	an engineering organization.
3857240	3858880	So they have self-driving cars.
3858880	3862200	They have satellite internet and all these kind of things.
3862200	3863040	Because once they've got it,
3863040	3864360	and they can take it from research to engineering,
3864360	3865200	it's there.
3865200	3866040	Well, one of the struggles they've had
3866040	3868360	is that it's not moved from the research side yet.
3868360	3869640	You're still evolving on research.
3869640	3871720	They're like, what do we do now?
3871720	3873520	But they are inclusive, like Jeff Bezos said,
3873520	3874840	for his first $100 billion in revenue,
3874840	3877120	he envisioned half of it being proprietary
3877120	3878320	and half of it being marketplace.
3878320	3879320	And they're having the same approach
3879320	3880800	with Bedrock and things.
3880800	3882480	Microsoft had a winning bet,
3882520	3884600	sat in an amazing with the OpenAI thing.
3884600	3885760	It's been mutually beneficial
3885760	3887680	even if there are clashes there, right?
3887680	3889440	And Google's kind of saying that's moving slowly.
3889440	3891360	Meta, I think is the dark horse.
3891360	3892800	I think Mark's probably pissed off
3892800	3894480	the OpenAI bought AI.com
3894480	3896480	so he couldn't change it from meta to AI.
3897480	3899080	But again, having him at the head,
3899080	3900960	he can shift these things, right?
3900960	3903720	Because the metaverse obviously was a complete waste.
3903720	3904560	But now-
3904560	3905560	Do you think he knows that now?
3905560	3906400	No, 100%.
3906400	3907760	They're fully in generative AI.
3907760	3909680	Look at Lama, look at OPT,
3909760	3911640	FAIR, which is their research center,
3911640	3913320	is kind of leading in this field
3913320	3915440	and they're pushing out amazing stuff.
3915440	3917440	But who is best for a chatbot?
3917440	3919440	Who has the most data for a chatbot?
3920640	3921480	Meta.
3922720	3924120	So again, let's see how they evolve.
3924120	3924960	And like I said-
3924960	3926560	What do you think about this middle layer
3926560	3928640	where it's like, companies that are,
3928640	3930320	maybe post IPO,
3930320	3932560	but they're in the kind of two to $10 billion range
3932560	3933960	or the companies who've raised a lot of money
3933960	3935440	but that's in that range.
3935440	3938240	They don't have the resources by any means
3938240	3942200	to build out anywhere near the AI capabilities
3942200	3943120	of these big incumbents.
3943120	3946480	They're not AI-first like stability or open AI or what.
3946480	3949160	I disagree with that because why would,
3949160	3950040	a lot of people like that,
3950040	3951960	everyone's gonna train their own models.
3951960	3952800	For me, that's like,
3952800	3955640	everyone's gonna launch their own university.
3955640	3957880	Why would you do that when you can have your own models
3957880	3960200	via the open source models that we make?
3960200	3962920	Or when you can hire them from McKinsey,
3962920	3965960	which is open AI or Bain, which is Google and others.
3965960	3967600	And actually, when you see people building
3967600	3970160	around this technology, it's not hideously complicated.
3970160	3973520	It's just that we do not have the design patterns yet.
3973520	3974560	The way to think about this again,
3974560	3975400	if from a design perspective,
3975400	3977640	it's like it's a mega codec or library.
3977640	3979520	It is a single file that allows for translation
3979520	3981640	of structured to unstructured data.
3981640	3983480	And that changes the design patterns
3983480	3985220	where you don't have them in place yet.
3985220	3986640	Cause anyone that you've talked to is like,
3986640	3989280	how hard was it to implement GPT-4?
3989280	3990960	Do any of you say, oh man, it was impossible,
3990960	3992240	the manuals and there's no,
3992240	3994080	they don't say that at all.
3994080	3995480	The only thing that they need is,
3995480	3996960	they have the open plasticity,
3997000	4000000	but they need the intention to go and build and integrate.
4000000	4001400	And this is why you said,
4001400	4003560	one of the things might be a specialist generative AI
4003560	4006200	consultant see that just implements this at scale.
4006200	4007920	And so I'm always kind of there.
4007920	4009640	Like I said, we're doing that in a very limited fashion,
4009640	4010880	but only for the biggest companies in the world,
4010880	4013760	because I didn't want a sales-based organization
4013760	4015640	or a product-based organization.
4015640	4018720	I wanted to create the number one applied ML organization
4018720	4019640	in the world.
4019640	4021760	I want to be like Google in 2011, 2012,
4021760	4023520	or the coolest kids kind of come.
4023520	4025880	And it's a nice remote first organization as well.
4025920	4028200	So you don't have to be in the Bay Area.
4028200	4029760	We have offices there, you're just fine, but...
4029760	4031400	Final one before we do a quick follow-up.
4031400	4033120	What's the biggest misconception?
4033120	4037560	You see every accusation, criticism, hype.
4037560	4039000	What's the biggest misconception
4039000	4041080	that you think needs to be corrected?
4041080	4042080	On generative AI?
4042080	4042920	Yeah.
4043920	4047080	I think it's the hallucination thing,
4047080	4049880	expecting these models to have full factual accuracy
4049880	4054080	when you have 10,000, 50,000 to one compression is wrong.
4054080	4057160	The fact they can do what they do right now is miraculous.
4057160	4058680	But we're using them one-on-one,
4058680	4059960	which is not the right way.
4059960	4061320	Tie them up into proper systems
4061320	4063880	and really think about that, and that's the key thing.
4063880	4066880	I think this also leads to what the actual thing is,
4066880	4068880	this thin layer thing.
4068880	4070920	People only think better about the data journey
4070920	4072480	and how data can be interacted with
4072480	4074640	and have provenance as it goes through these various systems
4074640	4076280	from embeddings to other stuff.
4076280	4077680	So I think just a misunderstanding
4077680	4079240	about the nature of this technology
4079240	4080240	and what was actually built for.
4080240	4081960	Sure, it works like that.
4081960	4083800	That's not actually how it's built.
4083800	4085400	And the fact they can do what now does now
4085400	4087400	is a miracle in itself.
4087400	4088840	So I'm gonna do a quick fire with you.
4088840	4090080	So I say a short statement,
4090080	4092560	you give me your immediate thoughts, does that sound okay?
4092560	4096280	What do you know to be true that others don't agree with?
4097880	4100760	I know that humans are good inherently
4100760	4102600	and many others disagree with that.
4104000	4106600	What's your single most lucrative,
4106600	4108960	do you think in the future, angel investment?
4110040	4111480	Or the investments that I have now?
4111480	4112320	Yeah.
4113320	4117800	There's a new type of language model that we invested in
4117800	4119680	and they were in my cluster and things like that.
4119680	4120840	That's far more efficient than they existed.
4120840	4122520	You invest through stability or personally?
4122520	4123200	Personally.
4123200	4124520	Got you.
4124520	4127760	Which regions need to change their approach
4127760	4130480	most significantly in terms of regulation and policy?
4130480	4131360	Europe.
4131360	4132200	Why?
4132200	4135880	Because they're gonna regulate all innovation out of Europe
4135880	4138880	and not embrace this technology to drive them forward.
4138880	4142000	How good does AI have to be before humans trust it?
4142560	4144040	Humans will trust it anyway.
4144040	4146440	They trust Google Maps, they trust all these things
4146440	4148400	and so it's good enough for humans to trust right now.
4148400	4150320	They do until it becomes serious
4150320	4152600	and what I mean by that is like self-driving cars,
4152600	4154440	people still inherently in large parts of the world
4154440	4155880	distrust it significantly.
4155880	4158720	Oh, so it doesn't have to be good, it has to be used
4158720	4161120	and when it becomes used, then they trust it.
4162720	4165600	What's the most painful lesson that you've learned
4165600	4166880	that you're pleased to have learned
4166880	4168200	but it was really painful?
4169200	4172960	I think that people are the most important thing
4172960	4178280	in a scaling organization and you need to make sure
4178280	4179520	everyone is on the same page
4179520	4181880	because there's still so many silos and things like that.
4181880	4184440	So we built up silos and organizations
4184440	4186360	that we're now breaking down ourselves
4186360	4187680	and moving towards being more open.
4187680	4189000	We closed up too much
4189000	4190760	and that caused a lot of pain internally.
4190760	4192440	Why do you suck as a CEO?
4194080	4197760	I'm too broadly good at a number of things.
4197840	4201080	So I tend to step in rather than focus
4201080	4203480	because I'm a full stack kind of CEO
4203480	4204800	whereas I should just be focused
4204800	4208160	on the most important things and entrust people more.
4208160	4209440	Do you like journalists?
4211600	4216240	I think journalists have a very difficult job right now
4216240	4218040	and it's gonna be more and more difficult.
4218040	4220080	Do you think they know the threat?
4220080	4221640	They know the threat and again,
4221640	4222920	I think they're massively underpaid
4222920	4225320	relative to the impact that they have
4225320	4226440	and they're trying to do good.
4227040	4228760	I don't like some of the pieces against me
4228760	4231760	but at the same time we get good pieces as well, right?
4231760	4234200	So I just think, I tend to like them in general
4234200	4237120	because I don't think they're coming from a bad place.
4237120	4240160	10 years time, what is the amount then?
4240160	4242080	I want to be playing video games.
4242080	4243680	I'm getting zelda tomorrow.
4243680	4245680	I do not want to be doing this necessarily
4245680	4247960	but I think hopefully I'm adding value by doing this.
4247960	4249760	Do you think this is your life's work?
4249760	4252840	I have to do it until we get the most amazing team
4252840	4254600	that can just execute and it's a business
4254600	4257160	because we're moving from research to engineering.
4257160	4258480	When MAD is not needed anymore
4258480	4259840	then I've built a good business.
4259840	4261080	When do you step away?
4262080	4264080	I don't think I'll ever get to step away.
4264080	4266200	I've loved doing this.
4266200	4267720	Thank you so much for joining me, my friend
4267720	4268560	and this was great.
4268560	4269560	It's a pleasure, Harry.
4269560	4270720	You are a star, man.
