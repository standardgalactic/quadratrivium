WEBVTT

00:00.000 --> 00:04.960
AI is going to bring a new renaissance for humanity, a new form of enlightenment if you want,

00:04.960 --> 00:09.920
because AI is going to amplify everybody's intelligence. It's like every one of us will

00:09.920 --> 00:14.560
have a staff of people who are smarter than us and know most things about most topics,

00:14.560 --> 00:20.240
so it's going to empower every one of us. Jan, I am so excited for this. I had so many great

00:20.240 --> 00:24.240
things from our mutual friends, obviously David Marcus and then Mathieu at Photoreal Room. So

00:24.240 --> 00:29.360
thank you so much for joining me today. And it's a pleasure. Now, I would love to start. I heard

00:29.360 --> 00:33.840
some of the early stories, but I want to start with one from David Marcus. How did you first

00:33.840 --> 00:40.080
enter the world of AI and make that first foray? I was still an undergraduate engineering student

00:40.080 --> 00:46.240
in France, and I stumbled on a philosophy book, which was a debate between Jean Piaget,

00:46.240 --> 00:51.600
you know, the cognitive psychologist, and Noam Chomsky, the famous linguist. And they were

00:51.600 --> 00:58.000
arguing about nature versus nurture for language, whether language is acquired or innate. So Chomsky

00:58.000 --> 01:03.760
was on the side of innate and Piaget on the side of acquired with, you know, some innate structure.

01:03.760 --> 01:08.320
And on the side of Piaget was a guy called Seymour Papert, who was a professor at MIT.

01:08.960 --> 01:13.280
In his argument, he talked about something called the perceptron, which was an early

01:14.320 --> 01:19.040
machine learning system. And I read this and discovered that people had been working on

01:19.840 --> 01:25.280
machines that could learn and I was fascinated and I started digging the literature, soon discovered

01:25.280 --> 01:31.920
that much of that literature was in the 1950s and 60s and basically stopped in the late 60s because

01:31.920 --> 01:39.600
of a book that they killed it. And Seymour Papert was a co-author of that book. So strangely enough,

01:39.600 --> 01:46.400
and here it was 10 years later, actually praising the perceptron as kind of an amazing concept.

01:46.400 --> 01:51.840
So I was hooked. I, you know, started getting interested in what was not yet called machine

01:51.840 --> 01:55.040
learning, but eventually became neural nets and now deep learning.

01:55.040 --> 01:59.120
Can I ask you, David asked this as well, how long did it take to get,

01:59.840 --> 02:04.160
in terms of like the major breakthroughs, how long did it take you to get to the major breakthroughs

02:04.160 --> 02:09.360
that you're at the origin of when you look back over that time to get to those major breakthroughs?

02:09.360 --> 02:14.400
Well, so there's a few breakthroughs. So the first one was in the, when I was still on

02:14.400 --> 02:20.240
Autobahn, basically finishing my engineering studies, I figured out that the way forward to

02:20.240 --> 02:25.200
kind of lift the limitations of the old systems that were abandoned in the 60s was to find

02:25.200 --> 02:30.160
learning algorithms that could train multilayer neural nets, essentially. And people had all

02:30.160 --> 02:38.800
but abandoned this type of research, except for a handful of people in Japan. And one guy I heard,

02:39.440 --> 02:46.160
I heard about, called Jeff Hinton, who had published a paper in 1983. So this was just

02:46.160 --> 02:51.920
the year I graduated on something called the Boston machine, which was clearly a method to

02:51.920 --> 02:59.120
go beyond those limitations. And so I had on my side kind of developed a method for training

02:59.120 --> 03:03.520
multilayer nets, which was very close to what we now call back propagation, but not exactly the

03:03.520 --> 03:09.520
same. It was closer to what we call target prop, actually, nowadays. And then, you know, published

03:09.520 --> 03:15.920
a few papers in French and eventually met Jeff at a meeting in France in 1985. And we

03:15.920 --> 03:19.680
realized we've been working on the same thing and we're seeking a like. And, but I was, you know,

03:20.320 --> 03:25.680
in the middle of my PhD, and he was an associate professor at Carnegie Mellon. So we started,

03:25.680 --> 03:29.520
you know, a discussion and then, you know, visited him at Carnegie Mellon for a summer

03:29.520 --> 03:34.240
school reorganized. And then I, when I finished my PhD, I did a postdoc with him and then John

03:34.240 --> 03:39.360
Bell Labs. And, and when I was in Toronto, I developed what was called convolutional nets.

03:39.360 --> 03:45.600
Now, convolutional networks, which, you know, is major method for image and speech processing.

03:45.600 --> 03:52.240
Now it is. And so that's, that's what I'm best known for. But, but it started much earlier.

03:52.240 --> 03:59.440
I have to ask, Joshua described kind of the, the hype cycles within AI and neural nets like

03:59.440 --> 04:05.760
deserts when you're not in them. And he asked the question, how did you not get discouraged

04:05.760 --> 04:10.960
when for a solid decade, we were in a desert where no one really cared about neural nets?

04:10.960 --> 04:15.680
How did you keep the enthusiasm bluntly when, as Joshua said, no one really cared?

04:16.480 --> 04:21.920
Both Joshua and Jeff and I had in the back of our minds that those methods would eventually come to

04:21.920 --> 04:28.000
the fore and that, you know, we would have to kind of snap people out of their preconceived ideas

04:28.000 --> 04:33.360
about, about neural nets. So yes, there was. So you're trying, we're actually working together at

04:33.360 --> 04:39.040
AT&T Bell Labs in the early 90s. And then the interest of the community for those methods

04:39.040 --> 04:45.040
started waning around 1995 or so. And it was indeed about 10 years when not only nobody was

04:45.040 --> 04:49.200
interested in neural nets, but people were even making fun of it, you know, talking about it in

04:49.200 --> 04:56.800
sort of disparaging terms. Now there is something though, in 1996, I kind of changed job. I stayed

04:56.800 --> 05:02.160
in the same company, I was still working at AT&T in the research labs, but I became a department head

05:02.160 --> 05:07.040
and this was the early days of the internet. And my group and I started working on something

05:07.040 --> 05:11.280
completely different that had nothing to do or not much to do, at least with machine learning,

05:11.280 --> 05:17.200
this image compression. I had this, this idea that with the internet coming up, we should have a

05:17.200 --> 05:22.480
way of scanning existing paper documents and then, you know, put them on the internet so that

05:22.480 --> 05:27.520
everybody could, could have access to them. And so I worked on this for five or six years together

05:27.520 --> 05:32.800
with Leon Botou, who had been a long, long term collaborator. Joshua was also involved

05:33.600 --> 05:39.360
peripherally and a bunch of other people, Patrick Hefner, et cetera. And that project ended when

05:39.360 --> 05:45.760
all of us basically left AT&T. That's when I restarted working on deep learning and Jeff also

05:45.760 --> 05:50.800
kind of came back to Canada. He had been in the UK for a while and Joshua, Jeff and I decided in

05:50.800 --> 05:58.000
the early 2000 to basically start a conspiracy to, you know, revive the interests of the community

05:58.000 --> 06:07.200
in neural nets by making them work, discovering new algorithms. And, you know, it took almost 10

06:07.200 --> 06:14.000
years, but it succeeded beyond the wildest dreams, basically. So I'm going to ask you a range of

06:14.000 --> 06:20.320
varying questions in terms of depth, breadth, and kind of obvious and non-obvious. So forgive me

06:20.320 --> 06:24.640
if some are obvious. I just want to ask, when I hear the historical context there from you,

06:24.640 --> 06:30.480
over many decades, how do you feel today when we look at what's happening today?

06:30.480 --> 06:36.880
Are we at a new inflection point in development? Or is this merely the continuation of what we've

06:36.880 --> 06:44.240
seen for many decades? It's a combination of the two. So on the one hand, a lot of what we see

06:44.240 --> 06:51.200
today, when you are kind of down in the trenches of research, looks at a logical extension. I was

06:51.200 --> 06:58.080
not as enthralled by the sort of recent progress as the public was because, you know, I've seen

06:58.080 --> 07:01.920
this progress happening over the last several years. Now, there are things that have been very

07:01.920 --> 07:08.480
surprising. The fact that self-supervised learning methods applied to transformer architectures

07:09.600 --> 07:14.000
work amazingly well and it worked, you know, way beyond what we could have expected. The fact that

07:14.000 --> 07:20.800
we can do basically train systems to understand language, translate language in multiple languages,

07:20.800 --> 07:24.720
and then continue text if you're trying them to do this, or answer questions if you're trying

07:24.720 --> 07:30.800
them to do this, works amazingly well to an extent that people didn't quite expect that

07:30.800 --> 07:35.440
was going to happen by just making them bigger and training them on more data. So that certainly

07:35.440 --> 07:41.200
has been surprising for everybody, but that revolution occurred two years ago. Whereas the

07:41.200 --> 07:48.400
wider public has learned about it through a tragedy that was made available for us. It's

07:48.400 --> 07:53.840
been more continuous. And you see this in, you know, a lot of marking events in technological

07:53.840 --> 08:00.160
progress or in AI, in particular, are marked by kind of splashy events that the public pays

08:00.160 --> 08:05.120
attention to. But too many of us, like it looks like more like a continuous thing. And generally,

08:06.080 --> 08:12.400
what those progress require is a bunch of people to take the techniques that already exist, push

08:12.400 --> 08:17.040
them a little further, do a bit of engineering, and then make a demo that demonstrates that it

08:17.040 --> 08:23.920
works. So that was the case for, you know, DBlue, the chess player that IBM built in the mid-90s,

08:23.920 --> 08:29.760
that beat Gary Kasparov, you know, same thing with the DARPA Grand Challenge, that Sebastian

08:29.760 --> 08:35.840
Sointim at Stanford won a card that could drive itself in the desert, right, for a hundred miles.

08:35.840 --> 08:40.720
And then, you know, AlphaGo and, you know, the IBM Geopedy, there's a number of those things,

08:40.720 --> 08:46.000
right, and, you know, which are just being the latest one. And it looks like kind of

08:46.000 --> 08:50.560
jumps when you look at it from far away. But when you're in the field, it's more like a

08:50.560 --> 08:56.240
continuous evolution. Can I ask, has there been any other surprising on the positive side,

08:56.240 --> 09:00.480
developmental things you've seen over the last year or so? You said about self-supervised learning

09:00.480 --> 09:04.400
and the efficiencies there. Is there anything else where you're like, I didn't expect it to

09:04.400 --> 09:10.160
go as well as it has done in the last year? Yeah, so I already mentioned it. You know,

09:10.240 --> 09:17.040
the fact that merely training a language model to predict the last word in the sequence of words,

09:17.680 --> 09:22.240
if you do it properly, you get a system that has capabilities that are somewhat unexpected,

09:22.240 --> 09:27.280
and they emerge as you make those systems bigger, and you train them on

09:28.400 --> 09:33.280
larger amounts of data, that's clearly been the surprise for everyone.

09:34.160 --> 09:41.840
Now, the thing is, you know, as researchers and scientists, we're always looking for the next

09:41.840 --> 09:46.880
thing. So what I'm interested in at the moment is, you know, what goes beyond that. Like, you know,

09:46.880 --> 09:51.120
a lot of people are going to work on applications of autoregressive large language models, which is

09:51.120 --> 09:55.920
great. There's going to be a lot of, you know, products and new ways for people to do things,

09:55.920 --> 10:01.840
and it's going to be wonderful. But I've already been thinking about the next stage for the last

10:02.800 --> 10:08.960
three, four years, four, five years even, actually more, which is like, what's missing from those

10:08.960 --> 10:13.440
systems? What are your thoughts on what's missing from those systems? In that logical next step,

10:13.440 --> 10:19.360
why does that lead you in your thinking? So those systems do not have anywhere close to human level

10:19.360 --> 10:25.840
intelligence. Okay, despite what you might think, we are kind of fooled into thinking it because

10:25.840 --> 10:31.440
those systems are very fluent with language, but their ability to think, to understand how the

10:31.440 --> 10:37.440
world works, to plan, are very, very limited. And they're understanding the world is very superficial.

10:37.440 --> 10:44.160
And the reason for it is that they are strictly trained on language. And language only contains a

10:44.160 --> 10:49.680
small proportion of all human knowledge. Most of human knowledge is not linguistic at all.

10:49.680 --> 10:54.720
And all of animal knowledge is non linguistic. And we take it for granted, you know, this is the

10:54.800 --> 11:01.360
Moravec paradox, right? All the capabilities and abilities that we take for granted, like, you

11:01.360 --> 11:07.760
know, planning a motion or something or very simple things that everyone can do. A 10 year old can,

11:07.760 --> 11:13.840
you know, clear up the dinner table and fill up the dishwasher. Any 17 year old can learn to drive.

11:13.840 --> 11:16.480
We still don't have so many cars, we don't have domestic robots.

11:16.480 --> 11:22.400
If they're non linguistic, like the majority, I'm sorry for the base questions, but then what are

11:22.400 --> 11:30.080
they? And is that that we don't have able to be ingested by AI models and engines over time?

11:31.040 --> 11:35.440
Well, so first of all, there is no question that eventually AI systems will understand the world

11:35.440 --> 11:40.880
in similar ways that that humans do. There has better ways. But they will not be

11:41.520 --> 11:45.680
autoregressive large language models as a type that we're now talking about.

11:45.680 --> 11:49.680
There will be different for a number of different different reasons. But to answer your question

11:49.680 --> 11:56.160
more directly, anything that has to do with sort of an intuition of the real world requires

11:56.160 --> 12:02.640
an experience of the real world or a simulated version of it, which those large English models

12:02.640 --> 12:07.440
don't have. They're purely trained from text. So you can add you, there's a number of questions

12:07.440 --> 12:11.920
that about the physical world that they'll be able to answer because there's a template for it in the

12:12.560 --> 12:16.400
or something very similar in the data that they've been trained on. Same for planning,

12:16.400 --> 12:22.880
you can ask them to plan a trip or something and they'll adapt a template that they've been trained

12:22.880 --> 12:28.160
on. But they don't really have sort of a model of a mental model of how the world works and allows

12:28.160 --> 12:33.680
them to plan complex action sequences or use tools or things like that. Can I ask, is that why you

12:33.680 --> 12:40.160
said that AI researchers face palm when they hear prophecies of doom? No, that's a different question.

12:40.160 --> 12:44.080
Those are kind of orthogonal concepts. So I mean, there is some some weak connection.

12:44.720 --> 12:51.920
There is a a flaw in a current autoreversive lens, which is that you can only control their answer

12:53.040 --> 12:59.200
in two ways. The first way is you modify the statistics of the training data that you train

12:59.200 --> 13:05.280
them on, possibly using human feedback for specific answers. And the second one is you

13:05.280 --> 13:10.560
change the point and the combination of the point that, you know, the question you ask them,

13:10.560 --> 13:15.680
the form in which you ask the question and the statistics of the training data entirely determines

13:15.680 --> 13:23.280
the answer to the system we produce. So there is no persistent memory, first of all. But second of all,

13:24.160 --> 13:30.080
you cannot control the system. You cannot impose constraints on it, like be factual, be understandable

13:30.080 --> 13:34.640
by a certain year old. You can try to put this in a prompt, but then, you know, you rely on

13:35.200 --> 13:40.400
whether the statistics of the training data is appropriate for for taking taking that into account.

13:40.400 --> 13:45.680
There's no direct way to constrain the answer of those systems to satisfy certain objectives.

13:45.680 --> 13:51.360
And that makes them very difficult to to control and steer. And so that creates some fears because

13:51.360 --> 13:55.680
people are kind of extrapolating. If we let those systems do whatever we connect them to

13:55.680 --> 14:00.240
internet and they can do whatever they want, they're going to do crazy things and stupid things

14:00.240 --> 14:04.320
and perhaps dangerous things. And we're not going to be able to control them. And they're going to

14:04.320 --> 14:07.680
escape of control. And they're going to become intelligent just because they're bigger, right?

14:08.560 --> 14:13.760
And that's nonsense. First of all, because this is not the type of system that we are going to give

14:13.760 --> 14:19.680
agency to the systems that will eventually be given agency that are going to be able to plan

14:19.680 --> 14:23.680
sequences of actions, our systems are going to have objectives that they're going to have to

14:23.680 --> 14:28.400
satisfy. And because of those objectives, they're going to be controllable. So they're going to be

14:28.400 --> 14:32.960
much more controllable than the current systems. Okay, so my prediction is that within a few years,

14:33.520 --> 14:39.040
nobody in their right mind would use autoregressive LMS, they'll go away in favor of something more

14:39.840 --> 14:44.240
sophisticated and controllable, they can plan its answer as opposed to just produce one order

14:44.240 --> 14:49.440
after the other, reactively. Okay, that's that's the first fallacy. The second fallacy is that

14:49.440 --> 14:58.480
there is this idea somehow that the desire to and the ability to dominate is linked with intelligence,

14:58.480 --> 15:03.440
right? So this is a statement that a lot of people are are making, including, you know,

15:03.440 --> 15:09.040
my friend, Jeffington recently, that somehow as soon as the machine becomes intelligent,

15:09.040 --> 15:14.640
it becomes uncontrollable because, you know, it's it being smarter than us, it can influence us in

15:14.640 --> 15:22.560
ways that we can even imagine. Now, I think this is a gigantic fallacy, because even within the

15:22.560 --> 15:30.160
human species, it is not the smartest among us that want to dominate the others. Okay, to dominate

15:30.160 --> 15:36.320
other entities, you don't necessarily need to be smarter than them, we need to want to dominate

15:36.320 --> 15:43.120
them. This is not something that every intelligent entity is going is going to do spontaneously.

15:43.920 --> 15:50.720
We do it as humans, because the desire to influence others was built into us by evolution,

15:50.720 --> 15:57.360
because we are a social species. Okay, same as baboons and chimpanzees and wolves and dogs and

15:57.360 --> 16:01.680
etc. It's not the case for orangutans. orangutans don't have the desire to dominate anybody,

16:01.680 --> 16:07.120
because they are non-social animals, they are solitary animals, they are territorial, in fact.

16:07.120 --> 16:15.360
So, we need to separate those two concepts, the will, the desire and the ability to dominate on

16:15.360 --> 16:19.760
one hand and intelligence on the other hand. The fact that we're going to have super-intelligent

16:19.760 --> 16:26.400
machines at our disposal means that every one of us is going to be like a business leader,

16:26.400 --> 16:30.720
a politician or an academic with a staff of people working for them that are more intelligent than

16:30.720 --> 16:39.360
themselves. I mean, it's great. It's not like if you feel threatened by being the boss of other

16:39.360 --> 16:43.360
people who work with you, but are smarter than you, you're not being a good leader.

16:44.160 --> 16:49.760
Can I ask you, how do we instill values within models where they don't have a desire to dominate?

16:50.560 --> 16:54.480
Right, so these objectives I was telling you about. So, okay, let me describe the

16:55.360 --> 16:59.440
architecture of future AI systems as I see it. We're going to have AI systems that

17:00.160 --> 17:06.400
basically are going to plan their actions and actions can include sequences of words that you

17:06.400 --> 17:12.880
tell someone, but they're going to plan the sequence of actions or words so as to optimize

17:12.880 --> 17:19.520
a series of objectives that we set them. So, one objective is, does this answer the question I just

17:19.520 --> 17:24.640
asked? Another objective might be, well, you're talking to a 13-year-old, make that answer

17:24.640 --> 17:31.680
understandable by a 13-year-old. Another objective might be, I asked you to answer a question about

17:31.680 --> 17:38.880
the world, so be factual. Or it's a question about yesterday's political event. Can you kind of be

17:38.880 --> 17:43.920
compatible with everything you've read in the press this morning? Things like that, right?

17:45.200 --> 17:51.440
So, you'll have those systems that have a series of objectives and their output, their answer,

17:52.320 --> 17:59.680
by construction, is going to have to satisfy those objectives. And some of those objectives

17:59.680 --> 18:06.880
will be hardwired to make those systems safe. Like, if it's a domestic robot that can cook dinner

18:06.880 --> 18:11.760
and can wield a kitchen knife in its arm, there's going to be a term in there that says,

18:11.760 --> 18:15.600
like, stop moving your arm when there's people around because you might hurt them. So, that's

18:15.600 --> 18:19.920
going to be an objective that the system cannot violate because by construction, we're going to

18:20.000 --> 18:26.160
have to satisfy them. So, that's the way to build safe AI system. You make them produce answers that,

18:26.160 --> 18:30.240
by construction, have to satisfy objectives. And you design those objectives so that their

18:30.240 --> 18:36.400
actions are safe. Now, how precisely to do this is not a completely solved question, but you try it,

18:36.400 --> 18:41.520
you deploy it at a small scale, you see what the effect is, and you correct it when it doesn't work,

18:41.520 --> 18:47.840
and you fix it progressively. And it's not like if you get it wrong, it's going to destroy humanity.

18:48.400 --> 18:55.120
That depends on that cooking robot, you never know. How do you determine who's able to set

18:55.120 --> 18:59.520
the objectives? Because there could be right or wrong depending on who sets them.

19:00.160 --> 19:06.640
That's true. So, that's going to have to be a process by which we allow people to do this,

19:06.640 --> 19:12.320
some vetting process. The same way that there's a vetting process for people to take care of

19:13.040 --> 19:19.600
your health or cut your hair, fix your plumbing or your car, right? So, there's some vetting

19:19.600 --> 19:26.240
process, certainly some testing and market deployment procedure with regulating agencies

19:26.240 --> 19:31.360
for things that are potentially dangerous, probably not for all applications, but for

19:31.360 --> 19:36.160
many applications, certainly in healthcare transportation and things like that. And then

19:36.240 --> 19:43.040
perhaps also, it could be that, let's take the example of intelligent assistance. So,

19:43.040 --> 19:50.480
let's imagine a future where everyone can talk to their intelligent assistant. That system will

19:50.480 --> 19:55.280
have pretty close to human-level intelligence for probably more accumulated knowledge than most

19:55.280 --> 20:01.200
humans. They could translate in any language and give you a quick summary of yesterday's

20:01.200 --> 20:06.240
newspaper and things like that, right? Explain mathematical concepts to you, things like that.

20:06.240 --> 20:12.400
So, people are probably going to use this almost exclusively in the future for their interaction

20:12.400 --> 20:16.720
with the digital world. You're not going to go to Google or Wikipedia, you're just going to talk

20:16.720 --> 20:22.560
to your assistant. And the only way to do this properly is for the base infrastructure for those

20:22.560 --> 20:28.960
assistants. I mean, they would be so pervasive, so much will ride on those systems that I don't

20:28.960 --> 20:37.760
think anyone will accept that those assistants being behind an event horizon in a private company.

20:37.760 --> 20:42.240
They will insist that the infrastructure is open. They will insist also that the vetting

20:42.240 --> 20:47.520
process by which those systems are trained be something maybe like Wikipedia, right?

20:47.520 --> 20:51.920
We tend to trust Wikipedia, sometimes with a grain of salt, but we tend to trust Wikipedia

20:51.920 --> 20:56.400
because there is a vetting process so that whenever an article is modified, you know,

20:56.400 --> 21:00.800
some editor kind of check on it and then the changes are accepted or not, things like that.

21:00.800 --> 21:06.320
So, you can imagine that the sort of common repository of all human knowledge that would be

21:06.320 --> 21:11.040
our assistants will be constructed through some sort of cross-sourcing process, perhaps similar

21:11.040 --> 21:15.600
to Wikipedia, where you're going to have a bunch of people training those systems and fine-tuning

21:15.600 --> 21:20.000
them so that, you know, whatever they and so they produced are correct.

21:20.000 --> 21:25.760
It's so funny you say about that kind of the benefit set of the open approach over the closed

21:25.760 --> 21:29.920
approach because that's where I've been kind of stuck, which is like, where does value accrue?

21:29.920 --> 21:34.400
Is it to the closed model or the open model? And then we had the leaked internal Mamo stay

21:34.400 --> 21:39.200
from the Google employee who said, you know, we're not ahead, open AI are not ahead, there's this

21:39.200 --> 21:46.480
third being which is actually far more significant and we haven't taken notice of and summarized.

21:46.480 --> 21:52.080
And that was triggered, that was triggered by by Lama, which is the model that was put together

21:52.080 --> 21:59.600
by my colleagues at FAIR, which was the code was open sourced. The model sadly

22:01.040 --> 22:04.480
was distributed only for research and non-commercial purpose.

22:05.840 --> 22:14.000
And the reason for that is basically complicated legal issues of what's the status of the data

22:14.000 --> 22:17.600
that the system has been trained on and things like that. So, it's more kind of,

22:18.560 --> 22:24.800
it's not a lack of desire from the from meta to open source. It's more kind of complex legal

22:24.800 --> 22:33.600
issues that go beyond my. I'm super naive, Jan. Why does open win against a more controlled,

22:33.600 --> 22:41.360
tight-knit, well-funded open AI or other large corporate with a big balance sheet and a very

22:41.440 --> 22:45.920
rigorous but streamlined team? It's very simple. It's because no outfit

22:47.040 --> 22:53.040
as powerful as they may be has a monopoly on good ideas. So, if you do it in the open,

22:53.840 --> 23:00.480
you basically recruit the entire world's intelligence to contribute to things and

23:00.480 --> 23:06.400
having ideas and ideas that you met as, you know, sorry about, which, you know, an outfit with 400

23:06.400 --> 23:12.240
people has no chance thinking about or even a large company with 50,000 employees may not want to

23:12.240 --> 23:18.880
devote any resources to because they may not think it's useful in the long term or they have,

23:18.880 --> 23:24.560
you know, more urgency to take care of. So, you give it away and then you have, you know,

23:24.560 --> 23:28.160
tons and tons of people, some of whom are, you know, undergraduate students or people,

23:28.160 --> 23:33.120
you know, in their parents' basement. So, coming up with amazing ideas that you would

23:33.120 --> 23:38.480
never have thought about or willing to spend the time to crunch down the, you know,

23:38.480 --> 23:45.760
7 billion weight llama so that it runs on the Mac, on the laptop. Like, oh, that's pretty amazing.

23:45.760 --> 23:53.280
So, I think that's why, you know, open-source projects succeed, particularly when they concern

23:53.280 --> 23:58.880
basic infrastructure. So, if you think about it, the early days of the Internet, there was a battle

23:58.880 --> 24:03.040
between Microsoft and SunMicroSystems to provide the basic infrastructure for the Internet.

24:04.080 --> 24:08.000
You know, the operating system, the web server, you know, things like that, right?

24:09.920 --> 24:14.880
So, on SunMicroSystems, it was Solaris and, you know, whatever web server and Java,

24:14.880 --> 24:19.680
and then on the Microsoft side, there was Windows with IIT or whatever, you know,

24:19.680 --> 24:26.720
an ASP, which was their kind of server and client-side protocol. Both of them lost.

24:27.360 --> 24:32.320
In fact, SunMicroSystem pretty much went bankrupt and was, you know, sold for parts to Oracle.

24:32.320 --> 24:37.360
One was Linux and Apache, which is completely open-source. And you might ask, why? You know,

24:37.360 --> 24:42.240
the entire Internet and the entire tech industry runs on Linux, right? And your phone probably

24:42.240 --> 24:47.440
runs on Linux, too, if you have Android. So, that's three-quarter of the phones in, you know, in the

24:47.440 --> 24:53.040
world. So, the reason for this is that, you know, it's just a much better way of gathering

24:53.040 --> 25:00.000
competence and talent around a common project, even if it's not motivated necessarily by profit.

25:00.800 --> 25:08.080
Yeah, I agree, and I love this. You work with matter. My question and David Marcus' question was,

25:08.080 --> 25:09.520
how does matter win, then?

25:10.480 --> 25:16.720
So, it's been the case that Meta in the past has open-source pretty much everybody,

25:16.720 --> 25:22.880
everything that it's ever produced in terms of basic infrastructure, right? So, you have, you know,

25:22.960 --> 25:30.480
React for, you know, the framework for web and mobile apps. You have PyTorch. PyTorch is not even

25:30.480 --> 25:33.600
owned by Meta anymore. The ownership was transferred to the Linux Foundation,

25:34.320 --> 25:39.120
because it's so essential to the, you know, AI, R&D infrastructure nowadays. You know,

25:39.120 --> 25:45.280
ChatGPT was developed on PyTorch. Okay. All open AI runs on PyTorch. The entire world, in fact,

25:45.280 --> 25:51.360
runs on PyTorch except Google, because they have their own team, right? But it goes beyond that,

25:51.360 --> 25:58.400
right? Meta open-sources its hardware server backplane design, so that hardware manufacturers can

25:58.400 --> 26:04.480
build to its specifications. And pretty much everything, aside from sort of legal issues that

26:04.480 --> 26:10.240
are sometimes due to kind of recent laws or court decisions, pretty much everything has been

26:10.240 --> 26:15.920
open-sourced. It is not because other people can use your technology that you can't exploit it

26:15.920 --> 26:23.200
to the same extent, right? Who can use smart NLP systems for, you know,

26:23.200 --> 26:28.560
translation or content moderation on Facebook, other than Facebook? It doesn't matter if other

26:28.560 --> 26:33.520
people have access to the same technology. I mean, I totally agree with you. And this kind of led

26:33.520 --> 26:38.240
to my next question, which you actually tweaked about, which comes to the size of like data modes

26:38.240 --> 26:44.560
and size of data availability. Is it simply a case that the largest model wins? And how do you think

26:44.560 --> 26:51.760
about value in small models as well? Yeah, so it's not the case. This is really what

26:52.720 --> 26:57.600
Lama has demonstrated and really kind of shown people. So the people behind Lama,

26:57.600 --> 27:02.400
Edouard Grave and Guillaume Lompland and their collaborators, mostly at Fair Paris,

27:02.400 --> 27:06.960
actually many of them are in Paris, they've demonstrated that you don't need those models

27:06.960 --> 27:12.480
to be very large to work really well. I think it caused a bit of an epiphany for a lot of people,

27:13.120 --> 27:17.840
realizing, oh, you know, you don't need, okay, maybe you need a thousand GPUs,

27:17.840 --> 27:22.640
you know, running for 10, you know, a couple of weeks to train it, the base system. In fact,

27:22.640 --> 27:27.600
this, that number is going down too, because people are kind of figuring out how to do this

27:27.600 --> 27:31.760
more efficiently. But once it's pre-trained, you can use it for all kinds of stuff and you can

27:31.760 --> 27:37.920
fine tune it really easily. And, and then at the end, you can run it on your laptop, right?

27:37.920 --> 27:43.680
That's kind of amazing. Or maybe on a, on a, you know, desktop machine with a GPU in it or a

27:43.680 --> 27:49.680
couple GPUs. So I think, you know, it sort of opened the minds of people to the fact that there is

27:49.680 --> 27:54.240
like enormous opportunities that really weren't thought to be possible before. And I think it's

27:54.240 --> 27:59.840
going to make even more progress, because if we go towards the design of AI systems, perhaps along

27:59.840 --> 28:04.880
the lines of what I described with objectives and planning, I think those systems could actually be

28:04.880 --> 28:11.120
even smaller to some extent. How would they be even smaller? Sorry, unpack that for me.

28:11.120 --> 28:16.720
Well, because the current models, for them to work here, to train them on gigantic amounts of data,

28:17.600 --> 28:21.120
way more data than any humans has ever been trained on, right? So the amount of data I

28:21.120 --> 28:30.400
lamar is trained on, for example, is something like 1.4 trillion tokens, which is a, you know,

28:30.400 --> 28:35.120
it's like a quarter of the internet or something. It's something absolutely enormous. It would take

28:35.120 --> 28:41.520
someone reading eight hours a day at normal speed, about 22,000 years to read through that. Okay.

28:41.520 --> 28:47.440
So obviously, those systems can accumulate a lot of knowledge from text, but they don't do it the

28:47.440 --> 28:51.600
same way humans do it, because we don't need that much time to be that smart and to learn

28:52.640 --> 28:57.280
that much. So obviously, we are much more efficient or brains are much more efficient than those models

28:58.000 --> 29:02.800
at learning things. Like, how is it that a teenager can learn to drive a car in about 20

29:02.800 --> 29:07.360
hours of practice? We still don't have level five sort of cars. So obviously, we're missing

29:07.360 --> 29:12.960
something really big. And what we're missing, I think, is abilities for AI systems to learn

29:12.960 --> 29:18.880
how the world works by observation mostly, and then this ability to plan so as to satisfy objectives.

29:19.680 --> 29:26.240
And then beyond that, the ability to set some objectives in the satisfaction of a bigger one.

29:26.320 --> 29:31.920
Okay, let's go to hierarchical planning. And we do this, humans do this. Some animals do this to

29:31.920 --> 29:36.960
some extent. Every animal, you know, mammal and bird is capable of some level of planning.

29:36.960 --> 29:40.720
Autoverseveral animals basically don't do planning or a very simple form of it.

29:41.440 --> 29:46.160
Jan, you mentioned the efficiency that can come from actually smaller models than expected

29:46.160 --> 29:51.600
and how actually size of models isn't everything. The other thing, we spoke about open and closed.

29:51.600 --> 29:54.560
The other thing that I've been thinking, and everyone's been thinking about, and I've interviewed

29:54.560 --> 30:01.040
many kind of leading AI experts, and they say the value will accrue to the incumbents. Startups,

30:01.040 --> 30:04.720
they don't have the data, they don't have the models, it'll accrue to the incumbents.

30:05.360 --> 30:09.680
Is that right? Will the value accrue to the incumbents? Or do you believe that given what

30:09.680 --> 30:15.120
you just said about size not being everything in terms of models, it could be startups as well?

30:16.480 --> 30:22.960
So it depends on which scenario you believe in. So the scenario I think will happen,

30:23.040 --> 30:26.800
and I'm certainly rooting for, is the scenario I described earlier where you have some sort of

30:26.800 --> 30:33.040
open platform for base LLMs. So base LLMs basically would be seen as the basic infrastructure,

30:34.160 --> 30:42.160
like TCP, IP, Linux, Apache, essentially, completely open. And then there would be an ecosystem of

30:42.160 --> 30:46.240
companies building stuff on top of it, which for vertical applications for specific things,

30:46.240 --> 30:50.800
right, to specialize those systems for particular applications, who offer support to make it,

30:51.360 --> 30:58.240
customize for enterprise applications, for personal things. There'll be a whole economy

30:58.240 --> 31:02.800
around this, which will create jobs, by the way, not make them disappear. So this is the scenario

31:02.800 --> 31:08.080
that I believe will happen. And the reason I think it will happen is because there is essentially a

31:08.080 --> 31:14.160
need to use, essentially millions of contributions for making those systems factual and correct,

31:14.160 --> 31:19.680
and etc., so Wikipedia style. So I think the proprietary approaches will actually fall behind.

31:19.680 --> 31:24.880
So that's one point. The second point is, you can ask yourself the question, how is it that

31:25.680 --> 31:32.240
the companies that were best positioned to produce something that charge EPT, namely Google

31:32.240 --> 31:42.320
and Meta, didn't? Why is it open AI? The small ad sheet was 400 people, more now, but actually

31:42.320 --> 31:48.720
small ad sheet. And the answer is, it's not because Google or Meta did not have the competence of

31:48.720 --> 31:56.560
the technology. It's just that they didn't have the pressure to produce new products, completely

31:56.560 --> 32:03.200
new products, that had a lot of risk attached to them. And the risks were, we know where the risks

32:03.200 --> 32:09.680
are, because a few weeks before charge EPT, my colleagues at fair, produced a large language

32:09.680 --> 32:14.800
model called Galactica, which was an experimental system. And they put out a demo, and the demo

32:14.800 --> 32:21.440
was to demonstrate that. So Galactica was a large language model trained on the entirety

32:21.440 --> 32:26.640
of the scientific literature. And it was basically designed to help scientists write papers.

32:27.360 --> 32:31.920
So you would start writing a paragraph or something like that to describe the topic of

32:31.920 --> 32:36.400
paragraphs. And then Galactica would basically complete the paragraph, and it wouldn't be

32:36.400 --> 32:42.400
factually correct. You would have to kind of fix it, but you would ask it to build a table

32:42.400 --> 32:48.160
result, and it would just put the latech commands to kind of build the thing and populate it with

32:48.160 --> 32:53.040
the known results on the literature about the topic that you're working on, or you would type a

32:53.040 --> 32:57.920
chemical formula for something, and it would turn it into an actual name for it, or things of that

32:57.920 --> 33:03.360
type. Very useful for scientists. As soon as the demo was put out, it was murdered by the

33:04.000 --> 33:09.840
social network Twitter sphere. Why? People said, oh, this is going to destroy scientific

33:09.840 --> 33:16.720
publication, because now any random person can write an authoritatively sounding scientific

33:16.720 --> 33:23.920
paper that is nonsense. And there was so much material thrown at the system that the people

33:23.920 --> 33:29.280
at Meta who built it couldn't take it, they took down the demo because they said, we can't sleep

33:29.280 --> 33:33.920
at night. So here is an example of a very useful system, a system that could have been extremely

33:33.920 --> 33:39.680
useful, particularly for writers or scientific papers who are non-native English speakers,

33:40.640 --> 33:48.400
that basically was destroyed by AI do-mers. People who just did not think about the risk-benefit

33:48.400 --> 33:55.840
analysis, the risk of flooding the literature with nonsense is ridiculous. I mean, because, you know,

33:55.840 --> 34:03.200
the scientific publications are vetted and things like that, so there was not a significant danger.

34:04.880 --> 34:09.360
And then Chatchitviti came two weeks later and was welcomed as the second

34:09.360 --> 34:16.560
coming of the Messiah, right? So what does that tell you? And then, you know, a few months later,

34:16.560 --> 34:24.160
Google came out with a bard, and in the demo, a bard made a tiny, you know, minor factual mistake

34:24.160 --> 34:31.520
about some astronomical fact, and, you know, Google's stock went down by 8%. Now, what it tells

34:31.520 --> 34:36.880
you is that when something is produced by a large company that has a reputation, particularly

34:36.880 --> 34:41.520
a reputation to defend, they can put out things that's true nonsense, but it's okay for a small

34:41.520 --> 34:49.680
company. So that's the landscape of what happens now, which is why I think there's a bit of a

34:49.680 --> 34:54.400
paradox which is that the companies that have, you know, the best technology basically can't

34:54.400 --> 35:00.640
have difficulties putting it out because of those legal issues and sort of public image.

35:01.520 --> 35:04.720
Do you not also think there's this core business model challenge there, which is

35:04.720 --> 35:08.720
it's the classic innovators dilemma? Like why didn't Google do this? Because it would have killed

35:08.720 --> 35:15.040
that absolute cash cow of Google ads. The cost to service a query versus the costs of this

35:15.040 --> 35:22.400
is so significantly different. You'd be killing your core cash cow with this, with unknown upside,

35:23.360 --> 35:30.160
versus retaining what is a great business. You don't have a choice. I mean, there's no question

35:30.240 --> 35:37.200
that within some time, it could take a while, but there is no question that people interact

35:37.200 --> 35:44.480
mostly with the digital world using AI assistance. And they may run into your augmented reality

35:44.480 --> 35:53.920
glasses or something of that type. Like in the Spike Jon's movie, Her, that's not a bad depiction

35:53.920 --> 36:00.080
of what, you know, the way things could develop. And so if you take the assumption, make the assumption

36:00.080 --> 36:05.600
this is going to happen, you have to build it as quickly as you can. And it might cannibalize

36:05.600 --> 36:10.320
your news feed algorithm or whatever, or because of Google, your search engine,

36:11.120 --> 36:17.040
but you have to do it. You know, it's like, I mean, meta has been known to make those choices

36:17.040 --> 36:26.080
in the past, like the move to mobile, for example, and the move to short form video, for example,

36:26.080 --> 36:32.640
which obviously TikTok has been very successful at. Meta has entered that business in kind of a

36:33.440 --> 36:38.320
big way, despite the fact that the amount of revenue it derives from it is lower than a traditional

36:38.320 --> 36:45.200
news feed, because it's hard to put ads and videos basically. You mentioned the job creation

36:45.200 --> 36:50.400
element there. I do just want to touch on the job side, because it's the classic AI doomer that's

36:50.400 --> 36:54.160
we're all going to be unemployed, and we're going to have universal basic income in an

36:54.160 --> 36:59.760
optimistic world. You said about job creation there. We don't hear about job creation through AI.

36:59.760 --> 37:05.520
How do you see what jobs will be created through this new ecosystem and what that world of employment

37:05.520 --> 37:14.160
could look like? So 100 years ago, or maybe 120 years ago, most people in most of the world

37:14.880 --> 37:22.720
worked in the fields in food production. There's pretty much a majority of the population.

37:23.680 --> 37:26.960
Today, in developed countries, it's between one and two percent.

37:29.600 --> 37:38.400
And that has caused a migration of people into the cities and the development of service, business,

37:39.200 --> 37:45.040
you know, the same thing 20 years ago or 20, 30 years ago, there was a big movement towards

37:45.040 --> 37:50.560
automation of manufacturing. And a lot of manufacturing jobs disappeared in developed

37:50.560 --> 37:54.960
countries, but they were replaced by other things. So 20 years ago, who would have thought that you

37:54.960 --> 38:02.320
could make a living with a podcast? I didn't think I could five years ago, Jan. I'm as surprised as

38:02.320 --> 38:09.440
everyone else. Right. So, you know, a lot of jobs appear like, you know, 30 years ago, there was no

38:09.440 --> 38:14.080
such thing as web designer. And now it's, you know, have engineers in the world basically do this,

38:14.080 --> 38:19.120
right? So, you know, the number of economists that I have talked to, which is pretty large,

38:19.120 --> 38:23.600
about where I asked that question, we tell me, well, we're going to run out of jobs because,

38:23.600 --> 38:29.440
you know, we're all going to be replaced by, I think, is exactly zero. Like, no economics believes

38:29.440 --> 38:34.000
this. No economics believes we're going to run out of job because no economics believes that we're

38:34.000 --> 38:39.040
going to run out of problems to solve or requirement for human creativity and human

38:39.040 --> 38:43.360
communication and stuff like that. So, you know, this is going to create as many jobs as it makes

38:43.360 --> 38:47.520
disappear. Now, the question is, though, and those jobs, by the way, are going to be more

38:47.520 --> 38:52.240
productive. So overall, technology makes people more productive. In other words, for the same

38:52.240 --> 39:00.560
amount of hours worked, you produce more wealth, okay? But every technological revolution, unless

39:00.560 --> 39:07.360
it's accompanied by sort of, you know, political changes and social changes, generally profit

39:07.360 --> 39:13.360
a small number of people, at least temporarily, right? That happened in the industrial revolution

39:13.360 --> 39:17.440
in the late 19th century, where, you know, a few people became extremely rich and a lot of people

39:17.440 --> 39:21.760
were exploited. And then, you know, society changed. And there were like social programs and,

39:22.320 --> 39:28.240
and, you know, income tax and, and high tax for richer people and stuff like that, which the U.S.

39:28.240 --> 39:33.920
has backpedaled on this, but not Europe, or the UK to some extent, too, but not the rest of Europe.

39:35.040 --> 39:38.640
So there is a question of, you know, how you distribute the wealth if you want, okay? How

39:38.640 --> 39:42.720
do you organize society so everyone profits from it? But that's a political question. There's not

39:42.720 --> 39:47.360
a technology question. It's not new. It's not caused by AI. It's just caused by

39:47.360 --> 39:51.440
technological evolution, right? It's not a recent, a recent phenomenon.

39:51.440 --> 39:56.080
This is so unfair of me to ask. But what do those jobs look like? Like, what are they?

39:56.080 --> 40:01.120
Are they, they're creative oriented? But what does that actually mean? Like, sorry, I know it's a

40:01.120 --> 40:04.800
really hard question, but I'm just trying to understand how, how we actually spend our time

40:04.800 --> 40:10.800
in my children, which I don't have, by the way, Jan. But what, what do they do? Like sculpt or paint?

40:11.040 --> 40:11.520
I don't know.

40:13.440 --> 40:18.000
I don't know. That's a good question. But it's not because I don't know that it won't happen,

40:18.000 --> 40:23.920
because I mean, look at like how many people exercise their creative juices today, right?

40:23.920 --> 40:29.440
With all the tools that are available that, you know, weren't available 10, 20 or 30 years ago,

40:29.440 --> 40:33.360
like 3D artists or something like this, you know, game designers, you know, all kinds of things.

40:33.360 --> 40:37.760
Like, you know, I think creative jobs are the other ones. So there are two types of jobs that

40:37.760 --> 40:42.400
that, you know, have a bright future of creative jobs, whether they are scientific, technical,

40:42.400 --> 40:47.520
educational, or artistic. ACI has to do with communication, right? And communication of

40:47.520 --> 40:52.480
human emotions, which is, you know, intrinsically human, if you want. So that's one category.

40:52.480 --> 40:57.280
And then the other one is personal services. So where you need, you know, actual people

40:58.080 --> 40:58.880
to interact with you.

40:59.680 --> 41:05.120
I totally agree and get you. And I love, I love that we shall see class. The only thing that I

41:05.120 --> 41:09.840
worry about is like the speed of transition. Like when you look at past industrial revolution,

41:09.840 --> 41:13.760
when you think even the introduction of PCs into kind of, you know, working environments,

41:13.760 --> 41:21.040
these were multi decade introductions. Blundly, what AI feels like in some industries today,

41:21.040 --> 41:26.800
we use it at the media company, and it's cutting our employee like the speed of transition is much,

41:26.800 --> 41:33.440
much more compressed in this timeline, which will lead to short term significant high unemployment.

41:33.440 --> 41:36.160
Do you concede that or do you not concede that?

41:36.160 --> 41:42.080
So this is something I used to be very worried about, that the speed of progress of technology

41:42.080 --> 41:48.160
was going to leave a certain number of people behind who, you know, cannot be basically retrained

41:48.160 --> 41:53.680
fast enough or maybe they're too old to retrain themselves for the new, the new world. I was

41:53.680 --> 41:57.760
worried about this. And then I talked to a bunch of economists and they say, oh, you know, not

41:57.760 --> 42:05.040
really because the speed at which a technology disseminate in the economy is actually limited

42:05.680 --> 42:11.280
by how fast people can learn to use it. So a good person to talk to about this is Eric

42:11.280 --> 42:17.280
Binobsen at Stanford. And what he says is that when a new technology is introduced, let's say the

42:17.280 --> 42:22.320
PC, right, with, you know, graphical user interface, the mouse, et cetera, right, in the mid 90s,

42:23.280 --> 42:30.720
how long did it take to have a measurable effect on productivity, you know, which is the amount

42:30.720 --> 42:36.000
of wealth produced by per hour worked. He says, you know, typically it's 15, 20 years. And the

42:36.000 --> 42:40.080
reason is that that's what it takes for people to learn to use that new technology basically.

42:40.640 --> 42:45.280
But you buy that here, like people are pretty good at prompts, you know, social media content

42:45.280 --> 42:50.640
managers are using prompts very efficiently to produce content plans, to create content ideas in

42:51.440 --> 42:57.440
under half an hour after watching a couple of TikToks. Yeah. But like, what is going to be the

42:57.440 --> 43:03.840
effect of this on, first of all, on measurable productivity, second of all, on the the job

43:03.840 --> 43:08.480
market, like, is it going to make people lose their job like right away? And no, it's going to take

43:08.480 --> 43:12.480
a while. It's going to take 10, 15 years, you know, possibly more. It depends when you start

43:12.480 --> 43:16.800
counting, right, because the AI revolution maybe started 10 years ago. So if you start counting

43:16.800 --> 43:21.200
then, then it might only take, you know, another 10 years. But you know, I mean, I don't think you

43:21.200 --> 43:27.920
want to underestimate the degree of conservativeness of the business world, right? Things tend to

43:27.920 --> 43:33.920
change not that quickly. But if it's that easy to learn, like people will learn it and then invent

43:33.920 --> 43:40.720
new professions out of it, or become more productive themselves. Why do you think we love the doom,

43:40.720 --> 43:45.760
Jan? You know, I love your approach in mindset, and I agree with it. But why do you think we are

43:45.760 --> 43:49.440
kind of magnetized to like, oh, we're all going to be unemployed in the doom?

43:50.960 --> 43:55.680
Well, because I think for a number of reasons, so I'm not a, you know, social psychologist or

43:55.680 --> 44:02.800
sociologist, but but clearly, I think we're hardwired to pay attention to things that occur or may

44:02.800 --> 44:08.320
occur that could be dangerous to us. Because it means that there's something about the world that

44:08.320 --> 44:12.800
we don't completely understand, and we do have to pay attention to it and be careful about it.

44:12.800 --> 44:18.320
So for example, take a young, a young child, five months old, and show a scenario to this

44:18.320 --> 44:23.280
small child of a little car that is sitting on the platform, and then you push the car off

44:23.280 --> 44:28.320
the platform and instead of falling, the car appears to float in the air. The five months old

44:28.320 --> 44:33.680
will barely pay attention to it. But if you show this to a 10 months old, the 10 months old will

44:34.480 --> 44:40.080
look at it with huge eyes and stare at it for a long time, wondering what's going on. Because in

44:40.080 --> 44:45.360
the meantime, babies around the age of, you know, between, between six and nine months learn about

44:45.360 --> 44:49.760
gravity. They learn that objects that are not supported are supposed to fall. And so the mental

44:49.760 --> 44:54.560
model is that an object is not supported to fall. And they see this object that appears to float in

44:54.560 --> 44:59.920
the air. And they say, like, this can be like, you know, there's something I didn't, I didn't,

44:59.920 --> 45:04.240
I don't understand about the world, I need to look at this and investigate. Okay, so we're hardwired

45:04.240 --> 45:08.560
for this, because that's the way we learn our internal mental model of the world that allows

45:08.640 --> 45:13.840
us to predict what's going to happen, allows us to plan. That's what makes us smart. That's the

45:13.840 --> 45:19.600
basis of intelligence, the ability to predict. And so we naturally pay attention to stuff that

45:19.600 --> 45:28.720
is surprising, or dangerous, or both, which is why, you know, you see a outrageous piece of news,

45:28.720 --> 45:34.480
you know, a clickbait at the bottom of some, you know, website. And like, you have to convince

45:34.560 --> 45:39.760
yourself not to click on it. Can I ask you a couple of direct questions? I'm just too

45:39.760 --> 45:45.360
interested and we can take them out if needed. What did you say to Jeff when you heard that he

45:45.360 --> 45:49.760
was obviously making the moves that he did? I'm sure you had a conversation with him. What did

45:49.760 --> 45:57.040
you say to him? We haven't spoken yet, actually. We're going to speak to kind of get, you know,

45:57.040 --> 46:03.360
each other's opinion on it. I don't think he knows my opinion on this, because I don't think he

46:03.360 --> 46:07.200
follows, you know, what I post on Twitter or whatever, even though he is on Twitter himself.

46:07.200 --> 46:11.840
But so I think we have, you know, a discussion to have. I've had this discussion before with

46:11.840 --> 46:20.240
Yoshua Bengio, but not with Jeff. And to me, the fact that he left Google is not particularly a

46:20.240 --> 46:28.400
surprise. The fact that he leaves Google to be able to speak his mind, I think is not surprising.

46:28.400 --> 46:34.720
So I have a very different deal at Nitta, which is that I say whatever I want. Okay. I'm not under

46:34.720 --> 46:42.640
the tight control of, you know, the communications department or anything. I just say what I think.

46:42.640 --> 46:50.720
All right. How did you get that deal? Yeah. But no, seriously, many of my friends at Mesa,

46:50.720 --> 46:56.000
in very high positions, as you know, with mutual friends, they don't have that deal.

46:56.160 --> 47:02.960
So there is, I mean, I mean, a particularly sweet spot because I have a

47:03.920 --> 47:11.280
quite a bit of following people who trust me or believe me or want to hear what I have to say,

47:11.280 --> 47:18.240
even if they don't trust me at all. And at the same time, I'm not an officer. So I,

47:18.240 --> 47:22.880
it's not like, you know, there are things I can't say because of legal issues of, you know,

47:22.880 --> 47:27.760
financial blah, blah, blah, right. I'm a vice president, but I'm just below the level where

47:27.760 --> 47:32.160
you had to be really, really careful and so control your message. And I think there is

47:32.160 --> 47:41.360
a cost-benefit tradeoff here of, you know, AI is such a complicated, fast-evolving issue that

47:41.360 --> 47:47.600
you basically, you need someone to be able to, you know, speak freely. And I think Jeff didn't

47:47.600 --> 47:53.920
feel like he had that option at Google, maybe, you know, for various reasons. So I understand why

47:54.800 --> 48:00.240
he might have wanted to leave, but I don't, I don't agree with him at all with the whole

48:00.240 --> 48:04.320
sort of, you know, probability of human extinction or whatever.

48:04.320 --> 48:09.120
Have you ever felt your role at Mesa has impeded your ability to be impartial?

48:09.760 --> 48:15.120
I don't believe so, no. But I mean, there are certain things that I would post on social media

48:15.120 --> 48:20.640
that are kind of, you know, kind of popping up the work of my colleagues. And, you know,

48:20.640 --> 48:24.160
I'm obviously biased about this because, you know, I know about the work and they are friends

48:24.160 --> 48:29.840
and colleagues. And, you know, I think it's interesting probably because, you know, I feel

48:29.840 --> 48:34.240
the part of it. I totally agree. For this kind of stuff, I might be biased. Take this with a

48:34.240 --> 48:38.240
grain of salt. You don't have to believe me. You know, things like that. But it's given me

48:38.240 --> 48:44.320
a vision also of, you know, how things are built, what the problems are. So, you know, for example,

48:44.560 --> 48:52.800
there's a narrative, a very, very common narrative that AI is the culprit for a lot of the bad side

48:52.800 --> 48:59.760
effects of social networks in the past. And in fact, it's completely backwards. AI is the solution

48:59.760 --> 49:06.720
to those problems. So, you know, let me tell you, you know, go back like, you know, backpedal

49:06.720 --> 49:10.800
12 years ago or something, you know, even before I joined META, where META, you know, started

49:10.800 --> 49:16.400
experimenting with the newsfeed. And the newsfeed was, you know, an algorithm that would pick,

49:16.400 --> 49:21.520
like, which piece of news to show to everyone. And, you know, originally it was decided by, you

49:21.520 --> 49:26.080
know, how friends are you with a person making the post and things like that, right? How many

49:26.080 --> 49:30.240
interactions you have with that person. Eventually, a bit of machine learning was put into it

49:30.240 --> 49:34.640
shortly before I joined META. It was very, very simple. It was something like logistic regression,

49:34.640 --> 49:39.920
something like the simplest method you can imagine, with a lot of engineering behind it and a lot of,

49:39.920 --> 49:44.400
you know, hacks by hand and special cases. But basically, it was something like logistic

49:44.400 --> 49:48.640
regression, you know, some big vector that describes, you know, what you click on, like,

49:48.640 --> 49:52.880
how many times you, you know, how much time you spent on a particular piece of content and blah,

49:52.880 --> 49:58.160
blah, blah. And then it would decide, like, you know, give a rating to everything. So,

49:58.160 --> 50:04.800
that was deployed. And people ended up spending more time on Facebook. But then also, it created

50:04.800 --> 50:09.120
problems that were quickly identified, like, you know, like information bubbles in the context

50:09.120 --> 50:17.600
of political discourse. And the fact that what I was talking about earlier, that people tend to

50:17.600 --> 50:23.200
click on things that is more interesting, right? So, it caused, you know, the appearance of

50:23.200 --> 50:27.600
clickbait companies that, because you were just like farms of, you know, teenagers in

50:28.400 --> 50:33.280
Montenegro or someplace, making false news to get people to click on them and get money from

50:33.280 --> 50:37.920
the ads that they show them. So, then, you know, this was realized there were, like, big groups

50:37.920 --> 50:43.280
that at Facebook at the time kind of studying the, where the effect of those things are,

50:43.280 --> 50:47.360
and this was corrected. So, that's the way you, you make some work, right? You,

50:48.720 --> 50:52.480
you try the most small scale, you see what the effect is, if there is bad side effect,

50:52.480 --> 50:56.720
you correct it, and then you sort of compare, you know, to two systems. And then sometimes,

50:56.720 --> 51:01.120
something unexpected occurs and you have to back that all and completely change the way you do

51:01.120 --> 51:05.360
things. That's what happened in 2017 after the presidential election, American presidential

51:05.360 --> 51:10.320
election in 2016. The main newsy algorithm was completely changed, so that, you know,

51:10.320 --> 51:14.960
there was no clickbaits anymore. There was no, like, you know, news outlets that could, like,

51:14.960 --> 51:19.360
push their content that was propaganda, basically, you know, much more effort to take down false

51:19.360 --> 51:24.880
accounts and attempts to corrupt the democratic system and stuff like that, right? So, you,

51:24.880 --> 51:29.520
you correct it. And then what the progress of AI over the last few years basically allowed

51:30.320 --> 51:34.720
systems to be deployed to do things like taking, take down hate speech relatively,

51:36.080 --> 51:40.880
reliably, in hundreds of different languages, which was basically impossible to do before.

51:41.520 --> 51:44.960
You mentioned correct it. I promised last question, then we'll do a quick fire. You mentioned

51:44.960 --> 51:50.160
correct it. Elon Musk said with Tucker Carlson, the trouble with AI is you can't release and then

51:50.160 --> 51:55.600
correct. Unlike all prior technological developments, once released, it is too powerful

51:55.600 --> 52:01.840
to be able to bring back into the box. It cannot be amended in that way. Is that not true?

52:02.480 --> 52:09.040
That's not true. That's completely false. It makes an assumption which Elon and some other people

52:09.040 --> 52:15.200
may have become convinced of by reading, you know, Nick Boxtron's book, Super Intelligence, or,

52:15.920 --> 52:23.120
or reading, you know, some of Elias O'Yudkowski's writing. So this is predicated on an assumption

52:23.120 --> 52:29.280
that is just false, which is the existence of a heart takeoff. Right. So the fact that

52:30.000 --> 52:34.400
the minute you turn on a super intelligent AI system is going to take over the world,

52:35.520 --> 52:41.280
and it's going to escape your control, and it's going to refine itself to be even more intelligent.

52:41.280 --> 52:47.920
And so, you know, and the world would be destroyed. And that's just ridiculous. It's just completely

52:47.920 --> 52:56.720
ridiculous because there is no process in the real world that is exponential for very long.

52:58.800 --> 53:02.880
You know, those systems will have to, like, recruit all the resources in the world.

53:02.880 --> 53:11.040
They would have to be given, you know, limitless power agency. Like, why would we do this? And

53:11.040 --> 53:16.080
what's more, they would have to be built so that they have a desire to take over. Like,

53:16.160 --> 53:19.280
you know, systems are not going to take over just because they are intelligent.

53:19.920 --> 53:24.480
Because again, you know, in, even within the human species, it is not the most intelligent

53:24.480 --> 53:31.200
among us that want to dominate others. So his desire and many other leaders desire to prevent

53:31.760 --> 53:36.320
any further development and to regulate intensely right now and stop all progression

53:36.960 --> 53:44.480
is BS, basically. It's obscurantism. Yeah. Right. It's like, it's like people who wanted to

53:44.480 --> 53:49.920
stop the printing press and the diffusion of printed books because, you know, if people could

53:49.920 --> 53:54.000
read the Bible for themselves, they wouldn't have to talk to priests anymore and then would have

53:54.000 --> 53:59.600
their own idea about religion. And that's exactly what happened. People read the Bible for themselves

53:59.600 --> 54:04.720
and that created the Protestant movement in Europe. And that created 200 years of religious conflicts.

54:04.720 --> 54:12.080
But it also brought to us the enlightenment, science, rationalism, philosophy, ideas of democracy,

54:12.080 --> 54:16.800
and then the French and American revolutions. And then, you know, you can compare this with the

54:16.800 --> 54:21.440
Ottoman Empire, which for reasons of being able to control their population, you know,

54:21.440 --> 54:27.200
basically stopped, forbid the use of the printing press. And it started 300 years of decline.

54:27.760 --> 54:31.520
They were dominating science in the Middle Ages, the Muslim world,

54:31.520 --> 54:34.400
which is why, you know, every star in the sky has an Arabic name.

54:35.520 --> 54:39.200
I love this. I'm going to do a quick fire around with you now. So I say a short statement and

54:39.200 --> 54:42.880
you give me your immediate thoughts and then we'll rock and roll. Does that sound okay?

54:42.880 --> 54:49.920
Sounds good. So which regions most need to change their modus operandi when it comes to the practice

54:49.920 --> 54:59.200
of scientific research and incentive mechanisms? Which region? Oh, wow. Pretty much every region.

54:59.840 --> 55:07.520
I'm afraid, but for different reasons. So you saw with China. So China has a bit of an epidemic

55:08.400 --> 55:13.680
of bad science. There's a lot of very smart people in China, a lot of very good researchers,

55:13.680 --> 55:17.280
a lot of very good work coming out of China, particularly in AI, particularly in computer

55:17.280 --> 55:22.960
vision. But a lot of absolutely terrible work that has to be retracted a few months later,

55:22.960 --> 55:29.440
it's been published. And it's partly because of the incentive mechanisms in the academic and

55:30.160 --> 55:35.280
system in China. So this is important to fix there. I can move to Europe. So

55:36.560 --> 55:42.400
in Europe, there are good things. So the education system for like undergrad rates,

55:42.400 --> 55:48.480
education in Europe is great. It's fantastic, because it's party free. So that allows

55:49.280 --> 55:54.400
talented people to go to the schools, even if they are not rich, right?

55:55.360 --> 55:58.720
Which is not the case in the US, for example, at least not to the same extent.

55:58.720 --> 56:03.920
That's good for Europe. A lot of European engineers and scientists are great,

56:03.920 --> 56:07.760
atop base in the world. But then what are the opportunities for people who want to

56:08.400 --> 56:15.200
go into science and research? And most European countries actually don't have systems that

56:15.200 --> 56:21.280
really encourage this and motivate the most talented people and students to go into science.

56:21.280 --> 56:28.560
And so some of them go to North America like me 35 years ago. There are opportunities now

56:28.720 --> 56:37.280
that are really good in research labs like fair in Paris, or Google also has labs in Paris.

56:37.280 --> 56:42.480
Actually, my brother works at Google in Paris. So there are other outfits. So that gives

56:42.480 --> 56:48.080
opportunities for people who really want to be productive and don't think that they can

56:48.080 --> 56:53.440
in the public research and academic system in France and the rest of Europe.

56:54.400 --> 57:00.080
The only European countries that can rival the US in terms of the quality of

57:00.640 --> 57:02.880
job for an academic or a scientist is Switzerland.

57:04.000 --> 57:08.800
What do you think they do to rival that? What is it about their incentive

57:08.800 --> 57:11.680
mechanism structure that gives them that ability?

57:12.400 --> 57:19.920
Two things. They pay people better. Second thing is they give them resources for research.

57:19.920 --> 57:23.840
They can get extra resources through grants and stuff like that, but they're good. And then

57:23.840 --> 57:28.320
they also attract some of the best students in the world. So you get the ideal combination that

57:28.320 --> 57:32.560
you only get in the top 30 universities in North America.

57:32.560 --> 57:37.280
So we've got China, we've got Europe. What about the US? What could they do differently or improve?

57:37.840 --> 57:43.840
Well, so there's a lot that the US does right in terms of research, which is to a large extent

57:44.800 --> 57:51.680
a bit of a partial explanation for the success of the technological industry, the tech industry

57:51.680 --> 57:59.440
in the US. I think partly because the US devotes a significant amount of resources to

57:59.440 --> 58:06.720
fundamental research through NSF and NIH and various other outfits, probably more than Europe.

58:06.720 --> 58:13.440
Universities pays their faculty pretty well, particularly in areas like computer science and AI.

58:13.440 --> 58:18.880
Now this comes with a downside. The downside is that studying in the US is expensive.

58:19.680 --> 58:22.400
It's a trade-off, right? So can you do one without the other?

58:22.400 --> 58:27.280
Switzerland figured out how to pay academics pretty well while actually

58:27.280 --> 58:31.680
offering free education to their students. So there is a way to do it.

58:32.240 --> 58:34.960
Canada also figured out a pretty good trade-off as well.

58:34.960 --> 58:41.440
So in other things, the US does right. But one thing that the US system or like the rough does

58:41.440 --> 58:47.680
right also is the willingness to take risk and invest on ideas that seem a little crazy,

58:47.680 --> 58:55.520
but basically the sort of vibrant startup scene in Silicon Valley and other places in the US,

58:55.520 --> 59:04.240
in New York, and in the Boston area is leading the world. Now you start seeing a similar thing in

59:04.240 --> 59:11.120
Europe now. There's been like enormous growth, for example, of tech startups in Paris,

59:11.120 --> 59:17.600
Paris area, in France more generally, and continental Europe, more widely in the UK as well.

59:17.600 --> 59:23.200
And so I think that's a good thing, but it's still a little more difficult to have access

59:23.200 --> 59:25.760
to investment money in Europe than it is in the US.

59:25.760 --> 59:31.920
That's why I'm here, Jan. I'm happy to provide it. I'm going to do an ultimate one for you.

59:31.920 --> 59:35.040
When you think about what you'd most like someone listening to take away,

59:35.600 --> 59:40.400
what would it be when they hear this? What do you want them to take away as the number one thing?

59:41.200 --> 59:50.160
AI is going to bring a new renaissance for humanity, a new kind of new form of enlightenment,

59:50.160 --> 59:57.280
if you want, because AI is going to amplify everybody's intelligence. It's like every one

59:57.360 --> 01:00:03.520
of us will have a staff of people who are smarter than us and know most things about most things

01:00:03.520 --> 01:00:08.480
and most topics. So it's going to empower every one of us. It's going to make us more creative

01:00:08.480 --> 01:00:15.840
because we'll be able to produce text, art, music, videos without necessarily having all the

01:00:15.840 --> 01:00:21.680
technical skills that are currently required for doing those things. And so exercise our creative

01:00:21.680 --> 01:00:26.880
juices. So that's the positive side. There are risks. There's no question. But it's not like

01:00:26.880 --> 01:00:31.520
those risks. Don't believe the people who tell you that those risks are inevitable or that they

01:00:31.520 --> 01:00:38.960
will inevitably lead to catastrophe. That's just not true. It's like, place yourself in 1920. Who

01:00:38.960 --> 01:00:46.000
would have thought that a mere 50 years later, you could cross the Atlantic in a few hours in

01:00:46.000 --> 01:00:54.480
complete safety at near the speed of sound? Would people seriously want to ban aviation

01:00:54.480 --> 01:01:00.160
or call for regulation of jet engines before jet engines existed? I mean, that's kind of insane.

01:01:00.960 --> 01:01:06.080
So I'm not against regulation. There should be regulation of AI products, particularly the ones

01:01:06.080 --> 01:01:11.360
that involve making critical decisions for people. But regulating or slowing down research is

01:01:11.360 --> 01:01:17.520
complete nonsense. It's just obscurantism. Who's incumbent team do you most respect and admire

01:01:17.520 --> 01:01:25.200
when you look at Amazon, Facebook, Google, in terms of their approach and talent internally?

01:01:25.200 --> 01:01:30.720
Outside of meta, obviously. So this is changing a lot. And the reason it's changing is because

01:01:31.280 --> 01:01:35.600
a lot of people are leaving large companies and large labs. And the reason they're doing this is

01:01:35.600 --> 01:01:41.120
that until recently, a lot of AI research was very exploratory. And now there's a path towards

01:01:41.120 --> 01:01:46.320
commercialization for a lot of things. And so people think that they're better off just leaving

01:01:46.320 --> 01:01:50.000
large companies and doing this on their own, doing a startup and things like that. So you see

01:01:51.280 --> 01:01:57.200
a relatively large motion of applied research engineers, a few scientists

01:01:58.800 --> 01:02:04.720
basically leaving those labs to do startups. And that's across the board. So you look at the

01:02:04.720 --> 01:02:10.720
original paper from Google about BERT or Transformers. The thing that revolutionized

01:02:10.720 --> 01:02:16.960
NLP, all of them have left. Okay, they're only startups. Some of the people who produced LANA,

01:02:16.960 --> 01:02:24.320
the open source LLMs from meta. So the key people have left already, okay, to do startups.

01:02:24.320 --> 01:02:29.600
I saw their companies. Yeah, there's one called Mistral.

01:02:29.600 --> 01:02:35.120
That's the one I saw. Yeah. Right. But they, yeah, there is insane amount of money in dates.

01:02:35.680 --> 01:02:42.960
Yeah. Yeah. So, you know, we're proud of them. But I mean, I'm sad that they'd have to, I mean,

01:02:42.960 --> 01:02:47.200
I would just say, but he's an existing team. Me, like, yeah, they're good.

01:02:47.840 --> 01:02:54.720
Yeah. So, but I think like in terms of the basic competence and the people who are going to push

01:02:55.520 --> 01:03:00.960
the science forward, because what we need now is not to work on applications with LLMs. There's

01:03:00.960 --> 01:03:04.640
a lot of people who are capable of doing this and they're going to do the job. What we need to do

01:03:05.440 --> 01:03:10.480
people like me who are, you know, reworking on research is kind of coming up with new concepts

01:03:10.480 --> 01:03:15.360
that will allow us to, you know, get machines that basically have common sense, have an experience

01:03:15.360 --> 01:03:21.120
of the real world, have, you know, basically human level intelligence, right? And, you know,

01:03:21.120 --> 01:03:25.760
in my opinion, the, the outfits that are best positioned for this are fair from on one side.

01:03:26.800 --> 01:03:30.560
And the new deep mind now, which is, you know, deep mind plus group of brain.

01:03:30.560 --> 01:03:33.680
Yeah. There are a lot of people there who are interested in that question. And I think they

01:03:33.680 --> 01:03:39.760
are probably the best together with Mita and fair. They're the best positions to position,

01:03:39.760 --> 01:03:44.480
to really kind of have an impact on this, something, you know, all of us have been working on for,

01:03:44.480 --> 01:03:45.440
for quite a while.

01:03:45.440 --> 01:03:51.760
Jan, if we do this again in 10 years time, where is Jan in 10 years time in 2033?

01:03:52.320 --> 01:03:59.200
Well, I'm 63. So, you know, 10 years on now, well, 12 years on now, I'll be, I'll be Jeff

01:03:59.200 --> 01:04:09.520
into the age. Okay. And I don't know. I think I'm excited like, like a teenager now,

01:04:10.240 --> 01:04:16.640
because I see the opportunity of like the next step in AI and opportunity perhaps to,

01:04:17.520 --> 01:04:22.320
you know, get to the goal that I set myself so that I imagined for myself when I started

01:04:22.320 --> 01:04:28.800
working on AI many years ago, which of course I was very naive about at the time of understanding

01:04:28.800 --> 01:04:33.600
intelligence, first of all, and it's a scientific question. What is intelligence? What is human

01:04:33.600 --> 01:04:38.240
intelligence? And one good way as an engineer, a good way to understand intelligence is to

01:04:38.240 --> 01:04:44.800
build a widget that actually reproduces it, right, to some extent. So I'm excited about this right

01:04:44.800 --> 01:04:55.520
now. I'll find the, you know, the, the, the, the substrate, the landscape, the, the location,

01:04:55.520 --> 01:05:03.600
the position where I can make the, the best contributions to this. And currently that just

01:05:03.600 --> 01:05:13.120
happens to be, to be fair at Meta. I keep a foot in academia because I think it's very

01:05:13.120 --> 01:05:17.520
complementary and also important. There are projects of different types that you do in

01:05:17.520 --> 01:05:22.240
academia and industry that are complementary. So I like the, the combination of the two.

01:05:23.200 --> 01:05:28.160
As long as my brain keeps working, that I think I can contribute and that I've given,

01:05:28.160 --> 01:05:33.200
I've given the means to contribute, I'll keep, I'll keep working. And then there's some point

01:05:33.200 --> 01:05:39.920
where my brain will turn into white sauce or, or I'm totally, you know, out of it or something

01:05:39.920 --> 01:05:45.920
and I'll stop. Yeah. And I want to say personally, thank you so much. I speak for many, I'm sure,

01:05:45.920 --> 01:05:50.480
when I say we've learned so much from you in terms of your public speaking and discourse and

01:05:50.560 --> 01:05:54.560
willing to speak. I think few are willing to speak as openly as you have been. So

01:05:54.560 --> 01:05:58.560
thank you for educating so many of us. And thank you so much for joining me today, Ann.

01:05:59.200 --> 01:06:02.160
Well, thank you so much for having me, Harry. This was, this was fun.

