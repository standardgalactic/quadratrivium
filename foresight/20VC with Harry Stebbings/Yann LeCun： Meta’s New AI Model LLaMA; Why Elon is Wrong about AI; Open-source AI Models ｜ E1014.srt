1
00:00:00,000 --> 00:00:04,960
AI is going to bring a new renaissance for humanity, a new form of enlightenment if you want,

2
00:00:04,960 --> 00:00:09,920
because AI is going to amplify everybody's intelligence. It's like every one of us will

3
00:00:09,920 --> 00:00:14,560
have a staff of people who are smarter than us and know most things about most topics,

4
00:00:14,560 --> 00:00:20,240
so it's going to empower every one of us. Jan, I am so excited for this. I had so many great

5
00:00:20,240 --> 00:00:24,240
things from our mutual friends, obviously David Marcus and then Mathieu at Photoreal Room. So

6
00:00:24,240 --> 00:00:29,360
thank you so much for joining me today. And it's a pleasure. Now, I would love to start. I heard

7
00:00:29,360 --> 00:00:33,840
some of the early stories, but I want to start with one from David Marcus. How did you first

8
00:00:33,840 --> 00:00:40,080
enter the world of AI and make that first foray? I was still an undergraduate engineering student

9
00:00:40,080 --> 00:00:46,240
in France, and I stumbled on a philosophy book, which was a debate between Jean Piaget,

10
00:00:46,240 --> 00:00:51,600
you know, the cognitive psychologist, and Noam Chomsky, the famous linguist. And they were

11
00:00:51,600 --> 00:00:58,000
arguing about nature versus nurture for language, whether language is acquired or innate. So Chomsky

12
00:00:58,000 --> 00:01:03,760
was on the side of innate and Piaget on the side of acquired with, you know, some innate structure.

13
00:01:03,760 --> 00:01:08,320
And on the side of Piaget was a guy called Seymour Papert, who was a professor at MIT.

14
00:01:08,960 --> 00:01:13,280
In his argument, he talked about something called the perceptron, which was an early

15
00:01:14,320 --> 00:01:19,040
machine learning system. And I read this and discovered that people had been working on

16
00:01:19,840 --> 00:01:25,280
machines that could learn and I was fascinated and I started digging the literature, soon discovered

17
00:01:25,280 --> 00:01:31,920
that much of that literature was in the 1950s and 60s and basically stopped in the late 60s because

18
00:01:31,920 --> 00:01:39,600
of a book that they killed it. And Seymour Papert was a co-author of that book. So strangely enough,

19
00:01:39,600 --> 00:01:46,400
and here it was 10 years later, actually praising the perceptron as kind of an amazing concept.

20
00:01:46,400 --> 00:01:51,840
So I was hooked. I, you know, started getting interested in what was not yet called machine

21
00:01:51,840 --> 00:01:55,040
learning, but eventually became neural nets and now deep learning.

22
00:01:55,040 --> 00:01:59,120
Can I ask you, David asked this as well, how long did it take to get,

23
00:01:59,840 --> 00:02:04,160
in terms of like the major breakthroughs, how long did it take you to get to the major breakthroughs

24
00:02:04,160 --> 00:02:09,360
that you're at the origin of when you look back over that time to get to those major breakthroughs?

25
00:02:09,360 --> 00:02:14,400
Well, so there's a few breakthroughs. So the first one was in the, when I was still on

26
00:02:14,400 --> 00:02:20,240
Autobahn, basically finishing my engineering studies, I figured out that the way forward to

27
00:02:20,240 --> 00:02:25,200
kind of lift the limitations of the old systems that were abandoned in the 60s was to find

28
00:02:25,200 --> 00:02:30,160
learning algorithms that could train multilayer neural nets, essentially. And people had all

29
00:02:30,160 --> 00:02:38,800
but abandoned this type of research, except for a handful of people in Japan. And one guy I heard,

30
00:02:39,440 --> 00:02:46,160
I heard about, called Jeff Hinton, who had published a paper in 1983. So this was just

31
00:02:46,160 --> 00:02:51,920
the year I graduated on something called the Boston machine, which was clearly a method to

32
00:02:51,920 --> 00:02:59,120
go beyond those limitations. And so I had on my side kind of developed a method for training

33
00:02:59,120 --> 00:03:03,520
multilayer nets, which was very close to what we now call back propagation, but not exactly the

34
00:03:03,520 --> 00:03:09,520
same. It was closer to what we call target prop, actually, nowadays. And then, you know, published

35
00:03:09,520 --> 00:03:15,920
a few papers in French and eventually met Jeff at a meeting in France in 1985. And we

36
00:03:15,920 --> 00:03:19,680
realized we've been working on the same thing and we're seeking a like. And, but I was, you know,

37
00:03:20,320 --> 00:03:25,680
in the middle of my PhD, and he was an associate professor at Carnegie Mellon. So we started,

38
00:03:25,680 --> 00:03:29,520
you know, a discussion and then, you know, visited him at Carnegie Mellon for a summer

39
00:03:29,520 --> 00:03:34,240
school reorganized. And then I, when I finished my PhD, I did a postdoc with him and then John

40
00:03:34,240 --> 00:03:39,360
Bell Labs. And, and when I was in Toronto, I developed what was called convolutional nets.

41
00:03:39,360 --> 00:03:45,600
Now, convolutional networks, which, you know, is major method for image and speech processing.

42
00:03:45,600 --> 00:03:52,240
Now it is. And so that's, that's what I'm best known for. But, but it started much earlier.

43
00:03:52,240 --> 00:03:59,440
I have to ask, Joshua described kind of the, the hype cycles within AI and neural nets like

44
00:03:59,440 --> 00:04:05,760
deserts when you're not in them. And he asked the question, how did you not get discouraged

45
00:04:05,760 --> 00:04:10,960
when for a solid decade, we were in a desert where no one really cared about neural nets?

46
00:04:10,960 --> 00:04:15,680
How did you keep the enthusiasm bluntly when, as Joshua said, no one really cared?

47
00:04:16,480 --> 00:04:21,920
Both Joshua and Jeff and I had in the back of our minds that those methods would eventually come to

48
00:04:21,920 --> 00:04:28,000
the fore and that, you know, we would have to kind of snap people out of their preconceived ideas

49
00:04:28,000 --> 00:04:33,360
about, about neural nets. So yes, there was. So you're trying, we're actually working together at

50
00:04:33,360 --> 00:04:39,040
AT&T Bell Labs in the early 90s. And then the interest of the community for those methods

51
00:04:39,040 --> 00:04:45,040
started waning around 1995 or so. And it was indeed about 10 years when not only nobody was

52
00:04:45,040 --> 00:04:49,200
interested in neural nets, but people were even making fun of it, you know, talking about it in

53
00:04:49,200 --> 00:04:56,800
sort of disparaging terms. Now there is something though, in 1996, I kind of changed job. I stayed

54
00:04:56,800 --> 00:05:02,160
in the same company, I was still working at AT&T in the research labs, but I became a department head

55
00:05:02,160 --> 00:05:07,040
and this was the early days of the internet. And my group and I started working on something

56
00:05:07,040 --> 00:05:11,280
completely different that had nothing to do or not much to do, at least with machine learning,

57
00:05:11,280 --> 00:05:17,200
this image compression. I had this, this idea that with the internet coming up, we should have a

58
00:05:17,200 --> 00:05:22,480
way of scanning existing paper documents and then, you know, put them on the internet so that

59
00:05:22,480 --> 00:05:27,520
everybody could, could have access to them. And so I worked on this for five or six years together

60
00:05:27,520 --> 00:05:32,800
with Leon Botou, who had been a long, long term collaborator. Joshua was also involved

61
00:05:33,600 --> 00:05:39,360
peripherally and a bunch of other people, Patrick Hefner, et cetera. And that project ended when

62
00:05:39,360 --> 00:05:45,760
all of us basically left AT&T. That's when I restarted working on deep learning and Jeff also

63
00:05:45,760 --> 00:05:50,800
kind of came back to Canada. He had been in the UK for a while and Joshua, Jeff and I decided in

64
00:05:50,800 --> 00:05:58,000
the early 2000 to basically start a conspiracy to, you know, revive the interests of the community

65
00:05:58,000 --> 00:06:07,200
in neural nets by making them work, discovering new algorithms. And, you know, it took almost 10

66
00:06:07,200 --> 00:06:14,000
years, but it succeeded beyond the wildest dreams, basically. So I'm going to ask you a range of

67
00:06:14,000 --> 00:06:20,320
varying questions in terms of depth, breadth, and kind of obvious and non-obvious. So forgive me

68
00:06:20,320 --> 00:06:24,640
if some are obvious. I just want to ask, when I hear the historical context there from you,

69
00:06:24,640 --> 00:06:30,480
over many decades, how do you feel today when we look at what's happening today?

70
00:06:30,480 --> 00:06:36,880
Are we at a new inflection point in development? Or is this merely the continuation of what we've

71
00:06:36,880 --> 00:06:44,240
seen for many decades? It's a combination of the two. So on the one hand, a lot of what we see

72
00:06:44,240 --> 00:06:51,200
today, when you are kind of down in the trenches of research, looks at a logical extension. I was

73
00:06:51,200 --> 00:06:58,080
not as enthralled by the sort of recent progress as the public was because, you know, I've seen

74
00:06:58,080 --> 00:07:01,920
this progress happening over the last several years. Now, there are things that have been very

75
00:07:01,920 --> 00:07:08,480
surprising. The fact that self-supervised learning methods applied to transformer architectures

76
00:07:09,600 --> 00:07:14,000
work amazingly well and it worked, you know, way beyond what we could have expected. The fact that

77
00:07:14,000 --> 00:07:20,800
we can do basically train systems to understand language, translate language in multiple languages,

78
00:07:20,800 --> 00:07:24,720
and then continue text if you're trying them to do this, or answer questions if you're trying

79
00:07:24,720 --> 00:07:30,800
them to do this, works amazingly well to an extent that people didn't quite expect that

80
00:07:30,800 --> 00:07:35,440
was going to happen by just making them bigger and training them on more data. So that certainly

81
00:07:35,440 --> 00:07:41,200
has been surprising for everybody, but that revolution occurred two years ago. Whereas the

82
00:07:41,200 --> 00:07:48,400
wider public has learned about it through a tragedy that was made available for us. It's

83
00:07:48,400 --> 00:07:53,840
been more continuous. And you see this in, you know, a lot of marking events in technological

84
00:07:53,840 --> 00:08:00,160
progress or in AI, in particular, are marked by kind of splashy events that the public pays

85
00:08:00,160 --> 00:08:05,120
attention to. But too many of us, like it looks like more like a continuous thing. And generally,

86
00:08:06,080 --> 00:08:12,400
what those progress require is a bunch of people to take the techniques that already exist, push

87
00:08:12,400 --> 00:08:17,040
them a little further, do a bit of engineering, and then make a demo that demonstrates that it

88
00:08:17,040 --> 00:08:23,920
works. So that was the case for, you know, DBlue, the chess player that IBM built in the mid-90s,

89
00:08:23,920 --> 00:08:29,760
that beat Gary Kasparov, you know, same thing with the DARPA Grand Challenge, that Sebastian

90
00:08:29,760 --> 00:08:35,840
Sointim at Stanford won a card that could drive itself in the desert, right, for a hundred miles.

91
00:08:35,840 --> 00:08:40,720
And then, you know, AlphaGo and, you know, the IBM Geopedy, there's a number of those things,

92
00:08:40,720 --> 00:08:46,000
right, and, you know, which are just being the latest one. And it looks like kind of

93
00:08:46,000 --> 00:08:50,560
jumps when you look at it from far away. But when you're in the field, it's more like a

94
00:08:50,560 --> 00:08:56,240
continuous evolution. Can I ask, has there been any other surprising on the positive side,

95
00:08:56,240 --> 00:09:00,480
developmental things you've seen over the last year or so? You said about self-supervised learning

96
00:09:00,480 --> 00:09:04,400
and the efficiencies there. Is there anything else where you're like, I didn't expect it to

97
00:09:04,400 --> 00:09:10,160
go as well as it has done in the last year? Yeah, so I already mentioned it. You know,

98
00:09:10,240 --> 00:09:17,040
the fact that merely training a language model to predict the last word in the sequence of words,

99
00:09:17,680 --> 00:09:22,240
if you do it properly, you get a system that has capabilities that are somewhat unexpected,

100
00:09:22,240 --> 00:09:27,280
and they emerge as you make those systems bigger, and you train them on

101
00:09:28,400 --> 00:09:33,280
larger amounts of data, that's clearly been the surprise for everyone.

102
00:09:34,160 --> 00:09:41,840
Now, the thing is, you know, as researchers and scientists, we're always looking for the next

103
00:09:41,840 --> 00:09:46,880
thing. So what I'm interested in at the moment is, you know, what goes beyond that. Like, you know,

104
00:09:46,880 --> 00:09:51,120
a lot of people are going to work on applications of autoregressive large language models, which is

105
00:09:51,120 --> 00:09:55,920
great. There's going to be a lot of, you know, products and new ways for people to do things,

106
00:09:55,920 --> 00:10:01,840
and it's going to be wonderful. But I've already been thinking about the next stage for the last

107
00:10:02,800 --> 00:10:08,960
three, four years, four, five years even, actually more, which is like, what's missing from those

108
00:10:08,960 --> 00:10:13,440
systems? What are your thoughts on what's missing from those systems? In that logical next step,

109
00:10:13,440 --> 00:10:19,360
why does that lead you in your thinking? So those systems do not have anywhere close to human level

110
00:10:19,360 --> 00:10:25,840
intelligence. Okay, despite what you might think, we are kind of fooled into thinking it because

111
00:10:25,840 --> 00:10:31,440
those systems are very fluent with language, but their ability to think, to understand how the

112
00:10:31,440 --> 00:10:37,440
world works, to plan, are very, very limited. And they're understanding the world is very superficial.

113
00:10:37,440 --> 00:10:44,160
And the reason for it is that they are strictly trained on language. And language only contains a

114
00:10:44,160 --> 00:10:49,680
small proportion of all human knowledge. Most of human knowledge is not linguistic at all.

115
00:10:49,680 --> 00:10:54,720
And all of animal knowledge is non linguistic. And we take it for granted, you know, this is the

116
00:10:54,800 --> 00:11:01,360
Moravec paradox, right? All the capabilities and abilities that we take for granted, like, you

117
00:11:01,360 --> 00:11:07,760
know, planning a motion or something or very simple things that everyone can do. A 10 year old can,

118
00:11:07,760 --> 00:11:13,840
you know, clear up the dinner table and fill up the dishwasher. Any 17 year old can learn to drive.

119
00:11:13,840 --> 00:11:16,480
We still don't have so many cars, we don't have domestic robots.

120
00:11:16,480 --> 00:11:22,400
If they're non linguistic, like the majority, I'm sorry for the base questions, but then what are

121
00:11:22,400 --> 00:11:30,080
they? And is that that we don't have able to be ingested by AI models and engines over time?

122
00:11:31,040 --> 00:11:35,440
Well, so first of all, there is no question that eventually AI systems will understand the world

123
00:11:35,440 --> 00:11:40,880
in similar ways that that humans do. There has better ways. But they will not be

124
00:11:41,520 --> 00:11:45,680
autoregressive large language models as a type that we're now talking about.

125
00:11:45,680 --> 00:11:49,680
There will be different for a number of different different reasons. But to answer your question

126
00:11:49,680 --> 00:11:56,160
more directly, anything that has to do with sort of an intuition of the real world requires

127
00:11:56,160 --> 00:12:02,640
an experience of the real world or a simulated version of it, which those large English models

128
00:12:02,640 --> 00:12:07,440
don't have. They're purely trained from text. So you can add you, there's a number of questions

129
00:12:07,440 --> 00:12:11,920
that about the physical world that they'll be able to answer because there's a template for it in the

130
00:12:12,560 --> 00:12:16,400
or something very similar in the data that they've been trained on. Same for planning,

131
00:12:16,400 --> 00:12:22,880
you can ask them to plan a trip or something and they'll adapt a template that they've been trained

132
00:12:22,880 --> 00:12:28,160
on. But they don't really have sort of a model of a mental model of how the world works and allows

133
00:12:28,160 --> 00:12:33,680
them to plan complex action sequences or use tools or things like that. Can I ask, is that why you

134
00:12:33,680 --> 00:12:40,160
said that AI researchers face palm when they hear prophecies of doom? No, that's a different question.

135
00:12:40,160 --> 00:12:44,080
Those are kind of orthogonal concepts. So I mean, there is some some weak connection.

136
00:12:44,720 --> 00:12:51,920
There is a a flaw in a current autoreversive lens, which is that you can only control their answer

137
00:12:53,040 --> 00:12:59,200
in two ways. The first way is you modify the statistics of the training data that you train

138
00:12:59,200 --> 00:13:05,280
them on, possibly using human feedback for specific answers. And the second one is you

139
00:13:05,280 --> 00:13:10,560
change the point and the combination of the point that, you know, the question you ask them,

140
00:13:10,560 --> 00:13:15,680
the form in which you ask the question and the statistics of the training data entirely determines

141
00:13:15,680 --> 00:13:23,280
the answer to the system we produce. So there is no persistent memory, first of all. But second of all,

142
00:13:24,160 --> 00:13:30,080
you cannot control the system. You cannot impose constraints on it, like be factual, be understandable

143
00:13:30,080 --> 00:13:34,640
by a certain year old. You can try to put this in a prompt, but then, you know, you rely on

144
00:13:35,200 --> 00:13:40,400
whether the statistics of the training data is appropriate for for taking taking that into account.

145
00:13:40,400 --> 00:13:45,680
There's no direct way to constrain the answer of those systems to satisfy certain objectives.

146
00:13:45,680 --> 00:13:51,360
And that makes them very difficult to to control and steer. And so that creates some fears because

147
00:13:51,360 --> 00:13:55,680
people are kind of extrapolating. If we let those systems do whatever we connect them to

148
00:13:55,680 --> 00:14:00,240
internet and they can do whatever they want, they're going to do crazy things and stupid things

149
00:14:00,240 --> 00:14:04,320
and perhaps dangerous things. And we're not going to be able to control them. And they're going to

150
00:14:04,320 --> 00:14:07,680
escape of control. And they're going to become intelligent just because they're bigger, right?

151
00:14:08,560 --> 00:14:13,760
And that's nonsense. First of all, because this is not the type of system that we are going to give

152
00:14:13,760 --> 00:14:19,680
agency to the systems that will eventually be given agency that are going to be able to plan

153
00:14:19,680 --> 00:14:23,680
sequences of actions, our systems are going to have objectives that they're going to have to

154
00:14:23,680 --> 00:14:28,400
satisfy. And because of those objectives, they're going to be controllable. So they're going to be

155
00:14:28,400 --> 00:14:32,960
much more controllable than the current systems. Okay, so my prediction is that within a few years,

156
00:14:33,520 --> 00:14:39,040
nobody in their right mind would use autoregressive LMS, they'll go away in favor of something more

157
00:14:39,840 --> 00:14:44,240
sophisticated and controllable, they can plan its answer as opposed to just produce one order

158
00:14:44,240 --> 00:14:49,440
after the other, reactively. Okay, that's that's the first fallacy. The second fallacy is that

159
00:14:49,440 --> 00:14:58,480
there is this idea somehow that the desire to and the ability to dominate is linked with intelligence,

160
00:14:58,480 --> 00:15:03,440
right? So this is a statement that a lot of people are are making, including, you know,

161
00:15:03,440 --> 00:15:09,040
my friend, Jeffington recently, that somehow as soon as the machine becomes intelligent,

162
00:15:09,040 --> 00:15:14,640
it becomes uncontrollable because, you know, it's it being smarter than us, it can influence us in

163
00:15:14,640 --> 00:15:22,560
ways that we can even imagine. Now, I think this is a gigantic fallacy, because even within the

164
00:15:22,560 --> 00:15:30,160
human species, it is not the smartest among us that want to dominate the others. Okay, to dominate

165
00:15:30,160 --> 00:15:36,320
other entities, you don't necessarily need to be smarter than them, we need to want to dominate

166
00:15:36,320 --> 00:15:43,120
them. This is not something that every intelligent entity is going is going to do spontaneously.

167
00:15:43,920 --> 00:15:50,720
We do it as humans, because the desire to influence others was built into us by evolution,

168
00:15:50,720 --> 00:15:57,360
because we are a social species. Okay, same as baboons and chimpanzees and wolves and dogs and

169
00:15:57,360 --> 00:16:01,680
etc. It's not the case for orangutans. orangutans don't have the desire to dominate anybody,

170
00:16:01,680 --> 00:16:07,120
because they are non-social animals, they are solitary animals, they are territorial, in fact.

171
00:16:07,120 --> 00:16:15,360
So, we need to separate those two concepts, the will, the desire and the ability to dominate on

172
00:16:15,360 --> 00:16:19,760
one hand and intelligence on the other hand. The fact that we're going to have super-intelligent

173
00:16:19,760 --> 00:16:26,400
machines at our disposal means that every one of us is going to be like a business leader,

174
00:16:26,400 --> 00:16:30,720
a politician or an academic with a staff of people working for them that are more intelligent than

175
00:16:30,720 --> 00:16:39,360
themselves. I mean, it's great. It's not like if you feel threatened by being the boss of other

176
00:16:39,360 --> 00:16:43,360
people who work with you, but are smarter than you, you're not being a good leader.

177
00:16:44,160 --> 00:16:49,760
Can I ask you, how do we instill values within models where they don't have a desire to dominate?

178
00:16:50,560 --> 00:16:54,480
Right, so these objectives I was telling you about. So, okay, let me describe the

179
00:16:55,360 --> 00:16:59,440
architecture of future AI systems as I see it. We're going to have AI systems that

180
00:17:00,160 --> 00:17:06,400
basically are going to plan their actions and actions can include sequences of words that you

181
00:17:06,400 --> 00:17:12,880
tell someone, but they're going to plan the sequence of actions or words so as to optimize

182
00:17:12,880 --> 00:17:19,520
a series of objectives that we set them. So, one objective is, does this answer the question I just

183
00:17:19,520 --> 00:17:24,640
asked? Another objective might be, well, you're talking to a 13-year-old, make that answer

184
00:17:24,640 --> 00:17:31,680
understandable by a 13-year-old. Another objective might be, I asked you to answer a question about

185
00:17:31,680 --> 00:17:38,880
the world, so be factual. Or it's a question about yesterday's political event. Can you kind of be

186
00:17:38,880 --> 00:17:43,920
compatible with everything you've read in the press this morning? Things like that, right?

187
00:17:45,200 --> 00:17:51,440
So, you'll have those systems that have a series of objectives and their output, their answer,

188
00:17:52,320 --> 00:17:59,680
by construction, is going to have to satisfy those objectives. And some of those objectives

189
00:17:59,680 --> 00:18:06,880
will be hardwired to make those systems safe. Like, if it's a domestic robot that can cook dinner

190
00:18:06,880 --> 00:18:11,760
and can wield a kitchen knife in its arm, there's going to be a term in there that says,

191
00:18:11,760 --> 00:18:15,600
like, stop moving your arm when there's people around because you might hurt them. So, that's

192
00:18:15,600 --> 00:18:19,920
going to be an objective that the system cannot violate because by construction, we're going to

193
00:18:20,000 --> 00:18:26,160
have to satisfy them. So, that's the way to build safe AI system. You make them produce answers that,

194
00:18:26,160 --> 00:18:30,240
by construction, have to satisfy objectives. And you design those objectives so that their

195
00:18:30,240 --> 00:18:36,400
actions are safe. Now, how precisely to do this is not a completely solved question, but you try it,

196
00:18:36,400 --> 00:18:41,520
you deploy it at a small scale, you see what the effect is, and you correct it when it doesn't work,

197
00:18:41,520 --> 00:18:47,840
and you fix it progressively. And it's not like if you get it wrong, it's going to destroy humanity.

198
00:18:48,400 --> 00:18:55,120
That depends on that cooking robot, you never know. How do you determine who's able to set

199
00:18:55,120 --> 00:18:59,520
the objectives? Because there could be right or wrong depending on who sets them.

200
00:19:00,160 --> 00:19:06,640
That's true. So, that's going to have to be a process by which we allow people to do this,

201
00:19:06,640 --> 00:19:12,320
some vetting process. The same way that there's a vetting process for people to take care of

202
00:19:13,040 --> 00:19:19,600
your health or cut your hair, fix your plumbing or your car, right? So, there's some vetting

203
00:19:19,600 --> 00:19:26,240
process, certainly some testing and market deployment procedure with regulating agencies

204
00:19:26,240 --> 00:19:31,360
for things that are potentially dangerous, probably not for all applications, but for

205
00:19:31,360 --> 00:19:36,160
many applications, certainly in healthcare transportation and things like that. And then

206
00:19:36,240 --> 00:19:43,040
perhaps also, it could be that, let's take the example of intelligent assistance. So,

207
00:19:43,040 --> 00:19:50,480
let's imagine a future where everyone can talk to their intelligent assistant. That system will

208
00:19:50,480 --> 00:19:55,280
have pretty close to human-level intelligence for probably more accumulated knowledge than most

209
00:19:55,280 --> 00:20:01,200
humans. They could translate in any language and give you a quick summary of yesterday's

210
00:20:01,200 --> 00:20:06,240
newspaper and things like that, right? Explain mathematical concepts to you, things like that.

211
00:20:06,240 --> 00:20:12,400
So, people are probably going to use this almost exclusively in the future for their interaction

212
00:20:12,400 --> 00:20:16,720
with the digital world. You're not going to go to Google or Wikipedia, you're just going to talk

213
00:20:16,720 --> 00:20:22,560
to your assistant. And the only way to do this properly is for the base infrastructure for those

214
00:20:22,560 --> 00:20:28,960
assistants. I mean, they would be so pervasive, so much will ride on those systems that I don't

215
00:20:28,960 --> 00:20:37,760
think anyone will accept that those assistants being behind an event horizon in a private company.

216
00:20:37,760 --> 00:20:42,240
They will insist that the infrastructure is open. They will insist also that the vetting

217
00:20:42,240 --> 00:20:47,520
process by which those systems are trained be something maybe like Wikipedia, right?

218
00:20:47,520 --> 00:20:51,920
We tend to trust Wikipedia, sometimes with a grain of salt, but we tend to trust Wikipedia

219
00:20:51,920 --> 00:20:56,400
because there is a vetting process so that whenever an article is modified, you know,

220
00:20:56,400 --> 00:21:00,800
some editor kind of check on it and then the changes are accepted or not, things like that.

221
00:21:00,800 --> 00:21:06,320
So, you can imagine that the sort of common repository of all human knowledge that would be

222
00:21:06,320 --> 00:21:11,040
our assistants will be constructed through some sort of cross-sourcing process, perhaps similar

223
00:21:11,040 --> 00:21:15,600
to Wikipedia, where you're going to have a bunch of people training those systems and fine-tuning

224
00:21:15,600 --> 00:21:20,000
them so that, you know, whatever they and so they produced are correct.

225
00:21:20,000 --> 00:21:25,760
It's so funny you say about that kind of the benefit set of the open approach over the closed

226
00:21:25,760 --> 00:21:29,920
approach because that's where I've been kind of stuck, which is like, where does value accrue?

227
00:21:29,920 --> 00:21:34,400
Is it to the closed model or the open model? And then we had the leaked internal Mamo stay

228
00:21:34,400 --> 00:21:39,200
from the Google employee who said, you know, we're not ahead, open AI are not ahead, there's this

229
00:21:39,200 --> 00:21:46,480
third being which is actually far more significant and we haven't taken notice of and summarized.

230
00:21:46,480 --> 00:21:52,080
And that was triggered, that was triggered by by Lama, which is the model that was put together

231
00:21:52,080 --> 00:21:59,600
by my colleagues at FAIR, which was the code was open sourced. The model sadly

232
00:22:01,040 --> 00:22:04,480
was distributed only for research and non-commercial purpose.

233
00:22:05,840 --> 00:22:14,000
And the reason for that is basically complicated legal issues of what's the status of the data

234
00:22:14,000 --> 00:22:17,600
that the system has been trained on and things like that. So, it's more kind of,

235
00:22:18,560 --> 00:22:24,800
it's not a lack of desire from the from meta to open source. It's more kind of complex legal

236
00:22:24,800 --> 00:22:33,600
issues that go beyond my. I'm super naive, Jan. Why does open win against a more controlled,

237
00:22:33,600 --> 00:22:41,360
tight-knit, well-funded open AI or other large corporate with a big balance sheet and a very

238
00:22:41,440 --> 00:22:45,920
rigorous but streamlined team? It's very simple. It's because no outfit

239
00:22:47,040 --> 00:22:53,040
as powerful as they may be has a monopoly on good ideas. So, if you do it in the open,

240
00:22:53,840 --> 00:23:00,480
you basically recruit the entire world's intelligence to contribute to things and

241
00:23:00,480 --> 00:23:06,400
having ideas and ideas that you met as, you know, sorry about, which, you know, an outfit with 400

242
00:23:06,400 --> 00:23:12,240
people has no chance thinking about or even a large company with 50,000 employees may not want to

243
00:23:12,240 --> 00:23:18,880
devote any resources to because they may not think it's useful in the long term or they have,

244
00:23:18,880 --> 00:23:24,560
you know, more urgency to take care of. So, you give it away and then you have, you know,

245
00:23:24,560 --> 00:23:28,160
tons and tons of people, some of whom are, you know, undergraduate students or people,

246
00:23:28,160 --> 00:23:33,120
you know, in their parents' basement. So, coming up with amazing ideas that you would

247
00:23:33,120 --> 00:23:38,480
never have thought about or willing to spend the time to crunch down the, you know,

248
00:23:38,480 --> 00:23:45,760
7 billion weight llama so that it runs on the Mac, on the laptop. Like, oh, that's pretty amazing.

249
00:23:45,760 --> 00:23:53,280
So, I think that's why, you know, open-source projects succeed, particularly when they concern

250
00:23:53,280 --> 00:23:58,880
basic infrastructure. So, if you think about it, the early days of the Internet, there was a battle

251
00:23:58,880 --> 00:24:03,040
between Microsoft and SunMicroSystems to provide the basic infrastructure for the Internet.

252
00:24:04,080 --> 00:24:08,000
You know, the operating system, the web server, you know, things like that, right?

253
00:24:09,920 --> 00:24:14,880
So, on SunMicroSystems, it was Solaris and, you know, whatever web server and Java,

254
00:24:14,880 --> 00:24:19,680
and then on the Microsoft side, there was Windows with IIT or whatever, you know,

255
00:24:19,680 --> 00:24:26,720
an ASP, which was their kind of server and client-side protocol. Both of them lost.

256
00:24:27,360 --> 00:24:32,320
In fact, SunMicroSystem pretty much went bankrupt and was, you know, sold for parts to Oracle.

257
00:24:32,320 --> 00:24:37,360
One was Linux and Apache, which is completely open-source. And you might ask, why? You know,

258
00:24:37,360 --> 00:24:42,240
the entire Internet and the entire tech industry runs on Linux, right? And your phone probably

259
00:24:42,240 --> 00:24:47,440
runs on Linux, too, if you have Android. So, that's three-quarter of the phones in, you know, in the

260
00:24:47,440 --> 00:24:53,040
world. So, the reason for this is that, you know, it's just a much better way of gathering

261
00:24:53,040 --> 00:25:00,000
competence and talent around a common project, even if it's not motivated necessarily by profit.

262
00:25:00,800 --> 00:25:08,080
Yeah, I agree, and I love this. You work with matter. My question and David Marcus' question was,

263
00:25:08,080 --> 00:25:09,520
how does matter win, then?

264
00:25:10,480 --> 00:25:16,720
So, it's been the case that Meta in the past has open-source pretty much everybody,

265
00:25:16,720 --> 00:25:22,880
everything that it's ever produced in terms of basic infrastructure, right? So, you have, you know,

266
00:25:22,960 --> 00:25:30,480
React for, you know, the framework for web and mobile apps. You have PyTorch. PyTorch is not even

267
00:25:30,480 --> 00:25:33,600
owned by Meta anymore. The ownership was transferred to the Linux Foundation,

268
00:25:34,320 --> 00:25:39,120
because it's so essential to the, you know, AI, R&D infrastructure nowadays. You know,

269
00:25:39,120 --> 00:25:45,280
ChatGPT was developed on PyTorch. Okay. All open AI runs on PyTorch. The entire world, in fact,

270
00:25:45,280 --> 00:25:51,360
runs on PyTorch except Google, because they have their own team, right? But it goes beyond that,

271
00:25:51,360 --> 00:25:58,400
right? Meta open-sources its hardware server backplane design, so that hardware manufacturers can

272
00:25:58,400 --> 00:26:04,480
build to its specifications. And pretty much everything, aside from sort of legal issues that

273
00:26:04,480 --> 00:26:10,240
are sometimes due to kind of recent laws or court decisions, pretty much everything has been

274
00:26:10,240 --> 00:26:15,920
open-sourced. It is not because other people can use your technology that you can't exploit it

275
00:26:15,920 --> 00:26:23,200
to the same extent, right? Who can use smart NLP systems for, you know,

276
00:26:23,200 --> 00:26:28,560
translation or content moderation on Facebook, other than Facebook? It doesn't matter if other

277
00:26:28,560 --> 00:26:33,520
people have access to the same technology. I mean, I totally agree with you. And this kind of led

278
00:26:33,520 --> 00:26:38,240
to my next question, which you actually tweaked about, which comes to the size of like data modes

279
00:26:38,240 --> 00:26:44,560
and size of data availability. Is it simply a case that the largest model wins? And how do you think

280
00:26:44,560 --> 00:26:51,760
about value in small models as well? Yeah, so it's not the case. This is really what

281
00:26:52,720 --> 00:26:57,600
Lama has demonstrated and really kind of shown people. So the people behind Lama,

282
00:26:57,600 --> 00:27:02,400
Edouard Grave and Guillaume Lompland and their collaborators, mostly at Fair Paris,

283
00:27:02,400 --> 00:27:06,960
actually many of them are in Paris, they've demonstrated that you don't need those models

284
00:27:06,960 --> 00:27:12,480
to be very large to work really well. I think it caused a bit of an epiphany for a lot of people,

285
00:27:13,120 --> 00:27:17,840
realizing, oh, you know, you don't need, okay, maybe you need a thousand GPUs,

286
00:27:17,840 --> 00:27:22,640
you know, running for 10, you know, a couple of weeks to train it, the base system. In fact,

287
00:27:22,640 --> 00:27:27,600
this, that number is going down too, because people are kind of figuring out how to do this

288
00:27:27,600 --> 00:27:31,760
more efficiently. But once it's pre-trained, you can use it for all kinds of stuff and you can

289
00:27:31,760 --> 00:27:37,920
fine tune it really easily. And, and then at the end, you can run it on your laptop, right?

290
00:27:37,920 --> 00:27:43,680
That's kind of amazing. Or maybe on a, on a, you know, desktop machine with a GPU in it or a

291
00:27:43,680 --> 00:27:49,680
couple GPUs. So I think, you know, it sort of opened the minds of people to the fact that there is

292
00:27:49,680 --> 00:27:54,240
like enormous opportunities that really weren't thought to be possible before. And I think it's

293
00:27:54,240 --> 00:27:59,840
going to make even more progress, because if we go towards the design of AI systems, perhaps along

294
00:27:59,840 --> 00:28:04,880
the lines of what I described with objectives and planning, I think those systems could actually be

295
00:28:04,880 --> 00:28:11,120
even smaller to some extent. How would they be even smaller? Sorry, unpack that for me.

296
00:28:11,120 --> 00:28:16,720
Well, because the current models, for them to work here, to train them on gigantic amounts of data,

297
00:28:17,600 --> 00:28:21,120
way more data than any humans has ever been trained on, right? So the amount of data I

298
00:28:21,120 --> 00:28:30,400
lamar is trained on, for example, is something like 1.4 trillion tokens, which is a, you know,

299
00:28:30,400 --> 00:28:35,120
it's like a quarter of the internet or something. It's something absolutely enormous. It would take

300
00:28:35,120 --> 00:28:41,520
someone reading eight hours a day at normal speed, about 22,000 years to read through that. Okay.

301
00:28:41,520 --> 00:28:47,440
So obviously, those systems can accumulate a lot of knowledge from text, but they don't do it the

302
00:28:47,440 --> 00:28:51,600
same way humans do it, because we don't need that much time to be that smart and to learn

303
00:28:52,640 --> 00:28:57,280
that much. So obviously, we are much more efficient or brains are much more efficient than those models

304
00:28:58,000 --> 00:29:02,800
at learning things. Like, how is it that a teenager can learn to drive a car in about 20

305
00:29:02,800 --> 00:29:07,360
hours of practice? We still don't have level five sort of cars. So obviously, we're missing

306
00:29:07,360 --> 00:29:12,960
something really big. And what we're missing, I think, is abilities for AI systems to learn

307
00:29:12,960 --> 00:29:18,880
how the world works by observation mostly, and then this ability to plan so as to satisfy objectives.

308
00:29:19,680 --> 00:29:26,240
And then beyond that, the ability to set some objectives in the satisfaction of a bigger one.

309
00:29:26,320 --> 00:29:31,920
Okay, let's go to hierarchical planning. And we do this, humans do this. Some animals do this to

310
00:29:31,920 --> 00:29:36,960
some extent. Every animal, you know, mammal and bird is capable of some level of planning.

311
00:29:36,960 --> 00:29:40,720
Autoverseveral animals basically don't do planning or a very simple form of it.

312
00:29:41,440 --> 00:29:46,160
Jan, you mentioned the efficiency that can come from actually smaller models than expected

313
00:29:46,160 --> 00:29:51,600
and how actually size of models isn't everything. The other thing, we spoke about open and closed.

314
00:29:51,600 --> 00:29:54,560
The other thing that I've been thinking, and everyone's been thinking about, and I've interviewed

315
00:29:54,560 --> 00:30:01,040
many kind of leading AI experts, and they say the value will accrue to the incumbents. Startups,

316
00:30:01,040 --> 00:30:04,720
they don't have the data, they don't have the models, it'll accrue to the incumbents.

317
00:30:05,360 --> 00:30:09,680
Is that right? Will the value accrue to the incumbents? Or do you believe that given what

318
00:30:09,680 --> 00:30:15,120
you just said about size not being everything in terms of models, it could be startups as well?

319
00:30:16,480 --> 00:30:22,960
So it depends on which scenario you believe in. So the scenario I think will happen,

320
00:30:23,040 --> 00:30:26,800
and I'm certainly rooting for, is the scenario I described earlier where you have some sort of

321
00:30:26,800 --> 00:30:33,040
open platform for base LLMs. So base LLMs basically would be seen as the basic infrastructure,

322
00:30:34,160 --> 00:30:42,160
like TCP, IP, Linux, Apache, essentially, completely open. And then there would be an ecosystem of

323
00:30:42,160 --> 00:30:46,240
companies building stuff on top of it, which for vertical applications for specific things,

324
00:30:46,240 --> 00:30:50,800
right, to specialize those systems for particular applications, who offer support to make it,

325
00:30:51,360 --> 00:30:58,240
customize for enterprise applications, for personal things. There'll be a whole economy

326
00:30:58,240 --> 00:31:02,800
around this, which will create jobs, by the way, not make them disappear. So this is the scenario

327
00:31:02,800 --> 00:31:08,080
that I believe will happen. And the reason I think it will happen is because there is essentially a

328
00:31:08,080 --> 00:31:14,160
need to use, essentially millions of contributions for making those systems factual and correct,

329
00:31:14,160 --> 00:31:19,680
and etc., so Wikipedia style. So I think the proprietary approaches will actually fall behind.

330
00:31:19,680 --> 00:31:24,880
So that's one point. The second point is, you can ask yourself the question, how is it that

331
00:31:25,680 --> 00:31:32,240
the companies that were best positioned to produce something that charge EPT, namely Google

332
00:31:32,240 --> 00:31:42,320
and Meta, didn't? Why is it open AI? The small ad sheet was 400 people, more now, but actually

333
00:31:42,320 --> 00:31:48,720
small ad sheet. And the answer is, it's not because Google or Meta did not have the competence of

334
00:31:48,720 --> 00:31:56,560
the technology. It's just that they didn't have the pressure to produce new products, completely

335
00:31:56,560 --> 00:32:03,200
new products, that had a lot of risk attached to them. And the risks were, we know where the risks

336
00:32:03,200 --> 00:32:09,680
are, because a few weeks before charge EPT, my colleagues at fair, produced a large language

337
00:32:09,680 --> 00:32:14,800
model called Galactica, which was an experimental system. And they put out a demo, and the demo

338
00:32:14,800 --> 00:32:21,440
was to demonstrate that. So Galactica was a large language model trained on the entirety

339
00:32:21,440 --> 00:32:26,640
of the scientific literature. And it was basically designed to help scientists write papers.

340
00:32:27,360 --> 00:32:31,920
So you would start writing a paragraph or something like that to describe the topic of

341
00:32:31,920 --> 00:32:36,400
paragraphs. And then Galactica would basically complete the paragraph, and it wouldn't be

342
00:32:36,400 --> 00:32:42,400
factually correct. You would have to kind of fix it, but you would ask it to build a table

343
00:32:42,400 --> 00:32:48,160
result, and it would just put the latech commands to kind of build the thing and populate it with

344
00:32:48,160 --> 00:32:53,040
the known results on the literature about the topic that you're working on, or you would type a

345
00:32:53,040 --> 00:32:57,920
chemical formula for something, and it would turn it into an actual name for it, or things of that

346
00:32:57,920 --> 00:33:03,360
type. Very useful for scientists. As soon as the demo was put out, it was murdered by the

347
00:33:04,000 --> 00:33:09,840
social network Twitter sphere. Why? People said, oh, this is going to destroy scientific

348
00:33:09,840 --> 00:33:16,720
publication, because now any random person can write an authoritatively sounding scientific

349
00:33:16,720 --> 00:33:23,920
paper that is nonsense. And there was so much material thrown at the system that the people

350
00:33:23,920 --> 00:33:29,280
at Meta who built it couldn't take it, they took down the demo because they said, we can't sleep

351
00:33:29,280 --> 00:33:33,920
at night. So here is an example of a very useful system, a system that could have been extremely

352
00:33:33,920 --> 00:33:39,680
useful, particularly for writers or scientific papers who are non-native English speakers,

353
00:33:40,640 --> 00:33:48,400
that basically was destroyed by AI do-mers. People who just did not think about the risk-benefit

354
00:33:48,400 --> 00:33:55,840
analysis, the risk of flooding the literature with nonsense is ridiculous. I mean, because, you know,

355
00:33:55,840 --> 00:34:03,200
the scientific publications are vetted and things like that, so there was not a significant danger.

356
00:34:04,880 --> 00:34:09,360
And then Chatchitviti came two weeks later and was welcomed as the second

357
00:34:09,360 --> 00:34:16,560
coming of the Messiah, right? So what does that tell you? And then, you know, a few months later,

358
00:34:16,560 --> 00:34:24,160
Google came out with a bard, and in the demo, a bard made a tiny, you know, minor factual mistake

359
00:34:24,160 --> 00:34:31,520
about some astronomical fact, and, you know, Google's stock went down by 8%. Now, what it tells

360
00:34:31,520 --> 00:34:36,880
you is that when something is produced by a large company that has a reputation, particularly

361
00:34:36,880 --> 00:34:41,520
a reputation to defend, they can put out things that's true nonsense, but it's okay for a small

362
00:34:41,520 --> 00:34:49,680
company. So that's the landscape of what happens now, which is why I think there's a bit of a

363
00:34:49,680 --> 00:34:54,400
paradox which is that the companies that have, you know, the best technology basically can't

364
00:34:54,400 --> 00:35:00,640
have difficulties putting it out because of those legal issues and sort of public image.

365
00:35:01,520 --> 00:35:04,720
Do you not also think there's this core business model challenge there, which is

366
00:35:04,720 --> 00:35:08,720
it's the classic innovators dilemma? Like why didn't Google do this? Because it would have killed

367
00:35:08,720 --> 00:35:15,040
that absolute cash cow of Google ads. The cost to service a query versus the costs of this

368
00:35:15,040 --> 00:35:22,400
is so significantly different. You'd be killing your core cash cow with this, with unknown upside,

369
00:35:23,360 --> 00:35:30,160
versus retaining what is a great business. You don't have a choice. I mean, there's no question

370
00:35:30,240 --> 00:35:37,200
that within some time, it could take a while, but there is no question that people interact

371
00:35:37,200 --> 00:35:44,480
mostly with the digital world using AI assistance. And they may run into your augmented reality

372
00:35:44,480 --> 00:35:53,920
glasses or something of that type. Like in the Spike Jon's movie, Her, that's not a bad depiction

373
00:35:53,920 --> 00:36:00,080
of what, you know, the way things could develop. And so if you take the assumption, make the assumption

374
00:36:00,080 --> 00:36:05,600
this is going to happen, you have to build it as quickly as you can. And it might cannibalize

375
00:36:05,600 --> 00:36:10,320
your news feed algorithm or whatever, or because of Google, your search engine,

376
00:36:11,120 --> 00:36:17,040
but you have to do it. You know, it's like, I mean, meta has been known to make those choices

377
00:36:17,040 --> 00:36:26,080
in the past, like the move to mobile, for example, and the move to short form video, for example,

378
00:36:26,080 --> 00:36:32,640
which obviously TikTok has been very successful at. Meta has entered that business in kind of a

379
00:36:33,440 --> 00:36:38,320
big way, despite the fact that the amount of revenue it derives from it is lower than a traditional

380
00:36:38,320 --> 00:36:45,200
news feed, because it's hard to put ads and videos basically. You mentioned the job creation

381
00:36:45,200 --> 00:36:50,400
element there. I do just want to touch on the job side, because it's the classic AI doomer that's

382
00:36:50,400 --> 00:36:54,160
we're all going to be unemployed, and we're going to have universal basic income in an

383
00:36:54,160 --> 00:36:59,760
optimistic world. You said about job creation there. We don't hear about job creation through AI.

384
00:36:59,760 --> 00:37:05,520
How do you see what jobs will be created through this new ecosystem and what that world of employment

385
00:37:05,520 --> 00:37:14,160
could look like? So 100 years ago, or maybe 120 years ago, most people in most of the world

386
00:37:14,880 --> 00:37:22,720
worked in the fields in food production. There's pretty much a majority of the population.

387
00:37:23,680 --> 00:37:26,960
Today, in developed countries, it's between one and two percent.

388
00:37:29,600 --> 00:37:38,400
And that has caused a migration of people into the cities and the development of service, business,

389
00:37:39,200 --> 00:37:45,040
you know, the same thing 20 years ago or 20, 30 years ago, there was a big movement towards

390
00:37:45,040 --> 00:37:50,560
automation of manufacturing. And a lot of manufacturing jobs disappeared in developed

391
00:37:50,560 --> 00:37:54,960
countries, but they were replaced by other things. So 20 years ago, who would have thought that you

392
00:37:54,960 --> 00:38:02,320
could make a living with a podcast? I didn't think I could five years ago, Jan. I'm as surprised as

393
00:38:02,320 --> 00:38:09,440
everyone else. Right. So, you know, a lot of jobs appear like, you know, 30 years ago, there was no

394
00:38:09,440 --> 00:38:14,080
such thing as web designer. And now it's, you know, have engineers in the world basically do this,

395
00:38:14,080 --> 00:38:19,120
right? So, you know, the number of economists that I have talked to, which is pretty large,

396
00:38:19,120 --> 00:38:23,600
about where I asked that question, we tell me, well, we're going to run out of jobs because,

397
00:38:23,600 --> 00:38:29,440
you know, we're all going to be replaced by, I think, is exactly zero. Like, no economics believes

398
00:38:29,440 --> 00:38:34,000
this. No economics believes we're going to run out of job because no economics believes that we're

399
00:38:34,000 --> 00:38:39,040
going to run out of problems to solve or requirement for human creativity and human

400
00:38:39,040 --> 00:38:43,360
communication and stuff like that. So, you know, this is going to create as many jobs as it makes

401
00:38:43,360 --> 00:38:47,520
disappear. Now, the question is, though, and those jobs, by the way, are going to be more

402
00:38:47,520 --> 00:38:52,240
productive. So overall, technology makes people more productive. In other words, for the same

403
00:38:52,240 --> 00:39:00,560
amount of hours worked, you produce more wealth, okay? But every technological revolution, unless

404
00:39:00,560 --> 00:39:07,360
it's accompanied by sort of, you know, political changes and social changes, generally profit

405
00:39:07,360 --> 00:39:13,360
a small number of people, at least temporarily, right? That happened in the industrial revolution

406
00:39:13,360 --> 00:39:17,440
in the late 19th century, where, you know, a few people became extremely rich and a lot of people

407
00:39:17,440 --> 00:39:21,760
were exploited. And then, you know, society changed. And there were like social programs and,

408
00:39:22,320 --> 00:39:28,240
and, you know, income tax and, and high tax for richer people and stuff like that, which the U.S.

409
00:39:28,240 --> 00:39:33,920
has backpedaled on this, but not Europe, or the UK to some extent, too, but not the rest of Europe.

410
00:39:35,040 --> 00:39:38,640
So there is a question of, you know, how you distribute the wealth if you want, okay? How

411
00:39:38,640 --> 00:39:42,720
do you organize society so everyone profits from it? But that's a political question. There's not

412
00:39:42,720 --> 00:39:47,360
a technology question. It's not new. It's not caused by AI. It's just caused by

413
00:39:47,360 --> 00:39:51,440
technological evolution, right? It's not a recent, a recent phenomenon.

414
00:39:51,440 --> 00:39:56,080
This is so unfair of me to ask. But what do those jobs look like? Like, what are they?

415
00:39:56,080 --> 00:40:01,120
Are they, they're creative oriented? But what does that actually mean? Like, sorry, I know it's a

416
00:40:01,120 --> 00:40:04,800
really hard question, but I'm just trying to understand how, how we actually spend our time

417
00:40:04,800 --> 00:40:10,800
in my children, which I don't have, by the way, Jan. But what, what do they do? Like sculpt or paint?

418
00:40:11,040 --> 00:40:11,520
I don't know.

419
00:40:13,440 --> 00:40:18,000
I don't know. That's a good question. But it's not because I don't know that it won't happen,

420
00:40:18,000 --> 00:40:23,920
because I mean, look at like how many people exercise their creative juices today, right?

421
00:40:23,920 --> 00:40:29,440
With all the tools that are available that, you know, weren't available 10, 20 or 30 years ago,

422
00:40:29,440 --> 00:40:33,360
like 3D artists or something like this, you know, game designers, you know, all kinds of things.

423
00:40:33,360 --> 00:40:37,760
Like, you know, I think creative jobs are the other ones. So there are two types of jobs that

424
00:40:37,760 --> 00:40:42,400
that, you know, have a bright future of creative jobs, whether they are scientific, technical,

425
00:40:42,400 --> 00:40:47,520
educational, or artistic. ACI has to do with communication, right? And communication of

426
00:40:47,520 --> 00:40:52,480
human emotions, which is, you know, intrinsically human, if you want. So that's one category.

427
00:40:52,480 --> 00:40:57,280
And then the other one is personal services. So where you need, you know, actual people

428
00:40:58,080 --> 00:40:58,880
to interact with you.

429
00:40:59,680 --> 00:41:05,120
I totally agree and get you. And I love, I love that we shall see class. The only thing that I

430
00:41:05,120 --> 00:41:09,840
worry about is like the speed of transition. Like when you look at past industrial revolution,

431
00:41:09,840 --> 00:41:13,760
when you think even the introduction of PCs into kind of, you know, working environments,

432
00:41:13,760 --> 00:41:21,040
these were multi decade introductions. Blundly, what AI feels like in some industries today,

433
00:41:21,040 --> 00:41:26,800
we use it at the media company, and it's cutting our employee like the speed of transition is much,

434
00:41:26,800 --> 00:41:33,440
much more compressed in this timeline, which will lead to short term significant high unemployment.

435
00:41:33,440 --> 00:41:36,160
Do you concede that or do you not concede that?

436
00:41:36,160 --> 00:41:42,080
So this is something I used to be very worried about, that the speed of progress of technology

437
00:41:42,080 --> 00:41:48,160
was going to leave a certain number of people behind who, you know, cannot be basically retrained

438
00:41:48,160 --> 00:41:53,680
fast enough or maybe they're too old to retrain themselves for the new, the new world. I was

439
00:41:53,680 --> 00:41:57,760
worried about this. And then I talked to a bunch of economists and they say, oh, you know, not

440
00:41:57,760 --> 00:42:05,040
really because the speed at which a technology disseminate in the economy is actually limited

441
00:42:05,680 --> 00:42:11,280
by how fast people can learn to use it. So a good person to talk to about this is Eric

442
00:42:11,280 --> 00:42:17,280
Binobsen at Stanford. And what he says is that when a new technology is introduced, let's say the

443
00:42:17,280 --> 00:42:22,320
PC, right, with, you know, graphical user interface, the mouse, et cetera, right, in the mid 90s,

444
00:42:23,280 --> 00:42:30,720
how long did it take to have a measurable effect on productivity, you know, which is the amount

445
00:42:30,720 --> 00:42:36,000
of wealth produced by per hour worked. He says, you know, typically it's 15, 20 years. And the

446
00:42:36,000 --> 00:42:40,080
reason is that that's what it takes for people to learn to use that new technology basically.

447
00:42:40,640 --> 00:42:45,280
But you buy that here, like people are pretty good at prompts, you know, social media content

448
00:42:45,280 --> 00:42:50,640
managers are using prompts very efficiently to produce content plans, to create content ideas in

449
00:42:51,440 --> 00:42:57,440
under half an hour after watching a couple of TikToks. Yeah. But like, what is going to be the

450
00:42:57,440 --> 00:43:03,840
effect of this on, first of all, on measurable productivity, second of all, on the the job

451
00:43:03,840 --> 00:43:08,480
market, like, is it going to make people lose their job like right away? And no, it's going to take

452
00:43:08,480 --> 00:43:12,480
a while. It's going to take 10, 15 years, you know, possibly more. It depends when you start

453
00:43:12,480 --> 00:43:16,800
counting, right, because the AI revolution maybe started 10 years ago. So if you start counting

454
00:43:16,800 --> 00:43:21,200
then, then it might only take, you know, another 10 years. But you know, I mean, I don't think you

455
00:43:21,200 --> 00:43:27,920
want to underestimate the degree of conservativeness of the business world, right? Things tend to

456
00:43:27,920 --> 00:43:33,920
change not that quickly. But if it's that easy to learn, like people will learn it and then invent

457
00:43:33,920 --> 00:43:40,720
new professions out of it, or become more productive themselves. Why do you think we love the doom,

458
00:43:40,720 --> 00:43:45,760
Jan? You know, I love your approach in mindset, and I agree with it. But why do you think we are

459
00:43:45,760 --> 00:43:49,440
kind of magnetized to like, oh, we're all going to be unemployed in the doom?

460
00:43:50,960 --> 00:43:55,680
Well, because I think for a number of reasons, so I'm not a, you know, social psychologist or

461
00:43:55,680 --> 00:44:02,800
sociologist, but but clearly, I think we're hardwired to pay attention to things that occur or may

462
00:44:02,800 --> 00:44:08,320
occur that could be dangerous to us. Because it means that there's something about the world that

463
00:44:08,320 --> 00:44:12,800
we don't completely understand, and we do have to pay attention to it and be careful about it.

464
00:44:12,800 --> 00:44:18,320
So for example, take a young, a young child, five months old, and show a scenario to this

465
00:44:18,320 --> 00:44:23,280
small child of a little car that is sitting on the platform, and then you push the car off

466
00:44:23,280 --> 00:44:28,320
the platform and instead of falling, the car appears to float in the air. The five months old

467
00:44:28,320 --> 00:44:33,680
will barely pay attention to it. But if you show this to a 10 months old, the 10 months old will

468
00:44:34,480 --> 00:44:40,080
look at it with huge eyes and stare at it for a long time, wondering what's going on. Because in

469
00:44:40,080 --> 00:44:45,360
the meantime, babies around the age of, you know, between, between six and nine months learn about

470
00:44:45,360 --> 00:44:49,760
gravity. They learn that objects that are not supported are supposed to fall. And so the mental

471
00:44:49,760 --> 00:44:54,560
model is that an object is not supported to fall. And they see this object that appears to float in

472
00:44:54,560 --> 00:44:59,920
the air. And they say, like, this can be like, you know, there's something I didn't, I didn't,

473
00:44:59,920 --> 00:45:04,240
I don't understand about the world, I need to look at this and investigate. Okay, so we're hardwired

474
00:45:04,240 --> 00:45:08,560
for this, because that's the way we learn our internal mental model of the world that allows

475
00:45:08,640 --> 00:45:13,840
us to predict what's going to happen, allows us to plan. That's what makes us smart. That's the

476
00:45:13,840 --> 00:45:19,600
basis of intelligence, the ability to predict. And so we naturally pay attention to stuff that

477
00:45:19,600 --> 00:45:28,720
is surprising, or dangerous, or both, which is why, you know, you see a outrageous piece of news,

478
00:45:28,720 --> 00:45:34,480
you know, a clickbait at the bottom of some, you know, website. And like, you have to convince

479
00:45:34,560 --> 00:45:39,760
yourself not to click on it. Can I ask you a couple of direct questions? I'm just too

480
00:45:39,760 --> 00:45:45,360
interested and we can take them out if needed. What did you say to Jeff when you heard that he

481
00:45:45,360 --> 00:45:49,760
was obviously making the moves that he did? I'm sure you had a conversation with him. What did

482
00:45:49,760 --> 00:45:57,040
you say to him? We haven't spoken yet, actually. We're going to speak to kind of get, you know,

483
00:45:57,040 --> 00:46:03,360
each other's opinion on it. I don't think he knows my opinion on this, because I don't think he

484
00:46:03,360 --> 00:46:07,200
follows, you know, what I post on Twitter or whatever, even though he is on Twitter himself.

485
00:46:07,200 --> 00:46:11,840
But so I think we have, you know, a discussion to have. I've had this discussion before with

486
00:46:11,840 --> 00:46:20,240
Yoshua Bengio, but not with Jeff. And to me, the fact that he left Google is not particularly a

487
00:46:20,240 --> 00:46:28,400
surprise. The fact that he leaves Google to be able to speak his mind, I think is not surprising.

488
00:46:28,400 --> 00:46:34,720
So I have a very different deal at Nitta, which is that I say whatever I want. Okay. I'm not under

489
00:46:34,720 --> 00:46:42,640
the tight control of, you know, the communications department or anything. I just say what I think.

490
00:46:42,640 --> 00:46:50,720
All right. How did you get that deal? Yeah. But no, seriously, many of my friends at Mesa,

491
00:46:50,720 --> 00:46:56,000
in very high positions, as you know, with mutual friends, they don't have that deal.

492
00:46:56,160 --> 00:47:02,960
So there is, I mean, I mean, a particularly sweet spot because I have a

493
00:47:03,920 --> 00:47:11,280
quite a bit of following people who trust me or believe me or want to hear what I have to say,

494
00:47:11,280 --> 00:47:18,240
even if they don't trust me at all. And at the same time, I'm not an officer. So I,

495
00:47:18,240 --> 00:47:22,880
it's not like, you know, there are things I can't say because of legal issues of, you know,

496
00:47:22,880 --> 00:47:27,760
financial blah, blah, blah, right. I'm a vice president, but I'm just below the level where

497
00:47:27,760 --> 00:47:32,160
you had to be really, really careful and so control your message. And I think there is

498
00:47:32,160 --> 00:47:41,360
a cost-benefit tradeoff here of, you know, AI is such a complicated, fast-evolving issue that

499
00:47:41,360 --> 00:47:47,600
you basically, you need someone to be able to, you know, speak freely. And I think Jeff didn't

500
00:47:47,600 --> 00:47:53,920
feel like he had that option at Google, maybe, you know, for various reasons. So I understand why

501
00:47:54,800 --> 00:48:00,240
he might have wanted to leave, but I don't, I don't agree with him at all with the whole

502
00:48:00,240 --> 00:48:04,320
sort of, you know, probability of human extinction or whatever.

503
00:48:04,320 --> 00:48:09,120
Have you ever felt your role at Mesa has impeded your ability to be impartial?

504
00:48:09,760 --> 00:48:15,120
I don't believe so, no. But I mean, there are certain things that I would post on social media

505
00:48:15,120 --> 00:48:20,640
that are kind of, you know, kind of popping up the work of my colleagues. And, you know,

506
00:48:20,640 --> 00:48:24,160
I'm obviously biased about this because, you know, I know about the work and they are friends

507
00:48:24,160 --> 00:48:29,840
and colleagues. And, you know, I think it's interesting probably because, you know, I feel

508
00:48:29,840 --> 00:48:34,240
the part of it. I totally agree. For this kind of stuff, I might be biased. Take this with a

509
00:48:34,240 --> 00:48:38,240
grain of salt. You don't have to believe me. You know, things like that. But it's given me

510
00:48:38,240 --> 00:48:44,320
a vision also of, you know, how things are built, what the problems are. So, you know, for example,

511
00:48:44,560 --> 00:48:52,800
there's a narrative, a very, very common narrative that AI is the culprit for a lot of the bad side

512
00:48:52,800 --> 00:48:59,760
effects of social networks in the past. And in fact, it's completely backwards. AI is the solution

513
00:48:59,760 --> 00:49:06,720
to those problems. So, you know, let me tell you, you know, go back like, you know, backpedal

514
00:49:06,720 --> 00:49:10,800
12 years ago or something, you know, even before I joined META, where META, you know, started

515
00:49:10,800 --> 00:49:16,400
experimenting with the newsfeed. And the newsfeed was, you know, an algorithm that would pick,

516
00:49:16,400 --> 00:49:21,520
like, which piece of news to show to everyone. And, you know, originally it was decided by, you

517
00:49:21,520 --> 00:49:26,080
know, how friends are you with a person making the post and things like that, right? How many

518
00:49:26,080 --> 00:49:30,240
interactions you have with that person. Eventually, a bit of machine learning was put into it

519
00:49:30,240 --> 00:49:34,640
shortly before I joined META. It was very, very simple. It was something like logistic regression,

520
00:49:34,640 --> 00:49:39,920
something like the simplest method you can imagine, with a lot of engineering behind it and a lot of,

521
00:49:39,920 --> 00:49:44,400
you know, hacks by hand and special cases. But basically, it was something like logistic

522
00:49:44,400 --> 00:49:48,640
regression, you know, some big vector that describes, you know, what you click on, like,

523
00:49:48,640 --> 00:49:52,880
how many times you, you know, how much time you spent on a particular piece of content and blah,

524
00:49:52,880 --> 00:49:58,160
blah, blah. And then it would decide, like, you know, give a rating to everything. So,

525
00:49:58,160 --> 00:50:04,800
that was deployed. And people ended up spending more time on Facebook. But then also, it created

526
00:50:04,800 --> 00:50:09,120
problems that were quickly identified, like, you know, like information bubbles in the context

527
00:50:09,120 --> 00:50:17,600
of political discourse. And the fact that what I was talking about earlier, that people tend to

528
00:50:17,600 --> 00:50:23,200
click on things that is more interesting, right? So, it caused, you know, the appearance of

529
00:50:23,200 --> 00:50:27,600
clickbait companies that, because you were just like farms of, you know, teenagers in

530
00:50:28,400 --> 00:50:33,280
Montenegro or someplace, making false news to get people to click on them and get money from

531
00:50:33,280 --> 00:50:37,920
the ads that they show them. So, then, you know, this was realized there were, like, big groups

532
00:50:37,920 --> 00:50:43,280
that at Facebook at the time kind of studying the, where the effect of those things are,

533
00:50:43,280 --> 00:50:47,360
and this was corrected. So, that's the way you, you make some work, right? You,

534
00:50:48,720 --> 00:50:52,480
you try the most small scale, you see what the effect is, if there is bad side effect,

535
00:50:52,480 --> 00:50:56,720
you correct it, and then you sort of compare, you know, to two systems. And then sometimes,

536
00:50:56,720 --> 00:51:01,120
something unexpected occurs and you have to back that all and completely change the way you do

537
00:51:01,120 --> 00:51:05,360
things. That's what happened in 2017 after the presidential election, American presidential

538
00:51:05,360 --> 00:51:10,320
election in 2016. The main newsy algorithm was completely changed, so that, you know,

539
00:51:10,320 --> 00:51:14,960
there was no clickbaits anymore. There was no, like, you know, news outlets that could, like,

540
00:51:14,960 --> 00:51:19,360
push their content that was propaganda, basically, you know, much more effort to take down false

541
00:51:19,360 --> 00:51:24,880
accounts and attempts to corrupt the democratic system and stuff like that, right? So, you,

542
00:51:24,880 --> 00:51:29,520
you correct it. And then what the progress of AI over the last few years basically allowed

543
00:51:30,320 --> 00:51:34,720
systems to be deployed to do things like taking, take down hate speech relatively,

544
00:51:36,080 --> 00:51:40,880
reliably, in hundreds of different languages, which was basically impossible to do before.

545
00:51:41,520 --> 00:51:44,960
You mentioned correct it. I promised last question, then we'll do a quick fire. You mentioned

546
00:51:44,960 --> 00:51:50,160
correct it. Elon Musk said with Tucker Carlson, the trouble with AI is you can't release and then

547
00:51:50,160 --> 00:51:55,600
correct. Unlike all prior technological developments, once released, it is too powerful

548
00:51:55,600 --> 00:52:01,840
to be able to bring back into the box. It cannot be amended in that way. Is that not true?

549
00:52:02,480 --> 00:52:09,040
That's not true. That's completely false. It makes an assumption which Elon and some other people

550
00:52:09,040 --> 00:52:15,200
may have become convinced of by reading, you know, Nick Boxtron's book, Super Intelligence, or,

551
00:52:15,920 --> 00:52:23,120
or reading, you know, some of Elias O'Yudkowski's writing. So this is predicated on an assumption

552
00:52:23,120 --> 00:52:29,280
that is just false, which is the existence of a heart takeoff. Right. So the fact that

553
00:52:30,000 --> 00:52:34,400
the minute you turn on a super intelligent AI system is going to take over the world,

554
00:52:35,520 --> 00:52:41,280
and it's going to escape your control, and it's going to refine itself to be even more intelligent.

555
00:52:41,280 --> 00:52:47,920
And so, you know, and the world would be destroyed. And that's just ridiculous. It's just completely

556
00:52:47,920 --> 00:52:56,720
ridiculous because there is no process in the real world that is exponential for very long.

557
00:52:58,800 --> 00:53:02,880
You know, those systems will have to, like, recruit all the resources in the world.

558
00:53:02,880 --> 00:53:11,040
They would have to be given, you know, limitless power agency. Like, why would we do this? And

559
00:53:11,040 --> 00:53:16,080
what's more, they would have to be built so that they have a desire to take over. Like,

560
00:53:16,160 --> 00:53:19,280
you know, systems are not going to take over just because they are intelligent.

561
00:53:19,920 --> 00:53:24,480
Because again, you know, in, even within the human species, it is not the most intelligent

562
00:53:24,480 --> 00:53:31,200
among us that want to dominate others. So his desire and many other leaders desire to prevent

563
00:53:31,760 --> 00:53:36,320
any further development and to regulate intensely right now and stop all progression

564
00:53:36,960 --> 00:53:44,480
is BS, basically. It's obscurantism. Yeah. Right. It's like, it's like people who wanted to

565
00:53:44,480 --> 00:53:49,920
stop the printing press and the diffusion of printed books because, you know, if people could

566
00:53:49,920 --> 00:53:54,000
read the Bible for themselves, they wouldn't have to talk to priests anymore and then would have

567
00:53:54,000 --> 00:53:59,600
their own idea about religion. And that's exactly what happened. People read the Bible for themselves

568
00:53:59,600 --> 00:54:04,720
and that created the Protestant movement in Europe. And that created 200 years of religious conflicts.

569
00:54:04,720 --> 00:54:12,080
But it also brought to us the enlightenment, science, rationalism, philosophy, ideas of democracy,

570
00:54:12,080 --> 00:54:16,800
and then the French and American revolutions. And then, you know, you can compare this with the

571
00:54:16,800 --> 00:54:21,440
Ottoman Empire, which for reasons of being able to control their population, you know,

572
00:54:21,440 --> 00:54:27,200
basically stopped, forbid the use of the printing press. And it started 300 years of decline.

573
00:54:27,760 --> 00:54:31,520
They were dominating science in the Middle Ages, the Muslim world,

574
00:54:31,520 --> 00:54:34,400
which is why, you know, every star in the sky has an Arabic name.

575
00:54:35,520 --> 00:54:39,200
I love this. I'm going to do a quick fire around with you now. So I say a short statement and

576
00:54:39,200 --> 00:54:42,880
you give me your immediate thoughts and then we'll rock and roll. Does that sound okay?

577
00:54:42,880 --> 00:54:49,920
Sounds good. So which regions most need to change their modus operandi when it comes to the practice

578
00:54:49,920 --> 00:54:59,200
of scientific research and incentive mechanisms? Which region? Oh, wow. Pretty much every region.

579
00:54:59,840 --> 00:55:07,520
I'm afraid, but for different reasons. So you saw with China. So China has a bit of an epidemic

580
00:55:08,400 --> 00:55:13,680
of bad science. There's a lot of very smart people in China, a lot of very good researchers,

581
00:55:13,680 --> 00:55:17,280
a lot of very good work coming out of China, particularly in AI, particularly in computer

582
00:55:17,280 --> 00:55:22,960
vision. But a lot of absolutely terrible work that has to be retracted a few months later,

583
00:55:22,960 --> 00:55:29,440
it's been published. And it's partly because of the incentive mechanisms in the academic and

584
00:55:30,160 --> 00:55:35,280
system in China. So this is important to fix there. I can move to Europe. So

585
00:55:36,560 --> 00:55:42,400
in Europe, there are good things. So the education system for like undergrad rates,

586
00:55:42,400 --> 00:55:48,480
education in Europe is great. It's fantastic, because it's party free. So that allows

587
00:55:49,280 --> 00:55:54,400
talented people to go to the schools, even if they are not rich, right?

588
00:55:55,360 --> 00:55:58,720
Which is not the case in the US, for example, at least not to the same extent.

589
00:55:58,720 --> 00:56:03,920
That's good for Europe. A lot of European engineers and scientists are great,

590
00:56:03,920 --> 00:56:07,760
atop base in the world. But then what are the opportunities for people who want to

591
00:56:08,400 --> 00:56:15,200
go into science and research? And most European countries actually don't have systems that

592
00:56:15,200 --> 00:56:21,280
really encourage this and motivate the most talented people and students to go into science.

593
00:56:21,280 --> 00:56:28,560
And so some of them go to North America like me 35 years ago. There are opportunities now

594
00:56:28,720 --> 00:56:37,280
that are really good in research labs like fair in Paris, or Google also has labs in Paris.

595
00:56:37,280 --> 00:56:42,480
Actually, my brother works at Google in Paris. So there are other outfits. So that gives

596
00:56:42,480 --> 00:56:48,080
opportunities for people who really want to be productive and don't think that they can

597
00:56:48,080 --> 00:56:53,440
in the public research and academic system in France and the rest of Europe.

598
00:56:54,400 --> 00:57:00,080
The only European countries that can rival the US in terms of the quality of

599
00:57:00,640 --> 00:57:02,880
job for an academic or a scientist is Switzerland.

600
00:57:04,000 --> 00:57:08,800
What do you think they do to rival that? What is it about their incentive

601
00:57:08,800 --> 00:57:11,680
mechanism structure that gives them that ability?

602
00:57:12,400 --> 00:57:19,920
Two things. They pay people better. Second thing is they give them resources for research.

603
00:57:19,920 --> 00:57:23,840
They can get extra resources through grants and stuff like that, but they're good. And then

604
00:57:23,840 --> 00:57:28,320
they also attract some of the best students in the world. So you get the ideal combination that

605
00:57:28,320 --> 00:57:32,560
you only get in the top 30 universities in North America.

606
00:57:32,560 --> 00:57:37,280
So we've got China, we've got Europe. What about the US? What could they do differently or improve?

607
00:57:37,840 --> 00:57:43,840
Well, so there's a lot that the US does right in terms of research, which is to a large extent

608
00:57:44,800 --> 00:57:51,680
a bit of a partial explanation for the success of the technological industry, the tech industry

609
00:57:51,680 --> 00:57:59,440
in the US. I think partly because the US devotes a significant amount of resources to

610
00:57:59,440 --> 00:58:06,720
fundamental research through NSF and NIH and various other outfits, probably more than Europe.

611
00:58:06,720 --> 00:58:13,440
Universities pays their faculty pretty well, particularly in areas like computer science and AI.

612
00:58:13,440 --> 00:58:18,880
Now this comes with a downside. The downside is that studying in the US is expensive.

613
00:58:19,680 --> 00:58:22,400
It's a trade-off, right? So can you do one without the other?

614
00:58:22,400 --> 00:58:27,280
Switzerland figured out how to pay academics pretty well while actually

615
00:58:27,280 --> 00:58:31,680
offering free education to their students. So there is a way to do it.

616
00:58:32,240 --> 00:58:34,960
Canada also figured out a pretty good trade-off as well.

617
00:58:34,960 --> 00:58:41,440
So in other things, the US does right. But one thing that the US system or like the rough does

618
00:58:41,440 --> 00:58:47,680
right also is the willingness to take risk and invest on ideas that seem a little crazy,

619
00:58:47,680 --> 00:58:55,520
but basically the sort of vibrant startup scene in Silicon Valley and other places in the US,

620
00:58:55,520 --> 00:59:04,240
in New York, and in the Boston area is leading the world. Now you start seeing a similar thing in

621
00:59:04,240 --> 00:59:11,120
Europe now. There's been like enormous growth, for example, of tech startups in Paris,

622
00:59:11,120 --> 00:59:17,600
Paris area, in France more generally, and continental Europe, more widely in the UK as well.

623
00:59:17,600 --> 00:59:23,200
And so I think that's a good thing, but it's still a little more difficult to have access

624
00:59:23,200 --> 00:59:25,760
to investment money in Europe than it is in the US.

625
00:59:25,760 --> 00:59:31,920
That's why I'm here, Jan. I'm happy to provide it. I'm going to do an ultimate one for you.

626
00:59:31,920 --> 00:59:35,040
When you think about what you'd most like someone listening to take away,

627
00:59:35,600 --> 00:59:40,400
what would it be when they hear this? What do you want them to take away as the number one thing?

628
00:59:41,200 --> 00:59:50,160
AI is going to bring a new renaissance for humanity, a new kind of new form of enlightenment,

629
00:59:50,160 --> 00:59:57,280
if you want, because AI is going to amplify everybody's intelligence. It's like every one

630
00:59:57,360 --> 01:00:03,520
of us will have a staff of people who are smarter than us and know most things about most things

631
01:00:03,520 --> 01:00:08,480
and most topics. So it's going to empower every one of us. It's going to make us more creative

632
01:00:08,480 --> 01:00:15,840
because we'll be able to produce text, art, music, videos without necessarily having all the

633
01:00:15,840 --> 01:00:21,680
technical skills that are currently required for doing those things. And so exercise our creative

634
01:00:21,680 --> 01:00:26,880
juices. So that's the positive side. There are risks. There's no question. But it's not like

635
01:00:26,880 --> 01:00:31,520
those risks. Don't believe the people who tell you that those risks are inevitable or that they

636
01:00:31,520 --> 01:00:38,960
will inevitably lead to catastrophe. That's just not true. It's like, place yourself in 1920. Who

637
01:00:38,960 --> 01:00:46,000
would have thought that a mere 50 years later, you could cross the Atlantic in a few hours in

638
01:00:46,000 --> 01:00:54,480
complete safety at near the speed of sound? Would people seriously want to ban aviation

639
01:00:54,480 --> 01:01:00,160
or call for regulation of jet engines before jet engines existed? I mean, that's kind of insane.

640
01:01:00,960 --> 01:01:06,080
So I'm not against regulation. There should be regulation of AI products, particularly the ones

641
01:01:06,080 --> 01:01:11,360
that involve making critical decisions for people. But regulating or slowing down research is

642
01:01:11,360 --> 01:01:17,520
complete nonsense. It's just obscurantism. Who's incumbent team do you most respect and admire

643
01:01:17,520 --> 01:01:25,200
when you look at Amazon, Facebook, Google, in terms of their approach and talent internally?

644
01:01:25,200 --> 01:01:30,720
Outside of meta, obviously. So this is changing a lot. And the reason it's changing is because

645
01:01:31,280 --> 01:01:35,600
a lot of people are leaving large companies and large labs. And the reason they're doing this is

646
01:01:35,600 --> 01:01:41,120
that until recently, a lot of AI research was very exploratory. And now there's a path towards

647
01:01:41,120 --> 01:01:46,320
commercialization for a lot of things. And so people think that they're better off just leaving

648
01:01:46,320 --> 01:01:50,000
large companies and doing this on their own, doing a startup and things like that. So you see

649
01:01:51,280 --> 01:01:57,200
a relatively large motion of applied research engineers, a few scientists

650
01:01:58,800 --> 01:02:04,720
basically leaving those labs to do startups. And that's across the board. So you look at the

651
01:02:04,720 --> 01:02:10,720
original paper from Google about BERT or Transformers. The thing that revolutionized

652
01:02:10,720 --> 01:02:16,960
NLP, all of them have left. Okay, they're only startups. Some of the people who produced LANA,

653
01:02:16,960 --> 01:02:24,320
the open source LLMs from meta. So the key people have left already, okay, to do startups.

654
01:02:24,320 --> 01:02:29,600
I saw their companies. Yeah, there's one called Mistral.

655
01:02:29,600 --> 01:02:35,120
That's the one I saw. Yeah. Right. But they, yeah, there is insane amount of money in dates.

656
01:02:35,680 --> 01:02:42,960
Yeah. Yeah. So, you know, we're proud of them. But I mean, I'm sad that they'd have to, I mean,

657
01:02:42,960 --> 01:02:47,200
I would just say, but he's an existing team. Me, like, yeah, they're good.

658
01:02:47,840 --> 01:02:54,720
Yeah. So, but I think like in terms of the basic competence and the people who are going to push

659
01:02:55,520 --> 01:03:00,960
the science forward, because what we need now is not to work on applications with LLMs. There's

660
01:03:00,960 --> 01:03:04,640
a lot of people who are capable of doing this and they're going to do the job. What we need to do

661
01:03:05,440 --> 01:03:10,480
people like me who are, you know, reworking on research is kind of coming up with new concepts

662
01:03:10,480 --> 01:03:15,360
that will allow us to, you know, get machines that basically have common sense, have an experience

663
01:03:15,360 --> 01:03:21,120
of the real world, have, you know, basically human level intelligence, right? And, you know,

664
01:03:21,120 --> 01:03:25,760
in my opinion, the, the outfits that are best positioned for this are fair from on one side.

665
01:03:26,800 --> 01:03:30,560
And the new deep mind now, which is, you know, deep mind plus group of brain.

666
01:03:30,560 --> 01:03:33,680
Yeah. There are a lot of people there who are interested in that question. And I think they

667
01:03:33,680 --> 01:03:39,760
are probably the best together with Mita and fair. They're the best positions to position,

668
01:03:39,760 --> 01:03:44,480
to really kind of have an impact on this, something, you know, all of us have been working on for,

669
01:03:44,480 --> 01:03:45,440
for quite a while.

670
01:03:45,440 --> 01:03:51,760
Jan, if we do this again in 10 years time, where is Jan in 10 years time in 2033?

671
01:03:52,320 --> 01:03:59,200
Well, I'm 63. So, you know, 10 years on now, well, 12 years on now, I'll be, I'll be Jeff

672
01:03:59,200 --> 01:04:09,520
into the age. Okay. And I don't know. I think I'm excited like, like a teenager now,

673
01:04:10,240 --> 01:04:16,640
because I see the opportunity of like the next step in AI and opportunity perhaps to,

674
01:04:17,520 --> 01:04:22,320
you know, get to the goal that I set myself so that I imagined for myself when I started

675
01:04:22,320 --> 01:04:28,800
working on AI many years ago, which of course I was very naive about at the time of understanding

676
01:04:28,800 --> 01:04:33,600
intelligence, first of all, and it's a scientific question. What is intelligence? What is human

677
01:04:33,600 --> 01:04:38,240
intelligence? And one good way as an engineer, a good way to understand intelligence is to

678
01:04:38,240 --> 01:04:44,800
build a widget that actually reproduces it, right, to some extent. So I'm excited about this right

679
01:04:44,800 --> 01:04:55,520
now. I'll find the, you know, the, the, the, the substrate, the landscape, the, the location,

680
01:04:55,520 --> 01:05:03,600
the position where I can make the, the best contributions to this. And currently that just

681
01:05:03,600 --> 01:05:13,120
happens to be, to be fair at Meta. I keep a foot in academia because I think it's very

682
01:05:13,120 --> 01:05:17,520
complementary and also important. There are projects of different types that you do in

683
01:05:17,520 --> 01:05:22,240
academia and industry that are complementary. So I like the, the combination of the two.

684
01:05:23,200 --> 01:05:28,160
As long as my brain keeps working, that I think I can contribute and that I've given,

685
01:05:28,160 --> 01:05:33,200
I've given the means to contribute, I'll keep, I'll keep working. And then there's some point

686
01:05:33,200 --> 01:05:39,920
where my brain will turn into white sauce or, or I'm totally, you know, out of it or something

687
01:05:39,920 --> 01:05:45,920
and I'll stop. Yeah. And I want to say personally, thank you so much. I speak for many, I'm sure,

688
01:05:45,920 --> 01:05:50,480
when I say we've learned so much from you in terms of your public speaking and discourse and

689
01:05:50,560 --> 01:05:54,560
willing to speak. I think few are willing to speak as openly as you have been. So

690
01:05:54,560 --> 01:05:58,560
thank you for educating so many of us. And thank you so much for joining me today, Ann.

691
01:05:59,200 --> 01:06:02,160
Well, thank you so much for having me, Harry. This was, this was fun.

