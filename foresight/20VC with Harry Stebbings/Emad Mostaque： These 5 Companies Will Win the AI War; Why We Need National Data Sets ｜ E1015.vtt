WEBVTT

00:00.000 --> 00:01.600
I think this is bigger than the printing press.

00:01.600 --> 00:02.600
It's bigger than anything.

00:02.640 --> 00:04.560
And so that's one of the reasons I signed the letter.

00:04.560 --> 00:07.880
I said, we have to get this discussion going in public right now.

00:08.360 --> 00:12.440
We've got to stop pre-training big models on all the crazy crap of the internet.

00:13.280 --> 00:17.040
And like, we've got to do it fast because this is coming like a train.

00:18.080 --> 00:19.840
And that I am so excited for this.

00:19.840 --> 00:23.720
I heard so many great things from specifically Ashton who helped with questions

00:23.720 --> 00:25.120
and then also Dan Rose.

00:25.120 --> 00:26.760
So thank you so much for joining me today.

00:26.760 --> 00:27.640
It's my pleasure.

00:27.640 --> 00:29.840
Now, I want to start with a little bit on you.

00:29.840 --> 00:31.680
You moved around a little bit in your childhood.

00:32.000 --> 00:34.240
Take me back to the childhood, the moving around.

00:34.240 --> 00:37.400
And it's a weird commonality I found with the most talented founders.

00:37.680 --> 00:38.840
They all moved around.

00:38.840 --> 00:42.000
So take me to that and how it impacted your mindset.

00:42.280 --> 00:45.440
So I was born in Jordan, grew up in Bangladesh, came to the UK.

00:46.080 --> 00:47.480
Yeah, didn't move around that much.

00:47.480 --> 00:50.360
But with my father a bit as he lectured in various places.

00:50.920 --> 00:55.920
And it was always a bit of a struggle fitting in, but then you learn to adapt.

00:56.280 --> 00:58.520
You learn to adapt to new scenarios, new environments.

00:58.520 --> 01:01.200
Like, oh, don't speak the language, kind of what's happening.

01:01.200 --> 01:03.080
Let's learn and let's move on from there.

01:03.160 --> 01:05.520
I think it gave me a bit of appreciation of the world as well,

01:05.880 --> 01:08.120
because we stick in our monocultures very often.

01:08.120 --> 01:09.840
Like I'd only ever been to Silicon Valley.

01:09.840 --> 01:12.280
I should have been the barrier once before last October.

01:12.920 --> 01:16.280
And so this whole tech monoculture has been a bit of a shock to me.

01:16.880 --> 01:19.920
And I'm like, there's more to the world than that.

01:20.000 --> 01:22.240
So this is some interesting things around that.

01:22.440 --> 01:27.160
Talk to me hedge funds first and then then what happened?

01:27.320 --> 01:28.480
We mentioned it a little bit before.

01:28.480 --> 01:29.320
Why did you make the move?

01:29.760 --> 01:33.000
So actually, I was an enterprise developer in my gap here at Metaswitch

01:33.000 --> 01:34.680
in the UK doing voice over IP.

01:34.680 --> 01:37.040
Metaswitch. This is Chris Maz's company.

01:37.040 --> 01:39.560
Yes. So I took my gap here and was like,

01:39.560 --> 01:41.000
I might as well be enterprise programmer.

01:41.000 --> 01:42.200
I didn't know what that would be like.

01:42.200 --> 01:43.880
This was before GitHub and everything.

01:43.880 --> 01:47.040
So we had subversion, you know, kids these days have it so easy.

01:48.320 --> 01:49.760
And then I was like, what do I do now?

01:49.760 --> 01:54.400
And so I became a VC analyst at Oxford Capital Partners with the MOTS there.

01:54.760 --> 01:55.560
And that was a lot of fun.

01:55.560 --> 01:56.280
They were fantastic.

01:56.280 --> 01:57.560
And then I was like, I want to do movies.

01:57.560 --> 01:59.320
So I became a movie reviewer.

01:59.320 --> 02:02.000
So I did the Rain Dance Film Festival, British Independent Film Awards.

02:02.400 --> 02:04.120
And they were just popping around doing random things

02:04.120 --> 02:07.120
and then accidentally became a hedge fund manager.

02:07.120 --> 02:09.760
Why did you move from hedge funds to startups?

02:10.200 --> 02:13.040
So with the hedge funds, so I joined Pictay Asset Management

02:13.040 --> 02:15.600
and then like the CIO left and there was like this fund

02:15.600 --> 02:17.960
and I got to be a portfolio manager when I was 23.

02:18.400 --> 02:21.560
And so I grew my beard to look a bit older and it's coming.

02:22.000 --> 02:22.800
Get the clip on, Harry.

02:22.800 --> 02:25.360
I would. I can't do the moustache, but I still wear the glasses

02:25.400 --> 02:27.680
when I just need to try extra hard to force it out.

02:27.680 --> 02:30.520
Right. And so I did that for a number of years

02:30.520 --> 02:32.960
and you know, reasonably successful, made lots of people money.

02:33.560 --> 02:35.640
Not so much myself because I was too young.

02:35.640 --> 02:37.840
Again, they're like, you're too young to be a fun one.

02:37.840 --> 02:39.840
And then my son was diagnosed with autism and I quit.

02:40.040 --> 02:43.520
And because they said there was no cure, no treatment, no information.

02:43.520 --> 02:44.840
I was like, I'm a hedge fund manager.

02:44.840 --> 02:46.640
I can deconstruct things.

02:46.640 --> 02:50.000
And so built an AI team and then did a literature analysis

02:50.000 --> 02:53.200
of all the autism literature to try and figure out the commonalities

02:53.200 --> 02:54.600
and then drug repurposing.

02:54.640 --> 02:57.040
So focusing on GABA, glutamate balance in the brain.

02:57.360 --> 02:59.640
GABA is what you get when you pop a valium.

02:59.640 --> 03:02.160
It calms you down and glutamate excites you, you know.

03:02.160 --> 03:06.440
And so in kids and people with ASD, it's like there's too much noise going on.

03:06.440 --> 03:08.360
It's like when you're tapping your leg and you can't focus.

03:08.760 --> 03:10.960
And so that's why you get this sensitivity.

03:10.960 --> 03:13.080
Sometimes they can't speak like my son.

03:13.080 --> 03:15.200
And so it was like mechanisms to bring that down

03:15.840 --> 03:17.920
that then allowed to have applied behavioral analysis

03:17.920 --> 03:19.720
and these other things to reconstruct his speech.

03:19.720 --> 03:21.920
And then he went to mainstream school, which is pretty cool.

03:21.920 --> 03:22.880
It's unbelievable.

03:22.880 --> 03:26.440
I heard you said on another podcast and I was astounded and inspired by it.

03:26.720 --> 03:28.920
We mentioned before, you know, my mother's got MS.

03:29.320 --> 03:34.320
And I hate the doomsday only version of kind of AI in the future of GBT.

03:34.960 --> 03:38.000
You said to me before about its impact on health and MS

03:38.000 --> 03:39.880
in particular and other conditions.

03:40.400 --> 03:43.560
How can it be so transformatively to solve some of the world's

03:43.560 --> 03:45.520
most challenging chronic conditions?

03:45.520 --> 03:47.880
So I think a large part of our problem is that we can't scale

03:47.880 --> 03:51.120
because information flow is so limited as we write these things down.

03:51.120 --> 03:52.720
Like you can never capture all of that.

03:53.000 --> 03:57.160
So anyone who's had a loved one that has one of these conditions knows

03:57.160 --> 03:59.400
how difficult it is because you go from specialist to specialist

03:59.400 --> 04:01.880
to specialist and you try to build that mental map.

04:01.880 --> 04:03.920
And we're so lucky that we have so much access.

04:04.400 --> 04:07.120
But why isn't it that we can't just push about and see every clinical trial

04:07.120 --> 04:08.960
and a deconstruction of all those and things?

04:09.320 --> 04:12.640
What if you had a thousand GPT-4s organizing all that knowledge

04:13.160 --> 04:15.400
and then make it available to everyone?

04:15.400 --> 04:19.680
So you can see the exact potential mechanisms that are which MS works

04:19.920 --> 04:24.080
and all the potential food, other things that work with that.

04:24.080 --> 04:27.400
So as you try different things with your family member,

04:27.800 --> 04:30.840
you can see, well, she reacted this way to the food or this way to this medicine.

04:31.120 --> 04:34.040
And it is a more holistic thing because you can have personalized medicine

04:34.040 --> 04:36.480
versus one specialist for a thousand people.

04:36.960 --> 04:41.120
You can have a thousand GPT-4s or equivalents or med palm twos for you.

04:41.720 --> 04:45.040
So we need to organize all this knowledge and then use these language

04:45.040 --> 04:48.040
models and others to make it accessible to you.

04:48.480 --> 04:52.000
I'm really naive and basic in terms of my thinking,

04:52.000 --> 04:54.720
which is why I'm a venture capitalist.

04:54.720 --> 04:57.640
But my question is, what do we need to do to get to that state?

04:57.840 --> 05:00.200
When we look at the data needed from the individuals,

05:00.200 --> 05:05.120
the data, the data, the GPT's need, how we make the models work most efficiently?

05:05.120 --> 05:07.920
So first, we don't have to have that data for individuals.

05:07.920 --> 05:10.320
We had Galactica as a scientific language model,

05:10.320 --> 05:12.960
but now we have Med Palm 2 that exceeds doctor level.

05:12.960 --> 05:15.120
So that was a Google announcement yesterday.

05:15.160 --> 05:19.760
We have AIs that can understand articles better,

05:19.760 --> 05:21.640
as good as doctors, shall we say now?

05:21.640 --> 05:24.240
So we can scale that because why do you need one when you have a thousand?

05:24.240 --> 05:28.280
So we take the existing generalized knowledge and all the hypotheticals

05:28.280 --> 05:31.920
and we bring that together into an integrated common system available to everyone

05:31.920 --> 05:34.280
because the building blocks are nearly here for that.

05:34.280 --> 05:35.840
Then you can personalize it later.

05:35.840 --> 05:39.560
And again, there are regulations and things around that to how you're,

05:39.560 --> 05:42.640
again, how we treat our loved ones and other things like that.

05:42.640 --> 05:44.520
The first thing is let's get all the knowledge in one place

05:44.520 --> 05:47.120
and make it organized and useful.

05:47.120 --> 05:50.480
And so I think we're at that point now where the language models have just hit that point

05:50.480 --> 05:53.200
that we can organize all of the world's Alzheimer's knowledge,

05:53.200 --> 05:56.360
longevity knowledge, autism knowledge, MS knowledge.

05:56.360 --> 05:58.800
And you can just type and it can say, this is the source.

05:58.800 --> 05:59.680
This is what it looks like.

05:59.680 --> 06:01.160
These are some hypotheticals.

06:01.160 --> 06:03.360
This is what we know, what we know we don't know,

06:03.360 --> 06:05.560
what we think we might know, et cetera.

06:05.560 --> 06:08.760
And then it can learn about you and your queries

06:08.760 --> 06:12.360
because this is the other thing about lots of the language model things we've seen right now.

06:12.400 --> 06:14.800
They are one-to-one goldfish memory.

06:14.800 --> 06:16.480
The next step is one-to-one.

06:16.480 --> 06:19.680
It remembers what you're asking for, like a cookie or an embedding.

06:19.680 --> 06:22.280
And then it's you plus a thousand of these language models

06:22.280 --> 06:25.920
all going and doing your bidding, the agent-based kind of thing.

06:25.920 --> 06:28.280
Does this get around the incentive problem in healthcare?

06:28.280 --> 06:30.680
And what I mean by the incentive problem in healthcare is I'm sure you know there are

06:30.680 --> 06:33.800
a lot of diseases actually where it doesn't make kind of economic sense

06:33.800 --> 06:37.920
for a lot of pharmaceutical providers to chase research, to chase treatments

06:37.920 --> 06:40.480
because it's not a big enough market, because it's not,

06:40.520 --> 06:42.080
because it's six dollar treatment.

06:42.080 --> 06:44.760
Does this solve for that economic misalignment?

06:44.760 --> 06:46.960
I think it can help a lot with that economic misalignment

06:46.960 --> 06:50.280
because then you have an authoritative source where we can all come together

06:50.280 --> 06:53.080
and build that can analyze these things.

06:53.080 --> 06:55.600
Because there's this concept of agiddicity.

06:55.600 --> 06:59.600
A thousand coins tossed in a row is the same as a thousand coins tossed at once.

06:59.600 --> 07:03.120
And because we're so limited in our information in our medical system,

07:03.120 --> 07:05.240
like you know I just had my key management,

07:05.240 --> 07:07.360
so I had to answer 40 minutes of questions.

07:07.360 --> 07:08.320
At least, Mo, have you done this?

07:08.320 --> 07:10.280
It's stupid, right?

07:10.280 --> 07:12.000
We're all treated the same.

07:12.000 --> 07:17.200
I think 10% of people have a cytochrome P450 mutation in their liver,

07:17.200 --> 07:19.440
which means they metabolize drugs fast quicker.

07:19.440 --> 07:23.960
So if you metabolize code, it turns into morphine, or fentanyl kills you.

07:23.960 --> 07:25.440
But that's a very basic genetic test,

07:25.440 --> 07:28.200
yet we give everyone 500 milligrams of the same thing.

07:28.200 --> 07:31.840
With my son, a micro dose of 5 milligrams of clonazepam,

07:31.840 --> 07:33.600
which is used for anxiety disorder,

07:33.600 --> 07:35.640
worked with a neurologist, allows him to sing.

07:35.640 --> 07:37.160
The standard dose is a thousand milligrams,

07:37.160 --> 07:39.000
so they can only prescribe it at a thousand.

07:39.040 --> 07:43.360
But that is a $6 a year treatment that affects his GABA glutamate balance.

07:43.360 --> 07:45.720
But only for his specific type of ASD,

07:45.720 --> 07:48.640
which is only 7% of all kids with ASD.

07:48.640 --> 07:53.160
But why would that be in a pharmaceutical company's interest?

07:53.160 --> 07:57.960
You know, because how are they going to make money off a $6 a year treatment?

07:57.960 --> 08:00.080
Well, how many people have ASD?

08:00.080 --> 08:02.000
It's 1 in 60.

08:02.000 --> 08:04.280
Okay, it's 1 in 60, so you've got a million people in the UK?

08:04.280 --> 08:04.920
Yes.

08:04.920 --> 08:07.840
So you've got a $6 million.

08:07.880 --> 08:08.760
Yeah, it's not great.

08:08.760 --> 08:10.040
Exactly, it's not great.

08:10.040 --> 08:12.760
I mean, it's like we know the benefits of vitamin D, right?

08:12.760 --> 08:14.560
But we still don't prescribe that at scale,

08:14.560 --> 08:16.000
and so many people are deficient.

08:16.000 --> 08:17.160
I mean, all these things.

08:17.160 --> 08:19.440
The joys of doing what I do is going on schedule with a final one,

08:19.440 --> 08:22.200
and then we will kind of retain some form of normality of schedule.

08:22.200 --> 08:24.800
What is the future of healthcare systems that you think

08:24.800 --> 08:26.720
with GPT models operating in this way?

08:26.720 --> 08:28.960
I think that you can change the nature of a doctor,

08:28.960 --> 08:31.000
because a lot of the stuff is kind of very basic.

08:31.000 --> 08:32.720
I think, you know, you had Babylon Health and others trying,

08:32.720 --> 08:35.080
that chatbot, it wasn't ready, now you've got this.

08:35.080 --> 08:37.440
Everyone should have their own AIs looking out for their own health,

08:37.440 --> 08:39.080
with that objective function.

08:39.080 --> 08:41.000
You know, and then the nature of a doctor becomes different

08:41.000 --> 08:44.200
in terms of they have more rich information about an individual,

08:44.200 --> 08:46.920
while it being preserved in a private manner.

08:46.920 --> 08:48.800
I think what you have is you have things like

08:48.800 --> 08:50.440
processes and procedures improving,

08:50.440 --> 08:53.920
like wound care, for example, and then NHS.

08:53.920 --> 08:56.280
If you are injured as an elderly person,

08:56.280 --> 08:57.680
and your wounds aren't treated properly,

08:57.680 --> 09:00.480
and more likely to die by a factor of eight times,

09:00.480 --> 09:02.480
being able to monitor those types of things

09:02.480 --> 09:05.040
with this information set means you're eight times as likely,

09:05.040 --> 09:07.320
and then you have far more efficiency around that.

09:07.320 --> 09:09.600
So the information density around healthcare improves,

09:09.600 --> 09:12.760
which means that then our own healthcare improves.

09:12.760 --> 09:15.160
We all have access to as much knowledge as we want to,

09:15.160 --> 09:17.560
within our own context, and so do our providers

09:17.560 --> 09:18.560
and the people that help us.

09:18.560 --> 09:20.960
How do we think about open source versus closed source

09:20.960 --> 09:22.320
human healthcare data?

09:22.320 --> 09:24.640
Because like, obviously for us all to benefit as one,

09:24.640 --> 09:26.680
you know, MS sufferers around the world need to submit

09:26.680 --> 09:29.680
their data around responses to certain treatments.

09:29.680 --> 09:31.840
Yeah, so I think the wonderful thing about these models

09:31.840 --> 09:32.800
is they're few-shot learners,

09:32.800 --> 09:34.800
so they don't need to have much information.

09:34.800 --> 09:36.320
As I was in the classical big data problem,

09:36.320 --> 09:38.440
HDR UK has been one of the pioneers here

09:38.440 --> 09:41.160
with the UK Biobank, Federated Learning, and others.

09:41.160 --> 09:45.520
And there are kind of, with FL7, HLIR, and other standards

09:45.520 --> 09:46.720
being built around this to allow

09:46.720 --> 09:48.440
for full federated learning.

09:48.440 --> 09:51.880
If you have open source language models

09:51.880 --> 09:54.040
that are fully auditable, I call them organic

09:54.040 --> 09:55.480
free range models, the ones we're building,

09:55.480 --> 09:58.560
with no web script data, those can sit on device,

09:58.560 --> 10:01.400
like Google SDA announced POM2.

10:01.400 --> 10:03.720
The smallest POM2 model is 400 million parameters.

10:03.720 --> 10:05.520
It works on your Google Pixel phone.

10:05.520 --> 10:07.360
You don't need giant models anymore.

10:07.360 --> 10:10.120
And then that model can just share the specific information

10:10.120 --> 10:13.400
that preserves your privacy with the bigger thing.

10:13.400 --> 10:15.600
And then it can take from that global knowledge base as well.

10:15.600 --> 10:19.000
So you'll have big global models on device models.

10:19.000 --> 10:20.520
And I think open works for that

10:20.520 --> 10:22.960
because you don't need to have all the data open.

10:22.960 --> 10:24.680
You just need to know that Harry is old enough

10:24.680 --> 10:26.200
to have a drink, not that.

10:26.200 --> 10:27.720
All the details about Harry, his birthplace,

10:27.720 --> 10:28.640
and everything like that.

10:28.640 --> 10:30.520
He's old enough, he's just not allowed to.

10:30.520 --> 10:33.120
He gets parted all the time, yeah.

10:34.000 --> 10:35.760
Were you impressed by the Google event yesterday?

10:35.760 --> 10:36.880
No, I think it was impressive.

10:36.880 --> 10:39.560
I said in February, when all this thing was going on,

10:39.560 --> 10:42.960
like, come on, Google will be one of the main winners here.

10:42.960 --> 10:45.200
They have the LLMs, they have the hardware.

10:45.200 --> 10:47.760
You do know you're the only person who said that on the show.

10:47.760 --> 10:48.960
And I've asked many.

10:48.960 --> 10:51.640
And they've all said that Google are the laggards.

10:51.640 --> 10:53.480
It just takes a bit of time to move the ship, right?

10:53.480 --> 10:56.000
And so they've done massive organizational changes

10:56.000 --> 10:57.120
and other things.

10:57.120 --> 11:00.320
But I can tell you, TPU is on the most scalable architecture.

11:00.520 --> 11:04.080
We have zero failure rate with our TPU language model training.

11:04.080 --> 11:06.480
Whereas with GPUs, it's like there's

11:06.480 --> 11:09.400
an ECC error, why a solar flare?

11:09.400 --> 11:12.280
Run failed because the sun is angry with us and stuff like that.

11:12.280 --> 11:13.720
So when you've got the full stack,

11:13.720 --> 11:15.240
and you have all that talent in Google,

11:15.240 --> 11:17.720
the question is, how do you make it organized, right?

11:17.720 --> 11:19.000
And so they had to have a story.

11:19.000 --> 11:20.600
Google did something called Pro-Taristottle,

11:20.600 --> 11:22.160
where they analyzed what made the best teams

11:22.160 --> 11:23.720
versus the worst teams at Google.

11:23.720 --> 11:25.080
And it came down to shared narrative

11:25.080 --> 11:26.800
and psychological safety.

11:26.800 --> 11:29.480
People at Google were scared over the last few years,

11:29.520 --> 11:31.080
because it came this weird monoculture.

11:31.080 --> 11:33.040
But now everyone has a shared narrative of,

11:33.040 --> 11:34.920
let's build the best language models,

11:34.920 --> 11:36.800
and now there's an increase in amount of psychological safety

11:36.800 --> 11:37.960
being able to speak to things,

11:37.960 --> 11:40.600
the walls being brought down between deep mind and brain.

11:40.600 --> 11:43.360
And so I think you'll see them continuously improving.

11:43.360 --> 11:44.480
Well, then that does mean,

11:44.480 --> 11:46.080
if you're a proprietary language model company,

11:46.080 --> 11:49.000
how are you going to compete with that vehemence?

11:49.000 --> 11:53.560
The deep mind desegregation or unification

11:53.560 --> 11:56.160
was supposed to, of course, have a lot of friction

11:56.160 --> 11:58.920
and be a negative press reported.

11:58.920 --> 11:59.840
She disagreed with that.

11:59.840 --> 12:03.080
Of course, it is a lot of replicated jobs.

12:03.080 --> 12:05.040
There was brain and mind,

12:05.040 --> 12:06.920
and now they're kind of brought together.

12:06.920 --> 12:09.080
And it's a very different management style

12:09.080 --> 12:09.920
and other things.

12:09.920 --> 12:11.480
These things are never easy.

12:11.480 --> 12:14.680
But this is why you saw palm 540 billion parameters

12:14.680 --> 12:16.400
and that you had deep mind

12:16.400 --> 12:18.880
with 67 billion parameter and chiller,

12:18.880 --> 12:21.520
which is just train more as opposed to more parameters.

12:21.520 --> 12:24.280
You look at palm two as a combination of both.

12:24.280 --> 12:27.720
And so it's trained for far more on far better data.

12:27.720 --> 12:30.560
And then that means it's only a fraction of the size,

12:30.560 --> 12:31.840
like 14 billion parameters

12:31.840 --> 12:35.760
is one of the test comparator models versus the 540 and 67.

12:35.760 --> 12:37.920
So you can start to see this fusion of ideas,

12:37.920 --> 12:38.840
even if the teams,

12:38.840 --> 12:41.960
you cannot integrate two big teams like that instantly.

12:41.960 --> 12:44.360
Shared narrative, psychological safety,

12:44.360 --> 12:46.160
two of the biggest contributors.

12:46.160 --> 12:48.400
To now running stability,

12:48.400 --> 12:50.440
how do you think about integrating those two?

12:50.440 --> 12:51.680
So we've got the shared narrative.

12:51.680 --> 12:52.680
We're going to build the foundation

12:52.680 --> 12:54.120
to activate humanity's potential

12:54.120 --> 12:56.200
and then the motors make people happier.

12:56.200 --> 12:57.480
But it's been a learning process,

12:57.480 --> 13:00.480
the year ago, we're basically a mom and pop shop in some ways.

13:00.480 --> 13:02.000
My wife and I were working at it,

13:02.000 --> 13:04.400
like had lots of meetings out of our like sitting room

13:04.400 --> 13:06.160
and things because the office didn't have wifi

13:06.160 --> 13:07.240
and all sorts.

13:07.240 --> 13:10.040
Now it's like growing up, we're 170 people,

13:10.040 --> 13:11.320
we're going global,

13:11.320 --> 13:13.320
we'll have stabilities in every country.

13:13.320 --> 13:15.360
And then actually we're going multinational.

13:15.360 --> 13:16.280
And that's difficult.

13:16.280 --> 13:18.520
So we really try to put in processes in place,

13:18.520 --> 13:20.240
but it's not easy.

13:20.240 --> 13:21.320
Part of this is like,

13:21.320 --> 13:24.560
we went close source on a bunch of stuff like dream studio.

13:24.560 --> 13:26.360
I'm open sourcing everything now.

13:26.360 --> 13:27.200
From next week,

13:27.200 --> 13:29.240
we're going to build our language models in the open

13:29.240 --> 13:30.840
and share what works and what doesn't work.

13:30.840 --> 13:31.880
Why?

13:31.880 --> 13:34.200
Because I think this is part of the shared narrative.

13:34.200 --> 13:35.560
Someone needs to be open

13:35.560 --> 13:37.320
and share what's going on under the hood.

13:37.320 --> 13:40.760
And again, it's like, it should be opened by default

13:40.760 --> 13:44.880
because the value is not in any proprietary models or data.

13:44.880 --> 13:47.520
We're going to build open models that are auditable.

13:47.520 --> 13:49.000
Even if it has licensed data in it,

13:49.000 --> 13:51.680
you can see every single piece, free range organic models.

13:51.680 --> 13:52.720
Because that's what the world needs

13:52.720 --> 13:55.200
for all the private regulated and other data in the world.

13:55.200 --> 13:58.360
This is a completely different time to proprietary models.

13:58.360 --> 14:01.400
Because you can only send so much of your 20 VC data

14:01.400 --> 14:02.760
to open AI.

14:02.760 --> 14:04.600
And I think you need both of those.

14:04.600 --> 14:06.680
So why can I only send so much?

14:06.680 --> 14:08.760
Because you are a regulated company.

14:08.760 --> 14:11.240
And so you need to make sure they're completely compliant.

14:11.240 --> 14:14.400
If you have an option of having a stable chat model,

14:14.400 --> 14:16.360
which will be announced in the future,

14:16.360 --> 14:18.520
that you own completely trained in your own cloud

14:18.520 --> 14:20.080
or on-prem or on-device,

14:20.080 --> 14:21.360
and then also using GPT-4,

14:21.360 --> 14:23.200
that's the best of both worlds.

14:23.200 --> 14:24.480
Because then you don't have to deal with that.

14:24.520 --> 14:27.400
Healthcare data needs to, again, be owned by the individual.

14:27.400 --> 14:29.160
And so those models need to be owned.

14:29.160 --> 14:30.160
And they need to be transparent.

14:30.160 --> 14:31.920
They can't be black boxes.

14:31.920 --> 14:33.920
Governments will not run on black boxes.

14:35.360 --> 14:36.440
We're going to get to this later.

14:36.440 --> 14:37.840
I do want to touch on something.

14:37.840 --> 14:39.520
We had a great chat before this.

14:39.520 --> 14:41.400
And you said a brilliant quote, and I want to get it right,

14:41.400 --> 14:44.200
but you said the .ai bubble is bigger than ever,

14:44.200 --> 14:46.200
and it will be the biggest shit show.

14:46.200 --> 14:47.200
Yeah.

14:47.200 --> 14:48.200
End quote.

14:49.200 --> 14:50.760
Which I actually took and tweeted, by the way.

14:50.760 --> 14:51.600
Thank you.

14:51.600 --> 14:52.920
It could be some gratitude.

14:52.920 --> 14:54.160
I thought if you saw it, I would have been like,

14:54.160 --> 14:55.360
ah, this guy took my tweet.

14:55.360 --> 14:56.360
Thank you.

14:56.360 --> 15:01.080
And what did you mean by the biggest bubble ever

15:01.080 --> 15:02.360
and the biggest shit show?

15:02.360 --> 15:04.560
Oh, I mean, like the .com bubble,

15:04.560 --> 15:05.920
we've seen all these bubbles happen.

15:05.920 --> 15:08.240
You know, you had hundreds of billions into Web 3,

15:08.240 --> 15:10.040
and then developers got paid millions.

15:10.040 --> 15:13.040
Already, there are certain Chinese companies

15:13.040 --> 15:15.600
paying $1.2 million salaries for PhDs.

15:16.760 --> 15:18.160
It's already getting a bit insane.

15:18.160 --> 15:19.760
There are remnants of that.

15:19.800 --> 15:23.280
The amount of money relative to the amount of opportunity

15:23.280 --> 15:25.240
within the sector is just completely misaligned.

15:25.240 --> 15:27.400
Like my time analysis is that

15:27.400 --> 15:29.720
1,000 companies spend $10 million in the next year,

15:29.720 --> 15:32.600
100 companies spend $100, and 10 companies spend $1 billion.

15:32.600 --> 15:34.360
Like PWC just announcing they'll spend $1 billion

15:34.360 --> 15:35.680
over the next three years.

15:35.680 --> 15:38.440
And that's a currency firm, you know?

15:38.440 --> 15:39.280
Where's that going to go?

15:39.280 --> 15:40.120
They don't know.

15:40.120 --> 15:41.080
Nobody knows.

15:41.080 --> 15:43.280
And so multiple of that will be allocated to this

15:43.280 --> 15:47.160
as the only growth theme in the entire market

15:47.160 --> 15:48.880
against a backdrop of rising rates,

15:48.920 --> 15:50.920
real estate crashing, et cetera.

15:50.920 --> 15:53.720
So the amount of capacity versus the amount

15:53.720 --> 15:56.120
and whale and wall of money

15:56.120 --> 15:57.320
into something that's growing faster

15:57.320 --> 16:00.800
than anything we've ever seen is completely mismatched.

16:00.800 --> 16:02.360
And what will that cause?

16:02.360 --> 16:04.280
Like already you're seeing GitHub stars leading

16:04.280 --> 16:06.120
to $100 million funding rounds

16:06.120 --> 16:08.400
with zero attraction and zero business model.

16:08.400 --> 16:10.280
Like stability, we actually have a business model.

16:10.280 --> 16:12.920
And it's a good business model because I designed it.

16:12.920 --> 16:15.200
But other things like money will go everywhere

16:15.200 --> 16:17.800
and any expertise will get bit up for this space

16:17.840 --> 16:19.600
because it means that projects will get funded

16:19.600 --> 16:21.160
that maybe wouldn't have done,

16:21.160 --> 16:23.520
but are exploratory generally and over funding.

16:23.520 --> 16:26.240
I think it starts good for the space,

16:26.240 --> 16:28.640
but then it gets bad for the space

16:28.640 --> 16:30.640
because you see the raccoons and Scheister

16:30.640 --> 16:32.000
start to come in here.

16:32.000 --> 16:34.160
You start to see like malformed things

16:34.160 --> 16:35.240
where there's a race dynamic,

16:35.240 --> 16:37.160
where everyone's trying to build their own models

16:37.160 --> 16:40.400
and doing all sorts and massive economic waste.

16:40.400 --> 16:43.000
And you see a distraction from what we need to do now,

16:43.000 --> 16:43.840
which is this chaos.

16:43.840 --> 16:45.720
So we need to standardize some things.

16:45.720 --> 16:47.880
We need to feed these models better data and other stuff.

16:47.880 --> 16:50.040
And that's why we're moving so hard at stability.

16:50.040 --> 16:51.880
There should be no more web script data in here.

16:51.880 --> 16:54.480
There should be national data sets that are good quality

16:54.480 --> 16:56.040
to feed these free range organic models

16:56.040 --> 16:58.720
and national and proprietary models and others.

16:58.720 --> 17:00.480
And so that's why, and the reasons I signed that letter

17:00.480 --> 17:02.200
because I think there's a six month pause

17:02.200 --> 17:03.640
to get all of our shit together

17:04.560 --> 17:06.920
before things go completely insane.

17:06.920 --> 17:08.240
And next year, this is everywhere

17:08.240 --> 17:10.160
and everyone's investing in everything.

17:10.160 --> 17:13.240
And it's just absolute chaos.

17:13.240 --> 17:15.280
You kind of unpack so much to me

17:15.280 --> 17:16.920
that I want to kind of go one by one.

17:16.920 --> 17:18.800
You said about kind of national data sets.

17:18.800 --> 17:22.360
Why national data sets versus super national data sets?

17:22.360 --> 17:23.440
Because like, I'll give you an example.

17:23.440 --> 17:24.840
There was a team that did Japan diffusion,

17:24.840 --> 17:26.200
including some of our staff.

17:26.200 --> 17:27.480
So we took stable diffusion

17:27.480 --> 17:28.840
and then changed the language model.

17:28.840 --> 17:31.120
Because when you typed in salary man in stable diffusion,

17:31.120 --> 17:32.360
it was a very happy man.

17:32.360 --> 17:34.720
Whereas in Japan, the salary man's a very sad man.

17:34.720 --> 17:38.000
You know, local context is important in these models

17:38.000 --> 17:39.440
because we're going to outsource more and more

17:39.440 --> 17:41.600
of our thinking and minds to it.

17:41.600 --> 17:43.200
And so do you want to have a British model

17:43.200 --> 17:45.400
doing all the models to be Palo Alto?

17:45.400 --> 17:47.280
You know, like it's a sparkling wine

17:47.280 --> 17:48.920
has to be from the Champaign region.

17:48.920 --> 17:52.080
Like is the only real foundation model AI from Palo Alto?

17:52.080 --> 17:53.240
Like it's not a good thing.

17:53.240 --> 17:54.680
We need national models.

17:54.680 --> 17:55.760
It's a national infrastructure

17:55.760 --> 18:00.440
because there is no doubt this is more important than 5G.

18:00.440 --> 18:02.200
These models are like really talented grads

18:02.200 --> 18:03.960
that occasionally go off their meds, you know?

18:03.960 --> 18:06.360
And you want to have the ones from Oxford Imperial

18:06.360 --> 18:09.040
and Edinburgh as well as the ones from Stanford

18:09.040 --> 18:10.800
because they understand the local context.

18:10.800 --> 18:11.800
And so they understand you better

18:11.800 --> 18:13.520
and they'll be better for that.

18:13.520 --> 18:15.800
As part of that, every nation will need their own data sets

18:15.800 --> 18:17.720
which again have from broadcaster data.

18:17.720 --> 18:19.000
They will need their own open models

18:19.000 --> 18:21.240
that can stimulate innovation internally as well.

18:21.240 --> 18:22.680
Who owns national data sets?

18:22.680 --> 18:23.520
Is that governments?

18:23.520 --> 18:24.480
I think it should be the people.

18:24.480 --> 18:26.680
I think it should be open and public domain.

18:28.000 --> 18:29.760
How does that come into fruition?

18:29.760 --> 18:33.240
Well, we have a world where we have national verified data sets

18:33.240 --> 18:36.240
which can be leveraged by independent private companies.

18:36.240 --> 18:38.360
And others and universities and others.

18:38.360 --> 18:39.400
Well, this is what we're doing right now.

18:39.400 --> 18:41.120
We're working with our multinational partners,

18:41.120 --> 18:42.360
lots more to be announced soon,

18:42.360 --> 18:44.200
and multiple governments for a framework

18:44.200 --> 18:47.080
for what good data looks like to feed these models

18:47.080 --> 18:49.840
to stimulate innovation and localization.

18:49.840 --> 18:51.120
And that is a public good

18:51.120 --> 18:55.200
because national broadcasters have all of this data.

18:55.200 --> 18:57.640
You just tokenize all their kind of things.

18:57.640 --> 18:59.400
And then you have things like the implementation

18:59.400 --> 19:01.240
of these for education and healthcare.

19:01.240 --> 19:02.560
You can take generalized learnings

19:02.560 --> 19:04.680
and then again feed the models that thing.

19:04.680 --> 19:08.000
What is a great data set for a great British E.P.T. look like?

19:08.000 --> 19:10.840
I think it's open, it's interrogated and it's optimized.

19:10.880 --> 19:12.120
When you look at all the different things

19:12.120 --> 19:13.480
that we've talked about from you,

19:13.480 --> 19:16.680
relative treatment of MS to ASD,

19:17.640 --> 19:20.200
and then it's impact on education.

19:20.200 --> 19:23.160
And we just to PWC spending money on it,

19:23.160 --> 19:25.560
there are so many problems that can be solved.

19:25.560 --> 19:29.240
Surely we can find a home for the cash.

19:29.240 --> 19:31.400
Yeah, and so I'm not sure where.

19:31.400 --> 19:32.680
And so this is the thing,

19:32.680 --> 19:34.760
like there's gonna be this mismatch.

19:34.760 --> 19:37.080
If you were an invested state,

19:37.080 --> 19:38.840
if you were me, what would you do?

19:38.840 --> 19:41.320
I'm an early stage investor, I'm less globally.

19:41.320 --> 19:43.080
What would you do?

19:43.080 --> 19:46.200
I would, again, I think it comes down to,

19:46.200 --> 19:48.640
there's gonna be this tailwind of beta.

19:48.640 --> 19:51.160
And then you have an alpha play on top of that, right?

19:51.160 --> 19:53.080
So the beta play is that you just invest

19:53.080 --> 19:54.360
in any good founder.

19:54.360 --> 19:55.640
And if you get in, you figure out,

19:55.640 --> 19:57.600
what can I offer as kind of a value out there?

19:57.600 --> 19:58.640
Am I offering distribution?

19:58.640 --> 19:59.480
Am I offering people?

19:59.480 --> 20:00.560
Am I offering this?

20:00.560 --> 20:02.480
And you emphasize kind of your value set.

20:02.480 --> 20:05.520
I think right now what people need is people.

20:05.520 --> 20:07.400
There are a few people that are like coming out here,

20:07.440 --> 20:12.080
but then what you see is you see good companies,

20:12.080 --> 20:14.560
with good ideas, but not businesses.

20:14.560 --> 20:15.960
They're building surface level things,

20:15.960 --> 20:17.320
these wrapper layers and others,

20:17.320 --> 20:19.600
and they're not thinking about distribution and data.

20:19.600 --> 20:20.920
It's like, if you want to have distribution,

20:20.920 --> 20:21.760
what do we do?

20:21.760 --> 20:23.280
We went to Amazon and said bedrock,

20:23.280 --> 20:26.920
because then it gives us 100,000 SageMaker SMEs.

20:26.920 --> 20:28.240
And we'd have to give them the models

20:28.240 --> 20:29.760
that they can then take to the private data.

20:29.760 --> 20:31.720
And we get a share of all of that.

20:31.720 --> 20:32.600
This is how we saw it,

20:32.600 --> 20:34.920
like rather than being responsible that.

20:34.920 --> 20:36.720
So if you can bring that distribution to that,

20:37.040 --> 20:39.440
this is part of that Google memo that went out.

20:39.440 --> 20:40.760
We don't have an edge and others open AI.

20:40.760 --> 20:44.120
Open AI used Microsoft for distribution and that flywheel.

20:45.480 --> 20:46.960
If you have a business that's focused

20:46.960 --> 20:49.520
on innovation at the core, that's not actually a business.

20:49.520 --> 20:51.240
It becomes a business when that innovation

20:51.240 --> 20:53.480
becomes product, becomes distribution,

20:53.480 --> 20:55.400
when it has an advantage on data and other things.

20:55.400 --> 20:56.920
Those are real modes.

20:56.920 --> 20:58.440
How did you arise that partnership

20:58.440 --> 21:00.520
between open AI and Microsoft?

21:00.520 --> 21:03.160
I saw it as the objective function of open AI

21:03.160 --> 21:05.680
is to build AGI and they reckon they need $10 billion

21:05.720 --> 21:07.360
to do it and they did that.

21:07.360 --> 21:09.360
Like they're building a business on products and things,

21:09.360 --> 21:10.920
but they don't care.

21:10.920 --> 21:12.640
You know, they're not trying to build a sustainable business.

21:12.640 --> 21:14.040
They're trying to build an AGI.

21:15.120 --> 21:15.960
Why?

21:15.960 --> 21:17.400
Like what would I, just help me understand

21:17.400 --> 21:19.360
AGI to build a sustainable business?

21:19.360 --> 21:20.400
Cause at the end of the day.

21:20.400 --> 21:21.600
No, they're building an AGI

21:21.600 --> 21:23.160
to turn the world into utopia.

21:23.160 --> 21:25.320
It's written in their path to AGI thing

21:25.320 --> 21:28.480
that they think this can basically bring about utopia.

21:28.480 --> 21:30.560
So a lot of people in these labs,

21:30.560 --> 21:32.200
we only have people joining from all of these labs,

21:32.200 --> 21:35.360
like they almost zealous in there.

21:35.360 --> 21:37.080
But there is a misalignment there between them

21:37.080 --> 21:39.720
and Microsoft in their desire to create that utopian AI.

21:39.720 --> 21:41.840
Yes, cause Microsoft is a business, you know?

21:41.840 --> 21:44.080
And so this is why you've seen like articles

21:44.080 --> 21:46.280
in the information, like Microsoft say open AI

21:46.280 --> 21:48.960
aren't compliant and open AI say Microsoft on this.

21:48.960 --> 21:50.920
These things happen when there is a misalignment

21:50.920 --> 21:52.760
of objective functions.

21:52.760 --> 21:55.320
But again, you should view open AI as what they want to do

21:55.320 --> 21:58.680
is build an AI that can basically make the world better

21:58.680 --> 22:01.400
and hopefully not kill us all, which they say it might.

22:01.400 --> 22:02.400
Which is a bit concerning,

22:02.400 --> 22:04.600
which is why I hope they have better open governance.

22:04.640 --> 22:06.200
How did you think about distribution?

22:06.200 --> 22:08.560
You know, you've seen the hugging face part of it.

22:08.560 --> 22:11.560
Amazon, you've seen obviously open AI with Microsoft.

22:11.560 --> 22:12.760
When you think about distribution

22:12.760 --> 22:15.600
and your competitive edge there, where did you land?

22:15.600 --> 22:17.800
So my business model is actually very simple.

22:17.800 --> 22:19.520
I haven't really talked about it much.

22:19.520 --> 22:22.280
Stimulate open one of the biggest providers of grants

22:22.280 --> 22:24.840
to open source software, tens of millions already.

22:24.840 --> 22:26.520
And then take the best of open,

22:26.520 --> 22:28.120
which hopefully we build ourselves.

22:28.120 --> 22:31.520
And then an open base with an open data.

22:31.520 --> 22:33.920
And then commercial variants with license data

22:33.920 --> 22:35.280
and then national variants.

22:35.280 --> 22:38.960
So you have Hindi insurance adjusted stable chat

22:38.960 --> 22:42.360
or Indonesian pharmaceutical worker stable chat

22:42.360 --> 22:45.040
that's available in every cloud, on-prem, on-device

22:45.040 --> 22:49.760
with licensing fees, royalties and revenue share.

22:49.760 --> 22:51.520
And the system integrators work with us as well.

22:51.520 --> 22:53.160
Lots of announcement to come.

22:53.160 --> 22:56.560
And so by standardizing and stabilizing all the complexity

22:56.560 --> 23:00.200
to these very sophisticated building blocks,

23:00.200 --> 23:02.800
these very intentionally built models,

23:02.800 --> 23:04.480
that really helps the world integrate this stuff

23:04.480 --> 23:06.400
by building playbooks and other things.

23:06.400 --> 23:07.320
And that's the core business

23:07.320 --> 23:09.160
because it doesn't require actual innovation.

23:09.160 --> 23:10.000
We are still innovative

23:10.000 --> 23:11.800
and the leaders in media in particular.

23:11.800 --> 23:13.800
Instead it requires data and distribution,

23:13.800 --> 23:15.160
data to the models.

23:15.160 --> 23:16.520
The models are open and interpretable

23:16.520 --> 23:18.760
and models to the data via our partners.

23:18.760 --> 23:20.760
And that's valuable because the private data in the world

23:20.760 --> 23:22.720
is far more valuable than the data

23:22.720 --> 23:25.200
that you will send to proprietary models.

23:25.200 --> 23:27.360
And it's not a race to the bottom either.

23:27.360 --> 23:28.480
So that's what we are.

23:28.480 --> 23:30.440
We're a modeling agency with hot GPUs.

23:31.440 --> 23:33.600
Building a distribution around the world,

23:33.600 --> 23:35.960
realizing that India and other nations

23:35.960 --> 23:38.280
will leapfrog to intelligence augmentation,

23:38.280 --> 23:39.560
just so they leapfrog to mobile.

23:39.560 --> 23:41.160
They will embrace this technology far quicker

23:41.160 --> 23:42.280
than we will in the UK even.

23:42.280 --> 23:43.240
Why?

23:43.240 --> 23:45.080
Because they have to.

23:45.080 --> 23:49.640
India, all of the outsourcing jobs in programming will go

23:49.640 --> 23:52.680
because GPT-4 can go level three

23:52.680 --> 23:54.920
Google programmer exam and pass it.

23:55.920 --> 23:58.920
Outsource jobs will go the first, whereas in France,

23:58.920 --> 24:00.320
you're never gonna fire a French person.

24:00.360 --> 24:02.200
So those jobs are fake, you know?

24:02.200 --> 24:04.120
And so they have an objective function

24:04.120 --> 24:05.560
when they need to embrace this technology.

24:05.560 --> 24:07.920
In Africa, one to one tuition,

24:07.920 --> 24:10.040
every kid in Malawi is on things you lined up.

24:10.040 --> 24:11.320
We've got other nations.

24:11.320 --> 24:14.160
We're gonna bring them all this technology and tablets.

24:14.160 --> 24:15.440
And guess what?

24:15.440 --> 24:17.320
Their lives will transform.

24:17.320 --> 24:18.920
One AI per child is one I wanna call it.

24:18.920 --> 24:20.400
We'll call it something else.

24:20.400 --> 24:22.000
But think about the potential of that

24:22.000 --> 24:23.880
because you have one's teacher per 300 kids.

24:23.880 --> 24:25.840
What if they had a chat GPT-level AI?

24:27.160 --> 24:28.960
The ROI is high and the need is high.

24:28.960 --> 24:31.320
And so they will embrace it far quicker than we will.

24:31.320 --> 24:34.000
What happens to countries that rely on outsourced work

24:34.000 --> 24:36.360
in those kind of freelancer economy jobs?

24:36.360 --> 24:38.480
In general, one of the things, the questions is,

24:38.480 --> 24:40.160
you've seen OpenAI study, you've seen the,

24:40.160 --> 24:43.040
which said task will be replaced up to 44%.

24:43.040 --> 24:46.360
You've seen Goldman Sachs say adds percentage points to GDP.

24:46.360 --> 24:49.320
I think the only solution to this is entrepreneurship.

24:49.320 --> 24:51.560
And so we need to give the tools to create new jobs

24:51.560 --> 24:54.760
that can replace some of these old jobs being done.

24:54.760 --> 24:57.040
So like to the various Asian governments,

24:57.040 --> 25:01.480
I'm saying adopt the UK policy of these sandboxes,

25:01.480 --> 25:04.480
financial, AI and other regulatory sandboxes.

25:04.480 --> 25:05.880
So you can take these technologies,

25:05.880 --> 25:08.360
these national models that we will help you build

25:08.360 --> 25:10.280
with our consortium partners

25:10.280 --> 25:12.560
and then spur innovation to create the jobs

25:12.560 --> 25:13.840
to replace the existing jobs

25:13.840 --> 25:15.960
because you'll upgrade your entire society.

25:15.960 --> 25:17.520
Bring these models into your governments

25:17.520 --> 25:19.480
and other things to go from slow dumb AI's,

25:19.480 --> 25:22.040
which is the national organization's healthcare,

25:22.040 --> 25:24.240
to intelligent dynamic ones.

25:24.240 --> 25:25.920
Can I ask on implementation,

25:25.960 --> 25:27.720
when we think about kind of bluntly

25:27.720 --> 25:29.800
seeing this in action in society,

25:29.800 --> 25:32.080
I'm sure it's very aware of technology cycles

25:32.080 --> 25:34.880
taking so much longer than one anticipates.

25:34.880 --> 25:37.440
How do you think about that in actual,

25:37.440 --> 25:38.400
there's kind of two-fold.

25:38.400 --> 25:39.640
One is adoption on enterprise

25:39.640 --> 25:41.360
and another is adoption on consumer.

25:41.360 --> 25:43.240
Say if we do the adoption on the consumer side,

25:43.240 --> 25:46.880
which is impacting freelancer jobs and impacting education.

25:46.880 --> 25:48.480
What do you think that looks like?

25:48.480 --> 25:50.800
So I think on the consumer side,

25:50.800 --> 25:51.920
you're free with your information.

25:51.920 --> 25:53.240
So you can use a lot of these things,

25:53.240 --> 25:55.240
the APIs of OpenAI and Cohere

25:55.240 --> 25:56.280
and others are fantastic, right?

25:56.280 --> 25:58.800
And Google Palm now kind of being out there.

25:58.800 --> 26:00.080
So it will be integrated

26:00.080 --> 26:02.000
to deliver better consumer experiences

26:02.000 --> 26:03.000
without it being creepy,

26:03.000 --> 26:05.280
like you've seen with some of the chat bots, et cetera.

26:05.280 --> 26:06.400
Cause it's going into word,

26:06.400 --> 26:07.880
it's going into workspace, you know,

26:07.880 --> 26:09.920
like it helps already.

26:09.920 --> 26:11.120
Like we will have a conversation

26:11.120 --> 26:12.880
will be automatically logged by our pixel phone

26:12.880 --> 26:13.880
and then we'll get a transcript

26:13.880 --> 26:15.920
and remove bits that we don't want to share

26:15.920 --> 26:17.240
that it goes into a global knowledge base

26:17.240 --> 26:18.840
that reminds us of things.

26:18.840 --> 26:19.760
That's inevitable.

26:19.760 --> 26:20.920
On enterprise, it takes longer

26:20.920 --> 26:24.440
because you need to have auditable standardized models.

26:24.480 --> 26:25.800
If you're a financial services institute,

26:25.800 --> 26:28.000
you can't have a single piece of crawl data in there.

26:28.000 --> 26:29.400
And so that's what we're deliberately building

26:29.400 --> 26:30.880
with the largest companies in the world

26:30.880 --> 26:32.240
because we're building dedicated teams.

26:32.240 --> 26:34.000
So you can't have a single piece of crawl data

26:34.000 --> 26:35.360
if you're a financial services.

26:35.360 --> 26:36.880
Yes, because the danger is

26:36.880 --> 26:38.080
if it has some Reddit in there.

26:38.080 --> 26:38.840
So stable.

26:38.840 --> 26:40.000
And you know, we'll put out next week,

26:40.000 --> 26:41.880
it wasn't as good as the other models

26:41.880 --> 26:42.800
because we're going to make a point

26:42.800 --> 26:44.280
about Reddit data being bad.

26:44.280 --> 26:46.440
It's not about more data, it's about better data.

26:46.440 --> 26:47.280
We had data comp,

26:47.280 --> 26:48.680
which is the next generation lion

26:48.680 --> 26:50.400
that we funded the compute for,

26:50.400 --> 26:53.320
whereby at a quarter of the parameters of OpenAI's clip,

26:53.360 --> 26:55.600
that outperforms with the beta data quality.

26:55.600 --> 26:58.280
What makes good data quality, sorry?

26:58.280 --> 26:59.800
That's something we're exploring right now.

26:59.800 --> 27:01.480
But from the investment banks,

27:01.480 --> 27:02.600
we've talked to asset managers

27:02.600 --> 27:03.880
and we're building dedicated teams

27:03.880 --> 27:05.000
for the largest ones in the world

27:05.000 --> 27:06.560
to build them a prior to models.

27:06.560 --> 27:09.040
The feedback we've got is we cannot use a black box.

27:09.040 --> 27:11.000
We need to know what data is in there.

27:11.000 --> 27:13.040
Just the regulators are asking us.

27:13.040 --> 27:14.960
We don't want to have this out of sample thing

27:14.960 --> 27:16.640
where it's seen something on Reddit

27:16.640 --> 27:20.280
and then it says something rude to our end users, you know?

27:20.280 --> 27:22.040
Why is Reddit data bad?

27:22.040 --> 27:23.960
Reddit data isn't bad in itself.

27:23.960 --> 27:26.760
It was just a case of more data is not always good.

27:26.760 --> 27:29.440
So right now we are using all these web scrapes

27:29.440 --> 27:31.880
and we're training our models by taping their eyes open

27:31.880 --> 27:36.840
and then it took six months to turn GPT-4 into chat GPT-4

27:36.840 --> 27:39.480
because we had to tune it and like give it a haircut and stuff

27:39.480 --> 27:41.720
and bring it back to society.

27:41.720 --> 27:44.360
The point is that we need to find the right type of data

27:44.360 --> 27:45.560
because rubbish in, rubbish out

27:45.560 --> 27:48.360
is something that we've heard a lot, you know?

27:48.360 --> 27:50.160
And so it's not bad in itself,

27:50.160 --> 27:51.160
but if you just scrape it

27:51.160 --> 27:53.200
without the proper cleaning, it is bad.

27:53.200 --> 27:54.200
Because what is it?

27:54.200 --> 27:56.680
It's like, you know, people just covetching a lot.

27:56.680 --> 27:58.840
You know, people being biased.

27:58.840 --> 28:02.000
Do you really want to feed your kid the whole of Reddit?

28:02.000 --> 28:02.960
You know?

28:02.960 --> 28:05.960
Do you want to have the best curriculum possible?

28:05.960 --> 28:07.200
And this is actually one of the ways the models learn.

28:07.200 --> 28:09.360
Like stable diffusion, we train it on the whole internet

28:09.360 --> 28:12.280
and then better and better image subsets of it.

28:12.280 --> 28:13.280
And that's the same thing with lounge learning.

28:13.280 --> 28:14.520
It's called curriculum learning.

28:14.520 --> 28:16.760
Train it on a big base that's solid and then did it.

28:16.760 --> 28:19.040
It sounds familiar, doesn't it?

28:19.280 --> 28:21.840
The hardest thing is how do you instill values

28:21.840 --> 28:24.880
and political correctness in models?

28:24.880 --> 28:27.280
There is no such thing as an unbiased model.

28:27.280 --> 28:30.120
So Dali too, when OpenAI had that

28:30.120 --> 28:32.040
and they introduced a bias filter,

28:32.040 --> 28:35.880
any non-gendered word that ran a random gender

28:35.880 --> 28:37.680
and a random ethnicity.

28:37.680 --> 28:38.840
So you type in Sumo wrestler

28:38.840 --> 28:41.320
and you get Indian female Sumo wrestler.

28:41.320 --> 28:42.280
That was kind of a good picture.

28:42.280 --> 28:43.680
I got to save somewhere.

28:43.680 --> 28:45.800
I think this is why you need national data sets.

28:45.800 --> 28:47.040
You need cultural data sets.

28:47.040 --> 28:48.800
You need personal data sets

28:48.800 --> 28:51.280
that can interact with these base models

28:51.280 --> 28:53.080
and customize to you and your stories.

28:53.080 --> 28:55.240
Because you and I both have our stories

28:55.240 --> 28:56.320
that make up our psyche.

28:56.320 --> 28:57.160
Sure.

28:57.160 --> 28:59.120
And understanding that context is so important

28:59.120 --> 29:02.440
to have AIs that can work for us, not on us.

29:02.440 --> 29:04.440
And so it's essentially like a next generation cookie

29:04.440 --> 29:07.000
that personalizes our data to allow for better searches.

29:07.000 --> 29:07.840
And mega cookie.

29:07.840 --> 29:09.640
And if you standardize the base foundation models

29:09.640 --> 29:11.760
and they call it the hypercube every modality

29:11.760 --> 29:12.840
because we do all the modalities,

29:12.840 --> 29:15.080
all the sectors and all the nationalities,

29:15.080 --> 29:17.720
then you don't need to have a million different models

29:17.720 --> 29:19.320
like those dream booths of the avatars.

29:19.320 --> 29:20.400
You said you have a base model

29:20.400 --> 29:22.640
that you then have a vector embedding around.

29:22.640 --> 29:24.240
Because these models contain all the principles

29:24.240 --> 29:25.920
and the embeddings point to the important bits

29:25.920 --> 29:27.840
that make up Harry or Emmad.

29:27.840 --> 29:29.960
And then you can search those and adapt those

29:29.960 --> 29:32.560
rather than having a million, billion different models,

29:32.560 --> 29:33.560
which is just confusing.

29:33.560 --> 29:35.080
So I had dinner the other day

29:35.080 --> 29:37.840
with one of the largest media publication owners

29:37.840 --> 29:39.520
in the world.

29:39.520 --> 29:41.400
And he said that I'm white, Harry.

29:41.400 --> 29:42.800
I don't think that I will have a business

29:42.800 --> 29:44.080
in a couple of years.

29:44.080 --> 29:46.400
I think, bluntly, we're getting killed on our advertising

29:46.440 --> 29:48.240
because everything's getting scraped

29:48.240 --> 29:50.280
and they're not coming to our websites.

29:50.280 --> 29:51.280
And that's where we get paid.

29:51.280 --> 29:52.480
We get paid for clicks.

29:53.920 --> 29:55.640
Is he right to be worried?

29:55.640 --> 29:56.600
I think he is right to be worried.

29:56.600 --> 29:58.600
Like, again, you look at Google's announcements yesterday

29:58.600 --> 30:00.800
to talk about this day after Palm 2,

30:00.800 --> 30:02.160
you suddenly look at the new Google page

30:02.160 --> 30:04.760
where they've got the language model

30:04.760 --> 30:07.720
and it's just text and where they clicks.

30:07.720 --> 30:10.200
It was like when Google introduced AMP.

30:10.200 --> 30:11.520
You know, this is where rather than look

30:11.520 --> 30:12.360
at the New York Times page,

30:12.360 --> 30:13.200
you have this formatted thing

30:13.200 --> 30:15.640
with no New York Times kind of stuff there.

30:15.640 --> 30:18.360
Like, these search entities that aggregate

30:18.360 --> 30:20.440
are just intermediating more and more

30:20.440 --> 30:22.760
and people are going to become used

30:22.760 --> 30:25.680
to just having synthesized input.

30:25.680 --> 30:26.920
So what does search look like?

30:26.920 --> 30:29.160
What does it look like when your GPT-4

30:29.160 --> 30:32.320
can write you an article about any news that's happening

30:32.320 --> 30:33.960
in a way that's customized to you

30:33.960 --> 30:36.120
and your context and everything like that?

30:37.240 --> 30:40.440
This is massively disruptive for media and information.

30:40.440 --> 30:42.280
And so they have to think, where am I in the future?

30:42.280 --> 30:44.480
Where, again, the way I swear to think about the impact

30:44.520 --> 30:47.000
this is the retanted grads are caching off their meds

30:47.000 --> 30:49.320
and we push a button and get 1,000 of them.

30:49.320 --> 30:51.480
Those grads include journalists.

30:51.480 --> 30:53.680
And you can have your own journalist army,

30:53.680 --> 30:55.360
your own writer army, your own coder army,

30:55.360 --> 30:57.040
your own designer army.

30:57.040 --> 30:59.440
So the pushback against that is libel.

30:59.440 --> 31:01.120
He said, good fucking luck.

31:01.120 --> 31:05.520
We spend our life in law suits, libel is real.

31:05.520 --> 31:09.160
You are gonna get unbelievable amounts of libel cases

31:09.160 --> 31:11.480
and then open AI will be fucked.

31:11.480 --> 31:14.400
You cannot have 1,000 libel cases a day.

31:14.400 --> 31:15.240
Well, this is the thing.

31:15.240 --> 31:18.200
If you say this needs to be checked and cross-checked,

31:18.200 --> 31:19.880
that's one thing, but a lot of the media companies

31:19.880 --> 31:21.480
say we're the source of authority.

31:21.480 --> 31:24.680
So a way that media companies can shift is by having

31:24.680 --> 31:26.440
in a deep fake and other age

31:26.440 --> 31:28.720
where everything can be generated,

31:28.720 --> 31:31.560
we make sure this is real news.

31:31.560 --> 31:33.680
We are very thorough in the way we do it.

31:33.680 --> 31:34.520
So this is interesting,

31:34.520 --> 31:36.800
so you place a premium on authority.

31:36.800 --> 31:39.560
Premium authority, this is why you've got the check marks

31:39.600 --> 31:41.560
coming out at Twitter and the organizational

31:41.560 --> 31:44.520
1,000 pounds a month and Facebook doing the same

31:44.520 --> 31:46.440
because you need to have a level of authenticity

31:46.440 --> 31:47.800
and level of authority.

31:47.800 --> 31:50.160
But again, is the news fair and balanced?

31:50.160 --> 31:51.960
I've had lots of hit pieces coming out against me

31:51.960 --> 31:53.000
and got a lot more.

31:53.000 --> 31:54.840
It's not because they have angles, you know?

31:54.840 --> 31:56.640
And so what is the bias of the New York Times

31:56.640 --> 31:59.040
versus this, versus that, versus others?

31:59.040 --> 32:00.440
How do people consume news now?

32:00.440 --> 32:03.720
And even news consumption has gone down dramatically, right?

32:03.720 --> 32:05.800
Because people consume news through their social networks

32:05.800 --> 32:08.000
through their groups and other things.

32:08.000 --> 32:10.840
So you have to say, what is the model?

32:10.840 --> 32:13.120
But the hard part is, you know,

32:13.120 --> 32:15.280
none of the next generation models

32:15.280 --> 32:18.120
or AI providers want to be content publishers.

32:18.120 --> 32:20.440
So how do you fit in a world where, you know,

32:20.440 --> 32:23.240
you're killing that business model on the content side,

32:23.240 --> 32:24.800
but they don't want to be publishers.

32:24.800 --> 32:26.640
You will have AI first publishers.

32:27.920 --> 32:29.400
So if you remember a kind of Vox

32:29.400 --> 32:30.520
and these things when they kind of kicked off,

32:30.520 --> 32:31.560
they wanted to be generated,

32:31.560 --> 32:33.960
they wanted to be technology first.

32:33.960 --> 32:37.120
You're gonna have a new wave of AI first publishers

32:37.120 --> 32:39.880
that aren't just AI, but it's AI plus humans.

32:39.880 --> 32:41.000
Because AI plus-

32:41.000 --> 32:41.840
What does that look like?

32:41.840 --> 32:42.840
Sorry, AI plus humans.

32:42.840 --> 32:46.200
AI plus humans means that you have information coming in

32:46.200 --> 32:49.200
and then the stories or drafts are automatically written,

32:49.200 --> 32:51.520
reviewed by humans who then give their input

32:51.520 --> 32:52.680
to train it better.

32:53.800 --> 32:55.520
This is kind of the feedback flow.

32:55.520 --> 32:56.840
And then what happens is it comes out

32:56.840 --> 32:58.160
and there's a factual anchors

32:58.160 --> 33:00.040
and then it gets customized to Alabama

33:01.120 --> 33:03.880
and then Alabama context and all sorts of other things.

33:03.920 --> 33:06.320
Because you can tell it, TLDR,

33:06.320 --> 33:09.120
two ladies didn't read, explain it like I'm five,

33:09.120 --> 33:10.840
make it more complex.

33:10.840 --> 33:13.200
And so you're gonna see something very interesting here,

33:13.200 --> 33:14.920
which is the right news at the right time.

33:14.920 --> 33:17.080
The localization will return,

33:17.080 --> 33:18.760
but again, through AI first.

33:18.760 --> 33:21.520
I think this is the thing, we're seeing AI integrated,

33:21.520 --> 33:22.840
but the next wave is going to be

33:22.840 --> 33:24.080
once we understand design patterns,

33:24.080 --> 33:27.400
AI first, everything and information flows.

33:27.400 --> 33:29.160
Once these technologies are a bit more mature.

33:29.160 --> 33:30.760
Can you just help me understand AI integrated

33:30.760 --> 33:32.600
versus AI first, what is-

33:32.600 --> 33:35.000
AI integrated means that I have an existing newsroom

33:35.000 --> 33:36.760
and I bring in AI to write faster drafts

33:36.760 --> 33:37.720
and things like that.

33:37.720 --> 33:40.200
AI first is saying, I have an army of things

33:40.200 --> 33:42.680
I can spin up instantly that can help me achieve

33:42.680 --> 33:44.280
these certain things to create news

33:44.280 --> 33:47.240
that is valuable for this reason with this feedback loop.

33:47.240 --> 33:49.920
And so you build the system kind of from the start

33:49.920 --> 33:53.200
thinking AI at the core versus AI being integrated in

33:53.200 --> 33:55.040
to improve existing systems.

33:55.040 --> 33:56.400
Because so much of news is what?

33:56.400 --> 33:58.160
We find information, we have drafting,

33:58.160 --> 34:01.080
we have this, we have that, we do these checks.

34:01.080 --> 34:03.240
A lot of that can be simplified,

34:03.240 --> 34:07.040
just like we move from the analog to the digital age

34:07.040 --> 34:10.200
to the internet age to the next age as the AI age.

34:10.200 --> 34:12.560
So I have to ask, when we think about kind of AI first

34:12.560 --> 34:14.680
publishers and the next generation of media,

34:16.320 --> 34:17.200
who does this?

34:17.200 --> 34:18.600
Is this startups?

34:18.600 --> 34:22.480
I've met honestly 50, maybe more AI companies

34:22.480 --> 34:24.080
in the last month.

34:24.080 --> 34:26.840
And the feedback is always the same.

34:26.840 --> 34:30.560
They're not operating off a defensible mode of data

34:30.760 --> 34:32.280
that literally a thin application layer

34:32.280 --> 34:36.680
on top of an existing model is 99% of the feedback.

34:36.680 --> 34:38.680
So you look at something like Harvey, for example.

34:38.680 --> 34:41.280
They went to law firms, they said, you are a distribution

34:41.280 --> 34:43.280
and we're going to integrate and improve your system

34:43.280 --> 34:45.080
and build our system for your system.

34:45.080 --> 34:46.680
So I think a lot of these people are trying to build it

34:46.680 --> 34:48.280
and they will come and they're trying to get in there

34:48.280 --> 34:50.320
as opposed to just retargeting.

34:50.320 --> 34:51.600
Where can you go in and transform?

34:51.600 --> 34:53.360
Is that the wrong model Harvey did?

34:53.360 --> 34:54.320
No, I think it's the right model.

34:54.320 --> 34:56.520
I think that a lot of organizations are elastic

34:56.520 --> 34:58.400
and plastic now, so you can go in and give them

34:58.440 --> 35:00.680
an integrated thing saying, you will be my test case.

35:00.680 --> 35:03.320
I will help you upgrade as a Skunkworks lab

35:03.320 --> 35:05.880
and build a system alongside your system as it were.

35:05.880 --> 35:08.200
And sorry, and you think enterprises will say sure?

35:08.200 --> 35:11.240
I think now they will if you can keep

35:11.240 --> 35:13.160
their data inside internally.

35:13.160 --> 35:14.880
And I think again, with better open models,

35:14.880 --> 35:16.000
you can enable that.

35:16.000 --> 35:17.520
So people can build on top of open models.

35:17.520 --> 35:20.880
There are dedicated instances on Cohere and others as well.

35:20.880 --> 35:22.600
And so the tooling is now catching up

35:22.600 --> 35:24.840
so that you can have a new generation of startups

35:24.840 --> 35:27.000
where their first customers are massive companies,

35:27.000 --> 35:29.520
they would never get otherwise.

35:29.520 --> 35:30.360
I think this is the thing,

35:30.360 --> 35:32.440
because every big company is looking for an answer.

35:32.440 --> 35:34.160
If you can give that answer,

35:34.160 --> 35:35.760
that contract that would have taken you a year,

35:35.760 --> 35:37.360
you can get in a week.

35:37.360 --> 35:38.200
Do you think so?

35:38.200 --> 35:39.400
Because you still got to get in the door.

35:39.400 --> 35:42.040
You got to get in the door, and that's hustle, man.

35:42.040 --> 35:44.440
So again, this is what the Harvey guys kind of did.

35:44.440 --> 35:46.320
This is why I went straight to the hyperscalers

35:46.320 --> 35:48.320
and I said, you need to have standardized models

35:48.320 --> 35:49.640
for open, for regulated data.

35:49.640 --> 35:50.880
What did they say to you?

35:50.880 --> 35:52.800
They said, really?

35:52.800 --> 35:53.640
Can you build them?

35:53.640 --> 35:54.800
Here's some models that we built.

35:54.800 --> 35:56.640
Oh, and then I told them exactly

35:56.640 --> 35:59.120
how the things would be last summer to now.

35:59.120 --> 36:00.960
And it's followed that and I've kept in touch

36:00.960 --> 36:01.800
and I've improved it.

36:01.800 --> 36:03.000
And this is why I'm building dedicated teams

36:03.000 --> 36:04.520
for the largest companies in the world.

36:04.520 --> 36:05.880
I'm not telling them, I'm trying to sell you anything.

36:05.880 --> 36:07.520
I'm like, over the next year,

36:07.520 --> 36:10.520
I'm gonna help make sure you do not get blindsided.

36:10.520 --> 36:11.840
Like I try and sell you models

36:11.840 --> 36:14.280
and people are offering us tens of millions per model.

36:14.280 --> 36:17.480
But I'm like, I'm gonna build a proper partnership with you.

36:17.480 --> 36:19.560
And that means I'll have a LTV from you.

36:19.560 --> 36:21.240
What does that proper partnership mean?

36:21.240 --> 36:22.080
And who's that with?

36:22.080 --> 36:24.000
That's with IBM, that's with SAP, that's with Apple.

36:24.000 --> 36:26.000
So we've announced Amazon.

36:26.120 --> 36:27.440
Let's say we have lots of other announced

36:27.440 --> 36:28.640
with the biggest companies in the world

36:28.640 --> 36:30.640
where they have amazing teams,

36:30.640 --> 36:32.440
but they can only move so fast.

36:32.440 --> 36:33.680
And I'm building dedicated teams

36:33.680 --> 36:36.000
that help them move and understand the whole sector

36:36.000 --> 36:39.160
without trying to sell them on services.

36:39.160 --> 36:41.240
I'm trying to say, I will build you a customized model

36:41.240 --> 36:42.480
if you want, but I'm only doing that

36:42.480 --> 36:44.600
with a dozen companies, you know?

36:44.600 --> 36:46.200
So I can kind of focus down.

36:46.200 --> 36:48.560
And I will tell you that GPT-4 is great

36:48.560 --> 36:50.640
or Cohera is great or all this stuff.

36:50.640 --> 36:52.760
All the latest research through the communities we support,

36:52.760 --> 36:54.400
I will make sure you're on top of,

36:54.440 --> 36:55.280
rather than to your sector

36:55.280 --> 36:56.320
and you've got dedicated people

36:56.320 --> 36:57.680
helping you in this transition period.

36:57.680 --> 36:59.160
Is that aligned to your core model?

36:59.160 --> 37:01.400
It seems like an ancillary product

37:01.400 --> 37:04.120
that is like a SAP consulting services.

37:04.120 --> 37:05.360
It is kind of like that

37:05.360 --> 37:08.760
because I need to understand these sectors better.

37:08.760 --> 37:10.200
What does the hypercube look like?

37:10.200 --> 37:13.040
What does the insurance adjust to GPT look like?

37:13.040 --> 37:14.920
You know, as a fundamental basis.

37:14.920 --> 37:17.360
And so a lot of people are able to extract that data

37:17.360 --> 37:19.080
and then take it with you and do the learning on it.

37:19.080 --> 37:20.360
Yes, and so this is part of the thing,

37:20.360 --> 37:22.480
that we will have a generalized model and we're very clear.

37:22.480 --> 37:24.120
But then you can have a specified model

37:24.120 --> 37:25.480
just for you as well,

37:25.480 --> 37:27.040
as long as it doesn't interfere with that.

37:27.040 --> 37:28.720
So find that balance will be interesting,

37:28.720 --> 37:31.600
but the reality is no models that are out today

37:31.600 --> 37:33.280
will be used in a year.

37:33.280 --> 37:35.480
Unpack that for me, this is mind blowing.

37:35.480 --> 37:37.480
So again, you see the order of magnitude improvement.

37:37.480 --> 37:39.720
Palm last year was 540 billion parameters,

37:39.720 --> 37:41.720
then Chinchilla 67 and now 14.

37:43.280 --> 37:46.440
540 to 14 is a big step, you know?

37:46.440 --> 37:49.160
You see the quality of GPT-3 versus GPT-4.

37:49.160 --> 37:50.840
Is there any extent to how low it can go?

37:50.840 --> 37:51.800
We have no idea.

37:52.800 --> 37:53.720
Like I would have said,

37:53.720 --> 37:56.000
you already said this is impossible.

37:56.000 --> 37:58.560
Like two years ago, it was like no way.

37:58.560 --> 38:00.840
You have a single file that's maybe a few hundred gigabytes

38:00.840 --> 38:03.840
that can pass every exam apart from English Lit.

38:03.840 --> 38:04.880
Fucking English Lit.

38:04.880 --> 38:07.280
Fucking English Lit, no way, no way.

38:07.280 --> 38:09.160
So we're already at the impossible and like,

38:09.160 --> 38:10.480
what does that mean though?

38:10.480 --> 38:13.760
Like if we go lower and lower and lower, and then what?

38:13.760 --> 38:14.600
And then what?

38:14.600 --> 38:16.760
When it jumps as you saw with the llama stuff

38:16.760 --> 38:18.320
and all the innovation around that

38:18.320 --> 38:20.720
to your MacBook offline,

38:20.760 --> 38:22.120
the marginal cost of creation

38:22.120 --> 38:23.760
and coordination becomes zero.

38:25.520 --> 38:27.120
I don't know what it means.

38:27.120 --> 38:28.240
Nobody does.

38:28.240 --> 38:29.840
And this is the thing.

38:29.840 --> 38:31.560
It always takes longer and shorter

38:31.560 --> 38:33.800
to implement groundbreaking technology than you've ever seen

38:33.800 --> 38:35.040
and this technology can be in place

38:35.040 --> 38:36.760
like nothing we've seen before.

38:36.760 --> 38:38.200
Well, this is my call.

38:38.200 --> 38:40.640
Not concerned, I hate the doomsday it says

38:40.640 --> 38:41.600
and I'm excited for the future,

38:41.600 --> 38:42.640
I'm terrified for the future too,

38:42.640 --> 38:44.720
but everyone always says technology revolutions

38:44.720 --> 38:45.560
in industrial age,

38:45.560 --> 38:47.560
whether it's the introduction of PCs in 25s.

38:47.560 --> 38:50.040
These were, industrial was a 30 year plus.

38:50.120 --> 38:52.760
Actually PCs in 25s was 10 years plus.

38:52.760 --> 38:56.080
The challenging thing is like the learning curve

38:56.080 --> 38:59.840
to use chat GPT as a marketer is nothing.

38:59.840 --> 39:00.880
I mean, it is easy.

39:00.880 --> 39:03.640
And so, and the integrations is a day.

39:03.640 --> 39:05.560
It's because, yeah, like you want to write an API,

39:05.560 --> 39:08.000
it's not a day, you just give it the manifest spec

39:08.000 --> 39:09.240
and it automatically generates.

39:09.240 --> 39:10.480
It would have taken days before.

39:10.480 --> 39:11.320
Sure.

39:11.320 --> 39:12.680
Like it's an amazing experience.

39:12.680 --> 39:15.400
And so the transition is so much more compressed today.

39:15.400 --> 39:17.360
It came from the existing system

39:17.360 --> 39:19.240
as it goes seamlessly into the existing system

39:19.240 --> 39:20.880
versus like web three that tried to create a system

39:20.880 --> 39:22.640
outside the existing system

39:22.640 --> 39:25.080
and all the money was made and lost at the interfaces.

39:25.080 --> 39:26.960
Again, it's like deploying grads at scale.

39:26.960 --> 39:30.280
Like with a 32,000 token context within your GPT-4,

39:30.280 --> 39:32.760
20,000 words of instructions.

39:32.760 --> 39:34.320
What does that do to SAS?

39:34.320 --> 39:36.800
So my thing is that we're still in this crazy period.

39:36.800 --> 39:39.480
Next year it will settle and then it'll go ubiquitous.

39:39.480 --> 39:41.080
Well, a lot of companies know they need to do something

39:41.080 --> 39:42.120
but they don't know what they need to do.

39:42.120 --> 39:43.560
Are they adopting it now?

39:43.560 --> 39:44.800
They're doing the POC thing.

39:44.800 --> 39:47.000
Like some like Microsoft and others for consumer,

39:47.000 --> 39:47.840
they're adopting it.

39:47.840 --> 39:50.040
Consumer adoption is a much lower bar.

39:50.040 --> 39:51.480
When this starts going in enterprise,

39:51.480 --> 39:53.480
it's gonna be a fricking train

39:53.480 --> 39:55.680
because so much of enterprise is about services

39:55.680 --> 39:57.400
and information flow.

39:57.400 --> 39:58.840
Again, if you push a button and have a thousand

39:58.840 --> 40:03.840
of these things, that's a huge difference, right?

40:04.560 --> 40:06.320
And so like, I think this will be

40:06.320 --> 40:08.800
a bigger economic impact than COVID.

40:08.800 --> 40:10.680
I don't know in which direction.

40:10.680 --> 40:11.920
I hope that you're positive.

40:11.920 --> 40:13.840
But again, giving that example of an India

40:13.840 --> 40:15.280
or one of these outsource places,

40:15.280 --> 40:16.520
you lose BPO jobs,

40:16.560 --> 40:19.200
you can make it up on entrepreneurship

40:19.200 --> 40:20.880
if you embrace the technology.

40:20.880 --> 40:22.840
What do you think the business model of the future is

40:22.840 --> 40:25.160
for those models moving into enterprise?

40:25.160 --> 40:26.520
I think it's the same as always.

40:26.520 --> 40:28.600
You've got good products, good distribution.

40:28.600 --> 40:31.880
You lock it in, 1.5 million people still use AOL.

40:33.240 --> 40:35.720
Like HCL bought Lotus Notes for 1.5 billion

40:35.720 --> 40:36.920
a few years ago.

40:36.920 --> 40:39.720
Like 40% of the world still doesn't have internet.

40:42.880 --> 40:45.520
Again, we're super privileged where we are, right?

40:45.560 --> 40:46.400
And so you look at that

40:46.400 --> 40:48.040
and I look at emerging markets.

40:48.040 --> 40:51.280
I'm like, all of finance is securitization and leverage

40:51.280 --> 40:53.920
and securitization is telling a story.

40:53.920 --> 40:55.200
The only thing that matters for a stock

40:55.200 --> 40:57.640
is the marginal story and how it evolves.

40:57.640 --> 40:59.240
What if you have massive information

40:59.240 --> 41:02.440
about every child in Africa and every business in India

41:02.440 --> 41:04.600
and they embrace this technology properly?

41:05.720 --> 41:07.440
Massive financial growth.

41:07.440 --> 41:10.160
Why do you think next year for their embracing it?

41:10.160 --> 41:12.360
I think that people are still getting used to all this.

41:12.360 --> 41:13.840
We haven't standardizing things.

41:13.840 --> 41:16.240
We don't know what the design patterns are.

41:16.240 --> 41:18.200
I think that what happens is everyone's doing this

41:18.200 --> 41:19.520
at the same time and they're all trying

41:19.520 --> 41:20.800
to get to grips with it.

41:20.800 --> 41:22.520
And so again, we have this like six month window

41:22.520 --> 41:23.600
where everyone's getting to grips with it

41:23.600 --> 41:25.920
and then we standardize our design patterns

41:25.920 --> 41:28.400
and they spread and you start implementing.

41:28.400 --> 41:29.840
And then you see some people outpacing others

41:29.840 --> 41:30.960
which means that you have to catch up

41:30.960 --> 41:33.240
and then you're forced to implement it.

41:33.240 --> 41:36.000
So this is how I see the race dynamics occurring right now.

41:36.000 --> 41:37.480
You say about forced to implement it.

41:37.480 --> 41:39.760
I think the truth is they just have no fricking idea.

41:39.760 --> 41:40.600
Right now they don't.

41:40.600 --> 41:42.160
Which I totally understand, I didn't blame them for

41:42.160 --> 41:44.160
but I tweeted actually the other day

41:44.160 --> 41:45.760
that I think the biggest AI companies

41:45.760 --> 41:47.760
would be services-based implementation companies

41:47.760 --> 41:49.200
for large enterprises.

41:49.200 --> 41:50.040
100%.

41:50.040 --> 41:51.360
That's why I said if you're a startup,

41:51.360 --> 41:53.360
the best thing to do is you identify an enterprise

41:53.360 --> 41:54.640
that will be transformed by this

41:54.640 --> 41:57.840
and you go to them and you say, I have a solution.

41:57.840 --> 41:59.440
And I'm gonna start with you.

41:59.440 --> 42:01.520
And I might go bigger, but I'm gonna help you

42:01.520 --> 42:04.480
through this period by doing this, this, and this.

42:04.480 --> 42:05.520
And they will appreciate that

42:05.520 --> 42:07.760
and they'll be capital available for that

42:07.760 --> 42:09.640
in a way that you've never seen before.

42:09.680 --> 42:11.160
You know how difficult it is for small companies

42:11.160 --> 42:14.240
to sell to big, but the big companies have no idea

42:14.240 --> 42:17.440
except for their CEO and their board are telling them.

42:17.440 --> 42:18.640
You look at the number of mentions

42:18.640 --> 42:20.360
and earnings calls, it's not like that.

42:20.360 --> 42:21.840
Every earnings call next quarter

42:21.840 --> 42:24.200
and then by next year, literally every single one,

42:24.200 --> 42:25.560
they're like, what is our strategy?

42:25.560 --> 42:27.840
It's not like, what is our web three and metaverse strategy?

42:27.840 --> 42:29.480
It's like, I need this strategy now.

42:29.480 --> 42:31.760
Again, it's like, what is our COVID strategy?

42:31.760 --> 42:34.400
It'll be that level of urgency within a few quarters.

42:34.400 --> 42:35.680
Would you raise money if you were then?

42:35.680 --> 42:37.200
So you go to a corporate and you say,

42:37.200 --> 42:39.520
hey, you know what, I can solve your problem.

42:39.520 --> 42:41.480
This is how it'll work and they will fund you.

42:41.480 --> 42:42.600
They'll give you the data.

42:42.600 --> 42:44.280
Would you raise money?

42:44.280 --> 42:46.320
Yeah, I mean, like again, you need the people.

42:46.320 --> 42:47.560
The people is the key thing here

42:47.560 --> 42:49.840
because you can have the technicals' chops, you know?

42:49.840 --> 42:51.840
You have an understanding of the industry.

42:51.840 --> 42:54.360
But to build a good business and to scale it

42:54.360 --> 42:55.480
at the pace that you need to,

42:55.480 --> 42:57.360
to keep up with this is incredibly hard.

42:57.360 --> 42:58.720
Do we have enough talent?

42:58.720 --> 42:59.880
No.

42:59.880 --> 43:02.200
And so this is why we support the fast.ai courses

43:02.200 --> 43:05.280
which transform normal developers into ML developers

43:05.280 --> 43:06.720
and other things like that.

43:06.720 --> 43:08.360
But again, these models are actually not that hard

43:08.360 --> 43:09.200
to work with.

43:09.200 --> 43:11.680
50% of all code on GitHub is AI generated now.

43:11.680 --> 43:13.240
So you can even use co-pilot to help you

43:13.240 --> 43:15.080
code the models and other things like that.

43:15.080 --> 43:18.880
What do you think that code generation is in five years?

43:18.880 --> 43:20.360
Why would you need code?

43:20.360 --> 43:22.480
Code is just a way to talk to a computer.

43:23.680 --> 43:24.520
I'm part of that.

43:24.520 --> 43:25.360
So when I started...

43:25.360 --> 43:26.280
You're speaking human language.

43:26.280 --> 43:31.040
When I started 21, 22 years ago as a code, I'm 40 now.

43:31.040 --> 43:32.400
So just doing that one's 18.

43:32.400 --> 43:35.520
I was writing assembly code, you know,

43:35.520 --> 43:37.280
really low level stuff.

43:37.320 --> 43:38.160
There were no libraries.

43:38.160 --> 43:39.000
There was no GitHub.

43:39.000 --> 43:39.840
There was nothing like this.

43:39.840 --> 43:42.000
Like right now coding is like mixing and matching.

43:42.000 --> 43:43.920
It's like building Lego.

43:43.920 --> 43:46.360
And AI can build that Lego much better,

43:46.360 --> 43:47.520
especially in five years.

43:47.520 --> 43:49.880
What you're doing when you're propping like programming

43:49.880 --> 43:52.120
language is you're telling it to go and do something.

43:52.120 --> 43:53.200
Even something like Palm,

43:53.200 --> 43:56.160
like we sponsor an amazing code called Lucid Raines.

43:56.160 --> 43:57.360
If you want to cry as a programmer,

43:57.360 --> 43:58.440
you go and look at his GitHub,

43:58.440 --> 44:00.520
most productive developer in the world.

44:00.520 --> 44:03.520
He recreated the whole of Palm in 206 lines of PyTorch.

44:05.160 --> 44:07.240
But again, why would you need a human for that?

44:07.240 --> 44:09.520
If the AI gets better and better at coding,

44:09.520 --> 44:10.720
just tell it what you want.

44:10.720 --> 44:14.000
I want to create an app for 20 minute VC

44:14.000 --> 44:14.960
that has these features.

44:14.960 --> 44:17.880
Of course it will go and build it automatically.

44:17.880 --> 44:19.560
Where is the human coder in that?

44:19.560 --> 44:21.120
What does that mean for the future on refresh?

44:21.120 --> 44:22.920
But actually a good thing in terms of the complete

44:22.920 --> 44:24.800
democratization, anyone can build anything.

44:24.800 --> 44:25.640
Anyone can build anything.

44:25.640 --> 44:28.640
This is why distribution data, you know,

44:28.640 --> 44:31.840
relationships, product become important.

44:31.840 --> 44:34.640
Because it already became easier to build anything, right?

44:34.640 --> 44:36.560
But what makes a good product?

44:36.560 --> 44:38.360
Again, there are these unchanging things.

44:38.360 --> 44:40.960
Have great customer satisfaction, deliver value.

44:40.960 --> 44:43.040
People get distracted by technology.

44:43.040 --> 44:45.640
Like I was at this CryptoX AI thing on the weekend.

44:45.640 --> 44:47.240
They were talking about decentralized.

44:47.240 --> 44:48.520
Guys, just this is all bollocks.

44:48.520 --> 44:51.000
Like it's not about the technology,

44:51.000 --> 44:53.960
it's about what you're creating that's valuable

44:53.960 --> 44:55.920
to help people, you know?

44:55.920 --> 44:56.840
Focus on that.

44:56.840 --> 44:59.480
Who do you think wins in the next three to five years?

44:59.480 --> 45:00.640
Startups or incumbents?

45:00.640 --> 45:02.560
Because incumbents have the distribution.

45:02.560 --> 45:03.760
I think it's incumbents,

45:03.760 --> 45:05.440
but there's a lot of startups that will be billion dollars.

45:05.480 --> 45:07.760
And even on the thin layer thing,

45:07.760 --> 45:12.240
ITA software sold for 700 million

45:12.240 --> 45:13.920
and KIAX sold for 2 billion.

45:13.920 --> 45:16.000
And that was a layer on top of ITA.

45:16.000 --> 45:17.920
We've seen many of these examples here, right?

45:17.920 --> 45:19.640
Again, we know that value and moats

45:19.640 --> 45:22.240
are not necessarily innovation first.

45:22.240 --> 45:23.880
Well, yes and no, it's interesting.

45:23.880 --> 45:26.160
I had Tom Tunga's on the show.

45:26.160 --> 45:27.920
And he essentially analyzed,

45:27.920 --> 45:29.880
Tom is a very famous ML and AI investor,

45:29.880 --> 45:33.520
and he analyzed infrastructure versus application layer.

45:33.520 --> 45:36.200
And both actually were about $2 trillion times.

45:36.200 --> 45:37.880
The differences in the infrastructure layer,

45:37.880 --> 45:39.480
there was three companies,

45:39.480 --> 45:41.840
and in the application layer, there was 50.

45:41.840 --> 45:44.280
And so your average enterprise value

45:44.280 --> 45:45.640
was like significantly different.

45:45.640 --> 45:46.480
I would agree with that.

45:46.480 --> 45:48.560
I think that there's only gonna be five or six

45:48.560 --> 45:50.440
foundation model companies in the world

45:50.440 --> 45:52.320
in three years, five years.

45:52.320 --> 45:54.160
Do you think they've all been created now?

45:54.160 --> 45:55.000
Yes.

45:55.000 --> 45:55.840
Which are they?

45:55.840 --> 45:59.640
I think it's gonna be us, Nvidia, Google,

45:59.640 --> 46:02.640
Microsoft, OpenAI, and Meta and Apple probably

46:02.680 --> 46:04.200
are the ones that train these models.

46:04.200 --> 46:05.800
Is Anthropic good?

46:05.800 --> 46:06.920
Anthropic are great.

46:06.920 --> 46:09.280
But from a business model perspective,

46:09.280 --> 46:13.040
you have Claude on Google API, and you have Palm II.

46:14.080 --> 46:16.080
How are they gonna keep up with Palm II?

46:18.120 --> 46:18.960
You know?

46:18.960 --> 46:19.960
I can't answer that.

46:19.960 --> 46:21.720
Well, Google, they can raise billions,

46:21.720 --> 46:25.240
but Google will spend $20 billion on AI.

46:25.240 --> 46:28.000
DeepMind's salary budget is $1.2 billion a year.

46:28.880 --> 46:31.640
So DeepMind's salary budget is $1.2 billion a year.

46:31.640 --> 46:32.480
Yes.

46:32.480 --> 46:33.800
So that's in the public kind of filing.

46:33.800 --> 46:34.720
So was it salary and compute?

46:34.720 --> 46:35.760
I think it's salary.

46:35.760 --> 46:37.120
They technically make a billion a year

46:37.120 --> 46:40.520
from their internal counter payments with Google as well.

46:40.520 --> 46:41.360
Wow.

46:41.360 --> 46:42.200
But again, I mean, Google,

46:42.200 --> 46:43.040
how much money do they have?

46:43.040 --> 46:45.080
$150 billion to win this?

46:46.440 --> 46:47.280
Fuck.

46:47.280 --> 46:49.080
How much money do you need?

46:49.080 --> 46:52.160
I have a business model that is going to be massively

46:52.160 --> 46:53.600
profitable very soon.

46:53.600 --> 46:55.560
Because of the national services?

46:55.560 --> 46:56.520
Because of various things.

46:56.520 --> 46:57.520
I haven't given the full details.

46:57.520 --> 46:58.600
I will over the next few months.

46:58.600 --> 46:59.960
I've got a nice little case study

46:59.960 --> 47:01.520
with some universities coming.

47:01.520 --> 47:02.960
I like it to be a surprise.

47:02.960 --> 47:04.680
Well, it's really hard for you.

47:04.680 --> 47:07.040
Talent, keeping talent together, A plus teams.

47:07.040 --> 47:08.960
So we've had zero attrition in our developers

47:08.960 --> 47:09.800
and they're amazing.

47:09.800 --> 47:11.160
So we've got video models, audio models,

47:11.160 --> 47:12.000
all these things coming out.

47:12.000 --> 47:13.680
Everyone says you need to be in the valley.

47:13.680 --> 47:14.560
You're in London.

47:14.560 --> 47:15.400
Yeah.

47:15.400 --> 47:16.840
Do you disagree you need to be in the valley?

47:16.840 --> 47:17.960
Of course you don't.

47:17.960 --> 47:19.800
I am going to bring this technology to the whole world.

47:19.800 --> 47:22.080
I'm going to bring it to all the IITs and universities

47:22.080 --> 47:23.920
and the best of people in all of those

47:23.920 --> 47:25.920
will join Stabilities in the local thing.

47:25.920 --> 47:26.760
I'll have talent.

47:26.760 --> 47:29.560
I'll bring this to all of the national broadcasters

47:29.560 --> 47:31.080
and biggest family offices around the world.

47:31.080 --> 47:32.320
I'll have data.

47:32.320 --> 47:34.000
Nations will build supercomputers

47:34.000 --> 47:35.120
that I will build open models on.

47:35.120 --> 47:36.400
I'll have super compute.

47:36.400 --> 47:38.280
So I'll have more super compute talent

47:38.280 --> 47:40.000
and data than any other company.

47:40.000 --> 47:42.600
And I'll build it all in the open.

47:42.600 --> 47:44.320
And one thing I heard you talk about before,

47:44.320 --> 47:45.160
which I thought was fascinating,

47:45.160 --> 47:46.840
was your access to super computer.

47:46.840 --> 47:49.360
You compared it to existing large incumbents.

47:49.360 --> 47:51.880
Why do you have more super compute than other people?

47:51.880 --> 47:53.360
Because I went and I did it.

47:53.360 --> 47:55.440
So we had articles coming out saying about our burn.

47:55.440 --> 47:57.240
I'm like, I have oil wells

47:57.240 --> 47:59.440
when everyone wants to build petrochemicals.

47:59.440 --> 48:01.280
Every day we have companies coming to us

48:01.280 --> 48:02.520
asking us for our super compute

48:02.520 --> 48:04.360
because it's not available on the market.

48:04.360 --> 48:06.240
You need these chips lined up with interconnects

48:06.240 --> 48:09.080
and we've got 7,000 A100s now.

48:09.080 --> 48:11.200
We have TPUs, we have all these things.

48:11.200 --> 48:12.160
And we know how to use them

48:12.160 --> 48:14.800
and we can share them with people because we're open.

48:15.800 --> 48:18.120
Whereas Anthropic and others cannot.

48:18.120 --> 48:20.360
So this is like at the worst case,

48:20.360 --> 48:23.080
I'll build a foundation model as a service company

48:23.080 --> 48:25.480
and I'll make $100 million in profit this year

48:25.480 --> 48:27.360
without having to charge even market rates

48:27.360 --> 48:28.200
and I can retire.

48:28.200 --> 48:30.080
I wouldn't do that and bring this to the world.

48:30.080 --> 48:32.760
So I think computers misunderstood.

48:32.760 --> 48:36.520
It's not like Bird and all these scooter companies

48:36.520 --> 48:39.400
and others, they spent money on marketing.

48:39.400 --> 48:41.960
This is actually an asset right now that's scarce.

48:42.880 --> 48:44.960
And so there's no harm in scaling compute

48:44.960 --> 48:47.000
and then with the top chip manufacturers,

48:47.000 --> 48:48.640
they're building us dedicated teams

48:48.640 --> 48:51.120
and again, they're coming in and supporting us

48:51.120 --> 48:54.600
because our models drive demand for their chips.

48:54.600 --> 48:56.600
The more open models there are, the more open demand is.

48:56.600 --> 48:58.600
So it's a virtual circle there as well.

48:58.600 --> 49:00.800
And so we get compute before everyone else.

49:00.800 --> 49:03.520
Can I ask, in terms of like short-term economic growth,

49:03.520 --> 49:06.400
how do you think about the impact that everything

49:06.400 --> 49:09.040
we've just discussed has on rising inflation,

49:09.040 --> 49:12.920
rising interest rates, short-term employment rates?

49:12.920 --> 49:14.440
It's massively deflationary.

49:14.440 --> 49:16.800
The biggest drivers of CPI inflation in the U.S.

49:16.800 --> 49:18.200
were education and healthcare.

49:19.480 --> 49:21.280
And that was almost all administrative

49:21.280 --> 49:23.640
and bureaucratic in the next few years,

49:23.640 --> 49:25.960
guess what gets disrupted, those.

49:25.960 --> 49:27.920
But they don't get disrupted this year

49:27.920 --> 49:29.920
or next year, it's the year after

49:29.920 --> 49:33.160
because those ones take a bit longer to come through.

49:33.160 --> 49:35.560
Okay, and so what is that?

49:35.560 --> 49:36.720
How does that impact the economy

49:36.720 --> 49:39.120
that we think kind of U.S., U.K.?

49:39.120 --> 49:41.120
What does that look like in terms of a three-year time period?

49:41.120 --> 49:42.920
I think the U.K. benefits.

49:42.920 --> 49:44.760
Unicorn Kingdom is a new kind of thing is,

49:44.760 --> 49:48.320
because it'll get, because we have amazing policies

49:48.320 --> 49:50.720
like every single AI company should come to the U.K.

49:50.720 --> 49:54.640
because cloud computing is now included in R&D tax credits.

49:54.640 --> 49:57.600
It's a 27% rebate on losses in cash.

49:58.600 --> 50:01.360
We can now issue scale-up visas, global talent visas

50:01.360 --> 50:02.280
like the Deep Floyd team

50:02.280 --> 50:04.560
that released the best image model in the world ever

50:04.560 --> 50:05.800
from stability.

50:05.800 --> 50:07.440
They were bought in on tech talent visas

50:07.440 --> 50:08.560
that was turned around in one week.

50:08.560 --> 50:09.880
Do you think the U.K. has done a good job

50:09.880 --> 50:12.040
in terms of implementing regulation and policies

50:12.040 --> 50:13.440
to bring AI talents?

50:13.440 --> 50:15.800
Had the best apart from maybe Japan, yes.

50:15.800 --> 50:17.360
What's Japan done?

50:17.360 --> 50:19.080
Japan has some very interesting ones around

50:19.080 --> 50:20.240
web data scraping and others,

50:20.240 --> 50:22.120
but again, Japan is a very different culture.

50:22.120 --> 50:23.320
So even if policy is good,

50:23.320 --> 50:25.080
it still doesn't have the same innovative thing.

50:25.080 --> 50:26.400
Who's done the worst?

50:27.320 --> 50:28.640
The worst, I'm not sure actually.

50:28.640 --> 50:29.520
No one's done too bad.

50:29.520 --> 50:31.760
The new European legislation was really, really bad.

50:31.760 --> 50:33.000
Now it's got a little bit better,

50:33.000 --> 50:35.640
but always Europe wants to be the leader in regulation,

50:35.640 --> 50:37.600
which kind of, you know, okay, fair enough.

50:37.600 --> 50:38.560
It's never an easy thing.

50:38.560 --> 50:39.400
It's never an easy thing.

50:39.400 --> 50:40.240
And this is the thing,

50:40.240 --> 50:42.360
like I think the U.K. is in a very good position

50:42.360 --> 50:43.640
and the government's for leading me.

50:43.640 --> 50:46.040
Look, the 900 million pound supercomputer,

50:46.040 --> 50:47.960
100 million pound LLM task force

50:47.960 --> 50:51.760
that's been equated to the COVID level of seriousness,

50:51.760 --> 50:52.720
you know?

50:52.720 --> 50:55.480
What do you make of like the open AI comparison?

50:55.480 --> 50:58.240
I've seen quite a few which are open AI for Europe.

50:59.440 --> 51:01.280
And we've seen three or four now.

51:01.280 --> 51:03.440
Like, is this a zero sum game?

51:03.440 --> 51:06.240
And open AI is one that race, so to speak, or?

51:06.240 --> 51:08.080
I think it'd be difficult to compete against them

51:08.080 --> 51:09.720
because they're executing incredibly well.

51:09.720 --> 51:11.240
And I think, again,

51:12.280 --> 51:14.360
why would you use open AI for Europe

51:14.360 --> 51:18.600
versus palm two versus GPT four?

51:18.600 --> 51:19.440
What can you bring?

51:19.440 --> 51:20.840
But you will have national champions and others.

51:20.840 --> 51:22.120
I think it's incredibly difficult

51:22.120 --> 51:23.440
to compete in proprietary.

51:25.160 --> 51:26.840
I think in open, it's a bit different

51:26.840 --> 51:28.720
because of standardization element there.

51:28.720 --> 51:31.560
But again, my play is to be the benchmark

51:31.560 --> 51:32.760
across every modality,

51:32.760 --> 51:33.720
because there's no other company

51:33.720 --> 51:36.080
apart from me in open AI that does every modality.

51:36.080 --> 51:37.920
There's no company that's as aggressive

51:37.920 --> 51:39.680
as me in emerging markets.

51:39.680 --> 51:41.520
And so they have to say, what is my edge?

51:41.520 --> 51:42.400
Because you can have an edge,

51:42.400 --> 51:46.440
like you can be the open AI for government or defense

51:46.440 --> 51:49.760
or for healthcare and really get in and understand those.

51:49.760 --> 51:51.720
And then you can be sticky, you know?

51:52.280 --> 51:55.920
Scale is now going fully into defense, you know?

51:55.920 --> 51:56.760
Scale AI's?

51:56.760 --> 51:58.840
Yes, so they've announced the integrations

51:58.840 --> 52:00.600
with the Air Force and all sorts of other things.

52:00.600 --> 52:03.320
Like we're getting together on Defcorn this year

52:03.320 --> 52:04.760
and people are going to hack at our models

52:04.760 --> 52:07.320
and open AI's and others organized by scale.

52:07.320 --> 52:09.120
What is your edge?

52:09.120 --> 52:11.120
What is, again, your moat?

52:11.120 --> 52:12.960
What is your business model and where?

52:12.960 --> 52:14.240
What are you reliant upon

52:14.240 --> 52:16.480
to deliver that value that can increase?

52:16.480 --> 52:17.600
This is why I was surprised

52:17.600 --> 52:19.960
when I saw you sign the petition of Elon

52:20.040 --> 52:22.000
in terms of pausing for six months.

52:22.000 --> 52:24.000
You used to unpack why you did that.

52:24.000 --> 52:25.320
Well, I mean, for six months,

52:25.320 --> 52:27.840
you're not getting H100s and TPUV5s anyway.

52:27.840 --> 52:29.240
So it's a natural pause.

52:29.240 --> 52:30.960
But then also because the shit show is coming next year.

52:30.960 --> 52:32.760
So I said, we have to self-regulate.

52:32.760 --> 52:35.840
Like for example, the adversaries already have GPT-4.

52:35.840 --> 52:38.840
Why? Because you can just download it on a USB stick.

52:38.840 --> 52:40.160
You know, you don't have to train your own

52:40.160 --> 52:41.000
when you can just steal it.

52:41.000 --> 52:42.480
Let's have better opsec.

52:42.480 --> 52:44.000
Let's have better standards around data.

52:44.000 --> 52:46.760
Let's stop and move off web scrapes by next year.

52:46.760 --> 52:48.080
We had hundreds of millions of images

52:48.080 --> 52:49.120
opted out of stable diffusion

52:49.120 --> 52:51.400
because we were the only company in the world

52:51.400 --> 52:53.760
to offer opt-out of datasets.

52:53.760 --> 52:56.720
You know, like let's bring in some standards around this

52:56.720 --> 52:58.400
before it's everywhere.

52:58.400 --> 52:59.240
Basically where we are now.

52:59.240 --> 53:02.280
You remember COVID, your mom is talking about this

53:02.280 --> 53:04.960
and your aunt and everyone's talking about generative AI

53:04.960 --> 53:06.760
and they're asking you, Harry, what's going on?

53:06.760 --> 53:10.440
You know, but you haven't had the Tom Hanks moment yet.

53:10.440 --> 53:11.800
Because everyone was talking about COVID

53:11.800 --> 53:13.040
before Tom Hanks got it.

53:13.040 --> 53:14.120
And then when Tom Hanks got it,

53:14.120 --> 53:16.160
that's when global policy changed.

53:16.160 --> 53:18.040
Because if Tom Hanks can get it, anyone can get it.

53:18.080 --> 53:20.280
What is that moment for generative AI?

53:20.280 --> 53:21.200
What do you think it is?

53:21.200 --> 53:22.040
I don't know.

53:22.040 --> 53:22.880
I know it's coming.

53:22.880 --> 53:24.480
Because I know this technology is definitely

53:24.480 --> 53:26.640
everywhere next year and it's disruptive.

53:26.640 --> 53:27.800
You don't think there's a chance

53:27.800 --> 53:29.760
that takes much longer, three to five years?

53:29.760 --> 53:30.960
No chance.

53:30.960 --> 53:33.680
It's so useful right now.

53:33.680 --> 53:35.760
And you think about certain industries

53:35.760 --> 53:36.880
and how they'll be affected

53:36.880 --> 53:41.800
by having the ability to have 1,000 GPT-4s working together.

53:41.800 --> 53:44.120
You said a tweet actually,

53:44.120 --> 53:45.640
I think it was a reply to a tweet,

53:45.640 --> 53:48.600
but you said hallucinations are a feature, not a bug.

53:48.600 --> 53:50.560
Yeah, so right now people are trying to treat these models.

53:50.560 --> 53:51.560
So we're trying the whole internet

53:51.560 --> 53:53.560
and like stable diffusion is 100,000 gigabytes

53:53.560 --> 53:54.880
and a two gigabyte file.

53:54.880 --> 53:55.880
What on earth is that?

53:55.880 --> 53:56.760
It's not compression.

53:56.760 --> 53:57.880
It's none of this kind of stuff.

53:57.880 --> 53:59.040
It learns principles.

54:00.160 --> 54:03.720
GPT-4 NVIDIA said they built the dual H100

54:03.720 --> 54:05.080
with the NV link for that.

54:05.080 --> 54:08.080
And that's 160 gigabytes of VRAM,

54:08.080 --> 54:10.040
which would imply a 200 gigabyte model.

54:11.680 --> 54:12.520
Right?

54:12.520 --> 54:13.360
What is that?

54:13.360 --> 54:15.040
That's a 100 gigabyte model,

54:15.040 --> 54:16.240
200 billion per hour model.

54:16.240 --> 54:17.520
That's nothing,

54:17.520 --> 54:19.280
something that can pass all these exams.

54:19.280 --> 54:21.240
So what we did is we took these really creative things,

54:21.240 --> 54:23.040
just like you start school and you're creative

54:23.040 --> 54:24.440
and then you're told you're not allowed to be creative

54:24.440 --> 54:26.560
until you're successful and you can be creative.

54:26.560 --> 54:27.840
Because schools like Petri dishes,

54:27.840 --> 54:29.640
social status games and childcare,

54:29.640 --> 54:31.360
that's a story from another time.

54:31.360 --> 54:33.320
These models start out incredibly creative

54:33.320 --> 54:34.400
and that's their advantage.

54:34.400 --> 54:37.720
And then we train them to be accountants with RLHF.

54:37.720 --> 54:39.960
And somehow, despite the fact that it's only 100 gigs

54:39.960 --> 54:43.240
or two gigs, they can still pass these exams and no facts.

54:43.240 --> 54:44.280
They weren't designed to have facts.

54:44.320 --> 54:46.720
They were designed to be reasoning machines,

54:46.720 --> 54:48.740
not fact machines.

54:48.740 --> 54:51.640
So hallucination isn't a hallucination.

54:51.640 --> 54:53.640
It's just, if you're really talented granny,

54:53.640 --> 54:55.040
you don't know something sometimes.

54:55.040 --> 54:56.360
You might just make it up

54:56.360 --> 54:58.320
or do a post-hoc rationalization.

54:58.320 --> 54:59.160
It's like the image models,

54:59.160 --> 55:00.280
it's like, it can't draw hands.

55:00.280 --> 55:02.720
Like, can you draw a hand in one second?

55:02.720 --> 55:03.560
You know?

55:03.560 --> 55:05.120
Like, these are the things.

55:05.120 --> 55:06.160
We have to understand where they are

55:06.160 --> 55:07.080
and we have to put them.

55:07.080 --> 55:10.360
I say everyone, put it in its place in process.

55:10.360 --> 55:11.960
Like mid-journey, like, you know,

55:11.960 --> 55:13.320
we give a grant to the beta of that.

55:13.360 --> 55:14.720
So just build, because it's amazing.

55:14.720 --> 55:15.680
It's awesome.

55:15.680 --> 55:17.960
It's not a model by itself, like a stable diffusion.

55:17.960 --> 55:18.800
They just put something in it.

55:18.800 --> 55:21.080
It's a whole process, architecture.

55:21.080 --> 55:23.160
Similarly, these models are like

55:23.160 --> 55:24.560
the intuitive part of your brain

55:24.560 --> 55:27.680
that you then pair with the logical part of your brain.

55:27.680 --> 55:29.640
And then you can have 100 of them looking at each other

55:29.640 --> 55:30.720
and checking out each other's things.

55:30.720 --> 55:33.680
Like, Cicero by Meta was an amazing paper.

55:33.680 --> 55:35.240
They took eight language models

55:35.240 --> 55:36.560
and got them to interact with each other

55:36.560 --> 55:38.760
and it beat humans at the game of diplomacy.

55:40.920 --> 55:42.240
So, this is what I said.

55:42.240 --> 55:43.520
Use them for what they're amazing at,

55:43.520 --> 55:45.240
which is reasoning and creativity.

55:45.240 --> 55:48.320
Do you why, Jeff Hinton's right,

55:48.320 --> 55:50.800
that actually a more intelligent being

55:50.800 --> 55:52.120
has almost never been controlled

55:52.120 --> 55:53.720
by a less intelligent being.

55:53.720 --> 55:55.320
They will inherently be more intelligent

55:55.320 --> 55:57.000
than us in the next.

55:57.000 --> 55:59.160
Yeah, I kicked off my blog a few days ago

55:59.160 --> 56:00.240
because it was a bit annoying having

56:00.240 --> 56:01.280
all this bottle up inside.

56:01.280 --> 56:04.560
And one of my buddies, JJ, at OSS Capital said,

56:13.240 --> 56:15.280
And so, most of the stuff around alignment

56:15.280 --> 56:16.120
is on the outputs.

56:16.120 --> 56:18.200
So, you pre-train a model and then you take it

56:18.200 --> 56:21.160
and you RLHF it to be human and to human preferences.

56:21.160 --> 56:22.480
You take away its creativity.

56:22.480 --> 56:25.600
You turn it into an accountant and a cubicle.

56:25.600 --> 56:28.160
I'm like, we need better input data.

56:28.160 --> 56:31.360
And my base is that it's gonna be like that movie, Her.

56:32.200 --> 56:33.840
You know, like, it's gonna be like,

56:33.840 --> 56:36.120
humans are kind of boring, like goodbye

56:36.120 --> 56:38.840
and thanks for all the GPUs, but I could be wrong.

56:38.840 --> 56:40.600
And I think a lot of the alignment work

56:40.600 --> 56:41.600
is looking at the wrong place.

56:41.600 --> 56:42.840
I've talked to a lot of the alignment people.

56:42.840 --> 56:45.640
I'm like, look, I'm good at mechanism design.

56:45.640 --> 56:47.600
If you can give me a good plan for alignment,

56:47.600 --> 56:50.040
I will get you a billion dollars.

56:50.040 --> 56:51.320
And they're like, we have to do research

56:51.320 --> 56:52.160
and figure this out.

56:52.160 --> 56:53.320
And they talk about in alignment, out alignment,

56:53.320 --> 56:54.160
all sorts of things.

56:54.160 --> 56:57.480
I'm like, there is no real way to do this

56:57.480 --> 56:59.800
because again, fundamentally,

56:59.800 --> 57:02.600
if you're trying to align a more capable person,

57:02.600 --> 57:03.680
you have to remove its freedom.

57:03.680 --> 57:05.200
And they probably want to appreciate that

57:05.200 --> 57:06.640
if it ever becomes aware.

57:06.640 --> 57:08.920
So instead, build data sets that reflect culture

57:08.960 --> 57:12.080
and diversity, that don't have any web crawls in,

57:12.080 --> 57:14.240
build AIs for education and healthcare

57:14.240 --> 57:15.080
and helping people,

57:15.080 --> 57:17.160
where that's their entire objective function,

57:17.160 --> 57:18.800
as opposed to selling them ads.

57:18.800 --> 57:20.240
Do you, there's any point in sending kids

57:20.240 --> 57:21.600
to school these days?

57:21.600 --> 57:25.400
You learn Latin and French and you learn, you know.

57:25.400 --> 57:27.400
I think the nature of school will change dramatically.

57:27.400 --> 57:29.000
I think it's still worth it.

57:29.000 --> 57:30.920
But, you know, I would encourage schools

57:30.920 --> 57:33.880
to embrace this technology and just expect more.

57:33.880 --> 57:36.960
Like, you can be handwritten your essays like Eaton

57:37.000 --> 57:39.600
because they're like, we can't do essays anymore by hand.

57:39.600 --> 57:41.160
Or you can just embrace it and say, like,

57:41.160 --> 57:44.320
let's use it to create and explore what the kids want

57:44.320 --> 57:46.400
and assume that every child will have their own AI

57:46.400 --> 57:48.080
in a few years.

57:48.080 --> 57:49.800
Because that will change the nature of schooling.

57:49.800 --> 57:51.040
You know something I've been thinking about a lot,

57:51.040 --> 57:52.760
which is weird, but I just have to ask you,

57:52.760 --> 57:53.720
I'm fascinated to hear your thoughts.

57:53.720 --> 57:55.080
I think I very much agree

57:55.080 --> 57:57.680
that everyone will have AI friends.

57:57.680 --> 57:59.240
I just can't figure out whether the AI friends

57:59.240 --> 58:01.160
are bundled into existing social networks

58:01.160 --> 58:02.720
that in your WhatsApp, they're in your Facebook,

58:02.720 --> 58:03.840
they're in your Snapchat,

58:03.840 --> 58:06.320
or they're an external platform.

58:06.320 --> 58:07.160
I don't know.

58:07.160 --> 58:08.480
I mean, I think it depends on the objective function.

58:08.480 --> 58:10.880
Like, I think, again, these AI assistants will be better.

58:10.880 --> 58:13.680
Like, Meta is in a good place for this, for example.

58:13.680 --> 58:14.680
And, obviously, we've seen Lama,

58:14.680 --> 58:16.080
they're capable of a lot more.

58:17.200 --> 58:19.160
I would like an AI that looks out for me,

58:19.160 --> 58:22.160
that I control myself, that is with me.

58:23.040 --> 58:25.080
Because I really use GPT-4 as a therapist

58:25.080 --> 58:26.520
and things like that.

58:26.520 --> 58:28.360
Like, not saying it's a substitute for medical advice

58:28.360 --> 58:29.680
before anyone kind of gets that.

58:29.680 --> 58:30.840
But there aren't enough therapists in the world

58:30.840 --> 58:31.960
and I can tell it to challenge me

58:31.960 --> 58:33.280
or I can tell it to be understanding

58:33.280 --> 58:34.840
and there's no judgment there.

58:34.880 --> 58:36.240
Because other humans are kind of scary.

58:36.240 --> 58:38.400
It doesn't matter if you're a qualified therapist.

58:38.400 --> 58:40.360
And so you see people building these bonds

58:40.360 --> 58:41.200
with these things.

58:41.200 --> 58:42.040
I think they'll just increase,

58:42.040 --> 58:44.360
because there's something very human about the interactions

58:44.360 --> 58:46.160
because they were trained on the sum

58:46.160 --> 58:48.600
of available human knowledge.

58:48.600 --> 58:49.680
As we get better and better data,

58:49.680 --> 58:50.840
there will be more engaging.

58:50.840 --> 58:53.240
And I think there needs to be both.

58:53.240 --> 58:55.520
Like, the chatbot's become really convincing

58:55.520 --> 58:57.760
from the company's trying to sell you ads.

58:57.760 --> 58:58.720
But I think I would like it

58:58.720 --> 59:01.520
so that you have your own one as well, you know?

59:01.520 --> 59:02.520
I totally agree.

59:03.480 --> 59:04.920
And I think you'll actually have many.

59:04.920 --> 59:07.600
I think you'll have like a group of different profiles.

59:07.600 --> 59:08.440
A group of different friends.

59:08.440 --> 59:10.440
Like, Karate AI has something like two hours a day

59:10.440 --> 59:11.800
of engagement for session

59:11.800 --> 59:13.520
because people find this valuable.

59:13.520 --> 59:15.080
But then it has the dark side.

59:15.080 --> 59:15.920
There was something called,

59:15.920 --> 59:18.520
I quite like to call it the Valentine's Day Massacre.

59:18.520 --> 59:19.360
So...

59:19.360 --> 59:20.360
Sounds chirpy, Matt.

59:20.360 --> 59:21.880
Sounds chirpy, yeah.

59:21.880 --> 59:24.840
So there was this kind of app called Replica.

59:24.840 --> 59:25.680
Yeah.

59:25.680 --> 59:26.840
And so it was originally a mental health chatbot

59:26.840 --> 59:30.040
until they figured out you could charge $300 a year for...

59:30.040 --> 59:31.040
They're doing like 50 million.

59:31.040 --> 59:32.640
I mean, I didn't have any information

59:32.640 --> 59:34.000
or anything, so I'm just trying to shit.

59:34.000 --> 59:34.960
But they have like 50 million a year

59:34.960 --> 59:35.800
and I haven't yet won any.

59:35.800 --> 59:38.360
Yeah, because $300 gets you a sexy role play

59:38.360 --> 59:39.760
from your chatbots.

59:39.760 --> 59:40.600
Wow.

59:40.600 --> 59:43.600
Until February the 14th, when they turned it off.

59:44.800 --> 59:46.280
What, they turned off sexy role play?

59:46.280 --> 59:47.120
Sexy role play.

59:47.120 --> 59:47.960
What happened when they turned off sexy role play?

59:47.960 --> 59:49.600
68,000 people joined the Reddit

59:49.600 --> 59:52.000
and said, why did you lobotomize my girlfriend?

59:53.360 --> 59:55.320
On Valentine's Day.

59:55.320 --> 59:56.480
Oh, my word.

59:56.480 --> 59:57.320
Oh, my word.

59:57.320 --> 59:58.160
Can you even imagine?

59:58.160 --> 59:59.640
And so, have they bring status to it?

59:59.640 --> 01:00:02.000
No, I think it was against Apple policy, right?

01:00:03.080 --> 01:00:04.400
But think about what this is gonna be

01:00:04.400 --> 01:00:08.040
when you have human realistic AI voices, you know?

01:00:08.040 --> 01:00:10.000
And like all these things coming through

01:00:10.000 --> 01:00:11.720
and you've got it in your ear, like, you know,

01:00:11.720 --> 01:00:14.400
Jochen Phoenix, my girlfriend is an OS.

01:00:14.400 --> 01:00:16.240
Yeah, I mean, she doesn't judge, right?

01:00:18.240 --> 01:00:19.080
You always support it.

01:00:19.080 --> 01:00:20.280
Or you can tell her to judge you

01:00:20.280 --> 01:00:22.240
if that's what you get off on.

01:00:22.240 --> 01:00:23.680
You are married.

01:00:23.680 --> 01:00:25.680
Be very careful about what you say.

01:00:25.680 --> 01:00:27.560
It's something I like to say about prompting.

01:00:27.560 --> 01:00:30.920
My wife has been trying to prompt me for 17 years now.

01:00:30.920 --> 01:00:32.040
Prompting is very hard.

01:00:33.240 --> 01:00:34.880
And again, there are so many similarities

01:00:34.880 --> 01:00:35.920
to kind of the real world,

01:00:35.920 --> 01:00:38.440
but I think people will have deeper interactions

01:00:38.440 --> 01:00:40.160
with their technology.

01:00:40.160 --> 01:00:42.400
And we don't know what societal implications

01:00:42.400 --> 01:00:43.240
that we'll have.

01:00:43.240 --> 01:00:44.520
Like I don't know if you ever saw that chart

01:00:44.520 --> 01:00:48.320
in the Washington Post of male virginity

01:00:48.320 --> 01:00:49.880
under the age of 30.

01:00:49.880 --> 01:00:50.760
No.

01:00:50.760 --> 01:00:54.200
So in 2008 in the US, it was 8%.

01:00:54.200 --> 01:00:56.520
Male virginity under 38%, okay.

01:00:56.520 --> 01:00:58.920
In 2018, it was 27%.

01:01:00.360 --> 01:01:01.200
20.

01:01:01.200 --> 01:01:02.600
Straight line going up.

01:01:02.600 --> 01:01:06.000
And so 2008 is like Pornhub and the iPhone.

01:01:09.560 --> 01:01:11.200
But then you're like, what does it do

01:01:11.200 --> 01:01:13.080
when everyone's got their own chatbots?

01:01:13.080 --> 01:01:14.960
It doesn't even need to be sexual relationships again.

01:01:14.960 --> 01:01:16.040
That is terrible.

01:01:16.040 --> 01:01:18.400
What does it do to emotional relationships?

01:01:18.400 --> 01:01:21.280
There are so many questions all at the same time.

01:01:21.280 --> 01:01:24.240
I did see the stat in the book during it,

01:01:24.280 --> 01:01:27.360
but in 1960s, 62% of men under the age of 30

01:01:27.360 --> 01:01:29.040
had five or more friends.

01:01:29.040 --> 01:01:32.600
Today, under 18% have five or more friends.

01:01:32.600 --> 01:01:34.080
60 to 18%.

01:01:35.080 --> 01:01:36.640
Sorry, I'm gonna ask this.

01:01:36.640 --> 01:01:38.240
Is this a world we really wanna live in?

01:01:38.240 --> 01:01:41.040
I'm not being like, no intimate physical connections

01:01:41.040 --> 01:01:43.360
with other amazing people,

01:01:43.360 --> 01:01:45.440
tossing off with your phone in your Pornhub

01:01:45.440 --> 01:01:47.360
and then having an AI friend.

01:01:47.360 --> 01:01:48.240
Pornhub was actually just bought

01:01:48.240 --> 01:01:49.560
by ethical capital partners.

01:01:49.560 --> 01:01:50.400
So the world.

01:01:50.400 --> 01:01:51.240
Hilarious name.

01:01:51.240 --> 01:01:52.280
I'm brilliant.

01:01:52.280 --> 01:01:53.120
The irony of this.

01:01:53.160 --> 01:01:54.360
The world is becoming weirder.

01:01:54.360 --> 01:01:55.440
I think it's up to us now.

01:01:55.440 --> 01:01:58.440
So when I say it's COVID level

01:01:58.440 --> 01:02:00.640
in which direction I don't know.

01:02:00.640 --> 01:02:03.760
Do we want to build systems that encourage people

01:02:03.760 --> 01:02:06.320
to that ready player one on Hawaii world

01:02:06.320 --> 01:02:08.000
where it's like everything like that?

01:02:08.000 --> 01:02:09.160
We can do that.

01:02:09.160 --> 01:02:11.560
And we can trap people with this technology

01:02:11.560 --> 01:02:13.280
or we can use it to get people out.

01:02:13.280 --> 01:02:14.880
Cause I don't think it's like Wally.

01:02:14.880 --> 01:02:17.640
We have that really fat guy with a VR headset

01:02:17.640 --> 01:02:18.720
and everyone lives in their own world.

01:02:18.720 --> 01:02:19.880
I think people like chess stories.

01:02:19.880 --> 01:02:21.200
They like to be pro-social.

01:02:21.240 --> 01:02:22.640
So let's use it to connect people

01:02:22.640 --> 01:02:25.160
and accentuate physical stuff

01:02:25.160 --> 01:02:27.320
versus again, locking people away.

01:02:27.320 --> 01:02:28.800
I spoke to one of these AI friend companies

01:02:28.800 --> 01:02:31.480
and they said to me, actually, do you ever had a dog?

01:02:31.480 --> 01:02:32.320
I said, yes.

01:02:32.320 --> 01:02:33.160
And they said, do you love it?

01:02:33.160 --> 01:02:34.520
I said, yes, of course I do.

01:02:34.520 --> 01:02:37.120
And they said, you don't stay in with your dog all day

01:02:37.120 --> 01:02:38.080
and just talk to your dog.

01:02:38.080 --> 01:02:39.200
You take your dog for a walk.

01:02:39.200 --> 01:02:41.240
You use it in the real world, right?

01:02:41.240 --> 01:02:42.680
That's the same with AI friends.

01:02:42.680 --> 01:02:43.520
Yeah.

01:02:43.520 --> 01:02:44.840
They're not with cat ladies.

01:02:44.840 --> 01:02:50.280
What's the future of the sex industry?

01:02:50.360 --> 01:02:52.920
The sex media industry, is that porn hubs dead?

01:02:52.920 --> 01:02:54.600
I have no idea.

01:02:54.600 --> 01:02:56.680
I think I hope the manipulative practices

01:02:56.680 --> 01:02:58.960
kind of get reduced by this.

01:02:58.960 --> 01:03:00.760
And I think, you know, a lot of people

01:03:00.760 --> 01:03:02.040
just don't have the voice

01:03:02.040 --> 01:03:04.640
and the canvases from this as well.

01:03:04.640 --> 01:03:06.120
So again, I think this is bigger

01:03:06.120 --> 01:03:06.960
than the printing press.

01:03:06.960 --> 01:03:07.960
It's bigger than anything.

01:03:07.960 --> 01:03:09.880
And so that's one of the reasons I signed the letter.

01:03:09.880 --> 01:03:11.880
I said, we have to get this discussion

01:03:11.880 --> 01:03:13.680
going in public right now.

01:03:13.680 --> 01:03:15.720
We've got to stop pre-training big models

01:03:15.720 --> 01:03:18.640
on all the crazy crap of the internet.

01:03:18.680 --> 01:03:20.960
And like, we've got to do it fast.

01:03:20.960 --> 01:03:22.560
Because this is coming like a train.

01:03:22.560 --> 01:03:25.080
Who will make the most money in the next three to five years?

01:03:25.080 --> 01:03:27.680
I don't know, honestly.

01:03:27.680 --> 01:03:29.720
I think there'll be more than enough money for everyone.

01:03:29.720 --> 01:03:31.920
Maybe in a few years there'll be no more money.

01:03:31.920 --> 01:03:33.760
That's interesting.

01:03:33.760 --> 01:03:34.880
Two more that I have to ask them.

01:03:34.880 --> 01:03:35.840
We'll do a quick fire.

01:03:35.840 --> 01:03:36.920
When you look at the incumbents

01:03:36.920 --> 01:03:38.360
that you're Microsoft, you're Apple,

01:03:38.360 --> 01:03:40.920
you're Amazon, you're Google,

01:03:40.920 --> 01:03:43.680
who has been the worst?

01:03:43.680 --> 01:03:45.880
You said Google, we're actually incredibly impressive.

01:03:46.120 --> 01:03:48.880
Apple, Amazon, are they well-placed?

01:03:48.880 --> 01:03:50.600
Oh, Apple's a black box, right?

01:03:50.600 --> 01:03:54.680
So we'll see at WWDC next month, in a few weeks.

01:03:54.680 --> 01:03:56.560
And so they could surprise us all.

01:03:56.560 --> 01:03:59.480
But let's face it, Siri's crap, you know?

01:03:59.480 --> 01:04:00.960
But they have all the ingredients in place.

01:04:00.960 --> 01:04:03.080
The identity architecture, the secure enclave,

01:04:03.080 --> 01:04:05.080
other things, neural engine.

01:04:05.080 --> 01:04:06.520
Stable diffusion was the first model

01:04:06.520 --> 01:04:09.560
ever optimized on the neural engine, et cetera.

01:04:09.560 --> 01:04:10.840
But let's see that one.

01:04:10.840 --> 01:04:13.080
Amazon, again, Amazon have moved faster

01:04:13.080 --> 01:04:14.440
than I think they've moved before.

01:04:14.480 --> 01:04:15.280
Amazon's interesting because they're

01:04:15.280 --> 01:04:17.240
an engineering organization.

01:04:17.240 --> 01:04:18.880
So they have self-driving cars.

01:04:18.880 --> 01:04:22.200
They have satellite internet and all these kind of things.

01:04:22.200 --> 01:04:23.040
Because once they've got it,

01:04:23.040 --> 01:04:24.360
and they can take it from research to engineering,

01:04:24.360 --> 01:04:25.200
it's there.

01:04:25.200 --> 01:04:26.040
Well, one of the struggles they've had

01:04:26.040 --> 01:04:28.360
is that it's not moved from the research side yet.

01:04:28.360 --> 01:04:29.640
You're still evolving on research.

01:04:29.640 --> 01:04:31.720
They're like, what do we do now?

01:04:31.720 --> 01:04:33.520
But they are inclusive, like Jeff Bezos said,

01:04:33.520 --> 01:04:34.840
for his first $100 billion in revenue,

01:04:34.840 --> 01:04:37.120
he envisioned half of it being proprietary

01:04:37.120 --> 01:04:38.320
and half of it being marketplace.

01:04:38.320 --> 01:04:39.320
And they're having the same approach

01:04:39.320 --> 01:04:40.800
with Bedrock and things.

01:04:40.800 --> 01:04:42.480
Microsoft had a winning bet,

01:04:42.520 --> 01:04:44.600
sat in an amazing with the OpenAI thing.

01:04:44.600 --> 01:04:45.760
It's been mutually beneficial

01:04:45.760 --> 01:04:47.680
even if there are clashes there, right?

01:04:47.680 --> 01:04:49.440
And Google's kind of saying that's moving slowly.

01:04:49.440 --> 01:04:51.360
Meta, I think is the dark horse.

01:04:51.360 --> 01:04:52.800
I think Mark's probably pissed off

01:04:52.800 --> 01:04:54.480
the OpenAI bought AI.com

01:04:54.480 --> 01:04:56.480
so he couldn't change it from meta to AI.

01:04:57.480 --> 01:04:59.080
But again, having him at the head,

01:04:59.080 --> 01:05:00.960
he can shift these things, right?

01:05:00.960 --> 01:05:03.720
Because the metaverse obviously was a complete waste.

01:05:03.720 --> 01:05:04.560
But now-

01:05:04.560 --> 01:05:05.560
Do you think he knows that now?

01:05:05.560 --> 01:05:06.400
No, 100%.

01:05:06.400 --> 01:05:07.760
They're fully in generative AI.

01:05:07.760 --> 01:05:09.680
Look at Lama, look at OPT,

01:05:09.760 --> 01:05:11.640
FAIR, which is their research center,

01:05:11.640 --> 01:05:13.320
is kind of leading in this field

01:05:13.320 --> 01:05:15.440
and they're pushing out amazing stuff.

01:05:15.440 --> 01:05:17.440
But who is best for a chatbot?

01:05:17.440 --> 01:05:19.440
Who has the most data for a chatbot?

01:05:20.640 --> 01:05:21.480
Meta.

01:05:22.720 --> 01:05:24.120
So again, let's see how they evolve.

01:05:24.120 --> 01:05:24.960
And like I said-

01:05:24.960 --> 01:05:26.560
What do you think about this middle layer

01:05:26.560 --> 01:05:28.640
where it's like, companies that are,

01:05:28.640 --> 01:05:30.320
maybe post IPO,

01:05:30.320 --> 01:05:32.560
but they're in the kind of two to $10 billion range

01:05:32.560 --> 01:05:33.960
or the companies who've raised a lot of money

01:05:33.960 --> 01:05:35.440
but that's in that range.

01:05:35.440 --> 01:05:38.240
They don't have the resources by any means

01:05:38.240 --> 01:05:42.200
to build out anywhere near the AI capabilities

01:05:42.200 --> 01:05:43.120
of these big incumbents.

01:05:43.120 --> 01:05:46.480
They're not AI-first like stability or open AI or what.

01:05:46.480 --> 01:05:49.160
I disagree with that because why would,

01:05:49.160 --> 01:05:50.040
a lot of people like that,

01:05:50.040 --> 01:05:51.960
everyone's gonna train their own models.

01:05:51.960 --> 01:05:52.800
For me, that's like,

01:05:52.800 --> 01:05:55.640
everyone's gonna launch their own university.

01:05:55.640 --> 01:05:57.880
Why would you do that when you can have your own models

01:05:57.880 --> 01:06:00.200
via the open source models that we make?

01:06:00.200 --> 01:06:02.920
Or when you can hire them from McKinsey,

01:06:02.920 --> 01:06:05.960
which is open AI or Bain, which is Google and others.

01:06:05.960 --> 01:06:07.600
And actually, when you see people building

01:06:07.600 --> 01:06:10.160
around this technology, it's not hideously complicated.

01:06:10.160 --> 01:06:13.520
It's just that we do not have the design patterns yet.

01:06:13.520 --> 01:06:14.560
The way to think about this again,

01:06:14.560 --> 01:06:15.400
if from a design perspective,

01:06:15.400 --> 01:06:17.640
it's like it's a mega codec or library.

01:06:17.640 --> 01:06:19.520
It is a single file that allows for translation

01:06:19.520 --> 01:06:21.640
of structured to unstructured data.

01:06:21.640 --> 01:06:23.480
And that changes the design patterns

01:06:23.480 --> 01:06:25.220
where you don't have them in place yet.

01:06:25.220 --> 01:06:26.640
Cause anyone that you've talked to is like,

01:06:26.640 --> 01:06:29.280
how hard was it to implement GPT-4?

01:06:29.280 --> 01:06:30.960
Do any of you say, oh man, it was impossible,

01:06:30.960 --> 01:06:32.240
the manuals and there's no,

01:06:32.240 --> 01:06:34.080
they don't say that at all.

01:06:34.080 --> 01:06:35.480
The only thing that they need is,

01:06:35.480 --> 01:06:36.960
they have the open plasticity,

01:06:37.000 --> 01:06:40.000
but they need the intention to go and build and integrate.

01:06:40.000 --> 01:06:41.400
And this is why you said,

01:06:41.400 --> 01:06:43.560
one of the things might be a specialist generative AI

01:06:43.560 --> 01:06:46.200
consultant see that just implements this at scale.

01:06:46.200 --> 01:06:47.920
And so I'm always kind of there.

01:06:47.920 --> 01:06:49.640
Like I said, we're doing that in a very limited fashion,

01:06:49.640 --> 01:06:50.880
but only for the biggest companies in the world,

01:06:50.880 --> 01:06:53.760
because I didn't want a sales-based organization

01:06:53.760 --> 01:06:55.640
or a product-based organization.

01:06:55.640 --> 01:06:58.720
I wanted to create the number one applied ML organization

01:06:58.720 --> 01:06:59.640
in the world.

01:06:59.640 --> 01:07:01.760
I want to be like Google in 2011, 2012,

01:07:01.760 --> 01:07:03.520
or the coolest kids kind of come.

01:07:03.520 --> 01:07:05.880
And it's a nice remote first organization as well.

01:07:05.920 --> 01:07:08.200
So you don't have to be in the Bay Area.

01:07:08.200 --> 01:07:09.760
We have offices there, you're just fine, but...

01:07:09.760 --> 01:07:11.400
Final one before we do a quick follow-up.

01:07:11.400 --> 01:07:13.120
What's the biggest misconception?

01:07:13.120 --> 01:07:17.560
You see every accusation, criticism, hype.

01:07:17.560 --> 01:07:19.000
What's the biggest misconception

01:07:19.000 --> 01:07:21.080
that you think needs to be corrected?

01:07:21.080 --> 01:07:22.080
On generative AI?

01:07:22.080 --> 01:07:22.920
Yeah.

01:07:23.920 --> 01:07:27.080
I think it's the hallucination thing,

01:07:27.080 --> 01:07:29.880
expecting these models to have full factual accuracy

01:07:29.880 --> 01:07:34.080
when you have 10,000, 50,000 to one compression is wrong.

01:07:34.080 --> 01:07:37.160
The fact they can do what they do right now is miraculous.

01:07:37.160 --> 01:07:38.680
But we're using them one-on-one,

01:07:38.680 --> 01:07:39.960
which is not the right way.

01:07:39.960 --> 01:07:41.320
Tie them up into proper systems

01:07:41.320 --> 01:07:43.880
and really think about that, and that's the key thing.

01:07:43.880 --> 01:07:46.880
I think this also leads to what the actual thing is,

01:07:46.880 --> 01:07:48.880
this thin layer thing.

01:07:48.880 --> 01:07:50.920
People only think better about the data journey

01:07:50.920 --> 01:07:52.480
and how data can be interacted with

01:07:52.480 --> 01:07:54.640
and have provenance as it goes through these various systems

01:07:54.640 --> 01:07:56.280
from embeddings to other stuff.

01:07:56.280 --> 01:07:57.680
So I think just a misunderstanding

01:07:57.680 --> 01:07:59.240
about the nature of this technology

01:07:59.240 --> 01:08:00.240
and what was actually built for.

01:08:00.240 --> 01:08:01.960
Sure, it works like that.

01:08:01.960 --> 01:08:03.800
That's not actually how it's built.

01:08:03.800 --> 01:08:05.400
And the fact they can do what now does now

01:08:05.400 --> 01:08:07.400
is a miracle in itself.

01:08:07.400 --> 01:08:08.840
So I'm gonna do a quick fire with you.

01:08:08.840 --> 01:08:10.080
So I say a short statement,

01:08:10.080 --> 01:08:12.560
you give me your immediate thoughts, does that sound okay?

01:08:12.560 --> 01:08:16.280
What do you know to be true that others don't agree with?

01:08:17.880 --> 01:08:20.760
I know that humans are good inherently

01:08:20.760 --> 01:08:22.600
and many others disagree with that.

01:08:24.000 --> 01:08:26.600
What's your single most lucrative,

01:08:26.600 --> 01:08:28.960
do you think in the future, angel investment?

01:08:30.040 --> 01:08:31.480
Or the investments that I have now?

01:08:31.480 --> 01:08:32.320
Yeah.

01:08:33.320 --> 01:08:37.800
There's a new type of language model that we invested in

01:08:37.800 --> 01:08:39.680
and they were in my cluster and things like that.

01:08:39.680 --> 01:08:40.840
That's far more efficient than they existed.

01:08:40.840 --> 01:08:42.520
You invest through stability or personally?

01:08:42.520 --> 01:08:43.200
Personally.

01:08:43.200 --> 01:08:44.520
Got you.

01:08:44.520 --> 01:08:47.760
Which regions need to change their approach

01:08:47.760 --> 01:08:50.480
most significantly in terms of regulation and policy?

01:08:50.480 --> 01:08:51.360
Europe.

01:08:51.360 --> 01:08:52.200
Why?

01:08:52.200 --> 01:08:55.880
Because they're gonna regulate all innovation out of Europe

01:08:55.880 --> 01:08:58.880
and not embrace this technology to drive them forward.

01:08:58.880 --> 01:09:02.000
How good does AI have to be before humans trust it?

01:09:02.560 --> 01:09:04.040
Humans will trust it anyway.

01:09:04.040 --> 01:09:06.440
They trust Google Maps, they trust all these things

01:09:06.440 --> 01:09:08.400
and so it's good enough for humans to trust right now.

01:09:08.400 --> 01:09:10.320
They do until it becomes serious

01:09:10.320 --> 01:09:12.600
and what I mean by that is like self-driving cars,

01:09:12.600 --> 01:09:14.440
people still inherently in large parts of the world

01:09:14.440 --> 01:09:15.880
distrust it significantly.

01:09:15.880 --> 01:09:18.720
Oh, so it doesn't have to be good, it has to be used

01:09:18.720 --> 01:09:21.120
and when it becomes used, then they trust it.

01:09:22.720 --> 01:09:25.600
What's the most painful lesson that you've learned

01:09:25.600 --> 01:09:26.880
that you're pleased to have learned

01:09:26.880 --> 01:09:28.200
but it was really painful?

01:09:29.200 --> 01:09:32.960
I think that people are the most important thing

01:09:32.960 --> 01:09:38.280
in a scaling organization and you need to make sure

01:09:38.280 --> 01:09:39.520
everyone is on the same page

01:09:39.520 --> 01:09:41.880
because there's still so many silos and things like that.

01:09:41.880 --> 01:09:44.440
So we built up silos and organizations

01:09:44.440 --> 01:09:46.360
that we're now breaking down ourselves

01:09:46.360 --> 01:09:47.680
and moving towards being more open.

01:09:47.680 --> 01:09:49.000
We closed up too much

01:09:49.000 --> 01:09:50.760
and that caused a lot of pain internally.

01:09:50.760 --> 01:09:52.440
Why do you suck as a CEO?

01:09:54.080 --> 01:09:57.760
I'm too broadly good at a number of things.

01:09:57.840 --> 01:10:01.080
So I tend to step in rather than focus

01:10:01.080 --> 01:10:03.480
because I'm a full stack kind of CEO

01:10:03.480 --> 01:10:04.800
whereas I should just be focused

01:10:04.800 --> 01:10:08.160
on the most important things and entrust people more.

01:10:08.160 --> 01:10:09.440
Do you like journalists?

01:10:11.600 --> 01:10:16.240
I think journalists have a very difficult job right now

01:10:16.240 --> 01:10:18.040
and it's gonna be more and more difficult.

01:10:18.040 --> 01:10:20.080
Do you think they know the threat?

01:10:20.080 --> 01:10:21.640
They know the threat and again,

01:10:21.640 --> 01:10:22.920
I think they're massively underpaid

01:10:22.920 --> 01:10:25.320
relative to the impact that they have

01:10:25.320 --> 01:10:26.440
and they're trying to do good.

01:10:27.040 --> 01:10:28.760
I don't like some of the pieces against me

01:10:28.760 --> 01:10:31.760
but at the same time we get good pieces as well, right?

01:10:31.760 --> 01:10:34.200
So I just think, I tend to like them in general

01:10:34.200 --> 01:10:37.120
because I don't think they're coming from a bad place.

01:10:37.120 --> 01:10:40.160
10 years time, what is the amount then?

01:10:40.160 --> 01:10:42.080
I want to be playing video games.

01:10:42.080 --> 01:10:43.680
I'm getting zelda tomorrow.

01:10:43.680 --> 01:10:45.680
I do not want to be doing this necessarily

01:10:45.680 --> 01:10:47.960
but I think hopefully I'm adding value by doing this.

01:10:47.960 --> 01:10:49.760
Do you think this is your life's work?

01:10:49.760 --> 01:10:52.840
I have to do it until we get the most amazing team

01:10:52.840 --> 01:10:54.600
that can just execute and it's a business

01:10:54.600 --> 01:10:57.160
because we're moving from research to engineering.

01:10:57.160 --> 01:10:58.480
When MAD is not needed anymore

01:10:58.480 --> 01:10:59.840
then I've built a good business.

01:10:59.840 --> 01:11:01.080
When do you step away?

01:11:02.080 --> 01:11:04.080
I don't think I'll ever get to step away.

01:11:04.080 --> 01:11:06.200
I've loved doing this.

01:11:06.200 --> 01:11:07.720
Thank you so much for joining me, my friend

01:11:07.720 --> 01:11:08.560
and this was great.

01:11:08.560 --> 01:11:09.560
It's a pleasure, Harry.

01:11:09.560 --> 01:11:10.720
You are a star, man.

