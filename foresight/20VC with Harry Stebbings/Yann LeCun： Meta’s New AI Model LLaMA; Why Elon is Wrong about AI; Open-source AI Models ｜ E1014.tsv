start	end	text
0	4960	AI is going to bring a new renaissance for humanity, a new form of enlightenment if you want,
4960	9920	because AI is going to amplify everybody's intelligence. It's like every one of us will
9920	14560	have a staff of people who are smarter than us and know most things about most topics,
14560	20240	so it's going to empower every one of us. Jan, I am so excited for this. I had so many great
20240	24240	things from our mutual friends, obviously David Marcus and then Mathieu at Photoreal Room. So
24240	29360	thank you so much for joining me today. And it's a pleasure. Now, I would love to start. I heard
29360	33840	some of the early stories, but I want to start with one from David Marcus. How did you first
33840	40080	enter the world of AI and make that first foray? I was still an undergraduate engineering student
40080	46240	in France, and I stumbled on a philosophy book, which was a debate between Jean Piaget,
46240	51600	you know, the cognitive psychologist, and Noam Chomsky, the famous linguist. And they were
51600	58000	arguing about nature versus nurture for language, whether language is acquired or innate. So Chomsky
58000	63760	was on the side of innate and Piaget on the side of acquired with, you know, some innate structure.
63760	68320	And on the side of Piaget was a guy called Seymour Papert, who was a professor at MIT.
68960	73280	In his argument, he talked about something called the perceptron, which was an early
74320	79040	machine learning system. And I read this and discovered that people had been working on
79840	85280	machines that could learn and I was fascinated and I started digging the literature, soon discovered
85280	91920	that much of that literature was in the 1950s and 60s and basically stopped in the late 60s because
91920	99600	of a book that they killed it. And Seymour Papert was a co-author of that book. So strangely enough,
99600	106400	and here it was 10 years later, actually praising the perceptron as kind of an amazing concept.
106400	111840	So I was hooked. I, you know, started getting interested in what was not yet called machine
111840	115040	learning, but eventually became neural nets and now deep learning.
115040	119120	Can I ask you, David asked this as well, how long did it take to get,
119840	124160	in terms of like the major breakthroughs, how long did it take you to get to the major breakthroughs
124160	129360	that you're at the origin of when you look back over that time to get to those major breakthroughs?
129360	134400	Well, so there's a few breakthroughs. So the first one was in the, when I was still on
134400	140240	Autobahn, basically finishing my engineering studies, I figured out that the way forward to
140240	145200	kind of lift the limitations of the old systems that were abandoned in the 60s was to find
145200	150160	learning algorithms that could train multilayer neural nets, essentially. And people had all
150160	158800	but abandoned this type of research, except for a handful of people in Japan. And one guy I heard,
159440	166160	I heard about, called Jeff Hinton, who had published a paper in 1983. So this was just
166160	171920	the year I graduated on something called the Boston machine, which was clearly a method to
171920	179120	go beyond those limitations. And so I had on my side kind of developed a method for training
179120	183520	multilayer nets, which was very close to what we now call back propagation, but not exactly the
183520	189520	same. It was closer to what we call target prop, actually, nowadays. And then, you know, published
189520	195920	a few papers in French and eventually met Jeff at a meeting in France in 1985. And we
195920	199680	realized we've been working on the same thing and we're seeking a like. And, but I was, you know,
200320	205680	in the middle of my PhD, and he was an associate professor at Carnegie Mellon. So we started,
205680	209520	you know, a discussion and then, you know, visited him at Carnegie Mellon for a summer
209520	214240	school reorganized. And then I, when I finished my PhD, I did a postdoc with him and then John
214240	219360	Bell Labs. And, and when I was in Toronto, I developed what was called convolutional nets.
219360	225600	Now, convolutional networks, which, you know, is major method for image and speech processing.
225600	232240	Now it is. And so that's, that's what I'm best known for. But, but it started much earlier.
232240	239440	I have to ask, Joshua described kind of the, the hype cycles within AI and neural nets like
239440	245760	deserts when you're not in them. And he asked the question, how did you not get discouraged
245760	250960	when for a solid decade, we were in a desert where no one really cared about neural nets?
250960	255680	How did you keep the enthusiasm bluntly when, as Joshua said, no one really cared?
256480	261920	Both Joshua and Jeff and I had in the back of our minds that those methods would eventually come to
261920	268000	the fore and that, you know, we would have to kind of snap people out of their preconceived ideas
268000	273360	about, about neural nets. So yes, there was. So you're trying, we're actually working together at
273360	279040	AT&T Bell Labs in the early 90s. And then the interest of the community for those methods
279040	285040	started waning around 1995 or so. And it was indeed about 10 years when not only nobody was
285040	289200	interested in neural nets, but people were even making fun of it, you know, talking about it in
289200	296800	sort of disparaging terms. Now there is something though, in 1996, I kind of changed job. I stayed
296800	302160	in the same company, I was still working at AT&T in the research labs, but I became a department head
302160	307040	and this was the early days of the internet. And my group and I started working on something
307040	311280	completely different that had nothing to do or not much to do, at least with machine learning,
311280	317200	this image compression. I had this, this idea that with the internet coming up, we should have a
317200	322480	way of scanning existing paper documents and then, you know, put them on the internet so that
322480	327520	everybody could, could have access to them. And so I worked on this for five or six years together
327520	332800	with Leon Botou, who had been a long, long term collaborator. Joshua was also involved
333600	339360	peripherally and a bunch of other people, Patrick Hefner, et cetera. And that project ended when
339360	345760	all of us basically left AT&T. That's when I restarted working on deep learning and Jeff also
345760	350800	kind of came back to Canada. He had been in the UK for a while and Joshua, Jeff and I decided in
350800	358000	the early 2000 to basically start a conspiracy to, you know, revive the interests of the community
358000	367200	in neural nets by making them work, discovering new algorithms. And, you know, it took almost 10
367200	374000	years, but it succeeded beyond the wildest dreams, basically. So I'm going to ask you a range of
374000	380320	varying questions in terms of depth, breadth, and kind of obvious and non-obvious. So forgive me
380320	384640	if some are obvious. I just want to ask, when I hear the historical context there from you,
384640	390480	over many decades, how do you feel today when we look at what's happening today?
390480	396880	Are we at a new inflection point in development? Or is this merely the continuation of what we've
396880	404240	seen for many decades? It's a combination of the two. So on the one hand, a lot of what we see
404240	411200	today, when you are kind of down in the trenches of research, looks at a logical extension. I was
411200	418080	not as enthralled by the sort of recent progress as the public was because, you know, I've seen
418080	421920	this progress happening over the last several years. Now, there are things that have been very
421920	428480	surprising. The fact that self-supervised learning methods applied to transformer architectures
429600	434000	work amazingly well and it worked, you know, way beyond what we could have expected. The fact that
434000	440800	we can do basically train systems to understand language, translate language in multiple languages,
440800	444720	and then continue text if you're trying them to do this, or answer questions if you're trying
444720	450800	them to do this, works amazingly well to an extent that people didn't quite expect that
450800	455440	was going to happen by just making them bigger and training them on more data. So that certainly
455440	461200	has been surprising for everybody, but that revolution occurred two years ago. Whereas the
461200	468400	wider public has learned about it through a tragedy that was made available for us. It's
468400	473840	been more continuous. And you see this in, you know, a lot of marking events in technological
473840	480160	progress or in AI, in particular, are marked by kind of splashy events that the public pays
480160	485120	attention to. But too many of us, like it looks like more like a continuous thing. And generally,
486080	492400	what those progress require is a bunch of people to take the techniques that already exist, push
492400	497040	them a little further, do a bit of engineering, and then make a demo that demonstrates that it
497040	503920	works. So that was the case for, you know, DBlue, the chess player that IBM built in the mid-90s,
503920	509760	that beat Gary Kasparov, you know, same thing with the DARPA Grand Challenge, that Sebastian
509760	515840	Sointim at Stanford won a card that could drive itself in the desert, right, for a hundred miles.
515840	520720	And then, you know, AlphaGo and, you know, the IBM Geopedy, there's a number of those things,
520720	526000	right, and, you know, which are just being the latest one. And it looks like kind of
526000	530560	jumps when you look at it from far away. But when you're in the field, it's more like a
530560	536240	continuous evolution. Can I ask, has there been any other surprising on the positive side,
536240	540480	developmental things you've seen over the last year or so? You said about self-supervised learning
540480	544400	and the efficiencies there. Is there anything else where you're like, I didn't expect it to
544400	550160	go as well as it has done in the last year? Yeah, so I already mentioned it. You know,
550240	557040	the fact that merely training a language model to predict the last word in the sequence of words,
557680	562240	if you do it properly, you get a system that has capabilities that are somewhat unexpected,
562240	567280	and they emerge as you make those systems bigger, and you train them on
568400	573280	larger amounts of data, that's clearly been the surprise for everyone.
574160	581840	Now, the thing is, you know, as researchers and scientists, we're always looking for the next
581840	586880	thing. So what I'm interested in at the moment is, you know, what goes beyond that. Like, you know,
586880	591120	a lot of people are going to work on applications of autoregressive large language models, which is
591120	595920	great. There's going to be a lot of, you know, products and new ways for people to do things,
595920	601840	and it's going to be wonderful. But I've already been thinking about the next stage for the last
602800	608960	three, four years, four, five years even, actually more, which is like, what's missing from those
608960	613440	systems? What are your thoughts on what's missing from those systems? In that logical next step,
613440	619360	why does that lead you in your thinking? So those systems do not have anywhere close to human level
619360	625840	intelligence. Okay, despite what you might think, we are kind of fooled into thinking it because
625840	631440	those systems are very fluent with language, but their ability to think, to understand how the
631440	637440	world works, to plan, are very, very limited. And they're understanding the world is very superficial.
637440	644160	And the reason for it is that they are strictly trained on language. And language only contains a
644160	649680	small proportion of all human knowledge. Most of human knowledge is not linguistic at all.
649680	654720	And all of animal knowledge is non linguistic. And we take it for granted, you know, this is the
654800	661360	Moravec paradox, right? All the capabilities and abilities that we take for granted, like, you
661360	667760	know, planning a motion or something or very simple things that everyone can do. A 10 year old can,
667760	673840	you know, clear up the dinner table and fill up the dishwasher. Any 17 year old can learn to drive.
673840	676480	We still don't have so many cars, we don't have domestic robots.
676480	682400	If they're non linguistic, like the majority, I'm sorry for the base questions, but then what are
682400	690080	they? And is that that we don't have able to be ingested by AI models and engines over time?
691040	695440	Well, so first of all, there is no question that eventually AI systems will understand the world
695440	700880	in similar ways that that humans do. There has better ways. But they will not be
701520	705680	autoregressive large language models as a type that we're now talking about.
705680	709680	There will be different for a number of different different reasons. But to answer your question
709680	716160	more directly, anything that has to do with sort of an intuition of the real world requires
716160	722640	an experience of the real world or a simulated version of it, which those large English models
722640	727440	don't have. They're purely trained from text. So you can add you, there's a number of questions
727440	731920	that about the physical world that they'll be able to answer because there's a template for it in the
732560	736400	or something very similar in the data that they've been trained on. Same for planning,
736400	742880	you can ask them to plan a trip or something and they'll adapt a template that they've been trained
742880	748160	on. But they don't really have sort of a model of a mental model of how the world works and allows
748160	753680	them to plan complex action sequences or use tools or things like that. Can I ask, is that why you
753680	760160	said that AI researchers face palm when they hear prophecies of doom? No, that's a different question.
760160	764080	Those are kind of orthogonal concepts. So I mean, there is some some weak connection.
764720	771920	There is a a flaw in a current autoreversive lens, which is that you can only control their answer
773040	779200	in two ways. The first way is you modify the statistics of the training data that you train
779200	785280	them on, possibly using human feedback for specific answers. And the second one is you
785280	790560	change the point and the combination of the point that, you know, the question you ask them,
790560	795680	the form in which you ask the question and the statistics of the training data entirely determines
795680	803280	the answer to the system we produce. So there is no persistent memory, first of all. But second of all,
804160	810080	you cannot control the system. You cannot impose constraints on it, like be factual, be understandable
810080	814640	by a certain year old. You can try to put this in a prompt, but then, you know, you rely on
815200	820400	whether the statistics of the training data is appropriate for for taking taking that into account.
820400	825680	There's no direct way to constrain the answer of those systems to satisfy certain objectives.
825680	831360	And that makes them very difficult to to control and steer. And so that creates some fears because
831360	835680	people are kind of extrapolating. If we let those systems do whatever we connect them to
835680	840240	internet and they can do whatever they want, they're going to do crazy things and stupid things
840240	844320	and perhaps dangerous things. And we're not going to be able to control them. And they're going to
844320	847680	escape of control. And they're going to become intelligent just because they're bigger, right?
848560	853760	And that's nonsense. First of all, because this is not the type of system that we are going to give
853760	859680	agency to the systems that will eventually be given agency that are going to be able to plan
859680	863680	sequences of actions, our systems are going to have objectives that they're going to have to
863680	868400	satisfy. And because of those objectives, they're going to be controllable. So they're going to be
868400	872960	much more controllable than the current systems. Okay, so my prediction is that within a few years,
873520	879040	nobody in their right mind would use autoregressive LMS, they'll go away in favor of something more
879840	884240	sophisticated and controllable, they can plan its answer as opposed to just produce one order
884240	889440	after the other, reactively. Okay, that's that's the first fallacy. The second fallacy is that
889440	898480	there is this idea somehow that the desire to and the ability to dominate is linked with intelligence,
898480	903440	right? So this is a statement that a lot of people are are making, including, you know,
903440	909040	my friend, Jeffington recently, that somehow as soon as the machine becomes intelligent,
909040	914640	it becomes uncontrollable because, you know, it's it being smarter than us, it can influence us in
914640	922560	ways that we can even imagine. Now, I think this is a gigantic fallacy, because even within the
922560	930160	human species, it is not the smartest among us that want to dominate the others. Okay, to dominate
930160	936320	other entities, you don't necessarily need to be smarter than them, we need to want to dominate
936320	943120	them. This is not something that every intelligent entity is going is going to do spontaneously.
943920	950720	We do it as humans, because the desire to influence others was built into us by evolution,
950720	957360	because we are a social species. Okay, same as baboons and chimpanzees and wolves and dogs and
957360	961680	etc. It's not the case for orangutans. orangutans don't have the desire to dominate anybody,
961680	967120	because they are non-social animals, they are solitary animals, they are territorial, in fact.
967120	975360	So, we need to separate those two concepts, the will, the desire and the ability to dominate on
975360	979760	one hand and intelligence on the other hand. The fact that we're going to have super-intelligent
979760	986400	machines at our disposal means that every one of us is going to be like a business leader,
986400	990720	a politician or an academic with a staff of people working for them that are more intelligent than
990720	999360	themselves. I mean, it's great. It's not like if you feel threatened by being the boss of other
999360	1003360	people who work with you, but are smarter than you, you're not being a good leader.
1004160	1009760	Can I ask you, how do we instill values within models where they don't have a desire to dominate?
1010560	1014480	Right, so these objectives I was telling you about. So, okay, let me describe the
1015360	1019440	architecture of future AI systems as I see it. We're going to have AI systems that
1020160	1026400	basically are going to plan their actions and actions can include sequences of words that you
1026400	1032880	tell someone, but they're going to plan the sequence of actions or words so as to optimize
1032880	1039520	a series of objectives that we set them. So, one objective is, does this answer the question I just
1039520	1044640	asked? Another objective might be, well, you're talking to a 13-year-old, make that answer
1044640	1051680	understandable by a 13-year-old. Another objective might be, I asked you to answer a question about
1051680	1058880	the world, so be factual. Or it's a question about yesterday's political event. Can you kind of be
1058880	1063920	compatible with everything you've read in the press this morning? Things like that, right?
1065200	1071440	So, you'll have those systems that have a series of objectives and their output, their answer,
1072320	1079680	by construction, is going to have to satisfy those objectives. And some of those objectives
1079680	1086880	will be hardwired to make those systems safe. Like, if it's a domestic robot that can cook dinner
1086880	1091760	and can wield a kitchen knife in its arm, there's going to be a term in there that says,
1091760	1095600	like, stop moving your arm when there's people around because you might hurt them. So, that's
1095600	1099920	going to be an objective that the system cannot violate because by construction, we're going to
1100000	1106160	have to satisfy them. So, that's the way to build safe AI system. You make them produce answers that,
1106160	1110240	by construction, have to satisfy objectives. And you design those objectives so that their
1110240	1116400	actions are safe. Now, how precisely to do this is not a completely solved question, but you try it,
1116400	1121520	you deploy it at a small scale, you see what the effect is, and you correct it when it doesn't work,
1121520	1127840	and you fix it progressively. And it's not like if you get it wrong, it's going to destroy humanity.
1128400	1135120	That depends on that cooking robot, you never know. How do you determine who's able to set
1135120	1139520	the objectives? Because there could be right or wrong depending on who sets them.
1140160	1146640	That's true. So, that's going to have to be a process by which we allow people to do this,
1146640	1152320	some vetting process. The same way that there's a vetting process for people to take care of
1153040	1159600	your health or cut your hair, fix your plumbing or your car, right? So, there's some vetting
1159600	1166240	process, certainly some testing and market deployment procedure with regulating agencies
1166240	1171360	for things that are potentially dangerous, probably not for all applications, but for
1171360	1176160	many applications, certainly in healthcare transportation and things like that. And then
1176240	1183040	perhaps also, it could be that, let's take the example of intelligent assistance. So,
1183040	1190480	let's imagine a future where everyone can talk to their intelligent assistant. That system will
1190480	1195280	have pretty close to human-level intelligence for probably more accumulated knowledge than most
1195280	1201200	humans. They could translate in any language and give you a quick summary of yesterday's
1201200	1206240	newspaper and things like that, right? Explain mathematical concepts to you, things like that.
1206240	1212400	So, people are probably going to use this almost exclusively in the future for their interaction
1212400	1216720	with the digital world. You're not going to go to Google or Wikipedia, you're just going to talk
1216720	1222560	to your assistant. And the only way to do this properly is for the base infrastructure for those
1222560	1228960	assistants. I mean, they would be so pervasive, so much will ride on those systems that I don't
1228960	1237760	think anyone will accept that those assistants being behind an event horizon in a private company.
1237760	1242240	They will insist that the infrastructure is open. They will insist also that the vetting
1242240	1247520	process by which those systems are trained be something maybe like Wikipedia, right?
1247520	1251920	We tend to trust Wikipedia, sometimes with a grain of salt, but we tend to trust Wikipedia
1251920	1256400	because there is a vetting process so that whenever an article is modified, you know,
1256400	1260800	some editor kind of check on it and then the changes are accepted or not, things like that.
1260800	1266320	So, you can imagine that the sort of common repository of all human knowledge that would be
1266320	1271040	our assistants will be constructed through some sort of cross-sourcing process, perhaps similar
1271040	1275600	to Wikipedia, where you're going to have a bunch of people training those systems and fine-tuning
1275600	1280000	them so that, you know, whatever they and so they produced are correct.
1280000	1285760	It's so funny you say about that kind of the benefit set of the open approach over the closed
1285760	1289920	approach because that's where I've been kind of stuck, which is like, where does value accrue?
1289920	1294400	Is it to the closed model or the open model? And then we had the leaked internal Mamo stay
1294400	1299200	from the Google employee who said, you know, we're not ahead, open AI are not ahead, there's this
1299200	1306480	third being which is actually far more significant and we haven't taken notice of and summarized.
1306480	1312080	And that was triggered, that was triggered by by Lama, which is the model that was put together
1312080	1319600	by my colleagues at FAIR, which was the code was open sourced. The model sadly
1321040	1324480	was distributed only for research and non-commercial purpose.
1325840	1334000	And the reason for that is basically complicated legal issues of what's the status of the data
1334000	1337600	that the system has been trained on and things like that. So, it's more kind of,
1338560	1344800	it's not a lack of desire from the from meta to open source. It's more kind of complex legal
1344800	1353600	issues that go beyond my. I'm super naive, Jan. Why does open win against a more controlled,
1353600	1361360	tight-knit, well-funded open AI or other large corporate with a big balance sheet and a very
1361440	1365920	rigorous but streamlined team? It's very simple. It's because no outfit
1367040	1373040	as powerful as they may be has a monopoly on good ideas. So, if you do it in the open,
1373840	1380480	you basically recruit the entire world's intelligence to contribute to things and
1380480	1386400	having ideas and ideas that you met as, you know, sorry about, which, you know, an outfit with 400
1386400	1392240	people has no chance thinking about or even a large company with 50,000 employees may not want to
1392240	1398880	devote any resources to because they may not think it's useful in the long term or they have,
1398880	1404560	you know, more urgency to take care of. So, you give it away and then you have, you know,
1404560	1408160	tons and tons of people, some of whom are, you know, undergraduate students or people,
1408160	1413120	you know, in their parents' basement. So, coming up with amazing ideas that you would
1413120	1418480	never have thought about or willing to spend the time to crunch down the, you know,
1418480	1425760	7 billion weight llama so that it runs on the Mac, on the laptop. Like, oh, that's pretty amazing.
1425760	1433280	So, I think that's why, you know, open-source projects succeed, particularly when they concern
1433280	1438880	basic infrastructure. So, if you think about it, the early days of the Internet, there was a battle
1438880	1443040	between Microsoft and SunMicroSystems to provide the basic infrastructure for the Internet.
1444080	1448000	You know, the operating system, the web server, you know, things like that, right?
1449920	1454880	So, on SunMicroSystems, it was Solaris and, you know, whatever web server and Java,
1454880	1459680	and then on the Microsoft side, there was Windows with IIT or whatever, you know,
1459680	1466720	an ASP, which was their kind of server and client-side protocol. Both of them lost.
1467360	1472320	In fact, SunMicroSystem pretty much went bankrupt and was, you know, sold for parts to Oracle.
1472320	1477360	One was Linux and Apache, which is completely open-source. And you might ask, why? You know,
1477360	1482240	the entire Internet and the entire tech industry runs on Linux, right? And your phone probably
1482240	1487440	runs on Linux, too, if you have Android. So, that's three-quarter of the phones in, you know, in the
1487440	1493040	world. So, the reason for this is that, you know, it's just a much better way of gathering
1493040	1500000	competence and talent around a common project, even if it's not motivated necessarily by profit.
1500800	1508080	Yeah, I agree, and I love this. You work with matter. My question and David Marcus' question was,
1508080	1509520	how does matter win, then?
1510480	1516720	So, it's been the case that Meta in the past has open-source pretty much everybody,
1516720	1522880	everything that it's ever produced in terms of basic infrastructure, right? So, you have, you know,
1522960	1530480	React for, you know, the framework for web and mobile apps. You have PyTorch. PyTorch is not even
1530480	1533600	owned by Meta anymore. The ownership was transferred to the Linux Foundation,
1534320	1539120	because it's so essential to the, you know, AI, R&D infrastructure nowadays. You know,
1539120	1545280	ChatGPT was developed on PyTorch. Okay. All open AI runs on PyTorch. The entire world, in fact,
1545280	1551360	runs on PyTorch except Google, because they have their own team, right? But it goes beyond that,
1551360	1558400	right? Meta open-sources its hardware server backplane design, so that hardware manufacturers can
1558400	1564480	build to its specifications. And pretty much everything, aside from sort of legal issues that
1564480	1570240	are sometimes due to kind of recent laws or court decisions, pretty much everything has been
1570240	1575920	open-sourced. It is not because other people can use your technology that you can't exploit it
1575920	1583200	to the same extent, right? Who can use smart NLP systems for, you know,
1583200	1588560	translation or content moderation on Facebook, other than Facebook? It doesn't matter if other
1588560	1593520	people have access to the same technology. I mean, I totally agree with you. And this kind of led
1593520	1598240	to my next question, which you actually tweaked about, which comes to the size of like data modes
1598240	1604560	and size of data availability. Is it simply a case that the largest model wins? And how do you think
1604560	1611760	about value in small models as well? Yeah, so it's not the case. This is really what
1612720	1617600	Lama has demonstrated and really kind of shown people. So the people behind Lama,
1617600	1622400	Edouard Grave and Guillaume Lompland and their collaborators, mostly at Fair Paris,
1622400	1626960	actually many of them are in Paris, they've demonstrated that you don't need those models
1626960	1632480	to be very large to work really well. I think it caused a bit of an epiphany for a lot of people,
1633120	1637840	realizing, oh, you know, you don't need, okay, maybe you need a thousand GPUs,
1637840	1642640	you know, running for 10, you know, a couple of weeks to train it, the base system. In fact,
1642640	1647600	this, that number is going down too, because people are kind of figuring out how to do this
1647600	1651760	more efficiently. But once it's pre-trained, you can use it for all kinds of stuff and you can
1651760	1657920	fine tune it really easily. And, and then at the end, you can run it on your laptop, right?
1657920	1663680	That's kind of amazing. Or maybe on a, on a, you know, desktop machine with a GPU in it or a
1663680	1669680	couple GPUs. So I think, you know, it sort of opened the minds of people to the fact that there is
1669680	1674240	like enormous opportunities that really weren't thought to be possible before. And I think it's
1674240	1679840	going to make even more progress, because if we go towards the design of AI systems, perhaps along
1679840	1684880	the lines of what I described with objectives and planning, I think those systems could actually be
1684880	1691120	even smaller to some extent. How would they be even smaller? Sorry, unpack that for me.
1691120	1696720	Well, because the current models, for them to work here, to train them on gigantic amounts of data,
1697600	1701120	way more data than any humans has ever been trained on, right? So the amount of data I
1701120	1710400	lamar is trained on, for example, is something like 1.4 trillion tokens, which is a, you know,
1710400	1715120	it's like a quarter of the internet or something. It's something absolutely enormous. It would take
1715120	1721520	someone reading eight hours a day at normal speed, about 22,000 years to read through that. Okay.
1721520	1727440	So obviously, those systems can accumulate a lot of knowledge from text, but they don't do it the
1727440	1731600	same way humans do it, because we don't need that much time to be that smart and to learn
1732640	1737280	that much. So obviously, we are much more efficient or brains are much more efficient than those models
1738000	1742800	at learning things. Like, how is it that a teenager can learn to drive a car in about 20
1742800	1747360	hours of practice? We still don't have level five sort of cars. So obviously, we're missing
1747360	1752960	something really big. And what we're missing, I think, is abilities for AI systems to learn
1752960	1758880	how the world works by observation mostly, and then this ability to plan so as to satisfy objectives.
1759680	1766240	And then beyond that, the ability to set some objectives in the satisfaction of a bigger one.
1766320	1771920	Okay, let's go to hierarchical planning. And we do this, humans do this. Some animals do this to
1771920	1776960	some extent. Every animal, you know, mammal and bird is capable of some level of planning.
1776960	1780720	Autoverseveral animals basically don't do planning or a very simple form of it.
1781440	1786160	Jan, you mentioned the efficiency that can come from actually smaller models than expected
1786160	1791600	and how actually size of models isn't everything. The other thing, we spoke about open and closed.
1791600	1794560	The other thing that I've been thinking, and everyone's been thinking about, and I've interviewed
1794560	1801040	many kind of leading AI experts, and they say the value will accrue to the incumbents. Startups,
1801040	1804720	they don't have the data, they don't have the models, it'll accrue to the incumbents.
1805360	1809680	Is that right? Will the value accrue to the incumbents? Or do you believe that given what
1809680	1815120	you just said about size not being everything in terms of models, it could be startups as well?
1816480	1822960	So it depends on which scenario you believe in. So the scenario I think will happen,
1823040	1826800	and I'm certainly rooting for, is the scenario I described earlier where you have some sort of
1826800	1833040	open platform for base LLMs. So base LLMs basically would be seen as the basic infrastructure,
1834160	1842160	like TCP, IP, Linux, Apache, essentially, completely open. And then there would be an ecosystem of
1842160	1846240	companies building stuff on top of it, which for vertical applications for specific things,
1846240	1850800	right, to specialize those systems for particular applications, who offer support to make it,
1851360	1858240	customize for enterprise applications, for personal things. There'll be a whole economy
1858240	1862800	around this, which will create jobs, by the way, not make them disappear. So this is the scenario
1862800	1868080	that I believe will happen. And the reason I think it will happen is because there is essentially a
1868080	1874160	need to use, essentially millions of contributions for making those systems factual and correct,
1874160	1879680	and etc., so Wikipedia style. So I think the proprietary approaches will actually fall behind.
1879680	1884880	So that's one point. The second point is, you can ask yourself the question, how is it that
1885680	1892240	the companies that were best positioned to produce something that charge EPT, namely Google
1892240	1902320	and Meta, didn't? Why is it open AI? The small ad sheet was 400 people, more now, but actually
1902320	1908720	small ad sheet. And the answer is, it's not because Google or Meta did not have the competence of
1908720	1916560	the technology. It's just that they didn't have the pressure to produce new products, completely
1916560	1923200	new products, that had a lot of risk attached to them. And the risks were, we know where the risks
1923200	1929680	are, because a few weeks before charge EPT, my colleagues at fair, produced a large language
1929680	1934800	model called Galactica, which was an experimental system. And they put out a demo, and the demo
1934800	1941440	was to demonstrate that. So Galactica was a large language model trained on the entirety
1941440	1946640	of the scientific literature. And it was basically designed to help scientists write papers.
1947360	1951920	So you would start writing a paragraph or something like that to describe the topic of
1951920	1956400	paragraphs. And then Galactica would basically complete the paragraph, and it wouldn't be
1956400	1962400	factually correct. You would have to kind of fix it, but you would ask it to build a table
1962400	1968160	result, and it would just put the latech commands to kind of build the thing and populate it with
1968160	1973040	the known results on the literature about the topic that you're working on, or you would type a
1973040	1977920	chemical formula for something, and it would turn it into an actual name for it, or things of that
1977920	1983360	type. Very useful for scientists. As soon as the demo was put out, it was murdered by the
1984000	1989840	social network Twitter sphere. Why? People said, oh, this is going to destroy scientific
1989840	1996720	publication, because now any random person can write an authoritatively sounding scientific
1996720	2003920	paper that is nonsense. And there was so much material thrown at the system that the people
2003920	2009280	at Meta who built it couldn't take it, they took down the demo because they said, we can't sleep
2009280	2013920	at night. So here is an example of a very useful system, a system that could have been extremely
2013920	2019680	useful, particularly for writers or scientific papers who are non-native English speakers,
2020640	2028400	that basically was destroyed by AI do-mers. People who just did not think about the risk-benefit
2028400	2035840	analysis, the risk of flooding the literature with nonsense is ridiculous. I mean, because, you know,
2035840	2043200	the scientific publications are vetted and things like that, so there was not a significant danger.
2044880	2049360	And then Chatchitviti came two weeks later and was welcomed as the second
2049360	2056560	coming of the Messiah, right? So what does that tell you? And then, you know, a few months later,
2056560	2064160	Google came out with a bard, and in the demo, a bard made a tiny, you know, minor factual mistake
2064160	2071520	about some astronomical fact, and, you know, Google's stock went down by 8%. Now, what it tells
2071520	2076880	you is that when something is produced by a large company that has a reputation, particularly
2076880	2081520	a reputation to defend, they can put out things that's true nonsense, but it's okay for a small
2081520	2089680	company. So that's the landscape of what happens now, which is why I think there's a bit of a
2089680	2094400	paradox which is that the companies that have, you know, the best technology basically can't
2094400	2100640	have difficulties putting it out because of those legal issues and sort of public image.
2101520	2104720	Do you not also think there's this core business model challenge there, which is
2104720	2108720	it's the classic innovators dilemma? Like why didn't Google do this? Because it would have killed
2108720	2115040	that absolute cash cow of Google ads. The cost to service a query versus the costs of this
2115040	2122400	is so significantly different. You'd be killing your core cash cow with this, with unknown upside,
2123360	2130160	versus retaining what is a great business. You don't have a choice. I mean, there's no question
2130240	2137200	that within some time, it could take a while, but there is no question that people interact
2137200	2144480	mostly with the digital world using AI assistance. And they may run into your augmented reality
2144480	2153920	glasses or something of that type. Like in the Spike Jon's movie, Her, that's not a bad depiction
2153920	2160080	of what, you know, the way things could develop. And so if you take the assumption, make the assumption
2160080	2165600	this is going to happen, you have to build it as quickly as you can. And it might cannibalize
2165600	2170320	your news feed algorithm or whatever, or because of Google, your search engine,
2171120	2177040	but you have to do it. You know, it's like, I mean, meta has been known to make those choices
2177040	2186080	in the past, like the move to mobile, for example, and the move to short form video, for example,
2186080	2192640	which obviously TikTok has been very successful at. Meta has entered that business in kind of a
2193440	2198320	big way, despite the fact that the amount of revenue it derives from it is lower than a traditional
2198320	2205200	news feed, because it's hard to put ads and videos basically. You mentioned the job creation
2205200	2210400	element there. I do just want to touch on the job side, because it's the classic AI doomer that's
2210400	2214160	we're all going to be unemployed, and we're going to have universal basic income in an
2214160	2219760	optimistic world. You said about job creation there. We don't hear about job creation through AI.
2219760	2225520	How do you see what jobs will be created through this new ecosystem and what that world of employment
2225520	2234160	could look like? So 100 years ago, or maybe 120 years ago, most people in most of the world
2234880	2242720	worked in the fields in food production. There's pretty much a majority of the population.
2243680	2246960	Today, in developed countries, it's between one and two percent.
2249600	2258400	And that has caused a migration of people into the cities and the development of service, business,
2259200	2265040	you know, the same thing 20 years ago or 20, 30 years ago, there was a big movement towards
2265040	2270560	automation of manufacturing. And a lot of manufacturing jobs disappeared in developed
2270560	2274960	countries, but they were replaced by other things. So 20 years ago, who would have thought that you
2274960	2282320	could make a living with a podcast? I didn't think I could five years ago, Jan. I'm as surprised as
2282320	2289440	everyone else. Right. So, you know, a lot of jobs appear like, you know, 30 years ago, there was no
2289440	2294080	such thing as web designer. And now it's, you know, have engineers in the world basically do this,
2294080	2299120	right? So, you know, the number of economists that I have talked to, which is pretty large,
2299120	2303600	about where I asked that question, we tell me, well, we're going to run out of jobs because,
2303600	2309440	you know, we're all going to be replaced by, I think, is exactly zero. Like, no economics believes
2309440	2314000	this. No economics believes we're going to run out of job because no economics believes that we're
2314000	2319040	going to run out of problems to solve or requirement for human creativity and human
2319040	2323360	communication and stuff like that. So, you know, this is going to create as many jobs as it makes
2323360	2327520	disappear. Now, the question is, though, and those jobs, by the way, are going to be more
2327520	2332240	productive. So overall, technology makes people more productive. In other words, for the same
2332240	2340560	amount of hours worked, you produce more wealth, okay? But every technological revolution, unless
2340560	2347360	it's accompanied by sort of, you know, political changes and social changes, generally profit
2347360	2353360	a small number of people, at least temporarily, right? That happened in the industrial revolution
2353360	2357440	in the late 19th century, where, you know, a few people became extremely rich and a lot of people
2357440	2361760	were exploited. And then, you know, society changed. And there were like social programs and,
2362320	2368240	and, you know, income tax and, and high tax for richer people and stuff like that, which the U.S.
2368240	2373920	has backpedaled on this, but not Europe, or the UK to some extent, too, but not the rest of Europe.
2375040	2378640	So there is a question of, you know, how you distribute the wealth if you want, okay? How
2378640	2382720	do you organize society so everyone profits from it? But that's a political question. There's not
2382720	2387360	a technology question. It's not new. It's not caused by AI. It's just caused by
2387360	2391440	technological evolution, right? It's not a recent, a recent phenomenon.
2391440	2396080	This is so unfair of me to ask. But what do those jobs look like? Like, what are they?
2396080	2401120	Are they, they're creative oriented? But what does that actually mean? Like, sorry, I know it's a
2401120	2404800	really hard question, but I'm just trying to understand how, how we actually spend our time
2404800	2410800	in my children, which I don't have, by the way, Jan. But what, what do they do? Like sculpt or paint?
2411040	2411520	I don't know.
2413440	2418000	I don't know. That's a good question. But it's not because I don't know that it won't happen,
2418000	2423920	because I mean, look at like how many people exercise their creative juices today, right?
2423920	2429440	With all the tools that are available that, you know, weren't available 10, 20 or 30 years ago,
2429440	2433360	like 3D artists or something like this, you know, game designers, you know, all kinds of things.
2433360	2437760	Like, you know, I think creative jobs are the other ones. So there are two types of jobs that
2437760	2442400	that, you know, have a bright future of creative jobs, whether they are scientific, technical,
2442400	2447520	educational, or artistic. ACI has to do with communication, right? And communication of
2447520	2452480	human emotions, which is, you know, intrinsically human, if you want. So that's one category.
2452480	2457280	And then the other one is personal services. So where you need, you know, actual people
2458080	2458880	to interact with you.
2459680	2465120	I totally agree and get you. And I love, I love that we shall see class. The only thing that I
2465120	2469840	worry about is like the speed of transition. Like when you look at past industrial revolution,
2469840	2473760	when you think even the introduction of PCs into kind of, you know, working environments,
2473760	2481040	these were multi decade introductions. Blundly, what AI feels like in some industries today,
2481040	2486800	we use it at the media company, and it's cutting our employee like the speed of transition is much,
2486800	2493440	much more compressed in this timeline, which will lead to short term significant high unemployment.
2493440	2496160	Do you concede that or do you not concede that?
2496160	2502080	So this is something I used to be very worried about, that the speed of progress of technology
2502080	2508160	was going to leave a certain number of people behind who, you know, cannot be basically retrained
2508160	2513680	fast enough or maybe they're too old to retrain themselves for the new, the new world. I was
2513680	2517760	worried about this. And then I talked to a bunch of economists and they say, oh, you know, not
2517760	2525040	really because the speed at which a technology disseminate in the economy is actually limited
2525680	2531280	by how fast people can learn to use it. So a good person to talk to about this is Eric
2531280	2537280	Binobsen at Stanford. And what he says is that when a new technology is introduced, let's say the
2537280	2542320	PC, right, with, you know, graphical user interface, the mouse, et cetera, right, in the mid 90s,
2543280	2550720	how long did it take to have a measurable effect on productivity, you know, which is the amount
2550720	2556000	of wealth produced by per hour worked. He says, you know, typically it's 15, 20 years. And the
2556000	2560080	reason is that that's what it takes for people to learn to use that new technology basically.
2560640	2565280	But you buy that here, like people are pretty good at prompts, you know, social media content
2565280	2570640	managers are using prompts very efficiently to produce content plans, to create content ideas in
2571440	2577440	under half an hour after watching a couple of TikToks. Yeah. But like, what is going to be the
2577440	2583840	effect of this on, first of all, on measurable productivity, second of all, on the the job
2583840	2588480	market, like, is it going to make people lose their job like right away? And no, it's going to take
2588480	2592480	a while. It's going to take 10, 15 years, you know, possibly more. It depends when you start
2592480	2596800	counting, right, because the AI revolution maybe started 10 years ago. So if you start counting
2596800	2601200	then, then it might only take, you know, another 10 years. But you know, I mean, I don't think you
2601200	2607920	want to underestimate the degree of conservativeness of the business world, right? Things tend to
2607920	2613920	change not that quickly. But if it's that easy to learn, like people will learn it and then invent
2613920	2620720	new professions out of it, or become more productive themselves. Why do you think we love the doom,
2620720	2625760	Jan? You know, I love your approach in mindset, and I agree with it. But why do you think we are
2625760	2629440	kind of magnetized to like, oh, we're all going to be unemployed in the doom?
2630960	2635680	Well, because I think for a number of reasons, so I'm not a, you know, social psychologist or
2635680	2642800	sociologist, but but clearly, I think we're hardwired to pay attention to things that occur or may
2642800	2648320	occur that could be dangerous to us. Because it means that there's something about the world that
2648320	2652800	we don't completely understand, and we do have to pay attention to it and be careful about it.
2652800	2658320	So for example, take a young, a young child, five months old, and show a scenario to this
2658320	2663280	small child of a little car that is sitting on the platform, and then you push the car off
2663280	2668320	the platform and instead of falling, the car appears to float in the air. The five months old
2668320	2673680	will barely pay attention to it. But if you show this to a 10 months old, the 10 months old will
2674480	2680080	look at it with huge eyes and stare at it for a long time, wondering what's going on. Because in
2680080	2685360	the meantime, babies around the age of, you know, between, between six and nine months learn about
2685360	2689760	gravity. They learn that objects that are not supported are supposed to fall. And so the mental
2689760	2694560	model is that an object is not supported to fall. And they see this object that appears to float in
2694560	2699920	the air. And they say, like, this can be like, you know, there's something I didn't, I didn't,
2699920	2704240	I don't understand about the world, I need to look at this and investigate. Okay, so we're hardwired
2704240	2708560	for this, because that's the way we learn our internal mental model of the world that allows
2708640	2713840	us to predict what's going to happen, allows us to plan. That's what makes us smart. That's the
2713840	2719600	basis of intelligence, the ability to predict. And so we naturally pay attention to stuff that
2719600	2728720	is surprising, or dangerous, or both, which is why, you know, you see a outrageous piece of news,
2728720	2734480	you know, a clickbait at the bottom of some, you know, website. And like, you have to convince
2734560	2739760	yourself not to click on it. Can I ask you a couple of direct questions? I'm just too
2739760	2745360	interested and we can take them out if needed. What did you say to Jeff when you heard that he
2745360	2749760	was obviously making the moves that he did? I'm sure you had a conversation with him. What did
2749760	2757040	you say to him? We haven't spoken yet, actually. We're going to speak to kind of get, you know,
2757040	2763360	each other's opinion on it. I don't think he knows my opinion on this, because I don't think he
2763360	2767200	follows, you know, what I post on Twitter or whatever, even though he is on Twitter himself.
2767200	2771840	But so I think we have, you know, a discussion to have. I've had this discussion before with
2771840	2780240	Yoshua Bengio, but not with Jeff. And to me, the fact that he left Google is not particularly a
2780240	2788400	surprise. The fact that he leaves Google to be able to speak his mind, I think is not surprising.
2788400	2794720	So I have a very different deal at Nitta, which is that I say whatever I want. Okay. I'm not under
2794720	2802640	the tight control of, you know, the communications department or anything. I just say what I think.
2802640	2810720	All right. How did you get that deal? Yeah. But no, seriously, many of my friends at Mesa,
2810720	2816000	in very high positions, as you know, with mutual friends, they don't have that deal.
2816160	2822960	So there is, I mean, I mean, a particularly sweet spot because I have a
2823920	2831280	quite a bit of following people who trust me or believe me or want to hear what I have to say,
2831280	2838240	even if they don't trust me at all. And at the same time, I'm not an officer. So I,
2838240	2842880	it's not like, you know, there are things I can't say because of legal issues of, you know,
2842880	2847760	financial blah, blah, blah, right. I'm a vice president, but I'm just below the level where
2847760	2852160	you had to be really, really careful and so control your message. And I think there is
2852160	2861360	a cost-benefit tradeoff here of, you know, AI is such a complicated, fast-evolving issue that
2861360	2867600	you basically, you need someone to be able to, you know, speak freely. And I think Jeff didn't
2867600	2873920	feel like he had that option at Google, maybe, you know, for various reasons. So I understand why
2874800	2880240	he might have wanted to leave, but I don't, I don't agree with him at all with the whole
2880240	2884320	sort of, you know, probability of human extinction or whatever.
2884320	2889120	Have you ever felt your role at Mesa has impeded your ability to be impartial?
2889760	2895120	I don't believe so, no. But I mean, there are certain things that I would post on social media
2895120	2900640	that are kind of, you know, kind of popping up the work of my colleagues. And, you know,
2900640	2904160	I'm obviously biased about this because, you know, I know about the work and they are friends
2904160	2909840	and colleagues. And, you know, I think it's interesting probably because, you know, I feel
2909840	2914240	the part of it. I totally agree. For this kind of stuff, I might be biased. Take this with a
2914240	2918240	grain of salt. You don't have to believe me. You know, things like that. But it's given me
2918240	2924320	a vision also of, you know, how things are built, what the problems are. So, you know, for example,
2924560	2932800	there's a narrative, a very, very common narrative that AI is the culprit for a lot of the bad side
2932800	2939760	effects of social networks in the past. And in fact, it's completely backwards. AI is the solution
2939760	2946720	to those problems. So, you know, let me tell you, you know, go back like, you know, backpedal
2946720	2950800	12 years ago or something, you know, even before I joined META, where META, you know, started
2950800	2956400	experimenting with the newsfeed. And the newsfeed was, you know, an algorithm that would pick,
2956400	2961520	like, which piece of news to show to everyone. And, you know, originally it was decided by, you
2961520	2966080	know, how friends are you with a person making the post and things like that, right? How many
2966080	2970240	interactions you have with that person. Eventually, a bit of machine learning was put into it
2970240	2974640	shortly before I joined META. It was very, very simple. It was something like logistic regression,
2974640	2979920	something like the simplest method you can imagine, with a lot of engineering behind it and a lot of,
2979920	2984400	you know, hacks by hand and special cases. But basically, it was something like logistic
2984400	2988640	regression, you know, some big vector that describes, you know, what you click on, like,
2988640	2992880	how many times you, you know, how much time you spent on a particular piece of content and blah,
2992880	2998160	blah, blah. And then it would decide, like, you know, give a rating to everything. So,
2998160	3004800	that was deployed. And people ended up spending more time on Facebook. But then also, it created
3004800	3009120	problems that were quickly identified, like, you know, like information bubbles in the context
3009120	3017600	of political discourse. And the fact that what I was talking about earlier, that people tend to
3017600	3023200	click on things that is more interesting, right? So, it caused, you know, the appearance of
3023200	3027600	clickbait companies that, because you were just like farms of, you know, teenagers in
3028400	3033280	Montenegro or someplace, making false news to get people to click on them and get money from
3033280	3037920	the ads that they show them. So, then, you know, this was realized there were, like, big groups
3037920	3043280	that at Facebook at the time kind of studying the, where the effect of those things are,
3043280	3047360	and this was corrected. So, that's the way you, you make some work, right? You,
3048720	3052480	you try the most small scale, you see what the effect is, if there is bad side effect,
3052480	3056720	you correct it, and then you sort of compare, you know, to two systems. And then sometimes,
3056720	3061120	something unexpected occurs and you have to back that all and completely change the way you do
3061120	3065360	things. That's what happened in 2017 after the presidential election, American presidential
3065360	3070320	election in 2016. The main newsy algorithm was completely changed, so that, you know,
3070320	3074960	there was no clickbaits anymore. There was no, like, you know, news outlets that could, like,
3074960	3079360	push their content that was propaganda, basically, you know, much more effort to take down false
3079360	3084880	accounts and attempts to corrupt the democratic system and stuff like that, right? So, you,
3084880	3089520	you correct it. And then what the progress of AI over the last few years basically allowed
3090320	3094720	systems to be deployed to do things like taking, take down hate speech relatively,
3096080	3100880	reliably, in hundreds of different languages, which was basically impossible to do before.
3101520	3104960	You mentioned correct it. I promised last question, then we'll do a quick fire. You mentioned
3104960	3110160	correct it. Elon Musk said with Tucker Carlson, the trouble with AI is you can't release and then
3110160	3115600	correct. Unlike all prior technological developments, once released, it is too powerful
3115600	3121840	to be able to bring back into the box. It cannot be amended in that way. Is that not true?
3122480	3129040	That's not true. That's completely false. It makes an assumption which Elon and some other people
3129040	3135200	may have become convinced of by reading, you know, Nick Boxtron's book, Super Intelligence, or,
3135920	3143120	or reading, you know, some of Elias O'Yudkowski's writing. So this is predicated on an assumption
3143120	3149280	that is just false, which is the existence of a heart takeoff. Right. So the fact that
3150000	3154400	the minute you turn on a super intelligent AI system is going to take over the world,
3155520	3161280	and it's going to escape your control, and it's going to refine itself to be even more intelligent.
3161280	3167920	And so, you know, and the world would be destroyed. And that's just ridiculous. It's just completely
3167920	3176720	ridiculous because there is no process in the real world that is exponential for very long.
3178800	3182880	You know, those systems will have to, like, recruit all the resources in the world.
3182880	3191040	They would have to be given, you know, limitless power agency. Like, why would we do this? And
3191040	3196080	what's more, they would have to be built so that they have a desire to take over. Like,
3196160	3199280	you know, systems are not going to take over just because they are intelligent.
3199920	3204480	Because again, you know, in, even within the human species, it is not the most intelligent
3204480	3211200	among us that want to dominate others. So his desire and many other leaders desire to prevent
3211760	3216320	any further development and to regulate intensely right now and stop all progression
3216960	3224480	is BS, basically. It's obscurantism. Yeah. Right. It's like, it's like people who wanted to
3224480	3229920	stop the printing press and the diffusion of printed books because, you know, if people could
3229920	3234000	read the Bible for themselves, they wouldn't have to talk to priests anymore and then would have
3234000	3239600	their own idea about religion. And that's exactly what happened. People read the Bible for themselves
3239600	3244720	and that created the Protestant movement in Europe. And that created 200 years of religious conflicts.
3244720	3252080	But it also brought to us the enlightenment, science, rationalism, philosophy, ideas of democracy,
3252080	3256800	and then the French and American revolutions. And then, you know, you can compare this with the
3256800	3261440	Ottoman Empire, which for reasons of being able to control their population, you know,
3261440	3267200	basically stopped, forbid the use of the printing press. And it started 300 years of decline.
3267760	3271520	They were dominating science in the Middle Ages, the Muslim world,
3271520	3274400	which is why, you know, every star in the sky has an Arabic name.
3275520	3279200	I love this. I'm going to do a quick fire around with you now. So I say a short statement and
3279200	3282880	you give me your immediate thoughts and then we'll rock and roll. Does that sound okay?
3282880	3289920	Sounds good. So which regions most need to change their modus operandi when it comes to the practice
3289920	3299200	of scientific research and incentive mechanisms? Which region? Oh, wow. Pretty much every region.
3299840	3307520	I'm afraid, but for different reasons. So you saw with China. So China has a bit of an epidemic
3308400	3313680	of bad science. There's a lot of very smart people in China, a lot of very good researchers,
3313680	3317280	a lot of very good work coming out of China, particularly in AI, particularly in computer
3317280	3322960	vision. But a lot of absolutely terrible work that has to be retracted a few months later,
3322960	3329440	it's been published. And it's partly because of the incentive mechanisms in the academic and
3330160	3335280	system in China. So this is important to fix there. I can move to Europe. So
3336560	3342400	in Europe, there are good things. So the education system for like undergrad rates,
3342400	3348480	education in Europe is great. It's fantastic, because it's party free. So that allows
3349280	3354400	talented people to go to the schools, even if they are not rich, right?
3355360	3358720	Which is not the case in the US, for example, at least not to the same extent.
3358720	3363920	That's good for Europe. A lot of European engineers and scientists are great,
3363920	3367760	atop base in the world. But then what are the opportunities for people who want to
3368400	3375200	go into science and research? And most European countries actually don't have systems that
3375200	3381280	really encourage this and motivate the most talented people and students to go into science.
3381280	3388560	And so some of them go to North America like me 35 years ago. There are opportunities now
3388720	3397280	that are really good in research labs like fair in Paris, or Google also has labs in Paris.
3397280	3402480	Actually, my brother works at Google in Paris. So there are other outfits. So that gives
3402480	3408080	opportunities for people who really want to be productive and don't think that they can
3408080	3413440	in the public research and academic system in France and the rest of Europe.
3414400	3420080	The only European countries that can rival the US in terms of the quality of
3420640	3422880	job for an academic or a scientist is Switzerland.
3424000	3428800	What do you think they do to rival that? What is it about their incentive
3428800	3431680	mechanism structure that gives them that ability?
3432400	3439920	Two things. They pay people better. Second thing is they give them resources for research.
3439920	3443840	They can get extra resources through grants and stuff like that, but they're good. And then
3443840	3448320	they also attract some of the best students in the world. So you get the ideal combination that
3448320	3452560	you only get in the top 30 universities in North America.
3452560	3457280	So we've got China, we've got Europe. What about the US? What could they do differently or improve?
3457840	3463840	Well, so there's a lot that the US does right in terms of research, which is to a large extent
3464800	3471680	a bit of a partial explanation for the success of the technological industry, the tech industry
3471680	3479440	in the US. I think partly because the US devotes a significant amount of resources to
3479440	3486720	fundamental research through NSF and NIH and various other outfits, probably more than Europe.
3486720	3493440	Universities pays their faculty pretty well, particularly in areas like computer science and AI.
3493440	3498880	Now this comes with a downside. The downside is that studying in the US is expensive.
3499680	3502400	It's a trade-off, right? So can you do one without the other?
3502400	3507280	Switzerland figured out how to pay academics pretty well while actually
3507280	3511680	offering free education to their students. So there is a way to do it.
3512240	3514960	Canada also figured out a pretty good trade-off as well.
3514960	3521440	So in other things, the US does right. But one thing that the US system or like the rough does
3521440	3527680	right also is the willingness to take risk and invest on ideas that seem a little crazy,
3527680	3535520	but basically the sort of vibrant startup scene in Silicon Valley and other places in the US,
3535520	3544240	in New York, and in the Boston area is leading the world. Now you start seeing a similar thing in
3544240	3551120	Europe now. There's been like enormous growth, for example, of tech startups in Paris,
3551120	3557600	Paris area, in France more generally, and continental Europe, more widely in the UK as well.
3557600	3563200	And so I think that's a good thing, but it's still a little more difficult to have access
3563200	3565760	to investment money in Europe than it is in the US.
3565760	3571920	That's why I'm here, Jan. I'm happy to provide it. I'm going to do an ultimate one for you.
3571920	3575040	When you think about what you'd most like someone listening to take away,
3575600	3580400	what would it be when they hear this? What do you want them to take away as the number one thing?
3581200	3590160	AI is going to bring a new renaissance for humanity, a new kind of new form of enlightenment,
3590160	3597280	if you want, because AI is going to amplify everybody's intelligence. It's like every one
3597360	3603520	of us will have a staff of people who are smarter than us and know most things about most things
3603520	3608480	and most topics. So it's going to empower every one of us. It's going to make us more creative
3608480	3615840	because we'll be able to produce text, art, music, videos without necessarily having all the
3615840	3621680	technical skills that are currently required for doing those things. And so exercise our creative
3621680	3626880	juices. So that's the positive side. There are risks. There's no question. But it's not like
3626880	3631520	those risks. Don't believe the people who tell you that those risks are inevitable or that they
3631520	3638960	will inevitably lead to catastrophe. That's just not true. It's like, place yourself in 1920. Who
3638960	3646000	would have thought that a mere 50 years later, you could cross the Atlantic in a few hours in
3646000	3654480	complete safety at near the speed of sound? Would people seriously want to ban aviation
3654480	3660160	or call for regulation of jet engines before jet engines existed? I mean, that's kind of insane.
3660960	3666080	So I'm not against regulation. There should be regulation of AI products, particularly the ones
3666080	3671360	that involve making critical decisions for people. But regulating or slowing down research is
3671360	3677520	complete nonsense. It's just obscurantism. Who's incumbent team do you most respect and admire
3677520	3685200	when you look at Amazon, Facebook, Google, in terms of their approach and talent internally?
3685200	3690720	Outside of meta, obviously. So this is changing a lot. And the reason it's changing is because
3691280	3695600	a lot of people are leaving large companies and large labs. And the reason they're doing this is
3695600	3701120	that until recently, a lot of AI research was very exploratory. And now there's a path towards
3701120	3706320	commercialization for a lot of things. And so people think that they're better off just leaving
3706320	3710000	large companies and doing this on their own, doing a startup and things like that. So you see
3711280	3717200	a relatively large motion of applied research engineers, a few scientists
3718800	3724720	basically leaving those labs to do startups. And that's across the board. So you look at the
3724720	3730720	original paper from Google about BERT or Transformers. The thing that revolutionized
3730720	3736960	NLP, all of them have left. Okay, they're only startups. Some of the people who produced LANA,
3736960	3744320	the open source LLMs from meta. So the key people have left already, okay, to do startups.
3744320	3749600	I saw their companies. Yeah, there's one called Mistral.
3749600	3755120	That's the one I saw. Yeah. Right. But they, yeah, there is insane amount of money in dates.
3755680	3762960	Yeah. Yeah. So, you know, we're proud of them. But I mean, I'm sad that they'd have to, I mean,
3762960	3767200	I would just say, but he's an existing team. Me, like, yeah, they're good.
3767840	3774720	Yeah. So, but I think like in terms of the basic competence and the people who are going to push
3775520	3780960	the science forward, because what we need now is not to work on applications with LLMs. There's
3780960	3784640	a lot of people who are capable of doing this and they're going to do the job. What we need to do
3785440	3790480	people like me who are, you know, reworking on research is kind of coming up with new concepts
3790480	3795360	that will allow us to, you know, get machines that basically have common sense, have an experience
3795360	3801120	of the real world, have, you know, basically human level intelligence, right? And, you know,
3801120	3805760	in my opinion, the, the outfits that are best positioned for this are fair from on one side.
3806800	3810560	And the new deep mind now, which is, you know, deep mind plus group of brain.
3810560	3813680	Yeah. There are a lot of people there who are interested in that question. And I think they
3813680	3819760	are probably the best together with Mita and fair. They're the best positions to position,
3819760	3824480	to really kind of have an impact on this, something, you know, all of us have been working on for,
3824480	3825440	for quite a while.
3825440	3831760	Jan, if we do this again in 10 years time, where is Jan in 10 years time in 2033?
3832320	3839200	Well, I'm 63. So, you know, 10 years on now, well, 12 years on now, I'll be, I'll be Jeff
3839200	3849520	into the age. Okay. And I don't know. I think I'm excited like, like a teenager now,
3850240	3856640	because I see the opportunity of like the next step in AI and opportunity perhaps to,
3857520	3862320	you know, get to the goal that I set myself so that I imagined for myself when I started
3862320	3868800	working on AI many years ago, which of course I was very naive about at the time of understanding
3868800	3873600	intelligence, first of all, and it's a scientific question. What is intelligence? What is human
3873600	3878240	intelligence? And one good way as an engineer, a good way to understand intelligence is to
3878240	3884800	build a widget that actually reproduces it, right, to some extent. So I'm excited about this right
3884800	3895520	now. I'll find the, you know, the, the, the, the substrate, the landscape, the, the location,
3895520	3903600	the position where I can make the, the best contributions to this. And currently that just
3903600	3913120	happens to be, to be fair at Meta. I keep a foot in academia because I think it's very
3913120	3917520	complementary and also important. There are projects of different types that you do in
3917520	3922240	academia and industry that are complementary. So I like the, the combination of the two.
3923200	3928160	As long as my brain keeps working, that I think I can contribute and that I've given,
3928160	3933200	I've given the means to contribute, I'll keep, I'll keep working. And then there's some point
3933200	3939920	where my brain will turn into white sauce or, or I'm totally, you know, out of it or something
3939920	3945920	and I'll stop. Yeah. And I want to say personally, thank you so much. I speak for many, I'm sure,
3945920	3950480	when I say we've learned so much from you in terms of your public speaking and discourse and
3950560	3954560	willing to speak. I think few are willing to speak as openly as you have been. So
3954560	3958560	thank you for educating so many of us. And thank you so much for joining me today, Ann.
3959200	3962160	Well, thank you so much for having me, Harry. This was, this was fun.
