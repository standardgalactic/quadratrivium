1
00:00:00,000 --> 00:00:03,520
Hello, everyone. Welcome to the Mindscape Podcast. I'm your host, Sean Carroll.

2
00:00:04,240 --> 00:00:09,840
When you think about how we conceptualize human beings, someone once pointed out that

3
00:00:09,840 --> 00:00:16,240
we're always using metaphors that depend on our current best technologies, you know, when clocks

4
00:00:16,240 --> 00:00:21,840
were just invented, wristwatches and so forth. It was the clockwork universe when robots and

5
00:00:21,840 --> 00:00:27,520
machines came on the scene. We thought of organic beings kind of like that. And now we have computers,

6
00:00:28,080 --> 00:00:34,480
besides which we have, you know, cameras and video cameras and audio recorders and so forth.

7
00:00:34,480 --> 00:00:41,280
So we tend, very, very roughly, you know, we tend to think about a person as kind of like

8
00:00:41,280 --> 00:00:49,600
a robot with some video cameras for eyes and audio recorders for ears hooked up to a computer

9
00:00:49,600 --> 00:00:56,320
inside. And the sensory apparatus brings information into the computer, which then tells

10
00:00:56,320 --> 00:01:03,200
the robot body what to do. It's a simple, kind of straightforward, compelling picture. It's also

11
00:01:03,200 --> 00:01:10,000
wrong. That's not actually a very good description of what we are, how we behave. For one thing,

12
00:01:10,000 --> 00:01:15,920
intelligent design is not the way that human beings came about. We evolved over many, many years,

13
00:01:15,920 --> 00:01:21,120
and we weren't aiming for that. We have to think about what is the kind of architecture that actually

14
00:01:21,120 --> 00:01:27,280
best serves the purposes of surviving and procreating and reproductive fitness and so forth.

15
00:01:27,280 --> 00:01:32,080
And it turns out to be very different. So today's guest is Andy Clark, who is a philosopher and a

16
00:01:32,080 --> 00:01:37,840
cognitive scientist. In fact, his title at the University of Sussex is Professor of Cognitive

17
00:01:37,840 --> 00:01:44,000
Philosophy, very well known in philosophy, very, very highly cited for thinking about the brain

18
00:01:44,000 --> 00:01:49,040
and the mind and how they're related and how they work. He became very famous with a co-author

19
00:01:49,040 --> 00:01:55,520
paper with David Chalmers, where they proposed the extended mind hypothesis, the idea that what

20
00:01:55,520 --> 00:02:01,760
you should count as your mind is not just your brain, but also all the little extensions of

21
00:02:01,760 --> 00:02:07,200
the brain that help us think, whether it's inside our bodies, or whether it is things we scribble

22
00:02:07,200 --> 00:02:12,400
down on a piece of paper or used to enhance our memories or calculation abilities, and so forth

23
00:02:12,480 --> 00:02:19,440
and so on. He also has a great interest in the idea of the brain as a predictive machine. And

24
00:02:19,440 --> 00:02:26,560
that is the subject of his new book, The Experience Machine, How Our Minds Predict and Shape Reality.

25
00:02:26,560 --> 00:02:32,880
So the idea here is that the brain is not a computer just bringing in sense data and then

26
00:02:32,880 --> 00:02:38,960
thinking about it. Our brains are constantly constructing a set of predictions for what's

27
00:02:38,960 --> 00:02:43,840
going to happen next, what is going to be the situation in which the body finds itself, what

28
00:02:43,840 --> 00:02:49,040
is the sensory data that we're going to bring in, and then you compare what you're actually

29
00:02:49,040 --> 00:02:55,120
experiencing versus what the brain was predicting, and you try to play the game of minimizing the

30
00:02:55,120 --> 00:03:00,240
error between what you predicted and what you're actually perceiving. This sounds like maybe a

31
00:03:00,240 --> 00:03:05,840
small change of emphasis or an angle on a similar kind of process, rather than the brain just being

32
00:03:05,840 --> 00:03:11,600
a passive receptor of information. It is sort of actively engaged in a feedback loop, but it has

33
00:03:11,600 --> 00:03:16,960
very, very significant consequences for how we think about thinking, how we think about fixing

34
00:03:16,960 --> 00:03:22,560
thinking, right? When we go wrong one way or the other, whether it's being in pain or having a mental

35
00:03:22,560 --> 00:03:28,400
disorder of some sort, how do we get better at it, taking seriously how the brain is a prediction

36
00:03:28,400 --> 00:03:33,120
machine is very useful here, as well as for philosophical problems about how you carve up

37
00:03:33,120 --> 00:03:37,360
nature, how you think about what the brain does, what is consciousness, what is free will,

38
00:03:37,360 --> 00:03:41,840
and so forth. So this is one of those podcasts that touches on many issues that we're interested in

39
00:03:41,840 --> 00:03:47,920
here at Mindscape, from consciousness to time all over the place. And occasional reminders,

40
00:03:47,920 --> 00:03:55,840
you can support Mindscape by pledging at Patreon, go to patreon.com slash Sean M. Carroll, and pledge

41
00:03:55,840 --> 00:04:01,200
a dollar or two or whatever you like for each episode in return, sense of fulfillment, but of

42
00:04:01,200 --> 00:04:06,800
course also add free versions of the podcast, as well as the ability to ask questions for the

43
00:04:06,800 --> 00:04:12,960
Ask Me Anything episodes. You can also, if you're interested, leave reviews of Mindscape at Apple

44
00:04:12,960 --> 00:04:18,800
Podcasts or wherever there are reviews. Those reviews help draw other listeners in. So if you

45
00:04:18,800 --> 00:04:23,280
think Mindscape's worth listening to, make it an even bigger community listening, and that would be

46
00:04:23,280 --> 00:04:26,160
awesome for all of us concerned. So with that, let's go.

47
00:04:41,680 --> 00:04:46,320
Andy Clark, welcome to the Mindscape podcast. Hey, it's great to be here. Thanks for having me.

48
00:04:46,320 --> 00:04:51,360
You know, you've done, as many people have of a certain age, many interesting things over your

49
00:04:51,360 --> 00:04:58,240
career. Your new book is The Prediction... What is your new book? What is the title?

50
00:04:58,240 --> 00:05:02,320
It's The Experience Machine. The Experience Machine. I keep wanting to say The Prediction Machine,

51
00:05:02,320 --> 00:05:06,960
because obviously prediction is playing a big role there. The Experience Machine with how predictions

52
00:05:06,960 --> 00:05:13,600
shape and build reality. Good, good. Yeah, reality will be a theme that we want to get to, but I can't

53
00:05:14,240 --> 00:05:19,680
give up the opportunity to also talk about extended Mind and extended Cognition and things like that.

54
00:05:19,680 --> 00:05:25,680
So I thought it would make sense to talk extended Mind first, and then get into prediction. Does

55
00:05:25,680 --> 00:05:30,000
that make sense logically to you? I think that's a good route. That's certainly that was my route,

56
00:05:30,000 --> 00:05:37,520
so why not? Good, good. Let's do it. So apparently there are people out there who think that, well,

57
00:05:37,520 --> 00:05:42,480
there are people who think that the Mind is not even related to the Brain, which is funny to me,

58
00:05:42,480 --> 00:05:48,240
but there are other people like yourself, perhaps, who think that the Brain is just one little part

59
00:05:48,240 --> 00:05:53,200
of our minds and our thinking. So I'm not going to put words into your mouth. What does it mean

60
00:05:53,200 --> 00:05:59,520
to talk about the extended Mind? Yeah, I mean, this is a view that I've kind of held and defended

61
00:05:59,520 --> 00:06:05,200
for many years. It goes back to a piece of work that I did with David Chalmers, who's famous for

62
00:06:05,200 --> 00:06:13,120
his kind of almost dualistic views on consciousness after all, many years ago, back in 1998. The basic

63
00:06:13,120 --> 00:06:20,320
idea that Chalmers and I agreed on is that when it comes to unconscious cognition, then there's

64
00:06:20,320 --> 00:06:26,880
no reason to think that the Brain is the limit of the machinery that can count as part of an

65
00:06:26,880 --> 00:06:33,200
individual's cognitive processing, where that has to include unconscious processing, because it's

66
00:06:33,200 --> 00:06:39,280
unconscious processing that we think mostly is what gets extended. There's a whole other debate

67
00:06:39,360 --> 00:06:45,520
about conscious processing. And the idea there is that moment by moment, the Brain doesn't really

68
00:06:45,520 --> 00:06:52,400
care where information is stored. It cares about what information can be accessed, how fluidly

69
00:06:53,520 --> 00:06:57,600
you can get at it, whether or not you've got some idea that it's there to be

70
00:06:57,600 --> 00:07:05,680
got at at all. So the idea was that calls to biological memory and calls to external stores,

71
00:07:05,680 --> 00:07:11,200
like a notebook or currently nowadays, maybe a smartphone or something like that,

72
00:07:11,200 --> 00:07:17,520
are working in fundamentally the same kind of way. And actually, interestingly, it's that that I

73
00:07:17,520 --> 00:07:24,000
think the predictive processing story ends up cashing out in an interesting way. So we can

74
00:07:24,000 --> 00:07:31,200
circle back around to that later. That's the core idea is that the machinery of mind doesn't all

75
00:07:31,200 --> 00:07:36,320
have to be in the head. David Cholmers, of course, another former Mindscape podcast guest. So we

76
00:07:36,320 --> 00:07:42,640
have an illustrious alumni base. But let's let's make it a little bit more concrete. What do we

77
00:07:42,640 --> 00:07:48,560
mean? You know, I think that what immediately comes to mind is I can remember almost no phone

78
00:07:48,560 --> 00:07:52,640
numbers now, because they're all in my smart phone. Does that count as extended mind?

79
00:07:53,280 --> 00:08:00,160
That counts. I think it's after all, you have a fluent ability to access at least a functional

80
00:08:00,240 --> 00:08:06,160
equivalent of that information as and when you need it, because, you know, you just pick up the

81
00:08:06,160 --> 00:08:12,000
phone and there it goes. If on the other hand, you had all those numbers stored in a notebook,

82
00:08:12,000 --> 00:08:17,920
but the notebook was in your basement. And, you know, you had to run down into the basement to get

83
00:08:17,920 --> 00:08:24,400
it whenever you needed it, that wouldn't count because you wouldn't have the constant robust

84
00:08:24,400 --> 00:08:31,200
availability of that resource woven into the way that you go about every kind of problem that

85
00:08:31,200 --> 00:08:36,960
daily life throws at you. So there is, I think, a genuine intuition, which is that whatever the

86
00:08:36,960 --> 00:08:42,320
machinery of mind is, it better be more or less portable. It better more or less kind of be going

87
00:08:42,320 --> 00:08:46,960
around the world where you're going around the world, where bio you is going around the world.

88
00:08:47,920 --> 00:08:54,880
And so for that reason, I think you need robust and trusted kinds of access, but only to the

89
00:08:54,880 --> 00:09:00,960
extent that biological stuff is robust and trusted. You know, now and again, my biological memory goes

90
00:09:00,960 --> 00:09:06,080
down. Now and again, I don't trust what it throws at, but it should be more or less in the same ball

91
00:09:06,080 --> 00:09:13,200
part. Well, okay, so robust and trusted access. Is that the criterion? I mean, I guess an immediate

92
00:09:13,280 --> 00:09:17,840
question that comes to mind is, where do I draw the boundary? Does everything in the world count

93
00:09:17,840 --> 00:09:24,800
as my mind? Yeah, I mean, it's a, it's one of the original worries about this view. It's one that

94
00:09:24,800 --> 00:09:31,040
we used to call, or I used to call cognitive bloat, why it somehow is no good way to stop in this

95
00:09:31,040 --> 00:09:37,600
process. But actually, if you take that robust availability and trust of the product stuff

96
00:09:37,600 --> 00:09:42,720
reasonably seriously, that does rule an awful lot of things out. But I think in addition,

97
00:09:42,720 --> 00:09:48,080
there's something which I've never, I've never given a full account of, to be honest, but there's

98
00:09:48,080 --> 00:09:55,440
something about the sort of delicate temporal nature of the integration of the biological system

99
00:09:55,440 --> 00:10:02,240
with this non biological crop or aid or resource or whatever it is. So that you can have the brain,

100
00:10:02,240 --> 00:10:08,960
if you like, making calls to this stuff in ways where it kind of knows about the temporality of

101
00:10:08,960 --> 00:10:13,600
that, it doesn't all have to go through a little bottleneck of attention, you don't have to kind

102
00:10:13,600 --> 00:10:17,680
of think to yourself, oh, I'd better get that information from over there. That's not what

103
00:10:17,680 --> 00:10:23,680
it feels like to go about your daily things as you. So it's this sort of idea of something where,

104
00:10:23,680 --> 00:10:28,240
if it were taken away suddenly, you would feel very much at the loss of a sort of

105
00:10:28,880 --> 00:10:33,600
kind of general purpose loss. There'll be many situations that you'd find yourself in

106
00:10:34,240 --> 00:10:38,240
where you were no longer able to perform in the ways that you expected yourself to perform.

107
00:10:39,360 --> 00:10:43,440
There's something there that could do with a bit more unpacking about temporal dovetailing.

108
00:10:44,000 --> 00:10:50,000
Does it relate to ongoing conversations that I'm very interested in about emergence and strong

109
00:10:50,000 --> 00:10:56,240
and weak emergence or just being able to coarse grain in the world in such a way that you have

110
00:10:56,240 --> 00:11:01,840
units that have explanatory power? Yes, I think it certainly relates to a lot of stuff about

111
00:11:01,840 --> 00:11:08,400
emergence because the idea here would be that by dovetailing things together in this way,

112
00:11:08,400 --> 00:11:14,400
you get basically an emergent you. You are the emergent property. You, the thing that goes around

113
00:11:15,280 --> 00:11:22,640
with a sort of sense of its own capacities, but doesn't really care how they get cashed out.

114
00:11:22,640 --> 00:11:27,600
So something that I think comes out quite strongly in the predictive processing stories

115
00:11:28,320 --> 00:11:34,800
is that brains are kind of location neutral when they estimate where they can get good

116
00:11:34,800 --> 00:11:39,600
information back from. So they're very good at estimating what they're uncertain about and

117
00:11:39,600 --> 00:11:44,480
going about getting more stuff, but it doesn't really matter where that stuff is. So as long as

118
00:11:44,480 --> 00:11:51,280
it's accessible, trusted, there when you want it, that's fine. And then there'll be a story to tell

119
00:11:51,280 --> 00:11:57,360
about how the actual selection process operates, but we'll come around to that once we talk about

120
00:11:57,360 --> 00:12:03,040
predictive processing. Let's stick with TXM. I mean, I guess one way that philosophers think

121
00:12:03,040 --> 00:12:09,120
about their job is that they're supposed to be carving nature at the joints. And does this

122
00:12:09,120 --> 00:12:15,600
count? Is that this? Is it a sort of betrayal of that goal because you're including all my

123
00:12:15,600 --> 00:12:21,280
smartphones and my card catalog as part of my mind? Or is it an improvement of that because

124
00:12:21,280 --> 00:12:28,880
really the action of our brains relies on all this stuff? Yeah, that's a very good and very

125
00:12:28,880 --> 00:12:34,800
delicate kind of question. So, you know, when when those charmers and I put it forward, we tried to

126
00:12:34,800 --> 00:12:41,280
argue that this was, this was where the right way to carve nature at the joints because of

127
00:12:41,280 --> 00:12:48,480
functional similarities. That said, you know, it doesn't intuitively always seem like the right

128
00:12:48,480 --> 00:12:53,840
way to carve nature at the joints. So I think that we're making a decision here, and that that

129
00:12:53,840 --> 00:13:02,160
decision, or the enough is in part a moral decision. So, okay, I, I'm very much moved by the analogy

130
00:13:02,160 --> 00:13:07,600
with prosthetic limbs, for example, if you took somebody with a well fitted prosthetic limb,

131
00:13:07,600 --> 00:13:13,680
and then looked at them and said, but you know, your basic physical capacities are just those of

132
00:13:13,680 --> 00:13:18,960
you without the limb, we would in a way be doing them a certain kind of injustice. And I think we

133
00:13:18,960 --> 00:13:24,640
be doing ourselves a certain kind of injustice. If we treated people say with mild dementia that

134
00:13:24,640 --> 00:13:31,760
rely very much on that on a smartphone or on their home environment, in just that kind of way, we

135
00:13:31,760 --> 00:13:37,920
would be, we would be sort of regarding them as less, I think, than they really are. So for me,

136
00:13:37,920 --> 00:13:45,840
the sort of non moral route, the kind of functional similarity route is a kind of stalemate, because

137
00:13:45,920 --> 00:13:50,400
it's kind of carving nature at the joints, and it kind of isn't. And then it's when you chop the

138
00:13:50,400 --> 00:13:55,760
moral or ethical considerations into the pot, that I think you have a strong argument that

139
00:13:56,400 --> 00:14:02,800
overall, the best way to go is to err on the side of caution, and buy into the picture of the extended

140
00:14:02,800 --> 00:14:06,560
mind. I bet that depends on one's definition of caution. But

141
00:14:06,880 --> 00:14:13,280
I think, all right, is there intermediate, there probably is an intermediate stance in which

142
00:14:14,000 --> 00:14:20,720
your whole body, your physical body counts as your, what goes into cognition and the mind,

143
00:14:20,720 --> 00:14:27,920
but things outside the body don't. Is that a well populated space in the thoughts about this?

144
00:14:28,640 --> 00:14:34,160
Yes, I think that is reasonably well populated. I personally, I think it's an unstable space.

145
00:14:34,160 --> 00:14:39,520
So I think you better go one way or the other. I think that, as soon as you start thinking, okay,

146
00:14:39,520 --> 00:14:44,880
if I'm using my fingers to help me count, then my fingers somehow count as part of the machinery

147
00:14:44,880 --> 00:14:51,520
of mind. A lot of people do feel that, but I think that's true. But I think you can't think that,

148
00:14:51,520 --> 00:14:57,360
and then simultaneously, you think that the constantly carried pocket calculator can't count

149
00:14:57,360 --> 00:15:03,600
as part of the machinery. So yes, I think the body is often, for me, it's a very useful step in

150
00:15:03,600 --> 00:15:10,080
stone because I can suck people in by getting them to agree that continuous reciprocal interactions

151
00:15:10,080 --> 00:15:15,840
between brain and body do a lot of what really looks like cognitive work. And there are good

152
00:15:15,840 --> 00:15:22,240
examples of that from even things like the role of gesture as we speak. So spontaneous gesture

153
00:15:22,240 --> 00:15:29,280
by Susan Golden Meadow and colleagues have shown that it kind of eases a certain sort of problem

154
00:15:29,280 --> 00:15:35,040
solving. If children are forced to explain how they solved the maths problem, but not allowed

155
00:15:35,040 --> 00:15:40,560
to gesture, they're actually a lot worse at it. So there's something going on, continuous

156
00:15:40,560 --> 00:15:47,520
reciprocal interaction that is probably allowing us to somehow offload aspects of work in memory

157
00:15:47,520 --> 00:15:52,960
into these gestures, stick what they're in the world, which is good news for philosophers that

158
00:15:52,960 --> 00:15:57,040
worry about hand waving. It's just part of my thinking.

159
00:15:57,040 --> 00:16:00,560
Respectable cognitive task going on right there. Can we

160
00:16:01,680 --> 00:16:06,320
ask how this was different back in the day? Obviously, a lot of the things we've been

161
00:16:06,320 --> 00:16:15,680
talking about are technological or modern. So was the mind of a human being 100,000 years ago

162
00:16:15,680 --> 00:16:20,560
very different? I think I would say it was, despite the fact that the brain of a human

163
00:16:20,560 --> 00:16:28,960
being 100,000 years ago wasn't very different. So yes, I would say that the mind of 100,000 year

164
00:16:28,960 --> 00:16:36,160
old humans was very different. But the brain obviously wasn't. But think of intermediate

165
00:16:36,160 --> 00:16:41,920
technologies before maybe we go back that things like just using pen and paper. And

166
00:16:42,720 --> 00:16:48,160
in particular, I'd like to think about the example of sort of scribbling while you think

167
00:16:48,880 --> 00:16:53,200
the thing that many of us, those of us at least old enough to be brought up with pen and paper,

168
00:16:53,200 --> 00:16:57,360
do an awful lot of. And so as we try and think a problem through, we write things down.

169
00:16:58,720 --> 00:17:03,600
I think it was Richard Feynman, who has a rather famous exchange with a historian,

170
00:17:04,720 --> 00:17:07,600
Weiner, I think the historian was, where Weiner said,

171
00:17:09,280 --> 00:17:14,000
these, so this is a record of your thinking. And Feynman said something like,

172
00:17:14,960 --> 00:17:21,360
no, it's not a record. It's, what did he say it was? It's not a record. It's working. You have to

173
00:17:21,360 --> 00:17:27,760
work on paper and this is working. And I think there he had an intuition that this is a just

174
00:17:27,760 --> 00:17:34,320
offloading stuff onto a different media. This is actually part of a process that solves the problem.

175
00:17:34,320 --> 00:17:39,760
Do we see this then in other animals? I mean, I presume the answer is yes. But is it more

176
00:17:39,760 --> 00:17:46,160
less dramatic depending on the species? Yeah, I mean, I think we humans do this to a really

177
00:17:46,160 --> 00:17:52,480
ridiculous degree. And it's because we invented symbolic culture somewhere along the way. We

178
00:17:52,480 --> 00:17:58,720
came to create these arbitrary systems of recombinable elements that we could then,

179
00:17:59,600 --> 00:18:07,120
you know, re-encounter as objects for perception. So creating these sort of structured encodings of,

180
00:18:07,120 --> 00:18:12,720
in some sense, our thoughts and ideas and putting them out in the world is a huge opportunity

181
00:18:12,720 --> 00:18:19,840
for any creature that is a good information forager, any creature that is very good at reaching out

182
00:18:19,840 --> 00:18:24,480
into the world to resolve some of its current uncertainty. And once again, that's a theme that

183
00:18:24,480 --> 00:18:30,640
we'll look back to later on. So humans do it a lot more. Other animals do it too. I don't think

184
00:18:30,640 --> 00:18:36,720
it's a purely human thing. It didn't just suddenly arise. You know, if you have a chimpanzee that

185
00:18:36,800 --> 00:18:42,960
or the orangutan, better example, that constantly uses a stick to gauge the depth of rivers before

186
00:18:42,960 --> 00:18:48,880
going into the rivers, I think there's enough of a sort of weave in and robustness for that to

187
00:18:48,880 --> 00:18:54,960
count as part of a sort of extended cognitive load. I guess I'm very interested in phase

188
00:18:54,960 --> 00:18:59,760
transitions, you know, small changes in things that allow enormously different capacities,

189
00:18:59,760 --> 00:19:06,960
et cetera. Clearly, the ability to think and communicate symbolically was a big one. It

190
00:19:06,960 --> 00:19:11,200
opened up things. But I guess what you're saying is one of the things that opened up is not just

191
00:19:11,200 --> 00:19:17,520
we can think more abstractly or generally, but we can think in literally different ways because

192
00:19:17,520 --> 00:19:22,560
these symbols allow us to more easily offload some of our cognition. Yes, I think that's right.

193
00:19:22,560 --> 00:19:30,880
I think that one thing we can do is we can use one of our biological capacities in very,

194
00:19:30,880 --> 00:19:35,920
very different ways. And that capacity is a tension. So when we put something out into the

195
00:19:35,920 --> 00:19:41,600
world in that sort of way, kind of clothe it in materiality, that lets us attend to it in a whole

196
00:19:41,600 --> 00:19:47,680
bunch of different ways. And interestingly, I think by doing that, we can often break the grip of our

197
00:19:47,680 --> 00:19:54,160
own sort of over-constructive internal models of what kind of thing it is and how it might behave

198
00:19:54,160 --> 00:19:59,840
and how you might re-engineer it. So even think about something like designing a new training

199
00:19:59,840 --> 00:20:04,640
show or something. If you build a big-scale model and then you start looking at it and poking it

200
00:20:04,640 --> 00:20:08,880
and prodding it, you can come up with all kinds of ideas that you wouldn't come up with just by

201
00:20:08,880 --> 00:20:14,720
kind of sitting down and trying to imagine all that in your head. So I think we're unusual in

202
00:20:14,800 --> 00:20:20,880
that respect, but it's a great power of us being perception action machines, that we can really

203
00:20:20,880 --> 00:20:26,160
make the most of this. And then you look at current more disembodied AI, they've got really

204
00:20:26,720 --> 00:20:33,360
no use for this kind of strategy. The way they work is just not such that they get to start putting

205
00:20:33,360 --> 00:20:38,480
stuff out into the world and then poking and prodding it just to think better. They might do

206
00:20:38,480 --> 00:20:42,560
that for all kinds of other reasons. But is that their fault or ours? I mean, we haven't really

207
00:20:42,560 --> 00:20:46,800
given them the capacity to do that. I think that's right. I think it's our fault. I think we've,

208
00:20:46,800 --> 00:20:52,400
to be honest, I think, well, it's our fault. We've designed a class of tools that are really,

209
00:20:52,400 --> 00:20:58,160
really useful for what they're useful for. But I don't think, actually, that those tools will

210
00:20:58,160 --> 00:21:04,960
ever achieve what I would call true understanding because they don't have their grip on reality

211
00:21:04,960 --> 00:21:10,560
grounded in perception action loops. And it's that grounding in perception action loops that I think

212
00:21:10,560 --> 00:21:16,080
we're leveraging with material symbolic culture when we just create these things and poke and

213
00:21:16,080 --> 00:21:20,880
prod them in different ways. We're super specialized for being good at perception action loops.

214
00:21:21,600 --> 00:21:25,840
So I think that's, there's a trick there that disembodied AI is missing out, but then it gets

215
00:21:25,840 --> 00:21:30,560
all kinds of other things instead. Sure, sure, it does. But okay, I'm going to, I'm just going to

216
00:21:30,560 --> 00:21:34,640
download a little bit because what you just said sparked several ideas and it's,

217
00:21:34,640 --> 00:21:40,000
it's prefiguring things we'll get to later. But two things. One, an idea I heard more than 10 years

218
00:21:40,000 --> 00:21:46,960
ago that attempts to make AI systems suddenly work much better when you put them in a robot

219
00:21:46,960 --> 00:21:54,000
that can go out there and interact with the world. And secondly, a podcast I did with Judea Pearl,

220
00:21:54,000 --> 00:21:59,520
who claims that babies spend all of their time making a causal map of the world by poking at

221
00:21:59,520 --> 00:22:06,320
things and seeing what happens. And I know that the whole prediction models that we'll be talking

222
00:22:06,320 --> 00:22:10,880
about are kind of this, like, you know, the brain making a model of the world. So all that

223
00:22:10,880 --> 00:22:16,080
together, do we imagine that if we do put modern large language models in an embodied

224
00:22:16,080 --> 00:22:20,000
context and let them go out and poke the world, they would change dramatically how they think?

225
00:22:20,880 --> 00:22:25,280
Yes, I think it would. You know, there are companies out there that are kind of trying

226
00:22:25,280 --> 00:22:31,440
to do this, like versus AI is one of them. And, you know, the idea certainly is to

227
00:22:32,080 --> 00:22:37,680
see what happens if you kind of leverage the active inference framework as a way of creating

228
00:22:37,680 --> 00:22:43,840
these sorts of new ecosystems of human artificial intelligences. Although I wouldn't start, I

229
00:22:43,840 --> 00:22:48,560
think, with a large language model, exactly, just because, you know, something like that has been

230
00:22:48,560 --> 00:22:55,120
trained to predict basically the next word in corpuses of text. And that's a very funny place

231
00:22:55,120 --> 00:23:00,400
to start if what you want to be is a perception action machine. It seems much more like

232
00:23:00,400 --> 00:23:06,960
developmental trajectories where you start without that huge body of, nonetheless, maybe

233
00:23:06,960 --> 00:23:15,680
relatively shallow successor item information. You want to learn basics about causation and flow,

234
00:23:15,680 --> 00:23:20,960
and you probably want to understand those things in a way that is more grounded than it would be

235
00:23:20,960 --> 00:23:24,800
if you just knew how all the words about causation and flow tend to follow one.

236
00:23:25,440 --> 00:23:26,160
It sounds like you-

237
00:23:26,400 --> 00:23:30,320
This isn't a very cool question. It remains to be seen. It might be that if you ban a large

238
00:23:30,320 --> 00:23:35,280
language model in a good enough robot, that it's just like giving a baby a super head start or

239
00:23:35,280 --> 00:23:40,720
something. But it does sound like you might be at least sympathetic to the idea that we need,

240
00:23:40,720 --> 00:23:47,840
that I've had some people on the podcast advance, that AI would be better off if we didn't just

241
00:23:47,840 --> 00:23:54,480
dump large data sets into deep learning networks and instead also poked and prodded the AI to have

242
00:23:54,480 --> 00:23:59,840
a symbolic representation of the world. Yes, I think that's right. I do agree with that. I think

243
00:23:59,840 --> 00:24:08,560
if our goal is not good tools to think alongside, but rather something like artificial general

244
00:24:08,560 --> 00:24:14,320
intelligences, so colleagues. If you want to build an artificial colleague, then I think

245
00:24:14,320 --> 00:24:20,320
large language models are not the right place to start. But if you want a really lovely tool

246
00:24:20,320 --> 00:24:25,680
that can do an awful lot of work and that you can work with, then they're really interesting.

247
00:24:25,680 --> 00:24:29,120
Honestly, I don't want an artificial colleague, but an artificial graduate student or postdoc

248
00:24:29,120 --> 00:24:35,600
would be very, very helpful or someone who could answer my email. I guess it would be weird not to

249
00:24:36,880 --> 00:24:44,000
ask the obvious question. I get annoyed when sometimes my philosophy colleagues seem to claim

250
00:24:44,000 --> 00:24:48,960
to have an immediate and obviously true view of what is natural and real in the world,

251
00:24:48,960 --> 00:24:54,800
but maybe I'm going to do that right now by saying, I feel like I'm inside my body. I feel

252
00:24:54,800 --> 00:25:03,600
like the I myself is located maybe even in my head. Is that just I have old intuitions that

253
00:25:03,600 --> 00:25:09,760
haven't quite caught up to the modern world? No, I think that that's a valid intuition,

254
00:25:09,760 --> 00:25:15,920
if you like, because I think that we infer that we are wherever it is that the perception action

255
00:25:15,920 --> 00:25:22,720
loop is closing. We're basic. We're perceiving the world and we're launching interventions and

256
00:25:22,720 --> 00:25:29,280
we're receiving the effects of those interventions. That's such a fundamental part of how we learn

257
00:25:29,280 --> 00:25:33,760
about things. I think it's no surprise that that's where we think we are. Notice that that's where

258
00:25:33,760 --> 00:25:40,400
we think we were, even if part of our brain was located in a nearby cliff top communicating

259
00:25:40,400 --> 00:25:45,040
wirelessly with the rest of the brain. We'd still think, oh, this is where the perception action loop

260
00:25:45,040 --> 00:25:51,200
is closing. This is where I am. Dan Dennett has a beautiful old philosophical short story called

261
00:25:51,200 --> 00:25:56,560
Where Am I? where he plays with that idea where there's someone whose brain is in the cliff top

262
00:25:57,120 --> 00:26:03,840
and they think that they are closing perception action loops using a body in the world,

263
00:26:03,840 --> 00:26:09,280
but then at some point, you know, it gets cut off and suddenly they think, oh, now I'm claustrophobically

264
00:26:09,280 --> 00:26:15,120
stuck in the cliff top. They knew their brain was there. Anyway, it's a beautiful story.

265
00:26:15,120 --> 00:26:22,080
No, that is very interesting. Okay, so we might as well dig into this phrase that you're using

266
00:26:22,080 --> 00:26:27,280
over and over again, the perception action loop. I think the word loop crucially important there,

267
00:26:27,280 --> 00:26:34,240
right? The way I like to think about it is, and perhaps this is true because of technology,

268
00:26:34,240 --> 00:26:40,560
but we have a naive metaphor for how the brain works as kind of like there's a video camera

269
00:26:40,560 --> 00:26:45,360
with a computer hooked up to it, right? Just taking in our sense data, processing it and

270
00:26:45,360 --> 00:26:50,400
doing something. And a big part of your message is, no, it's really not like that at all.

271
00:26:51,040 --> 00:26:57,360
Yeah, no, that's exactly right. I mean, let's think about one way, which is not like that.

272
00:26:57,360 --> 00:27:01,840
If you think about something like running to catch a fly ball in baseball, where you're kind

273
00:27:01,840 --> 00:27:08,240
of running to try and catch this is ball that's flying out there. The way to do that isn't to

274
00:27:08,240 --> 00:27:13,360
take in all the information about the flight so far, and then plot where you think the ball's

275
00:27:13,360 --> 00:27:19,520
going to go and then tell the robot you as it were go over there. Instead, what you do is you run

276
00:27:19,520 --> 00:27:26,080
in a way that keeps the apparent trajectory of the ball in the sky looking motionless as you

277
00:27:26,080 --> 00:27:32,080
look up there. It turns out that if you just keep doing that, then you'll be in the right place when

278
00:27:32,080 --> 00:27:36,240
the ball comes down. You need to do a bit more to actually catch it. You'll be in the right place

279
00:27:36,240 --> 00:27:42,000
when the ball comes down. And there's a way of solving an embodied action problem that involves

280
00:27:42,000 --> 00:27:50,480
keeping the perceptual signal within a certain sort of bounds and then acting so as to, well,

281
00:27:50,480 --> 00:27:55,920
so is to do that, basically. Something that, again, prediction error minimizing systems would

282
00:27:55,920 --> 00:28:02,240
actually be rather good at. So perception action loops, I think, the important thing to think

283
00:28:02,240 --> 00:28:06,800
about there is that they're not solving problems all in one go generally. It's sort of like,

284
00:28:06,800 --> 00:28:11,440
I do a little bit of this, I do something that gets more information or that keeps the perceptual

285
00:28:11,440 --> 00:28:16,080
stuff within bounds, and then repeat the process again and again and again until the thing is

286
00:28:16,960 --> 00:28:22,800
So I do appreciate your use of a USA based sports metaphor there, but we have an international

287
00:28:22,800 --> 00:28:31,120
audience. I have no idea what it really means. But okay, so the motto then is the brain is a

288
00:28:31,120 --> 00:28:38,800
prediction machine. And I just want to sort of home in on what is the difference between that

289
00:28:38,800 --> 00:28:43,680
view and another view. I think maybe if people don't think about the brain too much, they would

290
00:28:43,680 --> 00:28:48,560
say, okay, sure, yeah, my brain makes predictions, but you really want to put that front and center

291
00:28:48,560 --> 00:28:55,360
as the point of what the brain does. Yeah, yep. I mean, it's both the beauty and the burden,

292
00:28:55,360 --> 00:29:00,400
if you like, of this story that it is really all this account. I'm told about to use the word

293
00:29:00,400 --> 00:29:07,360
story because it doesn't sound scientific. It's a count that it's basically saying this is a

294
00:29:07,360 --> 00:29:13,280
canonical operation of the brain, if you like, the canonical computations that the brain performs

295
00:29:13,280 --> 00:29:19,680
and that it strings together in different ways to do motor control and interception and extraception

296
00:29:20,880 --> 00:29:26,320
is basically one in which you're attempting to predict or the brain's attempting to predict

297
00:29:26,320 --> 00:29:31,520
the current flow of sensory information. Of course, there are different timescales at which

298
00:29:31,520 --> 00:29:36,480
you could predict it. And the basic timescale, the bedrock one is predicting the present,

299
00:29:36,480 --> 00:29:41,040
which of course is a rather funny use of prediction that was saying predictions about the future

300
00:29:41,040 --> 00:29:47,200
somehow, but this is a sense in which the brain is a guessing machine. It's trying to

301
00:29:47,200 --> 00:29:53,760
guess what the current sensory evidence is most likely to be and then crunch in that guess based

302
00:29:53,760 --> 00:29:59,200
on past experience together with the current sensory evidence. And that turns out to be a very,

303
00:29:59,200 --> 00:30:04,880
very powerful way of estimating the true state of the external work. It goes wrong sometimes,

304
00:30:04,880 --> 00:30:10,480
but it's a very powerful strategy. It allows you to use what you've learned in the past

305
00:30:11,040 --> 00:30:18,880
to make better sense of what the raw sensory energies are kind of suggesting in the present.

306
00:30:19,440 --> 00:30:25,680
This seems very related, certainly, to Carl Friston's ideas. We had him on the podcast,

307
00:30:26,400 --> 00:30:30,480
the free energy principle, the Bayesian brain. What is the relationship between all these ideas?

308
00:30:31,600 --> 00:30:36,640
Yeah, I mean, basically, I'm just being an apologist for Carl Friston here.

309
00:30:37,440 --> 00:30:46,160
So, Carl's basic picture, the free energy minimization picture is a sort of high road

310
00:30:46,160 --> 00:30:54,560
version of the low road version that I give you. So, the low road version comes kind of out of

311
00:30:54,560 --> 00:31:00,160
thinking about perception and thinking about action, whereas the high road version comes out

312
00:31:00,160 --> 00:31:08,240
of thinking about what it takes to be a persistent, living system at all. You have to

313
00:31:08,240 --> 00:31:13,120
preferentially inhabit the kind of states that define you as a system that you are. That means

314
00:31:13,120 --> 00:31:17,600
that you end up in the states that in some broad sense, you predict you ought to be in.

315
00:31:17,600 --> 00:31:21,200
So, in that broad sense, the fish kind of predicts it ought to be in water

316
00:31:22,080 --> 00:31:30,800
because its actions are all designed with that in mind. The low road is a bit less

317
00:31:32,080 --> 00:31:36,320
challenging. I guess that'll be why I take it. The low road is just sort of saying, look,

318
00:31:36,320 --> 00:31:41,040
we have good evidence that the canonical computations that human brains and the brains of

319
00:31:41,040 --> 00:31:48,000
many other animals perform involve this attempt to guess the sensory signal. And it turns out

320
00:31:48,000 --> 00:31:53,280
that you can use that to do motor control in a very efficient way. It's been known for a long

321
00:31:53,280 --> 00:31:58,480
time, I think, that it's important in perception. And the other thing that I think Carl Friston's

322
00:31:58,480 --> 00:32:05,520
work has really done has been shown that that old story about perception becomes so much more

323
00:32:05,520 --> 00:32:11,920
powerful when you see that it's got a direct echo in an account of action. So, the idea there is that

324
00:32:12,640 --> 00:32:18,640
perception is about getting rid of prediction errors as you try to guess the state of the world

325
00:32:18,640 --> 00:32:24,800
using everything you already know. And action is getting rid of prediction errors, but it's

326
00:32:24,800 --> 00:32:29,680
getting rid of them by changing the world to fit the prediction. So, you have these two ways to

327
00:32:29,680 --> 00:32:33,840
get rid of prediction error. You find a better prediction, or you change the world to fit the

328
00:32:33,840 --> 00:32:40,000
prediction you got. One's perception, the other's action, but they're performed using the same kind

329
00:32:40,080 --> 00:32:49,040
of basic neuronal operations. There are differences between motor control and sensory perception.

330
00:32:49,040 --> 00:32:54,640
But if you look at the wiring of motor cortex, it turns out it's actually wired pretty much like

331
00:32:54,640 --> 00:33:01,040
ordinary sensory cortex. In fact, so much so that many people don't like to talk about sensory

332
00:33:01,040 --> 00:33:07,600
and motor cortex just for that reason. So, these stories or these accounts give you a sort of

333
00:33:07,680 --> 00:33:13,040
principled way of understanding that. In each case, you're predicting a sensory flow,

334
00:33:13,600 --> 00:33:20,080
but in one case, you're trying to retrieve an old model of the world to fit the flow,

335
00:33:20,080 --> 00:33:24,320
and that's perception. And in the other case, you're trying to change what you're

336
00:33:24,320 --> 00:33:28,720
getting to fit the prediction. We can circle around to that. I think I can say that a bit better

337
00:33:29,680 --> 00:33:39,520
Is the idea that it's the same neural circuitry that is doing sensing and control, or is it that

338
00:33:39,520 --> 00:33:46,960
the two circuitries look parallel? Yes, it's that they look very much parallel.

339
00:33:47,920 --> 00:33:54,640
They operate in the same sort of way. The reason they're not the same is a proprioceptive prediction,

340
00:33:54,640 --> 00:34:02,160
is playing a very special role in motor control. So, the idea is a proprioception is a sort of

341
00:34:02,960 --> 00:34:08,800
system of internal sensing that lets us know how our body is currently arranged in space.

342
00:34:09,360 --> 00:34:16,240
So, the idea is that in order to reach to pick up the coke can that's in front of me,

343
00:34:16,240 --> 00:34:22,000
I predict the proprioceptive flow that I would get if I were reaching for it,

344
00:34:22,000 --> 00:34:28,320
and then I slowly get rid of those errors by moving the motor plant in just that sort of way.

345
00:34:28,320 --> 00:34:33,040
So, you get rid of the errors by moving the arm, in this case, to reach up the coke can.

346
00:34:33,040 --> 00:34:38,320
So, proprioceptive predictions are especially placed here. They act as motor commands.

347
00:34:38,320 --> 00:34:44,480
The other kinds of predictions don't. So, I hesitate to use the word think here,

348
00:34:44,480 --> 00:34:49,680
because thinking and cognition and thought I'll have technical meanings that are slippery to me.

349
00:34:49,680 --> 00:34:56,640
But the idea is that your brain sort of thinks intentionally or unintentionally that your arm

350
00:34:56,640 --> 00:35:03,360
is somewhere slightly different than it is, and then the muscles move it to make that more accurate?

351
00:35:04,000 --> 00:35:07,920
That is one way of putting it. And people have put it that way. So, some people have said,

352
00:35:07,920 --> 00:35:12,720
look, you've got to kind of lie to yourself. Brace will go to lie to itself. It's got to

353
00:35:13,760 --> 00:35:19,600
ignore all that good information that says that my arm isn't moving. It's got to predict

354
00:35:19,840 --> 00:35:26,080
the trajectory of sensation that you would get if it was moved in. And that prediction is given

355
00:35:26,080 --> 00:35:30,720
so much weight that the other information is overwhelmed and off it goes as a way you get

356
00:35:30,720 --> 00:35:36,880
rid of the errors by moving it. I'm not sure personally that's exactly the best way to put it.

357
00:35:36,880 --> 00:35:42,640
I kind of think it's really about attention. So, I think it's like you've got information

358
00:35:42,640 --> 00:35:47,920
that says that you're not moving the arm. You've also, because of your current goals,

359
00:35:48,480 --> 00:35:53,280
you're predicting this proprioceptive flow that would correspond to reaching for the

360
00:35:53,280 --> 00:36:01,040
coke can. And by, as it were, disattending to the information that says that it's stationary,

361
00:36:01,840 --> 00:36:06,400
you allow the other prediction to do its work. So, I think it's sort of targeted

362
00:36:06,400 --> 00:36:11,920
disattention rather than actually lying to yourself. I'm not quite sure why I prefer that.

363
00:36:11,920 --> 00:36:16,240
I think we're probably saying the same thing. You know, the math and the models all work out

364
00:36:16,240 --> 00:36:22,400
yesterday. But I think if we call it targeted disattention, we understand it a bit better.

365
00:36:22,400 --> 00:36:27,680
I think it makes more contact with sports science, for example. But you don't really want to be

366
00:36:27,680 --> 00:36:34,080
attending to the position your body is currently in. You want to attend to something like how it

367
00:36:34,080 --> 00:36:41,440
ought to be. You want to entrain yourself by knowing what it would feel like if you were doing it

368
00:36:41,440 --> 00:36:50,560
right, right now. So, is it similar to the idea of file compression? You know, if we have a JPEG

369
00:36:50,560 --> 00:36:57,200
or an MPEG, that there are some patterns that are repeated so you don't have to keep track of every

370
00:36:57,200 --> 00:37:02,000
pixel. You can just assume that they keep going. And then you pay attention to the differences.

371
00:37:02,000 --> 00:37:06,480
And those are what we're talking about as errors in the brain case.

372
00:37:06,480 --> 00:37:11,520
That's certainly correct when we come back to thinking about perception and something like

373
00:37:11,520 --> 00:37:19,600
that is involved in action too. But it does seem like it's easier to think about that bit in the

374
00:37:19,600 --> 00:37:26,480
case of perception, where the idea would be that, you know, I predict the current sensory flow and

375
00:37:27,120 --> 00:37:33,120
it's only the errors in that prediction that then need further processing. So, you know,

376
00:37:33,120 --> 00:37:39,040
I think I've woken up in my bedroom and I predict a certain sensory flow and as I turn my head around,

377
00:37:39,040 --> 00:37:44,720
I'm not getting any information that corresponds to that. I might have to go scrabbling or use

378
00:37:44,720 --> 00:37:49,360
these prediction errors to retrieve the information that actually I'm in a hotel room and it's going

379
00:37:49,360 --> 00:37:57,040
to look pretty different. So, I think it's in that sense that when you don't have to scrabble that

380
00:37:57,040 --> 00:38:03,840
hard, this is a super, super efficient way of doing moment by moment processing because

381
00:38:03,840 --> 00:38:07,760
if you already predicted it properly using what you know about the world, you don't need to take

382
00:38:07,760 --> 00:38:13,440
any further action. So, in that sense, it's exactly like JPEGs and motion compressed video where the

383
00:38:13,440 --> 00:38:19,360
idea is, you know, if I've already transmitted the information about this frame of the video,

384
00:38:19,360 --> 00:38:25,680
then in order to know what the next frame is, all I need to do is react to whatever the errors

385
00:38:25,680 --> 00:38:30,080
would be if I assumed it was just like the frame before and normally then there's a few residual

386
00:38:30,080 --> 00:38:37,520
errors and if you use those as the information that you need to deal with, you get the right picture.

387
00:38:37,520 --> 00:38:44,880
So, these prediction errors, these residual errors, they carry whatever sensory information is

388
00:38:44,880 --> 00:38:50,560
currently unexplained. They'll explain whatever you can with prediction and then just you don't

389
00:38:50,560 --> 00:38:55,840
need that much bandwidth really to just use the rest to refine it. I remember hearing, it might

390
00:38:55,840 --> 00:39:00,880
have been in a conversation with David Eagleman here on the podcast, the idea that the reason why

391
00:39:01,680 --> 00:39:08,640
the years seem to go by faster as we get older is because there's less novelty in our lives. You

392
00:39:08,640 --> 00:39:13,280
know, the first time we go to a beach, it's all new and we really expend a lot of mental energy

393
00:39:13,280 --> 00:39:18,480
taking it in. Whereas the 20th time, we already have a background and the errors are not that big.

394
00:39:18,960 --> 00:39:21,200
Does that fit into this story? It sounds like it does.

395
00:39:21,200 --> 00:39:26,880
Yeah, I think that fits really, really well. It also helps explain why it's so hard to learn

396
00:39:26,880 --> 00:39:33,040
new things at a certain point in your life because the inputs that you're receiving get sucked into

397
00:39:33,040 --> 00:39:39,680
these well-worn troughs in your current world model, if you like. I think us academics find

398
00:39:39,680 --> 00:39:45,040
that a lot as we get older in something like, oh, yeah, I understand that. That fits with this

399
00:39:45,040 --> 00:39:51,120
kind of story that I've already got. Sometimes it's very, very hard to give the new evidence it's

400
00:39:51,120 --> 00:39:57,760
proper due. Of course, attention is a tool for doing that, but that takes a lot of deliberate

401
00:39:57,760 --> 00:40:05,520
effort sometimes to really, really attend. Attention reverses the dampening effect that

402
00:40:05,520 --> 00:40:13,120
prediction normally has. Part of the evidence base for this story, this account, is that

403
00:40:13,360 --> 00:40:21,040
well-predicted sensory inputs cause less neural activity than other sensory inputs.

404
00:40:21,840 --> 00:40:25,280
There's something odd there. These are the ones we deal with very fluidly,

405
00:40:25,840 --> 00:40:30,560
and yet there's less going on in the brain when you deal with them than in other cases.

406
00:40:31,600 --> 00:40:37,280
That's one of the signatures that led, I think, to predicted coding coming on the sea.

407
00:40:38,080 --> 00:40:42,800
You open a can of worms when you say how the old academics get stuck in a rut there,

408
00:40:42,880 --> 00:40:48,640
but I have to just follow up a little bit. I think there's two different perspectives

409
00:40:48,640 --> 00:40:54,160
I could put forward. One is more or less what you just said. As we get into our ruts, it takes

410
00:40:54,160 --> 00:40:59,280
more and more energy, activation energy or whatever to think in new ways, but the other is,

411
00:41:00,480 --> 00:41:07,200
maybe this is more optimistic, because we are older and good at certain ways of thinking,

412
00:41:07,760 --> 00:41:14,320
we forget that it was hard to not be good at any ways of thinking. We forget how hard it was to

413
00:41:14,320 --> 00:41:19,760
learn calculus or French or whatever. Therefore, we're just not willing to put in the work anymore,

414
00:41:19,760 --> 00:41:25,200
even if it were exactly the same amount of work. Yes, I think that's right. We end up in a situation

415
00:41:25,200 --> 00:41:31,840
of expert perceivers in general, where expert perception clearly is being able to take this

416
00:41:31,840 --> 00:41:39,760
unruly sensory information and just see what it actually means for victory on the chess board

417
00:41:39,760 --> 00:41:44,800
or whatever it or whatever you happen to be engaged in. I think, in a way, all successful

418
00:41:44,800 --> 00:41:50,800
perception is expert perception in that sense, and academic expertise is in the same sort of

419
00:41:50,800 --> 00:41:59,920
ballpark. It has the same sort of blind spots as well. If there is something interesting going on,

420
00:41:59,920 --> 00:42:07,360
but it doesn't fall within the bounds of the existing generative model to use the right

421
00:42:07,360 --> 00:42:14,000
vocabulary for the predictive processing story, then the only thing you can do with it is use it

422
00:42:14,000 --> 00:42:19,760
to drive torturous new slow learning and see why a lot of time we don't really want to do that.

423
00:42:19,760 --> 00:42:25,200
Don't want to do that. No. Well, look, you've used the word, relatedly, use the word efficiency as

424
00:42:25,200 --> 00:42:33,440
one of the benefits to this model. If we think of probably the first ever video storage software

425
00:42:33,440 --> 00:42:37,840
was more or less like a movie where you just had every frame and you stored it, right? Then only

426
00:42:37,840 --> 00:42:43,680
later did you realize it was much more efficient to just update the changes. Presumably, since our

427
00:42:43,680 --> 00:42:49,920
brains were not intelligently designed and grew up through evolution, this is a nice feature to

428
00:42:49,920 --> 00:42:54,640
have. We're trying to do the most with finite resources. Is that a big part of the attraction

429
00:42:54,640 --> 00:43:00,160
of this story that it requires less thinking or energy or calories or what have you?

430
00:43:00,800 --> 00:43:06,320
Yes, I think that's right. I think if the brain didn't work this way, but we were able to do the

431
00:43:06,320 --> 00:43:15,360
kinds of things that we do, we'd probably have brains overheated, repeated. The amount of energy,

432
00:43:15,360 --> 00:43:21,600
there is a trade-off here. The trade-off is keeping the world model, the generative model

433
00:43:21,680 --> 00:43:27,360
you're using to make the predictions going. That, of course, is a big metabolic cost. That's kind

434
00:43:27,360 --> 00:43:34,080
of the cost that is being his signature you see in spontaneous cortical activity. There's stuff

435
00:43:34,080 --> 00:43:38,400
going on all the time. All the time, the kind of thinking about the resting state work from

436
00:43:38,400 --> 00:43:46,080
Raeckel and others as well. That is a cost, but that is traded against a moment by moment cost

437
00:43:46,160 --> 00:43:52,320
of processing all this incoming information. I think what we're doing is we've traded the cost

438
00:43:52,320 --> 00:44:00,800
across time there and we're using a fairly metabolically expensive upkeep of a world model

439
00:44:00,800 --> 00:44:08,560
to allow a much more efficient moment-by-moment response, a faster one as well. Being somewhat

440
00:44:08,560 --> 00:44:13,520
anticipatory about what's going on in the world around you is a pretty good thing if you're an

441
00:44:13,520 --> 00:44:20,720
animal involved in all kinds of conflicts and dangerous situations. Also, there are wiring costs.

442
00:44:21,680 --> 00:44:28,480
It's hard to know how best to think about the wiring costs, but the amount of downward flowing

443
00:44:29,120 --> 00:44:35,920
wiring and information seems to very often outnumber the inward flowing stuff,

444
00:44:36,480 --> 00:44:43,680
sometimes up to 10 to 1, certainly 4 to 1. It looks as if brains have decided. It's been

445
00:44:43,680 --> 00:44:48,240
decided over evolutionary time that this is a good thing to do, and I think that that means

446
00:44:48,240 --> 00:44:54,400
it is overall the most efficient way to proceed. I also worry that in a way if you want understanding,

447
00:44:54,400 --> 00:44:58,400
there might be no other way to proceed. That's kind of maybe there's something more

448
00:44:59,520 --> 00:45:05,120
abstract or philosophical going on there, but I don't know what understanding would look like.

449
00:45:06,560 --> 00:45:12,400
As it were, you weren't bringing a world model of some kind to bear on the current sensory

450
00:45:13,360 --> 00:45:19,840
evidence in a context sensitive way. There's a question there that I don't know. I really

451
00:45:19,840 --> 00:45:25,440
don't know the answer to because I know they're a kind of formal proof. Anything that can be done

452
00:45:26,080 --> 00:45:32,880
using a system that has feedback like that can be unfolded into a feed-forward system.

453
00:45:33,520 --> 00:45:40,000
Maybe the upshot there is, well, maybe you could unfold us into a feed-forward system for a few

454
00:45:40,000 --> 00:45:46,000
bits of processing or something, but it would require a huge brain and a ridiculous amount of

455
00:45:46,000 --> 00:45:49,600
energy, but somebody out there maybe might want to think about that.

456
00:45:49,600 --> 00:45:52,720
Yeah, exactly. I was going to say this does sound like a research project for somebody,

457
00:45:53,280 --> 00:46:00,720
but there's the benefit of this mechanism that the brain uses, the predictive processing model.

458
00:46:01,680 --> 00:46:08,320
Presumably, there's also downsides. Maybe one is that we're very susceptible to illusions.

459
00:46:09,040 --> 00:46:12,000
We think we're seeing things that aren't there because it's part of what our brain

460
00:46:12,000 --> 00:46:18,800
is predicting very, very strongly. Yes, absolutely. I'm subject to several of these illusions. I'm

461
00:46:18,800 --> 00:46:23,760
sure we all are. Phantom phone vibration is probably a good one. I often might think that

462
00:46:23,760 --> 00:46:29,440
phone's going off in your pocket when it's not even in your pocket. I'm now susceptible to phantom

463
00:46:29,440 --> 00:46:36,480
wrist vibrations since I gave in and bought a smartwatch. The hollow mask illusion, I guess,

464
00:46:36,480 --> 00:46:45,760
is a classic in this area where an ordinary joke shop mask, if viewed from the concave side,

465
00:46:45,760 --> 00:46:51,120
so viewed from behind in a certain sense with a light source, behind the mask, you'll think that

466
00:46:51,120 --> 00:46:59,040
it's an ordinary outward face that you're seeing. Again, that seems to be because we have very strong

467
00:46:59,120 --> 00:47:05,040
predictions about the concavity of normal face structures. We very seldom see anything that

468
00:47:05,040 --> 00:47:12,960
isn't like that. That prediction now trumps the real sensory evidence specifying concavity,

469
00:47:14,080 --> 00:47:20,400
except it doesn't in everyone. Autism spectrum condition folk are slightly less susceptible

470
00:47:20,400 --> 00:47:27,120
to that illusion and to the McGurk illusion, for example, from a predictive processing viewpoint.

471
00:47:27,200 --> 00:47:32,080
That's probably because of a slightly altered balance, one where sensory inputs are somewhat

472
00:47:32,080 --> 00:47:37,680
enhanced relative to expectations. What was the other illusion you mentioned, the McGurk?

473
00:47:38,240 --> 00:47:43,440
Oh, the McGurk. It's like a ventriloquism illusion. Yes, sorry. It's this one where

474
00:47:44,160 --> 00:47:54,000
it's a sort of Bargar illusion. There is a sound which you can be played such that

475
00:47:54,800 --> 00:48:00,560
if the lips are moving a certain way, you'll hear it as Gar, and if they move in a different way,

476
00:48:00,560 --> 00:48:03,680
you'll hear it as Bar, but it's exactly the same sound. Exactly the same sound.

477
00:48:04,800 --> 00:48:09,360
It's not entirely clear whether that's an illusion or something else. It's hard to know exactly where

478
00:48:09,360 --> 00:48:16,160
the boundaries of illusions and other kinds of inference lie. I mean, these kind of optical

479
00:48:16,160 --> 00:48:26,480
and auditory illusions almost seem fun and benign, but presumably we can take the basic picture that

480
00:48:26,480 --> 00:48:29,840
there's an enormous amount of information coming into our senses at every moment,

481
00:48:29,840 --> 00:48:36,320
and we can't possibly process it all. Therefore, we filter it. We filter it to fit our perceptions

482
00:48:36,320 --> 00:48:42,720
and then correct for the errors. That must also work with abstract concepts or news items just

483
00:48:42,720 --> 00:48:49,680
as much as pictures that we get through our eyes. Yes, I think that's right. There is a tendency to

484
00:48:49,680 --> 00:48:57,600
not just to see what we expect to see, which can be very damaging too, but sometimes to read what

485
00:48:57,600 --> 00:49:04,160
we expect to read. Well, by read there, I mean sort of read into a text, what you expect to be

486
00:49:04,160 --> 00:49:10,000
learning from that take. I think many of us do this. I think all of us probably do this when

487
00:49:10,000 --> 00:49:15,920
we read news articles where it looks like they're saying something that we're very much either sympathetic

488
00:49:15,920 --> 00:49:21,680
to or opposed to, but maybe when you look at that text again or you go over it with a fine tooth

489
00:49:21,680 --> 00:49:25,360
car, when you look at it with someone that's got a different perspective, you realize that's

490
00:49:25,360 --> 00:49:33,040
really in that text. It's just what I brought to bear in some way. In the visual case, Louise

491
00:49:34,000 --> 00:49:44,640
Feldman Barrett, Lisa Feldman Barrett has this lovely but very, very scary example of the way that

492
00:49:45,680 --> 00:49:53,200
interceptive predictions, predictions about your own bodily state, are getting perhaps crunched

493
00:49:53,200 --> 00:49:59,840
together with extraceptive ordinary sensory information in ways that could perhaps give you

494
00:49:59,920 --> 00:50:10,320
experiences where an object that is reached for in a dark alley might actually appear to you as a

495
00:50:10,320 --> 00:50:20,640
gun when really it's just a smartphone. In sort of much less worrying and sort of more

496
00:50:20,640 --> 00:50:26,320
controllable cases, you can show that if you give people false cardiac feedback,

497
00:50:26,320 --> 00:50:31,520
so you make them think their heart's beating faster than it is, then a face that would otherwise

498
00:50:31,520 --> 00:50:38,720
look neutral to them is judged to be an angry face or a worrying face. So it does seem as if

499
00:50:38,720 --> 00:50:46,080
we're using internal information to help make the predictions that structure our experience of the

500
00:50:46,080 --> 00:50:52,080
external world. And so that's something that can go right or go wrong as well. So we end up seeing

501
00:50:52,160 --> 00:50:57,040
what we expect to see and maybe even believing what we expect to believe in some sense.

502
00:50:57,680 --> 00:51:04,240
Yes. I mean, there is a sort of, you know, some people say seeing is believing, but in these cases

503
00:51:04,240 --> 00:51:10,560
believing is kind of seeing. And maybe in these interception cases, feeling is seeing as well.

504
00:51:10,560 --> 00:51:14,960
That's the way that Feldman Barrett puts it. Another former Minescape podcast guest,

505
00:51:14,960 --> 00:51:22,160
I got to say Lisa Feldman Barrett. Are there therefore features or things out there in the

506
00:51:22,160 --> 00:51:28,960
world that we are systematically bad at perceiving because of this way that we process information?

507
00:51:29,760 --> 00:51:36,240
It's a lovely question and one that I haven't thought about. So things that we just, all of us,

508
00:51:36,240 --> 00:51:43,680
tend to miss because, well, I suppose the most obvious answer there is unusual events.

509
00:51:45,040 --> 00:51:51,760
So, you know, you would have seen the, you know, the footage of the gorilla walking across the

510
00:51:51,760 --> 00:51:59,200
scene where you're trying to count the passes of the baseball. So he's worked by Simon and

511
00:51:59,200 --> 00:52:08,400
Chappris. Anyway, it's his classic work that in sort of attentional blindness and sorry,

512
00:52:08,400 --> 00:52:12,400
inattentional blindness. So the idea there is that if you're concentrated on one task,

513
00:52:12,400 --> 00:52:17,840
something really quite dramatic could happen and you just wouldn't notice it. I think we're all very,

514
00:52:17,840 --> 00:52:27,840
very subject to that. So outliers, I suppose. So predictive brains are tuned in to patterns

515
00:52:27,840 --> 00:52:34,160
that have helped them solve problems before. And so whenever we're confronted with an outlier

516
00:52:34,160 --> 00:52:39,840
situation, we're likely to miss it unless for some reason we're attending right there. But why

517
00:52:39,840 --> 00:52:44,400
would we be? Because attention is driven by where we expect the good information to be.

518
00:52:45,200 --> 00:52:51,200
And this is why, for example, expert drivers are very, very good at a lot of things,

519
00:52:51,200 --> 00:52:57,040
but they can miss a cyclist if they're approaching a roundabout from entirely the wrong direction.

520
00:52:57,040 --> 00:53:01,840
Somewhere that, you know, no one comes from that direction at the roundabout. So there's a lot of

521
00:53:01,840 --> 00:53:07,920
these looked but didn't see, as they call them, accidents where people might even move their

522
00:53:07,920 --> 00:53:13,760
heads in that direction. But if it's that unexpected, they just don't see what's going on.

523
00:53:13,760 --> 00:53:17,600
Presumably, these are all quantitative questions. I mean, if something is

524
00:53:18,320 --> 00:53:22,160
blatant enough, we're going to see it even if we didn't predict it. Is that fair to say?

525
00:53:23,280 --> 00:53:32,160
I think, yes, that's fair to say as long as we're able to attend to it. So it needs to be able to,

526
00:53:32,720 --> 00:53:36,960
and some things will try and grab our attention. So like a loud noise. It's a really loud noise,

527
00:53:37,040 --> 00:53:42,720
like those scaffolders I had out my window earlier happened. Then that will grab my attention,

528
00:53:42,720 --> 00:53:49,520
unless I'm really, really desperately focused in all my attention somewhere else as I might be.

529
00:53:49,520 --> 00:53:56,480
So if you think about stage magic, stage magic is a really nice case where rather dramatic things

530
00:53:56,480 --> 00:54:02,880
can suddenly appear on stage. And most people don't notice them because attention has been so very,

531
00:54:03,280 --> 00:54:12,560
carefully controlled by the magician. Because attention is up in the waiting on either specific

532
00:54:12,560 --> 00:54:20,640
predictions or prediction errors. So it's a really, really super important part of the

533
00:54:20,640 --> 00:54:26,320
predictive brain story, this balancing act that is very in moment by moment.

534
00:54:27,120 --> 00:54:31,600
That's a great example. I like the idea that professional illusionists are just

535
00:54:31,600 --> 00:54:34,240
leveraging the fact that our brains are predictive processors.

536
00:54:35,040 --> 00:54:38,320
That's true. Yeah, no, there's a beautiful book. I can't remember the title now,

537
00:54:38,320 --> 00:54:46,800
but it's by Luis Martinez. And it is a book written by neuroscientists about stage magic,

538
00:54:46,800 --> 00:54:50,640
leveraging the picture of the predictive brain as a way of understanding a lot of it.

539
00:54:51,280 --> 00:54:55,760
Very, very good. Okay, so let me then be put on the skeptical hat just a little bit.

540
00:54:56,720 --> 00:55:01,360
And this is similar to questions that are raised for Carl Frist and etc.

541
00:55:01,360 --> 00:55:06,320
Look, if our brain is trying in some sense to minimize prediction error,

542
00:55:06,880 --> 00:55:14,080
can't it do that best by not collecting any data just by hiding away and being completely

543
00:55:14,080 --> 00:55:20,080
unsurprised because we never leave our room? Yeah, I think that's such a nice worry to have

544
00:55:20,080 --> 00:55:25,440
because it leads right into this hugely important dimension of intraceptive prediction and

545
00:55:25,440 --> 00:55:30,960
artificial curiosity. So let's maybe talk about these things for a moment. So there's this

546
00:55:30,960 --> 00:55:38,000
sort of worry that it's sometimes called the dark and room. But the best place for a predictive

547
00:55:38,000 --> 00:55:43,680
brain to be would be to lead us into a dark corner and just keep the same. So all of those sensory

548
00:55:43,680 --> 00:55:48,080
inputs they just keep on keep on coming just the same. You predict them perfectly, but you wither

549
00:55:48,080 --> 00:55:53,600
and die. Now we don't do that, obviously. Does that mean the predictive brain story is wrong?

550
00:55:54,400 --> 00:55:58,000
Obviously, I don't think so. I think what it means is that there's more to the predictive

551
00:55:58,000 --> 00:56:05,520
brain story than just bringing extra sensitive sensory information into line with predictions.

552
00:56:06,800 --> 00:56:10,400
We're making all these intraceptive predictions all the time. I'm predicting

553
00:56:11,200 --> 00:56:18,960
the state of my own body. I'm predicting, for example, that I should at all times have

554
00:56:19,040 --> 00:56:27,840
sufficient supplies of water and glucose, for example. And then action is automatically taken,

555
00:56:27,840 --> 00:56:33,600
not when I don't have enough water or glucose, but long before I don't have enough water or

556
00:56:33,600 --> 00:56:40,320
glucose. So if you start to feel thirsty, that will be happening before you've reached the

557
00:56:40,320 --> 00:56:45,680
point at which you're going to wither and die without water. And if you take a drink when you're

558
00:56:45,680 --> 00:56:51,120
thirsty or feel relief, but that water will have no effect on you physiologically for about 20

559
00:56:51,120 --> 00:56:57,040
minutes. So the relief is as much a prediction as the original thirst was. This again is an example

560
00:56:57,040 --> 00:57:03,360
from Lisa Feldman Barrett. So once you put intraception in that way into the picture, then of

561
00:57:03,360 --> 00:57:08,160
course we're not going to stay and not eat and not drink in the darkened room because we have these

562
00:57:08,160 --> 00:57:15,760
kind of chronic systemic expectations, if you like, of staying alive. But then there's a bit

563
00:57:15,760 --> 00:57:20,640
more to it than that, I think, as well. Because we don't stay in boring rooms either. So you could

564
00:57:20,640 --> 00:57:26,400
put me in a very boring room, but give me enough food and water and all of that stuff. And I wouldn't

565
00:57:26,400 --> 00:57:33,280
like that very much either. I might even start to do things. I might start to play games by

566
00:57:33,280 --> 00:57:40,080
drawing on the wall or something like this. And so this now falls under the umbrella of kind of

567
00:57:40,080 --> 00:57:49,280
work in artificial curiosity. And predictive brains are naturally artificially curious brains

568
00:57:49,280 --> 00:57:56,000
because minimizing prediction error is their basic reward. So you could sort of say for brains like

569
00:57:56,000 --> 00:58:00,560
that the only thing really that's rewarded is minimizing prediction error. That's what they

570
00:58:00,560 --> 00:58:06,240
want to do. And if they're not performing any particular tasks, they'll still try and find

571
00:58:06,240 --> 00:58:11,360
some prediction error to minimize because that's the kind of thing that they are. And this is what

572
00:58:11,360 --> 00:58:19,280
makes predictive brains general purpose structure learners. So there's some rather nice work that's

573
00:58:19,280 --> 00:58:27,280
been done by Rosalyn Moran at UCL. I think it's UCL. And what she's been doing is comparing reward

574
00:58:27,280 --> 00:58:34,400
driven learners with prediction driven learners and finding that the purely reward driven learners

575
00:58:35,200 --> 00:58:40,560
will learn a way of solving the problem more rapidly than the prediction driven learners

576
00:58:40,560 --> 00:58:46,800
very often, but frequently a more shallow way of solving the problem. Because as soon as they find

577
00:58:46,800 --> 00:58:52,880
a way to reliably get the reward, they kind of stick. Whereas a prediction driven system,

578
00:58:52,960 --> 00:58:58,720
it basically wants to minimize prediction error as much as possible. And that leads it to explore

579
00:58:58,720 --> 00:59:04,400
its environment repeatedly learning more and more about it. And if you then seed it with a goal,

580
00:59:05,200 --> 00:59:14,000
and simultaneously, it will outperform a pure reward driven agent. So that I think when you

581
00:59:14,000 --> 00:59:18,640
put those two things together, you kind of see that the darkened room isn't really much of a

582
00:59:18,640 --> 00:59:25,520
threat. We don't like death and we don't like boredom. And both of those things seem to be

583
00:59:25,520 --> 00:59:30,160
natural effects of being driven to minimize errors in prediction.

584
00:59:30,160 --> 00:59:35,360
I guess the way out that comes to my mind directly, which I'm not sure if it's the same

585
00:59:35,360 --> 00:59:41,520
thing as what you are proposing or it's something different. But in physics, when we calculate

586
00:59:41,520 --> 00:59:47,680
the entropy of a system and we say is that maximum entropy, that's telling us some distribution of

587
00:59:47,680 --> 00:59:53,440
all the possible states it could be in, blah, blah, blah. But famously, we can calculate maximum

588
00:59:53,440 --> 00:59:59,200
entropy of a system subject to different constraints, right? Like subject to it's at a certain

589
00:59:59,200 --> 01:00:04,640
temperature or it's at a certain pressure or whatever, and we'll get different forms for that

590
01:00:04,640 --> 01:00:12,240
probability distribution. So couldn't we just say that there are two things going on? We want to

591
01:00:12,320 --> 01:00:18,560
minimize prediction error, but we also want to survive. So there's a constraint we want to survive.

592
01:00:18,560 --> 01:00:25,040
And under that constraint, it's actually useful to go out and be surprised sometimes so we can

593
01:00:25,040 --> 01:00:29,600
update our predictive model. And I think that would, I don't know how mathematically that would

594
01:00:29,600 --> 01:00:34,800
work out, but it does seem a little bit intuitive to me. Yeah. And I think that does mathematically

595
01:00:34,800 --> 01:00:40,160
work out. I think this is the sort of stuff that Carl Friston can speak to more reliably than I can.

596
01:00:40,160 --> 01:00:48,640
But it looks as if, very often, the correct move for a prediction-driven system

597
01:00:49,280 --> 01:00:56,640
is to temporarily increase its own uncertainty, so as to do a better job over the long time scale

598
01:00:56,640 --> 01:01:02,320
of minimizing prediction errors. And that looks like the value of surprise, actually, and that we

599
01:01:02,320 --> 01:01:09,200
will, I think we artificially curate environments in which we can surprise ourselves. I think,

600
01:01:09,200 --> 01:01:15,120
actually, this is maybe what art and science is to some extent, at least, we're curating

601
01:01:15,120 --> 01:01:21,280
environments in which we can harvest the kind of surprises that improve our generative models,

602
01:01:21,280 --> 01:01:27,280
our understandings of the world, in ways that enable us to be less surprised about certain

603
01:01:27,280 --> 01:01:32,160
things in the future. I wonder if you could use this idea or a set of ideas to make

604
01:01:32,160 --> 01:01:38,240
predictive models for what kind of games people would like to play or what kind of stories or

605
01:01:38,240 --> 01:01:44,000
novels or movies people would like to experience. Or, I guess, in music, it's very famous, right?

606
01:01:44,000 --> 01:01:48,960
You want some rhythm, some predictability, but you also want some surprise also. There's a sweet

607
01:01:48,960 --> 01:01:56,720
spot in the middle. Yes, I think that's exactly right. Karen Kukonen, a literary theorist in

608
01:01:57,760 --> 01:02:03,680
Scandinavia somewhere, has written a nice book called Probability Designs. And so she is using

609
01:02:03,680 --> 01:02:10,960
the vision of the predictive brain as a way of understanding the shape of literary materials,

610
01:02:10,960 --> 01:02:15,360
poems and novels. Okay, there you go. And my idea is that we should think of every novel, every poem,

611
01:02:15,360 --> 01:02:21,280
as a probability design, leading us through sort of building up expectations, cashing them out,

612
01:02:22,000 --> 01:02:27,360
building up at multiple levels, giving precision, weighting to some of the expectations versus

613
01:02:27,360 --> 01:02:33,280
others. And clearly, you know, this makes sense of music as well. There's an awful lot of that

614
01:02:33,280 --> 01:02:39,040
going on in music. And probably it applies to all sorts of things, even like roller coaster design,

615
01:02:39,040 --> 01:02:45,040
I imagine. It's exactly that. A roller coaster is a kind of probability design. What's interesting

616
01:02:45,040 --> 01:02:50,400
is how we get surprised again and again, even if we ride the same roller coaster or read the same

617
01:02:50,400 --> 01:02:56,400
novel or listen to the same piece of music. And I think that shows the skill of the constructor

618
01:02:56,400 --> 01:03:03,200
in giving us inputs that activate bits of our model, drive in expectations again and again,

619
01:03:03,200 --> 01:03:08,080
to the point where you're still surprised in some sense, even though you could have said beforehand,

620
01:03:08,080 --> 01:03:13,760
that's what's going to happen. You still can't help but be surprised. And this, in some sense,

621
01:03:13,760 --> 01:03:18,560
at some level would be the best thing to say. And I think this speaks a little bit to the idea that

622
01:03:18,560 --> 01:03:25,040
as prediction machines, we are multi level machines, we're not just, it's not just there's a prediction,

623
01:03:25,040 --> 01:03:30,640
and it's either cashed or it isn't, but this prediction exists as a high level abstraction

624
01:03:30,640 --> 01:03:34,960
predicting a lower level one, predicting a lower level one, predicting a lower level one,

625
01:03:34,960 --> 01:03:41,440
all the way down to the actual incoming notes of the tune or words on the page.

626
01:03:42,880 --> 01:03:48,480
It's because we're multi level prediction machines, I think, that things like honest

627
01:03:48,480 --> 01:03:55,840
placebo's work. So you know, placebo's obviously for rather nicely under this sort of general

628
01:03:55,840 --> 01:04:02,160
account, because expectations of relief thrown into the pot can make a difference to the amount

629
01:04:02,160 --> 01:04:07,280
of relief that you feel. But if you're told that you're being given a placebo, it can still make

630
01:04:07,280 --> 01:04:11,840
a difference to the amount of relief that you feel. Presumably, that's because there are all

631
01:04:11,840 --> 01:04:18,000
these sub levels of processing that are getting automatically activated by good packaging delivery

632
01:04:18,000 --> 01:04:24,560
by people in white coats with authoritative voices on this sort of thing. So I think that's

633
01:04:24,560 --> 01:04:29,760
something that we might learn from these accounts too, that we should maybe medicine and society

634
01:04:29,760 --> 01:04:37,440
could make more use of ritual and, yeah, ritual and packaging and things that in some sense,

635
01:04:38,720 --> 01:04:43,280
I don't know, in some, well, I want to say ineffective. What I mean is they don't,

636
01:04:43,840 --> 01:04:47,440
they bring about their effects, but not through the standard routes.

637
01:04:47,440 --> 01:04:52,960
Good. Okay, very, very good. Okay, when, you know, we're late in the podcast, we can get a little

638
01:04:52,960 --> 01:05:02,000
bit wilder and more profound here. So you, in the book, you gesture toward ideas along the lines of

639
01:05:02,640 --> 01:05:06,320
we are not only, you know, the thing that the thing to get in mind is that we're not just

640
01:05:07,040 --> 01:05:11,920
sensing reality and writing it down, we are in some sense participating and bringing it about.

641
01:05:14,000 --> 01:05:18,480
Maybe even using phrases like what you think of as reality is really a hallucination.

642
01:05:19,120 --> 01:05:22,400
Tell us exactly how far we can go along that rhetorical road.

643
01:05:23,120 --> 01:05:32,080
Yeah. Proceed with caution would be the right thing to say because lots of people talking

644
01:05:32,080 --> 01:05:36,880
about these things, myself included, use this phrase perception is controlled hallucination.

645
01:05:37,840 --> 01:05:43,520
And you can see why, because the idea here is that perception is very much a constructive process

646
01:05:44,640 --> 01:05:50,160
in which your own expectations get thrown into the pot and they help you see what you see.

647
01:05:51,440 --> 01:05:57,840
And those same expectations are thrown into the pot of feeling your body the way that you feel it.

648
01:05:57,840 --> 01:06:02,960
And so a lot of our medical symptoms, in fact, all our medical symptoms reflect some kind of

649
01:06:03,760 --> 01:06:10,320
combination of expectation and whatever sensory evidence the body is actually kind of throwing

650
01:06:10,320 --> 01:06:18,320
up at that time. So this is really a very, very powerful story. But when you think about perception

651
01:06:18,320 --> 01:06:22,800
of controlled hallucination, we need to take the notion of control pretty seriously.

652
01:06:24,160 --> 01:06:30,800
For that reason, in some slightly more philosophical works, I've tried to argue that we should

653
01:06:30,800 --> 01:06:35,600
flip the phrase around and think of hallucination as uncontrolled perception.

654
01:06:35,600 --> 01:06:40,960
And that that, as it were, puts a boot on the right foot somehow. It lets us say that when these

655
01:06:40,960 --> 01:06:45,600
things are working properly, you're in touch with the world. This is a way of using what you know

656
01:06:45,600 --> 01:06:50,640
to stay in touch with the world as it matters to an embodied organism like you trying to do the

657
01:06:50,640 --> 01:06:57,600
things you're trying to do. But then of course, when it goes wrong, when the perception is

658
01:06:57,600 --> 01:07:03,520
uncontrolled, if you like, then you get hallucination. You will get a hallucination when

659
01:07:03,520 --> 01:07:07,440
you're kind of disconnected from the world in your predictions, your brain's predictions are

660
01:07:07,440 --> 01:07:13,120
doing all the work. And then if you turn the dial in the other direction, and your brain's predictions

661
01:07:13,120 --> 01:07:19,200
aren't doing enough work, you fail to spot fake patterns in noisy environments and be easily

662
01:07:19,200 --> 01:07:29,280
overwhelmed. So I feel like thinking of hallucination as uncontrolled perception is actually the

663
01:07:29,280 --> 01:07:33,760
better way to do it, even though it's clumsy to say, and it doesn't even roll of my tongue that

664
01:07:33,760 --> 01:07:39,760
easily, which is why I don't think I actually bothered making that move in the experience

665
01:07:39,760 --> 01:07:47,120
machine book. You know, it's really hard to resist drawing a parallel with large language models

666
01:07:47,120 --> 01:07:52,640
here, because of course, their entire job is predicting what's supposed to come next, right?

667
01:07:52,640 --> 01:07:57,600
And guess what? They famously hallucinate. They say things that are completely false,

668
01:07:57,600 --> 01:08:03,120
but interestingly, they do things with apparent confidence, right? They don't hesitate or mumble

669
01:08:03,120 --> 01:08:08,640
when they're hallucinating to them. It comes out just as definitive as the truth does. And maybe

670
01:08:08,640 --> 01:08:16,720
there is a parallel there. Yes, I think that's right. I mean, their hallucinations are in a way,

671
01:08:16,720 --> 01:08:22,400
I think, partly at least the result of them not being anchored in perception action loops in the

672
01:08:22,400 --> 01:08:27,360
right sorts of ways. So, you know, it's this anchoring in perception action loops that sort of

673
01:08:27,360 --> 01:08:32,000
teaches us a lesson when we're young. It's like, you know, if you get things wrong, bad things are

674
01:08:32,000 --> 01:08:37,920
going to happen to you. If I don't spot the edge of the path as being the right place, then I'm

675
01:08:37,920 --> 01:08:42,080
going to fall over and I'm going to get signals that I chronically don't like as it were. So

676
01:08:42,160 --> 01:08:48,000
all the interceptive predictions are coming into play there. Whereas if all you're doing is predicting

677
01:08:48,000 --> 01:08:55,440
the next word in the sentence, and your reward is basically being kept alive as a large language

678
01:08:55,440 --> 01:09:01,920
model, then why not just, you know, go the whole hog and be confident about a nice structured piece

679
01:09:01,920 --> 01:09:09,840
of bullshit that you can generate? At the same time, it's interesting that by changing the prompts

680
01:09:09,840 --> 01:09:15,440
to the large language models or to chat, chat GPT anyway, you can make it do substantially better.

681
01:09:15,440 --> 01:09:21,840
So you can say something like, you know, writes this for me in the style of a well-informed

682
01:09:22,560 --> 01:09:27,680
scientific expert. It makes less mistakes. It still still tends to hallucinate references,

683
01:09:27,680 --> 01:09:35,920
but at least it's a little bit better. But yeah, so something very thin about just predicting

684
01:09:35,920 --> 01:09:41,440
the next symbol. You know, I just feel like it's not very well anchored in reality. And so

685
01:09:41,440 --> 01:09:46,720
hallucinations are kind of, you can't tell the difference between a hallucination and

686
01:09:46,720 --> 01:09:51,680
something else. Maybe that's the point. Unfortunately, when we're in the grip of hallucination,

687
01:09:51,680 --> 01:09:55,200
we can't tell the difference either. Someone on Twitter coined the term

688
01:09:55,200 --> 01:10:01,680
hallucitations when chat GPT makes up papers you haven't written. I like that. I like that.

689
01:10:01,680 --> 01:10:04,720
Yeah, they appear all the time. I'm going to start including them on my CV.

690
01:10:05,680 --> 01:10:12,720
Are there implications, you know, if we get down and dirty and not philosophical for how to treat

691
01:10:13,680 --> 01:10:21,280
mental issues that we have, whether it's, you know, depression or pain or anything like that?

692
01:10:21,280 --> 01:10:24,080
Or do we get actionable intelligence from this way of thinking?

693
01:10:24,640 --> 01:10:30,880
Yes, I think we do. I mean, it's early days. But I think particularly in the case of pain,

694
01:10:30,880 --> 01:10:37,440
there are some clear sorts of recommendations here that are being implemented by people working

695
01:10:37,440 --> 01:10:42,480
in what they call pain reprocessing theory, which is just to find a high pollutant label for the

696
01:10:42,480 --> 01:10:53,520
idea that you reframe your pains. So, you know, the thought would be that we tend to treat pain

697
01:10:53,520 --> 01:11:00,640
as a signal that we shouldn't be doing something. If you reframe it as, okay, my pain

698
01:11:00,640 --> 01:11:06,640
signaling system is misfiring, then you can begin to think, so this pain doesn't mean that I shouldn't

699
01:11:06,640 --> 01:11:12,800
be doing it. And it turns out that if you get people involved in those regimes, they start to be able

700
01:11:12,800 --> 01:11:17,840
to do a bit more because they're not scared of stopping because of the pain. And actually, as

701
01:11:17,840 --> 01:11:24,400
they find that they can do more, the pain itself presents itself to them as lessening. I think

702
01:11:24,400 --> 01:11:28,880
because the brain sort of infers, well, if I'm doing this stuff, it can't be that bad, can it?

703
01:11:28,880 --> 01:11:35,600
So there's a kind of virtuous cycle that replaces a vicious cycle that was there before, which was,

704
01:11:35,600 --> 01:11:40,960
you know, this is going to hurt, so I'm not going to do it. And then if I do stop to do it,

705
01:11:40,960 --> 01:11:46,400
oh, it really seems to hurt, I'm not doing it. So pain reprocessing theory is one nice case.

706
01:11:48,000 --> 01:11:54,960
I suppose self affirmation is another case, the idea that the idea that, you know, if you're prone

707
01:11:54,960 --> 01:12:00,080
to thinking that perhaps you're not going to do well in some tests because you're in a certain

708
01:12:00,080 --> 01:12:07,280
minority group or you're female and it's an ass test or something like this, self affirmation

709
01:12:07,280 --> 01:12:11,760
in advance of the test can really make a difference. You know, I'm good at this, I can do this,

710
01:12:12,880 --> 01:12:20,320
lots of people like me do this, that kind of thing. It's important, of course, not to over

711
01:12:21,280 --> 01:12:25,840
over egg the custard, as we might say on this side of the Atlantic anyway.

712
01:12:27,040 --> 01:12:35,120
You know, you can't reframe, you can't reframe having this sort of bacterial infection in a way

713
01:12:35,120 --> 01:12:40,000
that is really going to make any difference, the bacterial infection that you've got and

714
01:12:40,000 --> 01:12:44,960
reframing makes a big difference to cancer related fatigue, it doesn't make much difference to cancer.

715
01:12:45,920 --> 01:12:51,200
So I think we have to be, you know, we have to be aware of the limits. And in general,

716
01:12:51,200 --> 01:12:57,760
I don't think that these stories, these counts are kind of positive thinking sorts of account,

717
01:12:57,760 --> 01:13:02,960
they're kind of, they're more like, you know, there are many factors that are involved as we

718
01:13:02,960 --> 01:13:08,720
construct our experiences. And some of those factors are our own expectations.

719
01:13:08,720 --> 01:13:13,280
Yeah, okay, that sounds perfectly sensible put that way. I'm not an expert, but my rough

720
01:13:13,280 --> 01:13:19,680
impression is that we don't, we know embarrassingly little about pain and how it works. It's an

721
01:13:19,680 --> 01:13:23,920
understudied area, I guess, I don't know whether it's for moral or psychological reasons. So any

722
01:13:23,920 --> 01:13:29,040
little insight might be very helpful. Yeah, yes, I agree. And I think that I think pain research

723
01:13:29,040 --> 01:13:35,440
is actually moving into some very, very interesting stages now as we understand more about the ways

724
01:13:35,440 --> 01:13:40,560
in which people's own expectations make a difference. So even where, where there's a very

725
01:13:40,560 --> 01:13:49,200
standard physiological cause, it, people's experiences of their, of their pain, very tremendously,

726
01:13:49,200 --> 01:13:54,560
and even within a certain individual, their experiences, very tremendously from context

727
01:13:54,560 --> 01:14:00,400
to context and day by day, in ways that just aren't tracking the organic as well. I don't

728
01:14:00,400 --> 01:14:04,160
like this word organic, there's always something organic going on, aren't tracking the sort of

729
01:14:04,160 --> 01:14:10,080
the standard, the standard calls. So what's probably happening is that different contexts

730
01:14:10,080 --> 01:14:16,880
are activating different expectations of pain or disability. And without the standard organic

731
01:14:16,880 --> 01:14:20,960
calls changing in any way, that's making a difference to how you feel and what you can do.

732
01:14:21,760 --> 01:14:25,360
I actually just realized I forgot to ask a crucially important question earlier.

733
01:14:26,400 --> 01:14:32,480
We had Nannis Mayel on the podcast a while back and my new colleague at Johns Hopkins,

734
01:14:32,480 --> 01:14:37,200
and talking about physics and the arrow of time. And you know, through talking to her and

735
01:14:37,200 --> 01:14:43,360
through talking to other people, I have this vague idea of why we think that time flows,

736
01:14:43,360 --> 01:14:49,600
why we have the sense of time passage. And it's because we are constantly predicting a moment

737
01:14:49,600 --> 01:14:54,400
in the future and also remembering a little bit in the past and updating, right? And you know,

738
01:14:54,400 --> 01:15:00,320
the updates happen in one direction of time and that's what gives us this sense of flow or passage.

739
01:15:01,280 --> 01:15:04,160
Given your expertise, does that sound at all on the right track?

740
01:15:04,880 --> 01:15:10,880
Yeah, that sounds very, that sounds like a really nice story to me or account even.

741
01:15:10,880 --> 01:15:13,520
There's an account there. I like story. Go ahead with story.

742
01:15:14,880 --> 01:15:22,080
It's, I mean, it reminds me a little bit actually of Husserl, the kind of classic

743
01:15:23,280 --> 01:15:28,720
philosophical phenomenologists who had this idea that experience is this sort of,

744
01:15:29,440 --> 01:15:34,400
this thing which is rooted, rooted in the past, but always looking towards the future.

745
01:15:34,400 --> 01:15:40,320
And the present is this sort of just a kind of just where those things meet,

746
01:15:41,440 --> 01:15:48,160
where the, you know, I don't know what really gives us the arrow of time, that sort of sense that

747
01:15:49,120 --> 01:15:54,240
the idea that you can't sort of unscramble the egg or whatever it is, it's not obvious to me quite

748
01:15:54,240 --> 01:16:03,200
why my prediction machinery is kind of what's delivering the fact that it looks like I really

749
01:16:03,200 --> 01:16:09,760
can't recreate the egg from scratch. Well, I think that's one for you. That is one for me.

750
01:16:09,760 --> 01:16:16,560
And I think that I halfway agree in that the account has not been fully fleshed out, although I'm

751
01:16:16,560 --> 01:16:20,960
100% sure it's ultimately because entropy is increasing. We just have to draw the connections

752
01:16:20,960 --> 01:16:25,280
there, which is the useful work to be done. It sounds like a great account of why we think

753
01:16:25,280 --> 01:16:31,280
there's an arrow of time. Yeah, why we feel it. Right. Why that's part of our image of the world.

754
01:16:31,280 --> 01:16:36,960
Okay. So then the last question is the flip side of the pain question. There's this idea called

755
01:16:36,960 --> 01:16:42,880
the hedonic treadmill, which I think some people have said have been, has been discredited. But

756
01:16:42,880 --> 01:16:51,040
the idea that we get happy not because of our overall welfare, but because of changes in our

757
01:16:51,040 --> 01:16:56,160
welfare. And if we win the lottery and now we're rich and living in luxury, soon we have exactly

758
01:16:56,160 --> 01:17:01,600
the same happiness as we had before. There are challenges to this view. So I'm not even sure

759
01:17:01,600 --> 01:17:06,320
if it's true. There's a replication crisis in psychology. Everything is very easy to me. But

760
01:17:06,960 --> 01:17:12,720
it does seem compatible with the whole predictive modeling view of what the brain is doing. If

761
01:17:12,720 --> 01:17:18,240
what we're doing is constantly predicting what's happening next, then can happiness be understood

762
01:17:18,240 --> 01:17:22,080
as noticing that our prediction was a little pessimistic and things are actually a little bit

763
01:17:22,080 --> 01:17:28,640
better? Yeah, that's interesting. I hadn't thought about this, but it sounds like it should for

764
01:17:28,640 --> 01:17:35,840
rather neatly into place with this sort of dampening of the well-predicted. So the fundamental

765
01:17:35,840 --> 01:17:41,360
starting point for a lot of these accounts was that the neuronal response to well-predicted

766
01:17:41,360 --> 01:17:48,160
sensory inputs is dampened. So if these sensory inputs are supposed to be driving pleasurable

767
01:17:48,160 --> 01:17:54,640
experiences, but you've really been through that 100 million times before, then the pleasure is,

768
01:17:54,640 --> 01:18:00,640
I think, going to be diminished, perhaps unless you can actively reach in there with a tension

769
01:18:00,640 --> 01:18:06,960
and try to stop that. So I wonder whether someone that really loves the taste of a particular wine

770
01:18:06,960 --> 01:18:14,160
that they had a million times before, as long as they can reach in with a tension and up the dial

771
01:18:14,160 --> 01:18:20,880
on what's coming in through the senses, then maybe they can artificially surprise themselves

772
01:18:20,880 --> 01:18:25,440
a little bit, if you see what I mean. I do. I'm not sure about that, but I feel like there's

773
01:18:25,440 --> 01:18:31,760
something there of wine tasters are told to do this to sort of sit back and let it speak for

774
01:18:31,840 --> 01:18:35,440
itself so that you don't get sucked into your own expectations.

775
01:18:36,000 --> 01:18:39,520
But now it brings up questions. I know I said it was the last question, but

776
01:18:40,080 --> 01:18:46,400
there's an issue here of high versus low pleasures or simple versus subtle pleasures, right? I mean,

777
01:18:46,400 --> 01:18:52,720
I'm a big wine fan and I absolutely do get pleasure from very fancy complicated sophisticated wine,

778
01:18:52,720 --> 01:18:58,240
because I can't afford to have it too often, so that it's not boring to me. But I also get pleasure

779
01:18:58,240 --> 01:19:02,720
from like the perfect slice of pizza, which is very simple and predictable and whatever, but

780
01:19:02,720 --> 01:19:06,400
I get that comforting pleasure. So now I'm not sure what to think.

781
01:19:07,120 --> 01:19:12,400
Yes, I mean, you know, I think I'm not quite sure what to think about those cases either. I mean,

782
01:19:12,400 --> 01:19:18,960
it does seem to me that we, you know, because our brains of prediction minimize in engine,

783
01:19:19,520 --> 01:19:26,880
then in a way, we do kind of want to live in worlds that we can predict well and that have

784
01:19:26,960 --> 01:19:32,240
certain sorts of things. And so when those things are bringing hedonic benefit, then I think

785
01:19:33,040 --> 01:19:37,280
existing there is going to be a rather comfortable way of existing, even though we also have

786
01:19:38,400 --> 01:19:43,840
a sort of a company and drive to increase our states of information to sort of, you know,

787
01:19:43,840 --> 01:19:50,000
learn a bit more in case it lets us generate a slightly alternative future in which, you know,

788
01:19:50,000 --> 01:19:55,440
the pizza is square, but we're liking it even better or something. But I think that

789
01:19:56,160 --> 01:20:01,360
I think that if we think about these things delicately as sort of multi level and multi

790
01:20:01,360 --> 01:20:07,600
dimensional prediction engines, then we can accommodate both the both the drive for novelty

791
01:20:07,600 --> 01:20:14,560
and the attractions of staying within the space where where actually all that stuff that wants

792
01:20:14,560 --> 01:20:20,240
us to minimize errors is doing rather well in a local sense. Even just looking at a pizza,

793
01:20:20,240 --> 01:20:24,080
you're minimizing lots of errors. After all, just to see the shape of the pizza in front of you,

794
01:20:24,080 --> 01:20:28,400
you're kind of moving your eyes around, harvesting information, minimizing errors,

795
01:20:28,400 --> 01:20:33,200
you're getting some hedonic kick out of it, nothing bad is happening anywhere. It's a pretty

796
01:20:33,200 --> 01:20:39,360
comfortable place to be. You know, I'd like to leave messages for the young intellectuals out

797
01:20:39,360 --> 01:20:44,160
there who are deciding what to do with their lives. It sounds like there's a sweet spot here

798
01:20:44,160 --> 01:20:49,120
where we do know something about the brain and the body and how they work and how they fit together,

799
01:20:49,120 --> 01:20:53,120
but there's still a lot of good questions left on the table to be answered by the future.

800
01:20:53,120 --> 01:20:57,840
I think there's a huge number of questions. You know, every story, every account we've had so

801
01:20:57,840 --> 01:21:02,560
far has turned out to be wrong and I'm sure this one will too. So the question is, you know,

802
01:21:02,560 --> 01:21:06,960
what's a stepping stone towards? Well, it's a very good story. You're telling us Andy Clark.

803
01:21:06,960 --> 01:21:11,360
Thanks so much for being on the Mindscape podcast. Thank you. It's been a real pleasure. Thank you.

804
01:21:53,120 --> 01:21:54,500
you

805
01:22:23,120 --> 01:22:24,000
you

