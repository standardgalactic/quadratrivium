start	end	text
0	5000	Hello, everyone, and welcome to the Mindscape Podcast. I'm your host, Sean Carroll.
5000	10000	If any of you out there own a copy of my book, Something Deeply Hidden,
10000	15000	you can go to the copyright page, the page right after the title page,
15000	20000	where there's the copyright information, et cetera, printed in the United States of America, or whatever.
20000	27000	There is a little notation on that page that gives the version number.
27000	31000	And the version number is not your copy of the book personally.
31000	37000	All of you and your friends and I and everyone else that we know has the same version number.
37000	48000	And the version number is 756 trillion, 132 billion, 390 million, 815,553.
48000	53000	If you know the story about what that version number is, when I wrote the book,
53000	62000	I generated a quantum random number between one and a quadrillion, an integer.
62000	65000	And that's what this version number is.
65000	73000	If you believe or if you, for the moment, imagine that the world is described
73000	79000	by the many worlds interpretation of quantum mechanics, which I explain and defend in the book,
79000	87000	then there are different branches of the wave function with almost exactly the same book written in them,
87000	89000	but different version numbers.
89000	96000	Every different integer of that size is represented somewhere on the branches of the wave function of the universe,
96000	103000	somewhere it's all zeros or all threes or whatever, and there's got to be some comment in that version of the book.
103000	106000	Well, wow, I guess we got really unlucky about that.
106000	115000	But this idea, this little joke that we sneaked into the book was supposed to be a way of really making you face up
115000	120000	to the claims involved in the many worlds formulation of quantum mechanics.
120000	125000	And those claims, I am very quick to admit, are weird.
125000	127000	They are bizarre.
127000	135000	The implications of the many worlds interpretation of quantum mechanics are very, very far away from our intuitive,
135000	138000	everyday experience of the world.
138000	143000	And some people will say, you know what, they're too far away.
143000	144000	I'm just not going to go there.
144000	150000	I do not believe there are a quadrillion different versions of something deeply hidden out there
150000	154000	and some quantum multiverse with different version numbers in them.
154000	163000	So what are we to do about that objection that this idea that many worlds interpretation of quantum mechanics is just too weird?
163000	168000	Because quantum mechanics is not the only place where this issue arises.
168000	175000	There are other attempts to make sense of the world, in other words, understand it even better than we do,
175000	181000	that like it or not lead us to some place that seems bizarre and weird to us.
181000	186000	That is the theme of Eric Schwitzgabel's new book.
186000	191000	Eric is a philosopher at University of California Riverside,
191000	196000	and he has a new book that is going to come out tomorrow by the day that this podcast gets released.
196000	199000	And it is literally called The Weirdness of the World.
199000	209000	And in this book, he basically faces up to the fact that some of our best attempts to make sense of the world end up looking pretty weird.
209000	214000	In fact, he argues that in some cases, they're going to have to look pretty weird.
214000	219000	There is no version of the correct theory that doesn't look weird to us.
219000	222000	Now, weird is an interesting thing.
222000	227000	Weird in his definition of it is a little bit different from bizarre.
227000	235000	He defines bizarre to mean contrary to our common sense, and weird is a little bit stronger than that.
235000	238000	It's contrary to our common sense in kind of an irrevocable way.
238000	241000	It's not going to go away just because we understand it better.
241000	243000	We're going to have to buy into it.
243000	247000	So he talks about not only quantum mechanics, but consciousness is a big theme.
247000	253000	All the different versions of consciousness theory, just like all the different versions of the foundations of quantum mechanics,
253000	256000	Eric will argue, involve a certain degree of weirdness.
256000	263000	And there's other weird things that may just be false, like maybe we live in a simulation or maybe we're brain in a vat or whatever.
263000	267000	Maybe those you can dismiss, but what about when you can't dismiss it?
267000	273000	What do you do when you need to balance different levels of weirdness of different kinds of theories?
273000	276000	Should we just ignore the weird possibilities?
276000	279000	Should we give substantial credence to them?
279000	284000	Should we be more cautious when all the options on the table are pretty weird?
284000	295000	So I like it as a work of philosophy that faces up to the real challenges that come along with taking seriously our best ways of understanding the world.
295000	301000	So over the course of all our different podcasts here, we've gone to some pretty weird places we might as well celebrate it.
301000	302000	Let's go.
319000	321000	Eric Schwitzkable, welcome to the Mindscape podcast.
321000	323000	Thanks for having me.
323000	330000	So you've written a book about the weirdness of the world, which is just a great provocative title that I'm sure will give us a lot to talk about.
330000	338000	But before we do that, I have to bring up a couple of things that you've also been involved with that are really interesting, but not on that topic.
338000	340000	But I have to get them out of the way a little bit.
340000	343000	One is virtual Dan Dennett.
343000	347000	You know, we had Dan Dennett as a previous Mindscape guest.
347000	349000	I think that's his claim to fame these days.
349000	356000	But those are before the days when we had large language models and could just program up a Dan Dennett to interview.
356000	359000	So you did that and did a little experiment.
359000	361000	So tell us about that.
361000	363000	Yeah, we sure did.
363000	369000	So we did this, I should say, this is collaborative with Anna Strasser and my son, David Schwitzkable.
369000	375000	And Matthew Crosby was a collaborator earlier on, but he had to leave to work for DeepMind.
375000	377000	That happens, yeah.
377000	381000	So we couldn't publish something using open AI technology.
381000	386000	I'm sure he's handsomely compensated for his problems.
386000	390000	He was involved early on and actually did some of the fine tuning.
390000	397000	So people know chat GPT, probably most of your listeners know where this is a large language model.
397000	398000	It can produce texts.
398000	400000	It's very human-like.
400000	408000	A precursor to that that came out, I think, in 2020 was GPT-3.
408000	414000	And one of the things that you could do with GPT-3 was you could fine tune it on a corpus of text.
414000	419000	So it's basically, it consumes large portions of the internet.
419000	423000	And from this, it can predict the next word when you input likely text.
423000	427000	So when you fine tune it, what you do is you add some more corpus.
427000	436000	And then it readjusts its weights so that its outputs look like a compromise between what it is in its original state
436000	440000	and the corpus that you've input, you fine-tuned it on.
440000	447000	So what we did was we took GPT-3 and we fine-tuned it on the corpus of Dan Dennett, about 2 million words.
447000	455000	Most of his, I don't know how many books he has, in the teens, in articles, we just pumped them into GPT.
455000	460000	And then what we did was we asked the actual Dan Dennett 10 philosophical questions
460000	466000	and we asked our fine-tuned model, we called DigiDan, the same 10 questions.
466000	468000	We did it four times.
468000	472000	We got four answers from each of the DigiDans.
472000	477000	We didn't do any cherry picking of those answers to see for quality.
477000	480000	We did make sure that they were sufficient length.
480000	482000	So we excluded short answers.
482000	486000	We wanted length similar to Dennett's own answers.
486000	489000	And then what we did was we took experts in Dennett's work and we said,
489000	494000	hey, could you guess which is Dennett's real answer and which has come from the model?
494000	497000	So chance would have been 20%.
497000	500000	The experts got it about 50%, right?
500000	507000	So they did better than chance, but still half the time they chose the model's answer over Dennett's answers.
507000	514000	And on two of the questions, the plurality of experts actually chose one of the model's answers over Dennett's answer.
514000	518000	And on one of the questions, Dennett said, you know what?
518000	521000	I kind of see why the experts chose that answer over my own answer.
521000	524000	If I'd thought about it more, I should have said something different.
524000	531000	Arguably, on one question, the model actually outperformed Dan himself.
531000	533000	So anyway, that was our experiment.
533000	536000	We wrote it up, we published it, and it was a lot of fun.
536000	541000	And when you did it, it was only a short while ago, but probably it was much more surprising at the time.
541000	548000	It wasn't quite as public the whole excitement these days about Lore's language models.
548000	549000	That's right.
549000	552000	We actually ran the experiment before ChatGPT was released.
552000	560000	And so people were getting the wind about the power of GPT-3, but it wasn't quite the phenomenon that it has since become.
560000	569000	And just as a technical matter, when you download this corpus, does that mean you had to have electronic copies of all of Dan's writings?
569000	573000	Did he help you with that, or did you just pirate a book or get a Kindle edition?
573000	578000	He helped us with this, and he collaborated with us on the project throughout.
579000	591000	And Anna Strosser did a huge amount of work converting these old PDFs and junky files into clean text that could be uploaded into the model.
591000	592000	But you've written things.
592000	593000	You have a blog.
593000	596000	Have you been tempted to do this for yourself, too?
596000	598000	Does that save you time?
598000	599000	Yes.
599000	604000	Actually, the first thing we did was we uploaded my blog into this.
604000	614000	So this didn't become a publication, but we uploaded my blog into GPT-3 kind of lower power version of it.
614000	619000	Actually, not the Da Vinci model, but the Curie model, which is a slightly lower powered version.
619000	622000	And then we had it produce blog posts in the style.
622000	630000	And so readers can go if they want, look on my blog, the Splintered Mind, and they can see the GPT-generated blog posts, which were actually pretty interesting.
631000	636000	I think I write blog posts better, but it was kind of cool.
636000	637000	Yeah.
637000	646000	And at the time that we did this, it was not generally well known that GPT could create well-structured answers over long strings of text.
646000	650000	I mean, if you're thinking, it essentially does next word prediction.
650000	658000	So you would kind of think it would lose the thread of ideas over time and wouldn't be able to create a well-structured argument that runs for paragraphs.
658000	661000	Anyway, that's kind of what we thought, but it was surprising.
661000	676000	One of the blog posts was, I think, not a convincing argument, but at least it had a kind of philosophical argumentative structure over the course of several paragraphs, which we found really interesting and surprising, given the basic structure of these models.
676000	680000	Clearly, GPT does have some memory of what it's recently said, right?
680000	683000	It's not literally going from one word to another.
683000	684000	Right.
684000	694000	It's got a window of, I forget how, something like a thousand tokens, and a token's like three-quarters of a word, something in that ballpark.
694000	695000	So yes.
695000	703000	So did you give it topics for those blog posts, or did you just say write another blog post that would fit in here?
703000	709000	We gave it titles, and then it wrote the post given the title, yeah.
709000	710000	This is very scary.
710000	715000	Do you think that the world is going to change dramatically because of this technology?
715000	716000	Yes.
716000	717000	Yeah.
717000	718000	Okay.
718000	720000	It's hard to know exactly how it's going to change.
720000	721000	Hard to know how.
721000	722000	Right.
722000	723000	Yeah.
723000	724000	Okay.
724000	725000	But for sure, it's going to change.
725000	727000	So that was one thing I wanted to get out of the way.
727000	741000	The other one, which I only found when I saw your Wikipedia page, is that you were one of the people involved in asking the questions, are ethicists, especially ethical, or does studying moral philosophy make you a more moral person?
741000	743000	Is that right?
743000	744000	Yeah.
744000	748000	Arguably, I'm the world's foremost expert on the moral behavior of ethics professors.
748000	752000	And what are your conclusions about this?
752000	769000	Well, I've done a fair number of empirical studies on this, and what I found over and over again, almost without exception, is that they behave about the same as comparison groups of other professors.
769000	774000	So sometimes we do the comparison group would be other philosophers who don't specialize in ethics.
774000	781000	Sometimes the comparison group is other professors at the same university in different departments.
782000	789000	There have been a couple studies where we found a little bit worse or a little bit better behavior in some respects for ethicists.
789000	795000	But generally speaking, it's a big null result.
795000	796000	Well, okay.
796000	810000	You just find they behave the same as other people, which I think is kind of interesting because you might have thought that ethicists would reflect on ethics and then behave differently as a result of their reflections.
810000	815000	And mostly that seems not to be the case.
815000	826000	One particularly interesting example of this is with respect to vegetarianism because ethicists on some issues will embrace more demanding moral standards.
826000	834000	So for example, ethicists are much more likely than professors in departments other than philosophy, where they were in 2009 when we collected these data.
834000	841000	To say that it's bad to eat meat, bad to eat the meat of mammals in particular is what we asked about.
841000	849000	And yet in our research, we found them just as likely to report having eaten meat at their previous evening meal.
849000	851000	Now, is that correlated?
851000	857000	I mean, are the people who say it is bad to eat meat also eating the meat?
857000	858000	Right.
858000	860000	It had the correlation, they would think.
860000	865000	So the people who, we gave them a nine point scale from very morally bad to very morally good.
865000	876000	And the people who ticked one or two very morally bad or one tick toward good from that, few of them reported having eaten meat at their previous evening meal.
876000	887000	But the ones who ticked three or four on our nine point scale, which was a lot of ethicists, seemed basically just as likely to have reported eating meat.
887000	895000	So there was a individual, there was a kind of correlation between the strength of the opinion and the self reported eating.
895000	897000	But there wasn't the group difference that we expected.
897000	898000	Right.
898000	905000	So ethicists as a group said the majority, 60% said it was morally bad.
905000	916000	But I don't know, 30 some percent of them reported having done at their previous evening meal nonetheless compared to I think was 38% of ethicists.
916000	921000	And 37% of ethicists and 38% of respondents overall.
921000	929000	So if this is a surprising result, which I'll entertain the possibility that it is, what's your theoretical understanding of what's going on?
929000	935000	Is it just that ethicists are better at arguing about ethics, but not actually better at being ethical?
935000	940000	Is there a different kind of study that would make you better at ethics?
940000	942000	Is it like coaching versus playing a sport?
943000	948000	Yeah, I think coaching versus playing is an interesting comparison.
948000	955000	We don't hire ethicists to be saints, just like we don't hire coaches to be football stars.
955000	956000	Yeah.
956000	965000	And yet you would expect if you took a coach and a random member of the population of the same age and gender and put them on a football field together,
965000	971000	you would expect the coach would still outperform, even if the coach is not a superstar.
972000	973000	Right.
973000	980000	So I think that's an interesting comparison that reveals partly why you might think it's still a little surprising,
980000	985000	even if we don't hold ethicists to saint-like standards.
985000	992000	I mean, in the vegetarianism results again, I think that strikes people as somewhat surprising.
992000	993000	Yeah.
993000	1003000	So one of the things that I draw from this is it fits with a view I have about moral psychology.
1003000	1007000	I suspect that real answer is complex and multi-causal.
1007000	1016000	But one of the things I think it fits pretty well with is the idea that people in general aim to be morally mediocre.
1016000	1017000	Okay.
1017000	1026000	So my inclination is to think, and this is grounded both in personal experience and in reading social and moral psychology,
1026000	1033000	that people don't generally aim to be good or bad by absolute standards.
1033000	1039000	Instead, they aim to be about as morally good as their peers.
1039000	1046000	They don't want to make the sacrifices that would be involved in being very morally better.
1046000	1049000	But they also don't want to be the worst in the bunch.
1049000	1050000	Interesting.
1050000	1053000	So people aim for peer-relative moral standards.
1053000	1058000	So if you think about that from the point of view of thinking philosophically about ethics,
1058000	1063000	maybe what you do when you think about ethics is you discover moral truths,
1063000	1066000	like maybe you discover it's bad to eat the meat of mammals.
1066000	1073000	But if you're aiming just for peer-relative goodness, your peers are still eating meat.
1073000	1079000	So what happens is your opinion about your peers' moral behavior and your own moral behavior goes down,
1079000	1082000	but your behavior doesn't change.
1082000	1092000	That's actually very nicely consonant with other podcasts I've done recently about other aspects of things like psychology or even epistemology,
1092000	1097000	having much more of a social slant than we would expect.
1097000	1104000	Brian Lowry explained to us how our sense of selves serves mostly as social function.
1104000	1110000	Hugo Mercier explained how our use of reasons serves largely as social function.
1110000	1118000	And you're saying that there's a very big social function that is served by our ethical practice at any rate.
1118000	1120000	Yeah, right. Absolutely.
1120000	1122000	Social animals. There we are.
1122000	1130000	But I guess this does segue even more smoothly than I thought it would into the weirdness of the world,
1130000	1135000	because when you say the world is weird and now we're going to get into the topic of your book,
1135000	1141000	immediately part of me wants to say, come on, the world can't be weird.
1141000	1142000	It's the world.
1142000	1147000	What we're seeing is a mismatch between our expectations and the world,
1147000	1150000	and maybe those can be colored somehow.
1150000	1154000	So what is the title of your book mean?
1154000	1155000	Right.
1155000	1157000	So, yeah, you're right.
1157000	1160000	It's not that the world is intrinsically weird.
1160000	1163000	I'm not sure what that would mean.
1163000	1171000	The idea of weirdness or bizarreness, which is a closely related idea in my book,
1171000	1183000	involves violating our expectations or our standards or a sense of what's normal or violating a common sense.
1183000	1193000	So when I say that the world is weird, I mean something like our common sense understandings of how the world is
1193000	1199000	are going to be sharply violated by how the world actually is.
1200000	1202000	That's the bizarreness element.
1202000	1208000	And then there's also an element which I call dubiety, which is that, and all of our answers to this are dubious.
1208000	1215000	It's not like, oh, well, it violates common sense, but we perfectly well understand it.
1215000	1222000	It's also a dimension of weirdness is that it kind of exceeds our ability to fully comprehend.
1222000	1223000	Right.
1223000	1230000	The example you gave in the book is special relativity, which tells us various things that happen at the speed of light
1230000	1231000	might seem bizarre to us.
1231000	1235000	It's anti-common sensical, but it's not weird in the same sense.
1235000	1238000	We can fully understand it once we learn what's going on.
1238000	1239000	Correct.
1239000	1242000	So special relativity is a nice example of something that's highly bizarre.
1242000	1247000	It violates common sense, but it's not dubious.
1248000	1253000	The full weirdness thesis also involves this dubiety claim.
1253000	1254000	Yeah.
1254000	1258000	You use the word dubiety in your book much more often than I've ever seen it used before.
1258000	1268000	So just to let people know, it's the existence of, or I guess, the claim that you should be dubious about something.
1268000	1269000	Right.
1269000	1271000	That doubt is justified.
1271000	1272000	Doubt is justified.
1272000	1273000	Good.
1273000	1282000	So the name, the URL of my personal website is preposterousuniverse.com for exactly the same reason.
1282000	1289000	It came out of thinking about naturalness and cosmology and the cosmological constant and the fact that the universe is surprising to us.
1289000	1297000	And it's funny because I get critics, you know, the usual crackpot on the street with opinions about cosmology saying,
1297000	1299000	No, Sean Carroll thinks the universe is preposterous.
1299000	1302000	He doesn't realize that it's just our ideas that are wrong.
1302000	1305000	But I'm trying to say, like, no, that's the point.
1305000	1308000	I don't think the universe is making a mistake.
1308000	1310000	It's definitely we are making a mistake.
1310000	1314000	That's the whole message that is trying to get across.
1314000	1315000	Exactly.
1315000	1316000	And I like the word preposterous too.
1316000	1323000	I was flirting with just borrowing that word from you, but I decided I liked weirdness a little bit better.
1323000	1326000	Weirdness works, especially weirdness of the world, the alliteration, etc.
1327000	1333000	OK, so is then let's take the universe's point of view here.
1333000	1338000	You know, why is it our fault that the universe looks weird to us?
1338000	1347000	Is there something about us that is, despite the fact that we're part of the world that kind of doesn't quite match on to what we see out there in principle?
1347000	1348000	Right.
1348000	1355000	Well, I'm inclined to think that our theories and our common sense.
1355000	1357000	Well, especially our common sense.
1357000	1358000	Let's start with that.
1358000	1367000	Our common sense is trained upon, built upon a very limited range of experiences.
1367000	1368000	Yeah.
1368000	1369000	Right.
1369000	1378000	So with respect to big picture cosmology, right, it's relatively low energy, middle sized, slow moving moving stuff on Earth.
1378000	1379000	Right.
1379000	1380000	And that's what we're good at.
1380000	1382000	That's what we evolve to be good at.
1382000	1384000	That's what the social pressures and learning environment is.
1384000	1390000	Makes us good at you take something at a very different scale, much larger, much smaller, much more energetic.
1390000	1402000	And those aren't the kinds of things that there's any particularly good evolutionary or developmental or social reason to think we would have well tuned common sense judgments about.
1402000	1412000	So in other words, the way that I've said similar things in some cases, we have, we only experience a tiny fraction of what the world is.
1412000	1415000	And but we make efforts to experience more and more of it.
1415000	1416000	And guess what?
1416000	1418000	It looks different and surprising to us.
1418000	1419000	Yes.
1419000	1421000	I completely agree with that.
1421000	1428000	So I think the same is true for our understanding of consciousness, although less, less obviously so.
1428000	1429000	Right.
1429000	1439000	So the thesis of the weirdness of the world is partly about large big picture cosmological issues.
1439000	1442000	It's also equally or even more about consciousness.
1442000	1443000	Yeah.
1443000	1444000	Right.
1444000	1457000	So again, with respect to issues of consciousness, we're familiar with the human case and with, you know, certain familiar vertebrates we like.
1457000	1461000	And that's about what we know about consciousness.
1461000	1466000	And we have not had experience, for example, with sophisticated AI systems.
1466000	1467000	Yeah.
1467000	1476000	So we shouldn't expect particularly to have well tuned common sensical intuitions about such things.
1476000	1477000	Good.
1477000	1486000	And so in some sense, I don't want to put words in your mouth, but it seems to me that the model of your book should be that you're asking us to be courageous.
1486000	1502000	To say, you know, we should expect that as we learn more and more about the world, we'll find that what we're learning seems weird to us and we need to develop tools and techniques to deal with that and handle it.
1502000	1504000	Yeah, absolutely.
1504000	1514000	And yeah, I guess that's in a sense courageous, although I'm not sure that's the word I would use.
1514000	1517000	I like the word wonderful instead.
1517000	1518000	Okay.
1518000	1523000	Because it's got this, it's got to the idea of wonderful, it's got two dimensions to it, right?
1523000	1531000	So it's got the root sense of wonder, right, that we live in a world that promotes wonder in us.
1531000	1538000	And wonderful, of course, is also something like a synonym for one for good.
1538000	1545000	So I think that's a good thing about the world that it's going to defies our understanding.
1545000	1560000	So you have lessons and nostrums that we should get from contemplating the weirdness of the world, but I thought that maybe we could just go through some examples, really think about them in depth, and that will help us extract what these lessons are.
1560000	1570000	And as you said, consciousness is a big one, but I first wanted to just talk about the existence of the external world, you know, these radically skeptical scenarios.
1570000	1577000	Like you talk a lot about, are we sure that we're awake, for example.
1577000	1579000	So give us the general lay of the land here.
1579000	1585000	Like how do you think about these skeptical possibilities and what are your favorite ones?
1585000	1587000	Right.
1587000	1594000	So you have a few different chapters where I tangle with these skeptical possibilities in different ways.
1594000	1597000	The dream argument, of course, is a famous one.
1597000	1599000	That's the one that you started with.
1599000	1604000	But I'm also interested in the simulation hypothesis, the idea that we might be living in a simulation.
1604000	1611000	The Boltzmann brain idea, which you, of course, talked about in your work very wonderfully.
1611000	1619000	And, you know, just the idea that even the idea that the universe might consist wholly of my own mind and nothing else.
1619000	1620000	Right.
1620000	1624000	So I talk about all of those possibilities in the book.
1624000	1628000	But we could start with the dream one, which is maybe the most familiar one.
1628000	1629000	Sure.
1629000	1638000	So this goes back in philosophy all the way at least to the ancient Chinese philosopher Zhuangze, although, of course, Descartes makes famous use of it.
1638000	1639000	Right.
1639000	1647000	The idea is how do you know, if you do know, that you're not currently dreaming right now?
1647000	1654000	And normally we think we feel pretty confident that we're awake.
1654000	1659000	At least if you ask a waking person, if they're confident they're awake, they'll tend to say yes.
1659000	1660000	Yeah.
1660000	1663000	But what justifies that?
1663000	1667000	And I think there are a few ways it could be justified.
1667000	1678000	I think there are some empirical features of dreams that make them different from waking life.
1678000	1686000	So, for example, I think of waking sensory experience as pretty richly detailed and pretty stable.
1686000	1698000	Whereas the experiences we have in dreams, arguably, are less detailed, kind of sketchier, more image-like, less stable.
1698000	1701000	Do you know this claim that in dream?
1701000	1702000	Go ahead.
1702000	1711000	There's a claim that I think actually has some backing, but I'm not sure how right it is, that you can't read text in dreams.
1711000	1713000	The text does not look like text.
1713000	1720000	It looks like that sort of bad AI text that's sort of like letter-like without actually having any meaning.
1720000	1724000	Yes, that is sometimes called a dream sign.
1724000	1729000	These are hypothesized tests for whether you're dreaming or not.
1729000	1732000	So, look at text and see if it's stable and if you can read it.
1732000	1736000	And some people think of that as, for themselves, a good dream sign.
1736000	1742000	But of course, there are also dream reports in which people report reading stable text, so it's not universally accepted.
1743000	1757000	So, I think that if we accept this fact about the stability of text as maybe, at least in the majority of dreams, you can't have a stable text,
1757000	1764000	that creates some evidence that I'm not dreaming right now.
1764000	1776000	But a lot of dream researchers, including, for example, Jennifer Vint, who I think is really amazingly knowledgeable about this kind of stuff,
1776000	1786000	think that we do often in dreams have stable experiences that are a lot like waking life,
1786000	1791000	maybe even experientially indistinguishable from waking life.
1791000	1798000	Even boring mundane experiences like that of listening to a podcast.
1798000	1803000	That's a very exciting experience, Eric.
1804000	1814000	And so, if we think that there are some dreams like this, or if we invest some credence in a theory of dreaming,
1814000	1823000	according to which there are either often, or at least sometimes, experiences like I'm having right now,
1823000	1831000	or like your listeners or you are having right now, in dreams, then it becomes kind of less experientially obvious.
1831000	1844000	Okay, I can't now be sure that I'm not dreaming based on what seems to be this stable experience that I'm having right now,
1844000	1846000	of seeming to be awake.
1846000	1852000	And of course, lots of people, including me and probably most of your listeners,
1852000	1858000	have had false awakening experiences where you kind of seem to wake up and think,
1858000	1864000	oh, I just had a dream, now I'm awake, right, and then you wake up again.
1864000	1873000	So there are reasons, I think, not to be perfectly certain that you are not dreaming right now.
1873000	1881000	Is it also worth contemplating a kind of inception scenario where we're all dreaming?
1881000	1891000	There's a more awake version of us that dreams like our existence, pretty detailed, we can read text, etc.,
1891000	1898000	and then we dream that we are dreaming in this fuzzier way, right?
1898000	1909000	I think that's possible, but I want to draw a distinction between what I think of as grounded and ungrounded skepticism.
1909000	1917000	So ungrounded skepticism says, well, I could be a brain in a vat, and then I wouldn't be able to tell the difference.
1917000	1919000	So can I really rule that out?
1919000	1928000	An ungrounded dream skepticism could be like, oh, well, maybe we're in an inception scenario.
1928000	1936000	Grounded skepticism starts with our ordinary background assumptions and says,
1936000	1943000	looking at these assumptions, there's some positive reason to give some credence to skeptical doubt, right?
1943000	1951000	So there's no real positive reason to think that you're a brain in a vat to assign that any more than the most trivial likelihood, right?
1951000	1960000	There's no positive reason to think we're in an inception scenario, but there is positive reason to think that this experience might be a dream experience.
1960000	1968000	Once we start thinking about the nature of dream experiences and weather experiences like this, at least maybe sometimes occur in dreams,
1968000	1972000	at least according to some theories that might be true, right?
1972000	1979000	So I prefer to focus on these grounded kind of sources of skepticism.
1979000	1986000	So I think it's not just, I mean, one of the critiques that philosophers sometimes give with skepticism is you could cook up anything.
1986000	1989000	There's no reason for us to take it seriously.
1989000	1993000	Whereas I think with dream skepticism, given the fact that we dream every night,
1993000	2001000	given that theories of dreams, at least some mainstream theories of dreams postulate that we have experiences like this in our sleep,
2001000	2003000	that creates grounds for doubt.
2003000	2006000	It's not completely just cooked up out of nothing.
2006000	2014000	Maybe you can add to that the idea that at least most of the time while we're dreaming, we don't think of ourselves as dreaming.
2014000	2015000	Right?
2015000	2016000	Most of the time.
2016000	2019000	Of course, there are some so-called lucid dreams.
2019000	2027000	And it is the case that if you can get yourself in the habit of thinking,
2027000	2034000	am I awake so much that that thought starts to come to you while you're actually dreaming,
2034000	2039000	then that's one way to discover that you're dreaming and start to have lucid dreams.
2039000	2040000	I'm very bad at remembering.
2040000	2044000	Even in the dream, sometimes people will say, am I dreaming and then decide,
2044000	2046000	no, I'm not dreaming, even though they really are.
2046000	2049000	That's just what I was going to ask because I was just going to say,
2049000	2051000	I don't remember my own dreams very well at all,
2051000	2055000	but I don't have any memory of ever being in a dream saying,
2055000	2057000	I wonder if I'm in a dream and then going,
2057000	2059000	nope, I don't think I am.
2059000	2063000	But you're saying other people have reported that experience.
2063000	2066000	Yes, that's definitely not an uncommon experience.
2066000	2067000	All right.
2068000	2072000	It could be the case that the majority of times when people are dreaming
2072000	2074000	and think to themselves explicitly, am I dreaming?
2074000	2076000	They come to realize they are,
2076000	2079000	but it's not clear that that's the majority.
2079000	2082000	And even if it is, it's not an overwhelming majority.
2082000	2085000	Yeah, okay, good.
2085000	2088000	Let's just go through some of the other famous ones
2088000	2093000	because you draw a very interesting distinction between grounded and ungrounded skeptical scenarios.
2093000	2096000	What are some of the other skeptical scenarios
2096000	2099000	you would classify as grounded, in other words, worthy of our attention?
2099000	2103000	So I find the simulation hypothesis pretty interesting.
2103000	2107000	So this is the idea that we might be artificial intelligences
2107000	2112000	living inside a simulated reality, kind of like the matrix.
2112000	2115000	Except in the matrix, they're really biological bodies,
2115000	2117000	but you might, you could have a matrix type scenario
2117000	2123000	where the confused entities are AI systems, right?
2123000	2126000	Or you could imagine the video game, the Sims,
2126000	2129000	with these artificial simulated people going around these environments,
2129000	2131000	except the Sims are really conscious.
2131000	2135000	There's a couple of ways of thinking about the simulation hypothesis, right?
2135000	2142000	So Nick Bostrom has a famous argument that gives us some grounds
2142000	2147000	for thinking it's at least possible that we are Sims, right?
2147000	2156000	So the idea here would be that it's not ridiculous to think
2156000	2163000	that consciousness could arise in artificially intelligent computational systems, right?
2163000	2165000	Philosophers have disputed that.
2165000	2168000	John Searle, who was actually one of my dissertation advisors,
2168000	2170000	is one of the most famous skeptics about that.
2170000	2176000	But there's, it's certainly no consensus that it's impossible.
2176000	2184000	So if we accept, give at least some credence to the possibility
2184000	2190000	that artificially intelligent beings could arise on computers,
2190000	2194000	then it seems possible that such beings could exist
2194000	2200000	in simulated artificial environments that they take to be their own,
2200000	2204000	the base level of reality, as Bostrom puts it.
2204000	2211000	Some of them might even think they're living in Earth in the 21st century.
2211000	2216000	And then the question arises, okay, how many such beings are there?
2216000	2221000	And one possibility would be, look, you know, they're not ever going to be beings
2221000	2223000	like this out there, right?
2223000	2225000	The civilization will not get that far enough.
2225000	2229000	Maybe it's really expensive to make such things and no one would bother.
2229000	2233000	Or maybe there'd be some ethical regulations, like you don't want to create real,
2233000	2236000	you want to create really conscious entities inside your computer
2236000	2239000	who think they're living in reality, right?
2239000	2243000	But on the other hand, it also seems like it's possible
2243000	2246000	that there would be many such beings, right?
2246000	2250000	Just like we run computer games like the Sims right now
2250000	2252000	and we run scientific simulations,
2252000	2256000	it could be the case that there are lots of games or scientific projects
2256000	2260000	that involve real conscious beings inside them
2260000	2263000	who think they're living in the base level of reality.
2263000	2267000	If the universe contains many such beings,
2267000	2273000	then it seems not totally implausible to think we might be among them.
2273000	2278000	So there are various reasons to think we're probably not them, right?
2278000	2282000	Every step of this argument admits of doubt.
2282000	2286000	And you kind of stack those doubts on top of each other
2286000	2288000	and it seems like, okay, probably not.
2288000	2291000	But again, it's like with the dream case,
2291000	2296000	it doesn't seem like we should be absolutely certain that we're not Sims.
2296000	2299000	So yeah, I guess it could be.
2299000	2302000	So I find that possibility interesting.
2302000	2306000	And then I guess to turn this into a skeptical scenario,
2306000	2310000	so one of the things that David Chalmers has particularly emphasized
2310000	2312000	in talking about this is he says,
2312000	2316000	well, look, if you're living in a simulation but it's large and stable,
2316000	2319000	then that's not really a skeptical scenario at all.
2319000	2324000	He says, because you have a long past, you're going to have a long future.
2324000	2327000	All the people you know really exist.
2327000	2330000	And there might be, say, a coffee mug,
2330000	2333000	and it might be fundamentally made out of computational bits,
2333000	2335000	but that's enough.
2335000	2337000	It's still going to be there.
2337000	2339000	It's going to react the way that you want.
2339000	2342000	So it really is kind of a coffee mug.
2342000	2347000	Well, basically, most of your ordinary beliefs would end up being true.
2347000	2352000	So to turn this into a skeptical simulation scenario,
2352000	2354000	what we have to do is think about what's the possibility
2354000	2359000	that if we're living in a Sim, it's a small or unstable one.
2359000	2363000	And there I guess I'm inclined to disagree with Chalmers.
2363000	2366000	And I'm inclined to think that if we are in a Sim,
2366000	2370000	there is a decent chance that it's a small or unstable one.
2370000	2373000	So if we think about the simulations we run,
2373000	2376000	they tend to be small and unstable.
2376000	2380000	If we think about the question of resources,
2380000	2385000	it probably would take a lot of resources to run a whole galaxy
2385000	2389000	from the Big Bang through now and on into the future.
2389000	2392000	So what scientist is really going to want to do that
2392000	2395000	if maybe all they're interested in is human cognition?
2395000	2398000	So just run a short, a few people having a discussion on a podcast
2398000	2400000	or something like that.
2400000	2404000	So I think conditional upon thinking we're living in a Sim,
2404000	2408000	we should assign a substantial portion of that credence,
2408000	2414000	maybe 50%, maybe 90%, maybe 10% to it's being a smaller, unstable simulation.
2414000	2420000	And in that case, then that in my mind counts as a radically skeptical scenario
2420000	2422000	because you might be radically wrong.
2422000	2425000	Maybe this whole simulation was created only 10 minutes ago,
2425000	2428000	maybe there's no one outside of your room,
2428000	2430000	it's just you listening to a podcast
2430000	2432000	or just the two of us having a conversation
2432000	2435000	and beyond the walls of our rooms, nothing exists.
2435000	2438000	Those would be various ways of it's being smaller and stable.
2438000	2440000	So as a professional philosopher, of course,
2440000	2445000	you know that the idea of these skeptical scenarios goes back to antiquity,
2445000	2449000	not just Zhuangzi, but the ancient Greeks thought about this.
2449000	2453000	So we've been worried for millennia now
2453000	2457000	that reality is not anything like what we think it is.
2457000	2463000	Absolutely. I love the ancient skeptics.
2463000	2466000	So what are some of your favorite ones?
2466000	2469000	Well, Zhuangzi is probably my favorite philosopher,
2469000	2473000	but Sexist Empiricus is also really wonderful.
2473000	2477000	And they did not know about the simulation argument.
2477000	2480000	Right, they didn't know about the simulation argument,
2480000	2486000	but that opens up the possibility that there are some rounds for doubt
2486000	2491000	that future philosophers and physicists will think of
2491000	2495000	that didn't even occur to us.
2495000	2501000	And maybe we should have some degree of skepticism reserved for that.
2501000	2503000	Right, I call this wildcard skepticism.
2503000	2509000	The idea that I should have a certain amount of doubt
2509000	2512000	about my ordinary assumptions about the world,
2512000	2515000	just on the basis of the fact that there's some skeptical possibility
2515000	2518000	that I'm not even capable of considering.
2518000	2520000	Right. Okay, good.
2520000	2522000	So there are other skeptical scenarios,
2522000	2526000	but we have the general feeling between living in a simulation,
2526000	2529000	we're just dreaming, what do we do about it?
2529000	2535000	Do we just ignore those possibilities because they're too weird?
2535000	2538000	Do we reserve a little bit of our credences to say,
2538000	2543000	who knows, maybe tomorrow I'll change my mind and think this is right?
2543000	2546000	Yeah, I think we reserve a little bit of our credence.
2546000	2551000	So the way that I think about it in terms of numerical credence
2551000	2558000	is that I think it's rational to assign about a 0.1 to 1% credence
2558000	2565000	to some radically skeptical scenario or other being correct.
2565000	2568000	That's rough and fuzzy.
2568000	2573000	It's somewhere between just being completely confident they're false
2573000	2578000	and being radically uncertain.
2578000	2584000	So 99.9%, the world's basically just how we think it is.
2584000	2588000	Setting aside the big picture cosmological stuff,
2588000	2594000	the ordinary Earth world of middle-sized dry goods at slow speeds
2594000	2596000	is more or less how we think.
2596000	2600000	99.9% of our credence maybe should go to that,
2600000	2603000	but save a little bit of your credence space, so to speak,
2603000	2606000	for these radically skeptical possibilities.
2606000	2611000	And then I think that having that little bit of space there
2611000	2614000	has some influence on your choices and your behavior.
2614000	2617000	Actually, I do want to get to exactly that issue,
2617000	2620000	but I realize there's a hanging thread that we should deal with,
2620000	2627000	which is there's another kind of way in which the world could be very different
2627000	2632000	than what we think it is, which is just there is a lower microscopic level
2632000	2636000	beneath our manifest image kind of world, right?
2636000	2639000	So you're not counting.
2639000	2642000	We are not talking about something like,
2642000	2647000	oh, there's a whole new theory where everything is little strings
2647000	2649000	or wave functions or whatever.
2649000	2651000	That doesn't count because in those scenarios,
2651000	2656000	the macroscopic world is still the macroscopic world and obeys the rules, right?
2656000	2658000	Exactly, right.
2658000	2661000	So I don't count that as a radically skeptical scenario of the relevant.
2662000	2667000	Because there still would be our everyday beliefs would still mostly be true, right?
2667000	2671000	It would be true that Earth has existed for billions of years
2671000	2674000	and it would be true that there's a coffee mug here
2674000	2678000	and that sort of stuff and the sun will rise tomorrow, so to speak.
2678000	2680000	Good, okay, good.
2680000	2686000	So then we can go back to that 1% skepticism that you're advocating.
2686000	2689000	I mean, one question is just where did that number come from?
2689000	2693000	You're saying we should attach a 10 to the minus 2, 10 to the minus 3 credence
2693000	2695000	to just being completely wrong.
2695000	2702000	I absolutely agree that we should attach some credence to any crazy idea you have.
2702000	2707000	I'm a disbeliever that you should attach zero credence to almost anything,
2707000	2709000	but why not tend to the minus 10?
2709000	2714000	Why something as big as 10 to the minus 2 or 3?
2715000	2721000	I don't have a rigorous argument for that,
2721000	2728000	but let me just do it for, say, the dream scenario, right?
2728000	2735000	So let's say we invest a 20% credence in the theory,
2735000	2738000	which some major dream researchers accept,
2738000	2743000	that we commonly have experiences like we're currently having in our dreams.
2743000	2747000	Let's say we get 20% credence to that and 80% credence to now dreams
2747000	2749000	are basically just always sketchy.
2749000	2753000	Then conditional on that, we say,
2753000	2759000	okay, how often do I have experiences relevantly like this in dreams?
2759000	2766000	Well, maybe this is not the kind of thing that I would tend to dream about very much.
2767000	2774000	Again, if we ordinarily have ordinary sensory experiences of mundane things,
2774000	2777000	it seems like this is the kind of thing that we should have.
2777000	2787000	So maybe I should assign a 2% of the time I'm having experiences like this
2787000	2790000	is actually in sleep, or I should invest 2% of my credence
2790000	2795000	to the idea that I have experiences like this in my sleep.
2795000	2799000	Now, once I've attached a 2% credence to the idea that I have experiences like this
2799000	2803000	commonly in my sleep, it's hard for me to see on the basis of that
2803000	2808000	how you would then go, okay, so I now only have a 1 in 10 billion credence
2808000	2811000	that this is a dream, right?
2811000	2819000	It seems like you can't knock too many orders of magnitude down off that 2%.
2819000	2824000	I'm not sure what the grounds would be for that kind of decrement.
2825000	2832000	I think it's a big philosophical problem that I don't know whether you know
2832000	2835000	if anyone sort of specializes in this particular question,
2835000	2840000	but what do we do with issues or questions or scenarios
2840000	2844000	where the credence that it'll happen is incredibly tiny,
2844000	2848000	but the consequences of it happening are incredibly large.
2848000	2853000	And it seems maybe AI destroying the world is an example of that
2853000	2857000	or even better because people think that AI destroying the world might be 10%,
2857000	2861000	but the large Hadron Collider turning on in a black hole eating up the world
2861000	2865000	and someone says, well, it's less than a 1 in 10 billion chance.
2865000	2869000	And someone else says, but it destroys the world, that should still count.
2869000	2871000	How do we reason about those cases?
2871000	2875000	1 in 10 billion times 10 billion people, so...
2875000	2877000	That's why I chose the number, right.
2877000	2880000	One expected murder as soon as you turn it on.
2882000	2885000	But even the number 1 in the 10 billion, I mean, if someone said,
2885000	2888000	oh, no, it's really only 1 in 10 million or it's 1 in 10 trillion,
2888000	2892000	probably I couldn't give a principled argument in any direction.
2892000	2893000	Right.
2893000	2895000	So I don't know how to deal with them.
2895000	2897000	You talk about those kinds of magnitudes, sometimes it's hard to get...
2897000	2901000	You lose your sense of magnitude once you get over, I don't know,
2901000	2903000	a trillion or something like that.
2903000	2907000	So do philosophers have a toolkit for dealing with these weird numbers?
2909000	2910000	No.
2911000	2912000	It's becoming...
2912000	2917000	It's an interesting issue that's been starting to get attention in the philosophical literature.
2919000	2923000	There's this idea that's sometimes called Nicolossian discounting,
2923000	2930000	which is the thought that once something has a low enough chance
2930000	2934000	or you give a low enough credence to it, you just ignore it completely.
2934000	2935000	Yeah.
2935000	2936000	Yeah.
2936000	2937000	So that is one approach.
2937000	2938000	What was that called?
2938000	2939000	And I...
2939000	2942000	Nicolossian discounting.
2943000	2948000	After someone named Nicholas, but I've forgotten which Nicholas it is.
2950000	2953000	So, right, so for example, in the weirdness of the world,
2953000	2960000	I suggest that once you give something 10 to the negative 30 credence,
2960000	2963000	you kind of just forget about it.
2963000	2968000	And this helps solve certain kinds of puzzles and paradoxes in decision theory.
2970000	2973000	But there are also arguments against this.
2973000	2975000	So there's been back and forth about this.
2975000	2979000	This also comes up in the debate about the ethics of long-termism.
2979000	2980000	Right, exactly.
2980000	2984000	Right, so long-termism is this idea in the effective altruism movement
2984000	2992000	that there's a small chance that things we do today could have a huge impact
2992000	2994000	on a huge number of lives in the future.
2995000	3000000	Right, so for example, if humanity goes extinct now,
3000000	3007000	then maybe there are no other entities in our galaxy who will ever have the kinds of lives
3007000	3010000	with the kind of value that our lives have, right?
3010000	3012000	And if we manage not to go extinct now,
3012000	3017000	then maybe we will have 10 to the 40 happy descendants
3017000	3020000	before the heat death of the universe, right?
3020000	3024000	So even if there is a one in a quadrillion chance
3024000	3028000	that something you did now could prevent humanity from going extinct,
3029000	3035000	given the stakes, maybe you should invest a huge amount in that tiny little chance.
3037000	3042000	So that's an interesting issue that's been coming up recently with long-termism
3042000	3048000	where this issue about what do you do when you're trying to balance tiny credences
3048000	3050000	and giant values against each other?
3050000	3055000	Do you have a take on what we should do, or is it just an open kind of question?
3058000	3059000	So I have two takes.
3059000	3065000	One is I'm still going to stand by Nicolassian discounting.
3065000	3066000	Okay.
3066000	3075000	And the other take is radical ignorance about what actions that are currently available to us
3075000	3077000	would have good versus bad effects.
3077000	3088000	So the long-termists will typically say or assume that human extinction is likely to have a bad effect
3088000	3091000	instead of a good effect on future history.
3091000	3095000	I don't think that that's necessarily true.
3096000	3107000	So for example, maybe humanity, because we are so technological and prone to violence,
3107000	3115000	is a kind of unstable species that is more or less certain to doom itself
3115000	3122000	sometime in the next 10,000 to 100,000 years we're going to blow ourselves up.
3122000	3131000	And if we do it in an explosive way, then we ruin the Earth for other future inhabitants.
3131000	3136000	But if we were to say bow out peacefully now by deciding never to reproduce again,
3136000	3144000	as anti-natalists suggest, then maybe we leave the Earth in a good position for other species
3144000	3153000	like say dolphins, who might have descendants that are capable of lives as rich as ours
3153000	3156000	but who aren't technological, aren't going to blow themselves up
3156000	3160000	and could endure potentially four billions of years on the planet
3160000	3165000	or maybe not billions, but maybe a billion.
3165000	3170000	If that's the case, then and you put the numbers into the equation in a certain way,
3170000	3174000	it turns out that would be better from a kind of long term perspective for human beings
3174000	3179000	to peacefully extinguish ourselves now.
3179000	3181000	So I'm not saying that's true.
3181000	3187000	What I'm saying is it's very hard to know what really,
3187000	3190000	when you take a billion year time perspective,
3190000	3198000	what really is kind of objectively good versus bad among options that we have available to us now.
3198000	3200000	I think that's fair.
3200000	3205000	So radical ignorance about the very distant future.
3205000	3212000	And relatedly then, what is the actionable fact about 1% skepticism?
3212000	3217000	Like how does it affect our daily lives to think that maybe there's a 1% chance
3217000	3222000	or a tenth of a percent chance that I'm dreaming or that I'm living the simulation?
3222000	3225000	Right.
3225000	3231000	Well, for example, just a fun example to start with,
3231000	3240000	I had been reading a lot about dream skepticism and it was particularly vivid for me this one particular winter break
3240000	3244000	when I was walking across campus and no else was around.
3244000	3246000	I was thinking, I wonder if I'm dreaming.
3246000	3250000	Maybe I should try to fly because I'm probably not dreaming.
3250000	3253000	But look, if I'm dreaming, it would be so awesome to fly.
3253000	3255000	No one's around, no one's looking.
3255000	3257000	There's no cost.
3257000	3260000	I was kind of walking across campus to the science library to get a book, right?
3260000	3263000	So why not just like try to fly to the library?
3263000	3267000	So I did try to fly to the library and I failed.
3267000	3275000	But I think that was a rational decision because I could have been dreaming
3275000	3277000	and then it would have been awesome.
3277000	3281000	It was not so rational that you would have done it had people been watching.
3281000	3282000	Right.
3282000	3284000	So low cost things, right?
3284000	3287000	Don't try to fly when you're standing on the edge of the cliff.
3287000	3291000	Plug that into your utility calculus and you will not get a positive result.
3291000	3297000	But if they're very small or no costs to trying to fly,
3297000	3304000	then why not if you think this might be a dream and could fly?
3304000	3306000	Of course, as soon as you try to fly and fail,
3306000	3309000	that should reduce your credence either that this is a dream
3309000	3313000	or that if this is a dream, you could fly.
3313000	3315000	So you might, it might not be repeatable.
3315000	3317000	You might not just be spending all your time trying to fly.
3317000	3319000	In my dreams, I can fly at least a little bit.
3319000	3321000	I can float.
3321000	3323000	Right.
3323000	3326000	It's just a matter of willpower.
3326000	3329000	Anyway, this is all, you know, fun, but it is 1% stuff
3329000	3332000	and a lot of the book, I don't want to give people the wrong impression.
3332000	3335000	A lot of your book, you're talking about consciousness,
3335000	3339000	which has the feature that we're all familiar with it.
3339000	3341000	I don't even want to say the feature that it exists
3341000	3344000	because people argue about that, but at least we're all familiar with the idea.
3344000	3348000	So what is it about our attempts to understand consciousness
3348000	3352000	that drives us into the weird zone?
3352000	3354000	Right.
3354000	3359000	So philosophers have tried over and over again for centuries
3359000	3364000	to make sense of how consciousness fits into the world.
3364000	3371000	And one of the striking empirical facts about the history of philosophy
3371000	3377000	is that every single attempt to make sense of this is jaw-droppingly bizarre.
3377000	3384000	So there are, I divide attempts to deal with the question
3384000	3389000	of how consciousness fits into the broader world into four broad categories.
3389000	3391000	One is substance dualism.
3391000	3393000	You've got an immaterial soul.
3393000	3395000	Another one is materialism.
3395000	3397000	There are no immaterial souls.
3397000	3399000	You're just a biological entity.
3399000	3401000	Another one is idealism.
3401000	3404000	This is the idea that there is no material world at all.
3404000	3407000	All that exists are minds or souls.
3407000	3411000	And then there's what I call compromise slash rejection views,
3411000	3414000	this kind of grab bag of other alternatives.
3414000	3418000	And the striking thing to me about this is
3418000	3425000	all of these alternatives end up committing to bizarre and dubious
3425000	3428000	theses of one form or another.
3428000	3433000	There's not really a live option here that is non-bizarre.
3433000	3434000	It's not always obvious.
3434000	3437000	I mean, idealism is bizarre on its face, I think.
3437000	3440000	It's contrary to common sense to think there's no material world,
3440000	3442000	and it's only just minds.
3442000	3446000	Dualism and materialism are not maybe bizarre in their face,
3446000	3449000	but once you try to get into the metaphysical details
3449000	3451000	and think about how it really works,
3451000	3455000	end up pretty swiftly faced with theoretical choices
3455000	3458000	where there are going to be bizarre consequences
3458000	3462000	for any of the choices that you make.
3462000	3463000	I think that's the important point,
3463000	3465000	because people are going to hear you say that,
3465000	3468000	and they'll be both materialists and dualists in the audience
3468000	3472000	who go, I have no trouble thinking that consciousness works that way.
3472000	3477000	But your point is that if you really take the consequences of that view
3477000	3482000	seriously, you're led to something that we should recognize as bizarre.
3482000	3484000	Correct.
3484000	3485000	Right.
3485000	3487000	So why don't you tell us for Ethan Pink one?
3487000	3490000	So for example with dualism, right?
3490000	3498000	The dualist faces two questions where all of the answers seem to be bizarre.
3498000	3503000	One concerns what kinds of entities have souls,
3503000	3510000	and the other concerns the causal relationship between material world and souls.
3510000	3514000	The causal question is a little more complicated,
3514000	3518000	so let me just talk about the question of what entities have souls.
3518000	3520000	Basically, you have four choices.
3520000	3524000	You could say only humans have souls,
3524000	3528000	or you could say everything in the world has a soul.
3528000	3530000	Both of those are pretty bizarre,
3530000	3533000	because if you think souls are the locus of consciousness,
3533000	3537000	then except the first, like Descartes,
3537000	3540000	then you think dogs and cats aren't conscious.
3540000	3542000	So there's this story of Descartes taking a cat
3542000	3545000	and throwing it out of a second-story window saying,
3545000	3547000	see, cats, they're just machines.
3547000	3548000	I never heard that story.
3548000	3551000	He probably didn't actually do this.
3551000	3552000	Man.
3552000	3555000	But that story kind of reveals the bizarreness of those
3555000	3558000	that only humans have souls.
3558000	3561000	And of course the panpsychist view that everything has a soul,
3561000	3565000	even say a proton, that's also pretty bizarre.
3565000	3567000	So then there are two other options.
3567000	3571000	One is that there's a sharp line somewhere.
3571000	3578000	So dogs have souls, cats have souls, but not frogs.
3578000	3580000	Where do you draw that line?
3580000	3582000	Across the continuum of animals,
3582000	3585000	it seems like there's a continuum of psychological capacities,
3585000	3587000	a continuum of physiology.
3587000	3589000	It would be weird if you said, OK,
3589000	3591000	toads of this genus have souls,
3591000	3593000	toads of this other genus do not.
3593000	3598000	So a sharp line is pretty implausible,
3598000	3600000	at least bizarre.
3600000	3602000	And then that gives you maybe the fourth option,
3602000	3605000	which is having a soul is not an on or off thing.
3605000	3608000	You could have a kind of soul or a half soul.
3608000	3610000	Dogs have an eighth of a soul.
3610000	3612000	What would that even mean?
3612000	3615000	We normally think of souls as things that either have or don't have.
3615000	3618000	It seems like a discrete category rather than a graded category.
3618000	3621000	But you've got to take one of those horns,
3621000	3623000	but they're all bizarre.
3623000	3627000	So that illustrates why I think on the face of it,
3627000	3630000	it seems like having a soul is not a bizarre view.
3630000	3632000	But as soon as you face that choice of saying, OK,
3632000	3634000	what animals have souls?
3634000	3638000	You're forced into committing to some
3638000	3640000	strikingly bizarre position.
3640000	3642000	And as a matter of fact,
3642000	3644000	this was a hot topic in ancient philosophy,
3644000	3646000	which animals have souls, right?
3646000	3648000	Exactly.
3648000	3652000	And the modern conception of the soul as a locus of consciousness
3652000	3655000	is a little different from an ancient philosophy.
3655000	3658000	There was the vegetative soul as well.
3658000	3661000	So there's a sense in which even plants had souls,
3661000	3663000	but they didn't think of souls maybe as a locus of consciousness.
3663000	3667000	So actually, the concept of a soul that we find
3667000	3670000	in our current philosophical and religious tradition
3670000	3672000	has a certain history.
3672000	3674000	It is not a straightforward translation from, say,
3674000	3676000	ancient Greek.
3676000	3678000	So it's complicated, but it's been an issue
3678000	3681000	throughout philosophical history.
3681000	3683000	But here at the Mindscape Podcasts,
3683000	3686000	we are hardcore materialists about consciousness.
3686000	3689000	So tell us why that leads us to weirdness.
3690000	3699000	Well, one of the issues here to think about
3699000	3704000	is, again, what kind of entities have experiences.
3704000	3708000	I guess there are a few different ways to angle in on this.
3708000	3718000	But one of them, and this is the theme of Chapter 3
3719000	3727000	is to point out that according to a broad class
3727000	3731000	of materialist theories, it's very plausible
3731000	3735000	that the United States has a stream of conscious experience.
3735000	3739000	The United States conceived of as a concrete entity
3739000	3742000	with people as its parts, kind of like you are a concrete entity
3742000	3744000	with cells as your parts.
3744000	3747000	So think about that concrete thing
3747000	3750000	with hundreds of billions of people.
3750000	3752000	Hundreds of millions.
3752000	3754000	Sorry, hundreds of millions.
3754000	3756000	Sorry, I misspoke there.
3756000	3758000	Don't want you misquoted, yeah.
3760000	3766000	So that entity processes a lot of information.
3766000	3770000	That entity represents itself.
3770000	3773000	That entity responds to its environment,
3773000	3776000	kind of intelligently or semi-intelligently.
3777000	3780000	You scan space for asteroids that might threaten Earth
3780000	3783000	and we're prepared to try to deal with them if that happens.
3783000	3786000	The United States monitors its borders.
3786000	3788000	It engages in import and export.
3788000	3791000	It sends its army out to do certain things.
3791000	3796000	It scolds people in UN Security Council meetings.
3796000	3800000	It digests bananas, mounts the bananas.
3800000	3803000	It exudes smoggy exhalations.
3804000	3808000	So if you take standard,
3808000	3812000	materialist theories of consciousness out of the box
3812000	3815000	and you don't rule around with them post-hoc
3815000	3817000	to try to exclude the case,
3817000	3822000	then I think it turns out that probably
3822000	3826000	the United States is going to count as conscious.
3826000	3830000	So there are a couple of ways to react to this.
3830000	3834000	You could say, okay, well, so much the worse for materialism.
3834000	3836000	Or you could say, okay, well, look,
3836000	3838000	we need to mess around with these theories
3838000	3843000	to exclude this bizarre possibility.
3843000	3849000	And I think that is maybe a reasonable response.
3849000	3852000	One question here is how we know
3852000	3854000	the United States is not conscious
3854000	3856000	whether you should take that as a fixed point
3856000	3859000	in our theorizing about consciousness or not.
3859000	3861000	You know, and the third possibility is to say,
3861000	3865000	okay, well, maybe there is group consciousness.
3865000	3868000	I mean, we don't have a consciousnessometer
3868000	3871000	that we could put up against the head of the United States.
3871000	3873000	It doesn't even really have a head, right,
3873000	3877000	to determine whether it's conscious.
3877000	3881000	So that would be one area where I think
3881000	3884000	if you accept the United States is conscious,
3884000	3888000	then you end up, I take that as a pretty bizarre kind of view.
3888000	3892000	If you don't, then you adopt other theoretical commitments,
3892000	3894000	and then we could get into the details of those.
3894000	3896000	But I think those other theoretical commitments
3896000	3899000	often then involve kind of further choices
3899000	3902000	among various bizarre possibilities, right,
3902000	3906000	kind of like with the immaterial soul case, right?
3906000	3909000	As soon as you start making those commitments,
3909000	3912000	once you develop them, you see, oh boy,
3912000	3914000	this is going to have this consequence.
3914000	3916000	This is pretty unintuitive.
3916000	3920000	Well, how certain should we be in that conclusion?
3920000	3923000	Is there a theorem that says that any version
3923000	3926000	of materialist theories of consciousness
3926000	3928000	are going to have this property,
3928000	3930000	or is it just, well, as far as we know,
3930000	3932000	according to our best current art,
3932000	3934000	that seems to be the case?
3934000	3936000	In other words, could the appearance of bizarreness
3936000	3940000	go away with better understanding?
3940000	3942000	Yes, it could.
3942000	3944000	And there are two separate reasons, right?
3944000	3948000	One is, kind of as you suggested, right?
3948000	3952000	Unlike what I call the dualist quadrilemma,
3952000	3954000	I don't think we have a kind of rigorous argument
3954000	3956000	that all of the choices have to be bizarre.
3956000	3961000	It's that the choices that I've seen articulated
3961000	3963000	all have bizarre consequences.
3963000	3965000	But there might be some unarticulated choice
3965000	3967000	that I haven't run across yet
3967000	3969000	that turns out to be commonsensical, right?
3969000	3972000	So I think that's possible, but empirically unlikely,
3972000	3974000	given the current state of things
3974000	3977000	and the history of philosophical discussion on this.
3977000	3980000	So it's an empirical conjecture.
3980000	3983000	The other way in which bizarreness could end up vanishing
3983000	3986000	is our intuitions and our sense of commonsense could change, right?
3986000	3990000	So the idea that the Earth moved around the Sun
3990000	3994000	was bizarre when Copernicus suggested it,
3994000	3998000	but we no longer seem to find that
3998000	4000000	a sharp violation of commonsense.
4000000	4002000	Commonsense has changed over time, right?
4002000	4007000	So it could be that someday we'll find it very commonsensical,
4007000	4009000	for example, that the United States is conscious.
4009000	4011000	You say, oh, yeah, of course.
4011000	4012000	Or maybe panpsychism.
4012000	4014000	Oh, the whole universe is conscious, right?
4014000	4017000	The ordinary person in the street, of course they think that.
4017000	4019000	Commonsense can change.
4019000	4022000	It's not a fixed point.
4022000	4023000	And we should...
4023000	4025000	So it's a little bit different
4025000	4029000	than the previous examples of the skeptical scenarios, right?
4029000	4031000	Here, unless I'm misinterpreting,
4031000	4034000	you're not arguing that we should hold out 1% credence
4034000	4036000	for some bizarre possibilities.
4036000	4039000	You're just saying, look, all the possibilities
4039000	4041000	seem to be bizarre.
4041000	4043000	We should, I guess, learn to accept that
4043000	4046000	or fold that in, not use it as a draft,
4046000	4049000	as a knockout argument against something.
4049000	4052000	We can't say, well, I can't accept that it's bizarre,
4052000	4054000	because all the other options are too.
4054000	4055000	Exactly.
4055000	4058000	So this is why people like panpsychists and idealists
4058000	4059000	like my stuff on this, right?
4059000	4061000	Because part of the reason,
4061000	4064000	the main reason I think people reject panpsychism,
4064000	4067000	for example, the idea that everything in the universe
4067000	4069000	or maybe the universe as a whole is conscious,
4069000	4072000	is that it just seems so contrary to common sense.
4072000	4074000	But if I'm right,
4074000	4077000	well, something contrary to common sense is probably true,
4077000	4079000	so maybe that's it, right?
4079000	4081000	So, right.
4081000	4084000	I mean, I do think we have to rely on common sense
4084000	4085000	to some extent.
4085000	4087000	I don't think we can just toss it out the window
4087000	4089000	when we talk about issues like this.
4089000	4091000	We don't have, in my view,
4091000	4094000	really any great tools for answering these questions,
4094000	4097000	and so we have to rely on highly imperfect ones
4097000	4099000	like common sense.
4099000	4102000	But the fact that something violates common sense
4102000	4105000	is not automatically defeated.
4105000	4108000	It does seem very similar to things
4108000	4111000	that even I have said about quantum mechanics.
4111000	4114000	I presume that we're going to put
4114000	4116000	the many-worlds interpretation of quantum mechanics
4116000	4118000	into the bucket of things that you would say
4118000	4120000	are pretty bizarre.
4120000	4122000	Yes.
4122000	4125000	I remember a quote from,
4125000	4127000	I think it was David Merman,
4127000	4129000	who is a very famous, very great physicist,
4129000	4133000	who is an epistemic person
4133000	4135000	when it comes to quantum mechanics.
4135000	4137000	So he thinks the wave function is just a tool
4137000	4139000	for understanding our knowledge and prediction,
4139000	4141000	not reflecting anything real.
4141000	4143000	And, you know, he does little surveys of the field,
4143000	4146000	et cetera, and at some point comes to many worlds,
4146000	4149000	and he says, yes, you can follow the Schrodinger equation
4149000	4152000	and its consequences, and you end up with a theory,
4152000	4155000	and the price you pay is seriousness.
4155000	4158000	So basically he's just saying, like,
4158000	4161000	surely you can't take that seriously, right?
4161000	4163000	And that's it. That's the entire argument.
4163000	4166000	And that feels like not a good enough argument to me,
4166000	4169000	because, like you said, in the context of consciousness,
4169000	4172000	for me, every version of quantum mechanics
4172000	4175000	is going to lead us somewhere strange.
4175000	4179000	Right. In fact, I think the,
4179000	4182000	I like the interpretations of quantum mechanics
4182000	4186000	as an illustration of what I call the universe of Dubaiity
4186000	4189000	and the universal bizarreness claim,
4189000	4191000	because I think maybe especially your listeners
4191000	4193000	will find that plausible, right?
4193000	4198000	Every viable interpretation of quantum mechanics is bizarre.
4198000	4201000	There's no, like, common sense way of thinking
4201000	4203000	about quantum mechanics, right?
4203000	4208000	And, you know, with apologies to the many worlds advocates,
4208000	4210000	right, they're all dubious, right?
4210000	4213000	There's at least grounds for doubting all of them.
4213000	4216000	I don't mean, when I say dubious,
4216000	4218000	I don't mean that we have to assign
4218000	4220000	a very low credence to them,
4220000	4222000	but it's reasonable to be doubtful among them,
4222000	4225000	to not feel like, ah, we're epistemically compelled
4225000	4229000	to accept many worlds over all the other interpretations.
4229000	4232000	So it's a good, interpretations of quantum mechanics
4232000	4235000	is a good example of a domain in which I think
4235000	4240000	the universal Dubaiity and universal bizarreness claim is true.
4240000	4243000	And then I want to say the same thing about, say,
4243000	4246000	the theories of consciousness,
4246000	4250000	and theories of the fundamental structure of the cosmos.
4250000	4252000	And both of those, I think, are, I mean,
4252000	4254000	the interpretation of quantum mechanics
4254000	4256000	and the nature of consciousness are both part of
4256000	4258000	the fundamental structure of the cosmos.
4258000	4262000	I kind of almost get that for free once you get those two.
4262000	4266000	You know, I generally, when pressed,
4266000	4270000	put my credence in many worlds at between 90% and 95%.
4270000	4271000	You know, depending on the time of day,
4271000	4273000	I'll give one of those two numbers.
4273000	4275000	And I did that in conversation with Philip Goff,
4275000	4278000	the famous panpsychist and previous mindscape cast,
4278000	4281000	and he was just flabbergasted.
4281000	4284000	He's like, you give a 95% credence
4284000	4287000	to the many worlds interpretation of quantum mechanics.
4287000	4289000	But dude, you're a panpsychist.
4289000	4291000	You think electrons have feelings.
4291000	4293000	Don't give me a hard time for giving large credence
4293000	4295000	to following the Schrodinger equation.
4295000	4298000	I'm sorry.
4298000	4301000	Right. Totally fair. Totally fair.
4301000	4303000	I mean, I wouldn't give many worlds quite that high
4303000	4304000	in interpretation.
4304000	4308000	I think we should be more epistemically cautious
4308000	4311000	about our favorite interpretation of quantum mechanics.
4311000	4313000	But yeah, there's room for reasonable disagreement.
4313000	4316000	I mean, I think if you're in the ballpark of 90 to 95%,
4317000	4322000	you're getting on the cusp of denying universal dubiety.
4322000	4327000	But you know, how, what exactly counts as being dubious
4327000	4328000	is kind of a fuzzy.
4328000	4331000	You do suggest in the book that when you're in this position
4331000	4333000	where every option is bizarre,
4333000	4337000	we should give fairly large credences
4337000	4339000	to the competing possibilities
4339000	4342000	because we kind of don't have a right to be too confident
4342000	4345000	about preferring one bizarre alternative
4345000	4347000	to other bizarre alternatives.
4347000	4350000	We shouldn't be too definitive.
4350000	4352000	Yes.
4352000	4354000	I think that's generally true
4354000	4356000	about the kinds of questions that we're asking
4356000	4359000	because I think we have basically three
4359000	4363000	broad types of epistemic grounds
4363000	4365000	for choosing among these theories, right?
4365000	4367000	One is common sense,
4367000	4369000	which is that what we've already talked about
4369000	4370000	is going to be imperfect
4370000	4372000	and things are going to violate it.
4372000	4373000	These theories are going to violate it
4373000	4375000	in one respect or another.
4375000	4378000	Another is scientific evidence.
4378000	4380000	You know, just kind of direct scientific evidence,
4380000	4382000	you know, like measure it, right?
4382000	4387000	And on something like whether electrons have souls
4387000	4391000	or what interpretation of quantum mechanics is correct,
4391000	4394000	we can't now at least run an experiment that says,
4394000	4396000	ah, yeah, this experiment proves,
4396000	4397000	obviously on the face of it,
4397000	4399000	that many worlds interpretation, right?
4399000	4401000	And then the third tool we have is something
4401000	4404000	like theoretical elegance.
4404000	4406000	And again, that's kind of going to be indecisive
4406000	4408000	because, you know, there's something elegant
4408000	4410000	about panpsychism
4410000	4413000	and there's something elegant about materialism
4413000	4415000	and there's something elegant about many worlds
4415000	4419000	and there's something elegant about other approaches too.
4419000	4421000	So these are not going to be,
4421000	4424000	they're going to be trade-offs among these very imperfect ways
4424000	4426000	of trying to settle these questions
4426000	4430000	rather than
4430000	4433000	bronze solid grounds.
4433000	4435000	Well, I wanted to ask you this specifically
4435000	4437000	in the context of quantum mechanics
4437000	4440000	because I've put it the following way sometimes.
4440000	4442000	I wanted to see how it fits in with your views.
4442000	4445000	If you take something like hidden variables,
4445000	4447000	versions of quantum mechanics,
4447000	4450000	so those listeners who don't know what I'm talking about,
4450000	4452000	we did an episode with Tim Maudlin recently
4452000	4453000	where he will explain.
4453000	4455000	And in those theories, you have particles
4455000	4457000	and they have locations.
4457000	4459000	And that's what you observe when you do a measurement
4459000	4461000	and you also have a wave function.
4461000	4465000	The phenomenology is much closer to the world
4465000	4468000	than it is in something like many worlds
4468000	4470000	where you have this abstract wave function
4470000	4473000	and there's many copies of reality, etc.
4473000	4477000	I think that there's much less elegance, simplicity,
4477000	4481000	austerity to the hidden variables version
4481000	4482000	as a theory.
4482000	4484000	I think this is indisputable.
4484000	4486000	Do you agree that it's the best theory or not?
4486000	4490000	You should also agree it's a clunkier theory
4490000	4491000	than many worlds.
4491000	4493000	Many worlds is very simple and austere,
4493000	4497000	but I should also accept that many worlds
4497000	4501000	is much further away from our everyday life
4501000	4505000	and our experience than the hidden variables theory is.
4505000	4507000	So the question is how do we weigh
4507000	4509000	these different considerations?
4509000	4512000	It's good to have a simple theory.
4512000	4515000	It's also good to have one that tells you pretty directly
4515000	4519000	and immediately what it predicts and how to understand it.
4519000	4522000	How do we be good philosophers and scientists
4522000	4525000	when we're faced with that kind of choice?
4525000	4526000	Right, exactly.
4526000	4529000	And I'd say just leave that hanging as a question.
4529000	4531000	So I am inclined to agree.
4531000	4533000	You're much more expert on this than I am,
4533000	4535000	but one of the things that I like about many worlds
4535000	4538000	is that it is have a certain kind of simplicity to it.
4538000	4541000	And these other theories all seem to involve
4541000	4545000	a lot of fussing around.
4545000	4550000	But right, how do you weigh that against other aspects
4550000	4555000	that reasonably draw people to resist many worlds
4555000	4557000	and prefer these other approaches?
4557000	4562000	And I don't think that there is a really good general answer
4562000	4564000	to that kind of question.
4564000	4567000	And that's one of the reasons to have kind of
4567000	4572000	non-extreme credences in these various theoretical possibilities.
4572000	4573000	Good, yeah.
4573000	4575000	And if you do have non-extreme credences,
4575000	4577000	then you can hope for progress.
4577000	4579000	You can hope to get better.
4579000	4582000	I guess maybe to wind up the conversation,
4582000	4587000	I like giving actionable advice to the people out there.
4587000	4590000	We've talked a little about how to deal with these crazy things.
4590000	4594000	Maybe to go back to that idea that I'd never heard of before,
4594000	4596000	Nicolosi and discounting.
4596000	4601000	Maybe that's the same idea as when your credences get small enough,
4601000	4603000	I'm allowed to stop thinking about it.
4603000	4605000	Is that right?
4605000	4607000	That is basically the idea, yeah.
4607000	4608000	That's basically the idea.
4608000	4610000	I think that idea is important,
4610000	4616000	but maybe part of your message in the book is
4616000	4623000	don't be quite so quick to dismiss the tinier, more bizarre possibilities.
4623000	4624000	I don't know.
4624000	4625000	Is that right?
4625000	4626000	Yes.
4626000	4628000	That is one of the messages.
4628000	4630000	Absolutely.
4630000	4634000	I think that people will tend to have a gut reaction
4634000	4638000	against views that strike them as bizarre,
4638000	4641000	whether it's many worlds or panpsychism
4641000	4646000	or the idea that only humans have souls or whatever it is.
4647000	4653000	I think there's reason to take that kind of reaction seriously,
4653000	4659000	but there's also reason to not just rely on that
4659000	4666000	and to allow that some of these theories that you might think are
4666000	4670000	so bizarre as to be absurd,
4670000	4675000	maybe they're only bizarre and not actually absurd.
4675000	4679000	I guess I'm caught maybe in a little bit of hypocrisy here
4679000	4683000	because that's exactly what I want to say to David Merman and his friends.
4683000	4686000	He will just dismiss many worlds,
4686000	4688000	even though he's a brilliant physicist,
4688000	4690000	he'll just say, that's too bizarre.
4690000	4692000	I'm just not going to accept that.
4692000	4695000	I want to say, no, you have to take it seriously.
4695000	4697000	But then there are other people, panpsychists,
4697000	4700000	maybe you're an example, who I will say,
4700000	4702000	no, that's too bizarre.
4702000	4703000	I don't need it.
4703000	4707000	I'm not quite sure what the principle stance is here.
4707000	4709000	Right, yeah.
4709000	4712000	Maybe you should give a little bit of your credence space to panpsychism.
4712000	4714000	Just a little.
4714000	4717000	Maybe here is the issue.
4717000	4720000	There's sort of in principle credence space
4720000	4724000	and then there's what I will spend my time worrying about credence space.
4724000	4726000	When the credence has become so small,
4726000	4728000	I'm not going to lose sleep over it,
4728000	4732000	even if maybe someday evidence will come in that will change my mind.
4732000	4734000	Right, yeah, that's fair.
4734000	4736000	That's fair, yeah.
4736000	4739000	Especially as an academic choice, right?
4739000	4743000	So there's also this question of,
4743000	4746000	what do you spend your academic time thinking about?
4746000	4748000	What do you invest your energy in?
4748000	4751000	And even if you were to say, give a non-trivial,
4751000	4755000	say, 5% credence to panpsychism, that's not tiny.
4755000	4760000	But that might not be worth enough of your academic time
4760000	4763000	to build theories on.
4763000	4766000	I have spent more time than my credence would warrant
4766000	4768000	thinking about panpsychism,
4768000	4771000	so I actually take this advice very well.
4771000	4773000	You've given it more than it's 5% due.
4773000	4775000	I think so, I think so.
4775000	4778000	Anyway, Eric Schwitzcape, if it was all a dream,
4778000	4781000	it was a very fun dream to have.
4781000	4784000	So I appreciate, thanks very much for being on the Mindscape podcast.
4784000	4787000	Yeah, thanks for having me. It's been fun.
4790000	4792000	Thank you.
