start	end	text
0	3520	Hello, everyone. Welcome to the Mindscape Podcast. I'm your host, Sean Carroll.
4240	9840	When you think about how we conceptualize human beings, someone once pointed out that
9840	16240	we're always using metaphors that depend on our current best technologies, you know, when clocks
16240	21840	were just invented, wristwatches and so forth. It was the clockwork universe when robots and
21840	27520	machines came on the scene. We thought of organic beings kind of like that. And now we have computers,
28080	34480	besides which we have, you know, cameras and video cameras and audio recorders and so forth.
34480	41280	So we tend, very, very roughly, you know, we tend to think about a person as kind of like
41280	49600	a robot with some video cameras for eyes and audio recorders for ears hooked up to a computer
49600	56320	inside. And the sensory apparatus brings information into the computer, which then tells
56320	63200	the robot body what to do. It's a simple, kind of straightforward, compelling picture. It's also
63200	70000	wrong. That's not actually a very good description of what we are, how we behave. For one thing,
70000	75920	intelligent design is not the way that human beings came about. We evolved over many, many years,
75920	81120	and we weren't aiming for that. We have to think about what is the kind of architecture that actually
81120	87280	best serves the purposes of surviving and procreating and reproductive fitness and so forth.
87280	92080	And it turns out to be very different. So today's guest is Andy Clark, who is a philosopher and a
92080	97840	cognitive scientist. In fact, his title at the University of Sussex is Professor of Cognitive
97840	104000	Philosophy, very well known in philosophy, very, very highly cited for thinking about the brain
104000	109040	and the mind and how they're related and how they work. He became very famous with a co-author
109040	115520	paper with David Chalmers, where they proposed the extended mind hypothesis, the idea that what
115520	121760	you should count as your mind is not just your brain, but also all the little extensions of
121760	127200	the brain that help us think, whether it's inside our bodies, or whether it is things we scribble
127200	132400	down on a piece of paper or used to enhance our memories or calculation abilities, and so forth
132480	139440	and so on. He also has a great interest in the idea of the brain as a predictive machine. And
139440	146560	that is the subject of his new book, The Experience Machine, How Our Minds Predict and Shape Reality.
146560	152880	So the idea here is that the brain is not a computer just bringing in sense data and then
152880	158960	thinking about it. Our brains are constantly constructing a set of predictions for what's
158960	163840	going to happen next, what is going to be the situation in which the body finds itself, what
163840	169040	is the sensory data that we're going to bring in, and then you compare what you're actually
169040	175120	experiencing versus what the brain was predicting, and you try to play the game of minimizing the
175120	180240	error between what you predicted and what you're actually perceiving. This sounds like maybe a
180240	185840	small change of emphasis or an angle on a similar kind of process, rather than the brain just being
185840	191600	a passive receptor of information. It is sort of actively engaged in a feedback loop, but it has
191600	196960	very, very significant consequences for how we think about thinking, how we think about fixing
196960	202560	thinking, right? When we go wrong one way or the other, whether it's being in pain or having a mental
202560	208400	disorder of some sort, how do we get better at it, taking seriously how the brain is a prediction
208400	213120	machine is very useful here, as well as for philosophical problems about how you carve up
213120	217360	nature, how you think about what the brain does, what is consciousness, what is free will,
217360	221840	and so forth. So this is one of those podcasts that touches on many issues that we're interested in
221840	227920	here at Mindscape, from consciousness to time all over the place. And occasional reminders,
227920	235840	you can support Mindscape by pledging at Patreon, go to patreon.com slash Sean M. Carroll, and pledge
235840	241200	a dollar or two or whatever you like for each episode in return, sense of fulfillment, but of
241200	246800	course also add free versions of the podcast, as well as the ability to ask questions for the
246800	252960	Ask Me Anything episodes. You can also, if you're interested, leave reviews of Mindscape at Apple
252960	258800	Podcasts or wherever there are reviews. Those reviews help draw other listeners in. So if you
258800	263280	think Mindscape's worth listening to, make it an even bigger community listening, and that would be
263280	266160	awesome for all of us concerned. So with that, let's go.
281680	286320	Andy Clark, welcome to the Mindscape podcast. Hey, it's great to be here. Thanks for having me.
286320	291360	You know, you've done, as many people have of a certain age, many interesting things over your
291360	298240	career. Your new book is The Prediction... What is your new book? What is the title?
298240	302320	It's The Experience Machine. The Experience Machine. I keep wanting to say The Prediction Machine,
302320	306960	because obviously prediction is playing a big role there. The Experience Machine with how predictions
306960	313600	shape and build reality. Good, good. Yeah, reality will be a theme that we want to get to, but I can't
314240	319680	give up the opportunity to also talk about extended Mind and extended Cognition and things like that.
319680	325680	So I thought it would make sense to talk extended Mind first, and then get into prediction. Does
325680	330000	that make sense logically to you? I think that's a good route. That's certainly that was my route,
330000	337520	so why not? Good, good. Let's do it. So apparently there are people out there who think that, well,
337520	342480	there are people who think that the Mind is not even related to the Brain, which is funny to me,
342480	348240	but there are other people like yourself, perhaps, who think that the Brain is just one little part
348240	353200	of our minds and our thinking. So I'm not going to put words into your mouth. What does it mean
353200	359520	to talk about the extended Mind? Yeah, I mean, this is a view that I've kind of held and defended
359520	365200	for many years. It goes back to a piece of work that I did with David Chalmers, who's famous for
365200	373120	his kind of almost dualistic views on consciousness after all, many years ago, back in 1998. The basic
373120	380320	idea that Chalmers and I agreed on is that when it comes to unconscious cognition, then there's
380320	386880	no reason to think that the Brain is the limit of the machinery that can count as part of an
386880	393200	individual's cognitive processing, where that has to include unconscious processing, because it's
393200	399280	unconscious processing that we think mostly is what gets extended. There's a whole other debate
399360	405520	about conscious processing. And the idea there is that moment by moment, the Brain doesn't really
405520	412400	care where information is stored. It cares about what information can be accessed, how fluidly
413520	417600	you can get at it, whether or not you've got some idea that it's there to be
417600	425680	got at at all. So the idea was that calls to biological memory and calls to external stores,
425680	431200	like a notebook or currently nowadays, maybe a smartphone or something like that,
431200	437520	are working in fundamentally the same kind of way. And actually, interestingly, it's that that I
437520	444000	think the predictive processing story ends up cashing out in an interesting way. So we can
444000	451200	circle back around to that later. That's the core idea is that the machinery of mind doesn't all
451200	456320	have to be in the head. David Cholmers, of course, another former Mindscape podcast guest. So we
456320	462640	have an illustrious alumni base. But let's let's make it a little bit more concrete. What do we
462640	468560	mean? You know, I think that what immediately comes to mind is I can remember almost no phone
468560	472640	numbers now, because they're all in my smart phone. Does that count as extended mind?
473280	480160	That counts. I think it's after all, you have a fluent ability to access at least a functional
480240	486160	equivalent of that information as and when you need it, because, you know, you just pick up the
486160	492000	phone and there it goes. If on the other hand, you had all those numbers stored in a notebook,
492000	497920	but the notebook was in your basement. And, you know, you had to run down into the basement to get
497920	504400	it whenever you needed it, that wouldn't count because you wouldn't have the constant robust
504400	511200	availability of that resource woven into the way that you go about every kind of problem that
511200	516960	daily life throws at you. So there is, I think, a genuine intuition, which is that whatever the
516960	522320	machinery of mind is, it better be more or less portable. It better more or less kind of be going
522320	526960	around the world where you're going around the world, where bio you is going around the world.
527920	534880	And so for that reason, I think you need robust and trusted kinds of access, but only to the
534880	540960	extent that biological stuff is robust and trusted. You know, now and again, my biological memory goes
540960	546080	down. Now and again, I don't trust what it throws at, but it should be more or less in the same ball
546080	553200	part. Well, okay, so robust and trusted access. Is that the criterion? I mean, I guess an immediate
553280	557840	question that comes to mind is, where do I draw the boundary? Does everything in the world count
557840	564800	as my mind? Yeah, I mean, it's a, it's one of the original worries about this view. It's one that
564800	571040	we used to call, or I used to call cognitive bloat, why it somehow is no good way to stop in this
571040	577600	process. But actually, if you take that robust availability and trust of the product stuff
577600	582720	reasonably seriously, that does rule an awful lot of things out. But I think in addition,
582720	588080	there's something which I've never, I've never given a full account of, to be honest, but there's
588080	595440	something about the sort of delicate temporal nature of the integration of the biological system
595440	602240	with this non biological crop or aid or resource or whatever it is. So that you can have the brain,
602240	608960	if you like, making calls to this stuff in ways where it kind of knows about the temporality of
608960	613600	that, it doesn't all have to go through a little bottleneck of attention, you don't have to kind
613600	617680	of think to yourself, oh, I'd better get that information from over there. That's not what
617680	623680	it feels like to go about your daily things as you. So it's this sort of idea of something where,
623680	628240	if it were taken away suddenly, you would feel very much at the loss of a sort of
628880	633600	kind of general purpose loss. There'll be many situations that you'd find yourself in
634240	638240	where you were no longer able to perform in the ways that you expected yourself to perform.
639360	643440	There's something there that could do with a bit more unpacking about temporal dovetailing.
644000	650000	Does it relate to ongoing conversations that I'm very interested in about emergence and strong
650000	656240	and weak emergence or just being able to coarse grain in the world in such a way that you have
656240	661840	units that have explanatory power? Yes, I think it certainly relates to a lot of stuff about
661840	668400	emergence because the idea here would be that by dovetailing things together in this way,
668400	674400	you get basically an emergent you. You are the emergent property. You, the thing that goes around
675280	682640	with a sort of sense of its own capacities, but doesn't really care how they get cashed out.
682640	687600	So something that I think comes out quite strongly in the predictive processing stories
688320	694800	is that brains are kind of location neutral when they estimate where they can get good
694800	699600	information back from. So they're very good at estimating what they're uncertain about and
699600	704480	going about getting more stuff, but it doesn't really matter where that stuff is. So as long as
704480	711280	it's accessible, trusted, there when you want it, that's fine. And then there'll be a story to tell
711280	717360	about how the actual selection process operates, but we'll come around to that once we talk about
717360	723040	predictive processing. Let's stick with TXM. I mean, I guess one way that philosophers think
723040	729120	about their job is that they're supposed to be carving nature at the joints. And does this
729120	735600	count? Is that this? Is it a sort of betrayal of that goal because you're including all my
735600	741280	smartphones and my card catalog as part of my mind? Or is it an improvement of that because
741280	748880	really the action of our brains relies on all this stuff? Yeah, that's a very good and very
748880	754800	delicate kind of question. So, you know, when when those charmers and I put it forward, we tried to
754800	761280	argue that this was, this was where the right way to carve nature at the joints because of
761280	768480	functional similarities. That said, you know, it doesn't intuitively always seem like the right
768480	773840	way to carve nature at the joints. So I think that we're making a decision here, and that that
773840	782160	decision, or the enough is in part a moral decision. So, okay, I, I'm very much moved by the analogy
782160	787600	with prosthetic limbs, for example, if you took somebody with a well fitted prosthetic limb,
787600	793680	and then looked at them and said, but you know, your basic physical capacities are just those of
793680	798960	you without the limb, we would in a way be doing them a certain kind of injustice. And I think we
798960	804640	be doing ourselves a certain kind of injustice. If we treated people say with mild dementia that
804640	811760	rely very much on that on a smartphone or on their home environment, in just that kind of way, we
811760	817920	would be, we would be sort of regarding them as less, I think, than they really are. So for me,
817920	825840	the sort of non moral route, the kind of functional similarity route is a kind of stalemate, because
825920	830400	it's kind of carving nature at the joints, and it kind of isn't. And then it's when you chop the
830400	835760	moral or ethical considerations into the pot, that I think you have a strong argument that
836400	842800	overall, the best way to go is to err on the side of caution, and buy into the picture of the extended
842800	846560	mind. I bet that depends on one's definition of caution. But
846880	853280	I think, all right, is there intermediate, there probably is an intermediate stance in which
854000	860720	your whole body, your physical body counts as your, what goes into cognition and the mind,
860720	867920	but things outside the body don't. Is that a well populated space in the thoughts about this?
868640	874160	Yes, I think that is reasonably well populated. I personally, I think it's an unstable space.
874160	879520	So I think you better go one way or the other. I think that, as soon as you start thinking, okay,
879520	884880	if I'm using my fingers to help me count, then my fingers somehow count as part of the machinery
884880	891520	of mind. A lot of people do feel that, but I think that's true. But I think you can't think that,
891520	897360	and then simultaneously, you think that the constantly carried pocket calculator can't count
897360	903600	as part of the machinery. So yes, I think the body is often, for me, it's a very useful step in
903600	910080	stone because I can suck people in by getting them to agree that continuous reciprocal interactions
910080	915840	between brain and body do a lot of what really looks like cognitive work. And there are good
915840	922240	examples of that from even things like the role of gesture as we speak. So spontaneous gesture
922240	929280	by Susan Golden Meadow and colleagues have shown that it kind of eases a certain sort of problem
929280	935040	solving. If children are forced to explain how they solved the maths problem, but not allowed
935040	940560	to gesture, they're actually a lot worse at it. So there's something going on, continuous
940560	947520	reciprocal interaction that is probably allowing us to somehow offload aspects of work in memory
947520	952960	into these gestures, stick what they're in the world, which is good news for philosophers that
952960	957040	worry about hand waving. It's just part of my thinking.
957040	960560	Respectable cognitive task going on right there. Can we
961680	966320	ask how this was different back in the day? Obviously, a lot of the things we've been
966320	975680	talking about are technological or modern. So was the mind of a human being 100,000 years ago
975680	980560	very different? I think I would say it was, despite the fact that the brain of a human
980560	988960	being 100,000 years ago wasn't very different. So yes, I would say that the mind of 100,000 year
988960	996160	old humans was very different. But the brain obviously wasn't. But think of intermediate
996160	1001920	technologies before maybe we go back that things like just using pen and paper. And
1002720	1008160	in particular, I'd like to think about the example of sort of scribbling while you think
1008880	1013200	the thing that many of us, those of us at least old enough to be brought up with pen and paper,
1013200	1017360	do an awful lot of. And so as we try and think a problem through, we write things down.
1018720	1023600	I think it was Richard Feynman, who has a rather famous exchange with a historian,
1024720	1027600	Weiner, I think the historian was, where Weiner said,
1029280	1034000	these, so this is a record of your thinking. And Feynman said something like,
1034960	1041360	no, it's not a record. It's, what did he say it was? It's not a record. It's working. You have to
1041360	1047760	work on paper and this is working. And I think there he had an intuition that this is a just
1047760	1054320	offloading stuff onto a different media. This is actually part of a process that solves the problem.
1054320	1059760	Do we see this then in other animals? I mean, I presume the answer is yes. But is it more
1059760	1066160	less dramatic depending on the species? Yeah, I mean, I think we humans do this to a really
1066160	1072480	ridiculous degree. And it's because we invented symbolic culture somewhere along the way. We
1072480	1078720	came to create these arbitrary systems of recombinable elements that we could then,
1079600	1087120	you know, re-encounter as objects for perception. So creating these sort of structured encodings of,
1087120	1092720	in some sense, our thoughts and ideas and putting them out in the world is a huge opportunity
1092720	1099840	for any creature that is a good information forager, any creature that is very good at reaching out
1099840	1104480	into the world to resolve some of its current uncertainty. And once again, that's a theme that
1104480	1110640	we'll look back to later on. So humans do it a lot more. Other animals do it too. I don't think
1110640	1116720	it's a purely human thing. It didn't just suddenly arise. You know, if you have a chimpanzee that
1116800	1122960	or the orangutan, better example, that constantly uses a stick to gauge the depth of rivers before
1122960	1128880	going into the rivers, I think there's enough of a sort of weave in and robustness for that to
1128880	1134960	count as part of a sort of extended cognitive load. I guess I'm very interested in phase
1134960	1139760	transitions, you know, small changes in things that allow enormously different capacities,
1139760	1146960	et cetera. Clearly, the ability to think and communicate symbolically was a big one. It
1146960	1151200	opened up things. But I guess what you're saying is one of the things that opened up is not just
1151200	1157520	we can think more abstractly or generally, but we can think in literally different ways because
1157520	1162560	these symbols allow us to more easily offload some of our cognition. Yes, I think that's right.
1162560	1170880	I think that one thing we can do is we can use one of our biological capacities in very,
1170880	1175920	very different ways. And that capacity is a tension. So when we put something out into the
1175920	1181600	world in that sort of way, kind of clothe it in materiality, that lets us attend to it in a whole
1181600	1187680	bunch of different ways. And interestingly, I think by doing that, we can often break the grip of our
1187680	1194160	own sort of over-constructive internal models of what kind of thing it is and how it might behave
1194160	1199840	and how you might re-engineer it. So even think about something like designing a new training
1199840	1204640	show or something. If you build a big-scale model and then you start looking at it and poking it
1204640	1208880	and prodding it, you can come up with all kinds of ideas that you wouldn't come up with just by
1208880	1214720	kind of sitting down and trying to imagine all that in your head. So I think we're unusual in
1214800	1220880	that respect, but it's a great power of us being perception action machines, that we can really
1220880	1226160	make the most of this. And then you look at current more disembodied AI, they've got really
1226720	1233360	no use for this kind of strategy. The way they work is just not such that they get to start putting
1233360	1238480	stuff out into the world and then poking and prodding it just to think better. They might do
1238480	1242560	that for all kinds of other reasons. But is that their fault or ours? I mean, we haven't really
1242560	1246800	given them the capacity to do that. I think that's right. I think it's our fault. I think we've,
1246800	1252400	to be honest, I think, well, it's our fault. We've designed a class of tools that are really,
1252400	1258160	really useful for what they're useful for. But I don't think, actually, that those tools will
1258160	1264960	ever achieve what I would call true understanding because they don't have their grip on reality
1264960	1270560	grounded in perception action loops. And it's that grounding in perception action loops that I think
1270560	1276080	we're leveraging with material symbolic culture when we just create these things and poke and
1276080	1280880	prod them in different ways. We're super specialized for being good at perception action loops.
1281600	1285840	So I think that's, there's a trick there that disembodied AI is missing out, but then it gets
1285840	1290560	all kinds of other things instead. Sure, sure, it does. But okay, I'm going to, I'm just going to
1290560	1294640	download a little bit because what you just said sparked several ideas and it's,
1294640	1300000	it's prefiguring things we'll get to later. But two things. One, an idea I heard more than 10 years
1300000	1306960	ago that attempts to make AI systems suddenly work much better when you put them in a robot
1306960	1314000	that can go out there and interact with the world. And secondly, a podcast I did with Judea Pearl,
1314000	1319520	who claims that babies spend all of their time making a causal map of the world by poking at
1319520	1326320	things and seeing what happens. And I know that the whole prediction models that we'll be talking
1326320	1330880	about are kind of this, like, you know, the brain making a model of the world. So all that
1330880	1336080	together, do we imagine that if we do put modern large language models in an embodied
1336080	1340000	context and let them go out and poke the world, they would change dramatically how they think?
1340880	1345280	Yes, I think it would. You know, there are companies out there that are kind of trying
1345280	1351440	to do this, like versus AI is one of them. And, you know, the idea certainly is to
1352080	1357680	see what happens if you kind of leverage the active inference framework as a way of creating
1357680	1363840	these sorts of new ecosystems of human artificial intelligences. Although I wouldn't start, I
1363840	1368560	think, with a large language model, exactly, just because, you know, something like that has been
1368560	1375120	trained to predict basically the next word in corpuses of text. And that's a very funny place
1375120	1380400	to start if what you want to be is a perception action machine. It seems much more like
1380400	1386960	developmental trajectories where you start without that huge body of, nonetheless, maybe
1386960	1395680	relatively shallow successor item information. You want to learn basics about causation and flow,
1395680	1400960	and you probably want to understand those things in a way that is more grounded than it would be
1400960	1404800	if you just knew how all the words about causation and flow tend to follow one.
1405440	1406160	It sounds like you-
1406400	1410320	This isn't a very cool question. It remains to be seen. It might be that if you ban a large
1410320	1415280	language model in a good enough robot, that it's just like giving a baby a super head start or
1415280	1420720	something. But it does sound like you might be at least sympathetic to the idea that we need,
1420720	1427840	that I've had some people on the podcast advance, that AI would be better off if we didn't just
1427840	1434480	dump large data sets into deep learning networks and instead also poked and prodded the AI to have
1434480	1439840	a symbolic representation of the world. Yes, I think that's right. I do agree with that. I think
1439840	1448560	if our goal is not good tools to think alongside, but rather something like artificial general
1448560	1454320	intelligences, so colleagues. If you want to build an artificial colleague, then I think
1454320	1460320	large language models are not the right place to start. But if you want a really lovely tool
1460320	1465680	that can do an awful lot of work and that you can work with, then they're really interesting.
1465680	1469120	Honestly, I don't want an artificial colleague, but an artificial graduate student or postdoc
1469120	1475600	would be very, very helpful or someone who could answer my email. I guess it would be weird not to
1476880	1484000	ask the obvious question. I get annoyed when sometimes my philosophy colleagues seem to claim
1484000	1488960	to have an immediate and obviously true view of what is natural and real in the world,
1488960	1494800	but maybe I'm going to do that right now by saying, I feel like I'm inside my body. I feel
1494800	1503600	like the I myself is located maybe even in my head. Is that just I have old intuitions that
1503600	1509760	haven't quite caught up to the modern world? No, I think that that's a valid intuition,
1509760	1515920	if you like, because I think that we infer that we are wherever it is that the perception action
1515920	1522720	loop is closing. We're basic. We're perceiving the world and we're launching interventions and
1522720	1529280	we're receiving the effects of those interventions. That's such a fundamental part of how we learn
1529280	1533760	about things. I think it's no surprise that that's where we think we are. Notice that that's where
1533760	1540400	we think we were, even if part of our brain was located in a nearby cliff top communicating
1540400	1545040	wirelessly with the rest of the brain. We'd still think, oh, this is where the perception action loop
1545040	1551200	is closing. This is where I am. Dan Dennett has a beautiful old philosophical short story called
1551200	1556560	Where Am I? where he plays with that idea where there's someone whose brain is in the cliff top
1557120	1563840	and they think that they are closing perception action loops using a body in the world,
1563840	1569280	but then at some point, you know, it gets cut off and suddenly they think, oh, now I'm claustrophobically
1569280	1575120	stuck in the cliff top. They knew their brain was there. Anyway, it's a beautiful story.
1575120	1582080	No, that is very interesting. Okay, so we might as well dig into this phrase that you're using
1582080	1587280	over and over again, the perception action loop. I think the word loop crucially important there,
1587280	1594240	right? The way I like to think about it is, and perhaps this is true because of technology,
1594240	1600560	but we have a naive metaphor for how the brain works as kind of like there's a video camera
1600560	1605360	with a computer hooked up to it, right? Just taking in our sense data, processing it and
1605360	1610400	doing something. And a big part of your message is, no, it's really not like that at all.
1611040	1617360	Yeah, no, that's exactly right. I mean, let's think about one way, which is not like that.
1617360	1621840	If you think about something like running to catch a fly ball in baseball, where you're kind
1621840	1628240	of running to try and catch this is ball that's flying out there. The way to do that isn't to
1628240	1633360	take in all the information about the flight so far, and then plot where you think the ball's
1633360	1639520	going to go and then tell the robot you as it were go over there. Instead, what you do is you run
1639520	1646080	in a way that keeps the apparent trajectory of the ball in the sky looking motionless as you
1646080	1652080	look up there. It turns out that if you just keep doing that, then you'll be in the right place when
1652080	1656240	the ball comes down. You need to do a bit more to actually catch it. You'll be in the right place
1656240	1662000	when the ball comes down. And there's a way of solving an embodied action problem that involves
1662000	1670480	keeping the perceptual signal within a certain sort of bounds and then acting so as to, well,
1670480	1675920	so is to do that, basically. Something that, again, prediction error minimizing systems would
1675920	1682240	actually be rather good at. So perception action loops, I think, the important thing to think
1682240	1686800	about there is that they're not solving problems all in one go generally. It's sort of like,
1686800	1691440	I do a little bit of this, I do something that gets more information or that keeps the perceptual
1691440	1696080	stuff within bounds, and then repeat the process again and again and again until the thing is
1696960	1702800	So I do appreciate your use of a USA based sports metaphor there, but we have an international
1702800	1711120	audience. I have no idea what it really means. But okay, so the motto then is the brain is a
1711120	1718800	prediction machine. And I just want to sort of home in on what is the difference between that
1718800	1723680	view and another view. I think maybe if people don't think about the brain too much, they would
1723680	1728560	say, okay, sure, yeah, my brain makes predictions, but you really want to put that front and center
1728560	1735360	as the point of what the brain does. Yeah, yep. I mean, it's both the beauty and the burden,
1735360	1740400	if you like, of this story that it is really all this account. I'm told about to use the word
1740400	1747360	story because it doesn't sound scientific. It's a count that it's basically saying this is a
1747360	1753280	canonical operation of the brain, if you like, the canonical computations that the brain performs
1753280	1759680	and that it strings together in different ways to do motor control and interception and extraception
1760880	1766320	is basically one in which you're attempting to predict or the brain's attempting to predict
1766320	1771520	the current flow of sensory information. Of course, there are different timescales at which
1771520	1776480	you could predict it. And the basic timescale, the bedrock one is predicting the present,
1776480	1781040	which of course is a rather funny use of prediction that was saying predictions about the future
1781040	1787200	somehow, but this is a sense in which the brain is a guessing machine. It's trying to
1787200	1793760	guess what the current sensory evidence is most likely to be and then crunch in that guess based
1793760	1799200	on past experience together with the current sensory evidence. And that turns out to be a very,
1799200	1804880	very powerful way of estimating the true state of the external work. It goes wrong sometimes,
1804880	1810480	but it's a very powerful strategy. It allows you to use what you've learned in the past
1811040	1818880	to make better sense of what the raw sensory energies are kind of suggesting in the present.
1819440	1825680	This seems very related, certainly, to Carl Friston's ideas. We had him on the podcast,
1826400	1830480	the free energy principle, the Bayesian brain. What is the relationship between all these ideas?
1831600	1836640	Yeah, I mean, basically, I'm just being an apologist for Carl Friston here.
1837440	1846160	So, Carl's basic picture, the free energy minimization picture is a sort of high road
1846160	1854560	version of the low road version that I give you. So, the low road version comes kind of out of
1854560	1860160	thinking about perception and thinking about action, whereas the high road version comes out
1860160	1868240	of thinking about what it takes to be a persistent, living system at all. You have to
1868240	1873120	preferentially inhabit the kind of states that define you as a system that you are. That means
1873120	1877600	that you end up in the states that in some broad sense, you predict you ought to be in.
1877600	1881200	So, in that broad sense, the fish kind of predicts it ought to be in water
1882080	1890800	because its actions are all designed with that in mind. The low road is a bit less
1892080	1896320	challenging. I guess that'll be why I take it. The low road is just sort of saying, look,
1896320	1901040	we have good evidence that the canonical computations that human brains and the brains of
1901040	1908000	many other animals perform involve this attempt to guess the sensory signal. And it turns out
1908000	1913280	that you can use that to do motor control in a very efficient way. It's been known for a long
1913280	1918480	time, I think, that it's important in perception. And the other thing that I think Carl Friston's
1918480	1925520	work has really done has been shown that that old story about perception becomes so much more
1925520	1931920	powerful when you see that it's got a direct echo in an account of action. So, the idea there is that
1932640	1938640	perception is about getting rid of prediction errors as you try to guess the state of the world
1938640	1944800	using everything you already know. And action is getting rid of prediction errors, but it's
1944800	1949680	getting rid of them by changing the world to fit the prediction. So, you have these two ways to
1949680	1953840	get rid of prediction error. You find a better prediction, or you change the world to fit the
1953840	1960000	prediction you got. One's perception, the other's action, but they're performed using the same kind
1960080	1969040	of basic neuronal operations. There are differences between motor control and sensory perception.
1969040	1974640	But if you look at the wiring of motor cortex, it turns out it's actually wired pretty much like
1974640	1981040	ordinary sensory cortex. In fact, so much so that many people don't like to talk about sensory
1981040	1987600	and motor cortex just for that reason. So, these stories or these accounts give you a sort of
1987680	1993040	principled way of understanding that. In each case, you're predicting a sensory flow,
1993600	2000080	but in one case, you're trying to retrieve an old model of the world to fit the flow,
2000080	2004320	and that's perception. And in the other case, you're trying to change what you're
2004320	2008720	getting to fit the prediction. We can circle around to that. I think I can say that a bit better
2009680	2019520	Is the idea that it's the same neural circuitry that is doing sensing and control, or is it that
2019520	2026960	the two circuitries look parallel? Yes, it's that they look very much parallel.
2027920	2034640	They operate in the same sort of way. The reason they're not the same is a proprioceptive prediction,
2034640	2042160	is playing a very special role in motor control. So, the idea is a proprioception is a sort of
2042960	2048800	system of internal sensing that lets us know how our body is currently arranged in space.
2049360	2056240	So, the idea is that in order to reach to pick up the coke can that's in front of me,
2056240	2062000	I predict the proprioceptive flow that I would get if I were reaching for it,
2062000	2068320	and then I slowly get rid of those errors by moving the motor plant in just that sort of way.
2068320	2073040	So, you get rid of the errors by moving the arm, in this case, to reach up the coke can.
2073040	2078320	So, proprioceptive predictions are especially placed here. They act as motor commands.
2078320	2084480	The other kinds of predictions don't. So, I hesitate to use the word think here,
2084480	2089680	because thinking and cognition and thought I'll have technical meanings that are slippery to me.
2089680	2096640	But the idea is that your brain sort of thinks intentionally or unintentionally that your arm
2096640	2103360	is somewhere slightly different than it is, and then the muscles move it to make that more accurate?
2104000	2107920	That is one way of putting it. And people have put it that way. So, some people have said,
2107920	2112720	look, you've got to kind of lie to yourself. Brace will go to lie to itself. It's got to
2113760	2119600	ignore all that good information that says that my arm isn't moving. It's got to predict
2119840	2126080	the trajectory of sensation that you would get if it was moved in. And that prediction is given
2126080	2130720	so much weight that the other information is overwhelmed and off it goes as a way you get
2130720	2136880	rid of the errors by moving it. I'm not sure personally that's exactly the best way to put it.
2136880	2142640	I kind of think it's really about attention. So, I think it's like you've got information
2142640	2147920	that says that you're not moving the arm. You've also, because of your current goals,
2148480	2153280	you're predicting this proprioceptive flow that would correspond to reaching for the
2153280	2161040	coke can. And by, as it were, disattending to the information that says that it's stationary,
2161840	2166400	you allow the other prediction to do its work. So, I think it's sort of targeted
2166400	2171920	disattention rather than actually lying to yourself. I'm not quite sure why I prefer that.
2171920	2176240	I think we're probably saying the same thing. You know, the math and the models all work out
2176240	2182400	yesterday. But I think if we call it targeted disattention, we understand it a bit better.
2182400	2187680	I think it makes more contact with sports science, for example. But you don't really want to be
2187680	2194080	attending to the position your body is currently in. You want to attend to something like how it
2194080	2201440	ought to be. You want to entrain yourself by knowing what it would feel like if you were doing it
2201440	2210560	right, right now. So, is it similar to the idea of file compression? You know, if we have a JPEG
2210560	2217200	or an MPEG, that there are some patterns that are repeated so you don't have to keep track of every
2217200	2222000	pixel. You can just assume that they keep going. And then you pay attention to the differences.
2222000	2226480	And those are what we're talking about as errors in the brain case.
2226480	2231520	That's certainly correct when we come back to thinking about perception and something like
2231520	2239600	that is involved in action too. But it does seem like it's easier to think about that bit in the
2239600	2246480	case of perception, where the idea would be that, you know, I predict the current sensory flow and
2247120	2253120	it's only the errors in that prediction that then need further processing. So, you know,
2253120	2259040	I think I've woken up in my bedroom and I predict a certain sensory flow and as I turn my head around,
2259040	2264720	I'm not getting any information that corresponds to that. I might have to go scrabbling or use
2264720	2269360	these prediction errors to retrieve the information that actually I'm in a hotel room and it's going
2269360	2277040	to look pretty different. So, I think it's in that sense that when you don't have to scrabble that
2277040	2283840	hard, this is a super, super efficient way of doing moment by moment processing because
2283840	2287760	if you already predicted it properly using what you know about the world, you don't need to take
2287760	2293440	any further action. So, in that sense, it's exactly like JPEGs and motion compressed video where the
2293440	2299360	idea is, you know, if I've already transmitted the information about this frame of the video,
2299360	2305680	then in order to know what the next frame is, all I need to do is react to whatever the errors
2305680	2310080	would be if I assumed it was just like the frame before and normally then there's a few residual
2310080	2317520	errors and if you use those as the information that you need to deal with, you get the right picture.
2317520	2324880	So, these prediction errors, these residual errors, they carry whatever sensory information is
2324880	2330560	currently unexplained. They'll explain whatever you can with prediction and then just you don't
2330560	2335840	need that much bandwidth really to just use the rest to refine it. I remember hearing, it might
2335840	2340880	have been in a conversation with David Eagleman here on the podcast, the idea that the reason why
2341680	2348640	the years seem to go by faster as we get older is because there's less novelty in our lives. You
2348640	2353280	know, the first time we go to a beach, it's all new and we really expend a lot of mental energy
2353280	2358480	taking it in. Whereas the 20th time, we already have a background and the errors are not that big.
2358960	2361200	Does that fit into this story? It sounds like it does.
2361200	2366880	Yeah, I think that fits really, really well. It also helps explain why it's so hard to learn
2366880	2373040	new things at a certain point in your life because the inputs that you're receiving get sucked into
2373040	2379680	these well-worn troughs in your current world model, if you like. I think us academics find
2379680	2385040	that a lot as we get older in something like, oh, yeah, I understand that. That fits with this
2385040	2391120	kind of story that I've already got. Sometimes it's very, very hard to give the new evidence it's
2391120	2397760	proper due. Of course, attention is a tool for doing that, but that takes a lot of deliberate
2397760	2405520	effort sometimes to really, really attend. Attention reverses the dampening effect that
2405520	2413120	prediction normally has. Part of the evidence base for this story, this account, is that
2413360	2421040	well-predicted sensory inputs cause less neural activity than other sensory inputs.
2421840	2425280	There's something odd there. These are the ones we deal with very fluidly,
2425840	2430560	and yet there's less going on in the brain when you deal with them than in other cases.
2431600	2437280	That's one of the signatures that led, I think, to predicted coding coming on the sea.
2438080	2442800	You open a can of worms when you say how the old academics get stuck in a rut there,
2442880	2448640	but I have to just follow up a little bit. I think there's two different perspectives
2448640	2454160	I could put forward. One is more or less what you just said. As we get into our ruts, it takes
2454160	2459280	more and more energy, activation energy or whatever to think in new ways, but the other is,
2460480	2467200	maybe this is more optimistic, because we are older and good at certain ways of thinking,
2467760	2474320	we forget that it was hard to not be good at any ways of thinking. We forget how hard it was to
2474320	2479760	learn calculus or French or whatever. Therefore, we're just not willing to put in the work anymore,
2479760	2485200	even if it were exactly the same amount of work. Yes, I think that's right. We end up in a situation
2485200	2491840	of expert perceivers in general, where expert perception clearly is being able to take this
2491840	2499760	unruly sensory information and just see what it actually means for victory on the chess board
2499760	2504800	or whatever it or whatever you happen to be engaged in. I think, in a way, all successful
2504800	2510800	perception is expert perception in that sense, and academic expertise is in the same sort of
2510800	2519920	ballpark. It has the same sort of blind spots as well. If there is something interesting going on,
2519920	2527360	but it doesn't fall within the bounds of the existing generative model to use the right
2527360	2534000	vocabulary for the predictive processing story, then the only thing you can do with it is use it
2534000	2539760	to drive torturous new slow learning and see why a lot of time we don't really want to do that.
2539760	2545200	Don't want to do that. No. Well, look, you've used the word, relatedly, use the word efficiency as
2545200	2553440	one of the benefits to this model. If we think of probably the first ever video storage software
2553440	2557840	was more or less like a movie where you just had every frame and you stored it, right? Then only
2557840	2563680	later did you realize it was much more efficient to just update the changes. Presumably, since our
2563680	2569920	brains were not intelligently designed and grew up through evolution, this is a nice feature to
2569920	2574640	have. We're trying to do the most with finite resources. Is that a big part of the attraction
2574640	2580160	of this story that it requires less thinking or energy or calories or what have you?
2580800	2586320	Yes, I think that's right. I think if the brain didn't work this way, but we were able to do the
2586320	2595360	kinds of things that we do, we'd probably have brains overheated, repeated. The amount of energy,
2595360	2601600	there is a trade-off here. The trade-off is keeping the world model, the generative model
2601680	2607360	you're using to make the predictions going. That, of course, is a big metabolic cost. That's kind
2607360	2614080	of the cost that is being his signature you see in spontaneous cortical activity. There's stuff
2614080	2618400	going on all the time. All the time, the kind of thinking about the resting state work from
2618400	2626080	Raeckel and others as well. That is a cost, but that is traded against a moment by moment cost
2626160	2632320	of processing all this incoming information. I think what we're doing is we've traded the cost
2632320	2640800	across time there and we're using a fairly metabolically expensive upkeep of a world model
2640800	2648560	to allow a much more efficient moment-by-moment response, a faster one as well. Being somewhat
2648560	2653520	anticipatory about what's going on in the world around you is a pretty good thing if you're an
2653520	2660720	animal involved in all kinds of conflicts and dangerous situations. Also, there are wiring costs.
2661680	2668480	It's hard to know how best to think about the wiring costs, but the amount of downward flowing
2669120	2675920	wiring and information seems to very often outnumber the inward flowing stuff,
2676480	2683680	sometimes up to 10 to 1, certainly 4 to 1. It looks as if brains have decided. It's been
2683680	2688240	decided over evolutionary time that this is a good thing to do, and I think that that means
2688240	2694400	it is overall the most efficient way to proceed. I also worry that in a way if you want understanding,
2694400	2698400	there might be no other way to proceed. That's kind of maybe there's something more
2699520	2705120	abstract or philosophical going on there, but I don't know what understanding would look like.
2706560	2712400	As it were, you weren't bringing a world model of some kind to bear on the current sensory
2713360	2719840	evidence in a context sensitive way. There's a question there that I don't know. I really
2719840	2725440	don't know the answer to because I know they're a kind of formal proof. Anything that can be done
2726080	2732880	using a system that has feedback like that can be unfolded into a feed-forward system.
2733520	2740000	Maybe the upshot there is, well, maybe you could unfold us into a feed-forward system for a few
2740000	2746000	bits of processing or something, but it would require a huge brain and a ridiculous amount of
2746000	2749600	energy, but somebody out there maybe might want to think about that.
2749600	2752720	Yeah, exactly. I was going to say this does sound like a research project for somebody,
2753280	2760720	but there's the benefit of this mechanism that the brain uses, the predictive processing model.
2761680	2768320	Presumably, there's also downsides. Maybe one is that we're very susceptible to illusions.
2769040	2772000	We think we're seeing things that aren't there because it's part of what our brain
2772000	2778800	is predicting very, very strongly. Yes, absolutely. I'm subject to several of these illusions. I'm
2778800	2783760	sure we all are. Phantom phone vibration is probably a good one. I often might think that
2783760	2789440	phone's going off in your pocket when it's not even in your pocket. I'm now susceptible to phantom
2789440	2796480	wrist vibrations since I gave in and bought a smartwatch. The hollow mask illusion, I guess,
2796480	2805760	is a classic in this area where an ordinary joke shop mask, if viewed from the concave side,
2805760	2811120	so viewed from behind in a certain sense with a light source, behind the mask, you'll think that
2811120	2819040	it's an ordinary outward face that you're seeing. Again, that seems to be because we have very strong
2819120	2825040	predictions about the concavity of normal face structures. We very seldom see anything that
2825040	2832960	isn't like that. That prediction now trumps the real sensory evidence specifying concavity,
2834080	2840400	except it doesn't in everyone. Autism spectrum condition folk are slightly less susceptible
2840400	2847120	to that illusion and to the McGurk illusion, for example, from a predictive processing viewpoint.
2847200	2852080	That's probably because of a slightly altered balance, one where sensory inputs are somewhat
2852080	2857680	enhanced relative to expectations. What was the other illusion you mentioned, the McGurk?
2858240	2863440	Oh, the McGurk. It's like a ventriloquism illusion. Yes, sorry. It's this one where
2864160	2874000	it's a sort of Bargar illusion. There is a sound which you can be played such that
2874800	2880560	if the lips are moving a certain way, you'll hear it as Gar, and if they move in a different way,
2880560	2883680	you'll hear it as Bar, but it's exactly the same sound. Exactly the same sound.
2884800	2889360	It's not entirely clear whether that's an illusion or something else. It's hard to know exactly where
2889360	2896160	the boundaries of illusions and other kinds of inference lie. I mean, these kind of optical
2896160	2906480	and auditory illusions almost seem fun and benign, but presumably we can take the basic picture that
2906480	2909840	there's an enormous amount of information coming into our senses at every moment,
2909840	2916320	and we can't possibly process it all. Therefore, we filter it. We filter it to fit our perceptions
2916320	2922720	and then correct for the errors. That must also work with abstract concepts or news items just
2922720	2929680	as much as pictures that we get through our eyes. Yes, I think that's right. There is a tendency to
2929680	2937600	not just to see what we expect to see, which can be very damaging too, but sometimes to read what
2937600	2944160	we expect to read. Well, by read there, I mean sort of read into a text, what you expect to be
2944160	2950000	learning from that take. I think many of us do this. I think all of us probably do this when
2950000	2955920	we read news articles where it looks like they're saying something that we're very much either sympathetic
2955920	2961680	to or opposed to, but maybe when you look at that text again or you go over it with a fine tooth
2961680	2965360	car, when you look at it with someone that's got a different perspective, you realize that's
2965360	2973040	really in that text. It's just what I brought to bear in some way. In the visual case, Louise
2974000	2984640	Feldman Barrett, Lisa Feldman Barrett has this lovely but very, very scary example of the way that
2985680	2993200	interceptive predictions, predictions about your own bodily state, are getting perhaps crunched
2993200	2999840	together with extraceptive ordinary sensory information in ways that could perhaps give you
2999920	3010320	experiences where an object that is reached for in a dark alley might actually appear to you as a
3010320	3020640	gun when really it's just a smartphone. In sort of much less worrying and sort of more
3020640	3026320	controllable cases, you can show that if you give people false cardiac feedback,
3026320	3031520	so you make them think their heart's beating faster than it is, then a face that would otherwise
3031520	3038720	look neutral to them is judged to be an angry face or a worrying face. So it does seem as if
3038720	3046080	we're using internal information to help make the predictions that structure our experience of the
3046080	3052080	external world. And so that's something that can go right or go wrong as well. So we end up seeing
3052160	3057040	what we expect to see and maybe even believing what we expect to believe in some sense.
3057680	3064240	Yes. I mean, there is a sort of, you know, some people say seeing is believing, but in these cases
3064240	3070560	believing is kind of seeing. And maybe in these interception cases, feeling is seeing as well.
3070560	3074960	That's the way that Feldman Barrett puts it. Another former Minescape podcast guest,
3074960	3082160	I got to say Lisa Feldman Barrett. Are there therefore features or things out there in the
3082160	3088960	world that we are systematically bad at perceiving because of this way that we process information?
3089760	3096240	It's a lovely question and one that I haven't thought about. So things that we just, all of us,
3096240	3103680	tend to miss because, well, I suppose the most obvious answer there is unusual events.
3105040	3111760	So, you know, you would have seen the, you know, the footage of the gorilla walking across the
3111760	3119200	scene where you're trying to count the passes of the baseball. So he's worked by Simon and
3119200	3128400	Chappris. Anyway, it's his classic work that in sort of attentional blindness and sorry,
3128400	3132400	inattentional blindness. So the idea there is that if you're concentrated on one task,
3132400	3137840	something really quite dramatic could happen and you just wouldn't notice it. I think we're all very,
3137840	3147840	very subject to that. So outliers, I suppose. So predictive brains are tuned in to patterns
3147840	3154160	that have helped them solve problems before. And so whenever we're confronted with an outlier
3154160	3159840	situation, we're likely to miss it unless for some reason we're attending right there. But why
3159840	3164400	would we be? Because attention is driven by where we expect the good information to be.
3165200	3171200	And this is why, for example, expert drivers are very, very good at a lot of things,
3171200	3177040	but they can miss a cyclist if they're approaching a roundabout from entirely the wrong direction.
3177040	3181840	Somewhere that, you know, no one comes from that direction at the roundabout. So there's a lot of
3181840	3187920	these looked but didn't see, as they call them, accidents where people might even move their
3187920	3193760	heads in that direction. But if it's that unexpected, they just don't see what's going on.
3193760	3197600	Presumably, these are all quantitative questions. I mean, if something is
3198320	3202160	blatant enough, we're going to see it even if we didn't predict it. Is that fair to say?
3203280	3212160	I think, yes, that's fair to say as long as we're able to attend to it. So it needs to be able to,
3212720	3216960	and some things will try and grab our attention. So like a loud noise. It's a really loud noise,
3217040	3222720	like those scaffolders I had out my window earlier happened. Then that will grab my attention,
3222720	3229520	unless I'm really, really desperately focused in all my attention somewhere else as I might be.
3229520	3236480	So if you think about stage magic, stage magic is a really nice case where rather dramatic things
3236480	3242880	can suddenly appear on stage. And most people don't notice them because attention has been so very,
3243280	3252560	carefully controlled by the magician. Because attention is up in the waiting on either specific
3252560	3260640	predictions or prediction errors. So it's a really, really super important part of the
3260640	3266320	predictive brain story, this balancing act that is very in moment by moment.
3267120	3271600	That's a great example. I like the idea that professional illusionists are just
3271600	3274240	leveraging the fact that our brains are predictive processors.
3275040	3278320	That's true. Yeah, no, there's a beautiful book. I can't remember the title now,
3278320	3286800	but it's by Luis Martinez. And it is a book written by neuroscientists about stage magic,
3286800	3290640	leveraging the picture of the predictive brain as a way of understanding a lot of it.
3291280	3295760	Very, very good. Okay, so let me then be put on the skeptical hat just a little bit.
3296720	3301360	And this is similar to questions that are raised for Carl Frist and etc.
3301360	3306320	Look, if our brain is trying in some sense to minimize prediction error,
3306880	3314080	can't it do that best by not collecting any data just by hiding away and being completely
3314080	3320080	unsurprised because we never leave our room? Yeah, I think that's such a nice worry to have
3320080	3325440	because it leads right into this hugely important dimension of intraceptive prediction and
3325440	3330960	artificial curiosity. So let's maybe talk about these things for a moment. So there's this
3330960	3338000	sort of worry that it's sometimes called the dark and room. But the best place for a predictive
3338000	3343680	brain to be would be to lead us into a dark corner and just keep the same. So all of those sensory
3343680	3348080	inputs they just keep on keep on coming just the same. You predict them perfectly, but you wither
3348080	3353600	and die. Now we don't do that, obviously. Does that mean the predictive brain story is wrong?
3354400	3358000	Obviously, I don't think so. I think what it means is that there's more to the predictive
3358000	3365520	brain story than just bringing extra sensitive sensory information into line with predictions.
3366800	3370400	We're making all these intraceptive predictions all the time. I'm predicting
3371200	3378960	the state of my own body. I'm predicting, for example, that I should at all times have
3379040	3387840	sufficient supplies of water and glucose, for example. And then action is automatically taken,
3387840	3393600	not when I don't have enough water or glucose, but long before I don't have enough water or
3393600	3400320	glucose. So if you start to feel thirsty, that will be happening before you've reached the
3400320	3405680	point at which you're going to wither and die without water. And if you take a drink when you're
3405680	3411120	thirsty or feel relief, but that water will have no effect on you physiologically for about 20
3411120	3417040	minutes. So the relief is as much a prediction as the original thirst was. This again is an example
3417040	3423360	from Lisa Feldman Barrett. So once you put intraception in that way into the picture, then of
3423360	3428160	course we're not going to stay and not eat and not drink in the darkened room because we have these
3428160	3435760	kind of chronic systemic expectations, if you like, of staying alive. But then there's a bit
3435760	3440640	more to it than that, I think, as well. Because we don't stay in boring rooms either. So you could
3440640	3446400	put me in a very boring room, but give me enough food and water and all of that stuff. And I wouldn't
3446400	3453280	like that very much either. I might even start to do things. I might start to play games by
3453280	3460080	drawing on the wall or something like this. And so this now falls under the umbrella of kind of
3460080	3469280	work in artificial curiosity. And predictive brains are naturally artificially curious brains
3469280	3476000	because minimizing prediction error is their basic reward. So you could sort of say for brains like
3476000	3480560	that the only thing really that's rewarded is minimizing prediction error. That's what they
3480560	3486240	want to do. And if they're not performing any particular tasks, they'll still try and find
3486240	3491360	some prediction error to minimize because that's the kind of thing that they are. And this is what
3491360	3499280	makes predictive brains general purpose structure learners. So there's some rather nice work that's
3499280	3507280	been done by Rosalyn Moran at UCL. I think it's UCL. And what she's been doing is comparing reward
3507280	3514400	driven learners with prediction driven learners and finding that the purely reward driven learners
3515200	3520560	will learn a way of solving the problem more rapidly than the prediction driven learners
3520560	3526800	very often, but frequently a more shallow way of solving the problem. Because as soon as they find
3526800	3532880	a way to reliably get the reward, they kind of stick. Whereas a prediction driven system,
3532960	3538720	it basically wants to minimize prediction error as much as possible. And that leads it to explore
3538720	3544400	its environment repeatedly learning more and more about it. And if you then seed it with a goal,
3545200	3554000	and simultaneously, it will outperform a pure reward driven agent. So that I think when you
3554000	3558640	put those two things together, you kind of see that the darkened room isn't really much of a
3558640	3565520	threat. We don't like death and we don't like boredom. And both of those things seem to be
3565520	3570160	natural effects of being driven to minimize errors in prediction.
3570160	3575360	I guess the way out that comes to my mind directly, which I'm not sure if it's the same
3575360	3581520	thing as what you are proposing or it's something different. But in physics, when we calculate
3581520	3587680	the entropy of a system and we say is that maximum entropy, that's telling us some distribution of
3587680	3593440	all the possible states it could be in, blah, blah, blah. But famously, we can calculate maximum
3593440	3599200	entropy of a system subject to different constraints, right? Like subject to it's at a certain
3599200	3604640	temperature or it's at a certain pressure or whatever, and we'll get different forms for that
3604640	3612240	probability distribution. So couldn't we just say that there are two things going on? We want to
3612320	3618560	minimize prediction error, but we also want to survive. So there's a constraint we want to survive.
3618560	3625040	And under that constraint, it's actually useful to go out and be surprised sometimes so we can
3625040	3629600	update our predictive model. And I think that would, I don't know how mathematically that would
3629600	3634800	work out, but it does seem a little bit intuitive to me. Yeah. And I think that does mathematically
3634800	3640160	work out. I think this is the sort of stuff that Carl Friston can speak to more reliably than I can.
3640160	3648640	But it looks as if, very often, the correct move for a prediction-driven system
3649280	3656640	is to temporarily increase its own uncertainty, so as to do a better job over the long time scale
3656640	3662320	of minimizing prediction errors. And that looks like the value of surprise, actually, and that we
3662320	3669200	will, I think we artificially curate environments in which we can surprise ourselves. I think,
3669200	3675120	actually, this is maybe what art and science is to some extent, at least, we're curating
3675120	3681280	environments in which we can harvest the kind of surprises that improve our generative models,
3681280	3687280	our understandings of the world, in ways that enable us to be less surprised about certain
3687280	3692160	things in the future. I wonder if you could use this idea or a set of ideas to make
3692160	3698240	predictive models for what kind of games people would like to play or what kind of stories or
3698240	3704000	novels or movies people would like to experience. Or, I guess, in music, it's very famous, right?
3704000	3708960	You want some rhythm, some predictability, but you also want some surprise also. There's a sweet
3708960	3716720	spot in the middle. Yes, I think that's exactly right. Karen Kukonen, a literary theorist in
3717760	3723680	Scandinavia somewhere, has written a nice book called Probability Designs. And so she is using
3723680	3730960	the vision of the predictive brain as a way of understanding the shape of literary materials,
3730960	3735360	poems and novels. Okay, there you go. And my idea is that we should think of every novel, every poem,
3735360	3741280	as a probability design, leading us through sort of building up expectations, cashing them out,
3742000	3747360	building up at multiple levels, giving precision, weighting to some of the expectations versus
3747360	3753280	others. And clearly, you know, this makes sense of music as well. There's an awful lot of that
3753280	3759040	going on in music. And probably it applies to all sorts of things, even like roller coaster design,
3759040	3765040	I imagine. It's exactly that. A roller coaster is a kind of probability design. What's interesting
3765040	3770400	is how we get surprised again and again, even if we ride the same roller coaster or read the same
3770400	3776400	novel or listen to the same piece of music. And I think that shows the skill of the constructor
3776400	3783200	in giving us inputs that activate bits of our model, drive in expectations again and again,
3783200	3788080	to the point where you're still surprised in some sense, even though you could have said beforehand,
3788080	3793760	that's what's going to happen. You still can't help but be surprised. And this, in some sense,
3793760	3798560	at some level would be the best thing to say. And I think this speaks a little bit to the idea that
3798560	3805040	as prediction machines, we are multi level machines, we're not just, it's not just there's a prediction,
3805040	3810640	and it's either cashed or it isn't, but this prediction exists as a high level abstraction
3810640	3814960	predicting a lower level one, predicting a lower level one, predicting a lower level one,
3814960	3821440	all the way down to the actual incoming notes of the tune or words on the page.
3822880	3828480	It's because we're multi level prediction machines, I think, that things like honest
3828480	3835840	placebo's work. So you know, placebo's obviously for rather nicely under this sort of general
3835840	3842160	account, because expectations of relief thrown into the pot can make a difference to the amount
3842160	3847280	of relief that you feel. But if you're told that you're being given a placebo, it can still make
3847280	3851840	a difference to the amount of relief that you feel. Presumably, that's because there are all
3851840	3858000	these sub levels of processing that are getting automatically activated by good packaging delivery
3858000	3864560	by people in white coats with authoritative voices on this sort of thing. So I think that's
3864560	3869760	something that we might learn from these accounts too, that we should maybe medicine and society
3869760	3877440	could make more use of ritual and, yeah, ritual and packaging and things that in some sense,
3878720	3883280	I don't know, in some, well, I want to say ineffective. What I mean is they don't,
3883840	3887440	they bring about their effects, but not through the standard routes.
3887440	3892960	Good. Okay, very, very good. Okay, when, you know, we're late in the podcast, we can get a little
3892960	3902000	bit wilder and more profound here. So you, in the book, you gesture toward ideas along the lines of
3902640	3906320	we are not only, you know, the thing that the thing to get in mind is that we're not just
3907040	3911920	sensing reality and writing it down, we are in some sense participating and bringing it about.
3914000	3918480	Maybe even using phrases like what you think of as reality is really a hallucination.
3919120	3922400	Tell us exactly how far we can go along that rhetorical road.
3923120	3932080	Yeah. Proceed with caution would be the right thing to say because lots of people talking
3932080	3936880	about these things, myself included, use this phrase perception is controlled hallucination.
3937840	3943520	And you can see why, because the idea here is that perception is very much a constructive process
3944640	3950160	in which your own expectations get thrown into the pot and they help you see what you see.
3951440	3957840	And those same expectations are thrown into the pot of feeling your body the way that you feel it.
3957840	3962960	And so a lot of our medical symptoms, in fact, all our medical symptoms reflect some kind of
3963760	3970320	combination of expectation and whatever sensory evidence the body is actually kind of throwing
3970320	3978320	up at that time. So this is really a very, very powerful story. But when you think about perception
3978320	3982800	of controlled hallucination, we need to take the notion of control pretty seriously.
3984160	3990800	For that reason, in some slightly more philosophical works, I've tried to argue that we should
3990800	3995600	flip the phrase around and think of hallucination as uncontrolled perception.
3995600	4000960	And that that, as it were, puts a boot on the right foot somehow. It lets us say that when these
4000960	4005600	things are working properly, you're in touch with the world. This is a way of using what you know
4005600	4010640	to stay in touch with the world as it matters to an embodied organism like you trying to do the
4010640	4017600	things you're trying to do. But then of course, when it goes wrong, when the perception is
4017600	4023520	uncontrolled, if you like, then you get hallucination. You will get a hallucination when
4023520	4027440	you're kind of disconnected from the world in your predictions, your brain's predictions are
4027440	4033120	doing all the work. And then if you turn the dial in the other direction, and your brain's predictions
4033120	4039200	aren't doing enough work, you fail to spot fake patterns in noisy environments and be easily
4039200	4049280	overwhelmed. So I feel like thinking of hallucination as uncontrolled perception is actually the
4049280	4053760	better way to do it, even though it's clumsy to say, and it doesn't even roll of my tongue that
4053760	4059760	easily, which is why I don't think I actually bothered making that move in the experience
4059760	4067120	machine book. You know, it's really hard to resist drawing a parallel with large language models
4067120	4072640	here, because of course, their entire job is predicting what's supposed to come next, right?
4072640	4077600	And guess what? They famously hallucinate. They say things that are completely false,
4077600	4083120	but interestingly, they do things with apparent confidence, right? They don't hesitate or mumble
4083120	4088640	when they're hallucinating to them. It comes out just as definitive as the truth does. And maybe
4088640	4096720	there is a parallel there. Yes, I think that's right. I mean, their hallucinations are in a way,
4096720	4102400	I think, partly at least the result of them not being anchored in perception action loops in the
4102400	4107360	right sorts of ways. So, you know, it's this anchoring in perception action loops that sort of
4107360	4112000	teaches us a lesson when we're young. It's like, you know, if you get things wrong, bad things are
4112000	4117920	going to happen to you. If I don't spot the edge of the path as being the right place, then I'm
4117920	4122080	going to fall over and I'm going to get signals that I chronically don't like as it were. So
4122160	4128000	all the interceptive predictions are coming into play there. Whereas if all you're doing is predicting
4128000	4135440	the next word in the sentence, and your reward is basically being kept alive as a large language
4135440	4141920	model, then why not just, you know, go the whole hog and be confident about a nice structured piece
4141920	4149840	of bullshit that you can generate? At the same time, it's interesting that by changing the prompts
4149840	4155440	to the large language models or to chat, chat GPT anyway, you can make it do substantially better.
4155440	4161840	So you can say something like, you know, writes this for me in the style of a well-informed
4162560	4167680	scientific expert. It makes less mistakes. It still still tends to hallucinate references,
4167680	4175920	but at least it's a little bit better. But yeah, so something very thin about just predicting
4175920	4181440	the next symbol. You know, I just feel like it's not very well anchored in reality. And so
4181440	4186720	hallucinations are kind of, you can't tell the difference between a hallucination and
4186720	4191680	something else. Maybe that's the point. Unfortunately, when we're in the grip of hallucination,
4191680	4195200	we can't tell the difference either. Someone on Twitter coined the term
4195200	4201680	hallucitations when chat GPT makes up papers you haven't written. I like that. I like that.
4201680	4204720	Yeah, they appear all the time. I'm going to start including them on my CV.
4205680	4212720	Are there implications, you know, if we get down and dirty and not philosophical for how to treat
4213680	4221280	mental issues that we have, whether it's, you know, depression or pain or anything like that?
4221280	4224080	Or do we get actionable intelligence from this way of thinking?
4224640	4230880	Yes, I think we do. I mean, it's early days. But I think particularly in the case of pain,
4230880	4237440	there are some clear sorts of recommendations here that are being implemented by people working
4237440	4242480	in what they call pain reprocessing theory, which is just to find a high pollutant label for the
4242480	4253520	idea that you reframe your pains. So, you know, the thought would be that we tend to treat pain
4253520	4260640	as a signal that we shouldn't be doing something. If you reframe it as, okay, my pain
4260640	4266640	signaling system is misfiring, then you can begin to think, so this pain doesn't mean that I shouldn't
4266640	4272800	be doing it. And it turns out that if you get people involved in those regimes, they start to be able
4272800	4277840	to do a bit more because they're not scared of stopping because of the pain. And actually, as
4277840	4284400	they find that they can do more, the pain itself presents itself to them as lessening. I think
4284400	4288880	because the brain sort of infers, well, if I'm doing this stuff, it can't be that bad, can it?
4288880	4295600	So there's a kind of virtuous cycle that replaces a vicious cycle that was there before, which was,
4295600	4300960	you know, this is going to hurt, so I'm not going to do it. And then if I do stop to do it,
4300960	4306400	oh, it really seems to hurt, I'm not doing it. So pain reprocessing theory is one nice case.
4308000	4314960	I suppose self affirmation is another case, the idea that the idea that, you know, if you're prone
4314960	4320080	to thinking that perhaps you're not going to do well in some tests because you're in a certain
4320080	4327280	minority group or you're female and it's an ass test or something like this, self affirmation
4327280	4331760	in advance of the test can really make a difference. You know, I'm good at this, I can do this,
4332880	4340320	lots of people like me do this, that kind of thing. It's important, of course, not to over
4341280	4345840	over egg the custard, as we might say on this side of the Atlantic anyway.
4347040	4355120	You know, you can't reframe, you can't reframe having this sort of bacterial infection in a way
4355120	4360000	that is really going to make any difference, the bacterial infection that you've got and
4360000	4364960	reframing makes a big difference to cancer related fatigue, it doesn't make much difference to cancer.
4365920	4371200	So I think we have to be, you know, we have to be aware of the limits. And in general,
4371200	4377760	I don't think that these stories, these counts are kind of positive thinking sorts of account,
4377760	4382960	they're kind of, they're more like, you know, there are many factors that are involved as we
4382960	4388720	construct our experiences. And some of those factors are our own expectations.
4388720	4393280	Yeah, okay, that sounds perfectly sensible put that way. I'm not an expert, but my rough
4393280	4399680	impression is that we don't, we know embarrassingly little about pain and how it works. It's an
4399680	4403920	understudied area, I guess, I don't know whether it's for moral or psychological reasons. So any
4403920	4409040	little insight might be very helpful. Yeah, yes, I agree. And I think that I think pain research
4409040	4415440	is actually moving into some very, very interesting stages now as we understand more about the ways
4415440	4420560	in which people's own expectations make a difference. So even where, where there's a very
4420560	4429200	standard physiological cause, it, people's experiences of their, of their pain, very tremendously,
4429200	4434560	and even within a certain individual, their experiences, very tremendously from context
4434560	4440400	to context and day by day, in ways that just aren't tracking the organic as well. I don't
4440400	4444160	like this word organic, there's always something organic going on, aren't tracking the sort of
4444160	4450080	the standard, the standard calls. So what's probably happening is that different contexts
4450080	4456880	are activating different expectations of pain or disability. And without the standard organic
4456880	4460960	calls changing in any way, that's making a difference to how you feel and what you can do.
4461760	4465360	I actually just realized I forgot to ask a crucially important question earlier.
4466400	4472480	We had Nannis Mayel on the podcast a while back and my new colleague at Johns Hopkins,
4472480	4477200	and talking about physics and the arrow of time. And you know, through talking to her and
4477200	4483360	through talking to other people, I have this vague idea of why we think that time flows,
4483360	4489600	why we have the sense of time passage. And it's because we are constantly predicting a moment
4489600	4494400	in the future and also remembering a little bit in the past and updating, right? And you know,
4494400	4500320	the updates happen in one direction of time and that's what gives us this sense of flow or passage.
4501280	4504160	Given your expertise, does that sound at all on the right track?
4504880	4510880	Yeah, that sounds very, that sounds like a really nice story to me or account even.
4510880	4513520	There's an account there. I like story. Go ahead with story.
4514880	4522080	It's, I mean, it reminds me a little bit actually of Husserl, the kind of classic
4523280	4528720	philosophical phenomenologists who had this idea that experience is this sort of,
4529440	4534400	this thing which is rooted, rooted in the past, but always looking towards the future.
4534400	4540320	And the present is this sort of just a kind of just where those things meet,
4541440	4548160	where the, you know, I don't know what really gives us the arrow of time, that sort of sense that
4549120	4554240	the idea that you can't sort of unscramble the egg or whatever it is, it's not obvious to me quite
4554240	4563200	why my prediction machinery is kind of what's delivering the fact that it looks like I really
4563200	4569760	can't recreate the egg from scratch. Well, I think that's one for you. That is one for me.
4569760	4576560	And I think that I halfway agree in that the account has not been fully fleshed out, although I'm
4576560	4580960	100% sure it's ultimately because entropy is increasing. We just have to draw the connections
4580960	4585280	there, which is the useful work to be done. It sounds like a great account of why we think
4585280	4591280	there's an arrow of time. Yeah, why we feel it. Right. Why that's part of our image of the world.
4591280	4596960	Okay. So then the last question is the flip side of the pain question. There's this idea called
4596960	4602880	the hedonic treadmill, which I think some people have said have been, has been discredited. But
4602880	4611040	the idea that we get happy not because of our overall welfare, but because of changes in our
4611040	4616160	welfare. And if we win the lottery and now we're rich and living in luxury, soon we have exactly
4616160	4621600	the same happiness as we had before. There are challenges to this view. So I'm not even sure
4621600	4626320	if it's true. There's a replication crisis in psychology. Everything is very easy to me. But
4626960	4632720	it does seem compatible with the whole predictive modeling view of what the brain is doing. If
4632720	4638240	what we're doing is constantly predicting what's happening next, then can happiness be understood
4638240	4642080	as noticing that our prediction was a little pessimistic and things are actually a little bit
4642080	4648640	better? Yeah, that's interesting. I hadn't thought about this, but it sounds like it should for
4648640	4655840	rather neatly into place with this sort of dampening of the well-predicted. So the fundamental
4655840	4661360	starting point for a lot of these accounts was that the neuronal response to well-predicted
4661360	4668160	sensory inputs is dampened. So if these sensory inputs are supposed to be driving pleasurable
4668160	4674640	experiences, but you've really been through that 100 million times before, then the pleasure is,
4674640	4680640	I think, going to be diminished, perhaps unless you can actively reach in there with a tension
4680640	4686960	and try to stop that. So I wonder whether someone that really loves the taste of a particular wine
4686960	4694160	that they had a million times before, as long as they can reach in with a tension and up the dial
4694160	4700880	on what's coming in through the senses, then maybe they can artificially surprise themselves
4700880	4705440	a little bit, if you see what I mean. I do. I'm not sure about that, but I feel like there's
4705440	4711760	something there of wine tasters are told to do this to sort of sit back and let it speak for
4711840	4715440	itself so that you don't get sucked into your own expectations.
4716000	4719520	But now it brings up questions. I know I said it was the last question, but
4720080	4726400	there's an issue here of high versus low pleasures or simple versus subtle pleasures, right? I mean,
4726400	4732720	I'm a big wine fan and I absolutely do get pleasure from very fancy complicated sophisticated wine,
4732720	4738240	because I can't afford to have it too often, so that it's not boring to me. But I also get pleasure
4738240	4742720	from like the perfect slice of pizza, which is very simple and predictable and whatever, but
4742720	4746400	I get that comforting pleasure. So now I'm not sure what to think.
4747120	4752400	Yes, I mean, you know, I think I'm not quite sure what to think about those cases either. I mean,
4752400	4758960	it does seem to me that we, you know, because our brains of prediction minimize in engine,
4759520	4766880	then in a way, we do kind of want to live in worlds that we can predict well and that have
4766960	4772240	certain sorts of things. And so when those things are bringing hedonic benefit, then I think
4773040	4777280	existing there is going to be a rather comfortable way of existing, even though we also have
4778400	4783840	a sort of a company and drive to increase our states of information to sort of, you know,
4783840	4790000	learn a bit more in case it lets us generate a slightly alternative future in which, you know,
4790000	4795440	the pizza is square, but we're liking it even better or something. But I think that
4796160	4801360	I think that if we think about these things delicately as sort of multi level and multi
4801360	4807600	dimensional prediction engines, then we can accommodate both the both the drive for novelty
4807600	4814560	and the attractions of staying within the space where where actually all that stuff that wants
4814560	4820240	us to minimize errors is doing rather well in a local sense. Even just looking at a pizza,
4820240	4824080	you're minimizing lots of errors. After all, just to see the shape of the pizza in front of you,
4824080	4828400	you're kind of moving your eyes around, harvesting information, minimizing errors,
4828400	4833200	you're getting some hedonic kick out of it, nothing bad is happening anywhere. It's a pretty
4833200	4839360	comfortable place to be. You know, I'd like to leave messages for the young intellectuals out
4839360	4844160	there who are deciding what to do with their lives. It sounds like there's a sweet spot here
4844160	4849120	where we do know something about the brain and the body and how they work and how they fit together,
4849120	4853120	but there's still a lot of good questions left on the table to be answered by the future.
4853120	4857840	I think there's a huge number of questions. You know, every story, every account we've had so
4857840	4862560	far has turned out to be wrong and I'm sure this one will too. So the question is, you know,
4862560	4866960	what's a stepping stone towards? Well, it's a very good story. You're telling us Andy Clark.
4866960	4871360	Thanks so much for being on the Mindscape podcast. Thank you. It's been a real pleasure. Thank you.
4913120	4914500	you
4943120	4944000	you
