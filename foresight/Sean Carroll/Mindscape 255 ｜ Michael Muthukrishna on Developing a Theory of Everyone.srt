1
00:00:00,000 --> 00:00:04,000
Hello, everyone. Welcome to the Mindscape Podcast. I'm your host, Sean Carroll.

2
00:00:04,000 --> 00:00:10,000
Probably everyone listening has heard the phrase, a theory of everything, right?

3
00:00:10,000 --> 00:00:13,000
The origins of this phrase are actually a little bit murky.

4
00:00:13,000 --> 00:00:18,000
You know, it's not such a weird collection of words that it hadn't appeared in the literature a long time ago.

5
00:00:18,000 --> 00:00:22,000
You know, people have talked about the idea of a theory of everything.

6
00:00:22,000 --> 00:00:33,000
But the context in which we currently usually talk about it is either super string theory in physics or some other competitor to super string theory.

7
00:00:33,000 --> 00:00:39,000
As far as I can tell, that coinage came from John Ellis, who is a theorist at CERN.

8
00:00:39,000 --> 00:00:48,000
He wrote a little article in the early days of the super string revolution, 1980s, called the super string, a theory of everything or of nothing.

9
00:00:48,000 --> 00:00:55,000
So it wasn't completely triumphant, right? He was actually talking about whether or not this possibly could be a theory of everything.

10
00:00:55,000 --> 00:01:03,000
It was immediately pointed out by non-physicists and even some physicists that the idea of a theory of everything in this sense

11
00:01:03,000 --> 00:01:10,000
is actually very, very restricted to a certain reductionistic way of looking at the fundamental constituents of nature,

12
00:01:10,000 --> 00:01:15,000
the idea being that nevertheless those fundamental constituents make up everything else.

13
00:01:15,000 --> 00:01:22,000
But there's no sense in which that idea of a theory of everything would be the final theory that we ever need in science, right?

14
00:01:22,000 --> 00:01:30,000
Because we do have higher level emergent things. We have biological organisms, we have economies and societies,

15
00:01:30,000 --> 00:01:40,000
and we have people in psychology, all this stuff, right, that is certainly not covered, not to mention easy questions about astrophysics or chemistry or so forth.

16
00:01:40,000 --> 00:01:46,000
So the root from that kind of theory of everything to many other scientific questions is a complicated one.

17
00:01:46,000 --> 00:01:53,000
Therefore, we still have room for other theories of large grandiose scope.

18
00:01:53,000 --> 00:02:00,000
And today's guest is Michael Muthukrishna, who has a book coming out called A Theory of Every One.

19
00:02:00,000 --> 00:02:04,000
That's intentional play on words from the theory of everything idea.

20
00:02:04,000 --> 00:02:11,000
The subtitle is The New Science of Who We Are, How We Got Here and Where We're Going.

21
00:02:11,000 --> 00:02:16,000
Obviously, kind of an ambitious book trying to synthesize many kinds of ideas.

22
00:02:16,000 --> 00:02:19,000
Michael is at the London School of Economics.

23
00:02:19,000 --> 00:02:25,000
He's sort of, I think his official title is a Professor of Economic Psychology.

24
00:02:25,000 --> 00:02:27,000
So there's both economics in there and psychology.

25
00:02:27,000 --> 00:02:34,000
He's actually collaborated with some of our previous Minescape guests, Joe Henrich and Edward Slingerland.

26
00:02:34,000 --> 00:02:46,000
So he thinks about the grand scope of human history and how we are influenced by biology and evolution, cultural evolution,

27
00:02:46,000 --> 00:02:50,000
how we pass down ideas from generation to generation.

28
00:02:50,000 --> 00:02:58,000
And a big thing, as he'll say in the episode, a big thing for him is the fact that human beings, unlike other animals,

29
00:02:58,000 --> 00:03:04,000
do have this sort of distributed cultural knowledge that we can pass down.

30
00:03:04,000 --> 00:03:09,000
Human beings have schools, right? Have an educational system in a way that other animals don't.

31
00:03:09,000 --> 00:03:12,000
Other animals do teach their young. Some of them do.

32
00:03:13,000 --> 00:03:23,000
I don't know. Lions, I'm told, teach their cubs to hunt by pretending to be in pain when their cubs bite them a little bit.

33
00:03:23,000 --> 00:03:26,000
So they're teaching them, like, oh, yes, that's the right thing you should do.

34
00:03:26,000 --> 00:03:28,000
But they don't have a curriculum. They don't have a syllabus.

35
00:03:28,000 --> 00:03:32,000
They don't have it quite as organized and systematized as human beings do.

36
00:03:32,000 --> 00:03:38,000
So that plays a large role in the exceptionalism of human beings.

37
00:03:38,000 --> 00:03:43,000
And of course, it's tied to other capacities, our brains, our languages, things like that.

38
00:03:43,000 --> 00:03:44,000
So we'll talk about all of that.

39
00:03:44,000 --> 00:03:46,000
But then Michael wants to go much further.

40
00:03:46,000 --> 00:03:51,000
He talks about not only economics, but the technological side of economics,

41
00:03:51,000 --> 00:04:00,000
the importance of different ways we have of harnessing energy from, you know, just burning wood to burning fossil fuels to solar energy

42
00:04:00,000 --> 00:04:09,000
and how these allow for expansions of the economy and therefore of human opportunity in a very clear way.

43
00:04:09,000 --> 00:04:16,000
And then he brings it back or brings it down, I should say, to the current era, our political polarization,

44
00:04:16,000 --> 00:04:23,000
our stagnation of innovation, if that's true, and what we can do about it, how we can do better.

45
00:04:23,000 --> 00:04:28,000
There are parts of this long, elaborate story that I am 100% on board with.

46
00:04:28,000 --> 00:04:33,000
Other parts that I will wait to see, let's put it that way, not 100%, but I'm open to it.

47
00:04:33,000 --> 00:04:35,000
I think it's fascinating stuff.

48
00:04:35,000 --> 00:04:39,000
And Michael is very clear and very well educated on a whole bunch of things.

49
00:04:39,000 --> 00:04:41,000
So I think it's a very good listen.

50
00:04:41,000 --> 00:04:44,000
With that in mind, let's go.

51
00:04:53,000 --> 00:05:03,000
Michael Muthukrishna, welcome to the Mindscape podcast.

52
00:05:03,000 --> 00:05:05,000
Thanks for having me, Sean.

53
00:05:05,000 --> 00:05:11,000
So you've written what sounds to the average reader of titles like an ambitious book.

54
00:05:11,000 --> 00:05:13,000
It's called A Theory of Everyone.

55
00:05:13,000 --> 00:05:16,000
And I like that you're kind of making fun of the physicists there.

56
00:05:16,000 --> 00:05:19,000
It is, you know, it's a modest title.

57
00:05:19,000 --> 00:05:21,000
It could have been a theory of everything, right?

58
00:05:21,000 --> 00:05:23,000
Yeah, exactly. Just people.

59
00:05:23,000 --> 00:05:26,000
Yeah, no, it's a play on a theory of everything, of course.

60
00:05:26,000 --> 00:05:33,000
But I mean, the title hints at the central claim of the book, which is that, you know, we are...

61
00:05:33,000 --> 00:05:37,000
So I'm sure your listeners know already, but, you know, the theory of everything in physics

62
00:05:37,000 --> 00:05:43,000
is this grand, unifying theory, you know, general relativity meets quantum mechanics, whatever.

63
00:05:43,000 --> 00:05:50,000
The idea or the central premise of the book is that we have reached this point in the human and social sciences

64
00:05:50,000 --> 00:05:54,000
where we have this overarching theoretical framework that unifies things

65
00:05:54,000 --> 00:05:59,000
that make sense of an otherwise chaotic and confusing world and turns it into a real science.

66
00:05:59,000 --> 00:06:04,000
And so, you know, I tell the story of how this happened in the more mature sciences

67
00:06:04,000 --> 00:06:08,000
to illustrate how this has happened for the human and social sciences.

68
00:06:08,000 --> 00:06:12,000
So, you know, like in physics, right?

69
00:06:12,000 --> 00:06:15,000
We used to think that Thor was banging his hammer

70
00:06:15,000 --> 00:06:18,000
and the physical world was the result of capricious gods.

71
00:06:18,000 --> 00:06:23,000
And then, you know, folks like Newton and Maxwell and Einstein come along and they write down equations.

72
00:06:23,000 --> 00:06:27,000
I mean, Maxwell is crazy, right? At the time of like whale oil and horse-drawn carriages,

73
00:06:27,000 --> 00:06:29,000
he's writing down equations for like a magnet.

74
00:06:29,000 --> 00:06:31,000
It is. It's amazing, yeah.

75
00:06:31,000 --> 00:06:34,000
But suddenly, like the world is less chaotic and confusing.

76
00:06:34,000 --> 00:06:36,000
We know, like the weather is still difficult to predict, actually,

77
00:06:36,000 --> 00:06:39,000
but we know that it's not caused by capricious gods.

78
00:06:39,000 --> 00:06:42,000
It's caused by these rules. We have the rules.

79
00:06:42,000 --> 00:06:45,000
And once we have those, we can also turn them into technologies, right?

80
00:06:45,000 --> 00:06:49,000
Like we can eventually land, you know, a spacecraft on an asteroid.

81
00:06:49,000 --> 00:06:52,000
And the same thing happened to chemistry.

82
00:06:52,000 --> 00:06:57,000
Now, Newton's not a dumb guy, as we just said, but he's trying to turn lead into gold.

83
00:06:57,000 --> 00:06:59,000
And again, it's not because he's a dumb guy.

84
00:06:59,000 --> 00:07:02,000
It's because he doesn't understand like collusion theory.

85
00:07:02,000 --> 00:07:06,000
He doesn't know the world is made up of elements that fit into a periodic table.

86
00:07:06,000 --> 00:07:12,000
But once you have a periodic table, you know, then alchemy becomes chemistry.

87
00:07:12,000 --> 00:07:15,000
So you can do stuff before you have a periodic table.

88
00:07:15,000 --> 00:07:17,000
Like we got to gunpowder, for example.

89
00:07:17,000 --> 00:07:21,000
But, you know, why is it that when you pour this acid on a metal,

90
00:07:21,000 --> 00:07:23,000
you're creating this gas and it's bubbling away?

91
00:07:23,000 --> 00:07:24,000
Like what's happening there, right?

92
00:07:24,000 --> 00:07:26,000
And can we turn lead into gold?

93
00:07:26,000 --> 00:07:28,000
Maybe it seems like another chemical reaction.

94
00:07:28,000 --> 00:07:30,000
But once you have a periodic table, you can be like,

95
00:07:30,000 --> 00:07:32,000
yes, we can make gunpowder and it's like,

96
00:07:32,000 --> 00:07:34,000
okay, yes, we know what's going on here.

97
00:07:34,000 --> 00:07:36,000
But no, we can't learn to turn lead into gold.

98
00:07:36,000 --> 00:07:37,000
Those are different elements.

99
00:07:37,000 --> 00:07:39,000
The world is less chaotic and confusing.

100
00:07:39,000 --> 00:07:43,000
And eventually you make your way not only to chemistry,

101
00:07:43,000 --> 00:07:46,000
like, you know, turning oil into plastics or whatever,

102
00:07:46,000 --> 00:07:49,000
but even, you know, eventually to things like protein folding.

103
00:07:49,000 --> 00:07:52,000
And of course, this happened in biology too, right?

104
00:07:52,000 --> 00:07:56,000
So it was a chaotic and confusing world.

105
00:07:56,000 --> 00:07:59,000
Why do some animals have live words and others lay eggs

106
00:07:59,000 --> 00:08:01,000
and why does the peacock have this giant elaborate tail

107
00:08:01,000 --> 00:08:03,000
and the peahen doesn't?

108
00:08:03,000 --> 00:08:07,000
And then along comes Darwin and later on the modern synthesis

109
00:08:07,000 --> 00:08:10,000
with, you know, Fisher and Wright and Hamilton and so on.

110
00:08:10,000 --> 00:08:13,000
And suddenly the world is less chaotic and confusing.

111
00:08:13,000 --> 00:08:16,000
It's still difficult to predict the trajectory of species

112
00:08:16,000 --> 00:08:18,000
or the behavior of ecologies.

113
00:08:18,000 --> 00:08:20,000
But if you're trying to do that without the rules

114
00:08:20,000 --> 00:08:22,000
that underlie the system, you're running blind.

115
00:08:22,000 --> 00:08:23,000
Right.

116
00:08:23,000 --> 00:08:25,000
So yeah, in other words, what I hear is that

117
00:08:25,000 --> 00:08:29,000
this underlying theoretical perspective on what's going on

118
00:08:29,000 --> 00:08:32,000
can be helpful at a higher practical level.

119
00:08:32,000 --> 00:08:35,000
And let me guess, you want to extend this to human beings.

120
00:08:35,000 --> 00:08:36,000
Exactly.

121
00:08:36,000 --> 00:08:38,000
I mean, the claim I make is not that, like,

122
00:08:38,000 --> 00:08:40,000
I have extended to human beings.

123
00:08:40,000 --> 00:08:42,000
But actually we are in the midst of a revolution

124
00:08:42,000 --> 00:08:45,000
on the scale of Newton and Maxwell and Einstein

125
00:08:45,000 --> 00:08:47,000
and the scale of the periodic table

126
00:08:47,000 --> 00:08:49,000
and the scale of Darwin and the modern synthesis.

127
00:08:49,000 --> 00:08:51,000
That is, we know the rules.

128
00:08:51,000 --> 00:08:54,000
We know why humans are different to other animals.

129
00:08:54,000 --> 00:08:56,000
We know the rules by which humans acquire

130
00:08:56,000 --> 00:08:58,000
and synthesize information.

131
00:08:58,000 --> 00:09:01,000
We know the rules by which societies are changing over time.

132
00:09:01,000 --> 00:09:04,000
And that should allow us to build the equivalent of technologies,

133
00:09:04,000 --> 00:09:06,000
a better job of public policy, for example,

134
00:09:06,000 --> 00:09:08,000
by applying that.

135
00:09:08,000 --> 00:09:10,000
So part one of the book is kind of laying out

136
00:09:10,000 --> 00:09:12,000
that theory of everyone, that theory of human behavior.

137
00:09:12,000 --> 00:09:14,000
And then part two is like, okay, well,

138
00:09:14,000 --> 00:09:16,000
take me at face value and assume that I'm correct

139
00:09:16,000 --> 00:09:18,000
about what I'm saying.

140
00:09:18,000 --> 00:09:22,000
Here's what this means about inequality and innovation

141
00:09:22,000 --> 00:09:24,000
and how we design the internet and, you know,

142
00:09:24,000 --> 00:09:26,000
what the future of education should look like

143
00:09:26,000 --> 00:09:30,000
and how we get past energy transitions and so on.

144
00:09:30,000 --> 00:09:32,000
And how many pages is this book, like 5,000?

145
00:09:32,000 --> 00:09:34,000
I was like 480 pages or something.

146
00:09:34,000 --> 00:09:36,000
You know, I mean, I lean heavily.

147
00:09:36,000 --> 00:09:38,000
It is a book for the public.

148
00:09:38,000 --> 00:09:40,000
So what I try to stay away from, like,

149
00:09:40,000 --> 00:09:42,000
you'll read some behavior books and it's like,

150
00:09:42,000 --> 00:09:44,000
in this study we showed XYZ.

151
00:09:44,000 --> 00:09:46,000
I mean, the studies are there.

152
00:09:46,000 --> 00:09:48,000
But what I'm really trying to say is like,

153
00:09:48,000 --> 00:09:50,000
there is a tapestry.

154
00:09:50,000 --> 00:09:52,000
There's a world that everything is connected up.

155
00:09:52,000 --> 00:09:54,000
It's like once I explain the periodic table to you,

156
00:09:54,000 --> 00:09:56,000
everything makes a lot more sense.

157
00:09:56,000 --> 00:09:58,000
And so I rely heavily on like, let me explain it to you

158
00:09:58,000 --> 00:10:00,000
so you can check it against your own life.

159
00:10:00,000 --> 00:10:03,000
It's that allow you to see the world as you never saw before.

160
00:10:03,000 --> 00:10:05,000
And once you see it, you can't unsee it.

161
00:10:05,000 --> 00:10:08,000
So maybe a good starting point then is the classic question

162
00:10:08,000 --> 00:10:11,000
of what makes human beings so special?

163
00:10:11,000 --> 00:10:16,000
What makes us different than the rest of our animal cousins

164
00:10:16,000 --> 00:10:19,000
even though we're actually very similar at the DNA level, right?

165
00:10:19,000 --> 00:10:20,000
Yeah.

166
00:10:20,000 --> 00:10:23,000
So what humans are a different kind of animal

167
00:10:23,000 --> 00:10:25,000
that have a third line of information

168
00:10:25,000 --> 00:10:27,000
or a second line of inheritance.

169
00:10:27,000 --> 00:10:29,000
And this is really fundamentally what allowed us

170
00:10:29,000 --> 00:10:31,000
to build the world that we have today.

171
00:10:31,000 --> 00:10:35,000
So we not only have a genetic inheritance like hardware

172
00:10:35,000 --> 00:10:38,000
that we get from our parents, right?

173
00:10:38,000 --> 00:10:41,000
And we not only have kind of a lifetime of experience

174
00:10:41,000 --> 00:10:44,000
where we learn things, but we have cultural software

175
00:10:44,000 --> 00:10:47,000
that has accumulated over time

176
00:10:47,000 --> 00:10:49,000
and is literally running on our brains

177
00:10:49,000 --> 00:10:51,000
allowing us to think new thoughts,

178
00:10:51,000 --> 00:10:53,000
see the world in richer new ways

179
00:10:53,000 --> 00:10:56,000
and understand the world as no other species ever has.

180
00:10:56,000 --> 00:10:59,000
And that cultural line isn't just learned.

181
00:10:59,000 --> 00:11:01,000
It is evolving.

182
00:11:01,000 --> 00:11:04,000
It's not an analogy to genetic evolution.

183
00:11:04,000 --> 00:11:07,000
It is an extension of population genetics

184
00:11:07,000 --> 00:11:08,000
into the world of culture.

185
00:11:08,000 --> 00:11:10,000
We call this cultural evolution.

186
00:11:10,000 --> 00:11:13,000
And it's evolving beyond conscious awareness.

187
00:11:13,000 --> 00:11:16,000
So we have a set of behaviors and values

188
00:11:16,000 --> 00:11:20,000
and sometimes technologies that are not only...

189
00:11:20,000 --> 00:11:22,000
The world is not only more complicated

190
00:11:22,000 --> 00:11:25,000
than the smartest among us could ever recreate,

191
00:11:25,000 --> 00:11:28,000
but often we don't really understand why it is

192
00:11:28,000 --> 00:11:29,000
that we do the things we do,

193
00:11:29,000 --> 00:11:30,000
certainly at an individual level

194
00:11:30,000 --> 00:11:32,000
and sometimes at a population level.

195
00:11:32,000 --> 00:11:34,000
Like you probably brush your teeth twice a day

196
00:11:34,000 --> 00:11:36,000
and if I ask you why,

197
00:11:36,000 --> 00:11:38,000
you'll tell me something about plaque and tartar

198
00:11:38,000 --> 00:11:39,000
or something like that,

199
00:11:39,000 --> 00:11:41,000
but you don't really have a good causal model of that.

200
00:11:41,000 --> 00:11:44,000
Or you probably finish your course of antibiotics, right?

201
00:11:44,000 --> 00:11:46,000
To avoid superbugs.

202
00:11:46,000 --> 00:11:49,000
And you probably have a causal explanation for that.

203
00:11:49,000 --> 00:11:51,000
But you have an illusion of explanatory depth

204
00:11:51,000 --> 00:11:53,000
that stops you from checking further.

205
00:11:53,000 --> 00:11:55,000
I'm going to say to you, it's like obviously Sean,

206
00:11:55,000 --> 00:11:57,000
like not every one of those bacteria are going to die.

207
00:11:57,000 --> 00:11:59,000
And so surely if not all of them die,

208
00:11:59,000 --> 00:12:01,000
surely finishing your course of antibiotics

209
00:12:01,000 --> 00:12:04,000
would have the most resistant bacteria left.

210
00:12:04,000 --> 00:12:06,000
That's what would create superbugs.

211
00:12:06,000 --> 00:12:08,000
And now you're like, oh, wait a minute.

212
00:12:08,000 --> 00:12:10,000
You know, but you don't have to worry

213
00:12:10,000 --> 00:12:12,000
about kind of antibiotic kill curves

214
00:12:12,000 --> 00:12:15,000
or, you know, the optimal dosage or anything like that.

215
00:12:15,000 --> 00:12:18,000
You have just acquired the interfaces

216
00:12:18,000 --> 00:12:19,000
to interact with the world.

217
00:12:19,000 --> 00:12:22,000
You check your emails on a device that may as well be magic.

218
00:12:22,000 --> 00:12:25,000
So the book is about how we evolved to have that.

219
00:12:25,000 --> 00:12:29,000
How it is that we became a different kind of animal.

220
00:12:29,000 --> 00:12:31,000
How that connects everything up.

221
00:12:31,000 --> 00:12:34,000
And what that means about how we should study ourselves

222
00:12:34,000 --> 00:12:36,000
and think about our future, right?

223
00:12:36,000 --> 00:12:38,000
Like if you want to understand the secrets of pivot tables

224
00:12:38,000 --> 00:12:41,000
in Excel, if you want to understand how chatGPT works,

225
00:12:41,000 --> 00:12:43,000
you don't look inside the CPU and GPU.

226
00:12:43,000 --> 00:12:44,000
It's not in the brain.

227
00:12:44,000 --> 00:12:45,000
It's not in the neuroscience actually.

228
00:12:45,000 --> 00:12:47,000
It's in the software that's running on that hardware.

229
00:12:47,000 --> 00:12:49,000
So the difference in other words,

230
00:12:49,000 --> 00:12:51,000
obviously other kinds of animals and so forth

231
00:12:51,000 --> 00:12:56,000
do have behaviors that are passed down from generations,

232
00:12:56,000 --> 00:12:58,000
but they're not through learning, right?

233
00:12:58,000 --> 00:13:01,000
I mean, are there, it's always a worry

234
00:13:01,000 --> 00:13:04,000
when you talk about vast generalizations to other species

235
00:13:04,000 --> 00:13:07,000
that there's going to be counter examples.

236
00:13:07,000 --> 00:13:10,000
Are there any counter examples of, you know,

237
00:13:10,000 --> 00:13:12,000
things that are not genetically encoded,

238
00:13:12,000 --> 00:13:15,000
but our knowledge that other animal species

239
00:13:15,000 --> 00:13:17,000
passed down from generation to generation?

240
00:13:17,000 --> 00:13:18,000
Yeah.

241
00:13:18,000 --> 00:13:20,000
So this area is called dual inheritance theory

242
00:13:20,000 --> 00:13:22,000
or gene culture co-evolution

243
00:13:22,000 --> 00:13:24,000
or the extended evolutionary synthesis.

244
00:13:24,000 --> 00:13:26,000
And particularly the extended evolutionary synthesis

245
00:13:26,000 --> 00:13:29,000
does, it is about like these other transmission,

246
00:13:29,000 --> 00:13:31,000
like so environments are transmitted, right?

247
00:13:31,000 --> 00:13:34,000
An earthworm can inherit the environment created,

248
00:13:34,000 --> 00:13:37,000
that more moist environment created by its ancestors.

249
00:13:37,000 --> 00:13:40,000
There's also information being transmitted,

250
00:13:40,000 --> 00:13:43,000
just not to the degree that humans do, right?

251
00:13:43,000 --> 00:13:45,000
So like chimps, for example, learn from their mothers

252
00:13:45,000 --> 00:13:47,000
and they have traditions of, you know,

253
00:13:47,000 --> 00:13:50,000
wadding up leaves into sponges and sponging up water

254
00:13:50,000 --> 00:13:53,000
or using sticks for digging termites or whatever.

255
00:13:53,000 --> 00:13:54,000
And those do seem to be trying,

256
00:13:54,000 --> 00:13:56,000
or cracking nuts is another example of this.

257
00:13:56,000 --> 00:13:58,000
They're entering the stone age, if you like.

258
00:13:58,000 --> 00:14:00,000
They do have that, but humans,

259
00:14:00,000 --> 00:14:02,000
for reasons I explained in the book,

260
00:14:02,000 --> 00:14:04,000
and maybe I can say a little bit about that now,

261
00:14:04,000 --> 00:14:07,000
rely on that social information

262
00:14:07,000 --> 00:14:11,000
and agree to use that information if you like.

263
00:14:11,000 --> 00:14:13,000
We use that information at the expense

264
00:14:13,000 --> 00:14:15,000
of our own everyday experience.

265
00:14:15,000 --> 00:14:17,000
And chimps do not do that.

266
00:14:17,000 --> 00:14:19,000
Like, let me give you an example.

267
00:14:19,000 --> 00:14:21,000
You're the worst person to give this example to,

268
00:14:21,000 --> 00:14:22,000
but I'm going to give it to you anyway.

269
00:14:22,000 --> 00:14:24,000
You'll see in a moment why that is.

270
00:14:24,000 --> 00:14:26,000
Like, if I ask the average person, right?

271
00:14:26,000 --> 00:14:28,000
Or the average person will swear up and down

272
00:14:28,000 --> 00:14:31,000
that we are on a spheroid rotating around a star,

273
00:14:31,000 --> 00:14:33,000
one of many stars in the Milky Way,

274
00:14:33,000 --> 00:14:35,000
you know, one of many galaxies in the universe.

275
00:14:35,000 --> 00:14:37,000
They will swear that up and down.

276
00:14:37,000 --> 00:14:39,000
And every day they look outside at a flat earth

277
00:14:39,000 --> 00:14:42,000
with a sun tracing the sky from east to west, right?

278
00:14:42,000 --> 00:14:43,000
Now you might be, you know,

279
00:14:43,000 --> 00:14:46,000
you're among a handful of people who checked, right?

280
00:14:46,000 --> 00:14:48,000
But for most people, they don't.

281
00:14:48,000 --> 00:14:51,000
They just believe it because the smartest people

282
00:14:51,000 --> 00:14:54,000
in their society, most people in their society,

283
00:14:54,000 --> 00:14:56,000
they believe that, right?

284
00:14:56,000 --> 00:14:59,000
For the longest time,

285
00:14:59,000 --> 00:15:02,000
UFOs were like in the realm of the flat earth, right?

286
00:15:02,000 --> 00:15:04,000
And maybe they still should be, but at the moment,

287
00:15:04,000 --> 00:15:07,000
because like the reliable sources are talking about

288
00:15:07,000 --> 00:15:10,000
what are they called now, UAPs or whatever, and aliens.

289
00:15:10,000 --> 00:15:12,000
Now people are like, wait, is that actually a thing?

290
00:15:12,000 --> 00:15:15,000
Like, should I be reconsidering my model of the world?

291
00:15:15,000 --> 00:15:18,000
We should go back to the previous consensus understanding,

292
00:15:18,000 --> 00:15:19,000
I think there.

293
00:15:19,000 --> 00:15:22,000
But this is interesting because you're saying two things,

294
00:15:22,000 --> 00:15:26,000
one of which I was expecting and one of which is a little bit,

295
00:15:26,000 --> 00:15:29,000
I've heard it before, but it's unexpected in this context.

296
00:15:29,000 --> 00:15:32,000
The expected thing is, you know, we not only have knowledge,

297
00:15:32,000 --> 00:15:35,000
we have some kind of symbolic knowledge, right?

298
00:15:35,000 --> 00:15:37,000
So, you know, we can package the knowledge

299
00:15:37,000 --> 00:15:40,000
and spread it more efficiently between ourselves.

300
00:15:40,000 --> 00:15:43,000
And that's obviously some kind of phased transition

301
00:15:43,000 --> 00:15:45,000
in human behavior that we can talk about.

302
00:15:45,000 --> 00:15:47,000
But you're putting more emphasis than I would have guessed

303
00:15:47,000 --> 00:15:51,000
on our limitations, you know, which I think are very important.

304
00:15:51,000 --> 00:15:53,000
But it's interesting to hear in this context, you know,

305
00:15:53,000 --> 00:15:57,000
we're finite, we don't have infinite ability to do our own research

306
00:15:57,000 --> 00:15:59,000
in the current parlance.

307
00:15:59,000 --> 00:16:03,000
So we kind of have to rely on our social networks a bit.

308
00:16:03,000 --> 00:16:04,000
That's right.

309
00:16:04,000 --> 00:16:06,000
And so it's actually because of those limitations

310
00:16:06,000 --> 00:16:09,000
that we ended up with this cultural evolutionary system.

311
00:16:09,000 --> 00:16:12,000
It's because of those limitations that we deferred the calculation,

312
00:16:12,000 --> 00:16:16,000
the computation, the innovation to the population level.

313
00:16:16,000 --> 00:16:21,000
So because we couldn't, you know, we couldn't fit all of that in our heads,

314
00:16:21,000 --> 00:16:27,000
we ended up like selectively learning and copying without really understanding

315
00:16:27,000 --> 00:16:29,000
that we were able to surpass it.

316
00:16:29,000 --> 00:16:31,000
So I'll give you, there's an experiment I talk about in the book

317
00:16:31,000 --> 00:16:32,000
that really illustrates this,

318
00:16:32,000 --> 00:16:34,000
what humans do compared to other great apes.

319
00:16:34,000 --> 00:16:38,000
So this was an experiment done with young children and young chimps, right?

320
00:16:38,000 --> 00:16:41,000
And so the experimenter has a box, and the box has a hole on the top

321
00:16:41,000 --> 00:16:43,000
and the hole on the side, right?

322
00:16:43,000 --> 00:16:44,000
And the experimenter takes the stick

323
00:16:44,000 --> 00:16:47,000
and pokes it through the hole in the top and pokes it through the hole on the side.

324
00:16:47,000 --> 00:16:50,000
And when they do this they poke it through the hole in the side,

325
00:16:50,000 --> 00:16:53,000
they, they get a piece of fruit, which the chimps love.

326
00:16:53,000 --> 00:16:55,000
And they get a sticker, which the kids love.

327
00:16:55,000 --> 00:16:58,000
So the experimenter pokes a hole through the top, pokes a hole through the side,

328
00:16:58,000 --> 00:16:59,000
you know, hands it to the chimp.

329
00:16:59,000 --> 00:17:00,000
What does the chimp do?

330
00:17:00,000 --> 00:17:02,000
Pokes a hole through the top, pokes a hole through the side.

331
00:17:02,000 --> 00:17:04,000
All right, experimenter pokes a hole through the top,

332
00:17:04,000 --> 00:17:06,000
pokes a hole through the side, hands it to the child.

333
00:17:06,000 --> 00:17:07,000
What does the child do?

334
00:17:07,000 --> 00:17:09,160
Pokes a hole through the top, pokes a hole through the side.

335
00:17:09,160 --> 00:17:14,440
Okay. Now in the key variation in the treatment, you have the same box, but it's no longer

336
00:17:14,440 --> 00:17:17,240
a black box. It's transparent. You can see into it.

337
00:17:17,240 --> 00:17:17,720
Okay.

338
00:17:17,720 --> 00:17:22,040
And you realize that that first action doesn't do anything. There's actually like a ceiling

339
00:17:22,040 --> 00:17:26,600
or a floor, you know, where it's just hitting that. It's to retrieve the fruit and the stick,

340
00:17:26,600 --> 00:17:27,960
or you just need the second action.

341
00:17:28,520 --> 00:17:28,920
Okay.

342
00:17:28,920 --> 00:17:32,760
But again, the experiment to take the stick pokes a hole through the top, pokes a hole through

343
00:17:32,760 --> 00:17:35,960
the side, hands it to the chimp. Now, if you've ever watched like a chimp, you know,

344
00:17:35,960 --> 00:17:39,000
doing a working memory task or spinning through Instagram, chips are smart, man.

345
00:17:39,000 --> 00:17:39,400
Very good.

346
00:17:39,400 --> 00:17:39,720
Yeah.

347
00:17:39,720 --> 00:17:43,560
And so what do they do? Ignore that first action. Go straight to the second action

348
00:17:43,560 --> 00:17:45,880
and they retrieve their fruit and they're a happy chimp.

349
00:17:47,000 --> 00:17:49,800
What does the child do? Experiment to pokes a hole through the top, pokes a hole through

350
00:17:49,800 --> 00:17:52,520
the side, hands to the child. What does the child do? Pokes a hole through the top,

351
00:17:52,520 --> 00:17:53,480
pokes a hole through the side.

352
00:17:54,200 --> 00:17:58,840
So children, you have a head full of recipes that you don't fully understand.

353
00:17:58,840 --> 00:17:59,240
Interesting.

354
00:18:00,280 --> 00:18:04,840
That you've accumulated and you acquire from the smartest people since you were a child around

355
00:18:04,840 --> 00:18:09,240
you. Some of those you've checked, many of those you haven't and if your life is going well,

356
00:18:09,240 --> 00:18:12,440
you've never had a need to check some of the beliefs that you hold and the behaviors you

357
00:18:12,440 --> 00:18:19,560
engage in. But because we're smart because we're dumb, we're smart because we ate better than

358
00:18:19,560 --> 00:18:24,760
apes do. And because we're willing to defer to that body of information culture,

359
00:18:25,800 --> 00:18:29,720
we don't sit naked in the rain like they do. They want to figure everything out from first

360
00:18:29,720 --> 00:18:34,440
principles. They want to understand it before they do it. And so they're stuck. They're stuck

361
00:18:34,520 --> 00:18:36,200
at a particular limit by their brains.

362
00:18:36,200 --> 00:18:40,280
That's a wonderful example. I love that experiment because the cynic might have said,

363
00:18:40,280 --> 00:18:44,440
see the children, human children, not as smart as the chimps because they can't figure it out.

364
00:18:44,440 --> 00:18:50,600
But your spin on it is whether or not they're smarter, they're relying on a whole thing that

365
00:18:50,600 --> 00:18:55,240
the chimps don't have access to, this sort of collective knowledge that gets us through the

366
00:18:55,880 --> 00:18:57,480
complicated social environment we're in.

367
00:18:58,040 --> 00:19:03,240
That's right. But you see that collective knowledge isn't just about behaviors and

368
00:19:03,240 --> 00:19:08,680
beliefs. It's also about mental tools, like literal upgrades to our cultural software for

369
00:19:08,680 --> 00:19:13,720
thinking. And we know some of this through history, thanks to history, which is the cultural

370
00:19:13,720 --> 00:19:18,760
fossil record if you like. So if you take something like numeracy, some small-scale

371
00:19:18,760 --> 00:19:24,280
societies still count one, two, three, and then many. And our ancestors probably counted like that

372
00:19:24,280 --> 00:19:32,760
too. We were able to get further using a metaphor. So many societies would use stones, think calcium,

373
00:19:32,760 --> 00:19:38,840
calculus, maybe stones pressed into clay. So you've got notches as well or cut into wood.

374
00:19:38,840 --> 00:19:43,880
Many societies use body parts. So we have a decimal system, which is super annoying,

375
00:19:43,880 --> 00:19:47,800
to be honest. It doesn't translate well to binary. It's the worst of all systems.

376
00:19:48,920 --> 00:19:52,920
And other societies use phalanges. So they count 12 with their thumb, the finger bones,

377
00:19:53,560 --> 00:19:57,000
and various other body parts. I wish we'd landed on 16, but here we are.

378
00:19:57,880 --> 00:20:04,040
And so as a result of this, we now had a metaphor for thinking that we no longer had to use. We

379
00:20:04,040 --> 00:20:07,880
could just store it in our heads, represent it in our heads. And now we could count the natural

380
00:20:07,880 --> 00:20:14,360
numbers above one. But stones and fingers and whatever, they don't make zero obvious,

381
00:20:15,000 --> 00:20:21,480
because zero stones is nothing. And nothing is, well, it's nothing. And so it took centuries

382
00:20:21,480 --> 00:20:26,680
before zero was recognized as a number. And centuries, it was actually the 17th, 18th century,

383
00:20:26,680 --> 00:20:32,120
the negative numbers became more obvious. And again, it became, it was an upgrade thanks to a,

384
00:20:32,120 --> 00:20:37,240
the number line. So we moved away from objects to position, right? And, you know, there's this

385
00:20:37,240 --> 00:20:41,560
quote I have in the book from a British mathematician Francis Mezry, and he's something like, you

386
00:20:41,560 --> 00:20:46,280
know, negative numbers darken the very fabric of reality. It's like way melodramatic, right? And

387
00:20:46,280 --> 00:20:49,640
he's right. But once you have a number line, it's simple enough that you can teach it to young

388
00:20:49,640 --> 00:20:53,480
children, which is what we do. Yeah. But it's given us a new ability, because obviously, once you

389
00:20:53,480 --> 00:20:58,920
can count, you can count to do anything. And then the number line itself gets extended, right? So

390
00:20:58,920 --> 00:21:03,560
it's like, what if you have another number line that is, you know, orthogonal to the other one,

391
00:21:03,560 --> 00:21:10,520
it's like, Oh, a complex plane emerges, right? And so you don't, you don't, you can't see this,

392
00:21:10,520 --> 00:21:15,000
because we live in a bubble, a very big bubble. So I don't mean like you and I, you know,

393
00:21:15,640 --> 00:21:19,720
in our ivory towers, you know, I don't mean like coastal elites and rural small town. I mean,

394
00:21:19,720 --> 00:21:25,000
we're all every person we ever meet went through education through school. And school has become

395
00:21:25,000 --> 00:21:28,840
since the Industrial Revolution, the primary means by which we download a cultural package to the

396
00:21:28,840 --> 00:21:33,400
next generation. Because we are a species that relies on socially transmitted information,

397
00:21:33,400 --> 00:21:37,160
you have every child has to catch up on the last several thousand years of human history. And we

398
00:21:37,160 --> 00:21:41,560
try to do it more efficiently, because if we lose that link, we literally become dumber. We

399
00:21:41,640 --> 00:21:46,360
literally lose, lose a bunch of technology, right? We have to transmit that every generation.

400
00:21:46,360 --> 00:21:49,880
But as a result of that, many of the things that we think of as being human

401
00:21:51,000 --> 00:21:56,360
are in fact endowed by us by accumulated cultural knowledge, and you can't get it by measuring it.

402
00:21:56,360 --> 00:21:59,400
So the Stroop test, I don't know if you've ever seen the Stroop test, basically,

403
00:21:59,400 --> 00:22:03,560
you've got words, the color words, and they're written in the color, like red written in red,

404
00:22:03,560 --> 00:22:05,640
blue written in blue. Oh yeah, I do know what you're talking about, but go ahead. Or,

405
00:22:05,720 --> 00:22:12,520
mismatched, right? And you ask people, don't say, don't read the word, just say the color. So,

406
00:22:12,520 --> 00:22:18,360
you know, even though it says red, it's written in blue, so say blue. And people struggle. Reading

407
00:22:18,360 --> 00:22:22,840
has become an instinct that overrides color perception. Now imagine, you know, a psychologist

408
00:22:22,840 --> 00:22:27,080
from Venus, you know, like the anthropologist from Mars. A psychologist from Venus comes down

409
00:22:27,080 --> 00:22:30,840
and studies these crazy apes on, you know, on the planet, and they give them the Stroop test,

410
00:22:30,840 --> 00:22:35,880
and they're like, oh, it appears that humans have an innate ability to read, but no innate

411
00:22:35,880 --> 00:22:40,440
ability or it overrides any kind of color perception, which would be wrong, of course, right?

412
00:22:40,440 --> 00:22:46,280
But because these culturally endowed skills have become part of our thinking, and they're

413
00:22:46,280 --> 00:22:51,560
ubiquitous at this point, you can't measure it. You know, all of experimental psychology, all of,

414
00:22:52,440 --> 00:22:58,200
even IQ tests were developed after the advent of truancy laws and compulsory formal education.

415
00:22:59,080 --> 00:23:02,600
So, even something like the ability to reason. So, I talk about some of my own work, right?

416
00:23:03,320 --> 00:23:07,640
So, Alexander Luria, we were replicating some of his work. So, Luria goes out to Uzbekistan. He

417
00:23:07,640 --> 00:23:12,440
wants to understand how education is changing people's psychology. And he uses if p, then Q

418
00:23:12,440 --> 00:23:17,640
reasoning, okay? So, this is actually one of his questions. He says, where it snows, the bears are

419
00:23:17,640 --> 00:23:22,360
white. In Novia's Emilia, it snows, what color are the bears? Now, you didn't say it, but you're

420
00:23:22,360 --> 00:23:28,840
thinking white. You know, like Uzbeks with education, white. My six-year-old, white, you know?

421
00:23:29,800 --> 00:23:34,440
But what happens to the Uzbeks who didn't have education? How do they answer? They're like,

422
00:23:34,440 --> 00:23:39,160
I think probably brown. I've seen a brown bear. You're like, what? Listen to what I'm saying.

423
00:23:39,160 --> 00:23:43,000
And you're like, well, I don't know what color the bears are. I haven't been to Novia's Emilia.

424
00:23:43,000 --> 00:23:47,240
So, it's not that humans can't reason. Of course we can, right? But it's a grounded,

425
00:23:47,240 --> 00:23:53,640
cultural reality-based reasoning. And it becomes an ability and a hypothetical ability to reason

426
00:23:53,640 --> 00:24:00,280
through our education system. And then the whole world becomes more complex, right? Like, a new

427
00:24:00,280 --> 00:24:07,720
baseline is achieved. So, if you think about TV shows, right? Like, think of like Wambam Batman

428
00:24:07,720 --> 00:24:13,480
for the 1960s, where it is like the dark night from the early Nazis, right? Like, even the most

429
00:24:13,480 --> 00:24:18,920
low-brow television has more convoluted storylines, more characters than, you know, what our parents

430
00:24:18,920 --> 00:24:24,760
and grandparents watched, right? So, that is the central premise. And so, the book is all about,

431
00:24:24,760 --> 00:24:28,920
well, how does that software get written? And how do we write better software for the future?

432
00:24:28,920 --> 00:24:33,720
Because although IQ test results have been going up, they have now stagnated and so have our schools.

433
00:24:33,720 --> 00:24:38,200
Let me ask the sort of chicken and egg problem here, because when we talk about human exceptionalism,

434
00:24:38,200 --> 00:24:42,760
there's a whole bunch of things that happen and maybe they're all related. There was some kind

435
00:24:42,760 --> 00:24:48,200
of phase transition, right? Like, some people will emphasize language and symbolic manipulation

436
00:24:48,200 --> 00:24:54,920
of information. Some people, I had Adam Bulley on the podcast, who is part of a group that likes

437
00:24:54,920 --> 00:25:00,200
mental time travel, the ability to hypothesize the future and speak counterfactually as something

438
00:25:00,200 --> 00:25:05,240
that is uniquely human. And you're, you know, like other people, emphasizing this communal aspect,

439
00:25:05,240 --> 00:25:12,440
the sort of offloading certain cognitive tasks to the collective. Which came first? Is there

440
00:25:12,440 --> 00:25:16,520
some causality there or is it a package deal? Absolutely. So, actually, the book is about,

441
00:25:16,520 --> 00:25:21,000
it tackles this very problem. So, you know, people who, people who posit things like,

442
00:25:21,000 --> 00:25:24,680
so I know Adam, you know, and I know the language people, I'm not sure who you spoke to, but

443
00:25:25,240 --> 00:25:31,640
people who posit language as an explanation are wrong because language is part of the

444
00:25:31,640 --> 00:25:37,640
package of humans. It doesn't explain anything because, you know, in evolutionary biology,

445
00:25:37,640 --> 00:25:41,800
you have a startup problem, right? So, let's say I'm the first person with like some genes for

446
00:25:41,880 --> 00:25:47,240
language. Well, that's useless because nobody else speaks, you know? So, like, you've got a,

447
00:25:47,240 --> 00:25:52,120
you've got a startup problem. And language only works if you've got something worth transmitting.

448
00:25:53,240 --> 00:25:57,480
So, so language isn't an explanation. And a lot of the other features of our,

449
00:25:57,480 --> 00:26:01,720
of our cognition are also this part of this package. So, you know, the story, the,

450
00:26:02,600 --> 00:26:05,880
here's what, here's what a mix of kind of the models and evidence and what you can kind of piece

451
00:26:05,880 --> 00:26:12,200
together tells us, okay? So, the first thing that happened was that there was a transition

452
00:26:12,200 --> 00:26:15,720
across the animal kingdom, but certainly for humans because of our generation length,

453
00:26:15,720 --> 00:26:20,600
toward relying on social information. And it is, it is, if you look at the models and

454
00:26:20,600 --> 00:26:25,400
population genetics, these are the models where you can branch off into a world of social learning.

455
00:26:25,400 --> 00:26:32,120
So, if you, when, when the environment is highly stable, then genes do the best job of adapting

456
00:26:32,120 --> 00:26:37,080
to that environment. So, even among humans, right? Skin color is a genetic adaptation that

457
00:26:37,080 --> 00:26:43,000
is well predicted by latitude because of UV radiation. You don't want so much UV radiation

458
00:26:43,000 --> 00:26:46,840
that you get cancer, but you don't want so little that you have vitamin D deficiency,

459
00:26:46,840 --> 00:26:48,840
which also leads to cancers and other diseases, right?

460
00:26:48,840 --> 00:26:53,320
But the time scale is very long for those adaptations to happen. So, a stable environment

461
00:26:53,320 --> 00:26:58,280
is where it works. They're fairly long, but they happen, right? For, for, for our, for the human

462
00:26:58,280 --> 00:27:01,880
generation length, which is not, you know, the last several thousand years, a hundred thousand years,

463
00:27:01,880 --> 00:27:07,480
right? It's not, sometimes if there's, if selection is strong enough, it can be quite fast.

464
00:27:07,480 --> 00:27:09,560
Okay. And if there's variation in the population.

465
00:27:10,520 --> 00:27:14,280
And so, by the way, you know, remember like humans, like part of this puzzle is that humans,

466
00:27:14,280 --> 00:27:17,400
when most animals encounter a new environment, they're forced to genetically adapt.

467
00:27:17,960 --> 00:27:24,200
And we did a little bit like human skin color, but, you know, like more, more fat for the arctic,

468
00:27:24,200 --> 00:27:30,600
more, you know, proteins to process the local plants. We, as hunter-gatherers, before, you know,

469
00:27:30,600 --> 00:27:34,840
physics, chemistry, biology, psychology, all of this stuff. And even before agriculture,

470
00:27:34,840 --> 00:27:39,800
we marched across the planet and we figured it out in each stage. You know, we figured out how

471
00:27:39,800 --> 00:27:45,320
to process the proteins using, you know, like new, new techniques and so that we could eat them.

472
00:27:45,960 --> 00:27:52,840
You know, we, we, we didn't get much more fat and fur. We hunted the local animals and avoided

473
00:27:52,920 --> 00:27:59,000
prey and war. Sometimes the predators, you know, skins as our own, right? So this is what we need

474
00:27:59,000 --> 00:28:03,000
to explain. This is what we need to explain. So genes do a great job if, if the environment is

475
00:28:03,000 --> 00:28:08,920
highly stable, if the environment is highly unstable, then genes are not a good solution.

476
00:28:08,920 --> 00:28:12,120
Because, you know, today the red berries are edible tomorrow. It's the, the blueberries and

477
00:28:12,120 --> 00:28:17,320
the water is here. The water is there or whatever, right? Instead, evolution has to rely on trial

478
00:28:17,320 --> 00:28:21,240
and error learning across the lifespan to figure it out. You got to have a big brain to do that.

479
00:28:21,240 --> 00:28:25,880
And you got to do a lot of trial and error and it's really inefficient. And so these key models

480
00:28:25,880 --> 00:28:31,000
developed by, you know, mostly by Roy, Rob Boyd and Pete Richardson and also by, you know,

481
00:28:31,000 --> 00:28:37,000
Kveli Shorts and Mark Feldman over at Stanford. There's an intermediate zone of environmental

482
00:28:37,000 --> 00:28:41,880
variability where it is matched to generation length such that your parents and grandparents

483
00:28:41,880 --> 00:28:46,840
have some knowledge worth paying attention to. And it is, it is at this point that we begin

484
00:28:46,840 --> 00:28:50,840
to rely on the information they have. So imagine something like a cyclical drought.

485
00:28:51,240 --> 00:28:56,760
Yeah. Maybe, you know, you've, you've never experienced one and the lag is too long, right?

486
00:28:56,760 --> 00:28:59,800
Maybe your parents haven't either, but grandma remembers that when she was a child, there was

487
00:28:59,800 --> 00:29:03,240
a drought and they went left to the forest past the mountain and, you know, she leads her tribe

488
00:29:03,240 --> 00:29:08,360
to safely. And, you know, I was, I was recently talking to an anthropologist, she was like,

489
00:29:08,360 --> 00:29:12,920
that actually happens. You know, this is a real thing. Kim Hill, you know, was telling me,

490
00:29:12,920 --> 00:29:18,520
you know, from his group. Yeah, check. So, so now in this zone, so this was a nice model developed

491
00:29:18,520 --> 00:29:24,360
in the 1980s, right? In 1981 and 1985, these models were developed. And everyone was like,

492
00:29:24,360 --> 00:29:28,520
oh, that's cool. Yeah, maybe that's what happened. And what happened was in the late 90s and early

493
00:29:28,520 --> 00:29:33,880
2000s, we actually got ice core data. So we could see climatic variation. What did we find?

494
00:29:33,880 --> 00:29:38,920
Massive amounts of variation followed by a moderate zone as, you know, exactly the moment

495
00:29:38,920 --> 00:29:42,680
when humans evolve. Now you should have to, you should pause for a second and be like, well,

496
00:29:42,680 --> 00:29:46,520
wait a minute, all those animals, surely they all experienced, right? There is a, there is a link to

497
00:29:46,520 --> 00:29:50,200
there is a link to generation length for the first step, but you also need something else

498
00:29:50,200 --> 00:29:55,320
to kick this thing off. You've got this bootstrap problem. Okay. So I suspect, you know, in the

499
00:29:55,320 --> 00:29:58,520
book, this is going a little, we don't have a model of this, you know, by the way, you know,

500
00:29:58,520 --> 00:30:04,200
theories only become theories if you can write them down. So, you know, this, what I just said,

501
00:30:04,200 --> 00:30:09,320
we've got good models for good empirical evidence, what I'm about to say, we don't have as good models

502
00:30:09,320 --> 00:30:15,560
except when it comes to brain evolution, which is some of my own work. So I suspect bipedalism

503
00:30:15,560 --> 00:30:20,440
actually did a lot for us. Okay. And the reason for that, so we were, we probably bipedal for

504
00:30:21,000 --> 00:30:25,800
completely separate reasons, right? This is kind of an early adaptation. And by being bipedal,

505
00:30:25,800 --> 00:30:31,880
it did two things. One, it freed our hands for you, for making tools and carrying them around,

506
00:30:31,880 --> 00:30:35,960
right? So you can invest in that stone axe, because you can carry it to the next location

507
00:30:35,960 --> 00:30:40,440
and use it again. Whereas if you're a quadrupedal ape, like a chimp or a orangutan or a gorilla,

508
00:30:40,440 --> 00:30:44,440
whatever, like, you don't want to carry this big ass rock around. So you don't, you have to reuse it.

509
00:30:44,440 --> 00:30:49,400
So cheapen the cost of tools. The second thing that it did is what I'm doing with you right now

510
00:30:49,400 --> 00:30:52,760
that unfortunately, audio listeners can't see, which is talking with my hands, right?

511
00:30:52,760 --> 00:30:57,000
Those Italians, we all do it. I saw you do it, you know, we talk with our hands. And so that

512
00:30:57,000 --> 00:31:02,680
opened up a new modality for communicating with. So now, now we can begin to solve that language

513
00:31:02,680 --> 00:31:07,000
problem, which as I said, it's not an explanation as part of that package. So imagine, I should say

514
00:31:07,000 --> 00:31:13,480
more, so the information begins to accumulate, because cultural evolution becomes a second

515
00:31:13,480 --> 00:31:17,880
line of inheritance. And that happens because of what I described there. So for any evolutionary

516
00:31:17,880 --> 00:31:23,560
system needs three ingredients, variation, like diversity, easy to solve for culture,

517
00:31:23,560 --> 00:31:26,600
people do all kinds of things for all kinds of reasons, different information, different

518
00:31:26,600 --> 00:31:32,760
personalities, whatever, transmission. So you need reliable, you don't want too much information

519
00:31:32,760 --> 00:31:37,320
to be lost during the transmission process. So for, for, for genes, of course, it's because

520
00:31:37,320 --> 00:31:42,440
genes are, are not mutating at a very high rate. And you need variation reduction. And if you

521
00:31:42,440 --> 00:31:46,600
want it to be adaptive, you need selection where more of the good stuff sticks around and less of

522
00:31:46,600 --> 00:31:51,320
the bad stuff. And if you meet those criteria, you have an evolutionary system. It's how genetic

523
00:31:51,320 --> 00:31:56,280
algorithms work, right? So genes, we know how that works. For culture, I just gave you the

524
00:31:56,280 --> 00:32:00,760
transmission. We copy without understanding. And so we don't lose too much information. And we do

525
00:32:00,760 --> 00:32:05,720
it selectively. We copy people who are more successful. We copy people who other people

526
00:32:05,720 --> 00:32:10,280
are copying. We copy people who are experts. You know, we have a, we have a psychology that

527
00:32:10,280 --> 00:32:13,800
it leads us to, for example, if a celebrity says, I'm drinking this drink, you're like,

528
00:32:13,800 --> 00:32:17,880
I want that drink. We do it instinctively. Why? It's got nothing to do with their success, right?

529
00:32:17,880 --> 00:32:22,040
But other people, you know, but we don't know. It's a black box. I don't know why, you know,

530
00:32:23,080 --> 00:32:26,360
Beyonce is such a great singer. So I'm just going to, I'm going to wear her perfume and I'm going

531
00:32:26,360 --> 00:32:30,120
to dress like her. I'm going to do everything because, you know, if I'm trying to copy the best

532
00:32:30,120 --> 00:32:35,320
hunter, maybe it's because of the shoes he wears or, you know, his weapons or where he goes hunting,

533
00:32:35,320 --> 00:32:39,800
but equally it's because of his kick-ass beard or fact that he shaves his head or the gods he

534
00:32:39,800 --> 00:32:43,640
worships. So a little hunter copies everything about them. Now, if you have that system, you've

535
00:32:43,640 --> 00:32:48,440
already deferred to the population level and information begins to accumulate. Okay. Now,

536
00:32:48,440 --> 00:32:52,760
we have evidence for early accumulation of information because one of the best pieces of

537
00:32:52,760 --> 00:32:58,120
evidence from, in my view, that this is absolutely why humans evolve is the fact that we rely on

538
00:32:58,120 --> 00:33:03,400
cooked food. Our jaws are too, too weak for chewing on like a gorilla on leaves all day.

539
00:33:03,400 --> 00:33:08,600
And our guts are too short to process that food anyway, even if we could, right? So that means

540
00:33:08,600 --> 00:33:13,480
we were cooking food. We had fire. Now, I don't know if you've ever tried to light a fire,

541
00:33:13,480 --> 00:33:19,160
but it's hard, man. It's hard, right? Even once you're taught, it's hard, right? And so like that

542
00:33:19,160 --> 00:33:24,040
meant that we needed fire to survive and we don't have genes for it. It's got to have been culturally

543
00:33:24,040 --> 00:33:29,960
transmitted. You couldn't have figured it out. So that's a very early, very early bit of information

544
00:33:29,960 --> 00:33:33,640
and maybe the stone tools too, and probably before the stone tools, the bone tools and the

545
00:33:33,640 --> 00:33:37,240
wooden tools that didn't survive the passage of time. When you say very early, what years are we

546
00:33:37,240 --> 00:33:43,560
guessing? So we're talking like millions of years. Millions of years ago. One and a half. Yeah. Yeah.

547
00:33:43,560 --> 00:33:48,600
So this is like very early on. So before like modern humans. Yeah. And we have evidence for

548
00:33:48,600 --> 00:33:52,120
this, right? Like we have little bits of evidence for, for a very, and so that meant while we were

549
00:33:52,120 --> 00:33:58,360
bipedal, we had information worth transmitting, right? And so that means you can get what's called

550
00:33:58,440 --> 00:34:04,440
a Baldwinian process in evolution. So a Baldwinian process is one where you can get to a place through

551
00:34:04,440 --> 00:34:09,160
learning. But if you can get there through genes more efficiently, those genes can be selected.

552
00:34:10,280 --> 00:34:14,600
So in this case, I can, there is a bunch of gestures I can give you so you can build the tool

553
00:34:14,600 --> 00:34:19,080
or light the fire or know that, you know, I can, I can have guttural utterances and I can wave my

554
00:34:19,080 --> 00:34:23,320
hands in a particular way. And you can learn that that means just through, you know, reinforcement

555
00:34:23,320 --> 00:34:27,960
learning, figure out what I'm talking about. But now if you have cognitive changes that allow you

556
00:34:27,960 --> 00:34:32,680
to acquire that bit of language, that you can do it more efficiently. And that means the next

557
00:34:32,680 --> 00:34:36,280
generation, you can do a little bit more and a little bit more and a little bit more. Now you

558
00:34:36,280 --> 00:34:41,800
have the ratchet to get up in the presence of an accumulating body of knowledge. You see where I'm

559
00:34:41,800 --> 00:34:47,880
going? Yeah, I do. And go ahead, keep going there. Yeah. So you have, you have, you can begin to

560
00:34:47,880 --> 00:34:52,200
accumulate this knowledge, right? And so some of the genetic changes, by the way, like made us

561
00:34:52,200 --> 00:34:56,280
more susceptible to choking. So as I say in the book, we were literally dying to speak to one

562
00:34:56,280 --> 00:35:02,440
another. And so, you know, and so we had, so this did a few things. So one, now we had all

563
00:35:02,440 --> 00:35:06,520
this knowledge, we had language begin to evolve, language continues to evolve, by the way. So for

564
00:35:06,520 --> 00:35:12,600
example, it becomes more efficient. Some of my research. And our brains grew. So they tripled

565
00:35:12,600 --> 00:35:16,680
in size compared to our last common ancestor, you know, five to seven million years ago with a chimp.

566
00:35:17,960 --> 00:35:21,160
Our brains were to the point where if you look at the medical literature today,

567
00:35:21,960 --> 00:35:25,320
once you hit about the 85th percentile, so doctors used to think it was like big babies were

568
00:35:25,320 --> 00:35:29,400
difficult to birth. Turns out it's not big babies, it's big heads. Once you hit about the 85th percentile,

569
00:35:29,400 --> 00:35:36,920
it hockey sticks up in terms of emergency cesareans and emergency, you know, forceps. So basically,

570
00:35:36,920 --> 00:35:40,760
you need these. So that means for our species, like it was kind of the curse of Eve, right? Like

571
00:35:40,760 --> 00:35:45,480
we had all this knowledge, big brain, it's painful and dangerous childbirth for mother and baby.

572
00:35:45,960 --> 00:35:54,040
Yeah. So we did other things. We began to, for example, extend our childhoods, right? So one

573
00:35:54,040 --> 00:35:58,760
change that seems to have happened for us is we had a self-domestication and we became neotonous,

574
00:35:58,760 --> 00:36:03,720
which is an easy change for evolution to make. I mean, we turned the magnificent wolf into a poodle.

575
00:36:05,000 --> 00:36:08,520
You know, so we neotonized dogs into permanent puppies. And the same thing happened for us.

576
00:36:08,520 --> 00:36:13,560
We are permanent youthful apes, able to learn for a very long time. So we had a longer child,

577
00:36:13,560 --> 00:36:17,960
a longer juvenile period, and we created an adolescence, which is the period from when you

578
00:36:17,960 --> 00:36:23,800
could reproduce to when you actually do, which has been growing and growing as the body of knowledge

579
00:36:23,800 --> 00:36:27,400
has begun to grow. It used to be that like a high school degree was enough to get a good job.

580
00:36:27,400 --> 00:36:31,000
Then it was a high school degree, plus any college degree, then a STEM degree,

581
00:36:31,000 --> 00:36:34,440
then also a master's degree and an unpaid internship and so on. And so now we're actually

582
00:36:34,440 --> 00:36:38,840
hitting a new limit. So before, the primary selection was on like being able to give birth

583
00:36:38,840 --> 00:36:42,760
to these bigger heads. Now it's like about giving birth at an older age, right? Yeah.

584
00:36:44,040 --> 00:36:49,080
All of this completely changed our societies in a variety of ways that puts the whole human

585
00:36:49,080 --> 00:36:54,200
package together. It's what created grandmothers. It's what created, you know, the difference between

586
00:36:54,200 --> 00:37:01,000
men and women, like you created dads, very unusual among great apes. It, yeah, I mean,

587
00:37:01,000 --> 00:37:05,080
the division of labor, you know, the division of information, all of this emerges. So it's not

588
00:37:05,080 --> 00:37:10,840
what, you know, the theory of everyone isn't like, so I make fun of what I call, I love this genre

589
00:37:10,840 --> 00:37:15,160
of book, by the way, the one thing that explains everything, you know, the Toti kind of books,

590
00:37:15,160 --> 00:37:19,320
you know, it's like, let me show you why my little bit of research that I just love happens to

591
00:37:19,320 --> 00:37:24,600
explain the whole damn world, right? This is a Toti adjacent book, like Toti's like an acronym

592
00:37:24,600 --> 00:37:28,360
for that, you know, it's adjacent, right? For sure. But I'm not like doing this one thing. Like I've

593
00:37:28,360 --> 00:37:32,680
got, you know, I kind of talk about the role that energy, like the big theme of the book is the role

594
00:37:32,680 --> 00:37:38,200
that energy and particularly excess energy has played in all of life and certainly in our civilization.

595
00:37:38,200 --> 00:37:41,480
And I talk about the fact that there are kind of four laws and you're going to hate this as a

596
00:37:41,480 --> 00:37:45,480
physicist, but I don't mean Newtonian laws. I mean, kind of lenses of how in which we can view the

597
00:37:45,480 --> 00:37:50,760
world, you know, the law of energy, which is fundamentally life is competing over excess

598
00:37:50,760 --> 00:37:56,840
energy, the law of innovations and efficiency. So life is evolving new ways to use that energy

599
00:37:56,840 --> 00:38:01,720
more efficiently, the law of cooperation, that the scale of cooperation to the level at which

600
00:38:01,720 --> 00:38:07,560
the individual unit, be they cells or individuals, or you know, in a society, or in a, or employees

601
00:38:07,640 --> 00:38:13,880
in a business is the level at which the per person per unit return on energy is not going to be higher

602
00:38:13,880 --> 00:38:19,400
in a larger or a smaller group. And the law of evolution, which is that both through genetic

603
00:38:19,400 --> 00:38:23,400
and cultural evolution and the interaction between these, we find the space of how to cooperate,

604
00:38:23,400 --> 00:38:28,040
how to innovate at a population level, and how to, and how to unlock new bits of energy,

605
00:38:28,040 --> 00:38:31,400
which, you know, thanks to the fossil fuel revolution with the industrial revolution,

606
00:38:31,400 --> 00:38:34,680
we now live in that world. Well, let's talk about energy a little bit, because the story

607
00:38:34,680 --> 00:38:41,240
that you just painted reminds us of this idea that history is basically a search for calories,

608
00:38:41,240 --> 00:38:47,560
right? Yeah. Food calories back in the day. And then when we invent industrial technology,

609
00:38:47,560 --> 00:38:53,480
we're just, we need even more calories in the sense of an energy source. And part of the theme

610
00:38:53,480 --> 00:38:58,840
of your book is how much of a central organizing principle that is for human history. Correct.

611
00:38:59,480 --> 00:39:04,920
Yeah, that's exactly right. So, you know, for, for us, the first, this is an ancient thing,

612
00:39:04,920 --> 00:39:08,040
you know, what you just said about calories. And, you know, I put it in terms of jewels or

613
00:39:08,040 --> 00:39:11,880
watts, like this is fundamentally what matters, you know, the first organisms,

614
00:39:11,880 --> 00:39:15,160
whenever we turn from RNA into the first self replicators or whatever.

615
00:39:17,000 --> 00:39:21,480
Sorry, the other way around, you know, the first several applicators into RNA and eventually

616
00:39:21,480 --> 00:39:28,120
organisms. The only energy we had is like the fact that we got a large moon, which was closer to

617
00:39:28,120 --> 00:39:32,200
the earth, sloshing the water back and forth over the, you know, the land and the seas. So,

618
00:39:32,200 --> 00:39:37,160
you got some gravitational energy there. But some of it's the sun falling. So, we have the earliest

619
00:39:37,160 --> 00:39:44,280
proto photosynthesis, which was, which didn't use any oxygen, right? Right. And then we had a

620
00:39:44,280 --> 00:39:49,400
mutation, which led to a more efficient photosynthesis, which, you know, polluted the world, a great

621
00:39:49,400 --> 00:39:53,880
oxygenation event, mass, mass extinction. But there was also a new opportunity. So, once we had

622
00:39:53,960 --> 00:39:58,600
photosynthesis turning solar energy into chemical energy in the form of like ATP and

623
00:39:58,600 --> 00:40:03,800
little sugar, little sugar cubes, basically, it meant that other organisms could begin to find

624
00:40:03,800 --> 00:40:08,120
a new niche of not directly converting the sun into, into organism and moving at plant pace,

625
00:40:08,120 --> 00:40:12,440
but eating other organisms and moving a little faster, right? There were like a little bundle,

626
00:40:12,440 --> 00:40:16,440
just as a story of colonialization, actually, you know, an energy rich little corner of Eurasia

627
00:40:16,440 --> 00:40:21,240
creates the largest empire the world has ever seen out competing other less energy, less,

628
00:40:21,400 --> 00:40:27,720
less cooperative societies in devastating ways. And so, you know, and so the same pattern happens

629
00:40:27,720 --> 00:40:32,040
throughout life. You got prokaryotes getting eaten by eukaryotes, eukaryotes being eaten by

630
00:40:32,840 --> 00:40:36,600
multicellular life, complex multicellular life, and eventually all the way to you,

631
00:40:36,600 --> 00:40:41,880
who is more like an ecosystem and Amazon rainforest of a microbiome and, you know,

632
00:40:41,880 --> 00:40:47,240
and interacting differentiated cells more than you are a single organism. And if you run out of

633
00:40:47,240 --> 00:40:51,400
energy, you're going to get defeated by the lower scales by the bacteria, or you're going to get

634
00:40:51,400 --> 00:40:56,120
your stuff stolen from you by the bigger scale, same pattern across the across everything. So,

635
00:40:56,120 --> 00:41:00,600
for humans, the in terms of our calories, the first unlocking of energy was the one I described

636
00:41:00,600 --> 00:41:06,520
before, which is fire, right? We it was a chemical energy technology that allowed us to predigest

637
00:41:06,520 --> 00:41:11,080
foods and make them those calories more bio available, which allowed us to grow those brains

638
00:41:11,080 --> 00:41:17,400
and shrink those guts, right? Occupy a new niche. The next major revolution was agriculture,

639
00:41:17,400 --> 00:41:22,920
which was a solar technology, right? Where now, rather than expending all that energy,

640
00:41:22,920 --> 00:41:26,680
walking around, searching, you know, hunting and gathering, like trying to find plants and small

641
00:41:26,680 --> 00:41:35,080
animals or large animals in a group, you domesticate and look after animals and you grow food more

642
00:41:35,080 --> 00:41:39,400
efficiently. It was a law of innovation, if you like, and the law of energy and you grow your

643
00:41:39,400 --> 00:41:44,600
population to the point where you out compete the hunter gather groups to the margins that are

644
00:41:44,600 --> 00:41:49,080
not suitable to agriculture, which is still where they live today, right? And those larger groups

645
00:41:49,080 --> 00:41:52,440
are able to innovate more and you lay the foundations for the beginnings of cities as

646
00:41:52,440 --> 00:41:58,600
kind of massive nodes of collective intelligence, right? This continues and we live in a Malthusian

647
00:41:58,600 --> 00:42:04,600
world, right? Because the pattern in this story across every scale is that when you get this

648
00:42:04,600 --> 00:42:08,600
unlocking, you get an era of abundance, and that would have been that first era of abundance. And

649
00:42:08,600 --> 00:42:12,360
then, because carrying capacity has gone up, the number of individuals meets that carrying

650
00:42:12,360 --> 00:42:17,560
capacity and you once again enter an era of scarcity where the per person calories or energy

651
00:42:17,560 --> 00:42:22,280
is lower. The next major unlocking that is most relevant to us was the industrial revolution,

652
00:42:22,280 --> 00:42:26,520
which if you look at any metric of progress, the size of our polities, the, you know, child

653
00:42:26,520 --> 00:42:33,000
survival rates, anything you care about, it just shoots up into the sky, right? And as in Morris

654
00:42:33,000 --> 00:42:37,400
puts it, it made a mockery of all the world's earlier history, scientific revolution, black death,

655
00:42:37,400 --> 00:42:43,800
renaissance, blips in anything we care about. And that's because we exploited stored solar energy

656
00:42:43,800 --> 00:42:49,000
in the form of like Pete turned to black rock coal and, you know, algae, you know, whatever turned

657
00:42:49,000 --> 00:42:53,880
to oil and natural gas, right? And we unlocked that in a matter of centuries, we began to burn

658
00:42:53,880 --> 00:42:58,520
that down. And we, using that excess energy, we multiplied ourselves and our efforts, we had

659
00:42:58,520 --> 00:43:04,120
another agricultural revolution, the green revolution, you know, Norman Borlag, where we

660
00:43:04,200 --> 00:43:08,520
synthesized through the Haber-Bosch process, we synthesized fertilizer with nitrogen in the air

661
00:43:08,520 --> 00:43:13,800
and, you know, natural gas. And this allowed us like four billion people, half the population

662
00:43:13,800 --> 00:43:17,160
along our life today, thanks to that process, we're literally eating our fossil fuels,

663
00:43:17,160 --> 00:43:21,560
we use them to develop new technologies. Now, here's, here's, here's where we transition to,

664
00:43:21,560 --> 00:43:25,800
you know, part two of the book in which is that the energy return on investment, which is a key

665
00:43:25,800 --> 00:43:32,440
metric I care a lot about, is decreasing. So the excess energy is shrinking and economics and a lot

666
00:43:32,440 --> 00:43:36,360
of what we do, we've focused on efficiency, because they all developed long after the industrial

667
00:43:36,360 --> 00:43:40,440
revolution, right? Or after the industrial revolution. And so if you look at one metric,

668
00:43:40,440 --> 00:43:45,960
like oil discovery, and all the metrics look like this, in 1919, one barrel of oil found

669
00:43:45,960 --> 00:43:52,200
you another 1,000 barrels. Okay. So massive return on energy. By 1950, one barrel of oil,

670
00:43:52,200 --> 00:43:56,120
a barrel of oil found you another 100. And by 2010, one barrel found you another five.

671
00:43:56,920 --> 00:44:02,120
So, so the level of cooperation is governed by this, if you like the space of the possible,

672
00:44:02,120 --> 00:44:06,680
created by an energy ceiling, and an efficiency floor, and that is shrinking. Our ability to

673
00:44:06,680 --> 00:44:10,920
innovate new efficiencies is decreasing, the rate of that is decreasing, and the ceiling is

674
00:44:10,920 --> 00:44:15,640
rapidly falling on us. And so our societies, all of the fractures are cracking, we're falling back

675
00:44:15,640 --> 00:44:21,320
to lower scales of cooperation, as a result of this. Let's actually get out of this. Let's pause

676
00:44:21,320 --> 00:44:26,840
and dwell on this, because I think this is an important thing. First, let me just sort of

677
00:44:26,840 --> 00:44:31,480
summarize this point, which I'm going to be very much in favor of as a physicist, that

678
00:44:32,040 --> 00:44:37,080
the progress, progress in the sense of just change, not necessarily improvement,

679
00:44:37,080 --> 00:44:43,240
of human society, is a story of transitions, right? It's more punctuated equilibrium than

680
00:44:43,240 --> 00:44:47,320
gradual improvement. We, you know, innovate, gradual and occasional punctuated. Yeah, sure.

681
00:44:47,320 --> 00:44:52,360
Yeah. But the, but the idea that we do innovate, we find some new mode for doing something,

682
00:44:52,360 --> 00:44:57,160
whether it's producing energy or technology or whatever. And then that gives us space, right?

683
00:44:57,160 --> 00:45:03,320
We can then grow into that. But you're saying that as we do grow into that, as we sort of reach

684
00:45:03,320 --> 00:45:10,440
what might be a new equilibrium, we also, because there's less spare excess wealth in various forms

685
00:45:10,440 --> 00:45:15,880
to go around, things get squeezed a little bit. Our social fabric gets a little bit harder to

686
00:45:16,760 --> 00:45:22,520
maintain. That's right. So I mean, our, not only does population grown, and so that alone is,

687
00:45:22,520 --> 00:45:26,200
but that's a good thing for the most part, because it means more innovation, right? Like more people

688
00:45:26,760 --> 00:45:30,760
insofar as everyone is able to access all of that cultural knowledge and, you know,

689
00:45:30,760 --> 00:45:34,600
opportunity matches talent and all of that stuff, which as we both know, isn't often.

690
00:45:34,600 --> 00:45:39,400
Not always true. Yeah. But, but, you know, the other side of it is simply that the space itself

691
00:45:39,400 --> 00:45:43,240
is shrinking, because not only is abundance turning to scarcity for the usual reasons,

692
00:45:43,240 --> 00:45:47,320
but we're reentering a Malthusian world simply because our excess energy is decreasing.

693
00:45:47,320 --> 00:45:51,000
Like it doesn't matter how fancy your gadgets and technology are, if you can't charge them.

694
00:45:51,720 --> 00:45:55,640
Like our ability to access energy isn't just like price, it's a proxy for it that's affected by a

695
00:45:55,640 --> 00:45:59,400
bunch of other things. It's literally how much energy do I need to get some amount of energy back?

696
00:46:00,120 --> 00:46:04,360
Ideal world, you want a small energy sector. It's like 5% of the economy at the moment, right?

697
00:46:04,360 --> 00:46:08,840
That's great. But as, because it means that the returns on that energy allow you to do

698
00:46:08,840 --> 00:46:13,400
everything else that makes life worth it. Like traveling on flights across the planet and, you

699
00:46:13,400 --> 00:46:17,320
know, hanging out with your friends and eating great food that requires ingredients drawn from

700
00:46:17,320 --> 00:46:22,120
all around the world. All of that stuff relies on abundant energy.

701
00:46:23,800 --> 00:46:28,760
And I guess, yeah, I think you're going to have to convince me of this one a little bit more,

702
00:46:28,760 --> 00:46:35,560
because I can imagine, you know, the same kind of general story, but then saying, well, once we

703
00:46:35,560 --> 00:46:42,440
reach some equilibrium, once we have some steady flow of energy in and things out,

704
00:46:42,440 --> 00:46:49,400
then we can figure out a way to organize our society that fits in to that equilibrium. You

705
00:46:49,400 --> 00:46:53,880
know, the moments of change in a different telling than what you just gave, the moments of change

706
00:46:53,880 --> 00:46:58,840
are the scary ones where we're not sure what's going on. But once everything is a steady flow,

707
00:46:58,840 --> 00:47:02,920
there we might reach a happy equilibrium. But you seem to be saying that's not empirically

708
00:47:02,920 --> 00:47:08,680
what happens. That's a very physicist answer, Sean. I mean, in biology, regardless of our,

709
00:47:08,680 --> 00:47:14,120
you know, our evolutionary game theory, there are no equilibrium. And the reason for that is

710
00:47:14,120 --> 00:47:18,520
because, you know, in a world, so, you know, I tell the story in the beginning of the book,

711
00:47:18,520 --> 00:47:21,720
like why I got into this, I'm trained as an engineer, you know, originally, and I kind of

712
00:47:21,720 --> 00:47:26,120
switched fields. And the reason that I did that is I got really concerned around climate change.

713
00:47:26,120 --> 00:47:29,560
And I got concerned about a lot of, I lived in a lot of different places like Africa and

714
00:47:29,560 --> 00:47:32,520
New Guinea and things like that. And I was concerned that people don't understand culture,

715
00:47:32,520 --> 00:47:37,960
first off. And second, climate change, we're all focused on mitigation. But as the world changes,

716
00:47:38,040 --> 00:47:41,240
we're also going to have to figure out how to live in a climate changed world. And some of that we

717
00:47:41,240 --> 00:47:45,560
get like carbon capture and those kinds of technologies. But how do you deal with a million

718
00:47:45,560 --> 00:47:50,120
Bangladeshis underwater coming into India? How do you like not have civilization come apart? What

719
00:47:50,120 --> 00:47:54,120
does that mean for, you know, the Middle East and then for Europe? Like the Syrian migration crisis

720
00:47:54,120 --> 00:47:58,200
is the first example of that, you know, you had climate change, probably induced droughts, you

721
00:47:58,200 --> 00:48:01,480
have a bunch of people coming into cities, there's not enough jobs to go around, not enough infrastructure

722
00:48:01,480 --> 00:48:06,520
to support them. Eventually, they start to, you know, to, and then the geopolitics kicks in,

723
00:48:06,520 --> 00:48:10,360
eventually you have a difficult problem at Europe store stuff, right? That's what I wanted to work

724
00:48:10,360 --> 00:48:15,960
on. So the thing is that in a world, like, you know, I remember watching Al Gore's documentary

725
00:48:15,960 --> 00:48:20,040
in 2007, and I was like, and I started reading the IPCC reports, I was reading the Pentagon

726
00:48:20,040 --> 00:48:24,280
reports, and I was like, Gore is absolutely right about the problem. But he's crazy. Like,

727
00:48:24,280 --> 00:48:29,240
are we going to slow the economy to save the planet? If we do, amazing, I'm happy. But I just,

728
00:48:29,240 --> 00:48:35,640
I just don't believe that because in a world where every, every country is trying to outcompete

729
00:48:35,640 --> 00:48:40,200
every other country, every company is trying to outcompete every other company, and every individual

730
00:48:40,200 --> 00:48:46,360
wants more than their neighbors, you're not going to get this equilibrium sustainable, you know,

731
00:48:46,360 --> 00:48:50,680
whatever, you're, all you're going to do is encourage a zero sum Malthusian world where your

732
00:48:50,680 --> 00:48:56,280
success means my loss because there's a limited pie from which we are taking stuff, unless you

733
00:48:56,280 --> 00:49:00,760
somehow equalize everything and people don't like that. So I mean, any theory of change or any theory

734
00:49:00,760 --> 00:49:06,040
of society that requires a complete shift in human nature and not just constraining in some way

735
00:49:06,760 --> 00:49:11,480
is broken. It's, it's broken from the beginning. It's broken before you got there. So this is

736
00:49:11,480 --> 00:49:16,760
an example. There isn't, you know, so like ideas like degrowth or, you know, like just pure sustainability

737
00:49:16,760 --> 00:49:21,000
of stagnation are dangerous because they will shove us back into a zero sum environment.

738
00:49:21,000 --> 00:49:23,560
Is that what it was like back in the hunter-gatherers?

739
00:49:24,600 --> 00:49:27,480
Yeah. So hunter-gatherers are really interesting. So we have some new work where we're trying to,

740
00:49:27,480 --> 00:49:32,120
so hunter-gatherers are, oh well, some hunter-gatherer groups are known for their egalitarianism,

741
00:49:32,120 --> 00:49:37,480
right? They do equalize, right? If you move to like pastoralists and chiefdoms, you start to see

742
00:49:37,480 --> 00:49:41,160
a bit more inequality and things like this. And so people sometimes interpret that as the ancestral

743
00:49:41,160 --> 00:49:46,680
state was, you know, was, was an egalitarian, humans are by nature egalitarian. No, that is

744
00:49:46,680 --> 00:49:52,120
probably an adaptation to those zero sum conditions where there's weak property rights. Why? Because

745
00:49:52,120 --> 00:49:57,000
in a world that is zero sum, and that means your, your gain is my loss and vice versa.

746
00:49:58,040 --> 00:50:02,280
If you like, you start a pizza business, right? And I'm like, you start a pizza business, you took

747
00:50:02,280 --> 00:50:05,720
a piece of that limited market. I'm going to like burn your business down or take from you. That's

748
00:50:05,720 --> 00:50:10,440
what it incentivizes. It incentivizes destructive competition. In a world that's positive sum,

749
00:50:10,440 --> 00:50:14,120
you start a pizza business. I'm like, yo, I got to start my own chain of pizza, you know, like,

750
00:50:14,120 --> 00:50:17,880
obviously the pizza business is booming, very different psychology, productive competition.

751
00:50:17,880 --> 00:50:21,320
So hunter-gatherers in a world of destructive competition, and there's two equilibria that

752
00:50:21,320 --> 00:50:25,960
emerge in the model. One, they just kill each other to extinction because of these destructive

753
00:50:25,960 --> 00:50:31,640
cycles, or two, they manage to flatten. But the consequence of that flattening is a decrease

754
00:50:31,640 --> 00:50:36,280
in productivity, because what is the point if you can't rise higher, right? It's a little decrease

755
00:50:36,280 --> 00:50:41,640
in production. And it is a kind of stagnation, if you like, right, that eventually gets you

756
00:50:41,640 --> 00:50:47,160
outcompeted by the pastoralists, by the agriculturalists, by, you know, the larger

757
00:50:47,160 --> 00:50:51,720
societies that are not engaged in that. And so that's the central problem with what I think the

758
00:50:51,720 --> 00:50:55,080
solution you're alluding to. Well, it's not even a solution. I'm just wondering what the

759
00:50:55,080 --> 00:51:00,360
space of possibilities is. But you're pointing out that, to put it in the harshest possible

760
00:51:00,360 --> 00:51:09,800
terms, inequality can be a driver of innovation. Correct. And evolution, like life, is competing.

761
00:51:09,800 --> 00:51:14,040
Now, you could get cycles, if you like, you know, like predator prey model type stuff, you know,

762
00:51:14,040 --> 00:51:18,360
where it's like you get a nice bird, and the rabbits go up, and you know, the hair go up,

763
00:51:18,360 --> 00:51:21,880
and then, you know, the foxes will catch up, and then you get a lion. You can get these kind of

764
00:51:21,880 --> 00:51:26,120
equilibrium urge, but it's not a, it's a Malthusian world that most animals inhabit.

765
00:51:26,120 --> 00:51:32,840
We escape the Malthusian trap simply because we got a little boost. So the question is, so my view

766
00:51:32,840 --> 00:51:37,560
is that in order to tackle these very real problems around sustainability, climate change,

767
00:51:37,560 --> 00:51:43,080
and not destroying ourselves and our planet, we need the next era of abundance, not, you know,

768
00:51:43,080 --> 00:51:48,440
an era of scarcity. Because, you know, the countries that are, first off, it's easy to

769
00:51:48,440 --> 00:51:53,320
be nice to people when there's plenty to go around, right? And second, the countries that can invest

770
00:51:53,320 --> 00:51:57,880
in looking after their environment and preserving blocks of land and wanting to live in a clean

771
00:51:57,880 --> 00:52:01,800
environment are those that are wealthy enough, energy rich enough, that they're not worried

772
00:52:01,800 --> 00:52:06,760
about simple things like food and, you know, having enough resources for hospitals and education and

773
00:52:06,760 --> 00:52:10,520
other infrastructure. Those are the countries, like, you know, as in Australia recently,

774
00:52:10,920 --> 00:52:14,920
I'm Australian citizen, actually, you know, like, so I, you know, I was, I was at the Great

775
00:52:14,920 --> 00:52:19,320
Barry Reef. I was there as a, as a teenager and I remember like, oh my God, like I was in these

776
00:52:19,320 --> 00:52:24,440
areas, I was like so bleached, right? This was in, I don't know, early 2000s or something. And I was

777
00:52:24,440 --> 00:52:28,280
back again last year and I was astonished. I was talking to people like, how did you manage to

778
00:52:28,280 --> 00:52:33,240
restore the reef? They have money, they have resources and they care about it enough because

779
00:52:33,240 --> 00:52:36,920
they have the highest household wealth in the world and the only country in the world that has,

780
00:52:36,920 --> 00:52:41,880
you know, positive migration from America, right? So they were able to invest in it,

781
00:52:41,880 --> 00:52:45,960
if they want to. You do that in a world of abundance, not scarcity.

782
00:52:46,520 --> 00:52:51,080
Well, I'm, yeah, and I don't want to disagree because I don't know enough to be able to disagree,

783
00:52:51,080 --> 00:52:58,120
but I'm curious about the possibilities. Like I said, is that, that story you just told is,

784
00:52:58,120 --> 00:53:04,440
is compatible in my mind with an equilibrium zero sum game where everybody has that level

785
00:53:04,440 --> 00:53:10,120
of wealth, right? I mean, I haven't quite seen the connection to inequality and competition.

786
00:53:10,120 --> 00:53:14,280
It's just exactly what you said. So there is no baseline, right? There's no like,

787
00:53:14,280 --> 00:53:20,120
now I'm satisfied, right? You know, like I have enough, like progress and, you know, and, and

788
00:53:20,120 --> 00:53:23,960
competition, like we want more, we want more than our neighbors and there's always problems to be

789
00:53:23,960 --> 00:53:28,920
solved. And even if you as a society, even if you as an individual decide, you know what, I have

790
00:53:29,000 --> 00:53:34,600
enough, we as a society, you're going to get outcompeted by the mutation, cultural or genetic,

791
00:53:34,600 --> 00:53:39,640
right? Where that says, actually, I want a little more. Yeah, it's not inequality. In other words,

792
00:53:39,640 --> 00:53:44,280
it's invadible by that other mutation. Okay, I'll make, I'll make one more stab at

793
00:53:44,280 --> 00:53:52,760
utopian egalitarianism and then we can move on. Even if we bought the idea that inequality is,

794
00:53:52,760 --> 00:53:57,720
is going to happen unless it's sort of, we enforce some kind of rigid, you know,

795
00:53:57,720 --> 00:54:04,280
equitability that everyone is unhappy. Can we imagine that we're in a world of such abundance now

796
00:54:04,280 --> 00:54:11,160
that we can have some equality, some competition, some motivation for innovation, and yet the worse

797
00:54:11,160 --> 00:54:14,760
off people are still much better off than they are today in the actual world.

798
00:54:15,800 --> 00:54:22,680
Yeah, I mean, I think the issue is, so I kind of surveyed the energy technologies available

799
00:54:22,680 --> 00:54:27,640
to us. And I really, apart from solar, if we solve the battery problem and hydro, which is

800
00:54:27,640 --> 00:54:32,280
fantastic, if you've got fast flowing rivers, please use them, get on your Canada, you know,

801
00:54:32,280 --> 00:54:37,000
it's really, it's really nuclear as having the right numbers apart from fossil fuels. And the

802
00:54:37,000 --> 00:54:40,840
fact that those are decreasing, we're in real trouble. And you know, there are, there are,

803
00:54:40,840 --> 00:54:44,520
you can have a society that's more redistributive and it comes at a cost or

804
00:54:45,320 --> 00:54:54,440
or look at the top 100 European companies, right? They're old, you know, very few were,

805
00:54:54,440 --> 00:54:59,080
you know, invented, you know, created like this, this century, if you like, look at the top 100

806
00:54:59,080 --> 00:55:04,040
American companies, how many of them, right? Europe redistributes a lot. And so it squashes,

807
00:55:04,040 --> 00:55:07,320
you know, it's a good, it's like, I live in Europe, right? I live in the UK, somewhere between,

808
00:55:07,320 --> 00:55:12,360
it squashes things. And so productivity is lower, production is low. And so you end up,

809
00:55:12,360 --> 00:55:15,560
you know, with America doing a lot of the innovation, and there's a little bit of

810
00:55:15,560 --> 00:55:19,400
free writing, you know, where you get to benefit from all of those drugs and all of those technologies,

811
00:55:19,400 --> 00:55:22,520
you get to use the iPhone, you get to use, you know, the internet, you get to use all

812
00:55:22,520 --> 00:55:27,400
of these different technologies. But it's being driven by this American, you know,

813
00:55:29,480 --> 00:55:33,560
this creation of inequality in America driven, like, you know, so I talk about Silicon Valley,

814
00:55:33,560 --> 00:55:37,720
right? Silicon Valley is a, people think of it as like a bastion of success. Now, man,

815
00:55:37,720 --> 00:55:43,960
it's a grave, you're at a failure, right? And that's what America, you know, America says,

816
00:55:43,960 --> 00:55:48,280
you do you, everybody go try, you can do this too, right? It's terrible at an individual level.

817
00:55:48,280 --> 00:55:52,680
The Asian model, like, you know, Asian tiger mom, you know, Japan, whatever is,

818
00:55:52,680 --> 00:55:56,040
you know, you don't do you, you do me, like you do this, you follow the script,

819
00:55:56,040 --> 00:55:59,640
and everyone is better off, but it leads to incremental innovation, like you have to believe

820
00:55:59,640 --> 00:56:04,680
in a crowded market where everybody also thinks this, and most people still fail,

821
00:56:04,680 --> 00:56:09,720
that you are going to make it. And a society that encourages that in a large enough marketplace with

822
00:56:09,720 --> 00:56:15,720
enough energy results in finding the few apples, what we call unicorns for a reason, right? The

823
00:56:15,720 --> 00:56:21,240
apples, the Amazons, the alphabets, the whatever's, right? And the graveyard of failure is forgotten

824
00:56:21,240 --> 00:56:28,520
because America is richer as a result of this. And you draw, good, I'm going to give up on

825
00:56:28,520 --> 00:56:32,760
the utopia there, you know, the egalitarian, I'm actually not highly committed to it.

826
00:56:32,840 --> 00:56:39,240
I would enjoy this. I'm not highly committed to it, but I want to learn. But you then take this

827
00:56:39,240 --> 00:56:43,720
picture, I know we've given it very short shrift because it's a deep book with a lot of things

828
00:56:43,720 --> 00:56:48,360
going on, but you take this picture of human nature and the development of history through

829
00:56:48,360 --> 00:56:54,280
these transitions and getting the energy, and you apply it to current issues that we have.

830
00:56:54,280 --> 00:56:59,320
I mean, climate change is one of them, of course, but also political polarization. I mean, what about,

831
00:56:59,320 --> 00:57:05,240
you know, the growth of populism and authoritarianism and so forth? How do we understand that through

832
00:57:05,240 --> 00:57:09,320
this lens? So, I mean, the first thing is, you know, like, ideally you want to predict,

833
00:57:09,320 --> 00:57:11,560
you want to explain, but then you want to be able to do something about it.

834
00:57:11,560 --> 00:57:16,840
I mean, so I see the rise of, there are always fractures that exist in a society, right? There

835
00:57:16,840 --> 00:57:20,840
are always fractures. They are the class divisions, ethnic divisions, you know,

836
00:57:23,400 --> 00:57:26,680
regional divisions, there's all these fractures that exist. But in a world where there's plenty

837
00:57:26,680 --> 00:57:31,240
to go around, people just mumble and grumble. So, you know, this analogy, imagine the buses

838
00:57:31,240 --> 00:57:35,880
coming along, right? That's the rate of economic growth. And people are waiting for the buses,

839
00:57:35,880 --> 00:57:39,880
and you know, you've got some people who have special passes, the 1%, get to the front of the

840
00:57:39,880 --> 00:57:44,120
line first. And you've got groups that favor their in-group, and they're like, come on, you can come

841
00:57:44,120 --> 00:57:47,560
in front, and you're waiting there. But if the buses are coming every five minutes, you mumble,

842
00:57:47,560 --> 00:57:52,040
you grumble, but you put up with it because you're going to get a seat. Just wait five minutes more.

843
00:57:52,040 --> 00:57:56,440
But if the rate of buses slows down, what an hour, what a day,

844
00:57:58,120 --> 00:58:01,800
it's like driving around at car park where there's plenty of spaces versus only one. And now some

845
00:58:01,800 --> 00:58:07,480
asshole, so can I say that in your podcast? Someone tries to take your place, right?

846
00:58:09,880 --> 00:58:15,320
It's a very different psychology. So, and it doesn't even have to be like a reality,

847
00:58:15,320 --> 00:58:19,560
like the perception that that is happening, the created perception can create the reality,

848
00:58:20,200 --> 00:58:24,040
right? Or if you're looking at, if you're, if you're, if we have a psychology that's sensitive

849
00:58:24,040 --> 00:58:28,520
to rate of change, right? And you feel like things are slowing down a little bit, right?

850
00:58:28,520 --> 00:58:33,160
That alone is enough to trigger this, those fractures becoming something more. So then you

851
00:58:33,160 --> 00:58:37,560
can have the rise of the right wing, you can, you end up with more ethnic fractures, you get

852
00:58:37,560 --> 00:58:43,080
political polarization, you get all those fractures becoming something more. And the solution to that

853
00:58:43,080 --> 00:58:47,720
is one, if you've got the resources, invest, right? You can have a million people turn up

854
00:58:47,720 --> 00:58:52,520
at your door if you don't have enough food and water and prepare for those guests, right?

855
00:58:52,520 --> 00:58:56,440
So you need to invest in infrastructure so that already stretched schools, you know,

856
00:58:56,440 --> 00:58:59,960
the good schools, the hospital lines, you know, people are not waiting and waiting and now they're

857
00:58:59,960 --> 00:59:05,320
annoyed because you're giving things to newcomers. Like that's a recipe for ethnic conflict. But if

858
00:59:05,320 --> 00:59:08,600
you're investing and you're like, you know, alongside this, this new group of people, we're

859
00:59:08,600 --> 00:59:11,800
going to put in all this money so that there's enough space for everyone. That's a good thing.

860
00:59:11,800 --> 00:59:15,960
And if you don't have that, you need to be investing in energy technologies that allow

861
00:59:15,960 --> 00:59:20,600
us to raise that ceiling. So talking about the umbrella model of multiculturalism is really

862
00:59:20,600 --> 00:59:25,880
fusing all of these solutions into something that would work in a way that current melting pots or

863
00:59:25,880 --> 00:59:31,640
mosaics or, you know, no hyphen French models really just don't.

864
00:59:31,640 --> 00:59:35,400
So just to try to rephrase it, because I do think this resonates with something that I

865
00:59:35,400 --> 00:59:42,200
had believed for a while, they're one of the things, the major factor that what is the acronym

866
00:59:42,200 --> 00:59:46,360
you use that, you know, the one thing that explains everything when it comes to,

867
00:59:47,880 --> 00:59:52,840
you know, populism, et cetera, is the idea that people who are not well off,

868
00:59:52,840 --> 00:59:57,480
which you might think are going to go for like leftists who are who want to spread the wealth,

869
00:59:57,480 --> 01:00:03,000
but they really are feeling powerless and both in the sense that their political voices aren't

870
01:00:03,000 --> 01:00:09,240
heard and that their lives are getting worse, right? And then they that, that combination

871
01:00:09,240 --> 01:00:12,600
pushes them toward appealing to a strong figure who can shake things up.

872
01:00:13,240 --> 01:00:17,400
Yeah. Yeah. Look, you know, as I pointed in the book, I don't think there's good evidence that

873
01:00:17,400 --> 01:00:22,520
people don't like inequality. What people don't like is a double negative. So you're saying that

874
01:00:22,520 --> 01:00:28,440
they're people like it like equality. No, I'm not saying that people like equality or inequality

875
01:00:28,440 --> 01:00:33,000
or whatever. What the people want is fairness. Yeah. Okay. Right. Like people want the outcomes

876
01:00:33,000 --> 01:00:37,800
to be matched to the input, you know, like they want a return like they don't want necessarily

877
01:00:37,800 --> 01:00:43,560
the hand down. Maybe some people do, right? But like people want the opportunity so that what

878
01:00:43,560 --> 01:00:48,360
they do that they're hard working, if they choose to wake up early and get to the front of the line

879
01:00:48,360 --> 01:00:52,840
at that bus stop, they will get at their seat. And when that stops happening, no matter how

880
01:00:52,840 --> 01:00:57,160
early they wake up or because of where they're born or who they're born to, they just never get

881
01:00:57,160 --> 01:01:02,760
that seat. That is the problem. That's when society starts to come apart. And that's what we need

882
01:01:02,760 --> 01:01:07,640
to resolve. So, you know, in the section on inequality, I differentiate between, you know,

883
01:01:07,880 --> 01:01:14,200
wealth creation versus wealth appropriation. Wealth appropriation is rent seeking. It's

884
01:01:14,200 --> 01:01:18,840
monopolistic behavior. It's like land, you know, like holding on to land for centuries,

885
01:01:18,840 --> 01:01:22,760
as in the country I live in, you know, where you're just, you're just, you're just taking,

886
01:01:22,760 --> 01:01:27,160
you know, a rent on the land, providing no additional value, right? This is why we need like

887
01:01:27,160 --> 01:01:30,840
land value taxes are kind of a way out of this. It's like a huge thing that I talk about in the

888
01:01:30,840 --> 01:01:36,920
book. Whereas wealth creation is like you created Apple or you created, you know, Windows or you

889
01:01:36,920 --> 01:01:41,400
created Amazon. Yeah, you put like a bunch of high streets and malls out of work, but you made

890
01:01:41,400 --> 01:01:45,960
things more efficient that consumers like. And you actually, you created wealth in some sense,

891
01:01:45,960 --> 01:01:49,240
either through the efficiency, the law of efficiency, sorry, the law of innovations and

892
01:01:49,240 --> 01:01:53,480
efficiency, or because you've found some new energy technology or something, right? You've

893
01:01:53,480 --> 01:01:57,640
literally created a new, you've increased the space as possible, and you are allowed to keep a part

894
01:01:57,640 --> 01:02:03,240
of that space. And you are allowed to make other bets that we hope will continue to expand the space.

895
01:02:03,240 --> 01:02:08,200
But the issue is that under an intergenerational transmission of wealth that entrenched that

896
01:02:08,200 --> 01:02:14,120
inequality slowly over time, having the remaining people fight over smaller and smaller scraps.

897
01:02:14,120 --> 01:02:16,840
That is when society comes apart. And that's what we need to solve.

898
01:02:16,840 --> 01:02:22,040
So you'd be in favor of larger estate taxes. Would be great. As I pointed out in the book,

899
01:02:22,040 --> 01:02:27,080
that would be great. But it runs up against a few issues. One is a smart estate planning wealth

900
01:02:27,080 --> 01:02:32,280
flight where people take their money out of the country. And it also goes against, you know,

901
01:02:32,280 --> 01:02:36,040
it goes against kind of instincts about like provisioning for your children and working

902
01:02:36,040 --> 01:02:40,600
harder later in life. It would be great. Here's the outcome you want. You want to create a world

903
01:02:41,240 --> 01:02:46,200
that is, every generation has a fairly fair playing field to start with, so that the best

904
01:02:46,200 --> 01:02:49,880
and brightest of every generation can push forward our species to where we need to go.

905
01:02:50,440 --> 01:02:54,440
And over time, things accrue that prevent that happening. So what I actually, so there's a,

906
01:02:54,440 --> 01:02:58,920
you know, there's a, there's an open secret within economics where across the political

907
01:02:58,920 --> 01:03:04,280
spectrum, you know, people agree that most taxes are distortionary. You know, they affect your

908
01:03:04,280 --> 01:03:07,720
behavior in negative ways. Like your income tax, right? Like if you are afraid, you're going to

909
01:03:07,720 --> 01:03:11,400
earn a little bit more and move into the next income bracket, you're like, that marginal gain

910
01:03:11,400 --> 01:03:17,640
is not worth it for me. So now it's distortionary on your productivity or production. You know,

911
01:03:17,640 --> 01:03:21,640
sales tax, like I'm not going to buy that because of the sales tax. It's distortionary on commerce.

912
01:03:22,440 --> 01:03:28,040
There's one tax that is non distortionary. And there's a moral case for it. And it is fair.

913
01:03:28,120 --> 01:03:34,360
And it would do away with the need for the other taxes. You could get rid of income tax. You can

914
01:03:34,360 --> 01:03:38,440
get rid of sales tax and get rid of capital gains tax. And you could still pay for the

915
01:03:38,440 --> 01:03:43,640
US military and Medicare and, you know, this country, many more. So that is land value taxes.

916
01:03:43,640 --> 01:03:49,320
So the one asset that people own that they did not create, the one asset that stands above all

917
01:03:49,320 --> 01:03:53,400
others is the land. And I don't mean like what you do on the land. You shouldn't be taxed for

918
01:03:53,400 --> 01:03:59,240
your buildings or anything you do on the land. But the land itself is common. And there's a moral

919
01:03:59,240 --> 01:04:04,280
case almost like slavery, where we really need to get rid of this ability to own land. Or at the

920
01:04:04,280 --> 01:04:10,520
very least, those who take that land should be taxed for, you know, for that use of that land,

921
01:04:10,520 --> 01:04:14,760
for its most, and this does all kinds of things. So it's non distortionary. It leads to the most

922
01:04:14,760 --> 01:04:20,600
productive use of that land. It solves a lot of housing issues. And the only barrier to it that

923
01:04:20,600 --> 01:04:24,360
people agree is is going to require revolution. Unless I guess the middle class realized that

924
01:04:24,360 --> 01:04:27,880
because we all, you know, I own land, you own land, you know, like we own land. But what you

925
01:04:27,880 --> 01:04:33,880
don't realize is how little you own relative to, you know, many others. 25,000 Britons who own

926
01:04:33,880 --> 01:04:38,040
Brits who own half of the country or, you know, the 10% of Americans who own I think 60% of the

927
01:04:38,040 --> 01:04:44,680
country. But just to just to point out, this is very radical as a proposal. And there's probably,

928
01:04:44,680 --> 01:04:50,520
you know, unanticipated consequences here. The difference in utopia and, you know,

929
01:04:50,520 --> 01:04:54,760
in a better world are constraints. And what you want, you know, there are real constraints on

930
01:04:54,760 --> 01:04:58,680
this. And so what I suggest, you know, I suggest pathways out of this. So the path dependence

931
01:04:58,680 --> 01:05:02,280
leads us into a world, like it's hard to rewrite the Constitution, it's hard to like change the

932
01:05:02,280 --> 01:05:07,880
tax code overnight. But what you can do is do what, for example, so, you know, last week I was in

933
01:05:07,880 --> 01:05:11,640
Estonia, because I want to understand how they are the top of the pizza tables in the Western

934
01:05:11,640 --> 01:05:17,640
world, beaten only by a handful of other handful of East Asian countries. And they do it while

935
01:05:17,640 --> 01:05:23,320
spending less per student than the rest of the OECD average and certainly less than the US,

936
01:05:23,320 --> 01:05:27,400
Canada, UK, Australia, you know, then, and they have better outcomes. How did they do that?

937
01:05:27,400 --> 01:05:29,560
Sorry, explain what these tables are that you're talking about?

938
01:05:29,560 --> 01:05:34,520
Oh, sorry. Yeah, sorry. The OECD's pizza tables test students around the world on mathematics,

939
01:05:34,520 --> 01:05:40,360
on reading, and on science. Good. Okay. And Estonia tops that table. Their students top the

940
01:05:40,360 --> 01:05:45,720
table. Right. A part, as I said, from a couple of a few Asian countries, how did they do that?

941
01:05:45,720 --> 01:05:48,840
So I went there to understand, you know, I spoke to the former education minister, I spoke to the

942
01:05:48,840 --> 01:05:54,040
person who founded what was called the Tiger Leap Foundation. So after the fall of the, well,

943
01:05:54,040 --> 01:05:59,720
after the Soviet occupation ended in 1991, only half the country had a telephone. So, you know,

944
01:05:59,720 --> 01:06:03,000
when Estonia became its own country, they were like, okay, if we want to leap ahead,

945
01:06:03,000 --> 01:06:05,480
we're going to have to do something different. We're not like super wealthy. We don't have like

946
01:06:05,480 --> 01:06:09,080
a bunch of resources. We just have our people, right? That's it. We got to invest in our people.

947
01:06:09,080 --> 01:06:13,560
And so, you know, you can tell, you can see how, why the people who did this thought of it because

948
01:06:13,560 --> 01:06:17,320
of their own experiences, but they decided to really invest in technology. So they created what

949
01:06:17,320 --> 01:06:22,120
was called the Tiger Leap Foundation, where they overnight gave it the access to computers,

950
01:06:23,000 --> 01:06:25,880
you know, for every student, they internet connected everybody, they train the teachers

951
01:06:25,880 --> 01:06:30,680
and so on, right? Then they were able to do this, because they have radical decentralization.

952
01:06:30,680 --> 01:06:34,760
Every school has autonomy over what they do. Every municipality has autonomy. And

953
01:06:34,760 --> 01:06:39,240
central government can't just say, let's do this. They have to incentivize. So teachers were

954
01:06:39,240 --> 01:06:43,000
incentivized to share knowledge through this teacher social network called School Life.

955
01:06:43,000 --> 01:06:47,560
They gave them opportunities and incentivized by sharing that knowledge. They increased their

956
01:06:47,560 --> 01:06:51,160
pay. They gave them opportunities to travel to other places to borrow the best from things from

957
01:06:51,160 --> 01:06:56,440
around the world, creating a brand new curriculum. And they tested them onto things at a local level.

958
01:06:56,440 --> 01:07:01,720
And when it worked, it got spread to the rest of the country. Many, you know, so, you know,

959
01:07:01,720 --> 01:07:04,680
in this case, for example, robotics is taught, like they were the first country to teach

960
01:07:04,680 --> 01:07:07,960
reading, writing, arithmetic, and now algorithms to like elementary school students.

961
01:07:08,680 --> 01:07:14,520
They, they trialed and, you know, Conrad Wolframs, I don't know if you come across Conrad

962
01:07:14,520 --> 01:07:20,040
Wolframs criticism of math education, where he says, you know, math education recapitulates

963
01:07:20,040 --> 01:07:24,200
history for some reason, like, we, you know, we're like, we're going to learn numbers. Now,

964
01:07:24,200 --> 01:07:28,920
the Greeks and Pythagoras for some reason, and, you know, eventually get to algebra and

965
01:07:28,920 --> 01:07:34,680
eventually calculus, right? Many students never get to calculus, and we don't learn any 20th century

966
01:07:34,680 --> 01:07:39,800
math in high school, let alone 21st century. Like we're missing huge swaths. Why is that really

967
01:07:39,800 --> 01:07:44,680
the most efficient way to do it? So Conrad, you know, obviously wanting to sell some Wolfram

968
01:07:44,680 --> 01:07:49,960
products, but, you know, he, he argues, well, in the real world, you and I do math on computers,

969
01:07:49,960 --> 01:07:53,880
right? Like we're not back in the envelope calculations. That's not where the hard stuff

970
01:07:53,880 --> 01:08:01,000
gets done. Right. And so what you want is for kids to understand what it means to take a derivative,

971
01:08:01,000 --> 01:08:04,440
what it means to what an integral actually is and how it can be used and maybe some stats.

972
01:08:04,440 --> 01:08:08,200
And, you know, like, you want them to have mental models and work alongside computers,

973
01:08:08,200 --> 01:08:13,160
and you can teach that before you, what's hard about calculus is not the derivatives and integrals,

974
01:08:13,160 --> 01:08:16,840
it's the chain rule and it's the mechanics, the quadratic, you know, like it's those that are

975
01:08:16,840 --> 01:08:20,600
difficult. I agree with you there. That's not what's important. That was built for a world before,

976
01:08:20,600 --> 01:08:23,400
you know, one middle school teacher was like, make sure you learn how to do mental math,

977
01:08:23,400 --> 01:08:27,160
you're not going to carry a calculator in your pocket. He didn't force you to do that, right?

978
01:08:27,160 --> 01:08:31,480
So we, so they were able to trial that. And now they're trialing this really radical new

979
01:08:31,480 --> 01:08:38,440
education system, where they, the learning happens at home, right? Where homework and

980
01:08:38,440 --> 01:08:42,920
schoolwork are swapped, basically, right? How can a teacher teach to 20 to 30 kids of different

981
01:08:42,920 --> 01:08:48,440
abilities? They can't. So the lower end get left behind, the upper end are ignored, right? And so

982
01:08:48,440 --> 01:08:52,680
in order to do that, what happens is you when you're at home, you get educational material,

983
01:08:52,680 --> 01:08:56,920
you get teachers and whatever, you get this from the best minds. And then you come to school to

984
01:08:56,920 --> 01:09:00,280
practice that you do the homework where the teacher is not the deliverer of knowledge,

985
01:09:00,280 --> 01:09:04,280
that the facilitator helping you use the internet and AI to practice your skills

986
01:09:04,280 --> 01:09:08,680
in a very kind of practical real world focus. And you do that through radical decentralization.

987
01:09:08,680 --> 01:09:12,840
So the US works the same way, by the way, you know, Justice Brandeis referred to each state as a

988
01:09:12,840 --> 01:09:17,960
laboratory for democracy, right? Arizona, you want to try something? Go try it. If you fail,

989
01:09:17,960 --> 01:09:21,640
you fail it at an Arizona level. And if it works, we bubble it up to the top.

990
01:09:22,200 --> 01:09:27,240
Such a Nadella converted Microsoft from a monolith to a series of startups using the same

991
01:09:27,240 --> 01:09:30,920
principle, you know, it's like, you're going to try different things and we're going to bubble

992
01:09:30,920 --> 01:09:35,240
it up to Microsoft and it works. And that they invested in chat GPT and open AI, right? Great

993
01:09:35,240 --> 01:09:39,480
moves. And the shares have been, have been soaring. The Catholic Church works this way, by the way.

994
01:09:39,480 --> 01:09:43,160
You know, it's like lasted for so long because they're like, well, we got a central authority,

995
01:09:43,160 --> 01:09:47,320
but you know, Jesuits, you go do your weird thing and we're going to laugh at you for most of it.

996
01:09:47,320 --> 01:09:51,400
Oh, now we have a Jesuit Pope because that worked out, right? You have to have space to

997
01:09:51,400 --> 01:09:55,800
kind of explore these possibilities to get out of that trap. So what do I why am I saying,

998
01:09:55,800 --> 01:09:59,400
talking about all this because land value taxes are something you can do in what I call startup

999
01:09:59,400 --> 01:10:05,720
cities, right? Like cities are the place where we all live and countries are increasingly untenable

1000
01:10:05,720 --> 01:10:09,560
in some ways. Like how do you get that? But if you empower cities, give them more autonomy,

1001
01:10:09,560 --> 01:10:14,600
allow them to learn from one another, have a more startup ecosystem around that. This is a way out,

1002
01:10:14,600 --> 01:10:18,360
right? This is the engine of development that Hong Kong represented for China. This is why

1003
01:10:18,360 --> 01:10:23,240
Singapore is at the top of the piece of tables, you know, it's, it's how Guangzhou works. And now

1004
01:10:23,240 --> 01:10:27,000
there's a bunch of startup cities or charter cities sometimes, you know, in, in Africa.

1005
01:10:27,000 --> 01:10:31,720
But this is a way past current democracy and eventually maybe programmable politics. But

1006
01:10:31,720 --> 01:10:35,320
I don't have time to explain that it's in the book. It's in the book. I think there's a lot in

1007
01:10:35,320 --> 01:10:40,040
the book and you know, we've covered a lot of it. But I do want to encourage people to check out the

1008
01:10:40,040 --> 01:10:44,920
book because it paints a much folder picture. But maybe we can sort of wind things up by

1009
01:10:45,560 --> 01:10:51,560
returning to something that we talked about right at the beginning, the, the children learning from

1010
01:10:51,560 --> 01:10:57,080
the adults rather than using their reasoning, right? That we rely on our cultural transmission

1011
01:10:57,080 --> 01:11:01,560
of knowledge. I want to feed that back into what we were just talking about in terms of

1012
01:11:02,120 --> 01:11:09,320
inequality and fairness and innovation and things like that. How much of a problem is it

1013
01:11:09,320 --> 01:11:15,800
that in the modern era, we're able to get that cultural knowledge in a, in a quite fine-tuned

1014
01:11:15,800 --> 01:11:22,360
or at least quite differentiated way, even within one culture, right? Like if I, in your

1015
01:11:22,360 --> 01:11:29,800
bus analogy, if I am constantly told that the buses come very rarely and discriminate against me,

1016
01:11:29,800 --> 01:11:33,480
even if it's not true, that becomes a problem for how society works.

1017
01:11:34,600 --> 01:11:37,960
Yeah. You know, like, you know, people talk about this as like the Walter Cronkite problem,

1018
01:11:37,960 --> 01:11:40,520
you know, like once upon a time we all got our news, well, not me, I'm too young.

1019
01:11:41,240 --> 01:11:47,160
Walter Cronkite, you know, was on the evening news and now like we all read different, you know,

1020
01:11:47,160 --> 01:11:51,080
we read Breitbart and, you know, and Huffington Post and New York Times, we're all getting it from

1021
01:11:51,080 --> 01:11:56,600
like different places. So I think the internet is interesting in that, so I argue that the internet

1022
01:11:56,600 --> 01:12:00,760
is actually creating, we are not in the midst of a fourth industrial revolution or fifth industrial

1023
01:12:00,760 --> 01:12:06,200
revolution. We are in the midst of a second enlightenment. Okay. One driven not by coffee

1024
01:12:06,200 --> 01:12:12,200
shops and pamphlets and books, but by tweets and, you know, and shared articles on social media

1025
01:12:12,200 --> 01:12:16,600
that piss you off. And they piss you off because they expose you to ideas that are not in your

1026
01:12:16,600 --> 01:12:23,240
in group. Now, alongside that is a countervailing force, which is that we are also able to assort

1027
01:12:23,240 --> 01:12:28,040
in smaller and smaller groups, and those groups can compete with one another. In other words,

1028
01:12:28,040 --> 01:12:34,920
the internet creates new tribes, right? So, you know, there's a show here, Little Britain,

1029
01:12:34,920 --> 01:12:39,160
and there's a character, you know, he's like, I'm the only gay in the village, you know, it's

1030
01:12:39,160 --> 01:12:44,600
not true. He actually, there's lots of other gays in the village. But once upon a time, you might

1031
01:12:44,600 --> 01:12:49,560
have been like, if you had, if you were a small minority in a small village, you were, you did

1032
01:12:49,560 --> 01:12:54,040
not have a critical mass to do anything. Right. But the internet enables you to find. So, you know,

1033
01:12:54,040 --> 01:12:59,240
I point to a couple of examples. So are you, are you into carrying sand in your pocket,

1034
01:12:59,320 --> 01:13:04,360
Sean? I am not. I've never have done it. Well, you know, like 30,000 people on like our, you know,

1035
01:13:04,360 --> 01:13:08,600
Reddit subreddit on pocket sand, talk about all the benefits of carrying sand in their pockets.

1036
01:13:08,600 --> 01:13:13,080
But that's actually kind of a niche interest on like stapling bread to trees. Like listeners,

1037
01:13:13,080 --> 01:13:18,760
go and Google bread staple to a tree. It is a hobby held by around 300,000 people. Right. So,

1038
01:13:18,760 --> 01:13:22,680
yeah, you get these different groups, but it's very hard. It's very hard to generalize whether

1039
01:13:22,680 --> 01:13:28,360
that is silo. Like it's an empirical question, whether we are now in silos or actually exposed

1040
01:13:28,360 --> 01:13:33,080
to more information and both are true. I think what is actually the case is it's not about the

1041
01:13:33,080 --> 01:13:37,480
information. It was never about the information. It was never about misinformation. It was about

1042
01:13:37,480 --> 01:13:43,880
whom we trust. And what has happened is that our trust in our institutions and to some degree,

1043
01:13:43,880 --> 01:13:49,880
as a result of that, in each other has decreased. And that is resulting in selective access to

1044
01:13:49,880 --> 01:13:54,440
that information and greater assortment. And the ultimate issue here, the ultimate level problem

1045
01:13:54,440 --> 01:13:57,640
needs to be solved. And that is it's an energy problem, actually.

1046
01:13:58,920 --> 01:14:04,760
Sorry, you made a leap there that I need to figure out. I mean, I agree that the trust in

1047
01:14:04,760 --> 01:14:12,520
institutions has waned. It seems to me like in some ways that's an outcome of the same game

1048
01:14:12,520 --> 01:14:20,200
theoretic analysis of different groups jockeying for space and things like that. And of course,

1049
01:14:20,200 --> 01:14:25,720
with the caveat that some institutions are super untrustworthy. So it's correct to not

1050
01:14:25,720 --> 01:14:27,800
trust them. But so how do you bring it back down to energy?

1051
01:14:28,760 --> 01:14:32,280
So sorry. So, you know, why I say that it's, you know, it's about the information is like,

1052
01:14:33,160 --> 01:14:38,520
there are always people always calling for the end of the world or saying that, you know, we live in

1053
01:14:38,520 --> 01:14:42,040
a racist society or that, you know, we live in a society where you are getting screwed over. There's

1054
01:14:42,040 --> 01:14:47,480
always people. There's always a bunch of people saying everything. But whom people listen to

1055
01:14:48,200 --> 01:14:53,240
depends on on circumstances. Right. And so what I'm arguing is there is a both a perception

1056
01:14:53,240 --> 01:14:58,440
and a reality. And yes, they can like feedback on each other. And the reality is that out truly

1057
01:14:58,440 --> 01:15:05,400
our space of the possible has indeed trunk. The amount of access energy available to each of us

1058
01:15:05,400 --> 01:15:11,080
has actually shrunk. And, you know, around about the 19, you know, the oil crisis, 1980s, 1970s,

1059
01:15:11,080 --> 01:15:15,960
you know, like, it was around, you know, what the fuck happened in 1971 1973. This was when it all

1060
01:15:15,960 --> 01:15:20,040
happened. The American dream died around then, because you suddenly had a 5050 probability of

1061
01:15:20,040 --> 01:15:25,960
being better off than your parents. That has truly happened. Right. And so that is fundamentally

1062
01:15:25,960 --> 01:15:31,320
an issue of excess energy, which ultimately drives GDP growth. So like 50% roughly of GDP

1063
01:15:31,320 --> 01:15:36,280
growth is energy or 50% is about it, you know, innovations and efficiency. Okay, I do get it

1064
01:15:36,280 --> 01:15:41,640
now. So it comes back to this shrinking distance between the ceiling and the floor basically

1065
01:15:41,640 --> 01:15:47,000
in our society. The amount that we have to all live in together, the scale of cooperation

1066
01:15:47,560 --> 01:15:53,240
that is that is incentivized. And I will let you close the these sets of thoughts with your

1067
01:15:53,240 --> 01:15:56,520
optimistic spin on how we're eventually going to work our way out of this.

1068
01:15:57,240 --> 01:16:00,440
Well, you know, so as I said, you know, part one of the book lays it all out lays out the

1069
01:16:00,440 --> 01:16:05,960
science lays out the problem. But then part two is is exactly about, you know, how do we

1070
01:16:06,280 --> 01:16:11,640
do the chapter titles like how do we reunite humanity? How do we create a new forms of

1071
01:16:11,640 --> 01:16:17,000
governance for the 21st century? How do we shatter that glass ceiling of inequality?

1072
01:16:17,000 --> 01:16:21,800
How do we trigger a creative explosion that maybe gets us diffusion and becoming the first

1073
01:16:21,800 --> 01:16:26,440
generation of a galactic civilization? How do we improve the internet? How do we as individuals

1074
01:16:26,440 --> 01:16:32,360
become brighter? And because like imagine you are the first generation to have the periodic table,

1075
01:16:32,360 --> 01:16:36,680
right? We can stop doing alchemy, we start doing some chemistry. And there are actual

1076
01:16:37,720 --> 01:16:42,760
solutions, given the constraints that we face, real life solutions that are possible

1077
01:16:42,760 --> 01:16:49,000
for tackling each of these problems. And that means that a brighter future is possible if we so

1078
01:16:49,000 --> 01:16:53,720
choose. I cannot close on any better sentence than that. So Michael Muthukrishna, thanks

1079
01:16:53,720 --> 01:17:05,080
very much for being on the Mindscape podcast. Thank you so much for having me, Sean. I loved it.

