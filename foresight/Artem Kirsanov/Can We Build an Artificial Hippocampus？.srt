1
00:00:00,000 --> 00:00:06,320
It is humanity's long-time dream to create machines that can think, but what exactly does it mean?

2
00:00:06,320 --> 00:00:11,120
One particular characteristic of intelligence is the ability to generalize knowledge

3
00:00:11,120 --> 00:00:17,680
and flexibly adapt it to new situations. Such generalization is indeed one of the cornerstone

4
00:00:17,680 --> 00:00:22,480
problems in modern machine learning. In this video, we are going to see how we can take

5
00:00:22,480 --> 00:00:28,720
biological organization over hippocampus, a brain structure involved in memory and navigation,

6
00:00:28,720 --> 00:00:34,000
as an inspiration in order to construct a computational model that can learn to build

7
00:00:34,000 --> 00:00:38,880
abstractions and generalizations, as well as the insights we can draw from this model,

8
00:00:38,880 --> 00:00:43,360
both about our own brains and the field of artificial intelligence.

9
00:00:43,360 --> 00:00:48,320
Before we begin, I'd like to warn you that this video is the continuation of the previous video

10
00:00:48,320 --> 00:00:54,080
in the series on cognitive maps. Last time we explored neurobiological background of hippocampal

11
00:00:54,080 --> 00:00:59,360
computations and introduced some general principles, so if you haven't seen it, I highly

12
00:00:59,360 --> 00:01:04,080
recommend you check it out before watching this one, since we are going to build up from there.

13
00:01:04,640 --> 00:01:06,320
If you're interested, stay tuned.

14
00:01:13,200 --> 00:01:19,200
Imagine you're an agent that walks around the world and his only goal is to find rewards.

15
00:01:19,200 --> 00:01:24,160
From an evolutionary perspective, you can think about such an agent as an early organism,

16
00:01:24,160 --> 00:01:30,080
which needs to look for food or mates. Now, as an agent, you have a certain repertoire

17
00:01:30,080 --> 00:01:35,360
of actions that you can take. For example, activate a sequence of muscles to move in a particular

18
00:01:35,360 --> 00:01:42,400
direction. To choose the most rewarding actions, you need to be able to predict the action outcomes,

19
00:01:42,400 --> 00:01:47,200
and that effectively requires a mental model of the surrounding environment.

20
00:01:47,200 --> 00:01:52,080
Existence of such a model allows you to run mental singulations in your head

21
00:01:52,080 --> 00:01:57,680
to weigh the actions. For example, what would happen if I go straight or is it better to turn

22
00:01:57,680 --> 00:02:03,360
right? Over the course of your lifetime, as you encounter a variety of different environments,

23
00:02:03,360 --> 00:02:09,280
initially you might build an entangled, indivisible model for each, without necessarily linking

24
00:02:09,280 --> 00:02:14,560
different models to each other. However, if you're being optimal in your representations,

25
00:02:14,560 --> 00:02:20,000
at some point you realize, wait a minute, all these models I've built so far actually have

26
00:02:20,000 --> 00:02:26,720
an awful lot in common. Indeed, walls that block your way, doors that allow you to go through the

27
00:02:26,720 --> 00:02:33,680
walls and even just the structure of an open 2D space itself all work similarly in every environment,

28
00:02:33,680 --> 00:02:40,640
so these common elements can be easily reused. In other words, it makes sense to break up or

29
00:02:40,640 --> 00:02:46,400
factor each model into its building blocks. For example, building blocks of space, of boundaries,

30
00:02:46,400 --> 00:02:52,560
rewards, etc. Once these building blocks are learned, we can rearrange and mix them in different

31
00:02:52,560 --> 00:02:59,200
configurations to build new models of the world on the fly, and thus generate flexible behavior.

32
00:03:00,560 --> 00:03:05,600
As you might remember from part 1, this is exactly what my mailing hippocampus does,

33
00:03:05,600 --> 00:03:11,120
and we can find neurobiological evidence for this process in responses of individual cells.

34
00:03:11,760 --> 00:03:15,360
Now, the question is, can we teach a machine to do the same?

35
00:03:16,320 --> 00:03:22,320
To make the task easier for an artificial system, let's formulate it as a prediction problem.

36
00:03:22,320 --> 00:03:28,240
Namely, the model will receive a sequence of observations, along with the sequence of actions

37
00:03:28,240 --> 00:03:34,000
that led to them, and learn to correctly predict the next observation in the sequence.

38
00:03:34,000 --> 00:03:39,280
It actually makes a lot of sense biologically. There is a great deal of data suggesting that

39
00:03:39,280 --> 00:03:44,400
the main purpose of the brain may be to predict the incoming stimuli and try to minimize the

40
00:03:44,400 --> 00:03:50,240
prediction error, a theory called predictive coding. For example, consider the following

41
00:03:50,240 --> 00:03:56,480
sequence of observations and actions. Can you tell me what should be the next element in the sequence?

42
00:03:57,440 --> 00:04:03,760
Seems impossible, right? However, what if I told you that those actions, one through four,

43
00:04:03,840 --> 00:04:11,360
actually stand for directions, north, west, south, and east? Now, the task becomes much easier.

44
00:04:11,920 --> 00:04:17,280
Because you know the rules how to chain these actions together, you can predict the next

45
00:04:17,280 --> 00:04:22,640
observation to be the same as the first one, since you know you essentially closed a loop.

46
00:04:22,640 --> 00:04:29,040
In other words, knowing the structure of space significantly simplifies the prediction problem.

47
00:04:29,040 --> 00:04:34,400
But the model, of course, would not know this underlying structure since it would be no fun.

48
00:04:34,400 --> 00:04:40,240
Instead, it would need to extract repeating patterns in order to somehow infer this structure

49
00:04:40,240 --> 00:04:46,000
of the underlying world from sequences of observations and actions. For example, after

50
00:04:46,000 --> 00:04:51,440
seeing a large number of sequences like these, it should infer the rules how different actions

51
00:04:51,440 --> 00:04:55,600
relate to each other, which is equivalent to constructing the structure of space.

52
00:04:55,600 --> 00:05:02,400
It's important to point out that although I'm saying things like the model will learn the

53
00:05:02,400 --> 00:05:10,240
underlying structure of the world, it is not told to do that exactly. The model has no other goal,

54
00:05:10,240 --> 00:05:15,120
so to speak, other than predicting the next observation in the sequence. In essence,

55
00:05:15,120 --> 00:05:20,400
it is just a fancy mathematical expression with a large number of parameters that takes the set

56
00:05:20,400 --> 00:05:25,440
of numbers encoding the observations and actions, performs computation on them, and spits out

57
00:05:25,440 --> 00:05:31,680
another set of numbers corresponding to the next predicted observation. But because we train it to

58
00:05:31,680 --> 00:05:37,840
minimize this prediction error, and since these observations are not random but come from some

59
00:05:37,840 --> 00:05:43,920
structured world, the optimal solution to this prediction problem is to construct some structural

60
00:05:43,920 --> 00:05:49,920
representation of this world, which underlies the regularities in the observations. So we simply

61
00:05:49,920 --> 00:05:55,840
expect the knowledge about the structure to emerge as a result of optimization. But how should the

62
00:05:55,840 --> 00:06:02,960
model look like? Well, because we are free to choose whichever architecture we want, it is reasonable

63
00:06:02,960 --> 00:06:09,520
to draw inspiration from an existing biological machine that solves this problem on a daily basis,

64
00:06:09,520 --> 00:06:15,840
the hippocampal formation. In the last video, we saw how the hippocampus receives two streams of

65
00:06:15,840 --> 00:06:22,400
inputs, sensory, the what am I seeing information, coming from the lateral entorhinal cortex,

66
00:06:22,400 --> 00:06:29,280
and structural, the where am I information, from medial entorhinal cortex. They are then combined

67
00:06:29,280 --> 00:06:36,960
in the hippocampus into a conjoined representation. Similarly, our model will have an analog of

68
00:06:36,960 --> 00:06:42,560
medial entorhinal area, responsible for keeping track of current location in the world.

69
00:06:43,520 --> 00:06:49,920
Let's call it a position module. At every point in time, it will receive an action

70
00:06:49,920 --> 00:06:56,640
and use it to compute the estimate of the current location, the best guess of where it is in space.

71
00:06:56,640 --> 00:07:00,800
You can think of this positional information as being encoded by the pattern of neuron

72
00:07:00,800 --> 00:07:07,840
activations inside of it. Note that the position module operates purely with actions and doesn't

73
00:07:07,840 --> 00:07:13,280
receive any information about the sensory observations. Similarly, how if you close your

74
00:07:13,280 --> 00:07:19,040
eyes and walk around the room, you have a rough idea of where you are located, even though you

75
00:07:19,040 --> 00:07:26,480
don't see anything. This is because your brain is able to accumulate self-movement vectors and

76
00:07:26,480 --> 00:07:33,360
estimate the position, a process known as path integration. So once the model is trained,

77
00:07:33,360 --> 00:07:36,960
we expect our position module to be able to do the same.

78
00:07:39,680 --> 00:07:46,000
Another crucial component is the hippocampus itself, which binds the where information

79
00:07:46,000 --> 00:07:52,240
with what. This binding is effectively forming an association between the two inputs.

80
00:07:53,360 --> 00:07:59,280
So we need to add a memory module that would receive the positional information provided

81
00:07:59,280 --> 00:08:05,200
by the position module together with this stream of sensory inputs and store each

82
00:08:05,200 --> 00:08:11,360
encountered combination in memory. Essentially, it memorizes the associations between position and

83
00:08:11,360 --> 00:08:19,040
observation. I was at X when I saw Y. But storing memories would be useless if we couldn't retrieve

84
00:08:19,040 --> 00:08:25,600
them. Importantly, since this is an associative memory module, it should be able to reconstruct

85
00:08:25,600 --> 00:08:33,040
the full memory from partial information. For example, we could provide it with just the position

86
00:08:33,040 --> 00:08:38,560
and it would go and search in all of the stored memories which observations were accompanied

87
00:08:38,560 --> 00:08:44,800
by this position. So essentially answering the question, what did I see last time I was here?

88
00:08:45,520 --> 00:08:51,200
And similarly, we could provide it with just the sensory observation and it would retrieve position.

89
00:08:51,200 --> 00:08:58,800
Where was I last time I saw this? Now we have all the necessary components

90
00:08:58,800 --> 00:09:05,600
to solve the prediction problem. Let's walk step by step on what the trained model will do to come

91
00:09:05,600 --> 00:09:11,520
up with a successful prediction when walking, for example, on a family tree. Remember, it should be

92
00:09:11,520 --> 00:09:18,720
capable of learning any type of structure, not just the four connected grids. So we start on John,

93
00:09:18,720 --> 00:09:24,720
transition to Mary via a sister action, and then to Kate via a daughter action.

94
00:09:25,360 --> 00:09:32,320
Finally, we give the model the action labeled as Uncle and ask it to make a prediction and what's

95
00:09:32,320 --> 00:09:38,560
happening under the hood is the following. At first, the position module has some initial belief

96
00:09:38,560 --> 00:09:44,720
about the current location which is combined with John and this combination is stored in the memory

97
00:09:44,720 --> 00:09:52,000
module. Next, the sister action is fed into the position module which comes up with a new

98
00:09:52,000 --> 00:09:58,720
belief about location that is combined with Mary and the corresponding conjunction is stored in

99
00:09:58,720 --> 00:10:06,160
memory. Similarly, daughter action is used to update the internal state of the position module

100
00:10:06,160 --> 00:10:13,600
which is combined with Kate and sent to the memory module. Finally, the uncle action is fed into the

101
00:10:13,600 --> 00:10:19,680
position module. Importantly, the resulting positional information, the pattern of neuron

102
00:10:19,680 --> 00:10:25,840
activations is the same as the one we started with. This is because after the model is trained on many

103
00:10:25,840 --> 00:10:32,960
family trees underlaying by the same rules, the position module is configured to always return

104
00:10:32,960 --> 00:10:39,840
to the same position when we make loops like these. In other words, the general laws governing the

105
00:10:39,840 --> 00:10:46,320
transition logic on the world graph become embedded into the rules of how the position module

106
00:10:46,320 --> 00:10:53,280
updates its state. After performing path integration correctly, we return to this starting position

107
00:10:54,240 --> 00:11:00,320
but there is no corresponding sensory observation to memorize. Instead, because the model has reached

108
00:11:00,320 --> 00:11:06,640
the end of the sequence, it tries to predict the next observation but it has the path integrated

109
00:11:06,640 --> 00:11:13,760
position to guide this prediction. So it queries the memory module with the positional information

110
00:11:13,760 --> 00:11:20,160
and retrieves a sensory observation corresponding to this particular position which in our case

111
00:11:20,160 --> 00:11:27,120
is John. Awesome, right? So far we have just been theorizing about this spherical model in a vacuum

112
00:11:27,760 --> 00:11:35,200
but does it actually, well, work? And if so, what does it tell us about our own navigational systems?

113
00:11:35,200 --> 00:11:42,400
The most direct way to assess how well the model is performing is to look at its accuracy which is

114
00:11:42,400 --> 00:11:48,400
just the percentage of predictions it made correctly and importantly look at how quickly the accuracy

115
00:11:48,400 --> 00:11:54,240
grows. Here is what I mean. Imagine for a moment that instead of this fancy machine we had just

116
00:11:54,240 --> 00:12:00,400
a good old lookup table which simply memorizes all the transitions as pairs. Previous observation

117
00:12:00,400 --> 00:12:07,440
plus action equals new observation. So it would store memories like John plus sister equals Mary,

118
00:12:07,440 --> 00:12:13,840
Mary plus daughter equals Kate, etc. And to predict the next observation it would simply scan the

119
00:12:13,840 --> 00:12:20,000
lookup table and search for a particular combination. In the case of our family tree example, on first

120
00:12:20,000 --> 00:12:26,320
try it would not be able to predict that Kate's uncle is John because it hadn't encountered this

121
00:12:26,320 --> 00:12:33,840
particular combination previously. In other words, to reach 100% accuracy it would need to first

122
00:12:33,840 --> 00:12:39,840
encounter all possible combinations of observations and actions and this means that the performance

123
00:12:39,840 --> 00:12:47,200
of the model depends on the number of edges of this graph that it visited. In contrast,

124
00:12:47,200 --> 00:12:54,080
tolman eigenbau machine or TEM doesn't need to be explicitly told at the outcome of every action

125
00:12:54,080 --> 00:12:59,840
from every node because it has the notion of a structure. For example, if I tell you that Kate

126
00:12:59,840 --> 00:13:05,360
is Mary's daughter that is enough for you to infer the rest of the relationships automatically.

127
00:13:06,160 --> 00:13:12,800
And this essentially means that to reach 100% accuracy it is enough for TEM to just visit

128
00:13:12,800 --> 00:13:18,640
all the nodes instead of all possible edges and hence its performance depends on the proportion

129
00:13:18,640 --> 00:13:25,520
of nodes visited which grows much faster than the proportion of edges. So our machine seems to

130
00:13:25,520 --> 00:13:31,840
indeed construct a representation of the world. Hooray! But what's going on inside of its brain,

131
00:13:31,840 --> 00:13:38,640
so to speak? Let's look inside the position module first. Remember, the belief about current

132
00:13:38,640 --> 00:13:44,960
location is encoded with a pattern of collective activation of neurons. But we can also interrogate

133
00:13:44,960 --> 00:13:51,040
individual neurons and look at what each one of them is doing as the agent randomly walks around.

134
00:13:51,040 --> 00:13:55,680
Here for the sake of visualization, I'm going to show results after the model was trained on

135
00:13:55,680 --> 00:14:01,600
regular four-connected grids analogs of physical 2D space rather than social hierarchies.

136
00:14:02,800 --> 00:14:08,560
Remarkably, we see that individual units in the position module develop periodic activity

137
00:14:08,560 --> 00:14:14,400
patterns as a function of position. They tile the space with the regular hexagonal grids of

138
00:14:14,480 --> 00:14:22,720
different size or these periodic stripes exactly like grid cells and band cells of the entorhinal

139
00:14:22,720 --> 00:14:30,080
cortex encode position in mammalian brains. And the selectivity of individual units is

140
00:14:30,080 --> 00:14:37,360
preserved across environments, suggesting that they indeed can generalize. Neurons in the memory

141
00:14:37,360 --> 00:14:43,040
module do something different. Since they form a conjunction between positional and sensory

142
00:14:43,040 --> 00:14:49,040
information, each neuron would be active when both of the two upstream components are active.

143
00:14:50,080 --> 00:14:55,920
Indeed, units in the memory module resemble hippocampal place cells of various size,

144
00:14:55,920 --> 00:15:02,560
which fire in a particular patch of space. Importantly, just like hippocampal representations

145
00:15:02,560 --> 00:15:09,520
in real brains, they are firing patterns differ across environments, since the incoming observations

146
00:15:09,520 --> 00:15:16,800
are different. This is known as hippocampal remapping. I'd like to emphasize that such grid-like

147
00:15:16,800 --> 00:15:23,600
and place-like representations were never hard-coded into the model. We started with essentially a

148
00:15:23,600 --> 00:15:28,640
random set of parameters, let the model optimize itself to come up with the best solution to the

149
00:15:28,640 --> 00:15:35,440
prediction problem, and those responses just emerged naturally. So far, we've trained the model

150
00:15:35,440 --> 00:15:41,120
on sequences that were generated from random walks in a given environment, which means that

151
00:15:41,120 --> 00:15:47,120
all the observations were equally likely. But in the real life, animals don't really move by

152
00:15:47,120 --> 00:15:53,600
diffusion. They are biased towards rewards and exploring objects. They like being near walls

153
00:15:53,600 --> 00:16:00,080
because it feels safe and avoid open spaces. So the question is, if we change the statistics of

154
00:16:00,080 --> 00:16:06,160
the sensory observations so that some stimuli are more common than others, would it affect the

155
00:16:06,160 --> 00:16:11,520
representations that emerge in our model as the optimal solution to the prediction problem?

156
00:16:12,320 --> 00:16:18,080
For example, let's train TEM on sequences of observations that mimic the behavior of a real

157
00:16:18,080 --> 00:16:25,520
mouse, which prefers to spend time near boundaries and approaches objects. In this case, representations

158
00:16:25,520 --> 00:16:31,760
that emerge in the position module now include boundary cells, which are selective to borders of

159
00:16:31,760 --> 00:16:38,080
the world, and object vector cells that seem to activate whenever the animal is at a certain

160
00:16:38,080 --> 00:16:44,480
distance and certain direction away from any object. Both of these types of responses,

161
00:16:44,480 --> 00:16:50,560
that by the way also generalize across contexts, are observed experimentally when recording from

162
00:16:50,560 --> 00:16:56,320
entorhinal cortex, while some neurons in the memory module develop selectivity to particular

163
00:16:56,320 --> 00:17:03,920
objects, resembling landmark cells of the hippocampus. If we take a more complex sequence,

164
00:17:03,920 --> 00:17:09,360
such as the one mimicking the animal that is performing an alternation task, the model successfully

165
00:17:09,360 --> 00:17:16,320
learns the rule that the reward is alternating between the sides. Importantly, representations of

166
00:17:16,320 --> 00:17:22,000
some neurons in the memory module resemble the splitter cells found experimentally,

167
00:17:22,000 --> 00:17:26,960
which are modulated by both the position and the direction of the future turn.

168
00:17:27,760 --> 00:17:34,480
This suggests that TEM has the capacity to learn and map latent spaces, which are not

169
00:17:34,480 --> 00:17:41,760
directly given to it in the observations. Another example of how TEM maps latent space is available

170
00:17:41,760 --> 00:17:46,560
as a bonus clip to my Patreon supporters. More details at the end of this video.

171
00:17:47,280 --> 00:17:52,320
Terrific! Now we have a model that can generalize and naturally develops

172
00:17:52,320 --> 00:17:58,000
same representations of space as the hippocampal formation. So what insights can we draw from it?

173
00:18:00,000 --> 00:18:06,160
Recall that play cells remap, which means they change their preferred firing locations in different

174
00:18:06,160 --> 00:18:11,920
environments. This process has not been thought to be random since there is no immediate logic in

175
00:18:11,920 --> 00:18:18,400
how these representations drift around. But having a model of hippocampal formation at hand,

176
00:18:18,400 --> 00:18:25,200
we can start to address this question on a whole other level. Notice that neurons in our memory

177
00:18:25,200 --> 00:18:31,760
module, the ones that resemble play cells, are actually conjunctions between sensory and structural

178
00:18:31,760 --> 00:18:38,800
information. This means that firing of a particular play cell is partially controlled by grid cells,

179
00:18:38,800 --> 00:18:44,160
which provide the structural information. So, for example, if in one environment,

180
00:18:44,160 --> 00:18:50,320
the location of a given play cell coincides with the hexagonal activity pattern of a particular

181
00:18:50,320 --> 00:18:57,280
grid cell, then after we change the surroundings and the play cell remaps, its place field will shift

182
00:18:57,280 --> 00:19:04,720
to another location which also lies on this grid. In other words, remapping is not completely random,

183
00:19:04,720 --> 00:19:09,760
but rather is controlled by grid cells, preserving some structural information.

184
00:19:10,640 --> 00:19:16,720
This relationship between the locations of place and grid cells implies that there should be a

185
00:19:16,720 --> 00:19:23,040
correlation between the degree to which firing locations of place and grid cells coincide across

186
00:19:23,040 --> 00:19:29,120
two environments. This is the case in the model, and remarkably, when the authors tested this

187
00:19:29,120 --> 00:19:34,160
prediction on experimental data, they found it to be true in real brains as well.

188
00:19:37,200 --> 00:19:42,720
Well, I know this was a ton of information to process, so let's try to tie everything together.

189
00:19:43,760 --> 00:19:48,000
The problem of constructing internal models of the world is cornerstone

190
00:19:48,000 --> 00:19:54,960
for both biological and artificial intelligence. It can be solved by factorizing the surrounding

191
00:19:54,960 --> 00:20:00,880
into building blocks and combining them with particular sensory contexts to generate new

192
00:20:00,880 --> 00:20:08,240
models on the go, allowing for rapid generalization. This factorization and composition can be

193
00:20:08,240 --> 00:20:13,840
demonstrated in a computational model, which, when tasked with predicting the next observation in

194
00:20:13,840 --> 00:20:20,800
its sequence, learns the underlying relational structure of the world. Representations that

195
00:20:20,800 --> 00:20:26,880
naturally emerge in this model resemble real neurons found in the hippocampal formation,

196
00:20:26,880 --> 00:20:33,120
suggesting a unified framework of interactions between entorhinal cortex and hippocampus.

197
00:20:35,920 --> 00:20:40,400
I want to take this opportunity to give huge thanks to Dr. James Whittington,

198
00:20:40,480 --> 00:20:46,160
the first author of the original TAM paper, and Gus, my friend and fellow patron with an

199
00:20:46,160 --> 00:20:51,920
expertise in machine learning, who both helped me immensely with preparing the script for this video.

200
00:20:52,800 --> 00:20:57,680
As a final note, I will mention that the Tolman Eigenbaum machine we've seen today

201
00:20:57,680 --> 00:21:03,280
is actually very similar to a transformer architecture, a type of neural network that

202
00:21:03,280 --> 00:21:09,040
is at the core of modern machine learning. In fact, with one little modification,

203
00:21:09,040 --> 00:21:15,520
we can turn this similarity into a precise mathematical equivalence. And this modified

204
00:21:15,520 --> 00:21:21,760
version, called the Tolman Eigenbaum machine transformer, learns much faster and performs

205
00:21:21,760 --> 00:21:28,000
better, while still resembling biological representations for the most part. This potentially

206
00:21:28,000 --> 00:21:32,800
provides a very promising link between neuroscience and modern machine learning,

207
00:21:32,800 --> 00:21:35,600
which makes both fields even more exciting than ever.

208
00:21:35,680 --> 00:21:42,240
Now, I know this was a very simplified description, but fully exploring this equivalence

209
00:21:42,800 --> 00:21:47,120
would require going over the transformer and hopfield networks in detail.

210
00:21:47,840 --> 00:21:53,120
Let me know down in the comment section if you would like to see a more technical video of this kind.

211
00:21:54,000 --> 00:21:58,960
In the meantime, if you're interested in machine learning and don't want to wait any longer,

212
00:21:58,960 --> 00:22:02,800
let me tell you about something that can take your understanding to the next level,

213
00:22:03,520 --> 00:22:10,240
brilliant.org. Brilliant is a revolutionary platform for engaging and interactive learning.

214
00:22:10,240 --> 00:22:15,200
Gone are the days of passive textbook reading. With Brilliant, you'll engage with the material

215
00:22:15,200 --> 00:22:21,600
in a hands-on way, solving problems, answering questions and participating in stunning interactive

216
00:22:21,600 --> 00:22:26,480
visualizations, which help you develop an intuitive understanding of the material.

217
00:22:26,480 --> 00:22:31,600
One course that you might find particularly interesting after watching this video is titled

218
00:22:31,600 --> 00:22:37,200
Artificial Neural Networks. It offers an accessible introduction into the world of artificial

219
00:22:37,200 --> 00:22:43,120
intelligence and how it is inspired by the human brain. You will learn how neural networks work,

220
00:22:43,120 --> 00:22:48,960
how to build your own, and even how to train them to recognize patterns. But that's just the tip of

221
00:22:48,960 --> 00:22:54,640
the iceberg. With over 80 courses to choose from, Brilliant has something for everyone,

222
00:22:54,640 --> 00:23:00,000
and with its personalized approach, you can learn at your own pace with bite-sized chunks.

223
00:23:00,640 --> 00:23:04,880
Take your curiosity to the next level today. Go to brilliant.org

224
00:23:04,880 --> 00:23:11,040
slash artemker sonoff to get a 30-day free trial of everything Brilliant has to offer,

225
00:23:11,040 --> 00:23:16,720
and the first 200 people to use this link will get 20% off the premium subscription.

226
00:23:18,080 --> 00:23:22,320
If you enjoyed this video, press the like button, share it with your friends and colleagues,

227
00:23:22,320 --> 00:23:27,600
subscribe to the channel if you haven't already, and consider supporting me on Patreon to suggest

228
00:23:27,600 --> 00:23:33,440
video topics and enjoy the bonus content. Stay tuned for more interesting topics coming up.

229
00:23:34,000 --> 00:23:43,520
Goodbye, and thank you for the interest in the brain.

