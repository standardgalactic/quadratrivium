{"text": " It is humanity's long-time dream to create machines that can think, but what exactly does it mean? One particular characteristic of intelligence is the ability to generalize knowledge and flexibly adapt it to new situations. Such generalization is indeed one of the cornerstone problems in modern machine learning. In this video, we are going to see how we can take biological organization over hippocampus, a brain structure involved in memory and navigation, as an inspiration in order to construct a computational model that can learn to build abstractions and generalizations, as well as the insights we can draw from this model, both about our own brains and the field of artificial intelligence. Before we begin, I'd like to warn you that this video is the continuation of the previous video in the series on cognitive maps. Last time we explored neurobiological background of hippocampal computations and introduced some general principles, so if you haven't seen it, I highly recommend you check it out before watching this one, since we are going to build up from there. If you're interested, stay tuned. Imagine you're an agent that walks around the world and his only goal is to find rewards. From an evolutionary perspective, you can think about such an agent as an early organism, which needs to look for food or mates. Now, as an agent, you have a certain repertoire of actions that you can take. For example, activate a sequence of muscles to move in a particular direction. To choose the most rewarding actions, you need to be able to predict the action outcomes, and that effectively requires a mental model of the surrounding environment. Existence of such a model allows you to run mental singulations in your head to weigh the actions. For example, what would happen if I go straight or is it better to turn right? Over the course of your lifetime, as you encounter a variety of different environments, initially you might build an entangled, indivisible model for each, without necessarily linking different models to each other. However, if you're being optimal in your representations, at some point you realize, wait a minute, all these models I've built so far actually have an awful lot in common. Indeed, walls that block your way, doors that allow you to go through the walls and even just the structure of an open 2D space itself all work similarly in every environment, so these common elements can be easily reused. In other words, it makes sense to break up or factor each model into its building blocks. For example, building blocks of space, of boundaries, rewards, etc. Once these building blocks are learned, we can rearrange and mix them in different configurations to build new models of the world on the fly, and thus generate flexible behavior. As you might remember from part 1, this is exactly what my mailing hippocampus does, and we can find neurobiological evidence for this process in responses of individual cells. Now, the question is, can we teach a machine to do the same? To make the task easier for an artificial system, let's formulate it as a prediction problem. Namely, the model will receive a sequence of observations, along with the sequence of actions that led to them, and learn to correctly predict the next observation in the sequence. It actually makes a lot of sense biologically. There is a great deal of data suggesting that the main purpose of the brain may be to predict the incoming stimuli and try to minimize the prediction error, a theory called predictive coding. For example, consider the following sequence of observations and actions. Can you tell me what should be the next element in the sequence? Seems impossible, right? However, what if I told you that those actions, one through four, actually stand for directions, north, west, south, and east? Now, the task becomes much easier. Because you know the rules how to chain these actions together, you can predict the next observation to be the same as the first one, since you know you essentially closed a loop. In other words, knowing the structure of space significantly simplifies the prediction problem. But the model, of course, would not know this underlying structure since it would be no fun. Instead, it would need to extract repeating patterns in order to somehow infer this structure of the underlying world from sequences of observations and actions. For example, after seeing a large number of sequences like these, it should infer the rules how different actions relate to each other, which is equivalent to constructing the structure of space. It's important to point out that although I'm saying things like the model will learn the underlying structure of the world, it is not told to do that exactly. The model has no other goal, so to speak, other than predicting the next observation in the sequence. In essence, it is just a fancy mathematical expression with a large number of parameters that takes the set of numbers encoding the observations and actions, performs computation on them, and spits out another set of numbers corresponding to the next predicted observation. But because we train it to minimize this prediction error, and since these observations are not random but come from some structured world, the optimal solution to this prediction problem is to construct some structural representation of this world, which underlies the regularities in the observations. So we simply expect the knowledge about the structure to emerge as a result of optimization. But how should the model look like? Well, because we are free to choose whichever architecture we want, it is reasonable to draw inspiration from an existing biological machine that solves this problem on a daily basis, the hippocampal formation. In the last video, we saw how the hippocampus receives two streams of inputs, sensory, the what am I seeing information, coming from the lateral entorhinal cortex, and structural, the where am I information, from medial entorhinal cortex. They are then combined in the hippocampus into a conjoined representation. Similarly, our model will have an analog of medial entorhinal area, responsible for keeping track of current location in the world. Let's call it a position module. At every point in time, it will receive an action and use it to compute the estimate of the current location, the best guess of where it is in space. You can think of this positional information as being encoded by the pattern of neuron activations inside of it. Note that the position module operates purely with actions and doesn't receive any information about the sensory observations. Similarly, how if you close your eyes and walk around the room, you have a rough idea of where you are located, even though you don't see anything. This is because your brain is able to accumulate self-movement vectors and estimate the position, a process known as path integration. So once the model is trained, we expect our position module to be able to do the same. Another crucial component is the hippocampus itself, which binds the where information with what. This binding is effectively forming an association between the two inputs. So we need to add a memory module that would receive the positional information provided by the position module together with this stream of sensory inputs and store each encountered combination in memory. Essentially, it memorizes the associations between position and observation. I was at X when I saw Y. But storing memories would be useless if we couldn't retrieve them. Importantly, since this is an associative memory module, it should be able to reconstruct the full memory from partial information. For example, we could provide it with just the position and it would go and search in all of the stored memories which observations were accompanied by this position. So essentially answering the question, what did I see last time I was here? And similarly, we could provide it with just the sensory observation and it would retrieve position. Where was I last time I saw this? Now we have all the necessary components to solve the prediction problem. Let's walk step by step on what the trained model will do to come up with a successful prediction when walking, for example, on a family tree. Remember, it should be capable of learning any type of structure, not just the four connected grids. So we start on John, transition to Mary via a sister action, and then to Kate via a daughter action. Finally, we give the model the action labeled as Uncle and ask it to make a prediction and what's happening under the hood is the following. At first, the position module has some initial belief about the current location which is combined with John and this combination is stored in the memory module. Next, the sister action is fed into the position module which comes up with a new belief about location that is combined with Mary and the corresponding conjunction is stored in memory. Similarly, daughter action is used to update the internal state of the position module which is combined with Kate and sent to the memory module. Finally, the uncle action is fed into the position module. Importantly, the resulting positional information, the pattern of neuron activations is the same as the one we started with. This is because after the model is trained on many family trees underlaying by the same rules, the position module is configured to always return to the same position when we make loops like these. In other words, the general laws governing the transition logic on the world graph become embedded into the rules of how the position module updates its state. After performing path integration correctly, we return to this starting position but there is no corresponding sensory observation to memorize. Instead, because the model has reached the end of the sequence, it tries to predict the next observation but it has the path integrated position to guide this prediction. So it queries the memory module with the positional information and retrieves a sensory observation corresponding to this particular position which in our case is John. Awesome, right? So far we have just been theorizing about this spherical model in a vacuum but does it actually, well, work? And if so, what does it tell us about our own navigational systems? The most direct way to assess how well the model is performing is to look at its accuracy which is just the percentage of predictions it made correctly and importantly look at how quickly the accuracy grows. Here is what I mean. Imagine for a moment that instead of this fancy machine we had just a good old lookup table which simply memorizes all the transitions as pairs. Previous observation plus action equals new observation. So it would store memories like John plus sister equals Mary, Mary plus daughter equals Kate, etc. And to predict the next observation it would simply scan the lookup table and search for a particular combination. In the case of our family tree example, on first try it would not be able to predict that Kate's uncle is John because it hadn't encountered this particular combination previously. In other words, to reach 100% accuracy it would need to first encounter all possible combinations of observations and actions and this means that the performance of the model depends on the number of edges of this graph that it visited. In contrast, tolman eigenbau machine or TEM doesn't need to be explicitly told at the outcome of every action from every node because it has the notion of a structure. For example, if I tell you that Kate is Mary's daughter that is enough for you to infer the rest of the relationships automatically. And this essentially means that to reach 100% accuracy it is enough for TEM to just visit all the nodes instead of all possible edges and hence its performance depends on the proportion of nodes visited which grows much faster than the proportion of edges. So our machine seems to indeed construct a representation of the world. Hooray! But what's going on inside of its brain, so to speak? Let's look inside the position module first. Remember, the belief about current location is encoded with a pattern of collective activation of neurons. But we can also interrogate individual neurons and look at what each one of them is doing as the agent randomly walks around. Here for the sake of visualization, I'm going to show results after the model was trained on regular four-connected grids analogs of physical 2D space rather than social hierarchies. Remarkably, we see that individual units in the position module develop periodic activity patterns as a function of position. They tile the space with the regular hexagonal grids of different size or these periodic stripes exactly like grid cells and band cells of the entorhinal cortex encode position in mammalian brains. And the selectivity of individual units is preserved across environments, suggesting that they indeed can generalize. Neurons in the memory module do something different. Since they form a conjunction between positional and sensory information, each neuron would be active when both of the two upstream components are active. Indeed, units in the memory module resemble hippocampal place cells of various size, which fire in a particular patch of space. Importantly, just like hippocampal representations in real brains, they are firing patterns differ across environments, since the incoming observations are different. This is known as hippocampal remapping. I'd like to emphasize that such grid-like and place-like representations were never hard-coded into the model. We started with essentially a random set of parameters, let the model optimize itself to come up with the best solution to the prediction problem, and those responses just emerged naturally. So far, we've trained the model on sequences that were generated from random walks in a given environment, which means that all the observations were equally likely. But in the real life, animals don't really move by diffusion. They are biased towards rewards and exploring objects. They like being near walls because it feels safe and avoid open spaces. So the question is, if we change the statistics of the sensory observations so that some stimuli are more common than others, would it affect the representations that emerge in our model as the optimal solution to the prediction problem? For example, let's train TEM on sequences of observations that mimic the behavior of a real mouse, which prefers to spend time near boundaries and approaches objects. In this case, representations that emerge in the position module now include boundary cells, which are selective to borders of the world, and object vector cells that seem to activate whenever the animal is at a certain distance and certain direction away from any object. Both of these types of responses, that by the way also generalize across contexts, are observed experimentally when recording from entorhinal cortex, while some neurons in the memory module develop selectivity to particular objects, resembling landmark cells of the hippocampus. If we take a more complex sequence, such as the one mimicking the animal that is performing an alternation task, the model successfully learns the rule that the reward is alternating between the sides. Importantly, representations of some neurons in the memory module resemble the splitter cells found experimentally, which are modulated by both the position and the direction of the future turn. This suggests that TEM has the capacity to learn and map latent spaces, which are not directly given to it in the observations. Another example of how TEM maps latent space is available as a bonus clip to my Patreon supporters. More details at the end of this video. Terrific! Now we have a model that can generalize and naturally develops same representations of space as the hippocampal formation. So what insights can we draw from it? Recall that play cells remap, which means they change their preferred firing locations in different environments. This process has not been thought to be random since there is no immediate logic in how these representations drift around. But having a model of hippocampal formation at hand, we can start to address this question on a whole other level. Notice that neurons in our memory module, the ones that resemble play cells, are actually conjunctions between sensory and structural information. This means that firing of a particular play cell is partially controlled by grid cells, which provide the structural information. So, for example, if in one environment, the location of a given play cell coincides with the hexagonal activity pattern of a particular grid cell, then after we change the surroundings and the play cell remaps, its place field will shift to another location which also lies on this grid. In other words, remapping is not completely random, but rather is controlled by grid cells, preserving some structural information. This relationship between the locations of place and grid cells implies that there should be a correlation between the degree to which firing locations of place and grid cells coincide across two environments. This is the case in the model, and remarkably, when the authors tested this prediction on experimental data, they found it to be true in real brains as well. Well, I know this was a ton of information to process, so let's try to tie everything together. The problem of constructing internal models of the world is cornerstone for both biological and artificial intelligence. It can be solved by factorizing the surrounding into building blocks and combining them with particular sensory contexts to generate new models on the go, allowing for rapid generalization. This factorization and composition can be demonstrated in a computational model, which, when tasked with predicting the next observation in its sequence, learns the underlying relational structure of the world. Representations that naturally emerge in this model resemble real neurons found in the hippocampal formation, suggesting a unified framework of interactions between entorhinal cortex and hippocampus. I want to take this opportunity to give huge thanks to Dr. James Whittington, the first author of the original TAM paper, and Gus, my friend and fellow patron with an expertise in machine learning, who both helped me immensely with preparing the script for this video. As a final note, I will mention that the Tolman Eigenbaum machine we've seen today is actually very similar to a transformer architecture, a type of neural network that is at the core of modern machine learning. In fact, with one little modification, we can turn this similarity into a precise mathematical equivalence. And this modified version, called the Tolman Eigenbaum machine transformer, learns much faster and performs better, while still resembling biological representations for the most part. This potentially provides a very promising link between neuroscience and modern machine learning, which makes both fields even more exciting than ever. Now, I know this was a very simplified description, but fully exploring this equivalence would require going over the transformer and hopfield networks in detail. Let me know down in the comment section if you would like to see a more technical video of this kind. In the meantime, if you're interested in machine learning and don't want to wait any longer, let me tell you about something that can take your understanding to the next level, brilliant.org. Brilliant is a revolutionary platform for engaging and interactive learning. Gone are the days of passive textbook reading. With Brilliant, you'll engage with the material in a hands-on way, solving problems, answering questions and participating in stunning interactive visualizations, which help you develop an intuitive understanding of the material. One course that you might find particularly interesting after watching this video is titled Artificial Neural Networks. It offers an accessible introduction into the world of artificial intelligence and how it is inspired by the human brain. You will learn how neural networks work, how to build your own, and even how to train them to recognize patterns. But that's just the tip of the iceberg. With over 80 courses to choose from, Brilliant has something for everyone, and with its personalized approach, you can learn at your own pace with bite-sized chunks. Take your curiosity to the next level today. Go to brilliant.org slash artemker sonoff to get a 30-day free trial of everything Brilliant has to offer, and the first 200 people to use this link will get 20% off the premium subscription. If you enjoyed this video, press the like button, share it with your friends and colleagues, subscribe to the channel if you haven't already, and consider supporting me on Patreon to suggest video topics and enjoy the bonus content. Stay tuned for more interesting topics coming up. Goodbye, and thank you for the interest in the brain.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 6.32, "text": " It is humanity's long-time dream to create machines that can think, but what exactly does it mean?", "tokens": [50364, 467, 307, 10243, 311, 938, 12, 3766, 3055, 281, 1884, 8379, 300, 393, 519, 11, 457, 437, 2293, 775, 309, 914, 30, 50680], "temperature": 0.0, "avg_logprob": -0.10559122633225847, "compression_ratio": 1.5972222222222223, "no_speech_prob": 0.00021993172413203865}, {"id": 1, "seek": 0, "start": 6.32, "end": 11.120000000000001, "text": " One particular characteristic of intelligence is the ability to generalize knowledge", "tokens": [50680, 1485, 1729, 16282, 295, 7599, 307, 264, 3485, 281, 2674, 1125, 3601, 50920], "temperature": 0.0, "avg_logprob": -0.10559122633225847, "compression_ratio": 1.5972222222222223, "no_speech_prob": 0.00021993172413203865}, {"id": 2, "seek": 0, "start": 11.120000000000001, "end": 17.68, "text": " and flexibly adapt it to new situations. Such generalization is indeed one of the cornerstone", "tokens": [50920, 293, 5896, 3545, 6231, 309, 281, 777, 6851, 13, 9653, 2674, 2144, 307, 6451, 472, 295, 264, 4538, 11243, 51248], "temperature": 0.0, "avg_logprob": -0.10559122633225847, "compression_ratio": 1.5972222222222223, "no_speech_prob": 0.00021993172413203865}, {"id": 3, "seek": 0, "start": 17.68, "end": 22.48, "text": " problems in modern machine learning. In this video, we are going to see how we can take", "tokens": [51248, 2740, 294, 4363, 3479, 2539, 13, 682, 341, 960, 11, 321, 366, 516, 281, 536, 577, 321, 393, 747, 51488], "temperature": 0.0, "avg_logprob": -0.10559122633225847, "compression_ratio": 1.5972222222222223, "no_speech_prob": 0.00021993172413203865}, {"id": 4, "seek": 0, "start": 22.48, "end": 28.72, "text": " biological organization over hippocampus, a brain structure involved in memory and navigation,", "tokens": [51488, 13910, 4475, 670, 27745, 905, 1215, 301, 11, 257, 3567, 3877, 3288, 294, 4675, 293, 17346, 11, 51800], "temperature": 0.0, "avg_logprob": -0.10559122633225847, "compression_ratio": 1.5972222222222223, "no_speech_prob": 0.00021993172413203865}, {"id": 5, "seek": 2872, "start": 28.72, "end": 34.0, "text": " as an inspiration in order to construct a computational model that can learn to build", "tokens": [50364, 382, 364, 10249, 294, 1668, 281, 7690, 257, 28270, 2316, 300, 393, 1466, 281, 1322, 50628], "temperature": 0.0, "avg_logprob": -0.0613097554629611, "compression_ratio": 1.6401515151515151, "no_speech_prob": 0.0005441727116703987}, {"id": 6, "seek": 2872, "start": 34.0, "end": 38.879999999999995, "text": " abstractions and generalizations, as well as the insights we can draw from this model,", "tokens": [50628, 12649, 626, 293, 2674, 14455, 11, 382, 731, 382, 264, 14310, 321, 393, 2642, 490, 341, 2316, 11, 50872], "temperature": 0.0, "avg_logprob": -0.0613097554629611, "compression_ratio": 1.6401515151515151, "no_speech_prob": 0.0005441727116703987}, {"id": 7, "seek": 2872, "start": 38.879999999999995, "end": 43.36, "text": " both about our own brains and the field of artificial intelligence.", "tokens": [50872, 1293, 466, 527, 1065, 15442, 293, 264, 2519, 295, 11677, 7599, 13, 51096], "temperature": 0.0, "avg_logprob": -0.0613097554629611, "compression_ratio": 1.6401515151515151, "no_speech_prob": 0.0005441727116703987}, {"id": 8, "seek": 2872, "start": 43.36, "end": 48.32, "text": " Before we begin, I'd like to warn you that this video is the continuation of the previous video", "tokens": [51096, 4546, 321, 1841, 11, 286, 1116, 411, 281, 12286, 291, 300, 341, 960, 307, 264, 29357, 295, 264, 3894, 960, 51344], "temperature": 0.0, "avg_logprob": -0.0613097554629611, "compression_ratio": 1.6401515151515151, "no_speech_prob": 0.0005441727116703987}, {"id": 9, "seek": 2872, "start": 48.32, "end": 54.08, "text": " in the series on cognitive maps. Last time we explored neurobiological background of hippocampal", "tokens": [51344, 294, 264, 2638, 322, 15605, 11317, 13, 5264, 565, 321, 24016, 16499, 5614, 4383, 3678, 295, 27745, 905, 1215, 304, 51632], "temperature": 0.0, "avg_logprob": -0.0613097554629611, "compression_ratio": 1.6401515151515151, "no_speech_prob": 0.0005441727116703987}, {"id": 10, "seek": 5408, "start": 54.08, "end": 59.36, "text": " computations and introduced some general principles, so if you haven't seen it, I highly", "tokens": [50364, 2807, 763, 293, 7268, 512, 2674, 9156, 11, 370, 498, 291, 2378, 380, 1612, 309, 11, 286, 5405, 50628], "temperature": 0.0, "avg_logprob": -0.1282659657796224, "compression_ratio": 1.5247524752475248, "no_speech_prob": 0.00015117997827474028}, {"id": 11, "seek": 5408, "start": 59.36, "end": 64.08, "text": " recommend you check it out before watching this one, since we are going to build up from there.", "tokens": [50628, 2748, 291, 1520, 309, 484, 949, 1976, 341, 472, 11, 1670, 321, 366, 516, 281, 1322, 493, 490, 456, 13, 50864], "temperature": 0.0, "avg_logprob": -0.1282659657796224, "compression_ratio": 1.5247524752475248, "no_speech_prob": 0.00015117997827474028}, {"id": 12, "seek": 5408, "start": 64.64, "end": 66.32, "text": " If you're interested, stay tuned.", "tokens": [50892, 759, 291, 434, 3102, 11, 1754, 10870, 13, 50976], "temperature": 0.0, "avg_logprob": -0.1282659657796224, "compression_ratio": 1.5247524752475248, "no_speech_prob": 0.00015117997827474028}, {"id": 13, "seek": 5408, "start": 73.2, "end": 79.2, "text": " Imagine you're an agent that walks around the world and his only goal is to find rewards.", "tokens": [51320, 11739, 291, 434, 364, 9461, 300, 12896, 926, 264, 1002, 293, 702, 787, 3387, 307, 281, 915, 17203, 13, 51620], "temperature": 0.0, "avg_logprob": -0.1282659657796224, "compression_ratio": 1.5247524752475248, "no_speech_prob": 0.00015117997827474028}, {"id": 14, "seek": 7920, "start": 79.2, "end": 84.16, "text": " From an evolutionary perspective, you can think about such an agent as an early organism,", "tokens": [50364, 3358, 364, 27567, 4585, 11, 291, 393, 519, 466, 1270, 364, 9461, 382, 364, 2440, 24128, 11, 50612], "temperature": 0.0, "avg_logprob": -0.056871891021728516, "compression_ratio": 1.6678966789667897, "no_speech_prob": 0.00019110363791696727}, {"id": 15, "seek": 7920, "start": 84.16, "end": 90.08, "text": " which needs to look for food or mates. Now, as an agent, you have a certain repertoire", "tokens": [50612, 597, 2203, 281, 574, 337, 1755, 420, 31488, 13, 823, 11, 382, 364, 9461, 11, 291, 362, 257, 1629, 49604, 50908], "temperature": 0.0, "avg_logprob": -0.056871891021728516, "compression_ratio": 1.6678966789667897, "no_speech_prob": 0.00019110363791696727}, {"id": 16, "seek": 7920, "start": 90.08, "end": 95.36, "text": " of actions that you can take. For example, activate a sequence of muscles to move in a particular", "tokens": [50908, 295, 5909, 300, 291, 393, 747, 13, 1171, 1365, 11, 13615, 257, 8310, 295, 9530, 281, 1286, 294, 257, 1729, 51172], "temperature": 0.0, "avg_logprob": -0.056871891021728516, "compression_ratio": 1.6678966789667897, "no_speech_prob": 0.00019110363791696727}, {"id": 17, "seek": 7920, "start": 95.36, "end": 102.4, "text": " direction. To choose the most rewarding actions, you need to be able to predict the action outcomes,", "tokens": [51172, 3513, 13, 1407, 2826, 264, 881, 20063, 5909, 11, 291, 643, 281, 312, 1075, 281, 6069, 264, 3069, 10070, 11, 51524], "temperature": 0.0, "avg_logprob": -0.056871891021728516, "compression_ratio": 1.6678966789667897, "no_speech_prob": 0.00019110363791696727}, {"id": 18, "seek": 7920, "start": 102.4, "end": 107.2, "text": " and that effectively requires a mental model of the surrounding environment.", "tokens": [51524, 293, 300, 8659, 7029, 257, 4973, 2316, 295, 264, 11498, 2823, 13, 51764], "temperature": 0.0, "avg_logprob": -0.056871891021728516, "compression_ratio": 1.6678966789667897, "no_speech_prob": 0.00019110363791696727}, {"id": 19, "seek": 10720, "start": 107.2, "end": 112.08, "text": " Existence of such a model allows you to run mental singulations in your head", "tokens": [50364, 2111, 468, 655, 295, 1270, 257, 2316, 4045, 291, 281, 1190, 4973, 1522, 4136, 294, 428, 1378, 50608], "temperature": 0.0, "avg_logprob": -0.06766071686377892, "compression_ratio": 1.64, "no_speech_prob": 1.8058495697914623e-05}, {"id": 20, "seek": 10720, "start": 112.08, "end": 117.68, "text": " to weigh the actions. For example, what would happen if I go straight or is it better to turn", "tokens": [50608, 281, 13843, 264, 5909, 13, 1171, 1365, 11, 437, 576, 1051, 498, 286, 352, 2997, 420, 307, 309, 1101, 281, 1261, 50888], "temperature": 0.0, "avg_logprob": -0.06766071686377892, "compression_ratio": 1.64, "no_speech_prob": 1.8058495697914623e-05}, {"id": 21, "seek": 10720, "start": 117.68, "end": 123.36, "text": " right? Over the course of your lifetime, as you encounter a variety of different environments,", "tokens": [50888, 558, 30, 4886, 264, 1164, 295, 428, 11364, 11, 382, 291, 8593, 257, 5673, 295, 819, 12388, 11, 51172], "temperature": 0.0, "avg_logprob": -0.06766071686377892, "compression_ratio": 1.64, "no_speech_prob": 1.8058495697914623e-05}, {"id": 22, "seek": 10720, "start": 123.36, "end": 129.28, "text": " initially you might build an entangled, indivisible model for each, without necessarily linking", "tokens": [51172, 9105, 291, 1062, 1322, 364, 948, 39101, 11, 1016, 592, 271, 964, 2316, 337, 1184, 11, 1553, 4725, 25775, 51468], "temperature": 0.0, "avg_logprob": -0.06766071686377892, "compression_ratio": 1.64, "no_speech_prob": 1.8058495697914623e-05}, {"id": 23, "seek": 10720, "start": 129.28, "end": 134.56, "text": " different models to each other. However, if you're being optimal in your representations,", "tokens": [51468, 819, 5245, 281, 1184, 661, 13, 2908, 11, 498, 291, 434, 885, 16252, 294, 428, 33358, 11, 51732], "temperature": 0.0, "avg_logprob": -0.06766071686377892, "compression_ratio": 1.64, "no_speech_prob": 1.8058495697914623e-05}, {"id": 24, "seek": 13456, "start": 134.56, "end": 140.0, "text": " at some point you realize, wait a minute, all these models I've built so far actually have", "tokens": [50364, 412, 512, 935, 291, 4325, 11, 1699, 257, 3456, 11, 439, 613, 5245, 286, 600, 3094, 370, 1400, 767, 362, 50636], "temperature": 0.0, "avg_logprob": -0.07854299647833711, "compression_ratio": 1.5826446280991735, "no_speech_prob": 4.611265831044875e-05}, {"id": 25, "seek": 13456, "start": 140.0, "end": 146.72, "text": " an awful lot in common. Indeed, walls that block your way, doors that allow you to go through the", "tokens": [50636, 364, 11232, 688, 294, 2689, 13, 15061, 11, 7920, 300, 3461, 428, 636, 11, 8077, 300, 2089, 291, 281, 352, 807, 264, 50972], "temperature": 0.0, "avg_logprob": -0.07854299647833711, "compression_ratio": 1.5826446280991735, "no_speech_prob": 4.611265831044875e-05}, {"id": 26, "seek": 13456, "start": 146.72, "end": 153.68, "text": " walls and even just the structure of an open 2D space itself all work similarly in every environment,", "tokens": [50972, 7920, 293, 754, 445, 264, 3877, 295, 364, 1269, 568, 35, 1901, 2564, 439, 589, 14138, 294, 633, 2823, 11, 51320], "temperature": 0.0, "avg_logprob": -0.07854299647833711, "compression_ratio": 1.5826446280991735, "no_speech_prob": 4.611265831044875e-05}, {"id": 27, "seek": 13456, "start": 153.68, "end": 160.64000000000001, "text": " so these common elements can be easily reused. In other words, it makes sense to break up or", "tokens": [51320, 370, 613, 2689, 4959, 393, 312, 3612, 319, 4717, 13, 682, 661, 2283, 11, 309, 1669, 2020, 281, 1821, 493, 420, 51668], "temperature": 0.0, "avg_logprob": -0.07854299647833711, "compression_ratio": 1.5826446280991735, "no_speech_prob": 4.611265831044875e-05}, {"id": 28, "seek": 16064, "start": 160.64, "end": 166.39999999999998, "text": " factor each model into its building blocks. For example, building blocks of space, of boundaries,", "tokens": [50364, 5952, 1184, 2316, 666, 1080, 2390, 8474, 13, 1171, 1365, 11, 2390, 8474, 295, 1901, 11, 295, 13180, 11, 50652], "temperature": 0.0, "avg_logprob": -0.11055909755618074, "compression_ratio": 1.6, "no_speech_prob": 0.0004238798574078828}, {"id": 29, "seek": 16064, "start": 166.39999999999998, "end": 172.55999999999997, "text": " rewards, etc. Once these building blocks are learned, we can rearrange and mix them in different", "tokens": [50652, 17203, 11, 5183, 13, 3443, 613, 2390, 8474, 366, 3264, 11, 321, 393, 39568, 293, 2890, 552, 294, 819, 50960], "temperature": 0.0, "avg_logprob": -0.11055909755618074, "compression_ratio": 1.6, "no_speech_prob": 0.0004238798574078828}, {"id": 30, "seek": 16064, "start": 172.55999999999997, "end": 179.2, "text": " configurations to build new models of the world on the fly, and thus generate flexible behavior.", "tokens": [50960, 31493, 281, 1322, 777, 5245, 295, 264, 1002, 322, 264, 3603, 11, 293, 8807, 8460, 11358, 5223, 13, 51292], "temperature": 0.0, "avg_logprob": -0.11055909755618074, "compression_ratio": 1.6, "no_speech_prob": 0.0004238798574078828}, {"id": 31, "seek": 16064, "start": 180.56, "end": 185.6, "text": " As you might remember from part 1, this is exactly what my mailing hippocampus does,", "tokens": [51360, 1018, 291, 1062, 1604, 490, 644, 502, 11, 341, 307, 2293, 437, 452, 41612, 27745, 905, 1215, 301, 775, 11, 51612], "temperature": 0.0, "avg_logprob": -0.11055909755618074, "compression_ratio": 1.6, "no_speech_prob": 0.0004238798574078828}, {"id": 32, "seek": 18560, "start": 185.6, "end": 191.12, "text": " and we can find neurobiological evidence for this process in responses of individual cells.", "tokens": [50364, 293, 321, 393, 915, 16499, 5614, 4383, 4467, 337, 341, 1399, 294, 13019, 295, 2609, 5438, 13, 50640], "temperature": 0.0, "avg_logprob": -0.07354775342074307, "compression_ratio": 1.6877470355731226, "no_speech_prob": 0.00020662879978772253}, {"id": 33, "seek": 18560, "start": 191.76, "end": 195.35999999999999, "text": " Now, the question is, can we teach a machine to do the same?", "tokens": [50672, 823, 11, 264, 1168, 307, 11, 393, 321, 2924, 257, 3479, 281, 360, 264, 912, 30, 50852], "temperature": 0.0, "avg_logprob": -0.07354775342074307, "compression_ratio": 1.6877470355731226, "no_speech_prob": 0.00020662879978772253}, {"id": 34, "seek": 18560, "start": 196.32, "end": 202.32, "text": " To make the task easier for an artificial system, let's formulate it as a prediction problem.", "tokens": [50900, 1407, 652, 264, 5633, 3571, 337, 364, 11677, 1185, 11, 718, 311, 47881, 309, 382, 257, 17630, 1154, 13, 51200], "temperature": 0.0, "avg_logprob": -0.07354775342074307, "compression_ratio": 1.6877470355731226, "no_speech_prob": 0.00020662879978772253}, {"id": 35, "seek": 18560, "start": 202.32, "end": 208.24, "text": " Namely, the model will receive a sequence of observations, along with the sequence of actions", "tokens": [51200, 10684, 736, 11, 264, 2316, 486, 4774, 257, 8310, 295, 18163, 11, 2051, 365, 264, 8310, 295, 5909, 51496], "temperature": 0.0, "avg_logprob": -0.07354775342074307, "compression_ratio": 1.6877470355731226, "no_speech_prob": 0.00020662879978772253}, {"id": 36, "seek": 18560, "start": 208.24, "end": 214.0, "text": " that led to them, and learn to correctly predict the next observation in the sequence.", "tokens": [51496, 300, 4684, 281, 552, 11, 293, 1466, 281, 8944, 6069, 264, 958, 14816, 294, 264, 8310, 13, 51784], "temperature": 0.0, "avg_logprob": -0.07354775342074307, "compression_ratio": 1.6877470355731226, "no_speech_prob": 0.00020662879978772253}, {"id": 37, "seek": 21400, "start": 214.0, "end": 219.28, "text": " It actually makes a lot of sense biologically. There is a great deal of data suggesting that", "tokens": [50364, 467, 767, 1669, 257, 688, 295, 2020, 3228, 17157, 13, 821, 307, 257, 869, 2028, 295, 1412, 18094, 300, 50628], "temperature": 0.0, "avg_logprob": -0.06715668164766751, "compression_ratio": 1.6654804270462633, "no_speech_prob": 1.9833300029858947e-05}, {"id": 38, "seek": 21400, "start": 219.28, "end": 224.4, "text": " the main purpose of the brain may be to predict the incoming stimuli and try to minimize the", "tokens": [50628, 264, 2135, 4334, 295, 264, 3567, 815, 312, 281, 6069, 264, 22341, 47752, 293, 853, 281, 17522, 264, 50884], "temperature": 0.0, "avg_logprob": -0.06715668164766751, "compression_ratio": 1.6654804270462633, "no_speech_prob": 1.9833300029858947e-05}, {"id": 39, "seek": 21400, "start": 224.4, "end": 230.24, "text": " prediction error, a theory called predictive coding. For example, consider the following", "tokens": [50884, 17630, 6713, 11, 257, 5261, 1219, 35521, 17720, 13, 1171, 1365, 11, 1949, 264, 3480, 51176], "temperature": 0.0, "avg_logprob": -0.06715668164766751, "compression_ratio": 1.6654804270462633, "no_speech_prob": 1.9833300029858947e-05}, {"id": 40, "seek": 21400, "start": 230.24, "end": 236.48, "text": " sequence of observations and actions. Can you tell me what should be the next element in the sequence?", "tokens": [51176, 8310, 295, 18163, 293, 5909, 13, 1664, 291, 980, 385, 437, 820, 312, 264, 958, 4478, 294, 264, 8310, 30, 51488], "temperature": 0.0, "avg_logprob": -0.06715668164766751, "compression_ratio": 1.6654804270462633, "no_speech_prob": 1.9833300029858947e-05}, {"id": 41, "seek": 21400, "start": 237.44, "end": 243.76, "text": " Seems impossible, right? However, what if I told you that those actions, one through four,", "tokens": [51536, 22524, 6243, 11, 558, 30, 2908, 11, 437, 498, 286, 1907, 291, 300, 729, 5909, 11, 472, 807, 1451, 11, 51852], "temperature": 0.0, "avg_logprob": -0.06715668164766751, "compression_ratio": 1.6654804270462633, "no_speech_prob": 1.9833300029858947e-05}, {"id": 42, "seek": 24376, "start": 243.84, "end": 251.35999999999999, "text": " actually stand for directions, north, west, south, and east? Now, the task becomes much easier.", "tokens": [50368, 767, 1463, 337, 11095, 11, 6830, 11, 7009, 11, 7377, 11, 293, 10648, 30, 823, 11, 264, 5633, 3643, 709, 3571, 13, 50744], "temperature": 0.0, "avg_logprob": -0.07180448373158772, "compression_ratio": 1.6130434782608696, "no_speech_prob": 0.0001088961580535397}, {"id": 43, "seek": 24376, "start": 251.92, "end": 257.28, "text": " Because you know the rules how to chain these actions together, you can predict the next", "tokens": [50772, 1436, 291, 458, 264, 4474, 577, 281, 5021, 613, 5909, 1214, 11, 291, 393, 6069, 264, 958, 51040], "temperature": 0.0, "avg_logprob": -0.07180448373158772, "compression_ratio": 1.6130434782608696, "no_speech_prob": 0.0001088961580535397}, {"id": 44, "seek": 24376, "start": 257.28, "end": 262.64, "text": " observation to be the same as the first one, since you know you essentially closed a loop.", "tokens": [51040, 14816, 281, 312, 264, 912, 382, 264, 700, 472, 11, 1670, 291, 458, 291, 4476, 5395, 257, 6367, 13, 51308], "temperature": 0.0, "avg_logprob": -0.07180448373158772, "compression_ratio": 1.6130434782608696, "no_speech_prob": 0.0001088961580535397}, {"id": 45, "seek": 24376, "start": 262.64, "end": 269.03999999999996, "text": " In other words, knowing the structure of space significantly simplifies the prediction problem.", "tokens": [51308, 682, 661, 2283, 11, 5276, 264, 3877, 295, 1901, 10591, 6883, 11221, 264, 17630, 1154, 13, 51628], "temperature": 0.0, "avg_logprob": -0.07180448373158772, "compression_ratio": 1.6130434782608696, "no_speech_prob": 0.0001088961580535397}, {"id": 46, "seek": 26904, "start": 269.04, "end": 274.40000000000003, "text": " But the model, of course, would not know this underlying structure since it would be no fun.", "tokens": [50364, 583, 264, 2316, 11, 295, 1164, 11, 576, 406, 458, 341, 14217, 3877, 1670, 309, 576, 312, 572, 1019, 13, 50632], "temperature": 0.0, "avg_logprob": -0.06774579098350124, "compression_ratio": 1.744186046511628, "no_speech_prob": 3.7052828702144325e-05}, {"id": 47, "seek": 26904, "start": 274.40000000000003, "end": 280.24, "text": " Instead, it would need to extract repeating patterns in order to somehow infer this structure", "tokens": [50632, 7156, 11, 309, 576, 643, 281, 8947, 18617, 8294, 294, 1668, 281, 6063, 13596, 341, 3877, 50924], "temperature": 0.0, "avg_logprob": -0.06774579098350124, "compression_ratio": 1.744186046511628, "no_speech_prob": 3.7052828702144325e-05}, {"id": 48, "seek": 26904, "start": 280.24, "end": 286.0, "text": " of the underlying world from sequences of observations and actions. For example, after", "tokens": [50924, 295, 264, 14217, 1002, 490, 22978, 295, 18163, 293, 5909, 13, 1171, 1365, 11, 934, 51212], "temperature": 0.0, "avg_logprob": -0.06774579098350124, "compression_ratio": 1.744186046511628, "no_speech_prob": 3.7052828702144325e-05}, {"id": 49, "seek": 26904, "start": 286.0, "end": 291.44, "text": " seeing a large number of sequences like these, it should infer the rules how different actions", "tokens": [51212, 2577, 257, 2416, 1230, 295, 22978, 411, 613, 11, 309, 820, 13596, 264, 4474, 577, 819, 5909, 51484], "temperature": 0.0, "avg_logprob": -0.06774579098350124, "compression_ratio": 1.744186046511628, "no_speech_prob": 3.7052828702144325e-05}, {"id": 50, "seek": 26904, "start": 291.44, "end": 295.6, "text": " relate to each other, which is equivalent to constructing the structure of space.", "tokens": [51484, 10961, 281, 1184, 661, 11, 597, 307, 10344, 281, 39969, 264, 3877, 295, 1901, 13, 51692], "temperature": 0.0, "avg_logprob": -0.06774579098350124, "compression_ratio": 1.744186046511628, "no_speech_prob": 3.7052828702144325e-05}, {"id": 51, "seek": 29560, "start": 295.6, "end": 302.40000000000003, "text": " It's important to point out that although I'm saying things like the model will learn the", "tokens": [50364, 467, 311, 1021, 281, 935, 484, 300, 4878, 286, 478, 1566, 721, 411, 264, 2316, 486, 1466, 264, 50704], "temperature": 0.0, "avg_logprob": -0.0905809124696602, "compression_ratio": 1.721189591078067, "no_speech_prob": 5.1442064432194456e-05}, {"id": 52, "seek": 29560, "start": 302.40000000000003, "end": 310.24, "text": " underlying structure of the world, it is not told to do that exactly. The model has no other goal,", "tokens": [50704, 14217, 3877, 295, 264, 1002, 11, 309, 307, 406, 1907, 281, 360, 300, 2293, 13, 440, 2316, 575, 572, 661, 3387, 11, 51096], "temperature": 0.0, "avg_logprob": -0.0905809124696602, "compression_ratio": 1.721189591078067, "no_speech_prob": 5.1442064432194456e-05}, {"id": 53, "seek": 29560, "start": 310.24, "end": 315.12, "text": " so to speak, other than predicting the next observation in the sequence. In essence,", "tokens": [51096, 370, 281, 1710, 11, 661, 813, 32884, 264, 958, 14816, 294, 264, 8310, 13, 682, 12801, 11, 51340], "temperature": 0.0, "avg_logprob": -0.0905809124696602, "compression_ratio": 1.721189591078067, "no_speech_prob": 5.1442064432194456e-05}, {"id": 54, "seek": 29560, "start": 315.12, "end": 320.40000000000003, "text": " it is just a fancy mathematical expression with a large number of parameters that takes the set", "tokens": [51340, 309, 307, 445, 257, 10247, 18894, 6114, 365, 257, 2416, 1230, 295, 9834, 300, 2516, 264, 992, 51604], "temperature": 0.0, "avg_logprob": -0.0905809124696602, "compression_ratio": 1.721189591078067, "no_speech_prob": 5.1442064432194456e-05}, {"id": 55, "seek": 29560, "start": 320.40000000000003, "end": 325.44, "text": " of numbers encoding the observations and actions, performs computation on them, and spits out", "tokens": [51604, 295, 3547, 43430, 264, 18163, 293, 5909, 11, 26213, 24903, 322, 552, 11, 293, 637, 1208, 484, 51856], "temperature": 0.0, "avg_logprob": -0.0905809124696602, "compression_ratio": 1.721189591078067, "no_speech_prob": 5.1442064432194456e-05}, {"id": 56, "seek": 32544, "start": 325.44, "end": 331.68, "text": " another set of numbers corresponding to the next predicted observation. But because we train it to", "tokens": [50364, 1071, 992, 295, 3547, 11760, 281, 264, 958, 19147, 14816, 13, 583, 570, 321, 3847, 309, 281, 50676], "temperature": 0.0, "avg_logprob": -0.06881714494604814, "compression_ratio": 1.7477477477477477, "no_speech_prob": 2.9311349862837233e-05}, {"id": 57, "seek": 32544, "start": 331.68, "end": 337.84, "text": " minimize this prediction error, and since these observations are not random but come from some", "tokens": [50676, 17522, 341, 17630, 6713, 11, 293, 1670, 613, 18163, 366, 406, 4974, 457, 808, 490, 512, 50984], "temperature": 0.0, "avg_logprob": -0.06881714494604814, "compression_ratio": 1.7477477477477477, "no_speech_prob": 2.9311349862837233e-05}, {"id": 58, "seek": 32544, "start": 337.84, "end": 343.92, "text": " structured world, the optimal solution to this prediction problem is to construct some structural", "tokens": [50984, 18519, 1002, 11, 264, 16252, 3827, 281, 341, 17630, 1154, 307, 281, 7690, 512, 15067, 51288], "temperature": 0.0, "avg_logprob": -0.06881714494604814, "compression_ratio": 1.7477477477477477, "no_speech_prob": 2.9311349862837233e-05}, {"id": 59, "seek": 32544, "start": 343.92, "end": 349.92, "text": " representation of this world, which underlies the regularities in the observations. So we simply", "tokens": [51288, 10290, 295, 341, 1002, 11, 597, 833, 24119, 264, 3890, 1088, 294, 264, 18163, 13, 407, 321, 2935, 51588], "temperature": 0.0, "avg_logprob": -0.06881714494604814, "compression_ratio": 1.7477477477477477, "no_speech_prob": 2.9311349862837233e-05}, {"id": 60, "seek": 34992, "start": 349.92, "end": 355.84000000000003, "text": " expect the knowledge about the structure to emerge as a result of optimization. But how should the", "tokens": [50364, 2066, 264, 3601, 466, 264, 3877, 281, 21511, 382, 257, 1874, 295, 19618, 13, 583, 577, 820, 264, 50660], "temperature": 0.0, "avg_logprob": -0.0652955516000812, "compression_ratio": 1.5903614457831325, "no_speech_prob": 0.0004173139459453523}, {"id": 61, "seek": 34992, "start": 355.84000000000003, "end": 362.96000000000004, "text": " model look like? Well, because we are free to choose whichever architecture we want, it is reasonable", "tokens": [50660, 2316, 574, 411, 30, 1042, 11, 570, 321, 366, 1737, 281, 2826, 24123, 9482, 321, 528, 11, 309, 307, 10585, 51016], "temperature": 0.0, "avg_logprob": -0.0652955516000812, "compression_ratio": 1.5903614457831325, "no_speech_prob": 0.0004173139459453523}, {"id": 62, "seek": 34992, "start": 362.96000000000004, "end": 369.52000000000004, "text": " to draw inspiration from an existing biological machine that solves this problem on a daily basis,", "tokens": [51016, 281, 2642, 10249, 490, 364, 6741, 13910, 3479, 300, 39890, 341, 1154, 322, 257, 5212, 5143, 11, 51344], "temperature": 0.0, "avg_logprob": -0.0652955516000812, "compression_ratio": 1.5903614457831325, "no_speech_prob": 0.0004173139459453523}, {"id": 63, "seek": 34992, "start": 369.52000000000004, "end": 375.84000000000003, "text": " the hippocampal formation. In the last video, we saw how the hippocampus receives two streams of", "tokens": [51344, 264, 27745, 905, 1215, 304, 11723, 13, 682, 264, 1036, 960, 11, 321, 1866, 577, 264, 27745, 905, 1215, 301, 20717, 732, 15842, 295, 51660], "temperature": 0.0, "avg_logprob": -0.0652955516000812, "compression_ratio": 1.5903614457831325, "no_speech_prob": 0.0004173139459453523}, {"id": 64, "seek": 37584, "start": 375.84, "end": 382.4, "text": " inputs, sensory, the what am I seeing information, coming from the lateral entorhinal cortex,", "tokens": [50364, 15743, 11, 27233, 11, 264, 437, 669, 286, 2577, 1589, 11, 1348, 490, 264, 25128, 948, 284, 71, 2071, 33312, 11, 50692], "temperature": 0.0, "avg_logprob": -0.09233841997511844, "compression_ratio": 1.7123287671232876, "no_speech_prob": 0.00014653051039204001}, {"id": 65, "seek": 37584, "start": 382.4, "end": 389.28, "text": " and structural, the where am I information, from medial entorhinal cortex. They are then combined", "tokens": [50692, 293, 15067, 11, 264, 689, 669, 286, 1589, 11, 490, 1205, 831, 948, 284, 71, 2071, 33312, 13, 814, 366, 550, 9354, 51036], "temperature": 0.0, "avg_logprob": -0.09233841997511844, "compression_ratio": 1.7123287671232876, "no_speech_prob": 0.00014653051039204001}, {"id": 66, "seek": 37584, "start": 389.28, "end": 396.96, "text": " in the hippocampus into a conjoined representation. Similarly, our model will have an analog of", "tokens": [51036, 294, 264, 27745, 905, 1215, 301, 666, 257, 416, 5134, 2001, 10290, 13, 13157, 11, 527, 2316, 486, 362, 364, 16660, 295, 51420], "temperature": 0.0, "avg_logprob": -0.09233841997511844, "compression_ratio": 1.7123287671232876, "no_speech_prob": 0.00014653051039204001}, {"id": 67, "seek": 37584, "start": 396.96, "end": 402.55999999999995, "text": " medial entorhinal area, responsible for keeping track of current location in the world.", "tokens": [51420, 1205, 831, 948, 284, 71, 2071, 1859, 11, 6250, 337, 5145, 2837, 295, 2190, 4914, 294, 264, 1002, 13, 51700], "temperature": 0.0, "avg_logprob": -0.09233841997511844, "compression_ratio": 1.7123287671232876, "no_speech_prob": 0.00014653051039204001}, {"id": 68, "seek": 40256, "start": 403.52, "end": 409.92, "text": " Let's call it a position module. At every point in time, it will receive an action", "tokens": [50412, 961, 311, 818, 309, 257, 2535, 10088, 13, 1711, 633, 935, 294, 565, 11, 309, 486, 4774, 364, 3069, 50732], "temperature": 0.0, "avg_logprob": -0.05078464885090673, "compression_ratio": 1.6636363636363636, "no_speech_prob": 9.610222332412377e-05}, {"id": 69, "seek": 40256, "start": 409.92, "end": 416.64, "text": " and use it to compute the estimate of the current location, the best guess of where it is in space.", "tokens": [50732, 293, 764, 309, 281, 14722, 264, 12539, 295, 264, 2190, 4914, 11, 264, 1151, 2041, 295, 689, 309, 307, 294, 1901, 13, 51068], "temperature": 0.0, "avg_logprob": -0.05078464885090673, "compression_ratio": 1.6636363636363636, "no_speech_prob": 9.610222332412377e-05}, {"id": 70, "seek": 40256, "start": 416.64, "end": 420.8, "text": " You can think of this positional information as being encoded by the pattern of neuron", "tokens": [51068, 509, 393, 519, 295, 341, 2535, 304, 1589, 382, 885, 2058, 12340, 538, 264, 5102, 295, 34090, 51276], "temperature": 0.0, "avg_logprob": -0.05078464885090673, "compression_ratio": 1.6636363636363636, "no_speech_prob": 9.610222332412377e-05}, {"id": 71, "seek": 40256, "start": 420.8, "end": 427.84000000000003, "text": " activations inside of it. Note that the position module operates purely with actions and doesn't", "tokens": [51276, 2430, 763, 1854, 295, 309, 13, 11633, 300, 264, 2535, 10088, 22577, 17491, 365, 5909, 293, 1177, 380, 51628], "temperature": 0.0, "avg_logprob": -0.05078464885090673, "compression_ratio": 1.6636363636363636, "no_speech_prob": 9.610222332412377e-05}, {"id": 72, "seek": 42784, "start": 427.84, "end": 433.28, "text": " receive any information about the sensory observations. Similarly, how if you close your", "tokens": [50364, 4774, 604, 1589, 466, 264, 27233, 18163, 13, 13157, 11, 577, 498, 291, 1998, 428, 50636], "temperature": 0.0, "avg_logprob": -0.05438727210549747, "compression_ratio": 1.5793991416309012, "no_speech_prob": 3.0718158086529e-05}, {"id": 73, "seek": 42784, "start": 433.28, "end": 439.03999999999996, "text": " eyes and walk around the room, you have a rough idea of where you are located, even though you", "tokens": [50636, 2575, 293, 1792, 926, 264, 1808, 11, 291, 362, 257, 5903, 1558, 295, 689, 291, 366, 6870, 11, 754, 1673, 291, 50924], "temperature": 0.0, "avg_logprob": -0.05438727210549747, "compression_ratio": 1.5793991416309012, "no_speech_prob": 3.0718158086529e-05}, {"id": 74, "seek": 42784, "start": 439.03999999999996, "end": 446.47999999999996, "text": " don't see anything. This is because your brain is able to accumulate self-movement vectors and", "tokens": [50924, 500, 380, 536, 1340, 13, 639, 307, 570, 428, 3567, 307, 1075, 281, 33384, 2698, 12, 76, 1682, 518, 18875, 293, 51296], "temperature": 0.0, "avg_logprob": -0.05438727210549747, "compression_ratio": 1.5793991416309012, "no_speech_prob": 3.0718158086529e-05}, {"id": 75, "seek": 42784, "start": 446.47999999999996, "end": 453.35999999999996, "text": " estimate the position, a process known as path integration. So once the model is trained,", "tokens": [51296, 12539, 264, 2535, 11, 257, 1399, 2570, 382, 3100, 10980, 13, 407, 1564, 264, 2316, 307, 8895, 11, 51640], "temperature": 0.0, "avg_logprob": -0.05438727210549747, "compression_ratio": 1.5793991416309012, "no_speech_prob": 3.0718158086529e-05}, {"id": 76, "seek": 45336, "start": 453.36, "end": 456.96000000000004, "text": " we expect our position module to be able to do the same.", "tokens": [50364, 321, 2066, 527, 2535, 10088, 281, 312, 1075, 281, 360, 264, 912, 13, 50544], "temperature": 0.0, "avg_logprob": -0.07161675180707659, "compression_ratio": 1.59, "no_speech_prob": 1.0616121471684892e-05}, {"id": 77, "seek": 45336, "start": 459.68, "end": 466.0, "text": " Another crucial component is the hippocampus itself, which binds the where information", "tokens": [50680, 3996, 11462, 6542, 307, 264, 27745, 905, 1215, 301, 2564, 11, 597, 41515, 264, 689, 1589, 50996], "temperature": 0.0, "avg_logprob": -0.07161675180707659, "compression_ratio": 1.59, "no_speech_prob": 1.0616121471684892e-05}, {"id": 78, "seek": 45336, "start": 466.0, "end": 472.24, "text": " with what. This binding is effectively forming an association between the two inputs.", "tokens": [50996, 365, 437, 13, 639, 17359, 307, 8659, 15745, 364, 14598, 1296, 264, 732, 15743, 13, 51308], "temperature": 0.0, "avg_logprob": -0.07161675180707659, "compression_ratio": 1.59, "no_speech_prob": 1.0616121471684892e-05}, {"id": 79, "seek": 45336, "start": 473.36, "end": 479.28000000000003, "text": " So we need to add a memory module that would receive the positional information provided", "tokens": [51364, 407, 321, 643, 281, 909, 257, 4675, 10088, 300, 576, 4774, 264, 2535, 304, 1589, 5649, 51660], "temperature": 0.0, "avg_logprob": -0.07161675180707659, "compression_ratio": 1.59, "no_speech_prob": 1.0616121471684892e-05}, {"id": 80, "seek": 47928, "start": 479.28, "end": 485.2, "text": " by the position module together with this stream of sensory inputs and store each", "tokens": [50364, 538, 264, 2535, 10088, 1214, 365, 341, 4309, 295, 27233, 15743, 293, 3531, 1184, 50660], "temperature": 0.0, "avg_logprob": -0.07451267006956501, "compression_ratio": 1.6419213973799127, "no_speech_prob": 8.750279084779322e-05}, {"id": 81, "seek": 47928, "start": 485.2, "end": 491.35999999999996, "text": " encountered combination in memory. Essentially, it memorizes the associations between position and", "tokens": [50660, 20381, 6562, 294, 4675, 13, 23596, 11, 309, 10560, 5660, 264, 26597, 1296, 2535, 293, 50968], "temperature": 0.0, "avg_logprob": -0.07451267006956501, "compression_ratio": 1.6419213973799127, "no_speech_prob": 8.750279084779322e-05}, {"id": 82, "seek": 47928, "start": 491.35999999999996, "end": 499.03999999999996, "text": " observation. I was at X when I saw Y. But storing memories would be useless if we couldn't retrieve", "tokens": [50968, 14816, 13, 286, 390, 412, 1783, 562, 286, 1866, 398, 13, 583, 26085, 8495, 576, 312, 14115, 498, 321, 2809, 380, 30254, 51352], "temperature": 0.0, "avg_logprob": -0.07451267006956501, "compression_ratio": 1.6419213973799127, "no_speech_prob": 8.750279084779322e-05}, {"id": 83, "seek": 47928, "start": 499.03999999999996, "end": 505.59999999999997, "text": " them. Importantly, since this is an associative memory module, it should be able to reconstruct", "tokens": [51352, 552, 13, 26391, 3627, 11, 1670, 341, 307, 364, 4180, 1166, 4675, 10088, 11, 309, 820, 312, 1075, 281, 31499, 51680], "temperature": 0.0, "avg_logprob": -0.07451267006956501, "compression_ratio": 1.6419213973799127, "no_speech_prob": 8.750279084779322e-05}, {"id": 84, "seek": 50560, "start": 505.6, "end": 513.0400000000001, "text": " the full memory from partial information. For example, we could provide it with just the position", "tokens": [50364, 264, 1577, 4675, 490, 14641, 1589, 13, 1171, 1365, 11, 321, 727, 2893, 309, 365, 445, 264, 2535, 50736], "temperature": 0.0, "avg_logprob": -0.06903139556326517, "compression_ratio": 1.75, "no_speech_prob": 0.0002234159765066579}, {"id": 85, "seek": 50560, "start": 513.0400000000001, "end": 518.5600000000001, "text": " and it would go and search in all of the stored memories which observations were accompanied", "tokens": [50736, 293, 309, 576, 352, 293, 3164, 294, 439, 295, 264, 12187, 8495, 597, 18163, 645, 24202, 51012], "temperature": 0.0, "avg_logprob": -0.06903139556326517, "compression_ratio": 1.75, "no_speech_prob": 0.0002234159765066579}, {"id": 86, "seek": 50560, "start": 518.5600000000001, "end": 524.8000000000001, "text": " by this position. So essentially answering the question, what did I see last time I was here?", "tokens": [51012, 538, 341, 2535, 13, 407, 4476, 13430, 264, 1168, 11, 437, 630, 286, 536, 1036, 565, 286, 390, 510, 30, 51324], "temperature": 0.0, "avg_logprob": -0.06903139556326517, "compression_ratio": 1.75, "no_speech_prob": 0.0002234159765066579}, {"id": 87, "seek": 50560, "start": 525.52, "end": 531.2, "text": " And similarly, we could provide it with just the sensory observation and it would retrieve position.", "tokens": [51360, 400, 14138, 11, 321, 727, 2893, 309, 365, 445, 264, 27233, 14816, 293, 309, 576, 30254, 2535, 13, 51644], "temperature": 0.0, "avg_logprob": -0.06903139556326517, "compression_ratio": 1.75, "no_speech_prob": 0.0002234159765066579}, {"id": 88, "seek": 53120, "start": 531.2, "end": 538.8000000000001, "text": " Where was I last time I saw this? Now we have all the necessary components", "tokens": [50364, 2305, 390, 286, 1036, 565, 286, 1866, 341, 30, 823, 321, 362, 439, 264, 4818, 6677, 50744], "temperature": 0.0, "avg_logprob": -0.10997764799329969, "compression_ratio": 1.5183673469387755, "no_speech_prob": 5.225220957072452e-05}, {"id": 89, "seek": 53120, "start": 538.8000000000001, "end": 545.6, "text": " to solve the prediction problem. Let's walk step by step on what the trained model will do to come", "tokens": [50744, 281, 5039, 264, 17630, 1154, 13, 961, 311, 1792, 1823, 538, 1823, 322, 437, 264, 8895, 2316, 486, 360, 281, 808, 51084], "temperature": 0.0, "avg_logprob": -0.10997764799329969, "compression_ratio": 1.5183673469387755, "no_speech_prob": 5.225220957072452e-05}, {"id": 90, "seek": 53120, "start": 545.6, "end": 551.5200000000001, "text": " up with a successful prediction when walking, for example, on a family tree. Remember, it should be", "tokens": [51084, 493, 365, 257, 4406, 17630, 562, 4494, 11, 337, 1365, 11, 322, 257, 1605, 4230, 13, 5459, 11, 309, 820, 312, 51380], "temperature": 0.0, "avg_logprob": -0.10997764799329969, "compression_ratio": 1.5183673469387755, "no_speech_prob": 5.225220957072452e-05}, {"id": 91, "seek": 53120, "start": 551.5200000000001, "end": 558.72, "text": " capable of learning any type of structure, not just the four connected grids. So we start on John,", "tokens": [51380, 8189, 295, 2539, 604, 2010, 295, 3877, 11, 406, 445, 264, 1451, 4582, 677, 3742, 13, 407, 321, 722, 322, 2619, 11, 51740], "temperature": 0.0, "avg_logprob": -0.10997764799329969, "compression_ratio": 1.5183673469387755, "no_speech_prob": 5.225220957072452e-05}, {"id": 92, "seek": 55872, "start": 558.72, "end": 564.72, "text": " transition to Mary via a sister action, and then to Kate via a daughter action.", "tokens": [50364, 6034, 281, 6059, 5766, 257, 4892, 3069, 11, 293, 550, 281, 16251, 5766, 257, 4653, 3069, 13, 50664], "temperature": 0.0, "avg_logprob": -0.0976719913712467, "compression_ratio": 1.6475770925110131, "no_speech_prob": 0.00012339414388407022}, {"id": 93, "seek": 55872, "start": 565.36, "end": 572.32, "text": " Finally, we give the model the action labeled as Uncle and ask it to make a prediction and what's", "tokens": [50696, 6288, 11, 321, 976, 264, 2316, 264, 3069, 21335, 382, 12347, 293, 1029, 309, 281, 652, 257, 17630, 293, 437, 311, 51044], "temperature": 0.0, "avg_logprob": -0.0976719913712467, "compression_ratio": 1.6475770925110131, "no_speech_prob": 0.00012339414388407022}, {"id": 94, "seek": 55872, "start": 572.32, "end": 578.5600000000001, "text": " happening under the hood is the following. At first, the position module has some initial belief", "tokens": [51044, 2737, 833, 264, 13376, 307, 264, 3480, 13, 1711, 700, 11, 264, 2535, 10088, 575, 512, 5883, 7107, 51356], "temperature": 0.0, "avg_logprob": -0.0976719913712467, "compression_ratio": 1.6475770925110131, "no_speech_prob": 0.00012339414388407022}, {"id": 95, "seek": 55872, "start": 578.5600000000001, "end": 584.72, "text": " about the current location which is combined with John and this combination is stored in the memory", "tokens": [51356, 466, 264, 2190, 4914, 597, 307, 9354, 365, 2619, 293, 341, 6562, 307, 12187, 294, 264, 4675, 51664], "temperature": 0.0, "avg_logprob": -0.0976719913712467, "compression_ratio": 1.6475770925110131, "no_speech_prob": 0.00012339414388407022}, {"id": 96, "seek": 58472, "start": 584.72, "end": 592.0, "text": " module. Next, the sister action is fed into the position module which comes up with a new", "tokens": [50364, 10088, 13, 3087, 11, 264, 4892, 3069, 307, 4636, 666, 264, 2535, 10088, 597, 1487, 493, 365, 257, 777, 50728], "temperature": 0.0, "avg_logprob": -0.05672725817052329, "compression_ratio": 1.8495145631067962, "no_speech_prob": 0.0014103470603004098}, {"id": 97, "seek": 58472, "start": 592.0, "end": 598.72, "text": " belief about location that is combined with Mary and the corresponding conjunction is stored in", "tokens": [50728, 7107, 466, 4914, 300, 307, 9354, 365, 6059, 293, 264, 11760, 27482, 307, 12187, 294, 51064], "temperature": 0.0, "avg_logprob": -0.05672725817052329, "compression_ratio": 1.8495145631067962, "no_speech_prob": 0.0014103470603004098}, {"id": 98, "seek": 58472, "start": 598.72, "end": 606.1600000000001, "text": " memory. Similarly, daughter action is used to update the internal state of the position module", "tokens": [51064, 4675, 13, 13157, 11, 4653, 3069, 307, 1143, 281, 5623, 264, 6920, 1785, 295, 264, 2535, 10088, 51436], "temperature": 0.0, "avg_logprob": -0.05672725817052329, "compression_ratio": 1.8495145631067962, "no_speech_prob": 0.0014103470603004098}, {"id": 99, "seek": 58472, "start": 606.1600000000001, "end": 613.6, "text": " which is combined with Kate and sent to the memory module. Finally, the uncle action is fed into the", "tokens": [51436, 597, 307, 9354, 365, 16251, 293, 2279, 281, 264, 4675, 10088, 13, 6288, 11, 264, 9153, 3069, 307, 4636, 666, 264, 51808], "temperature": 0.0, "avg_logprob": -0.05672725817052329, "compression_ratio": 1.8495145631067962, "no_speech_prob": 0.0014103470603004098}, {"id": 100, "seek": 61360, "start": 613.6, "end": 619.6800000000001, "text": " position module. Importantly, the resulting positional information, the pattern of neuron", "tokens": [50364, 2535, 10088, 13, 26391, 3627, 11, 264, 16505, 2535, 304, 1589, 11, 264, 5102, 295, 34090, 50668], "temperature": 0.0, "avg_logprob": -0.07174287285915641, "compression_ratio": 1.7155555555555555, "no_speech_prob": 0.00019411263929214329}, {"id": 101, "seek": 61360, "start": 619.6800000000001, "end": 625.84, "text": " activations is the same as the one we started with. This is because after the model is trained on many", "tokens": [50668, 2430, 763, 307, 264, 912, 382, 264, 472, 321, 1409, 365, 13, 639, 307, 570, 934, 264, 2316, 307, 8895, 322, 867, 50976], "temperature": 0.0, "avg_logprob": -0.07174287285915641, "compression_ratio": 1.7155555555555555, "no_speech_prob": 0.00019411263929214329}, {"id": 102, "seek": 61360, "start": 625.84, "end": 632.96, "text": " family trees underlaying by the same rules, the position module is configured to always return", "tokens": [50976, 1605, 5852, 833, 8376, 278, 538, 264, 912, 4474, 11, 264, 2535, 10088, 307, 30538, 281, 1009, 2736, 51332], "temperature": 0.0, "avg_logprob": -0.07174287285915641, "compression_ratio": 1.7155555555555555, "no_speech_prob": 0.00019411263929214329}, {"id": 103, "seek": 61360, "start": 632.96, "end": 639.84, "text": " to the same position when we make loops like these. In other words, the general laws governing the", "tokens": [51332, 281, 264, 912, 2535, 562, 321, 652, 16121, 411, 613, 13, 682, 661, 2283, 11, 264, 2674, 6064, 30054, 264, 51676], "temperature": 0.0, "avg_logprob": -0.07174287285915641, "compression_ratio": 1.7155555555555555, "no_speech_prob": 0.00019411263929214329}, {"id": 104, "seek": 63984, "start": 639.84, "end": 646.32, "text": " transition logic on the world graph become embedded into the rules of how the position module", "tokens": [50364, 6034, 9952, 322, 264, 1002, 4295, 1813, 16741, 666, 264, 4474, 295, 577, 264, 2535, 10088, 50688], "temperature": 0.0, "avg_logprob": -0.06915711133908002, "compression_ratio": 1.7043478260869565, "no_speech_prob": 1.3631336514663417e-05}, {"id": 105, "seek": 63984, "start": 646.32, "end": 653.2800000000001, "text": " updates its state. After performing path integration correctly, we return to this starting position", "tokens": [50688, 9205, 1080, 1785, 13, 2381, 10205, 3100, 10980, 8944, 11, 321, 2736, 281, 341, 2891, 2535, 51036], "temperature": 0.0, "avg_logprob": -0.06915711133908002, "compression_ratio": 1.7043478260869565, "no_speech_prob": 1.3631336514663417e-05}, {"id": 106, "seek": 63984, "start": 654.24, "end": 660.32, "text": " but there is no corresponding sensory observation to memorize. Instead, because the model has reached", "tokens": [51084, 457, 456, 307, 572, 11760, 27233, 14816, 281, 27478, 13, 7156, 11, 570, 264, 2316, 575, 6488, 51388], "temperature": 0.0, "avg_logprob": -0.06915711133908002, "compression_ratio": 1.7043478260869565, "no_speech_prob": 1.3631336514663417e-05}, {"id": 107, "seek": 63984, "start": 660.32, "end": 666.64, "text": " the end of the sequence, it tries to predict the next observation but it has the path integrated", "tokens": [51388, 264, 917, 295, 264, 8310, 11, 309, 9898, 281, 6069, 264, 958, 14816, 457, 309, 575, 264, 3100, 10919, 51704], "temperature": 0.0, "avg_logprob": -0.06915711133908002, "compression_ratio": 1.7043478260869565, "no_speech_prob": 1.3631336514663417e-05}, {"id": 108, "seek": 66664, "start": 666.64, "end": 673.76, "text": " position to guide this prediction. So it queries the memory module with the positional information", "tokens": [50364, 2535, 281, 5934, 341, 17630, 13, 407, 309, 24109, 264, 4675, 10088, 365, 264, 2535, 304, 1589, 50720], "temperature": 0.0, "avg_logprob": -0.0909382359365399, "compression_ratio": 1.6032388663967612, "no_speech_prob": 9.461234003538266e-05}, {"id": 109, "seek": 66664, "start": 673.76, "end": 680.16, "text": " and retrieves a sensory observation corresponding to this particular position which in our case", "tokens": [50720, 293, 19817, 977, 257, 27233, 14816, 11760, 281, 341, 1729, 2535, 597, 294, 527, 1389, 51040], "temperature": 0.0, "avg_logprob": -0.0909382359365399, "compression_ratio": 1.6032388663967612, "no_speech_prob": 9.461234003538266e-05}, {"id": 110, "seek": 66664, "start": 680.16, "end": 687.12, "text": " is John. Awesome, right? So far we have just been theorizing about this spherical model in a vacuum", "tokens": [51040, 307, 2619, 13, 10391, 11, 558, 30, 407, 1400, 321, 362, 445, 668, 27423, 3319, 466, 341, 37300, 2316, 294, 257, 14224, 51388], "temperature": 0.0, "avg_logprob": -0.0909382359365399, "compression_ratio": 1.6032388663967612, "no_speech_prob": 9.461234003538266e-05}, {"id": 111, "seek": 66664, "start": 687.76, "end": 695.2, "text": " but does it actually, well, work? And if so, what does it tell us about our own navigational systems?", "tokens": [51420, 457, 775, 309, 767, 11, 731, 11, 589, 30, 400, 498, 370, 11, 437, 775, 309, 980, 505, 466, 527, 1065, 7407, 1478, 3652, 30, 51792], "temperature": 0.0, "avg_logprob": -0.0909382359365399, "compression_ratio": 1.6032388663967612, "no_speech_prob": 9.461234003538266e-05}, {"id": 112, "seek": 69520, "start": 695.2, "end": 702.4000000000001, "text": " The most direct way to assess how well the model is performing is to look at its accuracy which is", "tokens": [50364, 440, 881, 2047, 636, 281, 5877, 577, 731, 264, 2316, 307, 10205, 307, 281, 574, 412, 1080, 14170, 597, 307, 50724], "temperature": 0.0, "avg_logprob": -0.09447260790093001, "compression_ratio": 1.6213991769547325, "no_speech_prob": 5.307499668560922e-05}, {"id": 113, "seek": 69520, "start": 702.4000000000001, "end": 708.4000000000001, "text": " just the percentage of predictions it made correctly and importantly look at how quickly the accuracy", "tokens": [50724, 445, 264, 9668, 295, 21264, 309, 1027, 8944, 293, 8906, 574, 412, 577, 2661, 264, 14170, 51024], "temperature": 0.0, "avg_logprob": -0.09447260790093001, "compression_ratio": 1.6213991769547325, "no_speech_prob": 5.307499668560922e-05}, {"id": 114, "seek": 69520, "start": 708.4000000000001, "end": 714.24, "text": " grows. Here is what I mean. Imagine for a moment that instead of this fancy machine we had just", "tokens": [51024, 13156, 13, 1692, 307, 437, 286, 914, 13, 11739, 337, 257, 1623, 300, 2602, 295, 341, 10247, 3479, 321, 632, 445, 51316], "temperature": 0.0, "avg_logprob": -0.09447260790093001, "compression_ratio": 1.6213991769547325, "no_speech_prob": 5.307499668560922e-05}, {"id": 115, "seek": 69520, "start": 714.24, "end": 720.4000000000001, "text": " a good old lookup table which simply memorizes all the transitions as pairs. Previous observation", "tokens": [51316, 257, 665, 1331, 574, 1010, 3199, 597, 2935, 10560, 5660, 439, 264, 23767, 382, 15494, 13, 6001, 1502, 14816, 51624], "temperature": 0.0, "avg_logprob": -0.09447260790093001, "compression_ratio": 1.6213991769547325, "no_speech_prob": 5.307499668560922e-05}, {"id": 116, "seek": 72040, "start": 720.4, "end": 727.4399999999999, "text": " plus action equals new observation. So it would store memories like John plus sister equals Mary,", "tokens": [50364, 1804, 3069, 6915, 777, 14816, 13, 407, 309, 576, 3531, 8495, 411, 2619, 1804, 4892, 6915, 6059, 11, 50716], "temperature": 0.0, "avg_logprob": -0.0687981356273998, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.0017546016024425626}, {"id": 117, "seek": 72040, "start": 727.4399999999999, "end": 733.84, "text": " Mary plus daughter equals Kate, etc. And to predict the next observation it would simply scan the", "tokens": [50716, 6059, 1804, 4653, 6915, 16251, 11, 5183, 13, 400, 281, 6069, 264, 958, 14816, 309, 576, 2935, 11049, 264, 51036], "temperature": 0.0, "avg_logprob": -0.0687981356273998, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.0017546016024425626}, {"id": 118, "seek": 72040, "start": 733.84, "end": 740.0, "text": " lookup table and search for a particular combination. In the case of our family tree example, on first", "tokens": [51036, 574, 1010, 3199, 293, 3164, 337, 257, 1729, 6562, 13, 682, 264, 1389, 295, 527, 1605, 4230, 1365, 11, 322, 700, 51344], "temperature": 0.0, "avg_logprob": -0.0687981356273998, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.0017546016024425626}, {"id": 119, "seek": 72040, "start": 740.0, "end": 746.3199999999999, "text": " try it would not be able to predict that Kate's uncle is John because it hadn't encountered this", "tokens": [51344, 853, 309, 576, 406, 312, 1075, 281, 6069, 300, 16251, 311, 9153, 307, 2619, 570, 309, 8782, 380, 20381, 341, 51660], "temperature": 0.0, "avg_logprob": -0.0687981356273998, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.0017546016024425626}, {"id": 120, "seek": 74632, "start": 746.32, "end": 753.84, "text": " particular combination previously. In other words, to reach 100% accuracy it would need to first", "tokens": [50364, 1729, 6562, 8046, 13, 682, 661, 2283, 11, 281, 2524, 2319, 4, 14170, 309, 576, 643, 281, 700, 50740], "temperature": 0.0, "avg_logprob": -0.12077817572168557, "compression_ratio": 1.594142259414226, "no_speech_prob": 9.761549881659448e-05}, {"id": 121, "seek": 74632, "start": 753.84, "end": 759.84, "text": " encounter all possible combinations of observations and actions and this means that the performance", "tokens": [50740, 8593, 439, 1944, 21267, 295, 18163, 293, 5909, 293, 341, 1355, 300, 264, 3389, 51040], "temperature": 0.0, "avg_logprob": -0.12077817572168557, "compression_ratio": 1.594142259414226, "no_speech_prob": 9.761549881659448e-05}, {"id": 122, "seek": 74632, "start": 759.84, "end": 767.2, "text": " of the model depends on the number of edges of this graph that it visited. In contrast,", "tokens": [51040, 295, 264, 2316, 5946, 322, 264, 1230, 295, 8819, 295, 341, 4295, 300, 309, 11220, 13, 682, 8712, 11, 51408], "temperature": 0.0, "avg_logprob": -0.12077817572168557, "compression_ratio": 1.594142259414226, "no_speech_prob": 9.761549881659448e-05}, {"id": 123, "seek": 74632, "start": 767.2, "end": 774.08, "text": " tolman eigenbau machine or TEM doesn't need to be explicitly told at the outcome of every action", "tokens": [51408, 281, 75, 1601, 10446, 26820, 3479, 420, 314, 6683, 1177, 380, 643, 281, 312, 20803, 1907, 412, 264, 9700, 295, 633, 3069, 51752], "temperature": 0.0, "avg_logprob": -0.12077817572168557, "compression_ratio": 1.594142259414226, "no_speech_prob": 9.761549881659448e-05}, {"id": 124, "seek": 77408, "start": 774.08, "end": 779.84, "text": " from every node because it has the notion of a structure. For example, if I tell you that Kate", "tokens": [50364, 490, 633, 9984, 570, 309, 575, 264, 10710, 295, 257, 3877, 13, 1171, 1365, 11, 498, 286, 980, 291, 300, 16251, 50652], "temperature": 0.0, "avg_logprob": -0.0478008644921439, "compression_ratio": 1.5601659751037344, "no_speech_prob": 0.0003199972852598876}, {"id": 125, "seek": 77408, "start": 779.84, "end": 785.36, "text": " is Mary's daughter that is enough for you to infer the rest of the relationships automatically.", "tokens": [50652, 307, 6059, 311, 4653, 300, 307, 1547, 337, 291, 281, 13596, 264, 1472, 295, 264, 6159, 6772, 13, 50928], "temperature": 0.0, "avg_logprob": -0.0478008644921439, "compression_ratio": 1.5601659751037344, "no_speech_prob": 0.0003199972852598876}, {"id": 126, "seek": 77408, "start": 786.1600000000001, "end": 792.8000000000001, "text": " And this essentially means that to reach 100% accuracy it is enough for TEM to just visit", "tokens": [50968, 400, 341, 4476, 1355, 300, 281, 2524, 2319, 4, 14170, 309, 307, 1547, 337, 314, 6683, 281, 445, 3441, 51300], "temperature": 0.0, "avg_logprob": -0.0478008644921439, "compression_ratio": 1.5601659751037344, "no_speech_prob": 0.0003199972852598876}, {"id": 127, "seek": 77408, "start": 792.8000000000001, "end": 798.6400000000001, "text": " all the nodes instead of all possible edges and hence its performance depends on the proportion", "tokens": [51300, 439, 264, 13891, 2602, 295, 439, 1944, 8819, 293, 16678, 1080, 3389, 5946, 322, 264, 16068, 51592], "temperature": 0.0, "avg_logprob": -0.0478008644921439, "compression_ratio": 1.5601659751037344, "no_speech_prob": 0.0003199972852598876}, {"id": 128, "seek": 79864, "start": 798.64, "end": 805.52, "text": " of nodes visited which grows much faster than the proportion of edges. So our machine seems to", "tokens": [50364, 295, 13891, 11220, 597, 13156, 709, 4663, 813, 264, 16068, 295, 8819, 13, 407, 527, 3479, 2544, 281, 50708], "temperature": 0.0, "avg_logprob": -0.09004606442017989, "compression_ratio": 1.5673469387755101, "no_speech_prob": 7.722187729086727e-05}, {"id": 129, "seek": 79864, "start": 805.52, "end": 811.84, "text": " indeed construct a representation of the world. Hooray! But what's going on inside of its brain,", "tokens": [50708, 6451, 7690, 257, 10290, 295, 264, 1002, 13, 3631, 284, 320, 0, 583, 437, 311, 516, 322, 1854, 295, 1080, 3567, 11, 51024], "temperature": 0.0, "avg_logprob": -0.09004606442017989, "compression_ratio": 1.5673469387755101, "no_speech_prob": 7.722187729086727e-05}, {"id": 130, "seek": 79864, "start": 811.84, "end": 818.64, "text": " so to speak? Let's look inside the position module first. Remember, the belief about current", "tokens": [51024, 370, 281, 1710, 30, 961, 311, 574, 1854, 264, 2535, 10088, 700, 13, 5459, 11, 264, 7107, 466, 2190, 51364], "temperature": 0.0, "avg_logprob": -0.09004606442017989, "compression_ratio": 1.5673469387755101, "no_speech_prob": 7.722187729086727e-05}, {"id": 131, "seek": 79864, "start": 818.64, "end": 824.96, "text": " location is encoded with a pattern of collective activation of neurons. But we can also interrogate", "tokens": [51364, 4914, 307, 2058, 12340, 365, 257, 5102, 295, 12590, 24433, 295, 22027, 13, 583, 321, 393, 611, 24871, 473, 51680], "temperature": 0.0, "avg_logprob": -0.09004606442017989, "compression_ratio": 1.5673469387755101, "no_speech_prob": 7.722187729086727e-05}, {"id": 132, "seek": 82496, "start": 824.96, "end": 831.0400000000001, "text": " individual neurons and look at what each one of them is doing as the agent randomly walks around.", "tokens": [50364, 2609, 22027, 293, 574, 412, 437, 1184, 472, 295, 552, 307, 884, 382, 264, 9461, 16979, 12896, 926, 13, 50668], "temperature": 0.0, "avg_logprob": -0.09952911340965415, "compression_ratio": 1.6097560975609757, "no_speech_prob": 2.668851266207639e-05}, {"id": 133, "seek": 82496, "start": 831.0400000000001, "end": 835.6800000000001, "text": " Here for the sake of visualization, I'm going to show results after the model was trained on", "tokens": [50668, 1692, 337, 264, 9717, 295, 25801, 11, 286, 478, 516, 281, 855, 3542, 934, 264, 2316, 390, 8895, 322, 50900], "temperature": 0.0, "avg_logprob": -0.09952911340965415, "compression_ratio": 1.6097560975609757, "no_speech_prob": 2.668851266207639e-05}, {"id": 134, "seek": 82496, "start": 835.6800000000001, "end": 841.6, "text": " regular four-connected grids analogs of physical 2D space rather than social hierarchies.", "tokens": [50900, 3890, 1451, 12, 9826, 292, 677, 3742, 16660, 82, 295, 4001, 568, 35, 1901, 2831, 813, 2093, 35250, 530, 13, 51196], "temperature": 0.0, "avg_logprob": -0.09952911340965415, "compression_ratio": 1.6097560975609757, "no_speech_prob": 2.668851266207639e-05}, {"id": 135, "seek": 82496, "start": 842.8000000000001, "end": 848.5600000000001, "text": " Remarkably, we see that individual units in the position module develop periodic activity", "tokens": [51256, 4080, 809, 1188, 11, 321, 536, 300, 2609, 6815, 294, 264, 2535, 10088, 1499, 27790, 5191, 51544], "temperature": 0.0, "avg_logprob": -0.09952911340965415, "compression_ratio": 1.6097560975609757, "no_speech_prob": 2.668851266207639e-05}, {"id": 136, "seek": 82496, "start": 848.5600000000001, "end": 854.4000000000001, "text": " patterns as a function of position. They tile the space with the regular hexagonal grids of", "tokens": [51544, 8294, 382, 257, 2445, 295, 2535, 13, 814, 20590, 264, 1901, 365, 264, 3890, 23291, 6709, 304, 677, 3742, 295, 51836], "temperature": 0.0, "avg_logprob": -0.09952911340965415, "compression_ratio": 1.6097560975609757, "no_speech_prob": 2.668851266207639e-05}, {"id": 137, "seek": 85440, "start": 854.48, "end": 862.72, "text": " different size or these periodic stripes exactly like grid cells and band cells of the entorhinal", "tokens": [50368, 819, 2744, 420, 613, 27790, 27308, 2293, 411, 10748, 5438, 293, 4116, 5438, 295, 264, 948, 284, 71, 2071, 50780], "temperature": 0.0, "avg_logprob": -0.1016087592402591, "compression_ratio": 1.5738396624472575, "no_speech_prob": 2.111243156832643e-05}, {"id": 138, "seek": 85440, "start": 862.72, "end": 870.0799999999999, "text": " cortex encode position in mammalian brains. And the selectivity of individual units is", "tokens": [50780, 33312, 2058, 1429, 2535, 294, 49312, 952, 15442, 13, 400, 264, 3048, 4253, 295, 2609, 6815, 307, 51148], "temperature": 0.0, "avg_logprob": -0.1016087592402591, "compression_ratio": 1.5738396624472575, "no_speech_prob": 2.111243156832643e-05}, {"id": 139, "seek": 85440, "start": 870.0799999999999, "end": 877.36, "text": " preserved across environments, suggesting that they indeed can generalize. Neurons in the memory", "tokens": [51148, 22242, 2108, 12388, 11, 18094, 300, 436, 6451, 393, 2674, 1125, 13, 1734, 374, 892, 294, 264, 4675, 51512], "temperature": 0.0, "avg_logprob": -0.1016087592402591, "compression_ratio": 1.5738396624472575, "no_speech_prob": 2.111243156832643e-05}, {"id": 140, "seek": 85440, "start": 877.36, "end": 883.04, "text": " module do something different. Since they form a conjunction between positional and sensory", "tokens": [51512, 10088, 360, 746, 819, 13, 4162, 436, 1254, 257, 27482, 1296, 2535, 304, 293, 27233, 51796], "temperature": 0.0, "avg_logprob": -0.1016087592402591, "compression_ratio": 1.5738396624472575, "no_speech_prob": 2.111243156832643e-05}, {"id": 141, "seek": 88304, "start": 883.04, "end": 889.04, "text": " information, each neuron would be active when both of the two upstream components are active.", "tokens": [50364, 1589, 11, 1184, 34090, 576, 312, 4967, 562, 1293, 295, 264, 732, 33915, 6677, 366, 4967, 13, 50664], "temperature": 0.0, "avg_logprob": -0.05992454886436462, "compression_ratio": 1.6077586206896552, "no_speech_prob": 4.469392661121674e-05}, {"id": 142, "seek": 88304, "start": 890.0799999999999, "end": 895.92, "text": " Indeed, units in the memory module resemble hippocampal place cells of various size,", "tokens": [50716, 15061, 11, 6815, 294, 264, 4675, 10088, 36870, 27745, 905, 1215, 304, 1081, 5438, 295, 3683, 2744, 11, 51008], "temperature": 0.0, "avg_logprob": -0.05992454886436462, "compression_ratio": 1.6077586206896552, "no_speech_prob": 4.469392661121674e-05}, {"id": 143, "seek": 88304, "start": 895.92, "end": 902.56, "text": " which fire in a particular patch of space. Importantly, just like hippocampal representations", "tokens": [51008, 597, 2610, 294, 257, 1729, 9972, 295, 1901, 13, 26391, 3627, 11, 445, 411, 27745, 905, 1215, 304, 33358, 51340], "temperature": 0.0, "avg_logprob": -0.05992454886436462, "compression_ratio": 1.6077586206896552, "no_speech_prob": 4.469392661121674e-05}, {"id": 144, "seek": 88304, "start": 902.56, "end": 909.52, "text": " in real brains, they are firing patterns differ across environments, since the incoming observations", "tokens": [51340, 294, 957, 15442, 11, 436, 366, 16045, 8294, 743, 2108, 12388, 11, 1670, 264, 22341, 18163, 51688], "temperature": 0.0, "avg_logprob": -0.05992454886436462, "compression_ratio": 1.6077586206896552, "no_speech_prob": 4.469392661121674e-05}, {"id": 145, "seek": 90952, "start": 909.52, "end": 916.8, "text": " are different. This is known as hippocampal remapping. I'd like to emphasize that such grid-like", "tokens": [50364, 366, 819, 13, 639, 307, 2570, 382, 27745, 905, 1215, 304, 890, 10534, 13, 286, 1116, 411, 281, 16078, 300, 1270, 10748, 12, 4092, 50728], "temperature": 0.0, "avg_logprob": -0.0650964569259476, "compression_ratio": 1.5708502024291497, "no_speech_prob": 0.00011774421000154689}, {"id": 146, "seek": 90952, "start": 916.8, "end": 923.6, "text": " and place-like representations were never hard-coded into the model. We started with essentially a", "tokens": [50728, 293, 1081, 12, 4092, 33358, 645, 1128, 1152, 12, 66, 12340, 666, 264, 2316, 13, 492, 1409, 365, 4476, 257, 51068], "temperature": 0.0, "avg_logprob": -0.0650964569259476, "compression_ratio": 1.5708502024291497, "no_speech_prob": 0.00011774421000154689}, {"id": 147, "seek": 90952, "start": 923.6, "end": 928.64, "text": " random set of parameters, let the model optimize itself to come up with the best solution to the", "tokens": [51068, 4974, 992, 295, 9834, 11, 718, 264, 2316, 19719, 2564, 281, 808, 493, 365, 264, 1151, 3827, 281, 264, 51320], "temperature": 0.0, "avg_logprob": -0.0650964569259476, "compression_ratio": 1.5708502024291497, "no_speech_prob": 0.00011774421000154689}, {"id": 148, "seek": 90952, "start": 928.64, "end": 935.4399999999999, "text": " prediction problem, and those responses just emerged naturally. So far, we've trained the model", "tokens": [51320, 17630, 1154, 11, 293, 729, 13019, 445, 20178, 8195, 13, 407, 1400, 11, 321, 600, 8895, 264, 2316, 51660], "temperature": 0.0, "avg_logprob": -0.0650964569259476, "compression_ratio": 1.5708502024291497, "no_speech_prob": 0.00011774421000154689}, {"id": 149, "seek": 93544, "start": 935.44, "end": 941.12, "text": " on sequences that were generated from random walks in a given environment, which means that", "tokens": [50364, 322, 22978, 300, 645, 10833, 490, 4974, 12896, 294, 257, 2212, 2823, 11, 597, 1355, 300, 50648], "temperature": 0.0, "avg_logprob": -0.06264702479044597, "compression_ratio": 1.5738396624472575, "no_speech_prob": 8.888055890565738e-05}, {"id": 150, "seek": 93544, "start": 941.12, "end": 947.12, "text": " all the observations were equally likely. But in the real life, animals don't really move by", "tokens": [50648, 439, 264, 18163, 645, 12309, 3700, 13, 583, 294, 264, 957, 993, 11, 4882, 500, 380, 534, 1286, 538, 50948], "temperature": 0.0, "avg_logprob": -0.06264702479044597, "compression_ratio": 1.5738396624472575, "no_speech_prob": 8.888055890565738e-05}, {"id": 151, "seek": 93544, "start": 947.12, "end": 953.6, "text": " diffusion. They are biased towards rewards and exploring objects. They like being near walls", "tokens": [50948, 25242, 13, 814, 366, 28035, 3030, 17203, 293, 12736, 6565, 13, 814, 411, 885, 2651, 7920, 51272], "temperature": 0.0, "avg_logprob": -0.06264702479044597, "compression_ratio": 1.5738396624472575, "no_speech_prob": 8.888055890565738e-05}, {"id": 152, "seek": 93544, "start": 953.6, "end": 960.08, "text": " because it feels safe and avoid open spaces. So the question is, if we change the statistics of", "tokens": [51272, 570, 309, 3417, 3273, 293, 5042, 1269, 7673, 13, 407, 264, 1168, 307, 11, 498, 321, 1319, 264, 12523, 295, 51596], "temperature": 0.0, "avg_logprob": -0.06264702479044597, "compression_ratio": 1.5738396624472575, "no_speech_prob": 8.888055890565738e-05}, {"id": 153, "seek": 96008, "start": 960.08, "end": 966.1600000000001, "text": " the sensory observations so that some stimuli are more common than others, would it affect the", "tokens": [50364, 264, 27233, 18163, 370, 300, 512, 47752, 366, 544, 2689, 813, 2357, 11, 576, 309, 3345, 264, 50668], "temperature": 0.0, "avg_logprob": -0.04543185535865494, "compression_ratio": 1.6092436974789917, "no_speech_prob": 7.602473488077521e-05}, {"id": 154, "seek": 96008, "start": 966.1600000000001, "end": 971.5200000000001, "text": " representations that emerge in our model as the optimal solution to the prediction problem?", "tokens": [50668, 33358, 300, 21511, 294, 527, 2316, 382, 264, 16252, 3827, 281, 264, 17630, 1154, 30, 50936], "temperature": 0.0, "avg_logprob": -0.04543185535865494, "compression_ratio": 1.6092436974789917, "no_speech_prob": 7.602473488077521e-05}, {"id": 155, "seek": 96008, "start": 972.32, "end": 978.08, "text": " For example, let's train TEM on sequences of observations that mimic the behavior of a real", "tokens": [50976, 1171, 1365, 11, 718, 311, 3847, 314, 6683, 322, 22978, 295, 18163, 300, 31075, 264, 5223, 295, 257, 957, 51264], "temperature": 0.0, "avg_logprob": -0.04543185535865494, "compression_ratio": 1.6092436974789917, "no_speech_prob": 7.602473488077521e-05}, {"id": 156, "seek": 96008, "start": 978.08, "end": 985.5200000000001, "text": " mouse, which prefers to spend time near boundaries and approaches objects. In this case, representations", "tokens": [51264, 9719, 11, 597, 44334, 281, 3496, 565, 2651, 13180, 293, 11587, 6565, 13, 682, 341, 1389, 11, 33358, 51636], "temperature": 0.0, "avg_logprob": -0.04543185535865494, "compression_ratio": 1.6092436974789917, "no_speech_prob": 7.602473488077521e-05}, {"id": 157, "seek": 98552, "start": 985.52, "end": 991.76, "text": " that emerge in the position module now include boundary cells, which are selective to borders of", "tokens": [50364, 300, 21511, 294, 264, 2535, 10088, 586, 4090, 12866, 5438, 11, 597, 366, 33930, 281, 16287, 295, 50676], "temperature": 0.0, "avg_logprob": -0.0903464464040903, "compression_ratio": 1.6217391304347826, "no_speech_prob": 0.0002378216595388949}, {"id": 158, "seek": 98552, "start": 991.76, "end": 998.0799999999999, "text": " the world, and object vector cells that seem to activate whenever the animal is at a certain", "tokens": [50676, 264, 1002, 11, 293, 2657, 8062, 5438, 300, 1643, 281, 13615, 5699, 264, 5496, 307, 412, 257, 1629, 50992], "temperature": 0.0, "avg_logprob": -0.0903464464040903, "compression_ratio": 1.6217391304347826, "no_speech_prob": 0.0002378216595388949}, {"id": 159, "seek": 98552, "start": 998.0799999999999, "end": 1004.48, "text": " distance and certain direction away from any object. Both of these types of responses,", "tokens": [50992, 4560, 293, 1629, 3513, 1314, 490, 604, 2657, 13, 6767, 295, 613, 3467, 295, 13019, 11, 51312], "temperature": 0.0, "avg_logprob": -0.0903464464040903, "compression_ratio": 1.6217391304347826, "no_speech_prob": 0.0002378216595388949}, {"id": 160, "seek": 98552, "start": 1004.48, "end": 1010.56, "text": " that by the way also generalize across contexts, are observed experimentally when recording from", "tokens": [51312, 300, 538, 264, 636, 611, 2674, 1125, 2108, 30628, 11, 366, 13095, 5120, 379, 562, 6613, 490, 51616], "temperature": 0.0, "avg_logprob": -0.0903464464040903, "compression_ratio": 1.6217391304347826, "no_speech_prob": 0.0002378216595388949}, {"id": 161, "seek": 101056, "start": 1010.56, "end": 1016.3199999999999, "text": " entorhinal cortex, while some neurons in the memory module develop selectivity to particular", "tokens": [50364, 948, 284, 71, 2071, 33312, 11, 1339, 512, 22027, 294, 264, 4675, 10088, 1499, 3048, 4253, 281, 1729, 50652], "temperature": 0.0, "avg_logprob": -0.07481007295496324, "compression_ratio": 1.6008403361344539, "no_speech_prob": 5.225201675784774e-05}, {"id": 162, "seek": 101056, "start": 1016.3199999999999, "end": 1023.92, "text": " objects, resembling landmark cells of the hippocampus. If we take a more complex sequence,", "tokens": [50652, 6565, 11, 20695, 1688, 26962, 5438, 295, 264, 27745, 905, 1215, 301, 13, 759, 321, 747, 257, 544, 3997, 8310, 11, 51032], "temperature": 0.0, "avg_logprob": -0.07481007295496324, "compression_ratio": 1.6008403361344539, "no_speech_prob": 5.225201675784774e-05}, {"id": 163, "seek": 101056, "start": 1023.92, "end": 1029.36, "text": " such as the one mimicking the animal that is performing an alternation task, the model successfully", "tokens": [51032, 1270, 382, 264, 472, 12247, 10401, 264, 5496, 300, 307, 10205, 364, 5400, 399, 5633, 11, 264, 2316, 10727, 51304], "temperature": 0.0, "avg_logprob": -0.07481007295496324, "compression_ratio": 1.6008403361344539, "no_speech_prob": 5.225201675784774e-05}, {"id": 164, "seek": 101056, "start": 1029.36, "end": 1036.32, "text": " learns the rule that the reward is alternating between the sides. Importantly, representations of", "tokens": [51304, 27152, 264, 4978, 300, 264, 7782, 307, 40062, 1296, 264, 4881, 13, 26391, 3627, 11, 33358, 295, 51652], "temperature": 0.0, "avg_logprob": -0.07481007295496324, "compression_ratio": 1.6008403361344539, "no_speech_prob": 5.225201675784774e-05}, {"id": 165, "seek": 103632, "start": 1036.32, "end": 1042.0, "text": " some neurons in the memory module resemble the splitter cells found experimentally,", "tokens": [50364, 512, 22027, 294, 264, 4675, 10088, 36870, 264, 4732, 3904, 5438, 1352, 5120, 379, 11, 50648], "temperature": 0.0, "avg_logprob": -0.046453069417904586, "compression_ratio": 1.5963302752293578, "no_speech_prob": 0.00010229968029307202}, {"id": 166, "seek": 103632, "start": 1042.0, "end": 1046.96, "text": " which are modulated by both the position and the direction of the future turn.", "tokens": [50648, 597, 366, 1072, 6987, 538, 1293, 264, 2535, 293, 264, 3513, 295, 264, 2027, 1261, 13, 50896], "temperature": 0.0, "avg_logprob": -0.046453069417904586, "compression_ratio": 1.5963302752293578, "no_speech_prob": 0.00010229968029307202}, {"id": 167, "seek": 103632, "start": 1047.76, "end": 1054.48, "text": " This suggests that TEM has the capacity to learn and map latent spaces, which are not", "tokens": [50936, 639, 13409, 300, 314, 6683, 575, 264, 6042, 281, 1466, 293, 4471, 48994, 7673, 11, 597, 366, 406, 51272], "temperature": 0.0, "avg_logprob": -0.046453069417904586, "compression_ratio": 1.5963302752293578, "no_speech_prob": 0.00010229968029307202}, {"id": 168, "seek": 103632, "start": 1054.48, "end": 1061.76, "text": " directly given to it in the observations. Another example of how TEM maps latent space is available", "tokens": [51272, 3838, 2212, 281, 309, 294, 264, 18163, 13, 3996, 1365, 295, 577, 314, 6683, 11317, 48994, 1901, 307, 2435, 51636], "temperature": 0.0, "avg_logprob": -0.046453069417904586, "compression_ratio": 1.5963302752293578, "no_speech_prob": 0.00010229968029307202}, {"id": 169, "seek": 106176, "start": 1061.76, "end": 1066.56, "text": " as a bonus clip to my Patreon supporters. More details at the end of this video.", "tokens": [50364, 382, 257, 10882, 7353, 281, 452, 15692, 17683, 13, 5048, 4365, 412, 264, 917, 295, 341, 960, 13, 50604], "temperature": 0.0, "avg_logprob": -0.11973317076520222, "compression_ratio": 1.5260869565217392, "no_speech_prob": 0.00010071378346765414}, {"id": 170, "seek": 106176, "start": 1067.28, "end": 1072.32, "text": " Terrific! Now we have a model that can generalize and naturally develops", "tokens": [50640, 36591, 1089, 0, 823, 321, 362, 257, 2316, 300, 393, 2674, 1125, 293, 8195, 25453, 50892], "temperature": 0.0, "avg_logprob": -0.11973317076520222, "compression_ratio": 1.5260869565217392, "no_speech_prob": 0.00010071378346765414}, {"id": 171, "seek": 106176, "start": 1072.32, "end": 1078.0, "text": " same representations of space as the hippocampal formation. So what insights can we draw from it?", "tokens": [50892, 912, 33358, 295, 1901, 382, 264, 27745, 905, 1215, 304, 11723, 13, 407, 437, 14310, 393, 321, 2642, 490, 309, 30, 51176], "temperature": 0.0, "avg_logprob": -0.11973317076520222, "compression_ratio": 1.5260869565217392, "no_speech_prob": 0.00010071378346765414}, {"id": 172, "seek": 106176, "start": 1080.0, "end": 1086.16, "text": " Recall that play cells remap, which means they change their preferred firing locations in different", "tokens": [51276, 9647, 336, 300, 862, 5438, 890, 569, 11, 597, 1355, 436, 1319, 641, 16494, 16045, 9253, 294, 819, 51584], "temperature": 0.0, "avg_logprob": -0.11973317076520222, "compression_ratio": 1.5260869565217392, "no_speech_prob": 0.00010071378346765414}, {"id": 173, "seek": 108616, "start": 1086.16, "end": 1091.92, "text": " environments. This process has not been thought to be random since there is no immediate logic in", "tokens": [50364, 12388, 13, 639, 1399, 575, 406, 668, 1194, 281, 312, 4974, 1670, 456, 307, 572, 11629, 9952, 294, 50652], "temperature": 0.0, "avg_logprob": -0.04510171729398061, "compression_ratio": 1.5819672131147542, "no_speech_prob": 0.00016093057638499886}, {"id": 174, "seek": 108616, "start": 1091.92, "end": 1098.4, "text": " how these representations drift around. But having a model of hippocampal formation at hand,", "tokens": [50652, 577, 613, 33358, 19699, 926, 13, 583, 1419, 257, 2316, 295, 27745, 905, 1215, 304, 11723, 412, 1011, 11, 50976], "temperature": 0.0, "avg_logprob": -0.04510171729398061, "compression_ratio": 1.5819672131147542, "no_speech_prob": 0.00016093057638499886}, {"id": 175, "seek": 108616, "start": 1098.4, "end": 1105.2, "text": " we can start to address this question on a whole other level. Notice that neurons in our memory", "tokens": [50976, 321, 393, 722, 281, 2985, 341, 1168, 322, 257, 1379, 661, 1496, 13, 13428, 300, 22027, 294, 527, 4675, 51316], "temperature": 0.0, "avg_logprob": -0.04510171729398061, "compression_ratio": 1.5819672131147542, "no_speech_prob": 0.00016093057638499886}, {"id": 176, "seek": 108616, "start": 1105.2, "end": 1111.76, "text": " module, the ones that resemble play cells, are actually conjunctions between sensory and structural", "tokens": [51316, 10088, 11, 264, 2306, 300, 36870, 862, 5438, 11, 366, 767, 18244, 3916, 1296, 27233, 293, 15067, 51644], "temperature": 0.0, "avg_logprob": -0.04510171729398061, "compression_ratio": 1.5819672131147542, "no_speech_prob": 0.00016093057638499886}, {"id": 177, "seek": 111176, "start": 1111.76, "end": 1118.8, "text": " information. This means that firing of a particular play cell is partially controlled by grid cells,", "tokens": [50364, 1589, 13, 639, 1355, 300, 16045, 295, 257, 1729, 862, 2815, 307, 18886, 10164, 538, 10748, 5438, 11, 50716], "temperature": 0.0, "avg_logprob": -0.07258432252066475, "compression_ratio": 1.7272727272727273, "no_speech_prob": 7.722178270341828e-05}, {"id": 178, "seek": 111176, "start": 1118.8, "end": 1124.16, "text": " which provide the structural information. So, for example, if in one environment,", "tokens": [50716, 597, 2893, 264, 15067, 1589, 13, 407, 11, 337, 1365, 11, 498, 294, 472, 2823, 11, 50984], "temperature": 0.0, "avg_logprob": -0.07258432252066475, "compression_ratio": 1.7272727272727273, "no_speech_prob": 7.722178270341828e-05}, {"id": 179, "seek": 111176, "start": 1124.16, "end": 1130.32, "text": " the location of a given play cell coincides with the hexagonal activity pattern of a particular", "tokens": [50984, 264, 4914, 295, 257, 2212, 862, 2815, 13001, 1875, 365, 264, 23291, 6709, 304, 5191, 5102, 295, 257, 1729, 51292], "temperature": 0.0, "avg_logprob": -0.07258432252066475, "compression_ratio": 1.7272727272727273, "no_speech_prob": 7.722178270341828e-05}, {"id": 180, "seek": 111176, "start": 1130.32, "end": 1137.28, "text": " grid cell, then after we change the surroundings and the play cell remaps, its place field will shift", "tokens": [51292, 10748, 2815, 11, 550, 934, 321, 1319, 264, 25314, 293, 264, 862, 2815, 890, 2382, 11, 1080, 1081, 2519, 486, 5513, 51640], "temperature": 0.0, "avg_logprob": -0.07258432252066475, "compression_ratio": 1.7272727272727273, "no_speech_prob": 7.722178270341828e-05}, {"id": 181, "seek": 113728, "start": 1137.28, "end": 1144.72, "text": " to another location which also lies on this grid. In other words, remapping is not completely random,", "tokens": [50364, 281, 1071, 4914, 597, 611, 9134, 322, 341, 10748, 13, 682, 661, 2283, 11, 890, 10534, 307, 406, 2584, 4974, 11, 50736], "temperature": 0.0, "avg_logprob": -0.065455719044334, "compression_ratio": 1.776190476190476, "no_speech_prob": 0.00031503752688877285}, {"id": 182, "seek": 113728, "start": 1144.72, "end": 1149.76, "text": " but rather is controlled by grid cells, preserving some structural information.", "tokens": [50736, 457, 2831, 307, 10164, 538, 10748, 5438, 11, 33173, 512, 15067, 1589, 13, 50988], "temperature": 0.0, "avg_logprob": -0.065455719044334, "compression_ratio": 1.776190476190476, "no_speech_prob": 0.00031503752688877285}, {"id": 183, "seek": 113728, "start": 1150.6399999999999, "end": 1156.72, "text": " This relationship between the locations of place and grid cells implies that there should be a", "tokens": [51032, 639, 2480, 1296, 264, 9253, 295, 1081, 293, 10748, 5438, 18779, 300, 456, 820, 312, 257, 51336], "temperature": 0.0, "avg_logprob": -0.065455719044334, "compression_ratio": 1.776190476190476, "no_speech_prob": 0.00031503752688877285}, {"id": 184, "seek": 113728, "start": 1156.72, "end": 1163.04, "text": " correlation between the degree to which firing locations of place and grid cells coincide across", "tokens": [51336, 20009, 1296, 264, 4314, 281, 597, 16045, 9253, 295, 1081, 293, 10748, 5438, 13001, 482, 2108, 51652], "temperature": 0.0, "avg_logprob": -0.065455719044334, "compression_ratio": 1.776190476190476, "no_speech_prob": 0.00031503752688877285}, {"id": 185, "seek": 116304, "start": 1163.04, "end": 1169.12, "text": " two environments. This is the case in the model, and remarkably, when the authors tested this", "tokens": [50364, 732, 12388, 13, 639, 307, 264, 1389, 294, 264, 2316, 11, 293, 37381, 11, 562, 264, 16552, 8246, 341, 50668], "temperature": 0.0, "avg_logprob": -0.06348503828048706, "compression_ratio": 1.559090909090909, "no_speech_prob": 0.0003250308218412101}, {"id": 186, "seek": 116304, "start": 1169.12, "end": 1174.1599999999999, "text": " prediction on experimental data, they found it to be true in real brains as well.", "tokens": [50668, 17630, 322, 17069, 1412, 11, 436, 1352, 309, 281, 312, 2074, 294, 957, 15442, 382, 731, 13, 50920], "temperature": 0.0, "avg_logprob": -0.06348503828048706, "compression_ratio": 1.559090909090909, "no_speech_prob": 0.0003250308218412101}, {"id": 187, "seek": 116304, "start": 1177.2, "end": 1182.72, "text": " Well, I know this was a ton of information to process, so let's try to tie everything together.", "tokens": [51072, 1042, 11, 286, 458, 341, 390, 257, 2952, 295, 1589, 281, 1399, 11, 370, 718, 311, 853, 281, 7582, 1203, 1214, 13, 51348], "temperature": 0.0, "avg_logprob": -0.06348503828048706, "compression_ratio": 1.559090909090909, "no_speech_prob": 0.0003250308218412101}, {"id": 188, "seek": 116304, "start": 1183.76, "end": 1188.0, "text": " The problem of constructing internal models of the world is cornerstone", "tokens": [51400, 440, 1154, 295, 39969, 6920, 5245, 295, 264, 1002, 307, 4538, 11243, 51612], "temperature": 0.0, "avg_logprob": -0.06348503828048706, "compression_ratio": 1.559090909090909, "no_speech_prob": 0.0003250308218412101}, {"id": 189, "seek": 118800, "start": 1188.0, "end": 1194.96, "text": " for both biological and artificial intelligence. It can be solved by factorizing the surrounding", "tokens": [50364, 337, 1293, 13910, 293, 11677, 7599, 13, 467, 393, 312, 13041, 538, 5952, 3319, 264, 11498, 50712], "temperature": 0.0, "avg_logprob": -0.06525338839178216, "compression_ratio": 1.6578947368421053, "no_speech_prob": 0.0018101766472682357}, {"id": 190, "seek": 118800, "start": 1194.96, "end": 1200.88, "text": " into building blocks and combining them with particular sensory contexts to generate new", "tokens": [50712, 666, 2390, 8474, 293, 21928, 552, 365, 1729, 27233, 30628, 281, 8460, 777, 51008], "temperature": 0.0, "avg_logprob": -0.06525338839178216, "compression_ratio": 1.6578947368421053, "no_speech_prob": 0.0018101766472682357}, {"id": 191, "seek": 118800, "start": 1200.88, "end": 1208.24, "text": " models on the go, allowing for rapid generalization. This factorization and composition can be", "tokens": [51008, 5245, 322, 264, 352, 11, 8293, 337, 7558, 2674, 2144, 13, 639, 5952, 2144, 293, 12686, 393, 312, 51376], "temperature": 0.0, "avg_logprob": -0.06525338839178216, "compression_ratio": 1.6578947368421053, "no_speech_prob": 0.0018101766472682357}, {"id": 192, "seek": 118800, "start": 1208.24, "end": 1213.84, "text": " demonstrated in a computational model, which, when tasked with predicting the next observation in", "tokens": [51376, 18772, 294, 257, 28270, 2316, 11, 597, 11, 562, 38621, 365, 32884, 264, 958, 14816, 294, 51656], "temperature": 0.0, "avg_logprob": -0.06525338839178216, "compression_ratio": 1.6578947368421053, "no_speech_prob": 0.0018101766472682357}, {"id": 193, "seek": 121384, "start": 1213.84, "end": 1220.8, "text": " its sequence, learns the underlying relational structure of the world. Representations that", "tokens": [50364, 1080, 8310, 11, 27152, 264, 14217, 38444, 3877, 295, 264, 1002, 13, 19945, 763, 300, 50712], "temperature": 0.0, "avg_logprob": -0.06466529308221279, "compression_ratio": 1.5605381165919283, "no_speech_prob": 0.00020988189498893917}, {"id": 194, "seek": 121384, "start": 1220.8, "end": 1226.8799999999999, "text": " naturally emerge in this model resemble real neurons found in the hippocampal formation,", "tokens": [50712, 8195, 21511, 294, 341, 2316, 36870, 957, 22027, 1352, 294, 264, 27745, 905, 1215, 304, 11723, 11, 51016], "temperature": 0.0, "avg_logprob": -0.06466529308221279, "compression_ratio": 1.5605381165919283, "no_speech_prob": 0.00020988189498893917}, {"id": 195, "seek": 121384, "start": 1226.8799999999999, "end": 1233.12, "text": " suggesting a unified framework of interactions between entorhinal cortex and hippocampus.", "tokens": [51016, 18094, 257, 26787, 8388, 295, 13280, 1296, 948, 284, 71, 2071, 33312, 293, 27745, 905, 1215, 301, 13, 51328], "temperature": 0.0, "avg_logprob": -0.06466529308221279, "compression_ratio": 1.5605381165919283, "no_speech_prob": 0.00020988189498893917}, {"id": 196, "seek": 121384, "start": 1235.9199999999998, "end": 1240.3999999999999, "text": " I want to take this opportunity to give huge thanks to Dr. James Whittington,", "tokens": [51468, 286, 528, 281, 747, 341, 2650, 281, 976, 2603, 3231, 281, 2491, 13, 5678, 506, 2414, 1756, 11, 51692], "temperature": 0.0, "avg_logprob": -0.06466529308221279, "compression_ratio": 1.5605381165919283, "no_speech_prob": 0.00020988189498893917}, {"id": 197, "seek": 124040, "start": 1240.48, "end": 1246.16, "text": " the first author of the original TAM paper, and Gus, my friend and fellow patron with an", "tokens": [50368, 264, 700, 3793, 295, 264, 3380, 314, 2865, 3035, 11, 293, 40619, 11, 452, 1277, 293, 7177, 21843, 365, 364, 50652], "temperature": 0.0, "avg_logprob": -0.11306719260640663, "compression_ratio": 1.6213235294117647, "no_speech_prob": 0.0010322119342163205}, {"id": 198, "seek": 124040, "start": 1246.16, "end": 1251.92, "text": " expertise in machine learning, who both helped me immensely with preparing the script for this video.", "tokens": [50652, 11769, 294, 3479, 2539, 11, 567, 1293, 4254, 385, 38674, 365, 10075, 264, 5755, 337, 341, 960, 13, 50940], "temperature": 0.0, "avg_logprob": -0.11306719260640663, "compression_ratio": 1.6213235294117647, "no_speech_prob": 0.0010322119342163205}, {"id": 199, "seek": 124040, "start": 1252.8000000000002, "end": 1257.68, "text": " As a final note, I will mention that the Tolman Eigenbaum machine we've seen today", "tokens": [50984, 1018, 257, 2572, 3637, 11, 286, 486, 2152, 300, 264, 21402, 1601, 30586, 46641, 3479, 321, 600, 1612, 965, 51228], "temperature": 0.0, "avg_logprob": -0.11306719260640663, "compression_ratio": 1.6213235294117647, "no_speech_prob": 0.0010322119342163205}, {"id": 200, "seek": 124040, "start": 1257.68, "end": 1263.2800000000002, "text": " is actually very similar to a transformer architecture, a type of neural network that", "tokens": [51228, 307, 767, 588, 2531, 281, 257, 31782, 9482, 11, 257, 2010, 295, 18161, 3209, 300, 51508], "temperature": 0.0, "avg_logprob": -0.11306719260640663, "compression_ratio": 1.6213235294117647, "no_speech_prob": 0.0010322119342163205}, {"id": 201, "seek": 124040, "start": 1263.2800000000002, "end": 1269.0400000000002, "text": " is at the core of modern machine learning. In fact, with one little modification,", "tokens": [51508, 307, 412, 264, 4965, 295, 4363, 3479, 2539, 13, 682, 1186, 11, 365, 472, 707, 26747, 11, 51796], "temperature": 0.0, "avg_logprob": -0.11306719260640663, "compression_ratio": 1.6213235294117647, "no_speech_prob": 0.0010322119342163205}, {"id": 202, "seek": 126904, "start": 1269.04, "end": 1275.52, "text": " we can turn this similarity into a precise mathematical equivalence. And this modified", "tokens": [50364, 321, 393, 1261, 341, 32194, 666, 257, 13600, 18894, 9052, 655, 13, 400, 341, 15873, 50688], "temperature": 0.0, "avg_logprob": -0.08002917766571045, "compression_ratio": 1.588235294117647, "no_speech_prob": 0.0003101533220615238}, {"id": 203, "seek": 126904, "start": 1275.52, "end": 1281.76, "text": " version, called the Tolman Eigenbaum machine transformer, learns much faster and performs", "tokens": [50688, 3037, 11, 1219, 264, 21402, 1601, 30586, 46641, 3479, 31782, 11, 27152, 709, 4663, 293, 26213, 51000], "temperature": 0.0, "avg_logprob": -0.08002917766571045, "compression_ratio": 1.588235294117647, "no_speech_prob": 0.0003101533220615238}, {"id": 204, "seek": 126904, "start": 1281.76, "end": 1288.0, "text": " better, while still resembling biological representations for the most part. This potentially", "tokens": [51000, 1101, 11, 1339, 920, 20695, 1688, 13910, 33358, 337, 264, 881, 644, 13, 639, 7263, 51312], "temperature": 0.0, "avg_logprob": -0.08002917766571045, "compression_ratio": 1.588235294117647, "no_speech_prob": 0.0003101533220615238}, {"id": 205, "seek": 126904, "start": 1288.0, "end": 1292.8, "text": " provides a very promising link between neuroscience and modern machine learning,", "tokens": [51312, 6417, 257, 588, 20257, 2113, 1296, 42762, 293, 4363, 3479, 2539, 11, 51552], "temperature": 0.0, "avg_logprob": -0.08002917766571045, "compression_ratio": 1.588235294117647, "no_speech_prob": 0.0003101533220615238}, {"id": 206, "seek": 126904, "start": 1292.8, "end": 1295.6, "text": " which makes both fields even more exciting than ever.", "tokens": [51552, 597, 1669, 1293, 7909, 754, 544, 4670, 813, 1562, 13, 51692], "temperature": 0.0, "avg_logprob": -0.08002917766571045, "compression_ratio": 1.588235294117647, "no_speech_prob": 0.0003101533220615238}, {"id": 207, "seek": 129560, "start": 1295.6799999999998, "end": 1302.24, "text": " Now, I know this was a very simplified description, but fully exploring this equivalence", "tokens": [50368, 823, 11, 286, 458, 341, 390, 257, 588, 26335, 3855, 11, 457, 4498, 12736, 341, 9052, 655, 50696], "temperature": 0.0, "avg_logprob": -0.1306018829345703, "compression_ratio": 1.6153846153846154, "no_speech_prob": 0.0006070671370252967}, {"id": 208, "seek": 129560, "start": 1302.8, "end": 1307.12, "text": " would require going over the transformer and hopfield networks in detail.", "tokens": [50724, 576, 3651, 516, 670, 264, 31782, 293, 3818, 7610, 9590, 294, 2607, 13, 50940], "temperature": 0.0, "avg_logprob": -0.1306018829345703, "compression_ratio": 1.6153846153846154, "no_speech_prob": 0.0006070671370252967}, {"id": 209, "seek": 129560, "start": 1307.84, "end": 1313.12, "text": " Let me know down in the comment section if you would like to see a more technical video of this kind.", "tokens": [50976, 961, 385, 458, 760, 294, 264, 2871, 3541, 498, 291, 576, 411, 281, 536, 257, 544, 6191, 960, 295, 341, 733, 13, 51240], "temperature": 0.0, "avg_logprob": -0.1306018829345703, "compression_ratio": 1.6153846153846154, "no_speech_prob": 0.0006070671370252967}, {"id": 210, "seek": 129560, "start": 1314.0, "end": 1318.9599999999998, "text": " In the meantime, if you're interested in machine learning and don't want to wait any longer,", "tokens": [51284, 682, 264, 14991, 11, 498, 291, 434, 3102, 294, 3479, 2539, 293, 500, 380, 528, 281, 1699, 604, 2854, 11, 51532], "temperature": 0.0, "avg_logprob": -0.1306018829345703, "compression_ratio": 1.6153846153846154, "no_speech_prob": 0.0006070671370252967}, {"id": 211, "seek": 129560, "start": 1318.9599999999998, "end": 1322.8, "text": " let me tell you about something that can take your understanding to the next level,", "tokens": [51532, 718, 385, 980, 291, 466, 746, 300, 393, 747, 428, 3701, 281, 264, 958, 1496, 11, 51724], "temperature": 0.0, "avg_logprob": -0.1306018829345703, "compression_ratio": 1.6153846153846154, "no_speech_prob": 0.0006070671370252967}, {"id": 212, "seek": 132280, "start": 1323.52, "end": 1330.24, "text": " brilliant.org. Brilliant is a revolutionary platform for engaging and interactive learning.", "tokens": [50400, 10248, 13, 4646, 13, 34007, 307, 257, 22687, 3663, 337, 11268, 293, 15141, 2539, 13, 50736], "temperature": 0.0, "avg_logprob": -0.09272617763943142, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.0033242383506149054}, {"id": 213, "seek": 132280, "start": 1330.24, "end": 1335.2, "text": " Gone are the days of passive textbook reading. With Brilliant, you'll engage with the material", "tokens": [50736, 39068, 366, 264, 1708, 295, 14975, 25591, 3760, 13, 2022, 34007, 11, 291, 603, 4683, 365, 264, 2527, 50984], "temperature": 0.0, "avg_logprob": -0.09272617763943142, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.0033242383506149054}, {"id": 214, "seek": 132280, "start": 1335.2, "end": 1341.6, "text": " in a hands-on way, solving problems, answering questions and participating in stunning interactive", "tokens": [50984, 294, 257, 2377, 12, 266, 636, 11, 12606, 2740, 11, 13430, 1651, 293, 13950, 294, 18550, 15141, 51304], "temperature": 0.0, "avg_logprob": -0.09272617763943142, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.0033242383506149054}, {"id": 215, "seek": 132280, "start": 1341.6, "end": 1346.48, "text": " visualizations, which help you develop an intuitive understanding of the material.", "tokens": [51304, 5056, 14455, 11, 597, 854, 291, 1499, 364, 21769, 3701, 295, 264, 2527, 13, 51548], "temperature": 0.0, "avg_logprob": -0.09272617763943142, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.0033242383506149054}, {"id": 216, "seek": 132280, "start": 1346.48, "end": 1351.6, "text": " One course that you might find particularly interesting after watching this video is titled", "tokens": [51548, 1485, 1164, 300, 291, 1062, 915, 4098, 1880, 934, 1976, 341, 960, 307, 19841, 51804], "temperature": 0.0, "avg_logprob": -0.09272617763943142, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.0033242383506149054}, {"id": 217, "seek": 135160, "start": 1351.6, "end": 1357.1999999999998, "text": " Artificial Neural Networks. It offers an accessible introduction into the world of artificial", "tokens": [50364, 5735, 10371, 1734, 1807, 12640, 82, 13, 467, 7736, 364, 9515, 9339, 666, 264, 1002, 295, 11677, 50644], "temperature": 0.0, "avg_logprob": -0.066715396453287, "compression_ratio": 1.6398601398601398, "no_speech_prob": 0.0018101573223248124}, {"id": 218, "seek": 135160, "start": 1357.1999999999998, "end": 1363.12, "text": " intelligence and how it is inspired by the human brain. You will learn how neural networks work,", "tokens": [50644, 7599, 293, 577, 309, 307, 7547, 538, 264, 1952, 3567, 13, 509, 486, 1466, 577, 18161, 9590, 589, 11, 50940], "temperature": 0.0, "avg_logprob": -0.066715396453287, "compression_ratio": 1.6398601398601398, "no_speech_prob": 0.0018101573223248124}, {"id": 219, "seek": 135160, "start": 1363.12, "end": 1368.9599999999998, "text": " how to build your own, and even how to train them to recognize patterns. But that's just the tip of", "tokens": [50940, 577, 281, 1322, 428, 1065, 11, 293, 754, 577, 281, 3847, 552, 281, 5521, 8294, 13, 583, 300, 311, 445, 264, 4125, 295, 51232], "temperature": 0.0, "avg_logprob": -0.066715396453287, "compression_ratio": 1.6398601398601398, "no_speech_prob": 0.0018101573223248124}, {"id": 220, "seek": 135160, "start": 1368.9599999999998, "end": 1374.6399999999999, "text": " the iceberg. With over 80 courses to choose from, Brilliant has something for everyone,", "tokens": [51232, 264, 38880, 13, 2022, 670, 4688, 7712, 281, 2826, 490, 11, 34007, 575, 746, 337, 1518, 11, 51516], "temperature": 0.0, "avg_logprob": -0.066715396453287, "compression_ratio": 1.6398601398601398, "no_speech_prob": 0.0018101573223248124}, {"id": 221, "seek": 135160, "start": 1374.6399999999999, "end": 1380.0, "text": " and with its personalized approach, you can learn at your own pace with bite-sized chunks.", "tokens": [51516, 293, 365, 1080, 28415, 3109, 11, 291, 393, 1466, 412, 428, 1065, 11638, 365, 7988, 12, 20614, 24004, 13, 51784], "temperature": 0.0, "avg_logprob": -0.066715396453287, "compression_ratio": 1.6398601398601398, "no_speech_prob": 0.0018101573223248124}, {"id": 222, "seek": 138000, "start": 1380.64, "end": 1384.88, "text": " Take your curiosity to the next level today. Go to brilliant.org", "tokens": [50396, 3664, 428, 18769, 281, 264, 958, 1496, 965, 13, 1037, 281, 10248, 13, 4646, 50608], "temperature": 0.0, "avg_logprob": -0.11466282489253025, "compression_ratio": 1.564102564102564, "no_speech_prob": 0.015905417501926422}, {"id": 223, "seek": 138000, "start": 1384.88, "end": 1391.04, "text": " slash artemker sonoff to get a 30-day free trial of everything Brilliant has to offer,", "tokens": [50608, 17330, 1523, 443, 5767, 1872, 4506, 281, 483, 257, 2217, 12, 810, 1737, 7308, 295, 1203, 34007, 575, 281, 2626, 11, 50916], "temperature": 0.0, "avg_logprob": -0.11466282489253025, "compression_ratio": 1.564102564102564, "no_speech_prob": 0.015905417501926422}, {"id": 224, "seek": 138000, "start": 1391.04, "end": 1396.72, "text": " and the first 200 people to use this link will get 20% off the premium subscription.", "tokens": [50916, 293, 264, 700, 2331, 561, 281, 764, 341, 2113, 486, 483, 945, 4, 766, 264, 12049, 17231, 13, 51200], "temperature": 0.0, "avg_logprob": -0.11466282489253025, "compression_ratio": 1.564102564102564, "no_speech_prob": 0.015905417501926422}, {"id": 225, "seek": 138000, "start": 1398.08, "end": 1402.32, "text": " If you enjoyed this video, press the like button, share it with your friends and colleagues,", "tokens": [51268, 759, 291, 4626, 341, 960, 11, 1886, 264, 411, 2960, 11, 2073, 309, 365, 428, 1855, 293, 7734, 11, 51480], "temperature": 0.0, "avg_logprob": -0.11466282489253025, "compression_ratio": 1.564102564102564, "no_speech_prob": 0.015905417501926422}, {"id": 226, "seek": 138000, "start": 1402.32, "end": 1407.6, "text": " subscribe to the channel if you haven't already, and consider supporting me on Patreon to suggest", "tokens": [51480, 3022, 281, 264, 2269, 498, 291, 2378, 380, 1217, 11, 293, 1949, 7231, 385, 322, 15692, 281, 3402, 51744], "temperature": 0.0, "avg_logprob": -0.11466282489253025, "compression_ratio": 1.564102564102564, "no_speech_prob": 0.015905417501926422}, {"id": 227, "seek": 140760, "start": 1407.6, "end": 1413.4399999999998, "text": " video topics and enjoy the bonus content. Stay tuned for more interesting topics coming up.", "tokens": [50364, 960, 8378, 293, 2103, 264, 10882, 2701, 13, 8691, 10870, 337, 544, 1880, 8378, 1348, 493, 13, 50656], "temperature": 0.0, "avg_logprob": -0.20552656229804545, "compression_ratio": 1.3302752293577982, "no_speech_prob": 0.0009546006331220269}, {"id": 228, "seek": 140760, "start": 1414.0, "end": 1423.52, "text": " Goodbye, and thank you for the interest in the brain.", "tokens": [50684, 15528, 11, 293, 1309, 291, 337, 264, 1179, 294, 264, 3567, 13, 51160], "temperature": 0.0, "avg_logprob": -0.20552656229804545, "compression_ratio": 1.3302752293577982, "no_speech_prob": 0.0009546006331220269}], "language": "en"}