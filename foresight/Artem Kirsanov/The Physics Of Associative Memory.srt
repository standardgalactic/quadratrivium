1
00:00:00,000 --> 00:00:06,000
Consider the following scenario. You are at a party when you hear a short snippet of your favorite song.

2
00:00:09,520 --> 00:00:15,360
Almost instantly, your brain recalls the lyrics of that song and many related memories,

3
00:00:15,920 --> 00:00:19,680
such as attending a recent concert featuring that artist.

4
00:00:22,720 --> 00:00:29,440
It seems very natural and unimpressive. After all, people can recall information all the time.

5
00:00:29,520 --> 00:00:34,400
However, if you think about it, this problem is computationally non-trivial.

6
00:00:35,520 --> 00:00:41,280
Let's put ourselves in the shoes of evolution and try to come up with an algorithm for the brain

7
00:00:41,280 --> 00:00:48,240
to solve it. The first approach that comes to mind is to actually store some kind of a database

8
00:00:48,240 --> 00:00:54,560
of all the songs you've heard a sufficient number of times, along with related information,

9
00:00:54,560 --> 00:01:00,560
like the title and lyrics. When an audio fragment of an unknown song is received,

10
00:01:00,560 --> 00:01:06,960
we can scan through all the songs in our database, find the one that has a close enough match,

11
00:01:06,960 --> 00:01:13,920
and retrieve its lyrics. However, the search space of every song I've ever heard

12
00:01:13,920 --> 00:01:20,480
is astronomically large, and it's even larger when considering every single memory you've formed

13
00:01:20,480 --> 00:01:25,520
since childhood. Performing an exhaustive search would be simply impossible.

14
00:01:26,720 --> 00:01:33,440
Yet, you seem to have no problem instantly recognizing familiar stimuli and finding associations

15
00:01:33,440 --> 00:01:40,640
between them. So, how does the brain accomplish this so quickly? In this video, we will lay the

16
00:01:40,640 --> 00:01:46,480
foundation for a new paradigm of information storage and retrieval, which is more in line with

17
00:01:46,560 --> 00:01:54,080
biology, and actually build one of the simpler models of this process, known as Hopfield Networks,

18
00:01:55,040 --> 00:02:02,320
developed by John Hopfield in 1982, who laid an important groundwork for many ideas in both

19
00:02:02,320 --> 00:02:16,320
neuroscience and machine learning. If you're interested, stay tuned!

20
00:02:17,440 --> 00:02:24,160
Just to reiterate, we need a way to somehow query what we know and find associations between

21
00:02:24,160 --> 00:02:30,480
existing memories and new inputs without explicitly checking individual entries for a match,

22
00:02:30,480 --> 00:02:37,440
which seems like an impossible problem. However, we can draw insights from a seemingly unrelated

23
00:02:37,440 --> 00:02:43,840
field of molecular biology, and in particular, a concept known as Leventhold's paradox of

24
00:02:43,840 --> 00:02:51,360
protein folding. As you may know, proteins are long chains of amino acids that fold into specific

25
00:02:51,360 --> 00:02:57,120
three-dimensional structures which determine their function. The number of possible structural

26
00:02:57,120 --> 00:03:03,200
configurations a protein can take, considering all the different ways you can arrange the atoms

27
00:03:03,200 --> 00:03:09,600
of an amino acid chain in three-dimensional space, is absolutely enormous. Given the number of

28
00:03:09,600 --> 00:03:14,880
possibilities, it seems like it would take an astronomical amount of time for a protein

29
00:03:14,880 --> 00:03:21,840
to search through all the possible structures to find its correct folded state. In fact, there are

30
00:03:21,840 --> 00:03:28,160
computations showing that even if the protein samples its different conformations at a nanosecond

31
00:03:28,160 --> 00:03:34,400
scale, it would still require more time than the age of the universe to arrive at the correct

32
00:03:34,480 --> 00:03:40,560
configuration. Yet, in reality, proteins fold into their native structures in a matter of

33
00:03:40,560 --> 00:03:47,120
milliseconds. So, how do they accomplish this? When I first heard this paradox in high school,

34
00:03:47,120 --> 00:03:52,800
it seemed to me like an ill-posed question. After all, the protein molecule is not a computer,

35
00:03:52,800 --> 00:03:59,360
so it doesn't do any sort of search. It just folds into the most stable and favorable configuration

36
00:03:59,360 --> 00:04:05,520
according to physical laws. This is similar how when you throw a ball, the ball doesn't

37
00:04:05,520 --> 00:04:11,600
search through all the possible trajectories to select the optimal parabolic one. It simply

38
00:04:11,600 --> 00:04:18,080
follows that path because, well, physics works this way. But how can we think about this folding

39
00:04:18,080 --> 00:04:25,680
into a favorable configuration? Favorable for what exactly? Let's introduce the concept of

40
00:04:25,680 --> 00:04:31,040
energy as it will come in handy in future videos as well. If you think back to your high school

41
00:04:31,040 --> 00:04:37,280
physics days, you may recall something along the lines of energy is a quantitative property that

42
00:04:37,280 --> 00:04:44,480
describes this state of a system, namely the capacity to do work or cause change. Energy can be

43
00:04:44,480 --> 00:04:50,240
stored in a variety of different forms, and for the case of proteins, we will be interested in

44
00:04:50,240 --> 00:04:55,440
potential energy stored in the interactions between the atoms in the protein chain.

45
00:04:56,880 --> 00:05:03,040
Each possible configuration of the protein chain has a specific potential energy level

46
00:05:03,040 --> 00:05:09,520
determined by the sum of all of these atomic interactions. In other words, we can assign

47
00:05:09,520 --> 00:05:15,760
a positive number to each state equal to its energy, which is a function in some very high

48
00:05:15,760 --> 00:05:21,600
dimensional space where different dimensions correspond to degrees of freedom you need

49
00:05:21,600 --> 00:05:28,320
to uniquely describe a configuration. For example, all possible dihedral angles of peptide bonds.

50
00:05:30,560 --> 00:05:36,240
Let's abstractly visualize it as having just two dimensions. Then the energy function can be

51
00:05:36,240 --> 00:05:42,960
thought of as a surface where each point on it represents a possible protein configuration,

52
00:05:42,960 --> 00:05:47,600
and the height of the point represents the potential energy of that configuration.

53
00:05:48,240 --> 00:05:55,200
This is what we are going to refer to as energy landscape. For a protein, it would be a complex

54
00:05:55,200 --> 00:06:02,880
rugged surface with many peaks and valleys. Now, here is the key point. A protein molecule,

55
00:06:02,880 --> 00:06:09,840
like any physical system, tends to minimize its potential energy, guided by this second law of

56
00:06:09,840 --> 00:06:17,200
thermodynamics. It will naturally seek out the configuration that has the lowest possible energy

57
00:06:17,200 --> 00:06:24,480
level, as this represents the optimal arrangement of its atoms. And this, in fact, corresponds to the

58
00:06:24,480 --> 00:06:32,640
native, correctly folded state. When a protein is folding, it is essentially rolling downhill on

59
00:06:32,640 --> 00:06:39,440
the energy landscape, following the steepest path towards the valley. This is why proteins can

60
00:06:39,440 --> 00:06:46,800
fall so quickly. They don't need to search through all possible configurations. They simply follow

61
00:06:46,800 --> 00:06:51,680
the natural tendency of physical systems to minimize their potential energy.

62
00:06:52,960 --> 00:06:58,480
The protein's folding process is guided by the shape of the energy landscape,

63
00:06:58,480 --> 00:07:04,560
which in turn is determined by the interaction between its atoms. And the descent along the

64
00:07:04,560 --> 00:07:10,320
surface is essentially driven by the underlying physical process of energy minimization.

65
00:07:12,560 --> 00:07:19,040
Now, the core idea is to achieve something similar for the case of associative memory. Suppose we

66
00:07:19,040 --> 00:07:25,600
have a system that can encode information in its states, and each configuration has a specific

67
00:07:25,600 --> 00:07:33,440
potential energy determined by the interaction between the states. Then we need to first somehow

68
00:07:33,440 --> 00:07:40,560
sculpt the underlying energy landscape so that memories or state patterns we want to store

69
00:07:40,560 --> 00:07:48,320
correspond to local minima, these wells in the energy surface. Second, we need something that

70
00:07:48,320 --> 00:07:54,800
would play the role of the second law of thermodynamics and would drive the changes in the states,

71
00:07:54,800 --> 00:08:02,160
directing the system towards the nearest local minimum. Once these two things are achieved,

72
00:08:02,160 --> 00:08:08,880
retrieving a memory that is most similar to the input pattern is done by configuring the system

73
00:08:08,880 --> 00:08:15,520
to encode the input pattern initially and letting it run to the equilibrium, descending into the

74
00:08:15,520 --> 00:08:24,000
energy well, from which we can read out the source memory. Sounds neat, right? So let's get into

75
00:08:24,000 --> 00:08:31,120
building it. Let's consider a set of neurons which we can think of as abstract units that can

76
00:08:31,120 --> 00:08:38,160
be in one of two possible states, plus one or minus one. This is a simplified analogy of how nerve

77
00:08:38,160 --> 00:08:43,440
cells in the brain encode information through patterns of firing. They either generate an

78
00:08:43,440 --> 00:08:50,400
electrical impulse at a given point in time or remain silent. We'll focus on the fully connected

79
00:08:50,400 --> 00:08:57,840
network where each neuron has connections to every other neuron. These connections have weights

80
00:08:57,840 --> 00:09:04,800
associated with them, real numbers that signify the strength of coupling between the corresponding

81
00:09:04,800 --> 00:09:11,920
pair of neurons. For a pair of units i and j, we denote the connection weight between them as

82
00:09:11,920 --> 00:09:20,720
wij and the states of neurons themselves as xi and xj. In the brain, connections between neurons

83
00:09:20,720 --> 00:09:27,760
or synapses have a well-defined direction. A pair of neurons is connected asymmetrically,

84
00:09:27,760 --> 00:09:34,240
meaning that the synapse from neuron A to neuron B is physiologically separate from the synapse that

85
00:09:34,240 --> 00:09:41,120
connects B to A if that one exists at all, and so they can have different weights. While we could

86
00:09:41,120 --> 00:09:46,560
generalize a Hopfield network to account for asymmetric connections, it would introduce

87
00:09:46,560 --> 00:09:53,360
complications and potentially unstable behavior. For simplicity, here we will stick to the original

88
00:09:53,360 --> 00:09:59,440
formulation of the Hopfield network, which assumes symmetric weights. In other words,

89
00:09:59,440 --> 00:10:04,080
neurons i and j are connected by the same weight in both directions.

90
00:10:04,560 --> 00:10:11,360
Now that we have a set of neurons symmetrically linked with each other through weighted connections,

91
00:10:11,920 --> 00:10:18,720
let's explore what these connection weights represent. If wij is greater than zero,

92
00:10:18,720 --> 00:10:24,480
the connection is said to be excitatory and favors the alignment between the states of

93
00:10:24,480 --> 00:10:30,960
two neurons. We can think of each connection as being either happy or unhappy, depending on the

94
00:10:30,960 --> 00:10:38,800
states of its neurons. For example, if wij is a large positive number, it means that neurons i

95
00:10:38,800 --> 00:10:45,120
and j are closely coupled, and one excites the other. In this case, when one neuron is active,

96
00:10:45,120 --> 00:10:50,880
the other tends to be active as well, and when one is silent, the other one is more likely to be silent.

97
00:10:52,800 --> 00:10:59,360
These configurations, where both xi and xj are either one or minus one, agree with the

98
00:10:59,360 --> 00:11:05,920
connection weight. However, if we observe, for example, that xi is equal to one and xj is equal

99
00:11:05,920 --> 00:11:13,040
to minus one, it conflicts with the excitatory nature of the connection, making such a configuration

100
00:11:13,040 --> 00:11:21,040
less likely. Conversely, when wij is negative, the connection promotes misalignment between the weights.

101
00:11:21,040 --> 00:11:30,640
This alignment between the signs can be expressed more concisely using the product xi times xj.

102
00:11:31,360 --> 00:11:37,440
This product will be positive when both neurons have the same sign and negative when they have

103
00:11:37,440 --> 00:11:44,320
different signs. By multiplying this product further by the connection weight, we obtain

104
00:11:44,320 --> 00:11:51,840
an expression for the happiness of that connection. For a positive wij, happiness will be positive

105
00:11:51,840 --> 00:11:59,120
when the product of the two states is positive. But this is just one edge. We can extend this idea

106
00:11:59,120 --> 00:12:05,840
and compute the happiness of the entire network as a whole by summing this quantity across all edges.

107
00:12:07,040 --> 00:12:12,880
The larger that number is, the more overall agreement there is between connection weights

108
00:12:12,880 --> 00:12:19,760
and pairwise states of neurons. Ultimately, we will search for a set of weights that maximize

109
00:12:19,760 --> 00:12:26,720
this quantity. And maximizing happiness is equivalent to minimizing it with a minus sign,

110
00:12:26,720 --> 00:12:32,000
which you can think of as the measure of overall conflict between the actual configuration of

111
00:12:32,000 --> 00:12:39,200
states and what's favored by the connection weights. This total conflict between the weights

112
00:12:39,200 --> 00:12:45,360
and pairwise states is exactly what we are going to define to be the energy of the system.

113
00:12:46,800 --> 00:12:53,440
As we discussed previously, we want the Hopfield network to be able to gradually evolve towards

114
00:12:53,440 --> 00:13:00,240
energy minima. But looking closely at the formula, we can see that the energy value depends both on

115
00:13:00,240 --> 00:13:05,840
the states and the weights. So there is a lot of things the system can tweak to change it.

116
00:13:05,840 --> 00:13:11,920
What exactly is getting adjusted? As we will see further, there are essentially

117
00:13:11,920 --> 00:13:18,160
two modes of network updates that nicely map to the two aspects of associative memory.

118
00:13:19,280 --> 00:13:26,080
Namely, adjusting the weights corresponds to shaping the energy landscape, defining

119
00:13:26,080 --> 00:13:32,880
which configurations are stable by digging energy wells around them. This is the act of learning

120
00:13:32,880 --> 00:13:38,080
when we are writing new memories into the network. Once the weights are fixed,

121
00:13:38,800 --> 00:13:44,960
tweaking the states of neurons to bring them into greater agreement with the weights

122
00:13:44,960 --> 00:13:52,320
corresponds to descending along the energy surface. This is the act of inference when we are

123
00:13:52,320 --> 00:13:58,640
recalling the memory that is at the bottom of the energy well, which is nearest to the configuration

124
00:13:58,640 --> 00:14:02,320
of the input pattern. Let's take a look at inference first.

125
00:14:05,200 --> 00:14:11,360
Suppose for a second, someone has already set the weights W and hence us the backbone of the

126
00:14:11,360 --> 00:14:17,920
network. The neurons themselves with all the connection weights. However, the exact configuration

127
00:14:17,920 --> 00:14:24,560
of states, which neurons are active and which are silent, is unknown. The question then becomes,

128
00:14:24,560 --> 00:14:29,520
how do we find the state pattern that would minimize the total energy?

129
00:14:30,400 --> 00:14:37,360
As we discussed, simply checking all possible states is not an option. So, we will start with

130
00:14:37,360 --> 00:14:43,600
some initial state, which could be either a partial or a noisy version of one of the memories

131
00:14:43,600 --> 00:14:51,360
or a random configuration altogether. Once the initial condition is set, we will iteratively

132
00:14:51,360 --> 00:14:58,000
try to lower the energy value by focusing on updating one neuron at a time. Let's denote

133
00:14:58,000 --> 00:15:04,480
the neuron we are currently considering as neuron i. We will calculate the total weighted input to

134
00:15:04,480 --> 00:15:11,920
it from all other neurons in the network. This input, which we'll denote as hi, is the sum of

135
00:15:11,920 --> 00:15:18,800
the states of all other neurons multiplied by their respective connection weights. If hi is

136
00:15:18,800 --> 00:15:25,840
positive, it means that the weighted sum of the other neuron states is in favor of neuron i being

137
00:15:25,840 --> 00:15:33,760
in the plus one state. Conversely, if hi is negative, it suggests that neuron i should be in the minus

138
00:15:33,760 --> 00:15:40,720
one state to minimize the conflict with the other neurons. So, we will update the state of the neuron

139
00:15:40,720 --> 00:15:48,000
i based on the sign of hi. Notice that this update is guaranteed to decrease the energy

140
00:15:48,000 --> 00:15:53,520
of the network, because from the two candidate states, we are selecting the more energetically

141
00:15:53,520 --> 00:16:00,400
favorable one. You can think of this as a kind of a voting process. Each neuron looks at the states of

142
00:16:00,400 --> 00:16:06,720
all other neurons weighted by the strength of their connections and decides whether to be active

143
00:16:06,720 --> 00:16:13,840
or silent based on the majority vote. We'll go through this process for each neuron in the network

144
00:16:13,840 --> 00:16:21,840
one by one chosen in random order, updating their states based on the input from all other neurons.

145
00:16:22,640 --> 00:16:29,280
Once we've updated all neurons, we will have completed one iteration of the network inference

146
00:16:29,280 --> 00:16:35,760
and decreased the system's energy by a little bit. We'll keep repeating this process, doing these

147
00:16:35,760 --> 00:16:42,320
sweeps through all neurons, updating them one at a time based on the current configuration.

148
00:16:42,320 --> 00:16:50,000
As we do this, the network will gradually evolve towards a configuration that minimizes the overall

149
00:16:50,000 --> 00:16:57,440
energy. At some point, however, we will reach a configuration where flipping any neuron would

150
00:16:57,440 --> 00:17:03,760
lead to an increase in energy. So, no further adjustments would be necessary. At that point,

151
00:17:03,760 --> 00:17:10,560
the network has converged to a stable configuration, where each neuron's state agrees with the majority

152
00:17:10,560 --> 00:17:16,320
vote. This stable configuration represents a local minimum in the energy landscape.

153
00:17:16,960 --> 00:17:22,800
Now, you might be wondering, is the network guaranteed to reach such a stable configuration?

154
00:17:22,800 --> 00:17:29,440
Could we possibly stumble into a particularly unlucky set of states and get stuck in a never-ending

155
00:17:29,440 --> 00:17:36,880
loop of flipping neurons back and forth? In other words, is such iterative flipping of one neuron

156
00:17:36,880 --> 00:17:43,600
at a time equivalent to doing a descent along the energy surface? This is where we come back to

157
00:17:43,600 --> 00:17:50,160
the point about symmetric weights. It turns out that there is a mathematical proof that I'm not

158
00:17:50,160 --> 00:17:57,520
going to cover here, stating that as long as your weights are symmetric, this simple majority vote

159
00:17:57,520 --> 00:18:04,240
single neuron update rule is guaranteed to eventually converge to a stable configuration

160
00:18:04,240 --> 00:18:11,280
if you do it enough times. To restate it, the Hopfield network can settle into different local

161
00:18:11,280 --> 00:18:17,760
minima based on its initial conditions. These local minima in the energy landscape correspond

162
00:18:17,760 --> 00:18:24,320
to distinct memories stored in the network. When we initialize the network with a pattern that is

163
00:18:24,320 --> 00:18:30,560
similar to one of these memories in some way and let it evolve, it will fall into the nearest

164
00:18:30,560 --> 00:18:37,680
local minimum, effectively recalling the complete stored memory, thus performing pattern completion

165
00:18:37,680 --> 00:18:43,840
or noise correction. But so far we haven't talked about how we come up with the set of connection

166
00:18:43,840 --> 00:18:50,560
weights that encode specific memories in the first place. So let's explore the learning process.

167
00:18:52,560 --> 00:18:58,960
Before we move to storing several memories, let's consider memorizing a single pattern of states.

168
00:18:59,600 --> 00:19:03,360
That means the network would have a single global minimum,

169
00:19:03,360 --> 00:19:09,520
one energy well, and would converge to the same pattern every time no matter where you initialize

170
00:19:09,520 --> 00:19:15,680
it. While it has little practical use, it provides a nice starting point to describe the learning

171
00:19:15,680 --> 00:19:23,600
procedure. Let's denote the template pattern that we'd like to store as XC, which is a vector

172
00:19:23,600 --> 00:19:31,760
packing the states of all neurons, and XCI will denote the ith component, the state of ith neuron

173
00:19:31,760 --> 00:19:38,400
encoding the memory. While XI refers to the state of ith neuron in the network in general,

174
00:19:38,400 --> 00:19:47,440
which could be tweaked. Revisiting our definition of energy, we want to set Wij so that this quantity

175
00:19:47,440 --> 00:19:55,520
would be at its minimal value for the memory pattern. If we plug XI equal to XCI, we get the

176
00:19:55,520 --> 00:20:01,920
equation for the energy of the reference pattern as a function of weights, which we want to turn

177
00:20:01,920 --> 00:20:08,400
into a global minimum. Notice that we don't really care about the absolute value of that energy.

178
00:20:08,960 --> 00:20:14,400
As long as the energy of the desired memory pattern is less than the energy of any other

179
00:20:14,400 --> 00:20:21,280
configuration. Now intuitively, the lowest possible energy is obtained when all the

180
00:20:21,280 --> 00:20:28,560
connection weights fully align with the state pairs. But when we have just a single pattern,

181
00:20:28,560 --> 00:20:36,160
this is very easy to do. All we need is to set the weight Wij to be the product of the corresponding

182
00:20:36,160 --> 00:20:42,800
pair of states in the memory pattern. This way, every connection is satisfied and the energy of

183
00:20:42,800 --> 00:20:48,480
the network when it is in the state XI becomes the negative of the total number of edges.

184
00:20:49,120 --> 00:20:56,000
When the network is in the state XI, any single flip of a neuron would increase the energy,

185
00:20:56,000 --> 00:21:03,040
thus making it a stable state. I want to reiterate the crucial point here. If we want to come up with

186
00:21:03,040 --> 00:21:09,840
the set of weights that would dig an energy well around some pattern, then all we need to know

187
00:21:09,840 --> 00:21:16,720
are the pairwise relationships between states in that pattern. If the two neurons are active

188
00:21:16,720 --> 00:21:22,560
together in the source memory, strengthening the connection between them lowers the hopfield

189
00:21:22,560 --> 00:21:28,320
energy of that memory, effectively storing it in the weights for associative recall.

190
00:21:29,280 --> 00:21:33,440
You may have heard the famous statement from Neuroscience attributed to Donald

191
00:21:33,840 --> 00:21:40,640
Hebb, neurons that fire together, wire together. And in fact, what we just did is known as the

192
00:21:40,640 --> 00:21:47,840
Hebbian learning rule. Great, so we found a way to make a single pattern a stable state of the

193
00:21:47,840 --> 00:21:55,520
network. But we want to store multiple patterns. How do we do that? Here's the key idea. We can

194
00:21:55,520 --> 00:22:02,240
simply sum the weights we would get for each pattern separately. So if we have three patterns,

195
00:22:02,320 --> 00:22:07,760
X1, X2 and X3, we can set the weights according to the following equation.

196
00:22:08,800 --> 00:22:15,200
What this will do is turn each of the patterns into a local minimum. It's pretty straightforward

197
00:22:15,200 --> 00:22:20,400
to show mathematically, and if you're interested, I encourage you to check out the references

198
00:22:20,400 --> 00:22:26,960
in the video description. However, intuitively, if the patterns you want to store are very

199
00:22:26,960 --> 00:22:34,400
different, so they are far away in the state space from each other, then if you first independently

200
00:22:34,400 --> 00:22:41,040
dig energy wells around each of them, and then simply add the energy landscapes together,

201
00:22:41,680 --> 00:22:48,960
the resulting surface will have local minima in the same three valleys. And this nicely brings

202
00:22:48,960 --> 00:22:56,160
us to the limitation of the hopfield networks. There is a limited number of valleys we can sculpt

203
00:22:56,160 --> 00:23:02,320
in the energy landscape before they start to interfere with each other. At some point,

204
00:23:02,320 --> 00:23:09,680
if we try to store too many patterns, the network will fail to converge to a stored pattern reliably

205
00:23:09,680 --> 00:23:16,000
and recall weird in-between kind of memories. The total maximum number of patterns you can

206
00:23:16,000 --> 00:23:23,680
store is thus limited and depends only on the size of the network. It is approximately 0.14

207
00:23:23,680 --> 00:23:30,720
times the number of neurons. So, if you have a hopfield network of 100 neurons, you can reliably

208
00:23:30,720 --> 00:23:37,280
store less than 14 patterns in the best case scenario. If you are unlucky, however, and some

209
00:23:37,280 --> 00:23:44,560
patterns are similar to each other or correlated, their energy wells will begin to interfere even

210
00:23:44,560 --> 00:23:50,720
before you reach the full capacity. All of this makes vanilla hopfield networks not useful for

211
00:23:50,800 --> 00:23:57,520
practical purposes. However, to this day, they provide a powerful and intuitive model of associative

212
00:23:57,520 --> 00:24:04,560
memory, a simple network of neuron-like units that can store and retrieve pattern through purely

213
00:24:04,560 --> 00:24:10,160
local learning and inference rules. Despite their limitations, hopfield networks have laid the

214
00:24:10,160 --> 00:24:16,720
groundwork for more advanced energy-based models. In one of the next videos, we will look at the

215
00:24:16,720 --> 00:24:23,520
extension of the hopfield networks known as Boltzmann machines. These generative architectures

216
00:24:23,520 --> 00:24:29,520
introduce additional hidden units and stochastic dynamics, allowing them to learn more complex

217
00:24:29,520 --> 00:24:35,360
probability distributions. There is also an extension to modern hopfield networks,

218
00:24:35,360 --> 00:24:41,600
published in 2016 with John Hopfield himself as one of the authors, but that's a topic for another

219
00:24:41,600 --> 00:24:47,040
time. In the meanwhile, I'd like to take a moment and thank Shortform, who are kindly sponsoring

220
00:24:47,040 --> 00:24:53,280
today's video. Shortform is a platform that lets you supercharge your reading and gain valuable

221
00:24:53,280 --> 00:25:00,320
insights from books. Their unique approach of book guides goes way beyond simple summaries,

222
00:25:00,320 --> 00:25:06,720
by providing a comprehensive overview of the material. Not only do you get a concise version

223
00:25:06,720 --> 00:25:14,000
of the main points, but you also benefit from related ideas sourced from other books and research

224
00:25:14,000 --> 00:25:20,640
papers on the topic. They have an actively growing library of books from all sorts of genres,

225
00:25:20,640 --> 00:25:26,080
such as science, health and technology. Not only that, but there is a useful AI-powered

226
00:25:26,080 --> 00:25:32,480
browser extension that allows you to generate similar guides for arbitrary content on the

227
00:25:32,480 --> 00:25:37,920
internet. Personally, I found Shortform to be really helpful, both when I'm choosing books to

228
00:25:37,920 --> 00:25:44,880
read and writing notes and flashcards on the topic. Don't hesitate to bring your reading to the next

229
00:25:44,880 --> 00:25:52,400
level by clicking the link down in the description to get 5 dates of unlimited access and 20% off

230
00:25:52,400 --> 00:25:58,080
on annual subscription. If you liked the video, share it with your friends, press the like button

231
00:25:58,080 --> 00:26:02,640
and subscribe to the channel if you haven't already. Stay tuned for more computational

232
00:26:02,640 --> 00:26:08,480
neuroscience and machine learning topics coming up. Goodbye and thank you for the interest in the brain.

233
00:26:28,080 --> 00:26:40,480
Thank you for watching and I'll see you in the next video.

