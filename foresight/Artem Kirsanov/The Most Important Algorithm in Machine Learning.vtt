WEBVTT

00:00.000 --> 00:08.240
What do nearly all machine learning systems have in common, from GPT and MeJourney to AlphaFold and various models of the brain?

00:09.080 --> 00:16.720
Despite being designed to solve different problems, having completely different architectures, and being trained on different data,

00:17.080 --> 00:19.720
there is something that unites all of them.

00:20.400 --> 00:26.160
A single algorithm that runs under the hood of the training procedures in all of those cases,

00:26.280 --> 00:32.600
this algorithm, called backpropagation, is the foundation of the entire field of machine learning,

00:33.120 --> 00:35.560
although its details are often overlooked.

00:36.840 --> 00:44.280
Surprisingly, what enables artificial networks to learn is also what makes them fundamentally different from the brain and

00:44.880 --> 00:49.160
incompatible with biology. This video is the first in a two-part series.

00:49.800 --> 00:54.840
Today, we will explore the concept of backpropagation in artificial systems and

00:55.240 --> 01:02.360
develop an intuitive understanding of what it is, why it works, and how you could have developed it from scratch yourself.

01:03.160 --> 01:09.920
In the next video, we will focus on synaptic plasticity, enabling learning in biological brains, and

01:10.360 --> 01:19.880
discuss whether backpropagation is biologically relevant, and if not, what kind of algorithms the brain may be using instead. If you're interested, stay tuned.

01:24.840 --> 01:37.000
Despite its transformative impact, it's hard to say who invented backpropagation in the first place, as certain concepts can be traced back to

01:37.000 --> 01:44.520
light needs in 17th century. However, it is believed that the first modern formulation of the algorithm, still in use today,

01:44.720 --> 01:53.320
was published by Seppo Linenma in his master's thesis in 1970, although he did not reference any neural networks explicitly.

01:54.320 --> 02:02.000
Another significant milestone occurred in 1986, when David Rumelhardt, Joffrey Hinton, and Ronald Williams

02:02.320 --> 02:07.360
published a paper titled Learning Representations by Backpropagating Errors.

02:07.440 --> 02:14.120
They applied the backpropagation algorithm to multi-layer perceptrons, a type of a neural network, and

02:14.680 --> 02:17.840
demonstrated for the first time that training with backpropagation

02:18.520 --> 02:28.280
enables the network to successfully solve problems and develop meaningful representations at the hidden neuron level, capturing important regularities in the task.

02:29.400 --> 02:38.720
As the field progressed, researchers scaled up these models significantly and introduced various architectures, but the fundamental principles of training

02:38.880 --> 02:46.480
remained largely unchanged. To gain a comprehensive understanding of what exactly it means to train a network,

02:46.520 --> 02:52.520
let's try to build the concept of backpropagation from the ground up. Consider the following problem.

02:53.080 --> 03:00.160
Suppose you have collected a set of points x, y on the plane, and you want to describe their relationship.

03:00.760 --> 03:06.480
To achieve this, you need to fit a curve y of x that best represents the data.

03:07.480 --> 03:12.120
Since there are infinitely many possible functions, we need to make some assumptions.

03:12.520 --> 03:20.200
For instance, let's assume we want to find a smooth approximation of the data using a polynomial of degree 5.

03:21.040 --> 03:29.400
That means that the resulting curve we are looking for will be a combination of a constant term, a polynomial of degree 0, a

03:30.120 --> 03:36.800
straight line, a parabola, and so on up to a power of 5, each weightened by specific coefficients.

03:37.040 --> 03:44.400
In other words, the equation for the curve is as follows. Where each k is some arbitrary real number.

03:45.000 --> 03:52.560
Our job then becomes finding the configuration of k0 through k5, which leads to the best fitting curve.

03:52.840 --> 03:58.480
To make the problem totally unambiguous, we need to agree on what the best curve even means.

03:59.040 --> 04:06.080
While you can just visually inspect the data points and estimate whether a given curve captures the pattern or not,

04:06.320 --> 04:11.400
this approach is highly subjective and impractical when dealing with large data sets.

04:12.000 --> 04:18.360
Instead, we need an objective measurement, a numerical value that quantifies the quality of a fit.

04:18.960 --> 04:25.320
One popular method is to measure the square distance between data points and the fitted curve.

04:25.720 --> 04:33.640
A high value suggests that the data points are significantly far from the curve, indicating a poor approximation.

04:34.640 --> 04:41.320
Conversely, low values indicate a better fit as the curve closely aligns with the data points.

04:41.800 --> 04:47.960
This measurement is commonly referred to as a loss and the objective is to minimize it.

04:48.360 --> 04:53.800
Now notice that for a fixed data, this distance, the value of the loss,

04:54.440 --> 05:02.280
depends only on the defining characteristics of the curve. In our case, the coefficients from k0 through k5.

05:03.080 --> 05:10.760
This means that it is effectively a function of parameters, so people usually refer to it as a loss function.

05:11.200 --> 05:16.520
It's important not to confuse two different functions we are implicitly dealing with here.

05:16.520 --> 05:23.080
The first one is the function y of x, which has one input number and one output number and

05:23.440 --> 05:29.160
defines the curve itself. It has this polynomial form given by k's.

05:29.440 --> 05:35.120
There are infinitely many such functions and we would like to find the best one. To achieve this,

05:35.120 --> 05:42.360
we introduce a loss function, which instead has six inputs, numbers k0 through k5.

05:42.920 --> 05:47.280
And for each configuration, it constructs the corresponding curve y,

05:48.120 --> 05:57.040
calculates the distance between observed data points and the curve, and outputs a single number, the particular value of the loss.

05:57.760 --> 06:03.960
Our job then becomes finding the configuration of k's that yields a minimum loss or

06:04.840 --> 06:07.840
minimizing the loss function with respect to the coefficients.

06:07.920 --> 06:15.920
Then, plugging these optimal k's into the general equation for the curve will give us the best curve of describing the data.

06:16.520 --> 06:23.120
All right, great, but how do we find this magic configuration of k's that minimizes the loss?

06:23.560 --> 06:25.560
Well, we might need some help.

06:25.800 --> 06:32.520
Let's build a machine called Curve Fitter 6000, designed to simplify manual calculations.

06:32.520 --> 06:39.800
It is equipped with six adjustable knobs for k0 through k5, which we can freely turn.

06:40.120 --> 06:47.120
To begin, we initialize the machine with our data points and then, for each setting of the knobs,

06:47.120 --> 06:50.120
it will evaluate the curve y of x,

06:50.600 --> 06:56.680
compute the distance from it to the data points and print out the value of the loss function.

06:57.320 --> 07:01.640
Now, we can begin twisting the knobs in order to find the minimum loss.

07:02.280 --> 07:08.840
For example, let's start with some initial setting and slightly notch knob number one to the right.

07:09.240 --> 07:15.880
The resulting curve changed as well, and we can see that the value of the loss function slightly decreased.

07:16.440 --> 07:18.840
Great, it means we are on the right track.

07:19.320 --> 07:23.000
Let's turn knob number one in the same direction once again.

07:23.000 --> 07:26.840
Uh-oh, this time the fit gets worse and the loss function increases.

07:27.720 --> 07:30.760
Apparently, that last notch was a bit too much.

07:30.760 --> 07:35.320
So, let's revert the knob to the previous position and try knob two.

07:35.320 --> 07:38.680
And we can keep doing this iteratively many, many times,

07:39.240 --> 07:45.400
nudging each individual knob one at a time to see whether the resulting curve is a better fit.

07:45.560 --> 07:53.000
This is a so-called random perturbation method, since we are essentially wandering in the dark,

07:53.000 --> 07:58.200
not knowing in advance how each adjustment will affect the loss function.

07:58.200 --> 08:01.320
This would certainly work, but it's not very efficient.

08:01.320 --> 08:06.280
Is there a way we can be more intelligent about the knob adjustments?

08:06.280 --> 08:10.760
In the most general case, when the machine is a complete black box,

08:10.760 --> 08:14.920
nothing better than a random perturbation is guaranteed to exist.

08:14.920 --> 08:21.160
However, a great deal of computations, including what's carried out under the hood of our curvefitter,

08:21.880 --> 08:26.120
have a special property to them, something called differentiability

08:26.120 --> 08:30.600
that allows us to compute the optimal knob setting much more efficiently.

08:31.160 --> 08:35.640
We will dive deeper into what differentiability means in just a minute.

08:35.640 --> 08:40.360
But for now, let's quickly see the big picture overview of where we are going.

08:41.160 --> 08:48.040
Our goal would be to upgrade the machine so that it would have a tiny screen next to each knob.

08:48.600 --> 08:54.280
And for any configuration, those screens should say which direction you need to

08:54.280 --> 08:59.240
nudge each knob in order to decrease the loss function and by how much.

09:00.200 --> 09:01.400
Think about it for a second.

09:02.200 --> 09:08.040
We are essentially asking the machine to predict the future and estimate the effect

09:08.040 --> 09:13.880
of the knob adjustment on the loss function without actually performing that adjustment,

09:13.880 --> 09:18.680
calculating the loss and then reverting the knob back like we did previously.

09:19.240 --> 09:23.800
Wouldn't this glance into the future violate some sort of principle?

09:24.600 --> 09:29.640
After all, we are jumping to the result of the computation without performing it.

09:30.280 --> 09:31.720
Sounds like cheating, right?

09:32.920 --> 09:37.960
Well, it turns out that this idea lies on a very simple mathematical foundation

09:37.960 --> 09:41.320
so let's spend the next few minutes building it up from scratch.

09:43.560 --> 09:46.840
All right, let's consider a simpler case first.

09:46.840 --> 09:49.800
Where we freeze five out of six knobs.

09:49.800 --> 09:54.680
For example, suppose someone tells you that the rest of them are already in the optimal position.

09:55.240 --> 09:59.960
So all you need to do is to find the best value for one remaining knob.

09:59.960 --> 10:05.560
Essentially, the machine now has only one variable parameter k1 that we can tweak.

10:06.120 --> 10:09.480
And so the loss function is also a simpler function

10:09.480 --> 10:15.480
which accepts one number, the knob setting, and outputs another number, the loss value.

10:15.480 --> 10:21.960
As a function of one variable, it can be conveniently visualized as a graph in a two-dimensional plane

10:21.960 --> 10:25.720
which captures the relationship between the input and the output.

10:25.720 --> 10:32.200
For example, it may have this shape right here and our goal is to find this value of k1

10:32.200 --> 10:35.080
which corresponds to the minimum of the loss function.

10:35.080 --> 10:38.680
But we don't have access to the true underlying shape.

10:38.680 --> 10:42.120
All we can do is to set the knob at a chosen position

10:42.680 --> 10:46.360
and kind of query the machine for the value of the loss.

10:46.360 --> 10:52.280
In other words, we can only sample individual points along the function we're trying to minimize.

10:52.840 --> 10:58.440
And we are essentially blind to how the function behaves in between the known points

10:58.440 --> 11:00.040
before we sample them.

11:00.040 --> 11:03.560
But suppose we would like to know something more about the function.

11:03.560 --> 11:05.640
Not just each value at each point.

11:06.520 --> 11:10.520
For example, whether at this point the function is going up or down.

11:11.160 --> 11:14.760
This information will ultimately guide our adjustments.

11:15.320 --> 11:20.040
Because if you know that the function is going down as you increase the input,

11:20.040 --> 11:23.080
turning the knob to the right is a safe bet,

11:23.080 --> 11:27.000
since you are guaranteed to decrease the loss with this manipulation.

11:27.560 --> 11:31.640
Let's put this notion of going up or down around a point

11:31.640 --> 11:33.880
on a stronger mathematical ground.

11:33.880 --> 11:38.360
Suppose we have just sampled the point x0, y0 on this graph.

11:38.360 --> 11:43.640
What we can do is increase the input by a small amount delta x.

11:43.640 --> 11:48.360
This new adjusted input will result in a new value of y,

11:48.360 --> 11:52.040
which will differ from the old value by some delta y.

11:52.600 --> 11:56.440
This delta depends on the magnitude of our adjustment.

11:56.440 --> 12:01.080
For example, if we take a step delta x, which is 10 times smaller,

12:01.080 --> 12:05.240
delta y will also be approximately 10 times as small.

12:06.680 --> 12:11.960
This is why it makes sense to take the ratio delta y over delta x,

12:11.960 --> 12:16.280
the amount of change in the output per unit change in the input.

12:16.920 --> 12:21.640
Graphically, this ratio corresponds to a slope of a straight line.

12:21.640 --> 12:26.360
Going through the points x0, y0 and x0 plus delta x,

12:26.440 --> 12:27.960
y0 plus delta y.

12:28.920 --> 12:32.680
Notice that as we take smaller and smaller steps,

12:32.680 --> 12:37.400
this straight line will more and more accurately align with the graph

12:37.400 --> 12:40.200
in the neighborhood of the point x0, y0.

12:40.920 --> 12:47.320
Let's take a limit of this ratio as delta x goes to infinitely small values.

12:47.320 --> 12:51.800
Then this limiting case value, which this ratio converges to

12:51.800 --> 12:57.640
for infinitesimally small delta x's, is what is called the derivative oa function,

12:58.200 --> 13:01.320
and it is denoted by dy over dx.

13:01.960 --> 13:05.960
Visually, the derivative oa function at some point

13:05.960 --> 13:09.880
is the slope of the line that is tangent to the graph,

13:09.880 --> 13:13.400
and thus corresponds to the instantaneous rate of change,

13:13.400 --> 13:17.160
or steepness of that function around that point.

13:17.160 --> 13:21.640
But different points along the graph might have different steepness values,

13:21.960 --> 13:26.520
so the derivative of the entire function is not a single number.

13:26.520 --> 13:32.520
In fact, the derivative dy by dx is itself a function of x

13:32.520 --> 13:40.200
that takes an arbitrary value of x and outputs the local steepness of y of x at that point.

13:40.200 --> 13:45.240
This definition assigns to every function its derivative alter ego.

13:45.240 --> 13:49.160
Another function operating on the same input domain,

13:49.160 --> 13:54.120
which carries information about the steepness of the original function.

13:54.120 --> 13:55.880
There is a bit of a subtlety.

13:55.880 --> 13:59.000
Strictly speaking, the derivative may not exist

13:59.000 --> 14:02.280
if the function doesn't have a steepness around some point.

14:02.920 --> 14:06.680
For example, if it has sharp corners or discontinuities.

14:07.720 --> 14:12.360
However, for the remainder of the video, we are going to assume that all functions we are

14:12.360 --> 14:16.600
dealing with are smooth, so that the derivative always exists.

14:17.560 --> 14:21.080
This is a reasonable claim, because we can control

14:21.080 --> 14:24.520
what sort of functions go into our models when we build them.

14:25.080 --> 14:29.960
And people usually restrict everything to smooth or differentiable functions

14:29.960 --> 14:32.120
to make all the math work out nicely.

14:32.680 --> 14:33.800
All right, great.

14:33.800 --> 14:37.720
Now, along with the underlying loss as a function of k1,

14:37.720 --> 14:42.040
which is hidden from us, we can also reason about its derivative.

14:42.680 --> 14:46.120
Another function of k1, which we also don't know,

14:46.120 --> 14:49.800
that is equal to the steepness of the loss function at that point.

14:51.160 --> 14:55.400
Let's suppose that similarly to how we can query the loss function

14:55.400 --> 14:59.000
by running our machine and obtaining individual samples.

14:59.720 --> 15:04.040
There is a mechanism for us to sample the derivative function as well.

15:05.400 --> 15:11.880
So, for every input value of k1, the machine will output the value of the loss

15:11.880 --> 15:15.640
and the local steepness of the loss function around that point.

15:16.280 --> 15:20.600
Notice that this derivative information is exactly the sort of

15:20.600 --> 15:25.080
look into the future we were looking for to make smarter knob adjustments.

15:25.720 --> 15:30.600
For example, let's use it to efficiently find the optimal value of k1.

15:31.400 --> 15:33.000
What we can do is the following.

15:33.880 --> 15:36.600
First, start at some random position.

15:37.560 --> 15:40.120
Ask the machine for the value of the loss

15:40.120 --> 15:43.320
and the derivative of the loss function at that position.

15:44.440 --> 15:48.600
Take a tiny step in the direction opposite of the derivative.

15:49.240 --> 15:53.240
If the derivative is negative, it means that the function is going down.

15:53.960 --> 15:56.520
And so, if we want to arrive at the minimum,

15:56.520 --> 16:00.440
we need to move in the direction of increasing value of k1.

16:01.080 --> 16:06.280
Repeat this procedure until you reach the point where the derivative is zero,

16:06.280 --> 16:10.600
which essentially corresponds to the minimum where the tangent line is flat.

16:11.240 --> 16:15.000
Essentially, each adjustment in such a guided fashion

16:15.000 --> 16:21.080
works kind of like a ball rolling down the hill along the graph until it reaches a valley.

16:23.480 --> 16:27.560
Although in the beginning we froze five out of six knobs for simplicity,

16:28.120 --> 16:31.880
this process is easily carried out to higher dimensions.

16:32.840 --> 16:38.600
For example, suppose now we are free to tweak two different knobs, k1 and k2.

16:39.640 --> 16:45.800
The loss would become a function of two variables, which can be visualized as a surface.

16:46.680 --> 16:48.680
But what about the derivative?

16:48.680 --> 16:52.920
Recall that by definition, the derivative at each point

16:52.920 --> 16:57.800
tells us how the output changes per unit change of the input.

16:57.800 --> 16:59.880
But now we have two different inputs.

17:00.440 --> 17:03.720
Should we nudge only k1, k2 or both?

17:05.640 --> 17:09.800
Essentially, our function will have two different derivatives

17:10.440 --> 17:17.000
that are usually called partial derivatives because of this ambiguity which input to nudge.

17:17.000 --> 17:21.400
Namely, when we have two knobs, the derivative of the loss function

17:21.400 --> 17:25.080
with respect to parameter k1 is written like this.

17:25.320 --> 17:33.480
It is how much the output changes per unit change in k1 if you hold k2 constant.

17:34.120 --> 17:39.320
And conversely, this expression tells you the rate of change of the output

17:39.320 --> 17:43.480
if you hold k1 constant and slightly nudge k2.

17:44.040 --> 17:50.760
Geometrically, you can imagine slicing the surface with planes parallel to the axes,

17:50.760 --> 17:53.960
intersecting at the point of interest k1 k2.

17:54.600 --> 18:00.440
So that each of the two cross sections is like a one-dimensional graph of the loss

18:00.440 --> 18:04.680
as a function of one variable while the other one is kept constant.

18:05.320 --> 18:09.240
Then the slope of a tangent line at each cross section

18:09.240 --> 18:13.640
will give you a corresponding partial derivative of the loss at that point.

18:14.760 --> 18:18.520
While thinking about partial derivatives as two separate surfaces,

18:19.080 --> 18:22.360
one for each variable, is a perfectly valid way.

18:23.080 --> 18:29.880
People usually plug the two different values into a vector called a gradient vector.

18:29.880 --> 18:36.120
Essentially, this is a mapping from two input values to another two numbers

18:36.120 --> 18:42.680
where the first signifies how much the output changes per tiny change in the first input.

18:43.400 --> 18:45.400
And similarly, for the second input.

18:46.520 --> 18:50.680
Geometrically, this vector points in the direction of steepest ascent.

18:51.480 --> 18:56.360
So if you want to minimize a function, like in the case for our loss,

18:57.080 --> 19:01.480
we need to take steps in the direction opposite to this gradient.

19:02.840 --> 19:09.960
This iterative procedure of nudging the parameters in the direction opposite of the gradient vector

19:09.960 --> 19:13.720
is called gradient descent, which you have probably heard of.

19:13.720 --> 19:18.520
This is analogous to a ball rolling down the hill for the two-dimensional case.

19:18.520 --> 19:23.640
And the partial derivatives essentially tell you which direction is downhill.

19:24.440 --> 19:29.080
Going beyond two dimensions is impossible to visualize directly,

19:29.080 --> 19:31.480
but the math stays exactly the same.

19:32.440 --> 19:37.000
For instance, if we are now free to tweak all the six knobs,

19:37.000 --> 19:41.160
the loss function is a hyper surface in six dimensions.

19:41.800 --> 19:45.800
And the gradient vector now has six numbers packed into it.

19:46.360 --> 19:50.280
But it still points in the direction of steepest ascent.

19:50.280 --> 19:55.160
So if we iteratively take small steps in the direction opposite to it,

19:55.880 --> 20:00.440
we are going to roll the ball down the hill in six dimensions

20:00.440 --> 20:03.640
and eventually reach the minimum of the loss function.

20:04.520 --> 20:06.040
Great, let's back up a bit.

20:06.760 --> 20:11.160
Remember how we were looking for ways to add screens next to each knob

20:11.160 --> 20:14.280
that would give us the direction of optimal adjustment?

20:15.240 --> 20:20.600
Well, it is essentially nothing more but the components of the gradient vector.

20:20.600 --> 20:25.160
If at a particular configuration, the partial derivative of the loss

20:25.160 --> 20:32.520
with respect to k1 is positive, it means that increasing k1 will lead to increased loss.

20:33.080 --> 20:37.400
So we need to decrease the value of the knob by turning it to the left.

20:38.040 --> 20:40.680
And similarly for all other parameters.

20:41.640 --> 20:46.760
This is how the derivatives serve as these windows into the future

20:46.760 --> 20:51.720
by providing us with information about local behavior of the function.

20:51.720 --> 20:55.160
And once we have a way of accessing the derivative,

20:55.160 --> 21:01.160
we can perform gradient descent and efficiently find the minimum of the loss function,

21:01.160 --> 21:03.720
thus solving the optimization problem.

21:04.280 --> 21:07.160
However, there is an elephant in a room.

21:07.160 --> 21:11.960
So far we have implicitly assumed the derivative information is given to us.

21:12.520 --> 21:16.200
Or that we can sample the derivative at a given point.

21:16.200 --> 21:19.560
Similarly to how we sample the loss function itself

21:19.560 --> 21:21.880
by running the calculation of the machine.

21:21.880 --> 21:24.600
But how do you actually find the derivative?

21:24.600 --> 21:29.320
As we will see further, this is the main purpose of the back propagation algorithm.

21:30.120 --> 21:35.560
Essentially, the way we find derivatives of arbitrarily complex functions is the following.

21:36.200 --> 21:39.960
First, there are a handful of building blocks to begin with.

21:39.960 --> 21:43.960
Simple functions, derivatives of which are known from calculus.

21:44.680 --> 21:48.600
These are the kind of derivative formulas you often memorize in college.

21:49.400 --> 21:55.880
For example, if the function is linear, it's pretty clear that its derivative will be a constant,

21:55.880 --> 22:01.640
equal to the slope of that line everywhere, which coincides with its own tangent line.

22:02.600 --> 22:08.120
A parabola x squared becomes more steep as you increase x.

22:08.120 --> 22:10.680
And its derivative is actually 2x.

22:11.560 --> 22:16.680
In fact, there is a more general formula for the derivative of x to the power of n.

22:17.480 --> 22:22.760
Similarly, derivatives of the exponent and logarithm can be written down explicitly.

22:23.640 --> 22:28.200
But these are just individual examples of simple, well-known functions.

22:28.840 --> 22:35.960
In order to compute arbitrary derivatives, we need a way to combine such atomic building blocks

22:35.960 --> 22:39.480
together. There are a few rules how to do it.

22:40.280 --> 22:46.440
For instance, the derivative of a sum of two functions is the sum of the derivatives.

22:46.440 --> 22:50.680
There is also a formula for the derivative of a product of two functions.

22:51.560 --> 22:57.320
This gives you a way to compute things like the derivative of 3x squared minus equal to the power

22:57.320 --> 23:04.600
of x. But to complete the picture and to be able to find derivatives of almost everything,

23:04.600 --> 23:11.800
we need one other rule called the chain rule, which powers the entire field of machine learning.

23:12.520 --> 23:17.320
It tells you how to compute the derivative of a combination of two functions,

23:17.320 --> 23:22.680
when one of them is an input to another. Here is a way to reason about this.

23:23.640 --> 23:29.800
Suppose you take one of those simpler machines, which receives a single input x that you can

23:29.800 --> 23:38.600
vary with an ALP, and spits out an output, j of x. Now, you take a second machine of this kind,

23:38.600 --> 23:46.600
which performs a different function, f of x. What would happen if you connect them in sequence,

23:46.600 --> 23:52.520
so that the output of the first machine is fed into the second one as an input?

23:53.480 --> 24:00.680
Notice that such a construction can be thought of as a single function, which also receives one

24:00.680 --> 24:07.240
input number and gives an output by computing a more complicated function, which is a composition

24:07.240 --> 24:14.280
of the two simpler functions. In fact, if you put a black box around it to conceal the fact

24:14.280 --> 24:21.160
that there are actually two machines operating sequentially, you can treat it as a single machine

24:21.160 --> 24:28.040
and ask, well, if I notch the input on one end, how will it affect the output on another end?

24:28.760 --> 24:32.600
In other words, what is the derivative of the resulting function?

24:33.880 --> 24:40.520
Suppose we know the individual derivatives of the two machines, f and j. If the knob is set at

24:40.520 --> 24:49.560
some value x, local steepness of the first function is evaluated at x. However, the number that is

24:49.560 --> 24:55.720
fed into the second machine is not x, because it was already processed by the first function.

24:56.760 --> 25:03.480
So, the thing that is being plugged into the second function is j of x. And so, the local rate of

25:03.480 --> 25:12.360
change of the second machine is thus the derivative of f evaluated at the point j of x. Now, imagine

25:12.360 --> 25:19.640
you notch the knob x by a tiny amount, delta. That input notch, when it comes out of the first machine,

25:19.640 --> 25:26.120
will be multiplied by the derivative of j, since the derivative is the rate of change in the output

25:26.120 --> 25:32.840
per unit change of the input. So, after the first function, the output will increase by delta,

25:32.840 --> 25:40.360
multiplied by the derivative of j. This expression is essentially a tiny notch in the input to the

25:40.360 --> 25:47.640
second machine, whose derivative at that point is given by this expression. This means that for

25:47.640 --> 25:56.360
each delta increase in the input, we bump the output by this much. Hence, the derivative when

25:56.360 --> 26:04.360
you divide that by delta will look like this. You can think about it as a set of three interconnected

26:04.360 --> 26:11.800
cog wheels, where the first one represents the input knob x. And the other two wheels are functions,

26:11.800 --> 26:18.600
j of x and f of j of x, respectively. When you notch the first wheel, it induces a

26:18.600 --> 26:24.360
notch in the middle wheel and the amplitude of that change is given by the derivative of j,

26:25.080 --> 26:30.840
which in turn causes the third wheel to rotate, and the amplitude of that resulting

26:30.840 --> 26:38.680
notch is given by changing the derivatives together. Alright, great. Now we have a straightforward way

26:38.680 --> 26:45.320
of obtaining a derivative of any arbitrarily complex function, as long as it can be decomposed

26:45.320 --> 26:51.880
into building blocks. Simple functions with explicit derivative formulas, such as summations,

26:51.880 --> 26:59.080
multiplications, exponents, logarithms, etc. But how can it be used to find the best curve

26:59.080 --> 27:05.320
using our curve fitter? The big picture we are aiming for is the following. For each of our

27:05.320 --> 27:12.040
parameter knobs, we will write down its effect on the loss in terms of simple, easily differentiable

27:12.040 --> 27:19.720
operations. Once we have that sequence of building blocks, no matter how long, we should be able to

27:19.720 --> 27:25.960
sequentially apply the chain rule to each of them in order to find the value of the derivative

27:25.960 --> 27:32.520
of the loss function with respect to each of the input knobs and perform iterative gradient descent

27:32.520 --> 27:39.720
to minimize the loss. Let's see an example of this. First, we are going to create a knob

27:39.720 --> 27:46.120
for each number the loss function can possibly depend on. This obviously includes the parameters,

27:46.680 --> 27:53.240
but there is also the data itself, coordinates of points to which we are fitting the curve

27:53.240 --> 28:00.120
in the first place. Now, during optimization, the data points are set in stone, so changing them

28:00.120 --> 28:07.480
in order to obtain a lower loss would make no sense. However, for conceptual purposes,

28:07.480 --> 28:14.360
we can think about these values as fixed knobs set in one position so that we cannot nudge them.

28:15.320 --> 28:21.880
Once we have all the existing numbers being fed into the machine, we can start to break down the

28:21.880 --> 28:29.640
loss calculation. Remember, by definition, it is the sum of squared vertical distances

28:29.640 --> 28:37.240
from each point to the curve parameterized by k's. So, for instance, let's take the first

28:37.320 --> 28:47.560
data point, x1, y1, multiply the x coordinate by k1, add that to the squared value of x1 multiplied

28:47.560 --> 28:56.440
by k2, and so on for other k's, including the constant term k0. This sum of weight and powers of

28:56.440 --> 29:05.960
x1 is the value of y predicted by the current curve, f of x1. Let's call it y1 hat. Next,

29:05.960 --> 29:12.440
we need to take the squared difference between the actual value and the predicted value. This is

29:12.440 --> 29:18.600
how much the first data point contributes to the resulting value of the loss function.

29:19.800 --> 29:27.480
Repeating the same procedure for all remaining data points and summing up the resulting squared

29:27.480 --> 29:35.800
distances gives us the overall total loss that we are trying to minimize. The computation we just

29:35.800 --> 29:42.920
performed, finding the value of the loss for a given configuration of parameter and data knobs,

29:43.560 --> 29:51.400
is known as the forward step. The entire sequence of calculations can be visualized

29:51.400 --> 29:58.920
as this kind of computational graph, where each node is some simple operation like addition or

29:58.920 --> 30:07.400
multiplication. Forward step then corresponds to computations flowing from left to right.

30:07.400 --> 30:14.040
But to perform optimization, we also need information about gradients, how each node

30:14.040 --> 30:21.880
influences the loss. Now we are going to do what's known as the backward step, and unroll the sequence

30:21.880 --> 30:29.240
of calculations in reverse order to find derivatives. What makes the backward step possible

30:29.240 --> 30:35.400
is the fact that every node in our compute graph is an easily differentiable operation.

30:35.400 --> 30:42.920
Think of individual nodes as these tiny machines which simply add, multiply or take powers. We

30:42.920 --> 30:50.040
know their derivatives, and because their outputs are connected sequentially, we can apply the chain

30:50.040 --> 30:58.840
rule. This means that for each node we can find its gradient, the partial derivative of the output

30:58.840 --> 31:07.320
loss with respect to that node. Let's see how it can be done. Consider a region of the compute

31:07.320 --> 31:15.160
graph, where two number nodes A and B are being fed into a machine that performs addition, and its

31:15.160 --> 31:22.040
result A plus B is further processed by the system to compute the overall output L.

31:23.240 --> 31:29.640
Suppose we already computed the gradient of A plus B earlier, so that we know how

31:29.640 --> 31:37.480
nudging the sum will affect the output. The question is, what are individual gradients of A and B?

31:37.800 --> 31:46.600
Well, intuitively, if you nudge A by sum amount, A plus B will be nudged by this same amount,

31:46.600 --> 31:53.000
so the gradient or the partial derivative of the loss with respect to A is the same as the gradient

31:53.000 --> 32:00.440
of the sum, and similarly for B. This can be seen more formally by writing down the chain rule

32:00.440 --> 32:07.240
and noticing that the derivative of A plus B with respect to A is just one. In other words,

32:07.320 --> 32:13.720
when you encounter this situation in the compute graph, then the gradient of the sum

32:13.720 --> 32:20.200
just simply propagates into the gradients of the nodes that plug into the sum machine.

32:20.200 --> 32:26.040
Another possible scenario is when A and B are multiplied. Just like before,

32:26.040 --> 32:32.280
suppose we know the gradient of their product because it was computed before. In this case,

32:32.280 --> 32:39.320
individual nudge to A will be scaled by a factor of B, so the product will be nudged

32:39.320 --> 32:46.600
B times as much, which propagates into the output. So, whatever the derivative of the

32:46.600 --> 32:53.720
output with respect to the product of A B is, the output derivative with respect to A

32:53.720 --> 33:00.440
will get scaled by a factor of B, and vice versa for the gradient of B. Once again,

33:00.440 --> 33:07.800
it can be seen more formally by examining the chain rule. In other words, the multiplication node

33:07.800 --> 33:14.920
in the compute graph distributes the downstream gradient across incoming nodes by multiplying

33:14.920 --> 33:21.960
it crossways by their values. Similar rules can be easily formulated for other building block

33:21.960 --> 33:28.360
calculations, such as raising a number to a power or taking the logarithm. Finally,

33:28.360 --> 33:33.960
when a single node takes part in multiple branches of the compute graph, gradients from the

33:33.960 --> 33:39.640
corresponding branches are simply added together. Indeed, suppose you have the following structure

33:39.640 --> 33:46.280
in the graph, where the C-minode A plugs into two different operations that contribute to

33:46.280 --> 33:53.560
the overall loss. Then, if you nudge A by delta, the output will be simultaneously nudged by this

33:53.560 --> 34:00.520
derivative from the first branch and this derivative from the second branch. So, the overall effect of

34:00.520 --> 34:07.320
nudging A will be the sum of the two gradients. Alright, great. Now that we have constructed

34:07.320 --> 34:14.360
a computational graph and established how to process individual chunks of it, we can just

34:14.360 --> 34:20.520
sequentially apply those rules starting from the output and working our way backwards.

34:21.480 --> 34:26.840
For instance, the rightmost node in the graph is the resulting value of the loss function.

34:26.840 --> 34:33.240
How does the incremental change in that node affect the output? Well, it is the output,

34:33.240 --> 34:39.960
so its gradient is by definition equal to 1. Next, the loss function is the sum of many delta

34:39.960 --> 34:46.200
y's squared. We know what to do with the summation node. It just copies whatever the gradient value

34:46.200 --> 34:52.840
is to the right of it into all incoming nodes. Consequently, the gradients of all delta y's

34:52.840 --> 35:00.600
squared will also be equal to 1. Each of those nodes is the squared value of the corresponding delta y

35:00.600 --> 35:05.880
and we know how to differentiate this squaring operation. The derivative of the loss function

35:05.880 --> 35:12.360
with respect to delta y1 will be 2 times the delta y1, which is just the number we found

35:12.360 --> 35:18.040
during the forward calculation. And we can keep doing this propagation of sequential derivative

35:18.040 --> 35:24.280
calculation backwards along our compute graph until we reach the leftmost nodes,

35:24.280 --> 35:30.360
which are the data and parameter knobs. The derivatives of the loss with respect to the input

35:30.360 --> 35:36.920
data don't really matter, but the derivatives with respect to the parameters is exactly what we want.

35:36.920 --> 35:43.400
Once these parameter gradients are found, we can perform one iteration of gradient descent.

35:43.400 --> 35:48.600
Namely, we are going to slightly tweak the knobs in the directions opposite to the gradient.

35:49.160 --> 35:55.000
The exact magnitude of each adjustment being the negative product of the gradient

35:55.000 --> 36:02.120
and some small number called the learning rate, for example, 0.01. Note that after the adjustment

36:02.120 --> 36:07.560
is performed, the configuration of the machine and the resulting loss are different.

36:08.280 --> 36:12.680
And so the old gradient values we found no longer hold.

36:13.720 --> 36:20.360
So we need to run the forward and backward calculations once again to obtain updated

36:20.360 --> 36:28.040
gradients and the new decreased loss. Performing this loop of forward pass, backward pass,

36:28.040 --> 36:33.640
notch, repeat is the essence of training every modern machine learning system.

36:34.200 --> 36:39.640
And exactly the same algorithm is used today in even the most complicated models.

36:39.640 --> 36:44.600
As long as the problem you are trying to solve with a given model architecture

36:44.600 --> 36:50.760
can be decomposed into individual operations that are differentiable, you can sequentially

36:50.760 --> 36:56.680
apply the chain rule many times to arrive at the optimal setting of the parameters.

36:56.680 --> 37:01.880
For instance, a feed-forward neural network is essentially a bunch of multiplications and

37:01.880 --> 37:07.560
summations with a few non-linear activation functions sprinkled between the layers.

37:07.560 --> 37:13.800
Each of those atomic computations is differentiable, so you can construct the compute graph

37:13.800 --> 37:21.080
and run the backward pass on it to find how each parameter, like connection weights between neurons,

37:21.080 --> 37:26.440
influence the loss function. And because neural networks, given enough neurons,

37:26.440 --> 37:33.080
can in theory approximate any function imaginable, we can create a large enough sequence of these

37:33.080 --> 37:39.000
building block mathematical machines to solve problems such as classifying images and even

37:39.000 --> 37:44.200
generating new text. This seems like a very elegant and efficient solution.

37:44.200 --> 37:50.280
After all, if you want to solve the optimization problem, derivatives tell you exactly which

37:50.280 --> 37:55.800
adjustments are necessary. But how similar is this to what the brain actually does?

37:56.520 --> 38:03.000
When we learn to walk, speak and read, is the brain also minimizing some sort of loss function?

38:03.000 --> 38:08.040
Does it calculate derivatives? Or could it be doing something totally different?

38:08.680 --> 38:13.320
In the next video, we are going to dive into the world of synaptic plasticity

38:13.320 --> 38:16.120
and talk about how biological neural networks learn.

38:17.080 --> 38:19.880
In keeping with the topic of biological learning,

38:19.880 --> 38:24.760
I'd like to take a moment to give a shout out to Shortform, a longtime partner of this channel.

38:25.720 --> 38:29.720
Shortform is a platform which lets you take your reading to the next level.

38:30.440 --> 38:37.080
They offer book guides, which are supercharged book summaries. Not only do you get the condensed

38:37.080 --> 38:43.720
version of all the key points, but they are also supplemented by ideas from related sources,

38:43.800 --> 38:49.160
such as other books and research papers. I really love this feature because it allows

38:49.160 --> 38:53.560
you to get the big picture overview and promotes the interlinking of ideas.

38:54.120 --> 38:59.240
The existing library contains books from a variety of genres, including science,

38:59.240 --> 39:05.000
education and technology, and new books are being added every week. Personally, I found

39:05.000 --> 39:09.880
Shortform to be really valuable when it comes to choosing books to read, as well as taking

39:09.880 --> 39:15.560
efficient notes. Don't hesitate to give it a try by following the link down in the description

39:15.560 --> 39:20.600
to get 5 days of unlimited access and 20% off the annual membership.

39:21.240 --> 39:25.320
If you enjoyed this video, press the like button, share it with your friends and colleagues,

39:25.320 --> 39:30.280
and subscribe to the channel if you haven't already. Stay tuned for more interesting topics

39:30.280 --> 39:46.200
coming up. Goodbye and thank you for the interest in the brain.

40:00.280 --> 40:01.660
you

