WEBVTT

00:00.000 --> 00:06.320
It is humanity's long-time dream to create machines that can think, but what exactly does it mean?

00:06.320 --> 00:11.120
One particular characteristic of intelligence is the ability to generalize knowledge

00:11.120 --> 00:17.680
and flexibly adapt it to new situations. Such generalization is indeed one of the cornerstone

00:17.680 --> 00:22.480
problems in modern machine learning. In this video, we are going to see how we can take

00:22.480 --> 00:28.720
biological organization over hippocampus, a brain structure involved in memory and navigation,

00:28.720 --> 00:34.000
as an inspiration in order to construct a computational model that can learn to build

00:34.000 --> 00:38.880
abstractions and generalizations, as well as the insights we can draw from this model,

00:38.880 --> 00:43.360
both about our own brains and the field of artificial intelligence.

00:43.360 --> 00:48.320
Before we begin, I'd like to warn you that this video is the continuation of the previous video

00:48.320 --> 00:54.080
in the series on cognitive maps. Last time we explored neurobiological background of hippocampal

00:54.080 --> 00:59.360
computations and introduced some general principles, so if you haven't seen it, I highly

00:59.360 --> 01:04.080
recommend you check it out before watching this one, since we are going to build up from there.

01:04.640 --> 01:06.320
If you're interested, stay tuned.

01:13.200 --> 01:19.200
Imagine you're an agent that walks around the world and his only goal is to find rewards.

01:19.200 --> 01:24.160
From an evolutionary perspective, you can think about such an agent as an early organism,

01:24.160 --> 01:30.080
which needs to look for food or mates. Now, as an agent, you have a certain repertoire

01:30.080 --> 01:35.360
of actions that you can take. For example, activate a sequence of muscles to move in a particular

01:35.360 --> 01:42.400
direction. To choose the most rewarding actions, you need to be able to predict the action outcomes,

01:42.400 --> 01:47.200
and that effectively requires a mental model of the surrounding environment.

01:47.200 --> 01:52.080
Existence of such a model allows you to run mental singulations in your head

01:52.080 --> 01:57.680
to weigh the actions. For example, what would happen if I go straight or is it better to turn

01:57.680 --> 02:03.360
right? Over the course of your lifetime, as you encounter a variety of different environments,

02:03.360 --> 02:09.280
initially you might build an entangled, indivisible model for each, without necessarily linking

02:09.280 --> 02:14.560
different models to each other. However, if you're being optimal in your representations,

02:14.560 --> 02:20.000
at some point you realize, wait a minute, all these models I've built so far actually have

02:20.000 --> 02:26.720
an awful lot in common. Indeed, walls that block your way, doors that allow you to go through the

02:26.720 --> 02:33.680
walls and even just the structure of an open 2D space itself all work similarly in every environment,

02:33.680 --> 02:40.640
so these common elements can be easily reused. In other words, it makes sense to break up or

02:40.640 --> 02:46.400
factor each model into its building blocks. For example, building blocks of space, of boundaries,

02:46.400 --> 02:52.560
rewards, etc. Once these building blocks are learned, we can rearrange and mix them in different

02:52.560 --> 02:59.200
configurations to build new models of the world on the fly, and thus generate flexible behavior.

03:00.560 --> 03:05.600
As you might remember from part 1, this is exactly what my mailing hippocampus does,

03:05.600 --> 03:11.120
and we can find neurobiological evidence for this process in responses of individual cells.

03:11.760 --> 03:15.360
Now, the question is, can we teach a machine to do the same?

03:16.320 --> 03:22.320
To make the task easier for an artificial system, let's formulate it as a prediction problem.

03:22.320 --> 03:28.240
Namely, the model will receive a sequence of observations, along with the sequence of actions

03:28.240 --> 03:34.000
that led to them, and learn to correctly predict the next observation in the sequence.

03:34.000 --> 03:39.280
It actually makes a lot of sense biologically. There is a great deal of data suggesting that

03:39.280 --> 03:44.400
the main purpose of the brain may be to predict the incoming stimuli and try to minimize the

03:44.400 --> 03:50.240
prediction error, a theory called predictive coding. For example, consider the following

03:50.240 --> 03:56.480
sequence of observations and actions. Can you tell me what should be the next element in the sequence?

03:57.440 --> 04:03.760
Seems impossible, right? However, what if I told you that those actions, one through four,

04:03.840 --> 04:11.360
actually stand for directions, north, west, south, and east? Now, the task becomes much easier.

04:11.920 --> 04:17.280
Because you know the rules how to chain these actions together, you can predict the next

04:17.280 --> 04:22.640
observation to be the same as the first one, since you know you essentially closed a loop.

04:22.640 --> 04:29.040
In other words, knowing the structure of space significantly simplifies the prediction problem.

04:29.040 --> 04:34.400
But the model, of course, would not know this underlying structure since it would be no fun.

04:34.400 --> 04:40.240
Instead, it would need to extract repeating patterns in order to somehow infer this structure

04:40.240 --> 04:46.000
of the underlying world from sequences of observations and actions. For example, after

04:46.000 --> 04:51.440
seeing a large number of sequences like these, it should infer the rules how different actions

04:51.440 --> 04:55.600
relate to each other, which is equivalent to constructing the structure of space.

04:55.600 --> 05:02.400
It's important to point out that although I'm saying things like the model will learn the

05:02.400 --> 05:10.240
underlying structure of the world, it is not told to do that exactly. The model has no other goal,

05:10.240 --> 05:15.120
so to speak, other than predicting the next observation in the sequence. In essence,

05:15.120 --> 05:20.400
it is just a fancy mathematical expression with a large number of parameters that takes the set

05:20.400 --> 05:25.440
of numbers encoding the observations and actions, performs computation on them, and spits out

05:25.440 --> 05:31.680
another set of numbers corresponding to the next predicted observation. But because we train it to

05:31.680 --> 05:37.840
minimize this prediction error, and since these observations are not random but come from some

05:37.840 --> 05:43.920
structured world, the optimal solution to this prediction problem is to construct some structural

05:43.920 --> 05:49.920
representation of this world, which underlies the regularities in the observations. So we simply

05:49.920 --> 05:55.840
expect the knowledge about the structure to emerge as a result of optimization. But how should the

05:55.840 --> 06:02.960
model look like? Well, because we are free to choose whichever architecture we want, it is reasonable

06:02.960 --> 06:09.520
to draw inspiration from an existing biological machine that solves this problem on a daily basis,

06:09.520 --> 06:15.840
the hippocampal formation. In the last video, we saw how the hippocampus receives two streams of

06:15.840 --> 06:22.400
inputs, sensory, the what am I seeing information, coming from the lateral entorhinal cortex,

06:22.400 --> 06:29.280
and structural, the where am I information, from medial entorhinal cortex. They are then combined

06:29.280 --> 06:36.960
in the hippocampus into a conjoined representation. Similarly, our model will have an analog of

06:36.960 --> 06:42.560
medial entorhinal area, responsible for keeping track of current location in the world.

06:43.520 --> 06:49.920
Let's call it a position module. At every point in time, it will receive an action

06:49.920 --> 06:56.640
and use it to compute the estimate of the current location, the best guess of where it is in space.

06:56.640 --> 07:00.800
You can think of this positional information as being encoded by the pattern of neuron

07:00.800 --> 07:07.840
activations inside of it. Note that the position module operates purely with actions and doesn't

07:07.840 --> 07:13.280
receive any information about the sensory observations. Similarly, how if you close your

07:13.280 --> 07:19.040
eyes and walk around the room, you have a rough idea of where you are located, even though you

07:19.040 --> 07:26.480
don't see anything. This is because your brain is able to accumulate self-movement vectors and

07:26.480 --> 07:33.360
estimate the position, a process known as path integration. So once the model is trained,

07:33.360 --> 07:36.960
we expect our position module to be able to do the same.

07:39.680 --> 07:46.000
Another crucial component is the hippocampus itself, which binds the where information

07:46.000 --> 07:52.240
with what. This binding is effectively forming an association between the two inputs.

07:53.360 --> 07:59.280
So we need to add a memory module that would receive the positional information provided

07:59.280 --> 08:05.200
by the position module together with this stream of sensory inputs and store each

08:05.200 --> 08:11.360
encountered combination in memory. Essentially, it memorizes the associations between position and

08:11.360 --> 08:19.040
observation. I was at X when I saw Y. But storing memories would be useless if we couldn't retrieve

08:19.040 --> 08:25.600
them. Importantly, since this is an associative memory module, it should be able to reconstruct

08:25.600 --> 08:33.040
the full memory from partial information. For example, we could provide it with just the position

08:33.040 --> 08:38.560
and it would go and search in all of the stored memories which observations were accompanied

08:38.560 --> 08:44.800
by this position. So essentially answering the question, what did I see last time I was here?

08:45.520 --> 08:51.200
And similarly, we could provide it with just the sensory observation and it would retrieve position.

08:51.200 --> 08:58.800
Where was I last time I saw this? Now we have all the necessary components

08:58.800 --> 09:05.600
to solve the prediction problem. Let's walk step by step on what the trained model will do to come

09:05.600 --> 09:11.520
up with a successful prediction when walking, for example, on a family tree. Remember, it should be

09:11.520 --> 09:18.720
capable of learning any type of structure, not just the four connected grids. So we start on John,

09:18.720 --> 09:24.720
transition to Mary via a sister action, and then to Kate via a daughter action.

09:25.360 --> 09:32.320
Finally, we give the model the action labeled as Uncle and ask it to make a prediction and what's

09:32.320 --> 09:38.560
happening under the hood is the following. At first, the position module has some initial belief

09:38.560 --> 09:44.720
about the current location which is combined with John and this combination is stored in the memory

09:44.720 --> 09:52.000
module. Next, the sister action is fed into the position module which comes up with a new

09:52.000 --> 09:58.720
belief about location that is combined with Mary and the corresponding conjunction is stored in

09:58.720 --> 10:06.160
memory. Similarly, daughter action is used to update the internal state of the position module

10:06.160 --> 10:13.600
which is combined with Kate and sent to the memory module. Finally, the uncle action is fed into the

10:13.600 --> 10:19.680
position module. Importantly, the resulting positional information, the pattern of neuron

10:19.680 --> 10:25.840
activations is the same as the one we started with. This is because after the model is trained on many

10:25.840 --> 10:32.960
family trees underlaying by the same rules, the position module is configured to always return

10:32.960 --> 10:39.840
to the same position when we make loops like these. In other words, the general laws governing the

10:39.840 --> 10:46.320
transition logic on the world graph become embedded into the rules of how the position module

10:46.320 --> 10:53.280
updates its state. After performing path integration correctly, we return to this starting position

10:54.240 --> 11:00.320
but there is no corresponding sensory observation to memorize. Instead, because the model has reached

11:00.320 --> 11:06.640
the end of the sequence, it tries to predict the next observation but it has the path integrated

11:06.640 --> 11:13.760
position to guide this prediction. So it queries the memory module with the positional information

11:13.760 --> 11:20.160
and retrieves a sensory observation corresponding to this particular position which in our case

11:20.160 --> 11:27.120
is John. Awesome, right? So far we have just been theorizing about this spherical model in a vacuum

11:27.760 --> 11:35.200
but does it actually, well, work? And if so, what does it tell us about our own navigational systems?

11:35.200 --> 11:42.400
The most direct way to assess how well the model is performing is to look at its accuracy which is

11:42.400 --> 11:48.400
just the percentage of predictions it made correctly and importantly look at how quickly the accuracy

11:48.400 --> 11:54.240
grows. Here is what I mean. Imagine for a moment that instead of this fancy machine we had just

11:54.240 --> 12:00.400
a good old lookup table which simply memorizes all the transitions as pairs. Previous observation

12:00.400 --> 12:07.440
plus action equals new observation. So it would store memories like John plus sister equals Mary,

12:07.440 --> 12:13.840
Mary plus daughter equals Kate, etc. And to predict the next observation it would simply scan the

12:13.840 --> 12:20.000
lookup table and search for a particular combination. In the case of our family tree example, on first

12:20.000 --> 12:26.320
try it would not be able to predict that Kate's uncle is John because it hadn't encountered this

12:26.320 --> 12:33.840
particular combination previously. In other words, to reach 100% accuracy it would need to first

12:33.840 --> 12:39.840
encounter all possible combinations of observations and actions and this means that the performance

12:39.840 --> 12:47.200
of the model depends on the number of edges of this graph that it visited. In contrast,

12:47.200 --> 12:54.080
tolman eigenbau machine or TEM doesn't need to be explicitly told at the outcome of every action

12:54.080 --> 12:59.840
from every node because it has the notion of a structure. For example, if I tell you that Kate

12:59.840 --> 13:05.360
is Mary's daughter that is enough for you to infer the rest of the relationships automatically.

13:06.160 --> 13:12.800
And this essentially means that to reach 100% accuracy it is enough for TEM to just visit

13:12.800 --> 13:18.640
all the nodes instead of all possible edges and hence its performance depends on the proportion

13:18.640 --> 13:25.520
of nodes visited which grows much faster than the proportion of edges. So our machine seems to

13:25.520 --> 13:31.840
indeed construct a representation of the world. Hooray! But what's going on inside of its brain,

13:31.840 --> 13:38.640
so to speak? Let's look inside the position module first. Remember, the belief about current

13:38.640 --> 13:44.960
location is encoded with a pattern of collective activation of neurons. But we can also interrogate

13:44.960 --> 13:51.040
individual neurons and look at what each one of them is doing as the agent randomly walks around.

13:51.040 --> 13:55.680
Here for the sake of visualization, I'm going to show results after the model was trained on

13:55.680 --> 14:01.600
regular four-connected grids analogs of physical 2D space rather than social hierarchies.

14:02.800 --> 14:08.560
Remarkably, we see that individual units in the position module develop periodic activity

14:08.560 --> 14:14.400
patterns as a function of position. They tile the space with the regular hexagonal grids of

14:14.480 --> 14:22.720
different size or these periodic stripes exactly like grid cells and band cells of the entorhinal

14:22.720 --> 14:30.080
cortex encode position in mammalian brains. And the selectivity of individual units is

14:30.080 --> 14:37.360
preserved across environments, suggesting that they indeed can generalize. Neurons in the memory

14:37.360 --> 14:43.040
module do something different. Since they form a conjunction between positional and sensory

14:43.040 --> 14:49.040
information, each neuron would be active when both of the two upstream components are active.

14:50.080 --> 14:55.920
Indeed, units in the memory module resemble hippocampal place cells of various size,

14:55.920 --> 15:02.560
which fire in a particular patch of space. Importantly, just like hippocampal representations

15:02.560 --> 15:09.520
in real brains, they are firing patterns differ across environments, since the incoming observations

15:09.520 --> 15:16.800
are different. This is known as hippocampal remapping. I'd like to emphasize that such grid-like

15:16.800 --> 15:23.600
and place-like representations were never hard-coded into the model. We started with essentially a

15:23.600 --> 15:28.640
random set of parameters, let the model optimize itself to come up with the best solution to the

15:28.640 --> 15:35.440
prediction problem, and those responses just emerged naturally. So far, we've trained the model

15:35.440 --> 15:41.120
on sequences that were generated from random walks in a given environment, which means that

15:41.120 --> 15:47.120
all the observations were equally likely. But in the real life, animals don't really move by

15:47.120 --> 15:53.600
diffusion. They are biased towards rewards and exploring objects. They like being near walls

15:53.600 --> 16:00.080
because it feels safe and avoid open spaces. So the question is, if we change the statistics of

16:00.080 --> 16:06.160
the sensory observations so that some stimuli are more common than others, would it affect the

16:06.160 --> 16:11.520
representations that emerge in our model as the optimal solution to the prediction problem?

16:12.320 --> 16:18.080
For example, let's train TEM on sequences of observations that mimic the behavior of a real

16:18.080 --> 16:25.520
mouse, which prefers to spend time near boundaries and approaches objects. In this case, representations

16:25.520 --> 16:31.760
that emerge in the position module now include boundary cells, which are selective to borders of

16:31.760 --> 16:38.080
the world, and object vector cells that seem to activate whenever the animal is at a certain

16:38.080 --> 16:44.480
distance and certain direction away from any object. Both of these types of responses,

16:44.480 --> 16:50.560
that by the way also generalize across contexts, are observed experimentally when recording from

16:50.560 --> 16:56.320
entorhinal cortex, while some neurons in the memory module develop selectivity to particular

16:56.320 --> 17:03.920
objects, resembling landmark cells of the hippocampus. If we take a more complex sequence,

17:03.920 --> 17:09.360
such as the one mimicking the animal that is performing an alternation task, the model successfully

17:09.360 --> 17:16.320
learns the rule that the reward is alternating between the sides. Importantly, representations of

17:16.320 --> 17:22.000
some neurons in the memory module resemble the splitter cells found experimentally,

17:22.000 --> 17:26.960
which are modulated by both the position and the direction of the future turn.

17:27.760 --> 17:34.480
This suggests that TEM has the capacity to learn and map latent spaces, which are not

17:34.480 --> 17:41.760
directly given to it in the observations. Another example of how TEM maps latent space is available

17:41.760 --> 17:46.560
as a bonus clip to my Patreon supporters. More details at the end of this video.

17:47.280 --> 17:52.320
Terrific! Now we have a model that can generalize and naturally develops

17:52.320 --> 17:58.000
same representations of space as the hippocampal formation. So what insights can we draw from it?

18:00.000 --> 18:06.160
Recall that play cells remap, which means they change their preferred firing locations in different

18:06.160 --> 18:11.920
environments. This process has not been thought to be random since there is no immediate logic in

18:11.920 --> 18:18.400
how these representations drift around. But having a model of hippocampal formation at hand,

18:18.400 --> 18:25.200
we can start to address this question on a whole other level. Notice that neurons in our memory

18:25.200 --> 18:31.760
module, the ones that resemble play cells, are actually conjunctions between sensory and structural

18:31.760 --> 18:38.800
information. This means that firing of a particular play cell is partially controlled by grid cells,

18:38.800 --> 18:44.160
which provide the structural information. So, for example, if in one environment,

18:44.160 --> 18:50.320
the location of a given play cell coincides with the hexagonal activity pattern of a particular

18:50.320 --> 18:57.280
grid cell, then after we change the surroundings and the play cell remaps, its place field will shift

18:57.280 --> 19:04.720
to another location which also lies on this grid. In other words, remapping is not completely random,

19:04.720 --> 19:09.760
but rather is controlled by grid cells, preserving some structural information.

19:10.640 --> 19:16.720
This relationship between the locations of place and grid cells implies that there should be a

19:16.720 --> 19:23.040
correlation between the degree to which firing locations of place and grid cells coincide across

19:23.040 --> 19:29.120
two environments. This is the case in the model, and remarkably, when the authors tested this

19:29.120 --> 19:34.160
prediction on experimental data, they found it to be true in real brains as well.

19:37.200 --> 19:42.720
Well, I know this was a ton of information to process, so let's try to tie everything together.

19:43.760 --> 19:48.000
The problem of constructing internal models of the world is cornerstone

19:48.000 --> 19:54.960
for both biological and artificial intelligence. It can be solved by factorizing the surrounding

19:54.960 --> 20:00.880
into building blocks and combining them with particular sensory contexts to generate new

20:00.880 --> 20:08.240
models on the go, allowing for rapid generalization. This factorization and composition can be

20:08.240 --> 20:13.840
demonstrated in a computational model, which, when tasked with predicting the next observation in

20:13.840 --> 20:20.800
its sequence, learns the underlying relational structure of the world. Representations that

20:20.800 --> 20:26.880
naturally emerge in this model resemble real neurons found in the hippocampal formation,

20:26.880 --> 20:33.120
suggesting a unified framework of interactions between entorhinal cortex and hippocampus.

20:35.920 --> 20:40.400
I want to take this opportunity to give huge thanks to Dr. James Whittington,

20:40.480 --> 20:46.160
the first author of the original TAM paper, and Gus, my friend and fellow patron with an

20:46.160 --> 20:51.920
expertise in machine learning, who both helped me immensely with preparing the script for this video.

20:52.800 --> 20:57.680
As a final note, I will mention that the Tolman Eigenbaum machine we've seen today

20:57.680 --> 21:03.280
is actually very similar to a transformer architecture, a type of neural network that

21:03.280 --> 21:09.040
is at the core of modern machine learning. In fact, with one little modification,

21:09.040 --> 21:15.520
we can turn this similarity into a precise mathematical equivalence. And this modified

21:15.520 --> 21:21.760
version, called the Tolman Eigenbaum machine transformer, learns much faster and performs

21:21.760 --> 21:28.000
better, while still resembling biological representations for the most part. This potentially

21:28.000 --> 21:32.800
provides a very promising link between neuroscience and modern machine learning,

21:32.800 --> 21:35.600
which makes both fields even more exciting than ever.

21:35.680 --> 21:42.240
Now, I know this was a very simplified description, but fully exploring this equivalence

21:42.800 --> 21:47.120
would require going over the transformer and hopfield networks in detail.

21:47.840 --> 21:53.120
Let me know down in the comment section if you would like to see a more technical video of this kind.

21:54.000 --> 21:58.960
In the meantime, if you're interested in machine learning and don't want to wait any longer,

21:58.960 --> 22:02.800
let me tell you about something that can take your understanding to the next level,

22:03.520 --> 22:10.240
brilliant.org. Brilliant is a revolutionary platform for engaging and interactive learning.

22:10.240 --> 22:15.200
Gone are the days of passive textbook reading. With Brilliant, you'll engage with the material

22:15.200 --> 22:21.600
in a hands-on way, solving problems, answering questions and participating in stunning interactive

22:21.600 --> 22:26.480
visualizations, which help you develop an intuitive understanding of the material.

22:26.480 --> 22:31.600
One course that you might find particularly interesting after watching this video is titled

22:31.600 --> 22:37.200
Artificial Neural Networks. It offers an accessible introduction into the world of artificial

22:37.200 --> 22:43.120
intelligence and how it is inspired by the human brain. You will learn how neural networks work,

22:43.120 --> 22:48.960
how to build your own, and even how to train them to recognize patterns. But that's just the tip of

22:48.960 --> 22:54.640
the iceberg. With over 80 courses to choose from, Brilliant has something for everyone,

22:54.640 --> 23:00.000
and with its personalized approach, you can learn at your own pace with bite-sized chunks.

23:00.640 --> 23:04.880
Take your curiosity to the next level today. Go to brilliant.org

23:04.880 --> 23:11.040
slash artemker sonoff to get a 30-day free trial of everything Brilliant has to offer,

23:11.040 --> 23:16.720
and the first 200 people to use this link will get 20% off the premium subscription.

23:18.080 --> 23:22.320
If you enjoyed this video, press the like button, share it with your friends and colleagues,

23:22.320 --> 23:27.600
subscribe to the channel if you haven't already, and consider supporting me on Patreon to suggest

23:27.600 --> 23:33.440
video topics and enjoy the bonus content. Stay tuned for more interesting topics coming up.

23:34.000 --> 23:43.520
Goodbye, and thank you for the interest in the brain.

