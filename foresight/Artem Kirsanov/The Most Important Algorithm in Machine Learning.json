{"text": " What do nearly all machine learning systems have in common, from GPT and MeJourney to AlphaFold and various models of the brain? Despite being designed to solve different problems, having completely different architectures, and being trained on different data, there is something that unites all of them. A single algorithm that runs under the hood of the training procedures in all of those cases, this algorithm, called backpropagation, is the foundation of the entire field of machine learning, although its details are often overlooked. Surprisingly, what enables artificial networks to learn is also what makes them fundamentally different from the brain and incompatible with biology. This video is the first in a two-part series. Today, we will explore the concept of backpropagation in artificial systems and develop an intuitive understanding of what it is, why it works, and how you could have developed it from scratch yourself. In the next video, we will focus on synaptic plasticity, enabling learning in biological brains, and discuss whether backpropagation is biologically relevant, and if not, what kind of algorithms the brain may be using instead. If you're interested, stay tuned. Despite its transformative impact, it's hard to say who invented backpropagation in the first place, as certain concepts can be traced back to light needs in 17th century. However, it is believed that the first modern formulation of the algorithm, still in use today, was published by Seppo Linenma in his master's thesis in 1970, although he did not reference any neural networks explicitly. Another significant milestone occurred in 1986, when David Rumelhardt, Joffrey Hinton, and Ronald Williams published a paper titled Learning Representations by Backpropagating Errors. They applied the backpropagation algorithm to multi-layer perceptrons, a type of a neural network, and demonstrated for the first time that training with backpropagation enables the network to successfully solve problems and develop meaningful representations at the hidden neuron level, capturing important regularities in the task. As the field progressed, researchers scaled up these models significantly and introduced various architectures, but the fundamental principles of training remained largely unchanged. To gain a comprehensive understanding of what exactly it means to train a network, let's try to build the concept of backpropagation from the ground up. Consider the following problem. Suppose you have collected a set of points x, y on the plane, and you want to describe their relationship. To achieve this, you need to fit a curve y of x that best represents the data. Since there are infinitely many possible functions, we need to make some assumptions. For instance, let's assume we want to find a smooth approximation of the data using a polynomial of degree 5. That means that the resulting curve we are looking for will be a combination of a constant term, a polynomial of degree 0, a straight line, a parabola, and so on up to a power of 5, each weightened by specific coefficients. In other words, the equation for the curve is as follows. Where each k is some arbitrary real number. Our job then becomes finding the configuration of k0 through k5, which leads to the best fitting curve. To make the problem totally unambiguous, we need to agree on what the best curve even means. While you can just visually inspect the data points and estimate whether a given curve captures the pattern or not, this approach is highly subjective and impractical when dealing with large data sets. Instead, we need an objective measurement, a numerical value that quantifies the quality of a fit. One popular method is to measure the square distance between data points and the fitted curve. A high value suggests that the data points are significantly far from the curve, indicating a poor approximation. Conversely, low values indicate a better fit as the curve closely aligns with the data points. This measurement is commonly referred to as a loss and the objective is to minimize it. Now notice that for a fixed data, this distance, the value of the loss, depends only on the defining characteristics of the curve. In our case, the coefficients from k0 through k5. This means that it is effectively a function of parameters, so people usually refer to it as a loss function. It's important not to confuse two different functions we are implicitly dealing with here. The first one is the function y of x, which has one input number and one output number and defines the curve itself. It has this polynomial form given by k's. There are infinitely many such functions and we would like to find the best one. To achieve this, we introduce a loss function, which instead has six inputs, numbers k0 through k5. And for each configuration, it constructs the corresponding curve y, calculates the distance between observed data points and the curve, and outputs a single number, the particular value of the loss. Our job then becomes finding the configuration of k's that yields a minimum loss or minimizing the loss function with respect to the coefficients. Then, plugging these optimal k's into the general equation for the curve will give us the best curve of describing the data. All right, great, but how do we find this magic configuration of k's that minimizes the loss? Well, we might need some help. Let's build a machine called Curve Fitter 6000, designed to simplify manual calculations. It is equipped with six adjustable knobs for k0 through k5, which we can freely turn. To begin, we initialize the machine with our data points and then, for each setting of the knobs, it will evaluate the curve y of x, compute the distance from it to the data points and print out the value of the loss function. Now, we can begin twisting the knobs in order to find the minimum loss. For example, let's start with some initial setting and slightly notch knob number one to the right. The resulting curve changed as well, and we can see that the value of the loss function slightly decreased. Great, it means we are on the right track. Let's turn knob number one in the same direction once again. Uh-oh, this time the fit gets worse and the loss function increases. Apparently, that last notch was a bit too much. So, let's revert the knob to the previous position and try knob two. And we can keep doing this iteratively many, many times, nudging each individual knob one at a time to see whether the resulting curve is a better fit. This is a so-called random perturbation method, since we are essentially wandering in the dark, not knowing in advance how each adjustment will affect the loss function. This would certainly work, but it's not very efficient. Is there a way we can be more intelligent about the knob adjustments? In the most general case, when the machine is a complete black box, nothing better than a random perturbation is guaranteed to exist. However, a great deal of computations, including what's carried out under the hood of our curvefitter, have a special property to them, something called differentiability that allows us to compute the optimal knob setting much more efficiently. We will dive deeper into what differentiability means in just a minute. But for now, let's quickly see the big picture overview of where we are going. Our goal would be to upgrade the machine so that it would have a tiny screen next to each knob. And for any configuration, those screens should say which direction you need to nudge each knob in order to decrease the loss function and by how much. Think about it for a second. We are essentially asking the machine to predict the future and estimate the effect of the knob adjustment on the loss function without actually performing that adjustment, calculating the loss and then reverting the knob back like we did previously. Wouldn't this glance into the future violate some sort of principle? After all, we are jumping to the result of the computation without performing it. Sounds like cheating, right? Well, it turns out that this idea lies on a very simple mathematical foundation so let's spend the next few minutes building it up from scratch. All right, let's consider a simpler case first. Where we freeze five out of six knobs. For example, suppose someone tells you that the rest of them are already in the optimal position. So all you need to do is to find the best value for one remaining knob. Essentially, the machine now has only one variable parameter k1 that we can tweak. And so the loss function is also a simpler function which accepts one number, the knob setting, and outputs another number, the loss value. As a function of one variable, it can be conveniently visualized as a graph in a two-dimensional plane which captures the relationship between the input and the output. For example, it may have this shape right here and our goal is to find this value of k1 which corresponds to the minimum of the loss function. But we don't have access to the true underlying shape. All we can do is to set the knob at a chosen position and kind of query the machine for the value of the loss. In other words, we can only sample individual points along the function we're trying to minimize. And we are essentially blind to how the function behaves in between the known points before we sample them. But suppose we would like to know something more about the function. Not just each value at each point. For example, whether at this point the function is going up or down. This information will ultimately guide our adjustments. Because if you know that the function is going down as you increase the input, turning the knob to the right is a safe bet, since you are guaranteed to decrease the loss with this manipulation. Let's put this notion of going up or down around a point on a stronger mathematical ground. Suppose we have just sampled the point x0, y0 on this graph. What we can do is increase the input by a small amount delta x. This new adjusted input will result in a new value of y, which will differ from the old value by some delta y. This delta depends on the magnitude of our adjustment. For example, if we take a step delta x, which is 10 times smaller, delta y will also be approximately 10 times as small. This is why it makes sense to take the ratio delta y over delta x, the amount of change in the output per unit change in the input. Graphically, this ratio corresponds to a slope of a straight line. Going through the points x0, y0 and x0 plus delta x, y0 plus delta y. Notice that as we take smaller and smaller steps, this straight line will more and more accurately align with the graph in the neighborhood of the point x0, y0. Let's take a limit of this ratio as delta x goes to infinitely small values. Then this limiting case value, which this ratio converges to for infinitesimally small delta x's, is what is called the derivative oa function, and it is denoted by dy over dx. Visually, the derivative oa function at some point is the slope of the line that is tangent to the graph, and thus corresponds to the instantaneous rate of change, or steepness of that function around that point. But different points along the graph might have different steepness values, so the derivative of the entire function is not a single number. In fact, the derivative dy by dx is itself a function of x that takes an arbitrary value of x and outputs the local steepness of y of x at that point. This definition assigns to every function its derivative alter ego. Another function operating on the same input domain, which carries information about the steepness of the original function. There is a bit of a subtlety. Strictly speaking, the derivative may not exist if the function doesn't have a steepness around some point. For example, if it has sharp corners or discontinuities. However, for the remainder of the video, we are going to assume that all functions we are dealing with are smooth, so that the derivative always exists. This is a reasonable claim, because we can control what sort of functions go into our models when we build them. And people usually restrict everything to smooth or differentiable functions to make all the math work out nicely. All right, great. Now, along with the underlying loss as a function of k1, which is hidden from us, we can also reason about its derivative. Another function of k1, which we also don't know, that is equal to the steepness of the loss function at that point. Let's suppose that similarly to how we can query the loss function by running our machine and obtaining individual samples. There is a mechanism for us to sample the derivative function as well. So, for every input value of k1, the machine will output the value of the loss and the local steepness of the loss function around that point. Notice that this derivative information is exactly the sort of look into the future we were looking for to make smarter knob adjustments. For example, let's use it to efficiently find the optimal value of k1. What we can do is the following. First, start at some random position. Ask the machine for the value of the loss and the derivative of the loss function at that position. Take a tiny step in the direction opposite of the derivative. If the derivative is negative, it means that the function is going down. And so, if we want to arrive at the minimum, we need to move in the direction of increasing value of k1. Repeat this procedure until you reach the point where the derivative is zero, which essentially corresponds to the minimum where the tangent line is flat. Essentially, each adjustment in such a guided fashion works kind of like a ball rolling down the hill along the graph until it reaches a valley. Although in the beginning we froze five out of six knobs for simplicity, this process is easily carried out to higher dimensions. For example, suppose now we are free to tweak two different knobs, k1 and k2. The loss would become a function of two variables, which can be visualized as a surface. But what about the derivative? Recall that by definition, the derivative at each point tells us how the output changes per unit change of the input. But now we have two different inputs. Should we nudge only k1, k2 or both? Essentially, our function will have two different derivatives that are usually called partial derivatives because of this ambiguity which input to nudge. Namely, when we have two knobs, the derivative of the loss function with respect to parameter k1 is written like this. It is how much the output changes per unit change in k1 if you hold k2 constant. And conversely, this expression tells you the rate of change of the output if you hold k1 constant and slightly nudge k2. Geometrically, you can imagine slicing the surface with planes parallel to the axes, intersecting at the point of interest k1 k2. So that each of the two cross sections is like a one-dimensional graph of the loss as a function of one variable while the other one is kept constant. Then the slope of a tangent line at each cross section will give you a corresponding partial derivative of the loss at that point. While thinking about partial derivatives as two separate surfaces, one for each variable, is a perfectly valid way. People usually plug the two different values into a vector called a gradient vector. Essentially, this is a mapping from two input values to another two numbers where the first signifies how much the output changes per tiny change in the first input. And similarly, for the second input. Geometrically, this vector points in the direction of steepest ascent. So if you want to minimize a function, like in the case for our loss, we need to take steps in the direction opposite to this gradient. This iterative procedure of nudging the parameters in the direction opposite of the gradient vector is called gradient descent, which you have probably heard of. This is analogous to a ball rolling down the hill for the two-dimensional case. And the partial derivatives essentially tell you which direction is downhill. Going beyond two dimensions is impossible to visualize directly, but the math stays exactly the same. For instance, if we are now free to tweak all the six knobs, the loss function is a hyper surface in six dimensions. And the gradient vector now has six numbers packed into it. But it still points in the direction of steepest ascent. So if we iteratively take small steps in the direction opposite to it, we are going to roll the ball down the hill in six dimensions and eventually reach the minimum of the loss function. Great, let's back up a bit. Remember how we were looking for ways to add screens next to each knob that would give us the direction of optimal adjustment? Well, it is essentially nothing more but the components of the gradient vector. If at a particular configuration, the partial derivative of the loss with respect to k1 is positive, it means that increasing k1 will lead to increased loss. So we need to decrease the value of the knob by turning it to the left. And similarly for all other parameters. This is how the derivatives serve as these windows into the future by providing us with information about local behavior of the function. And once we have a way of accessing the derivative, we can perform gradient descent and efficiently find the minimum of the loss function, thus solving the optimization problem. However, there is an elephant in a room. So far we have implicitly assumed the derivative information is given to us. Or that we can sample the derivative at a given point. Similarly to how we sample the loss function itself by running the calculation of the machine. But how do you actually find the derivative? As we will see further, this is the main purpose of the back propagation algorithm. Essentially, the way we find derivatives of arbitrarily complex functions is the following. First, there are a handful of building blocks to begin with. Simple functions, derivatives of which are known from calculus. These are the kind of derivative formulas you often memorize in college. For example, if the function is linear, it's pretty clear that its derivative will be a constant, equal to the slope of that line everywhere, which coincides with its own tangent line. A parabola x squared becomes more steep as you increase x. And its derivative is actually 2x. In fact, there is a more general formula for the derivative of x to the power of n. Similarly, derivatives of the exponent and logarithm can be written down explicitly. But these are just individual examples of simple, well-known functions. In order to compute arbitrary derivatives, we need a way to combine such atomic building blocks together. There are a few rules how to do it. For instance, the derivative of a sum of two functions is the sum of the derivatives. There is also a formula for the derivative of a product of two functions. This gives you a way to compute things like the derivative of 3x squared minus equal to the power of x. But to complete the picture and to be able to find derivatives of almost everything, we need one other rule called the chain rule, which powers the entire field of machine learning. It tells you how to compute the derivative of a combination of two functions, when one of them is an input to another. Here is a way to reason about this. Suppose you take one of those simpler machines, which receives a single input x that you can vary with an ALP, and spits out an output, j of x. Now, you take a second machine of this kind, which performs a different function, f of x. What would happen if you connect them in sequence, so that the output of the first machine is fed into the second one as an input? Notice that such a construction can be thought of as a single function, which also receives one input number and gives an output by computing a more complicated function, which is a composition of the two simpler functions. In fact, if you put a black box around it to conceal the fact that there are actually two machines operating sequentially, you can treat it as a single machine and ask, well, if I notch the input on one end, how will it affect the output on another end? In other words, what is the derivative of the resulting function? Suppose we know the individual derivatives of the two machines, f and j. If the knob is set at some value x, local steepness of the first function is evaluated at x. However, the number that is fed into the second machine is not x, because it was already processed by the first function. So, the thing that is being plugged into the second function is j of x. And so, the local rate of change of the second machine is thus the derivative of f evaluated at the point j of x. Now, imagine you notch the knob x by a tiny amount, delta. That input notch, when it comes out of the first machine, will be multiplied by the derivative of j, since the derivative is the rate of change in the output per unit change of the input. So, after the first function, the output will increase by delta, multiplied by the derivative of j. This expression is essentially a tiny notch in the input to the second machine, whose derivative at that point is given by this expression. This means that for each delta increase in the input, we bump the output by this much. Hence, the derivative when you divide that by delta will look like this. You can think about it as a set of three interconnected cog wheels, where the first one represents the input knob x. And the other two wheels are functions, j of x and f of j of x, respectively. When you notch the first wheel, it induces a notch in the middle wheel and the amplitude of that change is given by the derivative of j, which in turn causes the third wheel to rotate, and the amplitude of that resulting notch is given by changing the derivatives together. Alright, great. Now we have a straightforward way of obtaining a derivative of any arbitrarily complex function, as long as it can be decomposed into building blocks. Simple functions with explicit derivative formulas, such as summations, multiplications, exponents, logarithms, etc. But how can it be used to find the best curve using our curve fitter? The big picture we are aiming for is the following. For each of our parameter knobs, we will write down its effect on the loss in terms of simple, easily differentiable operations. Once we have that sequence of building blocks, no matter how long, we should be able to sequentially apply the chain rule to each of them in order to find the value of the derivative of the loss function with respect to each of the input knobs and perform iterative gradient descent to minimize the loss. Let's see an example of this. First, we are going to create a knob for each number the loss function can possibly depend on. This obviously includes the parameters, but there is also the data itself, coordinates of points to which we are fitting the curve in the first place. Now, during optimization, the data points are set in stone, so changing them in order to obtain a lower loss would make no sense. However, for conceptual purposes, we can think about these values as fixed knobs set in one position so that we cannot nudge them. Once we have all the existing numbers being fed into the machine, we can start to break down the loss calculation. Remember, by definition, it is the sum of squared vertical distances from each point to the curve parameterized by k's. So, for instance, let's take the first data point, x1, y1, multiply the x coordinate by k1, add that to the squared value of x1 multiplied by k2, and so on for other k's, including the constant term k0. This sum of weight and powers of x1 is the value of y predicted by the current curve, f of x1. Let's call it y1 hat. Next, we need to take the squared difference between the actual value and the predicted value. This is how much the first data point contributes to the resulting value of the loss function. Repeating the same procedure for all remaining data points and summing up the resulting squared distances gives us the overall total loss that we are trying to minimize. The computation we just performed, finding the value of the loss for a given configuration of parameter and data knobs, is known as the forward step. The entire sequence of calculations can be visualized as this kind of computational graph, where each node is some simple operation like addition or multiplication. Forward step then corresponds to computations flowing from left to right. But to perform optimization, we also need information about gradients, how each node influences the loss. Now we are going to do what's known as the backward step, and unroll the sequence of calculations in reverse order to find derivatives. What makes the backward step possible is the fact that every node in our compute graph is an easily differentiable operation. Think of individual nodes as these tiny machines which simply add, multiply or take powers. We know their derivatives, and because their outputs are connected sequentially, we can apply the chain rule. This means that for each node we can find its gradient, the partial derivative of the output loss with respect to that node. Let's see how it can be done. Consider a region of the compute graph, where two number nodes A and B are being fed into a machine that performs addition, and its result A plus B is further processed by the system to compute the overall output L. Suppose we already computed the gradient of A plus B earlier, so that we know how nudging the sum will affect the output. The question is, what are individual gradients of A and B? Well, intuitively, if you nudge A by sum amount, A plus B will be nudged by this same amount, so the gradient or the partial derivative of the loss with respect to A is the same as the gradient of the sum, and similarly for B. This can be seen more formally by writing down the chain rule and noticing that the derivative of A plus B with respect to A is just one. In other words, when you encounter this situation in the compute graph, then the gradient of the sum just simply propagates into the gradients of the nodes that plug into the sum machine. Another possible scenario is when A and B are multiplied. Just like before, suppose we know the gradient of their product because it was computed before. In this case, individual nudge to A will be scaled by a factor of B, so the product will be nudged B times as much, which propagates into the output. So, whatever the derivative of the output with respect to the product of A B is, the output derivative with respect to A will get scaled by a factor of B, and vice versa for the gradient of B. Once again, it can be seen more formally by examining the chain rule. In other words, the multiplication node in the compute graph distributes the downstream gradient across incoming nodes by multiplying it crossways by their values. Similar rules can be easily formulated for other building block calculations, such as raising a number to a power or taking the logarithm. Finally, when a single node takes part in multiple branches of the compute graph, gradients from the corresponding branches are simply added together. Indeed, suppose you have the following structure in the graph, where the C-minode A plugs into two different operations that contribute to the overall loss. Then, if you nudge A by delta, the output will be simultaneously nudged by this derivative from the first branch and this derivative from the second branch. So, the overall effect of nudging A will be the sum of the two gradients. Alright, great. Now that we have constructed a computational graph and established how to process individual chunks of it, we can just sequentially apply those rules starting from the output and working our way backwards. For instance, the rightmost node in the graph is the resulting value of the loss function. How does the incremental change in that node affect the output? Well, it is the output, so its gradient is by definition equal to 1. Next, the loss function is the sum of many delta y's squared. We know what to do with the summation node. It just copies whatever the gradient value is to the right of it into all incoming nodes. Consequently, the gradients of all delta y's squared will also be equal to 1. Each of those nodes is the squared value of the corresponding delta y and we know how to differentiate this squaring operation. The derivative of the loss function with respect to delta y1 will be 2 times the delta y1, which is just the number we found during the forward calculation. And we can keep doing this propagation of sequential derivative calculation backwards along our compute graph until we reach the leftmost nodes, which are the data and parameter knobs. The derivatives of the loss with respect to the input data don't really matter, but the derivatives with respect to the parameters is exactly what we want. Once these parameter gradients are found, we can perform one iteration of gradient descent. Namely, we are going to slightly tweak the knobs in the directions opposite to the gradient. The exact magnitude of each adjustment being the negative product of the gradient and some small number called the learning rate, for example, 0.01. Note that after the adjustment is performed, the configuration of the machine and the resulting loss are different. And so the old gradient values we found no longer hold. So we need to run the forward and backward calculations once again to obtain updated gradients and the new decreased loss. Performing this loop of forward pass, backward pass, notch, repeat is the essence of training every modern machine learning system. And exactly the same algorithm is used today in even the most complicated models. As long as the problem you are trying to solve with a given model architecture can be decomposed into individual operations that are differentiable, you can sequentially apply the chain rule many times to arrive at the optimal setting of the parameters. For instance, a feed-forward neural network is essentially a bunch of multiplications and summations with a few non-linear activation functions sprinkled between the layers. Each of those atomic computations is differentiable, so you can construct the compute graph and run the backward pass on it to find how each parameter, like connection weights between neurons, influence the loss function. And because neural networks, given enough neurons, can in theory approximate any function imaginable, we can create a large enough sequence of these building block mathematical machines to solve problems such as classifying images and even generating new text. This seems like a very elegant and efficient solution. After all, if you want to solve the optimization problem, derivatives tell you exactly which adjustments are necessary. But how similar is this to what the brain actually does? When we learn to walk, speak and read, is the brain also minimizing some sort of loss function? Does it calculate derivatives? Or could it be doing something totally different? In the next video, we are going to dive into the world of synaptic plasticity and talk about how biological neural networks learn. In keeping with the topic of biological learning, I'd like to take a moment to give a shout out to Shortform, a longtime partner of this channel. Shortform is a platform which lets you take your reading to the next level. They offer book guides, which are supercharged book summaries. Not only do you get the condensed version of all the key points, but they are also supplemented by ideas from related sources, such as other books and research papers. I really love this feature because it allows you to get the big picture overview and promotes the interlinking of ideas. The existing library contains books from a variety of genres, including science, education and technology, and new books are being added every week. Personally, I found Shortform to be really valuable when it comes to choosing books to read, as well as taking efficient notes. Don't hesitate to give it a try by following the link down in the description to get 5 days of unlimited access and 20% off the annual membership. If you enjoyed this video, press the like button, share it with your friends and colleagues, and subscribe to the channel if you haven't already. Stay tuned for more interesting topics coming up. Goodbye and thank you for the interest in the brain. you", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 8.24, "text": " What do nearly all machine learning systems have in common, from GPT and MeJourney to AlphaFold and various models of the brain?", "tokens": [50364, 708, 360, 6217, 439, 3479, 2539, 3652, 362, 294, 2689, 11, 490, 26039, 51, 293, 1923, 41, 396, 2397, 281, 20588, 37, 2641, 293, 3683, 5245, 295, 264, 3567, 30, 50776], "temperature": 0.0, "avg_logprob": -0.2557594427901707, "compression_ratio": 1.6244897959183673, "no_speech_prob": 6.302652036538348e-05}, {"id": 1, "seek": 0, "start": 9.08, "end": 16.72, "text": " Despite being designed to solve different problems, having completely different architectures, and being trained on different data,", "tokens": [50818, 11334, 885, 4761, 281, 5039, 819, 2740, 11, 1419, 2584, 819, 6331, 1303, 11, 293, 885, 8895, 322, 819, 1412, 11, 51200], "temperature": 0.0, "avg_logprob": -0.2557594427901707, "compression_ratio": 1.6244897959183673, "no_speech_prob": 6.302652036538348e-05}, {"id": 2, "seek": 0, "start": 17.080000000000002, "end": 19.72, "text": " there is something that unites all of them.", "tokens": [51218, 456, 307, 746, 300, 517, 3324, 439, 295, 552, 13, 51350], "temperature": 0.0, "avg_logprob": -0.2557594427901707, "compression_ratio": 1.6244897959183673, "no_speech_prob": 6.302652036538348e-05}, {"id": 3, "seek": 0, "start": 20.400000000000002, "end": 26.16, "text": " A single algorithm that runs under the hood of the training procedures in all of those cases,", "tokens": [51384, 316, 2167, 9284, 300, 6676, 833, 264, 13376, 295, 264, 3097, 13846, 294, 439, 295, 729, 3331, 11, 51672], "temperature": 0.0, "avg_logprob": -0.2557594427901707, "compression_ratio": 1.6244897959183673, "no_speech_prob": 6.302652036538348e-05}, {"id": 4, "seek": 2616, "start": 26.28, "end": 32.6, "text": " this algorithm, called backpropagation, is the foundation of the entire field of machine learning,", "tokens": [50370, 341, 9284, 11, 1219, 646, 79, 1513, 559, 399, 11, 307, 264, 7030, 295, 264, 2302, 2519, 295, 3479, 2539, 11, 50686], "temperature": 0.0, "avg_logprob": -0.241794505017869, "compression_ratio": 1.6613545816733069, "no_speech_prob": 0.00884645339101553}, {"id": 5, "seek": 2616, "start": 33.12, "end": 35.56, "text": " although its details are often overlooked.", "tokens": [50712, 4878, 1080, 4365, 366, 2049, 32269, 13, 50834], "temperature": 0.0, "avg_logprob": -0.241794505017869, "compression_ratio": 1.6613545816733069, "no_speech_prob": 0.00884645339101553}, {"id": 6, "seek": 2616, "start": 36.84, "end": 44.28, "text": " Surprisingly, what enables artificial networks to learn is also what makes them fundamentally different from the brain and", "tokens": [50898, 49908, 11, 437, 17077, 11677, 9590, 281, 1466, 307, 611, 437, 1669, 552, 17879, 819, 490, 264, 3567, 293, 51270], "temperature": 0.0, "avg_logprob": -0.241794505017869, "compression_ratio": 1.6613545816733069, "no_speech_prob": 0.00884645339101553}, {"id": 7, "seek": 2616, "start": 44.879999999999995, "end": 49.16, "text": " incompatible with biology. This video is the first in a two-part series.", "tokens": [51300, 40393, 267, 964, 365, 14956, 13, 639, 960, 307, 264, 700, 294, 257, 732, 12, 6971, 2638, 13, 51514], "temperature": 0.0, "avg_logprob": -0.241794505017869, "compression_ratio": 1.6613545816733069, "no_speech_prob": 0.00884645339101553}, {"id": 8, "seek": 2616, "start": 49.8, "end": 54.84, "text": " Today, we will explore the concept of backpropagation in artificial systems and", "tokens": [51546, 2692, 11, 321, 486, 6839, 264, 3410, 295, 646, 79, 1513, 559, 399, 294, 11677, 3652, 293, 51798], "temperature": 0.0, "avg_logprob": -0.241794505017869, "compression_ratio": 1.6613545816733069, "no_speech_prob": 0.00884645339101553}, {"id": 9, "seek": 5484, "start": 55.24, "end": 62.36000000000001, "text": " develop an intuitive understanding of what it is, why it works, and how you could have developed it from scratch yourself.", "tokens": [50384, 1499, 364, 21769, 3701, 295, 437, 309, 307, 11, 983, 309, 1985, 11, 293, 577, 291, 727, 362, 4743, 309, 490, 8459, 1803, 13, 50740], "temperature": 0.0, "avg_logprob": -0.2172863483428955, "compression_ratio": 1.5958333333333334, "no_speech_prob": 0.0002131811634171754}, {"id": 10, "seek": 5484, "start": 63.160000000000004, "end": 69.92, "text": " In the next video, we will focus on synaptic plasticity, enabling learning in biological brains, and", "tokens": [50780, 682, 264, 958, 960, 11, 321, 486, 1879, 322, 5451, 2796, 299, 5900, 507, 11, 23148, 2539, 294, 13910, 15442, 11, 293, 51118], "temperature": 0.0, "avg_logprob": -0.2172863483428955, "compression_ratio": 1.5958333333333334, "no_speech_prob": 0.0002131811634171754}, {"id": 11, "seek": 5484, "start": 70.36, "end": 79.88, "text": " discuss whether backpropagation is biologically relevant, and if not, what kind of algorithms the brain may be using instead. If you're interested, stay tuned.", "tokens": [51140, 2248, 1968, 646, 79, 1513, 559, 399, 307, 3228, 17157, 7340, 11, 293, 498, 406, 11, 437, 733, 295, 14642, 264, 3567, 815, 312, 1228, 2602, 13, 759, 291, 434, 3102, 11, 1754, 10870, 13, 51616], "temperature": 0.0, "avg_logprob": -0.2172863483428955, "compression_ratio": 1.5958333333333334, "no_speech_prob": 0.0002131811634171754}, {"id": 12, "seek": 8484, "start": 84.84, "end": 97.0, "text": " Despite its transformative impact, it's hard to say who invented backpropagation in the first place, as certain concepts can be traced back to", "tokens": [50364, 11334, 1080, 36070, 2712, 11, 309, 311, 1152, 281, 584, 567, 14479, 646, 79, 1513, 559, 399, 294, 264, 700, 1081, 11, 382, 1629, 10392, 393, 312, 38141, 646, 281, 50972], "temperature": 0.0, "avg_logprob": -0.25700851016574433, "compression_ratio": 1.5135135135135136, "no_speech_prob": 0.00019716870156116784}, {"id": 13, "seek": 8484, "start": 97.0, "end": 104.52000000000001, "text": " light needs in 17th century. However, it is believed that the first modern formulation of the algorithm, still in use today,", "tokens": [50972, 1442, 2203, 294, 3282, 392, 4901, 13, 2908, 11, 309, 307, 7847, 300, 264, 700, 4363, 37642, 295, 264, 9284, 11, 920, 294, 764, 965, 11, 51348], "temperature": 0.0, "avg_logprob": -0.25700851016574433, "compression_ratio": 1.5135135135135136, "no_speech_prob": 0.00019716870156116784}, {"id": 14, "seek": 8484, "start": 104.72, "end": 113.32000000000001, "text": " was published by Seppo Linenma in his master's thesis in 1970, although he did not reference any neural networks explicitly.", "tokens": [51358, 390, 6572, 538, 1100, 27000, 441, 5636, 1696, 294, 702, 4505, 311, 22288, 294, 14577, 11, 4878, 415, 630, 406, 6408, 604, 18161, 9590, 20803, 13, 51788], "temperature": 0.0, "avg_logprob": -0.25700851016574433, "compression_ratio": 1.5135135135135136, "no_speech_prob": 0.00019716870156116784}, {"id": 15, "seek": 11332, "start": 114.32, "end": 122.0, "text": " Another significant milestone occurred in 1986, when David Rumelhardt, Joffrey Hinton, and Ronald Williams", "tokens": [50414, 3996, 4776, 28048, 11068, 294, 27895, 11, 562, 4389, 31963, 338, 21491, 83, 11, 3139, 602, 7950, 389, 12442, 11, 293, 27397, 12929, 50798], "temperature": 0.0, "avg_logprob": -0.24280613866345635, "compression_ratio": 1.4957627118644068, "no_speech_prob": 4.611269832821563e-05}, {"id": 16, "seek": 11332, "start": 122.32, "end": 127.36, "text": " published a paper titled Learning Representations by Backpropagating Errors.", "tokens": [50814, 6572, 257, 3035, 19841, 15205, 19945, 763, 538, 5833, 79, 1513, 559, 990, 3300, 9734, 13, 51066], "temperature": 0.0, "avg_logprob": -0.24280613866345635, "compression_ratio": 1.4957627118644068, "no_speech_prob": 4.611269832821563e-05}, {"id": 17, "seek": 11332, "start": 127.44, "end": 134.12, "text": " They applied the backpropagation algorithm to multi-layer perceptrons, a type of a neural network, and", "tokens": [51070, 814, 6456, 264, 646, 79, 1513, 559, 399, 9284, 281, 4825, 12, 8376, 260, 43276, 13270, 11, 257, 2010, 295, 257, 18161, 3209, 11, 293, 51404], "temperature": 0.0, "avg_logprob": -0.24280613866345635, "compression_ratio": 1.4957627118644068, "no_speech_prob": 4.611269832821563e-05}, {"id": 18, "seek": 11332, "start": 134.68, "end": 137.84, "text": " demonstrated for the first time that training with backpropagation", "tokens": [51432, 18772, 337, 264, 700, 565, 300, 3097, 365, 646, 79, 1513, 559, 399, 51590], "temperature": 0.0, "avg_logprob": -0.24280613866345635, "compression_ratio": 1.4957627118644068, "no_speech_prob": 4.611269832821563e-05}, {"id": 19, "seek": 13784, "start": 138.52, "end": 148.28, "text": " enables the network to successfully solve problems and develop meaningful representations at the hidden neuron level, capturing important regularities in the task.", "tokens": [50398, 17077, 264, 3209, 281, 10727, 5039, 2740, 293, 1499, 10995, 33358, 412, 264, 7633, 34090, 1496, 11, 23384, 1021, 3890, 1088, 294, 264, 5633, 13, 50886], "temperature": 0.0, "avg_logprob": -0.19610375722249349, "compression_ratio": 1.6067415730337078, "no_speech_prob": 0.0004442169447429478}, {"id": 20, "seek": 13784, "start": 149.4, "end": 158.72, "text": " As the field progressed, researchers scaled up these models significantly and introduced various architectures, but the fundamental principles of training", "tokens": [50942, 1018, 264, 2519, 36789, 11, 10309, 36039, 493, 613, 5245, 10591, 293, 7268, 3683, 6331, 1303, 11, 457, 264, 8088, 9156, 295, 3097, 51408], "temperature": 0.0, "avg_logprob": -0.19610375722249349, "compression_ratio": 1.6067415730337078, "no_speech_prob": 0.0004442169447429478}, {"id": 21, "seek": 13784, "start": 158.88, "end": 166.48000000000002, "text": " remained largely unchanged. To gain a comprehensive understanding of what exactly it means to train a network,", "tokens": [51416, 12780, 11611, 44553, 13, 1407, 6052, 257, 13914, 3701, 295, 437, 2293, 309, 1355, 281, 3847, 257, 3209, 11, 51796], "temperature": 0.0, "avg_logprob": -0.19610375722249349, "compression_ratio": 1.6067415730337078, "no_speech_prob": 0.0004442169447429478}, {"id": 22, "seek": 16648, "start": 166.51999999999998, "end": 172.51999999999998, "text": " let's try to build the concept of backpropagation from the ground up. Consider the following problem.", "tokens": [50366, 718, 311, 853, 281, 1322, 264, 3410, 295, 646, 79, 1513, 559, 399, 490, 264, 2727, 493, 13, 17416, 264, 3480, 1154, 13, 50666], "temperature": 0.0, "avg_logprob": -0.1878075917561849, "compression_ratio": 1.5672268907563025, "no_speech_prob": 9.170172415906563e-05}, {"id": 23, "seek": 16648, "start": 173.07999999999998, "end": 180.16, "text": " Suppose you have collected a set of points x, y on the plane, and you want to describe their relationship.", "tokens": [50694, 21360, 291, 362, 11087, 257, 992, 295, 2793, 2031, 11, 288, 322, 264, 5720, 11, 293, 291, 528, 281, 6786, 641, 2480, 13, 51048], "temperature": 0.0, "avg_logprob": -0.1878075917561849, "compression_ratio": 1.5672268907563025, "no_speech_prob": 9.170172415906563e-05}, {"id": 24, "seek": 16648, "start": 180.76, "end": 186.48, "text": " To achieve this, you need to fit a curve y of x that best represents the data.", "tokens": [51078, 1407, 4584, 341, 11, 291, 643, 281, 3318, 257, 7605, 288, 295, 2031, 300, 1151, 8855, 264, 1412, 13, 51364], "temperature": 0.0, "avg_logprob": -0.1878075917561849, "compression_ratio": 1.5672268907563025, "no_speech_prob": 9.170172415906563e-05}, {"id": 25, "seek": 16648, "start": 187.48, "end": 192.12, "text": " Since there are infinitely many possible functions, we need to make some assumptions.", "tokens": [51414, 4162, 456, 366, 36227, 867, 1944, 6828, 11, 321, 643, 281, 652, 512, 17695, 13, 51646], "temperature": 0.0, "avg_logprob": -0.1878075917561849, "compression_ratio": 1.5672268907563025, "no_speech_prob": 9.170172415906563e-05}, {"id": 26, "seek": 19212, "start": 192.52, "end": 200.20000000000002, "text": " For instance, let's assume we want to find a smooth approximation of the data using a polynomial of degree 5.", "tokens": [50384, 1171, 5197, 11, 718, 311, 6552, 321, 528, 281, 915, 257, 5508, 28023, 295, 264, 1412, 1228, 257, 26110, 295, 4314, 1025, 13, 50768], "temperature": 0.0, "avg_logprob": -0.1832623540619273, "compression_ratio": 1.5933014354066986, "no_speech_prob": 0.00023413522285409272}, {"id": 27, "seek": 19212, "start": 201.04, "end": 209.4, "text": " That means that the resulting curve we are looking for will be a combination of a constant term, a polynomial of degree 0, a", "tokens": [50810, 663, 1355, 300, 264, 16505, 7605, 321, 366, 1237, 337, 486, 312, 257, 6562, 295, 257, 5754, 1433, 11, 257, 26110, 295, 4314, 1958, 11, 257, 51228], "temperature": 0.0, "avg_logprob": -0.1832623540619273, "compression_ratio": 1.5933014354066986, "no_speech_prob": 0.00023413522285409272}, {"id": 28, "seek": 19212, "start": 210.12, "end": 216.8, "text": " straight line, a parabola, and so on up to a power of 5, each weightened by specific coefficients.", "tokens": [51264, 2997, 1622, 11, 257, 45729, 4711, 11, 293, 370, 322, 493, 281, 257, 1347, 295, 1025, 11, 1184, 3364, 5320, 538, 2685, 31994, 13, 51598], "temperature": 0.0, "avg_logprob": -0.1832623540619273, "compression_ratio": 1.5933014354066986, "no_speech_prob": 0.00023413522285409272}, {"id": 29, "seek": 21680, "start": 217.04000000000002, "end": 224.4, "text": " In other words, the equation for the curve is as follows. Where each k is some arbitrary real number.", "tokens": [50376, 682, 661, 2283, 11, 264, 5367, 337, 264, 7605, 307, 382, 10002, 13, 2305, 1184, 350, 307, 512, 23211, 957, 1230, 13, 50744], "temperature": 0.0, "avg_logprob": -0.13874154289563498, "compression_ratio": 1.6046511627906976, "no_speech_prob": 4.06945327995345e-05}, {"id": 30, "seek": 21680, "start": 225.0, "end": 232.56, "text": " Our job then becomes finding the configuration of k0 through k5, which leads to the best fitting curve.", "tokens": [50774, 2621, 1691, 550, 3643, 5006, 264, 11694, 295, 350, 15, 807, 350, 20, 11, 597, 6689, 281, 264, 1151, 15669, 7605, 13, 51152], "temperature": 0.0, "avg_logprob": -0.13874154289563498, "compression_ratio": 1.6046511627906976, "no_speech_prob": 4.06945327995345e-05}, {"id": 31, "seek": 21680, "start": 232.84, "end": 238.48000000000002, "text": " To make the problem totally unambiguous, we need to agree on what the best curve even means.", "tokens": [51166, 1407, 652, 264, 1154, 3879, 517, 2173, 30525, 11, 321, 643, 281, 3986, 322, 437, 264, 1151, 7605, 754, 1355, 13, 51448], "temperature": 0.0, "avg_logprob": -0.13874154289563498, "compression_ratio": 1.6046511627906976, "no_speech_prob": 4.06945327995345e-05}, {"id": 32, "seek": 21680, "start": 239.04000000000002, "end": 246.08, "text": " While you can just visually inspect the data points and estimate whether a given curve captures the pattern or not,", "tokens": [51476, 3987, 291, 393, 445, 19622, 15018, 264, 1412, 2793, 293, 12539, 1968, 257, 2212, 7605, 27986, 264, 5102, 420, 406, 11, 51828], "temperature": 0.0, "avg_logprob": -0.13874154289563498, "compression_ratio": 1.6046511627906976, "no_speech_prob": 4.06945327995345e-05}, {"id": 33, "seek": 24608, "start": 246.32000000000002, "end": 251.4, "text": " this approach is highly subjective and impractical when dealing with large data sets.", "tokens": [50376, 341, 3109, 307, 5405, 25972, 293, 704, 1897, 804, 562, 6260, 365, 2416, 1412, 6352, 13, 50630], "temperature": 0.0, "avg_logprob": -0.15003088296177874, "compression_ratio": 1.6375, "no_speech_prob": 6.81485325912945e-05}, {"id": 34, "seek": 24608, "start": 252.0, "end": 258.36, "text": " Instead, we need an objective measurement, a numerical value that quantifies the quality of a fit.", "tokens": [50660, 7156, 11, 321, 643, 364, 10024, 13160, 11, 257, 29054, 2158, 300, 4426, 11221, 264, 3125, 295, 257, 3318, 13, 50978], "temperature": 0.0, "avg_logprob": -0.15003088296177874, "compression_ratio": 1.6375, "no_speech_prob": 6.81485325912945e-05}, {"id": 35, "seek": 24608, "start": 258.96000000000004, "end": 265.32, "text": " One popular method is to measure the square distance between data points and the fitted curve.", "tokens": [51008, 1485, 3743, 3170, 307, 281, 3481, 264, 3732, 4560, 1296, 1412, 2793, 293, 264, 26321, 7605, 13, 51326], "temperature": 0.0, "avg_logprob": -0.15003088296177874, "compression_ratio": 1.6375, "no_speech_prob": 6.81485325912945e-05}, {"id": 36, "seek": 24608, "start": 265.72, "end": 273.64, "text": " A high value suggests that the data points are significantly far from the curve, indicating a poor approximation.", "tokens": [51346, 316, 1090, 2158, 13409, 300, 264, 1412, 2793, 366, 10591, 1400, 490, 264, 7605, 11, 25604, 257, 4716, 28023, 13, 51742], "temperature": 0.0, "avg_logprob": -0.15003088296177874, "compression_ratio": 1.6375, "no_speech_prob": 6.81485325912945e-05}, {"id": 37, "seek": 27364, "start": 274.64, "end": 281.32, "text": " Conversely, low values indicate a better fit as the curve closely aligns with the data points.", "tokens": [50414, 33247, 736, 11, 2295, 4190, 13330, 257, 1101, 3318, 382, 264, 7605, 8185, 7975, 82, 365, 264, 1412, 2793, 13, 50748], "temperature": 0.0, "avg_logprob": -0.15625382565903936, "compression_ratio": 1.55793991416309, "no_speech_prob": 1.593659180798568e-05}, {"id": 38, "seek": 27364, "start": 281.8, "end": 287.96, "text": " This measurement is commonly referred to as a loss and the objective is to minimize it.", "tokens": [50772, 639, 13160, 307, 12719, 10839, 281, 382, 257, 4470, 293, 264, 10024, 307, 281, 17522, 309, 13, 51080], "temperature": 0.0, "avg_logprob": -0.15625382565903936, "compression_ratio": 1.55793991416309, "no_speech_prob": 1.593659180798568e-05}, {"id": 39, "seek": 27364, "start": 288.36, "end": 293.8, "text": " Now notice that for a fixed data, this distance, the value of the loss,", "tokens": [51100, 823, 3449, 300, 337, 257, 6806, 1412, 11, 341, 4560, 11, 264, 2158, 295, 264, 4470, 11, 51372], "temperature": 0.0, "avg_logprob": -0.15625382565903936, "compression_ratio": 1.55793991416309, "no_speech_prob": 1.593659180798568e-05}, {"id": 40, "seek": 27364, "start": 294.44, "end": 302.28, "text": " depends only on the defining characteristics of the curve. In our case, the coefficients from k0 through k5.", "tokens": [51404, 5946, 787, 322, 264, 17827, 10891, 295, 264, 7605, 13, 682, 527, 1389, 11, 264, 31994, 490, 350, 15, 807, 350, 20, 13, 51796], "temperature": 0.0, "avg_logprob": -0.15625382565903936, "compression_ratio": 1.55793991416309, "no_speech_prob": 1.593659180798568e-05}, {"id": 41, "seek": 30228, "start": 303.08, "end": 310.76, "text": " This means that it is effectively a function of parameters, so people usually refer to it as a loss function.", "tokens": [50404, 639, 1355, 300, 309, 307, 8659, 257, 2445, 295, 9834, 11, 370, 561, 2673, 2864, 281, 309, 382, 257, 4470, 2445, 13, 50788], "temperature": 0.0, "avg_logprob": -0.17112176758902414, "compression_ratio": 1.609865470852018, "no_speech_prob": 0.00012339437671471387}, {"id": 42, "seek": 30228, "start": 311.2, "end": 316.52, "text": " It's important not to confuse two different functions we are implicitly dealing with here.", "tokens": [50810, 467, 311, 1021, 406, 281, 28584, 732, 819, 6828, 321, 366, 26947, 356, 6260, 365, 510, 13, 51076], "temperature": 0.0, "avg_logprob": -0.17112176758902414, "compression_ratio": 1.609865470852018, "no_speech_prob": 0.00012339437671471387}, {"id": 43, "seek": 30228, "start": 316.52, "end": 323.08, "text": " The first one is the function y of x, which has one input number and one output number and", "tokens": [51076, 440, 700, 472, 307, 264, 2445, 288, 295, 2031, 11, 597, 575, 472, 4846, 1230, 293, 472, 5598, 1230, 293, 51404], "temperature": 0.0, "avg_logprob": -0.17112176758902414, "compression_ratio": 1.609865470852018, "no_speech_prob": 0.00012339437671471387}, {"id": 44, "seek": 30228, "start": 323.44, "end": 329.15999999999997, "text": " defines the curve itself. It has this polynomial form given by k's.", "tokens": [51422, 23122, 264, 7605, 2564, 13, 467, 575, 341, 26110, 1254, 2212, 538, 350, 311, 13, 51708], "temperature": 0.0, "avg_logprob": -0.17112176758902414, "compression_ratio": 1.609865470852018, "no_speech_prob": 0.00012339437671471387}, {"id": 45, "seek": 32916, "start": 329.44, "end": 335.12, "text": " There are infinitely many such functions and we would like to find the best one. To achieve this,", "tokens": [50378, 821, 366, 36227, 867, 1270, 6828, 293, 321, 576, 411, 281, 915, 264, 1151, 472, 13, 1407, 4584, 341, 11, 50662], "temperature": 0.0, "avg_logprob": -0.20040395890159168, "compression_ratio": 1.5899581589958158, "no_speech_prob": 6.108851084718481e-05}, {"id": 46, "seek": 32916, "start": 335.12, "end": 342.36, "text": " we introduce a loss function, which instead has six inputs, numbers k0 through k5.", "tokens": [50662, 321, 5366, 257, 4470, 2445, 11, 597, 2602, 575, 2309, 15743, 11, 3547, 350, 15, 807, 350, 20, 13, 51024], "temperature": 0.0, "avg_logprob": -0.20040395890159168, "compression_ratio": 1.5899581589958158, "no_speech_prob": 6.108851084718481e-05}, {"id": 47, "seek": 32916, "start": 342.92, "end": 347.28000000000003, "text": " And for each configuration, it constructs the corresponding curve y,", "tokens": [51052, 400, 337, 1184, 11694, 11, 309, 7690, 82, 264, 11760, 7605, 288, 11, 51270], "temperature": 0.0, "avg_logprob": -0.20040395890159168, "compression_ratio": 1.5899581589958158, "no_speech_prob": 6.108851084718481e-05}, {"id": 48, "seek": 32916, "start": 348.12, "end": 357.04, "text": " calculates the distance between observed data points and the curve, and outputs a single number, the particular value of the loss.", "tokens": [51312, 4322, 1024, 264, 4560, 1296, 13095, 1412, 2793, 293, 264, 7605, 11, 293, 23930, 257, 2167, 1230, 11, 264, 1729, 2158, 295, 264, 4470, 13, 51758], "temperature": 0.0, "avg_logprob": -0.20040395890159168, "compression_ratio": 1.5899581589958158, "no_speech_prob": 6.108851084718481e-05}, {"id": 49, "seek": 35704, "start": 357.76000000000005, "end": 363.96000000000004, "text": " Our job then becomes finding the configuration of k's that yields a minimum loss or", "tokens": [50400, 2621, 1691, 550, 3643, 5006, 264, 11694, 295, 350, 311, 300, 32168, 257, 7285, 4470, 420, 50710], "temperature": 0.0, "avg_logprob": -0.2313735100530809, "compression_ratio": 1.7142857142857142, "no_speech_prob": 0.0001398218737449497}, {"id": 50, "seek": 35704, "start": 364.84000000000003, "end": 367.84000000000003, "text": " minimizing the loss function with respect to the coefficients.", "tokens": [50754, 46608, 264, 4470, 2445, 365, 3104, 281, 264, 31994, 13, 50904], "temperature": 0.0, "avg_logprob": -0.2313735100530809, "compression_ratio": 1.7142857142857142, "no_speech_prob": 0.0001398218737449497}, {"id": 51, "seek": 35704, "start": 367.92, "end": 375.92, "text": " Then, plugging these optimal k's into the general equation for the curve will give us the best curve of describing the data.", "tokens": [50908, 1396, 11, 42975, 613, 16252, 350, 311, 666, 264, 2674, 5367, 337, 264, 7605, 486, 976, 505, 264, 1151, 7605, 295, 16141, 264, 1412, 13, 51308], "temperature": 0.0, "avg_logprob": -0.2313735100530809, "compression_ratio": 1.7142857142857142, "no_speech_prob": 0.0001398218737449497}, {"id": 52, "seek": 35704, "start": 376.52000000000004, "end": 383.12, "text": " All right, great, but how do we find this magic configuration of k's that minimizes the loss?", "tokens": [51338, 1057, 558, 11, 869, 11, 457, 577, 360, 321, 915, 341, 5585, 11694, 295, 350, 311, 300, 4464, 5660, 264, 4470, 30, 51668], "temperature": 0.0, "avg_logprob": -0.2313735100530809, "compression_ratio": 1.7142857142857142, "no_speech_prob": 0.0001398218737449497}, {"id": 53, "seek": 35704, "start": 383.56, "end": 385.56, "text": " Well, we might need some help.", "tokens": [51690, 1042, 11, 321, 1062, 643, 512, 854, 13, 51790], "temperature": 0.0, "avg_logprob": -0.2313735100530809, "compression_ratio": 1.7142857142857142, "no_speech_prob": 0.0001398218737449497}, {"id": 54, "seek": 38556, "start": 385.8, "end": 392.52, "text": " Let's build a machine called Curve Fitter 6000, designed to simplify manual calculations.", "tokens": [50376, 961, 311, 1322, 257, 3479, 1219, 7907, 303, 479, 3904, 41789, 11, 4761, 281, 20460, 9688, 20448, 13, 50712], "temperature": 0.0, "avg_logprob": -0.2383242739906794, "compression_ratio": 1.439252336448598, "no_speech_prob": 2.6274738047504798e-05}, {"id": 55, "seek": 38556, "start": 392.52, "end": 399.8, "text": " It is equipped with six adjustable knobs for k0 through k5, which we can freely turn.", "tokens": [50712, 467, 307, 15218, 365, 2309, 27804, 46999, 337, 350, 15, 807, 350, 20, 11, 597, 321, 393, 16433, 1261, 13, 51076], "temperature": 0.0, "avg_logprob": -0.2383242739906794, "compression_ratio": 1.439252336448598, "no_speech_prob": 2.6274738047504798e-05}, {"id": 56, "seek": 38556, "start": 400.12, "end": 407.12, "text": " To begin, we initialize the machine with our data points and then, for each setting of the knobs,", "tokens": [51092, 1407, 1841, 11, 321, 5883, 1125, 264, 3479, 365, 527, 1412, 2793, 293, 550, 11, 337, 1184, 3287, 295, 264, 46999, 11, 51442], "temperature": 0.0, "avg_logprob": -0.2383242739906794, "compression_ratio": 1.439252336448598, "no_speech_prob": 2.6274738047504798e-05}, {"id": 57, "seek": 38556, "start": 407.12, "end": 410.12, "text": " it will evaluate the curve y of x,", "tokens": [51442, 309, 486, 13059, 264, 7605, 288, 295, 2031, 11, 51592], "temperature": 0.0, "avg_logprob": -0.2383242739906794, "compression_ratio": 1.439252336448598, "no_speech_prob": 2.6274738047504798e-05}, {"id": 58, "seek": 41012, "start": 410.6, "end": 416.68, "text": " compute the distance from it to the data points and print out the value of the loss function.", "tokens": [50388, 14722, 264, 4560, 490, 309, 281, 264, 1412, 2793, 293, 4482, 484, 264, 2158, 295, 264, 4470, 2445, 13, 50692], "temperature": 0.0, "avg_logprob": -0.1467540914362127, "compression_ratio": 1.7478991596638656, "no_speech_prob": 0.0001941123919095844}, {"id": 59, "seek": 41012, "start": 417.32, "end": 421.64, "text": " Now, we can begin twisting the knobs in order to find the minimum loss.", "tokens": [50724, 823, 11, 321, 393, 1841, 34491, 264, 46999, 294, 1668, 281, 915, 264, 7285, 4470, 13, 50940], "temperature": 0.0, "avg_logprob": -0.1467540914362127, "compression_ratio": 1.7478991596638656, "no_speech_prob": 0.0001941123919095844}, {"id": 60, "seek": 41012, "start": 422.28000000000003, "end": 428.84000000000003, "text": " For example, let's start with some initial setting and slightly notch knob number one to the right.", "tokens": [50972, 1171, 1365, 11, 718, 311, 722, 365, 512, 5883, 3287, 293, 4748, 26109, 26759, 1230, 472, 281, 264, 558, 13, 51300], "temperature": 0.0, "avg_logprob": -0.1467540914362127, "compression_ratio": 1.7478991596638656, "no_speech_prob": 0.0001941123919095844}, {"id": 61, "seek": 41012, "start": 429.24, "end": 435.88, "text": " The resulting curve changed as well, and we can see that the value of the loss function slightly decreased.", "tokens": [51320, 440, 16505, 7605, 3105, 382, 731, 11, 293, 321, 393, 536, 300, 264, 2158, 295, 264, 4470, 2445, 4748, 24436, 13, 51652], "temperature": 0.0, "avg_logprob": -0.1467540914362127, "compression_ratio": 1.7478991596638656, "no_speech_prob": 0.0001941123919095844}, {"id": 62, "seek": 41012, "start": 436.44, "end": 438.84000000000003, "text": " Great, it means we are on the right track.", "tokens": [51680, 3769, 11, 309, 1355, 321, 366, 322, 264, 558, 2837, 13, 51800], "temperature": 0.0, "avg_logprob": -0.1467540914362127, "compression_ratio": 1.7478991596638656, "no_speech_prob": 0.0001941123919095844}, {"id": 63, "seek": 43884, "start": 439.32, "end": 443.0, "text": " Let's turn knob number one in the same direction once again.", "tokens": [50388, 961, 311, 1261, 26759, 1230, 472, 294, 264, 912, 3513, 1564, 797, 13, 50572], "temperature": 0.0, "avg_logprob": -0.11559788997356708, "compression_ratio": 1.611336032388664, "no_speech_prob": 4.5397624489851296e-05}, {"id": 64, "seek": 43884, "start": 443.0, "end": 446.84, "text": " Uh-oh, this time the fit gets worse and the loss function increases.", "tokens": [50572, 4019, 12, 1445, 11, 341, 565, 264, 3318, 2170, 5324, 293, 264, 4470, 2445, 8637, 13, 50764], "temperature": 0.0, "avg_logprob": -0.11559788997356708, "compression_ratio": 1.611336032388664, "no_speech_prob": 4.5397624489851296e-05}, {"id": 65, "seek": 43884, "start": 447.71999999999997, "end": 450.76, "text": " Apparently, that last notch was a bit too much.", "tokens": [50808, 16755, 11, 300, 1036, 26109, 390, 257, 857, 886, 709, 13, 50960], "temperature": 0.0, "avg_logprob": -0.11559788997356708, "compression_ratio": 1.611336032388664, "no_speech_prob": 4.5397624489851296e-05}, {"id": 66, "seek": 43884, "start": 450.76, "end": 455.32, "text": " So, let's revert the knob to the previous position and try knob two.", "tokens": [50960, 407, 11, 718, 311, 319, 3281, 264, 26759, 281, 264, 3894, 2535, 293, 853, 26759, 732, 13, 51188], "temperature": 0.0, "avg_logprob": -0.11559788997356708, "compression_ratio": 1.611336032388664, "no_speech_prob": 4.5397624489851296e-05}, {"id": 67, "seek": 43884, "start": 455.32, "end": 458.67999999999995, "text": " And we can keep doing this iteratively many, many times,", "tokens": [51188, 400, 321, 393, 1066, 884, 341, 17138, 19020, 867, 11, 867, 1413, 11, 51356], "temperature": 0.0, "avg_logprob": -0.11559788997356708, "compression_ratio": 1.611336032388664, "no_speech_prob": 4.5397624489851296e-05}, {"id": 68, "seek": 43884, "start": 459.23999999999995, "end": 465.4, "text": " nudging each individual knob one at a time to see whether the resulting curve is a better fit.", "tokens": [51384, 40045, 3249, 1184, 2609, 26759, 472, 412, 257, 565, 281, 536, 1968, 264, 16505, 7605, 307, 257, 1101, 3318, 13, 51692], "temperature": 0.0, "avg_logprob": -0.11559788997356708, "compression_ratio": 1.611336032388664, "no_speech_prob": 4.5397624489851296e-05}, {"id": 69, "seek": 46540, "start": 465.56, "end": 473.0, "text": " This is a so-called random perturbation method, since we are essentially wandering in the dark,", "tokens": [50372, 639, 307, 257, 370, 12, 11880, 4974, 40468, 399, 3170, 11, 1670, 321, 366, 4476, 26396, 294, 264, 2877, 11, 50744], "temperature": 0.0, "avg_logprob": -0.08141827583312988, "compression_ratio": 1.6436781609195403, "no_speech_prob": 2.1444928279379383e-05}, {"id": 70, "seek": 46540, "start": 473.0, "end": 478.2, "text": " not knowing in advance how each adjustment will affect the loss function.", "tokens": [50744, 406, 5276, 294, 7295, 577, 1184, 17132, 486, 3345, 264, 4470, 2445, 13, 51004], "temperature": 0.0, "avg_logprob": -0.08141827583312988, "compression_ratio": 1.6436781609195403, "no_speech_prob": 2.1444928279379383e-05}, {"id": 71, "seek": 46540, "start": 478.2, "end": 481.32, "text": " This would certainly work, but it's not very efficient.", "tokens": [51004, 639, 576, 3297, 589, 11, 457, 309, 311, 406, 588, 7148, 13, 51160], "temperature": 0.0, "avg_logprob": -0.08141827583312988, "compression_ratio": 1.6436781609195403, "no_speech_prob": 2.1444928279379383e-05}, {"id": 72, "seek": 46540, "start": 481.32, "end": 486.28, "text": " Is there a way we can be more intelligent about the knob adjustments?", "tokens": [51160, 1119, 456, 257, 636, 321, 393, 312, 544, 13232, 466, 264, 26759, 18624, 30, 51408], "temperature": 0.0, "avg_logprob": -0.08141827583312988, "compression_ratio": 1.6436781609195403, "no_speech_prob": 2.1444928279379383e-05}, {"id": 73, "seek": 46540, "start": 486.28, "end": 490.76, "text": " In the most general case, when the machine is a complete black box,", "tokens": [51408, 682, 264, 881, 2674, 1389, 11, 562, 264, 3479, 307, 257, 3566, 2211, 2424, 11, 51632], "temperature": 0.0, "avg_logprob": -0.08141827583312988, "compression_ratio": 1.6436781609195403, "no_speech_prob": 2.1444928279379383e-05}, {"id": 74, "seek": 46540, "start": 490.76, "end": 494.91999999999996, "text": " nothing better than a random perturbation is guaranteed to exist.", "tokens": [51632, 1825, 1101, 813, 257, 4974, 40468, 399, 307, 18031, 281, 2514, 13, 51840], "temperature": 0.0, "avg_logprob": -0.08141827583312988, "compression_ratio": 1.6436781609195403, "no_speech_prob": 2.1444928279379383e-05}, {"id": 75, "seek": 49492, "start": 494.92, "end": 501.16, "text": " However, a great deal of computations, including what's carried out under the hood of our curvefitter,", "tokens": [50364, 2908, 11, 257, 869, 2028, 295, 2807, 763, 11, 3009, 437, 311, 9094, 484, 833, 264, 13376, 295, 527, 7605, 69, 3904, 11, 50676], "temperature": 0.0, "avg_logprob": -0.08097019824352893, "compression_ratio": 1.5863453815261044, "no_speech_prob": 1.1300796359137166e-05}, {"id": 76, "seek": 49492, "start": 501.88, "end": 506.12, "text": " have a special property to them, something called differentiability", "tokens": [50712, 362, 257, 2121, 4707, 281, 552, 11, 746, 1219, 27372, 2310, 50924], "temperature": 0.0, "avg_logprob": -0.08097019824352893, "compression_ratio": 1.5863453815261044, "no_speech_prob": 1.1300796359137166e-05}, {"id": 77, "seek": 49492, "start": 506.12, "end": 510.6, "text": " that allows us to compute the optimal knob setting much more efficiently.", "tokens": [50924, 300, 4045, 505, 281, 14722, 264, 16252, 26759, 3287, 709, 544, 19621, 13, 51148], "temperature": 0.0, "avg_logprob": -0.08097019824352893, "compression_ratio": 1.5863453815261044, "no_speech_prob": 1.1300796359137166e-05}, {"id": 78, "seek": 49492, "start": 511.16, "end": 515.64, "text": " We will dive deeper into what differentiability means in just a minute.", "tokens": [51176, 492, 486, 9192, 7731, 666, 437, 27372, 2310, 1355, 294, 445, 257, 3456, 13, 51400], "temperature": 0.0, "avg_logprob": -0.08097019824352893, "compression_ratio": 1.5863453815261044, "no_speech_prob": 1.1300796359137166e-05}, {"id": 79, "seek": 49492, "start": 515.64, "end": 520.36, "text": " But for now, let's quickly see the big picture overview of where we are going.", "tokens": [51400, 583, 337, 586, 11, 718, 311, 2661, 536, 264, 955, 3036, 12492, 295, 689, 321, 366, 516, 13, 51636], "temperature": 0.0, "avg_logprob": -0.08097019824352893, "compression_ratio": 1.5863453815261044, "no_speech_prob": 1.1300796359137166e-05}, {"id": 80, "seek": 52036, "start": 521.16, "end": 528.04, "text": " Our goal would be to upgrade the machine so that it would have a tiny screen next to each knob.", "tokens": [50404, 2621, 3387, 576, 312, 281, 11484, 264, 3479, 370, 300, 309, 576, 362, 257, 5870, 2568, 958, 281, 1184, 26759, 13, 50748], "temperature": 0.0, "avg_logprob": -0.09218134766533262, "compression_ratio": 1.6071428571428572, "no_speech_prob": 0.00016603806579951197}, {"id": 81, "seek": 52036, "start": 528.6, "end": 534.28, "text": " And for any configuration, those screens should say which direction you need to", "tokens": [50776, 400, 337, 604, 11694, 11, 729, 11171, 820, 584, 597, 3513, 291, 643, 281, 51060], "temperature": 0.0, "avg_logprob": -0.09218134766533262, "compression_ratio": 1.6071428571428572, "no_speech_prob": 0.00016603806579951197}, {"id": 82, "seek": 52036, "start": 534.28, "end": 539.24, "text": " nudge each knob in order to decrease the loss function and by how much.", "tokens": [51060, 297, 16032, 1184, 26759, 294, 1668, 281, 11514, 264, 4470, 2445, 293, 538, 577, 709, 13, 51308], "temperature": 0.0, "avg_logprob": -0.09218134766533262, "compression_ratio": 1.6071428571428572, "no_speech_prob": 0.00016603806579951197}, {"id": 83, "seek": 52036, "start": 540.2, "end": 541.4, "text": " Think about it for a second.", "tokens": [51356, 6557, 466, 309, 337, 257, 1150, 13, 51416], "temperature": 0.0, "avg_logprob": -0.09218134766533262, "compression_ratio": 1.6071428571428572, "no_speech_prob": 0.00016603806579951197}, {"id": 84, "seek": 52036, "start": 542.2, "end": 548.04, "text": " We are essentially asking the machine to predict the future and estimate the effect", "tokens": [51456, 492, 366, 4476, 3365, 264, 3479, 281, 6069, 264, 2027, 293, 12539, 264, 1802, 51748], "temperature": 0.0, "avg_logprob": -0.09218134766533262, "compression_ratio": 1.6071428571428572, "no_speech_prob": 0.00016603806579951197}, {"id": 85, "seek": 54804, "start": 548.04, "end": 553.88, "text": " of the knob adjustment on the loss function without actually performing that adjustment,", "tokens": [50364, 295, 264, 26759, 17132, 322, 264, 4470, 2445, 1553, 767, 10205, 300, 17132, 11, 50656], "temperature": 0.0, "avg_logprob": -0.06514506186208417, "compression_ratio": 1.697211155378486, "no_speech_prob": 4.2647443478927016e-05}, {"id": 86, "seek": 54804, "start": 553.88, "end": 558.68, "text": " calculating the loss and then reverting the knob back like we did previously.", "tokens": [50656, 28258, 264, 4470, 293, 550, 18438, 783, 264, 26759, 646, 411, 321, 630, 8046, 13, 50896], "temperature": 0.0, "avg_logprob": -0.06514506186208417, "compression_ratio": 1.697211155378486, "no_speech_prob": 4.2647443478927016e-05}, {"id": 87, "seek": 54804, "start": 559.24, "end": 563.8, "text": " Wouldn't this glance into the future violate some sort of principle?", "tokens": [50924, 26291, 380, 341, 21094, 666, 264, 2027, 37478, 512, 1333, 295, 8665, 30, 51152], "temperature": 0.0, "avg_logprob": -0.06514506186208417, "compression_ratio": 1.697211155378486, "no_speech_prob": 4.2647443478927016e-05}, {"id": 88, "seek": 54804, "start": 564.5999999999999, "end": 569.64, "text": " After all, we are jumping to the result of the computation without performing it.", "tokens": [51192, 2381, 439, 11, 321, 366, 11233, 281, 264, 1874, 295, 264, 24903, 1553, 10205, 309, 13, 51444], "temperature": 0.0, "avg_logprob": -0.06514506186208417, "compression_ratio": 1.697211155378486, "no_speech_prob": 4.2647443478927016e-05}, {"id": 89, "seek": 54804, "start": 570.28, "end": 571.7199999999999, "text": " Sounds like cheating, right?", "tokens": [51476, 14576, 411, 18309, 11, 558, 30, 51548], "temperature": 0.0, "avg_logprob": -0.06514506186208417, "compression_ratio": 1.697211155378486, "no_speech_prob": 4.2647443478927016e-05}, {"id": 90, "seek": 54804, "start": 572.92, "end": 577.9599999999999, "text": " Well, it turns out that this idea lies on a very simple mathematical foundation", "tokens": [51608, 1042, 11, 309, 4523, 484, 300, 341, 1558, 9134, 322, 257, 588, 2199, 18894, 7030, 51860], "temperature": 0.0, "avg_logprob": -0.06514506186208417, "compression_ratio": 1.697211155378486, "no_speech_prob": 4.2647443478927016e-05}, {"id": 91, "seek": 57796, "start": 577.96, "end": 581.32, "text": " so let's spend the next few minutes building it up from scratch.", "tokens": [50364, 370, 718, 311, 3496, 264, 958, 1326, 2077, 2390, 309, 493, 490, 8459, 13, 50532], "temperature": 0.0, "avg_logprob": -0.10119932004720858, "compression_ratio": 1.5478927203065134, "no_speech_prob": 9.36870310397353e-06}, {"id": 92, "seek": 57796, "start": 583.5600000000001, "end": 586.84, "text": " All right, let's consider a simpler case first.", "tokens": [50644, 1057, 558, 11, 718, 311, 1949, 257, 18587, 1389, 700, 13, 50808], "temperature": 0.0, "avg_logprob": -0.10119932004720858, "compression_ratio": 1.5478927203065134, "no_speech_prob": 9.36870310397353e-06}, {"id": 93, "seek": 57796, "start": 586.84, "end": 589.8000000000001, "text": " Where we freeze five out of six knobs.", "tokens": [50808, 2305, 321, 15959, 1732, 484, 295, 2309, 46999, 13, 50956], "temperature": 0.0, "avg_logprob": -0.10119932004720858, "compression_ratio": 1.5478927203065134, "no_speech_prob": 9.36870310397353e-06}, {"id": 94, "seek": 57796, "start": 589.8000000000001, "end": 594.6800000000001, "text": " For example, suppose someone tells you that the rest of them are already in the optimal position.", "tokens": [50956, 1171, 1365, 11, 7297, 1580, 5112, 291, 300, 264, 1472, 295, 552, 366, 1217, 294, 264, 16252, 2535, 13, 51200], "temperature": 0.0, "avg_logprob": -0.10119932004720858, "compression_ratio": 1.5478927203065134, "no_speech_prob": 9.36870310397353e-06}, {"id": 95, "seek": 57796, "start": 595.24, "end": 599.96, "text": " So all you need to do is to find the best value for one remaining knob.", "tokens": [51228, 407, 439, 291, 643, 281, 360, 307, 281, 915, 264, 1151, 2158, 337, 472, 8877, 26759, 13, 51464], "temperature": 0.0, "avg_logprob": -0.10119932004720858, "compression_ratio": 1.5478927203065134, "no_speech_prob": 9.36870310397353e-06}, {"id": 96, "seek": 57796, "start": 599.96, "end": 605.5600000000001, "text": " Essentially, the machine now has only one variable parameter k1 that we can tweak.", "tokens": [51464, 23596, 11, 264, 3479, 586, 575, 787, 472, 7006, 13075, 350, 16, 300, 321, 393, 29879, 13, 51744], "temperature": 0.0, "avg_logprob": -0.10119932004720858, "compression_ratio": 1.5478927203065134, "no_speech_prob": 9.36870310397353e-06}, {"id": 97, "seek": 60556, "start": 606.1199999999999, "end": 609.4799999999999, "text": " And so the loss function is also a simpler function", "tokens": [50392, 400, 370, 264, 4470, 2445, 307, 611, 257, 18587, 2445, 50560], "temperature": 0.0, "avg_logprob": -0.0880160871541725, "compression_ratio": 1.7413127413127414, "no_speech_prob": 4.006362723885104e-05}, {"id": 98, "seek": 60556, "start": 609.4799999999999, "end": 615.4799999999999, "text": " which accepts one number, the knob setting, and outputs another number, the loss value.", "tokens": [50560, 597, 33538, 472, 1230, 11, 264, 26759, 3287, 11, 293, 23930, 1071, 1230, 11, 264, 4470, 2158, 13, 50860], "temperature": 0.0, "avg_logprob": -0.0880160871541725, "compression_ratio": 1.7413127413127414, "no_speech_prob": 4.006362723885104e-05}, {"id": 99, "seek": 60556, "start": 615.4799999999999, "end": 621.9599999999999, "text": " As a function of one variable, it can be conveniently visualized as a graph in a two-dimensional plane", "tokens": [50860, 1018, 257, 2445, 295, 472, 7006, 11, 309, 393, 312, 44375, 5056, 1602, 382, 257, 4295, 294, 257, 732, 12, 18759, 5720, 51184], "temperature": 0.0, "avg_logprob": -0.0880160871541725, "compression_ratio": 1.7413127413127414, "no_speech_prob": 4.006362723885104e-05}, {"id": 100, "seek": 60556, "start": 621.9599999999999, "end": 625.7199999999999, "text": " which captures the relationship between the input and the output.", "tokens": [51184, 597, 27986, 264, 2480, 1296, 264, 4846, 293, 264, 5598, 13, 51372], "temperature": 0.0, "avg_logprob": -0.0880160871541725, "compression_ratio": 1.7413127413127414, "no_speech_prob": 4.006362723885104e-05}, {"id": 101, "seek": 60556, "start": 625.7199999999999, "end": 632.1999999999999, "text": " For example, it may have this shape right here and our goal is to find this value of k1", "tokens": [51372, 1171, 1365, 11, 309, 815, 362, 341, 3909, 558, 510, 293, 527, 3387, 307, 281, 915, 341, 2158, 295, 350, 16, 51696], "temperature": 0.0, "avg_logprob": -0.0880160871541725, "compression_ratio": 1.7413127413127414, "no_speech_prob": 4.006362723885104e-05}, {"id": 102, "seek": 60556, "start": 632.1999999999999, "end": 635.0799999999999, "text": " which corresponds to the minimum of the loss function.", "tokens": [51696, 597, 23249, 281, 264, 7285, 295, 264, 4470, 2445, 13, 51840], "temperature": 0.0, "avg_logprob": -0.0880160871541725, "compression_ratio": 1.7413127413127414, "no_speech_prob": 4.006362723885104e-05}, {"id": 103, "seek": 63508, "start": 635.08, "end": 638.6800000000001, "text": " But we don't have access to the true underlying shape.", "tokens": [50364, 583, 321, 500, 380, 362, 2105, 281, 264, 2074, 14217, 3909, 13, 50544], "temperature": 0.0, "avg_logprob": -0.055822885261391696, "compression_ratio": 1.6923076923076923, "no_speech_prob": 2.1444922822411172e-05}, {"id": 104, "seek": 63508, "start": 638.6800000000001, "end": 642.12, "text": " All we can do is to set the knob at a chosen position", "tokens": [50544, 1057, 321, 393, 360, 307, 281, 992, 264, 26759, 412, 257, 8614, 2535, 50716], "temperature": 0.0, "avg_logprob": -0.055822885261391696, "compression_ratio": 1.6923076923076923, "no_speech_prob": 2.1444922822411172e-05}, {"id": 105, "seek": 63508, "start": 642.6800000000001, "end": 646.36, "text": " and kind of query the machine for the value of the loss.", "tokens": [50744, 293, 733, 295, 14581, 264, 3479, 337, 264, 2158, 295, 264, 4470, 13, 50928], "temperature": 0.0, "avg_logprob": -0.055822885261391696, "compression_ratio": 1.6923076923076923, "no_speech_prob": 2.1444922822411172e-05}, {"id": 106, "seek": 63508, "start": 646.36, "end": 652.2800000000001, "text": " In other words, we can only sample individual points along the function we're trying to minimize.", "tokens": [50928, 682, 661, 2283, 11, 321, 393, 787, 6889, 2609, 2793, 2051, 264, 2445, 321, 434, 1382, 281, 17522, 13, 51224], "temperature": 0.0, "avg_logprob": -0.055822885261391696, "compression_ratio": 1.6923076923076923, "no_speech_prob": 2.1444922822411172e-05}, {"id": 107, "seek": 63508, "start": 652.84, "end": 658.44, "text": " And we are essentially blind to how the function behaves in between the known points", "tokens": [51252, 400, 321, 366, 4476, 6865, 281, 577, 264, 2445, 36896, 294, 1296, 264, 2570, 2793, 51532], "temperature": 0.0, "avg_logprob": -0.055822885261391696, "compression_ratio": 1.6923076923076923, "no_speech_prob": 2.1444922822411172e-05}, {"id": 108, "seek": 63508, "start": 658.44, "end": 660.0400000000001, "text": " before we sample them.", "tokens": [51532, 949, 321, 6889, 552, 13, 51612], "temperature": 0.0, "avg_logprob": -0.055822885261391696, "compression_ratio": 1.6923076923076923, "no_speech_prob": 2.1444922822411172e-05}, {"id": 109, "seek": 63508, "start": 660.0400000000001, "end": 663.5600000000001, "text": " But suppose we would like to know something more about the function.", "tokens": [51612, 583, 7297, 321, 576, 411, 281, 458, 746, 544, 466, 264, 2445, 13, 51788], "temperature": 0.0, "avg_logprob": -0.055822885261391696, "compression_ratio": 1.6923076923076923, "no_speech_prob": 2.1444922822411172e-05}, {"id": 110, "seek": 66356, "start": 663.56, "end": 665.64, "text": " Not just each value at each point.", "tokens": [50364, 1726, 445, 1184, 2158, 412, 1184, 935, 13, 50468], "temperature": 0.0, "avg_logprob": -0.06331229932380444, "compression_ratio": 1.7226890756302522, "no_speech_prob": 1.7231563106179237e-05}, {"id": 111, "seek": 66356, "start": 666.52, "end": 670.52, "text": " For example, whether at this point the function is going up or down.", "tokens": [50512, 1171, 1365, 11, 1968, 412, 341, 935, 264, 2445, 307, 516, 493, 420, 760, 13, 50712], "temperature": 0.0, "avg_logprob": -0.06331229932380444, "compression_ratio": 1.7226890756302522, "no_speech_prob": 1.7231563106179237e-05}, {"id": 112, "seek": 66356, "start": 671.16, "end": 674.76, "text": " This information will ultimately guide our adjustments.", "tokens": [50744, 639, 1589, 486, 6284, 5934, 527, 18624, 13, 50924], "temperature": 0.0, "avg_logprob": -0.06331229932380444, "compression_ratio": 1.7226890756302522, "no_speech_prob": 1.7231563106179237e-05}, {"id": 113, "seek": 66356, "start": 675.3199999999999, "end": 680.04, "text": " Because if you know that the function is going down as you increase the input,", "tokens": [50952, 1436, 498, 291, 458, 300, 264, 2445, 307, 516, 760, 382, 291, 3488, 264, 4846, 11, 51188], "temperature": 0.0, "avg_logprob": -0.06331229932380444, "compression_ratio": 1.7226890756302522, "no_speech_prob": 1.7231563106179237e-05}, {"id": 114, "seek": 66356, "start": 680.04, "end": 683.0799999999999, "text": " turning the knob to the right is a safe bet,", "tokens": [51188, 6246, 264, 26759, 281, 264, 558, 307, 257, 3273, 778, 11, 51340], "temperature": 0.0, "avg_logprob": -0.06331229932380444, "compression_ratio": 1.7226890756302522, "no_speech_prob": 1.7231563106179237e-05}, {"id": 115, "seek": 66356, "start": 683.0799999999999, "end": 687.0, "text": " since you are guaranteed to decrease the loss with this manipulation.", "tokens": [51340, 1670, 291, 366, 18031, 281, 11514, 264, 4470, 365, 341, 26475, 13, 51536], "temperature": 0.0, "avg_logprob": -0.06331229932380444, "compression_ratio": 1.7226890756302522, "no_speech_prob": 1.7231563106179237e-05}, {"id": 116, "seek": 66356, "start": 687.56, "end": 691.64, "text": " Let's put this notion of going up or down around a point", "tokens": [51564, 961, 311, 829, 341, 10710, 295, 516, 493, 420, 760, 926, 257, 935, 51768], "temperature": 0.0, "avg_logprob": -0.06331229932380444, "compression_ratio": 1.7226890756302522, "no_speech_prob": 1.7231563106179237e-05}, {"id": 117, "seek": 69164, "start": 691.64, "end": 693.88, "text": " on a stronger mathematical ground.", "tokens": [50364, 322, 257, 7249, 18894, 2727, 13, 50476], "temperature": 0.0, "avg_logprob": -0.0815955194933661, "compression_ratio": 1.5776699029126213, "no_speech_prob": 7.031173299765214e-05}, {"id": 118, "seek": 69164, "start": 693.88, "end": 698.36, "text": " Suppose we have just sampled the point x0, y0 on this graph.", "tokens": [50476, 21360, 321, 362, 445, 3247, 15551, 264, 935, 2031, 15, 11, 288, 15, 322, 341, 4295, 13, 50700], "temperature": 0.0, "avg_logprob": -0.0815955194933661, "compression_ratio": 1.5776699029126213, "no_speech_prob": 7.031173299765214e-05}, {"id": 119, "seek": 69164, "start": 698.36, "end": 703.64, "text": " What we can do is increase the input by a small amount delta x.", "tokens": [50700, 708, 321, 393, 360, 307, 3488, 264, 4846, 538, 257, 1359, 2372, 8289, 2031, 13, 50964], "temperature": 0.0, "avg_logprob": -0.0815955194933661, "compression_ratio": 1.5776699029126213, "no_speech_prob": 7.031173299765214e-05}, {"id": 120, "seek": 69164, "start": 703.64, "end": 708.36, "text": " This new adjusted input will result in a new value of y,", "tokens": [50964, 639, 777, 19871, 4846, 486, 1874, 294, 257, 777, 2158, 295, 288, 11, 51200], "temperature": 0.0, "avg_logprob": -0.0815955194933661, "compression_ratio": 1.5776699029126213, "no_speech_prob": 7.031173299765214e-05}, {"id": 121, "seek": 69164, "start": 708.36, "end": 712.04, "text": " which will differ from the old value by some delta y.", "tokens": [51200, 597, 486, 743, 490, 264, 1331, 2158, 538, 512, 8289, 288, 13, 51384], "temperature": 0.0, "avg_logprob": -0.0815955194933661, "compression_ratio": 1.5776699029126213, "no_speech_prob": 7.031173299765214e-05}, {"id": 122, "seek": 69164, "start": 712.6, "end": 716.4399999999999, "text": " This delta depends on the magnitude of our adjustment.", "tokens": [51412, 639, 8289, 5946, 322, 264, 15668, 295, 527, 17132, 13, 51604], "temperature": 0.0, "avg_logprob": -0.0815955194933661, "compression_ratio": 1.5776699029126213, "no_speech_prob": 7.031173299765214e-05}, {"id": 123, "seek": 71644, "start": 716.44, "end": 721.08, "text": " For example, if we take a step delta x, which is 10 times smaller,", "tokens": [50364, 1171, 1365, 11, 498, 321, 747, 257, 1823, 8289, 2031, 11, 597, 307, 1266, 1413, 4356, 11, 50596], "temperature": 0.0, "avg_logprob": -0.10131593779021618, "compression_ratio": 1.6533333333333333, "no_speech_prob": 5.771911219198955e-06}, {"id": 124, "seek": 71644, "start": 721.08, "end": 725.24, "text": " delta y will also be approximately 10 times as small.", "tokens": [50596, 8289, 288, 486, 611, 312, 10447, 1266, 1413, 382, 1359, 13, 50804], "temperature": 0.0, "avg_logprob": -0.10131593779021618, "compression_ratio": 1.6533333333333333, "no_speech_prob": 5.771911219198955e-06}, {"id": 125, "seek": 71644, "start": 726.6800000000001, "end": 731.96, "text": " This is why it makes sense to take the ratio delta y over delta x,", "tokens": [50876, 639, 307, 983, 309, 1669, 2020, 281, 747, 264, 8509, 8289, 288, 670, 8289, 2031, 11, 51140], "temperature": 0.0, "avg_logprob": -0.10131593779021618, "compression_ratio": 1.6533333333333333, "no_speech_prob": 5.771911219198955e-06}, {"id": 126, "seek": 71644, "start": 731.96, "end": 736.2800000000001, "text": " the amount of change in the output per unit change in the input.", "tokens": [51140, 264, 2372, 295, 1319, 294, 264, 5598, 680, 4985, 1319, 294, 264, 4846, 13, 51356], "temperature": 0.0, "avg_logprob": -0.10131593779021618, "compression_ratio": 1.6533333333333333, "no_speech_prob": 5.771911219198955e-06}, {"id": 127, "seek": 71644, "start": 736.9200000000001, "end": 741.6400000000001, "text": " Graphically, this ratio corresponds to a slope of a straight line.", "tokens": [51388, 21884, 984, 11, 341, 8509, 23249, 281, 257, 13525, 295, 257, 2997, 1622, 13, 51624], "temperature": 0.0, "avg_logprob": -0.10131593779021618, "compression_ratio": 1.6533333333333333, "no_speech_prob": 5.771911219198955e-06}, {"id": 128, "seek": 71644, "start": 741.6400000000001, "end": 746.36, "text": " Going through the points x0, y0 and x0 plus delta x,", "tokens": [51624, 10963, 807, 264, 2793, 2031, 15, 11, 288, 15, 293, 2031, 15, 1804, 8289, 2031, 11, 51860], "temperature": 0.0, "avg_logprob": -0.10131593779021618, "compression_ratio": 1.6533333333333333, "no_speech_prob": 5.771911219198955e-06}, {"id": 129, "seek": 74636, "start": 746.44, "end": 747.96, "text": " y0 plus delta y.", "tokens": [50368, 288, 15, 1804, 8289, 288, 13, 50444], "temperature": 0.0, "avg_logprob": -0.08505485718508801, "compression_ratio": 1.6071428571428572, "no_speech_prob": 1.1843123502330855e-05}, {"id": 130, "seek": 74636, "start": 748.92, "end": 752.6800000000001, "text": " Notice that as we take smaller and smaller steps,", "tokens": [50492, 13428, 300, 382, 321, 747, 4356, 293, 4356, 4439, 11, 50680], "temperature": 0.0, "avg_logprob": -0.08505485718508801, "compression_ratio": 1.6071428571428572, "no_speech_prob": 1.1843123502330855e-05}, {"id": 131, "seek": 74636, "start": 752.6800000000001, "end": 757.4, "text": " this straight line will more and more accurately align with the graph", "tokens": [50680, 341, 2997, 1622, 486, 544, 293, 544, 20095, 7975, 365, 264, 4295, 50916], "temperature": 0.0, "avg_logprob": -0.08505485718508801, "compression_ratio": 1.6071428571428572, "no_speech_prob": 1.1843123502330855e-05}, {"id": 132, "seek": 74636, "start": 757.4, "end": 760.2, "text": " in the neighborhood of the point x0, y0.", "tokens": [50916, 294, 264, 7630, 295, 264, 935, 2031, 15, 11, 288, 15, 13, 51056], "temperature": 0.0, "avg_logprob": -0.08505485718508801, "compression_ratio": 1.6071428571428572, "no_speech_prob": 1.1843123502330855e-05}, {"id": 133, "seek": 74636, "start": 760.92, "end": 767.32, "text": " Let's take a limit of this ratio as delta x goes to infinitely small values.", "tokens": [51092, 961, 311, 747, 257, 4948, 295, 341, 8509, 382, 8289, 2031, 1709, 281, 36227, 1359, 4190, 13, 51412], "temperature": 0.0, "avg_logprob": -0.08505485718508801, "compression_ratio": 1.6071428571428572, "no_speech_prob": 1.1843123502330855e-05}, {"id": 134, "seek": 74636, "start": 767.32, "end": 771.8000000000001, "text": " Then this limiting case value, which this ratio converges to", "tokens": [51412, 1396, 341, 22083, 1389, 2158, 11, 597, 341, 8509, 9652, 2880, 281, 51636], "temperature": 0.0, "avg_logprob": -0.08505485718508801, "compression_ratio": 1.6071428571428572, "no_speech_prob": 1.1843123502330855e-05}, {"id": 135, "seek": 77180, "start": 771.8, "end": 777.64, "text": " for infinitesimally small delta x's, is what is called the derivative oa function,", "tokens": [50364, 337, 7193, 3324, 332, 379, 1359, 8289, 2031, 311, 11, 307, 437, 307, 1219, 264, 13760, 277, 64, 2445, 11, 50656], "temperature": 0.0, "avg_logprob": -0.09960227336698366, "compression_ratio": 1.811659192825112, "no_speech_prob": 1.6701407730579376e-05}, {"id": 136, "seek": 77180, "start": 778.1999999999999, "end": 781.3199999999999, "text": " and it is denoted by dy over dx.", "tokens": [50684, 293, 309, 307, 1441, 23325, 538, 14584, 670, 30017, 13, 50840], "temperature": 0.0, "avg_logprob": -0.09960227336698366, "compression_ratio": 1.811659192825112, "no_speech_prob": 1.6701407730579376e-05}, {"id": 137, "seek": 77180, "start": 781.9599999999999, "end": 785.9599999999999, "text": " Visually, the derivative oa function at some point", "tokens": [50872, 10410, 671, 11, 264, 13760, 277, 64, 2445, 412, 512, 935, 51072], "temperature": 0.0, "avg_logprob": -0.09960227336698366, "compression_ratio": 1.811659192825112, "no_speech_prob": 1.6701407730579376e-05}, {"id": 138, "seek": 77180, "start": 785.9599999999999, "end": 789.88, "text": " is the slope of the line that is tangent to the graph,", "tokens": [51072, 307, 264, 13525, 295, 264, 1622, 300, 307, 27747, 281, 264, 4295, 11, 51268], "temperature": 0.0, "avg_logprob": -0.09960227336698366, "compression_ratio": 1.811659192825112, "no_speech_prob": 1.6701407730579376e-05}, {"id": 139, "seek": 77180, "start": 789.88, "end": 793.4, "text": " and thus corresponds to the instantaneous rate of change,", "tokens": [51268, 293, 8807, 23249, 281, 264, 45596, 3314, 295, 1319, 11, 51444], "temperature": 0.0, "avg_logprob": -0.09960227336698366, "compression_ratio": 1.811659192825112, "no_speech_prob": 1.6701407730579376e-05}, {"id": 140, "seek": 77180, "start": 793.4, "end": 797.16, "text": " or steepness of that function around that point.", "tokens": [51444, 420, 16841, 1287, 295, 300, 2445, 926, 300, 935, 13, 51632], "temperature": 0.0, "avg_logprob": -0.09960227336698366, "compression_ratio": 1.811659192825112, "no_speech_prob": 1.6701407730579376e-05}, {"id": 141, "seek": 77180, "start": 797.16, "end": 801.64, "text": " But different points along the graph might have different steepness values,", "tokens": [51632, 583, 819, 2793, 2051, 264, 4295, 1062, 362, 819, 16841, 1287, 4190, 11, 51856], "temperature": 0.0, "avg_logprob": -0.09960227336698366, "compression_ratio": 1.811659192825112, "no_speech_prob": 1.6701407730579376e-05}, {"id": 142, "seek": 80164, "start": 801.96, "end": 806.52, "text": " so the derivative of the entire function is not a single number.", "tokens": [50380, 370, 264, 13760, 295, 264, 2302, 2445, 307, 406, 257, 2167, 1230, 13, 50608], "temperature": 0.0, "avg_logprob": -0.06241067250569662, "compression_ratio": 1.68, "no_speech_prob": 1.5206804164336063e-05}, {"id": 143, "seek": 80164, "start": 806.52, "end": 812.52, "text": " In fact, the derivative dy by dx is itself a function of x", "tokens": [50608, 682, 1186, 11, 264, 13760, 14584, 538, 30017, 307, 2564, 257, 2445, 295, 2031, 50908], "temperature": 0.0, "avg_logprob": -0.06241067250569662, "compression_ratio": 1.68, "no_speech_prob": 1.5206804164336063e-05}, {"id": 144, "seek": 80164, "start": 812.52, "end": 820.1999999999999, "text": " that takes an arbitrary value of x and outputs the local steepness of y of x at that point.", "tokens": [50908, 300, 2516, 364, 23211, 2158, 295, 2031, 293, 23930, 264, 2654, 16841, 1287, 295, 288, 295, 2031, 412, 300, 935, 13, 51292], "temperature": 0.0, "avg_logprob": -0.06241067250569662, "compression_ratio": 1.68, "no_speech_prob": 1.5206804164336063e-05}, {"id": 145, "seek": 80164, "start": 820.1999999999999, "end": 825.24, "text": " This definition assigns to every function its derivative alter ego.", "tokens": [51292, 639, 7123, 6269, 82, 281, 633, 2445, 1080, 13760, 11337, 14495, 13, 51544], "temperature": 0.0, "avg_logprob": -0.06241067250569662, "compression_ratio": 1.68, "no_speech_prob": 1.5206804164336063e-05}, {"id": 146, "seek": 80164, "start": 825.24, "end": 829.16, "text": " Another function operating on the same input domain,", "tokens": [51544, 3996, 2445, 7447, 322, 264, 912, 4846, 9274, 11, 51740], "temperature": 0.0, "avg_logprob": -0.06241067250569662, "compression_ratio": 1.68, "no_speech_prob": 1.5206804164336063e-05}, {"id": 147, "seek": 82916, "start": 829.16, "end": 834.12, "text": " which carries information about the steepness of the original function.", "tokens": [50364, 597, 16402, 1589, 466, 264, 16841, 1287, 295, 264, 3380, 2445, 13, 50612], "temperature": 0.0, "avg_logprob": -0.08123599461146763, "compression_ratio": 1.689516129032258, "no_speech_prob": 2.507158205844462e-05}, {"id": 148, "seek": 82916, "start": 834.12, "end": 835.88, "text": " There is a bit of a subtlety.", "tokens": [50612, 821, 307, 257, 857, 295, 257, 7257, 75, 2210, 13, 50700], "temperature": 0.0, "avg_logprob": -0.08123599461146763, "compression_ratio": 1.689516129032258, "no_speech_prob": 2.507158205844462e-05}, {"id": 149, "seek": 82916, "start": 835.88, "end": 839.0, "text": " Strictly speaking, the derivative may not exist", "tokens": [50700, 745, 3740, 356, 4124, 11, 264, 13760, 815, 406, 2514, 50856], "temperature": 0.0, "avg_logprob": -0.08123599461146763, "compression_ratio": 1.689516129032258, "no_speech_prob": 2.507158205844462e-05}, {"id": 150, "seek": 82916, "start": 839.0, "end": 842.28, "text": " if the function doesn't have a steepness around some point.", "tokens": [50856, 498, 264, 2445, 1177, 380, 362, 257, 16841, 1287, 926, 512, 935, 13, 51020], "temperature": 0.0, "avg_logprob": -0.08123599461146763, "compression_ratio": 1.689516129032258, "no_speech_prob": 2.507158205844462e-05}, {"id": 151, "seek": 82916, "start": 842.92, "end": 846.68, "text": " For example, if it has sharp corners or discontinuities.", "tokens": [51052, 1171, 1365, 11, 498, 309, 575, 8199, 12413, 420, 31420, 84, 1088, 13, 51240], "temperature": 0.0, "avg_logprob": -0.08123599461146763, "compression_ratio": 1.689516129032258, "no_speech_prob": 2.507158205844462e-05}, {"id": 152, "seek": 82916, "start": 847.7199999999999, "end": 852.36, "text": " However, for the remainder of the video, we are going to assume that all functions we are", "tokens": [51292, 2908, 11, 337, 264, 29837, 295, 264, 960, 11, 321, 366, 516, 281, 6552, 300, 439, 6828, 321, 366, 51524], "temperature": 0.0, "avg_logprob": -0.08123599461146763, "compression_ratio": 1.689516129032258, "no_speech_prob": 2.507158205844462e-05}, {"id": 153, "seek": 82916, "start": 852.36, "end": 856.6, "text": " dealing with are smooth, so that the derivative always exists.", "tokens": [51524, 6260, 365, 366, 5508, 11, 370, 300, 264, 13760, 1009, 8198, 13, 51736], "temperature": 0.0, "avg_logprob": -0.08123599461146763, "compression_ratio": 1.689516129032258, "no_speech_prob": 2.507158205844462e-05}, {"id": 154, "seek": 85660, "start": 857.5600000000001, "end": 861.08, "text": " This is a reasonable claim, because we can control", "tokens": [50412, 639, 307, 257, 10585, 3932, 11, 570, 321, 393, 1969, 50588], "temperature": 0.0, "avg_logprob": -0.11720225085382877, "compression_ratio": 1.5527426160337552, "no_speech_prob": 6.401995051419362e-05}, {"id": 155, "seek": 85660, "start": 861.08, "end": 864.52, "text": " what sort of functions go into our models when we build them.", "tokens": [50588, 437, 1333, 295, 6828, 352, 666, 527, 5245, 562, 321, 1322, 552, 13, 50760], "temperature": 0.0, "avg_logprob": -0.11720225085382877, "compression_ratio": 1.5527426160337552, "no_speech_prob": 6.401995051419362e-05}, {"id": 156, "seek": 85660, "start": 865.08, "end": 869.96, "text": " And people usually restrict everything to smooth or differentiable functions", "tokens": [50788, 400, 561, 2673, 7694, 1203, 281, 5508, 420, 819, 9364, 6828, 51032], "temperature": 0.0, "avg_logprob": -0.11720225085382877, "compression_ratio": 1.5527426160337552, "no_speech_prob": 6.401995051419362e-05}, {"id": 157, "seek": 85660, "start": 869.96, "end": 872.12, "text": " to make all the math work out nicely.", "tokens": [51032, 281, 652, 439, 264, 5221, 589, 484, 9594, 13, 51140], "temperature": 0.0, "avg_logprob": -0.11720225085382877, "compression_ratio": 1.5527426160337552, "no_speech_prob": 6.401995051419362e-05}, {"id": 158, "seek": 85660, "start": 872.6800000000001, "end": 873.8000000000001, "text": " All right, great.", "tokens": [51168, 1057, 558, 11, 869, 13, 51224], "temperature": 0.0, "avg_logprob": -0.11720225085382877, "compression_ratio": 1.5527426160337552, "no_speech_prob": 6.401995051419362e-05}, {"id": 159, "seek": 85660, "start": 873.8000000000001, "end": 877.72, "text": " Now, along with the underlying loss as a function of k1,", "tokens": [51224, 823, 11, 2051, 365, 264, 14217, 4470, 382, 257, 2445, 295, 350, 16, 11, 51420], "temperature": 0.0, "avg_logprob": -0.11720225085382877, "compression_ratio": 1.5527426160337552, "no_speech_prob": 6.401995051419362e-05}, {"id": 160, "seek": 85660, "start": 877.72, "end": 882.0400000000001, "text": " which is hidden from us, we can also reason about its derivative.", "tokens": [51420, 597, 307, 7633, 490, 505, 11, 321, 393, 611, 1778, 466, 1080, 13760, 13, 51636], "temperature": 0.0, "avg_logprob": -0.11720225085382877, "compression_ratio": 1.5527426160337552, "no_speech_prob": 6.401995051419362e-05}, {"id": 161, "seek": 88204, "start": 882.68, "end": 886.12, "text": " Another function of k1, which we also don't know,", "tokens": [50396, 3996, 2445, 295, 350, 16, 11, 597, 321, 611, 500, 380, 458, 11, 50568], "temperature": 0.0, "avg_logprob": -0.06889317207729694, "compression_ratio": 1.6883116883116882, "no_speech_prob": 0.00014425934932660311}, {"id": 162, "seek": 88204, "start": 886.12, "end": 889.8, "text": " that is equal to the steepness of the loss function at that point.", "tokens": [50568, 300, 307, 2681, 281, 264, 16841, 1287, 295, 264, 4470, 2445, 412, 300, 935, 13, 50752], "temperature": 0.0, "avg_logprob": -0.06889317207729694, "compression_ratio": 1.6883116883116882, "no_speech_prob": 0.00014425934932660311}, {"id": 163, "seek": 88204, "start": 891.16, "end": 895.4, "text": " Let's suppose that similarly to how we can query the loss function", "tokens": [50820, 961, 311, 7297, 300, 14138, 281, 577, 321, 393, 14581, 264, 4470, 2445, 51032], "temperature": 0.0, "avg_logprob": -0.06889317207729694, "compression_ratio": 1.6883116883116882, "no_speech_prob": 0.00014425934932660311}, {"id": 164, "seek": 88204, "start": 895.4, "end": 899.0, "text": " by running our machine and obtaining individual samples.", "tokens": [51032, 538, 2614, 527, 3479, 293, 36749, 2609, 10938, 13, 51212], "temperature": 0.0, "avg_logprob": -0.06889317207729694, "compression_ratio": 1.6883116883116882, "no_speech_prob": 0.00014425934932660311}, {"id": 165, "seek": 88204, "start": 899.7199999999999, "end": 904.04, "text": " There is a mechanism for us to sample the derivative function as well.", "tokens": [51248, 821, 307, 257, 7513, 337, 505, 281, 6889, 264, 13760, 2445, 382, 731, 13, 51464], "temperature": 0.0, "avg_logprob": -0.06889317207729694, "compression_ratio": 1.6883116883116882, "no_speech_prob": 0.00014425934932660311}, {"id": 166, "seek": 88204, "start": 905.4, "end": 911.88, "text": " So, for every input value of k1, the machine will output the value of the loss", "tokens": [51532, 407, 11, 337, 633, 4846, 2158, 295, 350, 16, 11, 264, 3479, 486, 5598, 264, 2158, 295, 264, 4470, 51856], "temperature": 0.0, "avg_logprob": -0.06889317207729694, "compression_ratio": 1.6883116883116882, "no_speech_prob": 0.00014425934932660311}, {"id": 167, "seek": 91188, "start": 911.88, "end": 915.64, "text": " and the local steepness of the loss function around that point.", "tokens": [50364, 293, 264, 2654, 16841, 1287, 295, 264, 4470, 2445, 926, 300, 935, 13, 50552], "temperature": 0.0, "avg_logprob": -0.05332347892579578, "compression_ratio": 1.5520361990950227, "no_speech_prob": 2.710869739530608e-05}, {"id": 168, "seek": 91188, "start": 916.28, "end": 920.6, "text": " Notice that this derivative information is exactly the sort of", "tokens": [50584, 13428, 300, 341, 13760, 1589, 307, 2293, 264, 1333, 295, 50800], "temperature": 0.0, "avg_logprob": -0.05332347892579578, "compression_ratio": 1.5520361990950227, "no_speech_prob": 2.710869739530608e-05}, {"id": 169, "seek": 91188, "start": 920.6, "end": 925.08, "text": " look into the future we were looking for to make smarter knob adjustments.", "tokens": [50800, 574, 666, 264, 2027, 321, 645, 1237, 337, 281, 652, 20294, 26759, 18624, 13, 51024], "temperature": 0.0, "avg_logprob": -0.05332347892579578, "compression_ratio": 1.5520361990950227, "no_speech_prob": 2.710869739530608e-05}, {"id": 170, "seek": 91188, "start": 925.72, "end": 930.6, "text": " For example, let's use it to efficiently find the optimal value of k1.", "tokens": [51056, 1171, 1365, 11, 718, 311, 764, 309, 281, 19621, 915, 264, 16252, 2158, 295, 350, 16, 13, 51300], "temperature": 0.0, "avg_logprob": -0.05332347892579578, "compression_ratio": 1.5520361990950227, "no_speech_prob": 2.710869739530608e-05}, {"id": 171, "seek": 91188, "start": 931.4, "end": 933.0, "text": " What we can do is the following.", "tokens": [51340, 708, 321, 393, 360, 307, 264, 3480, 13, 51420], "temperature": 0.0, "avg_logprob": -0.05332347892579578, "compression_ratio": 1.5520361990950227, "no_speech_prob": 2.710869739530608e-05}, {"id": 172, "seek": 91188, "start": 933.88, "end": 936.6, "text": " First, start at some random position.", "tokens": [51464, 2386, 11, 722, 412, 512, 4974, 2535, 13, 51600], "temperature": 0.0, "avg_logprob": -0.05332347892579578, "compression_ratio": 1.5520361990950227, "no_speech_prob": 2.710869739530608e-05}, {"id": 173, "seek": 93660, "start": 937.5600000000001, "end": 940.12, "text": " Ask the machine for the value of the loss", "tokens": [50412, 12320, 264, 3479, 337, 264, 2158, 295, 264, 4470, 50540], "temperature": 0.0, "avg_logprob": -0.059372091756283656, "compression_ratio": 1.8783783783783783, "no_speech_prob": 9.314573981100693e-05}, {"id": 174, "seek": 93660, "start": 940.12, "end": 943.32, "text": " and the derivative of the loss function at that position.", "tokens": [50540, 293, 264, 13760, 295, 264, 4470, 2445, 412, 300, 2535, 13, 50700], "temperature": 0.0, "avg_logprob": -0.059372091756283656, "compression_ratio": 1.8783783783783783, "no_speech_prob": 9.314573981100693e-05}, {"id": 175, "seek": 93660, "start": 944.44, "end": 948.6, "text": " Take a tiny step in the direction opposite of the derivative.", "tokens": [50756, 3664, 257, 5870, 1823, 294, 264, 3513, 6182, 295, 264, 13760, 13, 50964], "temperature": 0.0, "avg_logprob": -0.059372091756283656, "compression_ratio": 1.8783783783783783, "no_speech_prob": 9.314573981100693e-05}, {"id": 176, "seek": 93660, "start": 949.24, "end": 953.24, "text": " If the derivative is negative, it means that the function is going down.", "tokens": [50996, 759, 264, 13760, 307, 3671, 11, 309, 1355, 300, 264, 2445, 307, 516, 760, 13, 51196], "temperature": 0.0, "avg_logprob": -0.059372091756283656, "compression_ratio": 1.8783783783783783, "no_speech_prob": 9.314573981100693e-05}, {"id": 177, "seek": 93660, "start": 953.96, "end": 956.52, "text": " And so, if we want to arrive at the minimum,", "tokens": [51232, 400, 370, 11, 498, 321, 528, 281, 8881, 412, 264, 7285, 11, 51360], "temperature": 0.0, "avg_logprob": -0.059372091756283656, "compression_ratio": 1.8783783783783783, "no_speech_prob": 9.314573981100693e-05}, {"id": 178, "seek": 93660, "start": 956.52, "end": 960.44, "text": " we need to move in the direction of increasing value of k1.", "tokens": [51360, 321, 643, 281, 1286, 294, 264, 3513, 295, 5662, 2158, 295, 350, 16, 13, 51556], "temperature": 0.0, "avg_logprob": -0.059372091756283656, "compression_ratio": 1.8783783783783783, "no_speech_prob": 9.314573981100693e-05}, {"id": 179, "seek": 93660, "start": 961.08, "end": 966.28, "text": " Repeat this procedure until you reach the point where the derivative is zero,", "tokens": [51588, 28523, 341, 10747, 1826, 291, 2524, 264, 935, 689, 264, 13760, 307, 4018, 11, 51848], "temperature": 0.0, "avg_logprob": -0.059372091756283656, "compression_ratio": 1.8783783783783783, "no_speech_prob": 9.314573981100693e-05}, {"id": 180, "seek": 96628, "start": 966.28, "end": 970.6, "text": " which essentially corresponds to the minimum where the tangent line is flat.", "tokens": [50364, 597, 4476, 23249, 281, 264, 7285, 689, 264, 27747, 1622, 307, 4962, 13, 50580], "temperature": 0.0, "avg_logprob": -0.09130483478694768, "compression_ratio": 1.5462555066079295, "no_speech_prob": 8.530316335964017e-06}, {"id": 181, "seek": 96628, "start": 971.24, "end": 975.0, "text": " Essentially, each adjustment in such a guided fashion", "tokens": [50612, 23596, 11, 1184, 17132, 294, 1270, 257, 19663, 6700, 50800], "temperature": 0.0, "avg_logprob": -0.09130483478694768, "compression_ratio": 1.5462555066079295, "no_speech_prob": 8.530316335964017e-06}, {"id": 182, "seek": 96628, "start": 975.0, "end": 981.0799999999999, "text": " works kind of like a ball rolling down the hill along the graph until it reaches a valley.", "tokens": [50800, 1985, 733, 295, 411, 257, 2594, 9439, 760, 264, 10997, 2051, 264, 4295, 1826, 309, 14235, 257, 17636, 13, 51104], "temperature": 0.0, "avg_logprob": -0.09130483478694768, "compression_ratio": 1.5462555066079295, "no_speech_prob": 8.530316335964017e-06}, {"id": 183, "seek": 96628, "start": 983.48, "end": 987.56, "text": " Although in the beginning we froze five out of six knobs for simplicity,", "tokens": [51224, 5780, 294, 264, 2863, 321, 46077, 1732, 484, 295, 2309, 46999, 337, 25632, 11, 51428], "temperature": 0.0, "avg_logprob": -0.09130483478694768, "compression_ratio": 1.5462555066079295, "no_speech_prob": 8.530316335964017e-06}, {"id": 184, "seek": 96628, "start": 988.12, "end": 991.88, "text": " this process is easily carried out to higher dimensions.", "tokens": [51456, 341, 1399, 307, 3612, 9094, 484, 281, 2946, 12819, 13, 51644], "temperature": 0.0, "avg_logprob": -0.09130483478694768, "compression_ratio": 1.5462555066079295, "no_speech_prob": 8.530316335964017e-06}, {"id": 185, "seek": 99188, "start": 992.84, "end": 998.6, "text": " For example, suppose now we are free to tweak two different knobs, k1 and k2.", "tokens": [50412, 1171, 1365, 11, 7297, 586, 321, 366, 1737, 281, 29879, 732, 819, 46999, 11, 350, 16, 293, 350, 17, 13, 50700], "temperature": 0.0, "avg_logprob": -0.09587725964221326, "compression_ratio": 1.5619469026548674, "no_speech_prob": 5.829122892464511e-05}, {"id": 186, "seek": 99188, "start": 999.64, "end": 1005.8, "text": " The loss would become a function of two variables, which can be visualized as a surface.", "tokens": [50752, 440, 4470, 576, 1813, 257, 2445, 295, 732, 9102, 11, 597, 393, 312, 5056, 1602, 382, 257, 3753, 13, 51060], "temperature": 0.0, "avg_logprob": -0.09587725964221326, "compression_ratio": 1.5619469026548674, "no_speech_prob": 5.829122892464511e-05}, {"id": 187, "seek": 99188, "start": 1006.68, "end": 1008.68, "text": " But what about the derivative?", "tokens": [51104, 583, 437, 466, 264, 13760, 30, 51204], "temperature": 0.0, "avg_logprob": -0.09587725964221326, "compression_ratio": 1.5619469026548674, "no_speech_prob": 5.829122892464511e-05}, {"id": 188, "seek": 99188, "start": 1008.68, "end": 1012.92, "text": " Recall that by definition, the derivative at each point", "tokens": [51204, 9647, 336, 300, 538, 7123, 11, 264, 13760, 412, 1184, 935, 51416], "temperature": 0.0, "avg_logprob": -0.09587725964221326, "compression_ratio": 1.5619469026548674, "no_speech_prob": 5.829122892464511e-05}, {"id": 189, "seek": 99188, "start": 1012.92, "end": 1017.8, "text": " tells us how the output changes per unit change of the input.", "tokens": [51416, 5112, 505, 577, 264, 5598, 2962, 680, 4985, 1319, 295, 264, 4846, 13, 51660], "temperature": 0.0, "avg_logprob": -0.09587725964221326, "compression_ratio": 1.5619469026548674, "no_speech_prob": 5.829122892464511e-05}, {"id": 190, "seek": 99188, "start": 1017.8, "end": 1019.88, "text": " But now we have two different inputs.", "tokens": [51660, 583, 586, 321, 362, 732, 819, 15743, 13, 51764], "temperature": 0.0, "avg_logprob": -0.09587725964221326, "compression_ratio": 1.5619469026548674, "no_speech_prob": 5.829122892464511e-05}, {"id": 191, "seek": 101988, "start": 1020.4399999999999, "end": 1023.72, "text": " Should we nudge only k1, k2 or both?", "tokens": [50392, 6454, 321, 297, 16032, 787, 350, 16, 11, 350, 17, 420, 1293, 30, 50556], "temperature": 0.0, "avg_logprob": -0.0929310635516518, "compression_ratio": 1.5527638190954773, "no_speech_prob": 4.006367453257553e-05}, {"id": 192, "seek": 101988, "start": 1025.64, "end": 1029.8, "text": " Essentially, our function will have two different derivatives", "tokens": [50652, 23596, 11, 527, 2445, 486, 362, 732, 819, 33733, 50860], "temperature": 0.0, "avg_logprob": -0.0929310635516518, "compression_ratio": 1.5527638190954773, "no_speech_prob": 4.006367453257553e-05}, {"id": 193, "seek": 101988, "start": 1030.44, "end": 1037.0, "text": " that are usually called partial derivatives because of this ambiguity which input to nudge.", "tokens": [50892, 300, 366, 2673, 1219, 14641, 33733, 570, 295, 341, 46519, 597, 4846, 281, 297, 16032, 13, 51220], "temperature": 0.0, "avg_logprob": -0.0929310635516518, "compression_ratio": 1.5527638190954773, "no_speech_prob": 4.006367453257553e-05}, {"id": 194, "seek": 101988, "start": 1037.0, "end": 1041.4, "text": " Namely, when we have two knobs, the derivative of the loss function", "tokens": [51220, 10684, 736, 11, 562, 321, 362, 732, 46999, 11, 264, 13760, 295, 264, 4470, 2445, 51440], "temperature": 0.0, "avg_logprob": -0.0929310635516518, "compression_ratio": 1.5527638190954773, "no_speech_prob": 4.006367453257553e-05}, {"id": 195, "seek": 101988, "start": 1041.4, "end": 1045.08, "text": " with respect to parameter k1 is written like this.", "tokens": [51440, 365, 3104, 281, 13075, 350, 16, 307, 3720, 411, 341, 13, 51624], "temperature": 0.0, "avg_logprob": -0.0929310635516518, "compression_ratio": 1.5527638190954773, "no_speech_prob": 4.006367453257553e-05}, {"id": 196, "seek": 104508, "start": 1045.32, "end": 1053.48, "text": " It is how much the output changes per unit change in k1 if you hold k2 constant.", "tokens": [50376, 467, 307, 577, 709, 264, 5598, 2962, 680, 4985, 1319, 294, 350, 16, 498, 291, 1797, 350, 17, 5754, 13, 50784], "temperature": 0.0, "avg_logprob": -0.1245146009657118, "compression_ratio": 1.6354679802955665, "no_speech_prob": 7.967291458044201e-05}, {"id": 197, "seek": 104508, "start": 1054.12, "end": 1059.32, "text": " And conversely, this expression tells you the rate of change of the output", "tokens": [50816, 400, 2615, 736, 11, 341, 6114, 5112, 291, 264, 3314, 295, 1319, 295, 264, 5598, 51076], "temperature": 0.0, "avg_logprob": -0.1245146009657118, "compression_ratio": 1.6354679802955665, "no_speech_prob": 7.967291458044201e-05}, {"id": 198, "seek": 104508, "start": 1059.32, "end": 1063.48, "text": " if you hold k1 constant and slightly nudge k2.", "tokens": [51076, 498, 291, 1797, 350, 16, 5754, 293, 4748, 297, 16032, 350, 17, 13, 51284], "temperature": 0.0, "avg_logprob": -0.1245146009657118, "compression_ratio": 1.6354679802955665, "no_speech_prob": 7.967291458044201e-05}, {"id": 199, "seek": 104508, "start": 1064.04, "end": 1070.76, "text": " Geometrically, you can imagine slicing the surface with planes parallel to the axes,", "tokens": [51312, 2876, 649, 81, 984, 11, 291, 393, 3811, 46586, 264, 3753, 365, 14952, 8952, 281, 264, 35387, 11, 51648], "temperature": 0.0, "avg_logprob": -0.1245146009657118, "compression_ratio": 1.6354679802955665, "no_speech_prob": 7.967291458044201e-05}, {"id": 200, "seek": 104508, "start": 1070.76, "end": 1073.96, "text": " intersecting at the point of interest k1 k2.", "tokens": [51648, 27815, 278, 412, 264, 935, 295, 1179, 350, 16, 350, 17, 13, 51808], "temperature": 0.0, "avg_logprob": -0.1245146009657118, "compression_ratio": 1.6354679802955665, "no_speech_prob": 7.967291458044201e-05}, {"id": 201, "seek": 107396, "start": 1074.6000000000001, "end": 1080.44, "text": " So that each of the two cross sections is like a one-dimensional graph of the loss", "tokens": [50396, 407, 300, 1184, 295, 264, 732, 3278, 10863, 307, 411, 257, 472, 12, 18759, 4295, 295, 264, 4470, 50688], "temperature": 0.0, "avg_logprob": -0.0787641950275587, "compression_ratio": 1.7723214285714286, "no_speech_prob": 3.4268472518306226e-05}, {"id": 202, "seek": 107396, "start": 1080.44, "end": 1084.68, "text": " as a function of one variable while the other one is kept constant.", "tokens": [50688, 382, 257, 2445, 295, 472, 7006, 1339, 264, 661, 472, 307, 4305, 5754, 13, 50900], "temperature": 0.0, "avg_logprob": -0.0787641950275587, "compression_ratio": 1.7723214285714286, "no_speech_prob": 3.4268472518306226e-05}, {"id": 203, "seek": 107396, "start": 1085.32, "end": 1089.24, "text": " Then the slope of a tangent line at each cross section", "tokens": [50932, 1396, 264, 13525, 295, 257, 27747, 1622, 412, 1184, 3278, 3541, 51128], "temperature": 0.0, "avg_logprob": -0.0787641950275587, "compression_ratio": 1.7723214285714286, "no_speech_prob": 3.4268472518306226e-05}, {"id": 204, "seek": 107396, "start": 1089.24, "end": 1093.64, "text": " will give you a corresponding partial derivative of the loss at that point.", "tokens": [51128, 486, 976, 291, 257, 11760, 14641, 13760, 295, 264, 4470, 412, 300, 935, 13, 51348], "temperature": 0.0, "avg_logprob": -0.0787641950275587, "compression_ratio": 1.7723214285714286, "no_speech_prob": 3.4268472518306226e-05}, {"id": 205, "seek": 107396, "start": 1094.76, "end": 1098.52, "text": " While thinking about partial derivatives as two separate surfaces,", "tokens": [51404, 3987, 1953, 466, 14641, 33733, 382, 732, 4994, 16130, 11, 51592], "temperature": 0.0, "avg_logprob": -0.0787641950275587, "compression_ratio": 1.7723214285714286, "no_speech_prob": 3.4268472518306226e-05}, {"id": 206, "seek": 107396, "start": 1099.08, "end": 1102.3600000000001, "text": " one for each variable, is a perfectly valid way.", "tokens": [51620, 472, 337, 1184, 7006, 11, 307, 257, 6239, 7363, 636, 13, 51784], "temperature": 0.0, "avg_logprob": -0.0787641950275587, "compression_ratio": 1.7723214285714286, "no_speech_prob": 3.4268472518306226e-05}, {"id": 207, "seek": 110236, "start": 1103.08, "end": 1109.8799999999999, "text": " People usually plug the two different values into a vector called a gradient vector.", "tokens": [50400, 3432, 2673, 5452, 264, 732, 819, 4190, 666, 257, 8062, 1219, 257, 16235, 8062, 13, 50740], "temperature": 0.0, "avg_logprob": -0.09470709732600621, "compression_ratio": 1.6807511737089202, "no_speech_prob": 0.0001088967255782336}, {"id": 208, "seek": 110236, "start": 1109.8799999999999, "end": 1116.12, "text": " Essentially, this is a mapping from two input values to another two numbers", "tokens": [50740, 23596, 11, 341, 307, 257, 18350, 490, 732, 4846, 4190, 281, 1071, 732, 3547, 51052], "temperature": 0.0, "avg_logprob": -0.09470709732600621, "compression_ratio": 1.6807511737089202, "no_speech_prob": 0.0001088967255782336}, {"id": 209, "seek": 110236, "start": 1116.12, "end": 1122.6799999999998, "text": " where the first signifies how much the output changes per tiny change in the first input.", "tokens": [51052, 689, 264, 700, 1465, 11221, 577, 709, 264, 5598, 2962, 680, 5870, 1319, 294, 264, 700, 4846, 13, 51380], "temperature": 0.0, "avg_logprob": -0.09470709732600621, "compression_ratio": 1.6807511737089202, "no_speech_prob": 0.0001088967255782336}, {"id": 210, "seek": 110236, "start": 1123.3999999999999, "end": 1125.3999999999999, "text": " And similarly, for the second input.", "tokens": [51416, 400, 14138, 11, 337, 264, 1150, 4846, 13, 51516], "temperature": 0.0, "avg_logprob": -0.09470709732600621, "compression_ratio": 1.6807511737089202, "no_speech_prob": 0.0001088967255782336}, {"id": 211, "seek": 110236, "start": 1126.52, "end": 1130.6799999999998, "text": " Geometrically, this vector points in the direction of steepest ascent.", "tokens": [51572, 2876, 649, 81, 984, 11, 341, 8062, 2793, 294, 264, 3513, 295, 16841, 377, 382, 2207, 13, 51780], "temperature": 0.0, "avg_logprob": -0.09470709732600621, "compression_ratio": 1.6807511737089202, "no_speech_prob": 0.0001088967255782336}, {"id": 212, "seek": 113068, "start": 1131.48, "end": 1136.3600000000001, "text": " So if you want to minimize a function, like in the case for our loss,", "tokens": [50404, 407, 498, 291, 528, 281, 17522, 257, 2445, 11, 411, 294, 264, 1389, 337, 527, 4470, 11, 50648], "temperature": 0.0, "avg_logprob": -0.08557061934739016, "compression_ratio": 1.7293577981651376, "no_speech_prob": 6.401999416993931e-05}, {"id": 213, "seek": 113068, "start": 1137.0800000000002, "end": 1141.48, "text": " we need to take steps in the direction opposite to this gradient.", "tokens": [50684, 321, 643, 281, 747, 4439, 294, 264, 3513, 6182, 281, 341, 16235, 13, 50904], "temperature": 0.0, "avg_logprob": -0.08557061934739016, "compression_ratio": 1.7293577981651376, "no_speech_prob": 6.401999416993931e-05}, {"id": 214, "seek": 113068, "start": 1142.8400000000001, "end": 1149.96, "text": " This iterative procedure of nudging the parameters in the direction opposite of the gradient vector", "tokens": [50972, 639, 17138, 1166, 10747, 295, 40045, 3249, 264, 9834, 294, 264, 3513, 6182, 295, 264, 16235, 8062, 51328], "temperature": 0.0, "avg_logprob": -0.08557061934739016, "compression_ratio": 1.7293577981651376, "no_speech_prob": 6.401999416993931e-05}, {"id": 215, "seek": 113068, "start": 1149.96, "end": 1153.72, "text": " is called gradient descent, which you have probably heard of.", "tokens": [51328, 307, 1219, 16235, 23475, 11, 597, 291, 362, 1391, 2198, 295, 13, 51516], "temperature": 0.0, "avg_logprob": -0.08557061934739016, "compression_ratio": 1.7293577981651376, "no_speech_prob": 6.401999416993931e-05}, {"id": 216, "seek": 113068, "start": 1153.72, "end": 1158.52, "text": " This is analogous to a ball rolling down the hill for the two-dimensional case.", "tokens": [51516, 639, 307, 16660, 563, 281, 257, 2594, 9439, 760, 264, 10997, 337, 264, 732, 12, 18759, 1389, 13, 51756], "temperature": 0.0, "avg_logprob": -0.08557061934739016, "compression_ratio": 1.7293577981651376, "no_speech_prob": 6.401999416993931e-05}, {"id": 217, "seek": 115852, "start": 1158.52, "end": 1163.6399999999999, "text": " And the partial derivatives essentially tell you which direction is downhill.", "tokens": [50364, 400, 264, 14641, 33733, 4476, 980, 291, 597, 3513, 307, 29929, 13, 50620], "temperature": 0.0, "avg_logprob": -0.05801942871838081, "compression_ratio": 1.5822222222222222, "no_speech_prob": 7.411275873892009e-06}, {"id": 218, "seek": 115852, "start": 1164.44, "end": 1169.08, "text": " Going beyond two dimensions is impossible to visualize directly,", "tokens": [50660, 10963, 4399, 732, 12819, 307, 6243, 281, 23273, 3838, 11, 50892], "temperature": 0.0, "avg_logprob": -0.05801942871838081, "compression_ratio": 1.5822222222222222, "no_speech_prob": 7.411275873892009e-06}, {"id": 219, "seek": 115852, "start": 1169.08, "end": 1171.48, "text": " but the math stays exactly the same.", "tokens": [50892, 457, 264, 5221, 10834, 2293, 264, 912, 13, 51012], "temperature": 0.0, "avg_logprob": -0.05801942871838081, "compression_ratio": 1.5822222222222222, "no_speech_prob": 7.411275873892009e-06}, {"id": 220, "seek": 115852, "start": 1172.44, "end": 1177.0, "text": " For instance, if we are now free to tweak all the six knobs,", "tokens": [51060, 1171, 5197, 11, 498, 321, 366, 586, 1737, 281, 29879, 439, 264, 2309, 46999, 11, 51288], "temperature": 0.0, "avg_logprob": -0.05801942871838081, "compression_ratio": 1.5822222222222222, "no_speech_prob": 7.411275873892009e-06}, {"id": 221, "seek": 115852, "start": 1177.0, "end": 1181.16, "text": " the loss function is a hyper surface in six dimensions.", "tokens": [51288, 264, 4470, 2445, 307, 257, 9848, 3753, 294, 2309, 12819, 13, 51496], "temperature": 0.0, "avg_logprob": -0.05801942871838081, "compression_ratio": 1.5822222222222222, "no_speech_prob": 7.411275873892009e-06}, {"id": 222, "seek": 115852, "start": 1181.8, "end": 1185.8, "text": " And the gradient vector now has six numbers packed into it.", "tokens": [51528, 400, 264, 16235, 8062, 586, 575, 2309, 3547, 13265, 666, 309, 13, 51728], "temperature": 0.0, "avg_logprob": -0.05801942871838081, "compression_ratio": 1.5822222222222222, "no_speech_prob": 7.411275873892009e-06}, {"id": 223, "seek": 118580, "start": 1186.36, "end": 1190.28, "text": " But it still points in the direction of steepest ascent.", "tokens": [50392, 583, 309, 920, 2793, 294, 264, 3513, 295, 16841, 377, 382, 2207, 13, 50588], "temperature": 0.0, "avg_logprob": -0.053065648078918455, "compression_ratio": 1.6487603305785123, "no_speech_prob": 1.2411472198436968e-05}, {"id": 224, "seek": 118580, "start": 1190.28, "end": 1195.1599999999999, "text": " So if we iteratively take small steps in the direction opposite to it,", "tokens": [50588, 407, 498, 321, 17138, 19020, 747, 1359, 4439, 294, 264, 3513, 6182, 281, 309, 11, 50832], "temperature": 0.0, "avg_logprob": -0.053065648078918455, "compression_ratio": 1.6487603305785123, "no_speech_prob": 1.2411472198436968e-05}, {"id": 225, "seek": 118580, "start": 1195.8799999999999, "end": 1200.44, "text": " we are going to roll the ball down the hill in six dimensions", "tokens": [50868, 321, 366, 516, 281, 3373, 264, 2594, 760, 264, 10997, 294, 2309, 12819, 51096], "temperature": 0.0, "avg_logprob": -0.053065648078918455, "compression_ratio": 1.6487603305785123, "no_speech_prob": 1.2411472198436968e-05}, {"id": 226, "seek": 118580, "start": 1200.44, "end": 1203.6399999999999, "text": " and eventually reach the minimum of the loss function.", "tokens": [51096, 293, 4728, 2524, 264, 7285, 295, 264, 4470, 2445, 13, 51256], "temperature": 0.0, "avg_logprob": -0.053065648078918455, "compression_ratio": 1.6487603305785123, "no_speech_prob": 1.2411472198436968e-05}, {"id": 227, "seek": 118580, "start": 1204.52, "end": 1206.04, "text": " Great, let's back up a bit.", "tokens": [51300, 3769, 11, 718, 311, 646, 493, 257, 857, 13, 51376], "temperature": 0.0, "avg_logprob": -0.053065648078918455, "compression_ratio": 1.6487603305785123, "no_speech_prob": 1.2411472198436968e-05}, {"id": 228, "seek": 118580, "start": 1206.76, "end": 1211.1599999999999, "text": " Remember how we were looking for ways to add screens next to each knob", "tokens": [51412, 5459, 577, 321, 645, 1237, 337, 2098, 281, 909, 11171, 958, 281, 1184, 26759, 51632], "temperature": 0.0, "avg_logprob": -0.053065648078918455, "compression_ratio": 1.6487603305785123, "no_speech_prob": 1.2411472198436968e-05}, {"id": 229, "seek": 118580, "start": 1211.1599999999999, "end": 1214.28, "text": " that would give us the direction of optimal adjustment?", "tokens": [51632, 300, 576, 976, 505, 264, 3513, 295, 16252, 17132, 30, 51788], "temperature": 0.0, "avg_logprob": -0.053065648078918455, "compression_ratio": 1.6487603305785123, "no_speech_prob": 1.2411472198436968e-05}, {"id": 230, "seek": 121428, "start": 1215.24, "end": 1220.6, "text": " Well, it is essentially nothing more but the components of the gradient vector.", "tokens": [50412, 1042, 11, 309, 307, 4476, 1825, 544, 457, 264, 6677, 295, 264, 16235, 8062, 13, 50680], "temperature": 0.0, "avg_logprob": -0.11090086741619799, "compression_ratio": 1.608294930875576, "no_speech_prob": 1.9223160052206367e-05}, {"id": 231, "seek": 121428, "start": 1220.6, "end": 1225.16, "text": " If at a particular configuration, the partial derivative of the loss", "tokens": [50680, 759, 412, 257, 1729, 11694, 11, 264, 14641, 13760, 295, 264, 4470, 50908], "temperature": 0.0, "avg_logprob": -0.11090086741619799, "compression_ratio": 1.608294930875576, "no_speech_prob": 1.9223160052206367e-05}, {"id": 232, "seek": 121428, "start": 1225.16, "end": 1232.52, "text": " with respect to k1 is positive, it means that increasing k1 will lead to increased loss.", "tokens": [50908, 365, 3104, 281, 350, 16, 307, 3353, 11, 309, 1355, 300, 5662, 350, 16, 486, 1477, 281, 6505, 4470, 13, 51276], "temperature": 0.0, "avg_logprob": -0.11090086741619799, "compression_ratio": 1.608294930875576, "no_speech_prob": 1.9223160052206367e-05}, {"id": 233, "seek": 121428, "start": 1233.08, "end": 1237.3999999999999, "text": " So we need to decrease the value of the knob by turning it to the left.", "tokens": [51304, 407, 321, 643, 281, 11514, 264, 2158, 295, 264, 26759, 538, 6246, 309, 281, 264, 1411, 13, 51520], "temperature": 0.0, "avg_logprob": -0.11090086741619799, "compression_ratio": 1.608294930875576, "no_speech_prob": 1.9223160052206367e-05}, {"id": 234, "seek": 121428, "start": 1238.04, "end": 1240.68, "text": " And similarly for all other parameters.", "tokens": [51552, 400, 14138, 337, 439, 661, 9834, 13, 51684], "temperature": 0.0, "avg_logprob": -0.11090086741619799, "compression_ratio": 1.608294930875576, "no_speech_prob": 1.9223160052206367e-05}, {"id": 235, "seek": 124068, "start": 1241.64, "end": 1246.76, "text": " This is how the derivatives serve as these windows into the future", "tokens": [50412, 639, 307, 577, 264, 33733, 4596, 382, 613, 9309, 666, 264, 2027, 50668], "temperature": 0.0, "avg_logprob": -0.06493655443191529, "compression_ratio": 1.6108597285067874, "no_speech_prob": 2.8857088182121515e-05}, {"id": 236, "seek": 124068, "start": 1246.76, "end": 1251.72, "text": " by providing us with information about local behavior of the function.", "tokens": [50668, 538, 6530, 505, 365, 1589, 466, 2654, 5223, 295, 264, 2445, 13, 50916], "temperature": 0.0, "avg_logprob": -0.06493655443191529, "compression_ratio": 1.6108597285067874, "no_speech_prob": 2.8857088182121515e-05}, {"id": 237, "seek": 124068, "start": 1251.72, "end": 1255.16, "text": " And once we have a way of accessing the derivative,", "tokens": [50916, 400, 1564, 321, 362, 257, 636, 295, 26440, 264, 13760, 11, 51088], "temperature": 0.0, "avg_logprob": -0.06493655443191529, "compression_ratio": 1.6108597285067874, "no_speech_prob": 2.8857088182121515e-05}, {"id": 238, "seek": 124068, "start": 1255.16, "end": 1261.16, "text": " we can perform gradient descent and efficiently find the minimum of the loss function,", "tokens": [51088, 321, 393, 2042, 16235, 23475, 293, 19621, 915, 264, 7285, 295, 264, 4470, 2445, 11, 51388], "temperature": 0.0, "avg_logprob": -0.06493655443191529, "compression_ratio": 1.6108597285067874, "no_speech_prob": 2.8857088182121515e-05}, {"id": 239, "seek": 124068, "start": 1261.16, "end": 1263.72, "text": " thus solving the optimization problem.", "tokens": [51388, 8807, 12606, 264, 19618, 1154, 13, 51516], "temperature": 0.0, "avg_logprob": -0.06493655443191529, "compression_ratio": 1.6108597285067874, "no_speech_prob": 2.8857088182121515e-05}, {"id": 240, "seek": 124068, "start": 1264.28, "end": 1267.16, "text": " However, there is an elephant in a room.", "tokens": [51544, 2908, 11, 456, 307, 364, 19791, 294, 257, 1808, 13, 51688], "temperature": 0.0, "avg_logprob": -0.06493655443191529, "compression_ratio": 1.6108597285067874, "no_speech_prob": 2.8857088182121515e-05}, {"id": 241, "seek": 126716, "start": 1267.16, "end": 1271.96, "text": " So far we have implicitly assumed the derivative information is given to us.", "tokens": [50364, 407, 1400, 321, 362, 26947, 356, 15895, 264, 13760, 1589, 307, 2212, 281, 505, 13, 50604], "temperature": 0.0, "avg_logprob": -0.07367953599668016, "compression_ratio": 1.7529411764705882, "no_speech_prob": 1.0129984730156139e-05}, {"id": 242, "seek": 126716, "start": 1272.52, "end": 1276.2, "text": " Or that we can sample the derivative at a given point.", "tokens": [50632, 1610, 300, 321, 393, 6889, 264, 13760, 412, 257, 2212, 935, 13, 50816], "temperature": 0.0, "avg_logprob": -0.07367953599668016, "compression_ratio": 1.7529411764705882, "no_speech_prob": 1.0129984730156139e-05}, {"id": 243, "seek": 126716, "start": 1276.2, "end": 1279.5600000000002, "text": " Similarly to how we sample the loss function itself", "tokens": [50816, 13157, 281, 577, 321, 6889, 264, 4470, 2445, 2564, 50984], "temperature": 0.0, "avg_logprob": -0.07367953599668016, "compression_ratio": 1.7529411764705882, "no_speech_prob": 1.0129984730156139e-05}, {"id": 244, "seek": 126716, "start": 1279.5600000000002, "end": 1281.88, "text": " by running the calculation of the machine.", "tokens": [50984, 538, 2614, 264, 17108, 295, 264, 3479, 13, 51100], "temperature": 0.0, "avg_logprob": -0.07367953599668016, "compression_ratio": 1.7529411764705882, "no_speech_prob": 1.0129984730156139e-05}, {"id": 245, "seek": 126716, "start": 1281.88, "end": 1284.6000000000001, "text": " But how do you actually find the derivative?", "tokens": [51100, 583, 577, 360, 291, 767, 915, 264, 13760, 30, 51236], "temperature": 0.0, "avg_logprob": -0.07367953599668016, "compression_ratio": 1.7529411764705882, "no_speech_prob": 1.0129984730156139e-05}, {"id": 246, "seek": 126716, "start": 1284.6000000000001, "end": 1289.3200000000002, "text": " As we will see further, this is the main purpose of the back propagation algorithm.", "tokens": [51236, 1018, 321, 486, 536, 3052, 11, 341, 307, 264, 2135, 4334, 295, 264, 646, 38377, 9284, 13, 51472], "temperature": 0.0, "avg_logprob": -0.07367953599668016, "compression_ratio": 1.7529411764705882, "no_speech_prob": 1.0129984730156139e-05}, {"id": 247, "seek": 126716, "start": 1290.1200000000001, "end": 1295.5600000000002, "text": " Essentially, the way we find derivatives of arbitrarily complex functions is the following.", "tokens": [51512, 23596, 11, 264, 636, 321, 915, 33733, 295, 19071, 3289, 3997, 6828, 307, 264, 3480, 13, 51784], "temperature": 0.0, "avg_logprob": -0.07367953599668016, "compression_ratio": 1.7529411764705882, "no_speech_prob": 1.0129984730156139e-05}, {"id": 248, "seek": 129556, "start": 1296.2, "end": 1299.96, "text": " First, there are a handful of building blocks to begin with.", "tokens": [50396, 2386, 11, 456, 366, 257, 16458, 295, 2390, 8474, 281, 1841, 365, 13, 50584], "temperature": 0.0, "avg_logprob": -0.07895324446938255, "compression_ratio": 1.61864406779661, "no_speech_prob": 7.967292185639963e-05}, {"id": 249, "seek": 129556, "start": 1299.96, "end": 1303.96, "text": " Simple functions, derivatives of which are known from calculus.", "tokens": [50584, 21532, 6828, 11, 33733, 295, 597, 366, 2570, 490, 33400, 13, 50784], "temperature": 0.0, "avg_logprob": -0.07895324446938255, "compression_ratio": 1.61864406779661, "no_speech_prob": 7.967292185639963e-05}, {"id": 250, "seek": 129556, "start": 1304.6799999999998, "end": 1308.6, "text": " These are the kind of derivative formulas you often memorize in college.", "tokens": [50820, 1981, 366, 264, 733, 295, 13760, 30546, 291, 2049, 27478, 294, 3859, 13, 51016], "temperature": 0.0, "avg_logprob": -0.07895324446938255, "compression_ratio": 1.61864406779661, "no_speech_prob": 7.967292185639963e-05}, {"id": 251, "seek": 129556, "start": 1309.3999999999999, "end": 1315.8799999999999, "text": " For example, if the function is linear, it's pretty clear that its derivative will be a constant,", "tokens": [51056, 1171, 1365, 11, 498, 264, 2445, 307, 8213, 11, 309, 311, 1238, 1850, 300, 1080, 13760, 486, 312, 257, 5754, 11, 51380], "temperature": 0.0, "avg_logprob": -0.07895324446938255, "compression_ratio": 1.61864406779661, "no_speech_prob": 7.967292185639963e-05}, {"id": 252, "seek": 129556, "start": 1315.8799999999999, "end": 1321.6399999999999, "text": " equal to the slope of that line everywhere, which coincides with its own tangent line.", "tokens": [51380, 2681, 281, 264, 13525, 295, 300, 1622, 5315, 11, 597, 13001, 1875, 365, 1080, 1065, 27747, 1622, 13, 51668], "temperature": 0.0, "avg_logprob": -0.07895324446938255, "compression_ratio": 1.61864406779661, "no_speech_prob": 7.967292185639963e-05}, {"id": 253, "seek": 132164, "start": 1322.6000000000001, "end": 1328.1200000000001, "text": " A parabola x squared becomes more steep as you increase x.", "tokens": [50412, 316, 45729, 4711, 2031, 8889, 3643, 544, 16841, 382, 291, 3488, 2031, 13, 50688], "temperature": 0.0, "avg_logprob": -0.07195608790327863, "compression_ratio": 1.5462962962962963, "no_speech_prob": 9.915264672599733e-05}, {"id": 254, "seek": 132164, "start": 1328.1200000000001, "end": 1330.68, "text": " And its derivative is actually 2x.", "tokens": [50688, 400, 1080, 13760, 307, 767, 568, 87, 13, 50816], "temperature": 0.0, "avg_logprob": -0.07195608790327863, "compression_ratio": 1.5462962962962963, "no_speech_prob": 9.915264672599733e-05}, {"id": 255, "seek": 132164, "start": 1331.5600000000002, "end": 1336.68, "text": " In fact, there is a more general formula for the derivative of x to the power of n.", "tokens": [50860, 682, 1186, 11, 456, 307, 257, 544, 2674, 8513, 337, 264, 13760, 295, 2031, 281, 264, 1347, 295, 297, 13, 51116], "temperature": 0.0, "avg_logprob": -0.07195608790327863, "compression_ratio": 1.5462962962962963, "no_speech_prob": 9.915264672599733e-05}, {"id": 256, "seek": 132164, "start": 1337.48, "end": 1342.76, "text": " Similarly, derivatives of the exponent and logarithm can be written down explicitly.", "tokens": [51156, 13157, 11, 33733, 295, 264, 37871, 293, 41473, 32674, 393, 312, 3720, 760, 20803, 13, 51420], "temperature": 0.0, "avg_logprob": -0.07195608790327863, "compression_ratio": 1.5462962962962963, "no_speech_prob": 9.915264672599733e-05}, {"id": 257, "seek": 132164, "start": 1343.64, "end": 1348.2, "text": " But these are just individual examples of simple, well-known functions.", "tokens": [51464, 583, 613, 366, 445, 2609, 5110, 295, 2199, 11, 731, 12, 6861, 6828, 13, 51692], "temperature": 0.0, "avg_logprob": -0.07195608790327863, "compression_ratio": 1.5462962962962963, "no_speech_prob": 9.915264672599733e-05}, {"id": 258, "seek": 134820, "start": 1348.8400000000001, "end": 1355.96, "text": " In order to compute arbitrary derivatives, we need a way to combine such atomic building blocks", "tokens": [50396, 682, 1668, 281, 14722, 23211, 33733, 11, 321, 643, 257, 636, 281, 10432, 1270, 22275, 2390, 8474, 50752], "temperature": 0.0, "avg_logprob": -0.06678408257504727, "compression_ratio": 1.8302752293577982, "no_speech_prob": 4.832558988709934e-05}, {"id": 259, "seek": 134820, "start": 1355.96, "end": 1359.48, "text": " together. There are a few rules how to do it.", "tokens": [50752, 1214, 13, 821, 366, 257, 1326, 4474, 577, 281, 360, 309, 13, 50928], "temperature": 0.0, "avg_logprob": -0.06678408257504727, "compression_ratio": 1.8302752293577982, "no_speech_prob": 4.832558988709934e-05}, {"id": 260, "seek": 134820, "start": 1360.28, "end": 1366.44, "text": " For instance, the derivative of a sum of two functions is the sum of the derivatives.", "tokens": [50968, 1171, 5197, 11, 264, 13760, 295, 257, 2408, 295, 732, 6828, 307, 264, 2408, 295, 264, 33733, 13, 51276], "temperature": 0.0, "avg_logprob": -0.06678408257504727, "compression_ratio": 1.8302752293577982, "no_speech_prob": 4.832558988709934e-05}, {"id": 261, "seek": 134820, "start": 1366.44, "end": 1370.68, "text": " There is also a formula for the derivative of a product of two functions.", "tokens": [51276, 821, 307, 611, 257, 8513, 337, 264, 13760, 295, 257, 1674, 295, 732, 6828, 13, 51488], "temperature": 0.0, "avg_logprob": -0.06678408257504727, "compression_ratio": 1.8302752293577982, "no_speech_prob": 4.832558988709934e-05}, {"id": 262, "seek": 134820, "start": 1371.56, "end": 1377.32, "text": " This gives you a way to compute things like the derivative of 3x squared minus equal to the power", "tokens": [51532, 639, 2709, 291, 257, 636, 281, 14722, 721, 411, 264, 13760, 295, 805, 87, 8889, 3175, 2681, 281, 264, 1347, 51820], "temperature": 0.0, "avg_logprob": -0.06678408257504727, "compression_ratio": 1.8302752293577982, "no_speech_prob": 4.832558988709934e-05}, {"id": 263, "seek": 137732, "start": 1377.32, "end": 1384.6, "text": " of x. But to complete the picture and to be able to find derivatives of almost everything,", "tokens": [50364, 295, 2031, 13, 583, 281, 3566, 264, 3036, 293, 281, 312, 1075, 281, 915, 33733, 295, 1920, 1203, 11, 50728], "temperature": 0.0, "avg_logprob": -0.04352650409791528, "compression_ratio": 1.6363636363636365, "no_speech_prob": 4.908656046609394e-05}, {"id": 264, "seek": 137732, "start": 1384.6, "end": 1391.8, "text": " we need one other rule called the chain rule, which powers the entire field of machine learning.", "tokens": [50728, 321, 643, 472, 661, 4978, 1219, 264, 5021, 4978, 11, 597, 8674, 264, 2302, 2519, 295, 3479, 2539, 13, 51088], "temperature": 0.0, "avg_logprob": -0.04352650409791528, "compression_ratio": 1.6363636363636365, "no_speech_prob": 4.908656046609394e-05}, {"id": 265, "seek": 137732, "start": 1392.52, "end": 1397.32, "text": " It tells you how to compute the derivative of a combination of two functions,", "tokens": [51124, 467, 5112, 291, 577, 281, 14722, 264, 13760, 295, 257, 6562, 295, 732, 6828, 11, 51364], "temperature": 0.0, "avg_logprob": -0.04352650409791528, "compression_ratio": 1.6363636363636365, "no_speech_prob": 4.908656046609394e-05}, {"id": 266, "seek": 137732, "start": 1397.32, "end": 1402.6799999999998, "text": " when one of them is an input to another. Here is a way to reason about this.", "tokens": [51364, 562, 472, 295, 552, 307, 364, 4846, 281, 1071, 13, 1692, 307, 257, 636, 281, 1778, 466, 341, 13, 51632], "temperature": 0.0, "avg_logprob": -0.04352650409791528, "compression_ratio": 1.6363636363636365, "no_speech_prob": 4.908656046609394e-05}, {"id": 267, "seek": 140268, "start": 1403.64, "end": 1409.8, "text": " Suppose you take one of those simpler machines, which receives a single input x that you can", "tokens": [50412, 21360, 291, 747, 472, 295, 729, 18587, 8379, 11, 597, 20717, 257, 2167, 4846, 2031, 300, 291, 393, 50720], "temperature": 0.0, "avg_logprob": -0.12721148052731077, "compression_ratio": 1.4791666666666667, "no_speech_prob": 0.0002868537849280983}, {"id": 268, "seek": 140268, "start": 1409.8, "end": 1418.6000000000001, "text": " vary with an ALP, and spits out an output, j of x. Now, you take a second machine of this kind,", "tokens": [50720, 10559, 365, 364, 7056, 47, 11, 293, 637, 1208, 484, 364, 5598, 11, 361, 295, 2031, 13, 823, 11, 291, 747, 257, 1150, 3479, 295, 341, 733, 11, 51160], "temperature": 0.0, "avg_logprob": -0.12721148052731077, "compression_ratio": 1.4791666666666667, "no_speech_prob": 0.0002868537849280983}, {"id": 269, "seek": 140268, "start": 1418.6000000000001, "end": 1426.6000000000001, "text": " which performs a different function, f of x. What would happen if you connect them in sequence,", "tokens": [51160, 597, 26213, 257, 819, 2445, 11, 283, 295, 2031, 13, 708, 576, 1051, 498, 291, 1745, 552, 294, 8310, 11, 51560], "temperature": 0.0, "avg_logprob": -0.12721148052731077, "compression_ratio": 1.4791666666666667, "no_speech_prob": 0.0002868537849280983}, {"id": 270, "seek": 142660, "start": 1426.6, "end": 1432.52, "text": " so that the output of the first machine is fed into the second one as an input?", "tokens": [50364, 370, 300, 264, 5598, 295, 264, 700, 3479, 307, 4636, 666, 264, 1150, 472, 382, 364, 4846, 30, 50660], "temperature": 0.0, "avg_logprob": -0.05676306429363433, "compression_ratio": 1.6820276497695852, "no_speech_prob": 3.3214175346074626e-05}, {"id": 271, "seek": 142660, "start": 1433.48, "end": 1440.6799999999998, "text": " Notice that such a construction can be thought of as a single function, which also receives one", "tokens": [50708, 13428, 300, 1270, 257, 6435, 393, 312, 1194, 295, 382, 257, 2167, 2445, 11, 597, 611, 20717, 472, 51068], "temperature": 0.0, "avg_logprob": -0.05676306429363433, "compression_ratio": 1.6820276497695852, "no_speech_prob": 3.3214175346074626e-05}, {"id": 272, "seek": 142660, "start": 1440.6799999999998, "end": 1447.24, "text": " input number and gives an output by computing a more complicated function, which is a composition", "tokens": [51068, 4846, 1230, 293, 2709, 364, 5598, 538, 15866, 257, 544, 6179, 2445, 11, 597, 307, 257, 12686, 51396], "temperature": 0.0, "avg_logprob": -0.05676306429363433, "compression_ratio": 1.6820276497695852, "no_speech_prob": 3.3214175346074626e-05}, {"id": 273, "seek": 142660, "start": 1447.24, "end": 1454.28, "text": " of the two simpler functions. In fact, if you put a black box around it to conceal the fact", "tokens": [51396, 295, 264, 732, 18587, 6828, 13, 682, 1186, 11, 498, 291, 829, 257, 2211, 2424, 926, 309, 281, 40170, 264, 1186, 51748], "temperature": 0.0, "avg_logprob": -0.05676306429363433, "compression_ratio": 1.6820276497695852, "no_speech_prob": 3.3214175346074626e-05}, {"id": 274, "seek": 145428, "start": 1454.28, "end": 1461.16, "text": " that there are actually two machines operating sequentially, you can treat it as a single machine", "tokens": [50364, 300, 456, 366, 767, 732, 8379, 7447, 5123, 3137, 11, 291, 393, 2387, 309, 382, 257, 2167, 3479, 50708], "temperature": 0.0, "avg_logprob": -0.05551584376845249, "compression_ratio": 1.6525821596244132, "no_speech_prob": 2.31874619203154e-05}, {"id": 275, "seek": 145428, "start": 1461.16, "end": 1468.04, "text": " and ask, well, if I notch the input on one end, how will it affect the output on another end?", "tokens": [50708, 293, 1029, 11, 731, 11, 498, 286, 26109, 264, 4846, 322, 472, 917, 11, 577, 486, 309, 3345, 264, 5598, 322, 1071, 917, 30, 51052], "temperature": 0.0, "avg_logprob": -0.05551584376845249, "compression_ratio": 1.6525821596244132, "no_speech_prob": 2.31874619203154e-05}, {"id": 276, "seek": 145428, "start": 1468.76, "end": 1472.6, "text": " In other words, what is the derivative of the resulting function?", "tokens": [51088, 682, 661, 2283, 11, 437, 307, 264, 13760, 295, 264, 16505, 2445, 30, 51280], "temperature": 0.0, "avg_logprob": -0.05551584376845249, "compression_ratio": 1.6525821596244132, "no_speech_prob": 2.31874619203154e-05}, {"id": 277, "seek": 145428, "start": 1473.8799999999999, "end": 1480.52, "text": " Suppose we know the individual derivatives of the two machines, f and j. If the knob is set at", "tokens": [51344, 21360, 321, 458, 264, 2609, 33733, 295, 264, 732, 8379, 11, 283, 293, 361, 13, 759, 264, 26759, 307, 992, 412, 51676], "temperature": 0.0, "avg_logprob": -0.05551584376845249, "compression_ratio": 1.6525821596244132, "no_speech_prob": 2.31874619203154e-05}, {"id": 278, "seek": 148052, "start": 1480.52, "end": 1489.56, "text": " some value x, local steepness of the first function is evaluated at x. However, the number that is", "tokens": [50364, 512, 2158, 2031, 11, 2654, 16841, 1287, 295, 264, 700, 2445, 307, 25509, 412, 2031, 13, 2908, 11, 264, 1230, 300, 307, 50816], "temperature": 0.0, "avg_logprob": -0.0777615586372271, "compression_ratio": 1.686046511627907, "no_speech_prob": 4.683884617406875e-05}, {"id": 279, "seek": 148052, "start": 1489.56, "end": 1495.72, "text": " fed into the second machine is not x, because it was already processed by the first function.", "tokens": [50816, 4636, 666, 264, 1150, 3479, 307, 406, 2031, 11, 570, 309, 390, 1217, 18846, 538, 264, 700, 2445, 13, 51124], "temperature": 0.0, "avg_logprob": -0.0777615586372271, "compression_ratio": 1.686046511627907, "no_speech_prob": 4.683884617406875e-05}, {"id": 280, "seek": 148052, "start": 1496.76, "end": 1503.48, "text": " So, the thing that is being plugged into the second function is j of x. And so, the local rate of", "tokens": [51176, 407, 11, 264, 551, 300, 307, 885, 25679, 666, 264, 1150, 2445, 307, 361, 295, 2031, 13, 400, 370, 11, 264, 2654, 3314, 295, 51512], "temperature": 0.0, "avg_logprob": -0.0777615586372271, "compression_ratio": 1.686046511627907, "no_speech_prob": 4.683884617406875e-05}, {"id": 281, "seek": 150348, "start": 1503.48, "end": 1512.3600000000001, "text": " change of the second machine is thus the derivative of f evaluated at the point j of x. Now, imagine", "tokens": [50364, 1319, 295, 264, 1150, 3479, 307, 8807, 264, 13760, 295, 283, 25509, 412, 264, 935, 361, 295, 2031, 13, 823, 11, 3811, 50808], "temperature": 0.0, "avg_logprob": -0.06793571005062181, "compression_ratio": 1.8054298642533937, "no_speech_prob": 0.00033534984686411917}, {"id": 282, "seek": 150348, "start": 1512.3600000000001, "end": 1519.64, "text": " you notch the knob x by a tiny amount, delta. That input notch, when it comes out of the first machine,", "tokens": [50808, 291, 26109, 264, 26759, 2031, 538, 257, 5870, 2372, 11, 8289, 13, 663, 4846, 26109, 11, 562, 309, 1487, 484, 295, 264, 700, 3479, 11, 51172], "temperature": 0.0, "avg_logprob": -0.06793571005062181, "compression_ratio": 1.8054298642533937, "no_speech_prob": 0.00033534984686411917}, {"id": 283, "seek": 150348, "start": 1519.64, "end": 1526.1200000000001, "text": " will be multiplied by the derivative of j, since the derivative is the rate of change in the output", "tokens": [51172, 486, 312, 17207, 538, 264, 13760, 295, 361, 11, 1670, 264, 13760, 307, 264, 3314, 295, 1319, 294, 264, 5598, 51496], "temperature": 0.0, "avg_logprob": -0.06793571005062181, "compression_ratio": 1.8054298642533937, "no_speech_prob": 0.00033534984686411917}, {"id": 284, "seek": 150348, "start": 1526.1200000000001, "end": 1532.84, "text": " per unit change of the input. So, after the first function, the output will increase by delta,", "tokens": [51496, 680, 4985, 1319, 295, 264, 4846, 13, 407, 11, 934, 264, 700, 2445, 11, 264, 5598, 486, 3488, 538, 8289, 11, 51832], "temperature": 0.0, "avg_logprob": -0.06793571005062181, "compression_ratio": 1.8054298642533937, "no_speech_prob": 0.00033534984686411917}, {"id": 285, "seek": 153284, "start": 1532.84, "end": 1540.36, "text": " multiplied by the derivative of j. This expression is essentially a tiny notch in the input to the", "tokens": [50364, 17207, 538, 264, 13760, 295, 361, 13, 639, 6114, 307, 4476, 257, 5870, 26109, 294, 264, 4846, 281, 264, 50740], "temperature": 0.0, "avg_logprob": -0.07247000474196214, "compression_ratio": 1.7041420118343196, "no_speech_prob": 6.402001599781215e-05}, {"id": 286, "seek": 153284, "start": 1540.36, "end": 1547.6399999999999, "text": " second machine, whose derivative at that point is given by this expression. This means that for", "tokens": [50740, 1150, 3479, 11, 6104, 13760, 412, 300, 935, 307, 2212, 538, 341, 6114, 13, 639, 1355, 300, 337, 51104], "temperature": 0.0, "avg_logprob": -0.07247000474196214, "compression_ratio": 1.7041420118343196, "no_speech_prob": 6.402001599781215e-05}, {"id": 287, "seek": 153284, "start": 1547.6399999999999, "end": 1556.36, "text": " each delta increase in the input, we bump the output by this much. Hence, the derivative when", "tokens": [51104, 1184, 8289, 3488, 294, 264, 4846, 11, 321, 9961, 264, 5598, 538, 341, 709, 13, 22229, 11, 264, 13760, 562, 51540], "temperature": 0.0, "avg_logprob": -0.07247000474196214, "compression_ratio": 1.7041420118343196, "no_speech_prob": 6.402001599781215e-05}, {"id": 288, "seek": 155636, "start": 1556.36, "end": 1564.36, "text": " you divide that by delta will look like this. You can think about it as a set of three interconnected", "tokens": [50364, 291, 9845, 300, 538, 8289, 486, 574, 411, 341, 13, 509, 393, 519, 466, 309, 382, 257, 992, 295, 1045, 36611, 50764], "temperature": 0.0, "avg_logprob": -0.0783950724500291, "compression_ratio": 1.6391304347826088, "no_speech_prob": 2.5867422664305195e-05}, {"id": 289, "seek": 155636, "start": 1564.36, "end": 1571.8, "text": " cog wheels, where the first one represents the input knob x. And the other two wheels are functions,", "tokens": [50764, 46521, 10046, 11, 689, 264, 700, 472, 8855, 264, 4846, 26759, 2031, 13, 400, 264, 661, 732, 10046, 366, 6828, 11, 51136], "temperature": 0.0, "avg_logprob": -0.0783950724500291, "compression_ratio": 1.6391304347826088, "no_speech_prob": 2.5867422664305195e-05}, {"id": 290, "seek": 155636, "start": 1571.8, "end": 1578.6, "text": " j of x and f of j of x, respectively. When you notch the first wheel, it induces a", "tokens": [51136, 361, 295, 2031, 293, 283, 295, 361, 295, 2031, 11, 25009, 13, 1133, 291, 26109, 264, 700, 5589, 11, 309, 13716, 887, 257, 51476], "temperature": 0.0, "avg_logprob": -0.0783950724500291, "compression_ratio": 1.6391304347826088, "no_speech_prob": 2.5867422664305195e-05}, {"id": 291, "seek": 155636, "start": 1578.6, "end": 1584.36, "text": " notch in the middle wheel and the amplitude of that change is given by the derivative of j,", "tokens": [51476, 26109, 294, 264, 2808, 5589, 293, 264, 27433, 295, 300, 1319, 307, 2212, 538, 264, 13760, 295, 361, 11, 51764], "temperature": 0.0, "avg_logprob": -0.0783950724500291, "compression_ratio": 1.6391304347826088, "no_speech_prob": 2.5867422664305195e-05}, {"id": 292, "seek": 158436, "start": 1585.08, "end": 1590.84, "text": " which in turn causes the third wheel to rotate, and the amplitude of that resulting", "tokens": [50400, 597, 294, 1261, 7700, 264, 2636, 5589, 281, 13121, 11, 293, 264, 27433, 295, 300, 16505, 50688], "temperature": 0.0, "avg_logprob": -0.10490784645080567, "compression_ratio": 1.6025641025641026, "no_speech_prob": 4.133538459427655e-05}, {"id": 293, "seek": 158436, "start": 1590.84, "end": 1598.6799999999998, "text": " notch is given by changing the derivatives together. Alright, great. Now we have a straightforward way", "tokens": [50688, 26109, 307, 2212, 538, 4473, 264, 33733, 1214, 13, 2798, 11, 869, 13, 823, 321, 362, 257, 15325, 636, 51080], "temperature": 0.0, "avg_logprob": -0.10490784645080567, "compression_ratio": 1.6025641025641026, "no_speech_prob": 4.133538459427655e-05}, {"id": 294, "seek": 158436, "start": 1598.6799999999998, "end": 1605.32, "text": " of obtaining a derivative of any arbitrarily complex function, as long as it can be decomposed", "tokens": [51080, 295, 36749, 257, 13760, 295, 604, 19071, 3289, 3997, 2445, 11, 382, 938, 382, 309, 393, 312, 22867, 1744, 51412], "temperature": 0.0, "avg_logprob": -0.10490784645080567, "compression_ratio": 1.6025641025641026, "no_speech_prob": 4.133538459427655e-05}, {"id": 295, "seek": 158436, "start": 1605.32, "end": 1611.8799999999999, "text": " into building blocks. Simple functions with explicit derivative formulas, such as summations,", "tokens": [51412, 666, 2390, 8474, 13, 21532, 6828, 365, 13691, 13760, 30546, 11, 1270, 382, 8367, 763, 11, 51740], "temperature": 0.0, "avg_logprob": -0.10490784645080567, "compression_ratio": 1.6025641025641026, "no_speech_prob": 4.133538459427655e-05}, {"id": 296, "seek": 161188, "start": 1611.88, "end": 1619.0800000000002, "text": " multiplications, exponents, logarithms, etc. But how can it be used to find the best curve", "tokens": [50364, 17596, 763, 11, 12680, 791, 11, 41473, 355, 2592, 11, 5183, 13, 583, 577, 393, 309, 312, 1143, 281, 915, 264, 1151, 7605, 50724], "temperature": 0.0, "avg_logprob": -0.06341209411621093, "compression_ratio": 1.569672131147541, "no_speech_prob": 6.205045792739838e-05}, {"id": 297, "seek": 161188, "start": 1619.0800000000002, "end": 1625.3200000000002, "text": " using our curve fitter? The big picture we are aiming for is the following. For each of our", "tokens": [50724, 1228, 527, 7605, 3318, 391, 30, 440, 955, 3036, 321, 366, 20253, 337, 307, 264, 3480, 13, 1171, 1184, 295, 527, 51036], "temperature": 0.0, "avg_logprob": -0.06341209411621093, "compression_ratio": 1.569672131147541, "no_speech_prob": 6.205045792739838e-05}, {"id": 298, "seek": 161188, "start": 1625.3200000000002, "end": 1632.0400000000002, "text": " parameter knobs, we will write down its effect on the loss in terms of simple, easily differentiable", "tokens": [51036, 13075, 46999, 11, 321, 486, 2464, 760, 1080, 1802, 322, 264, 4470, 294, 2115, 295, 2199, 11, 3612, 819, 9364, 51372], "temperature": 0.0, "avg_logprob": -0.06341209411621093, "compression_ratio": 1.569672131147541, "no_speech_prob": 6.205045792739838e-05}, {"id": 299, "seek": 161188, "start": 1632.0400000000002, "end": 1639.72, "text": " operations. Once we have that sequence of building blocks, no matter how long, we should be able to", "tokens": [51372, 7705, 13, 3443, 321, 362, 300, 8310, 295, 2390, 8474, 11, 572, 1871, 577, 938, 11, 321, 820, 312, 1075, 281, 51756], "temperature": 0.0, "avg_logprob": -0.06341209411621093, "compression_ratio": 1.569672131147541, "no_speech_prob": 6.205045792739838e-05}, {"id": 300, "seek": 163972, "start": 1639.72, "end": 1645.96, "text": " sequentially apply the chain rule to each of them in order to find the value of the derivative", "tokens": [50364, 5123, 3137, 3079, 264, 5021, 4978, 281, 1184, 295, 552, 294, 1668, 281, 915, 264, 2158, 295, 264, 13760, 50676], "temperature": 0.0, "avg_logprob": -0.04465507074844006, "compression_ratio": 1.6784140969162995, "no_speech_prob": 6.74804687150754e-06}, {"id": 301, "seek": 163972, "start": 1645.96, "end": 1652.52, "text": " of the loss function with respect to each of the input knobs and perform iterative gradient descent", "tokens": [50676, 295, 264, 4470, 2445, 365, 3104, 281, 1184, 295, 264, 4846, 46999, 293, 2042, 17138, 1166, 16235, 23475, 51004], "temperature": 0.0, "avg_logprob": -0.04465507074844006, "compression_ratio": 1.6784140969162995, "no_speech_prob": 6.74804687150754e-06}, {"id": 302, "seek": 163972, "start": 1652.52, "end": 1659.72, "text": " to minimize the loss. Let's see an example of this. First, we are going to create a knob", "tokens": [51004, 281, 17522, 264, 4470, 13, 961, 311, 536, 364, 1365, 295, 341, 13, 2386, 11, 321, 366, 516, 281, 1884, 257, 26759, 51364], "temperature": 0.0, "avg_logprob": -0.04465507074844006, "compression_ratio": 1.6784140969162995, "no_speech_prob": 6.74804687150754e-06}, {"id": 303, "seek": 163972, "start": 1659.72, "end": 1666.1200000000001, "text": " for each number the loss function can possibly depend on. This obviously includes the parameters,", "tokens": [51364, 337, 1184, 1230, 264, 4470, 2445, 393, 6264, 5672, 322, 13, 639, 2745, 5974, 264, 9834, 11, 51684], "temperature": 0.0, "avg_logprob": -0.04465507074844006, "compression_ratio": 1.6784140969162995, "no_speech_prob": 6.74804687150754e-06}, {"id": 304, "seek": 166612, "start": 1666.6799999999998, "end": 1673.2399999999998, "text": " but there is also the data itself, coordinates of points to which we are fitting the curve", "tokens": [50392, 457, 456, 307, 611, 264, 1412, 2564, 11, 21056, 295, 2793, 281, 597, 321, 366, 15669, 264, 7605, 50720], "temperature": 0.0, "avg_logprob": -0.07782645117152821, "compression_ratio": 1.6271929824561404, "no_speech_prob": 9.028028580360115e-05}, {"id": 305, "seek": 166612, "start": 1673.2399999999998, "end": 1680.12, "text": " in the first place. Now, during optimization, the data points are set in stone, so changing them", "tokens": [50720, 294, 264, 700, 1081, 13, 823, 11, 1830, 19618, 11, 264, 1412, 2793, 366, 992, 294, 7581, 11, 370, 4473, 552, 51064], "temperature": 0.0, "avg_logprob": -0.07782645117152821, "compression_ratio": 1.6271929824561404, "no_speech_prob": 9.028028580360115e-05}, {"id": 306, "seek": 166612, "start": 1680.12, "end": 1687.4799999999998, "text": " in order to obtain a lower loss would make no sense. However, for conceptual purposes,", "tokens": [51064, 294, 1668, 281, 12701, 257, 3126, 4470, 576, 652, 572, 2020, 13, 2908, 11, 337, 24106, 9932, 11, 51432], "temperature": 0.0, "avg_logprob": -0.07782645117152821, "compression_ratio": 1.6271929824561404, "no_speech_prob": 9.028028580360115e-05}, {"id": 307, "seek": 166612, "start": 1687.4799999999998, "end": 1694.36, "text": " we can think about these values as fixed knobs set in one position so that we cannot nudge them.", "tokens": [51432, 321, 393, 519, 466, 613, 4190, 382, 6806, 46999, 992, 294, 472, 2535, 370, 300, 321, 2644, 297, 16032, 552, 13, 51776], "temperature": 0.0, "avg_logprob": -0.07782645117152821, "compression_ratio": 1.6271929824561404, "no_speech_prob": 9.028028580360115e-05}, {"id": 308, "seek": 169436, "start": 1695.32, "end": 1701.8799999999999, "text": " Once we have all the existing numbers being fed into the machine, we can start to break down the", "tokens": [50412, 3443, 321, 362, 439, 264, 6741, 3547, 885, 4636, 666, 264, 3479, 11, 321, 393, 722, 281, 1821, 760, 264, 50740], "temperature": 0.0, "avg_logprob": -0.12104327750928474, "compression_ratio": 1.467741935483871, "no_speech_prob": 2.8409729566192254e-05}, {"id": 309, "seek": 169436, "start": 1701.8799999999999, "end": 1709.6399999999999, "text": " loss calculation. Remember, by definition, it is the sum of squared vertical distances", "tokens": [50740, 4470, 17108, 13, 5459, 11, 538, 7123, 11, 309, 307, 264, 2408, 295, 8889, 9429, 22182, 51128], "temperature": 0.0, "avg_logprob": -0.12104327750928474, "compression_ratio": 1.467741935483871, "no_speech_prob": 2.8409729566192254e-05}, {"id": 310, "seek": 169436, "start": 1709.6399999999999, "end": 1717.24, "text": " from each point to the curve parameterized by k's. So, for instance, let's take the first", "tokens": [51128, 490, 1184, 935, 281, 264, 7605, 13075, 1602, 538, 350, 311, 13, 407, 11, 337, 5197, 11, 718, 311, 747, 264, 700, 51508], "temperature": 0.0, "avg_logprob": -0.12104327750928474, "compression_ratio": 1.467741935483871, "no_speech_prob": 2.8409729566192254e-05}, {"id": 311, "seek": 171724, "start": 1717.32, "end": 1727.56, "text": " data point, x1, y1, multiply the x coordinate by k1, add that to the squared value of x1 multiplied", "tokens": [50368, 1412, 935, 11, 2031, 16, 11, 288, 16, 11, 12972, 264, 2031, 15670, 538, 350, 16, 11, 909, 300, 281, 264, 8889, 2158, 295, 2031, 16, 17207, 50880], "temperature": 0.0, "avg_logprob": -0.10612210263027234, "compression_ratio": 1.4818652849740932, "no_speech_prob": 0.0007096687913872302}, {"id": 312, "seek": 171724, "start": 1727.56, "end": 1736.44, "text": " by k2, and so on for other k's, including the constant term k0. This sum of weight and powers of", "tokens": [50880, 538, 350, 17, 11, 293, 370, 322, 337, 661, 350, 311, 11, 3009, 264, 5754, 1433, 350, 15, 13, 639, 2408, 295, 3364, 293, 8674, 295, 51324], "temperature": 0.0, "avg_logprob": -0.10612210263027234, "compression_ratio": 1.4818652849740932, "no_speech_prob": 0.0007096687913872302}, {"id": 313, "seek": 171724, "start": 1736.44, "end": 1745.96, "text": " x1 is the value of y predicted by the current curve, f of x1. Let's call it y1 hat. Next,", "tokens": [51324, 2031, 16, 307, 264, 2158, 295, 288, 19147, 538, 264, 2190, 7605, 11, 283, 295, 2031, 16, 13, 961, 311, 818, 309, 288, 16, 2385, 13, 3087, 11, 51800], "temperature": 0.0, "avg_logprob": -0.10612210263027234, "compression_ratio": 1.4818652849740932, "no_speech_prob": 0.0007096687913872302}, {"id": 314, "seek": 174596, "start": 1745.96, "end": 1752.44, "text": " we need to take the squared difference between the actual value and the predicted value. This is", "tokens": [50364, 321, 643, 281, 747, 264, 8889, 2649, 1296, 264, 3539, 2158, 293, 264, 19147, 2158, 13, 639, 307, 50688], "temperature": 0.0, "avg_logprob": -0.0739193445519556, "compression_ratio": 1.7136363636363636, "no_speech_prob": 2.2827971406513825e-05}, {"id": 315, "seek": 174596, "start": 1752.44, "end": 1758.6000000000001, "text": " how much the first data point contributes to the resulting value of the loss function.", "tokens": [50688, 577, 709, 264, 700, 1412, 935, 32035, 281, 264, 16505, 2158, 295, 264, 4470, 2445, 13, 50996], "temperature": 0.0, "avg_logprob": -0.0739193445519556, "compression_ratio": 1.7136363636363636, "no_speech_prob": 2.2827971406513825e-05}, {"id": 316, "seek": 174596, "start": 1759.8, "end": 1767.48, "text": " Repeating the same procedure for all remaining data points and summing up the resulting squared", "tokens": [51056, 24927, 990, 264, 912, 10747, 337, 439, 8877, 1412, 2793, 293, 2408, 2810, 493, 264, 16505, 8889, 51440], "temperature": 0.0, "avg_logprob": -0.0739193445519556, "compression_ratio": 1.7136363636363636, "no_speech_prob": 2.2827971406513825e-05}, {"id": 317, "seek": 174596, "start": 1767.48, "end": 1775.8, "text": " distances gives us the overall total loss that we are trying to minimize. The computation we just", "tokens": [51440, 22182, 2709, 505, 264, 4787, 3217, 4470, 300, 321, 366, 1382, 281, 17522, 13, 440, 24903, 321, 445, 51856], "temperature": 0.0, "avg_logprob": -0.0739193445519556, "compression_ratio": 1.7136363636363636, "no_speech_prob": 2.2827971406513825e-05}, {"id": 318, "seek": 177580, "start": 1775.8, "end": 1782.9199999999998, "text": " performed, finding the value of the loss for a given configuration of parameter and data knobs,", "tokens": [50364, 10332, 11, 5006, 264, 2158, 295, 264, 4470, 337, 257, 2212, 11694, 295, 13075, 293, 1412, 46999, 11, 50720], "temperature": 0.0, "avg_logprob": -0.08898842536796958, "compression_ratio": 1.5307262569832403, "no_speech_prob": 0.00025714162620715797}, {"id": 319, "seek": 177580, "start": 1783.56, "end": 1791.3999999999999, "text": " is known as the forward step. The entire sequence of calculations can be visualized", "tokens": [50752, 307, 2570, 382, 264, 2128, 1823, 13, 440, 2302, 8310, 295, 20448, 393, 312, 5056, 1602, 51144], "temperature": 0.0, "avg_logprob": -0.08898842536796958, "compression_ratio": 1.5307262569832403, "no_speech_prob": 0.00025714162620715797}, {"id": 320, "seek": 177580, "start": 1791.3999999999999, "end": 1798.9199999999998, "text": " as this kind of computational graph, where each node is some simple operation like addition or", "tokens": [51144, 382, 341, 733, 295, 28270, 4295, 11, 689, 1184, 9984, 307, 512, 2199, 6916, 411, 4500, 420, 51520], "temperature": 0.0, "avg_logprob": -0.08898842536796958, "compression_ratio": 1.5307262569832403, "no_speech_prob": 0.00025714162620715797}, {"id": 321, "seek": 179892, "start": 1798.92, "end": 1807.4, "text": " multiplication. Forward step then corresponds to computations flowing from left to right.", "tokens": [50364, 27290, 13, 35524, 1823, 550, 23249, 281, 2807, 763, 13974, 490, 1411, 281, 558, 13, 50788], "temperature": 0.0, "avg_logprob": -0.10614436672579858, "compression_ratio": 1.4972972972972973, "no_speech_prob": 0.00011061157420044765}, {"id": 322, "seek": 179892, "start": 1807.4, "end": 1814.04, "text": " But to perform optimization, we also need information about gradients, how each node", "tokens": [50788, 583, 281, 2042, 19618, 11, 321, 611, 643, 1589, 466, 2771, 2448, 11, 577, 1184, 9984, 51120], "temperature": 0.0, "avg_logprob": -0.10614436672579858, "compression_ratio": 1.4972972972972973, "no_speech_prob": 0.00011061157420044765}, {"id": 323, "seek": 179892, "start": 1814.04, "end": 1821.88, "text": " influences the loss. Now we are going to do what's known as the backward step, and unroll the sequence", "tokens": [51120, 21222, 264, 4470, 13, 823, 321, 366, 516, 281, 360, 437, 311, 2570, 382, 264, 23897, 1823, 11, 293, 517, 3970, 264, 8310, 51512], "temperature": 0.0, "avg_logprob": -0.10614436672579858, "compression_ratio": 1.4972972972972973, "no_speech_prob": 0.00011061157420044765}, {"id": 324, "seek": 182188, "start": 1821.88, "end": 1829.24, "text": " of calculations in reverse order to find derivatives. What makes the backward step possible", "tokens": [50364, 295, 20448, 294, 9943, 1668, 281, 915, 33733, 13, 708, 1669, 264, 23897, 1823, 1944, 50732], "temperature": 0.0, "avg_logprob": -0.07919985208755885, "compression_ratio": 1.5889830508474576, "no_speech_prob": 0.002115697367116809}, {"id": 325, "seek": 182188, "start": 1829.24, "end": 1835.4, "text": " is the fact that every node in our compute graph is an easily differentiable operation.", "tokens": [50732, 307, 264, 1186, 300, 633, 9984, 294, 527, 14722, 4295, 307, 364, 3612, 819, 9364, 6916, 13, 51040], "temperature": 0.0, "avg_logprob": -0.07919985208755885, "compression_ratio": 1.5889830508474576, "no_speech_prob": 0.002115697367116809}, {"id": 326, "seek": 182188, "start": 1835.4, "end": 1842.92, "text": " Think of individual nodes as these tiny machines which simply add, multiply or take powers. We", "tokens": [51040, 6557, 295, 2609, 13891, 382, 613, 5870, 8379, 597, 2935, 909, 11, 12972, 420, 747, 8674, 13, 492, 51416], "temperature": 0.0, "avg_logprob": -0.07919985208755885, "compression_ratio": 1.5889830508474576, "no_speech_prob": 0.002115697367116809}, {"id": 327, "seek": 182188, "start": 1842.92, "end": 1850.0400000000002, "text": " know their derivatives, and because their outputs are connected sequentially, we can apply the chain", "tokens": [51416, 458, 641, 33733, 11, 293, 570, 641, 23930, 366, 4582, 5123, 3137, 11, 321, 393, 3079, 264, 5021, 51772], "temperature": 0.0, "avg_logprob": -0.07919985208755885, "compression_ratio": 1.5889830508474576, "no_speech_prob": 0.002115697367116809}, {"id": 328, "seek": 185004, "start": 1850.04, "end": 1858.84, "text": " rule. This means that for each node we can find its gradient, the partial derivative of the output", "tokens": [50364, 4978, 13, 639, 1355, 300, 337, 1184, 9984, 321, 393, 915, 1080, 16235, 11, 264, 14641, 13760, 295, 264, 5598, 50804], "temperature": 0.0, "avg_logprob": -0.07229162269914655, "compression_ratio": 1.5129533678756477, "no_speech_prob": 0.0001686525356490165}, {"id": 329, "seek": 185004, "start": 1858.84, "end": 1867.32, "text": " loss with respect to that node. Let's see how it can be done. Consider a region of the compute", "tokens": [50804, 4470, 365, 3104, 281, 300, 9984, 13, 961, 311, 536, 577, 309, 393, 312, 1096, 13, 17416, 257, 4458, 295, 264, 14722, 51228], "temperature": 0.0, "avg_logprob": -0.07229162269914655, "compression_ratio": 1.5129533678756477, "no_speech_prob": 0.0001686525356490165}, {"id": 330, "seek": 185004, "start": 1867.32, "end": 1875.1599999999999, "text": " graph, where two number nodes A and B are being fed into a machine that performs addition, and its", "tokens": [51228, 4295, 11, 689, 732, 1230, 13891, 316, 293, 363, 366, 885, 4636, 666, 257, 3479, 300, 26213, 4500, 11, 293, 1080, 51620], "temperature": 0.0, "avg_logprob": -0.07229162269914655, "compression_ratio": 1.5129533678756477, "no_speech_prob": 0.0001686525356490165}, {"id": 331, "seek": 187516, "start": 1875.16, "end": 1882.0400000000002, "text": " result A plus B is further processed by the system to compute the overall output L.", "tokens": [50364, 1874, 316, 1804, 363, 307, 3052, 18846, 538, 264, 1185, 281, 14722, 264, 4787, 5598, 441, 13, 50708], "temperature": 0.0, "avg_logprob": -0.1016668466421274, "compression_ratio": 1.4915254237288136, "no_speech_prob": 1.7778551409719512e-05}, {"id": 332, "seek": 187516, "start": 1883.24, "end": 1889.64, "text": " Suppose we already computed the gradient of A plus B earlier, so that we know how", "tokens": [50768, 21360, 321, 1217, 40610, 264, 16235, 295, 316, 1804, 363, 3071, 11, 370, 300, 321, 458, 577, 51088], "temperature": 0.0, "avg_logprob": -0.1016668466421274, "compression_ratio": 1.4915254237288136, "no_speech_prob": 1.7778551409719512e-05}, {"id": 333, "seek": 187516, "start": 1889.64, "end": 1897.48, "text": " nudging the sum will affect the output. The question is, what are individual gradients of A and B?", "tokens": [51088, 40045, 3249, 264, 2408, 486, 3345, 264, 5598, 13, 440, 1168, 307, 11, 437, 366, 2609, 2771, 2448, 295, 316, 293, 363, 30, 51480], "temperature": 0.0, "avg_logprob": -0.1016668466421274, "compression_ratio": 1.4915254237288136, "no_speech_prob": 1.7778551409719512e-05}, {"id": 334, "seek": 189748, "start": 1897.8, "end": 1906.6, "text": " Well, intuitively, if you nudge A by sum amount, A plus B will be nudged by this same amount,", "tokens": [50380, 1042, 11, 46506, 11, 498, 291, 297, 16032, 316, 538, 2408, 2372, 11, 316, 1804, 363, 486, 312, 40045, 3004, 538, 341, 912, 2372, 11, 50820], "temperature": 0.0, "avg_logprob": -0.13080814204265162, "compression_ratio": 1.7272727272727273, "no_speech_prob": 0.0001609307073522359}, {"id": 335, "seek": 189748, "start": 1906.6, "end": 1913.0, "text": " so the gradient or the partial derivative of the loss with respect to A is the same as the gradient", "tokens": [50820, 370, 264, 16235, 420, 264, 14641, 13760, 295, 264, 4470, 365, 3104, 281, 316, 307, 264, 912, 382, 264, 16235, 51140], "temperature": 0.0, "avg_logprob": -0.13080814204265162, "compression_ratio": 1.7272727272727273, "no_speech_prob": 0.0001609307073522359}, {"id": 336, "seek": 189748, "start": 1913.0, "end": 1920.44, "text": " of the sum, and similarly for B. This can be seen more formally by writing down the chain rule", "tokens": [51140, 295, 264, 2408, 11, 293, 14138, 337, 363, 13, 639, 393, 312, 1612, 544, 25983, 538, 3579, 760, 264, 5021, 4978, 51512], "temperature": 0.0, "avg_logprob": -0.13080814204265162, "compression_ratio": 1.7272727272727273, "no_speech_prob": 0.0001609307073522359}, {"id": 337, "seek": 189748, "start": 1920.44, "end": 1927.24, "text": " and noticing that the derivative of A plus B with respect to A is just one. In other words,", "tokens": [51512, 293, 21814, 300, 264, 13760, 295, 316, 1804, 363, 365, 3104, 281, 316, 307, 445, 472, 13, 682, 661, 2283, 11, 51852], "temperature": 0.0, "avg_logprob": -0.13080814204265162, "compression_ratio": 1.7272727272727273, "no_speech_prob": 0.0001609307073522359}, {"id": 338, "seek": 192724, "start": 1927.32, "end": 1933.72, "text": " when you encounter this situation in the compute graph, then the gradient of the sum", "tokens": [50368, 562, 291, 8593, 341, 2590, 294, 264, 14722, 4295, 11, 550, 264, 16235, 295, 264, 2408, 50688], "temperature": 0.0, "avg_logprob": -0.06004279000418527, "compression_ratio": 1.661764705882353, "no_speech_prob": 2.1444935555336997e-05}, {"id": 339, "seek": 192724, "start": 1933.72, "end": 1940.2, "text": " just simply propagates into the gradients of the nodes that plug into the sum machine.", "tokens": [50688, 445, 2935, 12425, 1024, 666, 264, 2771, 2448, 295, 264, 13891, 300, 5452, 666, 264, 2408, 3479, 13, 51012], "temperature": 0.0, "avg_logprob": -0.06004279000418527, "compression_ratio": 1.661764705882353, "no_speech_prob": 2.1444935555336997e-05}, {"id": 340, "seek": 192724, "start": 1940.2, "end": 1946.04, "text": " Another possible scenario is when A and B are multiplied. Just like before,", "tokens": [51012, 3996, 1944, 9005, 307, 562, 316, 293, 363, 366, 17207, 13, 1449, 411, 949, 11, 51304], "temperature": 0.0, "avg_logprob": -0.06004279000418527, "compression_ratio": 1.661764705882353, "no_speech_prob": 2.1444935555336997e-05}, {"id": 341, "seek": 192724, "start": 1946.04, "end": 1952.28, "text": " suppose we know the gradient of their product because it was computed before. In this case,", "tokens": [51304, 7297, 321, 458, 264, 16235, 295, 641, 1674, 570, 309, 390, 40610, 949, 13, 682, 341, 1389, 11, 51616], "temperature": 0.0, "avg_logprob": -0.06004279000418527, "compression_ratio": 1.661764705882353, "no_speech_prob": 2.1444935555336997e-05}, {"id": 342, "seek": 195228, "start": 1952.28, "end": 1959.32, "text": " individual nudge to A will be scaled by a factor of B, so the product will be nudged", "tokens": [50364, 2609, 297, 16032, 281, 316, 486, 312, 36039, 538, 257, 5952, 295, 363, 11, 370, 264, 1674, 486, 312, 40045, 3004, 50716], "temperature": 0.0, "avg_logprob": -0.07358121871948242, "compression_ratio": 1.8579234972677596, "no_speech_prob": 5.2553923524101265e-06}, {"id": 343, "seek": 195228, "start": 1959.32, "end": 1966.6, "text": " B times as much, which propagates into the output. So, whatever the derivative of the", "tokens": [50716, 363, 1413, 382, 709, 11, 597, 12425, 1024, 666, 264, 5598, 13, 407, 11, 2035, 264, 13760, 295, 264, 51080], "temperature": 0.0, "avg_logprob": -0.07358121871948242, "compression_ratio": 1.8579234972677596, "no_speech_prob": 5.2553923524101265e-06}, {"id": 344, "seek": 195228, "start": 1966.6, "end": 1973.72, "text": " output with respect to the product of A B is, the output derivative with respect to A", "tokens": [51080, 5598, 365, 3104, 281, 264, 1674, 295, 316, 363, 307, 11, 264, 5598, 13760, 365, 3104, 281, 316, 51436], "temperature": 0.0, "avg_logprob": -0.07358121871948242, "compression_ratio": 1.8579234972677596, "no_speech_prob": 5.2553923524101265e-06}, {"id": 345, "seek": 195228, "start": 1973.72, "end": 1980.44, "text": " will get scaled by a factor of B, and vice versa for the gradient of B. Once again,", "tokens": [51436, 486, 483, 36039, 538, 257, 5952, 295, 363, 11, 293, 11964, 25650, 337, 264, 16235, 295, 363, 13, 3443, 797, 11, 51772], "temperature": 0.0, "avg_logprob": -0.07358121871948242, "compression_ratio": 1.8579234972677596, "no_speech_prob": 5.2553923524101265e-06}, {"id": 346, "seek": 198044, "start": 1980.44, "end": 1987.8, "text": " it can be seen more formally by examining the chain rule. In other words, the multiplication node", "tokens": [50364, 309, 393, 312, 1612, 544, 25983, 538, 34662, 264, 5021, 4978, 13, 682, 661, 2283, 11, 264, 27290, 9984, 50732], "temperature": 0.0, "avg_logprob": -0.04928123033963717, "compression_ratio": 1.6255506607929515, "no_speech_prob": 6.60520454403013e-05}, {"id": 347, "seek": 198044, "start": 1987.8, "end": 1994.92, "text": " in the compute graph distributes the downstream gradient across incoming nodes by multiplying", "tokens": [50732, 294, 264, 14722, 4295, 4400, 1819, 264, 30621, 16235, 2108, 22341, 13891, 538, 30955, 51088], "temperature": 0.0, "avg_logprob": -0.04928123033963717, "compression_ratio": 1.6255506607929515, "no_speech_prob": 6.60520454403013e-05}, {"id": 348, "seek": 198044, "start": 1994.92, "end": 2001.96, "text": " it crossways by their values. Similar rules can be easily formulated for other building block", "tokens": [51088, 309, 3278, 942, 538, 641, 4190, 13, 10905, 4474, 393, 312, 3612, 48936, 337, 661, 2390, 3461, 51440], "temperature": 0.0, "avg_logprob": -0.04928123033963717, "compression_ratio": 1.6255506607929515, "no_speech_prob": 6.60520454403013e-05}, {"id": 349, "seek": 198044, "start": 2001.96, "end": 2008.3600000000001, "text": " calculations, such as raising a number to a power or taking the logarithm. Finally,", "tokens": [51440, 20448, 11, 1270, 382, 11225, 257, 1230, 281, 257, 1347, 420, 1940, 264, 41473, 32674, 13, 6288, 11, 51760], "temperature": 0.0, "avg_logprob": -0.04928123033963717, "compression_ratio": 1.6255506607929515, "no_speech_prob": 6.60520454403013e-05}, {"id": 350, "seek": 200836, "start": 2008.36, "end": 2013.9599999999998, "text": " when a single node takes part in multiple branches of the compute graph, gradients from the", "tokens": [50364, 562, 257, 2167, 9984, 2516, 644, 294, 3866, 14770, 295, 264, 14722, 4295, 11, 2771, 2448, 490, 264, 50644], "temperature": 0.0, "avg_logprob": -0.09045702429378734, "compression_ratio": 1.588235294117647, "no_speech_prob": 5.9551371123234276e-06}, {"id": 351, "seek": 200836, "start": 2013.9599999999998, "end": 2019.6399999999999, "text": " corresponding branches are simply added together. Indeed, suppose you have the following structure", "tokens": [50644, 11760, 14770, 366, 2935, 3869, 1214, 13, 15061, 11, 7297, 291, 362, 264, 3480, 3877, 50928], "temperature": 0.0, "avg_logprob": -0.09045702429378734, "compression_ratio": 1.588235294117647, "no_speech_prob": 5.9551371123234276e-06}, {"id": 352, "seek": 200836, "start": 2019.6399999999999, "end": 2026.28, "text": " in the graph, where the C-minode A plugs into two different operations that contribute to", "tokens": [50928, 294, 264, 4295, 11, 689, 264, 383, 12, 2367, 1429, 316, 33899, 666, 732, 819, 7705, 300, 10586, 281, 51260], "temperature": 0.0, "avg_logprob": -0.09045702429378734, "compression_ratio": 1.588235294117647, "no_speech_prob": 5.9551371123234276e-06}, {"id": 353, "seek": 200836, "start": 2026.28, "end": 2033.56, "text": " the overall loss. Then, if you nudge A by delta, the output will be simultaneously nudged by this", "tokens": [51260, 264, 4787, 4470, 13, 1396, 11, 498, 291, 297, 16032, 316, 538, 8289, 11, 264, 5598, 486, 312, 16561, 40045, 3004, 538, 341, 51624], "temperature": 0.0, "avg_logprob": -0.09045702429378734, "compression_ratio": 1.588235294117647, "no_speech_prob": 5.9551371123234276e-06}, {"id": 354, "seek": 203356, "start": 2033.56, "end": 2040.52, "text": " derivative from the first branch and this derivative from the second branch. So, the overall effect of", "tokens": [50364, 13760, 490, 264, 700, 9819, 293, 341, 13760, 490, 264, 1150, 9819, 13, 407, 11, 264, 4787, 1802, 295, 50712], "temperature": 0.0, "avg_logprob": -0.08223413839572813, "compression_ratio": 1.5965665236051503, "no_speech_prob": 3.0241932108765468e-05}, {"id": 355, "seek": 203356, "start": 2040.52, "end": 2047.32, "text": " nudging A will be the sum of the two gradients. Alright, great. Now that we have constructed", "tokens": [50712, 40045, 3249, 316, 486, 312, 264, 2408, 295, 264, 732, 2771, 2448, 13, 2798, 11, 869, 13, 823, 300, 321, 362, 17083, 51052], "temperature": 0.0, "avg_logprob": -0.08223413839572813, "compression_ratio": 1.5965665236051503, "no_speech_prob": 3.0241932108765468e-05}, {"id": 356, "seek": 203356, "start": 2047.32, "end": 2054.36, "text": " a computational graph and established how to process individual chunks of it, we can just", "tokens": [51052, 257, 28270, 4295, 293, 7545, 577, 281, 1399, 2609, 24004, 295, 309, 11, 321, 393, 445, 51404], "temperature": 0.0, "avg_logprob": -0.08223413839572813, "compression_ratio": 1.5965665236051503, "no_speech_prob": 3.0241932108765468e-05}, {"id": 357, "seek": 203356, "start": 2054.36, "end": 2060.52, "text": " sequentially apply those rules starting from the output and working our way backwards.", "tokens": [51404, 5123, 3137, 3079, 729, 4474, 2891, 490, 264, 5598, 293, 1364, 527, 636, 12204, 13, 51712], "temperature": 0.0, "avg_logprob": -0.08223413839572813, "compression_ratio": 1.5965665236051503, "no_speech_prob": 3.0241932108765468e-05}, {"id": 358, "seek": 206052, "start": 2061.48, "end": 2066.84, "text": " For instance, the rightmost node in the graph is the resulting value of the loss function.", "tokens": [50412, 1171, 5197, 11, 264, 558, 1761, 9984, 294, 264, 4295, 307, 264, 16505, 2158, 295, 264, 4470, 2445, 13, 50680], "temperature": 0.0, "avg_logprob": -0.0697936905754937, "compression_ratio": 1.6681614349775784, "no_speech_prob": 7.722184818703681e-05}, {"id": 359, "seek": 206052, "start": 2066.84, "end": 2073.24, "text": " How does the incremental change in that node affect the output? Well, it is the output,", "tokens": [50680, 1012, 775, 264, 35759, 1319, 294, 300, 9984, 3345, 264, 5598, 30, 1042, 11, 309, 307, 264, 5598, 11, 51000], "temperature": 0.0, "avg_logprob": -0.0697936905754937, "compression_ratio": 1.6681614349775784, "no_speech_prob": 7.722184818703681e-05}, {"id": 360, "seek": 206052, "start": 2073.24, "end": 2079.96, "text": " so its gradient is by definition equal to 1. Next, the loss function is the sum of many delta", "tokens": [51000, 370, 1080, 16235, 307, 538, 7123, 2681, 281, 502, 13, 3087, 11, 264, 4470, 2445, 307, 264, 2408, 295, 867, 8289, 51336], "temperature": 0.0, "avg_logprob": -0.0697936905754937, "compression_ratio": 1.6681614349775784, "no_speech_prob": 7.722184818703681e-05}, {"id": 361, "seek": 206052, "start": 2079.96, "end": 2086.2, "text": " y's squared. We know what to do with the summation node. It just copies whatever the gradient value", "tokens": [51336, 288, 311, 8889, 13, 492, 458, 437, 281, 360, 365, 264, 28811, 9984, 13, 467, 445, 14341, 2035, 264, 16235, 2158, 51648], "temperature": 0.0, "avg_logprob": -0.0697936905754937, "compression_ratio": 1.6681614349775784, "no_speech_prob": 7.722184818703681e-05}, {"id": 362, "seek": 208620, "start": 2086.2, "end": 2092.8399999999997, "text": " is to the right of it into all incoming nodes. Consequently, the gradients of all delta y's", "tokens": [50364, 307, 281, 264, 558, 295, 309, 666, 439, 22341, 13891, 13, 2656, 46027, 11, 264, 2771, 2448, 295, 439, 8289, 288, 311, 50696], "temperature": 0.0, "avg_logprob": -0.06412278050961702, "compression_ratio": 1.6391304347826088, "no_speech_prob": 5.307506216922775e-05}, {"id": 363, "seek": 208620, "start": 2092.8399999999997, "end": 2100.6, "text": " squared will also be equal to 1. Each of those nodes is the squared value of the corresponding delta y", "tokens": [50696, 8889, 486, 611, 312, 2681, 281, 502, 13, 6947, 295, 729, 13891, 307, 264, 8889, 2158, 295, 264, 11760, 8289, 288, 51084], "temperature": 0.0, "avg_logprob": -0.06412278050961702, "compression_ratio": 1.6391304347826088, "no_speech_prob": 5.307506216922775e-05}, {"id": 364, "seek": 208620, "start": 2100.6, "end": 2105.8799999999997, "text": " and we know how to differentiate this squaring operation. The derivative of the loss function", "tokens": [51084, 293, 321, 458, 577, 281, 23203, 341, 2339, 1921, 6916, 13, 440, 13760, 295, 264, 4470, 2445, 51348], "temperature": 0.0, "avg_logprob": -0.06412278050961702, "compression_ratio": 1.6391304347826088, "no_speech_prob": 5.307506216922775e-05}, {"id": 365, "seek": 208620, "start": 2105.8799999999997, "end": 2112.3599999999997, "text": " with respect to delta y1 will be 2 times the delta y1, which is just the number we found", "tokens": [51348, 365, 3104, 281, 8289, 288, 16, 486, 312, 568, 1413, 264, 8289, 288, 16, 11, 597, 307, 445, 264, 1230, 321, 1352, 51672], "temperature": 0.0, "avg_logprob": -0.06412278050961702, "compression_ratio": 1.6391304347826088, "no_speech_prob": 5.307506216922775e-05}, {"id": 366, "seek": 211236, "start": 2112.36, "end": 2118.04, "text": " during the forward calculation. And we can keep doing this propagation of sequential derivative", "tokens": [50364, 1830, 264, 2128, 17108, 13, 400, 321, 393, 1066, 884, 341, 38377, 295, 42881, 13760, 50648], "temperature": 0.0, "avg_logprob": -0.05762065540660511, "compression_ratio": 1.7222222222222223, "no_speech_prob": 5.562203659792431e-05}, {"id": 367, "seek": 211236, "start": 2118.04, "end": 2124.28, "text": " calculation backwards along our compute graph until we reach the leftmost nodes,", "tokens": [50648, 17108, 12204, 2051, 527, 14722, 4295, 1826, 321, 2524, 264, 1411, 1761, 13891, 11, 50960], "temperature": 0.0, "avg_logprob": -0.05762065540660511, "compression_ratio": 1.7222222222222223, "no_speech_prob": 5.562203659792431e-05}, {"id": 368, "seek": 211236, "start": 2124.28, "end": 2130.36, "text": " which are the data and parameter knobs. The derivatives of the loss with respect to the input", "tokens": [50960, 597, 366, 264, 1412, 293, 13075, 46999, 13, 440, 33733, 295, 264, 4470, 365, 3104, 281, 264, 4846, 51264], "temperature": 0.0, "avg_logprob": -0.05762065540660511, "compression_ratio": 1.7222222222222223, "no_speech_prob": 5.562203659792431e-05}, {"id": 369, "seek": 211236, "start": 2130.36, "end": 2136.92, "text": " data don't really matter, but the derivatives with respect to the parameters is exactly what we want.", "tokens": [51264, 1412, 500, 380, 534, 1871, 11, 457, 264, 33733, 365, 3104, 281, 264, 9834, 307, 2293, 437, 321, 528, 13, 51592], "temperature": 0.0, "avg_logprob": -0.05762065540660511, "compression_ratio": 1.7222222222222223, "no_speech_prob": 5.562203659792431e-05}, {"id": 370, "seek": 213692, "start": 2136.92, "end": 2143.4, "text": " Once these parameter gradients are found, we can perform one iteration of gradient descent.", "tokens": [50364, 3443, 613, 13075, 2771, 2448, 366, 1352, 11, 321, 393, 2042, 472, 24784, 295, 16235, 23475, 13, 50688], "temperature": 0.0, "avg_logprob": -0.08654600381851196, "compression_ratio": 1.625, "no_speech_prob": 2.885710637201555e-05}, {"id": 371, "seek": 213692, "start": 2143.4, "end": 2148.6, "text": " Namely, we are going to slightly tweak the knobs in the directions opposite to the gradient.", "tokens": [50688, 10684, 736, 11, 321, 366, 516, 281, 4748, 29879, 264, 46999, 294, 264, 11095, 6182, 281, 264, 16235, 13, 50948], "temperature": 0.0, "avg_logprob": -0.08654600381851196, "compression_ratio": 1.625, "no_speech_prob": 2.885710637201555e-05}, {"id": 372, "seek": 213692, "start": 2149.16, "end": 2155.0, "text": " The exact magnitude of each adjustment being the negative product of the gradient", "tokens": [50976, 440, 1900, 15668, 295, 1184, 17132, 885, 264, 3671, 1674, 295, 264, 16235, 51268], "temperature": 0.0, "avg_logprob": -0.08654600381851196, "compression_ratio": 1.625, "no_speech_prob": 2.885710637201555e-05}, {"id": 373, "seek": 213692, "start": 2155.0, "end": 2162.12, "text": " and some small number called the learning rate, for example, 0.01. Note that after the adjustment", "tokens": [51268, 293, 512, 1359, 1230, 1219, 264, 2539, 3314, 11, 337, 1365, 11, 1958, 13, 10607, 13, 11633, 300, 934, 264, 17132, 51624], "temperature": 0.0, "avg_logprob": -0.08654600381851196, "compression_ratio": 1.625, "no_speech_prob": 2.885710637201555e-05}, {"id": 374, "seek": 216212, "start": 2162.12, "end": 2167.56, "text": " is performed, the configuration of the machine and the resulting loss are different.", "tokens": [50364, 307, 10332, 11, 264, 11694, 295, 264, 3479, 293, 264, 16505, 4470, 366, 819, 13, 50636], "temperature": 0.0, "avg_logprob": -0.09587608256810148, "compression_ratio": 1.6458333333333333, "no_speech_prob": 3.705295603140257e-05}, {"id": 375, "seek": 216212, "start": 2168.2799999999997, "end": 2172.68, "text": " And so the old gradient values we found no longer hold.", "tokens": [50672, 400, 370, 264, 1331, 16235, 4190, 321, 1352, 572, 2854, 1797, 13, 50892], "temperature": 0.0, "avg_logprob": -0.09587608256810148, "compression_ratio": 1.6458333333333333, "no_speech_prob": 3.705295603140257e-05}, {"id": 376, "seek": 216212, "start": 2173.72, "end": 2180.3599999999997, "text": " So we need to run the forward and backward calculations once again to obtain updated", "tokens": [50944, 407, 321, 643, 281, 1190, 264, 2128, 293, 23897, 20448, 1564, 797, 281, 12701, 10588, 51276], "temperature": 0.0, "avg_logprob": -0.09587608256810148, "compression_ratio": 1.6458333333333333, "no_speech_prob": 3.705295603140257e-05}, {"id": 377, "seek": 216212, "start": 2180.3599999999997, "end": 2188.04, "text": " gradients and the new decreased loss. Performing this loop of forward pass, backward pass,", "tokens": [51276, 2771, 2448, 293, 264, 777, 24436, 4470, 13, 19351, 278, 341, 6367, 295, 2128, 1320, 11, 23897, 1320, 11, 51660], "temperature": 0.0, "avg_logprob": -0.09587608256810148, "compression_ratio": 1.6458333333333333, "no_speech_prob": 3.705295603140257e-05}, {"id": 378, "seek": 218804, "start": 2188.04, "end": 2193.64, "text": " notch, repeat is the essence of training every modern machine learning system.", "tokens": [50364, 26109, 11, 7149, 307, 264, 12801, 295, 3097, 633, 4363, 3479, 2539, 1185, 13, 50644], "temperature": 0.0, "avg_logprob": -0.0589122013612227, "compression_ratio": 1.6626506024096386, "no_speech_prob": 5.6497832702007145e-05}, {"id": 379, "seek": 218804, "start": 2194.2, "end": 2199.64, "text": " And exactly the same algorithm is used today in even the most complicated models.", "tokens": [50672, 400, 2293, 264, 912, 9284, 307, 1143, 965, 294, 754, 264, 881, 6179, 5245, 13, 50944], "temperature": 0.0, "avg_logprob": -0.0589122013612227, "compression_ratio": 1.6626506024096386, "no_speech_prob": 5.6497832702007145e-05}, {"id": 380, "seek": 218804, "start": 2199.64, "end": 2204.6, "text": " As long as the problem you are trying to solve with a given model architecture", "tokens": [50944, 1018, 938, 382, 264, 1154, 291, 366, 1382, 281, 5039, 365, 257, 2212, 2316, 9482, 51192], "temperature": 0.0, "avg_logprob": -0.0589122013612227, "compression_ratio": 1.6626506024096386, "no_speech_prob": 5.6497832702007145e-05}, {"id": 381, "seek": 218804, "start": 2204.6, "end": 2210.7599999999998, "text": " can be decomposed into individual operations that are differentiable, you can sequentially", "tokens": [51192, 393, 312, 22867, 1744, 666, 2609, 7705, 300, 366, 819, 9364, 11, 291, 393, 5123, 3137, 51500], "temperature": 0.0, "avg_logprob": -0.0589122013612227, "compression_ratio": 1.6626506024096386, "no_speech_prob": 5.6497832702007145e-05}, {"id": 382, "seek": 218804, "start": 2210.7599999999998, "end": 2216.68, "text": " apply the chain rule many times to arrive at the optimal setting of the parameters.", "tokens": [51500, 3079, 264, 5021, 4978, 867, 1413, 281, 8881, 412, 264, 16252, 3287, 295, 264, 9834, 13, 51796], "temperature": 0.0, "avg_logprob": -0.0589122013612227, "compression_ratio": 1.6626506024096386, "no_speech_prob": 5.6497832702007145e-05}, {"id": 383, "seek": 221668, "start": 2216.68, "end": 2221.8799999999997, "text": " For instance, a feed-forward neural network is essentially a bunch of multiplications and", "tokens": [50364, 1171, 5197, 11, 257, 3154, 12, 13305, 18161, 3209, 307, 4476, 257, 3840, 295, 17596, 763, 293, 50624], "temperature": 0.0, "avg_logprob": -0.07789777200433272, "compression_ratio": 1.6194690265486726, "no_speech_prob": 4.936985988024389e-06}, {"id": 384, "seek": 221668, "start": 2221.8799999999997, "end": 2227.56, "text": " summations with a few non-linear activation functions sprinkled between the layers.", "tokens": [50624, 8367, 763, 365, 257, 1326, 2107, 12, 28263, 24433, 6828, 30885, 1493, 1296, 264, 7914, 13, 50908], "temperature": 0.0, "avg_logprob": -0.07789777200433272, "compression_ratio": 1.6194690265486726, "no_speech_prob": 4.936985988024389e-06}, {"id": 385, "seek": 221668, "start": 2227.56, "end": 2233.7999999999997, "text": " Each of those atomic computations is differentiable, so you can construct the compute graph", "tokens": [50908, 6947, 295, 729, 22275, 2807, 763, 307, 819, 9364, 11, 370, 291, 393, 7690, 264, 14722, 4295, 51220], "temperature": 0.0, "avg_logprob": -0.07789777200433272, "compression_ratio": 1.6194690265486726, "no_speech_prob": 4.936985988024389e-06}, {"id": 386, "seek": 221668, "start": 2233.7999999999997, "end": 2241.08, "text": " and run the backward pass on it to find how each parameter, like connection weights between neurons,", "tokens": [51220, 293, 1190, 264, 23897, 1320, 322, 309, 281, 915, 577, 1184, 13075, 11, 411, 4984, 17443, 1296, 22027, 11, 51584], "temperature": 0.0, "avg_logprob": -0.07789777200433272, "compression_ratio": 1.6194690265486726, "no_speech_prob": 4.936985988024389e-06}, {"id": 387, "seek": 224108, "start": 2241.08, "end": 2246.44, "text": " influence the loss function. And because neural networks, given enough neurons,", "tokens": [50364, 6503, 264, 4470, 2445, 13, 400, 570, 18161, 9590, 11, 2212, 1547, 22027, 11, 50632], "temperature": 0.0, "avg_logprob": -0.05647464816489916, "compression_ratio": 1.649056603773585, "no_speech_prob": 9.461253648623824e-05}, {"id": 388, "seek": 224108, "start": 2246.44, "end": 2253.08, "text": " can in theory approximate any function imaginable, we can create a large enough sequence of these", "tokens": [50632, 393, 294, 5261, 30874, 604, 2445, 23427, 712, 11, 321, 393, 1884, 257, 2416, 1547, 8310, 295, 613, 50964], "temperature": 0.0, "avg_logprob": -0.05647464816489916, "compression_ratio": 1.649056603773585, "no_speech_prob": 9.461253648623824e-05}, {"id": 389, "seek": 224108, "start": 2253.08, "end": 2259.0, "text": " building block mathematical machines to solve problems such as classifying images and even", "tokens": [50964, 2390, 3461, 18894, 8379, 281, 5039, 2740, 1270, 382, 1508, 5489, 5267, 293, 754, 51260], "temperature": 0.0, "avg_logprob": -0.05647464816489916, "compression_ratio": 1.649056603773585, "no_speech_prob": 9.461253648623824e-05}, {"id": 390, "seek": 224108, "start": 2259.0, "end": 2264.2, "text": " generating new text. This seems like a very elegant and efficient solution.", "tokens": [51260, 17746, 777, 2487, 13, 639, 2544, 411, 257, 588, 21117, 293, 7148, 3827, 13, 51520], "temperature": 0.0, "avg_logprob": -0.05647464816489916, "compression_ratio": 1.649056603773585, "no_speech_prob": 9.461253648623824e-05}, {"id": 391, "seek": 224108, "start": 2264.2, "end": 2270.2799999999997, "text": " After all, if you want to solve the optimization problem, derivatives tell you exactly which", "tokens": [51520, 2381, 439, 11, 498, 291, 528, 281, 5039, 264, 19618, 1154, 11, 33733, 980, 291, 2293, 597, 51824], "temperature": 0.0, "avg_logprob": -0.05647464816489916, "compression_ratio": 1.649056603773585, "no_speech_prob": 9.461253648623824e-05}, {"id": 392, "seek": 227028, "start": 2270.28, "end": 2275.8, "text": " adjustments are necessary. But how similar is this to what the brain actually does?", "tokens": [50364, 18624, 366, 4818, 13, 583, 577, 2531, 307, 341, 281, 437, 264, 3567, 767, 775, 30, 50640], "temperature": 0.0, "avg_logprob": -0.04946341881385216, "compression_ratio": 1.5766129032258065, "no_speech_prob": 4.98595618410036e-05}, {"id": 393, "seek": 227028, "start": 2276.52, "end": 2283.0, "text": " When we learn to walk, speak and read, is the brain also minimizing some sort of loss function?", "tokens": [50676, 1133, 321, 1466, 281, 1792, 11, 1710, 293, 1401, 11, 307, 264, 3567, 611, 46608, 512, 1333, 295, 4470, 2445, 30, 51000], "temperature": 0.0, "avg_logprob": -0.04946341881385216, "compression_ratio": 1.5766129032258065, "no_speech_prob": 4.98595618410036e-05}, {"id": 394, "seek": 227028, "start": 2283.0, "end": 2288.0400000000004, "text": " Does it calculate derivatives? Or could it be doing something totally different?", "tokens": [51000, 4402, 309, 8873, 33733, 30, 1610, 727, 309, 312, 884, 746, 3879, 819, 30, 51252], "temperature": 0.0, "avg_logprob": -0.04946341881385216, "compression_ratio": 1.5766129032258065, "no_speech_prob": 4.98595618410036e-05}, {"id": 395, "seek": 227028, "start": 2288.6800000000003, "end": 2293.32, "text": " In the next video, we are going to dive into the world of synaptic plasticity", "tokens": [51284, 682, 264, 958, 960, 11, 321, 366, 516, 281, 9192, 666, 264, 1002, 295, 5451, 2796, 299, 5900, 507, 51516], "temperature": 0.0, "avg_logprob": -0.04946341881385216, "compression_ratio": 1.5766129032258065, "no_speech_prob": 4.98595618410036e-05}, {"id": 396, "seek": 227028, "start": 2293.32, "end": 2296.1200000000003, "text": " and talk about how biological neural networks learn.", "tokens": [51516, 293, 751, 466, 577, 13910, 18161, 9590, 1466, 13, 51656], "temperature": 0.0, "avg_logprob": -0.04946341881385216, "compression_ratio": 1.5766129032258065, "no_speech_prob": 4.98595618410036e-05}, {"id": 397, "seek": 229612, "start": 2297.08, "end": 2299.88, "text": " In keeping with the topic of biological learning,", "tokens": [50412, 682, 5145, 365, 264, 4829, 295, 13910, 2539, 11, 50552], "temperature": 0.0, "avg_logprob": -0.07575927599511965, "compression_ratio": 1.60546875, "no_speech_prob": 0.0006166273378767073}, {"id": 398, "seek": 229612, "start": 2299.88, "end": 2304.7599999999998, "text": " I'd like to take a moment to give a shout out to Shortform, a longtime partner of this channel.", "tokens": [50552, 286, 1116, 411, 281, 747, 257, 1623, 281, 976, 257, 8043, 484, 281, 16881, 837, 11, 257, 44363, 4975, 295, 341, 2269, 13, 50796], "temperature": 0.0, "avg_logprob": -0.07575927599511965, "compression_ratio": 1.60546875, "no_speech_prob": 0.0006166273378767073}, {"id": 399, "seek": 229612, "start": 2305.72, "end": 2309.72, "text": " Shortform is a platform which lets you take your reading to the next level.", "tokens": [50844, 16881, 837, 307, 257, 3663, 597, 6653, 291, 747, 428, 3760, 281, 264, 958, 1496, 13, 51044], "temperature": 0.0, "avg_logprob": -0.07575927599511965, "compression_ratio": 1.60546875, "no_speech_prob": 0.0006166273378767073}, {"id": 400, "seek": 229612, "start": 2310.44, "end": 2317.08, "text": " They offer book guides, which are supercharged book summaries. Not only do you get the condensed", "tokens": [51080, 814, 2626, 1446, 17007, 11, 597, 366, 1687, 25064, 1446, 8367, 4889, 13, 1726, 787, 360, 291, 483, 264, 36398, 51412], "temperature": 0.0, "avg_logprob": -0.07575927599511965, "compression_ratio": 1.60546875, "no_speech_prob": 0.0006166273378767073}, {"id": 401, "seek": 229612, "start": 2317.08, "end": 2323.72, "text": " version of all the key points, but they are also supplemented by ideas from related sources,", "tokens": [51412, 3037, 295, 439, 264, 2141, 2793, 11, 457, 436, 366, 611, 15436, 292, 538, 3487, 490, 4077, 7139, 11, 51744], "temperature": 0.0, "avg_logprob": -0.07575927599511965, "compression_ratio": 1.60546875, "no_speech_prob": 0.0006166273378767073}, {"id": 402, "seek": 232372, "start": 2323.7999999999997, "end": 2329.16, "text": " such as other books and research papers. I really love this feature because it allows", "tokens": [50368, 1270, 382, 661, 3642, 293, 2132, 10577, 13, 286, 534, 959, 341, 4111, 570, 309, 4045, 50636], "temperature": 0.0, "avg_logprob": -0.07583125094149976, "compression_ratio": 1.6068702290076335, "no_speech_prob": 0.00029595629894174635}, {"id": 403, "seek": 232372, "start": 2329.16, "end": 2333.56, "text": " you to get the big picture overview and promotes the interlinking of ideas.", "tokens": [50636, 291, 281, 483, 264, 955, 3036, 12492, 293, 36015, 264, 728, 75, 12408, 295, 3487, 13, 50856], "temperature": 0.0, "avg_logprob": -0.07583125094149976, "compression_ratio": 1.6068702290076335, "no_speech_prob": 0.00029595629894174635}, {"id": 404, "seek": 232372, "start": 2334.12, "end": 2339.24, "text": " The existing library contains books from a variety of genres, including science,", "tokens": [50884, 440, 6741, 6405, 8306, 3642, 490, 257, 5673, 295, 30057, 11, 3009, 3497, 11, 51140], "temperature": 0.0, "avg_logprob": -0.07583125094149976, "compression_ratio": 1.6068702290076335, "no_speech_prob": 0.00029595629894174635}, {"id": 405, "seek": 232372, "start": 2339.24, "end": 2345.0, "text": " education and technology, and new books are being added every week. Personally, I found", "tokens": [51140, 3309, 293, 2899, 11, 293, 777, 3642, 366, 885, 3869, 633, 1243, 13, 21079, 11, 286, 1352, 51428], "temperature": 0.0, "avg_logprob": -0.07583125094149976, "compression_ratio": 1.6068702290076335, "no_speech_prob": 0.00029595629894174635}, {"id": 406, "seek": 232372, "start": 2345.0, "end": 2349.8799999999997, "text": " Shortform to be really valuable when it comes to choosing books to read, as well as taking", "tokens": [51428, 16881, 837, 281, 312, 534, 8263, 562, 309, 1487, 281, 10875, 3642, 281, 1401, 11, 382, 731, 382, 1940, 51672], "temperature": 0.0, "avg_logprob": -0.07583125094149976, "compression_ratio": 1.6068702290076335, "no_speech_prob": 0.00029595629894174635}, {"id": 407, "seek": 234988, "start": 2349.88, "end": 2355.56, "text": " efficient notes. Don't hesitate to give it a try by following the link down in the description", "tokens": [50364, 7148, 5570, 13, 1468, 380, 20842, 281, 976, 309, 257, 853, 538, 3480, 264, 2113, 760, 294, 264, 3855, 50648], "temperature": 0.0, "avg_logprob": -0.09175116419792176, "compression_ratio": 1.5130434782608695, "no_speech_prob": 0.010168736800551414}, {"id": 408, "seek": 234988, "start": 2355.56, "end": 2360.6, "text": " to get 5 days of unlimited access and 20% off the annual membership.", "tokens": [50648, 281, 483, 1025, 1708, 295, 21950, 2105, 293, 945, 4, 766, 264, 9784, 16560, 13, 50900], "temperature": 0.0, "avg_logprob": -0.09175116419792176, "compression_ratio": 1.5130434782608695, "no_speech_prob": 0.010168736800551414}, {"id": 409, "seek": 234988, "start": 2361.2400000000002, "end": 2365.32, "text": " If you enjoyed this video, press the like button, share it with your friends and colleagues,", "tokens": [50932, 759, 291, 4626, 341, 960, 11, 1886, 264, 411, 2960, 11, 2073, 309, 365, 428, 1855, 293, 7734, 11, 51136], "temperature": 0.0, "avg_logprob": -0.09175116419792176, "compression_ratio": 1.5130434782608695, "no_speech_prob": 0.010168736800551414}, {"id": 410, "seek": 234988, "start": 2365.32, "end": 2370.28, "text": " and subscribe to the channel if you haven't already. Stay tuned for more interesting topics", "tokens": [51136, 293, 3022, 281, 264, 2269, 498, 291, 2378, 380, 1217, 13, 8691, 10870, 337, 544, 1880, 8378, 51384], "temperature": 0.0, "avg_logprob": -0.09175116419792176, "compression_ratio": 1.5130434782608695, "no_speech_prob": 0.010168736800551414}, {"id": 411, "seek": 237028, "start": 2370.28, "end": 2386.2000000000003, "text": " coming up. Goodbye and thank you for the interest in the brain.", "tokens": [50364, 1348, 493, 13, 15528, 293, 1309, 291, 337, 264, 1179, 294, 264, 3567, 13, 51160], "temperature": 0.0, "avg_logprob": -0.3587470054626465, "compression_ratio": 0.9692307692307692, "no_speech_prob": 0.16647633910179138}, {"id": 412, "seek": 240028, "start": 2400.28, "end": 2401.6600000000003, "text": " you", "tokens": [50408, 291, 50433], "temperature": 0.0, "avg_logprob": -0.9182581901550293, "compression_ratio": 0.2727272727272727, "no_speech_prob": 0.9031811952590942}], "language": "en"}