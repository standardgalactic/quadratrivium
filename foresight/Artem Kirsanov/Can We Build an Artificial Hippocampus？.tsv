start	end	text
0	6320	It is humanity's long-time dream to create machines that can think, but what exactly does it mean?
6320	11120	One particular characteristic of intelligence is the ability to generalize knowledge
11120	17680	and flexibly adapt it to new situations. Such generalization is indeed one of the cornerstone
17680	22480	problems in modern machine learning. In this video, we are going to see how we can take
22480	28720	biological organization over hippocampus, a brain structure involved in memory and navigation,
28720	34000	as an inspiration in order to construct a computational model that can learn to build
34000	38880	abstractions and generalizations, as well as the insights we can draw from this model,
38880	43360	both about our own brains and the field of artificial intelligence.
43360	48320	Before we begin, I'd like to warn you that this video is the continuation of the previous video
48320	54080	in the series on cognitive maps. Last time we explored neurobiological background of hippocampal
54080	59360	computations and introduced some general principles, so if you haven't seen it, I highly
59360	64080	recommend you check it out before watching this one, since we are going to build up from there.
64640	66320	If you're interested, stay tuned.
73200	79200	Imagine you're an agent that walks around the world and his only goal is to find rewards.
79200	84160	From an evolutionary perspective, you can think about such an agent as an early organism,
84160	90080	which needs to look for food or mates. Now, as an agent, you have a certain repertoire
90080	95360	of actions that you can take. For example, activate a sequence of muscles to move in a particular
95360	102400	direction. To choose the most rewarding actions, you need to be able to predict the action outcomes,
102400	107200	and that effectively requires a mental model of the surrounding environment.
107200	112080	Existence of such a model allows you to run mental singulations in your head
112080	117680	to weigh the actions. For example, what would happen if I go straight or is it better to turn
117680	123360	right? Over the course of your lifetime, as you encounter a variety of different environments,
123360	129280	initially you might build an entangled, indivisible model for each, without necessarily linking
129280	134560	different models to each other. However, if you're being optimal in your representations,
134560	140000	at some point you realize, wait a minute, all these models I've built so far actually have
140000	146720	an awful lot in common. Indeed, walls that block your way, doors that allow you to go through the
146720	153680	walls and even just the structure of an open 2D space itself all work similarly in every environment,
153680	160640	so these common elements can be easily reused. In other words, it makes sense to break up or
160640	166400	factor each model into its building blocks. For example, building blocks of space, of boundaries,
166400	172560	rewards, etc. Once these building blocks are learned, we can rearrange and mix them in different
172560	179200	configurations to build new models of the world on the fly, and thus generate flexible behavior.
180560	185600	As you might remember from part 1, this is exactly what my mailing hippocampus does,
185600	191120	and we can find neurobiological evidence for this process in responses of individual cells.
191760	195360	Now, the question is, can we teach a machine to do the same?
196320	202320	To make the task easier for an artificial system, let's formulate it as a prediction problem.
202320	208240	Namely, the model will receive a sequence of observations, along with the sequence of actions
208240	214000	that led to them, and learn to correctly predict the next observation in the sequence.
214000	219280	It actually makes a lot of sense biologically. There is a great deal of data suggesting that
219280	224400	the main purpose of the brain may be to predict the incoming stimuli and try to minimize the
224400	230240	prediction error, a theory called predictive coding. For example, consider the following
230240	236480	sequence of observations and actions. Can you tell me what should be the next element in the sequence?
237440	243760	Seems impossible, right? However, what if I told you that those actions, one through four,
243840	251360	actually stand for directions, north, west, south, and east? Now, the task becomes much easier.
251920	257280	Because you know the rules how to chain these actions together, you can predict the next
257280	262640	observation to be the same as the first one, since you know you essentially closed a loop.
262640	269040	In other words, knowing the structure of space significantly simplifies the prediction problem.
269040	274400	But the model, of course, would not know this underlying structure since it would be no fun.
274400	280240	Instead, it would need to extract repeating patterns in order to somehow infer this structure
280240	286000	of the underlying world from sequences of observations and actions. For example, after
286000	291440	seeing a large number of sequences like these, it should infer the rules how different actions
291440	295600	relate to each other, which is equivalent to constructing the structure of space.
295600	302400	It's important to point out that although I'm saying things like the model will learn the
302400	310240	underlying structure of the world, it is not told to do that exactly. The model has no other goal,
310240	315120	so to speak, other than predicting the next observation in the sequence. In essence,
315120	320400	it is just a fancy mathematical expression with a large number of parameters that takes the set
320400	325440	of numbers encoding the observations and actions, performs computation on them, and spits out
325440	331680	another set of numbers corresponding to the next predicted observation. But because we train it to
331680	337840	minimize this prediction error, and since these observations are not random but come from some
337840	343920	structured world, the optimal solution to this prediction problem is to construct some structural
343920	349920	representation of this world, which underlies the regularities in the observations. So we simply
349920	355840	expect the knowledge about the structure to emerge as a result of optimization. But how should the
355840	362960	model look like? Well, because we are free to choose whichever architecture we want, it is reasonable
362960	369520	to draw inspiration from an existing biological machine that solves this problem on a daily basis,
369520	375840	the hippocampal formation. In the last video, we saw how the hippocampus receives two streams of
375840	382400	inputs, sensory, the what am I seeing information, coming from the lateral entorhinal cortex,
382400	389280	and structural, the where am I information, from medial entorhinal cortex. They are then combined
389280	396960	in the hippocampus into a conjoined representation. Similarly, our model will have an analog of
396960	402560	medial entorhinal area, responsible for keeping track of current location in the world.
403520	409920	Let's call it a position module. At every point in time, it will receive an action
409920	416640	and use it to compute the estimate of the current location, the best guess of where it is in space.
416640	420800	You can think of this positional information as being encoded by the pattern of neuron
420800	427840	activations inside of it. Note that the position module operates purely with actions and doesn't
427840	433280	receive any information about the sensory observations. Similarly, how if you close your
433280	439040	eyes and walk around the room, you have a rough idea of where you are located, even though you
439040	446480	don't see anything. This is because your brain is able to accumulate self-movement vectors and
446480	453360	estimate the position, a process known as path integration. So once the model is trained,
453360	456960	we expect our position module to be able to do the same.
459680	466000	Another crucial component is the hippocampus itself, which binds the where information
466000	472240	with what. This binding is effectively forming an association between the two inputs.
473360	479280	So we need to add a memory module that would receive the positional information provided
479280	485200	by the position module together with this stream of sensory inputs and store each
485200	491360	encountered combination in memory. Essentially, it memorizes the associations between position and
491360	499040	observation. I was at X when I saw Y. But storing memories would be useless if we couldn't retrieve
499040	505600	them. Importantly, since this is an associative memory module, it should be able to reconstruct
505600	513040	the full memory from partial information. For example, we could provide it with just the position
513040	518560	and it would go and search in all of the stored memories which observations were accompanied
518560	524800	by this position. So essentially answering the question, what did I see last time I was here?
525520	531200	And similarly, we could provide it with just the sensory observation and it would retrieve position.
531200	538800	Where was I last time I saw this? Now we have all the necessary components
538800	545600	to solve the prediction problem. Let's walk step by step on what the trained model will do to come
545600	551520	up with a successful prediction when walking, for example, on a family tree. Remember, it should be
551520	558720	capable of learning any type of structure, not just the four connected grids. So we start on John,
558720	564720	transition to Mary via a sister action, and then to Kate via a daughter action.
565360	572320	Finally, we give the model the action labeled as Uncle and ask it to make a prediction and what's
572320	578560	happening under the hood is the following. At first, the position module has some initial belief
578560	584720	about the current location which is combined with John and this combination is stored in the memory
584720	592000	module. Next, the sister action is fed into the position module which comes up with a new
592000	598720	belief about location that is combined with Mary and the corresponding conjunction is stored in
598720	606160	memory. Similarly, daughter action is used to update the internal state of the position module
606160	613600	which is combined with Kate and sent to the memory module. Finally, the uncle action is fed into the
613600	619680	position module. Importantly, the resulting positional information, the pattern of neuron
619680	625840	activations is the same as the one we started with. This is because after the model is trained on many
625840	632960	family trees underlaying by the same rules, the position module is configured to always return
632960	639840	to the same position when we make loops like these. In other words, the general laws governing the
639840	646320	transition logic on the world graph become embedded into the rules of how the position module
646320	653280	updates its state. After performing path integration correctly, we return to this starting position
654240	660320	but there is no corresponding sensory observation to memorize. Instead, because the model has reached
660320	666640	the end of the sequence, it tries to predict the next observation but it has the path integrated
666640	673760	position to guide this prediction. So it queries the memory module with the positional information
673760	680160	and retrieves a sensory observation corresponding to this particular position which in our case
680160	687120	is John. Awesome, right? So far we have just been theorizing about this spherical model in a vacuum
687760	695200	but does it actually, well, work? And if so, what does it tell us about our own navigational systems?
695200	702400	The most direct way to assess how well the model is performing is to look at its accuracy which is
702400	708400	just the percentage of predictions it made correctly and importantly look at how quickly the accuracy
708400	714240	grows. Here is what I mean. Imagine for a moment that instead of this fancy machine we had just
714240	720400	a good old lookup table which simply memorizes all the transitions as pairs. Previous observation
720400	727440	plus action equals new observation. So it would store memories like John plus sister equals Mary,
727440	733840	Mary plus daughter equals Kate, etc. And to predict the next observation it would simply scan the
733840	740000	lookup table and search for a particular combination. In the case of our family tree example, on first
740000	746320	try it would not be able to predict that Kate's uncle is John because it hadn't encountered this
746320	753840	particular combination previously. In other words, to reach 100% accuracy it would need to first
753840	759840	encounter all possible combinations of observations and actions and this means that the performance
759840	767200	of the model depends on the number of edges of this graph that it visited. In contrast,
767200	774080	tolman eigenbau machine or TEM doesn't need to be explicitly told at the outcome of every action
774080	779840	from every node because it has the notion of a structure. For example, if I tell you that Kate
779840	785360	is Mary's daughter that is enough for you to infer the rest of the relationships automatically.
786160	792800	And this essentially means that to reach 100% accuracy it is enough for TEM to just visit
792800	798640	all the nodes instead of all possible edges and hence its performance depends on the proportion
798640	805520	of nodes visited which grows much faster than the proportion of edges. So our machine seems to
805520	811840	indeed construct a representation of the world. Hooray! But what's going on inside of its brain,
811840	818640	so to speak? Let's look inside the position module first. Remember, the belief about current
818640	824960	location is encoded with a pattern of collective activation of neurons. But we can also interrogate
824960	831040	individual neurons and look at what each one of them is doing as the agent randomly walks around.
831040	835680	Here for the sake of visualization, I'm going to show results after the model was trained on
835680	841600	regular four-connected grids analogs of physical 2D space rather than social hierarchies.
842800	848560	Remarkably, we see that individual units in the position module develop periodic activity
848560	854400	patterns as a function of position. They tile the space with the regular hexagonal grids of
854480	862720	different size or these periodic stripes exactly like grid cells and band cells of the entorhinal
862720	870080	cortex encode position in mammalian brains. And the selectivity of individual units is
870080	877360	preserved across environments, suggesting that they indeed can generalize. Neurons in the memory
877360	883040	module do something different. Since they form a conjunction between positional and sensory
883040	889040	information, each neuron would be active when both of the two upstream components are active.
890080	895920	Indeed, units in the memory module resemble hippocampal place cells of various size,
895920	902560	which fire in a particular patch of space. Importantly, just like hippocampal representations
902560	909520	in real brains, they are firing patterns differ across environments, since the incoming observations
909520	916800	are different. This is known as hippocampal remapping. I'd like to emphasize that such grid-like
916800	923600	and place-like representations were never hard-coded into the model. We started with essentially a
923600	928640	random set of parameters, let the model optimize itself to come up with the best solution to the
928640	935440	prediction problem, and those responses just emerged naturally. So far, we've trained the model
935440	941120	on sequences that were generated from random walks in a given environment, which means that
941120	947120	all the observations were equally likely. But in the real life, animals don't really move by
947120	953600	diffusion. They are biased towards rewards and exploring objects. They like being near walls
953600	960080	because it feels safe and avoid open spaces. So the question is, if we change the statistics of
960080	966160	the sensory observations so that some stimuli are more common than others, would it affect the
966160	971520	representations that emerge in our model as the optimal solution to the prediction problem?
972320	978080	For example, let's train TEM on sequences of observations that mimic the behavior of a real
978080	985520	mouse, which prefers to spend time near boundaries and approaches objects. In this case, representations
985520	991760	that emerge in the position module now include boundary cells, which are selective to borders of
991760	998080	the world, and object vector cells that seem to activate whenever the animal is at a certain
998080	1004480	distance and certain direction away from any object. Both of these types of responses,
1004480	1010560	that by the way also generalize across contexts, are observed experimentally when recording from
1010560	1016320	entorhinal cortex, while some neurons in the memory module develop selectivity to particular
1016320	1023920	objects, resembling landmark cells of the hippocampus. If we take a more complex sequence,
1023920	1029360	such as the one mimicking the animal that is performing an alternation task, the model successfully
1029360	1036320	learns the rule that the reward is alternating between the sides. Importantly, representations of
1036320	1042000	some neurons in the memory module resemble the splitter cells found experimentally,
1042000	1046960	which are modulated by both the position and the direction of the future turn.
1047760	1054480	This suggests that TEM has the capacity to learn and map latent spaces, which are not
1054480	1061760	directly given to it in the observations. Another example of how TEM maps latent space is available
1061760	1066560	as a bonus clip to my Patreon supporters. More details at the end of this video.
1067280	1072320	Terrific! Now we have a model that can generalize and naturally develops
1072320	1078000	same representations of space as the hippocampal formation. So what insights can we draw from it?
1080000	1086160	Recall that play cells remap, which means they change their preferred firing locations in different
1086160	1091920	environments. This process has not been thought to be random since there is no immediate logic in
1091920	1098400	how these representations drift around. But having a model of hippocampal formation at hand,
1098400	1105200	we can start to address this question on a whole other level. Notice that neurons in our memory
1105200	1111760	module, the ones that resemble play cells, are actually conjunctions between sensory and structural
1111760	1118800	information. This means that firing of a particular play cell is partially controlled by grid cells,
1118800	1124160	which provide the structural information. So, for example, if in one environment,
1124160	1130320	the location of a given play cell coincides with the hexagonal activity pattern of a particular
1130320	1137280	grid cell, then after we change the surroundings and the play cell remaps, its place field will shift
1137280	1144720	to another location which also lies on this grid. In other words, remapping is not completely random,
1144720	1149760	but rather is controlled by grid cells, preserving some structural information.
1150640	1156720	This relationship between the locations of place and grid cells implies that there should be a
1156720	1163040	correlation between the degree to which firing locations of place and grid cells coincide across
1163040	1169120	two environments. This is the case in the model, and remarkably, when the authors tested this
1169120	1174160	prediction on experimental data, they found it to be true in real brains as well.
1177200	1182720	Well, I know this was a ton of information to process, so let's try to tie everything together.
1183760	1188000	The problem of constructing internal models of the world is cornerstone
1188000	1194960	for both biological and artificial intelligence. It can be solved by factorizing the surrounding
1194960	1200880	into building blocks and combining them with particular sensory contexts to generate new
1200880	1208240	models on the go, allowing for rapid generalization. This factorization and composition can be
1208240	1213840	demonstrated in a computational model, which, when tasked with predicting the next observation in
1213840	1220800	its sequence, learns the underlying relational structure of the world. Representations that
1220800	1226880	naturally emerge in this model resemble real neurons found in the hippocampal formation,
1226880	1233120	suggesting a unified framework of interactions between entorhinal cortex and hippocampus.
1235920	1240400	I want to take this opportunity to give huge thanks to Dr. James Whittington,
1240480	1246160	the first author of the original TAM paper, and Gus, my friend and fellow patron with an
1246160	1251920	expertise in machine learning, who both helped me immensely with preparing the script for this video.
1252800	1257680	As a final note, I will mention that the Tolman Eigenbaum machine we've seen today
1257680	1263280	is actually very similar to a transformer architecture, a type of neural network that
1263280	1269040	is at the core of modern machine learning. In fact, with one little modification,
1269040	1275520	we can turn this similarity into a precise mathematical equivalence. And this modified
1275520	1281760	version, called the Tolman Eigenbaum machine transformer, learns much faster and performs
1281760	1288000	better, while still resembling biological representations for the most part. This potentially
1288000	1292800	provides a very promising link between neuroscience and modern machine learning,
1292800	1295600	which makes both fields even more exciting than ever.
1295680	1302240	Now, I know this was a very simplified description, but fully exploring this equivalence
1302800	1307120	would require going over the transformer and hopfield networks in detail.
1307840	1313120	Let me know down in the comment section if you would like to see a more technical video of this kind.
1314000	1318960	In the meantime, if you're interested in machine learning and don't want to wait any longer,
1318960	1322800	let me tell you about something that can take your understanding to the next level,
1323520	1330240	brilliant.org. Brilliant is a revolutionary platform for engaging and interactive learning.
1330240	1335200	Gone are the days of passive textbook reading. With Brilliant, you'll engage with the material
1335200	1341600	in a hands-on way, solving problems, answering questions and participating in stunning interactive
1341600	1346480	visualizations, which help you develop an intuitive understanding of the material.
1346480	1351600	One course that you might find particularly interesting after watching this video is titled
1351600	1357200	Artificial Neural Networks. It offers an accessible introduction into the world of artificial
1357200	1363120	intelligence and how it is inspired by the human brain. You will learn how neural networks work,
1363120	1368960	how to build your own, and even how to train them to recognize patterns. But that's just the tip of
1368960	1374640	the iceberg. With over 80 courses to choose from, Brilliant has something for everyone,
1374640	1380000	and with its personalized approach, you can learn at your own pace with bite-sized chunks.
1380640	1384880	Take your curiosity to the next level today. Go to brilliant.org
1384880	1391040	slash artemker sonoff to get a 30-day free trial of everything Brilliant has to offer,
1391040	1396720	and the first 200 people to use this link will get 20% off the premium subscription.
1398080	1402320	If you enjoyed this video, press the like button, share it with your friends and colleagues,
1402320	1407600	subscribe to the channel if you haven't already, and consider supporting me on Patreon to suggest
1407600	1413440	video topics and enjoy the bonus content. Stay tuned for more interesting topics coming up.
1414000	1423520	Goodbye, and thank you for the interest in the brain.
