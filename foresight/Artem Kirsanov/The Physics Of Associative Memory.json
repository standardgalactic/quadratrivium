{"text": " Consider the following scenario. You are at a party when you hear a short snippet of your favorite song. Almost instantly, your brain recalls the lyrics of that song and many related memories, such as attending a recent concert featuring that artist. It seems very natural and unimpressive. After all, people can recall information all the time. However, if you think about it, this problem is computationally non-trivial. Let's put ourselves in the shoes of evolution and try to come up with an algorithm for the brain to solve it. The first approach that comes to mind is to actually store some kind of a database of all the songs you've heard a sufficient number of times, along with related information, like the title and lyrics. When an audio fragment of an unknown song is received, we can scan through all the songs in our database, find the one that has a close enough match, and retrieve its lyrics. However, the search space of every song I've ever heard is astronomically large, and it's even larger when considering every single memory you've formed since childhood. Performing an exhaustive search would be simply impossible. Yet, you seem to have no problem instantly recognizing familiar stimuli and finding associations between them. So, how does the brain accomplish this so quickly? In this video, we will lay the foundation for a new paradigm of information storage and retrieval, which is more in line with biology, and actually build one of the simpler models of this process, known as Hopfield Networks, developed by John Hopfield in 1982, who laid an important groundwork for many ideas in both neuroscience and machine learning. If you're interested, stay tuned! Just to reiterate, we need a way to somehow query what we know and find associations between existing memories and new inputs without explicitly checking individual entries for a match, which seems like an impossible problem. However, we can draw insights from a seemingly unrelated field of molecular biology, and in particular, a concept known as Leventhold's paradox of protein folding. As you may know, proteins are long chains of amino acids that fold into specific three-dimensional structures which determine their function. The number of possible structural configurations a protein can take, considering all the different ways you can arrange the atoms of an amino acid chain in three-dimensional space, is absolutely enormous. Given the number of possibilities, it seems like it would take an astronomical amount of time for a protein to search through all the possible structures to find its correct folded state. In fact, there are computations showing that even if the protein samples its different conformations at a nanosecond scale, it would still require more time than the age of the universe to arrive at the correct configuration. Yet, in reality, proteins fold into their native structures in a matter of milliseconds. So, how do they accomplish this? When I first heard this paradox in high school, it seemed to me like an ill-posed question. After all, the protein molecule is not a computer, so it doesn't do any sort of search. It just folds into the most stable and favorable configuration according to physical laws. This is similar how when you throw a ball, the ball doesn't search through all the possible trajectories to select the optimal parabolic one. It simply follows that path because, well, physics works this way. But how can we think about this folding into a favorable configuration? Favorable for what exactly? Let's introduce the concept of energy as it will come in handy in future videos as well. If you think back to your high school physics days, you may recall something along the lines of energy is a quantitative property that describes this state of a system, namely the capacity to do work or cause change. Energy can be stored in a variety of different forms, and for the case of proteins, we will be interested in potential energy stored in the interactions between the atoms in the protein chain. Each possible configuration of the protein chain has a specific potential energy level determined by the sum of all of these atomic interactions. In other words, we can assign a positive number to each state equal to its energy, which is a function in some very high dimensional space where different dimensions correspond to degrees of freedom you need to uniquely describe a configuration. For example, all possible dihedral angles of peptide bonds. Let's abstractly visualize it as having just two dimensions. Then the energy function can be thought of as a surface where each point on it represents a possible protein configuration, and the height of the point represents the potential energy of that configuration. This is what we are going to refer to as energy landscape. For a protein, it would be a complex rugged surface with many peaks and valleys. Now, here is the key point. A protein molecule, like any physical system, tends to minimize its potential energy, guided by this second law of thermodynamics. It will naturally seek out the configuration that has the lowest possible energy level, as this represents the optimal arrangement of its atoms. And this, in fact, corresponds to the native, correctly folded state. When a protein is folding, it is essentially rolling downhill on the energy landscape, following the steepest path towards the valley. This is why proteins can fall so quickly. They don't need to search through all possible configurations. They simply follow the natural tendency of physical systems to minimize their potential energy. The protein's folding process is guided by the shape of the energy landscape, which in turn is determined by the interaction between its atoms. And the descent along the surface is essentially driven by the underlying physical process of energy minimization. Now, the core idea is to achieve something similar for the case of associative memory. Suppose we have a system that can encode information in its states, and each configuration has a specific potential energy determined by the interaction between the states. Then we need to first somehow sculpt the underlying energy landscape so that memories or state patterns we want to store correspond to local minima, these wells in the energy surface. Second, we need something that would play the role of the second law of thermodynamics and would drive the changes in the states, directing the system towards the nearest local minimum. Once these two things are achieved, retrieving a memory that is most similar to the input pattern is done by configuring the system to encode the input pattern initially and letting it run to the equilibrium, descending into the energy well, from which we can read out the source memory. Sounds neat, right? So let's get into building it. Let's consider a set of neurons which we can think of as abstract units that can be in one of two possible states, plus one or minus one. This is a simplified analogy of how nerve cells in the brain encode information through patterns of firing. They either generate an electrical impulse at a given point in time or remain silent. We'll focus on the fully connected network where each neuron has connections to every other neuron. These connections have weights associated with them, real numbers that signify the strength of coupling between the corresponding pair of neurons. For a pair of units i and j, we denote the connection weight between them as wij and the states of neurons themselves as xi and xj. In the brain, connections between neurons or synapses have a well-defined direction. A pair of neurons is connected asymmetrically, meaning that the synapse from neuron A to neuron B is physiologically separate from the synapse that connects B to A if that one exists at all, and so they can have different weights. While we could generalize a Hopfield network to account for asymmetric connections, it would introduce complications and potentially unstable behavior. For simplicity, here we will stick to the original formulation of the Hopfield network, which assumes symmetric weights. In other words, neurons i and j are connected by the same weight in both directions. Now that we have a set of neurons symmetrically linked with each other through weighted connections, let's explore what these connection weights represent. If wij is greater than zero, the connection is said to be excitatory and favors the alignment between the states of two neurons. We can think of each connection as being either happy or unhappy, depending on the states of its neurons. For example, if wij is a large positive number, it means that neurons i and j are closely coupled, and one excites the other. In this case, when one neuron is active, the other tends to be active as well, and when one is silent, the other one is more likely to be silent. These configurations, where both xi and xj are either one or minus one, agree with the connection weight. However, if we observe, for example, that xi is equal to one and xj is equal to minus one, it conflicts with the excitatory nature of the connection, making such a configuration less likely. Conversely, when wij is negative, the connection promotes misalignment between the weights. This alignment between the signs can be expressed more concisely using the product xi times xj. This product will be positive when both neurons have the same sign and negative when they have different signs. By multiplying this product further by the connection weight, we obtain an expression for the happiness of that connection. For a positive wij, happiness will be positive when the product of the two states is positive. But this is just one edge. We can extend this idea and compute the happiness of the entire network as a whole by summing this quantity across all edges. The larger that number is, the more overall agreement there is between connection weights and pairwise states of neurons. Ultimately, we will search for a set of weights that maximize this quantity. And maximizing happiness is equivalent to minimizing it with a minus sign, which you can think of as the measure of overall conflict between the actual configuration of states and what's favored by the connection weights. This total conflict between the weights and pairwise states is exactly what we are going to define to be the energy of the system. As we discussed previously, we want the Hopfield network to be able to gradually evolve towards energy minima. But looking closely at the formula, we can see that the energy value depends both on the states and the weights. So there is a lot of things the system can tweak to change it. What exactly is getting adjusted? As we will see further, there are essentially two modes of network updates that nicely map to the two aspects of associative memory. Namely, adjusting the weights corresponds to shaping the energy landscape, defining which configurations are stable by digging energy wells around them. This is the act of learning when we are writing new memories into the network. Once the weights are fixed, tweaking the states of neurons to bring them into greater agreement with the weights corresponds to descending along the energy surface. This is the act of inference when we are recalling the memory that is at the bottom of the energy well, which is nearest to the configuration of the input pattern. Let's take a look at inference first. Suppose for a second, someone has already set the weights W and hence us the backbone of the network. The neurons themselves with all the connection weights. However, the exact configuration of states, which neurons are active and which are silent, is unknown. The question then becomes, how do we find the state pattern that would minimize the total energy? As we discussed, simply checking all possible states is not an option. So, we will start with some initial state, which could be either a partial or a noisy version of one of the memories or a random configuration altogether. Once the initial condition is set, we will iteratively try to lower the energy value by focusing on updating one neuron at a time. Let's denote the neuron we are currently considering as neuron i. We will calculate the total weighted input to it from all other neurons in the network. This input, which we'll denote as hi, is the sum of the states of all other neurons multiplied by their respective connection weights. If hi is positive, it means that the weighted sum of the other neuron states is in favor of neuron i being in the plus one state. Conversely, if hi is negative, it suggests that neuron i should be in the minus one state to minimize the conflict with the other neurons. So, we will update the state of the neuron i based on the sign of hi. Notice that this update is guaranteed to decrease the energy of the network, because from the two candidate states, we are selecting the more energetically favorable one. You can think of this as a kind of a voting process. Each neuron looks at the states of all other neurons weighted by the strength of their connections and decides whether to be active or silent based on the majority vote. We'll go through this process for each neuron in the network one by one chosen in random order, updating their states based on the input from all other neurons. Once we've updated all neurons, we will have completed one iteration of the network inference and decreased the system's energy by a little bit. We'll keep repeating this process, doing these sweeps through all neurons, updating them one at a time based on the current configuration. As we do this, the network will gradually evolve towards a configuration that minimizes the overall energy. At some point, however, we will reach a configuration where flipping any neuron would lead to an increase in energy. So, no further adjustments would be necessary. At that point, the network has converged to a stable configuration, where each neuron's state agrees with the majority vote. This stable configuration represents a local minimum in the energy landscape. Now, you might be wondering, is the network guaranteed to reach such a stable configuration? Could we possibly stumble into a particularly unlucky set of states and get stuck in a never-ending loop of flipping neurons back and forth? In other words, is such iterative flipping of one neuron at a time equivalent to doing a descent along the energy surface? This is where we come back to the point about symmetric weights. It turns out that there is a mathematical proof that I'm not going to cover here, stating that as long as your weights are symmetric, this simple majority vote single neuron update rule is guaranteed to eventually converge to a stable configuration if you do it enough times. To restate it, the Hopfield network can settle into different local minima based on its initial conditions. These local minima in the energy landscape correspond to distinct memories stored in the network. When we initialize the network with a pattern that is similar to one of these memories in some way and let it evolve, it will fall into the nearest local minimum, effectively recalling the complete stored memory, thus performing pattern completion or noise correction. But so far we haven't talked about how we come up with the set of connection weights that encode specific memories in the first place. So let's explore the learning process. Before we move to storing several memories, let's consider memorizing a single pattern of states. That means the network would have a single global minimum, one energy well, and would converge to the same pattern every time no matter where you initialize it. While it has little practical use, it provides a nice starting point to describe the learning procedure. Let's denote the template pattern that we'd like to store as XC, which is a vector packing the states of all neurons, and XCI will denote the ith component, the state of ith neuron encoding the memory. While XI refers to the state of ith neuron in the network in general, which could be tweaked. Revisiting our definition of energy, we want to set Wij so that this quantity would be at its minimal value for the memory pattern. If we plug XI equal to XCI, we get the equation for the energy of the reference pattern as a function of weights, which we want to turn into a global minimum. Notice that we don't really care about the absolute value of that energy. As long as the energy of the desired memory pattern is less than the energy of any other configuration. Now intuitively, the lowest possible energy is obtained when all the connection weights fully align with the state pairs. But when we have just a single pattern, this is very easy to do. All we need is to set the weight Wij to be the product of the corresponding pair of states in the memory pattern. This way, every connection is satisfied and the energy of the network when it is in the state XI becomes the negative of the total number of edges. When the network is in the state XI, any single flip of a neuron would increase the energy, thus making it a stable state. I want to reiterate the crucial point here. If we want to come up with the set of weights that would dig an energy well around some pattern, then all we need to know are the pairwise relationships between states in that pattern. If the two neurons are active together in the source memory, strengthening the connection between them lowers the hopfield energy of that memory, effectively storing it in the weights for associative recall. You may have heard the famous statement from Neuroscience attributed to Donald Hebb, neurons that fire together, wire together. And in fact, what we just did is known as the Hebbian learning rule. Great, so we found a way to make a single pattern a stable state of the network. But we want to store multiple patterns. How do we do that? Here's the key idea. We can simply sum the weights we would get for each pattern separately. So if we have three patterns, X1, X2 and X3, we can set the weights according to the following equation. What this will do is turn each of the patterns into a local minimum. It's pretty straightforward to show mathematically, and if you're interested, I encourage you to check out the references in the video description. However, intuitively, if the patterns you want to store are very different, so they are far away in the state space from each other, then if you first independently dig energy wells around each of them, and then simply add the energy landscapes together, the resulting surface will have local minima in the same three valleys. And this nicely brings us to the limitation of the hopfield networks. There is a limited number of valleys we can sculpt in the energy landscape before they start to interfere with each other. At some point, if we try to store too many patterns, the network will fail to converge to a stored pattern reliably and recall weird in-between kind of memories. The total maximum number of patterns you can store is thus limited and depends only on the size of the network. It is approximately 0.14 times the number of neurons. So, if you have a hopfield network of 100 neurons, you can reliably store less than 14 patterns in the best case scenario. If you are unlucky, however, and some patterns are similar to each other or correlated, their energy wells will begin to interfere even before you reach the full capacity. All of this makes vanilla hopfield networks not useful for practical purposes. However, to this day, they provide a powerful and intuitive model of associative memory, a simple network of neuron-like units that can store and retrieve pattern through purely local learning and inference rules. Despite their limitations, hopfield networks have laid the groundwork for more advanced energy-based models. In one of the next videos, we will look at the extension of the hopfield networks known as Boltzmann machines. These generative architectures introduce additional hidden units and stochastic dynamics, allowing them to learn more complex probability distributions. There is also an extension to modern hopfield networks, published in 2016 with John Hopfield himself as one of the authors, but that's a topic for another time. In the meanwhile, I'd like to take a moment and thank Shortform, who are kindly sponsoring today's video. Shortform is a platform that lets you supercharge your reading and gain valuable insights from books. Their unique approach of book guides goes way beyond simple summaries, by providing a comprehensive overview of the material. Not only do you get a concise version of the main points, but you also benefit from related ideas sourced from other books and research papers on the topic. They have an actively growing library of books from all sorts of genres, such as science, health and technology. Not only that, but there is a useful AI-powered browser extension that allows you to generate similar guides for arbitrary content on the internet. Personally, I found Shortform to be really helpful, both when I'm choosing books to read and writing notes and flashcards on the topic. Don't hesitate to bring your reading to the next level by clicking the link down in the description to get 5 dates of unlimited access and 20% off on annual subscription. If you liked the video, share it with your friends, press the like button and subscribe to the channel if you haven't already. Stay tuned for more computational neuroscience and machine learning topics coming up. Goodbye and thank you for the interest in the brain. Thank you for watching and I'll see you in the next video.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 6.0, "text": " Consider the following scenario. You are at a party when you hear a short snippet of your favorite song.", "tokens": [50364, 17416, 264, 3480, 9005, 13, 509, 366, 412, 257, 3595, 562, 291, 1568, 257, 2099, 35623, 302, 295, 428, 2954, 2153, 13, 50664], "temperature": 0.0, "avg_logprob": -0.13063664979572537, "compression_ratio": 1.554054054054054, "no_speech_prob": 0.00029133891803212464}, {"id": 1, "seek": 0, "start": 9.52, "end": 15.36, "text": " Almost instantly, your brain recalls the lyrics of that song and many related memories,", "tokens": [50840, 12627, 13518, 11, 428, 3567, 9901, 82, 264, 12189, 295, 300, 2153, 293, 867, 4077, 8495, 11, 51132], "temperature": 0.0, "avg_logprob": -0.13063664979572537, "compression_ratio": 1.554054054054054, "no_speech_prob": 0.00029133891803212464}, {"id": 2, "seek": 0, "start": 15.92, "end": 19.68, "text": " such as attending a recent concert featuring that artist.", "tokens": [51160, 1270, 382, 15862, 257, 5162, 8543, 19742, 300, 5748, 13, 51348], "temperature": 0.0, "avg_logprob": -0.13063664979572537, "compression_ratio": 1.554054054054054, "no_speech_prob": 0.00029133891803212464}, {"id": 3, "seek": 0, "start": 22.72, "end": 29.44, "text": " It seems very natural and unimpressive. After all, people can recall information all the time.", "tokens": [51500, 467, 2544, 588, 3303, 293, 517, 8814, 22733, 13, 2381, 439, 11, 561, 393, 9901, 1589, 439, 264, 565, 13, 51836], "temperature": 0.0, "avg_logprob": -0.13063664979572537, "compression_ratio": 1.554054054054054, "no_speech_prob": 0.00029133891803212464}, {"id": 4, "seek": 2944, "start": 29.52, "end": 34.4, "text": " However, if you think about it, this problem is computationally non-trivial.", "tokens": [50368, 2908, 11, 498, 291, 519, 466, 309, 11, 341, 1154, 307, 24903, 379, 2107, 12, 83, 470, 22640, 13, 50612], "temperature": 0.0, "avg_logprob": -0.05177521431583098, "compression_ratio": 1.5833333333333333, "no_speech_prob": 0.00011959613038925454}, {"id": 5, "seek": 2944, "start": 35.52, "end": 41.28, "text": " Let's put ourselves in the shoes of evolution and try to come up with an algorithm for the brain", "tokens": [50668, 961, 311, 829, 4175, 294, 264, 6654, 295, 9303, 293, 853, 281, 808, 493, 365, 364, 9284, 337, 264, 3567, 50956], "temperature": 0.0, "avg_logprob": -0.05177521431583098, "compression_ratio": 1.5833333333333333, "no_speech_prob": 0.00011959613038925454}, {"id": 6, "seek": 2944, "start": 41.28, "end": 48.24, "text": " to solve it. The first approach that comes to mind is to actually store some kind of a database", "tokens": [50956, 281, 5039, 309, 13, 440, 700, 3109, 300, 1487, 281, 1575, 307, 281, 767, 3531, 512, 733, 295, 257, 8149, 51304], "temperature": 0.0, "avg_logprob": -0.05177521431583098, "compression_ratio": 1.5833333333333333, "no_speech_prob": 0.00011959613038925454}, {"id": 7, "seek": 2944, "start": 48.24, "end": 54.56, "text": " of all the songs you've heard a sufficient number of times, along with related information,", "tokens": [51304, 295, 439, 264, 5781, 291, 600, 2198, 257, 11563, 1230, 295, 1413, 11, 2051, 365, 4077, 1589, 11, 51620], "temperature": 0.0, "avg_logprob": -0.05177521431583098, "compression_ratio": 1.5833333333333333, "no_speech_prob": 0.00011959613038925454}, {"id": 8, "seek": 5456, "start": 54.56, "end": 60.56, "text": " like the title and lyrics. When an audio fragment of an unknown song is received,", "tokens": [50364, 411, 264, 4876, 293, 12189, 13, 1133, 364, 6278, 26424, 295, 364, 9841, 2153, 307, 4613, 11, 50664], "temperature": 0.0, "avg_logprob": -0.05057897912450584, "compression_ratio": 1.6018099547511313, "no_speech_prob": 0.00043733190977945924}, {"id": 9, "seek": 5456, "start": 60.56, "end": 66.96000000000001, "text": " we can scan through all the songs in our database, find the one that has a close enough match,", "tokens": [50664, 321, 393, 11049, 807, 439, 264, 5781, 294, 527, 8149, 11, 915, 264, 472, 300, 575, 257, 1998, 1547, 2995, 11, 50984], "temperature": 0.0, "avg_logprob": -0.05057897912450584, "compression_ratio": 1.6018099547511313, "no_speech_prob": 0.00043733190977945924}, {"id": 10, "seek": 5456, "start": 66.96000000000001, "end": 73.92, "text": " and retrieve its lyrics. However, the search space of every song I've ever heard", "tokens": [50984, 293, 30254, 1080, 12189, 13, 2908, 11, 264, 3164, 1901, 295, 633, 2153, 286, 600, 1562, 2198, 51332], "temperature": 0.0, "avg_logprob": -0.05057897912450584, "compression_ratio": 1.6018099547511313, "no_speech_prob": 0.00043733190977945924}, {"id": 11, "seek": 5456, "start": 73.92, "end": 80.48, "text": " is astronomically large, and it's even larger when considering every single memory you've formed", "tokens": [51332, 307, 26302, 984, 2416, 11, 293, 309, 311, 754, 4833, 562, 8079, 633, 2167, 4675, 291, 600, 8693, 51660], "temperature": 0.0, "avg_logprob": -0.05057897912450584, "compression_ratio": 1.6018099547511313, "no_speech_prob": 0.00043733190977945924}, {"id": 12, "seek": 8048, "start": 80.48, "end": 85.52000000000001, "text": " since childhood. Performing an exhaustive search would be simply impossible.", "tokens": [50364, 1670, 9278, 13, 19351, 278, 364, 14687, 488, 3164, 576, 312, 2935, 6243, 13, 50616], "temperature": 0.0, "avg_logprob": -0.07429393333724782, "compression_ratio": 1.5103734439834025, "no_speech_prob": 9.170182602247223e-05}, {"id": 13, "seek": 8048, "start": 86.72, "end": 93.44, "text": " Yet, you seem to have no problem instantly recognizing familiar stimuli and finding associations", "tokens": [50676, 10890, 11, 291, 1643, 281, 362, 572, 1154, 13518, 18538, 4963, 47752, 293, 5006, 26597, 51012], "temperature": 0.0, "avg_logprob": -0.07429393333724782, "compression_ratio": 1.5103734439834025, "no_speech_prob": 9.170182602247223e-05}, {"id": 14, "seek": 8048, "start": 93.44, "end": 100.64, "text": " between them. So, how does the brain accomplish this so quickly? In this video, we will lay the", "tokens": [51012, 1296, 552, 13, 407, 11, 577, 775, 264, 3567, 9021, 341, 370, 2661, 30, 682, 341, 960, 11, 321, 486, 2360, 264, 51372], "temperature": 0.0, "avg_logprob": -0.07429393333724782, "compression_ratio": 1.5103734439834025, "no_speech_prob": 9.170182602247223e-05}, {"id": 15, "seek": 8048, "start": 100.64, "end": 106.48, "text": " foundation for a new paradigm of information storage and retrieval, which is more in line with", "tokens": [51372, 7030, 337, 257, 777, 24709, 295, 1589, 6725, 293, 19817, 3337, 11, 597, 307, 544, 294, 1622, 365, 51664], "temperature": 0.0, "avg_logprob": -0.07429393333724782, "compression_ratio": 1.5103734439834025, "no_speech_prob": 9.170182602247223e-05}, {"id": 16, "seek": 10648, "start": 106.56, "end": 114.08, "text": " biology, and actually build one of the simpler models of this process, known as Hopfield Networks,", "tokens": [50368, 14956, 11, 293, 767, 1322, 472, 295, 264, 18587, 5245, 295, 341, 1399, 11, 2570, 382, 13438, 7610, 12640, 82, 11, 50744], "temperature": 0.0, "avg_logprob": -0.13373552958170573, "compression_ratio": 1.3850267379679144, "no_speech_prob": 0.0004305517068132758}, {"id": 17, "seek": 10648, "start": 115.04, "end": 122.32000000000001, "text": " developed by John Hopfield in 1982, who laid an important groundwork for many ideas in both", "tokens": [50792, 4743, 538, 2619, 13438, 7610, 294, 31352, 11, 567, 9897, 364, 1021, 2727, 1902, 337, 867, 3487, 294, 1293, 51156], "temperature": 0.0, "avg_logprob": -0.13373552958170573, "compression_ratio": 1.3850267379679144, "no_speech_prob": 0.0004305517068132758}, {"id": 18, "seek": 10648, "start": 122.32000000000001, "end": 136.32, "text": " neuroscience and machine learning. If you're interested, stay tuned!", "tokens": [51156, 42762, 293, 3479, 2539, 13, 759, 291, 434, 3102, 11, 1754, 10870, 0, 51856], "temperature": 0.0, "avg_logprob": -0.13373552958170573, "compression_ratio": 1.3850267379679144, "no_speech_prob": 0.0004305517068132758}, {"id": 19, "seek": 13648, "start": 137.44, "end": 144.16, "text": " Just to reiterate, we need a way to somehow query what we know and find associations between", "tokens": [50412, 1449, 281, 33528, 11, 321, 643, 257, 636, 281, 6063, 14581, 437, 321, 458, 293, 915, 26597, 1296, 50748], "temperature": 0.0, "avg_logprob": -0.08093136171751385, "compression_ratio": 1.5435684647302905, "no_speech_prob": 0.0010005005169659853}, {"id": 20, "seek": 13648, "start": 144.16, "end": 150.48, "text": " existing memories and new inputs without explicitly checking individual entries for a match,", "tokens": [50748, 6741, 8495, 293, 777, 15743, 1553, 20803, 8568, 2609, 23041, 337, 257, 2995, 11, 51064], "temperature": 0.0, "avg_logprob": -0.08093136171751385, "compression_ratio": 1.5435684647302905, "no_speech_prob": 0.0010005005169659853}, {"id": 21, "seek": 13648, "start": 150.48, "end": 157.44, "text": " which seems like an impossible problem. However, we can draw insights from a seemingly unrelated", "tokens": [51064, 597, 2544, 411, 364, 6243, 1154, 13, 2908, 11, 321, 393, 2642, 14310, 490, 257, 18709, 38967, 51412], "temperature": 0.0, "avg_logprob": -0.08093136171751385, "compression_ratio": 1.5435684647302905, "no_speech_prob": 0.0010005005169659853}, {"id": 22, "seek": 13648, "start": 157.44, "end": 163.83999999999997, "text": " field of molecular biology, and in particular, a concept known as Leventhold's paradox of", "tokens": [51412, 2519, 295, 19046, 14956, 11, 293, 294, 1729, 11, 257, 3410, 2570, 382, 1456, 553, 392, 2641, 311, 26221, 295, 51732], "temperature": 0.0, "avg_logprob": -0.08093136171751385, "compression_ratio": 1.5435684647302905, "no_speech_prob": 0.0010005005169659853}, {"id": 23, "seek": 16384, "start": 163.84, "end": 171.36, "text": " protein folding. As you may know, proteins are long chains of amino acids that fold into specific", "tokens": [50364, 7944, 25335, 13, 1018, 291, 815, 458, 11, 15577, 366, 938, 12626, 295, 24674, 21667, 300, 4860, 666, 2685, 50740], "temperature": 0.0, "avg_logprob": -0.06881981018262032, "compression_ratio": 1.6872246696035242, "no_speech_prob": 0.00034599119680933654}, {"id": 24, "seek": 16384, "start": 171.36, "end": 177.12, "text": " three-dimensional structures which determine their function. The number of possible structural", "tokens": [50740, 1045, 12, 18759, 9227, 597, 6997, 641, 2445, 13, 440, 1230, 295, 1944, 15067, 51028], "temperature": 0.0, "avg_logprob": -0.06881981018262032, "compression_ratio": 1.6872246696035242, "no_speech_prob": 0.00034599119680933654}, {"id": 25, "seek": 16384, "start": 177.12, "end": 183.2, "text": " configurations a protein can take, considering all the different ways you can arrange the atoms", "tokens": [51028, 31493, 257, 7944, 393, 747, 11, 8079, 439, 264, 819, 2098, 291, 393, 9424, 264, 16871, 51332], "temperature": 0.0, "avg_logprob": -0.06881981018262032, "compression_ratio": 1.6872246696035242, "no_speech_prob": 0.00034599119680933654}, {"id": 26, "seek": 16384, "start": 183.2, "end": 189.6, "text": " of an amino acid chain in three-dimensional space, is absolutely enormous. Given the number of", "tokens": [51332, 295, 364, 24674, 8258, 5021, 294, 1045, 12, 18759, 1901, 11, 307, 3122, 11322, 13, 18600, 264, 1230, 295, 51652], "temperature": 0.0, "avg_logprob": -0.06881981018262032, "compression_ratio": 1.6872246696035242, "no_speech_prob": 0.00034599119680933654}, {"id": 27, "seek": 18960, "start": 189.6, "end": 194.88, "text": " possibilities, it seems like it would take an astronomical amount of time for a protein", "tokens": [50364, 12178, 11, 309, 2544, 411, 309, 576, 747, 364, 49035, 2372, 295, 565, 337, 257, 7944, 50628], "temperature": 0.0, "avg_logprob": -0.052534039427594444, "compression_ratio": 1.6651982378854626, "no_speech_prob": 5.307504761731252e-05}, {"id": 28, "seek": 18960, "start": 194.88, "end": 201.84, "text": " to search through all the possible structures to find its correct folded state. In fact, there are", "tokens": [50628, 281, 3164, 807, 439, 264, 1944, 9227, 281, 915, 1080, 3006, 23940, 1785, 13, 682, 1186, 11, 456, 366, 50976], "temperature": 0.0, "avg_logprob": -0.052534039427594444, "compression_ratio": 1.6651982378854626, "no_speech_prob": 5.307504761731252e-05}, {"id": 29, "seek": 18960, "start": 201.84, "end": 208.16, "text": " computations showing that even if the protein samples its different conformations at a nanosecond", "tokens": [50976, 2807, 763, 4099, 300, 754, 498, 264, 7944, 10938, 1080, 819, 18975, 763, 412, 257, 14067, 541, 18882, 51292], "temperature": 0.0, "avg_logprob": -0.052534039427594444, "compression_ratio": 1.6651982378854626, "no_speech_prob": 5.307504761731252e-05}, {"id": 30, "seek": 18960, "start": 208.16, "end": 214.4, "text": " scale, it would still require more time than the age of the universe to arrive at the correct", "tokens": [51292, 4373, 11, 309, 576, 920, 3651, 544, 565, 813, 264, 3205, 295, 264, 6445, 281, 8881, 412, 264, 3006, 51604], "temperature": 0.0, "avg_logprob": -0.052534039427594444, "compression_ratio": 1.6651982378854626, "no_speech_prob": 5.307504761731252e-05}, {"id": 31, "seek": 21440, "start": 214.48000000000002, "end": 220.56, "text": " configuration. Yet, in reality, proteins fold into their native structures in a matter of", "tokens": [50368, 11694, 13, 10890, 11, 294, 4103, 11, 15577, 4860, 666, 641, 8470, 9227, 294, 257, 1871, 295, 50672], "temperature": 0.0, "avg_logprob": -0.07199825710720487, "compression_ratio": 1.5661157024793388, "no_speech_prob": 0.00019110392895527184}, {"id": 32, "seek": 21440, "start": 220.56, "end": 227.12, "text": " milliseconds. So, how do they accomplish this? When I first heard this paradox in high school,", "tokens": [50672, 34184, 13, 407, 11, 577, 360, 436, 9021, 341, 30, 1133, 286, 700, 2198, 341, 26221, 294, 1090, 1395, 11, 51000], "temperature": 0.0, "avg_logprob": -0.07199825710720487, "compression_ratio": 1.5661157024793388, "no_speech_prob": 0.00019110392895527184}, {"id": 33, "seek": 21440, "start": 227.12, "end": 232.8, "text": " it seemed to me like an ill-posed question. After all, the protein molecule is not a computer,", "tokens": [51000, 309, 6576, 281, 385, 411, 364, 3171, 12, 79, 1744, 1168, 13, 2381, 439, 11, 264, 7944, 15582, 307, 406, 257, 3820, 11, 51284], "temperature": 0.0, "avg_logprob": -0.07199825710720487, "compression_ratio": 1.5661157024793388, "no_speech_prob": 0.00019110392895527184}, {"id": 34, "seek": 21440, "start": 232.8, "end": 239.36, "text": " so it doesn't do any sort of search. It just folds into the most stable and favorable configuration", "tokens": [51284, 370, 309, 1177, 380, 360, 604, 1333, 295, 3164, 13, 467, 445, 31341, 666, 264, 881, 8351, 293, 29557, 11694, 51612], "temperature": 0.0, "avg_logprob": -0.07199825710720487, "compression_ratio": 1.5661157024793388, "no_speech_prob": 0.00019110392895527184}, {"id": 35, "seek": 23936, "start": 239.36, "end": 245.52, "text": " according to physical laws. This is similar how when you throw a ball, the ball doesn't", "tokens": [50364, 4650, 281, 4001, 6064, 13, 639, 307, 2531, 577, 562, 291, 3507, 257, 2594, 11, 264, 2594, 1177, 380, 50672], "temperature": 0.0, "avg_logprob": -0.06398039553538863, "compression_ratio": 1.5228215767634854, "no_speech_prob": 0.00027803072589449584}, {"id": 36, "seek": 23936, "start": 245.52, "end": 251.60000000000002, "text": " search through all the possible trajectories to select the optimal parabolic one. It simply", "tokens": [50672, 3164, 807, 439, 264, 1944, 18257, 2083, 281, 3048, 264, 16252, 971, 26956, 472, 13, 467, 2935, 50976], "temperature": 0.0, "avg_logprob": -0.06398039553538863, "compression_ratio": 1.5228215767634854, "no_speech_prob": 0.00027803072589449584}, {"id": 37, "seek": 23936, "start": 251.60000000000002, "end": 258.08000000000004, "text": " follows that path because, well, physics works this way. But how can we think about this folding", "tokens": [50976, 10002, 300, 3100, 570, 11, 731, 11, 10649, 1985, 341, 636, 13, 583, 577, 393, 321, 519, 466, 341, 25335, 51300], "temperature": 0.0, "avg_logprob": -0.06398039553538863, "compression_ratio": 1.5228215767634854, "no_speech_prob": 0.00027803072589449584}, {"id": 38, "seek": 23936, "start": 258.08000000000004, "end": 265.68, "text": " into a favorable configuration? Favorable for what exactly? Let's introduce the concept of", "tokens": [51300, 666, 257, 29557, 11694, 30, 34240, 712, 337, 437, 2293, 30, 961, 311, 5366, 264, 3410, 295, 51680], "temperature": 0.0, "avg_logprob": -0.06398039553538863, "compression_ratio": 1.5228215767634854, "no_speech_prob": 0.00027803072589449584}, {"id": 39, "seek": 26568, "start": 265.68, "end": 271.04, "text": " energy as it will come in handy in future videos as well. If you think back to your high school", "tokens": [50364, 2281, 382, 309, 486, 808, 294, 13239, 294, 2027, 2145, 382, 731, 13, 759, 291, 519, 646, 281, 428, 1090, 1395, 50632], "temperature": 0.0, "avg_logprob": -0.09414970874786377, "compression_ratio": 1.602510460251046, "no_speech_prob": 0.0003920406452380121}, {"id": 40, "seek": 26568, "start": 271.04, "end": 277.28000000000003, "text": " physics days, you may recall something along the lines of energy is a quantitative property that", "tokens": [50632, 10649, 1708, 11, 291, 815, 9901, 746, 2051, 264, 3876, 295, 2281, 307, 257, 27778, 4707, 300, 50944], "temperature": 0.0, "avg_logprob": -0.09414970874786377, "compression_ratio": 1.602510460251046, "no_speech_prob": 0.0003920406452380121}, {"id": 41, "seek": 26568, "start": 277.28000000000003, "end": 284.48, "text": " describes this state of a system, namely the capacity to do work or cause change. Energy can be", "tokens": [50944, 15626, 341, 1785, 295, 257, 1185, 11, 20926, 264, 6042, 281, 360, 589, 420, 3082, 1319, 13, 14939, 393, 312, 51304], "temperature": 0.0, "avg_logprob": -0.09414970874786377, "compression_ratio": 1.602510460251046, "no_speech_prob": 0.0003920406452380121}, {"id": 42, "seek": 26568, "start": 284.48, "end": 290.24, "text": " stored in a variety of different forms, and for the case of proteins, we will be interested in", "tokens": [51304, 12187, 294, 257, 5673, 295, 819, 6422, 11, 293, 337, 264, 1389, 295, 15577, 11, 321, 486, 312, 3102, 294, 51592], "temperature": 0.0, "avg_logprob": -0.09414970874786377, "compression_ratio": 1.602510460251046, "no_speech_prob": 0.0003920406452380121}, {"id": 43, "seek": 29024, "start": 290.24, "end": 295.44, "text": " potential energy stored in the interactions between the atoms in the protein chain.", "tokens": [50364, 3995, 2281, 12187, 294, 264, 13280, 1296, 264, 16871, 294, 264, 7944, 5021, 13, 50624], "temperature": 0.0, "avg_logprob": -0.028409252295622956, "compression_ratio": 1.7073170731707317, "no_speech_prob": 7.254353840835392e-05}, {"id": 44, "seek": 29024, "start": 296.88, "end": 303.04, "text": " Each possible configuration of the protein chain has a specific potential energy level", "tokens": [50696, 6947, 1944, 11694, 295, 264, 7944, 5021, 575, 257, 2685, 3995, 2281, 1496, 51004], "temperature": 0.0, "avg_logprob": -0.028409252295622956, "compression_ratio": 1.7073170731707317, "no_speech_prob": 7.254353840835392e-05}, {"id": 45, "seek": 29024, "start": 303.04, "end": 309.52, "text": " determined by the sum of all of these atomic interactions. In other words, we can assign", "tokens": [51004, 9540, 538, 264, 2408, 295, 439, 295, 613, 22275, 13280, 13, 682, 661, 2283, 11, 321, 393, 6269, 51328], "temperature": 0.0, "avg_logprob": -0.028409252295622956, "compression_ratio": 1.7073170731707317, "no_speech_prob": 7.254353840835392e-05}, {"id": 46, "seek": 29024, "start": 309.52, "end": 315.76, "text": " a positive number to each state equal to its energy, which is a function in some very high", "tokens": [51328, 257, 3353, 1230, 281, 1184, 1785, 2681, 281, 1080, 2281, 11, 597, 307, 257, 2445, 294, 512, 588, 1090, 51640], "temperature": 0.0, "avg_logprob": -0.028409252295622956, "compression_ratio": 1.7073170731707317, "no_speech_prob": 7.254353840835392e-05}, {"id": 47, "seek": 31576, "start": 315.76, "end": 321.59999999999997, "text": " dimensional space where different dimensions correspond to degrees of freedom you need", "tokens": [50364, 18795, 1901, 689, 819, 12819, 6805, 281, 5310, 295, 5645, 291, 643, 50656], "temperature": 0.0, "avg_logprob": -0.0694249240975631, "compression_ratio": 1.6043478260869566, "no_speech_prob": 2.1444933736347593e-05}, {"id": 48, "seek": 31576, "start": 321.59999999999997, "end": 328.32, "text": " to uniquely describe a configuration. For example, all possible dihedral angles of peptide bonds.", "tokens": [50656, 281, 31474, 6786, 257, 11694, 13, 1171, 1365, 11, 439, 1944, 1026, 71, 24764, 14708, 295, 41781, 482, 14713, 13, 50992], "temperature": 0.0, "avg_logprob": -0.0694249240975631, "compression_ratio": 1.6043478260869566, "no_speech_prob": 2.1444933736347593e-05}, {"id": 49, "seek": 31576, "start": 330.56, "end": 336.24, "text": " Let's abstractly visualize it as having just two dimensions. Then the energy function can be", "tokens": [51104, 961, 311, 12649, 356, 23273, 309, 382, 1419, 445, 732, 12819, 13, 1396, 264, 2281, 2445, 393, 312, 51388], "temperature": 0.0, "avg_logprob": -0.0694249240975631, "compression_ratio": 1.6043478260869566, "no_speech_prob": 2.1444933736347593e-05}, {"id": 50, "seek": 31576, "start": 336.24, "end": 342.96, "text": " thought of as a surface where each point on it represents a possible protein configuration,", "tokens": [51388, 1194, 295, 382, 257, 3753, 689, 1184, 935, 322, 309, 8855, 257, 1944, 7944, 11694, 11, 51724], "temperature": 0.0, "avg_logprob": -0.0694249240975631, "compression_ratio": 1.6043478260869566, "no_speech_prob": 2.1444933736347593e-05}, {"id": 51, "seek": 34296, "start": 342.96, "end": 347.59999999999997, "text": " and the height of the point represents the potential energy of that configuration.", "tokens": [50364, 293, 264, 6681, 295, 264, 935, 8855, 264, 3995, 2281, 295, 300, 11694, 13, 50596], "temperature": 0.0, "avg_logprob": -0.08948129699343726, "compression_ratio": 1.58008658008658, "no_speech_prob": 7.411263595713535e-06}, {"id": 52, "seek": 34296, "start": 348.23999999999995, "end": 355.2, "text": " This is what we are going to refer to as energy landscape. For a protein, it would be a complex", "tokens": [50628, 639, 307, 437, 321, 366, 516, 281, 2864, 281, 382, 2281, 9661, 13, 1171, 257, 7944, 11, 309, 576, 312, 257, 3997, 50976], "temperature": 0.0, "avg_logprob": -0.08948129699343726, "compression_ratio": 1.58008658008658, "no_speech_prob": 7.411263595713535e-06}, {"id": 53, "seek": 34296, "start": 355.2, "end": 362.88, "text": " rugged surface with many peaks and valleys. Now, here is the key point. A protein molecule,", "tokens": [50976, 42662, 3753, 365, 867, 26897, 293, 45614, 13, 823, 11, 510, 307, 264, 2141, 935, 13, 316, 7944, 15582, 11, 51360], "temperature": 0.0, "avg_logprob": -0.08948129699343726, "compression_ratio": 1.58008658008658, "no_speech_prob": 7.411263595713535e-06}, {"id": 54, "seek": 34296, "start": 362.88, "end": 369.84, "text": " like any physical system, tends to minimize its potential energy, guided by this second law of", "tokens": [51360, 411, 604, 4001, 1185, 11, 12258, 281, 17522, 1080, 3995, 2281, 11, 19663, 538, 341, 1150, 2101, 295, 51708], "temperature": 0.0, "avg_logprob": -0.08948129699343726, "compression_ratio": 1.58008658008658, "no_speech_prob": 7.411263595713535e-06}, {"id": 55, "seek": 36984, "start": 369.84, "end": 377.2, "text": " thermodynamics. It will naturally seek out the configuration that has the lowest possible energy", "tokens": [50364, 8810, 35483, 13, 467, 486, 8195, 8075, 484, 264, 11694, 300, 575, 264, 12437, 1944, 2281, 50732], "temperature": 0.0, "avg_logprob": -0.08630210634261842, "compression_ratio": 1.528497409326425, "no_speech_prob": 1.4738975551153999e-05}, {"id": 56, "seek": 36984, "start": 377.2, "end": 384.47999999999996, "text": " level, as this represents the optimal arrangement of its atoms. And this, in fact, corresponds to the", "tokens": [50732, 1496, 11, 382, 341, 8855, 264, 16252, 17620, 295, 1080, 16871, 13, 400, 341, 11, 294, 1186, 11, 23249, 281, 264, 51096], "temperature": 0.0, "avg_logprob": -0.08630210634261842, "compression_ratio": 1.528497409326425, "no_speech_prob": 1.4738975551153999e-05}, {"id": 57, "seek": 36984, "start": 384.47999999999996, "end": 392.64, "text": " native, correctly folded state. When a protein is folding, it is essentially rolling downhill on", "tokens": [51096, 8470, 11, 8944, 23940, 1785, 13, 1133, 257, 7944, 307, 25335, 11, 309, 307, 4476, 9439, 29929, 322, 51504], "temperature": 0.0, "avg_logprob": -0.08630210634261842, "compression_ratio": 1.528497409326425, "no_speech_prob": 1.4738975551153999e-05}, {"id": 58, "seek": 39264, "start": 392.64, "end": 399.44, "text": " the energy landscape, following the steepest path towards the valley. This is why proteins can", "tokens": [50364, 264, 2281, 9661, 11, 3480, 264, 16841, 377, 3100, 3030, 264, 17636, 13, 639, 307, 983, 15577, 393, 50704], "temperature": 0.0, "avg_logprob": -0.0582663065766635, "compression_ratio": 1.6415094339622642, "no_speech_prob": 4.683885708800517e-05}, {"id": 59, "seek": 39264, "start": 399.44, "end": 406.8, "text": " fall so quickly. They don't need to search through all possible configurations. They simply follow", "tokens": [50704, 2100, 370, 2661, 13, 814, 500, 380, 643, 281, 3164, 807, 439, 1944, 31493, 13, 814, 2935, 1524, 51072], "temperature": 0.0, "avg_logprob": -0.0582663065766635, "compression_ratio": 1.6415094339622642, "no_speech_prob": 4.683885708800517e-05}, {"id": 60, "seek": 39264, "start": 406.8, "end": 411.68, "text": " the natural tendency of physical systems to minimize their potential energy.", "tokens": [51072, 264, 3303, 18187, 295, 4001, 3652, 281, 17522, 641, 3995, 2281, 13, 51316], "temperature": 0.0, "avg_logprob": -0.0582663065766635, "compression_ratio": 1.6415094339622642, "no_speech_prob": 4.683885708800517e-05}, {"id": 61, "seek": 39264, "start": 412.96, "end": 418.47999999999996, "text": " The protein's folding process is guided by the shape of the energy landscape,", "tokens": [51380, 440, 7944, 311, 25335, 1399, 307, 19663, 538, 264, 3909, 295, 264, 2281, 9661, 11, 51656], "temperature": 0.0, "avg_logprob": -0.0582663065766635, "compression_ratio": 1.6415094339622642, "no_speech_prob": 4.683885708800517e-05}, {"id": 62, "seek": 41848, "start": 418.48, "end": 424.56, "text": " which in turn is determined by the interaction between its atoms. And the descent along the", "tokens": [50364, 597, 294, 1261, 307, 9540, 538, 264, 9285, 1296, 1080, 16871, 13, 400, 264, 23475, 2051, 264, 50668], "temperature": 0.0, "avg_logprob": -0.057130867921853366, "compression_ratio": 1.5872340425531914, "no_speech_prob": 6.748049599991646e-06}, {"id": 63, "seek": 41848, "start": 424.56, "end": 430.32, "text": " surface is essentially driven by the underlying physical process of energy minimization.", "tokens": [50668, 3753, 307, 4476, 9555, 538, 264, 14217, 4001, 1399, 295, 2281, 4464, 2144, 13, 50956], "temperature": 0.0, "avg_logprob": -0.057130867921853366, "compression_ratio": 1.5872340425531914, "no_speech_prob": 6.748049599991646e-06}, {"id": 64, "seek": 41848, "start": 432.56, "end": 439.04, "text": " Now, the core idea is to achieve something similar for the case of associative memory. Suppose we", "tokens": [51068, 823, 11, 264, 4965, 1558, 307, 281, 4584, 746, 2531, 337, 264, 1389, 295, 4180, 1166, 4675, 13, 21360, 321, 51392], "temperature": 0.0, "avg_logprob": -0.057130867921853366, "compression_ratio": 1.5872340425531914, "no_speech_prob": 6.748049599991646e-06}, {"id": 65, "seek": 41848, "start": 439.04, "end": 445.6, "text": " have a system that can encode information in its states, and each configuration has a specific", "tokens": [51392, 362, 257, 1185, 300, 393, 2058, 1429, 1589, 294, 1080, 4368, 11, 293, 1184, 11694, 575, 257, 2685, 51720], "temperature": 0.0, "avg_logprob": -0.057130867921853366, "compression_ratio": 1.5872340425531914, "no_speech_prob": 6.748049599991646e-06}, {"id": 66, "seek": 44560, "start": 445.6, "end": 453.44, "text": " potential energy determined by the interaction between the states. Then we need to first somehow", "tokens": [50364, 3995, 2281, 9540, 538, 264, 9285, 1296, 264, 4368, 13, 1396, 321, 643, 281, 700, 6063, 50756], "temperature": 0.0, "avg_logprob": -0.0938457727432251, "compression_ratio": 1.7194570135746607, "no_speech_prob": 5.144219539943151e-05}, {"id": 67, "seek": 44560, "start": 453.44, "end": 460.56, "text": " sculpt the underlying energy landscape so that memories or state patterns we want to store", "tokens": [50756, 12613, 264, 14217, 2281, 9661, 370, 300, 8495, 420, 1785, 8294, 321, 528, 281, 3531, 51112], "temperature": 0.0, "avg_logprob": -0.0938457727432251, "compression_ratio": 1.7194570135746607, "no_speech_prob": 5.144219539943151e-05}, {"id": 68, "seek": 44560, "start": 460.56, "end": 468.32000000000005, "text": " correspond to local minima, these wells in the energy surface. Second, we need something that", "tokens": [51112, 6805, 281, 2654, 4464, 64, 11, 613, 30984, 294, 264, 2281, 3753, 13, 5736, 11, 321, 643, 746, 300, 51500], "temperature": 0.0, "avg_logprob": -0.0938457727432251, "compression_ratio": 1.7194570135746607, "no_speech_prob": 5.144219539943151e-05}, {"id": 69, "seek": 44560, "start": 468.32000000000005, "end": 474.8, "text": " would play the role of the second law of thermodynamics and would drive the changes in the states,", "tokens": [51500, 576, 862, 264, 3090, 295, 264, 1150, 2101, 295, 8810, 35483, 293, 576, 3332, 264, 2962, 294, 264, 4368, 11, 51824], "temperature": 0.0, "avg_logprob": -0.0938457727432251, "compression_ratio": 1.7194570135746607, "no_speech_prob": 5.144219539943151e-05}, {"id": 70, "seek": 47480, "start": 474.8, "end": 482.16, "text": " directing the system towards the nearest local minimum. Once these two things are achieved,", "tokens": [50364, 26979, 264, 1185, 3030, 264, 23831, 2654, 7285, 13, 3443, 613, 732, 721, 366, 11042, 11, 50732], "temperature": 0.0, "avg_logprob": -0.06085431298544241, "compression_ratio": 1.6784140969162995, "no_speech_prob": 1.8058495697914623e-05}, {"id": 71, "seek": 47480, "start": 482.16, "end": 488.88, "text": " retrieving a memory that is most similar to the input pattern is done by configuring the system", "tokens": [50732, 19817, 798, 257, 4675, 300, 307, 881, 2531, 281, 264, 4846, 5102, 307, 1096, 538, 6662, 1345, 264, 1185, 51068], "temperature": 0.0, "avg_logprob": -0.06085431298544241, "compression_ratio": 1.6784140969162995, "no_speech_prob": 1.8058495697914623e-05}, {"id": 72, "seek": 47480, "start": 488.88, "end": 495.52, "text": " to encode the input pattern initially and letting it run to the equilibrium, descending into the", "tokens": [51068, 281, 2058, 1429, 264, 4846, 5102, 9105, 293, 8295, 309, 1190, 281, 264, 15625, 11, 40182, 666, 264, 51400], "temperature": 0.0, "avg_logprob": -0.06085431298544241, "compression_ratio": 1.6784140969162995, "no_speech_prob": 1.8058495697914623e-05}, {"id": 73, "seek": 47480, "start": 495.52, "end": 504.0, "text": " energy well, from which we can read out the source memory. Sounds neat, right? So let's get into", "tokens": [51400, 2281, 731, 11, 490, 597, 321, 393, 1401, 484, 264, 4009, 4675, 13, 14576, 10654, 11, 558, 30, 407, 718, 311, 483, 666, 51824], "temperature": 0.0, "avg_logprob": -0.06085431298544241, "compression_ratio": 1.6784140969162995, "no_speech_prob": 1.8058495697914623e-05}, {"id": 74, "seek": 50400, "start": 504.0, "end": 511.12, "text": " building it. Let's consider a set of neurons which we can think of as abstract units that can", "tokens": [50364, 2390, 309, 13, 961, 311, 1949, 257, 992, 295, 22027, 597, 321, 393, 519, 295, 382, 12649, 6815, 300, 393, 50720], "temperature": 0.0, "avg_logprob": -0.059462530859585465, "compression_ratio": 1.5726141078838174, "no_speech_prob": 0.00012148156383773312}, {"id": 75, "seek": 50400, "start": 511.12, "end": 518.16, "text": " be in one of two possible states, plus one or minus one. This is a simplified analogy of how nerve", "tokens": [50720, 312, 294, 472, 295, 732, 1944, 4368, 11, 1804, 472, 420, 3175, 472, 13, 639, 307, 257, 26335, 21663, 295, 577, 16355, 51072], "temperature": 0.0, "avg_logprob": -0.059462530859585465, "compression_ratio": 1.5726141078838174, "no_speech_prob": 0.00012148156383773312}, {"id": 76, "seek": 50400, "start": 518.16, "end": 523.44, "text": " cells in the brain encode information through patterns of firing. They either generate an", "tokens": [51072, 5438, 294, 264, 3567, 2058, 1429, 1589, 807, 8294, 295, 16045, 13, 814, 2139, 8460, 364, 51336], "temperature": 0.0, "avg_logprob": -0.059462530859585465, "compression_ratio": 1.5726141078838174, "no_speech_prob": 0.00012148156383773312}, {"id": 77, "seek": 50400, "start": 523.44, "end": 530.4, "text": " electrical impulse at a given point in time or remain silent. We'll focus on the fully connected", "tokens": [51336, 12147, 26857, 412, 257, 2212, 935, 294, 565, 420, 6222, 12784, 13, 492, 603, 1879, 322, 264, 4498, 4582, 51684], "temperature": 0.0, "avg_logprob": -0.059462530859585465, "compression_ratio": 1.5726141078838174, "no_speech_prob": 0.00012148156383773312}, {"id": 78, "seek": 53040, "start": 530.4, "end": 537.84, "text": " network where each neuron has connections to every other neuron. These connections have weights", "tokens": [50364, 3209, 689, 1184, 34090, 575, 9271, 281, 633, 661, 34090, 13, 1981, 9271, 362, 17443, 50736], "temperature": 0.0, "avg_logprob": -0.06363933881123861, "compression_ratio": 1.6457142857142857, "no_speech_prob": 3.5356220905669034e-05}, {"id": 79, "seek": 53040, "start": 537.84, "end": 544.8, "text": " associated with them, real numbers that signify the strength of coupling between the corresponding", "tokens": [50736, 6615, 365, 552, 11, 957, 3547, 300, 1465, 2505, 264, 3800, 295, 37447, 1296, 264, 11760, 51084], "temperature": 0.0, "avg_logprob": -0.06363933881123861, "compression_ratio": 1.6457142857142857, "no_speech_prob": 3.5356220905669034e-05}, {"id": 80, "seek": 53040, "start": 544.8, "end": 551.92, "text": " pair of neurons. For a pair of units i and j, we denote the connection weight between them as", "tokens": [51084, 6119, 295, 22027, 13, 1171, 257, 6119, 295, 6815, 741, 293, 361, 11, 321, 45708, 264, 4984, 3364, 1296, 552, 382, 51440], "temperature": 0.0, "avg_logprob": -0.06363933881123861, "compression_ratio": 1.6457142857142857, "no_speech_prob": 3.5356220905669034e-05}, {"id": 81, "seek": 55192, "start": 551.92, "end": 560.7199999999999, "text": " wij and the states of neurons themselves as xi and xj. In the brain, connections between neurons", "tokens": [50364, 261, 1718, 293, 264, 4368, 295, 22027, 2969, 382, 36800, 293, 2031, 73, 13, 682, 264, 3567, 11, 9271, 1296, 22027, 50804], "temperature": 0.0, "avg_logprob": -0.10352813436629925, "compression_ratio": 1.7264573991031391, "no_speech_prob": 0.00010229978215647861}, {"id": 82, "seek": 55192, "start": 560.7199999999999, "end": 567.76, "text": " or synapses have a well-defined direction. A pair of neurons is connected asymmetrically,", "tokens": [50804, 420, 5451, 2382, 279, 362, 257, 731, 12, 37716, 3513, 13, 316, 6119, 295, 22027, 307, 4582, 37277, 27965, 984, 11, 51156], "temperature": 0.0, "avg_logprob": -0.10352813436629925, "compression_ratio": 1.7264573991031391, "no_speech_prob": 0.00010229978215647861}, {"id": 83, "seek": 55192, "start": 567.76, "end": 574.24, "text": " meaning that the synapse from neuron A to neuron B is physiologically separate from the synapse that", "tokens": [51156, 3620, 300, 264, 5451, 11145, 490, 34090, 316, 281, 34090, 363, 307, 21265, 17157, 4994, 490, 264, 5451, 11145, 300, 51480], "temperature": 0.0, "avg_logprob": -0.10352813436629925, "compression_ratio": 1.7264573991031391, "no_speech_prob": 0.00010229978215647861}, {"id": 84, "seek": 55192, "start": 574.24, "end": 581.12, "text": " connects B to A if that one exists at all, and so they can have different weights. While we could", "tokens": [51480, 16967, 363, 281, 316, 498, 300, 472, 8198, 412, 439, 11, 293, 370, 436, 393, 362, 819, 17443, 13, 3987, 321, 727, 51824], "temperature": 0.0, "avg_logprob": -0.10352813436629925, "compression_ratio": 1.7264573991031391, "no_speech_prob": 0.00010229978215647861}, {"id": 85, "seek": 58112, "start": 581.12, "end": 586.5600000000001, "text": " generalize a Hopfield network to account for asymmetric connections, it would introduce", "tokens": [50364, 2674, 1125, 257, 13438, 7610, 3209, 281, 2696, 337, 37277, 17475, 9271, 11, 309, 576, 5366, 50636], "temperature": 0.0, "avg_logprob": -0.06069599257575141, "compression_ratio": 1.6208530805687205, "no_speech_prob": 3.591292988858186e-05}, {"id": 86, "seek": 58112, "start": 586.5600000000001, "end": 593.36, "text": " complications and potentially unstable behavior. For simplicity, here we will stick to the original", "tokens": [50636, 26566, 293, 7263, 23742, 5223, 13, 1171, 25632, 11, 510, 321, 486, 2897, 281, 264, 3380, 50976], "temperature": 0.0, "avg_logprob": -0.06069599257575141, "compression_ratio": 1.6208530805687205, "no_speech_prob": 3.591292988858186e-05}, {"id": 87, "seek": 58112, "start": 593.36, "end": 599.44, "text": " formulation of the Hopfield network, which assumes symmetric weights. In other words,", "tokens": [50976, 37642, 295, 264, 13438, 7610, 3209, 11, 597, 37808, 32330, 17443, 13, 682, 661, 2283, 11, 51280], "temperature": 0.0, "avg_logprob": -0.06069599257575141, "compression_ratio": 1.6208530805687205, "no_speech_prob": 3.591292988858186e-05}, {"id": 88, "seek": 58112, "start": 599.44, "end": 604.08, "text": " neurons i and j are connected by the same weight in both directions.", "tokens": [51280, 22027, 741, 293, 361, 366, 4582, 538, 264, 912, 3364, 294, 1293, 11095, 13, 51512], "temperature": 0.0, "avg_logprob": -0.06069599257575141, "compression_ratio": 1.6208530805687205, "no_speech_prob": 3.591292988858186e-05}, {"id": 89, "seek": 60408, "start": 604.5600000000001, "end": 611.36, "text": " Now that we have a set of neurons symmetrically linked with each other through weighted connections,", "tokens": [50388, 823, 300, 321, 362, 257, 992, 295, 22027, 14232, 27965, 984, 9408, 365, 1184, 661, 807, 32807, 9271, 11, 50728], "temperature": 0.0, "avg_logprob": -0.11866517714512201, "compression_ratio": 1.6681818181818182, "no_speech_prob": 1.777854959073011e-05}, {"id": 90, "seek": 60408, "start": 611.9200000000001, "end": 618.72, "text": " let's explore what these connection weights represent. If wij is greater than zero,", "tokens": [50756, 718, 311, 6839, 437, 613, 4984, 17443, 2906, 13, 759, 261, 1718, 307, 5044, 813, 4018, 11, 51096], "temperature": 0.0, "avg_logprob": -0.11866517714512201, "compression_ratio": 1.6681818181818182, "no_speech_prob": 1.777854959073011e-05}, {"id": 91, "seek": 60408, "start": 618.72, "end": 624.48, "text": " the connection is said to be excitatory and favors the alignment between the states of", "tokens": [51096, 264, 4984, 307, 848, 281, 312, 13101, 4745, 293, 40554, 264, 18515, 1296, 264, 4368, 295, 51384], "temperature": 0.0, "avg_logprob": -0.11866517714512201, "compression_ratio": 1.6681818181818182, "no_speech_prob": 1.777854959073011e-05}, {"id": 92, "seek": 60408, "start": 624.48, "end": 630.96, "text": " two neurons. We can think of each connection as being either happy or unhappy, depending on the", "tokens": [51384, 732, 22027, 13, 492, 393, 519, 295, 1184, 4984, 382, 885, 2139, 2055, 420, 22172, 11, 5413, 322, 264, 51708], "temperature": 0.0, "avg_logprob": -0.11866517714512201, "compression_ratio": 1.6681818181818182, "no_speech_prob": 1.777854959073011e-05}, {"id": 93, "seek": 63096, "start": 630.96, "end": 638.8000000000001, "text": " states of its neurons. For example, if wij is a large positive number, it means that neurons i", "tokens": [50364, 4368, 295, 1080, 22027, 13, 1171, 1365, 11, 498, 261, 1718, 307, 257, 2416, 3353, 1230, 11, 309, 1355, 300, 22027, 741, 50756], "temperature": 0.0, "avg_logprob": -0.0674041449421584, "compression_ratio": 1.731818181818182, "no_speech_prob": 0.00018522552272770554}, {"id": 94, "seek": 63096, "start": 638.8000000000001, "end": 645.12, "text": " and j are closely coupled, and one excites the other. In this case, when one neuron is active,", "tokens": [50756, 293, 361, 366, 8185, 29482, 11, 293, 472, 1624, 3324, 264, 661, 13, 682, 341, 1389, 11, 562, 472, 34090, 307, 4967, 11, 51072], "temperature": 0.0, "avg_logprob": -0.0674041449421584, "compression_ratio": 1.731818181818182, "no_speech_prob": 0.00018522552272770554}, {"id": 95, "seek": 63096, "start": 645.12, "end": 650.88, "text": " the other tends to be active as well, and when one is silent, the other one is more likely to be silent.", "tokens": [51072, 264, 661, 12258, 281, 312, 4967, 382, 731, 11, 293, 562, 472, 307, 12784, 11, 264, 661, 472, 307, 544, 3700, 281, 312, 12784, 13, 51360], "temperature": 0.0, "avg_logprob": -0.0674041449421584, "compression_ratio": 1.731818181818182, "no_speech_prob": 0.00018522552272770554}, {"id": 96, "seek": 63096, "start": 652.8000000000001, "end": 659.36, "text": " These configurations, where both xi and xj are either one or minus one, agree with the", "tokens": [51456, 1981, 31493, 11, 689, 1293, 36800, 293, 2031, 73, 366, 2139, 472, 420, 3175, 472, 11, 3986, 365, 264, 51784], "temperature": 0.0, "avg_logprob": -0.0674041449421584, "compression_ratio": 1.731818181818182, "no_speech_prob": 0.00018522552272770554}, {"id": 97, "seek": 65936, "start": 659.36, "end": 665.92, "text": " connection weight. However, if we observe, for example, that xi is equal to one and xj is equal", "tokens": [50364, 4984, 3364, 13, 2908, 11, 498, 321, 11441, 11, 337, 1365, 11, 300, 36800, 307, 2681, 281, 472, 293, 2031, 73, 307, 2681, 50692], "temperature": 0.0, "avg_logprob": -0.044104168812433876, "compression_ratio": 1.5595854922279793, "no_speech_prob": 5.649787271977402e-05}, {"id": 98, "seek": 65936, "start": 665.92, "end": 673.04, "text": " to minus one, it conflicts with the excitatory nature of the connection, making such a configuration", "tokens": [50692, 281, 3175, 472, 11, 309, 19807, 365, 264, 13101, 4745, 3687, 295, 264, 4984, 11, 1455, 1270, 257, 11694, 51048], "temperature": 0.0, "avg_logprob": -0.044104168812433876, "compression_ratio": 1.5595854922279793, "no_speech_prob": 5.649787271977402e-05}, {"id": 99, "seek": 65936, "start": 673.04, "end": 681.04, "text": " less likely. Conversely, when wij is negative, the connection promotes misalignment between the weights.", "tokens": [51048, 1570, 3700, 13, 33247, 736, 11, 562, 261, 1718, 307, 3671, 11, 264, 4984, 36015, 3346, 304, 41134, 1296, 264, 17443, 13, 51448], "temperature": 0.0, "avg_logprob": -0.044104168812433876, "compression_ratio": 1.5595854922279793, "no_speech_prob": 5.649787271977402e-05}, {"id": 100, "seek": 68104, "start": 681.04, "end": 690.64, "text": " This alignment between the signs can be expressed more concisely using the product xi times xj.", "tokens": [50364, 639, 18515, 1296, 264, 7880, 393, 312, 12675, 544, 1588, 271, 736, 1228, 264, 1674, 36800, 1413, 2031, 73, 13, 50844], "temperature": 0.0, "avg_logprob": -0.13367064793904623, "compression_ratio": 1.55, "no_speech_prob": 0.00018814181385096163}, {"id": 101, "seek": 68104, "start": 691.36, "end": 697.4399999999999, "text": " This product will be positive when both neurons have the same sign and negative when they have", "tokens": [50880, 639, 1674, 486, 312, 3353, 562, 1293, 22027, 362, 264, 912, 1465, 293, 3671, 562, 436, 362, 51184], "temperature": 0.0, "avg_logprob": -0.13367064793904623, "compression_ratio": 1.55, "no_speech_prob": 0.00018814181385096163}, {"id": 102, "seek": 68104, "start": 697.4399999999999, "end": 704.3199999999999, "text": " different signs. By multiplying this product further by the connection weight, we obtain", "tokens": [51184, 819, 7880, 13, 3146, 30955, 341, 1674, 3052, 538, 264, 4984, 3364, 11, 321, 12701, 51528], "temperature": 0.0, "avg_logprob": -0.13367064793904623, "compression_ratio": 1.55, "no_speech_prob": 0.00018814181385096163}, {"id": 103, "seek": 70432, "start": 704.32, "end": 711.84, "text": " an expression for the happiness of that connection. For a positive wij, happiness will be positive", "tokens": [50364, 364, 6114, 337, 264, 8324, 295, 300, 4984, 13, 1171, 257, 3353, 261, 1718, 11, 8324, 486, 312, 3353, 50740], "temperature": 0.0, "avg_logprob": -0.04137092690135157, "compression_ratio": 1.706140350877193, "no_speech_prob": 0.00028685375582426786}, {"id": 104, "seek": 70432, "start": 711.84, "end": 719.12, "text": " when the product of the two states is positive. But this is just one edge. We can extend this idea", "tokens": [50740, 562, 264, 1674, 295, 264, 732, 4368, 307, 3353, 13, 583, 341, 307, 445, 472, 4691, 13, 492, 393, 10101, 341, 1558, 51104], "temperature": 0.0, "avg_logprob": -0.04137092690135157, "compression_ratio": 1.706140350877193, "no_speech_prob": 0.00028685375582426786}, {"id": 105, "seek": 70432, "start": 719.12, "end": 725.84, "text": " and compute the happiness of the entire network as a whole by summing this quantity across all edges.", "tokens": [51104, 293, 14722, 264, 8324, 295, 264, 2302, 3209, 382, 257, 1379, 538, 2408, 2810, 341, 11275, 2108, 439, 8819, 13, 51440], "temperature": 0.0, "avg_logprob": -0.04137092690135157, "compression_ratio": 1.706140350877193, "no_speech_prob": 0.00028685375582426786}, {"id": 106, "seek": 70432, "start": 727.0400000000001, "end": 732.8800000000001, "text": " The larger that number is, the more overall agreement there is between connection weights", "tokens": [51500, 440, 4833, 300, 1230, 307, 11, 264, 544, 4787, 8106, 456, 307, 1296, 4984, 17443, 51792], "temperature": 0.0, "avg_logprob": -0.04137092690135157, "compression_ratio": 1.706140350877193, "no_speech_prob": 0.00028685375582426786}, {"id": 107, "seek": 73288, "start": 732.88, "end": 739.76, "text": " and pairwise states of neurons. Ultimately, we will search for a set of weights that maximize", "tokens": [50364, 293, 6119, 3711, 4368, 295, 22027, 13, 23921, 11, 321, 486, 3164, 337, 257, 992, 295, 17443, 300, 19874, 50708], "temperature": 0.0, "avg_logprob": -0.049201286756075345, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.0002531557984184474}, {"id": 108, "seek": 73288, "start": 739.76, "end": 746.72, "text": " this quantity. And maximizing happiness is equivalent to minimizing it with a minus sign,", "tokens": [50708, 341, 11275, 13, 400, 5138, 3319, 8324, 307, 10344, 281, 46608, 309, 365, 257, 3175, 1465, 11, 51056], "temperature": 0.0, "avg_logprob": -0.049201286756075345, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.0002531557984184474}, {"id": 109, "seek": 73288, "start": 746.72, "end": 752.0, "text": " which you can think of as the measure of overall conflict between the actual configuration of", "tokens": [51056, 597, 291, 393, 519, 295, 382, 264, 3481, 295, 4787, 6596, 1296, 264, 3539, 11694, 295, 51320], "temperature": 0.0, "avg_logprob": -0.049201286756075345, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.0002531557984184474}, {"id": 110, "seek": 73288, "start": 752.0, "end": 759.2, "text": " states and what's favored by the connection weights. This total conflict between the weights", "tokens": [51320, 4368, 293, 437, 311, 44420, 538, 264, 4984, 17443, 13, 639, 3217, 6596, 1296, 264, 17443, 51680], "temperature": 0.0, "avg_logprob": -0.049201286756075345, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.0002531557984184474}, {"id": 111, "seek": 75920, "start": 759.2, "end": 765.36, "text": " and pairwise states is exactly what we are going to define to be the energy of the system.", "tokens": [50364, 293, 6119, 3711, 4368, 307, 2293, 437, 321, 366, 516, 281, 6964, 281, 312, 264, 2281, 295, 264, 1185, 13, 50672], "temperature": 0.0, "avg_logprob": -0.07752905421786838, "compression_ratio": 1.6391304347826088, "no_speech_prob": 0.00020988270989619195}, {"id": 112, "seek": 75920, "start": 766.8000000000001, "end": 773.44, "text": " As we discussed previously, we want the Hopfield network to be able to gradually evolve towards", "tokens": [50744, 1018, 321, 7152, 8046, 11, 321, 528, 264, 13438, 7610, 3209, 281, 312, 1075, 281, 13145, 16693, 3030, 51076], "temperature": 0.0, "avg_logprob": -0.07752905421786838, "compression_ratio": 1.6391304347826088, "no_speech_prob": 0.00020988270989619195}, {"id": 113, "seek": 75920, "start": 773.44, "end": 780.24, "text": " energy minima. But looking closely at the formula, we can see that the energy value depends both on", "tokens": [51076, 2281, 4464, 64, 13, 583, 1237, 8185, 412, 264, 8513, 11, 321, 393, 536, 300, 264, 2281, 2158, 5946, 1293, 322, 51416], "temperature": 0.0, "avg_logprob": -0.07752905421786838, "compression_ratio": 1.6391304347826088, "no_speech_prob": 0.00020988270989619195}, {"id": 114, "seek": 75920, "start": 780.24, "end": 785.84, "text": " the states and the weights. So there is a lot of things the system can tweak to change it.", "tokens": [51416, 264, 4368, 293, 264, 17443, 13, 407, 456, 307, 257, 688, 295, 721, 264, 1185, 393, 29879, 281, 1319, 309, 13, 51696], "temperature": 0.0, "avg_logprob": -0.07752905421786838, "compression_ratio": 1.6391304347826088, "no_speech_prob": 0.00020988270989619195}, {"id": 115, "seek": 78584, "start": 785.84, "end": 791.9200000000001, "text": " What exactly is getting adjusted? As we will see further, there are essentially", "tokens": [50364, 708, 2293, 307, 1242, 19871, 30, 1018, 321, 486, 536, 3052, 11, 456, 366, 4476, 50668], "temperature": 0.0, "avg_logprob": -0.06342804921816474, "compression_ratio": 1.5422222222222222, "no_speech_prob": 8.267870725831017e-06}, {"id": 116, "seek": 78584, "start": 791.9200000000001, "end": 798.1600000000001, "text": " two modes of network updates that nicely map to the two aspects of associative memory.", "tokens": [50668, 732, 14068, 295, 3209, 9205, 300, 9594, 4471, 281, 264, 732, 7270, 295, 4180, 1166, 4675, 13, 50980], "temperature": 0.0, "avg_logprob": -0.06342804921816474, "compression_ratio": 1.5422222222222222, "no_speech_prob": 8.267870725831017e-06}, {"id": 117, "seek": 78584, "start": 799.2800000000001, "end": 806.08, "text": " Namely, adjusting the weights corresponds to shaping the energy landscape, defining", "tokens": [51036, 10684, 736, 11, 23559, 264, 17443, 23249, 281, 25945, 264, 2281, 9661, 11, 17827, 51376], "temperature": 0.0, "avg_logprob": -0.06342804921816474, "compression_ratio": 1.5422222222222222, "no_speech_prob": 8.267870725831017e-06}, {"id": 118, "seek": 78584, "start": 806.08, "end": 812.88, "text": " which configurations are stable by digging energy wells around them. This is the act of learning", "tokens": [51376, 597, 31493, 366, 8351, 538, 17343, 2281, 30984, 926, 552, 13, 639, 307, 264, 605, 295, 2539, 51716], "temperature": 0.0, "avg_logprob": -0.06342804921816474, "compression_ratio": 1.5422222222222222, "no_speech_prob": 8.267870725831017e-06}, {"id": 119, "seek": 81288, "start": 812.88, "end": 818.08, "text": " when we are writing new memories into the network. Once the weights are fixed,", "tokens": [50364, 562, 321, 366, 3579, 777, 8495, 666, 264, 3209, 13, 3443, 264, 17443, 366, 6806, 11, 50624], "temperature": 0.0, "avg_logprob": -0.0493317200587346, "compression_ratio": 1.7163461538461537, "no_speech_prob": 3.0241913918871433e-05}, {"id": 120, "seek": 81288, "start": 818.8, "end": 824.96, "text": " tweaking the states of neurons to bring them into greater agreement with the weights", "tokens": [50660, 6986, 2456, 264, 4368, 295, 22027, 281, 1565, 552, 666, 5044, 8106, 365, 264, 17443, 50968], "temperature": 0.0, "avg_logprob": -0.0493317200587346, "compression_ratio": 1.7163461538461537, "no_speech_prob": 3.0241913918871433e-05}, {"id": 121, "seek": 81288, "start": 824.96, "end": 832.32, "text": " corresponds to descending along the energy surface. This is the act of inference when we are", "tokens": [50968, 23249, 281, 40182, 2051, 264, 2281, 3753, 13, 639, 307, 264, 605, 295, 38253, 562, 321, 366, 51336], "temperature": 0.0, "avg_logprob": -0.0493317200587346, "compression_ratio": 1.7163461538461537, "no_speech_prob": 3.0241913918871433e-05}, {"id": 122, "seek": 81288, "start": 832.32, "end": 838.64, "text": " recalling the memory that is at the bottom of the energy well, which is nearest to the configuration", "tokens": [51336, 9901, 278, 264, 4675, 300, 307, 412, 264, 2767, 295, 264, 2281, 731, 11, 597, 307, 23831, 281, 264, 11694, 51652], "temperature": 0.0, "avg_logprob": -0.0493317200587346, "compression_ratio": 1.7163461538461537, "no_speech_prob": 3.0241913918871433e-05}, {"id": 123, "seek": 83864, "start": 838.64, "end": 842.3199999999999, "text": " of the input pattern. Let's take a look at inference first.", "tokens": [50364, 295, 264, 4846, 5102, 13, 961, 311, 747, 257, 574, 412, 38253, 700, 13, 50548], "temperature": 0.0, "avg_logprob": -0.099464718299576, "compression_ratio": 1.591743119266055, "no_speech_prob": 0.00010554678738117218}, {"id": 124, "seek": 83864, "start": 845.1999999999999, "end": 851.36, "text": " Suppose for a second, someone has already set the weights W and hence us the backbone of the", "tokens": [50692, 21360, 337, 257, 1150, 11, 1580, 575, 1217, 992, 264, 17443, 343, 293, 16678, 505, 264, 34889, 295, 264, 51000], "temperature": 0.0, "avg_logprob": -0.099464718299576, "compression_ratio": 1.591743119266055, "no_speech_prob": 0.00010554678738117218}, {"id": 125, "seek": 83864, "start": 851.36, "end": 857.92, "text": " network. The neurons themselves with all the connection weights. However, the exact configuration", "tokens": [51000, 3209, 13, 440, 22027, 2969, 365, 439, 264, 4984, 17443, 13, 2908, 11, 264, 1900, 11694, 51328], "temperature": 0.0, "avg_logprob": -0.099464718299576, "compression_ratio": 1.591743119266055, "no_speech_prob": 0.00010554678738117218}, {"id": 126, "seek": 83864, "start": 857.92, "end": 864.56, "text": " of states, which neurons are active and which are silent, is unknown. The question then becomes,", "tokens": [51328, 295, 4368, 11, 597, 22027, 366, 4967, 293, 597, 366, 12784, 11, 307, 9841, 13, 440, 1168, 550, 3643, 11, 51660], "temperature": 0.0, "avg_logprob": -0.099464718299576, "compression_ratio": 1.591743119266055, "no_speech_prob": 0.00010554678738117218}, {"id": 127, "seek": 86456, "start": 864.56, "end": 869.52, "text": " how do we find the state pattern that would minimize the total energy?", "tokens": [50364, 577, 360, 321, 915, 264, 1785, 5102, 300, 576, 17522, 264, 3217, 2281, 30, 50612], "temperature": 0.0, "avg_logprob": -0.07503091096878052, "compression_ratio": 1.5954545454545455, "no_speech_prob": 4.264743984094821e-05}, {"id": 128, "seek": 86456, "start": 870.4, "end": 877.3599999999999, "text": " As we discussed, simply checking all possible states is not an option. So, we will start with", "tokens": [50656, 1018, 321, 7152, 11, 2935, 8568, 439, 1944, 4368, 307, 406, 364, 3614, 13, 407, 11, 321, 486, 722, 365, 51004], "temperature": 0.0, "avg_logprob": -0.07503091096878052, "compression_ratio": 1.5954545454545455, "no_speech_prob": 4.264743984094821e-05}, {"id": 129, "seek": 86456, "start": 877.3599999999999, "end": 883.5999999999999, "text": " some initial state, which could be either a partial or a noisy version of one of the memories", "tokens": [51004, 512, 5883, 1785, 11, 597, 727, 312, 2139, 257, 14641, 420, 257, 24518, 3037, 295, 472, 295, 264, 8495, 51316], "temperature": 0.0, "avg_logprob": -0.07503091096878052, "compression_ratio": 1.5954545454545455, "no_speech_prob": 4.264743984094821e-05}, {"id": 130, "seek": 86456, "start": 883.5999999999999, "end": 891.3599999999999, "text": " or a random configuration altogether. Once the initial condition is set, we will iteratively", "tokens": [51316, 420, 257, 4974, 11694, 19051, 13, 3443, 264, 5883, 4188, 307, 992, 11, 321, 486, 17138, 19020, 51704], "temperature": 0.0, "avg_logprob": -0.07503091096878052, "compression_ratio": 1.5954545454545455, "no_speech_prob": 4.264743984094821e-05}, {"id": 131, "seek": 89136, "start": 891.36, "end": 898.0, "text": " try to lower the energy value by focusing on updating one neuron at a time. Let's denote", "tokens": [50364, 853, 281, 3126, 264, 2281, 2158, 538, 8416, 322, 25113, 472, 34090, 412, 257, 565, 13, 961, 311, 45708, 50696], "temperature": 0.0, "avg_logprob": -0.07378159567367198, "compression_ratio": 1.6726457399103138, "no_speech_prob": 4.985955456504598e-05}, {"id": 132, "seek": 89136, "start": 898.0, "end": 904.48, "text": " the neuron we are currently considering as neuron i. We will calculate the total weighted input to", "tokens": [50696, 264, 34090, 321, 366, 4362, 8079, 382, 34090, 741, 13, 492, 486, 8873, 264, 3217, 32807, 4846, 281, 51020], "temperature": 0.0, "avg_logprob": -0.07378159567367198, "compression_ratio": 1.6726457399103138, "no_speech_prob": 4.985955456504598e-05}, {"id": 133, "seek": 89136, "start": 904.48, "end": 911.92, "text": " it from all other neurons in the network. This input, which we'll denote as hi, is the sum of", "tokens": [51020, 309, 490, 439, 661, 22027, 294, 264, 3209, 13, 639, 4846, 11, 597, 321, 603, 45708, 382, 4879, 11, 307, 264, 2408, 295, 51392], "temperature": 0.0, "avg_logprob": -0.07378159567367198, "compression_ratio": 1.6726457399103138, "no_speech_prob": 4.985955456504598e-05}, {"id": 134, "seek": 89136, "start": 911.92, "end": 918.8000000000001, "text": " the states of all other neurons multiplied by their respective connection weights. If hi is", "tokens": [51392, 264, 4368, 295, 439, 661, 22027, 17207, 538, 641, 23649, 4984, 17443, 13, 759, 4879, 307, 51736], "temperature": 0.0, "avg_logprob": -0.07378159567367198, "compression_ratio": 1.6726457399103138, "no_speech_prob": 4.985955456504598e-05}, {"id": 135, "seek": 91880, "start": 918.8, "end": 925.8399999999999, "text": " positive, it means that the weighted sum of the other neuron states is in favor of neuron i being", "tokens": [50364, 3353, 11, 309, 1355, 300, 264, 32807, 2408, 295, 264, 661, 34090, 4368, 307, 294, 2294, 295, 34090, 741, 885, 50716], "temperature": 0.0, "avg_logprob": -0.060031690905171055, "compression_ratio": 1.8055555555555556, "no_speech_prob": 0.00021654393640346825}, {"id": 136, "seek": 91880, "start": 925.8399999999999, "end": 933.76, "text": " in the plus one state. Conversely, if hi is negative, it suggests that neuron i should be in the minus", "tokens": [50716, 294, 264, 1804, 472, 1785, 13, 33247, 736, 11, 498, 4879, 307, 3671, 11, 309, 13409, 300, 34090, 741, 820, 312, 294, 264, 3175, 51112], "temperature": 0.0, "avg_logprob": -0.060031690905171055, "compression_ratio": 1.8055555555555556, "no_speech_prob": 0.00021654393640346825}, {"id": 137, "seek": 91880, "start": 933.76, "end": 940.7199999999999, "text": " one state to minimize the conflict with the other neurons. So, we will update the state of the neuron", "tokens": [51112, 472, 1785, 281, 17522, 264, 6596, 365, 264, 661, 22027, 13, 407, 11, 321, 486, 5623, 264, 1785, 295, 264, 34090, 51460], "temperature": 0.0, "avg_logprob": -0.060031690905171055, "compression_ratio": 1.8055555555555556, "no_speech_prob": 0.00021654393640346825}, {"id": 138, "seek": 91880, "start": 940.7199999999999, "end": 948.0, "text": " i based on the sign of hi. Notice that this update is guaranteed to decrease the energy", "tokens": [51460, 741, 2361, 322, 264, 1465, 295, 4879, 13, 13428, 300, 341, 5623, 307, 18031, 281, 11514, 264, 2281, 51824], "temperature": 0.0, "avg_logprob": -0.060031690905171055, "compression_ratio": 1.8055555555555556, "no_speech_prob": 0.00021654393640346825}, {"id": 139, "seek": 94800, "start": 948.0, "end": 953.52, "text": " of the network, because from the two candidate states, we are selecting the more energetically", "tokens": [50364, 295, 264, 3209, 11, 570, 490, 264, 732, 11532, 4368, 11, 321, 366, 18182, 264, 544, 2043, 847, 984, 50640], "temperature": 0.0, "avg_logprob": -0.05760290947827426, "compression_ratio": 1.6794871794871795, "no_speech_prob": 0.0001273107627639547}, {"id": 140, "seek": 94800, "start": 953.52, "end": 960.4, "text": " favorable one. You can think of this as a kind of a voting process. Each neuron looks at the states of", "tokens": [50640, 29557, 472, 13, 509, 393, 519, 295, 341, 382, 257, 733, 295, 257, 10419, 1399, 13, 6947, 34090, 1542, 412, 264, 4368, 295, 50984], "temperature": 0.0, "avg_logprob": -0.05760290947827426, "compression_ratio": 1.6794871794871795, "no_speech_prob": 0.0001273107627639547}, {"id": 141, "seek": 94800, "start": 960.4, "end": 966.72, "text": " all other neurons weighted by the strength of their connections and decides whether to be active", "tokens": [50984, 439, 661, 22027, 32807, 538, 264, 3800, 295, 641, 9271, 293, 14898, 1968, 281, 312, 4967, 51300], "temperature": 0.0, "avg_logprob": -0.05760290947827426, "compression_ratio": 1.6794871794871795, "no_speech_prob": 0.0001273107627639547}, {"id": 142, "seek": 94800, "start": 966.72, "end": 973.84, "text": " or silent based on the majority vote. We'll go through this process for each neuron in the network", "tokens": [51300, 420, 12784, 2361, 322, 264, 6286, 4740, 13, 492, 603, 352, 807, 341, 1399, 337, 1184, 34090, 294, 264, 3209, 51656], "temperature": 0.0, "avg_logprob": -0.05760290947827426, "compression_ratio": 1.6794871794871795, "no_speech_prob": 0.0001273107627639547}, {"id": 143, "seek": 97384, "start": 973.84, "end": 981.84, "text": " one by one chosen in random order, updating their states based on the input from all other neurons.", "tokens": [50364, 472, 538, 472, 8614, 294, 4974, 1668, 11, 25113, 641, 4368, 2361, 322, 264, 4846, 490, 439, 661, 22027, 13, 50764], "temperature": 0.0, "avg_logprob": -0.05522188298842486, "compression_ratio": 1.7174887892376682, "no_speech_prob": 6.401996506610885e-05}, {"id": 144, "seek": 97384, "start": 982.64, "end": 989.2800000000001, "text": " Once we've updated all neurons, we will have completed one iteration of the network inference", "tokens": [50804, 3443, 321, 600, 10588, 439, 22027, 11, 321, 486, 362, 7365, 472, 24784, 295, 264, 3209, 38253, 51136], "temperature": 0.0, "avg_logprob": -0.05522188298842486, "compression_ratio": 1.7174887892376682, "no_speech_prob": 6.401996506610885e-05}, {"id": 145, "seek": 97384, "start": 989.2800000000001, "end": 995.76, "text": " and decreased the system's energy by a little bit. We'll keep repeating this process, doing these", "tokens": [51136, 293, 24436, 264, 1185, 311, 2281, 538, 257, 707, 857, 13, 492, 603, 1066, 18617, 341, 1399, 11, 884, 613, 51460], "temperature": 0.0, "avg_logprob": -0.05522188298842486, "compression_ratio": 1.7174887892376682, "no_speech_prob": 6.401996506610885e-05}, {"id": 146, "seek": 97384, "start": 995.76, "end": 1002.32, "text": " sweeps through all neurons, updating them one at a time based on the current configuration.", "tokens": [51460, 2484, 10653, 807, 439, 22027, 11, 25113, 552, 472, 412, 257, 565, 2361, 322, 264, 2190, 11694, 13, 51788], "temperature": 0.0, "avg_logprob": -0.05522188298842486, "compression_ratio": 1.7174887892376682, "no_speech_prob": 6.401996506610885e-05}, {"id": 147, "seek": 100232, "start": 1002.32, "end": 1010.0, "text": " As we do this, the network will gradually evolve towards a configuration that minimizes the overall", "tokens": [50364, 1018, 321, 360, 341, 11, 264, 3209, 486, 13145, 16693, 3030, 257, 11694, 300, 4464, 5660, 264, 4787, 50748], "temperature": 0.0, "avg_logprob": -0.07376775180592257, "compression_ratio": 1.703056768558952, "no_speech_prob": 2.430022504995577e-05}, {"id": 148, "seek": 100232, "start": 1010.0, "end": 1017.44, "text": " energy. At some point, however, we will reach a configuration where flipping any neuron would", "tokens": [50748, 2281, 13, 1711, 512, 935, 11, 4461, 11, 321, 486, 2524, 257, 11694, 689, 26886, 604, 34090, 576, 51120], "temperature": 0.0, "avg_logprob": -0.07376775180592257, "compression_ratio": 1.703056768558952, "no_speech_prob": 2.430022504995577e-05}, {"id": 149, "seek": 100232, "start": 1017.44, "end": 1023.7600000000001, "text": " lead to an increase in energy. So, no further adjustments would be necessary. At that point,", "tokens": [51120, 1477, 281, 364, 3488, 294, 2281, 13, 407, 11, 572, 3052, 18624, 576, 312, 4818, 13, 1711, 300, 935, 11, 51436], "temperature": 0.0, "avg_logprob": -0.07376775180592257, "compression_ratio": 1.703056768558952, "no_speech_prob": 2.430022504995577e-05}, {"id": 150, "seek": 100232, "start": 1023.7600000000001, "end": 1030.56, "text": " the network has converged to a stable configuration, where each neuron's state agrees with the majority", "tokens": [51436, 264, 3209, 575, 9652, 3004, 281, 257, 8351, 11694, 11, 689, 1184, 34090, 311, 1785, 26383, 365, 264, 6286, 51776], "temperature": 0.0, "avg_logprob": -0.07376775180592257, "compression_ratio": 1.703056768558952, "no_speech_prob": 2.430022504995577e-05}, {"id": 151, "seek": 103056, "start": 1030.56, "end": 1036.32, "text": " vote. This stable configuration represents a local minimum in the energy landscape.", "tokens": [50364, 4740, 13, 639, 8351, 11694, 8855, 257, 2654, 7285, 294, 264, 2281, 9661, 13, 50652], "temperature": 0.0, "avg_logprob": -0.07927544911702473, "compression_ratio": 1.5847457627118644, "no_speech_prob": 0.0001911028375616297}, {"id": 152, "seek": 103056, "start": 1036.96, "end": 1042.8, "text": " Now, you might be wondering, is the network guaranteed to reach such a stable configuration?", "tokens": [50684, 823, 11, 291, 1062, 312, 6359, 11, 307, 264, 3209, 18031, 281, 2524, 1270, 257, 8351, 11694, 30, 50976], "temperature": 0.0, "avg_logprob": -0.07927544911702473, "compression_ratio": 1.5847457627118644, "no_speech_prob": 0.0001911028375616297}, {"id": 153, "seek": 103056, "start": 1042.8, "end": 1049.44, "text": " Could we possibly stumble into a particularly unlucky set of states and get stuck in a never-ending", "tokens": [50976, 7497, 321, 6264, 41302, 666, 257, 4098, 38838, 992, 295, 4368, 293, 483, 5541, 294, 257, 1128, 12, 2029, 51308], "temperature": 0.0, "avg_logprob": -0.07927544911702473, "compression_ratio": 1.5847457627118644, "no_speech_prob": 0.0001911028375616297}, {"id": 154, "seek": 103056, "start": 1049.44, "end": 1056.8799999999999, "text": " loop of flipping neurons back and forth? In other words, is such iterative flipping of one neuron", "tokens": [51308, 6367, 295, 26886, 22027, 646, 293, 5220, 30, 682, 661, 2283, 11, 307, 1270, 17138, 1166, 26886, 295, 472, 34090, 51680], "temperature": 0.0, "avg_logprob": -0.07927544911702473, "compression_ratio": 1.5847457627118644, "no_speech_prob": 0.0001911028375616297}, {"id": 155, "seek": 105688, "start": 1056.88, "end": 1063.6000000000001, "text": " at a time equivalent to doing a descent along the energy surface? This is where we come back to", "tokens": [50364, 412, 257, 565, 10344, 281, 884, 257, 23475, 2051, 264, 2281, 3753, 30, 639, 307, 689, 321, 808, 646, 281, 50700], "temperature": 0.0, "avg_logprob": -0.04567266982278706, "compression_ratio": 1.5924369747899159, "no_speech_prob": 4.832562626688741e-05}, {"id": 156, "seek": 105688, "start": 1063.6000000000001, "end": 1070.16, "text": " the point about symmetric weights. It turns out that there is a mathematical proof that I'm not", "tokens": [50700, 264, 935, 466, 32330, 17443, 13, 467, 4523, 484, 300, 456, 307, 257, 18894, 8177, 300, 286, 478, 406, 51028], "temperature": 0.0, "avg_logprob": -0.04567266982278706, "compression_ratio": 1.5924369747899159, "no_speech_prob": 4.832562626688741e-05}, {"id": 157, "seek": 105688, "start": 1070.16, "end": 1077.5200000000002, "text": " going to cover here, stating that as long as your weights are symmetric, this simple majority vote", "tokens": [51028, 516, 281, 2060, 510, 11, 26688, 300, 382, 938, 382, 428, 17443, 366, 32330, 11, 341, 2199, 6286, 4740, 51396], "temperature": 0.0, "avg_logprob": -0.04567266982278706, "compression_ratio": 1.5924369747899159, "no_speech_prob": 4.832562626688741e-05}, {"id": 158, "seek": 105688, "start": 1077.5200000000002, "end": 1084.24, "text": " single neuron update rule is guaranteed to eventually converge to a stable configuration", "tokens": [51396, 2167, 34090, 5623, 4978, 307, 18031, 281, 4728, 41881, 281, 257, 8351, 11694, 51732], "temperature": 0.0, "avg_logprob": -0.04567266982278706, "compression_ratio": 1.5924369747899159, "no_speech_prob": 4.832562626688741e-05}, {"id": 159, "seek": 108424, "start": 1084.24, "end": 1091.28, "text": " if you do it enough times. To restate it, the Hopfield network can settle into different local", "tokens": [50364, 498, 291, 360, 309, 1547, 1413, 13, 1407, 1472, 473, 309, 11, 264, 13438, 7610, 3209, 393, 11852, 666, 819, 2654, 50716], "temperature": 0.0, "avg_logprob": -0.08062515587642274, "compression_ratio": 1.6888888888888889, "no_speech_prob": 5.475971192936413e-05}, {"id": 160, "seek": 108424, "start": 1091.28, "end": 1097.76, "text": " minima based on its initial conditions. These local minima in the energy landscape correspond", "tokens": [50716, 4464, 64, 2361, 322, 1080, 5883, 4487, 13, 1981, 2654, 4464, 64, 294, 264, 2281, 9661, 6805, 51040], "temperature": 0.0, "avg_logprob": -0.08062515587642274, "compression_ratio": 1.6888888888888889, "no_speech_prob": 5.475971192936413e-05}, {"id": 161, "seek": 108424, "start": 1097.76, "end": 1104.32, "text": " to distinct memories stored in the network. When we initialize the network with a pattern that is", "tokens": [51040, 281, 10644, 8495, 12187, 294, 264, 3209, 13, 1133, 321, 5883, 1125, 264, 3209, 365, 257, 5102, 300, 307, 51368], "temperature": 0.0, "avg_logprob": -0.08062515587642274, "compression_ratio": 1.6888888888888889, "no_speech_prob": 5.475971192936413e-05}, {"id": 162, "seek": 108424, "start": 1104.32, "end": 1110.56, "text": " similar to one of these memories in some way and let it evolve, it will fall into the nearest", "tokens": [51368, 2531, 281, 472, 295, 613, 8495, 294, 512, 636, 293, 718, 309, 16693, 11, 309, 486, 2100, 666, 264, 23831, 51680], "temperature": 0.0, "avg_logprob": -0.08062515587642274, "compression_ratio": 1.6888888888888889, "no_speech_prob": 5.475971192936413e-05}, {"id": 163, "seek": 111056, "start": 1110.56, "end": 1117.6799999999998, "text": " local minimum, effectively recalling the complete stored memory, thus performing pattern completion", "tokens": [50364, 2654, 7285, 11, 8659, 9901, 278, 264, 3566, 12187, 4675, 11, 8807, 10205, 5102, 19372, 50720], "temperature": 0.0, "avg_logprob": -0.06155548209235782, "compression_ratio": 1.6470588235294117, "no_speech_prob": 3.7636422348441556e-05}, {"id": 164, "seek": 111056, "start": 1117.6799999999998, "end": 1123.84, "text": " or noise correction. But so far we haven't talked about how we come up with the set of connection", "tokens": [50720, 420, 5658, 19984, 13, 583, 370, 1400, 321, 2378, 380, 2825, 466, 577, 321, 808, 493, 365, 264, 992, 295, 4984, 51028], "temperature": 0.0, "avg_logprob": -0.06155548209235782, "compression_ratio": 1.6470588235294117, "no_speech_prob": 3.7636422348441556e-05}, {"id": 165, "seek": 111056, "start": 1123.84, "end": 1130.56, "text": " weights that encode specific memories in the first place. So let's explore the learning process.", "tokens": [51028, 17443, 300, 2058, 1429, 2685, 8495, 294, 264, 700, 1081, 13, 407, 718, 311, 6839, 264, 2539, 1399, 13, 51364], "temperature": 0.0, "avg_logprob": -0.06155548209235782, "compression_ratio": 1.6470588235294117, "no_speech_prob": 3.7636422348441556e-05}, {"id": 166, "seek": 111056, "start": 1132.56, "end": 1138.96, "text": " Before we move to storing several memories, let's consider memorizing a single pattern of states.", "tokens": [51464, 4546, 321, 1286, 281, 26085, 2940, 8495, 11, 718, 311, 1949, 10560, 3319, 257, 2167, 5102, 295, 4368, 13, 51784], "temperature": 0.0, "avg_logprob": -0.06155548209235782, "compression_ratio": 1.6470588235294117, "no_speech_prob": 3.7636422348441556e-05}, {"id": 167, "seek": 113896, "start": 1139.6000000000001, "end": 1143.3600000000001, "text": " That means the network would have a single global minimum,", "tokens": [50396, 663, 1355, 264, 3209, 576, 362, 257, 2167, 4338, 7285, 11, 50584], "temperature": 0.0, "avg_logprob": -0.10139656066894531, "compression_ratio": 1.5466666666666666, "no_speech_prob": 9.610214328859001e-05}, {"id": 168, "seek": 113896, "start": 1143.3600000000001, "end": 1149.52, "text": " one energy well, and would converge to the same pattern every time no matter where you initialize", "tokens": [50584, 472, 2281, 731, 11, 293, 576, 41881, 281, 264, 912, 5102, 633, 565, 572, 1871, 689, 291, 5883, 1125, 50892], "temperature": 0.0, "avg_logprob": -0.10139656066894531, "compression_ratio": 1.5466666666666666, "no_speech_prob": 9.610214328859001e-05}, {"id": 169, "seek": 113896, "start": 1149.52, "end": 1155.68, "text": " it. While it has little practical use, it provides a nice starting point to describe the learning", "tokens": [50892, 309, 13, 3987, 309, 575, 707, 8496, 764, 11, 309, 6417, 257, 1481, 2891, 935, 281, 6786, 264, 2539, 51200], "temperature": 0.0, "avg_logprob": -0.10139656066894531, "compression_ratio": 1.5466666666666666, "no_speech_prob": 9.610214328859001e-05}, {"id": 170, "seek": 113896, "start": 1155.68, "end": 1163.6000000000001, "text": " procedure. Let's denote the template pattern that we'd like to store as XC, which is a vector", "tokens": [51200, 10747, 13, 961, 311, 45708, 264, 12379, 5102, 300, 321, 1116, 411, 281, 3531, 382, 1783, 34, 11, 597, 307, 257, 8062, 51596], "temperature": 0.0, "avg_logprob": -0.10139656066894531, "compression_ratio": 1.5466666666666666, "no_speech_prob": 9.610214328859001e-05}, {"id": 171, "seek": 116360, "start": 1163.6, "end": 1171.76, "text": " packing the states of all neurons, and XCI will denote the ith component, the state of ith neuron", "tokens": [50364, 20815, 264, 4368, 295, 439, 22027, 11, 293, 1783, 25240, 486, 45708, 264, 309, 71, 6542, 11, 264, 1785, 295, 309, 71, 34090, 50772], "temperature": 0.0, "avg_logprob": -0.13993893171611585, "compression_ratio": 1.5846994535519126, "no_speech_prob": 0.00016346453048754483}, {"id": 172, "seek": 116360, "start": 1171.76, "end": 1178.3999999999999, "text": " encoding the memory. While XI refers to the state of ith neuron in the network in general,", "tokens": [50772, 43430, 264, 4675, 13, 3987, 1783, 40, 14942, 281, 264, 1785, 295, 309, 71, 34090, 294, 264, 3209, 294, 2674, 11, 51104], "temperature": 0.0, "avg_logprob": -0.13993893171611585, "compression_ratio": 1.5846994535519126, "no_speech_prob": 0.00016346453048754483}, {"id": 173, "seek": 116360, "start": 1178.3999999999999, "end": 1187.4399999999998, "text": " which could be tweaked. Revisiting our definition of energy, we want to set Wij so that this quantity", "tokens": [51104, 597, 727, 312, 6986, 7301, 13, 1300, 4938, 1748, 527, 7123, 295, 2281, 11, 321, 528, 281, 992, 343, 1718, 370, 300, 341, 11275, 51556], "temperature": 0.0, "avg_logprob": -0.13993893171611585, "compression_ratio": 1.5846994535519126, "no_speech_prob": 0.00016346453048754483}, {"id": 174, "seek": 118744, "start": 1187.44, "end": 1195.52, "text": " would be at its minimal value for the memory pattern. If we plug XI equal to XCI, we get the", "tokens": [50364, 576, 312, 412, 1080, 13206, 2158, 337, 264, 4675, 5102, 13, 759, 321, 5452, 1783, 40, 2681, 281, 1783, 25240, 11, 321, 483, 264, 50768], "temperature": 0.0, "avg_logprob": -0.06065984872671274, "compression_ratio": 1.7123287671232876, "no_speech_prob": 0.0006667011766694486}, {"id": 175, "seek": 118744, "start": 1195.52, "end": 1201.92, "text": " equation for the energy of the reference pattern as a function of weights, which we want to turn", "tokens": [50768, 5367, 337, 264, 2281, 295, 264, 6408, 5102, 382, 257, 2445, 295, 17443, 11, 597, 321, 528, 281, 1261, 51088], "temperature": 0.0, "avg_logprob": -0.06065984872671274, "compression_ratio": 1.7123287671232876, "no_speech_prob": 0.0006667011766694486}, {"id": 176, "seek": 118744, "start": 1201.92, "end": 1208.4, "text": " into a global minimum. Notice that we don't really care about the absolute value of that energy.", "tokens": [51088, 666, 257, 4338, 7285, 13, 13428, 300, 321, 500, 380, 534, 1127, 466, 264, 8236, 2158, 295, 300, 2281, 13, 51412], "temperature": 0.0, "avg_logprob": -0.06065984872671274, "compression_ratio": 1.7123287671232876, "no_speech_prob": 0.0006667011766694486}, {"id": 177, "seek": 118744, "start": 1208.96, "end": 1214.4, "text": " As long as the energy of the desired memory pattern is less than the energy of any other", "tokens": [51440, 1018, 938, 382, 264, 2281, 295, 264, 14721, 4675, 5102, 307, 1570, 813, 264, 2281, 295, 604, 661, 51712], "temperature": 0.0, "avg_logprob": -0.06065984872671274, "compression_ratio": 1.7123287671232876, "no_speech_prob": 0.0006667011766694486}, {"id": 178, "seek": 121440, "start": 1214.4, "end": 1221.2800000000002, "text": " configuration. Now intuitively, the lowest possible energy is obtained when all the", "tokens": [50364, 11694, 13, 823, 46506, 11, 264, 12437, 1944, 2281, 307, 14879, 562, 439, 264, 50708], "temperature": 0.0, "avg_logprob": -0.07060125575346106, "compression_ratio": 1.6288209606986899, "no_speech_prob": 0.00020027330901939422}, {"id": 179, "seek": 121440, "start": 1221.2800000000002, "end": 1228.5600000000002, "text": " connection weights fully align with the state pairs. But when we have just a single pattern,", "tokens": [50708, 4984, 17443, 4498, 7975, 365, 264, 1785, 15494, 13, 583, 562, 321, 362, 445, 257, 2167, 5102, 11, 51072], "temperature": 0.0, "avg_logprob": -0.07060125575346106, "compression_ratio": 1.6288209606986899, "no_speech_prob": 0.00020027330901939422}, {"id": 180, "seek": 121440, "start": 1228.5600000000002, "end": 1236.16, "text": " this is very easy to do. All we need is to set the weight Wij to be the product of the corresponding", "tokens": [51072, 341, 307, 588, 1858, 281, 360, 13, 1057, 321, 643, 307, 281, 992, 264, 3364, 343, 1718, 281, 312, 264, 1674, 295, 264, 11760, 51452], "temperature": 0.0, "avg_logprob": -0.07060125575346106, "compression_ratio": 1.6288209606986899, "no_speech_prob": 0.00020027330901939422}, {"id": 181, "seek": 121440, "start": 1236.16, "end": 1242.8000000000002, "text": " pair of states in the memory pattern. This way, every connection is satisfied and the energy of", "tokens": [51452, 6119, 295, 4368, 294, 264, 4675, 5102, 13, 639, 636, 11, 633, 4984, 307, 11239, 293, 264, 2281, 295, 51784], "temperature": 0.0, "avg_logprob": -0.07060125575346106, "compression_ratio": 1.6288209606986899, "no_speech_prob": 0.00020027330901939422}, {"id": 182, "seek": 124280, "start": 1242.8, "end": 1248.48, "text": " the network when it is in the state XI becomes the negative of the total number of edges.", "tokens": [50364, 264, 3209, 562, 309, 307, 294, 264, 1785, 1783, 40, 3643, 264, 3671, 295, 264, 3217, 1230, 295, 8819, 13, 50648], "temperature": 0.0, "avg_logprob": -0.06553791431670493, "compression_ratio": 1.7104072398190044, "no_speech_prob": 1.6701413187547587e-05}, {"id": 183, "seek": 124280, "start": 1249.12, "end": 1256.0, "text": " When the network is in the state XI, any single flip of a neuron would increase the energy,", "tokens": [50680, 1133, 264, 3209, 307, 294, 264, 1785, 1783, 40, 11, 604, 2167, 7929, 295, 257, 34090, 576, 3488, 264, 2281, 11, 51024], "temperature": 0.0, "avg_logprob": -0.06553791431670493, "compression_ratio": 1.7104072398190044, "no_speech_prob": 1.6701413187547587e-05}, {"id": 184, "seek": 124280, "start": 1256.0, "end": 1263.04, "text": " thus making it a stable state. I want to reiterate the crucial point here. If we want to come up with", "tokens": [51024, 8807, 1455, 309, 257, 8351, 1785, 13, 286, 528, 281, 33528, 264, 11462, 935, 510, 13, 759, 321, 528, 281, 808, 493, 365, 51376], "temperature": 0.0, "avg_logprob": -0.06553791431670493, "compression_ratio": 1.7104072398190044, "no_speech_prob": 1.6701413187547587e-05}, {"id": 185, "seek": 124280, "start": 1263.04, "end": 1269.84, "text": " the set of weights that would dig an energy well around some pattern, then all we need to know", "tokens": [51376, 264, 992, 295, 17443, 300, 576, 2528, 364, 2281, 731, 926, 512, 5102, 11, 550, 439, 321, 643, 281, 458, 51716], "temperature": 0.0, "avg_logprob": -0.06553791431670493, "compression_ratio": 1.7104072398190044, "no_speech_prob": 1.6701413187547587e-05}, {"id": 186, "seek": 126984, "start": 1269.84, "end": 1276.72, "text": " are the pairwise relationships between states in that pattern. If the two neurons are active", "tokens": [50364, 366, 264, 6119, 3711, 6159, 1296, 4368, 294, 300, 5102, 13, 759, 264, 732, 22027, 366, 4967, 50708], "temperature": 0.0, "avg_logprob": -0.06922676828172472, "compression_ratio": 1.6308411214953271, "no_speech_prob": 3.705293420352973e-05}, {"id": 187, "seek": 126984, "start": 1276.72, "end": 1282.56, "text": " together in the source memory, strengthening the connection between them lowers the hopfield", "tokens": [50708, 1214, 294, 264, 4009, 4675, 11, 28224, 264, 4984, 1296, 552, 44936, 264, 3818, 7610, 51000], "temperature": 0.0, "avg_logprob": -0.06922676828172472, "compression_ratio": 1.6308411214953271, "no_speech_prob": 3.705293420352973e-05}, {"id": 188, "seek": 126984, "start": 1282.56, "end": 1288.32, "text": " energy of that memory, effectively storing it in the weights for associative recall.", "tokens": [51000, 2281, 295, 300, 4675, 11, 8659, 26085, 309, 294, 264, 17443, 337, 4180, 1166, 9901, 13, 51288], "temperature": 0.0, "avg_logprob": -0.06922676828172472, "compression_ratio": 1.6308411214953271, "no_speech_prob": 3.705293420352973e-05}, {"id": 189, "seek": 126984, "start": 1289.28, "end": 1293.4399999999998, "text": " You may have heard the famous statement from Neuroscience attributed to Donald", "tokens": [51336, 509, 815, 362, 2198, 264, 4618, 5629, 490, 1734, 8977, 6699, 30976, 281, 8632, 51544], "temperature": 0.0, "avg_logprob": -0.06922676828172472, "compression_ratio": 1.6308411214953271, "no_speech_prob": 3.705293420352973e-05}, {"id": 190, "seek": 129344, "start": 1293.8400000000001, "end": 1300.64, "text": " Hebb, neurons that fire together, wire together. And in fact, what we just did is known as the", "tokens": [50384, 634, 6692, 11, 22027, 300, 2610, 1214, 11, 6234, 1214, 13, 400, 294, 1186, 11, 437, 321, 445, 630, 307, 2570, 382, 264, 50724], "temperature": 0.0, "avg_logprob": -0.09323408146097202, "compression_ratio": 1.596638655462185, "no_speech_prob": 0.00021318737708497792}, {"id": 191, "seek": 129344, "start": 1300.64, "end": 1307.8400000000001, "text": " Hebbian learning rule. Great, so we found a way to make a single pattern a stable state of the", "tokens": [50724, 634, 6692, 952, 2539, 4978, 13, 3769, 11, 370, 321, 1352, 257, 636, 281, 652, 257, 2167, 5102, 257, 8351, 1785, 295, 264, 51084], "temperature": 0.0, "avg_logprob": -0.09323408146097202, "compression_ratio": 1.596638655462185, "no_speech_prob": 0.00021318737708497792}, {"id": 192, "seek": 129344, "start": 1307.8400000000001, "end": 1315.52, "text": " network. But we want to store multiple patterns. How do we do that? Here's the key idea. We can", "tokens": [51084, 3209, 13, 583, 321, 528, 281, 3531, 3866, 8294, 13, 1012, 360, 321, 360, 300, 30, 1692, 311, 264, 2141, 1558, 13, 492, 393, 51468], "temperature": 0.0, "avg_logprob": -0.09323408146097202, "compression_ratio": 1.596638655462185, "no_speech_prob": 0.00021318737708497792}, {"id": 193, "seek": 129344, "start": 1315.52, "end": 1322.24, "text": " simply sum the weights we would get for each pattern separately. So if we have three patterns,", "tokens": [51468, 2935, 2408, 264, 17443, 321, 576, 483, 337, 1184, 5102, 14759, 13, 407, 498, 321, 362, 1045, 8294, 11, 51804], "temperature": 0.0, "avg_logprob": -0.09323408146097202, "compression_ratio": 1.596638655462185, "no_speech_prob": 0.00021318737708497792}, {"id": 194, "seek": 132224, "start": 1322.32, "end": 1327.76, "text": " X1, X2 and X3, we can set the weights according to the following equation.", "tokens": [50368, 1783, 16, 11, 1783, 17, 293, 1783, 18, 11, 321, 393, 992, 264, 17443, 4650, 281, 264, 3480, 5367, 13, 50640], "temperature": 0.0, "avg_logprob": -0.0766760321224437, "compression_ratio": 1.554585152838428, "no_speech_prob": 0.0003353494976181537}, {"id": 195, "seek": 132224, "start": 1328.8, "end": 1335.2, "text": " What this will do is turn each of the patterns into a local minimum. It's pretty straightforward", "tokens": [50692, 708, 341, 486, 360, 307, 1261, 1184, 295, 264, 8294, 666, 257, 2654, 7285, 13, 467, 311, 1238, 15325, 51012], "temperature": 0.0, "avg_logprob": -0.0766760321224437, "compression_ratio": 1.554585152838428, "no_speech_prob": 0.0003353494976181537}, {"id": 196, "seek": 132224, "start": 1335.2, "end": 1340.4, "text": " to show mathematically, and if you're interested, I encourage you to check out the references", "tokens": [51012, 281, 855, 44003, 11, 293, 498, 291, 434, 3102, 11, 286, 5373, 291, 281, 1520, 484, 264, 15400, 51272], "temperature": 0.0, "avg_logprob": -0.0766760321224437, "compression_ratio": 1.554585152838428, "no_speech_prob": 0.0003353494976181537}, {"id": 197, "seek": 132224, "start": 1340.4, "end": 1346.96, "text": " in the video description. However, intuitively, if the patterns you want to store are very", "tokens": [51272, 294, 264, 960, 3855, 13, 2908, 11, 46506, 11, 498, 264, 8294, 291, 528, 281, 3531, 366, 588, 51600], "temperature": 0.0, "avg_logprob": -0.0766760321224437, "compression_ratio": 1.554585152838428, "no_speech_prob": 0.0003353494976181537}, {"id": 198, "seek": 134696, "start": 1346.96, "end": 1354.4, "text": " different, so they are far away in the state space from each other, then if you first independently", "tokens": [50364, 819, 11, 370, 436, 366, 1400, 1314, 294, 264, 1785, 1901, 490, 1184, 661, 11, 550, 498, 291, 700, 21761, 50736], "temperature": 0.0, "avg_logprob": -0.07230285195743337, "compression_ratio": 1.668122270742358, "no_speech_prob": 0.001325017772614956}, {"id": 199, "seek": 134696, "start": 1354.4, "end": 1361.04, "text": " dig energy wells around each of them, and then simply add the energy landscapes together,", "tokens": [50736, 2528, 2281, 30984, 926, 1184, 295, 552, 11, 293, 550, 2935, 909, 264, 2281, 29822, 1214, 11, 51068], "temperature": 0.0, "avg_logprob": -0.07230285195743337, "compression_ratio": 1.668122270742358, "no_speech_prob": 0.001325017772614956}, {"id": 200, "seek": 134696, "start": 1361.68, "end": 1368.96, "text": " the resulting surface will have local minima in the same three valleys. And this nicely brings", "tokens": [51100, 264, 16505, 3753, 486, 362, 2654, 4464, 64, 294, 264, 912, 1045, 45614, 13, 400, 341, 9594, 5607, 51464], "temperature": 0.0, "avg_logprob": -0.07230285195743337, "compression_ratio": 1.668122270742358, "no_speech_prob": 0.001325017772614956}, {"id": 201, "seek": 134696, "start": 1368.96, "end": 1376.16, "text": " us to the limitation of the hopfield networks. There is a limited number of valleys we can sculpt", "tokens": [51464, 505, 281, 264, 27432, 295, 264, 3818, 7610, 9590, 13, 821, 307, 257, 5567, 1230, 295, 45614, 321, 393, 12613, 51824], "temperature": 0.0, "avg_logprob": -0.07230285195743337, "compression_ratio": 1.668122270742358, "no_speech_prob": 0.001325017772614956}, {"id": 202, "seek": 137616, "start": 1376.16, "end": 1382.3200000000002, "text": " in the energy landscape before they start to interfere with each other. At some point,", "tokens": [50364, 294, 264, 2281, 9661, 949, 436, 722, 281, 23946, 365, 1184, 661, 13, 1711, 512, 935, 11, 50672], "temperature": 0.0, "avg_logprob": -0.05228754492367015, "compression_ratio": 1.5879828326180256, "no_speech_prob": 7.722184818703681e-05}, {"id": 203, "seek": 137616, "start": 1382.3200000000002, "end": 1389.68, "text": " if we try to store too many patterns, the network will fail to converge to a stored pattern reliably", "tokens": [50672, 498, 321, 853, 281, 3531, 886, 867, 8294, 11, 264, 3209, 486, 3061, 281, 41881, 281, 257, 12187, 5102, 49927, 51040], "temperature": 0.0, "avg_logprob": -0.05228754492367015, "compression_ratio": 1.5879828326180256, "no_speech_prob": 7.722184818703681e-05}, {"id": 204, "seek": 137616, "start": 1389.68, "end": 1396.0, "text": " and recall weird in-between kind of memories. The total maximum number of patterns you can", "tokens": [51040, 293, 9901, 3657, 294, 12, 32387, 733, 295, 8495, 13, 440, 3217, 6674, 1230, 295, 8294, 291, 393, 51356], "temperature": 0.0, "avg_logprob": -0.05228754492367015, "compression_ratio": 1.5879828326180256, "no_speech_prob": 7.722184818703681e-05}, {"id": 205, "seek": 137616, "start": 1396.0, "end": 1403.68, "text": " store is thus limited and depends only on the size of the network. It is approximately 0.14", "tokens": [51356, 3531, 307, 8807, 5567, 293, 5946, 787, 322, 264, 2744, 295, 264, 3209, 13, 467, 307, 10447, 1958, 13, 7271, 51740], "temperature": 0.0, "avg_logprob": -0.05228754492367015, "compression_ratio": 1.5879828326180256, "no_speech_prob": 7.722184818703681e-05}, {"id": 206, "seek": 140368, "start": 1403.68, "end": 1410.72, "text": " times the number of neurons. So, if you have a hopfield network of 100 neurons, you can reliably", "tokens": [50364, 1413, 264, 1230, 295, 22027, 13, 407, 11, 498, 291, 362, 257, 3818, 7610, 3209, 295, 2319, 22027, 11, 291, 393, 49927, 50716], "temperature": 0.0, "avg_logprob": -0.06270790100097656, "compression_ratio": 1.5916666666666666, "no_speech_prob": 8.888079173630103e-05}, {"id": 207, "seek": 140368, "start": 1410.72, "end": 1417.28, "text": " store less than 14 patterns in the best case scenario. If you are unlucky, however, and some", "tokens": [50716, 3531, 1570, 813, 3499, 8294, 294, 264, 1151, 1389, 9005, 13, 759, 291, 366, 38838, 11, 4461, 11, 293, 512, 51044], "temperature": 0.0, "avg_logprob": -0.06270790100097656, "compression_ratio": 1.5916666666666666, "no_speech_prob": 8.888079173630103e-05}, {"id": 208, "seek": 140368, "start": 1417.28, "end": 1424.5600000000002, "text": " patterns are similar to each other or correlated, their energy wells will begin to interfere even", "tokens": [51044, 8294, 366, 2531, 281, 1184, 661, 420, 38574, 11, 641, 2281, 30984, 486, 1841, 281, 23946, 754, 51408], "temperature": 0.0, "avg_logprob": -0.06270790100097656, "compression_ratio": 1.5916666666666666, "no_speech_prob": 8.888079173630103e-05}, {"id": 209, "seek": 140368, "start": 1424.5600000000002, "end": 1430.72, "text": " before you reach the full capacity. All of this makes vanilla hopfield networks not useful for", "tokens": [51408, 949, 291, 2524, 264, 1577, 6042, 13, 1057, 295, 341, 1669, 17528, 3818, 7610, 9590, 406, 4420, 337, 51716], "temperature": 0.0, "avg_logprob": -0.06270790100097656, "compression_ratio": 1.5916666666666666, "no_speech_prob": 8.888079173630103e-05}, {"id": 210, "seek": 143072, "start": 1430.8, "end": 1437.52, "text": " practical purposes. However, to this day, they provide a powerful and intuitive model of associative", "tokens": [50368, 8496, 9932, 13, 2908, 11, 281, 341, 786, 11, 436, 2893, 257, 4005, 293, 21769, 2316, 295, 4180, 1166, 50704], "temperature": 0.0, "avg_logprob": -0.061337796379538144, "compression_ratio": 1.5942622950819672, "no_speech_prob": 0.00013982175732962787}, {"id": 211, "seek": 143072, "start": 1437.52, "end": 1444.56, "text": " memory, a simple network of neuron-like units that can store and retrieve pattern through purely", "tokens": [50704, 4675, 11, 257, 2199, 3209, 295, 34090, 12, 4092, 6815, 300, 393, 3531, 293, 30254, 5102, 807, 17491, 51056], "temperature": 0.0, "avg_logprob": -0.061337796379538144, "compression_ratio": 1.5942622950819672, "no_speech_prob": 0.00013982175732962787}, {"id": 212, "seek": 143072, "start": 1444.56, "end": 1450.16, "text": " local learning and inference rules. Despite their limitations, hopfield networks have laid the", "tokens": [51056, 2654, 2539, 293, 38253, 4474, 13, 11334, 641, 15705, 11, 3818, 7610, 9590, 362, 9897, 264, 51336], "temperature": 0.0, "avg_logprob": -0.061337796379538144, "compression_ratio": 1.5942622950819672, "no_speech_prob": 0.00013982175732962787}, {"id": 213, "seek": 143072, "start": 1450.16, "end": 1456.72, "text": " groundwork for more advanced energy-based models. In one of the next videos, we will look at the", "tokens": [51336, 2727, 1902, 337, 544, 7339, 2281, 12, 6032, 5245, 13, 682, 472, 295, 264, 958, 2145, 11, 321, 486, 574, 412, 264, 51664], "temperature": 0.0, "avg_logprob": -0.061337796379538144, "compression_ratio": 1.5942622950819672, "no_speech_prob": 0.00013982175732962787}, {"id": 214, "seek": 145672, "start": 1456.72, "end": 1463.52, "text": " extension of the hopfield networks known as Boltzmann machines. These generative architectures", "tokens": [50364, 10320, 295, 264, 3818, 7610, 9590, 2570, 382, 37884, 89, 14912, 8379, 13, 1981, 1337, 1166, 6331, 1303, 50704], "temperature": 0.0, "avg_logprob": -0.05797326413890983, "compression_ratio": 1.5458333333333334, "no_speech_prob": 0.00023782180505804718}, {"id": 215, "seek": 145672, "start": 1463.52, "end": 1469.52, "text": " introduce additional hidden units and stochastic dynamics, allowing them to learn more complex", "tokens": [50704, 5366, 4497, 7633, 6815, 293, 342, 8997, 2750, 15679, 11, 8293, 552, 281, 1466, 544, 3997, 51004], "temperature": 0.0, "avg_logprob": -0.05797326413890983, "compression_ratio": 1.5458333333333334, "no_speech_prob": 0.00023782180505804718}, {"id": 216, "seek": 145672, "start": 1469.52, "end": 1475.3600000000001, "text": " probability distributions. There is also an extension to modern hopfield networks,", "tokens": [51004, 8482, 37870, 13, 821, 307, 611, 364, 10320, 281, 4363, 3818, 7610, 9590, 11, 51296], "temperature": 0.0, "avg_logprob": -0.05797326413890983, "compression_ratio": 1.5458333333333334, "no_speech_prob": 0.00023782180505804718}, {"id": 217, "seek": 145672, "start": 1475.3600000000001, "end": 1481.6000000000001, "text": " published in 2016 with John Hopfield himself as one of the authors, but that's a topic for another", "tokens": [51296, 6572, 294, 6549, 365, 2619, 13438, 7610, 3647, 382, 472, 295, 264, 16552, 11, 457, 300, 311, 257, 4829, 337, 1071, 51608], "temperature": 0.0, "avg_logprob": -0.05797326413890983, "compression_ratio": 1.5458333333333334, "no_speech_prob": 0.00023782180505804718}, {"id": 218, "seek": 148160, "start": 1481.6, "end": 1487.04, "text": " time. In the meanwhile, I'd like to take a moment and thank Shortform, who are kindly sponsoring", "tokens": [50364, 565, 13, 682, 264, 29252, 11, 286, 1116, 411, 281, 747, 257, 1623, 293, 1309, 16881, 837, 11, 567, 366, 29736, 30311, 50636], "temperature": 0.0, "avg_logprob": -0.06381153218886432, "compression_ratio": 1.532520325203252, "no_speech_prob": 0.0011513929348438978}, {"id": 219, "seek": 148160, "start": 1487.04, "end": 1493.28, "text": " today's video. Shortform is a platform that lets you supercharge your reading and gain valuable", "tokens": [50636, 965, 311, 960, 13, 16881, 837, 307, 257, 3663, 300, 6653, 291, 1687, 13604, 428, 3760, 293, 6052, 8263, 50948], "temperature": 0.0, "avg_logprob": -0.06381153218886432, "compression_ratio": 1.532520325203252, "no_speech_prob": 0.0011513929348438978}, {"id": 220, "seek": 148160, "start": 1493.28, "end": 1500.32, "text": " insights from books. Their unique approach of book guides goes way beyond simple summaries,", "tokens": [50948, 14310, 490, 3642, 13, 6710, 3845, 3109, 295, 1446, 17007, 1709, 636, 4399, 2199, 8367, 4889, 11, 51300], "temperature": 0.0, "avg_logprob": -0.06381153218886432, "compression_ratio": 1.532520325203252, "no_speech_prob": 0.0011513929348438978}, {"id": 221, "seek": 148160, "start": 1500.32, "end": 1506.7199999999998, "text": " by providing a comprehensive overview of the material. Not only do you get a concise version", "tokens": [51300, 538, 6530, 257, 13914, 12492, 295, 264, 2527, 13, 1726, 787, 360, 291, 483, 257, 44882, 3037, 51620], "temperature": 0.0, "avg_logprob": -0.06381153218886432, "compression_ratio": 1.532520325203252, "no_speech_prob": 0.0011513929348438978}, {"id": 222, "seek": 150672, "start": 1506.72, "end": 1514.0, "text": " of the main points, but you also benefit from related ideas sourced from other books and research", "tokens": [50364, 295, 264, 2135, 2793, 11, 457, 291, 611, 5121, 490, 4077, 3487, 11006, 1232, 490, 661, 3642, 293, 2132, 50728], "temperature": 0.0, "avg_logprob": -0.04041653144650343, "compression_ratio": 1.5702127659574467, "no_speech_prob": 0.004198753274977207}, {"id": 223, "seek": 150672, "start": 1514.0, "end": 1520.64, "text": " papers on the topic. They have an actively growing library of books from all sorts of genres,", "tokens": [50728, 10577, 322, 264, 4829, 13, 814, 362, 364, 13022, 4194, 6405, 295, 3642, 490, 439, 7527, 295, 30057, 11, 51060], "temperature": 0.0, "avg_logprob": -0.04041653144650343, "compression_ratio": 1.5702127659574467, "no_speech_prob": 0.004198753274977207}, {"id": 224, "seek": 150672, "start": 1520.64, "end": 1526.08, "text": " such as science, health and technology. Not only that, but there is a useful AI-powered", "tokens": [51060, 1270, 382, 3497, 11, 1585, 293, 2899, 13, 1726, 787, 300, 11, 457, 456, 307, 257, 4420, 7318, 12, 27178, 51332], "temperature": 0.0, "avg_logprob": -0.04041653144650343, "compression_ratio": 1.5702127659574467, "no_speech_prob": 0.004198753274977207}, {"id": 225, "seek": 150672, "start": 1526.08, "end": 1532.48, "text": " browser extension that allows you to generate similar guides for arbitrary content on the", "tokens": [51332, 11185, 10320, 300, 4045, 291, 281, 8460, 2531, 17007, 337, 23211, 2701, 322, 264, 51652], "temperature": 0.0, "avg_logprob": -0.04041653144650343, "compression_ratio": 1.5702127659574467, "no_speech_prob": 0.004198753274977207}, {"id": 226, "seek": 153248, "start": 1532.48, "end": 1537.92, "text": " internet. Personally, I found Shortform to be really helpful, both when I'm choosing books to", "tokens": [50364, 4705, 13, 21079, 11, 286, 1352, 16881, 837, 281, 312, 534, 4961, 11, 1293, 562, 286, 478, 10875, 3642, 281, 50636], "temperature": 0.0, "avg_logprob": -0.09214621323805589, "compression_ratio": 1.5537848605577689, "no_speech_prob": 0.005730174947530031}, {"id": 227, "seek": 153248, "start": 1537.92, "end": 1544.88, "text": " read and writing notes and flashcards on the topic. Don't hesitate to bring your reading to the next", "tokens": [50636, 1401, 293, 3579, 5570, 293, 7319, 40604, 322, 264, 4829, 13, 1468, 380, 20842, 281, 1565, 428, 3760, 281, 264, 958, 50984], "temperature": 0.0, "avg_logprob": -0.09214621323805589, "compression_ratio": 1.5537848605577689, "no_speech_prob": 0.005730174947530031}, {"id": 228, "seek": 153248, "start": 1544.88, "end": 1552.4, "text": " level by clicking the link down in the description to get 5 dates of unlimited access and 20% off", "tokens": [50984, 1496, 538, 9697, 264, 2113, 760, 294, 264, 3855, 281, 483, 1025, 11691, 295, 21950, 2105, 293, 945, 4, 766, 51360], "temperature": 0.0, "avg_logprob": -0.09214621323805589, "compression_ratio": 1.5537848605577689, "no_speech_prob": 0.005730174947530031}, {"id": 229, "seek": 153248, "start": 1552.4, "end": 1558.08, "text": " on annual subscription. If you liked the video, share it with your friends, press the like button", "tokens": [51360, 322, 9784, 17231, 13, 759, 291, 4501, 264, 960, 11, 2073, 309, 365, 428, 1855, 11, 1886, 264, 411, 2960, 51644], "temperature": 0.0, "avg_logprob": -0.09214621323805589, "compression_ratio": 1.5537848605577689, "no_speech_prob": 0.005730174947530031}, {"id": 230, "seek": 155808, "start": 1558.08, "end": 1562.6399999999999, "text": " and subscribe to the channel if you haven't already. Stay tuned for more computational", "tokens": [50364, 293, 3022, 281, 264, 2269, 498, 291, 2378, 380, 1217, 13, 8691, 10870, 337, 544, 28270, 50592], "temperature": 0.0, "avg_logprob": -0.1526602864265442, "compression_ratio": 1.3741007194244603, "no_speech_prob": 0.005384294781833887}, {"id": 231, "seek": 155808, "start": 1562.6399999999999, "end": 1568.48, "text": " neuroscience and machine learning topics coming up. Goodbye and thank you for the interest in the brain.", "tokens": [50592, 42762, 293, 3479, 2539, 8378, 1348, 493, 13, 15528, 293, 1309, 291, 337, 264, 1179, 294, 264, 3567, 13, 50884], "temperature": 0.0, "avg_logprob": -0.1526602864265442, "compression_ratio": 1.3741007194244603, "no_speech_prob": 0.005384294781833887}, {"id": 232, "seek": 158808, "start": 1588.08, "end": 1600.48, "text": " Thank you for watching and I'll see you in the next video.", "tokens": [50364, 1044, 291, 337, 1976, 293, 286, 603, 536, 291, 294, 264, 958, 960, 13, 50984], "temperature": 0.0, "avg_logprob": -0.9864357219022863, "compression_ratio": 0.9206349206349206, "no_speech_prob": 0.93922358751297}], "language": "en"}