start	end	text
0	6000	Consider the following scenario. You are at a party when you hear a short snippet of your favorite song.
9520	15360	Almost instantly, your brain recalls the lyrics of that song and many related memories,
15920	19680	such as attending a recent concert featuring that artist.
22720	29440	It seems very natural and unimpressive. After all, people can recall information all the time.
29520	34400	However, if you think about it, this problem is computationally non-trivial.
35520	41280	Let's put ourselves in the shoes of evolution and try to come up with an algorithm for the brain
41280	48240	to solve it. The first approach that comes to mind is to actually store some kind of a database
48240	54560	of all the songs you've heard a sufficient number of times, along with related information,
54560	60560	like the title and lyrics. When an audio fragment of an unknown song is received,
60560	66960	we can scan through all the songs in our database, find the one that has a close enough match,
66960	73920	and retrieve its lyrics. However, the search space of every song I've ever heard
73920	80480	is astronomically large, and it's even larger when considering every single memory you've formed
80480	85520	since childhood. Performing an exhaustive search would be simply impossible.
86720	93440	Yet, you seem to have no problem instantly recognizing familiar stimuli and finding associations
93440	100640	between them. So, how does the brain accomplish this so quickly? In this video, we will lay the
100640	106480	foundation for a new paradigm of information storage and retrieval, which is more in line with
106560	114080	biology, and actually build one of the simpler models of this process, known as Hopfield Networks,
115040	122320	developed by John Hopfield in 1982, who laid an important groundwork for many ideas in both
122320	136320	neuroscience and machine learning. If you're interested, stay tuned!
137440	144160	Just to reiterate, we need a way to somehow query what we know and find associations between
144160	150480	existing memories and new inputs without explicitly checking individual entries for a match,
150480	157440	which seems like an impossible problem. However, we can draw insights from a seemingly unrelated
157440	163840	field of molecular biology, and in particular, a concept known as Leventhold's paradox of
163840	171360	protein folding. As you may know, proteins are long chains of amino acids that fold into specific
171360	177120	three-dimensional structures which determine their function. The number of possible structural
177120	183200	configurations a protein can take, considering all the different ways you can arrange the atoms
183200	189600	of an amino acid chain in three-dimensional space, is absolutely enormous. Given the number of
189600	194880	possibilities, it seems like it would take an astronomical amount of time for a protein
194880	201840	to search through all the possible structures to find its correct folded state. In fact, there are
201840	208160	computations showing that even if the protein samples its different conformations at a nanosecond
208160	214400	scale, it would still require more time than the age of the universe to arrive at the correct
214480	220560	configuration. Yet, in reality, proteins fold into their native structures in a matter of
220560	227120	milliseconds. So, how do they accomplish this? When I first heard this paradox in high school,
227120	232800	it seemed to me like an ill-posed question. After all, the protein molecule is not a computer,
232800	239360	so it doesn't do any sort of search. It just folds into the most stable and favorable configuration
239360	245520	according to physical laws. This is similar how when you throw a ball, the ball doesn't
245520	251600	search through all the possible trajectories to select the optimal parabolic one. It simply
251600	258080	follows that path because, well, physics works this way. But how can we think about this folding
258080	265680	into a favorable configuration? Favorable for what exactly? Let's introduce the concept of
265680	271040	energy as it will come in handy in future videos as well. If you think back to your high school
271040	277280	physics days, you may recall something along the lines of energy is a quantitative property that
277280	284480	describes this state of a system, namely the capacity to do work or cause change. Energy can be
284480	290240	stored in a variety of different forms, and for the case of proteins, we will be interested in
290240	295440	potential energy stored in the interactions between the atoms in the protein chain.
296880	303040	Each possible configuration of the protein chain has a specific potential energy level
303040	309520	determined by the sum of all of these atomic interactions. In other words, we can assign
309520	315760	a positive number to each state equal to its energy, which is a function in some very high
315760	321600	dimensional space where different dimensions correspond to degrees of freedom you need
321600	328320	to uniquely describe a configuration. For example, all possible dihedral angles of peptide bonds.
330560	336240	Let's abstractly visualize it as having just two dimensions. Then the energy function can be
336240	342960	thought of as a surface where each point on it represents a possible protein configuration,
342960	347600	and the height of the point represents the potential energy of that configuration.
348240	355200	This is what we are going to refer to as energy landscape. For a protein, it would be a complex
355200	362880	rugged surface with many peaks and valleys. Now, here is the key point. A protein molecule,
362880	369840	like any physical system, tends to minimize its potential energy, guided by this second law of
369840	377200	thermodynamics. It will naturally seek out the configuration that has the lowest possible energy
377200	384480	level, as this represents the optimal arrangement of its atoms. And this, in fact, corresponds to the
384480	392640	native, correctly folded state. When a protein is folding, it is essentially rolling downhill on
392640	399440	the energy landscape, following the steepest path towards the valley. This is why proteins can
399440	406800	fall so quickly. They don't need to search through all possible configurations. They simply follow
406800	411680	the natural tendency of physical systems to minimize their potential energy.
412960	418480	The protein's folding process is guided by the shape of the energy landscape,
418480	424560	which in turn is determined by the interaction between its atoms. And the descent along the
424560	430320	surface is essentially driven by the underlying physical process of energy minimization.
432560	439040	Now, the core idea is to achieve something similar for the case of associative memory. Suppose we
439040	445600	have a system that can encode information in its states, and each configuration has a specific
445600	453440	potential energy determined by the interaction between the states. Then we need to first somehow
453440	460560	sculpt the underlying energy landscape so that memories or state patterns we want to store
460560	468320	correspond to local minima, these wells in the energy surface. Second, we need something that
468320	474800	would play the role of the second law of thermodynamics and would drive the changes in the states,
474800	482160	directing the system towards the nearest local minimum. Once these two things are achieved,
482160	488880	retrieving a memory that is most similar to the input pattern is done by configuring the system
488880	495520	to encode the input pattern initially and letting it run to the equilibrium, descending into the
495520	504000	energy well, from which we can read out the source memory. Sounds neat, right? So let's get into
504000	511120	building it. Let's consider a set of neurons which we can think of as abstract units that can
511120	518160	be in one of two possible states, plus one or minus one. This is a simplified analogy of how nerve
518160	523440	cells in the brain encode information through patterns of firing. They either generate an
523440	530400	electrical impulse at a given point in time or remain silent. We'll focus on the fully connected
530400	537840	network where each neuron has connections to every other neuron. These connections have weights
537840	544800	associated with them, real numbers that signify the strength of coupling between the corresponding
544800	551920	pair of neurons. For a pair of units i and j, we denote the connection weight between them as
551920	560720	wij and the states of neurons themselves as xi and xj. In the brain, connections between neurons
560720	567760	or synapses have a well-defined direction. A pair of neurons is connected asymmetrically,
567760	574240	meaning that the synapse from neuron A to neuron B is physiologically separate from the synapse that
574240	581120	connects B to A if that one exists at all, and so they can have different weights. While we could
581120	586560	generalize a Hopfield network to account for asymmetric connections, it would introduce
586560	593360	complications and potentially unstable behavior. For simplicity, here we will stick to the original
593360	599440	formulation of the Hopfield network, which assumes symmetric weights. In other words,
599440	604080	neurons i and j are connected by the same weight in both directions.
604560	611360	Now that we have a set of neurons symmetrically linked with each other through weighted connections,
611920	618720	let's explore what these connection weights represent. If wij is greater than zero,
618720	624480	the connection is said to be excitatory and favors the alignment between the states of
624480	630960	two neurons. We can think of each connection as being either happy or unhappy, depending on the
630960	638800	states of its neurons. For example, if wij is a large positive number, it means that neurons i
638800	645120	and j are closely coupled, and one excites the other. In this case, when one neuron is active,
645120	650880	the other tends to be active as well, and when one is silent, the other one is more likely to be silent.
652800	659360	These configurations, where both xi and xj are either one or minus one, agree with the
659360	665920	connection weight. However, if we observe, for example, that xi is equal to one and xj is equal
665920	673040	to minus one, it conflicts with the excitatory nature of the connection, making such a configuration
673040	681040	less likely. Conversely, when wij is negative, the connection promotes misalignment between the weights.
681040	690640	This alignment between the signs can be expressed more concisely using the product xi times xj.
691360	697440	This product will be positive when both neurons have the same sign and negative when they have
697440	704320	different signs. By multiplying this product further by the connection weight, we obtain
704320	711840	an expression for the happiness of that connection. For a positive wij, happiness will be positive
711840	719120	when the product of the two states is positive. But this is just one edge. We can extend this idea
719120	725840	and compute the happiness of the entire network as a whole by summing this quantity across all edges.
727040	732880	The larger that number is, the more overall agreement there is between connection weights
732880	739760	and pairwise states of neurons. Ultimately, we will search for a set of weights that maximize
739760	746720	this quantity. And maximizing happiness is equivalent to minimizing it with a minus sign,
746720	752000	which you can think of as the measure of overall conflict between the actual configuration of
752000	759200	states and what's favored by the connection weights. This total conflict between the weights
759200	765360	and pairwise states is exactly what we are going to define to be the energy of the system.
766800	773440	As we discussed previously, we want the Hopfield network to be able to gradually evolve towards
773440	780240	energy minima. But looking closely at the formula, we can see that the energy value depends both on
780240	785840	the states and the weights. So there is a lot of things the system can tweak to change it.
785840	791920	What exactly is getting adjusted? As we will see further, there are essentially
791920	798160	two modes of network updates that nicely map to the two aspects of associative memory.
799280	806080	Namely, adjusting the weights corresponds to shaping the energy landscape, defining
806080	812880	which configurations are stable by digging energy wells around them. This is the act of learning
812880	818080	when we are writing new memories into the network. Once the weights are fixed,
818800	824960	tweaking the states of neurons to bring them into greater agreement with the weights
824960	832320	corresponds to descending along the energy surface. This is the act of inference when we are
832320	838640	recalling the memory that is at the bottom of the energy well, which is nearest to the configuration
838640	842320	of the input pattern. Let's take a look at inference first.
845200	851360	Suppose for a second, someone has already set the weights W and hence us the backbone of the
851360	857920	network. The neurons themselves with all the connection weights. However, the exact configuration
857920	864560	of states, which neurons are active and which are silent, is unknown. The question then becomes,
864560	869520	how do we find the state pattern that would minimize the total energy?
870400	877360	As we discussed, simply checking all possible states is not an option. So, we will start with
877360	883600	some initial state, which could be either a partial or a noisy version of one of the memories
883600	891360	or a random configuration altogether. Once the initial condition is set, we will iteratively
891360	898000	try to lower the energy value by focusing on updating one neuron at a time. Let's denote
898000	904480	the neuron we are currently considering as neuron i. We will calculate the total weighted input to
904480	911920	it from all other neurons in the network. This input, which we'll denote as hi, is the sum of
911920	918800	the states of all other neurons multiplied by their respective connection weights. If hi is
918800	925840	positive, it means that the weighted sum of the other neuron states is in favor of neuron i being
925840	933760	in the plus one state. Conversely, if hi is negative, it suggests that neuron i should be in the minus
933760	940720	one state to minimize the conflict with the other neurons. So, we will update the state of the neuron
940720	948000	i based on the sign of hi. Notice that this update is guaranteed to decrease the energy
948000	953520	of the network, because from the two candidate states, we are selecting the more energetically
953520	960400	favorable one. You can think of this as a kind of a voting process. Each neuron looks at the states of
960400	966720	all other neurons weighted by the strength of their connections and decides whether to be active
966720	973840	or silent based on the majority vote. We'll go through this process for each neuron in the network
973840	981840	one by one chosen in random order, updating their states based on the input from all other neurons.
982640	989280	Once we've updated all neurons, we will have completed one iteration of the network inference
989280	995760	and decreased the system's energy by a little bit. We'll keep repeating this process, doing these
995760	1002320	sweeps through all neurons, updating them one at a time based on the current configuration.
1002320	1010000	As we do this, the network will gradually evolve towards a configuration that minimizes the overall
1010000	1017440	energy. At some point, however, we will reach a configuration where flipping any neuron would
1017440	1023760	lead to an increase in energy. So, no further adjustments would be necessary. At that point,
1023760	1030560	the network has converged to a stable configuration, where each neuron's state agrees with the majority
1030560	1036320	vote. This stable configuration represents a local minimum in the energy landscape.
1036960	1042800	Now, you might be wondering, is the network guaranteed to reach such a stable configuration?
1042800	1049440	Could we possibly stumble into a particularly unlucky set of states and get stuck in a never-ending
1049440	1056880	loop of flipping neurons back and forth? In other words, is such iterative flipping of one neuron
1056880	1063600	at a time equivalent to doing a descent along the energy surface? This is where we come back to
1063600	1070160	the point about symmetric weights. It turns out that there is a mathematical proof that I'm not
1070160	1077520	going to cover here, stating that as long as your weights are symmetric, this simple majority vote
1077520	1084240	single neuron update rule is guaranteed to eventually converge to a stable configuration
1084240	1091280	if you do it enough times. To restate it, the Hopfield network can settle into different local
1091280	1097760	minima based on its initial conditions. These local minima in the energy landscape correspond
1097760	1104320	to distinct memories stored in the network. When we initialize the network with a pattern that is
1104320	1110560	similar to one of these memories in some way and let it evolve, it will fall into the nearest
1110560	1117680	local minimum, effectively recalling the complete stored memory, thus performing pattern completion
1117680	1123840	or noise correction. But so far we haven't talked about how we come up with the set of connection
1123840	1130560	weights that encode specific memories in the first place. So let's explore the learning process.
1132560	1138960	Before we move to storing several memories, let's consider memorizing a single pattern of states.
1139600	1143360	That means the network would have a single global minimum,
1143360	1149520	one energy well, and would converge to the same pattern every time no matter where you initialize
1149520	1155680	it. While it has little practical use, it provides a nice starting point to describe the learning
1155680	1163600	procedure. Let's denote the template pattern that we'd like to store as XC, which is a vector
1163600	1171760	packing the states of all neurons, and XCI will denote the ith component, the state of ith neuron
1171760	1178400	encoding the memory. While XI refers to the state of ith neuron in the network in general,
1178400	1187440	which could be tweaked. Revisiting our definition of energy, we want to set Wij so that this quantity
1187440	1195520	would be at its minimal value for the memory pattern. If we plug XI equal to XCI, we get the
1195520	1201920	equation for the energy of the reference pattern as a function of weights, which we want to turn
1201920	1208400	into a global minimum. Notice that we don't really care about the absolute value of that energy.
1208960	1214400	As long as the energy of the desired memory pattern is less than the energy of any other
1214400	1221280	configuration. Now intuitively, the lowest possible energy is obtained when all the
1221280	1228560	connection weights fully align with the state pairs. But when we have just a single pattern,
1228560	1236160	this is very easy to do. All we need is to set the weight Wij to be the product of the corresponding
1236160	1242800	pair of states in the memory pattern. This way, every connection is satisfied and the energy of
1242800	1248480	the network when it is in the state XI becomes the negative of the total number of edges.
1249120	1256000	When the network is in the state XI, any single flip of a neuron would increase the energy,
1256000	1263040	thus making it a stable state. I want to reiterate the crucial point here. If we want to come up with
1263040	1269840	the set of weights that would dig an energy well around some pattern, then all we need to know
1269840	1276720	are the pairwise relationships between states in that pattern. If the two neurons are active
1276720	1282560	together in the source memory, strengthening the connection between them lowers the hopfield
1282560	1288320	energy of that memory, effectively storing it in the weights for associative recall.
1289280	1293440	You may have heard the famous statement from Neuroscience attributed to Donald
1293840	1300640	Hebb, neurons that fire together, wire together. And in fact, what we just did is known as the
1300640	1307840	Hebbian learning rule. Great, so we found a way to make a single pattern a stable state of the
1307840	1315520	network. But we want to store multiple patterns. How do we do that? Here's the key idea. We can
1315520	1322240	simply sum the weights we would get for each pattern separately. So if we have three patterns,
1322320	1327760	X1, X2 and X3, we can set the weights according to the following equation.
1328800	1335200	What this will do is turn each of the patterns into a local minimum. It's pretty straightforward
1335200	1340400	to show mathematically, and if you're interested, I encourage you to check out the references
1340400	1346960	in the video description. However, intuitively, if the patterns you want to store are very
1346960	1354400	different, so they are far away in the state space from each other, then if you first independently
1354400	1361040	dig energy wells around each of them, and then simply add the energy landscapes together,
1361680	1368960	the resulting surface will have local minima in the same three valleys. And this nicely brings
1368960	1376160	us to the limitation of the hopfield networks. There is a limited number of valleys we can sculpt
1376160	1382320	in the energy landscape before they start to interfere with each other. At some point,
1382320	1389680	if we try to store too many patterns, the network will fail to converge to a stored pattern reliably
1389680	1396000	and recall weird in-between kind of memories. The total maximum number of patterns you can
1396000	1403680	store is thus limited and depends only on the size of the network. It is approximately 0.14
1403680	1410720	times the number of neurons. So, if you have a hopfield network of 100 neurons, you can reliably
1410720	1417280	store less than 14 patterns in the best case scenario. If you are unlucky, however, and some
1417280	1424560	patterns are similar to each other or correlated, their energy wells will begin to interfere even
1424560	1430720	before you reach the full capacity. All of this makes vanilla hopfield networks not useful for
1430800	1437520	practical purposes. However, to this day, they provide a powerful and intuitive model of associative
1437520	1444560	memory, a simple network of neuron-like units that can store and retrieve pattern through purely
1444560	1450160	local learning and inference rules. Despite their limitations, hopfield networks have laid the
1450160	1456720	groundwork for more advanced energy-based models. In one of the next videos, we will look at the
1456720	1463520	extension of the hopfield networks known as Boltzmann machines. These generative architectures
1463520	1469520	introduce additional hidden units and stochastic dynamics, allowing them to learn more complex
1469520	1475360	probability distributions. There is also an extension to modern hopfield networks,
1475360	1481600	published in 2016 with John Hopfield himself as one of the authors, but that's a topic for another
1481600	1487040	time. In the meanwhile, I'd like to take a moment and thank Shortform, who are kindly sponsoring
1487040	1493280	today's video. Shortform is a platform that lets you supercharge your reading and gain valuable
1493280	1500320	insights from books. Their unique approach of book guides goes way beyond simple summaries,
1500320	1506720	by providing a comprehensive overview of the material. Not only do you get a concise version
1506720	1514000	of the main points, but you also benefit from related ideas sourced from other books and research
1514000	1520640	papers on the topic. They have an actively growing library of books from all sorts of genres,
1520640	1526080	such as science, health and technology. Not only that, but there is a useful AI-powered
1526080	1532480	browser extension that allows you to generate similar guides for arbitrary content on the
1532480	1537920	internet. Personally, I found Shortform to be really helpful, both when I'm choosing books to
1537920	1544880	read and writing notes and flashcards on the topic. Don't hesitate to bring your reading to the next
1544880	1552400	level by clicking the link down in the description to get 5 dates of unlimited access and 20% off
1552400	1558080	on annual subscription. If you liked the video, share it with your friends, press the like button
1558080	1562640	and subscribe to the channel if you haven't already. Stay tuned for more computational
1562640	1568480	neuroscience and machine learning topics coming up. Goodbye and thank you for the interest in the brain.
1588080	1600480	Thank you for watching and I'll see you in the next video.
