1
00:00:00,000 --> 00:00:08,240
What do nearly all machine learning systems have in common, from GPT and MeJourney to AlphaFold and various models of the brain?

2
00:00:09,080 --> 00:00:16,720
Despite being designed to solve different problems, having completely different architectures, and being trained on different data,

3
00:00:17,080 --> 00:00:19,720
there is something that unites all of them.

4
00:00:20,400 --> 00:00:26,160
A single algorithm that runs under the hood of the training procedures in all of those cases,

5
00:00:26,280 --> 00:00:32,600
this algorithm, called backpropagation, is the foundation of the entire field of machine learning,

6
00:00:33,120 --> 00:00:35,560
although its details are often overlooked.

7
00:00:36,840 --> 00:00:44,280
Surprisingly, what enables artificial networks to learn is also what makes them fundamentally different from the brain and

8
00:00:44,880 --> 00:00:49,160
incompatible with biology. This video is the first in a two-part series.

9
00:00:49,800 --> 00:00:54,840
Today, we will explore the concept of backpropagation in artificial systems and

10
00:00:55,240 --> 00:01:02,360
develop an intuitive understanding of what it is, why it works, and how you could have developed it from scratch yourself.

11
00:01:03,160 --> 00:01:09,920
In the next video, we will focus on synaptic plasticity, enabling learning in biological brains, and

12
00:01:10,360 --> 00:01:19,880
discuss whether backpropagation is biologically relevant, and if not, what kind of algorithms the brain may be using instead. If you're interested, stay tuned.

13
00:01:24,840 --> 00:01:37,000
Despite its transformative impact, it's hard to say who invented backpropagation in the first place, as certain concepts can be traced back to

14
00:01:37,000 --> 00:01:44,520
light needs in 17th century. However, it is believed that the first modern formulation of the algorithm, still in use today,

15
00:01:44,720 --> 00:01:53,320
was published by Seppo Linenma in his master's thesis in 1970, although he did not reference any neural networks explicitly.

16
00:01:54,320 --> 00:02:02,000
Another significant milestone occurred in 1986, when David Rumelhardt, Joffrey Hinton, and Ronald Williams

17
00:02:02,320 --> 00:02:07,360
published a paper titled Learning Representations by Backpropagating Errors.

18
00:02:07,440 --> 00:02:14,120
They applied the backpropagation algorithm to multi-layer perceptrons, a type of a neural network, and

19
00:02:14,680 --> 00:02:17,840
demonstrated for the first time that training with backpropagation

20
00:02:18,520 --> 00:02:28,280
enables the network to successfully solve problems and develop meaningful representations at the hidden neuron level, capturing important regularities in the task.

21
00:02:29,400 --> 00:02:38,720
As the field progressed, researchers scaled up these models significantly and introduced various architectures, but the fundamental principles of training

22
00:02:38,880 --> 00:02:46,480
remained largely unchanged. To gain a comprehensive understanding of what exactly it means to train a network,

23
00:02:46,520 --> 00:02:52,520
let's try to build the concept of backpropagation from the ground up. Consider the following problem.

24
00:02:53,080 --> 00:03:00,160
Suppose you have collected a set of points x, y on the plane, and you want to describe their relationship.

25
00:03:00,760 --> 00:03:06,480
To achieve this, you need to fit a curve y of x that best represents the data.

26
00:03:07,480 --> 00:03:12,120
Since there are infinitely many possible functions, we need to make some assumptions.

27
00:03:12,520 --> 00:03:20,200
For instance, let's assume we want to find a smooth approximation of the data using a polynomial of degree 5.

28
00:03:21,040 --> 00:03:29,400
That means that the resulting curve we are looking for will be a combination of a constant term, a polynomial of degree 0, a

29
00:03:30,120 --> 00:03:36,800
straight line, a parabola, and so on up to a power of 5, each weightened by specific coefficients.

30
00:03:37,040 --> 00:03:44,400
In other words, the equation for the curve is as follows. Where each k is some arbitrary real number.

31
00:03:45,000 --> 00:03:52,560
Our job then becomes finding the configuration of k0 through k5, which leads to the best fitting curve.

32
00:03:52,840 --> 00:03:58,480
To make the problem totally unambiguous, we need to agree on what the best curve even means.

33
00:03:59,040 --> 00:04:06,080
While you can just visually inspect the data points and estimate whether a given curve captures the pattern or not,

34
00:04:06,320 --> 00:04:11,400
this approach is highly subjective and impractical when dealing with large data sets.

35
00:04:12,000 --> 00:04:18,360
Instead, we need an objective measurement, a numerical value that quantifies the quality of a fit.

36
00:04:18,960 --> 00:04:25,320
One popular method is to measure the square distance between data points and the fitted curve.

37
00:04:25,720 --> 00:04:33,640
A high value suggests that the data points are significantly far from the curve, indicating a poor approximation.

38
00:04:34,640 --> 00:04:41,320
Conversely, low values indicate a better fit as the curve closely aligns with the data points.

39
00:04:41,800 --> 00:04:47,960
This measurement is commonly referred to as a loss and the objective is to minimize it.

40
00:04:48,360 --> 00:04:53,800
Now notice that for a fixed data, this distance, the value of the loss,

41
00:04:54,440 --> 00:05:02,280
depends only on the defining characteristics of the curve. In our case, the coefficients from k0 through k5.

42
00:05:03,080 --> 00:05:10,760
This means that it is effectively a function of parameters, so people usually refer to it as a loss function.

43
00:05:11,200 --> 00:05:16,520
It's important not to confuse two different functions we are implicitly dealing with here.

44
00:05:16,520 --> 00:05:23,080
The first one is the function y of x, which has one input number and one output number and

45
00:05:23,440 --> 00:05:29,160
defines the curve itself. It has this polynomial form given by k's.

46
00:05:29,440 --> 00:05:35,120
There are infinitely many such functions and we would like to find the best one. To achieve this,

47
00:05:35,120 --> 00:05:42,360
we introduce a loss function, which instead has six inputs, numbers k0 through k5.

48
00:05:42,920 --> 00:05:47,280
And for each configuration, it constructs the corresponding curve y,

49
00:05:48,120 --> 00:05:57,040
calculates the distance between observed data points and the curve, and outputs a single number, the particular value of the loss.

50
00:05:57,760 --> 00:06:03,960
Our job then becomes finding the configuration of k's that yields a minimum loss or

51
00:06:04,840 --> 00:06:07,840
minimizing the loss function with respect to the coefficients.

52
00:06:07,920 --> 00:06:15,920
Then, plugging these optimal k's into the general equation for the curve will give us the best curve of describing the data.

53
00:06:16,520 --> 00:06:23,120
All right, great, but how do we find this magic configuration of k's that minimizes the loss?

54
00:06:23,560 --> 00:06:25,560
Well, we might need some help.

55
00:06:25,800 --> 00:06:32,520
Let's build a machine called Curve Fitter 6000, designed to simplify manual calculations.

56
00:06:32,520 --> 00:06:39,800
It is equipped with six adjustable knobs for k0 through k5, which we can freely turn.

57
00:06:40,120 --> 00:06:47,120
To begin, we initialize the machine with our data points and then, for each setting of the knobs,

58
00:06:47,120 --> 00:06:50,120
it will evaluate the curve y of x,

59
00:06:50,600 --> 00:06:56,680
compute the distance from it to the data points and print out the value of the loss function.

60
00:06:57,320 --> 00:07:01,640
Now, we can begin twisting the knobs in order to find the minimum loss.

61
00:07:02,280 --> 00:07:08,840
For example, let's start with some initial setting and slightly notch knob number one to the right.

62
00:07:09,240 --> 00:07:15,880
The resulting curve changed as well, and we can see that the value of the loss function slightly decreased.

63
00:07:16,440 --> 00:07:18,840
Great, it means we are on the right track.

64
00:07:19,320 --> 00:07:23,000
Let's turn knob number one in the same direction once again.

65
00:07:23,000 --> 00:07:26,840
Uh-oh, this time the fit gets worse and the loss function increases.

66
00:07:27,720 --> 00:07:30,760
Apparently, that last notch was a bit too much.

67
00:07:30,760 --> 00:07:35,320
So, let's revert the knob to the previous position and try knob two.

68
00:07:35,320 --> 00:07:38,680
And we can keep doing this iteratively many, many times,

69
00:07:39,240 --> 00:07:45,400
nudging each individual knob one at a time to see whether the resulting curve is a better fit.

70
00:07:45,560 --> 00:07:53,000
This is a so-called random perturbation method, since we are essentially wandering in the dark,

71
00:07:53,000 --> 00:07:58,200
not knowing in advance how each adjustment will affect the loss function.

72
00:07:58,200 --> 00:08:01,320
This would certainly work, but it's not very efficient.

73
00:08:01,320 --> 00:08:06,280
Is there a way we can be more intelligent about the knob adjustments?

74
00:08:06,280 --> 00:08:10,760
In the most general case, when the machine is a complete black box,

75
00:08:10,760 --> 00:08:14,920
nothing better than a random perturbation is guaranteed to exist.

76
00:08:14,920 --> 00:08:21,160
However, a great deal of computations, including what's carried out under the hood of our curvefitter,

77
00:08:21,880 --> 00:08:26,120
have a special property to them, something called differentiability

78
00:08:26,120 --> 00:08:30,600
that allows us to compute the optimal knob setting much more efficiently.

79
00:08:31,160 --> 00:08:35,640
We will dive deeper into what differentiability means in just a minute.

80
00:08:35,640 --> 00:08:40,360
But for now, let's quickly see the big picture overview of where we are going.

81
00:08:41,160 --> 00:08:48,040
Our goal would be to upgrade the machine so that it would have a tiny screen next to each knob.

82
00:08:48,600 --> 00:08:54,280
And for any configuration, those screens should say which direction you need to

83
00:08:54,280 --> 00:08:59,240
nudge each knob in order to decrease the loss function and by how much.

84
00:09:00,200 --> 00:09:01,400
Think about it for a second.

85
00:09:02,200 --> 00:09:08,040
We are essentially asking the machine to predict the future and estimate the effect

86
00:09:08,040 --> 00:09:13,880
of the knob adjustment on the loss function without actually performing that adjustment,

87
00:09:13,880 --> 00:09:18,680
calculating the loss and then reverting the knob back like we did previously.

88
00:09:19,240 --> 00:09:23,800
Wouldn't this glance into the future violate some sort of principle?

89
00:09:24,600 --> 00:09:29,640
After all, we are jumping to the result of the computation without performing it.

90
00:09:30,280 --> 00:09:31,720
Sounds like cheating, right?

91
00:09:32,920 --> 00:09:37,960
Well, it turns out that this idea lies on a very simple mathematical foundation

92
00:09:37,960 --> 00:09:41,320
so let's spend the next few minutes building it up from scratch.

93
00:09:43,560 --> 00:09:46,840
All right, let's consider a simpler case first.

94
00:09:46,840 --> 00:09:49,800
Where we freeze five out of six knobs.

95
00:09:49,800 --> 00:09:54,680
For example, suppose someone tells you that the rest of them are already in the optimal position.

96
00:09:55,240 --> 00:09:59,960
So all you need to do is to find the best value for one remaining knob.

97
00:09:59,960 --> 00:10:05,560
Essentially, the machine now has only one variable parameter k1 that we can tweak.

98
00:10:06,120 --> 00:10:09,480
And so the loss function is also a simpler function

99
00:10:09,480 --> 00:10:15,480
which accepts one number, the knob setting, and outputs another number, the loss value.

100
00:10:15,480 --> 00:10:21,960
As a function of one variable, it can be conveniently visualized as a graph in a two-dimensional plane

101
00:10:21,960 --> 00:10:25,720
which captures the relationship between the input and the output.

102
00:10:25,720 --> 00:10:32,200
For example, it may have this shape right here and our goal is to find this value of k1

103
00:10:32,200 --> 00:10:35,080
which corresponds to the minimum of the loss function.

104
00:10:35,080 --> 00:10:38,680
But we don't have access to the true underlying shape.

105
00:10:38,680 --> 00:10:42,120
All we can do is to set the knob at a chosen position

106
00:10:42,680 --> 00:10:46,360
and kind of query the machine for the value of the loss.

107
00:10:46,360 --> 00:10:52,280
In other words, we can only sample individual points along the function we're trying to minimize.

108
00:10:52,840 --> 00:10:58,440
And we are essentially blind to how the function behaves in between the known points

109
00:10:58,440 --> 00:11:00,040
before we sample them.

110
00:11:00,040 --> 00:11:03,560
But suppose we would like to know something more about the function.

111
00:11:03,560 --> 00:11:05,640
Not just each value at each point.

112
00:11:06,520 --> 00:11:10,520
For example, whether at this point the function is going up or down.

113
00:11:11,160 --> 00:11:14,760
This information will ultimately guide our adjustments.

114
00:11:15,320 --> 00:11:20,040
Because if you know that the function is going down as you increase the input,

115
00:11:20,040 --> 00:11:23,080
turning the knob to the right is a safe bet,

116
00:11:23,080 --> 00:11:27,000
since you are guaranteed to decrease the loss with this manipulation.

117
00:11:27,560 --> 00:11:31,640
Let's put this notion of going up or down around a point

118
00:11:31,640 --> 00:11:33,880
on a stronger mathematical ground.

119
00:11:33,880 --> 00:11:38,360
Suppose we have just sampled the point x0, y0 on this graph.

120
00:11:38,360 --> 00:11:43,640
What we can do is increase the input by a small amount delta x.

121
00:11:43,640 --> 00:11:48,360
This new adjusted input will result in a new value of y,

122
00:11:48,360 --> 00:11:52,040
which will differ from the old value by some delta y.

123
00:11:52,600 --> 00:11:56,440
This delta depends on the magnitude of our adjustment.

124
00:11:56,440 --> 00:12:01,080
For example, if we take a step delta x, which is 10 times smaller,

125
00:12:01,080 --> 00:12:05,240
delta y will also be approximately 10 times as small.

126
00:12:06,680 --> 00:12:11,960
This is why it makes sense to take the ratio delta y over delta x,

127
00:12:11,960 --> 00:12:16,280
the amount of change in the output per unit change in the input.

128
00:12:16,920 --> 00:12:21,640
Graphically, this ratio corresponds to a slope of a straight line.

129
00:12:21,640 --> 00:12:26,360
Going through the points x0, y0 and x0 plus delta x,

130
00:12:26,440 --> 00:12:27,960
y0 plus delta y.

131
00:12:28,920 --> 00:12:32,680
Notice that as we take smaller and smaller steps,

132
00:12:32,680 --> 00:12:37,400
this straight line will more and more accurately align with the graph

133
00:12:37,400 --> 00:12:40,200
in the neighborhood of the point x0, y0.

134
00:12:40,920 --> 00:12:47,320
Let's take a limit of this ratio as delta x goes to infinitely small values.

135
00:12:47,320 --> 00:12:51,800
Then this limiting case value, which this ratio converges to

136
00:12:51,800 --> 00:12:57,640
for infinitesimally small delta x's, is what is called the derivative oa function,

137
00:12:58,200 --> 00:13:01,320
and it is denoted by dy over dx.

138
00:13:01,960 --> 00:13:05,960
Visually, the derivative oa function at some point

139
00:13:05,960 --> 00:13:09,880
is the slope of the line that is tangent to the graph,

140
00:13:09,880 --> 00:13:13,400
and thus corresponds to the instantaneous rate of change,

141
00:13:13,400 --> 00:13:17,160
or steepness of that function around that point.

142
00:13:17,160 --> 00:13:21,640
But different points along the graph might have different steepness values,

143
00:13:21,960 --> 00:13:26,520
so the derivative of the entire function is not a single number.

144
00:13:26,520 --> 00:13:32,520
In fact, the derivative dy by dx is itself a function of x

145
00:13:32,520 --> 00:13:40,200
that takes an arbitrary value of x and outputs the local steepness of y of x at that point.

146
00:13:40,200 --> 00:13:45,240
This definition assigns to every function its derivative alter ego.

147
00:13:45,240 --> 00:13:49,160
Another function operating on the same input domain,

148
00:13:49,160 --> 00:13:54,120
which carries information about the steepness of the original function.

149
00:13:54,120 --> 00:13:55,880
There is a bit of a subtlety.

150
00:13:55,880 --> 00:13:59,000
Strictly speaking, the derivative may not exist

151
00:13:59,000 --> 00:14:02,280
if the function doesn't have a steepness around some point.

152
00:14:02,920 --> 00:14:06,680
For example, if it has sharp corners or discontinuities.

153
00:14:07,720 --> 00:14:12,360
However, for the remainder of the video, we are going to assume that all functions we are

154
00:14:12,360 --> 00:14:16,600
dealing with are smooth, so that the derivative always exists.

155
00:14:17,560 --> 00:14:21,080
This is a reasonable claim, because we can control

156
00:14:21,080 --> 00:14:24,520
what sort of functions go into our models when we build them.

157
00:14:25,080 --> 00:14:29,960
And people usually restrict everything to smooth or differentiable functions

158
00:14:29,960 --> 00:14:32,120
to make all the math work out nicely.

159
00:14:32,680 --> 00:14:33,800
All right, great.

160
00:14:33,800 --> 00:14:37,720
Now, along with the underlying loss as a function of k1,

161
00:14:37,720 --> 00:14:42,040
which is hidden from us, we can also reason about its derivative.

162
00:14:42,680 --> 00:14:46,120
Another function of k1, which we also don't know,

163
00:14:46,120 --> 00:14:49,800
that is equal to the steepness of the loss function at that point.

164
00:14:51,160 --> 00:14:55,400
Let's suppose that similarly to how we can query the loss function

165
00:14:55,400 --> 00:14:59,000
by running our machine and obtaining individual samples.

166
00:14:59,720 --> 00:15:04,040
There is a mechanism for us to sample the derivative function as well.

167
00:15:05,400 --> 00:15:11,880
So, for every input value of k1, the machine will output the value of the loss

168
00:15:11,880 --> 00:15:15,640
and the local steepness of the loss function around that point.

169
00:15:16,280 --> 00:15:20,600
Notice that this derivative information is exactly the sort of

170
00:15:20,600 --> 00:15:25,080
look into the future we were looking for to make smarter knob adjustments.

171
00:15:25,720 --> 00:15:30,600
For example, let's use it to efficiently find the optimal value of k1.

172
00:15:31,400 --> 00:15:33,000
What we can do is the following.

173
00:15:33,880 --> 00:15:36,600
First, start at some random position.

174
00:15:37,560 --> 00:15:40,120
Ask the machine for the value of the loss

175
00:15:40,120 --> 00:15:43,320
and the derivative of the loss function at that position.

176
00:15:44,440 --> 00:15:48,600
Take a tiny step in the direction opposite of the derivative.

177
00:15:49,240 --> 00:15:53,240
If the derivative is negative, it means that the function is going down.

178
00:15:53,960 --> 00:15:56,520
And so, if we want to arrive at the minimum,

179
00:15:56,520 --> 00:16:00,440
we need to move in the direction of increasing value of k1.

180
00:16:01,080 --> 00:16:06,280
Repeat this procedure until you reach the point where the derivative is zero,

181
00:16:06,280 --> 00:16:10,600
which essentially corresponds to the minimum where the tangent line is flat.

182
00:16:11,240 --> 00:16:15,000
Essentially, each adjustment in such a guided fashion

183
00:16:15,000 --> 00:16:21,080
works kind of like a ball rolling down the hill along the graph until it reaches a valley.

184
00:16:23,480 --> 00:16:27,560
Although in the beginning we froze five out of six knobs for simplicity,

185
00:16:28,120 --> 00:16:31,880
this process is easily carried out to higher dimensions.

186
00:16:32,840 --> 00:16:38,600
For example, suppose now we are free to tweak two different knobs, k1 and k2.

187
00:16:39,640 --> 00:16:45,800
The loss would become a function of two variables, which can be visualized as a surface.

188
00:16:46,680 --> 00:16:48,680
But what about the derivative?

189
00:16:48,680 --> 00:16:52,920
Recall that by definition, the derivative at each point

190
00:16:52,920 --> 00:16:57,800
tells us how the output changes per unit change of the input.

191
00:16:57,800 --> 00:16:59,880
But now we have two different inputs.

192
00:17:00,440 --> 00:17:03,720
Should we nudge only k1, k2 or both?

193
00:17:05,640 --> 00:17:09,800
Essentially, our function will have two different derivatives

194
00:17:10,440 --> 00:17:17,000
that are usually called partial derivatives because of this ambiguity which input to nudge.

195
00:17:17,000 --> 00:17:21,400
Namely, when we have two knobs, the derivative of the loss function

196
00:17:21,400 --> 00:17:25,080
with respect to parameter k1 is written like this.

197
00:17:25,320 --> 00:17:33,480
It is how much the output changes per unit change in k1 if you hold k2 constant.

198
00:17:34,120 --> 00:17:39,320
And conversely, this expression tells you the rate of change of the output

199
00:17:39,320 --> 00:17:43,480
if you hold k1 constant and slightly nudge k2.

200
00:17:44,040 --> 00:17:50,760
Geometrically, you can imagine slicing the surface with planes parallel to the axes,

201
00:17:50,760 --> 00:17:53,960
intersecting at the point of interest k1 k2.

202
00:17:54,600 --> 00:18:00,440
So that each of the two cross sections is like a one-dimensional graph of the loss

203
00:18:00,440 --> 00:18:04,680
as a function of one variable while the other one is kept constant.

204
00:18:05,320 --> 00:18:09,240
Then the slope of a tangent line at each cross section

205
00:18:09,240 --> 00:18:13,640
will give you a corresponding partial derivative of the loss at that point.

206
00:18:14,760 --> 00:18:18,520
While thinking about partial derivatives as two separate surfaces,

207
00:18:19,080 --> 00:18:22,360
one for each variable, is a perfectly valid way.

208
00:18:23,080 --> 00:18:29,880
People usually plug the two different values into a vector called a gradient vector.

209
00:18:29,880 --> 00:18:36,120
Essentially, this is a mapping from two input values to another two numbers

210
00:18:36,120 --> 00:18:42,680
where the first signifies how much the output changes per tiny change in the first input.

211
00:18:43,400 --> 00:18:45,400
And similarly, for the second input.

212
00:18:46,520 --> 00:18:50,680
Geometrically, this vector points in the direction of steepest ascent.

213
00:18:51,480 --> 00:18:56,360
So if you want to minimize a function, like in the case for our loss,

214
00:18:57,080 --> 00:19:01,480
we need to take steps in the direction opposite to this gradient.

215
00:19:02,840 --> 00:19:09,960
This iterative procedure of nudging the parameters in the direction opposite of the gradient vector

216
00:19:09,960 --> 00:19:13,720
is called gradient descent, which you have probably heard of.

217
00:19:13,720 --> 00:19:18,520
This is analogous to a ball rolling down the hill for the two-dimensional case.

218
00:19:18,520 --> 00:19:23,640
And the partial derivatives essentially tell you which direction is downhill.

219
00:19:24,440 --> 00:19:29,080
Going beyond two dimensions is impossible to visualize directly,

220
00:19:29,080 --> 00:19:31,480
but the math stays exactly the same.

221
00:19:32,440 --> 00:19:37,000
For instance, if we are now free to tweak all the six knobs,

222
00:19:37,000 --> 00:19:41,160
the loss function is a hyper surface in six dimensions.

223
00:19:41,800 --> 00:19:45,800
And the gradient vector now has six numbers packed into it.

224
00:19:46,360 --> 00:19:50,280
But it still points in the direction of steepest ascent.

225
00:19:50,280 --> 00:19:55,160
So if we iteratively take small steps in the direction opposite to it,

226
00:19:55,880 --> 00:20:00,440
we are going to roll the ball down the hill in six dimensions

227
00:20:00,440 --> 00:20:03,640
and eventually reach the minimum of the loss function.

228
00:20:04,520 --> 00:20:06,040
Great, let's back up a bit.

229
00:20:06,760 --> 00:20:11,160
Remember how we were looking for ways to add screens next to each knob

230
00:20:11,160 --> 00:20:14,280
that would give us the direction of optimal adjustment?

231
00:20:15,240 --> 00:20:20,600
Well, it is essentially nothing more but the components of the gradient vector.

232
00:20:20,600 --> 00:20:25,160
If at a particular configuration, the partial derivative of the loss

233
00:20:25,160 --> 00:20:32,520
with respect to k1 is positive, it means that increasing k1 will lead to increased loss.

234
00:20:33,080 --> 00:20:37,400
So we need to decrease the value of the knob by turning it to the left.

235
00:20:38,040 --> 00:20:40,680
And similarly for all other parameters.

236
00:20:41,640 --> 00:20:46,760
This is how the derivatives serve as these windows into the future

237
00:20:46,760 --> 00:20:51,720
by providing us with information about local behavior of the function.

238
00:20:51,720 --> 00:20:55,160
And once we have a way of accessing the derivative,

239
00:20:55,160 --> 00:21:01,160
we can perform gradient descent and efficiently find the minimum of the loss function,

240
00:21:01,160 --> 00:21:03,720
thus solving the optimization problem.

241
00:21:04,280 --> 00:21:07,160
However, there is an elephant in a room.

242
00:21:07,160 --> 00:21:11,960
So far we have implicitly assumed the derivative information is given to us.

243
00:21:12,520 --> 00:21:16,200
Or that we can sample the derivative at a given point.

244
00:21:16,200 --> 00:21:19,560
Similarly to how we sample the loss function itself

245
00:21:19,560 --> 00:21:21,880
by running the calculation of the machine.

246
00:21:21,880 --> 00:21:24,600
But how do you actually find the derivative?

247
00:21:24,600 --> 00:21:29,320
As we will see further, this is the main purpose of the back propagation algorithm.

248
00:21:30,120 --> 00:21:35,560
Essentially, the way we find derivatives of arbitrarily complex functions is the following.

249
00:21:36,200 --> 00:21:39,960
First, there are a handful of building blocks to begin with.

250
00:21:39,960 --> 00:21:43,960
Simple functions, derivatives of which are known from calculus.

251
00:21:44,680 --> 00:21:48,600
These are the kind of derivative formulas you often memorize in college.

252
00:21:49,400 --> 00:21:55,880
For example, if the function is linear, it's pretty clear that its derivative will be a constant,

253
00:21:55,880 --> 00:22:01,640
equal to the slope of that line everywhere, which coincides with its own tangent line.

254
00:22:02,600 --> 00:22:08,120
A parabola x squared becomes more steep as you increase x.

255
00:22:08,120 --> 00:22:10,680
And its derivative is actually 2x.

256
00:22:11,560 --> 00:22:16,680
In fact, there is a more general formula for the derivative of x to the power of n.

257
00:22:17,480 --> 00:22:22,760
Similarly, derivatives of the exponent and logarithm can be written down explicitly.

258
00:22:23,640 --> 00:22:28,200
But these are just individual examples of simple, well-known functions.

259
00:22:28,840 --> 00:22:35,960
In order to compute arbitrary derivatives, we need a way to combine such atomic building blocks

260
00:22:35,960 --> 00:22:39,480
together. There are a few rules how to do it.

261
00:22:40,280 --> 00:22:46,440
For instance, the derivative of a sum of two functions is the sum of the derivatives.

262
00:22:46,440 --> 00:22:50,680
There is also a formula for the derivative of a product of two functions.

263
00:22:51,560 --> 00:22:57,320
This gives you a way to compute things like the derivative of 3x squared minus equal to the power

264
00:22:57,320 --> 00:23:04,600
of x. But to complete the picture and to be able to find derivatives of almost everything,

265
00:23:04,600 --> 00:23:11,800
we need one other rule called the chain rule, which powers the entire field of machine learning.

266
00:23:12,520 --> 00:23:17,320
It tells you how to compute the derivative of a combination of two functions,

267
00:23:17,320 --> 00:23:22,680
when one of them is an input to another. Here is a way to reason about this.

268
00:23:23,640 --> 00:23:29,800
Suppose you take one of those simpler machines, which receives a single input x that you can

269
00:23:29,800 --> 00:23:38,600
vary with an ALP, and spits out an output, j of x. Now, you take a second machine of this kind,

270
00:23:38,600 --> 00:23:46,600
which performs a different function, f of x. What would happen if you connect them in sequence,

271
00:23:46,600 --> 00:23:52,520
so that the output of the first machine is fed into the second one as an input?

272
00:23:53,480 --> 00:24:00,680
Notice that such a construction can be thought of as a single function, which also receives one

273
00:24:00,680 --> 00:24:07,240
input number and gives an output by computing a more complicated function, which is a composition

274
00:24:07,240 --> 00:24:14,280
of the two simpler functions. In fact, if you put a black box around it to conceal the fact

275
00:24:14,280 --> 00:24:21,160
that there are actually two machines operating sequentially, you can treat it as a single machine

276
00:24:21,160 --> 00:24:28,040
and ask, well, if I notch the input on one end, how will it affect the output on another end?

277
00:24:28,760 --> 00:24:32,600
In other words, what is the derivative of the resulting function?

278
00:24:33,880 --> 00:24:40,520
Suppose we know the individual derivatives of the two machines, f and j. If the knob is set at

279
00:24:40,520 --> 00:24:49,560
some value x, local steepness of the first function is evaluated at x. However, the number that is

280
00:24:49,560 --> 00:24:55,720
fed into the second machine is not x, because it was already processed by the first function.

281
00:24:56,760 --> 00:25:03,480
So, the thing that is being plugged into the second function is j of x. And so, the local rate of

282
00:25:03,480 --> 00:25:12,360
change of the second machine is thus the derivative of f evaluated at the point j of x. Now, imagine

283
00:25:12,360 --> 00:25:19,640
you notch the knob x by a tiny amount, delta. That input notch, when it comes out of the first machine,

284
00:25:19,640 --> 00:25:26,120
will be multiplied by the derivative of j, since the derivative is the rate of change in the output

285
00:25:26,120 --> 00:25:32,840
per unit change of the input. So, after the first function, the output will increase by delta,

286
00:25:32,840 --> 00:25:40,360
multiplied by the derivative of j. This expression is essentially a tiny notch in the input to the

287
00:25:40,360 --> 00:25:47,640
second machine, whose derivative at that point is given by this expression. This means that for

288
00:25:47,640 --> 00:25:56,360
each delta increase in the input, we bump the output by this much. Hence, the derivative when

289
00:25:56,360 --> 00:26:04,360
you divide that by delta will look like this. You can think about it as a set of three interconnected

290
00:26:04,360 --> 00:26:11,800
cog wheels, where the first one represents the input knob x. And the other two wheels are functions,

291
00:26:11,800 --> 00:26:18,600
j of x and f of j of x, respectively. When you notch the first wheel, it induces a

292
00:26:18,600 --> 00:26:24,360
notch in the middle wheel and the amplitude of that change is given by the derivative of j,

293
00:26:25,080 --> 00:26:30,840
which in turn causes the third wheel to rotate, and the amplitude of that resulting

294
00:26:30,840 --> 00:26:38,680
notch is given by changing the derivatives together. Alright, great. Now we have a straightforward way

295
00:26:38,680 --> 00:26:45,320
of obtaining a derivative of any arbitrarily complex function, as long as it can be decomposed

296
00:26:45,320 --> 00:26:51,880
into building blocks. Simple functions with explicit derivative formulas, such as summations,

297
00:26:51,880 --> 00:26:59,080
multiplications, exponents, logarithms, etc. But how can it be used to find the best curve

298
00:26:59,080 --> 00:27:05,320
using our curve fitter? The big picture we are aiming for is the following. For each of our

299
00:27:05,320 --> 00:27:12,040
parameter knobs, we will write down its effect on the loss in terms of simple, easily differentiable

300
00:27:12,040 --> 00:27:19,720
operations. Once we have that sequence of building blocks, no matter how long, we should be able to

301
00:27:19,720 --> 00:27:25,960
sequentially apply the chain rule to each of them in order to find the value of the derivative

302
00:27:25,960 --> 00:27:32,520
of the loss function with respect to each of the input knobs and perform iterative gradient descent

303
00:27:32,520 --> 00:27:39,720
to minimize the loss. Let's see an example of this. First, we are going to create a knob

304
00:27:39,720 --> 00:27:46,120
for each number the loss function can possibly depend on. This obviously includes the parameters,

305
00:27:46,680 --> 00:27:53,240
but there is also the data itself, coordinates of points to which we are fitting the curve

306
00:27:53,240 --> 00:28:00,120
in the first place. Now, during optimization, the data points are set in stone, so changing them

307
00:28:00,120 --> 00:28:07,480
in order to obtain a lower loss would make no sense. However, for conceptual purposes,

308
00:28:07,480 --> 00:28:14,360
we can think about these values as fixed knobs set in one position so that we cannot nudge them.

309
00:28:15,320 --> 00:28:21,880
Once we have all the existing numbers being fed into the machine, we can start to break down the

310
00:28:21,880 --> 00:28:29,640
loss calculation. Remember, by definition, it is the sum of squared vertical distances

311
00:28:29,640 --> 00:28:37,240
from each point to the curve parameterized by k's. So, for instance, let's take the first

312
00:28:37,320 --> 00:28:47,560
data point, x1, y1, multiply the x coordinate by k1, add that to the squared value of x1 multiplied

313
00:28:47,560 --> 00:28:56,440
by k2, and so on for other k's, including the constant term k0. This sum of weight and powers of

314
00:28:56,440 --> 00:29:05,960
x1 is the value of y predicted by the current curve, f of x1. Let's call it y1 hat. Next,

315
00:29:05,960 --> 00:29:12,440
we need to take the squared difference between the actual value and the predicted value. This is

316
00:29:12,440 --> 00:29:18,600
how much the first data point contributes to the resulting value of the loss function.

317
00:29:19,800 --> 00:29:27,480
Repeating the same procedure for all remaining data points and summing up the resulting squared

318
00:29:27,480 --> 00:29:35,800
distances gives us the overall total loss that we are trying to minimize. The computation we just

319
00:29:35,800 --> 00:29:42,920
performed, finding the value of the loss for a given configuration of parameter and data knobs,

320
00:29:43,560 --> 00:29:51,400
is known as the forward step. The entire sequence of calculations can be visualized

321
00:29:51,400 --> 00:29:58,920
as this kind of computational graph, where each node is some simple operation like addition or

322
00:29:58,920 --> 00:30:07,400
multiplication. Forward step then corresponds to computations flowing from left to right.

323
00:30:07,400 --> 00:30:14,040
But to perform optimization, we also need information about gradients, how each node

324
00:30:14,040 --> 00:30:21,880
influences the loss. Now we are going to do what's known as the backward step, and unroll the sequence

325
00:30:21,880 --> 00:30:29,240
of calculations in reverse order to find derivatives. What makes the backward step possible

326
00:30:29,240 --> 00:30:35,400
is the fact that every node in our compute graph is an easily differentiable operation.

327
00:30:35,400 --> 00:30:42,920
Think of individual nodes as these tiny machines which simply add, multiply or take powers. We

328
00:30:42,920 --> 00:30:50,040
know their derivatives, and because their outputs are connected sequentially, we can apply the chain

329
00:30:50,040 --> 00:30:58,840
rule. This means that for each node we can find its gradient, the partial derivative of the output

330
00:30:58,840 --> 00:31:07,320
loss with respect to that node. Let's see how it can be done. Consider a region of the compute

331
00:31:07,320 --> 00:31:15,160
graph, where two number nodes A and B are being fed into a machine that performs addition, and its

332
00:31:15,160 --> 00:31:22,040
result A plus B is further processed by the system to compute the overall output L.

333
00:31:23,240 --> 00:31:29,640
Suppose we already computed the gradient of A plus B earlier, so that we know how

334
00:31:29,640 --> 00:31:37,480
nudging the sum will affect the output. The question is, what are individual gradients of A and B?

335
00:31:37,800 --> 00:31:46,600
Well, intuitively, if you nudge A by sum amount, A plus B will be nudged by this same amount,

336
00:31:46,600 --> 00:31:53,000
so the gradient or the partial derivative of the loss with respect to A is the same as the gradient

337
00:31:53,000 --> 00:32:00,440
of the sum, and similarly for B. This can be seen more formally by writing down the chain rule

338
00:32:00,440 --> 00:32:07,240
and noticing that the derivative of A plus B with respect to A is just one. In other words,

339
00:32:07,320 --> 00:32:13,720
when you encounter this situation in the compute graph, then the gradient of the sum

340
00:32:13,720 --> 00:32:20,200
just simply propagates into the gradients of the nodes that plug into the sum machine.

341
00:32:20,200 --> 00:32:26,040
Another possible scenario is when A and B are multiplied. Just like before,

342
00:32:26,040 --> 00:32:32,280
suppose we know the gradient of their product because it was computed before. In this case,

343
00:32:32,280 --> 00:32:39,320
individual nudge to A will be scaled by a factor of B, so the product will be nudged

344
00:32:39,320 --> 00:32:46,600
B times as much, which propagates into the output. So, whatever the derivative of the

345
00:32:46,600 --> 00:32:53,720
output with respect to the product of A B is, the output derivative with respect to A

346
00:32:53,720 --> 00:33:00,440
will get scaled by a factor of B, and vice versa for the gradient of B. Once again,

347
00:33:00,440 --> 00:33:07,800
it can be seen more formally by examining the chain rule. In other words, the multiplication node

348
00:33:07,800 --> 00:33:14,920
in the compute graph distributes the downstream gradient across incoming nodes by multiplying

349
00:33:14,920 --> 00:33:21,960
it crossways by their values. Similar rules can be easily formulated for other building block

350
00:33:21,960 --> 00:33:28,360
calculations, such as raising a number to a power or taking the logarithm. Finally,

351
00:33:28,360 --> 00:33:33,960
when a single node takes part in multiple branches of the compute graph, gradients from the

352
00:33:33,960 --> 00:33:39,640
corresponding branches are simply added together. Indeed, suppose you have the following structure

353
00:33:39,640 --> 00:33:46,280
in the graph, where the C-minode A plugs into two different operations that contribute to

354
00:33:46,280 --> 00:33:53,560
the overall loss. Then, if you nudge A by delta, the output will be simultaneously nudged by this

355
00:33:53,560 --> 00:34:00,520
derivative from the first branch and this derivative from the second branch. So, the overall effect of

356
00:34:00,520 --> 00:34:07,320
nudging A will be the sum of the two gradients. Alright, great. Now that we have constructed

357
00:34:07,320 --> 00:34:14,360
a computational graph and established how to process individual chunks of it, we can just

358
00:34:14,360 --> 00:34:20,520
sequentially apply those rules starting from the output and working our way backwards.

359
00:34:21,480 --> 00:34:26,840
For instance, the rightmost node in the graph is the resulting value of the loss function.

360
00:34:26,840 --> 00:34:33,240
How does the incremental change in that node affect the output? Well, it is the output,

361
00:34:33,240 --> 00:34:39,960
so its gradient is by definition equal to 1. Next, the loss function is the sum of many delta

362
00:34:39,960 --> 00:34:46,200
y's squared. We know what to do with the summation node. It just copies whatever the gradient value

363
00:34:46,200 --> 00:34:52,840
is to the right of it into all incoming nodes. Consequently, the gradients of all delta y's

364
00:34:52,840 --> 00:35:00,600
squared will also be equal to 1. Each of those nodes is the squared value of the corresponding delta y

365
00:35:00,600 --> 00:35:05,880
and we know how to differentiate this squaring operation. The derivative of the loss function

366
00:35:05,880 --> 00:35:12,360
with respect to delta y1 will be 2 times the delta y1, which is just the number we found

367
00:35:12,360 --> 00:35:18,040
during the forward calculation. And we can keep doing this propagation of sequential derivative

368
00:35:18,040 --> 00:35:24,280
calculation backwards along our compute graph until we reach the leftmost nodes,

369
00:35:24,280 --> 00:35:30,360
which are the data and parameter knobs. The derivatives of the loss with respect to the input

370
00:35:30,360 --> 00:35:36,920
data don't really matter, but the derivatives with respect to the parameters is exactly what we want.

371
00:35:36,920 --> 00:35:43,400
Once these parameter gradients are found, we can perform one iteration of gradient descent.

372
00:35:43,400 --> 00:35:48,600
Namely, we are going to slightly tweak the knobs in the directions opposite to the gradient.

373
00:35:49,160 --> 00:35:55,000
The exact magnitude of each adjustment being the negative product of the gradient

374
00:35:55,000 --> 00:36:02,120
and some small number called the learning rate, for example, 0.01. Note that after the adjustment

375
00:36:02,120 --> 00:36:07,560
is performed, the configuration of the machine and the resulting loss are different.

376
00:36:08,280 --> 00:36:12,680
And so the old gradient values we found no longer hold.

377
00:36:13,720 --> 00:36:20,360
So we need to run the forward and backward calculations once again to obtain updated

378
00:36:20,360 --> 00:36:28,040
gradients and the new decreased loss. Performing this loop of forward pass, backward pass,

379
00:36:28,040 --> 00:36:33,640
notch, repeat is the essence of training every modern machine learning system.

380
00:36:34,200 --> 00:36:39,640
And exactly the same algorithm is used today in even the most complicated models.

381
00:36:39,640 --> 00:36:44,600
As long as the problem you are trying to solve with a given model architecture

382
00:36:44,600 --> 00:36:50,760
can be decomposed into individual operations that are differentiable, you can sequentially

383
00:36:50,760 --> 00:36:56,680
apply the chain rule many times to arrive at the optimal setting of the parameters.

384
00:36:56,680 --> 00:37:01,880
For instance, a feed-forward neural network is essentially a bunch of multiplications and

385
00:37:01,880 --> 00:37:07,560
summations with a few non-linear activation functions sprinkled between the layers.

386
00:37:07,560 --> 00:37:13,800
Each of those atomic computations is differentiable, so you can construct the compute graph

387
00:37:13,800 --> 00:37:21,080
and run the backward pass on it to find how each parameter, like connection weights between neurons,

388
00:37:21,080 --> 00:37:26,440
influence the loss function. And because neural networks, given enough neurons,

389
00:37:26,440 --> 00:37:33,080
can in theory approximate any function imaginable, we can create a large enough sequence of these

390
00:37:33,080 --> 00:37:39,000
building block mathematical machines to solve problems such as classifying images and even

391
00:37:39,000 --> 00:37:44,200
generating new text. This seems like a very elegant and efficient solution.

392
00:37:44,200 --> 00:37:50,280
After all, if you want to solve the optimization problem, derivatives tell you exactly which

393
00:37:50,280 --> 00:37:55,800
adjustments are necessary. But how similar is this to what the brain actually does?

394
00:37:56,520 --> 00:38:03,000
When we learn to walk, speak and read, is the brain also minimizing some sort of loss function?

395
00:38:03,000 --> 00:38:08,040
Does it calculate derivatives? Or could it be doing something totally different?

396
00:38:08,680 --> 00:38:13,320
In the next video, we are going to dive into the world of synaptic plasticity

397
00:38:13,320 --> 00:38:16,120
and talk about how biological neural networks learn.

398
00:38:17,080 --> 00:38:19,880
In keeping with the topic of biological learning,

399
00:38:19,880 --> 00:38:24,760
I'd like to take a moment to give a shout out to Shortform, a longtime partner of this channel.

400
00:38:25,720 --> 00:38:29,720
Shortform is a platform which lets you take your reading to the next level.

401
00:38:30,440 --> 00:38:37,080
They offer book guides, which are supercharged book summaries. Not only do you get the condensed

402
00:38:37,080 --> 00:38:43,720
version of all the key points, but they are also supplemented by ideas from related sources,

403
00:38:43,800 --> 00:38:49,160
such as other books and research papers. I really love this feature because it allows

404
00:38:49,160 --> 00:38:53,560
you to get the big picture overview and promotes the interlinking of ideas.

405
00:38:54,120 --> 00:38:59,240
The existing library contains books from a variety of genres, including science,

406
00:38:59,240 --> 00:39:05,000
education and technology, and new books are being added every week. Personally, I found

407
00:39:05,000 --> 00:39:09,880
Shortform to be really valuable when it comes to choosing books to read, as well as taking

408
00:39:09,880 --> 00:39:15,560
efficient notes. Don't hesitate to give it a try by following the link down in the description

409
00:39:15,560 --> 00:39:20,600
to get 5 days of unlimited access and 20% off the annual membership.

410
00:39:21,240 --> 00:39:25,320
If you enjoyed this video, press the like button, share it with your friends and colleagues,

411
00:39:25,320 --> 00:39:30,280
and subscribe to the channel if you haven't already. Stay tuned for more interesting topics

412
00:39:30,280 --> 00:39:46,200
coming up. Goodbye and thank you for the interest in the brain.

413
00:40:00,280 --> 00:40:01,660
you

