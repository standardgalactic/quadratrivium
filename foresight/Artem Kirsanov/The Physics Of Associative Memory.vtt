WEBVTT

00:00.000 --> 00:06.000
Consider the following scenario. You are at a party when you hear a short snippet of your favorite song.

00:09.520 --> 00:15.360
Almost instantly, your brain recalls the lyrics of that song and many related memories,

00:15.920 --> 00:19.680
such as attending a recent concert featuring that artist.

00:22.720 --> 00:29.440
It seems very natural and unimpressive. After all, people can recall information all the time.

00:29.520 --> 00:34.400
However, if you think about it, this problem is computationally non-trivial.

00:35.520 --> 00:41.280
Let's put ourselves in the shoes of evolution and try to come up with an algorithm for the brain

00:41.280 --> 00:48.240
to solve it. The first approach that comes to mind is to actually store some kind of a database

00:48.240 --> 00:54.560
of all the songs you've heard a sufficient number of times, along with related information,

00:54.560 --> 01:00.560
like the title and lyrics. When an audio fragment of an unknown song is received,

01:00.560 --> 01:06.960
we can scan through all the songs in our database, find the one that has a close enough match,

01:06.960 --> 01:13.920
and retrieve its lyrics. However, the search space of every song I've ever heard

01:13.920 --> 01:20.480
is astronomically large, and it's even larger when considering every single memory you've formed

01:20.480 --> 01:25.520
since childhood. Performing an exhaustive search would be simply impossible.

01:26.720 --> 01:33.440
Yet, you seem to have no problem instantly recognizing familiar stimuli and finding associations

01:33.440 --> 01:40.640
between them. So, how does the brain accomplish this so quickly? In this video, we will lay the

01:40.640 --> 01:46.480
foundation for a new paradigm of information storage and retrieval, which is more in line with

01:46.560 --> 01:54.080
biology, and actually build one of the simpler models of this process, known as Hopfield Networks,

01:55.040 --> 02:02.320
developed by John Hopfield in 1982, who laid an important groundwork for many ideas in both

02:02.320 --> 02:16.320
neuroscience and machine learning. If you're interested, stay tuned!

02:17.440 --> 02:24.160
Just to reiterate, we need a way to somehow query what we know and find associations between

02:24.160 --> 02:30.480
existing memories and new inputs without explicitly checking individual entries for a match,

02:30.480 --> 02:37.440
which seems like an impossible problem. However, we can draw insights from a seemingly unrelated

02:37.440 --> 02:43.840
field of molecular biology, and in particular, a concept known as Leventhold's paradox of

02:43.840 --> 02:51.360
protein folding. As you may know, proteins are long chains of amino acids that fold into specific

02:51.360 --> 02:57.120
three-dimensional structures which determine their function. The number of possible structural

02:57.120 --> 03:03.200
configurations a protein can take, considering all the different ways you can arrange the atoms

03:03.200 --> 03:09.600
of an amino acid chain in three-dimensional space, is absolutely enormous. Given the number of

03:09.600 --> 03:14.880
possibilities, it seems like it would take an astronomical amount of time for a protein

03:14.880 --> 03:21.840
to search through all the possible structures to find its correct folded state. In fact, there are

03:21.840 --> 03:28.160
computations showing that even if the protein samples its different conformations at a nanosecond

03:28.160 --> 03:34.400
scale, it would still require more time than the age of the universe to arrive at the correct

03:34.480 --> 03:40.560
configuration. Yet, in reality, proteins fold into their native structures in a matter of

03:40.560 --> 03:47.120
milliseconds. So, how do they accomplish this? When I first heard this paradox in high school,

03:47.120 --> 03:52.800
it seemed to me like an ill-posed question. After all, the protein molecule is not a computer,

03:52.800 --> 03:59.360
so it doesn't do any sort of search. It just folds into the most stable and favorable configuration

03:59.360 --> 04:05.520
according to physical laws. This is similar how when you throw a ball, the ball doesn't

04:05.520 --> 04:11.600
search through all the possible trajectories to select the optimal parabolic one. It simply

04:11.600 --> 04:18.080
follows that path because, well, physics works this way. But how can we think about this folding

04:18.080 --> 04:25.680
into a favorable configuration? Favorable for what exactly? Let's introduce the concept of

04:25.680 --> 04:31.040
energy as it will come in handy in future videos as well. If you think back to your high school

04:31.040 --> 04:37.280
physics days, you may recall something along the lines of energy is a quantitative property that

04:37.280 --> 04:44.480
describes this state of a system, namely the capacity to do work or cause change. Energy can be

04:44.480 --> 04:50.240
stored in a variety of different forms, and for the case of proteins, we will be interested in

04:50.240 --> 04:55.440
potential energy stored in the interactions between the atoms in the protein chain.

04:56.880 --> 05:03.040
Each possible configuration of the protein chain has a specific potential energy level

05:03.040 --> 05:09.520
determined by the sum of all of these atomic interactions. In other words, we can assign

05:09.520 --> 05:15.760
a positive number to each state equal to its energy, which is a function in some very high

05:15.760 --> 05:21.600
dimensional space where different dimensions correspond to degrees of freedom you need

05:21.600 --> 05:28.320
to uniquely describe a configuration. For example, all possible dihedral angles of peptide bonds.

05:30.560 --> 05:36.240
Let's abstractly visualize it as having just two dimensions. Then the energy function can be

05:36.240 --> 05:42.960
thought of as a surface where each point on it represents a possible protein configuration,

05:42.960 --> 05:47.600
and the height of the point represents the potential energy of that configuration.

05:48.240 --> 05:55.200
This is what we are going to refer to as energy landscape. For a protein, it would be a complex

05:55.200 --> 06:02.880
rugged surface with many peaks and valleys. Now, here is the key point. A protein molecule,

06:02.880 --> 06:09.840
like any physical system, tends to minimize its potential energy, guided by this second law of

06:09.840 --> 06:17.200
thermodynamics. It will naturally seek out the configuration that has the lowest possible energy

06:17.200 --> 06:24.480
level, as this represents the optimal arrangement of its atoms. And this, in fact, corresponds to the

06:24.480 --> 06:32.640
native, correctly folded state. When a protein is folding, it is essentially rolling downhill on

06:32.640 --> 06:39.440
the energy landscape, following the steepest path towards the valley. This is why proteins can

06:39.440 --> 06:46.800
fall so quickly. They don't need to search through all possible configurations. They simply follow

06:46.800 --> 06:51.680
the natural tendency of physical systems to minimize their potential energy.

06:52.960 --> 06:58.480
The protein's folding process is guided by the shape of the energy landscape,

06:58.480 --> 07:04.560
which in turn is determined by the interaction between its atoms. And the descent along the

07:04.560 --> 07:10.320
surface is essentially driven by the underlying physical process of energy minimization.

07:12.560 --> 07:19.040
Now, the core idea is to achieve something similar for the case of associative memory. Suppose we

07:19.040 --> 07:25.600
have a system that can encode information in its states, and each configuration has a specific

07:25.600 --> 07:33.440
potential energy determined by the interaction between the states. Then we need to first somehow

07:33.440 --> 07:40.560
sculpt the underlying energy landscape so that memories or state patterns we want to store

07:40.560 --> 07:48.320
correspond to local minima, these wells in the energy surface. Second, we need something that

07:48.320 --> 07:54.800
would play the role of the second law of thermodynamics and would drive the changes in the states,

07:54.800 --> 08:02.160
directing the system towards the nearest local minimum. Once these two things are achieved,

08:02.160 --> 08:08.880
retrieving a memory that is most similar to the input pattern is done by configuring the system

08:08.880 --> 08:15.520
to encode the input pattern initially and letting it run to the equilibrium, descending into the

08:15.520 --> 08:24.000
energy well, from which we can read out the source memory. Sounds neat, right? So let's get into

08:24.000 --> 08:31.120
building it. Let's consider a set of neurons which we can think of as abstract units that can

08:31.120 --> 08:38.160
be in one of two possible states, plus one or minus one. This is a simplified analogy of how nerve

08:38.160 --> 08:43.440
cells in the brain encode information through patterns of firing. They either generate an

08:43.440 --> 08:50.400
electrical impulse at a given point in time or remain silent. We'll focus on the fully connected

08:50.400 --> 08:57.840
network where each neuron has connections to every other neuron. These connections have weights

08:57.840 --> 09:04.800
associated with them, real numbers that signify the strength of coupling between the corresponding

09:04.800 --> 09:11.920
pair of neurons. For a pair of units i and j, we denote the connection weight between them as

09:11.920 --> 09:20.720
wij and the states of neurons themselves as xi and xj. In the brain, connections between neurons

09:20.720 --> 09:27.760
or synapses have a well-defined direction. A pair of neurons is connected asymmetrically,

09:27.760 --> 09:34.240
meaning that the synapse from neuron A to neuron B is physiologically separate from the synapse that

09:34.240 --> 09:41.120
connects B to A if that one exists at all, and so they can have different weights. While we could

09:41.120 --> 09:46.560
generalize a Hopfield network to account for asymmetric connections, it would introduce

09:46.560 --> 09:53.360
complications and potentially unstable behavior. For simplicity, here we will stick to the original

09:53.360 --> 09:59.440
formulation of the Hopfield network, which assumes symmetric weights. In other words,

09:59.440 --> 10:04.080
neurons i and j are connected by the same weight in both directions.

10:04.560 --> 10:11.360
Now that we have a set of neurons symmetrically linked with each other through weighted connections,

10:11.920 --> 10:18.720
let's explore what these connection weights represent. If wij is greater than zero,

10:18.720 --> 10:24.480
the connection is said to be excitatory and favors the alignment between the states of

10:24.480 --> 10:30.960
two neurons. We can think of each connection as being either happy or unhappy, depending on the

10:30.960 --> 10:38.800
states of its neurons. For example, if wij is a large positive number, it means that neurons i

10:38.800 --> 10:45.120
and j are closely coupled, and one excites the other. In this case, when one neuron is active,

10:45.120 --> 10:50.880
the other tends to be active as well, and when one is silent, the other one is more likely to be silent.

10:52.800 --> 10:59.360
These configurations, where both xi and xj are either one or minus one, agree with the

10:59.360 --> 11:05.920
connection weight. However, if we observe, for example, that xi is equal to one and xj is equal

11:05.920 --> 11:13.040
to minus one, it conflicts with the excitatory nature of the connection, making such a configuration

11:13.040 --> 11:21.040
less likely. Conversely, when wij is negative, the connection promotes misalignment between the weights.

11:21.040 --> 11:30.640
This alignment between the signs can be expressed more concisely using the product xi times xj.

11:31.360 --> 11:37.440
This product will be positive when both neurons have the same sign and negative when they have

11:37.440 --> 11:44.320
different signs. By multiplying this product further by the connection weight, we obtain

11:44.320 --> 11:51.840
an expression for the happiness of that connection. For a positive wij, happiness will be positive

11:51.840 --> 11:59.120
when the product of the two states is positive. But this is just one edge. We can extend this idea

11:59.120 --> 12:05.840
and compute the happiness of the entire network as a whole by summing this quantity across all edges.

12:07.040 --> 12:12.880
The larger that number is, the more overall agreement there is between connection weights

12:12.880 --> 12:19.760
and pairwise states of neurons. Ultimately, we will search for a set of weights that maximize

12:19.760 --> 12:26.720
this quantity. And maximizing happiness is equivalent to minimizing it with a minus sign,

12:26.720 --> 12:32.000
which you can think of as the measure of overall conflict between the actual configuration of

12:32.000 --> 12:39.200
states and what's favored by the connection weights. This total conflict between the weights

12:39.200 --> 12:45.360
and pairwise states is exactly what we are going to define to be the energy of the system.

12:46.800 --> 12:53.440
As we discussed previously, we want the Hopfield network to be able to gradually evolve towards

12:53.440 --> 13:00.240
energy minima. But looking closely at the formula, we can see that the energy value depends both on

13:00.240 --> 13:05.840
the states and the weights. So there is a lot of things the system can tweak to change it.

13:05.840 --> 13:11.920
What exactly is getting adjusted? As we will see further, there are essentially

13:11.920 --> 13:18.160
two modes of network updates that nicely map to the two aspects of associative memory.

13:19.280 --> 13:26.080
Namely, adjusting the weights corresponds to shaping the energy landscape, defining

13:26.080 --> 13:32.880
which configurations are stable by digging energy wells around them. This is the act of learning

13:32.880 --> 13:38.080
when we are writing new memories into the network. Once the weights are fixed,

13:38.800 --> 13:44.960
tweaking the states of neurons to bring them into greater agreement with the weights

13:44.960 --> 13:52.320
corresponds to descending along the energy surface. This is the act of inference when we are

13:52.320 --> 13:58.640
recalling the memory that is at the bottom of the energy well, which is nearest to the configuration

13:58.640 --> 14:02.320
of the input pattern. Let's take a look at inference first.

14:05.200 --> 14:11.360
Suppose for a second, someone has already set the weights W and hence us the backbone of the

14:11.360 --> 14:17.920
network. The neurons themselves with all the connection weights. However, the exact configuration

14:17.920 --> 14:24.560
of states, which neurons are active and which are silent, is unknown. The question then becomes,

14:24.560 --> 14:29.520
how do we find the state pattern that would minimize the total energy?

14:30.400 --> 14:37.360
As we discussed, simply checking all possible states is not an option. So, we will start with

14:37.360 --> 14:43.600
some initial state, which could be either a partial or a noisy version of one of the memories

14:43.600 --> 14:51.360
or a random configuration altogether. Once the initial condition is set, we will iteratively

14:51.360 --> 14:58.000
try to lower the energy value by focusing on updating one neuron at a time. Let's denote

14:58.000 --> 15:04.480
the neuron we are currently considering as neuron i. We will calculate the total weighted input to

15:04.480 --> 15:11.920
it from all other neurons in the network. This input, which we'll denote as hi, is the sum of

15:11.920 --> 15:18.800
the states of all other neurons multiplied by their respective connection weights. If hi is

15:18.800 --> 15:25.840
positive, it means that the weighted sum of the other neuron states is in favor of neuron i being

15:25.840 --> 15:33.760
in the plus one state. Conversely, if hi is negative, it suggests that neuron i should be in the minus

15:33.760 --> 15:40.720
one state to minimize the conflict with the other neurons. So, we will update the state of the neuron

15:40.720 --> 15:48.000
i based on the sign of hi. Notice that this update is guaranteed to decrease the energy

15:48.000 --> 15:53.520
of the network, because from the two candidate states, we are selecting the more energetically

15:53.520 --> 16:00.400
favorable one. You can think of this as a kind of a voting process. Each neuron looks at the states of

16:00.400 --> 16:06.720
all other neurons weighted by the strength of their connections and decides whether to be active

16:06.720 --> 16:13.840
or silent based on the majority vote. We'll go through this process for each neuron in the network

16:13.840 --> 16:21.840
one by one chosen in random order, updating their states based on the input from all other neurons.

16:22.640 --> 16:29.280
Once we've updated all neurons, we will have completed one iteration of the network inference

16:29.280 --> 16:35.760
and decreased the system's energy by a little bit. We'll keep repeating this process, doing these

16:35.760 --> 16:42.320
sweeps through all neurons, updating them one at a time based on the current configuration.

16:42.320 --> 16:50.000
As we do this, the network will gradually evolve towards a configuration that minimizes the overall

16:50.000 --> 16:57.440
energy. At some point, however, we will reach a configuration where flipping any neuron would

16:57.440 --> 17:03.760
lead to an increase in energy. So, no further adjustments would be necessary. At that point,

17:03.760 --> 17:10.560
the network has converged to a stable configuration, where each neuron's state agrees with the majority

17:10.560 --> 17:16.320
vote. This stable configuration represents a local minimum in the energy landscape.

17:16.960 --> 17:22.800
Now, you might be wondering, is the network guaranteed to reach such a stable configuration?

17:22.800 --> 17:29.440
Could we possibly stumble into a particularly unlucky set of states and get stuck in a never-ending

17:29.440 --> 17:36.880
loop of flipping neurons back and forth? In other words, is such iterative flipping of one neuron

17:36.880 --> 17:43.600
at a time equivalent to doing a descent along the energy surface? This is where we come back to

17:43.600 --> 17:50.160
the point about symmetric weights. It turns out that there is a mathematical proof that I'm not

17:50.160 --> 17:57.520
going to cover here, stating that as long as your weights are symmetric, this simple majority vote

17:57.520 --> 18:04.240
single neuron update rule is guaranteed to eventually converge to a stable configuration

18:04.240 --> 18:11.280
if you do it enough times. To restate it, the Hopfield network can settle into different local

18:11.280 --> 18:17.760
minima based on its initial conditions. These local minima in the energy landscape correspond

18:17.760 --> 18:24.320
to distinct memories stored in the network. When we initialize the network with a pattern that is

18:24.320 --> 18:30.560
similar to one of these memories in some way and let it evolve, it will fall into the nearest

18:30.560 --> 18:37.680
local minimum, effectively recalling the complete stored memory, thus performing pattern completion

18:37.680 --> 18:43.840
or noise correction. But so far we haven't talked about how we come up with the set of connection

18:43.840 --> 18:50.560
weights that encode specific memories in the first place. So let's explore the learning process.

18:52.560 --> 18:58.960
Before we move to storing several memories, let's consider memorizing a single pattern of states.

18:59.600 --> 19:03.360
That means the network would have a single global minimum,

19:03.360 --> 19:09.520
one energy well, and would converge to the same pattern every time no matter where you initialize

19:09.520 --> 19:15.680
it. While it has little practical use, it provides a nice starting point to describe the learning

19:15.680 --> 19:23.600
procedure. Let's denote the template pattern that we'd like to store as XC, which is a vector

19:23.600 --> 19:31.760
packing the states of all neurons, and XCI will denote the ith component, the state of ith neuron

19:31.760 --> 19:38.400
encoding the memory. While XI refers to the state of ith neuron in the network in general,

19:38.400 --> 19:47.440
which could be tweaked. Revisiting our definition of energy, we want to set Wij so that this quantity

19:47.440 --> 19:55.520
would be at its minimal value for the memory pattern. If we plug XI equal to XCI, we get the

19:55.520 --> 20:01.920
equation for the energy of the reference pattern as a function of weights, which we want to turn

20:01.920 --> 20:08.400
into a global minimum. Notice that we don't really care about the absolute value of that energy.

20:08.960 --> 20:14.400
As long as the energy of the desired memory pattern is less than the energy of any other

20:14.400 --> 20:21.280
configuration. Now intuitively, the lowest possible energy is obtained when all the

20:21.280 --> 20:28.560
connection weights fully align with the state pairs. But when we have just a single pattern,

20:28.560 --> 20:36.160
this is very easy to do. All we need is to set the weight Wij to be the product of the corresponding

20:36.160 --> 20:42.800
pair of states in the memory pattern. This way, every connection is satisfied and the energy of

20:42.800 --> 20:48.480
the network when it is in the state XI becomes the negative of the total number of edges.

20:49.120 --> 20:56.000
When the network is in the state XI, any single flip of a neuron would increase the energy,

20:56.000 --> 21:03.040
thus making it a stable state. I want to reiterate the crucial point here. If we want to come up with

21:03.040 --> 21:09.840
the set of weights that would dig an energy well around some pattern, then all we need to know

21:09.840 --> 21:16.720
are the pairwise relationships between states in that pattern. If the two neurons are active

21:16.720 --> 21:22.560
together in the source memory, strengthening the connection between them lowers the hopfield

21:22.560 --> 21:28.320
energy of that memory, effectively storing it in the weights for associative recall.

21:29.280 --> 21:33.440
You may have heard the famous statement from Neuroscience attributed to Donald

21:33.840 --> 21:40.640
Hebb, neurons that fire together, wire together. And in fact, what we just did is known as the

21:40.640 --> 21:47.840
Hebbian learning rule. Great, so we found a way to make a single pattern a stable state of the

21:47.840 --> 21:55.520
network. But we want to store multiple patterns. How do we do that? Here's the key idea. We can

21:55.520 --> 22:02.240
simply sum the weights we would get for each pattern separately. So if we have three patterns,

22:02.320 --> 22:07.760
X1, X2 and X3, we can set the weights according to the following equation.

22:08.800 --> 22:15.200
What this will do is turn each of the patterns into a local minimum. It's pretty straightforward

22:15.200 --> 22:20.400
to show mathematically, and if you're interested, I encourage you to check out the references

22:20.400 --> 22:26.960
in the video description. However, intuitively, if the patterns you want to store are very

22:26.960 --> 22:34.400
different, so they are far away in the state space from each other, then if you first independently

22:34.400 --> 22:41.040
dig energy wells around each of them, and then simply add the energy landscapes together,

22:41.680 --> 22:48.960
the resulting surface will have local minima in the same three valleys. And this nicely brings

22:48.960 --> 22:56.160
us to the limitation of the hopfield networks. There is a limited number of valleys we can sculpt

22:56.160 --> 23:02.320
in the energy landscape before they start to interfere with each other. At some point,

23:02.320 --> 23:09.680
if we try to store too many patterns, the network will fail to converge to a stored pattern reliably

23:09.680 --> 23:16.000
and recall weird in-between kind of memories. The total maximum number of patterns you can

23:16.000 --> 23:23.680
store is thus limited and depends only on the size of the network. It is approximately 0.14

23:23.680 --> 23:30.720
times the number of neurons. So, if you have a hopfield network of 100 neurons, you can reliably

23:30.720 --> 23:37.280
store less than 14 patterns in the best case scenario. If you are unlucky, however, and some

23:37.280 --> 23:44.560
patterns are similar to each other or correlated, their energy wells will begin to interfere even

23:44.560 --> 23:50.720
before you reach the full capacity. All of this makes vanilla hopfield networks not useful for

23:50.800 --> 23:57.520
practical purposes. However, to this day, they provide a powerful and intuitive model of associative

23:57.520 --> 24:04.560
memory, a simple network of neuron-like units that can store and retrieve pattern through purely

24:04.560 --> 24:10.160
local learning and inference rules. Despite their limitations, hopfield networks have laid the

24:10.160 --> 24:16.720
groundwork for more advanced energy-based models. In one of the next videos, we will look at the

24:16.720 --> 24:23.520
extension of the hopfield networks known as Boltzmann machines. These generative architectures

24:23.520 --> 24:29.520
introduce additional hidden units and stochastic dynamics, allowing them to learn more complex

24:29.520 --> 24:35.360
probability distributions. There is also an extension to modern hopfield networks,

24:35.360 --> 24:41.600
published in 2016 with John Hopfield himself as one of the authors, but that's a topic for another

24:41.600 --> 24:47.040
time. In the meanwhile, I'd like to take a moment and thank Shortform, who are kindly sponsoring

24:47.040 --> 24:53.280
today's video. Shortform is a platform that lets you supercharge your reading and gain valuable

24:53.280 --> 25:00.320
insights from books. Their unique approach of book guides goes way beyond simple summaries,

25:00.320 --> 25:06.720
by providing a comprehensive overview of the material. Not only do you get a concise version

25:06.720 --> 25:14.000
of the main points, but you also benefit from related ideas sourced from other books and research

25:14.000 --> 25:20.640
papers on the topic. They have an actively growing library of books from all sorts of genres,

25:20.640 --> 25:26.080
such as science, health and technology. Not only that, but there is a useful AI-powered

25:26.080 --> 25:32.480
browser extension that allows you to generate similar guides for arbitrary content on the

25:32.480 --> 25:37.920
internet. Personally, I found Shortform to be really helpful, both when I'm choosing books to

25:37.920 --> 25:44.880
read and writing notes and flashcards on the topic. Don't hesitate to bring your reading to the next

25:44.880 --> 25:52.400
level by clicking the link down in the description to get 5 dates of unlimited access and 20% off

25:52.400 --> 25:58.080
on annual subscription. If you liked the video, share it with your friends, press the like button

25:58.080 --> 26:02.640
and subscribe to the channel if you haven't already. Stay tuned for more computational

26:02.640 --> 26:08.480
neuroscience and machine learning topics coming up. Goodbye and thank you for the interest in the brain.

26:28.080 --> 26:40.480
Thank you for watching and I'll see you in the next video.

